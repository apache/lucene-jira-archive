Index: src/test/org/apache/lucene/queryParser/TestMultiAnalyzer.java
===================================================================
--- src/test/org/apache/lucene/queryParser/TestMultiAnalyzer.java	(revision 686801)
+++ src/test/org/apache/lucene/queryParser/TestMultiAnalyzer.java	(working copy)
@@ -141,34 +141,34 @@
 
   private final class TestFilter extends TokenFilter {
     
-    private org.apache.lucene.analysis.Token prevToken;
+    private Token prevToken;
     
     public TestFilter(TokenStream in) {
       super(in);
     }
 
-    public final org.apache.lucene.analysis.Token next() throws java.io.IOException {
+    public final Token next(final Token reusableToken) throws java.io.IOException {
       if (multiToken > 0) {
-        org.apache.lucene.analysis.Token token = 
-          new org.apache.lucene.analysis.Token("multi"+(multiToken+1), prevToken.startOffset(),
-          prevToken.endOffset(), prevToken.type());
-        token.setPositionIncrement(0);
+        reusableToken.reinit("multi"+(multiToken+1), prevToken.startOffset(), prevToken.endOffset(), prevToken.type());
+        reusableToken.setPositionIncrement(0);
         multiToken--;
-        return token;
+        return reusableToken;
       } else {
-        org.apache.lucene.analysis.Token t = input.next();
-        prevToken = t;
-        if (t == null)
+        Token nextToken = input.next(reusableToken);
+        if (nextToken == null) {
+          prevToken = null;
           return null;
-        String text = t.termText();
+        }
+        prevToken = (Token) nextToken.clone();
+        String text = nextToken.term();
         if (text.equals("triplemulti")) {
           multiToken = 2;
-          return t;
+          return nextToken;
         } else if (text.equals("multi")) {
           multiToken = 1;
-          return t;
+          return nextToken;
         } else {
-          return t;
+          return nextToken;
         }
       }
     }
@@ -197,22 +197,16 @@
       super(in);
     }
 
-    public final org.apache.lucene.analysis.Token next() throws java.io.IOException {
-      for (Token t = input.next(); t != null; t = input.next()) {
-        if (t.termText().equals("the")) {
+    public final Token next(final Token reusableToken) throws java.io.IOException {
+      for (Token nextToken = input.next(reusableToken); nextToken != null; nextToken = input.next(reusableToken)) {
+        if (nextToken.term().equals("the")) {
           // stopword, do nothing
-        } else if (t.termText().equals("quick")) {
-          org.apache.lucene.analysis.Token token = 
-            new org.apache.lucene.analysis.Token(t.termText(), t.startOffset(),
-                t.endOffset(), t.type());
-          token.setPositionIncrement(2);
-          return token;
+        } else if (nextToken.term().equals("quick")) {
+          nextToken.setPositionIncrement(2);
+          return nextToken;
         } else {
-          org.apache.lucene.analysis.Token token = 
-            new org.apache.lucene.analysis.Token(t.termText(), t.startOffset(),
-                t.endOffset(), t.type());
-          token.setPositionIncrement(1);
-          return token;
+          nextToken.setPositionIncrement(1);
+          return nextToken;
         }
       }
       return null;
Index: src/test/org/apache/lucene/queryParser/TestMultiFieldQueryParser.java
===================================================================
--- src/test/org/apache/lucene/queryParser/TestMultiFieldQueryParser.java	(revision 686801)
+++ src/test/org/apache/lucene/queryParser/TestMultiFieldQueryParser.java	(working copy)
@@ -319,7 +319,7 @@
     }
 
     private static class EmptyTokenStream extends TokenStream {
-      public Token next() {
+      public Token next(final Token reusableToken) {
         return null;
       }
     }
Index: src/test/org/apache/lucene/queryParser/TestQueryParser.java
===================================================================
--- src/test/org/apache/lucene/queryParser/TestQueryParser.java	(revision 686801)
+++ src/test/org/apache/lucene/queryParser/TestQueryParser.java	(working copy)
@@ -75,19 +75,20 @@
     boolean inPhrase = false;
     int savedStart = 0, savedEnd = 0;
 
-    public Token next() throws IOException {
+    public Token next(final Token reusableToken) throws IOException {
+      assert reusableToken != null;
       if (inPhrase) {
         inPhrase = false;
-        return new Token("phrase2", savedStart, savedEnd);
+        return reusableToken.reinit("phrase2", savedStart, savedEnd);
       } else
-        for (Token token = input.next(); token != null; token = input.next()) {
-          if (token.termText().equals("phrase")) {
+        for (Token nextToken = input.next(reusableToken); nextToken != null; nextToken = input.next(reusableToken)) {
+          if (nextToken.term().equals("phrase")) {
             inPhrase = true;
-            savedStart = token.startOffset();
-            savedEnd = token.endOffset();
-            return new Token("phrase1", savedStart, savedEnd);
-          } else if (!token.termText().equals("stop"))
-            return token;
+            savedStart = nextToken.startOffset();
+            savedEnd = nextToken.endOffset();
+            return nextToken.reinit("phrase1", savedStart, savedEnd);
+          } else if (!nextToken.term().equals("stop"))
+            return nextToken;
         }
       return null;
     }
Index: src/test/org/apache/lucene/analysis/TestToken.java
===================================================================
--- src/test/org/apache/lucene/analysis/TestToken.java	(revision 686801)
+++ src/test/org/apache/lucene/analysis/TestToken.java	(working copy)
@@ -17,7 +17,6 @@
  * limitations under the License.
  */
 
-import java.io.*;
 import org.apache.lucene.util.LuceneTestCase;
 
 public class TestToken extends LuceneTestCase {
@@ -26,6 +25,119 @@
     super(name);
   }
 
+  public void testCtor() throws Exception {
+    Token t = new Token();
+    char[] content = "hello".toCharArray();
+    t.setTermBuffer(content, 0, content.length);
+    char[] buf = t.termBuffer();
+    assertNotSame(t.termBuffer(), content);
+    assertEquals("hello", t.term());
+    assertEquals("word", t.type());
+    assertEquals(0, t.getFlags());
+
+    t = new Token(6, 22);
+    t.setTermBuffer(content, 0, content.length);
+    assertEquals("hello", t.term());
+    assertEquals("(hello,6,22)", t.toString());
+    assertEquals("word", t.type());
+    assertEquals(0, t.getFlags());
+
+    t = new Token(6, 22, 7);
+    t.setTermBuffer(content, 0, content.length);
+    assertEquals("hello", t.term());
+    assertEquals("(hello,6,22)", t.toString());
+    assertEquals(7, t.getFlags());
+
+    t = new Token(6, 22, "junk");
+    t.setTermBuffer(content, 0, content.length);
+    assertEquals("hello", t.term());
+    assertEquals("(hello,6,22,type=junk)", t.toString());
+    assertEquals(0, t.getFlags());
+  }
+
+  public void testResize() {
+    Token t = new Token();
+    char[] content = "hello".toCharArray();
+    t.setTermBuffer(content, 0, content.length);
+    for (int i = 0; i < 2000; i++)
+    {
+      t.resizeTermBuffer(i);
+      assertTrue(i <= t.termBuffer().length);
+      assertEquals("hello", t.term());
+    }
+  }
+
+  public void testGrow() {
+    Token t = new Token();
+    StringBuffer buf = new StringBuffer("ab");
+    for (int i = 0; i < 20; i++)
+    {
+      char[] content = buf.toString().toCharArray();
+      t.setTermBuffer(content, 0, content.length);
+      assertEquals(buf.length(), t.termLength());
+      assertEquals(buf.toString(), t.term());
+      buf.append(buf.toString());
+    }
+    assertEquals(1048576, t.termLength());
+    assertEquals(1179654, t.termBuffer().length);
+
+    // now as a string, first variant
+    t = new Token();
+    buf = new StringBuffer("ab");
+    for (int i = 0; i < 20; i++)
+    {
+      String content = buf.toString();
+      t.setTermBuffer(content, 0, content.length());
+      assertEquals(content.length(), t.termLength());
+      assertEquals(content, t.term());
+      buf.append(content);
+    }
+    assertEquals(1048576, t.termLength());
+    assertEquals(1179654, t.termBuffer().length);
+
+    // now as a string, second variant
+    t = new Token();
+    buf = new StringBuffer("ab");
+    for (int i = 0; i < 20; i++)
+    {
+      String content = buf.toString();
+      t.setTermBuffer(content);
+      assertEquals(content.length(), t.termLength());
+      assertEquals(content, t.term());
+      buf.append(content);
+    }
+    assertEquals(1048576, t.termLength());
+    assertEquals(1179654, t.termBuffer().length);
+
+    // Test for slow growth to a long term
+    t = new Token();
+    buf = new StringBuffer("a");
+    for (int i = 0; i < 20000; i++)
+    {
+      String content = buf.toString();
+      t.setTermBuffer(content);
+      assertEquals(content.length(), t.termLength());
+      assertEquals(content, t.term());
+      buf.append("a");
+    }
+    assertEquals(20000, t.termLength());
+    assertEquals(20331, t.termBuffer().length);
+
+    // Test for slow growth to a long term
+    t = new Token();
+    buf = new StringBuffer("a");
+    for (int i = 0; i < 20000; i++)
+    {
+      String content = buf.toString();
+      t.setTermBuffer(content);
+      assertEquals(content.length(), t.termLength());
+      assertEquals(content, t.term());
+      buf.append("a");
+    }
+    assertEquals(20000, t.termLength());
+    assertEquals(20331, t.termBuffer().length);
+  }
+
   public void testToString() throws Exception {
     char[] b = {'a', 'l', 'o', 'h', 'a'};
     Token t = new Token("", 0, 5);
@@ -40,10 +152,10 @@
     Token t = new Token("hello", 0, 5);
     assertEquals(t.termText(), "hello");
     assertEquals(t.termLength(), 5);
-    assertEquals(new String(t.termBuffer(), 0, 5), "hello");
+    assertEquals(t.term(), "hello");
     t.setTermText("hello2");
     assertEquals(t.termLength(), 6);
-    assertEquals(new String(t.termBuffer(), 0, 6), "hello2");
+    assertEquals(t.term(), "hello2");
     t.setTermBuffer("hello3".toCharArray(), 0, 6);
     assertEquals(t.termText(), "hello3");
 
@@ -53,4 +165,13 @@
     buffer[1] = 'o';
     assertEquals(t.termText(), "hollo3");
   }
+  
+  public void testClone() throws Exception {
+    Token t = new Token(0, 5);
+    char[] content = "hello".toCharArray();
+    t.setTermBuffer(content, 0, 5);
+    char[] buf = t.termBuffer();
+    Token copy = (Token) t.clone();
+    assertNotSame(buf, copy.termBuffer());
+  }
 }
Index: src/test/org/apache/lucene/analysis/TestPerFieldAnalzyerWrapper.java
===================================================================
--- src/test/org/apache/lucene/analysis/TestPerFieldAnalzyerWrapper.java	(revision 686801)
+++ src/test/org/apache/lucene/analysis/TestPerFieldAnalzyerWrapper.java	(working copy)
@@ -29,16 +29,17 @@
 
     TokenStream tokenStream = analyzer.tokenStream("field",
                                             new StringReader(text));
-    Token token = tokenStream.next();
+    final Token reusableToken = new Token();
+    Token nextToken = tokenStream.next(reusableToken);
     assertEquals("WhitespaceAnalyzer does not lowercase",
                  "Qwerty",
-                 token.termText());
+                 nextToken.term());
 
     tokenStream = analyzer.tokenStream("special",
                                             new StringReader(text));
-    token = tokenStream.next();
+    nextToken = tokenStream.next(reusableToken);
     assertEquals("SimpleAnalyzer lowercases",
                  "qwerty",
-                 token.termText());
+                 nextToken.term());
   }
 }
Index: src/test/org/apache/lucene/analysis/TeeSinkTokenTest.java
===================================================================
--- src/test/org/apache/lucene/analysis/TeeSinkTokenTest.java	(revision 686801)
+++ src/test/org/apache/lucene/analysis/TeeSinkTokenTest.java	(working copy)
@@ -16,10 +16,10 @@
  * limitations under the License.
  */
 
-import junit.framework.TestCase;
 import org.apache.lucene.analysis.standard.StandardFilter;
 import org.apache.lucene.analysis.standard.StandardTokenizer;
 import org.apache.lucene.util.English;
+import org.apache.lucene.util.LuceneTestCase;
 
 import java.io.IOException;
 import java.io.StringReader;
@@ -29,7 +29,7 @@
 /**
  * tests for the TeeTokenFilter and SinkTokenizer
  */
-public class TeeSinkTokenTest extends TestCase {
+public class TeeSinkTokenTest extends LuceneTestCase {
   protected StringBuffer buffer1;
   protected StringBuffer buffer2;
   protected String[] tokens1;
@@ -63,23 +63,23 @@
 
     SinkTokenizer sink1 = new SinkTokenizer(null) {
       public void add(Token t) {
-        if (t != null && t.termText().equalsIgnoreCase("The")) {
+        if (t != null && t.term().equalsIgnoreCase("The")) {
           super.add(t);
         }
       }
     };
     TokenStream source = new TeeTokenFilter(new WhitespaceTokenizer(new StringReader(buffer1.toString())), sink1);
-    Token token = null;
     int i = 0;
-    while ((token = source.next()) != null) {
-      assertTrue(token.termText() + " is not equal to " + tokens1[i], token.termText().equals(tokens1[i]) == true);
+    final Token reusableToken = new Token();
+    for (Token nextToken = source.next(reusableToken); nextToken != null; nextToken = source.next(reusableToken)) {
+      assertTrue(nextToken.term() + " is not equal to " + tokens1[i], nextToken.term().equals(tokens1[i]) == true);
       i++;
     }
     assertTrue(i + " does not equal: " + tokens1.length, i == tokens1.length);
     assertTrue("sink1 Size: " + sink1.getTokens().size() + " is not: " + 2, sink1.getTokens().size() == 2);
     i = 0;
-    while ((token = sink1.next()) != null) {
-      assertTrue(token.termText() + " is not equal to " + "The", token.termText().equalsIgnoreCase("The") == true);
+    for (Token token = sink1.next(reusableToken); token != null; token = sink1.next(reusableToken)) {
+      assertTrue(token.term() + " is not equal to " + "The", token.term().equalsIgnoreCase("The") == true);
       i++;
     }
     assertTrue(i + " does not equal: " + sink1.getTokens().size(), i == sink1.getTokens().size());
@@ -88,54 +88,54 @@
   public void testMultipleSources() throws Exception {
     SinkTokenizer theDetector = new SinkTokenizer(null) {
       public void add(Token t) {
-        if (t != null && t.termText().equalsIgnoreCase("The")) {
+        if (t != null && t.term().equalsIgnoreCase("The")) {
           super.add(t);
         }
       }
     };
     SinkTokenizer dogDetector = new SinkTokenizer(null) {
       public void add(Token t) {
-        if (t != null && t.termText().equalsIgnoreCase("Dogs")) {
+        if (t != null && t.term().equalsIgnoreCase("Dogs")) {
           super.add(t);
         }
       }
     };
     TokenStream source1 = new CachingTokenFilter(new TeeTokenFilter(new TeeTokenFilter(new WhitespaceTokenizer(new StringReader(buffer1.toString())), theDetector), dogDetector));
     TokenStream source2 = new TeeTokenFilter(new TeeTokenFilter(new WhitespaceTokenizer(new StringReader(buffer2.toString())), theDetector), dogDetector);
-    Token token = null;
     int i = 0;
-    while ((token = source1.next()) != null) {
-      assertTrue(token.termText() + " is not equal to " + tokens1[i], token.termText().equals(tokens1[i]) == true);
+    final Token reusableToken = new Token();
+    for (Token nextToken = source1.next(reusableToken); nextToken != null; nextToken = source1.next(reusableToken)) {
+      assertTrue(nextToken.term() + " is not equal to " + tokens1[i], nextToken.term().equals(tokens1[i]) == true);
       i++;
     }
     assertTrue(i + " does not equal: " + tokens1.length, i == tokens1.length);
     assertTrue("theDetector Size: " + theDetector.getTokens().size() + " is not: " + 2, theDetector.getTokens().size() == 2);
     assertTrue("dogDetector Size: " + dogDetector.getTokens().size() + " is not: " + 1, dogDetector.getTokens().size() == 1);
     i = 0;
-    while ((token = source2.next()) != null) {
-      assertTrue(token.termText() + " is not equal to " + tokens2[i], token.termText().equals(tokens2[i]) == true);
+    for (Token nextToken = source2.next(reusableToken); nextToken != null; nextToken = source2.next(reusableToken)) {
+      assertTrue(nextToken.term() + " is not equal to " + tokens2[i], nextToken.term().equals(tokens2[i]) == true);
       i++;
     }
     assertTrue(i + " does not equal: " + tokens2.length, i == tokens2.length);
     assertTrue("theDetector Size: " + theDetector.getTokens().size() + " is not: " + 4, theDetector.getTokens().size() == 4);
     assertTrue("dogDetector Size: " + dogDetector.getTokens().size() + " is not: " + 2, dogDetector.getTokens().size() == 2);
     i = 0;
-    while ((token = theDetector.next()) != null) {
-      assertTrue(token.termText() + " is not equal to " + "The", token.termText().equalsIgnoreCase("The") == true);
+    for (Token nextToken = theDetector.next(reusableToken); nextToken != null; nextToken = theDetector.next(reusableToken)) {
+      assertTrue(nextToken.term() + " is not equal to " + "The", nextToken.term().equalsIgnoreCase("The") == true);
       i++;
     }
     assertTrue(i + " does not equal: " + theDetector.getTokens().size(), i == theDetector.getTokens().size());
     i = 0;
-    while ((token = dogDetector.next()) != null) {
-      assertTrue(token.termText() + " is not equal to " + "Dogs", token.termText().equalsIgnoreCase("Dogs") == true);
+    for (Token nextToken = dogDetector.next(reusableToken); nextToken != null; nextToken = dogDetector.next(reusableToken)) {
+      assertTrue(nextToken.term() + " is not equal to " + "Dogs", nextToken.term().equalsIgnoreCase("Dogs") == true);
       i++;
     }
     assertTrue(i + " does not equal: " + dogDetector.getTokens().size(), i == dogDetector.getTokens().size());
     source1.reset();
     TokenStream lowerCasing = new LowerCaseFilter(source1);
     i = 0;
-    while ((token = lowerCasing.next()) != null) {
-      assertTrue(token.termText() + " is not equal to " + tokens1[i].toLowerCase(), token.termText().equals(tokens1[i].toLowerCase()) == true);
+    for (Token nextToken = lowerCasing.next(reusableToken); nextToken != null; nextToken = lowerCasing.next(reusableToken)) {
+      assertTrue(nextToken.term() + " is not equal to " + tokens1[i].toLowerCase(), nextToken.term().equals(tokens1[i].toLowerCase()) == true);
       i++;
     }
     assertTrue(i + " does not equal: " + tokens1.length, i == tokens1.length);
@@ -157,22 +157,21 @@
       }
       //make sure we produce the same tokens
       ModuloSinkTokenizer sink = new ModuloSinkTokenizer(tokCount[k], 100);
-      Token next = new Token();
-      TokenStream result = new TeeTokenFilter(new StandardFilter(new StandardTokenizer(new StringReader(buffer.toString()))), sink);
-      while ((next = result.next(next)) != null) {
+      final Token reusableToken = new Token();
+      TokenStream stream = new TeeTokenFilter(new StandardFilter(new StandardTokenizer(new StringReader(buffer.toString()))), sink);
+      while (stream.next(reusableToken) != null) {
       }
-      result = new ModuloTokenFilter(new StandardFilter(new StandardTokenizer(new StringReader(buffer.toString()))), 100);
-      next = new Token();
+      stream = new ModuloTokenFilter(new StandardFilter(new StandardTokenizer(new StringReader(buffer.toString()))), 100);
       List tmp = new ArrayList();
-      while ((next = result.next(next)) != null) {
-        tmp.add(next.clone());
+      for (Token nextToken = stream.next(reusableToken); nextToken != null; nextToken = stream.next(reusableToken)) {
+        tmp.add(nextToken.clone());
       }
       List sinkList = sink.getTokens();
       assertTrue("tmp Size: " + tmp.size() + " is not: " + sinkList.size(), tmp.size() == sinkList.size());
       for (int i = 0; i < tmp.size(); i++) {
         Token tfTok = (Token) tmp.get(i);
         Token sinkTok = (Token) sinkList.get(i);
-        assertTrue(tfTok.termText() + " is not equal to " + sinkTok.termText() + " at token: " + i, tfTok.termText().equals(sinkTok.termText()) == true);
+        assertTrue(tfTok.term() + " is not equal to " + sinkTok.term() + " at token: " + i, tfTok.term().equals(sinkTok.term()) == true);
       }
       //simulate two fields, each being analyzed once, for 20 documents
 
@@ -180,15 +179,13 @@
         int tfPos = 0;
         long start = System.currentTimeMillis();
         for (int i = 0; i < 20; i++) {
-          next = new Token();
-          result = new StandardFilter(new StandardTokenizer(new StringReader(buffer.toString())));
-          while ((next = result.next(next)) != null) {
-            tfPos += next.getPositionIncrement();
+          stream = new StandardFilter(new StandardTokenizer(new StringReader(buffer.toString())));
+          for (Token nextToken = stream.next(reusableToken); nextToken != null; nextToken = stream.next(reusableToken)) {
+            tfPos += nextToken.getPositionIncrement();
           }
-          next = new Token();
-          result = new ModuloTokenFilter(new StandardFilter(new StandardTokenizer(new StringReader(buffer.toString()))), modCounts[j]);
-          while ((next = result.next(next)) != null) {
-            tfPos += next.getPositionIncrement();
+          stream = new ModuloTokenFilter(new StandardFilter(new StandardTokenizer(new StringReader(buffer.toString()))), modCounts[j]);
+          for (Token nextToken = stream.next(reusableToken); nextToken != null; nextToken = stream.next(reusableToken)) {
+            tfPos += nextToken.getPositionIncrement();
           }
         }
         long finish = System.currentTimeMillis();
@@ -198,15 +195,14 @@
         start = System.currentTimeMillis();
         for (int i = 0; i < 20; i++) {
           sink = new ModuloSinkTokenizer(tokCount[k], modCounts[j]);
-          next = new Token();
-          result = new TeeTokenFilter(new StandardFilter(new StandardTokenizer(new StringReader(buffer.toString()))), sink);
-          while ((next = result.next(next)) != null) {
-            sinkPos += next.getPositionIncrement();
+          stream = new TeeTokenFilter(new StandardFilter(new StandardTokenizer(new StringReader(buffer.toString()))), sink);
+          for (Token nextToken = stream.next(reusableToken); nextToken != null; nextToken = stream.next(reusableToken)) {
+            sinkPos += nextToken.getPositionIncrement();
           }
           //System.out.println("Modulo--------");
-          result = sink;
-          while ((next = result.next(next)) != null) {
-            sinkPos += next.getPositionIncrement();
+          stream = sink;
+          for (Token nextToken = stream.next(reusableToken); nextToken != null; nextToken = stream.next(reusableToken)) {
+            sinkPos += nextToken.getPositionIncrement();
           }
         }
         finish = System.currentTimeMillis();
@@ -232,13 +228,15 @@
     int count = 0;
 
     //return every 100 tokens
-    public Token next(Token result) throws IOException {
-
-      while ((result = input.next(result)) != null && count % modCount != 0) {
+    public Token next(final Token reusableToken) throws IOException {
+      Token nextToken = null;
+      for (nextToken = input.next(reusableToken);
+           nextToken != null && count % modCount != 0;
+           nextToken = input.next(reusableToken)) {
         count++;
       }
       count++;
-      return result;
+      return nextToken;
     }
   }
 
@@ -254,7 +252,7 @@
 
     public void add(Token t) {
       if (t != null && count % modCount == 0) {
-        lst.add(t.clone());
+        super.add(t);
       }
       count++;
     }
Index: src/test/org/apache/lucene/analysis/TestCachingTokenFilter.java
===================================================================
--- src/test/org/apache/lucene/analysis/TestCachingTokenFilter.java	(revision 686801)
+++ src/test/org/apache/lucene/analysis/TestCachingTokenFilter.java	(working copy)
@@ -42,11 +42,12 @@
     TokenStream stream = new TokenStream() {
       private int index = 0;
       
-      public Token next() throws IOException {
+      public Token next(final Token reusableToken) throws IOException {
+        assert reusableToken != null;
         if (index == tokens.length) {
           return null;
         } else {
-          return new Token(tokens[index++], 0, 0);
+          return reusableToken.reinit(tokens[index++], 0, 0);
         }        
       }
       
@@ -91,10 +92,10 @@
   
   private void checkTokens(TokenStream stream) throws IOException {
     int count = 0;
-    Token token;
-    while ((token = stream.next()) != null) {
+    final Token reusableToken = new Token();
+    for (Token nextToken = stream.next(reusableToken); nextToken != null; nextToken = stream.next(reusableToken)) {
       assertTrue(count < tokens.length);
-      assertEquals(tokens[count], token.termText());
+      assertEquals(tokens[count], nextToken.term());
       count++;
     }
     
Index: src/test/org/apache/lucene/analysis/TestStandardAnalyzer.java
===================================================================
--- src/test/org/apache/lucene/analysis/TestStandardAnalyzer.java	(revision 686801)
+++ src/test/org/apache/lucene/analysis/TestStandardAnalyzer.java	(working copy)
@@ -35,18 +35,19 @@
 
   public void assertAnalyzesTo(Analyzer a, String input, String[] expectedImages, String[] expectedTypes, int[] expectedPosIncrs) throws Exception {
     TokenStream ts = a.tokenStream("dummy", new StringReader(input));
+    final Token reusableToken = new Token();
     for (int i = 0; i < expectedImages.length; i++) {
-      Token t = ts.next();
-      assertNotNull(t);
-      assertEquals(expectedImages[i], t.termText());
+      Token nextToken = ts.next(reusableToken);
+      assertNotNull(nextToken);
+      assertEquals(expectedImages[i], nextToken.term());
       if (expectedTypes != null) {
-        assertEquals(expectedTypes[i], t.type());
+        assertEquals(expectedTypes[i], nextToken.type());
       }
       if (expectedPosIncrs != null) {
-        assertEquals(expectedPosIncrs[i], t.getPositionIncrement());
+        assertEquals(expectedPosIncrs[i], nextToken.getPositionIncrement());
       }
     }
-    assertNull(ts.next());
+    assertNull(ts.next(reusableToken));
     ts.close();
   }
 
Index: src/test/org/apache/lucene/analysis/TestISOLatin1AccentFilter.java
===================================================================
--- src/test/org/apache/lucene/analysis/TestISOLatin1AccentFilter.java	(revision 686801)
+++ src/test/org/apache/lucene/analysis/TestISOLatin1AccentFilter.java	(working copy)
@@ -25,81 +25,82 @@
   public void testU() throws Exception {
     TokenStream stream = new WhitespaceTokenizer(new StringReader("Des mot clés À LA CHAÎNE À Á Â Ã Ä Å Æ Ç È É Ê Ë Ì Í Î Ï Ĳ Ð Ñ Ò Ó Ô Õ Ö Ø Œ Þ Ù Ú Û Ü Ý Ÿ à á â ã ä å æ ç è é ê ë ì í î ï ĳ ð ñ ò ó ô õ ö ø œ ß þ ù ú û ü ý ÿ ﬁ ﬂ"));
     ISOLatin1AccentFilter filter = new ISOLatin1AccentFilter(stream);
-    assertEquals("Des", filter.next().termText());
-    assertEquals("mot", filter.next().termText());
-    assertEquals("cles", filter.next().termText());
-    assertEquals("A", filter.next().termText());
-    assertEquals("LA", filter.next().termText());
-    assertEquals("CHAINE", filter.next().termText());
-    assertEquals("A", filter.next().termText());
-    assertEquals("A", filter.next().termText());
-    assertEquals("A", filter.next().termText());
-    assertEquals("A", filter.next().termText());
-    assertEquals("A", filter.next().termText());
-    assertEquals("A", filter.next().termText());
-    assertEquals("AE", filter.next().termText());
-    assertEquals("C", filter.next().termText());
-    assertEquals("E", filter.next().termText());
-    assertEquals("E", filter.next().termText());
-    assertEquals("E", filter.next().termText());
-    assertEquals("E", filter.next().termText());
-    assertEquals("I", filter.next().termText());
-    assertEquals("I", filter.next().termText());
-    assertEquals("I", filter.next().termText());
-    assertEquals("I", filter.next().termText());
-    assertEquals("IJ", filter.next().termText());
-    assertEquals("D", filter.next().termText());
-    assertEquals("N", filter.next().termText());
-    assertEquals("O", filter.next().termText());
-    assertEquals("O", filter.next().termText());
-    assertEquals("O", filter.next().termText());
-    assertEquals("O", filter.next().termText());
-    assertEquals("O", filter.next().termText());
-    assertEquals("O", filter.next().termText());
-    assertEquals("OE", filter.next().termText());
-    assertEquals("TH", filter.next().termText());
-    assertEquals("U", filter.next().termText());
-    assertEquals("U", filter.next().termText());
-    assertEquals("U", filter.next().termText());
-    assertEquals("U", filter.next().termText());
-    assertEquals("Y", filter.next().termText());
-    assertEquals("Y", filter.next().termText());
-    assertEquals("a", filter.next().termText());
-    assertEquals("a", filter.next().termText());
-    assertEquals("a", filter.next().termText());
-    assertEquals("a", filter.next().termText());
-    assertEquals("a", filter.next().termText());
-    assertEquals("a", filter.next().termText());
-    assertEquals("ae", filter.next().termText());
-    assertEquals("c", filter.next().termText());
-    assertEquals("e", filter.next().termText());
-    assertEquals("e", filter.next().termText());
-    assertEquals("e", filter.next().termText());
-    assertEquals("e", filter.next().termText());
-    assertEquals("i", filter.next().termText());
-    assertEquals("i", filter.next().termText());
-    assertEquals("i", filter.next().termText());
-    assertEquals("i", filter.next().termText());
-    assertEquals("ij", filter.next().termText());
-    assertEquals("d", filter.next().termText());
-    assertEquals("n", filter.next().termText());
-    assertEquals("o", filter.next().termText());
-    assertEquals("o", filter.next().termText());
-    assertEquals("o", filter.next().termText());
-    assertEquals("o", filter.next().termText());
-    assertEquals("o", filter.next().termText());
-    assertEquals("o", filter.next().termText());
-    assertEquals("oe", filter.next().termText());
-    assertEquals("ss", filter.next().termText());
-    assertEquals("th", filter.next().termText());
-    assertEquals("u", filter.next().termText());
-    assertEquals("u", filter.next().termText());
-    assertEquals("u", filter.next().termText());
-    assertEquals("u", filter.next().termText());
-    assertEquals("y", filter.next().termText());
-    assertEquals("y", filter.next().termText());
-    assertEquals("fi", filter.next().termText());
-    assertEquals("fl", filter.next().termText());
-    assertNull(filter.next());
+    final Token reusableToken = new Token();
+    assertEquals("Des", filter.next(reusableToken).term());
+    assertEquals("mot", filter.next(reusableToken).term());
+    assertEquals("cles", filter.next(reusableToken).term());
+    assertEquals("A", filter.next(reusableToken).term());
+    assertEquals("LA", filter.next(reusableToken).term());
+    assertEquals("CHAINE", filter.next(reusableToken).term());
+    assertEquals("A", filter.next(reusableToken).term());
+    assertEquals("A", filter.next(reusableToken).term());
+    assertEquals("A", filter.next(reusableToken).term());
+    assertEquals("A", filter.next(reusableToken).term());
+    assertEquals("A", filter.next(reusableToken).term());
+    assertEquals("A", filter.next(reusableToken).term());
+    assertEquals("AE", filter.next(reusableToken).term());
+    assertEquals("C", filter.next(reusableToken).term());
+    assertEquals("E", filter.next(reusableToken).term());
+    assertEquals("E", filter.next(reusableToken).term());
+    assertEquals("E", filter.next(reusableToken).term());
+    assertEquals("E", filter.next(reusableToken).term());
+    assertEquals("I", filter.next(reusableToken).term());
+    assertEquals("I", filter.next(reusableToken).term());
+    assertEquals("I", filter.next(reusableToken).term());
+    assertEquals("I", filter.next(reusableToken).term());
+    assertEquals("IJ", filter.next(reusableToken).term());
+    assertEquals("D", filter.next(reusableToken).term());
+    assertEquals("N", filter.next(reusableToken).term());
+    assertEquals("O", filter.next(reusableToken).term());
+    assertEquals("O", filter.next(reusableToken).term());
+    assertEquals("O", filter.next(reusableToken).term());
+    assertEquals("O", filter.next(reusableToken).term());
+    assertEquals("O", filter.next(reusableToken).term());
+    assertEquals("O", filter.next(reusableToken).term());
+    assertEquals("OE", filter.next(reusableToken).term());
+    assertEquals("TH", filter.next(reusableToken).term());
+    assertEquals("U", filter.next(reusableToken).term());
+    assertEquals("U", filter.next(reusableToken).term());
+    assertEquals("U", filter.next(reusableToken).term());
+    assertEquals("U", filter.next(reusableToken).term());
+    assertEquals("Y", filter.next(reusableToken).term());
+    assertEquals("Y", filter.next(reusableToken).term());
+    assertEquals("a", filter.next(reusableToken).term());
+    assertEquals("a", filter.next(reusableToken).term());
+    assertEquals("a", filter.next(reusableToken).term());
+    assertEquals("a", filter.next(reusableToken).term());
+    assertEquals("a", filter.next(reusableToken).term());
+    assertEquals("a", filter.next(reusableToken).term());
+    assertEquals("ae", filter.next(reusableToken).term());
+    assertEquals("c", filter.next(reusableToken).term());
+    assertEquals("e", filter.next(reusableToken).term());
+    assertEquals("e", filter.next(reusableToken).term());
+    assertEquals("e", filter.next(reusableToken).term());
+    assertEquals("e", filter.next(reusableToken).term());
+    assertEquals("i", filter.next(reusableToken).term());
+    assertEquals("i", filter.next(reusableToken).term());
+    assertEquals("i", filter.next(reusableToken).term());
+    assertEquals("i", filter.next(reusableToken).term());
+    assertEquals("ij", filter.next(reusableToken).term());
+    assertEquals("d", filter.next(reusableToken).term());
+    assertEquals("n", filter.next(reusableToken).term());
+    assertEquals("o", filter.next(reusableToken).term());
+    assertEquals("o", filter.next(reusableToken).term());
+    assertEquals("o", filter.next(reusableToken).term());
+    assertEquals("o", filter.next(reusableToken).term());
+    assertEquals("o", filter.next(reusableToken).term());
+    assertEquals("o", filter.next(reusableToken).term());
+    assertEquals("oe", filter.next(reusableToken).term());
+    assertEquals("ss", filter.next(reusableToken).term());
+    assertEquals("th", filter.next(reusableToken).term());
+    assertEquals("u", filter.next(reusableToken).term());
+    assertEquals("u", filter.next(reusableToken).term());
+    assertEquals("u", filter.next(reusableToken).term());
+    assertEquals("u", filter.next(reusableToken).term());
+    assertEquals("y", filter.next(reusableToken).term());
+    assertEquals("y", filter.next(reusableToken).term());
+    assertEquals("fi", filter.next(reusableToken).term());
+    assertEquals("fl", filter.next(reusableToken).term());
+    assertNull(filter.next(reusableToken));
   }
 }
Index: src/test/org/apache/lucene/analysis/TestLengthFilter.java
===================================================================
--- src/test/org/apache/lucene/analysis/TestLengthFilter.java	(revision 686801)
+++ src/test/org/apache/lucene/analysis/TestLengthFilter.java	(working copy)
@@ -27,10 +27,11 @@
     TokenStream stream = new WhitespaceTokenizer(
         new StringReader("short toolong evenmuchlongertext a ab toolong foo"));
     LengthFilter filter = new LengthFilter(stream, 2, 6);
-    assertEquals("short", filter.next().termText());
-    assertEquals("ab", filter.next().termText());
-    assertEquals("foo", filter.next().termText());
-    assertNull(filter.next());
+    final Token reusableToken = new Token();
+    assertEquals("short", filter.next(reusableToken).term());
+    assertEquals("ab", filter.next(reusableToken).term());
+    assertEquals("foo", filter.next(reusableToken).term());
+    assertNull(filter.next(reusableToken));
   }
 
 }
Index: src/test/org/apache/lucene/analysis/TestAnalyzers.java
===================================================================
--- src/test/org/apache/lucene/analysis/TestAnalyzers.java	(revision 686801)
+++ src/test/org/apache/lucene/analysis/TestAnalyzers.java	(working copy)
@@ -17,13 +17,14 @@
  * limitations under the License.
  */
 
-import java.io.*;
+import java.io.IOException;
+import java.io.StringReader;
+import java.util.LinkedList;
 import java.util.List;
-import java.util.LinkedList;
 
+import org.apache.lucene.analysis.standard.StandardTokenizer;
+import org.apache.lucene.index.Payload;
 import org.apache.lucene.util.LuceneTestCase;
-import org.apache.lucene.index.Payload;
-import org.apache.lucene.analysis.standard.StandardTokenizer;
 
 public class TestAnalyzers extends LuceneTestCase {
 
@@ -35,12 +36,13 @@
                                String input, 
                                String[] output) throws Exception {
     TokenStream ts = a.tokenStream("dummy", new StringReader(input));
+    final Token reusableToken  = new Token();
     for (int i=0; i<output.length; i++) {
-      Token t = ts.next();
-      assertNotNull(t);
-      assertEquals(t.termText(), output[i]);
+      Token nextToken = ts.next(reusableToken);
+      assertNotNull(nextToken);
+      assertEquals(nextToken.term(), output[i]);
     }
-    assertNull(ts.next());
+    assertNull(ts.next(reusableToken));
     ts.close();
   }
 
@@ -93,14 +95,14 @@
   }
 
   void verifyPayload(TokenStream ts) throws IOException {
-    Token t = new Token();
+    final Token reusableToken = new Token();
     for(byte b=1;;b++) {
-      t.clear();
-      t = ts.next(t);
-      if (t==null) break;
-      // System.out.println("id="+System.identityHashCode(t) + " " + t);
-      // System.out.println("payload=" + (int)t.getPayload().toByteArray()[0]);
-      assertEquals(b, t.getPayload().toByteArray()[0]);
+      reusableToken.clear();
+      Token nextToken = ts.next(reusableToken);
+      if (nextToken==null) break;
+      // System.out.println("id="+System.identityHashCode(nextToken) + " " + t);
+      // System.out.println("payload=" + (int)nextToken.getPayload().toByteArray()[0]);
+      assertEquals(b, nextToken.getPayload().toByteArray()[0]);
     }
   }
 
@@ -141,13 +143,11 @@
     super(input);
   }
 
-  public Token next() throws IOException {
+  public Token next(final Token reusableToken) throws IOException {
     if (lst == null) {
       lst = new LinkedList();
-      for(;;) {
-        Token t = input.next();
-        if (t==null) break;
-        lst.add(t);
+      for(Token nextToken = input.next(reusableToken); nextToken != null; nextToken = input.next(reusableToken)) {
+        lst.add(nextToken.clone());
       }
     }
     return lst.size()==0 ? null : (Token)lst.remove(0);
@@ -162,11 +162,12 @@
   byte[] data = new byte[1];
   Payload p = new Payload(data,0,1);
 
-  public Token next(Token target) throws IOException {
-    target = input.next(target);
-    if (target==null) return null;
-    target.setPayload(p);  // reuse the payload / byte[]
+  public Token next(final Token reusableToken) throws IOException {
+    assert reusableToken != null;
+    Token nextToken = input.next(reusableToken);
+    if (nextToken==null) return null;
+    nextToken.setPayload(p);  // reuse the payload / byte[]
     data[0]++;
-    return target;
+    return nextToken;
   }
 }
\ No newline at end of file
Index: src/test/org/apache/lucene/analysis/TestStopAnalyzer.java
===================================================================
--- src/test/org/apache/lucene/analysis/TestStopAnalyzer.java	(revision 686801)
+++ src/test/org/apache/lucene/analysis/TestStopAnalyzer.java	(working copy)
@@ -45,9 +45,9 @@
     StringReader reader = new StringReader("This is a test of the english stop analyzer");
     TokenStream stream = stop.tokenStream("test", reader);
     assertTrue(stream != null);
-    Token token = null;
-    while ((token = stream.next()) != null) {
-      assertFalse(inValidTokens.contains(token.termText()));
+    final Token reusableToken = new Token();
+    for (Token nextToken = stream.next(reusableToken); nextToken != null; nextToken = stream.next(reusableToken)) {
+      assertFalse(inValidTokens.contains(nextToken.term()));
     }
   }
 
@@ -60,11 +60,11 @@
     StringReader reader = new StringReader("This is a good test of the english stop analyzer");
     TokenStream stream = newStop.tokenStream("test", reader);
     assertNotNull(stream);
-    Token token = null;
-    while ((token = stream.next()) != null) {
-      String text = token.termText();
+    final Token reusableToken = new Token();
+    for (Token nextToken = stream.next(reusableToken); nextToken != null; nextToken = stream.next(reusableToken)) {
+      String text = nextToken.term();
       assertFalse(stopWordsSet.contains(text));
-      assertEquals(1,token.getPositionIncrement()); // by default stop tokenizer does not apply increments.
+      assertEquals(1,nextToken.getPositionIncrement()); // by default stop tokenizer does not apply increments.
     }
   }
 
@@ -81,12 +81,12 @@
       int expectedIncr[] =                  { 1,   1, 1,          3, 1,  1,      1,            2,   1};
       TokenStream stream = newStop.tokenStream("test", reader);
       assertNotNull(stream);
-      Token token = null;
       int i = 0;
-      while ((token = stream.next()) != null) {
-        String text = token.termText();
+      final Token reusableToken = new Token();
+      for (Token nextToken = stream.next(reusableToken); nextToken != null; nextToken = stream.next(reusableToken)) {
+        String text = nextToken.term();
         assertFalse(stopWordsSet.contains(text));
-        assertEquals(expectedIncr[i++],token.getPositionIncrement());
+        assertEquals(expectedIncr[i++],nextToken.getPositionIncrement());
       }
     } finally {
       StopFilter.setEnablePositionIncrementsDefault(defaultEnable);
Index: src/test/org/apache/lucene/analysis/TestStopFilter.java
===================================================================
--- src/test/org/apache/lucene/analysis/TestStopFilter.java	(revision 686801)
+++ src/test/org/apache/lucene/analysis/TestStopFilter.java	(working copy)
@@ -37,17 +37,19 @@
     StringReader reader = new StringReader("Now is The Time");
     String[] stopWords = new String[] { "is", "the", "Time" };
     TokenStream stream = new StopFilter(new WhitespaceTokenizer(reader), stopWords);
-    assertEquals("Now", stream.next().termText());
-    assertEquals("The", stream.next().termText());
-    assertEquals(null, stream.next());
+    final Token reusableToken = new Token();
+    assertEquals("Now", stream.next(reusableToken).term());
+    assertEquals("The", stream.next(reusableToken).term());
+    assertEquals(null, stream.next(reusableToken));
   }
 
   public void testIgnoreCase() throws IOException {
     StringReader reader = new StringReader("Now is The Time");
     String[] stopWords = new String[] { "is", "the", "Time" };
     TokenStream stream = new StopFilter(new WhitespaceTokenizer(reader), stopWords, true);
-    assertEquals("Now", stream.next().termText());
-    assertEquals(null,stream.next());
+    final Token reusableToken = new Token();
+    assertEquals("Now", stream.next(reusableToken).term());
+    assertEquals(null,stream.next(reusableToken));
   }
 
   public void testStopFilt() throws IOException {
@@ -55,9 +57,10 @@
     String[] stopWords = new String[] { "is", "the", "Time" };
     Set stopSet = StopFilter.makeStopSet(stopWords);
     TokenStream stream = new StopFilter(new WhitespaceTokenizer(reader), stopSet);
-    assertEquals("Now", stream.next().termText());
-    assertEquals("The", stream.next().termText());
-    assertEquals(null, stream.next());
+    final Token reusableToken = new Token();
+    assertEquals("Now", stream.next(reusableToken).term());
+    assertEquals("The", stream.next(reusableToken).term());
+    assertEquals(null, stream.next(reusableToken));
   }
 
   /**
@@ -109,14 +112,15 @@
   private void doTestStopPositons(StopFilter stpf, boolean enableIcrements) throws IOException {
     log("---> test with enable-increments-"+(enableIcrements?"enabled":"disabled"));
     stpf.setEnablePositionIncrements(enableIcrements);
+    final Token reusableToken = new Token();
     for (int i=0; i<20; i+=3) {
-      Token t = stpf.next();
-      log("Token "+i+": "+t);
+      Token nextToken = stpf.next(reusableToken);
+      log("Token "+i+": "+nextToken);
       String w = English.intToEnglish(i).trim();
-      assertEquals("expecting token "+i+" to be "+w,w,t.termText());
-      assertEquals("all but first token must have position increment of 3",enableIcrements?(i==0?1:3):1,t.getPositionIncrement());
+      assertEquals("expecting token "+i+" to be "+w,w,nextToken.term());
+      assertEquals("all but first token must have position increment of 3",enableIcrements?(i==0?1:3):1,nextToken.getPositionIncrement());
     }
-    assertNull(stpf.next());
+    assertNull(stpf.next(reusableToken));
   }
   
   // print debug info depending on VERBOSE
Index: src/test/org/apache/lucene/AnalysisTest.java
===================================================================
--- src/test/org/apache/lucene/AnalysisTest.java	(revision 686801)
+++ src/test/org/apache/lucene/AnalysisTest.java	(working copy)
@@ -70,11 +70,12 @@
     Date start = new Date();
 
     int count = 0;
-    for (Token t = stream.next(); t!=null; t = stream.next()) {
+    final Token reusableToken = new Token();
+    for (Token nextToken = stream.next(reusableToken); nextToken != null; nextToken = stream.next(reusableToken)) {
       if (verbose) {
-	System.out.println("Text=" + new String(t.termBuffer(), 0, t.termLength())
-			   + " start=" + t.startOffset()
-			   + " end=" + t.endOffset());
+	System.out.println("Text=" + nextToken.term()
+			   + " start=" + nextToken.startOffset()
+			   + " end=" + nextToken.endOffset());
       }
       count++;
     }
Index: src/test/org/apache/lucene/search/TestPositionIncrement.java
===================================================================
--- src/test/org/apache/lucene/search/TestPositionIncrement.java	(revision 686801)
+++ src/test/org/apache/lucene/search/TestPositionIncrement.java	(working copy)
@@ -49,13 +49,14 @@
           private final int[] INCREMENTS = {1, 2, 1, 0, 1};
           private int i = 0;
 
-          public Token next() {
+          public Token next(final Token reusableToken) {
+            assert reusableToken != null;
             if (i == TOKENS.length)
               return null;
-            Token t = new Token(TOKENS[i], i, i);
-            t.setPositionIncrement(INCREMENTS[i]);
+            reusableToken.reinit(TOKENS[i], i, i);
+            reusableToken.setPositionIncrement(INCREMENTS[i]);
             i++;
-            return t;
+            return reusableToken;
           }
         };
       }
@@ -204,11 +205,9 @@
     Analyzer analyzer = new WhitespaceAnalyzer();
     TokenStream ts = analyzer.tokenStream("field",
                                 new StringReader("one two three four five"));
-
-    while (true) {
-      Token token = ts.next();
-      if (token == null) break;
-      assertEquals(token.termText(), 1, token.getPositionIncrement());
+    final Token reusableToken = new Token();
+    for (Token nextToken = ts.next(reusableToken); nextToken != null; nextToken = ts.next(reusableToken)) {
+      assertEquals(nextToken.term(), 1, nextToken.getPositionIncrement());
     }
   }
 }
Index: src/test/org/apache/lucene/search/payloads/TestBoostingTermQuery.java
===================================================================
--- src/test/org/apache/lucene/search/payloads/TestBoostingTermQuery.java	(revision 686801)
+++ src/test/org/apache/lucene/search/payloads/TestBoostingTermQuery.java	(working copy)
@@ -16,22 +16,32 @@
  * limitations under the License.
  */
 
-import org.apache.lucene.util.LuceneTestCase;
-import org.apache.lucene.analysis.*;
+import java.io.IOException;
+import java.io.Reader;
+
+import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.analysis.LowerCaseTokenizer;
+import org.apache.lucene.analysis.Token;
+import org.apache.lucene.analysis.TokenFilter;
+import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.index.IndexWriter;
 import org.apache.lucene.index.Payload;
 import org.apache.lucene.index.Term;
-import org.apache.lucene.search.*;
+import org.apache.lucene.search.BooleanClause;
+import org.apache.lucene.search.BooleanQuery;
+import org.apache.lucene.search.CheckHits;
+import org.apache.lucene.search.DefaultSimilarity;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.ScoreDoc;
+import org.apache.lucene.search.TopDocs;
 import org.apache.lucene.search.spans.Spans;
 import org.apache.lucene.search.spans.TermSpans;
 import org.apache.lucene.store.RAMDirectory;
 import org.apache.lucene.util.English;
+import org.apache.lucene.util.LuceneTestCase;
 
-import java.io.IOException;
-import java.io.Reader;
-
 public class TestBoostingTermQuery extends LuceneTestCase {
   private IndexSearcher searcher;
   private BoostingSimilarity similarity = new BoostingSimilarity();
@@ -62,22 +72,23 @@
       this.fieldName = fieldName;
     }
 
-    public Token next() throws IOException {
-      Token result = input.next();
-      if (result != null) {
+    public Token next(final Token reusableToken) throws IOException {
+      assert reusableToken != null;
+      Token nextToken = input.next(reusableToken);
+      if (nextToken != null) {
         if (fieldName.equals("field")) {
-          result.setPayload(new Payload(payloadField));
+          nextToken.setPayload(new Payload(payloadField));
         } else if (fieldName.equals("multiField")) {
           if (numSeen % 2 == 0) {
-            result.setPayload(new Payload(payloadMultiField1));
+            nextToken.setPayload(new Payload(payloadMultiField1));
           } else {
-            result.setPayload(new Payload(payloadMultiField2));
+            nextToken.setPayload(new Payload(payloadMultiField2));
           }
           numSeen++;
         }
 
       }
-      return result;
+      return nextToken;
     }
   }
 
Index: src/test/org/apache/lucene/index/TestTermVectorsReader.java
===================================================================
--- src/test/org/apache/lucene/index/TestTermVectorsReader.java	(revision 686801)
+++ src/test/org/apache/lucene/index/TestTermVectorsReader.java	(working copy)
@@ -118,20 +118,17 @@
 
   private class MyTokenStream extends TokenStream {
     int tokenUpto;
-    public Token next() {
+    public Token next(final Token reusableToken) {
       if (tokenUpto >= tokens.length)
         return null;
       else {
-        final Token t = new Token();
         final TestToken testToken = tokens[tokenUpto++];
-        t.setTermText(testToken.text);
+        reusableToken.reinit(testToken.text, testToken.startOffset, testToken.endOffset);
         if (tokenUpto > 1)
-          t.setPositionIncrement(testToken.pos - tokens[tokenUpto-2].pos);
+          reusableToken.setPositionIncrement(testToken.pos - tokens[tokenUpto-2].pos);
         else
-          t.setPositionIncrement(testToken.pos+1);
-        t.setStartOffset(testToken.startOffset);
-        t.setEndOffset(testToken.endOffset);
-        return t;
+          reusableToken.setPositionIncrement(testToken.pos+1);
+        return reusableToken;
       }
     }
   }
Index: src/test/org/apache/lucene/index/TestIndexWriter.java
===================================================================
--- src/test/org/apache/lucene/index/TestIndexWriter.java	(revision 686801)
+++ src/test/org/apache/lucene/index/TestIndexWriter.java	(working copy)
@@ -1786,11 +1786,11 @@
         return new TokenFilter(new StandardTokenizer(reader)) {
           private int count = 0;
 
-          public Token next() throws IOException {
+          public Token next(final Token reusableToken) throws IOException {
             if (count++ == 5) {
               throw new IOException();
             }
-            return input.next();
+            return input.next(reusableToken);
           }
         };
       }
@@ -1909,10 +1909,10 @@
       this.fieldName = fieldName;
     }
 
-    public Token next(Token result) throws IOException {
+    public Token next(final Token reusableToken) throws IOException {
       if (this.fieldName.equals("crash") && count++ >= 4)
         throw new IOException("I'm experiencing problems");
-      return input.next(result);
+      return input.next(reusableToken);
     }
 
     public void reset() throws IOException {
@@ -3574,13 +3574,13 @@
   public void testNegativePositions() throws Throwable {
     SinkTokenizer tokens = new SinkTokenizer();
     Token t = new Token();
-    t.setTermText("a");
+    t.setTermBuffer("a");
     t.setPositionIncrement(0);
     tokens.add(t);
-    t.setTermText("b");
+    t.setTermBuffer("b");
     t.setPositionIncrement(1);
     tokens.add(t);
-    t.setTermText("c");
+    t.setTermBuffer("c");
     tokens.add(t);
 
     MockRAMDirectory dir = new MockRAMDirectory();
Index: src/test/org/apache/lucene/index/TestMultiLevelSkipList.java
===================================================================
--- src/test/org/apache/lucene/index/TestMultiLevelSkipList.java	(revision 686801)
+++ src/test/org/apache/lucene/index/TestMultiLevelSkipList.java	(working copy)
@@ -103,12 +103,13 @@
       super(input);
     }
 
-    public Token next() throws IOException {
-      Token t = input.next();
-      if (t != null) {
-        t.setPayload(new Payload(new byte[] { (byte) count++ }));
+    public Token next(final Token reusableToken) throws IOException {
+      assert reusableToken != null;
+      Token nextToken = input.next(reusableToken);
+      if (nextToken != null) {
+        nextToken.setPayload(new Payload(new byte[] { (byte) count++ }));
       }
-      return t;
+      return nextToken;
     }
 
   }
Index: src/test/org/apache/lucene/index/TestTermdocPerf.java
===================================================================
--- src/test/org/apache/lucene/index/TestTermdocPerf.java	(revision 686801)
+++ src/test/org/apache/lucene/index/TestTermdocPerf.java	(working copy)
@@ -40,11 +40,12 @@
   Token t;
 
    public RepeatingTokenStream(String val) {
-     t = new Token(val,0,val.length());
+     t = new Token(0,val.length());
+     t.setTermBuffer(val);
    }
 
-   public Token next() throws IOException {
-     return --num<0 ? null : t;
+   public Token next(final Token reusableToken) throws IOException {
+     return --num<0 ? null : (Token) t.clone();
    }
 }
 
Index: src/test/org/apache/lucene/index/TestDocumentWriter.java
===================================================================
--- src/test/org/apache/lucene/index/TestDocumentWriter.java	(revision 686801)
+++ src/test/org/apache/lucene/index/TestDocumentWriter.java	(working copy)
@@ -17,21 +17,27 @@
  * limitations under the License.
  */
 
-import org.apache.lucene.analysis.*;
+import java.io.IOException;
+import java.io.Reader;
+
+import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.analysis.SimpleAnalyzer;
+import org.apache.lucene.analysis.Token;
+import org.apache.lucene.analysis.TokenFilter;
+import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.analysis.WhitespaceAnalyzer;
+import org.apache.lucene.analysis.WhitespaceTokenizer;
 import org.apache.lucene.analysis.standard.StandardAnalyzer;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
+import org.apache.lucene.document.Fieldable;
 import org.apache.lucene.document.Field.Index;
 import org.apache.lucene.document.Field.Store;
 import org.apache.lucene.document.Field.TermVector;
-import org.apache.lucene.document.Fieldable;
 import org.apache.lucene.store.RAMDirectory;
 import org.apache.lucene.util.LuceneTestCase;
 import org.apache.lucene.util._TestUtil;
 
-import java.io.IOException;
-import java.io.Reader;
-
 public class TestDocumentWriter extends LuceneTestCase {
   private RAMDirectory dir;
 
@@ -134,34 +140,30 @@
           boolean first=true;
           Token buffered;
 
-          public Token next() throws IOException {
-            return input.next();
-          }
-
-          public Token next(Token result) throws IOException {
+          public Token next(final Token reusableToken) throws IOException {
             if (buffered != null) {
-              Token t = buffered;
+              Token nextToken = buffered;
               buffered=null;
-              return t;
+              return nextToken;
             }
-            Token t = input.next(result);
-            if (t==null) return null;
-            if (Character.isDigit(t.termBuffer()[0])) {
-              t.setPositionIncrement(t.termBuffer()[0] - '0');
+            Token nextToken = input.next(reusableToken);
+            if (nextToken==null) return null;
+            if (Character.isDigit(nextToken.termBuffer()[0])) {
+              nextToken.setPositionIncrement(nextToken.termBuffer()[0] - '0');
             }
             if (first) {
               // set payload on first position only
-              t.setPayload(new Payload(new byte[]{100}));
+              nextToken.setPayload(new Payload(new byte[]{100}));
               first = false;
             }
 
             // index a "synonym" for every token
-            buffered = (Token)t.clone();
+            buffered = (Token)nextToken.clone();
             buffered.setPayload(null);
             buffered.setPositionIncrement(0);
             buffered.setTermBuffer(new char[]{'b'}, 0, 1);
 
-            return t;
+            return nextToken;
           }
         };
       }
@@ -199,11 +201,12 @@
       private String[] tokens = new String[] {"term1", "term2", "term3", "term2"};
       private int index = 0;
       
-      public Token next() throws IOException {
+      public Token next(final Token reusableToken) throws IOException {
+        assert reusableToken != null;
         if (index == tokens.length) {
           return null;
         } else {
-          return new Token(tokens[index++], 0, 0);
+          return reusableToken.reinit(tokens[index++], 0, 0);
         }        
       }
       
Index: src/test/org/apache/lucene/index/TestPayloads.java
===================================================================
--- src/test/org/apache/lucene/index/TestPayloads.java	(revision 686801)
+++ src/test/org/apache/lucene/index/TestPayloads.java	(working copy)
@@ -450,23 +450,24 @@
             this.offset = offset;
         }
         
-        public Token next(Token token) throws IOException {
-            token = input.next(token);
-            if (token != null) {
+        public Token next(final Token reusableToken) throws IOException {
+            assert reusableToken != null;
+            Token nextToken = input.next(reusableToken);
+            if (nextToken != null) {
                 if (offset + length <= data.length) {
                     Payload p = null;
                     if (p == null) {
                         p = new Payload();
-                        token.setPayload(p);
+                        nextToken.setPayload(p);
                     }
                     p.setData(data, offset, length);
                     offset += length;                
                 } else {
-                    token.setPayload(null);
+                    nextToken.setPayload(null);
                 }
             }
             
-            return token;
+            return nextToken;
         }
     }
     
@@ -536,11 +537,11 @@
             first = true;
         }
         
-        public Token next() throws IOException {
-            if (!first) return null;            
-            Token t = new Token(term, 0, 0);
-            t.setPayload(new Payload(payload));
-            return t;        
+        public Token next(final Token reusableToken) throws IOException {
+            if (!first) return null;
+            reusableToken.reinit(term, 0, 0);
+            reusableToken.setPayload(new Payload(payload));
+            return reusableToken;
         }
         
         public void close() throws IOException {
Index: src/java/org/apache/lucene/queryParser/TokenMgrError.java
===================================================================
--- src/java/org/apache/lucene/queryParser/TokenMgrError.java	(revision 686801)
+++ src/java/org/apache/lucene/queryParser/TokenMgrError.java	(working copy)
@@ -72,7 +72,7 @@
            default:
               if ((ch = str.charAt(i)) < 0x20 || ch > 0x7e) {
                  String s = "0000" + Integer.toString(ch, 16);
-                 retval.append("\\u").append(s.substring(s.length() - 4, s.length()));
+                 retval.append("\\u" + s.substring(s.length() - 4, s.length()));
               } else {
                  retval.append(ch);
               }
Index: src/java/org/apache/lucene/queryParser/QueryParser.java
===================================================================
--- src/java/org/apache/lucene/queryParser/QueryParser.java	(revision 686801)
+++ src/java/org/apache/lucene/queryParser/QueryParser.java	(working copy)
@@ -1,14 +1,35 @@
 /* Generated By:JavaCC: Do not edit this line. QueryParser.java */
 package org.apache.lucene.queryParser;
 
+import java.io.IOException;
+import java.io.StringReader;
+import java.text.DateFormat;
+import java.util.ArrayList;
+import java.util.Calendar;
+import java.util.Date;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Locale;
+import java.util.Map;
 import java.util.Vector;
-import java.io.*;
-import java.text.*;
-import java.util.*;
+
+import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.document.DateField;
+import org.apache.lucene.document.DateTools;
 import org.apache.lucene.index.Term;
-import org.apache.lucene.analysis.*;
-import org.apache.lucene.document.*;
-import org.apache.lucene.search.*;
+import org.apache.lucene.search.BooleanClause;
+import org.apache.lucene.search.BooleanQuery;
+import org.apache.lucene.search.ConstantScoreRangeQuery;
+import org.apache.lucene.search.FuzzyQuery;
+import org.apache.lucene.search.MatchAllDocsQuery;
+import org.apache.lucene.search.MultiPhraseQuery;
+import org.apache.lucene.search.PhraseQuery;
+import org.apache.lucene.search.PrefixQuery;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.RangeQuery;
+import org.apache.lucene.search.TermQuery;
+import org.apache.lucene.search.WildcardQuery;
 import org.apache.lucene.util.Parameter;
 
 /**
@@ -451,22 +472,23 @@
 
     TokenStream source = analyzer.tokenStream(field, new StringReader(queryText));
     Vector v = new Vector();
-    org.apache.lucene.analysis.Token t;
+    final org.apache.lucene.analysis.Token reusableToken = new org.apache.lucene.analysis.Token();
+    org.apache.lucene.analysis.Token nextToken;
     int positionCount = 0;
     boolean severalTokensAtSamePosition = false;
 
     while (true) {
       try {
-        t = source.next();
+        nextToken = source.next(reusableToken);
       }
       catch (IOException e) {
-        t = null;
+        nextToken = null;
       }
-      if (t == null)
+      if (nextToken == null)
         break;
-      v.addElement(t);
-      if (t.getPositionIncrement() != 0)
-        positionCount += t.getPositionIncrement();
+      v.addElement(nextToken.clone());
+      if (nextToken.getPositionIncrement() != 0)
+        positionCount += nextToken.getPositionIncrement();
       else
         severalTokensAtSamePosition = true;
     }
@@ -480,17 +502,17 @@
     if (v.size() == 0)
       return null;
     else if (v.size() == 1) {
-      t = (org.apache.lucene.analysis.Token) v.elementAt(0);
-      return new TermQuery(new Term(field, t.termText()));
+      nextToken = (org.apache.lucene.analysis.Token) v.elementAt(0);
+      return new TermQuery(new Term(field, nextToken.term()));
     } else {
       if (severalTokensAtSamePosition) {
         if (positionCount == 1) {
           // no phrase query:
           BooleanQuery q = new BooleanQuery(true);
           for (int i = 0; i < v.size(); i++) {
-            t = (org.apache.lucene.analysis.Token) v.elementAt(i);
+            nextToken = (org.apache.lucene.analysis.Token) v.elementAt(i);
             TermQuery currentQuery = new TermQuery(
-                new Term(field, t.termText()));
+                new Term(field, nextToken.term()));
             q.add(currentQuery, BooleanClause.Occur.SHOULD);
           }
           return q;
@@ -502,8 +524,8 @@
           List multiTerms = new ArrayList();
           int position = -1;
           for (int i = 0; i < v.size(); i++) {
-            t = (org.apache.lucene.analysis.Token) v.elementAt(i);
-            if (t.getPositionIncrement() > 0 && multiTerms.size() > 0) {
+            nextToken = (org.apache.lucene.analysis.Token) v.elementAt(i);
+            if (nextToken.getPositionIncrement() > 0 && multiTerms.size() > 0) {
               if (enablePositionIncrements) {
                 mpq.add((Term[])multiTerms.toArray(new Term[0]),position);
               } else {
@@ -511,8 +533,8 @@
               }
               multiTerms.clear();
             }
-            position += t.getPositionIncrement();
-            multiTerms.add(new Term(field, t.termText()));
+            position += nextToken.getPositionIncrement();
+            multiTerms.add(new Term(field, nextToken.term()));
           }
           if (enablePositionIncrements) {
             mpq.add((Term[])multiTerms.toArray(new Term[0]),position);
@@ -527,12 +549,12 @@
         pq.setSlop(phraseSlop);
         int position = -1;
         for (int i = 0; i < v.size(); i++) {
-          t = (org.apache.lucene.analysis.Token) v.elementAt(i);
+          nextToken = (org.apache.lucene.analysis.Token) v.elementAt(i);
           if (enablePositionIncrements) {
-            position += t.getPositionIncrement();
-            pq.add(new Term(field, t.termText()),position);
+            position += nextToken.getPositionIncrement();
+            pq.add(new Term(field, nextToken.term()),position);
           } else {
-            pq.add(new Term(field, t.termText()));
+            pq.add(new Term(field, nextToken.term()));
           }
         }
         return pq;
@@ -1490,6 +1512,9 @@
   public ParseException generateParseException() {
     jj_expentries.removeAllElements();
     boolean[] la1tokens = new boolean[34];
+    for (int i = 0; i < 34; i++) {
+      la1tokens[i] = false;
+    }
     if (jj_kind >= 0) {
       la1tokens[jj_kind] = true;
       jj_kind = -1;
Index: src/java/org/apache/lucene/queryParser/QueryParser.jj
===================================================================
--- src/java/org/apache/lucene/queryParser/QueryParser.jj	(revision 686801)
+++ src/java/org/apache/lucene/queryParser/QueryParser.jj	(working copy)
@@ -25,14 +25,35 @@
 
 package org.apache.lucene.queryParser;
 
+import java.io.IOException;
+import java.io.StringReader;
+import java.text.DateFormat;
+import java.util.ArrayList;
+import java.util.Calendar;
+import java.util.Date;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Locale;
+import java.util.Map;
 import java.util.Vector;
-import java.io.*;
-import java.text.*;
-import java.util.*;
+
+import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.document.DateField;
+import org.apache.lucene.document.DateTools;
 import org.apache.lucene.index.Term;
-import org.apache.lucene.analysis.*;
-import org.apache.lucene.document.*;
-import org.apache.lucene.search.*;
+import org.apache.lucene.search.BooleanClause;
+import org.apache.lucene.search.BooleanQuery;
+import org.apache.lucene.search.ConstantScoreRangeQuery;
+import org.apache.lucene.search.FuzzyQuery;
+import org.apache.lucene.search.MatchAllDocsQuery;
+import org.apache.lucene.search.MultiPhraseQuery;
+import org.apache.lucene.search.PhraseQuery;
+import org.apache.lucene.search.PrefixQuery;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.RangeQuery;
+import org.apache.lucene.search.TermQuery;
+import org.apache.lucene.search.WildcardQuery;
 import org.apache.lucene.util.Parameter;
 
 /**
@@ -475,22 +496,23 @@
 
     TokenStream source = analyzer.tokenStream(field, new StringReader(queryText));
     Vector v = new Vector();
-    org.apache.lucene.analysis.Token t;
+    final org.apache.lucene.analysis.Token reusableToken = new org.apache.lucene.analysis.Token();
+    org.apache.lucene.analysis.Token nextToken;
     int positionCount = 0;
     boolean severalTokensAtSamePosition = false;
 
     while (true) {
       try {
-        t = source.next();
+        nextToken = source.next(reusableToken);
       }
       catch (IOException e) {
-        t = null;
+        nextToken = null;
       }
-      if (t == null)
+      if (nextToken == null)
         break;
-      v.addElement(t);
-      if (t.getPositionIncrement() != 0)
-        positionCount += t.getPositionIncrement();
+      v.addElement(nextToken.clone());
+      if (nextToken.getPositionIncrement() != 0)
+        positionCount += nextToken.getPositionIncrement();
       else
         severalTokensAtSamePosition = true;
     }
@@ -504,17 +526,17 @@
     if (v.size() == 0)
       return null;
     else if (v.size() == 1) {
-      t = (org.apache.lucene.analysis.Token) v.elementAt(0);
-      return new TermQuery(new Term(field, t.termText()));
+      nextToken = (org.apache.lucene.analysis.Token) v.elementAt(0);
+      return new TermQuery(new Term(field, nextToken.term()));
     } else {
       if (severalTokensAtSamePosition) {
         if (positionCount == 1) {
           // no phrase query:
           BooleanQuery q = new BooleanQuery(true);
           for (int i = 0; i < v.size(); i++) {
-            t = (org.apache.lucene.analysis.Token) v.elementAt(i);
+            nextToken = (org.apache.lucene.analysis.Token) v.elementAt(i);
             TermQuery currentQuery = new TermQuery(
-                new Term(field, t.termText()));
+                new Term(field, nextToken.term()));
             q.add(currentQuery, BooleanClause.Occur.SHOULD);
           }
           return q;
@@ -526,8 +548,8 @@
           List multiTerms = new ArrayList();
           int position = -1;
           for (int i = 0; i < v.size(); i++) {
-            t = (org.apache.lucene.analysis.Token) v.elementAt(i);
-            if (t.getPositionIncrement() > 0 && multiTerms.size() > 0) {
+            nextToken = (org.apache.lucene.analysis.Token) v.elementAt(i);
+            if (nextToken.getPositionIncrement() > 0 && multiTerms.size() > 0) {
               if (enablePositionIncrements) {
                 mpq.add((Term[])multiTerms.toArray(new Term[0]),position);
               } else {
@@ -535,8 +557,8 @@
               }
               multiTerms.clear();
             }
-            position += t.getPositionIncrement();
-            multiTerms.add(new Term(field, t.termText()));
+            position += nextToken.getPositionIncrement();
+            multiTerms.add(new Term(field, nextToken.term()));
           }
           if (enablePositionIncrements) {
             mpq.add((Term[])multiTerms.toArray(new Term[0]),position);
@@ -553,10 +575,10 @@
         for (int i = 0; i < v.size(); i++) {
           t = (org.apache.lucene.analysis.Token) v.elementAt(i);
           if (enablePositionIncrements) {
-            position += t.getPositionIncrement();
-            pq.add(new Term(field, t.termText()),position);
+            position += nextToken.getPositionIncrement();
+            pq.add(new Term(field, nextToken.term()),position);
           } else {
-            pq.add(new Term(field, t.termText()));
+            pq.add(new Term(field, nextToken.term()));
           }
         }
         return pq;
Index: src/java/org/apache/lucene/queryParser/CharStream.java
===================================================================
--- src/java/org/apache/lucene/queryParser/CharStream.java	(revision 686801)
+++ src/java/org/apache/lucene/queryParser/CharStream.java	(working copy)
@@ -1,4 +1,4 @@
-/* Generated By:JavaCC: Do not edit this line. CharStream.java Version 3.0 */
+/* Generated By:JavaCC: Do not edit this line. CharStream.java Version 4.0 */
 package org.apache.lucene.queryParser;
 
 /**
Index: src/java/org/apache/lucene/queryParser/ParseException.java
===================================================================
--- src/java/org/apache/lucene/queryParser/ParseException.java	(revision 686801)
+++ src/java/org/apache/lucene/queryParser/ParseException.java	(working copy)
@@ -98,19 +98,19 @@
     if (!specialConstructor) {
       return super.getMessage();
     }
-    String expected = "";
+    StringBuffer expected = new StringBuffer();
     int maxSize = 0;
     for (int i = 0; i < expectedTokenSequences.length; i++) {
       if (maxSize < expectedTokenSequences[i].length) {
         maxSize = expectedTokenSequences[i].length;
       }
       for (int j = 0; j < expectedTokenSequences[i].length; j++) {
-        expected += tokenImage[expectedTokenSequences[i][j]] + " ";
+        expected.append(tokenImage[expectedTokenSequences[i][j]]).append(" ");
       }
       if (expectedTokenSequences[i][expectedTokenSequences[i].length - 1] != 0) {
-        expected += "...";
+        expected.append("...");
       }
-      expected += eol + "    ";
+      expected.append(eol).append("    ");
     }
     String retval = "Encountered \"";
     Token tok = currentToken.next;
@@ -130,7 +130,7 @@
     } else {
       retval += "Was expecting one of:" + eol + "    ";
     }
-    retval += expected;
+    retval += expected.toString();
     return retval;
   }
 
@@ -179,7 +179,7 @@
            default:
               if ((ch = str.charAt(i)) < 0x20 || ch > 0x7e) {
                  String s = "0000" + Integer.toString(ch, 16);
-                 retval.append("\\u").append(s.substring(s.length() - 4, s.length()));
+                 retval.append("\\u" + s.substring(s.length() - 4, s.length()));
               } else {
                  retval.append(ch);
               }
Index: src/java/org/apache/lucene/queryParser/QueryParserTokenManager.java
===================================================================
--- src/java/org/apache/lucene/queryParser/QueryParserTokenManager.java	(revision 686801)
+++ src/java/org/apache/lucene/queryParser/QueryParserTokenManager.java	(working copy)
@@ -1,13 +1,33 @@
 /* Generated By:JavaCC: Do not edit this line. QueryParserTokenManager.java */
 package org.apache.lucene.queryParser;
+import java.io.IOException;
+import java.io.StringReader;
+import java.text.DateFormat;
+import java.util.ArrayList;
+import java.util.Calendar;
+import java.util.Date;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Locale;
+import java.util.Map;
 import java.util.Vector;
-import java.io.*;
-import java.text.*;
-import java.util.*;
+import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.document.DateField;
+import org.apache.lucene.document.DateTools;
 import org.apache.lucene.index.Term;
-import org.apache.lucene.analysis.*;
-import org.apache.lucene.document.*;
-import org.apache.lucene.search.*;
+import org.apache.lucene.search.BooleanClause;
+import org.apache.lucene.search.BooleanQuery;
+import org.apache.lucene.search.ConstantScoreRangeQuery;
+import org.apache.lucene.search.FuzzyQuery;
+import org.apache.lucene.search.MatchAllDocsQuery;
+import org.apache.lucene.search.MultiPhraseQuery;
+import org.apache.lucene.search.PhraseQuery;
+import org.apache.lucene.search.PrefixQuery;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.RangeQuery;
+import org.apache.lucene.search.TermQuery;
+import org.apache.lucene.search.WildcardQuery;
 import org.apache.lucene.util.Parameter;
 
 public class QueryParserTokenManager implements QueryParserConstants
Index: src/java/org/apache/lucene/analysis/SinkTokenizer.java
===================================================================
--- src/java/org/apache/lucene/analysis/SinkTokenizer.java	(revision 686801)
+++ src/java/org/apache/lucene/analysis/SinkTokenizer.java	(working copy)
@@ -22,11 +22,11 @@
   }
 
   public SinkTokenizer() {
-    this.lst = new ArrayList();
+    this.lst = new ArrayList/*<Token>*/();
   }
 
   public SinkTokenizer(int initCap){
-    this.lst = new ArrayList(initCap);
+    this.lst = new ArrayList/*<Token>*/(initCap);
   }
 
   /**
@@ -35,6 +35,8 @@
    * WARNING: Adding tokens to this list requires the {@link #reset()} method to be called in order for them
    * to be made available.  Also, this Tokenizer does nothing to protect against {@link java.util.ConcurrentModificationException}s
    * in the case of adds happening while {@link #next(org.apache.lucene.analysis.Token)} is being called.
+   * <p/>
+   * WARNING: Since this SinkTokenizer can be reset and the cached tokens made available again, do not modify them. Modify clones instead.
    *
    * @return A List of {@link org.apache.lucene.analysis.Token}s
    */
@@ -47,9 +49,15 @@
    * @return The next {@link org.apache.lucene.analysis.Token} in the Sink.
    * @throws IOException
    */
-  public Token next() throws IOException {
+  public Token next(final Token reusableToken) throws IOException {
+    assert reusableToken != null;
     if (iter == null) iter = lst.iterator();
-    return iter.hasNext() ? (Token) iter.next() : null;
+    // Since this TokenStream can be reset we have to maintain the tokens as immutable
+    if (iter.hasNext()) {
+      Token nextToken = (Token) iter.next();
+      return (Token) nextToken.clone();
+    }
+    return null;
   }
 
 
Index: src/java/org/apache/lucene/analysis/CachingTokenFilter.java
===================================================================
--- src/java/org/apache/lucene/analysis/CachingTokenFilter.java	(revision 686801)
+++ src/java/org/apache/lucene/analysis/CachingTokenFilter.java	(working copy)
@@ -40,11 +40,12 @@
     super(input);
   }
   
-  public Token next() throws IOException {
+  public Token next(final Token reusableToken) throws IOException {
+    assert reusableToken != null;
     if (cache == null) {
       // fill cache lazily
       cache = new LinkedList();
-      fillCache();
+      fillCache(reusableToken);
       iterator = cache.iterator();
     }
     
@@ -52,8 +53,9 @@
       // the cache is exhausted, return null
       return null;
     }
-    
-    return (Token) iterator.next();
+    // Since the TokenFilter can be reset, the tokens need to be preserved as immutable.
+    Token nextToken = (Token) iterator.next();
+    return (Token) nextToken.clone();
   }
   
   public void reset() throws IOException {
@@ -62,10 +64,9 @@
     }
   }
   
-  private void fillCache() throws IOException {
-    Token token;
-    while ( (token = input.next()) != null) {
-      cache.add(token);
+  private void fillCache(final Token reusableToken) throws IOException {
+    for (Token nextToken = input.next(reusableToken); nextToken != null; nextToken = input.next(reusableToken)) {
+      cache.add(nextToken.clone());
     }
   }
 
Index: src/java/org/apache/lucene/analysis/CharTokenizer.java
===================================================================
--- src/java/org/apache/lucene/analysis/CharTokenizer.java	(revision 686801)
+++ src/java/org/apache/lucene/analysis/CharTokenizer.java	(working copy)
@@ -44,11 +44,12 @@
     return c;
   }
 
-  public final Token next(Token token) throws IOException {
-    token.clear();
+  public final Token next(final Token reusableToken) throws IOException {
+    assert reusableToken != null;
+    reusableToken.clear();
     int length = 0;
     int start = bufferIndex;
-    char[] buffer = token.termBuffer();
+    char[] buffer = reusableToken.termBuffer();
     while (true) {
 
       if (bufferIndex >= dataLen) {
@@ -70,7 +71,7 @@
         if (length == 0)			           // start of token
           start = offset + bufferIndex - 1;
         else if (length == buffer.length)
-          buffer = token.resizeTermBuffer(1+length);
+          buffer = reusableToken.resizeTermBuffer(1+length);
 
         buffer[length++] = normalize(c); // buffer it, normalized
 
@@ -81,10 +82,10 @@
         break;                           // return 'em
     }
 
-    token.termLength = length;
-    token.startOffset = start;
-    token.endOffset = start+length;
-    return token;
+    reusableToken.setTermLength(length);
+    reusableToken.setStartOffset(start);
+    reusableToken.setEndOffset(start+length);
+    return reusableToken;
   }
 
   public void reset(Reader input) throws IOException {
Index: src/java/org/apache/lucene/analysis/PorterStemFilter.java
===================================================================
--- src/java/org/apache/lucene/analysis/PorterStemFilter.java	(revision 686801)
+++ src/java/org/apache/lucene/analysis/PorterStemFilter.java	(working copy)
@@ -45,13 +45,14 @@
     stemmer = new PorterStemmer();
   }
 
-  public final Token next(Token result) throws IOException {
-    result = input.next(result);
-    if (result != null) {
-      if (stemmer.stem(result.termBuffer(), 0, result.termLength))
-        result.setTermBuffer(stemmer.getResultBuffer(), 0, stemmer.getResultLength());
-      return result;
-    } else
+  public final Token next(final Token reusableToken) throws IOException {
+    assert reusableToken != null;
+    Token nextToken = input.next(reusableToken);
+    if (nextToken == null)
       return null;
+
+    if (stemmer.stem(nextToken.termBuffer(), 0, nextToken.termLength()))
+      nextToken.setTermBuffer(stemmer.getResultBuffer(), 0, stemmer.getResultLength());
+    return nextToken;
   }
 }
Index: src/java/org/apache/lucene/analysis/Tokenizer.java
===================================================================
--- src/java/org/apache/lucene/analysis/Tokenizer.java	(revision 686801)
+++ src/java/org/apache/lucene/analysis/Tokenizer.java	(working copy)
@@ -24,8 +24,9 @@
   <p>
   This is an abstract class.
   <p>
-  NOTE: subclasses must override at least one of {@link
-  #next()} or {@link #next(Token)}.
+  NOTE: subclasses must override {@link #next(Token)}.  It's
+  also OK to instead override {@link #next()} but that
+  method is now deprecated in favor of {@link #next(Token)}.
   <p>
   NOTE: subclasses overriding {@link #next(Token)} must  
   call {@link Token#clear()}.
Index: src/java/org/apache/lucene/analysis/KeywordTokenizer.java
===================================================================
--- src/java/org/apache/lucene/analysis/KeywordTokenizer.java	(revision 686801)
+++ src/java/org/apache/lucene/analysis/KeywordTokenizer.java	(working copy)
@@ -38,21 +38,22 @@
     this.done = false;
   }
 
-  public Token next(Token result) throws IOException {
+  public Token next(final Token reusableToken) throws IOException {
+    assert reusableToken != null;
     if (!done) {
       done = true;
       int upto = 0;
-      result.clear();
-      char[] buffer = result.termBuffer();
+      reusableToken.clear();
+      char[] buffer = reusableToken.termBuffer();
       while (true) {
         final int length = input.read(buffer, upto, buffer.length-upto);
         if (length == -1) break;
         upto += length;
         if (upto == buffer.length)
-          buffer = result.resizeTermBuffer(1+buffer.length);
+          buffer = reusableToken.resizeTermBuffer(1+buffer.length);
       }
-      result.termLength = upto;
-      return result;
+      reusableToken.setTermLength(upto);
+      return reusableToken;
     }
     return null;
   }
Index: src/java/org/apache/lucene/analysis/Token.java
===================================================================
--- src/java/org/apache/lucene/analysis/Token.java	(revision 686801)
+++ src/java/org/apache/lucene/analysis/Token.java	(working copy)
@@ -19,8 +19,9 @@
 
 import org.apache.lucene.index.Payload;
 import org.apache.lucene.index.TermPositions;     // for javadoc
+import org.apache.lucene.util.ArrayUtil;
 
-/** A Token is an occurence of a term from the text of a field.  It consists of
+/** A Token is an occurrence of a term from the text of a field.  It consists of
   a term's text, the start and end offset of the term in the text of the field,
   and a type string.
   <p>
@@ -29,7 +30,7 @@
   browser, or to show matching text fragments in a KWIC (KeyWord In Context)
   display, etc.
   <p>
-  The type is an interned string, assigned by a lexical analyzer
+  The type is a string, assigned by a lexical analyzer
   (a.k.a. tokenizer), naming the lexical or syntactic class that the token
   belongs to.  For example an end of sentence marker token might be implemented
   with type "eos".  The default token type is "word".  
@@ -49,7 +50,7 @@
   <p><b>NOTE:</b> As of 2.3, Token stores the term text
   internally as a malleable char[] termBuffer instead of
   String termText.  The indexing code and core tokenizers
-  have been changed re-use a single Token instance, changing
+  have been changed to re-use a single Token instance, changing
   its buffer and other fields in-place as the Token is
   processed.  This provides substantially better indexing
   performance as it saves the GC cost of new'ing a Token and
@@ -62,14 +63,55 @@
   instance when possible for best performance, by
   implementing the {@link TokenStream#next(Token)} API.
   Failing that, to create a new Token you should first use
-  one of the constructors that starts with null text.  Then
-  you should call either {@link #termBuffer()} or {@link
-  #resizeTermBuffer(int)} to retrieve the Token's
-  termBuffer.  Fill in the characters of your term into this
-  buffer, and finally call {@link #setTermLength(int)} to
+  one of the constructors that starts with null text.  To load
+  the token from a char[] use {@link #setTermBuffer(char[], int, int)}.
+  To load from a String use {@link #setTermBuffer(String)} or {@link #setTermBuffer(String, int, int)}.
+  Alternatively you can get the Token's termBuffer by calling either {@link #termBuffer()},
+  if you know that your text is shorter than the capacity of the termBuffer
+  or {@link #resizeTermBuffer(int)}, if there is any possibility
+  that you may need to grow the buffer. Fill in the characters of your term into this
+  buffer, with {@link String#getChars(int, int, char[], int)} if loading from a string,
+  or with {@link System#arraycopy(Object, int, Object, int, int)}, and finally call {@link #setTermLength(int)} to
   set the length of the term text.  See <a target="_top"
   href="https://issues.apache.org/jira/browse/LUCENE-969">LUCENE-969</a>
   for details.</p>
+  <p>Typical reuse patterns:
+  <ul>
+  <li> Copying text from a string (type is reset to #DEFAULT_TYPE if not specified):<br/>
+  <pre>
+    return reusableToken.reinit(string, startOffset, endOffset[, type]);
+  </pre>
+  </li>
+  <li> Copying some text from a string (type is reset to #DEFAULT_TYPE if not specified):<br/>
+  <pre>
+    return reusableToken.reinit(string, 0, string.length(), startOffset, endOffset[, type]);
+  </pre>
+  </li>
+  </li>
+  <li> Copying text from char[] buffer (type is reset to #DEFAULT_TYPE if not specified):<br/>
+  <pre>
+    return reusableToken.reinit(buffer, 0, buffer.length, startOffset, endOffset[, type]);
+  </pre>
+  </li>
+  <li> Copying some text from a char[] buffer (type is reset to #DEFAULT_TYPE if not specified):<br/>
+  <pre>
+    return reusableToken.reinit(buffer, start, end - start, startOffset, endOffset[, type]);
+  </pre>
+  </li>
+  <li> Copying from one one Token to another (type is reset to #DEFAULT_TYPE if not specified):<br/>
+  <pre>
+    return reusableToken.reinit(source.termBuffer(), 0, source.termLength(), source.startOffset(), source.endOffset()[, source.type()]);
+  </pre>
+  </li>
+  </ul>
+  A few things to note:
+  <ul>
+  <li>clear() initializes most of the fields to default values, but not startOffset, endOffset and type.</li>
+  <li>Because <code>TokenStreams</code> can be chained, one cannot assume that the <code>Token's</code> current type is correct.</li>
+  <li>The startOffset and endOffset represent the start and offset in the source text. So be careful in adjusting them.</li>
+  <li>When caching a reusable token, clone it. When injecting a cached token into a stream that can be reset, clone it again.</li>
+  </ul>
+  </p>
 
   @see org.apache.lucene.index.Payload
 */
@@ -83,16 +125,56 @@
    * deprecated APIs */
   private String termText;
 
-  char[] termBuffer;                              // characters for the term text
-  int termLength;                                 // length of term text in buffer
+  /**
+   * Characters for the term text.
+   * @deprecated This will be made private. Instead, use:
+   * {@link termBuffer()}, 
+   * {@link #setTermBuffer(char[], int, int)},
+   * {@link #setTermBuffer(String)}, or
+   * {@link #setTermBuffer(String, int, int)}
+   */
+  char[] termBuffer;
 
-  int startOffset;				  // start in source text
-  int endOffset;				  // end in source text
-  String type = DEFAULT_TYPE;                     // lexical type
+  /**
+   * Length of term text in the buffer.
+   * @deprecated This will be made private. Instead, use:
+   * {@link termLength()}, or @{link setTermLength(int)}.
+   */
+  int termLength;
+
+  /**
+   * Start in source text.
+   * @deprecated This will be made private. Instead, use:
+   * {@link startOffset()}, or @{link setStartOffset(int)}.
+   */
+  int startOffset;
+
+  /**
+   * End in source text.
+   * @deprecated This will be made private. Instead, use:
+   * {@link endOffset()}, or @{link setEndOffset(int)}.
+   */
+  int endOffset;
+
+  /**
+   * The lexical type of the token.
+   * @deprecated This will be made private. Instead, use:
+   * {@link type()}, or @{link setType(String)}.
+   */
+  String type = DEFAULT_TYPE;
+
   private int flags;
   
+  /**
+   * @deprecated This will be made private. Instead, use:
+   * {@link getPayload()}, or @{link setPayload(Payload)}.
+   */
   Payload payload;
   
+  /**
+   * @deprecated This will be made private. Instead, use:
+   * {@link getPositionIncrement()}, or @{link setPositionIncrement(String)}.
+   */
   int positionIncrement = 1;
 
   /** Constructs a Token will null text. */
@@ -101,8 +183,8 @@
 
   /** Constructs a Token with null text and start & end
    *  offsets.
-   *  @param start start offset
-   *  @param end end offset */
+   *  @param start start offset in the source text
+   *  @param end end offset in the source text */
   public Token(int start, int end) {
     startOffset = start;
     endOffset = end;
@@ -110,8 +192,9 @@
 
   /** Constructs a Token with null text and start & end
    *  offsets plus the Token type.
-   *  @param start start offset
-   *  @param end end offset */
+   *  @param start start offset in the source text
+   *  @param end end offset in the source text
+   *  @param typ the lexical type of this Token */
   public Token(int start, int end, String typ) {
     startOffset = start;
     endOffset = end;
@@ -120,12 +203,12 @@
 
   /**
    * Constructs a Token with null text and start & end
-   *  offsets plus the Token type.
-   *  @param start start offset
-   *  @param end end offset
-   * @param flags The bits to set for this token
+   *  offsets plus flags. NOTE: flags is EXPERIMENTAL.
+   *  @param start start offset in the source text
+   *  @param end end offset in the source text
+   *  @param flags The bits to set for this token
    */
-  public Token(int start, int end, int flags){
+  public Token(int start, int end, int flags) {
     startOffset = start;
     endOffset = end;
     this.flags = flags;
@@ -138,7 +221,9 @@
    *  term text.
    *  @param text term text
    *  @param start start offset
-   *  @param end end offset */
+   *  @param end end offset
+   *  @deprecated
+   */
   public Token(String text, int start, int end) {
     termText = text;
     startOffset = start;
@@ -152,7 +237,9 @@
    *  @param text term text
    *  @param start start offset
    *  @param end end offset
-   *  @param typ token type */
+   *  @param typ token type
+   *  @deprecated
+   */
   public Token(String text, int start, int end, String typ) {
     termText = text;
     startOffset = start;
@@ -169,6 +256,7 @@
    * @param start
    * @param end
    * @param flags token type bits
+   * @deprecated
    */
   public Token(String text, int start, int end, int flags) {
     termText = text;
@@ -177,6 +265,22 @@
     this.flags = flags;
   }
 
+  /**
+   *  Constructs a Token with the given term buffer (offset
+   *  & length), start and end
+   *  offsets
+   * @param startTermBuffer
+   * @param termBufferOffset
+   * @param termBufferLength
+   * @param start
+   * @param end
+   */
+  public Token(char[] startTermBuffer, int termBufferOffset, int termBufferLength, int start, int end) {
+    setTermBuffer(startTermBuffer, termBufferOffset, termBufferLength);
+    startOffset = start;
+    endOffset = end;
+  }
+
   /** Set the position increment.  This determines the position of this token
    * relative to the previous Token in a {@link TokenStream}, used in phrase
    * searching.
@@ -200,6 +304,7 @@
    * occur with no intervening stop words.
    *
    * </ul>
+   * @param positionIncrement the distance from the prior term
    * @see org.apache.lucene.index.TermPositions
    */
   public void setPositionIncrement(int positionIncrement) {
@@ -218,7 +323,11 @@
 
   /** Sets the Token's term text.  <b>NOTE:</b> for better
    *  indexing speed you should instead use the char[]
-   *  termBuffer methods to set the term text. */
+   *  termBuffer methods to set the term text.
+   *  @deprecated use {@link #setTermBuffer(char[], int, int)} or
+   *                  {@link #setTermBuffer(String)} or
+   *                  {@link #setTermBuffer(String, int, int)}.
+   */
   public void setTermText(String text) {
     termText = text;
     termBuffer = null;
@@ -230,7 +339,7 @@
    * because the text is stored internally in a char[].  If
    * possible, use {@link #termBuffer()} and {@link
    * #termLength()} directly instead.  If you really need a
-   * String, use <b>new String(token.termBuffer(), 0, token.termLength())</b>
+   * String, use {@link #term()}</b>
    */
   public final String termText() {
     if (termText == null && termBuffer != null)
@@ -238,19 +347,70 @@
     return termText;
   }
 
+  /** Returns the Token's term text.
+   * 
+   * This method has a performance penalty
+   * because the text is stored internally in a char[].  If
+   * possible, use {@link #termBuffer()} and {@link
+   * #termLength()} directly instead.  If you really need a
+   * String, use this method, which is nothing more than
+   * a convenience call to <b>new String(token.termBuffer(), 0, token.termLength())</b>
+   */
+  public final String term() {
+    if (termText != null)
+      return termText;
+    initTermBuffer();
+    return new String(termBuffer, 0, termLength);
+  }
+
   /** Copies the contents of buffer, starting at offset for
-   *  length characters, into the termBuffer
-   *  array. <b>NOTE:</b> for better indexing speed you
-   *  should instead retrieve the termBuffer, using {@link
-   *  #termBuffer()} or {@link #resizeTermBuffer(int)}, and
-   *  fill it in directly to set the term text.  This saves
-   *  an extra copy. */
+   *  length characters, into the termBuffer array.
+   *  @param buffer the buffer to copy
+   *  @param offset the index in the buffer of the first character to copy
+   *  @param length the number of characters to copy
+   */
   public final void setTermBuffer(char[] buffer, int offset, int length) {
-    resizeTermBuffer(length);
+    termText = null;
+    char[] newCharBuffer = growTermBuffer(length);
+    if (newCharBuffer != null) {
+      termBuffer = newCharBuffer;
+    }
     System.arraycopy(buffer, offset, termBuffer, 0, length);
     termLength = length;
   }
 
+  /** Copies the contents of buffer into the termBuffer array.
+   *  @param buffer the buffer to copy
+   */
+  public final void setTermBuffer(String buffer) {
+    termText = null;
+    int length = buffer.length();
+    char[] newCharBuffer = growTermBuffer(length);
+    if (newCharBuffer != null) {
+      termBuffer = newCharBuffer;
+    }
+    buffer.getChars(0, length, termBuffer, 0);
+    termLength = length;
+  }
+
+  /** Copies the contents of buffer, starting at offset and continuing
+   *  for length characters, into the termBuffer array.
+   *  @param buffer the buffer to copy
+   *  @param offset the index in the buffer of the first character to copy
+   *  @param length the number of characters to copy
+   */
+  public final void setTermBuffer(String buffer, int offset, int length) {
+    assert offset <= buffer.length();
+    assert offset + length <= buffer.length();
+    termText = null;
+    char[] newCharBuffer = growTermBuffer(length);
+    if (newCharBuffer != null) {
+      termBuffer = newCharBuffer;
+    }
+    buffer.getChars(offset, offset + length, termBuffer, 0);
+    termLength = length;
+  }
+
   /** Returns the internal termBuffer character array which
    *  you can then directly alter.  If the array is too
    *  small for your token, use {@link
@@ -263,23 +423,69 @@
     return termBuffer;
   }
 
-  /** Grows the termBuffer to at least size newSize.
+  /** Grows the termBuffer to at least size newSize, preserving the
+   *  existing content. Note: If the next operation is to change
+   *  the contents of the term buffer use
+   *  {@link #setTermBuffer(char[], int, int)},
+   *  {@link #setTermBuffer(String)}, or
+   *  {@link #setTermBuffer(String, int, int)}
+   *  to optimally combine the resize with the setting of the termBuffer.
    *  @param newSize minimum size of the new termBuffer
    *  @return newly created termBuffer with length >= newSize
    */
   public char[] resizeTermBuffer(int newSize) {
-    initTermBuffer();
-    if (newSize > termBuffer.length) {
-      int size = termBuffer.length;
-      while(size < newSize)
-        size *= 2;
-      char[] newBuffer = new char[size];
-      System.arraycopy(termBuffer, 0, newBuffer, 0, termBuffer.length);
-      termBuffer = newBuffer;
+    char[] newCharBuffer = growTermBuffer(newSize);
+    if (termBuffer == null) {
+      // If there were termText, then preserve it.
+      // note that if termBuffer is null then newCharBuffer cannot be null
+      assert newCharBuffer != null;
+      if (termText != null) {
+        termText.getChars(0, termText.length(), newCharBuffer, 0);
+      }
+      termBuffer = newCharBuffer;
+    } else if (newCharBuffer != null) {
+      // Note: if newCharBuffer != null then termBuffer needs to grow.
+      // If there were a termBuffer, then preserve it
+      System.arraycopy(termBuffer, 0, newCharBuffer, 0, termBuffer.length);
+      termBuffer = newCharBuffer;      
     }
+    termText = null;
     return termBuffer;
   }
 
+  /** Allocates a buffer char[] of at least newSize
+   *  @param newSize minimum size of the buffer
+   *  @return newly created buffer with length >= newSize or null if the current termBuffer is big enough
+   */
+  private char[] growTermBuffer(int newSize) {
+    if (termBuffer != null) {
+      if (termBuffer.length >= newSize)
+        // Already big enough
+        return null;
+      else
+        // Not big enough; create a new array with slight
+        // over allocation:
+        return new char[ArrayUtil.getNextSize(newSize)];
+    } else {
+
+      // determine the best size
+      // The buffer is always at least MIN_BUFFER_SIZE
+      if (newSize < MIN_BUFFER_SIZE) {
+        newSize = MIN_BUFFER_SIZE;
+      }
+
+      // If there is already a termText, then the size has to be at least that big
+      if (termText != null) {
+        int ttLength = termText.length();
+        if (newSize < ttLength) {
+          newSize = ttLength;
+        }
+      }
+
+      return new char[newSize];
+    }
+  }
+
   // TODO: once we remove the deprecated termText() method
   // and switch entirely to char[] termBuffer we don't need
   // to use this method anymore
@@ -308,9 +514,16 @@
   }
 
   /** Set number of valid characters (length of the term) in
-   *  the termBuffer array. */
+   *  the termBuffer array. Use this to truncate the termBuffer
+   *  or to synchronize with external manipulation of the termBuffer.
+   *  Note: to grow the size of the array,
+   *  use {@link #resizeTermBuffer(int)} first.
+   *  @param length the truncated length
+   */
   public final void setTermLength(int length) {
     initTermBuffer();
+    if (length > termBuffer.length)
+      throw new IllegalArgumentException("length " + length + " exceeds the size of the termBuffer (" + termBuffer.length + ")");
     termLength = length;
   }
 
@@ -331,7 +544,8 @@
   }
 
   /** Returns this Token's ending offset, one greater than the position of the
-    last character corresponding to this token in the source text. */
+    last character corresponding to this token in the source text. The length
+    of the token in the source text is (endOffset - startOffset). */
   public final int endOffset() {
     return endOffset;
   }
@@ -374,8 +588,6 @@
     this.flags = flags;
   }
 
-  
-
   /**
    * Returns this Token's payload.
    */ 
@@ -424,9 +636,9 @@
   public Object clone() {
     try {
       Token t = (Token)super.clone();
+      // Do a deep clone
       if (termBuffer != null) {
-        t.termBuffer = null;
-        t.setTermBuffer(termBuffer, 0, termLength);
+        t.termBuffer = (char[]) termBuffer.clone();
       }
       if (payload != null) {
         t.setPayload((Payload) payload.clone());
@@ -436,4 +648,212 @@
       throw new RuntimeException(e);  // shouldn't happen
     }
   }
+
+  /** Makes a clone, but replaces the term buffer &
+   * start/end offset in the process.  This is more
+   * efficient than doing a full clone (and then calling
+   * setTermBuffer) because it saves a wasted copy of the old
+   * termBuffer. */
+  public Token clone(char[] newTermBuffer, int newTermOffset, int newTermLength, int newStartOffset, int newEndOffset) {
+    final Token t = new Token(newTermBuffer, newTermOffset, newTermLength, newStartOffset, newEndOffset);
+    t.positionIncrement = positionIncrement;
+    t.flags = flags;
+    t.type = type;
+    if (payload != null)
+      t.payload = (Payload) payload.clone();
+    return t;
+  }
+
+  public boolean equals(Object obj) {
+    if (obj == this)
+      return true;
+
+    if (obj instanceof Token) {
+      Token other = (Token) obj;
+
+      initTermBuffer();
+      other.initTermBuffer();
+      
+      if (termLength == other.termLength &&
+          startOffset == other.startOffset &&
+          endOffset == other.endOffset && 
+          flags == other.flags &&
+          positionIncrement == other.positionIncrement &&
+          subEqual(type, other.type) &&
+          subEqual(payload, other.payload)) {
+        for(int i=0;i<termLength;i++)
+          if (termBuffer[i] != other.termBuffer[i])
+            return false;
+        return true;
+      } else
+        return false;
+    } else
+      return false;
+  }
+
+  private boolean subEqual(Object o1, Object o2) {
+    if (o1 == null)
+      return o2 == null;
+    else
+      return o1.equals(o2);
+  }
+
+  public int hashCode() {
+    initTermBuffer();
+    int code = termLength;
+    code = code * 31 + startOffset;
+    code = code * 31 + endOffset;
+    code = code * 31 + flags;
+    code = code * 31 + positionIncrement;
+    code = code * 31 + type.hashCode();
+    code = (payload == null ? code : code * 31 + payload.hashCode());
+    code = code * 31 + ArrayUtil.hashCode(termBuffer, 0, termLength);
+    return code;
+  }
+      
+  // like clear() but doesn't clear termBuffer/text
+  private void clearNoTermBuffer() {
+    payload = null;
+    positionIncrement = 1;
+    flags = 0;
+  }
+
+  /** Shorthand for calling {@link #clear},
+   *  {@link #setTermBuffer(char[], int, int)},
+   *  {@link #setStartOffset},
+   *  {@link #setEndOffset},
+   *  {@link #setType}
+   *  @return this Token instance */
+  public Token reinit(char[] newTermBuffer, int newTermOffset, int newTermLength, int newStartOffset, int newEndOffset, String newType) {
+    clearNoTermBuffer();
+    payload = null;
+    positionIncrement = 1;
+    setTermBuffer(newTermBuffer, newTermOffset, newTermLength);
+    startOffset = newStartOffset;
+    endOffset = newEndOffset;
+    type = newType;
+    return this;
+  }
+
+  /** Shorthand for calling {@link #clear},
+   *  {@link #setTermBuffer(char[], int, int)},
+   *  {@link #setStartOffset},
+   *  {@link #setEndOffset}
+   *  {@link #setType} on Token.DEFAULT_TYPE
+   *  @return this Token instance */
+  public Token reinit(char[] newTermBuffer, int newTermOffset, int newTermLength, int newStartOffset, int newEndOffset) {
+    clearNoTermBuffer();
+    setTermBuffer(newTermBuffer, newTermOffset, newTermLength);
+    startOffset = newStartOffset;
+    endOffset = newEndOffset;
+    type = DEFAULT_TYPE;
+    return this;
+  }
+
+  /** Shorthand for calling {@link #clear},
+   *  {@link #setTermBuffer(String)},
+   *  {@link #setStartOffset},
+   *  {@link #setEndOffset}
+   *  {@link #setType}
+   *  @return this Token instance */
+  public Token reinit(String newTerm, int newStartOffset, int newEndOffset, String newType) {
+    clearNoTermBuffer();
+    setTermBuffer(newTerm);
+    startOffset = newStartOffset;
+    endOffset = newEndOffset;
+    type = newType;
+    return this;
+  }
+
+  /** Shorthand for calling {@link #clear},
+   *  {@link #setTermBuffer(String, int, int)},
+   *  {@link #setStartOffset},
+   *  {@link #setEndOffset}
+   *  {@link #setType}
+   *  @return this Token instance */
+  public Token reinit(String newTerm, int newTermOffset, int newTermLength, int newStartOffset, int newEndOffset, String newType) {
+    clearNoTermBuffer();
+    setTermBuffer(newTerm, newTermOffset, newTermLength);
+    startOffset = newStartOffset;
+    endOffset = newEndOffset;
+    type = newType;
+    return this;
+  }
+
+  /** Shorthand for calling {@link #clear},
+   *  {@link #setTermBuffer(String)},
+   *  {@link #setStartOffset},
+   *  {@link #setEndOffset}
+   *  {@link #setType} on Token.DEFAULT_TYPE
+   *  @return this Token instance */
+  public Token reinit(String newTerm, int newStartOffset, int newEndOffset) {
+    clearNoTermBuffer();
+    setTermBuffer(newTerm);
+    startOffset = newStartOffset;
+    endOffset = newEndOffset;
+    type = DEFAULT_TYPE;
+    return this;
+  }
+
+  /** Shorthand for calling {@link #clear},
+   *  {@link #setTermBuffer(String, int, int)},
+   *  {@link #setStartOffset},
+   *  {@link #setEndOffset}
+   *  {@link #setType} on Token.DEFAULT_TYPE
+   *  @return this Token instance */
+  public Token reinit(String newTerm, int newTermOffset, int newTermLength, int newStartOffset, int newEndOffset) {
+    clearNoTermBuffer();
+    setTermBuffer(newTerm, newTermOffset, newTermLength);
+    startOffset = newStartOffset;
+    endOffset = newEndOffset;
+    type = DEFAULT_TYPE;
+    return this;
+  }
+
+  /**
+   * Copy the prototype token's fields into this one. Note: Payloads are shared.
+   * @param prototype
+   */
+  public void reinit(Token prototype) {
+    prototype.initTermBuffer();
+    setTermBuffer(prototype.termBuffer, 0, prototype.termLength);
+    positionIncrement = prototype.positionIncrement;
+    flags = prototype.flags;
+    startOffset = prototype.startOffset;
+    endOffset = prototype.endOffset;
+    type = prototype.type;
+    payload =  prototype.payload;
+  }
+
+  /**
+   * Copy the prototype token's fields into this one, with a different term. Note: Payloads are shared.
+   * @param prototype
+   * @param newTerm
+   */
+  public void reinit(Token prototype, String newTerm) {
+    setTermBuffer(newTerm);
+    positionIncrement = prototype.positionIncrement;
+    flags = prototype.flags;
+    startOffset = prototype.startOffset;
+    endOffset = prototype.endOffset;
+    type = prototype.type;
+    payload =  prototype.payload;
+  }
+
+  /**
+   * Copy the prototype token's fields into this one, with a different term. Note: Payloads are shared.
+   * @param prototype
+   * @param newTermBuffer
+   * @param offset
+   * @param length
+   */
+  public void reinit(Token prototype, char[] newTermBuffer, int offset, int length) {
+    setTermBuffer(newTermBuffer, offset, length);
+    positionIncrement = prototype.positionIncrement;
+    flags = prototype.flags;
+    startOffset = prototype.startOffset;
+    endOffset = prototype.endOffset;
+    type = prototype.type;
+    payload =  prototype.payload;
+  }
 }
Index: src/java/org/apache/lucene/analysis/standard/StandardFilter.java
===================================================================
--- src/java/org/apache/lucene/analysis/standard/StandardFilter.java	(revision 686801)
+++ src/java/org/apache/lucene/analysis/standard/StandardFilter.java	(working copy)
@@ -38,22 +38,23 @@
    * <p>Removes <tt>'s</tt> from the end of words.
    * <p>Removes dots from acronyms.
    */
-  public final Token next(Token result) throws java.io.IOException {
-    Token t = input.next(result);
+  public final Token next(final Token reusableToken) throws java.io.IOException {
+    assert reusableToken != null;
+    Token nextToken = input.next(reusableToken);
 
-    if (t == null)
+    if (nextToken == null)
       return null;
 
-    char[] buffer = t.termBuffer();
-    final int bufferLength = t.termLength();
-    final String type = t.type();
+    char[] buffer = nextToken.termBuffer();
+    final int bufferLength = nextToken.termLength();
+    final String type = nextToken.type();
 
     if (type == APOSTROPHE_TYPE &&		  // remove 's
 	bufferLength >= 2 &&
         buffer[bufferLength-2] == '\'' &&
         (buffer[bufferLength-1] == 's' || buffer[bufferLength-1] == 'S')) {
       // Strip last 2 characters off
-      t.setTermLength(bufferLength - 2);
+      nextToken.setTermLength(bufferLength - 2);
     } else if (type == ACRONYM_TYPE) {		  // remove dots
       int upto = 0;
       for(int i=0;i<bufferLength;i++) {
@@ -61,9 +62,9 @@
         if (c != '.')
           buffer[upto++] = c;
       }
-      t.setTermLength(upto);
+      nextToken.setTermLength(upto);
     }
 
-    return t;
+    return nextToken;
   }
 }
Index: src/java/org/apache/lucene/analysis/standard/StandardTokenizer.java
===================================================================
--- src/java/org/apache/lucene/analysis/standard/StandardTokenizer.java	(revision 686801)
+++ src/java/org/apache/lucene/analysis/standard/StandardTokenizer.java	(working copy)
@@ -132,7 +132,8 @@
    *
    * @see org.apache.lucene.analysis.TokenStream#next()
    */
-  public Token next(Token result) throws IOException {
+  public Token next(final Token reusableToken) throws IOException {
+      assert reusableToken != null;
       int posIncr = 1;
 
       while(true) {
@@ -143,26 +144,26 @@
 	}
 
         if (scanner.yylength() <= maxTokenLength) {
-          result.clear();
-          result.setPositionIncrement(posIncr);
-          scanner.getText(result);
+          reusableToken.clear();
+          reusableToken.setPositionIncrement(posIncr);
+          scanner.getText(reusableToken);
           final int start = scanner.yychar();
-          result.setStartOffset(start);
-          result.setEndOffset(start+result.termLength());
+          reusableToken.setStartOffset(start);
+          reusableToken.setEndOffset(start+reusableToken.termLength());
           // This 'if' should be removed in the next release. For now, it converts
           // invalid acronyms to HOST. When removed, only the 'else' part should
           // remain.
           if (tokenType == StandardTokenizerImpl.ACRONYM_DEP) {
             if (replaceInvalidAcronym) {
-              result.setType(StandardTokenizerImpl.TOKEN_TYPES[StandardTokenizerImpl.HOST]);
-              result.setTermLength(result.termLength() - 1); // remove extra '.'
+              reusableToken.setType(StandardTokenizerImpl.TOKEN_TYPES[StandardTokenizerImpl.HOST]);
+              reusableToken.setTermLength(reusableToken.termLength() - 1); // remove extra '.'
             } else {
-              result.setType(StandardTokenizerImpl.TOKEN_TYPES[StandardTokenizerImpl.ACRONYM]);
+              reusableToken.setType(StandardTokenizerImpl.TOKEN_TYPES[StandardTokenizerImpl.ACRONYM]);
             }
           } else {
-            result.setType(StandardTokenizerImpl.TOKEN_TYPES[tokenType]);
+            reusableToken.setType(StandardTokenizerImpl.TOKEN_TYPES[tokenType]);
           }
-          return result;
+          return reusableToken;
         } else
           // When we skip a too-long term, we still increment the
           // position increment
Index: src/java/org/apache/lucene/analysis/TeeTokenFilter.java
===================================================================
--- src/java/org/apache/lucene/analysis/TeeTokenFilter.java	(revision 686801)
+++ src/java/org/apache/lucene/analysis/TeeTokenFilter.java	(working copy)
@@ -45,10 +45,11 @@
     this.sink = sink;
   }
 
-  public Token next(Token result) throws IOException {
-    Token t = input.next(result);
-    sink.add(t);
-    return t;
+  public Token next(final Token reusableToken) throws IOException {
+    assert reusableToken != null;
+    Token nextToken = input.next(reusableToken);
+    sink.add(nextToken);
+    return nextToken;
   }
 
 }
Index: src/java/org/apache/lucene/analysis/TokenFilter.java
===================================================================
--- src/java/org/apache/lucene/analysis/TokenFilter.java	(revision 686801)
+++ src/java/org/apache/lucene/analysis/TokenFilter.java	(working copy)
@@ -22,8 +22,9 @@
 /** A TokenFilter is a TokenStream whose input is another token stream.
   <p>
   This is an abstract class.
-  NOTE: subclasses must override at least one of {@link
-  #next()} or {@link #next(Token)}.
+  NOTE: subclasses must override {@link #next(Token)}.  It's
+  also OK to instead override {@link #next()} but that
+  method is now deprecated in favor of {@link #next(Token)}.
   */
 public abstract class TokenFilter extends TokenStream {
   /** The source of tokens for this filter. */
Index: src/java/org/apache/lucene/analysis/LengthFilter.java
===================================================================
--- src/java/org/apache/lucene/analysis/LengthFilter.java	(revision 686801)
+++ src/java/org/apache/lucene/analysis/LengthFilter.java	(working copy)
@@ -42,16 +42,17 @@
   }
 
   /**
-   * Returns the next input Token whose termText() is the right len
+   * Returns the next input Token whose term() is the right len
    */
-  public final Token next(Token result) throws IOException
+  public final Token next(final Token reusableToken) throws IOException
   {
+    assert reusableToken != null;
     // return the first non-stop word found
-    for (Token token = input.next(result); token != null; token = input.next(result))
+    for (Token nextToken = input.next(reusableToken); nextToken != null; nextToken = input.next(reusableToken))
     {
-      int len = token.termLength();
+      int len = nextToken.termLength();
       if (len >= min && len <= max) {
-          return token;
+          return nextToken;
       }
       // note: else we ignore it but should we index each part of it?
     }
Index: src/java/org/apache/lucene/analysis/ISOLatin1AccentFilter.java
===================================================================
--- src/java/org/apache/lucene/analysis/ISOLatin1AccentFilter.java	(revision 686801)
+++ src/java/org/apache/lucene/analysis/ISOLatin1AccentFilter.java	(working copy)
@@ -32,22 +32,23 @@
   private char[] output = new char[256];
   private int outputPos;
 
-  public final Token next(Token result) throws java.io.IOException {
-    result = input.next(result);
-    if (result != null) {
-      final char[] buffer = result.termBuffer();
-      final int length = result.termLength();
+  public final Token next(final Token reusableToken) throws java.io.IOException {
+    assert reusableToken != null;
+    Token nextToken = input.next(reusableToken);
+    if (nextToken != null) {
+      final char[] buffer = nextToken.termBuffer();
+      final int length = nextToken.termLength();
       // If no characters actually require rewriting then we
       // just return token as-is:
       for(int i=0;i<length;i++) {
         final char c = buffer[i];
         if (c >= '\u00c0' && c <= '\uFB06') {
           removeAccents(buffer, length);
-          result.setTermBuffer(output, 0, outputPos);
+          nextToken.setTermBuffer(output, 0, outputPos);
           break;
         }
       }
-      return result;
+      return nextToken;
     } else
       return null;
   }
Index: src/java/org/apache/lucene/analysis/LowerCaseFilter.java
===================================================================
--- src/java/org/apache/lucene/analysis/LowerCaseFilter.java	(revision 686801)
+++ src/java/org/apache/lucene/analysis/LowerCaseFilter.java	(working copy)
@@ -29,16 +29,17 @@
     super(in);
   }
 
-  public final Token next(Token result) throws IOException {
-    result = input.next(result);
-    if (result != null) {
+  public final Token next(final Token reusableToken) throws IOException {
+    assert reusableToken != null;
+    Token nextToken = input.next(reusableToken);
+    if (nextToken != null) {
 
-      final char[] buffer = result.termBuffer();
-      final int length = result.termLength;
+      final char[] buffer = nextToken.termBuffer();
+      final int length = nextToken.termLength();
       for(int i=0;i<length;i++)
         buffer[i] = Character.toLowerCase(buffer[i]);
 
-      return result;
+      return nextToken;
     } else
       return null;
   }
Index: src/java/org/apache/lucene/analysis/StopFilter.java
===================================================================
--- src/java/org/apache/lucene/analysis/StopFilter.java	(revision 686801)
+++ src/java/org/apache/lucene/analysis/StopFilter.java	(working copy)
@@ -111,19 +111,20 @@
   }
 
   /**
-   * Returns the next input Token whose termText() is not a stop word.
+   * Returns the next input Token whose term() is not a stop word.
    */
-  public final Token next(Token result) throws IOException {
+  public final Token next(final Token reusableToken) throws IOException {
+    assert reusableToken != null;
     // return the first non-stop word found
     int skippedPositions = 0;
-    while((result = input.next(result)) != null) {
-      if (!stopWords.contains(result.termBuffer(), 0, result.termLength)) {
+    for (Token nextToken = input.next(reusableToken); nextToken != null; nextToken = input.next(reusableToken)) {
+      if (!stopWords.contains(nextToken.termBuffer(), 0, nextToken.termLength())) {
         if (enablePositionIncrements) {
-          result.setPositionIncrement(result.getPositionIncrement() + skippedPositions);
+          nextToken.setPositionIncrement(nextToken.getPositionIncrement() + skippedPositions);
         }
-        return result;
+        return nextToken;
       }
-      skippedPositions += result.getPositionIncrement();
+      skippedPositions += nextToken.getPositionIncrement();
     }
     // reached EOS -- return null
     return null;
Index: src/java/org/apache/lucene/analysis/TokenStream.java
===================================================================
--- src/java/org/apache/lucene/analysis/TokenStream.java	(revision 686801)
+++ src/java/org/apache/lucene/analysis/TokenStream.java	(working copy)
@@ -31,27 +31,29 @@
   <li>{@link TokenFilter}, a TokenStream
   whose input is another TokenStream.
   </ul>
-  NOTE: subclasses must override at least one of {@link
-  #next()} or {@link #next(Token)}.
+  NOTE: subclasses must override {@link #next(Token)}.  It's
+  also OK to instead override {@link #next()} but that
+  method is now deprecated in favor of {@link #next(Token)}.
   */
 
 public abstract class TokenStream {
 
   /** Returns the next token in the stream, or null at EOS.
-   *  The returned Token is a "full private copy" (not
+   *  @deprecated The returned Token is a "full private copy" (not
    *  re-used across calls to next()) but will be slower
    *  than calling {@link #next(Token)} instead.. */
   public Token next() throws IOException {
-    Token result = next(new Token());
+    final Token reusableToken = new Token();
+    Token nextToken = next(reusableToken);
 
-    if (result != null) {
-      Payload p = result.getPayload();
+    if (nextToken != null) {
+      Payload p = nextToken.getPayload();
       if (p != null) {
-        result.setPayload((Payload) p.clone());
+        nextToken.setPayload((Payload) p.clone());
       }
     }
 
-    return result;
+    return nextToken;
   }
 
   /** Returns the next token in the stream, or null at EOS.
@@ -71,11 +73,21 @@
    *   <li>A producer must call {@link Token#clear()}
    *       before setting the fields in it & returning it</li>
    *  </ul>
+   *  Also, the producer must make no assumptions about a
+   *  Token after it has been returned: the caller may
+   *  arbitrarily change it.  If the producer needs to hold
+   *  onto the token for subsequent calls, it must clone()
+   *  it before storing it.
    *  Note that a {@link TokenFilter} is considered a consumer.
-   *  @param result a Token that may or may not be used to return
+   *  @param reusableToken a Token that may or may not be used to
+   *  return; this parameter should never be null (the callee
+   *  is not required to check for null before using it, but it is a
+   *  good idea to assert that it is not null.)
    *  @return next token in the stream or null if end-of-stream was hit
    */
-  public Token next(Token result) throws IOException {
+  public Token next(final Token reusableToken) throws IOException {
+    // We don't actually use inputToken, but still add this assert
+    assert reusableToken != null;
     return next();
   }
 
@@ -84,7 +96,12 @@
    *  implement this method. Reset() is not needed for
    *  the standard indexing process. However, if the Tokens 
    *  of a TokenStream are intended to be consumed more than 
-   *  once, it is necessary to implement reset(). 
+   *  once, it is necessary to implement reset().  Note that
+   *  if your TokenStream caches tokens and feeds them back
+   *  again after a reset, it is imperative that you
+   *  clone the tokens when you store them away (on the
+   *  first pass) as well as when you return them (on future
+   *  passes after reset()).
    */
   public void reset() throws IOException {}
   
Index: src/java/org/apache/lucene/search/QueryTermVector.java
===================================================================
--- src/java/org/apache/lucene/search/QueryTermVector.java	(revision 686801)
+++ src/java/org/apache/lucene/search/QueryTermVector.java	(working copy)
@@ -17,15 +17,20 @@
  * limitations under the License.
  */
 
+import java.io.IOException;
+import java.io.StringReader;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.HashMap;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Map;
+
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.Token;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.index.TermFreqVector;
 
-import java.io.IOException;
-import java.io.StringReader;
-import java.util.*;
-
 /**
  *
  *
@@ -51,12 +56,11 @@
       TokenStream stream = analyzer.tokenStream("", new StringReader(queryString));
       if (stream != null)
       {
-        Token next = null;
         List terms = new ArrayList();
         try {
-          while ((next = stream.next()) != null)
-          {
-            terms.add(next.termText());
+          final Token reusableToken = new Token();
+          for (Token nextToken = stream.next(reusableToken); nextToken != null; nextToken = stream.next(reusableToken)) {
+            terms.add(nextToken.term());
           }
           processTerms((String[])terms.toArray(new String[terms.size()]));
         } catch (IOException e) {
Index: src/java/org/apache/lucene/index/DocInverterPerField.java
===================================================================
--- src/java/org/apache/lucene/index/DocInverterPerField.java	(revision 686801)
+++ src/java/org/apache/lucene/index/DocInverterPerField.java	(working copy)
@@ -79,15 +79,7 @@
         if (!field.isTokenized()) {		  // un-tokenized field
           String stringValue = field.stringValue();
           final int valueLength = stringValue.length();
-          Token token = perThread.localToken;
-          token.clear();
-          char[] termBuffer = token.termBuffer();
-          if (termBuffer.length < valueLength)
-            termBuffer = token.resizeTermBuffer(valueLength);
-          stringValue.getChars(0, valueLength, termBuffer, 0);
-          token.setTermLength(valueLength);
-          token.setStartOffset(fieldState.offset);
-          token.setEndOffset(fieldState.offset + stringValue.length());
+          Token token = perThread.localToken.reinit(stringValue, fieldState.offset, fieldState.offset + valueLength);
           boolean success = false;
           try {
             consumer.add(token);
@@ -96,7 +88,7 @@
             if (!success)
               docState.docWriter.setAborting();
           }
-          fieldState.offset += stringValue.length();
+          fieldState.offset += valueLength;
           fieldState.length++;
           fieldState.position++;
         } else {                                  // tokenized field
Index: src/java/org/apache/lucene/index/Payload.java
===================================================================
--- src/java/org/apache/lucene/index/Payload.java	(revision 686801)
+++ src/java/org/apache/lucene/index/Payload.java	(working copy)
@@ -21,143 +21,179 @@
 
 import org.apache.lucene.analysis.Token;
 import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.util.ArrayUtil;
 
- /**
-  *  A Payload is metadata that can be stored together with each occurrence 
-  *  of a term. This metadata is stored inline in the posting list of the
-  *  specific term.  
-  *  <p>
-  *  To store payloads in the index a {@link TokenStream} has to be used that
-  *  produces {@link Token}s containing payload data.
-  *  <p>
-  *  Use {@link TermPositions#getPayloadLength()} and {@link TermPositions#getPayload(byte[], int)}
-  *  to retrieve the payloads from the index.<br>
-  *
-  */
-  public class Payload implements Serializable, Cloneable {
-    /** the byte array containing the payload data */
-    protected byte[] data;
+/**
+ *  A Payload is metadata that can be stored together with each occurrence 
+ *  of a term. This metadata is stored inline in the posting list of the
+ *  specific term.  
+ *  <p>
+ *  To store payloads in the index a {@link TokenStream} has to be used that
+ *  produces {@link Token}s containing payload data.
+ *  <p>
+ *  Use {@link TermPositions#getPayloadLength()} and {@link TermPositions#getPayload(byte[], int)}
+ *  to retrieve the payloads from the index.<br>
+ *
+ */
+public class Payload implements Serializable, Cloneable {
+  /** the byte array containing the payload data */
+  protected byte[] data;
     
-    /** the offset within the byte array */
-    protected int offset;
+  /** the offset within the byte array */
+  protected int offset;
     
-    /** the length of the payload data */
-    protected int length;
+  /** the length of the payload data */
+  protected int length;
     
-    /** Creates an empty payload and does not allocate a byte array. */
-    public Payload() {
-      // nothing to do
-    }
+  /** Creates an empty payload and does not allocate a byte array. */
+  public Payload() {
+    // nothing to do
+  }
     
-    /**
-     * Creates a new payload with the the given array as data.
-     * A reference to the passed-in array is held, i. e. no 
-     * copy is made.
-     * 
-     * @param data the data of this payload
-     */
-    public Payload(byte[] data) {
-      this(data, 0, data.length);
-    }
+  /**
+   * Creates a new payload with the the given array as data.
+   * A reference to the passed-in array is held, i. e. no 
+   * copy is made.
+   * 
+   * @param data the data of this payload
+   */
+  public Payload(byte[] data) {
+    this(data, 0, data.length);
+  }
 
-    /**
-     * Creates a new payload with the the given array as data. 
-     * A reference to the passed-in array is held, i. e. no 
-     * copy is made.
-     * 
-     * @param data the data of this payload
-     * @param offset the offset in the data byte array
-     * @param length the length of the data
-     */
-    public Payload(byte[] data, int offset, int length) {
-      if (offset < 0 || offset + length > data.length) {
-        throw new IllegalArgumentException();
-      }
-      this.data = data;
-      this.offset = offset;
-      this.length = length;
+  /**
+   * Creates a new payload with the the given array as data. 
+   * A reference to the passed-in array is held, i. e. no 
+   * copy is made.
+   * 
+   * @param data the data of this payload
+   * @param offset the offset in the data byte array
+   * @param length the length of the data
+   */
+  public Payload(byte[] data, int offset, int length) {
+    if (offset < 0 || offset + length > data.length) {
+      throw new IllegalArgumentException();
     }
+    this.data = data;
+    this.offset = offset;
+    this.length = length;
+  }
     
-    /**
-     * Sets this payloads data. 
-     * A reference to the passed-in array is held, i. e. no 
-     * copy is made.
-     */
-    public void setData(byte[] data) {
-      setData(data, 0, data.length);
-    }
+  /**
+   * Sets this payloads data. 
+   * A reference to the passed-in array is held, i. e. no 
+   * copy is made.
+   */
+  public void setData(byte[] data) {
+    setData(data, 0, data.length);
+  }
 
-    /**
-     * Sets this payloads data. 
-     * A reference to the passed-in array is held, i. e. no 
-     * copy is made.
-     */
-    public void setData(byte[] data, int offset, int length) {
-      this.data = data;
-      this.offset = offset;
-      this.length = length;
-    }
+  /**
+   * Sets this payloads data. 
+   * A reference to the passed-in array is held, i. e. no 
+   * copy is made.
+   */
+  public void setData(byte[] data, int offset, int length) {
+    this.data = data;
+    this.offset = offset;
+    this.length = length;
+  }
     
-    /**
-     * Returns a reference to the underlying byte array
-     * that holds this payloads data.
-     */
-    public byte[] getData() {
-      return this.data;
-    }
+  /**
+   * Returns a reference to the underlying byte array
+   * that holds this payloads data.
+   */
+  public byte[] getData() {
+    return this.data;
+  }
     
-    /**
-     * Returns the offset in the underlying byte array 
-     */
-    public int getOffset() {
-      return this.offset;
-    }
+  /**
+   * Returns the offset in the underlying byte array 
+   */
+  public int getOffset() {
+    return this.offset;
+  }
     
-    /**
-     * Returns the length of the payload data. 
-     */
-    public int length() {
-      return this.length;
-    }
+  /**
+   * Returns the length of the payload data. 
+   */
+  public int length() {
+    return this.length;
+  }
     
-    /**
-     * Returns the byte at the given index.
-     */
-    public byte byteAt(int index) {
-      if (0 <= index && index < this.length) {
-        return this.data[this.offset + index];    
-      }
-      throw new ArrayIndexOutOfBoundsException(index);
+  /**
+   * Returns the byte at the given index.
+   */
+  public byte byteAt(int index) {
+    if (0 <= index && index < this.length) {
+      return this.data[this.offset + index];    
     }
+    throw new ArrayIndexOutOfBoundsException(index);
+  }
     
-    /**
-     * Allocates a new byte array, copies the payload data into it and returns it. 
-     */
-    public byte[] toByteArray() {
-      byte[] retArray = new byte[this.length];
-      System.arraycopy(this.data, this.offset, retArray, 0, this.length);
-      return retArray;
-    }
+  /**
+   * Allocates a new byte array, copies the payload data into it and returns it. 
+   */
+  public byte[] toByteArray() {
+    byte[] retArray = new byte[this.length];
+    System.arraycopy(this.data, this.offset, retArray, 0, this.length);
+    return retArray;
+  }
     
-    /**
-     * Copies the payload data to a byte array.
-     * 
-     * @param target the target byte array
-     * @param targetOffset the offset in the target byte array
-     */
-    public void copyTo(byte[] target, int targetOffset) {
-      if (this.length > target.length + targetOffset) {
-        throw new ArrayIndexOutOfBoundsException();
-      }
-      System.arraycopy(this.data, this.offset, target, targetOffset, this.length);
+  /**
+   * Copies the payload data to a byte array.
+   * 
+   * @param target the target byte array
+   * @param targetOffset the offset in the target byte array
+   */
+  public void copyTo(byte[] target, int targetOffset) {
+    if (this.length > target.length + targetOffset) {
+      throw new ArrayIndexOutOfBoundsException();
     }
+    System.arraycopy(this.data, this.offset, target, targetOffset, this.length);
+  }
 
-    /**
-     * Clones this payload by creating a copy of the underlying
-     * byte array.
-     */
-    public Object clone() {
-      Payload clone = new Payload(this.toByteArray());
+  /**
+   * Clones this payload by creating a copy of the underlying
+   * byte array.
+   */
+  public Object clone() {
+    try {
+      // Start with a shallow copy of data
+      Payload clone = (Payload) super.clone();
+      // Only copy the part of data that belongs to this Payload
+      if (offset == 0 && length == data.length) {
+        // It is the whole thing, so just clone it.
+        clone.data = (byte[]) data.clone();
+      }
+      else {
+        // Just get the part
+        clone.data = this.toByteArray();
+        clone.offset = 0;
+      }
       return clone;
+    } catch (CloneNotSupportedException e) {
+      throw new RuntimeException(e);  // shouldn't happen
     }
+  }
+
+  public boolean equals(Object obj) {
+    if (obj == this)
+      return true;
+    if (obj instanceof Payload) {
+      Payload other = (Payload) obj;
+      if (length == other.length) {
+        for(int i=0;i<length;i++)
+          if (data[offset+i] != other.data[other.offset+i])
+            return false;
+        return true;
+      } else
+        return false;
+    } else
+      return false;
+  }
+
+  public int hashCode() {
+    return ArrayUtil.hashCode(data, offset, offset+length);
+  }
 }
Index: src/java/org/apache/lucene/util/ArrayUtil.java
===================================================================
--- src/java/org/apache/lucene/util/ArrayUtil.java	(revision 686801)
+++ src/java/org/apache/lucene/util/ArrayUtil.java	(working copy)
@@ -32,7 +32,10 @@
 
   public static int getShrinkSize(int currentSize, int targetSize) {
     final int newSize = getNextSize(targetSize);
-    if (newSize < currentSize && currentSize > newSize*2)
+    // Only reallocate if we are "substantially" smaller.
+    // This saves us from "running hot" (constantly making a
+    // bit bigger then a bit smaller, over and over):
+    if (newSize < currentSize/2)
       return newSize;
     else
       return currentSize;
@@ -106,4 +109,22 @@
     } else
       return array;
   }
+
+  /** Returns hash of chars in range start (inclusive) to
+   *  end (inclusive) */
+  public static int hashCode(char[] array, int start, int end) {
+    int code = 0;
+    for(int i=end-1;i>=start;i--)
+      code = code*31 + array[i];
+    return code;
+  }
+
+  /** Returns hash of chars in range start (inclusive) to
+   *  end (inclusive) */
+  public static int hashCode(byte[] array, int start, int end) {
+    int code = 0;
+    for(int i=end-1;i>=start;i--)
+      code = code*31 + array[i];
+    return code;
+  }
 }
Index: src/demo/org/apache/lucene/demo/html/TokenMgrError.java
===================================================================
--- src/demo/org/apache/lucene/demo/html/TokenMgrError.java	(revision 686801)
+++ src/demo/org/apache/lucene/demo/html/TokenMgrError.java	(working copy)
@@ -72,7 +72,7 @@
            default:
               if ((ch = str.charAt(i)) < 0x20 || ch > 0x7e) {
                  String s = "0000" + Integer.toString(ch, 16);
-                 retval.append("\\u").append(s.substring(s.length() - 4, s.length()));
+                 retval.append("\\u" + s.substring(s.length() - 4, s.length()));
               } else {
                  retval.append(ch);
               }
Index: src/demo/org/apache/lucene/demo/html/HTMLParser.java
===================================================================
--- src/demo/org/apache/lucene/demo/html/HTMLParser.java	(revision 686801)
+++ src/demo/org/apache/lucene/demo/html/HTMLParser.java	(working copy)
@@ -487,7 +487,10 @@
   private int jj_gc = 0;
 
   public HTMLParser(java.io.InputStream stream) {
-    jj_input_stream = new SimpleCharStream(stream, 1, 1);
+     this(stream, null);
+  }
+  public HTMLParser(java.io.InputStream stream, String encoding) {
+    try { jj_input_stream = new SimpleCharStream(stream, encoding, 1, 1); } catch(java.io.UnsupportedEncodingException e) { throw new RuntimeException(e); }
     token_source = new HTMLParserTokenManager(jj_input_stream);
     token = new Token();
     jj_ntk = -1;
@@ -497,7 +500,10 @@
   }
 
   public void ReInit(java.io.InputStream stream) {
-    jj_input_stream.ReInit(stream, 1, 1);
+     ReInit(stream, null);
+  }
+  public void ReInit(java.io.InputStream stream, String encoding) {
+    try { jj_input_stream.ReInit(stream, encoding, 1, 1); } catch(java.io.UnsupportedEncodingException e) { throw new RuntimeException(e); }
     token_source.ReInit(jj_input_stream);
     token = new Token();
     jj_ntk = -1;
@@ -627,7 +633,9 @@
       jj_lasttokens[jj_endpos++] = kind;
     } else if (jj_endpos != 0) {
       jj_expentry = new int[jj_endpos];
-      System.arraycopy(jj_lasttokens, 0, jj_expentry, 0, jj_endpos);
+      for (int i = 0; i < jj_endpos; i++) {
+        jj_expentry[i] = jj_lasttokens[i];
+      }
       boolean exists = false;
       for (java.util.Enumeration e = jj_expentries.elements(); e.hasMoreElements();) {
         int[] oldentry = (int[])(e.nextElement());
@@ -692,6 +700,7 @@
   final private void jj_rescan_token() {
     jj_rescan = true;
     for (int i = 0; i < 2; i++) {
+    try {
       JJCalls p = jj_2_rtns[i];
       do {
         if (p.gen > jj_gen) {
@@ -703,6 +712,7 @@
         }
         p = p.next;
       } while (p != null);
+      } catch(LookaheadSuccess ls) { }
     }
     jj_rescan = false;
   }
Index: src/demo/org/apache/lucene/demo/html/SimpleCharStream.java
===================================================================
--- src/demo/org/apache/lucene/demo/html/SimpleCharStream.java	(revision 686801)
+++ src/demo/org/apache/lucene/demo/html/SimpleCharStream.java	(working copy)
@@ -1,4 +1,4 @@
-/* Generated By:JavaCC: Do not edit this line. SimpleCharStream.java Version 3.0 */
+/* Generated By:JavaCC: Do not edit this line. SimpleCharStream.java Version 4.0 */
 package org.apache.lucene.demo.html;
 
 /**
@@ -27,7 +27,12 @@
   protected char[] buffer;
   protected int maxNextCharInd = 0;
   protected int inBuf = 0;
+  protected int tabSize = 8;
 
+  protected void setTabSize(int i) { tabSize = i; }
+  protected int getTabSize(int i) { return tabSize; }
+
+
   protected void ExpandBuff(boolean wrapAround)
   {
      char[] newbuffer = new char[bufsize + 2048];
@@ -162,7 +167,7 @@
            break;
         case '\t' :
            column--;
-           column += (8 - (column & 07));
+           column += (tabSize - (column % tabSize));
            break;
         default :
            break;
@@ -248,7 +253,7 @@
   }
 
   public SimpleCharStream(java.io.Reader dstream, int startline,
-                                                           int startcolumn)
+                          int startcolumn)
   {
      this(dstream, startline, startcolumn, 4096);
   }
@@ -277,7 +282,7 @@
   }
 
   public void ReInit(java.io.Reader dstream, int startline,
-                                                           int startcolumn)
+                     int startcolumn)
   {
      ReInit(dstream, startline, startcolumn, 4096);
   }
@@ -286,35 +291,68 @@
   {
      ReInit(dstream, 1, 1, 4096);
   }
+  public SimpleCharStream(java.io.InputStream dstream, String encoding, int startline,
+  int startcolumn, int buffersize) throws java.io.UnsupportedEncodingException
+  {
+     this(encoding == null ? new java.io.InputStreamReader(dstream) : new java.io.InputStreamReader(dstream, encoding), startline, startcolumn, buffersize);
+  }
+
   public SimpleCharStream(java.io.InputStream dstream, int startline,
   int startcolumn, int buffersize)
   {
-     this(new java.io.InputStreamReader(dstream), startline, startcolumn, 4096);
+     this(new java.io.InputStreamReader(dstream), startline, startcolumn, buffersize);
   }
 
+  public SimpleCharStream(java.io.InputStream dstream, String encoding, int startline,
+                          int startcolumn) throws java.io.UnsupportedEncodingException
+  {
+     this(dstream, encoding, startline, startcolumn, 4096);
+  }
+
   public SimpleCharStream(java.io.InputStream dstream, int startline,
-                                                           int startcolumn)
+                          int startcolumn)
   {
      this(dstream, startline, startcolumn, 4096);
   }
 
+  public SimpleCharStream(java.io.InputStream dstream, String encoding) throws java.io.UnsupportedEncodingException
+  {
+     this(dstream, encoding, 1, 1, 4096);
+  }
+
   public SimpleCharStream(java.io.InputStream dstream)
   {
      this(dstream, 1, 1, 4096);
   }
 
+  public void ReInit(java.io.InputStream dstream, String encoding, int startline,
+                          int startcolumn, int buffersize) throws java.io.UnsupportedEncodingException
+  {
+     ReInit(encoding == null ? new java.io.InputStreamReader(dstream) : new java.io.InputStreamReader(dstream, encoding), startline, startcolumn, buffersize);
+  }
+
   public void ReInit(java.io.InputStream dstream, int startline,
                           int startcolumn, int buffersize)
   {
-     ReInit(new java.io.InputStreamReader(dstream), startline, startcolumn, 4096);
+     ReInit(new java.io.InputStreamReader(dstream), startline, startcolumn, buffersize);
   }
 
+  public void ReInit(java.io.InputStream dstream, String encoding) throws java.io.UnsupportedEncodingException
+  {
+     ReInit(dstream, encoding, 1, 1, 4096);
+  }
+
   public void ReInit(java.io.InputStream dstream)
   {
      ReInit(dstream, 1, 1, 4096);
   }
+  public void ReInit(java.io.InputStream dstream, String encoding, int startline,
+                     int startcolumn) throws java.io.UnsupportedEncodingException
+  {
+     ReInit(dstream, encoding, startline, startcolumn, 4096);
+  }
   public void ReInit(java.io.InputStream dstream, int startline,
-                                                           int startcolumn)
+                     int startcolumn)
   {
      ReInit(dstream, startline, startcolumn, 4096);
   }
Index: src/demo/org/apache/lucene/demo/html/ParseException.java
===================================================================
--- src/demo/org/apache/lucene/demo/html/ParseException.java	(revision 686801)
+++ src/demo/org/apache/lucene/demo/html/ParseException.java	(working copy)
@@ -98,19 +98,19 @@
     if (!specialConstructor) {
       return super.getMessage();
     }
-    String expected = "";
+    StringBuffer expected = new StringBuffer();
     int maxSize = 0;
     for (int i = 0; i < expectedTokenSequences.length; i++) {
       if (maxSize < expectedTokenSequences[i].length) {
         maxSize = expectedTokenSequences[i].length;
       }
       for (int j = 0; j < expectedTokenSequences[i].length; j++) {
-        expected += tokenImage[expectedTokenSequences[i][j]] + " ";
+        expected.append(tokenImage[expectedTokenSequences[i][j]]).append(" ");
       }
       if (expectedTokenSequences[i][expectedTokenSequences[i].length - 1] != 0) {
-        expected += "...";
+        expected.append("...");
       }
-      expected += eol + "    ";
+      expected.append(eol).append("    ");
     }
     String retval = "Encountered \"";
     Token tok = currentToken.next;
@@ -130,7 +130,7 @@
     } else {
       retval += "Was expecting one of:" + eol + "    ";
     }
-    retval += expected;
+    retval += expected.toString();
     return retval;
   }
 
@@ -179,7 +179,7 @@
            default:
               if ((ch = str.charAt(i)) < 0x20 || ch > 0x7e) {
                  String s = "0000" + Integer.toString(ch, 16);
-                 retval.append("\\u").append(s.substring(s.length() - 4, s.length()));
+                 retval.append("\\u" + s.substring(s.length() - 4, s.length()));
               } else {
                  retval.append(ch);
               }
Index: src/demo/org/apache/lucene/demo/html/HTMLParserTokenManager.java
===================================================================
--- src/demo/org/apache/lucene/demo/html/HTMLParserTokenManager.java	(revision 686801)
+++ src/demo/org/apache/lucene/demo/html/HTMLParserTokenManager.java	(working copy)
@@ -1457,14 +1457,12 @@
 private final int[] jjrounds = new int[28];
 private final int[] jjstateSet = new int[56];
 protected char curChar;
-public HTMLParserTokenManager(SimpleCharStream stream)
-{
+public HTMLParserTokenManager(SimpleCharStream stream){
    if (SimpleCharStream.staticFlag)
       throw new Error("ERROR: Cannot use a static CharStream class with a non-static lexical analyzer.");
    input_stream = stream;
 }
-public HTMLParserTokenManager(SimpleCharStream stream, int lexState)
-{
+public HTMLParserTokenManager(SimpleCharStream stream, int lexState){
    this(stream);
    SwitchTo(lexState);
 }
Index: contrib/snowball/src/test/org/apache/lucene/analysis/snowball/TestSnowball.java
===================================================================
--- contrib/snowball/src/test/org/apache/lucene/analysis/snowball/TestSnowball.java	(revision 686801)
+++ contrib/snowball/src/test/org/apache/lucene/analysis/snowball/TestSnowball.java	(working copy)
@@ -1,64 +1,30 @@
 package org.apache.lucene.analysis.snowball;
 
-/* ====================================================================
- * The Apache Software License, Version 1.1
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
  *
- * Copyright (c) 2004 The Apache Software Foundation.  All rights
- * reserved.
+ *     http://www.apache.org/licenses/LICENSE-2.0
  *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- *
- * 1. Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- *
- * 2. Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in
- *    the documentation and/or other materials provided with the
- *    distribution.
- *
- * 3. The end-user documentation included with the redistribution,
- *    if any, must include the following acknowledgment:
- *       "This product includes software developed by the
- *        Apache Software Foundation (http://www.apache.org/)."
- *    Alternately, this acknowledgment may appear in the software itself,
- *    if and wherever such third-party acknowledgments normally appear.
- *
- * 4. The names "Apache" and "Apache Software Foundation" and
- *    "Apache Lucene" must not be used to endorse or promote products
- *    derived from this software without prior written permission. For
- *    written permission, please contact apache@apache.org.
- *
- * 5. Products derived from this software may not be called "Apache",
- *    "Apache Lucene", nor may "Apache" appear in their name, without
- *    prior written permission of the Apache Software Foundation.
- *
- * THIS SOFTWARE IS PROVIDED ``AS IS'' AND ANY EXPRESSED OR IMPLIED
- * WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
- * OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
- * DISCLAIMED.  IN NO EVENT SHALL THE APACHE SOFTWARE FOUNDATION OR
- * ITS CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
- * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
- * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF
- * USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
- * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
- * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT
- * OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
- * SUCH DAMAGE.
- * ====================================================================
- *
- * This software consists of voluntary contributions made by many
- * individuals on behalf of the Apache Software Foundation.  For more
- * information on the Apache Software Foundation, please see
- * <http://www.apache.org/>.
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
  */
 
-import java.io.*;
+import java.io.StringReader;
 
-import junit.framework.*;
+import junit.framework.TestCase;
 
-import org.apache.lucene.analysis.*;
+import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.analysis.Token;
+import org.apache.lucene.index.Payload;
+import org.apache.lucene.analysis.TokenStream;
 
 public class TestSnowball extends TestCase {
 
@@ -66,12 +32,12 @@
                                String input,
                                String[] output) throws Exception {
     TokenStream ts = a.tokenStream("dummy", new StringReader(input));
+    final Token reusableToken = new Token();
     for (int i = 0; i < output.length; i++) {
-      Token t = ts.next();
-      assertNotNull(t);
-      assertEquals(output[i], t.termText());
+      Token nextToken = ts.next(reusableToken);
+      assertEquals(output[i], nextToken.term());
     }
-    assertNull(ts.next());
+    assertNull(ts.next(reusableToken));
     ts.close();
   }
 
@@ -83,25 +49,33 @@
 
 
   public void testFilterTokens() throws Exception {
-    final Token tok = new Token("accents", 2, 7, "wrd");
+    final Token tok = new Token(2, 7, "wrd");
+    tok.setTermBuffer("accents");
     tok.setPositionIncrement(3);
+    Payload tokPayload = new Payload(new byte[]{0,1,2,3});
+    tok.setPayload(tokPayload);
+    int tokFlags = 77;
+    tok.setFlags(tokFlags);
 
     SnowballFilter filter = new SnowballFilter(
         new TokenStream() {
-          public Token next() {
+          public Token next(final Token reusableToken) {
+            assert reusableToken != null;
             return tok;
           }
         },
         "English"
     );
 
-    Token newtok = filter.next();
+    final Token reusableToken = new Token();
+    Token nextToken = filter.next(reusableToken);
 
-    assertEquals("accent", newtok.termText());
-    assertEquals(2, newtok.startOffset());
-    assertEquals(7, newtok.endOffset());
-    assertEquals("wrd", newtok.type());
-    assertEquals(3, newtok.getPositionIncrement());
+    assertEquals("accent", nextToken.term());
+    assertEquals(2, nextToken.startOffset());
+    assertEquals(7, nextToken.endOffset());
+    assertEquals("wrd", nextToken.type());
+    assertEquals(3, nextToken.getPositionIncrement());
+    assertEquals(tokFlags, nextToken.getFlags());
+    assertEquals(tokPayload, nextToken.getPayload());
   }
-}
-
+}
\ No newline at end of file
Index: contrib/snowball/src/java/org/apache/lucene/analysis/snowball/SnowballFilter.java
===================================================================
--- contrib/snowball/src/java/org/apache/lucene/analysis/snowball/SnowballFilter.java	(revision 686801)
+++ contrib/snowball/src/java/org/apache/lucene/analysis/snowball/SnowballFilter.java	(working copy)
@@ -18,11 +18,10 @@
  */
 
 import java.io.IOException;
-
 import java.lang.reflect.Method;
 
 import net.sf.snowball.SnowballProgram;
-import net.sf.snowball.ext.*;
+import net.sf.snowball.ext.EnglishStemmer;
 
 import org.apache.lucene.analysis.Token;
 import org.apache.lucene.analysis.TokenFilter;
@@ -60,20 +59,22 @@
   }
 
   /** Returns the next input Token, after being stemmed */
-  public final Token next() throws IOException {
-    Token token = input.next();
-    if (token == null)
+  public final Token next(final Token reusableToken) throws IOException {
+    assert reusableToken != null;
+    Token nextToken = input.next(reusableToken);
+    if (nextToken == null)
       return null;
-    stemmer.setCurrent(token.termText());
+    String originalTerm = nextToken.term();
+    stemmer.setCurrent(originalTerm);
     try {
       stemMethod.invoke(stemmer, EMPTY_ARGS);
     } catch (Exception e) {
       throw new RuntimeException(e.toString());
     }
-    
-    Token newToken = new Token(stemmer.getCurrent(),
-                      token.startOffset(), token.endOffset(), token.type());
-    newToken.setPositionIncrement(token.getPositionIncrement());
-    return newToken;
+    String finalTerm = stemmer.getCurrent();
+    // Don't bother updating, if it is unchanged.
+    if (!originalTerm.equals(finalTerm))
+      nextToken.setTermBuffer(finalTerm);
+    return nextToken;
   }
 }
Index: contrib/wordnet/src/java/org/apache/lucene/wordnet/SynLookup.java
===================================================================
--- contrib/wordnet/src/java/org/apache/lucene/wordnet/SynLookup.java	(revision 686801)
+++ contrib/wordnet/src/java/org/apache/lucene/wordnet/SynLookup.java	(working copy)
@@ -17,15 +17,29 @@
  * limitations under the License.
  */
 
-import org.apache.lucene.store.*;
-import org.apache.lucene.search.*;
-import org.apache.lucene.index.*;
-import org.apache.lucene.document.*;
-import org.apache.lucene.analysis.*;
-import java.io.*;
-import java.util.*;
+import java.io.IOException;
+import java.io.StringReader;
+import java.util.HashSet;
+import java.util.Iterator;
+import java.util.LinkedList;
+import java.util.List;
+import java.util.Set;
 
+import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.analysis.Token;
+import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.index.Term;
+import org.apache.lucene.search.BooleanClause;
+import org.apache.lucene.search.BooleanQuery;
+import org.apache.lucene.search.Hits;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.Searcher;
+import org.apache.lucene.search.TermQuery;
+import org.apache.lucene.store.FSDirectory;
 
+
 /**
  * Test program to look up synonyms.
  */
@@ -86,10 +100,9 @@
 
 		// [1] Parse query into separate words so that when we expand we can avoid dups
 		TokenStream ts = a.tokenStream( field, new StringReader( query));
-		org.apache.lucene.analysis.Token t;
-		while ( (t = ts.next()) != null)
-		{
-			String word = t.termText();
+                final Token reusableToken = new Token();
+		for (Token nextToken = ts.next(reusableToken); nextToken != null; nextToken = ts.next(reusableToken)) {
+			String word = nextToken.term();
 			if ( already.add( word))
 				top.add( word);
 		}
Index: contrib/wordnet/src/java/org/apache/lucene/wordnet/SynExpand.java
===================================================================
--- contrib/wordnet/src/java/org/apache/lucene/wordnet/SynExpand.java	(revision 686801)
+++ contrib/wordnet/src/java/org/apache/lucene/wordnet/SynExpand.java	(working copy)
@@ -17,16 +17,30 @@
  * limitations under the License.
  */
 
-import org.apache.lucene.store.*;
-import org.apache.lucene.search.*;
-import org.apache.lucene.index.*;
-import org.apache.lucene.document.*;
-import org.apache.lucene.analysis.*;
-import org.apache.lucene.analysis.standard.*;
-import java.io.*;
-import java.util.*;
+import java.io.IOException;
+import java.io.StringReader;
+import java.util.HashSet;
+import java.util.Iterator;
+import java.util.LinkedList;
+import java.util.List;
+import java.util.Set;
 
+import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.analysis.Token;
+import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.analysis.standard.StandardAnalyzer;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.index.Term;
+import org.apache.lucene.search.BooleanClause;
+import org.apache.lucene.search.BooleanQuery;
+import org.apache.lucene.search.Hits;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.Searcher;
+import org.apache.lucene.search.TermQuery;
+import org.apache.lucene.store.FSDirectory;
 
+
 /**
  * Expand a query by looking up synonyms for every term.
  * You need to invoke {@link Syns2Index} first to build the synonym index.
@@ -99,10 +113,10 @@
 
 		// [1] Parse query into separate words so that when we expand we can avoid dups
 		TokenStream ts = a.tokenStream( field, new StringReader( query));
-		org.apache.lucene.analysis.Token t;
-		while ( (t = ts.next()) != null)
-		{
-			String word = t.termText();
+                
+                final Token reusableToken = new Token();
+		for (Token nextToken = ts.next(reusableToken); nextToken != null; nextToken = ts.next(reusableToken)) {
+			String word = nextToken.term();
 			if ( already.add( word))
 				top.add( word);
 		}
Index: contrib/instantiated/src/test/org/apache/lucene/store/instantiated/TestIndicesEquals.java
===================================================================
--- contrib/instantiated/src/test/org/apache/lucene/store/instantiated/TestIndicesEquals.java	(revision 686801)
+++ contrib/instantiated/src/test/org/apache/lucene/store/instantiated/TestIndicesEquals.java	(working copy)
@@ -15,19 +15,32 @@
  * limitations under the License.
  */
 
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.Comparator;
+import java.util.Iterator;
+import java.util.List;
+
 import junit.framework.TestCase;
+
 import org.apache.lucene.analysis.Token;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.standard.StandardAnalyzer;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
-import org.apache.lucene.index.*;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.index.Payload;
+import org.apache.lucene.index.Term;
+import org.apache.lucene.index.TermDocs;
+import org.apache.lucene.index.TermEnum;
+import org.apache.lucene.index.TermFreqVector;
+import org.apache.lucene.index.TermPositionVector;
+import org.apache.lucene.index.TermPositions;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.RAMDirectory;
 
-import java.io.IOException;
-import java.util.*;
-
 /**
  * Asserts equality of content and behaviour of two index readers.
  */
@@ -151,21 +164,24 @@
             document.add(f);
             if (i > 4) {
               final List<Token> tokens = new ArrayList<Token>(2);
-              Token t = new Token("the", 0, 2, "text");
+              Token t = createToken("the", 0, 2, "text");
               t.setPayload(new Payload(new byte[]{1, 2, 3}));
               tokens.add(t);
-              t = new Token("end", 3, 5, "text");
+              t = createToken("end", 3, 5, "text");
               t.setPayload(new Payload(new byte[]{2}));
               tokens.add(t);
-              tokens.add(new Token("fin", 7, 9));
+              tokens.add(createToken("fin", 7, 9));
               document.add(new Field("f", new TokenStream() {
                 Iterator<Token> it = tokens.iterator();
 
-                public Token next() throws IOException {
+                public Token next(final Token reusableToken) throws IOException {
+                  assert reusableToken != null;
                   if (!it.hasNext()) {
                     return null;
                   }
-                  return it.next();
+                  // Resettable token streams need to return clones.
+                  Token nextToken = (Token) it.next();
+                  return (Token) nextToken.clone();
                 }
 
                 public void reset() throws IOException {
@@ -466,4 +482,19 @@
     testReader.close();
   }
 
+  private static Token createToken(String term, int start, int offset)
+  {
+    Token token = new Token(start, offset);
+    token.setTermBuffer(term);
+    return token;
+  }
+
+  private static Token createToken(String term, int start, int offset, String type)
+  {
+    Token token = new Token(start, offset, type);
+    token.setTermBuffer(term);
+    return token;
+  }
+
+
 }
Index: contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedIndexWriter.java
===================================================================
--- contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedIndexWriter.java	(revision 686801)
+++ contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedIndexWriter.java	(working copy)
@@ -520,12 +520,10 @@
           } else {
             tokenStream = analyzer.tokenStream(field.name(), new StringReader(field.stringValue()));
           }
-          Token next = tokenStream.next();
 
-          while (next != null) {
-            next.setTermText(next.termText().intern()); // todo: not sure this needs to be interned?
-            tokens.add(next); // the vector will be built on commit.
-            next = tokenStream.next();
+          final Token reusableToken = new Token();
+          for (Token nextToken = tokenStream.next(reusableToken); nextToken != null; nextToken = tokenStream.next(reusableToken)) {
+            tokens.add((Token) nextToken.clone()); // the vector will be built on commit.
             fieldSetting.fieldLength++;
             if (fieldSetting.fieldLength > maxFieldLength) {
               break;
@@ -533,7 +531,10 @@
           }
         } else {
           // untokenized
-          tokens.add(new Token(field.stringValue().intern(), 0, field.stringValue().length(), "untokenized"));
+          String fieldVal = field.stringValue();
+          Token token = new Token(0, fieldVal.length(), "untokenized");
+          token.setTermBuffer(fieldVal);
+          tokens.add(token);
           fieldSetting.fieldLength++;
         }
       }
@@ -567,10 +568,10 @@
 
       for (Token token : eField_Tokens.getValue()) {
 
-        TermDocumentInformationFactory termDocumentInformationFactory = termDocumentInformationFactoryByTermText.get(token.termText());
+        TermDocumentInformationFactory termDocumentInformationFactory = termDocumentInformationFactoryByTermText.get(token.term());
         if (termDocumentInformationFactory == null) {
           termDocumentInformationFactory = new TermDocumentInformationFactory();
-          termDocumentInformationFactoryByTermText.put(token.termText(), termDocumentInformationFactory);
+          termDocumentInformationFactoryByTermText.put(token.term(), termDocumentInformationFactory);
         }
         //termDocumentInformationFactory.termFrequency++;
 
Index: contrib/xml-query-parser/src/java/org/apache/lucene/xmlparser/builders/TermsFilterBuilder.java
===================================================================
--- contrib/xml-query-parser/src/java/org/apache/lucene/xmlparser/builders/TermsFilterBuilder.java	(revision 686801)
+++ contrib/xml-query-parser/src/java/org/apache/lucene/xmlparser/builders/TermsFilterBuilder.java	(working copy)
@@ -59,20 +59,18 @@
 
 		try
 		{
-			Token token = ts.next();
+                  final Token reusableToken = new Token();
 			Term term = null;
-			while (token != null)
-			{
+	                for (Token nextToken = ts.next(reusableToken); nextToken != null; nextToken = ts.next(reusableToken)) {
 				if (term == null)
 				{
-					term = new Term(fieldName, token.termText());
+					term = new Term(fieldName, nextToken.term());
 				} else
 				{
 //					 create from previous to save fieldName.intern overhead
-					term = term.createTerm(token.termText()); 
+					term = term.createTerm(nextToken.term()); 
 				}
 				tf.addTerm(term);
-				token = ts.next();
 			}
 		} 
 		catch (IOException ioe)
Index: contrib/xml-query-parser/src/java/org/apache/lucene/xmlparser/builders/LikeThisQueryBuilder.java
===================================================================
--- contrib/xml-query-parser/src/java/org/apache/lucene/xmlparser/builders/LikeThisQueryBuilder.java	(revision 686801)
+++ contrib/xml-query-parser/src/java/org/apache/lucene/xmlparser/builders/LikeThisQueryBuilder.java	(working copy)
@@ -74,16 +74,14 @@
 		if((stopWords!=null)&&(fields!=null))
 		{
 		    stopWordsSet=new HashSet();
+                    final Token reusableToken = new Token();
 		    for (int i = 0; i < fields.length; i++)
             {
                 TokenStream ts = analyzer.tokenStream(fields[i],new StringReader(stopWords));
                 try
                 {
-	                Token stopToken=ts.next();
-	                while(stopToken!=null)
-	                {
-	                    stopWordsSet.add(stopToken.termText());
-	                    stopToken=ts.next();
+	                for (Token nextToken = ts.next(reusableToken); nextToken != null; nextToken = ts.next(reusableToken)) {
+	                    stopWordsSet.add(nextToken.term());
 	                }
                 }
                 catch(IOException ioe)
Index: contrib/xml-query-parser/src/java/org/apache/lucene/xmlparser/builders/SpanOrTermsBuilder.java
===================================================================
--- contrib/xml-query-parser/src/java/org/apache/lucene/xmlparser/builders/SpanOrTermsBuilder.java	(revision 686801)
+++ contrib/xml-query-parser/src/java/org/apache/lucene/xmlparser/builders/SpanOrTermsBuilder.java	(working copy)
@@ -52,12 +52,10 @@
 		{
 			ArrayList clausesList=new ArrayList();
 			TokenStream ts=analyzer.tokenStream(fieldName,new StringReader(value));
-			Token token=ts.next();
-			while(token!=null)
-			{
-			    SpanTermQuery stq=new SpanTermQuery(new Term(fieldName,token.termText()));
+			final Token reusableToken = new Token();
+	                for (Token nextToken = ts.next(reusableToken); nextToken != null; nextToken = ts.next(reusableToken)) {
+			    SpanTermQuery stq=new SpanTermQuery(new Term(fieldName,nextToken.term()));
 			    clausesList.add(stq);
-				token=ts.next();		    
 			}
 			SpanOrQuery soq=new SpanOrQuery((SpanQuery[]) clausesList.toArray(new SpanQuery[clausesList.size()]));
 			soq.setBoost(DOMUtils.getAttribute(e,"boost",1.0f));
Index: contrib/xml-query-parser/src/java/org/apache/lucene/xmlparser/builders/TermsQueryBuilder.java
===================================================================
--- contrib/xml-query-parser/src/java/org/apache/lucene/xmlparser/builders/TermsQueryBuilder.java	(revision 686801)
+++ contrib/xml-query-parser/src/java/org/apache/lucene/xmlparser/builders/TermsQueryBuilder.java	(working copy)
@@ -58,20 +58,18 @@
 		TokenStream ts = analyzer.tokenStream(fieldName, new StringReader(text));
 		try
 		{
-			Token token = ts.next();
+                  final Token reusableToken = new Token();
 			Term term = null;
-			while (token != null)
-			{
+	                for (Token nextToken = ts.next(reusableToken); nextToken != null; nextToken = ts.next(reusableToken)) {
 				if (term == null)
 				{
-					term = new Term(fieldName, token.termText());
+					term = new Term(fieldName, nextToken.term());
 				} else
 				{
 //					 create from previous to save fieldName.intern overhead
-					term = term.createTerm(token.termText()); 
+					term = term.createTerm(nextToken.term()); 
 				}
 				bq.add(new BooleanClause(new TermQuery(term),BooleanClause.Occur.SHOULD));
-				token = ts.next();
 			}
 		} 
 		catch (IOException ioe)
Index: contrib/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterTest.java
===================================================================
--- contrib/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterTest.java	(revision 686801)
+++ contrib/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterTest.java	(working copy)
@@ -1120,21 +1120,22 @@
       {
         lst = new ArrayList();
         Token t;
-        t = new Token("hi", 0, 2);
+        t = createToken("hi", 0, 2);
         lst.add(t);
-        t = new Token("hispeed", 0, 8);
+        t = createToken("hispeed", 0, 8);
         lst.add(t);
-        t = new Token("speed", 3, 8);
+        t = createToken("speed", 3, 8);
         t.setPositionIncrement(0);
         lst.add(t);
-        t = new Token("10", 8, 10);
+        t = createToken("10", 8, 10);
         lst.add(t);
-        t = new Token("foo", 11, 14);
+        t = createToken("foo", 11, 14);
         lst.add(t);
         iter = lst.iterator();
       }
 
-      public Token next() throws IOException {
+      public Token next(final Token reusableToken) throws IOException {
+        assert reusableToken != null;
         return iter.hasNext() ? (Token) iter.next() : null;
       }
     };
@@ -1149,21 +1150,22 @@
       {
         lst = new ArrayList();
         Token t;
-        t = new Token("hispeed", 0, 8);
+        t = createToken("hispeed", 0, 8);
         lst.add(t);
-        t = new Token("hi", 0, 2);
+        t = createToken("hi", 0, 2);
         t.setPositionIncrement(0);
         lst.add(t);
-        t = new Token("speed", 3, 8);
+        t = createToken("speed", 3, 8);
         lst.add(t);
-        t = new Token("10", 8, 10);
+        t = createToken("10", 8, 10);
         lst.add(t);
-        t = new Token("foo", 11, 14);
+        t = createToken("foo", 11, 14);
         lst.add(t);
         iter = lst.iterator();
       }
 
-      public Token next() throws IOException {
+      public Token next(final Token reusableToken) throws IOException {
+        assert reusableToken != null;
         return iter.hasNext() ? (Token) iter.next() : null;
       }
     };
@@ -1346,6 +1348,13 @@
     super.tearDown();
   }
 
+  private static Token createToken(String term, int start, int offset)
+  {
+    Token token = new Token(start, offset);
+    token.setTermBuffer(term);
+    return token;
+  }
+
 }
 
 // ===================================================================
@@ -1392,31 +1401,32 @@
     this.synonyms = synonyms;
   }
 
-  public Token next() throws IOException {
+  public Token next(final Token reusableToken) throws IOException {
+    assert reusableToken != null;
     if (currentRealToken == null) {
-      Token nextRealToken = realStream.next();
+      Token nextRealToken = realStream.next(reusableToken);
       if (nextRealToken == null) {
         return null;
       }
-      String expansions = (String) synonyms.get(nextRealToken.termText());
+      String expansions = (String) synonyms.get(nextRealToken.term());
       if (expansions == null) {
         return nextRealToken;
       }
       st = new StringTokenizer(expansions, ",");
       if (st.hasMoreTokens()) {
-        currentRealToken = nextRealToken;
+        currentRealToken = (Token) nextRealToken.clone();
       }
       return currentRealToken;
     } else {
-      String nextExpandedValue = st.nextToken();
-      Token expandedToken = new Token(nextExpandedValue, currentRealToken.startOffset(),
-          currentRealToken.endOffset());
-      expandedToken.setPositionIncrement(0);
+      reusableToken.reinit(st.nextToken(),
+                           currentRealToken.startOffset(),
+                           currentRealToken.endOffset());
+      reusableToken.setPositionIncrement(0);
       if (!st.hasMoreTokens()) {
         currentRealToken = null;
         st = null;
       }
-      return expandedToken;
+      return reusableToken;
     }
   }
 
Index: contrib/highlighter/src/java/org/apache/lucene/search/highlight/SpanScorer.java
===================================================================
--- contrib/highlighter/src/java/org/apache/lucene/search/highlight/SpanScorer.java	(revision 686801)
+++ contrib/highlighter/src/java/org/apache/lucene/search/highlight/SpanScorer.java	(working copy)
@@ -121,7 +121,7 @@
    */
   public float getTokenScore(Token token) {
     position += token.getPositionIncrement();
-    String termText = new String(token.termBuffer(), 0, token.termLength());
+    String termText = token.term();
 
     WeightedSpanTerm weightedSpanTerm;
 
Index: contrib/highlighter/src/java/org/apache/lucene/search/highlight/QueryScorer.java
===================================================================
--- contrib/highlighter/src/java/org/apache/lucene/search/highlight/QueryScorer.java	(revision 686801)
+++ contrib/highlighter/src/java/org/apache/lucene/search/highlight/QueryScorer.java	(working copy)
@@ -106,7 +106,7 @@
 	 */
 	public float getTokenScore(Token token)
 	{
-		String termText=token.termText();
+		String termText=token.term();
 		
 		WeightedTerm queryTerm=(WeightedTerm) termsToFind.get(termText);
 		if(queryTerm==null)
Index: contrib/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources.java
===================================================================
--- contrib/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources.java	(revision 686801)
+++ contrib/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources.java	(working copy)
@@ -147,8 +147,9 @@
             {
                 this.tokens=tokens;
             }
-            public Token next()
+            public Token next(final Token reusableToken)
             {
+                assert reusableToken != null;
                 if(currentToken>=tokens.length)
                 {
                     return null;
@@ -160,6 +161,7 @@
         String[] terms=tpv.getTerms();          
         int[] freq=tpv.getTermFrequencies();
         int totalTokens=0;
+        Token newToken = new Token();
         for (int t = 0; t < freq.length; t++)
         {
             totalTokens+=freq[t];
@@ -189,9 +191,8 @@
                 }
                 for (int tp = 0; tp < offsets.length; tp++)
                 {
-                    unsortedTokens.add(new Token(terms[t],
-                        offsets[tp].getStartOffset(),
-                        offsets[tp].getEndOffset()));
+                  newToken.reinit(terms[t], offsets[tp].getStartOffset(), offsets[tp].getEndOffset());
+                  unsortedTokens.add(newToken.clone());
                 }
             }
             else
@@ -204,9 +205,8 @@
                 //tokens stored with positions - can use this to index straight into sorted array
                 for (int tp = 0; tp < pos.length; tp++)
                 {
-                    tokensInOriginalOrder[pos[tp]]=new Token(terms[t],
-                            offsets[tp].getStartOffset(),
-                            offsets[tp].getEndOffset());
+                  newToken.reinit(terms[t], offsets[tp].getStartOffset(), offsets[tp].getEndOffset());
+                  tokensInOriginalOrder[pos[tp]] = (Token) newToken.clone();
                 }                
             }
         }
@@ -261,7 +261,7 @@
 		}
         return getTokenStream(field, contents, analyzer);
   }
-  //conevenience method
+  //convenience method
   public static TokenStream getTokenStream(String field, String contents, Analyzer analyzer){
     return analyzer.tokenStream(field,new StringReader(contents));
   }
Index: contrib/highlighter/src/java/org/apache/lucene/search/highlight/SimpleSpanFragmenter.java
===================================================================
--- contrib/highlighter/src/java/org/apache/lucene/search/highlight/SimpleSpanFragmenter.java	(revision 686801)
+++ contrib/highlighter/src/java/org/apache/lucene/search/highlight/SimpleSpanFragmenter.java	(working copy)
@@ -62,7 +62,7 @@
       return false;
     }
 
-    WeightedSpanTerm wSpanTerm = spanScorer.getWeightedSpanTerm(new String(token.termBuffer(), 0, token.termLength()));
+    WeightedSpanTerm wSpanTerm = spanScorer.getWeightedSpanTerm(token.term());
 
     if (wSpanTerm != null) {
       List positionSpans = wSpanTerm.getPositionSpans();
Index: contrib/highlighter/src/java/org/apache/lucene/search/highlight/TokenGroup.java
===================================================================
--- contrib/highlighter/src/java/org/apache/lucene/search/highlight/TokenGroup.java	(revision 686801)
+++ contrib/highlighter/src/java/org/apache/lucene/search/highlight/TokenGroup.java	(working copy)
@@ -61,7 +61,7 @@
           tot+=score;
         }
       }
-			tokens[numTokens]=token;
+			tokens[numTokens]= (Token) token.clone();
 			scores[numTokens]=score;
 			numTokens++;
         }
Index: contrib/highlighter/src/java/org/apache/lucene/search/highlight/Highlighter.java
===================================================================
--- contrib/highlighter/src/java/org/apache/lucene/search/highlight/Highlighter.java	(revision 686801)
+++ contrib/highlighter/src/java/org/apache/lucene/search/highlight/Highlighter.java	(working copy)
@@ -22,6 +22,7 @@
 import java.util.Iterator;
 
 import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.analysis.Token;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.util.PriorityQueue;
 
@@ -217,7 +218,7 @@
 
 		try
 		{
-			org.apache.lucene.analysis.Token token;
+                  final Token reusableToken = new Token();
 			String tokenText;
 			int startOffset;
 			int endOffset;
@@ -225,10 +226,12 @@
 			textFragmenter.start(text);
 
 			TokenGroup tokenGroup=new TokenGroup();
-			token = tokenStream.next();
-			while ((token!= null)&&(token.startOffset()< maxDocCharsToAnalyze))
+			
+			for (Token nextToken = tokenStream.next(reusableToken);
+			     (nextToken!= null)&&(nextToken.startOffset()< maxDocCharsToAnalyze);
+			     nextToken = tokenStream.next(reusableToken))
 			{
-				if((tokenGroup.numTokens>0)&&(tokenGroup.isDistinct(token)))
+				if((tokenGroup.numTokens>0)&&(tokenGroup.isDistinct(nextToken)))
 				{
 					//the current token is distinct from previous tokens -
 					// markup the cached token group info
@@ -244,7 +247,7 @@
 					tokenGroup.clear();
 
 					//check if current token marks the start of a new fragment
-					if(textFragmenter.isNewFragment(token))
+					if(textFragmenter.isNewFragment(nextToken))
 					{
 						currentFrag.setScore(fragmentScorer.getFragmentScore());
 						//record stats for a new fragment
@@ -255,13 +258,12 @@
 					}
 				}
 
-				tokenGroup.addToken(token,fragmentScorer.getTokenScore(token));
+				tokenGroup.addToken(nextToken,fragmentScorer.getTokenScore(nextToken));
 
 //				if(lastEndOffset>maxDocBytesToAnalyze)
 //				{
 //					break;
 //				}
-				token = tokenStream.next();
 			}
 			currentFrag.setScore(fragmentScorer.getFragmentScore());
 
Index: contrib/miscellaneous/src/test/org/apache/lucene/queryParser/precedence/TestPrecedenceQueryParser.java
===================================================================
--- contrib/miscellaneous/src/test/org/apache/lucene/queryParser/precedence/TestPrecedenceQueryParser.java	(revision 686801)
+++ contrib/miscellaneous/src/test/org/apache/lucene/queryParser/precedence/TestPrecedenceQueryParser.java	(working copy)
@@ -57,19 +57,26 @@
     boolean inPhrase = false;
     int savedStart = 0, savedEnd = 0;
 
-    public Token next() throws IOException {
+    public Token next(final Token reusableToken) throws IOException {
+      assert reusableToken != null;
       if (inPhrase) {
         inPhrase = false;
-        return new Token("phrase2", savedStart, savedEnd);
+        reusableToken.setTermBuffer("phrase2");
+        reusableToken.setStartOffset(savedStart);
+        reusableToken.setEndOffset(savedEnd);
+        return reusableToken;
       } else
-        for (Token token = input.next(); token != null; token = input.next()) {
-          if (token.termText().equals("phrase")) {
+        for (Token nextToken = input.next(reusableToken); nextToken != null; nextToken = input.next(reusableToken)) {
+          if (nextToken.term().equals("phrase")) {
             inPhrase = true;
-            savedStart = token.startOffset();
-            savedEnd = token.endOffset();
-            return new Token("phrase1", savedStart, savedEnd);
-          } else if (!token.termText().equals("stop"))
-            return token;
+            savedStart = nextToken.startOffset();
+            savedEnd = nextToken.endOffset();
+            nextToken.setTermBuffer("phrase1");
+            nextToken.setStartOffset(savedStart);
+            nextToken.setEndOffset(savedEnd);
+            return nextToken;
+          } else if (!nextToken.term().equals("stop"))
+            return nextToken;
         }
       return null;
     }
Index: contrib/miscellaneous/src/java/org/apache/lucene/queryParser/analyzing/AnalyzingQueryParser.java
===================================================================
--- contrib/miscellaneous/src/java/org/apache/lucene/queryParser/analyzing/AnalyzingQueryParser.java	(revision 686801)
+++ contrib/miscellaneous/src/java/org/apache/lucene/queryParser/analyzing/AnalyzingQueryParser.java	(working copy)
@@ -23,6 +23,7 @@
 import java.util.List;
 
 import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.analysis.Token;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.queryParser.ParseException;
 import org.apache.lucene.search.Query;
@@ -105,21 +106,23 @@
 
     // get Analyzer from superclass and tokenize the term
     TokenStream source = getAnalyzer().tokenStream(field, new StringReader(termStr));
-    org.apache.lucene.analysis.Token t;
+    final Token reusableToken = new Token();
+    Token nextToken;
 
     int countTokens = 0;
     while (true) {
       try {
-        t = source.next();
+        nextToken = source.next(reusableToken);
       } catch (IOException e) {
-        t = null;
+        nextToken = null;
       }
-      if (t == null) {
+      if (nextToken == null) {
         break;
       }
-      if (!"".equals(t.termText())) {
+      String term = nextToken.term();
+      if (!"".equals(term)) {
         try {
-          tlist.set(countTokens++, t.termText());
+          tlist.set(countTokens++, term);
         } catch (IndexOutOfBoundsException ioobe) {
           countTokens = -1;
         }
@@ -189,18 +192,19 @@
     // get Analyzer from superclass and tokenize the term
     TokenStream source = getAnalyzer().tokenStream(field, new StringReader(termStr));
     List tlist = new ArrayList();
-    org.apache.lucene.analysis.Token t;
+    final Token reusableToken = new Token();
+    Token nextToken;
 
     while (true) {
       try {
-        t = source.next();
+        nextToken = source.next(reusableToken);
       } catch (IOException e) {
-        t = null;
+        nextToken = null;
       }
-      if (t == null) {
+      if (nextToken == null) {
         break;
       }
-      tlist.add(t.termText());
+      tlist.add(nextToken.term());
     }
 
     try {
@@ -238,14 +242,15 @@
       throws ParseException {
     // get Analyzer from superclass and tokenize the term
     TokenStream source = getAnalyzer().tokenStream(field, new StringReader(termStr));
-    org.apache.lucene.analysis.Token t;
+    final Token reusableToken = new Token();
+    Token nextToken;
     boolean multipleTokens = false;
 
     try {
-      t = source.next();
-      multipleTokens = source.next() != null;
+      nextToken = source.next(reusableToken);
+      multipleTokens = source.next(reusableToken) != null;
     } catch (IOException e) {
-      t = null;
+      nextToken = null;
     }
 
     try {
@@ -259,7 +264,7 @@
           + " - tokens were added");
     }
 
-    return (t == null) ? null : super.getFuzzyQuery(field, t.termText(), minSimilarity);
+    return (nextToken == null) ? null : super.getFuzzyQuery(field, nextToken.term(), minSimilarity);
   }
 
   /**
@@ -270,18 +275,20 @@
       throws ParseException {
     // get Analyzer from superclass and tokenize the terms
     TokenStream source = getAnalyzer().tokenStream(field, new StringReader(part1));
-    org.apache.lucene.analysis.Token t;
+    final Token reusableToken = new Token();
+    Token nextToken;
+    Token multipleToken;
     boolean multipleTokens = false;
 
     // part1
     try {
-      t = source.next();
-      if (t != null) {
-        part1 = t.termText();
+      nextToken = source.next(reusableToken);
+      if (nextToken != null) {
+        part1 = nextToken.term();
       }
-      multipleTokens = source.next() != null;
+      multipleTokens = source.next(reusableToken) != null;
     } catch (IOException e) {
-      t = null;
+      nextToken = null;
     }
     try {
       source.close();
@@ -293,16 +300,16 @@
           + " - tokens were added to part1");
     }
 
+    // part2
     source = getAnalyzer().tokenStream(field, new StringReader(part2));
-    // part2
     try {
-      t = source.next();
-      if (t != null) {
-        part2 = t.termText();
+      nextToken = source.next(reusableToken);
+      if (nextToken != null) {
+        part2 = nextToken.term();
       }
-      multipleTokens = source.next() != null;
+      multipleTokens = source.next(reusableToken) != null;
     } catch (IOException e) {
-      t = null;
+      nextToken = null;
     }
     try {
       source.close();
Index: contrib/miscellaneous/src/java/org/apache/lucene/queryParser/precedence/PrecedenceQueryParser.java
===================================================================
--- contrib/miscellaneous/src/java/org/apache/lucene/queryParser/precedence/PrecedenceQueryParser.java	(revision 686801)
+++ contrib/miscellaneous/src/java/org/apache/lucene/queryParser/precedence/PrecedenceQueryParser.java	(working copy)
@@ -1,14 +1,29 @@
 /* Generated By:JavaCC: Do not edit this line. PrecedenceQueryParser.java */
 package org.apache.lucene.queryParser.precedence;
 
+import java.io.IOException;
+import java.io.StringReader;
+import java.text.DateFormat;
+import java.util.ArrayList;
+import java.util.Date;
+import java.util.List;
+import java.util.Locale;
 import java.util.Vector;
-import java.io.*;
-import java.text.*;
-import java.util.*;
+
+import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.document.DateTools;
 import org.apache.lucene.index.Term;
-import org.apache.lucene.analysis.*;
-import org.apache.lucene.document.*;
-import org.apache.lucene.search.*;
+import org.apache.lucene.search.BooleanClause;
+import org.apache.lucene.search.BooleanQuery;
+import org.apache.lucene.search.FuzzyQuery;
+import org.apache.lucene.search.MultiPhraseQuery;
+import org.apache.lucene.search.PhraseQuery;
+import org.apache.lucene.search.PrefixQuery;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.RangeQuery;
+import org.apache.lucene.search.TermQuery;
+import org.apache.lucene.search.WildcardQuery;
 import org.apache.lucene.util.Parameter;
 
 /**
@@ -296,21 +311,22 @@
 
     TokenStream source = analyzer.tokenStream(field, new StringReader(queryText));
     Vector v = new Vector();
-    org.apache.lucene.analysis.Token t;
+    final org.apache.lucene.analysis.Token reusableToken = new org.apache.lucene.analysis.Token();
+    org.apache.lucene.analysis.Token nextToken;
     int positionCount = 0;
     boolean severalTokensAtSamePosition = false;
 
     while (true) {
       try {
-        t = source.next();
+        nextToken = source.next(reusableToken);
       }
       catch (IOException e) {
-        t = null;
+        nextToken = null;
       }
-      if (t == null)
+      if (nextToken == null)
         break;
-      v.addElement(t);
-      if (t.getPositionIncrement() == 1)
+      v.addElement(nextToken.clone());
+      if (nextToken.getPositionIncrement() == 1)
         positionCount++;
       else
         severalTokensAtSamePosition = true;
@@ -325,17 +341,17 @@
     if (v.size() == 0)
       return null;
     else if (v.size() == 1) {
-      t = (org.apache.lucene.analysis.Token) v.elementAt(0);
-      return new TermQuery(new Term(field, t.termText()));
+      nextToken = (org.apache.lucene.analysis.Token) v.elementAt(0);
+      return new TermQuery(new Term(field, nextToken.term()));
     } else {
       if (severalTokensAtSamePosition) {
         if (positionCount == 1) {
           // no phrase query:
           BooleanQuery q = new BooleanQuery();
           for (int i = 0; i < v.size(); i++) {
-            t = (org.apache.lucene.analysis.Token) v.elementAt(i);
+            nextToken = (org.apache.lucene.analysis.Token) v.elementAt(i);
             TermQuery currentQuery = new TermQuery(
-                new Term(field, t.termText()));
+                new Term(field, nextToken.term()));
             q.add(currentQuery, BooleanClause.Occur.SHOULD);
           }
           return q;
@@ -345,12 +361,12 @@
           MultiPhraseQuery mpq = new MultiPhraseQuery();
           List multiTerms = new ArrayList();
           for (int i = 0; i < v.size(); i++) {
-            t = (org.apache.lucene.analysis.Token) v.elementAt(i);
-            if (t.getPositionIncrement() == 1 && multiTerms.size() > 0) {
+            nextToken = (org.apache.lucene.analysis.Token) v.elementAt(i);
+            if (nextToken.getPositionIncrement() == 1 && multiTerms.size() > 0) {
               mpq.add((Term[])multiTerms.toArray(new Term[0]));
               multiTerms.clear();
             }
-            multiTerms.add(new Term(field, t.termText()));
+            multiTerms.add(new Term(field, nextToken.term()));
           }
           mpq.add((Term[])multiTerms.toArray(new Term[0]));
           return mpq;
@@ -361,7 +377,7 @@
         q.setSlop(phraseSlop);
         for (int i = 0; i < v.size(); i++) {
           q.add(new Term(field, ((org.apache.lucene.analysis.Token)
-              v.elementAt(i)).termText()));
+              v.elementAt(i)).term()));
 
         }
         return q;
Index: contrib/miscellaneous/src/java/org/apache/lucene/queryParser/precedence/PrecedenceQueryParser.jj
===================================================================
--- contrib/miscellaneous/src/java/org/apache/lucene/queryParser/precedence/PrecedenceQueryParser.jj	(revision 686801)
+++ contrib/miscellaneous/src/java/org/apache/lucene/queryParser/precedence/PrecedenceQueryParser.jj	(working copy)
@@ -25,14 +25,29 @@
 
 package org.apache.lucene.queryParser.precedence;
 
+import java.io.IOException;
+import java.io.StringReader;
+import java.text.DateFormat;
+import java.util.ArrayList;
+import java.util.Date;
+import java.util.List;
+import java.util.Locale;
 import java.util.Vector;
-import java.io.*;
-import java.text.*;
-import java.util.*;
+
+import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.document.DateTools;
 import org.apache.lucene.index.Term;
-import org.apache.lucene.analysis.*;
-import org.apache.lucene.document.*;
-import org.apache.lucene.search.*;
+import org.apache.lucene.search.BooleanClause;
+import org.apache.lucene.search.BooleanQuery;
+import org.apache.lucene.search.FuzzyQuery;
+import org.apache.lucene.search.MultiPhraseQuery;
+import org.apache.lucene.search.PhraseQuery;
+import org.apache.lucene.search.PrefixQuery;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.RangeQuery;
+import org.apache.lucene.search.TermQuery;
+import org.apache.lucene.search.WildcardQuery;
 import org.apache.lucene.util.Parameter;
 
 /**
@@ -320,21 +335,22 @@
 
     TokenStream source = analyzer.tokenStream(field, new StringReader(queryText));
     Vector v = new Vector();
-    org.apache.lucene.analysis.Token t;
+    final org.apache.lucene.analysis.Token reusableToken = new org.apache.lucene.analysis.Token();
+    org.apache.lucene.analysis.Token nextToken;
     int positionCount = 0;
     boolean severalTokensAtSamePosition = false;
 
     while (true) {
       try {
-        t = source.next();
+        nextToken = source.next(reusableToken);
       }
       catch (IOException e) {
-        t = null;
+        nextToken = null;
       }
-      if (t == null)
+      if (nextToken == null)
         break;
-      v.addElement(t);
-      if (t.getPositionIncrement() == 1)
+      v.addElement(nextToken.clone());
+      if (nextToken.getPositionIncrement() == 1)
         positionCount++;
       else
         severalTokensAtSamePosition = true;
@@ -349,17 +365,17 @@
     if (v.size() == 0)
       return null;
     else if (v.size() == 1) {
-      t = (org.apache.lucene.analysis.Token) v.elementAt(0);
-      return new TermQuery(new Term(field, t.termText()));
+      nextToken = (org.apache.lucene.analysis.Token) v.elementAt(0);
+      return new TermQuery(new Term(field, nextToken.term()));
     } else {
       if (severalTokensAtSamePosition) {
         if (positionCount == 1) {
           // no phrase query:
           BooleanQuery q = new BooleanQuery();
           for (int i = 0; i < v.size(); i++) {
-            t = (org.apache.lucene.analysis.Token) v.elementAt(i);
+            nextToken = (org.apache.lucene.analysis.Token) v.elementAt(i);
             TermQuery currentQuery = new TermQuery(
-                new Term(field, t.termText()));
+                new Term(field, nextToken.term()));
             q.add(currentQuery, BooleanClause.Occur.SHOULD);
           }
           return q;
@@ -369,12 +385,12 @@
           MultiPhraseQuery mpq = new MultiPhraseQuery();
           List multiTerms = new ArrayList();
           for (int i = 0; i < v.size(); i++) {
-            t = (org.apache.lucene.analysis.Token) v.elementAt(i);
-            if (t.getPositionIncrement() == 1 && multiTerms.size() > 0) {
+            nextToken = (org.apache.lucene.analysis.Token) v.elementAt(i);
+            if (nextToken.getPositionIncrement() == 1 && multiTerms.size() > 0) {
               mpq.add((Term[])multiTerms.toArray(new Term[0]));
               multiTerms.clear();
             }
-            multiTerms.add(new Term(field, t.termText()));
+            multiTerms.add(new Term(field, nextToken.term()));
           }
           mpq.add((Term[])multiTerms.toArray(new Term[0]));
           return mpq;
@@ -385,7 +401,7 @@
         q.setSlop(phraseSlop);
         for (int i = 0; i < v.size(); i++) {
           q.add(new Term(field, ((org.apache.lucene.analysis.Token) 
-              v.elementAt(i)).termText()));
+              v.elementAt(i)).term()));
 
         }
         return q;
Index: contrib/miscellaneous/src/java/org/apache/lucene/queryParser/precedence/CharStream.java
===================================================================
--- contrib/miscellaneous/src/java/org/apache/lucene/queryParser/precedence/CharStream.java	(revision 686801)
+++ contrib/miscellaneous/src/java/org/apache/lucene/queryParser/precedence/CharStream.java	(working copy)
@@ -26,6 +26,20 @@
   char readChar() throws java.io.IOException;
 
   /**
+   * Returns the column position of the character last read.
+   * @deprecated 
+   * @see #getEndColumn
+   */
+  int getColumn();
+
+  /**
+   * Returns the line number of the character last read.
+   * @deprecated 
+   * @see #getEndLine
+   */
+  int getLine();
+
+  /**
    * Returns the column number of the last character for current token (being
    * matched after the last call to BeginTOken).
    */
Index: contrib/miscellaneous/src/java/org/apache/lucene/queryParser/precedence/PrecedenceQueryParserTokenManager.java
===================================================================
--- contrib/miscellaneous/src/java/org/apache/lucene/queryParser/precedence/PrecedenceQueryParserTokenManager.java	(revision 686801)
+++ contrib/miscellaneous/src/java/org/apache/lucene/queryParser/precedence/PrecedenceQueryParserTokenManager.java	(working copy)
@@ -1,13 +1,27 @@
 /* Generated By:JavaCC: Do not edit this line. PrecedenceQueryParserTokenManager.java */
 package org.apache.lucene.queryParser.precedence;
+import java.io.IOException;
+import java.io.StringReader;
+import java.text.DateFormat;
+import java.util.ArrayList;
+import java.util.Date;
+import java.util.List;
+import java.util.Locale;
 import java.util.Vector;
-import java.io.*;
-import java.text.*;
-import java.util.*;
+import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.document.DateTools;
 import org.apache.lucene.index.Term;
-import org.apache.lucene.analysis.*;
-import org.apache.lucene.document.*;
-import org.apache.lucene.search.*;
+import org.apache.lucene.search.BooleanClause;
+import org.apache.lucene.search.BooleanQuery;
+import org.apache.lucene.search.FuzzyQuery;
+import org.apache.lucene.search.MultiPhraseQuery;
+import org.apache.lucene.search.PhraseQuery;
+import org.apache.lucene.search.PrefixQuery;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.RangeQuery;
+import org.apache.lucene.search.TermQuery;
+import org.apache.lucene.search.WildcardQuery;
 import org.apache.lucene.util.Parameter;
 
 public class PrecedenceQueryParserTokenManager implements PrecedenceQueryParserConstants
Index: contrib/wikipedia/src/test/org/apache/lucene/wikipedia/analysis/WikipediaTokenizerTest.java
===================================================================
--- contrib/wikipedia/src/test/org/apache/lucene/wikipedia/analysis/WikipediaTokenizerTest.java	(revision 686801)
+++ contrib/wikipedia/src/test/org/apache/lucene/wikipedia/analysis/WikipediaTokenizerTest.java	(working copy)
@@ -126,28 +126,28 @@
     tcm.put("3.25", "<NUM>");
     tcm.put("3.50", "<NUM>");
     WikipediaTokenizer tf = new WikipediaTokenizer(new StringReader(test));
-    Token token = new Token();
     int count = 0;
     int numItalics = 0;
     int numBoldItalics = 0;
     int numCategory = 0;
     int numCitation = 0;
-    while ((token = tf.next(token)) != null) {
-      String tokText = token.termText();
+    final Token reusableToken = new Token();
+    for (Token nextToken = tf.next(reusableToken); nextToken != null; nextToken = tf.next(reusableToken)) {
+      String tokText = nextToken.term();
       //System.out.println("Text: " + tokText + " Type: " + token.type());
-      assertTrue("token is null and it shouldn't be", token != null);
+      assertTrue("nextToken is null and it shouldn't be", nextToken != null);
       String expectedType = (String) tcm.get(tokText);
-      assertTrue("expectedType is null and it shouldn't be for: " + token, expectedType != null);
-      assertTrue(token.type() + " is not equal to " + expectedType + " for " + token, token.type().equals(expectedType) == true);
+      assertTrue("expectedType is null and it shouldn't be for: " + nextToken, expectedType != null);
+      assertTrue(nextToken.type() + " is not equal to " + expectedType + " for " + nextToken, nextToken.type().equals(expectedType) == true);
       count++;
-      if (token.type().equals(WikipediaTokenizer.ITALICS)  == true){
+      if (nextToken.type().equals(WikipediaTokenizer.ITALICS)  == true){
         numItalics++;
-      } else if (token.type().equals(WikipediaTokenizer.BOLD_ITALICS)  == true){
+      } else if (nextToken.type().equals(WikipediaTokenizer.BOLD_ITALICS)  == true){
         numBoldItalics++;
-      } else if (token.type().equals(WikipediaTokenizer.CATEGORY)  == true){
+      } else if (nextToken.type().equals(WikipediaTokenizer.CATEGORY)  == true){
         numCategory++;
       }
-      else if (token.type().equals(WikipediaTokenizer.CITATION)  == true){
+      else if (nextToken.type().equals(WikipediaTokenizer.CITATION)  == true){
         numCitation++;
       }
     }
@@ -166,105 +166,105 @@
   }
 
   private void checkLinkPhrases(WikipediaTokenizer tf) throws IOException {
-    Token token = new Token();
-    token = tf.next(token);
-    assertTrue("token is null and it shouldn't be", token != null);
-    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + " is not equal to " + "click", new String(token.termBuffer(), 0, token.termLength()).equals("click") == true);
-    assertTrue(token.getPositionIncrement() + " does not equal: " + 1, token.getPositionIncrement() == 1);
-    token = tf.next(token);
-    assertTrue("token is null and it shouldn't be", token != null);
-    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + " is not equal to " + "link", new String(token.termBuffer(), 0, token.termLength()).equals("link") == true);
-    assertTrue(token.getPositionIncrement() + " does not equal: " + 1, token.getPositionIncrement() == 1);
-    token = tf.next(token);
-    assertTrue("token is null and it shouldn't be", token != null);
-    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + " is not equal to " + "here",
-            new String(token.termBuffer(), 0, token.termLength()).equals("here") == true);
+    final Token reusableToken = new Token();
+    Token nextToken = tf.next(reusableToken);
+    assertTrue("nextToken is null and it shouldn't be", nextToken != null);
+    assertTrue(nextToken.term() + " is not equal to " + "click", nextToken.term().equals("click") == true);
+    assertTrue(nextToken.getPositionIncrement() + " does not equal: " + 1, nextToken.getPositionIncrement() == 1);
+    nextToken = tf.next(reusableToken);
+    assertTrue("nextToken is null and it shouldn't be", nextToken != null);
+    assertTrue(nextToken.term() + " is not equal to " + "link", nextToken.term().equals("link") == true);
+    assertTrue(nextToken.getPositionIncrement() + " does not equal: " + 1, nextToken.getPositionIncrement() == 1);
+    nextToken = tf.next(reusableToken);
+    assertTrue("nextToken is null and it shouldn't be", nextToken != null);
+    assertTrue(nextToken.term() + " is not equal to " + "here",
+            nextToken.term().equals("here") == true);
     //The link, and here should be at the same position for phrases to work
-    assertTrue(token.getPositionIncrement() + " does not equal: " + 1, token.getPositionIncrement() == 1);
-    token = tf.next(token);
-    assertTrue("token is null and it shouldn't be", token != null);
-    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + " is not equal to " + "again",
-            new String(token.termBuffer(), 0, token.termLength()).equals("again") == true);
-    assertTrue(token.getPositionIncrement() + " does not equal: " + 1, token.getPositionIncrement() == 1);
+    assertTrue(nextToken.getPositionIncrement() + " does not equal: " + 1, nextToken.getPositionIncrement() == 1);
+    nextToken = tf.next(reusableToken);
+    assertTrue("nextToken is null and it shouldn't be", nextToken != null);
+    assertTrue(nextToken.term() + " is not equal to " + "again",
+            nextToken.term().equals("again") == true);
+    assertTrue(nextToken.getPositionIncrement() + " does not equal: " + 1, nextToken.getPositionIncrement() == 1);
 
-    token = tf.next(token);
-    assertTrue("token is null and it shouldn't be", token != null);
-    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + " is not equal to " + "click",
-            new String(token.termBuffer(), 0, token.termLength()).equals("click") == true);
-    assertTrue(token.getPositionIncrement() + " does not equal: " + 1, token.getPositionIncrement() == 1);
+    nextToken = tf.next(reusableToken);
+    assertTrue("nextToken is null and it shouldn't be", nextToken != null);
+    assertTrue(nextToken.term() + " is not equal to " + "click",
+            nextToken.term().equals("click") == true);
+    assertTrue(nextToken.getPositionIncrement() + " does not equal: " + 1, nextToken.getPositionIncrement() == 1);
 
-    token = tf.next(token);
-    assertTrue("token is null and it shouldn't be", token != null);
-    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + " is not equal to " + "http://lucene.apache.org",
-            new String(token.termBuffer(), 0, token.termLength()).equals("http://lucene.apache.org") == true);
-    assertTrue(token.getPositionIncrement() + " does not equal: " + 1, token.getPositionIncrement() == 1);
+    nextToken = tf.next(reusableToken);
+    assertTrue("nextToken is null and it shouldn't be", nextToken != null);
+    assertTrue(nextToken.term() + " is not equal to " + "http://lucene.apache.org",
+            nextToken.term().equals("http://lucene.apache.org") == true);
+    assertTrue(nextToken.getPositionIncrement() + " does not equal: " + 1, nextToken.getPositionIncrement() == 1);
 
-    token = tf.next(token);
-    assertTrue("token is null and it shouldn't be", token != null);
-    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + " is not equal to " + "here",
-            new String(token.termBuffer(), 0, token.termLength()).equals("here") == true);
-    assertTrue(token.getPositionIncrement() + " does not equal: " + 0, token.getPositionIncrement() == 0);
+    nextToken = tf.next(reusableToken);
+    assertTrue("nextToken is null and it shouldn't be", nextToken != null);
+    assertTrue(nextToken.term() + " is not equal to " + "here",
+            nextToken.term().equals("here") == true);
+    assertTrue(nextToken.getPositionIncrement() + " does not equal: " + 0, nextToken.getPositionIncrement() == 0);
 
-    token = tf.next(token);
-    assertTrue("token is null and it shouldn't be", token != null);
-    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + " is not equal to " + "again",
-            new String(token.termBuffer(), 0, token.termLength()).equals("again") == true);
-    assertTrue(token.getPositionIncrement() + " does not equal: " + 1, token.getPositionIncrement() == 1);
+    nextToken = tf.next(reusableToken);
+    assertTrue("nextToken is null and it shouldn't be", nextToken != null);
+    assertTrue(nextToken.term() + " is not equal to " + "again",
+            nextToken.term().equals("again") == true);
+    assertTrue(nextToken.getPositionIncrement() + " does not equal: " + 1, nextToken.getPositionIncrement() == 1);
 
-    token = tf.next(token);
-    assertTrue("token is null and it shouldn't be", token != null);
-    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + " is not equal to " + "a",
-            new String(token.termBuffer(), 0, token.termLength()).equals("a") == true);
-    assertTrue(token.getPositionIncrement() + " does not equal: " + 1, token.getPositionIncrement() == 1);
+    nextToken = tf.next(reusableToken);
+    assertTrue("nextToken is null and it shouldn't be", nextToken != null);
+    assertTrue(nextToken.term() + " is not equal to " + "a",
+            nextToken.term().equals("a") == true);
+    assertTrue(nextToken.getPositionIncrement() + " does not equal: " + 1, nextToken.getPositionIncrement() == 1);
 
-    token = tf.next(token);
-    assertTrue("token is null and it shouldn't be", token != null);
-    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + " is not equal to " + "b",
-            new String(token.termBuffer(), 0, token.termLength()).equals("b") == true);
-    assertTrue(token.getPositionIncrement() + " does not equal: " + 1, token.getPositionIncrement() == 1);
+    nextToken = tf.next(reusableToken);
+    assertTrue("nextToken is null and it shouldn't be", nextToken != null);
+    assertTrue(nextToken.term() + " is not equal to " + "b",
+            nextToken.term().equals("b") == true);
+    assertTrue(nextToken.getPositionIncrement() + " does not equal: " + 1, nextToken.getPositionIncrement() == 1);
 
-    token = tf.next(token);
-    assertTrue("token is null and it shouldn't be", token != null);
-    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + " is not equal to " + "c",
-            new String(token.termBuffer(), 0, token.termLength()).equals("c") == true);
-    assertTrue(token.getPositionIncrement() + " does not equal: " + 1, token.getPositionIncrement() == 1);
+    nextToken = tf.next(reusableToken);
+    assertTrue("nextToken is null and it shouldn't be", nextToken != null);
+    assertTrue(nextToken.term() + " is not equal to " + "c",
+            nextToken.term().equals("c") == true);
+    assertTrue(nextToken.getPositionIncrement() + " does not equal: " + 1, nextToken.getPositionIncrement() == 1);
 
-    token = tf.next(token);
-    assertTrue("token is null and it shouldn't be", token != null);
-    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + " is not equal to " + "d",
-            new String(token.termBuffer(), 0, token.termLength()).equals("d") == true);
-    assertTrue(token.getPositionIncrement() + " does not equal: " + 1, token.getPositionIncrement() == 1);
+    nextToken = tf.next(reusableToken);
+    assertTrue("nextToken is null and it shouldn't be", nextToken != null);
+    assertTrue(nextToken.term() + " is not equal to " + "d",
+            nextToken.term().equals("d") == true);
+    assertTrue(nextToken.getPositionIncrement() + " does not equal: " + 1, nextToken.getPositionIncrement() == 1);
 
-    token = tf.next();
-    assertTrue("token is not null and it should be", token == null);
+    nextToken = tf.next(reusableToken);
+    assertTrue("nextToken is not null and it should be", nextToken == null);
   }
 
   public void testLinks() throws Exception {
     String test = "[http://lucene.apache.org/java/docs/index.html#news here] [http://lucene.apache.org/java/docs/index.html?b=c here] [https://lucene.apache.org/java/docs/index.html?b=c here]";
     WikipediaTokenizer tf = new WikipediaTokenizer(new StringReader(test));
-    Token token = new Token();
-    token = tf.next(token);
-    assertTrue("token is null and it shouldn't be", token != null);
-    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + " is not equal to " + "http://lucene.apache.org/java/docs/index.html#news",
-            new String(token.termBuffer(), 0, token.termLength()).equals("http://lucene.apache.org/java/docs/index.html#news") == true);
-    assertTrue(token.type() + " is not equal to " + WikipediaTokenizer.EXTERNAL_LINK_URL, token.type().equals(WikipediaTokenizer.EXTERNAL_LINK_URL) == true);
-    tf.next(token);//skip here
-    token = tf.next(token);
-    assertTrue("token is null and it shouldn't be", token != null);
-    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + " is not equal to " + "http://lucene.apache.org/java/docs/index.html?b=c",
-            new String(token.termBuffer(), 0, token.termLength()).equals("http://lucene.apache.org/java/docs/index.html?b=c") == true);
-    assertTrue(token.type() + " is not equal to " + WikipediaTokenizer.EXTERNAL_LINK_URL, token.type().equals(WikipediaTokenizer.EXTERNAL_LINK_URL) == true);
-    tf.next(token);//skip here
-    token = tf.next(token);
-    assertTrue("token is null and it shouldn't be", token != null);
-    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + " is not equal to " + "https://lucene.apache.org/java/docs/index.html?b=c",
-            new String(token.termBuffer(), 0, token.termLength()).equals("https://lucene.apache.org/java/docs/index.html?b=c") == true);
-    assertTrue(token.type() + " is not equal to " + WikipediaTokenizer.EXTERNAL_LINK_URL, token.type().equals(WikipediaTokenizer.EXTERNAL_LINK_URL) == true);
-    token = tf.next();
-    assertTrue("token is null and it shouldn't be", token != null);
+    final Token reusableToken = new Token();
+    Token nextToken = tf.next(reusableToken);
+    assertTrue("nextToken is null and it shouldn't be", nextToken != null);
+    assertTrue(nextToken.term() + " is not equal to " + "http://lucene.apache.org/java/docs/index.html#news",
+            nextToken.term().equals("http://lucene.apache.org/java/docs/index.html#news") == true);
+    assertTrue(nextToken.type() + " is not equal to " + WikipediaTokenizer.EXTERNAL_LINK_URL, nextToken.type().equals(WikipediaTokenizer.EXTERNAL_LINK_URL) == true);
+    tf.next(reusableToken);//skip here
+    nextToken = tf.next(reusableToken);
+    assertTrue("nextToken is null and it shouldn't be", nextToken != null);
+    assertTrue(nextToken.term() + " is not equal to " + "http://lucene.apache.org/java/docs/index.html?b=c",
+            nextToken.term().equals("http://lucene.apache.org/java/docs/index.html?b=c") == true);
+    assertTrue(nextToken.type() + " is not equal to " + WikipediaTokenizer.EXTERNAL_LINK_URL, nextToken.type().equals(WikipediaTokenizer.EXTERNAL_LINK_URL) == true);
+    tf.next(reusableToken);//skip here
+    nextToken = tf.next(reusableToken);
+    assertTrue("nextToken is null and it shouldn't be", nextToken != null);
+    assertTrue(nextToken.term() + " is not equal to " + "https://lucene.apache.org/java/docs/index.html?b=c",
+            nextToken.term().equals("https://lucene.apache.org/java/docs/index.html?b=c") == true);
+    assertTrue(nextToken.type() + " is not equal to " + WikipediaTokenizer.EXTERNAL_LINK_URL, nextToken.type().equals(WikipediaTokenizer.EXTERNAL_LINK_URL) == true);
+    nextToken = tf.next(reusableToken);
+    assertTrue("nextToken is null and it shouldn't be", nextToken != null);
 
-    token = tf.next();
-    assertTrue("token is not null and it should be", token == null);
+    nextToken = tf.next(reusableToken);
+    assertTrue("nextToken is not null and it should be", nextToken == null);
 
   }
 
@@ -277,72 +277,72 @@
     checkLinkPhrases(tf);
     String test = "[[Category:a b c d]] [[Category:e f g]] [[link here]] [[link there]] ''italics here'' something ''more italics'' [[Category:h   i   j]]";
     tf = new WikipediaTokenizer(new StringReader(test), WikipediaTokenizer.UNTOKENIZED_ONLY, untoks);
-    Token token;
-    token = tf.next();
-    assertTrue("token is null and it shouldn't be", token != null);
-    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + " is not equal to " + "a b c d",
-            new String(token.termBuffer(), 0, token.termLength()).equals("a b c d") == true);
-    assertTrue(token.getPositionIncrement() + " does not equal: " + 1, token.getPositionIncrement() == 1);
-    assertTrue(token.startOffset() + " does not equal: " + 11, token.startOffset() == 11);
-    assertTrue(token.endOffset() + " does not equal: " + 18, token.endOffset() == 18);
-    token = tf.next();
-    assertTrue("token is null and it shouldn't be", token != null);
-    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + " is not equal to " + "e f g",
-            new String(token.termBuffer(), 0, token.termLength()).equals("e f g") == true);
-    assertTrue(token.startOffset() + " does not equal: " + 32, token.startOffset() == 32);
-    assertTrue(token.endOffset() + " does not equal: " + 37, token.endOffset() == 37);
-    token = tf.next();
-    assertTrue("token is null and it shouldn't be", token != null);
-    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + " is not equal to " + "link",
-            new String(token.termBuffer(), 0, token.termLength()).equals("link") == true);
-    assertTrue(token.startOffset() + " does not equal: " + 42, token.startOffset() == 42);
-    assertTrue(token.endOffset() + " does not equal: " + 46, token.endOffset() == 46);
-    token = tf.next();
-    assertTrue("token is null and it shouldn't be", token != null);
-    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + " is not equal to " + "here",
-            new String(token.termBuffer(), 0, token.termLength()).equals("here") == true);
-    assertTrue(token.startOffset() + " does not equal: " + 47, token.startOffset() == 47);
-    assertTrue(token.endOffset() + " does not equal: " + 51, token.endOffset() == 51);
-    token = tf.next();
-    assertTrue("token is null and it shouldn't be", token != null);
-    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + " is not equal to " + "link",
-            new String(token.termBuffer(), 0, token.termLength()).equals("link") == true);
-    assertTrue(token.startOffset() + " does not equal: " + 56, token.startOffset() == 56);
-    assertTrue(token.endOffset() + " does not equal: " + 60, token.endOffset() == 60);
-    token = tf.next();
-    assertTrue("token is null and it shouldn't be", token != null);
-    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + " is not equal to " + "there",
-            new String(token.termBuffer(), 0, token.termLength()).equals("there") == true);
-    assertTrue(token.startOffset() + " does not equal: " + 61, token.startOffset() == 61);
-    assertTrue(token.endOffset() + " does not equal: " + 66, token.endOffset() == 66);
-    token = tf.next();
-    assertTrue("token is null and it shouldn't be", token != null);
-    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + " is not equal to " + "italics here",
-            new String(token.termBuffer(), 0, token.termLength()).equals("italics here") == true);
-    assertTrue(token.startOffset() + " does not equal: " + 71, token.startOffset() == 71);
-    assertTrue(token.endOffset() + " does not equal: " + 83, token.endOffset() == 83);
-    token = tf.next();
-    assertTrue("token is null and it shouldn't be", token != null);
-    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + " is not equal to " + "something",
-            new String(token.termBuffer(), 0, token.termLength()).equals("something") == true);
-    assertTrue(token.startOffset() + " does not equal: " + 86, token.startOffset() == 86);
-    assertTrue(token.endOffset() + " does not equal: " + 95, token.endOffset() == 95);
-    token = tf.next();
-    assertTrue("token is null and it shouldn't be", token != null);
-    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + " is not equal to " + "more italics",
-            new String(token.termBuffer(), 0, token.termLength()).equals("more italics") == true);
-    assertTrue(token.startOffset() + " does not equal: " + 98, token.startOffset() == 98);
-    assertTrue(token.endOffset() + " does not equal: " + 110, token.endOffset() == 110);
+    final Token reusableToken = new Token();
+    Token nextToken = tf.next(reusableToken);
+    assertTrue("nextToken is null and it shouldn't be", nextToken != null);
+    assertTrue(nextToken.term() + " is not equal to " + "a b c d",
+            nextToken.term().equals("a b c d") == true);
+    assertTrue(nextToken.getPositionIncrement() + " does not equal: " + 1, nextToken.getPositionIncrement() == 1);
+    assertTrue(nextToken.startOffset() + " does not equal: " + 11, nextToken.startOffset() == 11);
+    assertTrue(nextToken.endOffset() + " does not equal: " + 18, nextToken.endOffset() == 18);
+    nextToken = tf.next(reusableToken);
+    assertTrue("nextToken is null and it shouldn't be", nextToken != null);
+    assertTrue(nextToken.term() + " is not equal to " + "e f g",
+            nextToken.term().equals("e f g") == true);
+    assertTrue(nextToken.startOffset() + " does not equal: " + 32, nextToken.startOffset() == 32);
+    assertTrue(nextToken.endOffset() + " does not equal: " + 37, nextToken.endOffset() == 37);
+    nextToken = tf.next(reusableToken);
+    assertTrue("nextToken is null and it shouldn't be", nextToken != null);
+    assertTrue(nextToken.term() + " is not equal to " + "link",
+            nextToken.term().equals("link") == true);
+    assertTrue(nextToken.startOffset() + " does not equal: " + 42, nextToken.startOffset() == 42);
+    assertTrue(nextToken.endOffset() + " does not equal: " + 46, nextToken.endOffset() == 46);
+    nextToken = tf.next(reusableToken);
+    assertTrue("nextToken is null and it shouldn't be", nextToken != null);
+    assertTrue(nextToken.term() + " is not equal to " + "here",
+            nextToken.term().equals("here") == true);
+    assertTrue(nextToken.startOffset() + " does not equal: " + 47, nextToken.startOffset() == 47);
+    assertTrue(nextToken.endOffset() + " does not equal: " + 51, nextToken.endOffset() == 51);
+    nextToken = tf.next(reusableToken);
+    assertTrue("nextToken is null and it shouldn't be", nextToken != null);
+    assertTrue(nextToken.term() + " is not equal to " + "link",
+            nextToken.term().equals("link") == true);
+    assertTrue(nextToken.startOffset() + " does not equal: " + 56, nextToken.startOffset() == 56);
+    assertTrue(nextToken.endOffset() + " does not equal: " + 60, nextToken.endOffset() == 60);
+    nextToken = tf.next(reusableToken);
+    assertTrue("nextToken is null and it shouldn't be", nextToken != null);
+    assertTrue(nextToken.term() + " is not equal to " + "there",
+            nextToken.term().equals("there") == true);
+    assertTrue(nextToken.startOffset() + " does not equal: " + 61, nextToken.startOffset() == 61);
+    assertTrue(nextToken.endOffset() + " does not equal: " + 66, nextToken.endOffset() == 66);
+    nextToken = tf.next(reusableToken);
+    assertTrue("nextToken is null and it shouldn't be", nextToken != null);
+    assertTrue(nextToken.term() + " is not equal to " + "italics here",
+            nextToken.term().equals("italics here") == true);
+    assertTrue(nextToken.startOffset() + " does not equal: " + 71, nextToken.startOffset() == 71);
+    assertTrue(nextToken.endOffset() + " does not equal: " + 83, nextToken.endOffset() == 83);
+    nextToken = tf.next(reusableToken);
+    assertTrue("nextToken is null and it shouldn't be", nextToken != null);
+    assertTrue(nextToken.term() + " is not equal to " + "something",
+            nextToken.term().equals("something") == true);
+    assertTrue(nextToken.startOffset() + " does not equal: " + 86, nextToken.startOffset() == 86);
+    assertTrue(nextToken.endOffset() + " does not equal: " + 95, nextToken.endOffset() == 95);
+    nextToken = tf.next(reusableToken);
+    assertTrue("nextToken is null and it shouldn't be", nextToken != null);
+    assertTrue(nextToken.term() + " is not equal to " + "more italics",
+            nextToken.term().equals("more italics") == true);
+    assertTrue(nextToken.startOffset() + " does not equal: " + 98, nextToken.startOffset() == 98);
+    assertTrue(nextToken.endOffset() + " does not equal: " + 110, nextToken.endOffset() == 110);
 
-    token = tf.next();
-    assertTrue("token is null and it shouldn't be", token != null);
-    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + " is not equal to " + "h   i   j",
-            new String(token.termBuffer(), 0, token.termLength()).equals("h   i   j") == true);
-    assertTrue(token.startOffset() + " does not equal: " + 124, token.startOffset() == 124);
-    assertTrue(token.endOffset() + " does not equal: " + 133, token.endOffset() == 133);
+    nextToken = tf.next(reusableToken);
+    assertTrue("nextToken is null and it shouldn't be", nextToken != null);
+    assertTrue(nextToken.term() + " is not equal to " + "h   i   j",
+            nextToken.term().equals("h   i   j") == true);
+    assertTrue(nextToken.startOffset() + " does not equal: " + 124, nextToken.startOffset() == 124);
+    assertTrue(nextToken.endOffset() + " does not equal: " + 133, nextToken.endOffset() == 133);
 
-    token = tf.next();
-    assertTrue("token is not null and it should be", token == null);
+    nextToken = tf.next(reusableToken);
+    assertTrue("nextToken is not null and it should be", nextToken == null);
   }
 
   public void testBoth() throws Exception {
@@ -352,225 +352,225 @@
     String test = "[[Category:a b c d]] [[Category:e f g]] [[link here]] [[link there]] ''italics here'' something ''more italics'' [[Category:h   i   j]]";
     //should output all the indivual tokens plus the untokenized tokens as well.  Untokenized tokens
     WikipediaTokenizer tf = new WikipediaTokenizer(new StringReader(test), WikipediaTokenizer.BOTH, untoks);
-    Token token;
-    token = tf.next();
-    assertTrue("token is null and it shouldn't be", token != null);
-    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + " is not equal to " + "a b c d",
-            new String(token.termBuffer(), 0, token.termLength()).equals("a b c d") == true);
-    assertTrue(token.getPositionIncrement() + " does not equal: " + 1, token.getPositionIncrement() == 1);
-    assertTrue(token.type() + " is not equal to " + WikipediaTokenizer.CATEGORY, token.type().equals(WikipediaTokenizer.CATEGORY) == true);
-    assertTrue(token.getFlags() + " does not equal: " + WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG, token.getFlags() == WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG);
-    assertTrue(token.startOffset() + " does not equal: " + 11, token.startOffset() == 11);
-    assertTrue(token.endOffset() + " does not equal: " + 18, token.endOffset() == 18);
-    token = tf.next();
-    assertTrue("token is null and it shouldn't be", token != null);
-    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + " is not equal to " + "a",
-            new String(token.termBuffer(), 0, token.termLength()).equals("a") == true);
-    assertTrue(token.getPositionIncrement() + " does not equal: " + 0, token.getPositionIncrement() == 0);
-    assertTrue(token.type() + " is not equal to " + WikipediaTokenizer.CATEGORY, token.type().equals(WikipediaTokenizer.CATEGORY) == true);
-    assertTrue(token.getFlags() + " equals: " + WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG + " and it shouldn't", token.getFlags() != WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG);
-    assertTrue(token.startOffset() + " does not equal: " + 11, token.startOffset() == 11);
-    assertTrue(token.endOffset() + " does not equal: " + 12, token.endOffset() == 12);
+    final Token reusableToken = new Token();
+    Token nextToken = tf.next(reusableToken);
+    assertTrue("nextToken is null and it shouldn't be", nextToken != null);
+    assertTrue(nextToken.term() + " is not equal to " + "a b c d",
+            nextToken.term().equals("a b c d") == true);
+    assertTrue(nextToken.getPositionIncrement() + " does not equal: " + 1, nextToken.getPositionIncrement() == 1);
+    assertTrue(nextToken.type() + " is not equal to " + WikipediaTokenizer.CATEGORY, nextToken.type().equals(WikipediaTokenizer.CATEGORY) == true);
+    assertTrue(nextToken.getFlags() + " does not equal: " + WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG, nextToken.getFlags() == WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG);
+    assertTrue(nextToken.startOffset() + " does not equal: " + 11, nextToken.startOffset() == 11);
+    assertTrue(nextToken.endOffset() + " does not equal: " + 18, nextToken.endOffset() == 18);
+    nextToken = tf.next(reusableToken);
+    assertTrue("nextToken is null and it shouldn't be", nextToken != null);
+    assertTrue(nextToken.term() + " is not equal to " + "a",
+            nextToken.term().equals("a") == true);
+    assertTrue(nextToken.getPositionIncrement() + " does not equal: " + 0, nextToken.getPositionIncrement() == 0);
+    assertTrue(nextToken.type() + " is not equal to " + WikipediaTokenizer.CATEGORY, nextToken.type().equals(WikipediaTokenizer.CATEGORY) == true);
+    assertTrue(nextToken.getFlags() + " equals: " + WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG + " and it shouldn't", nextToken.getFlags() != WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG);
+    assertTrue(nextToken.startOffset() + " does not equal: " + 11, nextToken.startOffset() == 11);
+    assertTrue(nextToken.endOffset() + " does not equal: " + 12, nextToken.endOffset() == 12);
 
-    token = tf.next();
-    assertTrue("token is null and it shouldn't be", token != null);
-    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + " is not equal to " + "b",
-            new String(token.termBuffer(), 0, token.termLength()).equals("b") == true);
-    assertTrue(token.getPositionIncrement() + " does not equal: " + 1, token.getPositionIncrement() == 1);
-    assertTrue(token.type() + " is not equal to " + WikipediaTokenizer.CATEGORY, token.type().equals(WikipediaTokenizer.CATEGORY) == true);
-    assertTrue(token.startOffset() + " does not equal: " + 13, token.startOffset() == 13);
-    assertTrue(token.endOffset() + " does not equal: " + 14, token.endOffset() == 14);
+    nextToken = tf.next(reusableToken);
+    assertTrue("nextToken is null and it shouldn't be", nextToken != null);
+    assertTrue(nextToken.term() + " is not equal to " + "b",
+            nextToken.term().equals("b") == true);
+    assertTrue(nextToken.getPositionIncrement() + " does not equal: " + 1, nextToken.getPositionIncrement() == 1);
+    assertTrue(nextToken.type() + " is not equal to " + WikipediaTokenizer.CATEGORY, nextToken.type().equals(WikipediaTokenizer.CATEGORY) == true);
+    assertTrue(nextToken.startOffset() + " does not equal: " + 13, nextToken.startOffset() == 13);
+    assertTrue(nextToken.endOffset() + " does not equal: " + 14, nextToken.endOffset() == 14);
 
-    token = tf.next();
-    assertTrue("token is null and it shouldn't be", token != null);
-    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + " is not equal to " + "c",
-            new String(token.termBuffer(), 0, token.termLength()).equals("c") == true);
-    assertTrue(token.getPositionIncrement() + " does not equal: " + 1, token.getPositionIncrement() == 1);
-    assertTrue(token.type() + " is not equal to " + WikipediaTokenizer.CATEGORY, token.type().equals(WikipediaTokenizer.CATEGORY) == true);
-    assertTrue(token.startOffset() + " does not equal: " + 15, token.startOffset() == 15);
-    assertTrue(token.endOffset() + " does not equal: " + 16, token.endOffset() == 16);
+    nextToken = tf.next(reusableToken);
+    assertTrue("nextToken is null and it shouldn't be", nextToken != null);
+    assertTrue(nextToken.term() + " is not equal to " + "c",
+            nextToken.term().equals("c") == true);
+    assertTrue(nextToken.getPositionIncrement() + " does not equal: " + 1, nextToken.getPositionIncrement() == 1);
+    assertTrue(nextToken.type() + " is not equal to " + WikipediaTokenizer.CATEGORY, nextToken.type().equals(WikipediaTokenizer.CATEGORY) == true);
+    assertTrue(nextToken.startOffset() + " does not equal: " + 15, nextToken.startOffset() == 15);
+    assertTrue(nextToken.endOffset() + " does not equal: " + 16, nextToken.endOffset() == 16);
 
-    token = tf.next();
-    assertTrue("token is null and it shouldn't be", token != null);
-    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + " is not equal to " + "d",
-            new String(token.termBuffer(), 0, token.termLength()).equals("d") == true);
-    assertTrue(token.getPositionIncrement() + " does not equal: " + 1, token.getPositionIncrement() == 1);
-    assertTrue(token.type() + " is not equal to " + WikipediaTokenizer.CATEGORY, token.type().equals(WikipediaTokenizer.CATEGORY) == true);
-    assertTrue(token.startOffset() + " does not equal: " + 17, token.startOffset() == 17);
-    assertTrue(token.endOffset() + " does not equal: " + 18, token.endOffset() == 18);
+    nextToken = tf.next(reusableToken);
+    assertTrue("nextToken is null and it shouldn't be", nextToken != null);
+    assertTrue(nextToken.term() + " is not equal to " + "d",
+            nextToken.term().equals("d") == true);
+    assertTrue(nextToken.getPositionIncrement() + " does not equal: " + 1, nextToken.getPositionIncrement() == 1);
+    assertTrue(nextToken.type() + " is not equal to " + WikipediaTokenizer.CATEGORY, nextToken.type().equals(WikipediaTokenizer.CATEGORY) == true);
+    assertTrue(nextToken.startOffset() + " does not equal: " + 17, nextToken.startOffset() == 17);
+    assertTrue(nextToken.endOffset() + " does not equal: " + 18, nextToken.endOffset() == 18);
 
 
 
-    token = tf.next();
-    assertTrue("token is null and it shouldn't be", token != null);
-    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + " is not equal to " + "e f g",
-            new String(token.termBuffer(), 0, token.termLength()).equals("e f g") == true);
-    assertTrue(token.type() + " is not equal to " + WikipediaTokenizer.CATEGORY, token.type().equals(WikipediaTokenizer.CATEGORY) == true);
-    assertTrue(token.getFlags() + " does not equal: " + WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG, token.getFlags() == WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG);
-    assertTrue(token.startOffset() + " does not equal: " + 32, token.startOffset() == 32);
-    assertTrue(token.endOffset() + " does not equal: " + 37, token.endOffset() == 37);
+    nextToken = tf.next(reusableToken);
+    assertTrue("nextToken is null and it shouldn't be", nextToken != null);
+    assertTrue(nextToken.term() + " is not equal to " + "e f g",
+            nextToken.term().equals("e f g") == true);
+    assertTrue(nextToken.type() + " is not equal to " + WikipediaTokenizer.CATEGORY, nextToken.type().equals(WikipediaTokenizer.CATEGORY) == true);
+    assertTrue(nextToken.getFlags() + " does not equal: " + WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG, nextToken.getFlags() == WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG);
+    assertTrue(nextToken.startOffset() + " does not equal: " + 32, nextToken.startOffset() == 32);
+    assertTrue(nextToken.endOffset() + " does not equal: " + 37, nextToken.endOffset() == 37);
 
-    token = tf.next();
-    assertTrue("token is null and it shouldn't be", token != null);
-    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + " is not equal to " + "e",
-            new String(token.termBuffer(), 0, token.termLength()).equals("e") == true);
-    assertTrue(token.type() + " is not equal to " + WikipediaTokenizer.CATEGORY, token.type().equals(WikipediaTokenizer.CATEGORY) == true);
-    assertTrue(token.getPositionIncrement() + " does not equal: " + 0, token.getPositionIncrement() == 0);
-    assertTrue(token.startOffset() + " does not equal: " + 32, token.startOffset() == 32);
-    assertTrue(token.endOffset() + " does not equal: " + 33, token.endOffset() == 33);
+    nextToken = tf.next(reusableToken);
+    assertTrue("nextToken is null and it shouldn't be", nextToken != null);
+    assertTrue(nextToken.term() + " is not equal to " + "e",
+            nextToken.term().equals("e") == true);
+    assertTrue(nextToken.type() + " is not equal to " + WikipediaTokenizer.CATEGORY, nextToken.type().equals(WikipediaTokenizer.CATEGORY) == true);
+    assertTrue(nextToken.getPositionIncrement() + " does not equal: " + 0, nextToken.getPositionIncrement() == 0);
+    assertTrue(nextToken.startOffset() + " does not equal: " + 32, nextToken.startOffset() == 32);
+    assertTrue(nextToken.endOffset() + " does not equal: " + 33, nextToken.endOffset() == 33);
 
-    token = tf.next();
-    assertTrue("token is null and it shouldn't be", token != null);
-    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + " is not equal to " + "f",
-            new String(token.termBuffer(), 0, token.termLength()).equals("f") == true);
-    assertTrue(token.type() + " is not equal to " + WikipediaTokenizer.CATEGORY, token.type().equals(WikipediaTokenizer.CATEGORY) == true);
-    assertTrue(token.getPositionIncrement() + " does not equal: " + 1, token.getPositionIncrement() == 1);
-    assertTrue(token.startOffset() + " does not equal: " + 34, token.startOffset() == 34);
-    assertTrue(token.endOffset() + " does not equal: " + 35, token.endOffset() == 35);
+    nextToken = tf.next(reusableToken);
+    assertTrue("nextToken is null and it shouldn't be", nextToken != null);
+    assertTrue(nextToken.term() + " is not equal to " + "f",
+            nextToken.term().equals("f") == true);
+    assertTrue(nextToken.type() + " is not equal to " + WikipediaTokenizer.CATEGORY, nextToken.type().equals(WikipediaTokenizer.CATEGORY) == true);
+    assertTrue(nextToken.getPositionIncrement() + " does not equal: " + 1, nextToken.getPositionIncrement() == 1);
+    assertTrue(nextToken.startOffset() + " does not equal: " + 34, nextToken.startOffset() == 34);
+    assertTrue(nextToken.endOffset() + " does not equal: " + 35, nextToken.endOffset() == 35);
 
-    token = tf.next();
-    assertTrue("token is null and it shouldn't be", token != null);
-    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + " is not equal to " + "g",
-            new String(token.termBuffer(), 0, token.termLength()).equals("g") == true);
-    assertTrue(token.type() + " is not equal to " + WikipediaTokenizer.CATEGORY, token.type().equals(WikipediaTokenizer.CATEGORY) == true);
-    assertTrue(token.getPositionIncrement() + " does not equal: " + 1, token.getPositionIncrement() == 1);
-    assertTrue(token.startOffset() + " does not equal: " + 36, token.startOffset() == 36);
-    assertTrue(token.endOffset() + " does not equal: " + 37, token.endOffset() == 37);
+    nextToken = tf.next(reusableToken);
+    assertTrue("nextToken is null and it shouldn't be", nextToken != null);
+    assertTrue(nextToken.term() + " is not equal to " + "g",
+            nextToken.term().equals("g") == true);
+    assertTrue(nextToken.type() + " is not equal to " + WikipediaTokenizer.CATEGORY, nextToken.type().equals(WikipediaTokenizer.CATEGORY) == true);
+    assertTrue(nextToken.getPositionIncrement() + " does not equal: " + 1, nextToken.getPositionIncrement() == 1);
+    assertTrue(nextToken.startOffset() + " does not equal: " + 36, nextToken.startOffset() == 36);
+    assertTrue(nextToken.endOffset() + " does not equal: " + 37, nextToken.endOffset() == 37);
 
-    token = tf.next();
-    assertTrue("token is null and it shouldn't be", token != null);
-    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + " is not equal to " + "link",
-            new String(token.termBuffer(), 0, token.termLength()).equals("link") == true);
-    assertTrue(token.getPositionIncrement() + " does not equal: " + 1, token.getPositionIncrement() == 1);
-    assertTrue(token.type() + " is not equal to " + WikipediaTokenizer.INTERNAL_LINK, token.type().equals(WikipediaTokenizer.INTERNAL_LINK) == true);
-    assertTrue(token.startOffset() + " does not equal: " + 42, token.startOffset() == 42);
-    assertTrue(token.endOffset() + " does not equal: " + 46, token.endOffset() == 46);
-    token = tf.next();
-    assertTrue("token is null and it shouldn't be", token != null);
-    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + " is not equal to " + "here",
-            new String(token.termBuffer(), 0, token.termLength()).equals("here") == true);
-    assertTrue(token.getPositionIncrement() + " does not equal: " + 1, token.getPositionIncrement() == 1);
-    assertTrue(token.type() + " is not equal to " + WikipediaTokenizer.INTERNAL_LINK, token.type().equals(WikipediaTokenizer.INTERNAL_LINK) == true);
-    assertTrue(token.startOffset() + " does not equal: " + 47, token.startOffset() == 47);
-    assertTrue(token.endOffset() + " does not equal: " + 51, token.endOffset() == 51);
-    token = tf.next();
-    assertTrue("token is null and it shouldn't be", token != null);
-    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + " is not equal to " + "link",
-            new String(token.termBuffer(), 0, token.termLength()).equals("link") == true);
-    assertTrue(token.getPositionIncrement() + " does not equal: " + 1, token.getPositionIncrement() == 1);
-    assertTrue(token.startOffset() + " does not equal: " + 56, token.startOffset() == 56);
-    assertTrue(token.type() + " is not equal to " + WikipediaTokenizer.INTERNAL_LINK, token.type().equals(WikipediaTokenizer.INTERNAL_LINK) == true);
-    assertTrue(token.endOffset() + " does not equal: " + 60, token.endOffset() == 60);
-    token = tf.next();
-    assertTrue("token is null and it shouldn't be", token != null);
-    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + " is not equal to " + "there",
-            new String(token.termBuffer(), 0, token.termLength()).equals("there") == true);
-    assertTrue(token.getPositionIncrement() + " does not equal: " + 1, token.getPositionIncrement() == 1);
-    assertTrue(token.type() + " is not equal to " + WikipediaTokenizer.INTERNAL_LINK, token.type().equals(WikipediaTokenizer.INTERNAL_LINK) == true);
-    assertTrue(token.startOffset() + " does not equal: " + 61, token.startOffset() == 61);
-    assertTrue(token.endOffset() + " does not equal: " + 66, token.endOffset() == 66);
-    token = tf.next();
-    assertTrue("token is null and it shouldn't be", token != null);
-    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + " is not equal to " + "italics here",
-            new String(token.termBuffer(), 0, token.termLength()).equals("italics here") == true);
-    assertTrue(token.getPositionIncrement() + " does not equal: " + 1, token.getPositionIncrement() == 1);
-    assertTrue(token.type() + " is not equal to " + WikipediaTokenizer.ITALICS, token.type().equals(WikipediaTokenizer.ITALICS) == true);
-    assertTrue(token.getFlags() + " does not equal: " + WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG, token.getFlags() == WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG);
-    assertTrue(token.startOffset() + " does not equal: " + 71, token.startOffset() == 71);
-    assertTrue(token.endOffset() + " does not equal: " + 83, token.endOffset() == 83);
+    nextToken = tf.next(reusableToken);
+    assertTrue("nextToken is null and it shouldn't be", nextToken != null);
+    assertTrue(nextToken.term() + " is not equal to " + "link",
+            nextToken.term().equals("link") == true);
+    assertTrue(nextToken.getPositionIncrement() + " does not equal: " + 1, nextToken.getPositionIncrement() == 1);
+    assertTrue(nextToken.type() + " is not equal to " + WikipediaTokenizer.INTERNAL_LINK, nextToken.type().equals(WikipediaTokenizer.INTERNAL_LINK) == true);
+    assertTrue(nextToken.startOffset() + " does not equal: " + 42, nextToken.startOffset() == 42);
+    assertTrue(nextToken.endOffset() + " does not equal: " + 46, nextToken.endOffset() == 46);
+    nextToken = tf.next(reusableToken);
+    assertTrue("nextToken is null and it shouldn't be", nextToken != null);
+    assertTrue(nextToken.term() + " is not equal to " + "here",
+            nextToken.term().equals("here") == true);
+    assertTrue(nextToken.getPositionIncrement() + " does not equal: " + 1, nextToken.getPositionIncrement() == 1);
+    assertTrue(nextToken.type() + " is not equal to " + WikipediaTokenizer.INTERNAL_LINK, nextToken.type().equals(WikipediaTokenizer.INTERNAL_LINK) == true);
+    assertTrue(nextToken.startOffset() + " does not equal: " + 47, nextToken.startOffset() == 47);
+    assertTrue(nextToken.endOffset() + " does not equal: " + 51, nextToken.endOffset() == 51);
+    nextToken = tf.next(reusableToken);
+    assertTrue("nextToken is null and it shouldn't be", nextToken != null);
+    assertTrue(nextToken.term() + " is not equal to " + "link",
+            nextToken.term().equals("link") == true);
+    assertTrue(nextToken.getPositionIncrement() + " does not equal: " + 1, nextToken.getPositionIncrement() == 1);
+    assertTrue(nextToken.startOffset() + " does not equal: " + 56, nextToken.startOffset() == 56);
+    assertTrue(nextToken.type() + " is not equal to " + WikipediaTokenizer.INTERNAL_LINK, nextToken.type().equals(WikipediaTokenizer.INTERNAL_LINK) == true);
+    assertTrue(nextToken.endOffset() + " does not equal: " + 60, nextToken.endOffset() == 60);
+    nextToken = tf.next(reusableToken);
+    assertTrue("nextToken is null and it shouldn't be", nextToken != null);
+    assertTrue(nextToken.term() + " is not equal to " + "there",
+            nextToken.term().equals("there") == true);
+    assertTrue(nextToken.getPositionIncrement() + " does not equal: " + 1, nextToken.getPositionIncrement() == 1);
+    assertTrue(nextToken.type() + " is not equal to " + WikipediaTokenizer.INTERNAL_LINK, nextToken.type().equals(WikipediaTokenizer.INTERNAL_LINK) == true);
+    assertTrue(nextToken.startOffset() + " does not equal: " + 61, nextToken.startOffset() == 61);
+    assertTrue(nextToken.endOffset() + " does not equal: " + 66, nextToken.endOffset() == 66);
+    nextToken = tf.next(reusableToken);
+    assertTrue("nextToken is null and it shouldn't be", nextToken != null);
+    assertTrue(nextToken.term() + " is not equal to " + "italics here",
+            nextToken.term().equals("italics here") == true);
+    assertTrue(nextToken.getPositionIncrement() + " does not equal: " + 1, nextToken.getPositionIncrement() == 1);
+    assertTrue(nextToken.type() + " is not equal to " + WikipediaTokenizer.ITALICS, nextToken.type().equals(WikipediaTokenizer.ITALICS) == true);
+    assertTrue(nextToken.getFlags() + " does not equal: " + WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG, nextToken.getFlags() == WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG);
+    assertTrue(nextToken.startOffset() + " does not equal: " + 71, nextToken.startOffset() == 71);
+    assertTrue(nextToken.endOffset() + " does not equal: " + 83, nextToken.endOffset() == 83);
 
-    token = tf.next();
-    assertTrue("token is null and it shouldn't be", token != null);
-    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + " is not equal to " + "italics",
-            new String(token.termBuffer(), 0, token.termLength()).equals("italics") == true);
-    assertTrue(token.getPositionIncrement() + " does not equal: " + 0, token.getPositionIncrement() == 0);
-    assertTrue(token.type() + " is not equal to " + WikipediaTokenizer.ITALICS, token.type().equals(WikipediaTokenizer.ITALICS) == true);
-    assertTrue(token.startOffset() + " does not equal: " + 71, token.startOffset() == 71);
-    assertTrue(token.endOffset() + " does not equal: " + 78, token.endOffset() == 78);
+    nextToken = tf.next(reusableToken);
+    assertTrue("nextToken is null and it shouldn't be", nextToken != null);
+    assertTrue(nextToken.term() + " is not equal to " + "italics",
+            nextToken.term().equals("italics") == true);
+    assertTrue(nextToken.getPositionIncrement() + " does not equal: " + 0, nextToken.getPositionIncrement() == 0);
+    assertTrue(nextToken.type() + " is not equal to " + WikipediaTokenizer.ITALICS, nextToken.type().equals(WikipediaTokenizer.ITALICS) == true);
+    assertTrue(nextToken.startOffset() + " does not equal: " + 71, nextToken.startOffset() == 71);
+    assertTrue(nextToken.endOffset() + " does not equal: " + 78, nextToken.endOffset() == 78);
 
-    token = tf.next();
-    assertTrue("token is null and it shouldn't be", token != null);
-    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + " is not equal to " + "here",
-            new String(token.termBuffer(), 0, token.termLength()).equals("here") == true);
-    assertTrue(token.getPositionIncrement() + " does not equal: " + 1, token.getPositionIncrement() == 1);
-    assertTrue(token.type() + " is not equal to " + WikipediaTokenizer.ITALICS, token.type().equals(WikipediaTokenizer.ITALICS) == true);
-    assertTrue(token.startOffset() + " does not equal: " + 79, token.startOffset() == 79);
-    assertTrue(token.endOffset() + " does not equal: " + 83, token.endOffset() == 83);
+    nextToken = tf.next(reusableToken);
+    assertTrue("nextToken is null and it shouldn't be", nextToken != null);
+    assertTrue(nextToken.term() + " is not equal to " + "here",
+            nextToken.term().equals("here") == true);
+    assertTrue(nextToken.getPositionIncrement() + " does not equal: " + 1, nextToken.getPositionIncrement() == 1);
+    assertTrue(nextToken.type() + " is not equal to " + WikipediaTokenizer.ITALICS, nextToken.type().equals(WikipediaTokenizer.ITALICS) == true);
+    assertTrue(nextToken.startOffset() + " does not equal: " + 79, nextToken.startOffset() == 79);
+    assertTrue(nextToken.endOffset() + " does not equal: " + 83, nextToken.endOffset() == 83);
 
-    token = tf.next();
-    assertTrue("token is null and it shouldn't be", token != null);
-    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + " is not equal to " + "something",
-            new String(token.termBuffer(), 0, token.termLength()).equals("something") == true);
-    assertTrue(token.getPositionIncrement() + " does not equal: " + 1, token.getPositionIncrement() == 1);
-    assertTrue(token.startOffset() + " does not equal: " + 86, token.startOffset() == 86);
-    assertTrue(token.endOffset() + " does not equal: " + 95, token.endOffset() == 95);
-    token = tf.next();
-    assertTrue("token is null and it shouldn't be", token != null);
-    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + " is not equal to " + "more italics",
-            new String(token.termBuffer(), 0, token.termLength()).equals("more italics") == true);
-    assertTrue(token.getPositionIncrement() + " does not equal: " + 1, token.getPositionIncrement() == 1);
-    assertTrue(token.type() + " is not equal to " + WikipediaTokenizer.ITALICS, token.type().equals(WikipediaTokenizer.ITALICS) == true);
-    assertTrue(token.getFlags() + " does not equal: " + WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG, token.getFlags() == WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG);
-    assertTrue(token.startOffset() + " does not equal: " + 98, token.startOffset() == 98);
-    assertTrue(token.endOffset() + " does not equal: " + 110, token.endOffset() == 110);
+    nextToken = tf.next(reusableToken);
+    assertTrue("nextToken is null and it shouldn't be", nextToken != null);
+    assertTrue(nextToken.term() + " is not equal to " + "something",
+            nextToken.term().equals("something") == true);
+    assertTrue(nextToken.getPositionIncrement() + " does not equal: " + 1, nextToken.getPositionIncrement() == 1);
+    assertTrue(nextToken.startOffset() + " does not equal: " + 86, nextToken.startOffset() == 86);
+    assertTrue(nextToken.endOffset() + " does not equal: " + 95, nextToken.endOffset() == 95);
+    nextToken = tf.next(reusableToken);
+    assertTrue("nextToken is null and it shouldn't be", nextToken != null);
+    assertTrue(nextToken.term() + " is not equal to " + "more italics",
+            nextToken.term().equals("more italics") == true);
+    assertTrue(nextToken.getPositionIncrement() + " does not equal: " + 1, nextToken.getPositionIncrement() == 1);
+    assertTrue(nextToken.type() + " is not equal to " + WikipediaTokenizer.ITALICS, nextToken.type().equals(WikipediaTokenizer.ITALICS) == true);
+    assertTrue(nextToken.getFlags() + " does not equal: " + WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG, nextToken.getFlags() == WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG);
+    assertTrue(nextToken.startOffset() + " does not equal: " + 98, nextToken.startOffset() == 98);
+    assertTrue(nextToken.endOffset() + " does not equal: " + 110, nextToken.endOffset() == 110);
 
-    token = tf.next();
-    assertTrue("token is null and it shouldn't be", token != null);
-    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + " is not equal to " + "more",
-            new String(token.termBuffer(), 0, token.termLength()).equals("more") == true);
-    assertTrue(token.getPositionIncrement() + " does not equal: " + 0, token.getPositionIncrement() == 0);
-    assertTrue(token.type() + " is not equal to " + WikipediaTokenizer.ITALICS, token.type().equals(WikipediaTokenizer.ITALICS) == true);
-    assertTrue(token.startOffset() + " does not equal: " + 98, token.startOffset() == 98);
-    assertTrue(token.endOffset() + " does not equal: " + 102, token.endOffset() == 102);
+    nextToken = tf.next(reusableToken);
+    assertTrue("nextToken is null and it shouldn't be", nextToken != null);
+    assertTrue(nextToken.term() + " is not equal to " + "more",
+            nextToken.term().equals("more") == true);
+    assertTrue(nextToken.getPositionIncrement() + " does not equal: " + 0, nextToken.getPositionIncrement() == 0);
+    assertTrue(nextToken.type() + " is not equal to " + WikipediaTokenizer.ITALICS, nextToken.type().equals(WikipediaTokenizer.ITALICS) == true);
+    assertTrue(nextToken.startOffset() + " does not equal: " + 98, nextToken.startOffset() == 98);
+    assertTrue(nextToken.endOffset() + " does not equal: " + 102, nextToken.endOffset() == 102);
 
-    token = tf.next();
-    assertTrue("token is null and it shouldn't be", token != null);
-    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + " is not equal to " + "italics",
-            new String(token.termBuffer(), 0, token.termLength()).equals("italics") == true);
-    assertTrue(token.getPositionIncrement() + " does not equal: " + 1, token.getPositionIncrement() == 1);
-        assertTrue(token.type() + " is not equal to " + WikipediaTokenizer.ITALICS, token.type().equals(WikipediaTokenizer.ITALICS) == true);
+    nextToken = tf.next(reusableToken);
+    assertTrue("nextToken is null and it shouldn't be", nextToken != null);
+    assertTrue(nextToken.term() + " is not equal to " + "italics",
+            nextToken.term().equals("italics") == true);
+    assertTrue(nextToken.getPositionIncrement() + " does not equal: " + 1, nextToken.getPositionIncrement() == 1);
+        assertTrue(nextToken.type() + " is not equal to " + WikipediaTokenizer.ITALICS, nextToken.type().equals(WikipediaTokenizer.ITALICS) == true);
 
-    assertTrue(token.startOffset() + " does not equal: " + 103, token.startOffset() == 103);
-    assertTrue(token.endOffset() + " does not equal: " + 110, token.endOffset() == 110);
+    assertTrue(nextToken.startOffset() + " does not equal: " + 103, nextToken.startOffset() == 103);
+    assertTrue(nextToken.endOffset() + " does not equal: " + 110, nextToken.endOffset() == 110);
 
-    token = tf.next();
-    assertTrue("token is null and it shouldn't be", token != null);
-    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + " is not equal to " + "h   i   j",
-            new String(token.termBuffer(), 0, token.termLength()).equals("h   i   j") == true);
-    assertTrue(token.getPositionIncrement() + " does not equal: " + 1, token.getPositionIncrement() == 1);
-    assertTrue(token.type() + " is not equal to " + WikipediaTokenizer.CATEGORY, token.type().equals(WikipediaTokenizer.CATEGORY) == true);
-    assertTrue(token.getFlags() + " does not equal: " + WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG, token.getFlags() == WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG);
-    assertTrue(token.startOffset() + " does not equal: " + 124, token.startOffset() == 124);
-    assertTrue(token.endOffset() + " does not equal: " + 133, token.endOffset() == 133);
+    nextToken = tf.next(reusableToken);
+    assertTrue("nextToken is null and it shouldn't be", nextToken != null);
+    assertTrue(nextToken.term() + " is not equal to " + "h   i   j",
+            nextToken.term().equals("h   i   j") == true);
+    assertTrue(nextToken.getPositionIncrement() + " does not equal: " + 1, nextToken.getPositionIncrement() == 1);
+    assertTrue(nextToken.type() + " is not equal to " + WikipediaTokenizer.CATEGORY, nextToken.type().equals(WikipediaTokenizer.CATEGORY) == true);
+    assertTrue(nextToken.getFlags() + " does not equal: " + WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG, nextToken.getFlags() == WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG);
+    assertTrue(nextToken.startOffset() + " does not equal: " + 124, nextToken.startOffset() == 124);
+    assertTrue(nextToken.endOffset() + " does not equal: " + 133, nextToken.endOffset() == 133);
 
-    token = tf.next();
-    assertTrue("token is null and it shouldn't be", token != null);
-    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + " is not equal to " + "h",
-            new String(token.termBuffer(), 0, token.termLength()).equals("h") == true);
-    assertTrue(token.getPositionIncrement() + " does not equal: " + 0, token.getPositionIncrement() == 0);
-    assertTrue(token.type() + " is not equal to " + WikipediaTokenizer.CATEGORY, token.type().equals(WikipediaTokenizer.CATEGORY) == true);
-    assertTrue(token.startOffset() + " does not equal: " + 124, token.startOffset() == 124);
-    assertTrue(token.endOffset() + " does not equal: " + 125, token.endOffset() == 125);
+    nextToken = tf.next(reusableToken);
+    assertTrue("nextToken is null and it shouldn't be", nextToken != null);
+    assertTrue(nextToken.term() + " is not equal to " + "h",
+            nextToken.term().equals("h") == true);
+    assertTrue(nextToken.getPositionIncrement() + " does not equal: " + 0, nextToken.getPositionIncrement() == 0);
+    assertTrue(nextToken.type() + " is not equal to " + WikipediaTokenizer.CATEGORY, nextToken.type().equals(WikipediaTokenizer.CATEGORY) == true);
+    assertTrue(nextToken.startOffset() + " does not equal: " + 124, nextToken.startOffset() == 124);
+    assertTrue(nextToken.endOffset() + " does not equal: " + 125, nextToken.endOffset() == 125);
 
-    token = tf.next();
-    assertTrue("token is null and it shouldn't be", token != null);
-    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + " is not equal to " + "i",
-            new String(token.termBuffer(), 0, token.termLength()).equals("i") == true);
-    assertTrue(token.getPositionIncrement() + " does not equal: " + 1, token.getPositionIncrement() == 1);
-    assertTrue(token.type() + " is not equal to " + WikipediaTokenizer.CATEGORY, token.type().equals(WikipediaTokenizer.CATEGORY) == true);
-    assertTrue(token.startOffset() + " does not equal: " + 128, token.startOffset() == 128);
-    assertTrue(token.endOffset() + " does not equal: " + 129, token.endOffset() == 129);
-    token = tf.next();
-    assertTrue("token is null and it shouldn't be", token != null);
-    assertTrue(new String(token.termBuffer(), 0, token.termLength()) + " is not equal to " + "j",
-            new String(token.termBuffer(), 0, token.termLength()).equals("j") == true);
-    assertTrue(token.getPositionIncrement() + " does not equal: " + 1, token.getPositionIncrement() == 1);
-    assertTrue(token.type() + " is not equal to " + WikipediaTokenizer.CATEGORY, token.type().equals(WikipediaTokenizer.CATEGORY) == true);
-    assertTrue(token.startOffset() + " does not equal: " + 132, token.startOffset() == 132);
-    assertTrue(token.endOffset() + " does not equal: " + 133, token.endOffset() == 133);
+    nextToken = tf.next(reusableToken);
+    assertTrue("nextToken is null and it shouldn't be", nextToken != null);
+    assertTrue(nextToken.term() + " is not equal to " + "i",
+            nextToken.term().equals("i") == true);
+    assertTrue(nextToken.getPositionIncrement() + " does not equal: " + 1, nextToken.getPositionIncrement() == 1);
+    assertTrue(nextToken.type() + " is not equal to " + WikipediaTokenizer.CATEGORY, nextToken.type().equals(WikipediaTokenizer.CATEGORY) == true);
+    assertTrue(nextToken.startOffset() + " does not equal: " + 128, nextToken.startOffset() == 128);
+    assertTrue(nextToken.endOffset() + " does not equal: " + 129, nextToken.endOffset() == 129);
+    nextToken = tf.next(reusableToken);
+    assertTrue("nextToken is null and it shouldn't be", nextToken != null);
+    assertTrue(nextToken.term() + " is not equal to " + "j",
+            nextToken.term().equals("j") == true);
+    assertTrue(nextToken.getPositionIncrement() + " does not equal: " + 1, nextToken.getPositionIncrement() == 1);
+    assertTrue(nextToken.type() + " is not equal to " + WikipediaTokenizer.CATEGORY, nextToken.type().equals(WikipediaTokenizer.CATEGORY) == true);
+    assertTrue(nextToken.startOffset() + " does not equal: " + 132, nextToken.startOffset() == 132);
+    assertTrue(nextToken.endOffset() + " does not equal: " + 133, nextToken.endOffset() == 133);
 
-    token = tf.next();
-    assertTrue("token is not null and it should be", token == null);
+    nextToken = tf.next(reusableToken);
+    assertTrue("nextToken is not null and it should be", nextToken == null);
 
   }
 }
Index: contrib/wikipedia/src/java/org/apache/lucene/wikipedia/analysis/WikipediaTokenizer.java
===================================================================
--- contrib/wikipedia/src/java/org/apache/lucene/wikipedia/analysis/WikipediaTokenizer.java	(revision 686801)
+++ contrib/wikipedia/src/java/org/apache/lucene/wikipedia/analysis/WikipediaTokenizer.java	(working copy)
@@ -133,7 +133,8 @@
   *
   * @see org.apache.lucene.analysis.TokenStream#next()
   */
-  public Token next(Token result) throws IOException {
+  public Token next(final Token reusableToken) throws IOException {
+    assert reusableToken != null;
     if (tokens != null && tokens.hasNext()){
       return (Token)tokens.next();
     }
@@ -144,22 +145,22 @@
     }
     String type = WikipediaTokenizerImpl.TOKEN_TYPES[tokenType];
     if (tokenOutput == TOKENS_ONLY || untokenizedTypes.contains(type) == false){
-      setupToken(result);
+      setupToken(reusableToken);
     } else if (tokenOutput == UNTOKENIZED_ONLY && untokenizedTypes.contains(type) == true){
-      collapseTokens(result, tokenType);
+      collapseTokens(reusableToken, tokenType);
 
     }
     else if (tokenOutput == BOTH){
       //collapse into a single token, add it to tokens AND output the individual tokens
       //output the untokenized Token first
-      collapseAndSaveTokens(result, tokenType, type);
+      collapseAndSaveTokens(reusableToken, tokenType, type);
     }
-    result.setPositionIncrement(scanner.getPositionIncrement());
-    result.setType(type);
-    return result;
+    reusableToken.setPositionIncrement(scanner.getPositionIncrement());
+    reusableToken.setType(type);
+    return reusableToken;
   }
 
-  private void collapseAndSaveTokens(Token result, int tokenType, String type) throws IOException {
+  private void collapseAndSaveTokens(final Token reusableToken, int tokenType, String type) throws IOException {
     //collapse
     StringBuffer buffer = new StringBuffer(32);
     int numAdded = scanner.setText(buffer);
@@ -188,10 +189,10 @@
     }
     //trim the buffer
     String s = buffer.toString().trim();
-    result.setTermBuffer(s.toCharArray(), 0, s.length());
-    result.setStartOffset(theStart);
-    result.setEndOffset(theStart + s.length());
-    result.setFlags(UNTOKENIZED_TOKEN_FLAG);
+    reusableToken.setTermBuffer(s.toCharArray(), 0, s.length());
+    reusableToken.setStartOffset(theStart);
+    reusableToken.setEndOffset(theStart + s.length());
+    reusableToken.setFlags(UNTOKENIZED_TOKEN_FLAG);
     //The way the loop is written, we will have proceeded to the next token.  We need to pushback the scanner to lastPos
     if (tmpTokType != WikipediaTokenizerImpl.YYEOF){
       scanner.yypushback(scanner.yylength());
@@ -205,7 +206,7 @@
     saved.setType(type);
   }
 
-  private void collapseTokens(Token result, int tokenType) throws IOException {
+  private void collapseTokens(final Token reusableToken, int tokenType) throws IOException {
     //collapse
     StringBuffer buffer = new StringBuffer(32);
     int numAdded = scanner.setText(buffer);
@@ -227,10 +228,10 @@
     }
     //trim the buffer
     String s = buffer.toString().trim();
-    result.setTermBuffer(s.toCharArray(), 0, s.length());
-    result.setStartOffset(theStart);
-    result.setEndOffset(theStart + s.length());
-    result.setFlags(UNTOKENIZED_TOKEN_FLAG);
+    reusableToken.setTermBuffer(s.toCharArray(), 0, s.length());
+    reusableToken.setStartOffset(theStart);
+    reusableToken.setEndOffset(theStart + s.length());
+    reusableToken.setFlags(UNTOKENIZED_TOKEN_FLAG);
     //The way the loop is written, we will have proceeded to the next token.  We need to pushback the scanner to lastPos
     if (tmpTokType != WikipediaTokenizerImpl.YYEOF){
       scanner.yypushback(scanner.yylength());
@@ -239,11 +240,11 @@
     }
   }
 
-  private void setupToken(Token result) {
-    scanner.getText(result);
+  private void setupToken(final Token reusableToken) {
+    scanner.getText(reusableToken);
     final int start = scanner.yychar();
-    result.setStartOffset(start);
-    result.setEndOffset(start + result.termLength());
+    reusableToken.setStartOffset(start);
+    reusableToken.setEndOffset(start + reusableToken.termLength());
   }
 
   /*
Index: contrib/memory/src/test/org/apache/lucene/index/memory/PatternAnalyzerTest.java
===================================================================
--- contrib/memory/src/test/org/apache/lucene/index/memory/PatternAnalyzerTest.java	(revision 686801)
+++ contrib/memory/src/test/org/apache/lucene/index/memory/PatternAnalyzerTest.java	(working copy)
@@ -197,9 +197,9 @@
   
   private List getTokens(TokenStream stream) throws IOException {
     ArrayList tokens = new ArrayList();
-    Token token;
-    while ((token = stream.next()) != null) {
-      tokens.add(token);
+    final Token reusableToken = new Token();
+    for (Token nextToken = stream.next(reusableToken); nextToken != null; nextToken = stream.next(reusableToken)) {
+      tokens.add(nextToken.clone());
     }
     return tokens;
   }
@@ -211,7 +211,7 @@
       for (; i < size; i++) {
         Token t1 = (Token) tokens1.get(i);
         Token t2 = (Token) tokens2.get(i);
-        if (!(t1.termText().equals(t2.termText()))) throw new IllegalStateException("termText");
+        if (!(t1.term().equals(t2.term()))) throw new IllegalStateException("termText");
         if (t1.startOffset() != t2.startOffset()) throw new IllegalStateException("startOffset");
         if (t1.endOffset() != t2.endOffset()) throw new IllegalStateException("endOffset");
         if (!(t1.type().equals(t2.type()))) throw new IllegalStateException("type");
@@ -222,8 +222,8 @@
     catch (IllegalStateException e) {
       if (size > 0) {
         System.out.println("i=" + i + ", size=" + size);
-        System.out.println("t1[size]='" + ((Token) tokens1.get(size-1)).termText() + "'");
-        System.out.println("t2[size]='" + ((Token) tokens2.get(size-1)).termText() + "'");
+        System.out.println("t1[size]='" + ((Token) tokens1.get(size-1)).term() + "'");
+        System.out.println("t2[size]='" + ((Token) tokens2.get(size-1)).term() + "'");
       }
       throw e;
     }
@@ -234,7 +234,7 @@
     String str = "[";
     for (int i=0; i < tokens.size(); i++) {
       Token t1 = (Token) tokens.get(i);
-      str = str + "'" + t1.termText() + "', ";
+      str = str + "'" + t1.term() + "', ";
     }
     return str + "]";
   }
Index: contrib/memory/src/java/org/apache/lucene/index/memory/MemoryIndex.java
===================================================================
--- contrib/memory/src/java/org/apache/lucene/index/memory/MemoryIndex.java	(revision 686801)
+++ contrib/memory/src/java/org/apache/lucene/index/memory/MemoryIndex.java	(working copy)
@@ -275,7 +275,8 @@
     return new TokenStream() {
       private Iterator iter = keywords.iterator();
       private int start = 0;
-      public Token next() {
+      public Token next(final Token reusableToken) {
+        assert reusableToken != null;
         if (!iter.hasNext()) return null;
         
         Object obj = iter.next();
@@ -283,9 +284,9 @@
           throw new IllegalArgumentException("keyword must not be null");
         
         String term = obj.toString();
-        Token token = new Token(term, start, start + term.length());
+        reusableToken.reinit(term, start, start+reusableToken.termLength());
         start += term.length() + 1; // separate words by 1 (blank) character
-        return token;
+        return reusableToken;
       }
     };
   }
@@ -349,14 +350,13 @@
       HashMap terms = new HashMap();
       int numTokens = 0;
       int pos = -1;
-      Token token;
-      
-      while ((token = stream.next()) != null) {
-        String term = token.termText();
+      final Token reusableToken = new Token();
+      for (Token nextToken = stream.next(reusableToken); nextToken != null; nextToken = stream.next(reusableToken)) {
+        String term = nextToken.term();
         if (term.length() == 0) continue; // nothing to do
 //        if (DEBUG) System.err.println("token='" + term + "'");
         numTokens++;
-        pos += token.getPositionIncrement();
+        pos += nextToken.getPositionIncrement();
         
         ArrayIntList positions = (ArrayIntList) terms.get(term);
         if (positions == null) { // term not seen before
@@ -366,7 +366,7 @@
         if (stride == 1) {
           positions.add(pos);
         } else {
-          positions.add(pos, token.startOffset(), token.endOffset());
+          positions.add(pos, nextToken.startOffset(), nextToken.endOffset());
         }
       }
       
Index: contrib/memory/src/java/org/apache/lucene/index/memory/AnalyzerUtil.java
===================================================================
--- contrib/memory/src/java/org/apache/lucene/index/memory/AnalyzerUtil.java	(revision 686801)
+++ contrib/memory/src/java/org/apache/lucene/index/memory/AnalyzerUtil.java	(working copy)
@@ -73,10 +73,11 @@
         return new TokenFilter(child.tokenStream(fieldName, reader)) {
           private int position = -1;
           
-          public Token next() throws IOException {
-            Token token = input.next(); // from filter super class
-            log.println(toString(token));
-            return token;
+          public Token next(final Token reusableToken) throws IOException {
+            assert reusableToken != null;
+            Token nextToken = input.next(reusableToken); // from filter super class
+            log.println(toString(nextToken));
+            return nextToken;
           }
           
           private String toString(Token token) {
@@ -84,7 +85,7 @@
             
             position += token.getPositionIncrement();
             return "[" + logName + ":" + position + ":" + fieldName + ":"
-                + token.termText() + ":" + token.startOffset()
+                + token.term() + ":" + token.startOffset()
                 + "-" + token.endOffset() + ":" + token.type()
                 + "]";
           }         
@@ -121,8 +122,9 @@
         return new TokenFilter(child.tokenStream(fieldName, reader)) {
           private int todo = maxTokens;
           
-          public Token next() throws IOException {
-            return --todo >= 0 ? input.next() : null;
+          public Token next(final Token reusableToken) throws IOException {
+            assert reusableToken != null;
+            return --todo >= 0 ? input.next(reusableToken) : null;
           }
         };
       }
@@ -239,10 +241,11 @@
           final ArrayList tokens2 = new ArrayList();
           TokenStream tokenStream = new TokenFilter(child.tokenStream(fieldName, reader)) {
 
-            public Token next() throws IOException {
-              Token token = input.next(); // from filter super class
-              if (token != null) tokens2.add(token);
-              return token;
+            public Token next(final Token reusableToken) throws IOException {
+              assert reusableToken != null;
+              Token nextToken = input.next(reusableToken); // from filter super class
+              if (nextToken != null) tokens2.add(nextToken.clone());
+              return nextToken;
             }
           };
           
@@ -253,7 +256,8 @@
 
             private Iterator iter = tokens.iterator();
 
-            public Token next() {
+            public Token next(Token token) {
+              assert token != null;
               if (!iter.hasNext()) return null;
               return (Token) iter.next();
             }
@@ -300,12 +304,12 @@
     HashMap map = new HashMap();
     TokenStream stream = analyzer.tokenStream("", new StringReader(text));
     try {
-      Token token;
-      while ((token = stream.next()) != null) {
-        MutableInteger freq = (MutableInteger) map.get(token.termText());
+      final Token reusableToken = new Token();
+      for (Token nextToken = stream.next(reusableToken); nextToken != null; nextToken = stream.next(reusableToken)) {
+        MutableInteger freq = (MutableInteger) map.get(nextToken.term());
         if (freq == null) {
           freq = new MutableInteger(1);
-          map.put(token.termText(), freq);
+          map.put(nextToken.term(), freq);
         } else {
           freq.setValue(freq.intValue() + 1);
         }
Index: contrib/memory/src/java/org/apache/lucene/index/memory/PatternAnalyzer.java
===================================================================
--- contrib/memory/src/java/org/apache/lucene/index/memory/PatternAnalyzer.java	(revision 686801)
+++ contrib/memory/src/java/org/apache/lucene/index/memory/PatternAnalyzer.java	(working copy)
@@ -334,7 +334,8 @@
       this.toLowerCase = toLowerCase;
     }
 
-    public Token next() {
+    public Token next(final Token reusableToken) {
+      assert reusableToken != null;
       if (matcher == null) return null;
       
       while (true) { // loop takes care of leading and trailing boundary cases
@@ -352,7 +353,7 @@
         if (start != end) { // non-empty match (header/trailer)
           String text = str.substring(start, end);
           if (toLowerCase) text = text.toLowerCase(locale);
-          return new Token(text, start, end);
+          return reusableToken.reinit(text, start, end);
         }
         if (!isMatch) return null;
       }
@@ -384,7 +385,8 @@
       this.stopWords = stopWords;
     }
 
-    public Token next() {
+    public Token next(final Token reusableToken) {
+      assert reusableToken != null;
       // cache loop instance vars (performance)
       String s = str;
       int len = s.length();
@@ -422,7 +424,11 @@
       } while (text != null && isStopWord(text));
       
       pos = i;
-      return text != null ? new Token(text, start, i) : null;
+      if (text == null)
+      {
+        return null;
+      }
+      return reusableToken.reinit(text, start, i);
     }
     
     private boolean isTokenChar(char c, boolean isLetter) {
Index: contrib/memory/src/java/org/apache/lucene/index/memory/SynonymTokenFilter.java
===================================================================
--- contrib/memory/src/java/org/apache/lucene/index/memory/SynonymTokenFilter.java	(revision 686801)
+++ contrib/memory/src/java/org/apache/lucene/index/memory/SynonymTokenFilter.java	(working copy)
@@ -68,48 +68,51 @@
   }
   
   /** Returns the next token in the stream, or null at EOS. */
-  public Token next() throws IOException {
-    Token token;
+  public Token next(final Token reusableToken) throws IOException {
+    assert reusableToken != null;
     while (todo > 0 && index < stack.length) { // pop from stack
-      token = createToken(stack[index++], current);
-      if (token != null) {
+      Token nextToken = createToken(stack[index++], current, reusableToken);
+      if (nextToken != null) {
         todo--;
-        return token;
+        return nextToken;
       }
     }
     
-    token = input.next();
-    if (token == null) return null; // EOS; iterator exhausted
+    Token nextToken = input.next(reusableToken);
+    if (nextToken == null) return null; // EOS; iterator exhausted
     
-    stack = synonyms.getSynonyms(token.termText()); // push onto stack
+    stack = synonyms.getSynonyms(nextToken.term()); // push onto stack
     if (stack.length > maxSynonyms) randomize(stack);
     index = 0;
-    current = token;
+    current = (Token) nextToken.clone();
     todo = maxSynonyms;
-    return token;
+    return nextToken;
   }
   
   /**
    * Creates and returns a token for the given synonym of the current input
-   * token; Override for custom (stateless or stateful) behaviour, if desired.
+   * token; Override for custom (stateless or stateful) behavior, if desired.
    * 
    * @param synonym 
    *            a synonym for the current token's term
    * @param current
    *            the current token from the underlying child stream
+   * @param reusableToken
+   *            the token to reuse
    * @return a new token, or null to indicate that the given synonym should be
    *         ignored
    */
-  protected Token createToken(String synonym, Token current) {
-    Token token = new Token(
-      synonym, current.startOffset(), current.endOffset(), SYNONYM_TOKEN_TYPE);
-    token.setPositionIncrement(0);
-    return token;
+  protected Token createToken(String synonym, Token current, final Token reusableToken) {
+    reusableToken.reinit(current, synonym);
+    reusableToken.setTermBuffer(synonym);
+    reusableToken.setType(SYNONYM_TOKEN_TYPE);
+    reusableToken.setPositionIncrement(0);
+    return reusableToken;
   }
   
   /**
    * Randomize synonyms to later sample a subset. Uses constant random seed
-   * for reproducability. Uses "DRand", a simple, fast, uniform pseudo-random
+   * for reproducibility. Uses "DRand", a simple, fast, uniform pseudo-random
    * number generator with medium statistical quality (multiplicative
    * congruential method), producing integers in the range [Integer.MIN_VALUE,
    * Integer.MAX_VALUE].
Index: contrib/lucli/src/java/lucli/LuceneMethods.java
===================================================================
--- contrib/lucli/src/java/lucli/LuceneMethods.java	(revision 686801)
+++ contrib/lucli/src/java/lucli/LuceneMethods.java	(working copy)
@@ -279,6 +279,7 @@
 
     Analyzer analyzer = new StandardAnalyzer();
     Enumeration fields = doc.fields();
+    final Token reusableToken = new Token();
     while (fields.hasMoreElements()) {
       Field field = (Field) fields.nextElement();
       String fieldName = field.name();
@@ -299,10 +300,10 @@
           // Tokenize field and add to postingTable
           TokenStream stream = analyzer.tokenStream(fieldName, reader);
           try {
-            for (Token t = stream.next(); t != null; t = stream.next()) {
-              position += (t.getPositionIncrement() - 1);
+            for (Token nextToken = stream.next(reusableToken); nextToken != null; nextToken = stream.next(reusableToken)) {
+              position += (nextToken.getPositionIncrement() - 1);
               position++;
-              String name = t.termText();
+              String name = nextToken.term();
               Integer Count = (Integer) tokenHash.get(name);
               if (Count == null) { // not in there yet
                 tokenHash.put(name, new Integer(1)); //first one
Index: contrib/analyzers/src/test/org/apache/lucene/analysis/cn/TestChineseTokenizer.java
===================================================================
--- contrib/analyzers/src/test/org/apache/lucene/analysis/cn/TestChineseTokenizer.java	(revision 686801)
+++ contrib/analyzers/src/test/org/apache/lucene/analysis/cn/TestChineseTokenizer.java	(working copy)
@@ -33,14 +33,13 @@
     {
         String s = "a天b";
         ChineseTokenizer tokenizer = new ChineseTokenizer(new StringReader(s));
-        Token token;
 
         int correctStartOffset = 0;
         int correctEndOffset = 1;
-        while ((token = tokenizer.next()) != null)
-        {
-            assertEquals(correctStartOffset, token.startOffset());
-            assertEquals(correctEndOffset, token.endOffset());
+        final Token reusableToken = new Token();
+        for (Token nextToken = tokenizer.next(reusableToken); nextToken != null; nextToken = tokenizer.next(reusableToken)) {
+            assertEquals(correctStartOffset, nextToken.startOffset());
+            assertEquals(correctEndOffset, nextToken.endOffset());
             correctStartOffset++;
             correctEndOffset++;
         }
Index: contrib/analyzers/src/test/org/apache/lucene/analysis/el/GreekAnalyzerTest.java
===================================================================
--- contrib/analyzers/src/test/org/apache/lucene/analysis/el/GreekAnalyzerTest.java	(revision 686801)
+++ contrib/analyzers/src/test/org/apache/lucene/analysis/el/GreekAnalyzerTest.java	(working copy)
@@ -42,12 +42,13 @@
 	 */
 	private void assertAnalyzesTo(Analyzer a, String input, String[] output) throws Exception {
 		TokenStream ts = a.tokenStream("dummy", new StringReader(input));
+                final Token reusableToken = new Token();
 		for (int i=0; i<output.length; i++) {
-			Token t = ts.next();
-			assertNotNull(t);
-			assertEquals(t.termText(), output[i]);
+		        Token nextToken = ts.next(reusableToken);
+			assertNotNull(nextToken);
+			assertEquals(nextToken.term(), output[i]);
 		}
-		assertNull(ts.next());
+		assertNull(ts.next(reusableToken));
 		ts.close();
 	}
 
Index: contrib/analyzers/src/test/org/apache/lucene/analysis/miscellaneous/TestPrefixAwareTokenFilter.java
===================================================================
--- contrib/analyzers/src/test/org/apache/lucene/analysis/miscellaneous/TestPrefixAwareTokenFilter.java	(revision 686801)
+++ contrib/analyzers/src/test/org/apache/lucene/analysis/miscellaneous/TestPrefixAwareTokenFilter.java	(working copy)
@@ -32,33 +32,40 @@
     PrefixAwareTokenFilter ts;
 
     ts = new PrefixAwareTokenFilter(
-        new SingleTokenTokenStream(new Token("a", 0, 1)),
-        new SingleTokenTokenStream(new Token("b", 0, 1)));
-    assertNext(ts, "a", 0, 1);
-    assertNext(ts, "b", 1, 2);
-    assertNull(ts.next());
+        new SingleTokenTokenStream(createToken("a", 0, 1)),
+        new SingleTokenTokenStream(createToken("b", 0, 1)));
+    final Token reusableToken = new Token();
+    assertNext(ts, reusableToken, "a", 0, 1);
+    assertNext(ts, reusableToken, "b", 1, 2);
+    assertNull(ts.next(reusableToken));
 
 
     // prefix and suffix using 2x prefix
 
-    ts = new PrefixAwareTokenFilter(new SingleTokenTokenStream(new Token("^", 0, 0)), new WhitespaceTokenizer(new StringReader("hello world")));
-    ts = new PrefixAwareTokenFilter(ts, new SingleTokenTokenStream(new Token("$", 0, 0)));
+    ts = new PrefixAwareTokenFilter(new SingleTokenTokenStream(createToken("^", 0, 0)), new WhitespaceTokenizer(new StringReader("hello world")));
+    ts = new PrefixAwareTokenFilter(ts, new SingleTokenTokenStream(createToken("$", 0, 0)));
 
-    assertNext(ts, "^", 0, 0);
-    assertNext(ts, "hello", 0, 5);
-    assertNext(ts, "world", 6, 11);
-    assertNext(ts, "$", 11, 11);
-    assertNull(ts.next());
+    assertNext(ts, reusableToken, "^", 0, 0);
+    assertNext(ts, reusableToken, "hello", 0, 5);
+    assertNext(ts, reusableToken, "world", 6, 11);
+    assertNext(ts, reusableToken, "$", 11, 11);
+    assertNull(ts.next(reusableToken));
   }
 
 
-  private Token assertNext(TokenStream ts, String text, int startOffset, int endOffset) throws IOException {
-    Token token = ts.next();
-    assertNotNull(token);
-    assertEquals(text, new String(token.termBuffer(), 0, token.termLength()));
-    assertEquals(startOffset, token.startOffset());
-    assertEquals(endOffset, token.endOffset());
+  private Token assertNext(TokenStream ts, final Token reusableToken, String text, int startOffset, int endOffset) throws IOException {
+    Token nextToken = ts.next(reusableToken);
+    assertNotNull(nextToken);
+    assertEquals(text, nextToken.term());
+    assertEquals(startOffset, nextToken.startOffset());
+    assertEquals(endOffset, nextToken.endOffset());
+    return nextToken;
+  }
+
+  private static Token createToken(String term, int start, int offset)
+  {
+    Token token = new Token(start, offset);
+    token.setTermBuffer(term);
     return token;
   }
-
 }
Index: contrib/analyzers/src/test/org/apache/lucene/analysis/miscellaneous/TestSingleTokenTokenFilter.java
===================================================================
--- contrib/analyzers/src/test/org/apache/lucene/analysis/miscellaneous/TestSingleTokenTokenFilter.java	(revision 686801)
+++ contrib/analyzers/src/test/org/apache/lucene/analysis/miscellaneous/TestSingleTokenTokenFilter.java	(working copy)
@@ -17,23 +17,20 @@
  * limitations under the License.
  */
 
-import junit.framework.TestCase;
-
 import java.io.IOException;
 
 import org.apache.lucene.analysis.Token;
+import org.apache.lucene.util.LuceneTestCase;
 
-public class TestSingleTokenTokenFilter extends TestCase {
+public class TestSingleTokenTokenFilter extends LuceneTestCase {
 
   public void test() throws IOException {
-
     Token token = new Token();
 
     SingleTokenTokenStream ts = new SingleTokenTokenStream(token);
 
-    assertEquals(token, ts.next());
-    assertNull(ts.next());
-
+    final Token reusableToken = new Token();
+    assertEquals(token, ts.next(reusableToken));
+    assertNull(ts.next(reusableToken));
   }
-
 }
Index: contrib/analyzers/src/test/org/apache/lucene/analysis/miscellaneous/TestPrefixAndSuffixAwareTokenFilter.java
===================================================================
--- contrib/analyzers/src/test/org/apache/lucene/analysis/miscellaneous/TestPrefixAndSuffixAwareTokenFilter.java	(revision 686801)
+++ contrib/analyzers/src/test/org/apache/lucene/analysis/miscellaneous/TestPrefixAndSuffixAwareTokenFilter.java	(working copy)
@@ -30,25 +30,32 @@
   public void test() throws IOException {
 
     PrefixAndSuffixAwareTokenFilter ts = new PrefixAndSuffixAwareTokenFilter(
-        new SingleTokenTokenStream(new Token("^", 0, 0)),
+        new SingleTokenTokenStream(createToken("^", 0, 0)),
         new WhitespaceTokenizer(new StringReader("hello world")),
-        new SingleTokenTokenStream(new Token("$", 0, 0)));
+        new SingleTokenTokenStream(createToken("$", 0, 0)));
 
-    assertNext(ts, "^", 0, 0);
-    assertNext(ts, "hello", 0, 5);
-    assertNext(ts, "world", 6, 11);
-    assertNext(ts, "$", 11, 11);
-    assertNull(ts.next());
+    Token token = new Token();
+    assertNext(ts, token, "^", 0, 0);
+    assertNext(ts, token, "hello", 0, 5);
+    assertNext(ts, token, "world", 6, 11);
+    assertNext(ts, token, "$", 11, 11);
+    assertNull(ts.next(token));
   }
 
 
-  private Token assertNext(TokenStream ts, String text, int startOffset, int endOffset) throws IOException {
-    Token token = ts.next();
-    assertNotNull(token);
-    assertEquals(text, new String(token.termBuffer(), 0, token.termLength()));
-    assertEquals(startOffset, token.startOffset());
-    assertEquals(endOffset, token.endOffset());
+  private Token assertNext(TokenStream ts, final Token reusableToken, String text, int startOffset, int endOffset) throws IOException {
+    Token nextToken = ts.next(reusableToken);
+    assertNotNull(nextToken);
+    assertEquals(text, nextToken.term());
+    assertEquals(startOffset, nextToken.startOffset());
+    assertEquals(endOffset, nextToken.endOffset());
+    return nextToken;
+  }
+
+  private static Token createToken(String term, int start, int offset)
+  {
+    Token token = new Token(start, offset);
+    token.setTermBuffer(term);
     return token;
   }
-
 }
Index: contrib/analyzers/src/test/org/apache/lucene/analysis/compound/TestCompoundWordTokenFilter.java
===================================================================
--- contrib/analyzers/src/test/org/apache/lucene/analysis/compound/TestCompoundWordTokenFilter.java	(revision 686801)
+++ contrib/analyzers/src/test/org/apache/lucene/analysis/compound/TestCompoundWordTokenFilter.java	(working copy)
@@ -153,15 +153,16 @@
 
   private void assertFiltersTo(TokenFilter tf, String[] s, int[] startOffset,
       int[] endOffset, int[] posIncr) throws Exception {
+    final Token reusableToken = new Token();
     for (int i = 0; i < s.length; ++i) {
-      Token t = tf.next();
-      assertNotNull(t);
-      assertEquals(s[i], new String(t.termBuffer(), 0, t.termLength()));
-      assertEquals(startOffset[i], t.startOffset());
-      assertEquals(endOffset[i], t.endOffset());
-      assertEquals(posIncr[i], t.getPositionIncrement());
+      Token nextToken = tf.next(reusableToken);
+      assertNotNull(nextToken);
+      assertEquals(s[i], nextToken.term());
+      assertEquals(startOffset[i], nextToken.startOffset());
+      assertEquals(endOffset[i], nextToken.endOffset());
+      assertEquals(posIncr[i], nextToken.getPositionIncrement());
     }
-    assertNull(tf.next());
+    assertNull(tf.next(reusableToken));
   }
 
   private void getHyphenationPatternFileContents() {
Index: contrib/analyzers/src/test/org/apache/lucene/analysis/ngram/NGramTokenizerTest.java
===================================================================
--- contrib/analyzers/src/test/org/apache/lucene/analysis/ngram/NGramTokenizerTest.java	(revision 686801)
+++ contrib/analyzers/src/test/org/apache/lucene/analysis/ngram/NGramTokenizerTest.java	(working copy)
@@ -59,16 +59,13 @@
     public void testUnigrams() throws Exception {
         NGramTokenizer tokenizer = new NGramTokenizer(input, 1, 1);
         
-        Token token = null;
-        do { 
-            token = tokenizer.next();
-            if (token != null) {
-                tokens.add(token.toString());
-//                System.out.println(token.termText());
-//                System.out.println(token);
-//                Thread.sleep(1000);
-            }
-        } while (token != null);
+        final Token reusableToken = new Token();
+        for (Token nextToken = tokenizer.next(reusableToken); nextToken != null; nextToken = tokenizer.next(reusableToken)) {
+          tokens.add(nextToken.toString());
+//        System.out.println(token.term());
+//        System.out.println(token);
+//        Thread.sleep(1000);
+      }
 
         assertEquals(5, tokens.size());
         ArrayList exp = new ArrayList();
@@ -78,17 +75,13 @@
 
     public void testBigrams() throws Exception {
         NGramTokenizer tokenizer = new NGramTokenizer(input, 2, 2);
-        
-        Token token = null;
-        do { 
-            token = tokenizer.next();
-            if (token != null) {
-                tokens.add(token.toString());
-//                System.out.println(token.termText());
-//                System.out.println(token);
-//                Thread.sleep(1000);
-            }
-        } while (token != null);
+        final Token reusableToken = new Token();
+        for (Token nextToken = tokenizer.next(reusableToken); nextToken != null; nextToken = tokenizer.next(reusableToken)) {
+          tokens.add(nextToken.toString());
+//        System.out.println(token.term());
+//        System.out.println(token);
+//        Thread.sleep(1000);
+      }
 
         assertEquals(4, tokens.size());
         ArrayList exp = new ArrayList();
@@ -98,17 +91,13 @@
 
     public void testNgrams() throws Exception {
         NGramTokenizer tokenizer = new NGramTokenizer(input, 1, 3);
-        
-        Token token = null;
-        do { 
-            token = tokenizer.next();
-            if (token != null) {
-                tokens.add(token.toString());
-//                System.out.println(token.termText());
-//                System.out.println(token);
-//                Thread.sleep(1000);
-            }
-        } while (token != null);
+        final Token reusableToken = new Token();
+        for (Token nextToken = tokenizer.next(reusableToken); nextToken != null; nextToken = tokenizer.next(reusableToken)) {
+          tokens.add(nextToken.toString());
+//        System.out.println(token.term());
+//        System.out.println(token);
+//        Thread.sleep(1000);
+      }
 
         assertEquals(12, tokens.size());
         ArrayList exp = new ArrayList();
@@ -120,18 +109,15 @@
 
     public void testOversizedNgrams() throws Exception {
         NGramTokenizer tokenizer = new NGramTokenizer(input, 6, 7);
-        
-        Token token = null;
-        do { 
-            token = tokenizer.next();
-            if (token != null) {
-                tokens.add(token.toString());
-//                System.out.println(token.termText());
-//                System.out.println(token);
-//                Thread.sleep(1000);
-            }
-        } while (token != null);
 
+        final Token reusableToken = new Token();
+        for (Token nextToken = tokenizer.next(reusableToken); nextToken != null; nextToken = tokenizer.next(reusableToken)) {
+          tokens.add(nextToken.toString());
+//        System.out.println(token.term());
+//        System.out.println(token);
+//        Thread.sleep(1000);
+      }
+
         assertTrue(tokens.isEmpty());
     }
 }
Index: contrib/analyzers/src/test/org/apache/lucene/analysis/ngram/EdgeNGramTokenFilterTest.java
===================================================================
--- contrib/analyzers/src/test/org/apache/lucene/analysis/ngram/EdgeNGramTokenFilterTest.java	(revision 686801)
+++ contrib/analyzers/src/test/org/apache/lucene/analysis/ngram/EdgeNGramTokenFilterTest.java	(working copy)
@@ -68,52 +68,46 @@
 
   public void testFrontUnigram() throws Exception {
     EdgeNGramTokenFilter tokenizer = new EdgeNGramTokenFilter(input, EdgeNGramTokenFilter.Side.FRONT, 1, 1);
-    Token token = null;
-    token = tokenizer.next();
-    assertEquals("(a,0,1)", token.toString());
-    token = tokenizer.next();
-    assertNull(token);
+    final Token reusableToken = new Token();
+    Token nextToken = tokenizer.next(reusableToken);
+    assertEquals("(a,0,1)", nextToken.toString());
+    assertNull(tokenizer.next(reusableToken));
   }
 
   public void testBackUnigram() throws Exception {
     EdgeNGramTokenFilter tokenizer = new EdgeNGramTokenFilter(input, EdgeNGramTokenFilter.Side.BACK, 1, 1);
-    Token token = null;
-    token = tokenizer.next();
-    assertEquals("(e,4,5)", token.toString());
-    token = tokenizer.next();
-    assertNull(token);
+    final Token reusableToken = new Token();
+    Token nextToken = tokenizer.next(reusableToken);
+    assertEquals("(e,4,5)", nextToken.toString());
+    assertNull(tokenizer.next(reusableToken));
   }
 
   public void testOversizedNgrams() throws Exception {
     EdgeNGramTokenFilter tokenizer = new EdgeNGramTokenFilter(input, EdgeNGramTokenFilter.Side.FRONT, 6, 6);
-    Token token = null;
-    token = tokenizer.next();
-    assertNull(token);
+    assertNull(tokenizer.next(new Token()));
   }
 
   public void testFrontRangeOfNgrams() throws Exception {
     EdgeNGramTokenFilter tokenizer = new EdgeNGramTokenFilter(input, EdgeNGramTokenFilter.Side.FRONT, 1, 3);
-    Token token = null;
-    token = tokenizer.next();
-    assertEquals("(a,0,1)", token.toString());
-    token = tokenizer.next();
-    assertEquals("(ab,0,2)", token.toString());
-    token = tokenizer.next();
-    assertEquals("(abc,0,3)", token.toString());
-    token = tokenizer.next();
-    assertNull(token);
+    final Token reusableToken = new Token();
+    Token nextToken = tokenizer.next(reusableToken);
+    assertEquals("(a,0,1)", nextToken.toString());
+    nextToken = tokenizer.next(reusableToken);
+    assertEquals("(ab,0,2)", nextToken.toString());
+    nextToken = tokenizer.next(reusableToken);
+    assertEquals("(abc,0,3)", nextToken.toString());
+    assertNull(tokenizer.next(reusableToken));
   }
 
   public void testBackRangeOfNgrams() throws Exception {
     EdgeNGramTokenFilter tokenizer = new EdgeNGramTokenFilter(input, EdgeNGramTokenFilter.Side.BACK, 1, 3);
-    Token token = null;
-    token = tokenizer.next();
-    assertEquals("(e,4,5)", token.toString());
-    token = tokenizer.next();
-    assertEquals("(de,3,5)", token.toString());
-    token = tokenizer.next();
-    assertEquals("(cde,2,5)", token.toString());
-    token = tokenizer.next();
-    assertNull(token);
+    final Token reusableToken = new Token();
+    Token nextToken = tokenizer.next(reusableToken);
+    assertEquals("(e,4,5)", nextToken.toString());
+    nextToken = tokenizer.next(reusableToken);
+    assertEquals("(de,3,5)", nextToken.toString());
+    nextToken = tokenizer.next(reusableToken);
+    assertEquals("(cde,2,5)", nextToken.toString());
+    assertNull(tokenizer.next(reusableToken));
   }
 }
Index: contrib/analyzers/src/test/org/apache/lucene/analysis/ngram/EdgeNGramTokenizerTest.java
===================================================================
--- contrib/analyzers/src/test/org/apache/lucene/analysis/ngram/EdgeNGramTokenizerTest.java	(revision 686801)
+++ contrib/analyzers/src/test/org/apache/lucene/analysis/ngram/EdgeNGramTokenizerTest.java	(working copy)
@@ -66,52 +66,46 @@
 
   public void testFrontUnigram() throws Exception {
     EdgeNGramTokenizer tokenizer = new EdgeNGramTokenizer(input, EdgeNGramTokenizer.Side.FRONT, 1, 1);
-    Token token = null;
-    token = tokenizer.next();
-    assertEquals("(a,0,1)", token.toString());
-    token = tokenizer.next();
-    assertNull(token);
+    final Token reusableToken = new Token();
+    Token nextToken = tokenizer.next(reusableToken);
+    assertEquals("(a,0,1)", nextToken.toString());
+    assertNull(tokenizer.next(reusableToken));
   }
 
   public void testBackUnigram() throws Exception {
     EdgeNGramTokenizer tokenizer = new EdgeNGramTokenizer(input, EdgeNGramTokenizer.Side.BACK, 1, 1);
-    Token token = null;
-    token = tokenizer.next();
-    assertEquals("(e,4,5)", token.toString());
-    token = tokenizer.next();
-    assertNull(token);
+    final Token reusableToken = new Token();
+    Token nextToken = tokenizer.next(reusableToken);
+    assertEquals("(e,4,5)", nextToken.toString());
+    assertNull(tokenizer.next(reusableToken));
   }
 
   public void testOversizedNgrams() throws Exception {
     EdgeNGramTokenizer tokenizer = new EdgeNGramTokenizer(input, EdgeNGramTokenizer.Side.FRONT, 6, 6);
-    Token token = null;
-    token = tokenizer.next();
-    assertNull(token);
+    assertNull(tokenizer.next(new Token()));
   }
 
   public void testFrontRangeOfNgrams() throws Exception {
     EdgeNGramTokenizer tokenizer = new EdgeNGramTokenizer(input, EdgeNGramTokenizer.Side.FRONT, 1, 3);
-    Token token = null;
-    token = tokenizer.next();
-    assertEquals("(a,0,1)", token.toString());
-    token = tokenizer.next();
-    assertEquals("(ab,0,2)", token.toString());
-    token = tokenizer.next();
-    assertEquals("(abc,0,3)", token.toString());
-    token = tokenizer.next();
-    assertNull(token);
+    final Token reusableToken = new Token();
+    Token nextToken = tokenizer.next(reusableToken);
+    assertEquals("(a,0,1)", nextToken.toString());
+    nextToken = tokenizer.next(reusableToken);
+    assertEquals("(ab,0,2)", nextToken.toString());
+    nextToken = tokenizer.next(reusableToken);
+    assertEquals("(abc,0,3)", nextToken.toString());
+    assertNull(tokenizer.next(reusableToken));
   }
 
   public void testBackRangeOfNgrams() throws Exception {
     EdgeNGramTokenizer tokenizer = new EdgeNGramTokenizer(input, EdgeNGramTokenizer.Side.BACK, 1, 3);
-    Token token = null;
-    token = tokenizer.next();
-    assertEquals("(e,4,5)", token.toString());
-    token = tokenizer.next();
-    assertEquals("(de,3,5)", token.toString());
-    token = tokenizer.next();
-    assertEquals("(cde,2,5)", token.toString());
-    token = tokenizer.next();
-    assertNull(token);
+    final Token reusableToken = new Token();
+    Token nextToken = tokenizer.next(reusableToken);
+    assertEquals("(e,4,5)", nextToken.toString());
+    nextToken = tokenizer.next(reusableToken);
+    assertEquals("(de,3,5)", nextToken.toString());
+    nextToken = tokenizer.next(reusableToken);
+    assertEquals("(cde,2,5)", nextToken.toString());
+    assertNull(tokenizer.next(reusableToken));
   }
 }
Index: contrib/analyzers/src/test/org/apache/lucene/analysis/ngram/NGramTokenFilterTest.java
===================================================================
--- contrib/analyzers/src/test/org/apache/lucene/analysis/ngram/NGramTokenFilterTest.java	(revision 686801)
+++ contrib/analyzers/src/test/org/apache/lucene/analysis/ngram/NGramTokenFilterTest.java	(working copy)
@@ -60,18 +60,15 @@
 
     public void testUnigrams() throws Exception {
       NGramTokenFilter filter = new NGramTokenFilter(input, 1, 1);
-        
-        Token token = null;
-        do { 
-            token = filter.next();
-            if (token != null) {
-                tokens.add(token.toString());
-//                System.out.println(token.termText());
-//                System.out.println(token);
-//                Thread.sleep(1000);
-            }
-        } while (token != null);
 
+      final Token reusableToken = new Token();
+        for (Token nextToken = filter.next(reusableToken); nextToken != null; nextToken = filter.next(reusableToken)) {
+            tokens.add(nextToken.toString());
+//          System.out.println(token.term());
+//          System.out.println(token);
+//          Thread.sleep(1000);
+        }
+
         assertEquals(5, tokens.size());
         ArrayList exp = new ArrayList();
         exp.add("(a,0,1)"); exp.add("(b,1,2)"); exp.add("(c,2,3)"); exp.add("(d,3,4)"); exp.add("(e,4,5)");
@@ -80,17 +77,13 @@
 
     public void testBigrams() throws Exception {
       NGramTokenFilter filter = new NGramTokenFilter(input, 2, 2);
-        
-        Token token = null;
-        do { 
-            token = filter.next();
-            if (token != null) {
-                tokens.add(token.toString());
-//                System.out.println(token.termText());
-//                System.out.println(token);
-//                Thread.sleep(1000);
-            }
-        } while (token != null);
+      final Token reusableToken = new Token();
+        for (Token nextToken = filter.next(reusableToken); nextToken != null; nextToken = filter.next(reusableToken)) {
+            tokens.add(nextToken.toString());
+//          System.out.println(token.term());
+//          System.out.println(token);
+//          Thread.sleep(1000);
+        }
 
         assertEquals(4, tokens.size());
         ArrayList exp = new ArrayList();
@@ -100,17 +93,13 @@
 
     public void testNgrams() throws Exception {
       NGramTokenFilter filter = new NGramTokenFilter(input, 1, 3);
-        
-        Token token = null;
-        do { 
-            token = filter.next();
-            if (token != null) {
-                tokens.add(token.toString());
-//                System.out.println(token.termText());
-//                System.out.println(token);
-//                Thread.sleep(1000);
-            }
-        } while (token != null);
+      final Token reusableToken = new Token();
+        for (Token nextToken = filter.next(reusableToken); nextToken != null; nextToken = filter.next(reusableToken)) {
+            tokens.add(nextToken.toString());
+//          System.out.println(token.term());
+//          System.out.println(token);
+//          Thread.sleep(1000);
+        }
 
         assertEquals(12, tokens.size());
         ArrayList exp = new ArrayList();
@@ -122,17 +111,13 @@
 
     public void testOversizedNgrams() throws Exception {
       NGramTokenFilter filter = new NGramTokenFilter(input, 6, 7);
-        
-        Token token = null;
-        do { 
-            token = filter.next();
-            if (token != null) {
-                tokens.add(token.toString());
-//                System.out.println(token.termText());
-//                System.out.println(token);
-//                Thread.sleep(1000);
-            }
-        } while (token != null);
+      final Token reusableToken = new Token();
+        for (Token nextToken = filter.next(reusableToken); nextToken != null; nextToken = filter.next(reusableToken)) {
+            tokens.add(nextToken.toString());
+//          System.out.println(token.term());
+//          System.out.println(token);
+//          Thread.sleep(1000);
+        }
 
         assertTrue(tokens.isEmpty());
     }
Index: contrib/analyzers/src/test/org/apache/lucene/analysis/ru/TestRussianAnalyzer.java
===================================================================
--- contrib/analyzers/src/test/org/apache/lucene/analysis/ru/TestRussianAnalyzer.java	(revision 686801)
+++ contrib/analyzers/src/test/org/apache/lucene/analysis/ru/TestRussianAnalyzer.java	(working copy)
@@ -17,12 +17,17 @@
  * limitations under the License.
  */
 
+import java.io.File;
+import java.io.FileInputStream;
+import java.io.IOException;
+import java.io.InputStreamReader;
+import java.io.Reader;
+import java.io.StringReader;
+
 import junit.framework.TestCase;
 
-import java.io.*;
-
+import org.apache.lucene.analysis.Token;
 import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.Token;
 
 /**
  * Test case for RussianAnalyzer.
@@ -72,22 +77,26 @@
                 sampleUnicode,
                 RussianCharsets.UnicodeRussian);
 
+        final Token reusableToken = new Token();
+        final Token reusableSampleToken = new Token();
+        Token nextToken;
+        Token nextSampleToken;
         for (;;)
         {
-            Token token = in.next();
+            nextToken = in.next(reusableToken);
 
-            if (token == null)
+            if (nextToken == null)
             {
                 break;
             }
 
-            Token sampleToken = sample.next();
+            nextSampleToken = sample.next(reusableSampleToken);
             assertEquals(
                 "Unicode",
-                token.termText(),
-                sampleToken == null
+                nextToken.term(),
+                nextSampleToken == null
                 ? null
-                : sampleToken.termText());
+                : nextSampleToken.term());
         }
 
         inWords.close();
@@ -109,22 +118,26 @@
                 sampleKOI8,
                 RussianCharsets.KOI8);
 
+        final Token reusableToken = new Token();
+        final Token reusableSampleToken = new Token();
+        Token nextToken;
+        Token nextSampleToken;
         for (;;)
         {
-            Token token = in.next();
+            nextToken = in.next(reusableToken);
 
-            if (token == null)
+            if (nextToken == null)
             {
                 break;
             }
 
-            Token sampleToken = sample.next();
+            nextSampleToken = sample.next(reusableSampleToken);
             assertEquals(
                 "KOI8",
-                token.termText(),
-                sampleToken == null
+                nextToken.term(),
+                nextSampleToken == null
                 ? null
-                : sampleToken.termText());
+                : nextSampleToken.term());
 
         }
 
@@ -146,22 +159,26 @@
                 sample1251,
                 RussianCharsets.CP1251);
 
+        final Token reusableToken = new Token();
+        final Token reusableSampleToken = new Token();
+        Token nextToken;
+        Token nextSampleToken;
         for (;;)
         {
-            Token token = in.next();
+          nextToken = in.next(reusableToken);
 
-            if (token == null)
+            if (nextToken == null)
             {
                 break;
             }
 
-            Token sampleToken = sample.next();
+            nextSampleToken = sample.next(reusableSampleToken);
             assertEquals(
                 "1251",
-                token.termText(),
-                sampleToken == null
+                nextToken.term(),
+                nextSampleToken == null
                 ? null
-                : sampleToken.termText());
+                : nextSampleToken.term());
 
         }
 
@@ -175,9 +192,10 @@
         RussianAnalyzer ra = new RussianAnalyzer();
         TokenStream stream = ra.tokenStream("", reader);
 
+        final Token reusableToken = new Token();
         try {
-            assertEquals("text", stream.next().termText());
-            assertNotNull("RussianAnalyzer's tokenizer skips numbers from input text", stream.next());
+            assertEquals("text", stream.next(reusableToken).term());
+            assertNotNull("RussianAnalyzer's tokenizer skips numbers from input text", stream.next(reusableToken));
         }
         catch (IOException e)
         {
Index: contrib/analyzers/src/test/org/apache/lucene/analysis/fr/TestElision.java
===================================================================
--- contrib/analyzers/src/test/org/apache/lucene/analysis/fr/TestElision.java	(revision 686801)
+++ contrib/analyzers/src/test/org/apache/lucene/analysis/fr/TestElision.java	(working copy)
@@ -53,13 +53,9 @@
   private List filtre(TokenFilter filter) {
     List tas = new ArrayList();
     try {
-      boolean encore = true;
-      Token token;
-      while (encore) {
-        token = filter.next();
-        encore = token != null;
-        if (token != null)
-          tas.add(token.termText());
+      final Token reusableToken = new Token();
+      for (Token nextToken = filter.next(reusableToken); nextToken != null; nextToken = filter.next(reusableToken)) {
+        tas.add(nextToken.term());
       }
     } catch (IOException e) {
       e.printStackTrace();
Index: contrib/analyzers/src/test/org/apache/lucene/analysis/fr/TestFrenchAnalyzer.java
===================================================================
--- contrib/analyzers/src/test/org/apache/lucene/analysis/fr/TestFrenchAnalyzer.java	(revision 686801)
+++ contrib/analyzers/src/test/org/apache/lucene/analysis/fr/TestFrenchAnalyzer.java	(working copy)
@@ -77,12 +77,13 @@
 
 		TokenStream ts = a.tokenStream("dummy", new StringReader(input));
 
+                final Token reusableToken = new Token();
 		for (int i = 0; i < output.length; i++) {
-			Token t = ts.next();
-			assertNotNull(t);
-			assertEquals(t.termText(), output[i]);
+			Token nextToken = ts.next(reusableToken);
+			assertNotNull(nextToken);
+			assertEquals(nextToken.term(), output[i]);
 		}
-		assertNull(ts.next());
+		assertNull(ts.next(reusableToken));
 		ts.close();
 	}
 
Index: contrib/analyzers/src/test/org/apache/lucene/analysis/sinks/TokenTypeSinkTokenizerTest.java
===================================================================
--- contrib/analyzers/src/test/org/apache/lucene/analysis/sinks/TokenTypeSinkTokenizerTest.java	(revision 686801)
+++ contrib/analyzers/src/test/org/apache/lucene/analysis/sinks/TokenTypeSinkTokenizerTest.java	(working copy)
@@ -16,13 +16,17 @@
  * limitations under the License.
  */
 
-import junit.framework.TestCase;
-import org.apache.lucene.analysis.*;
-import org.apache.lucene.analysis.payloads.NumericPayloadTokenFilter;
-
 import java.io.IOException;
 import java.io.StringReader;
 
+import junit.framework.TestCase;
+
+import org.apache.lucene.analysis.TeeTokenFilter;
+import org.apache.lucene.analysis.Token;
+import org.apache.lucene.analysis.TokenFilter;
+import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.analysis.WhitespaceTokenizer;
+
 public class TokenTypeSinkTokenizerTest extends TestCase {
 
 
@@ -42,14 +46,14 @@
     String test = "The quick red fox jumped over the lazy brown dogs";
 
     TeeTokenFilter ttf = new TeeTokenFilter(new WordTokenFilter(new WhitespaceTokenizer(new StringReader(test))), sink);
-    Token tok = new Token();
     boolean seenDogs = false;
-    while ((tok = ttf.next(tok)) != null) {
-      if (tok.termText().equals("dogs")) {
+    final Token reusableToken = new Token();
+    for (Token nextToken = ttf.next(reusableToken); nextToken != null; nextToken = ttf.next(reusableToken)) {
+      if (nextToken.term().equals("dogs")) {
         seenDogs = true;
-        assertTrue(tok.type() + " is not equal to " + "D", tok.type().equals("D") == true);
+        assertTrue(nextToken.type() + " is not equal to " + "D", nextToken.type().equals("D") == true);
       } else {
-        assertTrue(tok.type() + " is not null and it should be", tok.type().equals("word"));
+        assertTrue(nextToken.type() + " is not null and it should be", nextToken.type().equals("word"));
       }
     }
     assertTrue(seenDogs + " does not equal: " + true, seenDogs == true);
@@ -61,12 +65,13 @@
       super(input);
     }
 
-    public Token next(Token result) throws IOException {
-      result = input.next(result);
-      if (result != null && result.termText().equals("dogs")) {
-        result.setType("D");
+    public Token next(final Token reusableToken) throws IOException {
+      assert reusableToken != null;
+      Token nextToken = input.next(reusableToken);
+      if (nextToken != null && nextToken.term().equals("dogs")) {
+        nextToken.setType("D");
       }
-      return result;
+      return nextToken;
     }
   }
 }
\ No newline at end of file
Index: contrib/analyzers/src/test/org/apache/lucene/analysis/sinks/DateRecognizerSinkTokenizerTest.java
===================================================================
--- contrib/analyzers/src/test/org/apache/lucene/analysis/sinks/DateRecognizerSinkTokenizerTest.java	(revision 686801)
+++ contrib/analyzers/src/test/org/apache/lucene/analysis/sinks/DateRecognizerSinkTokenizerTest.java	(working copy)
@@ -43,13 +43,13 @@
     DateRecognizerSinkTokenizer sink = new DateRecognizerSinkTokenizer(new SimpleDateFormat("MM/dd/yyyy"));
     String test = "The quick red fox jumped over the lazy brown dogs on 7/11/2006  The dogs finally reacted on 7/12/2006";
     TeeTokenFilter tee = new TeeTokenFilter(new WhitespaceTokenizer(new StringReader(test)), sink);
-    Token tok = null;
     int count = 0;
-    while ((tok = tee.next()) != null){
-      assertTrue("tok is null and it shouldn't be", tok != null);
-      if (tok.termBuffer()[0] == '7'){
-        assertTrue(tok.type() + " is not equal to " + DateRecognizerSinkTokenizer.DATE_TYPE,
-                tok.type().equals(DateRecognizerSinkTokenizer.DATE_TYPE) == true);
+    final Token reusableToken = new Token();
+    for (Token nextToken = tee.next(reusableToken); nextToken != null; nextToken = tee.next(reusableToken)) {
+      assertTrue("nextToken is null and it shouldn't be", nextToken != null);
+      if (nextToken.termBuffer()[0] == '7'){
+        assertTrue(nextToken.type() + " is not equal to " + DateRecognizerSinkTokenizer.DATE_TYPE,
+                nextToken.type().equals(DateRecognizerSinkTokenizer.DATE_TYPE) == true);
       }
       count++;
     }
Index: contrib/analyzers/src/test/org/apache/lucene/analysis/sinks/TokenRangeSinkTokenizerTest.java
===================================================================
--- contrib/analyzers/src/test/org/apache/lucene/analysis/sinks/TokenRangeSinkTokenizerTest.java	(revision 686801)
+++ contrib/analyzers/src/test/org/apache/lucene/analysis/sinks/TokenRangeSinkTokenizerTest.java	(working copy)
@@ -42,10 +42,10 @@
     TokenRangeSinkTokenizer rangeToks = new TokenRangeSinkTokenizer(2, 4);
     String test = "The quick red fox jumped over the lazy brown dogs";
     TeeTokenFilter tee = new TeeTokenFilter(new WhitespaceTokenizer(new StringReader(test)), rangeToks);
-    Token tok = null;
     int count = 0;
-    while ((tok = tee.next()) != null){
-      assertTrue("tok is null and it shouldn't be", tok != null);
+    final Token reusableToken = new Token();
+    for (Token nextToken = tee.next(reusableToken); nextToken != null; nextToken = tee.next(reusableToken)) {
+      assertTrue("nextToken is null and it shouldn't be", nextToken != null);
       count++;
     }
     assertTrue(count + " does not equal: " + 10, count == 10);
Index: contrib/analyzers/src/test/org/apache/lucene/analysis/de/TestGermanStemFilter.java
===================================================================
--- contrib/analyzers/src/test/org/apache/lucene/analysis/de/TestGermanStemFilter.java	(revision 686801)
+++ contrib/analyzers/src/test/org/apache/lucene/analysis/de/TestGermanStemFilter.java	(working copy)
@@ -69,10 +69,11 @@
   private void check(final String input, final String expected) throws IOException {
     StandardTokenizer tokenStream = new StandardTokenizer(new StringReader(input));
     GermanStemFilter filter = new GermanStemFilter(tokenStream);
-    Token t = filter.next();
-    if (t == null)
+    final Token reusableToken = new Token();
+    Token nextToken = filter.next(reusableToken);
+    if (nextToken == null)
       fail();
-    assertEquals(expected, t.termText());
+    assertEquals(expected, nextToken.term());
     filter.close();
   }
 
Index: contrib/analyzers/src/test/org/apache/lucene/analysis/shingle/ShingleFilterTest.java
===================================================================
--- contrib/analyzers/src/test/org/apache/lucene/analysis/shingle/ShingleFilterTest.java	(revision 686801)
+++ contrib/analyzers/src/test/org/apache/lucene/analysis/shingle/ShingleFilterTest.java	(working copy)
@@ -35,7 +35,8 @@
       this.testToken = testToken;
     }
 
-    public Token next() throws IOException {
+    public Token next(final Token reusableToken) throws IOException {
+      assert reusableToken != null;
       if (index < testToken.length) {
         return testToken[index++];
       } else {
@@ -49,28 +50,28 @@
   }
 
   public static final Token[] TEST_TOKEN = new Token[] {
-      new Token("please", 0, 6),
-      new Token("divide", 7, 13),
-      new Token("this", 14, 18),
-      new Token("sentence", 19, 27),
-      new Token("into", 28, 32),
-      new Token("shingles", 33, 39),
+      createToken("please", 0, 6),
+      createToken("divide", 7, 13),
+      createToken("this", 14, 18),
+      createToken("sentence", 19, 27),
+      createToken("into", 28, 32),
+      createToken("shingles", 33, 39),
   };
 
   public static Token[] testTokenWithHoles;
 
   public static final Token[] BI_GRAM_TOKENS = new Token[] {
-    new Token("please", 0, 6),
-    new Token("please divide", 0, 13),
-    new Token("divide", 7, 13),
-    new Token("divide this", 7, 18),
-    new Token("this", 14, 18),
-    new Token("this sentence", 14, 27),
-    new Token("sentence", 19, 27),
-    new Token("sentence into", 19, 32),
-    new Token("into", 28, 32),
-    new Token("into shingles", 28, 39),
-    new Token("shingles", 33, 39),
+    createToken("please", 0, 6),
+    createToken("please divide", 0, 13),
+    createToken("divide", 7, 13),
+    createToken("divide this", 7, 18),
+    createToken("this", 14, 18),
+    createToken("this sentence", 14, 27),
+    createToken("sentence", 19, 27),
+    createToken("sentence into", 19, 32),
+    createToken("into", 28, 32),
+    createToken("into shingles", 28, 39),
+    createToken("shingles", 33, 39),
   };
 
   public static final int[] BI_GRAM_POSITION_INCREMENTS = new int[] {
@@ -83,17 +84,17 @@
   };
 
   public static final Token[] BI_GRAM_TOKENS_WITH_HOLES = new Token[] {
-    new Token("please", 0, 6),
-    new Token("please divide", 0, 13),
-    new Token("divide", 7, 13),
-    new Token("divide _", 7, 19),
-    new Token("_", 19, 19),
-    new Token("_ sentence", 19, 27),
-    new Token("sentence", 19, 27),
-    new Token("sentence _", 19, 33),
-    new Token("_", 33, 33),
-    new Token("_ shingles", 33, 39),
-    new Token("shingles", 33, 39),
+    createToken("please", 0, 6),
+    createToken("please divide", 0, 13),
+    createToken("divide", 7, 13),
+    createToken("divide _", 7, 19),
+    createToken("_", 19, 19),
+    createToken("_ sentence", 19, 27),
+    createToken("sentence", 19, 27),
+    createToken("sentence _", 19, 33),
+    createToken("_", 33, 33),
+    createToken("_ shingles", 33, 39),
+    createToken("shingles", 33, 39),
   };
 
   public static final int[] BI_GRAM_POSITION_INCREMENTS_WITH_HOLES = new int[] {
@@ -101,21 +102,21 @@
   };
 
   public static final Token[] TRI_GRAM_TOKENS = new Token[] {
-    new Token("please", 0, 6),
-    new Token("please divide", 0, 13),
-    new Token("please divide this", 0, 18),
-    new Token("divide", 7, 13),
-    new Token("divide this", 7, 18),
-    new Token("divide this sentence", 7, 27),
-    new Token("this", 14, 18),
-    new Token("this sentence", 14, 27),
-    new Token("this sentence into", 14, 32),
-    new Token("sentence", 19, 27),
-    new Token("sentence into", 19, 32),
-    new Token("sentence into shingles", 19, 39),
-    new Token("into", 28, 32),
-    new Token("into shingles", 28, 39),
-    new Token("shingles", 33, 39)
+    createToken("please", 0, 6),
+    createToken("please divide", 0, 13),
+    createToken("please divide this", 0, 18),
+    createToken("divide", 7, 13),
+    createToken("divide this", 7, 18),
+    createToken("divide this sentence", 7, 27),
+    createToken("this", 14, 18),
+    createToken("this sentence", 14, 27),
+    createToken("this sentence into", 14, 32),
+    createToken("sentence", 19, 27),
+    createToken("sentence into", 19, 32),
+    createToken("sentence into shingles", 19, 39),
+    createToken("into", 28, 32),
+    createToken("into shingles", 28, 39),
+    createToken("shingles", 33, 39)
   };
 
   public static final int[] TRI_GRAM_POSITION_INCREMENTS = new int[] {
@@ -135,10 +136,10 @@
   protected void setUp() throws Exception {
     super.setUp();
     testTokenWithHoles = new Token[] {
-      new Token("please", 0, 6),
-      new Token("divide", 7, 13),
-      new Token("sentence", 19, 27),
-      new Token("shingles", 33, 39),
+      createToken("please", 0, 6),
+      createToken("divide", 7, 13),
+      createToken("sentence", 19, 27),
+      createToken("shingles", 33, 39),
     };
 
     testTokenWithHoles[2].setPositionIncrement(2);
@@ -168,22 +169,27 @@
     throws IOException {
 
     TokenStream filter = new ShingleFilter(new TestTokenStream(tokensToShingle), maxSize);
-    Token token;
     int i = 0;
-
-    while ((token = filter.next()) != null) {
-      String termText = new String(token.termBuffer(), 0, token.termLength());
-      String goldText
-        = new String(tokensToCompare[i].termBuffer(), 0, tokensToCompare[i].termLength());
+    final Token reusableToken = new Token();
+    for (Token nextToken = filter.next(reusableToken); nextToken != null; nextToken = filter.next(reusableToken)) {
+      String termText = nextToken.term();
+      String goldText = tokensToCompare[i].term();
       assertEquals("Wrong termText", goldText, termText);
       assertEquals("Wrong startOffset for token \"" + termText + "\"",
-          tokensToCompare[i].startOffset(), token.startOffset());
+          tokensToCompare[i].startOffset(), nextToken.startOffset());
       assertEquals("Wrong endOffset for token \"" + termText + "\"",
-          tokensToCompare[i].endOffset(), token.endOffset());
+          tokensToCompare[i].endOffset(), nextToken.endOffset());
       assertEquals("Wrong positionIncrement for token \"" + termText + "\"",
-          positionIncrements[i], token.getPositionIncrement());
-      assertEquals("Wrong type for token \"" + termText + "\"", types[i], token.type());
+          positionIncrements[i], nextToken.getPositionIncrement());
+      assertEquals("Wrong type for token \"" + termText + "\"", types[i], nextToken.type());
       i++;
     }
   }
+
+  private static Token createToken(String term, int start, int offset)
+  {
+    Token token = new Token(start, offset);
+    token.setTermBuffer(term);
+    return token;
+  }
 }
Index: contrib/analyzers/src/test/org/apache/lucene/analysis/shingle/ShingleAnalyzerWrapperTest.java
===================================================================
--- contrib/analyzers/src/test/org/apache/lucene/analysis/shingle/ShingleAnalyzerWrapperTest.java	(revision 686801)
+++ contrib/analyzers/src/test/org/apache/lucene/analysis/shingle/ShingleAnalyzerWrapperTest.java	(working copy)
@@ -156,11 +156,11 @@
 
     TokenStream ts = analyzer.tokenStream("content",
                                           new StringReader("this sentence"));
-    Token token;
     int j = -1;
-    while ((token = ts.next()) != null) {
-      j += token.getPositionIncrement();
-      String termText = new String(token.termBuffer(), 0, token.termLength());
+    final Token reusableToken = new Token();
+    for (Token nextToken = ts.next(reusableToken); nextToken != null; nextToken = ts.next(reusableToken)) {
+      j += nextToken.getPositionIncrement();
+      String termText = nextToken.term();
       q.add(new Term("content", termText), j);
     }
 
@@ -182,9 +182,9 @@
 
     TokenStream ts = analyzer.tokenStream("content",
                                           new StringReader("test sentence"));
-    Token token;
-    while ((token = ts.next()) != null) {
-      String termText =  new String(token.termBuffer(), 0, token.termLength());
+    final Token reusableToken = new Token();
+    for (Token nextToken = ts.next(reusableToken); nextToken != null; nextToken = ts.next(reusableToken)) {
+      String termText =  nextToken.term();
       q.add(new TermQuery(new Term("content", termText)),
             BooleanClause.Occur.SHOULD);
     }
Index: contrib/analyzers/src/test/org/apache/lucene/analysis/shingle/TestShingleMatrixFilter.java
===================================================================
--- contrib/analyzers/src/test/org/apache/lucene/analysis/shingle/TestShingleMatrixFilter.java	(revision 686801)
+++ contrib/analyzers/src/test/org/apache/lucene/analysis/shingle/TestShingleMatrixFilter.java	(working copy)
@@ -40,29 +40,23 @@
 
     ShingleMatrixFilter.defaultSettingsCodec = null;
 
-    Token token = new Token(); // for debug use only
-
-
-
-
     TokenStream ts;
 
-
     ts = new ShingleMatrixFilter(new EmptyTokenStream(), 1, 2, ' ', false, new ShingleMatrixFilter.OneDimensionalNonWeightedTokenSettingsCodec());
-    assertNull(ts.next());
+    assertNull(ts.next(new Token()));
 
     TokenListStream tls;
     LinkedList<Token> tokens;
 
-    // test a plain old token stream with synonyms tranlated to rows.
+    // test a plain old token stream with synonyms translated to rows.
 
     tokens = new LinkedList<Token>();
-    tokens.add(new Token("please", 0, 6));
-    tokens.add(new Token("divide", 7, 13));
-    tokens.add(new Token("this", 14, 18));
-    tokens.add(new Token("sentence", 19, 27));
-    tokens.add(new Token("into", 28, 32));
-    tokens.add(new Token("shingles", 33, 39));
+    tokens.add(createToken("please", 0, 6));
+    tokens.add(createToken("divide", 7, 13));
+    tokens.add(createToken("this", 14, 18));
+    tokens.add(createToken("sentence", 19, 27));
+    tokens.add(createToken("into", 28, 32));
+    tokens.add(createToken("shingles", 33, 39));
 
     tls = new TokenListStream(tokens);
 
@@ -70,21 +64,23 @@
 
     ts = new ShingleMatrixFilter(tls, 1, 2, ' ', false, new ShingleMatrixFilter.OneDimensionalNonWeightedTokenSettingsCodec());
 
-    assertNext(ts, "please", 0, 6);
-    assertNext(ts, "please divide", 0, 13);
-    assertNext(ts, "divide", 7, 13);
-    assertNext(ts, "divide this", 7, 18);
-    assertNext(ts, "this", 14, 18);
-    assertNext(ts, "this sentence", 14, 27);
-    assertNext(ts, "sentence", 19, 27);
-    assertNext(ts, "sentence into", 19, 32);
-    assertNext(ts, "into", 28, 32);
-    assertNext(ts, "into shingles", 28, 39);
-    assertNext(ts, "shingles", 33, 39);
+    Token reusableToken = new Token();
 
+    assertNext(ts, reusableToken, "please", 0, 6);
+    assertNext(ts, reusableToken, "please divide", 0, 13);
+    assertNext(ts, reusableToken, "divide", 7, 13);
+    assertNext(ts, reusableToken, "divide this", 7, 18);
+    assertNext(ts, reusableToken, "this", 14, 18);
+    assertNext(ts, reusableToken, "this sentence", 14, 27);
+    assertNext(ts, reusableToken, "sentence", 19, 27);
+    assertNext(ts, reusableToken, "sentence into", 19, 32);
+    assertNext(ts, reusableToken, "into", 28, 32);
+    assertNext(ts, reusableToken, "into shingles", 28, 39);
+    assertNext(ts, reusableToken, "shingles", 33, 39);
 
-    assertNull(ts.next());
 
+    assertNull(ts.next(reusableToken));
+
   }
 
   /**
@@ -95,9 +91,6 @@
 
     ShingleMatrixFilter.defaultSettingsCodec = null;//new ShingleMatrixFilter.SimpleThreeDimensionalTokenSettingsCodec();
 
-    Token token = new Token(); // for debug use only
-
-
     TokenStream ts;
     TokenListStream tls;
     LinkedList<Token> tokens;
@@ -117,25 +110,26 @@
 
     ts = new ShingleMatrixFilter(tls, 2, 2, '_', false, new ShingleMatrixFilter.TwoDimensionalNonWeightedSynonymTokenSettingsCodec());
 
-    assertNext(ts, "hello_world");
-    assertNext(ts, "greetings_world");
-    assertNext(ts, "hello_earth");
-    assertNext(ts, "greetings_earth");
-    assertNext(ts, "hello_tellus");
-    assertNext(ts, "greetings_tellus");
-    assertNull(ts.next());
+    final Token reusableToken = new Token();
+    assertNext(ts, reusableToken, "hello_world");
+    assertNext(ts, reusableToken, "greetings_world");
+    assertNext(ts, reusableToken, "hello_earth");
+    assertNext(ts, reusableToken, "greetings_earth");
+    assertNext(ts, reusableToken, "hello_tellus");
+    assertNext(ts, reusableToken, "greetings_tellus");
+    assertNull(ts.next(reusableToken));
 
     // bi-grams with no spacer character, start offset, end offset
 
     tls.reset();
     ts = new ShingleMatrixFilter(tls, 2, 2, null, false, new ShingleMatrixFilter.TwoDimensionalNonWeightedSynonymTokenSettingsCodec());
-    assertNext(ts, "helloworld", 0, 10);
-    assertNext(ts, "greetingsworld", 0, 10);
-    assertNext(ts, "helloearth", 0, 10);
-    assertNext(ts, "greetingsearth", 0, 10);
-    assertNext(ts, "hellotellus", 0, 10);
-    assertNext(ts, "greetingstellus", 0, 10);
-    assertNull(ts.next());
+    assertNext(ts, reusableToken, "helloworld", 0, 10);
+    assertNext(ts, reusableToken, "greetingsworld", 0, 10);
+    assertNext(ts, reusableToken, "helloearth", 0, 10);
+    assertNext(ts, reusableToken, "greetingsearth", 0, 10);
+    assertNext(ts, reusableToken, "hellotellus", 0, 10);
+    assertNext(ts, reusableToken, "greetingstellus", 0, 10);
+    assertNull(ts.next(reusableToken));
 
 
     // add ^_prefix_and_suffix_$
@@ -160,119 +154,119 @@
 
     ts = new ShingleMatrixFilter(tls, 2, 2, '_', false);
 //
-//    while ((token = ts.next(token)) != null) {
-//      System.out.println("assertNext(ts, \"" + token.termText() + "\", " + token.getPositionIncrement() + ", " + (token.getPayload() == null ? "1.0" : PayloadHelper.decodeFloat(token.getPayload().getData())) + "f, " + token.startOffset() + ", " + token.endOffset() + ");");
+//    for (Token token = ts.next(new Token()); token != null; token = ts.next(token)) {
+//      System.out.println("assertNext(ts, \"" + token.term() + "\", " + token.getPositionIncrement() + ", " + (token.getPayload() == null ? "1.0" : PayloadHelper.decodeFloat(token.getPayload().getData())) + "f, " + token.startOffset() + ", " + token.endOffset() + ");");
 //      token.clear();
 //    }
 
-    assertNext(ts, "^_hello", 1, 10.049875f, 0, 4);
-    assertNext(ts, "^_greetings", 1, 10.049875f, 0, 4);
-    assertNext(ts, "hello_world", 1, 1.4142135f, 0, 10);
-    assertNext(ts, "greetings_world", 1, 1.4142135f, 0, 10);
-    assertNext(ts, "hello_earth", 1, 1.4142135f, 0, 10);
-    assertNext(ts, "greetings_earth", 1, 1.4142135f, 0, 10);
-    assertNext(ts, "hello_tellus", 1, 1.4142135f, 0, 10);
-    assertNext(ts, "greetings_tellus", 1, 1.4142135f, 0, 10);
-    assertNext(ts, "world_$", 1, 7.1414285f, 5, 10);
-    assertNext(ts, "earth_$", 1, 7.1414285f, 5, 10);
-    assertNext(ts, "tellus_$", 1, 7.1414285f, 5, 10);
-    assertNull(ts.next());
+    assertNext(ts, reusableToken, "^_hello", 1, 10.049875f, 0, 4);
+    assertNext(ts, reusableToken, "^_greetings", 1, 10.049875f, 0, 4);
+    assertNext(ts, reusableToken, "hello_world", 1, 1.4142135f, 0, 10);
+    assertNext(ts, reusableToken, "greetings_world", 1, 1.4142135f, 0, 10);
+    assertNext(ts, reusableToken, "hello_earth", 1, 1.4142135f, 0, 10);
+    assertNext(ts, reusableToken, "greetings_earth", 1, 1.4142135f, 0, 10);
+    assertNext(ts, reusableToken, "hello_tellus", 1, 1.4142135f, 0, 10);
+    assertNext(ts, reusableToken, "greetings_tellus", 1, 1.4142135f, 0, 10);
+    assertNext(ts, reusableToken, "world_$", 1, 7.1414285f, 5, 10);
+    assertNext(ts, reusableToken, "earth_$", 1, 7.1414285f, 5, 10);
+    assertNext(ts, reusableToken, "tellus_$", 1, 7.1414285f, 5, 10);
+    assertNull(ts.next(reusableToken));
 
     // test unlimited size and allow single boundary token as shingle
     tls.reset();
     ts = new ShingleMatrixFilter(tls, 1, Integer.MAX_VALUE, '_', false);
 
 //
-//    while ((token = ts.next(token)) != null) {
-//      System.out.println("assertNext(ts, \"" + token.termText() + "\", " + token.getPositionIncrement() + ", " + (token.getPayload() == null ? "1.0" : PayloadHelper.decodeFloat(token.getPayload().getData())) + "f, " + token.startOffset() + ", " + token.endOffset() + ");");
+//  for (Token token = ts.next(new Token()); token != null; token = ts.next(token)) {
+//      System.out.println("assertNext(ts, \"" + token.term() + "\", " + token.getPositionIncrement() + ", " + (token.getPayload() == null ? "1.0" : PayloadHelper.decodeFloat(token.getPayload().getData())) + "f, " + token.startOffset() + ", " + token.endOffset() + ");");
 //      token.clear();
 //    }
 
-    assertNext(ts, "^", 1, 10.0f, 0, 0);
-    assertNext(ts, "^_hello", 1, 10.049875f, 0, 4);
-    assertNext(ts, "^_hello_world", 1, 10.099504f, 0, 10);
-    assertNext(ts, "^_hello_world_$", 1, 12.328828f, 0, 10);
-    assertNext(ts, "hello", 1, 1.0f, 0, 4);
-    assertNext(ts, "hello_world", 1, 1.4142135f, 0, 10);
-    assertNext(ts, "hello_world_$", 1, 7.2111025f, 0, 10);
-    assertNext(ts, "world", 1, 1.0f, 5, 10);
-    assertNext(ts, "world_$", 1, 7.1414285f, 5, 10);
-    assertNext(ts, "$", 1, 7.071068f, 10, 10);
-    assertNext(ts, "^_greetings", 1, 10.049875f, 0, 4);
-    assertNext(ts, "^_greetings_world", 1, 10.099504f, 0, 10);
-    assertNext(ts, "^_greetings_world_$", 1, 12.328828f, 0, 10);
-    assertNext(ts, "greetings", 1, 1.0f, 0, 4);
-    assertNext(ts, "greetings_world", 1, 1.4142135f, 0, 10);
-    assertNext(ts, "greetings_world_$", 1, 7.2111025f, 0, 10);
-    assertNext(ts, "^_hello_earth", 1, 10.099504f, 0, 10);
-    assertNext(ts, "^_hello_earth_$", 1, 12.328828f, 0, 10);
-    assertNext(ts, "hello_earth", 1, 1.4142135f, 0, 10);
-    assertNext(ts, "hello_earth_$", 1, 7.2111025f, 0, 10);
-    assertNext(ts, "earth", 1, 1.0f, 5, 10);
-    assertNext(ts, "earth_$", 1, 7.1414285f, 5, 10);
-    assertNext(ts, "^_greetings_earth", 1, 10.099504f, 0, 10);
-    assertNext(ts, "^_greetings_earth_$", 1, 12.328828f, 0, 10);
-    assertNext(ts, "greetings_earth", 1, 1.4142135f, 0, 10);
-    assertNext(ts, "greetings_earth_$", 1, 7.2111025f, 0, 10);
-    assertNext(ts, "^_hello_tellus", 1, 10.099504f, 0, 10);
-    assertNext(ts, "^_hello_tellus_$", 1, 12.328828f, 0, 10);
-    assertNext(ts, "hello_tellus", 1, 1.4142135f, 0, 10);
-    assertNext(ts, "hello_tellus_$", 1, 7.2111025f, 0, 10);
-    assertNext(ts, "tellus", 1, 1.0f, 5, 10);
-    assertNext(ts, "tellus_$", 1, 7.1414285f, 5, 10);
-    assertNext(ts, "^_greetings_tellus", 1, 10.099504f, 0, 10);
-    assertNext(ts, "^_greetings_tellus_$", 1, 12.328828f, 0, 10);
-    assertNext(ts, "greetings_tellus", 1, 1.4142135f, 0, 10);
-    assertNext(ts, "greetings_tellus_$", 1, 7.2111025f, 0, 10);
+    assertNext(ts, reusableToken, "^", 1, 10.0f, 0, 0);
+    assertNext(ts, reusableToken, "^_hello", 1, 10.049875f, 0, 4);
+    assertNext(ts, reusableToken, "^_hello_world", 1, 10.099504f, 0, 10);
+    assertNext(ts, reusableToken, "^_hello_world_$", 1, 12.328828f, 0, 10);
+    assertNext(ts, reusableToken, "hello", 1, 1.0f, 0, 4);
+    assertNext(ts, reusableToken, "hello_world", 1, 1.4142135f, 0, 10);
+    assertNext(ts, reusableToken, "hello_world_$", 1, 7.2111025f, 0, 10);
+    assertNext(ts, reusableToken, "world", 1, 1.0f, 5, 10);
+    assertNext(ts, reusableToken, "world_$", 1, 7.1414285f, 5, 10);
+    assertNext(ts, reusableToken, "$", 1, 7.071068f, 10, 10);
+    assertNext(ts, reusableToken, "^_greetings", 1, 10.049875f, 0, 4);
+    assertNext(ts, reusableToken, "^_greetings_world", 1, 10.099504f, 0, 10);
+    assertNext(ts, reusableToken, "^_greetings_world_$", 1, 12.328828f, 0, 10);
+    assertNext(ts, reusableToken, "greetings", 1, 1.0f, 0, 4);
+    assertNext(ts, reusableToken, "greetings_world", 1, 1.4142135f, 0, 10);
+    assertNext(ts, reusableToken, "greetings_world_$", 1, 7.2111025f, 0, 10);
+    assertNext(ts, reusableToken, "^_hello_earth", 1, 10.099504f, 0, 10);
+    assertNext(ts, reusableToken, "^_hello_earth_$", 1, 12.328828f, 0, 10);
+    assertNext(ts, reusableToken, "hello_earth", 1, 1.4142135f, 0, 10);
+    assertNext(ts, reusableToken, "hello_earth_$", 1, 7.2111025f, 0, 10);
+    assertNext(ts, reusableToken, "earth", 1, 1.0f, 5, 10);
+    assertNext(ts, reusableToken, "earth_$", 1, 7.1414285f, 5, 10);
+    assertNext(ts, reusableToken, "^_greetings_earth", 1, 10.099504f, 0, 10);
+    assertNext(ts, reusableToken, "^_greetings_earth_$", 1, 12.328828f, 0, 10);
+    assertNext(ts, reusableToken, "greetings_earth", 1, 1.4142135f, 0, 10);
+    assertNext(ts, reusableToken, "greetings_earth_$", 1, 7.2111025f, 0, 10);
+    assertNext(ts, reusableToken, "^_hello_tellus", 1, 10.099504f, 0, 10);
+    assertNext(ts, reusableToken, "^_hello_tellus_$", 1, 12.328828f, 0, 10);
+    assertNext(ts, reusableToken, "hello_tellus", 1, 1.4142135f, 0, 10);
+    assertNext(ts, reusableToken, "hello_tellus_$", 1, 7.2111025f, 0, 10);
+    assertNext(ts, reusableToken, "tellus", 1, 1.0f, 5, 10);
+    assertNext(ts, reusableToken, "tellus_$", 1, 7.1414285f, 5, 10);
+    assertNext(ts, reusableToken, "^_greetings_tellus", 1, 10.099504f, 0, 10);
+    assertNext(ts, reusableToken, "^_greetings_tellus_$", 1, 12.328828f, 0, 10);
+    assertNext(ts, reusableToken, "greetings_tellus", 1, 1.4142135f, 0, 10);
+    assertNext(ts, reusableToken, "greetings_tellus_$", 1, 7.2111025f, 0, 10);
 
-    assertNull(ts.next());
+    assertNull(ts.next(reusableToken));
 
     // test unlimited size but don't allow single boundary token as shingle
 
     tls.reset();
     ts = new ShingleMatrixFilter(tls, 1, Integer.MAX_VALUE, '_', true);
-//    while ((token = ts.next(token)) != null) {
-//      System.out.println("assertNext(ts, \"" + token.termText() + "\", " + token.getPositionIncrement() + ", " + (token.getPayload() == null ? "1.0" : PayloadHelper.decodeFloat(token.getPayload().getData())) + "f, " + token.startOffset() + ", " + token.endOffset() + ");");
+//  for (Token token = ts.next(new Token()); token != null; token = ts.next(token)) {
+//      System.out.println("assertNext(ts, \"" + token.term() + "\", " + token.getPositionIncrement() + ", " + (token.getPayload() == null ? "1.0" : PayloadHelper.decodeFloat(token.getPayload().getData())) + "f, " + token.startOffset() + ", " + token.endOffset() + ");");
 //      token.clear();
 //    }
 
-    assertNext(ts, "^_hello", 1, 10.049875f, 0, 4);
-    assertNext(ts, "^_hello_world", 1, 10.099504f, 0, 10);
-    assertNext(ts, "^_hello_world_$", 1, 12.328828f, 0, 10);
-    assertNext(ts, "hello", 1, 1.0f, 0, 4);
-    assertNext(ts, "hello_world", 1, 1.4142135f, 0, 10);
-    assertNext(ts, "hello_world_$", 1, 7.2111025f, 0, 10);
-    assertNext(ts, "world", 1, 1.0f, 5, 10);
-    assertNext(ts, "world_$", 1, 7.1414285f, 5, 10);
-    assertNext(ts, "^_greetings", 1, 10.049875f, 0, 4);
-    assertNext(ts, "^_greetings_world", 1, 10.099504f, 0, 10);
-    assertNext(ts, "^_greetings_world_$", 1, 12.328828f, 0, 10);
-    assertNext(ts, "greetings", 1, 1.0f, 0, 4);
-    assertNext(ts, "greetings_world", 1, 1.4142135f, 0, 10);
-    assertNext(ts, "greetings_world_$", 1, 7.2111025f, 0, 10);
-    assertNext(ts, "^_hello_earth", 1, 10.099504f, 0, 10);
-    assertNext(ts, "^_hello_earth_$", 1, 12.328828f, 0, 10);
-    assertNext(ts, "hello_earth", 1, 1.4142135f, 0, 10);
-    assertNext(ts, "hello_earth_$", 1, 7.2111025f, 0, 10);
-    assertNext(ts, "earth", 1, 1.0f, 5, 10);
-    assertNext(ts, "earth_$", 1, 7.1414285f, 5, 10);
-    assertNext(ts, "^_greetings_earth", 1, 10.099504f, 0, 10);
-    assertNext(ts, "^_greetings_earth_$", 1, 12.328828f, 0, 10);
-    assertNext(ts, "greetings_earth", 1, 1.4142135f, 0, 10);
-    assertNext(ts, "greetings_earth_$", 1, 7.2111025f, 0, 10);
-    assertNext(ts, "^_hello_tellus", 1, 10.099504f, 0, 10);
-    assertNext(ts, "^_hello_tellus_$", 1, 12.328828f, 0, 10);
-    assertNext(ts, "hello_tellus", 1, 1.4142135f, 0, 10);
-    assertNext(ts, "hello_tellus_$", 1, 7.2111025f, 0, 10);
-    assertNext(ts, "tellus", 1, 1.0f, 5, 10);
-    assertNext(ts, "tellus_$", 1, 7.1414285f, 5, 10);
-    assertNext(ts, "^_greetings_tellus", 1, 10.099504f, 0, 10);
-    assertNext(ts, "^_greetings_tellus_$", 1, 12.328828f, 0, 10);
-    assertNext(ts, "greetings_tellus", 1, 1.4142135f, 0, 10);
-    assertNext(ts, "greetings_tellus_$", 1, 7.2111025f, 0, 10);
+    assertNext(ts, reusableToken, "^_hello", 1, 10.049875f, 0, 4);
+    assertNext(ts, reusableToken, "^_hello_world", 1, 10.099504f, 0, 10);
+    assertNext(ts, reusableToken, "^_hello_world_$", 1, 12.328828f, 0, 10);
+    assertNext(ts, reusableToken, "hello", 1, 1.0f, 0, 4);
+    assertNext(ts, reusableToken, "hello_world", 1, 1.4142135f, 0, 10);
+    assertNext(ts, reusableToken, "hello_world_$", 1, 7.2111025f, 0, 10);
+    assertNext(ts, reusableToken, "world", 1, 1.0f, 5, 10);
+    assertNext(ts, reusableToken, "world_$", 1, 7.1414285f, 5, 10);
+    assertNext(ts, reusableToken, "^_greetings", 1, 10.049875f, 0, 4);
+    assertNext(ts, reusableToken, "^_greetings_world", 1, 10.099504f, 0, 10);
+    assertNext(ts, reusableToken, "^_greetings_world_$", 1, 12.328828f, 0, 10);
+    assertNext(ts, reusableToken, "greetings", 1, 1.0f, 0, 4);
+    assertNext(ts, reusableToken, "greetings_world", 1, 1.4142135f, 0, 10);
+    assertNext(ts, reusableToken, "greetings_world_$", 1, 7.2111025f, 0, 10);
+    assertNext(ts, reusableToken, "^_hello_earth", 1, 10.099504f, 0, 10);
+    assertNext(ts, reusableToken, "^_hello_earth_$", 1, 12.328828f, 0, 10);
+    assertNext(ts, reusableToken, "hello_earth", 1, 1.4142135f, 0, 10);
+    assertNext(ts, reusableToken, "hello_earth_$", 1, 7.2111025f, 0, 10);
+    assertNext(ts, reusableToken, "earth", 1, 1.0f, 5, 10);
+    assertNext(ts, reusableToken, "earth_$", 1, 7.1414285f, 5, 10);
+    assertNext(ts, reusableToken, "^_greetings_earth", 1, 10.099504f, 0, 10);
+    assertNext(ts, reusableToken, "^_greetings_earth_$", 1, 12.328828f, 0, 10);
+    assertNext(ts, reusableToken, "greetings_earth", 1, 1.4142135f, 0, 10);
+    assertNext(ts, reusableToken, "greetings_earth_$", 1, 7.2111025f, 0, 10);
+    assertNext(ts, reusableToken, "^_hello_tellus", 1, 10.099504f, 0, 10);
+    assertNext(ts, reusableToken, "^_hello_tellus_$", 1, 12.328828f, 0, 10);
+    assertNext(ts, reusableToken, "hello_tellus", 1, 1.4142135f, 0, 10);
+    assertNext(ts, reusableToken, "hello_tellus_$", 1, 7.2111025f, 0, 10);
+    assertNext(ts, reusableToken, "tellus", 1, 1.0f, 5, 10);
+    assertNext(ts, reusableToken, "tellus_$", 1, 7.1414285f, 5, 10);
+    assertNext(ts, reusableToken, "^_greetings_tellus", 1, 10.099504f, 0, 10);
+    assertNext(ts, reusableToken, "^_greetings_tellus_$", 1, 12.328828f, 0, 10);
+    assertNext(ts, reusableToken, "greetings_tellus", 1, 1.4142135f, 0, 10);
+    assertNext(ts, reusableToken, "greetings_tellus_$", 1, 7.2111025f, 0, 10);
 
 
-    assertNull(ts.next());
+    assertNull(ts.next(reusableToken));
 
     System.currentTimeMillis();
 
@@ -300,27 +294,27 @@
 
     ts = new ShingleMatrixFilter(tls, 2, 3, '_', false);
 
-//    while ((token = ts.next(token)) != null) {
-//      System.out.println("assertNext(ts, \"" + token.termText() + "\", " + token.getPositionIncrement() + ", " + (token.getPayload() == null ? "1.0" : PayloadHelper.decodeFloat(token.getPayload().getData())) + "f, " + token.startOffset() + ", " + token.endOffset() + ");");
+//  for (Token token = ts.next(new Token()); token != null; token = ts.next(token)) {
+//      System.out.println("assertNext(ts, \"" + token.term() + "\", " + token.getPositionIncrement() + ", " + (token.getPayload() == null ? "1.0" : PayloadHelper.decodeFloat(token.getPayload().getData())) + "f, " + token.startOffset() + ", " + token.endOffset() + ");");
 //      token.clear();
 //    }
 
     // shingle, position increment, weight, start offset, end offset
 
-    assertNext(ts, "hello_world", 1, 1.4142135f, 0, 10);
-    assertNext(ts, "greetings_and", 1, 1.4142135f, 0, 4);
-    assertNext(ts, "greetings_and_salutations", 1, 1.7320508f, 0, 4);
-    assertNext(ts, "and_salutations", 1, 1.4142135f, 0, 4);
-    assertNext(ts, "and_salutations_world", 1, 1.7320508f, 0, 10);
-    assertNext(ts, "salutations_world", 1, 1.4142135f, 0, 10);
-    assertNext(ts, "hello_earth", 1, 1.4142135f, 0, 10);
-    assertNext(ts, "and_salutations_earth", 1, 1.7320508f, 0, 10);
-    assertNext(ts, "salutations_earth", 1, 1.4142135f, 0, 10);
-    assertNext(ts, "hello_tellus", 1, 1.4142135f, 0, 10);
-    assertNext(ts, "and_salutations_tellus", 1, 1.7320508f, 0, 10);
-    assertNext(ts, "salutations_tellus", 1, 1.4142135f, 0, 10);
+    assertNext(ts, reusableToken, "hello_world", 1, 1.4142135f, 0, 10);
+    assertNext(ts, reusableToken, "greetings_and", 1, 1.4142135f, 0, 4);
+    assertNext(ts, reusableToken, "greetings_and_salutations", 1, 1.7320508f, 0, 4);
+    assertNext(ts, reusableToken, "and_salutations", 1, 1.4142135f, 0, 4);
+    assertNext(ts, reusableToken, "and_salutations_world", 1, 1.7320508f, 0, 10);
+    assertNext(ts, reusableToken, "salutations_world", 1, 1.4142135f, 0, 10);
+    assertNext(ts, reusableToken, "hello_earth", 1, 1.4142135f, 0, 10);
+    assertNext(ts, reusableToken, "and_salutations_earth", 1, 1.7320508f, 0, 10);
+    assertNext(ts, reusableToken, "salutations_earth", 1, 1.4142135f, 0, 10);
+    assertNext(ts, reusableToken, "hello_tellus", 1, 1.4142135f, 0, 10);
+    assertNext(ts, reusableToken, "and_salutations_tellus", 1, 1.7320508f, 0, 10);
+    assertNext(ts, reusableToken, "salutations_tellus", 1, 1.4142135f, 0, 10);
 
-    assertNull(ts.next());
+    assertNull(ts.next(reusableToken));
 
     System.currentTimeMillis();
 
@@ -361,53 +355,53 @@
 
     TokenStream ts = new ShingleMatrixFilter(matrix, 2, 4, '_', true, new ShingleMatrixFilter.SimpleThreeDimensionalTokenSettingsCodec());
 
-//    Token token = new Token();
-//    while ((token = ts.next(token)) != null) {
-//      System.out.println("assertNext(ts, \"" + token.termText() + "\", " + token.getPositionIncrement() + ", " + (token.getPayload() == null ? "1.0" : PayloadHelper.decodeFloat(token.getPayload().getData())) + "f, " + token.startOffset() + ", " + token.endOffset() + ");");
+//  for (Token token = ts.next(new Token()); token != null; token = ts.next(token)) {
+//      System.out.println("assertNext(ts, \"" + token.term() + "\", " + token.getPositionIncrement() + ", " + (token.getPayload() == null ? "1.0" : PayloadHelper.decodeFloat(token.getPayload().getData())) + "f, " + token.startOffset() + ", " + token.endOffset() + ");");
 //      token.clear();
 //    }
 
-    assertNext(ts, "no_surprise", 1, 1.4142135f, 0, 0);
-    assertNext(ts, "no_surprise_to", 1, 1.7320508f, 0, 0);
-    assertNext(ts, "no_surprise_to_see", 1, 2.0f, 0, 0);
-    assertNext(ts, "surprise_to", 1, 1.4142135f, 0, 0);
-    assertNext(ts, "surprise_to_see", 1, 1.7320508f, 0, 0);
-    assertNext(ts, "surprise_to_see_england", 1, 2.0f, 0, 0);
-    assertNext(ts, "to_see", 1, 1.4142135f, 0, 0);
-    assertNext(ts, "to_see_england", 1, 1.7320508f, 0, 0);
-    assertNext(ts, "to_see_england_manager", 1, 2.0f, 0, 0);
-    assertNext(ts, "see_england", 1, 1.4142135f, 0, 0);
-    assertNext(ts, "see_england_manager", 1, 1.7320508f, 0, 0);
-    assertNext(ts, "see_england_manager_svennis", 1, 2.0f, 0, 0);
-    assertNext(ts, "england_manager", 1, 1.4142135f, 0, 0);
-    assertNext(ts, "england_manager_svennis", 1, 1.7320508f, 0, 0);
-    assertNext(ts, "england_manager_svennis_in", 1, 2.0f, 0, 0);
-    assertNext(ts, "manager_svennis", 1, 1.4142135f, 0, 0);
-    assertNext(ts, "manager_svennis_in", 1, 1.7320508f, 0, 0);
-    assertNext(ts, "manager_svennis_in_the", 1, 2.0f, 0, 0);
-    assertNext(ts, "svennis_in", 1, 1.4142135f, 0, 0);
-    assertNext(ts, "svennis_in_the", 1, 1.7320508f, 0, 0);
-    assertNext(ts, "svennis_in_the_croud", 1, 2.0f, 0, 0);
-    assertNext(ts, "in_the", 1, 1.4142135f, 0, 0);
-    assertNext(ts, "in_the_croud", 1, 1.7320508f, 0, 0);
-    assertNext(ts, "the_croud", 1, 1.4142135f, 0, 0);
-    assertNext(ts, "see_england_manager_sven", 1, 2.0f, 0, 0);
-    assertNext(ts, "england_manager_sven", 1, 1.7320508f, 0, 0);
-    assertNext(ts, "england_manager_sven_göran", 1, 2.0f, 0, 0);
-    assertNext(ts, "manager_sven", 1, 1.4142135f, 0, 0);
-    assertNext(ts, "manager_sven_göran", 1, 1.7320508f, 0, 0);
-    assertNext(ts, "manager_sven_göran_eriksson", 1, 2.0f, 0, 0);
-    assertNext(ts, "sven_göran", 1, 1.4142135f, 0, 0);
-    assertNext(ts, "sven_göran_eriksson", 1, 1.7320508f, 0, 0);
-    assertNext(ts, "sven_göran_eriksson_in", 1, 2.0f, 0, 0);
-    assertNext(ts, "göran_eriksson", 1, 1.4142135f, 0, 0);
-    assertNext(ts, "göran_eriksson_in", 1, 1.7320508f, 0, 0);
-    assertNext(ts, "göran_eriksson_in_the", 1, 2.0f, 0, 0);
-    assertNext(ts, "eriksson_in", 1, 1.4142135f, 0, 0);
-    assertNext(ts, "eriksson_in_the", 1, 1.7320508f, 0, 0);
-    assertNext(ts, "eriksson_in_the_croud", 1, 2.0f, 0, 0);
+    final Token reusableToken = new Token();
+    assertNext(ts, reusableToken, "no_surprise", 1, 1.4142135f, 0, 0);
+    assertNext(ts, reusableToken, "no_surprise_to", 1, 1.7320508f, 0, 0);
+    assertNext(ts, reusableToken, "no_surprise_to_see", 1, 2.0f, 0, 0);
+    assertNext(ts, reusableToken, "surprise_to", 1, 1.4142135f, 0, 0);
+    assertNext(ts, reusableToken, "surprise_to_see", 1, 1.7320508f, 0, 0);
+    assertNext(ts, reusableToken, "surprise_to_see_england", 1, 2.0f, 0, 0);
+    assertNext(ts, reusableToken, "to_see", 1, 1.4142135f, 0, 0);
+    assertNext(ts, reusableToken, "to_see_england", 1, 1.7320508f, 0, 0);
+    assertNext(ts, reusableToken, "to_see_england_manager", 1, 2.0f, 0, 0);
+    assertNext(ts, reusableToken, "see_england", 1, 1.4142135f, 0, 0);
+    assertNext(ts, reusableToken, "see_england_manager", 1, 1.7320508f, 0, 0);
+    assertNext(ts, reusableToken, "see_england_manager_svennis", 1, 2.0f, 0, 0);
+    assertNext(ts, reusableToken, "england_manager", 1, 1.4142135f, 0, 0);
+    assertNext(ts, reusableToken, "england_manager_svennis", 1, 1.7320508f, 0, 0);
+    assertNext(ts, reusableToken, "england_manager_svennis_in", 1, 2.0f, 0, 0);
+    assertNext(ts, reusableToken, "manager_svennis", 1, 1.4142135f, 0, 0);
+    assertNext(ts, reusableToken, "manager_svennis_in", 1, 1.7320508f, 0, 0);
+    assertNext(ts, reusableToken, "manager_svennis_in_the", 1, 2.0f, 0, 0);
+    assertNext(ts, reusableToken, "svennis_in", 1, 1.4142135f, 0, 0);
+    assertNext(ts, reusableToken, "svennis_in_the", 1, 1.7320508f, 0, 0);
+    assertNext(ts, reusableToken, "svennis_in_the_croud", 1, 2.0f, 0, 0);
+    assertNext(ts, reusableToken, "in_the", 1, 1.4142135f, 0, 0);
+    assertNext(ts, reusableToken, "in_the_croud", 1, 1.7320508f, 0, 0);
+    assertNext(ts, reusableToken, "the_croud", 1, 1.4142135f, 0, 0);
+    assertNext(ts, reusableToken, "see_england_manager_sven", 1, 2.0f, 0, 0);
+    assertNext(ts, reusableToken, "england_manager_sven", 1, 1.7320508f, 0, 0);
+    assertNext(ts, reusableToken, "england_manager_sven_göran", 1, 2.0f, 0, 0);
+    assertNext(ts, reusableToken, "manager_sven", 1, 1.4142135f, 0, 0);
+    assertNext(ts, reusableToken, "manager_sven_göran", 1, 1.7320508f, 0, 0);
+    assertNext(ts, reusableToken, "manager_sven_göran_eriksson", 1, 2.0f, 0, 0);
+    assertNext(ts, reusableToken, "sven_göran", 1, 1.4142135f, 0, 0);
+    assertNext(ts, reusableToken, "sven_göran_eriksson", 1, 1.7320508f, 0, 0);
+    assertNext(ts, reusableToken, "sven_göran_eriksson_in", 1, 2.0f, 0, 0);
+    assertNext(ts, reusableToken, "göran_eriksson", 1, 1.4142135f, 0, 0);
+    assertNext(ts, reusableToken, "göran_eriksson_in", 1, 1.7320508f, 0, 0);
+    assertNext(ts, reusableToken, "göran_eriksson_in_the", 1, 2.0f, 0, 0);
+    assertNext(ts, reusableToken, "eriksson_in", 1, 1.4142135f, 0, 0);
+    assertNext(ts, reusableToken, "eriksson_in_the", 1, 1.7320508f, 0, 0);
+    assertNext(ts, reusableToken, "eriksson_in_the_croud", 1, 2.0f, 0, 0);
 
-    assertNull(ts.next());
+    assertNull(ts.next(reusableToken));
 
   }
 
@@ -417,11 +411,9 @@
 
 
   private Token tokenFactory(String text, int posIncr, int startOffset, int endOffset) {
-    Token token = new Token();
-    token.setTermText(text);
+    Token token = new Token(startOffset, endOffset);
+    token.setTermBuffer(text);
     token.setPositionIncrement(posIncr);
-    token.setStartOffset(startOffset);
-    token.setEndOffset(endOffset);
     return token;
   }
 
@@ -435,61 +427,64 @@
   }
 
   private Token tokenFactory(String text, int posIncr, float weight, int startOffset, int endOffset) {
-    Token token = new Token();
-    token.setTermText(text);
+    Token token = new Token(startOffset, endOffset);
+    token.setTermBuffer(text);
     token.setPositionIncrement(posIncr);
     ShingleMatrixFilter.defaultSettingsCodec.setWeight(token, weight);
-    token.setStartOffset(startOffset);
-    token.setEndOffset(endOffset);
     return token;
   }
 
   private Token tokenFactory(String text, int posIncr, float weight, int startOffset, int endOffset, ShingleMatrixFilter.TokenPositioner positioner) {
-    Token token = new Token();
-    token.setTermText(text);
+    Token token = new Token(startOffset, endOffset);
+    token.setTermBuffer(text);
     token.setPositionIncrement(posIncr);
     ShingleMatrixFilter.defaultSettingsCodec.setWeight(token, weight);
-    token.setStartOffset(startOffset);
-    token.setEndOffset(endOffset);
     ShingleMatrixFilter.defaultSettingsCodec.setTokenPositioner(token, positioner);
     return token;
   }
 
   // assert-methods start here
 
-  private Token assertNext(TokenStream ts, String text) throws IOException {
-    Token token = ts.next(new Token());
-    assertNotNull(token);
-    assertEquals(text, new String(token.termBuffer(), 0, token.termLength()));
-    return token;
+  private Token assertNext(TokenStream ts, final Token reusableToken, String text) throws IOException {
+    Token nextToken = ts.next(reusableToken);
+    assertNotNull(nextToken);
+    assertEquals(text, nextToken.term());
+    return nextToken;
   }
 
-  private Token assertNext(TokenStream ts, String text, int positionIncrement, float boost) throws IOException {
-    Token token = ts.next(new Token());
-    assertNotNull(token);
-    assertEquals(text, new String(token.termBuffer(), 0, token.termLength()));
-    assertEquals(positionIncrement, token.getPositionIncrement());
-    assertEquals(boost, token.getPayload() == null ? 1f : PayloadHelper.decodeFloat(token.getPayload().getData()));
-    return token;
+  private Token assertNext(TokenStream ts, final Token reusableToken, String text, int positionIncrement, float boost) throws IOException {
+    Token nextToken = ts.next(reusableToken);
+    assertNotNull(nextToken);
+    assertEquals(text, nextToken.term());
+    assertEquals(positionIncrement, nextToken.getPositionIncrement());
+    assertEquals(boost, nextToken.getPayload() == null ? 1f : PayloadHelper.decodeFloat(nextToken.getPayload().getData()));
+    return nextToken;
   }
 
-  private Token assertNext(TokenStream ts, String text, int positionIncrement, float boost, int startOffset, int endOffset) throws IOException {
-    Token token = ts.next(new Token());
-    assertNotNull(token);
-    assertEquals(text, new String(token.termBuffer(), 0, token.termLength()));
-    assertEquals(positionIncrement, token.getPositionIncrement());
-    assertEquals(boost, token.getPayload() == null ? 1f : PayloadHelper.decodeFloat(token.getPayload().getData()));
-    assertEquals(startOffset, token.startOffset());
-    assertEquals(endOffset, token.endOffset());
-    return token;
+  private Token assertNext(TokenStream ts, final Token reusableToken, String text, int positionIncrement, float boost, int startOffset, int endOffset) throws IOException {
+    Token nextToken = ts.next(reusableToken);
+    assertNotNull(nextToken);
+    assertEquals(text, nextToken.term());
+    assertEquals(positionIncrement, nextToken.getPositionIncrement());
+    assertEquals(boost, nextToken.getPayload() == null ? 1f : PayloadHelper.decodeFloat(nextToken.getPayload().getData()));
+    assertEquals(startOffset, nextToken.startOffset());
+    assertEquals(endOffset, nextToken.endOffset());
+    return nextToken;
   }
 
-  private Token assertNext(TokenStream ts, String text, int startOffset, int endOffset) throws IOException {
-    Token token = ts.next(new Token());
-    assertNotNull(token);
-    assertEquals(text, new String(token.termBuffer(), 0, token.termLength()));
-    assertEquals(startOffset, token.startOffset());
-    assertEquals(endOffset, token.endOffset());
+  private Token assertNext(TokenStream ts, final Token reusableToken, String text, int startOffset, int endOffset) throws IOException {
+    Token nextToken = ts.next(reusableToken);
+    assertNotNull(nextToken);
+    assertEquals(text, nextToken.term());
+    assertEquals(startOffset, nextToken.startOffset());
+    assertEquals(endOffset, nextToken.endOffset());
+    return nextToken;
+  }
+
+  private static Token createToken(String term, int start, int offset)
+  {
+    Token token = new Token(start, offset);
+    token.setTermBuffer(term);
     return token;
   }
 
@@ -500,9 +495,9 @@
 
     public TokenListStream(TokenStream ts) throws IOException {
       tokens = new ArrayList<Token>();
-      Token token;
-      while ((token = ts.next(new Token())) != null) {
-        tokens.add(token);
+      final Token reusableToken = new Token();
+      for (Token nextToken = ts.next(reusableToken); nextToken != null; nextToken = ts.next(reusableToken)) {
+        tokens.add((Token) nextToken.clone());
       }
     }
 
@@ -512,14 +507,16 @@
 
     private Iterator<Token> iterator;
 
-    public Token next() throws IOException {
+    public Token next(final Token reusableToken) throws IOException {
+      assert reusableToken != null;
       if (iterator == null) {
         iterator = tokens.iterator();
       }
       if (!iterator.hasNext()) {
         return null;
       }
-      return iterator.next();
+      Token nextToken = (Token) iterator.next();
+      return (Token) nextToken.clone();
     }
 
 
Index: contrib/analyzers/src/test/org/apache/lucene/analysis/th/TestThaiAnalyzer.java
===================================================================
--- contrib/analyzers/src/test/org/apache/lucene/analysis/th/TestThaiAnalyzer.java	(revision 686801)
+++ contrib/analyzers/src/test/org/apache/lucene/analysis/th/TestThaiAnalyzer.java	(working copy)
@@ -36,13 +36,13 @@
 		throws Exception {
 
 		TokenStream ts = a.tokenStream("dummy", new StringReader(input));
-
+                final Token reusableToken = new Token();
 		for (int i = 0; i < output.length; i++) {
-			Token t = ts.next();
-			assertNotNull(t);
-			assertEquals(t.termText(), output[i]);
+			Token nextToken = ts.next(reusableToken);
+			assertNotNull(nextToken);
+			assertEquals(nextToken.term(), output[i]);
 		}
-		assertNull(ts.next());
+		assertNull(ts.next(reusableToken));
 		ts.close();
 	}
 
Index: contrib/analyzers/src/test/org/apache/lucene/analysis/payloads/NumericPayloadTokenFilterTest.java
===================================================================
--- contrib/analyzers/src/test/org/apache/lucene/analysis/payloads/NumericPayloadTokenFilterTest.java	(revision 686801)
+++ contrib/analyzers/src/test/org/apache/lucene/analysis/payloads/NumericPayloadTokenFilterTest.java	(working copy)
@@ -43,20 +43,20 @@
     String test = "The quick red fox jumped over the lazy brown dogs";
 
     NumericPayloadTokenFilter nptf = new NumericPayloadTokenFilter(new WordTokenFilter(new WhitespaceTokenizer(new StringReader(test))), 3, "D");
-    Token tok = new Token();
     boolean seenDogs = false;
-    while ((tok = nptf.next(tok)) != null){
-      if (tok.termText().equals("dogs")){
+    final Token reusableToken = new Token();
+    for (Token nextToken = nptf.next(reusableToken); nextToken != null; nextToken = nptf.next(reusableToken)) {
+      if (nextToken.term().equals("dogs")){
         seenDogs = true;
-        assertTrue(tok.type() + " is not equal to " + "D", tok.type().equals("D") == true);
-        assertTrue("tok.getPayload() is null and it shouldn't be", tok.getPayload() != null);
-        byte [] bytes = tok.getPayload().getData();//safe here to just use the bytes, otherwise we should use offset, length
-        assertTrue(bytes.length + " does not equal: " + tok.getPayload().length(), bytes.length == tok.getPayload().length());
-        assertTrue(tok.getPayload().getOffset() + " does not equal: " + 0, tok.getPayload().getOffset() == 0);
+        assertTrue(nextToken.type() + " is not equal to " + "D", nextToken.type().equals("D") == true);
+        assertTrue("nextToken.getPayload() is null and it shouldn't be", nextToken.getPayload() != null);
+        byte [] bytes = nextToken.getPayload().getData();//safe here to just use the bytes, otherwise we should use offset, length
+        assertTrue(bytes.length + " does not equal: " + nextToken.getPayload().length(), bytes.length == nextToken.getPayload().length());
+        assertTrue(nextToken.getPayload().getOffset() + " does not equal: " + 0, nextToken.getPayload().getOffset() == 0);
         float pay = PayloadHelper.decodeFloat(bytes);
         assertTrue(pay + " does not equal: " + 3, pay == 3);
       } else {
-        assertTrue(tok.type() + " is not null and it should be", tok.type().equals("word"));
+        assertTrue(nextToken.type() + " is not null and it should be", nextToken.type().equals("word"));
       }
     }
     assertTrue(seenDogs + " does not equal: " + true, seenDogs == true);
@@ -67,12 +67,13 @@
       super(input);
     }
 
-    public Token next(Token result) throws IOException {
-      result = input.next(result);
-      if (result != null && result.termText().equals("dogs")) {
-        result.setType("D");
+    public Token next(final Token reusableToken) throws IOException {
+      assert reusableToken != null;
+      Token nextToken = input.next(reusableToken);
+      if (nextToken != null && nextToken.term().equals("dogs")) {
+        nextToken.setType("D");
       }
-      return result;
+      return nextToken;
     }
   }
 
Index: contrib/analyzers/src/test/org/apache/lucene/analysis/payloads/TypeAsPayloadTokenFilterTest.java
===================================================================
--- contrib/analyzers/src/test/org/apache/lucene/analysis/payloads/TypeAsPayloadTokenFilterTest.java	(revision 686801)
+++ contrib/analyzers/src/test/org/apache/lucene/analysis/payloads/TypeAsPayloadTokenFilterTest.java	(working copy)
@@ -44,14 +44,14 @@
     String test = "The quick red fox jumped over the lazy brown dogs";
 
     TypeAsPayloadTokenFilter nptf = new TypeAsPayloadTokenFilter(new WordTokenFilter(new WhitespaceTokenizer(new StringReader(test))));
-    Token tok = new Token();
     int count = 0;
-    while ((tok = nptf.next(tok)) != null){
-      assertTrue(tok.type() + " is not null and it should be", tok.type().equals(String.valueOf(Character.toUpperCase(tok.termBuffer()[0]))));
-      assertTrue("tok.getPayload() is null and it shouldn't be", tok.getPayload() != null);
-      String type = new String(tok.getPayload().getData(), "UTF-8");
+    final Token reusableToken = new Token();
+    for (Token nextToken = nptf.next(reusableToken); nextToken != null; nextToken = nptf.next(reusableToken)) {
+      assertTrue(nextToken.type() + " is not null and it should be", nextToken.type().equals(String.valueOf(Character.toUpperCase(nextToken.termBuffer()[0]))));
+      assertTrue("nextToken.getPayload() is null and it shouldn't be", nextToken.getPayload() != null);
+      String type = new String(nextToken.getPayload().getData(), "UTF-8");
       assertTrue("type is null and it shouldn't be", type != null);
-      assertTrue(type + " is not equal to " + tok.type(), type.equals(tok.type()) == true);
+      assertTrue(type + " is not equal to " + nextToken.type(), type.equals(nextToken.type()) == true);
       count++;
     }
     assertTrue(count + " does not equal: " + 10, count == 10);
@@ -64,12 +64,13 @@
 
 
 
-    public Token next(Token result) throws IOException {
-      result = input.next(result);
-      if (result != null) {
-        result.setType(String.valueOf(Character.toUpperCase(result.termBuffer()[0])));
+    public Token next(final Token reusableToken) throws IOException {
+      assert reusableToken != null;
+      Token nextToken = input.next(reusableToken);
+      if (nextToken != null) {
+        nextToken.setType(String.valueOf(Character.toUpperCase(nextToken.termBuffer()[0])));
       }
-      return result;
+      return nextToken;
     }
   }
 
Index: contrib/analyzers/src/test/org/apache/lucene/analysis/payloads/TokenOffsetPayloadTokenFilterTest.java
===================================================================
--- contrib/analyzers/src/test/org/apache/lucene/analysis/payloads/TokenOffsetPayloadTokenFilterTest.java	(revision 686801)
+++ contrib/analyzers/src/test/org/apache/lucene/analysis/payloads/TokenOffsetPayloadTokenFilterTest.java	(working copy)
@@ -42,17 +42,17 @@
     String test = "The quick red fox jumped over the lazy brown dogs";
 
     TokenOffsetPayloadTokenFilter nptf = new TokenOffsetPayloadTokenFilter(new WhitespaceTokenizer(new StringReader(test)));
-    Token tok = new Token();
     int count = 0;
-    while ((tok = nptf.next(tok)) != null){
-      assertTrue("tok is null and it shouldn't be", tok != null);
-      Payload pay = tok.getPayload();
+    final Token reusableToken = new Token();
+    for (Token nextToken = nptf.next(reusableToken); nextToken != null; nextToken = nptf.next(reusableToken)) {
+      assertTrue("nextToken is null and it shouldn't be", nextToken != null);
+      Payload pay = nextToken.getPayload();
       assertTrue("pay is null and it shouldn't be", pay != null);
       byte [] data = pay.getData();
       int start = PayloadHelper.decodeInt(data, 0);
-      assertTrue(start + " does not equal: " + tok.startOffset(), start == tok.startOffset());
+      assertTrue(start + " does not equal: " + nextToken.startOffset(), start == nextToken.startOffset());
       int end = PayloadHelper.decodeInt(data, 4);
-      assertTrue(end + " does not equal: " + tok.endOffset(), end == tok.endOffset());
+      assertTrue(end + " does not equal: " + nextToken.endOffset(), end == nextToken.endOffset());
       count++;
     }
     assertTrue(count + " does not equal: " + 10, count == 10);
Index: contrib/analyzers/src/java/org/apache/lucene/analysis/compound/CompoundWordTokenFilterBase.java
===================================================================
--- contrib/analyzers/src/java/org/apache/lucene/analysis/compound/CompoundWordTokenFilterBase.java	(revision 686801)
+++ contrib/analyzers/src/java/org/apache/lucene/analysis/compound/CompoundWordTokenFilterBase.java	(working copy)
@@ -105,17 +105,18 @@
     return dict;
   }
   
-  public Token next() throws IOException {
+  public Token next(final Token reusableToken) throws IOException {
+    assert reusableToken != null;
     if (tokens.size() > 0) {
       return (Token)tokens.removeFirst();
     }
 
-    Token token = input.next();
-    if (token == null) {
+    Token nextToken = input.next(reusableToken);
+    if (nextToken == null) {
       return null;
     }
 
-    decompose(token);
+    decompose(nextToken);
 
     if (tokens.size() > 0) {
       return (Token)tokens.removeFirst();
@@ -145,17 +146,15 @@
   
   protected final Token createToken(final int offset, final int length,
       final Token prototype) {
-    Token t = new Token(prototype.startOffset() + offset, prototype
-        .startOffset()
-        + offset + length, prototype.type());
-    t.setTermBuffer(prototype.termBuffer(), offset, length);
+    int newStart = prototype.startOffset() + offset;
+    Token t = prototype.clone(prototype.termBuffer(), offset, length, newStart, newStart+length);
     t.setPositionIncrement(0);
     return t;
   }
 
   protected void decompose(final Token token) {
     // In any case we give the original token back
-    tokens.add(token);
+    tokens.add((Token) token.clone());
 
     // Only words longer than minWordSize get processed
     if (token.termLength() < this.minWordSize) {
Index: contrib/analyzers/src/java/org/apache/lucene/analysis/ru/RussianLowerCaseFilter.java
===================================================================
--- contrib/analyzers/src/java/org/apache/lucene/analysis/ru/RussianLowerCaseFilter.java	(revision 686801)
+++ contrib/analyzers/src/java/org/apache/lucene/analysis/ru/RussianLowerCaseFilter.java	(working copy)
@@ -37,25 +37,20 @@
         this.charset = charset;
     }
 
-    public final Token next() throws java.io.IOException
+    public final Token next(final Token reusableToken) throws java.io.IOException
     {
-        Token t = input.next();
+        assert reusableToken != null;
+        Token nextToken = input.next(reusableToken);
 
-        if (t == null)
+        if (nextToken == null)
             return null;
 
-        String txt = t.termText();
-
-        char[] chArray = txt.toCharArray();
-        for (int i = 0; i < chArray.length; i++)
+        char[] chArray = nextToken.termBuffer();
+        int chLen = nextToken.termLength();
+        for (int i = 0; i < chLen; i++)
         {
             chArray[i] = RussianCharsets.toLowerCase(chArray[i], charset);
         }
-
-        String newTxt = new String(chArray);
-        // create new token
-        Token newToken = new Token(newTxt, t.startOffset(), t.endOffset());
-
-        return newToken;
+        return nextToken;
     }
 }
Index: contrib/analyzers/src/java/org/apache/lucene/analysis/ru/RussianStemFilter.java
===================================================================
--- contrib/analyzers/src/java/org/apache/lucene/analysis/ru/RussianStemFilter.java	(revision 686801)
+++ contrib/analyzers/src/java/org/apache/lucene/analysis/ru/RussianStemFilter.java	(working copy)
@@ -35,7 +35,6 @@
     /**
      * The actual token in the input stream.
      */
-    private Token token = null;
     private RussianStemmer stemmer = null;
 
     public RussianStemFilter(TokenStream in, char[] charset)
@@ -47,22 +46,18 @@
     /**
      * @return  Returns the next token in the stream, or null at EOS
      */
-    public final Token next() throws IOException
+    public final Token next(final Token reusableToken) throws IOException
     {
-        if ((token = input.next()) == null)
-        {
+        assert reusableToken != null;
+        Token nextToken = input.next(reusableToken);
+        if (nextToken == null)
             return null;
-        }
-        else
-        {
-            String s = stemmer.stem(token.termText());
-            if (!s.equals(token.termText()))
-            {
-                return new Token(s, token.startOffset(), token.endOffset(),
-                    token.type());
-            }
-            return token;
-        }
+
+        String term = nextToken.term();
+        String s = stemmer.stem(term);
+        if (s != null && !s.equals(term))
+          nextToken.setTermBuffer(s);
+        return nextToken;
     }
 
     /**
Index: contrib/analyzers/src/java/org/apache/lucene/analysis/sinks/TokenTypeSinkTokenizer.java
===================================================================
--- contrib/analyzers/src/java/org/apache/lucene/analysis/sinks/TokenTypeSinkTokenizer.java	(revision 686801)
+++ contrib/analyzers/src/java/org/apache/lucene/analysis/sinks/TokenTypeSinkTokenizer.java	(working copy)
@@ -48,7 +48,7 @@
   public void add(Token t) {
     //check to see if this is a Category
     if (t != null && typeToMatch.equals(t.type())){
-      lst.add(t.clone());
+      super.add(t);
     }
   }
 }
Index: contrib/analyzers/src/java/org/apache/lucene/analysis/sinks/DateRecognizerSinkTokenizer.java
===================================================================
--- contrib/analyzers/src/java/org/apache/lucene/analysis/sinks/DateRecognizerSinkTokenizer.java	(revision 686801)
+++ contrib/analyzers/src/java/org/apache/lucene/analysis/sinks/DateRecognizerSinkTokenizer.java	(working copy)
@@ -73,10 +73,10 @@
     //Check to see if this token is a date
     if (t != null) {
       try {
-        Date date = dateFormat.parse(new String(t.termBuffer(), 0, t.termLength()));//We don't care about the date, just that we can parse it as a date
+        Date date = dateFormat.parse(t.term());//We don't care about the date, just that we can parse it as a date
         if (date != null) {
           t.setType(DATE_TYPE);
-          lst.add(t.clone());
+          super.add(t);
         }
       } catch (ParseException e) {
 
Index: contrib/analyzers/src/java/org/apache/lucene/analysis/de/GermanStemFilter.java
===================================================================
--- contrib/analyzers/src/java/org/apache/lucene/analysis/de/GermanStemFilter.java	(revision 686801)
+++ contrib/analyzers/src/java/org/apache/lucene/analysis/de/GermanStemFilter.java	(working copy)
@@ -37,7 +37,6 @@
     /**
      * The actual token in the input stream.
      */
-    private Token token = null;
     private GermanStemmer stemmer = null;
     private Set exclusionSet = null;
 
@@ -48,7 +47,7 @@
     }
 
     /**
-     * Builds a GermanStemFilter that uses an exclusiontable.
+     * Builds a GermanStemFilter that uses an exclusion table.
      */
     public GermanStemFilter( TokenStream in, Set exclusionSet )
     {
@@ -59,25 +58,24 @@
     /**
      * @return  Returns the next token in the stream, or null at EOS
      */
-    public final Token next()
+    public final Token next(final Token reusableToken)
       throws IOException
     {
-      if ( ( token = input.next() ) == null ) {
+      assert reusableToken != null;
+      Token nextToken = input.next(reusableToken);
+
+      if (nextToken == null)
         return null;
+
+      String term = nextToken.term();
+      // Check the exclusion table.
+      if (exclusionSet == null || !exclusionSet.contains(term)) {
+        String s = stemmer.stem(term);
+        // If not stemmed, don't waste the time adjusting the token.
+        if ((s != null) && !s.equals(term))
+          nextToken.setTermBuffer(s);
       }
-      // Check the exclusiontable
-      else if ( exclusionSet != null && exclusionSet.contains( token.termText() ) ) {
-        return token;
-      }
-      else {
-        String s = stemmer.stem( token.termText() );
-        // If not stemmed, dont waste the time creating a new token
-        if ( !s.equals( token.termText() ) ) {
-          return new Token( s, token.startOffset(),
-            token.endOffset(), token.type() );
-        }
-        return token;
-      }
+      return nextToken;
     }
 
     /**
Index: contrib/analyzers/src/java/org/apache/lucene/analysis/shingle/ShingleFilter.java
===================================================================
--- contrib/analyzers/src/java/org/apache/lucene/analysis/shingle/ShingleFilter.java	(revision 686801)
+++ contrib/analyzers/src/java/org/apache/lucene/analysis/shingle/ShingleFilter.java	(working copy)
@@ -47,7 +47,7 @@
   /**
    * filler token for when positionIncrement is more than 1
    */
-  public static final String FILLER_TOKEN = "_";
+  public static final char[] FILLER_TOKEN = { '_' };
 
 
   /**
@@ -150,11 +150,12 @@
   }
 
   /* (non-Javadoc)
-	 * @see org.apache.lucene.analysis.TokenStream#next()
-	 */
-  public Token next() throws IOException {
+   * @see org.apache.lucene.analysis.TokenStream#next()
+   */
+  public Token next(final Token reusableToken) throws IOException {
+    assert reusableToken != null;
     if (outputBuf.isEmpty()) {
-      fillOutputBuf();
+      fillOutputBuf(reusableToken);
     }
     Token nextToken = null;
     if ( ! outputBuf.isEmpty())
@@ -173,16 +174,19 @@
    * @return the next token, or null if at end of input stream
    * @throws IOException if the input stream has a problem
    */
-  private Token getNextToken() throws IOException {
+  private Token getNextToken(final Token reusableToken) throws IOException {
     if (tokenBuf.isEmpty()) {
-      Token lastToken = input.next();
-      if (lastToken != null) {
-        for (int i = 1; i < lastToken.getPositionIncrement(); i++) {
-          tokenBuf.add(new Token(FILLER_TOKEN, lastToken.startOffset(),
-                                 lastToken.startOffset()));
+      Token nextToken = input.next(reusableToken);
+      if (nextToken != null) {
+        for (int i = 1; i < nextToken.getPositionIncrement(); i++) {
+          Token fillerToken = (Token) nextToken.clone();
+          // A filler token occupies no space
+          fillerToken.setEndOffset(fillerToken.startOffset());
+          fillerToken.setTermBuffer(FILLER_TOKEN, 0, FILLER_TOKEN.length);
+          tokenBuf.add(fillerToken);
         }
-        tokenBuf.add(lastToken);
-        return getNextToken();
+        tokenBuf.add(nextToken.clone());
+        return getNextToken(nextToken);
       } else {
         return null;
       }
@@ -196,15 +200,15 @@
    *
    * @throws IOException if there's a problem getting the next token
    */
-  private void fillOutputBuf() throws IOException {
+  private void fillOutputBuf(Token token) throws IOException {
     boolean addedToken = false;
     /*
      * Try to fill the shingle buffer.
      */
     do {
-      Token token = getNextToken();
+      token = getNextToken(token);
       if (token != null) {
-        shingleBuf.add(token);
+        shingleBuf.add(token.clone());
         if (shingleBuf.size() > maxShingleSize)
         {
           shingleBuf.remove(0);
@@ -235,17 +239,17 @@
     }
 
     int i = 0;
-    Token token = null;
+    Token shingle = null;
     for (Iterator it = shingleBuf.iterator(); it.hasNext(); ) {
-      token = (Token) it.next();
+      shingle = (Token) it.next();
       for (int j = i; j < shingles.length; j++) {
         if (shingles[j].length() != 0) {
           shingles[j].append(TOKEN_SEPARATOR);
         }
-        shingles[j].append(token.termBuffer(), 0, token.termLength());
+        shingles[j].append(shingle.termBuffer(), 0, shingle.termLength());
       }
 
-      endOffsets[i] = token.endOffset();
+      endOffsets[i] = shingle.endOffset();
       i++;
     }
 
@@ -258,17 +262,26 @@
     /*
      * Push new tokens to the output buffer.
      */
+    if (!shingleBuf.isEmpty()) {
+      Token firstShingle = (Token) shingleBuf.get(0);
+      shingle = (Token) firstShingle.clone();
+      shingle.setType(tokenType);
+    }
     for (int j = 1; j < shingleBuf.size(); j++) {
-      Token shingle = new Token(shingles[j].toString(),
-                                ((Token) shingleBuf.get(0)).startOffset(),
-                                endOffsets[j],
-                                tokenType);
+      shingle.setEndOffset(endOffsets[j]);
+      StringBuffer buf = shingles[j];
+      int termLength = buf.length();
+      char[] termBuffer = shingle.termBuffer();
+      if (termBuffer.length < termLength)
+        termBuffer = shingle.resizeTermBuffer(termLength);
+      buf.getChars(0, termLength, termBuffer, 0);
+      shingle.setTermLength(termLength);
       if ((! outputUnigrams) && j == 1) {
         shingle.setPositionIncrement(1);
       } else {
         shingle.setPositionIncrement(0);
       }
-      outputBuf.add(shingle);
+      outputBuf.add(shingle.clone());
     }
   }
 }
Index: contrib/analyzers/src/java/org/apache/lucene/analysis/shingle/ShingleMatrixFilter.java
===================================================================
--- contrib/analyzers/src/java/org/apache/lucene/analysis/shingle/ShingleMatrixFilter.java	(revision 686801)
+++ contrib/analyzers/src/java/org/apache/lucene/analysis/shingle/ShingleMatrixFilter.java	(working copy)
@@ -17,16 +17,23 @@
  * limitations under the License.
  */
 
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.HashSet;
+import java.util.Iterator;
+import java.util.LinkedList;
+import java.util.List;
+import java.util.NoSuchElementException;
+import java.util.Set;
+
 import org.apache.lucene.analysis.Token;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.miscellaneous.EmptyTokenStream;
 import org.apache.lucene.analysis.payloads.PayloadHelper;
 import org.apache.lucene.index.Payload;
 
-import java.io.IOException;
-import java.util.*;
 
-
 /**
  * <p>A ShingleFilter constructs shingles (token n-grams) from a token stream.
  * In other words, it creates combinations of tokens as a single token.
@@ -298,7 +305,8 @@
 
   private Matrix matrix;
 
-  public Token next(Token token) throws IOException {
+  public Token next(final Token reusableToken) throws IOException {
+    assert reusableToken != null;
     if (matrix == null) {
       matrix = new Matrix();
       // fill matrix with maximumShingleSize columns
@@ -318,7 +326,7 @@
         if (ignoringSinglePrefixOrSuffixShingle
             && currentShingleLength == 1
             && (currentPermutationRows.get(currentPermutationTokensStartOffset).getColumn().isFirst() || currentPermutationRows.get(currentPermutationTokensStartOffset).getColumn().isLast())) {
-          return next(token);
+          return next(reusableToken);
         }
 
         int termLength = 0;
@@ -336,21 +344,21 @@
 
         // only produce shingles that not already has been created
         if (!shinglesSeen.add(shingle)) {
-          return next(token);
+          return next(reusableToken);
         }
 
         // shingle token factory
-        StringBuilder sb = new StringBuilder(termLength + 10); // paranormal abillity to forsay the future.
+        StringBuilder sb = new StringBuilder(termLength + 10); // paranormal ability to foresee the future.
         for (Token shingleToken : shingle) {
           if (spacerCharacter != null && sb.length() > 0) {
             sb.append(spacerCharacter);
           }
           sb.append(shingleToken.termBuffer(), 0, shingleToken.termLength());
         }
-        token.setTermText(sb.toString());
-        updateToken(token, shingle, currentPermutationTokensStartOffset, currentPermutationRows, currentPermuationTokens);
+        reusableToken.setTermBuffer(sb.toString());
+        updateToken(reusableToken, shingle, currentPermutationTokensStartOffset, currentPermutationRows, currentPermuationTokens);
 
-        return token;
+        return reusableToken;
 
       } else {
 
@@ -360,7 +368,7 @@
           // reset shingle size and move one step to the right in the current tokens permutation
           currentPermutationTokensStartOffset++;
           currentShingleLength = minimumShingleSize - 1;
-          return next(token);
+          return next(reusableToken);
         }
 
 
@@ -411,7 +419,7 @@
         }
 
         nextTokensPermutation();
-        return next(token);
+        return next(reusableToken);
 
       }
     }
@@ -426,7 +434,7 @@
 
     nextTokensPermutation();
 
-    return next(token);
+    return next(reusableToken);
   }
 
   /**
Index: contrib/analyzers/src/java/org/apache/lucene/analysis/el/GreekLowerCaseFilter.java
===================================================================
--- contrib/analyzers/src/java/org/apache/lucene/analysis/el/GreekLowerCaseFilter.java	(revision 686801)
+++ contrib/analyzers/src/java/org/apache/lucene/analysis/el/GreekLowerCaseFilter.java	(working copy)
@@ -35,25 +35,20 @@
         this.charset = charset;
     }
 
-    public final Token next() throws java.io.IOException
+    public final Token next(final Token reusableToken) throws java.io.IOException
     {
-        Token t = input.next();
+        assert reusableToken != null;
+        Token nextToken = input.next(reusableToken);
 
-        if (t == null)
+        if (nextToken == null)
             return null;
 
-        String txt = t.termText();
-
-        char[] chArray = txt.toCharArray();
-        for (int i = 0; i < chArray.length; i++)
+        char[] chArray = nextToken.termBuffer();
+        int chLen = nextToken.termLength();
+        for (int i = 0; i < chLen; i++)
         {
             chArray[i] = GreekCharsets.toLowerCase(chArray[i], charset);
         }
-
-        String newTxt = new String(chArray);
-        // create new token
-        Token newToken = new Token(newTxt, t.startOffset(), t.endOffset());
-
-        return newToken;
+        return nextToken;
     }
 }
Index: contrib/analyzers/src/java/org/apache/lucene/analysis/cn/ChineseFilter.java
===================================================================
--- contrib/analyzers/src/java/org/apache/lucene/analysis/cn/ChineseFilter.java	(revision 686801)
+++ contrib/analyzers/src/java/org/apache/lucene/analysis/cn/ChineseFilter.java	(working copy)
@@ -18,8 +18,11 @@
  */
 
 import java.util.Hashtable;
-import org.apache.lucene.analysis.*;
 
+import org.apache.lucene.analysis.Token;
+import org.apache.lucene.analysis.TokenFilter;
+import org.apache.lucene.analysis.TokenStream;
+
 /**
  * Title: ChineseFilter
  * Description: Filter with a stop word table
@@ -61,10 +64,11 @@
             stopTable.put(STOP_WORDS[i], STOP_WORDS[i]);
     }
 
-    public final Token next() throws java.io.IOException {
+    public final Token next(final Token reusableToken) throws java.io.IOException {
+        assert reusableToken != null;
 
-        for (Token token = input.next(); token != null; token = input.next()) {
-            String text = token.termText();
+        for (Token nextToken = input.next(reusableToken); nextToken != null; nextToken = input.next(reusableToken)) {
+            String text = nextToken.term();
 
           // why not key off token type here assuming ChineseTokenizer comes first?
             if (stopTable.get(text) == null) {
@@ -75,7 +79,7 @@
 
                     // English word/token should larger than 1 character.
                     if (text.length()>1) {
-                        return token;
+                        return nextToken;
                     }
                     break;
                 case Character.OTHER_LETTER:
@@ -83,7 +87,7 @@
                     // One Chinese character as one Chinese word.
                     // Chinese word extraction to be added later here.
 
-                    return token;
+                    return nextToken;
                 }
 
             }
Index: contrib/analyzers/src/java/org/apache/lucene/analysis/cn/ChineseTokenizer.java
===================================================================
--- contrib/analyzers/src/java/org/apache/lucene/analysis/cn/ChineseTokenizer.java	(revision 686801)
+++ contrib/analyzers/src/java/org/apache/lucene/analysis/cn/ChineseTokenizer.java	(working copy)
@@ -19,9 +19,11 @@
 
 
 import java.io.Reader;
-import org.apache.lucene.analysis.*;
 
+import org.apache.lucene.analysis.Token;
+import org.apache.lucene.analysis.Tokenizer;
 
+
 /**
  * Title: ChineseTokenizer
  * Description: Extract tokens from the Stream using Character.getType()
@@ -75,17 +77,19 @@
 
     }
 
-    private final Token flush() {
+    private final Token flush(final Token token) {
 
         if (length>0) {
-            //System.out.println(new String(buffer, 0, length));
-            return new Token(new String(buffer, 0, length), start, start+length);
+            //System.out.println(new String(buffer, 0,
+            //length));
+          return token.reinit(buffer, 0, length, start, start+length);
         }
         else
             return null;
     }
 
-    public final Token next() throws java.io.IOException {
+    public final Token next(final Token reusableToken) throws java.io.IOException {
+        assert reusableToken != null;
 
         length = 0;
         start = offset;
@@ -101,7 +105,7 @@
                 bufferIndex = 0;
             }
 
-            if (dataLen == -1) return flush();
+            if (dataLen == -1) return flush(reusableToken);
             else
                 c = ioBuffer[bufferIndex++];
 
@@ -112,20 +116,20 @@
             case Character.LOWERCASE_LETTER:
             case Character.UPPERCASE_LETTER:
                 push(c);
-                if (length == MAX_WORD_LEN) return flush();
+                if (length == MAX_WORD_LEN) return flush(reusableToken);
                 break;
 
             case Character.OTHER_LETTER:
                 if (length>0) {
                     bufferIndex--;
                     offset--;
-                    return flush();
+                    return flush(reusableToken);
                 }
                 push(c);
-                return flush();
+                return flush(reusableToken);
 
             default:
-                if (length>0) return flush();
+                if (length>0) return flush(reusableToken);
                 break;
             }
         }
Index: contrib/analyzers/src/java/org/apache/lucene/analysis/miscellaneous/SingleTokenTokenStream.java
===================================================================
--- contrib/analyzers/src/java/org/apache/lucene/analysis/miscellaneous/SingleTokenTokenStream.java	(revision 686801)
+++ contrib/analyzers/src/java/org/apache/lucene/analysis/miscellaneous/SingleTokenTokenStream.java	(working copy)
@@ -28,20 +28,23 @@
 public class SingleTokenTokenStream extends TokenStream {
 
   private boolean exhausted = false;
+  // The token needs to be immutable, so work with clones!
   private Token token;
 
 
   public SingleTokenTokenStream(Token token) {
-    this.token = token;
+    assert token != null;
+    this.token = (Token) token.clone();
   }
 
 
-  public Token next(Token result) throws IOException {
+  public Token next(final Token reusableToken) throws IOException {
+    assert reusableToken != null;
     if (exhausted) {
       return null;
     }
     exhausted = true;
-    return token;
+    return (Token) token.clone();
   }
 
 
@@ -50,10 +53,10 @@
   }
 
   public Token getToken() {
-    return token;
+    return (Token) token.clone();
   }
 
   public void setToken(Token token) {
-    this.token = token;
+    this.token = (Token) token.clone();
   }
 }
Index: contrib/analyzers/src/java/org/apache/lucene/analysis/miscellaneous/PrefixAwareTokenFilter.java
===================================================================
--- contrib/analyzers/src/java/org/apache/lucene/analysis/miscellaneous/PrefixAwareTokenFilter.java	(revision 686801)
+++ contrib/analyzers/src/java/org/apache/lucene/analysis/miscellaneous/PrefixAwareTokenFilter.java	(working copy)
@@ -41,30 +41,34 @@
     prefixExhausted = false;
   }
 
-  private CopyableToken previousPrefixToken = new CopyableToken();
+  private Token previousPrefixToken = new Token();
 
   private boolean prefixExhausted;
 
-  public Token next(Token result) throws IOException {
+  public Token next(final Token reusableToken) throws IOException {
+    assert reusableToken != null;
 
-    Token buf = result;
-
     if (!prefixExhausted) {
-      result = prefix.next(result);
-      if (result == null) {
+      Token nextToken = prefix.next(reusableToken);
+      if (nextToken == null) {
         prefixExhausted = true;
       } else {
-        previousPrefixToken.copyFrom(result);        
-        return result;
+        previousPrefixToken.reinit(nextToken);
+        // Make it a deep copy
+        Payload p = previousPrefixToken.getPayload();
+        if (p != null) {
+          previousPrefixToken.setPayload((Payload) p.clone());
+        }
+        return nextToken;
       }
     }
 
-    result = suffix.next(buf);
-    if (result == null) {
+    Token nextToken = suffix.next(reusableToken);
+    if (nextToken == null) {
       return null;
     }
 
-    return updateSuffixToken(result, previousPrefixToken);
+    return updateSuffixToken(nextToken, previousPrefixToken);
   }
 
   /**
@@ -98,7 +102,6 @@
 
   }
 
-
   public TokenStream getPrefix() {
     return prefix;
   }
@@ -114,35 +117,4 @@
   public void setSuffix(TokenStream suffix) {
     this.suffix = suffix;
   }
-
-
-  public static class CopyableToken extends Token {
-
-    private Payload buf = new Payload();
-
-    public void copyFrom(Token source) {
-      if (source.termBuffer() != null) {
-        setTermBuffer(source.termBuffer(), 0, source.termLength());
-      } else {
-        setTermText(null);
-        setTermLength(0);
-      }
-
-      setPositionIncrement(source.getPositionIncrement());
-      setFlags(source.getFlags());
-      setStartOffset(source.startOffset());
-      setEndOffset(source.endOffset());
-      setType(source.type());
-      if (source.getPayload() == null) {
-        setPayload(null);
-      } else {
-        setPayload(buf);        
-        if (buf.getData() == null || buf.getData().length < source.getPayload().length()) {
-          buf.setData(new byte[source.getPayload().length()]);
-        }
-        source.getPayload().copyTo(buf.getData(), 0);
-        buf.setData(buf.getData(), 0, source.getPayload().length());
-      }
-    }
-  }
 }
Index: contrib/analyzers/src/java/org/apache/lucene/analysis/miscellaneous/EmptyTokenStream.java
===================================================================
--- contrib/analyzers/src/java/org/apache/lucene/analysis/miscellaneous/EmptyTokenStream.java	(revision 686801)
+++ contrib/analyzers/src/java/org/apache/lucene/analysis/miscellaneous/EmptyTokenStream.java	(working copy)
@@ -27,18 +27,8 @@
  */
 public class EmptyTokenStream extends TokenStream {
 
-  public Token next() throws IOException {
+  public Token next(final Token reusableToken) throws IOException {
+    assert reusableToken != null;
     return null;
   }
-
-  public Token next(Token result) throws IOException {
-    return null;
-  }
-
-  public void reset() throws IOException {
-  }
-
-  public void close() throws IOException {
-  }
-
 }
Index: contrib/analyzers/src/java/org/apache/lucene/analysis/miscellaneous/PrefixAndSuffixAwareTokenFilter.java
===================================================================
--- contrib/analyzers/src/java/org/apache/lucene/analysis/miscellaneous/PrefixAndSuffixAwareTokenFilter.java	(revision 686801)
+++ contrib/analyzers/src/java/org/apache/lucene/analysis/miscellaneous/PrefixAndSuffixAwareTokenFilter.java	(working copy)
@@ -55,8 +55,9 @@
   }
 
 
-  public Token next(Token result) throws IOException {
-    return suffix.next(result);
+  public Token next(final Token reusableToken) throws IOException {
+    assert reusableToken != null;
+    return suffix.next(reusableToken);
   }
 
 
Index: contrib/analyzers/src/java/org/apache/lucene/analysis/br/BrazilianStemFilter.java
===================================================================
--- contrib/analyzers/src/java/org/apache/lucene/analysis/br/BrazilianStemFilter.java	(revision 686801)
+++ contrib/analyzers/src/java/org/apache/lucene/analysis/br/BrazilianStemFilter.java	(working copy)
@@ -36,7 +36,6 @@
   /**
    * The actual token in the input stream.
    */
-  private Token token = null;
   private BrazilianStemmer stemmer = null;
   private Set exclusions = null;
 
@@ -53,22 +52,23 @@
   /**
    * @return Returns the next token in the stream, or null at EOS.
    */
-  public final Token next()
+  public final Token next(final Token reusableToken)
       throws IOException {
-    if ((token = input.next()) == null) {
+    assert reusableToken != null;
+    Token nextToken = input.next(reusableToken);
+    if (nextToken == null)
       return null;
+
+    String term = nextToken.term();
+
+    // Check the exclusion table.
+    if (exclusions == null || !exclusions.contains(term)) {
+      String s = stemmer.stem(term);
+      // If not stemmed, don't waste the time adjusting the token.
+      if ((s != null) && !s.equals(term))
+        nextToken.setTermBuffer(s);
     }
-    // Check the exclusiontable.
-    else if (exclusions != null && exclusions.contains(token.termText())) {
-      return token;
-    } else {
-      String s = stemmer.stem(token.termText());
-      // If not stemmed, dont waste the time creating a new token.
-      if ((s != null) && !s.equals(token.termText())) {
-        return new Token(s, token.startOffset(), token.endOffset(), token.type());
-      }
-      return token;
-    }
+    return nextToken;
   }
 }
 
Index: contrib/analyzers/src/java/org/apache/lucene/analysis/ngram/NGramTokenizer.java
===================================================================
--- contrib/analyzers/src/java/org/apache/lucene/analysis/ngram/NGramTokenizer.java	(revision 686801)
+++ contrib/analyzers/src/java/org/apache/lucene/analysis/ngram/NGramTokenizer.java	(working copy)
@@ -64,7 +64,8 @@
   }
 
   /** Returns the next token in the stream, or null at EOS. */
-  public final Token next() throws IOException {
+  public final Token next(final Token reusableToken) throws IOException {
+    assert reusableToken != null;
     if (!started) {
       started = true;
       gramSize = minGram;
@@ -82,9 +83,9 @@
       if (pos+gramSize > inLen)
         return null;
     }
-    String gram = inStr.substring(pos, pos+gramSize);
+
     int oldPos = pos;
     pos++;
-    return new Token(gram, oldPos, oldPos+gramSize);
+    return reusableToken.reinit(inStr, oldPos, gramSize, oldPos, oldPos+gramSize);
   }
 }
Index: contrib/analyzers/src/java/org/apache/lucene/analysis/ngram/EdgeNGramTokenFilter.java
===================================================================
--- contrib/analyzers/src/java/org/apache/lucene/analysis/ngram/EdgeNGramTokenFilter.java	(revision 686801)
+++ contrib/analyzers/src/java/org/apache/lucene/analysis/ngram/EdgeNGramTokenFilter.java	(working copy)
@@ -115,30 +115,30 @@
   }
 
   /** Returns the next token in the stream, or null at EOS. */
-  public final Token next() throws IOException {
+  public final Token next(final Token reusableToken) throws IOException {
+    assert reusableToken != null;
     if (ngrams.size() > 0) {
       return (Token) ngrams.removeFirst();
     }
 
-    Token token = input.next();
-    if (token == null) {
+    Token nextToken = input.next(reusableToken);
+    if (nextToken == null)
       return null;
-    }
 
-    ngram(token);
+    ngram(nextToken);
     if (ngrams.size() > 0)
       return (Token) ngrams.removeFirst();
     else
       return null;
   }
 
-  private void ngram(Token token) {
-    String inStr = token.termText();
-    int inLen = inStr.length();
+  private void ngram(final Token token) {
+    int termLength = token.termLength();
+    char[] termBuffer = token.termBuffer();
     int gramSize = minGram;
     while (gramSize <= maxGram) {
       // if the remaining input is too short, we can't generate any n-grams
-      if (gramSize > inLen) {
+      if (gramSize > termLength) {
         return;
       }
 
@@ -147,13 +147,13 @@
         return;
       }
 
-      Token tok;
-      if (side == Side.FRONT) {
-        tok = new Token(inStr.substring(0, gramSize), 0, gramSize);
-      }
-      else {
-        tok = new Token(inStr.substring(inLen-gramSize), inLen-gramSize, inLen);
-      }
+      // grab gramSize chars from front or back
+      int start = side == Side.FRONT ? 0 : termLength - gramSize;
+      int end = start + gramSize;
+      Token tok = (Token) token.clone();
+      tok.setStartOffset(start);
+      tok.setEndOffset(end);
+      tok.setTermBuffer(termBuffer, start, gramSize);
       ngrams.add(tok);
       gramSize++;
     }
Index: contrib/analyzers/src/java/org/apache/lucene/analysis/ngram/EdgeNGramTokenizer.java
===================================================================
--- contrib/analyzers/src/java/org/apache/lucene/analysis/ngram/EdgeNGramTokenizer.java	(revision 686801)
+++ contrib/analyzers/src/java/org/apache/lucene/analysis/ngram/EdgeNGramTokenizer.java	(working copy)
@@ -19,6 +19,7 @@
 
 import org.apache.lucene.analysis.Token;
 import org.apache.lucene.analysis.Tokenizer;
+import org.apache.lucene.analysis.ngram.EdgeNGramTokenFilter.Side;
 
 import java.io.IOException;
 import java.io.Reader;
@@ -113,13 +114,14 @@
   }
 
   /** Returns the next token in the stream, or null at EOS. */
-  public final Token next() throws IOException {
+  public final Token next(final Token reusableToken) throws IOException {
+    assert reusableToken != null;
     // if we are just starting, read the whole input
     if (!started) {
       started = true;
       char[] chars = new char[1024];
       input.read(chars);
-      inStr = new String(chars).trim();  // remove any trailing empty strings
+      inStr = new String(chars).trim();  // remove any leading or trailing spaces
       inLen = inStr.length();
       gramSize = minGram;
     }
@@ -134,15 +136,13 @@
       return null;
     }
 
-    Token tok;
-    if (side == Side.FRONT) {
-      tok = new Token(inStr.substring(0, gramSize), 0, gramSize);
-    }
-    else {
-      tok = new Token(inStr.substring(inLen-gramSize), inLen-gramSize, inLen);
-    }
-
+    // grab gramSize chars from front or back
+    int start = side == Side.FRONT ? 0 : inLen - gramSize;
+    int end = start + gramSize;
+    reusableToken.setTermBuffer(inStr, start, gramSize);
+    reusableToken.setStartOffset(start);
+    reusableToken.setEndOffset(end);
     gramSize++;
-    return tok;
+    return reusableToken;
   }
 }
Index: contrib/analyzers/src/java/org/apache/lucene/analysis/ngram/NGramTokenFilter.java
===================================================================
--- contrib/analyzers/src/java/org/apache/lucene/analysis/ngram/NGramTokenFilter.java	(revision 686801)
+++ contrib/analyzers/src/java/org/apache/lucene/analysis/ngram/NGramTokenFilter.java	(working copy)
@@ -63,17 +63,17 @@
   }
 
   /** Returns the next token in the stream, or null at EOS. */
-  public final Token next() throws IOException {
+  public final Token next(final Token reusableToken) throws IOException {
+    assert reusableToken != null;
     if (ngrams.size() > 0) {
       return (Token) ngrams.removeFirst();
     }
 
-    Token token = input.next();
-    if (token == null) {
+    Token nextToken = input.next(reusableToken);
+    if (nextToken == null)
       return null;
-    }
 
-    ngram(token);
+    ngram(nextToken);
     if (ngrams.size() > 0)
       return (Token) ngrams.removeFirst();
     else
@@ -81,16 +81,13 @@
   }
 
   private void ngram(Token token) { 
-    String inStr = token.termText();
-    int inLen = inStr.length();
+    char[] termBuffer = token.termBuffer();
+    int termLength = token.termLength();
     int gramSize = minGram;
     while (gramSize <= maxGram) {
       int pos = 0;                        // reset to beginning of string
-      while (pos+gramSize <= inLen) {     // while there is input
-        String gram = inStr.substring(pos, pos+gramSize);
-        Token tok = new Token(gram, pos, pos+gramSize);
-//        tok.setPositionIncrement(pos);
-        ngrams.add(tok);
+      while (pos+gramSize <= termLength) {     // while there is input
+        ngrams.add(token.clone(termBuffer, pos, gramSize, pos, pos+gramSize));
         pos++;
       }
       gramSize++;                         // increase n-gram size
Index: contrib/analyzers/src/java/org/apache/lucene/analysis/fr/FrenchStemFilter.java
===================================================================
--- contrib/analyzers/src/java/org/apache/lucene/analysis/fr/FrenchStemFilter.java	(revision 686801)
+++ contrib/analyzers/src/java/org/apache/lucene/analysis/fr/FrenchStemFilter.java	(working copy)
@@ -37,12 +37,11 @@
 	/**
 	 * The actual token in the input stream.
 	 */
-	private Token token = null;
 	private FrenchStemmer stemmer = null;
 	private Set exclusions = null;
 
 	public FrenchStemFilter( TokenStream in ) {
-    super(in);
+          super(in);
 		stemmer = new FrenchStemmer();
 	}
 
@@ -55,23 +54,23 @@
 	/**
 	 * @return  Returns the next token in the stream, or null at EOS
 	 */
-	public final Token next()
+	public final Token next(final Token reusableToken)
 		throws IOException {
-		if ( ( token = input.next() ) == null ) {
+                assert reusableToken != null;
+                Token nextToken = input.next(reusableToken);
+		if (nextToken == null)
 			return null;
+
+		String term = nextToken.term();
+
+		// Check the exclusion table
+		if ( exclusions == null || !exclusions.contains( term ) ) {
+			String s = stemmer.stem( term );
+			// If not stemmed, don't waste the time  adjusting the token.
+			if ((s != null) && !s.equals( term ) )
+			   nextToken.setTermBuffer(s);
 		}
-		// Check the exclusiontable
-		else if ( exclusions != null && exclusions.contains( token.termText() ) ) {
-			return token;
-		}
-		else {
-			String s = stemmer.stem( token.termText() );
-			// If not stemmed, dont waste the time creating a new token
-			if ( !s.equals( token.termText() ) ) {
-			   return new Token( s, token.startOffset(), token.endOffset(), token.type());
-			}
-			return token;
-		}
+                return nextToken;
 	}
 	/**
 	 * Set a alternative/custom FrenchStemmer for this filter.
Index: contrib/analyzers/src/java/org/apache/lucene/analysis/fr/ElisionFilter.java
===================================================================
--- contrib/analyzers/src/java/org/apache/lucene/analysis/fr/ElisionFilter.java	(revision 686801)
+++ contrib/analyzers/src/java/org/apache/lucene/analysis/fr/ElisionFilter.java	(working copy)
@@ -38,7 +38,7 @@
 public class ElisionFilter extends TokenFilter {
   private Set articles = null;
 
-  private static String apostrophes = "'’";
+  private static char[] apostrophes = {'\'', '’'};
 
   public void setArticles(Set articles) {
     this.articles = new HashSet();
@@ -74,25 +74,36 @@
   }
 
   /**
-   * Returns the next input Token whith termText() without elisioned start
+   * Returns the next input Token with term() without elisioned start
    */
-  public Token next() throws IOException {
-    Token t = input.next();
-    if (t == null)
+  public Token next(final Token reusableToken) throws IOException {
+    assert reusableToken != null;
+    Token nextToken = input.next(reusableToken);
+    if (nextToken == null)
       return null;
-    String text = t.termText();
-    System.out.println(text);
-    int minPoz = -1;
-    int poz;
-    for (int i = 0; i < apostrophes.length(); i++) {
-      poz = text.indexOf(apostrophes.charAt(i));
-      if (poz != -1)
-        minPoz = (minPoz == -1) ? poz : Math.min(poz, minPoz);
+
+    char[] termBuffer = nextToken.termBuffer();
+    int termLength = nextToken.termLength();
+
+    int minPoz = Integer.MAX_VALUE;
+    for (int i = 0; i < apostrophes.length; i++) {
+      char apos = apostrophes[i];
+      // The equivalent of String.indexOf(ch)
+      for (int poz = 0; poz < termLength ; poz++) {
+        if (termBuffer[poz] == apos) {
+            minPoz = Math.min(poz, minPoz);
+            break;
+        }
+      }
     }
-    if (minPoz != -1
-        && articles.contains(text.substring(0, minPoz).toLowerCase()))
-      text = text.substring(minPoz + 1);
-    return new Token(text, t.startOffset(), t.endOffset(), t.type());
+
+    // An apostrophe has been found. If the prefix is an article strip it off.
+    if (minPoz != Integer.MAX_VALUE
+        && articles.contains(new String(nextToken.termBuffer(), 0, minPoz).toLowerCase())) {
+      nextToken.setTermBuffer(nextToken.termBuffer(), minPoz + 1, nextToken.termLength() - (minPoz + 1));
+    }
+
+    return nextToken;
   }
 
 }
Index: contrib/analyzers/src/java/org/apache/lucene/analysis/cjk/CJKTokenizer.java
===================================================================
--- contrib/analyzers/src/java/org/apache/lucene/analysis/cjk/CJKTokenizer.java	(revision 686801)
+++ contrib/analyzers/src/java/org/apache/lucene/analysis/cjk/CJKTokenizer.java	(working copy)
@@ -26,7 +26,7 @@
 /**
  * CJKTokenizer was modified from StopTokenizer which does a decent job for
  * most European languages. It performs other token methods for double-byte
- * Characters: the token will return at each two charactors with overlap match.<br>
+ * Characters: the token will return at each two characters with overlap match.<br>
  * Example: "java C1C2C3C4" will be segment to: "java" "C1C2" "C2C3" "C3C4" it
  * also need filter filter zero length token ""<br>
  * for Digit: digit, '+', '#' will token as letter<br>
@@ -96,24 +96,26 @@
      * See http://java.sun.com/j2se/1.3/docs/api/java/lang/Character.UnicodeBlock.html
      * for detail.
      *
+     * @param reusableToken a reusable token
      * @return Token
      *
      * @throws java.io.IOException - throw IOException when read error <br>
-     *         hanppened in the InputStream
+     *         happened in the InputStream
      *
      */
-    public final Token next() throws java.io.IOException {
+    public final Token next(final Token reusableToken) throws java.io.IOException {
         /** how many character(s) has been stored in buffer */
+        assert reusableToken != null;
         int length = 0;
 
         /** the position used to create Token */
         int start = offset;
 
         while (true) {
-            /** current charactor */
+            /** current character */
             char c;
 
-            /** unicode block of current charactor for detail */
+            /** unicode block of current character for detail */
             Character.UnicodeBlock ub;
 
             offset++;
@@ -198,7 +200,7 @@
                     }
                 }
             } else {
-                // non-ASCII letter, eg."C1C2C3C4"
+                // non-ASCII letter, e.g."C1C2C3C4"
                 if (Character.isLetter(c)) {
                     if (length == 0) {
                         start = offset - 1;
@@ -236,8 +238,6 @@
             }
         }
 
-        return new Token(new String(buffer, 0, length), start, start + length,
-                         tokenType
-                        );
+        return reusableToken.reinit(buffer, 0, length, start, start+length, tokenType);
     }
 }
Index: contrib/analyzers/src/java/org/apache/lucene/analysis/nl/DutchStemFilter.java
===================================================================
--- contrib/analyzers/src/java/org/apache/lucene/analysis/nl/DutchStemFilter.java	(revision 686801)
+++ contrib/analyzers/src/java/org/apache/lucene/analysis/nl/DutchStemFilter.java	(working copy)
@@ -38,7 +38,6 @@
   /**
    * The actual token in the input stream.
    */
-  private Token token = null;
   private DutchStemmer stemmer = null;
   private Set exclusions = null;
 
@@ -48,7 +47,7 @@
   }
 
   /**
-   * Builds a DutchStemFilter that uses an exclusiontable.
+   * Builds a DutchStemFilter that uses an exclusion table.
    */
   public DutchStemFilter(TokenStream _in, Set exclusiontable) {
     this(_in);
@@ -66,23 +65,22 @@
   /**
    * @return Returns the next token in the stream, or null at EOS
    */
-  public Token next() throws IOException {
-    if ((token = input.next()) == null) {
+  public Token next(Token reusableToken) throws IOException {
+    assert reusableToken != null;
+    Token nextToken = input.next(reusableToken);
+    if (nextToken == null)
       return null;
-    }
 
-    // Check the exclusiontable
-    else if (exclusions != null && exclusions.contains(token.termText())) {
-      return token;
-    } else {
-      String s = stemmer.stem(token.termText());
-      // If not stemmed, dont waste the time creating a new token
-      if (!s.equals(token.termText())) {
-        return new Token(s, token.startOffset(),
-            token.endOffset(), token.type());
-      }
-      return token;
+    String term = nextToken.term();
+
+    // Check the exclusion table.
+    if (exclusions == null || !exclusions.contains(term)) {
+      String s = stemmer.stem(term);
+      // If not stemmed, don't waste the time adjusting the token.
+      if ((s != null) && !s.equals(term))
+        nextToken.setTermBuffer(s);
     }
+    return nextToken;
   }
 
   /**
Index: contrib/analyzers/src/java/org/apache/lucene/analysis/th/ThaiWordFilter.java
===================================================================
--- contrib/analyzers/src/java/org/apache/lucene/analysis/th/ThaiWordFilter.java	(revision 686801)
+++ contrib/analyzers/src/java/org/apache/lucene/analysis/th/ThaiWordFilter.java	(working copy)
@@ -40,31 +40,38 @@
     breaker = BreakIterator.getWordInstance(new Locale("th"));
   }
   
-  public Token next() throws IOException {
+  public Token next(final Token reusableToken) throws IOException {
+    assert reusableToken != null;
     if (thaiToken != null) {
-      String text = thaiToken.termText();
       int start = breaker.current();
       int end = breaker.next();
       if (end != BreakIterator.DONE) {
-        return new Token(text.substring(start, end), 
-            thaiToken.startOffset()+start, thaiToken.startOffset()+end, thaiToken.type());
+        reusableToken.reinit(thaiToken, thaiToken.termBuffer(), start, end - start);
+        reusableToken.setStartOffset(thaiToken.startOffset()+start);
+        reusableToken.setEndOffset(thaiToken.endOffset()+end);
+        return reusableToken;
       }
       thaiToken = null;
     }
-    Token tk = input.next();
-    if (tk == null) {
+
+    Token nextToken = input.next(reusableToken);
+    if (nextToken == null || nextToken.termLength() == 0) {
       return null;
     }
-    String text = tk.termText();
+
+    String text = nextToken.term();
     if (UnicodeBlock.of(text.charAt(0)) != UnicodeBlock.THAI) {
-      return new Token(text.toLowerCase(), tk.startOffset(), tk.endOffset(), tk.type());
+      nextToken.setTermBuffer(text.toLowerCase());
+      return nextToken;
     }
-    thaiToken = tk;
+
+    thaiToken = (Token) nextToken.clone();
     breaker.setText(text);
     int end = breaker.next();
     if (end != BreakIterator.DONE) {
-      return new Token(text.substring(0, end), 
-          thaiToken.startOffset(), thaiToken.startOffset()+end, thaiToken.type());
+      nextToken.setTermBuffer(text, 0, end);
+      nextToken.setEndOffset(nextToken.startOffset() + end);
+      return nextToken;
     }
     return null;
   }
Index: contrib/analyzers/src/java/org/apache/lucene/analysis/payloads/NumericPayloadTokenFilter.java
===================================================================
--- contrib/analyzers/src/java/org/apache/lucene/analysis/payloads/NumericPayloadTokenFilter.java	(revision 686801)
+++ contrib/analyzers/src/java/org/apache/lucene/analysis/payloads/NumericPayloadTokenFilter.java	(working copy)
@@ -41,11 +41,12 @@
     this.typeMatch = typeMatch;
   }
 
-  public Token next(Token result) throws IOException {
-    result = input.next(result);
-    if (result != null && result.type().equals(typeMatch)){
-      result.setPayload(thePayload);
+  public Token next(final Token reusableToken) throws IOException {
+    assert reusableToken != null;
+    Token nextToken = input.next(reusableToken);
+    if (nextToken != null && nextToken.type().equals(typeMatch)){
+      nextToken.setPayload(thePayload);
     }
-    return result;
+    return nextToken;
   }
 }
Index: contrib/analyzers/src/java/org/apache/lucene/analysis/payloads/TypeAsPayloadTokenFilter.java
===================================================================
--- contrib/analyzers/src/java/org/apache/lucene/analysis/payloads/TypeAsPayloadTokenFilter.java	(revision 686801)
+++ contrib/analyzers/src/java/org/apache/lucene/analysis/payloads/TypeAsPayloadTokenFilter.java	(working copy)
@@ -39,11 +39,12 @@
   }
 
 
-  public Token next(Token result) throws IOException {
-    result = input.next(result);
-    if (result != null && result.type() != null && result.type().equals("") == false){
-      result.setPayload(new Payload(result.type().getBytes("UTF-8")));
+  public Token next(final Token reusableToken) throws IOException {
+    assert reusableToken != null;
+    Token nextToken = input.next(reusableToken);
+    if (nextToken != null && nextToken.type() != null && nextToken.type().equals("") == false){
+      nextToken.setPayload(new Payload(nextToken.type().getBytes("UTF-8")));
     }
-    return result;
+    return nextToken;
   }
 }
\ No newline at end of file
Index: contrib/analyzers/src/java/org/apache/lucene/analysis/payloads/TokenOffsetPayloadTokenFilter.java
===================================================================
--- contrib/analyzers/src/java/org/apache/lucene/analysis/payloads/TokenOffsetPayloadTokenFilter.java	(revision 686801)
+++ contrib/analyzers/src/java/org/apache/lucene/analysis/payloads/TokenOffsetPayloadTokenFilter.java	(working copy)
@@ -38,15 +38,16 @@
     super(input);
   }
 
-  public Token next(Token result) throws IOException {
-    result = input.next(result);
-    if (result != null){
+  public Token next(final Token reusableToken) throws IOException {
+    assert reusableToken != null;
+    Token nextToken = input.next(reusableToken);
+    if (nextToken != null){
       byte[] data = new byte[8];
-      PayloadHelper.encodeInt(result.startOffset(), data, 0);
-      PayloadHelper.encodeInt(result.endOffset(), data, 4);
+      PayloadHelper.encodeInt(nextToken.startOffset(), data, 0);
+      PayloadHelper.encodeInt(nextToken.endOffset(), data, 4);
       Payload payload = new Payload(data);
-      result.setPayload(payload);
+      nextToken.setPayload(payload);
     }
-    return result;
+    return nextToken;
   }
 }
\ No newline at end of file
Index: contrib/queries/src/java/org/apache/lucene/search/similar/MoreLikeThis.java
===================================================================
--- contrib/queries/src/java/org/apache/lucene/search/similar/MoreLikeThis.java	(revision 686801)
+++ contrib/queries/src/java/org/apache/lucene/search/similar/MoreLikeThis.java	(working copy)
@@ -28,6 +28,7 @@
 import org.apache.lucene.search.Query;
 import org.apache.lucene.search.Hits;
 import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.analysis.Token;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.standard.StandardAnalyzer;
 import org.apache.lucene.document.Document;
@@ -808,10 +809,11 @@
 		throws IOException
 	{
 		   TokenStream ts = analyzer.tokenStream(fieldName, r);
-			org.apache.lucene.analysis.Token token;
 			int tokenCount=0;
-			while ((token = ts.next()) != null) { // for every token
-				String word = token.termText();
+			// for every token
+                        final Token reusableToken = new Token();
+			for (Token nextToken = ts.next(reusableToken); nextToken != null; nextToken = ts.next(reusableToken)) {
+				String word = nextToken.term();
 				tokenCount++;
 				if(tokenCount>maxNumTokensParsed)
 				{
@@ -872,7 +874,7 @@
 	 * For an easier method to call see {@link #retrieveInterestingTerms retrieveInterestingTerms()}.
      *
      * @param r the reader that has the content of the document
-	 * @return the most intresting words in the document ordered by score, with the highest scoring, or best entry, first
+	 * @return the most interesting words in the document ordered by score, with the highest scoring, or best entry, first
 	 *
 	 * @see #retrieveInterestingTerms
      */
Index: contrib/queries/src/java/org/apache/lucene/search/similar/SimilarityQueries.java
===================================================================
--- contrib/queries/src/java/org/apache/lucene/search/similar/SimilarityQueries.java	(revision 686801)
+++ contrib/queries/src/java/org/apache/lucene/search/similar/SimilarityQueries.java	(working copy)
@@ -21,6 +21,7 @@
 import java.util.Set;
 
 import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.analysis.Token;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.search.BooleanClause;
@@ -85,12 +86,11 @@
 										  throws IOException
 	{	
 		TokenStream ts = a.tokenStream( field, new StringReader( body));
-		org.apache.lucene.analysis.Token t;
 		BooleanQuery tmp = new BooleanQuery();
 		Set already = new HashSet(); // ignore dups
-		while ( (t = ts.next()) != null)
-		{
-			String word = t.termText();
+                final Token reusableToken = new Token();
+		for (Token nextToken = ts.next(reusableToken); nextToken != null; nextToken = ts.next(reusableToken)) {
+			String word = nextToken.term();
 			// ignore opt stop words
 			if ( stop != null &&
 				 stop.contains( word)) continue;
Index: contrib/queries/src/java/org/apache/lucene/search/FuzzyLikeThisQuery.java
===================================================================
--- contrib/queries/src/java/org/apache/lucene/search/FuzzyLikeThisQuery.java	(revision 686801)
+++ contrib/queries/src/java/org/apache/lucene/search/FuzzyLikeThisQuery.java	(working copy)
@@ -104,18 +104,19 @@
     {
         if(f.queryString==null) return;
         TokenStream ts=analyzer.tokenStream(f.fieldName,new StringReader(f.queryString));
-        Token token=ts.next();
+        final Token reusableToken = new Token();
         int corpusNumDocs=reader.numDocs();
         Term internSavingTemplateTerm =new Term(f.fieldName,""); //optimization to avoid constructing new Term() objects
         HashSet processedTerms=new HashSet();
-        while(token!=null)
-        {            
-        	if(!processedTerms.contains(token.termText()))
+        for (Token nextToken = ts.next(reusableToken); nextToken!=null; nextToken = ts.next(reusableToken))
+        {
+                String term = nextToken.term();
+        	if(!processedTerms.contains(term))
         	{
-        		processedTerms.add(token.termText());
+        		processedTerms.add(term);
                 ScoreTermQueue variantsQ=new ScoreTermQueue(MAX_VARIANTS_PER_TERM); //maxNum variants considered for any one term
                 float minScore=0;
-                Term startTerm=internSavingTemplateTerm.createTerm(token.termText());
+                Term startTerm=internSavingTemplateTerm.createTerm(term);
                 FuzzyTermEnum fe=new FuzzyTermEnum(reader,startTerm,f.minSimilarity,f.prefixLength);
                 TermEnum origEnum = reader.terms(startTerm);
                 int df=0;
@@ -162,8 +163,7 @@
                   q.insert(st);
                 }                            
         	}
-            token=ts.next();
-        }        
+        }     
     }
             
     public Query rewrite(IndexReader reader) throws IOException
