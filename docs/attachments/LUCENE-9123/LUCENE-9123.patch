diff --git a/lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/JapaneseAnalyzer.java b/lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/JapaneseAnalyzer.java
index 82c71dced6e..e2f5959d2bf 100644
--- a/lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/JapaneseAnalyzer.java
+++ b/lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/JapaneseAnalyzer.java
@@ -87,7 +87,7 @@ public class JapaneseAnalyzer extends StopwordAnalyzerBase {
   
   @Override
   protected TokenStreamComponents createComponents(String fieldName) {
-    Tokenizer tokenizer = new JapaneseTokenizer(userDict, true, mode);
+    Tokenizer tokenizer = new JapaneseTokenizer(userDict, true, true, mode);
     TokenStream stream = new JapaneseBaseFormFilter(tokenizer);
     stream = new JapanesePartOfSpeechStopFilter(stream, stoptags);
     stream = new CJKWidthFilter(stream);
diff --git a/lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/JapaneseTokenizer.java b/lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/JapaneseTokenizer.java
index b2b279be88f..19c17552335 100644
--- a/lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/JapaneseTokenizer.java
+++ b/lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/JapaneseTokenizer.java
@@ -198,7 +198,21 @@ public final class JapaneseTokenizer extends Tokenizer {
    * @param mode tokenization mode.
    */
   public JapaneseTokenizer(UserDictionary userDictionary, boolean discardPunctuation, Mode mode) {
-    this(DEFAULT_TOKEN_ATTRIBUTE_FACTORY, userDictionary, discardPunctuation, mode);
+    this(DEFAULT_TOKEN_ATTRIBUTE_FACTORY, userDictionary, discardPunctuation, true, mode);
+  }
+
+  /**
+   * Create a new JapaneseTokenizer.
+   * <p>
+   * Uses the default AttributeFactory.
+   *
+   * @param userDictionary Optional: if non-null, user dictionary.
+   * @param discardPunctuation true if punctuation tokens should be dropped from the output.
+   * @param discardCompoundToken true if compound tokens should be dropped from the output when tokenization mode is not NORMAL.
+   * @param mode tokenization mode.
+   */
+  public JapaneseTokenizer(UserDictionary userDictionary, boolean discardPunctuation, boolean discardCompoundToken, Mode mode) {
+    this(DEFAULT_TOKEN_ATTRIBUTE_FACTORY, userDictionary, discardPunctuation, discardCompoundToken, mode);
   }
 
   /**
@@ -215,7 +229,25 @@ public final class JapaneseTokenizer extends Tokenizer {
          TokenInfoDictionary.getInstance(),
          UnknownDictionary.getInstance(),
          ConnectionCosts.getInstance(),
-         userDictionary, discardPunctuation, mode);
+         userDictionary, discardPunctuation, true, mode);
+  }
+
+  /**
+   * Create a new JapaneseTokenizer using the system and unknown dictionaries shipped with Lucene.
+   *
+   * @param factory the AttributeFactory to use
+   * @param userDictionary Optional: if non-null, user dictionary.
+   * @param discardPunctuation true if punctuation tokens should be dropped from the output.
+   * @param discardCompoundToken true if compound tokens should be dropped from the output when tokenization mode is not NORMAL.
+   * @param mode tokenization mode.
+   */
+  public JapaneseTokenizer
+  (AttributeFactory factory, UserDictionary userDictionary, boolean discardPunctuation, boolean discardCompoundToken, Mode mode) {
+    this(factory,
+        TokenInfoDictionary.getInstance(),
+        UnknownDictionary.getInstance(),
+        ConnectionCosts.getInstance(),
+        userDictionary, discardPunctuation, discardCompoundToken, mode);
   }
 
   /**
@@ -229,6 +261,7 @@ public final class JapaneseTokenizer extends Tokenizer {
    * @param connectionCosts custom token transition costs
    * @param userDictionary Optional: if non-null, user dictionary.
    * @param discardPunctuation true if punctuation tokens should be dropped from the output.
+   * @param discardCompoundToken true if compound tokens should be dropped from the output when tokenization mode is not NORMAL.
    * @param mode tokenization mode.
    * @lucene.experimental
    */
@@ -238,6 +271,7 @@ public final class JapaneseTokenizer extends Tokenizer {
                            ConnectionCosts connectionCosts,
                            UserDictionary userDictionary,
                            boolean discardPunctuation,
+                           boolean discardCompoundToken,
                            Mode mode) {
     super(factory);
     this.dictionary = systemDictionary;
@@ -259,12 +293,12 @@ public final class JapaneseTokenizer extends Tokenizer {
       case SEARCH:
         searchMode = true;
         extendedMode = false;
-        outputCompounds = true;
+        outputCompounds = !discardCompoundToken;
         break;
       case EXTENDED:
         searchMode = true;
         extendedMode = true;
-        outputCompounds = false;
+        outputCompounds = !discardCompoundToken;
         break;
       default:
         searchMode = false;
diff --git a/lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/JapaneseTokenizerFactory.java b/lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/JapaneseTokenizerFactory.java
index 1eaecd7c5b7..9316d063bd4 100644
--- a/lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/JapaneseTokenizerFactory.java
+++ b/lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/JapaneseTokenizerFactory.java
@@ -45,6 +45,7 @@ import org.apache.lucene.analysis.util.ResourceLoaderAware;
  *       userDictionary="user.txt"
  *       userDictionaryEncoding="UTF-8"
  *       discardPunctuation="true"
+ *       discardCompoundToken="false"
  *     /&gt;
  *     &lt;filter class="solr.JapaneseBaseFormFilterFactory"/&gt;
  *   &lt;/analyzer&gt;
@@ -93,6 +94,8 @@ public class JapaneseTokenizerFactory extends TokenizerFactory implements Resour
 
   private static final String DISCARD_PUNCTUATION = "discardPunctuation"; // Expert option
 
+  private static final String DISCARD_COMPOUND_TOKEN = "discardCompoundToken"; // Expert option
+
   private static final String NBEST_COST = "nBestCost";
 
   private static final String NBEST_EXAMPLES = "nBestExamples";
@@ -101,6 +104,7 @@ public class JapaneseTokenizerFactory extends TokenizerFactory implements Resour
 
   private final Mode mode;
   private final boolean discardPunctuation;
+  private final boolean discardCompoundToken;
   private final String userDictionaryPath;
   private final String userDictionaryEncoding;
 
@@ -124,6 +128,7 @@ public class JapaneseTokenizerFactory extends TokenizerFactory implements Resour
     userDictionaryPath = args.remove(USER_DICT_PATH);
     userDictionaryEncoding = args.remove(USER_DICT_ENCODING);
     discardPunctuation = getBoolean(args, DISCARD_PUNCTUATION, true);
+    discardCompoundToken = getBoolean(args, DISCARD_COMPOUND_TOKEN, true);
     nbestCost = getInt(args, NBEST_COST, 0);
     nbestExamples = args.remove(NBEST_EXAMPLES);
     if (!args.isEmpty()) {
@@ -152,7 +157,7 @@ public class JapaneseTokenizerFactory extends TokenizerFactory implements Resour
 
   @Override
   public JapaneseTokenizer create(AttributeFactory factory) {
-    JapaneseTokenizer t = new JapaneseTokenizer(factory, userDictionary, discardPunctuation, mode);
+    JapaneseTokenizer t = new JapaneseTokenizer(factory, userDictionary, discardPunctuation, discardCompoundToken, mode);
     if (nbestExamples != null) {
       nbestCost = Math.max(nbestCost, t.calcNBestCost(nbestExamples));
     }
diff --git a/lucene/analysis/kuromoji/src/test/org/apache/lucene/analysis/ja/TestJapaneseAnalyzer.java b/lucene/analysis/kuromoji/src/test/org/apache/lucene/analysis/ja/TestJapaneseAnalyzer.java
index 7793528f377..2a92fea0db7 100644
--- a/lucene/analysis/kuromoji/src/test/org/apache/lucene/analysis/ja/TestJapaneseAnalyzer.java
+++ b/lucene/analysis/kuromoji/src/test/org/apache/lucene/analysis/ja/TestJapaneseAnalyzer.java
@@ -62,59 +62,54 @@ public class TestJapaneseAnalyzer extends BaseTokenStreamTestCase {
     // Senior software engineer:
     assertAnalyzesToPositions(a, "シニアソフトウェアエンジニア",
                               new String[] { "シニア",
-                                             "シニアソフトウェアエンジニア", // zero pos inc
                                              "ソフトウェア",
                                              "エンジニア" },
-                              new int[] { 1, 0, 1, 1},
-                              new int[] { 1, 3, 1, 1}
+                              new int[] { 1, 1, 1},
+                              new int[] { 1, 1, 1}
                               );
 
     // Senior project manager: also tests katakana spelling variation stemming
     assertAnalyzesToPositions(a, "シニアプロジェクトマネージャー",
                               new String[] { "シニア",
-                                              "シニアプロジェクトマネージャ", // trailing ー removed by stemming, zero pos inc
                                               "プロジェクト",
                                               "マネージャ"}, // trailing ー removed by stemming
-                              new int[]{1, 0, 1, 1},
-                              new int[]{1, 3, 1, 1}
+                              new int[]{1, 1, 1},
+                              new int[]{1, 1, 1}
                               );
 
     // Kansai International Airport:
     assertAnalyzesToPositions(a, "関西国際空港",
                               new String[] { "関西",
-                                             "関西国際空港", // zero pos inc
                                              "国際",
                                              "空港" },
-                              new int[] {1, 0, 1, 1},
-                              new int[] {1, 3, 1, 1}
+                              new int[] {1, 1, 1},
+                              new int[] {1, 1, 1}
                               );
 
     // Konika Minolta Holdings; not quite the right
     // segmentation (see LUCENE-3726):
     assertAnalyzesToPositions(a, "コニカミノルタホールディングス",
                               new String[] { "コニカ",
-                                             "コニカミノルタホールディングス", // zero pos inc
-                                             "ミノルタ", 
+                                             "ミノルタ",
                                              "ホールディングス"},
-                              new int[] {1, 0, 1, 1},
-                              new int[] {1, 3, 1, 1}
+                              new int[] {1, 1, 1},
+                              new int[] {1, 1, 1}
                               );
 
     // Narita Airport
     assertAnalyzesToPositions(a, "成田空港",
                               new String[] { "成田",
-                                             "成田空港",
                                              "空港" },
-                              new int[] {1, 0, 1},
-                              new int[] {1, 2, 1}
+                              new int[] {1, 1},
+                              new int[] {1, 1}
                               );
 
     // Kyoto University Baseball Club
     a.close();
     a = new JapaneseAnalyzer();
     assertAnalyzesToPositions(a, "京都大学硬式野球部",
-                     new String[] { "京都大",
-                                    "学",
+                     new String[] { "京都",
+                                    "大学",
                                     "硬式",
                                     "野球",
                                     "部" },
diff --git a/lucene/analysis/kuromoji/src/test/org/apache/lucene/analysis/ja/TestJapaneseNumberFilter.java b/lucene/analysis/kuromoji/src/test/org/apache/lucene/analysis/ja/TestJapaneseNumberFilter.java
index 4d58e368a43..f7df161265c 100644
--- a/lucene/analysis/kuromoji/src/test/org/apache/lucene/analysis/ja/TestJapaneseNumberFilter.java
+++ b/lucene/analysis/kuromoji/src/test/org/apache/lucene/analysis/ja/TestJapaneseNumberFilter.java
@@ -44,7 +44,7 @@ public class TestJapaneseNumberFilter extends BaseTokenStreamTestCase {
     analyzer = new Analyzer() {
       @Override
       protected TokenStreamComponents createComponents(String fieldName) {
-        Tokenizer tokenizer = new JapaneseTokenizer(newAttributeFactory(), null, false, JapaneseTokenizer.Mode.SEARCH);
+        Tokenizer tokenizer = new JapaneseTokenizer(newAttributeFactory(), null, false, false, JapaneseTokenizer.Mode.SEARCH);
         return new TokenStreamComponents(tokenizer, new JapaneseNumberFilter(tokenizer));
       }
     };
diff --git a/lucene/analysis/kuromoji/src/test/org/apache/lucene/analysis/ja/TestJapaneseTokenizer.java b/lucene/analysis/kuromoji/src/test/org/apache/lucene/analysis/ja/TestJapaneseTokenizer.java
index 257ed4b3de3..0ac33eb6b98 100644
--- a/lucene/analysis/kuromoji/src/test/org/apache/lucene/analysis/ja/TestJapaneseTokenizer.java
+++ b/lucene/analysis/kuromoji/src/test/org/apache/lucene/analysis/ja/TestJapaneseTokenizer.java
@@ -64,12 +64,16 @@ public class
     }
   }
 
-  private Analyzer analyzer, analyzerNormal, analyzerNormalNBest, analyzerNoPunct, extendedModeAnalyzerNoPunct;
+  private Analyzer analyzer, analyzerNormal, analyzerNormalNBest, analyzerNoPunct, extendedModeAnalyzerNoPunct, analyzerNoCompound, extendedModeAnalyzerNoCompound;
 
   private JapaneseTokenizer makeTokenizer(boolean discardPunctuation, Mode mode) {
     return new JapaneseTokenizer(newAttributeFactory(), readDict(), discardPunctuation, mode);
   }
 
+  private JapaneseTokenizer makeTokenizer(boolean discardPunctuation, boolean discardCompoundToken, Mode mode) {
+    return new JapaneseTokenizer(newAttributeFactory(), readDict(), discardPunctuation, discardCompoundToken, mode);
+  }
+
   private Analyzer makeAnalyzer(final Tokenizer t) {
     return new Analyzer() {
       @Override
@@ -85,21 +89,21 @@ public class
     analyzer = new Analyzer() {
       @Override
       protected TokenStreamComponents createComponents(String fieldName) {
-        Tokenizer tokenizer = new JapaneseTokenizer(newAttributeFactory(), readDict(), false, Mode.SEARCH);
+        Tokenizer tokenizer = new JapaneseTokenizer(newAttributeFactory(), readDict(), false, false, Mode.SEARCH);
         return new TokenStreamComponents(tokenizer, tokenizer);
       }
     };
     analyzerNormal = new Analyzer() {
       @Override
       protected TokenStreamComponents createComponents(String fieldName) {
-        Tokenizer tokenizer = new JapaneseTokenizer(newAttributeFactory(), readDict(), false, Mode.NORMAL);
+        Tokenizer tokenizer = new JapaneseTokenizer(newAttributeFactory(), readDict(), false, false, Mode.NORMAL);
         return new TokenStreamComponents(tokenizer, tokenizer);
       }
     };
     analyzerNormalNBest = new Analyzer() {
       @Override
       protected TokenStreamComponents createComponents(String fieldName) {
-        JapaneseTokenizer tokenizer = new JapaneseTokenizer(newAttributeFactory(), readDict(), false, Mode.NORMAL);
+        JapaneseTokenizer tokenizer = new JapaneseTokenizer(newAttributeFactory(), readDict(), false, false, Mode.NORMAL);
         tokenizer.setNBestCost(2000);
         return new TokenStreamComponents(tokenizer, tokenizer);
       }
@@ -107,14 +111,28 @@ public class
     analyzerNoPunct = new Analyzer() {
       @Override
       protected TokenStreamComponents createComponents(String fieldName) {
-        Tokenizer tokenizer = new JapaneseTokenizer(newAttributeFactory(), readDict(), true, Mode.SEARCH);
+        Tokenizer tokenizer = new JapaneseTokenizer(newAttributeFactory(), readDict(), true, false, Mode.SEARCH);
         return new TokenStreamComponents(tokenizer, tokenizer);
       }
     };
     extendedModeAnalyzerNoPunct = new Analyzer() {
       @Override
       protected TokenStreamComponents createComponents(String fieldName) {
-        Tokenizer tokenizer = new JapaneseTokenizer(newAttributeFactory(), readDict(), true, Mode.EXTENDED);
+        Tokenizer tokenizer = new JapaneseTokenizer(newAttributeFactory(), readDict(), true, false, Mode.EXTENDED);
+        return new TokenStreamComponents(tokenizer, tokenizer);
+      }
+    };
+    analyzerNoCompound = new Analyzer() {
+      @Override
+      protected TokenStreamComponents createComponents(String fieldName) {
+        Tokenizer tokenizer = new JapaneseTokenizer(newAttributeFactory(), readDict(), false, true, Mode.SEARCH);
+        return new TokenStreamComponents(tokenizer, tokenizer);
+      }
+    };
+    extendedModeAnalyzerNoCompound = new Analyzer() {
+      @Override
+      protected TokenStreamComponents createComponents(String fieldName) {
+        Tokenizer tokenizer = new JapaneseTokenizer(newAttributeFactory(), readDict(), false, true, Mode.EXTENDED);
         return new TokenStreamComponents(tokenizer, tokenizer);
       }
     };
@@ -122,7 +140,7 @@ public class
 
   @Override
   public void tearDown() throws Exception {
-    IOUtils.close(analyzer, analyzerNormal, analyzerNoPunct, extendedModeAnalyzerNoPunct);
+    IOUtils.close(analyzer, analyzerNormal, analyzerNoPunct, extendedModeAnalyzerNoPunct, analyzerNoCompound, extendedModeAnalyzerNoCompound);
     super.tearDown();
   }
 
@@ -184,7 +202,7 @@ public class
     t.setNBestCost(0);
     assertAnalyzesTo(a,
                      "成田空港、米原油流出",
-                     new String[] {"成田", "成田空港", "空港", "米", "原油", "流出"});
+                     new String[] {"成田", "空港", "米", "原油", "流出"});
 
     t.setNBestCost(4000);
     assertAnalyzesTo(a,
@@ -450,7 +468,7 @@ public class
         new TokenInfoDictionary(ResourceScheme.CLASSPATH, "org/apache/lucene/analysis/ja/dict/TokenInfoDictionary"),
         new UnknownDictionary(ResourceScheme.CLASSPATH, "org/apache/lucene/analysis/ja/dict/UnknownDictionary"),
         new ConnectionCosts(ResourceScheme.CLASSPATH, "org/apache/lucene/analysis/ja/dict/ConnectionCosts"),
-        readDict(), true, Mode.SEARCH);
+        readDict(), true, false, Mode.SEARCH);
     try (Analyzer a = makeAnalyzer(tokenizer)) {
       assertTokenStreamContents(a.tokenStream("foo", "abcd"),
                                 new String[] { "a", "b", "cd"  },
@@ -868,4 +886,21 @@ public class
         new int[]{0, 2},
         new int[]{2, 4});
   }
+
+  public void testNoCompoundToken() throws Exception {
+    assertAnalyzesTo(analyzerNormal, "株式会社とアカデミア",
+        new String[]{"株式会社", "と", "アカデミア"});
+
+    assertAnalyzesTo(analyzer, "株式会社とアカデミア",
+        new String[]{"株式", "株式会社", "会社", "と", "アカデミア"});
+
+    assertAnalyzesTo(analyzerNoCompound, "株式会社とアカデミア",
+        new String[]{"株式", "会社", "と", "アカデミア"});
+
+    assertAnalyzesTo(extendedModeAnalyzerNoPunct, "株式会社とアカデミア",
+        new String[]{"株式", "株式会社", "会社", "と", "ア", "カ", "デ", "ミ", "ア"});
+
+    assertAnalyzesTo(extendedModeAnalyzerNoCompound, "株式会社とアカデミア",
+        new String[]{"株式", "会社", "と", "ア", "カ", "デ", "ミ", "ア"});
+  }
 }
diff --git a/lucene/analysis/kuromoji/src/test/org/apache/lucene/analysis/ja/TestJapaneseTokenizerFactory.java b/lucene/analysis/kuromoji/src/test/org/apache/lucene/analysis/ja/TestJapaneseTokenizerFactory.java
index bdf22cf9e48..43727d5b692 100644
--- a/lucene/analysis/kuromoji/src/test/org/apache/lucene/analysis/ja/TestJapaneseTokenizerFactory.java
+++ b/lucene/analysis/kuromoji/src/test/org/apache/lucene/analysis/ja/TestJapaneseTokenizerFactory.java
@@ -51,7 +51,7 @@ public class TestJapaneseTokenizerFactory extends BaseTokenStreamTestCase {
     TokenStream ts = factory.create(newAttributeFactory());
     ((Tokenizer)ts).setReader(new StringReader("シニアソフトウェアエンジニア"));
     assertTokenStreamContents(ts,
-                              new String[] { "シニア", "シニアソフトウェアエンジニア", "ソフトウェア", "エンジニア" }
+                              new String[] { "シニア", "ソフトウェア", "エンジニア" }
     );
   }
 
@@ -109,6 +109,21 @@ public class TestJapaneseTokenizerFactory extends BaseTokenStreamTestCase {
     );
   }
 
+  /**
+   * Test discarding compound (original) token
+   */
+  public void testPreserveCompoundToken() throws IOException {
+    Map<String,String> args = new HashMap<>();
+    args.put("discardCompoundToken", "false");
+    JapaneseTokenizerFactory factory = new JapaneseTokenizerFactory(args);
+    factory.inform(new StringMockResourceLoader(""));
+    TokenStream ts = factory.create(newAttributeFactory());
+    ((Tokenizer)ts).setReader(new StringReader("シニアソフトウェアエンジニア"));
+    assertTokenStreamContents(ts,
+        new String[] { "シニア", "シニアソフトウェアエンジニア", "ソフトウェア", "エンジニア" }
+    );
+  }
+
   /** Test that bogus arguments result in exception */
   public void testBogusArguments() throws Exception {
     IllegalArgumentException expected = expectThrows(IllegalArgumentException.class, () -> {
diff --git a/lucene/analysis/kuromoji/src/test/org/apache/lucene/analysis/ja/TestSearchMode.java b/lucene/analysis/kuromoji/src/test/org/apache/lucene/analysis/ja/TestSearchMode.java
index ee47f9143eb..2367d0c6fc4 100644
--- a/lucene/analysis/kuromoji/src/test/org/apache/lucene/analysis/ja/TestSearchMode.java
+++ b/lucene/analysis/kuromoji/src/test/org/apache/lucene/analysis/ja/TestSearchMode.java
@@ -23,6 +23,8 @@ import java.io.InputStream;
 import java.io.InputStreamReader;
 import java.io.LineNumberReader;
 import java.nio.charset.StandardCharsets;
+import java.util.ArrayList;
+import java.util.List;
 
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
@@ -31,7 +33,7 @@ import org.apache.lucene.analysis.ja.JapaneseTokenizer.Mode;
 
 public class TestSearchMode extends BaseTokenStreamTestCase {
   private final static String SEGMENTATION_FILENAME = "search-segmentation-tests.txt";
-  private Analyzer analyzer;
+  private Analyzer analyzer, analyzerNoOriginal;
   
   @Override
   public void setUp() throws Exception {
@@ -39,7 +41,14 @@ public class TestSearchMode extends BaseTokenStreamTestCase {
     analyzer = new Analyzer() {
       @Override
       protected TokenStreamComponents createComponents(String fieldName) {
-        Tokenizer tokenizer = new JapaneseTokenizer(newAttributeFactory(), null, true, Mode.SEARCH);
+        Tokenizer tokenizer = new JapaneseTokenizer(newAttributeFactory(), null, true, false, Mode.SEARCH);
+        return new TokenStreamComponents(tokenizer, tokenizer);
+      }
+    };
+    analyzerNoOriginal = new Analyzer() {
+      @Override
+      protected TokenStreamComponents createComponents(String fieldName) {
+        Tokenizer tokenizer = new JapaneseTokenizer(newAttributeFactory(), null, true, true, Mode.SEARCH);
         return new TokenStreamComponents(tokenizer, tokenizer);
       }
     };
@@ -48,6 +57,7 @@ public class TestSearchMode extends BaseTokenStreamTestCase {
   @Override
   public void tearDown() throws Exception {
     analyzer.close();
+    analyzerNoOriginal.close();
     super.tearDown();
   }
 
@@ -90,4 +100,51 @@ public class TestSearchMode extends BaseTokenStreamTestCase {
       is.close();
     }
   }
+
+  public void testSearchSegmentationNoOriginal() throws IOException {
+    InputStream is = TestSearchMode.class.getResourceAsStream(SEGMENTATION_FILENAME);
+    if (is == null) {
+      throw new FileNotFoundException("Cannot find " + SEGMENTATION_FILENAME + " in test classpath");
+    }
+    try {
+      LineNumberReader reader = new LineNumberReader(new InputStreamReader(is, StandardCharsets.UTF_8));
+      String line = null;
+      while ((line = reader.readLine()) != null) {
+        // Remove comments
+        line = line.replaceAll("#.*$", "");
+        // Skip empty lines or comment lines
+        if (line.trim().isEmpty()) {
+          continue;
+        }
+        if (VERBOSE) {
+          System.out.println("Line no. " + reader.getLineNumber() + ": " + line);
+        }
+        String[] fields = line.split("\t", 2);
+        String sourceText = fields[0];
+        String[] tmpExpectedTokens = fields[1].split("\\s+");
+        if (sourceText.equals("京都大学硬式野球部")) {
+          // This is the only case that tokenization result is different from discardCompoundToken=false
+          tmpExpectedTokens[0] = "京都";
+          tmpExpectedTokens[1] = "大学";
+        }
+
+        List<String> expectedTokenList = new ArrayList<>();
+        for(int tokIDX=0;tokIDX<tmpExpectedTokens.length;tokIDX++) {
+          if (!tmpExpectedTokens[tokIDX].endsWith("/0")) {
+            expectedTokenList.add(tmpExpectedTokens[tokIDX]);
+          }
+        }
+
+        int[] expectedPosIncrs = new int[expectedTokenList.size()];
+        int[] expectedPosLengths = new int[expectedTokenList.size()];
+        for(int tokIDX=0;tokIDX<expectedTokenList.size();tokIDX++) {
+          expectedPosIncrs[tokIDX] = 1;
+          expectedPosLengths[tokIDX] = 1;
+        }
+        assertAnalyzesTo(analyzerNoOriginal, sourceText, expectedTokenList.toArray(new String[expectedTokenList.size()]), expectedPosIncrs);
+      }
+    } finally {
+      is.close();
+    }
+  }
 }
