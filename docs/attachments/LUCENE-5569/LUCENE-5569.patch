diff --git lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/TestReuseDocsEnum.java lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/TestReuseDocsEnum.java
index 1bbdd8f..33f36fc 100644
--- lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/TestReuseDocsEnum.java
+++ lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/TestReuseDocsEnum.java
@@ -22,8 +22,8 @@ import java.util.Random;
 
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.codecs.Codec;
-import org.apache.lucene.index.AtomicReader;
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReader;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.DocsEnum;
 import org.apache.lucene.index.IndexWriter;
@@ -38,7 +38,6 @@ import org.apache.lucene.util.IOUtils;
 import org.apache.lucene.util.LineFileDocs;
 import org.apache.lucene.util.LuceneTestCase;
 import org.apache.lucene.util.TestUtil;
-import org.junit.BeforeClass;
 
 // TODO: really this should be in BaseTestPF or somewhere else? useful test!
 public class TestReuseDocsEnum extends LuceneTestCase {
@@ -53,8 +52,8 @@ public class TestReuseDocsEnum extends LuceneTestCase {
     writer.commit();
 
     DirectoryReader open = DirectoryReader.open(dir);
-    for (AtomicReaderContext ctx : open.leaves()) {
-      AtomicReader indexReader = ctx.reader();
+    for (LeafReaderContext ctx : open.leaves()) {
+      LeafReader indexReader = ctx.reader();
       Terms terms = indexReader.terms("body");
       TermsEnum iterator = terms.iterator(null);
       IdentityHashMap<DocsEnum, Boolean> enums = new IdentityHashMap<>();
@@ -81,7 +80,7 @@ public class TestReuseDocsEnum extends LuceneTestCase {
     writer.commit();
 
     DirectoryReader open = DirectoryReader.open(dir);
-    for (AtomicReaderContext ctx : open.leaves()) {
+    for (LeafReaderContext ctx : open.leaves()) {
       Terms terms = ctx.reader().terms("body");
       TermsEnum iterator = terms.iterator(null);
       IdentityHashMap<DocsEnum, Boolean> enums = new IdentityHashMap<>();
@@ -130,10 +129,10 @@ public class TestReuseDocsEnum extends LuceneTestCase {
 
     DirectoryReader firstReader = DirectoryReader.open(dir);
     DirectoryReader secondReader = DirectoryReader.open(dir);
-    List<AtomicReaderContext> leaves = firstReader.leaves();
-    List<AtomicReaderContext> leaves2 = secondReader.leaves();
+    List<LeafReaderContext> leaves = firstReader.leaves();
+    List<LeafReaderContext> leaves2 = secondReader.leaves();
     
-    for (AtomicReaderContext ctx : leaves) {
+    for (LeafReaderContext ctx : leaves) {
       Terms terms = ctx.reader().terms("body");
       TermsEnum iterator = terms.iterator(null);
       IdentityHashMap<DocsEnum, Boolean> enums = new IdentityHashMap<>();
@@ -160,11 +159,11 @@ public class TestReuseDocsEnum extends LuceneTestCase {
     IOUtils.close(firstReader, secondReader, dir);
   }
   
-  public DocsEnum randomDocsEnum(String field, BytesRef term, List<AtomicReaderContext> readers, Bits bits) throws IOException {
+  public DocsEnum randomDocsEnum(String field, BytesRef term, List<LeafReaderContext> readers, Bits bits) throws IOException {
     if (random().nextInt(10) == 0) {
       return null;
     }
-    AtomicReader indexReader = readers.get(random().nextInt(readers.size())).reader();
+    LeafReader indexReader = readers.get(random().nextInt(readers.size())).reader();
     Terms terms = indexReader.terms(field);
     if (terms == null) {
       return null;
diff --git lucene/backward-codecs/src/test/org/apache/lucene/index/TestBackwardsCompatibility.java lucene/backward-codecs/src/test/org/apache/lucene/index/TestBackwardsCompatibility.java
index cb8fc19..4b749dd 100644
--- lucene/backward-codecs/src/test/org/apache/lucene/index/TestBackwardsCompatibility.java
+++ lucene/backward-codecs/src/test/org/apache/lucene/index/TestBackwardsCompatibility.java
@@ -992,7 +992,7 @@ public class TestBackwardsCompatibility extends LuceneTestCase {
     for (String name : oldNames) {
       Directory dir = oldIndexDirs.get(name);
       DirectoryReader r = DirectoryReader.open(dir);
-      for (AtomicReaderContext context : r.leaves()) {
+      for (LeafReaderContext context : r.leaves()) {
         air = (SegmentReader) context.reader();
         Version oldVersion = air.getSegmentInfo().info.getVersion();
         assertNotNull(oldVersion); // only 3.0 segments can have a null version
@@ -1210,7 +1210,7 @@ public class TestBackwardsCompatibility extends LuceneTestCase {
 
   public static final String dvUpdatesIndex = "dvupdates.48.zip";
 
-  private void assertNumericDocValues(AtomicReader r, String f, String cf) throws IOException {
+  private void assertNumericDocValues(LeafReader r, String f, String cf) throws IOException {
     NumericDocValues ndvf = r.getNumericDocValues(f);
     NumericDocValues ndvcf = r.getNumericDocValues(cf);
     for (int i = 0; i < r.maxDoc(); i++) {
@@ -1218,7 +1218,7 @@ public class TestBackwardsCompatibility extends LuceneTestCase {
     }
   }
   
-  private void assertBinaryDocValues(AtomicReader r, String f, String cf) throws IOException {
+  private void assertBinaryDocValues(LeafReader r, String f, String cf) throws IOException {
     BinaryDocValues bdvf = r.getBinaryDocValues(f);
     BinaryDocValues bdvcf = r.getBinaryDocValues(cf);
     for (int i = 0; i < r.maxDoc(); i++) {
@@ -1228,8 +1228,8 @@ public class TestBackwardsCompatibility extends LuceneTestCase {
   
   private void verifyDocValues(Directory dir) throws IOException {
     DirectoryReader reader = DirectoryReader.open(dir);
-    for (AtomicReaderContext context : reader.leaves()) {
-      AtomicReader r = context.reader();
+    for (LeafReaderContext context : reader.leaves()) {
+      LeafReader r = context.reader();
       assertNumericDocValues(r, "ndv1", "ndv1_c");
       assertNumericDocValues(r, "ndv2", "ndv2_c");
       assertBinaryDocValues(r, "bdv1", "bdv1_c");
diff --git lucene/classification/src/java/org/apache/lucene/classification/BooleanPerceptronClassifier.java lucene/classification/src/java/org/apache/lucene/classification/BooleanPerceptronClassifier.java
index e7e7a1c..497cc88 100644
--- lucene/classification/src/java/org/apache/lucene/classification/BooleanPerceptronClassifier.java
+++ lucene/classification/src/java/org/apache/lucene/classification/BooleanPerceptronClassifier.java
@@ -19,7 +19,7 @@ package org.apache.lucene.classification;
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
-import org.apache.lucene.index.AtomicReader;
+import org.apache.lucene.index.LeafReader;
 import org.apache.lucene.index.MultiFields;
 import org.apache.lucene.index.StorableField;
 import org.apache.lucene.index.StoredDocument;
@@ -34,7 +34,6 @@ import org.apache.lucene.search.ScoreDoc;
 import org.apache.lucene.search.WildcardQuery;
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.BytesRefBuilder;
-import org.apache.lucene.util.IntsRef;
 import org.apache.lucene.util.IntsRefBuilder;
 import org.apache.lucene.util.fst.Builder;
 import org.apache.lucene.util.fst.FST;
@@ -80,7 +79,7 @@ public class BooleanPerceptronClassifier implements Classifier<Boolean> {
   /**
    * Default constructor, no batch updates of FST, perceptron threshold is
    * calculated via underlying index metrics during
-   * {@link #train(org.apache.lucene.index.AtomicReader, String, String, org.apache.lucene.analysis.Analyzer)
+   * {@link #train(org.apache.lucene.index.LeafReader, String, String, org.apache.lucene.analysis.Analyzer)
    * training}
    */
   public BooleanPerceptronClassifier() {
@@ -118,18 +117,18 @@ public class BooleanPerceptronClassifier implements Classifier<Boolean> {
    * {@inheritDoc}
    */
   @Override
-  public void train(AtomicReader atomicReader, String textFieldName,
+  public void train(LeafReader leafReader, String textFieldName,
                     String classFieldName, Analyzer analyzer) throws IOException {
-    train(atomicReader, textFieldName, classFieldName, analyzer, null);
+    train(leafReader, textFieldName, classFieldName, analyzer, null);
   }
 
   /**
    * {@inheritDoc}
    */
   @Override
-  public void train(AtomicReader atomicReader, String textFieldName,
+  public void train(LeafReader leafReader, String textFieldName,
       String classFieldName, Analyzer analyzer, Query query) throws IOException {
-    this.textTerms = MultiFields.getTerms(atomicReader, textFieldName);
+    this.textTerms = MultiFields.getTerms(leafReader, textFieldName);
 
     if (textTerms == null) {
       throw new IOException("term vectors need to be available for field " + textFieldName);
@@ -140,7 +139,7 @@ public class BooleanPerceptronClassifier implements Classifier<Boolean> {
 
     if (threshold == null || threshold == 0d) {
       // automatic assign a threshold
-      long sumDocFreq = atomicReader.getSumDocFreq(textFieldName);
+      long sumDocFreq = leafReader.getSumDocFreq(textFieldName);
       if (sumDocFreq != -1) {
         this.threshold = (double) sumDocFreq / 2d;
       } else {
@@ -160,7 +159,7 @@ public class BooleanPerceptronClassifier implements Classifier<Boolean> {
     }
     updateFST(weights);
 
-    IndexSearcher indexSearcher = new IndexSearcher(atomicReader);
+    IndexSearcher indexSearcher = new IndexSearcher(leafReader);
 
     int batchCount = 0;
 
@@ -185,7 +184,7 @@ public class BooleanPerceptronClassifier implements Classifier<Boolean> {
       Boolean correctClass = Boolean.valueOf(field.stringValue());
       long modifier = correctClass.compareTo(assignedClass);
       if (modifier != 0) {
-        reuse = updateWeights(atomicReader, reuse, scoreDoc.doc, assignedClass,
+        reuse = updateWeights(leafReader, reuse, scoreDoc.doc, assignedClass,
             weights, modifier, batchCount % batchSize == 0);
       }
       batchCount++;
@@ -194,17 +193,17 @@ public class BooleanPerceptronClassifier implements Classifier<Boolean> {
   }
 
   @Override
-  public void train(AtomicReader atomicReader, String[] textFieldNames, String classFieldName, Analyzer analyzer, Query query) throws IOException {
+  public void train(LeafReader leafReader, String[] textFieldNames, String classFieldName, Analyzer analyzer, Query query) throws IOException {
     throw new IOException("training with multiple fields not supported by boolean perceptron classifier");
   }
 
-  private TermsEnum updateWeights(AtomicReader atomicReader, TermsEnum reuse,
+  private TermsEnum updateWeights(LeafReader leafReader, TermsEnum reuse,
       int docId, Boolean assignedClass, SortedMap<String,Double> weights,
       double modifier, boolean updateFST) throws IOException {
     TermsEnum cte = textTerms.iterator(reuse);
 
     // get the doc term vectors
-    Terms terms = atomicReader.getTermVector(docId, textFieldName);
+    Terms terms = leafReader.getTermVector(docId, textFieldName);
 
     if (terms == null) {
       throw new IOException("term vectors must be stored for field "
diff --git lucene/classification/src/java/org/apache/lucene/classification/CachingNaiveBayesClassifier.java lucene/classification/src/java/org/apache/lucene/classification/CachingNaiveBayesClassifier.java
index b1d0795..f43c1f8 100644
--- lucene/classification/src/java/org/apache/lucene/classification/CachingNaiveBayesClassifier.java
+++ lucene/classification/src/java/org/apache/lucene/classification/CachingNaiveBayesClassifier.java
@@ -9,7 +9,7 @@ import java.util.Map;
 import java.util.concurrent.ConcurrentHashMap;
 
 import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.index.AtomicReader;
+import org.apache.lucene.index.LeafReader;
 import org.apache.lucene.index.MultiFields;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.index.Terms;
@@ -60,7 +60,7 @@ public class CachingNaiveBayesClassifier extends SimpleNaiveBayesClassifier {
 
   /**
    * Creates a new NaiveBayes classifier with inside caching. Note that you must
-   * call {@link #train(AtomicReader, String, String, Analyzer) train()} before
+   * call {@link #train(org.apache.lucene.index.LeafReader, String, String, Analyzer) train()} before
    * you can classify any documents. If you want less memory usage you could
    * call {@link #reInitCache(int, boolean) reInitCache()}.
    */
@@ -71,30 +71,30 @@ public class CachingNaiveBayesClassifier extends SimpleNaiveBayesClassifier {
    * {@inheritDoc}
    */
   @Override
-  public void train(AtomicReader atomicReader, String textFieldName, String classFieldName, Analyzer analyzer) throws IOException {
-    train(atomicReader, textFieldName, classFieldName, analyzer, null);
+  public void train(LeafReader leafReader, String textFieldName, String classFieldName, Analyzer analyzer) throws IOException {
+    train(leafReader, textFieldName, classFieldName, analyzer, null);
   }
 
   /**
    * {@inheritDoc}
    */
   @Override
-  public void train(AtomicReader atomicReader, String textFieldName, String classFieldName, Analyzer analyzer, Query query) throws IOException {
-    train(atomicReader, new String[]{textFieldName}, classFieldName, analyzer, query);
+  public void train(LeafReader leafReader, String textFieldName, String classFieldName, Analyzer analyzer, Query query) throws IOException {
+    train(leafReader, new String[]{textFieldName}, classFieldName, analyzer, query);
   }
 
   /**
    * {@inheritDoc}
    */
   @Override
-  public void train(AtomicReader atomicReader, String[] textFieldNames, String classFieldName, Analyzer analyzer, Query query) throws IOException {
-    super.train(atomicReader, textFieldNames, classFieldName, analyzer, query);
+  public void train(LeafReader leafReader, String[] textFieldNames, String classFieldName, Analyzer analyzer, Query query) throws IOException {
+    super.train(leafReader, textFieldNames, classFieldName, analyzer, query);
     // building the cache
     reInitCache(0, true);
   }
 
   private List<ClassificationResult<BytesRef>> assignClassNormalizedList(String inputDocument) throws IOException {
-    if (atomicReader == null) {
+    if (leafReader == null) {
       throw new IOException("You must first call Classifier#train");
     }
 
@@ -241,7 +241,7 @@ public class CachingNaiveBayesClassifier extends SimpleNaiveBayesClassifier {
     // build the cache for the word
     Map<String, Long> frequencyMap = new HashMap<>();
     for (String textFieldName : textFieldNames) {
-      TermsEnum termsEnum = atomicReader.terms(textFieldName).iterator(null);
+      TermsEnum termsEnum = leafReader.terms(textFieldName).iterator(null);
       while (termsEnum.next() != null) {
         BytesRef term = termsEnum.term();
         String termText = term.utf8ToString();
@@ -258,7 +258,7 @@ public class CachingNaiveBayesClassifier extends SimpleNaiveBayesClassifier {
     }
 
     // fill the class list
-    Terms terms = MultiFields.getTerms(atomicReader, classFieldName);
+    Terms terms = MultiFields.getTerms(leafReader, classFieldName);
     TermsEnum termsEnum = terms.iterator(null);
     while ((termsEnum.next()) != null) {
       cclasses.add(BytesRef.deepCopyOf(termsEnum.term()));
@@ -267,11 +267,11 @@ public class CachingNaiveBayesClassifier extends SimpleNaiveBayesClassifier {
     for (BytesRef cclass : cclasses) {
       double avgNumberOfUniqueTerms = 0;
       for (String textFieldName : textFieldNames) {
-        terms = MultiFields.getTerms(atomicReader, textFieldName);
+        terms = MultiFields.getTerms(leafReader, textFieldName);
         long numPostings = terms.getSumDocFreq(); // number of term/doc pairs
         avgNumberOfUniqueTerms += numPostings / (double) terms.getDocCount();
       }
-      int docsWithC = atomicReader.docFreq(new Term(classFieldName, cclass));
+      int docsWithC = leafReader.docFreq(new Term(classFieldName, cclass));
       classTermFreq.put(cclass, avgNumberOfUniqueTerms * docsWithC);
     }
   }
diff --git lucene/classification/src/java/org/apache/lucene/classification/Classifier.java lucene/classification/src/java/org/apache/lucene/classification/Classifier.java
index 360ff5d..7591747 100644
--- lucene/classification/src/java/org/apache/lucene/classification/Classifier.java
+++ lucene/classification/src/java/org/apache/lucene/classification/Classifier.java
@@ -20,7 +20,7 @@ import java.io.IOException;
 import java.util.List;
 
 import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.index.AtomicReader;
+import org.apache.lucene.index.LeafReader;
 import org.apache.lucene.search.Query;
 import org.apache.lucene.util.BytesRef;
 
@@ -63,39 +63,39 @@ public interface Classifier<T> {
   /**
    * Train the classifier using the underlying Lucene index
    *
-   * @param atomicReader   the reader to use to access the Lucene index
+   * @param leafReader   the reader to use to access the Lucene index
    * @param textFieldName  the name of the field used to compare documents
    * @param classFieldName the name of the field containing the class assigned to documents
    * @param analyzer       the analyzer used to tokenize / filter the unseen text
    * @throws IOException If there is a low-level I/O error.
    */
-  public void train(AtomicReader atomicReader, String textFieldName, String classFieldName, Analyzer analyzer)
+  public void train(LeafReader leafReader, String textFieldName, String classFieldName, Analyzer analyzer)
       throws IOException;
 
   /**
    * Train the classifier using the underlying Lucene index
    *
-   * @param atomicReader   the reader to use to access the Lucene index
+   * @param leafReader   the reader to use to access the Lucene index
    * @param textFieldName  the name of the field used to compare documents
    * @param classFieldName the name of the field containing the class assigned to documents
    * @param analyzer       the analyzer used to tokenize / filter the unseen text
    * @param query          the query to filter which documents use for training
    * @throws IOException If there is a low-level I/O error.
    */
-  public void train(AtomicReader atomicReader, String textFieldName, String classFieldName, Analyzer analyzer, Query query)
+  public void train(LeafReader leafReader, String textFieldName, String classFieldName, Analyzer analyzer, Query query)
       throws IOException;
 
   /**
    * Train the classifier using the underlying Lucene index
    *
-   * @param atomicReader   the reader to use to access the Lucene index
+   * @param leafReader   the reader to use to access the Lucene index
    * @param textFieldNames the names of the fields to be used to compare documents
    * @param classFieldName the name of the field containing the class assigned to documents
    * @param analyzer       the analyzer used to tokenize / filter the unseen text
    * @param query          the query to filter which documents use for training
    * @throws IOException If there is a low-level I/O error.
    */
-  public void train(AtomicReader atomicReader, String[] textFieldNames, String classFieldName, Analyzer analyzer, Query query)
+  public void train(LeafReader leafReader, String[] textFieldNames, String classFieldName, Analyzer analyzer, Query query)
       throws IOException;
 
 }
diff --git lucene/classification/src/java/org/apache/lucene/classification/KNearestNeighborClassifier.java lucene/classification/src/java/org/apache/lucene/classification/KNearestNeighborClassifier.java
index d8590e2..f778726 100644
--- lucene/classification/src/java/org/apache/lucene/classification/KNearestNeighborClassifier.java
+++ lucene/classification/src/java/org/apache/lucene/classification/KNearestNeighborClassifier.java
@@ -17,7 +17,7 @@
 package org.apache.lucene.classification;
 
 import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.index.AtomicReader;
+import org.apache.lucene.index.LeafReader;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.queries.mlt.MoreLikeThis;
 import org.apache.lucene.search.BooleanClause;
@@ -166,29 +166,29 @@ public class KNearestNeighborClassifier implements Classifier<BytesRef> {
    * {@inheritDoc}
    */
   @Override
-  public void train(AtomicReader atomicReader, String textFieldName, String classFieldName, Analyzer analyzer) throws IOException {
-    train(atomicReader, textFieldName, classFieldName, analyzer, null);
+  public void train(LeafReader leafReader, String textFieldName, String classFieldName, Analyzer analyzer) throws IOException {
+    train(leafReader, textFieldName, classFieldName, analyzer, null);
   }
 
   /**
    * {@inheritDoc}
    */
   @Override
-  public void train(AtomicReader atomicReader, String textFieldName, String classFieldName, Analyzer analyzer, Query query) throws IOException {
-    train(atomicReader, new String[]{textFieldName}, classFieldName, analyzer, query);
+  public void train(LeafReader leafReader, String textFieldName, String classFieldName, Analyzer analyzer, Query query) throws IOException {
+    train(leafReader, new String[]{textFieldName}, classFieldName, analyzer, query);
   }
 
   /**
    * {@inheritDoc}
    */
   @Override
-  public void train(AtomicReader atomicReader, String[] textFieldNames, String classFieldName, Analyzer analyzer, Query query) throws IOException {
+  public void train(LeafReader leafReader, String[] textFieldNames, String classFieldName, Analyzer analyzer, Query query) throws IOException {
     this.textFieldNames = textFieldNames;
     this.classFieldName = classFieldName;
-    mlt = new MoreLikeThis(atomicReader);
+    mlt = new MoreLikeThis(leafReader);
     mlt.setAnalyzer(analyzer);
     mlt.setFieldNames(textFieldNames);
-    indexSearcher = new IndexSearcher(atomicReader);
+    indexSearcher = new IndexSearcher(leafReader);
     if (minDocsFreq > 0) {
       mlt.setMinDocFreq(minDocsFreq);
     }
diff --git lucene/classification/src/java/org/apache/lucene/classification/SimpleNaiveBayesClassifier.java lucene/classification/src/java/org/apache/lucene/classification/SimpleNaiveBayesClassifier.java
index 4d521d6..a5a9c5f 100644
--- lucene/classification/src/java/org/apache/lucene/classification/SimpleNaiveBayesClassifier.java
+++ lucene/classification/src/java/org/apache/lucene/classification/SimpleNaiveBayesClassifier.java
@@ -26,7 +26,7 @@ import java.util.List;
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
-import org.apache.lucene.index.AtomicReader;
+import org.apache.lucene.index.LeafReader;
 import org.apache.lucene.index.MultiFields;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.index.Terms;
@@ -48,10 +48,10 @@ import org.apache.lucene.util.BytesRef;
 public class SimpleNaiveBayesClassifier implements Classifier<BytesRef> {
 
   /**
-   * {@link org.apache.lucene.index.AtomicReader} used to access the {@link org.apache.lucene.classification.Classifier}'s
+   * {@link org.apache.lucene.index.LeafReader} used to access the {@link org.apache.lucene.classification.Classifier}'s
    * index
    */
-  protected AtomicReader atomicReader;
+  protected LeafReader leafReader;
 
   /**
    * names of the fields to be used as input text
@@ -80,7 +80,7 @@ public class SimpleNaiveBayesClassifier implements Classifier<BytesRef> {
 
   /**
    * Creates a new NaiveBayes classifier.
-   * Note that you must call {@link #train(AtomicReader, String, String, Analyzer) train()} before you can
+   * Note that you must call {@link #train(org.apache.lucene.index.LeafReader, String, String, Analyzer) train()} before you can
    * classify any documents.
    */
   public SimpleNaiveBayesClassifier() {
@@ -90,27 +90,27 @@ public class SimpleNaiveBayesClassifier implements Classifier<BytesRef> {
    * {@inheritDoc}
    */
   @Override
-  public void train(AtomicReader atomicReader, String textFieldName, String classFieldName, Analyzer analyzer) throws IOException {
-    train(atomicReader, textFieldName, classFieldName, analyzer, null);
+  public void train(LeafReader leafReader, String textFieldName, String classFieldName, Analyzer analyzer) throws IOException {
+    train(leafReader, textFieldName, classFieldName, analyzer, null);
   }
 
   /**
    * {@inheritDoc}
    */
   @Override
-  public void train(AtomicReader atomicReader, String textFieldName, String classFieldName, Analyzer analyzer, Query query)
+  public void train(LeafReader leafReader, String textFieldName, String classFieldName, Analyzer analyzer, Query query)
       throws IOException {
-    train(atomicReader, new String[]{textFieldName}, classFieldName, analyzer, query);
+    train(leafReader, new String[]{textFieldName}, classFieldName, analyzer, query);
   }
 
   /**
    * {@inheritDoc}
    */
   @Override
-  public void train(AtomicReader atomicReader, String[] textFieldNames, String classFieldName, Analyzer analyzer, Query query)
+  public void train(LeafReader leafReader, String[] textFieldNames, String classFieldName, Analyzer analyzer, Query query)
       throws IOException {
-    this.atomicReader = atomicReader;
-    this.indexSearcher = new IndexSearcher(this.atomicReader);
+    this.leafReader = leafReader;
+    this.indexSearcher = new IndexSearcher(this.leafReader);
     this.textFieldNames = textFieldNames;
     this.classFieldName = classFieldName;
     this.analyzer = analyzer;
@@ -155,12 +155,12 @@ public class SimpleNaiveBayesClassifier implements Classifier<BytesRef> {
   }
 
   private List<ClassificationResult<BytesRef>> assignClassNormalizedList(String inputDocument) throws IOException {
-    if (atomicReader == null) {
+    if (leafReader == null) {
       throw new IOException("You must first call Classifier#train");
     }
     List<ClassificationResult<BytesRef>> dataList = new ArrayList<>();
 
-    Terms terms = MultiFields.getTerms(atomicReader, classFieldName);
+    Terms terms = MultiFields.getTerms(leafReader, classFieldName);
     TermsEnum termsEnum = terms.iterator(null);
     BytesRef next;
     String[] tokenizedDoc = tokenizeDoc(inputDocument);
@@ -203,7 +203,7 @@ public class SimpleNaiveBayesClassifier implements Classifier<BytesRef> {
    * @throws IOException if accessing to term vectors or search fails
    */
   protected int countDocsWithClass() throws IOException {
-    int docCount = MultiFields.getTerms(this.atomicReader, this.classFieldName).getDocCount();
+    int docCount = MultiFields.getTerms(this.leafReader, this.classFieldName).getDocCount();
     if (docCount == -1) { // in case codec doesn't support getDocCount
       TotalHitCountCollector totalHitCountCollector = new TotalHitCountCollector();
       BooleanQuery q = new BooleanQuery();
@@ -265,11 +265,11 @@ public class SimpleNaiveBayesClassifier implements Classifier<BytesRef> {
   private double getTextTermFreqForClass(BytesRef c) throws IOException {
     double avgNumberOfUniqueTerms = 0;
     for (String textFieldName : textFieldNames) {
-      Terms terms = MultiFields.getTerms(atomicReader, textFieldName);
+      Terms terms = MultiFields.getTerms(leafReader, textFieldName);
       long numPostings = terms.getSumDocFreq(); // number of term/doc pairs
       avgNumberOfUniqueTerms += numPostings / (double) terms.getDocCount(); // avg # of unique terms per doc
     }
-    int docsWithC = atomicReader.docFreq(new Term(classFieldName, c));
+    int docsWithC = leafReader.docFreq(new Term(classFieldName, c));
     return avgNumberOfUniqueTerms * docsWithC; // avg # of unique terms in text fields per doc * # docs with c
   }
 
@@ -294,6 +294,6 @@ public class SimpleNaiveBayesClassifier implements Classifier<BytesRef> {
   }
 
   private int docCount(BytesRef countedClass) throws IOException {
-    return atomicReader.docFreq(new Term(classFieldName, countedClass));
+    return leafReader.docFreq(new Term(classFieldName, countedClass));
   }
 }
diff --git lucene/classification/src/java/org/apache/lucene/classification/utils/DatasetSplitter.java lucene/classification/src/java/org/apache/lucene/classification/utils/DatasetSplitter.java
index 48d93c1..84fa73b 100644
--- lucene/classification/src/java/org/apache/lucene/classification/utils/DatasetSplitter.java
+++ lucene/classification/src/java/org/apache/lucene/classification/utils/DatasetSplitter.java
@@ -22,7 +22,7 @@ import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.FieldType;
 import org.apache.lucene.document.TextField;
-import org.apache.lucene.index.AtomicReader;
+import org.apache.lucene.index.LeafReader;
 import org.apache.lucene.index.IndexWriter;
 import org.apache.lucene.index.IndexWriterConfig;
 import org.apache.lucene.index.StorableField;
@@ -56,7 +56,7 @@ public class DatasetSplitter {
   /**
    * Split a given index into 3 indexes for training, test and cross validation tasks respectively
    *
-   * @param originalIndex        an {@link AtomicReader} on the source index
+   * @param originalIndex        an {@link org.apache.lucene.index.LeafReader} on the source index
    * @param trainingIndex        a {@link Directory} used to write the training index
    * @param testIndex            a {@link Directory} used to write the test index
    * @param crossValidationIndex a {@link Directory} used to write the cross validation index
@@ -64,7 +64,7 @@ public class DatasetSplitter {
    * @param fieldNames           names of fields that need to be put in the new indexes or <code>null</code> if all should be used
    * @throws IOException if any writing operation fails on any of the indexes
    */
-  public void split(AtomicReader originalIndex, Directory trainingIndex, Directory testIndex, Directory crossValidationIndex,
+  public void split(LeafReader originalIndex, Directory trainingIndex, Directory testIndex, Directory crossValidationIndex,
                     Analyzer analyzer, String... fieldNames) throws IOException {
 
     // create IWs for train / test / cv IDXs
diff --git lucene/classification/src/test/org/apache/lucene/classification/ClassificationTestBase.java lucene/classification/src/test/org/apache/lucene/classification/ClassificationTestBase.java
index 86ef649..179cf4e 100644
--- lucene/classification/src/test/org/apache/lucene/classification/ClassificationTestBase.java
+++ lucene/classification/src/test/org/apache/lucene/classification/ClassificationTestBase.java
@@ -21,7 +21,7 @@ import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.FieldType;
 import org.apache.lucene.document.TextField;
-import org.apache.lucene.index.AtomicReader;
+import org.apache.lucene.index.LeafReader;
 import org.apache.lucene.index.RandomIndexWriter;
 import org.apache.lucene.index.SlowCompositeReaderWrapper;
 import org.apache.lucene.search.Query;
@@ -83,18 +83,18 @@ public abstract class ClassificationTestBase<T> extends LuceneTestCase {
   }
 
   protected void checkCorrectClassification(Classifier<T> classifier, String inputDoc, T expectedResult, Analyzer analyzer, String textFieldName, String classFieldName, Query query) throws Exception {
-    AtomicReader atomicReader = null;
+    LeafReader leafReader = null;
     try {
       populateSampleIndex(analyzer);
-      atomicReader = SlowCompositeReaderWrapper.wrap(indexWriter.getReader());
-      classifier.train(atomicReader, textFieldName, classFieldName, analyzer, query);
+      leafReader = SlowCompositeReaderWrapper.wrap(indexWriter.getReader());
+      classifier.train(leafReader, textFieldName, classFieldName, analyzer, query);
       ClassificationResult<T> classificationResult = classifier.assignClass(inputDoc);
       assertNotNull(classificationResult.getAssignedClass());
       assertEquals("got an assigned class of " + classificationResult.getAssignedClass(), expectedResult, classificationResult.getAssignedClass());
       assertTrue("got a not positive score " + classificationResult.getScore(), classificationResult.getScore() > 0);
     } finally {
-      if (atomicReader != null)
-        atomicReader.close();
+      if (leafReader != null)
+        leafReader.close();
     }
   }
   protected void checkOnlineClassification(Classifier<T> classifier, String inputDoc, T expectedResult, Analyzer analyzer, String textFieldName, String classFieldName) throws Exception {
@@ -102,11 +102,11 @@ public abstract class ClassificationTestBase<T> extends LuceneTestCase {
   }
 
   protected void checkOnlineClassification(Classifier<T> classifier, String inputDoc, T expectedResult, Analyzer analyzer, String textFieldName, String classFieldName, Query query) throws Exception {
-    AtomicReader atomicReader = null;
+    LeafReader leafReader = null;
     try {
       populateSampleIndex(analyzer);
-      atomicReader = SlowCompositeReaderWrapper.wrap(indexWriter.getReader());
-      classifier.train(atomicReader, textFieldName, classFieldName, analyzer, query);
+      leafReader = SlowCompositeReaderWrapper.wrap(indexWriter.getReader());
+      classifier.train(leafReader, textFieldName, classFieldName, analyzer, query);
       ClassificationResult<T> classificationResult = classifier.assignClass(inputDoc);
       assertNotNull(classificationResult.getAssignedClass());
       assertEquals("got an assigned class of " + classificationResult.getAssignedClass(), expectedResult, classificationResult.getAssignedClass());
@@ -117,8 +117,8 @@ public abstract class ClassificationTestBase<T> extends LuceneTestCase {
       assertEquals(Double.valueOf(classificationResult.getScore()), Double.valueOf(secondClassificationResult.getScore()));
 
     } finally {
-      if (atomicReader != null)
-        atomicReader.close();
+      if (leafReader != null)
+        leafReader.close();
     }
   }
 
@@ -199,18 +199,18 @@ public abstract class ClassificationTestBase<T> extends LuceneTestCase {
   }
 
   protected void checkPerformance(Classifier<T> classifier, Analyzer analyzer, String classFieldName) throws Exception {
-    AtomicReader atomicReader = null;
+    LeafReader leafReader = null;
     long trainStart = System.currentTimeMillis();
     try {
       populatePerformanceIndex(analyzer);
-      atomicReader = SlowCompositeReaderWrapper.wrap(indexWriter.getReader());
-      classifier.train(atomicReader, textFieldName, classFieldName, analyzer);
+      leafReader = SlowCompositeReaderWrapper.wrap(indexWriter.getReader());
+      classifier.train(leafReader, textFieldName, classFieldName, analyzer);
       long trainEnd = System.currentTimeMillis();
       long trainTime = trainEnd - trainStart;
       assertTrue("training took more than 2 mins : " + trainTime / 1000 + "s", trainTime < 120000);
     } finally {
-      if (atomicReader != null)
-        atomicReader.close();
+      if (leafReader != null)
+        leafReader.close();
     }
   }
 
diff --git lucene/classification/src/test/org/apache/lucene/classification/utils/DataSplitterTest.java lucene/classification/src/test/org/apache/lucene/classification/utils/DataSplitterTest.java
index 84cccb7..e749345 100644
--- lucene/classification/src/test/org/apache/lucene/classification/utils/DataSplitterTest.java
+++ lucene/classification/src/test/org/apache/lucene/classification/utils/DataSplitterTest.java
@@ -23,7 +23,7 @@ import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.FieldType;
 import org.apache.lucene.document.TextField;
-import org.apache.lucene.index.AtomicReader;
+import org.apache.lucene.index.LeafReader;
 import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.RandomIndexWriter;
@@ -44,7 +44,7 @@ import java.util.Random;
  */
 public class DataSplitterTest extends LuceneTestCase {
 
-  private AtomicReader originalIndex;
+  private LeafReader originalIndex;
   private RandomIndexWriter indexWriter;
   private Directory dir;
 
@@ -103,7 +103,7 @@ public class DataSplitterTest extends LuceneTestCase {
     assertSplit(originalIndex, 0.2, 0.35, idFieldName, textFieldName);
   }
 
-  public static void assertSplit(AtomicReader originalIndex, double testRatio, double crossValidationRatio, String... fieldNames) throws Exception {
+  public static void assertSplit(LeafReader originalIndex, double testRatio, double crossValidationRatio, String... fieldNames) throws Exception {
 
     BaseDirectoryWrapper trainingIndex = newDirectory();
     BaseDirectoryWrapper testIndex = newDirectory();
diff --git lucene/core/src/java/org/apache/lucene/codecs/DocValuesConsumer.java lucene/core/src/java/org/apache/lucene/codecs/DocValuesConsumer.java
index d90a6b4..3aeb247 100644
--- lucene/core/src/java/org/apache/lucene/codecs/DocValuesConsumer.java
+++ lucene/core/src/java/org/apache/lucene/codecs/DocValuesConsumer.java
@@ -24,7 +24,7 @@ import java.util.Iterator;
 import java.util.List;
 import java.util.NoSuchElementException;
 
-import org.apache.lucene.index.AtomicReader;
+import org.apache.lucene.index.LeafReader;
 import org.apache.lucene.index.BinaryDocValues;
 import org.apache.lucene.index.FieldInfo;
 import org.apache.lucene.index.FilteredTermsEnum;
@@ -137,7 +137,7 @@ public abstract class DocValuesConsumer implements Closeable {
         if (type == DocValuesType.NUMERIC) {
           List<NumericDocValues> toMerge = new ArrayList<>();
           List<Bits> docsWithField = new ArrayList<>();
-          for (AtomicReader reader : mergeState.readers) {
+          for (LeafReader reader : mergeState.readers) {
             NumericDocValues values = reader.getNumericDocValues(field.name);
             Bits bits = reader.getDocsWithField(field.name);
             if (values == null) {
@@ -151,7 +151,7 @@ public abstract class DocValuesConsumer implements Closeable {
         } else if (type == DocValuesType.BINARY) {
           List<BinaryDocValues> toMerge = new ArrayList<>();
           List<Bits> docsWithField = new ArrayList<>();
-          for (AtomicReader reader : mergeState.readers) {
+          for (LeafReader reader : mergeState.readers) {
             BinaryDocValues values = reader.getBinaryDocValues(field.name);
             Bits bits = reader.getDocsWithField(field.name);
             if (values == null) {
@@ -164,7 +164,7 @@ public abstract class DocValuesConsumer implements Closeable {
           mergeBinaryField(field, mergeState, toMerge, docsWithField);
         } else if (type == DocValuesType.SORTED) {
           List<SortedDocValues> toMerge = new ArrayList<>();
-          for (AtomicReader reader : mergeState.readers) {
+          for (LeafReader reader : mergeState.readers) {
             SortedDocValues values = reader.getSortedDocValues(field.name);
             if (values == null) {
               values = DocValues.emptySorted();
@@ -174,7 +174,7 @@ public abstract class DocValuesConsumer implements Closeable {
           mergeSortedField(field, mergeState, toMerge);
         } else if (type == DocValuesType.SORTED_SET) {
           List<SortedSetDocValues> toMerge = new ArrayList<>();
-          for (AtomicReader reader : mergeState.readers) {
+          for (LeafReader reader : mergeState.readers) {
             SortedSetDocValues values = reader.getSortedSetDocValues(field.name);
             if (values == null) {
               values = DocValues.emptySortedSet();
@@ -184,7 +184,7 @@ public abstract class DocValuesConsumer implements Closeable {
           mergeSortedSetField(field, mergeState, toMerge);
         } else if (type == DocValuesType.SORTED_NUMERIC) {
           List<SortedNumericDocValues> toMerge = new ArrayList<>();
-          for (AtomicReader reader : mergeState.readers) {
+          for (LeafReader reader : mergeState.readers) {
             SortedNumericDocValues values = reader.getSortedNumericDocValues(field.name);
             if (values == null) {
               values = DocValues.emptySortedNumeric(reader.maxDoc());
@@ -216,7 +216,7 @@ public abstract class DocValuesConsumer implements Closeable {
                           int docIDUpto;
                           long nextValue;
                           boolean nextHasValue;
-                          AtomicReader currentReader;
+                          LeafReader currentReader;
                           NumericDocValues currentValues;
                           Bits currentLiveDocs;
                           Bits currentDocsWithField;
@@ -297,7 +297,7 @@ public abstract class DocValuesConsumer implements Closeable {
                          int docIDUpto;
                          BytesRef nextValue;
                          BytesRef nextPointer; // points to null if missing, or nextValue
-                         AtomicReader currentReader;
+                         LeafReader currentReader;
                          BinaryDocValues currentValues;
                          Bits currentLiveDocs;
                          Bits currentDocsWithField;
@@ -368,7 +368,7 @@ public abstract class DocValuesConsumer implements Closeable {
    * iterables that filter deleted documents.
    */
   public void mergeSortedNumericField(FieldInfo fieldInfo, final MergeState mergeState, List<SortedNumericDocValues> toMerge) throws IOException {
-    final AtomicReader readers[] = mergeState.readers.toArray(new AtomicReader[toMerge.size()]);
+    final LeafReader readers[] = mergeState.readers.toArray(new LeafReader[toMerge.size()]);
     final SortedNumericDocValues dvs[] = toMerge.toArray(new SortedNumericDocValues[toMerge.size()]);
     
     // step 3: add field
@@ -381,7 +381,7 @@ public abstract class DocValuesConsumer implements Closeable {
               int readerUpto = -1;
               int docIDUpto;
               int nextValue;
-              AtomicReader currentReader;
+              LeafReader currentReader;
               Bits currentLiveDocs;
               boolean nextIsSet;
 
@@ -444,7 +444,7 @@ public abstract class DocValuesConsumer implements Closeable {
               int readerUpto = -1;
               int docIDUpto;
               long nextValue;
-              AtomicReader currentReader;
+              LeafReader currentReader;
               Bits currentLiveDocs;
               boolean nextIsSet;
               int valueUpto;
@@ -519,14 +519,14 @@ public abstract class DocValuesConsumer implements Closeable {
    * an Iterable that merges ordinals and values and filters deleted documents .
    */
   public void mergeSortedField(FieldInfo fieldInfo, final MergeState mergeState, List<SortedDocValues> toMerge) throws IOException {
-    final AtomicReader readers[] = mergeState.readers.toArray(new AtomicReader[toMerge.size()]);
+    final LeafReader readers[] = mergeState.readers.toArray(new LeafReader[toMerge.size()]);
     final SortedDocValues dvs[] = toMerge.toArray(new SortedDocValues[toMerge.size()]);
     
     // step 1: iterate thru each sub and mark terms still in use
     TermsEnum liveTerms[] = new TermsEnum[dvs.length];
     long[] weights = new long[liveTerms.length];
     for (int sub = 0; sub < liveTerms.length; sub++) {
-      AtomicReader reader = readers[sub];
+      LeafReader reader = readers[sub];
       SortedDocValues dv = dvs[sub];
       Bits liveDocs = reader.getLiveDocs();
       if (liveDocs == null) {
@@ -591,7 +591,7 @@ public abstract class DocValuesConsumer implements Closeable {
               int readerUpto = -1;
               int docIDUpto;
               int nextValue;
-              AtomicReader currentReader;
+              LeafReader currentReader;
               Bits currentLiveDocs;
               LongValues currentMap;
               boolean nextIsSet;
@@ -658,14 +658,14 @@ public abstract class DocValuesConsumer implements Closeable {
    * an Iterable that merges ordinals and values and filters deleted documents .
    */
   public void mergeSortedSetField(FieldInfo fieldInfo, final MergeState mergeState, List<SortedSetDocValues> toMerge) throws IOException {
-    final AtomicReader readers[] = mergeState.readers.toArray(new AtomicReader[toMerge.size()]);
+    final LeafReader readers[] = mergeState.readers.toArray(new LeafReader[toMerge.size()]);
     final SortedSetDocValues dvs[] = toMerge.toArray(new SortedSetDocValues[toMerge.size()]);
     
     // step 1: iterate thru each sub and mark terms still in use
     TermsEnum liveTerms[] = new TermsEnum[dvs.length];
     long[] weights = new long[liveTerms.length];
     for (int sub = 0; sub < liveTerms.length; sub++) {
-      AtomicReader reader = readers[sub];
+      LeafReader reader = readers[sub];
       SortedSetDocValues dv = dvs[sub];
       Bits liveDocs = reader.getLiveDocs();
       if (liveDocs == null) {
@@ -731,7 +731,7 @@ public abstract class DocValuesConsumer implements Closeable {
               int readerUpto = -1;
               int docIDUpto;
               int nextValue;
-              AtomicReader currentReader;
+              LeafReader currentReader;
               Bits currentLiveDocs;
               boolean nextIsSet;
 
@@ -798,7 +798,7 @@ public abstract class DocValuesConsumer implements Closeable {
               int readerUpto = -1;
               int docIDUpto;
               long nextValue;
-              AtomicReader currentReader;
+              LeafReader currentReader;
               Bits currentLiveDocs;
               LongValues currentMap;
               boolean nextIsSet;
diff --git lucene/core/src/java/org/apache/lucene/codecs/FieldsConsumer.java lucene/core/src/java/org/apache/lucene/codecs/FieldsConsumer.java
index e787b7a..aaa535a 100644
--- lucene/core/src/java/org/apache/lucene/codecs/FieldsConsumer.java
+++ lucene/core/src/java/org/apache/lucene/codecs/FieldsConsumer.java
@@ -22,7 +22,7 @@ import java.io.IOException;
 import java.util.ArrayList;
 import java.util.List;
 
-import org.apache.lucene.index.AtomicReader;
+import org.apache.lucene.index.LeafReader;
 import org.apache.lucene.index.Fields;
 import org.apache.lucene.index.MappedMultiFields;
 import org.apache.lucene.index.MergeState;
@@ -91,7 +91,7 @@ public abstract class FieldsConsumer implements Closeable {
     int docBase = 0;
 
     for(int readerIndex=0;readerIndex<mergeState.readers.size();readerIndex++) {
-      final AtomicReader reader = mergeState.readers.get(readerIndex);
+      final LeafReader reader = mergeState.readers.get(readerIndex);
       final Fields f = reader.fields();
       final int maxDoc = reader.maxDoc();
       if (f != null) {
diff --git lucene/core/src/java/org/apache/lucene/codecs/NormsConsumer.java lucene/core/src/java/org/apache/lucene/codecs/NormsConsumer.java
index 6905b28..40c43cd 100644
--- lucene/core/src/java/org/apache/lucene/codecs/NormsConsumer.java
+++ lucene/core/src/java/org/apache/lucene/codecs/NormsConsumer.java
@@ -24,7 +24,7 @@ import java.util.Iterator;
 import java.util.List;
 import java.util.NoSuchElementException;
 
-import org.apache.lucene.index.AtomicReader;
+import org.apache.lucene.index.LeafReader;
 import org.apache.lucene.index.DocValues;
 import org.apache.lucene.index.FieldInfo;
 import org.apache.lucene.index.MergeState;
@@ -75,7 +75,7 @@ public abstract class NormsConsumer implements Closeable {
     for (FieldInfo field : mergeState.fieldInfos) {
       if (field.hasNorms()) {
         List<NumericDocValues> toMerge = new ArrayList<>();
-        for (AtomicReader reader : mergeState.readers) {
+        for (LeafReader reader : mergeState.readers) {
           NumericDocValues norms = reader.getNormValues(field.name);
           if (norms == null) {
             norms = DocValues.emptyNumeric();
@@ -104,7 +104,7 @@ public abstract class NormsConsumer implements Closeable {
                           int readerUpto = -1;
                           int docIDUpto;
                           long nextValue;
-                          AtomicReader currentReader;
+                          LeafReader currentReader;
                           NumericDocValues currentValues;
                           Bits currentLiveDocs;
                           boolean nextIsSet;
diff --git lucene/core/src/java/org/apache/lucene/codecs/StoredFieldsWriter.java lucene/core/src/java/org/apache/lucene/codecs/StoredFieldsWriter.java
index a17e890..067b1dc 100644
--- lucene/core/src/java/org/apache/lucene/codecs/StoredFieldsWriter.java
+++ lucene/core/src/java/org/apache/lucene/codecs/StoredFieldsWriter.java
@@ -25,7 +25,7 @@ import org.apache.lucene.index.MergeState;
 import org.apache.lucene.index.StorableField;
 import org.apache.lucene.index.StoredDocument;
 import org.apache.lucene.util.Bits;
-import org.apache.lucene.index.AtomicReader;
+import org.apache.lucene.index.LeafReader;
 
 /**
  * Codec API for writing stored fields:
@@ -82,7 +82,7 @@ public abstract class StoredFieldsWriter implements Closeable {
    *  merging (bulk-byte copying, etc). */
   public int merge(MergeState mergeState) throws IOException {
     int docCount = 0;
-    for (AtomicReader reader : mergeState.readers) {
+    for (LeafReader reader : mergeState.readers) {
       final int maxDoc = reader.maxDoc();
       final Bits liveDocs = reader.getLiveDocs();
       for (int i = 0; i < maxDoc; i++) {
diff --git lucene/core/src/java/org/apache/lucene/codecs/TermVectorsWriter.java lucene/core/src/java/org/apache/lucene/codecs/TermVectorsWriter.java
index 81ca327..84fbad1 100644
--- lucene/core/src/java/org/apache/lucene/codecs/TermVectorsWriter.java
+++ lucene/core/src/java/org/apache/lucene/codecs/TermVectorsWriter.java
@@ -21,7 +21,7 @@ import java.io.Closeable;
 import java.io.IOException;
 import java.util.Iterator;
 
-import org.apache.lucene.index.AtomicReader;
+import org.apache.lucene.index.LeafReader;
 import org.apache.lucene.index.DocsAndPositionsEnum;
 import org.apache.lucene.index.FieldInfo;
 import org.apache.lucene.index.FieldInfos;
@@ -178,7 +178,7 @@ public abstract class TermVectorsWriter implements Closeable {
   public int merge(MergeState mergeState) throws IOException {
     int docCount = 0;
     for (int i = 0; i < mergeState.readers.size(); i++) {
-      final AtomicReader reader = mergeState.readers.get(i);
+      final LeafReader reader = mergeState.readers.get(i);
       final int maxDoc = reader.maxDoc();
       final Bits liveDocs = reader.getLiveDocs();
 
diff --git lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsWriter.java lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsWriter.java
index 62d3b64..7f43dca 100644
--- lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsWriter.java
+++ lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsWriter.java
@@ -24,7 +24,7 @@ import org.apache.lucene.codecs.CodecUtil;
 import org.apache.lucene.codecs.StoredFieldsReader;
 import org.apache.lucene.codecs.StoredFieldsWriter;
 import org.apache.lucene.codecs.compressing.CompressingStoredFieldsReader.ChunkIterator;
-import org.apache.lucene.index.AtomicReader;
+import org.apache.lucene.index.LeafReader;
 import org.apache.lucene.index.CorruptIndexException;
 import org.apache.lucene.index.FieldInfo;
 import org.apache.lucene.index.FieldInfos;
@@ -337,7 +337,7 @@ public final class CompressingStoredFieldsWriter extends StoredFieldsWriter {
     
     MatchingReaders matching = new MatchingReaders(mergeState);
     
-    for (AtomicReader reader : mergeState.readers) {
+    for (LeafReader reader : mergeState.readers) {
       final SegmentReader matchingSegmentReader = matching.matchingSegmentReaders[idx++];
       CompressingStoredFieldsReader matchingFieldsReader = null;
       if (matchingSegmentReader != null) {
diff --git lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingTermVectorsWriter.java lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingTermVectorsWriter.java
index 304bd9c..a59e50b 100644
--- lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingTermVectorsWriter.java
+++ lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingTermVectorsWriter.java
@@ -28,7 +28,7 @@ import java.util.TreeSet;
 import org.apache.lucene.codecs.CodecUtil;
 import org.apache.lucene.codecs.TermVectorsReader;
 import org.apache.lucene.codecs.TermVectorsWriter;
-import org.apache.lucene.index.AtomicReader;
+import org.apache.lucene.index.LeafReader;
 import org.apache.lucene.index.FieldInfo;
 import org.apache.lucene.index.FieldInfos;
 import org.apache.lucene.index.Fields;
@@ -732,7 +732,7 @@ public final class CompressingTermVectorsWriter extends TermVectorsWriter {
 
     MatchingReaders matching = new MatchingReaders(mergeState);
     
-    for (AtomicReader reader : mergeState.readers) {
+    for (LeafReader reader : mergeState.readers) {
       final SegmentReader matchingSegmentReader = matching.matchingSegmentReaders[idx++];
       CompressingTermVectorsReader matchingVectorsReader = null;
       if (matchingSegmentReader != null) {
diff --git lucene/core/src/java/org/apache/lucene/codecs/compressing/MatchingReaders.java lucene/core/src/java/org/apache/lucene/codecs/compressing/MatchingReaders.java
index f79ead0..490f8fd 100644
--- lucene/core/src/java/org/apache/lucene/codecs/compressing/MatchingReaders.java
+++ lucene/core/src/java/org/apache/lucene/codecs/compressing/MatchingReaders.java
@@ -17,7 +17,7 @@ package org.apache.lucene.codecs.compressing;
  * limitations under the License.
  */
 
-import org.apache.lucene.index.AtomicReader;
+import org.apache.lucene.index.LeafReader;
 import org.apache.lucene.index.FieldInfo;
 import org.apache.lucene.index.FieldInfos;
 import org.apache.lucene.index.MergeState;
@@ -50,7 +50,7 @@ class MatchingReaders {
     // FieldInfos, then we can do a bulk copy of the
     // stored fields:
     for (int i = 0; i < numReaders; i++) {
-      AtomicReader reader = mergeState.readers.get(i);
+      LeafReader reader = mergeState.readers.get(i);
       // TODO: we may be able to broaden this to
       // non-SegmentReaders, since FieldInfos is now
       // required?  But... this'd also require exposing
diff --git lucene/core/src/java/org/apache/lucene/codecs/perfield/PerFieldPostingsFormat.java lucene/core/src/java/org/apache/lucene/codecs/perfield/PerFieldPostingsFormat.java
index 0980eef..ddd78d0 100644
--- lucene/core/src/java/org/apache/lucene/codecs/perfield/PerFieldPostingsFormat.java
+++ lucene/core/src/java/org/apache/lucene/codecs/perfield/PerFieldPostingsFormat.java
@@ -43,7 +43,7 @@ import org.apache.lucene.util.Accountables;
 import org.apache.lucene.util.IOUtils;
 import org.apache.lucene.util.RamUsageEstimator;
 
-import static org.apache.lucene.index.FilterAtomicReader.FilterFields;
+import static org.apache.lucene.index.FilterLeafReader.FilterFields;
 
 /**
  * Enables per field postings support.
diff --git lucene/core/src/java/org/apache/lucene/document/DoubleDocValuesField.java lucene/core/src/java/org/apache/lucene/document/DoubleDocValuesField.java
index a9ff000..f0e14d5 100644
--- lucene/core/src/java/org/apache/lucene/document/DoubleDocValuesField.java
+++ lucene/core/src/java/org/apache/lucene/document/DoubleDocValuesField.java
@@ -17,14 +17,12 @@ package org.apache.lucene.document;
  * limitations under the License.
  */
 
-import org.apache.lucene.index.AtomicReader; // javadocs
-
 /**
  * Syntactic sugar for encoding doubles as NumericDocValues
  * via {@link Double#doubleToRawLongBits(double)}.
  * <p>
  * Per-document double values can be retrieved via
- * {@link AtomicReader#getNumericDocValues(String)}.
+ * {@link org.apache.lucene.index.LeafReader#getNumericDocValues(String)}.
  * <p>
  * <b>NOTE</b>: In most all cases this will be rather inefficient,
  * requiring eight bytes per document. Consider encoding double
diff --git lucene/core/src/java/org/apache/lucene/document/DoubleField.java lucene/core/src/java/org/apache/lucene/document/DoubleField.java
index d58d4bb..e1728e5 100644
--- lucene/core/src/java/org/apache/lucene/document/DoubleField.java
+++ lucene/core/src/java/org/apache/lucene/document/DoubleField.java
@@ -18,7 +18,6 @@ package org.apache.lucene.document;
  */
 
 import org.apache.lucene.analysis.NumericTokenStream; // javadocs
-import org.apache.lucene.index.AtomicReader;
 import org.apache.lucene.index.FieldInfo.IndexOptions;
 import org.apache.lucene.search.NumericRangeFilter; // javadocs
 import org.apache.lucene.search.NumericRangeQuery; // javadocs
@@ -57,7 +56,7 @@ import org.apache.lucene.util.NumericUtils;
  * NumericRangeFilter}.  To sort according to a
  * <code>DoubleField</code>, use the normal numeric sort types, eg
  * {@link org.apache.lucene.search.SortField.Type#DOUBLE}. <code>DoubleField</code> 
- * values can also be loaded directly from {@link AtomicReader#getNumericDocValues}.</p>
+ * values can also be loaded directly from {@link org.apache.lucene.index.LeafReader#getNumericDocValues}.</p>
  *
  * <p>You may add the same field name as an <code>DoubleField</code> to
  * the same document more than once.  Range querying and
diff --git lucene/core/src/java/org/apache/lucene/document/FloatDocValuesField.java lucene/core/src/java/org/apache/lucene/document/FloatDocValuesField.java
index 5260c97..db0524f 100644
--- lucene/core/src/java/org/apache/lucene/document/FloatDocValuesField.java
+++ lucene/core/src/java/org/apache/lucene/document/FloatDocValuesField.java
@@ -17,14 +17,12 @@ package org.apache.lucene.document;
  * limitations under the License.
  */
 
-import org.apache.lucene.index.AtomicReader; // javadocs
-
 /**
  * Syntactic sugar for encoding floats as NumericDocValues
  * via {@link Float#floatToRawIntBits(float)}.
  * <p>
  * Per-document floating point values can be retrieved via
- * {@link AtomicReader#getNumericDocValues(String)}.
+ * {@link org.apache.lucene.index.LeafReader#getNumericDocValues(String)}.
  * <p>
  * <b>NOTE</b>: In most all cases this will be rather inefficient,
  * requiring four bytes per document. Consider encoding floating
diff --git lucene/core/src/java/org/apache/lucene/document/FloatField.java lucene/core/src/java/org/apache/lucene/document/FloatField.java
index f44a355..269f15e 100644
--- lucene/core/src/java/org/apache/lucene/document/FloatField.java
+++ lucene/core/src/java/org/apache/lucene/document/FloatField.java
@@ -18,7 +18,6 @@ package org.apache.lucene.document;
  */
 
 import org.apache.lucene.analysis.NumericTokenStream; // javadocs
-import org.apache.lucene.index.AtomicReader;
 import org.apache.lucene.index.FieldInfo.IndexOptions;
 import org.apache.lucene.search.NumericRangeFilter; // javadocs
 import org.apache.lucene.search.NumericRangeQuery; // javadocs
@@ -57,7 +56,7 @@ import org.apache.lucene.util.NumericUtils;
  * NumericRangeFilter}.  To sort according to a
  * <code>FloatField</code>, use the normal numeric sort types, eg
  * {@link org.apache.lucene.search.SortField.Type#FLOAT}. <code>FloatField</code> 
- * values can also be loaded directly from {@link AtomicReader#getNumericDocValues}.</p>
+ * values can also be loaded directly from {@link org.apache.lucene.index.LeafReader#getNumericDocValues}.</p>
  *
  * <p>You may add the same field name as an <code>FloatField</code> to
  * the same document more than once.  Range querying and
diff --git lucene/core/src/java/org/apache/lucene/document/IntField.java lucene/core/src/java/org/apache/lucene/document/IntField.java
index 97bd9f1..e810cb6 100644
--- lucene/core/src/java/org/apache/lucene/document/IntField.java
+++ lucene/core/src/java/org/apache/lucene/document/IntField.java
@@ -18,7 +18,6 @@ package org.apache.lucene.document;
  */
 
 import org.apache.lucene.analysis.NumericTokenStream; // javadocs
-import org.apache.lucene.index.AtomicReader;
 import org.apache.lucene.index.FieldInfo.IndexOptions;
 import org.apache.lucene.search.NumericRangeFilter; // javadocs
 import org.apache.lucene.search.NumericRangeQuery; // javadocs
@@ -57,7 +56,7 @@ import org.apache.lucene.util.NumericUtils;
  * NumericRangeFilter}.  To sort according to a
  * <code>IntField</code>, use the normal numeric sort types, eg
  * {@link org.apache.lucene.search.SortField.Type#INT}. <code>IntField</code> 
- * values can also be loaded directly from {@link AtomicReader#getNumericDocValues}.</p>
+ * values can also be loaded directly from {@link org.apache.lucene.index.LeafReader#getNumericDocValues}.</p>
  *
  * <p>You may add the same field name as an <code>IntField</code> to
  * the same document more than once.  Range querying and
diff --git lucene/core/src/java/org/apache/lucene/document/LongField.java lucene/core/src/java/org/apache/lucene/document/LongField.java
index 3e4dfb8..98d4408 100644
--- lucene/core/src/java/org/apache/lucene/document/LongField.java
+++ lucene/core/src/java/org/apache/lucene/document/LongField.java
@@ -18,7 +18,6 @@ package org.apache.lucene.document;
  */
 
 import org.apache.lucene.analysis.NumericTokenStream; // javadocs
-import org.apache.lucene.index.AtomicReader;
 import org.apache.lucene.index.FieldInfo.IndexOptions;
 import org.apache.lucene.search.NumericRangeFilter; // javadocs
 import org.apache.lucene.search.NumericRangeQuery; // javadocs
@@ -67,7 +66,7 @@ import org.apache.lucene.util.NumericUtils;
  * NumericRangeFilter}.  To sort according to a
  * <code>LongField</code>, use the normal numeric sort types, eg
  * {@link org.apache.lucene.search.SortField.Type#LONG}. <code>LongField</code> 
- * values can also be loaded directly from {@link AtomicReader#getNumericDocValues}.</p>
+ * values can also be loaded directly from {@link org.apache.lucene.index.LeafReader#getNumericDocValues}.</p>
  *
  * <p>You may add the same field name as an <code>LongField</code> to
  * the same document more than once.  Range querying and
diff --git lucene/core/src/java/org/apache/lucene/index/AtomicReader.java lucene/core/src/java/org/apache/lucene/index/AtomicReader.java
deleted file mode 100644
index 3f62251..0000000
--- lucene/core/src/java/org/apache/lucene/index/AtomicReader.java
+++ /dev/null
@@ -1,330 +0,0 @@
-package org.apache.lucene.index;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.index.IndexReader.ReaderClosedListener;
-import org.apache.lucene.util.Bits;
-
-/** {@code AtomicReader} is an abstract class, providing an interface for accessing an
- index.  Search of an index is done entirely through this abstract interface,
- so that any subclass which implements it is searchable. IndexReaders implemented
- by this subclass do not consist of several sub-readers,
- they are atomic. They support retrieval of stored fields, doc values, terms,
- and postings.
-
- <p>For efficiency, in this API documents are often referred to via
- <i>document numbers</i>, non-negative integers which each name a unique
- document in the index.  These document numbers are ephemeral -- they may change
- as documents are added to and deleted from an index.  Clients should thus not
- rely on a given document having the same number between sessions.
-
- <p>
- <a name="thread-safety"></a><p><b>NOTE</b>: {@link
- IndexReader} instances are completely thread
- safe, meaning multiple threads can call any of its methods,
- concurrently.  If your application requires external
- synchronization, you should <b>not</b> synchronize on the
- <code>IndexReader</code> instance; use your own
- (non-Lucene) objects instead.
-*/
-public abstract class AtomicReader extends IndexReader {
-
-  private final AtomicReaderContext readerContext = new AtomicReaderContext(this);
-
-  /** Sole constructor. (For invocation by subclass
-   *  constructors, typically implicit.) */
-  protected AtomicReader() {
-    super();
-  }
-
-  @Override
-  public final AtomicReaderContext getContext() {
-    ensureOpen();
-    return readerContext;
-  }
-
-  /**
-   * Called when the shared core for this {@link AtomicReader}
-   * is closed.
-   * <p>
-   * If this {@link AtomicReader} impl has the ability to share
-   * resources across instances that might only vary through
-   * deleted documents and doc values updates, then this listener
-   * will only be called when the shared core is closed.
-   * Otherwise, this listener will be called when this reader is
-   * closed.</p>
-   * <p>
-   * This is typically useful to manage per-segment caches: when
-   * the listener is called, it is safe to evict this reader from
-   * any caches keyed on {@link #getCoreCacheKey}.</p>
-   *
-   * @lucene.experimental
-   */
-  public static interface CoreClosedListener {
-    /** Invoked when the shared core of the original {@code
-     *  SegmentReader} has closed. */
-    public void onClose(Object ownerCoreCacheKey);
-  }
-
-  private static class CoreClosedListenerWrapper implements ReaderClosedListener {
-
-    private final CoreClosedListener listener;
-
-    CoreClosedListenerWrapper(CoreClosedListener listener) {
-      this.listener = listener;
-    }
-
-    @Override
-    public void onClose(IndexReader reader) {
-      listener.onClose(reader.getCoreCacheKey());
-    }
-
-    @Override
-    public int hashCode() {
-      return listener.hashCode();
-    }
-
-    @Override
-    public boolean equals(Object other) {
-      if (!(other instanceof CoreClosedListenerWrapper)) {
-        return false;
-      }
-      return listener.equals(((CoreClosedListenerWrapper) other).listener);
-    }
-
-  }
-
-  /** Add a {@link CoreClosedListener} as a {@link ReaderClosedListener}. This
-   * method is typically useful for {@link AtomicReader} implementations that
-   * don't have the concept of a core that is shared across several
-   * {@link AtomicReader} instances in which case the {@link CoreClosedListener}
-   * is called when closing the reader. */
-  protected static void addCoreClosedListenerAsReaderClosedListener(IndexReader reader, CoreClosedListener listener) {
-    reader.addReaderClosedListener(new CoreClosedListenerWrapper(listener));
-  }
-
-  /** Remove a {@link CoreClosedListener} which has been added with
-   * {@link #addCoreClosedListenerAsReaderClosedListener(IndexReader, CoreClosedListener)}. */
-  protected static void removeCoreClosedListenerAsReaderClosedListener(IndexReader reader, CoreClosedListener listener) {
-    reader.removeReaderClosedListener(new CoreClosedListenerWrapper(listener));
-  }
-
-  /** Expert: adds a CoreClosedListener to this reader's shared core
-   *  @lucene.experimental */
-  public abstract void addCoreClosedListener(CoreClosedListener listener);
-
-  /** Expert: removes a CoreClosedListener from this reader's shared core
-   *  @lucene.experimental */
-  public abstract void removeCoreClosedListener(CoreClosedListener listener);
-
-  /**
-   * Returns {@link Fields} for this reader.
-   * This method may return null if the reader has no
-   * postings.
-   */
-  public abstract Fields fields() throws IOException;
-
-  @Override
-  public final int docFreq(Term term) throws IOException {
-    final Fields fields = fields();
-    if (fields == null) {
-      return 0;
-    }
-    final Terms terms = fields.terms(term.field());
-    if (terms == null) {
-      return 0;
-    }
-    final TermsEnum termsEnum = terms.iterator(null);
-    if (termsEnum.seekExact(term.bytes())) {
-      return termsEnum.docFreq();
-    } else {
-      return 0;
-    }
-  }
-
-  /** Returns the number of documents containing the term
-   * <code>t</code>.  This method returns 0 if the term or
-   * field does not exists.  This method does not take into
-   * account deleted documents that have not yet been merged
-   * away. */
-  @Override
-  public final long totalTermFreq(Term term) throws IOException {
-    final Fields fields = fields();
-    if (fields == null) {
-      return 0;
-    }
-    final Terms terms = fields.terms(term.field());
-    if (terms == null) {
-      return 0;
-    }
-    final TermsEnum termsEnum = terms.iterator(null);
-    if (termsEnum.seekExact(term.bytes())) {
-      return termsEnum.totalTermFreq();
-    } else {
-      return 0;
-    }
-  }
-
-  @Override
-  public final long getSumDocFreq(String field) throws IOException {
-    final Terms terms = terms(field);
-    if (terms == null) {
-      return 0;
-    }
-    return terms.getSumDocFreq();
-  }
-
-  @Override
-  public final int getDocCount(String field) throws IOException {
-    final Terms terms = terms(field);
-    if (terms == null) {
-      return 0;
-    }
-    return terms.getDocCount();
-  }
-
-  @Override
-  public final long getSumTotalTermFreq(String field) throws IOException {
-    final Terms terms = terms(field);
-    if (terms == null) {
-      return 0;
-    }
-    return terms.getSumTotalTermFreq();
-  }
-
-  /** This may return null if the field does not exist.*/
-  public final Terms terms(String field) throws IOException {
-    final Fields fields = fields();
-    if (fields == null) {
-      return null;
-    }
-    return fields.terms(field);
-  }
-
-  /** Returns {@link DocsEnum} for the specified term.
-   *  This will return null if either the field or
-   *  term does not exist.
-   *  @see TermsEnum#docs(Bits, DocsEnum) */
-  public final DocsEnum termDocsEnum(Term term) throws IOException {
-    assert term.field() != null;
-    assert term.bytes() != null;
-    final Fields fields = fields();
-    if (fields != null) {
-      final Terms terms = fields.terms(term.field());
-      if (terms != null) {
-        final TermsEnum termsEnum = terms.iterator(null);
-        if (termsEnum.seekExact(term.bytes())) {
-          return termsEnum.docs(getLiveDocs(), null);
-        }
-      }
-    }
-    return null;
-  }
-
-  /** Returns {@link DocsAndPositionsEnum} for the specified
-   *  term.  This will return null if the
-   *  field or term does not exist or positions weren't indexed.
-   *  @see TermsEnum#docsAndPositions(Bits, DocsAndPositionsEnum) */
-  public final DocsAndPositionsEnum termPositionsEnum(Term term) throws IOException {
-    assert term.field() != null;
-    assert term.bytes() != null;
-    final Fields fields = fields();
-    if (fields != null) {
-      final Terms terms = fields.terms(term.field());
-      if (terms != null) {
-        final TermsEnum termsEnum = terms.iterator(null);
-        if (termsEnum.seekExact(term.bytes())) {
-          return termsEnum.docsAndPositions(getLiveDocs(), null);
-        }
-      }
-    }
-    return null;
-  }
-
-  /** Returns {@link NumericDocValues} for this field, or
-   *  null if no {@link NumericDocValues} were indexed for
-   *  this field.  The returned instance should only be
-   *  used by a single thread. */
-  public abstract NumericDocValues getNumericDocValues(String field) throws IOException;
-
-  /** Returns {@link BinaryDocValues} for this field, or
-   *  null if no {@link BinaryDocValues} were indexed for
-   *  this field.  The returned instance should only be
-   *  used by a single thread. */
-  public abstract BinaryDocValues getBinaryDocValues(String field) throws IOException;
-
-  /** Returns {@link SortedDocValues} for this field, or
-   *  null if no {@link SortedDocValues} were indexed for
-   *  this field.  The returned instance should only be
-   *  used by a single thread. */
-  public abstract SortedDocValues getSortedDocValues(String field) throws IOException;
-  
-  /** Returns {@link SortedNumericDocValues} for this field, or
-   *  null if no {@link SortedNumericDocValues} were indexed for
-   *  this field.  The returned instance should only be
-   *  used by a single thread. */
-  public abstract SortedNumericDocValues getSortedNumericDocValues(String field) throws IOException;
-
-  /** Returns {@link SortedSetDocValues} for this field, or
-   *  null if no {@link SortedSetDocValues} were indexed for
-   *  this field.  The returned instance should only be
-   *  used by a single thread. */
-  public abstract SortedSetDocValues getSortedSetDocValues(String field) throws IOException;
-
-  /** Returns a {@link Bits} at the size of <code>reader.maxDoc()</code>,
-   *  with turned on bits for each docid that does have a value for this field,
-   *  or null if no DocValues were indexed for this field. The
-   *  returned instance should only be used by a single thread */
-  public abstract Bits getDocsWithField(String field) throws IOException;
-
-  /** Returns {@link NumericDocValues} representing norms
-   *  for this field, or null if no {@link NumericDocValues}
-   *  were indexed. The returned instance should only be
-   *  used by a single thread. */
-  public abstract NumericDocValues getNormValues(String field) throws IOException;
-
-  /**
-   * Get the {@link FieldInfos} describing all fields in
-   * this reader.
-   * @lucene.experimental
-   */
-  public abstract FieldInfos getFieldInfos();
-
-  /** Returns the {@link Bits} representing live (not
-   *  deleted) docs.  A set bit indicates the doc ID has not
-   *  been deleted.  If this method returns null it means
-   *  there are no deleted documents (all documents are
-   *  live).
-   *
-   *  The returned instance has been safely published for
-   *  use by multiple threads without additional
-   *  synchronization.
-   */
-  public abstract Bits getLiveDocs();
-
-  /**
-   * Checks consistency of this reader.
-   * <p>
-   * Note that this may be costly in terms of I/O, e.g.
-   * may involve computing a checksum value against large data files.
-   * @lucene.internal
-   */
-  public abstract void checkIntegrity() throws IOException;
-}
diff --git lucene/core/src/java/org/apache/lucene/index/AtomicReaderContext.java lucene/core/src/java/org/apache/lucene/index/AtomicReaderContext.java
deleted file mode 100644
index decfc33..0000000
--- lucene/core/src/java/org/apache/lucene/index/AtomicReaderContext.java
+++ /dev/null
@@ -1,69 +0,0 @@
-package org.apache.lucene.index;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.util.Collections;
-import java.util.List;
-
-/**
- * {@link IndexReaderContext} for {@link AtomicReader} instances.
- */
-public final class AtomicReaderContext extends IndexReaderContext {
-  /** The readers ord in the top-level's leaves array */
-  public final int ord;
-  /** The readers absolute doc base */
-  public final int docBase;
-  
-  private final AtomicReader reader;
-  private final List<AtomicReaderContext> leaves;
-  
-  /**
-   * Creates a new {@link AtomicReaderContext} 
-   */    
-  AtomicReaderContext(CompositeReaderContext parent, AtomicReader reader,
-      int ord, int docBase, int leafOrd, int leafDocBase) {
-    super(parent, ord, docBase);
-    this.ord = leafOrd;
-    this.docBase = leafDocBase;
-    this.reader = reader;
-    this.leaves = isTopLevel ? Collections.singletonList(this) : null;
-  }
-  
-  AtomicReaderContext(AtomicReader atomicReader) {
-    this(null, atomicReader, 0, 0, 0, 0);
-  }
-  
-  @Override
-  public List<AtomicReaderContext> leaves() {
-    if (!isTopLevel) {
-      throw new UnsupportedOperationException("This is not a top-level context.");
-    }
-    assert leaves != null;
-    return leaves;
-  }
-  
-  @Override
-  public List<IndexReaderContext> children() {
-    return null;
-  }
-  
-  @Override
-  public AtomicReader reader() {
-    return reader;
-  }
-}
\ No newline at end of file
diff --git lucene/core/src/java/org/apache/lucene/index/BufferedUpdatesStream.java lucene/core/src/java/org/apache/lucene/index/BufferedUpdatesStream.java
index 10aaf73..143ecf9 100644
--- lucene/core/src/java/org/apache/lucene/index/BufferedUpdatesStream.java
+++ lucene/core/src/java/org/apache/lucene/index/BufferedUpdatesStream.java
@@ -542,7 +542,7 @@ class BufferedUpdatesStream implements Accountable {
   // Delete by query
   private static long applyQueryDeletes(Iterable<QueryAndLimit> queriesIter, ReadersAndUpdates rld, final SegmentReader reader) throws IOException {
     long delCount = 0;
-    final AtomicReaderContext readerContext = reader.getContext();
+    final LeafReaderContext readerContext = reader.getContext();
     boolean any = false;
     for (QueryAndLimit ent : queriesIter) {
       Query query = ent.query;
diff --git lucene/core/src/java/org/apache/lucene/index/CheckIndex.java lucene/core/src/java/org/apache/lucene/index/CheckIndex.java
index 52178f7..2a47003 100644
--- lucene/core/src/java/org/apache/lucene/index/CheckIndex.java
+++ lucene/core/src/java/org/apache/lucene/index/CheckIndex.java
@@ -712,7 +712,7 @@ public class CheckIndex {
    * Test field norms.
    * @lucene.experimental
    */
-  public static Status.FieldNormStatus testFieldNorms(AtomicReader reader, PrintStream infoStream, boolean failFast) throws IOException {
+  public static Status.FieldNormStatus testFieldNorms(LeafReader reader, PrintStream infoStream, boolean failFast) throws IOException {
     final Status.FieldNormStatus status = new Status.FieldNormStatus();
 
     try {
@@ -1306,7 +1306,7 @@ public class CheckIndex {
    * Test the term index.
    * @lucene.experimental
    */
-  public static Status.TermIndexStatus testPostings(AtomicReader reader, PrintStream infoStream) throws IOException {
+  public static Status.TermIndexStatus testPostings(LeafReader reader, PrintStream infoStream) throws IOException {
     return testPostings(reader, infoStream, false, false);
   }
   
@@ -1314,7 +1314,7 @@ public class CheckIndex {
    * Test the term index.
    * @lucene.experimental
    */
-  public static Status.TermIndexStatus testPostings(AtomicReader reader, PrintStream infoStream, boolean verbose, boolean failFast) throws IOException {
+  public static Status.TermIndexStatus testPostings(LeafReader reader, PrintStream infoStream, boolean verbose, boolean failFast) throws IOException {
 
     // TODO: we should go and verify term vectors match, if
     // crossCheckTermVectors is on...
@@ -1356,7 +1356,7 @@ public class CheckIndex {
    * Test stored fields.
    * @lucene.experimental
    */
-  public static Status.StoredFieldStatus testStoredFields(AtomicReader reader, PrintStream infoStream, boolean failFast) throws IOException {
+  public static Status.StoredFieldStatus testStoredFields(LeafReader reader, PrintStream infoStream, boolean failFast) throws IOException {
     final Status.StoredFieldStatus status = new Status.StoredFieldStatus();
 
     try {
@@ -1401,7 +1401,7 @@ public class CheckIndex {
    * Test docvalues.
    * @lucene.experimental
    */
-  public static Status.DocValuesStatus testDocValues(AtomicReader reader,
+  public static Status.DocValuesStatus testDocValues(LeafReader reader,
                                                      PrintStream infoStream,
                                                      boolean failFast) throws IOException {
     final Status.DocValuesStatus status = new Status.DocValuesStatus();
@@ -1443,7 +1443,7 @@ public class CheckIndex {
     return status;
   }
   
-  private static void checkBinaryDocValues(String fieldName, AtomicReader reader, BinaryDocValues dv, Bits docsWithField) {
+  private static void checkBinaryDocValues(String fieldName, LeafReader reader, BinaryDocValues dv, Bits docsWithField) {
     for (int i = 0; i < reader.maxDoc(); i++) {
       final BytesRef term = dv.get(i);
       assert term.isValid();
@@ -1453,7 +1453,7 @@ public class CheckIndex {
     }
   }
   
-  private static void checkSortedDocValues(String fieldName, AtomicReader reader, SortedDocValues dv, Bits docsWithField) {
+  private static void checkSortedDocValues(String fieldName, LeafReader reader, SortedDocValues dv, Bits docsWithField) {
     checkBinaryDocValues(fieldName, reader, dv, docsWithField);
     final int maxOrd = dv.getValueCount()-1;
     FixedBitSet seenOrds = new FixedBitSet(dv.getValueCount());
@@ -1493,7 +1493,7 @@ public class CheckIndex {
     }
   }
   
-  private static void checkSortedSetDocValues(String fieldName, AtomicReader reader, SortedSetDocValues dv, Bits docsWithField) {
+  private static void checkSortedSetDocValues(String fieldName, LeafReader reader, SortedSetDocValues dv, Bits docsWithField) {
     final long maxOrd = dv.getValueCount()-1;
     LongBitSet seenOrds = new LongBitSet(dv.getValueCount());
     long maxOrd2 = -1;
@@ -1563,7 +1563,7 @@ public class CheckIndex {
     }
   }
   
-  private static void checkSortedNumericDocValues(String fieldName, AtomicReader reader, SortedNumericDocValues ndv, Bits docsWithField) {
+  private static void checkSortedNumericDocValues(String fieldName, LeafReader reader, SortedNumericDocValues ndv, Bits docsWithField) {
     for (int i = 0; i < reader.maxDoc(); i++) {
       ndv.setDocument(i);
       int count = ndv.count();
@@ -1587,7 +1587,7 @@ public class CheckIndex {
     }
   }
 
-  private static void checkNumericDocValues(String fieldName, AtomicReader reader, NumericDocValues ndv, Bits docsWithField) {
+  private static void checkNumericDocValues(String fieldName, LeafReader reader, NumericDocValues ndv, Bits docsWithField) {
     for (int i = 0; i < reader.maxDoc(); i++) {
       long value = ndv.get(i);
       if (docsWithField.get(i) == false && value != 0) {
@@ -1596,7 +1596,7 @@ public class CheckIndex {
     }
   }
   
-  private static void checkDocValues(FieldInfo fi, AtomicReader reader, PrintStream infoStream, DocValuesStatus status) throws Exception {
+  private static void checkDocValues(FieldInfo fi, LeafReader reader, PrintStream infoStream, DocValuesStatus status) throws Exception {
     Bits docsWithField = reader.getDocsWithField(fi.name);
     if (docsWithField == null) {
       throw new RuntimeException(fi.name + " docsWithField does not exist");
@@ -1659,7 +1659,7 @@ public class CheckIndex {
     }
   }
   
-  private static void checkNorms(FieldInfo fi, AtomicReader reader, PrintStream infoStream) throws IOException {
+  private static void checkNorms(FieldInfo fi, LeafReader reader, PrintStream infoStream) throws IOException {
     switch(fi.getNormType()) {
       case NUMERIC:
         checkNumericDocValues(fi.name, reader, reader.getNormValues(fi.name), new Bits.MatchAllBits(reader.maxDoc()));
@@ -1673,7 +1673,7 @@ public class CheckIndex {
    * Test term vectors.
    * @lucene.experimental
    */
-  public static Status.TermVectorStatus testTermVectors(AtomicReader reader, PrintStream infoStream) throws IOException {
+  public static Status.TermVectorStatus testTermVectors(LeafReader reader, PrintStream infoStream) throws IOException {
     return testTermVectors(reader, infoStream, false, false, false);
   }
 
@@ -1681,7 +1681,7 @@ public class CheckIndex {
    * Test term vectors.
    * @lucene.experimental
    */
-  public static Status.TermVectorStatus testTermVectors(AtomicReader reader, PrintStream infoStream, boolean verbose, boolean crossCheckTermVectors, boolean failFast) throws IOException {
+  public static Status.TermVectorStatus testTermVectors(LeafReader reader, PrintStream infoStream, boolean verbose, boolean crossCheckTermVectors, boolean failFast) throws IOException {
     final Status.TermVectorStatus status = new Status.TermVectorStatus();
     final FieldInfos fieldInfos = reader.getFieldInfos();
     final Bits onlyDocIsDeleted = new FixedBitSet(1);
diff --git lucene/core/src/java/org/apache/lucene/index/CompositeReader.java lucene/core/src/java/org/apache/lucene/index/CompositeReader.java
index 8265da7..5516a3e 100644
--- lucene/core/src/java/org/apache/lucene/index/CompositeReader.java
+++ lucene/core/src/java/org/apache/lucene/index/CompositeReader.java
@@ -19,15 +19,14 @@ package org.apache.lucene.index;
 
 import java.util.List;
 
-import org.apache.lucene.search.SearcherManager; // javadocs
 import org.apache.lucene.store.*;
 
 /**
  Instances of this reader type can only
  be used to get stored fields from the underlying AtomicReaders,
  but it is not possible to directly retrieve postings. To do that, get
- the {@link AtomicReaderContext} for all sub-readers via {@link #leaves()}.
- Alternatively, you can mimic an {@link AtomicReader} (with a serious slowdown),
+ the {@link LeafReaderContext} for all sub-readers via {@link #leaves()}.
+ Alternatively, you can mimic an {@link LeafReader} (with a serious slowdown),
  by wrapping composite readers with {@link SlowCompositeReaderWrapper}.
  
  <p>IndexReader instances for indexes on disk are usually constructed
@@ -91,7 +90,7 @@ public abstract class CompositeReader extends IndexReader {
    *  return {@code null}.
    *  
    *  <p><b>NOTE:</b> In contrast to previous Lucene versions this method
-   *  is no longer public, code that wants to get all {@link AtomicReader}s
+   *  is no longer public, code that wants to get all {@link LeafReader}s
    *  this composite is composed of should use {@link IndexReader#leaves()}.
    * @see IndexReader#leaves()
    */
diff --git lucene/core/src/java/org/apache/lucene/index/CompositeReaderContext.java lucene/core/src/java/org/apache/lucene/index/CompositeReaderContext.java
index d892e18..bc38eb3 100644
--- lucene/core/src/java/org/apache/lucene/index/CompositeReaderContext.java
+++ lucene/core/src/java/org/apache/lucene/index/CompositeReaderContext.java
@@ -27,7 +27,7 @@ import java.util.List;
  */
 public final class CompositeReaderContext extends IndexReaderContext {
   private final List<IndexReaderContext> children;
-  private final List<AtomicReaderContext> leaves;
+  private final List<LeafReaderContext> leaves;
   private final CompositeReader reader;
   
   static CompositeReaderContext create(CompositeReader reader) {
@@ -46,13 +46,13 @@ public final class CompositeReaderContext extends IndexReaderContext {
   /**
    * Creates a {@link CompositeReaderContext} for top-level readers with parent set to <code>null</code>
    */
-  CompositeReaderContext(CompositeReader reader, List<IndexReaderContext> children, List<AtomicReaderContext> leaves) {
+  CompositeReaderContext(CompositeReader reader, List<IndexReaderContext> children, List<LeafReaderContext> leaves) {
     this(null, reader, 0, 0, children, leaves);
   }
   
   private CompositeReaderContext(CompositeReaderContext parent, CompositeReader reader,
       int ordInParent, int docbaseInParent, List<IndexReaderContext> children,
-      List<AtomicReaderContext> leaves) {
+      List<LeafReaderContext> leaves) {
     super(parent, ordInParent, docbaseInParent);
     this.children = Collections.unmodifiableList(children);
     this.leaves = leaves == null ? null : Collections.unmodifiableList(leaves);
@@ -60,7 +60,7 @@ public final class CompositeReaderContext extends IndexReaderContext {
   }
 
   @Override
-  public List<AtomicReaderContext> leaves() throws UnsupportedOperationException {
+  public List<LeafReaderContext> leaves() throws UnsupportedOperationException {
     if (!isTopLevel)
       throw new UnsupportedOperationException("This is not a top-level context.");
     assert leaves != null;
@@ -80,7 +80,7 @@ public final class CompositeReaderContext extends IndexReaderContext {
   
   private static final class Builder {
     private final CompositeReader reader;
-    private final List<AtomicReaderContext> leaves = new ArrayList<>();
+    private final List<LeafReaderContext> leaves = new ArrayList<>();
     private int leafDocBase = 0;
     
     public Builder(CompositeReader reader) {
@@ -92,9 +92,9 @@ public final class CompositeReaderContext extends IndexReaderContext {
     }
     
     private IndexReaderContext build(CompositeReaderContext parent, IndexReader reader, int ord, int docBase) {
-      if (reader instanceof AtomicReader) {
-        final AtomicReader ar = (AtomicReader) reader;
-        final AtomicReaderContext atomic = new AtomicReaderContext(parent, ar, ord, docBase, leaves.size(), leafDocBase);
+      if (reader instanceof LeafReader) {
+        final LeafReader ar = (LeafReader) reader;
+        final LeafReaderContext atomic = new LeafReaderContext(parent, ar, ord, docBase, leaves.size(), leafDocBase);
         leaves.add(atomic);
         leafDocBase += reader.maxDoc();
         return atomic;
diff --git lucene/core/src/java/org/apache/lucene/index/DirectoryReader.java lucene/core/src/java/org/apache/lucene/index/DirectoryReader.java
index 083714e..fb2ae1d 100644
--- lucene/core/src/java/org/apache/lucene/index/DirectoryReader.java
+++ lucene/core/src/java/org/apache/lucene/index/DirectoryReader.java
@@ -49,7 +49,7 @@ import org.apache.lucene.store.Directory;
  <code>IndexReader</code> instance; use your own
  (non-Lucene) objects instead.
 */
-public abstract class DirectoryReader extends BaseCompositeReader<AtomicReader> {
+public abstract class DirectoryReader extends BaseCompositeReader<LeafReader> {
 
   /** The index directory. */
   protected final Directory directory;
@@ -307,7 +307,7 @@ public abstract class DirectoryReader extends BaseCompositeReader<AtomicReader>
    * Subclasses of {@code DirectoryReader} should take care to not allow
    * modification of this internal array, e.g. {@link #doOpenIfChanged()}.
    */
-  protected DirectoryReader(Directory directory, AtomicReader[] segmentReaders) {
+  protected DirectoryReader(Directory directory, LeafReader[] segmentReaders) {
     super(segmentReaders);
     this.directory = directory;
   }
diff --git lucene/core/src/java/org/apache/lucene/index/DocValues.java lucene/core/src/java/org/apache/lucene/index/DocValues.java
index 00b49cc..fa7e87a 100644
--- lucene/core/src/java/org/apache/lucene/index/DocValues.java
+++ lucene/core/src/java/org/apache/lucene/index/DocValues.java
@@ -202,7 +202,7 @@ public final class DocValues {
   /**
    * Returns NumericDocValues for the reader, or {@link #emptyNumeric()} if it has none. 
    */
-  public static NumericDocValues getNumeric(AtomicReader in, String field) throws IOException {
+  public static NumericDocValues getNumeric(LeafReader in, String field) throws IOException {
     NumericDocValues dv = in.getNumericDocValues(field);
     if (dv == null) {
       return emptyNumeric();
@@ -214,7 +214,7 @@ public final class DocValues {
   /**
    * Returns BinaryDocValues for the reader, or {@link #emptyBinary} if it has none. 
    */
-  public static BinaryDocValues getBinary(AtomicReader in, String field) throws IOException {
+  public static BinaryDocValues getBinary(LeafReader in, String field) throws IOException {
     BinaryDocValues dv = in.getBinaryDocValues(field);
     if (dv == null) {
       dv = in.getSortedDocValues(field);
@@ -228,7 +228,7 @@ public final class DocValues {
   /**
    * Returns SortedDocValues for the reader, or {@link #emptySorted} if it has none. 
    */
-  public static SortedDocValues getSorted(AtomicReader in, String field) throws IOException {
+  public static SortedDocValues getSorted(LeafReader in, String field) throws IOException {
     SortedDocValues dv = in.getSortedDocValues(field);
     if (dv == null) {
       return emptySorted();
@@ -240,7 +240,7 @@ public final class DocValues {
   /**
    * Returns SortedNumericDocValues for the reader, or {@link #emptySortedNumeric} if it has none. 
    */
-  public static SortedNumericDocValues getSortedNumeric(AtomicReader in, String field) throws IOException {
+  public static SortedNumericDocValues getSortedNumeric(LeafReader in, String field) throws IOException {
     SortedNumericDocValues dv = in.getSortedNumericDocValues(field);
     if (dv == null) {
       NumericDocValues single = in.getNumericDocValues(field);
@@ -256,7 +256,7 @@ public final class DocValues {
   /**
    * Returns SortedSetDocValues for the reader, or {@link #emptySortedSet} if it has none. 
    */
-  public static SortedSetDocValues getSortedSet(AtomicReader in, String field) throws IOException {
+  public static SortedSetDocValues getSortedSet(LeafReader in, String field) throws IOException {
     SortedSetDocValues dv = in.getSortedSetDocValues(field);
     if (dv == null) {
       SortedDocValues sorted = in.getSortedDocValues(field);
@@ -271,7 +271,7 @@ public final class DocValues {
   /**
    * Returns Bits for the reader, or {@link Bits} matching nothing if it has none. 
    */
-  public static Bits getDocsWithField(AtomicReader in, String field) throws IOException {
+  public static Bits getDocsWithField(LeafReader in, String field) throws IOException {
     Bits dv = in.getDocsWithField(field);
     if (dv == null) {
       return new Bits.MatchNoBits(in.maxDoc());
diff --git lucene/core/src/java/org/apache/lucene/index/FilterAtomicReader.java lucene/core/src/java/org/apache/lucene/index/FilterAtomicReader.java
deleted file mode 100644
index 52f542d..0000000
--- lucene/core/src/java/org/apache/lucene/index/FilterAtomicReader.java
+++ /dev/null
@@ -1,446 +0,0 @@
-package org.apache.lucene.index;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Iterator;
-
-import org.apache.lucene.search.CachingWrapperFilter;
-import org.apache.lucene.util.AttributeSource;
-import org.apache.lucene.util.Bits;
-import org.apache.lucene.util.BytesRef;
-
-/**  A <code>FilterAtomicReader</code> contains another AtomicReader, which it
- * uses as its basic source of data, possibly transforming the data along the
- * way or providing additional functionality. The class
- * <code>FilterAtomicReader</code> itself simply implements all abstract methods
- * of <code>IndexReader</code> with versions that pass all requests to the
- * contained index reader. Subclasses of <code>FilterAtomicReader</code> may
- * further override some of these methods and may also provide additional
- * methods and fields.
- * <p><b>NOTE</b>: If you override {@link #getLiveDocs()}, you will likely need
- * to override {@link #numDocs()} as well and vice-versa.
- * <p><b>NOTE</b>: If this {@link FilterAtomicReader} does not change the
- * content the contained reader, you could consider overriding
- * {@link #getCoreCacheKey()} so that
- * {@link CachingWrapperFilter} shares the same entries for this atomic reader
- * and the wrapped one. {@link #getCombinedCoreAndDeletesKey()} could be
- * overridden as well if the {@link #getLiveDocs() live docs} are not changed
- * either.
- */
-public class FilterAtomicReader extends AtomicReader {
-
-  /** Get the wrapped instance by <code>reader</code> as long as this reader is
-   *  an intance of {@link FilterAtomicReader}.  */
-  public static AtomicReader unwrap(AtomicReader reader) {
-    while (reader instanceof FilterAtomicReader) {
-      reader = ((FilterAtomicReader) reader).in;
-    }
-    return reader;
-  }
-
-  /** Base class for filtering {@link Fields}
-   *  implementations. */
-  public static class FilterFields extends Fields {
-    /** The underlying Fields instance. */
-    protected final Fields in;
-
-    /**
-     * Creates a new FilterFields.
-     * @param in the underlying Fields instance.
-     */
-    public FilterFields(Fields in) {
-      this.in = in;
-    }
-
-    @Override
-    public Iterator<String> iterator() {
-      return in.iterator();
-    }
-
-    @Override
-    public Terms terms(String field) throws IOException {
-      return in.terms(field);
-    }
-
-    @Override
-    public int size() {
-      return in.size();
-    }
-  }
-
-  /** Base class for filtering {@link Terms} implementations.
-   * <p><b>NOTE</b>: If the order of terms and documents is not changed, and if
-   * these terms are going to be intersected with automata, you could consider
-   * overriding {@link #intersect} for better performance.
-   */
-  public static class FilterTerms extends Terms {
-    /** The underlying Terms instance. */
-    protected final Terms in;
-
-    /**
-     * Creates a new FilterTerms
-     * @param in the underlying Terms instance.
-     */
-    public FilterTerms(Terms in) {
-      this.in = in;
-    }
-
-    @Override
-    public TermsEnum iterator(TermsEnum reuse) throws IOException {
-      return in.iterator(reuse);
-    }
-
-    @Override
-    public long size() throws IOException {
-      return in.size();
-    }
-
-    @Override
-    public long getSumTotalTermFreq() throws IOException {
-      return in.getSumTotalTermFreq();
-    }
-
-    @Override
-    public long getSumDocFreq() throws IOException {
-      return in.getSumDocFreq();
-    }
-
-    @Override
-    public int getDocCount() throws IOException {
-      return in.getDocCount();
-    }
-
-    @Override
-    public boolean hasFreqs() {
-      return in.hasFreqs();
-    }
-
-    @Override
-    public boolean hasOffsets() {
-      return in.hasOffsets();
-    }
-
-    @Override
-    public boolean hasPositions() {
-      return in.hasPositions();
-    }
-    
-    @Override
-    public boolean hasPayloads() {
-      return in.hasPayloads();
-    }
-  }
-
-  /** Base class for filtering {@link TermsEnum} implementations. */
-  public static class FilterTermsEnum extends TermsEnum {
-    /** The underlying TermsEnum instance. */
-    protected final TermsEnum in;
-
-    /**
-     * Creates a new FilterTermsEnum
-     * @param in the underlying TermsEnum instance.
-     */
-    public FilterTermsEnum(TermsEnum in) { this.in = in; }
-
-    @Override
-    public AttributeSource attributes() {
-      return in.attributes();
-    }
-
-    @Override
-    public SeekStatus seekCeil(BytesRef text) throws IOException {
-      return in.seekCeil(text);
-    }
-
-    @Override
-    public void seekExact(long ord) throws IOException {
-      in.seekExact(ord);
-    }
-
-    @Override
-    public BytesRef next() throws IOException {
-      return in.next();
-    }
-
-    @Override
-    public BytesRef term() throws IOException {
-      return in.term();
-    }
-
-    @Override
-    public long ord() throws IOException {
-      return in.ord();
-    }
-
-    @Override
-    public int docFreq() throws IOException {
-      return in.docFreq();
-    }
-
-    @Override
-    public long totalTermFreq() throws IOException {
-      return in.totalTermFreq();
-    }
-
-    @Override
-    public DocsEnum docs(Bits liveDocs, DocsEnum reuse, int flags) throws IOException {
-      return in.docs(liveDocs, reuse, flags);
-    }
-
-    @Override
-    public DocsAndPositionsEnum docsAndPositions(Bits liveDocs, DocsAndPositionsEnum reuse, int flags) throws IOException {
-      return in.docsAndPositions(liveDocs, reuse, flags);
-    }
-  }
-
-  /** Base class for filtering {@link DocsEnum} implementations. */
-  public static class FilterDocsEnum extends DocsEnum {
-    /** The underlying DocsEnum instance. */
-    protected final DocsEnum in;
-
-    /**
-     * Create a new FilterDocsEnum
-     * @param in the underlying DocsEnum instance.
-     */
-    public FilterDocsEnum(DocsEnum in) {
-      this.in = in;
-    }
-
-    @Override
-    public AttributeSource attributes() {
-      return in.attributes();
-    }
-
-    @Override
-    public int docID() {
-      return in.docID();
-    }
-
-    @Override
-    public int freq() throws IOException {
-      return in.freq();
-    }
-
-    @Override
-    public int nextDoc() throws IOException {
-      return in.nextDoc();
-    }
-
-    @Override
-    public int advance(int target) throws IOException {
-      return in.advance(target);
-    }
-
-    @Override
-    public long cost() {
-      return in.cost();
-    }
-  }
-
-  /** Base class for filtering {@link DocsAndPositionsEnum} implementations. */
-  public static class FilterDocsAndPositionsEnum extends DocsAndPositionsEnum {
-    /** The underlying DocsAndPositionsEnum instance. */
-    protected final DocsAndPositionsEnum in;
-
-    /**
-     * Create a new FilterDocsAndPositionsEnum
-     * @param in the underlying DocsAndPositionsEnum instance.
-     */
-    public FilterDocsAndPositionsEnum(DocsAndPositionsEnum in) {
-      this.in = in;
-    }
-
-    @Override
-    public AttributeSource attributes() {
-      return in.attributes();
-    }
-
-    @Override
-    public int docID() {
-      return in.docID();
-    }
-
-    @Override
-    public int freq() throws IOException {
-      return in.freq();
-    }
-
-    @Override
-    public int nextDoc() throws IOException {
-      return in.nextDoc();
-    }
-
-    @Override
-    public int advance(int target) throws IOException {
-      return in.advance(target);
-    }
-
-    @Override
-    public int nextPosition() throws IOException {
-      return in.nextPosition();
-    }
-
-    @Override
-    public int startOffset() throws IOException {
-      return in.startOffset();
-    }
-
-    @Override
-    public int endOffset() throws IOException {
-      return in.endOffset();
-    }
-
-    @Override
-    public BytesRef getPayload() throws IOException {
-      return in.getPayload();
-    }
-    
-    @Override
-    public long cost() {
-      return in.cost();
-    }
-  }
-
-  /** The underlying AtomicReader. */
-  protected final AtomicReader in;
-
-  /**
-   * <p>Construct a FilterAtomicReader based on the specified base reader.
-   * <p>Note that base reader is closed if this FilterAtomicReader is closed.</p>
-   * @param in specified base reader.
-   */
-  public FilterAtomicReader(AtomicReader in) {
-    super();
-    this.in = in;
-    in.registerParentReader(this);
-  }
-
-  @Override
-  public void addCoreClosedListener(CoreClosedListener listener) {
-    in.addCoreClosedListener(listener);
-  }
-
-  @Override
-  public void removeCoreClosedListener(CoreClosedListener listener) {
-    in.removeCoreClosedListener(listener);
-  }
-
-  @Override
-  public Bits getLiveDocs() {
-    ensureOpen();
-    return in.getLiveDocs();
-  }
-  
-  @Override
-  public FieldInfos getFieldInfos() {
-    return in.getFieldInfos();
-  }
-
-  @Override
-  public Fields getTermVectors(int docID)
-          throws IOException {
-    ensureOpen();
-    return in.getTermVectors(docID);
-  }
-
-  @Override
-  public int numDocs() {
-    // Don't call ensureOpen() here (it could affect performance)
-    return in.numDocs();
-  }
-
-  @Override
-  public int maxDoc() {
-    // Don't call ensureOpen() here (it could affect performance)
-    return in.maxDoc();
-  }
-
-  @Override
-  public void document(int docID, StoredFieldVisitor visitor) throws IOException {
-    ensureOpen();
-    in.document(docID, visitor);
-  }
-
-  @Override
-  protected void doClose() throws IOException {
-    in.close();
-  }
-  
-  @Override
-  public Fields fields() throws IOException {
-    ensureOpen();
-    return in.fields();
-  }
-
-  @Override
-  public String toString() {
-    final StringBuilder buffer = new StringBuilder("FilterAtomicReader(");
-    buffer.append(in);
-    buffer.append(')');
-    return buffer.toString();
-  }
-
-  @Override
-  public NumericDocValues getNumericDocValues(String field) throws IOException {
-    ensureOpen();
-    return in.getNumericDocValues(field);
-  }
-  
-  @Override
-  public BinaryDocValues getBinaryDocValues(String field) throws IOException {
-    ensureOpen();
-    return in.getBinaryDocValues(field);
-  }
-
-  @Override
-  public SortedDocValues getSortedDocValues(String field) throws IOException {
-    ensureOpen();
-    return in.getSortedDocValues(field);
-  }
-  
-  @Override
-  public SortedNumericDocValues getSortedNumericDocValues(String field) throws IOException {
-    ensureOpen();
-    return in.getSortedNumericDocValues(field);
-  }
-
-  @Override
-  public SortedSetDocValues getSortedSetDocValues(String field) throws IOException {
-    ensureOpen();
-    return in.getSortedSetDocValues(field);
-  }
-
-  @Override
-  public NumericDocValues getNormValues(String field) throws IOException {
-    ensureOpen();
-    return in.getNormValues(field);
-  }
-
-  @Override
-  public Bits getDocsWithField(String field) throws IOException {
-    ensureOpen();
-    return in.getDocsWithField(field);
-  }
-
-  @Override
-  public void checkIntegrity() throws IOException {
-    ensureOpen();
-    in.checkIntegrity();
-  }
-}
diff --git lucene/core/src/java/org/apache/lucene/index/FilterDirectoryReader.java lucene/core/src/java/org/apache/lucene/index/FilterDirectoryReader.java
index 3cf0d60..091bc21 100644
--- lucene/core/src/java/org/apache/lucene/index/FilterDirectoryReader.java
+++ lucene/core/src/java/org/apache/lucene/index/FilterDirectoryReader.java
@@ -41,8 +41,8 @@ public abstract class FilterDirectoryReader extends DirectoryReader {
    */
   public static abstract class SubReaderWrapper {
 
-    private AtomicReader[] wrap(List<? extends AtomicReader> readers) {
-      AtomicReader[] wrapped = new AtomicReader[readers.size()];
+    private LeafReader[] wrap(List<? extends LeafReader> readers) {
+      LeafReader[] wrapped = new LeafReader[readers.size()];
       for (int i = 0; i < readers.size(); i++) {
         wrapped[i] = wrap(readers.get(i));
       }
@@ -57,7 +57,7 @@ public abstract class FilterDirectoryReader extends DirectoryReader {
      * @param reader the subreader to wrap
      * @return a wrapped/filtered AtomicReader
      */
-    public abstract AtomicReader wrap(AtomicReader reader);
+    public abstract LeafReader wrap(LeafReader reader);
 
   }
 
@@ -71,7 +71,7 @@ public abstract class FilterDirectoryReader extends DirectoryReader {
     public StandardReaderWrapper() {}
 
     @Override
-    public AtomicReader wrap(AtomicReader reader) {
+    public LeafReader wrap(LeafReader reader) {
       return reader;
     }
   }
diff --git lucene/core/src/java/org/apache/lucene/index/FilterLeafReader.java lucene/core/src/java/org/apache/lucene/index/FilterLeafReader.java
new file mode 100644
index 0000000..822bda2
--- /dev/null
+++ lucene/core/src/java/org/apache/lucene/index/FilterLeafReader.java
@@ -0,0 +1,446 @@
+package org.apache.lucene.index;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Iterator;
+
+import org.apache.lucene.search.CachingWrapperFilter;
+import org.apache.lucene.util.AttributeSource;
+import org.apache.lucene.util.Bits;
+import org.apache.lucene.util.BytesRef;
+
+/**  A <code>FilterAtomicReader</code> contains another AtomicReader, which it
+ * uses as its basic source of data, possibly transforming the data along the
+ * way or providing additional functionality. The class
+ * <code>FilterAtomicReader</code> itself simply implements all abstract methods
+ * of <code>IndexReader</code> with versions that pass all requests to the
+ * contained index reader. Subclasses of <code>FilterAtomicReader</code> may
+ * further override some of these methods and may also provide additional
+ * methods and fields.
+ * <p><b>NOTE</b>: If you override {@link #getLiveDocs()}, you will likely need
+ * to override {@link #numDocs()} as well and vice-versa.
+ * <p><b>NOTE</b>: If this {@link FilterLeafReader} does not change the
+ * content the contained reader, you could consider overriding
+ * {@link #getCoreCacheKey()} so that
+ * {@link CachingWrapperFilter} shares the same entries for this atomic reader
+ * and the wrapped one. {@link #getCombinedCoreAndDeletesKey()} could be
+ * overridden as well if the {@link #getLiveDocs() live docs} are not changed
+ * either.
+ */
+public class FilterLeafReader extends LeafReader {
+
+  /** Get the wrapped instance by <code>reader</code> as long as this reader is
+   *  an intance of {@link FilterLeafReader}.  */
+  public static LeafReader unwrap(LeafReader reader) {
+    while (reader instanceof FilterLeafReader) {
+      reader = ((FilterLeafReader) reader).in;
+    }
+    return reader;
+  }
+
+  /** Base class for filtering {@link Fields}
+   *  implementations. */
+  public static class FilterFields extends Fields {
+    /** The underlying Fields instance. */
+    protected final Fields in;
+
+    /**
+     * Creates a new FilterFields.
+     * @param in the underlying Fields instance.
+     */
+    public FilterFields(Fields in) {
+      this.in = in;
+    }
+
+    @Override
+    public Iterator<String> iterator() {
+      return in.iterator();
+    }
+
+    @Override
+    public Terms terms(String field) throws IOException {
+      return in.terms(field);
+    }
+
+    @Override
+    public int size() {
+      return in.size();
+    }
+  }
+
+  /** Base class for filtering {@link Terms} implementations.
+   * <p><b>NOTE</b>: If the order of terms and documents is not changed, and if
+   * these terms are going to be intersected with automata, you could consider
+   * overriding {@link #intersect} for better performance.
+   */
+  public static class FilterTerms extends Terms {
+    /** The underlying Terms instance. */
+    protected final Terms in;
+
+    /**
+     * Creates a new FilterTerms
+     * @param in the underlying Terms instance.
+     */
+    public FilterTerms(Terms in) {
+      this.in = in;
+    }
+
+    @Override
+    public TermsEnum iterator(TermsEnum reuse) throws IOException {
+      return in.iterator(reuse);
+    }
+
+    @Override
+    public long size() throws IOException {
+      return in.size();
+    }
+
+    @Override
+    public long getSumTotalTermFreq() throws IOException {
+      return in.getSumTotalTermFreq();
+    }
+
+    @Override
+    public long getSumDocFreq() throws IOException {
+      return in.getSumDocFreq();
+    }
+
+    @Override
+    public int getDocCount() throws IOException {
+      return in.getDocCount();
+    }
+
+    @Override
+    public boolean hasFreqs() {
+      return in.hasFreqs();
+    }
+
+    @Override
+    public boolean hasOffsets() {
+      return in.hasOffsets();
+    }
+
+    @Override
+    public boolean hasPositions() {
+      return in.hasPositions();
+    }
+    
+    @Override
+    public boolean hasPayloads() {
+      return in.hasPayloads();
+    }
+  }
+
+  /** Base class for filtering {@link TermsEnum} implementations. */
+  public static class FilterTermsEnum extends TermsEnum {
+    /** The underlying TermsEnum instance. */
+    protected final TermsEnum in;
+
+    /**
+     * Creates a new FilterTermsEnum
+     * @param in the underlying TermsEnum instance.
+     */
+    public FilterTermsEnum(TermsEnum in) { this.in = in; }
+
+    @Override
+    public AttributeSource attributes() {
+      return in.attributes();
+    }
+
+    @Override
+    public SeekStatus seekCeil(BytesRef text) throws IOException {
+      return in.seekCeil(text);
+    }
+
+    @Override
+    public void seekExact(long ord) throws IOException {
+      in.seekExact(ord);
+    }
+
+    @Override
+    public BytesRef next() throws IOException {
+      return in.next();
+    }
+
+    @Override
+    public BytesRef term() throws IOException {
+      return in.term();
+    }
+
+    @Override
+    public long ord() throws IOException {
+      return in.ord();
+    }
+
+    @Override
+    public int docFreq() throws IOException {
+      return in.docFreq();
+    }
+
+    @Override
+    public long totalTermFreq() throws IOException {
+      return in.totalTermFreq();
+    }
+
+    @Override
+    public DocsEnum docs(Bits liveDocs, DocsEnum reuse, int flags) throws IOException {
+      return in.docs(liveDocs, reuse, flags);
+    }
+
+    @Override
+    public DocsAndPositionsEnum docsAndPositions(Bits liveDocs, DocsAndPositionsEnum reuse, int flags) throws IOException {
+      return in.docsAndPositions(liveDocs, reuse, flags);
+    }
+  }
+
+  /** Base class for filtering {@link DocsEnum} implementations. */
+  public static class FilterDocsEnum extends DocsEnum {
+    /** The underlying DocsEnum instance. */
+    protected final DocsEnum in;
+
+    /**
+     * Create a new FilterDocsEnum
+     * @param in the underlying DocsEnum instance.
+     */
+    public FilterDocsEnum(DocsEnum in) {
+      this.in = in;
+    }
+
+    @Override
+    public AttributeSource attributes() {
+      return in.attributes();
+    }
+
+    @Override
+    public int docID() {
+      return in.docID();
+    }
+
+    @Override
+    public int freq() throws IOException {
+      return in.freq();
+    }
+
+    @Override
+    public int nextDoc() throws IOException {
+      return in.nextDoc();
+    }
+
+    @Override
+    public int advance(int target) throws IOException {
+      return in.advance(target);
+    }
+
+    @Override
+    public long cost() {
+      return in.cost();
+    }
+  }
+
+  /** Base class for filtering {@link DocsAndPositionsEnum} implementations. */
+  public static class FilterDocsAndPositionsEnum extends DocsAndPositionsEnum {
+    /** The underlying DocsAndPositionsEnum instance. */
+    protected final DocsAndPositionsEnum in;
+
+    /**
+     * Create a new FilterDocsAndPositionsEnum
+     * @param in the underlying DocsAndPositionsEnum instance.
+     */
+    public FilterDocsAndPositionsEnum(DocsAndPositionsEnum in) {
+      this.in = in;
+    }
+
+    @Override
+    public AttributeSource attributes() {
+      return in.attributes();
+    }
+
+    @Override
+    public int docID() {
+      return in.docID();
+    }
+
+    @Override
+    public int freq() throws IOException {
+      return in.freq();
+    }
+
+    @Override
+    public int nextDoc() throws IOException {
+      return in.nextDoc();
+    }
+
+    @Override
+    public int advance(int target) throws IOException {
+      return in.advance(target);
+    }
+
+    @Override
+    public int nextPosition() throws IOException {
+      return in.nextPosition();
+    }
+
+    @Override
+    public int startOffset() throws IOException {
+      return in.startOffset();
+    }
+
+    @Override
+    public int endOffset() throws IOException {
+      return in.endOffset();
+    }
+
+    @Override
+    public BytesRef getPayload() throws IOException {
+      return in.getPayload();
+    }
+    
+    @Override
+    public long cost() {
+      return in.cost();
+    }
+  }
+
+  /** The underlying AtomicReader. */
+  protected final LeafReader in;
+
+  /**
+   * <p>Construct a FilterAtomicReader based on the specified base reader.
+   * <p>Note that base reader is closed if this FilterAtomicReader is closed.</p>
+   * @param in specified base reader.
+   */
+  public FilterLeafReader(LeafReader in) {
+    super();
+    this.in = in;
+    in.registerParentReader(this);
+  }
+
+  @Override
+  public void addCoreClosedListener(CoreClosedListener listener) {
+    in.addCoreClosedListener(listener);
+  }
+
+  @Override
+  public void removeCoreClosedListener(CoreClosedListener listener) {
+    in.removeCoreClosedListener(listener);
+  }
+
+  @Override
+  public Bits getLiveDocs() {
+    ensureOpen();
+    return in.getLiveDocs();
+  }
+  
+  @Override
+  public FieldInfos getFieldInfos() {
+    return in.getFieldInfos();
+  }
+
+  @Override
+  public Fields getTermVectors(int docID)
+          throws IOException {
+    ensureOpen();
+    return in.getTermVectors(docID);
+  }
+
+  @Override
+  public int numDocs() {
+    // Don't call ensureOpen() here (it could affect performance)
+    return in.numDocs();
+  }
+
+  @Override
+  public int maxDoc() {
+    // Don't call ensureOpen() here (it could affect performance)
+    return in.maxDoc();
+  }
+
+  @Override
+  public void document(int docID, StoredFieldVisitor visitor) throws IOException {
+    ensureOpen();
+    in.document(docID, visitor);
+  }
+
+  @Override
+  protected void doClose() throws IOException {
+    in.close();
+  }
+  
+  @Override
+  public Fields fields() throws IOException {
+    ensureOpen();
+    return in.fields();
+  }
+
+  @Override
+  public String toString() {
+    final StringBuilder buffer = new StringBuilder("FilterAtomicReader(");
+    buffer.append(in);
+    buffer.append(')');
+    return buffer.toString();
+  }
+
+  @Override
+  public NumericDocValues getNumericDocValues(String field) throws IOException {
+    ensureOpen();
+    return in.getNumericDocValues(field);
+  }
+  
+  @Override
+  public BinaryDocValues getBinaryDocValues(String field) throws IOException {
+    ensureOpen();
+    return in.getBinaryDocValues(field);
+  }
+
+  @Override
+  public SortedDocValues getSortedDocValues(String field) throws IOException {
+    ensureOpen();
+    return in.getSortedDocValues(field);
+  }
+  
+  @Override
+  public SortedNumericDocValues getSortedNumericDocValues(String field) throws IOException {
+    ensureOpen();
+    return in.getSortedNumericDocValues(field);
+  }
+
+  @Override
+  public SortedSetDocValues getSortedSetDocValues(String field) throws IOException {
+    ensureOpen();
+    return in.getSortedSetDocValues(field);
+  }
+
+  @Override
+  public NumericDocValues getNormValues(String field) throws IOException {
+    ensureOpen();
+    return in.getNormValues(field);
+  }
+
+  @Override
+  public Bits getDocsWithField(String field) throws IOException {
+    ensureOpen();
+    return in.getDocsWithField(field);
+  }
+
+  @Override
+  public void checkIntegrity() throws IOException {
+    ensureOpen();
+    in.checkIntegrity();
+  }
+}
diff --git lucene/core/src/java/org/apache/lucene/index/IndexReader.java lucene/core/src/java/org/apache/lucene/index/IndexReader.java
index 6d1c5bc..6378974 100644
--- lucene/core/src/java/org/apache/lucene/index/IndexReader.java
+++ lucene/core/src/java/org/apache/lucene/index/IndexReader.java
@@ -48,7 +48,7 @@ import java.util.concurrent.atomic.AtomicInteger;
 
  <p>There are two different types of IndexReaders:
  <ul>
-  <li>{@link AtomicReader}: These indexes do not consist of several sub-readers,
+  <li>{@link LeafReader}: These indexes do not consist of several sub-readers,
   they are atomic. They support retrieval of stored fields, doc values, terms,
   and postings.
   <li>{@link CompositeReader}: Instances (like {@link DirectoryReader})
@@ -56,7 +56,7 @@ import java.util.concurrent.atomic.AtomicInteger;
   be used to get stored fields from the underlying AtomicReaders,
   but it is not possible to directly retrieve postings. To do that, get
   the sub-readers via {@link CompositeReader#getSequentialSubReaders}.
-  Alternatively, you can mimic an {@link AtomicReader} (with a serious slowdown),
+  Alternatively, you can mimic an {@link LeafReader} (with a serious slowdown),
   by wrapping composite readers with {@link SlowCompositeReaderWrapper}.
  </ul>
  
@@ -87,7 +87,7 @@ public abstract class IndexReader implements Closeable {
   private final AtomicInteger refCount = new AtomicInteger(1);
 
   IndexReader() {
-    if (!(this instanceof CompositeReader || this instanceof AtomicReader))
+    if (!(this instanceof CompositeReader || this instanceof LeafReader))
       throw new Error("IndexReader should never be directly extended, subclass AtomicReader or CompositeReader instead.");
   }
   
@@ -128,7 +128,7 @@ public abstract class IndexReader implements Closeable {
   }
   
   /** Expert: This method is called by {@code IndexReader}s which wrap other readers
-   * (e.g. {@link CompositeReader} or {@link FilterAtomicReader}) to register the parent
+   * (e.g. {@link CompositeReader} or {@link FilterLeafReader}) to register the parent
    * at the child (this reader) on construction of the parent. When this reader is closed,
    * it will mark all registered parents as closed, too. The references to parent readers
    * are weak only, so they can be GCed once they are no longer in use.
@@ -420,7 +420,7 @@ public abstract class IndexReader implements Closeable {
    * context are private to this reader and are not shared with another context
    * tree. For example, IndexSearcher uses this API to drive searching by one
    * atomic leaf reader at a time. If this reader is not composed of child
-   * readers, this method returns an {@link AtomicReaderContext}.
+   * readers, this method returns an {@link LeafReaderContext}.
    * <p>
    * Note: Any of the sub-{@link CompositeReaderContext} instances referenced
    * from this top-level context do not support {@link CompositeReaderContext#leaves()}.
@@ -434,7 +434,7 @@ public abstract class IndexReader implements Closeable {
    * This is a convenience method calling {@code this.getContext().leaves()}.
    * @see IndexReaderContext#leaves()
    */
-  public final List<AtomicReaderContext> leaves() {
+  public final List<LeafReaderContext> leaves() {
     return getContext().leaves();
   }
 
diff --git lucene/core/src/java/org/apache/lucene/index/IndexReaderContext.java lucene/core/src/java/org/apache/lucene/index/IndexReaderContext.java
index 49e4a8e..a13abff 100644
--- lucene/core/src/java/org/apache/lucene/index/IndexReaderContext.java
+++ lucene/core/src/java/org/apache/lucene/index/IndexReaderContext.java
@@ -34,7 +34,7 @@ public abstract class IndexReaderContext {
   public final int ordInParent;
   
   IndexReaderContext(CompositeReaderContext parent, int ordInParent, int docBaseInParent) {
-    if (!(this instanceof CompositeReaderContext || this instanceof AtomicReaderContext))
+    if (!(this instanceof CompositeReaderContext || this instanceof LeafReaderContext))
       throw new Error("This class should never be extended by custom code!");
     this.parent = parent;
     this.docBaseInParent = docBaseInParent;
@@ -47,14 +47,14 @@ public abstract class IndexReaderContext {
   
   /**
    * Returns the context's leaves if this context is a top-level context.
-   * For convenience, if this is an {@link AtomicReaderContext} this
+   * For convenience, if this is an {@link LeafReaderContext} this
    * returns itself as the only leaf.
    * <p>Note: this is convenience method since leaves can always be obtained by
    * walking the context tree using {@link #children()}.
    * @throws UnsupportedOperationException if this is not a top-level context.
    * @see #children()
    */
-  public abstract List<AtomicReaderContext> leaves() throws UnsupportedOperationException;
+  public abstract List<LeafReaderContext> leaves() throws UnsupportedOperationException;
   
   /**
    * Returns the context's children iff this context is a composite context
diff --git lucene/core/src/java/org/apache/lucene/index/IndexWriter.java lucene/core/src/java/org/apache/lucene/index/IndexWriter.java
index 8479391..8cf6761 100644
--- lucene/core/src/java/org/apache/lucene/index/IndexWriter.java
+++ lucene/core/src/java/org/apache/lucene/index/IndexWriter.java
@@ -1224,13 +1224,13 @@ public class IndexWriter implements Closeable, TwoPhaseCommit, Accountable {
    *  reader you must use {@link #deleteDocuments(Term...)}). */
   public synchronized boolean tryDeleteDocument(IndexReader readerIn, int docID) throws IOException {
 
-    final AtomicReader reader;
-    if (readerIn instanceof AtomicReader) {
+    final LeafReader reader;
+    if (readerIn instanceof LeafReader) {
       // Reader is already atomic: use the incoming docID:
-      reader = (AtomicReader) readerIn;
+      reader = (LeafReader) readerIn;
     } else {
       // Composite reader: lookup sub-reader and re-base docID:
-      List<AtomicReaderContext> leaves = readerIn.leaves();
+      List<LeafReaderContext> leaves = readerIn.leaves();
       int subIndex = ReaderUtil.subIndex(docID, leaves);
       reader = leaves.get(subIndex).reader();
       docID -= leaves.get(subIndex).docBase;
@@ -2481,10 +2481,10 @@ public class IndexWriter implements Closeable, TwoPhaseCommit, Accountable {
       flush(false, true);
 
       String mergedName = newSegmentName();
-      final List<AtomicReader> mergeReaders = new ArrayList<>();
+      final List<LeafReader> mergeReaders = new ArrayList<>();
       for (IndexReader indexReader : readers) {
         numDocs += indexReader.numDocs();
-        for (AtomicReaderContext ctx : indexReader.leaves()) {
+        for (LeafReaderContext ctx : indexReader.leaves()) {
           mergeReaders.add(ctx.reader());
         }
       }
@@ -3938,7 +3938,7 @@ public class IndexWriter implements Closeable, TwoPhaseCommit, Accountable {
       try {
         if (!merger.shouldMerge()) {
           // would result in a 0 document segment: nothing to merge!
-          mergeState = new MergeState(new ArrayList<AtomicReader>(), merge.info.info, infoStream, checkAbort);
+          mergeState = new MergeState(new ArrayList<LeafReader>(), merge.info.info, infoStream, checkAbort);
         } else {
           mergeState = merger.merge();
         }
@@ -4368,10 +4368,10 @@ public class IndexWriter implements Closeable, TwoPhaseCommit, Accountable {
     protected IndexReaderWarmer() {
     }
 
-    /** Invoked on the {@link AtomicReader} for the newly
+    /** Invoked on the {@link LeafReader} for the newly
      *  merged segment, before that segment is made visible
      *  to near-real-time readers. */
-    public abstract void warm(AtomicReader reader) throws IOException;
+    public abstract void warm(LeafReader reader) throws IOException;
   }
 
   private void tragicEvent(Throwable tragedy, String location) {
diff --git lucene/core/src/java/org/apache/lucene/index/IndexWriterConfig.java lucene/core/src/java/org/apache/lucene/index/IndexWriterConfig.java
index 3b647b3..ece9cd7 100644
--- lucene/core/src/java/org/apache/lucene/index/IndexWriterConfig.java
+++ lucene/core/src/java/org/apache/lucene/index/IndexWriterConfig.java
@@ -110,7 +110,7 @@ public final class IndexWriterConfig extends LiveIndexWriterConfig {
    *  ram buffers use <code>false</code> */
   public final static boolean DEFAULT_USE_COMPOUND_FILE_SYSTEM = true;
   
-  /** Default value for calling {@link AtomicReader#checkIntegrity()} before
+  /** Default value for calling {@link LeafReader#checkIntegrity()} before
    *  merging segments (set to <code>false</code>). You can set this
    *  to <code>true</code> for additional safety. */
   public final static boolean DEFAULT_CHECK_INTEGRITY_AT_MERGE = false;
diff --git lucene/core/src/java/org/apache/lucene/index/LeafReader.java lucene/core/src/java/org/apache/lucene/index/LeafReader.java
new file mode 100644
index 0000000..ba09d17
--- /dev/null
+++ lucene/core/src/java/org/apache/lucene/index/LeafReader.java
@@ -0,0 +1,329 @@
+package org.apache.lucene.index;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.util.Bits;
+
+/** {@code AtomicReader} is an abstract class, providing an interface for accessing an
+ index.  Search of an index is done entirely through this abstract interface,
+ so that any subclass which implements it is searchable. IndexReaders implemented
+ by this subclass do not consist of several sub-readers,
+ they are atomic. They support retrieval of stored fields, doc values, terms,
+ and postings.
+
+ <p>For efficiency, in this API documents are often referred to via
+ <i>document numbers</i>, non-negative integers which each name a unique
+ document in the index.  These document numbers are ephemeral -- they may change
+ as documents are added to and deleted from an index.  Clients should thus not
+ rely on a given document having the same number between sessions.
+
+ <p>
+ <a name="thread-safety"></a><p><b>NOTE</b>: {@link
+ IndexReader} instances are completely thread
+ safe, meaning multiple threads can call any of its methods,
+ concurrently.  If your application requires external
+ synchronization, you should <b>not</b> synchronize on the
+ <code>IndexReader</code> instance; use your own
+ (non-Lucene) objects instead.
+*/
+public abstract class LeafReader extends IndexReader {
+
+  private final LeafReaderContext readerContext = new LeafReaderContext(this);
+
+  /** Sole constructor. (For invocation by subclass
+   *  constructors, typically implicit.) */
+  protected LeafReader() {
+    super();
+  }
+
+  @Override
+  public final LeafReaderContext getContext() {
+    ensureOpen();
+    return readerContext;
+  }
+
+  /**
+   * Called when the shared core for this {@link LeafReader}
+   * is closed.
+   * <p>
+   * If this {@link LeafReader} impl has the ability to share
+   * resources across instances that might only vary through
+   * deleted documents and doc values updates, then this listener
+   * will only be called when the shared core is closed.
+   * Otherwise, this listener will be called when this reader is
+   * closed.</p>
+   * <p>
+   * This is typically useful to manage per-segment caches: when
+   * the listener is called, it is safe to evict this reader from
+   * any caches keyed on {@link #getCoreCacheKey}.</p>
+   *
+   * @lucene.experimental
+   */
+  public static interface CoreClosedListener {
+    /** Invoked when the shared core of the original {@code
+     *  SegmentReader} has closed. */
+    public void onClose(Object ownerCoreCacheKey);
+  }
+
+  private static class CoreClosedListenerWrapper implements ReaderClosedListener {
+
+    private final CoreClosedListener listener;
+
+    CoreClosedListenerWrapper(CoreClosedListener listener) {
+      this.listener = listener;
+    }
+
+    @Override
+    public void onClose(IndexReader reader) {
+      listener.onClose(reader.getCoreCacheKey());
+    }
+
+    @Override
+    public int hashCode() {
+      return listener.hashCode();
+    }
+
+    @Override
+    public boolean equals(Object other) {
+      if (!(other instanceof CoreClosedListenerWrapper)) {
+        return false;
+      }
+      return listener.equals(((CoreClosedListenerWrapper) other).listener);
+    }
+
+  }
+
+  /** Add a {@link CoreClosedListener} as a {@link ReaderClosedListener}. This
+   * method is typically useful for {@link LeafReader} implementations that
+   * don't have the concept of a core that is shared across several
+   * {@link LeafReader} instances in which case the {@link CoreClosedListener}
+   * is called when closing the reader. */
+  protected static void addCoreClosedListenerAsReaderClosedListener(IndexReader reader, CoreClosedListener listener) {
+    reader.addReaderClosedListener(new CoreClosedListenerWrapper(listener));
+  }
+
+  /** Remove a {@link CoreClosedListener} which has been added with
+   * {@link #addCoreClosedListenerAsReaderClosedListener(IndexReader, CoreClosedListener)}. */
+  protected static void removeCoreClosedListenerAsReaderClosedListener(IndexReader reader, CoreClosedListener listener) {
+    reader.removeReaderClosedListener(new CoreClosedListenerWrapper(listener));
+  }
+
+  /** Expert: adds a CoreClosedListener to this reader's shared core
+   *  @lucene.experimental */
+  public abstract void addCoreClosedListener(CoreClosedListener listener);
+
+  /** Expert: removes a CoreClosedListener from this reader's shared core
+   *  @lucene.experimental */
+  public abstract void removeCoreClosedListener(CoreClosedListener listener);
+
+  /**
+   * Returns {@link Fields} for this reader.
+   * This method may return null if the reader has no
+   * postings.
+   */
+  public abstract Fields fields() throws IOException;
+
+  @Override
+  public final int docFreq(Term term) throws IOException {
+    final Fields fields = fields();
+    if (fields == null) {
+      return 0;
+    }
+    final Terms terms = fields.terms(term.field());
+    if (terms == null) {
+      return 0;
+    }
+    final TermsEnum termsEnum = terms.iterator(null);
+    if (termsEnum.seekExact(term.bytes())) {
+      return termsEnum.docFreq();
+    } else {
+      return 0;
+    }
+  }
+
+  /** Returns the number of documents containing the term
+   * <code>t</code>.  This method returns 0 if the term or
+   * field does not exists.  This method does not take into
+   * account deleted documents that have not yet been merged
+   * away. */
+  @Override
+  public final long totalTermFreq(Term term) throws IOException {
+    final Fields fields = fields();
+    if (fields == null) {
+      return 0;
+    }
+    final Terms terms = fields.terms(term.field());
+    if (terms == null) {
+      return 0;
+    }
+    final TermsEnum termsEnum = terms.iterator(null);
+    if (termsEnum.seekExact(term.bytes())) {
+      return termsEnum.totalTermFreq();
+    } else {
+      return 0;
+    }
+  }
+
+  @Override
+  public final long getSumDocFreq(String field) throws IOException {
+    final Terms terms = terms(field);
+    if (terms == null) {
+      return 0;
+    }
+    return terms.getSumDocFreq();
+  }
+
+  @Override
+  public final int getDocCount(String field) throws IOException {
+    final Terms terms = terms(field);
+    if (terms == null) {
+      return 0;
+    }
+    return terms.getDocCount();
+  }
+
+  @Override
+  public final long getSumTotalTermFreq(String field) throws IOException {
+    final Terms terms = terms(field);
+    if (terms == null) {
+      return 0;
+    }
+    return terms.getSumTotalTermFreq();
+  }
+
+  /** This may return null if the field does not exist.*/
+  public final Terms terms(String field) throws IOException {
+    final Fields fields = fields();
+    if (fields == null) {
+      return null;
+    }
+    return fields.terms(field);
+  }
+
+  /** Returns {@link DocsEnum} for the specified term.
+   *  This will return null if either the field or
+   *  term does not exist.
+   *  @see TermsEnum#docs(Bits, DocsEnum) */
+  public final DocsEnum termDocsEnum(Term term) throws IOException {
+    assert term.field() != null;
+    assert term.bytes() != null;
+    final Fields fields = fields();
+    if (fields != null) {
+      final Terms terms = fields.terms(term.field());
+      if (terms != null) {
+        final TermsEnum termsEnum = terms.iterator(null);
+        if (termsEnum.seekExact(term.bytes())) {
+          return termsEnum.docs(getLiveDocs(), null);
+        }
+      }
+    }
+    return null;
+  }
+
+  /** Returns {@link DocsAndPositionsEnum} for the specified
+   *  term.  This will return null if the
+   *  field or term does not exist or positions weren't indexed.
+   *  @see TermsEnum#docsAndPositions(Bits, DocsAndPositionsEnum) */
+  public final DocsAndPositionsEnum termPositionsEnum(Term term) throws IOException {
+    assert term.field() != null;
+    assert term.bytes() != null;
+    final Fields fields = fields();
+    if (fields != null) {
+      final Terms terms = fields.terms(term.field());
+      if (terms != null) {
+        final TermsEnum termsEnum = terms.iterator(null);
+        if (termsEnum.seekExact(term.bytes())) {
+          return termsEnum.docsAndPositions(getLiveDocs(), null);
+        }
+      }
+    }
+    return null;
+  }
+
+  /** Returns {@link NumericDocValues} for this field, or
+   *  null if no {@link NumericDocValues} were indexed for
+   *  this field.  The returned instance should only be
+   *  used by a single thread. */
+  public abstract NumericDocValues getNumericDocValues(String field) throws IOException;
+
+  /** Returns {@link BinaryDocValues} for this field, or
+   *  null if no {@link BinaryDocValues} were indexed for
+   *  this field.  The returned instance should only be
+   *  used by a single thread. */
+  public abstract BinaryDocValues getBinaryDocValues(String field) throws IOException;
+
+  /** Returns {@link SortedDocValues} for this field, or
+   *  null if no {@link SortedDocValues} were indexed for
+   *  this field.  The returned instance should only be
+   *  used by a single thread. */
+  public abstract SortedDocValues getSortedDocValues(String field) throws IOException;
+  
+  /** Returns {@link SortedNumericDocValues} for this field, or
+   *  null if no {@link SortedNumericDocValues} were indexed for
+   *  this field.  The returned instance should only be
+   *  used by a single thread. */
+  public abstract SortedNumericDocValues getSortedNumericDocValues(String field) throws IOException;
+
+  /** Returns {@link SortedSetDocValues} for this field, or
+   *  null if no {@link SortedSetDocValues} were indexed for
+   *  this field.  The returned instance should only be
+   *  used by a single thread. */
+  public abstract SortedSetDocValues getSortedSetDocValues(String field) throws IOException;
+
+  /** Returns a {@link Bits} at the size of <code>reader.maxDoc()</code>,
+   *  with turned on bits for each docid that does have a value for this field,
+   *  or null if no DocValues were indexed for this field. The
+   *  returned instance should only be used by a single thread */
+  public abstract Bits getDocsWithField(String field) throws IOException;
+
+  /** Returns {@link NumericDocValues} representing norms
+   *  for this field, or null if no {@link NumericDocValues}
+   *  were indexed. The returned instance should only be
+   *  used by a single thread. */
+  public abstract NumericDocValues getNormValues(String field) throws IOException;
+
+  /**
+   * Get the {@link FieldInfos} describing all fields in
+   * this reader.
+   * @lucene.experimental
+   */
+  public abstract FieldInfos getFieldInfos();
+
+  /** Returns the {@link Bits} representing live (not
+   *  deleted) docs.  A set bit indicates the doc ID has not
+   *  been deleted.  If this method returns null it means
+   *  there are no deleted documents (all documents are
+   *  live).
+   *
+   *  The returned instance has been safely published for
+   *  use by multiple threads without additional
+   *  synchronization.
+   */
+  public abstract Bits getLiveDocs();
+
+  /**
+   * Checks consistency of this reader.
+   * <p>
+   * Note that this may be costly in terms of I/O, e.g.
+   * may involve computing a checksum value against large data files.
+   * @lucene.internal
+   */
+  public abstract void checkIntegrity() throws IOException;
+}
diff --git lucene/core/src/java/org/apache/lucene/index/LeafReaderContext.java lucene/core/src/java/org/apache/lucene/index/LeafReaderContext.java
new file mode 100644
index 0000000..5571ea2
--- /dev/null
+++ lucene/core/src/java/org/apache/lucene/index/LeafReaderContext.java
@@ -0,0 +1,69 @@
+package org.apache.lucene.index;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.util.Collections;
+import java.util.List;
+
+/**
+ * {@link IndexReaderContext} for {@link LeafReader} instances.
+ */
+public final class LeafReaderContext extends IndexReaderContext {
+  /** The readers ord in the top-level's leaves array */
+  public final int ord;
+  /** The readers absolute doc base */
+  public final int docBase;
+  
+  private final LeafReader reader;
+  private final List<LeafReaderContext> leaves;
+  
+  /**
+   * Creates a new {@link LeafReaderContext} 
+   */    
+  LeafReaderContext(CompositeReaderContext parent, LeafReader reader,
+                    int ord, int docBase, int leafOrd, int leafDocBase) {
+    super(parent, ord, docBase);
+    this.ord = leafOrd;
+    this.docBase = leafDocBase;
+    this.reader = reader;
+    this.leaves = isTopLevel ? Collections.singletonList(this) : null;
+  }
+  
+  LeafReaderContext(LeafReader leafReader) {
+    this(null, leafReader, 0, 0, 0, 0);
+  }
+  
+  @Override
+  public List<LeafReaderContext> leaves() {
+    if (!isTopLevel) {
+      throw new UnsupportedOperationException("This is not a top-level context.");
+    }
+    assert leaves != null;
+    return leaves;
+  }
+  
+  @Override
+  public List<IndexReaderContext> children() {
+    return null;
+  }
+  
+  @Override
+  public LeafReader reader() {
+    return reader;
+  }
+}
\ No newline at end of file
diff --git lucene/core/src/java/org/apache/lucene/index/LiveIndexWriterConfig.java lucene/core/src/java/org/apache/lucene/index/LiveIndexWriterConfig.java
index 4a19ecc..5d39075 100644
--- lucene/core/src/java/org/apache/lucene/index/LiveIndexWriterConfig.java
+++ lucene/core/src/java/org/apache/lucene/index/LiveIndexWriterConfig.java
@@ -464,7 +464,7 @@ public class LiveIndexWriterConfig {
   }
   
   /**
-   * Sets if {@link IndexWriter} should call {@link AtomicReader#checkIntegrity()}
+   * Sets if {@link IndexWriter} should call {@link LeafReader#checkIntegrity()}
    * on existing segments before merging them into a new one.
    * <p>
    * Use <code>true</code> to enable this safety check, which can help
@@ -477,7 +477,7 @@ public class LiveIndexWriterConfig {
     return this;
   }
   
-  /** Returns true if {@link AtomicReader#checkIntegrity()} is called before 
+  /** Returns true if {@link LeafReader#checkIntegrity()} is called before 
    *  merging segments. */
   public boolean getCheckIntegrityAtMerge() {
     return checkIntegrityAtMerge;
diff --git lucene/core/src/java/org/apache/lucene/index/MappedMultiFields.java lucene/core/src/java/org/apache/lucene/index/MappedMultiFields.java
index 597164b..fad0eed 100644
--- lucene/core/src/java/org/apache/lucene/index/MappedMultiFields.java
+++ lucene/core/src/java/org/apache/lucene/index/MappedMultiFields.java
@@ -21,9 +21,9 @@ import java.io.IOException;
 
 import org.apache.lucene.util.Bits;
 
-import static org.apache.lucene.index.FilterAtomicReader.FilterFields;
-import static org.apache.lucene.index.FilterAtomicReader.FilterTerms;
-import static org.apache.lucene.index.FilterAtomicReader.FilterTermsEnum;
+import static org.apache.lucene.index.FilterLeafReader.FilterFields;
+import static org.apache.lucene.index.FilterLeafReader.FilterTerms;
+import static org.apache.lucene.index.FilterLeafReader.FilterTermsEnum;
 
 /** A {@link Fields} implementation that merges multiple
  *  Fields into one, and maps around deleted documents.
diff --git lucene/core/src/java/org/apache/lucene/index/MergePolicy.java lucene/core/src/java/org/apache/lucene/index/MergePolicy.java
index 6dad9f4..50d81ad 100644
--- lucene/core/src/java/org/apache/lucene/index/MergePolicy.java
+++ lucene/core/src/java/org/apache/lucene/index/MergePolicy.java
@@ -134,12 +134,12 @@ public abstract class MergePolicy {
      *  reorders doc IDs, it must override {@link #getDocMap} too so that
      *  deletes that happened during the merge can be applied to the newly
      *  merged segment. */
-    public List<AtomicReader> getMergeReaders() throws IOException {
+    public List<LeafReader> getMergeReaders() throws IOException {
       if (readers == null) {
         throw new IllegalStateException("IndexWriter has not initialized readers from the segment infos yet");
       }
-      final List<AtomicReader> readers = new ArrayList<>(this.readers.size());
-      for (AtomicReader reader : this.readers) {
+      final List<LeafReader> readers = new ArrayList<>(this.readers.size());
+      for (LeafReader reader : this.readers) {
         if (reader.numDocs() > 0) {
           readers.add(reader);
         }
diff --git lucene/core/src/java/org/apache/lucene/index/MergeState.java lucene/core/src/java/org/apache/lucene/index/MergeState.java
index 673e45f..198de5d 100644
--- lucene/core/src/java/org/apache/lucene/index/MergeState.java
+++ lucene/core/src/java/org/apache/lucene/index/MergeState.java
@@ -59,7 +59,7 @@ public class MergeState {
 
     /** Creates a {@link DocMap} instance appropriate for
      *  this reader. */
-    public static DocMap build(AtomicReader reader) {
+    public static DocMap build(LeafReader reader) {
       final int maxDoc = reader.maxDoc();
       if (!reader.hasDeletions()) {
         return new NoDelDocMap(maxDoc);
@@ -137,7 +137,7 @@ public class MergeState {
   public FieldInfos fieldInfos;
 
   /** Readers being merged. */
-  public final List<AtomicReader> readers;
+  public final List<LeafReader> readers;
 
   /** Maps docIDs around deletions. */
   public DocMap[] docMaps;
@@ -157,7 +157,7 @@ public class MergeState {
   public int checkAbortCount;
 
   /** Sole constructor. */
-  MergeState(List<AtomicReader> readers, SegmentInfo segmentInfo, InfoStream infoStream, CheckAbort checkAbort) {
+  MergeState(List<LeafReader> readers, SegmentInfo segmentInfo, InfoStream infoStream, CheckAbort checkAbort) {
     this.readers = readers;
     this.segmentInfo = segmentInfo;
     this.infoStream = infoStream;
diff --git lucene/core/src/java/org/apache/lucene/index/MultiDocValues.java lucene/core/src/java/org/apache/lucene/index/MultiDocValues.java
index afc460a..4532105 100644
--- lucene/core/src/java/org/apache/lucene/index/MultiDocValues.java
+++ lucene/core/src/java/org/apache/lucene/index/MultiDocValues.java
@@ -57,11 +57,11 @@ public class MultiDocValues {
   /** Returns a NumericDocValues for a reader's norms (potentially merging on-the-fly).
    * <p>
    * This is a slow way to access normalization values. Instead, access them per-segment
-   * with {@link AtomicReader#getNormValues(String)}
+   * with {@link LeafReader#getNormValues(String)}
    * </p> 
    */
   public static NumericDocValues getNormValues(final IndexReader r, final String field) throws IOException {
-    final List<AtomicReaderContext> leaves = r.leaves();
+    final List<LeafReaderContext> leaves = r.leaves();
     final int size = leaves.size();
     if (size == 0) {
       return null;
@@ -77,7 +77,7 @@ public class MultiDocValues {
     final NumericDocValues[] values = new NumericDocValues[size];
     final int[] starts = new int[size+1];
     for (int i = 0; i < size; i++) {
-      AtomicReaderContext context = leaves.get(i);
+      LeafReaderContext context = leaves.get(i);
       NumericDocValues v = context.reader().getNormValues(field);
       if (v == null) {
         v = DocValues.emptyNumeric();
@@ -103,11 +103,11 @@ public class MultiDocValues {
   /** Returns a NumericDocValues for a reader's docvalues (potentially merging on-the-fly) 
    * <p>
    * This is a slow way to access numeric values. Instead, access them per-segment
-   * with {@link AtomicReader#getNumericDocValues(String)}
+   * with {@link LeafReader#getNumericDocValues(String)}
    * </p> 
    * */
   public static NumericDocValues getNumericValues(final IndexReader r, final String field) throws IOException {
-    final List<AtomicReaderContext> leaves = r.leaves();
+    final List<LeafReaderContext> leaves = r.leaves();
     final int size = leaves.size();
     if (size == 0) {
       return null;
@@ -119,7 +119,7 @@ public class MultiDocValues {
     final NumericDocValues[] values = new NumericDocValues[size];
     final int[] starts = new int[size+1];
     for (int i = 0; i < size; i++) {
-      AtomicReaderContext context = leaves.get(i);
+      LeafReaderContext context = leaves.get(i);
       NumericDocValues v = context.reader().getNumericDocValues(field);
       if (v == null) {
         v = DocValues.emptyNumeric();
@@ -147,11 +147,11 @@ public class MultiDocValues {
   /** Returns a Bits for a reader's docsWithField (potentially merging on-the-fly) 
    * <p>
    * This is a slow way to access this bitset. Instead, access them per-segment
-   * with {@link AtomicReader#getDocsWithField(String)}
+   * with {@link LeafReader#getDocsWithField(String)}
    * </p> 
    * */
   public static Bits getDocsWithField(final IndexReader r, final String field) throws IOException {
-    final List<AtomicReaderContext> leaves = r.leaves();
+    final List<LeafReaderContext> leaves = r.leaves();
     final int size = leaves.size();
     if (size == 0) {
       return null;
@@ -164,7 +164,7 @@ public class MultiDocValues {
     final Bits[] values = new Bits[size];
     final int[] starts = new int[size+1];
     for (int i = 0; i < size; i++) {
-      AtomicReaderContext context = leaves.get(i);
+      LeafReaderContext context = leaves.get(i);
       Bits v = context.reader().getDocsWithField(field);
       if (v == null) {
         v = new Bits.MatchNoBits(context.reader().maxDoc());
@@ -192,11 +192,11 @@ public class MultiDocValues {
   /** Returns a BinaryDocValues for a reader's docvalues (potentially merging on-the-fly)
    * <p>
    * This is a slow way to access binary values. Instead, access them per-segment
-   * with {@link AtomicReader#getBinaryDocValues(String)}
+   * with {@link LeafReader#getBinaryDocValues(String)}
    * </p>  
    */
   public static BinaryDocValues getBinaryValues(final IndexReader r, final String field) throws IOException {
-    final List<AtomicReaderContext> leaves = r.leaves();
+    final List<LeafReaderContext> leaves = r.leaves();
     final int size = leaves.size();
     
     if (size == 0) {
@@ -209,7 +209,7 @@ public class MultiDocValues {
     final BinaryDocValues[] values = new BinaryDocValues[size];
     final int[] starts = new int[size+1];
     for (int i = 0; i < size; i++) {
-      AtomicReaderContext context = leaves.get(i);
+      LeafReaderContext context = leaves.get(i);
       BinaryDocValues v = context.reader().getBinaryDocValues(field);
       if (v == null) {
         v = DocValues.emptyBinary();
@@ -237,11 +237,11 @@ public class MultiDocValues {
   /** Returns a SortedNumericDocValues for a reader's docvalues (potentially merging on-the-fly) 
    * <p>
    * This is a slow way to access sorted numeric values. Instead, access them per-segment
-   * with {@link AtomicReader#getSortedNumericDocValues(String)}
+   * with {@link LeafReader#getSortedNumericDocValues(String)}
    * </p> 
    * */
   public static SortedNumericDocValues getSortedNumericValues(final IndexReader r, final String field) throws IOException {
-    final List<AtomicReaderContext> leaves = r.leaves();
+    final List<LeafReaderContext> leaves = r.leaves();
     final int size = leaves.size();
     if (size == 0) {
       return null;
@@ -253,7 +253,7 @@ public class MultiDocValues {
     final SortedNumericDocValues[] values = new SortedNumericDocValues[size];
     final int[] starts = new int[size+1];
     for (int i = 0; i < size; i++) {
-      AtomicReaderContext context = leaves.get(i);
+      LeafReaderContext context = leaves.get(i);
       SortedNumericDocValues v = context.reader().getSortedNumericDocValues(field);
       if (v == null) {
         v = DocValues.emptySortedNumeric(context.reader().maxDoc());
@@ -294,11 +294,11 @@ public class MultiDocValues {
   /** Returns a SortedDocValues for a reader's docvalues (potentially doing extremely slow things).
    * <p>
    * This is an extremely slow way to access sorted values. Instead, access them per-segment
-   * with {@link AtomicReader#getSortedDocValues(String)}
+   * with {@link LeafReader#getSortedDocValues(String)}
    * </p>  
    */
   public static SortedDocValues getSortedValues(final IndexReader r, final String field) throws IOException {
-    final List<AtomicReaderContext> leaves = r.leaves();
+    final List<LeafReaderContext> leaves = r.leaves();
     final int size = leaves.size();
     
     if (size == 0) {
@@ -311,7 +311,7 @@ public class MultiDocValues {
     final SortedDocValues[] values = new SortedDocValues[size];
     final int[] starts = new int[size+1];
     for (int i = 0; i < size; i++) {
-      AtomicReaderContext context = leaves.get(i);
+      LeafReaderContext context = leaves.get(i);
       SortedDocValues v = context.reader().getSortedDocValues(field);
       if (v == null) {
         v = DocValues.emptySorted();
@@ -334,11 +334,11 @@ public class MultiDocValues {
   /** Returns a SortedSetDocValues for a reader's docvalues (potentially doing extremely slow things).
    * <p>
    * This is an extremely slow way to access sorted values. Instead, access them per-segment
-   * with {@link AtomicReader#getSortedSetDocValues(String)}
+   * with {@link LeafReader#getSortedSetDocValues(String)}
    * </p>  
    */
   public static SortedSetDocValues getSortedSetValues(final IndexReader r, final String field) throws IOException {
-    final List<AtomicReaderContext> leaves = r.leaves();
+    final List<LeafReaderContext> leaves = r.leaves();
     final int size = leaves.size();
     
     if (size == 0) {
@@ -351,7 +351,7 @@ public class MultiDocValues {
     final SortedSetDocValues[] values = new SortedSetDocValues[size];
     final int[] starts = new int[size+1];
     for (int i = 0; i < size; i++) {
-      AtomicReaderContext context = leaves.get(i);
+      LeafReaderContext context = leaves.get(i);
       SortedSetDocValues v = context.reader().getSortedSetDocValues(field);
       if (v == null) {
         v = DocValues.emptySortedSet();
diff --git lucene/core/src/java/org/apache/lucene/index/MultiFields.java lucene/core/src/java/org/apache/lucene/index/MultiFields.java
index c16738c..48fe362 100644
--- lucene/core/src/java/org/apache/lucene/index/MultiFields.java
+++ lucene/core/src/java/org/apache/lucene/index/MultiFields.java
@@ -60,7 +60,7 @@ public final class MultiFields extends Fields {
    *  It's better to get the sub-readers and iterate through them
    *  yourself. */
   public static Fields getFields(IndexReader reader) throws IOException {
-    final List<AtomicReaderContext> leaves = reader.leaves();
+    final List<LeafReaderContext> leaves = reader.leaves();
     switch (leaves.size()) {
       case 0:
         // no fields
@@ -71,8 +71,8 @@ public final class MultiFields extends Fields {
       default:
         final List<Fields> fields = new ArrayList<>();
         final List<ReaderSlice> slices = new ArrayList<>();
-        for (final AtomicReaderContext ctx : leaves) {
-          final AtomicReader r = ctx.reader();
+        for (final LeafReaderContext ctx : leaves) {
+          final LeafReader r = ctx.reader();
           final Fields f = r.fields();
           if (f != null) {
             fields.add(f);
@@ -101,7 +101,7 @@ public final class MultiFields extends Fields {
    *  yourself. */
   public static Bits getLiveDocs(IndexReader reader) {
     if (reader.hasDeletions()) {
-      final List<AtomicReaderContext> leaves = reader.leaves();
+      final List<LeafReaderContext> leaves = reader.leaves();
       final int size = leaves.size();
       assert size > 0 : "A reader with deletions must have at least one leave";
       if (size == 1) {
@@ -111,7 +111,7 @@ public final class MultiFields extends Fields {
       final int[] starts = new int[size + 1];
       for (int i = 0; i < size; i++) {
         // record all liveDocs, even if they are null
-        final AtomicReaderContext ctx = leaves.get(i);
+        final LeafReaderContext ctx = leaves.get(i);
         liveDocs[i] = ctx.reader().getLiveDocs();
         starts[i] = ctx.docBase;
       }
@@ -254,7 +254,7 @@ public final class MultiFields extends Fields {
    */
   public static FieldInfos getMergedFieldInfos(IndexReader reader) {
     final FieldInfos.Builder builder = new FieldInfos.Builder();
-    for(final AtomicReaderContext ctx : reader.leaves()) {
+    for(final LeafReaderContext ctx : reader.leaves()) {
       builder.add(ctx.reader().getFieldInfos());
     }
     return builder.finish();
diff --git lucene/core/src/java/org/apache/lucene/index/ParallelAtomicReader.java lucene/core/src/java/org/apache/lucene/index/ParallelAtomicReader.java
deleted file mode 100644
index 56b0602..0000000
--- lucene/core/src/java/org/apache/lucene/index/ParallelAtomicReader.java
+++ /dev/null
@@ -1,327 +0,0 @@
-package org.apache.lucene.index;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Collections;
-import java.util.IdentityHashMap;
-import java.util.Iterator;
-import java.util.Map;
-import java.util.Set;
-import java.util.SortedMap;
-import java.util.TreeMap;
-
-import org.apache.lucene.util.Bits;
-
-
-/** An {@link AtomicReader} which reads multiple, parallel indexes.  Each index
- * added must have the same number of documents, but typically each contains
- * different fields. Deletions are taken from the first reader.
- * Each document contains the union of the fields of all documents
- * with the same document number.  When searching, matches for a
- * query term are from the first index added that has the field.
- *
- * <p>This is useful, e.g., with collections that have large fields which
- * change rarely and small fields that change more frequently.  The smaller
- * fields may be re-indexed in a new index and both indexes may be searched
- * together.
- * 
- * <p><strong>Warning:</strong> It is up to you to make sure all indexes
- * are created and modified the same way. For example, if you add
- * documents to one index, you need to add the same documents in the
- * same order to the other indexes. <em>Failure to do so will result in
- * undefined behavior</em>.
- */
-public class ParallelAtomicReader extends AtomicReader {
-  private final FieldInfos fieldInfos;
-  private final ParallelFields fields = new ParallelFields();
-  private final AtomicReader[] parallelReaders, storedFieldsReaders;
-  private final Set<AtomicReader> completeReaderSet =
-    Collections.newSetFromMap(new IdentityHashMap<AtomicReader,Boolean>());
-  private final boolean closeSubReaders;
-  private final int maxDoc, numDocs;
-  private final boolean hasDeletions;
-  private final SortedMap<String,AtomicReader> fieldToReader = new TreeMap<>();
-  private final SortedMap<String,AtomicReader> tvFieldToReader = new TreeMap<>();
-  
-  /** Create a ParallelAtomicReader based on the provided
-   *  readers; auto-closes the given readers on {@link #close()}. */
-  public ParallelAtomicReader(AtomicReader... readers) throws IOException {
-    this(true, readers);
-  }
-
-  /** Create a ParallelAtomicReader based on the provided
-   *  readers. */
-  public ParallelAtomicReader(boolean closeSubReaders, AtomicReader... readers) throws IOException {
-    this(closeSubReaders, readers, readers);
-  }
-
-  /** Expert: create a ParallelAtomicReader based on the provided
-   *  readers and storedFieldReaders; when a document is
-   *  loaded, only storedFieldsReaders will be used. */
-  public ParallelAtomicReader(boolean closeSubReaders, AtomicReader[] readers, AtomicReader[] storedFieldsReaders) throws IOException {
-    this.closeSubReaders = closeSubReaders;
-    if (readers.length == 0 && storedFieldsReaders.length > 0)
-      throw new IllegalArgumentException("There must be at least one main reader if storedFieldsReaders are used.");
-    this.parallelReaders = readers.clone();
-    this.storedFieldsReaders = storedFieldsReaders.clone();
-    if (parallelReaders.length > 0) {
-      final AtomicReader first = parallelReaders[0];
-      this.maxDoc = first.maxDoc();
-      this.numDocs = first.numDocs();
-      this.hasDeletions = first.hasDeletions();
-    } else {
-      this.maxDoc = this.numDocs = 0;
-      this.hasDeletions = false;
-    }
-    Collections.addAll(completeReaderSet, this.parallelReaders);
-    Collections.addAll(completeReaderSet, this.storedFieldsReaders);
-    
-    // check compatibility:
-    for(AtomicReader reader : completeReaderSet) {
-      if (reader.maxDoc() != maxDoc) {
-        throw new IllegalArgumentException("All readers must have same maxDoc: "+maxDoc+"!="+reader.maxDoc());
-      }
-    }
-    
-    // TODO: make this read-only in a cleaner way?
-    FieldInfos.Builder builder = new FieldInfos.Builder();
-    // build FieldInfos and fieldToReader map:
-    for (final AtomicReader reader : this.parallelReaders) {
-      final FieldInfos readerFieldInfos = reader.getFieldInfos();
-      for (FieldInfo fieldInfo : readerFieldInfos) {
-        // NOTE: first reader having a given field "wins":
-        if (!fieldToReader.containsKey(fieldInfo.name)) {
-          builder.add(fieldInfo);
-          fieldToReader.put(fieldInfo.name, reader);
-          if (fieldInfo.hasVectors()) {
-            tvFieldToReader.put(fieldInfo.name, reader);
-          }
-        }
-      }
-    }
-    fieldInfos = builder.finish();
-    
-    // build Fields instance
-    for (final AtomicReader reader : this.parallelReaders) {
-      final Fields readerFields = reader.fields();
-      if (readerFields != null) {
-        for (String field : readerFields) {
-          // only add if the reader responsible for that field name is the current:
-          if (fieldToReader.get(field) == reader) {
-            this.fields.addField(field, readerFields.terms(field));
-          }
-        }
-      }
-    }
-
-    // do this finally so any Exceptions occurred before don't affect refcounts:
-    for (AtomicReader reader : completeReaderSet) {
-      if (!closeSubReaders) {
-        reader.incRef();
-      }
-      reader.registerParentReader(this);
-    }
-  }
-
-  @Override
-  public String toString() {
-    final StringBuilder buffer = new StringBuilder("ParallelAtomicReader(");
-    for (final Iterator<AtomicReader> iter = completeReaderSet.iterator(); iter.hasNext();) {
-      buffer.append(iter.next());
-      if (iter.hasNext()) buffer.append(", ");
-    }
-    return buffer.append(')').toString();
-  }
-
-  @Override
-  public void addCoreClosedListener(CoreClosedListener listener) {
-    addCoreClosedListenerAsReaderClosedListener(this, listener);
-  }
-
-  @Override
-  public void removeCoreClosedListener(CoreClosedListener listener) {
-    removeCoreClosedListenerAsReaderClosedListener(this, listener);
-  }
-
-  // Single instance of this, per ParallelReader instance
-  private final class ParallelFields extends Fields {
-    final Map<String,Terms> fields = new TreeMap<>();
-    
-    ParallelFields() {
-    }
-    
-    void addField(String fieldName, Terms terms) {
-      fields.put(fieldName, terms);
-    }
-    
-    @Override
-    public Iterator<String> iterator() {
-      return Collections.unmodifiableSet(fields.keySet()).iterator();
-    }
-    
-    @Override
-    public Terms terms(String field) {
-      return fields.get(field);
-    }
-    
-    @Override
-    public int size() {
-      return fields.size();
-    }
-  }
-  
-  /**
-   * {@inheritDoc}
-   * <p>
-   * NOTE: the returned field numbers will likely not
-   * correspond to the actual field numbers in the underlying
-   * readers, and codec metadata ({@link FieldInfo#getAttribute(String)}
-   * will be unavailable.
-   */
-  @Override
-  public FieldInfos getFieldInfos() {
-    return fieldInfos;
-  }
-  
-  @Override
-  public Bits getLiveDocs() {
-    ensureOpen();
-    return hasDeletions ? parallelReaders[0].getLiveDocs() : null;
-  }
-  
-  @Override
-  public Fields fields() {
-    ensureOpen();
-    return fields;
-  }
-  
-  @Override
-  public int numDocs() {
-    // Don't call ensureOpen() here (it could affect performance)
-    return numDocs;
-  }
-  
-  @Override
-  public int maxDoc() {
-    // Don't call ensureOpen() here (it could affect performance)
-    return maxDoc;
-  }
-  
-  @Override
-  public void document(int docID, StoredFieldVisitor visitor) throws IOException {
-    ensureOpen();
-    for (final AtomicReader reader: storedFieldsReaders) {
-      reader.document(docID, visitor);
-    }
-  }
-  
-  @Override
-  public Fields getTermVectors(int docID) throws IOException {
-    ensureOpen();
-    ParallelFields fields = null;
-    for (Map.Entry<String,AtomicReader> ent : tvFieldToReader.entrySet()) {
-      String fieldName = ent.getKey();
-      Terms vector = ent.getValue().getTermVector(docID, fieldName);
-      if (vector != null) {
-        if (fields == null) {
-          fields = new ParallelFields();
-        }
-        fields.addField(fieldName, vector);
-      }
-    }
-    
-    return fields;
-  }
-  
-  @Override
-  protected synchronized void doClose() throws IOException {
-    IOException ioe = null;
-    for (AtomicReader reader : completeReaderSet) {
-      try {
-        if (closeSubReaders) {
-          reader.close();
-        } else {
-          reader.decRef();
-        }
-      } catch (IOException e) {
-        if (ioe == null) ioe = e;
-      }
-    }
-    // throw the first exception
-    if (ioe != null) throw ioe;
-  }
-
-  @Override
-  public NumericDocValues getNumericDocValues(String field) throws IOException {
-    ensureOpen();
-    AtomicReader reader = fieldToReader.get(field);
-    return reader == null ? null : reader.getNumericDocValues(field);
-  }
-  
-  @Override
-  public BinaryDocValues getBinaryDocValues(String field) throws IOException {
-    ensureOpen();
-    AtomicReader reader = fieldToReader.get(field);
-    return reader == null ? null : reader.getBinaryDocValues(field);
-  }
-  
-  @Override
-  public SortedDocValues getSortedDocValues(String field) throws IOException {
-    ensureOpen();
-    AtomicReader reader = fieldToReader.get(field);
-    return reader == null ? null : reader.getSortedDocValues(field);
-  }
-  
-  @Override
-  public SortedNumericDocValues getSortedNumericDocValues(String field) throws IOException {
-    ensureOpen();
-    AtomicReader reader = fieldToReader.get(field);
-    return reader == null ? null : reader.getSortedNumericDocValues(field);
-  }
-
-  @Override
-  public SortedSetDocValues getSortedSetDocValues(String field) throws IOException {
-    ensureOpen();
-    AtomicReader reader = fieldToReader.get(field);
-    return reader == null ? null : reader.getSortedSetDocValues(field);
-  }
-
-  @Override
-  public Bits getDocsWithField(String field) throws IOException {
-    ensureOpen();
-    AtomicReader reader = fieldToReader.get(field);
-    return reader == null ? null : reader.getDocsWithField(field);
-  }
-
-  @Override
-  public NumericDocValues getNormValues(String field) throws IOException {
-    ensureOpen();
-    AtomicReader reader = fieldToReader.get(field);
-    NumericDocValues values = reader == null ? null : reader.getNormValues(field);
-    return values;
-  }
-
-  @Override
-  public void checkIntegrity() throws IOException {
-    ensureOpen();
-    for (AtomicReader reader : completeReaderSet) {
-      reader.checkIntegrity();
-    }
-  }
-}
diff --git lucene/core/src/java/org/apache/lucene/index/ParallelCompositeReader.java lucene/core/src/java/org/apache/lucene/index/ParallelCompositeReader.java
index 431beb8..f6905d4 100644
--- lucene/core/src/java/org/apache/lucene/index/ParallelCompositeReader.java
+++ lucene/core/src/java/org/apache/lucene/index/ParallelCompositeReader.java
@@ -96,7 +96,7 @@ public class ParallelCompositeReader extends BaseCompositeReader<IndexReader> {
       for (int i = 0; i < noSubs; i++) {
         final IndexReader r = firstSubReaders.get(i);
         childMaxDoc[i] = r.maxDoc();
-        childAtomic[i] = r instanceof AtomicReader;
+        childAtomic[i] = r instanceof LeafReader;
       }
       validate(readers, maxDoc, childMaxDoc, childAtomic);
       validate(storedFieldsReaders, maxDoc, childMaxDoc, childAtomic);
@@ -104,18 +104,18 @@ public class ParallelCompositeReader extends BaseCompositeReader<IndexReader> {
       // hierarchically build the same subreader structure as the first CompositeReader with Parallel*Readers:
       final IndexReader[] subReaders = new IndexReader[noSubs];
       for (int i = 0; i < subReaders.length; i++) {
-        if (firstSubReaders.get(i) instanceof AtomicReader) {
-          final AtomicReader[] atomicSubs = new AtomicReader[readers.length];
+        if (firstSubReaders.get(i) instanceof LeafReader) {
+          final LeafReader[] atomicSubs = new LeafReader[readers.length];
           for (int j = 0; j < readers.length; j++) {
-            atomicSubs[j] = (AtomicReader) readers[j].getSequentialSubReaders().get(i);
+            atomicSubs[j] = (LeafReader) readers[j].getSequentialSubReaders().get(i);
           }
-          final AtomicReader[] storedSubs = new AtomicReader[storedFieldsReaders.length];
+          final LeafReader[] storedSubs = new LeafReader[storedFieldsReaders.length];
           for (int j = 0; j < storedFieldsReaders.length; j++) {
-            storedSubs[j] = (AtomicReader) storedFieldsReaders[j].getSequentialSubReaders().get(i);
+            storedSubs[j] = (LeafReader) storedFieldsReaders[j].getSequentialSubReaders().get(i);
           }
           // We pass true for closeSubs and we prevent closing of subreaders in doClose():
           // By this the synthetic throw-away readers used here are completely invisible to ref-counting
-          subReaders[i] = new ParallelAtomicReader(true, atomicSubs, storedSubs) {
+          subReaders[i] = new ParallelLeafReader(true, atomicSubs, storedSubs) {
             @Override
             protected void doClose() {}
           };
@@ -157,7 +157,7 @@ public class ParallelCompositeReader extends BaseCompositeReader<IndexReader> {
         if (r.maxDoc() != childMaxDoc[subIDX]) {
           throw new IllegalArgumentException("All readers must have same corresponding subReader maxDoc");
         }
-        if (!(childAtomic[subIDX] ? (r instanceof AtomicReader) : (r instanceof CompositeReader))) {
+        if (!(childAtomic[subIDX] ? (r instanceof LeafReader) : (r instanceof CompositeReader))) {
           throw new IllegalArgumentException("All readers must have same corresponding subReader types (atomic or composite)");
         }
       }
diff --git lucene/core/src/java/org/apache/lucene/index/ParallelLeafReader.java lucene/core/src/java/org/apache/lucene/index/ParallelLeafReader.java
new file mode 100644
index 0000000..6d1e952
--- /dev/null
+++ lucene/core/src/java/org/apache/lucene/index/ParallelLeafReader.java
@@ -0,0 +1,327 @@
+package org.apache.lucene.index;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Collections;
+import java.util.IdentityHashMap;
+import java.util.Iterator;
+import java.util.Map;
+import java.util.Set;
+import java.util.SortedMap;
+import java.util.TreeMap;
+
+import org.apache.lucene.util.Bits;
+
+
+/** An {@link LeafReader} which reads multiple, parallel indexes.  Each index
+ * added must have the same number of documents, but typically each contains
+ * different fields. Deletions are taken from the first reader.
+ * Each document contains the union of the fields of all documents
+ * with the same document number.  When searching, matches for a
+ * query term are from the first index added that has the field.
+ *
+ * <p>This is useful, e.g., with collections that have large fields which
+ * change rarely and small fields that change more frequently.  The smaller
+ * fields may be re-indexed in a new index and both indexes may be searched
+ * together.
+ * 
+ * <p><strong>Warning:</strong> It is up to you to make sure all indexes
+ * are created and modified the same way. For example, if you add
+ * documents to one index, you need to add the same documents in the
+ * same order to the other indexes. <em>Failure to do so will result in
+ * undefined behavior</em>.
+ */
+public class ParallelLeafReader extends LeafReader {
+  private final FieldInfos fieldInfos;
+  private final ParallelFields fields = new ParallelFields();
+  private final LeafReader[] parallelReaders, storedFieldsReaders;
+  private final Set<LeafReader> completeReaderSet =
+    Collections.newSetFromMap(new IdentityHashMap<LeafReader,Boolean>());
+  private final boolean closeSubReaders;
+  private final int maxDoc, numDocs;
+  private final boolean hasDeletions;
+  private final SortedMap<String,LeafReader> fieldToReader = new TreeMap<>();
+  private final SortedMap<String,LeafReader> tvFieldToReader = new TreeMap<>();
+  
+  /** Create a ParallelAtomicReader based on the provided
+   *  readers; auto-closes the given readers on {@link #close()}. */
+  public ParallelLeafReader(LeafReader... readers) throws IOException {
+    this(true, readers);
+  }
+
+  /** Create a ParallelAtomicReader based on the provided
+   *  readers. */
+  public ParallelLeafReader(boolean closeSubReaders, LeafReader... readers) throws IOException {
+    this(closeSubReaders, readers, readers);
+  }
+
+  /** Expert: create a ParallelAtomicReader based on the provided
+   *  readers and storedFieldReaders; when a document is
+   *  loaded, only storedFieldsReaders will be used. */
+  public ParallelLeafReader(boolean closeSubReaders, LeafReader[] readers, LeafReader[] storedFieldsReaders) throws IOException {
+    this.closeSubReaders = closeSubReaders;
+    if (readers.length == 0 && storedFieldsReaders.length > 0)
+      throw new IllegalArgumentException("There must be at least one main reader if storedFieldsReaders are used.");
+    this.parallelReaders = readers.clone();
+    this.storedFieldsReaders = storedFieldsReaders.clone();
+    if (parallelReaders.length > 0) {
+      final LeafReader first = parallelReaders[0];
+      this.maxDoc = first.maxDoc();
+      this.numDocs = first.numDocs();
+      this.hasDeletions = first.hasDeletions();
+    } else {
+      this.maxDoc = this.numDocs = 0;
+      this.hasDeletions = false;
+    }
+    Collections.addAll(completeReaderSet, this.parallelReaders);
+    Collections.addAll(completeReaderSet, this.storedFieldsReaders);
+    
+    // check compatibility:
+    for(LeafReader reader : completeReaderSet) {
+      if (reader.maxDoc() != maxDoc) {
+        throw new IllegalArgumentException("All readers must have same maxDoc: "+maxDoc+"!="+reader.maxDoc());
+      }
+    }
+    
+    // TODO: make this read-only in a cleaner way?
+    FieldInfos.Builder builder = new FieldInfos.Builder();
+    // build FieldInfos and fieldToReader map:
+    for (final LeafReader reader : this.parallelReaders) {
+      final FieldInfos readerFieldInfos = reader.getFieldInfos();
+      for (FieldInfo fieldInfo : readerFieldInfos) {
+        // NOTE: first reader having a given field "wins":
+        if (!fieldToReader.containsKey(fieldInfo.name)) {
+          builder.add(fieldInfo);
+          fieldToReader.put(fieldInfo.name, reader);
+          if (fieldInfo.hasVectors()) {
+            tvFieldToReader.put(fieldInfo.name, reader);
+          }
+        }
+      }
+    }
+    fieldInfos = builder.finish();
+    
+    // build Fields instance
+    for (final LeafReader reader : this.parallelReaders) {
+      final Fields readerFields = reader.fields();
+      if (readerFields != null) {
+        for (String field : readerFields) {
+          // only add if the reader responsible for that field name is the current:
+          if (fieldToReader.get(field) == reader) {
+            this.fields.addField(field, readerFields.terms(field));
+          }
+        }
+      }
+    }
+
+    // do this finally so any Exceptions occurred before don't affect refcounts:
+    for (LeafReader reader : completeReaderSet) {
+      if (!closeSubReaders) {
+        reader.incRef();
+      }
+      reader.registerParentReader(this);
+    }
+  }
+
+  @Override
+  public String toString() {
+    final StringBuilder buffer = new StringBuilder("ParallelAtomicReader(");
+    for (final Iterator<LeafReader> iter = completeReaderSet.iterator(); iter.hasNext();) {
+      buffer.append(iter.next());
+      if (iter.hasNext()) buffer.append(", ");
+    }
+    return buffer.append(')').toString();
+  }
+
+  @Override
+  public void addCoreClosedListener(CoreClosedListener listener) {
+    addCoreClosedListenerAsReaderClosedListener(this, listener);
+  }
+
+  @Override
+  public void removeCoreClosedListener(CoreClosedListener listener) {
+    removeCoreClosedListenerAsReaderClosedListener(this, listener);
+  }
+
+  // Single instance of this, per ParallelReader instance
+  private final class ParallelFields extends Fields {
+    final Map<String,Terms> fields = new TreeMap<>();
+    
+    ParallelFields() {
+    }
+    
+    void addField(String fieldName, Terms terms) {
+      fields.put(fieldName, terms);
+    }
+    
+    @Override
+    public Iterator<String> iterator() {
+      return Collections.unmodifiableSet(fields.keySet()).iterator();
+    }
+    
+    @Override
+    public Terms terms(String field) {
+      return fields.get(field);
+    }
+    
+    @Override
+    public int size() {
+      return fields.size();
+    }
+  }
+  
+  /**
+   * {@inheritDoc}
+   * <p>
+   * NOTE: the returned field numbers will likely not
+   * correspond to the actual field numbers in the underlying
+   * readers, and codec metadata ({@link FieldInfo#getAttribute(String)}
+   * will be unavailable.
+   */
+  @Override
+  public FieldInfos getFieldInfos() {
+    return fieldInfos;
+  }
+  
+  @Override
+  public Bits getLiveDocs() {
+    ensureOpen();
+    return hasDeletions ? parallelReaders[0].getLiveDocs() : null;
+  }
+  
+  @Override
+  public Fields fields() {
+    ensureOpen();
+    return fields;
+  }
+  
+  @Override
+  public int numDocs() {
+    // Don't call ensureOpen() here (it could affect performance)
+    return numDocs;
+  }
+  
+  @Override
+  public int maxDoc() {
+    // Don't call ensureOpen() here (it could affect performance)
+    return maxDoc;
+  }
+  
+  @Override
+  public void document(int docID, StoredFieldVisitor visitor) throws IOException {
+    ensureOpen();
+    for (final LeafReader reader: storedFieldsReaders) {
+      reader.document(docID, visitor);
+    }
+  }
+  
+  @Override
+  public Fields getTermVectors(int docID) throws IOException {
+    ensureOpen();
+    ParallelFields fields = null;
+    for (Map.Entry<String,LeafReader> ent : tvFieldToReader.entrySet()) {
+      String fieldName = ent.getKey();
+      Terms vector = ent.getValue().getTermVector(docID, fieldName);
+      if (vector != null) {
+        if (fields == null) {
+          fields = new ParallelFields();
+        }
+        fields.addField(fieldName, vector);
+      }
+    }
+    
+    return fields;
+  }
+  
+  @Override
+  protected synchronized void doClose() throws IOException {
+    IOException ioe = null;
+    for (LeafReader reader : completeReaderSet) {
+      try {
+        if (closeSubReaders) {
+          reader.close();
+        } else {
+          reader.decRef();
+        }
+      } catch (IOException e) {
+        if (ioe == null) ioe = e;
+      }
+    }
+    // throw the first exception
+    if (ioe != null) throw ioe;
+  }
+
+  @Override
+  public NumericDocValues getNumericDocValues(String field) throws IOException {
+    ensureOpen();
+    LeafReader reader = fieldToReader.get(field);
+    return reader == null ? null : reader.getNumericDocValues(field);
+  }
+  
+  @Override
+  public BinaryDocValues getBinaryDocValues(String field) throws IOException {
+    ensureOpen();
+    LeafReader reader = fieldToReader.get(field);
+    return reader == null ? null : reader.getBinaryDocValues(field);
+  }
+  
+  @Override
+  public SortedDocValues getSortedDocValues(String field) throws IOException {
+    ensureOpen();
+    LeafReader reader = fieldToReader.get(field);
+    return reader == null ? null : reader.getSortedDocValues(field);
+  }
+  
+  @Override
+  public SortedNumericDocValues getSortedNumericDocValues(String field) throws IOException {
+    ensureOpen();
+    LeafReader reader = fieldToReader.get(field);
+    return reader == null ? null : reader.getSortedNumericDocValues(field);
+  }
+
+  @Override
+  public SortedSetDocValues getSortedSetDocValues(String field) throws IOException {
+    ensureOpen();
+    LeafReader reader = fieldToReader.get(field);
+    return reader == null ? null : reader.getSortedSetDocValues(field);
+  }
+
+  @Override
+  public Bits getDocsWithField(String field) throws IOException {
+    ensureOpen();
+    LeafReader reader = fieldToReader.get(field);
+    return reader == null ? null : reader.getDocsWithField(field);
+  }
+
+  @Override
+  public NumericDocValues getNormValues(String field) throws IOException {
+    ensureOpen();
+    LeafReader reader = fieldToReader.get(field);
+    NumericDocValues values = reader == null ? null : reader.getNormValues(field);
+    return values;
+  }
+
+  @Override
+  public void checkIntegrity() throws IOException {
+    ensureOpen();
+    for (LeafReader reader : completeReaderSet) {
+      reader.checkIntegrity();
+    }
+  }
+}
diff --git lucene/core/src/java/org/apache/lucene/index/ReaderUtil.java lucene/core/src/java/org/apache/lucene/index/ReaderUtil.java
index ae97143..1e8608e 100644
--- lucene/core/src/java/org/apache/lucene/index/ReaderUtil.java
+++ lucene/core/src/java/org/apache/lucene/index/ReaderUtil.java
@@ -70,7 +70,7 @@ public final class ReaderUtil {
    * Returns index of the searcher/reader for document <code>n</code> in the
    * array used to construct this searcher/reader.
    */
-  public static int subIndex(int n, List<AtomicReaderContext> leaves) { // find
+  public static int subIndex(int n, List<LeafReaderContext> leaves) { // find
     // searcher/reader for doc n:
     int size = leaves.size();
     int lo = 0; // search starts array
diff --git lucene/core/src/java/org/apache/lucene/index/SegmentCoreReaders.java lucene/core/src/java/org/apache/lucene/index/SegmentCoreReaders.java
index d971af2..1f60465 100644
--- lucene/core/src/java/org/apache/lucene/index/SegmentCoreReaders.java
+++ lucene/core/src/java/org/apache/lucene/index/SegmentCoreReaders.java
@@ -26,13 +26,12 @@ import java.util.Set;
 import java.util.concurrent.atomic.AtomicInteger;
 
 import org.apache.lucene.codecs.Codec;
-import org.apache.lucene.codecs.DocValuesProducer;
 import org.apache.lucene.codecs.FieldsProducer;
 import org.apache.lucene.codecs.NormsProducer;
 import org.apache.lucene.codecs.PostingsFormat;
 import org.apache.lucene.codecs.StoredFieldsReader;
 import org.apache.lucene.codecs.TermVectorsReader;
-import org.apache.lucene.index.AtomicReader.CoreClosedListener;
+import org.apache.lucene.index.LeafReader.CoreClosedListener;
 import org.apache.lucene.store.AlreadyClosedException;
 import org.apache.lucene.store.CompoundFileDirectory;
 import org.apache.lucene.store.Directory;
diff --git lucene/core/src/java/org/apache/lucene/index/SegmentMerger.java lucene/core/src/java/org/apache/lucene/index/SegmentMerger.java
index 3827122..836d2f3 100644
--- lucene/core/src/java/org/apache/lucene/index/SegmentMerger.java
+++ lucene/core/src/java/org/apache/lucene/index/SegmentMerger.java
@@ -50,11 +50,11 @@ final class SegmentMerger {
   private final FieldInfos.Builder fieldInfosBuilder;
 
   // note, just like in codec apis Directory 'dir' is NOT the same as segmentInfo.dir!!
-  SegmentMerger(List<AtomicReader> readers, SegmentInfo segmentInfo, InfoStream infoStream, Directory dir,
+  SegmentMerger(List<LeafReader> readers, SegmentInfo segmentInfo, InfoStream infoStream, Directory dir,
                 MergeState.CheckAbort checkAbort, FieldInfos.FieldNumbers fieldNumbers, IOContext context, boolean validate) throws IOException {
     // validate incoming readers
     if (validate) {
-      for (AtomicReader reader : readers) {
+      for (LeafReader reader : readers) {
         reader.checkIntegrity();
       }
     }
@@ -182,7 +182,7 @@ final class SegmentMerger {
   }
   
   public void mergeFieldInfos() throws IOException {
-    for (AtomicReader reader : mergeState.readers) {
+    for (LeafReader reader : mergeState.readers) {
       FieldInfos readerFieldInfos = reader.getFieldInfos();
       for (FieldInfo fi : readerFieldInfos) {
         fieldInfosBuilder.add(fi);
@@ -250,7 +250,7 @@ final class SegmentMerger {
     int i = 0;
     while(i < mergeState.readers.size()) {
 
-      final AtomicReader reader = mergeState.readers.get(i);
+      final LeafReader reader = mergeState.readers.get(i);
 
       mergeState.docBase[i] = docBase;
       final MergeState.DocMap docMap = MergeState.DocMap.build(reader);
diff --git lucene/core/src/java/org/apache/lucene/index/SegmentReader.java lucene/core/src/java/org/apache/lucene/index/SegmentReader.java
index 1a836de..27ed603 100644
--- lucene/core/src/java/org/apache/lucene/index/SegmentReader.java
+++ lucene/core/src/java/org/apache/lucene/index/SegmentReader.java
@@ -50,7 +50,7 @@ import org.apache.lucene.util.RamUsageEstimator;
  * may share the same core data.
  * @lucene.experimental
  */
-public final class SegmentReader extends AtomicReader implements Accountable {
+public final class SegmentReader extends LeafReader implements Accountable {
 
   private static final long BASE_RAM_BYTES_USED =
         RamUsageEstimator.shallowSizeOfInstance(SegmentReader.class)
diff --git lucene/core/src/java/org/apache/lucene/index/SimpleMergedSegmentWarmer.java lucene/core/src/java/org/apache/lucene/index/SimpleMergedSegmentWarmer.java
index 378c684..f095c2a 100644
--- lucene/core/src/java/org/apache/lucene/index/SimpleMergedSegmentWarmer.java
+++ lucene/core/src/java/org/apache/lucene/index/SimpleMergedSegmentWarmer.java
@@ -38,7 +38,7 @@ public class SimpleMergedSegmentWarmer extends IndexReaderWarmer {
   }
   
   @Override
-  public void warm(AtomicReader reader) throws IOException {
+  public void warm(LeafReader reader) throws IOException {
     long startTime = System.currentTimeMillis();
     int indexedCount = 0;
     int docValuesCount = 0;
diff --git lucene/core/src/java/org/apache/lucene/index/SlowCompositeReaderWrapper.java lucene/core/src/java/org/apache/lucene/index/SlowCompositeReaderWrapper.java
index ebf0409..4dff639 100644
--- lucene/core/src/java/org/apache/lucene/index/SlowCompositeReaderWrapper.java
+++ lucene/core/src/java/org/apache/lucene/index/SlowCompositeReaderWrapper.java
@@ -23,12 +23,10 @@ import java.util.Map;
 
 import org.apache.lucene.util.Bits;
 
-import org.apache.lucene.index.DirectoryReader; // javadoc
 import org.apache.lucene.index.FieldInfo.DocValuesType;
 import org.apache.lucene.index.MultiDocValues.MultiSortedDocValues;
 import org.apache.lucene.index.MultiDocValues.MultiSortedSetDocValues;
 import org.apache.lucene.index.MultiDocValues.OrdinalMap;
-import org.apache.lucene.index.MultiReader; // javadoc
 
 /**
  * This class forces a composite reader (eg a {@link
@@ -45,22 +43,22 @@ import org.apache.lucene.index.MultiReader; // javadoc
  * atomic leaves and then operate per-AtomicReader,
  * instead of using this class.
  */
-public final class SlowCompositeReaderWrapper extends AtomicReader {
+public final class SlowCompositeReaderWrapper extends LeafReader {
 
   private final CompositeReader in;
   private final Fields fields;
   private final Bits liveDocs;
   
-  /** This method is sugar for getting an {@link AtomicReader} from
+  /** This method is sugar for getting an {@link LeafReader} from
    * an {@link IndexReader} of any kind. If the reader is already atomic,
    * it is returned unchanged, otherwise wrapped by this class.
    */
-  public static AtomicReader wrap(IndexReader reader) throws IOException {
+  public static LeafReader wrap(IndexReader reader) throws IOException {
     if (reader instanceof CompositeReader) {
       return new SlowCompositeReaderWrapper((CompositeReader) reader);
     } else {
-      assert reader instanceof AtomicReader;
-      return (AtomicReader) reader;
+      assert reader instanceof LeafReader;
+      return (LeafReader) reader;
     }
   }
 
@@ -143,7 +141,7 @@ public final class SlowCompositeReaderWrapper extends AtomicReader {
     final SortedDocValues[] values = new SortedDocValues[size];
     final int[] starts = new int[size+1];
     for (int i = 0; i < size; i++) {
-      AtomicReaderContext context = in.leaves().get(i);
+      LeafReaderContext context = in.leaves().get(i);
       SortedDocValues v = context.reader().getSortedDocValues(field);
       if (v == null) {
         v = DocValues.emptySorted();
@@ -182,7 +180,7 @@ public final class SlowCompositeReaderWrapper extends AtomicReader {
     final SortedSetDocValues[] values = new SortedSetDocValues[size];
     final int[] starts = new int[size+1];
     for (int i = 0; i < size; i++) {
-      AtomicReaderContext context = in.leaves().get(i);
+      LeafReaderContext context = in.leaves().get(i);
       SortedSetDocValues v = context.reader().getSortedSetDocValues(field);
       if (v == null) {
         v = DocValues.emptySortedSet();
@@ -259,7 +257,7 @@ public final class SlowCompositeReaderWrapper extends AtomicReader {
   @Override
   public void checkIntegrity() throws IOException {
     ensureOpen();
-    for (AtomicReaderContext ctx : in.leaves()) {
+    for (LeafReaderContext ctx : in.leaves()) {
       ctx.reader().checkIntegrity();
     }
   }
diff --git lucene/core/src/java/org/apache/lucene/index/StandardDirectoryReader.java lucene/core/src/java/org/apache/lucene/index/StandardDirectoryReader.java
index 6049315..eb04bd5 100644
--- lucene/core/src/java/org/apache/lucene/index/StandardDirectoryReader.java
+++ lucene/core/src/java/org/apache/lucene/index/StandardDirectoryReader.java
@@ -37,7 +37,7 @@ final class StandardDirectoryReader extends DirectoryReader {
   private final boolean applyAllDeletes;
   
   /** called only from static open() methods */
-  StandardDirectoryReader(Directory directory, AtomicReader[] readers, IndexWriter writer,
+  StandardDirectoryReader(Directory directory, LeafReader[] readers, IndexWriter writer,
     SegmentInfos sis, boolean applyAllDeletes) {
     super(directory, readers);
     this.writer = writer;
@@ -128,7 +128,7 @@ final class StandardDirectoryReader extends DirectoryReader {
   }
 
   /** This constructor is only used for {@link #doOpenIfChanged(SegmentInfos)} */
-  private static DirectoryReader open(Directory directory, SegmentInfos infos, List<? extends AtomicReader> oldReaders) throws IOException {
+  private static DirectoryReader open(Directory directory, SegmentInfos infos, List<? extends LeafReader> oldReaders) throws IOException {
 
     // we put the old SegmentReaders in a map, that allows us
     // to lookup a reader using its segment name
@@ -234,7 +234,7 @@ final class StandardDirectoryReader extends DirectoryReader {
     if (writer != null) {
       buffer.append(":nrt");
     }
-    for (final AtomicReader r : getSequentialSubReaders()) {
+    for (final LeafReader r : getSequentialSubReaders()) {
       buffer.append(' ');
       buffer.append(r);
     }
@@ -351,7 +351,7 @@ final class StandardDirectoryReader extends DirectoryReader {
   @Override
   protected void doClose() throws IOException {
     Throwable firstExc = null;
-    for (final AtomicReader r : getSequentialSubReaders()) {
+    for (final LeafReader r : getSequentialSubReaders()) {
       // try to close each reader, even if an exception is thrown
       try {
         r.decRef();
diff --git lucene/core/src/java/org/apache/lucene/index/TermContext.java lucene/core/src/java/org/apache/lucene/index/TermContext.java
index ac80a94..5eebec8 100644
--- lucene/core/src/java/org/apache/lucene/index/TermContext.java
+++ lucene/core/src/java/org/apache/lucene/index/TermContext.java
@@ -85,7 +85,7 @@ public final class TermContext {
     final BytesRef bytes = term.bytes();
     final TermContext perReaderTermState = new TermContext(context);
     //if (DEBUG) System.out.println("prts.build term=" + term);
-    for (final AtomicReaderContext ctx : context.leaves()) {
+    for (final LeafReaderContext ctx : context.leaves()) {
       //if (DEBUG) System.out.println("  r=" + leaves[i].reader);
       final Fields fields = ctx.reader().fields();
       if (fields != null) {
diff --git lucene/core/src/java/org/apache/lucene/search/BooleanQuery.java lucene/core/src/java/org/apache/lucene/search/BooleanQuery.java
index 4d7635d..466dc90 100644
--- lucene/core/src/java/org/apache/lucene/search/BooleanQuery.java
+++ lucene/core/src/java/org/apache/lucene/search/BooleanQuery.java
@@ -24,7 +24,7 @@ import java.util.Iterator;
 import java.util.List;
 import java.util.Set;
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.search.BooleanClause.Occur;
@@ -228,7 +228,7 @@ public class BooleanQuery extends Query implements Iterable<BooleanClause> {
     }
 
     @Override
-    public Explanation explain(AtomicReaderContext context, int doc)
+    public Explanation explain(LeafReaderContext context, int doc)
       throws IOException {
       final int minShouldMatch =
         BooleanQuery.this.getMinimumNumberShouldMatch();
@@ -305,7 +305,7 @@ public class BooleanQuery extends Query implements Iterable<BooleanClause> {
     }
 
     @Override
-    public BulkScorer bulkScorer(AtomicReaderContext context, boolean scoreDocsInOrder,
+    public BulkScorer bulkScorer(LeafReaderContext context, boolean scoreDocsInOrder,
                                  Bits acceptDocs) throws IOException {
 
       if (scoreDocsInOrder || minNrShouldMatch > 1) {
@@ -340,7 +340,7 @@ public class BooleanQuery extends Query implements Iterable<BooleanClause> {
     }
 
     @Override
-    public Scorer scorer(AtomicReaderContext context, Bits acceptDocs)
+    public Scorer scorer(LeafReaderContext context, Bits acceptDocs)
         throws IOException {
       // initially the user provided value,
       // but if minNrShouldMatch == optional.size(),
diff --git lucene/core/src/java/org/apache/lucene/search/BooleanScorer.java lucene/core/src/java/org/apache/lucene/search/BooleanScorer.java
index 173bb44..5c85bdb 100644
--- lucene/core/src/java/org/apache/lucene/search/BooleanScorer.java
+++ lucene/core/src/java/org/apache/lucene/search/BooleanScorer.java
@@ -18,12 +18,8 @@ package org.apache.lucene.search;
  */
 
 import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Collection;
 import java.util.List;
 
-import org.apache.lucene.index.AtomicReaderContext;
-import org.apache.lucene.index.DocsEnum;
 import org.apache.lucene.search.BooleanQuery.BooleanWeight;
 
 /* Description from Doug Cutting (excerpted from
diff --git lucene/core/src/java/org/apache/lucene/search/CachingCollector.java lucene/core/src/java/org/apache/lucene/search/CachingCollector.java
index c5957d8..0fe0ea9 100644
--- lucene/core/src/java/org/apache/lucene/search/CachingCollector.java
+++ lucene/core/src/java/org/apache/lucene/search/CachingCollector.java
@@ -17,7 +17,7 @@ package org.apache.lucene.search;
  * limitations under the License.
  */
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.util.ArrayUtil;
 import org.apache.lucene.util.RamUsageEstimator;
 
@@ -83,7 +83,7 @@ public abstract class CachingCollector extends FilterCollector {
   private static class NoScoreCachingCollector extends CachingCollector {
 
     List<Boolean> acceptDocsOutOfOrders;
-    List<AtomicReaderContext> contexts;
+    List<LeafReaderContext> contexts;
     List<int[]> docs;
     int maxDocsToCache;
     NoScoreCachingLeafCollector lastCollector;
@@ -100,7 +100,7 @@ public abstract class CachingCollector extends FilterCollector {
       return new NoScoreCachingLeafCollector(in, maxDocsToCache);
     }
 
-    public LeafCollector getLeafCollector(AtomicReaderContext context) throws IOException {
+    public LeafCollector getLeafCollector(LeafReaderContext context) throws IOException {
       postCollection();
       final LeafCollector in = this.in.getLeafCollector(context);
       if (contexts != null) {
@@ -151,7 +151,7 @@ public abstract class CachingCollector extends FilterCollector {
       }
       assert docs.size() == contexts.size();
       for (int i = 0; i < contexts.size(); ++i) {
-        final AtomicReaderContext context = contexts.get(i);
+        final LeafReaderContext context = contexts.get(i);
         final boolean docsInOrder = !acceptDocsOutOfOrders.get(i);
         final LeafCollector collector = other.getLeafCollector(context);
         if (!collector.acceptsDocsOutOfOrder() && !docsInOrder) {
diff --git lucene/core/src/java/org/apache/lucene/search/CachingWrapperFilter.java lucene/core/src/java/org/apache/lucene/search/CachingWrapperFilter.java
index 941ee83..f038d3d 100644
--- lucene/core/src/java/org/apache/lucene/search/CachingWrapperFilter.java
+++ lucene/core/src/java/org/apache/lucene/search/CachingWrapperFilter.java
@@ -26,8 +26,8 @@ import java.util.List;
 import java.util.Map;
 import java.util.WeakHashMap;
 
-import org.apache.lucene.index.AtomicReader;
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReader;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.util.Accountable;
 import org.apache.lucene.util.Accountables;
 import org.apache.lucene.util.Bits;
@@ -61,12 +61,12 @@ public class CachingWrapperFilter extends Filter implements Accountable {
    *  Provide the DocIdSet to be cached, using the DocIdSet provided
    *  by the wrapped Filter. <p>This implementation returns the given {@link DocIdSet},
    *  if {@link DocIdSet#isCacheable} returns <code>true</code>, else it calls
-   *  {@link #cacheImpl(DocIdSetIterator,AtomicReader)}
+   *  {@link #cacheImpl(DocIdSetIterator, org.apache.lucene.index.LeafReader)}
    *  <p>Note: This method returns {@linkplain DocIdSet#EMPTY} if the given docIdSet
    *  is <code>null</code> or if {@link DocIdSet#iterator()} return <code>null</code>. The empty
    *  instance is use as a placeholder in the cache instead of the <code>null</code> value.
    */
-  protected DocIdSet docIdSetToCache(DocIdSet docIdSet, AtomicReader reader) throws IOException {
+  protected DocIdSet docIdSetToCache(DocIdSet docIdSet, LeafReader reader) throws IOException {
     if (docIdSet == null) {
       // this is better than returning null, as the nonnull result can be cached
       return EMPTY;
@@ -88,7 +88,7 @@ public class CachingWrapperFilter extends Filter implements Accountable {
   /**
    * Default cache implementation: uses {@link WAH8DocIdSet}.
    */
-  protected DocIdSet cacheImpl(DocIdSetIterator iterator, AtomicReader reader) throws IOException {
+  protected DocIdSet cacheImpl(DocIdSetIterator iterator, LeafReader reader) throws IOException {
     WAH8DocIdSet.Builder builder = new WAH8DocIdSet.Builder();
     builder.add(iterator);
     return builder.build();
@@ -98,8 +98,8 @@ public class CachingWrapperFilter extends Filter implements Accountable {
   int hitCount, missCount;
 
   @Override
-  public DocIdSet getDocIdSet(AtomicReaderContext context, final Bits acceptDocs) throws IOException {
-    final AtomicReader reader = context.reader();
+  public DocIdSet getDocIdSet(LeafReaderContext context, final Bits acceptDocs) throws IOException {
+    final LeafReader reader = context.reader();
     final Object key = reader.getCoreCacheKey();
 
     DocIdSet docIdSet = cache.get(key);
diff --git lucene/core/src/java/org/apache/lucene/search/Collector.java lucene/core/src/java/org/apache/lucene/search/Collector.java
index bb47394..0ac853f 100644
--- lucene/core/src/java/org/apache/lucene/search/Collector.java
+++ lucene/core/src/java/org/apache/lucene/search/Collector.java
@@ -19,7 +19,7 @@ package org.apache.lucene.search;
 
 import java.io.IOException;
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 
 /**
  * <p>Expert: Collectors are primarily meant to be used to
@@ -71,6 +71,6 @@ public interface Collector {
    * @param context
    *          next atomic reader context
    */
-  LeafCollector getLeafCollector(AtomicReaderContext context) throws IOException;
+  LeafCollector getLeafCollector(LeafReaderContext context) throws IOException;
 
 }
diff --git lucene/core/src/java/org/apache/lucene/search/ConstantScoreQuery.java lucene/core/src/java/org/apache/lucene/search/ConstantScoreQuery.java
index 2b7f4ed..d9e7f32 100644
--- lucene/core/src/java/org/apache/lucene/search/ConstantScoreQuery.java
+++ lucene/core/src/java/org/apache/lucene/search/ConstantScoreQuery.java
@@ -17,7 +17,7 @@ package org.apache.lucene.search;
  * limitations under the License.
  */
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.util.Bits;
@@ -134,7 +134,7 @@ public class ConstantScoreQuery extends Query {
     }
 
     @Override
-    public BulkScorer bulkScorer(AtomicReaderContext context, boolean scoreDocsInOrder, Bits acceptDocs) throws IOException {
+    public BulkScorer bulkScorer(LeafReaderContext context, boolean scoreDocsInOrder, Bits acceptDocs) throws IOException {
       final DocIdSetIterator disi;
       if (filter != null) {
         assert query == null;
@@ -150,7 +150,7 @@ public class ConstantScoreQuery extends Query {
     }
 
     @Override
-    public Scorer scorer(AtomicReaderContext context, Bits acceptDocs) throws IOException {
+    public Scorer scorer(LeafReaderContext context, Bits acceptDocs) throws IOException {
       final DocIdSetIterator disi;
       if (filter != null) {
         assert query == null;
@@ -176,7 +176,7 @@ public class ConstantScoreQuery extends Query {
     }
 
     @Override
-    public Explanation explain(AtomicReaderContext context, int doc) throws IOException {
+    public Explanation explain(LeafReaderContext context, int doc) throws IOException {
       final Scorer cs = scorer(context, context.reader().getLiveDocs());
       final boolean exists = (cs != null && cs.advance(doc) == doc);
 
diff --git lucene/core/src/java/org/apache/lucene/search/DisjunctionMaxQuery.java lucene/core/src/java/org/apache/lucene/search/DisjunctionMaxQuery.java
index c195497..e27063a 100644
--- lucene/core/src/java/org/apache/lucene/search/DisjunctionMaxQuery.java
+++ lucene/core/src/java/org/apache/lucene/search/DisjunctionMaxQuery.java
@@ -23,7 +23,7 @@ import java.util.Iterator;
 import java.util.List;
 import java.util.Set;
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.util.Bits;
@@ -153,7 +153,7 @@ public class DisjunctionMaxQuery extends Query implements Iterable<Query> {
 
     /** Create the scorer used to score our associated DisjunctionMaxQuery */
     @Override
-    public Scorer scorer(AtomicReaderContext context, Bits acceptDocs) throws IOException {
+    public Scorer scorer(LeafReaderContext context, Bits acceptDocs) throws IOException {
       List<Scorer> scorers = new ArrayList<>();
       for (Weight w : weights) {
         // we will advance() subscorers
@@ -175,7 +175,7 @@ public class DisjunctionMaxQuery extends Query implements Iterable<Query> {
 
     /** Explain the score we computed for doc */
     @Override
-    public Explanation explain(AtomicReaderContext context, int doc) throws IOException {
+    public Explanation explain(LeafReaderContext context, int doc) throws IOException {
       if (disjuncts.size() == 1) return weights.get(0).explain(context,doc);
       ComplexExplanation result = new ComplexExplanation();
       float max = 0.0f, sum = 0.0f;
diff --git lucene/core/src/java/org/apache/lucene/search/DocTermOrdsRangeFilter.java lucene/core/src/java/org/apache/lucene/search/DocTermOrdsRangeFilter.java
index ff6ca17..dd5672c 100644
--- lucene/core/src/java/org/apache/lucene/search/DocTermOrdsRangeFilter.java
+++ lucene/core/src/java/org/apache/lucene/search/DocTermOrdsRangeFilter.java
@@ -18,15 +18,14 @@ package org.apache.lucene.search;
 
 import java.io.IOException;
 
-import org.apache.lucene.index.AtomicReader;
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.DocValues;
 import org.apache.lucene.index.SortedSetDocValues;
 import org.apache.lucene.util.Bits;
 import org.apache.lucene.util.BytesRef;
 
 /**
- * A range filter built on top of a cached multi-valued term field (from {@link AtomicReader#getSortedSetDocValues}).
+ * A range filter built on top of a cached multi-valued term field (from {@link org.apache.lucene.index.LeafReader#getSortedSetDocValues}).
  * 
  * <p>Like {@link DocValuesRangeFilter}, this is just a specialized range query versus
  *    using a TermRangeQuery with {@link DocTermOrdsRewriteMethod}: it will only do
@@ -50,17 +49,17 @@ public abstract class DocTermOrdsRangeFilter extends Filter {
   
   /** This method is implemented for each data type */
   @Override
-  public abstract DocIdSet getDocIdSet(AtomicReaderContext context, Bits acceptDocs) throws IOException;
+  public abstract DocIdSet getDocIdSet(LeafReaderContext context, Bits acceptDocs) throws IOException;
   
   /**
-   * Creates a BytesRef range filter using {@link AtomicReader#getSortedSetDocValues}. This works with all
+   * Creates a BytesRef range filter using {@link org.apache.lucene.index.LeafReader#getSortedSetDocValues}. This works with all
    * fields containing zero or one term in the field. The range can be half-open by setting one
    * of the values to <code>null</code>.
    */
   public static DocTermOrdsRangeFilter newBytesRefRange(String field, BytesRef lowerVal, BytesRef upperVal, boolean includeLower, boolean includeUpper) {
     return new DocTermOrdsRangeFilter(field, lowerVal, upperVal, includeLower, includeUpper) {
       @Override
-      public DocIdSet getDocIdSet(AtomicReaderContext context, Bits acceptDocs) throws IOException {
+      public DocIdSet getDocIdSet(LeafReaderContext context, Bits acceptDocs) throws IOException {
         final SortedSetDocValues docTermOrds = DocValues.getSortedSet(context.reader(), field);
         final long lowerPoint = lowerVal == null ? -1 : docTermOrds.lookupTerm(lowerVal);
         final long upperPoint = upperVal == null ? -1 : docTermOrds.lookupTerm(upperVal);
diff --git lucene/core/src/java/org/apache/lucene/search/DocTermOrdsRewriteMethod.java lucene/core/src/java/org/apache/lucene/search/DocTermOrdsRewriteMethod.java
index eef62fb..9c38bc7 100644
--- lucene/core/src/java/org/apache/lucene/search/DocTermOrdsRewriteMethod.java
+++ lucene/core/src/java/org/apache/lucene/search/DocTermOrdsRewriteMethod.java
@@ -19,7 +19,7 @@ package org.apache.lucene.search;
 
 import java.io.IOException;
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.DocValues;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.SortedSetDocValues;
@@ -83,7 +83,7 @@ public final class DocTermOrdsRewriteMethod extends MultiTermQuery.RewriteMethod
      * results.
      */
     @Override
-    public DocIdSet getDocIdSet(AtomicReaderContext context, final Bits acceptDocs) throws IOException {
+    public DocIdSet getDocIdSet(LeafReaderContext context, final Bits acceptDocs) throws IOException {
       final SortedSetDocValues docTermOrds = DocValues.getSortedSet(context.reader(), query.field);
       // Cannot use FixedBitSet because we require long index (ord):
       final LongBitSet termSet = new LongBitSet(docTermOrds.getValueCount());
diff --git lucene/core/src/java/org/apache/lucene/search/DocValuesRangeFilter.java lucene/core/src/java/org/apache/lucene/search/DocValuesRangeFilter.java
index 336bda2..03eb14c 100644
--- lucene/core/src/java/org/apache/lucene/search/DocValuesRangeFilter.java
+++ lucene/core/src/java/org/apache/lucene/search/DocValuesRangeFilter.java
@@ -22,8 +22,7 @@ import org.apache.lucene.document.DoubleField; // for javadocs
 import org.apache.lucene.document.FloatField; // for javadocs
 import org.apache.lucene.document.IntField; // for javadocs
 import org.apache.lucene.document.LongField; // for javadocs
-import org.apache.lucene.index.AtomicReader; // for javadocs
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.DocValues;
 import org.apache.lucene.index.NumericDocValues;
 import org.apache.lucene.index.SortedDocValues;
@@ -33,7 +32,7 @@ import org.apache.lucene.util.NumericUtils;
 
 /**
  * A range filter built on top of numeric doc values field 
- * (from {@link AtomicReader#getNumericDocValues(String)}).
+ * (from {@link org.apache.lucene.index.LeafReader#getNumericDocValues(String)}).
  * 
  * <p>{@code DocValuesRangeFilter} builds a single cache for the field the first time it is used.
  * Each subsequent {@code DocValuesRangeFilter} on the same field then reuses this cache,
@@ -50,7 +49,7 @@ import org.apache.lucene.util.NumericUtils;
  * LongField} or {@link DoubleField}. But
  * it has the problem that it only works with exact one value/document (see below).
  *
- * <p>As with all {@link AtomicReader#getNumericDocValues} based functionality, 
+ * <p>As with all {@link org.apache.lucene.index.LeafReader#getNumericDocValues} based functionality, 
  * {@code DocValuesRangeFilter} is only valid for 
  * fields which exact one term for each document (except for {@link #newStringRange}
  * where 0 terms are also allowed). Due to historical reasons, for numeric ranges
@@ -81,17 +80,17 @@ public abstract class DocValuesRangeFilter<T> extends Filter {
   
   /** This method is implemented for each data type */
   @Override
-  public abstract DocIdSet getDocIdSet(AtomicReaderContext context, Bits acceptDocs) throws IOException;
+  public abstract DocIdSet getDocIdSet(LeafReaderContext context, Bits acceptDocs) throws IOException;
 
   /**
-   * Creates a string range filter using {@link AtomicReader#getSortedDocValues(String)}. This works with all
+   * Creates a string range filter using {@link org.apache.lucene.index.LeafReader#getSortedDocValues(String)}. This works with all
    * fields containing zero or one term in the field. The range can be half-open by setting one
    * of the values to <code>null</code>.
    */
   public static DocValuesRangeFilter<String> newStringRange(String field, String lowerVal, String upperVal, boolean includeLower, boolean includeUpper) {
     return new DocValuesRangeFilter<String>(field, lowerVal, upperVal, includeLower, includeUpper) {
       @Override
-      public DocIdSet getDocIdSet(AtomicReaderContext context, Bits acceptDocs) throws IOException {
+      public DocIdSet getDocIdSet(LeafReaderContext context, Bits acceptDocs) throws IOException {
         final SortedDocValues fcsi = DocValues.getSorted(context.reader(), field);
         final int lowerPoint = lowerVal == null ? -1 : fcsi.lookupTerm(new BytesRef(lowerVal));
         final int upperPoint = upperVal == null ? -1 : fcsi.lookupTerm(new BytesRef(upperVal));
@@ -140,7 +139,7 @@ public abstract class DocValuesRangeFilter<T> extends Filter {
   }
   
   /**
-   * Creates a BytesRef range filter using {@link AtomicReader#getSortedDocValues(String)}. This works with all
+   * Creates a BytesRef range filter using {@link org.apache.lucene.index.LeafReader#getSortedDocValues(String)}. This works with all
    * fields containing zero or one term in the field. The range can be half-open by setting one
    * of the values to <code>null</code>.
    */
@@ -148,7 +147,7 @@ public abstract class DocValuesRangeFilter<T> extends Filter {
   public static DocValuesRangeFilter<BytesRef> newBytesRefRange(String field, BytesRef lowerVal, BytesRef upperVal, boolean includeLower, boolean includeUpper) {
     return new DocValuesRangeFilter<BytesRef>(field, lowerVal, upperVal, includeLower, includeUpper) {
       @Override
-      public DocIdSet getDocIdSet(AtomicReaderContext context, Bits acceptDocs) throws IOException {
+      public DocIdSet getDocIdSet(LeafReaderContext context, Bits acceptDocs) throws IOException {
         final SortedDocValues fcsi = DocValues.getSorted(context.reader(), field);
         final int lowerPoint = lowerVal == null ? -1 : fcsi.lookupTerm(lowerVal);
         final int upperPoint = upperVal == null ? -1 : fcsi.lookupTerm(upperVal);
@@ -197,14 +196,14 @@ public abstract class DocValuesRangeFilter<T> extends Filter {
   }
 
   /**
-   * Creates a numeric range filter using {@link AtomicReader#getSortedDocValues(String)}. This works with all
+   * Creates a numeric range filter using {@link org.apache.lucene.index.LeafReader#getSortedDocValues(String)}. This works with all
    * int fields containing exactly one numeric term in the field. The range can be half-open by setting one
    * of the values to <code>null</code>.
    */
   public static DocValuesRangeFilter<Integer> newIntRange(String field, Integer lowerVal, Integer upperVal, boolean includeLower, boolean includeUpper) {
     return new DocValuesRangeFilter<Integer>(field, lowerVal, upperVal, includeLower, includeUpper) {
       @Override
-      public DocIdSet getDocIdSet(AtomicReaderContext context, Bits acceptDocs) throws IOException {
+      public DocIdSet getDocIdSet(LeafReaderContext context, Bits acceptDocs) throws IOException {
         final int inclusiveLowerPoint, inclusiveUpperPoint;
         if (lowerVal != null) {
           int i = lowerVal.intValue();
@@ -239,14 +238,14 @@ public abstract class DocValuesRangeFilter<T> extends Filter {
   }
   
   /**
-   * Creates a numeric range filter using {@link AtomicReader#getNumericDocValues(String)}. This works with all
+   * Creates a numeric range filter using {@link org.apache.lucene.index.LeafReader#getNumericDocValues(String)}. This works with all
    * long fields containing exactly one numeric term in the field. The range can be half-open by setting one
    * of the values to <code>null</code>.
    */
   public static DocValuesRangeFilter<Long> newLongRange(String field, Long lowerVal, Long upperVal, boolean includeLower, boolean includeUpper) {
     return new DocValuesRangeFilter<Long>(field, lowerVal, upperVal, includeLower, includeUpper) {
       @Override
-      public DocIdSet getDocIdSet(AtomicReaderContext context, Bits acceptDocs) throws IOException {
+      public DocIdSet getDocIdSet(LeafReaderContext context, Bits acceptDocs) throws IOException {
         final long inclusiveLowerPoint, inclusiveUpperPoint;
         if (lowerVal != null) {
           long i = lowerVal.longValue();
@@ -281,14 +280,14 @@ public abstract class DocValuesRangeFilter<T> extends Filter {
   }
   
   /**
-   * Creates a numeric range filter using {@link AtomicReader#getNumericDocValues(String)}. This works with all
+   * Creates a numeric range filter using {@link org.apache.lucene.index.LeafReader#getNumericDocValues(String)}. This works with all
    * float fields containing exactly one numeric term in the field. The range can be half-open by setting one
    * of the values to <code>null</code>.
    */
   public static DocValuesRangeFilter<Float> newFloatRange(String field, Float lowerVal, Float upperVal, boolean includeLower, boolean includeUpper) {
     return new DocValuesRangeFilter<Float>(field, lowerVal, upperVal, includeLower, includeUpper) {
       @Override
-      public DocIdSet getDocIdSet(AtomicReaderContext context, Bits acceptDocs) throws IOException {
+      public DocIdSet getDocIdSet(LeafReaderContext context, Bits acceptDocs) throws IOException {
         // we transform the floating point numbers to sortable integers
         // using NumericUtils to easier find the next bigger/lower value
         final float inclusiveLowerPoint, inclusiveUpperPoint;
@@ -327,14 +326,14 @@ public abstract class DocValuesRangeFilter<T> extends Filter {
   }
   
   /**
-   * Creates a numeric range filter using {@link AtomicReader#getNumericDocValues(String)}. This works with all
+   * Creates a numeric range filter using {@link org.apache.lucene.index.LeafReader#getNumericDocValues(String)}. This works with all
    * double fields containing exactly one numeric term in the field. The range can be half-open by setting one
    * of the values to <code>null</code>.
    */
   public static DocValuesRangeFilter<Double> newDoubleRange(String field, Double lowerVal, Double upperVal, boolean includeLower, boolean includeUpper) {
     return new DocValuesRangeFilter<Double>(field, lowerVal, upperVal, includeLower, includeUpper) {
       @Override
-      public DocIdSet getDocIdSet(AtomicReaderContext context, Bits acceptDocs) throws IOException {
+      public DocIdSet getDocIdSet(LeafReaderContext context, Bits acceptDocs) throws IOException {
         // we transform the floating point numbers to sortable integers
         // using NumericUtils to easier find the next bigger/lower value
         final double inclusiveLowerPoint, inclusiveUpperPoint;
diff --git lucene/core/src/java/org/apache/lucene/search/DocValuesRewriteMethod.java lucene/core/src/java/org/apache/lucene/search/DocValuesRewriteMethod.java
index fffcfec..cd48822 100644
--- lucene/core/src/java/org/apache/lucene/search/DocValuesRewriteMethod.java
+++ lucene/core/src/java/org/apache/lucene/search/DocValuesRewriteMethod.java
@@ -19,7 +19,7 @@ package org.apache.lucene.search;
 
 import java.io.IOException;
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.DocValues;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.SortedDocValues;
@@ -83,7 +83,7 @@ public final class DocValuesRewriteMethod extends MultiTermQuery.RewriteMethod {
      * results.
      */
     @Override
-    public DocIdSet getDocIdSet(AtomicReaderContext context, final Bits acceptDocs) throws IOException {
+    public DocIdSet getDocIdSet(LeafReaderContext context, final Bits acceptDocs) throws IOException {
       final SortedDocValues fcsi = DocValues.getSorted(context.reader(), query.field);
       // Cannot use FixedBitSet because we require long index (ord):
       final LongBitSet termSet = new LongBitSet(fcsi.getValueCount());
diff --git lucene/core/src/java/org/apache/lucene/search/DocValuesTermsFilter.java lucene/core/src/java/org/apache/lucene/search/DocValuesTermsFilter.java
index 86d7011..464e5b2 100644
--- lucene/core/src/java/org/apache/lucene/search/DocValuesTermsFilter.java
+++ lucene/core/src/java/org/apache/lucene/search/DocValuesTermsFilter.java
@@ -19,7 +19,7 @@ package org.apache.lucene.search;
 
 import java.io.IOException;
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.DocValues;
 import org.apache.lucene.index.DocsEnum; // javadoc @link
 import org.apache.lucene.index.IndexReader;
@@ -104,7 +104,7 @@ public class DocValuesTermsFilter extends Filter {
   }
 
   @Override
-  public DocIdSet getDocIdSet(AtomicReaderContext context, Bits acceptDocs) throws IOException {
+  public DocIdSet getDocIdSet(LeafReaderContext context, Bits acceptDocs) throws IOException {
     final SortedDocValues fcsi = DocValues.getSorted(context.reader(), field);
     final FixedBitSet bits = new FixedBitSet(fcsi.getValueCount());
     for (int i=0;i<terms.length;i++) {
diff --git lucene/core/src/java/org/apache/lucene/search/FieldComparator.java lucene/core/src/java/org/apache/lucene/search/FieldComparator.java
index 67f0fd4..09bf533 100644
--- lucene/core/src/java/org/apache/lucene/search/FieldComparator.java
+++ lucene/core/src/java/org/apache/lucene/search/FieldComparator.java
@@ -19,8 +19,7 @@ package org.apache.lucene.search;
 
 import java.io.IOException;
 
-import org.apache.lucene.index.AtomicReader;
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.BinaryDocValues;
 import org.apache.lucene.index.DocValues;
 import org.apache.lucene.index.NumericDocValues;
@@ -78,7 +77,7 @@ import org.apache.lucene.util.BytesRefBuilder;
  *       priority queue.  The {@link FieldValueHitQueue}
  *       calls this method when a new hit is competitive.
  *
- *  <li> {@link #setNextReader(AtomicReaderContext)} Invoked
+ *  <li> {@link #setNextReader(org.apache.lucene.index.LeafReaderContext)} Invoked
  *       when the search is switching to the next segment.
  *       You may need to update internal state of the
  *       comparator, for example retrieving new values from
@@ -170,7 +169,7 @@ public abstract class FieldComparator<T> {
   public abstract void copy(int slot, int doc) throws IOException;
 
   /**
-   * Set a new {@link AtomicReaderContext}. All subsequent docIDs are relative to
+   * Set a new {@link org.apache.lucene.index.LeafReaderContext}. All subsequent docIDs are relative to
    * the current reader (you must add docBase if you need to
    * map it to a top-level docID).
    * 
@@ -180,7 +179,7 @@ public abstract class FieldComparator<T> {
    *   comparator across segments
    * @throws IOException if there is a low-level IO error
    */
-  public abstract FieldComparator<T> setNextReader(AtomicReaderContext context) throws IOException;
+  public abstract FieldComparator<T> setNextReader(LeafReaderContext context) throws IOException;
 
   /** Sets the Scorer to use in case a document's score is
    *  needed.
@@ -235,7 +234,7 @@ public abstract class FieldComparator<T> {
     }
 
     @Override
-    public FieldComparator<T> setNextReader(AtomicReaderContext context) throws IOException {
+    public FieldComparator<T> setNextReader(LeafReaderContext context) throws IOException {
       currentReaderValues = getNumericDocValues(context, field);
       if (missingValue != null) {
         docsWithField = DocValues.getDocsWithField(context.reader(), field);
@@ -250,13 +249,13 @@ public abstract class FieldComparator<T> {
     }
     
     /** Retrieves the NumericDocValues for the field in this segment */
-    protected NumericDocValues getNumericDocValues(AtomicReaderContext context, String field) throws IOException {
+    protected NumericDocValues getNumericDocValues(LeafReaderContext context, String field) throws IOException {
       return DocValues.getNumeric(context.reader(), field);
     }
   }
 
   /** Parses field's values as double (using {@link
-   *  AtomicReader#getNumericDocValues} and sorts by ascending value */
+   *  org.apache.lucene.index.LeafReader#getNumericDocValues} and sorts by ascending value */
   public static class DoubleComparator extends NumericComparator<Double> {
     private final double[] values;
     private double bottom;
@@ -324,7 +323,7 @@ public abstract class FieldComparator<T> {
   }
 
   /** Parses field's values as float (using {@link
-   *  AtomicReader#getNumericDocValues(String)} and sorts by ascending value */
+   *  org.apache.lucene.index.LeafReader#getNumericDocValues(String)} and sorts by ascending value */
   public static class FloatComparator extends NumericComparator<Float> {
     private final float[] values;
     private float bottom;
@@ -393,7 +392,7 @@ public abstract class FieldComparator<T> {
   }
 
   /** Parses field's values as int (using {@link
-   *  AtomicReader#getNumericDocValues(String)} and sorts by ascending value */
+   *  org.apache.lucene.index.LeafReader#getNumericDocValues(String)} and sorts by ascending value */
   public static class IntComparator extends NumericComparator<Integer> {
     private final int[] values;
     private int bottom;                           // Value of bottom of queue
@@ -461,7 +460,7 @@ public abstract class FieldComparator<T> {
   }
 
   /** Parses field's values as long (using {@link
-   *  AtomicReader#getNumericDocValues(String)} and sorts by ascending value */
+   *  org.apache.lucene.index.LeafReader#getNumericDocValues(String)} and sorts by ascending value */
   public static class LongComparator extends NumericComparator<Long> {
     private final long[] values;
     private long bottom;
@@ -565,7 +564,7 @@ public abstract class FieldComparator<T> {
     }
 
     @Override
-    public FieldComparator<Float> setNextReader(AtomicReaderContext context) {
+    public FieldComparator<Float> setNextReader(LeafReaderContext context) {
       return this;
     }
     
@@ -641,7 +640,7 @@ public abstract class FieldComparator<T> {
     }
 
     @Override
-    public FieldComparator<Integer> setNextReader(AtomicReaderContext context) {
+    public FieldComparator<Integer> setNextReader(LeafReaderContext context) {
       // TODO: can we "map" our docIDs to the current
       // reader? saves having to then subtract on every
       // compare call
@@ -675,7 +674,7 @@ public abstract class FieldComparator<T> {
    *  ordinals.  This is functionally equivalent to {@link
    *  org.apache.lucene.search.FieldComparator.TermValComparator}, but it first resolves the string
    *  to their relative ordinal positions (using the index
-   *  returned by {@link AtomicReader#getSortedDocValues(String)}), and
+   *  returned by {@link org.apache.lucene.index.LeafReader#getSortedDocValues(String)}), and
    *  does most comparisons using the ordinals.  For medium
    *  to large results, this comparator will be much faster
    *  than {@link org.apache.lucene.search.FieldComparator.TermValComparator}.  For very small
@@ -819,12 +818,12 @@ public abstract class FieldComparator<T> {
     }
     
     /** Retrieves the SortedDocValues for the field in this segment */
-    protected SortedDocValues getSortedDocValues(AtomicReaderContext context, String field) throws IOException {
+    protected SortedDocValues getSortedDocValues(LeafReaderContext context, String field) throws IOException {
       return DocValues.getSorted(context.reader(), field);
     }
     
     @Override
-    public FieldComparator<BytesRef> setNextReader(AtomicReaderContext context) throws IOException {
+    public FieldComparator<BytesRef> setNextReader(LeafReaderContext context) throws IOException {
       termsIndex = getSortedDocValues(context, field);
       currentReaderGen++;
 
@@ -983,12 +982,12 @@ public abstract class FieldComparator<T> {
     }
 
     /** Retrieves the BinaryDocValues for the field in this segment */
-    protected BinaryDocValues getBinaryDocValues(AtomicReaderContext context, String field) throws IOException {
+    protected BinaryDocValues getBinaryDocValues(LeafReaderContext context, String field) throws IOException {
       return DocValues.getBinary(context.reader(), field);
     }
 
     /** Retrieves the set of documents that have a value in this segment */
-    protected Bits getDocsWithField(AtomicReaderContext context, String field) throws IOException {
+    protected Bits getDocsWithField(LeafReaderContext context, String field) throws IOException {
       return DocValues.getDocsWithField(context.reader(), field);
     }
 
@@ -1002,7 +1001,7 @@ public abstract class FieldComparator<T> {
     }
 
     @Override
-    public FieldComparator<BytesRef> setNextReader(AtomicReaderContext context) throws IOException {
+    public FieldComparator<BytesRef> setNextReader(LeafReaderContext context) throws IOException {
       docTerms = getBinaryDocValues(context, field);
       docsWithField = getDocsWithField(context, field);
       if (docsWithField instanceof Bits.MatchAllBits) {
diff --git lucene/core/src/java/org/apache/lucene/search/FieldValueFilter.java lucene/core/src/java/org/apache/lucene/search/FieldValueFilter.java
index 045b0b2..88d683e 100644
--- lucene/core/src/java/org/apache/lucene/search/FieldValueFilter.java
+++ lucene/core/src/java/org/apache/lucene/search/FieldValueFilter.java
@@ -18,8 +18,7 @@ package org.apache.lucene.search;
  */
 import java.io.IOException;
 
-import org.apache.lucene.index.AtomicReader;
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.DocValues;
 import org.apache.lucene.util.Bits;
 import org.apache.lucene.util.Bits.MatchAllBits;
@@ -28,7 +27,7 @@ import org.apache.lucene.util.Bits.MatchNoBits;
 /**
  * A {@link Filter} that accepts all documents that have one or more values in a
  * given field. This {@link Filter} request {@link Bits} from
- * {@link AtomicReader#getDocsWithField}
+ * {@link org.apache.lucene.index.LeafReader#getDocsWithField}
  */
 public class FieldValueFilter extends Filter {
   private final String field;
@@ -76,7 +75,7 @@ public class FieldValueFilter extends Filter {
   }
 
   @Override
-  public DocIdSet getDocIdSet(AtomicReaderContext context, Bits acceptDocs)
+  public DocIdSet getDocIdSet(LeafReaderContext context, Bits acceptDocs)
       throws IOException {
     final Bits docsWithField = DocValues.getDocsWithField(
         context.reader(), field);
diff --git lucene/core/src/java/org/apache/lucene/search/Filter.java lucene/core/src/java/org/apache/lucene/search/Filter.java
index c899477..623e4d9 100644
--- lucene/core/src/java/org/apache/lucene/search/Filter.java
+++ lucene/core/src/java/org/apache/lucene/search/Filter.java
@@ -19,8 +19,7 @@ package org.apache.lucene.search;
 
 import java.io.IOException;
 
-import org.apache.lucene.index.AtomicReader; // javadocs
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.util.Bits;
 
 /** 
@@ -39,12 +38,12 @@ public abstract class Filter {
    * must refer to document IDs for that segment, not for
    * the top-level reader.
    * 
-   * @param context a {@link AtomicReaderContext} instance opened on the index currently
+   * @param context a {@link org.apache.lucene.index.LeafReaderContext} instance opened on the index currently
    *         searched on. Note, it is likely that the provided reader info does not
    *         represent the whole underlying index i.e. if the index has more than
    *         one segment the given reader only represents a single segment.
    *         The provided context is always an atomic context, so you can call 
-   *         {@link AtomicReader#fields()}
+   *         {@link org.apache.lucene.index.LeafReader#fields()}
    *         on the context's reader, for example.
    *
    * @param acceptDocs
@@ -56,5 +55,5 @@ public abstract class Filter {
    *         the filter doesn't accept any documents otherwise internal optimization might not apply
    *         in the case an <i>empty</i> {@link DocIdSet} is returned.
    */
-  public abstract DocIdSet getDocIdSet(AtomicReaderContext context, Bits acceptDocs) throws IOException;
+  public abstract DocIdSet getDocIdSet(LeafReaderContext context, Bits acceptDocs) throws IOException;
 }
diff --git lucene/core/src/java/org/apache/lucene/search/FilterCollector.java lucene/core/src/java/org/apache/lucene/search/FilterCollector.java
index 247bb03..ba1b735 100644
--- lucene/core/src/java/org/apache/lucene/search/FilterCollector.java
+++ lucene/core/src/java/org/apache/lucene/search/FilterCollector.java
@@ -2,7 +2,7 @@ package org.apache.lucene.search;
 
 import java.io.IOException;
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 
 /*
  * Licensed to the Apache Software Foundation (ASF) under one or more
@@ -36,7 +36,7 @@ public class FilterCollector implements Collector {
   }
 
   @Override
-  public LeafCollector getLeafCollector(AtomicReaderContext context) throws IOException {
+  public LeafCollector getLeafCollector(LeafReaderContext context) throws IOException {
     return in.getLeafCollector(context);
   }
 
diff --git lucene/core/src/java/org/apache/lucene/search/FilteredQuery.java lucene/core/src/java/org/apache/lucene/search/FilteredQuery.java
index d700a30..0e10bb3 100644
--- lucene/core/src/java/org/apache/lucene/search/FilteredQuery.java
+++ lucene/core/src/java/org/apache/lucene/search/FilteredQuery.java
@@ -17,7 +17,7 @@ package org.apache.lucene.search;
  * limitations under the License.
  */
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.util.Bits;
@@ -98,7 +98,7 @@ public class FilteredQuery extends Query {
       }
 
       @Override
-      public Explanation explain(AtomicReaderContext ir, int i) throws IOException {
+      public Explanation explain(LeafReaderContext ir, int i) throws IOException {
         Explanation inner = weight.explain (ir, i);
         Filter f = FilteredQuery.this.filter;
         DocIdSet docIdSet = f.getDocIdSet(ir, ir.reader().getLiveDocs());
@@ -124,7 +124,7 @@ public class FilteredQuery extends Query {
 
       // return a filtering scorer
       @Override
-      public Scorer scorer(AtomicReaderContext context, Bits acceptDocs) throws IOException {
+      public Scorer scorer(LeafReaderContext context, Bits acceptDocs) throws IOException {
         assert filter != null;
 
         DocIdSet filterDocIdSet = filter.getDocIdSet(context, acceptDocs);
@@ -138,7 +138,7 @@ public class FilteredQuery extends Query {
 
       // return a filtering top scorer
       @Override
-      public BulkScorer bulkScorer(AtomicReaderContext context, boolean scoreDocsInOrder, Bits acceptDocs) throws IOException {
+      public BulkScorer bulkScorer(LeafReaderContext context, boolean scoreDocsInOrder, Bits acceptDocs) throws IOException {
         assert filter != null;
 
         DocIdSet filterDocIdSet = filter.getDocIdSet(context, acceptDocs);
@@ -477,14 +477,14 @@ public class FilteredQuery extends Query {
      * Returns a filtered {@link Scorer} based on this strategy.
      * 
      * @param context
-     *          the {@link AtomicReaderContext} for which to return the {@link Scorer}.
+     *          the {@link org.apache.lucene.index.LeafReaderContext} for which to return the {@link Scorer}.
      * @param weight the {@link FilteredQuery} {@link Weight} to create the filtered scorer.
      * @param docIdSet the filter {@link DocIdSet} to apply
      * @return a filtered scorer
      * 
      * @throws IOException if an {@link IOException} occurs
      */
-    public abstract Scorer filteredScorer(AtomicReaderContext context,
+    public abstract Scorer filteredScorer(LeafReaderContext context,
         Weight weight, DocIdSet docIdSet) throws IOException;
 
     /**
@@ -494,12 +494,12 @@ public class FilteredQuery extends Query {
      * wraps that into a BulkScorer.
      *
      * @param context
-     *          the {@link AtomicReaderContext} for which to return the {@link Scorer}.
+     *          the {@link org.apache.lucene.index.LeafReaderContext} for which to return the {@link Scorer}.
      * @param weight the {@link FilteredQuery} {@link Weight} to create the filtered scorer.
      * @param docIdSet the filter {@link DocIdSet} to apply
      * @return a filtered top scorer
      */
-    public BulkScorer filteredBulkScorer(AtomicReaderContext context,
+    public BulkScorer filteredBulkScorer(LeafReaderContext context,
         Weight weight, boolean scoreDocsInOrder, DocIdSet docIdSet) throws IOException {
       Scorer scorer = filteredScorer(context, weight, docIdSet);
       if (scorer == null) {
@@ -522,7 +522,7 @@ public class FilteredQuery extends Query {
   public static class RandomAccessFilterStrategy extends FilterStrategy {
 
     @Override
-    public Scorer filteredScorer(AtomicReaderContext context, Weight weight, DocIdSet docIdSet) throws IOException {
+    public Scorer filteredScorer(LeafReaderContext context, Weight weight, DocIdSet docIdSet) throws IOException {
       final DocIdSetIterator filterIter = docIdSet.iterator();
       if (filterIter == null) {
         // this means the filter does not accept any documents.
@@ -577,7 +577,7 @@ public class FilteredQuery extends Query {
     }
 
     @Override
-    public Scorer filteredScorer(AtomicReaderContext context,
+    public Scorer filteredScorer(LeafReaderContext context,
         Weight weight, DocIdSet docIdSet) throws IOException {
       final DocIdSetIterator filterIter = docIdSet.iterator();
       if (filterIter == null) {
@@ -613,7 +613,7 @@ public class FilteredQuery extends Query {
    */
   private static final class QueryFirstFilterStrategy extends FilterStrategy {
     @Override
-    public Scorer filteredScorer(final AtomicReaderContext context,
+    public Scorer filteredScorer(final LeafReaderContext context,
         Weight weight,
         DocIdSet docIdSet) throws IOException {
       Bits filterAcceptDocs = docIdSet.bits();
@@ -628,7 +628,7 @@ public class FilteredQuery extends Query {
     }
 
     @Override
-    public BulkScorer filteredBulkScorer(final AtomicReaderContext context,
+    public BulkScorer filteredBulkScorer(final LeafReaderContext context,
         Weight weight,
         boolean scoreDocsInOrder, // ignored (we always top-score in order)
         DocIdSet docIdSet) throws IOException {
diff --git lucene/core/src/java/org/apache/lucene/search/IndexSearcher.java lucene/core/src/java/org/apache/lucene/search/IndexSearcher.java
index 116304e..69010e2 100644
--- lucene/core/src/java/org/apache/lucene/search/IndexSearcher.java
+++ lucene/core/src/java/org/apache/lucene/search/IndexSearcher.java
@@ -32,7 +32,7 @@ import java.util.concurrent.ExecutorService;
 import java.util.concurrent.locks.Lock;
 import java.util.concurrent.locks.ReentrantLock;
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.DirectoryReader; // javadocs
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.MultiFields;
@@ -81,7 +81,7 @@ public class IndexSearcher {
   // NOTE: these members might change in incompatible ways
   // in the next release
   protected final IndexReaderContext readerContext;
-  protected final List<AtomicReaderContext> leafContexts;
+  protected final List<LeafReaderContext> leafContexts;
   /** used with executor - each slice holds a set of leafs executed within one thread */
   protected final LeafSlice[] leafSlices;
 
@@ -164,9 +164,9 @@ public class IndexSearcher {
   /**
    * Expert: Creates an array of leaf slices each holding a subset of the given leaves.
    * Each {@link LeafSlice} is executed in a single thread. By default there
-   * will be one {@link LeafSlice} per leaf ({@link AtomicReaderContext}).
+   * will be one {@link LeafSlice} per leaf ({@link org.apache.lucene.index.LeafReaderContext}).
    */
-  protected LeafSlice[] slices(List<AtomicReaderContext> leaves) {
+  protected LeafSlice[] slices(List<LeafReaderContext> leaves) {
     LeafSlice[] slices = new LeafSlice[leaves.size()];
     for (int i = 0; i < slices.length; i++) {
       slices[i] = new LeafSlice(leaves.get(i));
@@ -472,7 +472,7 @@ public class IndexSearcher {
    * @throws BooleanQuery.TooManyClauses If a query would exceed 
    *         {@link BooleanQuery#getMaxClauseCount()} clauses.
    */
-  protected TopDocs search(List<AtomicReaderContext> leaves, Weight weight, ScoreDoc after, int nDocs) throws IOException {
+  protected TopDocs search(List<LeafReaderContext> leaves, Weight weight, ScoreDoc after, int nDocs) throws IOException {
     // single thread
     int limit = reader.maxDoc();
     if (limit == 0) {
@@ -558,7 +558,7 @@ public class IndexSearcher {
    * whether or not the fields in the returned {@link FieldDoc} instances should
    * be set by specifying fillFields.
    */
-  protected TopFieldDocs search(List<AtomicReaderContext> leaves, Weight weight, FieldDoc after, int nDocs,
+  protected TopFieldDocs search(List<LeafReaderContext> leaves, Weight weight, FieldDoc after, int nDocs,
                                 Sort sort, boolean fillFields, boolean doDocScores, boolean doMaxScore) throws IOException {
     // single thread
     int limit = reader.maxDoc();
@@ -593,13 +593,13 @@ public class IndexSearcher {
    * @throws BooleanQuery.TooManyClauses If a query would exceed 
    *         {@link BooleanQuery#getMaxClauseCount()} clauses.
    */
-  protected void search(List<AtomicReaderContext> leaves, Weight weight, Collector collector)
+  protected void search(List<LeafReaderContext> leaves, Weight weight, Collector collector)
       throws IOException {
 
     // TODO: should we make this
     // threaded...?  the Collector could be sync'd?
     // always use single thread:
-    for (AtomicReaderContext ctx : leaves) { // search each subreader
+    for (LeafReaderContext ctx : leaves) { // search each subreader
       final LeafCollector leafCollector;
       try {
         leafCollector = collector.getLeafCollector(ctx);
@@ -659,7 +659,7 @@ public class IndexSearcher {
    */
   protected Explanation explain(Weight weight, int doc) throws IOException {
     int n = ReaderUtil.subIndex(doc, leafContexts);
-    final AtomicReaderContext ctx = leafContexts.get(n);
+    final LeafReaderContext ctx = leafContexts.get(n);
     int deBasedDoc = doc - ctx.docBase;
     
     return weight.explain(ctx, deBasedDoc);
@@ -778,7 +778,7 @@ public class IndexSearcher {
           weight, after, nDocs, sort, true, doDocScores || sort.needsScores(), doMaxScore);
       lock.lock();
       try {
-        final AtomicReaderContext ctx = slice.leaves[0];
+        final LeafReaderContext ctx = slice.leaves[0];
         final int base = ctx.docBase;
         final LeafCollector collector = hq.getLeafCollector(ctx);
         collector.setScorer(fakeScorer);
@@ -858,9 +858,9 @@ public class IndexSearcher {
    * @lucene.experimental
    */
   public static class LeafSlice {
-    final AtomicReaderContext[] leaves;
+    final LeafReaderContext[] leaves;
     
-    public LeafSlice(AtomicReaderContext... leaves) {
+    public LeafSlice(LeafReaderContext... leaves) {
       this.leaves = leaves;
     }
   }
diff --git lucene/core/src/java/org/apache/lucene/search/LeafCollector.java lucene/core/src/java/org/apache/lucene/search/LeafCollector.java
index 562e76d..5fd20d2 100644
--- lucene/core/src/java/org/apache/lucene/search/LeafCollector.java
+++ lucene/core/src/java/org/apache/lucene/search/LeafCollector.java
@@ -19,8 +19,6 @@ package org.apache.lucene.search;
 
 import java.io.IOException;
 
-import org.apache.lucene.index.AtomicReaderContext;
-
 /**
  * <p>Collector decouples the score from the collected doc:
  * the score computation is skipped entirely if it's not
@@ -90,7 +88,7 @@ public interface LeafCollector {
    * number.
    * <p>Note: The collection of the current segment can be terminated by throwing
    * a {@link CollectionTerminatedException}. In this case, the last docs of the
-   * current {@link AtomicReaderContext} will be skipped and {@link IndexSearcher}
+   * current {@link org.apache.lucene.index.LeafReaderContext} will be skipped and {@link IndexSearcher}
    * will swallow the exception and continue collection with the next leaf.
    * <p>
    * Note: This is called in an inner search loop. For good search performance,
diff --git lucene/core/src/java/org/apache/lucene/search/MatchAllDocsQuery.java lucene/core/src/java/org/apache/lucene/search/MatchAllDocsQuery.java
index 8f2edd7..d8b751b 100644
--- lucene/core/src/java/org/apache/lucene/search/MatchAllDocsQuery.java
+++ lucene/core/src/java/org/apache/lucene/search/MatchAllDocsQuery.java
@@ -17,7 +17,7 @@ package org.apache.lucene.search;
  * limitations under the License.
  */
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.util.ToStringUtils;
@@ -114,12 +114,12 @@ public class MatchAllDocsQuery extends Query {
     }
 
     @Override
-    public Scorer scorer(AtomicReaderContext context, Bits acceptDocs) throws IOException {
+    public Scorer scorer(LeafReaderContext context, Bits acceptDocs) throws IOException {
       return new MatchAllScorer(context.reader(), acceptDocs, this, queryWeight);
     }
 
     @Override
-    public Explanation explain(AtomicReaderContext context, int doc) {
+    public Explanation explain(LeafReaderContext context, int doc) {
       // explain query weight
       Explanation queryExpl = new ComplexExplanation
         (true, queryWeight, "MatchAllDocsQuery, product of:");
diff --git lucene/core/src/java/org/apache/lucene/search/MultiCollector.java lucene/core/src/java/org/apache/lucene/search/MultiCollector.java
index 859b893..b901515 100644
--- lucene/core/src/java/org/apache/lucene/search/MultiCollector.java
+++ lucene/core/src/java/org/apache/lucene/search/MultiCollector.java
@@ -20,9 +20,7 @@ package org.apache.lucene.search;
 import java.io.IOException;
 import java.util.Arrays;
 
-import org.apache.lucene.index.AtomicReaderContext;
-import org.apache.lucene.search.Collector;
-import org.apache.lucene.search.Scorer;
+import org.apache.lucene.index.LeafReaderContext;
 
 /**
  * A {@link Collector} which allows running a search with several
@@ -95,7 +93,7 @@ public class MultiCollector implements Collector {
   }
 
   @Override
-  public LeafCollector getLeafCollector(AtomicReaderContext context) throws IOException {
+  public LeafCollector getLeafCollector(LeafReaderContext context) throws IOException {
     final LeafCollector[] leafCollectors = new LeafCollector[collectors.length];
     for (int i = 0; i < collectors.length; ++i) {
       leafCollectors[i] = collectors[i].getLeafCollector(context);
diff --git lucene/core/src/java/org/apache/lucene/search/MultiPhraseQuery.java lucene/core/src/java/org/apache/lucene/search/MultiPhraseQuery.java
index 902e6aa..ffaa701 100644
--- lucene/core/src/java/org/apache/lucene/search/MultiPhraseQuery.java
+++ lucene/core/src/java/org/apache/lucene/search/MultiPhraseQuery.java
@@ -20,9 +20,9 @@ package org.apache.lucene.search;
 import java.io.IOException;
 import java.util.*;
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.DocsAndPositionsEnum;
-import org.apache.lucene.index.AtomicReader;
+import org.apache.lucene.index.LeafReader;
 import org.apache.lucene.index.DocsEnum;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexReaderContext;
@@ -179,9 +179,9 @@ public class MultiPhraseQuery extends Query {
     }
 
     @Override
-    public Scorer scorer(AtomicReaderContext context, Bits acceptDocs) throws IOException {
+    public Scorer scorer(LeafReaderContext context, Bits acceptDocs) throws IOException {
       assert !termArrays.isEmpty();
-      final AtomicReader reader = context.reader();
+      final LeafReader reader = context.reader();
       final Bits liveDocs = acceptDocs;
       
       PhraseQuery.PostingsAndFreq[] postingsFreqs = new PhraseQuery.PostingsAndFreq[termArrays.size()];
@@ -256,7 +256,7 @@ public class MultiPhraseQuery extends Query {
     }
 
     @Override
-    public Explanation explain(AtomicReaderContext context, int doc) throws IOException {
+    public Explanation explain(LeafReaderContext context, int doc) throws IOException {
       Scorer scorer = scorer(context, context.reader().getLiveDocs());
       if (scorer != null) {
         int newDoc = scorer.advance(doc);
@@ -473,7 +473,7 @@ class UnionDocsAndPositionsEnum extends DocsAndPositionsEnum {
   private IntQueue _posList;
   private long cost;
 
-  public UnionDocsAndPositionsEnum(Bits liveDocs, AtomicReaderContext context, Term[] terms, Map<Term,TermContext> termContexts, TermsEnum termsEnum) throws IOException {
+  public UnionDocsAndPositionsEnum(Bits liveDocs, LeafReaderContext context, Term[] terms, Map<Term,TermContext> termContexts, TermsEnum termsEnum) throws IOException {
     List<DocsAndPositionsEnum> docsEnums = new LinkedList<>();
     for (int i = 0; i < terms.length; i++) {
       final Term term = terms[i];
diff --git lucene/core/src/java/org/apache/lucene/search/MultiTermQueryWrapperFilter.java lucene/core/src/java/org/apache/lucene/search/MultiTermQueryWrapperFilter.java
index 0fea863..403d529 100644
--- lucene/core/src/java/org/apache/lucene/search/MultiTermQueryWrapperFilter.java
+++ lucene/core/src/java/org/apache/lucene/search/MultiTermQueryWrapperFilter.java
@@ -19,10 +19,10 @@ package org.apache.lucene.search;
 
 import java.io.IOException;
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.DocsEnum;
 import org.apache.lucene.index.Fields;
-import org.apache.lucene.index.AtomicReader;
+import org.apache.lucene.index.LeafReader;
 import org.apache.lucene.index.Terms;
 import org.apache.lucene.index.TermsEnum;
 import org.apache.lucene.util.FixedBitSet;
@@ -83,8 +83,8 @@ public class MultiTermQueryWrapperFilter<Q extends MultiTermQuery> extends Filte
    * results.
    */
   @Override
-  public DocIdSet getDocIdSet(AtomicReaderContext context, Bits acceptDocs) throws IOException {
-    final AtomicReader reader = context.reader();
+  public DocIdSet getDocIdSet(LeafReaderContext context, Bits acceptDocs) throws IOException {
+    final LeafReader reader = context.reader();
     final Fields fields = reader.fields();
     if (fields == null) {
       // reader has no fields
diff --git lucene/core/src/java/org/apache/lucene/search/PhraseQuery.java lucene/core/src/java/org/apache/lucene/search/PhraseQuery.java
index cdca801..bf5a373 100644
--- lucene/core/src/java/org/apache/lucene/search/PhraseQuery.java
+++ lucene/core/src/java/org/apache/lucene/search/PhraseQuery.java
@@ -22,11 +22,11 @@ import java.util.ArrayList;
 import java.util.Arrays;
 import java.util.Set;
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.DocsAndPositionsEnum;
 import org.apache.lucene.index.DocsEnum;
 import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.AtomicReader;
+import org.apache.lucene.index.LeafReader;
 import org.apache.lucene.index.IndexReaderContext;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.index.TermContext;
@@ -245,9 +245,9 @@ public class PhraseQuery extends Query {
     }
 
     @Override
-    public Scorer scorer(AtomicReaderContext context, Bits acceptDocs) throws IOException {
+    public Scorer scorer(LeafReaderContext context, Bits acceptDocs) throws IOException {
       assert !terms.isEmpty();
-      final AtomicReader reader = context.reader();
+      final LeafReader reader = context.reader();
       final Bits liveDocs = acceptDocs;
       PostingsAndFreq[] postingsFreqs = new PostingsAndFreq[terms.size()];
 
@@ -292,12 +292,12 @@ public class PhraseQuery extends Query {
     }
     
     // only called from assert
-    private boolean termNotInReader(AtomicReader reader, Term term) throws IOException {
+    private boolean termNotInReader(LeafReader reader, Term term) throws IOException {
       return reader.docFreq(term) == 0;
     }
 
     @Override
-    public Explanation explain(AtomicReaderContext context, int doc) throws IOException {
+    public Explanation explain(LeafReaderContext context, int doc) throws IOException {
       Scorer scorer = scorer(context, context.reader().getLiveDocs());
       if (scorer != null) {
         int newDoc = scorer.advance(doc);
diff --git lucene/core/src/java/org/apache/lucene/search/PositiveScoresOnlyCollector.java lucene/core/src/java/org/apache/lucene/search/PositiveScoresOnlyCollector.java
index ba22295..75840d4 100644
--- lucene/core/src/java/org/apache/lucene/search/PositiveScoresOnlyCollector.java
+++ lucene/core/src/java/org/apache/lucene/search/PositiveScoresOnlyCollector.java
@@ -19,7 +19,7 @@ package org.apache.lucene.search;
 
 import java.io.IOException;
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 
 /**
  * A {@link Collector} implementation which wraps another
@@ -33,7 +33,7 @@ public class PositiveScoresOnlyCollector extends FilterCollector {
   }
 
   @Override
-  public LeafCollector getLeafCollector(AtomicReaderContext context)
+  public LeafCollector getLeafCollector(LeafReaderContext context)
       throws IOException {
     return new FilterLeafCollector(super.getLeafCollector(context)) {
 
diff --git lucene/core/src/java/org/apache/lucene/search/QueryRescorer.java lucene/core/src/java/org/apache/lucene/search/QueryRescorer.java
index 755c3cd..2f17145 100644
--- lucene/core/src/java/org/apache/lucene/search/QueryRescorer.java
+++ lucene/core/src/java/org/apache/lucene/search/QueryRescorer.java
@@ -22,7 +22,7 @@ import java.util.Arrays;
 import java.util.Comparator;
 import java.util.List;
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 
 /** A {@link Rescorer} that uses a provided Query to assign
  *  scores to the first-pass hits.
@@ -58,7 +58,7 @@ public abstract class QueryRescorer extends Rescorer {
                   }
                 });
 
-    List<AtomicReaderContext> leaves = searcher.getIndexReader().leaves();
+    List<LeafReaderContext> leaves = searcher.getIndexReader().leaves();
 
     Weight weight = searcher.createNormalizedWeight(query);
 
@@ -72,7 +72,7 @@ public abstract class QueryRescorer extends Rescorer {
     while (hitUpto < hits.length) {
       ScoreDoc hit = hits[hitUpto];
       int docID = hit.doc;
-      AtomicReaderContext readerContext = null;
+      LeafReaderContext readerContext = null;
       while (docID >= endDoc) {
         readerUpto++;
         readerContext = leaves.get(readerUpto);
diff --git lucene/core/src/java/org/apache/lucene/search/QueryWrapperFilter.java lucene/core/src/java/org/apache/lucene/search/QueryWrapperFilter.java
index 50bc03e..8d8a010 100644
--- lucene/core/src/java/org/apache/lucene/search/QueryWrapperFilter.java
+++ lucene/core/src/java/org/apache/lucene/search/QueryWrapperFilter.java
@@ -19,7 +19,7 @@ package org.apache.lucene.search;
 
 import java.io.IOException;
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.util.Bits;
 
 /** 
@@ -49,9 +49,9 @@ public class QueryWrapperFilter extends Filter {
   }
 
   @Override
-  public DocIdSet getDocIdSet(final AtomicReaderContext context, final Bits acceptDocs) throws IOException {
+  public DocIdSet getDocIdSet(final LeafReaderContext context, final Bits acceptDocs) throws IOException {
     // get a private context that is used to rewrite, createWeight and score eventually
-    final AtomicReaderContext privateContext = context.reader().getContext();
+    final LeafReaderContext privateContext = context.reader().getContext();
     final Weight weight = new IndexSearcher(privateContext).createNormalizedWeight(query);
     return new DocIdSet() {
       @Override
diff --git lucene/core/src/java/org/apache/lucene/search/SimpleCollector.java lucene/core/src/java/org/apache/lucene/search/SimpleCollector.java
index 5803b2e..960d965 100644
--- lucene/core/src/java/org/apache/lucene/search/SimpleCollector.java
+++ lucene/core/src/java/org/apache/lucene/search/SimpleCollector.java
@@ -19,7 +19,7 @@ package org.apache.lucene.search;
 
 import java.io.IOException;
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 
 /**
  * Base {@link Collector} implementation that is used to collect all contexts.
@@ -29,13 +29,13 @@ import org.apache.lucene.index.AtomicReaderContext;
 public abstract class SimpleCollector implements Collector, LeafCollector {
 
   @Override
-  public final LeafCollector getLeafCollector(AtomicReaderContext context) throws IOException {
+  public final LeafCollector getLeafCollector(LeafReaderContext context) throws IOException {
     doSetNextReader(context);
     return this;
   }
 
   /** This method is called before collecting <code>context</code>. */
-  protected void doSetNextReader(AtomicReaderContext context) throws IOException {}
+  protected void doSetNextReader(LeafReaderContext context) throws IOException {}
 
   @Override
   public void setScorer(Scorer scorer) throws IOException {
diff --git lucene/core/src/java/org/apache/lucene/search/SortRescorer.java lucene/core/src/java/org/apache/lucene/search/SortRescorer.java
index 6f125e8..8d51440 100644
--- lucene/core/src/java/org/apache/lucene/search/SortRescorer.java
+++ lucene/core/src/java/org/apache/lucene/search/SortRescorer.java
@@ -22,7 +22,7 @@ import java.util.Arrays;
 import java.util.Comparator;
 import java.util.List;
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 
 /**
  * A {@link Rescorer} that re-sorts according to a provided
@@ -51,7 +51,7 @@ public class SortRescorer extends Rescorer {
                   }
                 });
 
-    List<AtomicReaderContext> leaves = searcher.getIndexReader().leaves();
+    List<LeafReaderContext> leaves = searcher.getIndexReader().leaves();
 
     TopFieldCollector collector = TopFieldCollector.create(sort, topN, true, true, true, false);
 
@@ -66,7 +66,7 @@ public class SortRescorer extends Rescorer {
     while (hitUpto < hits.length) {
       ScoreDoc hit = hits[hitUpto];
       int docID = hit.doc;
-      AtomicReaderContext readerContext = null;
+      LeafReaderContext readerContext = null;
       while (docID >= endDoc) {
         readerUpto++;
         readerContext = leaves.get(readerUpto);
diff --git lucene/core/src/java/org/apache/lucene/search/SortedNumericSortField.java lucene/core/src/java/org/apache/lucene/search/SortedNumericSortField.java
index f948463..eca8ab6 100644
--- lucene/core/src/java/org/apache/lucene/search/SortedNumericSortField.java
+++ lucene/core/src/java/org/apache/lucene/search/SortedNumericSortField.java
@@ -19,12 +19,10 @@ package org.apache.lucene.search;
 
 import java.io.IOException;
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.DocValues;
 import org.apache.lucene.index.NumericDocValues;
 import org.apache.lucene.index.SortedNumericDocValues;
-import org.apache.lucene.search.FieldComparator;
-import org.apache.lucene.search.SortField;
 
 /** 
  * SortField for {@link SortedNumericDocValues}.
@@ -139,28 +137,28 @@ public class SortedNumericSortField extends SortField {
       case INT:
         return new FieldComparator.IntComparator(numHits, getField(), (Integer) missingValue) {
           @Override
-          protected NumericDocValues getNumericDocValues(AtomicReaderContext context, String field) throws IOException {
+          protected NumericDocValues getNumericDocValues(LeafReaderContext context, String field) throws IOException {
             return SortedNumericSelector.wrap(DocValues.getSortedNumeric(context.reader(), field), selector, type);
           } 
         };
       case FLOAT:
         return new FieldComparator.FloatComparator(numHits, getField(), (Float) missingValue) {
           @Override
-          protected NumericDocValues getNumericDocValues(AtomicReaderContext context, String field) throws IOException {
+          protected NumericDocValues getNumericDocValues(LeafReaderContext context, String field) throws IOException {
             return SortedNumericSelector.wrap(DocValues.getSortedNumeric(context.reader(), field), selector, type);
           } 
         };
       case LONG:
         return new FieldComparator.LongComparator(numHits, getField(), (Long) missingValue) {
           @Override
-          protected NumericDocValues getNumericDocValues(AtomicReaderContext context, String field) throws IOException {
+          protected NumericDocValues getNumericDocValues(LeafReaderContext context, String field) throws IOException {
             return SortedNumericSelector.wrap(DocValues.getSortedNumeric(context.reader(), field), selector, type);
           }
         };
       case DOUBLE:
         return new FieldComparator.DoubleComparator(numHits, getField(), (Double) missingValue) {
           @Override
-          protected NumericDocValues getNumericDocValues(AtomicReaderContext context, String field) throws IOException {
+          protected NumericDocValues getNumericDocValues(LeafReaderContext context, String field) throws IOException {
             return SortedNumericSelector.wrap(DocValues.getSortedNumeric(context.reader(), field), selector, type);
           } 
         };
diff --git lucene/core/src/java/org/apache/lucene/search/SortedSetSortField.java lucene/core/src/java/org/apache/lucene/search/SortedSetSortField.java
index 157d4a3..402d99e 100644
--- lucene/core/src/java/org/apache/lucene/search/SortedSetSortField.java
+++ lucene/core/src/java/org/apache/lucene/search/SortedSetSortField.java
@@ -19,12 +19,10 @@ package org.apache.lucene.search;
 
 import java.io.IOException;
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.DocValues;
 import org.apache.lucene.index.SortedDocValues;
 import org.apache.lucene.index.SortedSetDocValues;
-import org.apache.lucene.search.FieldComparator;
-import org.apache.lucene.search.SortField;
 
 /** 
  * SortField for {@link SortedSetDocValues}.
@@ -124,7 +122,7 @@ public class SortedSetSortField extends SortField {
   public FieldComparator<?> getComparator(int numHits, int sortPos) throws IOException {
     return new FieldComparator.TermOrdValComparator(numHits, getField(), missingValue == STRING_LAST) {
       @Override
-      protected SortedDocValues getSortedDocValues(AtomicReaderContext context, String field) throws IOException {
+      protected SortedDocValues getSortedDocValues(LeafReaderContext context, String field) throws IOException {
         SortedSetDocValues sortedSet = DocValues.getSortedSet(context.reader(), field);
         return SortedSetSelector.wrap(sortedSet, selector);
       }
diff --git lucene/core/src/java/org/apache/lucene/search/TermCollectingRewrite.java lucene/core/src/java/org/apache/lucene/search/TermCollectingRewrite.java
index 07d9727..99b75dd 100644
--- lucene/core/src/java/org/apache/lucene/search/TermCollectingRewrite.java
+++ lucene/core/src/java/org/apache/lucene/search/TermCollectingRewrite.java
@@ -19,7 +19,7 @@ package org.apache.lucene.search;
 
 import java.io.IOException;
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.Fields;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexReaderContext;
@@ -46,7 +46,7 @@ abstract class TermCollectingRewrite<Q extends Query> extends MultiTermQuery.Rew
   
   final void collectTerms(IndexReader reader, MultiTermQuery query, TermCollector collector) throws IOException {
     IndexReaderContext topReaderContext = reader.getContext();
-    for (AtomicReaderContext context : topReaderContext.leaves()) {
+    for (LeafReaderContext context : topReaderContext.leaves()) {
       final Fields fields = context.reader().fields();
       if (fields == null) {
         // reader has no fields
@@ -77,10 +77,10 @@ abstract class TermCollectingRewrite<Q extends Query> extends MultiTermQuery.Rew
   
   static abstract class TermCollector {
     
-    protected AtomicReaderContext readerContext;
+    protected LeafReaderContext readerContext;
     protected IndexReaderContext topReaderContext;
 
-    public void setReaderContext(IndexReaderContext topReaderContext, AtomicReaderContext readerContext) {
+    public void setReaderContext(IndexReaderContext topReaderContext, LeafReaderContext readerContext) {
       this.readerContext = readerContext;
       this.topReaderContext = topReaderContext;
     }
diff --git lucene/core/src/java/org/apache/lucene/search/TermQuery.java lucene/core/src/java/org/apache/lucene/search/TermQuery.java
index 5435ccd..1bc2978 100644
--- lucene/core/src/java/org/apache/lucene/search/TermQuery.java
+++ lucene/core/src/java/org/apache/lucene/search/TermQuery.java
@@ -20,9 +20,9 @@ package org.apache.lucene.search;
 import java.io.IOException;
 import java.util.Set;
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.DocsEnum;
-import org.apache.lucene.index.AtomicReader;
+import org.apache.lucene.index.LeafReader;
 import org.apache.lucene.index.IndexReaderContext;
 import org.apache.lucene.index.ReaderUtil;
 import org.apache.lucene.index.Term;
@@ -75,7 +75,7 @@ public class TermQuery extends Query {
     }
 
     @Override
-    public Scorer scorer(AtomicReaderContext context, Bits acceptDocs) throws IOException {
+    public Scorer scorer(LeafReaderContext context, Bits acceptDocs) throws IOException {
       assert termStates.topReaderContext == ReaderUtil.getTopLevelContext(context) : "The top-reader used to create Weight (" + termStates.topReaderContext + ") is not the same as the current reader's top-reader (" + ReaderUtil.getTopLevelContext(context);
       final TermsEnum termsEnum = getTermsEnum(context);
       if (termsEnum == null) {
@@ -90,7 +90,7 @@ public class TermQuery extends Query {
      * Returns a {@link TermsEnum} positioned at this weights Term or null if
      * the term does not exist in the given context
      */
-    private TermsEnum getTermsEnum(AtomicReaderContext context) throws IOException {
+    private TermsEnum getTermsEnum(LeafReaderContext context) throws IOException {
       final TermState state = termStates.get(context.ord);
       if (state == null) { // term is not present in that reader
         assert termNotInReader(context.reader(), term) : "no termstate found but term exists in reader term=" + term;
@@ -102,14 +102,14 @@ public class TermQuery extends Query {
       return termsEnum;
     }
     
-    private boolean termNotInReader(AtomicReader reader, Term term) throws IOException {
+    private boolean termNotInReader(LeafReader reader, Term term) throws IOException {
       // only called from assert
       //System.out.println("TQ.termNotInReader reader=" + reader + " term=" + field + ":" + bytes.utf8ToString());
       return reader.docFreq(term) == 0;
     }
     
     @Override
-    public Explanation explain(AtomicReaderContext context, int doc) throws IOException {
+    public Explanation explain(LeafReaderContext context, int doc) throws IOException {
       Scorer scorer = scorer(context, context.reader().getLiveDocs());
       if (scorer != null) {
         int newDoc = scorer.advance(doc);
diff --git lucene/core/src/java/org/apache/lucene/search/TimeLimitingCollector.java lucene/core/src/java/org/apache/lucene/search/TimeLimitingCollector.java
index 9a08a2b..18ff25c 100644
--- lucene/core/src/java/org/apache/lucene/search/TimeLimitingCollector.java
+++ lucene/core/src/java/org/apache/lucene/search/TimeLimitingCollector.java
@@ -17,7 +17,7 @@ package org.apache.lucene.search;
  * limitations under the License.
  */
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.util.Counter;
 import org.apache.lucene.util.ThreadInterruptedException;
 
@@ -132,7 +132,7 @@ public class TimeLimitingCollector implements Collector {
   }
   
   @Override
-  public LeafCollector getLeafCollector(AtomicReaderContext context) throws IOException {
+  public LeafCollector getLeafCollector(LeafReaderContext context) throws IOException {
     this.docBase = context.docBase;
     if (Long.MIN_VALUE == t0) {
       setBaseline();
diff --git lucene/core/src/java/org/apache/lucene/search/TopFieldCollector.java lucene/core/src/java/org/apache/lucene/search/TopFieldCollector.java
index 6f038c4..092ad3c 100644
--- lucene/core/src/java/org/apache/lucene/search/TopFieldCollector.java
+++ lucene/core/src/java/org/apache/lucene/search/TopFieldCollector.java
@@ -19,7 +19,7 @@ package org.apache.lucene.search;
 
 import java.io.IOException;
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.search.FieldValueHitQueue.Entry;
 import org.apache.lucene.util.PriorityQueue;
 
@@ -92,7 +92,7 @@ public abstract class TopFieldCollector extends TopDocsCollector<Entry> {
     }
     
     @Override
-    protected void doSetNextReader(AtomicReaderContext context) throws IOException {
+    protected void doSetNextReader(LeafReaderContext context) throws IOException {
       this.docBase = context.docBase;
       queue.setComparator(0, comparator.setNextReader(context));
       comparator = queue.firstComparator;
@@ -446,7 +446,7 @@ public abstract class TopFieldCollector extends TopDocsCollector<Entry> {
     }
 
     @Override
-    protected void doSetNextReader(AtomicReaderContext context) throws IOException {
+    protected void doSetNextReader(LeafReaderContext context) throws IOException {
       docBase = context.docBase;
       for (int i = 0; i < comparators.length; i++) {
         queue.setComparator(i, comparators[i].setNextReader(context));
@@ -1001,7 +1001,7 @@ public abstract class TopFieldCollector extends TopDocsCollector<Entry> {
     }
 
     @Override
-    protected void doSetNextReader(AtomicReaderContext context) throws IOException {
+    protected void doSetNextReader(LeafReaderContext context) throws IOException {
       docBase = context.docBase;
       afterDoc = after.doc - docBase;
       for (int i = 0; i < comparators.length; i++) {
diff --git lucene/core/src/java/org/apache/lucene/search/TopScoreDocCollector.java lucene/core/src/java/org/apache/lucene/search/TopScoreDocCollector.java
index bfebeda..e343e67 100644
--- lucene/core/src/java/org/apache/lucene/search/TopScoreDocCollector.java
+++ lucene/core/src/java/org/apache/lucene/search/TopScoreDocCollector.java
@@ -19,7 +19,7 @@ package org.apache.lucene.search;
 
 import java.io.IOException;
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 
 /**
  * A {@link Collector} implementation that collects the top-scoring hits,
@@ -113,7 +113,7 @@ public abstract class TopScoreDocCollector extends TopDocsCollector<ScoreDoc> {
     }
 
     @Override
-    protected void doSetNextReader(AtomicReaderContext context) throws IOException {
+    protected void doSetNextReader(LeafReaderContext context) throws IOException {
       super.doSetNextReader(context);
       afterDoc = after.doc - context.docBase;
     }
@@ -208,7 +208,7 @@ public abstract class TopScoreDocCollector extends TopDocsCollector<ScoreDoc> {
     }
     
     @Override
-    protected void doSetNextReader(AtomicReaderContext context) throws IOException {
+    protected void doSetNextReader(LeafReaderContext context) throws IOException {
       super.doSetNextReader(context);
       afterDoc = after.doc - context.docBase;
     }
@@ -300,7 +300,7 @@ public abstract class TopScoreDocCollector extends TopDocsCollector<ScoreDoc> {
   }
   
   @Override
-  protected void doSetNextReader(AtomicReaderContext context) throws IOException {
+  protected void doSetNextReader(LeafReaderContext context) throws IOException {
     docBase = context.docBase;
   }
   
diff --git lucene/core/src/java/org/apache/lucene/search/Weight.java lucene/core/src/java/org/apache/lucene/search/Weight.java
index 8398157..422356c 100644
--- lucene/core/src/java/org/apache/lucene/search/Weight.java
+++ lucene/core/src/java/org/apache/lucene/search/Weight.java
@@ -19,8 +19,7 @@ package org.apache.lucene.search;
 
 import java.io.IOException;
 
-import org.apache.lucene.index.AtomicReader; // javadocs
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.IndexReaderContext; // javadocs
 import org.apache.lucene.search.similarities.Similarity;
 import org.apache.lucene.util.Bits;
@@ -32,10 +31,10 @@ import org.apache.lucene.util.Bits;
  * {@link Query}, so that a {@link Query} instance can be reused. <br>
  * {@link IndexSearcher} dependent state of the query should reside in the
  * {@link Weight}. <br>
- * {@link AtomicReader} dependent state should reside in the {@link Scorer}.
+ * {@link org.apache.lucene.index.LeafReader} dependent state should reside in the {@link Scorer}.
  * <p>
  * Since {@link Weight} creates {@link Scorer} instances for a given
- * {@link AtomicReaderContext} ({@link #scorer(AtomicReaderContext, Bits)})
+ * {@link org.apache.lucene.index.LeafReaderContext} ({@link #scorer(org.apache.lucene.index.LeafReaderContext, Bits)})
  * callers must maintain the relationship between the searcher's top-level
  * {@link IndexReaderContext} and the context used to create a {@link Scorer}. 
  * <p>
@@ -50,7 +49,7 @@ import org.apache.lucene.util.Bits;
  * <li>The query normalization factor is passed to {@link #normalize(float, float)}. At
  * this point the weighting is complete.
  * <li>A <code>Scorer</code> is constructed by
- * {@link #scorer(AtomicReaderContext, Bits)}.
+ * {@link #scorer(org.apache.lucene.index.LeafReaderContext, Bits)}.
  * </ol>
  * 
  * @since 2.9
@@ -65,7 +64,7 @@ public abstract class Weight {
    * @return an Explanation for the score
    * @throws IOException if an {@link IOException} occurs
    */
-  public abstract Explanation explain(AtomicReaderContext context, int doc) throws IOException;
+  public abstract Explanation explain(LeafReaderContext context, int doc) throws IOException;
 
   /** The query that this concerns. */
   public abstract Query getQuery();
@@ -89,7 +88,7 @@ public abstract class Weight {
    * query.
    * 
    * @param context
-   *          the {@link AtomicReaderContext} for which to return the {@link Scorer}.
+   *          the {@link org.apache.lucene.index.LeafReaderContext} for which to return the {@link Scorer}.
    * @param acceptDocs
    *          Bits that represent the allowable docs to match (typically deleted docs
    *          but possibly filtering other documents)
@@ -97,7 +96,7 @@ public abstract class Weight {
    * @return a {@link Scorer} which scores documents in/out-of order.
    * @throws IOException if there is a low-level I/O error
    */
-  public abstract Scorer scorer(AtomicReaderContext context, Bits acceptDocs) throws IOException;
+  public abstract Scorer scorer(LeafReaderContext context, Bits acceptDocs) throws IOException;
 
   /**
    * Optional method, to return a {@link BulkScorer} to
@@ -108,7 +107,7 @@ public abstract class Weight {
    * collects the resulting hits.
    *
    * @param context
-   *          the {@link AtomicReaderContext} for which to return the {@link Scorer}.
+   *          the {@link org.apache.lucene.index.LeafReaderContext} for which to return the {@link Scorer}.
    * @param scoreDocsInOrder
    *          specifies whether in-order scoring of documents is required. Note
    *          that if set to false (i.e., out-of-order scoring is required),
@@ -126,7 +125,7 @@ public abstract class Weight {
    * passes them to a collector.
    * @throws IOException if there is a low-level I/O error
    */
-  public BulkScorer bulkScorer(AtomicReaderContext context, boolean scoreDocsInOrder, Bits acceptDocs) throws IOException {
+  public BulkScorer bulkScorer(LeafReaderContext context, boolean scoreDocsInOrder, Bits acceptDocs) throws IOException {
 
     Scorer scorer = scorer(context, acceptDocs);
     if (scorer == null) {
@@ -199,7 +198,7 @@ public abstract class Weight {
    * Returns true iff this implementation scores docs only out of order. This
    * method is used in conjunction with {@link Collector}'s
    * {@link LeafCollector#acceptsDocsOutOfOrder() acceptsDocsOutOfOrder} and
-   * {@link #bulkScorer(AtomicReaderContext, boolean, Bits)} to
+   * {@link #bulkScorer(org.apache.lucene.index.LeafReaderContext, boolean, Bits)} to
    * create a matching {@link Scorer} instance for a given {@link Collector}, or
    * vice versa.
    * <p>
diff --git lucene/core/src/java/org/apache/lucene/search/payloads/PayloadNearQuery.java lucene/core/src/java/org/apache/lucene/search/payloads/PayloadNearQuery.java
index d2e924e..1be23e6 100644
--- lucene/core/src/java/org/apache/lucene/search/payloads/PayloadNearQuery.java
+++ lucene/core/src/java/org/apache/lucene/search/payloads/PayloadNearQuery.java
@@ -17,7 +17,7 @@ package org.apache.lucene.search.payloads;
  * limitations under the License.
  */
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.search.ComplexExplanation;
 import org.apache.lucene.search.Explanation;
 import org.apache.lucene.search.Scorer;
@@ -148,13 +148,13 @@ public class PayloadNearQuery extends SpanNearQuery {
     }
 
     @Override
-    public Scorer scorer(AtomicReaderContext context, Bits acceptDocs) throws IOException {
+    public Scorer scorer(LeafReaderContext context, Bits acceptDocs) throws IOException {
       return new PayloadNearSpanScorer(query.getSpans(context, acceptDocs, termContexts), this,
           similarity, similarity.simScorer(stats, context));
     }
     
     @Override
-    public Explanation explain(AtomicReaderContext context, int doc) throws IOException {
+    public Explanation explain(LeafReaderContext context, int doc) throws IOException {
       PayloadNearSpanScorer scorer = (PayloadNearSpanScorer) scorer(context, context.reader().getLiveDocs());
       if (scorer != null) {
         int newDoc = scorer.advance(doc);
diff --git lucene/core/src/java/org/apache/lucene/search/payloads/PayloadSpanUtil.java lucene/core/src/java/org/apache/lucene/search/payloads/PayloadSpanUtil.java
index e48f4f9..0329acc 100644
--- lucene/core/src/java/org/apache/lucene/search/payloads/PayloadSpanUtil.java
+++ lucene/core/src/java/org/apache/lucene/search/payloads/PayloadSpanUtil.java
@@ -26,7 +26,7 @@ import java.util.List;
 import java.util.Map;
 import java.util.TreeSet;
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexReaderContext;
 import org.apache.lucene.index.Term;
@@ -184,8 +184,8 @@ public class PayloadSpanUtil {
     for (Term term : terms) {
       termContexts.put(term, TermContext.build(context, term));
     }
-    for (AtomicReaderContext atomicReaderContext : context.leaves()) {
-      final Spans spans = query.getSpans(atomicReaderContext, atomicReaderContext.reader().getLiveDocs(), termContexts);
+    for (LeafReaderContext leafReaderContext : context.leaves()) {
+      final Spans spans = query.getSpans(leafReaderContext, leafReaderContext.reader().getLiveDocs(), termContexts);
       while (spans.next() == true) {
         if (spans.isPayloadAvailable()) {
           Collection<byte[]> payload = spans.getPayload();
diff --git lucene/core/src/java/org/apache/lucene/search/payloads/PayloadTermQuery.java lucene/core/src/java/org/apache/lucene/search/payloads/PayloadTermQuery.java
index 04ecd80..6afbdf2 100644
--- lucene/core/src/java/org/apache/lucene/search/payloads/PayloadTermQuery.java
+++ lucene/core/src/java/org/apache/lucene/search/payloads/PayloadTermQuery.java
@@ -17,7 +17,7 @@ package org.apache.lucene.search.payloads;
  * limitations under the License.
  */
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.index.DocsAndPositionsEnum;
 import org.apache.lucene.search.IndexSearcher;
@@ -79,7 +79,7 @@ public class PayloadTermQuery extends SpanTermQuery {
     }
 
     @Override
-    public Scorer scorer(AtomicReaderContext context, Bits acceptDocs) throws IOException {
+    public Scorer scorer(LeafReaderContext context, Bits acceptDocs) throws IOException {
       return new PayloadTermSpanScorer((TermSpans) query.getSpans(context, acceptDocs, termContexts),
           this, similarity.simScorer(stats, context));
     }
@@ -175,7 +175,7 @@ public class PayloadTermQuery extends SpanTermQuery {
     }
     
     @Override
-    public Explanation explain(AtomicReaderContext context, int doc) throws IOException {
+    public Explanation explain(LeafReaderContext context, int doc) throws IOException {
       PayloadTermSpanScorer scorer = (PayloadTermSpanScorer) scorer(context, context.reader().getLiveDocs());
       if (scorer != null) {
         int newDoc = scorer.advance(doc);
diff --git lucene/core/src/java/org/apache/lucene/search/similarities/BM25Similarity.java lucene/core/src/java/org/apache/lucene/search/similarities/BM25Similarity.java
index d062015..9f58ebe 100644
--- lucene/core/src/java/org/apache/lucene/search/similarities/BM25Similarity.java
+++ lucene/core/src/java/org/apache/lucene/search/similarities/BM25Similarity.java
@@ -19,7 +19,7 @@ package org.apache.lucene.search.similarities;
 
 import java.io.IOException;
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.FieldInvertState;
 import org.apache.lucene.index.NumericDocValues;
 import org.apache.lucene.search.CollectionStatistics;
@@ -212,7 +212,7 @@ public class BM25Similarity extends Similarity {
   }
 
   @Override
-  public final SimScorer simScorer(SimWeight stats, AtomicReaderContext context) throws IOException {
+  public final SimScorer simScorer(SimWeight stats, LeafReaderContext context) throws IOException {
     BM25Stats bm25stats = (BM25Stats) stats;
     return new BM25DocScorer(bm25stats, context.reader().getNormValues(bm25stats.field));
   }
diff --git lucene/core/src/java/org/apache/lucene/search/similarities/MultiSimilarity.java lucene/core/src/java/org/apache/lucene/search/similarities/MultiSimilarity.java
index 507c568..1870282 100644
--- lucene/core/src/java/org/apache/lucene/search/similarities/MultiSimilarity.java
+++ lucene/core/src/java/org/apache/lucene/search/similarities/MultiSimilarity.java
@@ -19,7 +19,7 @@ package org.apache.lucene.search.similarities;
 
 import java.io.IOException;
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.FieldInvertState;
 import org.apache.lucene.search.CollectionStatistics;
 import org.apache.lucene.search.Explanation;
@@ -57,7 +57,7 @@ public class MultiSimilarity extends Similarity {
   }
 
   @Override
-  public SimScorer simScorer(SimWeight stats, AtomicReaderContext context) throws IOException {
+  public SimScorer simScorer(SimWeight stats, LeafReaderContext context) throws IOException {
     SimScorer subScorers[] = new SimScorer[sims.length];
     for (int i = 0; i < subScorers.length; i++) {
       subScorers[i] = sims[i].simScorer(((MultiStats)stats).subStats[i], context);
diff --git lucene/core/src/java/org/apache/lucene/search/similarities/PerFieldSimilarityWrapper.java lucene/core/src/java/org/apache/lucene/search/similarities/PerFieldSimilarityWrapper.java
index 17a461e..282be56 100644
--- lucene/core/src/java/org/apache/lucene/search/similarities/PerFieldSimilarityWrapper.java
+++ lucene/core/src/java/org/apache/lucene/search/similarities/PerFieldSimilarityWrapper.java
@@ -19,7 +19,7 @@ package org.apache.lucene.search.similarities;
 
 import java.io.IOException;
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.FieldInvertState;
 import org.apache.lucene.search.CollectionStatistics;
 import org.apache.lucene.search.TermStatistics;
@@ -54,7 +54,7 @@ public abstract class PerFieldSimilarityWrapper extends Similarity {
   }
 
   @Override
-  public final SimScorer simScorer(SimWeight weight, AtomicReaderContext context) throws IOException {
+  public final SimScorer simScorer(SimWeight weight, LeafReaderContext context) throws IOException {
     PerFieldSimWeight perFieldWeight = (PerFieldSimWeight) weight;
     return perFieldWeight.delegate.simScorer(perFieldWeight.delegateWeight, context);
   }
diff --git lucene/core/src/java/org/apache/lucene/search/similarities/Similarity.java lucene/core/src/java/org/apache/lucene/search/similarities/Similarity.java
index b4ff8bb..cc7da6f 100644
--- lucene/core/src/java/org/apache/lucene/search/similarities/Similarity.java
+++ lucene/core/src/java/org/apache/lucene/search/similarities/Similarity.java
@@ -19,8 +19,7 @@ package org.apache.lucene.search.similarities;
 
 import java.io.IOException;
 
-import org.apache.lucene.index.AtomicReader; // javadoc
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.FieldInvertState;
 import org.apache.lucene.search.BooleanQuery;
 import org.apache.lucene.search.CollectionStatistics;
@@ -51,7 +50,7 @@ import org.apache.lucene.util.SmallFloat; // javadoc
  * <a name="indextime"/>
  * At indexing time, the indexer calls {@link #computeNorm(FieldInvertState)}, allowing
  * the Similarity implementation to set a per-document value for the field that will 
- * be later accessible via {@link AtomicReader#getNormValues(String)}.  Lucene makes no assumption
+ * be later accessible via {@link org.apache.lucene.index.LeafReader#getNormValues(String)}.  Lucene makes no assumption
  * about what is in this norm, but it is most useful for encoding length normalization 
  * information.
  * <p>
@@ -67,7 +66,7 @@ import org.apache.lucene.util.SmallFloat; // javadoc
  * <p>
  * Additional scoring factors can be stored in named
  * <code>NumericDocValuesField</code>s and accessed
- * at query-time with {@link AtomicReader#getNumericDocValues(String)}.
+ * at query-time with {@link org.apache.lucene.index.LeafReader#getNumericDocValues(String)}.
  * <p>
  * Finally, using index-time boosts (either via folding into the normalization byte or
  * via DocValues), is an inefficient way to boost the scores of different fields if the
@@ -88,7 +87,7 @@ import org.apache.lucene.util.SmallFloat; // javadoc
  *       is called for each query leaf node, {@link Similarity#queryNorm(float)} is called for the top-level
  *       query, and finally {@link Similarity.SimWeight#normalize(float, float)} passes down the normalization value
  *       and any top-level boosts (e.g. from enclosing {@link BooleanQuery}s).
- *   <li>For each segment in the index, the Query creates a {@link #simScorer(SimWeight, AtomicReaderContext)}
+ *   <li>For each segment in the index, the Query creates a {@link #simScorer(SimWeight, org.apache.lucene.index.LeafReaderContext)}
  *       The score() method is called for each matching document.
  * </ol>
  * <p>
@@ -172,7 +171,7 @@ public abstract class Similarity {
    * @return SloppySimScorer for scoring documents across <code>context</code>
    * @throws IOException if there is a low-level I/O error
    */
-  public abstract SimScorer simScorer(SimWeight weight, AtomicReaderContext context) throws IOException;
+  public abstract SimScorer simScorer(SimWeight weight, LeafReaderContext context) throws IOException;
   
   /**
    * API for scoring "sloppy" queries such as {@link TermQuery},
diff --git lucene/core/src/java/org/apache/lucene/search/similarities/SimilarityBase.java lucene/core/src/java/org/apache/lucene/search/similarities/SimilarityBase.java
index 72806df..4e99e1c 100644
--- lucene/core/src/java/org/apache/lucene/search/similarities/SimilarityBase.java
+++ lucene/core/src/java/org/apache/lucene/search/similarities/SimilarityBase.java
@@ -19,7 +19,7 @@ package org.apache.lucene.search.similarities;
 
 import java.io.IOException;
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.FieldInvertState;
 import org.apache.lucene.index.NumericDocValues;
 import org.apache.lucene.search.CollectionStatistics;
@@ -190,7 +190,7 @@ public abstract class SimilarityBase extends Similarity {
   }
   
   @Override
-  public SimScorer simScorer(SimWeight stats, AtomicReaderContext context) throws IOException {
+  public SimScorer simScorer(SimWeight stats, LeafReaderContext context) throws IOException {
     if (stats instanceof MultiSimilarity.MultiStats) {
       // a multi term query (e.g. phrase). return the summation, 
       // scoring almost as if it were boolean query
diff --git lucene/core/src/java/org/apache/lucene/search/similarities/TFIDFSimilarity.java lucene/core/src/java/org/apache/lucene/search/similarities/TFIDFSimilarity.java
index 1d5c9d3..236777c 100644
--- lucene/core/src/java/org/apache/lucene/search/similarities/TFIDFSimilarity.java
+++ lucene/core/src/java/org/apache/lucene/search/similarities/TFIDFSimilarity.java
@@ -19,7 +19,7 @@ package org.apache.lucene.search.similarities;
 
 import java.io.IOException;
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.FieldInvertState;
 import org.apache.lucene.index.NumericDocValues;
 import org.apache.lucene.search.CollectionStatistics;
@@ -693,7 +693,7 @@ public abstract class TFIDFSimilarity extends Similarity {
   }
 
   @Override
-  public final SimScorer simScorer(SimWeight stats, AtomicReaderContext context) throws IOException {
+  public final SimScorer simScorer(SimWeight stats, LeafReaderContext context) throws IOException {
     IDFStats idfstats = (IDFStats) stats;
     return new TFIDFSimScorer(idfstats, context.reader().getNormValues(idfstats.field));
   }
diff --git lucene/core/src/java/org/apache/lucene/search/spans/FieldMaskingSpanQuery.java lucene/core/src/java/org/apache/lucene/search/spans/FieldMaskingSpanQuery.java
index f65ba56..b2c9cd5 100644
--- lucene/core/src/java/org/apache/lucene/search/spans/FieldMaskingSpanQuery.java
+++ lucene/core/src/java/org/apache/lucene/search/spans/FieldMaskingSpanQuery.java
@@ -21,7 +21,7 @@ import java.io.IOException;
 import java.util.Map;
 import java.util.Set;
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.index.TermContext;
@@ -96,7 +96,7 @@ public class FieldMaskingSpanQuery extends SpanQuery {
   // ...this is done to be more consistent with things like SpanFirstQuery
   
   @Override
-  public Spans getSpans(AtomicReaderContext context, Bits acceptDocs, Map<Term,TermContext> termContexts) throws IOException {
+  public Spans getSpans(LeafReaderContext context, Bits acceptDocs, Map<Term,TermContext> termContexts) throws IOException {
     return maskedQuery.getSpans(context, acceptDocs, termContexts);
   }
 
diff --git lucene/core/src/java/org/apache/lucene/search/spans/NearSpansOrdered.java lucene/core/src/java/org/apache/lucene/search/spans/NearSpansOrdered.java
index aea50c6..6e804a8 100644
--- lucene/core/src/java/org/apache/lucene/search/spans/NearSpansOrdered.java
+++ lucene/core/src/java/org/apache/lucene/search/spans/NearSpansOrdered.java
@@ -17,7 +17,7 @@ package org.apache.lucene.search.spans;
  * limitations under the License.
  */
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.index.TermContext;
 import org.apache.lucene.util.ArrayUtil;
@@ -26,7 +26,6 @@ import org.apache.lucene.util.InPlaceMergeSorter;
 
 import java.io.IOException;
 import java.util.ArrayList;
-import java.util.Comparator;
 import java.util.HashSet;
 import java.util.LinkedList;
 import java.util.List;
@@ -89,11 +88,11 @@ public class NearSpansOrdered extends Spans {
   private SpanNearQuery query;
   private boolean collectPayloads = true;
   
-  public NearSpansOrdered(SpanNearQuery spanNearQuery, AtomicReaderContext context, Bits acceptDocs, Map<Term,TermContext> termContexts) throws IOException {
+  public NearSpansOrdered(SpanNearQuery spanNearQuery, LeafReaderContext context, Bits acceptDocs, Map<Term,TermContext> termContexts) throws IOException {
     this(spanNearQuery, context, acceptDocs, termContexts, true);
   }
 
-  public NearSpansOrdered(SpanNearQuery spanNearQuery, AtomicReaderContext context, Bits acceptDocs, Map<Term,TermContext> termContexts, boolean collectPayloads)
+  public NearSpansOrdered(SpanNearQuery spanNearQuery, LeafReaderContext context, Bits acceptDocs, Map<Term,TermContext> termContexts, boolean collectPayloads)
   throws IOException {
     if (spanNearQuery.getClauses().length < 2) {
       throw new IllegalArgumentException("Less than 2 clauses: "
diff --git lucene/core/src/java/org/apache/lucene/search/spans/NearSpansUnordered.java lucene/core/src/java/org/apache/lucene/search/spans/NearSpansUnordered.java
index 544d932..3c82cd2 100644
--- lucene/core/src/java/org/apache/lucene/search/spans/NearSpansUnordered.java
+++ lucene/core/src/java/org/apache/lucene/search/spans/NearSpansUnordered.java
@@ -17,7 +17,7 @@ package org.apache.lucene.search.spans;
  * limitations under the License.
  */
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.index.TermContext;
 import org.apache.lucene.util.Bits;
@@ -140,7 +140,7 @@ public class NearSpansUnordered extends Spans {
   }
 
 
-  public NearSpansUnordered(SpanNearQuery query, AtomicReaderContext context, Bits acceptDocs, Map<Term,TermContext> termContexts)
+  public NearSpansUnordered(SpanNearQuery query, LeafReaderContext context, Bits acceptDocs, Map<Term,TermContext> termContexts)
     throws IOException {
     this.query = query;
     this.slop = query.getSlop();
diff --git lucene/core/src/java/org/apache/lucene/search/spans/SpanMultiTermQueryWrapper.java lucene/core/src/java/org/apache/lucene/search/spans/SpanMultiTermQueryWrapper.java
index 849732a..b300b51 100644
--- lucene/core/src/java/org/apache/lucene/search/spans/SpanMultiTermQueryWrapper.java
+++ lucene/core/src/java/org/apache/lucene/search/spans/SpanMultiTermQueryWrapper.java
@@ -20,7 +20,7 @@ package org.apache.lucene.search.spans;
 import java.io.IOException;
 import java.util.Map;
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.index.TermContext;
@@ -92,7 +92,7 @@ public class SpanMultiTermQueryWrapper<Q extends MultiTermQuery> extends SpanQue
   }
   
   @Override
-  public Spans getSpans(AtomicReaderContext context, Bits acceptDocs, Map<Term,TermContext> termContexts) throws IOException {
+  public Spans getSpans(LeafReaderContext context, Bits acceptDocs, Map<Term,TermContext> termContexts) throws IOException {
     throw new UnsupportedOperationException("Query should have been rewritten");
   }
 
diff --git lucene/core/src/java/org/apache/lucene/search/spans/SpanNearQuery.java lucene/core/src/java/org/apache/lucene/search/spans/SpanNearQuery.java
index 0774d35..4f6f212 100644
--- lucene/core/src/java/org/apache/lucene/search/spans/SpanNearQuery.java
+++ lucene/core/src/java/org/apache/lucene/search/spans/SpanNearQuery.java
@@ -27,7 +27,7 @@ import java.util.Map;
 import java.util.Set;
 
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.index.TermContext;
@@ -120,7 +120,7 @@ public class SpanNearQuery extends SpanQuery implements Cloneable {
   }
 
   @Override
-  public Spans getSpans(final AtomicReaderContext context, Bits acceptDocs, Map<Term,TermContext> termContexts) throws IOException {
+  public Spans getSpans(final LeafReaderContext context, Bits acceptDocs, Map<Term,TermContext> termContexts) throws IOException {
     if (clauses.size() == 0)                      // optimize 0-clause case
       return new SpanOrQuery(getClauses()).getSpans(context, acceptDocs, termContexts);
 
diff --git lucene/core/src/java/org/apache/lucene/search/spans/SpanNotQuery.java lucene/core/src/java/org/apache/lucene/search/spans/SpanNotQuery.java
index 16ae1c6..88c439d 100644
--- lucene/core/src/java/org/apache/lucene/search/spans/SpanNotQuery.java
+++ lucene/core/src/java/org/apache/lucene/search/spans/SpanNotQuery.java
@@ -17,7 +17,7 @@ package org.apache.lucene.search.spans;
  * limitations under the License.
  */
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.index.TermContext;
@@ -103,7 +103,7 @@ public class SpanNotQuery extends SpanQuery implements Cloneable {
   }
 
   @Override
-  public Spans getSpans(final AtomicReaderContext context, final Bits acceptDocs, final Map<Term,TermContext> termContexts) throws IOException {
+  public Spans getSpans(final LeafReaderContext context, final Bits acceptDocs, final Map<Term,TermContext> termContexts) throws IOException {
     return new Spans() {
         private Spans includeSpans = include.getSpans(context, acceptDocs, termContexts);
         private boolean moreInclude = true;
diff --git lucene/core/src/java/org/apache/lucene/search/spans/SpanOrQuery.java lucene/core/src/java/org/apache/lucene/search/spans/SpanOrQuery.java
index 5f8bcbc..2b617e4 100644
--- lucene/core/src/java/org/apache/lucene/search/spans/SpanOrQuery.java
+++ lucene/core/src/java/org/apache/lucene/search/spans/SpanOrQuery.java
@@ -26,7 +26,7 @@ import java.util.Iterator;
 import java.util.Map;
 import java.util.Set;
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.index.TermContext;
@@ -165,7 +165,7 @@ public class SpanOrQuery extends SpanQuery implements Cloneable {
   }
 
   @Override
-  public Spans getSpans(final AtomicReaderContext context, final Bits acceptDocs, final Map<Term,TermContext> termContexts) throws IOException {
+  public Spans getSpans(final LeafReaderContext context, final Bits acceptDocs, final Map<Term,TermContext> termContexts) throws IOException {
     if (clauses.size() == 1)                      // optimize 1-clause case
       return (clauses.get(0)).getSpans(context, acceptDocs, termContexts);
 
diff --git lucene/core/src/java/org/apache/lucene/search/spans/SpanPositionCheckQuery.java lucene/core/src/java/org/apache/lucene/search/spans/SpanPositionCheckQuery.java
index 7c0994e..a41442d 100644
--- lucene/core/src/java/org/apache/lucene/search/spans/SpanPositionCheckQuery.java
+++ lucene/core/src/java/org/apache/lucene/search/spans/SpanPositionCheckQuery.java
@@ -17,7 +17,7 @@ package org.apache.lucene.search.spans;
  */
 
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.index.TermContext;
@@ -94,7 +94,7 @@ public abstract class SpanPositionCheckQuery extends SpanQuery implements Clonea
   protected abstract AcceptStatus acceptPosition(Spans spans) throws IOException;
 
   @Override
-  public Spans getSpans(final AtomicReaderContext context, Bits acceptDocs, Map<Term,TermContext> termContexts) throws IOException {
+  public Spans getSpans(final LeafReaderContext context, Bits acceptDocs, Map<Term,TermContext> termContexts) throws IOException {
     return new PositionCheckSpan(context, acceptDocs, termContexts);
   }
 
@@ -119,7 +119,7 @@ public abstract class SpanPositionCheckQuery extends SpanQuery implements Clonea
   protected class PositionCheckSpan extends Spans {
     private Spans spans;
 
-    public PositionCheckSpan(AtomicReaderContext context, Bits acceptDocs, Map<Term,TermContext> termContexts) throws IOException {
+    public PositionCheckSpan(LeafReaderContext context, Bits acceptDocs, Map<Term,TermContext> termContexts) throws IOException {
       spans = match.getSpans(context, acceptDocs, termContexts);
     }
 
diff --git lucene/core/src/java/org/apache/lucene/search/spans/SpanQuery.java lucene/core/src/java/org/apache/lucene/search/spans/SpanQuery.java
index d617616..f30c467 100644
--- lucene/core/src/java/org/apache/lucene/search/spans/SpanQuery.java
+++ lucene/core/src/java/org/apache/lucene/search/spans/SpanQuery.java
@@ -20,7 +20,7 @@ package org.apache.lucene.search.spans;
 import java.io.IOException;
 import java.util.Map;
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.index.TermContext;
 import org.apache.lucene.search.Query;
@@ -32,7 +32,7 @@ import org.apache.lucene.util.Bits;
 public abstract class SpanQuery extends Query {
   /** Expert: Returns the matches for this query in an index.  Used internally
    * to search for spans. */
-  public abstract Spans getSpans(AtomicReaderContext context, Bits acceptDocs, Map<Term,TermContext> termContexts) throws IOException;
+  public abstract Spans getSpans(LeafReaderContext context, Bits acceptDocs, Map<Term,TermContext> termContexts) throws IOException;
 
   /** 
    * Returns the name of the field matched by this query.
diff --git lucene/core/src/java/org/apache/lucene/search/spans/SpanTermQuery.java lucene/core/src/java/org/apache/lucene/search/spans/SpanTermQuery.java
index f0a27c4..f51a4d0 100644
--- lucene/core/src/java/org/apache/lucene/search/spans/SpanTermQuery.java
+++ lucene/core/src/java/org/apache/lucene/search/spans/SpanTermQuery.java
@@ -17,7 +17,7 @@ package org.apache.lucene.search.spans;
  * limitations under the License.
  */
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.Fields;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.index.DocsAndPositionsEnum;
@@ -87,7 +87,7 @@ public class SpanTermQuery extends SpanQuery {
   }
 
   @Override
-  public Spans getSpans(final AtomicReaderContext context, Bits acceptDocs, Map<Term,TermContext> termContexts) throws IOException {
+  public Spans getSpans(final LeafReaderContext context, Bits acceptDocs, Map<Term,TermContext> termContexts) throws IOException {
     TermContext termContext = termContexts.get(term);
     final TermState state;
     if (termContext == null) {
diff --git lucene/core/src/java/org/apache/lucene/search/spans/SpanWeight.java lucene/core/src/java/org/apache/lucene/search/spans/SpanWeight.java
index 0b20cdb..0e06343 100644
--- lucene/core/src/java/org/apache/lucene/search/spans/SpanWeight.java
+++ lucene/core/src/java/org/apache/lucene/search/spans/SpanWeight.java
@@ -17,7 +17,7 @@ package org.apache.lucene.search.spans;
  * limitations under the License.
  */
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.IndexReaderContext;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.index.TermContext;
@@ -81,7 +81,7 @@ public class SpanWeight extends Weight {
   }
 
   @Override
-  public Scorer scorer(AtomicReaderContext context, Bits acceptDocs) throws IOException {
+  public Scorer scorer(LeafReaderContext context, Bits acceptDocs) throws IOException {
     if (stats == null) {
       return null;
     } else {
@@ -90,7 +90,7 @@ public class SpanWeight extends Weight {
   }
 
   @Override
-  public Explanation explain(AtomicReaderContext context, int doc) throws IOException {
+  public Explanation explain(LeafReaderContext context, int doc) throws IOException {
     SpanScorer scorer = (SpanScorer) scorer(context, context.reader().getLiveDocs());
     if (scorer != null) {
       int newDoc = scorer.advance(doc);
diff --git lucene/core/src/java/org/apache/lucene/util/NumericUtils.java lucene/core/src/java/org/apache/lucene/util/NumericUtils.java
index 4380e36..a415b49 100644
--- lucene/core/src/java/org/apache/lucene/util/NumericUtils.java
+++ lucene/core/src/java/org/apache/lucene/util/NumericUtils.java
@@ -24,7 +24,7 @@ import org.apache.lucene.document.DoubleField; // javadocs
 import org.apache.lucene.document.FloatField; // javadocs
 import org.apache.lucene.document.IntField; // javadocs
 import org.apache.lucene.document.LongField; // javadocs
-import org.apache.lucene.index.FilterAtomicReader;
+import org.apache.lucene.index.FilterLeafReader;
 import org.apache.lucene.index.FilteredTermsEnum;
 import org.apache.lucene.index.Terms;
 import org.apache.lucene.index.TermsEnum;
@@ -532,7 +532,7 @@ public final class NumericUtils {
   }
 
   private static Terms intTerms(Terms terms) {
-    return new FilterAtomicReader.FilterTerms(terms) {
+    return new FilterLeafReader.FilterTerms(terms) {
         @Override
         public TermsEnum iterator(TermsEnum reuse) throws IOException {
           return filterPrefixCodedInts(in.iterator(reuse));
@@ -541,7 +541,7 @@ public final class NumericUtils {
   }
 
   private static Terms longTerms(Terms terms) {
-    return new FilterAtomicReader.FilterTerms(terms) {
+    return new FilterLeafReader.FilterTerms(terms) {
         @Override
         public TermsEnum iterator(TermsEnum reuse) throws IOException {
           return filterPrefixCodedLongs(in.iterator(reuse));
diff --git lucene/core/src/test/org/apache/lucene/analysis/TestMockAnalyzer.java lucene/core/src/test/org/apache/lucene/analysis/TestMockAnalyzer.java
index 957b4d1..f3fadb9 100644
--- lucene/core/src/test/org/apache/lucene/analysis/TestMockAnalyzer.java
+++ lucene/core/src/test/org/apache/lucene/analysis/TestMockAnalyzer.java
@@ -25,7 +25,7 @@ import java.util.Random;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.FieldType;
-import org.apache.lucene.index.AtomicReader;
+import org.apache.lucene.index.LeafReader;
 import org.apache.lucene.index.DocsAndPositionsEnum;
 import org.apache.lucene.index.FieldInfo.IndexOptions;
 import org.apache.lucene.index.Fields;
@@ -314,7 +314,7 @@ public class TestMockAnalyzer extends BaseTokenStreamTestCase {
     doc.add(new Field("f", "a", ft));
     doc.add(new Field("f", "a", ft));
     writer.addDocument(doc, a);
-    final AtomicReader reader = getOnlySegmentReader(writer.getReader());
+    final LeafReader reader = getOnlySegmentReader(writer.getReader());
     final Fields fields = reader.getTermVectors(0);
     final Terms terms = fields.terms("f");
     final TermsEnum te = terms.iterator(null);
diff --git lucene/core/src/test/org/apache/lucene/codecs/compressing/TestCompressingTermVectorsFormat.java lucene/core/src/test/org/apache/lucene/codecs/compressing/TestCompressingTermVectorsFormat.java
index 70fc572..c0d336c 100644
--- lucene/core/src/test/org/apache/lucene/codecs/compressing/TestCompressingTermVectorsFormat.java
+++ lucene/core/src/test/org/apache/lucene/codecs/compressing/TestCompressingTermVectorsFormat.java
@@ -5,7 +5,7 @@ import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.FieldType;
 import org.apache.lucene.document.TextField;
-import org.apache.lucene.index.AtomicReader;
+import org.apache.lucene.index.LeafReader;
 import org.apache.lucene.index.BaseTermVectorsFormatTestCase;
 import org.apache.lucene.index.RandomIndexWriter;
 import org.apache.lucene.index.Terms;
@@ -50,7 +50,7 @@ public class TestCompressingTermVectorsFormat extends BaseTermVectorsFormatTestC
     ft.setStoreTermVectors(true);
     doc.add(new Field("foo", "this is a test", ft));
     iw.addDocument(doc);
-    AtomicReader ir = getOnlySegmentReader(iw.getReader());
+    LeafReader ir = getOnlySegmentReader(iw.getReader());
     Terms terms = ir.getTermVector(0, "foo");
     assertNotNull(terms);
     TermsEnum termsEnum = terms.iterator(null);
diff --git lucene/core/src/test/org/apache/lucene/codecs/lucene41/TestBlockPostingsFormat3.java lucene/core/src/test/org/apache/lucene/codecs/lucene41/TestBlockPostingsFormat3.java
index cf92df3..4f45e6c 100644
--- lucene/core/src/test/org/apache/lucene/codecs/lucene41/TestBlockPostingsFormat3.java
+++ lucene/core/src/test/org/apache/lucene/codecs/lucene41/TestBlockPostingsFormat3.java
@@ -32,8 +32,8 @@ import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.FieldType;
 import org.apache.lucene.document.TextField;
-import org.apache.lucene.index.AtomicReader;
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReader;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.FieldInfo.IndexOptions;
 import org.apache.lucene.index.IndexWriterConfig.OpenMode;
 import org.apache.lucene.index.TermsEnum.SeekStatus;
@@ -148,8 +148,8 @@ public class TestBlockPostingsFormat3 extends LuceneTestCase {
   
   private void verify(Directory dir) throws Exception {
     DirectoryReader ir = DirectoryReader.open(dir);
-    for (AtomicReaderContext leaf : ir.leaves()) {
-      AtomicReader leafReader = leaf.reader();
+    for (LeafReaderContext leaf : ir.leaves()) {
+      LeafReader leafReader = leaf.reader();
       assertTerms(leafReader.terms("field1docs"), leafReader.terms("field2freqs"), true);
       assertTerms(leafReader.terms("field3positions"), leafReader.terms("field4offsets"), true);
       assertTerms(leafReader.terms("field4offsets"), leafReader.terms("field5payloadsFixed"), true);
diff --git lucene/core/src/test/org/apache/lucene/codecs/lucene410/TestLucene410DocValuesFormat.java lucene/core/src/test/org/apache/lucene/codecs/lucene410/TestLucene410DocValuesFormat.java
index 8c5f5df..aad8f49 100644
--- lucene/core/src/test/org/apache/lucene/codecs/lucene410/TestLucene410DocValuesFormat.java
+++ lucene/core/src/test/org/apache/lucene/codecs/lucene410/TestLucene410DocValuesFormat.java
@@ -27,13 +27,12 @@ import org.apache.lucene.codecs.DocValuesFormat;
 import org.apache.lucene.codecs.PostingsFormat;
 import org.apache.lucene.codecs.blocktreeords.Ords41PostingsFormat;
 import org.apache.lucene.codecs.lucene41ords.Lucene41WithOrds;
-import org.apache.lucene.codecs.memory.FSTOrdPostingsFormat;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.SortedSetDocValuesField;
 import org.apache.lucene.document.StringField;
-import org.apache.lucene.index.AtomicReader;
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReader;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.BaseCompressingDocValuesFormatTestCase;
 import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexWriterConfig;
@@ -188,8 +187,8 @@ public class TestLucene410DocValuesFormat extends BaseCompressingDocValuesFormat
     
     // compare per-segment
     DirectoryReader ir = writer.getReader();
-    for (AtomicReaderContext context : ir.leaves()) {
-      AtomicReader r = context.reader();
+    for (LeafReaderContext context : ir.leaves()) {
+      LeafReader r = context.reader();
       Terms terms = r.terms("indexed");
       if (terms != null) {
         assertEquals(terms.size(), r.getSortedSetDocValues("dv").getValueCount());
@@ -204,7 +203,7 @@ public class TestLucene410DocValuesFormat extends BaseCompressingDocValuesFormat
     
     // now compare again after the merge
     ir = writer.getReader();
-    AtomicReader ar = getOnlySegmentReader(ir);
+    LeafReader ar = getOnlySegmentReader(ir);
     Terms terms = ar.terms("indexed");
     if (terms != null) {
       assertEquals(terms.size(), ar.getSortedSetDocValues("dv").getValueCount());
diff --git lucene/core/src/test/org/apache/lucene/index/Test2BBinaryDocValues.java lucene/core/src/test/org/apache/lucene/index/Test2BBinaryDocValues.java
index c98778c..598d4ff 100644
--- lucene/core/src/test/org/apache/lucene/index/Test2BBinaryDocValues.java
+++ lucene/core/src/test/org/apache/lucene/index/Test2BBinaryDocValues.java
@@ -78,8 +78,8 @@ public class Test2BBinaryDocValues extends LuceneTestCase {
     
     DirectoryReader r = DirectoryReader.open(dir);
     int expectedValue = 0;
-    for (AtomicReaderContext context : r.leaves()) {
-      AtomicReader reader = context.reader();
+    for (LeafReaderContext context : r.leaves()) {
+      LeafReader reader = context.reader();
       BinaryDocValues dv = reader.getBinaryDocValues("dv");
       for (int i = 0; i < reader.maxDoc(); i++) {
         bytes[0] = (byte)(expectedValue >> 24);
@@ -138,8 +138,8 @@ public class Test2BBinaryDocValues extends LuceneTestCase {
     DirectoryReader r = DirectoryReader.open(dir);
     int expectedValue = 0;
     ByteArrayDataInput input = new ByteArrayDataInput();
-    for (AtomicReaderContext context : r.leaves()) {
-      AtomicReader reader = context.reader();
+    for (LeafReaderContext context : r.leaves()) {
+      LeafReader reader = context.reader();
       BinaryDocValues dv = reader.getBinaryDocValues("dv");
       for (int i = 0; i < reader.maxDoc(); i++) {
         final BytesRef term = dv.get(i);
diff --git lucene/core/src/test/org/apache/lucene/index/Test2BNumericDocValues.java lucene/core/src/test/org/apache/lucene/index/Test2BNumericDocValues.java
index adc0239..d9cba8d 100644
--- lucene/core/src/test/org/apache/lucene/index/Test2BNumericDocValues.java
+++ lucene/core/src/test/org/apache/lucene/index/Test2BNumericDocValues.java
@@ -70,8 +70,8 @@ public class Test2BNumericDocValues extends LuceneTestCase {
     
     DirectoryReader r = DirectoryReader.open(dir);
     long expectedValue = 0;
-    for (AtomicReaderContext context : r.leaves()) {
-      AtomicReader reader = context.reader();
+    for (LeafReaderContext context : r.leaves()) {
+      LeafReader reader = context.reader();
       NumericDocValues dv = reader.getNumericDocValues("dv");
       for (int i = 0; i < reader.maxDoc(); i++) {
         assertEquals(expectedValue, dv.get(i));
diff --git lucene/core/src/test/org/apache/lucene/index/Test2BSortedDocValues.java lucene/core/src/test/org/apache/lucene/index/Test2BSortedDocValues.java
index 20e2de7..8260e3d 100644
--- lucene/core/src/test/org/apache/lucene/index/Test2BSortedDocValues.java
+++ lucene/core/src/test/org/apache/lucene/index/Test2BSortedDocValues.java
@@ -74,8 +74,8 @@ public class Test2BSortedDocValues extends LuceneTestCase {
     
     DirectoryReader r = DirectoryReader.open(dir);
     int expectedValue = 0;
-    for (AtomicReaderContext context : r.leaves()) {
-      AtomicReader reader = context.reader();
+    for (LeafReaderContext context : r.leaves()) {
+      LeafReader reader = context.reader();
       BinaryDocValues dv = reader.getSortedDocValues("dv");
       for (int i = 0; i < reader.maxDoc(); i++) {
         bytes[0] = (byte)(expectedValue >> 8);
@@ -131,8 +131,8 @@ public class Test2BSortedDocValues extends LuceneTestCase {
     
     DirectoryReader r = DirectoryReader.open(dir);
     int counter = 0;
-    for (AtomicReaderContext context : r.leaves()) {
-      AtomicReader reader = context.reader();
+    for (LeafReaderContext context : r.leaves()) {
+      LeafReader reader = context.reader();
       BytesRef scratch = new BytesRef();
       BinaryDocValues dv = reader.getSortedDocValues("dv");
       for (int i = 0; i < reader.maxDoc(); i++) {
diff --git lucene/core/src/test/org/apache/lucene/index/TestAddIndexes.java lucene/core/src/test/org/apache/lucene/index/TestAddIndexes.java
index c293117..b901d6c 100644
--- lucene/core/src/test/org/apache/lucene/index/TestAddIndexes.java
+++ lucene/core/src/test/org/apache/lucene/index/TestAddIndexes.java
@@ -1223,7 +1223,7 @@ public class TestAddIndexes extends LuceneTestCase {
     w.addIndexes(empty);
     w.close();
     DirectoryReader dr = DirectoryReader.open(d1);
-    for (AtomicReaderContext ctx : dr.leaves()) {
+    for (LeafReaderContext ctx : dr.leaves()) {
       assertTrue("empty segments should be dropped by addIndexes", ctx.reader().maxDoc() > 0);
     }
     dr.close();
@@ -1245,7 +1245,7 @@ public class TestAddIndexes extends LuceneTestCase {
     w.addIndexes(allDeletedReader);
     w.close();
     DirectoryReader dr = DirectoryReader.open(src);
-    for (AtomicReaderContext ctx : dr.leaves()) {
+    for (LeafReaderContext ctx : dr.leaves()) {
       assertTrue("empty segments should be dropped by addIndexes", ctx.reader().maxDoc() > 0);
     }
     dr.close();
diff --git lucene/core/src/test/org/apache/lucene/index/TestBagOfPositions.java lucene/core/src/test/org/apache/lucene/index/TestBagOfPositions.java
index 5fe850d..c919f96 100644
--- lucene/core/src/test/org/apache/lucene/index/TestBagOfPositions.java
+++ lucene/core/src/test/org/apache/lucene/index/TestBagOfPositions.java
@@ -134,7 +134,7 @@ public class TestBagOfPositions extends LuceneTestCase {
     iw.forceMerge(1);
     DirectoryReader ir = iw.getReader();
     assertEquals(1, ir.leaves().size());
-    AtomicReader air = ir.leaves().get(0).reader();
+    LeafReader air = ir.leaves().get(0).reader();
     Terms terms = air.terms("field");
     // numTerms-1 because there cannot be a term 0 with 0 postings:
     assertEquals(numTerms-1, terms.size());
diff --git lucene/core/src/test/org/apache/lucene/index/TestBagOfPostings.java lucene/core/src/test/org/apache/lucene/index/TestBagOfPostings.java
index cb0e8db..831e634 100644
--- lucene/core/src/test/org/apache/lucene/index/TestBagOfPostings.java
+++ lucene/core/src/test/org/apache/lucene/index/TestBagOfPostings.java
@@ -125,7 +125,7 @@ public class TestBagOfPostings extends LuceneTestCase {
     iw.forceMerge(1);
     DirectoryReader ir = iw.getReader();
     assertEquals(1, ir.leaves().size());
-    AtomicReader air = ir.leaves().get(0).reader();
+    LeafReader air = ir.leaves().get(0).reader();
     Terms terms = air.terms("field");
     // numTerms-1 because there cannot be a term 0 with 0 postings:
     assertEquals(numTerms-1, terms.size());
diff --git lucene/core/src/test/org/apache/lucene/index/TestBinaryDocValuesUpdates.java lucene/core/src/test/org/apache/lucene/index/TestBinaryDocValuesUpdates.java
index 38d23e5..e758c89 100644
--- lucene/core/src/test/org/apache/lucene/index/TestBinaryDocValuesUpdates.java
+++ lucene/core/src/test/org/apache/lucene/index/TestBinaryDocValuesUpdates.java
@@ -129,7 +129,7 @@ public class TestBinaryDocValuesUpdates extends LuceneTestCase {
     }
     
     assertEquals(1, reader.leaves().size());
-    AtomicReader r = reader.leaves().get(0).reader();
+    LeafReader r = reader.leaves().get(0).reader();
     BinaryDocValues bdv = r.getBinaryDocValues("val");
     assertEquals(2, getValue(bdv, 0));
     assertEquals(2, getValue(bdv, 1));
@@ -170,8 +170,8 @@ public class TestBinaryDocValuesUpdates extends LuceneTestCase {
       writer.close();
     }
     
-    for (AtomicReaderContext context : reader.leaves()) {
-      AtomicReader r = context.reader();
+    for (LeafReaderContext context : reader.leaves()) {
+      LeafReader r = context.reader();
       BinaryDocValues bdv = r.getBinaryDocValues("val");
       assertNotNull(bdv);
       for (int i = 0; i < r.maxDoc(); i++) {
@@ -253,7 +253,7 @@ public class TestBinaryDocValuesUpdates extends LuceneTestCase {
       writer.close();
     }
     
-    AtomicReader slow = SlowCompositeReaderWrapper.wrap(reader);
+    LeafReader slow = SlowCompositeReaderWrapper.wrap(reader);
     
     Bits liveDocs = slow.getLiveDocs();
     boolean[] expectedLiveDocs = new boolean[] { true, false, false, true, true, true };
@@ -297,7 +297,7 @@ public class TestBinaryDocValuesUpdates extends LuceneTestCase {
       writer.close();
     }
     
-    AtomicReader r = reader.leaves().get(0).reader();
+    LeafReader r = reader.leaves().get(0).reader();
     assertFalse(r.getLiveDocs().get(0));
     assertEquals(17, getValue(r.getBinaryDocValues("val"), 1));
     
@@ -331,7 +331,7 @@ public class TestBinaryDocValuesUpdates extends LuceneTestCase {
       writer.close();
     }
     
-    AtomicReader r = reader.leaves().get(0).reader();
+    LeafReader r = reader.leaves().get(0).reader();
     assertFalse(r.getLiveDocs().get(0));
     assertEquals(1, getValue(r.getBinaryDocValues("val"), 0)); // deletes are currently applied first
     
@@ -362,7 +362,7 @@ public class TestBinaryDocValuesUpdates extends LuceneTestCase {
     writer.close();
     
     final DirectoryReader reader = DirectoryReader.open(dir);
-    AtomicReader r = reader.leaves().get(0).reader();
+    LeafReader r = reader.leaves().get(0).reader();
     NumericDocValues ndv = r.getNumericDocValues("ndv");
     BinaryDocValues bdv = r.getBinaryDocValues("bdv");
     SortedDocValues sdv = r.getSortedDocValues("sdv");
@@ -408,7 +408,7 @@ public class TestBinaryDocValuesUpdates extends LuceneTestCase {
     writer.close();
     
     final DirectoryReader reader = DirectoryReader.open(dir);
-    AtomicReader r = reader.leaves().get(0).reader();
+    LeafReader r = reader.leaves().get(0).reader();
     
     BinaryDocValues bdv1 = r.getBinaryDocValues("bdv1");
     BinaryDocValues bdv2 = r.getBinaryDocValues("bdv2");
@@ -441,7 +441,7 @@ public class TestBinaryDocValuesUpdates extends LuceneTestCase {
     writer.close();
     
     final DirectoryReader reader = DirectoryReader.open(dir);
-    AtomicReader r = reader.leaves().get(0).reader();
+    LeafReader r = reader.leaves().get(0).reader();
     BinaryDocValues bdv = r.getBinaryDocValues("bdv");
     for (int i = 0; i < r.maxDoc(); i++) {
       assertEquals(17, getValue(bdv, i));
@@ -507,7 +507,7 @@ public class TestBinaryDocValuesUpdates extends LuceneTestCase {
     
     final DirectoryReader reader = DirectoryReader.open(dir);
     
-    AtomicReader r = SlowCompositeReaderWrapper.wrap(reader);
+    LeafReader r = SlowCompositeReaderWrapper.wrap(reader);
     BinaryDocValues bdv = r.getBinaryDocValues("bdv");
     SortedDocValues sdv = r.getSortedDocValues("sorted");
     for (int i = 0; i < r.maxDoc(); i++) {
@@ -537,7 +537,7 @@ public class TestBinaryDocValuesUpdates extends LuceneTestCase {
     writer.close();
     
     final DirectoryReader reader = DirectoryReader.open(dir);
-    final AtomicReader r = SlowCompositeReaderWrapper.wrap(reader);
+    final LeafReader r = SlowCompositeReaderWrapper.wrap(reader);
     BinaryDocValues bdv = r.getBinaryDocValues("bdv");
     for (int i = 0; i < r.maxDoc(); i++) {
       assertEquals(3, getValue(bdv, i));
@@ -603,7 +603,7 @@ public class TestBinaryDocValuesUpdates extends LuceneTestCase {
       }
       
       assertEquals(1, reader.leaves().size());
-      final AtomicReader r = reader.leaves().get(0).reader();
+      final LeafReader r = reader.leaves().get(0).reader();
       assertNull("index should have no deletes after forceMerge", r.getLiveDocs());
       BinaryDocValues bdv = r.getBinaryDocValues("bdv");
       assertNotNull(bdv);
@@ -636,7 +636,7 @@ public class TestBinaryDocValuesUpdates extends LuceneTestCase {
     writer.close();
     
     final DirectoryReader reader = DirectoryReader.open(dir);
-    final AtomicReader r = SlowCompositeReaderWrapper.wrap(reader);
+    final LeafReader r = SlowCompositeReaderWrapper.wrap(reader);
     BinaryDocValues bdv = r.getBinaryDocValues("bdv");
     for (int i = 0; i < r.maxDoc(); i++) {
       assertEquals(3, getValue(bdv, i));
@@ -709,8 +709,8 @@ public class TestBinaryDocValuesUpdates extends LuceneTestCase {
       reader = newReader;
 //      System.out.println("[" + Thread.currentThread().getName() + "]: reopened reader: " + reader);
       assertTrue(reader.numDocs() > 0); // we delete at most one document per round
-      for (AtomicReaderContext context : reader.leaves()) {
-        AtomicReader r = context.reader();
+      for (LeafReaderContext context : reader.leaves()) {
+        LeafReader r = context.reader();
 //        System.out.println(((SegmentReader) r).getSegmentName());
         Bits liveDocs = r.getLiveDocs();
         for (int field = 0; field < fieldValues.length; field++) {
@@ -773,8 +773,8 @@ public class TestBinaryDocValuesUpdates extends LuceneTestCase {
     writer.close();
 
     DirectoryReader reader = DirectoryReader.open(dir);
-    for (AtomicReaderContext context : reader.leaves()) {
-      AtomicReader r = context.reader();
+    for (LeafReaderContext context : reader.leaves()) {
+      LeafReader r = context.reader();
       BinaryDocValues bdv = r.getBinaryDocValues("bdv");
       Bits docsWithField = r.getDocsWithField("bdv");
       assertNotNull(docsWithField);
@@ -818,8 +818,8 @@ public class TestBinaryDocValuesUpdates extends LuceneTestCase {
     writer.close();
 
     DirectoryReader reader = DirectoryReader.open(dir);
-    for (AtomicReaderContext context : reader.leaves()) {
-      AtomicReader r = context.reader();
+    for (LeafReaderContext context : reader.leaves()) {
+      LeafReader r = context.reader();
       BinaryDocValues bdv = r.getBinaryDocValues("bdv");
       for (int i = 0; i < r.maxDoc(); i++) {
         assertEquals(5L, getValue(bdv, i));
@@ -959,8 +959,8 @@ public class TestBinaryDocValuesUpdates extends LuceneTestCase {
     writer.close();
     
     DirectoryReader reader = DirectoryReader.open(dir);
-    for (AtomicReaderContext context : reader.leaves()) {
-      AtomicReader r = context.reader();
+    for (LeafReaderContext context : reader.leaves()) {
+      LeafReader r = context.reader();
       for (int i = 0; i < numFields; i++) {
         BinaryDocValues bdv = r.getBinaryDocValues("f" + i);
         BinaryDocValues control = r.getBinaryDocValues("cf" + i);
@@ -1004,8 +1004,8 @@ public class TestBinaryDocValuesUpdates extends LuceneTestCase {
       long value = random().nextLong();
       writer.updateDocValues(t, new BinaryDocValuesField("f", toBytes(value)), new BinaryDocValuesField("cf", toBytes(value*2)));
       DirectoryReader reader = DirectoryReader.open(writer, true);
-      for (AtomicReaderContext context : reader.leaves()) {
-        AtomicReader r = context.reader();
+      for (LeafReaderContext context : reader.leaves()) {
+        LeafReader r = context.reader();
         BinaryDocValues fbdv = r.getBinaryDocValues("f");
         BinaryDocValues cfbdv = r.getBinaryDocValues("cf");
         for (int j = 0; j < r.maxDoc(); j++) {
@@ -1055,7 +1055,7 @@ public class TestBinaryDocValuesUpdates extends LuceneTestCase {
     writer.close();
     
     DirectoryReader reader = DirectoryReader.open(dir);
-    AtomicReader r = SlowCompositeReaderWrapper.wrap(reader);
+    LeafReader r = SlowCompositeReaderWrapper.wrap(reader);
     BinaryDocValues f1 = r.getBinaryDocValues("f1");
     BinaryDocValues f2 = r.getBinaryDocValues("f2");
     assertEquals(12L, getValue(f1, 0));
@@ -1110,8 +1110,8 @@ public class TestBinaryDocValuesUpdates extends LuceneTestCase {
     writer.close();
     
     DirectoryReader reader = DirectoryReader.open(dir2);
-    for (AtomicReaderContext context : reader.leaves()) {
-      AtomicReader r = context.reader();
+    for (LeafReaderContext context : reader.leaves()) {
+      LeafReader r = context.reader();
       BinaryDocValues bdv = r.getBinaryDocValues("bdv");
       BinaryDocValues control = r.getBinaryDocValues("control");
       for (int i = 0; i < r.maxDoc(); i++) {
@@ -1209,9 +1209,9 @@ public class TestBinaryDocValuesUpdates extends LuceneTestCase {
     writer.close();
     
     DirectoryReader reader = DirectoryReader.open(dir);
-    for (AtomicReaderContext context : reader.leaves()) {
+    for (LeafReaderContext context : reader.leaves()) {
       for (int i = 0; i < numBinaryFields; i++) {
-        AtomicReader r = context.reader();
+        LeafReader r = context.reader();
         BinaryDocValues f = r.getBinaryDocValues("f" + i);
         BinaryDocValues cf = r.getBinaryDocValues("cf" + i);
         for (int j = 0; j < r.maxDoc(); j++) {
diff --git lucene/core/src/test/org/apache/lucene/index/TestCodecHoldsOpenFiles.java lucene/core/src/test/org/apache/lucene/index/TestCodecHoldsOpenFiles.java
index b5bd50d..546a6bd 100644
--- lucene/core/src/test/org/apache/lucene/index/TestCodecHoldsOpenFiles.java
+++ lucene/core/src/test/org/apache/lucene/index/TestCodecHoldsOpenFiles.java
@@ -22,7 +22,6 @@ import java.io.IOException;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.TextField;
 import org.apache.lucene.store.BaseDirectoryWrapper;
-import org.apache.lucene.store.Directory;
 import org.apache.lucene.util.LuceneTestCase;
 import org.apache.lucene.util.TestUtil;
 
@@ -53,7 +52,7 @@ public class TestCodecHoldsOpenFiles extends LuceneTestCase {
       }
     }
 
-    for(AtomicReaderContext cxt : r.leaves()) {
+    for(LeafReaderContext cxt : r.leaves()) {
       TestUtil.checkReader(cxt.reader());
     }
 
diff --git lucene/core/src/test/org/apache/lucene/index/TestCodecs.java lucene/core/src/test/org/apache/lucene/index/TestCodecs.java
index e0a0a88..6889e72 100644
--- lucene/core/src/test/org/apache/lucene/index/TestCodecs.java
+++ lucene/core/src/test/org/apache/lucene/index/TestCodecs.java
@@ -29,7 +29,6 @@ import org.apache.lucene.codecs.FieldsConsumer;
 import org.apache.lucene.codecs.FieldsProducer;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field.Store;
-import org.apache.lucene.document.NumericDocValuesField;
 import org.apache.lucene.document.StringField;
 import org.apache.lucene.index.FieldInfo.DocValuesType;
 import org.apache.lucene.index.FieldInfo.IndexOptions;
@@ -836,7 +835,7 @@ public class TestCodecs extends LuceneTestCase {
     
     Term term = new Term("f", new BytesRef("doc"));
     DirectoryReader reader = DirectoryReader.open(dir);
-    for (AtomicReaderContext ctx : reader.leaves()) {
+    for (LeafReaderContext ctx : reader.leaves()) {
       DocsEnum de = ctx.reader().termDocsEnum(term);
       while (de.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {
         assertEquals("wrong freq for doc " + de.docID(), 1, de.freq());
diff --git lucene/core/src/test/org/apache/lucene/index/TestCustomNorms.java lucene/core/src/test/org/apache/lucene/index/TestCustomNorms.java
index 0f95286..fe324f9 100644
--- lucene/core/src/test/org/apache/lucene/index/TestCustomNorms.java
+++ lucene/core/src/test/org/apache/lucene/index/TestCustomNorms.java
@@ -66,7 +66,7 @@ public class TestCustomNorms extends LuceneTestCase {
     }
     writer.commit();
     writer.close();
-    AtomicReader open = SlowCompositeReaderWrapper.wrap(DirectoryReader.open(dir));
+    LeafReader open = SlowCompositeReaderWrapper.wrap(DirectoryReader.open(dir));
     NumericDocValues norms = open.getNormValues(floatTestField);
     assertNotNull(norms);
     for (int i = 0; i < open.maxDoc(); i++) {
@@ -115,7 +115,7 @@ public class TestCustomNorms extends LuceneTestCase {
     }
 
     @Override
-    public SimScorer simScorer(SimWeight weight, AtomicReaderContext context) throws IOException {
+    public SimScorer simScorer(SimWeight weight, LeafReaderContext context) throws IOException {
       throw new UnsupportedOperationException();
     }
   }
diff --git lucene/core/src/test/org/apache/lucene/index/TestDirectoryReader.java lucene/core/src/test/org/apache/lucene/index/TestDirectoryReader.java
index fc82dce..90cc11e 100644
--- lucene/core/src/test/org/apache/lucene/index/TestDirectoryReader.java
+++ lucene/core/src/test/org/apache/lucene/index/TestDirectoryReader.java
@@ -17,7 +17,6 @@ package org.apache.lucene.index;
  * limitations under the License.
  */
 
-import java.io.File;
 import java.io.FileNotFoundException;
 import java.io.IOException;
 import java.nio.file.Files;
@@ -770,7 +769,7 @@ public void testFilesOpenClose() throws IOException {
     writer.commit();
   
     DirectoryReader r = DirectoryReader.open(dir);
-    AtomicReader r1 = getOnlySegmentReader(r);
+    LeafReader r1 = getOnlySegmentReader(r);
     assertEquals(26, r1.terms("field").size());
     assertEquals(10, r1.terms("number").size());
     writer.addDocument(doc);
@@ -779,7 +778,7 @@ public void testFilesOpenClose() throws IOException {
     assertNotNull(r2);
     r.close();
   
-    for(AtomicReaderContext s : r2.leaves()) {
+    for(LeafReaderContext s : r2.leaves()) {
       assertEquals(26, s.reader().terms("field").size());
       assertEquals(10, s.reader().terms("number").size());
     }
diff --git lucene/core/src/test/org/apache/lucene/index/TestDoc.java lucene/core/src/test/org/apache/lucene/index/TestDoc.java
index 78c7534..bb41c0c 100644
--- lucene/core/src/test/org/apache/lucene/index/TestDoc.java
+++ lucene/core/src/test/org/apache/lucene/index/TestDoc.java
@@ -220,7 +220,7 @@ public class TestDoc extends LuceneTestCase {
       TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(si1.info.dir);
       final SegmentInfo si = new SegmentInfo(si1.info.dir, Version.LATEST, merged, -1, false, codec, null);
 
-      SegmentMerger merger = new SegmentMerger(Arrays.<AtomicReader>asList(r1, r2),
+      SegmentMerger merger = new SegmentMerger(Arrays.<LeafReader>asList(r1, r2),
           si, InfoStream.getDefault(), trackingDir,
           MergeState.CheckAbort.NONE, new FieldInfos.FieldNumbers(), context, true);
 
diff --git lucene/core/src/test/org/apache/lucene/index/TestDocValuesIndexing.java lucene/core/src/test/org/apache/lucene/index/TestDocValuesIndexing.java
index 027c1ac..9891d0c 100644
--- lucene/core/src/test/org/apache/lucene/index/TestDocValuesIndexing.java
+++ lucene/core/src/test/org/apache/lucene/index/TestDocValuesIndexing.java
@@ -78,7 +78,7 @@ public class TestDocValuesIndexing extends LuceneTestCase {
     w.forceMerge(1);
     DirectoryReader r3 = w.getReader();
     w.close();
-    AtomicReader sr = getOnlySegmentReader(r3);
+    LeafReader sr = getOnlySegmentReader(r3);
     assertEquals(2, sr.numDocs());
     NumericDocValues docValues = sr.getNumericDocValues("dv");
     assertNotNull(docValues);
@@ -203,7 +203,7 @@ public class TestDocValuesIndexing extends LuceneTestCase {
       writer.addDocument(doc);
     }
     DirectoryReader r = writer.getReader();
-    AtomicReader slow = SlowCompositeReaderWrapper.wrap(r);
+    LeafReader slow = SlowCompositeReaderWrapper.wrap(r);
     FieldInfos fi = slow.getFieldInfos();
     FieldInfo dvInfo = fi.fieldInfo("dv");
     assertTrue(dvInfo.hasDocValues());
@@ -784,7 +784,7 @@ public class TestDocValuesIndexing extends LuceneTestCase {
     DirectoryReader r = writer.getReader();
     writer.close();
 
-    AtomicReader subR = r.leaves().get(0).reader();
+    LeafReader subR = r.leaves().get(0).reader();
     assertEquals(2, subR.numDocs());
 
     Bits bits = DocValues.getDocsWithField(subR, "dv");
diff --git lucene/core/src/test/org/apache/lucene/index/TestDocsAndPositions.java lucene/core/src/test/org/apache/lucene/index/TestDocsAndPositions.java
index edb81cc..2c185cb 100644
--- lucene/core/src/test/org/apache/lucene/index/TestDocsAndPositions.java
+++ lucene/core/src/test/org/apache/lucene/index/TestDocsAndPositions.java
@@ -64,14 +64,14 @@ public class TestDocsAndPositions extends LuceneTestCase {
     for (int i = 0; i < num; i++) {
       BytesRef bytes = new BytesRef("1");
       IndexReaderContext topReaderContext = reader.getContext();
-      for (AtomicReaderContext atomicReaderContext : topReaderContext.leaves()) {
+      for (LeafReaderContext leafReaderContext : topReaderContext.leaves()) {
         DocsAndPositionsEnum docsAndPosEnum = getDocsAndPositions(
-            atomicReaderContext.reader(), bytes, null);
+            leafReaderContext.reader(), bytes, null);
         assertNotNull(docsAndPosEnum);
-        if (atomicReaderContext.reader().maxDoc() == 0) {
+        if (leafReaderContext.reader().maxDoc() == 0) {
           continue;
         }
-        final int advance = docsAndPosEnum.advance(random().nextInt(atomicReaderContext.reader().maxDoc()));
+        final int advance = docsAndPosEnum.advance(random().nextInt(leafReaderContext.reader().maxDoc()));
         do {
           String msg = "Advanced to: " + advance + " current doc: "
               + docsAndPosEnum.docID(); // TODO: + " usePayloads: " + usePayload;
@@ -90,7 +90,7 @@ public class TestDocsAndPositions extends LuceneTestCase {
     directory.close();
   }
 
-  public DocsAndPositionsEnum getDocsAndPositions(AtomicReader reader,
+  public DocsAndPositionsEnum getDocsAndPositions(LeafReader reader,
       BytesRef bytes, Bits liveDocs) throws IOException {
     Terms terms = reader.terms(fieldName);
     if (terms != null) {
@@ -147,12 +147,12 @@ public class TestDocsAndPositions extends LuceneTestCase {
     for (int i = 0; i < num; i++) {
       BytesRef bytes = new BytesRef("" + term);
       IndexReaderContext topReaderContext = reader.getContext();
-      for (AtomicReaderContext atomicReaderContext : topReaderContext.leaves()) {
+      for (LeafReaderContext leafReaderContext : topReaderContext.leaves()) {
         DocsAndPositionsEnum docsAndPosEnum = getDocsAndPositions(
-            atomicReaderContext.reader(), bytes, null);
+            leafReaderContext.reader(), bytes, null);
         assertNotNull(docsAndPosEnum);
         int initDoc = 0;
-        int maxDoc = atomicReaderContext.reader().maxDoc();
+        int maxDoc = leafReaderContext.reader().maxDoc();
         // initially advance or do next doc
         if (random().nextBoolean()) {
           initDoc = docsAndPosEnum.nextDoc();
@@ -165,7 +165,7 @@ public class TestDocsAndPositions extends LuceneTestCase {
           if (docID == DocIdSetIterator.NO_MORE_DOCS) {
             break;
           }
-          Integer[] pos = positionsInDoc[atomicReaderContext.docBase + docID];
+          Integer[] pos = positionsInDoc[leafReaderContext.docBase + docID];
           assertEquals(pos.length, docsAndPosEnum.freq());
           // number of positions read should be random - don't read all of them
           // allways
@@ -173,7 +173,7 @@ public class TestDocsAndPositions extends LuceneTestCase {
               - random().nextInt(pos.length) : pos.length;
           for (int j = 0; j < howMany; j++) {
             assertEquals("iteration: " + i + " initDoc: " + initDoc + " doc: "
-                + docID + " base: " + atomicReaderContext.docBase
+                + docID + " base: " + leafReaderContext.docBase
                 + " positions: " + Arrays.toString(pos) /* TODO: + " usePayloads: "
                 + usePayload*/, pos[j].intValue(), docsAndPosEnum.nextPosition());
           }
@@ -224,7 +224,7 @@ public class TestDocsAndPositions extends LuceneTestCase {
     for (int i = 0; i < num; i++) {
       BytesRef bytes = new BytesRef("" + term);
       IndexReaderContext topReaderContext = reader.getContext();
-      for (AtomicReaderContext context : topReaderContext.leaves()) {
+      for (LeafReaderContext context : topReaderContext.leaves()) {
         int maxDoc = context.reader().maxDoc();
         DocsEnum docsEnum = TestUtil.docs(random(), context.reader(), fieldName, bytes, null, null, DocsEnum.FLAG_FREQS);
         if (findNext(freqInDoc, context.docBase, context.docBase + maxDoc) == Integer.MAX_VALUE) {
@@ -302,13 +302,13 @@ public class TestDocsAndPositions extends LuceneTestCase {
       BytesRef bytes = new BytesRef("even");
 
       IndexReaderContext topReaderContext = reader.getContext();
-      for (AtomicReaderContext atomicReaderContext : topReaderContext.leaves()) {
+      for (LeafReaderContext leafReaderContext : topReaderContext.leaves()) {
         DocsAndPositionsEnum docsAndPosEnum = getDocsAndPositions(
-            atomicReaderContext.reader(), bytes, null);
+            leafReaderContext.reader(), bytes, null);
         assertNotNull(docsAndPosEnum);
 
         int initDoc = 0;
-        int maxDoc = atomicReaderContext.reader().maxDoc();
+        int maxDoc = leafReaderContext.reader().maxDoc();
         // initially advance or do next doc
         if (random().nextBoolean()) {
           initDoc = docsAndPosEnum.nextDoc();
@@ -335,7 +335,7 @@ public class TestDocsAndPositions extends LuceneTestCase {
     doc.add(newStringField("foo", "bar", Field.Store.NO));
     writer.addDocument(doc);
     DirectoryReader reader = writer.getReader();
-    AtomicReader r = getOnlySegmentReader(reader);
+    LeafReader r = getOnlySegmentReader(reader);
     DocsEnum disi = TestUtil.docs(random(), r, "foo", new BytesRef("bar"), null, null, DocsEnum.FLAG_NONE);
     int docid = disi.docID();
     assertEquals(-1, docid);
@@ -360,7 +360,7 @@ public class TestDocsAndPositions extends LuceneTestCase {
     doc.add(newTextField("foo", "bar", Field.Store.NO));
     writer.addDocument(doc);
     DirectoryReader reader = writer.getReader();
-    AtomicReader r = getOnlySegmentReader(reader);
+    LeafReader r = getOnlySegmentReader(reader);
     DocsAndPositionsEnum disi = r.termPositionsEnum(new Term("foo", "bar"));
     int docid = disi.docID();
     assertEquals(-1, docid);
diff --git lucene/core/src/test/org/apache/lucene/index/TestFilterAtomicReader.java lucene/core/src/test/org/apache/lucene/index/TestFilterAtomicReader.java
index 0995133..ad35569 100644
--- lucene/core/src/test/org/apache/lucene/index/TestFilterAtomicReader.java
+++ lucene/core/src/test/org/apache/lucene/index/TestFilterAtomicReader.java
@@ -34,7 +34,7 @@ import org.apache.lucene.util.LuceneTestCase;
 
 public class TestFilterAtomicReader extends LuceneTestCase {
 
-  private static class TestReader extends FilterAtomicReader {
+  private static class TestReader extends FilterLeafReader {
 
     /** Filter that only permits terms containing 'e'.*/
     private static class TestFields extends FilterFields {
@@ -184,12 +184,12 @@ public class TestFilterAtomicReader extends LuceneTestCase {
   }
 
   public void testOverrideMethods() throws Exception {
-    checkOverrideMethods(FilterAtomicReader.class);
-    checkOverrideMethods(FilterAtomicReader.FilterFields.class);
-    checkOverrideMethods(FilterAtomicReader.FilterTerms.class);
-    checkOverrideMethods(FilterAtomicReader.FilterTermsEnum.class);
-    checkOverrideMethods(FilterAtomicReader.FilterDocsEnum.class);
-    checkOverrideMethods(FilterAtomicReader.FilterDocsAndPositionsEnum.class);
+    checkOverrideMethods(FilterLeafReader.class);
+    checkOverrideMethods(FilterLeafReader.FilterFields.class);
+    checkOverrideMethods(FilterLeafReader.FilterTerms.class);
+    checkOverrideMethods(FilterLeafReader.FilterTermsEnum.class);
+    checkOverrideMethods(FilterLeafReader.FilterDocsEnum.class);
+    checkOverrideMethods(FilterLeafReader.FilterDocsAndPositionsEnum.class);
   }
 
 }
diff --git lucene/core/src/test/org/apache/lucene/index/TestIndexReaderClose.java lucene/core/src/test/org/apache/lucene/index/TestIndexReaderClose.java
index 9e7741b..7f00e47 100644
--- lucene/core/src/test/org/apache/lucene/index/TestIndexReaderClose.java
+++ lucene/core/src/test/org/apache/lucene/index/TestIndexReaderClose.java
@@ -43,8 +43,8 @@ public class TestIndexReaderClose extends LuceneTestCase {
       writer.close();
       DirectoryReader open = DirectoryReader.open(dir);
       final boolean throwOnClose = !rarely();
-      AtomicReader wrap = SlowCompositeReaderWrapper.wrap(open);
-      FilterAtomicReader reader = new FilterAtomicReader(wrap) {
+      LeafReader wrap = SlowCompositeReaderWrapper.wrap(open);
+      FilterLeafReader reader = new FilterLeafReader(wrap) {
         @Override
         protected void doClose() throws IOException {
           super.doClose();
@@ -107,37 +107,37 @@ public class TestIndexReaderClose extends LuceneTestCase {
     w.close();
 
     final IndexReader reader = DirectoryReader.open(w.w.getDirectory());
-    final AtomicReader atomicReader = SlowCompositeReaderWrapper.wrap(reader);
+    final LeafReader leafReader = SlowCompositeReaderWrapper.wrap(reader);
     
     final int numListeners = TestUtil.nextInt(random(), 1, 10);
-    final List<AtomicReader.CoreClosedListener> listeners = new ArrayList<>();
+    final List<LeafReader.CoreClosedListener> listeners = new ArrayList<>();
     AtomicInteger counter = new AtomicInteger(numListeners);
     
     for (int i = 0; i < numListeners; ++i) {
       CountCoreListener listener = new CountCoreListener(counter);
       listeners.add(listener);
-      atomicReader.addCoreClosedListener(listener);
+      leafReader.addCoreClosedListener(listener);
     }
     for (int i = 0; i < 100; ++i) {
-      atomicReader.addCoreClosedListener(listeners.get(random().nextInt(listeners.size())));
+      leafReader.addCoreClosedListener(listeners.get(random().nextInt(listeners.size())));
     }
     final int removed = random().nextInt(numListeners);
     Collections.shuffle(listeners);
     for (int i = 0; i < removed; ++i) {
-      atomicReader.removeCoreClosedListener(listeners.get(i));
+      leafReader.removeCoreClosedListener(listeners.get(i));
     }
     assertEquals(numListeners, counter.get());
     // make sure listeners are registered on the wrapped reader and that closing any of them has the same effect
     if (random().nextBoolean()) {
       reader.close();
     } else {
-      atomicReader.close();
+      leafReader.close();
     }
     assertEquals(removed, counter.get());
     w.w.getDirectory().close();
   }
 
-  private static final class CountCoreListener implements AtomicReader.CoreClosedListener {
+  private static final class CountCoreListener implements LeafReader.CoreClosedListener {
 
     private final AtomicInteger count;
 
diff --git lucene/core/src/test/org/apache/lucene/index/TestIndexWriter.java lucene/core/src/test/org/apache/lucene/index/TestIndexWriter.java
index f64c141..ccf69cd 100644
--- lucene/core/src/test/org/apache/lucene/index/TestIndexWriter.java
+++ lucene/core/src/test/org/apache/lucene/index/TestIndexWriter.java
@@ -740,7 +740,7 @@ public class TestIndexWriter extends LuceneTestCase {
     writer.addDocument(doc);  
     writer.close();
     DirectoryReader reader = DirectoryReader.open(dir);
-    AtomicReader subreader = getOnlySegmentReader(reader);
+    LeafReader subreader = getOnlySegmentReader(reader);
     TermsEnum te = subreader.fields().terms("").iterator(null);
     assertEquals(new BytesRef("a"), te.next());
     assertEquals(new BytesRef("b"), te.next());
@@ -761,7 +761,7 @@ public class TestIndexWriter extends LuceneTestCase {
     writer.addDocument(doc);  
     writer.close();
     DirectoryReader reader = DirectoryReader.open(dir);
-    AtomicReader subreader = getOnlySegmentReader(reader);
+    LeafReader subreader = getOnlySegmentReader(reader);
     TermsEnum te = subreader.fields().terms("").iterator(null);
     assertEquals(new BytesRef(""), te.next());
     assertEquals(new BytesRef("a"), te.next());
@@ -1600,7 +1600,7 @@ public class TestIndexWriter extends LuceneTestCase {
 
     assertNoUnreferencedFiles(dir, "no tv files");
     DirectoryReader r0 = DirectoryReader.open(dir);
-    for (AtomicReaderContext ctx : r0.leaves()) {
+    for (LeafReaderContext ctx : r0.leaves()) {
       SegmentReader sr = (SegmentReader) ctx.reader();
       assertFalse(sr.getFieldInfos().hasVectors());
     }
@@ -2241,9 +2241,9 @@ public class TestIndexWriter extends LuceneTestCase {
     }
     DirectoryReader reader = w.getReader();
     assertEquals(docCount, reader.numDocs());
-    List<AtomicReaderContext> leaves = reader.leaves();
-    for (AtomicReaderContext atomicReaderContext : leaves) {
-      AtomicReader ar = atomicReaderContext.reader();
+    List<LeafReaderContext> leaves = reader.leaves();
+    for (LeafReaderContext leafReaderContext : leaves) {
+      LeafReader ar = leafReaderContext.reader();
       Bits liveDocs = ar.getLiveDocs();
       int maxDoc = ar.maxDoc();
       for (int i = 0; i < maxDoc; i++) {
@@ -2293,9 +2293,9 @@ public class TestIndexWriter extends LuceneTestCase {
     }
     DirectoryReader reader = w.getReader();
     assertEquals(docCount, reader.numDocs());
-    List<AtomicReaderContext> leaves = reader.leaves();
-    for (AtomicReaderContext atomicReaderContext : leaves) {
-      AtomicReader ar = atomicReaderContext.reader();
+    List<LeafReaderContext> leaves = reader.leaves();
+    for (LeafReaderContext leafReaderContext : leaves) {
+      LeafReader ar = leafReaderContext.reader();
       Bits liveDocs = ar.getLiveDocs();
       int maxDoc = ar.maxDoc();
       for (int i = 0; i < maxDoc; i++) {
diff --git lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions.java lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions.java
index faf4e44..2ad49e5 100644
--- lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions.java
+++ lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions.java
@@ -46,7 +46,6 @@ import org.apache.lucene.document.SortedSetDocValuesField;
 import org.apache.lucene.document.StoredField;
 import org.apache.lucene.document.StringField;
 import org.apache.lucene.document.TextField;
-import org.apache.lucene.index.IndexWriterConfig.OpenMode;
 import org.apache.lucene.search.DocIdSetIterator;
 import org.apache.lucene.search.IndexSearcher;
 import org.apache.lucene.search.PhraseQuery;
@@ -1312,7 +1311,7 @@ public class TestIndexWriterExceptions extends LuceneTestCase {
         assertTrue(reader.numDocs() > 0);
         SegmentInfos sis = new SegmentInfos();
         sis.read(dir);
-        for(AtomicReaderContext context : reader.leaves()) {
+        for(LeafReaderContext context : reader.leaves()) {
           assertFalse(context.reader().getFieldInfos().hasVectors());
         }
         reader.close();
@@ -2076,8 +2075,8 @@ public class TestIndexWriterExceptions extends LuceneTestCase {
       }
       assertEquals(docCount-deleteCount, r.numDocs());
       BytesRef scratch = new BytesRef();
-      for (AtomicReaderContext context : r.leaves()) {
-        AtomicReader reader = context.reader();
+      for (LeafReaderContext context : r.leaves()) {
+        LeafReader reader = context.reader();
         Bits liveDocs = reader.getLiveDocs();
         NumericDocValues f = reader.getNumericDocValues("f");
         NumericDocValues cf = reader.getNumericDocValues("cf");
diff --git lucene/core/src/test/org/apache/lucene/index/TestIndexWriterReader.java lucene/core/src/test/org/apache/lucene/index/TestIndexWriterReader.java
index df0e9b2..5021fe8 100644
--- lucene/core/src/test/org/apache/lucene/index/TestIndexWriterReader.java
+++ lucene/core/src/test/org/apache/lucene/index/TestIndexWriterReader.java
@@ -599,7 +599,7 @@ public class TestIndexWriterReader extends LuceneTestCase {
   private static class MyWarmer extends IndexWriter.IndexReaderWarmer {
     int warmCount;
     @Override
-    public void warm(AtomicReader reader) throws IOException {
+    public void warm(LeafReader reader) throws IOException {
       warmCount++;
     }
   }
@@ -956,7 +956,7 @@ public class TestIndexWriterReader extends LuceneTestCase {
             setReaderPooling(true).
             setMergedSegmentWarmer(new IndexWriter.IndexReaderWarmer() {
               @Override
-              public void warm(AtomicReader r) throws IOException {
+              public void warm(LeafReader r) throws IOException {
                 IndexSearcher s = newSearcher(r);
                 TopDocs hits = s.search(new TermQuery(new Term("foo", "bar")), 10);
                 assertEquals(20, hits.totalHits);
diff --git lucene/core/src/test/org/apache/lucene/index/TestIndexWriterUnicode.java lucene/core/src/test/org/apache/lucene/index/TestIndexWriterUnicode.java
index 28ba3b4..504a0df 100644
--- lucene/core/src/test/org/apache/lucene/index/TestIndexWriterUnicode.java
+++ lucene/core/src/test/org/apache/lucene/index/TestIndexWriterUnicode.java
@@ -316,7 +316,7 @@ public class TestIndexWriterUnicode extends LuceneTestCase {
     IndexReader r = writer.getReader();
 
     // Test each sub-segment
-    for (AtomicReaderContext ctx : r.leaves()) {
+    for (LeafReaderContext ctx : r.leaves()) {
       checkTermsOrder(ctx.reader(), allTerms, false);
     }
     checkTermsOrder(r, allTerms, true);
diff --git lucene/core/src/test/org/apache/lucene/index/TestMixedDocValuesUpdates.java lucene/core/src/test/org/apache/lucene/index/TestMixedDocValuesUpdates.java
index afadc6c..e73d5e6 100644
--- lucene/core/src/test/org/apache/lucene/index/TestMixedDocValuesUpdates.java
+++ lucene/core/src/test/org/apache/lucene/index/TestMixedDocValuesUpdates.java
@@ -114,8 +114,8 @@ public class TestMixedDocValuesUpdates extends LuceneTestCase {
       reader = newReader;
 //      System.out.println("[" + Thread.currentThread().getName() + "]: reopened reader: " + reader);
       assertTrue(reader.numDocs() > 0); // we delete at most one document per round
-      for (AtomicReaderContext context : reader.leaves()) {
-        AtomicReader r = context.reader();
+      for (LeafReaderContext context : reader.leaves()) {
+        LeafReader r = context.reader();
 //        System.out.println(((SegmentReader) r).getSegmentName());
         Bits liveDocs = r.getLiveDocs();
         for (int field = 0; field < fieldValues.length; field++) {
@@ -259,8 +259,8 @@ public class TestMixedDocValuesUpdates extends LuceneTestCase {
     
     DirectoryReader reader = DirectoryReader.open(dir);
     BytesRef scratch = new BytesRef();
-    for (AtomicReaderContext context : reader.leaves()) {
-      AtomicReader r = context.reader();
+    for (LeafReaderContext context : reader.leaves()) {
+      LeafReader r = context.reader();
       for (int i = 0; i < numFields; i++) {
         BinaryDocValues bdv = r.getBinaryDocValues("f" + i);
         NumericDocValues control = r.getNumericDocValues("cf" + i);
@@ -310,8 +310,8 @@ public class TestMixedDocValuesUpdates extends LuceneTestCase {
       writer.updateDocValues(t, new BinaryDocValuesField("f", TestBinaryDocValuesUpdates.toBytes(value)),
           new NumericDocValuesField("cf", value*2));
       DirectoryReader reader = DirectoryReader.open(writer, true);
-      for (AtomicReaderContext context : reader.leaves()) {
-        AtomicReader r = context.reader();
+      for (LeafReaderContext context : reader.leaves()) {
+        LeafReader r = context.reader();
         BinaryDocValues fbdv = r.getBinaryDocValues("f");
         NumericDocValues cfndv = r.getNumericDocValues("cf");
         for (int j = 0; j < r.maxDoc(); j++) {
@@ -377,9 +377,9 @@ public class TestMixedDocValuesUpdates extends LuceneTestCase {
     writer.close();
     
     DirectoryReader reader = DirectoryReader.open(dir);
-    for (AtomicReaderContext context : reader.leaves()) {
+    for (LeafReaderContext context : reader.leaves()) {
       for (int i = 0; i < numBinaryFields; i++) {
-        AtomicReader r = context.reader();
+        LeafReader r = context.reader();
         BinaryDocValues f = r.getBinaryDocValues("f" + i);
         NumericDocValues cf = r.getNumericDocValues("cf" + i);
         for (int j = 0; j < r.maxDoc(); j++) {
diff --git lucene/core/src/test/org/apache/lucene/index/TestMultiDocValues.java lucene/core/src/test/org/apache/lucene/index/TestMultiDocValues.java
index b6698e0..c53824f 100644
--- lucene/core/src/test/org/apache/lucene/index/TestMultiDocValues.java
+++ lucene/core/src/test/org/apache/lucene/index/TestMultiDocValues.java
@@ -56,7 +56,7 @@ public class TestMultiDocValues extends LuceneTestCase {
     DirectoryReader ir = iw.getReader();
     iw.forceMerge(1);
     DirectoryReader ir2 = iw.getReader();
-    AtomicReader merged = getOnlySegmentReader(ir2);
+    LeafReader merged = getOnlySegmentReader(ir2);
     iw.close();
     
     NumericDocValues multi = MultiDocValues.getNumericValues(ir, "numbers");
@@ -91,7 +91,7 @@ public class TestMultiDocValues extends LuceneTestCase {
     DirectoryReader ir = iw.getReader();
     iw.forceMerge(1);
     DirectoryReader ir2 = iw.getReader();
-    AtomicReader merged = getOnlySegmentReader(ir2);
+    LeafReader merged = getOnlySegmentReader(ir2);
     iw.close();
     
     BinaryDocValues multi = MultiDocValues.getBinaryValues(ir, "bytes");
@@ -131,7 +131,7 @@ public class TestMultiDocValues extends LuceneTestCase {
     DirectoryReader ir = iw.getReader();
     iw.forceMerge(1);
     DirectoryReader ir2 = iw.getReader();
-    AtomicReader merged = getOnlySegmentReader(ir2);
+    LeafReader merged = getOnlySegmentReader(ir2);
     iw.close();
     
     SortedDocValues multi = MultiDocValues.getSortedValues(ir, "bytes");
@@ -173,7 +173,7 @@ public class TestMultiDocValues extends LuceneTestCase {
     DirectoryReader ir = iw.getReader();
     iw.forceMerge(1);
     DirectoryReader ir2 = iw.getReader();
-    AtomicReader merged = getOnlySegmentReader(ir2);
+    LeafReader merged = getOnlySegmentReader(ir2);
     iw.close();
     
     SortedDocValues multi = MultiDocValues.getSortedValues(ir, "bytes");
@@ -214,7 +214,7 @@ public class TestMultiDocValues extends LuceneTestCase {
     DirectoryReader ir = iw.getReader();
     iw.forceMerge(1);
     DirectoryReader ir2 = iw.getReader();
-    AtomicReader merged = getOnlySegmentReader(ir2);
+    LeafReader merged = getOnlySegmentReader(ir2);
     iw.close();
     
     SortedSetDocValues multi = MultiDocValues.getSortedSetValues(ir, "bytes");
@@ -276,7 +276,7 @@ public class TestMultiDocValues extends LuceneTestCase {
     DirectoryReader ir = iw.getReader();
     iw.forceMerge(1);
     DirectoryReader ir2 = iw.getReader();
-    AtomicReader merged = getOnlySegmentReader(ir2);
+    LeafReader merged = getOnlySegmentReader(ir2);
     iw.close();
     
     SortedSetDocValues multi = MultiDocValues.getSortedSetValues(ir, "bytes");
@@ -337,7 +337,7 @@ public class TestMultiDocValues extends LuceneTestCase {
     DirectoryReader ir = iw.getReader();
     iw.forceMerge(1);
     DirectoryReader ir2 = iw.getReader();
-    AtomicReader merged = getOnlySegmentReader(ir2);
+    LeafReader merged = getOnlySegmentReader(ir2);
     iw.close();
     
     SortedNumericDocValues multi = MultiDocValues.getSortedNumericValues(ir, "nums");
@@ -388,7 +388,7 @@ public class TestMultiDocValues extends LuceneTestCase {
     DirectoryReader ir = iw.getReader();
     iw.forceMerge(1);
     DirectoryReader ir2 = iw.getReader();
-    AtomicReader merged = getOnlySegmentReader(ir2);
+    LeafReader merged = getOnlySegmentReader(ir2);
     iw.close();
     
     Bits multi = MultiDocValues.getDocsWithField(ir, "numbers");
diff --git lucene/core/src/test/org/apache/lucene/index/TestMultiLevelSkipList.java lucene/core/src/test/org/apache/lucene/index/TestMultiLevelSkipList.java
index caf1cdc..f67d508 100644
--- lucene/core/src/test/org/apache/lucene/index/TestMultiLevelSkipList.java
+++ lucene/core/src/test/org/apache/lucene/index/TestMultiLevelSkipList.java
@@ -81,7 +81,7 @@ public class TestMultiLevelSkipList extends LuceneTestCase {
     writer.forceMerge(1);
     writer.close();
 
-    AtomicReader reader = getOnlySegmentReader(DirectoryReader.open(dir));
+    LeafReader reader = getOnlySegmentReader(DirectoryReader.open(dir));
     
     for (int i = 0; i < 2; i++) {
       counter = 0;
diff --git lucene/core/src/test/org/apache/lucene/index/TestNorms.java lucene/core/src/test/org/apache/lucene/index/TestNorms.java
index fa9e0ac..3ffc109 100644
--- lucene/core/src/test/org/apache/lucene/index/TestNorms.java
+++ lucene/core/src/test/org/apache/lucene/index/TestNorms.java
@@ -111,7 +111,7 @@ public class TestNorms extends LuceneTestCase {
   public void testMaxByteNorms() throws IOException {
     Directory dir = newFSDirectory(createTempDir("TestNorms.testMaxByteNorms"));
     buildIndex(dir);
-    AtomicReader open = SlowCompositeReaderWrapper.wrap(DirectoryReader.open(dir));
+    LeafReader open = SlowCompositeReaderWrapper.wrap(DirectoryReader.open(dir));
     NumericDocValues normValues = open.getNormValues(byteTestField);
     assertNotNull(normValues);
     for (int i = 0; i < open.maxDoc(); i++) {
@@ -192,7 +192,7 @@ public class TestNorms extends LuceneTestCase {
     }
 
     @Override
-    public SimScorer simScorer(SimWeight weight, AtomicReaderContext context) throws IOException {
+    public SimScorer simScorer(SimWeight weight, LeafReaderContext context) throws IOException {
       throw new UnsupportedOperationException();
     }
   } 
diff --git lucene/core/src/test/org/apache/lucene/index/TestNumericDocValuesUpdates.java lucene/core/src/test/org/apache/lucene/index/TestNumericDocValuesUpdates.java
index 1e336f3..4bb834a 100644
--- lucene/core/src/test/org/apache/lucene/index/TestNumericDocValuesUpdates.java
+++ lucene/core/src/test/org/apache/lucene/index/TestNumericDocValuesUpdates.java
@@ -9,7 +9,6 @@ import java.util.concurrent.atomic.AtomicInteger;
 
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.analysis.MockTokenizer;
-import org.apache.lucene.codecs.Codec;
 import org.apache.lucene.codecs.DocValuesFormat;
 import org.apache.lucene.codecs.asserting.AssertingDocValuesFormat;
 import org.apache.lucene.codecs.lucene410.Lucene410DocValuesFormat;
@@ -109,7 +108,7 @@ public class TestNumericDocValuesUpdates extends LuceneTestCase {
     }
     
     assertEquals(1, reader.leaves().size());
-    AtomicReader r = reader.leaves().get(0).reader();
+    LeafReader r = reader.leaves().get(0).reader();
     NumericDocValues ndv = r.getNumericDocValues("val");
     assertEquals(2, ndv.get(0));
     assertEquals(2, ndv.get(1));
@@ -151,8 +150,8 @@ public class TestNumericDocValuesUpdates extends LuceneTestCase {
       writer.close();
     }
     
-    for (AtomicReaderContext context : reader.leaves()) {
-      AtomicReader r = context.reader();
+    for (LeafReaderContext context : reader.leaves()) {
+      LeafReader r = context.reader();
       NumericDocValues ndv = r.getNumericDocValues("val");
       assertNotNull(ndv);
       for (int i = 0; i < r.maxDoc(); i++) {
@@ -234,7 +233,7 @@ public class TestNumericDocValuesUpdates extends LuceneTestCase {
       writer.close();
     }
     
-    AtomicReader slow = SlowCompositeReaderWrapper.wrap(reader);
+    LeafReader slow = SlowCompositeReaderWrapper.wrap(reader);
     
     Bits liveDocs = slow.getLiveDocs();
     boolean[] expectedLiveDocs = new boolean[] { true, false, false, true, true, true };
@@ -279,7 +278,7 @@ public class TestNumericDocValuesUpdates extends LuceneTestCase {
       writer.close();
     }
     
-    AtomicReader r = reader.leaves().get(0).reader();
+    LeafReader r = reader.leaves().get(0).reader();
     assertFalse(r.getLiveDocs().get(0));
     assertEquals(17, r.getNumericDocValues("val").get(1));
     
@@ -314,7 +313,7 @@ public class TestNumericDocValuesUpdates extends LuceneTestCase {
       writer.close();
     }
     
-    AtomicReader r = reader.leaves().get(0).reader();
+    LeafReader r = reader.leaves().get(0).reader();
     assertFalse(r.getLiveDocs().get(0));
     assertEquals(1, r.getNumericDocValues("val").get(0)); // deletes are currently applied first
     
@@ -346,7 +345,7 @@ public class TestNumericDocValuesUpdates extends LuceneTestCase {
     writer.close();
     
     final DirectoryReader reader = DirectoryReader.open(dir);
-    AtomicReader r = reader.leaves().get(0).reader();
+    LeafReader r = reader.leaves().get(0).reader();
     NumericDocValues ndv = r.getNumericDocValues("ndv");
     BinaryDocValues bdv = r.getBinaryDocValues("bdv");
     SortedDocValues sdv = r.getSortedDocValues("sdv");
@@ -394,7 +393,7 @@ public class TestNumericDocValuesUpdates extends LuceneTestCase {
     writer.close();
     
     final DirectoryReader reader = DirectoryReader.open(dir);
-    AtomicReader r = reader.leaves().get(0).reader();
+    LeafReader r = reader.leaves().get(0).reader();
     NumericDocValues ndv1 = r.getNumericDocValues("ndv1");
     NumericDocValues ndv2 = r.getNumericDocValues("ndv2");
     for (int i = 0; i < r.maxDoc(); i++) {
@@ -427,7 +426,7 @@ public class TestNumericDocValuesUpdates extends LuceneTestCase {
     writer.close();
     
     final DirectoryReader reader = DirectoryReader.open(dir);
-    AtomicReader r = reader.leaves().get(0).reader();
+    LeafReader r = reader.leaves().get(0).reader();
     NumericDocValues ndv = r.getNumericDocValues("ndv");
     for (int i = 0; i < r.maxDoc(); i++) {
       assertEquals(17, ndv.get(i));
@@ -495,7 +494,7 @@ public class TestNumericDocValuesUpdates extends LuceneTestCase {
     
     final DirectoryReader reader = DirectoryReader.open(dir);
     
-    AtomicReader r = SlowCompositeReaderWrapper.wrap(reader);
+    LeafReader r = SlowCompositeReaderWrapper.wrap(reader);
     NumericDocValues ndv = r.getNumericDocValues("ndv");
     SortedDocValues sdv = r.getSortedDocValues("sorted");
     for (int i = 0; i < r.maxDoc(); i++) {
@@ -526,7 +525,7 @@ public class TestNumericDocValuesUpdates extends LuceneTestCase {
     writer.close();
     
     final DirectoryReader reader = DirectoryReader.open(dir);
-    final AtomicReader r = SlowCompositeReaderWrapper.wrap(reader);
+    final LeafReader r = SlowCompositeReaderWrapper.wrap(reader);
     NumericDocValues ndv = r.getNumericDocValues("ndv");
     for (int i = 0; i < r.maxDoc(); i++) {
       assertEquals(3, ndv.get(i));
@@ -593,7 +592,7 @@ public class TestNumericDocValuesUpdates extends LuceneTestCase {
       }
       
       assertEquals(1, reader.leaves().size());
-      final AtomicReader r = reader.leaves().get(0).reader();
+      final LeafReader r = reader.leaves().get(0).reader();
       assertNull("index should have no deletes after forceMerge", r.getLiveDocs());
       NumericDocValues ndv = r.getNumericDocValues("ndv");
       assertNotNull(ndv);
@@ -627,7 +626,7 @@ public class TestNumericDocValuesUpdates extends LuceneTestCase {
     writer.close();
     
     final DirectoryReader reader = DirectoryReader.open(dir);
-    final AtomicReader r = SlowCompositeReaderWrapper.wrap(reader);
+    final LeafReader r = SlowCompositeReaderWrapper.wrap(reader);
     NumericDocValues ndv = r.getNumericDocValues("ndv");
     for (int i = 0; i < r.maxDoc(); i++) {
       assertEquals(3, ndv.get(i));
@@ -701,8 +700,8 @@ public class TestNumericDocValuesUpdates extends LuceneTestCase {
       reader = newReader;
 //      System.out.println("[" + Thread.currentThread().getName() + "]: reopened reader: " + reader);
       assertTrue(reader.numDocs() > 0); // we delete at most one document per round
-      for (AtomicReaderContext context : reader.leaves()) {
-        AtomicReader r = context.reader();
+      for (LeafReaderContext context : reader.leaves()) {
+        LeafReader r = context.reader();
 //        System.out.println(((SegmentReader) r).getSegmentName());
         Bits liveDocs = r.getLiveDocs();
         for (int field = 0; field < fieldValues.length; field++) {
@@ -766,8 +765,8 @@ public class TestNumericDocValuesUpdates extends LuceneTestCase {
     writer.close();
 
     DirectoryReader reader = DirectoryReader.open(dir);
-    for (AtomicReaderContext context : reader.leaves()) {
-      AtomicReader r = context.reader();
+    for (LeafReaderContext context : reader.leaves()) {
+      LeafReader r = context.reader();
       NumericDocValues ndv = r.getNumericDocValues("ndv");
       Bits docsWithField = r.getDocsWithField("ndv");
       assertNotNull(docsWithField);
@@ -811,8 +810,8 @@ public class TestNumericDocValuesUpdates extends LuceneTestCase {
     writer.close();
 
     DirectoryReader reader = DirectoryReader.open(dir);
-    for (AtomicReaderContext context : reader.leaves()) {
-      AtomicReader r = context.reader();
+    for (LeafReaderContext context : reader.leaves()) {
+      LeafReader r = context.reader();
       NumericDocValues ndv = r.getNumericDocValues("ndv");
       for (int i = 0; i < r.maxDoc(); i++) {
         assertEquals(5L, ndv.get(i));
@@ -954,8 +953,8 @@ public class TestNumericDocValuesUpdates extends LuceneTestCase {
     writer.close();
     
     DirectoryReader reader = DirectoryReader.open(dir);
-    for (AtomicReaderContext context : reader.leaves()) {
-      AtomicReader r = context.reader();
+    for (LeafReaderContext context : reader.leaves()) {
+      LeafReader r = context.reader();
       for (int i = 0; i < numFields; i++) {
         NumericDocValues ndv = r.getNumericDocValues("f" + i);
         NumericDocValues control = r.getNumericDocValues("cf" + i);
@@ -1000,8 +999,8 @@ public class TestNumericDocValuesUpdates extends LuceneTestCase {
       long value = random().nextLong();
       writer.updateDocValues(t, new NumericDocValuesField("f", value), new NumericDocValuesField("cf", value*2));
       DirectoryReader reader = DirectoryReader.open(writer, true);
-      for (AtomicReaderContext context : reader.leaves()) {
-        AtomicReader r = context.reader();
+      for (LeafReaderContext context : reader.leaves()) {
+        LeafReader r = context.reader();
         NumericDocValues fndv = r.getNumericDocValues("f");
         NumericDocValues cfndv = r.getNumericDocValues("cf");
         for (int j = 0; j < r.maxDoc(); j++) {
@@ -1052,7 +1051,7 @@ public class TestNumericDocValuesUpdates extends LuceneTestCase {
     writer.close();
     
     DirectoryReader reader = DirectoryReader.open(dir);
-    AtomicReader r = SlowCompositeReaderWrapper.wrap(reader);
+    LeafReader r = SlowCompositeReaderWrapper.wrap(reader);
     NumericDocValues f1 = r.getNumericDocValues("f1");
     NumericDocValues f2 = r.getNumericDocValues("f2");
     assertEquals(12L, f1.get(0));
@@ -1108,8 +1107,8 @@ public class TestNumericDocValuesUpdates extends LuceneTestCase {
     writer.close();
     
     DirectoryReader reader = DirectoryReader.open(dir2);
-    for (AtomicReaderContext context : reader.leaves()) {
-      AtomicReader r = context.reader();
+    for (LeafReaderContext context : reader.leaves()) {
+      LeafReader r = context.reader();
       NumericDocValues ndv = r.getNumericDocValues("ndv");
       NumericDocValues control = r.getNumericDocValues("control");
       for (int i = 0; i < r.maxDoc(); i++) {
@@ -1208,9 +1207,9 @@ public class TestNumericDocValuesUpdates extends LuceneTestCase {
     writer.close();
     
     DirectoryReader reader = DirectoryReader.open(dir);
-    for (AtomicReaderContext context : reader.leaves()) {
+    for (LeafReaderContext context : reader.leaves()) {
       for (int i = 0; i < numNumericFields; i++) {
-        AtomicReader r = context.reader();
+        LeafReader r = context.reader();
         NumericDocValues f = r.getNumericDocValues("f" + i);
         NumericDocValues cf = r.getNumericDocValues("cf" + i);
         for (int j = 0; j < r.maxDoc(); j++) {
diff --git lucene/core/src/test/org/apache/lucene/index/TestOmitTf.java lucene/core/src/test/org/apache/lucene/index/TestOmitTf.java
index eb4a7c7..666f471 100644
--- lucene/core/src/test/org/apache/lucene/index/TestOmitTf.java
+++ lucene/core/src/test/org/apache/lucene/index/TestOmitTf.java
@@ -435,7 +435,7 @@ public class TestOmitTf extends LuceneTestCase {
     public static int getSum() { return sum; }
     
     @Override
-    protected void doSetNextReader(AtomicReaderContext context) throws IOException {
+    protected void doSetNextReader(LeafReaderContext context) throws IOException {
       docBase = context.docBase;
     }
     @Override
diff --git lucene/core/src/test/org/apache/lucene/index/TestOrdinalMap.java lucene/core/src/test/org/apache/lucene/index/TestOrdinalMap.java
index d301db7..2aa43ff 100644
--- lucene/core/src/test/org/apache/lucene/index/TestOrdinalMap.java
+++ lucene/core/src/test/org/apache/lucene/index/TestOrdinalMap.java
@@ -84,7 +84,7 @@ public class TestOrdinalMap extends LuceneTestCase {
     }
     iw.commit();
     DirectoryReader r = iw.getReader();
-    AtomicReader ar = SlowCompositeReaderWrapper.wrap(r);
+    LeafReader ar = SlowCompositeReaderWrapper.wrap(r);
     SortedDocValues sdv = ar.getSortedDocValues("sdv");
     if (sdv instanceof MultiSortedDocValues) {
       OrdinalMap map = ((MultiSortedDocValues) sdv).mapping;
diff --git lucene/core/src/test/org/apache/lucene/index/TestParallelAtomicReader.java lucene/core/src/test/org/apache/lucene/index/TestParallelAtomicReader.java
index e049033..d406444 100644
--- lucene/core/src/test/org/apache/lucene/index/TestParallelAtomicReader.java
+++ lucene/core/src/test/org/apache/lucene/index/TestParallelAtomicReader.java
@@ -63,7 +63,7 @@ public class TestParallelAtomicReader extends LuceneTestCase {
   public void testFieldNames() throws Exception {
     Directory dir1 = getDir1(random());
     Directory dir2 = getDir2(random());
-    ParallelAtomicReader pr = new ParallelAtomicReader(SlowCompositeReaderWrapper.wrap(DirectoryReader.open(dir1)),
+    ParallelLeafReader pr = new ParallelLeafReader(SlowCompositeReaderWrapper.wrap(DirectoryReader.open(dir1)),
                                                        SlowCompositeReaderWrapper.wrap(DirectoryReader.open(dir2)));
     FieldInfos fieldInfos = pr.getFieldInfos();
     assertEquals(4, fieldInfos.size());
@@ -79,9 +79,9 @@ public class TestParallelAtomicReader extends LuceneTestCase {
   public void testRefCounts1() throws IOException {
     Directory dir1 = getDir1(random());
     Directory dir2 = getDir2(random());
-    AtomicReader ir1, ir2;
+    LeafReader ir1, ir2;
     // close subreaders, ParallelReader will not change refCounts, but close on its own close
-    ParallelAtomicReader pr = new ParallelAtomicReader(ir1 = SlowCompositeReaderWrapper.wrap(DirectoryReader.open(dir1)),
+    ParallelLeafReader pr = new ParallelLeafReader(ir1 = SlowCompositeReaderWrapper.wrap(DirectoryReader.open(dir1)),
                                                        ir2 = SlowCompositeReaderWrapper.wrap(DirectoryReader.open(dir2)));
                                                        
     // check RefCounts
@@ -97,10 +97,10 @@ public class TestParallelAtomicReader extends LuceneTestCase {
   public void testRefCounts2() throws IOException {
     Directory dir1 = getDir1(random());
     Directory dir2 = getDir2(random());
-    AtomicReader ir1 = SlowCompositeReaderWrapper.wrap(DirectoryReader.open(dir1));
-    AtomicReader ir2 = SlowCompositeReaderWrapper.wrap(DirectoryReader.open(dir2));
+    LeafReader ir1 = SlowCompositeReaderWrapper.wrap(DirectoryReader.open(dir1));
+    LeafReader ir2 = SlowCompositeReaderWrapper.wrap(DirectoryReader.open(dir2));
     // don't close subreaders, so ParallelReader will increment refcounts
-    ParallelAtomicReader pr = new ParallelAtomicReader(false, ir1, ir2);
+    ParallelLeafReader pr = new ParallelLeafReader(false, ir1, ir2);
     // check RefCounts
     assertEquals(2, ir1.getRefCount());
     assertEquals(2, ir2.getRefCount());
@@ -117,12 +117,12 @@ public class TestParallelAtomicReader extends LuceneTestCase {
   
   public void testCloseInnerReader() throws Exception {
     Directory dir1 = getDir1(random());
-    AtomicReader ir1 = SlowCompositeReaderWrapper.wrap(DirectoryReader.open(dir1));
+    LeafReader ir1 = SlowCompositeReaderWrapper.wrap(DirectoryReader.open(dir1));
     
     // with overlapping
-    ParallelAtomicReader pr = new ParallelAtomicReader(true,
-     new AtomicReader[] {ir1},
-     new AtomicReader[] {ir1});
+    ParallelLeafReader pr = new ParallelLeafReader(true,
+     new LeafReader[] {ir1},
+     new LeafReader[] {ir1});
 
     ir1.close();
     
@@ -151,20 +151,20 @@ public class TestParallelAtomicReader extends LuceneTestCase {
     w2.addDocument(d3);
     w2.close();
     
-    AtomicReader ir1 = SlowCompositeReaderWrapper.wrap(DirectoryReader.open(dir1));
-    AtomicReader ir2 = SlowCompositeReaderWrapper.wrap(DirectoryReader.open(dir2));
+    LeafReader ir1 = SlowCompositeReaderWrapper.wrap(DirectoryReader.open(dir1));
+    LeafReader ir2 = SlowCompositeReaderWrapper.wrap(DirectoryReader.open(dir2));
 
     try {
-      new ParallelAtomicReader(ir1, ir2);
+      new ParallelLeafReader(ir1, ir2);
       fail("didn't get exptected exception: indexes don't have same number of documents");
     } catch (IllegalArgumentException e) {
       // expected exception
     }
 
     try {
-      new ParallelAtomicReader(random().nextBoolean(),
-                               new AtomicReader[] {ir1, ir2},
-                               new AtomicReader[] {ir1, ir2});
+      new ParallelLeafReader(random().nextBoolean(),
+                               new LeafReader[] {ir1, ir2},
+                               new LeafReader[] {ir1, ir2});
       fail("didn't get expected exception: indexes don't have same number of documents");
     } catch (IllegalArgumentException e) {
       // expected exception
@@ -181,13 +181,13 @@ public class TestParallelAtomicReader extends LuceneTestCase {
   public void testIgnoreStoredFields() throws IOException {
     Directory dir1 = getDir1(random());
     Directory dir2 = getDir2(random());
-    AtomicReader ir1 = SlowCompositeReaderWrapper.wrap(DirectoryReader.open(dir1));
-    AtomicReader ir2 = SlowCompositeReaderWrapper.wrap(DirectoryReader.open(dir2));
+    LeafReader ir1 = SlowCompositeReaderWrapper.wrap(DirectoryReader.open(dir1));
+    LeafReader ir2 = SlowCompositeReaderWrapper.wrap(DirectoryReader.open(dir2));
     
     // with overlapping
-    ParallelAtomicReader pr = new ParallelAtomicReader(false,
-        new AtomicReader[] {ir1, ir2},
-        new AtomicReader[] {ir1});
+    ParallelLeafReader pr = new ParallelLeafReader(false,
+        new LeafReader[] {ir1, ir2},
+        new LeafReader[] {ir1});
     assertEquals("v1", pr.document(0).get("f1"));
     assertEquals("v1", pr.document(0).get("f2"));
     assertNull(pr.document(0).get("f3"));
@@ -200,9 +200,9 @@ public class TestParallelAtomicReader extends LuceneTestCase {
     pr.close();
     
     // no stored fields at all
-    pr = new ParallelAtomicReader(false,
-        new AtomicReader[] {ir2},
-        new AtomicReader[0]);
+    pr = new ParallelLeafReader(false,
+        new LeafReader[] {ir2},
+        new LeafReader[0]);
     assertNull(pr.document(0).get("f1"));
     assertNull(pr.document(0).get("f2"));
     assertNull(pr.document(0).get("f3"));
@@ -215,9 +215,9 @@ public class TestParallelAtomicReader extends LuceneTestCase {
     pr.close();
     
     // without overlapping
-    pr = new ParallelAtomicReader(true,
-        new AtomicReader[] {ir2},
-        new AtomicReader[] {ir1});
+    pr = new ParallelLeafReader(true,
+        new LeafReader[] {ir2},
+        new LeafReader[] {ir1});
     assertEquals("v1", pr.document(0).get("f1"));
     assertEquals("v1", pr.document(0).get("f2"));
     assertNull(pr.document(0).get("f3"));
@@ -231,9 +231,9 @@ public class TestParallelAtomicReader extends LuceneTestCase {
     
     // no main readers
     try {
-      new ParallelAtomicReader(true,
-        new AtomicReader[0],
-        new AtomicReader[] {ir1});
+      new ParallelLeafReader(true,
+        new LeafReader[0],
+        new LeafReader[] {ir1});
       fail("didn't get expected exception: need a non-empty main-reader array");
     } catch (IllegalArgumentException iae) {
       // pass
@@ -284,7 +284,7 @@ public class TestParallelAtomicReader extends LuceneTestCase {
   private IndexSearcher parallel(Random random) throws IOException {
     dir1 = getDir1(random);
     dir2 = getDir2(random);
-    ParallelAtomicReader pr = new ParallelAtomicReader(
+    ParallelLeafReader pr = new ParallelLeafReader(
         SlowCompositeReaderWrapper.wrap(DirectoryReader.open(dir1)),
         SlowCompositeReaderWrapper.wrap(DirectoryReader.open(dir2)));
     TestUtil.checkReader(pr);
diff --git lucene/core/src/test/org/apache/lucene/index/TestParallelCompositeReader.java lucene/core/src/test/org/apache/lucene/index/TestParallelCompositeReader.java
index c544254..64ccb79 100644
--- lucene/core/src/test/org/apache/lucene/index/TestParallelCompositeReader.java
+++ lucene/core/src/test/org/apache/lucene/index/TestParallelCompositeReader.java
@@ -137,7 +137,7 @@ public class TestParallelCompositeReader extends LuceneTestCase {
 
     assertEquals(3, pr.leaves().size());
 
-    for(AtomicReaderContext cxt : pr.leaves()) {
+    for(LeafReaderContext cxt : pr.leaves()) {
       cxt.reader().addReaderClosedListener(new ReaderClosedListener() {
           @Override
           public void onClose(IndexReader reader) {
@@ -165,7 +165,7 @@ public class TestParallelCompositeReader extends LuceneTestCase {
 
     assertEquals(3, pr.leaves().size());
 
-    for(AtomicReaderContext cxt : pr.leaves()) {
+    for(LeafReaderContext cxt : pr.leaves()) {
       cxt.reader().addReaderClosedListener(new ReaderClosedListener() {
           @Override
           public void onClose(IndexReader reader) {
@@ -324,7 +324,7 @@ public class TestParallelCompositeReader extends LuceneTestCase {
     assertNull(pr.document(0).get("f3"));
     assertNull(pr.document(0).get("f4"));
     // check that fields are there
-    AtomicReader slow = SlowCompositeReaderWrapper.wrap(pr);
+    LeafReader slow = SlowCompositeReaderWrapper.wrap(pr);
     assertNotNull(slow.terms("f1"));
     assertNotNull(slow.terms("f2"));
     assertNotNull(slow.terms("f3"));
diff --git lucene/core/src/test/org/apache/lucene/index/TestParallelReaderEmptyIndex.java lucene/core/src/test/org/apache/lucene/index/TestParallelReaderEmptyIndex.java
index c0607c9..ea8baca 100644
--- lucene/core/src/test/org/apache/lucene/index/TestParallelReaderEmptyIndex.java
+++ lucene/core/src/test/org/apache/lucene/index/TestParallelReaderEmptyIndex.java
@@ -29,7 +29,7 @@ import org.apache.lucene.store.Directory;
 import org.apache.lucene.util.LuceneTestCase;
 
 /**
- * Some tests for {@link ParallelAtomicReader}s with empty indexes
+ * Some tests for {@link ParallelLeafReader}s with empty indexes
  */
 public class TestParallelReaderEmptyIndex extends LuceneTestCase {
 
@@ -48,7 +48,7 @@ public class TestParallelReaderEmptyIndex extends LuceneTestCase {
 
     IndexWriter iwOut = new IndexWriter(rdOut, newIndexWriterConfig(new MockAnalyzer(random())));
     
-    ParallelAtomicReader apr = new ParallelAtomicReader(
+    ParallelLeafReader apr = new ParallelLeafReader(
         SlowCompositeReaderWrapper.wrap(DirectoryReader.open(rd1)),
         SlowCompositeReaderWrapper.wrap(DirectoryReader.open(rd2)));
     
@@ -57,7 +57,7 @@ public class TestParallelReaderEmptyIndex extends LuceneTestCase {
     iwOut.forceMerge(1);
     
     // 2nd try with a readerless parallel reader
-    iwOut.addIndexes(new ParallelAtomicReader());
+    iwOut.addIndexes(new ParallelLeafReader());
     iwOut.forceMerge(1);
 
     ParallelCompositeReader cpr = new ParallelCompositeReader(
@@ -135,7 +135,7 @@ public class TestParallelReaderEmptyIndex extends LuceneTestCase {
 
     IndexWriter iwOut = new IndexWriter(rdOut, newIndexWriterConfig(new MockAnalyzer(random())));
     final DirectoryReader reader1, reader2;
-    ParallelAtomicReader pr = new ParallelAtomicReader(
+    ParallelLeafReader pr = new ParallelLeafReader(
         SlowCompositeReaderWrapper.wrap(reader1 = DirectoryReader.open(rd1)),
         SlowCompositeReaderWrapper.wrap(reader2 = DirectoryReader.open(rd2)));
 
diff --git lucene/core/src/test/org/apache/lucene/index/TestParallelTermEnum.java lucene/core/src/test/org/apache/lucene/index/TestParallelTermEnum.java
index 20fdbcd..7fd9182 100644
--- lucene/core/src/test/org/apache/lucene/index/TestParallelTermEnum.java
+++ lucene/core/src/test/org/apache/lucene/index/TestParallelTermEnum.java
@@ -31,8 +31,8 @@ import org.apache.lucene.util.LuceneTestCase;
 import org.apache.lucene.util.TestUtil;
 
 public class TestParallelTermEnum extends LuceneTestCase {
-  private AtomicReader ir1;
-  private AtomicReader ir2;
+  private LeafReader ir1;
+  private LeafReader ir2;
   private Directory rd1;
   private Directory rd2;
   
@@ -89,7 +89,7 @@ public class TestParallelTermEnum extends LuceneTestCase {
   }
 
   public void test1() throws IOException {
-    ParallelAtomicReader pr = new ParallelAtomicReader(ir1, ir2);
+    ParallelLeafReader pr = new ParallelLeafReader(ir1, ir2);
 
     Bits liveDocs = pr.getLiveDocs();
 
diff --git lucene/core/src/test/org/apache/lucene/index/TestPayloads.java lucene/core/src/test/org/apache/lucene/index/TestPayloads.java
index 2ce4614..a9e068c 100644
--- lucene/core/src/test/org/apache/lucene/index/TestPayloads.java
+++ lucene/core/src/test/org/apache/lucene/index/TestPayloads.java
@@ -603,7 +603,7 @@ public class TestPayloads extends LuceneTestCase {
     field.setTokenStream(ts);
     writer.addDocument(doc);
     DirectoryReader reader = writer.getReader();
-    AtomicReader sr = SlowCompositeReaderWrapper.wrap(reader);
+    LeafReader sr = SlowCompositeReaderWrapper.wrap(reader);
     DocsAndPositionsEnum de = sr.termPositionsEnum(new Term("field", "withPayload"));
     de.nextDoc();
     de.nextPosition();
diff --git lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets.java lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets.java
index 2722f74..80e149d 100644
--- lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets.java
+++ lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets.java
@@ -290,9 +290,9 @@ public class TestPostingsOffsets extends LuceneTestCase {
     w.close();
 
     final String[] terms = new String[] {"a", "b", "c", "d"};
-    for(AtomicReaderContext ctx : r.leaves()) {
+    for(LeafReaderContext ctx : r.leaves()) {
       // TODO: improve this
-      AtomicReader sub = ctx.reader();
+      LeafReader sub = ctx.reader();
       //System.out.println("\nsub=" + sub);
       final TermsEnum termsEnum = sub.fields().terms("content").iterator(null);
       DocsEnum docs = null;
@@ -379,7 +379,7 @@ public class TestPostingsOffsets extends LuceneTestCase {
       riw.addDocument(doc);
     }
     CompositeReader ir = riw.getReader();
-    AtomicReader slow = SlowCompositeReaderWrapper.wrap(ir);
+    LeafReader slow = SlowCompositeReaderWrapper.wrap(ir);
     FieldInfos fis = slow.getFieldInfos();
     assertEquals(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS, fis.fieldInfo("foo").getIndexOptions());
     slow.close();
diff --git lucene/core/src/test/org/apache/lucene/index/TestReaderClosed.java lucene/core/src/test/org/apache/lucene/index/TestReaderClosed.java
index 9505576..bbfa396 100644
--- lucene/core/src/test/org/apache/lucene/index/TestReaderClosed.java
+++ lucene/core/src/test/org/apache/lucene/index/TestReaderClosed.java
@@ -72,7 +72,7 @@ public class TestReaderClosed extends LuceneTestCase {
   public void testReaderChaining() throws Exception {
     assertTrue(reader.getRefCount() > 0);
     IndexReader wrappedReader = SlowCompositeReaderWrapper.wrap(reader);
-    wrappedReader = new ParallelAtomicReader((AtomicReader) wrappedReader);
+    wrappedReader = new ParallelLeafReader((LeafReader) wrappedReader);
 
     IndexSearcher searcher = newSearcher(wrappedReader);
 
diff --git lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger.java lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger.java
index 041ca16..230200f 100644
--- lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger.java
+++ lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger.java
@@ -80,7 +80,7 @@ public class TestSegmentMerger extends LuceneTestCase {
     final Codec codec = Codec.getDefault();
     final SegmentInfo si = new SegmentInfo(mergedDir, Version.LATEST, mergedSegment, -1, false, codec, null);
 
-    SegmentMerger merger = new SegmentMerger(Arrays.<AtomicReader>asList(reader1, reader2),
+    SegmentMerger merger = new SegmentMerger(Arrays.<LeafReader>asList(reader1, reader2),
         si, InfoStream.getDefault(), mergedDir,
         MergeState.CheckAbort.NONE, new FieldInfos.FieldNumbers(), newIOContext(random()), true);
     MergeState mergeState = merger.merge();
diff --git lucene/core/src/test/org/apache/lucene/index/TestSegmentReader.java lucene/core/src/test/org/apache/lucene/index/TestSegmentReader.java
index f9f1f49..e1d0f03 100644
--- lucene/core/src/test/org/apache/lucene/index/TestSegmentReader.java
+++ lucene/core/src/test/org/apache/lucene/index/TestSegmentReader.java
@@ -172,7 +172,7 @@ public class TestSegmentReader extends LuceneTestCase {
     checkNorms(reader);
   }
 
-  public static void checkNorms(AtomicReader reader) throws IOException {
+  public static void checkNorms(LeafReader reader) throws IOException {
     // test omit norms
     for (int i=0; i<DocHelper.fields.length; i++) {
       IndexableField f = DocHelper.fields[i];
diff --git lucene/core/src/test/org/apache/lucene/index/TestStressIndexing2.java lucene/core/src/test/org/apache/lucene/index/TestStressIndexing2.java
index e0a501d..a37434b 100644
--- lucene/core/src/test/org/apache/lucene/index/TestStressIndexing2.java
+++ lucene/core/src/test/org/apache/lucene/index/TestStressIndexing2.java
@@ -280,9 +280,9 @@ public class TestStressIndexing2 extends LuceneTestCase {
   }
 
   private static void printDocs(DirectoryReader r) throws Throwable {
-    for(AtomicReaderContext ctx : r.leaves()) {
+    for(LeafReaderContext ctx : r.leaves()) {
       // TODO: improve this
-      AtomicReader sub = ctx.reader();
+      LeafReader sub = ctx.reader();
       Bits liveDocs = sub.getLiveDocs();
       System.out.println("  " + ((SegmentReader) sub).getSegmentInfo());
       for(int docID=0;docID<sub.maxDoc();docID++) {
diff --git lucene/core/src/test/org/apache/lucene/index/TestTermVectorsReader.java lucene/core/src/test/org/apache/lucene/index/TestTermVectorsReader.java
index e726517..b4605f1 100644
--- lucene/core/src/test/org/apache/lucene/index/TestTermVectorsReader.java
+++ lucene/core/src/test/org/apache/lucene/index/TestTermVectorsReader.java
@@ -187,7 +187,7 @@ public class TestTermVectorsReader extends LuceneTestCase {
   public void test() throws IOException {
     //Check to see the files were created properly in setup
     DirectoryReader reader = DirectoryReader.open(dir);
-    for (AtomicReaderContext ctx : reader.leaves()) {
+    for (LeafReaderContext ctx : reader.leaves()) {
       SegmentReader sr = (SegmentReader) ctx.reader();
       assertTrue(sr.getFieldInfos().hasVectors());
     }
diff --git lucene/core/src/test/org/apache/lucene/index/TestTermsEnum.java lucene/core/src/test/org/apache/lucene/index/TestTermsEnum.java
index 5df59e0..449c8b5 100644
--- lucene/core/src/test/org/apache/lucene/index/TestTermsEnum.java
+++ lucene/core/src/test/org/apache/lucene/index/TestTermsEnum.java
@@ -735,7 +735,7 @@ public class TestTermsEnum extends LuceneTestCase {
     w.forceMerge(1);
     DirectoryReader r = w.getReader();
     w.close();
-    AtomicReader sub = getOnlySegmentReader(r);
+    LeafReader sub = getOnlySegmentReader(r);
     Terms terms = sub.fields().terms("field");
     Automaton automaton = new RegExp(".*", RegExp.NONE).toAutomaton();
     CompiledAutomaton ca = new CompiledAutomaton(automaton, false, false);    
@@ -789,7 +789,7 @@ public class TestTermsEnum extends LuceneTestCase {
     w.forceMerge(1);
     DirectoryReader r = w.getReader();
     w.close();
-    AtomicReader sub = getOnlySegmentReader(r);
+    LeafReader sub = getOnlySegmentReader(r);
     Terms terms = sub.fields().terms("field");
 
     Automaton automaton = new RegExp(".*d", RegExp.NONE).toAutomaton();
@@ -843,7 +843,7 @@ public class TestTermsEnum extends LuceneTestCase {
     w.forceMerge(1);
     DirectoryReader r = w.getReader();
     w.close();
-    AtomicReader sub = getOnlySegmentReader(r);
+    LeafReader sub = getOnlySegmentReader(r);
     Terms terms = sub.fields().terms("field");
 
     Automaton automaton = new RegExp(".*", RegExp.NONE).toAutomaton();  // accept ALL
diff --git lucene/core/src/test/org/apache/lucene/index/TestTieredMergePolicy.java lucene/core/src/test/org/apache/lucene/index/TestTieredMergePolicy.java
index 43519e2..b237f6d 100644
--- lucene/core/src/test/org/apache/lucene/index/TestTieredMergePolicy.java
+++ lucene/core/src/test/org/apache/lucene/index/TestTieredMergePolicy.java
@@ -18,7 +18,6 @@ package org.apache.lucene.index;
  */
 
 import org.apache.lucene.analysis.MockAnalyzer;
-import org.apache.lucene.codecs.Codec;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.store.Directory;
@@ -233,7 +232,7 @@ public class TestTieredMergePolicy extends BaseMergePolicyTestCase {
     IndexReader r = DirectoryReader.open(w, true);
 
     // Make sure TMP always merged equal-number-of-docs segments:
-    for(AtomicReaderContext ctx : r.leaves()) {
+    for(LeafReaderContext ctx : r.leaves()) {
       int numDocs = ctx.reader().numDocs();
       assertTrue("got numDocs=" + numDocs, numDocs == 100 || numDocs == 1000 || numDocs == 10000);
     }
diff --git lucene/core/src/test/org/apache/lucene/index/TestUniqueTermCount.java lucene/core/src/test/org/apache/lucene/index/TestUniqueTermCount.java
index 8378a76..eab5f11 100644
--- lucene/core/src/test/org/apache/lucene/index/TestUniqueTermCount.java
+++ lucene/core/src/test/org/apache/lucene/index/TestUniqueTermCount.java
@@ -110,7 +110,7 @@ public class TestUniqueTermCount extends LuceneTestCase {
     }
 
     @Override
-    public SimScorer simScorer(SimWeight weight, AtomicReaderContext context) throws IOException {
+    public SimScorer simScorer(SimWeight weight, LeafReaderContext context) throws IOException {
       throw new UnsupportedOperationException();
     }
   }
diff --git lucene/core/src/test/org/apache/lucene/search/JustCompileSearch.java lucene/core/src/test/org/apache/lucene/search/JustCompileSearch.java
index f014510..cf11e26 100644
--- lucene/core/src/test/org/apache/lucene/search/JustCompileSearch.java
+++ lucene/core/src/test/org/apache/lucene/search/JustCompileSearch.java
@@ -19,12 +19,9 @@ package org.apache.lucene.search;
 
 import java.io.IOException;
 
-import org.apache.lucene.index.AtomicReaderContext;
-import org.apache.lucene.index.Terms;
-import org.apache.lucene.index.TermsEnum;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.search.similarities.Similarity;
 import org.apache.lucene.util.Bits;
-import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.index.FieldInvertState;
 import org.apache.lucene.util.PriorityQueue;
 
@@ -47,7 +44,7 @@ final class JustCompileSearch {
     }
 
     @Override
-    protected void doSetNextReader(AtomicReaderContext context) throws IOException {
+    protected void doSetNextReader(LeafReaderContext context) throws IOException {
       throw new UnsupportedOperationException(UNSUPPORTED_MSG);
     }
 
@@ -127,7 +124,7 @@ final class JustCompileSearch {
     }
 
     @Override
-    public FieldComparator<Object> setNextReader(AtomicReaderContext context) {
+    public FieldComparator<Object> setNextReader(LeafReaderContext context) {
       throw new UnsupportedOperationException(UNSUPPORTED_MSG);
     }
 
@@ -157,7 +154,7 @@ final class JustCompileSearch {
     // still added here in case someone will add abstract methods in the future.
     
     @Override
-    public DocIdSet getDocIdSet(AtomicReaderContext context, Bits acceptDocs) {
+    public DocIdSet getDocIdSet(LeafReaderContext context, Bits acceptDocs) {
       return null;
     }
   }
@@ -246,7 +243,7 @@ final class JustCompileSearch {
     }
 
     @Override
-    public SimScorer simScorer(SimWeight stats, AtomicReaderContext context) {
+    public SimScorer simScorer(SimWeight stats, LeafReaderContext context) {
       throw new UnsupportedOperationException(UNSUPPORTED_MSG);
     }
 
@@ -268,7 +265,7 @@ final class JustCompileSearch {
     }
 
     @Override
-    protected void doSetNextReader(AtomicReaderContext context) throws IOException {
+    protected void doSetNextReader(LeafReaderContext context) throws IOException {
       throw new UnsupportedOperationException(UNSUPPORTED_MSG);
     }
 
@@ -302,7 +299,7 @@ final class JustCompileSearch {
   static final class JustCompileWeight extends Weight {
 
     @Override
-    public Explanation explain(AtomicReaderContext context, int doc) {
+    public Explanation explain(LeafReaderContext context, int doc) {
       throw new UnsupportedOperationException(UNSUPPORTED_MSG);
     }
 
@@ -322,7 +319,7 @@ final class JustCompileSearch {
     }
 
     @Override
-    public Scorer scorer(AtomicReaderContext context, Bits acceptDocs) {
+    public Scorer scorer(LeafReaderContext context, Bits acceptDocs) {
       throw new UnsupportedOperationException(UNSUPPORTED_MSG);
     }
     
diff --git lucene/core/src/test/org/apache/lucene/search/MockFilter.java lucene/core/src/test/org/apache/lucene/search/MockFilter.java
index 56b6dce..a6e2f35 100644
--- lucene/core/src/test/org/apache/lucene/search/MockFilter.java
+++ lucene/core/src/test/org/apache/lucene/search/MockFilter.java
@@ -17,8 +17,7 @@ package org.apache.lucene.search;
  * limitations under the License.
  */
 
-import org.apache.lucene.index.AtomicReaderContext;
-import org.apache.lucene.util.DocIdBitSet;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.util.FixedBitSet;
 import org.apache.lucene.util.Bits;
 
@@ -26,7 +25,7 @@ public class MockFilter extends Filter {
   private boolean wasCalled;
 
   @Override
-  public DocIdSet getDocIdSet(AtomicReaderContext context, Bits acceptDocs) {
+  public DocIdSet getDocIdSet(LeafReaderContext context, Bits acceptDocs) {
     wasCalled = true;
     return new FixedBitSet(context.reader().maxDoc());
   }
diff --git lucene/core/src/test/org/apache/lucene/search/MultiCollectorTest.java lucene/core/src/test/org/apache/lucene/search/MultiCollectorTest.java
index 5a7df3c..d4f0d74 100644
--- lucene/core/src/test/org/apache/lucene/search/MultiCollectorTest.java
+++ lucene/core/src/test/org/apache/lucene/search/MultiCollectorTest.java
@@ -19,9 +19,7 @@ package org.apache.lucene.search;
 
 import java.io.IOException;
 
-import org.apache.lucene.index.AtomicReaderContext;
-import org.apache.lucene.search.Collector;
-import org.apache.lucene.search.Scorer;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.util.LuceneTestCase;
 import org.junit.Test;
 
@@ -46,7 +44,7 @@ public class MultiCollectorTest extends LuceneTestCase {
     }
 
     @Override
-    protected void doSetNextReader(AtomicReaderContext context) throws IOException {
+    protected void doSetNextReader(LeafReaderContext context) throws IOException {
       setNextReaderCalled = true;
     }
 
diff --git lucene/core/src/test/org/apache/lucene/search/SingleDocTestFilter.java lucene/core/src/test/org/apache/lucene/search/SingleDocTestFilter.java
index df4fd07..90bc6ff 100644
--- lucene/core/src/test/org/apache/lucene/search/SingleDocTestFilter.java
+++ lucene/core/src/test/org/apache/lucene/search/SingleDocTestFilter.java
@@ -17,7 +17,7 @@ package org.apache.lucene.search;
  * limitations under the License.
  */
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.util.Bits;
 import org.apache.lucene.util.FixedBitSet;
 
@@ -31,7 +31,7 @@ public class SingleDocTestFilter extends Filter {
   }
 
   @Override
-  public DocIdSet getDocIdSet(AtomicReaderContext context, Bits acceptDocs) throws IOException {
+  public DocIdSet getDocIdSet(LeafReaderContext context, Bits acceptDocs) throws IOException {
     FixedBitSet bits = new FixedBitSet(context.reader().maxDoc());
     bits.set(doc);
     if (acceptDocs != null && !acceptDocs.get(doc)) bits.clear(doc);
diff --git lucene/core/src/test/org/apache/lucene/search/TestBooleanCoord.java lucene/core/src/test/org/apache/lucene/search/TestBooleanCoord.java
index 181b64b..5a9dbdd 100644
--- lucene/core/src/test/org/apache/lucene/search/TestBooleanCoord.java
+++ lucene/core/src/test/org/apache/lucene/search/TestBooleanCoord.java
@@ -23,7 +23,7 @@ import java.util.concurrent.atomic.AtomicBoolean;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.StringField;
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.FieldInvertState;
 import org.apache.lucene.index.IndexWriter;
@@ -95,7 +95,7 @@ public class TestBooleanCoord extends LuceneTestCase {
       }
 
       @Override
-      public SimScorer simScorer(SimWeight weight, AtomicReaderContext context) throws IOException {
+      public SimScorer simScorer(SimWeight weight, LeafReaderContext context) throws IOException {
         return new SimScorer() {
           @Override
           public float score(int doc, float freq) {
diff --git lucene/core/src/test/org/apache/lucene/search/TestBooleanQuery.java lucene/core/src/test/org/apache/lucene/search/TestBooleanQuery.java
index d8e05fb..2add6cd 100644
--- lucene/core/src/test/org/apache/lucene/search/TestBooleanQuery.java
+++ lucene/core/src/test/org/apache/lucene/search/TestBooleanQuery.java
@@ -30,7 +30,7 @@ import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.TextField;
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexWriter;
@@ -337,7 +337,7 @@ public class TestBooleanQuery extends LuceneTestCase {
     w.close();
     IndexSearcher s = new IndexSearcher(r) {
         @Override
-        protected void search(List<AtomicReaderContext> leaves, Weight weight, Collector collector) throws IOException {
+        protected void search(List<LeafReaderContext> leaves, Weight weight, Collector collector) throws IOException {
           assertEquals(-1, collector.getClass().getSimpleName().indexOf("OutOfOrder"));
           super.search(leaves, weight, collector);
         }
diff --git lucene/core/src/test/org/apache/lucene/search/TestBooleanQueryVisitSubscorers.java lucene/core/src/test/org/apache/lucene/search/TestBooleanQueryVisitSubscorers.java
index 15e42fe..9f8568c 100644
--- lucene/core/src/test/org/apache/lucene/search/TestBooleanQueryVisitSubscorers.java
+++ lucene/core/src/test/org/apache/lucene/search/TestBooleanQueryVisitSubscorers.java
@@ -30,7 +30,7 @@ import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field.Store;
 import org.apache.lucene.document.TextField;
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexWriterConfig;
 import org.apache.lucene.index.RandomIndexWriter;
@@ -133,7 +133,7 @@ public class TestBooleanQueryVisitSubscorers extends LuceneTestCase {
       super(TopScoreDocCollector.create(10, true));
     }
 
-    public LeafCollector getLeafCollector(AtomicReaderContext context)
+    public LeafCollector getLeafCollector(LeafReaderContext context)
         throws IOException {
       final int docBase = context.docBase;
       return new FilterLeafCollector(super.getLeafCollector(context)) {
@@ -232,7 +232,7 @@ public class TestBooleanQueryVisitSubscorers extends LuceneTestCase {
     }
 
     @Override
-    public LeafCollector getLeafCollector(AtomicReaderContext context) throws IOException {
+    public LeafCollector getLeafCollector(LeafReaderContext context) throws IOException {
       return new LeafCollector() {
 
         @Override
diff --git lucene/core/src/test/org/apache/lucene/search/TestBooleanScorer.java lucene/core/src/test/org/apache/lucene/search/TestBooleanScorer.java
index f1ed5bc..a4ffa0d 100644
--- lucene/core/src/test/org/apache/lucene/search/TestBooleanScorer.java
+++ lucene/core/src/test/org/apache/lucene/search/TestBooleanScorer.java
@@ -26,7 +26,7 @@ import java.util.List;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.TextField;
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.RandomIndexWriter;
 import org.apache.lucene.index.Term;
@@ -111,7 +111,7 @@ public class TestBooleanScorer extends LuceneTestCase {
       }
       
       @Override
-      protected void doSetNextReader(AtomicReaderContext context) throws IOException {
+      protected void doSetNextReader(LeafReaderContext context) throws IOException {
         docBase = context.docBase;
       }
       
@@ -188,7 +188,7 @@ public class TestBooleanScorer extends LuceneTestCase {
     public Weight createWeight(IndexSearcher searcher) throws IOException {
       return new Weight() {
         @Override
-        public Explanation explain(AtomicReaderContext context, int doc) {
+        public Explanation explain(LeafReaderContext context, int doc) {
           throw new UnsupportedOperationException();
         }
 
@@ -207,12 +207,12 @@ public class TestBooleanScorer extends LuceneTestCase {
         }
 
         @Override
-        public Scorer scorer(AtomicReaderContext context, Bits acceptDocs) {
+        public Scorer scorer(LeafReaderContext context, Bits acceptDocs) {
           throw new UnsupportedOperationException();
         }
 
         @Override
-        public BulkScorer bulkScorer(AtomicReaderContext context, boolean scoreDocsInOrder, Bits acceptDocs) {
+        public BulkScorer bulkScorer(LeafReaderContext context, boolean scoreDocsInOrder, Bits acceptDocs) {
           return new BulkScorer() {
 
             @Override
diff --git lucene/core/src/test/org/apache/lucene/search/TestCachingWrapperFilter.java lucene/core/src/test/org/apache/lucene/search/TestCachingWrapperFilter.java
index 4955ece..2f2b650 100644
--- lucene/core/src/test/org/apache/lucene/search/TestCachingWrapperFilter.java
+++ lucene/core/src/test/org/apache/lucene/search/TestCachingWrapperFilter.java
@@ -23,7 +23,7 @@ import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.StringField;
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.RandomIndexWriter;
@@ -147,7 +147,7 @@ public class TestCachingWrapperFilter extends LuceneTestCase {
     writer.close();
 
     IndexReader reader = SlowCompositeReaderWrapper.wrap(DirectoryReader.open(dir));
-    AtomicReaderContext context = (AtomicReaderContext) reader.getContext();
+    LeafReaderContext context = (LeafReaderContext) reader.getContext();
     MockFilter filter = new MockFilter();
     CachingWrapperFilter cacher = new CachingWrapperFilter(filter);
 
@@ -173,11 +173,11 @@ public class TestCachingWrapperFilter extends LuceneTestCase {
     writer.close();
 
     IndexReader reader = SlowCompositeReaderWrapper.wrap(DirectoryReader.open(dir));
-    AtomicReaderContext context = (AtomicReaderContext) reader.getContext();
+    LeafReaderContext context = (LeafReaderContext) reader.getContext();
 
     final Filter filter = new Filter() {
       @Override
-      public DocIdSet getDocIdSet(AtomicReaderContext context, Bits acceptDocs) {
+      public DocIdSet getDocIdSet(LeafReaderContext context, Bits acceptDocs) {
         return null;
       }
     };
@@ -196,11 +196,11 @@ public class TestCachingWrapperFilter extends LuceneTestCase {
     writer.close();
 
     IndexReader reader = SlowCompositeReaderWrapper.wrap(DirectoryReader.open(dir));
-    AtomicReaderContext context = (AtomicReaderContext) reader.getContext();
+    LeafReaderContext context = (LeafReaderContext) reader.getContext();
 
     final Filter filter = new Filter() {
       @Override
-      public DocIdSet getDocIdSet(AtomicReaderContext context, Bits acceptDocs) {
+      public DocIdSet getDocIdSet(LeafReaderContext context, Bits acceptDocs) {
         return new DocIdSet() {
           @Override
           public DocIdSetIterator iterator() {
@@ -224,8 +224,8 @@ public class TestCachingWrapperFilter extends LuceneTestCase {
   }
   
   private static void assertDocIdSetCacheable(IndexReader reader, Filter filter, boolean shouldCacheable) throws IOException {
-    assertTrue(reader.getContext() instanceof AtomicReaderContext);
-    AtomicReaderContext context = (AtomicReaderContext) reader.getContext();
+    assertTrue(reader.getContext() instanceof LeafReaderContext);
+    LeafReaderContext context = (LeafReaderContext) reader.getContext();
     final CachingWrapperFilter cacher = new CachingWrapperFilter(filter);
     final DocIdSet originalSet = filter.getDocIdSet(context, context.reader().getLiveDocs());
     final DocIdSet cachedSet = cacher.getDocIdSet(context, context.reader().getLiveDocs());
@@ -263,7 +263,7 @@ public class TestCachingWrapperFilter extends LuceneTestCase {
     // a fixedbitset filter is always cacheable
     assertDocIdSetCacheable(reader, new Filter() {
       @Override
-      public DocIdSet getDocIdSet(AtomicReaderContext context, Bits acceptDocs) {
+      public DocIdSet getDocIdSet(LeafReaderContext context, Bits acceptDocs) {
         return new FixedBitSet(context.reader().maxDoc());
       }
     }, true);
diff --git lucene/core/src/test/org/apache/lucene/search/TestConjunctions.java lucene/core/src/test/org/apache/lucene/search/TestConjunctions.java
index 5c57bef..fdde65b 100644
--- lucene/core/src/test/org/apache/lucene/search/TestConjunctions.java
+++ lucene/core/src/test/org/apache/lucene/search/TestConjunctions.java
@@ -25,7 +25,7 @@ import org.apache.lucene.document.Document;
 import org.apache.lucene.document.StringField;
 import org.apache.lucene.document.TextField;
 import org.apache.lucene.document.Field.Store;
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.FieldInvertState;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexWriterConfig;
@@ -109,7 +109,7 @@ public class TestConjunctions extends LuceneTestCase {
     }
 
     @Override
-    public SimScorer simScorer(SimWeight weight, AtomicReaderContext context) throws IOException {
+    public SimScorer simScorer(SimWeight weight, LeafReaderContext context) throws IOException {
       return new SimScorer() {
         @Override
         public float score(int doc, float freq) {
diff --git lucene/core/src/test/org/apache/lucene/search/TestDisjunctionMaxQuery.java lucene/core/src/test/org/apache/lucene/search/TestDisjunctionMaxQuery.java
index 7c7aaa6..2817bf5 100644
--- lucene/core/src/test/org/apache/lucene/search/TestDisjunctionMaxQuery.java
+++ lucene/core/src/test/org/apache/lucene/search/TestDisjunctionMaxQuery.java
@@ -18,13 +18,13 @@ package org.apache.lucene.search;
  */
 
 import org.apache.lucene.document.Field;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.util.LuceneTestCase;
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.FieldType;
 import org.apache.lucene.document.TextField;
-import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexWriter;
@@ -177,9 +177,9 @@ public class TestDisjunctionMaxQuery extends LuceneTestCase {
     dq.add(tq("dek", "DOES_NOT_EXIST"));
     
     QueryUtils.check(random(), dq, s);
-    assertTrue(s.getTopReaderContext() instanceof AtomicReaderContext);
+    assertTrue(s.getTopReaderContext() instanceof LeafReaderContext);
     final Weight dw = s.createNormalizedWeight(dq);
-    AtomicReaderContext context = (AtomicReaderContext)s.getTopReaderContext();
+    LeafReaderContext context = (LeafReaderContext)s.getTopReaderContext();
     final Scorer ds = dw.scorer(context, context.reader().getLiveDocs());
     final boolean skipOk = ds.advance(3) != DocIdSetIterator.NO_MORE_DOCS;
     if (skipOk) {
@@ -192,10 +192,10 @@ public class TestDisjunctionMaxQuery extends LuceneTestCase {
     final DisjunctionMaxQuery dq = new DisjunctionMaxQuery(0.0f);
     dq.add(tq("dek", "albino"));
     dq.add(tq("dek", "DOES_NOT_EXIST"));
-    assertTrue(s.getTopReaderContext() instanceof AtomicReaderContext);
+    assertTrue(s.getTopReaderContext() instanceof LeafReaderContext);
     QueryUtils.check(random(), dq, s);
     final Weight dw = s.createNormalizedWeight(dq);
-    AtomicReaderContext context = (AtomicReaderContext)s.getTopReaderContext();
+    LeafReaderContext context = (LeafReaderContext)s.getTopReaderContext();
     final Scorer ds = dw.scorer(context, context.reader().getLiveDocs());
     assertTrue("firsttime skipTo found no match",
         ds.advance(3) != DocIdSetIterator.NO_MORE_DOCS);
diff --git lucene/core/src/test/org/apache/lucene/search/TestDocBoost.java lucene/core/src/test/org/apache/lucene/search/TestDocBoost.java
index 33d6983..13f9268 100644
--- lucene/core/src/test/org/apache/lucene/search/TestDocBoost.java
+++ lucene/core/src/test/org/apache/lucene/search/TestDocBoost.java
@@ -21,7 +21,7 @@ import java.io.IOException;
 
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.document.*;
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.RandomIndexWriter;
 import org.apache.lucene.index.Term;
@@ -71,7 +71,7 @@ public class TestDocBoost extends LuceneTestCase {
            scores[doc + base] = scorer.score();
          }
          @Override
-         protected void doSetNextReader(AtomicReaderContext context) throws IOException {
+         protected void doSetNextReader(LeafReaderContext context) throws IOException {
            base = context.docBase;
          }
          @Override
diff --git lucene/core/src/test/org/apache/lucene/search/TestDocIdSet.java lucene/core/src/test/org/apache/lucene/search/TestDocIdSet.java
index 59b7f8d..35c80f3 100644
--- lucene/core/src/test/org/apache/lucene/search/TestDocIdSet.java
+++ lucene/core/src/test/org/apache/lucene/search/TestDocIdSet.java
@@ -26,7 +26,7 @@ import junit.framework.Assert;
 
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.RandomIndexWriter;
 import org.apache.lucene.store.Directory;
@@ -124,7 +124,7 @@ public class TestDocIdSet extends LuceneTestCase {
     // Now search w/ a Filter which returns a null DocIdSet
     Filter f = new Filter() {
       @Override
-      public DocIdSet getDocIdSet(AtomicReaderContext context, Bits acceptDocs) {
+      public DocIdSet getDocIdSet(LeafReaderContext context, Bits acceptDocs) {
         return null;
       }
     };
@@ -150,7 +150,7 @@ public class TestDocIdSet extends LuceneTestCase {
       // Now search w/ a Filter which returns a null DocIdSet
     Filter f = new Filter() {
       @Override
-      public DocIdSet getDocIdSet(AtomicReaderContext context, Bits acceptDocs) {
+      public DocIdSet getDocIdSet(LeafReaderContext context, Bits acceptDocs) {
         final DocIdSet innerNullIteratorSet = new DocIdSet() {
           @Override
           public DocIdSetIterator iterator() {
diff --git lucene/core/src/test/org/apache/lucene/search/TestDocValuesScoring.java lucene/core/src/test/org/apache/lucene/search/TestDocValuesScoring.java
index 436d3d0..89821c5 100644
--- lucene/core/src/test/org/apache/lucene/search/TestDocValuesScoring.java
+++ lucene/core/src/test/org/apache/lucene/search/TestDocValuesScoring.java
@@ -22,7 +22,7 @@ import java.io.IOException;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.FloatDocValuesField;
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.DocValues;
 import org.apache.lucene.index.FieldInvertState;
 import org.apache.lucene.index.IndexReader;
@@ -158,7 +158,7 @@ public class TestDocValuesScoring extends LuceneTestCase {
     }
 
     @Override
-    public SimScorer simScorer(SimWeight stats, AtomicReaderContext context) throws IOException {
+    public SimScorer simScorer(SimWeight stats, LeafReaderContext context) throws IOException {
       final SimScorer sub = sim.simScorer(stats, context);
       final NumericDocValues values = DocValues.getNumeric(context.reader(), boostField);
       
diff --git lucene/core/src/test/org/apache/lucene/search/TestEarlyTermination.java lucene/core/src/test/org/apache/lucene/search/TestEarlyTermination.java
index 7388d00..33e91dd 100644
--- lucene/core/src/test/org/apache/lucene/search/TestEarlyTermination.java
+++ lucene/core/src/test/org/apache/lucene/search/TestEarlyTermination.java
@@ -20,7 +20,7 @@ package org.apache.lucene.search;
 import java.io.IOException;
 
 import org.apache.lucene.document.Document;
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.RandomIndexWriter;
 import org.apache.lucene.store.Directory;
@@ -73,7 +73,7 @@ public class TestEarlyTermination extends LuceneTestCase {
         }
 
         @Override
-        protected void doSetNextReader(AtomicReaderContext context) throws IOException {
+        protected void doSetNextReader(LeafReaderContext context) throws IOException {
           if (random().nextBoolean()) {
             collectionTerminated = true;
             throw new CollectionTerminatedException();
diff --git lucene/core/src/test/org/apache/lucene/search/TestElevationComparator.java lucene/core/src/test/org/apache/lucene/search/TestElevationComparator.java
index d17a7a9..8a397b6 100644
--- lucene/core/src/test/org/apache/lucene/search/TestElevationComparator.java
+++ lucene/core/src/test/org/apache/lucene/search/TestElevationComparator.java
@@ -187,7 +187,7 @@ class ElevationComparatorSource extends FieldComparatorSource {
      }
 
      @Override
-     public FieldComparator<Integer> setNextReader(AtomicReaderContext context) throws IOException {
+     public FieldComparator<Integer> setNextReader(LeafReaderContext context) throws IOException {
        idIndex = DocValues.getSorted(context.reader(), fieldname);
        return this;
      }
diff --git lucene/core/src/test/org/apache/lucene/search/TestFilteredQuery.java lucene/core/src/test/org/apache/lucene/search/TestFilteredQuery.java
index 9b314f0..69b31e9 100644
--- lucene/core/src/test/org/apache/lucene/search/TestFilteredQuery.java
+++ lucene/core/src/test/org/apache/lucene/search/TestFilteredQuery.java
@@ -24,8 +24,8 @@ import java.util.Random;
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
-import org.apache.lucene.index.AtomicReader;
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReader;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.DocsEnum;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.RandomIndexWriter;
@@ -99,7 +99,7 @@ public class TestFilteredQuery extends LuceneTestCase {
   private static Filter newStaticFilterB() {
     return new Filter() {
       @Override
-      public DocIdSet getDocIdSet (AtomicReaderContext context, Bits acceptDocs) {
+      public DocIdSet getDocIdSet (LeafReaderContext context, Bits acceptDocs) {
         if (acceptDocs == null) acceptDocs = new Bits.MatchAllBits(5);
         BitSet bitset = new BitSet(5);
         if (acceptDocs.get(1)) bitset.set(1);
@@ -181,7 +181,7 @@ public class TestFilteredQuery extends LuceneTestCase {
   private static Filter newStaticFilterA() {
     return new Filter() {
       @Override
-      public DocIdSet getDocIdSet (AtomicReaderContext context, Bits acceptDocs) {
+      public DocIdSet getDocIdSet (LeafReaderContext context, Bits acceptDocs) {
         assertNull("acceptDocs should be null, as we have an index without deletions", acceptDocs);
         BitSet bitset = new BitSet(5);
         bitset.set(0, 5);
@@ -421,10 +421,10 @@ public class TestFilteredQuery extends LuceneTestCase {
     Query query = new FilteredQuery(new TermQuery(new Term("field", "0")),
         new Filter() {
           @Override
-          public DocIdSet getDocIdSet(AtomicReaderContext context,
+          public DocIdSet getDocIdSet(LeafReaderContext context,
               Bits acceptDocs) throws IOException {
             final boolean nullBitset = random().nextInt(10) == 5;
-            final AtomicReader reader = context.reader();
+            final LeafReader reader = context.reader();
             DocsEnum termDocsEnum = reader.termDocsEnum(new Term("field", "0"));
             if (termDocsEnum == null) {
               return null; // no docs -- return null
@@ -504,7 +504,7 @@ public class TestFilteredQuery extends LuceneTestCase {
     IndexSearcher searcher = newSearcher(reader);
     Query query = new FilteredQuery(new TermQuery(new Term("field", "0")), new Filter() {
       @Override
-      public DocIdSet getDocIdSet(final AtomicReaderContext context, Bits acceptDocs)
+      public DocIdSet getDocIdSet(final LeafReaderContext context, Bits acceptDocs)
           throws IOException {
         return new DocIdSet() {
 
diff --git lucene/core/src/test/org/apache/lucene/search/TestFilteredSearch.java lucene/core/src/test/org/apache/lucene/search/TestFilteredSearch.java
index 4e4554a..a421495 100644
--- lucene/core/src/test/org/apache/lucene/search/TestFilteredSearch.java
+++ lucene/core/src/test/org/apache/lucene/search/TestFilteredSearch.java
@@ -20,10 +20,11 @@ package org.apache.lucene.search;
 import java.io.IOException;
 
 import org.apache.lucene.document.Field;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.util.LuceneTestCase;
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.document.Document;
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexWriter;
@@ -87,7 +88,7 @@ public class TestFilteredSearch extends LuceneTestCase {
     }
 
     @Override
-    public DocIdSet getDocIdSet(AtomicReaderContext context, Bits acceptDocs) {
+    public DocIdSet getDocIdSet(LeafReaderContext context, Bits acceptDocs) {
       assertNull("acceptDocs should be null, as we have an index without deletions", acceptDocs);
       final FixedBitSet set = new FixedBitSet(context.reader().maxDoc());
       int docBase = context.docBase;
diff --git lucene/core/src/test/org/apache/lucene/search/TestMinShouldMatch2.java lucene/core/src/test/org/apache/lucene/search/TestMinShouldMatch2.java
index d34f495..2216ed5 100644
--- lucene/core/src/test/org/apache/lucene/search/TestMinShouldMatch2.java
+++ lucene/core/src/test/org/apache/lucene/search/TestMinShouldMatch2.java
@@ -29,7 +29,7 @@ import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.SortedSetDocValuesField;
 import org.apache.lucene.document.StringField;
-import org.apache.lucene.index.AtomicReader;
+import org.apache.lucene.index.LeafReader;
 import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.RandomIndexWriter;
 import org.apache.lucene.index.SortedSetDocValues;
@@ -50,7 +50,7 @@ import org.junit.BeforeClass;
 public class TestMinShouldMatch2 extends LuceneTestCase {
   static Directory dir;
   static DirectoryReader r;
-  static AtomicReader reader;
+  static LeafReader reader;
   static IndexSearcher searcher;
   
   static final String alwaysTerms[] = { "a" };
@@ -277,7 +277,7 @@ public class TestMinShouldMatch2 extends LuceneTestCase {
     
     double score = Float.NaN;
 
-    SlowMinShouldMatchScorer(BooleanWeight weight, AtomicReader reader, IndexSearcher searcher) throws IOException {
+    SlowMinShouldMatchScorer(BooleanWeight weight, LeafReader reader, IndexSearcher searcher) throws IOException {
       super(weight);
       this.dv = reader.getSortedSetDocValues("dv");
       this.maxDoc = reader.maxDoc();
diff --git lucene/core/src/test/org/apache/lucene/search/TestMultiTermConstantScore.java lucene/core/src/test/org/apache/lucene/search/TestMultiTermConstantScore.java
index 3b3786c..785ac97 100644
--- lucene/core/src/test/org/apache/lucene/search/TestMultiTermConstantScore.java
+++ lucene/core/src/test/org/apache/lucene/search/TestMultiTermConstantScore.java
@@ -23,7 +23,7 @@ import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.FieldType;
 import org.apache.lucene.document.TextField;
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.RandomIndexWriter;
 import org.apache.lucene.index.Term;
@@ -238,7 +238,7 @@ public class TestMultiTermConstantScore extends BaseTestRangeFilter {
         assertEquals("score for doc " + (doc + base) + " was not correct", 1.0f, scorer.score(), SCORE_COMP_THRESH);
       }
       @Override
-      protected void doSetNextReader(AtomicReaderContext context) throws IOException {
+      protected void doSetNextReader(LeafReaderContext context) throws IOException {
         base = context.docBase;
       }
       @Override
diff --git lucene/core/src/test/org/apache/lucene/search/TestNumericRangeQuery32.java lucene/core/src/test/org/apache/lucene/search/TestNumericRangeQuery32.java
index 55ca52a..0782e42 100644
--- lucene/core/src/test/org/apache/lucene/search/TestNumericRangeQuery32.java
+++ lucene/core/src/test/org/apache/lucene/search/TestNumericRangeQuery32.java
@@ -23,7 +23,7 @@ import org.apache.lucene.document.Field;
 import org.apache.lucene.document.FieldType;
 import org.apache.lucene.document.FloatField;
 import org.apache.lucene.document.IntField;
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.MultiFields;
@@ -199,7 +199,7 @@ public class TestNumericRangeQuery32 extends LuceneTestCase {
   
   @Test
   public void testInverseRange() throws Exception {
-    AtomicReaderContext context = SlowCompositeReaderWrapper.wrap(reader).getContext();
+    LeafReaderContext context = SlowCompositeReaderWrapper.wrap(reader).getContext();
     NumericRangeFilter<Integer> f = NumericRangeFilter.newIntRange("field8", 8, 1000, -1000, true, true);
     assertNull("A inverse range should return the null instance", f.getDocIdSet(context, context.reader().getLiveDocs()));
     f = NumericRangeFilter.newIntRange("field8", 8, Integer.MAX_VALUE, null, false, false);
diff --git lucene/core/src/test/org/apache/lucene/search/TestNumericRangeQuery64.java lucene/core/src/test/org/apache/lucene/search/TestNumericRangeQuery64.java
index c125afc..42dd518 100644
--- lucene/core/src/test/org/apache/lucene/search/TestNumericRangeQuery64.java
+++ lucene/core/src/test/org/apache/lucene/search/TestNumericRangeQuery64.java
@@ -23,7 +23,7 @@ import org.apache.lucene.document.DoubleField;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.FieldType;
 import org.apache.lucene.document.LongField;
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.MultiFields;
@@ -213,7 +213,7 @@ public class TestNumericRangeQuery64 extends LuceneTestCase {
   
   @Test
   public void testInverseRange() throws Exception {
-    AtomicReaderContext context = SlowCompositeReaderWrapper.wrap(searcher.getIndexReader()).getContext();
+    LeafReaderContext context = SlowCompositeReaderWrapper.wrap(searcher.getIndexReader()).getContext();
     NumericRangeFilter<Long> f = NumericRangeFilter.newLongRange("field8", 8, 1000L, -1000L, true, true);
     assertNull("A inverse range should return the null instance", 
         f.getDocIdSet(context, context.reader().getLiveDocs()));
diff --git lucene/core/src/test/org/apache/lucene/search/TestPositionIncrement.java lucene/core/src/test/org/apache/lucene/search/TestPositionIncrement.java
index bd512e8..6086ff6 100644
--- lucene/core/src/test/org/apache/lucene/search/TestPositionIncrement.java
+++ lucene/core/src/test/org/apache/lucene/search/TestPositionIncrement.java
@@ -29,7 +29,7 @@ import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.TextField;
-import org.apache.lucene.index.AtomicReader;
+import org.apache.lucene.index.LeafReader;
 import org.apache.lucene.index.MultiFields;
 import org.apache.lucene.index.DocsAndPositionsEnum;
 import org.apache.lucene.index.IndexReader;
@@ -210,7 +210,7 @@ public class TestPositionIncrement extends LuceneTestCase {
     writer.addDocument(doc);
 
     final IndexReader readerFromWriter = writer.getReader();
-    AtomicReader r = SlowCompositeReaderWrapper.wrap(readerFromWriter);
+    LeafReader r = SlowCompositeReaderWrapper.wrap(readerFromWriter);
 
     DocsAndPositionsEnum tp = r.termPositionsEnum(new Term("content", "a"));
     
diff --git lucene/core/src/test/org/apache/lucene/search/TestQueryRescorer.java lucene/core/src/test/org/apache/lucene/search/TestQueryRescorer.java
index 63b3075..20e337c 100644
--- lucene/core/src/test/org/apache/lucene/search/TestQueryRescorer.java
+++ lucene/core/src/test/org/apache/lucene/search/TestQueryRescorer.java
@@ -25,7 +25,7 @@ import java.util.Set;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.NumericDocValuesField;
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.RandomIndexWriter;
 import org.apache.lucene.index.Term;
@@ -443,7 +443,7 @@ public class TestQueryRescorer extends LuceneTestCase {
         }
 
         @Override
-        public Scorer scorer(final AtomicReaderContext context, Bits acceptDocs) throws IOException {
+        public Scorer scorer(final LeafReaderContext context, Bits acceptDocs) throws IOException {
 
           return new Scorer(null) {
             int docID = -1;
@@ -493,7 +493,7 @@ public class TestQueryRescorer extends LuceneTestCase {
         }
 
         @Override
-        public Explanation explain(AtomicReaderContext context, int doc) throws IOException {
+        public Explanation explain(LeafReaderContext context, int doc) throws IOException {
           return null;
         }
       };
diff --git lucene/core/src/test/org/apache/lucene/search/TestScorerPerf.java lucene/core/src/test/org/apache/lucene/search/TestScorerPerf.java
index 8629dc1..b5a5a94 100644
--- lucene/core/src/test/org/apache/lucene/search/TestScorerPerf.java
+++ lucene/core/src/test/org/apache/lucene/search/TestScorerPerf.java
@@ -1,6 +1,7 @@
 package org.apache.lucene.search;
 
 import org.apache.lucene.document.Field;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.util.Bits;
 import org.apache.lucene.util.DocIdBitSet;
 import org.apache.lucene.util.LuceneTestCase;
@@ -8,7 +9,6 @@ import org.apache.lucene.util.LuceneTestCase;
 import java.util.BitSet;
 import java.io.IOException;
 
-import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexWriter;
@@ -112,7 +112,7 @@ public class TestScorerPerf extends LuceneTestCase {
     public int getSum() { return sum; }
 
     @Override
-    protected void doSetNextReader(AtomicReaderContext context) throws IOException {
+    protected void doSetNextReader(LeafReaderContext context) throws IOException {
       docBase = context.docBase;
     }
     @Override
@@ -144,7 +144,7 @@ public class TestScorerPerf extends LuceneTestCase {
     final BitSet rnd = sets[random().nextInt(sets.length)];
     Query q = new ConstantScoreQuery(new Filter() {
       @Override
-      public DocIdSet getDocIdSet (AtomicReaderContext context, Bits acceptDocs) {
+      public DocIdSet getDocIdSet (LeafReaderContext context, Bits acceptDocs) {
         assertNull("acceptDocs should be null, as we have an index without deletions", acceptDocs);
         return new DocIdBitSet(rnd);
       }
diff --git lucene/core/src/test/org/apache/lucene/search/TestSimilarity.java lucene/core/src/test/org/apache/lucene/search/TestSimilarity.java
index a884585..e48fa5f 100644
--- lucene/core/src/test/org/apache/lucene/search/TestSimilarity.java
+++ lucene/core/src/test/org/apache/lucene/search/TestSimilarity.java
@@ -18,11 +18,11 @@ package org.apache.lucene.search;
  */
 
 import org.apache.lucene.document.Field;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.util.LuceneTestCase;
 
 import java.io.IOException;
 
-import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.FieldInvertState;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.RandomIndexWriter;
@@ -108,7 +108,7 @@ public class TestSimilarity extends LuceneTestCase {
            assertEquals((float)doc+base+1, scorer.score(), 0);
          }
          @Override
-         protected void doSetNextReader(AtomicReaderContext context) throws IOException {
+         protected void doSetNextReader(LeafReaderContext context) throws IOException {
            base = context.docBase;
          }
          @Override
diff --git lucene/core/src/test/org/apache/lucene/search/TestSimilarityProvider.java lucene/core/src/test/org/apache/lucene/search/TestSimilarityProvider.java
index f30f017..81da556 100644
--- lucene/core/src/test/org/apache/lucene/search/TestSimilarityProvider.java
+++ lucene/core/src/test/org/apache/lucene/search/TestSimilarityProvider.java
@@ -20,7 +20,7 @@ package org.apache.lucene.search;
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
-import org.apache.lucene.index.AtomicReader;
+import org.apache.lucene.index.LeafReader;
 import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.FieldInvertState;
 import org.apache.lucene.index.IndexWriterConfig;
@@ -75,7 +75,7 @@ public class TestSimilarityProvider extends LuceneTestCase {
   public void testBasics() throws Exception {
     // sanity check of norms writer
     // TODO: generalize
-    AtomicReader slow = SlowCompositeReaderWrapper.wrap(reader);
+    LeafReader slow = SlowCompositeReaderWrapper.wrap(reader);
     NumericDocValues fooNorms = slow.getNormValues("foo");
     NumericDocValues barNorms = slow.getNormValues("bar");
     for (int i = 0; i < slow.maxDoc(); i++) {
diff --git lucene/core/src/test/org/apache/lucene/search/TestSortRandom.java lucene/core/src/test/org/apache/lucene/search/TestSortRandom.java
index eb868bd..9df13d6 100644
--- lucene/core/src/test/org/apache/lucene/search/TestSortRandom.java
+++ lucene/core/src/test/org/apache/lucene/search/TestSortRandom.java
@@ -30,7 +30,7 @@ import org.apache.lucene.document.Document;
 import org.apache.lucene.document.NumericDocValuesField;
 import org.apache.lucene.document.SortedDocValuesField;
 import org.apache.lucene.document.StoredField;
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.DocValues;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.NumericDocValues;
@@ -247,7 +247,7 @@ public class TestSortRandom extends LuceneTestCase {
     }
 
     @Override
-    public DocIdSet getDocIdSet(AtomicReaderContext context, Bits acceptDocs) throws IOException {
+    public DocIdSet getDocIdSet(LeafReaderContext context, Bits acceptDocs) throws IOException {
       final int maxDoc = context.reader().maxDoc();
       final NumericDocValues idSource = DocValues.getNumeric(context.reader(), "id");
       assertNotNull(idSource);
diff --git lucene/core/src/test/org/apache/lucene/search/TestSubScorerFreqs.java lucene/core/src/test/org/apache/lucene/search/TestSubScorerFreqs.java
index 8ddc53d..82b34e4 100644
--- lucene/core/src/test/org/apache/lucene/search/TestSubScorerFreqs.java
+++ lucene/core/src/test/org/apache/lucene/search/TestSubScorerFreqs.java
@@ -89,7 +89,7 @@ public class TestSubScorerFreqs extends LuceneTestCase {
       subScorers.put(scorer.getWeight().getQuery(), scorer);
     }
     
-    public LeafCollector getLeafCollector(AtomicReaderContext context)
+    public LeafCollector getLeafCollector(LeafReaderContext context)
         throws IOException {
       final int docBase = context.docBase;
       return new FilterLeafCollector(super.getLeafCollector(context)) {
diff --git lucene/core/src/test/org/apache/lucene/search/TestTermScorer.java lucene/core/src/test/org/apache/lucene/search/TestTermScorer.java
index c33e35a..1a37f96 100644
--- lucene/core/src/test/org/apache/lucene/search/TestTermScorer.java
+++ lucene/core/src/test/org/apache/lucene/search/TestTermScorer.java
@@ -24,7 +24,7 @@ import java.util.List;
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.RandomIndexWriter;
 import org.apache.lucene.index.SlowCompositeReaderWrapper;
@@ -76,8 +76,8 @@ public class TestTermScorer extends LuceneTestCase {
     TermQuery termQuery = new TermQuery(allTerm);
     
     Weight weight = indexSearcher.createNormalizedWeight(termQuery);
-    assertTrue(indexSearcher.getTopReaderContext() instanceof AtomicReaderContext);
-    AtomicReaderContext context = (AtomicReaderContext)indexSearcher.getTopReaderContext();
+    assertTrue(indexSearcher.getTopReaderContext() instanceof LeafReaderContext);
+    LeafReaderContext context = (LeafReaderContext)indexSearcher.getTopReaderContext();
     BulkScorer ts = weight.bulkScorer(context, true, context.reader().getLiveDocs());
     // we have 2 documents with the term all in them, one document for all the
     // other values
@@ -104,7 +104,7 @@ public class TestTermScorer extends LuceneTestCase {
       }
       
       @Override
-      protected void doSetNextReader(AtomicReaderContext context) throws IOException {
+      protected void doSetNextReader(LeafReaderContext context) throws IOException {
         base = context.docBase;
       }
       
@@ -138,8 +138,8 @@ public class TestTermScorer extends LuceneTestCase {
     TermQuery termQuery = new TermQuery(allTerm);
     
     Weight weight = indexSearcher.createNormalizedWeight(termQuery);
-    assertTrue(indexSearcher.getTopReaderContext() instanceof AtomicReaderContext);
-    AtomicReaderContext context = (AtomicReaderContext) indexSearcher.getTopReaderContext();
+    assertTrue(indexSearcher.getTopReaderContext() instanceof LeafReaderContext);
+    LeafReaderContext context = (LeafReaderContext) indexSearcher.getTopReaderContext();
     Scorer ts = weight.scorer(context, context.reader().getLiveDocs());
     assertTrue("next did not return a doc",
         ts.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);
@@ -157,8 +157,8 @@ public class TestTermScorer extends LuceneTestCase {
     TermQuery termQuery = new TermQuery(allTerm);
     
     Weight weight = indexSearcher.createNormalizedWeight(termQuery);
-    assertTrue(indexSearcher.getTopReaderContext() instanceof AtomicReaderContext);
-    AtomicReaderContext context = (AtomicReaderContext) indexSearcher.getTopReaderContext();
+    assertTrue(indexSearcher.getTopReaderContext() instanceof LeafReaderContext);
+    LeafReaderContext context = (LeafReaderContext) indexSearcher.getTopReaderContext();
     Scorer ts = weight.scorer(context, context.reader().getLiveDocs());
     assertTrue("Didn't skip", ts.advance(3) != DocIdSetIterator.NO_MORE_DOCS);
     // The next doc should be doc 5
diff --git lucene/core/src/test/org/apache/lucene/search/TestTimeLimitingCollector.java lucene/core/src/test/org/apache/lucene/search/TestTimeLimitingCollector.java
index 7dd0379..c2e7b93 100644
--- lucene/core/src/test/org/apache/lucene/search/TestTimeLimitingCollector.java
+++ lucene/core/src/test/org/apache/lucene/search/TestTimeLimitingCollector.java
@@ -23,7 +23,7 @@ import java.util.BitSet;
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.RandomIndexWriter;
 import org.apache.lucene.index.Term;
@@ -351,7 +351,7 @@ public class TestTimeLimitingCollector extends LuceneTestCase {
     }
     
     @Override
-    protected void doSetNextReader(AtomicReaderContext context) throws IOException {
+    protected void doSetNextReader(LeafReaderContext context) throws IOException {
       docBase = context.docBase;
     }
     
diff --git lucene/core/src/test/org/apache/lucene/search/TestTopDocsCollector.java lucene/core/src/test/org/apache/lucene/search/TestTopDocsCollector.java
index 0c56e11..c3b9ec1 100644
--- lucene/core/src/test/org/apache/lucene/search/TestTopDocsCollector.java
+++ lucene/core/src/test/org/apache/lucene/search/TestTopDocsCollector.java
@@ -20,7 +20,7 @@ package org.apache.lucene.search;
 import java.io.IOException;
 
 import org.apache.lucene.document.Document;
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.RandomIndexWriter;
 import org.apache.lucene.store.Directory;
@@ -61,7 +61,7 @@ public class TestTopDocsCollector extends LuceneTestCase {
     }
 
     @Override
-    protected void doSetNextReader(AtomicReaderContext context) throws IOException {
+    protected void doSetNextReader(LeafReaderContext context) throws IOException {
       base = context.docBase;
     }
 
diff --git lucene/core/src/test/org/apache/lucene/search/TestTopDocsMerge.java lucene/core/src/test/org/apache/lucene/search/TestTopDocsMerge.java
index 716d5f8..6aba98c 100644
--- lucene/core/src/test/org/apache/lucene/search/TestTopDocsMerge.java
+++ lucene/core/src/test/org/apache/lucene/search/TestTopDocsMerge.java
@@ -21,7 +21,7 @@ import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.FloatField;
 import org.apache.lucene.document.IntField;
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.CompositeReaderContext;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexReaderContext;
@@ -40,9 +40,9 @@ import java.util.List;
 public class TestTopDocsMerge extends LuceneTestCase {
 
   private static class ShardSearcher extends IndexSearcher {
-    private final List<AtomicReaderContext> ctx;
+    private final List<LeafReaderContext> ctx;
 
-    public ShardSearcher(AtomicReaderContext ctx, IndexReaderContext parent) {
+    public ShardSearcher(LeafReaderContext ctx, IndexReaderContext parent) {
       super(parent);
       this.ctx = Collections.singletonList(ctx);
     }
@@ -133,10 +133,10 @@ public class TestTopDocsMerge extends LuceneTestCase {
     final ShardSearcher[] subSearchers;
     final int[] docStarts;
 
-    if (ctx instanceof AtomicReaderContext) {
+    if (ctx instanceof LeafReaderContext) {
       subSearchers = new ShardSearcher[1];
       docStarts = new int[1];
-      subSearchers[0] = new ShardSearcher((AtomicReaderContext) ctx, ctx);
+      subSearchers[0] = new ShardSearcher((LeafReaderContext) ctx, ctx);
       docStarts[0] = 0;
     } else {
       final CompositeReaderContext compCTX = (CompositeReaderContext) ctx;
@@ -145,7 +145,7 @@ public class TestTopDocsMerge extends LuceneTestCase {
       docStarts = new int[size];
       int docBase = 0;
       for(int searcherIDX=0;searcherIDX<subSearchers.length;searcherIDX++) {
-        final AtomicReaderContext leave = compCTX.leaves().get(searcherIDX);
+        final LeafReaderContext leave = compCTX.leaves().get(searcherIDX);
         subSearchers[searcherIDX] = new ShardSearcher(leave, compCTX);
         docStarts[searcherIDX] = docBase;
         docBase += leave.reader().maxDoc();
diff --git lucene/core/src/test/org/apache/lucene/search/spans/JustCompileSearchSpans.java lucene/core/src/test/org/apache/lucene/search/spans/JustCompileSearchSpans.java
index 177874f..56afd7e 100644
--- lucene/core/src/test/org/apache/lucene/search/spans/JustCompileSearchSpans.java
+++ lucene/core/src/test/org/apache/lucene/search/spans/JustCompileSearchSpans.java
@@ -21,7 +21,7 @@ import java.io.IOException;
 import java.util.Collection;
 import java.util.Map;
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.index.TermContext;
 import org.apache.lucene.search.Weight;
@@ -90,7 +90,7 @@ final class JustCompileSearchSpans {
     }
 
     @Override
-    public Spans getSpans(AtomicReaderContext context, Bits acceptDocs, Map<Term,TermContext> termContexts) {
+    public Spans getSpans(LeafReaderContext context, Bits acceptDocs, Map<Term,TermContext> termContexts) {
       throw new UnsupportedOperationException(UNSUPPORTED_MSG);
     }
 
diff --git lucene/core/src/test/org/apache/lucene/search/spans/MultiSpansWrapper.java lucene/core/src/test/org/apache/lucene/search/spans/MultiSpansWrapper.java
index a07db83..7490c61 100644
--- lucene/core/src/test/org/apache/lucene/search/spans/MultiSpansWrapper.java
+++ lucene/core/src/test/org/apache/lucene/search/spans/MultiSpansWrapper.java
@@ -25,7 +25,7 @@ import java.util.List;
 import java.util.Map;
 import java.util.TreeSet;
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.IndexReaderContext;
 import org.apache.lucene.index.ReaderUtil;
 import org.apache.lucene.index.Term;
@@ -42,13 +42,13 @@ import org.apache.lucene.search.DocIdSetIterator;
 public class MultiSpansWrapper extends Spans { // can't be package private due to payloads
 
   private SpanQuery query;
-  private List<AtomicReaderContext> leaves;
+  private List<LeafReaderContext> leaves;
   private int leafOrd = 0;
   private Spans current;
   private Map<Term,TermContext> termContexts;
   private final int numLeaves;
 
-  private MultiSpansWrapper(List<AtomicReaderContext> leaves, SpanQuery query, Map<Term,TermContext> termContexts) {
+  private MultiSpansWrapper(List<LeafReaderContext> leaves, SpanQuery query, Map<Term,TermContext> termContexts) {
     this.query = query;
     this.leaves = leaves;
     this.numLeaves = leaves.size();
@@ -62,9 +62,9 @@ public class MultiSpansWrapper extends Spans { // can't be package private due t
     for (Term term : terms) {
       termContexts.put(term, TermContext.build(topLevelReaderContext, term));
     }
-    final List<AtomicReaderContext> leaves = topLevelReaderContext.leaves();
+    final List<LeafReaderContext> leaves = topLevelReaderContext.leaves();
     if(leaves.size() == 1) {
-      final AtomicReaderContext ctx = leaves.get(0);
+      final LeafReaderContext ctx = leaves.get(0);
       return query.getSpans(ctx, ctx.reader().getLiveDocs(), termContexts);
     }
     return new MultiSpansWrapper(leaves, query, termContexts);
@@ -76,7 +76,7 @@ public class MultiSpansWrapper extends Spans { // can't be package private due t
       return false;
     }
     if (current == null) {
-      final AtomicReaderContext ctx = leaves.get(leafOrd);
+      final LeafReaderContext ctx = leaves.get(leafOrd);
       current = query.getSpans(ctx, ctx.reader().getLiveDocs(), termContexts);
     }
     while(true) {
@@ -84,7 +84,7 @@ public class MultiSpansWrapper extends Spans { // can't be package private due t
         return true;
       }
       if (++leafOrd < numLeaves) {
-        final AtomicReaderContext ctx = leaves.get(leafOrd);
+        final LeafReaderContext ctx = leaves.get(leafOrd);
         current = query.getSpans(ctx, ctx.reader().getLiveDocs(), termContexts);
       } else {
         current = null;
@@ -103,11 +103,11 @@ public class MultiSpansWrapper extends Spans { // can't be package private due t
     int subIndex = ReaderUtil.subIndex(target, leaves);
     assert subIndex >= leafOrd;
     if (subIndex != leafOrd) {
-      final AtomicReaderContext ctx = leaves.get(subIndex);
+      final LeafReaderContext ctx = leaves.get(subIndex);
       current = query.getSpans(ctx, ctx.reader().getLiveDocs(), termContexts);
       leafOrd = subIndex;
     } else if (current == null) {
-      final AtomicReaderContext ctx = leaves.get(leafOrd);
+      final LeafReaderContext ctx = leaves.get(leafOrd);
       current = query.getSpans(ctx, ctx.reader().getLiveDocs(), termContexts);
     }
     while (true) {
@@ -120,7 +120,7 @@ public class MultiSpansWrapper extends Spans { // can't be package private due t
         return true;
       }
       if (++leafOrd < numLeaves) {
-        final AtomicReaderContext ctx = leaves.get(leafOrd);
+        final LeafReaderContext ctx = leaves.get(leafOrd);
         current = query.getSpans(ctx, ctx.reader().getLiveDocs(), termContexts);
       } else {
         current = null;
diff --git lucene/core/src/test/org/apache/lucene/search/spans/TestNearSpansOrdered.java lucene/core/src/test/org/apache/lucene/search/spans/TestNearSpansOrdered.java
index 475fb28..5b55d31 100644
--- lucene/core/src/test/org/apache/lucene/search/spans/TestNearSpansOrdered.java
+++ lucene/core/src/test/org/apache/lucene/search/spans/TestNearSpansOrdered.java
@@ -20,7 +20,7 @@ package org.apache.lucene.search.spans;
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.RandomIndexWriter;
 import org.apache.lucene.index.IndexReaderContext;
@@ -166,7 +166,7 @@ public class TestNearSpansOrdered extends LuceneTestCase {
     SpanNearQuery q = makeQuery();
     Weight w = searcher.createNormalizedWeight(q);
     IndexReaderContext topReaderContext = searcher.getTopReaderContext();
-    AtomicReaderContext leave = topReaderContext.leaves().get(0);
+    LeafReaderContext leave = topReaderContext.leaves().get(0);
     Scorer s = w.scorer(leave, leave.reader().getLiveDocs());
     assertEquals(1, s.advance(1));
   }
diff --git lucene/core/src/test/org/apache/lucene/search/spans/TestSpans.java lucene/core/src/test/org/apache/lucene/search/spans/TestSpans.java
index de0038c..5a960f3 100644
--- lucene/core/src/test/org/apache/lucene/search/spans/TestSpans.java
+++ lucene/core/src/test/org/apache/lucene/search/spans/TestSpans.java
@@ -23,7 +23,7 @@ import java.util.List;
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexReaderContext;
@@ -406,10 +406,10 @@ public class TestSpans extends LuceneTestCase {
     boolean ordered = true;
     int slop = 1;
     IndexReaderContext topReaderContext = searcher.getTopReaderContext();
-    List<AtomicReaderContext> leaves = topReaderContext.leaves();
+    List<LeafReaderContext> leaves = topReaderContext.leaves();
     int subIndex = ReaderUtil.subIndex(11, leaves);
     for (int i = 0, c = leaves.size(); i < c; i++) {
-      final AtomicReaderContext ctx = leaves.get(i);
+      final LeafReaderContext ctx = leaves.get(i);
      
       final Similarity sim = new DefaultSimilarity() {
         @Override
diff --git lucene/expressions/src/java/org/apache/lucene/expressions/ExpressionComparator.java lucene/expressions/src/java/org/apache/lucene/expressions/ExpressionComparator.java
index 47d3eaa..ebeeca9 100644
--- lucene/expressions/src/java/org/apache/lucene/expressions/ExpressionComparator.java
+++ lucene/expressions/src/java/org/apache/lucene/expressions/ExpressionComparator.java
@@ -20,7 +20,7 @@ import java.io.IOException;
 import java.util.HashMap;
 import java.util.Map;
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.search.FieldComparator;
@@ -34,7 +34,7 @@ class ExpressionComparator extends FieldComparator<Double> {
   
   private ValueSource source;
   private FunctionValues scores;
-  private AtomicReaderContext readerContext;
+  private LeafReaderContext readerContext;
   
   public ExpressionComparator(ValueSource source, int numHits) {
     values = new double[numHits];
@@ -83,7 +83,7 @@ class ExpressionComparator extends FieldComparator<Double> {
   }
   
   @Override
-  public FieldComparator<Double> setNextReader(AtomicReaderContext context) throws IOException {
+  public FieldComparator<Double> setNextReader(LeafReaderContext context) throws IOException {
     this.readerContext = context;
     return this;
   }
diff --git lucene/expressions/src/java/org/apache/lucene/expressions/ExpressionRescorer.java lucene/expressions/src/java/org/apache/lucene/expressions/ExpressionRescorer.java
index 4314c27..18ec02c 100644
--- lucene/expressions/src/java/org/apache/lucene/expressions/ExpressionRescorer.java
+++ lucene/expressions/src/java/org/apache/lucene/expressions/ExpressionRescorer.java
@@ -23,7 +23,7 @@ import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.ReaderUtil;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.search.Explanation;
@@ -112,9 +112,9 @@ class ExpressionRescorer extends SortRescorer {
   public Explanation explain(IndexSearcher searcher, Explanation firstPassExplanation, int docID) throws IOException {
     Explanation result = super.explain(searcher, firstPassExplanation, docID);
 
-    List<AtomicReaderContext> leaves = searcher.getIndexReader().leaves();
+    List<LeafReaderContext> leaves = searcher.getIndexReader().leaves();
     int subReader = ReaderUtil.subIndex(docID, leaves);
-    AtomicReaderContext readerContext = leaves.get(subReader);
+    LeafReaderContext readerContext = leaves.get(subReader);
     int docIDInSegment = docID - readerContext.docBase;
     Map<String,Object> context = new HashMap<>();
 
diff --git lucene/expressions/src/java/org/apache/lucene/expressions/ExpressionValueSource.java lucene/expressions/src/java/org/apache/lucene/expressions/ExpressionValueSource.java
index 3cc0fc9..5839796 100644
--- lucene/expressions/src/java/org/apache/lucene/expressions/ExpressionValueSource.java
+++ lucene/expressions/src/java/org/apache/lucene/expressions/ExpressionValueSource.java
@@ -21,7 +21,7 @@ import java.util.Arrays;
 import java.util.HashMap;
 import java.util.Map;
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.search.SortField;
@@ -58,7 +58,7 @@ final class ExpressionValueSource extends ValueSource {
   }
 
   @Override
-  public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
+  public FunctionValues getValues(Map context, LeafReaderContext readerContext) throws IOException {
     Map<String, FunctionValues> valuesCache = (Map<String, FunctionValues>)context.get("valuesCache");
     if (valuesCache == null) {
       valuesCache = new HashMap<>();
diff --git lucene/expressions/src/java/org/apache/lucene/expressions/ScoreValueSource.java lucene/expressions/src/java/org/apache/lucene/expressions/ScoreValueSource.java
index 6502cf1..c6ffb2a 100644
--- lucene/expressions/src/java/org/apache/lucene/expressions/ScoreValueSource.java
+++ lucene/expressions/src/java/org/apache/lucene/expressions/ScoreValueSource.java
@@ -17,7 +17,7 @@ package org.apache.lucene.expressions;
  * limitations under the License.
  */
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.search.Scorer;
@@ -36,7 +36,7 @@ class ScoreValueSource extends ValueSource {
    * <code>context</code> must contain a key "scorer" which is a {@link Scorer}.
    */
   @Override
-  public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
+  public FunctionValues getValues(Map context, LeafReaderContext readerContext) throws IOException {
     Scorer v = (Scorer) context.get("scorer");
     if (v == null) {
       throw new IllegalStateException("Expressions referencing the score can only be used for sorting");
diff --git lucene/expressions/src/test/org/apache/lucene/expressions/TestExpressionValueSource.java lucene/expressions/src/test/org/apache/lucene/expressions/TestExpressionValueSource.java
index 5ba6268..debcb81 100644
--- lucene/expressions/src/test/org/apache/lucene/expressions/TestExpressionValueSource.java
+++ lucene/expressions/src/test/org/apache/lucene/expressions/TestExpressionValueSource.java
@@ -24,7 +24,7 @@ import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.NumericDocValuesField;
 import org.apache.lucene.expressions.js.JavascriptCompiler;
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexWriterConfig;
 import org.apache.lucene.index.RandomIndexWriter;
@@ -85,7 +85,7 @@ public class TestExpressionValueSource extends LuceneTestCase {
     ValueSource vs = expr.getValueSource(bindings);
     
     assertEquals(1, reader.leaves().size());
-    AtomicReaderContext leaf = reader.leaves().get(0);
+    LeafReaderContext leaf = reader.leaves().get(0);
     FunctionValues values = vs.getValues(new HashMap<String,Object>(), leaf);
     
     assertEquals(10, values.doubleVal(0), 0);
@@ -123,7 +123,7 @@ public class TestExpressionValueSource extends LuceneTestCase {
     ValueSource vs = expr.getValueSource(bindings);
     
     assertEquals(1, reader.leaves().size());
-    AtomicReaderContext leaf = reader.leaves().get(0);
+    LeafReaderContext leaf = reader.leaves().get(0);
     FunctionValues values = vs.getValues(new HashMap<String,Object>(), leaf);
     
     // everything
diff --git lucene/facet/src/java/org/apache/lucene/facet/DrillSidewaysQuery.java lucene/facet/src/java/org/apache/lucene/facet/DrillSidewaysQuery.java
index 972df94..45f647e 100644
--- lucene/facet/src/java/org/apache/lucene/facet/DrillSidewaysQuery.java
+++ lucene/facet/src/java/org/apache/lucene/facet/DrillSidewaysQuery.java
@@ -19,7 +19,7 @@ package org.apache.lucene.facet;
 import java.io.IOException;
 import java.util.Arrays;
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.search.Collector;
 import org.apache.lucene.search.DocIdSet;
@@ -91,7 +91,7 @@ class DrillSidewaysQuery extends Query {
 
     return new Weight() {
       @Override
-      public Explanation explain(AtomicReaderContext context, int doc) throws IOException {
+      public Explanation explain(LeafReaderContext context, int doc) throws IOException {
         return baseWeight.explain(context, doc);
       }
 
@@ -118,13 +118,13 @@ class DrillSidewaysQuery extends Query {
       }
 
       @Override
-      public Scorer scorer(AtomicReaderContext context, Bits acceptDocs) throws IOException {
+      public Scorer scorer(LeafReaderContext context, Bits acceptDocs) throws IOException {
         // We can only run as a top scorer:
         throw new UnsupportedOperationException();
       }
 
       @Override
-      public BulkScorer bulkScorer(AtomicReaderContext context, boolean scoreDocsInOrder, Bits acceptDocs) throws IOException {
+      public BulkScorer bulkScorer(LeafReaderContext context, boolean scoreDocsInOrder, Bits acceptDocs) throws IOException {
 
         // TODO: it could be better if we take acceptDocs
         // into account instead of baseScorer?
diff --git lucene/facet/src/java/org/apache/lucene/facet/DrillSidewaysScorer.java lucene/facet/src/java/org/apache/lucene/facet/DrillSidewaysScorer.java
index 273b6b1..4f8c116 100644
--- lucene/facet/src/java/org/apache/lucene/facet/DrillSidewaysScorer.java
+++ lucene/facet/src/java/org/apache/lucene/facet/DrillSidewaysScorer.java
@@ -21,7 +21,7 @@ import java.io.IOException;
 import java.util.Collection;
 import java.util.Collections;
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.DocsEnum;
 import org.apache.lucene.search.LeafCollector;
 import org.apache.lucene.search.Collector;
@@ -44,7 +44,7 @@ class DrillSidewaysScorer extends BulkScorer {
   // DrillDown DocsEnums:
   private final Scorer baseScorer;
 
-  private final AtomicReaderContext context;
+  private final LeafReaderContext context;
 
   final boolean scoreSubDocsAtOnce;
 
@@ -54,7 +54,7 @@ class DrillSidewaysScorer extends BulkScorer {
   private int collectDocID = -1;
   private float collectScore;
 
-  DrillSidewaysScorer(AtomicReaderContext context, Scorer baseScorer, Collector drillDownCollector,
+  DrillSidewaysScorer(LeafReaderContext context, Scorer baseScorer, Collector drillDownCollector,
                       DocsAndCost[] dims, boolean scoreSubDocsAtOnce) {
     this.dims = dims;
     this.context = context;
diff --git lucene/facet/src/java/org/apache/lucene/facet/FacetsCollector.java lucene/facet/src/java/org/apache/lucene/facet/FacetsCollector.java
index 90bbba6..13369ba 100644
--- lucene/facet/src/java/org/apache/lucene/facet/FacetsCollector.java
+++ lucene/facet/src/java/org/apache/lucene/facet/FacetsCollector.java
@@ -21,7 +21,7 @@ import java.io.IOException;
 import java.util.ArrayList;
 import java.util.List;
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.search.Collector;
 import org.apache.lucene.search.DocIdSet;
 import org.apache.lucene.search.FieldDoc;
@@ -50,7 +50,7 @@ import org.apache.lucene.util.FixedBitSet;
  *  {@link Collector}. */
 public class FacetsCollector extends SimpleCollector {
 
-  private AtomicReaderContext context;
+  private LeafReaderContext context;
   private Scorer scorer;
   private int totalHits;
   private float[] scores;
@@ -75,13 +75,13 @@ public class FacetsCollector extends SimpleCollector {
   }
 
   /**
-   * Holds the documents that were matched in the {@link AtomicReaderContext}.
+   * Holds the documents that were matched in the {@link org.apache.lucene.index.LeafReaderContext}.
    * If scores were required, then {@code scores} is not null.
    */
   public final static class MatchingDocs {
     
     /** Context for this segment. */
-    public final AtomicReaderContext context;
+    public final LeafReaderContext context;
 
     /** Which documents were seen. */
     public final DocIdSet bits;
@@ -93,7 +93,7 @@ public class FacetsCollector extends SimpleCollector {
     public final int totalHits;
 
     /** Sole constructor. */
-    public MatchingDocs(AtomicReaderContext context, DocIdSet bits, int totalHits, float[] scores) {
+    public MatchingDocs(LeafReaderContext context, DocIdSet bits, int totalHits, float[] scores) {
       this.context = context;
       this.bits = bits;
       this.scores = scores;
@@ -181,7 +181,7 @@ public class FacetsCollector extends SimpleCollector {
   }
     
   @Override
-  protected void doSetNextReader(AtomicReaderContext context) throws IOException {
+  protected void doSetNextReader(LeafReaderContext context) throws IOException {
     if (docs != null) {
       matchingDocs.add(new MatchingDocs(this.context, docs.getDocIdSet(), totalHits, scores));
     }
diff --git lucene/facet/src/java/org/apache/lucene/facet/range/DoubleRange.java lucene/facet/src/java/org/apache/lucene/facet/range/DoubleRange.java
index 1b71c30..f6e8e3b 100644
--- lucene/facet/src/java/org/apache/lucene/facet/range/DoubleRange.java
+++ lucene/facet/src/java/org/apache/lucene/facet/range/DoubleRange.java
@@ -20,7 +20,7 @@ package org.apache.lucene.facet.range;
 import java.io.IOException;
 import java.util.Collections;
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.search.DocIdSet;
@@ -110,7 +110,7 @@ public final class DoubleRange extends Range {
       }
 
       @Override
-      public DocIdSet getDocIdSet(AtomicReaderContext context, final Bits acceptDocs) throws IOException {
+      public DocIdSet getDocIdSet(LeafReaderContext context, final Bits acceptDocs) throws IOException {
 
         // TODO: this is just like ValueSourceScorer,
         // ValueSourceFilter (spatial),
diff --git lucene/facet/src/java/org/apache/lucene/facet/range/LongRange.java lucene/facet/src/java/org/apache/lucene/facet/range/LongRange.java
index 51afc8e..c382012 100644
--- lucene/facet/src/java/org/apache/lucene/facet/range/LongRange.java
+++ lucene/facet/src/java/org/apache/lucene/facet/range/LongRange.java
@@ -20,7 +20,7 @@ package org.apache.lucene.facet.range;
 import java.io.IOException;
 import java.util.Collections;
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.search.DocIdSet;
@@ -102,7 +102,7 @@ public final class LongRange extends Range {
       }
 
       @Override
-      public DocIdSet getDocIdSet(AtomicReaderContext context, final Bits acceptDocs) throws IOException {
+      public DocIdSet getDocIdSet(LeafReaderContext context, final Bits acceptDocs) throws IOException {
 
         // TODO: this is just like ValueSourceScorer,
         // ValueSourceFilter (spatial),
diff --git lucene/facet/src/java/org/apache/lucene/facet/sortedset/DefaultSortedSetDocValuesReaderState.java lucene/facet/src/java/org/apache/lucene/facet/sortedset/DefaultSortedSetDocValuesReaderState.java
index 53ecd3a..b5f627d 100644
--- lucene/facet/src/java/org/apache/lucene/facet/sortedset/DefaultSortedSetDocValuesReaderState.java
+++ lucene/facet/src/java/org/apache/lucene/facet/sortedset/DefaultSortedSetDocValuesReaderState.java
@@ -6,8 +6,7 @@ import java.util.HashMap;
 import java.util.Map;
 
 import org.apache.lucene.facet.FacetsConfig;
-import org.apache.lucene.facet.sortedset.SortedSetDocValuesReaderState.OrdRange;
-import org.apache.lucene.index.AtomicReader;
+import org.apache.lucene.index.LeafReader;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.SlowCompositeReaderWrapper;
 import org.apache.lucene.index.SortedSetDocValues;
@@ -36,7 +35,7 @@ import org.apache.lucene.util.BytesRef;
 public class DefaultSortedSetDocValuesReaderState extends SortedSetDocValuesReaderState {
 
   private final String field;
-  private final AtomicReader topReader;
+  private final LeafReader topReader;
   private final int valueCount;
 
   /** {@link IndexReader} passed to the constructor. */
diff --git lucene/facet/src/java/org/apache/lucene/facet/sortedset/SortedSetDocValuesFacetCounts.java lucene/facet/src/java/org/apache/lucene/facet/sortedset/SortedSetDocValuesFacetCounts.java
index ccbae26..a54932f 100644
--- lucene/facet/src/java/org/apache/lucene/facet/sortedset/SortedSetDocValuesFacetCounts.java
+++ lucene/facet/src/java/org/apache/lucene/facet/sortedset/SortedSetDocValuesFacetCounts.java
@@ -32,7 +32,7 @@ import org.apache.lucene.facet.FacetsConfig;
 import org.apache.lucene.facet.LabelAndValue;
 import org.apache.lucene.facet.TopOrdAndIntQueue;
 import org.apache.lucene.facet.sortedset.SortedSetDocValuesReaderState.OrdRange;
-import org.apache.lucene.index.AtomicReader;
+import org.apache.lucene.index.LeafReader;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.MultiDocValues;
 import org.apache.lucene.index.MultiDocValues.MultiSortedSetDocValues;
@@ -162,7 +162,7 @@ public class SortedSetDocValuesFacetCounts extends Facets {
 
     for(MatchingDocs hits : matchingDocs) {
 
-      AtomicReader reader = hits.context.reader();
+      LeafReader reader = hits.context.reader();
       //System.out.println("  reader=" + reader);
       // LUCENE-5090: make sure the provided reader context "matches"
       // the top-level reader passed to the
diff --git lucene/facet/src/java/org/apache/lucene/facet/taxonomy/CachedOrdinalsReader.java lucene/facet/src/java/org/apache/lucene/facet/taxonomy/CachedOrdinalsReader.java
index d50c5a3..fc334c5 100644
--- lucene/facet/src/java/org/apache/lucene/facet/taxonomy/CachedOrdinalsReader.java
+++ lucene/facet/src/java/org/apache/lucene/facet/taxonomy/CachedOrdinalsReader.java
@@ -23,7 +23,7 @@ import java.util.Map;
 import java.util.WeakHashMap;
 
 import org.apache.lucene.codecs.DocValuesFormat;
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.BinaryDocValues;
 import org.apache.lucene.util.Accountable;
 import org.apache.lucene.util.Accountables;
@@ -67,7 +67,7 @@ public class CachedOrdinalsReader extends OrdinalsReader implements Accountable
     this.source = source;
   }
 
-  private synchronized CachedOrds getCachedOrds(AtomicReaderContext context) throws IOException {
+  private synchronized CachedOrds getCachedOrds(LeafReaderContext context) throws IOException {
     Object cacheKey = context.reader().getCoreCacheKey();
     CachedOrds ords = ordsCache.get(cacheKey);
     if (ords == null) {
@@ -84,7 +84,7 @@ public class CachedOrdinalsReader extends OrdinalsReader implements Accountable
   }
 
   @Override
-  public OrdinalsSegmentReader getReader(AtomicReaderContext context) throws IOException {
+  public OrdinalsSegmentReader getReader(LeafReaderContext context) throws IOException {
     final CachedOrds cachedOrds = getCachedOrds(context);
     return new OrdinalsSegmentReader() {
       @Override
diff --git lucene/facet/src/java/org/apache/lucene/facet/taxonomy/DocValuesOrdinalsReader.java lucene/facet/src/java/org/apache/lucene/facet/taxonomy/DocValuesOrdinalsReader.java
index 4afbfb7..8205132 100644
--- lucene/facet/src/java/org/apache/lucene/facet/taxonomy/DocValuesOrdinalsReader.java
+++ lucene/facet/src/java/org/apache/lucene/facet/taxonomy/DocValuesOrdinalsReader.java
@@ -20,7 +20,7 @@ package org.apache.lucene.facet.taxonomy;
 import java.io.IOException;
 
 import org.apache.lucene.facet.FacetsConfig;
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.BinaryDocValues;
 import org.apache.lucene.index.DocValues;
 import org.apache.lucene.util.ArrayUtil;
@@ -43,7 +43,7 @@ public class DocValuesOrdinalsReader extends OrdinalsReader {
   }
 
   @Override
-  public OrdinalsSegmentReader getReader(AtomicReaderContext context) throws IOException {
+  public OrdinalsSegmentReader getReader(LeafReaderContext context) throws IOException {
     BinaryDocValues values0 = context.reader().getBinaryDocValues(field);
     if (values0 == null) {
       values0 = DocValues.emptyBinary();
diff --git lucene/facet/src/java/org/apache/lucene/facet/taxonomy/OrdinalMappingAtomicReader.java lucene/facet/src/java/org/apache/lucene/facet/taxonomy/OrdinalMappingAtomicReader.java
deleted file mode 100644
index 7fb6742..0000000
--- lucene/facet/src/java/org/apache/lucene/facet/taxonomy/OrdinalMappingAtomicReader.java
+++ /dev/null
@@ -1,160 +0,0 @@
-package org.apache.lucene.facet.taxonomy;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.HashSet;
-import java.util.Set;
-
-import org.apache.lucene.facet.FacetsConfig;
-import org.apache.lucene.facet.FacetsConfig.DimConfig;
-import org.apache.lucene.facet.taxonomy.OrdinalsReader.OrdinalsSegmentReader;
-import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter.OrdinalMap;
-import org.apache.lucene.index.AtomicReader;
-import org.apache.lucene.index.BinaryDocValues;
-import org.apache.lucene.index.FilterAtomicReader;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IntsRef;
-
-/**
- * A {@link FilterAtomicReader} for updating facets ordinal references,
- * based on an ordinal map. You should use this code in conjunction with merging
- * taxonomies - after you merge taxonomies, you receive an {@link OrdinalMap}
- * which maps the 'old' ordinals to the 'new' ones. You can use that map to
- * re-map the doc values which contain the facets information (ordinals) either
- * before or while merging the indexes.
- * <p>
- * For re-mapping the ordinals during index merge, do the following:
- * 
- * <pre class="prettyprint">
- * // merge the old taxonomy with the new one.
- * OrdinalMap map = new MemoryOrdinalMap();
- * DirectoryTaxonomyWriter.addTaxonomy(srcTaxoDir, map);
- * int[] ordmap = map.getMap();
- * 
- * // Add the index and re-map ordinals on the go
- * DirectoryReader reader = DirectoryReader.open(oldDir);
- * IndexWriterConfig conf = new IndexWriterConfig(VER, ANALYZER);
- * IndexWriter writer = new IndexWriter(newDir, conf);
- * List&lt;AtomicReaderContext&gt; leaves = reader.leaves();
- * AtomicReader wrappedLeaves[] = new AtomicReader[leaves.size()];
- * for (int i = 0; i < leaves.size(); i++) {
- *   wrappedLeaves[i] = new OrdinalMappingAtomicReader(leaves.get(i).reader(), ordmap);
- * }
- * writer.addIndexes(new MultiReader(wrappedLeaves));
- * writer.commit();
- * </pre>
- * 
- * @lucene.experimental
- */
-public class OrdinalMappingAtomicReader extends FilterAtomicReader {
-  
-  // silly way, but we need to use dedupAndEncode and it's protected on FacetsConfig.
-  private static class InnerFacetsConfig extends FacetsConfig {
-    
-    InnerFacetsConfig() {}
-    
-    @Override
-    public BytesRef dedupAndEncode(IntsRef ordinals) {
-      return super.dedupAndEncode(ordinals);
-    }
-    
-  }
-  
-  private class OrdinalMappingBinaryDocValues extends BinaryDocValues {
-    
-    private final IntsRef ordinals = new IntsRef(32);
-    private final OrdinalsSegmentReader ordsReader;
-    
-    OrdinalMappingBinaryDocValues(OrdinalsSegmentReader ordsReader) throws IOException {
-      this.ordsReader = ordsReader;
-    }
-    
-    @SuppressWarnings("synthetic-access")
-    @Override
-    public BytesRef get(int docID) {
-      try {
-        // NOTE: this isn't quite koscher, because in general
-        // multiple threads can call BinaryDV.get which would
-        // then conflict on the single ordinals instance, but
-        // because this impl is only used for merging, we know
-        // only 1 thread calls us:
-        ordsReader.get(docID, ordinals);
-        
-        // map the ordinals
-        for (int i = 0; i < ordinals.length; i++) {
-          ordinals.ints[i] = ordinalMap[ordinals.ints[i]];
-        }
-        
-        return encode(ordinals);
-      } catch (IOException e) {
-        throw new RuntimeException("error reading category ordinals for doc " + docID, e);
-      }
-    }
-  }
-  
-  private final int[] ordinalMap;
-  private final InnerFacetsConfig facetsConfig;
-  private final Set<String> facetFields;
-  
-  /**
-   * Wraps an AtomicReader, mapping ordinals according to the ordinalMap, using
-   * the provided {@link FacetsConfig} which was used to build the wrapped
-   * reader.
-   */
-  public OrdinalMappingAtomicReader(AtomicReader in, int[] ordinalMap, FacetsConfig srcConfig) {
-    super(in);
-    this.ordinalMap = ordinalMap;
-    facetsConfig = new InnerFacetsConfig();
-    facetFields = new HashSet<>();
-    for (DimConfig dc : srcConfig.getDimConfigs().values()) {
-      facetFields.add(dc.indexFieldName);
-    }
-    // always add the default indexFieldName. This is because FacetsConfig does
-    // not explicitly record dimensions that were indexed under the default
-    // DimConfig, unless they have a custome DimConfig.
-    facetFields.add(FacetsConfig.DEFAULT_DIM_CONFIG.indexFieldName);
-  }
-  
-  /**
-   * Expert: encodes category ordinals into a BytesRef. Override in case you use
-   * custom encoding, other than the default done by FacetsConfig.
-   */
-  protected BytesRef encode(IntsRef ordinals) {
-    return facetsConfig.dedupAndEncode(ordinals);
-  }
-  
-  /**
-   * Expert: override in case you used custom encoding for the categories under
-   * this field.
-   */
-  protected OrdinalsReader getOrdinalsReader(String field) {
-    return new DocValuesOrdinalsReader(field);
-  }
-  
-  @Override
-  public BinaryDocValues getBinaryDocValues(String field) throws IOException {
-    if (facetFields.contains(field)) {
-      final OrdinalsReader ordsReader = getOrdinalsReader(field);
-      return new OrdinalMappingBinaryDocValues(ordsReader.getReader(in.getContext()));
-    } else {
-      return in.getBinaryDocValues(field);
-    }
-  }
-  
-}
diff --git lucene/facet/src/java/org/apache/lucene/facet/taxonomy/OrdinalMappingLeafReader.java lucene/facet/src/java/org/apache/lucene/facet/taxonomy/OrdinalMappingLeafReader.java
new file mode 100644
index 0000000..672e7cb
--- /dev/null
+++ lucene/facet/src/java/org/apache/lucene/facet/taxonomy/OrdinalMappingLeafReader.java
@@ -0,0 +1,160 @@
+package org.apache.lucene.facet.taxonomy;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.HashSet;
+import java.util.Set;
+
+import org.apache.lucene.facet.FacetsConfig;
+import org.apache.lucene.facet.FacetsConfig.DimConfig;
+import org.apache.lucene.facet.taxonomy.OrdinalsReader.OrdinalsSegmentReader;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter.OrdinalMap;
+import org.apache.lucene.index.FilterLeafReader;
+import org.apache.lucene.index.LeafReader;
+import org.apache.lucene.index.BinaryDocValues;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IntsRef;
+
+/**
+ * A {@link org.apache.lucene.index.FilterLeafReader} for updating facets ordinal references,
+ * based on an ordinal map. You should use this code in conjunction with merging
+ * taxonomies - after you merge taxonomies, you receive an {@link OrdinalMap}
+ * which maps the 'old' ordinals to the 'new' ones. You can use that map to
+ * re-map the doc values which contain the facets information (ordinals) either
+ * before or while merging the indexes.
+ * <p>
+ * For re-mapping the ordinals during index merge, do the following:
+ * 
+ * <pre class="prettyprint">
+ * // merge the old taxonomy with the new one.
+ * OrdinalMap map = new MemoryOrdinalMap();
+ * DirectoryTaxonomyWriter.addTaxonomy(srcTaxoDir, map);
+ * int[] ordmap = map.getMap();
+ * 
+ * // Add the index and re-map ordinals on the go
+ * DirectoryReader reader = DirectoryReader.open(oldDir);
+ * IndexWriterConfig conf = new IndexWriterConfig(VER, ANALYZER);
+ * IndexWriter writer = new IndexWriter(newDir, conf);
+ * List&lt;AtomicReaderContext&gt; leaves = reader.leaves();
+ * AtomicReader wrappedLeaves[] = new AtomicReader[leaves.size()];
+ * for (int i = 0; i < leaves.size(); i++) {
+ *   wrappedLeaves[i] = new OrdinalMappingAtomicReader(leaves.get(i).reader(), ordmap);
+ * }
+ * writer.addIndexes(new MultiReader(wrappedLeaves));
+ * writer.commit();
+ * </pre>
+ * 
+ * @lucene.experimental
+ */
+public class OrdinalMappingLeafReader extends FilterLeafReader {
+  
+  // silly way, but we need to use dedupAndEncode and it's protected on FacetsConfig.
+  private static class InnerFacetsConfig extends FacetsConfig {
+    
+    InnerFacetsConfig() {}
+    
+    @Override
+    public BytesRef dedupAndEncode(IntsRef ordinals) {
+      return super.dedupAndEncode(ordinals);
+    }
+    
+  }
+  
+  private class OrdinalMappingBinaryDocValues extends BinaryDocValues {
+    
+    private final IntsRef ordinals = new IntsRef(32);
+    private final OrdinalsSegmentReader ordsReader;
+    
+    OrdinalMappingBinaryDocValues(OrdinalsSegmentReader ordsReader) throws IOException {
+      this.ordsReader = ordsReader;
+    }
+    
+    @SuppressWarnings("synthetic-access")
+    @Override
+    public BytesRef get(int docID) {
+      try {
+        // NOTE: this isn't quite koscher, because in general
+        // multiple threads can call BinaryDV.get which would
+        // then conflict on the single ordinals instance, but
+        // because this impl is only used for merging, we know
+        // only 1 thread calls us:
+        ordsReader.get(docID, ordinals);
+        
+        // map the ordinals
+        for (int i = 0; i < ordinals.length; i++) {
+          ordinals.ints[i] = ordinalMap[ordinals.ints[i]];
+        }
+        
+        return encode(ordinals);
+      } catch (IOException e) {
+        throw new RuntimeException("error reading category ordinals for doc " + docID, e);
+      }
+    }
+  }
+  
+  private final int[] ordinalMap;
+  private final InnerFacetsConfig facetsConfig;
+  private final Set<String> facetFields;
+  
+  /**
+   * Wraps an AtomicReader, mapping ordinals according to the ordinalMap, using
+   * the provided {@link FacetsConfig} which was used to build the wrapped
+   * reader.
+   */
+  public OrdinalMappingLeafReader(LeafReader in, int[] ordinalMap, FacetsConfig srcConfig) {
+    super(in);
+    this.ordinalMap = ordinalMap;
+    facetsConfig = new InnerFacetsConfig();
+    facetFields = new HashSet<>();
+    for (DimConfig dc : srcConfig.getDimConfigs().values()) {
+      facetFields.add(dc.indexFieldName);
+    }
+    // always add the default indexFieldName. This is because FacetsConfig does
+    // not explicitly record dimensions that were indexed under the default
+    // DimConfig, unless they have a custome DimConfig.
+    facetFields.add(FacetsConfig.DEFAULT_DIM_CONFIG.indexFieldName);
+  }
+  
+  /**
+   * Expert: encodes category ordinals into a BytesRef. Override in case you use
+   * custom encoding, other than the default done by FacetsConfig.
+   */
+  protected BytesRef encode(IntsRef ordinals) {
+    return facetsConfig.dedupAndEncode(ordinals);
+  }
+  
+  /**
+   * Expert: override in case you used custom encoding for the categories under
+   * this field.
+   */
+  protected OrdinalsReader getOrdinalsReader(String field) {
+    return new DocValuesOrdinalsReader(field);
+  }
+  
+  @Override
+  public BinaryDocValues getBinaryDocValues(String field) throws IOException {
+    if (facetFields.contains(field)) {
+      final OrdinalsReader ordsReader = getOrdinalsReader(field);
+      return new OrdinalMappingBinaryDocValues(ordsReader.getReader(in.getContext()));
+    } else {
+      return in.getBinaryDocValues(field);
+    }
+  }
+  
+}
diff --git lucene/facet/src/java/org/apache/lucene/facet/taxonomy/OrdinalsReader.java lucene/facet/src/java/org/apache/lucene/facet/taxonomy/OrdinalsReader.java
index 098008e..985059e 100644
--- lucene/facet/src/java/org/apache/lucene/facet/taxonomy/OrdinalsReader.java
+++ lucene/facet/src/java/org/apache/lucene/facet/taxonomy/OrdinalsReader.java
@@ -19,7 +19,7 @@ package org.apache.lucene.facet.taxonomy;
 
 import java.io.IOException;
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.util.IntsRef;
 
 /** Provides per-document ordinals. */
@@ -42,7 +42,7 @@ public abstract class OrdinalsReader {
   }
 
   /** Set current atomic reader. */
-  public abstract OrdinalsSegmentReader getReader(AtomicReaderContext context) throws IOException;
+  public abstract OrdinalsSegmentReader getReader(LeafReaderContext context) throws IOException;
 
   /** Returns the indexed field name this {@code
    *  OrdinalsReader} is reading from. */
diff --git lucene/facet/src/java/org/apache/lucene/facet/taxonomy/TaxonomyFacetSumValueSource.java lucene/facet/src/java/org/apache/lucene/facet/taxonomy/TaxonomyFacetSumValueSource.java
index 6514a3c..a35660d 100644
--- lucene/facet/src/java/org/apache/lucene/facet/taxonomy/TaxonomyFacetSumValueSource.java
+++ lucene/facet/src/java/org/apache/lucene/facet/taxonomy/TaxonomyFacetSumValueSource.java
@@ -26,7 +26,7 @@ import java.util.Map;
 import org.apache.lucene.facet.FacetsCollector.MatchingDocs;
 import org.apache.lucene.facet.FacetsCollector;
 import org.apache.lucene.facet.FacetsConfig;
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.queries.function.docvalues.DoubleDocValues;
@@ -127,7 +127,7 @@ public class TaxonomyFacetSumValueSource extends FloatTaxonomyFacets {
     }
 
     @Override
-    public FunctionValues getValues(@SuppressWarnings("rawtypes") Map context, AtomicReaderContext readerContext) throws IOException {
+    public FunctionValues getValues(@SuppressWarnings("rawtypes") Map context, LeafReaderContext readerContext) throws IOException {
       final Scorer scorer = (Scorer) context.get("scorer");
       if (scorer == null) {
         throw new IllegalStateException("scores are missing; be sure to pass keepScores=true to FacetsCollector");
diff --git lucene/facet/src/java/org/apache/lucene/facet/taxonomy/TaxonomyMergeUtils.java lucene/facet/src/java/org/apache/lucene/facet/taxonomy/TaxonomyMergeUtils.java
index b2c82df..bdd11e7 100644
--- lucene/facet/src/java/org/apache/lucene/facet/taxonomy/TaxonomyMergeUtils.java
+++ lucene/facet/src/java/org/apache/lucene/facet/taxonomy/TaxonomyMergeUtils.java
@@ -6,8 +6,8 @@ import java.util.List;
 import org.apache.lucene.facet.FacetsConfig;
 import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
 import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter.OrdinalMap;
-import org.apache.lucene.index.AtomicReader;
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReader;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexWriter;
 import org.apache.lucene.index.MultiReader;
@@ -50,11 +50,11 @@ public abstract class TaxonomyMergeUtils {
     int ordinalMap[] = map.getMap();
     DirectoryReader reader = DirectoryReader.open(srcIndexDir);
     try {
-      List<AtomicReaderContext> leaves = reader.leaves();
+      List<LeafReaderContext> leaves = reader.leaves();
       int numReaders = leaves.size();
-      AtomicReader wrappedLeaves[] = new AtomicReader[numReaders];
+      LeafReader wrappedLeaves[] = new LeafReader[numReaders];
       for (int i = 0; i < numReaders; i++) {
-        wrappedLeaves[i] = new OrdinalMappingAtomicReader(leaves.get(i).reader(), ordinalMap, srcConfig);
+        wrappedLeaves[i] = new OrdinalMappingLeafReader(leaves.get(i).reader(), ordinalMap, srcConfig);
       }
       destIndexWriter.addIndexes(new MultiReader(wrappedLeaves));
       
diff --git lucene/facet/src/java/org/apache/lucene/facet/taxonomy/directory/DirectoryTaxonomyWriter.java lucene/facet/src/java/org/apache/lucene/facet/taxonomy/directory/DirectoryTaxonomyWriter.java
index 2bde43d..2ed6288 100644
--- lucene/facet/src/java/org/apache/lucene/facet/taxonomy/directory/DirectoryTaxonomyWriter.java
+++ lucene/facet/src/java/org/apache/lucene/facet/taxonomy/directory/DirectoryTaxonomyWriter.java
@@ -26,8 +26,8 @@ import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
 import org.apache.lucene.facet.taxonomy.writercache.TaxonomyWriterCache;
 import org.apache.lucene.facet.taxonomy.writercache.Cl2oTaxonomyWriterCache;
 import org.apache.lucene.facet.taxonomy.writercache.LruTaxonomyWriterCache;
-import org.apache.lucene.index.AtomicReader;
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReader;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.CorruptIndexException; // javadocs
 import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.DocsEnum;
@@ -405,7 +405,7 @@ public class DirectoryTaxonomyWriter implements TaxonomyWriter {
       final BytesRef catTerm = new BytesRef(FacetsConfig.pathToString(categoryPath.components, categoryPath.length));
       TermsEnum termsEnum = null; // reuse
       DocsEnum docs = null; // reuse
-      for (AtomicReaderContext ctx : reader.leaves()) {
+      for (LeafReaderContext ctx : reader.leaves()) {
         Terms terms = ctx.reader().terms(Consts.FULL);
         if (terms != null) {
           termsEnum = terms.iterator(termsEnum);
@@ -698,7 +698,7 @@ public class DirectoryTaxonomyWriter implements TaxonomyWriter {
     try {
       TermsEnum termsEnum = null;
       DocsEnum docsEnum = null;
-      for (AtomicReaderContext ctx : reader.leaves()) {
+      for (LeafReaderContext ctx : reader.leaves()) {
         Terms terms = ctx.reader().terms(Consts.FULL);
         if (terms != null) { // cannot really happen, but be on the safe side
           termsEnum = terms.iterator(termsEnum);
@@ -794,8 +794,8 @@ public class DirectoryTaxonomyWriter implements TaxonomyWriter {
       int base = 0;
       TermsEnum te = null;
       DocsEnum docs = null;
-      for (final AtomicReaderContext ctx : r.leaves()) {
-        final AtomicReader ar = ctx.reader();
+      for (final LeafReaderContext ctx : r.leaves()) {
+        final LeafReader ar = ctx.reader();
         final Terms terms = ar.terms(Consts.FULL);
         te = terms.iterator(te);
         while (te.next() != null) {
diff --git lucene/facet/src/test/org/apache/lucene/facet/TestDrillSideways.java lucene/facet/src/test/org/apache/lucene/facet/TestDrillSideways.java
index f2b34bb..4339514 100644
--- lucene/facet/src/test/org/apache/lucene/facet/TestDrillSideways.java
+++ lucene/facet/src/test/org/apache/lucene/facet/TestDrillSideways.java
@@ -39,7 +39,7 @@ import org.apache.lucene.facet.sortedset.SortedSetDocValuesReaderState;
 import org.apache.lucene.facet.taxonomy.TaxonomyReader;
 import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
 import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexWriterConfig;
 import org.apache.lucene.index.RandomIndexWriter;
@@ -648,7 +648,7 @@ public class TestDrillSideways extends FacetTestCase {
         }
         filter = new Filter() {
             @Override
-            public DocIdSet getDocIdSet(AtomicReaderContext context, Bits acceptDocs) throws IOException {
+            public DocIdSet getDocIdSet(LeafReaderContext context, Bits acceptDocs) throws IOException {
               int maxDoc = context.reader().maxDoc();
               final FixedBitSet bits = new FixedBitSet(maxDoc);
               for(int docID=0;docID < maxDoc;docID++) {
@@ -678,7 +678,7 @@ public class TestDrillSideways extends FacetTestCase {
                              }
 
                              @Override
-                             protected void doSetNextReader(AtomicReaderContext context) throws IOException {
+                             protected void doSetNextReader(LeafReaderContext context) throws IOException {
                                lastDocID = -1;
                              }
 
diff --git lucene/facet/src/test/org/apache/lucene/facet/TestMultipleIndexFields.java lucene/facet/src/test/org/apache/lucene/facet/TestMultipleIndexFields.java
index df9242d..97528cd 100644
--- lucene/facet/src/test/org/apache/lucene/facet/TestMultipleIndexFields.java
+++ lucene/facet/src/test/org/apache/lucene/facet/TestMultipleIndexFields.java
@@ -30,8 +30,8 @@ import org.apache.lucene.facet.taxonomy.TaxonomyReader;
 import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
 import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
 import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
-import org.apache.lucene.index.AtomicReader;
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReader;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexWriterConfig.OpenMode;
 import org.apache.lucene.index.RandomIndexWriter;
@@ -180,8 +180,8 @@ public class TestMultipleIndexFields extends FacetTestCase {
   }
 
   private void assertOrdinalsExist(String field, IndexReader ir) throws IOException {
-    for (AtomicReaderContext context : ir.leaves()) {
-      AtomicReader r = context.reader();
+    for (LeafReaderContext context : ir.leaves()) {
+      LeafReader r = context.reader();
       if (r.getBinaryDocValues(field) != null) {
         return; // not all segments must have this DocValues
       }
diff --git lucene/facet/src/test/org/apache/lucene/facet/range/TestRangeFacetCounts.java lucene/facet/src/test/org/apache/lucene/facet/range/TestRangeFacetCounts.java
index 7f00aad..02914ac 100644
--- lucene/facet/src/test/org/apache/lucene/facet/range/TestRangeFacetCounts.java
+++ lucene/facet/src/test/org/apache/lucene/facet/range/TestRangeFacetCounts.java
@@ -44,8 +44,8 @@ import org.apache.lucene.facet.MultiFacets;
 import org.apache.lucene.facet.taxonomy.TaxonomyReader;
 import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
 import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
-import org.apache.lucene.index.AtomicReader;
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReader;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexWriterConfig;
 import org.apache.lucene.index.RandomIndexWriter;
@@ -874,7 +874,7 @@ public class TestRangeFacetCounts extends FacetTestCase {
     final ValueSource vs = new ValueSource() {
         @SuppressWarnings("rawtypes")
         @Override
-        public FunctionValues getValues(Map ignored, AtomicReaderContext ignored2) {
+        public FunctionValues getValues(Map ignored, LeafReaderContext ignored2) {
           return new DoubleDocValues(null) {
             @Override
             public double doubleVal(int doc) {
@@ -921,7 +921,7 @@ public class TestRangeFacetCounts extends FacetTestCase {
       // Sort of silly:
       fastMatchFilter = new CachingWrapperFilter(new QueryWrapperFilter(new MatchAllDocsQuery())) {
           @Override
-          protected DocIdSet cacheImpl(DocIdSetIterator iterator, AtomicReader reader)
+          protected DocIdSet cacheImpl(DocIdSetIterator iterator, LeafReader reader)
             throws IOException {
             final FixedBitSet cached = new FixedBitSet(reader.maxDoc());
             filterWasUsed.set(true);
diff --git lucene/facet/src/test/org/apache/lucene/facet/taxonomy/TestCachedOrdinalsReader.java lucene/facet/src/test/org/apache/lucene/facet/taxonomy/TestCachedOrdinalsReader.java
index 3749a23..849eebb 100644
--- lucene/facet/src/test/org/apache/lucene/facet/taxonomy/TestCachedOrdinalsReader.java
+++ lucene/facet/src/test/org/apache/lucene/facet/taxonomy/TestCachedOrdinalsReader.java
@@ -25,7 +25,7 @@ import org.apache.lucene.facet.FacetField;
 import org.apache.lucene.facet.FacetTestCase;
 import org.apache.lucene.facet.FacetsConfig;
 import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexWriter;
 import org.apache.lucene.index.IndexWriterConfig;
@@ -59,7 +59,7 @@ public class TestCachedOrdinalsReader extends FacetTestCase {
       threads[i] = new Thread("CachedOrdsThread-" + i) {
         @Override
         public void run() {
-          for (AtomicReaderContext context : reader.leaves()) {
+          for (LeafReaderContext context : reader.leaves()) {
             try {
               ordsReader.getReader(context);
             } catch (IOException e) {
diff --git lucene/facet/src/test/org/apache/lucene/facet/taxonomy/TestTaxonomyFacetSumValueSource.java lucene/facet/src/test/org/apache/lucene/facet/taxonomy/TestTaxonomyFacetSumValueSource.java
index 9c32685..da5d503 100644
--- lucene/facet/src/test/org/apache/lucene/facet/taxonomy/TestTaxonomyFacetSumValueSource.java
+++ lucene/facet/src/test/org/apache/lucene/facet/taxonomy/TestTaxonomyFacetSumValueSource.java
@@ -39,7 +39,7 @@ import org.apache.lucene.facet.FacetsConfig;
 import org.apache.lucene.facet.LabelAndValue;
 import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
 import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexWriter;
 import org.apache.lucene.index.IndexWriterConfig;
@@ -329,7 +329,7 @@ public class TestTaxonomyFacetSumValueSource extends FacetTestCase {
 
     ValueSource valueSource = new ValueSource() {
       @Override
-      public FunctionValues getValues(@SuppressWarnings("rawtypes") Map context, AtomicReaderContext readerContext) throws IOException {
+      public FunctionValues getValues(@SuppressWarnings("rawtypes") Map context, LeafReaderContext readerContext) throws IOException {
         final Scorer scorer = (Scorer) context.get("scorer");
         assert scorer != null;
         return new DoubleDocValues(this) {
diff --git lucene/grouping/src/java/org/apache/lucene/search/grouping/AbstractFirstPassGroupingCollector.java lucene/grouping/src/java/org/apache/lucene/search/grouping/AbstractFirstPassGroupingCollector.java
index 0c342ec..ce64abd 100644
--- lucene/grouping/src/java/org/apache/lucene/search/grouping/AbstractFirstPassGroupingCollector.java
+++ lucene/grouping/src/java/org/apache/lucene/search/grouping/AbstractFirstPassGroupingCollector.java
@@ -17,7 +17,7 @@ package org.apache.lucene.search.grouping;
  * limitations under the License.
  */
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.search.*;
 
 import java.io.IOException;
@@ -326,7 +326,7 @@ abstract public class AbstractFirstPassGroupingCollector<GROUP_VALUE_TYPE> exten
   }
 
   @Override
-  protected void doSetNextReader(AtomicReaderContext readerContext) throws IOException {
+  protected void doSetNextReader(LeafReaderContext readerContext) throws IOException {
     docBase = readerContext.docBase;
     for (int i=0; i<comparators.length; i++) {
       comparators[i] = comparators[i].setNextReader(readerContext);
diff --git lucene/grouping/src/java/org/apache/lucene/search/grouping/AbstractSecondPassGroupingCollector.java lucene/grouping/src/java/org/apache/lucene/search/grouping/AbstractSecondPassGroupingCollector.java
index aedfa9e..2dba7a9 100644
--- lucene/grouping/src/java/org/apache/lucene/search/grouping/AbstractSecondPassGroupingCollector.java
+++ lucene/grouping/src/java/org/apache/lucene/search/grouping/AbstractSecondPassGroupingCollector.java
@@ -17,7 +17,7 @@ package org.apache.lucene.search.grouping;
  * limitations under the License.
  */
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.search.*;
 
 import java.io.IOException;
@@ -107,7 +107,7 @@ public abstract class AbstractSecondPassGroupingCollector<GROUP_VALUE_TYPE> exte
   protected abstract SearchGroupDocs<GROUP_VALUE_TYPE> retrieveGroup(int doc) throws IOException;
 
   @Override
-  protected void doSetNextReader(AtomicReaderContext readerContext) throws IOException {
+  protected void doSetNextReader(LeafReaderContext readerContext) throws IOException {
     //System.out.println("SP.setNextReader");
     for (SearchGroupDocs<GROUP_VALUE_TYPE> group : groupMap.values()) {
       group.collector.getLeafCollector(readerContext);
diff --git lucene/grouping/src/java/org/apache/lucene/search/grouping/BlockGroupingCollector.java lucene/grouping/src/java/org/apache/lucene/search/grouping/BlockGroupingCollector.java
index f40c2a7..503ffbe 100644
--- lucene/grouping/src/java/org/apache/lucene/search/grouping/BlockGroupingCollector.java
+++ lucene/grouping/src/java/org/apache/lucene/search/grouping/BlockGroupingCollector.java
@@ -21,7 +21,7 @@ package org.apache.lucene.search.grouping;
 import java.io.IOException;
 import java.util.Collection;
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.IndexWriter;
 import org.apache.lucene.search.*;
 import org.apache.lucene.util.ArrayUtil;
@@ -73,7 +73,7 @@ public class BlockGroupingCollector extends SimpleCollector {
   private final int compIDXEnd;
   private int bottomSlot;
   private boolean queueFull;
-  private AtomicReaderContext currentReaderContext;
+  private LeafReaderContext currentReaderContext;
 
   private int topGroupDoc;
   private int totalHitCount;
@@ -136,7 +136,7 @@ public class BlockGroupingCollector extends SimpleCollector {
   }
 
   private static final class OneGroup {
-    AtomicReaderContext readerContext;
+    LeafReaderContext readerContext;
     //int groupOrd;
     int topGroupDoc;
     int[] docs;
@@ -516,7 +516,7 @@ public class BlockGroupingCollector extends SimpleCollector {
   }
 
   @Override
-  protected void doSetNextReader(AtomicReaderContext readerContext) throws IOException {
+  protected void doSetNextReader(LeafReaderContext readerContext) throws IOException {
     if (subDocUpto != 0) {
       processGroup();
     }
diff --git lucene/grouping/src/java/org/apache/lucene/search/grouping/function/FunctionAllGroupHeadsCollector.java lucene/grouping/src/java/org/apache/lucene/search/grouping/function/FunctionAllGroupHeadsCollector.java
index 64ad845..771660d 100644
--- lucene/grouping/src/java/org/apache/lucene/search/grouping/function/FunctionAllGroupHeadsCollector.java
+++ lucene/grouping/src/java/org/apache/lucene/search/grouping/function/FunctionAllGroupHeadsCollector.java
@@ -17,10 +17,9 @@ package org.apache.lucene.search.grouping.function;
  * limitations under the License.
  */
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
-import org.apache.lucene.search.LeafCollector;
 import org.apache.lucene.search.FieldComparator;
 import org.apache.lucene.search.Scorer;
 import org.apache.lucene.search.Sort;
@@ -48,7 +47,7 @@ public class FunctionAllGroupHeadsCollector extends AbstractAllGroupHeadsCollect
 
   private FunctionValues.ValueFiller filler;
   private MutableValue mval;
-  private AtomicReaderContext readerContext;
+  private LeafReaderContext readerContext;
   private Scorer scorer;
 
   /**
@@ -102,7 +101,7 @@ public class FunctionAllGroupHeadsCollector extends AbstractAllGroupHeadsCollect
   }
 
   @Override
-  protected void doSetNextReader(AtomicReaderContext context) throws IOException {
+  protected void doSetNextReader(LeafReaderContext context) throws IOException {
     this.readerContext = context;
     FunctionValues values = groupBy.getValues(vsContext, context);
     filler = values.getValueFiller();
diff --git lucene/grouping/src/java/org/apache/lucene/search/grouping/function/FunctionAllGroupsCollector.java lucene/grouping/src/java/org/apache/lucene/search/grouping/function/FunctionAllGroupsCollector.java
index d949bec..c9c6111 100644
--- lucene/grouping/src/java/org/apache/lucene/search/grouping/function/FunctionAllGroupsCollector.java
+++ lucene/grouping/src/java/org/apache/lucene/search/grouping/function/FunctionAllGroupsCollector.java
@@ -17,10 +17,9 @@ package org.apache.lucene.search.grouping.function;
  * limitations under the License.
  */
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
-import org.apache.lucene.search.LeafCollector;
 import org.apache.lucene.search.grouping.AbstractAllGroupsCollector;
 import org.apache.lucene.util.mutable.MutableValue;
 
@@ -76,7 +75,7 @@ public class FunctionAllGroupsCollector extends AbstractAllGroupsCollector<Mutab
   }
 
   @Override
-  protected void doSetNextReader(AtomicReaderContext context) throws IOException {
+  protected void doSetNextReader(LeafReaderContext context) throws IOException {
     FunctionValues values = groupBy.getValues(vsContext, context);
     filler = values.getValueFiller();
     mval = filler.getValue();
diff --git lucene/grouping/src/java/org/apache/lucene/search/grouping/function/FunctionDistinctValuesCollector.java lucene/grouping/src/java/org/apache/lucene/search/grouping/function/FunctionDistinctValuesCollector.java
index 597a196..d8025a9 100644
--- lucene/grouping/src/java/org/apache/lucene/search/grouping/function/FunctionDistinctValuesCollector.java
+++ lucene/grouping/src/java/org/apache/lucene/search/grouping/function/FunctionDistinctValuesCollector.java
@@ -17,10 +17,9 @@ package org.apache.lucene.search.grouping.function;
  * limitations under the License.
  */
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
-import org.apache.lucene.search.LeafCollector;
 import org.apache.lucene.search.grouping.AbstractDistinctValuesCollector;
 import org.apache.lucene.search.grouping.SearchGroup;
 import org.apache.lucene.util.mutable.MutableValue;
@@ -71,7 +70,7 @@ public class FunctionDistinctValuesCollector extends AbstractDistinctValuesColle
   }
 
   @Override
-  protected void doSetNextReader(AtomicReaderContext context) throws IOException {
+  protected void doSetNextReader(LeafReaderContext context) throws IOException {
     FunctionValues values = groupSource.getValues(vsContext, context);
     groupFiller = values.getValueFiller();
     groupMval = groupFiller.getValue();
diff --git lucene/grouping/src/java/org/apache/lucene/search/grouping/function/FunctionFirstPassGroupingCollector.java lucene/grouping/src/java/org/apache/lucene/search/grouping/function/FunctionFirstPassGroupingCollector.java
index b9737e1..73866fb 100644
--- lucene/grouping/src/java/org/apache/lucene/search/grouping/function/FunctionFirstPassGroupingCollector.java
+++ lucene/grouping/src/java/org/apache/lucene/search/grouping/function/FunctionFirstPassGroupingCollector.java
@@ -17,10 +17,9 @@ package org.apache.lucene.search.grouping.function;
  * limitations under the License.
  */
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
-import org.apache.lucene.search.LeafCollector;
 import org.apache.lucene.search.Sort;
 import org.apache.lucene.search.grouping.AbstractFirstPassGroupingCollector;
 import org.apache.lucene.util.mutable.MutableValue;
@@ -78,7 +77,7 @@ public class FunctionFirstPassGroupingCollector extends AbstractFirstPassGroupin
   }
 
   @Override
-  protected void doSetNextReader(AtomicReaderContext readerContext) throws IOException {
+  protected void doSetNextReader(LeafReaderContext readerContext) throws IOException {
     super.doSetNextReader(readerContext);
     FunctionValues values = groupByVS.getValues(vsContext, readerContext);
     filler = values.getValueFiller();
diff --git lucene/grouping/src/java/org/apache/lucene/search/grouping/function/FunctionSecondPassGroupingCollector.java lucene/grouping/src/java/org/apache/lucene/search/grouping/function/FunctionSecondPassGroupingCollector.java
index 9df094b..a5e0db3 100644
--- lucene/grouping/src/java/org/apache/lucene/search/grouping/function/FunctionSecondPassGroupingCollector.java
+++ lucene/grouping/src/java/org/apache/lucene/search/grouping/function/FunctionSecondPassGroupingCollector.java
@@ -17,10 +17,9 @@ package org.apache.lucene.search.grouping.function;
  * limitations under the License.
  */
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
-import org.apache.lucene.search.LeafCollector;
 import org.apache.lucene.search.Sort;
 import org.apache.lucene.search.grouping.AbstractSecondPassGroupingCollector;
 import org.apache.lucene.search.grouping.SearchGroup;
@@ -72,7 +71,7 @@ public class FunctionSecondPassGroupingCollector extends AbstractSecondPassGroup
   }
 
   @Override
-  protected void doSetNextReader(AtomicReaderContext readerContext) throws IOException {
+  protected void doSetNextReader(LeafReaderContext readerContext) throws IOException {
     super.doSetNextReader(readerContext);
     FunctionValues values = groupByVS.getValues(vsContext, readerContext);
     filler = values.getValueFiller();
diff --git lucene/grouping/src/java/org/apache/lucene/search/grouping/term/TermAllGroupHeadsCollector.java lucene/grouping/src/java/org/apache/lucene/search/grouping/term/TermAllGroupHeadsCollector.java
index 1994aa5..df0c0bb 100644
--- lucene/grouping/src/java/org/apache/lucene/search/grouping/term/TermAllGroupHeadsCollector.java
+++ lucene/grouping/src/java/org/apache/lucene/search/grouping/term/TermAllGroupHeadsCollector.java
@@ -17,7 +17,7 @@ package org.apache.lucene.search.grouping.term;
  * limitations under the License.
  */
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.DocValues;
 import org.apache.lucene.index.SortedDocValues;
 import org.apache.lucene.search.FieldComparator;
@@ -50,7 +50,7 @@ public abstract class TermAllGroupHeadsCollector<GH extends AbstractAllGroupHead
   final String groupField;
 
   SortedDocValues groupIndex;
-  AtomicReaderContext readerContext;
+  LeafReaderContext readerContext;
 
   protected TermAllGroupHeadsCollector(String groupField, int numberOfSorts) {
     super(numberOfSorts);
@@ -158,7 +158,7 @@ public abstract class TermAllGroupHeadsCollector<GH extends AbstractAllGroupHead
     }
 
     @Override
-    protected void doSetNextReader(AtomicReaderContext context) throws IOException {
+    protected void doSetNextReader(LeafReaderContext context) throws IOException {
       this.readerContext = context;
       groupIndex = DocValues.getSorted(context.reader(), groupField);
 
@@ -271,7 +271,7 @@ public abstract class TermAllGroupHeadsCollector<GH extends AbstractAllGroupHead
     }
 
     @Override
-    protected void doSetNextReader(AtomicReaderContext context) throws IOException {
+    protected void doSetNextReader(LeafReaderContext context) throws IOException {
       this.readerContext = context;
       groupIndex = DocValues.getSorted(context.reader(), groupField);
       for (int i = 0; i < fields.length; i++) {
@@ -430,7 +430,7 @@ public abstract class TermAllGroupHeadsCollector<GH extends AbstractAllGroupHead
     }
 
     @Override
-    protected void doSetNextReader(AtomicReaderContext context) throws IOException {
+    protected void doSetNextReader(LeafReaderContext context) throws IOException {
       this.readerContext = context;
       groupIndex = DocValues.getSorted(context.reader(), groupField);
       for (int i = 0; i < fields.length; i++) {
@@ -562,7 +562,7 @@ public abstract class TermAllGroupHeadsCollector<GH extends AbstractAllGroupHead
     }
 
     @Override
-    protected void doSetNextReader(AtomicReaderContext context) throws IOException {
+    protected void doSetNextReader(LeafReaderContext context) throws IOException {
       this.readerContext = context;
       groupIndex = DocValues.getSorted(context.reader(), groupField);
 
diff --git lucene/grouping/src/java/org/apache/lucene/search/grouping/term/TermAllGroupsCollector.java lucene/grouping/src/java/org/apache/lucene/search/grouping/term/TermAllGroupsCollector.java
index 76c4652..dcb43ad 100644
--- lucene/grouping/src/java/org/apache/lucene/search/grouping/term/TermAllGroupsCollector.java
+++ lucene/grouping/src/java/org/apache/lucene/search/grouping/term/TermAllGroupsCollector.java
@@ -17,7 +17,7 @@ package org.apache.lucene.search.grouping.term;
  * limitations under the License.
  */
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.DocValues;
 import org.apache.lucene.index.SortedDocValues;
 import org.apache.lucene.search.grouping.AbstractAllGroupsCollector;
@@ -102,7 +102,7 @@ public class TermAllGroupsCollector extends AbstractAllGroupsCollector<BytesRef>
   }
 
   @Override
-  protected void doSetNextReader(AtomicReaderContext context) throws IOException {
+  protected void doSetNextReader(LeafReaderContext context) throws IOException {
     index = DocValues.getSorted(context.reader(), groupField);
 
     // Clear ordSet and fill it with previous encountered groups that can occur in the current segment.
diff --git lucene/grouping/src/java/org/apache/lucene/search/grouping/term/TermDistinctValuesCollector.java lucene/grouping/src/java/org/apache/lucene/search/grouping/term/TermDistinctValuesCollector.java
index f997ef3..87e1c27 100644
--- lucene/grouping/src/java/org/apache/lucene/search/grouping/term/TermDistinctValuesCollector.java
+++ lucene/grouping/src/java/org/apache/lucene/search/grouping/term/TermDistinctValuesCollector.java
@@ -17,7 +17,7 @@ package org.apache.lucene.search.grouping.term;
  * limitations under the License.
  */
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.DocValues;
 import org.apache.lucene.index.SortedDocValues;
 import org.apache.lucene.search.grouping.AbstractDistinctValuesCollector;
@@ -106,7 +106,7 @@ public class TermDistinctValuesCollector extends AbstractDistinctValuesCollector
   }
 
   @Override
-  protected void doSetNextReader(AtomicReaderContext context) throws IOException {
+  protected void doSetNextReader(LeafReaderContext context) throws IOException {
     groupFieldTermIndex = DocValues.getSorted(context.reader(), groupField);
     countFieldTermIndex = DocValues.getSorted(context.reader(), countField);
     ordSet.clear();
diff --git lucene/grouping/src/java/org/apache/lucene/search/grouping/term/TermFirstPassGroupingCollector.java lucene/grouping/src/java/org/apache/lucene/search/grouping/term/TermFirstPassGroupingCollector.java
index b394fb9..53d0739 100644
--- lucene/grouping/src/java/org/apache/lucene/search/grouping/term/TermFirstPassGroupingCollector.java
+++ lucene/grouping/src/java/org/apache/lucene/search/grouping/term/TermFirstPassGroupingCollector.java
@@ -19,7 +19,7 @@ package org.apache.lucene.search.grouping.term;
 
 import java.io.IOException;
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.DocValues;
 import org.apache.lucene.index.SortedDocValues;
 import org.apache.lucene.search.Sort;
@@ -87,7 +87,7 @@ public class TermFirstPassGroupingCollector extends AbstractFirstPassGroupingCol
   }
 
   @Override
-  protected void doSetNextReader(AtomicReaderContext readerContext) throws IOException {
+  protected void doSetNextReader(LeafReaderContext readerContext) throws IOException {
     super.doSetNextReader(readerContext);
     index = DocValues.getSorted(readerContext.reader(), groupField);
   }
diff --git lucene/grouping/src/java/org/apache/lucene/search/grouping/term/TermGroupFacetCollector.java lucene/grouping/src/java/org/apache/lucene/search/grouping/term/TermGroupFacetCollector.java
index 97c96bc..7a78c1e 100644
--- lucene/grouping/src/java/org/apache/lucene/search/grouping/term/TermGroupFacetCollector.java
+++ lucene/grouping/src/java/org/apache/lucene/search/grouping/term/TermGroupFacetCollector.java
@@ -17,7 +17,7 @@ package org.apache.lucene.search.grouping.term;
  * limitations under the License.
  */
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.DocValues;
 import org.apache.lucene.index.SortedDocValues;
 import org.apache.lucene.index.SortedSetDocValues;
@@ -121,7 +121,7 @@ public abstract class TermGroupFacetCollector extends AbstractGroupFacetCollecto
     }
 
     @Override
-    protected void doSetNextReader(AtomicReaderContext context) throws IOException {
+    protected void doSetNextReader(LeafReaderContext context) throws IOException {
       if (segmentFacetCounts != null) {
         segmentResults.add(createSegmentResult());
       }
@@ -273,7 +273,7 @@ public abstract class TermGroupFacetCollector extends AbstractGroupFacetCollecto
     }
 
     @Override
-    protected void doSetNextReader(AtomicReaderContext context) throws IOException {
+    protected void doSetNextReader(LeafReaderContext context) throws IOException {
       if (segmentFacetCounts != null) {
         segmentResults.add(createSegmentResult());
       }
diff --git lucene/grouping/src/java/org/apache/lucene/search/grouping/term/TermSecondPassGroupingCollector.java lucene/grouping/src/java/org/apache/lucene/search/grouping/term/TermSecondPassGroupingCollector.java
index 043bd5f..a500fe7 100644
--- lucene/grouping/src/java/org/apache/lucene/search/grouping/term/TermSecondPassGroupingCollector.java
+++ lucene/grouping/src/java/org/apache/lucene/search/grouping/term/TermSecondPassGroupingCollector.java
@@ -20,7 +20,7 @@ package org.apache.lucene.search.grouping.term;
 import java.io.IOException;
 import java.util.Collection;
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.DocValues;
 import org.apache.lucene.index.SortedDocValues;
 import org.apache.lucene.search.Sort;
@@ -53,7 +53,7 @@ public class TermSecondPassGroupingCollector extends AbstractSecondPassGroupingC
   }
 
   @Override
-  protected void doSetNextReader(AtomicReaderContext readerContext) throws IOException {
+  protected void doSetNextReader(LeafReaderContext readerContext) throws IOException {
     super.doSetNextReader(readerContext);
     index = DocValues.getSorted(readerContext.reader(), groupField);
 
diff --git lucene/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping.java lucene/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping.java
index ebaafb1..f5942b0 100644
--- lucene/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping.java
+++ lucene/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping.java
@@ -19,7 +19,7 @@ package org.apache.lucene.search.grouping;
 
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.document.*;
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.MultiDocValues;
 import org.apache.lucene.index.NumericDocValues;
@@ -602,7 +602,7 @@ public class TestGrouping extends LuceneTestCase {
 
     public ShardState(IndexSearcher s) {
       final IndexReaderContext ctx = s.getTopReaderContext();
-      final List<AtomicReaderContext> leaves = ctx.leaves();
+      final List<LeafReaderContext> leaves = ctx.leaves();
       subSearchers = new ShardSearcher[leaves.size()];
       for(int searcherIDX=0;searcherIDX<subSearchers.length;searcherIDX++) {
         subSearchers[searcherIDX] = new ShardSearcher(leaves.get(searcherIDX), ctx);
@@ -1294,9 +1294,9 @@ public class TestGrouping extends LuceneTestCase {
   }
 
   private static class ShardSearcher extends IndexSearcher {
-    private final List<AtomicReaderContext> ctx;
+    private final List<LeafReaderContext> ctx;
 
-    public ShardSearcher(AtomicReaderContext ctx, IndexReaderContext parent) {
+    public ShardSearcher(LeafReaderContext ctx, IndexReaderContext parent) {
       super(parent);
       this.ctx = Collections.singletonList(ctx);
     }
diff --git lucene/highlighter/src/java/org/apache/lucene/search/highlight/WeightedSpanTermExtractor.java lucene/highlighter/src/java/org/apache/lucene/search/highlight/WeightedSpanTermExtractor.java
index f38f74d..e4b0004 100644
--- lucene/highlighter/src/java/org/apache/lucene/search/highlight/WeightedSpanTermExtractor.java
+++ lucene/highlighter/src/java/org/apache/lucene/search/highlight/WeightedSpanTermExtractor.java
@@ -29,12 +29,12 @@ import java.util.TreeSet;
 
 import org.apache.lucene.analysis.CachingTokenFilter;
 import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.index.AtomicReader;
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.FilterLeafReader;
+import org.apache.lucene.index.LeafReader;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.BinaryDocValues;
 import org.apache.lucene.index.FieldInfos;
 import org.apache.lucene.index.Fields;
-import org.apache.lucene.index.FilterAtomicReader;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.NumericDocValues;
 import org.apache.lucene.index.SortedDocValues;
@@ -69,7 +69,7 @@ public class WeightedSpanTermExtractor {
   private boolean cachedTokenStream;
   private boolean wrapToCaching = true;
   private int maxDocCharsToAnalyze;
-  private AtomicReader internalReader = null;
+  private LeafReader internalReader = null;
 
 
   public WeightedSpanTermExtractor() {
@@ -277,7 +277,7 @@ public class WeightedSpanTermExtractor {
       } else {
         q = spanQuery;
       }
-      AtomicReaderContext context = getLeafContext();
+      LeafReaderContext context = getLeafContext();
       Map<Term,TermContext> termContexts = new HashMap<>();
       TreeSet<Term> extractedTerms = new TreeSet<>();
       q.extractTerms(extractedTerms);
@@ -349,7 +349,7 @@ public class WeightedSpanTermExtractor {
     return rv;
   }
 
-  protected AtomicReaderContext getLeafContext() throws IOException {
+  protected LeafReaderContext getLeafContext() throws IOException {
     if (internalReader == null) {
       if(wrapToCaching && !(tokenStream instanceof CachingTokenFilter)) {
         assert !cachedTokenStream;
@@ -357,11 +357,11 @@ public class WeightedSpanTermExtractor {
         cachedTokenStream = true;
       }
       final MemoryIndex indexer = new MemoryIndex(true);
-      indexer.addField(DelegatingAtomicReader.FIELD_NAME, tokenStream);
+      indexer.addField(DelegatingLeafReader.FIELD_NAME, tokenStream);
       tokenStream.reset();
       final IndexSearcher searcher = indexer.createSearcher();
       // MEM index has only atomic ctx
-      internalReader = new DelegatingAtomicReader(((AtomicReaderContext)searcher.getTopReaderContext()).reader());
+      internalReader = new DelegatingLeafReader(((LeafReaderContext)searcher.getTopReaderContext()).reader());
     }
     return internalReader.getContext();
   }
@@ -371,10 +371,10 @@ public class WeightedSpanTermExtractor {
    * AtomicReader. This way we only need to build this field once rather than
    * N-Times
    */
-  static final class DelegatingAtomicReader extends FilterAtomicReader {
+  static final class DelegatingLeafReader extends FilterLeafReader {
     private static final String FIELD_NAME = "shadowed_field";
 
-    DelegatingAtomicReader(AtomicReader in) {
+    DelegatingLeafReader(LeafReader in) {
       super(in);
     }
     
@@ -388,12 +388,12 @@ public class WeightedSpanTermExtractor {
       return new FilterFields(super.fields()) {
         @Override
         public Terms terms(String field) throws IOException {
-          return super.terms(DelegatingAtomicReader.FIELD_NAME);
+          return super.terms(DelegatingLeafReader.FIELD_NAME);
         }
 
         @Override
         public Iterator<String> iterator() {
-          return Collections.singletonList(DelegatingAtomicReader.FIELD_NAME).iterator();
+          return Collections.singletonList(DelegatingLeafReader.FIELD_NAME).iterator();
         }
 
         @Override
diff --git lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/PostingsHighlighter.java lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/PostingsHighlighter.java
index e55ae36..4e6731d 100644
--- lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/PostingsHighlighter.java
+++ lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/PostingsHighlighter.java
@@ -31,8 +31,8 @@ import java.util.SortedSet;
 import java.util.TreeSet;
 
 import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.index.AtomicReader;
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReader;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.DocsAndPositionsEnum;
 import org.apache.lucene.index.FieldInfo;
 import org.apache.lucene.index.FieldInfo.IndexOptions;
@@ -350,7 +350,7 @@ public class PostingsHighlighter {
     rewritten.extractTerms(queryTerms);
 
     IndexReaderContext readerContext = reader.getContext();
-    List<AtomicReaderContext> leaves = readerContext.leaves();
+    List<LeafReaderContext> leaves = readerContext.leaves();
 
     // Make our own copies because we sort in-place:
     int[] docids = new int[docidsIn.length];
@@ -453,7 +453,7 @@ public class PostingsHighlighter {
     return null;
   }
     
-  private Map<Integer,Object> highlightField(String field, String contents[], BreakIterator bi, BytesRef terms[], int[] docids, List<AtomicReaderContext> leaves, int maxPassages, Query query) throws IOException {  
+  private Map<Integer,Object> highlightField(String field, String contents[], BreakIterator bi, BytesRef terms[], int[] docids, List<LeafReaderContext> leaves, int maxPassages, Query query) throws IOException {  
     Map<Integer,Object> highlights = new HashMap<>();
     
     PassageFormatter fieldFormatter = getFormatter(field);
@@ -489,8 +489,8 @@ public class PostingsHighlighter {
       bi.setText(content);
       int doc = docids[i];
       int leaf = ReaderUtil.subIndex(doc, leaves);
-      AtomicReaderContext subContext = leaves.get(leaf);
-      AtomicReader r = subContext.reader();
+      LeafReaderContext subContext = leaves.get(leaf);
+      LeafReader r = subContext.reader();
       
       assert leaf >= lastLeaf; // increasing order
       
diff --git lucene/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterPhraseTest.java lucene/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterPhraseTest.java
index 7a2c6d3..7bfee20 100644
--- lucene/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterPhraseTest.java
+++ lucene/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterPhraseTest.java
@@ -30,7 +30,7 @@ import org.apache.lucene.document.FieldType;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.TextField;
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexWriter;
@@ -129,7 +129,7 @@ public class HighlighterPhraseTest extends LuceneTestCase {
         }
 
         @Override
-        protected void doSetNextReader(AtomicReaderContext context) throws IOException {
+        protected void doSetNextReader(LeafReaderContext context) throws IOException {
           this.baseDoc = context.docBase;
         }
 
diff --git lucene/highlighter/src/test/org/apache/lucene/search/vectorhighlight/FieldQueryTest.java lucene/highlighter/src/test/org/apache/lucene/search/vectorhighlight/FieldQueryTest.java
index a0a9233..690e42c 100644
--- lucene/highlighter/src/test/org/apache/lucene/search/vectorhighlight/FieldQueryTest.java
+++ lucene/highlighter/src/test/org/apache/lucene/search/vectorhighlight/FieldQueryTest.java
@@ -23,14 +23,13 @@ import java.util.List;
 import java.util.Map;
 import java.util.Set;
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.search.BooleanQuery;
 import org.apache.lucene.search.ConstantScoreQuery;
 import org.apache.lucene.search.DocIdSet;
 import org.apache.lucene.search.Filter;
 import org.apache.lucene.search.FilteredQuery;
-import org.apache.lucene.search.PhraseQuery;
 import org.apache.lucene.search.PrefixQuery;
 import org.apache.lucene.search.Query;
 import org.apache.lucene.search.BooleanClause.Occur;
@@ -940,7 +939,7 @@ public class FieldQueryTest extends AbstractTestCase {
     initBoost();
     Query query = new FilteredQuery(pqF( "A" ), new Filter() {
       @Override
-      public DocIdSet getDocIdSet(AtomicReaderContext context, Bits acceptDocs)
+      public DocIdSet getDocIdSet(LeafReaderContext context, Bits acceptDocs)
           throws IOException {
         return null;
       }
diff --git lucene/join/src/java/org/apache/lucene/search/join/FixedBitSetCachingWrapperFilter.java lucene/join/src/java/org/apache/lucene/search/join/FixedBitSetCachingWrapperFilter.java
index f36b1b4..1801e3a 100644
--- lucene/join/src/java/org/apache/lucene/search/join/FixedBitSetCachingWrapperFilter.java
+++ lucene/join/src/java/org/apache/lucene/search/join/FixedBitSetCachingWrapperFilter.java
@@ -21,7 +21,7 @@ import static org.apache.lucene.search.DocIdSet.EMPTY;
 
 import java.io.IOException;
 
-import org.apache.lucene.index.AtomicReader;
+import org.apache.lucene.index.LeafReader;
 import org.apache.lucene.search.CachingWrapperFilter;
 import org.apache.lucene.search.DocIdSet;
 import org.apache.lucene.search.DocIdSetIterator;
@@ -38,7 +38,7 @@ public final class FixedBitSetCachingWrapperFilter extends CachingWrapperFilter
   }
 
   @Override
-  protected DocIdSet docIdSetToCache(DocIdSet docIdSet, AtomicReader reader)
+  protected DocIdSet docIdSetToCache(DocIdSet docIdSet, LeafReader reader)
       throws IOException {
     if (docIdSet == null) {
       return EMPTY;
diff --git lucene/join/src/java/org/apache/lucene/search/join/TermsCollector.java lucene/join/src/java/org/apache/lucene/search/join/TermsCollector.java
index 996ba4e..3d85349 100644
--- lucene/join/src/java/org/apache/lucene/search/join/TermsCollector.java
+++ lucene/join/src/java/org/apache/lucene/search/join/TermsCollector.java
@@ -19,7 +19,7 @@ package org.apache.lucene.search.join;
 
 import java.io.IOException;
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.BinaryDocValues;
 import org.apache.lucene.index.DocValues;
 import org.apache.lucene.index.SortedSetDocValues;
@@ -81,7 +81,7 @@ abstract class TermsCollector extends SimpleCollector {
     }
 
     @Override
-    protected void doSetNextReader(AtomicReaderContext context) throws IOException {
+    protected void doSetNextReader(LeafReaderContext context) throws IOException {
       docTermOrds = DocValues.getSortedSet(context.reader(), field);
     }
   }
@@ -103,7 +103,7 @@ abstract class TermsCollector extends SimpleCollector {
     }
 
     @Override
-    protected void doSetNextReader(AtomicReaderContext context) throws IOException {
+    protected void doSetNextReader(LeafReaderContext context) throws IOException {
       fromDocTerms = DocValues.getBinary(context.reader(), field);
     }
   }
diff --git lucene/join/src/java/org/apache/lucene/search/join/TermsIncludingScoreQuery.java lucene/join/src/java/org/apache/lucene/search/join/TermsIncludingScoreQuery.java
index 220d0e1..fb1744b 100644
--- lucene/join/src/java/org/apache/lucene/search/join/TermsIncludingScoreQuery.java
+++ lucene/join/src/java/org/apache/lucene/search/join/TermsIncludingScoreQuery.java
@@ -21,14 +21,13 @@ import java.io.IOException;
 import java.util.Locale;
 import java.util.Set;
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.DocsEnum;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.index.Terms;
 import org.apache.lucene.index.TermsEnum;
 import org.apache.lucene.search.LeafCollector;
-import org.apache.lucene.search.Collector;
 import org.apache.lucene.search.ComplexExplanation;
 import org.apache.lucene.search.DocIdSetIterator;
 import org.apache.lucene.search.Explanation;
@@ -132,7 +131,7 @@ class TermsIncludingScoreQuery extends Query {
       private TermsEnum segmentTermsEnum;
 
       @Override
-      public Explanation explain(AtomicReaderContext context, int doc) throws IOException {
+      public Explanation explain(LeafReaderContext context, int doc) throws IOException {
         SVInnerScorer scorer = (SVInnerScorer) bulkScorer(context, false, null);
         if (scorer != null) {
           return scorer.explain(doc);
@@ -163,7 +162,7 @@ class TermsIncludingScoreQuery extends Query {
       }
 
       @Override
-      public Scorer scorer(AtomicReaderContext context, Bits acceptDocs) throws IOException {
+      public Scorer scorer(LeafReaderContext context, Bits acceptDocs) throws IOException {
         Terms terms = context.reader().terms(field);
         if (terms == null) {
           return null;
@@ -181,7 +180,7 @@ class TermsIncludingScoreQuery extends Query {
       }
 
       @Override
-      public BulkScorer bulkScorer(AtomicReaderContext context, boolean scoreDocsInOrder, Bits acceptDocs) throws IOException {
+      public BulkScorer bulkScorer(LeafReaderContext context, boolean scoreDocsInOrder, Bits acceptDocs) throws IOException {
 
         if (scoreDocsInOrder) {
           return super.bulkScorer(context, scoreDocsInOrder, acceptDocs);
diff --git lucene/join/src/java/org/apache/lucene/search/join/TermsWithScoreCollector.java lucene/join/src/java/org/apache/lucene/search/join/TermsWithScoreCollector.java
index 4c2a483..0bdbc14 100644
--- lucene/join/src/java/org/apache/lucene/search/join/TermsWithScoreCollector.java
+++ lucene/join/src/java/org/apache/lucene/search/join/TermsWithScoreCollector.java
@@ -19,16 +19,13 @@ package org.apache.lucene.search.join;
 
 import java.io.IOException;
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.BinaryDocValues;
 import org.apache.lucene.index.DocValues;
 import org.apache.lucene.index.SortedSetDocValues;
-import org.apache.lucene.search.LeafCollector;
-import org.apache.lucene.search.Collector;
 import org.apache.lucene.search.Scorer;
 import org.apache.lucene.search.SimpleCollector;
 import org.apache.lucene.util.ArrayUtil;
-import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.BytesRefHash;
 
 abstract class TermsWithScoreCollector extends SimpleCollector {
@@ -128,7 +125,7 @@ abstract class TermsWithScoreCollector extends SimpleCollector {
     }
 
     @Override
-    protected void doSetNextReader(AtomicReaderContext context) throws IOException {
+    protected void doSetNextReader(LeafReaderContext context) throws IOException {
       fromDocTerms = DocValues.getBinary(context.reader(), field);
     }
 
@@ -210,7 +207,7 @@ abstract class TermsWithScoreCollector extends SimpleCollector {
     }
 
     @Override
-    protected void doSetNextReader(AtomicReaderContext context) throws IOException {
+    protected void doSetNextReader(LeafReaderContext context) throws IOException {
       fromDocTermOrds = DocValues.getSortedSet(context.reader(), field);
     }
 
diff --git lucene/join/src/java/org/apache/lucene/search/join/ToChildBlockJoinQuery.java lucene/join/src/java/org/apache/lucene/search/join/ToChildBlockJoinQuery.java
index c555c4a..e7cba34 100644
--- lucene/join/src/java/org/apache/lucene/search/join/ToChildBlockJoinQuery.java
+++ lucene/join/src/java/org/apache/lucene/search/join/ToChildBlockJoinQuery.java
@@ -22,7 +22,7 @@ import java.util.Collection;
 import java.util.Collections;
 import java.util.Set;
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.search.DocIdSet;
@@ -124,7 +124,7 @@ public class ToChildBlockJoinQuery extends Query {
     // NOTE: acceptDocs applies (and is checked) only in the
     // child document space
     @Override
-    public Scorer scorer(AtomicReaderContext readerContext, Bits acceptDocs) throws IOException {
+    public Scorer scorer(LeafReaderContext readerContext, Bits acceptDocs) throws IOException {
 
       final Scorer parentScorer = parentWeight.scorer(readerContext, null);
 
@@ -152,7 +152,7 @@ public class ToChildBlockJoinQuery extends Query {
     }
 
     @Override
-    public Explanation explain(AtomicReaderContext reader, int doc) throws IOException {
+    public Explanation explain(LeafReaderContext reader, int doc) throws IOException {
       // TODO
       throw new UnsupportedOperationException(getClass().getName() +
                                               " cannot explain match on parent document");
diff --git lucene/join/src/java/org/apache/lucene/search/join/ToParentBlockJoinCollector.java lucene/join/src/java/org/apache/lucene/search/join/ToParentBlockJoinCollector.java
index 65767fc..6fe1fc4 100644
--- lucene/join/src/java/org/apache/lucene/search/join/ToParentBlockJoinCollector.java
+++ lucene/join/src/java/org/apache/lucene/search/join/ToParentBlockJoinCollector.java
@@ -17,7 +17,7 @@ package org.apache.lucene.search.join;
  * limitations under the License.
  */
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.IndexWriter; // javadocs
 import org.apache.lucene.search.*;
 import org.apache.lucene.search.Scorer.ChildScorer;
@@ -91,7 +91,7 @@ public class ToParentBlockJoinCollector extends SimpleCollector {
 
   private int docBase;
   private ToParentBlockJoinQuery.BlockJoinScorer[] joinScorers = new ToParentBlockJoinQuery.BlockJoinScorer[0];
-  private AtomicReaderContext currentReaderContext;
+  private LeafReaderContext currentReaderContext;
   private Scorer scorer;
   private boolean queueFull;
 
@@ -136,7 +136,7 @@ public class ToParentBlockJoinCollector extends SimpleCollector {
       }
       counts = new int[numJoins];
     }
-    AtomicReaderContext readerContext;
+    LeafReaderContext readerContext;
     int[][] docs;
     float[][] scores;
     int[] counts;
@@ -269,7 +269,7 @@ public class ToParentBlockJoinCollector extends SimpleCollector {
   }
 
   @Override
-  protected void doSetNextReader(AtomicReaderContext context) throws IOException {
+  protected void doSetNextReader(LeafReaderContext context) throws IOException {
     currentReaderContext = context;
     docBase = context.docBase;
     for (int compIDX = 0; compIDX < comparators.length; compIDX++) {
diff --git lucene/join/src/java/org/apache/lucene/search/join/ToParentBlockJoinFieldComparator.java lucene/join/src/java/org/apache/lucene/search/join/ToParentBlockJoinFieldComparator.java
index bbf5949..3fdf7ad 100644
--- lucene/join/src/java/org/apache/lucene/search/join/ToParentBlockJoinFieldComparator.java
+++ lucene/join/src/java/org/apache/lucene/search/join/ToParentBlockJoinFieldComparator.java
@@ -17,7 +17,7 @@ package org.apache.lucene.search.join;
  * limitations under the License.
  */
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.search.DocIdSet;
 import org.apache.lucene.search.DocIdSetIterator;
 import org.apache.lucene.search.FieldComparator;
@@ -65,7 +65,7 @@ public abstract class ToParentBlockJoinFieldComparator extends FieldComparator<O
   }
 
   @Override
-  public FieldComparator<Object> setNextReader(AtomicReaderContext context) throws IOException {
+  public FieldComparator<Object> setNextReader(LeafReaderContext context) throws IOException {
     DocIdSet innerDocuments = childFilter.getDocIdSet(context, null);
     if (isEmpty(innerDocuments)) {
       this.childDocuments = null;
diff --git lucene/join/src/java/org/apache/lucene/search/join/ToParentBlockJoinQuery.java lucene/join/src/java/org/apache/lucene/search/join/ToParentBlockJoinQuery.java
index f6985e2..6150df9 100644
--- lucene/join/src/java/org/apache/lucene/search/join/ToParentBlockJoinQuery.java
+++ lucene/join/src/java/org/apache/lucene/search/join/ToParentBlockJoinQuery.java
@@ -23,7 +23,7 @@ import java.util.Collections;
 import java.util.Locale;
 import java.util.Set;
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexWriter;
 import org.apache.lucene.index.Term;
@@ -158,7 +158,7 @@ public class ToParentBlockJoinQuery extends Query {
     // NOTE: acceptDocs applies (and is checked) only in the
     // parent document space
     @Override
-    public Scorer scorer(AtomicReaderContext readerContext, Bits acceptDocs) throws IOException {
+    public Scorer scorer(LeafReaderContext readerContext, Bits acceptDocs) throws IOException {
 
       final Scorer childScorer = childWeight.scorer(readerContext, readerContext.reader().getLiveDocs());
       if (childScorer == null) {
@@ -191,7 +191,7 @@ public class ToParentBlockJoinQuery extends Query {
     }
 
     @Override
-    public Explanation explain(AtomicReaderContext context, int doc) throws IOException {
+    public Explanation explain(LeafReaderContext context, int doc) throws IOException {
       BlockJoinScorer scorer = (BlockJoinScorer) scorer(context, context.reader().getLiveDocs());
       if (scorer != null && scorer.advance(doc) == doc) {
         return scorer.explain(context.docBase);
diff --git lucene/join/src/test/org/apache/lucene/search/join/TestBlockJoin.java lucene/join/src/test/org/apache/lucene/search/join/TestBlockJoin.java
index 8642d3c..9e33682 100644
--- lucene/join/src/test/org/apache/lucene/search/join/TestBlockJoin.java
+++ lucene/join/src/test/org/apache/lucene/search/join/TestBlockJoin.java
@@ -356,9 +356,9 @@ public class TestBlockJoin extends LuceneTestCase {
   }
   
   private StoredDocument getParentDoc(IndexReader reader, Filter parents, int childDocID) throws IOException {
-    final List<AtomicReaderContext> leaves = reader.leaves();
+    final List<LeafReaderContext> leaves = reader.leaves();
     final int subIndex = ReaderUtil.subIndex(childDocID, leaves);
-    final AtomicReaderContext leaf = leaves.get(subIndex);
+    final LeafReaderContext leaf = leaves.get(subIndex);
     final FixedBitSet bits = (FixedBitSet) parents.getDocIdSet(leaf, null);
     return leaf.reader().document(bits.nextSetBit(childDocID - leaf.docBase));
   }
diff --git lucene/join/src/test/org/apache/lucene/search/join/TestJoinUtil.java lucene/join/src/test/org/apache/lucene/search/join/TestJoinUtil.java
index fbd65b4..688589a 100644
--- lucene/join/src/test/org/apache/lucene/search/join/TestJoinUtil.java
+++ lucene/join/src/test/org/apache/lucene/search/join/TestJoinUtil.java
@@ -37,8 +37,8 @@ import org.apache.lucene.document.Field;
 import org.apache.lucene.document.SortedDocValuesField;
 import org.apache.lucene.document.SortedSetDocValuesField;
 import org.apache.lucene.document.TextField;
-import org.apache.lucene.index.AtomicReader;
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReader;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.BinaryDocValues;
 import org.apache.lucene.index.DocValues;
 import org.apache.lucene.index.DocsEnum;
@@ -486,7 +486,7 @@ public class TestJoinUtil extends LuceneTestCase {
           }
 
           @Override
-          protected void doSetNextReader(AtomicReaderContext context) throws IOException {
+          protected void doSetNextReader(LeafReaderContext context) throws IOException {
             docBase = context.docBase;
             topScoreDocCollector.getLeafCollector(context);
           }
@@ -670,7 +670,7 @@ public class TestJoinUtil extends LuceneTestCase {
           }
 
           @Override
-          protected void doSetNextReader(AtomicReaderContext context) throws IOException {
+          protected void doSetNextReader(LeafReaderContext context) throws IOException {
             docTermOrds = DocValues.getSortedSet(context.reader(), fromField);
           }
 
@@ -706,7 +706,7 @@ public class TestJoinUtil extends LuceneTestCase {
           }
 
           @Override
-          protected void doSetNextReader(AtomicReaderContext context) throws IOException {
+          protected void doSetNextReader(LeafReaderContext context) throws IOException {
             terms = DocValues.getBinary(context.reader(), fromField);
             docsWithField = DocValues.getDocsWithField(context.reader(), fromField);
           }
@@ -726,7 +726,7 @@ public class TestJoinUtil extends LuceneTestCase {
       final Map<Integer, JoinScore> docToJoinScore = new HashMap<>();
       if (multipleValuesPerDocument) {
         if (scoreDocsInOrder) {
-          AtomicReader slowCompositeReader = SlowCompositeReaderWrapper.wrap(toSearcher.getIndexReader());
+          LeafReader slowCompositeReader = SlowCompositeReaderWrapper.wrap(toSearcher.getIndexReader());
           Terms terms = slowCompositeReader.terms(toField);
           if (terms != null) {
             DocsEnum docsEnum = null;
@@ -775,7 +775,7 @@ public class TestJoinUtil extends LuceneTestCase {
             }
 
             @Override
-            protected void doSetNextReader(AtomicReaderContext context) throws IOException {
+            protected void doSetNextReader(LeafReaderContext context) throws IOException {
               docBase = context.docBase;
               docTermOrds = DocValues.getSortedSet(context.reader(), toField);
             }
@@ -803,7 +803,7 @@ public class TestJoinUtil extends LuceneTestCase {
           }
 
           @Override
-          protected void doSetNextReader(AtomicReaderContext context) throws IOException {
+          protected void doSetNextReader(LeafReaderContext context) throws IOException {
             terms = DocValues.getBinary(context.reader(), toField);
             docBase = context.docBase;
           }
diff --git lucene/memory/src/java/org/apache/lucene/index/memory/MemoryIndex.java lucene/memory/src/java/org/apache/lucene/index/memory/MemoryIndex.java
index f7cdf60..94ede86 100644
--- lucene/memory/src/java/org/apache/lucene/index/memory/MemoryIndex.java
+++ lucene/memory/src/java/org/apache/lucene/index/memory/MemoryIndex.java
@@ -32,7 +32,7 @@ import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
 import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
 import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;
 import org.apache.lucene.analysis.tokenattributes.TermToBytesRefAttribute;
-import org.apache.lucene.index.AtomicReader;
+import org.apache.lucene.index.LeafReader;
 import org.apache.lucene.index.BinaryDocValues;
 import org.apache.lucene.index.DocsAndPositionsEnum;
 import org.apache.lucene.index.DocsEnum;
@@ -730,7 +730,7 @@ public class MemoryIndex {
    * Search support for Lucene framework integration; implements all methods
    * required by the Lucene IndexReader contracts.
    */
-  private final class MemoryIndexReader extends AtomicReader {
+  private final class MemoryIndexReader extends LeafReader {
     
     private IndexSearcher searcher; // needed to find searcher.getSimilarity() 
     
diff --git lucene/memory/src/test/org/apache/lucene/index/memory/MemoryIndexTest.java lucene/memory/src/test/org/apache/lucene/index/memory/MemoryIndexTest.java
index 0de8e72..bd24b9d 100644
--- lucene/memory/src/test/org/apache/lucene/index/memory/MemoryIndexTest.java
+++ lucene/memory/src/test/org/apache/lucene/index/memory/MemoryIndexTest.java
@@ -41,7 +41,7 @@ import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.FieldType;
 import org.apache.lucene.document.TextField;
-import org.apache.lucene.index.AtomicReader;
+import org.apache.lucene.index.LeafReader;
 import org.apache.lucene.index.CompositeReader;
 import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.DocsAndPositionsEnum;
@@ -160,7 +160,7 @@ public class MemoryIndexTest extends BaseTokenStreamTestCase {
     memory.addField("foo", fooField.toString(), analyzer);
     memory.addField("term", termField.toString(), analyzer);
     
-    AtomicReader reader = (AtomicReader) memory.createSearcher().getIndexReader();
+    LeafReader reader = (LeafReader) memory.createSearcher().getIndexReader();
     DirectoryReader competitor = DirectoryReader.open(ramdir);
     duellReaders(competitor, reader);
     IOUtils.close(reader, competitor);
@@ -168,9 +168,9 @@ public class MemoryIndexTest extends BaseTokenStreamTestCase {
     ramdir.close();    
   }
 
-  private void duellReaders(CompositeReader other, AtomicReader memIndexReader)
+  private void duellReaders(CompositeReader other, LeafReader memIndexReader)
       throws IOException {
-    AtomicReader competitor = SlowCompositeReaderWrapper.wrap(other);
+    LeafReader competitor = SlowCompositeReaderWrapper.wrap(other);
     Fields memFields = memIndexReader.fields();
     for (String field : competitor.fields()) {
       Terms memTerms = memFields.terms(field);
@@ -313,7 +313,7 @@ public class MemoryIndexTest extends BaseTokenStreamTestCase {
     Analyzer analyzer = new MockAnalyzer(random());
     MemoryIndex memory = new MemoryIndex(random().nextBoolean(),  random().nextInt(50) * 1024 * 1024);
     memory.addField("foo", "bar", analyzer);
-    AtomicReader reader = (AtomicReader) memory.createSearcher().getIndexReader();
+    LeafReader reader = (LeafReader) memory.createSearcher().getIndexReader();
     DocsEnum disi = TestUtil.docs(random(), reader, "foo", new BytesRef("bar"), null, null, DocsEnum.FLAG_NONE);
     int docid = disi.docID();
     assertEquals(-1, docid);
@@ -343,7 +343,7 @@ public class MemoryIndexTest extends BaseTokenStreamTestCase {
     MemoryIndex memory = new MemoryIndex(true,  random().nextInt(50) * 1024 * 1024);
     for (int i = 0; i < numIters; i++) { // check reuse
       memory.addField("foo", "bar", analyzer);
-      AtomicReader reader = (AtomicReader) memory.createSearcher().getIndexReader();
+      LeafReader reader = (LeafReader) memory.createSearcher().getIndexReader();
       assertEquals(1, reader.terms("foo").getSumTotalTermFreq());
       DocsAndPositionsEnum disi = reader.termPositionsEnum(new Term("foo", "bar"));
       int docid = disi.docID();
@@ -394,7 +394,7 @@ public class MemoryIndexTest extends BaseTokenStreamTestCase {
     MockAnalyzer mockAnalyzer = new MockAnalyzer(random());
     mindex.addField("field", "the quick brown fox", mockAnalyzer);
     mindex.addField("field", "jumps over the", mockAnalyzer);
-    AtomicReader reader = (AtomicReader) mindex.createSearcher().getIndexReader();
+    LeafReader reader = (LeafReader) mindex.createSearcher().getIndexReader();
     assertEquals(7, reader.terms("field").getSumTotalTermFreq());
     PhraseQuery query = new PhraseQuery();
     query.add(new Term("field", "fox"));
@@ -413,7 +413,7 @@ public class MemoryIndexTest extends BaseTokenStreamTestCase {
     MemoryIndex mindex = new MemoryIndex(random().nextBoolean(),  random().nextInt(50) * 1024 * 1024);
     MockAnalyzer mockAnalyzer = new MockAnalyzer(random());
     mindex.addField("field", "the quick brown fox", mockAnalyzer);
-    AtomicReader reader = (AtomicReader) mindex.createSearcher().getIndexReader();
+    LeafReader reader = (LeafReader) mindex.createSearcher().getIndexReader();
     assertNull(reader.getNumericDocValues("not-in-index"));
     assertNull(reader.getNormValues("not-in-index"));
     assertNull(reader.termDocsEnum(new Term("not-in-index", "foo")));
@@ -447,7 +447,7 @@ public class MemoryIndexTest extends BaseTokenStreamTestCase {
           memory.addField(field.name(), ((Field)field).stringValue(), mockAnalyzer);  
       }
       DirectoryReader competitor = DirectoryReader.open(dir);
-      AtomicReader memIndexReader= (AtomicReader) memory.createSearcher().getIndexReader();
+      LeafReader memIndexReader= (LeafReader) memory.createSearcher().getIndexReader();
       duellReaders(competitor, memIndexReader);
       IOUtils.close(competitor, memIndexReader);
       memory.reset();
diff --git lucene/misc/src/java/org/apache/lucene/index/MultiPassIndexSplitter.java lucene/misc/src/java/org/apache/lucene/index/MultiPassIndexSplitter.java
index f41361b..6784606 100644
--- lucene/misc/src/java/org/apache/lucene/index/MultiPassIndexSplitter.java
+++ lucene/misc/src/java/org/apache/lucene/index/MultiPassIndexSplitter.java
@@ -101,7 +101,7 @@ public class MultiPassIndexSplitter {
           .setOpenMode(OpenMode.CREATE));
       System.err.println("Writing part " + (i + 1) + " ...");
       // pass the subreaders directly, as our wrapper's numDocs/hasDeletetions are not up-to-date
-      final List<? extends FakeDeleteAtomicIndexReader> sr = input.getSequentialSubReaders();
+      final List<? extends FakeDeleteLeafIndexReader> sr = input.getSequentialSubReaders();
       w.addIndexes(sr.toArray(new IndexReader[sr.size()])); // TODO: maybe take List<IR> here?
       w.close();
     }
@@ -176,18 +176,18 @@ public class MultiPassIndexSplitter {
   /**
    * This class emulates deletions on the underlying index.
    */
-  private static final class FakeDeleteIndexReader extends BaseCompositeReader<FakeDeleteAtomicIndexReader> {
+  private static final class FakeDeleteIndexReader extends BaseCompositeReader<FakeDeleteLeafIndexReader> {
 
     public FakeDeleteIndexReader(IndexReader reader) {
       super(initSubReaders(reader));
     }
     
-    private static FakeDeleteAtomicIndexReader[] initSubReaders(IndexReader reader) {
-      final List<AtomicReaderContext> leaves = reader.leaves();
-      final FakeDeleteAtomicIndexReader[] subs = new FakeDeleteAtomicIndexReader[leaves.size()];
+    private static FakeDeleteLeafIndexReader[] initSubReaders(IndexReader reader) {
+      final List<LeafReaderContext> leaves = reader.leaves();
+      final FakeDeleteLeafIndexReader[] subs = new FakeDeleteLeafIndexReader[leaves.size()];
       int i = 0;
-      for (final AtomicReaderContext ctx : leaves) {
-        subs[i++] = new FakeDeleteAtomicIndexReader(ctx.reader());
+      for (final LeafReaderContext ctx : leaves) {
+        subs[i++] = new FakeDeleteLeafIndexReader(ctx.reader());
       }
       return subs;
     }
@@ -198,7 +198,7 @@ public class MultiPassIndexSplitter {
     }
 
     public void undeleteAll()  {
-      for (FakeDeleteAtomicIndexReader r : getSequentialSubReaders()) {
+      for (FakeDeleteLeafIndexReader r : getSequentialSubReaders()) {
         r.undeleteAll();
       }
     }
@@ -210,10 +210,10 @@ public class MultiPassIndexSplitter {
     // as we pass the subreaders directly to IW.addIndexes().
   }
   
-  private static final class FakeDeleteAtomicIndexReader extends FilterAtomicReader {
+  private static final class FakeDeleteLeafIndexReader extends FilterLeafReader {
     FixedBitSet liveDocs;
 
-    public FakeDeleteAtomicIndexReader(AtomicReader reader) {
+    public FakeDeleteLeafIndexReader(LeafReader reader) {
       super(reader);
       undeleteAll(); // initialize main bitset
     }
diff --git lucene/misc/src/java/org/apache/lucene/index/PKIndexSplitter.java lucene/misc/src/java/org/apache/lucene/index/PKIndexSplitter.java
index f71db1e..f69f422 100644
--- lucene/misc/src/java/org/apache/lucene/index/PKIndexSplitter.java
+++ lucene/misc/src/java/org/apache/lucene/index/PKIndexSplitter.java
@@ -101,11 +101,11 @@ public class PKIndexSplitter {
     boolean success = false;
     final IndexWriter w = new IndexWriter(target, config);
     try {
-      final List<AtomicReaderContext> leaves = reader.leaves();
+      final List<LeafReaderContext> leaves = reader.leaves();
       final IndexReader[] subReaders = new IndexReader[leaves.size()];
       int i = 0;
-      for (final AtomicReaderContext ctx : leaves) {
-        subReaders[i++] = new DocumentFilteredAtomicIndexReader(ctx, preserveFilter, negateFilter);
+      for (final LeafReaderContext ctx : leaves) {
+        subReaders[i++] = new DocumentFilteredLeafIndexReader(ctx, preserveFilter, negateFilter);
       }
       w.addIndexes(subReaders);
       success = true;
@@ -118,11 +118,11 @@ public class PKIndexSplitter {
     }
   }
     
-  private static class DocumentFilteredAtomicIndexReader extends FilterAtomicReader {
+  private static class DocumentFilteredLeafIndexReader extends FilterLeafReader {
     final Bits liveDocs;
     final int numDocs;
     
-    public DocumentFilteredAtomicIndexReader(AtomicReaderContext context, Filter preserveFilter, boolean negateFilter) throws IOException {
+    public DocumentFilteredLeafIndexReader(LeafReaderContext context, Filter preserveFilter, boolean negateFilter) throws IOException {
       super(context.reader());
       final int maxDoc = in.maxDoc();
       final FixedBitSet bits = new FixedBitSet(maxDoc);
diff --git lucene/misc/src/java/org/apache/lucene/index/sorter/BlockJoinComparatorSource.java lucene/misc/src/java/org/apache/lucene/index/sorter/BlockJoinComparatorSource.java
index af91463..ec5eeb3 100644
--- lucene/misc/src/java/org/apache/lucene/index/sorter/BlockJoinComparatorSource.java
+++ lucene/misc/src/java/org/apache/lucene/index/sorter/BlockJoinComparatorSource.java
@@ -19,7 +19,7 @@ package org.apache.lucene.index.sorter;
 
 import java.io.IOException;
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.search.DocIdSet;
 import org.apache.lucene.search.FieldComparator;
 import org.apache.lucene.search.FieldComparatorSource;
@@ -143,7 +143,7 @@ public class BlockJoinComparatorSource extends FieldComparatorSource {
       }
 
       @Override
-      public FieldComparator<Integer> setNextReader(AtomicReaderContext context) throws IOException {
+      public FieldComparator<Integer> setNextReader(LeafReaderContext context) throws IOException {
         final DocIdSet parents = parentsFilter.getDocIdSet(context, null);
         if (parents == null) {
           throw new IllegalStateException("AtomicReader " + context.reader() + " contains no parents!");
diff --git lucene/misc/src/java/org/apache/lucene/index/sorter/EarlyTerminatingSortingCollector.java lucene/misc/src/java/org/apache/lucene/index/sorter/EarlyTerminatingSortingCollector.java
index 27f61ad..7c5ae57 100644
--- lucene/misc/src/java/org/apache/lucene/index/sorter/EarlyTerminatingSortingCollector.java
+++ lucene/misc/src/java/org/apache/lucene/index/sorter/EarlyTerminatingSortingCollector.java
@@ -19,7 +19,7 @@ package org.apache.lucene.index.sorter;
 
 import java.io.IOException;
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.IndexWriter;
 import org.apache.lucene.search.LeafCollector;
 import org.apache.lucene.search.CollectionTerminatedException;
@@ -92,7 +92,7 @@ public class EarlyTerminatingSortingCollector extends FilterCollector {
   }
 
   @Override
-  public LeafCollector getLeafCollector(AtomicReaderContext context) throws IOException {
+  public LeafCollector getLeafCollector(LeafReaderContext context) throws IOException {
     if (SortingMergePolicy.isSorted(context.reader(), sort)) {
       // segment is sorted, can early-terminate
       return new FilterLeafCollector(super.getLeafCollector(context)) {
diff --git lucene/misc/src/java/org/apache/lucene/index/sorter/Sorter.java lucene/misc/src/java/org/apache/lucene/index/sorter/Sorter.java
index 1926d7b..a3c75ae 100644
--- lucene/misc/src/java/org/apache/lucene/index/sorter/Sorter.java
+++ lucene/misc/src/java/org/apache/lucene/index/sorter/Sorter.java
@@ -20,7 +20,7 @@ package org.apache.lucene.index.sorter;
 import java.io.IOException;
 import java.util.Comparator;
 
-import org.apache.lucene.index.AtomicReader;
+import org.apache.lucene.index.LeafReader;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.search.FieldComparator;
 import org.apache.lucene.search.Scorer;
@@ -61,8 +61,8 @@ final class Sorter {
     abstract int newToOld(int docID);
 
     /** Return the number of documents in this map. This must be equal to the
-     *  {@link AtomicReader#maxDoc() number of documents} of the
-     *  {@link AtomicReader} which is sorted. */
+     *  {@link org.apache.lucene.index.LeafReader#maxDoc() number of documents} of the
+     *  {@link org.apache.lucene.index.LeafReader} which is sorted. */
     abstract int size();
   }
 
@@ -211,7 +211,7 @@ final class Sorter {
    * <b>NOTE:</b> deleted documents are expected to appear in the mapping as
    * well, they will however be marked as deleted in the sorted view.
    */
-  DocMap sort(AtomicReader reader) throws IOException {
+  DocMap sort(LeafReader reader) throws IOException {
     SortField fields[] = sort.getSort();
     final int reverseMul[] = new int[fields.length];
     final FieldComparator<?> comparators[] = new FieldComparator[fields.length];
diff --git lucene/misc/src/java/org/apache/lucene/index/sorter/SortingAtomicReader.java lucene/misc/src/java/org/apache/lucene/index/sorter/SortingAtomicReader.java
deleted file mode 100644
index e81dea7..0000000
--- lucene/misc/src/java/org/apache/lucene/index/sorter/SortingAtomicReader.java
+++ /dev/null
@@ -1,864 +0,0 @@
-package org.apache.lucene.index.sorter;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Arrays;
-
-import org.apache.lucene.index.AtomicReader;
-import org.apache.lucene.index.BinaryDocValues;
-import org.apache.lucene.index.DocsAndPositionsEnum;
-import org.apache.lucene.index.DocsEnum;
-import org.apache.lucene.index.FieldInfo.IndexOptions;
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.Fields;
-import org.apache.lucene.index.FilterAtomicReader;
-import org.apache.lucene.index.NumericDocValues;
-import org.apache.lucene.index.SortedDocValues;
-import org.apache.lucene.index.SortedNumericDocValues;
-import org.apache.lucene.index.SortedSetDocValues;
-import org.apache.lucene.index.StoredFieldVisitor;
-import org.apache.lucene.index.Terms;
-import org.apache.lucene.index.TermsEnum;
-import org.apache.lucene.index.sorter.Sorter.DocMap;
-import org.apache.lucene.search.DocIdSetIterator;
-import org.apache.lucene.search.Sort;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.store.RAMFile;
-import org.apache.lucene.store.RAMInputStream;
-import org.apache.lucene.store.RAMOutputStream;
-import org.apache.lucene.util.ArrayUtil;
-import org.apache.lucene.util.Bits;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.TimSorter;
-import org.apache.lucene.util.automaton.CompiledAutomaton;
-
-/**
- * An {@link AtomicReader} which supports sorting documents by a given
- * {@link Sort}. You can use this class to sort an index as follows:
- * 
- * <pre class="prettyprint">
- * IndexWriter writer; // writer to which the sorted index will be added
- * DirectoryReader reader; // reader on the input index
- * Sort sort; // determines how the documents are sorted
- * AtomicReader sortingReader = SortingAtomicReader.wrap(SlowCompositeReaderWrapper.wrap(reader), sort);
- * writer.addIndexes(reader);
- * writer.close();
- * reader.close();
- * </pre>
- * 
- * @lucene.experimental
- */
-public class SortingAtomicReader extends FilterAtomicReader {
-
-  private static class SortingFields extends FilterFields {
-
-    private final Sorter.DocMap docMap;
-    private final FieldInfos infos;
-
-    public SortingFields(final Fields in, FieldInfos infos, Sorter.DocMap docMap) {
-      super(in);
-      this.docMap = docMap;
-      this.infos = infos;
-    }
-
-    @Override
-    public Terms terms(final String field) throws IOException {
-      Terms terms = in.terms(field);
-      if (terms == null) {
-        return null;
-      } else {
-        return new SortingTerms(terms, infos.fieldInfo(field).getIndexOptions(), docMap);
-      }
-    }
-
-  }
-
-  private static class SortingTerms extends FilterTerms {
-
-    private final Sorter.DocMap docMap;
-    private final IndexOptions indexOptions;
-    
-    public SortingTerms(final Terms in, IndexOptions indexOptions, final Sorter.DocMap docMap) {
-      super(in);
-      this.docMap = docMap;
-      this.indexOptions = indexOptions;
-    }
-
-    @Override
-    public TermsEnum iterator(final TermsEnum reuse) throws IOException {
-      return new SortingTermsEnum(in.iterator(reuse), docMap, indexOptions);
-    }
-
-    @Override
-    public TermsEnum intersect(CompiledAutomaton compiled, BytesRef startTerm)
-        throws IOException {
-      return new SortingTermsEnum(in.intersect(compiled, startTerm), docMap, indexOptions);
-    }
-
-  }
-
-  private static class SortingTermsEnum extends FilterTermsEnum {
-
-    final Sorter.DocMap docMap; // pkg-protected to avoid synthetic accessor methods
-    private final IndexOptions indexOptions;
-    
-    public SortingTermsEnum(final TermsEnum in, Sorter.DocMap docMap, IndexOptions indexOptions) {
-      super(in);
-      this.docMap = docMap;
-      this.indexOptions = indexOptions;
-    }
-
-    Bits newToOld(final Bits liveDocs) {
-      if (liveDocs == null) {
-        return null;
-      }
-      return new Bits() {
-
-        @Override
-        public boolean get(int index) {
-          return liveDocs.get(docMap.oldToNew(index));
-        }
-
-        @Override
-        public int length() {
-          return liveDocs.length();
-        }
-
-      };
-    }
-
-    @Override
-    public DocsEnum docs(Bits liveDocs, DocsEnum reuse, final int flags) throws IOException {
-      final DocsEnum inReuse;
-      final SortingDocsEnum wrapReuse;
-      if (reuse != null && reuse instanceof SortingDocsEnum) {
-        // if we're asked to reuse the given DocsEnum and it is Sorting, return
-        // the wrapped one, since some Codecs expect it.
-        wrapReuse = (SortingDocsEnum) reuse;
-        inReuse = wrapReuse.getWrapped();
-      } else {
-        wrapReuse = null;
-        inReuse = reuse;
-      }
-
-      final DocsEnum inDocs = in.docs(newToOld(liveDocs), inReuse, flags);
-      final boolean withFreqs = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >=0 && (flags & DocsEnum.FLAG_FREQS) != 0;
-      return new SortingDocsEnum(docMap.size(), wrapReuse, inDocs, withFreqs, docMap);
-    }
-
-    @Override
-    public DocsAndPositionsEnum docsAndPositions(Bits liveDocs, DocsAndPositionsEnum reuse, final int flags) throws IOException {
-      final DocsAndPositionsEnum inReuse;
-      final SortingDocsAndPositionsEnum wrapReuse;
-      if (reuse != null && reuse instanceof SortingDocsAndPositionsEnum) {
-        // if we're asked to reuse the given DocsEnum and it is Sorting, return
-        // the wrapped one, since some Codecs expect it.
-        wrapReuse = (SortingDocsAndPositionsEnum) reuse;
-        inReuse = wrapReuse.getWrapped();
-      } else {
-        wrapReuse = null;
-        inReuse = reuse;
-      }
-
-      final DocsAndPositionsEnum inDocsAndPositions = in.docsAndPositions(newToOld(liveDocs), inReuse, flags);
-      if (inDocsAndPositions == null) {
-        return null;
-      }
-
-      // we ignore the fact that offsets may be stored but not asked for,
-      // since this code is expected to be used during addIndexes which will
-      // ask for everything. if that assumption changes in the future, we can
-      // factor in whether 'flags' says offsets are not required.
-      final boolean storeOffsets = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;
-      return new SortingDocsAndPositionsEnum(docMap.size(), wrapReuse, inDocsAndPositions, docMap, storeOffsets);
-    }
-
-  }
-
-  private static class SortingBinaryDocValues extends BinaryDocValues {
-    
-    private final BinaryDocValues in;
-    private final Sorter.DocMap docMap;
-    
-    SortingBinaryDocValues(BinaryDocValues in, Sorter.DocMap docMap) {
-      this.in = in;
-      this.docMap = docMap;
-    }
-
-    @Override
-    public BytesRef get(int docID) {
-      return in.get(docMap.newToOld(docID));
-    }
-  }
-  
-  private static class SortingNumericDocValues extends NumericDocValues {
-
-    private final NumericDocValues in;
-    private final Sorter.DocMap docMap;
-
-    public SortingNumericDocValues(final NumericDocValues in, Sorter.DocMap docMap) {
-      this.in = in;
-      this.docMap = docMap;
-    }
-
-    @Override
-    public long get(int docID) {
-      return in.get(docMap.newToOld(docID));
-    }
-  }
-  
-  private static class SortingSortedNumericDocValues extends SortedNumericDocValues {
-    
-    private final SortedNumericDocValues in;
-    private final Sorter.DocMap docMap;
-    
-    SortingSortedNumericDocValues(SortedNumericDocValues in, DocMap docMap) {
-      this.in = in;
-      this.docMap = docMap;
-    }
-    
-    @Override
-    public int count() {
-      return in.count();
-    }
-    
-    @Override
-    public void setDocument(int doc) {
-      in.setDocument(docMap.newToOld(doc));
-    }
-    
-    @Override
-    public long valueAt(int index) {
-      return in.valueAt(index);
-    }
-  }
-  
-  private static class SortingBits implements Bits {
-
-    private final Bits in;
-    private final Sorter.DocMap docMap;
-
-    public SortingBits(final Bits in, Sorter.DocMap docMap) {
-      this.in = in;
-      this.docMap = docMap;
-    }
-
-    @Override
-    public boolean get(int index) {
-      return in.get(docMap.newToOld(index));
-    }
-
-    @Override
-    public int length() {
-      return in.length();
-    }
-  }
-  
-  private static class SortingSortedDocValues extends SortedDocValues {
-    
-    private final SortedDocValues in;
-    private final Sorter.DocMap docMap;
-    
-    SortingSortedDocValues(SortedDocValues in, Sorter.DocMap docMap) {
-      this.in = in;
-      this.docMap = docMap;
-    }
-
-    @Override
-    public int getOrd(int docID) {
-      return in.getOrd(docMap.newToOld(docID));
-    }
-
-    @Override
-    public BytesRef lookupOrd(int ord) {
-      return in.lookupOrd(ord);
-    }
-
-    @Override
-    public int getValueCount() {
-      return in.getValueCount();
-    }
-
-    @Override
-    public BytesRef get(int docID) {
-      return in.get(docMap.newToOld(docID));
-    }
-
-    @Override
-    public int lookupTerm(BytesRef key) {
-      return in.lookupTerm(key);
-    }
-  }
-  
-  private static class SortingSortedSetDocValues extends SortedSetDocValues {
-    
-    private final SortedSetDocValues in;
-    private final Sorter.DocMap docMap;
-    
-    SortingSortedSetDocValues(SortedSetDocValues in, Sorter.DocMap docMap) {
-      this.in = in;
-      this.docMap = docMap;
-    }
-
-    @Override
-    public long nextOrd() {
-      return in.nextOrd();
-    }
-
-    @Override
-    public void setDocument(int docID) {
-      in.setDocument(docMap.newToOld(docID));
-    }
-
-    @Override
-    public BytesRef lookupOrd(long ord) {
-      return in.lookupOrd(ord);
-    }
-
-    @Override
-    public long getValueCount() {
-      return in.getValueCount();
-    }
-
-    @Override
-    public long lookupTerm(BytesRef key) {
-      return in.lookupTerm(key);
-    }
-  }
-
-  static class SortingDocsEnum extends FilterDocsEnum {
-    
-    private static final class DocFreqSorter extends TimSorter {
-      
-      private int[] docs;
-      private int[] freqs;
-      private final int[] tmpDocs;
-      private int[] tmpFreqs;
-      
-      public DocFreqSorter(int maxDoc) {
-        super(maxDoc / 64);
-        this.tmpDocs = new int[maxDoc / 64];
-      }
-
-      public void reset(int[] docs, int[] freqs) {
-        this.docs = docs;
-        this.freqs = freqs;
-        if (freqs != null && tmpFreqs == null) {
-          tmpFreqs = new int[tmpDocs.length];
-        }
-      }
-
-      @Override
-      protected int compare(int i, int j) {
-        return docs[i] - docs[j];
-      }
-      
-      @Override
-      protected void swap(int i, int j) {
-        int tmpDoc = docs[i];
-        docs[i] = docs[j];
-        docs[j] = tmpDoc;
-        
-        if (freqs != null) {
-          int tmpFreq = freqs[i];
-          freqs[i] = freqs[j];
-          freqs[j] = tmpFreq;
-        }
-      }
-
-      @Override
-      protected void copy(int src, int dest) {
-        docs[dest] = docs[src];
-        if (freqs != null) {
-          freqs[dest] = freqs[src];
-        }
-      }
-
-      @Override
-      protected void save(int i, int len) {
-        System.arraycopy(docs, i, tmpDocs, 0, len);
-        if (freqs != null) {
-          System.arraycopy(freqs, i, tmpFreqs, 0, len);
-        }
-      }
-
-      @Override
-      protected void restore(int i, int j) {
-        docs[j] = tmpDocs[i];
-        if (freqs != null) {
-          freqs[j] = tmpFreqs[i];
-        }
-      }
-
-      @Override
-      protected int compareSaved(int i, int j) {
-        return tmpDocs[i] - docs[j];
-      }
-    }
-
-    private final int maxDoc;
-    private final DocFreqSorter sorter;
-    private int[] docs;
-    private int[] freqs;
-    private int docIt = -1;
-    private final int upto;
-    private final boolean withFreqs;
-
-    SortingDocsEnum(int maxDoc, SortingDocsEnum reuse, final DocsEnum in, boolean withFreqs, final Sorter.DocMap docMap) throws IOException {
-      super(in);
-      this.maxDoc = maxDoc;
-      this.withFreqs = withFreqs;
-      if (reuse != null) {
-        if (reuse.maxDoc == maxDoc) {
-          sorter = reuse.sorter;
-        } else {
-          sorter = new DocFreqSorter(maxDoc);
-        }
-        docs = reuse.docs;
-        freqs = reuse.freqs; // maybe null
-      } else {
-        docs = new int[64];
-        sorter = new DocFreqSorter(maxDoc);
-      }
-      docIt = -1;
-      int i = 0;
-      int doc;
-      if (withFreqs) {
-        if (freqs == null || freqs.length < docs.length) {
-          freqs = new int[docs.length];
-        }
-        while ((doc = in.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS){
-          if (i >= docs.length) {
-            docs = ArrayUtil.grow(docs, docs.length + 1);
-            freqs = ArrayUtil.grow(freqs, freqs.length + 1);
-          }
-          docs[i] = docMap.oldToNew(doc);
-          freqs[i] = in.freq();
-          ++i;
-        }
-      } else {
-        freqs = null;
-        while ((doc = in.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS){
-          if (i >= docs.length) {
-            docs = ArrayUtil.grow(docs, docs.length + 1);
-          }
-          docs[i++] = docMap.oldToNew(doc);
-        }
-      }
-      // TimSort can save much time compared to other sorts in case of
-      // reverse sorting, or when sorting a concatenation of sorted readers
-      sorter.reset(docs, freqs);
-      sorter.sort(0, i);
-      upto = i;
-    }
-
-    // for testing
-    boolean reused(DocsEnum other) {
-      if (other == null || !(other instanceof SortingDocsEnum)) {
-        return false;
-      }
-      return docs == ((SortingDocsEnum) other).docs;
-    }
-
-    @Override
-    public int advance(final int target) throws IOException {
-      // need to support it for checkIndex, but in practice it won't be called, so
-      // don't bother to implement efficiently for now.
-      return slowAdvance(target);
-    }
-    
-    @Override
-    public int docID() {
-      return docIt < 0 ? -1 : docIt >= upto ? NO_MORE_DOCS : docs[docIt];
-    }
-    
-    @Override
-    public int freq() throws IOException {
-      return withFreqs && docIt < upto ? freqs[docIt] : 1;
-    }
-    
-    @Override
-    public int nextDoc() throws IOException {
-      if (++docIt >= upto) return NO_MORE_DOCS;
-      return docs[docIt];
-    }
-    
-    /** Returns the wrapped {@link DocsEnum}. */
-    DocsEnum getWrapped() {
-      return in;
-    }
-  }
-  
-  static class SortingDocsAndPositionsEnum extends FilterDocsAndPositionsEnum {
-    
-    /**
-     * A {@link TimSorter} which sorts two parallel arrays of doc IDs and
-     * offsets in one go. Everytime a doc ID is 'swapped', its correponding offset
-     * is swapped too.
-     */
-    private static final class DocOffsetSorter extends TimSorter {
-      
-      private int[] docs;
-      private long[] offsets;
-      private final int[] tmpDocs;
-      private final long[] tmpOffsets;
-      
-      public DocOffsetSorter(int maxDoc) {
-        super(maxDoc / 64);
-        this.tmpDocs = new int[maxDoc / 64];
-        this.tmpOffsets = new long[maxDoc / 64];
-      }
-
-      public void reset(int[] docs, long[] offsets) {
-        this.docs = docs;
-        this.offsets = offsets;
-      }
-
-      @Override
-      protected int compare(int i, int j) {
-        return docs[i] - docs[j];
-      }
-      
-      @Override
-      protected void swap(int i, int j) {
-        int tmpDoc = docs[i];
-        docs[i] = docs[j];
-        docs[j] = tmpDoc;
-        
-        long tmpOffset = offsets[i];
-        offsets[i] = offsets[j];
-        offsets[j] = tmpOffset;
-      }
-
-      @Override
-      protected void copy(int src, int dest) {
-        docs[dest] = docs[src];
-        offsets[dest] = offsets[src];
-      }
-
-      @Override
-      protected void save(int i, int len) {
-        System.arraycopy(docs, i, tmpDocs, 0, len);
-        System.arraycopy(offsets, i, tmpOffsets, 0, len);
-      }
-
-      @Override
-      protected void restore(int i, int j) {
-        docs[j] = tmpDocs[i];
-        offsets[j] = tmpOffsets[i];
-      }
-
-      @Override
-      protected int compareSaved(int i, int j) {
-        return tmpDocs[i] - docs[j];
-      }
-    }
-    
-    private final int maxDoc;
-    private final DocOffsetSorter sorter;
-    private int[] docs;
-    private long[] offsets;
-    private final int upto;
-    
-    private final IndexInput postingInput;
-    private final boolean storeOffsets;
-    
-    private int docIt = -1;
-    private int pos;
-    private int startOffset = -1;
-    private int endOffset = -1;
-    private final BytesRef payload;
-    private int currFreq;
-
-    private final RAMFile file;
-
-    SortingDocsAndPositionsEnum(int maxDoc, SortingDocsAndPositionsEnum reuse, final DocsAndPositionsEnum in, Sorter.DocMap docMap, boolean storeOffsets) throws IOException {
-      super(in);
-      this.maxDoc = maxDoc;
-      this.storeOffsets = storeOffsets;
-      if (reuse != null) {
-        docs = reuse.docs;
-        offsets = reuse.offsets;
-        payload = reuse.payload;
-        file = reuse.file;
-        if (reuse.maxDoc == maxDoc) {
-          sorter = reuse.sorter;
-        } else {
-          sorter = new DocOffsetSorter(maxDoc);
-        }
-      } else {
-        docs = new int[32];
-        offsets = new long[32];
-        payload = new BytesRef(32);
-        file = new RAMFile();
-        sorter = new DocOffsetSorter(maxDoc);
-      }
-      final IndexOutput out = new RAMOutputStream(file, false);
-      int doc;
-      int i = 0;
-      while ((doc = in.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {
-        if (i == docs.length) {
-          final int newLength = ArrayUtil.oversize(i + 1, 4);
-          docs = Arrays.copyOf(docs, newLength);
-          offsets = Arrays.copyOf(offsets, newLength);
-        }
-        docs[i] = docMap.oldToNew(doc);
-        offsets[i] = out.getFilePointer();
-        addPositions(in, out);
-        i++;
-      }
-      upto = i;
-      sorter.reset(docs, offsets);
-      sorter.sort(0, upto);
-      out.close();
-      this.postingInput = new RAMInputStream("", file);
-    }
-
-    // for testing
-    boolean reused(DocsAndPositionsEnum other) {
-      if (other == null || !(other instanceof SortingDocsAndPositionsEnum)) {
-        return false;
-      }
-      return docs == ((SortingDocsAndPositionsEnum) other).docs;
-    }
-
-    private void addPositions(final DocsAndPositionsEnum in, final IndexOutput out) throws IOException {
-      int freq = in.freq();
-      out.writeVInt(freq);
-      int previousPosition = 0;
-      int previousEndOffset = 0;
-      for (int i = 0; i < freq; i++) {
-        final int pos = in.nextPosition();
-        final BytesRef payload = in.getPayload();
-        // The low-order bit of token is set only if there is a payload, the
-        // previous bits are the delta-encoded position. 
-        final int token = (pos - previousPosition) << 1 | (payload == null ? 0 : 1);
-        out.writeVInt(token);
-        previousPosition = pos;
-        if (storeOffsets) { // don't encode offsets if they are not stored
-          final int startOffset = in.startOffset();
-          final int endOffset = in.endOffset();
-          out.writeVInt(startOffset - previousEndOffset);
-          out.writeVInt(endOffset - startOffset);
-          previousEndOffset = endOffset;
-        }
-        if (payload != null) {
-          out.writeVInt(payload.length);
-          out.writeBytes(payload.bytes, payload.offset, payload.length);
-        }
-      }
-    }
-    
-    @Override
-    public int advance(final int target) throws IOException {
-      // need to support it for checkIndex, but in practice it won't be called, so
-      // don't bother to implement efficiently for now.
-      return slowAdvance(target);
-    }
-    
-    @Override
-    public int docID() {
-      return docIt < 0 ? -1 : docIt >= upto ? NO_MORE_DOCS : docs[docIt];
-    }
-    
-    @Override
-    public int endOffset() throws IOException {
-      return endOffset;
-    }
-    
-    @Override
-    public int freq() throws IOException {
-      return currFreq;
-    }
-    
-    @Override
-    public BytesRef getPayload() throws IOException {
-      return payload.length == 0 ? null : payload;
-    }
-    
-    @Override
-    public int nextDoc() throws IOException {
-      if (++docIt >= upto) return DocIdSetIterator.NO_MORE_DOCS;
-      postingInput.seek(offsets[docIt]);
-      currFreq = postingInput.readVInt();
-      // reset variables used in nextPosition
-      pos = 0;
-      endOffset = 0;
-      return docs[docIt];
-    }
-    
-    @Override
-    public int nextPosition() throws IOException {
-      final int token = postingInput.readVInt();
-      pos += token >>> 1;
-      if (storeOffsets) {
-        startOffset = endOffset + postingInput.readVInt();
-        endOffset = startOffset + postingInput.readVInt();
-      }
-      if ((token & 1) != 0) {
-        payload.offset = 0;
-        payload.length = postingInput.readVInt();
-        if (payload.length > payload.bytes.length) {
-          payload.bytes = new byte[ArrayUtil.oversize(payload.length, 1)];
-        }
-        postingInput.readBytes(payload.bytes, 0, payload.length);
-      } else {
-        payload.length = 0;
-      }
-      return pos;
-    }
-    
-    @Override
-    public int startOffset() throws IOException {
-      return startOffset;
-    }
-
-    /** Returns the wrapped {@link DocsAndPositionsEnum}. */
-    DocsAndPositionsEnum getWrapped() {
-      return in;
-    }
-  }
-
-  /** Return a sorted view of <code>reader</code> according to the order
-   *  defined by <code>sort</code>. If the reader is already sorted, this
-   *  method might return the reader as-is. */
-  public static AtomicReader wrap(AtomicReader reader, Sort sort) throws IOException {
-    return wrap(reader, new Sorter(sort).sort(reader));
-  }
-
-  /** Expert: same as {@link #wrap(AtomicReader, Sort)} but operates directly on a {@link Sorter.DocMap}. */
-  static AtomicReader wrap(AtomicReader reader, Sorter.DocMap docMap) {
-    if (docMap == null) {
-      // the reader is already sorter
-      return reader;
-    }
-    if (reader.maxDoc() != docMap.size()) {
-      throw new IllegalArgumentException("reader.maxDoc() should be equal to docMap.size(), got" + reader.maxDoc() + " != " + docMap.size());
-    }
-    assert Sorter.isConsistent(docMap);
-    return new SortingAtomicReader(reader, docMap);
-  }
-
-  final Sorter.DocMap docMap; // pkg-protected to avoid synthetic accessor methods
-
-  private SortingAtomicReader(final AtomicReader in, final Sorter.DocMap docMap) {
-    super(in);
-    this.docMap = docMap;
-  }
-
-  @Override
-  public void document(final int docID, final StoredFieldVisitor visitor) throws IOException {
-    in.document(docMap.newToOld(docID), visitor);
-  }
-  
-  @Override
-  public Fields fields() throws IOException {
-    Fields fields = in.fields();
-    if (fields == null) {
-      return null;
-    } else {
-      return new SortingFields(fields, in.getFieldInfos(), docMap);
-    }
-  }
-  
-  @Override
-  public BinaryDocValues getBinaryDocValues(String field) throws IOException {
-    BinaryDocValues oldDocValues = in.getBinaryDocValues(field);
-    if (oldDocValues == null) {
-      return null;
-    } else {
-      return new SortingBinaryDocValues(oldDocValues, docMap);
-    }
-  }
-  
-  @Override
-  public Bits getLiveDocs() {
-    final Bits inLiveDocs = in.getLiveDocs();
-    if (inLiveDocs == null) {
-      return null;
-    } else {
-      return new SortingBits(inLiveDocs, docMap);
-    }
-  }
-  
-  @Override
-  public NumericDocValues getNormValues(String field) throws IOException {
-    final NumericDocValues norm = in.getNormValues(field);
-    if (norm == null) {
-      return null;
-    } else {
-      return new SortingNumericDocValues(norm, docMap);
-    }
-  }
-
-  @Override
-  public NumericDocValues getNumericDocValues(String field) throws IOException {
-    final NumericDocValues oldDocValues = in.getNumericDocValues(field);
-    if (oldDocValues == null) return null;
-    return new SortingNumericDocValues(oldDocValues, docMap);
-  }
-  
-  @Override
-  public SortedNumericDocValues getSortedNumericDocValues(String field)
-      throws IOException {
-    final SortedNumericDocValues oldDocValues = in.getSortedNumericDocValues(field);
-    if (oldDocValues == null) {
-      return null;
-    } else {
-      return new SortingSortedNumericDocValues(oldDocValues, docMap);
-    }
-  }
-
-  @Override
-  public SortedDocValues getSortedDocValues(String field) throws IOException {
-    SortedDocValues sortedDV = in.getSortedDocValues(field);
-    if (sortedDV == null) {
-      return null;
-    } else {
-      return new SortingSortedDocValues(sortedDV, docMap);
-    }
-  }
-  
-  @Override
-  public SortedSetDocValues getSortedSetDocValues(String field) throws IOException {
-    SortedSetDocValues sortedSetDV = in.getSortedSetDocValues(field);
-    if (sortedSetDV == null) {
-      return null;
-    } else {
-      return new SortingSortedSetDocValues(sortedSetDV, docMap);
-    }  
-  }
-
-  @Override
-  public Bits getDocsWithField(String field) throws IOException {
-    Bits bits = in.getDocsWithField(field);
-    if (bits == null || bits instanceof Bits.MatchAllBits || bits instanceof Bits.MatchNoBits) {
-      return bits;
-    } else {
-      return new SortingBits(bits, docMap);
-    }
-  }
-
-  @Override
-  public Fields getTermVectors(final int docID) throws IOException {
-    return in.getTermVectors(docMap.newToOld(docID));
-  }
-  
-}
diff --git lucene/misc/src/java/org/apache/lucene/index/sorter/SortingLeafReader.java lucene/misc/src/java/org/apache/lucene/index/sorter/SortingLeafReader.java
new file mode 100644
index 0000000..f96bff7
--- /dev/null
+++ lucene/misc/src/java/org/apache/lucene/index/sorter/SortingLeafReader.java
@@ -0,0 +1,864 @@
+package org.apache.lucene.index.sorter;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Arrays;
+
+import org.apache.lucene.index.FilterLeafReader;
+import org.apache.lucene.index.LeafReader;
+import org.apache.lucene.index.BinaryDocValues;
+import org.apache.lucene.index.DocsAndPositionsEnum;
+import org.apache.lucene.index.DocsEnum;
+import org.apache.lucene.index.FieldInfo.IndexOptions;
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.Fields;
+import org.apache.lucene.index.NumericDocValues;
+import org.apache.lucene.index.SortedDocValues;
+import org.apache.lucene.index.SortedNumericDocValues;
+import org.apache.lucene.index.SortedSetDocValues;
+import org.apache.lucene.index.StoredFieldVisitor;
+import org.apache.lucene.index.Terms;
+import org.apache.lucene.index.TermsEnum;
+import org.apache.lucene.index.sorter.Sorter.DocMap;
+import org.apache.lucene.search.DocIdSetIterator;
+import org.apache.lucene.search.Sort;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.store.RAMFile;
+import org.apache.lucene.store.RAMInputStream;
+import org.apache.lucene.store.RAMOutputStream;
+import org.apache.lucene.util.ArrayUtil;
+import org.apache.lucene.util.Bits;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.TimSorter;
+import org.apache.lucene.util.automaton.CompiledAutomaton;
+
+/**
+ * An {@link org.apache.lucene.index.LeafReader} which supports sorting documents by a given
+ * {@link Sort}. You can use this class to sort an index as follows:
+ * 
+ * <pre class="prettyprint">
+ * IndexWriter writer; // writer to which the sorted index will be added
+ * DirectoryReader reader; // reader on the input index
+ * Sort sort; // determines how the documents are sorted
+ * AtomicReader sortingReader = SortingAtomicReader.wrap(SlowCompositeReaderWrapper.wrap(reader), sort);
+ * writer.addIndexes(reader);
+ * writer.close();
+ * reader.close();
+ * </pre>
+ * 
+ * @lucene.experimental
+ */
+public class SortingLeafReader extends FilterLeafReader {
+
+  private static class SortingFields extends FilterFields {
+
+    private final Sorter.DocMap docMap;
+    private final FieldInfos infos;
+
+    public SortingFields(final Fields in, FieldInfos infos, Sorter.DocMap docMap) {
+      super(in);
+      this.docMap = docMap;
+      this.infos = infos;
+    }
+
+    @Override
+    public Terms terms(final String field) throws IOException {
+      Terms terms = in.terms(field);
+      if (terms == null) {
+        return null;
+      } else {
+        return new SortingTerms(terms, infos.fieldInfo(field).getIndexOptions(), docMap);
+      }
+    }
+
+  }
+
+  private static class SortingTerms extends FilterTerms {
+
+    private final Sorter.DocMap docMap;
+    private final IndexOptions indexOptions;
+    
+    public SortingTerms(final Terms in, IndexOptions indexOptions, final Sorter.DocMap docMap) {
+      super(in);
+      this.docMap = docMap;
+      this.indexOptions = indexOptions;
+    }
+
+    @Override
+    public TermsEnum iterator(final TermsEnum reuse) throws IOException {
+      return new SortingTermsEnum(in.iterator(reuse), docMap, indexOptions);
+    }
+
+    @Override
+    public TermsEnum intersect(CompiledAutomaton compiled, BytesRef startTerm)
+        throws IOException {
+      return new SortingTermsEnum(in.intersect(compiled, startTerm), docMap, indexOptions);
+    }
+
+  }
+
+  private static class SortingTermsEnum extends FilterTermsEnum {
+
+    final Sorter.DocMap docMap; // pkg-protected to avoid synthetic accessor methods
+    private final IndexOptions indexOptions;
+    
+    public SortingTermsEnum(final TermsEnum in, Sorter.DocMap docMap, IndexOptions indexOptions) {
+      super(in);
+      this.docMap = docMap;
+      this.indexOptions = indexOptions;
+    }
+
+    Bits newToOld(final Bits liveDocs) {
+      if (liveDocs == null) {
+        return null;
+      }
+      return new Bits() {
+
+        @Override
+        public boolean get(int index) {
+          return liveDocs.get(docMap.oldToNew(index));
+        }
+
+        @Override
+        public int length() {
+          return liveDocs.length();
+        }
+
+      };
+    }
+
+    @Override
+    public DocsEnum docs(Bits liveDocs, DocsEnum reuse, final int flags) throws IOException {
+      final DocsEnum inReuse;
+      final SortingDocsEnum wrapReuse;
+      if (reuse != null && reuse instanceof SortingDocsEnum) {
+        // if we're asked to reuse the given DocsEnum and it is Sorting, return
+        // the wrapped one, since some Codecs expect it.
+        wrapReuse = (SortingDocsEnum) reuse;
+        inReuse = wrapReuse.getWrapped();
+      } else {
+        wrapReuse = null;
+        inReuse = reuse;
+      }
+
+      final DocsEnum inDocs = in.docs(newToOld(liveDocs), inReuse, flags);
+      final boolean withFreqs = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >=0 && (flags & DocsEnum.FLAG_FREQS) != 0;
+      return new SortingDocsEnum(docMap.size(), wrapReuse, inDocs, withFreqs, docMap);
+    }
+
+    @Override
+    public DocsAndPositionsEnum docsAndPositions(Bits liveDocs, DocsAndPositionsEnum reuse, final int flags) throws IOException {
+      final DocsAndPositionsEnum inReuse;
+      final SortingDocsAndPositionsEnum wrapReuse;
+      if (reuse != null && reuse instanceof SortingDocsAndPositionsEnum) {
+        // if we're asked to reuse the given DocsEnum and it is Sorting, return
+        // the wrapped one, since some Codecs expect it.
+        wrapReuse = (SortingDocsAndPositionsEnum) reuse;
+        inReuse = wrapReuse.getWrapped();
+      } else {
+        wrapReuse = null;
+        inReuse = reuse;
+      }
+
+      final DocsAndPositionsEnum inDocsAndPositions = in.docsAndPositions(newToOld(liveDocs), inReuse, flags);
+      if (inDocsAndPositions == null) {
+        return null;
+      }
+
+      // we ignore the fact that offsets may be stored but not asked for,
+      // since this code is expected to be used during addIndexes which will
+      // ask for everything. if that assumption changes in the future, we can
+      // factor in whether 'flags' says offsets are not required.
+      final boolean storeOffsets = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;
+      return new SortingDocsAndPositionsEnum(docMap.size(), wrapReuse, inDocsAndPositions, docMap, storeOffsets);
+    }
+
+  }
+
+  private static class SortingBinaryDocValues extends BinaryDocValues {
+    
+    private final BinaryDocValues in;
+    private final Sorter.DocMap docMap;
+    
+    SortingBinaryDocValues(BinaryDocValues in, Sorter.DocMap docMap) {
+      this.in = in;
+      this.docMap = docMap;
+    }
+
+    @Override
+    public BytesRef get(int docID) {
+      return in.get(docMap.newToOld(docID));
+    }
+  }
+  
+  private static class SortingNumericDocValues extends NumericDocValues {
+
+    private final NumericDocValues in;
+    private final Sorter.DocMap docMap;
+
+    public SortingNumericDocValues(final NumericDocValues in, Sorter.DocMap docMap) {
+      this.in = in;
+      this.docMap = docMap;
+    }
+
+    @Override
+    public long get(int docID) {
+      return in.get(docMap.newToOld(docID));
+    }
+  }
+  
+  private static class SortingSortedNumericDocValues extends SortedNumericDocValues {
+    
+    private final SortedNumericDocValues in;
+    private final Sorter.DocMap docMap;
+    
+    SortingSortedNumericDocValues(SortedNumericDocValues in, DocMap docMap) {
+      this.in = in;
+      this.docMap = docMap;
+    }
+    
+    @Override
+    public int count() {
+      return in.count();
+    }
+    
+    @Override
+    public void setDocument(int doc) {
+      in.setDocument(docMap.newToOld(doc));
+    }
+    
+    @Override
+    public long valueAt(int index) {
+      return in.valueAt(index);
+    }
+  }
+  
+  private static class SortingBits implements Bits {
+
+    private final Bits in;
+    private final Sorter.DocMap docMap;
+
+    public SortingBits(final Bits in, Sorter.DocMap docMap) {
+      this.in = in;
+      this.docMap = docMap;
+    }
+
+    @Override
+    public boolean get(int index) {
+      return in.get(docMap.newToOld(index));
+    }
+
+    @Override
+    public int length() {
+      return in.length();
+    }
+  }
+  
+  private static class SortingSortedDocValues extends SortedDocValues {
+    
+    private final SortedDocValues in;
+    private final Sorter.DocMap docMap;
+    
+    SortingSortedDocValues(SortedDocValues in, Sorter.DocMap docMap) {
+      this.in = in;
+      this.docMap = docMap;
+    }
+
+    @Override
+    public int getOrd(int docID) {
+      return in.getOrd(docMap.newToOld(docID));
+    }
+
+    @Override
+    public BytesRef lookupOrd(int ord) {
+      return in.lookupOrd(ord);
+    }
+
+    @Override
+    public int getValueCount() {
+      return in.getValueCount();
+    }
+
+    @Override
+    public BytesRef get(int docID) {
+      return in.get(docMap.newToOld(docID));
+    }
+
+    @Override
+    public int lookupTerm(BytesRef key) {
+      return in.lookupTerm(key);
+    }
+  }
+  
+  private static class SortingSortedSetDocValues extends SortedSetDocValues {
+    
+    private final SortedSetDocValues in;
+    private final Sorter.DocMap docMap;
+    
+    SortingSortedSetDocValues(SortedSetDocValues in, Sorter.DocMap docMap) {
+      this.in = in;
+      this.docMap = docMap;
+    }
+
+    @Override
+    public long nextOrd() {
+      return in.nextOrd();
+    }
+
+    @Override
+    public void setDocument(int docID) {
+      in.setDocument(docMap.newToOld(docID));
+    }
+
+    @Override
+    public BytesRef lookupOrd(long ord) {
+      return in.lookupOrd(ord);
+    }
+
+    @Override
+    public long getValueCount() {
+      return in.getValueCount();
+    }
+
+    @Override
+    public long lookupTerm(BytesRef key) {
+      return in.lookupTerm(key);
+    }
+  }
+
+  static class SortingDocsEnum extends FilterDocsEnum {
+    
+    private static final class DocFreqSorter extends TimSorter {
+      
+      private int[] docs;
+      private int[] freqs;
+      private final int[] tmpDocs;
+      private int[] tmpFreqs;
+      
+      public DocFreqSorter(int maxDoc) {
+        super(maxDoc / 64);
+        this.tmpDocs = new int[maxDoc / 64];
+      }
+
+      public void reset(int[] docs, int[] freqs) {
+        this.docs = docs;
+        this.freqs = freqs;
+        if (freqs != null && tmpFreqs == null) {
+          tmpFreqs = new int[tmpDocs.length];
+        }
+      }
+
+      @Override
+      protected int compare(int i, int j) {
+        return docs[i] - docs[j];
+      }
+      
+      @Override
+      protected void swap(int i, int j) {
+        int tmpDoc = docs[i];
+        docs[i] = docs[j];
+        docs[j] = tmpDoc;
+        
+        if (freqs != null) {
+          int tmpFreq = freqs[i];
+          freqs[i] = freqs[j];
+          freqs[j] = tmpFreq;
+        }
+      }
+
+      @Override
+      protected void copy(int src, int dest) {
+        docs[dest] = docs[src];
+        if (freqs != null) {
+          freqs[dest] = freqs[src];
+        }
+      }
+
+      @Override
+      protected void save(int i, int len) {
+        System.arraycopy(docs, i, tmpDocs, 0, len);
+        if (freqs != null) {
+          System.arraycopy(freqs, i, tmpFreqs, 0, len);
+        }
+      }
+
+      @Override
+      protected void restore(int i, int j) {
+        docs[j] = tmpDocs[i];
+        if (freqs != null) {
+          freqs[j] = tmpFreqs[i];
+        }
+      }
+
+      @Override
+      protected int compareSaved(int i, int j) {
+        return tmpDocs[i] - docs[j];
+      }
+    }
+
+    private final int maxDoc;
+    private final DocFreqSorter sorter;
+    private int[] docs;
+    private int[] freqs;
+    private int docIt = -1;
+    private final int upto;
+    private final boolean withFreqs;
+
+    SortingDocsEnum(int maxDoc, SortingDocsEnum reuse, final DocsEnum in, boolean withFreqs, final Sorter.DocMap docMap) throws IOException {
+      super(in);
+      this.maxDoc = maxDoc;
+      this.withFreqs = withFreqs;
+      if (reuse != null) {
+        if (reuse.maxDoc == maxDoc) {
+          sorter = reuse.sorter;
+        } else {
+          sorter = new DocFreqSorter(maxDoc);
+        }
+        docs = reuse.docs;
+        freqs = reuse.freqs; // maybe null
+      } else {
+        docs = new int[64];
+        sorter = new DocFreqSorter(maxDoc);
+      }
+      docIt = -1;
+      int i = 0;
+      int doc;
+      if (withFreqs) {
+        if (freqs == null || freqs.length < docs.length) {
+          freqs = new int[docs.length];
+        }
+        while ((doc = in.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS){
+          if (i >= docs.length) {
+            docs = ArrayUtil.grow(docs, docs.length + 1);
+            freqs = ArrayUtil.grow(freqs, freqs.length + 1);
+          }
+          docs[i] = docMap.oldToNew(doc);
+          freqs[i] = in.freq();
+          ++i;
+        }
+      } else {
+        freqs = null;
+        while ((doc = in.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS){
+          if (i >= docs.length) {
+            docs = ArrayUtil.grow(docs, docs.length + 1);
+          }
+          docs[i++] = docMap.oldToNew(doc);
+        }
+      }
+      // TimSort can save much time compared to other sorts in case of
+      // reverse sorting, or when sorting a concatenation of sorted readers
+      sorter.reset(docs, freqs);
+      sorter.sort(0, i);
+      upto = i;
+    }
+
+    // for testing
+    boolean reused(DocsEnum other) {
+      if (other == null || !(other instanceof SortingDocsEnum)) {
+        return false;
+      }
+      return docs == ((SortingDocsEnum) other).docs;
+    }
+
+    @Override
+    public int advance(final int target) throws IOException {
+      // need to support it for checkIndex, but in practice it won't be called, so
+      // don't bother to implement efficiently for now.
+      return slowAdvance(target);
+    }
+    
+    @Override
+    public int docID() {
+      return docIt < 0 ? -1 : docIt >= upto ? NO_MORE_DOCS : docs[docIt];
+    }
+    
+    @Override
+    public int freq() throws IOException {
+      return withFreqs && docIt < upto ? freqs[docIt] : 1;
+    }
+    
+    @Override
+    public int nextDoc() throws IOException {
+      if (++docIt >= upto) return NO_MORE_DOCS;
+      return docs[docIt];
+    }
+    
+    /** Returns the wrapped {@link DocsEnum}. */
+    DocsEnum getWrapped() {
+      return in;
+    }
+  }
+  
+  static class SortingDocsAndPositionsEnum extends FilterDocsAndPositionsEnum {
+    
+    /**
+     * A {@link TimSorter} which sorts two parallel arrays of doc IDs and
+     * offsets in one go. Everytime a doc ID is 'swapped', its correponding offset
+     * is swapped too.
+     */
+    private static final class DocOffsetSorter extends TimSorter {
+      
+      private int[] docs;
+      private long[] offsets;
+      private final int[] tmpDocs;
+      private final long[] tmpOffsets;
+      
+      public DocOffsetSorter(int maxDoc) {
+        super(maxDoc / 64);
+        this.tmpDocs = new int[maxDoc / 64];
+        this.tmpOffsets = new long[maxDoc / 64];
+      }
+
+      public void reset(int[] docs, long[] offsets) {
+        this.docs = docs;
+        this.offsets = offsets;
+      }
+
+      @Override
+      protected int compare(int i, int j) {
+        return docs[i] - docs[j];
+      }
+      
+      @Override
+      protected void swap(int i, int j) {
+        int tmpDoc = docs[i];
+        docs[i] = docs[j];
+        docs[j] = tmpDoc;
+        
+        long tmpOffset = offsets[i];
+        offsets[i] = offsets[j];
+        offsets[j] = tmpOffset;
+      }
+
+      @Override
+      protected void copy(int src, int dest) {
+        docs[dest] = docs[src];
+        offsets[dest] = offsets[src];
+      }
+
+      @Override
+      protected void save(int i, int len) {
+        System.arraycopy(docs, i, tmpDocs, 0, len);
+        System.arraycopy(offsets, i, tmpOffsets, 0, len);
+      }
+
+      @Override
+      protected void restore(int i, int j) {
+        docs[j] = tmpDocs[i];
+        offsets[j] = tmpOffsets[i];
+      }
+
+      @Override
+      protected int compareSaved(int i, int j) {
+        return tmpDocs[i] - docs[j];
+      }
+    }
+    
+    private final int maxDoc;
+    private final DocOffsetSorter sorter;
+    private int[] docs;
+    private long[] offsets;
+    private final int upto;
+    
+    private final IndexInput postingInput;
+    private final boolean storeOffsets;
+    
+    private int docIt = -1;
+    private int pos;
+    private int startOffset = -1;
+    private int endOffset = -1;
+    private final BytesRef payload;
+    private int currFreq;
+
+    private final RAMFile file;
+
+    SortingDocsAndPositionsEnum(int maxDoc, SortingDocsAndPositionsEnum reuse, final DocsAndPositionsEnum in, Sorter.DocMap docMap, boolean storeOffsets) throws IOException {
+      super(in);
+      this.maxDoc = maxDoc;
+      this.storeOffsets = storeOffsets;
+      if (reuse != null) {
+        docs = reuse.docs;
+        offsets = reuse.offsets;
+        payload = reuse.payload;
+        file = reuse.file;
+        if (reuse.maxDoc == maxDoc) {
+          sorter = reuse.sorter;
+        } else {
+          sorter = new DocOffsetSorter(maxDoc);
+        }
+      } else {
+        docs = new int[32];
+        offsets = new long[32];
+        payload = new BytesRef(32);
+        file = new RAMFile();
+        sorter = new DocOffsetSorter(maxDoc);
+      }
+      final IndexOutput out = new RAMOutputStream(file, false);
+      int doc;
+      int i = 0;
+      while ((doc = in.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {
+        if (i == docs.length) {
+          final int newLength = ArrayUtil.oversize(i + 1, 4);
+          docs = Arrays.copyOf(docs, newLength);
+          offsets = Arrays.copyOf(offsets, newLength);
+        }
+        docs[i] = docMap.oldToNew(doc);
+        offsets[i] = out.getFilePointer();
+        addPositions(in, out);
+        i++;
+      }
+      upto = i;
+      sorter.reset(docs, offsets);
+      sorter.sort(0, upto);
+      out.close();
+      this.postingInput = new RAMInputStream("", file);
+    }
+
+    // for testing
+    boolean reused(DocsAndPositionsEnum other) {
+      if (other == null || !(other instanceof SortingDocsAndPositionsEnum)) {
+        return false;
+      }
+      return docs == ((SortingDocsAndPositionsEnum) other).docs;
+    }
+
+    private void addPositions(final DocsAndPositionsEnum in, final IndexOutput out) throws IOException {
+      int freq = in.freq();
+      out.writeVInt(freq);
+      int previousPosition = 0;
+      int previousEndOffset = 0;
+      for (int i = 0; i < freq; i++) {
+        final int pos = in.nextPosition();
+        final BytesRef payload = in.getPayload();
+        // The low-order bit of token is set only if there is a payload, the
+        // previous bits are the delta-encoded position. 
+        final int token = (pos - previousPosition) << 1 | (payload == null ? 0 : 1);
+        out.writeVInt(token);
+        previousPosition = pos;
+        if (storeOffsets) { // don't encode offsets if they are not stored
+          final int startOffset = in.startOffset();
+          final int endOffset = in.endOffset();
+          out.writeVInt(startOffset - previousEndOffset);
+          out.writeVInt(endOffset - startOffset);
+          previousEndOffset = endOffset;
+        }
+        if (payload != null) {
+          out.writeVInt(payload.length);
+          out.writeBytes(payload.bytes, payload.offset, payload.length);
+        }
+      }
+    }
+    
+    @Override
+    public int advance(final int target) throws IOException {
+      // need to support it for checkIndex, but in practice it won't be called, so
+      // don't bother to implement efficiently for now.
+      return slowAdvance(target);
+    }
+    
+    @Override
+    public int docID() {
+      return docIt < 0 ? -1 : docIt >= upto ? NO_MORE_DOCS : docs[docIt];
+    }
+    
+    @Override
+    public int endOffset() throws IOException {
+      return endOffset;
+    }
+    
+    @Override
+    public int freq() throws IOException {
+      return currFreq;
+    }
+    
+    @Override
+    public BytesRef getPayload() throws IOException {
+      return payload.length == 0 ? null : payload;
+    }
+    
+    @Override
+    public int nextDoc() throws IOException {
+      if (++docIt >= upto) return DocIdSetIterator.NO_MORE_DOCS;
+      postingInput.seek(offsets[docIt]);
+      currFreq = postingInput.readVInt();
+      // reset variables used in nextPosition
+      pos = 0;
+      endOffset = 0;
+      return docs[docIt];
+    }
+    
+    @Override
+    public int nextPosition() throws IOException {
+      final int token = postingInput.readVInt();
+      pos += token >>> 1;
+      if (storeOffsets) {
+        startOffset = endOffset + postingInput.readVInt();
+        endOffset = startOffset + postingInput.readVInt();
+      }
+      if ((token & 1) != 0) {
+        payload.offset = 0;
+        payload.length = postingInput.readVInt();
+        if (payload.length > payload.bytes.length) {
+          payload.bytes = new byte[ArrayUtil.oversize(payload.length, 1)];
+        }
+        postingInput.readBytes(payload.bytes, 0, payload.length);
+      } else {
+        payload.length = 0;
+      }
+      return pos;
+    }
+    
+    @Override
+    public int startOffset() throws IOException {
+      return startOffset;
+    }
+
+    /** Returns the wrapped {@link DocsAndPositionsEnum}. */
+    DocsAndPositionsEnum getWrapped() {
+      return in;
+    }
+  }
+
+  /** Return a sorted view of <code>reader</code> according to the order
+   *  defined by <code>sort</code>. If the reader is already sorted, this
+   *  method might return the reader as-is. */
+  public static LeafReader wrap(LeafReader reader, Sort sort) throws IOException {
+    return wrap(reader, new Sorter(sort).sort(reader));
+  }
+
+  /** Expert: same as {@link #wrap(org.apache.lucene.index.LeafReader, Sort)} but operates directly on a {@link Sorter.DocMap}. */
+  static LeafReader wrap(LeafReader reader, Sorter.DocMap docMap) {
+    if (docMap == null) {
+      // the reader is already sorter
+      return reader;
+    }
+    if (reader.maxDoc() != docMap.size()) {
+      throw new IllegalArgumentException("reader.maxDoc() should be equal to docMap.size(), got" + reader.maxDoc() + " != " + docMap.size());
+    }
+    assert Sorter.isConsistent(docMap);
+    return new SortingLeafReader(reader, docMap);
+  }
+
+  final Sorter.DocMap docMap; // pkg-protected to avoid synthetic accessor methods
+
+  private SortingLeafReader(final LeafReader in, final Sorter.DocMap docMap) {
+    super(in);
+    this.docMap = docMap;
+  }
+
+  @Override
+  public void document(final int docID, final StoredFieldVisitor visitor) throws IOException {
+    in.document(docMap.newToOld(docID), visitor);
+  }
+  
+  @Override
+  public Fields fields() throws IOException {
+    Fields fields = in.fields();
+    if (fields == null) {
+      return null;
+    } else {
+      return new SortingFields(fields, in.getFieldInfos(), docMap);
+    }
+  }
+  
+  @Override
+  public BinaryDocValues getBinaryDocValues(String field) throws IOException {
+    BinaryDocValues oldDocValues = in.getBinaryDocValues(field);
+    if (oldDocValues == null) {
+      return null;
+    } else {
+      return new SortingBinaryDocValues(oldDocValues, docMap);
+    }
+  }
+  
+  @Override
+  public Bits getLiveDocs() {
+    final Bits inLiveDocs = in.getLiveDocs();
+    if (inLiveDocs == null) {
+      return null;
+    } else {
+      return new SortingBits(inLiveDocs, docMap);
+    }
+  }
+  
+  @Override
+  public NumericDocValues getNormValues(String field) throws IOException {
+    final NumericDocValues norm = in.getNormValues(field);
+    if (norm == null) {
+      return null;
+    } else {
+      return new SortingNumericDocValues(norm, docMap);
+    }
+  }
+
+  @Override
+  public NumericDocValues getNumericDocValues(String field) throws IOException {
+    final NumericDocValues oldDocValues = in.getNumericDocValues(field);
+    if (oldDocValues == null) return null;
+    return new SortingNumericDocValues(oldDocValues, docMap);
+  }
+  
+  @Override
+  public SortedNumericDocValues getSortedNumericDocValues(String field)
+      throws IOException {
+    final SortedNumericDocValues oldDocValues = in.getSortedNumericDocValues(field);
+    if (oldDocValues == null) {
+      return null;
+    } else {
+      return new SortingSortedNumericDocValues(oldDocValues, docMap);
+    }
+  }
+
+  @Override
+  public SortedDocValues getSortedDocValues(String field) throws IOException {
+    SortedDocValues sortedDV = in.getSortedDocValues(field);
+    if (sortedDV == null) {
+      return null;
+    } else {
+      return new SortingSortedDocValues(sortedDV, docMap);
+    }
+  }
+  
+  @Override
+  public SortedSetDocValues getSortedSetDocValues(String field) throws IOException {
+    SortedSetDocValues sortedSetDV = in.getSortedSetDocValues(field);
+    if (sortedSetDV == null) {
+      return null;
+    } else {
+      return new SortingSortedSetDocValues(sortedSetDV, docMap);
+    }  
+  }
+
+  @Override
+  public Bits getDocsWithField(String field) throws IOException {
+    Bits bits = in.getDocsWithField(field);
+    if (bits == null || bits instanceof Bits.MatchAllBits || bits instanceof Bits.MatchNoBits) {
+      return bits;
+    } else {
+      return new SortingBits(bits, docMap);
+    }
+  }
+
+  @Override
+  public Fields getTermVectors(final int docID) throws IOException {
+    return in.getTermVectors(docMap.newToOld(docID));
+  }
+  
+}
diff --git lucene/misc/src/java/org/apache/lucene/index/sorter/SortingMergePolicy.java lucene/misc/src/java/org/apache/lucene/index/sorter/SortingMergePolicy.java
index fc37adf..7c0f781 100644
--- lucene/misc/src/java/org/apache/lucene/index/sorter/SortingMergePolicy.java
+++ lucene/misc/src/java/org/apache/lucene/index/sorter/SortingMergePolicy.java
@@ -23,7 +23,7 @@ import java.util.List;
 import java.util.Map;
 
 import org.apache.lucene.analysis.Analyzer; // javadocs
-import org.apache.lucene.index.AtomicReader;
+import org.apache.lucene.index.LeafReader;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexWriter;
 import org.apache.lucene.index.MergePolicy;
@@ -64,27 +64,27 @@ public final class SortingMergePolicy extends MergePolicy {
   
   class SortingOneMerge extends OneMerge {
 
-    List<AtomicReader> unsortedReaders;
+    List<LeafReader> unsortedReaders;
     Sorter.DocMap docMap;
-    AtomicReader sortedView;
+    LeafReader sortedView;
 
     SortingOneMerge(List<SegmentCommitInfo> segments) {
       super(segments);
     }
 
     @Override
-    public List<AtomicReader> getMergeReaders() throws IOException {
+    public List<LeafReader> getMergeReaders() throws IOException {
       if (unsortedReaders == null) {
         unsortedReaders = super.getMergeReaders();
-        final AtomicReader atomicView;
+        final LeafReader atomicView;
         if (unsortedReaders.size() == 1) {
           atomicView = unsortedReaders.get(0);
         } else {
-          final IndexReader multiReader = new MultiReader(unsortedReaders.toArray(new AtomicReader[unsortedReaders.size()]));
+          final IndexReader multiReader = new MultiReader(unsortedReaders.toArray(new LeafReader[unsortedReaders.size()]));
           atomicView = SlowCompositeReaderWrapper.wrap(multiReader);
         }
         docMap = sorter.sort(atomicView);
-        sortedView = SortingAtomicReader.wrap(atomicView, docMap);
+        sortedView = SortingLeafReader.wrap(atomicView, docMap);
       }
       // a null doc map means that the readers are already sorted
       return docMap == null ? unsortedReaders : Collections.singletonList(sortedView);
@@ -97,10 +97,10 @@ public final class SortingMergePolicy extends MergePolicy {
       super.setInfo(info);
     }
 
-    private PackedLongValues getDeletes(List<AtomicReader> readers) {
+    private PackedLongValues getDeletes(List<LeafReader> readers) {
       PackedLongValues.Builder deletes = PackedLongValues.monotonicBuilder(PackedInts.COMPACT);
       int deleteCount = 0;
-      for (AtomicReader reader : readers) {
+      for (LeafReader reader : readers) {
         final int maxDoc = reader.maxDoc();
         final Bits liveDocs = reader.getLiveDocs();
         for (int i = 0; i < maxDoc; ++i) {
@@ -151,7 +151,7 @@ public final class SortingMergePolicy extends MergePolicy {
   }
 
   /** Returns {@code true} if the given {@code reader} is sorted by the specified {@code sort}. */
-  public static boolean isSorted(AtomicReader reader, Sort sort) {
+  public static boolean isSorted(LeafReader reader, Sort sort) {
     if (reader instanceof SegmentReader) {
       final SegmentReader segReader = (SegmentReader) reader;
       final Map<String, String> diagnostics = segReader.getSegmentInfo().info.getDiagnostics();
diff --git lucene/misc/src/java/org/apache/lucene/uninverting/DocTermOrds.java lucene/misc/src/java/org/apache/lucene/uninverting/DocTermOrds.java
index 3c1acd1..012047f 100644
--- lucene/misc/src/java/org/apache/lucene/uninverting/DocTermOrds.java
+++ lucene/misc/src/java/org/apache/lucene/uninverting/DocTermOrds.java
@@ -24,7 +24,7 @@ import java.util.Collections;
 import java.util.List;
 
 import org.apache.lucene.codecs.PostingsFormat; // javadocs
-import org.apache.lucene.index.AtomicReader;
+import org.apache.lucene.index.LeafReader;
 import org.apache.lucene.index.DocValues;
 import org.apache.lucene.index.DocsAndPositionsEnum;
 import org.apache.lucene.index.DocsEnum;
@@ -188,21 +188,21 @@ public class DocTermOrds implements Accountable {
   }
 
   /** Inverts all terms */
-  public DocTermOrds(AtomicReader reader, Bits liveDocs, String field) throws IOException {
+  public DocTermOrds(LeafReader reader, Bits liveDocs, String field) throws IOException {
     this(reader, liveDocs, field, null, Integer.MAX_VALUE);
   }
   
   // TODO: instead of all these ctors and options, take termsenum!
 
   /** Inverts only terms starting w/ prefix */
-  public DocTermOrds(AtomicReader reader, Bits liveDocs, String field, BytesRef termPrefix) throws IOException {
+  public DocTermOrds(LeafReader reader, Bits liveDocs, String field, BytesRef termPrefix) throws IOException {
     this(reader, liveDocs, field, termPrefix, Integer.MAX_VALUE);
   }
 
   /** Inverts only terms starting w/ prefix, and only terms
    *  whose docFreq (not taking deletions into account) is
    *  <=  maxTermDocFreq */
-  public DocTermOrds(AtomicReader reader, Bits liveDocs, String field, BytesRef termPrefix, int maxTermDocFreq) throws IOException {
+  public DocTermOrds(LeafReader reader, Bits liveDocs, String field, BytesRef termPrefix, int maxTermDocFreq) throws IOException {
     this(reader, liveDocs, field, termPrefix, maxTermDocFreq, DEFAULT_INDEX_INTERVAL_BITS);
   }
 
@@ -210,7 +210,7 @@ public class DocTermOrds implements Accountable {
    *  whose docFreq (not taking deletions into account) is
    *  <=  maxTermDocFreq, with a custom indexing interval
    *  (default is every 128nd term). */
-  public DocTermOrds(AtomicReader reader, Bits liveDocs, String field, BytesRef termPrefix, int maxTermDocFreq, int indexIntervalBits) throws IOException {
+  public DocTermOrds(LeafReader reader, Bits liveDocs, String field, BytesRef termPrefix, int maxTermDocFreq, int indexIntervalBits) throws IOException {
     this(field, maxTermDocFreq, indexIntervalBits);
     uninvert(reader, liveDocs, termPrefix);
   }
@@ -236,7 +236,7 @@ public class DocTermOrds implements Accountable {
    *
    *  <p><b>NOTE</b>: you must pass the same reader that was
    *  used when creating this class */
-  public TermsEnum getOrdTermsEnum(AtomicReader reader) throws IOException {
+  public TermsEnum getOrdTermsEnum(LeafReader reader) throws IOException {
     if (indexedTermsArray == null) {
       //System.out.println("GET normal enum");
       final Fields fields = reader.fields();
@@ -273,14 +273,14 @@ public class DocTermOrds implements Accountable {
   protected void visitTerm(TermsEnum te, int termNum) throws IOException {
   }
 
-  /** Invoked during {@link #uninvert(AtomicReader,Bits,BytesRef)}
+  /** Invoked during {@link #uninvert(org.apache.lucene.index.LeafReader,Bits,BytesRef)}
    *  to record the document frequency for each uninverted
    *  term. */
   protected void setActualDocFreq(int termNum, int df) throws IOException {
   }
 
   /** Call this only once (if you subclass!) */
-  protected void uninvert(final AtomicReader reader, Bits liveDocs, final BytesRef termPrefix) throws IOException {
+  protected void uninvert(final LeafReader reader, Bits liveDocs, final BytesRef termPrefix) throws IOException {
     final FieldInfo info = reader.getFieldInfos().fieldInfo(field);
     if (info != null && info.hasDocValues()) {
       throw new IllegalStateException("Type mismatch: " + field + " was indexed as " + info.getDocValuesType());
@@ -622,7 +622,7 @@ public class DocTermOrds implements Accountable {
     private BytesRef term;
     private long ord = -indexInterval-1;          // force "real" seek
     
-    public OrdWrappedTermsEnum(AtomicReader reader) throws IOException {
+    public OrdWrappedTermsEnum(LeafReader reader) throws IOException {
       assert indexedTermsArray != null;
       termsEnum = reader.fields().terms(field).iterator(null);
     }
@@ -777,7 +777,7 @@ public class DocTermOrds implements Accountable {
   }
   
   /** Returns a SortedSetDocValues view of this instance */
-  public SortedSetDocValues iterator(AtomicReader reader) throws IOException {
+  public SortedSetDocValues iterator(LeafReader reader) throws IOException {
     if (isEmpty()) {
       return DocValues.emptySortedSet();
     } else {
@@ -786,7 +786,7 @@ public class DocTermOrds implements Accountable {
   }
   
   private class Iterator extends SortedSetDocValues {
-    final AtomicReader reader;
+    final LeafReader reader;
     final TermsEnum te;  // used internally for lookupOrd() and lookupTerm()
     // currently we read 5 at a time (using the logic of the old iterator)
     final int buffer[] = new int[5];
@@ -797,7 +797,7 @@ public class DocTermOrds implements Accountable {
     private int upto;
     private byte[] arr;
     
-    Iterator(AtomicReader reader) throws IOException {
+    Iterator(LeafReader reader) throws IOException {
       this.reader = reader;
       this.te = termsEnum();
     }
diff --git lucene/misc/src/java/org/apache/lucene/uninverting/FieldCache.java lucene/misc/src/java/org/apache/lucene/uninverting/FieldCache.java
index f264167..c29b95b 100644
--- lucene/misc/src/java/org/apache/lucene/uninverting/FieldCache.java
+++ lucene/misc/src/java/org/apache/lucene/uninverting/FieldCache.java
@@ -27,7 +27,7 @@ import org.apache.lucene.document.FloatField;
 import org.apache.lucene.document.IntField;
 import org.apache.lucene.document.LongField;
 import org.apache.lucene.document.NumericDocValuesField;
-import org.apache.lucene.index.AtomicReader;
+import org.apache.lucene.index.LeafReader;
 import org.apache.lucene.index.BinaryDocValues;
 import org.apache.lucene.index.IndexReader; // javadocs
 import org.apache.lucene.index.NumericDocValues;
@@ -184,12 +184,12 @@ interface FieldCache {
    *  <code>reader.maxDoc()</code>, with turned on bits for each docid that 
    *  does have a value for this field.
    */
-  public Bits getDocsWithField(AtomicReader reader, String field) throws IOException;
+  public Bits getDocsWithField(LeafReader reader, String field) throws IOException;
 
   /**
    * Returns a {@link NumericDocValues} over the values found in documents in the given
    * field. If the field was indexed as {@link NumericDocValuesField}, it simply
-   * uses {@link AtomicReader#getNumericDocValues(String)} to read the values.
+   * uses {@link org.apache.lucene.index.LeafReader#getNumericDocValues(String)} to read the values.
    * Otherwise, it checks the internal cache for an appropriate entry, and if
    * none is found, reads the terms in <code>field</code> as longs and returns
    * an array of size <code>reader.maxDoc()</code> of the value each document
@@ -210,7 +210,7 @@ interface FieldCache {
    * @throws IOException
    *           If any error occurs.
    */
-  public NumericDocValues getNumerics(AtomicReader reader, String field, Parser parser, boolean setDocsWithField) throws IOException;
+  public NumericDocValues getNumerics(LeafReader reader, String field, Parser parser, boolean setDocsWithField) throws IOException;
 
   /** Checks the internal cache for an appropriate entry, and if none
    * is found, reads the term values in <code>field</code>
@@ -223,14 +223,14 @@ interface FieldCache {
    * @return The values in the given field for each document.
    * @throws IOException  If any error occurs.
    */
-  public BinaryDocValues getTerms(AtomicReader reader, String field, boolean setDocsWithField) throws IOException;
+  public BinaryDocValues getTerms(LeafReader reader, String field, boolean setDocsWithField) throws IOException;
 
-  /** Expert: just like {@link #getTerms(AtomicReader,String,boolean)},
+  /** Expert: just like {@link #getTerms(org.apache.lucene.index.LeafReader,String,boolean)},
    *  but you can specify whether more RAM should be consumed in exchange for
    *  faster lookups (default is "true").  Note that the
    *  first call for a given reader and field "wins",
    *  subsequent calls will share the same cache entry. */
-  public BinaryDocValues getTerms(AtomicReader reader, String field, boolean setDocsWithField, float acceptableOverheadRatio) throws IOException;
+  public BinaryDocValues getTerms(LeafReader reader, String field, boolean setDocsWithField, float acceptableOverheadRatio) throws IOException;
 
   /** Checks the internal cache for an appropriate entry, and if none
    * is found, reads the term values in <code>field</code>
@@ -242,15 +242,15 @@ interface FieldCache {
    * @return The values in the given field for each document.
    * @throws IOException  If any error occurs.
    */
-  public SortedDocValues getTermsIndex(AtomicReader reader, String field) throws IOException;
+  public SortedDocValues getTermsIndex(LeafReader reader, String field) throws IOException;
 
   /** Expert: just like {@link
-   *  #getTermsIndex(AtomicReader,String)}, but you can specify
+   *  #getTermsIndex(org.apache.lucene.index.LeafReader,String)}, but you can specify
    *  whether more RAM should be consumed in exchange for
    *  faster lookups (default is "true").  Note that the
    *  first call for a given reader and field "wins",
    *  subsequent calls will share the same cache entry. */
-  public SortedDocValues getTermsIndex(AtomicReader reader, String field, float acceptableOverheadRatio) throws IOException;
+  public SortedDocValues getTermsIndex(LeafReader reader, String field, float acceptableOverheadRatio) throws IOException;
 
   /** Can be passed to {@link #getDocTermOrds} to filter for 32-bit numeric terms */
   public static final BytesRef INT32_TERM_PREFIX = new BytesRef(new byte[] { NumericUtils.SHIFT_START_INT });
@@ -270,7 +270,7 @@ interface FieldCache {
    * @return a {@link DocTermOrds} instance
    * @throws IOException  If any error occurs.
    */
-  public SortedSetDocValues getDocTermOrds(AtomicReader reader, String field, BytesRef prefix) throws IOException;
+  public SortedSetDocValues getDocTermOrds(LeafReader reader, String field, BytesRef prefix) throws IOException;
 
   /**
    * EXPERT: A unique Identifier/Description for each item in the FieldCache. 
diff --git lucene/misc/src/java/org/apache/lucene/uninverting/FieldCacheImpl.java lucene/misc/src/java/org/apache/lucene/uninverting/FieldCacheImpl.java
index 8856292..b65563d 100644
--- lucene/misc/src/java/org/apache/lucene/uninverting/FieldCacheImpl.java
+++ lucene/misc/src/java/org/apache/lucene/uninverting/FieldCacheImpl.java
@@ -26,7 +26,7 @@ import java.util.List;
 import java.util.Map;
 import java.util.WeakHashMap;
 
-import org.apache.lucene.index.AtomicReader;
+import org.apache.lucene.index.LeafReader;
 import org.apache.lucene.index.BinaryDocValues;
 import org.apache.lucene.index.DocValues;
 import org.apache.lucene.index.DocsEnum;
@@ -114,7 +114,7 @@ class FieldCacheImpl implements FieldCache {
     }
   };
   
-  private void initReader(AtomicReader reader) {
+  private void initReader(LeafReader reader) {
     reader.addCoreClosedListener(purgeCore);
   }
 
@@ -129,7 +129,7 @@ class FieldCacheImpl implements FieldCache {
 
     final Map<Object,Map<CacheKey,Accountable>> readerCache = new WeakHashMap<>();
     
-    protected abstract Accountable createValue(AtomicReader reader, CacheKey key, boolean setDocsWithField)
+    protected abstract Accountable createValue(LeafReader reader, CacheKey key, boolean setDocsWithField)
         throws IOException;
 
     /** Remove this reader from the cache, if present. */
@@ -141,7 +141,7 @@ class FieldCacheImpl implements FieldCache {
 
     /** Sets the key to the value for the provided reader;
      *  if the key is already set then this doesn't change it. */
-    public void put(AtomicReader reader, CacheKey key, Accountable value) {
+    public void put(LeafReader reader, CacheKey key, Accountable value) {
       final Object readerKey = reader.getCoreCacheKey();
       synchronized (readerCache) {
         Map<CacheKey,Accountable> innerCache = readerCache.get(readerKey);
@@ -160,7 +160,7 @@ class FieldCacheImpl implements FieldCache {
       }
     }
 
-    public Object get(AtomicReader reader, CacheKey key, boolean setDocsWithField) throws IOException {
+    public Object get(LeafReader reader, CacheKey key, boolean setDocsWithField) throws IOException {
       Map<CacheKey,Accountable> innerCache;
       Accountable value;
       final Object readerKey = reader.getCoreCacheKey();
@@ -261,7 +261,7 @@ class FieldCacheImpl implements FieldCache {
 
     public Bits docsWithField;
 
-    public void uninvert(AtomicReader reader, String field, boolean setDocsWithField) throws IOException {
+    public void uninvert(LeafReader reader, String field, boolean setDocsWithField) throws IOException {
       final int maxDoc = reader.maxDoc();
       Terms terms = reader.terms(field);
       if (terms != null) {
@@ -310,7 +310,7 @@ class FieldCacheImpl implements FieldCache {
   }
 
   // null Bits means no docs matched
-  void setDocsWithField(AtomicReader reader, String field, Bits docsWithField) {
+  void setDocsWithField(LeafReader reader, String field, Bits docsWithField) {
     final int maxDoc = reader.maxDoc();
     final Bits bits;
     if (docsWithField == null) {
@@ -351,7 +351,7 @@ class FieldCacheImpl implements FieldCache {
     public long minValue;
   }
 
-  public Bits getDocsWithField(AtomicReader reader, String field) throws IOException {
+  public Bits getDocsWithField(LeafReader reader, String field) throws IOException {
     final FieldInfo fieldInfo = reader.getFieldInfos().fieldInfo(field);
     if (fieldInfo == null) {
       // field does not exist or has no value
@@ -394,7 +394,7 @@ class FieldCacheImpl implements FieldCache {
     }
     
     @Override
-    protected BitsEntry createValue(AtomicReader reader, CacheKey key, boolean setDocsWithField /* ignored */)
+    protected BitsEntry createValue(LeafReader reader, CacheKey key, boolean setDocsWithField /* ignored */)
     throws IOException {
       final String field = key.field;
       final int maxDoc = reader.maxDoc();
@@ -446,7 +446,7 @@ class FieldCacheImpl implements FieldCache {
   }
   
   @Override
-  public NumericDocValues getNumerics(AtomicReader reader, String field, Parser parser, boolean setDocsWithField) throws IOException {
+  public NumericDocValues getNumerics(LeafReader reader, String field, Parser parser, boolean setDocsWithField) throws IOException {
     if (parser == null) {
       throw new NullPointerException();
     }
@@ -499,7 +499,7 @@ class FieldCacheImpl implements FieldCache {
     }
 
     @Override
-    protected Accountable createValue(final AtomicReader reader, CacheKey key, boolean setDocsWithField)
+    protected Accountable createValue(final LeafReader reader, CacheKey key, boolean setDocsWithField)
         throws IOException {
 
       final Parser parser = (Parser) key.custom;
@@ -620,11 +620,11 @@ class FieldCacheImpl implements FieldCache {
     }
   }
 
-  public SortedDocValues getTermsIndex(AtomicReader reader, String field) throws IOException {
+  public SortedDocValues getTermsIndex(LeafReader reader, String field) throws IOException {
     return getTermsIndex(reader, field, PackedInts.FAST);
   }
 
-  public SortedDocValues getTermsIndex(AtomicReader reader, String field, float acceptableOverheadRatio) throws IOException {
+  public SortedDocValues getTermsIndex(LeafReader reader, String field, float acceptableOverheadRatio) throws IOException {
     SortedDocValues valuesIn = reader.getSortedDocValues(field);
     if (valuesIn != null) {
       // Not cached here by FieldCacheImpl (cached instead
@@ -652,7 +652,7 @@ class FieldCacheImpl implements FieldCache {
     }
 
     @Override
-    protected Accountable createValue(AtomicReader reader, CacheKey key, boolean setDocsWithField /* ignored */)
+    protected Accountable createValue(LeafReader reader, CacheKey key, boolean setDocsWithField /* ignored */)
         throws IOException {
 
       final int maxDoc = reader.maxDoc();
@@ -764,11 +764,11 @@ class FieldCacheImpl implements FieldCache {
 
   // TODO: this if DocTermsIndex was already created, we
   // should share it...
-  public BinaryDocValues getTerms(AtomicReader reader, String field, boolean setDocsWithField) throws IOException {
+  public BinaryDocValues getTerms(LeafReader reader, String field, boolean setDocsWithField) throws IOException {
     return getTerms(reader, field, setDocsWithField, PackedInts.FAST);
   }
 
-  public BinaryDocValues getTerms(AtomicReader reader, String field, boolean setDocsWithField, float acceptableOverheadRatio) throws IOException {
+  public BinaryDocValues getTerms(LeafReader reader, String field, boolean setDocsWithField, float acceptableOverheadRatio) throws IOException {
     BinaryDocValues valuesIn = reader.getBinaryDocValues(field);
     if (valuesIn == null) {
       valuesIn = reader.getSortedDocValues(field);
@@ -799,7 +799,7 @@ class FieldCacheImpl implements FieldCache {
     }
 
     @Override
-    protected Accountable createValue(AtomicReader reader, CacheKey key, boolean setDocsWithField)
+    protected Accountable createValue(LeafReader reader, CacheKey key, boolean setDocsWithField)
         throws IOException {
 
       // TODO: would be nice to first check if DocTermsIndex
@@ -889,7 +889,7 @@ class FieldCacheImpl implements FieldCache {
 
   // TODO: this if DocTermsIndex was already created, we
   // should share it...
-  public SortedSetDocValues getDocTermOrds(AtomicReader reader, String field, BytesRef prefix) throws IOException {
+  public SortedSetDocValues getDocTermOrds(LeafReader reader, String field, BytesRef prefix) throws IOException {
     // not a general purpose filtering mechanism...
     assert prefix == null || prefix == INT32_TERM_PREFIX || prefix == INT64_TERM_PREFIX;
     
@@ -937,7 +937,7 @@ class FieldCacheImpl implements FieldCache {
     }
 
     @Override
-    protected Accountable createValue(AtomicReader reader, CacheKey key, boolean setDocsWithField /* ignored */)
+    protected Accountable createValue(LeafReader reader, CacheKey key, boolean setDocsWithField /* ignored */)
         throws IOException {
       BytesRef prefix = (BytesRef) key.custom;
       return new DocTermOrds(reader, null, key.field, prefix);
diff --git lucene/misc/src/java/org/apache/lucene/uninverting/UninvertingReader.java lucene/misc/src/java/org/apache/lucene/uninverting/UninvertingReader.java
index 49a367f..1cbb678 100644
--- lucene/misc/src/java/org/apache/lucene/uninverting/UninvertingReader.java
+++ lucene/misc/src/java/org/apache/lucene/uninverting/UninvertingReader.java
@@ -30,20 +30,18 @@ import org.apache.lucene.document.NumericDocValuesField; // javadocs
 import org.apache.lucene.document.SortedDocValuesField; // javadocs
 import org.apache.lucene.document.SortedSetDocValuesField; // javadocs
 import org.apache.lucene.document.StringField; // javadocs
-import org.apache.lucene.index.AtomicReader;
+import org.apache.lucene.index.FilterLeafReader;
+import org.apache.lucene.index.LeafReader;
 import org.apache.lucene.index.BinaryDocValues;
 import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.FieldInfo;
 import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.FilterAtomicReader;
 import org.apache.lucene.index.FilterDirectoryReader;
 import org.apache.lucene.index.NumericDocValues;
 import org.apache.lucene.index.SortedDocValues;
 import org.apache.lucene.index.SortedSetDocValues;
 import org.apache.lucene.uninverting.FieldCache.CacheEntry;
 import org.apache.lucene.util.Bits;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.NumericUtils;
 
 /**
  * A FilterReader that exposes <i>indexed</i> values as if they also had
@@ -52,11 +50,11 @@ import org.apache.lucene.util.NumericUtils;
  * This is accomplished by "inverting the inverted index" or "uninversion".
  * <p>
  * The uninversion process happens lazily: upon the first request for the 
- * field's docvalues (e.g. via {@link AtomicReader#getNumericDocValues(String)} 
+ * field's docvalues (e.g. via {@link org.apache.lucene.index.LeafReader#getNumericDocValues(String)} 
  * or similar), it will create the docvalues on-the-fly if needed and cache it,
  * based on the core cache key of the wrapped AtomicReader.
  */
-public class UninvertingReader extends FilterAtomicReader {
+public class UninvertingReader extends FilterLeafReader {
   
   /**
    * Specifies the type of uninversion to apply for the field. 
@@ -156,7 +154,7 @@ public class UninvertingReader extends FilterAtomicReader {
     public UninvertingDirectoryReader(DirectoryReader in, final Map<String,Type> mapping) {
       super(in, new FilterDirectoryReader.SubReaderWrapper() {
         @Override
-        public AtomicReader wrap(AtomicReader reader) {
+        public LeafReader wrap(LeafReader reader) {
           return new UninvertingReader(reader, mapping);
         }
       });
@@ -180,7 +178,7 @@ public class UninvertingReader extends FilterAtomicReader {
    *  
    * @lucene.internal
    */
-  public UninvertingReader(AtomicReader in, Map<String,Type> mapping) {
+  public UninvertingReader(LeafReader in, Map<String,Type> mapping) {
     super(in);
     this.mapping = mapping;
     ArrayList<FieldInfo> filteredInfos = new ArrayList<>();
diff --git lucene/misc/src/test/org/apache/lucene/index/sorter/IndexSortingTest.java lucene/misc/src/test/org/apache/lucene/index/sorter/IndexSortingTest.java
index 5a3bcdb..f15c666 100644
--- lucene/misc/src/test/org/apache/lucene/index/sorter/IndexSortingTest.java
+++ lucene/misc/src/test/org/apache/lucene/index/sorter/IndexSortingTest.java
@@ -68,7 +68,7 @@ public class IndexSortingTest extends SorterTestBase {
 
     Directory target = newDirectory();
     IndexWriter writer = new IndexWriter(target, newIndexWriterConfig(null));
-    reader = SortingAtomicReader.wrap(reader, sorter);
+    reader = SortingLeafReader.wrap(reader, sorter);
     writer.addIndexes(reader);
     writer.close();
     reader.close();
diff --git lucene/misc/src/test/org/apache/lucene/index/sorter/SorterTestBase.java lucene/misc/src/test/org/apache/lucene/index/sorter/SorterTestBase.java
index 0a43802..59949f6 100644
--- lucene/misc/src/test/org/apache/lucene/index/sorter/SorterTestBase.java
+++ lucene/misc/src/test/org/apache/lucene/index/sorter/SorterTestBase.java
@@ -40,8 +40,8 @@ import org.apache.lucene.document.SortedNumericDocValuesField;
 import org.apache.lucene.document.SortedSetDocValuesField;
 import org.apache.lucene.document.StringField;
 import org.apache.lucene.document.TextField;
-import org.apache.lucene.index.AtomicReader;
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReader;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.BinaryDocValues;
 import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.DocsAndPositionsEnum;
@@ -59,8 +59,8 @@ import org.apache.lucene.index.Term;
 import org.apache.lucene.index.Terms;
 import org.apache.lucene.index.TermsEnum;
 import org.apache.lucene.index.TermsEnum.SeekStatus;
-import org.apache.lucene.index.sorter.SortingAtomicReader.SortingDocsAndPositionsEnum;
-import org.apache.lucene.index.sorter.SortingAtomicReader.SortingDocsEnum;
+import org.apache.lucene.index.sorter.SortingLeafReader.SortingDocsAndPositionsEnum;
+import org.apache.lucene.index.sorter.SortingLeafReader.SortingDocsEnum;
 import org.apache.lucene.search.CollectionStatistics;
 import org.apache.lucene.search.DocIdSetIterator;
 import org.apache.lucene.search.TermStatistics;
@@ -100,7 +100,7 @@ public abstract class SorterTestBase extends LuceneTestCase {
     }
     
     @Override
-    public SimScorer simScorer(SimWeight weight, AtomicReaderContext context) throws IOException {
+    public SimScorer simScorer(SimWeight weight, LeafReaderContext context) throws IOException {
       return in.simScorer(weight, context);
     }
     
@@ -167,7 +167,7 @@ public abstract class SorterTestBase extends LuceneTestCase {
   }
   
   protected static Directory dir;
-  protected static AtomicReader reader;
+  protected static LeafReader reader;
   protected static Integer[] sortedValues;
 
   private static Document doc(final int id, PositionsTokenStream positions) {
diff --git lucene/misc/src/test/org/apache/lucene/index/sorter/SortingAtomicReaderTest.java lucene/misc/src/test/org/apache/lucene/index/sorter/SortingAtomicReaderTest.java
deleted file mode 100644
index bb75fbc..0000000
--- lucene/misc/src/test/org/apache/lucene/index/sorter/SortingAtomicReaderTest.java
+++ /dev/null
@@ -1,75 +0,0 @@
-package org.apache.lucene.index.sorter;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.util.Arrays;
-
-import org.apache.lucene.index.NumericDocValues;
-import org.apache.lucene.search.Sort;
-import org.apache.lucene.search.SortField;
-import org.apache.lucene.util.Bits;
-import org.apache.lucene.util.TestUtil;
-import org.junit.BeforeClass;
-
-public class SortingAtomicReaderTest extends SorterTestBase {
-  
-  @BeforeClass
-  public static void beforeClassSortingAtomicReaderTest() throws Exception {
-    
-    // sort the index by id (as integer, in NUMERIC_DV_FIELD)
-    Sort sort = new Sort(new SortField(NUMERIC_DV_FIELD, SortField.Type.INT));
-    final Sorter.DocMap docMap = new Sorter(sort).sort(reader);
- 
-    // Sorter.compute also sorts the values
-    NumericDocValues dv = reader.getNumericDocValues(NUMERIC_DV_FIELD);
-    sortedValues = new Integer[reader.maxDoc()];
-    for (int i = 0; i < reader.maxDoc(); ++i) {
-      sortedValues[docMap.oldToNew(i)] = (int)dv.get(i);
-    }
-    if (VERBOSE) {
-      System.out.println("docMap: " + docMap);
-      System.out.println("sortedValues: " + Arrays.toString(sortedValues));
-    }
-    
-    // sort the index by id (as integer, in NUMERIC_DV_FIELD)
-    reader = SortingAtomicReader.wrap(reader, sort);
-    
-    if (VERBOSE) {
-      System.out.print("mapped-deleted-docs: ");
-      Bits mappedLiveDocs = reader.getLiveDocs();
-      for (int i = 0; i < mappedLiveDocs.length(); i++) {
-        if (!mappedLiveDocs.get(i)) {
-          System.out.print(i + " ");
-        }
-      }
-      System.out.println();
-    }
-    
-    TestUtil.checkReader(reader);
-  }
-  
-  public void testBadSort() throws Exception {
-    try {
-      SortingAtomicReader.wrap(reader, Sort.RELEVANCE);
-      fail("Didn't get expected exception");
-    } catch (IllegalArgumentException e) {
-      assertEquals("Cannot sort an index with a Sort that refers to the relevance score", e.getMessage());
-    }
-  }
-
-}
diff --git lucene/misc/src/test/org/apache/lucene/index/sorter/SortingLeafReaderTest.java lucene/misc/src/test/org/apache/lucene/index/sorter/SortingLeafReaderTest.java
new file mode 100644
index 0000000..b055a07
--- /dev/null
+++ lucene/misc/src/test/org/apache/lucene/index/sorter/SortingLeafReaderTest.java
@@ -0,0 +1,75 @@
+package org.apache.lucene.index.sorter;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.util.Arrays;
+
+import org.apache.lucene.index.NumericDocValues;
+import org.apache.lucene.search.Sort;
+import org.apache.lucene.search.SortField;
+import org.apache.lucene.util.Bits;
+import org.apache.lucene.util.TestUtil;
+import org.junit.BeforeClass;
+
+public class SortingLeafReaderTest extends SorterTestBase {
+  
+  @BeforeClass
+  public static void beforeClassSortingAtomicReaderTest() throws Exception {
+    
+    // sort the index by id (as integer, in NUMERIC_DV_FIELD)
+    Sort sort = new Sort(new SortField(NUMERIC_DV_FIELD, SortField.Type.INT));
+    final Sorter.DocMap docMap = new Sorter(sort).sort(reader);
+ 
+    // Sorter.compute also sorts the values
+    NumericDocValues dv = reader.getNumericDocValues(NUMERIC_DV_FIELD);
+    sortedValues = new Integer[reader.maxDoc()];
+    for (int i = 0; i < reader.maxDoc(); ++i) {
+      sortedValues[docMap.oldToNew(i)] = (int)dv.get(i);
+    }
+    if (VERBOSE) {
+      System.out.println("docMap: " + docMap);
+      System.out.println("sortedValues: " + Arrays.toString(sortedValues));
+    }
+    
+    // sort the index by id (as integer, in NUMERIC_DV_FIELD)
+    reader = SortingLeafReader.wrap(reader, sort);
+    
+    if (VERBOSE) {
+      System.out.print("mapped-deleted-docs: ");
+      Bits mappedLiveDocs = reader.getLiveDocs();
+      for (int i = 0; i < mappedLiveDocs.length(); i++) {
+        if (!mappedLiveDocs.get(i)) {
+          System.out.print(i + " ");
+        }
+      }
+      System.out.println();
+    }
+    
+    TestUtil.checkReader(reader);
+  }
+  
+  public void testBadSort() throws Exception {
+    try {
+      SortingLeafReader.wrap(reader, Sort.RELEVANCE);
+      fail("Didn't get expected exception");
+    } catch (IllegalArgumentException e) {
+      assertEquals("Cannot sort an index with a Sort that refers to the relevance score", e.getMessage());
+    }
+  }
+
+}
diff --git lucene/misc/src/test/org/apache/lucene/index/sorter/TestBlockJoinSorter.java lucene/misc/src/test/org/apache/lucene/index/sorter/TestBlockJoinSorter.java
index 89fd008..5d75d9e 100644
--- lucene/misc/src/test/org/apache/lucene/index/sorter/TestBlockJoinSorter.java
+++ lucene/misc/src/test/org/apache/lucene/index/sorter/TestBlockJoinSorter.java
@@ -26,7 +26,7 @@ import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field.Store;
 import org.apache.lucene.document.NumericDocValuesField;
 import org.apache.lucene.document.StringField;
-import org.apache.lucene.index.AtomicReader;
+import org.apache.lucene.index.LeafReader;
 import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexWriterConfig;
 import org.apache.lucene.index.NumericDocValues;
@@ -53,7 +53,7 @@ public class TestBlockJoinSorter extends LuceneTestCase {
     }
 
     @Override
-    protected DocIdSet cacheImpl(DocIdSetIterator iterator, AtomicReader reader)
+    protected DocIdSet cacheImpl(DocIdSetIterator iterator, LeafReader reader)
         throws IOException {
       final FixedBitSet cached = new FixedBitSet(reader.maxDoc());
       cached.or(iterator);
@@ -88,7 +88,7 @@ public class TestBlockJoinSorter extends LuceneTestCase {
     final DirectoryReader indexReader = writer.getReader();
     writer.close();
 
-    final AtomicReader reader = getOnlySegmentReader(indexReader);
+    final LeafReader reader = getOnlySegmentReader(indexReader);
     final Filter parentsFilter = new FixedBitSetCachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term("parent", "true"))));
     final FixedBitSet parentBits = (FixedBitSet) parentsFilter.getDocIdSet(reader.getContext(), null);
     final NumericDocValues parentValues = reader.getNumericDocValues("parent_val");
diff --git lucene/misc/src/test/org/apache/lucene/index/sorter/TestEarlyTermination.java lucene/misc/src/test/org/apache/lucene/index/sorter/TestEarlyTermination.java
index fd3b022..76b9e86 100644
--- lucene/misc/src/test/org/apache/lucene/index/sorter/TestEarlyTermination.java
+++ lucene/misc/src/test/org/apache/lucene/index/sorter/TestEarlyTermination.java
@@ -29,7 +29,7 @@ import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field.Store;
 import org.apache.lucene.document.NumericDocValuesField;
 import org.apache.lucene.document.StringField;
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexWriterConfig;
 import org.apache.lucene.index.RandomIndexWriter;
@@ -167,7 +167,7 @@ public class TestEarlyTermination extends LuceneTestCase {
       Sort different = new Sort(new SortField("ndv2", SortField.Type.LONG));
       searcher.search(query, new EarlyTerminatingSortingCollector(collector2, different, numHits) {
         @Override
-        public LeafCollector getLeafCollector(AtomicReaderContext context) throws IOException {
+        public LeafCollector getLeafCollector(LeafReaderContext context) throws IOException {
           final LeafCollector ret = super.getLeafCollector(context);
           assertTrue("segment should not be recognized as sorted as different sorter was used", ret.getClass() == in.getLeafCollector(context).getClass());
           return ret;
diff --git lucene/misc/src/test/org/apache/lucene/index/sorter/TestSortingMergePolicy.java lucene/misc/src/test/org/apache/lucene/index/sorter/TestSortingMergePolicy.java
index 490eb28..ca2d798 100644
--- lucene/misc/src/test/org/apache/lucene/index/sorter/TestSortingMergePolicy.java
+++ lucene/misc/src/test/org/apache/lucene/index/sorter/TestSortingMergePolicy.java
@@ -29,7 +29,7 @@ import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field.Store;
 import org.apache.lucene.document.NumericDocValuesField;
 import org.apache.lucene.document.StringField;
-import org.apache.lucene.index.AtomicReader;
+import org.apache.lucene.index.LeafReader;
 import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexWriterConfig;
@@ -158,7 +158,7 @@ public class TestSortingMergePolicy extends LuceneTestCase {
     super.tearDown();
   }
 
-  private static void assertSorted(AtomicReader reader) throws IOException {
+  private static void assertSorted(LeafReader reader) throws IOException {
     final NumericDocValues ndv = reader.getNumericDocValues("ndv");
     for (int i = 1; i < reader.maxDoc(); ++i) {
       assertTrue("ndv(" + (i-1) + ")=" + ndv.get(i-1) + ",ndv(" + i + ")=" + ndv.get(i), ndv.get(i-1) <= ndv.get(i));
@@ -166,8 +166,8 @@ public class TestSortingMergePolicy extends LuceneTestCase {
   }
 
   public void testSortingMP() throws IOException {
-    final AtomicReader sortedReader1 = SortingAtomicReader.wrap(SlowCompositeReaderWrapper.wrap(reader), sort);
-    final AtomicReader sortedReader2 = SlowCompositeReaderWrapper.wrap(sortedReader);
+    final LeafReader sortedReader1 = SortingLeafReader.wrap(SlowCompositeReaderWrapper.wrap(reader), sort);
+    final LeafReader sortedReader2 = SlowCompositeReaderWrapper.wrap(sortedReader);
 
     assertSorted(sortedReader1);
     assertSorted(sortedReader2);
diff --git lucene/misc/src/test/org/apache/lucene/uninverting/TestDocTermOrds.java lucene/misc/src/test/org/apache/lucene/uninverting/TestDocTermOrds.java
index bd9a42f..f4c6059 100644
--- lucene/misc/src/test/org/apache/lucene/uninverting/TestDocTermOrds.java
+++ lucene/misc/src/test/org/apache/lucene/uninverting/TestDocTermOrds.java
@@ -33,8 +33,8 @@ import org.apache.lucene.document.Field;
 import org.apache.lucene.document.IntField;
 import org.apache.lucene.document.LongField;
 import org.apache.lucene.document.StringField;
-import org.apache.lucene.index.AtomicReader;
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReader;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.DocValues;
 import org.apache.lucene.index.IndexReader;
@@ -82,7 +82,7 @@ public class TestDocTermOrds extends LuceneTestCase {
     final IndexReader r = w.getReader();
     w.close();
 
-    final AtomicReader ar = SlowCompositeReaderWrapper.wrap(r);
+    final LeafReader ar = SlowCompositeReaderWrapper.wrap(r);
     final DocTermOrds dto = new DocTermOrds(ar, ar.getLiveDocs(), "field");
     SortedSetDocValues iter = dto.iterator(ar);
     
@@ -173,7 +173,7 @@ public class TestDocTermOrds extends LuceneTestCase {
       System.out.println("TEST: reader=" + r);
     }
 
-    for(AtomicReaderContext ctx : r.leaves()) {
+    for(LeafReaderContext ctx : r.leaves()) {
       if (VERBOSE) {
         System.out.println("\nTEST: sub=" + ctx.reader());
       }
@@ -185,7 +185,7 @@ public class TestDocTermOrds extends LuceneTestCase {
     if (VERBOSE) {
       System.out.println("TEST: top reader");
     }
-    AtomicReader slowR = SlowCompositeReaderWrapper.wrap(r);
+    LeafReader slowR = SlowCompositeReaderWrapper.wrap(r);
     verify(slowR, idToOrds, termsArray, null);
 
     FieldCache.DEFAULT.purgeByCacheKey(slowR.getCoreCacheKey());
@@ -270,7 +270,7 @@ public class TestDocTermOrds extends LuceneTestCase {
       System.out.println("TEST: reader=" + r);
     }
     
-    AtomicReader slowR = SlowCompositeReaderWrapper.wrap(r);
+    LeafReader slowR = SlowCompositeReaderWrapper.wrap(r);
     for(String prefix : prefixesArray) {
 
       final BytesRef prefixRef = prefix == null ? null : new BytesRef(prefix);
@@ -292,7 +292,7 @@ public class TestDocTermOrds extends LuceneTestCase {
         idToOrdsPrefix[id] = newOrdsArray;
       }
 
-      for(AtomicReaderContext ctx : r.leaves()) {
+      for(LeafReaderContext ctx : r.leaves()) {
         if (VERBOSE) {
           System.out.println("\nTEST: sub=" + ctx.reader());
         }
@@ -313,7 +313,7 @@ public class TestDocTermOrds extends LuceneTestCase {
     dir.close();
   }
 
-  private void verify(AtomicReader r, int[][] idToOrds, BytesRef[] termsArray, BytesRef prefixRef) throws Exception {
+  private void verify(LeafReader r, int[][] idToOrds, BytesRef[] termsArray, BytesRef prefixRef) throws Exception {
 
     final DocTermOrds dto = new DocTermOrds(r, r.getLiveDocs(),
                                             "field",
@@ -442,7 +442,7 @@ public class TestDocTermOrds extends LuceneTestCase {
     iw.close();
     
     DirectoryReader ir = DirectoryReader.open(dir);
-    AtomicReader ar = getOnlySegmentReader(ir);
+    LeafReader ar = getOnlySegmentReader(ir);
     
     SortedSetDocValues v = FieldCache.DEFAULT.getDocTermOrds(ar, "foo", FieldCache.INT32_TERM_PREFIX);
     assertEquals(2, v.getValueCount());
@@ -483,7 +483,7 @@ public class TestDocTermOrds extends LuceneTestCase {
     iw.close();
     
     DirectoryReader ir = DirectoryReader.open(dir);
-    AtomicReader ar = getOnlySegmentReader(ir);
+    LeafReader ar = getOnlySegmentReader(ir);
     
     SortedSetDocValues v = FieldCache.DEFAULT.getDocTermOrds(ar, "foo", FieldCache.INT64_TERM_PREFIX);
     assertEquals(2, v.getValueCount());
@@ -532,7 +532,7 @@ public class TestDocTermOrds extends LuceneTestCase {
     DirectoryReader ireader = iwriter.getReader();
     iwriter.close();
 
-    AtomicReader ar = getOnlySegmentReader(ireader);
+    LeafReader ar = getOnlySegmentReader(ireader);
     SortedSetDocValues dv = FieldCache.DEFAULT.getDocTermOrds(ar, "field", null);
     assertEquals(3, dv.getValueCount());
     
@@ -617,7 +617,7 @@ public class TestDocTermOrds extends LuceneTestCase {
     iw.close();
     
     DirectoryReader ir = DirectoryReader.open(dir);
-    AtomicReader ar = getOnlySegmentReader(ir);
+    LeafReader ar = getOnlySegmentReader(ir);
     
     SortedSetDocValues v = FieldCache.DEFAULT.getDocTermOrds(ar, "foo", null);
     assertNotNull(DocValues.unwrapSingleton(v)); // actually a single-valued field
diff --git lucene/misc/src/test/org/apache/lucene/uninverting/TestFieldCache.java lucene/misc/src/test/org/apache/lucene/uninverting/TestFieldCache.java
index cb9cf2c..29392c1 100644
--- lucene/misc/src/test/org/apache/lucene/uninverting/TestFieldCache.java
+++ lucene/misc/src/test/org/apache/lucene/uninverting/TestFieldCache.java
@@ -40,7 +40,7 @@ import org.apache.lucene.document.NumericDocValuesField;
 import org.apache.lucene.document.SortedDocValuesField;
 import org.apache.lucene.document.SortedSetDocValuesField;
 import org.apache.lucene.document.StoredField;
-import org.apache.lucene.index.AtomicReader;
+import org.apache.lucene.index.LeafReader;
 import org.apache.lucene.index.BinaryDocValues;
 import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexReader;
@@ -64,7 +64,7 @@ import org.junit.AfterClass;
 import org.junit.BeforeClass;
 
 public class TestFieldCache extends LuceneTestCase {
-  private static AtomicReader reader;
+  private static LeafReader reader;
   private static int NUM_DOCS;
   private static int NUM_ORDS;
   private static String[] unicodeStrings;
@@ -292,7 +292,7 @@ public class TestFieldCache extends LuceneTestCase {
     IndexWriter writer= new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random())).setMaxBufferedDocs(500));
     writer.close();
     IndexReader r = DirectoryReader.open(dir);
-    AtomicReader reader = SlowCompositeReaderWrapper.wrap(r);
+    LeafReader reader = SlowCompositeReaderWrapper.wrap(r);
     FieldCache.DEFAULT.getTerms(reader, "foobar", true);
     FieldCache.DEFAULT.getTermsIndex(reader, "foobar");
     FieldCache.DEFAULT.purgeByCacheKey(reader.getCoreCacheKey());
@@ -435,7 +435,7 @@ public class TestFieldCache extends LuceneTestCase {
     iw.addDocument(doc);
     DirectoryReader ir = iw.getReader();
     iw.close();
-    AtomicReader ar = getOnlySegmentReader(ir);
+    LeafReader ar = getOnlySegmentReader(ir);
     
     // Binary type: can be retrieved via getTerms()
     try {
@@ -565,7 +565,7 @@ public class TestFieldCache extends LuceneTestCase {
     DirectoryReader ir = iw.getReader();
     iw.close();
     
-    AtomicReader ar = getOnlySegmentReader(ir);
+    LeafReader ar = getOnlySegmentReader(ir);
     
     final FieldCache cache = FieldCache.DEFAULT;
     cache.purgeAllCaches();
@@ -623,7 +623,7 @@ public class TestFieldCache extends LuceneTestCase {
     DirectoryReader ir = iw.getReader();
     iw.close();
     
-    AtomicReader ar = getOnlySegmentReader(ir);
+    LeafReader ar = getOnlySegmentReader(ir);
     
     final FieldCache cache = FieldCache.DEFAULT;
     cache.purgeAllCaches();
diff --git lucene/misc/src/test/org/apache/lucene/uninverting/TestFieldCacheReopen.java lucene/misc/src/test/org/apache/lucene/uninverting/TestFieldCacheReopen.java
index de5baa7..b8ab53d 100644
--- lucene/misc/src/test/org/apache/lucene/uninverting/TestFieldCacheReopen.java
+++ lucene/misc/src/test/org/apache/lucene/uninverting/TestFieldCacheReopen.java
@@ -21,7 +21,7 @@ import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.IntField;
-import org.apache.lucene.index.AtomicReader;
+import org.apache.lucene.index.LeafReader;
 import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexWriter;
 import org.apache.lucene.index.NumericDocValues;
@@ -49,7 +49,7 @@ public class TestFieldCacheReopen extends LuceneTestCase {
   
     // Open reader1
     DirectoryReader r = DirectoryReader.open(dir);
-    AtomicReader r1 = getOnlySegmentReader(r);
+    LeafReader r1 = getOnlySegmentReader(r);
     final NumericDocValues ints = FieldCache.DEFAULT.getNumerics(r1, "number", FieldCache.NUMERIC_UTILS_INT_PARSER, false);
     assertEquals(17, ints.get(0));
   
@@ -61,7 +61,7 @@ public class TestFieldCacheReopen extends LuceneTestCase {
     DirectoryReader r2 = DirectoryReader.openIfChanged(r);
     assertNotNull(r2);
     r.close();
-    AtomicReader sub0 = r2.leaves().get(0).reader();
+    LeafReader sub0 = r2.leaves().get(0).reader();
     final NumericDocValues ints2 = FieldCache.DEFAULT.getNumerics(sub0, "number", FieldCache.NUMERIC_UTILS_INT_PARSER, false);
     r2.close();
     assertTrue(ints == ints2);
diff --git lucene/misc/src/test/org/apache/lucene/uninverting/TestFieldCacheSanityChecker.java lucene/misc/src/test/org/apache/lucene/uninverting/TestFieldCacheSanityChecker.java
index 52e5030..b8f67eb 100644
--- lucene/misc/src/test/org/apache/lucene/uninverting/TestFieldCacheSanityChecker.java
+++ lucene/misc/src/test/org/apache/lucene/uninverting/TestFieldCacheSanityChecker.java
@@ -25,7 +25,7 @@ import org.apache.lucene.document.Field;
 import org.apache.lucene.document.FloatField;
 import org.apache.lucene.document.IntField;
 import org.apache.lucene.document.LongField;
-import org.apache.lucene.index.AtomicReader;
+import org.apache.lucene.index.LeafReader;
 import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexWriter;
 import org.apache.lucene.index.MultiReader;
@@ -37,10 +37,10 @@ import org.apache.lucene.util.LuceneTestCase;
 
 public class TestFieldCacheSanityChecker extends LuceneTestCase {
 
-  protected AtomicReader readerA;
-  protected AtomicReader readerB;
-  protected AtomicReader readerX;
-  protected AtomicReader readerAclone;
+  protected LeafReader readerA;
+  protected LeafReader readerB;
+  protected LeafReader readerX;
+  protected LeafReader readerAclone;
   protected Directory dirA, dirB;
   private static final int NUM_DOCS = 1000;
 
diff --git lucene/misc/src/test/org/apache/lucene/uninverting/TestFieldCacheSortRandom.java lucene/misc/src/test/org/apache/lucene/uninverting/TestFieldCacheSortRandom.java
index 9b7a185..bc1aa16 100644
--- lucene/misc/src/test/org/apache/lucene/uninverting/TestFieldCacheSortRandom.java
+++ lucene/misc/src/test/org/apache/lucene/uninverting/TestFieldCacheSortRandom.java
@@ -32,7 +32,7 @@ import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.IntField;
 import org.apache.lucene.document.StringField;
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.DocValues;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.NumericDocValues;
@@ -277,7 +277,7 @@ public class TestFieldCacheSortRandom extends LuceneTestCase {
     }
 
     @Override
-    public DocIdSet getDocIdSet(AtomicReaderContext context, Bits acceptDocs) throws IOException {
+    public DocIdSet getDocIdSet(LeafReaderContext context, Bits acceptDocs) throws IOException {
       final int maxDoc = context.reader().maxDoc();
       final NumericDocValues idSource = DocValues.getNumeric(context.reader(), "id");
       assertNotNull(idSource);
diff --git lucene/misc/src/test/org/apache/lucene/uninverting/TestFieldCacheVsDocValues.java lucene/misc/src/test/org/apache/lucene/uninverting/TestFieldCacheVsDocValues.java
index 42f0b45..0a16f06 100644
--- lucene/misc/src/test/org/apache/lucene/uninverting/TestFieldCacheVsDocValues.java
+++ lucene/misc/src/test/org/apache/lucene/uninverting/TestFieldCacheVsDocValues.java
@@ -32,8 +32,8 @@ import org.apache.lucene.document.NumericDocValuesField;
 import org.apache.lucene.document.SortedDocValuesField;
 import org.apache.lucene.document.SortedSetDocValuesField;
 import org.apache.lucene.document.StringField;
-import org.apache.lucene.index.AtomicReader;
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReader;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.BinaryDocValues;
 import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.DocValues;
@@ -191,7 +191,7 @@ public class TestFieldCacheVsDocValues extends LuceneTestCase {
     }
     w.close();
 
-    AtomicReader ar = SlowCompositeReaderWrapper.wrap(r);
+    LeafReader ar = SlowCompositeReaderWrapper.wrap(r);
 
     BinaryDocValues s = FieldCache.DEFAULT.getTerms(ar, "field", false);
     for(int docID=0;docID<docBytes.size();docID++) {
@@ -264,7 +264,7 @@ public class TestFieldCacheVsDocValues extends LuceneTestCase {
     DirectoryReader r = DirectoryReader.open(w, true);
     w.close();
 
-    AtomicReader ar = SlowCompositeReaderWrapper.wrap(r);
+    LeafReader ar = SlowCompositeReaderWrapper.wrap(r);
 
     BinaryDocValues s = FieldCache.DEFAULT.getTerms(ar, "field", false);
     for(int docID=0;docID<docBytes.size();docID++) {
@@ -320,8 +320,8 @@ public class TestFieldCacheVsDocValues extends LuceneTestCase {
     
     // compare
     DirectoryReader ir = DirectoryReader.open(dir);
-    for (AtomicReaderContext context : ir.leaves()) {
-      AtomicReader r = context.reader();
+    for (LeafReaderContext context : ir.leaves()) {
+      LeafReader r = context.reader();
       SortedDocValues expected = FieldCache.DEFAULT.getTermsIndex(r, "indexed");
       SortedDocValues actual = r.getSortedDocValues("dv");
       assertEquals(r.maxDoc(), expected, actual);
@@ -378,8 +378,8 @@ public class TestFieldCacheVsDocValues extends LuceneTestCase {
     
     // compare per-segment
     DirectoryReader ir = writer.getReader();
-    for (AtomicReaderContext context : ir.leaves()) {
-      AtomicReader r = context.reader();
+    for (LeafReaderContext context : ir.leaves()) {
+      LeafReader r = context.reader();
       SortedSetDocValues expected = FieldCache.DEFAULT.getDocTermOrds(r, "indexed", null);
       SortedSetDocValues actual = r.getSortedSetDocValues("dv");
       assertEquals(r.maxDoc(), expected, actual);
@@ -390,7 +390,7 @@ public class TestFieldCacheVsDocValues extends LuceneTestCase {
     
     // now compare again after the merge
     ir = writer.getReader();
-    AtomicReader ar = getOnlySegmentReader(ir);
+    LeafReader ar = getOnlySegmentReader(ir);
     SortedSetDocValues expected = FieldCache.DEFAULT.getDocTermOrds(ar, "indexed", null);
     SortedSetDocValues actual = ar.getSortedSetDocValues("dv");
     assertEquals(ir.maxDoc(), expected, actual);
@@ -447,8 +447,8 @@ public class TestFieldCacheVsDocValues extends LuceneTestCase {
     
     // compare
     DirectoryReader ir = DirectoryReader.open(dir);
-    for (AtomicReaderContext context : ir.leaves()) {
-      AtomicReader r = context.reader();
+    for (LeafReaderContext context : ir.leaves()) {
+      LeafReader r = context.reader();
       Bits expected = FieldCache.DEFAULT.getDocsWithField(r, "indexed");
       Bits actual = FieldCache.DEFAULT.getDocsWithField(r, "dv");
       assertEquals(expected, actual);
diff --git lucene/misc/src/test/org/apache/lucene/uninverting/TestFieldCacheWithThreads.java lucene/misc/src/test/org/apache/lucene/uninverting/TestFieldCacheWithThreads.java
index b284471..eea0dbd 100644
--- lucene/misc/src/test/org/apache/lucene/uninverting/TestFieldCacheWithThreads.java
+++ lucene/misc/src/test/org/apache/lucene/uninverting/TestFieldCacheWithThreads.java
@@ -30,7 +30,7 @@ import org.apache.lucene.document.BinaryDocValuesField;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.NumericDocValuesField;
 import org.apache.lucene.document.SortedDocValuesField;
-import org.apache.lucene.index.AtomicReader;
+import org.apache.lucene.index.LeafReader;
 import org.apache.lucene.index.BinaryDocValues;
 import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexReader;
@@ -72,7 +72,7 @@ public class TestFieldCacheWithThreads extends LuceneTestCase {
     w.close();
 
     assertEquals(1, r.leaves().size());
-    final AtomicReader ar = r.leaves().get(0).reader();
+    final LeafReader ar = r.leaves().get(0).reader();
 
     int numThreads = TestUtil.nextInt(random(), 2, 5);
     List<Thread> threads = new ArrayList<>();
@@ -181,7 +181,7 @@ public class TestFieldCacheWithThreads extends LuceneTestCase {
     final DirectoryReader r = writer.getReader();
     writer.close();
     
-    final AtomicReader sr = getOnlySegmentReader(r);
+    final LeafReader sr = getOnlySegmentReader(r);
 
     final long END_TIME = System.currentTimeMillis() + (TEST_NIGHTLY ? 30 : 1);
 
diff --git lucene/misc/src/test/org/apache/lucene/uninverting/TestUninvertingReader.java lucene/misc/src/test/org/apache/lucene/uninverting/TestUninvertingReader.java
index 8447695..69ee86d 100644
--- lucene/misc/src/test/org/apache/lucene/uninverting/TestUninvertingReader.java
+++ lucene/misc/src/test/org/apache/lucene/uninverting/TestUninvertingReader.java
@@ -24,7 +24,7 @@ import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.IntField;
 import org.apache.lucene.document.LongField;
-import org.apache.lucene.index.AtomicReader;
+import org.apache.lucene.index.LeafReader;
 import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexWriter;
 import org.apache.lucene.index.SortedSetDocValues;
@@ -55,7 +55,7 @@ public class TestUninvertingReader extends LuceneTestCase {
     
     DirectoryReader ir = UninvertingReader.wrap(DirectoryReader.open(dir), 
                          Collections.singletonMap("foo", Type.SORTED_SET_INTEGER));
-    AtomicReader ar = ir.leaves().get(0).reader();
+    LeafReader ar = ir.leaves().get(0).reader();
     SortedSetDocValues v = ar.getSortedSetDocValues("foo");
     assertEquals(2, v.getValueCount());
     
@@ -96,7 +96,7 @@ public class TestUninvertingReader extends LuceneTestCase {
     
     DirectoryReader ir = UninvertingReader.wrap(DirectoryReader.open(dir), 
                          Collections.singletonMap("foo", Type.SORTED_SET_FLOAT));
-    AtomicReader ar = ir.leaves().get(0).reader();
+    LeafReader ar = ir.leaves().get(0).reader();
     
     SortedSetDocValues v = ar.getSortedSetDocValues("foo");
     assertEquals(2, v.getValueCount());
@@ -138,7 +138,7 @@ public class TestUninvertingReader extends LuceneTestCase {
     
     DirectoryReader ir = UninvertingReader.wrap(DirectoryReader.open(dir), 
         Collections.singletonMap("foo", Type.SORTED_SET_LONG));
-    AtomicReader ar = ir.leaves().get(0).reader();
+    LeafReader ar = ir.leaves().get(0).reader();
     SortedSetDocValues v = ar.getSortedSetDocValues("foo");
     assertEquals(2, v.getValueCount());
     
@@ -179,7 +179,7 @@ public class TestUninvertingReader extends LuceneTestCase {
     
     DirectoryReader ir = UninvertingReader.wrap(DirectoryReader.open(dir), 
         Collections.singletonMap("foo", Type.SORTED_SET_DOUBLE));
-    AtomicReader ar = ir.leaves().get(0).reader();
+    LeafReader ar = ir.leaves().get(0).reader();
     SortedSetDocValues v = ar.getSortedSetDocValues("foo");
     assertEquals(2, v.getValueCount());
     
diff --git lucene/queries/src/java/org/apache/lucene/queries/BooleanFilter.java lucene/queries/src/java/org/apache/lucene/queries/BooleanFilter.java
index ce2497e..9c244db 100644
--- lucene/queries/src/java/org/apache/lucene/queries/BooleanFilter.java
+++ lucene/queries/src/java/org/apache/lucene/queries/BooleanFilter.java
@@ -22,8 +22,8 @@ import java.util.ArrayList;
 import java.util.List;
 import java.util.Iterator;
 
-import org.apache.lucene.index.AtomicReader;
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReader;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.search.BitsFilteredDocIdSet;
 import org.apache.lucene.search.BooleanClause.Occur;
 import org.apache.lucene.search.DocIdSet;
@@ -50,9 +50,9 @@ public class BooleanFilter extends Filter implements Iterable<FilterClause> {
    * of the filters that have been added.
    */
   @Override
-  public DocIdSet getDocIdSet(AtomicReaderContext context, Bits acceptDocs) throws IOException {
+  public DocIdSet getDocIdSet(LeafReaderContext context, Bits acceptDocs) throws IOException {
     FixedBitSet res = null;
-    final AtomicReader reader = context.reader();
+    final LeafReader reader = context.reader();
     
     boolean hasShouldClauses = false;
     for (final FilterClause fc : clauses) {
@@ -101,7 +101,7 @@ public class BooleanFilter extends Filter implements Iterable<FilterClause> {
     return BitsFilteredDocIdSet.wrap(res, acceptDocs);
   }
 
-  private static DocIdSetIterator getDISI(Filter filter, AtomicReaderContext context)
+  private static DocIdSetIterator getDISI(Filter filter, LeafReaderContext context)
       throws IOException {
     // we dont pass acceptDocs, we will filter at the end using an additional filter
     final DocIdSet set = filter.getDocIdSet(context, null);
diff --git lucene/queries/src/java/org/apache/lucene/queries/ChainedFilter.java lucene/queries/src/java/org/apache/lucene/queries/ChainedFilter.java
index f326cf5..034af01 100644
--- lucene/queries/src/java/org/apache/lucene/queries/ChainedFilter.java
+++ lucene/queries/src/java/org/apache/lucene/queries/ChainedFilter.java
@@ -19,8 +19,8 @@ package org.apache.lucene.queries;
 
 import java.io.IOException;
 
-import org.apache.lucene.index.AtomicReader;
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReader;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.search.BitsFilteredDocIdSet;
 import org.apache.lucene.search.DocIdSet;
 import org.apache.lucene.search.DocIdSetIterator;
@@ -98,7 +98,7 @@ public class ChainedFilter extends Filter {
    * {@link Filter#getDocIdSet}.
    */
   @Override
-  public DocIdSet getDocIdSet(AtomicReaderContext context, Bits acceptDocs) throws IOException {
+  public DocIdSet getDocIdSet(LeafReaderContext context, Bits acceptDocs) throws IOException {
     int[] index = new int[1]; // use array as reference to modifiable int;
     index[0] = 0;             // an object attribute would not be thread safe.
     if (logic != -1) {
@@ -110,7 +110,7 @@ public class ChainedFilter extends Filter {
     return BitsFilteredDocIdSet.wrap(getDocIdSet(context, DEFAULT, index), acceptDocs);
   }
 
-  private DocIdSetIterator getDISI(Filter filter, AtomicReaderContext context)
+  private DocIdSetIterator getDISI(Filter filter, LeafReaderContext context)
       throws IOException {
     // we dont pass acceptDocs, we will filter at the end using an additional filter
     DocIdSet docIdSet = filter.getDocIdSet(context, null);
@@ -126,9 +126,9 @@ public class ChainedFilter extends Filter {
     }
   }
 
-  private FixedBitSet initialResult(AtomicReaderContext context, int logic, int[] index)
+  private FixedBitSet initialResult(LeafReaderContext context, int logic, int[] index)
       throws IOException {
-    AtomicReader reader = context.reader();
+    LeafReader reader = context.reader();
     FixedBitSet result = new FixedBitSet(reader.maxDoc());
     if (logic == AND) {
       result.or(getDISI(chain[index[0]], context));
@@ -148,7 +148,7 @@ public class ChainedFilter extends Filter {
    * @param logic Logical operation
    * @return DocIdSet
    */
-  private DocIdSet getDocIdSet(AtomicReaderContext context, int logic, int[] index)
+  private DocIdSet getDocIdSet(LeafReaderContext context, int logic, int[] index)
       throws IOException {
     FixedBitSet result = initialResult(context, logic, index);
     for (; index[0] < chain.length; index[0]++) {
@@ -165,7 +165,7 @@ public class ChainedFilter extends Filter {
    * @param logic Logical operation
    * @return DocIdSet
    */
-  private DocIdSet getDocIdSet(AtomicReaderContext context, int[] logic, int[] index)
+  private DocIdSet getDocIdSet(LeafReaderContext context, int[] logic, int[] index)
       throws IOException {
     if (logic.length != chain.length) {
       throw new IllegalArgumentException("Invalid number of elements in logic array");
diff --git lucene/queries/src/java/org/apache/lucene/queries/CommonTermsQuery.java lucene/queries/src/java/org/apache/lucene/queries/CommonTermsQuery.java
index a78cef3..8a84fa5 100644
--- lucene/queries/src/java/org/apache/lucene/queries/CommonTermsQuery.java
+++ lucene/queries/src/java/org/apache/lucene/queries/CommonTermsQuery.java
@@ -16,7 +16,7 @@ package org.apache.lucene.queries;
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.Fields;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.Term;
@@ -153,7 +153,7 @@ public class CommonTermsQuery extends Query {
       tq.setBoost(getBoost());
       return tq;
     }
-    final List<AtomicReaderContext> leaves = reader.leaves();
+    final List<LeafReaderContext> leaves = reader.leaves();
     final int maxDoc = reader.maxDoc();
     final TermContext[] contextArray = new TermContext[terms.size()];
     final Term[] queryTerms = this.terms.toArray(new Term[0]);
@@ -234,10 +234,10 @@ public class CommonTermsQuery extends Query {
   }
   
   public void collectTermContext(IndexReader reader,
-      List<AtomicReaderContext> leaves, TermContext[] contextArray,
+      List<LeafReaderContext> leaves, TermContext[] contextArray,
       Term[] queryTerms) throws IOException {
     TermsEnum termsEnum = null;
-    for (AtomicReaderContext context : leaves) {
+    for (LeafReaderContext context : leaves) {
       final Fields fields = context.reader().fields();
       if (fields == null) {
         // reader has no fields
diff --git lucene/queries/src/java/org/apache/lucene/queries/CustomScoreProvider.java lucene/queries/src/java/org/apache/lucene/queries/CustomScoreProvider.java
index aef52ea..edd42e0 100644
--- lucene/queries/src/java/org/apache/lucene/queries/CustomScoreProvider.java
+++ lucene/queries/src/java/org/apache/lucene/queries/CustomScoreProvider.java
@@ -19,7 +19,7 @@ package org.apache.lucene.queries;
 
 import java.io.IOException;
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.IndexReader; // for javadocs
 import org.apache.lucene.queries.function.FunctionQuery;
 import org.apache.lucene.search.Explanation;
@@ -38,12 +38,12 @@ import org.apache.lucene.search.Explanation;
  */
 public class CustomScoreProvider {
 
-  protected final AtomicReaderContext context;
+  protected final LeafReaderContext context;
 
   /**
    * Creates a new instance of the provider class for the given {@link IndexReader}.
    */
-  public CustomScoreProvider(AtomicReaderContext context) {
+  public CustomScoreProvider(LeafReaderContext context) {
     this.context = context;
   }
 
diff --git lucene/queries/src/java/org/apache/lucene/queries/CustomScoreQuery.java lucene/queries/src/java/org/apache/lucene/queries/CustomScoreQuery.java
index 9861617..6e6727a 100644
--- lucene/queries/src/java/org/apache/lucene/queries/CustomScoreQuery.java
+++ lucene/queries/src/java/org/apache/lucene/queries/CustomScoreQuery.java
@@ -23,7 +23,7 @@ import java.util.Collections;
 import java.util.Set;
 import java.util.Arrays;
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.queries.function.FunctionQuery;
@@ -174,7 +174,7 @@ public class CustomScoreQuery extends Query {
    * implementation as specified in the docs of {@link CustomScoreProvider}.
    * @since 2.9.2
    */
-  protected CustomScoreProvider getCustomScoreProvider(AtomicReaderContext context) throws IOException {
+  protected CustomScoreProvider getCustomScoreProvider(LeafReaderContext context) throws IOException {
     return new CustomScoreProvider(context);
   }
 
@@ -234,7 +234,7 @@ public class CustomScoreQuery extends Query {
     }
 
     @Override
-    public Scorer scorer(AtomicReaderContext context, Bits acceptDocs) throws IOException {
+    public Scorer scorer(LeafReaderContext context, Bits acceptDocs) throws IOException {
       Scorer subQueryScorer = subQueryWeight.scorer(context, acceptDocs);
       if (subQueryScorer == null) {
         return null;
@@ -247,12 +247,12 @@ public class CustomScoreQuery extends Query {
     }
 
     @Override
-    public Explanation explain(AtomicReaderContext context, int doc) throws IOException {
+    public Explanation explain(LeafReaderContext context, int doc) throws IOException {
       Explanation explain = doExplain(context, doc);
       return explain == null ? new Explanation(0.0f, "no matching docs") : explain;
     }
     
-    private Explanation doExplain(AtomicReaderContext info, int doc) throws IOException {
+    private Explanation doExplain(LeafReaderContext info, int doc) throws IOException {
       Explanation subQueryExpl = subQueryWeight.explain(info, doc);
       if (!subQueryExpl.isMatch()) {
         return subQueryExpl;
diff --git lucene/queries/src/java/org/apache/lucene/queries/TermFilter.java lucene/queries/src/java/org/apache/lucene/queries/TermFilter.java
index 8a009c6..c075984 100644
--- lucene/queries/src/java/org/apache/lucene/queries/TermFilter.java
+++ lucene/queries/src/java/org/apache/lucene/queries/TermFilter.java
@@ -17,7 +17,7 @@ package org.apache.lucene.queries;
  * limitations under the License.
  */
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.DocsEnum;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.index.Terms;
@@ -56,7 +56,7 @@ final public class TermFilter extends Filter {
   }
 
   @Override
-  public DocIdSet getDocIdSet(AtomicReaderContext context, final Bits acceptDocs) throws IOException {
+  public DocIdSet getDocIdSet(LeafReaderContext context, final Bits acceptDocs) throws IOException {
     Terms terms = context.reader().terms(term.field());
     if (terms == null) {
       return null;
diff --git lucene/queries/src/java/org/apache/lucene/queries/TermsFilter.java lucene/queries/src/java/org/apache/lucene/queries/TermsFilter.java
index 652addc..615a1de 100644
--- lucene/queries/src/java/org/apache/lucene/queries/TermsFilter.java
+++ lucene/queries/src/java/org/apache/lucene/queries/TermsFilter.java
@@ -176,8 +176,8 @@ public final class TermsFilter extends Filter {
   
   
   @Override
-  public DocIdSet getDocIdSet(AtomicReaderContext context, Bits acceptDocs) throws IOException {
-    final AtomicReader reader = context.reader();
+  public DocIdSet getDocIdSet(LeafReaderContext context, Bits acceptDocs) throws IOException {
+    final LeafReader reader = context.reader();
     FixedBitSet result = null;  // lazy init if needed - no need to create a big bitset ahead of time
     final Fields fields = reader.fields();
     final BytesRef spare = new BytesRef(this.termsBytes);
diff --git lucene/queries/src/java/org/apache/lucene/queries/function/BoostedQuery.java lucene/queries/src/java/org/apache/lucene/queries/function/BoostedQuery.java
index 7ba5f4f..90bb2e0 100644
--- lucene/queries/src/java/org/apache/lucene/queries/function/BoostedQuery.java
+++ lucene/queries/src/java/org/apache/lucene/queries/function/BoostedQuery.java
@@ -17,8 +17,8 @@ package org.apache.lucene.queries.function;
  * limitations under the License.
  */
 
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.search.*;
-import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.util.Bits;
@@ -97,7 +97,7 @@ public class BoostedQuery extends Query {
     }
 
     @Override
-    public Scorer scorer(AtomicReaderContext context, Bits acceptDocs) throws IOException {
+    public Scorer scorer(LeafReaderContext context, Bits acceptDocs) throws IOException {
       Scorer subQueryScorer = qWeight.scorer(context, acceptDocs);
       if (subQueryScorer == null) {
         return null;
@@ -106,7 +106,7 @@ public class BoostedQuery extends Query {
     }
 
     @Override
-    public Explanation explain(AtomicReaderContext readerContext, int doc) throws IOException {
+    public Explanation explain(LeafReaderContext readerContext, int doc) throws IOException {
       Explanation subQueryExpl = qWeight.explain(readerContext,doc);
       if (!subQueryExpl.isMatch()) {
         return subQueryExpl;
@@ -127,9 +127,9 @@ public class BoostedQuery extends Query {
     private final float qWeight;
     private final Scorer scorer;
     private final FunctionValues vals;
-    private final AtomicReaderContext readerContext;
+    private final LeafReaderContext readerContext;
 
-    private CustomScorer(AtomicReaderContext readerContext, BoostedQuery.BoostedWeight w, float qWeight,
+    private CustomScorer(LeafReaderContext readerContext, BoostedQuery.BoostedWeight w, float qWeight,
         Scorer scorer, ValueSource vs) throws IOException {
       super(w);
       this.weight = w;
diff --git lucene/queries/src/java/org/apache/lucene/queries/function/FunctionQuery.java lucene/queries/src/java/org/apache/lucene/queries/function/FunctionQuery.java
index 726b97e..4abc312 100644
--- lucene/queries/src/java/org/apache/lucene/queries/function/FunctionQuery.java
+++ lucene/queries/src/java/org/apache/lucene/queries/function/FunctionQuery.java
@@ -17,11 +17,10 @@ package org.apache.lucene.queries.function;
  * limitations under the License.
  */
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.search.*;
-import org.apache.lucene.index.MultiFields;
 import org.apache.lucene.util.Bits;
 
 import java.io.IOException;
@@ -90,12 +89,12 @@ public class FunctionQuery extends Query {
     }
 
     @Override
-    public Scorer scorer(AtomicReaderContext context, Bits acceptDocs) throws IOException {
+    public Scorer scorer(LeafReaderContext context, Bits acceptDocs) throws IOException {
       return new AllScorer(context, acceptDocs, this, queryWeight);
     }
 
     @Override
-    public Explanation explain(AtomicReaderContext context, int doc) throws IOException {
+    public Explanation explain(LeafReaderContext context, int doc) throws IOException {
       return ((AllScorer)scorer(context, context.reader().getLiveDocs())).explain(doc);
     }
   }
@@ -109,7 +108,7 @@ public class FunctionQuery extends Query {
     final FunctionValues vals;
     final Bits acceptDocs;
 
-    public AllScorer(AtomicReaderContext context, Bits acceptDocs, FunctionWeight w, float qWeight) throws IOException {
+    public AllScorer(LeafReaderContext context, Bits acceptDocs, FunctionWeight w, float qWeight) throws IOException {
       super(w);
       this.weight = w;
       this.qWeight = qWeight;
diff --git lucene/queries/src/java/org/apache/lucene/queries/function/ValueSource.java lucene/queries/src/java/org/apache/lucene/queries/function/ValueSource.java
index d09da4e..3cf9081 100644
--- lucene/queries/src/java/org/apache/lucene/queries/function/ValueSource.java
+++ lucene/queries/src/java/org/apache/lucene/queries/function/ValueSource.java
@@ -17,7 +17,7 @@ package org.apache.lucene.queries.function;
  * limitations under the License.
  */
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.search.FieldComparator;
 import org.apache.lucene.search.FieldComparatorSource;
 import org.apache.lucene.search.IndexSearcher;
@@ -40,7 +40,7 @@ public abstract class ValueSource {
    * Gets the values for this reader and the context that was previously
    * passed to createWeight()
    */
-  public abstract FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException;
+  public abstract FunctionValues getValues(Map context, LeafReaderContext readerContext) throws IOException;
 
   @Override
   public abstract boolean equals(Object o);
@@ -84,7 +84,7 @@ public abstract class ValueSource {
   /**
    * EXPERIMENTAL: This method is subject to change.
    * <p>
-   * Get the SortField for this ValueSource.  Uses the {@link #getValues(java.util.Map, AtomicReaderContext)}
+   * Get the SortField for this ValueSource.  Uses the {@link #getValues(java.util.Map, org.apache.lucene.index.LeafReaderContext)}
    * to populate the SortField.
    *
    * @param reverse true if this is a reverse sort.
@@ -154,7 +154,7 @@ public abstract class ValueSource {
     }
 
     @Override
-    public FieldComparator setNextReader(AtomicReaderContext context) throws IOException {
+    public FieldComparator setNextReader(LeafReaderContext context) throws IOException {
       docVals = getValues(fcontext, context);
       return this;
     }
diff --git lucene/queries/src/java/org/apache/lucene/queries/function/docvalues/DocTermsIndexDocValues.java lucene/queries/src/java/org/apache/lucene/queries/function/docvalues/DocTermsIndexDocValues.java
index dea3ea3..babe65b 100644
--- lucene/queries/src/java/org/apache/lucene/queries/function/docvalues/DocTermsIndexDocValues.java
+++ lucene/queries/src/java/org/apache/lucene/queries/function/docvalues/DocTermsIndexDocValues.java
@@ -19,7 +19,7 @@ package org.apache.lucene.queries.function.docvalues;
 
 import java.io.IOException;
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.DocValues;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.SortedDocValues;
@@ -42,7 +42,7 @@ public abstract class DocTermsIndexDocValues extends FunctionValues {
   protected final MutableValueStr val = new MutableValueStr();
   protected final CharsRefBuilder spareChars = new CharsRefBuilder();
 
-  public DocTermsIndexDocValues(ValueSource vs, AtomicReaderContext context, String field) throws IOException {
+  public DocTermsIndexDocValues(ValueSource vs, LeafReaderContext context, String field) throws IOException {
     this(vs, open(context, field));
   }
   
@@ -159,7 +159,7 @@ public abstract class DocTermsIndexDocValues extends FunctionValues {
   }
 
   // TODO: why?
-  static SortedDocValues open(AtomicReaderContext context, String field) throws IOException {
+  static SortedDocValues open(LeafReaderContext context, String field) throws IOException {
     try {
       return DocValues.getSorted(context.reader(), field);
     } catch (RuntimeException e) {
diff --git lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/BytesRefFieldSource.java lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/BytesRefFieldSource.java
index ade6543..7585e7b 100644
--- lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/BytesRefFieldSource.java
+++ lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/BytesRefFieldSource.java
@@ -20,7 +20,7 @@ package org.apache.lucene.queries.function.valuesource;
 import java.io.IOException;
 import java.util.Map;
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.BinaryDocValues;
 import org.apache.lucene.index.DocValues;
 import org.apache.lucene.index.FieldInfo.DocValuesType;
@@ -42,7 +42,7 @@ public class BytesRefFieldSource extends FieldCacheSource {
   }
 
   @Override
-  public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
+  public FunctionValues getValues(Map context, LeafReaderContext readerContext) throws IOException {
     final FieldInfo fieldInfo = readerContext.reader().getFieldInfos().fieldInfo(field);
 
     // To be sorted or not to be sorted, that is the question
diff --git lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/ConstValueSource.java lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/ConstValueSource.java
index 457a87d..3836d64 100644
--- lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/ConstValueSource.java
+++ lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/ConstValueSource.java
@@ -17,7 +17,7 @@
 
 package org.apache.lucene.queries.function.valuesource;
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.docvalues.FloatDocValues;
 
@@ -42,7 +42,7 @@ public class ConstValueSource extends ConstNumberSource {
   }
 
   @Override
-  public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
+  public FunctionValues getValues(Map context, LeafReaderContext readerContext) throws IOException {
     return new FloatDocValues(this) {
       @Override
       public float floatVal(int doc) {
diff --git lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/DefFunction.java lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/DefFunction.java
index 7a0326f..c51add4 100644
--- lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/DefFunction.java
+++ lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/DefFunction.java
@@ -16,7 +16,7 @@ package org.apache.lucene.queries.function.valuesource;
  * limitations under the License.
  */
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.util.BytesRefBuilder;
@@ -43,7 +43,7 @@ public class DefFunction extends MultiFunction {
 
 
   @Override
-  public FunctionValues getValues(Map fcontext, AtomicReaderContext readerContext) throws IOException {
+  public FunctionValues getValues(Map fcontext, LeafReaderContext readerContext) throws IOException {
 
 
     return new Values(valsArr(sources, fcontext, readerContext)) {
diff --git lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/DocFreqValueSource.java lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/DocFreqValueSource.java
index 3b42539..8d9b7ce 100644
--- lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/DocFreqValueSource.java
+++ lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/DocFreqValueSource.java
@@ -17,7 +17,7 @@
 
 package org.apache.lucene.queries.function.valuesource;
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
@@ -146,7 +146,7 @@ public class DocFreqValueSource extends ValueSource {
   }
 
   @Override
-  public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
+  public FunctionValues getValues(Map context, LeafReaderContext readerContext) throws IOException {
     IndexSearcher searcher = (IndexSearcher)context.get("searcher");
     int docfreq = searcher.getIndexReader().docFreq(new Term(indexedField, indexedBytes));
     return new ConstIntDocValues(docfreq, this);
diff --git lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/DoubleConstValueSource.java lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/DoubleConstValueSource.java
index a3cea88..7ff0e5e 100644
--- lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/DoubleConstValueSource.java
+++ lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/DoubleConstValueSource.java
@@ -17,7 +17,7 @@
 
 package org.apache.lucene.queries.function.valuesource;
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.docvalues.DoubleDocValues;
 
@@ -44,7 +44,7 @@ public class DoubleConstValueSource extends ConstNumberSource {
   }
 
   @Override
-  public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
+  public FunctionValues getValues(Map context, LeafReaderContext readerContext) throws IOException {
     return new DoubleDocValues(this) {
       @Override
       public float floatVal(int doc) {
diff --git lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/DoubleFieldSource.java lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/DoubleFieldSource.java
index 1d5fd53..c04a202 100644
--- lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/DoubleFieldSource.java
+++ lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/DoubleFieldSource.java
@@ -20,8 +20,8 @@ package org.apache.lucene.queries.function.valuesource;
 import java.io.IOException;
 import java.util.Map;
 
-import org.apache.lucene.index.AtomicReader;
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReader;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.DocValues;
 import org.apache.lucene.index.NumericDocValues;
 import org.apache.lucene.queries.function.FunctionValues;
@@ -31,7 +31,7 @@ import org.apache.lucene.util.mutable.MutableValue;
 import org.apache.lucene.util.mutable.MutableValueDouble;
 
 /**
- * Obtains double field values from {@link AtomicReader#getNumericDocValues} and makes
+ * Obtains double field values from {@link org.apache.lucene.index.LeafReader#getNumericDocValues} and makes
  * those values available as other numeric types, casting as needed.
  */
 public class DoubleFieldSource extends FieldCacheSource {
@@ -46,7 +46,7 @@ public class DoubleFieldSource extends FieldCacheSource {
   }
 
   @Override
-  public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
+  public FunctionValues getValues(Map context, LeafReaderContext readerContext) throws IOException {
     final NumericDocValues arr = DocValues.getNumeric(readerContext.reader(), field);
     final Bits valid = DocValues.getDocsWithField(readerContext.reader(), field);
     return new DoubleDocValues(this) {
diff --git lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/DualFloatFunction.java lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/DualFloatFunction.java
index 241dfa0..3f3649a 100644
--- lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/DualFloatFunction.java
+++ lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/DualFloatFunction.java
@@ -17,7 +17,7 @@
 
 package org.apache.lucene.queries.function.valuesource;
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.queries.function.docvalues.FloatDocValues;
@@ -52,7 +52,7 @@ public abstract class DualFloatFunction extends ValueSource {
   }
 
   @Override
-  public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
+  public FunctionValues getValues(Map context, LeafReaderContext readerContext) throws IOException {
     final FunctionValues aVals =  a.getValues(context, readerContext);
     final FunctionValues bVals =  b.getValues(context, readerContext);
     return new FloatDocValues(this) {
diff --git lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/EnumFieldSource.java lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/EnumFieldSource.java
index bd8f7fd..59da29d 100644
--- lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/EnumFieldSource.java
+++ lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/EnumFieldSource.java
@@ -20,8 +20,7 @@ package org.apache.lucene.queries.function.valuesource;
 import java.io.IOException;
 import java.util.Map;
 
-import org.apache.lucene.index.AtomicReader;
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.DocValues;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.NumericDocValues;
@@ -33,7 +32,7 @@ import org.apache.lucene.util.mutable.MutableValue;
 import org.apache.lucene.util.mutable.MutableValueInt;
 
 /**
- * Obtains int field values from {@link AtomicReader#getNumericDocValues} and makes
+ * Obtains int field values from {@link org.apache.lucene.index.LeafReader#getNumericDocValues} and makes
  * those values available as other numeric types, casting as needed.
  * strVal of the value is not the int value, but its string (displayed) value
  */
@@ -97,7 +96,7 @@ public class EnumFieldSource extends FieldCacheSource {
 
 
   @Override
-  public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
+  public FunctionValues getValues(Map context, LeafReaderContext readerContext) throws IOException {
     final NumericDocValues arr = DocValues.getNumeric(readerContext.reader(), field);
     final Bits valid = DocValues.getDocsWithField(readerContext.reader(), field);
 
diff --git lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/FloatFieldSource.java lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/FloatFieldSource.java
index 36d578b..8f16661 100644
--- lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/FloatFieldSource.java
+++ lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/FloatFieldSource.java
@@ -20,8 +20,7 @@ package org.apache.lucene.queries.function.valuesource;
 import java.io.IOException;
 import java.util.Map;
 
-import org.apache.lucene.index.AtomicReader;
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.DocValues;
 import org.apache.lucene.index.NumericDocValues;
 import org.apache.lucene.queries.function.FunctionValues;
@@ -31,7 +30,7 @@ import org.apache.lucene.util.mutable.MutableValue;
 import org.apache.lucene.util.mutable.MutableValueFloat;
 
 /**
- * Obtains float field values from {@link AtomicReader#getNumericDocValues} and makes those
+ * Obtains float field values from {@link org.apache.lucene.index.LeafReader#getNumericDocValues} and makes those
  * values available as other numeric types, casting as needed.
  */
 public class FloatFieldSource extends FieldCacheSource {
@@ -46,7 +45,7 @@ public class FloatFieldSource extends FieldCacheSource {
   }
 
   @Override
-  public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
+  public FunctionValues getValues(Map context, LeafReaderContext readerContext) throws IOException {
     final NumericDocValues arr = DocValues.getNumeric(readerContext.reader(), field);
     final Bits valid = DocValues.getDocsWithField(readerContext.reader(), field);
 
diff --git lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/IDFValueSource.java lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/IDFValueSource.java
index 0c62020..2ef59df 100644
--- lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/IDFValueSource.java
+++ lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/IDFValueSource.java
@@ -46,7 +46,7 @@ public class IDFValueSource extends DocFreqValueSource {
   }
 
   @Override
-  public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
+  public FunctionValues getValues(Map context, LeafReaderContext readerContext) throws IOException {
     IndexSearcher searcher = (IndexSearcher)context.get("searcher");
     TFIDFSimilarity sim = asTFIDF(searcher.getSimilarity(), field);
     if (sim == null) {
diff --git lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/IfFunction.java lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/IfFunction.java
index 4062c63..84efbf8 100644
--- lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/IfFunction.java
+++ lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/IfFunction.java
@@ -17,16 +17,13 @@
 
 package org.apache.lucene.queries.function.valuesource;
 
-import org.apache.lucene.index.AtomicReaderContext;
-import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
-import org.apache.lucene.search.Explanation;
 import org.apache.lucene.search.IndexSearcher;
 import org.apache.lucene.util.BytesRefBuilder;
 
 import java.io.IOException;
-import java.util.List;
 import java.util.Map;
 
 
@@ -47,7 +44,7 @@ public class IfFunction extends BoolFunction {
   }
 
   @Override
-  public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
+  public FunctionValues getValues(Map context, LeafReaderContext readerContext) throws IOException {
     final FunctionValues ifVals = ifSource.getValues(context, readerContext);
     final FunctionValues trueVals = trueSource.getValues(context, readerContext);
     final FunctionValues falseVals = falseSource.getValues(context, readerContext);
diff --git lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/IntFieldSource.java lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/IntFieldSource.java
index 9659cc2..6d1414d 100644
--- lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/IntFieldSource.java
+++ lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/IntFieldSource.java
@@ -20,20 +20,17 @@ package org.apache.lucene.queries.function.valuesource;
 import java.io.IOException;
 import java.util.Map;
 
-import org.apache.lucene.index.AtomicReader;
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.DocValues;
-import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.NumericDocValues;
 import org.apache.lucene.queries.function.FunctionValues;
-import org.apache.lucene.queries.function.ValueSourceScorer;
 import org.apache.lucene.queries.function.docvalues.IntDocValues;
 import org.apache.lucene.util.Bits;
 import org.apache.lucene.util.mutable.MutableValue;
 import org.apache.lucene.util.mutable.MutableValueInt;
 
 /**
- * Obtains int field values from {@link AtomicReader#getNumericDocValues} and makes those
+ * Obtains int field values from {@link org.apache.lucene.index.LeafReader#getNumericDocValues} and makes those
  * values available as other numeric types, casting as needed.
  */
 public class IntFieldSource extends FieldCacheSource {
@@ -49,7 +46,7 @@ public class IntFieldSource extends FieldCacheSource {
 
 
   @Override
-  public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
+  public FunctionValues getValues(Map context, LeafReaderContext readerContext) throws IOException {
     final NumericDocValues arr = DocValues.getNumeric(readerContext.reader(), field);
     final Bits valid = DocValues.getDocsWithField(readerContext.reader(), field);
     
diff --git lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/JoinDocFreqValueSource.java lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/JoinDocFreqValueSource.java
index d5fa5f5..90f671b 100644
--- lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/JoinDocFreqValueSource.java
+++ lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/JoinDocFreqValueSource.java
@@ -20,7 +20,7 @@ package org.apache.lucene.queries.function.valuesource;
 import java.io.IOException;
 import java.util.Map;
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.BinaryDocValues;
 import org.apache.lucene.index.DocValues;
 import org.apache.lucene.index.IndexReader;
@@ -54,7 +54,7 @@ public class JoinDocFreqValueSource extends FieldCacheSource {
   }
 
   @Override
-  public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException
+  public FunctionValues getValues(Map context, LeafReaderContext readerContext) throws IOException
   {
     final BinaryDocValues terms = DocValues.getBinary(readerContext.reader(), field);
     final IndexReader top = ReaderUtil.getTopLevelContext(readerContext).reader();
diff --git lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/LinearFloatFunction.java lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/LinearFloatFunction.java
index 34eed4b..72c33e2 100644
--- lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/LinearFloatFunction.java
+++ lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/LinearFloatFunction.java
@@ -17,7 +17,7 @@
 
 package org.apache.lucene.queries.function.valuesource;
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.queries.function.docvalues.FloatDocValues;
@@ -51,7 +51,7 @@ public class LinearFloatFunction extends ValueSource {
   }
 
   @Override
-  public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
+  public FunctionValues getValues(Map context, LeafReaderContext readerContext) throws IOException {
     final FunctionValues vals =  source.getValues(context, readerContext);
     return new FloatDocValues(this) {
       @Override
diff --git lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/LiteralValueSource.java lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/LiteralValueSource.java
index a369bab..ab3f1d8 100644
--- lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/LiteralValueSource.java
+++ lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/LiteralValueSource.java
@@ -16,7 +16,7 @@ package org.apache.lucene.queries.function.valuesource;
  * limitations under the License.
  */
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.queries.function.docvalues.StrDocValues;
@@ -46,7 +46,7 @@ public class LiteralValueSource extends ValueSource {
   }
 
   @Override
-  public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
+  public FunctionValues getValues(Map context, LeafReaderContext readerContext) throws IOException {
 
     return new StrDocValues(this) {
       @Override
diff --git lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/LongFieldSource.java lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/LongFieldSource.java
index c63867e..d5900d9 100644
--- lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/LongFieldSource.java
+++ lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/LongFieldSource.java
@@ -20,8 +20,7 @@ package org.apache.lucene.queries.function.valuesource;
 import java.io.IOException;
 import java.util.Map;
 
-import org.apache.lucene.index.AtomicReader;
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.DocValues;
 import org.apache.lucene.index.NumericDocValues;
 import org.apache.lucene.queries.function.FunctionValues;
@@ -31,7 +30,7 @@ import org.apache.lucene.util.mutable.MutableValue;
 import org.apache.lucene.util.mutable.MutableValueLong;
 
 /**
- * Obtains long field values from {@link AtomicReader#getNumericDocValues} and makes those
+ * Obtains long field values from {@link org.apache.lucene.index.LeafReader#getNumericDocValues} and makes those
  * values available as other numeric types, casting as needed.
  */
 public class LongFieldSource extends FieldCacheSource {
@@ -58,7 +57,7 @@ public class LongFieldSource extends FieldCacheSource {
   }
 
   @Override
-  public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
+  public FunctionValues getValues(Map context, LeafReaderContext readerContext) throws IOException {
     final NumericDocValues arr = DocValues.getNumeric(readerContext.reader(), field);
     final Bits valid = DocValues.getDocsWithField(readerContext.reader(), field);
     
diff --git lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/MaxDocValueSource.java lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/MaxDocValueSource.java
index 66c58e5..ccd7326 100644
--- lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/MaxDocValueSource.java
+++ lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/MaxDocValueSource.java
@@ -16,7 +16,7 @@
  */
 package org.apache.lucene.queries.function.valuesource;
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.IndexReader; // javadocs
 import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
@@ -46,7 +46,7 @@ public class MaxDocValueSource extends ValueSource {
   }
 
   @Override
-  public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
+  public FunctionValues getValues(Map context, LeafReaderContext readerContext) throws IOException {
     IndexSearcher searcher = (IndexSearcher)context.get("searcher");
     return new ConstIntDocValues(searcher.getIndexReader().maxDoc(), this);
   }
diff --git lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/MultiBoolFunction.java lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/MultiBoolFunction.java
index 0075aa4..0f7bc2e 100644
--- lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/MultiBoolFunction.java
+++ lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/MultiBoolFunction.java
@@ -17,7 +17,7 @@
 
 package org.apache.lucene.queries.function.valuesource;
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.queries.function.docvalues.BoolDocValues;
@@ -43,7 +43,7 @@ public abstract class MultiBoolFunction extends BoolFunction {
   protected abstract boolean func(int doc, FunctionValues[] vals);
 
   @Override
-  public BoolDocValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
+  public BoolDocValues getValues(Map context, LeafReaderContext readerContext) throws IOException {
     final FunctionValues[] vals =  new FunctionValues[sources.size()];
     int i=0;
     for (ValueSource source : sources) {
diff --git lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/MultiFloatFunction.java lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/MultiFloatFunction.java
index d23c209..47b9ec1 100644
--- lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/MultiFloatFunction.java
+++ lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/MultiFloatFunction.java
@@ -16,7 +16,7 @@ package org.apache.lucene.queries.function.valuesource;
  * limitations under the License.
  */
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.queries.function.docvalues.FloatDocValues;
@@ -59,7 +59,7 @@ public abstract class MultiFloatFunction extends ValueSource {
   }
 
   @Override
-  public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
+  public FunctionValues getValues(Map context, LeafReaderContext readerContext) throws IOException {
     final FunctionValues[] valsArr = new FunctionValues[sources.length];
     for (int i=0; i<sources.length; i++) {
       valsArr[i] = sources[i].getValues(context, readerContext);
diff --git lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/MultiFunction.java lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/MultiFunction.java
index d3272a7..bfa7a2c 100644
--- lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/MultiFunction.java
+++ lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/MultiFunction.java
@@ -16,14 +16,12 @@ package org.apache.lucene.queries.function.valuesource;
  * limitations under the License.
  */
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.search.IndexSearcher;
-import org.apache.lucene.util.BytesRef;
 
 import java.io.IOException;
-import java.util.Arrays;
 import java.util.List;
 import java.util.Map;
 
@@ -61,7 +59,7 @@ public abstract class MultiFunction extends ValueSource {
     return sb.toString();
   }
 
-  public static FunctionValues[] valsArr(List<ValueSource> sources, Map fcontext, AtomicReaderContext readerContext) throws IOException {
+  public static FunctionValues[] valsArr(List<ValueSource> sources, Map fcontext, LeafReaderContext readerContext) throws IOException {
     final FunctionValues[] valsArr = new FunctionValues[sources.size()];
     int i=0;
     for (ValueSource source : sources) {
diff --git lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/NormValueSource.java lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/NormValueSource.java
index 81e4067..32ea93e 100644
--- lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/NormValueSource.java
+++ lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/NormValueSource.java
@@ -17,7 +17,7 @@
 
 package org.apache.lucene.queries.function.valuesource;
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.NumericDocValues;
 import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
@@ -56,7 +56,7 @@ public class NormValueSource extends ValueSource {
   }
 
   @Override
-  public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
+  public FunctionValues getValues(Map context, LeafReaderContext readerContext) throws IOException {
     IndexSearcher searcher = (IndexSearcher)context.get("searcher");
     final TFIDFSimilarity similarity = IDFValueSource.asTFIDF(searcher.getSimilarity(), field);
     if (similarity == null) {
diff --git lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/NumDocsValueSource.java lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/NumDocsValueSource.java
index 3d874ee..6f92f1e 100644
--- lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/NumDocsValueSource.java
+++ lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/NumDocsValueSource.java
@@ -16,7 +16,7 @@
  */
 package org.apache.lucene.queries.function.valuesource;
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.ReaderUtil;
 import org.apache.lucene.queries.function.FunctionValues;
@@ -41,7 +41,7 @@ public class NumDocsValueSource extends ValueSource {
   }
 
   @Override
-  public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
+  public FunctionValues getValues(Map context, LeafReaderContext readerContext) throws IOException {
     // Searcher has no numdocs so we must use the reader instead
     return new ConstIntDocValues(ReaderUtil.getTopLevelContext(readerContext).reader().numDocs(), this);
   }
diff --git lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/QueryValueSource.java lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/QueryValueSource.java
index 10a5f0d..3d57315 100644
--- lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/QueryValueSource.java
+++ lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/QueryValueSource.java
@@ -17,7 +17,7 @@
 
 package org.apache.lucene.queries.function.valuesource;
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.ReaderUtil;
 import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
@@ -51,7 +51,7 @@ public class QueryValueSource extends ValueSource {
   }
 
   @Override
-  public FunctionValues getValues(Map fcontext, AtomicReaderContext readerContext) throws IOException {
+  public FunctionValues getValues(Map fcontext, LeafReaderContext readerContext) throws IOException {
     return new QueryDocValues(this, readerContext, fcontext);
   }
 
@@ -76,7 +76,7 @@ public class QueryValueSource extends ValueSource {
 
 
 class QueryDocValues extends FloatDocValues {
-  final AtomicReaderContext readerContext;
+  final LeafReaderContext readerContext;
   final Bits acceptDocs;
   final Weight weight;
   final float defVal;
@@ -92,7 +92,7 @@ class QueryDocValues extends FloatDocValues {
   int lastDocRequested=Integer.MAX_VALUE;
   
 
-  public QueryDocValues(QueryValueSource vs, AtomicReaderContext readerContext, Map fcontext) throws IOException {
+  public QueryDocValues(QueryValueSource vs, LeafReaderContext readerContext, Map fcontext) throws IOException {
     super(vs);
 
     this.readerContext = readerContext;
diff --git lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/RangeMapFloatFunction.java lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/RangeMapFloatFunction.java
index 2402af8..6319655 100644
--- lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/RangeMapFloatFunction.java
+++ lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/RangeMapFloatFunction.java
@@ -17,7 +17,7 @@
 
 package org.apache.lucene.queries.function.valuesource;
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.queries.function.docvalues.FloatDocValues;
@@ -59,7 +59,7 @@ public class RangeMapFloatFunction extends ValueSource {
   }
 
   @Override
-  public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
+  public FunctionValues getValues(Map context, LeafReaderContext readerContext) throws IOException {
     final FunctionValues vals =  source.getValues(context, readerContext);
     final FunctionValues targets = target.getValues(context, readerContext);
     final FunctionValues defaults = (this.defaultVal == null) ? null : defaultVal.getValues(context, readerContext);
diff --git lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/ReciprocalFloatFunction.java lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/ReciprocalFloatFunction.java
index 63aa5d6..e939850 100644
--- lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/ReciprocalFloatFunction.java
+++ lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/ReciprocalFloatFunction.java
@@ -17,7 +17,7 @@
 
 package org.apache.lucene.queries.function.valuesource;
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.queries.function.docvalues.FloatDocValues;
@@ -61,7 +61,7 @@ public class ReciprocalFloatFunction extends ValueSource {
   }
 
   @Override
-  public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
+  public FunctionValues getValues(Map context, LeafReaderContext readerContext) throws IOException {
     final FunctionValues vals = source.getValues(context, readerContext);
     return new FloatDocValues(this) {
       @Override
diff --git lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/ScaleFloatFunction.java lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/ScaleFloatFunction.java
index 4771e32..9001307 100644
--- lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/ScaleFloatFunction.java
+++ lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/ScaleFloatFunction.java
@@ -17,7 +17,7 @@
 
 package org.apache.lucene.queries.function.valuesource;
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.ReaderUtil;
 import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
@@ -60,13 +60,13 @@ public class ScaleFloatFunction extends ValueSource {
     float maxVal;
   }
 
-  private ScaleInfo createScaleInfo(Map context, AtomicReaderContext readerContext) throws IOException {
-    final List<AtomicReaderContext> leaves = ReaderUtil.getTopLevelContext(readerContext).leaves();
+  private ScaleInfo createScaleInfo(Map context, LeafReaderContext readerContext) throws IOException {
+    final List<LeafReaderContext> leaves = ReaderUtil.getTopLevelContext(readerContext).leaves();
 
     float minVal = Float.POSITIVE_INFINITY;
     float maxVal = Float.NEGATIVE_INFINITY;
 
-    for (AtomicReaderContext leaf : leaves) {
+    for (LeafReaderContext leaf : leaves) {
       int maxDoc = leaf.reader().maxDoc();
       FunctionValues vals =  source.getValues(context, leaf);
       for (int i=0; i<maxDoc; i++) {
@@ -99,7 +99,7 @@ public class ScaleFloatFunction extends ValueSource {
   }
 
   @Override
-  public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
+  public FunctionValues getValues(Map context, LeafReaderContext readerContext) throws IOException {
 
     ScaleInfo scaleInfo = (ScaleInfo)context.get(ScaleFloatFunction.this);
     if (scaleInfo == null) {
diff --git lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/SimpleBoolFunction.java lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/SimpleBoolFunction.java
index d29aeff..b944b9b 100644
--- lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/SimpleBoolFunction.java
+++ lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/SimpleBoolFunction.java
@@ -17,7 +17,7 @@
 
 package org.apache.lucene.queries.function.valuesource;
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.queries.function.docvalues.BoolDocValues;
@@ -45,7 +45,7 @@ public abstract class SimpleBoolFunction extends BoolFunction {
   protected abstract boolean func(int doc, FunctionValues vals);
 
   @Override
-  public BoolDocValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
+  public BoolDocValues getValues(Map context, LeafReaderContext readerContext) throws IOException {
     final FunctionValues vals =  source.getValues(context, readerContext);
     return new BoolDocValues(this) {
       @Override
diff --git lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/SimpleFloatFunction.java lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/SimpleFloatFunction.java
index 54dd606..050b80e 100644
--- lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/SimpleFloatFunction.java
+++ lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/SimpleFloatFunction.java
@@ -17,7 +17,7 @@
 
 package org.apache.lucene.queries.function.valuesource;
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.queries.function.docvalues.FloatDocValues;
@@ -35,7 +35,7 @@ import java.util.Map;
   protected abstract float func(int doc, FunctionValues vals);
 
   @Override
-  public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
+  public FunctionValues getValues(Map context, LeafReaderContext readerContext) throws IOException {
     final FunctionValues vals =  source.getValues(context, readerContext);
     return new FloatDocValues(this) {
       @Override
diff --git lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/SortedSetFieldSource.java lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/SortedSetFieldSource.java
index eda61c5..cbb3085 100644
--- lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/SortedSetFieldSource.java
+++ lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/SortedSetFieldSource.java
@@ -20,7 +20,7 @@ package org.apache.lucene.queries.function.valuesource;
 import java.io.IOException;
 import java.util.Map;
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.DocValues;
 import org.apache.lucene.index.SortedDocValues;
 import org.apache.lucene.index.SortedSetDocValues;
@@ -49,7 +49,7 @@ public class SortedSetFieldSource extends FieldCacheSource {
   }
 
   @Override
-  public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
+  public FunctionValues getValues(Map context, LeafReaderContext readerContext) throws IOException {
     SortedSetDocValues sortedSet = DocValues.getSortedSet(readerContext.reader(), field);
     SortedDocValues view = SortedSetSelector.wrap(sortedSet, selector);
     return new DocTermsIndexDocValues(this, view) {
diff --git lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/SumTotalTermFreqValueSource.java lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/SumTotalTermFreqValueSource.java
index e9ab075..1dd8a98 100644
--- lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/SumTotalTermFreqValueSource.java
+++ lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/SumTotalTermFreqValueSource.java
@@ -17,14 +17,13 @@
 
 package org.apache.lucene.queries.function.valuesource;
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.Fields;
 import org.apache.lucene.index.Terms;
 import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.queries.function.docvalues.LongDocValues;
 import org.apache.lucene.search.IndexSearcher;
-import org.apache.lucene.util.BytesRef;
 
 import java.io.IOException;
 import java.util.Map;
@@ -53,14 +52,14 @@ public class SumTotalTermFreqValueSource extends ValueSource {
   }
 
   @Override
-  public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
+  public FunctionValues getValues(Map context, LeafReaderContext readerContext) throws IOException {
     return (FunctionValues)context.get(this);
   }
 
   @Override
   public void createWeight(Map context, IndexSearcher searcher) throws IOException {
     long sumTotalTermFreq = 0;
-    for (AtomicReaderContext readerContext : searcher.getTopReaderContext().leaves()) {
+    for (LeafReaderContext readerContext : searcher.getTopReaderContext().leaves()) {
       Fields fields = readerContext.reader().fields();
       if (fields == null) continue;
       Terms terms = fields.terms(indexedField);
diff --git lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/TFValueSource.java lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/TFValueSource.java
index 5fe1af4..4d73d55 100644
--- lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/TFValueSource.java
+++ lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/TFValueSource.java
@@ -46,7 +46,7 @@ public class TFValueSource extends TermFreqValueSource {
   }
 
   @Override
-  public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
+  public FunctionValues getValues(Map context, LeafReaderContext readerContext) throws IOException {
     Fields fields = readerContext.reader().fields();
     final Terms terms = fields.terms(indexedField);
     IndexSearcher searcher = (IndexSearcher)context.get("searcher");
diff --git lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/TermFreqValueSource.java lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/TermFreqValueSource.java
index 3fba5f8..b5e4bc2 100644
--- lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/TermFreqValueSource.java
+++ lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/TermFreqValueSource.java
@@ -44,7 +44,7 @@ public class TermFreqValueSource extends DocFreqValueSource {
   }
 
   @Override
-  public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
+  public FunctionValues getValues(Map context, LeafReaderContext readerContext) throws IOException {
     Fields fields = readerContext.reader().fields();
     final Terms terms = fields.terms(indexedField);
 
diff --git lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/TotalTermFreqValueSource.java lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/TotalTermFreqValueSource.java
index 90ae4e1..5002928 100644
--- lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/TotalTermFreqValueSource.java
+++ lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/TotalTermFreqValueSource.java
@@ -17,7 +17,7 @@
 
 package org.apache.lucene.queries.function.valuesource;
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
@@ -58,14 +58,14 @@ public class TotalTermFreqValueSource extends ValueSource {
   }
 
   @Override
-  public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
+  public FunctionValues getValues(Map context, LeafReaderContext readerContext) throws IOException {
     return (FunctionValues)context.get(this);
   }
 
   @Override
   public void createWeight(Map context, IndexSearcher searcher) throws IOException {
     long totalTermFreq = 0;
-    for (AtomicReaderContext readerContext : searcher.getTopReaderContext().leaves()) {
+    for (LeafReaderContext readerContext : searcher.getTopReaderContext().leaves()) {
       long val = readerContext.reader().totalTermFreq(new Term(indexedField, indexedBytes));
       if (val == -1) {
         totalTermFreq = -1;
diff --git lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/VectorValueSource.java lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/VectorValueSource.java
index 8d443b4..78db619 100644
--- lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/VectorValueSource.java
+++ lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/VectorValueSource.java
@@ -16,7 +16,7 @@ package org.apache.lucene.queries.function.valuesource;
  * limitations under the License.
  */
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.search.IndexSearcher;
@@ -53,7 +53,7 @@ public class VectorValueSource extends MultiValueSource {
   }
 
   @Override
-  public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
+  public FunctionValues getValues(Map context, LeafReaderContext readerContext) throws IOException {
     int size = sources.size();
 
     // special-case x,y and lat,lon since it's so common
diff --git lucene/queries/src/test/org/apache/lucene/queries/BooleanFilterTest.java lucene/queries/src/test/org/apache/lucene/queries/BooleanFilterTest.java
index e225af4..c0d0261 100644
--- lucene/queries/src/test/org/apache/lucene/queries/BooleanFilterTest.java
+++ lucene/queries/src/test/org/apache/lucene/queries/BooleanFilterTest.java
@@ -21,8 +21,8 @@ import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.analysis.MockTokenizer;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
-import org.apache.lucene.index.AtomicReader;
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReader;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.RandomIndexWriter;
 import org.apache.lucene.index.SlowCompositeReaderWrapper;
 import org.apache.lucene.index.Term;
@@ -42,7 +42,7 @@ import java.io.IOException;
 
 public class BooleanFilterTest extends LuceneTestCase {
   private Directory directory;
-  private AtomicReader reader;
+  private LeafReader reader;
 
   @Override
   public void setUp() throws Exception {
@@ -92,7 +92,7 @@ public class BooleanFilterTest extends LuceneTestCase {
   private Filter getEmptyFilter() {
     return new Filter() {
       @Override
-      public DocIdSet getDocIdSet(AtomicReaderContext context, Bits acceptDocs) {
+      public DocIdSet getDocIdSet(LeafReaderContext context, Bits acceptDocs) {
         return new FixedBitSet(context.reader().maxDoc());
       }
     };
@@ -101,7 +101,7 @@ public class BooleanFilterTest extends LuceneTestCase {
   private Filter getNullDISFilter() {
     return new Filter() {
       @Override
-      public DocIdSet getDocIdSet(AtomicReaderContext context, Bits acceptDocs) {
+      public DocIdSet getDocIdSet(LeafReaderContext context, Bits acceptDocs) {
         return null;
       }
     };
@@ -110,7 +110,7 @@ public class BooleanFilterTest extends LuceneTestCase {
   private Filter getNullDISIFilter() {
     return new Filter() {
       @Override
-      public DocIdSet getDocIdSet(AtomicReaderContext context, Bits acceptDocs) {
+      public DocIdSet getDocIdSet(LeafReaderContext context, Bits acceptDocs) {
         return DocIdSet.EMPTY;
       }
     };
diff --git lucene/queries/src/test/org/apache/lucene/queries/CommonTermsQueryTest.java lucene/queries/src/test/org/apache/lucene/queries/CommonTermsQueryTest.java
index 6888489..0f2fc08 100644
--- lucene/queries/src/test/org/apache/lucene/queries/CommonTermsQueryTest.java
+++ lucene/queries/src/test/org/apache/lucene/queries/CommonTermsQueryTest.java
@@ -28,7 +28,7 @@ import java.util.Set;
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
-import org.apache.lucene.index.AtomicReader;
+import org.apache.lucene.index.LeafReader;
 import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexWriter;
@@ -407,7 +407,7 @@ public class CommonTermsQueryTest extends LuceneTestCase {
     RandomIndexWriter w = new RandomIndexWriter(random(), dir, analyzer);
     createRandomIndex(atLeast(50), w, random().nextLong());
     DirectoryReader reader = w.getReader();
-    AtomicReader wrapper = SlowCompositeReaderWrapper.wrap(reader);
+    LeafReader wrapper = SlowCompositeReaderWrapper.wrap(reader);
     String field = "body";
     Terms terms = wrapper.terms(field);
     PriorityQueue<TermAndFreq> lowFreqQueue = new PriorityQueue<CommonTermsQueryTest.TermAndFreq>(
diff --git lucene/queries/src/test/org/apache/lucene/queries/TermFilterTest.java lucene/queries/src/test/org/apache/lucene/queries/TermFilterTest.java
index 41e30db..7f0da76 100644
--- lucene/queries/src/test/org/apache/lucene/queries/TermFilterTest.java
+++ lucene/queries/src/test/org/apache/lucene/queries/TermFilterTest.java
@@ -19,7 +19,7 @@ package org.apache.lucene.queries;
 
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.RandomIndexWriter;
 import org.apache.lucene.index.SlowCompositeReaderWrapper;
@@ -60,8 +60,8 @@ public class TermFilterTest extends LuceneTestCase {
     doc.add(newStringField(fieldName, "value1", Field.Store.NO));
     w.addDocument(doc);
     IndexReader reader = SlowCompositeReaderWrapper.wrap(w.getReader());
-    assertTrue(reader.getContext() instanceof AtomicReaderContext);
-    AtomicReaderContext context = (AtomicReaderContext) reader.getContext();
+    assertTrue(reader.getContext() instanceof LeafReaderContext);
+    LeafReaderContext context = (LeafReaderContext) reader.getContext();
     w.close();
 
     DocIdSet idSet = termFilter(fieldName, "value1").getDocIdSet(context, context.reader().getLiveDocs());
diff --git lucene/queries/src/test/org/apache/lucene/queries/TermsFilterTest.java lucene/queries/src/test/org/apache/lucene/queries/TermsFilterTest.java
index 36ebadd..8d1be0c 100644
--- lucene/queries/src/test/org/apache/lucene/queries/TermsFilterTest.java
+++ lucene/queries/src/test/org/apache/lucene/queries/TermsFilterTest.java
@@ -28,7 +28,7 @@ import java.util.Set;
 
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.MultiReader;
 import org.apache.lucene.index.RandomIndexWriter;
@@ -75,8 +75,8 @@ public class TermsFilterTest extends LuceneTestCase {
       w.addDocument(doc);
     }
     IndexReader reader = SlowCompositeReaderWrapper.wrap(w.getReader());
-    assertTrue(reader.getContext() instanceof AtomicReaderContext);
-    AtomicReaderContext context = (AtomicReaderContext) reader.getContext();
+    assertTrue(reader.getContext() instanceof LeafReaderContext);
+    LeafReaderContext context = (LeafReaderContext) reader.getContext();
     w.close();
 
     List<Term> terms = new ArrayList<>();
@@ -121,7 +121,7 @@ public class TermsFilterTest extends LuceneTestCase {
     
     TermsFilter tf = new TermsFilter(new Term(fieldName, "content1"));
     MultiReader multi = new MultiReader(reader1, reader2);
-    for (AtomicReaderContext context : multi.leaves()) {
+    for (LeafReaderContext context : multi.leaves()) {
       DocIdSet docIdSet = tf.getDocIdSet(context, context.reader().getLiveDocs());
       if (context.reader().docFreq(new Term(fieldName, "content1")) == 0) {
         assertNull(docIdSet);
@@ -160,7 +160,7 @@ public class TermsFilterTest extends LuceneTestCase {
     
     
     
-    AtomicReaderContext context = reader.leaves().get(0);
+    LeafReaderContext context = reader.leaves().get(0);
     TermsFilter tf = new TermsFilter(terms);
 
     FixedBitSet bits = (FixedBitSet) tf.getDocIdSet(context, context.reader().getLiveDocs());
@@ -196,7 +196,7 @@ public class TermsFilterTest extends LuceneTestCase {
     IndexReader reader = w.getReader();
     w.close();
     assertEquals(1, reader.leaves().size());
-    AtomicReaderContext context = reader.leaves().get(0);
+    LeafReaderContext context = reader.leaves().get(0);
     TermsFilter tf = new TermsFilter(new ArrayList<>(terms));
 
     FixedBitSet bits = (FixedBitSet) tf.getDocIdSet(context, context.reader().getLiveDocs());
diff --git lucene/queries/src/test/org/apache/lucene/queries/TestCustomScoreQuery.java lucene/queries/src/test/org/apache/lucene/queries/TestCustomScoreQuery.java
index 4f32e58..1709a01 100644
--- lucene/queries/src/test/org/apache/lucene/queries/TestCustomScoreQuery.java
+++ lucene/queries/src/test/org/apache/lucene/queries/TestCustomScoreQuery.java
@@ -17,6 +17,7 @@ package org.apache.lucene.queries;
  * limitations under the License.
  */
 
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.queries.function.FunctionQuery;
 import org.apache.lucene.queries.function.FunctionTestSetup;
 import org.apache.lucene.queries.function.ValueSource;
@@ -36,7 +37,6 @@ import java.io.IOException;
 import java.util.HashMap;
 import java.util.Map;
 
-import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.DocValues;
 import org.apache.lucene.index.IndexReader;
@@ -85,7 +85,7 @@ public class TestCustomScoreQuery extends FunctionTestSetup {
     }
     
     @Override
-    protected CustomScoreProvider getCustomScoreProvider(AtomicReaderContext context) {
+    protected CustomScoreProvider getCustomScoreProvider(LeafReaderContext context) {
       return new CustomScoreProvider(context) {
         @Override
         public float customScore(int doc, float subQueryScore, float valSrcScore) {
@@ -120,7 +120,7 @@ public class TestCustomScoreQuery extends FunctionTestSetup {
     }
 
     @Override
-    protected CustomScoreProvider getCustomScoreProvider(AtomicReaderContext context) {
+    protected CustomScoreProvider getCustomScoreProvider(LeafReaderContext context) {
       return new CustomScoreProvider(context) {
         @Override
         public float customScore(int doc, float subQueryScore, float valSrcScores[]) {
@@ -159,7 +159,7 @@ public class TestCustomScoreQuery extends FunctionTestSetup {
   private final class CustomExternalQuery extends CustomScoreQuery {
 
     @Override
-    protected CustomScoreProvider getCustomScoreProvider(AtomicReaderContext context) throws IOException {
+    protected CustomScoreProvider getCustomScoreProvider(LeafReaderContext context) throws IOException {
       final NumericDocValues values = DocValues.getNumeric(context.reader(), INT_FIELD);
       return new CustomScoreProvider(context) {
         @Override
diff --git lucene/queries/src/test/org/apache/lucene/queries/function/TestDocValuesFieldSources.java lucene/queries/src/test/org/apache/lucene/queries/function/TestDocValuesFieldSources.java
index c4dbd76..beae905 100644
--- lucene/queries/src/test/org/apache/lucene/queries/function/TestDocValuesFieldSources.java
+++ lucene/queries/src/test/org/apache/lucene/queries/function/TestDocValuesFieldSources.java
@@ -25,7 +25,7 @@ import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.NumericDocValuesField;
 import org.apache.lucene.document.SortedDocValuesField;
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.FieldInfo.DocValuesType;
 import org.apache.lucene.index.IndexWriterConfig;
@@ -94,7 +94,7 @@ public class TestDocValuesFieldSources extends LuceneTestCase {
     iw.close();
 
     DirectoryReader rd = DirectoryReader.open(d);
-    for (AtomicReaderContext leave : rd.leaves()) {
+    for (LeafReaderContext leave : rd.leaves()) {
       final FunctionValues ids = new LongFieldSource("id").getValues(null, leave);
       final ValueSource vs;
       switch (type) {
diff --git lucene/queries/src/test/org/apache/lucene/queries/function/TestSortedSetFieldSource.java lucene/queries/src/test/org/apache/lucene/queries/function/TestSortedSetFieldSource.java
index 09fc6ae..c689198 100644
--- lucene/queries/src/test/org/apache/lucene/queries/function/TestSortedSetFieldSource.java
+++ lucene/queries/src/test/org/apache/lucene/queries/function/TestSortedSetFieldSource.java
@@ -22,7 +22,7 @@ import java.util.Collections;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.SortedSetDocValuesField;
-import org.apache.lucene.index.AtomicReader;
+import org.apache.lucene.index.LeafReader;
 import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexWriter;
 import org.apache.lucene.queries.function.valuesource.SortedSetFieldSource;
@@ -47,7 +47,7 @@ public class TestSortedSetFieldSource extends LuceneTestCase {
     writer.close();
 
     DirectoryReader ir = DirectoryReader.open(dir);
-    AtomicReader ar = getOnlySegmentReader(ir);
+    LeafReader ar = getOnlySegmentReader(ir);
     
     ValueSource vs = new SortedSetFieldSource("value");
     FunctionValues values = vs.getValues(Collections.emptyMap(), ar.getContext());
diff --git lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/builders/NumericRangeFilterBuilder.java lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/builders/NumericRangeFilterBuilder.java
index e7fba72..8f962c8 100644
--- lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/builders/NumericRangeFilterBuilder.java
+++ lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/builders/NumericRangeFilterBuilder.java
@@ -17,7 +17,7 @@ package org.apache.lucene.queryparser.xml.builders;
  * limitations under the License.
  */
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.search.DocIdSet;
 import org.apache.lucene.search.Filter;
 import org.apache.lucene.search.NumericRangeFilter;
@@ -155,7 +155,7 @@ public class NumericRangeFilterBuilder implements FilterBuilder {
   static class NoMatchFilter extends Filter {
 
     @Override
-    public DocIdSet getDocIdSet(AtomicReaderContext context, Bits acceptDocs) throws IOException {
+    public DocIdSet getDocIdSet(LeafReaderContext context, Bits acceptDocs) throws IOException {
       return null;
     }
 
diff --git lucene/queryparser/src/test/org/apache/lucene/queryparser/surround/query/BooleanQueryTst.java lucene/queryparser/src/test/org/apache/lucene/queryparser/surround/query/BooleanQueryTst.java
index 4c6349b..842d43b 100644
--- lucene/queryparser/src/test/org/apache/lucene/queryparser/surround/query/BooleanQueryTst.java
+++ lucene/queryparser/src/test/org/apache/lucene/queryparser/surround/query/BooleanQueryTst.java
@@ -19,12 +19,10 @@ package org.apache.lucene.queryparser.surround.query;
 
 import java.io.IOException;
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.search.LeafCollector;
 import org.apache.lucene.search.IndexSearcher;
-import org.apache.lucene.search.Collector;
 import org.apache.lucene.search.Scorer;
 import org.apache.lucene.search.Query;
 import org.apache.lucene.search.SimpleCollector;
@@ -79,7 +77,7 @@ public class BooleanQueryTst {
     }
 
     @Override
-    protected void doSetNextReader(AtomicReaderContext context) throws IOException {
+    protected void doSetNextReader(LeafReaderContext context) throws IOException {
       docBase = context.docBase;
     }
     
diff --git lucene/queryparser/src/test/org/apache/lucene/queryparser/xml/TestParser.java lucene/queryparser/src/test/org/apache/lucene/queryparser/xml/TestParser.java
index 9db7890..e5c61c2 100644
--- lucene/queryparser/src/test/org/apache/lucene/queryparser/xml/TestParser.java
+++ lucene/queryparser/src/test/org/apache/lucene/queryparser/xml/TestParser.java
@@ -24,7 +24,7 @@ import org.apache.lucene.analysis.MockTokenizer;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.IntField;
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexWriter;
@@ -197,7 +197,7 @@ public class TestParser extends LuceneTestCase {
   }
 
   public void testDuplicateFilterQueryXML() throws ParserException, IOException {
-    List<AtomicReaderContext> leaves = searcher.getTopReaderContext().leaves();
+    List<LeafReaderContext> leaves = searcher.getTopReaderContext().leaves();
     Assume.assumeTrue(leaves.size() == 1);
     Query q = parse("DuplicateFilterQuery.xml");
     int h = searcher.search(q, null, 1000).totalHits;
diff --git lucene/queryparser/src/test/org/apache/lucene/queryparser/xml/builders/TestNumericRangeFilterBuilder.java lucene/queryparser/src/test/org/apache/lucene/queryparser/xml/builders/TestNumericRangeFilterBuilder.java
index 516228d..bb154ac 100644
--- lucene/queryparser/src/test/org/apache/lucene/queryparser/xml/builders/TestNumericRangeFilterBuilder.java
+++ lucene/queryparser/src/test/org/apache/lucene/queryparser/xml/builders/TestNumericRangeFilterBuilder.java
@@ -17,7 +17,7 @@ package org.apache.lucene.queryparser.xml.builders;
  * limitations under the License.
  */
 
-import org.apache.lucene.index.AtomicReader;
+import org.apache.lucene.index.LeafReader;
 import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexWriter;
 import org.apache.lucene.index.SlowCompositeReaderWrapper;
@@ -65,7 +65,7 @@ public class TestNumericRangeFilterBuilder extends LuceneTestCase {
     IndexWriter writer = new IndexWriter(ramDir, newIndexWriterConfig(null));
     writer.commit();
     try {
-      AtomicReader reader = SlowCompositeReaderWrapper.wrap(DirectoryReader.open(ramDir));
+      LeafReader reader = SlowCompositeReaderWrapper.wrap(DirectoryReader.open(ramDir));
       try {
         assertNull(filter.getDocIdSet(reader.getContext(), reader.getLiveDocs()));
       }
diff --git lucene/sandbox/src/java/org/apache/lucene/sandbox/queries/DuplicateFilter.java lucene/sandbox/src/java/org/apache/lucene/sandbox/queries/DuplicateFilter.java
index 5459fdf..07c8c3d 100644
--- lucene/sandbox/src/java/org/apache/lucene/sandbox/queries/DuplicateFilter.java
+++ lucene/sandbox/src/java/org/apache/lucene/sandbox/queries/DuplicateFilter.java
@@ -79,7 +79,7 @@ public class DuplicateFilter extends Filter {
   }
 
   @Override
-  public DocIdSet getDocIdSet(AtomicReaderContext context, Bits acceptDocs) throws IOException {
+  public DocIdSet getDocIdSet(LeafReaderContext context, Bits acceptDocs) throws IOException {
     if (processingMode == ProcessingMode.PM_FAST_INVALIDATION) {
       return fastBits(context.reader(), acceptDocs);
     } else {
@@ -87,7 +87,7 @@ public class DuplicateFilter extends Filter {
     }
   }
 
-  private FixedBitSet correctBits(AtomicReader reader, Bits acceptDocs) throws IOException {
+  private FixedBitSet correctBits(LeafReader reader, Bits acceptDocs) throws IOException {
     FixedBitSet bits = new FixedBitSet(reader.maxDoc()); //assume all are INvalid
     Terms terms = reader.fields().terms(fieldName);
 
@@ -124,7 +124,7 @@ public class DuplicateFilter extends Filter {
     return bits;
   }
 
-  private FixedBitSet fastBits(AtomicReader reader, Bits acceptDocs) throws IOException {
+  private FixedBitSet fastBits(LeafReader reader, Bits acceptDocs) throws IOException {
     FixedBitSet bits = new FixedBitSet(reader.maxDoc());
     bits.set(0, reader.maxDoc()); //assume all are valid
     Terms terms = reader.fields().terms(fieldName);
diff --git lucene/sandbox/src/java/org/apache/lucene/sandbox/queries/SlowCollatedStringComparator.java lucene/sandbox/src/java/org/apache/lucene/sandbox/queries/SlowCollatedStringComparator.java
index f3eee67..311e128 100644
--- lucene/sandbox/src/java/org/apache/lucene/sandbox/queries/SlowCollatedStringComparator.java
+++ lucene/sandbox/src/java/org/apache/lucene/sandbox/queries/SlowCollatedStringComparator.java
@@ -20,7 +20,7 @@ package org.apache.lucene.sandbox.queries;
 import java.io.IOException;
 import java.text.Collator;
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.BinaryDocValues;
 import org.apache.lucene.index.DocValues;
 import org.apache.lucene.search.FieldComparator;
@@ -93,7 +93,7 @@ public final class SlowCollatedStringComparator extends FieldComparator<String>
   }
 
   @Override
-  public FieldComparator<String> setNextReader(AtomicReaderContext context) throws IOException {
+  public FieldComparator<String> setNextReader(LeafReaderContext context) throws IOException {
     currentDocTerms = DocValues.getBinary(context.reader(), field);
     docsWithField = DocValues.getDocsWithField(context.reader(), field);
     return this;
diff --git lucene/sandbox/src/java/org/apache/lucene/search/TermAutomatonQuery.java lucene/sandbox/src/java/org/apache/lucene/search/TermAutomatonQuery.java
index 6c24bde..dbd1707 100644
--- lucene/sandbox/src/java/org/apache/lucene/search/TermAutomatonQuery.java
+++ lucene/sandbox/src/java/org/apache/lucene/search/TermAutomatonQuery.java
@@ -24,7 +24,7 @@ import java.util.List;
 import java.util.Map;
 import java.util.Set;
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.DocsAndPositionsEnum;
 import org.apache.lucene.index.IndexReaderContext;
 import org.apache.lucene.index.ReaderUtil;
@@ -372,7 +372,7 @@ public class TermAutomatonQuery extends Query {
     }
 
     @Override
-    public Scorer scorer(AtomicReaderContext context, Bits acceptDocs) throws IOException {
+    public Scorer scorer(LeafReaderContext context, Bits acceptDocs) throws IOException {
 
       // Initialize the enums; null for a given slot means that term didn't appear in this reader
       EnumAndScorer[] enums = new EnumAndScorer[idToTerm.size()];
@@ -395,7 +395,7 @@ public class TermAutomatonQuery extends Query {
     }
     
     @Override
-    public Explanation explain(AtomicReaderContext context, int doc) throws IOException {
+    public Explanation explain(LeafReaderContext context, int doc) throws IOException {
       // TODO
       return null;
     }
diff --git lucene/sandbox/src/java/org/apache/lucene/search/TermAutomatonScorer.java lucene/sandbox/src/java/org/apache/lucene/search/TermAutomatonScorer.java
index b862869..106c307 100644
--- lucene/sandbox/src/java/org/apache/lucene/search/TermAutomatonScorer.java
+++ lucene/sandbox/src/java/org/apache/lucene/search/TermAutomatonScorer.java
@@ -20,7 +20,6 @@ package org.apache.lucene.search;
 import java.io.IOException;
 import java.util.Map;
 
-import org.apache.lucene.index.AtomicReader;
 import org.apache.lucene.search.TermAutomatonQuery.EnumAndScorer;
 import org.apache.lucene.search.TermAutomatonQuery.TermAutomatonWeight;
 import org.apache.lucene.search.similarities.Similarity;
diff --git lucene/sandbox/src/test/org/apache/lucene/search/TestTermAutomatonQuery.java lucene/sandbox/src/test/org/apache/lucene/search/TestTermAutomatonQuery.java
index cdcdf6f..7107f9b 100644
--- lucene/sandbox/src/test/org/apache/lucene/search/TestTermAutomatonQuery.java
+++ lucene/sandbox/src/test/org/apache/lucene/search/TestTermAutomatonQuery.java
@@ -38,7 +38,7 @@ import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.StoredField;
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexWriterConfig;
 import org.apache.lucene.index.RandomIndexWriter;
@@ -51,7 +51,6 @@ import org.apache.lucene.util.LuceneTestCase;
 import org.apache.lucene.util.TestUtil;
 import org.apache.lucene.util.automaton.Automata;
 import org.apache.lucene.util.automaton.Automaton;
-import org.apache.lucene.util.automaton.Operations;
 import org.apache.lucene.util.automaton.Transition;
 
 public class TestTermAutomatonQuery extends LuceneTestCase {
@@ -627,7 +626,7 @@ public class TestTermAutomatonQuery extends LuceneTestCase {
     }
 
     @Override
-    public DocIdSet getDocIdSet(AtomicReaderContext context, Bits acceptDocs) throws IOException {
+    public DocIdSet getDocIdSet(LeafReaderContext context, Bits acceptDocs) throws IOException {
       int maxDoc = context.reader().maxDoc();
       FixedBitSet bits = new FixedBitSet(maxDoc);
       Random random = new Random(seed ^ context.docBase);
diff --git lucene/spatial/src/java/org/apache/lucene/spatial/bbox/BBoxSimilarityValueSource.java lucene/spatial/src/java/org/apache/lucene/spatial/bbox/BBoxSimilarityValueSource.java
index 50e4fa2..28062f6 100644
--- lucene/spatial/src/java/org/apache/lucene/spatial/bbox/BBoxSimilarityValueSource.java
+++ lucene/spatial/src/java/org/apache/lucene/spatial/bbox/BBoxSimilarityValueSource.java
@@ -18,7 +18,7 @@ package org.apache.lucene.spatial.bbox;
  */
 
 import com.spatial4j.core.shape.Rectangle;
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.queries.function.docvalues.DoubleDocValues;
@@ -60,7 +60,7 @@ public abstract class BBoxSimilarityValueSource extends ValueSource {
   protected abstract String similarityDescription();
 
   @Override
-  public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
+  public FunctionValues getValues(Map context, LeafReaderContext readerContext) throws IOException {
 
     final FunctionValues shapeValues = bboxValueSource.getValues(context, readerContext);
 
diff --git lucene/spatial/src/java/org/apache/lucene/spatial/bbox/BBoxStrategy.java lucene/spatial/src/java/org/apache/lucene/spatial/bbox/BBoxStrategy.java
index 9e40f45..8060926 100644
--- lucene/spatial/src/java/org/apache/lucene/spatial/bbox/BBoxStrategy.java
+++ lucene/spatial/src/java/org/apache/lucene/spatial/bbox/BBoxStrategy.java
@@ -26,7 +26,6 @@ import org.apache.lucene.document.DoubleField;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.FieldType;
 import org.apache.lucene.document.StringField;
-import org.apache.lucene.index.AtomicReader;
 import org.apache.lucene.index.FieldInfo;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.queries.function.ValueSource;
@@ -43,7 +42,6 @@ import org.apache.lucene.spatial.query.SpatialArgs;
 import org.apache.lucene.spatial.query.SpatialOperation;
 import org.apache.lucene.spatial.query.UnsupportedSpatialOperation;
 import org.apache.lucene.spatial.util.DistanceToShapeValueSource;
-import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.BytesRefBuilder;
 import org.apache.lucene.util.NumericUtils;
 
@@ -71,7 +69,7 @@ import org.apache.lucene.util.NumericUtils;
  * The {@link #makeOverlapRatioValueSource(com.spatial4j.core.shape.Rectangle, double)}
  * works by calculating the query bbox overlap percentage against the indexed
  * shape overlap percentage. The indexed shape's coordinates are retrieved from
- * {@link AtomicReader#getNumericDocValues}.
+ * {@link org.apache.lucene.index.LeafReader#getNumericDocValues}.
  *
  * @lucene.experimental
  */
diff --git lucene/spatial/src/java/org/apache/lucene/spatial/bbox/BBoxValueSource.java lucene/spatial/src/java/org/apache/lucene/spatial/bbox/BBoxValueSource.java
index 5e62260..6c9a929 100644
--- lucene/spatial/src/java/org/apache/lucene/spatial/bbox/BBoxValueSource.java
+++ lucene/spatial/src/java/org/apache/lucene/spatial/bbox/BBoxValueSource.java
@@ -18,8 +18,8 @@ package org.apache.lucene.spatial.bbox;
  */
 
 import com.spatial4j.core.shape.Rectangle;
-import org.apache.lucene.index.AtomicReader;
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReader;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.DocValues;
 import org.apache.lucene.index.NumericDocValues;
 import org.apache.lucene.queries.function.FunctionValues;
@@ -50,8 +50,8 @@ class BBoxValueSource extends ValueSource {
   }
 
   @Override
-  public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
-    AtomicReader reader = readerContext.reader();
+  public FunctionValues getValues(Map context, LeafReaderContext readerContext) throws IOException {
+    LeafReader reader = readerContext.reader();
     final NumericDocValues minX = DocValues.getNumeric(reader, strategy.field_minX);
     final NumericDocValues minY = DocValues.getNumeric(reader, strategy.field_minY);
     final NumericDocValues maxX = DocValues.getNumeric(reader, strategy.field_maxX);
diff --git lucene/spatial/src/java/org/apache/lucene/spatial/prefix/AbstractPrefixTreeFilter.java lucene/spatial/src/java/org/apache/lucene/spatial/prefix/AbstractPrefixTreeFilter.java
index f5eca43..71d3693 100644
--- lucene/spatial/src/java/org/apache/lucene/spatial/prefix/AbstractPrefixTreeFilter.java
+++ lucene/spatial/src/java/org/apache/lucene/spatial/prefix/AbstractPrefixTreeFilter.java
@@ -18,8 +18,8 @@ package org.apache.lucene.spatial.prefix;
  */
 
 import com.spatial4j.core.shape.Shape;
-import org.apache.lucene.index.AtomicReader;
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReader;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.DocsEnum;
 import org.apache.lucene.index.Terms;
 import org.apache.lucene.index.TermsEnum;
@@ -76,16 +76,16 @@ public abstract class AbstractPrefixTreeFilter extends Filter {
    * traversing a {@link TermsEnum}. */
   public abstract class BaseTermsEnumTraverser {
 
-    protected final AtomicReaderContext context;
+    protected final LeafReaderContext context;
     protected Bits acceptDocs;
     protected final int maxDoc;
 
     protected TermsEnum termsEnum;//remember to check for null in getDocIdSet
     protected DocsEnum docsEnum;
 
-    public BaseTermsEnumTraverser(AtomicReaderContext context, Bits acceptDocs) throws IOException {
+    public BaseTermsEnumTraverser(LeafReaderContext context, Bits acceptDocs) throws IOException {
       this.context = context;
-      AtomicReader reader = context.reader();
+      LeafReader reader = context.reader();
       this.acceptDocs = acceptDocs;
       this.maxDoc = reader.maxDoc();
       Terms terms = reader.terms(fieldName);
diff --git lucene/spatial/src/java/org/apache/lucene/spatial/prefix/AbstractVisitingPrefixTreeFilter.java lucene/spatial/src/java/org/apache/lucene/spatial/prefix/AbstractVisitingPrefixTreeFilter.java
index f7b2fb8..9bc3f97 100644
--- lucene/spatial/src/java/org/apache/lucene/spatial/prefix/AbstractVisitingPrefixTreeFilter.java
+++ lucene/spatial/src/java/org/apache/lucene/spatial/prefix/AbstractVisitingPrefixTreeFilter.java
@@ -18,7 +18,7 @@ package org.apache.lucene.spatial.prefix;
  */
 
 import com.spatial4j.core.shape.Shape;
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.TermsEnum;
 import org.apache.lucene.search.DocIdSet;
 import org.apache.lucene.spatial.prefix.tree.Cell;
@@ -35,7 +35,7 @@ import java.util.Iterator;
  * visitor design patterns for subclasses to guide the traversal and collect
  * matching documents.
  * <p/>
- * Subclasses implement {@link #getDocIdSet(org.apache.lucene.index.AtomicReaderContext,
+ * Subclasses implement {@link #getDocIdSet(org.apache.lucene.index.LeafReaderContext,
  * org.apache.lucene.util.Bits)} by instantiating a custom {@link
  * VisitorTemplate} subclass (i.e. an anonymous inner class) and implement the
  * required methods.
@@ -81,7 +81,7 @@ public abstract class AbstractVisitingPrefixTreeFilter extends AbstractPrefixTre
    * other operations on a {@link SpatialPrefixTree} indexed field. An instance
    * of this class is not designed to be re-used across AtomicReaderContext
    * instances so simply create a new one for each call to, say a {@link
-   * org.apache.lucene.search.Filter#getDocIdSet(org.apache.lucene.index.AtomicReaderContext, org.apache.lucene.util.Bits)}.
+   * org.apache.lucene.search.Filter#getDocIdSet(org.apache.lucene.index.LeafReaderContext, org.apache.lucene.util.Bits)}.
    * The {@link #getDocIdSet()} method here starts the work. It first checks
    * that there are indexed terms; if not it quickly returns null. Then it calls
    * {@link #start()} so a subclass can set up a return value, like an
@@ -132,7 +132,7 @@ public abstract class AbstractVisitingPrefixTreeFilter extends AbstractPrefixTre
 
     private BytesRef thisTerm;//the result of termsEnum.term()
 
-    public VisitorTemplate(AtomicReaderContext context, Bits acceptDocs,
+    public VisitorTemplate(LeafReaderContext context, Bits acceptDocs,
                            boolean hasIndexedLeaves) throws IOException {
       super(context, acceptDocs);
       this.hasIndexedLeaves = hasIndexedLeaves;
diff --git lucene/spatial/src/java/org/apache/lucene/spatial/prefix/ContainsPrefixTreeFilter.java lucene/spatial/src/java/org/apache/lucene/spatial/prefix/ContainsPrefixTreeFilter.java
index fe859bd..2ffa86b 100644
--- lucene/spatial/src/java/org/apache/lucene/spatial/prefix/ContainsPrefixTreeFilter.java
+++ lucene/spatial/src/java/org/apache/lucene/spatial/prefix/ContainsPrefixTreeFilter.java
@@ -20,7 +20,7 @@ package org.apache.lucene.spatial.prefix;
 import com.spatial4j.core.shape.Shape;
 import com.spatial4j.core.shape.SpatialRelation;
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.DocsEnum;
 import org.apache.lucene.search.DocIdSet;
 import org.apache.lucene.search.DocIdSetIterator;
@@ -75,13 +75,13 @@ public class ContainsPrefixTreeFilter extends AbstractPrefixTreeFilter {
   }
 
   @Override
-  public DocIdSet getDocIdSet(AtomicReaderContext context, Bits acceptDocs) throws IOException {
+  public DocIdSet getDocIdSet(LeafReaderContext context, Bits acceptDocs) throws IOException {
     return new ContainsVisitor(context, acceptDocs).visit(grid.getWorldCell(), acceptDocs);
   }
 
   private class ContainsVisitor extends BaseTermsEnumTraverser {
 
-    public ContainsVisitor(AtomicReaderContext context, Bits acceptDocs) throws IOException {
+    public ContainsVisitor(LeafReaderContext context, Bits acceptDocs) throws IOException {
       super(context, acceptDocs);
     }
 
diff --git lucene/spatial/src/java/org/apache/lucene/spatial/prefix/IntersectsPrefixTreeFilter.java lucene/spatial/src/java/org/apache/lucene/spatial/prefix/IntersectsPrefixTreeFilter.java
index 08e0126..aa9bf11 100644
--- lucene/spatial/src/java/org/apache/lucene/spatial/prefix/IntersectsPrefixTreeFilter.java
+++ lucene/spatial/src/java/org/apache/lucene/spatial/prefix/IntersectsPrefixTreeFilter.java
@@ -19,7 +19,7 @@ package org.apache.lucene.spatial.prefix;
 
 import com.spatial4j.core.shape.Shape;
 import com.spatial4j.core.shape.SpatialRelation;
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.search.DocIdSet;
 import org.apache.lucene.spatial.prefix.tree.Cell;
 import org.apache.lucene.spatial.prefix.tree.SpatialPrefixTree;
@@ -51,7 +51,7 @@ public class IntersectsPrefixTreeFilter extends AbstractVisitingPrefixTreeFilter
   }
 
   @Override
-  public DocIdSet getDocIdSet(AtomicReaderContext context, Bits acceptDocs) throws IOException {
+  public DocIdSet getDocIdSet(LeafReaderContext context, Bits acceptDocs) throws IOException {
     /* Possible optimizations (in IN ADDITION TO THOSE LISTED IN VISITORTEMPLATE):
 
     * If docFreq is 1 (or < than some small threshold), then check to see if we've already
diff --git lucene/spatial/src/java/org/apache/lucene/spatial/prefix/WithinPrefixTreeFilter.java lucene/spatial/src/java/org/apache/lucene/spatial/prefix/WithinPrefixTreeFilter.java
index d00a050..758d1bb 100644
--- lucene/spatial/src/java/org/apache/lucene/spatial/prefix/WithinPrefixTreeFilter.java
+++ lucene/spatial/src/java/org/apache/lucene/spatial/prefix/WithinPrefixTreeFilter.java
@@ -24,7 +24,7 @@ import com.spatial4j.core.shape.Point;
 import com.spatial4j.core.shape.Rectangle;
 import com.spatial4j.core.shape.Shape;
 import com.spatial4j.core.shape.SpatialRelation;
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.search.DocIdSet;
 import org.apache.lucene.spatial.prefix.tree.Cell;
 import org.apache.lucene.spatial.prefix.tree.CellIterator;
@@ -119,7 +119,7 @@ public class WithinPrefixTreeFilter extends AbstractVisitingPrefixTreeFilter {
 
 
   @Override
-  public DocIdSet getDocIdSet(AtomicReaderContext context, Bits acceptDocs) throws IOException {
+  public DocIdSet getDocIdSet(LeafReaderContext context, Bits acceptDocs) throws IOException {
     return new VisitorTemplate(context, acceptDocs, true) {
       private FixedBitSet inside;
       private FixedBitSet outside;
diff --git lucene/spatial/src/java/org/apache/lucene/spatial/serialized/SerializedDVStrategy.java lucene/spatial/src/java/org/apache/lucene/spatial/serialized/SerializedDVStrategy.java
index 8c46dca..4a398d8 100644
--- lucene/spatial/src/java/org/apache/lucene/spatial/serialized/SerializedDVStrategy.java
+++ lucene/spatial/src/java/org/apache/lucene/spatial/serialized/SerializedDVStrategy.java
@@ -24,7 +24,7 @@ import com.spatial4j.core.shape.Shape;
 
 import org.apache.lucene.document.BinaryDocValuesField;
 import org.apache.lucene.document.Field;
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.BinaryDocValues;
 import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
@@ -143,7 +143,7 @@ public class SerializedDVStrategy extends SpatialStrategy {
     }
 
     @Override
-    public DocIdSet getDocIdSet(final AtomicReaderContext context, final Bits acceptDocs) throws IOException {
+    public DocIdSet getDocIdSet(final LeafReaderContext context, final Bits acceptDocs) throws IOException {
       return new DocIdSet() {
         @Override
         public DocIdSetIterator iterator() throws IOException {
@@ -213,7 +213,7 @@ public class SerializedDVStrategy extends SpatialStrategy {
     }
 
     @Override
-    public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
+    public FunctionValues getValues(Map context, LeafReaderContext readerContext) throws IOException {
       final BinaryDocValues docValues = readerContext.reader().getBinaryDocValues(fieldName);
 
       return new FunctionValues() {
diff --git lucene/spatial/src/java/org/apache/lucene/spatial/util/CachingDoubleValueSource.java lucene/spatial/src/java/org/apache/lucene/spatial/util/CachingDoubleValueSource.java
index dd2b411..9a91782 100644
--- lucene/spatial/src/java/org/apache/lucene/spatial/util/CachingDoubleValueSource.java
+++ lucene/spatial/src/java/org/apache/lucene/spatial/util/CachingDoubleValueSource.java
@@ -17,7 +17,7 @@
 
 package org.apache.lucene.spatial.util;
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 
@@ -47,7 +47,7 @@ public class CachingDoubleValueSource extends ValueSource {
   }
 
   @Override
-  public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
+  public FunctionValues getValues(Map context, LeafReaderContext readerContext) throws IOException {
     final int base = readerContext.docBase;
     final FunctionValues vals = source.getValues(context,readerContext);
     return new FunctionValues() {
diff --git lucene/spatial/src/java/org/apache/lucene/spatial/util/DistanceToShapeValueSource.java lucene/spatial/src/java/org/apache/lucene/spatial/util/DistanceToShapeValueSource.java
index b6ac073..9216a91 100644
--- lucene/spatial/src/java/org/apache/lucene/spatial/util/DistanceToShapeValueSource.java
+++ lucene/spatial/src/java/org/apache/lucene/spatial/util/DistanceToShapeValueSource.java
@@ -21,7 +21,7 @@ import com.spatial4j.core.context.SpatialContext;
 import com.spatial4j.core.distance.DistanceCalculator;
 import com.spatial4j.core.shape.Point;
 import com.spatial4j.core.shape.Shape;
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.queries.function.docvalues.DoubleDocValues;
@@ -68,7 +68,7 @@ public class DistanceToShapeValueSource extends ValueSource {
   }
 
   @Override
-  public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
+  public FunctionValues getValues(Map context, LeafReaderContext readerContext) throws IOException {
     final FunctionValues shapeValues = shapeValueSource.getValues(context, readerContext);
 
     return new DoubleDocValues(this) {
diff --git lucene/spatial/src/java/org/apache/lucene/spatial/util/ShapeAreaValueSource.java lucene/spatial/src/java/org/apache/lucene/spatial/util/ShapeAreaValueSource.java
index 5d61733..c5eb590 100644
--- lucene/spatial/src/java/org/apache/lucene/spatial/util/ShapeAreaValueSource.java
+++ lucene/spatial/src/java/org/apache/lucene/spatial/util/ShapeAreaValueSource.java
@@ -19,7 +19,7 @@ package org.apache.lucene.spatial.util;
 
 import com.spatial4j.core.context.SpatialContext;
 import com.spatial4j.core.shape.Shape;
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.queries.function.docvalues.DoubleDocValues;
@@ -59,7 +59,7 @@ public class ShapeAreaValueSource extends ValueSource {
   }
 
   @Override
-  public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
+  public FunctionValues getValues(Map context, LeafReaderContext readerContext) throws IOException {
     final FunctionValues shapeValues = shapeValueSource.getValues(context, readerContext);
 
     return new DoubleDocValues(this) {
diff --git lucene/spatial/src/java/org/apache/lucene/spatial/util/ShapeFieldCacheDistanceValueSource.java lucene/spatial/src/java/org/apache/lucene/spatial/util/ShapeFieldCacheDistanceValueSource.java
index b99e9de..198b062 100644
--- lucene/spatial/src/java/org/apache/lucene/spatial/util/ShapeFieldCacheDistanceValueSource.java
+++ lucene/spatial/src/java/org/apache/lucene/spatial/util/ShapeFieldCacheDistanceValueSource.java
@@ -20,7 +20,7 @@ package org.apache.lucene.spatial.util;
 import com.spatial4j.core.context.SpatialContext;
 import com.spatial4j.core.distance.DistanceCalculator;
 import com.spatial4j.core.shape.Point;
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 
@@ -57,7 +57,7 @@ public class ShapeFieldCacheDistanceValueSource extends ValueSource {
   }
 
   @Override
-  public FunctionValues getValues(Map context, final AtomicReaderContext readerContext) throws IOException {
+  public FunctionValues getValues(Map context, final LeafReaderContext readerContext) throws IOException {
     return new FunctionValues() {
       private final ShapeFieldCache<Point> cache =
           provider.getCache(readerContext.reader());
diff --git lucene/spatial/src/java/org/apache/lucene/spatial/util/ShapeFieldCacheProvider.java lucene/spatial/src/java/org/apache/lucene/spatial/util/ShapeFieldCacheProvider.java
index 9458ff1..bf5b726 100644
--- lucene/spatial/src/java/org/apache/lucene/spatial/util/ShapeFieldCacheProvider.java
+++ lucene/spatial/src/java/org/apache/lucene/spatial/util/ShapeFieldCacheProvider.java
@@ -27,7 +27,7 @@ import java.util.WeakHashMap;
 import java.util.logging.Logger;
 
 /**
- * Provides access to a {@link ShapeFieldCache} for a given {@link AtomicReader}.
+ * Provides access to a {@link ShapeFieldCache} for a given {@link org.apache.lucene.index.LeafReader}.
  *
  * If a Cache does not exist for the Reader, then it is built by iterating over
  * the all terms for a given field, reconstructing the Shape from them, and adding
@@ -51,7 +51,7 @@ public abstract class ShapeFieldCacheProvider<T extends Shape> {
 
   protected abstract T readShape( BytesRef term );
 
-  public synchronized ShapeFieldCache<T> getCache(AtomicReader reader) throws IOException {
+  public synchronized ShapeFieldCache<T> getCache(LeafReader reader) throws IOException {
     ShapeFieldCache<T> idx = sidx.get(reader);
     if (idx != null) {
       return idx;
diff --git lucene/spatial/src/java/org/apache/lucene/spatial/util/ShapePredicateValueSource.java lucene/spatial/src/java/org/apache/lucene/spatial/util/ShapePredicateValueSource.java
index e5c7265..a799bbb 100644
--- lucene/spatial/src/java/org/apache/lucene/spatial/util/ShapePredicateValueSource.java
+++ lucene/spatial/src/java/org/apache/lucene/spatial/util/ShapePredicateValueSource.java
@@ -18,7 +18,7 @@ package org.apache.lucene.spatial.util;
  */
 
 import com.spatial4j.core.shape.Shape;
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.queries.function.docvalues.BoolDocValues;
@@ -64,7 +64,7 @@ public class ShapePredicateValueSource extends ValueSource {
   }
 
   @Override
-  public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
+  public FunctionValues getValues(Map context, LeafReaderContext readerContext) throws IOException {
     final FunctionValues shapeValues = shapeValuesource.getValues(context, readerContext);
 
     return new BoolDocValues(this) {
diff --git lucene/spatial/src/java/org/apache/lucene/spatial/util/ValueSourceFilter.java lucene/spatial/src/java/org/apache/lucene/spatial/util/ValueSourceFilter.java
index ee1639f..cff54c7 100644
--- lucene/spatial/src/java/org/apache/lucene/spatial/util/ValueSourceFilter.java
+++ lucene/spatial/src/java/org/apache/lucene/spatial/util/ValueSourceFilter.java
@@ -17,7 +17,7 @@
 
 package org.apache.lucene.spatial.util;
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.search.DocIdSet;
@@ -52,7 +52,7 @@ public class ValueSourceFilter extends Filter {
   }
 
   @Override
-  public DocIdSet getDocIdSet(AtomicReaderContext context, Bits acceptDocs) throws IOException {
+  public DocIdSet getDocIdSet(LeafReaderContext context, Bits acceptDocs) throws IOException {
     final FunctionValues values = source.getValues( null, context );
     return new FilteredDocIdSet(startingFilter.getDocIdSet(context, acceptDocs)) {
       @Override
diff --git lucene/spatial/src/java/org/apache/lucene/spatial/vector/DistanceValueSource.java lucene/spatial/src/java/org/apache/lucene/spatial/vector/DistanceValueSource.java
index 30bde03..85afeae 100644
--- lucene/spatial/src/java/org/apache/lucene/spatial/vector/DistanceValueSource.java
+++ lucene/spatial/src/java/org/apache/lucene/spatial/vector/DistanceValueSource.java
@@ -19,8 +19,8 @@ package org.apache.lucene.spatial.vector;
 
 import com.spatial4j.core.distance.DistanceCalculator;
 import com.spatial4j.core.shape.Point;
-import org.apache.lucene.index.AtomicReader;
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReader;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.DocValues;
 import org.apache.lucene.index.NumericDocValues;
 import org.apache.lucene.queries.function.FunctionValues;
@@ -63,8 +63,8 @@ public class DistanceValueSource extends ValueSource {
    * Returns the FunctionValues used by the function query.
    */
   @Override
-  public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
-    AtomicReader reader = readerContext.reader();
+  public FunctionValues getValues(Map context, LeafReaderContext readerContext) throws IOException {
+    LeafReader reader = readerContext.reader();
 
     final NumericDocValues ptX = DocValues.getNumeric(reader, strategy.getFieldNameX());
     final NumericDocValues ptY = DocValues.getNumeric(reader, strategy.getFieldNameY());
diff --git lucene/suggest/src/java/org/apache/lucene/search/spell/SpellChecker.java lucene/suggest/src/java/org/apache/lucene/search/spell/SpellChecker.java
index bb6eaa3..c2fa23f 100644
--- lucene/suggest/src/java/org/apache/lucene/search/spell/SpellChecker.java
+++ lucene/suggest/src/java/org/apache/lucene/search/spell/SpellChecker.java
@@ -27,7 +27,7 @@ import org.apache.lucene.document.Field;
 import org.apache.lucene.document.FieldType;
 import org.apache.lucene.index.FieldInfo.IndexOptions;
 import org.apache.lucene.document.StringField;
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexWriter;
@@ -496,7 +496,7 @@ public class SpellChecker implements java.io.Closeable {
 
       final IndexReader reader = searcher.getIndexReader();
       if (reader.maxDoc() > 0) {
-        for (final AtomicReaderContext ctx : reader.leaves()) {
+        for (final LeafReaderContext ctx : reader.leaves()) {
           Terms terms = ctx.reader().terms(F_WORD);
           if (terms != null)
             termsEnums.add(terms.iterator(null));
diff --git lucene/suggest/src/java/org/apache/lucene/search/suggest/DocumentValueSourceDictionary.java lucene/suggest/src/java/org/apache/lucene/search/suggest/DocumentValueSourceDictionary.java
index b5b1a56..9eedbc6 100644
--- lucene/suggest/src/java/org/apache/lucene/search/suggest/DocumentValueSourceDictionary.java
+++ lucene/suggest/src/java/org/apache/lucene/search/suggest/DocumentValueSourceDictionary.java
@@ -21,7 +21,7 @@ import java.io.IOException;
 import java.util.HashMap;
 import java.util.List;
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.ReaderUtil;
 import org.apache.lucene.index.StoredDocument;
@@ -110,7 +110,7 @@ public class DocumentValueSourceDictionary extends DocumentDictionary {
     
     private FunctionValues currentWeightValues;
     /** leaves of the reader */
-    private final List<AtomicReaderContext> leaves;
+    private final List<LeafReaderContext> leaves;
     /** starting docIds of all the leaves */
     private final int[] starts;
     /** current leave index */
diff --git lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggester.java lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggester.java
index 9de7c2b..365a9ad 100644
--- lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggester.java
+++ lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggester.java
@@ -42,12 +42,13 @@ import org.apache.lucene.document.NumericDocValuesField;
 import org.apache.lucene.document.SortedSetDocValuesField;
 import org.apache.lucene.document.StringField;
 import org.apache.lucene.document.TextField;
-import org.apache.lucene.index.AtomicReader;
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.FilterLeafReader;
+import org.apache.lucene.index.LeafReader;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.BinaryDocValues;
 import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.FieldInfo.IndexOptions;
-import org.apache.lucene.index.FilterAtomicReader;
+import org.apache.lucene.index.FilterLeafReader;
 import org.apache.lucene.index.IndexWriter;
 import org.apache.lucene.index.IndexWriterConfig;
 import org.apache.lucene.index.MultiDocValues;
@@ -71,7 +72,6 @@ import org.apache.lucene.search.TermQuery;
 import org.apache.lucene.search.TopFieldCollector;
 import org.apache.lucene.search.TopFieldDocs;
 import org.apache.lucene.search.suggest.InputIterator;
-import org.apache.lucene.search.suggest.Lookup.LookupResult; // javadocs
 import org.apache.lucene.search.suggest.Lookup;
 import org.apache.lucene.store.DataInput;
 import org.apache.lucene.store.DataOutput;
@@ -544,7 +544,7 @@ public class AnalyzingInfixSuggester extends Lookup implements Closeable {
     // This will just be null if app didn't pass payloads to build():
     // TODO: maybe just stored fields?  they compress...
     BinaryDocValues payloadsDV = MultiDocValues.getBinaryValues(searcher.getIndexReader(), "payloads");
-    List<AtomicReaderContext> leaves = searcher.getIndexReader().leaves();
+    List<LeafReaderContext> leaves = searcher.getIndexReader().leaves();
     List<LookupResult> results = new ArrayList<>();
     for (int i=0;i<hits.scoreDocs.length;i++) {
       FieldDoc fd = (FieldDoc) hits.scoreDocs[i];
@@ -708,8 +708,8 @@ public class AnalyzingInfixSuggester extends Lookup implements Closeable {
       if (searcherMgr != null) {
         IndexSearcher searcher = searcherMgr.acquire();
         try {
-          for (AtomicReaderContext context : searcher.getIndexReader().leaves()) {
-            AtomicReader reader = FilterAtomicReader.unwrap(context.reader());
+          for (LeafReaderContext context : searcher.getIndexReader().leaves()) {
+            LeafReader reader = FilterLeafReader.unwrap(context.reader());
             if (reader instanceof SegmentReader) {
               mem += ((SegmentReader) context.reader()).ramBytesUsed();
             }
@@ -731,8 +731,8 @@ public class AnalyzingInfixSuggester extends Lookup implements Closeable {
       if (searcherMgr != null) {
         IndexSearcher searcher = searcherMgr.acquire();
         try {
-          for (AtomicReaderContext context : searcher.getIndexReader().leaves()) {
-            AtomicReader reader = FilterAtomicReader.unwrap(context.reader());
+          for (LeafReaderContext context : searcher.getIndexReader().leaves()) {
+            LeafReader reader = FilterLeafReader.unwrap(context.reader());
             if (reader instanceof SegmentReader) {
               resources.add(Accountables.namedAccountable("segment", (SegmentReader)reader));
             }
diff --git lucene/test-framework/src/java/org/apache/lucene/codecs/asserting/AssertingDocValuesFormat.java lucene/test-framework/src/java/org/apache/lucene/codecs/asserting/AssertingDocValuesFormat.java
index ffa56c0..8c0a96f 100644
--- lucene/test-framework/src/java/org/apache/lucene/codecs/asserting/AssertingDocValuesFormat.java
+++ lucene/test-framework/src/java/org/apache/lucene/codecs/asserting/AssertingDocValuesFormat.java
@@ -24,7 +24,7 @@ import org.apache.lucene.codecs.DocValuesConsumer;
 import org.apache.lucene.codecs.DocValuesFormat;
 import org.apache.lucene.codecs.DocValuesProducer;
 import org.apache.lucene.codecs.lucene410.Lucene410DocValuesFormat;
-import org.apache.lucene.index.AssertingAtomicReader;
+import org.apache.lucene.index.AssertingLeafReader;
 import org.apache.lucene.index.BinaryDocValues;
 import org.apache.lucene.index.FieldInfo;
 import org.apache.lucene.index.NumericDocValues;
@@ -226,7 +226,7 @@ public class AssertingDocValuesFormat extends DocValuesFormat {
       assert field.getDocValuesType() == FieldInfo.DocValuesType.NUMERIC;
       NumericDocValues values = in.getNumeric(field);
       assert values != null;
-      return new AssertingAtomicReader.AssertingNumericDocValues(values, maxDoc);
+      return new AssertingLeafReader.AssertingNumericDocValues(values, maxDoc);
     }
 
     @Override
@@ -234,7 +234,7 @@ public class AssertingDocValuesFormat extends DocValuesFormat {
       assert field.getDocValuesType() == FieldInfo.DocValuesType.BINARY;
       BinaryDocValues values = in.getBinary(field);
       assert values != null;
-      return new AssertingAtomicReader.AssertingBinaryDocValues(values, maxDoc);
+      return new AssertingLeafReader.AssertingBinaryDocValues(values, maxDoc);
     }
 
     @Override
@@ -242,7 +242,7 @@ public class AssertingDocValuesFormat extends DocValuesFormat {
       assert field.getDocValuesType() == FieldInfo.DocValuesType.SORTED;
       SortedDocValues values = in.getSorted(field);
       assert values != null;
-      return new AssertingAtomicReader.AssertingSortedDocValues(values, maxDoc);
+      return new AssertingLeafReader.AssertingSortedDocValues(values, maxDoc);
     }
     
     @Override
@@ -250,7 +250,7 @@ public class AssertingDocValuesFormat extends DocValuesFormat {
       assert field.getDocValuesType() == FieldInfo.DocValuesType.SORTED_NUMERIC;
       SortedNumericDocValues values = in.getSortedNumeric(field);
       assert values != null;
-      return new AssertingAtomicReader.AssertingSortedNumericDocValues(values, maxDoc);
+      return new AssertingLeafReader.AssertingSortedNumericDocValues(values, maxDoc);
     }
     
     @Override
@@ -258,7 +258,7 @@ public class AssertingDocValuesFormat extends DocValuesFormat {
       assert field.getDocValuesType() == FieldInfo.DocValuesType.SORTED_SET;
       SortedSetDocValues values = in.getSortedSet(field);
       assert values != null;
-      return new AssertingAtomicReader.AssertingSortedSetDocValues(values, maxDoc);
+      return new AssertingLeafReader.AssertingSortedSetDocValues(values, maxDoc);
     }
     
     @Override
@@ -267,7 +267,7 @@ public class AssertingDocValuesFormat extends DocValuesFormat {
       Bits bits = in.getDocsWithField(field);
       assert bits != null;
       assert bits.length() == maxDoc;
-      return new AssertingAtomicReader.AssertingBits(bits);
+      return new AssertingLeafReader.AssertingBits(bits);
     }
 
     @Override
diff --git lucene/test-framework/src/java/org/apache/lucene/codecs/asserting/AssertingNormsFormat.java lucene/test-framework/src/java/org/apache/lucene/codecs/asserting/AssertingNormsFormat.java
index 4b2f0fc..24db732 100644
--- lucene/test-framework/src/java/org/apache/lucene/codecs/asserting/AssertingNormsFormat.java
+++ lucene/test-framework/src/java/org/apache/lucene/codecs/asserting/AssertingNormsFormat.java
@@ -23,7 +23,7 @@ import org.apache.lucene.codecs.NormsConsumer;
 import org.apache.lucene.codecs.NormsFormat;
 import org.apache.lucene.codecs.NormsProducer;
 import org.apache.lucene.codecs.lucene49.Lucene49NormsFormat;
-import org.apache.lucene.index.AssertingAtomicReader;
+import org.apache.lucene.index.AssertingLeafReader;
 import org.apache.lucene.index.FieldInfo;
 import org.apache.lucene.index.NumericDocValues;
 import org.apache.lucene.index.SegmentReadState;
@@ -97,7 +97,7 @@ public class AssertingNormsFormat extends NormsFormat {
       assert field.getNormType() == FieldInfo.DocValuesType.NUMERIC;
       NumericDocValues values = in.getNorms(field);
       assert values != null;
-      return new AssertingAtomicReader.AssertingNumericDocValues(values, maxDoc);
+      return new AssertingLeafReader.AssertingNumericDocValues(values, maxDoc);
     }
 
     @Override
diff --git lucene/test-framework/src/java/org/apache/lucene/codecs/asserting/AssertingPostingsFormat.java lucene/test-framework/src/java/org/apache/lucene/codecs/asserting/AssertingPostingsFormat.java
index de873c0..8128589 100644
--- lucene/test-framework/src/java/org/apache/lucene/codecs/asserting/AssertingPostingsFormat.java
+++ lucene/test-framework/src/java/org/apache/lucene/codecs/asserting/AssertingPostingsFormat.java
@@ -24,7 +24,7 @@ import org.apache.lucene.codecs.FieldsConsumer;
 import org.apache.lucene.codecs.FieldsProducer;
 import org.apache.lucene.codecs.PostingsFormat;
 import org.apache.lucene.codecs.lucene41.Lucene41PostingsFormat;
-import org.apache.lucene.index.AssertingAtomicReader;
+import org.apache.lucene.index.AssertingLeafReader;
 import org.apache.lucene.index.DocsAndPositionsEnum;
 import org.apache.lucene.index.DocsEnum;
 import org.apache.lucene.index.FieldInfo;
@@ -84,7 +84,7 @@ public final class AssertingPostingsFormat extends PostingsFormat {
     @Override
     public Terms terms(String field) throws IOException {
       Terms terms = in.terms(field);
-      return terms == null ? null : new AssertingAtomicReader.AssertingTerms(terms);
+      return terms == null ? null : new AssertingLeafReader.AssertingTerms(terms);
     }
 
     @Override
diff --git lucene/test-framework/src/java/org/apache/lucene/codecs/asserting/AssertingTermVectorsFormat.java lucene/test-framework/src/java/org/apache/lucene/codecs/asserting/AssertingTermVectorsFormat.java
index e9659c0..e7fb072 100644
--- lucene/test-framework/src/java/org/apache/lucene/codecs/asserting/AssertingTermVectorsFormat.java
+++ lucene/test-framework/src/java/org/apache/lucene/codecs/asserting/AssertingTermVectorsFormat.java
@@ -23,7 +23,7 @@ import org.apache.lucene.codecs.TermVectorsFormat;
 import org.apache.lucene.codecs.TermVectorsReader;
 import org.apache.lucene.codecs.TermVectorsWriter;
 import org.apache.lucene.codecs.lucene42.Lucene42TermVectorsFormat;
-import org.apache.lucene.index.AssertingAtomicReader;
+import org.apache.lucene.index.AssertingLeafReader;
 import org.apache.lucene.index.FieldInfo;
 import org.apache.lucene.index.FieldInfos;
 import org.apache.lucene.index.Fields;
@@ -69,7 +69,7 @@ public class AssertingTermVectorsFormat extends TermVectorsFormat {
     @Override
     public Fields get(int doc) throws IOException {
       Fields fields = in.get(doc);
-      return fields == null ? null : new AssertingAtomicReader.AssertingFields(fields);
+      return fields == null ? null : new AssertingLeafReader.AssertingFields(fields);
     }
 
     @Override
diff --git lucene/test-framework/src/java/org/apache/lucene/index/AllDeletedFilterReader.java lucene/test-framework/src/java/org/apache/lucene/index/AllDeletedFilterReader.java
index d20583f..9404433 100644
--- lucene/test-framework/src/java/org/apache/lucene/index/AllDeletedFilterReader.java
+++ lucene/test-framework/src/java/org/apache/lucene/index/AllDeletedFilterReader.java
@@ -22,10 +22,10 @@ import org.apache.lucene.util.Bits;
 /**
  * Filters the incoming reader and makes all documents appear deleted.
  */
-public class AllDeletedFilterReader extends FilterAtomicReader {
+public class AllDeletedFilterReader extends FilterLeafReader {
   final Bits liveDocs;
   
-  public AllDeletedFilterReader(AtomicReader in) {
+  public AllDeletedFilterReader(LeafReader in) {
     super(in);
     liveDocs = new Bits.MatchNoBits(in.maxDoc());
     assert maxDoc() == 0 || hasDeletions();
diff --git lucene/test-framework/src/java/org/apache/lucene/index/AssertingAtomicReader.java lucene/test-framework/src/java/org/apache/lucene/index/AssertingAtomicReader.java
deleted file mode 100644
index cd9b163..0000000
--- lucene/test-framework/src/java/org/apache/lucene/index/AssertingAtomicReader.java
+++ /dev/null
@@ -1,749 +0,0 @@
-package org.apache.lucene.index;
-
-import java.io.IOException;
-import java.util.Iterator;
-
-import org.apache.lucene.search.DocIdSetIterator;
-import org.apache.lucene.util.Bits;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.VirtualMethod;
-import org.apache.lucene.util.automaton.CompiledAutomaton;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * A {@link FilterAtomicReader} that can be used to apply
- * additional checks for tests.
- */
-public class AssertingAtomicReader extends FilterAtomicReader {
-
-  public AssertingAtomicReader(AtomicReader in) {
-    super(in);
-    // check some basic reader sanity
-    assert in.maxDoc() >= 0;
-    assert in.numDocs() <= in.maxDoc();
-    assert in.numDeletedDocs() + in.numDocs() == in.maxDoc();
-    assert !in.hasDeletions() || in.numDeletedDocs() > 0 && in.numDocs() < in.maxDoc();
-  }
-
-  @Override
-  public Fields fields() throws IOException {
-    Fields fields = super.fields();
-    return fields == null ? null : new AssertingFields(fields);
-  }
-  
-  @Override
-  public Fields getTermVectors(int docID) throws IOException {
-    Fields fields = super.getTermVectors(docID);
-    return fields == null ? null : new AssertingFields(fields);
-  }
-
-  /**
-   * Wraps a Fields but with additional asserts
-   */
-  public static class AssertingFields extends FilterFields {
-    public AssertingFields(Fields in) {
-      super(in);
-    }
-
-    @Override
-    public Iterator<String> iterator() {
-      Iterator<String> iterator = super.iterator();
-      assert iterator != null;
-      return iterator;
-    }
-
-    @Override
-    public Terms terms(String field) throws IOException {
-      Terms terms = super.terms(field);
-      return terms == null ? null : new AssertingTerms(terms);
-    }
-  }
-  
-  /**
-   * Wraps a Terms but with additional asserts
-   */
-  public static class AssertingTerms extends FilterTerms {
-    public AssertingTerms(Terms in) {
-      super(in);
-    }
-
-    @Override
-    public TermsEnum intersect(CompiledAutomaton automaton, BytesRef bytes) throws IOException {
-      TermsEnum termsEnum = in.intersect(automaton, bytes);
-      assert termsEnum != null;
-      assert bytes == null || bytes.isValid();
-      return new AssertingTermsEnum(termsEnum);
-    }
-
-    @Override
-    public BytesRef getMin() throws IOException {
-      BytesRef v = in.getMin();
-      assert v == null || v.isValid();
-      return v;
-    }
-
-    @Override
-    public BytesRef getMax() throws IOException {
-      BytesRef v = in.getMax();
-      assert v == null || v.isValid();
-      return v;
-    }
-
-    @Override
-    public TermsEnum iterator(TermsEnum reuse) throws IOException {
-      // TODO: should we give this thing a random to be super-evil,
-      // and randomly *not* unwrap?
-      if (reuse instanceof AssertingTermsEnum) {
-        reuse = ((AssertingTermsEnum) reuse).in;
-      }
-      TermsEnum termsEnum = super.iterator(reuse);
-      assert termsEnum != null;
-      return new AssertingTermsEnum(termsEnum);
-    }
-  }
-  
-  static final VirtualMethod<TermsEnum> SEEK_EXACT = new VirtualMethod<>(TermsEnum.class, "seekExact", BytesRef.class);
-
-  static class AssertingTermsEnum extends FilterTermsEnum {
-    private enum State {INITIAL, POSITIONED, UNPOSITIONED};
-    private State state = State.INITIAL;
-    private final boolean delegateOverridesSeekExact;
-
-    public AssertingTermsEnum(TermsEnum in) {
-      super(in);
-      delegateOverridesSeekExact = SEEK_EXACT.isOverriddenAsOf(in.getClass());
-    }
-
-    @Override
-    public DocsEnum docs(Bits liveDocs, DocsEnum reuse, int flags) throws IOException {
-      assert state == State.POSITIONED: "docs(...) called on unpositioned TermsEnum";
-
-      // TODO: should we give this thing a random to be super-evil,
-      // and randomly *not* unwrap?
-      if (reuse instanceof AssertingDocsEnum) {
-        reuse = ((AssertingDocsEnum) reuse).in;
-      }
-      DocsEnum docs = super.docs(liveDocs, reuse, flags);
-      return docs == null ? null : new AssertingDocsEnum(docs);
-    }
-
-    @Override
-    public DocsAndPositionsEnum docsAndPositions(Bits liveDocs, DocsAndPositionsEnum reuse, int flags) throws IOException {
-      assert state == State.POSITIONED: "docsAndPositions(...) called on unpositioned TermsEnum";
-
-      // TODO: should we give this thing a random to be super-evil,
-      // and randomly *not* unwrap?
-      if (reuse instanceof AssertingDocsAndPositionsEnum) {
-        reuse = ((AssertingDocsAndPositionsEnum) reuse).in;
-      }
-      DocsAndPositionsEnum docs = super.docsAndPositions(liveDocs, reuse, flags);
-      return docs == null ? null : new AssertingDocsAndPositionsEnum(docs);
-    }
-
-    // TODO: we should separately track if we are 'at the end' ?
-    // someone should not call next() after it returns null!!!!
-    @Override
-    public BytesRef next() throws IOException {
-      assert state == State.INITIAL || state == State.POSITIONED: "next() called on unpositioned TermsEnum";
-      BytesRef result = super.next();
-      if (result == null) {
-        state = State.UNPOSITIONED;
-      } else {
-        assert result.isValid();
-        state = State.POSITIONED;
-      }
-      return result;
-    }
-
-    @Override
-    public long ord() throws IOException {
-      assert state == State.POSITIONED : "ord() called on unpositioned TermsEnum";
-      return super.ord();
-    }
-
-    @Override
-    public int docFreq() throws IOException {
-      assert state == State.POSITIONED : "docFreq() called on unpositioned TermsEnum";
-      return super.docFreq();
-    }
-
-    @Override
-    public long totalTermFreq() throws IOException {
-      assert state == State.POSITIONED : "totalTermFreq() called on unpositioned TermsEnum";
-      return super.totalTermFreq();
-    }
-
-    @Override
-    public BytesRef term() throws IOException {
-      assert state == State.POSITIONED : "term() called on unpositioned TermsEnum";
-      BytesRef ret = super.term();
-      assert ret == null || ret.isValid();
-      return ret;
-    }
-
-    @Override
-    public void seekExact(long ord) throws IOException {
-      super.seekExact(ord);
-      state = State.POSITIONED;
-    }
-
-    @Override
-    public SeekStatus seekCeil(BytesRef term) throws IOException {
-      assert term.isValid();
-      SeekStatus result = super.seekCeil(term);
-      if (result == SeekStatus.END) {
-        state = State.UNPOSITIONED;
-      } else {
-        state = State.POSITIONED;
-      }
-      return result;
-    }
-
-    @Override
-    public boolean seekExact(BytesRef text) throws IOException {
-      assert text.isValid();
-      boolean result;
-      if (delegateOverridesSeekExact) {
-        result = in.seekExact(text);
-      } else {
-        result = super.seekExact(text);
-      }
-      if (result) {
-        state = State.POSITIONED;
-      } else {
-        state = State.UNPOSITIONED;
-      }
-      return result;
-    }
-
-    @Override
-    public TermState termState() throws IOException {
-      assert state == State.POSITIONED : "termState() called on unpositioned TermsEnum";
-      return super.termState();
-    }
-
-    @Override
-    public void seekExact(BytesRef term, TermState state) throws IOException {
-      assert term.isValid();
-      super.seekExact(term, state);
-      this.state = State.POSITIONED;
-    }
-
-    @Override
-    public String toString() {
-      return "AssertingTermsEnum(" + in + ")";
-    }
-  }
-  
-  static enum DocsEnumState { START, ITERATING, FINISHED };
-
-  /** Wraps a docsenum with additional checks */
-  public static class AssertingDocsEnum extends FilterDocsEnum {
-    private DocsEnumState state = DocsEnumState.START;
-    private int doc;
-    
-    public AssertingDocsEnum(DocsEnum in) {
-      this(in, true);
-    }
-
-    public AssertingDocsEnum(DocsEnum in, boolean failOnUnsupportedDocID) {
-      super(in);
-      try {
-        int docid = in.docID();
-        assert docid == -1 : in.getClass() + ": invalid initial doc id: " + docid;
-      } catch (UnsupportedOperationException e) {
-        if (failOnUnsupportedDocID) {
-          throw e;
-        }
-      }
-      doc = -1;
-    }
-
-    @Override
-    public int nextDoc() throws IOException {
-      assert state != DocsEnumState.FINISHED : "nextDoc() called after NO_MORE_DOCS";
-      int nextDoc = super.nextDoc();
-      assert nextDoc > doc : "backwards nextDoc from " + doc + " to " + nextDoc + " " + in;
-      if (nextDoc == DocIdSetIterator.NO_MORE_DOCS) {
-        state = DocsEnumState.FINISHED;
-      } else {
-        state = DocsEnumState.ITERATING;
-      }
-      assert super.docID() == nextDoc;
-      return doc = nextDoc;
-    }
-
-    @Override
-    public int advance(int target) throws IOException {
-      assert state != DocsEnumState.FINISHED : "advance() called after NO_MORE_DOCS";
-      assert target > doc : "target must be > docID(), got " + target + " <= " + doc;
-      int advanced = super.advance(target);
-      assert advanced >= target : "backwards advance from: " + target + " to: " + advanced;
-      if (advanced == DocIdSetIterator.NO_MORE_DOCS) {
-        state = DocsEnumState.FINISHED;
-      } else {
-        state = DocsEnumState.ITERATING;
-      }
-      assert super.docID() == advanced;
-      return doc = advanced;
-    }
-
-    @Override
-    public int docID() {
-      assert doc == super.docID() : " invalid docID() in " + in.getClass() + " " + super.docID() + " instead of " + doc;
-      return doc;
-    }
-
-    @Override
-    public int freq() throws IOException {
-      assert state != DocsEnumState.START : "freq() called before nextDoc()/advance()";
-      assert state != DocsEnumState.FINISHED : "freq() called after NO_MORE_DOCS";
-      int freq = super.freq();
-      assert freq > 0;
-      return freq;
-    }
-  }
-  
-  static class AssertingDocsAndPositionsEnum extends FilterDocsAndPositionsEnum {
-    private DocsEnumState state = DocsEnumState.START;
-    private int positionMax = 0;
-    private int positionCount = 0;
-    private int doc;
-
-    public AssertingDocsAndPositionsEnum(DocsAndPositionsEnum in) {
-      super(in);
-      int docid = in.docID();
-      assert docid == -1 : "invalid initial doc id: " + docid;
-      doc = -1;
-    }
-
-    @Override
-    public int nextDoc() throws IOException {
-      assert state != DocsEnumState.FINISHED : "nextDoc() called after NO_MORE_DOCS";
-      int nextDoc = super.nextDoc();
-      assert nextDoc > doc : "backwards nextDoc from " + doc + " to " + nextDoc;
-      positionCount = 0;
-      if (nextDoc == DocIdSetIterator.NO_MORE_DOCS) {
-        state = DocsEnumState.FINISHED;
-        positionMax = 0;
-      } else {
-        state = DocsEnumState.ITERATING;
-        positionMax = super.freq();
-      }
-      assert super.docID() == nextDoc;
-      return doc = nextDoc;
-    }
-
-    @Override
-    public int advance(int target) throws IOException {
-      assert state != DocsEnumState.FINISHED : "advance() called after NO_MORE_DOCS";
-      assert target > doc : "target must be > docID(), got " + target + " <= " + doc;
-      int advanced = super.advance(target);
-      assert advanced >= target : "backwards advance from: " + target + " to: " + advanced;
-      positionCount = 0;
-      if (advanced == DocIdSetIterator.NO_MORE_DOCS) {
-        state = DocsEnumState.FINISHED;
-        positionMax = 0;
-      } else {
-        state = DocsEnumState.ITERATING;
-        positionMax = super.freq();
-      }
-      assert super.docID() == advanced;
-      return doc = advanced;
-    }
-
-    @Override
-    public int docID() {
-      assert doc == super.docID() : " invalid docID() in " + in.getClass() + " " + super.docID() + " instead of " + doc;
-      return doc;
-    }
-
-    @Override
-    public int freq() throws IOException {
-      assert state != DocsEnumState.START : "freq() called before nextDoc()/advance()";
-      assert state != DocsEnumState.FINISHED : "freq() called after NO_MORE_DOCS";
-      int freq = super.freq();
-      assert freq > 0;
-      return freq;
-    }
-
-    @Override
-    public int nextPosition() throws IOException {
-      assert state != DocsEnumState.START : "nextPosition() called before nextDoc()/advance()";
-      assert state != DocsEnumState.FINISHED : "nextPosition() called after NO_MORE_DOCS";
-      assert positionCount < positionMax : "nextPosition() called more than freq() times!";
-      int position = super.nextPosition();
-      assert position >= 0 || position == -1 : "invalid position: " + position;
-      positionCount++;
-      return position;
-    }
-
-    @Override
-    public int startOffset() throws IOException {
-      assert state != DocsEnumState.START : "startOffset() called before nextDoc()/advance()";
-      assert state != DocsEnumState.FINISHED : "startOffset() called after NO_MORE_DOCS";
-      assert positionCount > 0 : "startOffset() called before nextPosition()!";
-      return super.startOffset();
-    }
-
-    @Override
-    public int endOffset() throws IOException {
-      assert state != DocsEnumState.START : "endOffset() called before nextDoc()/advance()";
-      assert state != DocsEnumState.FINISHED : "endOffset() called after NO_MORE_DOCS";
-      assert positionCount > 0 : "endOffset() called before nextPosition()!";
-      return super.endOffset();
-    }
-
-    @Override
-    public BytesRef getPayload() throws IOException {
-      assert state != DocsEnumState.START : "getPayload() called before nextDoc()/advance()";
-      assert state != DocsEnumState.FINISHED : "getPayload() called after NO_MORE_DOCS";
-      assert positionCount > 0 : "getPayload() called before nextPosition()!";
-      BytesRef payload = super.getPayload();
-      assert payload == null || payload.isValid() && payload.length > 0 : "getPayload() returned payload with invalid length!";
-      return payload;
-    }
-  }
-  
-  /** Wraps a NumericDocValues but with additional asserts */
-  public static class AssertingNumericDocValues extends NumericDocValues {
-    private final NumericDocValues in;
-    private final int maxDoc;
-    
-    public AssertingNumericDocValues(NumericDocValues in, int maxDoc) {
-      this.in = in;
-      this.maxDoc = maxDoc;
-    }
-
-    @Override
-    public long get(int docID) {
-      assert docID >= 0 && docID < maxDoc;
-      return in.get(docID);
-    }    
-  }
-  
-  /** Wraps a BinaryDocValues but with additional asserts */
-  public static class AssertingBinaryDocValues extends BinaryDocValues {
-    private final BinaryDocValues in;
-    private final int maxDoc;
-    
-    public AssertingBinaryDocValues(BinaryDocValues in, int maxDoc) {
-      this.in = in;
-      this.maxDoc = maxDoc;
-    }
-
-    @Override
-    public BytesRef get(int docID) {
-      assert docID >= 0 && docID < maxDoc;
-      final BytesRef result = in.get(docID);
-      assert result.isValid();
-      return result;
-    }
-  }
-  
-  /** Wraps a SortedDocValues but with additional asserts */
-  public static class AssertingSortedDocValues extends SortedDocValues {
-    private final SortedDocValues in;
-    private final int maxDoc;
-    private final int valueCount;
-    
-    public AssertingSortedDocValues(SortedDocValues in, int maxDoc) {
-      this.in = in;
-      this.maxDoc = maxDoc;
-      this.valueCount = in.getValueCount();
-      assert valueCount >= 0 && valueCount <= maxDoc;
-    }
-
-    @Override
-    public int getOrd(int docID) {
-      assert docID >= 0 && docID < maxDoc;
-      int ord = in.getOrd(docID);
-      assert ord >= -1 && ord < valueCount;
-      return ord;
-    }
-
-    @Override
-    public BytesRef lookupOrd(int ord) {
-      assert ord >= 0 && ord < valueCount;
-      final BytesRef result = in.lookupOrd(ord);
-      assert result.isValid();
-      return result;
-    }
-
-    @Override
-    public int getValueCount() {
-      int valueCount = in.getValueCount();
-      assert valueCount == this.valueCount; // should not change
-      return valueCount;
-    }
-
-    @Override
-    public BytesRef get(int docID) {
-      assert docID >= 0 && docID < maxDoc;
-      final BytesRef result = in.get(docID);
-      assert result.isValid();
-      return result;
-    }
-
-    @Override
-    public int lookupTerm(BytesRef key) {
-      assert key.isValid();
-      int result = in.lookupTerm(key);
-      assert result < valueCount;
-      assert key.isValid();
-      return result;
-    }
-  }
-  
-  /** Wraps a SortedSetDocValues but with additional asserts */
-  public static class AssertingSortedNumericDocValues extends SortedNumericDocValues {
-    private final SortedNumericDocValues in;
-    private final int maxDoc;
-    
-    public AssertingSortedNumericDocValues(SortedNumericDocValues in, int maxDoc) {
-      this.in = in;
-      this.maxDoc = maxDoc;
-    }
-
-    @Override
-    public void setDocument(int doc) {
-      assert doc >= 0 && doc < maxDoc;
-      in.setDocument(doc);
-      // check the values are actually sorted
-      long previous = Long.MIN_VALUE;
-      for (int i = 0; i < in.count(); i++) {
-        long v = in.valueAt(i);
-        assert v >= previous;
-        previous = v;
-      }
-    }
-
-    @Override
-    public long valueAt(int index) {
-      assert index < in.count();
-      return in.valueAt(index);
-    }
-
-    @Override
-    public int count() {
-      return in.count();
-    } 
-  }
-  
-  /** Wraps a SortedSetDocValues but with additional asserts */
-  public static class AssertingSortedSetDocValues extends SortedSetDocValues {
-    private final SortedSetDocValues in;
-    private final int maxDoc;
-    private final long valueCount;
-    long lastOrd = NO_MORE_ORDS;
-    
-    public AssertingSortedSetDocValues(SortedSetDocValues in, int maxDoc) {
-      this.in = in;
-      this.maxDoc = maxDoc;
-      this.valueCount = in.getValueCount();
-      assert valueCount >= 0;
-    }
-    
-    @Override
-    public long nextOrd() {
-      assert lastOrd != NO_MORE_ORDS;
-      long ord = in.nextOrd();
-      assert ord < valueCount;
-      assert ord == NO_MORE_ORDS || ord > lastOrd;
-      lastOrd = ord;
-      return ord;
-    }
-
-    @Override
-    public void setDocument(int docID) {
-      assert docID >= 0 && docID < maxDoc : "docid=" + docID + ",maxDoc=" + maxDoc;
-      in.setDocument(docID);
-      lastOrd = -2;
-    }
-
-    @Override
-    public BytesRef lookupOrd(long ord) {
-      assert ord >= 0 && ord < valueCount;
-      final BytesRef result = in.lookupOrd(ord);
-      assert result.isValid();
-      return result;
-    }
-
-    @Override
-    public long getValueCount() {
-      long valueCount = in.getValueCount();
-      assert valueCount == this.valueCount; // should not change
-      return valueCount;
-    }
-
-    @Override
-    public long lookupTerm(BytesRef key) {
-      assert key.isValid();
-      long result = in.lookupTerm(key);
-      assert result < valueCount;
-      assert key.isValid();
-      return result;
-    }
-  }
-
-  @Override
-  public NumericDocValues getNumericDocValues(String field) throws IOException {
-    NumericDocValues dv = super.getNumericDocValues(field);
-    FieldInfo fi = getFieldInfos().fieldInfo(field);
-    if (dv != null) {
-      assert fi != null;
-      assert fi.getDocValuesType() == FieldInfo.DocValuesType.NUMERIC;
-      return new AssertingNumericDocValues(dv, maxDoc());
-    } else {
-      assert fi == null || fi.getDocValuesType() != FieldInfo.DocValuesType.NUMERIC;
-      return null;
-    }
-  }
-
-  @Override
-  public BinaryDocValues getBinaryDocValues(String field) throws IOException {
-    BinaryDocValues dv = super.getBinaryDocValues(field);
-    FieldInfo fi = getFieldInfos().fieldInfo(field);
-    if (dv != null) {
-      assert fi != null;
-      assert fi.getDocValuesType() == FieldInfo.DocValuesType.BINARY;
-      return new AssertingBinaryDocValues(dv, maxDoc());
-    } else {
-      assert fi == null || fi.getDocValuesType() != FieldInfo.DocValuesType.BINARY;
-      return null;
-    }
-  }
-
-  @Override
-  public SortedDocValues getSortedDocValues(String field) throws IOException {
-    SortedDocValues dv = super.getSortedDocValues(field);
-    FieldInfo fi = getFieldInfos().fieldInfo(field);
-    if (dv != null) {
-      assert fi != null;
-      assert fi.getDocValuesType() == FieldInfo.DocValuesType.SORTED;
-      return new AssertingSortedDocValues(dv, maxDoc());
-    } else {
-      assert fi == null || fi.getDocValuesType() != FieldInfo.DocValuesType.SORTED;
-      return null;
-    }
-  }
-  
-  @Override
-  public SortedNumericDocValues getSortedNumericDocValues(String field) throws IOException {
-    SortedNumericDocValues dv = super.getSortedNumericDocValues(field);
-    FieldInfo fi = getFieldInfos().fieldInfo(field);
-    if (dv != null) {
-      assert fi != null;
-      assert fi.getDocValuesType() == FieldInfo.DocValuesType.SORTED_NUMERIC;
-      return new AssertingSortedNumericDocValues(dv, maxDoc());
-    } else {
-      assert fi == null || fi.getDocValuesType() != FieldInfo.DocValuesType.SORTED_NUMERIC;
-      return null;
-    }
-  }
-
-  @Override
-  public SortedSetDocValues getSortedSetDocValues(String field) throws IOException {
-    SortedSetDocValues dv = super.getSortedSetDocValues(field);
-    FieldInfo fi = getFieldInfos().fieldInfo(field);
-    if (dv != null) {
-      assert fi != null;
-      assert fi.getDocValuesType() == FieldInfo.DocValuesType.SORTED_SET;
-      return new AssertingSortedSetDocValues(dv, maxDoc());
-    } else {
-      assert fi == null || fi.getDocValuesType() != FieldInfo.DocValuesType.SORTED_SET;
-      return null;
-    }
-  }
-
-  @Override
-  public NumericDocValues getNormValues(String field) throws IOException {
-    NumericDocValues dv = super.getNormValues(field);
-    FieldInfo fi = getFieldInfos().fieldInfo(field);
-    if (dv != null) {
-      assert fi != null;
-      assert fi.hasNorms();
-      return new AssertingNumericDocValues(dv, maxDoc());
-    } else {
-      assert fi == null || fi.hasNorms() == false;
-      return null;
-    }
-  }
-  
-  /** Wraps a Bits but with additional asserts */
-  public static class AssertingBits implements Bits {
-    final Bits in;
-    
-    public AssertingBits(Bits in) {
-      this.in = in;
-    }
-    
-    @Override
-    public boolean get(int index) {
-      assert index >= 0 && index < length();
-      return in.get(index);
-    }
-
-    @Override
-    public int length() {
-      return in.length();
-    }
-  }
-
-  @Override
-  public Bits getLiveDocs() {
-    Bits liveDocs = super.getLiveDocs();
-    if (liveDocs != null) {
-      assert maxDoc() == liveDocs.length();
-      liveDocs = new AssertingBits(liveDocs);
-    } else {
-      assert maxDoc() == numDocs();
-      assert !hasDeletions();
-    }
-    return liveDocs;
-  }
-
-  @Override
-  public Bits getDocsWithField(String field) throws IOException {
-    Bits docsWithField = super.getDocsWithField(field);
-    FieldInfo fi = getFieldInfos().fieldInfo(field);
-    if (docsWithField != null) {
-      assert fi != null;
-      assert fi.hasDocValues();
-      assert maxDoc() == docsWithField.length();
-      docsWithField = new AssertingBits(docsWithField);
-    } else {
-      assert fi == null || fi.hasDocValues() == false;
-    }
-    return docsWithField;
-  }
-
-  // this is the same hack as FCInvisible
-  @Override
-  public Object getCoreCacheKey() {
-    return cacheKey;
-  }
-
-  @Override
-  public Object getCombinedCoreAndDeletesKey() {
-    return cacheKey;
-  }
-  
-  private final Object cacheKey = new Object();
-}
diff --git lucene/test-framework/src/java/org/apache/lucene/index/AssertingDirectoryReader.java lucene/test-framework/src/java/org/apache/lucene/index/AssertingDirectoryReader.java
index 84931da..f05be75 100644
--- lucene/test-framework/src/java/org/apache/lucene/index/AssertingDirectoryReader.java
+++ lucene/test-framework/src/java/org/apache/lucene/index/AssertingDirectoryReader.java
@@ -19,14 +19,14 @@ package org.apache.lucene.index;
 
 /**
  * A {@link DirectoryReader} that wraps all its subreaders with
- * {@link AssertingAtomicReader}
+ * {@link AssertingLeafReader}
  */
 public class AssertingDirectoryReader extends FilterDirectoryReader {
 
   static class AssertingSubReaderWrapper extends SubReaderWrapper {
     @Override
-    public AtomicReader wrap(AtomicReader reader) {
-      return new AssertingAtomicReader(reader);
+    public LeafReader wrap(LeafReader reader) {
+      return new AssertingLeafReader(reader);
     }
   }
 
diff --git lucene/test-framework/src/java/org/apache/lucene/index/AssertingLeafReader.java lucene/test-framework/src/java/org/apache/lucene/index/AssertingLeafReader.java
new file mode 100644
index 0000000..710bce3
--- /dev/null
+++ lucene/test-framework/src/java/org/apache/lucene/index/AssertingLeafReader.java
@@ -0,0 +1,749 @@
+package org.apache.lucene.index;
+
+import java.io.IOException;
+import java.util.Iterator;
+
+import org.apache.lucene.search.DocIdSetIterator;
+import org.apache.lucene.util.Bits;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.VirtualMethod;
+import org.apache.lucene.util.automaton.CompiledAutomaton;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * A {@link FilterLeafReader} that can be used to apply
+ * additional checks for tests.
+ */
+public class AssertingLeafReader extends FilterLeafReader {
+
+  public AssertingLeafReader(LeafReader in) {
+    super(in);
+    // check some basic reader sanity
+    assert in.maxDoc() >= 0;
+    assert in.numDocs() <= in.maxDoc();
+    assert in.numDeletedDocs() + in.numDocs() == in.maxDoc();
+    assert !in.hasDeletions() || in.numDeletedDocs() > 0 && in.numDocs() < in.maxDoc();
+  }
+
+  @Override
+  public Fields fields() throws IOException {
+    Fields fields = super.fields();
+    return fields == null ? null : new AssertingFields(fields);
+  }
+  
+  @Override
+  public Fields getTermVectors(int docID) throws IOException {
+    Fields fields = super.getTermVectors(docID);
+    return fields == null ? null : new AssertingFields(fields);
+  }
+
+  /**
+   * Wraps a Fields but with additional asserts
+   */
+  public static class AssertingFields extends FilterFields {
+    public AssertingFields(Fields in) {
+      super(in);
+    }
+
+    @Override
+    public Iterator<String> iterator() {
+      Iterator<String> iterator = super.iterator();
+      assert iterator != null;
+      return iterator;
+    }
+
+    @Override
+    public Terms terms(String field) throws IOException {
+      Terms terms = super.terms(field);
+      return terms == null ? null : new AssertingTerms(terms);
+    }
+  }
+  
+  /**
+   * Wraps a Terms but with additional asserts
+   */
+  public static class AssertingTerms extends FilterTerms {
+    public AssertingTerms(Terms in) {
+      super(in);
+    }
+
+    @Override
+    public TermsEnum intersect(CompiledAutomaton automaton, BytesRef bytes) throws IOException {
+      TermsEnum termsEnum = in.intersect(automaton, bytes);
+      assert termsEnum != null;
+      assert bytes == null || bytes.isValid();
+      return new AssertingTermsEnum(termsEnum);
+    }
+
+    @Override
+    public BytesRef getMin() throws IOException {
+      BytesRef v = in.getMin();
+      assert v == null || v.isValid();
+      return v;
+    }
+
+    @Override
+    public BytesRef getMax() throws IOException {
+      BytesRef v = in.getMax();
+      assert v == null || v.isValid();
+      return v;
+    }
+
+    @Override
+    public TermsEnum iterator(TermsEnum reuse) throws IOException {
+      // TODO: should we give this thing a random to be super-evil,
+      // and randomly *not* unwrap?
+      if (reuse instanceof AssertingTermsEnum) {
+        reuse = ((AssertingTermsEnum) reuse).in;
+      }
+      TermsEnum termsEnum = super.iterator(reuse);
+      assert termsEnum != null;
+      return new AssertingTermsEnum(termsEnum);
+    }
+  }
+  
+  static final VirtualMethod<TermsEnum> SEEK_EXACT = new VirtualMethod<>(TermsEnum.class, "seekExact", BytesRef.class);
+
+  static class AssertingTermsEnum extends FilterTermsEnum {
+    private enum State {INITIAL, POSITIONED, UNPOSITIONED};
+    private State state = State.INITIAL;
+    private final boolean delegateOverridesSeekExact;
+
+    public AssertingTermsEnum(TermsEnum in) {
+      super(in);
+      delegateOverridesSeekExact = SEEK_EXACT.isOverriddenAsOf(in.getClass());
+    }
+
+    @Override
+    public DocsEnum docs(Bits liveDocs, DocsEnum reuse, int flags) throws IOException {
+      assert state == State.POSITIONED: "docs(...) called on unpositioned TermsEnum";
+
+      // TODO: should we give this thing a random to be super-evil,
+      // and randomly *not* unwrap?
+      if (reuse instanceof AssertingDocsEnum) {
+        reuse = ((AssertingDocsEnum) reuse).in;
+      }
+      DocsEnum docs = super.docs(liveDocs, reuse, flags);
+      return docs == null ? null : new AssertingDocsEnum(docs);
+    }
+
+    @Override
+    public DocsAndPositionsEnum docsAndPositions(Bits liveDocs, DocsAndPositionsEnum reuse, int flags) throws IOException {
+      assert state == State.POSITIONED: "docsAndPositions(...) called on unpositioned TermsEnum";
+
+      // TODO: should we give this thing a random to be super-evil,
+      // and randomly *not* unwrap?
+      if (reuse instanceof AssertingDocsAndPositionsEnum) {
+        reuse = ((AssertingDocsAndPositionsEnum) reuse).in;
+      }
+      DocsAndPositionsEnum docs = super.docsAndPositions(liveDocs, reuse, flags);
+      return docs == null ? null : new AssertingDocsAndPositionsEnum(docs);
+    }
+
+    // TODO: we should separately track if we are 'at the end' ?
+    // someone should not call next() after it returns null!!!!
+    @Override
+    public BytesRef next() throws IOException {
+      assert state == State.INITIAL || state == State.POSITIONED: "next() called on unpositioned TermsEnum";
+      BytesRef result = super.next();
+      if (result == null) {
+        state = State.UNPOSITIONED;
+      } else {
+        assert result.isValid();
+        state = State.POSITIONED;
+      }
+      return result;
+    }
+
+    @Override
+    public long ord() throws IOException {
+      assert state == State.POSITIONED : "ord() called on unpositioned TermsEnum";
+      return super.ord();
+    }
+
+    @Override
+    public int docFreq() throws IOException {
+      assert state == State.POSITIONED : "docFreq() called on unpositioned TermsEnum";
+      return super.docFreq();
+    }
+
+    @Override
+    public long totalTermFreq() throws IOException {
+      assert state == State.POSITIONED : "totalTermFreq() called on unpositioned TermsEnum";
+      return super.totalTermFreq();
+    }
+
+    @Override
+    public BytesRef term() throws IOException {
+      assert state == State.POSITIONED : "term() called on unpositioned TermsEnum";
+      BytesRef ret = super.term();
+      assert ret == null || ret.isValid();
+      return ret;
+    }
+
+    @Override
+    public void seekExact(long ord) throws IOException {
+      super.seekExact(ord);
+      state = State.POSITIONED;
+    }
+
+    @Override
+    public SeekStatus seekCeil(BytesRef term) throws IOException {
+      assert term.isValid();
+      SeekStatus result = super.seekCeil(term);
+      if (result == SeekStatus.END) {
+        state = State.UNPOSITIONED;
+      } else {
+        state = State.POSITIONED;
+      }
+      return result;
+    }
+
+    @Override
+    public boolean seekExact(BytesRef text) throws IOException {
+      assert text.isValid();
+      boolean result;
+      if (delegateOverridesSeekExact) {
+        result = in.seekExact(text);
+      } else {
+        result = super.seekExact(text);
+      }
+      if (result) {
+        state = State.POSITIONED;
+      } else {
+        state = State.UNPOSITIONED;
+      }
+      return result;
+    }
+
+    @Override
+    public TermState termState() throws IOException {
+      assert state == State.POSITIONED : "termState() called on unpositioned TermsEnum";
+      return super.termState();
+    }
+
+    @Override
+    public void seekExact(BytesRef term, TermState state) throws IOException {
+      assert term.isValid();
+      super.seekExact(term, state);
+      this.state = State.POSITIONED;
+    }
+
+    @Override
+    public String toString() {
+      return "AssertingTermsEnum(" + in + ")";
+    }
+  }
+  
+  static enum DocsEnumState { START, ITERATING, FINISHED };
+
+  /** Wraps a docsenum with additional checks */
+  public static class AssertingDocsEnum extends FilterDocsEnum {
+    private DocsEnumState state = DocsEnumState.START;
+    private int doc;
+    
+    public AssertingDocsEnum(DocsEnum in) {
+      this(in, true);
+    }
+
+    public AssertingDocsEnum(DocsEnum in, boolean failOnUnsupportedDocID) {
+      super(in);
+      try {
+        int docid = in.docID();
+        assert docid == -1 : in.getClass() + ": invalid initial doc id: " + docid;
+      } catch (UnsupportedOperationException e) {
+        if (failOnUnsupportedDocID) {
+          throw e;
+        }
+      }
+      doc = -1;
+    }
+
+    @Override
+    public int nextDoc() throws IOException {
+      assert state != DocsEnumState.FINISHED : "nextDoc() called after NO_MORE_DOCS";
+      int nextDoc = super.nextDoc();
+      assert nextDoc > doc : "backwards nextDoc from " + doc + " to " + nextDoc + " " + in;
+      if (nextDoc == DocIdSetIterator.NO_MORE_DOCS) {
+        state = DocsEnumState.FINISHED;
+      } else {
+        state = DocsEnumState.ITERATING;
+      }
+      assert super.docID() == nextDoc;
+      return doc = nextDoc;
+    }
+
+    @Override
+    public int advance(int target) throws IOException {
+      assert state != DocsEnumState.FINISHED : "advance() called after NO_MORE_DOCS";
+      assert target > doc : "target must be > docID(), got " + target + " <= " + doc;
+      int advanced = super.advance(target);
+      assert advanced >= target : "backwards advance from: " + target + " to: " + advanced;
+      if (advanced == DocIdSetIterator.NO_MORE_DOCS) {
+        state = DocsEnumState.FINISHED;
+      } else {
+        state = DocsEnumState.ITERATING;
+      }
+      assert super.docID() == advanced;
+      return doc = advanced;
+    }
+
+    @Override
+    public int docID() {
+      assert doc == super.docID() : " invalid docID() in " + in.getClass() + " " + super.docID() + " instead of " + doc;
+      return doc;
+    }
+
+    @Override
+    public int freq() throws IOException {
+      assert state != DocsEnumState.START : "freq() called before nextDoc()/advance()";
+      assert state != DocsEnumState.FINISHED : "freq() called after NO_MORE_DOCS";
+      int freq = super.freq();
+      assert freq > 0;
+      return freq;
+    }
+  }
+  
+  static class AssertingDocsAndPositionsEnum extends FilterDocsAndPositionsEnum {
+    private DocsEnumState state = DocsEnumState.START;
+    private int positionMax = 0;
+    private int positionCount = 0;
+    private int doc;
+
+    public AssertingDocsAndPositionsEnum(DocsAndPositionsEnum in) {
+      super(in);
+      int docid = in.docID();
+      assert docid == -1 : "invalid initial doc id: " + docid;
+      doc = -1;
+    }
+
+    @Override
+    public int nextDoc() throws IOException {
+      assert state != DocsEnumState.FINISHED : "nextDoc() called after NO_MORE_DOCS";
+      int nextDoc = super.nextDoc();
+      assert nextDoc > doc : "backwards nextDoc from " + doc + " to " + nextDoc;
+      positionCount = 0;
+      if (nextDoc == DocIdSetIterator.NO_MORE_DOCS) {
+        state = DocsEnumState.FINISHED;
+        positionMax = 0;
+      } else {
+        state = DocsEnumState.ITERATING;
+        positionMax = super.freq();
+      }
+      assert super.docID() == nextDoc;
+      return doc = nextDoc;
+    }
+
+    @Override
+    public int advance(int target) throws IOException {
+      assert state != DocsEnumState.FINISHED : "advance() called after NO_MORE_DOCS";
+      assert target > doc : "target must be > docID(), got " + target + " <= " + doc;
+      int advanced = super.advance(target);
+      assert advanced >= target : "backwards advance from: " + target + " to: " + advanced;
+      positionCount = 0;
+      if (advanced == DocIdSetIterator.NO_MORE_DOCS) {
+        state = DocsEnumState.FINISHED;
+        positionMax = 0;
+      } else {
+        state = DocsEnumState.ITERATING;
+        positionMax = super.freq();
+      }
+      assert super.docID() == advanced;
+      return doc = advanced;
+    }
+
+    @Override
+    public int docID() {
+      assert doc == super.docID() : " invalid docID() in " + in.getClass() + " " + super.docID() + " instead of " + doc;
+      return doc;
+    }
+
+    @Override
+    public int freq() throws IOException {
+      assert state != DocsEnumState.START : "freq() called before nextDoc()/advance()";
+      assert state != DocsEnumState.FINISHED : "freq() called after NO_MORE_DOCS";
+      int freq = super.freq();
+      assert freq > 0;
+      return freq;
+    }
+
+    @Override
+    public int nextPosition() throws IOException {
+      assert state != DocsEnumState.START : "nextPosition() called before nextDoc()/advance()";
+      assert state != DocsEnumState.FINISHED : "nextPosition() called after NO_MORE_DOCS";
+      assert positionCount < positionMax : "nextPosition() called more than freq() times!";
+      int position = super.nextPosition();
+      assert position >= 0 || position == -1 : "invalid position: " + position;
+      positionCount++;
+      return position;
+    }
+
+    @Override
+    public int startOffset() throws IOException {
+      assert state != DocsEnumState.START : "startOffset() called before nextDoc()/advance()";
+      assert state != DocsEnumState.FINISHED : "startOffset() called after NO_MORE_DOCS";
+      assert positionCount > 0 : "startOffset() called before nextPosition()!";
+      return super.startOffset();
+    }
+
+    @Override
+    public int endOffset() throws IOException {
+      assert state != DocsEnumState.START : "endOffset() called before nextDoc()/advance()";
+      assert state != DocsEnumState.FINISHED : "endOffset() called after NO_MORE_DOCS";
+      assert positionCount > 0 : "endOffset() called before nextPosition()!";
+      return super.endOffset();
+    }
+
+    @Override
+    public BytesRef getPayload() throws IOException {
+      assert state != DocsEnumState.START : "getPayload() called before nextDoc()/advance()";
+      assert state != DocsEnumState.FINISHED : "getPayload() called after NO_MORE_DOCS";
+      assert positionCount > 0 : "getPayload() called before nextPosition()!";
+      BytesRef payload = super.getPayload();
+      assert payload == null || payload.isValid() && payload.length > 0 : "getPayload() returned payload with invalid length!";
+      return payload;
+    }
+  }
+  
+  /** Wraps a NumericDocValues but with additional asserts */
+  public static class AssertingNumericDocValues extends NumericDocValues {
+    private final NumericDocValues in;
+    private final int maxDoc;
+    
+    public AssertingNumericDocValues(NumericDocValues in, int maxDoc) {
+      this.in = in;
+      this.maxDoc = maxDoc;
+    }
+
+    @Override
+    public long get(int docID) {
+      assert docID >= 0 && docID < maxDoc;
+      return in.get(docID);
+    }    
+  }
+  
+  /** Wraps a BinaryDocValues but with additional asserts */
+  public static class AssertingBinaryDocValues extends BinaryDocValues {
+    private final BinaryDocValues in;
+    private final int maxDoc;
+    
+    public AssertingBinaryDocValues(BinaryDocValues in, int maxDoc) {
+      this.in = in;
+      this.maxDoc = maxDoc;
+    }
+
+    @Override
+    public BytesRef get(int docID) {
+      assert docID >= 0 && docID < maxDoc;
+      final BytesRef result = in.get(docID);
+      assert result.isValid();
+      return result;
+    }
+  }
+  
+  /** Wraps a SortedDocValues but with additional asserts */
+  public static class AssertingSortedDocValues extends SortedDocValues {
+    private final SortedDocValues in;
+    private final int maxDoc;
+    private final int valueCount;
+    
+    public AssertingSortedDocValues(SortedDocValues in, int maxDoc) {
+      this.in = in;
+      this.maxDoc = maxDoc;
+      this.valueCount = in.getValueCount();
+      assert valueCount >= 0 && valueCount <= maxDoc;
+    }
+
+    @Override
+    public int getOrd(int docID) {
+      assert docID >= 0 && docID < maxDoc;
+      int ord = in.getOrd(docID);
+      assert ord >= -1 && ord < valueCount;
+      return ord;
+    }
+
+    @Override
+    public BytesRef lookupOrd(int ord) {
+      assert ord >= 0 && ord < valueCount;
+      final BytesRef result = in.lookupOrd(ord);
+      assert result.isValid();
+      return result;
+    }
+
+    @Override
+    public int getValueCount() {
+      int valueCount = in.getValueCount();
+      assert valueCount == this.valueCount; // should not change
+      return valueCount;
+    }
+
+    @Override
+    public BytesRef get(int docID) {
+      assert docID >= 0 && docID < maxDoc;
+      final BytesRef result = in.get(docID);
+      assert result.isValid();
+      return result;
+    }
+
+    @Override
+    public int lookupTerm(BytesRef key) {
+      assert key.isValid();
+      int result = in.lookupTerm(key);
+      assert result < valueCount;
+      assert key.isValid();
+      return result;
+    }
+  }
+  
+  /** Wraps a SortedSetDocValues but with additional asserts */
+  public static class AssertingSortedNumericDocValues extends SortedNumericDocValues {
+    private final SortedNumericDocValues in;
+    private final int maxDoc;
+    
+    public AssertingSortedNumericDocValues(SortedNumericDocValues in, int maxDoc) {
+      this.in = in;
+      this.maxDoc = maxDoc;
+    }
+
+    @Override
+    public void setDocument(int doc) {
+      assert doc >= 0 && doc < maxDoc;
+      in.setDocument(doc);
+      // check the values are actually sorted
+      long previous = Long.MIN_VALUE;
+      for (int i = 0; i < in.count(); i++) {
+        long v = in.valueAt(i);
+        assert v >= previous;
+        previous = v;
+      }
+    }
+
+    @Override
+    public long valueAt(int index) {
+      assert index < in.count();
+      return in.valueAt(index);
+    }
+
+    @Override
+    public int count() {
+      return in.count();
+    } 
+  }
+  
+  /** Wraps a SortedSetDocValues but with additional asserts */
+  public static class AssertingSortedSetDocValues extends SortedSetDocValues {
+    private final SortedSetDocValues in;
+    private final int maxDoc;
+    private final long valueCount;
+    long lastOrd = NO_MORE_ORDS;
+    
+    public AssertingSortedSetDocValues(SortedSetDocValues in, int maxDoc) {
+      this.in = in;
+      this.maxDoc = maxDoc;
+      this.valueCount = in.getValueCount();
+      assert valueCount >= 0;
+    }
+    
+    @Override
+    public long nextOrd() {
+      assert lastOrd != NO_MORE_ORDS;
+      long ord = in.nextOrd();
+      assert ord < valueCount;
+      assert ord == NO_MORE_ORDS || ord > lastOrd;
+      lastOrd = ord;
+      return ord;
+    }
+
+    @Override
+    public void setDocument(int docID) {
+      assert docID >= 0 && docID < maxDoc : "docid=" + docID + ",maxDoc=" + maxDoc;
+      in.setDocument(docID);
+      lastOrd = -2;
+    }
+
+    @Override
+    public BytesRef lookupOrd(long ord) {
+      assert ord >= 0 && ord < valueCount;
+      final BytesRef result = in.lookupOrd(ord);
+      assert result.isValid();
+      return result;
+    }
+
+    @Override
+    public long getValueCount() {
+      long valueCount = in.getValueCount();
+      assert valueCount == this.valueCount; // should not change
+      return valueCount;
+    }
+
+    @Override
+    public long lookupTerm(BytesRef key) {
+      assert key.isValid();
+      long result = in.lookupTerm(key);
+      assert result < valueCount;
+      assert key.isValid();
+      return result;
+    }
+  }
+
+  @Override
+  public NumericDocValues getNumericDocValues(String field) throws IOException {
+    NumericDocValues dv = super.getNumericDocValues(field);
+    FieldInfo fi = getFieldInfos().fieldInfo(field);
+    if (dv != null) {
+      assert fi != null;
+      assert fi.getDocValuesType() == FieldInfo.DocValuesType.NUMERIC;
+      return new AssertingNumericDocValues(dv, maxDoc());
+    } else {
+      assert fi == null || fi.getDocValuesType() != FieldInfo.DocValuesType.NUMERIC;
+      return null;
+    }
+  }
+
+  @Override
+  public BinaryDocValues getBinaryDocValues(String field) throws IOException {
+    BinaryDocValues dv = super.getBinaryDocValues(field);
+    FieldInfo fi = getFieldInfos().fieldInfo(field);
+    if (dv != null) {
+      assert fi != null;
+      assert fi.getDocValuesType() == FieldInfo.DocValuesType.BINARY;
+      return new AssertingBinaryDocValues(dv, maxDoc());
+    } else {
+      assert fi == null || fi.getDocValuesType() != FieldInfo.DocValuesType.BINARY;
+      return null;
+    }
+  }
+
+  @Override
+  public SortedDocValues getSortedDocValues(String field) throws IOException {
+    SortedDocValues dv = super.getSortedDocValues(field);
+    FieldInfo fi = getFieldInfos().fieldInfo(field);
+    if (dv != null) {
+      assert fi != null;
+      assert fi.getDocValuesType() == FieldInfo.DocValuesType.SORTED;
+      return new AssertingSortedDocValues(dv, maxDoc());
+    } else {
+      assert fi == null || fi.getDocValuesType() != FieldInfo.DocValuesType.SORTED;
+      return null;
+    }
+  }
+  
+  @Override
+  public SortedNumericDocValues getSortedNumericDocValues(String field) throws IOException {
+    SortedNumericDocValues dv = super.getSortedNumericDocValues(field);
+    FieldInfo fi = getFieldInfos().fieldInfo(field);
+    if (dv != null) {
+      assert fi != null;
+      assert fi.getDocValuesType() == FieldInfo.DocValuesType.SORTED_NUMERIC;
+      return new AssertingSortedNumericDocValues(dv, maxDoc());
+    } else {
+      assert fi == null || fi.getDocValuesType() != FieldInfo.DocValuesType.SORTED_NUMERIC;
+      return null;
+    }
+  }
+
+  @Override
+  public SortedSetDocValues getSortedSetDocValues(String field) throws IOException {
+    SortedSetDocValues dv = super.getSortedSetDocValues(field);
+    FieldInfo fi = getFieldInfos().fieldInfo(field);
+    if (dv != null) {
+      assert fi != null;
+      assert fi.getDocValuesType() == FieldInfo.DocValuesType.SORTED_SET;
+      return new AssertingSortedSetDocValues(dv, maxDoc());
+    } else {
+      assert fi == null || fi.getDocValuesType() != FieldInfo.DocValuesType.SORTED_SET;
+      return null;
+    }
+  }
+
+  @Override
+  public NumericDocValues getNormValues(String field) throws IOException {
+    NumericDocValues dv = super.getNormValues(field);
+    FieldInfo fi = getFieldInfos().fieldInfo(field);
+    if (dv != null) {
+      assert fi != null;
+      assert fi.hasNorms();
+      return new AssertingNumericDocValues(dv, maxDoc());
+    } else {
+      assert fi == null || fi.hasNorms() == false;
+      return null;
+    }
+  }
+  
+  /** Wraps a Bits but with additional asserts */
+  public static class AssertingBits implements Bits {
+    final Bits in;
+    
+    public AssertingBits(Bits in) {
+      this.in = in;
+    }
+    
+    @Override
+    public boolean get(int index) {
+      assert index >= 0 && index < length();
+      return in.get(index);
+    }
+
+    @Override
+    public int length() {
+      return in.length();
+    }
+  }
+
+  @Override
+  public Bits getLiveDocs() {
+    Bits liveDocs = super.getLiveDocs();
+    if (liveDocs != null) {
+      assert maxDoc() == liveDocs.length();
+      liveDocs = new AssertingBits(liveDocs);
+    } else {
+      assert maxDoc() == numDocs();
+      assert !hasDeletions();
+    }
+    return liveDocs;
+  }
+
+  @Override
+  public Bits getDocsWithField(String field) throws IOException {
+    Bits docsWithField = super.getDocsWithField(field);
+    FieldInfo fi = getFieldInfos().fieldInfo(field);
+    if (docsWithField != null) {
+      assert fi != null;
+      assert fi.hasDocValues();
+      assert maxDoc() == docsWithField.length();
+      docsWithField = new AssertingBits(docsWithField);
+    } else {
+      assert fi == null || fi.hasDocValues() == false;
+    }
+    return docsWithField;
+  }
+
+  // this is the same hack as FCInvisible
+  @Override
+  public Object getCoreCacheKey() {
+    return cacheKey;
+  }
+
+  @Override
+  public Object getCombinedCoreAndDeletesKey() {
+    return cacheKey;
+  }
+  
+  private final Object cacheKey = new Object();
+}
diff --git lucene/test-framework/src/java/org/apache/lucene/index/BaseDocValuesFormatTestCase.java lucene/test-framework/src/java/org/apache/lucene/index/BaseDocValuesFormatTestCase.java
index 0c63836..faf057c 100644
--- lucene/test-framework/src/java/org/apache/lucene/index/BaseDocValuesFormatTestCase.java
+++ lucene/test-framework/src/java/org/apache/lucene/index/BaseDocValuesFormatTestCase.java
@@ -1121,7 +1121,7 @@ public abstract class BaseDocValuesFormatTestCase extends BaseIndexFileFormatTes
       int ord = docValues.lookupTerm(expected);
       assertEquals(i, ord);
     }
-    AtomicReader slowR = SlowCompositeReaderWrapper.wrap(reader);
+    LeafReader slowR = SlowCompositeReaderWrapper.wrap(reader);
     Set<Entry<String, String>> entrySet = docToString.entrySet();
 
     for (Entry<String, String> entry : entrySet) {
@@ -1194,8 +1194,8 @@ public abstract class BaseDocValuesFormatTestCase extends BaseIndexFileFormatTes
     
     // compare
     DirectoryReader ir = DirectoryReader.open(dir);
-    for (AtomicReaderContext context : ir.leaves()) {
-      AtomicReader r = context.reader();
+    for (LeafReaderContext context : ir.leaves()) {
+      LeafReader r = context.reader();
       NumericDocValues docValues = r.getNumericDocValues("dv");
       for (int i = 0; i < r.maxDoc(); i++) {
         long storedValue = Long.parseLong(r.document(i).get("stored"));
@@ -1252,8 +1252,8 @@ public abstract class BaseDocValuesFormatTestCase extends BaseIndexFileFormatTes
     
     // compare
     DirectoryReader ir = DirectoryReader.open(dir);
-    for (AtomicReaderContext context : ir.leaves()) {
-      AtomicReader r = context.reader();
+    for (LeafReaderContext context : ir.leaves()) {
+      LeafReader r = context.reader();
       SortedNumericDocValues docValues = DocValues.getSortedNumeric(r, "dv");
       for (int i = 0; i < r.maxDoc(); i++) {
         String expected[] = r.document(i).getValues("stored");
@@ -1345,8 +1345,8 @@ public abstract class BaseDocValuesFormatTestCase extends BaseIndexFileFormatTes
     
     // compare
     DirectoryReader ir = writer.getReader();
-    for (AtomicReaderContext context : ir.leaves()) {
-      AtomicReader r = context.reader();
+    for (LeafReaderContext context : ir.leaves()) {
+      LeafReader r = context.reader();
       BinaryDocValues docValues = r.getBinaryDocValues("dv");
       for (int i = 0; i < r.maxDoc(); i++) {
         BytesRef binaryValue = r.document(i).getBinaryValue("stored");
@@ -1359,8 +1359,8 @@ public abstract class BaseDocValuesFormatTestCase extends BaseIndexFileFormatTes
     // compare again
     writer.forceMerge(1);
     ir = writer.getReader();
-    for (AtomicReaderContext context : ir.leaves()) {
-      AtomicReader r = context.reader();
+    for (LeafReaderContext context : ir.leaves()) {
+      LeafReader r = context.reader();
       BinaryDocValues docValues = r.getBinaryDocValues("dv");
       for (int i = 0; i < r.maxDoc(); i++) {
         BytesRef binaryValue = r.document(i).getBinaryValue("stored");
@@ -1428,8 +1428,8 @@ public abstract class BaseDocValuesFormatTestCase extends BaseIndexFileFormatTes
     
     // compare
     DirectoryReader ir = writer.getReader();
-    for (AtomicReaderContext context : ir.leaves()) {
-      AtomicReader r = context.reader();
+    for (LeafReaderContext context : ir.leaves()) {
+      LeafReader r = context.reader();
       BinaryDocValues docValues = r.getSortedDocValues("dv");
       for (int i = 0; i < r.maxDoc(); i++) {
         BytesRef binaryValue = r.document(i).getBinaryValue("stored");
@@ -1442,8 +1442,8 @@ public abstract class BaseDocValuesFormatTestCase extends BaseIndexFileFormatTes
     
     // compare again
     ir = writer.getReader();
-    for (AtomicReaderContext context : ir.leaves()) {
-      AtomicReader r = context.reader();
+    for (LeafReaderContext context : ir.leaves()) {
+      LeafReader r = context.reader();
       BinaryDocValues docValues = r.getSortedDocValues("dv");
       for (int i = 0; i < r.maxDoc(); i++) {
         BytesRef binaryValue = r.document(i).getBinaryValue("stored");
@@ -1959,8 +1959,8 @@ public abstract class BaseDocValuesFormatTestCase extends BaseIndexFileFormatTes
     
     // compare
     DirectoryReader ir = writer.getReader();
-    for (AtomicReaderContext context : ir.leaves()) {
-      AtomicReader r = context.reader();
+    for (LeafReaderContext context : ir.leaves()) {
+      LeafReader r = context.reader();
       SortedSetDocValues docValues = r.getSortedSetDocValues("dv");
       for (int i = 0; i < r.maxDoc(); i++) {
         String stringValues[] = r.document(i).getValues("stored");
@@ -1982,8 +1982,8 @@ public abstract class BaseDocValuesFormatTestCase extends BaseIndexFileFormatTes
     
     // compare again
     ir = writer.getReader();
-    for (AtomicReaderContext context : ir.leaves()) {
-      AtomicReader r = context.reader();
+    for (LeafReaderContext context : ir.leaves()) {
+      LeafReader r = context.reader();
       SortedSetDocValues docValues = r.getSortedSetDocValues("dv");
       for (int i = 0; i < r.maxDoc(); i++) {
         String stringValues[] = r.document(i).getValues("stored");
@@ -2154,7 +2154,7 @@ public abstract class BaseDocValuesFormatTestCase extends BaseIndexFileFormatTes
     
     IndexReader ir = DirectoryReader.open(directory);
     assertEquals(1, ir.leaves().size());
-    AtomicReader ar = ir.leaves().get(0).reader();
+    LeafReader ar = ir.leaves().get(0).reader();
     NumericDocValues dv = ar.getNumericDocValues("dv1");
     assertEquals(0, dv.get(0));
     assertEquals(0, dv.get(1));
@@ -2184,7 +2184,7 @@ public abstract class BaseDocValuesFormatTestCase extends BaseIndexFileFormatTes
     
     IndexReader ir = DirectoryReader.open(directory);
     assertEquals(1, ir.leaves().size());
-    AtomicReader ar = ir.leaves().get(0).reader();
+    LeafReader ar = ir.leaves().get(0).reader();
     NumericDocValues dv = ar.getNumericDocValues("dv1");
     assertEquals(0, dv.get(0));
     assertEquals(0, dv.get(1));
@@ -2218,7 +2218,7 @@ public abstract class BaseDocValuesFormatTestCase extends BaseIndexFileFormatTes
     
     IndexReader ir = DirectoryReader.open(directory);
     assertEquals(1, ir.leaves().size());
-    AtomicReader ar = ir.leaves().get(0).reader();
+    LeafReader ar = ir.leaves().get(0).reader();
     NumericDocValues dv = ar.getNumericDocValues("dv1");
     assertEquals(0, dv.get(0));
     assertEquals(0, dv.get(1));
@@ -2249,7 +2249,7 @@ public abstract class BaseDocValuesFormatTestCase extends BaseIndexFileFormatTes
     
     IndexReader ir = DirectoryReader.open(directory);
     assertEquals(1, ir.leaves().size());
-    AtomicReader ar = ir.leaves().get(0).reader();
+    LeafReader ar = ir.leaves().get(0).reader();
     BinaryDocValues dv = ar.getBinaryDocValues("dv1");
     BytesRef ref = dv.get(0);
     assertEquals(new BytesRef(), ref);
@@ -2281,7 +2281,7 @@ public abstract class BaseDocValuesFormatTestCase extends BaseIndexFileFormatTes
     
     IndexReader ir = DirectoryReader.open(directory);
     assertEquals(1, ir.leaves().size());
-    AtomicReader ar = ir.leaves().get(0).reader();
+    LeafReader ar = ir.leaves().get(0).reader();
     BinaryDocValues dv = ar.getBinaryDocValues("dv1");
     BytesRef ref = dv.get(0);
     assertEquals(new BytesRef(), ref);
@@ -2317,7 +2317,7 @@ public abstract class BaseDocValuesFormatTestCase extends BaseIndexFileFormatTes
     
     IndexReader ir = DirectoryReader.open(directory);
     assertEquals(1, ir.leaves().size());
-    AtomicReader ar = ir.leaves().get(0).reader();
+    LeafReader ar = ir.leaves().get(0).reader();
     BinaryDocValues dv = ar.getBinaryDocValues("dv1");
     BytesRef ref = dv.get(0);
     assertEquals(new BytesRef(), ref);
@@ -2391,8 +2391,8 @@ public abstract class BaseDocValuesFormatTestCase extends BaseIndexFileFormatTes
         public void run() {
           try {
             startingGun.await();
-            for (AtomicReaderContext context : ir.leaves()) {
-              AtomicReader r = context.reader();
+            for (LeafReaderContext context : ir.leaves()) {
+              LeafReader r = context.reader();
               BinaryDocValues binaries = r.getBinaryDocValues("dvBin");
               SortedDocValues sorted = r.getSortedDocValues("dvSorted");
               NumericDocValues numerics = r.getNumericDocValues("dvNum");
@@ -2505,8 +2505,8 @@ public abstract class BaseDocValuesFormatTestCase extends BaseIndexFileFormatTes
         public void run() {
           try {
             startingGun.await();
-            for (AtomicReaderContext context : ir.leaves()) {
-              AtomicReader r = context.reader();
+            for (LeafReaderContext context : ir.leaves()) {
+              LeafReader r = context.reader();
               BinaryDocValues binaries = r.getBinaryDocValues("dvBin");
               Bits binaryBits = r.getDocsWithField("dvBin");
               SortedDocValues sorted = r.getSortedDocValues("dvSorted");
@@ -2641,7 +2641,7 @@ public abstract class BaseDocValuesFormatTestCase extends BaseIndexFileFormatTes
               ByteArrayOutputStream bos = new ByteArrayOutputStream(1024);
               PrintStream infoStream = new PrintStream(bos, false, IOUtils.UTF_8);
               startingGun.await();
-              for (AtomicReaderContext leaf : r.leaves()) {
+              for (LeafReaderContext leaf : r.leaves()) {
                 CheckIndex.testDocValues(leaf.reader(), infoStream, true);
               }
             } catch (Exception e) {
@@ -2688,7 +2688,7 @@ public abstract class BaseDocValuesFormatTestCase extends BaseIndexFileFormatTes
       IndexReader r = w.getReader();
       w.close();
 
-      AtomicReader ar = SlowCompositeReaderWrapper.wrap(r);
+      LeafReader ar = SlowCompositeReaderWrapper.wrap(r);
       BinaryDocValues values = ar.getBinaryDocValues("field");
       for(int j=0;j<5;j++) {
         BytesRef result = values.get(0);
diff --git lucene/test-framework/src/java/org/apache/lucene/index/BaseIndexFileFormatTestCase.java lucene/test-framework/src/java/org/apache/lucene/index/BaseIndexFileFormatTestCase.java
index fa33657..83209ac 100644
--- lucene/test-framework/src/java/org/apache/lucene/index/BaseIndexFileFormatTestCase.java
+++ lucene/test-framework/src/java/org/apache/lucene/index/BaseIndexFileFormatTestCase.java
@@ -224,7 +224,7 @@ abstract class BaseIndexFileFormatTestCase extends LuceneTestCase {
     IndexWriter w = new IndexWriter(dir, cfg);
     // we need to index enough documents so that constant overhead doesn't dominate
     final int numDocs = atLeast(10000);
-    AtomicReader reader1 = null;
+    LeafReader reader1 = null;
     for (int i = 0; i < numDocs; ++i) {
       Document d = new Document();
       addRandomFields(d);
@@ -239,9 +239,9 @@ abstract class BaseIndexFileFormatTestCase extends LuceneTestCase {
     w.commit();
     w.close();
 
-    AtomicReader reader2 = getOnlySegmentReader(DirectoryReader.open(dir));
+    LeafReader reader2 = getOnlySegmentReader(DirectoryReader.open(dir));
 
-    for (AtomicReader reader : Arrays.asList(reader1, reader2)) {
+    for (LeafReader reader : Arrays.asList(reader1, reader2)) {
       new SimpleMergedSegmentWarmer(InfoStream.NO_OUTPUT).warm(reader);
     }
 
diff --git lucene/test-framework/src/java/org/apache/lucene/index/BaseNormsFormatTestCase.java lucene/test-framework/src/java/org/apache/lucene/index/BaseNormsFormatTestCase.java
index 546d438..2305f82 100644
--- lucene/test-framework/src/java/org/apache/lucene/index/BaseNormsFormatTestCase.java
+++ lucene/test-framework/src/java/org/apache/lucene/index/BaseNormsFormatTestCase.java
@@ -178,8 +178,8 @@ public abstract class BaseNormsFormatTestCase extends BaseIndexFileFormatTestCas
     
     // compare
     DirectoryReader ir = DirectoryReader.open(dir);
-    for (AtomicReaderContext context : ir.leaves()) {
-      AtomicReader r = context.reader();
+    for (LeafReaderContext context : ir.leaves()) {
+      LeafReader r = context.reader();
       NumericDocValues docValues = r.getNormValues("stored");
       for (int i = 0; i < r.maxDoc(); i++) {
         long storedValue = Long.parseLong(r.document(i).get("stored"));
@@ -192,8 +192,8 @@ public abstract class BaseNormsFormatTestCase extends BaseIndexFileFormatTestCas
     
     // compare again
     ir = DirectoryReader.open(dir);
-    for (AtomicReaderContext context : ir.leaves()) {
-      AtomicReader r = context.reader();
+    for (LeafReaderContext context : ir.leaves()) {
+      LeafReader r = context.reader();
       NumericDocValues docValues = r.getNormValues("stored");
       for (int i = 0; i < r.maxDoc(); i++) {
         long storedValue = Long.parseLong(r.document(i).get("stored"));
@@ -230,7 +230,7 @@ public abstract class BaseNormsFormatTestCase extends BaseIndexFileFormatTestCas
     }
 
     @Override
-    public SimScorer simScorer(SimWeight weight, AtomicReaderContext context) throws IOException {
+    public SimScorer simScorer(SimWeight weight, LeafReaderContext context) throws IOException {
       throw new UnsupportedOperationException();
     }
   }
diff --git lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase.java lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase.java
index b6ba0e1..67fc568 100644
--- lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase.java
+++ lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase.java
@@ -17,7 +17,6 @@ package org.apache.lucene.index;
  * limitations under the License.
  */
 
-import java.io.File;
 import java.io.IOException;
 import java.nio.file.Path;
 import java.util.ArrayList;
@@ -1419,7 +1418,7 @@ public abstract class BasePostingsFormatTestCase extends BaseIndexFileFormatTest
     doc.add(newStringField("", "something", Field.Store.NO));
     iw.addDocument(doc);
     DirectoryReader ir = iw.getReader();
-    AtomicReader ar = getOnlySegmentReader(ir);
+    LeafReader ar = getOnlySegmentReader(ir);
     Fields fields = ar.fields();
     int fieldCount = fields.size();
     // -1 is allowed, if the codec doesn't implement fields.size():
@@ -1444,7 +1443,7 @@ public abstract class BasePostingsFormatTestCase extends BaseIndexFileFormatTest
     doc.add(newStringField("", "", Field.Store.NO));
     iw.addDocument(doc);
     DirectoryReader ir = iw.getReader();
-    AtomicReader ar = getOnlySegmentReader(ir);
+    LeafReader ar = getOnlySegmentReader(ir);
     Fields fields = ar.fields();
     int fieldCount = fields.size();
     // -1 is allowed, if the codec doesn't implement fields.size():
@@ -1476,7 +1475,7 @@ public abstract class BasePostingsFormatTestCase extends BaseIndexFileFormatTest
     iw.deleteDocuments(new Term("ghostField", "something")); // delete the only term for the field
     iw.forceMerge(1);
     DirectoryReader ir = iw.getReader();
-    AtomicReader ar = getOnlySegmentReader(ir);
+    LeafReader ar = getOnlySegmentReader(ir);
     Fields fields = ar.fields();
     // Ghost busting terms dict impls will have
     // fields.size() == 0; all others must be == 1:
diff --git lucene/test-framework/src/java/org/apache/lucene/index/BaseStoredFieldsFormatTestCase.java lucene/test-framework/src/java/org/apache/lucene/index/BaseStoredFieldsFormatTestCase.java
index 6910968..ce803d9 100644
--- lucene/test-framework/src/java/org/apache/lucene/index/BaseStoredFieldsFormatTestCase.java
+++ lucene/test-framework/src/java/org/apache/lucene/index/BaseStoredFieldsFormatTestCase.java
@@ -296,8 +296,8 @@ public abstract class BaseStoredFieldsFormatTestCase extends BaseIndexFileFormat
     
     assertEquals(numDocs, r.numDocs());
 
-    for(AtomicReaderContext ctx : r.leaves()) {
-      final AtomicReader sub = ctx.reader();
+    for(LeafReaderContext ctx : r.leaves()) {
+      final LeafReader sub = ctx.reader();
       final NumericDocValues ids = DocValues.getNumeric(sub, "id");
       for(int docID=0;docID<sub.numDocs();docID++) {
         final StoredDocument doc = sub.document(docID);
diff --git lucene/test-framework/src/java/org/apache/lucene/index/FieldFilterAtomicReader.java lucene/test-framework/src/java/org/apache/lucene/index/FieldFilterAtomicReader.java
deleted file mode 100644
index 9ca1101..0000000
--- lucene/test-framework/src/java/org/apache/lucene/index/FieldFilterAtomicReader.java
+++ /dev/null
@@ -1,192 +0,0 @@
-package org.apache.lucene.index;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Iterator;
-import java.util.Set;
-
-import org.apache.lucene.util.Bits;
-import org.apache.lucene.util.FilterIterator;
-
-/**
- * A {@link FilterAtomicReader} that exposes only a subset
- * of fields from the underlying wrapped reader.
- */
-public final class FieldFilterAtomicReader extends FilterAtomicReader {
-  
-  private final Set<String> fields;
-  private final boolean negate;
-  private final FieldInfos fieldInfos;
-
-  public FieldFilterAtomicReader(AtomicReader in, Set<String> fields, boolean negate) {
-    super(in);
-    this.fields = fields;
-    this.negate = negate;
-    ArrayList<FieldInfo> filteredInfos = new ArrayList<>();
-    for (FieldInfo fi : in.getFieldInfos()) {
-      if (hasField(fi.name)) {
-        filteredInfos.add(fi);
-      }
-    }
-    fieldInfos = new FieldInfos(filteredInfos.toArray(new FieldInfo[filteredInfos.size()]));
-  }
-  
-  boolean hasField(String field) {
-    return negate ^ fields.contains(field);
-  }
-
-  @Override
-  public FieldInfos getFieldInfos() {
-    return fieldInfos;
-  }
-
-  @Override
-  public Fields getTermVectors(int docID) throws IOException {
-    Fields f = super.getTermVectors(docID);
-    if (f == null) {
-      return null;
-    }
-    f = new FieldFilterFields(f);
-    // we need to check for emptyness, so we can return
-    // null:
-    return f.iterator().hasNext() ? f : null;
-  }
-
-  @Override
-  public void document(final int docID, final StoredFieldVisitor visitor) throws IOException {
-    super.document(docID, new StoredFieldVisitor() {
-      @Override
-      public void binaryField(FieldInfo fieldInfo, byte[] value) throws IOException {
-        visitor.binaryField(fieldInfo, value);
-      }
-
-      @Override
-      public void stringField(FieldInfo fieldInfo, String value) throws IOException {
-        visitor.stringField(fieldInfo, value);
-      }
-
-      @Override
-      public void intField(FieldInfo fieldInfo, int value) throws IOException {
-        visitor.intField(fieldInfo, value);
-      }
-
-      @Override
-      public void longField(FieldInfo fieldInfo, long value) throws IOException {
-        visitor.longField(fieldInfo, value);
-      }
-
-      @Override
-      public void floatField(FieldInfo fieldInfo, float value) throws IOException {
-        visitor.floatField(fieldInfo, value);
-      }
-
-      @Override
-      public void doubleField(FieldInfo fieldInfo, double value) throws IOException {
-        visitor.doubleField(fieldInfo, value);
-      }
-
-      @Override
-      public Status needsField(FieldInfo fieldInfo) throws IOException {
-        return hasField(fieldInfo.name) ? visitor.needsField(fieldInfo) : Status.NO;
-      }
-    });
-  }
-
-  @Override
-  public Fields fields() throws IOException {
-    final Fields f = super.fields();
-    return (f == null) ? null : new FieldFilterFields(f);
-  }
-  
-  
-
-  @Override
-  public NumericDocValues getNumericDocValues(String field) throws IOException {
-    return hasField(field) ? super.getNumericDocValues(field) : null;
-  }
-
-  @Override
-  public BinaryDocValues getBinaryDocValues(String field) throws IOException {
-    return hasField(field) ? super.getBinaryDocValues(field) : null;
-  }
-
-  @Override
-  public SortedDocValues getSortedDocValues(String field) throws IOException {
-    return hasField(field) ? super.getSortedDocValues(field) : null;
-  }
-  
-  @Override
-  public SortedNumericDocValues getSortedNumericDocValues(String field) throws IOException {
-    return hasField(field) ? super.getSortedNumericDocValues(field) : null;
-  }
-  
-  @Override
-  public SortedSetDocValues getSortedSetDocValues(String field) throws IOException {
-    return hasField(field) ? super.getSortedSetDocValues(field) : null;
-  }
-
-  @Override
-  public NumericDocValues getNormValues(String field) throws IOException {
-    return hasField(field) ? super.getNormValues(field) : null;
-  }
-
-  @Override
-  public Bits getDocsWithField(String field) throws IOException {
-    return hasField(field) ? super.getDocsWithField(field) : null;
-  }
-
-  @Override
-  public String toString() {
-    final StringBuilder sb = new StringBuilder("FieldFilterAtomicReader(reader=");
-    sb.append(in).append(", fields=");
-    if (negate) sb.append('!');
-    return sb.append(fields).append(')').toString();
-  }
-  
-  private class FieldFilterFields extends FilterFields {
-
-    public FieldFilterFields(Fields in) {
-      super(in);
-    }
-
-    @Override
-    public int size() {
-      // this information is not cheap, return -1 like MultiFields does:
-      return -1;
-    }
-
-    @Override
-    public Iterator<String> iterator() {
-      return new FilterIterator<String, String>(super.iterator()) {
-        @Override
-        protected boolean predicateFunction(String field) {
-          return hasField(field);
-        }
-      };
-    }
-
-    @Override
-    public Terms terms(String field) throws IOException {
-      return hasField(field) ? super.terms(field) : null;
-    }
-    
-  }
-  
-}
diff --git lucene/test-framework/src/java/org/apache/lucene/index/FieldFilterLeafReader.java lucene/test-framework/src/java/org/apache/lucene/index/FieldFilterLeafReader.java
new file mode 100644
index 0000000..14e65f7
--- /dev/null
+++ lucene/test-framework/src/java/org/apache/lucene/index/FieldFilterLeafReader.java
@@ -0,0 +1,192 @@
+package org.apache.lucene.index;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Iterator;
+import java.util.Set;
+
+import org.apache.lucene.util.Bits;
+import org.apache.lucene.util.FilterIterator;
+
+/**
+ * A {@link FilterLeafReader} that exposes only a subset
+ * of fields from the underlying wrapped reader.
+ */
+public final class FieldFilterLeafReader extends FilterLeafReader {
+  
+  private final Set<String> fields;
+  private final boolean negate;
+  private final FieldInfos fieldInfos;
+
+  public FieldFilterLeafReader(LeafReader in, Set<String> fields, boolean negate) {
+    super(in);
+    this.fields = fields;
+    this.negate = negate;
+    ArrayList<FieldInfo> filteredInfos = new ArrayList<>();
+    for (FieldInfo fi : in.getFieldInfos()) {
+      if (hasField(fi.name)) {
+        filteredInfos.add(fi);
+      }
+    }
+    fieldInfos = new FieldInfos(filteredInfos.toArray(new FieldInfo[filteredInfos.size()]));
+  }
+  
+  boolean hasField(String field) {
+    return negate ^ fields.contains(field);
+  }
+
+  @Override
+  public FieldInfos getFieldInfos() {
+    return fieldInfos;
+  }
+
+  @Override
+  public Fields getTermVectors(int docID) throws IOException {
+    Fields f = super.getTermVectors(docID);
+    if (f == null) {
+      return null;
+    }
+    f = new FieldFilterFields(f);
+    // we need to check for emptyness, so we can return
+    // null:
+    return f.iterator().hasNext() ? f : null;
+  }
+
+  @Override
+  public void document(final int docID, final StoredFieldVisitor visitor) throws IOException {
+    super.document(docID, new StoredFieldVisitor() {
+      @Override
+      public void binaryField(FieldInfo fieldInfo, byte[] value) throws IOException {
+        visitor.binaryField(fieldInfo, value);
+      }
+
+      @Override
+      public void stringField(FieldInfo fieldInfo, String value) throws IOException {
+        visitor.stringField(fieldInfo, value);
+      }
+
+      @Override
+      public void intField(FieldInfo fieldInfo, int value) throws IOException {
+        visitor.intField(fieldInfo, value);
+      }
+
+      @Override
+      public void longField(FieldInfo fieldInfo, long value) throws IOException {
+        visitor.longField(fieldInfo, value);
+      }
+
+      @Override
+      public void floatField(FieldInfo fieldInfo, float value) throws IOException {
+        visitor.floatField(fieldInfo, value);
+      }
+
+      @Override
+      public void doubleField(FieldInfo fieldInfo, double value) throws IOException {
+        visitor.doubleField(fieldInfo, value);
+      }
+
+      @Override
+      public Status needsField(FieldInfo fieldInfo) throws IOException {
+        return hasField(fieldInfo.name) ? visitor.needsField(fieldInfo) : Status.NO;
+      }
+    });
+  }
+
+  @Override
+  public Fields fields() throws IOException {
+    final Fields f = super.fields();
+    return (f == null) ? null : new FieldFilterFields(f);
+  }
+  
+  
+
+  @Override
+  public NumericDocValues getNumericDocValues(String field) throws IOException {
+    return hasField(field) ? super.getNumericDocValues(field) : null;
+  }
+
+  @Override
+  public BinaryDocValues getBinaryDocValues(String field) throws IOException {
+    return hasField(field) ? super.getBinaryDocValues(field) : null;
+  }
+
+  @Override
+  public SortedDocValues getSortedDocValues(String field) throws IOException {
+    return hasField(field) ? super.getSortedDocValues(field) : null;
+  }
+  
+  @Override
+  public SortedNumericDocValues getSortedNumericDocValues(String field) throws IOException {
+    return hasField(field) ? super.getSortedNumericDocValues(field) : null;
+  }
+  
+  @Override
+  public SortedSetDocValues getSortedSetDocValues(String field) throws IOException {
+    return hasField(field) ? super.getSortedSetDocValues(field) : null;
+  }
+
+  @Override
+  public NumericDocValues getNormValues(String field) throws IOException {
+    return hasField(field) ? super.getNormValues(field) : null;
+  }
+
+  @Override
+  public Bits getDocsWithField(String field) throws IOException {
+    return hasField(field) ? super.getDocsWithField(field) : null;
+  }
+
+  @Override
+  public String toString() {
+    final StringBuilder sb = new StringBuilder("FieldFilterAtomicReader(reader=");
+    sb.append(in).append(", fields=");
+    if (negate) sb.append('!');
+    return sb.append(fields).append(')').toString();
+  }
+  
+  private class FieldFilterFields extends FilterFields {
+
+    public FieldFilterFields(Fields in) {
+      super(in);
+    }
+
+    @Override
+    public int size() {
+      // this information is not cheap, return -1 like MultiFields does:
+      return -1;
+    }
+
+    @Override
+    public Iterator<String> iterator() {
+      return new FilterIterator<String, String>(super.iterator()) {
+        @Override
+        protected boolean predicateFunction(String field) {
+          return hasField(field);
+        }
+      };
+    }
+
+    @Override
+    public Terms terms(String field) throws IOException {
+      return hasField(field) ? super.terms(field) : null;
+    }
+    
+  }
+  
+}
diff --git lucene/test-framework/src/java/org/apache/lucene/index/MockRandomMergePolicy.java lucene/test-framework/src/java/org/apache/lucene/index/MockRandomMergePolicy.java
index 4cc6038..85f262d 100644
--- lucene/test-framework/src/java/org/apache/lucene/index/MockRandomMergePolicy.java
+++ lucene/test-framework/src/java/org/apache/lucene/index/MockRandomMergePolicy.java
@@ -138,7 +138,7 @@ public class MockRandomMergePolicy extends MergePolicy {
   
   static class MockRandomOneMerge extends OneMerge {
     final Random r;
-    ArrayList<AtomicReader> readers;
+    ArrayList<LeafReader> readers;
 
     MockRandomOneMerge(List<SegmentCommitInfo> segments, long seed) {
       super(segments);
@@ -146,13 +146,13 @@ public class MockRandomMergePolicy extends MergePolicy {
     }
 
     @Override
-    public List<AtomicReader> getMergeReaders() throws IOException {
+    public List<LeafReader> getMergeReaders() throws IOException {
       if (readers == null) {
-        readers = new ArrayList<AtomicReader>(super.getMergeReaders());
+        readers = new ArrayList<LeafReader>(super.getMergeReaders());
         for (int i = 0; i < readers.size(); i++) {
           // wrap it (e.g. prevent bulk merge etc)
           if (r.nextInt(4) == 0) {
-            readers.set(i, new FilterAtomicReader(readers.get(i)));
+            readers.set(i, new FilterLeafReader(readers.get(i)));
           }
         }
       }
diff --git lucene/test-framework/src/java/org/apache/lucene/index/PerThreadPKLookup.java lucene/test-framework/src/java/org/apache/lucene/index/PerThreadPKLookup.java
index 8e6dd51..34d5253 100644
--- lucene/test-framework/src/java/org/apache/lucene/index/PerThreadPKLookup.java
+++ lucene/test-framework/src/java/org/apache/lucene/index/PerThreadPKLookup.java
@@ -43,12 +43,12 @@ public class PerThreadPKLookup {
 
   public PerThreadPKLookup(IndexReader r, String idFieldName) throws IOException {
 
-    List<AtomicReaderContext> leaves = new ArrayList<>(r.leaves());
+    List<LeafReaderContext> leaves = new ArrayList<>(r.leaves());
 
     // Larger segments are more likely to have the id, so we sort largest to smallest by numDocs:
-    Collections.sort(leaves, new Comparator<AtomicReaderContext>() {
+    Collections.sort(leaves, new Comparator<LeafReaderContext>() {
         @Override
-        public int compare(AtomicReaderContext c1, AtomicReaderContext c2) {
+        public int compare(LeafReaderContext c1, LeafReaderContext c2) {
           return c2.reader().numDocs() - c1.reader().numDocs();
         }
       });
diff --git lucene/test-framework/src/java/org/apache/lucene/index/ThreadedIndexingAndSearchingTestCase.java lucene/test-framework/src/java/org/apache/lucene/index/ThreadedIndexingAndSearchingTestCase.java
index 4fc43f9..d91a48a 100644
--- lucene/test-framework/src/java/org/apache/lucene/index/ThreadedIndexingAndSearchingTestCase.java
+++ lucene/test-framework/src/java/org/apache/lucene/index/ThreadedIndexingAndSearchingTestCase.java
@@ -17,7 +17,6 @@ package org.apache.lucene.index;
  * limitations under the License.
  */
 
-import java.io.File;
 import java.io.IOException;
 import java.nio.file.Path;
 import java.util.*;
@@ -340,7 +339,7 @@ public abstract class ThreadedIndexingAndSearchingTestCase extends LuceneTestCas
                   // Verify 1) IW is correctly setting
                   // diagnostics, and 2) segment warming for
                   // merged segments is actually happening:
-                  for(final AtomicReaderContext sub : s.getIndexReader().leaves()) {
+                  for(final LeafReaderContext sub : s.getIndexReader().leaves()) {
                     SegmentReader segReader = (SegmentReader) sub.reader();
                     Map<String,String> diagnostics = segReader.getSegmentInfo().info.getDiagnostics();
                     assertNotNull(diagnostics);
@@ -466,7 +465,7 @@ public abstract class ThreadedIndexingAndSearchingTestCase extends LuceneTestCas
 
     conf.setMergedSegmentWarmer(new IndexWriter.IndexReaderWarmer() {
       @Override
-      public void warm(AtomicReader reader) throws IOException {
+      public void warm(LeafReader reader) throws IOException {
         if (VERBOSE) {
           System.out.println("TEST: now warm merged reader=" + reader);
         }
diff --git lucene/test-framework/src/java/org/apache/lucene/search/AssertingCollector.java lucene/test-framework/src/java/org/apache/lucene/search/AssertingCollector.java
index 7aa8a2e..7f1ca9c 100644
--- lucene/test-framework/src/java/org/apache/lucene/search/AssertingCollector.java
+++ lucene/test-framework/src/java/org/apache/lucene/search/AssertingCollector.java
@@ -20,7 +20,7 @@ package org.apache.lucene.search;
 import java.io.IOException;
 import java.util.Random;
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 
 /** Wraps another Collector and checks that
  *  acceptsDocsOutOfOrder is respected. */
@@ -41,7 +41,7 @@ public class AssertingCollector extends FilterCollector {
   }
 
   @Override
-  public LeafCollector getLeafCollector(AtomicReaderContext context) throws IOException {
+  public LeafCollector getLeafCollector(LeafReaderContext context) throws IOException {
     return new FilterLeafCollector(super.getLeafCollector(context)) {
 
       int lastCollected = -1;
diff --git lucene/test-framework/src/java/org/apache/lucene/search/AssertingIndexSearcher.java lucene/test-framework/src/java/org/apache/lucene/search/AssertingIndexSearcher.java
index 47725db..e2eefa0 100644
--- lucene/test-framework/src/java/org/apache/lucene/search/AssertingIndexSearcher.java
+++ lucene/test-framework/src/java/org/apache/lucene/search/AssertingIndexSearcher.java
@@ -22,7 +22,7 @@ import java.util.List;
 import java.util.Random;
 import java.util.concurrent.ExecutorService;
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexReaderContext;
 import org.apache.lucene.util.TestUtil;
@@ -89,7 +89,7 @@ public class AssertingIndexSearcher extends IndexSearcher {
   }
 
   @Override
-  protected void search(List<AtomicReaderContext> leaves, Weight weight, Collector collector) throws IOException {
+  protected void search(List<LeafReaderContext> leaves, Weight weight, Collector collector) throws IOException {
     // TODO: shouldn't we AssertingCollector.wrap(collector) here?
     super.search(leaves, AssertingWeight.wrap(random, weight), collector);
   }
diff --git lucene/test-framework/src/java/org/apache/lucene/search/AssertingScorer.java lucene/test-framework/src/java/org/apache/lucene/search/AssertingScorer.java
index 705a8a0..1abda77 100644
--- lucene/test-framework/src/java/org/apache/lucene/search/AssertingScorer.java
+++ lucene/test-framework/src/java/org/apache/lucene/search/AssertingScorer.java
@@ -25,7 +25,7 @@ import java.util.Map;
 import java.util.Random;
 import java.util.WeakHashMap;
 
-import org.apache.lucene.index.AssertingAtomicReader;
+import org.apache.lucene.index.AssertingLeafReader;
 
 /** Wraps a Scorer with additional checks */
 public class AssertingScorer extends Scorer {
@@ -63,13 +63,13 @@ public class AssertingScorer extends Scorer {
 
   final Random random;
   final Scorer in;
-  final AssertingAtomicReader.AssertingDocsEnum docsEnumIn;
+  final AssertingLeafReader.AssertingDocsEnum docsEnumIn;
 
   private AssertingScorer(Random random, Scorer in) {
     super(in.weight);
     this.random = random;
     this.in = in;
-    this.docsEnumIn = new AssertingAtomicReader.AssertingDocsEnum(in);
+    this.docsEnumIn = new AssertingLeafReader.AssertingDocsEnum(in);
   }
 
   public Scorer getIn() {
diff --git lucene/test-framework/src/java/org/apache/lucene/search/AssertingWeight.java lucene/test-framework/src/java/org/apache/lucene/search/AssertingWeight.java
index b075247..eb08c2d 100644
--- lucene/test-framework/src/java/org/apache/lucene/search/AssertingWeight.java
+++ lucene/test-framework/src/java/org/apache/lucene/search/AssertingWeight.java
@@ -20,7 +20,7 @@ package org.apache.lucene.search;
 import java.io.IOException;
 import java.util.Random;
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.util.Bits;
 
 class AssertingWeight extends Weight {
@@ -40,7 +40,7 @@ class AssertingWeight extends Weight {
   }
 
   @Override
-  public Explanation explain(AtomicReaderContext context, int doc) throws IOException {
+  public Explanation explain(LeafReaderContext context, int doc) throws IOException {
     return in.explain(context, doc);
   }
 
@@ -60,7 +60,7 @@ class AssertingWeight extends Weight {
   }
 
   @Override
-  public Scorer scorer(AtomicReaderContext context, Bits acceptDocs) throws IOException {
+  public Scorer scorer(LeafReaderContext context, Bits acceptDocs) throws IOException {
     // if the caller asks for in-order scoring or if the weight does not support
     // out-of order scoring then collection will have to happen in-order.
     final Scorer inScorer = in.scorer(context, acceptDocs);
@@ -68,7 +68,7 @@ class AssertingWeight extends Weight {
   }
 
   @Override
-  public BulkScorer bulkScorer(AtomicReaderContext context, boolean scoreDocsInOrder, Bits acceptDocs) throws IOException {
+  public BulkScorer bulkScorer(LeafReaderContext context, boolean scoreDocsInOrder, Bits acceptDocs) throws IOException {
     // if the caller asks for in-order scoring or if the weight does not support
     // out-of order scoring then collection will have to happen in-order.
     BulkScorer inScorer = in.bulkScorer(context, scoreDocsInOrder, acceptDocs);
diff --git lucene/test-framework/src/java/org/apache/lucene/search/CheckHits.java lucene/test-framework/src/java/org/apache/lucene/search/CheckHits.java
index 042ad9b..a40c8df 100644
--- lucene/test-framework/src/java/org/apache/lucene/search/CheckHits.java
+++ lucene/test-framework/src/java/org/apache/lucene/search/CheckHits.java
@@ -25,7 +25,7 @@ import java.util.Random;
 
 import junit.framework.Assert;
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.util.LuceneTestCase;
 
@@ -136,7 +136,7 @@ public class CheckHits {
       bag.add(Integer.valueOf(doc + base));
     }
     @Override
-    protected void doSetNextReader(AtomicReaderContext context) throws IOException {
+    protected void doSetNextReader(LeafReaderContext context) throws IOException {
       base = context.docBase;
     }
     @Override
@@ -508,7 +508,7 @@ public class CheckHits {
                         exp.isMatch());
     }
     @Override
-    protected void doSetNextReader(AtomicReaderContext context) throws IOException {
+    protected void doSetNextReader(LeafReaderContext context) throws IOException {
       base = context.docBase;
     }
     @Override
diff --git lucene/test-framework/src/java/org/apache/lucene/search/QueryUtils.java lucene/test-framework/src/java/org/apache/lucene/search/QueryUtils.java
index 9320af9..f8ae4ca 100644
--- lucene/test-framework/src/java/org/apache/lucene/search/QueryUtils.java
+++ lucene/test-framework/src/java/org/apache/lucene/search/QueryUtils.java
@@ -26,8 +26,8 @@ import junit.framework.Assert;
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.index.AllDeletedFilterReader;
-import org.apache.lucene.index.AtomicReader;
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReader;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexWriter;
@@ -218,7 +218,7 @@ public class QueryUtils {
    */
   public static void checkSkipTo(final Query q, final IndexSearcher s) throws IOException {
     //System.out.println("Checking "+q);
-    final List<AtomicReaderContext> readerContextArray = s.getTopReaderContext().leaves();
+    final List<LeafReaderContext> readerContextArray = s.getTopReaderContext().leaves();
     if (s.createNormalizedWeight(q).scoresDocsOutOfOrder()) return;  // in this case order of skipTo() might differ from that of next().
 
     final int skip_op = 0;
@@ -244,7 +244,7 @@ public class QueryUtils {
         // FUTURE: ensure scorer.doc()==-1
 
         final float maxDiff = 1e-5f;
-        final AtomicReader lastReader[] = {null};
+        final LeafReader lastReader[] = {null};
 
         s.search(q, new SimpleCollector() {
           private Scorer sc;
@@ -263,7 +263,7 @@ public class QueryUtils {
             try {
               if (scorer == null) {
                 Weight w = s.createNormalizedWeight(q);
-                AtomicReaderContext context = readerContextArray.get(leafPtr);
+                LeafReaderContext context = readerContextArray.get(leafPtr);
                 scorer = w.scorer(context, context.reader().getLiveDocs());
               }
               
@@ -302,15 +302,15 @@ public class QueryUtils {
           }
 
           @Override
-          protected void doSetNextReader(AtomicReaderContext context) throws IOException {
+          protected void doSetNextReader(LeafReaderContext context) throws IOException {
             // confirm that skipping beyond the last doc, on the
             // previous reader, hits NO_MORE_DOCS
             if (lastReader[0] != null) {
-              final AtomicReader previousReader = lastReader[0];
+              final LeafReader previousReader = lastReader[0];
               IndexSearcher indexSearcher = LuceneTestCase.newSearcher(previousReader);
               indexSearcher.setSimilarity(s.getSimilarity());
               Weight w = indexSearcher.createNormalizedWeight(q);
-              AtomicReaderContext ctx = (AtomicReaderContext)indexSearcher.getTopReaderContext();
+              LeafReaderContext ctx = (LeafReaderContext)indexSearcher.getTopReaderContext();
               Scorer scorer = w.scorer(ctx, ctx.reader().getLiveDocs());
               if (scorer != null) {
                 boolean more = scorer.advance(lastDoc[0] + 1) != DocIdSetIterator.NO_MORE_DOCS;
@@ -333,11 +333,11 @@ public class QueryUtils {
         if (lastReader[0] != null) {
           // confirm that skipping beyond the last doc, on the
           // previous reader, hits NO_MORE_DOCS
-          final AtomicReader previousReader = lastReader[0];
+          final LeafReader previousReader = lastReader[0];
           IndexSearcher indexSearcher = LuceneTestCase.newSearcher(previousReader, false);
           indexSearcher.setSimilarity(s.getSimilarity());
           Weight w = indexSearcher.createNormalizedWeight(q);
-          AtomicReaderContext ctx = previousReader.getContext();
+          LeafReaderContext ctx = previousReader.getContext();
           Scorer scorer = w.scorer(ctx, ctx.reader().getLiveDocs());
           if (scorer != null) {
             boolean more = scorer.advance(lastDoc[0] + 1) != DocIdSetIterator.NO_MORE_DOCS;
@@ -352,8 +352,8 @@ public class QueryUtils {
     //System.out.println("checkFirstSkipTo: "+q);
     final float maxDiff = 1e-3f;
     final int lastDoc[] = {-1};
-    final AtomicReader lastReader[] = {null};
-    final List<AtomicReaderContext> context = s.getTopReaderContext().leaves();
+    final LeafReader lastReader[] = {null};
+    final List<LeafReaderContext> context = s.getTopReaderContext().leaves();
     s.search(q,new SimpleCollector() {
       private Scorer scorer;
       private int leafPtr;
@@ -389,15 +389,15 @@ public class QueryUtils {
       }
 
       @Override
-      protected void doSetNextReader(AtomicReaderContext context) throws IOException {
+      protected void doSetNextReader(LeafReaderContext context) throws IOException {
         // confirm that skipping beyond the last doc, on the
         // previous reader, hits NO_MORE_DOCS
         if (lastReader[0] != null) {
-          final AtomicReader previousReader = lastReader[0];
+          final LeafReader previousReader = lastReader[0];
           IndexSearcher indexSearcher = LuceneTestCase.newSearcher(previousReader);
           indexSearcher.setSimilarity(s.getSimilarity());
           Weight w = indexSearcher.createNormalizedWeight(q);
-          Scorer scorer = w.scorer((AtomicReaderContext)indexSearcher.getTopReaderContext(), previousReader.getLiveDocs());
+          Scorer scorer = w.scorer((LeafReaderContext)indexSearcher.getTopReaderContext(), previousReader.getLiveDocs());
           if (scorer != null) {
             boolean more = scorer.advance(lastDoc[0] + 1) != DocIdSetIterator.NO_MORE_DOCS;
             Assert.assertFalse("query's last doc was "+ lastDoc[0] +" but skipTo("+(lastDoc[0]+1)+") got to "+scorer.docID(),more);
@@ -418,11 +418,11 @@ public class QueryUtils {
     if (lastReader[0] != null) {
       // confirm that skipping beyond the last doc, on the
       // previous reader, hits NO_MORE_DOCS
-      final AtomicReader previousReader = lastReader[0];
+      final LeafReader previousReader = lastReader[0];
       IndexSearcher indexSearcher = LuceneTestCase.newSearcher(previousReader);
       indexSearcher.setSimilarity(s.getSimilarity());
       Weight w = indexSearcher.createNormalizedWeight(q);
-      Scorer scorer = w.scorer((AtomicReaderContext)indexSearcher.getTopReaderContext(), previousReader.getLiveDocs());
+      Scorer scorer = w.scorer((LeafReaderContext)indexSearcher.getTopReaderContext(), previousReader.getLiveDocs());
       if (scorer != null) {
         boolean more = scorer.advance(lastDoc[0] + 1) != DocIdSetIterator.NO_MORE_DOCS;
         Assert.assertFalse("query's last doc was "+ lastDoc[0] +" but skipTo("+(lastDoc[0]+1)+") got to "+scorer.docID(),more);
diff --git lucene/test-framework/src/java/org/apache/lucene/util/LuceneTestCase.java lucene/test-framework/src/java/org/apache/lucene/util/LuceneTestCase.java
index c9fc75f..eb179e3 100644
--- lucene/test-framework/src/java/org/apache/lucene/util/LuceneTestCase.java
+++ lucene/test-framework/src/java/org/apache/lucene/util/LuceneTestCase.java
@@ -56,59 +56,15 @@ import java.util.concurrent.atomic.AtomicReference;
 import java.util.logging.Logger;
 
 import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.codecs.Codec;
-import org.apache.lucene.codecs.SegmentInfoFormat;
-import org.apache.lucene.codecs.lucene46.Lucene46SegmentInfoFormat;
-import org.apache.lucene.codecs.simpletext.SimpleTextSegmentInfoFormat;
 import org.apache.lucene.document.Field.Store;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.FieldType;
 import org.apache.lucene.document.StringField;
 import org.apache.lucene.document.TextField;
-import org.apache.lucene.index.AlcoholicMergePolicy;
-import org.apache.lucene.index.AssertingAtomicReader;
-import org.apache.lucene.index.AssertingDirectoryReader;
-import org.apache.lucene.index.AtomicReader;
-import org.apache.lucene.index.AtomicReaderContext;
-import org.apache.lucene.index.BinaryDocValues;
-import org.apache.lucene.index.CompositeReader;
-import org.apache.lucene.index.ConcurrentMergeScheduler;
-import org.apache.lucene.index.DirectoryReader;
-import org.apache.lucene.index.DocsAndPositionsEnum;
-import org.apache.lucene.index.DocsEnum;
-import org.apache.lucene.index.FieldFilterAtomicReader;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.Fields;
+import org.apache.lucene.index.*;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.IndexReader.ReaderClosedListener;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.IndexWriter;
-import org.apache.lucene.index.IndexWriterConfig;
-import org.apache.lucene.index.LiveIndexWriterConfig;
-import org.apache.lucene.index.LogByteSizeMergePolicy;
-import org.apache.lucene.index.LogDocMergePolicy;
-import org.apache.lucene.index.LogMergePolicy;
-import org.apache.lucene.index.MergePolicy;
-import org.apache.lucene.index.MergeScheduler;
-import org.apache.lucene.index.MockRandomMergePolicy;
-import org.apache.lucene.index.MultiDocValues;
-import org.apache.lucene.index.MultiFields;
-import org.apache.lucene.index.NumericDocValues;
-import org.apache.lucene.index.ParallelAtomicReader;
-import org.apache.lucene.index.ParallelCompositeReader;
-import org.apache.lucene.index.SegmentReader;
-import org.apache.lucene.index.SerialMergeScheduler;
-import org.apache.lucene.index.SimpleMergedSegmentWarmer;
-import org.apache.lucene.index.SlowCompositeReaderWrapper;
-import org.apache.lucene.index.SortedDocValues;
-import org.apache.lucene.index.SortedNumericDocValues;
-import org.apache.lucene.index.SortedSetDocValues;
-import org.apache.lucene.index.StorableField;
-import org.apache.lucene.index.StoredDocument;
-import org.apache.lucene.index.Terms;
 import org.apache.lucene.index.TermsEnum.SeekStatus;
-import org.apache.lucene.index.TermsEnum;
-import org.apache.lucene.index.TieredMergePolicy;
 import org.apache.lucene.search.AssertingIndexSearcher;
 import org.apache.lucene.search.DocIdSetIterator;
 import org.apache.lucene.search.IndexSearcher;
@@ -750,10 +706,10 @@ public abstract class LuceneTestCase extends Assert {
    * do tests on that segment's reader. This is an utility method to help them.
    */
   public static SegmentReader getOnlySegmentReader(DirectoryReader reader) {
-    List<AtomicReaderContext> subReaders = reader.leaves();
+    List<LeafReaderContext> subReaders = reader.leaves();
     if (subReaders.size() != 1)
       throw new IllegalArgumentException(reader + " has " + subReaders.size() + " segments instead of exactly one");
-    final AtomicReader r = subReaders.get(0).reader();
+    final LeafReader r = subReaders.get(0).reader();
     assertTrue(r instanceof SegmentReader);
     return (SegmentReader) r;
   }
@@ -1468,7 +1424,7 @@ public abstract class LuceneTestCase extends Assert {
     Random random = random();
     if (rarely()) {
       // TODO: remove this, and fix those tests to wrap before putting slow around:
-      final boolean wasOriginallyAtomic = r instanceof AtomicReader;
+      final boolean wasOriginallyAtomic = r instanceof LeafReader;
       for (int i = 0, c = random.nextInt(6)+1; i < c; i++) {
         switch(random.nextInt(5)) {
           case 0:
@@ -1476,8 +1432,8 @@ public abstract class LuceneTestCase extends Assert {
             break;
           case 1:
             // will create no FC insanity in atomic case, as ParallelAtomicReader has own cache key:
-            r = (r instanceof AtomicReader) ?
-              new ParallelAtomicReader((AtomicReader) r) :
+            r = (r instanceof LeafReader) ?
+              new ParallelLeafReader((LeafReader) r) :
               new ParallelCompositeReader((CompositeReader) r);
             break;
           case 2:
@@ -1487,7 +1443,7 @@ public abstract class LuceneTestCase extends Assert {
             r = new FCInvisibleMultiReader(r);
             break;
           case 3:
-            final AtomicReader ar = SlowCompositeReaderWrapper.wrap(r);
+            final LeafReader ar = SlowCompositeReaderWrapper.wrap(r);
             final List<String> allFields = new ArrayList<>();
             for (FieldInfo fi : ar.getFieldInfos()) {
               allFields.add(fi.name);
@@ -1496,17 +1452,17 @@ public abstract class LuceneTestCase extends Assert {
             final int end = allFields.isEmpty() ? 0 : random.nextInt(allFields.size());
             final Set<String> fields = new HashSet<>(allFields.subList(0, end));
             // will create no FC insanity as ParallelAtomicReader has own cache key:
-            r = new ParallelAtomicReader(
-              new FieldFilterAtomicReader(ar, fields, false),
-              new FieldFilterAtomicReader(ar, fields, true)
+            r = new ParallelLeafReader(
+              new FieldFilterLeafReader(ar, fields, false),
+              new FieldFilterLeafReader(ar, fields, true)
             );
             break;
           case 4:
             // Häckidy-Hick-Hack: a standard Reader will cause FC insanity, so we use
             // QueryUtils' reader with a fake cache key, so insanity checker cannot walk
             // along our reader:
-            if (r instanceof AtomicReader) {
-              r = new AssertingAtomicReader((AtomicReader)r);
+            if (r instanceof LeafReader) {
+              r = new AssertingLeafReader((LeafReader)r);
             } else if (r instanceof DirectoryReader) {
               r = new AssertingDirectoryReader((DirectoryReader)r);
             }
@@ -1606,7 +1562,7 @@ public abstract class LuceneTestCase extends Assert {
       }
       // TODO: this whole check is a coverage hack, we should move it to tests for various filterreaders.
       // ultimately whatever you do will be checkIndex'd at the end anyway. 
-      if (random.nextInt(500) == 0 && r instanceof AtomicReader) {
+      if (random.nextInt(500) == 0 && r instanceof LeafReader) {
         // TODO: not useful to check DirectoryReader (redundant with checkindex)
         // but maybe sometimes run this on the other crazy readers maybeWrapReader creates?
         try {
diff --git lucene/test-framework/src/java/org/apache/lucene/util/TestUtil.java lucene/test-framework/src/java/org/apache/lucene/util/TestUtil.java
index 6664aa2..9af0b55 100644
--- lucene/test-framework/src/java/org/apache/lucene/util/TestUtil.java
+++ lucene/test-framework/src/java/org/apache/lucene/util/TestUtil.java
@@ -57,8 +57,9 @@ import org.apache.lucene.document.IntField;
 import org.apache.lucene.document.LongField;
 import org.apache.lucene.document.NumericDocValuesField;
 import org.apache.lucene.document.SortedDocValuesField;
-import org.apache.lucene.index.AtomicReader;
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.FilterLeafReader;
+import org.apache.lucene.index.LeafReader;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.CheckIndex;
 import org.apache.lucene.index.CheckIndex.Status.DocValuesStatus;
 import org.apache.lucene.index.CheckIndex.Status.FieldNormStatus;
@@ -69,7 +70,6 @@ import org.apache.lucene.index.ConcurrentMergeScheduler;
 import org.apache.lucene.index.DocsAndPositionsEnum;
 import org.apache.lucene.index.DocsEnum;
 import org.apache.lucene.index.FieldInfo.DocValuesType;
-import org.apache.lucene.index.FilterAtomicReader;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexWriter;
 import org.apache.lucene.index.IndexableField;
@@ -235,12 +235,12 @@ public final class TestUtil {
   /** This runs the CheckIndex tool on the Reader.  If any
    *  issues are hit, a RuntimeException is thrown */
   public static void checkReader(IndexReader reader) throws IOException {
-    for (AtomicReaderContext context : reader.leaves()) {
+    for (LeafReaderContext context : reader.leaves()) {
       checkReader(context.reader(), true);
     }
   }
   
-  public static void checkReader(AtomicReader reader, boolean crossCheckTermVectors) throws IOException {
+  public static void checkReader(LeafReader reader, boolean crossCheckTermVectors) throws IOException {
     ByteArrayOutputStream bos = new ByteArrayOutputStream(1024);
     PrintStream infoStream = new PrintStream(bos, false, IOUtils.UTF_8);
 
@@ -255,7 +255,7 @@ public final class TestUtil {
       System.out.println(bos.toString(IOUtils.UTF_8));
     }
     
-    AtomicReader unwrapped = FilterAtomicReader.unwrap(reader);
+    LeafReader unwrapped = FilterLeafReader.unwrap(reader);
     if (unwrapped instanceof SegmentReader) {
       SegmentReader sr = (SegmentReader) unwrapped;
       long bytesUsed = sr.ramBytesUsed(); 
diff --git solr/core/src/java/org/apache/solr/handler/admin/LukeRequestHandler.java solr/core/src/java/org/apache/solr/handler/admin/LukeRequestHandler.java
index ed4dfd1..3cf97ae 100644
--- solr/core/src/java/org/apache/solr/handler/admin/LukeRequestHandler.java
+++ solr/core/src/java/org/apache/solr/handler/admin/LukeRequestHandler.java
@@ -33,10 +33,8 @@ import org.apache.lucene.search.DocIdSetIterator;
 import org.apache.lucene.search.similarities.Similarity;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.CharsRef;
 import org.apache.lucene.util.CharsRefBuilder;
 import org.apache.lucene.util.PriorityQueue;
-import org.apache.lucene.util.UnicodeUtil;
 import org.apache.solr.analysis.TokenizerChain;
 import org.apache.solr.common.SolrException;
 import org.apache.solr.common.SolrException.ErrorCode;
@@ -315,7 +313,7 @@ public class LukeRequestHandler extends RequestHandlerBase
       fields = new TreeSet<>(Arrays.asList(fl.split( "[,\\s]+" )));
     }
 
-    AtomicReader reader = searcher.getAtomicReader();
+    LeafReader reader = searcher.getLeafReader();
     IndexSchema schema = searcher.getSchema();
 
     // Don't be tempted to put this in the loop below, the whole point here is to alphabetize the fields!
@@ -385,7 +383,7 @@ public class LukeRequestHandler extends RequestHandlerBase
   // Just get a document with the term in it, the first one will do!
   // Is there a better way to do this? Shouldn't actually be very costly
   // to do it this way.
-  private static StoredDocument getFirstLiveDoc(Terms terms, AtomicReader reader) throws IOException {
+  private static StoredDocument getFirstLiveDoc(Terms terms, LeafReader reader) throws IOException {
     DocsEnum docsEnum = null;
     TermsEnum termsEnum = terms.iterator(null);
     BytesRef text;
@@ -572,10 +570,10 @@ public class LukeRequestHandler extends RequestHandlerBase
   /** Returns the sum of RAM bytes used by each segment */
   private static long getIndexHeapUsed(DirectoryReader reader) {
     long indexHeapRamBytesUsed = 0;
-    for(AtomicReaderContext atomicReaderContext : reader.leaves()) {
-      AtomicReader atomicReader = atomicReaderContext.reader();
-      if (atomicReader instanceof SegmentReader) {
-        indexHeapRamBytesUsed += ((SegmentReader) atomicReader).ramBytesUsed();
+    for(LeafReaderContext leafReaderContext : reader.leaves()) {
+      LeafReader leafReader = leafReaderContext.reader();
+      if (leafReader instanceof SegmentReader) {
+        indexHeapRamBytesUsed += ((SegmentReader) leafReader).ramBytesUsed();
       } else {
         // Not supported for any reader that is not a SegmentReader
         return -1;
diff --git solr/core/src/java/org/apache/solr/handler/component/ExpandComponent.java solr/core/src/java/org/apache/solr/handler/component/ExpandComponent.java
index 5781036..54b9c24 100644
--- solr/core/src/java/org/apache/solr/handler/component/ExpandComponent.java
+++ solr/core/src/java/org/apache/solr/handler/component/ExpandComponent.java
@@ -23,8 +23,8 @@ import com.carrotsearch.hppc.IntOpenHashSet;
 import com.carrotsearch.hppc.cursors.IntObjectCursor;
 import com.carrotsearch.hppc.cursors.ObjectCursor;
 
-import org.apache.lucene.index.AtomicReader;
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReader;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.DocValues;
 import org.apache.lucene.index.SortedDocValues;
 import org.apache.lucene.search.Collector;
@@ -39,7 +39,6 @@ import org.apache.lucene.search.TopDocsCollector;
 import org.apache.lucene.search.TopFieldCollector;
 import org.apache.lucene.search.TopScoreDocCollector;
 import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.CharsRef;
 import org.apache.lucene.util.CharsRefBuilder;
 import org.apache.lucene.util.FixedBitSet;
 import org.apache.solr.common.SolrDocumentList;
@@ -188,7 +187,7 @@ public class ExpandComponent extends SearchComponent implements PluginInfoInitia
     }
 
     SolrIndexSearcher searcher = req.getSearcher();
-    AtomicReader reader = searcher.getAtomicReader();
+    LeafReader reader = searcher.getLeafReader();
     SortedDocValues values = DocValues.getSorted(reader, field);
     FixedBitSet groupBits = new FixedBitSet(values.getValueCount());
     DocList docList = rb.getResults().docList;
@@ -320,7 +319,7 @@ public class ExpandComponent extends SearchComponent implements PluginInfoInitia
       this.docValues = docValues;
     }
 
-    public LeafCollector getLeafCollector(AtomicReaderContext context) throws IOException {
+    public LeafCollector getLeafCollector(LeafReaderContext context) throws IOException {
       final int docBase = context.docBase;
       final IntObjectMap<LeafCollector> leafCollectors = new IntObjectOpenHashMap<>();
       for (IntObjectCursor<Collector> entry : groups) {
diff --git solr/core/src/java/org/apache/solr/handler/component/FieldFacetStats.java solr/core/src/java/org/apache/solr/handler/component/FieldFacetStats.java
index 4e6294b..6c3c94a 100644
--- solr/core/src/java/org/apache/solr/handler/component/FieldFacetStats.java
+++ solr/core/src/java/org/apache/solr/handler/component/FieldFacetStats.java
@@ -23,8 +23,8 @@ import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
 
-import org.apache.lucene.index.AtomicReader;
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReader;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.DocValues;
 import org.apache.lucene.index.SortedDocValues;
 import org.apache.lucene.queries.function.FunctionValues;
@@ -54,10 +54,10 @@ public class FieldFacetStats {
   private final Map<Integer, Integer> missingStats;
   List<HashMap<String, Integer>> facetStatsTerms;
 
-  final AtomicReader topLevelReader;
-  AtomicReaderContext leave;
+  final LeafReader topLevelReader;
+  LeafReaderContext leave;
   final ValueSource valueSource;
-  AtomicReaderContext context;
+  LeafReaderContext context;
   FunctionValues values;
 
   SortedDocValues topLevelSortedValues = null;
@@ -68,7 +68,7 @@ public class FieldFacetStats {
     this.facet_sf = facet_sf;
     this.calcDistinct = calcDistinct;
 
-    topLevelReader = searcher.getAtomicReader();
+    topLevelReader = searcher.getLeafReader();
     valueSource = facet_sf.getType().getValueSource(facet_sf, null);
 
     facetStatsValues = new HashMap<>();
@@ -153,7 +153,7 @@ public class FieldFacetStats {
     return true;
   }
 
-  public void setNextReader(AtomicReaderContext ctx) throws IOException {
+  public void setNextReader(LeafReaderContext ctx) throws IOException {
     this.context = ctx;
     values = valueSource.getValues(Collections.emptyMap(), ctx);
     for (StatsValues stats : facetStatsValues.values()) {
diff --git solr/core/src/java/org/apache/solr/handler/component/QueryComponent.java solr/core/src/java/org/apache/solr/handler/component/QueryComponent.java
index 45d5731..721a46a 100644
--- solr/core/src/java/org/apache/solr/handler/component/QueryComponent.java
+++ solr/core/src/java/org/apache/solr/handler/component/QueryComponent.java
@@ -32,8 +32,7 @@ import java.util.Locale;
 import java.util.Map;
 import java.util.regex.Pattern;
 
-import org.apache.commons.lang.StringUtils;
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.IndexReaderContext;
 import org.apache.lucene.index.ReaderUtil;
 import org.apache.lucene.index.Term;
@@ -100,7 +99,6 @@ import org.apache.solr.search.grouping.endresulttransformer.MainEndResultTransfo
 import org.apache.solr.search.grouping.endresulttransformer.SimpleEndResultTransformer;
 import org.apache.solr.util.SolrPluginUtils;
 import java.util.Collections;
-import java.util.Comparator;
 
 /**
  * TODO!
@@ -520,8 +518,8 @@ public class QueryComponent extends SearchComponent
     if(fsv){
       NamedList<Object[]> sortVals = new NamedList<>(); // order is important for the sort fields
       IndexReaderContext topReaderContext = searcher.getTopReaderContext();
-      List<AtomicReaderContext> leaves = topReaderContext.leaves();
-      AtomicReaderContext currentLeaf = null;
+      List<LeafReaderContext> leaves = topReaderContext.leaves();
+      LeafReaderContext currentLeaf = null;
       if (leaves.size()==1) {
         // if there is a single segment, use that subReader and avoid looking up each time
         currentLeaf = leaves.get(0);
diff --git solr/core/src/java/org/apache/solr/handler/component/QueryElevationComponent.java solr/core/src/java/org/apache/solr/handler/component/QueryElevationComponent.java
index 246b9c7..1c0ca43 100644
--- solr/core/src/java/org/apache/solr/handler/component/QueryElevationComponent.java
+++ solr/core/src/java/org/apache/solr/handler/component/QueryElevationComponent.java
@@ -17,14 +17,13 @@
 
 package org.apache.solr.handler.component;
 
-import com.carrotsearch.hppc.IntOpenHashSet;
 import com.carrotsearch.hppc.IntIntOpenHashMap;
 
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
-import org.apache.lucene.index.AtomicReader;
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReader;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.DocsEnum;
 import org.apache.lucene.index.Fields;
 import org.apache.lucene.index.IndexReader;
@@ -559,11 +558,11 @@ public class QueryElevationComponent extends SearchComponent implements SolrCore
 
       boostDocs = new IntIntOpenHashMap(boosted.size()*2);
 
-      List<AtomicReaderContext>leaves = indexSearcher.getTopReaderContext().leaves();
+      List<LeafReaderContext>leaves = indexSearcher.getTopReaderContext().leaves();
       TermsEnum termsEnum = null;
       DocsEnum docsEnum = null;
-      for(AtomicReaderContext leaf : leaves) {
-        AtomicReader reader = leaf.reader();
+      for(LeafReaderContext leaf : leaves) {
+        LeafReader reader = leaf.reader();
         int docBase = leaf.docBase;
         Bits liveDocs = reader.getLiveDocs();
         Terms terms = reader.terms(fieldName);
@@ -676,7 +675,7 @@ public class QueryElevationComponent extends SearchComponent implements SolrCore
       }
 
       @Override
-      public FieldComparator setNextReader(AtomicReaderContext context) throws IOException {
+      public FieldComparator setNextReader(LeafReaderContext context) throws IOException {
         //convert the ids to Lucene doc ids, the ordSet and termValues needs to be the same size as the number of elevation docs we have
         ordSet.clear();
         Fields fields = context.reader().fields();
diff --git solr/core/src/java/org/apache/solr/handler/component/StatsComponent.java solr/core/src/java/org/apache/solr/handler/component/StatsComponent.java
index 0860589..d9aaf79 100644
--- solr/core/src/java/org/apache/solr/handler/component/StatsComponent.java
+++ solr/core/src/java/org/apache/solr/handler/component/StatsComponent.java
@@ -27,8 +27,8 @@ import java.util.Iterator;
 import java.util.List;
 import java.util.Map;
 
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.search.*;
-import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.solr.common.SolrException;
 import org.apache.solr.common.SolrException.ErrorCode;
 import org.apache.solr.common.params.CommonParams;
@@ -40,8 +40,6 @@ import org.apache.solr.common.util.NamedList;
 import org.apache.solr.common.util.SimpleOrderedMap;
 import org.apache.solr.common.util.StrUtils;
 import org.apache.solr.request.DocValuesStats;
-import org.apache.solr.request.SolrQueryRequest;
-import org.apache.solr.schema.FieldType;
 import org.apache.solr.schema.IndexSchema;
 import org.apache.solr.schema.SchemaField;
 import org.apache.solr.search.DocIterator;
@@ -389,8 +387,8 @@ class StatsField {
       facetStats.add(new FieldFacetStats(searcher, facetField, sf, fsf, calcDistinct));
     }
 
-    final Iterator<AtomicReaderContext> ctxIt = searcher.getIndexReader().leaves().iterator();
-    AtomicReaderContext ctx = null;
+    final Iterator<LeafReaderContext> ctxIt = searcher.getIndexReader().leaves().iterator();
+    LeafReaderContext ctx = null;
     for (DocIterator docsIt = base.iterator(); docsIt.hasNext(); ) {
       final int doc = docsIt.nextDoc();
       if (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc()) {
diff --git solr/core/src/java/org/apache/solr/handler/component/StatsValues.java solr/core/src/java/org/apache/solr/handler/component/StatsValues.java
index cbcde04..87e2d2a 100644
--- solr/core/src/java/org/apache/solr/handler/component/StatsValues.java
+++ solr/core/src/java/org/apache/solr/handler/component/StatsValues.java
@@ -22,11 +22,9 @@ package org.apache.solr.handler.component;
 import java.io.IOException;
 import java.util.Map;
 
-import org.apache.lucene.index.AtomicReaderContext;
-import org.apache.lucene.queries.function.FunctionValues;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.util.BytesRef;
 import org.apache.solr.common.util.NamedList;
-import org.apache.solr.schema.FieldType;
 
 /**
  * StatsValue defines the interface for the collection of statistical values about fields and facets.
@@ -42,7 +40,7 @@ public interface StatsValues {
   void accumulate(NamedList stv);
 
   /** Accumulate the value associated with <code>docID</code>.
-   *  @see #setNextReader(AtomicReaderContext) */
+   *  @see #setNextReader(org.apache.lucene.index.LeafReaderContext) */
   void accumulate(int docID);
 
   /**
@@ -81,5 +79,5 @@ public interface StatsValues {
   NamedList<?> getStatsValues();
 
   /** Set the context for {@link #accumulate(int)}. */
-  void setNextReader(AtomicReaderContext ctx) throws IOException;
+  void setNextReader(LeafReaderContext ctx) throws IOException;
 }
diff --git solr/core/src/java/org/apache/solr/handler/component/StatsValuesFactory.java solr/core/src/java/org/apache/solr/handler/component/StatsValuesFactory.java
index c18624d..7a64fcf 100644
--- solr/core/src/java/org/apache/solr/handler/component/StatsValuesFactory.java
+++ solr/core/src/java/org/apache/solr/handler/component/StatsValuesFactory.java
@@ -20,7 +20,7 @@ package org.apache.solr.handler.component;
 import java.io.IOException;
 import java.util.*;
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.util.BytesRef;
@@ -203,7 +203,7 @@ abstract class AbstractStatsValues<T> implements StatsValues {
     return res;
   }
 
-  public void setNextReader(AtomicReaderContext ctx) throws IOException {
+  public void setNextReader(LeafReaderContext ctx) throws IOException {
     if (valueSource == null) {
       valueSource = ft.getValueSource(sf, null);
     }
diff --git solr/core/src/java/org/apache/solr/handler/component/TermsComponent.java solr/core/src/java/org/apache/solr/handler/component/TermsComponent.java
index ae6c927..8992b72 100644
--- solr/core/src/java/org/apache/solr/handler/component/TermsComponent.java
+++ solr/core/src/java/org/apache/solr/handler/component/TermsComponent.java
@@ -19,7 +19,6 @@ package org.apache.solr.handler.component;
 import org.apache.lucene.index.*;
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.BytesRefBuilder;
-import org.apache.lucene.util.CharsRef;
 import org.apache.lucene.util.CharsRefBuilder;
 import org.apache.lucene.util.StringHelper;
 import org.apache.solr.common.SolrException;
@@ -118,7 +117,7 @@ public class TermsComponent extends SearchComponent {
     boolean raw = params.getBool(TermsParams.TERMS_RAW, false);
 
 
-    final AtomicReader indexReader = rb.req.getSearcher().getAtomicReader();
+    final LeafReader indexReader = rb.req.getSearcher().getLeafReader();
     Fields lfields = indexReader.fields();
 
     for (String field : fields) {
diff --git solr/core/src/java/org/apache/solr/request/DocValuesFacets.java solr/core/src/java/org/apache/solr/request/DocValuesFacets.java
index 2a70e85..9985671 100644
--- solr/core/src/java/org/apache/solr/request/DocValuesFacets.java
+++ solr/core/src/java/org/apache/solr/request/DocValuesFacets.java
@@ -20,7 +20,7 @@ package org.apache.solr.request;
 import java.io.IOException;
 import java.util.List;
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.DocValues;
 import org.apache.lucene.index.MultiDocValues.MultiSortedDocValues;
 import org.apache.lucene.index.MultiDocValues.MultiSortedSetDocValues;
@@ -32,7 +32,6 @@ import org.apache.lucene.search.DocIdSetIterator;
 import org.apache.lucene.search.Filter;
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.BytesRefBuilder;
-import org.apache.lucene.util.CharsRef;
 import org.apache.lucene.util.CharsRefBuilder;
 import org.apache.lucene.util.LongValues;
 import org.apache.lucene.util.UnicodeUtil;
@@ -70,12 +69,12 @@ public class DocValuesFacets {
     final SortedSetDocValues si; // for term lookups only
     OrdinalMap ordinalMap = null; // for mapping per-segment ords to global ones
     if (multiValued) {
-      si = searcher.getAtomicReader().getSortedSetDocValues(fieldName);
+      si = searcher.getLeafReader().getSortedSetDocValues(fieldName);
       if (si instanceof MultiSortedSetDocValues) {
         ordinalMap = ((MultiSortedSetDocValues)si).mapping;
       }
     } else {
-      SortedDocValues single = searcher.getAtomicReader().getSortedDocValues(fieldName);
+      SortedDocValues single = searcher.getLeafReader().getSortedDocValues(fieldName);
       si = single == null ? null : DocValues.singleton(single);
       if (single instanceof MultiSortedDocValues) {
         ordinalMap = ((MultiSortedDocValues)single).mapping;
@@ -122,9 +121,9 @@ public class DocValuesFacets {
       final int[] counts = new int[nTerms];
 
       Filter filter = docs.getTopFilter();
-      List<AtomicReaderContext> leaves = searcher.getTopReaderContext().leaves();
+      List<LeafReaderContext> leaves = searcher.getTopReaderContext().leaves();
       for (int subIndex = 0; subIndex < leaves.size(); subIndex++) {
-        AtomicReaderContext leaf = leaves.get(subIndex);
+        LeafReaderContext leaf = leaves.get(subIndex);
         DocIdSet dis = filter.getDocIdSet(leaf, null); // solr docsets already exclude any deleted docs
         DocIdSetIterator disi = null;
         if (dis != null) {
diff --git solr/core/src/java/org/apache/solr/request/DocValuesStats.java solr/core/src/java/org/apache/solr/request/DocValuesStats.java
index 4b2dc6c..9bf02c3 100644
--- solr/core/src/java/org/apache/solr/request/DocValuesStats.java
+++ solr/core/src/java/org/apache/solr/request/DocValuesStats.java
@@ -21,7 +21,7 @@ import java.io.IOException;
 import java.util.List;
 import java.util.Map;
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.DocValues;
 import org.apache.lucene.index.MultiDocValues.MultiSortedDocValues;
 import org.apache.lucene.index.MultiDocValues.MultiSortedSetDocValues;
@@ -77,13 +77,13 @@ public class DocValuesStats {
     SortedSetDocValues si; // for term lookups only
     OrdinalMap ordinalMap = null; // for mapping per-segment ords to global ones
     if (multiValued) {
-      si = searcher.getAtomicReader().getSortedSetDocValues(fieldName);
+      si = searcher.getLeafReader().getSortedSetDocValues(fieldName);
       
       if (si instanceof MultiSortedSetDocValues) {
         ordinalMap = ((MultiSortedSetDocValues)si).mapping;
       }
     } else {
-      SortedDocValues single = searcher.getAtomicReader().getSortedDocValues(fieldName);
+      SortedDocValues single = searcher.getLeafReader().getSortedDocValues(fieldName);
       si = single == null ? null : DocValues.singleton(single);
       if (single instanceof MultiSortedDocValues) {
         ordinalMap = ((MultiSortedDocValues)single).mapping;
@@ -103,10 +103,10 @@ public class DocValuesStats {
     final int[] counts = new int[nTerms];
     
     Filter filter = docs.getTopFilter();
-    List<AtomicReaderContext> leaves = searcher.getTopReaderContext().leaves();
+    List<LeafReaderContext> leaves = searcher.getTopReaderContext().leaves();
     
     for (int subIndex = 0; subIndex < leaves.size(); subIndex++) {
-      AtomicReaderContext leaf = leaves.get(subIndex);
+      LeafReaderContext leaf = leaves.get(subIndex);
       DocIdSet dis = filter.getDocIdSet(leaf, null); // solr docsets already exclude any deleted docs
       DocIdSetIterator disi = null;
       
diff --git solr/core/src/java/org/apache/solr/request/IntervalFacets.java solr/core/src/java/org/apache/solr/request/IntervalFacets.java
index b928141..90c8381 100644
--- solr/core/src/java/org/apache/solr/request/IntervalFacets.java
+++ solr/core/src/java/org/apache/solr/request/IntervalFacets.java
@@ -10,7 +10,7 @@ import java.util.List;
 import java.util.Locale;
 
 import org.apache.lucene.document.FieldType.NumericType;
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.DocValues;
 import org.apache.lucene.index.NumericDocValues;
 import org.apache.lucene.index.SortedDocValues;
@@ -162,10 +162,10 @@ public class IntervalFacets implements Iterable<FacetInterval> {
     if (numericType == null) {
       throw new IllegalStateException();
     }
-    final List<AtomicReaderContext> leaves = searcher.getIndexReader().leaves();
+    final List<LeafReaderContext> leaves = searcher.getIndexReader().leaves();
 
-    final Iterator<AtomicReaderContext> ctxIt = leaves.iterator();
-    AtomicReaderContext ctx = null;
+    final Iterator<LeafReaderContext> ctxIt = leaves.iterator();
+    LeafReaderContext ctx = null;
     NumericDocValues longs = null;
     Bits docsWithField = null;
     for (DocIterator docsIt = docs.iterator(); docsIt.hasNext(); ) {
@@ -220,9 +220,9 @@ public class IntervalFacets implements Iterable<FacetInterval> {
 
   private void getCountString() throws IOException {
     Filter filter = docs.getTopFilter();
-    List<AtomicReaderContext> leaves = searcher.getTopReaderContext().leaves();
+    List<LeafReaderContext> leaves = searcher.getTopReaderContext().leaves();
     for (int subIndex = 0; subIndex < leaves.size(); subIndex++) {
-      AtomicReaderContext leaf = leaves.get(subIndex);
+      LeafReaderContext leaf = leaves.get(subIndex);
       DocIdSet dis = filter.getDocIdSet(leaf, null); // solr docsets already exclude any deleted docs
       if (dis == null) {
         continue;
diff --git solr/core/src/java/org/apache/solr/request/NumericFacets.java solr/core/src/java/org/apache/solr/request/NumericFacets.java
index bf07e1c..3b43674 100644
--- solr/core/src/java/org/apache/solr/request/NumericFacets.java
+++ solr/core/src/java/org/apache/solr/request/NumericFacets.java
@@ -29,7 +29,7 @@ import java.util.Map;
 import java.util.Set;
 
 import org.apache.lucene.document.FieldType.NumericType;
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.DocValues;
 import org.apache.lucene.index.NumericDocValues;
 import org.apache.lucene.index.ReaderUtil;
@@ -39,7 +39,6 @@ import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.util.Bits;
 import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.CharsRef;
 import org.apache.lucene.util.CharsRefBuilder;
 import org.apache.lucene.util.PriorityQueue;
 import org.apache.lucene.util.StringHelper;
@@ -139,12 +138,12 @@ final class NumericFacets {
     if (numericType == null) {
       throw new IllegalStateException();
     }
-    final List<AtomicReaderContext> leaves = searcher.getIndexReader().leaves();
+    final List<LeafReaderContext> leaves = searcher.getIndexReader().leaves();
 
     // 1. accumulate
     final HashTable hashTable = new HashTable();
-    final Iterator<AtomicReaderContext> ctxIt = leaves.iterator();
-    AtomicReaderContext ctx = null;
+    final Iterator<LeafReaderContext> ctxIt = leaves.iterator();
+    LeafReaderContext ctx = null;
     NumericDocValues longs = null;
     Bits docsWithField = null;
     int missingCount = 0;
@@ -269,7 +268,7 @@ final class NumericFacets {
         for (int i = 0; i < result.size(); ++i) {
           alreadySeen.add(result.getName(i));
         }
-        final Terms terms = searcher.getAtomicReader().terms(fieldName);
+        final Terms terms = searcher.getLeafReader().terms(fieldName);
         if (terms != null) {
           final String prefixStr = TrieField.getMainValuePrefix(ft);
           final BytesRef prefix;
@@ -322,7 +321,7 @@ final class NumericFacets {
         final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));
         counts.put(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);
       }
-      final Terms terms = searcher.getAtomicReader().terms(fieldName);
+      final Terms terms = searcher.getLeafReader().terms(fieldName);
       if (terms != null) {
         final String prefixStr = TrieField.getMainValuePrefix(ft);
         final BytesRef prefix;
diff --git solr/core/src/java/org/apache/solr/request/PerSegmentSingleValuedFaceting.java solr/core/src/java/org/apache/solr/request/PerSegmentSingleValuedFaceting.java
index dcd565d..beccaf8 100644
--- solr/core/src/java/org/apache/solr/request/PerSegmentSingleValuedFaceting.java
+++ solr/core/src/java/org/apache/solr/request/PerSegmentSingleValuedFaceting.java
@@ -21,7 +21,7 @@ import java.io.IOException;
 import java.util.*;
 import java.util.concurrent.*;
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.DocValues;
 import org.apache.lucene.index.SortedDocValues;
 import org.apache.lucene.index.TermsEnum;
@@ -30,11 +30,9 @@ import org.apache.lucene.search.DocIdSetIterator;
 import org.apache.lucene.search.Filter;
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.BytesRefBuilder;
-import org.apache.lucene.util.CharsRef;
 import org.apache.lucene.util.CharsRefBuilder;
 import org.apache.lucene.util.PriorityQueue;
 import org.apache.lucene.util.UnicodeUtil;
-import org.apache.lucene.util.packed.PackedInts;
 import org.apache.solr.common.SolrException;
 import org.apache.solr.common.params.FacetParams;
 import org.apache.solr.common.util.NamedList;
@@ -85,7 +83,7 @@ class PerSegmentSingleValuedFaceting {
     // reuse the translation logic to go from top level set to per-segment set
     baseSet = docs.getTopFilter();
 
-    final List<AtomicReaderContext> leaves = searcher.getTopReaderContext().leaves();
+    final List<LeafReaderContext> leaves = searcher.getTopReaderContext().leaves();
     // The list of pending tasks that aren't immediately submitted
     // TODO: Is there a completion service, or a delegating executor that can
     // limit the number of concurrent tasks submitted to a bigger executor?
@@ -93,7 +91,7 @@ class PerSegmentSingleValuedFaceting {
 
     int threads = nThreads <= 0 ? Integer.MAX_VALUE : nThreads;
 
-    for (final AtomicReaderContext leave : leaves) {
+    for (final LeafReaderContext leave : leaves) {
       final SegFacet segFacet = new SegFacet(leave);
 
       Callable<SegFacet> task = new Callable<SegFacet>() {
@@ -222,8 +220,8 @@ class PerSegmentSingleValuedFaceting {
   }
 
   class SegFacet {
-    AtomicReaderContext context;
-    SegFacet(AtomicReaderContext context) {
+    LeafReaderContext context;
+    SegFacet(LeafReaderContext context) {
       this.context = context;
     }
     
diff --git solr/core/src/java/org/apache/solr/request/SimpleFacets.java solr/core/src/java/org/apache/solr/request/SimpleFacets.java
index 4c19f21..462ca1a 100644
--- solr/core/src/java/org/apache/solr/request/SimpleFacets.java
+++ solr/core/src/java/org/apache/solr/request/SimpleFacets.java
@@ -37,8 +37,8 @@ import java.util.concurrent.SynchronousQueue;
 import java.util.concurrent.ThreadPoolExecutor;
 import java.util.concurrent.TimeUnit;
 
-import org.apache.lucene.index.AtomicReader;
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReader;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.DocsEnum;
 import org.apache.lucene.index.Fields;
 import org.apache.lucene.index.MultiDocsEnum;
@@ -57,7 +57,6 @@ import org.apache.lucene.search.grouping.AbstractAllGroupHeadsCollector;
 import org.apache.lucene.search.grouping.term.TermAllGroupsCollector;
 import org.apache.lucene.search.grouping.term.TermGroupFacetCollector;
 import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.CharsRef;
 import org.apache.lucene.util.CharsRefBuilder;
 import org.apache.lucene.util.StringHelper;
 import org.apache.solr.common.SolrException;
@@ -496,8 +495,8 @@ public class SimpleFacets {
       // there isnt a GroupedFacetCollector that works on numerics right now...
       searcher.search(new MatchAllDocsQuery(), base.getTopFilter(), new FilterCollector(collector) {
         @Override
-        public LeafCollector getLeafCollector(AtomicReaderContext context) throws IOException {
-          AtomicReader insane = Insanity.wrapInsanity(context.reader(), groupField);
+        public LeafCollector getLeafCollector(LeafReaderContext context) throws IOException {
+          LeafReader insane = Insanity.wrapInsanity(context.reader(), groupField);
           return in.getLeafCollector(insane.getContext());
         }
       });
@@ -701,7 +700,7 @@ public class SimpleFacets {
 
 
     IndexSchema schema = searcher.getSchema();
-    AtomicReader r = searcher.getAtomicReader();
+    LeafReader r = searcher.getLeafReader();
     FieldType ft = schema.getFieldType(field);
 
     boolean sortByCount = sort.equals("count") || sort.equals("true");
diff --git solr/core/src/java/org/apache/solr/request/UnInvertedField.java solr/core/src/java/org/apache/solr/request/UnInvertedField.java
index 3e771e5..4486bd07 100644
--- solr/core/src/java/org/apache/solr/request/UnInvertedField.java
+++ solr/core/src/java/org/apache/solr/request/UnInvertedField.java
@@ -22,7 +22,7 @@ import java.util.LinkedHashMap;
 import java.util.Map;
 import java.util.concurrent.atomic.AtomicLong;
 
-import org.apache.lucene.index.AtomicReader;
+import org.apache.lucene.index.LeafReader;
 import org.apache.lucene.index.SortedDocValues;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.index.TermsEnum;
@@ -31,7 +31,6 @@ import org.apache.lucene.search.TermRangeQuery;
 import org.apache.lucene.uninverting.DocTermOrds;
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.BytesRefBuilder;
-import org.apache.lucene.util.CharsRef;
 import org.apache.lucene.util.CharsRefBuilder;
 import org.apache.lucene.util.FixedBitSet;
 import org.apache.lucene.util.UnicodeUtil;
@@ -136,7 +135,7 @@ public class UnInvertedField extends DocTermOrds {
       if (deState == null) {
         deState = new SolrIndexSearcher.DocsEnumState();
         deState.fieldName = field;
-        deState.liveDocs = searcher.getAtomicReader().getLiveDocs();
+        deState.liveDocs = searcher.getLeafReader().getLiveDocs();
         deState.termsEnum = te;  // TODO: check for MultiTermsEnum in SolrIndexSearcher could now fail?
         deState.docsEnum = docsEnum;
         deState.minSetSizeCached = maxTermDocFreq;
@@ -186,7 +185,7 @@ public class UnInvertedField extends DocTermOrds {
     final String prefix = TrieField.getMainValuePrefix(searcher.getSchema().getFieldType(field));
     this.searcher = searcher;
     try {
-      AtomicReader r = searcher.getAtomicReader();
+      LeafReader r = searcher.getLeafReader();
       uninvert(r, r.getLiveDocs(), prefix == null ? null : new BytesRef(prefix));
     } catch (IllegalStateException ise) {
       throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, ise.getMessage());
@@ -239,7 +238,7 @@ public class UnInvertedField extends DocTermOrds {
       int startTerm = 0;
       int endTerm = numTermsInField;  // one past the end
 
-      TermsEnum te = getOrdTermsEnum(searcher.getAtomicReader());
+      TermsEnum te = getOrdTermsEnum(searcher.getLeafReader());
       if (te != null && prefix != null && prefix.length() > 0) {
         final BytesRefBuilder prefixBr = new BytesRefBuilder();
         prefixBr.copyChars(prefix);
@@ -505,7 +504,7 @@ public class UnInvertedField extends DocTermOrds {
     final int[] index = this.index;
     final int[] counts = new int[numTermsInField];//keep track of the number of times we see each word in the field for all the documents in the docset
 
-    TermsEnum te = getOrdTermsEnum(searcher.getAtomicReader());
+    TermsEnum te = getOrdTermsEnum(searcher.getLeafReader());
 
     boolean doNegative = false;
     if (finfo.length == 0) {
diff --git solr/core/src/java/org/apache/solr/response/SortingResponseWriter.java solr/core/src/java/org/apache/solr/response/SortingResponseWriter.java
index d248989..ee3f143 100644
--- solr/core/src/java/org/apache/solr/response/SortingResponseWriter.java
+++ solr/core/src/java/org/apache/solr/response/SortingResponseWriter.java
@@ -17,8 +17,8 @@
     
 package org.apache.solr.response;
 
-import org.apache.lucene.index.AtomicReaderContext;
-import org.apache.lucene.index.AtomicReader;
+import org.apache.lucene.index.LeafReaderContext;
+import org.apache.lucene.index.LeafReader;
 import org.apache.lucene.index.MultiDocValues;
 import org.apache.lucene.index.SortedDocValues;
 import org.apache.lucene.index.NumericDocValues;
@@ -28,7 +28,6 @@ import org.apache.lucene.search.Sort;
 import org.apache.lucene.search.SortField;
 import org.apache.lucene.util.ArrayUtil;
 import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.CharsRef;
 import org.apache.lucene.util.CharsRefBuilder;
 import org.apache.lucene.util.LongValues;
 import org.apache.lucene.util.FixedBitSet;
@@ -53,7 +52,6 @@ import org.slf4j.LoggerFactory;
 import java.io.IOException;
 import java.io.Writer;
 import java.io.PrintWriter;
-import java.net.SocketException;
 import java.util.List;
 
 
@@ -122,7 +120,7 @@ public class SortingResponseWriter implements QueryResponseWriter {
     writer.write("{\"numFound\":"+totalHits+", \"docs\":[");
 
     //Write the data.
-    List<AtomicReaderContext> leaves = req.getSearcher().getTopReaderContext().leaves();
+    List<LeafReaderContext> leaves = req.getSearcher().getTopReaderContext().leaves();
     SortDoc sortDoc = getSortDoc(req.getSearcher(), sort.getSort());
     int count = 0;
     int queueSize = 30000;
@@ -208,7 +206,7 @@ public class SortingResponseWriter implements QueryResponseWriter {
 
 
   protected void writeDoc(SortDoc sortDoc,
-                          List<AtomicReaderContext> leaves,
+                          List<LeafReaderContext> leaves,
                           FieldWriter[] fieldWriters,
                           FixedBitSet[] sets,
                           Writer out) throws IOException{
@@ -216,7 +214,7 @@ public class SortingResponseWriter implements QueryResponseWriter {
     int ord = sortDoc.ord;
     FixedBitSet set = sets[ord];
     set.clear(sortDoc.docId);
-    AtomicReaderContext context = leaves.get(ord);
+    LeafReaderContext context = leaves.get(ord);
     boolean needsComma = false;
     for(FieldWriter fieldWriter : fieldWriters) {
       if(needsComma) {
@@ -316,7 +314,7 @@ public class SortingResponseWriter implements QueryResponseWriter {
           sortValues[i] = new LongValue(field, new LongAsc());
         }
       } else if(ft instanceof StrField) {
-        AtomicReader reader = searcher.getAtomicReader();
+        LeafReader reader = searcher.getLeafReader();
         SortedDocValues vals =  reader.getSortedDocValues(field);
         if(reverse) {
           sortValues[i] = new StringValue(vals, field, new IntDesc());
@@ -387,7 +385,7 @@ public class SortingResponseWriter implements QueryResponseWriter {
 
     }
 
-    public void setNextReader(AtomicReaderContext context) throws IOException {
+    public void setNextReader(LeafReaderContext context) throws IOException {
       this.ord = context.ord;
       for(SortValue value : sortValues) {
         value.setNextReader(context);
@@ -454,7 +452,7 @@ public class SortingResponseWriter implements QueryResponseWriter {
 
     protected SortValue value1;
 
-    public void setNextReader(AtomicReaderContext context) throws IOException {
+    public void setNextReader(LeafReaderContext context) throws IOException {
       this.ord = context.ord;
       value1.setNextReader(context);
     }
@@ -510,7 +508,7 @@ public class SortingResponseWriter implements QueryResponseWriter {
 
     protected SortValue value2;
 
-    public void setNextReader(AtomicReaderContext context) throws IOException {
+    public void setNextReader(LeafReaderContext context) throws IOException {
       this.ord = context.ord;
       value1.setNextReader(context);
       value2.setNextReader(context);
@@ -578,7 +576,7 @@ public class SortingResponseWriter implements QueryResponseWriter {
 
     protected SortValue value3;
 
-    public void setNextReader(AtomicReaderContext context) throws IOException {
+    public void setNextReader(LeafReaderContext context) throws IOException {
       this.ord = context.ord;
       value1.setNextReader(context);
       value2.setNextReader(context);
@@ -664,7 +662,7 @@ public class SortingResponseWriter implements QueryResponseWriter {
 
     protected SortValue value4;
 
-    public void setNextReader(AtomicReaderContext context) throws IOException {
+    public void setNextReader(LeafReaderContext context) throws IOException {
       this.ord = context.ord;
       value1.setNextReader(context);
       value2.setNextReader(context);
@@ -763,7 +761,7 @@ public class SortingResponseWriter implements QueryResponseWriter {
 
   public interface SortValue extends Comparable<SortValue> {
     public void setCurrentValue(int docId) throws IOException;
-    public void setNextReader(AtomicReaderContext context) throws IOException;
+    public void setNextReader(LeafReaderContext context) throws IOException;
     public void setCurrentValue(SortValue value);
     public void reset();
     public SortValue copy();
@@ -786,7 +784,7 @@ public class SortingResponseWriter implements QueryResponseWriter {
       this.currentValue = comp.resetValue();
     }
 
-    public void setNextReader(AtomicReaderContext context) throws IOException {
+    public void setNextReader(LeafReaderContext context) throws IOException {
       this.vals = context.reader().getNumericDocValues(field);
     }
 
@@ -864,7 +862,7 @@ public class SortingResponseWriter implements QueryResponseWriter {
       return new LongValue(field, comp);
     }
 
-    public void setNextReader(AtomicReaderContext context) throws IOException {
+    public void setNextReader(LeafReaderContext context) throws IOException {
       this.vals = context.reader().getNumericDocValues(field);
     }
 
@@ -943,7 +941,7 @@ public class SortingResponseWriter implements QueryResponseWriter {
       return new FloatValue(field, comp);
     }
 
-    public void setNextReader(AtomicReaderContext context) throws IOException {
+    public void setNextReader(LeafReaderContext context) throws IOException {
       this.vals = context.reader().getNumericDocValues(field);
     }
 
@@ -1020,7 +1018,7 @@ public class SortingResponseWriter implements QueryResponseWriter {
       return new DoubleValue(field, comp);
     }
 
-    public void setNextReader(AtomicReaderContext context) throws IOException {
+    public void setNextReader(LeafReaderContext context) throws IOException {
       this.vals = context.reader().getNumericDocValues(field);
     }
 
@@ -1127,7 +1125,7 @@ public class SortingResponseWriter implements QueryResponseWriter {
       this.currentOrd = v.currentOrd;
     }
 
-    public void setNextReader(AtomicReaderContext context) {
+    public void setNextReader(LeafReaderContext context) {
       segment = context.ord;
       if(ordinalMap != null) {
         globalOrds = ordinalMap.getGlobalOrds(segment);
@@ -1152,7 +1150,7 @@ public class SortingResponseWriter implements QueryResponseWriter {
   }
 
   protected abstract class FieldWriter {
-    public abstract void write(int docId, AtomicReader reader, Writer out) throws IOException;
+    public abstract void write(int docId, LeafReader reader, Writer out) throws IOException;
   }
 
   class IntFieldWriter extends FieldWriter {
@@ -1162,7 +1160,7 @@ public class SortingResponseWriter implements QueryResponseWriter {
       this.field = field;
     }
 
-    public void write(int docId, AtomicReader reader, Writer out) throws IOException {
+    public void write(int docId, LeafReader reader, Writer out) throws IOException {
       NumericDocValues vals = reader.getNumericDocValues(this.field);
       int val = (int)vals.get(docId);
        out.write('"');
@@ -1185,7 +1183,7 @@ public class SortingResponseWriter implements QueryResponseWriter {
       this.numeric = numeric;
     }
 
-    public void write(int docId, AtomicReader reader, Writer out) throws IOException {
+    public void write(int docId, LeafReader reader, Writer out) throws IOException {
       SortedSetDocValues vals = reader.getSortedSetDocValues(this.field);
       vals.setDocument(docId);
       out.write('"');
@@ -1224,7 +1222,7 @@ public class SortingResponseWriter implements QueryResponseWriter {
       this.field = field;
     }
 
-    public void write(int docId, AtomicReader reader, Writer out) throws IOException {
+    public void write(int docId, LeafReader reader, Writer out) throws IOException {
       NumericDocValues vals = reader.getNumericDocValues(this.field);
       long val = vals.get(docId);
       out.write('"');
@@ -1242,7 +1240,7 @@ public class SortingResponseWriter implements QueryResponseWriter {
       this.field = field;
     }
 
-    public void write(int docId, AtomicReader reader, Writer out) throws IOException {
+    public void write(int docId, LeafReader reader, Writer out) throws IOException {
       NumericDocValues vals = reader.getNumericDocValues(this.field);
       int val = (int)vals.get(docId);
       out.write('"');
@@ -1260,7 +1258,7 @@ public class SortingResponseWriter implements QueryResponseWriter {
       this.field = field;
     }
 
-    public void write(int docId, AtomicReader reader, Writer out) throws IOException {
+    public void write(int docId, LeafReader reader, Writer out) throws IOException {
       NumericDocValues vals = reader.getNumericDocValues(this.field);
       long val = vals.get(docId);
       out.write('"');
@@ -1281,7 +1279,7 @@ public class SortingResponseWriter implements QueryResponseWriter {
       this.fieldType = fieldType;
     }
 
-    public void write(int docId, AtomicReader reader, Writer out) throws IOException {
+    public void write(int docId, LeafReader reader, Writer out) throws IOException {
       SortedDocValues vals = reader.getSortedDocValues(this.field);
       BytesRef ref = vals.get(docId);
       fieldType.indexedToReadable(ref, cref);
diff --git solr/core/src/java/org/apache/solr/response/transform/ValueSourceAugmenter.java solr/core/src/java/org/apache/solr/response/transform/ValueSourceAugmenter.java
index ed594a7..2db72f9 100644
--- solr/core/src/java/org/apache/solr/response/transform/ValueSourceAugmenter.java
+++ solr/core/src/java/org/apache/solr/response/transform/ValueSourceAugmenter.java
@@ -20,7 +20,7 @@ import java.io.IOException;
 import java.util.List;
 import java.util.Map;
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.ReaderUtil;
 import org.apache.lucene.queries.function.FunctionValues;
@@ -77,7 +77,7 @@ public class ValueSourceAugmenter extends DocTransformer
 
   Map fcontext;
   SolrIndexSearcher searcher;
-  List<AtomicReaderContext> readerContexts;
+  List<LeafReaderContext> readerContexts;
   FunctionValues docValuesArr[];
 
 
@@ -89,7 +89,7 @@ public class ValueSourceAugmenter extends DocTransformer
 
       // TODO: calculate this stuff just once across diff functions
       int idx = ReaderUtil.subIndex(docid, readerContexts);
-      AtomicReaderContext rcontext = readerContexts.get(idx);
+      LeafReaderContext rcontext = readerContexts.get(idx);
       FunctionValues values = docValuesArr[idx];
       if (values == null) {
         docValuesArr[idx] = values = valueSource.getValues(fcontext, rcontext);
diff --git solr/core/src/java/org/apache/solr/schema/BoolField.java solr/core/src/java/org/apache/solr/schema/BoolField.java
index 81bc063..dad044b 100644
--- solr/core/src/java/org/apache/solr/schema/BoolField.java
+++ solr/core/src/java/org/apache/solr/schema/BoolField.java
@@ -18,15 +18,13 @@
 package org.apache.solr.schema;
 
 import java.io.IOException;
-import java.io.Reader;
 import java.util.Map;
 
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.Tokenizer;
 import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.DocValues;
-import org.apache.lucene.index.GeneralField;
 import org.apache.lucene.index.SortedDocValues;
 import org.apache.lucene.index.StorableField;
 import org.apache.lucene.queries.function.FunctionValues;
@@ -189,7 +187,7 @@ class BoolFieldSource extends ValueSource {
 
 
   @Override
-  public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
+  public FunctionValues getValues(Map context, LeafReaderContext readerContext) throws IOException {
     final SortedDocValues sindex = DocValues.getSorted(readerContext.reader(), field);
 
     // figure out what ord maps to true
diff --git solr/core/src/java/org/apache/solr/schema/CurrencyField.java solr/core/src/java/org/apache/solr/schema/CurrencyField.java
index ca0c785..009baef 100644
--- solr/core/src/java/org/apache/solr/schema/CurrencyField.java
+++ solr/core/src/java/org/apache/solr/schema/CurrencyField.java
@@ -18,7 +18,7 @@ package org.apache.solr.schema;
 
 import org.apache.lucene.analysis.util.ResourceLoader;
 import org.apache.lucene.analysis.util.ResourceLoaderAware;
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.StorableField;
 import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
@@ -385,7 +385,7 @@ public class CurrencyField extends FieldType implements SchemaAware, ResourceLoa
     }
 
     @Override
-    public FunctionValues getValues(Map context, AtomicReaderContext reader) 
+    public FunctionValues getValues(Map context, LeafReaderContext reader) 
       throws IOException {
       final FunctionValues amounts = source.getValues(context, reader);
       // the target digits & currency of our source, 
@@ -498,7 +498,7 @@ public class CurrencyField extends FieldType implements SchemaAware, ResourceLoa
     public Currency getTargetCurrency() { return targetCurrency; }
 
     @Override
-    public FunctionValues getValues(Map context, AtomicReaderContext reader) throws IOException {
+    public FunctionValues getValues(Map context, LeafReaderContext reader) throws IOException {
       final FunctionValues amounts = amountValues.getValues(context, reader);
       final FunctionValues currencies = currencyValues.getValues(context, reader);
 
diff --git solr/core/src/java/org/apache/solr/schema/LatLonType.java solr/core/src/java/org/apache/solr/schema/LatLonType.java
index de5bc61..d29122a 100644
--- solr/core/src/java/org/apache/solr/schema/LatLonType.java
+++ solr/core/src/java/org/apache/solr/schema/LatLonType.java
@@ -25,13 +25,12 @@ import java.util.Set;
 import com.spatial4j.core.shape.Point;
 
 import org.apache.lucene.document.FieldType;
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.StorableField;
 import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.queries.function.valuesource.VectorValueSource;
-import org.apache.lucene.search.LeafCollector;
 import org.apache.lucene.search.BooleanClause;
 import org.apache.lucene.search.BooleanQuery;
 import org.apache.lucene.search.ComplexExplanation;
@@ -342,12 +341,12 @@ class SpatialDistanceQuery extends ExtendedQueryBase implements PostFilter {
     }
 
     @Override
-    public Scorer scorer(AtomicReaderContext context, Bits acceptDocs) throws IOException {
+    public Scorer scorer(LeafReaderContext context, Bits acceptDocs) throws IOException {
       return new SpatialScorer(context, acceptDocs, this, queryWeight);
     }
 
     @Override
-    public Explanation explain(AtomicReaderContext context, int doc) throws IOException {
+    public Explanation explain(LeafReaderContext context, int doc) throws IOException {
       return ((SpatialScorer)scorer(context, context.reader().getLiveDocs())).explain(doc);
     }
   }
@@ -376,7 +375,7 @@ class SpatialDistanceQuery extends ExtendedQueryBase implements PostFilter {
     int lastDistDoc;
     double lastDist;
 
-    public SpatialScorer(AtomicReaderContext readerContext, Bits acceptDocs, SpatialWeight w, float qWeight) throws IOException {
+    public SpatialScorer(LeafReaderContext readerContext, Bits acceptDocs, SpatialWeight w, float qWeight) throws IOException {
       super(w);
       this.weight = w;
       this.qWeight = qWeight;
@@ -535,7 +534,7 @@ class SpatialDistanceQuery extends ExtendedQueryBase implements PostFilter {
     }
 
     @Override
-    protected void doSetNextReader(AtomicReaderContext context) throws IOException {
+    protected void doSetNextReader(LeafReaderContext context) throws IOException {
       super.doSetNextReader(context);
       maxdoc = context.reader().maxDoc();
       spatialScorer = new SpatialScorer(context, null, weight, 1.0f);
diff --git solr/core/src/java/org/apache/solr/schema/RandomSortField.java solr/core/src/java/org/apache/solr/schema/RandomSortField.java
index aa516fd..cb2adbb 100644
--- solr/core/src/java/org/apache/solr/schema/RandomSortField.java
+++ solr/core/src/java/org/apache/solr/schema/RandomSortField.java
@@ -20,10 +20,8 @@ package org.apache.solr.schema;
 import java.io.IOException;
 import java.util.Map;
 
-import org.apache.lucene.index.GeneralField;
-import org.apache.lucene.index.IndexableField;
 import org.apache.lucene.index.DirectoryReader;
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.ReaderUtil;
 import org.apache.lucene.index.StorableField;
 import org.apache.lucene.queries.function.FunctionValues;
@@ -82,7 +80,7 @@ public class RandomSortField extends FieldType {
    * Given a field name and an IndexReader, get a random hash seed.
    * Using dynamic fields, you can force the random order to change 
    */
-  private static int getSeed(String fieldName, AtomicReaderContext context) {
+  private static int getSeed(String fieldName, LeafReaderContext context) {
     final DirectoryReader top = (DirectoryReader) ReaderUtil.getTopLevelContext(context).reader();
     // calling getVersion() on a segment will currently give you a null pointer exception, so
     // we use the top-level reader.
@@ -143,7 +141,7 @@ public class RandomSortField extends FieldType {
         }
 
         @Override
-        public FieldComparator setNextReader(AtomicReaderContext context) {
+        public FieldComparator setNextReader(LeafReaderContext context) {
           seed = getSeed(fieldname, context);
           return this;
         }
@@ -177,7 +175,7 @@ public class RandomSortField extends FieldType {
     }
 
     @Override
-    public FunctionValues getValues(Map context, final AtomicReaderContext readerContext) throws IOException {
+    public FunctionValues getValues(Map context, final LeafReaderContext readerContext) throws IOException {
       return new IntDocValues(this) {
           private final int seed = getSeed(field, readerContext);
           @Override
diff --git solr/core/src/java/org/apache/solr/schema/StrFieldSource.java solr/core/src/java/org/apache/solr/schema/StrFieldSource.java
index 46629f0..9310ed1 100644
--- solr/core/src/java/org/apache/solr/schema/StrFieldSource.java
+++ solr/core/src/java/org/apache/solr/schema/StrFieldSource.java
@@ -17,7 +17,7 @@
 
 package org.apache.solr.schema;
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.docvalues.DocTermsIndexDocValues;
 import org.apache.lucene.queries.function.valuesource.FieldCacheSource;
@@ -37,7 +37,7 @@ public class StrFieldSource extends FieldCacheSource {
   }
 
   @Override
-  public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
+  public FunctionValues getValues(Map context, LeafReaderContext readerContext) throws IOException {
     return new DocTermsIndexDocValues(this, readerContext, field) {
 
       @Override
diff --git solr/core/src/java/org/apache/solr/search/BitDocSet.java solr/core/src/java/org/apache/solr/search/BitDocSet.java
index 6274f2f..577b100 100644
--- solr/core/src/java/org/apache/solr/search/BitDocSet.java
+++ solr/core/src/java/org/apache/solr/search/BitDocSet.java
@@ -17,8 +17,8 @@
 
 package org.apache.solr.search;
 
-import org.apache.lucene.index.AtomicReader;
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReader;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.search.BitsFilteredDocIdSet;
 import org.apache.lucene.search.DocIdSet;
 import org.apache.lucene.search.DocIdSetIterator;
@@ -270,8 +270,8 @@ public class BitDocSet extends DocSetBase {
 
     return new Filter() {
       @Override
-      public DocIdSet getDocIdSet(final AtomicReaderContext context, final Bits acceptDocs) {
-        AtomicReader reader = context.reader();
+      public DocIdSet getDocIdSet(final LeafReaderContext context, final Bits acceptDocs) {
+        LeafReader reader = context.reader();
         // all Solr DocSets that are used as filters only include live docs
         final Bits acceptDocs2 = acceptDocs == null ? null : (reader.getLiveDocs() == acceptDocs ? null : acceptDocs);
 
diff --git solr/core/src/java/org/apache/solr/search/CollapsingQParserPlugin.java solr/core/src/java/org/apache/solr/search/CollapsingQParserPlugin.java
index 2073ffe..48051d3 100644
--- solr/core/src/java/org/apache/solr/search/CollapsingQParserPlugin.java
+++ solr/core/src/java/org/apache/solr/search/CollapsingQParserPlugin.java
@@ -22,7 +22,7 @@ import java.util.Arrays;
 import java.util.Iterator;
 import java.util.Map;
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.DocValues;
 import org.apache.lucene.index.NumericDocValues;
 import org.apache.lucene.index.SortedDocValues;
@@ -222,7 +222,7 @@ public class CollapsingQParserPlugin extends QParserPlugin {
 
         SortedDocValues docValues = null;
         FunctionQuery funcQuery = null;
-        docValues = DocValues.getSorted(searcher.getAtomicReader(), this.field);
+        docValues = DocValues.getSorted(searcher.getLeafReader(), this.field);
 
         FieldType fieldType = null;
 
@@ -373,7 +373,7 @@ public class CollapsingQParserPlugin extends QParserPlugin {
 
   private class CollapsingScoreCollector extends DelegatingCollector {
 
-    private AtomicReaderContext[] contexts;
+    private LeafReaderContext[] contexts;
     private FixedBitSet collapsedSet;
     private SortedDocValues values;
     private int[] ords;
@@ -392,7 +392,7 @@ public class CollapsingQParserPlugin extends QParserPlugin {
                                     int nullPolicy,
                                     IntIntOpenHashMap boostDocs) {
       this.maxDoc = maxDoc;
-      this.contexts = new AtomicReaderContext[segments];
+      this.contexts = new LeafReaderContext[segments];
       this.collapsedSet = new FixedBitSet(maxDoc);
       this.boostDocs = boostDocs;
       if(this.boostDocs != null) {
@@ -430,7 +430,7 @@ public class CollapsingQParserPlugin extends QParserPlugin {
     }
 
     @Override
-    protected void doSetNextReader(AtomicReaderContext context) throws IOException {
+    protected void doSetNextReader(LeafReaderContext context) throws IOException {
       this.contexts[context.ord] = context;
       this.docBase = context.docBase;
     }
@@ -528,7 +528,7 @@ public class CollapsingQParserPlugin extends QParserPlugin {
   }
 
   private class CollapsingFieldValueCollector extends DelegatingCollector {
-    private AtomicReaderContext[] contexts;
+    private LeafReaderContext[] contexts;
     private SortedDocValues values;
 
     private int maxDoc;
@@ -550,7 +550,7 @@ public class CollapsingQParserPlugin extends QParserPlugin {
                                          FunctionQuery funcQuery, IndexSearcher searcher) throws IOException{
 
       this.maxDoc = maxDoc;
-      this.contexts = new AtomicReaderContext[segments];
+      this.contexts = new LeafReaderContext[segments];
       this.values = values;
       int valueCount = values.getValueCount();
       this.nullPolicy = nullPolicy;
@@ -580,7 +580,7 @@ public class CollapsingQParserPlugin extends QParserPlugin {
       this.fieldValueCollapse.setScorer(scorer);
     }
 
-    public void doSetNextReader(AtomicReaderContext context) throws IOException {
+    public void doSetNextReader(LeafReaderContext context) throws IOException {
       this.contexts[context.ord] = context;
       this.docBase = context.docBase;
       this.fieldValueCollapse.setNextReader(context);
@@ -660,7 +660,7 @@ public class CollapsingQParserPlugin extends QParserPlugin {
     protected String field;
 
     public abstract void collapse(int ord, int contextDoc, int globalDoc) throws IOException;
-    public abstract void setNextReader(AtomicReaderContext context) throws IOException;
+    public abstract void setNextReader(LeafReaderContext context) throws IOException;
 
     public FieldValueCollapse(int maxDoc,
                               String field,
@@ -766,7 +766,7 @@ public class CollapsingQParserPlugin extends QParserPlugin {
       }
     }
 
-    public void setNextReader(AtomicReaderContext context) throws IOException {
+    public void setNextReader(LeafReaderContext context) throws IOException {
       this.vals = DocValues.getNumeric(context.reader(), this.field);
     }
 
@@ -834,7 +834,7 @@ public class CollapsingQParserPlugin extends QParserPlugin {
       }
     }
 
-    public void setNextReader(AtomicReaderContext context) throws IOException {
+    public void setNextReader(LeafReaderContext context) throws IOException {
       this.vals = DocValues.getNumeric(context.reader(), this.field);
     }
 
@@ -903,7 +903,7 @@ public class CollapsingQParserPlugin extends QParserPlugin {
       }
     }
 
-    public void setNextReader(AtomicReaderContext context) throws IOException {
+    public void setNextReader(LeafReaderContext context) throws IOException {
       this.vals = DocValues.getNumeric(context.reader(), this.field);
     }
 
@@ -985,7 +985,7 @@ public class CollapsingQParserPlugin extends QParserPlugin {
       }
     }
 
-    public void setNextReader(AtomicReaderContext context) throws IOException {
+    public void setNextReader(LeafReaderContext context) throws IOException {
       functionValues = this.valueSource.getValues(rcontext, context);
     }
 
diff --git solr/core/src/java/org/apache/solr/search/CursorMark.java solr/core/src/java/org/apache/solr/search/CursorMark.java
index 75c822c..5b3f5cf 100644
--- solr/core/src/java/org/apache/solr/search/CursorMark.java
+++ solr/core/src/java/org/apache/solr/search/CursorMark.java
@@ -17,14 +17,9 @@
 
 package org.apache.solr.search;
 
-import org.apache.lucene.index.AtomicReaderContext;
-import org.apache.lucene.search.FieldComparator;
-import org.apache.lucene.search.IndexSearcher;
 import org.apache.lucene.search.FieldDoc;
-import org.apache.lucene.search.Scorer;
 import org.apache.lucene.search.Sort;
 import org.apache.lucene.search.SortField;
-import org.apache.lucene.search.Query;
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.BytesRefBuilder;
 import org.apache.solr.common.SolrException;
@@ -37,17 +32,11 @@ import org.apache.solr.common.util.JavaBinCodec;
 import org.apache.solr.schema.IndexSchema;
 import org.apache.solr.schema.FieldType;
 import org.apache.solr.schema.SchemaField;
-import org.apache.solr.search.PostFilter;
-import org.apache.solr.search.ExtendedQueryBase;
-import org.apache.solr.search.DelegatingCollector;
-import org.apache.commons.lang.StringUtils;
 
 import java.util.List;
 import java.util.ArrayList;
-import java.util.Arrays;
 import java.io.ByteArrayOutputStream;
 import java.io.ByteArrayInputStream;
-import java.io.IOException;
 
 /**
  * An object that encapsulates the basic information about the current Mark Point of a 
diff --git solr/core/src/java/org/apache/solr/search/DelegatingCollector.java solr/core/src/java/org/apache/solr/search/DelegatingCollector.java
index 06b9658..65af211 100644
--- solr/core/src/java/org/apache/solr/search/DelegatingCollector.java
+++ solr/core/src/java/org/apache/solr/search/DelegatingCollector.java
@@ -20,7 +20,7 @@ package org.apache.solr.search;
 
 import java.io.IOException;
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.search.LeafCollector;
 import org.apache.lucene.search.Collector;
 import org.apache.lucene.search.Scorer;
@@ -36,7 +36,7 @@ public class DelegatingCollector extends SimpleCollector {
   protected Collector delegate;
   protected LeafCollector leafDelegate;
   protected Scorer scorer;
-  protected AtomicReaderContext context;
+  protected LeafReaderContext context;
   protected int docBase;
 
   public Collector getDelegate() {
@@ -69,7 +69,7 @@ public class DelegatingCollector extends SimpleCollector {
   }
 
   @Override
-  protected void doSetNextReader(AtomicReaderContext context) throws IOException {
+  protected void doSetNextReader(LeafReaderContext context) throws IOException {
     this.context = context;
     this.docBase = context.docBase;
     leafDelegate = delegate.getLeafCollector(context);
diff --git solr/core/src/java/org/apache/solr/search/DocSetBase.java solr/core/src/java/org/apache/solr/search/DocSetBase.java
index 800843c..af07405 100644
--- solr/core/src/java/org/apache/solr/search/DocSetBase.java
+++ solr/core/src/java/org/apache/solr/search/DocSetBase.java
@@ -17,8 +17,8 @@
 
 package org.apache.solr.search;
 
-import org.apache.lucene.index.AtomicReader;
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReader;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.search.BitsFilteredDocIdSet;
 import org.apache.lucene.search.DocIdSet;
 import org.apache.lucene.search.DocIdSetIterator;
@@ -165,8 +165,8 @@ abstract class DocSetBase implements DocSet {
 
     return new Filter() {
       @Override
-      public DocIdSet getDocIdSet(final AtomicReaderContext context, Bits acceptDocs) {
-        AtomicReader reader = context.reader();
+      public DocIdSet getDocIdSet(final LeafReaderContext context, Bits acceptDocs) {
+        LeafReader reader = context.reader();
         // all Solr DocSets that are used as filters only include live docs
         final Bits acceptDocs2 = acceptDocs == null ? null : (reader.getLiveDocs() == acceptDocs ? null : acceptDocs);
 
diff --git solr/core/src/java/org/apache/solr/search/DocSetCollector.java solr/core/src/java/org/apache/solr/search/DocSetCollector.java
index cbc179b..09412d1 100644
--- solr/core/src/java/org/apache/solr/search/DocSetCollector.java
+++ solr/core/src/java/org/apache/solr/search/DocSetCollector.java
@@ -19,8 +19,7 @@ package org.apache.solr.search;
 
 import java.io.IOException;
 
-import org.apache.lucene.index.AtomicReaderContext;
-import org.apache.lucene.search.LeafCollector;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.search.Scorer;
 import org.apache.lucene.search.SimpleCollector;
 import org.apache.lucene.util.FixedBitSet;
@@ -85,7 +84,7 @@ public class DocSetCollector extends SimpleCollector {
   }
 
   @Override
-  protected void doSetNextReader(AtomicReaderContext context) throws IOException {
+  protected void doSetNextReader(LeafReaderContext context) throws IOException {
     this.base = context.docBase;
   }
 
diff --git solr/core/src/java/org/apache/solr/search/EarlyTerminatingCollector.java solr/core/src/java/org/apache/solr/search/EarlyTerminatingCollector.java
index 200d326..bc4ef3e 100644
--- solr/core/src/java/org/apache/solr/search/EarlyTerminatingCollector.java
+++ solr/core/src/java/org/apache/solr/search/EarlyTerminatingCollector.java
@@ -19,7 +19,7 @@ package org.apache.solr.search;
 
 import java.io.IOException;
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.search.LeafCollector;
 import org.apache.lucene.search.Collector;
 import org.apache.lucene.search.FilterLeafCollector;
@@ -56,7 +56,7 @@ public class EarlyTerminatingCollector extends FilterCollector {
   }
 
   @Override
-  public LeafCollector getLeafCollector(AtomicReaderContext context)
+  public LeafCollector getLeafCollector(LeafReaderContext context)
       throws IOException {
     prevReaderCumulativeSize += currentReaderSize; // not current any more
     currentReaderSize = context.reader().maxDoc() - 1;
diff --git solr/core/src/java/org/apache/solr/search/ExportQParserPlugin.java solr/core/src/java/org/apache/solr/search/ExportQParserPlugin.java
index 53fad90..878d8a9 100644
--- solr/core/src/java/org/apache/solr/search/ExportQParserPlugin.java
+++ solr/core/src/java/org/apache/solr/search/ExportQParserPlugin.java
@@ -17,7 +17,6 @@
 
 package org.apache.solr.search;
 
-import com.carrotsearch.hppc.IntArrayList;
 import org.apache.lucene.util.FixedBitSet;
 import org.apache.solr.handler.component.MergeStrategy;
 import org.apache.solr.request.SolrRequestInfo;
@@ -26,7 +25,7 @@ import org.apache.lucene.index.*;
 import org.apache.solr.common.util.NamedList;
 import org.apache.solr.request.SolrQueryRequest;
 import org.apache.solr.common.params.SolrParams;
-import org.apache.lucene.util.OpenBitSet;
+
 import java.io.IOException;
 import java.util.Map;
 import java.util.Set;
@@ -136,7 +135,7 @@ public class ExportQParserPlugin extends QParserPlugin {
       this.sets = sets;
     }
     
-    public void doSetNextReader(AtomicReaderContext context) throws IOException {
+    public void doSetNextReader(LeafReaderContext context) throws IOException {
       this.set = new FixedBitSet(context.reader().maxDoc());
       this.sets[context.ord] = set;
 
diff --git solr/core/src/java/org/apache/solr/search/FunctionRangeQuery.java solr/core/src/java/org/apache/solr/search/FunctionRangeQuery.java
index 91bc1c0..8451f0a 100644
--- solr/core/src/java/org/apache/solr/search/FunctionRangeQuery.java
+++ solr/core/src/java/org/apache/solr/search/FunctionRangeQuery.java
@@ -17,14 +17,10 @@
 
 package org.apache.solr.search;
 
-import org.apache.lucene.index.AtomicReaderContext;
-import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.queries.function.ValueSourceScorer;
-import org.apache.lucene.search.LeafCollector;
-import org.apache.lucene.search.Collector;
-import org.apache.lucene.search.FilterCollector;
 import org.apache.lucene.search.IndexSearcher;
 import org.apache.solr.search.function.ValueSourceRangeFilter;
 
@@ -63,7 +59,7 @@ public class FunctionRangeQuery extends SolrConstantScoreQuery implements PostFi
     }
 
     @Override
-    protected void doSetNextReader(AtomicReaderContext context) throws IOException {
+    protected void doSetNextReader(LeafReaderContext context) throws IOException {
       super.doSetNextReader(context);
       maxdoc = context.reader().maxDoc();
       FunctionValues dv = rangeFilt.getValueSource().getValues(fcontext, context);
diff --git solr/core/src/java/org/apache/solr/search/Insanity.java solr/core/src/java/org/apache/solr/search/Insanity.java
index 5b9f567..815edba 100644
--- solr/core/src/java/org/apache/solr/search/Insanity.java
+++ solr/core/src/java/org/apache/solr/search/Insanity.java
@@ -21,11 +21,11 @@ import java.io.IOException;
 import java.util.ArrayList;
 import java.util.Collections;
 
-import org.apache.lucene.index.AtomicReader;
+import org.apache.lucene.index.FilterLeafReader;
+import org.apache.lucene.index.LeafReader;
 import org.apache.lucene.index.BinaryDocValues;
 import org.apache.lucene.index.FieldInfo;
 import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.FilterAtomicReader;
 import org.apache.lucene.index.NumericDocValues;
 import org.apache.lucene.index.SortedDocValues;
 import org.apache.lucene.index.SortedSetDocValues;
@@ -48,17 +48,17 @@ public class Insanity {
    * Returns a view over {@code sane} where {@code insaneField} is a string
    * instead of a numeric.
    */
-  public static AtomicReader wrapInsanity(AtomicReader sane, String insaneField) {
+  public static LeafReader wrapInsanity(LeafReader sane, String insaneField) {
     return new UninvertingReader(new InsaneReader(sane, insaneField),
                                  Collections.singletonMap(insaneField, UninvertingReader.Type.SORTED));
   }
   
   /** Hides the proper numeric dv type for the field */
-  private static class InsaneReader extends FilterAtomicReader {
+  private static class InsaneReader extends FilterLeafReader {
     final String insaneField;
     final FieldInfos fieldInfos;
     
-    InsaneReader(AtomicReader in, String insaneField) {
+    InsaneReader(LeafReader in, String insaneField) {
       super(in);
       this.insaneField = insaneField;
       ArrayList<FieldInfo> filteredInfos = new ArrayList<>();
diff --git solr/core/src/java/org/apache/solr/search/JoinQParserPlugin.java solr/core/src/java/org/apache/solr/search/JoinQParserPlugin.java
index a2b4958..c371ec1 100644
--- solr/core/src/java/org/apache/solr/search/JoinQParserPlugin.java
+++ solr/core/src/java/org/apache/solr/search/JoinQParserPlugin.java
@@ -23,7 +23,7 @@ import java.util.Arrays;
 import java.util.List;
 import java.util.Set;
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.DocsEnum;
 import org.apache.lucene.index.Fields;
 import org.apache.lucene.index.IndexReader;
@@ -232,7 +232,7 @@ class JoinQuery extends Query {
 
 
     @Override
-    public Scorer scorer(AtomicReaderContext context, Bits acceptDocs) throws IOException {
+    public Scorer scorer(LeafReaderContext context, Bits acceptDocs) throws IOException {
       if (filter == null) {
         boolean debug = rb != null && rb.isDebug();
         long start = debug ? System.currentTimeMillis() : 0;
@@ -304,8 +304,8 @@ class JoinQuery extends Query {
         fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());
       }
 
-      Fields fromFields = fromSearcher.getAtomicReader().fields();
-      Fields toFields = fromSearcher==toSearcher ? fromFields : toSearcher.getAtomicReader().fields();
+      Fields fromFields = fromSearcher.getLeafReader().fields();
+      Fields toFields = fromSearcher==toSearcher ? fromFields : toSearcher.getLeafReader().fields();
       if (fromFields == null) return DocSet.EMPTY;
       Terms terms = fromFields.terms(fromField);
       Terms toTerms = toFields.terms(toField);
@@ -327,8 +327,8 @@ class JoinQuery extends Query {
         }
       }
 
-      Bits fromLiveDocs = fromSearcher.getAtomicReader().getLiveDocs();
-      Bits toLiveDocs = fromSearcher == toSearcher ? fromLiveDocs : toSearcher.getAtomicReader().getLiveDocs();
+      Bits fromLiveDocs = fromSearcher.getLeafReader().getLiveDocs();
+      Bits toLiveDocs = fromSearcher == toSearcher ? fromLiveDocs : toSearcher.getLeafReader().getLiveDocs();
 
       fromDeState = new SolrIndexSearcher.DocsEnumState();
       fromDeState.fieldName = fromField;
@@ -500,7 +500,7 @@ class JoinQuery extends Query {
     }
 
     @Override
-    public Explanation explain(AtomicReaderContext context, int doc) throws IOException {
+    public Explanation explain(LeafReaderContext context, int doc) throws IOException {
       Scorer scorer = scorer(context, context.reader().getLiveDocs());
       boolean exists = scorer.advance(doc) == doc;
 
diff --git solr/core/src/java/org/apache/solr/search/ReRankQParserPlugin.java solr/core/src/java/org/apache/solr/search/ReRankQParserPlugin.java
index bf25ad9..5e744be 100644
--- solr/core/src/java/org/apache/solr/search/ReRankQParserPlugin.java
+++ solr/core/src/java/org/apache/solr/search/ReRankQParserPlugin.java
@@ -18,7 +18,7 @@
 package org.apache.solr.search;
 
 import com.carrotsearch.hppc.IntIntOpenHashMap;
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.search.Explanation;
 import org.apache.lucene.search.MatchAllDocsQuery;
 import org.apache.lucene.util.BytesRef;
@@ -194,7 +194,7 @@ public class ReRankQParserPlugin extends QParserPlugin {
       return mainWeight.getValueForNormalization();
     }
 
-    public Scorer scorer(AtomicReaderContext context, Bits bits) throws IOException {
+    public Scorer scorer(LeafReaderContext context, Bits bits) throws IOException {
       return mainWeight.scorer(context, bits);
     }
 
@@ -206,7 +206,7 @@ public class ReRankQParserPlugin extends QParserPlugin {
       mainWeight.normalize(norm, topLevelBoost);
     }
 
-    public Explanation explain(AtomicReaderContext context, int doc) throws IOException {
+    public Explanation explain(LeafReaderContext context, int doc) throws IOException {
       Explanation mainExplain = mainWeight.explain(context, doc);
       return new QueryRescorer(reRankQuery) {
         @Override
@@ -267,7 +267,7 @@ public class ReRankQParserPlugin extends QParserPlugin {
       mainCollector.setScorer(scorer);
     }
 
-    public void doSetNextReader(AtomicReaderContext context) throws IOException{
+    public void doSetNextReader(LeafReaderContext context) throws IOException{
       mainCollector.getLeafCollector(context);
     }
 
diff --git solr/core/src/java/org/apache/solr/search/SolrConstantScoreQuery.java solr/core/src/java/org/apache/solr/search/SolrConstantScoreQuery.java
index dfdf7ad..fa37caf 100644
--- solr/core/src/java/org/apache/solr/search/SolrConstantScoreQuery.java
+++ solr/core/src/java/org/apache/solr/search/SolrConstantScoreQuery.java
@@ -1,10 +1,10 @@
 package org.apache.solr.search;
 
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.search.*;
 import org.apache.lucene.util.Bits;
 import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.solr.common.SolrException;
 
 import java.io.IOException;
@@ -119,12 +119,12 @@ public class SolrConstantScoreQuery extends ConstantScoreQuery implements Extend
     }
 
     @Override
-    public Scorer scorer(AtomicReaderContext context, Bits acceptDocs) throws IOException {
+    public Scorer scorer(LeafReaderContext context, Bits acceptDocs) throws IOException {
       return new ConstantScorer(context, this, queryWeight, acceptDocs);
     }
 
     @Override
-    public Explanation explain(AtomicReaderContext context, int doc) throws IOException {
+    public Explanation explain(LeafReaderContext context, int doc) throws IOException {
 
       ConstantScorer cs = new ConstantScorer(context, this, queryWeight, context.reader().getLiveDocs());
       boolean exists = cs.docIdSetIterator.advance(doc) == doc;
@@ -154,7 +154,7 @@ public class SolrConstantScoreQuery extends ConstantScoreQuery implements Extend
     final Bits acceptDocs;
     int doc = -1;
 
-    public ConstantScorer(AtomicReaderContext context, ConstantWeight w, float theScore, Bits acceptDocs) throws IOException {
+    public ConstantScorer(LeafReaderContext context, ConstantWeight w, float theScore, Bits acceptDocs) throws IOException {
       super(w);
       this.theScore = theScore;
       this.acceptDocs = acceptDocs;
diff --git solr/core/src/java/org/apache/solr/search/SolrFilter.java solr/core/src/java/org/apache/solr/search/SolrFilter.java
index 989af96..2383501 100644
--- solr/core/src/java/org/apache/solr/search/SolrFilter.java
+++ solr/core/src/java/org/apache/solr/search/SolrFilter.java
@@ -17,13 +17,11 @@
 
 package org.apache.solr.search;
 
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.search.Filter;
 import org.apache.lucene.search.IndexSearcher;
 import org.apache.lucene.search.DocIdSet;
 import org.apache.lucene.util.Bits;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.AtomicReaderContext;
-import org.apache.lucene.index.AtomicReaderContext;
 
 import java.util.Map;
 import java.io.IOException;
@@ -40,10 +38,10 @@ public abstract class SolrFilter extends Filter {
    * The context object will be passed to getDocIdSet() where this info can be retrieved. */
   public abstract void createWeight(Map context, IndexSearcher searcher) throws IOException;
   
-  public abstract DocIdSet getDocIdSet(Map context, AtomicReaderContext readerContext, Bits acceptDocs) throws IOException;
+  public abstract DocIdSet getDocIdSet(Map context, LeafReaderContext readerContext, Bits acceptDocs) throws IOException;
 
   @Override
-  public DocIdSet getDocIdSet(AtomicReaderContext context, Bits acceptDocs) throws IOException {
+  public DocIdSet getDocIdSet(LeafReaderContext context, Bits acceptDocs) throws IOException {
     return getDocIdSet(null, context, acceptDocs);
   }
 }
diff --git solr/core/src/java/org/apache/solr/search/SolrIndexSearcher.java solr/core/src/java/org/apache/solr/search/SolrIndexSearcher.java
index bd2110c..5f9c174 100644
--- solr/core/src/java/org/apache/solr/search/SolrIndexSearcher.java
+++ solr/core/src/java/org/apache/solr/search/SolrIndexSearcher.java
@@ -44,8 +44,8 @@ import org.apache.lucene.document.LazyDocument;
 import org.apache.lucene.document.LongField;
 import org.apache.lucene.document.StoredField;
 import org.apache.lucene.document.TextField;
-import org.apache.lucene.index.AtomicReader;
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReader;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.DocsEnum;
 import org.apache.lucene.index.FieldInfo;
@@ -169,7 +169,7 @@ public class SolrIndexSearcher extends IndexSearcher implements Closeable,SolrIn
   private Collection<String> storedHighlightFieldNames;
   private DirectoryFactory directoryFactory;
   
-  private final AtomicReader atomicReader;
+  private final LeafReader leafReader;
   // only for addIndexes etc (no fieldcache)
   private final DirectoryReader rawReader;
   
@@ -245,7 +245,7 @@ public class SolrIndexSearcher extends IndexSearcher implements Closeable,SolrIn
     this.directoryFactory = directoryFactory;
     this.reader = (DirectoryReader) super.readerContext.reader();
     this.rawReader = r;
-    this.atomicReader = SlowCompositeReaderWrapper.wrap(this.reader);
+    this.leafReader = SlowCompositeReaderWrapper.wrap(this.reader);
     this.core = core;
     this.schema = schema;
     this.name = "Searcher@" + Integer.toHexString(hashCode()) + "[" + core.getName() + "]" + (name != null ? " " + name : "");
@@ -315,7 +315,7 @@ public class SolrIndexSearcher extends IndexSearcher implements Closeable,SolrIn
     optimizer = null;
     
     fieldNames = new HashSet<>();
-    fieldInfos = atomicReader.getFieldInfos();
+    fieldInfos = leafReader.getFieldInfos();
     for(FieldInfo fieldInfo : fieldInfos) {
       fieldNames.add(fieldInfo.name);
     }
@@ -347,8 +347,8 @@ public class SolrIndexSearcher extends IndexSearcher implements Closeable,SolrIn
     return reader.docFreq(term);
   }
   
-  public final AtomicReader getAtomicReader() {
-    return atomicReader;
+  public final LeafReader getLeafReader() {
+    return leafReader;
   }
   
   /** Raw reader (no fieldcaches etc). Useful for operations like addIndexes */
@@ -772,7 +772,7 @@ public class SolrIndexSearcher extends IndexSearcher implements Closeable,SolrIn
    * @return the first document number containing the term
    */
   public int getFirstMatch(Term t) throws IOException {
-    Fields fields = atomicReader.fields();
+    Fields fields = leafReader.fields();
     if (fields == null) return -1;
     Terms terms = fields.terms(t.field());
     if (terms == null) return -1;
@@ -781,7 +781,7 @@ public class SolrIndexSearcher extends IndexSearcher implements Closeable,SolrIn
     if (!termsEnum.seekExact(termBytes)) {
       return -1;
     }
-    DocsEnum docs = termsEnum.docs(atomicReader.getLiveDocs(), null, DocsEnum.FLAG_NONE);
+    DocsEnum docs = termsEnum.docs(leafReader.getLiveDocs(), null, DocsEnum.FLAG_NONE);
     if (docs == null) return -1;
     int id = docs.nextDoc();
     return id == DocIdSetIterator.NO_MORE_DOCS ? -1 : id;
@@ -795,8 +795,8 @@ public class SolrIndexSearcher extends IndexSearcher implements Closeable,SolrIn
     String field = schema.getUniqueKeyField().getName();
 
     for (int i=0, c=leafContexts.size(); i<c; i++) {
-      final AtomicReaderContext leaf = leafContexts.get(i);
-      final AtomicReader reader = leaf.reader();
+      final LeafReaderContext leaf = leafContexts.get(i);
+      final LeafReader reader = leaf.reader();
 
       final Terms terms = reader.terms(field);
       if (terms == null) continue;
@@ -972,8 +972,8 @@ public class SolrIndexSearcher extends IndexSearcher implements Closeable,SolrIn
       collector = pf.postFilter;
     }
 
-    for (final AtomicReaderContext leaf : leafContexts) {
-      final AtomicReader reader = leaf.reader();
+    for (final LeafReaderContext leaf : leafContexts) {
+      final LeafReader reader = leaf.reader();
       final Bits liveDocs = reader.getLiveDocs();   // TODO: the filter may already only have liveDocs...
       DocIdSet idSet = null;
       if (pf.filter != null) {
@@ -2005,7 +2005,7 @@ public class SolrIndexSearcher extends IndexSearcher implements Closeable,SolrIn
     while (iter.hasNext()) {
       int doc = iter.nextDoc();
       while (doc>=end) {
-        AtomicReaderContext leaf = leafContexts.get(readerIndex++);
+        LeafReaderContext leaf = leafContexts.get(readerIndex++);
         base = leaf.docBase;
         end = base + leaf.reader().maxDoc();
         topCollector.getLeafCollector(leaf);
@@ -2427,7 +2427,7 @@ class FilterImpl extends Filter {
   }
 
   @Override
-  public DocIdSet getDocIdSet(AtomicReaderContext context, Bits acceptDocs) throws IOException {
+  public DocIdSet getDocIdSet(LeafReaderContext context, Bits acceptDocs) throws IOException {
     DocIdSet sub = topFilter == null ? null : topFilter.getDocIdSet(context, acceptDocs);
     if (weights.size() == 0) return sub;
     return new FilterSet(sub, context);
@@ -2435,9 +2435,9 @@ class FilterImpl extends Filter {
 
   private class FilterSet extends DocIdSet {
     DocIdSet docIdSet;
-    AtomicReaderContext context;
+    LeafReaderContext context;
 
-    public FilterSet(DocIdSet docIdSet, AtomicReaderContext context) {
+    public FilterSet(DocIdSet docIdSet, LeafReaderContext context) {
       this.docIdSet = docIdSet;
       this.context = context;
     }
diff --git solr/core/src/java/org/apache/solr/search/SortedIntDocSet.java solr/core/src/java/org/apache/solr/search/SortedIntDocSet.java
index 3e5d3a1..e5b891f 100644
--- solr/core/src/java/org/apache/solr/search/SortedIntDocSet.java
+++ solr/core/src/java/org/apache/solr/search/SortedIntDocSet.java
@@ -17,8 +17,8 @@
 
 package org.apache.solr.search;
 
-import org.apache.lucene.index.AtomicReader;
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReader;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.search.BitsFilteredDocIdSet;
 import org.apache.lucene.search.DocIdSet;
 import org.apache.lucene.search.DocIdSetIterator;
@@ -664,8 +664,8 @@ public class SortedIntDocSet extends DocSetBase {
       int lastEndIdx = 0;
 
       @Override
-      public DocIdSet getDocIdSet(final AtomicReaderContext context, final Bits acceptDocs) {
-        AtomicReader reader = context.reader();
+      public DocIdSet getDocIdSet(final LeafReaderContext context, final Bits acceptDocs) {
+        LeafReader reader = context.reader();
         // all Solr DocSets that are used as filters only include live docs
         final Bits acceptDocs2 = acceptDocs == null ? null : (reader.getLiveDocs() == acceptDocs ? null : acceptDocs);
 
diff --git solr/core/src/java/org/apache/solr/search/ValueSourceParser.java solr/core/src/java/org/apache/solr/search/ValueSourceParser.java
index 0ac1a42..30a157d 100644
--- solr/core/src/java/org/apache/solr/search/ValueSourceParser.java
+++ solr/core/src/java/org/apache/solr/search/ValueSourceParser.java
@@ -18,7 +18,7 @@ package org.apache.solr.search;
 
 import com.spatial4j.core.distance.DistanceUtils;
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.queries.function.BoostedQuery;
 import org.apache.lucene.queries.function.FunctionValues;
@@ -35,9 +35,7 @@ import org.apache.lucene.search.spell.JaroWinklerDistance;
 import org.apache.lucene.search.spell.LevensteinDistance;
 import org.apache.lucene.search.spell.NGramDistance;
 import org.apache.lucene.search.spell.StringDistance;
-import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.BytesRefBuilder;
-import org.apache.lucene.util.UnicodeUtil;
 import org.apache.solr.common.SolrException;
 import org.apache.solr.common.util.NamedList;
 import org.apache.solr.request.SolrRequestInfo;
@@ -999,7 +997,7 @@ class LongConstValueSource extends ConstNumberSource {
   }
 
   @Override
-  public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
+  public FunctionValues getValues(Map context, LeafReaderContext readerContext) throws IOException {
     return new LongDocValues(this) {
       @Override
       public float floatVal(int doc) {
@@ -1106,7 +1104,7 @@ abstract class DoubleParser extends NamedParser {
     }
 
     @Override
-    public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
+    public FunctionValues getValues(Map context, LeafReaderContext readerContext) throws IOException {
       final FunctionValues vals =  source.getValues(context, readerContext);
       return new DoubleDocValues(this) {
         @Override
@@ -1154,7 +1152,7 @@ abstract class Double2Parser extends NamedParser {
     }
 
     @Override
-    public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
+    public FunctionValues getValues(Map context, LeafReaderContext readerContext) throws IOException {
       final FunctionValues aVals =  a.getValues(context, readerContext);
       final FunctionValues bVals =  b.getValues(context, readerContext);
       return new DoubleDocValues(this) {
@@ -1208,7 +1206,7 @@ class BoolConstValueSource extends ConstNumberSource {
   }
 
   @Override
-  public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
+  public FunctionValues getValues(Map context, LeafReaderContext readerContext) throws IOException {
     return new BoolDocValues(this) {
       @Override
       public boolean boolVal(int doc) {
@@ -1269,7 +1267,7 @@ class TestValueSource extends ValueSource {
   }
   
   @Override
-  public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
+  public FunctionValues getValues(Map context, LeafReaderContext readerContext) throws IOException {
     if (context.get(this) == null) {
       SolrRequestInfo requestInfo = SolrRequestInfo.getRequestInfo();
       throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, "testfunc: unweighted value source detected.  delegate="+source + " request=" + (requestInfo==null ? "null" : requestInfo.getReq()));
diff --git solr/core/src/java/org/apache/solr/search/function/CollapseScoreFunction.java solr/core/src/java/org/apache/solr/search/function/CollapseScoreFunction.java
index fb6d9c0..4034c4d 100644
--- solr/core/src/java/org/apache/solr/search/function/CollapseScoreFunction.java
+++ solr/core/src/java/org/apache/solr/search/function/CollapseScoreFunction.java
@@ -17,9 +17,9 @@
 
 package org.apache.solr.search.function;
 
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.queries.function.FunctionValues;
-import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.solr.search.CollapsingQParserPlugin.CollapseScore;
 import java.util.Map;
 import java.io.IOException;
@@ -42,7 +42,7 @@ public class CollapseScoreFunction extends ValueSource {
     return 1213241257;
   }
 
-  public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
+  public FunctionValues getValues(Map context, LeafReaderContext readerContext) throws IOException {
     return new CollapseScoreFunctionValues(context);
   }
 
diff --git solr/core/src/java/org/apache/solr/search/function/FileFloatSource.java solr/core/src/java/org/apache/solr/search/function/FileFloatSource.java
index a8643107..fcdb1f9 100644
--- solr/core/src/java/org/apache/solr/search/function/FileFloatSource.java
+++ solr/core/src/java/org/apache/solr/search/function/FileFloatSource.java
@@ -21,7 +21,6 @@ import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.queries.function.docvalues.FloatDocValues;
 import org.apache.lucene.search.DocIdSetIterator;
-import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.BytesRefBuilder;
 import org.apache.solr.core.SolrCore;
 import org.apache.solr.handler.RequestHandlerBase;
@@ -78,7 +77,7 @@ public class FileFloatSource extends ValueSource {
   }
 
   @Override
-  public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
+  public FunctionValues getValues(Map context, LeafReaderContext readerContext) throws IOException {
     final int off = readerContext.docBase;
     IndexReaderContext topLevelContext = ReaderUtil.getTopLevelContext(readerContext);
 
diff --git solr/core/src/java/org/apache/solr/search/function/OrdFieldSource.java solr/core/src/java/org/apache/solr/search/function/OrdFieldSource.java
index 0a1aa15..36745c8 100644
--- solr/core/src/java/org/apache/solr/search/function/OrdFieldSource.java
+++ solr/core/src/java/org/apache/solr/search/function/OrdFieldSource.java
@@ -21,8 +21,8 @@ import java.io.IOException;
 import java.util.List;
 import java.util.Map;
 
-import org.apache.lucene.index.AtomicReader;
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReader;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.DocValues;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.MultiReader;
@@ -40,7 +40,7 @@ import org.apache.solr.search.Insanity;
 import org.apache.solr.search.SolrIndexSearcher;
 
 /**
- * Obtains the ordinal of the field value from {@link AtomicReader#getSortedDocValues}.
+ * Obtains the ordinal of the field value from {@link org.apache.lucene.index.LeafReader#getSortedDocValues}.
  * <br>
  * The native lucene index order is used to assign an ordinal value for each field value.
  * <br>Field values (terms) are lexicographically ordered by unicode value, and numbered starting at 1.
@@ -71,25 +71,25 @@ public class OrdFieldSource extends ValueSource {
 
 
   @Override
-  public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
+  public FunctionValues getValues(Map context, LeafReaderContext readerContext) throws IOException {
     final int off = readerContext.docBase;
-    final AtomicReader r;
+    final LeafReader r;
     Object o = context.get("searcher");
     if (o instanceof SolrIndexSearcher) {
       SolrIndexSearcher is = (SolrIndexSearcher) o;
       SchemaField sf = is.getSchema().getFieldOrNull(field);
       if (sf != null && sf.hasDocValues() == false && sf.multiValued() == false && sf.getType().getNumericType() != null) {
         // its a single-valued numeric field: we must currently create insanity :(
-        List<AtomicReaderContext> leaves = is.getIndexReader().leaves();
-        AtomicReader insaneLeaves[] = new AtomicReader[leaves.size()];
+        List<LeafReaderContext> leaves = is.getIndexReader().leaves();
+        LeafReader insaneLeaves[] = new LeafReader[leaves.size()];
         int upto = 0;
-        for (AtomicReaderContext raw : leaves) {
+        for (LeafReaderContext raw : leaves) {
           insaneLeaves[upto++] = Insanity.wrapInsanity(raw.reader(), field);
         }
         r = SlowCompositeReaderWrapper.wrap(new MultiReader(insaneLeaves));
       } else {
         // reuse ordinalmap
-        r = ((SolrIndexSearcher)o).getAtomicReader();
+        r = ((SolrIndexSearcher)o).getLeafReader();
       }
     } else {
       IndexReader topReader = ReaderUtil.getTopLevelContext(readerContext).reader();
diff --git solr/core/src/java/org/apache/solr/search/function/ReverseOrdFieldSource.java solr/core/src/java/org/apache/solr/search/function/ReverseOrdFieldSource.java
index 6567735..7b6adbf 100644
--- solr/core/src/java/org/apache/solr/search/function/ReverseOrdFieldSource.java
+++ solr/core/src/java/org/apache/solr/search/function/ReverseOrdFieldSource.java
@@ -21,9 +21,8 @@ import java.io.IOException;
 import java.util.List;
 import java.util.Map;
 
-import org.apache.lucene.index.AtomicReader;
-import org.apache.lucene.index.AtomicReaderContext;
-import org.apache.lucene.index.CompositeReader;
+import org.apache.lucene.index.LeafReader;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.DocValues;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.MultiReader;
@@ -39,7 +38,7 @@ import org.apache.solr.search.Insanity;
 import org.apache.solr.search.SolrIndexSearcher;
 
 /**
- * Obtains the ordinal of the field value from {@link AtomicReader#getSortedDocValues}
+ * Obtains the ordinal of the field value from {@link org.apache.lucene.index.LeafReader#getSortedDocValues}
  * and reverses the order.
  * <br>
  * The native lucene index order is used to assign an ordinal value for each field value.
@@ -72,25 +71,25 @@ public class ReverseOrdFieldSource extends ValueSource {
   }
 
   @Override
-  public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
+  public FunctionValues getValues(Map context, LeafReaderContext readerContext) throws IOException {
     final int off = readerContext.docBase;
-    final AtomicReader r;
+    final LeafReader r;
     Object o = context.get("searcher");
     if (o instanceof SolrIndexSearcher) {
       SolrIndexSearcher is = (SolrIndexSearcher) o;
       SchemaField sf = is.getSchema().getFieldOrNull(field);
       if (sf != null && sf.hasDocValues() == false && sf.multiValued() == false && sf.getType().getNumericType() != null) {
         // its a single-valued numeric field: we must currently create insanity :(
-        List<AtomicReaderContext> leaves = is.getIndexReader().leaves();
-        AtomicReader insaneLeaves[] = new AtomicReader[leaves.size()];
+        List<LeafReaderContext> leaves = is.getIndexReader().leaves();
+        LeafReader insaneLeaves[] = new LeafReader[leaves.size()];
         int upto = 0;
-        for (AtomicReaderContext raw : leaves) {
+        for (LeafReaderContext raw : leaves) {
           insaneLeaves[upto++] = Insanity.wrapInsanity(raw.reader(), field);
         }
         r = SlowCompositeReaderWrapper.wrap(new MultiReader(insaneLeaves));
       } else {
         // reuse ordinalmap
-        r = ((SolrIndexSearcher)o).getAtomicReader();
+        r = ((SolrIndexSearcher)o).getLeafReader();
       }
     } else {
       IndexReader topReader = ReaderUtil.getTopLevelContext(readerContext).reader();
diff --git solr/core/src/java/org/apache/solr/search/function/ValueSourceRangeFilter.java solr/core/src/java/org/apache/solr/search/function/ValueSourceRangeFilter.java
index 206f2dd..87e1e5b 100644
--- solr/core/src/java/org/apache/solr/search/function/ValueSourceRangeFilter.java
+++ solr/core/src/java/org/apache/solr/search/function/ValueSourceRangeFilter.java
@@ -17,12 +17,13 @@
 
 package org.apache.solr.search.function;
 
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.search.DocIdSet;
 import org.apache.lucene.search.DocIdSetIterator;
 import org.apache.lucene.search.IndexSearcher;
 import org.apache.lucene.search.BitsFilteredDocIdSet;
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.util.Bits;
 import org.apache.solr.search.SolrFilter;
 
@@ -74,7 +75,7 @@ public class ValueSourceRangeFilter extends SolrFilter {
 
 
   @Override
-  public DocIdSet getDocIdSet(final Map context, final AtomicReaderContext readerContext, Bits acceptDocs) throws IOException {
+  public DocIdSet getDocIdSet(final Map context, final LeafReaderContext readerContext, Bits acceptDocs) throws IOException {
      return BitsFilteredDocIdSet.wrap(new DocIdSet() {
        @Override
        public DocIdSetIterator iterator() throws IOException {
diff --git solr/core/src/java/org/apache/solr/search/function/distance/GeohashFunction.java solr/core/src/java/org/apache/solr/search/function/distance/GeohashFunction.java
index 0be1dd4..85e0c3e 100644
--- solr/core/src/java/org/apache/solr/search/function/distance/GeohashFunction.java
+++ solr/core/src/java/org/apache/solr/search/function/distance/GeohashFunction.java
@@ -16,7 +16,7 @@ package org.apache.solr.search.function.distance;
  * limitations under the License.
  */
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 import com.spatial4j.core.io.GeohashUtils;
@@ -46,7 +46,7 @@ public class GeohashFunction extends ValueSource {
   }
 
   @Override
-  public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
+  public FunctionValues getValues(Map context, LeafReaderContext readerContext) throws IOException {
     final FunctionValues latDV = lat.getValues(context, readerContext);
     final FunctionValues lonDV = lon.getValues(context, readerContext);
 
diff --git solr/core/src/java/org/apache/solr/search/function/distance/GeohashHaversineFunction.java solr/core/src/java/org/apache/solr/search/function/distance/GeohashHaversineFunction.java
index 35e95d4..c73d955 100644
--- solr/core/src/java/org/apache/solr/search/function/distance/GeohashHaversineFunction.java
+++ solr/core/src/java/org/apache/solr/search/function/distance/GeohashHaversineFunction.java
@@ -22,7 +22,7 @@ import com.spatial4j.core.distance.DistanceUtils;
 import com.spatial4j.core.distance.GeodesicSphereDistCalc;
 import com.spatial4j.core.io.GeohashUtils;
 import com.spatial4j.core.shape.Point;
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.queries.function.docvalues.DoubleDocValues;
@@ -61,7 +61,7 @@ public class GeohashHaversineFunction extends ValueSource {
   }
 
   @Override
-  public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
+  public FunctionValues getValues(Map context, LeafReaderContext readerContext) throws IOException {
     final FunctionValues gh1DV = geoHash1.getValues(context, readerContext);
     final FunctionValues gh2DV = geoHash2.getValues(context, readerContext);
 
diff --git solr/core/src/java/org/apache/solr/search/function/distance/HaversineConstFunction.java solr/core/src/java/org/apache/solr/search/function/distance/HaversineConstFunction.java
index 3352720..03f98f3 100644
--- solr/core/src/java/org/apache/solr/search/function/distance/HaversineConstFunction.java
+++ solr/core/src/java/org/apache/solr/search/function/distance/HaversineConstFunction.java
@@ -17,7 +17,7 @@ package org.apache.solr.search.function.distance;
  */
 
 import com.spatial4j.core.distance.DistanceUtils;
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.queries.function.docvalues.DoubleDocValues;
@@ -57,7 +57,7 @@ public class HaversineConstFunction extends ValueSource {
   }
 
   @Override
-  public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
+  public FunctionValues getValues(Map context, LeafReaderContext readerContext) throws IOException {
     final FunctionValues latVals = latSource.getValues(context, readerContext);
     final FunctionValues lonVals = lonSource.getValues(context, readerContext);
     final double latCenterRad = this.latCenter * DEGREES_TO_RADIANS;
diff --git solr/core/src/java/org/apache/solr/search/function/distance/HaversineFunction.java solr/core/src/java/org/apache/solr/search/function/distance/HaversineFunction.java
index 7484a2a..c86683a 100644
--- solr/core/src/java/org/apache/solr/search/function/distance/HaversineFunction.java
+++ solr/core/src/java/org/apache/solr/search/function/distance/HaversineFunction.java
@@ -16,7 +16,7 @@ package org.apache.solr.search.function.distance;
  * limitations under the License.
  */
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.queries.function.docvalues.DoubleDocValues;
@@ -94,7 +94,7 @@ public class HaversineFunction extends ValueSource {
 
 
   @Override
-  public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
+  public FunctionValues getValues(Map context, LeafReaderContext readerContext) throws IOException {
     final FunctionValues vals1 = p1.getValues(context, readerContext);
 
     final FunctionValues vals2 = p2.getValues(context, readerContext);
diff --git solr/core/src/java/org/apache/solr/search/function/distance/StringDistanceFunction.java solr/core/src/java/org/apache/solr/search/function/distance/StringDistanceFunction.java
index f0bd4903..bf56c6e 100644
--- solr/core/src/java/org/apache/solr/search/function/distance/StringDistanceFunction.java
+++ solr/core/src/java/org/apache/solr/search/function/distance/StringDistanceFunction.java
@@ -17,7 +17,7 @@ package org.apache.solr.search.function.distance;
  * limitations under the License.
  */
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.queries.function.docvalues.FloatDocValues;
@@ -44,7 +44,7 @@ public class StringDistanceFunction extends ValueSource {
   }
 
   @Override
-  public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
+  public FunctionValues getValues(Map context, LeafReaderContext readerContext) throws IOException {
     final FunctionValues str1DV = str1.getValues(context, readerContext);
     final FunctionValues str2DV = str2.getValues(context, readerContext);
     return new FloatDocValues(this) {
diff --git solr/core/src/java/org/apache/solr/search/function/distance/VectorDistanceFunction.java solr/core/src/java/org/apache/solr/search/function/distance/VectorDistanceFunction.java
index e5cd8bc..670b896 100644
--- solr/core/src/java/org/apache/solr/search/function/distance/VectorDistanceFunction.java
+++ solr/core/src/java/org/apache/solr/search/function/distance/VectorDistanceFunction.java
@@ -16,7 +16,7 @@ package org.apache.solr.search.function.distance;
  * limitations under the License.
  */
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.queries.function.docvalues.DoubleDocValues;
@@ -150,7 +150,7 @@ public class VectorDistanceFunction extends ValueSource {
   }
 
   @Override
-  public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
+  public FunctionValues getValues(Map context, LeafReaderContext readerContext) throws IOException {
 
     final FunctionValues vals1 = source1.getValues(context, readerContext);
 
diff --git solr/core/src/java/org/apache/solr/search/grouping/collector/FilterCollector.java solr/core/src/java/org/apache/solr/search/grouping/collector/FilterCollector.java
index 3dd8545..45cc8e9 100644
--- solr/core/src/java/org/apache/solr/search/grouping/collector/FilterCollector.java
+++ solr/core/src/java/org/apache/solr/search/grouping/collector/FilterCollector.java
@@ -19,7 +19,7 @@ package org.apache.solr.search.grouping.collector;
 
 import java.io.IOException;
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.search.LeafCollector;
 import org.apache.lucene.search.Collector;
 import org.apache.lucene.search.FilterLeafCollector;
@@ -41,7 +41,7 @@ public class FilterCollector extends org.apache.lucene.search.FilterCollector {
   }
 
   @Override
-  public LeafCollector getLeafCollector(AtomicReaderContext context)
+  public LeafCollector getLeafCollector(LeafReaderContext context)
       throws IOException {
     final int docBase = context.docBase;
     return new FilterLeafCollector(super.getLeafCollector(context)) {
diff --git solr/core/src/java/org/apache/solr/search/join/IgnoreAcceptDocsQuery.java solr/core/src/java/org/apache/solr/search/join/IgnoreAcceptDocsQuery.java
index d61b4ce..d550889 100644
--- solr/core/src/java/org/apache/solr/search/join/IgnoreAcceptDocsQuery.java
+++ solr/core/src/java/org/apache/solr/search/join/IgnoreAcceptDocsQuery.java
@@ -17,7 +17,7 @@
 
 package org.apache.solr.search.join;
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.search.Explanation;
@@ -66,7 +66,7 @@ public class IgnoreAcceptDocsQuery extends Query {
     }
 
     @Override
-    public Explanation explain(AtomicReaderContext context, int doc) throws IOException {
+    public Explanation explain(LeafReaderContext context, int doc) throws IOException {
       return w.explain(context, doc);
     }
 
@@ -86,7 +86,7 @@ public class IgnoreAcceptDocsQuery extends Query {
     }
 
     @Override
-    public Scorer scorer(AtomicReaderContext context, Bits acceptDocs) throws IOException {
+    public Scorer scorer(LeafReaderContext context, Bits acceptDocs) throws IOException {
       return w.scorer(context, null);
     }
   }
diff --git solr/core/src/java/org/apache/solr/update/DeleteByQueryWrapper.java solr/core/src/java/org/apache/solr/update/DeleteByQueryWrapper.java
index 1c1fccc..68d888a 100644
--- solr/core/src/java/org/apache/solr/update/DeleteByQueryWrapper.java
+++ solr/core/src/java/org/apache/solr/update/DeleteByQueryWrapper.java
@@ -19,8 +19,8 @@ package org.apache.solr.update;
 
 import java.io.IOException;
 
-import org.apache.lucene.index.AtomicReader;
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReader;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.search.Explanation;
 import org.apache.lucene.search.IndexSearcher;
@@ -47,7 +47,7 @@ final class DeleteByQueryWrapper extends Query {
     this.schema = schema;
   }
   
-  AtomicReader wrap(AtomicReader reader) {
+  LeafReader wrap(LeafReader reader) {
     return new UninvertingReader(reader, schema.getUninversionMap(reader));
   }
   
@@ -65,12 +65,12 @@ final class DeleteByQueryWrapper extends Query {
   
   @Override
   public Weight createWeight(IndexSearcher searcher) throws IOException {
-    final AtomicReader wrapped = wrap((AtomicReader) searcher.getIndexReader());
+    final LeafReader wrapped = wrap((LeafReader) searcher.getIndexReader());
     final IndexSearcher privateContext = new IndexSearcher(wrapped);
     final Weight inner = in.createWeight(privateContext);
     return new Weight() {
       @Override
-      public Explanation explain(AtomicReaderContext context, int doc) throws IOException { throw new UnsupportedOperationException(); }
+      public Explanation explain(LeafReaderContext context, int doc) throws IOException { throw new UnsupportedOperationException(); }
 
       @Override
       public Query getQuery() { return DeleteByQueryWrapper.this; }
@@ -82,7 +82,7 @@ final class DeleteByQueryWrapper extends Query {
       public void normalize(float norm, float topLevelBoost) { inner.normalize(norm, topLevelBoost); }
 
       @Override
-      public Scorer scorer(AtomicReaderContext context, Bits acceptDocs) throws IOException {
+      public Scorer scorer(LeafReaderContext context, Bits acceptDocs) throws IOException {
         return inner.scorer(privateContext.getIndexReader().leaves().get(0), acceptDocs);
       }
     };
diff --git solr/core/src/java/org/apache/solr/update/SolrIndexSplitter.java solr/core/src/java/org/apache/solr/update/SolrIndexSplitter.java
index 7074b57..650c1a3 100644
--- solr/core/src/java/org/apache/solr/update/SolrIndexSplitter.java
+++ solr/core/src/java/org/apache/solr/update/SolrIndexSplitter.java
@@ -21,11 +21,11 @@ import java.io.IOException;
 import java.util.ArrayList;
 import java.util.List;
 
-import org.apache.lucene.index.AtomicReader;
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.FilterLeafReader;
+import org.apache.lucene.index.LeafReader;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.DocsEnum;
 import org.apache.lucene.index.Fields;
-import org.apache.lucene.index.FilterAtomicReader;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexWriter;
 import org.apache.lucene.index.Terms;
@@ -33,7 +33,6 @@ import org.apache.lucene.index.TermsEnum;
 import org.apache.lucene.search.DocIdSetIterator;
 import org.apache.lucene.util.Bits;
 import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.CharsRef;
 import org.apache.lucene.util.CharsRefBuilder;
 import org.apache.lucene.util.FixedBitSet;
 import org.apache.lucene.util.IOUtils;
@@ -90,12 +89,12 @@ public class SolrIndexSplitter {
 
   public void split() throws IOException {
 
-    List<AtomicReaderContext> leaves = searcher.getRawReader().leaves();
+    List<LeafReaderContext> leaves = searcher.getRawReader().leaves();
     List<FixedBitSet[]> segmentDocSets = new ArrayList<>(leaves.size());
 
     log.info("SolrIndexSplitter: partitions=" + numPieces + " segments="+leaves.size());
 
-    for (AtomicReaderContext readerContext : leaves) {
+    for (LeafReaderContext readerContext : leaves) {
       assert readerContext.ordInParent == segmentDocSets.size();  // make sure we're going in order
       FixedBitSet[] docSets = split(readerContext);
       segmentDocSets.add( docSets );
@@ -152,8 +151,8 @@ public class SolrIndexSplitter {
 
 
 
-  FixedBitSet[] split(AtomicReaderContext readerContext) throws IOException {
-    AtomicReader reader = readerContext.reader();
+  FixedBitSet[] split(LeafReaderContext readerContext) throws IOException {
+    LeafReader reader = readerContext.reader();
     FixedBitSet[] docSets = new FixedBitSet[numPieces];
     for (int i=0; i<docSets.length; i++) {
       docSets[i] = new FixedBitSet(reader.maxDoc());
@@ -233,11 +232,11 @@ public class SolrIndexSplitter {
 
 
   // change livedocs on the reader to delete those docs we don't want
-  static class LiveDocsReader extends FilterAtomicReader {
+  static class LiveDocsReader extends FilterLeafReader {
     final FixedBitSet liveDocs;
     final int numDocs;
 
-    public LiveDocsReader(AtomicReaderContext context, FixedBitSet liveDocs) throws IOException {
+    public LiveDocsReader(LeafReaderContext context, FixedBitSet liveDocs) throws IOException {
       super(context.reader());
       this.liveDocs = liveDocs;
       this.numDocs = liveDocs.cardinality();
diff --git solr/core/src/test/org/apache/solr/core/CountUsageValueSourceParser.java solr/core/src/test/org/apache/solr/core/CountUsageValueSourceParser.java
index fa8eb85..8dc75d5 100644
--- solr/core/src/test/org/apache/solr/core/CountUsageValueSourceParser.java
+++ solr/core/src/test/org/apache/solr/core/CountUsageValueSourceParser.java
@@ -16,12 +16,12 @@ package org.apache.solr.core;
  * limitations under the License.
  */
 
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.queries.function.docvalues.DoubleDocValues;
 import org.apache.lucene.queries.function.valuesource.DoubleConstValueSource;
-import org.apache.lucene.index.AtomicReaderContext;
-import org.apache.solr.common.util.NamedList;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.solr.search.FunctionQParser;
 import org.apache.solr.search.SyntaxError;
 import org.apache.solr.search.ValueSourceParser;
@@ -74,7 +74,7 @@ public class CountUsageValueSourceParser extends ValueSourceParser {
       this.counter = counter;
     }
     @Override
-    public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
+    public FunctionValues getValues(Map context, LeafReaderContext readerContext) throws IOException {
       return new DoubleDocValues(this) {
         @Override
         public double doubleVal(int doc) {
diff --git solr/core/src/test/org/apache/solr/core/TestMergePolicyConfig.java solr/core/src/test/org/apache/solr/core/TestMergePolicyConfig.java
index 2507ae6..068e26c 100644
--- solr/core/src/test/org/apache/solr/core/TestMergePolicyConfig.java
+++ solr/core/src/test/org/apache/solr/core/TestMergePolicyConfig.java
@@ -17,21 +17,16 @@ package org.apache.solr.core;
  * limitations under the License.
  */
 
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.solr.update.SolrIndexConfigTest;
 
 import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.AtomicReader;
-import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.SegmentReader;
-import org.apache.lucene.index.SegmentInfo;
 import org.apache.lucene.index.IndexWriterConfig;
-import org.apache.lucene.index.MergePolicy;
 import org.apache.lucene.index.TieredMergePolicy;
 import org.apache.lucene.index.LogMergePolicy;
 import org.apache.lucene.index.LogByteSizeMergePolicy;
 import org.apache.lucene.index.LogDocMergePolicy;
-import org.apache.lucene.index.ConcurrentMergeScheduler;
-import org.apache.solr.core.SolrCore;
 import org.apache.solr.util.RefCounted;
 import org.apache.solr.search.SolrIndexSearcher;
 import org.apache.solr.SolrTestCaseJ4;
@@ -213,7 +208,7 @@ public class TestMergePolicyConfig extends SolrTestCaseJ4 {
     assertNotNull("Null leaves", reader.leaves());
     assertTrue("no leaves", 0 < reader.leaves().size());
 
-    for (AtomicReaderContext atomic : reader.leaves()) {
+    for (LeafReaderContext atomic : reader.leaves()) {
       assertTrue("not a segment reader: " + atomic.reader().toString(), 
                  atomic.reader() instanceof SegmentReader);
       
diff --git solr/core/src/test/org/apache/solr/core/TestNRTOpen.java solr/core/src/test/org/apache/solr/core/TestNRTOpen.java
index 9ceca36..11807fc 100644
--- solr/core/src/test/org/apache/solr/core/TestNRTOpen.java
+++ solr/core/src/test/org/apache/solr/core/TestNRTOpen.java
@@ -19,11 +19,10 @@ package org.apache.solr.core;
 
 import java.io.File;
 import java.util.Collections;
-import java.util.HashSet;
 import java.util.IdentityHashMap;
 import java.util.Set;
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.DirectoryReader;
 import org.apache.solr.SolrTestCaseJ4;
 import org.apache.solr.search.SolrIndexSearcher;
@@ -142,7 +141,7 @@ public class TestNRTOpen extends SolrTestCaseJ4 {
     Set<Object> set = Collections.newSetFromMap(new IdentityHashMap<Object,Boolean>());
     try {
       DirectoryReader ir = searcher.get().getRawReader();
-      for (AtomicReaderContext context : ir.leaves()) {
+      for (LeafReaderContext context : ir.leaves()) {
         set.add(context.reader().getCoreCacheKey());
       }
     } finally {
diff --git solr/core/src/test/org/apache/solr/core/TestNonNRTOpen.java solr/core/src/test/org/apache/solr/core/TestNonNRTOpen.java
index a3bfd7d..eca1886 100644
--- solr/core/src/test/org/apache/solr/core/TestNonNRTOpen.java
+++ solr/core/src/test/org/apache/solr/core/TestNonNRTOpen.java
@@ -22,7 +22,7 @@ import java.util.Collections;
 import java.util.IdentityHashMap;
 import java.util.Set;
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.DirectoryReader;
 import org.apache.solr.SolrTestCaseJ4;
 import org.apache.solr.search.SolrIndexSearcher;
@@ -152,7 +152,7 @@ public class TestNonNRTOpen extends SolrTestCaseJ4 {
     Set<Object> set = Collections.newSetFromMap(new IdentityHashMap<Object,Boolean>());
     try {
       DirectoryReader ir = searcher.get().getRawReader();
-      for (AtomicReaderContext context : ir.leaves()) {
+      for (LeafReaderContext context : ir.leaves()) {
         set.add(context.reader().getCoreCacheKey());
       }
     } finally {
diff --git solr/core/src/test/org/apache/solr/request/TestFaceting.java solr/core/src/test/org/apache/solr/request/TestFaceting.java
index ea14884..30e0ea8 100644
--- solr/core/src/test/org/apache/solr/request/TestFaceting.java
+++ solr/core/src/test/org/apache/solr/request/TestFaceting.java
@@ -23,8 +23,6 @@ import java.util.Locale;
 import java.util.Random;
 
 import org.apache.lucene.index.DocValues;
-import org.apache.lucene.index.MultiDocValues;
-import org.apache.lucene.index.SortedDocValues;
 import org.apache.lucene.index.SortedSetDocValues;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.index.TermsEnum;
@@ -85,7 +83,7 @@ public class TestFaceting extends SolrTestCaseJ4 {
     createIndex(size);
     req = lrf.makeRequest("q","*:*");
 
-    SortedSetDocValues dv = DocValues.getSortedSet(req.getSearcher().getAtomicReader(), proto.field());
+    SortedSetDocValues dv = DocValues.getSortedSet(req.getSearcher().getLeafReader(), proto.field());
 
     assertEquals(size, dv.getValueCount());
 
@@ -766,16 +764,16 @@ public class TestFaceting extends SolrTestCaseJ4 {
     RefCounted<SolrIndexSearcher> currentSearcherRef = h.getCore().getSearcher();
     try {
       SolrIndexSearcher currentSearcher = currentSearcherRef.get();
-      SortedSetDocValues ui0 = DocValues.getSortedSet(currentSearcher.getAtomicReader(), "f0_ws");
-      SortedSetDocValues ui1 = DocValues.getSortedSet(currentSearcher.getAtomicReader(), "f1_ws");
-      SortedSetDocValues ui2 = DocValues.getSortedSet(currentSearcher.getAtomicReader(), "f2_ws");
-      SortedSetDocValues ui3 = DocValues.getSortedSet(currentSearcher.getAtomicReader(), "f3_ws");
-      SortedSetDocValues ui4 = DocValues.getSortedSet(currentSearcher.getAtomicReader(), "f4_ws");
-      SortedSetDocValues ui5 = DocValues.getSortedSet(currentSearcher.getAtomicReader(), "f5_ws");
-      SortedSetDocValues ui6 = DocValues.getSortedSet(currentSearcher.getAtomicReader(), "f6_ws");
-      SortedSetDocValues ui7 = DocValues.getSortedSet(currentSearcher.getAtomicReader(), "f7_ws");
-      SortedSetDocValues ui8 = DocValues.getSortedSet(currentSearcher.getAtomicReader(), "f8_ws");
-      SortedSetDocValues ui9 = DocValues.getSortedSet(currentSearcher.getAtomicReader(), "f9_ws");
+      SortedSetDocValues ui0 = DocValues.getSortedSet(currentSearcher.getLeafReader(), "f0_ws");
+      SortedSetDocValues ui1 = DocValues.getSortedSet(currentSearcher.getLeafReader(), "f1_ws");
+      SortedSetDocValues ui2 = DocValues.getSortedSet(currentSearcher.getLeafReader(), "f2_ws");
+      SortedSetDocValues ui3 = DocValues.getSortedSet(currentSearcher.getLeafReader(), "f3_ws");
+      SortedSetDocValues ui4 = DocValues.getSortedSet(currentSearcher.getLeafReader(), "f4_ws");
+      SortedSetDocValues ui5 = DocValues.getSortedSet(currentSearcher.getLeafReader(), "f5_ws");
+      SortedSetDocValues ui6 = DocValues.getSortedSet(currentSearcher.getLeafReader(), "f6_ws");
+      SortedSetDocValues ui7 = DocValues.getSortedSet(currentSearcher.getLeafReader(), "f7_ws");
+      SortedSetDocValues ui8 = DocValues.getSortedSet(currentSearcher.getLeafReader(), "f8_ws");
+      SortedSetDocValues ui9 = DocValues.getSortedSet(currentSearcher.getLeafReader(), "f9_ws");
 
       assertQ("check threading, more threads than fields",
           req("q", "id:*", "indent", "true", "fl", "id", "rows", "1"
diff --git solr/core/src/test/org/apache/solr/schema/DocValuesMultiTest.java solr/core/src/test/org/apache/solr/schema/DocValuesMultiTest.java
index 6386b3b..40d6659 100644
--- solr/core/src/test/org/apache/solr/schema/DocValuesMultiTest.java
+++ solr/core/src/test/org/apache/solr/schema/DocValuesMultiTest.java
@@ -17,7 +17,7 @@ package org.apache.solr.schema;
  * limitations under the License.
  */
 
-import org.apache.lucene.index.AtomicReader;
+import org.apache.lucene.index.LeafReader;
 import org.apache.lucene.index.FieldInfo.DocValuesType;
 import org.apache.lucene.index.FieldInfos;
 import org.apache.lucene.index.SortedSetDocValues;
@@ -48,7 +48,7 @@ public class DocValuesMultiTest extends SolrTestCaseJ4 {
       final RefCounted<SolrIndexSearcher> searcherRef = core.openNewSearcher(true, true);
       final SolrIndexSearcher searcher = searcherRef.get();
       try {
-        final AtomicReader reader = searcher.getAtomicReader();
+        final LeafReader reader = searcher.getLeafReader();
         assertEquals(1, reader.numDocs());
         final FieldInfos infos = reader.getFieldInfos();
         assertEquals(DocValuesType.SORTED_SET, infos.fieldInfo("stringdv").getDocValuesType());
diff --git solr/core/src/test/org/apache/solr/schema/DocValuesTest.java solr/core/src/test/org/apache/solr/schema/DocValuesTest.java
index a4693a8..3fc9f48 100644
--- solr/core/src/test/org/apache/solr/schema/DocValuesTest.java
+++ solr/core/src/test/org/apache/solr/schema/DocValuesTest.java
@@ -17,7 +17,7 @@ package org.apache.solr.schema;
  * limitations under the License.
  */
 
-import org.apache.lucene.index.AtomicReader;
+import org.apache.lucene.index.LeafReader;
 import org.apache.lucene.index.FieldInfo.DocValuesType;
 import org.apache.lucene.index.FieldInfos;
 import org.apache.lucene.queries.function.FunctionValues;
@@ -48,7 +48,7 @@ public class DocValuesTest extends SolrTestCaseJ4 {
       final RefCounted<SolrIndexSearcher> searcherRef = core.openNewSearcher(true, true);
       final SolrIndexSearcher searcher = searcherRef.get();
       try {
-        final AtomicReader reader = searcher.getAtomicReader();
+        final LeafReader reader = searcher.getLeafReader();
         assertEquals(1, reader.numDocs());
         final FieldInfos infos = reader.getFieldInfos();
         assertEquals(DocValuesType.NUMERIC, infos.fieldInfo("floatdv").getDocValuesType());
@@ -68,16 +68,16 @@ public class DocValuesTest extends SolrTestCaseJ4 {
         final SchemaField doubleDv = schema.getField("doubledv");
         final SchemaField longDv = schema.getField("longdv");
 
-        FunctionValues values = floatDv.getType().getValueSource(floatDv, null).getValues(null, searcher.getAtomicReader().leaves().get(0));
+        FunctionValues values = floatDv.getType().getValueSource(floatDv, null).getValues(null, searcher.getLeafReader().leaves().get(0));
         assertEquals(1f, values.floatVal(0), 0f);
         assertEquals(1f, values.objectVal(0));
-        values = intDv.getType().getValueSource(intDv, null).getValues(null, searcher.getAtomicReader().leaves().get(0));
+        values = intDv.getType().getValueSource(intDv, null).getValues(null, searcher.getLeafReader().leaves().get(0));
         assertEquals(2, values.intVal(0));
         assertEquals(2, values.objectVal(0));
-        values = doubleDv.getType().getValueSource(doubleDv, null).getValues(null, searcher.getAtomicReader().leaves().get(0));
+        values = doubleDv.getType().getValueSource(doubleDv, null).getValues(null, searcher.getLeafReader().leaves().get(0));
         assertEquals(3d, values.doubleVal(0), 0d);
         assertEquals(3d, values.objectVal(0));
-        values = longDv.getType().getValueSource(longDv, null).getValues(null, searcher.getAtomicReader().leaves().get(0));
+        values = longDv.getType().getValueSource(longDv, null).getValues(null, searcher.getLeafReader().leaves().get(0));
         assertEquals(4L, values.longVal(0));
         assertEquals(4L, values.objectVal(0));
       } finally {
diff --git solr/core/src/test/org/apache/solr/search/TestDocSet.java solr/core/src/test/org/apache/solr/search/TestDocSet.java
index e74f55c..669dbd9 100644
--- solr/core/src/test/org/apache/solr/search/TestDocSet.java
+++ solr/core/src/test/org/apache/solr/search/TestDocSet.java
@@ -22,8 +22,8 @@ import java.util.Arrays;
 import java.util.List;
 import java.util.Random;
 
-import org.apache.lucene.index.AtomicReader;
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReader;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.BinaryDocValues;
 import org.apache.lucene.index.FieldInfo;
 import org.apache.lucene.index.FieldInfos;
@@ -350,8 +350,8 @@ public class TestDocSet extends LuceneTestCase {
   }
   ***/
 
-  public AtomicReader dummyIndexReader(final int maxDoc) {
-    return new AtomicReader() {
+  public LeafReader dummyIndexReader(final int maxDoc) {
+    return new LeafReader() {
       @Override
       public int maxDoc() {
         return maxDoc;
@@ -508,10 +508,10 @@ public class TestDocSet extends LuceneTestCase {
 
     DocIdSet da;
     DocIdSet db;
-    List<AtomicReaderContext> leaves = topLevelContext.leaves();
+    List<LeafReaderContext> leaves = topLevelContext.leaves();
 
     // first test in-sequence sub readers
-    for (AtomicReaderContext readerContext : leaves) {
+    for (LeafReaderContext readerContext : leaves) {
       da = fa.getDocIdSet(readerContext, null);
       db = fb.getDocIdSet(readerContext, null);
       doTestIteratorEqual(da, db);
@@ -520,7 +520,7 @@ public class TestDocSet extends LuceneTestCase {
     int nReaders = leaves.size();
     // now test out-of-sequence sub readers
     for (int i=0; i<nReaders; i++) {
-      AtomicReaderContext readerContext = leaves.get(rand.nextInt(nReaders));
+      LeafReaderContext readerContext = leaves.get(rand.nextInt(nReaders));
       da = fa.getDocIdSet(readerContext, null);
       db = fb.getDocIdSet(readerContext, null);
       doTestIteratorEqual(da, db);
diff --git solr/core/src/test/org/apache/solr/search/TestIndexSearcher.java solr/core/src/test/org/apache/solr/search/TestIndexSearcher.java
index e62595e..9e8c11d 100644
--- solr/core/src/test/org/apache/solr/search/TestIndexSearcher.java
+++ solr/core/src/test/org/apache/solr/search/TestIndexSearcher.java
@@ -17,8 +17,9 @@
 package org.apache.solr.search;
 
 import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.LogDocMergePolicy;
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.IndexReaderContext;
 import org.apache.lucene.index.ReaderUtil;
 import org.apache.lucene.queries.function.FunctionValues;
@@ -58,9 +59,9 @@ public class TestIndexSearcher extends SolrTestCaseJ4 {
     Map context = ValueSource.newContext(sqr.getSearcher());
     vs.createWeight(context, sqr.getSearcher());
     IndexReaderContext topReaderContext = sqr.getSearcher().getTopReaderContext();
-    List<AtomicReaderContext> leaves = topReaderContext.leaves();
+    List<LeafReaderContext> leaves = topReaderContext.leaves();
     int idx = ReaderUtil.subIndex(doc, leaves);
-    AtomicReaderContext leaf = leaves.get(idx);
+    LeafReaderContext leaf = leaves.get(idx);
     FunctionValues vals = vs.getValues(context, leaf);
     return vals.strVal(doc-leaf.docBase);
   }
diff --git solr/core/src/test/org/apache/solr/search/TestRankQueryPlugin.java solr/core/src/test/org/apache/solr/search/TestRankQueryPlugin.java
index cea0185..2e9a11a 100644
--- solr/core/src/test/org/apache/solr/search/TestRankQueryPlugin.java
+++ solr/core/src/test/org/apache/solr/search/TestRankQueryPlugin.java
@@ -17,7 +17,7 @@
 
 package org.apache.solr.search;
 
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.DocValues;
 import org.apache.lucene.index.IndexReaderContext;
 import org.apache.lucene.index.NumericDocValues;
@@ -357,8 +357,8 @@ public class TestRankQueryPlugin extends QParserPlugin {
       if(fsv){
         NamedList<Object[]> sortVals = new NamedList<>(); // order is important for the sort fields
         IndexReaderContext topReaderContext = searcher.getTopReaderContext();
-        List<AtomicReaderContext> leaves = topReaderContext.leaves();
-        AtomicReaderContext currentLeaf = null;
+        List<LeafReaderContext> leaves = topReaderContext.leaves();
+        LeafReaderContext currentLeaf = null;
         if (leaves.size()==1) {
           // if there is a single segment, use that subReader and avoid looking up each time
           currentLeaf = leaves.get(0);
@@ -716,7 +716,7 @@ public class TestRankQueryPlugin extends QParserPlugin {
       return false;
     }
 
-    public void doSetNextReader(AtomicReaderContext context) throws IOException {
+    public void doSetNextReader(LeafReaderContext context) throws IOException {
       values = DocValues.getNumeric(context.reader(), "sort_i");
       base = context.docBase;
     }
@@ -770,7 +770,7 @@ public class TestRankQueryPlugin extends QParserPlugin {
       return false;
     }
 
-    public void doSetNextReader(AtomicReaderContext context) throws IOException {
+    public void doSetNextReader(LeafReaderContext context) throws IOException {
       base = context.docBase;
     }
 
diff --git solr/core/src/test/org/apache/solr/search/TestSort.java solr/core/src/test/org/apache/solr/search/TestSort.java
index 2faeea3..d6dac48 100644
--- solr/core/src/test/org/apache/solr/search/TestSort.java
+++ solr/core/src/test/org/apache/solr/search/TestSort.java
@@ -30,7 +30,7 @@ import org.apache.lucene.analysis.core.SimpleAnalyzer;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.StringField;
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.LeafReaderContext;
 import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexWriter;
 import org.apache.lucene.index.IndexWriterConfig;
@@ -235,7 +235,7 @@ public class TestSort extends SolrTestCaseJ4 {
       for (int i=0; i<qiter; i++) {
         Filter filt = new Filter() {
           @Override
-          public DocIdSet getDocIdSet(AtomicReaderContext context, Bits acceptDocs) {
+          public DocIdSet getDocIdSet(LeafReaderContext context, Bits acceptDocs) {
             return BitsFilteredDocIdSet.wrap(randSet(context.reader().maxDoc()), acceptDocs);
           }
         };
@@ -276,7 +276,7 @@ public class TestSort extends SolrTestCaseJ4 {
         Collector myCollector = new FilterCollector(topCollector) {
 
           @Override
-          public LeafCollector getLeafCollector(AtomicReaderContext context)
+          public LeafCollector getLeafCollector(LeafReaderContext context)
               throws IOException {
             final int docBase = context.docBase;
             return new FilterLeafCollector(super.getLeafCollector(context)) {
diff --git solr/core/src/test/org/apache/solr/update/DocumentBuilderTest.java solr/core/src/test/org/apache/solr/update/DocumentBuilderTest.java
index 6045731..f4479fc 100644
--- solr/core/src/test/org/apache/solr/update/DocumentBuilderTest.java
+++ solr/core/src/test/org/apache/solr/update/DocumentBuilderTest.java
@@ -19,7 +19,7 @@ package org.apache.solr.update;
 
 import org.apache.lucene.document.Document;
 import org.apache.lucene.index.IndexableField;
-import org.apache.lucene.index.AtomicReader;
+import org.apache.lucene.index.LeafReader;
 import org.apache.lucene.index.NumericDocValues;
 import org.apache.lucene.index.SlowCompositeReaderWrapper;
 import org.apache.lucene.search.similarities.DefaultSimilarity;
@@ -327,7 +327,7 @@ public class DocumentBuilderTest extends SolrTestCaseJ4 {
       int docid = dl.iterator().nextDoc();
 
       SolrIndexSearcher searcher = req.getSearcher();
-      AtomicReader reader = SlowCompositeReaderWrapper.wrap(searcher.getTopReaderContext().reader());
+      LeafReader reader = SlowCompositeReaderWrapper.wrap(searcher.getTopReaderContext().reader());
 
       assertTrue("similarity doesn't extend DefaultSimilarity, " + 
                  "config or defaults have changed since test was written",
