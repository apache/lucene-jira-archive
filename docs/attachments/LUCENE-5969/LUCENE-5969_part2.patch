Index: lucene/CHANGES.txt
===================================================================
--- lucene/CHANGES.txt	(revision 1629405)
+++ lucene/CHANGES.txt	(working copy)
@@ -73,6 +73,10 @@
 * LUCENE-5911: Add MemoryIndex.freeze() to allow thread-safe searching over a 
   MemoryIndex. (Alan Woodward)
 
+* LUCENE-5969: Lucene 5.0 has a new index format with mismatched file detection,
+  improved exception handling, and indirect norms encoding for sparse fields.
+  (Mike McCandless, Robert Muir)
+
 API Changes
 
 * LUCENE-5900: Deprecated more constructors taking Version in *InfixSuggester and
@@ -142,6 +146,11 @@
 * LUCENE-5924: Rename CheckIndex -fix option to -exorcise. This option does not
   actually fix the index, it just drops data.  (Robert Muir)
 
+* LUCENE-5969: Add Codec.compoundFormat, which handles the encoding of compound 
+  files. Add getMergeInstance() to codec producer APIs, which can be overridden
+  to return an instance optimized for merging instead of searching.
+  (Mike McCandless, Robert Muir)
+
 Bug Fixes
 
 * LUCENE-5650: Enforce read-only access to any path outside the temporary
@@ -193,6 +202,10 @@
   queries that match few documents by using a sparse bit set implementation.
   (Adrien Grand)
 
+* LUCENE-5969: Refactor merging to be more efficient, checksum calculation is
+  per-segment/per-producer, and norms and doc values merging no longer cause 
+  RAM spikes for latent fields. (Mike McCandless, Robert Muir)
+
 Build
 
 * LUCENE-5909: Smoke tester now has better command line parsing and
Index: lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/BitVector.java
===================================================================
--- lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/BitVector.java	(revision 1629405)
+++ lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/BitVector.java	(working copy)
@@ -23,7 +23,6 @@
 import org.apache.lucene.codecs.CodecUtil;
 import org.apache.lucene.index.IndexFormatTooOldException;
 import org.apache.lucene.store.ChecksumIndexInput;
-import org.apache.lucene.store.CompoundFileDirectory;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.IOContext;
 import org.apache.lucene.store.IndexInput;
@@ -31,19 +30,11 @@
 import org.apache.lucene.util.BitUtil;
 import org.apache.lucene.util.MutableBits;
 
-/** Optimized implementation of a vector of bits.  This is more-or-less like
- *  java.util.BitSet, but also includes the following:
- *  <ul>
- *  <li>a count() method, which efficiently computes the number of one bits;</li>
- *  <li>optimized read from and write to disk;</li>
- *  <li>inlinable get() method;</li>
- *  <li>store and load, as bit set or d-gaps, depending on sparseness;</li> 
- *  </ul>
- *
- *  @lucene.internal
+/** 
+ * Bitset for support of 4.x live documents
+ * @deprecated only for old 4.x segments
  */
-// pkg-private: if this thing is generally useful then it can go back in .util,
-// but the serialization must be here underneath the codec.
+@Deprecated
 final class BitVector implements Cloneable, MutableBits {
 
   private byte[] bits;
@@ -52,7 +43,7 @@
   private int version;
 
   /** Constructs a vector capable of holding <code>n</code> bits. */
-  public BitVector(int n) {
+  BitVector(int n) {
     size = n;
     bits = new byte[getNumBytes(size)];
     count = 0;
@@ -90,27 +81,6 @@
     count = -1;
   }
 
-  /** Sets the value of <code>bit</code> to true, and
-   *  returns true if bit was already set */
-  public final boolean getAndSet(int bit) {
-    if (bit >= size) {
-      throw new ArrayIndexOutOfBoundsException("bit=" + bit + " size=" + size);
-    }
-    final int pos = bit >> 3;
-    final int v = bits[pos];
-    final int flag = 1 << (bit & 7);
-    if ((flag & v) != 0)
-      return true;
-    else {
-      bits[pos] = (byte) (v | flag);
-      if (count != -1) {
-        count++;
-        assert count <= size;
-      }
-      return false;
-    }
-  }
-
   /** Sets the value of <code>bit</code> to zero. */
   @Override
   public final void clear(int bit) {
@@ -121,25 +91,6 @@
     count = -1;
   }
 
-  public final boolean getAndClear(int bit) {
-    if (bit >= size) {
-      throw new ArrayIndexOutOfBoundsException(bit);
-    }
-    final int pos = bit >> 3;
-    final int v = bits[pos];
-    final int flag = 1 << (bit & 7);
-    if ((flag & v) == 0) {
-      return false;
-    } else {
-      bits[pos] &= ~flag;
-      if (count != -1) {
-        count--;
-        assert count >= 0;
-      }
-      return true;
-    }
-  }
-
   /** Returns <code>true</code> if <code>bit</code> is one and
     <code>false</code> if it is zero. */
   @Override
@@ -150,7 +101,7 @@
 
   /** Returns the number of bits in this vector.  This is also one greater than
     the number of the largest valid bit number. */
-  public final int size() {
+  final int size() {
     return size;
   }
 
@@ -162,7 +113,7 @@
   /** Returns the total number of one bits in this vector.  This is efficiently
     computed and cached, so that, if the vector is not changed, no
     recomputation is done for repeated calls. */
-  public final int count() {
+  final int count() {
     // if the vector has been modified
     if (count == -1) {
       int c = 0;
@@ -177,7 +128,7 @@
   }
 
   /** For testing */
-  public final int getRecomputedCount() {
+  final int getRecomputedCount() {
     int c = 0;
     int end = bits.length;
     for (int i = 0; i < end; i++) {
@@ -191,22 +142,22 @@
   private static String CODEC = "BitVector";
 
   // Version before version tracking was added:
-  public final static int VERSION_PRE = -1;
+  final static int VERSION_PRE = -1;
 
   // First version:
-  public final static int VERSION_START = 0;
+  final static int VERSION_START = 0;
 
   // Changed DGaps to encode gaps between cleared bits, not
   // set:
-  public final static int VERSION_DGAPS_CLEARED = 1;
+  final static int VERSION_DGAPS_CLEARED = 1;
   
   // added checksum
-  public final static int VERSION_CHECKSUM = 2;
+  final static int VERSION_CHECKSUM = 2;
 
   // Increment version to change it:
-  public final static int VERSION_CURRENT = VERSION_CHECKSUM;
+  final static int VERSION_CURRENT = VERSION_CHECKSUM;
 
-  public int getVersion() {
+  int getVersion() {
     return version;
   }
 
@@ -213,8 +164,8 @@
   /** Writes this vector to the file <code>name</code> in Directory
     <code>d</code>, in a format that can be read by the constructor {@link
     #BitVector(Directory, String, IOContext)}.  */
-  public final void write(Directory d, String name, IOContext context) throws IOException {
-    assert !(d instanceof CompoundFileDirectory);
+  final void write(Directory d, String name, IOContext context) throws IOException {
+    assert !(d instanceof Lucene40CompoundReader);
     try (IndexOutput output = d.createOutput(name, context)) {
       output.writeInt(-2);
       CodecUtil.writeHeader(output, CODEC, VERSION_CURRENT);
@@ -230,7 +181,7 @@
   }
 
   /** Invert all bits */
-  public void invertAll() {
+  void invertAll() {
     if (count != -1) {
       count = size - count;
     }
@@ -254,13 +205,6 @@
     }
   }
 
-  /** Set all bits */
-  public void setAll() {
-    Arrays.fill(bits, (byte) 0xff);
-    clearUnusedBits();
-    count = size;
-  }
-     
   /** Write as a bit set */
   private void writeBits(IndexOutput output) throws IOException {
     output.writeInt(size());        // write size
@@ -325,7 +269,7 @@
   /** Constructs a bit vector from the file <code>name</code> in Directory
     <code>d</code>, as written by the {@link #write} method.
     */
-  public BitVector(Directory d, String name, IOContext context) throws IOException {
+  BitVector(Directory d, String name, IOContext context) throws IOException {
     try (ChecksumIndexInput input = d.openChecksumInput(name, context)) {
       final int firstInt = input.readInt();
 
Index: lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40Codec.java
===================================================================
--- lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40Codec.java	(revision 1629405)
+++ lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40Codec.java	(working copy)
@@ -18,8 +18,8 @@
  */
 
 import org.apache.lucene.codecs.Codec;
+import org.apache.lucene.codecs.CompoundFormat;
 import org.apache.lucene.codecs.FieldInfosFormat;
-import org.apache.lucene.codecs.FilterCodec;
 import org.apache.lucene.codecs.LiveDocsFormat;
 import org.apache.lucene.codecs.PostingsFormat;
 import org.apache.lucene.codecs.SegmentInfoFormat;
@@ -30,17 +30,9 @@
 import org.apache.lucene.codecs.perfield.PerFieldPostingsFormat;
 
 /**
- * Implements the Lucene 4.0 index format, with configurable per-field postings formats.
- * <p>
- * If you want to reuse functionality of this codec in another codec, extend
- * {@link FilterCodec}.
- *
- * @see org.apache.lucene.codecs.lucene40 package documentation for file format details.
+ * Reader for the 4.0 file format
  * @deprecated Only for reading old 4.0 segments
  */
-// NOTE: if we make largish changes in a minor release, easier to just make Lucene42Codec or whatever
-// if they are backwards compatible or smallish we can probably do the backwards in the postingsreader
-// (it writes a minor version, etc).
 @Deprecated
 public class Lucene40Codec extends Codec {
   private final StoredFieldsFormat fieldsFormat = new Lucene40StoredFieldsFormat();
@@ -48,6 +40,7 @@
   private final FieldInfosFormat fieldInfosFormat = new Lucene40FieldInfosFormat();
   private final SegmentInfoFormat infosFormat = new Lucene40SegmentInfoFormat();
   private final LiveDocsFormat liveDocsFormat = new Lucene40LiveDocsFormat();
+  private final CompoundFormat compoundFormat = new Lucene40CompoundFormat();
   
   private final PostingsFormat postingsFormat = new PerFieldPostingsFormat() {
     @Override
@@ -86,6 +79,11 @@
     return infosFormat;
   }
   
+  @Override
+  public CompoundFormat compoundFormat() {
+    return compoundFormat;
+  }
+
   private final DocValuesFormat defaultDVFormat = new Lucene40DocValuesFormat();
 
   @Override
Index: lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40CompoundFormat.java
===================================================================
--- lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40CompoundFormat.java	(revision 0)
+++ lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40CompoundFormat.java	(working copy)
@@ -0,0 +1,70 @@
+package org.apache.lucene.codecs.lucene40;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Collection;
+
+import org.apache.lucene.codecs.CompoundFormat;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.MergeState.CheckAbort;
+import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+
+/**
+ * Lucene 4.0 compound file format
+ * @deprecated only for reading old 4.x segments
+ */
+@Deprecated
+public final class Lucene40CompoundFormat extends CompoundFormat {
+  
+  /** Sole constructor. */
+  public Lucene40CompoundFormat() {
+  }
+
+  @Override
+  public Directory getCompoundReader(Directory dir, SegmentInfo si, IOContext context) throws IOException {
+    String fileName = IndexFileNames.segmentFileName(si.name, "", COMPOUND_FILE_EXTENSION);
+    return new Lucene40CompoundReader(dir, fileName, context, false);
+  }
+
+  @Override
+  public void write(Directory dir, SegmentInfo si, Collection<String> files, CheckAbort checkAbort, IOContext context) throws IOException {
+    String fileName = IndexFileNames.segmentFileName(si.name, "", COMPOUND_FILE_EXTENSION);
+    try (Directory cfs = new Lucene40CompoundReader(dir, fileName, context, true)) {
+      for (String file : files) {
+        dir.copy(cfs, file, file, context);
+        checkAbort.work(dir.fileLength(file));
+      }
+    }
+  }
+  
+  @Override
+  public String[] files(SegmentInfo si) {
+    return new String[] {
+      IndexFileNames.segmentFileName(si.name, "", COMPOUND_FILE_EXTENSION),
+      IndexFileNames.segmentFileName(si.name, "", COMPOUND_FILE_ENTRIES_EXTENSION)
+    };
+  }
+  
+  /** Extension of compound file */
+  static final String COMPOUND_FILE_EXTENSION = "cfs";
+  /** Extension of compound file entries */
+  static final String COMPOUND_FILE_ENTRIES_EXTENSION = "cfe";
+}

Property changes on: lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40CompoundFormat.java
___________________________________________________________________
Added: svn:eol-style
## -0,0 +1 ##
+native
\ No newline at end of property
Index: lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40CompoundReader.java
===================================================================
--- lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40CompoundReader.java	(revision 0)
+++ lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40CompoundReader.java	(working copy)
@@ -0,0 +1,251 @@
+package org.apache.lucene.codecs.lucene40;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.codecs.CodecUtil;
+import org.apache.lucene.index.CorruptIndexException;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.store.BaseDirectory;
+import org.apache.lucene.store.BufferedIndexInput;
+import org.apache.lucene.store.ChecksumIndexInput;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.store.Lock;
+import org.apache.lucene.util.IOUtils;
+
+import java.util.Collection;
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.Map;
+import java.io.FileNotFoundException;
+import java.io.IOException;
+
+/**
+ * Lucene 4.x compound file format
+ * @deprecated only for reading 4.x segments
+ */
+@Deprecated
+final class Lucene40CompoundReader extends BaseDirectory {
+  
+  // TODO: would be great to move this read-write stuff out of here into test.
+
+  /** Offset/Length for a slice inside of a compound file */
+  public static final class FileEntry {
+    long offset;
+    long length;
+  }
+  
+  private final Directory directory;
+  private final String fileName;
+  protected final int readBufferSize;  
+  private final Map<String,FileEntry> entries;
+  private final boolean openForWrite;
+  private static final Map<String,FileEntry> SENTINEL = Collections.emptyMap();
+  private final Lucene40CompoundWriter writer;
+  private final IndexInput handle;
+  private int version;
+  
+  /**
+   * Create a new CompoundFileDirectory.
+   */
+  public Lucene40CompoundReader(Directory directory, String fileName, IOContext context, boolean openForWrite) throws IOException {
+    this.directory = directory;
+    this.fileName = fileName;
+    this.readBufferSize = BufferedIndexInput.bufferSize(context);
+    this.isOpen = false;
+    this.openForWrite = openForWrite;
+    if (!openForWrite) {
+      boolean success = false;
+      handle = directory.openInput(fileName, context);
+      try {
+        this.entries = readEntries(directory, fileName);
+        if (version >= Lucene40CompoundWriter.VERSION_CHECKSUM) {
+          CodecUtil.checkHeader(handle, Lucene40CompoundWriter.DATA_CODEC, version, version);
+          // NOTE: data file is too costly to verify checksum against all the bytes on open,
+          // but for now we at least verify proper structure of the checksum footer: which looks
+          // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption
+          // such as file truncation.
+          CodecUtil.retrieveChecksum(handle);
+        }
+        success = true;
+      } finally {
+        if (!success) {
+          IOUtils.closeWhileHandlingException(handle);
+        }
+      }
+      this.isOpen = true;
+      writer = null;
+    } else {
+      assert !(directory instanceof Lucene40CompoundReader) : "compound file inside of compound file: " + fileName;
+      this.entries = SENTINEL;
+      this.isOpen = true;
+      writer = new Lucene40CompoundWriter(directory, fileName, context);
+      handle = null;
+    }
+  }
+
+  /** Helper method that reads CFS entries from an input stream */
+  private final Map<String, FileEntry> readEntries(Directory dir, String name) throws IOException {
+    ChecksumIndexInput entriesStream = null;
+    Map<String,FileEntry> mapping = null;
+    boolean success = false;
+    try {
+      final String entriesFileName = IndexFileNames.segmentFileName(
+                                            IndexFileNames.stripExtension(name), "",
+                                             Lucene40CompoundFormat.COMPOUND_FILE_ENTRIES_EXTENSION);
+      entriesStream = dir.openChecksumInput(entriesFileName, IOContext.READONCE);
+      version = CodecUtil.checkHeader(entriesStream, Lucene40CompoundWriter.ENTRY_CODEC, Lucene40CompoundWriter.VERSION_START, Lucene40CompoundWriter.VERSION_CURRENT);
+      final int numEntries = entriesStream.readVInt();
+      mapping = new HashMap<>(numEntries);
+      for (int i = 0; i < numEntries; i++) {
+        final FileEntry fileEntry = new FileEntry();
+        final String id = entriesStream.readString();
+        FileEntry previous = mapping.put(id, fileEntry);
+        if (previous != null) {
+          throw new CorruptIndexException("Duplicate cfs entry id=" + id + " in CFS ", entriesStream);
+        }
+        fileEntry.offset = entriesStream.readLong();
+        fileEntry.length = entriesStream.readLong();
+      }
+      if (version >= Lucene40CompoundWriter.VERSION_CHECKSUM) {
+        CodecUtil.checkFooter(entriesStream);
+      } else {
+        CodecUtil.checkEOF(entriesStream);
+      }
+      success = true;
+    } finally {
+      if (success) {
+        IOUtils.close(entriesStream);
+      } else {
+        IOUtils.closeWhileHandlingException(entriesStream);
+      }
+    }
+    return mapping;
+  }
+  
+  public Directory getDirectory() {
+    return directory;
+  }
+  
+  public String getName() {
+    return fileName;
+  }
+  
+  @Override
+  public synchronized void close() throws IOException {
+    if (!isOpen) {
+      // allow double close - usually to be consistent with other closeables
+      return; // already closed
+     }
+    isOpen = false;
+    if (writer != null) {
+      assert openForWrite;
+      writer.close();
+    } else {
+      IOUtils.close(handle);
+    }
+  }
+  
+  @Override
+  public synchronized IndexInput openInput(String name, IOContext context) throws IOException {
+    ensureOpen();
+    assert !openForWrite;
+    final String id = IndexFileNames.stripSegmentName(name);
+    final FileEntry entry = entries.get(id);
+    if (entry == null) {
+      throw new FileNotFoundException("No sub-file with id " + id + " found (fileName=" + name + " files: " + entries.keySet() + ")");
+    }
+    return handle.slice(name, entry.offset, entry.length);
+  }
+  
+  /** Returns an array of strings, one for each file in the directory. */
+  @Override
+  public String[] listAll() {
+    ensureOpen();
+    String[] res;
+    if (writer != null) {
+      res = writer.listAll(); 
+    } else {
+      res = entries.keySet().toArray(new String[entries.size()]);
+      // Add the segment name
+      String seg = IndexFileNames.parseSegmentName(fileName);
+      for (int i = 0; i < res.length; i++) {
+        res[i] = seg + res[i];
+      }
+    }
+    return res;
+  }
+  
+  /** Not implemented
+   * @throws UnsupportedOperationException always: not supported by CFS */
+  @Override
+  public void deleteFile(String name) {
+    throw new UnsupportedOperationException();
+  }
+  
+  /** Not implemented
+   * @throws UnsupportedOperationException always: not supported by CFS */
+  public void renameFile(String from, String to) {
+    throw new UnsupportedOperationException();
+  }
+  
+  /** Returns the length of a file in the directory.
+   * @throws IOException if the file does not exist */
+  @Override
+  public long fileLength(String name) throws IOException {
+    ensureOpen();
+    if (this.writer != null) {
+      return writer.fileLength(name);
+    }
+    FileEntry e = entries.get(IndexFileNames.stripSegmentName(name));
+    if (e == null)
+      throw new FileNotFoundException(name);
+    return e.length;
+  }
+  
+  @Override
+  public IndexOutput createOutput(String name, IOContext context) throws IOException {
+    ensureOpen();
+    if (!openForWrite) {
+      throw new UnsupportedOperationException();
+    }
+    return writer.createOutput(name, context);
+  }
+  
+  @Override
+  public void sync(Collection<String> names) {
+    throw new UnsupportedOperationException();
+  }
+  
+  @Override
+  public Lock makeLock(String name) {
+    throw new UnsupportedOperationException();
+  }
+  
+  @Override
+  public void clearLock(String name) throws IOException {
+    throw new UnsupportedOperationException();
+  }
+
+  @Override
+  public String toString() {
+    return "CompoundFileDirectory(file=\"" + fileName + "\" in dir=" + directory + ")";
+  }
+}

Property changes on: lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40CompoundReader.java
___________________________________________________________________
Added: svn:eol-style
## -0,0 +1 ##
+native
\ No newline at end of property
Index: lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40CompoundWriter.java
===================================================================
--- lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40CompoundWriter.java	(revision 0)
+++ lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40CompoundWriter.java	(working copy)
@@ -0,0 +1,360 @@
+package org.apache.lucene.codecs.lucene40;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.Closeable;
+import java.io.FileNotFoundException;
+import java.io.IOException;
+import java.util.Collection;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.LinkedList;
+import java.util.Map;
+import java.util.Queue;
+import java.util.Set;
+import java.util.concurrent.atomic.AtomicBoolean;
+
+import org.apache.lucene.codecs.CodecUtil;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.store.AlreadyClosedException;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.FlushInfo;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.util.IOUtils;
+
+/**
+ * Combines multiple files into a single compound file.
+ * @deprecated only for testing
+ */
+@Deprecated
+final class Lucene40CompoundWriter implements Closeable{
+
+  private static final class FileEntry {
+    /** source file */
+    String file;
+    long length;
+    /** temporary holder for the start of this file's data section */
+    long offset;
+    /** the directory which contains the file. */
+    Directory dir;
+  }
+
+  // versioning for the .cfs file
+  static final String DATA_CODEC = "CompoundFileWriterData";
+  static final int VERSION_START = 0;
+  static final int VERSION_CHECKSUM = 1;
+  static final int VERSION_CURRENT = VERSION_CHECKSUM;
+
+  // versioning for the .cfe file
+  static final String ENTRY_CODEC = "CompoundFileWriterEntries";
+
+  private final Directory directory;
+  private final Map<String, FileEntry> entries = new HashMap<>();
+  private final Set<String> seenIDs = new HashSet<>();
+  // all entries that are written to a sep. file but not yet moved into CFS
+  private final Queue<FileEntry> pendingEntries = new LinkedList<>();
+  private boolean closed = false;
+  private IndexOutput dataOut;
+  private final AtomicBoolean outputTaken = new AtomicBoolean(false);
+  final String entryTableName;
+  final String dataFileName;
+  
+  // preserve the IOContext we were originally passed
+  // previously this was not also passed to the .CFE
+  private final IOContext context;
+
+  /**
+   * Create the compound stream in the specified file. The file name is the
+   * entire name (no extensions are added).
+   * 
+   * @throws NullPointerException
+   *           if <code>dir</code> or <code>name</code> is null
+   */
+  Lucene40CompoundWriter(Directory dir, String name, IOContext context) {
+    if (dir == null)
+      throw new NullPointerException("directory cannot be null");
+    if (name == null)
+      throw new NullPointerException("name cannot be null");
+    directory = dir;
+    entryTableName = IndexFileNames.segmentFileName(
+        IndexFileNames.stripExtension(name), "",
+        Lucene40CompoundFormat.COMPOUND_FILE_ENTRIES_EXTENSION);
+    dataFileName = name;
+    this.context = context;
+  }
+  
+  private synchronized IndexOutput getOutput(IOContext context) throws IOException {
+    if (dataOut == null) {
+      boolean success = false;
+      try {
+        dataOut = directory.createOutput(dataFileName, this.context);
+        CodecUtil.writeHeader(dataOut, DATA_CODEC, VERSION_CURRENT);
+        success = true;
+      } finally {
+        if (!success) {
+          IOUtils.closeWhileHandlingException(dataOut);
+        }
+      }
+    } 
+    return dataOut;
+  }
+
+  /** Returns the directory of the compound file. */
+  Directory getDirectory() {
+    return directory;
+  }
+
+  /** Returns the name of the compound file. */
+  String getName() {
+    return dataFileName;
+  }
+
+  /**
+   * Closes all resources and writes the entry table
+   * 
+   * @throws IllegalStateException
+   *           if close() had been called before or if no file has been added to
+   *           this object
+   */
+  @Override
+  public void close() throws IOException {
+    if (closed) {
+      return;
+    }
+    IndexOutput entryTableOut = null;
+    // TODO this code should clean up after itself
+    // (remove partial .cfs/.cfe)
+    boolean success = false;
+    try {
+      if (!pendingEntries.isEmpty() || outputTaken.get()) {
+        throw new IllegalStateException("CFS has pending open files");
+      }
+      closed = true;
+      getOutput(this.context);
+      assert dataOut != null;
+      CodecUtil.writeFooter(dataOut);
+      success = true;
+    } finally {
+      if (success) {
+        IOUtils.close(dataOut);
+      } else {
+        IOUtils.closeWhileHandlingException(dataOut);
+      }
+    }
+    success = false;
+    try {
+      entryTableOut = directory.createOutput(entryTableName, this.context);
+      writeEntryTable(entries.values(), entryTableOut);
+      success = true;
+    } finally {
+      if (success) {
+        IOUtils.close(entryTableOut);
+      } else {
+        IOUtils.closeWhileHandlingException(entryTableOut);
+      }
+    }
+  }
+
+  private final void ensureOpen() {
+    if (closed) {
+      throw new AlreadyClosedException("CFS Directory is already closed");
+    }
+  }
+
+  /**
+   * Copy the contents of the file with specified extension into the provided
+   * output stream.
+   */
+  private final long copyFileEntry(IndexOutput dataOut, FileEntry fileEntry)
+      throws IOException {
+    final IndexInput is = fileEntry.dir.openInput(fileEntry.file, IOContext.READONCE);
+    boolean success = false;
+    try {
+      final long startPtr = dataOut.getFilePointer();
+      final long length = fileEntry.length;
+      dataOut.copyBytes(is, length);
+      // Verify that the output length diff is equal to original file
+      long endPtr = dataOut.getFilePointer();
+      long diff = endPtr - startPtr;
+      if (diff != length)
+        throw new IOException("Difference in the output file offsets " + diff
+            + " does not match the original file length " + length);
+      fileEntry.offset = startPtr;
+      success = true;
+      return length;
+    } finally {
+      if (success) {
+        IOUtils.close(is);
+        // copy successful - delete file
+        // if we can't we rely on IFD to pick up and retry
+        IOUtils.deleteFilesIgnoringExceptions(fileEntry.dir, fileEntry.file);
+      } else {
+        IOUtils.closeWhileHandlingException(is);
+      }
+    }
+  }
+
+  protected void writeEntryTable(Collection<FileEntry> entries,
+      IndexOutput entryOut) throws IOException {
+    CodecUtil.writeHeader(entryOut, ENTRY_CODEC, VERSION_CURRENT);
+    entryOut.writeVInt(entries.size());
+    for (FileEntry fe : entries) {
+      entryOut.writeString(IndexFileNames.stripSegmentName(fe.file));
+      entryOut.writeLong(fe.offset);
+      entryOut.writeLong(fe.length);
+    }
+    CodecUtil.writeFooter(entryOut);
+  }
+
+  IndexOutput createOutput(String name, IOContext context) throws IOException {
+    ensureOpen();
+    boolean success = false;
+    boolean outputLocked = false;
+    try {
+      assert name != null : "name must not be null";
+      if (entries.containsKey(name)) {
+        throw new IllegalArgumentException("File " + name + " already exists");
+      }
+      final FileEntry entry = new FileEntry();
+      entry.file = name;
+      entries.put(name, entry);
+      final String id = IndexFileNames.stripSegmentName(name);
+      assert !seenIDs.contains(id): "file=\"" + name + "\" maps to id=\"" + id + "\", which was already written";
+      seenIDs.add(id);
+      final DirectCFSIndexOutput out;
+
+      if ((outputLocked = outputTaken.compareAndSet(false, true))) {
+        out = new DirectCFSIndexOutput(getOutput(this.context), entry, false);
+      } else {
+        entry.dir = this.directory;
+        out = new DirectCFSIndexOutput(directory.createOutput(name, this.context), entry,
+            true);
+      }
+      success = true;
+      return out;
+    } finally {
+      if (!success) {
+        entries.remove(name);
+        if (outputLocked) { // release the output lock if not successful
+          assert outputTaken.get();
+          releaseOutputLock();
+        }
+      }
+    }
+  }
+
+  final void releaseOutputLock() {
+    outputTaken.compareAndSet(true, false);
+  }
+
+  private final void prunePendingEntries() throws IOException {
+    // claim the output and copy all pending files in
+    if (outputTaken.compareAndSet(false, true)) {
+      try {
+        while (!pendingEntries.isEmpty()) {
+          FileEntry entry = pendingEntries.poll();
+          copyFileEntry(getOutput(this.context), entry);
+          entries.put(entry.file, entry);
+        }
+      } finally {
+        final boolean compareAndSet = outputTaken.compareAndSet(true, false);
+        assert compareAndSet;
+      }
+    }
+  }
+
+  long fileLength(String name) throws IOException {
+    FileEntry fileEntry = entries.get(name);
+    if (fileEntry == null) {
+      throw new FileNotFoundException(name + " does not exist");
+    }
+    return fileEntry.length;
+  }
+
+  boolean fileExists(String name) {
+    return entries.containsKey(name);
+  }
+
+  String[] listAll() {
+    return entries.keySet().toArray(new String[0]);
+  }
+
+  private final class DirectCFSIndexOutput extends IndexOutput {
+    private final IndexOutput delegate;
+    private final long offset;
+    private boolean closed;
+    private FileEntry entry;
+    private long writtenBytes;
+    private final boolean isSeparate;
+
+    DirectCFSIndexOutput(IndexOutput delegate, FileEntry entry,
+        boolean isSeparate) {
+      super();
+      this.delegate = delegate;
+      this.entry = entry;
+      entry.offset = offset = delegate.getFilePointer();
+      this.isSeparate = isSeparate;
+
+    }
+
+    @Override
+    public void close() throws IOException {
+      if (!closed) {
+        closed = true;
+        entry.length = writtenBytes;
+        if (isSeparate) {
+          delegate.close();
+          // we are a separate file - push into the pending entries
+          pendingEntries.add(entry);
+        } else {
+          // we have been written into the CFS directly - release the lock
+          releaseOutputLock();
+        }
+        // now prune all pending entries and push them into the CFS
+        prunePendingEntries();
+      }
+    }
+
+    @Override
+    public long getFilePointer() {
+      return delegate.getFilePointer() - offset;
+    }
+
+    @Override
+    public void writeByte(byte b) throws IOException {
+      assert !closed;
+      writtenBytes++;
+      delegate.writeByte(b);
+    }
+
+    @Override
+    public void writeBytes(byte[] b, int offset, int length) throws IOException {
+      assert !closed;
+      writtenBytes += length;
+      delegate.writeBytes(b, offset, length);
+    }
+
+    @Override
+    public long getChecksum() throws IOException {
+      return delegate.getChecksum();
+    }
+  }
+
+}

Property changes on: lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40CompoundWriter.java
___________________________________________________________________
Added: svn:eol-style
## -0,0 +1 ##
+native
\ No newline at end of property
Index: lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40DocValuesFormat.java
===================================================================
--- lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40DocValuesFormat.java	(revision 1629405)
+++ lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40DocValuesFormat.java	(working copy)
@@ -19,7 +19,6 @@
 
 import java.io.IOException;
 
-import org.apache.lucene.codecs.CodecUtil;
 import org.apache.lucene.codecs.DocValuesConsumer;
 import org.apache.lucene.codecs.DocValuesFormat;
 import org.apache.lucene.codecs.DocValuesProducer;
@@ -26,112 +25,16 @@
 import org.apache.lucene.index.IndexFileNames;
 import org.apache.lucene.index.SegmentReadState;
 import org.apache.lucene.index.SegmentWriteState;
-import org.apache.lucene.store.CompoundFileDirectory;
-import org.apache.lucene.store.DataOutput;
-import org.apache.lucene.util.packed.PackedInts;
 
 /**
  * Lucene 4.0 DocValues format.
- * <p>
- * Files:
- * <ul>
- *   <li><tt>.dv.cfs</tt>: {@link CompoundFileDirectory compound container}</li>
- *   <li><tt>.dv.cfe</tt>: {@link CompoundFileDirectory compound entries}</li>
- * </ul>
- * Entries within the compound file:
- * <ul>
- *   <li><tt>&lt;segment&gt;_&lt;fieldNumber&gt;.dat</tt>: data values</li>
- *   <li><tt>&lt;segment&gt;_&lt;fieldNumber&gt;.idx</tt>: index into the .dat for DEREF types</li>
- * </ul>
- * <p>
- * There are several many types of {@code DocValues} with different encodings.
- * From the perspective of filenames, all types store their values in <tt>.dat</tt>
- * entries within the compound file. In the case of dereferenced/sorted types, the <tt>.dat</tt>
- * actually contains only the unique values, and an additional <tt>.idx</tt> file contains
- * pointers to these unique values.
- * </p>
- * Formats:
- * <ul>
- *    <li>{@code VAR_INTS} .dat --&gt; Header, PackedType, MinValue, 
- *        DefaultValue, PackedStream</li>
- *    <li>{@code FIXED_INTS_8} .dat --&gt; Header, ValueSize, 
- *        {@link DataOutput#writeByte Byte}<sup>maxdoc</sup></li>
- *    <li>{@code FIXED_INTS_16} .dat --&gt; Header, ValueSize,
- *        {@link DataOutput#writeShort Short}<sup>maxdoc</sup></li>
- *    <li>{@code FIXED_INTS_32} .dat --&gt; Header, ValueSize,
- *        {@link DataOutput#writeInt Int32}<sup>maxdoc</sup></li>
- *    <li>{@code FIXED_INTS_64} .dat --&gt; Header, ValueSize,
- *        {@link DataOutput#writeLong Int64}<sup>maxdoc</sup></li>
- *    <li>{@code FLOAT_32} .dat --&gt; Header, ValueSize, Float32<sup>maxdoc</sup></li>
- *    <li>{@code FLOAT_64} .dat --&gt; Header, ValueSize, Float64<sup>maxdoc</sup></li>
- *    <li>{@code BYTES_FIXED_STRAIGHT} .dat --&gt; Header, ValueSize,
- *        ({@link DataOutput#writeByte Byte} * ValueSize)<sup>maxdoc</sup></li>
- *    <li>{@code BYTES_VAR_STRAIGHT} .idx --&gt; Header, TotalBytes, Addresses</li>
- *    <li>{@code BYTES_VAR_STRAIGHT} .dat --&gt; Header,
-          ({@link DataOutput#writeByte Byte} * <i>variable ValueSize</i>)<sup>maxdoc</sup></li>
- *    <li>{@code BYTES_FIXED_DEREF} .idx --&gt; Header, NumValues, Addresses</li>
- *    <li>{@code BYTES_FIXED_DEREF} .dat --&gt; Header, ValueSize,
- *        ({@link DataOutput#writeByte Byte} * ValueSize)<sup>NumValues</sup></li>
- *    <li>{@code BYTES_VAR_DEREF} .idx --&gt; Header, TotalVarBytes, Addresses</li>
- *    <li>{@code BYTES_VAR_DEREF} .dat --&gt; Header,
- *        (LengthPrefix + {@link DataOutput#writeByte Byte} * <i>variable ValueSize</i>)<sup>NumValues</sup></li>
- *    <li>{@code BYTES_FIXED_SORTED} .idx --&gt; Header, NumValues, Ordinals</li>
- *    <li>{@code BYTES_FIXED_SORTED} .dat --&gt; Header, ValueSize,
- *        ({@link DataOutput#writeByte Byte} * ValueSize)<sup>NumValues</sup></li>
- *    <li>{@code BYTES_VAR_SORTED} .idx --&gt; Header, TotalVarBytes, Addresses, Ordinals</li>
- *    <li>{@code BYTES_VAR_SORTED} .dat --&gt; Header,
- *        ({@link DataOutput#writeByte Byte} * <i>variable ValueSize</i>)<sup>NumValues</sup></li>
- * </ul>
- * Data Types:
- * <ul>
- *    <li>Header --&gt; {@link CodecUtil#writeHeader CodecHeader}</li>
- *    <li>PackedType --&gt; {@link DataOutput#writeByte Byte}</li>
- *    <li>MaxAddress, MinValue, DefaultValue --&gt; {@link DataOutput#writeLong Int64}</li>
- *    <li>PackedStream, Addresses, Ordinals --&gt; {@link PackedInts}</li>
- *    <li>ValueSize, NumValues --&gt; {@link DataOutput#writeInt Int32}</li>
- *    <li>Float32 --&gt; 32-bit float encoded with {@link Float#floatToRawIntBits(float)}
- *                       then written as {@link DataOutput#writeInt Int32}</li>
- *    <li>Float64 --&gt; 64-bit float encoded with {@link Double#doubleToRawLongBits(double)}
- *                       then written as {@link DataOutput#writeLong Int64}</li>
- *    <li>TotalBytes --&gt; {@link DataOutput#writeVLong VLong}</li>
- *    <li>TotalVarBytes --&gt; {@link DataOutput#writeLong Int64}</li>
- *    <li>LengthPrefix --&gt; Length of the data value as {@link DataOutput#writeVInt VInt} (maximum
- *                       of 2 bytes)</li>
- * </ul>
- * Notes:
- * <ul>
- *    <li>PackedType is a 0 when compressed, 1 when the stream is written as 64-bit integers.</li>
- *    <li>Addresses stores pointers to the actual byte location (indexed by docid). In the VAR_STRAIGHT
- *        case, each entry can have a different length, so to determine the length, docid+1 is 
- *        retrieved. A sentinel address is written at the end for the VAR_STRAIGHT case, so the Addresses 
- *        stream contains maxdoc+1 indices. For the deduplicated VAR_DEREF case, each length
- *        is encoded as a prefix to the data itself as a {@link DataOutput#writeVInt VInt} 
- *        (maximum of 2 bytes).</li>
- *    <li>Ordinals stores the term ID in sorted order (indexed by docid). In the FIXED_SORTED case,
- *        the address into the .dat can be computed from the ordinal as 
- *        <code>Header+ValueSize+(ordinal*ValueSize)</code> because the byte length is fixed.
- *        In the VAR_SORTED case, there is double indirection (docid -> ordinal -> address), but
- *        an additional sentinel ordinal+address is always written (so there are NumValues+1 ordinals). To
- *        determine the length, ord+1's address is looked up as well.</li>
- *    <li>{@code BYTES_VAR_STRAIGHT BYTES_VAR_STRAIGHT} in contrast to other straight 
- *        variants uses a <tt>.idx</tt> file to improve lookup perfromance. In contrast to 
- *        {@code BYTES_VAR_DEREF BYTES_VAR_DEREF} it doesn't apply deduplication of the document values.
- *    </li>
- * </ul>
- * <p>
- * Limitations:
- * <ul>
- *   <li> Binary doc values can be at most {@link #MAX_BINARY_FIELD_LENGTH} in length.
- * </ul>
  * @deprecated Only for reading old 4.0 and 4.1 segments
  */
 @Deprecated
-// NOTE: not registered in SPI, doesnt respect segment suffix, etc
-// for back compat only!
 public class Lucene40DocValuesFormat extends DocValuesFormat {
   
   /** Maximum length for each binary doc values field. */
-  public static final int MAX_BINARY_FIELD_LENGTH = (1 << 15) - 2;
+  static final int MAX_BINARY_FIELD_LENGTH = (1 << 15) - 2;
   
   /** Sole constructor. */
   public Lucene40DocValuesFormat() {
@@ -144,10 +47,10 @@
   }
   
   @Override
-  public DocValuesProducer fieldsProducer(SegmentReadState state) throws IOException {
+  public final DocValuesProducer fieldsProducer(SegmentReadState state) throws IOException {
     String filename = IndexFileNames.segmentFileName(state.segmentInfo.name, 
                                                      "dv", 
-                                                     IndexFileNames.COMPOUND_FILE_EXTENSION);
+                                                     Lucene40CompoundFormat.COMPOUND_FILE_EXTENSION);
     return new Lucene40DocValuesReader(state, filename, Lucene40FieldInfosReader.LEGACY_DV_TYPE_KEY);
   }
   
Index: lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40DocValuesReader.java
===================================================================
--- lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40DocValuesReader.java	(revision 1629405)
+++ lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40DocValuesReader.java	(working copy)
@@ -34,7 +34,6 @@
 import org.apache.lucene.index.SortedDocValues;
 import org.apache.lucene.index.SortedNumericDocValues;
 import org.apache.lucene.index.SortedSetDocValues;
-import org.apache.lucene.store.CompoundFileDirectory;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.IndexInput;
 import org.apache.lucene.util.Accountable;
@@ -48,7 +47,6 @@
 
 /**
  * Reads the 4.0 format of norms/docvalues
- * @lucene.experimental
  * @deprecated Only for reading old 4.0 and 4.1 segments
  */
 @Deprecated
@@ -66,12 +64,29 @@
   private final Map<String,Accountable> instanceInfo = new HashMap<>();
 
   private final AtomicLong ramBytesUsed;
+  
+  private final boolean merging;
+  
+  // clone for merge: when merging we don't do any instances.put()s
+  Lucene40DocValuesReader(Lucene40DocValuesReader original) throws IOException {
+    assert Thread.holdsLock(original);
+    dir = original.dir;
+    state = original.state;
+    legacyKey = original.legacyKey;
+    numericInstances.putAll(original.numericInstances);
+    binaryInstances.putAll(original.binaryInstances);
+    sortedInstances.putAll(original.sortedInstances);
+    instanceInfo.putAll(original.instanceInfo);
+    ramBytesUsed = new AtomicLong(original.ramBytesUsed.get());
+    merging = true;
+  }
 
   Lucene40DocValuesReader(SegmentReadState state, String filename, String legacyKey) throws IOException {
     this.state = state;
     this.legacyKey = legacyKey;
-    this.dir = new CompoundFileDirectory(state.directory, filename, state.context, false);
+    this.dir = new Lucene40CompoundReader(state.directory, filename, state.context, false);
     ramBytesUsed = new AtomicLong(RamUsageEstimator.shallowSizeOf(getClass()));
+    merging = false;
   }
 
   @Override
@@ -116,7 +131,9 @@
           IOUtils.closeWhileHandlingException(input);
         }
       }
-      numericInstances.put(field.name, instance);
+      if (!merging) {
+        numericInstances.put(field.name, instance);
+      }
     }
     return instance;
   }
@@ -133,8 +150,10 @@
         values[i] = input.readLong();
       }
       long bytesUsed = RamUsageEstimator.sizeOf(values);
-      instanceInfo.put(field.name, Accountables.namedAccountable("long array", bytesUsed));
-      ramBytesUsed.addAndGet(bytesUsed);
+      if (!merging) {
+        instanceInfo.put(field.name, Accountables.namedAccountable("long array", bytesUsed));
+        ramBytesUsed.addAndGet(bytesUsed);
+      }
       return new NumericDocValues() {
         @Override
         public long get(int docID) {
@@ -145,8 +164,10 @@
       final long minValue = input.readLong();
       final long defaultValue = input.readLong();
       final PackedInts.Reader reader = PackedInts.getReader(input);
-      instanceInfo.put(field.name, reader);
-      ramBytesUsed.addAndGet(reader.ramBytesUsed());
+      if (!merging) {
+        instanceInfo.put(field.name, reader);
+        ramBytesUsed.addAndGet(reader.ramBytesUsed());
+      }
       return new NumericDocValues() {
         @Override
         public long get(int docID) {
@@ -175,8 +196,10 @@
     final byte values[] = new byte[maxDoc];
     input.readBytes(values, 0, values.length);
     long bytesUsed = RamUsageEstimator.sizeOf(values);
-    instanceInfo.put(field.name, Accountables.namedAccountable("byte array", bytesUsed));
-    ramBytesUsed.addAndGet(bytesUsed);
+    if (!merging) {
+      instanceInfo.put(field.name, Accountables.namedAccountable("byte array", bytesUsed));
+      ramBytesUsed.addAndGet(bytesUsed);
+    }
     return new NumericDocValues() {
       @Override
       public long get(int docID) {
@@ -199,8 +222,10 @@
       values[i] = input.readShort();
     }
     long bytesUsed = RamUsageEstimator.sizeOf(values);
-    instanceInfo.put(field.name, Accountables.namedAccountable("short array", bytesUsed));
-    ramBytesUsed.addAndGet(bytesUsed);
+    if (!merging) {
+      instanceInfo.put(field.name, Accountables.namedAccountable("short array", bytesUsed));
+      ramBytesUsed.addAndGet(bytesUsed);
+    }
     return new NumericDocValues() {
       @Override
       public long get(int docID) {
@@ -223,8 +248,10 @@
       values[i] = input.readInt();
     }
     long bytesUsed = RamUsageEstimator.sizeOf(values);
-    instanceInfo.put(field.name, Accountables.namedAccountable("int array", bytesUsed));
-    ramBytesUsed.addAndGet(bytesUsed);
+    if (!merging) {
+      instanceInfo.put(field.name, Accountables.namedAccountable("int array", bytesUsed));
+      ramBytesUsed.addAndGet(bytesUsed);
+    }
     return new NumericDocValues() {
       @Override
       public long get(int docID) {
@@ -247,8 +274,10 @@
       values[i] = input.readLong();
     }
     long bytesUsed = RamUsageEstimator.sizeOf(values);
-    instanceInfo.put(field.name, Accountables.namedAccountable("long array", bytesUsed));
-    ramBytesUsed.addAndGet(bytesUsed);
+    if (!merging) {
+      instanceInfo.put(field.name, Accountables.namedAccountable("long array", bytesUsed));
+      ramBytesUsed.addAndGet(bytesUsed);
+    }
     return new NumericDocValues() {
       @Override
       public long get(int docID) {
@@ -271,8 +300,10 @@
       values[i] = input.readInt();
     }
     long bytesUsed = RamUsageEstimator.sizeOf(values);
-    instanceInfo.put(field.name, Accountables.namedAccountable("float array", bytesUsed));
-    ramBytesUsed.addAndGet(bytesUsed);
+    if (!merging) {
+      instanceInfo.put(field.name, Accountables.namedAccountable("float array", bytesUsed));
+      ramBytesUsed.addAndGet(bytesUsed);
+    }
     return new NumericDocValues() {
       @Override
       public long get(int docID) {
@@ -295,8 +326,10 @@
       values[i] = input.readLong();
     }
     long bytesUsed = RamUsageEstimator.sizeOf(values);
-    instanceInfo.put(field.name, Accountables.namedAccountable("double array", bytesUsed));
-    ramBytesUsed.addAndGet(bytesUsed);
+    if (!merging) {
+      instanceInfo.put(field.name, Accountables.namedAccountable("double array", bytesUsed));
+      ramBytesUsed.addAndGet(bytesUsed);
+    }
     return new NumericDocValues() {
       @Override
       public long get(int docID) {
@@ -325,7 +358,9 @@
         default:
           throw new AssertionError();
       }
-      binaryInstances.put(field.name, instance);
+      if (!merging) {
+        binaryInstances.put(field.name, instance);
+      }
     }
     return instance;
   }
@@ -344,8 +379,10 @@
       final PagedBytes.Reader bytesReader = bytes.freeze(true);
       CodecUtil.checkEOF(input);
       success = true;
-      ramBytesUsed.addAndGet(bytesReader.ramBytesUsed());
-      instanceInfo.put(field.name, bytesReader);
+      if (!merging) {
+        ramBytesUsed.addAndGet(bytesReader.ramBytesUsed());
+        instanceInfo.put(field.name, bytesReader);
+      }
       return new BinaryDocValues() {
 
         @Override
@@ -388,8 +425,10 @@
       CodecUtil.checkEOF(index);
       success = true;
       long bytesUsed = bytesReader.ramBytesUsed() + reader.ramBytesUsed();
-      ramBytesUsed.addAndGet(bytesUsed);
-      instanceInfo.put(field.name, Accountables.namedAccountable("variable straight", bytesUsed));
+      if (!merging) {
+        ramBytesUsed.addAndGet(bytesUsed);
+        instanceInfo.put(field.name, Accountables.namedAccountable("variable straight", bytesUsed));
+      }
       return new BinaryDocValues() {
         @Override
         public BytesRef get(int docID) {
@@ -434,8 +473,10 @@
       CodecUtil.checkEOF(data);
       CodecUtil.checkEOF(index);
       long bytesUsed = bytesReader.ramBytesUsed() + reader.ramBytesUsed();
-      ramBytesUsed.addAndGet(bytesUsed);
-      instanceInfo.put(field.name, Accountables.namedAccountable("fixed deref", bytesUsed));
+      if (!merging) {
+        ramBytesUsed.addAndGet(bytesUsed);
+        instanceInfo.put(field.name, Accountables.namedAccountable("fixed deref", bytesUsed));
+      }
       success = true;
       return new BinaryDocValues() {
         @Override
@@ -479,8 +520,10 @@
       CodecUtil.checkEOF(data);
       CodecUtil.checkEOF(index);
       long bytesUsed = bytesReader.ramBytesUsed() + reader.ramBytesUsed();
-      ramBytesUsed.addAndGet(bytesUsed);
-      instanceInfo.put(field.name, Accountables.namedAccountable("variable deref", bytesUsed));
+      if (!merging) {
+        ramBytesUsed.addAndGet(bytesUsed);
+        instanceInfo.put(field.name, Accountables.namedAccountable("variable deref", bytesUsed));
+      }
       success = true;
       return new BinaryDocValues() {
         
@@ -543,7 +586,9 @@
           IOUtils.closeWhileHandlingException(data, index);
         }
       }
-      sortedInstances.put(field.name, instance);
+      if (!merging) {
+        sortedInstances.put(field.name, instance);
+      }
     }
     return instance;
   }
@@ -564,8 +609,10 @@
     final PagedBytes.Reader bytesReader = bytes.freeze(true);
     final PackedInts.Reader reader = PackedInts.getReader(index);
     long bytesUsed = bytesReader.ramBytesUsed() + reader.ramBytesUsed();
-    ramBytesUsed.addAndGet(bytesUsed);
-    instanceInfo.put(field.name, Accountables.namedAccountable("fixed sorted", bytesUsed));
+    if (!merging) {
+      ramBytesUsed.addAndGet(bytesUsed);
+      instanceInfo.put(field.name, Accountables.namedAccountable("fixed sorted", bytesUsed));
+    }
 
     return correctBuggyOrds(new SortedDocValues() {
       @Override
@@ -604,8 +651,10 @@
 
     final int valueCount = addressReader.size() - 1;
     long bytesUsed = bytesReader.ramBytesUsed() + addressReader.ramBytesUsed() + ordsReader.ramBytesUsed();
-    ramBytesUsed.addAndGet(bytesUsed);
-    instanceInfo.put(field.name, Accountables.namedAccountable("var sorted", bytesUsed));
+    if (!merging) {
+      ramBytesUsed.addAndGet(bytesUsed);
+      instanceInfo.put(field.name, Accountables.namedAccountable("var sorted", bytesUsed));
+    }
 
     return correctBuggyOrds(new SortedDocValues() {
       @Override
@@ -692,6 +741,11 @@
   }
 
   @Override
+  public synchronized DocValuesProducer getMergeInstance() throws IOException {
+    return new Lucene40DocValuesReader(this);
+  }
+
+  @Override
   public String toString() {
     return getClass().getSimpleName();
   }
Index: lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40FieldInfosFormat.java
===================================================================
--- lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40FieldInfosFormat.java	(revision 1629405)
+++ lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40FieldInfosFormat.java	(working copy)
@@ -19,79 +19,12 @@
 
 import java.io.IOException;
 
-import org.apache.lucene.codecs.CodecUtil;
 import org.apache.lucene.codecs.FieldInfosFormat;
 import org.apache.lucene.codecs.FieldInfosReader;
 import org.apache.lucene.codecs.FieldInfosWriter;
-import org.apache.lucene.store.DataOutput; // javadoc
 
 /**
  * Lucene 4.0 Field Infos format.
- * <p>
- * <p>Field names are stored in the field info file, with suffix <tt>.fnm</tt>.</p>
- * <p>FieldInfos (.fnm) --&gt; Header,FieldsCount, &lt;FieldName,FieldNumber,
- * FieldBits,DocValuesBits,Attributes&gt; <sup>FieldsCount</sup></p>
- * <p>Data types:
- * <ul>
- *   <li>Header --&gt; {@link CodecUtil#checkHeader CodecHeader}</li>
- *   <li>FieldsCount --&gt; {@link DataOutput#writeVInt VInt}</li>
- *   <li>FieldName --&gt; {@link DataOutput#writeString String}</li>
- *   <li>FieldBits, DocValuesBits --&gt; {@link DataOutput#writeByte Byte}</li>
- *   <li>FieldNumber --&gt; {@link DataOutput#writeInt VInt}</li>
- *   <li>Attributes --&gt; {@link DataOutput#writeStringStringMap Map&lt;String,String&gt;}</li>
- * </ul>
- * </p>
- * Field Descriptions:
- * <ul>
- *   <li>FieldsCount: the number of fields in this file.</li>
- *   <li>FieldName: name of the field as a UTF-8 String.</li>
- *   <li>FieldNumber: the field's number. Note that unlike previous versions of
- *       Lucene, the fields are not numbered implicitly by their order in the
- *       file, instead explicitly.</li>
- *   <li>FieldBits: a byte containing field options.
- *       <ul>
- *         <li>The low-order bit is one for indexed fields, and zero for non-indexed
- *             fields.</li>
- *         <li>The second lowest-order bit is one for fields that have term vectors
- *             stored, and zero for fields without term vectors.</li>
- *         <li>If the third lowest order-bit is set (0x4), offsets are stored into
- *             the postings list in addition to positions.</li>
- *         <li>Fourth bit is unused.</li>
- *         <li>If the fifth lowest-order bit is set (0x10), norms are omitted for the
- *             indexed field.</li>
- *         <li>If the sixth lowest-order bit is set (0x20), payloads are stored for the
- *             indexed field.</li>
- *         <li>If the seventh lowest-order bit is set (0x40), term frequencies and
- *             positions omitted for the indexed field.</li>
- *         <li>If the eighth lowest-order bit is set (0x80), positions are omitted for the
- *             indexed field.</li>
- *       </ul>
- *    </li>
- *    <li>DocValuesBits: a byte containing per-document value types. The type
- *        recorded as two four-bit integers, with the high-order bits representing
- *        <code>norms</code> options, and the low-order bits representing 
- *        {@code DocValues} options. Each four-bit integer can be decoded as such:
- *        <ul>
- *          <li>0: no DocValues for this field.</li>
- *          <li>1: variable-width signed integers. ({@code Type#VAR_INTS VAR_INTS})</li>
- *          <li>2: 32-bit floating point values. ({@code Type#FLOAT_32 FLOAT_32})</li>
- *          <li>3: 64-bit floating point values. ({@code Type#FLOAT_64 FLOAT_64})</li>
- *          <li>4: fixed-length byte array values. ({@code Type#BYTES_FIXED_STRAIGHT BYTES_FIXED_STRAIGHT})</li>
- *          <li>5: fixed-length dereferenced byte array values. ({@code Type#BYTES_FIXED_DEREF BYTES_FIXED_DEREF})</li>
- *          <li>6: variable-length byte array values. ({@code Type#BYTES_VAR_STRAIGHT BYTES_VAR_STRAIGHT})</li>
- *          <li>7: variable-length dereferenced byte array values. ({@code Type#BYTES_VAR_DEREF BYTES_VAR_DEREF})</li>
- *          <li>8: 16-bit signed integers. ({@code Type#FIXED_INTS_16 FIXED_INTS_16})</li>
- *          <li>9: 32-bit signed integers. ({@code Type#FIXED_INTS_32 FIXED_INTS_32})</li>
- *          <li>10: 64-bit signed integers. ({@code Type#FIXED_INTS_64 FIXED_INTS_64})</li>
- *          <li>11: 8-bit signed integers. ({@code Type#FIXED_INTS_8 FIXED_INTS_8})</li>
- *          <li>12: fixed-length sorted byte array values. ({@code Type#BYTES_FIXED_SORTED BYTES_FIXED_SORTED})</li>
- *          <li>13: variable-length sorted byte array values. ({@code Type#BYTES_VAR_SORTED BYTES_VAR_SORTED})</li>
- *        </ul>
- *    </li>
- *    <li>Attributes: a key-value map of codec-private attributes.</li>
- * </ul>
- *
- * @lucene.experimental
  * @deprecated Only for reading old 4.0 and 4.1 segments
  */
 @Deprecated
@@ -103,7 +36,7 @@
   }
 
   @Override
-  public FieldInfosReader getFieldInfosReader() throws IOException {
+  public final FieldInfosReader getFieldInfosReader() throws IOException {
     return reader;
   }
 
Index: lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40FieldInfosReader.java
===================================================================
--- lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40FieldInfosReader.java	(revision 1629405)
+++ lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40FieldInfosReader.java	(working copy)
@@ -37,13 +37,10 @@
 
 /**
  * Lucene 4.0 FieldInfos reader.
- * 
- * @lucene.experimental
- * @see Lucene40FieldInfosFormat
  * @deprecated Only for reading old 4.0 and 4.1 segments
  */
 @Deprecated
-class Lucene40FieldInfosReader extends FieldInfosReader {
+final class Lucene40FieldInfosReader extends FieldInfosReader {
 
   /** Sole constructor. */
   public Lucene40FieldInfosReader() {
Index: lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40LiveDocsFormat.java
===================================================================
--- lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40LiveDocsFormat.java	(revision 1629405)
+++ lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40LiveDocsFormat.java	(working copy)
@@ -20,12 +20,10 @@
 import java.io.IOException;
 import java.util.Collection;
 
-import org.apache.lucene.codecs.CodecUtil;
 import org.apache.lucene.codecs.LiveDocsFormat;
 import org.apache.lucene.index.CorruptIndexException;
 import org.apache.lucene.index.IndexFileNames;
 import org.apache.lucene.index.SegmentCommitInfo;
-import org.apache.lucene.store.DataOutput; // javadocs
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.IOContext;
 import org.apache.lucene.util.Bits;
@@ -33,37 +31,10 @@
 
 /**
  * Lucene 4.0 Live Documents Format.
- * <p>
- * <p>The .del file is optional, and only exists when a segment contains
- * deletions.</p>
- * <p>Although per-segment, this file is maintained exterior to compound segment
- * files.</p>
- * <p>Deletions (.del) --&gt; Format,Header,ByteCount,BitCount, Bits | DGaps (depending
- * on Format)</p>
- * <ul>
- *   <li>Format,ByteSize,BitCount --&gt; {@link DataOutput#writeInt Uint32}</li>
- *   <li>Bits --&gt; &lt;{@link DataOutput#writeByte Byte}&gt; <sup>ByteCount</sup></li>
- *   <li>DGaps --&gt; &lt;DGap,NonOnesByte&gt; <sup>NonzeroBytesCount</sup></li>
- *   <li>DGap --&gt; {@link DataOutput#writeVInt VInt}</li>
- *   <li>NonOnesByte --&gt; {@link DataOutput#writeByte Byte}</li>
- *   <li>Header --&gt; {@link CodecUtil#writeHeader CodecHeader}</li>
- * </ul>
- * <p>Format is 1: indicates cleared DGaps.</p>
- * <p>ByteCount indicates the number of bytes in Bits. It is typically
- * (SegSize/8)+1.</p>
- * <p>BitCount indicates the number of bits that are currently set in Bits.</p>
- * <p>Bits contains one bit for each document indexed. When the bit corresponding
- * to a document number is cleared, that document is marked as deleted. Bit ordering
- * is from least to most significant. Thus, if Bits contains two bytes, 0x00 and
- * 0x02, then document 9 is marked as alive (not deleted).</p>
- * <p>DGaps represents sparse bit-vectors more efficiently than Bits. It is made
- * of DGaps on indexes of nonOnes bytes in Bits, and the nonOnes bytes themselves.
- * The number of nonOnes bytes in Bits (NonOnesBytesCount) is not stored.</p>
- * <p>For example, if there are 8000 bits and only bits 10,12,32 are cleared, DGaps
- * would be used:</p>
- * <p>(VInt) 1 , (byte) 20 , (VInt) 3 , (Byte) 1</p>
+ * @deprecated Only for reading old 4.x segments
  */
-public class Lucene40LiveDocsFormat extends LiveDocsFormat {
+@Deprecated
+public final class Lucene40LiveDocsFormat extends LiveDocsFormat {
 
   /** Extension of deletes */
   static final String DELETES_EXTENSION = "del";
Index: lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40NormsFormat.java
===================================================================
--- lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40NormsFormat.java	(revision 1629405)
+++ lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40NormsFormat.java	(working copy)
@@ -25,21 +25,9 @@
 import org.apache.lucene.index.IndexFileNames;
 import org.apache.lucene.index.SegmentReadState;
 import org.apache.lucene.index.SegmentWriteState;
-import org.apache.lucene.store.CompoundFileDirectory;
 
 /**
  * Lucene 4.0 Norms Format.
- * <p>
- * Files:
- * <ul>
- *   <li><tt>.nrm.cfs</tt>: {@link CompoundFileDirectory compound container}</li>
- *   <li><tt>.nrm.cfe</tt>: {@link CompoundFileDirectory compound entries}</li>
- * </ul>
- * Norms are implemented as DocValues, so other than file extension, norms are 
- * written exactly the same way as {@link Lucene40DocValuesFormat DocValues}.
- * 
- * @see Lucene40DocValuesFormat
- * @lucene.experimental
  * @deprecated Only for reading old 4.0 and 4.1 segments
  */
 @Deprecated
@@ -57,7 +45,7 @@
   public NormsProducer normsProducer(SegmentReadState state) throws IOException {
     String filename = IndexFileNames.segmentFileName(state.segmentInfo.name, 
                                                      "nrm", 
-                                                     IndexFileNames.COMPOUND_FILE_EXTENSION);
+                                                     Lucene40CompoundFormat.COMPOUND_FILE_EXTENSION);
     return new Lucene40NormsReader(state, filename);
   }
 }
Index: lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40NormsReader.java
===================================================================
--- lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40NormsReader.java	(revision 1629405)
+++ lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40NormsReader.java	(working copy)
@@ -19,6 +19,7 @@
 
 import java.io.IOException;
 
+import org.apache.lucene.codecs.DocValuesProducer;
 import org.apache.lucene.codecs.NormsProducer;
 import org.apache.lucene.index.FieldInfo;
 import org.apache.lucene.index.NumericDocValues;
@@ -27,14 +28,18 @@
 
 /**
  * Reads 4.0/4.1 norms.
- * Implemented the same as docvalues, but with a different filename.
  * @deprecated Only for reading old 4.0 and 4.1 segments
  */
 @Deprecated
-class Lucene40NormsReader extends NormsProducer {
-  private final Lucene40DocValuesReader impl;
+final class Lucene40NormsReader extends NormsProducer {
+  private final DocValuesProducer impl;
   
-  public Lucene40NormsReader(SegmentReadState state, String filename) throws IOException {
+  // clone for merge
+  Lucene40NormsReader(DocValuesProducer impl) throws IOException {
+    this.impl = impl.getMergeInstance();
+  }
+  
+  Lucene40NormsReader(SegmentReadState state, String filename) throws IOException {
     impl = new Lucene40DocValuesReader(state, filename, Lucene40FieldInfosReader.LEGACY_NORM_TYPE_KEY);
   }
   
@@ -64,6 +69,11 @@
   }
   
   @Override
+  public NormsProducer getMergeInstance() throws IOException {
+    return new Lucene40NormsReader(impl);
+  }
+
+  @Override
   public String toString() {
     return getClass().getSimpleName() + "(" + impl + ")";
   }
Index: lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40PostingsBaseFormat.java
===================================================================
--- lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40PostingsBaseFormat.java	(revision 1629405)
+++ lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40PostingsBaseFormat.java	(working copy)
@@ -26,17 +26,13 @@
 import org.apache.lucene.index.SegmentWriteState;
 
 /** 
- * Provides a {@link PostingsReaderBase} and {@link
- * PostingsWriterBase}.
- *
+ * PostingsReaderBase for 4.0 segments
  * @deprecated Only for reading old 4.0 segments */
-
-// TODO: should these also be named / looked up via SPI?
 @Deprecated
-public final class Lucene40PostingsBaseFormat extends PostingsBaseFormat {
+final class Lucene40PostingsBaseFormat extends PostingsBaseFormat {
 
   /** Sole constructor. */
-  public Lucene40PostingsBaseFormat() {
+  Lucene40PostingsBaseFormat() {
     super("Lucene40");
   }
 
Index: lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40PostingsFormat.java
===================================================================
--- lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40PostingsFormat.java	(revision 1629405)
+++ lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40PostingsFormat.java	(working copy)
@@ -19,226 +19,25 @@
 
 import java.io.IOException;
 
-import org.apache.lucene.codecs.CodecUtil;
 import org.apache.lucene.codecs.FieldsConsumer;
 import org.apache.lucene.codecs.FieldsProducer;
 import org.apache.lucene.codecs.PostingsFormat;
 import org.apache.lucene.codecs.PostingsReaderBase;
-import org.apache.lucene.codecs.PostingsWriterBase; // javadocs
 import org.apache.lucene.codecs.blocktree.BlockTreeTermsReader;
-import org.apache.lucene.codecs.blocktree.BlockTreeTermsWriter;
-import org.apache.lucene.index.DocsEnum; // javadocs
-import org.apache.lucene.index.FieldInfo.IndexOptions; // javadocs
-import org.apache.lucene.index.FieldInfos; // javadocs
 import org.apache.lucene.index.SegmentReadState;
 import org.apache.lucene.index.SegmentWriteState;
-import org.apache.lucene.store.DataOutput; // javadocs
-import org.apache.lucene.util.fst.FST; // javadocs
 
 /** 
  * Lucene 4.0 Postings format.
- * <p>
- * Files:
- * <ul>
- *   <li><tt>.tim</tt>: <a href="#Termdictionary">Term Dictionary</a></li>
- *   <li><tt>.tip</tt>: <a href="#Termindex">Term Index</a></li>
- *   <li><tt>.frq</tt>: <a href="#Frequencies">Frequencies</a></li>
- *   <li><tt>.prx</tt>: <a href="#Positions">Positions</a></li>
- * </ul>
- * </p>
- * <p>
- * <a name="Termdictionary" id="Termdictionary"></a>
- * <h3>Term Dictionary</h3>
- *
- * <p>The .tim file contains the list of terms in each
- * field along with per-term statistics (such as docfreq)
- * and pointers to the frequencies, positions and
- * skip data in the .frq and .prx files.
- * See {@link BlockTreeTermsWriter} for more details on the format.
- * </p>
- *
- * <p>NOTE: The term dictionary can plug into different postings implementations:
- * the postings writer/reader are actually responsible for encoding 
- * and decoding the Postings Metadata and Term Metadata sections described here:</p>
- * <ul>
- *    <li>Postings Metadata --&gt; Header, SkipInterval, MaxSkipLevels, SkipMinimum</li>
- *    <li>Term Metadata --&gt; FreqDelta, SkipDelta?, ProxDelta?
- *    <li>Header --&gt; {@link CodecUtil#writeHeader CodecHeader}</li>
- *    <li>SkipInterval,MaxSkipLevels,SkipMinimum --&gt; {@link DataOutput#writeInt Uint32}</li>
- *    <li>SkipDelta,FreqDelta,ProxDelta --&gt; {@link DataOutput#writeVLong VLong}</li>
- * </ul>
- * <p>Notes:</p>
- * <ul>
- *    <li>Header is a {@link CodecUtil#writeHeader CodecHeader} storing the version information
- *        for the postings.</li>
- *    <li>SkipInterval is the fraction of TermDocs stored in skip tables. It is used to accelerate 
- *        {@link DocsEnum#advance(int)}. Larger values result in smaller indexes, greater 
- *        acceleration, but fewer accelerable cases, while smaller values result in bigger indexes, 
- *        less acceleration (in case of a small value for MaxSkipLevels) and more accelerable cases.
- *        </li>
- *    <li>MaxSkipLevels is the max. number of skip levels stored for each term in the .frq file. A 
- *        low value results in smaller indexes but less acceleration, a larger value results in 
- *        slightly larger indexes but greater acceleration. See format of .frq file for more 
- *        information about skip levels.</li>
- *    <li>SkipMinimum is the minimum document frequency a term must have in order to write any 
- *        skip data at all.</li>
- *    <li>FreqDelta determines the position of this term's TermFreqs within the .frq
- *        file. In particular, it is the difference between the position of this term's
- *        data in that file and the position of the previous term's data (or zero, for
- *        the first term in the block).</li>
- *    <li>ProxDelta determines the position of this term's TermPositions within the
- *        .prx file. In particular, it is the difference between the position of this
- *        term's data in that file and the position of the previous term's data (or zero,
- *        for the first term in the block. For fields that omit position data, this will
- *        be 0 since prox information is not stored.</li>
- *    <li>SkipDelta determines the position of this term's SkipData within the .frq
- *        file. In particular, it is the number of bytes after TermFreqs that the
- *        SkipData starts. In other words, it is the length of the TermFreq data.
- *        SkipDelta is only stored if DocFreq is not smaller than SkipMinimum.</li>
- * </ul>
- * <a name="Termindex" id="Termindex"></a>
- * <h3>Term Index</h3>
- * <p>The .tip file contains an index into the term dictionary, so that it can be 
- * accessed randomly.  See {@link BlockTreeTermsWriter} for more details on the format.</p>
- * <a name="Frequencies" id="Frequencies"></a>
- * <h3>Frequencies</h3>
- * <p>The .frq file contains the lists of documents which contain each term, along
- * with the frequency of the term in that document (except when frequencies are
- * omitted: {@link IndexOptions#DOCS_ONLY}).</p>
- * <ul>
- *   <li>FreqFile (.frq) --&gt; Header, &lt;TermFreqs, SkipData?&gt; <sup>TermCount</sup></li>
- *   <li>Header --&gt; {@link CodecUtil#writeHeader CodecHeader}</li>
- *   <li>TermFreqs --&gt; &lt;TermFreq&gt; <sup>DocFreq</sup></li>
- *   <li>TermFreq --&gt; DocDelta[, Freq?]</li>
- *   <li>SkipData --&gt; &lt;&lt;SkipLevelLength, SkipLevel&gt;
- *       <sup>NumSkipLevels-1</sup>, SkipLevel&gt; &lt;SkipDatum&gt;</li>
- *   <li>SkipLevel --&gt; &lt;SkipDatum&gt; <sup>DocFreq/(SkipInterval^(Level +
- *       1))</sup></li>
- *   <li>SkipDatum --&gt;
- *       DocSkip,PayloadLength?,OffsetLength?,FreqSkip,ProxSkip,SkipChildLevelPointer?</li>
- *   <li>DocDelta,Freq,DocSkip,PayloadLength,OffsetLength,FreqSkip,ProxSkip --&gt; {@link DataOutput#writeVInt VInt}</li>
- *   <li>SkipChildLevelPointer --&gt; {@link DataOutput#writeVLong VLong}</li>
- * </ul>
- * <p>TermFreqs are ordered by term (the term is implicit, from the term dictionary).</p>
- * <p>TermFreq entries are ordered by increasing document number.</p>
- * <p>DocDelta: if frequencies are indexed, this determines both the document
- * number and the frequency. In particular, DocDelta/2 is the difference between
- * this document number and the previous document number (or zero when this is the
- * first document in a TermFreqs). When DocDelta is odd, the frequency is one.
- * When DocDelta is even, the frequency is read as another VInt. If frequencies
- * are omitted, DocDelta contains the gap (not multiplied by 2) between document
- * numbers and no frequency information is stored.</p>
- * <p>For example, the TermFreqs for a term which occurs once in document seven
- * and three times in document eleven, with frequencies indexed, would be the
- * following sequence of VInts:</p>
- * <p>15, 8, 3</p>
- * <p>If frequencies were omitted ({@link IndexOptions#DOCS_ONLY}) it would be this
- * sequence of VInts instead:</p>
- * <p>7,4</p>
- * <p>DocSkip records the document number before every SkipInterval <sup>th</sup>
- * document in TermFreqs. If payloads and offsets are disabled for the term's field, then
- * DocSkip represents the difference from the previous value in the sequence. If
- * payloads and/or offsets are enabled for the term's field, then DocSkip/2 represents the
- * difference from the previous value in the sequence. In this case when
- * DocSkip is odd, then PayloadLength and/or OffsetLength are stored indicating the length of 
- * the last payload/offset before the SkipInterval<sup>th</sup> document in TermPositions.</p>
- * <p>PayloadLength indicates the length of the last payload.</p>
- * <p>OffsetLength indicates the length of the last offset (endOffset-startOffset).</p>
- * <p>
- * FreqSkip and ProxSkip record the position of every SkipInterval <sup>th</sup>
- * entry in FreqFile and ProxFile, respectively. File positions are relative to
- * the start of TermFreqs and Positions, to the previous SkipDatum in the
- * sequence.</p>
- * <p>For example, if DocFreq=35 and SkipInterval=16, then there are two SkipData
- * entries, containing the 15 <sup>th</sup> and 31 <sup>st</sup> document numbers
- * in TermFreqs. The first FreqSkip names the number of bytes after the beginning
- * of TermFreqs that the 16 <sup>th</sup> SkipDatum starts, and the second the
- * number of bytes after that that the 32 <sup>nd</sup> starts. The first ProxSkip
- * names the number of bytes after the beginning of Positions that the 16
- * <sup>th</sup> SkipDatum starts, and the second the number of bytes after that
- * that the 32 <sup>nd</sup> starts.</p>
- * <p>Each term can have multiple skip levels. The amount of skip levels for a
- * term is NumSkipLevels = Min(MaxSkipLevels,
- * floor(log(DocFreq/log(SkipInterval)))). The number of SkipData entries for a
- * skip level is DocFreq/(SkipInterval^(Level + 1)), whereas the lowest skip level
- * is Level=0.<br>
- * Example: SkipInterval = 4, MaxSkipLevels = 2, DocFreq = 35. Then skip level 0
- * has 8 SkipData entries, containing the 3<sup>rd</sup>, 7<sup>th</sup>,
- * 11<sup>th</sup>, 15<sup>th</sup>, 19<sup>th</sup>, 23<sup>rd</sup>,
- * 27<sup>th</sup>, and 31<sup>st</sup> document numbers in TermFreqs. Skip level
- * 1 has 2 SkipData entries, containing the 15<sup>th</sup> and 31<sup>st</sup>
- * document numbers in TermFreqs.<br>
- * The SkipData entries on all upper levels &gt; 0 contain a SkipChildLevelPointer
- * referencing the corresponding SkipData entry in level-1. In the example has
- * entry 15 on level 1 a pointer to entry 15 on level 0 and entry 31 on level 1 a
- * pointer to entry 31 on level 0.
- * </p>
- * <a name="Positions" id="Positions"></a>
- * <h3>Positions</h3>
- * <p>The .prx file contains the lists of positions that each term occurs at
- * within documents. Note that fields omitting positional data do not store
- * anything into this file, and if all fields in the index omit positional data
- * then the .prx file will not exist.</p>
- * <ul>
- *   <li>ProxFile (.prx) --&gt; Header, &lt;TermPositions&gt; <sup>TermCount</sup></li>
- *   <li>Header --&gt; {@link CodecUtil#writeHeader CodecHeader}</li>
- *   <li>TermPositions --&gt; &lt;Positions&gt; <sup>DocFreq</sup></li>
- *   <li>Positions --&gt; &lt;PositionDelta,PayloadLength?,OffsetDelta?,OffsetLength?,PayloadData?&gt; <sup>Freq</sup></li>
- *   <li>PositionDelta,OffsetDelta,OffsetLength,PayloadLength --&gt; {@link DataOutput#writeVInt VInt}</li>
- *   <li>PayloadData --&gt; {@link DataOutput#writeByte byte}<sup>PayloadLength</sup></li>
- * </ul>
- * <p>TermPositions are ordered by term (the term is implicit, from the term dictionary).</p>
- * <p>Positions entries are ordered by increasing document number (the document
- * number is implicit from the .frq file).</p>
- * <p>PositionDelta is, if payloads are disabled for the term's field, the
- * difference between the position of the current occurrence in the document and
- * the previous occurrence (or zero, if this is the first occurrence in this
- * document). If payloads are enabled for the term's field, then PositionDelta/2
- * is the difference between the current and the previous position. If payloads
- * are enabled and PositionDelta is odd, then PayloadLength is stored, indicating
- * the length of the payload at the current term position.</p>
- * <p>For example, the TermPositions for a term which occurs as the fourth term in
- * one document, and as the fifth and ninth term in a subsequent document, would
- * be the following sequence of VInts (payloads disabled):</p>
- * <p>4, 5, 4</p>
- * <p>PayloadData is metadata associated with the current term position. If
- * PayloadLength is stored at the current position, then it indicates the length
- * of this payload. If PayloadLength is not stored, then this payload has the same
- * length as the payload at the previous position.</p>
- * <p>OffsetDelta/2 is the difference between this position's startOffset from the
- * previous occurrence (or zero, if this is the first occurrence in this document).
- * If OffsetDelta is odd, then the length (endOffset-startOffset) differs from the
- * previous occurrence and an OffsetLength follows. Offset data is only written for
- * {@link IndexOptions#DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS}.</p>
- * 
- *  @deprecated Only for reading old 4.0 segments */
-
-// TODO: this class could be created by wrapping
-// BlockTreeTermsDict around Lucene40PostingsBaseFormat; ie
-// we should not duplicate the code from that class here:
+ * @deprecated Only for reading old 4.0 segments 
+ */
 @Deprecated
 public class Lucene40PostingsFormat extends PostingsFormat {
 
-  /** minimum items (terms or sub-blocks) per block for BlockTree */
-  protected final int minBlockSize;
-  /** maximum items (terms or sub-blocks) per block for BlockTree */
-  protected final int maxBlockSize;
-
   /** Creates {@code Lucene40PostingsFormat} with default
    *  settings. */
   public Lucene40PostingsFormat() {
-    this(BlockTreeTermsWriter.DEFAULT_MIN_BLOCK_SIZE, BlockTreeTermsWriter.DEFAULT_MAX_BLOCK_SIZE);
-  }
-
-  /** Creates {@code Lucene40PostingsFormat} with custom
-   *  values for {@code minBlockSize} and {@code
-   *  maxBlockSize} passed to block terms dictionary.
-   *  @see BlockTreeTermsWriter#BlockTreeTermsWriter(SegmentWriteState,PostingsWriterBase,int,int) */
-  private Lucene40PostingsFormat(int minBlockSize, int maxBlockSize) {
     super("Lucene40");
-    this.minBlockSize = minBlockSize;
-    assert minBlockSize > 1;
-    this.maxBlockSize = maxBlockSize;
   }
 
   @Override
@@ -247,7 +46,7 @@
   }
 
   @Override
-  public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {
+  public final FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {
     PostingsReaderBase postings = new Lucene40PostingsReader(state.directory, state.fieldInfos, state.segmentInfo, state.context, state.segmentSuffix);
 
     boolean success = false;
@@ -276,6 +75,6 @@
 
   @Override
   public String toString() {
-    return getName() + "(minBlockSize=" + minBlockSize + " maxBlockSize=" + maxBlockSize + ")";
+    return getName();
   }
 }
Index: lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40PostingsReader.java
===================================================================
--- lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40PostingsReader.java	(revision 1629405)
+++ lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40PostingsReader.java	(working copy)
@@ -43,13 +43,10 @@
 import org.apache.lucene.util.IOUtils;
 
 /** 
- * Concrete class that reads the 4.0 frq/prox
- * postings format. 
- *  
- *  @see Lucene40PostingsFormat
- *  @deprecated Only for reading old 4.0 segments */
+ * Reader for 4.0 postings format
+ * @deprecated Only for reading old 4.0 segments */
 @Deprecated
-public class Lucene40PostingsReader extends PostingsReaderBase {
+final class Lucene40PostingsReader extends PostingsReaderBase {
 
   final static String TERMS_CODEC = "Lucene40PostingsWriterTerms";
   final static String FRQ_CODEC = "Lucene40PostingsWriterFrq";
Index: lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40SegmentInfoFormat.java
===================================================================
--- lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40SegmentInfoFormat.java	(revision 1629405)
+++ lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40SegmentInfoFormat.java	(working copy)
@@ -17,57 +17,14 @@
  * limitations under the License.
  */
 
-import org.apache.lucene.codecs.CodecUtil;
 import org.apache.lucene.codecs.SegmentInfoFormat;
 import org.apache.lucene.codecs.SegmentInfoReader;
 import org.apache.lucene.codecs.SegmentInfoWriter;
-import org.apache.lucene.index.IndexWriter; // javadocs
-import org.apache.lucene.index.SegmentInfo; // javadocs
-import org.apache.lucene.index.SegmentInfos; // javadocs
-import org.apache.lucene.store.DataOutput; // javadocs
+import org.apache.lucene.index.SegmentInfo;
 
 /**
  * Lucene 4.0 Segment info format.
- * <p>
- * Files:
- * <ul>
- *   <li><tt>.si</tt>: Header, SegVersion, SegSize, IsCompoundFile, Diagnostics, Attributes, Files
- * </ul>
- * </p>
- * Data types:
- * <p>
- * <ul>
- *   <li>Header --&gt; {@link CodecUtil#writeHeader CodecHeader}</li>
- *   <li>SegSize --&gt; {@link DataOutput#writeInt Int32}</li>
- *   <li>SegVersion --&gt; {@link DataOutput#writeString String}</li>
- *   <li>Files --&gt; {@link DataOutput#writeStringSet Set&lt;String&gt;}</li>
- *   <li>Diagnostics, Attributes --&gt; {@link DataOutput#writeStringStringMap Map&lt;String,String&gt;}</li>
- *   <li>IsCompoundFile --&gt; {@link DataOutput#writeByte Int8}</li>
- * </ul>
- * </p>
- * Field Descriptions:
- * <p>
- * <ul>
- *   <li>SegVersion is the code version that created the segment.</li>
- *   <li>SegSize is the number of documents contained in the segment index.</li>
- *   <li>IsCompoundFile records whether the segment is written as a compound file or
- *       not. If this is -1, the segment is not a compound file. If it is 1, the segment
- *       is a compound file.</li>
- *   <li>Checksum contains the CRC32 checksum of all bytes in the segments_N file up
- *       until the checksum. This is used to verify integrity of the file on opening the
- *       index.</li>
- *   <li>The Diagnostics Map is privately written by {@link IndexWriter}, as a debugging aid,
- *       for each segment it creates. It includes metadata like the current Lucene
- *       version, OS, Java version, why the segment was created (merge, flush,
- *       addIndexes), etc.</li>
- *   <li>Attributes: a key-value map of codec-private attributes.</li>
- *   <li>Files is a list of files referred to by this segment.</li>
- * </ul>
- * </p>
- * 
- * @see SegmentInfos
- * @lucene.experimental
- * @deprecated Only for reading old 4.0-4.5 segments, and supporting IndexWriter.addIndexes
+ * @deprecated Only for reading old 4.0-4.5 segments
  */
 @Deprecated
 public class Lucene40SegmentInfoFormat extends SegmentInfoFormat {
@@ -78,7 +35,7 @@
   }
   
   @Override
-  public SegmentInfoReader getSegmentInfoReader() {
+  public final SegmentInfoReader getSegmentInfoReader() {
     return reader;
   }
 
@@ -88,7 +45,7 @@
   }
 
   /** File extension used to store {@link SegmentInfo}. */
-  public final static String SI_EXTENSION = "si";
+  static final String SI_EXTENSION = "si";
   static final String CODEC_NAME = "Lucene40SegmentInfo";
   static final int VERSION_START = 0;
   static final int VERSION_CURRENT = VERSION_START;
Index: lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40SegmentInfoReader.java
===================================================================
--- lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40SegmentInfoReader.java	(revision 1629405)
+++ lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40SegmentInfoReader.java	(working copy)
@@ -34,14 +34,11 @@
 import org.apache.lucene.util.Version;
 
 /**
- * Lucene 4.0 implementation of {@link SegmentInfoReader}.
- * 
- * @see Lucene40SegmentInfoFormat
- * @lucene.experimental
+ * Lucene 4.0 SI reader
  * @deprecated Only for reading old 4.0-4.5 segments
  */
 @Deprecated
-public class Lucene40SegmentInfoReader extends SegmentInfoReader {
+final class Lucene40SegmentInfoReader extends SegmentInfoReader {
 
   /** Sole constructor. */
   public Lucene40SegmentInfoReader() {
Index: lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40SkipListReader.java
===================================================================
--- lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40SkipListReader.java	(revision 1629405)
+++ lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40SkipListReader.java	(working copy)
@@ -24,14 +24,11 @@
 import org.apache.lucene.store.IndexInput;
 
 /**
- * Implements the skip list reader for the 4.0 posting list format
- * that stores positions and payloads.
- * 
- * @see Lucene40PostingsFormat
+ * Lucene 4.0 skiplist reader
  * @deprecated Only for reading old 4.0 segments
  */
 @Deprecated
-public class Lucene40SkipListReader extends MultiLevelSkipListReader {
+final class Lucene40SkipListReader extends MultiLevelSkipListReader {
   private boolean currentFieldStoresPayloads;
   private boolean currentFieldStoresOffsets;
   private long freqPointer[];
Index: lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40StoredFieldsFormat.java
===================================================================
--- lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40StoredFieldsFormat.java	(revision 1629405)
+++ lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40StoredFieldsFormat.java	(working copy)
@@ -19,66 +19,18 @@
 
 import java.io.IOException;
 
-import org.apache.lucene.codecs.CodecUtil;
 import org.apache.lucene.codecs.StoredFieldsFormat;
 import org.apache.lucene.codecs.StoredFieldsReader;
 import org.apache.lucene.codecs.StoredFieldsWriter;
 import org.apache.lucene.index.FieldInfos;
 import org.apache.lucene.index.SegmentInfo;
-import org.apache.lucene.store.DataOutput; // javadocs
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.IOContext;
 
 /** 
  * Lucene 4.0 Stored Fields Format.
- * <p>Stored fields are represented by two files:</p>
- * <ol>
- * <li><a name="field_index" id="field_index"></a>
- * <p>The field index, or <tt>.fdx</tt> file.</p>
- * <p>This is used to find the location within the field data file of the fields
- * of a particular document. Because it contains fixed-length data, this file may
- * be easily randomly accessed. The position of document <i>n</i> 's field data is
- * the {@link DataOutput#writeLong Uint64} at <i>n*8</i> in this file.</p>
- * <p>This contains, for each document, a pointer to its field data, as
- * follows:</p>
- * <ul>
- * <li>FieldIndex (.fdx) --&gt; &lt;Header&gt;, &lt;FieldValuesPosition&gt; <sup>SegSize</sup></li>
- * <li>Header --&gt; {@link CodecUtil#writeHeader CodecHeader}</li>
- * <li>FieldValuesPosition --&gt; {@link DataOutput#writeLong Uint64}</li>
- * </ul>
- * </li>
- * <li>
- * <p><a name="field_data" id="field_data"></a>The field data, or <tt>.fdt</tt> file.</p>
- * <p>This contains the stored fields of each document, as follows:</p>
- * <ul>
- * <li>FieldData (.fdt) --&gt; &lt;Header&gt;, &lt;DocFieldData&gt; <sup>SegSize</sup></li>
- * <li>Header --&gt; {@link CodecUtil#writeHeader CodecHeader}</li>
- * <li>DocFieldData --&gt; FieldCount, &lt;FieldNum, Bits, Value&gt;
- * <sup>FieldCount</sup></li>
- * <li>FieldCount --&gt; {@link DataOutput#writeVInt VInt}</li>
- * <li>FieldNum --&gt; {@link DataOutput#writeVInt VInt}</li>
- * <li>Bits --&gt; {@link DataOutput#writeByte Byte}</li>
- * <ul>
- * <li>low order bit reserved.</li>
- * <li>second bit is one for fields containing binary data</li>
- * <li>third bit reserved.</li>
- * <li>4th to 6th bit (mask: 0x7&lt;&lt;3) define the type of a numeric field:
- * <ul>
- * <li>all bits in mask are cleared if no numeric field at all</li>
- * <li>1&lt;&lt;3: Value is Int</li>
- * <li>2&lt;&lt;3: Value is Long</li>
- * <li>3&lt;&lt;3: Value is Int as Float (as of {@link Float#intBitsToFloat(int)}</li>
- * <li>4&lt;&lt;3: Value is Long as Double (as of {@link Double#longBitsToDouble(long)}</li>
- * </ul>
- * </li>
- * </ul>
- * <li>Value --&gt; String | BinaryValue | Int | Long (depending on Bits)</li>
- * <li>BinaryValue --&gt; ValueSize, &lt;{@link DataOutput#writeByte Byte}&gt;^ValueSize</li>
- * <li>ValueSize --&gt; {@link DataOutput#writeVInt VInt}</li>
- * </li>
- * </ul>
- * </ol>
- * @lucene.experimental */
+ * @deprecated only for reading 4.0 segments */
+@Deprecated
 public class Lucene40StoredFieldsFormat extends StoredFieldsFormat {
 
   /** Sole constructor. */
@@ -86,7 +38,7 @@
   }
 
   @Override
-  public StoredFieldsReader fieldsReader(Directory directory, SegmentInfo si,
+  public final StoredFieldsReader fieldsReader(Directory directory, SegmentInfo si,
       FieldInfos fn, IOContext context) throws IOException {
     return new Lucene40StoredFieldsReader(directory, si, fn, context);
   }
Index: lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40StoredFieldsReader.java
===================================================================
--- lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40StoredFieldsReader.java	(revision 1629405)
+++ lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40StoredFieldsReader.java	(working copy)
@@ -40,14 +40,11 @@
 import java.util.Collections;
 
 /**
- * Class responsible for access to stored document fields.
- * <p/>
- * It uses &lt;segment&gt;.fdt and &lt;segment&gt;.fdx; files.
- * 
- * @see Lucene40StoredFieldsFormat
- * @lucene.internal
+ * Reader for 4.0 stored fields
+ * @deprecated only for reading 4.0 segments
  */
-public final class Lucene40StoredFieldsReader extends StoredFieldsReader implements Cloneable, Closeable {
+@Deprecated
+final class Lucene40StoredFieldsReader extends StoredFieldsReader implements Cloneable, Closeable {
 
   // NOTE: bit 0 is free here!  You can steal it!
   static final int FIELD_IS_BINARY = 1 << 1;
@@ -76,10 +73,10 @@
 
 
   /** Extension of stored fields file */
-  public static final String FIELDS_EXTENSION = "fdt";
+  static final String FIELDS_EXTENSION = "fdt";
   
   /** Extension of stored fields index file */
-  public static final String FIELDS_INDEX_EXTENSION = "fdx";
+  static final String FIELDS_INDEX_EXTENSION = "fdx";
   
   private static final long RAM_BYTES_USED = RamUsageEstimator.shallowSizeOfInstance(Lucene40StoredFieldsReader.class);
 
Index: lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40TermVectorsFormat.java
===================================================================
--- lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40TermVectorsFormat.java	(revision 1629405)
+++ lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40TermVectorsFormat.java	(working copy)
@@ -19,100 +19,19 @@
 
 import java.io.IOException;
 
-import org.apache.lucene.codecs.CodecUtil;
 import org.apache.lucene.codecs.TermVectorsFormat;
 import org.apache.lucene.codecs.TermVectorsReader;
 import org.apache.lucene.codecs.TermVectorsWriter;
 import org.apache.lucene.index.FieldInfos;
 import org.apache.lucene.index.SegmentInfo;
-import org.apache.lucene.store.DataOutput; // javadocs
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.IOContext;
 
 /**
  * Lucene 4.0 Term Vectors format.
- * <p>Term Vector support is an optional on a field by field basis. It consists of
- * 3 files.</p>
- * <ol>
- * <li><a name="tvx" id="tvx"></a>
- * <p>The Document Index or .tvx file.</p>
- * <p>For each document, this stores the offset into the document data (.tvd) and
- * field data (.tvf) files.</p>
- * <p>DocumentIndex (.tvx) --&gt; Header,&lt;DocumentPosition,FieldPosition&gt;
- * <sup>NumDocs</sup></p>
- * <ul>
- *   <li>Header --&gt; {@link CodecUtil#writeHeader CodecHeader}</li>
- *   <li>DocumentPosition --&gt; {@link DataOutput#writeLong UInt64} (offset in the .tvd file)</li>
- *   <li>FieldPosition --&gt; {@link DataOutput#writeLong UInt64} (offset in the .tvf file)</li>
- * </ul>
- * </li>
- * <li><a name="tvd" id="tvd"></a>
- * <p>The Document or .tvd file.</p>
- * <p>This contains, for each document, the number of fields, a list of the fields
- * with term vector info and finally a list of pointers to the field information
- * in the .tvf (Term Vector Fields) file.</p>
- * <p>The .tvd file is used to map out the fields that have term vectors stored
- * and where the field information is in the .tvf file.</p>
- * <p>Document (.tvd) --&gt; Header,&lt;NumFields, FieldNums,
- * FieldPositions&gt; <sup>NumDocs</sup></p>
- * <ul>
- *   <li>Header --&gt; {@link CodecUtil#writeHeader CodecHeader}</li>
- *   <li>NumFields --&gt; {@link DataOutput#writeVInt VInt}</li>
- *   <li>FieldNums --&gt; &lt;FieldNumDelta&gt; <sup>NumFields</sup></li>
- *   <li>FieldNumDelta --&gt; {@link DataOutput#writeVInt VInt}</li>
- *   <li>FieldPositions --&gt; &lt;FieldPositionDelta&gt; <sup>NumFields-1</sup></li>
- *   <li>FieldPositionDelta --&gt; {@link DataOutput#writeVLong VLong}</li>
- * </ul>
- * </li>
- * <li><a name="tvf" id="tvf"></a>
- * <p>The Field or .tvf file.</p>
- * <p>This file contains, for each field that has a term vector stored, a list of
- * the terms, their frequencies and, optionally, position, offset, and payload
- * information.</p>
- * <p>Field (.tvf) --&gt; Header,&lt;NumTerms, Flags, TermFreqs&gt;
- * <sup>NumFields</sup></p>
- * <ul>
- *   <li>Header --&gt; {@link CodecUtil#writeHeader CodecHeader}</li>
- *   <li>NumTerms --&gt; {@link DataOutput#writeVInt VInt}</li>
- *   <li>Flags --&gt; {@link DataOutput#writeByte Byte}</li>
- *   <li>TermFreqs --&gt; &lt;TermText, TermFreq, Positions?, PayloadData?, Offsets?&gt;
- *       <sup>NumTerms</sup></li>
- *   <li>TermText --&gt; &lt;PrefixLength, Suffix&gt;</li>
- *   <li>PrefixLength --&gt; {@link DataOutput#writeVInt VInt}</li>
- *   <li>Suffix --&gt; {@link DataOutput#writeString String}</li>
- *   <li>TermFreq --&gt; {@link DataOutput#writeVInt VInt}</li>
- *   <li>Positions --&gt; &lt;PositionDelta PayloadLength?&gt;<sup>TermFreq</sup></li>
- *   <li>PositionDelta --&gt; {@link DataOutput#writeVInt VInt}</li>
- *   <li>PayloadLength --&gt; {@link DataOutput#writeVInt VInt}</li>
- *   <li>PayloadData --&gt; {@link DataOutput#writeByte Byte}<sup>NumPayloadBytes</sup></li>
- *   <li>Offsets --&gt; &lt;{@link DataOutput#writeVInt VInt}, {@link DataOutput#writeVInt VInt}&gt;<sup>TermFreq</sup></li>
- * </ul>
- * <p>Notes:</p>
- * <ul>
- * <li>Flags byte stores whether this term vector has position, offset, payload.
- * information stored.</li>
- * <li>Term byte prefixes are shared. The PrefixLength is the number of initial
- * bytes from the previous term which must be pre-pended to a term's suffix
- * in order to form the term's bytes. Thus, if the previous term's text was "bone"
- * and the term is "boy", the PrefixLength is two and the suffix is "y".</li>
- * <li>PositionDelta is, if payloads are disabled for the term's field, the
- * difference between the position of the current occurrence in the document and
- * the previous occurrence (or zero, if this is the first occurrence in this
- * document). If payloads are enabled for the term's field, then PositionDelta/2
- * is the difference between the current and the previous position. If payloads
- * are enabled and PositionDelta is odd, then PayloadLength is stored, indicating
- * the length of the payload at the current term position.</li>
- * <li>PayloadData is metadata associated with a term position. If
- * PayloadLength is stored at the current position, then it indicates the length
- * of this payload. If PayloadLength is not stored, then this payload has the same
- * length as the payload at the previous position. PayloadData encodes the 
- * concatenated bytes for all of a terms occurrences.</li>
- * <li>Offsets are stored as delta encoded VInts. The first VInt is the
- * startOffset, the second is the endOffset.</li>
- * </ul>
- * </li>
- * </ol>
+ * @deprecated only for reading 4.0 and 4.1 segments
  */
+@Deprecated
 public class Lucene40TermVectorsFormat extends TermVectorsFormat {
 
   /** Sole constructor. */
@@ -120,7 +39,7 @@
   }
   
   @Override
-  public TermVectorsReader vectorsReader(Directory directory, SegmentInfo segmentInfo, FieldInfos fieldInfos, IOContext context) throws IOException {
+  public final TermVectorsReader vectorsReader(Directory directory, SegmentInfo segmentInfo, FieldInfos fieldInfos, IOContext context) throws IOException {
     return new Lucene40TermVectorsReader(directory, segmentInfo, fieldInfos, context);
   }
 
Index: lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40TermVectorsReader.java
===================================================================
--- lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40TermVectorsReader.java	(revision 1629405)
+++ lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40TermVectorsReader.java	(working copy)
@@ -48,12 +48,10 @@
 
 /**
  * Lucene 4.0 Term Vectors reader.
- * <p>
- * It reads .tvd, .tvf, and .tvx files.
- * 
- * @see Lucene40TermVectorsFormat
+ * @deprecated only for reading 4.0 and 4.1 segments
  */
-public class Lucene40TermVectorsReader extends TermVectorsReader implements Closeable {
+@Deprecated
+final class Lucene40TermVectorsReader extends TermVectorsReader implements Closeable {
 
   static final byte STORE_POSITIONS_WITH_TERMVECTOR = 0x1;
 
Index: lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene41/Lucene41Codec.java
===================================================================
--- lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene41/Lucene41Codec.java	(revision 1629405)
+++ lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene41/Lucene41Codec.java	(working copy)
@@ -17,11 +17,9 @@
  * limitations under the License.
  */
 
-import java.io.IOException;
-
 import org.apache.lucene.codecs.Codec;
+import org.apache.lucene.codecs.CompoundFormat;
 import org.apache.lucene.codecs.FieldInfosFormat;
-import org.apache.lucene.codecs.FilterCodec;
 import org.apache.lucene.codecs.LiveDocsFormat;
 import org.apache.lucene.codecs.PostingsFormat;
 import org.apache.lucene.codecs.SegmentInfoFormat;
@@ -28,10 +26,8 @@
 import org.apache.lucene.codecs.DocValuesFormat;
 import org.apache.lucene.codecs.NormsFormat;
 import org.apache.lucene.codecs.StoredFieldsFormat;
-import org.apache.lucene.codecs.StoredFieldsWriter;
 import org.apache.lucene.codecs.TermVectorsFormat;
-import org.apache.lucene.codecs.compressing.CompressingStoredFieldsFormat;
-import org.apache.lucene.codecs.compressing.CompressionMode;
+import org.apache.lucene.codecs.lucene40.Lucene40CompoundFormat;
 import org.apache.lucene.codecs.lucene40.Lucene40DocValuesFormat;
 import org.apache.lucene.codecs.lucene40.Lucene40FieldInfosFormat;
 import org.apache.lucene.codecs.lucene40.Lucene40LiveDocsFormat;
@@ -39,33 +35,19 @@
 import org.apache.lucene.codecs.lucene40.Lucene40SegmentInfoFormat;
 import org.apache.lucene.codecs.lucene40.Lucene40TermVectorsFormat;
 import org.apache.lucene.codecs.perfield.PerFieldPostingsFormat;
-import org.apache.lucene.index.SegmentInfo;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
 
 /**
- * Implements the Lucene 4.1 index format, with configurable per-field postings formats.
- * <p>
- * If you want to reuse functionality of this codec in another codec, extend
- * {@link FilterCodec}.
- *
- * @see org.apache.lucene.codecs.lucene41 package documentation for file format details.
- * @deprecated Only for reading old 4.0 segments
- * @lucene.experimental
+ * Implements the Lucene 4.1 index format
+ * @deprecated Only for reading old 4.1 segments
  */
 @Deprecated
 public class Lucene41Codec extends Codec {
-  // TODO: slightly evil
-  private final StoredFieldsFormat fieldsFormat = new CompressingStoredFieldsFormat("Lucene41StoredFields", CompressionMode.FAST, 1 << 14) {
-    @Override
-    public StoredFieldsWriter fieldsWriter(Directory directory, SegmentInfo si, IOContext context) throws IOException {
-      throw new UnsupportedOperationException("this codec can only be used for reading");
-    }
-  };
+  private final StoredFieldsFormat fieldsFormat = new Lucene41StoredFieldsFormat();
   private final TermVectorsFormat vectorsFormat = new Lucene40TermVectorsFormat();
   private final FieldInfosFormat fieldInfosFormat = new Lucene40FieldInfosFormat();
   private final SegmentInfoFormat infosFormat = new Lucene40SegmentInfoFormat();
   private final LiveDocsFormat liveDocsFormat = new Lucene40LiveDocsFormat();
+  private final CompoundFormat compoundFormat = new Lucene40CompoundFormat();
   
   private final PostingsFormat postingsFormat = new PerFieldPostingsFormat() {
     @Override
@@ -79,7 +61,6 @@
     super("Lucene41");
   }
   
-  // TODO: slightly evil
   @Override
   public StoredFieldsFormat storedFieldsFormat() {
     return fieldsFormat;
@@ -109,6 +90,11 @@
   public final LiveDocsFormat liveDocsFormat() {
     return liveDocsFormat;
   }
+  
+  @Override
+  public CompoundFormat compoundFormat() {
+    return compoundFormat;
+  }
 
   /** Returns the postings format that should be used for writing 
    *  new segments of <code>field</code>.
Index: lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene41/Lucene41StoredFieldsFormat.java
===================================================================
--- lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene41/Lucene41StoredFieldsFormat.java	(revision 0)
+++ lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene41/Lucene41StoredFieldsFormat.java	(working copy)
@@ -0,0 +1,56 @@
+package org.apache.lucene.codecs.lucene41;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.codecs.StoredFieldsFormat;
+import org.apache.lucene.codecs.StoredFieldsReader;
+import org.apache.lucene.codecs.StoredFieldsWriter;
+import org.apache.lucene.codecs.compressing.CompressionMode;
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+
+/**
+ * Lucene 4.1 stored fields format.
+ * @deprecated only for reading old 4.x segments
+ */
+@Deprecated
+public class Lucene41StoredFieldsFormat extends StoredFieldsFormat {
+  static final String FORMAT_NAME = "Lucene41StoredFields";
+  static final String SEGMENT_SUFFIX = "";
+  static final CompressionMode COMPRESSION_MODE = CompressionMode.FAST;
+  static final int CHUNK_SIZE = 1 << 14;
+
+  @Override
+  public final StoredFieldsReader fieldsReader(Directory directory, SegmentInfo si, FieldInfos fn, IOContext context) throws IOException {
+    return new Lucene41StoredFieldsReader(directory, si, SEGMENT_SUFFIX, fn, context, FORMAT_NAME, COMPRESSION_MODE);
+  }
+
+  @Override
+  public StoredFieldsWriter fieldsWriter(Directory directory, SegmentInfo si, IOContext context) throws IOException {
+    throw new UnsupportedOperationException("this codec can only be used for reading");
+  }
+
+  @Override
+  public String toString() {
+    return getClass().getSimpleName() + "(compressionMode=" + COMPRESSION_MODE + ", chunkSize=" + CHUNK_SIZE + ")";
+  }
+}

Property changes on: lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene41/Lucene41StoredFieldsFormat.java
___________________________________________________________________
Added: svn:eol-style
## -0,0 +1 ##
+native
\ No newline at end of property
Index: lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene41/Lucene41StoredFieldsIndexReader.java
===================================================================
--- lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene41/Lucene41StoredFieldsIndexReader.java	(revision 0)
+++ lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene41/Lucene41StoredFieldsIndexReader.java	(working copy)
@@ -0,0 +1,214 @@
+package org.apache.lucene.codecs.lucene41;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import static org.apache.lucene.util.BitUtil.zigZagDecode;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.List;
+
+import org.apache.lucene.index.CorruptIndexException;
+import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.util.Accountable;
+import org.apache.lucene.util.Accountables;
+import org.apache.lucene.util.ArrayUtil;
+import org.apache.lucene.util.RamUsageEstimator;
+import org.apache.lucene.util.packed.PackedInts;
+
+/**
+ * Reader for 4.x stored fields/term vectors index
+ * @deprecated only for reading old segments
+ */
+@Deprecated
+public final class Lucene41StoredFieldsIndexReader implements Cloneable, Accountable {
+
+  private static final long BASE_RAM_BYTES_USED = RamUsageEstimator.shallowSizeOfInstance(Lucene41StoredFieldsIndexReader.class);
+
+  final int maxDoc;
+  final int[] docBases;
+  final long[] startPointers;
+  final int[] avgChunkDocs;
+  final long[] avgChunkSizes;
+  final PackedInts.Reader[] docBasesDeltas; // delta from the avg
+  final PackedInts.Reader[] startPointersDeltas; // delta from the avg
+
+  // It is the responsibility of the caller to close fieldsIndexIn after this constructor
+  // has been called
+  public Lucene41StoredFieldsIndexReader(IndexInput fieldsIndexIn, SegmentInfo si) throws IOException {
+    maxDoc = si.getDocCount();
+    int[] docBases = new int[16];
+    long[] startPointers = new long[16];
+    int[] avgChunkDocs = new int[16];
+    long[] avgChunkSizes = new long[16];
+    PackedInts.Reader[] docBasesDeltas = new PackedInts.Reader[16];
+    PackedInts.Reader[] startPointersDeltas = new PackedInts.Reader[16];
+
+    final int packedIntsVersion = fieldsIndexIn.readVInt();
+
+    int blockCount = 0;
+
+    for (;;) {
+      final int numChunks = fieldsIndexIn.readVInt();
+      if (numChunks == 0) {
+        break;
+      }
+      if (blockCount == docBases.length) {
+        final int newSize = ArrayUtil.oversize(blockCount + 1, 8);
+        docBases = Arrays.copyOf(docBases, newSize);
+        startPointers = Arrays.copyOf(startPointers, newSize);
+        avgChunkDocs = Arrays.copyOf(avgChunkDocs, newSize);
+        avgChunkSizes = Arrays.copyOf(avgChunkSizes, newSize);
+        docBasesDeltas = Arrays.copyOf(docBasesDeltas, newSize);
+        startPointersDeltas = Arrays.copyOf(startPointersDeltas, newSize);
+      }
+
+      // doc bases
+      docBases[blockCount] = fieldsIndexIn.readVInt();
+      avgChunkDocs[blockCount] = fieldsIndexIn.readVInt();
+      final int bitsPerDocBase = fieldsIndexIn.readVInt();
+      if (bitsPerDocBase > 32) {
+        throw new CorruptIndexException("Corrupted bitsPerDocBase: " + bitsPerDocBase, fieldsIndexIn);
+      }
+      docBasesDeltas[blockCount] = PackedInts.getReaderNoHeader(fieldsIndexIn, PackedInts.Format.PACKED, packedIntsVersion, numChunks, bitsPerDocBase);
+
+      // start pointers
+      startPointers[blockCount] = fieldsIndexIn.readVLong();
+      avgChunkSizes[blockCount] = fieldsIndexIn.readVLong();
+      final int bitsPerStartPointer = fieldsIndexIn.readVInt();
+      if (bitsPerStartPointer > 64) {
+        throw new CorruptIndexException("Corrupted bitsPerStartPointer: " + bitsPerStartPointer, fieldsIndexIn);
+      }
+      startPointersDeltas[blockCount] = PackedInts.getReaderNoHeader(fieldsIndexIn, PackedInts.Format.PACKED, packedIntsVersion, numChunks, bitsPerStartPointer);
+
+      ++blockCount;
+    }
+
+    this.docBases = Arrays.copyOf(docBases, blockCount);
+    this.startPointers = Arrays.copyOf(startPointers, blockCount);
+    this.avgChunkDocs = Arrays.copyOf(avgChunkDocs, blockCount);
+    this.avgChunkSizes = Arrays.copyOf(avgChunkSizes, blockCount);
+    this.docBasesDeltas = Arrays.copyOf(docBasesDeltas, blockCount);
+    this.startPointersDeltas = Arrays.copyOf(startPointersDeltas, blockCount);
+  }
+
+  private int block(int docID) {
+    int lo = 0, hi = docBases.length - 1;
+    while (lo <= hi) {
+      final int mid = (lo + hi) >>> 1;
+      final int midValue = docBases[mid];
+      if (midValue == docID) {
+        return mid;
+      } else if (midValue < docID) {
+        lo = mid + 1;
+      } else {
+        hi = mid - 1;
+      }
+    }
+    return hi;
+  }
+
+  private int relativeDocBase(int block, int relativeChunk) {
+    final int expected = avgChunkDocs[block] * relativeChunk;
+    final long delta = zigZagDecode(docBasesDeltas[block].get(relativeChunk));
+    return expected + (int) delta;
+  }
+
+  private long relativeStartPointer(int block, int relativeChunk) {
+    final long expected = avgChunkSizes[block] * relativeChunk;
+    final long delta = zigZagDecode(startPointersDeltas[block].get(relativeChunk));
+    return expected + delta;
+  }
+
+  private int relativeChunk(int block, int relativeDoc) {
+    int lo = 0, hi = docBasesDeltas[block].size() - 1;
+    while (lo <= hi) {
+      final int mid = (lo + hi) >>> 1;
+      final int midValue = relativeDocBase(block, mid);
+      if (midValue == relativeDoc) {
+        return mid;
+      } else if (midValue < relativeDoc) {
+        lo = mid + 1;
+      } else {
+        hi = mid - 1;
+      }
+    }
+    return hi;
+  }
+
+  public long getStartPointer(int docID) {
+    if (docID < 0 || docID >= maxDoc) {
+      throw new IllegalArgumentException("docID out of range [0-" + maxDoc + "]: " + docID);
+    }
+    final int block = block(docID);
+    final int relativeChunk = relativeChunk(block, docID - docBases[block]);
+    return startPointers[block] + relativeStartPointer(block, relativeChunk);
+  }
+
+  @Override
+  public Lucene41StoredFieldsIndexReader clone() {
+    return this;
+  }
+
+  @Override
+  public long ramBytesUsed() {
+    long res = BASE_RAM_BYTES_USED;
+
+    res += RamUsageEstimator.shallowSizeOf(docBasesDeltas);
+    for (PackedInts.Reader r : docBasesDeltas) {
+      res += r.ramBytesUsed();
+    }
+    res += RamUsageEstimator.shallowSizeOf(startPointersDeltas);
+    for (PackedInts.Reader r : startPointersDeltas) {
+      res += r.ramBytesUsed();
+    }
+
+    res += RamUsageEstimator.sizeOf(docBases);
+    res += RamUsageEstimator.sizeOf(startPointers);
+    res += RamUsageEstimator.sizeOf(avgChunkDocs); 
+    res += RamUsageEstimator.sizeOf(avgChunkSizes);
+
+    return res;
+  }
+
+  @Override
+  public Iterable<? extends Accountable> getChildResources() {
+    List<Accountable> resources = new ArrayList<>();
+    
+    long docBaseDeltaBytes = RamUsageEstimator.shallowSizeOf(docBasesDeltas);
+    for (PackedInts.Reader r : docBasesDeltas) {
+      docBaseDeltaBytes += r.ramBytesUsed();
+    }
+    resources.add(Accountables.namedAccountable("doc base deltas", docBaseDeltaBytes));
+    
+    long startPointerDeltaBytes = RamUsageEstimator.shallowSizeOf(startPointersDeltas);
+    for (PackedInts.Reader r : startPointersDeltas) {
+      startPointerDeltaBytes += r.ramBytesUsed();
+    }
+    resources.add(Accountables.namedAccountable("start pointer deltas", startPointerDeltaBytes));
+    
+    return resources;
+  }
+
+  @Override
+  public String toString() {
+    return getClass().getSimpleName() + "(blocks=" + docBases.length + ")";
+  }
+}

Property changes on: lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene41/Lucene41StoredFieldsIndexReader.java
___________________________________________________________________
Added: svn:eol-style
## -0,0 +1 ##
+native
\ No newline at end of property
Index: lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene41/Lucene41StoredFieldsReader.java
===================================================================
--- lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene41/Lucene41StoredFieldsReader.java	(revision 0)
+++ lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene41/Lucene41StoredFieldsReader.java	(working copy)
@@ -0,0 +1,417 @@
+package org.apache.lucene.codecs.lucene41;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.EOFException;
+import java.io.IOException;
+import java.nio.charset.StandardCharsets;
+import java.util.Collections;
+
+import org.apache.lucene.codecs.CodecUtil;
+import org.apache.lucene.codecs.StoredFieldsReader;
+import org.apache.lucene.codecs.compressing.CompressionMode;
+import org.apache.lucene.codecs.compressing.Decompressor;
+import org.apache.lucene.index.CorruptIndexException;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.index.StoredFieldVisitor;
+import org.apache.lucene.store.AlreadyClosedException;
+import org.apache.lucene.store.ByteArrayDataInput;
+import org.apache.lucene.store.ChecksumIndexInput;
+import org.apache.lucene.store.DataInput;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.util.Accountable;
+import org.apache.lucene.util.Accountables;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util.packed.PackedInts;
+
+/**
+ * {@link StoredFieldsReader} impl for {@code Lucene41StoredFieldsFormat}.
+ * @deprecated only for reading old segments
+ */
+@Deprecated
+final class Lucene41StoredFieldsReader extends StoredFieldsReader {
+
+  // Do not reuse the decompression buffer when there is more than 32kb to decompress
+  private static final int BUFFER_REUSE_THRESHOLD = 1 << 15;
+  
+  static final int         STRING = 0x00;
+  static final int       BYTE_ARR = 0x01;
+  static final int    NUMERIC_INT = 0x02;
+  static final int  NUMERIC_FLOAT = 0x03;
+  static final int   NUMERIC_LONG = 0x04;
+  static final int NUMERIC_DOUBLE = 0x05;
+  
+  static final String CODEC_SFX_IDX = "Index";
+  static final String CODEC_SFX_DAT = "Data";
+  
+  static final int TYPE_BITS = PackedInts.bitsRequired(NUMERIC_DOUBLE);
+  static final int TYPE_MASK = (int) PackedInts.maxValue(TYPE_BITS);
+  
+  static final int VERSION_START = 0;
+  static final int VERSION_BIG_CHUNKS = 1;
+  static final int VERSION_CHECKSUM = 2;
+  static final int VERSION_CURRENT = VERSION_CHECKSUM;
+  
+  /** Extension of stored fields file */
+  public static final String FIELDS_EXTENSION = "fdt";
+  
+  /** Extension of stored fields index file */
+  public static final String FIELDS_INDEX_EXTENSION = "fdx";
+
+  private final int version;
+  private final FieldInfos fieldInfos;
+  private final Lucene41StoredFieldsIndexReader indexReader;
+  private final long maxPointer;
+  private final IndexInput fieldsStream;
+  private final int chunkSize;
+  private final int packedIntsVersion;
+  private final CompressionMode compressionMode;
+  private final Decompressor decompressor;
+  private final BytesRef bytes;
+  private final int numDocs;
+  private boolean closed;
+
+  // used by clone
+  private Lucene41StoredFieldsReader(Lucene41StoredFieldsReader reader) {
+    this.version = reader.version;
+    this.fieldInfos = reader.fieldInfos;
+    this.fieldsStream = reader.fieldsStream.clone();
+    this.indexReader = reader.indexReader.clone();
+    this.maxPointer = reader.maxPointer;
+    this.chunkSize = reader.chunkSize;
+    this.packedIntsVersion = reader.packedIntsVersion;
+    this.compressionMode = reader.compressionMode;
+    this.decompressor = reader.decompressor.clone();
+    this.numDocs = reader.numDocs;
+    this.bytes = new BytesRef(reader.bytes.bytes.length);
+    this.closed = false;
+  }
+
+  /** Sole constructor. */
+  public Lucene41StoredFieldsReader(Directory d, SegmentInfo si, String segmentSuffix, FieldInfos fn,
+      IOContext context, String formatName, CompressionMode compressionMode) throws IOException {
+    this.compressionMode = compressionMode;
+    final String segment = si.name;
+    boolean success = false;
+    fieldInfos = fn;
+    numDocs = si.getDocCount();
+    ChecksumIndexInput indexStream = null;
+    try {
+      final String indexStreamFN = IndexFileNames.segmentFileName(segment, segmentSuffix, FIELDS_INDEX_EXTENSION);
+      final String fieldsStreamFN = IndexFileNames.segmentFileName(segment, segmentSuffix, FIELDS_EXTENSION);
+      // Load the index into memory
+      indexStream = d.openChecksumInput(indexStreamFN, context);
+      final String codecNameIdx = formatName + CODEC_SFX_IDX;
+      version = CodecUtil.checkHeader(indexStream, codecNameIdx, VERSION_START, VERSION_CURRENT);
+      assert CodecUtil.headerLength(codecNameIdx) == indexStream.getFilePointer();
+      indexReader = new Lucene41StoredFieldsIndexReader(indexStream, si);
+
+      long maxPointer = -1;
+      
+      if (version >= VERSION_CHECKSUM) {
+        maxPointer = indexStream.readVLong();
+        CodecUtil.checkFooter(indexStream);
+      } else {
+        CodecUtil.checkEOF(indexStream);
+      }
+      indexStream.close();
+      indexStream = null;
+
+      // Open the data file and read metadata
+      fieldsStream = d.openInput(fieldsStreamFN, context);
+      if (version >= VERSION_CHECKSUM) {
+        if (maxPointer + CodecUtil.footerLength() != fieldsStream.length()) {
+          throw new CorruptIndexException("Invalid fieldsStream maxPointer (file truncated?): maxPointer=" + maxPointer + ", length=" + fieldsStream.length(), fieldsStream);
+        }
+      } else {
+        maxPointer = fieldsStream.length();
+      }
+      this.maxPointer = maxPointer;
+      final String codecNameDat = formatName + CODEC_SFX_DAT;
+      final int fieldsVersion = CodecUtil.checkHeader(fieldsStream, codecNameDat, VERSION_START, VERSION_CURRENT);
+      if (version != fieldsVersion) {
+        throw new CorruptIndexException("Version mismatch between stored fields index and data: " + version + " != " + fieldsVersion, fieldsStream);
+      }
+      assert CodecUtil.headerLength(codecNameDat) == fieldsStream.getFilePointer();
+
+      if (version >= VERSION_BIG_CHUNKS) {
+        chunkSize = fieldsStream.readVInt();
+      } else {
+        chunkSize = -1;
+      }
+      packedIntsVersion = fieldsStream.readVInt();
+      decompressor = compressionMode.newDecompressor();
+      this.bytes = new BytesRef();
+      
+      if (version >= VERSION_CHECKSUM) {
+        // NOTE: data file is too costly to verify checksum against all the bytes on open,
+        // but for now we at least verify proper structure of the checksum footer: which looks
+        // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption
+        // such as file truncation.
+        CodecUtil.retrieveChecksum(fieldsStream);
+      }
+
+      success = true;
+    } finally {
+      if (!success) {
+        IOUtils.closeWhileHandlingException(this, indexStream);
+      }
+    }
+  }
+
+  /**
+   * @throws AlreadyClosedException if this FieldsReader is closed
+   */
+  private void ensureOpen() throws AlreadyClosedException {
+    if (closed) {
+      throw new AlreadyClosedException("this FieldsReader is closed");
+    }
+  }
+
+  /** 
+   * Close the underlying {@link IndexInput}s.
+   */
+  @Override
+  public void close() throws IOException {
+    if (!closed) {
+      IOUtils.close(fieldsStream);
+      closed = true;
+    }
+  }
+
+  private static void readField(DataInput in, StoredFieldVisitor visitor, FieldInfo info, int bits) throws IOException {
+    switch (bits & TYPE_MASK) {
+      case BYTE_ARR:
+        int length = in.readVInt();
+        byte[] data = new byte[length];
+        in.readBytes(data, 0, length);
+        visitor.binaryField(info, data);
+        break;
+      case STRING:
+        length = in.readVInt();
+        data = new byte[length];
+        in.readBytes(data, 0, length);
+        visitor.stringField(info, new String(data, StandardCharsets.UTF_8));
+        break;
+      case NUMERIC_INT:
+        visitor.intField(info, in.readInt());
+        break;
+      case NUMERIC_FLOAT:
+        visitor.floatField(info, Float.intBitsToFloat(in.readInt()));
+        break;
+      case NUMERIC_LONG:
+        visitor.longField(info, in.readLong());
+        break;
+      case NUMERIC_DOUBLE:
+        visitor.doubleField(info, Double.longBitsToDouble(in.readLong()));
+        break;
+      default:
+        throw new AssertionError("Unknown type flag: " + Integer.toHexString(bits));
+    }
+  }
+
+  private static void skipField(DataInput in, int bits) throws IOException {
+    switch (bits & TYPE_MASK) {
+      case BYTE_ARR:
+      case STRING:
+        final int length = in.readVInt();
+        in.skipBytes(length);
+        break;
+      case NUMERIC_INT:
+      case NUMERIC_FLOAT:
+        in.readInt();
+        break;
+      case NUMERIC_LONG:
+      case NUMERIC_DOUBLE:
+        in.readLong();
+        break;
+      default:
+        throw new AssertionError("Unknown type flag: " + Integer.toHexString(bits));
+    }
+  }
+
+  @Override
+  public void visitDocument(int docID, StoredFieldVisitor visitor)
+      throws IOException {
+    fieldsStream.seek(indexReader.getStartPointer(docID));
+
+    final int docBase = fieldsStream.readVInt();
+    final int chunkDocs = fieldsStream.readVInt();
+    if (docID < docBase
+        || docID >= docBase + chunkDocs
+        || docBase + chunkDocs > numDocs) {
+      throw new CorruptIndexException("Corrupted: docID=" + docID
+          + ", docBase=" + docBase + ", chunkDocs=" + chunkDocs
+          + ", numDocs=" + numDocs, fieldsStream);
+    }
+
+    final int numStoredFields, offset, length, totalLength;
+    if (chunkDocs == 1) {
+      numStoredFields = fieldsStream.readVInt();
+      offset = 0;
+      length = fieldsStream.readVInt();
+      totalLength = length;
+    } else {
+      final int bitsPerStoredFields = fieldsStream.readVInt();
+      if (bitsPerStoredFields == 0) {
+        numStoredFields = fieldsStream.readVInt();
+      } else if (bitsPerStoredFields > 31) {
+        throw new CorruptIndexException("bitsPerStoredFields=" + bitsPerStoredFields, fieldsStream);
+      } else {
+        final long filePointer = fieldsStream.getFilePointer();
+        final PackedInts.Reader reader = PackedInts.getDirectReaderNoHeader(fieldsStream, PackedInts.Format.PACKED, packedIntsVersion, chunkDocs, bitsPerStoredFields);
+        numStoredFields = (int) (reader.get(docID - docBase));
+        fieldsStream.seek(filePointer + PackedInts.Format.PACKED.byteCount(packedIntsVersion, chunkDocs, bitsPerStoredFields));
+      }
+
+      final int bitsPerLength = fieldsStream.readVInt();
+      if (bitsPerLength == 0) {
+        length = fieldsStream.readVInt();
+        offset = (docID - docBase) * length;
+        totalLength = chunkDocs * length;
+      } else if (bitsPerStoredFields > 31) {
+        throw new CorruptIndexException("bitsPerLength=" + bitsPerLength, fieldsStream);
+      } else {
+        final PackedInts.ReaderIterator it = PackedInts.getReaderIteratorNoHeader(fieldsStream, PackedInts.Format.PACKED, packedIntsVersion, chunkDocs, bitsPerLength, 1);
+        int off = 0;
+        for (int i = 0; i < docID - docBase; ++i) {
+          off += it.next();
+        }
+        offset = off;
+        length = (int) it.next();
+        off += length;
+        for (int i = docID - docBase + 1; i < chunkDocs; ++i) {
+          off += it.next();
+        }
+        totalLength = off;
+      }
+    }
+
+    if ((length == 0) != (numStoredFields == 0)) {
+      throw new CorruptIndexException("length=" + length + ", numStoredFields=" + numStoredFields, fieldsStream);
+    }
+    if (numStoredFields == 0) {
+      // nothing to do
+      return;
+    }
+
+    final DataInput documentInput;
+    if (version >= VERSION_BIG_CHUNKS && totalLength >= 2 * chunkSize) {
+      assert chunkSize > 0;
+      assert offset < chunkSize;
+
+      decompressor.decompress(fieldsStream, chunkSize, offset, Math.min(length, chunkSize - offset), bytes);
+      documentInput = new DataInput() {
+
+        int decompressed = bytes.length;
+
+        void fillBuffer() throws IOException {
+          assert decompressed <= length;
+          if (decompressed == length) {
+            throw new EOFException();
+          }
+          final int toDecompress = Math.min(length - decompressed, chunkSize);
+          decompressor.decompress(fieldsStream, toDecompress, 0, toDecompress, bytes);
+          decompressed += toDecompress;
+        }
+
+        @Override
+        public byte readByte() throws IOException {
+          if (bytes.length == 0) {
+            fillBuffer();
+          }
+          --bytes.length;
+          return bytes.bytes[bytes.offset++];
+        }
+
+        @Override
+        public void readBytes(byte[] b, int offset, int len) throws IOException {
+          while (len > bytes.length) {
+            System.arraycopy(bytes.bytes, bytes.offset, b, offset, bytes.length);
+            len -= bytes.length;
+            offset += bytes.length;
+            fillBuffer();
+          }
+          System.arraycopy(bytes.bytes, bytes.offset, b, offset, len);
+          bytes.offset += len;
+          bytes.length -= len;
+        }
+
+      };
+    } else {
+      final BytesRef bytes = totalLength <= BUFFER_REUSE_THRESHOLD ? this.bytes : new BytesRef();
+      decompressor.decompress(fieldsStream, totalLength, offset, length, bytes);
+      assert bytes.length == length;
+      documentInput = new ByteArrayDataInput(bytes.bytes, bytes.offset, bytes.length);
+    }
+
+    for (int fieldIDX = 0; fieldIDX < numStoredFields; fieldIDX++) {
+      final long infoAndBits = documentInput.readVLong();
+      final int fieldNumber = (int) (infoAndBits >>> TYPE_BITS);
+      final FieldInfo fieldInfo = fieldInfos.fieldInfo(fieldNumber);
+
+      final int bits = (int) (infoAndBits & TYPE_MASK);
+      assert bits <= NUMERIC_DOUBLE: "bits=" + Integer.toHexString(bits);
+
+      switch(visitor.needsField(fieldInfo)) {
+        case YES:
+          readField(documentInput, visitor, fieldInfo, bits);
+          break;
+        case NO:
+          skipField(documentInput, bits);
+          break;
+        case STOP:
+          return;
+      }
+    }
+  }
+
+  @Override
+  public StoredFieldsReader clone() {
+    ensureOpen();
+    return new Lucene41StoredFieldsReader(this);
+  }
+
+  @Override
+  public long ramBytesUsed() {
+    return indexReader.ramBytesUsed();
+  }
+  
+  @Override
+  public Iterable<? extends Accountable> getChildResources() {
+    return Collections.singleton(Accountables.namedAccountable("stored field index", indexReader));
+  }
+
+  @Override
+  public void checkIntegrity() throws IOException {
+    if (version >= VERSION_CHECKSUM) {
+      CodecUtil.checksumEntireFile(fieldsStream);
+    }
+  }
+
+  @Override
+  public String toString() {
+    return getClass().getSimpleName() + "(mode=" + compressionMode + ",chunksize=" + chunkSize + ")";
+  }
+}

Property changes on: lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene41/Lucene41StoredFieldsReader.java
___________________________________________________________________
Added: svn:eol-style
## -0,0 +1 ##
+native
\ No newline at end of property
Index: lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene410/Lucene410Codec.java
===================================================================
--- lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene410/Lucene410Codec.java	(revision 1629405)
+++ lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene410/Lucene410Codec.java	(working copy)
@@ -20,9 +20,9 @@
 import java.io.IOException;
 
 import org.apache.lucene.codecs.Codec;
+import org.apache.lucene.codecs.CompoundFormat;
 import org.apache.lucene.codecs.DocValuesFormat;
 import org.apache.lucene.codecs.FieldInfosFormat;
-import org.apache.lucene.codecs.FilterCodec;
 import org.apache.lucene.codecs.LiveDocsFormat;
 import org.apache.lucene.codecs.NormsConsumer;
 import org.apache.lucene.codecs.NormsFormat;
@@ -30,6 +30,7 @@
 import org.apache.lucene.codecs.SegmentInfoFormat;
 import org.apache.lucene.codecs.StoredFieldsFormat;
 import org.apache.lucene.codecs.TermVectorsFormat;
+import org.apache.lucene.codecs.lucene40.Lucene40CompoundFormat;
 import org.apache.lucene.codecs.lucene40.Lucene40LiveDocsFormat;
 import org.apache.lucene.codecs.lucene41.Lucene41StoredFieldsFormat;
 import org.apache.lucene.codecs.lucene42.Lucene42TermVectorsFormat;
@@ -41,18 +42,10 @@
 import org.apache.lucene.index.SegmentWriteState;
 
 /**
- * Implements the Lucene 4.10 index format, with configurable per-field postings
- * and docvalues formats.
- * <p>
- * If you want to reuse functionality of this codec in another codec, extend
- * {@link FilterCodec}.
- *
- * @see org.apache.lucene.codecs.lucene410 package documentation for file format details.
- * @lucene.experimental
+ * Implements the Lucene 4.10 codec
+ * @deprecated only for reading old 4.10 segments
  */
-// NOTE: if we make largish changes in a minor release, easier to just make Lucene411Codec or whatever
-// if they are backwards compatible or smallish we can probably do the backwards in the postingsreader
-// (it writes a minor version, etc).
+@Deprecated
 public class Lucene410Codec extends Codec {
   private final StoredFieldsFormat fieldsFormat = new Lucene41StoredFieldsFormat();
   private final TermVectorsFormat vectorsFormat = new Lucene42TermVectorsFormat();
@@ -59,6 +52,7 @@
   private final FieldInfosFormat fieldInfosFormat = new Lucene46FieldInfosFormat();
   private final SegmentInfoFormat segmentInfosFormat = new Lucene46SegmentInfoFormat();
   private final LiveDocsFormat liveDocsFormat = new Lucene40LiveDocsFormat();
+  private final CompoundFormat compoundFormat = new Lucene40CompoundFormat();
   
   private final PostingsFormat postingsFormat = new PerFieldPostingsFormat() {
     @Override
@@ -108,6 +102,11 @@
   public final LiveDocsFormat liveDocsFormat() {
     return liveDocsFormat;
   }
+  
+  @Override
+  public CompoundFormat compoundFormat() {
+    return compoundFormat;
+  }
 
   /** Returns the postings format that should be used for writing 
    *  new segments of <code>field</code>.
Index: lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene42/Lucene42Codec.java
===================================================================
--- lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene42/Lucene42Codec.java	(revision 1629405)
+++ lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene42/Lucene42Codec.java	(working copy)
@@ -20,9 +20,9 @@
 import java.io.IOException;
 
 import org.apache.lucene.codecs.Codec;
+import org.apache.lucene.codecs.CompoundFormat;
 import org.apache.lucene.codecs.DocValuesFormat;
 import org.apache.lucene.codecs.FieldInfosFormat;
-import org.apache.lucene.codecs.FilterCodec;
 import org.apache.lucene.codecs.LiveDocsFormat;
 import org.apache.lucene.codecs.NormsConsumer;
 import org.apache.lucene.codecs.NormsFormat;
@@ -30,6 +30,7 @@
 import org.apache.lucene.codecs.SegmentInfoFormat;
 import org.apache.lucene.codecs.StoredFieldsFormat;
 import org.apache.lucene.codecs.TermVectorsFormat;
+import org.apache.lucene.codecs.lucene40.Lucene40CompoundFormat;
 import org.apache.lucene.codecs.lucene40.Lucene40LiveDocsFormat;
 import org.apache.lucene.codecs.lucene40.Lucene40SegmentInfoFormat;
 import org.apache.lucene.codecs.lucene41.Lucene41StoredFieldsFormat;
@@ -38,19 +39,9 @@
 import org.apache.lucene.index.SegmentWriteState;
 
 /**
- * Implements the Lucene 4.2 index format, with configurable per-field postings
- * and docvalues formats.
- * <p>
- * If you want to reuse functionality of this codec in another codec, extend
- * {@link FilterCodec}.
- *
- * @see org.apache.lucene.codecs.lucene42 package documentation for file format details.
- * @lucene.experimental
+ * Implements the Lucene 4.2 index format
  * @deprecated Only for reading old 4.2 segments
  */
-// NOTE: if we make largish changes in a minor release, easier to just make Lucene43Codec or whatever
-// if they are backwards compatible or smallish we can probably do the backwards in the postingsreader
-// (it writes a minor version, etc).
 @Deprecated
 public class Lucene42Codec extends Codec {
   private final StoredFieldsFormat fieldsFormat = new Lucene41StoredFieldsFormat();
@@ -58,6 +49,7 @@
   private final FieldInfosFormat fieldInfosFormat = new Lucene42FieldInfosFormat();
   private final SegmentInfoFormat infosFormat = new Lucene40SegmentInfoFormat();
   private final LiveDocsFormat liveDocsFormat = new Lucene40LiveDocsFormat();
+  private final CompoundFormat compoundFormat = new Lucene40CompoundFormat();
   
   private final PostingsFormat postingsFormat = new PerFieldPostingsFormat() {
     @Override
@@ -80,12 +72,12 @@
   }
   
   @Override
-  public final StoredFieldsFormat storedFieldsFormat() {
+  public StoredFieldsFormat storedFieldsFormat() {
     return fieldsFormat;
   }
   
   @Override
-  public final TermVectorsFormat termVectorsFormat() {
+  public TermVectorsFormat termVectorsFormat() {
     return vectorsFormat;
   }
 
@@ -108,6 +100,11 @@
   public final LiveDocsFormat liveDocsFormat() {
     return liveDocsFormat;
   }
+  
+  @Override
+  public CompoundFormat compoundFormat() {
+    return compoundFormat;
+  }
 
   /** Returns the postings format that should be used for writing 
    *  new segments of <code>field</code>.
Index: lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene42/Lucene42DocValuesFormat.java
===================================================================
--- lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene42/Lucene42DocValuesFormat.java	(revision 1629405)
+++ lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene42/Lucene42DocValuesFormat.java	(working copy)
@@ -19,112 +19,15 @@
 
 import java.io.IOException;
 
-import org.apache.lucene.codecs.CodecUtil;
 import org.apache.lucene.codecs.DocValuesConsumer;
 import org.apache.lucene.codecs.DocValuesProducer;
 import org.apache.lucene.codecs.DocValuesFormat;
 import org.apache.lucene.index.SegmentReadState;
 import org.apache.lucene.index.SegmentWriteState;
-import org.apache.lucene.store.DataOutput;
-import org.apache.lucene.util.fst.FST;
-import org.apache.lucene.util.packed.MonotonicBlockPackedWriter;
 import org.apache.lucene.util.packed.PackedInts;
-import org.apache.lucene.util.packed.BlockPackedWriter;
 
 /**
  * Lucene 4.2 DocValues format.
- * <p>
- * Encodes the four per-document value types (Numeric,Binary,Sorted,SortedSet) with seven basic strategies.
- * <p>
- * <ul>
- *    <li>Delta-compressed Numerics: per-document integers written in blocks of 4096. For each block
- *        the minimum value is encoded, and each entry is a delta from that minimum value.
- *    <li>Table-compressed Numerics: when the number of unique values is very small, a lookup table
- *        is written instead. Each per-document entry is instead the ordinal to this table.
- *    <li>Uncompressed Numerics: when all values would fit into a single byte, and the 
- *        <code>acceptableOverheadRatio</code> would pack values into 8 bits per value anyway, they
- *        are written as absolute values (with no indirection or packing) for performance.
- *    <li>GCD-compressed Numerics: when all numbers share a common divisor, such as dates, the greatest
- *        common denominator (GCD) is computed, and quotients are stored using Delta-compressed Numerics.
- *    <li>Fixed-width Binary: one large concatenated byte[] is written, along with the fixed length.
- *        Each document's value can be addressed by maxDoc*length. 
- *    <li>Variable-width Binary: one large concatenated byte[] is written, along with end addresses 
- *        for each document. The addresses are written in blocks of 4096, with the current absolute
- *        start for the block, and the average (expected) delta per entry. For each document the 
- *        deviation from the delta (actual - expected) is written.
- *    <li>Sorted: an FST mapping deduplicated terms to ordinals is written, along with the per-document
- *        ordinals written using one of the numeric strategies above.
- *    <li>SortedSet: an FST mapping deduplicated terms to ordinals is written, along with the per-document
- *        ordinal list written using one of the binary strategies above.  
- * </ul>
- * <p>
- * Files:
- * <ol>
- *   <li><tt>.dvd</tt>: DocValues data</li>
- *   <li><tt>.dvm</tt>: DocValues metadata</li>
- * </ol>
- * <ol>
- *   <li><a name="dvm" id="dvm"></a>
- *   <p>The DocValues metadata or .dvm file.</p>
- *   <p>For DocValues field, this stores metadata, such as the offset into the 
- *      DocValues data (.dvd)</p>
- *   <p>DocValues metadata (.dvm) --&gt; Header,&lt;FieldNumber,EntryType,Entry&gt;<sup>NumFields</sup>,Footer</p>
- *   <ul>
- *     <li>Entry --&gt; NumericEntry | BinaryEntry | SortedEntry</li>
- *     <li>NumericEntry --&gt; DataOffset,CompressionType,PackedVersion</li>
- *     <li>BinaryEntry --&gt; DataOffset,DataLength,MinLength,MaxLength,PackedVersion?,BlockSize?</li>
- *     <li>SortedEntry --&gt; DataOffset,ValueCount</li>
- *     <li>FieldNumber,PackedVersion,MinLength,MaxLength,BlockSize,ValueCount --&gt; {@link DataOutput#writeVInt VInt}</li>
- *     <li>DataOffset,DataLength --&gt; {@link DataOutput#writeLong Int64}</li>
- *     <li>EntryType,CompressionType --&gt; {@link DataOutput#writeByte Byte}</li>
- *     <li>Header --&gt; {@link CodecUtil#writeHeader CodecHeader}</li>
- *     <li>Footer --&gt; {@link CodecUtil#writeFooter CodecFooter}</li>
- *   </ul>
- *   <p>Sorted fields have two entries: a SortedEntry with the FST metadata,
- *      and an ordinary NumericEntry for the document-to-ord metadata.</p>
- *   <p>SortedSet fields have two entries: a SortedEntry with the FST metadata,
- *      and an ordinary BinaryEntry for the document-to-ord-list metadata.</p>
- *   <p>FieldNumber of -1 indicates the end of metadata.</p>
- *   <p>EntryType is a 0 (NumericEntry), 1 (BinaryEntry, or 2 (SortedEntry)</p>
- *   <p>DataOffset is the pointer to the start of the data in the DocValues data (.dvd)</p>
- *   <p>CompressionType indicates how Numeric values will be compressed:
- *      <ul>
- *         <li>0 --&gt; delta-compressed. For each block of 4096 integers, every integer is delta-encoded
- *             from the minimum value within the block. 
- *         <li>1 --&gt; table-compressed. When the number of unique numeric values is small and it would save space,
- *             a lookup table of unique values is written, followed by the ordinal for each document.
- *         <li>2 --&gt; uncompressed. When the <code>acceptableOverheadRatio</code> parameter would upgrade the number
- *             of bits required to 8, and all values fit in a byte, these are written as absolute binary values
- *             for performance.
- *         <li>3 --&gt, gcd-compressed. When all integers share a common divisor, only quotients are stored
- *             using blocks of delta-encoded ints.
- *      </ul>
- *   <p>MinLength and MaxLength represent the min and max byte[] value lengths for Binary values.
- *      If they are equal, then all values are of a fixed size, and can be addressed as DataOffset + (docID * length).
- *      Otherwise, the binary values are of variable size, and packed integer metadata (PackedVersion,BlockSize)
- *      is written for the addresses.
- *   <li><a name="dvd" id="dvd"></a>
- *   <p>The DocValues data or .dvd file.</p>
- *   <p>For DocValues field, this stores the actual per-document data (the heavy-lifting)</p>
- *   <p>DocValues data (.dvd) --&gt; Header,&lt;NumericData | BinaryData | SortedData&gt;<sup>NumFields</sup>,Footer</p>
- *   <ul>
- *     <li>NumericData --&gt; DeltaCompressedNumerics | TableCompressedNumerics | UncompressedNumerics | GCDCompressedNumerics</li>
- *     <li>BinaryData --&gt;  {@link DataOutput#writeByte Byte}<sup>DataLength</sup>,Addresses</li>
- *     <li>SortedData --&gt; {@link FST FST&lt;Int64&gt;}</li>
- *     <li>DeltaCompressedNumerics --&gt; {@link BlockPackedWriter BlockPackedInts(blockSize=4096)}</li>
- *     <li>TableCompressedNumerics --&gt; TableSize,{@link DataOutput#writeLong Int64}<sup>TableSize</sup>,{@link PackedInts PackedInts}</li>
- *     <li>UncompressedNumerics --&gt; {@link DataOutput#writeByte Byte}<sup>maxdoc</sup></li>
- *     <li>Addresses --&gt; {@link MonotonicBlockPackedWriter MonotonicBlockPackedInts(blockSize=4096)}</li>
- *     <li>Footer --&gt; {@link CodecUtil#writeFooter CodecFooter}</li>
- *   </ul>
- *   <p>SortedSet entries store the list of ordinals in their BinaryData as a
- *      sequences of increasing {@link DataOutput#writeVLong vLong}s, delta-encoded.</p>       
- * </ol>
- * <p>
- * Limitations:
- * <ul>
- *   <li> Binary doc values can be at most {@link #MAX_BINARY_FIELD_LENGTH} in length.
- * </ul>
  * @deprecated Only for reading old 4.2 segments
  */
 @Deprecated
@@ -131,7 +34,7 @@
 public class Lucene42DocValuesFormat extends DocValuesFormat {
 
   /** Maximum length for each binary doc values field. */
-  public static final int MAX_BINARY_FIELD_LENGTH = (1 << 15) - 2;
+  static final int MAX_BINARY_FIELD_LENGTH = (1 << 15) - 2;
   
   final float acceptableOverheadRatio;
   
@@ -162,7 +65,7 @@
   }
   
   @Override
-  public DocValuesProducer fieldsProducer(SegmentReadState state) throws IOException {
+  public final DocValuesProducer fieldsProducer(SegmentReadState state) throws IOException {
     return new Lucene42DocValuesProducer(state, DATA_CODEC, DATA_EXTENSION, METADATA_CODEC, METADATA_EXTENSION);
   }
   
Index: lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene42/Lucene42DocValuesProducer.java
===================================================================
--- lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene42/Lucene42DocValuesProducer.java	(revision 1629405)
+++ lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene42/Lucene42DocValuesProducer.java	(working copy)
@@ -66,9 +66,11 @@
 import org.apache.lucene.util.packed.PackedInts;
 
 /**
- * Reader for {@link Lucene42DocValuesFormat}
+ * Reader for 4.2 docvalues
+ * @deprecated only for reading old 4.x segments
  */
-class Lucene42DocValuesProducer extends DocValuesProducer {
+@Deprecated
+final class Lucene42DocValuesProducer extends DocValuesProducer {
   // metadata maps (just file pointers and minimal stuff)
   private final Map<String,NumericEntry> numerics;
   private final Map<String,BinaryEntry> binaries;
@@ -104,9 +106,34 @@
   static final int VERSION_GCD_COMPRESSION = 1;
   static final int VERSION_CHECKSUM = 2;
   static final int VERSION_CURRENT = VERSION_CHECKSUM;
+  
+  private final boolean merging;
+
+  // clone for merge: when merging we don't do any instances.put()s
+  Lucene42DocValuesProducer(Lucene42DocValuesProducer original) throws IOException {
+    assert Thread.holdsLock(original);
+    numerics = original.numerics;
+    binaries = original.binaries;
+    fsts = original.fsts;
+    data = original.data.clone();
+    version = original.version;
+    numEntries = original.numEntries;
     
+    numericInstances.putAll(original.numericInstances);
+    binaryInstances.putAll(original.binaryInstances);
+    fstInstances.putAll(original.fstInstances);
+    numericInfo.putAll(original.numericInfo);
+    binaryInfo.putAll(original.binaryInfo);
+    addressInfo.putAll(original.addressInfo);
+    
+    maxDoc = original.maxDoc;
+    ramBytesUsed = new AtomicLong(original.ramBytesUsed.get());
+    merging = true;
+  }
+    
   Lucene42DocValuesProducer(SegmentReadState state, String dataCodec, String dataExtension, String metaCodec, String metaExtension) throws IOException {
     maxDoc = state.segmentInfo.getDocCount();
+    merging = false;
     String metaName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, metaExtension);
     // read in the entries from the metadata file.
     ChecksumIndexInput in = state.directory.openChecksumInput(metaName, state.context);
@@ -221,7 +248,9 @@
     NumericDocValues instance = numericInstances.get(field.name);
     if (instance == null) {
       instance = loadNumeric(field);
-      numericInstances.put(field.name, instance);
+      if (!merging) {
+        numericInstances.put(field.name, instance);
+      }
     }
     return instance;
   }
@@ -269,8 +298,10 @@
         final int formatID = data.readVInt();
         final int bitsPerValue = data.readVInt();
         final PackedInts.Reader ordsReader = PackedInts.getReaderNoHeader(data, PackedInts.Format.byId(formatID), entry.packedIntsVersion, maxDoc, bitsPerValue);
-        ramBytesUsed.addAndGet(RamUsageEstimator.sizeOf(decode) + ordsReader.ramBytesUsed());
-        numericInfo.put(field.name, ordsReader);
+        if (!merging) {
+          ramBytesUsed.addAndGet(RamUsageEstimator.sizeOf(decode) + ordsReader.ramBytesUsed());
+          numericInfo.put(field.name, ordsReader);
+        }
         return new NumericDocValues() {
           @Override
           public long get(int docID) {
@@ -280,14 +311,18 @@
       case DELTA_COMPRESSED:
         final int blockSize = data.readVInt();
         final BlockPackedReader reader = new BlockPackedReader(data, entry.packedIntsVersion, blockSize, maxDoc, false);
-        ramBytesUsed.addAndGet(reader.ramBytesUsed());
-        numericInfo.put(field.name, reader);
+        if (!merging) {
+          ramBytesUsed.addAndGet(reader.ramBytesUsed());
+          numericInfo.put(field.name, reader);
+        }
         return reader;
       case UNCOMPRESSED:
         final byte bytes[] = new byte[maxDoc];
         data.readBytes(bytes, 0, bytes.length);
-        ramBytesUsed.addAndGet(RamUsageEstimator.sizeOf(bytes));
-        numericInfo.put(field.name, Accountables.namedAccountable("byte array", maxDoc));
+        if (!merging) {
+          ramBytesUsed.addAndGet(RamUsageEstimator.sizeOf(bytes));
+          numericInfo.put(field.name, Accountables.namedAccountable("byte array", maxDoc));
+        }
         return new NumericDocValues() {
           @Override
           public long get(int docID) {
@@ -299,8 +334,10 @@
         final long mult = data.readLong();
         final int quotientBlockSize = data.readVInt();
         final BlockPackedReader quotientReader = new BlockPackedReader(data, entry.packedIntsVersion, quotientBlockSize, maxDoc, false);
-        ramBytesUsed.addAndGet(quotientReader.ramBytesUsed());
-        numericInfo.put(field.name, quotientReader);
+        if (!merging) {
+          ramBytesUsed.addAndGet(quotientReader.ramBytesUsed());
+          numericInfo.put(field.name, quotientReader);
+        }
         return new NumericDocValues() {
           @Override
           public long get(int docID) {
@@ -317,7 +354,9 @@
     BinaryDocValues instance = binaryInstances.get(field.name);
     if (instance == null) {
       instance = loadBinary(field);
-      binaryInstances.put(field.name, instance);
+      if (!merging) {
+        binaryInstances.put(field.name, instance);
+      }
     }
     return instance;
   }
@@ -328,10 +367,14 @@
     PagedBytes bytes = new PagedBytes(16);
     bytes.copy(data, entry.numBytes);
     final PagedBytes.Reader bytesReader = bytes.freeze(true);
-    binaryInfo.put(field.name, bytesReader);
+    if (!merging) {
+      binaryInfo.put(field.name, bytesReader);
+    }
     if (entry.minLength == entry.maxLength) {
       final int fixedLength = entry.minLength;
-      ramBytesUsed.addAndGet(bytesReader.ramBytesUsed());
+      if (!merging) {
+        ramBytesUsed.addAndGet(bytesReader.ramBytesUsed());
+      }
       return new BinaryDocValues() {
         @Override
         public BytesRef get(int docID) {
@@ -342,8 +385,10 @@
       };
     } else {
       final MonotonicBlockPackedReader addresses = MonotonicBlockPackedReader.of(data, entry.packedIntsVersion, entry.blockSize, maxDoc, false);
-      addressInfo.put(field.name, addresses);
-      ramBytesUsed.addAndGet(bytesReader.ramBytesUsed() + addresses.ramBytesUsed());
+      if (!merging) {
+        addressInfo.put(field.name, addresses);
+        ramBytesUsed.addAndGet(bytesReader.ramBytesUsed() + addresses.ramBytesUsed());
+      }
       return new BinaryDocValues() {
 
         @Override
@@ -367,8 +412,10 @@
       if (instance == null) {
         data.seek(entry.offset);
         instance = new FST<>(data, PositiveIntOutputs.getSingleton());
-        ramBytesUsed.addAndGet(instance.ramBytesUsed());
-        fstInstances.put(field.name, instance);
+        if (!merging) {
+          ramBytesUsed.addAndGet(instance.ramBytesUsed());
+          fstInstances.put(field.name, instance);
+        }
       }
     }
     final NumericDocValues docToOrd = getNumeric(field);
@@ -444,8 +491,10 @@
       if (instance == null) {
         data.seek(entry.offset);
         instance = new FST<>(data, PositiveIntOutputs.getSingleton());
-        ramBytesUsed.addAndGet(instance.ramBytesUsed());
-        fstInstances.put(field.name, instance);
+        if (!merging) {
+          ramBytesUsed.addAndGet(instance.ramBytesUsed());
+          fstInstances.put(field.name, instance);
+        }
       }
     }
     final BinaryDocValues docToOrds = getBinary(field);
@@ -537,6 +586,11 @@
   }
 
   @Override
+  public synchronized DocValuesProducer getMergeInstance() throws IOException {
+    return new Lucene42DocValuesProducer(this);
+  }
+
+  @Override
   public void close() throws IOException {
     data.close();
   }
Index: lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene42/Lucene42FieldInfosFormat.java
===================================================================
--- lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene42/Lucene42FieldInfosFormat.java	(revision 1629405)
+++ lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene42/Lucene42FieldInfosFormat.java	(working copy)
@@ -19,70 +19,12 @@
 
 import java.io.IOException;
 
-import org.apache.lucene.codecs.CodecUtil;
 import org.apache.lucene.codecs.FieldInfosFormat;
 import org.apache.lucene.codecs.FieldInfosReader;
 import org.apache.lucene.codecs.FieldInfosWriter;
-import org.apache.lucene.index.FieldInfo.DocValuesType; // javadoc
-import org.apache.lucene.store.DataOutput; // javadoc
 
 /**
  * Lucene 4.2 Field Infos format.
- * <p>
- * <p>Field names are stored in the field info file, with suffix <tt>.fnm</tt>.</p>
- * <p>FieldInfos (.fnm) --&gt; Header,FieldsCount, &lt;FieldName,FieldNumber,
- * FieldBits,DocValuesBits,Attributes&gt; <sup>FieldsCount</sup></p>
- * <p>Data types:
- * <ul>
- *   <li>Header --&gt; {@link CodecUtil#checkHeader CodecHeader}</li>
- *   <li>FieldsCount --&gt; {@link DataOutput#writeVInt VInt}</li>
- *   <li>FieldName --&gt; {@link DataOutput#writeString String}</li>
- *   <li>FieldBits, DocValuesBits --&gt; {@link DataOutput#writeByte Byte}</li>
- *   <li>FieldNumber --&gt; {@link DataOutput#writeInt VInt}</li>
- *   <li>Attributes --&gt; {@link DataOutput#writeStringStringMap Map&lt;String,String&gt;}</li>
- * </ul>
- * </p>
- * Field Descriptions:
- * <ul>
- *   <li>FieldsCount: the number of fields in this file.</li>
- *   <li>FieldName: name of the field as a UTF-8 String.</li>
- *   <li>FieldNumber: the field's number. Note that unlike previous versions of
- *       Lucene, the fields are not numbered implicitly by their order in the
- *       file, instead explicitly.</li>
- *   <li>FieldBits: a byte containing field options.
- *       <ul>
- *         <li>The low-order bit is one for indexed fields, and zero for non-indexed
- *             fields.</li>
- *         <li>The second lowest-order bit is one for fields that have term vectors
- *             stored, and zero for fields without term vectors.</li>
- *         <li>If the third lowest order-bit is set (0x4), offsets are stored into
- *             the postings list in addition to positions.</li>
- *         <li>Fourth bit is unused.</li>
- *         <li>If the fifth lowest-order bit is set (0x10), norms are omitted for the
- *             indexed field.</li>
- *         <li>If the sixth lowest-order bit is set (0x20), payloads are stored for the
- *             indexed field.</li>
- *         <li>If the seventh lowest-order bit is set (0x40), term frequencies and
- *             positions omitted for the indexed field.</li>
- *         <li>If the eighth lowest-order bit is set (0x80), positions are omitted for the
- *             indexed field.</li>
- *       </ul>
- *    </li>
- *    <li>DocValuesBits: a byte containing per-document value types. The type
- *        recorded as two four-bit integers, with the high-order bits representing
- *        <code>norms</code> options, and the low-order bits representing 
- *        {@code DocValues} options. Each four-bit integer can be decoded as such:
- *        <ul>
- *          <li>0: no DocValues for this field.</li>
- *          <li>1: NumericDocValues. ({@link DocValuesType#NUMERIC})</li>
- *          <li>2: BinaryDocValues. ({@code DocValuesType#BINARY})</li>
- *          <li>3: SortedDocValues. ({@code DocValuesType#SORTED})</li>
- *        </ul>
- *    </li>
- *    <li>Attributes: a key-value map of codec-private attributes.</li>
- * </ul>
- *
- * @lucene.experimental
  * @deprecated Only for reading old 4.2-4.5 segments
  */
 @Deprecated
@@ -94,7 +36,7 @@
   }
 
   @Override
-  public FieldInfosReader getFieldInfosReader() throws IOException {
+  public final FieldInfosReader getFieldInfosReader() throws IOException {
     return reader;
   }
 
Index: lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene42/Lucene42FieldInfosReader.java
===================================================================
--- lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene42/Lucene42FieldInfosReader.java	(revision 1629405)
+++ lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene42/Lucene42FieldInfosReader.java	(working copy)
@@ -38,9 +38,7 @@
 /**
  * Lucene 4.2 FieldInfos reader.
  * 
- * @lucene.experimental
  * @deprecated Only for reading old 4.2-4.5 segments
- * @see Lucene42FieldInfosFormat
  */
 @Deprecated
 final class Lucene42FieldInfosReader extends FieldInfosReader {
Index: lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene42/Lucene42NormsFormat.java
===================================================================
--- lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene42/Lucene42NormsFormat.java	(revision 1629405)
+++ lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene42/Lucene42NormsFormat.java	(working copy)
@@ -19,8 +19,6 @@
 
 import java.io.IOException;
 
-import org.apache.lucene.codecs.DocValuesConsumer;
-import org.apache.lucene.codecs.DocValuesProducer;
 import org.apache.lucene.codecs.NormsConsumer;
 import org.apache.lucene.codecs.NormsFormat;
 import org.apache.lucene.codecs.NormsProducer;
@@ -30,19 +28,9 @@
 
 /**
  * Lucene 4.2 score normalization format.
- * <p>
- * NOTE: this uses the same format as {@link Lucene42DocValuesFormat}
- * Numeric DocValues, but with different file extensions, and passing
- * {@link PackedInts#FASTEST} for uncompressed encoding: trading off
- * space for performance.
- * <p>
- * Files:
- * <ul>
- *   <li><tt>.nvd</tt>: DocValues data</li>
- *   <li><tt>.nvm</tt>: DocValues metadata</li>
- * </ul>
- * @see Lucene42DocValuesFormat
+ * @deprecated only for reading old 4.x segments
  */
+@Deprecated
 public class Lucene42NormsFormat extends NormsFormat {
   final float acceptableOverheadRatio;
 
@@ -73,7 +61,7 @@
   }
   
   @Override
-  public NormsProducer normsProducer(SegmentReadState state) throws IOException {
+  public final NormsProducer normsProducer(SegmentReadState state) throws IOException {
     return new Lucene42NormsProducer(state, DATA_CODEC, DATA_EXTENSION, METADATA_CODEC, METADATA_EXTENSION);
   }
   
Index: lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene42/Lucene42NormsProducer.java
===================================================================
--- lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene42/Lucene42NormsProducer.java	(revision 1629405)
+++ lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene42/Lucene42NormsProducer.java	(working copy)
@@ -18,8 +18,8 @@
  */
 
 import java.io.IOException;
-import java.util.Collections;
 
+import org.apache.lucene.codecs.DocValuesProducer;
 import org.apache.lucene.codecs.NormsProducer;
 import org.apache.lucene.index.FieldInfo;
 import org.apache.lucene.index.NumericDocValues;
@@ -28,13 +28,17 @@
 
 /**
  * Reads 4.2-4.8 norms.
- * Implemented the same as docvalues, but with a different filename.
  * @deprecated Only for reading old segments
  */
 @Deprecated
-class Lucene42NormsProducer extends NormsProducer {
-  private final Lucene42DocValuesProducer impl;
+final class Lucene42NormsProducer extends NormsProducer {
+  private final DocValuesProducer impl;
   
+  // clone for merge
+  Lucene42NormsProducer(DocValuesProducer impl) throws IOException {
+    this.impl = impl.getMergeInstance();
+  }
+  
   Lucene42NormsProducer(SegmentReadState state, String dataCodec, String dataExtension, String metaCodec, String metaExtension) throws IOException {
     impl = new Lucene42DocValuesProducer(state, dataCodec, dataExtension, metaCodec, metaExtension);
   }
@@ -65,6 +69,11 @@
   }
 
   @Override
+  public NormsProducer getMergeInstance() throws IOException {
+    return new Lucene42NormsProducer(impl);
+  }
+
+  @Override
   public String toString() {
     return getClass().getSimpleName() + "(" + impl + ")";
   }
Index: lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene42/Lucene42TermVectorsFormat.java
===================================================================
--- lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene42/Lucene42TermVectorsFormat.java	(revision 0)
+++ lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene42/Lucene42TermVectorsFormat.java	(working copy)
@@ -0,0 +1,57 @@
+package org.apache.lucene.codecs.lucene42;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.codecs.TermVectorsFormat;
+import org.apache.lucene.codecs.TermVectorsReader;
+import org.apache.lucene.codecs.TermVectorsWriter;
+import org.apache.lucene.codecs.compressing.CompressionMode;
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+
+/**
+ * Lucene 4.2 {@link TermVectorsFormat term vectors format}.
+ * @deprecated only for reading old segments
+ */
+@Deprecated
+public class Lucene42TermVectorsFormat extends TermVectorsFormat {
+  // this is actually what 4.2 TVF wrote!
+  static final String FORMAT_NAME = "Lucene41StoredFields";
+  static final String SEGMENT_SUFFIX = "";
+  static final CompressionMode COMPRESSION_MODE = CompressionMode.FAST;
+  static final int CHUNK_SIZE = 1 << 12;
+
+  @Override
+  public final TermVectorsReader vectorsReader(Directory directory, SegmentInfo segmentInfo, FieldInfos fieldInfos, IOContext context) throws IOException {
+    return new Lucene42TermVectorsReader(directory, segmentInfo, SEGMENT_SUFFIX, fieldInfos, context, FORMAT_NAME, COMPRESSION_MODE);
+  }
+
+  @Override
+  public TermVectorsWriter vectorsWriter(Directory directory, SegmentInfo segmentInfo, IOContext context) throws IOException {
+    throw new UnsupportedOperationException("this codec can only be used for reading");
+  }
+
+  @Override
+  public String toString() {
+    return getClass().getSimpleName() + "(compressionMode=" + COMPRESSION_MODE + ", chunkSize=" + CHUNK_SIZE + ")";
+  }
+}

Property changes on: lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene42/Lucene42TermVectorsFormat.java
___________________________________________________________________
Added: svn:eol-style
## -0,0 +1 ##
+native
\ No newline at end of property
Index: lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene42/Lucene42TermVectorsReader.java
===================================================================
--- lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene42/Lucene42TermVectorsReader.java	(revision 0)
+++ lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene42/Lucene42TermVectorsReader.java	(working copy)
@@ -0,0 +1,1073 @@
+package org.apache.lucene.codecs.lucene42;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.Closeable;
+import java.io.IOException;
+import java.util.Collections;
+import java.util.Iterator;
+import java.util.NoSuchElementException;
+
+import org.apache.lucene.codecs.CodecUtil;
+import org.apache.lucene.codecs.TermVectorsReader;
+import org.apache.lucene.codecs.compressing.CompressionMode;
+import org.apache.lucene.codecs.compressing.Decompressor;
+import org.apache.lucene.codecs.lucene41.Lucene41StoredFieldsIndexReader;
+import org.apache.lucene.index.CorruptIndexException;
+import org.apache.lucene.index.DocsAndPositionsEnum;
+import org.apache.lucene.index.DocsEnum;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.Fields;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.index.Terms;
+import org.apache.lucene.index.TermsEnum;
+import org.apache.lucene.store.AlreadyClosedException;
+import org.apache.lucene.store.ByteArrayDataInput;
+import org.apache.lucene.store.ChecksumIndexInput;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.util.Accountable;
+import org.apache.lucene.util.Accountables;
+import org.apache.lucene.util.ArrayUtil;
+import org.apache.lucene.util.Bits;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util.LongsRef;
+import org.apache.lucene.util.packed.BlockPackedReaderIterator;
+import org.apache.lucene.util.packed.PackedInts;
+
+/**
+ * 4.2 term vectors reader
+ * @deprecated only for reading old segments
+ */
+@Deprecated
+final class Lucene42TermVectorsReader extends TermVectorsReader implements Closeable {
+
+  private final FieldInfos fieldInfos;
+  final Lucene41StoredFieldsIndexReader indexReader;
+  final IndexInput vectorsStream;
+  private final int version;
+  private final int packedIntsVersion;
+  private final CompressionMode compressionMode;
+  private final Decompressor decompressor;
+  private final int chunkSize;
+  private final int numDocs;
+  private boolean closed;
+  private final BlockPackedReaderIterator reader;
+  
+  static final String VECTORS_EXTENSION = "tvd";
+  static final String VECTORS_INDEX_EXTENSION = "tvx";
+
+  static final String CODEC_SFX_IDX = "Index";
+  static final String CODEC_SFX_DAT = "Data";
+
+  static final int VERSION_START = 0;
+  static final int VERSION_CHECKSUM = 1;
+  static final int VERSION_CURRENT = VERSION_CHECKSUM;
+  
+  static final int BLOCK_SIZE = 64;
+
+  static final int POSITIONS = 0x01;
+  static final int   OFFSETS = 0x02;
+  static final int  PAYLOADS = 0x04;
+  static final int FLAGS_BITS = PackedInts.bitsRequired(POSITIONS | OFFSETS | PAYLOADS);
+
+  // used by clone
+  private Lucene42TermVectorsReader(Lucene42TermVectorsReader reader) {
+    this.fieldInfos = reader.fieldInfos;
+    this.vectorsStream = reader.vectorsStream.clone();
+    this.indexReader = reader.indexReader.clone();
+    this.packedIntsVersion = reader.packedIntsVersion;
+    this.compressionMode = reader.compressionMode;
+    this.decompressor = reader.decompressor.clone();
+    this.chunkSize = reader.chunkSize;
+    this.numDocs = reader.numDocs;
+    this.reader = new BlockPackedReaderIterator(vectorsStream, packedIntsVersion, BLOCK_SIZE, 0);
+    this.version = reader.version;
+    this.closed = false;
+  }
+
+  /** Sole constructor. */
+  public Lucene42TermVectorsReader(Directory d, SegmentInfo si, String segmentSuffix, FieldInfos fn,
+      IOContext context, String formatName, CompressionMode compressionMode) throws IOException {
+    this.compressionMode = compressionMode;
+    final String segment = si.name;
+    boolean success = false;
+    fieldInfos = fn;
+    numDocs = si.getDocCount();
+    ChecksumIndexInput indexStream = null;
+    try {
+      // Load the index into memory
+      final String indexStreamFN = IndexFileNames.segmentFileName(segment, segmentSuffix, VECTORS_INDEX_EXTENSION);
+      indexStream = d.openChecksumInput(indexStreamFN, context);
+      final String codecNameIdx = formatName + CODEC_SFX_IDX;
+      version = CodecUtil.checkHeader(indexStream, codecNameIdx, VERSION_START, VERSION_CURRENT);
+      assert CodecUtil.headerLength(codecNameIdx) == indexStream.getFilePointer();
+      indexReader = new Lucene41StoredFieldsIndexReader(indexStream, si);
+      
+      if (version >= VERSION_CHECKSUM) {
+        indexStream.readVLong(); // the end of the data file
+        CodecUtil.checkFooter(indexStream);
+      } else {
+        CodecUtil.checkEOF(indexStream);
+      }
+      indexStream.close();
+      indexStream = null;
+
+      // Open the data file and read metadata
+      final String vectorsStreamFN = IndexFileNames.segmentFileName(segment, segmentSuffix, VECTORS_EXTENSION);
+      vectorsStream = d.openInput(vectorsStreamFN, context);
+      final String codecNameDat = formatName + CODEC_SFX_DAT;
+      int version2 = CodecUtil.checkHeader(vectorsStream, codecNameDat, VERSION_START, VERSION_CURRENT);
+      if (version != version2) {
+        throw new CorruptIndexException("Version mismatch between stored fields index and data: " + version + " != " + version2, vectorsStream);
+      }
+      assert CodecUtil.headerLength(codecNameDat) == vectorsStream.getFilePointer();
+      
+      long pos = vectorsStream.getFilePointer();
+      if (version >= VERSION_CHECKSUM) {
+        // NOTE: data file is too costly to verify checksum against all the bytes on open,
+        // but for now we at least verify proper structure of the checksum footer: which looks
+        // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption
+        // such as file truncation.
+        CodecUtil.retrieveChecksum(vectorsStream);
+        vectorsStream.seek(pos);
+      }
+
+      packedIntsVersion = vectorsStream.readVInt();
+      chunkSize = vectorsStream.readVInt();
+      decompressor = compressionMode.newDecompressor();
+      this.reader = new BlockPackedReaderIterator(vectorsStream, packedIntsVersion, BLOCK_SIZE, 0);
+
+      success = true;
+    } finally {
+      if (!success) {
+        IOUtils.closeWhileHandlingException(this, indexStream);
+      }
+    }
+  }
+
+  /**
+   * @throws AlreadyClosedException if this TermVectorsReader is closed
+   */
+  private void ensureOpen() throws AlreadyClosedException {
+    if (closed) {
+      throw new AlreadyClosedException("this FieldsReader is closed");
+    }
+  }
+
+  @Override
+  public void close() throws IOException {
+    if (!closed) {
+      IOUtils.close(vectorsStream);
+      closed = true;
+    }
+  }
+
+  @Override
+  public TermVectorsReader clone() {
+    return new Lucene42TermVectorsReader(this);
+  }
+
+  @Override
+  public Fields get(int doc) throws IOException {
+    ensureOpen();
+
+    // seek to the right place
+    {
+      final long startPointer = indexReader.getStartPointer(doc);
+      vectorsStream.seek(startPointer);
+    }
+
+    // decode
+    // - docBase: first doc ID of the chunk
+    // - chunkDocs: number of docs of the chunk
+    final int docBase = vectorsStream.readVInt();
+    final int chunkDocs = vectorsStream.readVInt();
+    if (doc < docBase || doc >= docBase + chunkDocs || docBase + chunkDocs > numDocs) {
+      throw new CorruptIndexException("docBase=" + docBase + ",chunkDocs=" + chunkDocs + ",doc=" + doc, vectorsStream);
+    }
+
+    final int skip; // number of fields to skip
+    final int numFields; // number of fields of the document we're looking for
+    final int totalFields; // total number of fields of the chunk (sum for all docs)
+    if (chunkDocs == 1) {
+      skip = 0;
+      numFields = totalFields = vectorsStream.readVInt();
+    } else {
+      reader.reset(vectorsStream, chunkDocs);
+      int sum = 0;
+      for (int i = docBase; i < doc; ++i) {
+        sum += reader.next();
+      }
+      skip = sum;
+      numFields = (int) reader.next();
+      sum += numFields;
+      for (int i = doc + 1; i < docBase + chunkDocs; ++i) {
+        sum += reader.next();
+      }
+      totalFields = sum;
+    }
+
+    if (numFields == 0) {
+      // no vectors
+      return null;
+    }
+
+    // read field numbers that have term vectors
+    final int[] fieldNums;
+    {
+      final int token = vectorsStream.readByte() & 0xFF;
+      assert token != 0; // means no term vectors, cannot happen since we checked for numFields == 0
+      final int bitsPerFieldNum = token & 0x1F;
+      int totalDistinctFields = token >>> 5;
+      if (totalDistinctFields == 0x07) {
+        totalDistinctFields += vectorsStream.readVInt();
+      }
+      ++totalDistinctFields;
+      final PackedInts.ReaderIterator it = PackedInts.getReaderIteratorNoHeader(vectorsStream, PackedInts.Format.PACKED, packedIntsVersion, totalDistinctFields, bitsPerFieldNum, 1);
+      fieldNums = new int[totalDistinctFields];
+      for (int i = 0; i < totalDistinctFields; ++i) {
+        fieldNums[i] = (int) it.next();
+      }
+    }
+
+    // read field numbers and flags
+    final int[] fieldNumOffs = new int[numFields];
+    final PackedInts.Reader flags;
+    {
+      final int bitsPerOff = PackedInts.bitsRequired(fieldNums.length - 1);
+      final PackedInts.Reader allFieldNumOffs = PackedInts.getReaderNoHeader(vectorsStream, PackedInts.Format.PACKED, packedIntsVersion, totalFields, bitsPerOff);
+      switch (vectorsStream.readVInt()) {
+        case 0:
+          final PackedInts.Reader fieldFlags = PackedInts.getReaderNoHeader(vectorsStream, PackedInts.Format.PACKED, packedIntsVersion, fieldNums.length, FLAGS_BITS);
+          PackedInts.Mutable f = PackedInts.getMutable(totalFields, FLAGS_BITS, PackedInts.COMPACT);
+          for (int i = 0; i < totalFields; ++i) {
+            final int fieldNumOff = (int) allFieldNumOffs.get(i);
+            assert fieldNumOff >= 0 && fieldNumOff < fieldNums.length;
+            final int fgs = (int) fieldFlags.get(fieldNumOff);
+            f.set(i, fgs);
+          }
+          flags = f;
+          break;
+        case 1:
+          flags = PackedInts.getReaderNoHeader(vectorsStream, PackedInts.Format.PACKED, packedIntsVersion, totalFields, FLAGS_BITS);
+          break;
+        default:
+          throw new AssertionError();
+      }
+      for (int i = 0; i < numFields; ++i) {
+        fieldNumOffs[i] = (int) allFieldNumOffs.get(skip + i);
+      }
+    }
+
+    // number of terms per field for all fields
+    final PackedInts.Reader numTerms;
+    final int totalTerms;
+    {
+      final int bitsRequired = vectorsStream.readVInt();
+      numTerms = PackedInts.getReaderNoHeader(vectorsStream, PackedInts.Format.PACKED, packedIntsVersion, totalFields, bitsRequired);
+      int sum = 0;
+      for (int i = 0; i < totalFields; ++i) {
+        sum += numTerms.get(i);
+      }
+      totalTerms = sum;
+    }
+
+    // term lengths
+    int docOff = 0, docLen = 0, totalLen;
+    final int[] fieldLengths = new int[numFields];
+    final int[][] prefixLengths = new int[numFields][];
+    final int[][] suffixLengths = new int[numFields][];
+    {
+      reader.reset(vectorsStream, totalTerms);
+      // skip
+      int toSkip = 0;
+      for (int i = 0; i < skip; ++i) {
+        toSkip += numTerms.get(i);
+      }
+      reader.skip(toSkip);
+      // read prefix lengths
+      for (int i = 0; i < numFields; ++i) {
+        final int termCount = (int) numTerms.get(skip + i);
+        final int[] fieldPrefixLengths = new int[termCount];
+        prefixLengths[i] = fieldPrefixLengths;
+        for (int j = 0; j < termCount; ) {
+          final LongsRef next = reader.next(termCount - j);
+          for (int k = 0; k < next.length; ++k) {
+            fieldPrefixLengths[j++] = (int) next.longs[next.offset + k];
+          }
+        }
+      }
+      reader.skip(totalTerms - reader.ord());
+
+      reader.reset(vectorsStream, totalTerms);
+      // skip
+      toSkip = 0;
+      for (int i = 0; i < skip; ++i) {
+        for (int j = 0; j < numTerms.get(i); ++j) {
+          docOff += reader.next();
+        }
+      }
+      for (int i = 0; i < numFields; ++i) {
+        final int termCount = (int) numTerms.get(skip + i);
+        final int[] fieldSuffixLengths = new int[termCount];
+        suffixLengths[i] = fieldSuffixLengths;
+        for (int j = 0; j < termCount; ) {
+          final LongsRef next = reader.next(termCount - j);
+          for (int k = 0; k < next.length; ++k) {
+            fieldSuffixLengths[j++] = (int) next.longs[next.offset + k];
+          }
+        }
+        fieldLengths[i] = sum(suffixLengths[i]);
+        docLen += fieldLengths[i];
+      }
+      totalLen = docOff + docLen;
+      for (int i = skip + numFields; i < totalFields; ++i) {
+        for (int j = 0; j < numTerms.get(i); ++j) {
+          totalLen += reader.next();
+        }
+      }
+    }
+
+    // term freqs
+    final int[] termFreqs = new int[totalTerms];
+    {
+      reader.reset(vectorsStream, totalTerms);
+      for (int i = 0; i < totalTerms; ) {
+        final LongsRef next = reader.next(totalTerms - i);
+        for (int k = 0; k < next.length; ++k) {
+          termFreqs[i++] = 1 + (int) next.longs[next.offset + k];
+        }
+      }
+    }
+
+    // total number of positions, offsets and payloads
+    int totalPositions = 0, totalOffsets = 0, totalPayloads = 0;
+    for (int i = 0, termIndex = 0; i < totalFields; ++i) {
+      final int f = (int) flags.get(i);
+      final int termCount = (int) numTerms.get(i);
+      for (int j = 0; j < termCount; ++j) {
+        final int freq = termFreqs[termIndex++];
+        if ((f & POSITIONS) != 0) {
+          totalPositions += freq;
+        }
+        if ((f & OFFSETS) != 0) {
+          totalOffsets += freq;
+        }
+        if ((f & PAYLOADS) != 0) {
+          totalPayloads += freq;
+        }
+      }
+      assert i != totalFields - 1 || termIndex == totalTerms : termIndex + " " + totalTerms;
+    }
+
+    final int[][] positionIndex = positionIndex(skip, numFields, numTerms, termFreqs);
+    final int[][] positions, startOffsets, lengths;
+    if (totalPositions > 0) {
+      positions = readPositions(skip, numFields, flags, numTerms, termFreqs, POSITIONS, totalPositions, positionIndex);
+    } else {
+      positions = new int[numFields][];
+    }
+
+    if (totalOffsets > 0) {
+      // average number of chars per term
+      final float[] charsPerTerm = new float[fieldNums.length];
+      for (int i = 0; i < charsPerTerm.length; ++i) {
+        charsPerTerm[i] = Float.intBitsToFloat(vectorsStream.readInt());
+      }
+      startOffsets = readPositions(skip, numFields, flags, numTerms, termFreqs, OFFSETS, totalOffsets, positionIndex);
+      lengths = readPositions(skip, numFields, flags, numTerms, termFreqs, OFFSETS, totalOffsets, positionIndex);
+
+      for (int i = 0; i < numFields; ++i) {
+        final int[] fStartOffsets = startOffsets[i];
+        final int[] fPositions = positions[i];
+        // patch offsets from positions
+        if (fStartOffsets != null && fPositions != null) {
+          final float fieldCharsPerTerm = charsPerTerm[fieldNumOffs[i]];
+          for (int j = 0; j < startOffsets[i].length; ++j) {
+            fStartOffsets[j] += (int) (fieldCharsPerTerm * fPositions[j]);
+          }
+        }
+        if (fStartOffsets != null) {
+          final int[] fPrefixLengths = prefixLengths[i];
+          final int[] fSuffixLengths = suffixLengths[i];
+          final int[] fLengths = lengths[i];
+          for (int j = 0, end = (int) numTerms.get(skip + i); j < end; ++j) {
+            // delta-decode start offsets and  patch lengths using term lengths
+            final int termLength = fPrefixLengths[j] + fSuffixLengths[j];
+            lengths[i][positionIndex[i][j]] += termLength;
+            for (int k = positionIndex[i][j] + 1; k < positionIndex[i][j + 1]; ++k) {
+              fStartOffsets[k] += fStartOffsets[k - 1];
+              fLengths[k] += termLength;
+            }
+          }
+        }
+      }
+    } else {
+      startOffsets = lengths = new int[numFields][];
+    }
+    if (totalPositions > 0) {
+      // delta-decode positions
+      for (int i = 0; i < numFields; ++i) {
+        final int[] fPositions = positions[i];
+        final int[] fpositionIndex = positionIndex[i];
+        if (fPositions != null) {
+          for (int j = 0, end = (int) numTerms.get(skip + i); j < end; ++j) {
+            // delta-decode start offsets
+            for (int k = fpositionIndex[j] + 1; k < fpositionIndex[j + 1]; ++k) {
+              fPositions[k] += fPositions[k - 1];
+            }
+          }
+        }
+      }
+    }
+
+    // payload lengths
+    final int[][] payloadIndex = new int[numFields][];
+    int totalPayloadLength = 0;
+    int payloadOff = 0;
+    int payloadLen = 0;
+    if (totalPayloads > 0) {
+      reader.reset(vectorsStream, totalPayloads);
+      // skip
+      int termIndex = 0;
+      for (int i = 0; i < skip; ++i) {
+        final int f = (int) flags.get(i);
+        final int termCount = (int) numTerms.get(i);
+        if ((f & PAYLOADS) != 0) {
+          for (int j = 0; j < termCount; ++j) {
+            final int freq = termFreqs[termIndex + j];
+            for (int k = 0; k < freq; ++k) {
+              final int l = (int) reader.next();
+              payloadOff += l;
+            }
+          }
+        }
+        termIndex += termCount;
+      }
+      totalPayloadLength = payloadOff;
+      // read doc payload lengths
+      for (int i = 0; i < numFields; ++i) {
+        final int f = (int) flags.get(skip + i);
+        final int termCount = (int) numTerms.get(skip + i);
+        if ((f & PAYLOADS) != 0) {
+          final int totalFreq = positionIndex[i][termCount];
+          payloadIndex[i] = new int[totalFreq + 1];
+          int posIdx = 0;
+          payloadIndex[i][posIdx] = payloadLen;
+          for (int j = 0; j < termCount; ++j) {
+            final int freq = termFreqs[termIndex + j];
+            for (int k = 0; k < freq; ++k) {
+              final int payloadLength = (int) reader.next();
+              payloadLen += payloadLength;
+              payloadIndex[i][posIdx+1] = payloadLen;
+              ++posIdx;
+            }
+          }
+          assert posIdx == totalFreq;
+        }
+        termIndex += termCount;
+      }
+      totalPayloadLength += payloadLen;
+      for (int i = skip + numFields; i < totalFields; ++i) {
+        final int f = (int) flags.get(i);
+        final int termCount = (int) numTerms.get(i);
+        if ((f & PAYLOADS) != 0) {
+          for (int j = 0; j < termCount; ++j) {
+            final int freq = termFreqs[termIndex + j];
+            for (int k = 0; k < freq; ++k) {
+              totalPayloadLength += reader.next();
+            }
+          }
+        }
+        termIndex += termCount;
+      }
+      assert termIndex == totalTerms : termIndex + " " + totalTerms;
+    }
+
+    // decompress data
+    final BytesRef suffixBytes = new BytesRef();
+    decompressor.decompress(vectorsStream, totalLen + totalPayloadLength, docOff + payloadOff, docLen + payloadLen, suffixBytes);
+    suffixBytes.length = docLen;
+    final BytesRef payloadBytes = new BytesRef(suffixBytes.bytes, suffixBytes.offset + docLen, payloadLen);
+
+    final int[] fieldFlags = new int[numFields];
+    for (int i = 0; i < numFields; ++i) {
+      fieldFlags[i] = (int) flags.get(skip + i);
+    }
+
+    final int[] fieldNumTerms = new int[numFields];
+    for (int i = 0; i < numFields; ++i) {
+      fieldNumTerms[i] = (int) numTerms.get(skip + i);
+    }
+
+    final int[][] fieldTermFreqs = new int[numFields][];
+    {
+      int termIdx = 0;
+      for (int i = 0; i < skip; ++i) {
+        termIdx += numTerms.get(i);
+      }
+      for (int i = 0; i < numFields; ++i) {
+        final int termCount = (int) numTerms.get(skip + i);
+        fieldTermFreqs[i] = new int[termCount];
+        for (int j = 0; j < termCount; ++j) {
+          fieldTermFreqs[i][j] = termFreqs[termIdx++];
+        }
+      }
+    }
+
+    assert sum(fieldLengths) == docLen : sum(fieldLengths) + " != " + docLen;
+
+    return new TVFields(fieldNums, fieldFlags, fieldNumOffs, fieldNumTerms, fieldLengths,
+        prefixLengths, suffixLengths, fieldTermFreqs,
+        positionIndex, positions, startOffsets, lengths,
+        payloadBytes, payloadIndex,
+        suffixBytes);
+  }
+
+  // field -> term index -> position index
+  private int[][] positionIndex(int skip, int numFields, PackedInts.Reader numTerms, int[] termFreqs) {
+    final int[][] positionIndex = new int[numFields][];
+    int termIndex = 0;
+    for (int i = 0; i < skip; ++i) {
+      final int termCount = (int) numTerms.get(i);
+      termIndex += termCount;
+    }
+    for (int i = 0; i < numFields; ++i) {
+      final int termCount = (int) numTerms.get(skip + i);
+      positionIndex[i] = new int[termCount + 1];
+      for (int j = 0; j < termCount; ++j) {
+        final int freq = termFreqs[termIndex+j];
+        positionIndex[i][j + 1] = positionIndex[i][j] + freq;
+      }
+      termIndex += termCount;
+    }
+    return positionIndex;
+  }
+
+  private int[][] readPositions(int skip, int numFields, PackedInts.Reader flags, PackedInts.Reader numTerms, int[] termFreqs, int flag, final int totalPositions, int[][] positionIndex) throws IOException {
+    final int[][] positions = new int[numFields][];
+    reader.reset(vectorsStream, totalPositions);
+    // skip
+    int toSkip = 0;
+    int termIndex = 0;
+    for (int i = 0; i < skip; ++i) {
+      final int f = (int) flags.get(i);
+      final int termCount = (int) numTerms.get(i);
+      if ((f & flag) != 0) {
+        for (int j = 0; j < termCount; ++j) {
+          final int freq = termFreqs[termIndex+j];
+          toSkip += freq;
+        }
+      }
+      termIndex += termCount;
+    }
+    reader.skip(toSkip);
+    // read doc positions
+    for (int i = 0; i < numFields; ++i) {
+      final int f = (int) flags.get(skip + i);
+      final int termCount = (int) numTerms.get(skip + i);
+      if ((f & flag) != 0) {
+        final int totalFreq = positionIndex[i][termCount];
+        final int[] fieldPositions = new int[totalFreq];
+        positions[i] = fieldPositions;
+        for (int j = 0; j < totalFreq; ) {
+          final LongsRef nextPositions = reader.next(totalFreq - j);
+          for (int k = 0; k < nextPositions.length; ++k) {
+            fieldPositions[j++] = (int) nextPositions.longs[nextPositions.offset + k];
+          }
+        }
+      }
+      termIndex += termCount;
+    }
+    reader.skip(totalPositions - reader.ord());
+    return positions;
+  }
+
+  private class TVFields extends Fields {
+
+    private final int[] fieldNums, fieldFlags, fieldNumOffs, numTerms, fieldLengths;
+    private final int[][] prefixLengths, suffixLengths, termFreqs, positionIndex, positions, startOffsets, lengths, payloadIndex;
+    private final BytesRef suffixBytes, payloadBytes;
+
+    public TVFields(int[] fieldNums, int[] fieldFlags, int[] fieldNumOffs, int[] numTerms, int[] fieldLengths,
+        int[][] prefixLengths, int[][] suffixLengths, int[][] termFreqs,
+        int[][] positionIndex, int[][] positions, int[][] startOffsets, int[][] lengths,
+        BytesRef payloadBytes, int[][] payloadIndex,
+        BytesRef suffixBytes) {
+      this.fieldNums = fieldNums;
+      this.fieldFlags = fieldFlags;
+      this.fieldNumOffs = fieldNumOffs;
+      this.numTerms = numTerms;
+      this.fieldLengths = fieldLengths;
+      this.prefixLengths = prefixLengths;
+      this.suffixLengths = suffixLengths;
+      this.termFreqs = termFreqs;
+      this.positionIndex = positionIndex;
+      this.positions = positions;
+      this.startOffsets = startOffsets;
+      this.lengths = lengths;
+      this.payloadBytes = payloadBytes;
+      this.payloadIndex = payloadIndex;
+      this.suffixBytes = suffixBytes;
+    }
+
+    @Override
+    public Iterator<String> iterator() {
+      return new Iterator<String>() {
+        int i = 0;
+        @Override
+        public boolean hasNext() {
+          return i < fieldNumOffs.length;
+        }
+        @Override
+        public String next() {
+          if (!hasNext()) {
+            throw new NoSuchElementException();
+          }
+          final int fieldNum = fieldNums[fieldNumOffs[i++]];
+          return fieldInfos.fieldInfo(fieldNum).name;
+        }
+        @Override
+        public void remove() {
+          throw new UnsupportedOperationException();
+        }
+      };
+    }
+
+    @Override
+    public Terms terms(String field) throws IOException {
+      final FieldInfo fieldInfo = fieldInfos.fieldInfo(field);
+      if (fieldInfo == null) {
+        return null;
+      }
+      int idx = -1;
+      for (int i = 0; i < fieldNumOffs.length; ++i) {
+        if (fieldNums[fieldNumOffs[i]] == fieldInfo.number) {
+          idx = i;
+          break;
+        }
+      }
+
+      if (idx == -1 || numTerms[idx] == 0) {
+        // no term
+        return null;
+      }
+      int fieldOff = 0, fieldLen = -1;
+      for (int i = 0; i < fieldNumOffs.length; ++i) {
+        if (i < idx) {
+          fieldOff += fieldLengths[i];
+        } else {
+          fieldLen = fieldLengths[i];
+          break;
+        }
+      }
+      assert fieldLen >= 0;
+      return new TVTerms(numTerms[idx], fieldFlags[idx],
+          prefixLengths[idx], suffixLengths[idx], termFreqs[idx],
+          positionIndex[idx], positions[idx], startOffsets[idx], lengths[idx],
+          payloadIndex[idx], payloadBytes,
+          new BytesRef(suffixBytes.bytes, suffixBytes.offset + fieldOff, fieldLen));
+    }
+
+    @Override
+    public int size() {
+      return fieldNumOffs.length;
+    }
+
+  }
+
+  private class TVTerms extends Terms {
+
+    private final int numTerms, flags;
+    private final int[] prefixLengths, suffixLengths, termFreqs, positionIndex, positions, startOffsets, lengths, payloadIndex;
+    private final BytesRef termBytes, payloadBytes;
+
+    TVTerms(int numTerms, int flags, int[] prefixLengths, int[] suffixLengths, int[] termFreqs,
+        int[] positionIndex, int[] positions, int[] startOffsets, int[] lengths,
+        int[] payloadIndex, BytesRef payloadBytes,
+        BytesRef termBytes) {
+      this.numTerms = numTerms;
+      this.flags = flags;
+      this.prefixLengths = prefixLengths;
+      this.suffixLengths = suffixLengths;
+      this.termFreqs = termFreqs;
+      this.positionIndex = positionIndex;
+      this.positions = positions;
+      this.startOffsets = startOffsets;
+      this.lengths = lengths;
+      this.payloadIndex = payloadIndex;
+      this.payloadBytes = payloadBytes;
+      this.termBytes = termBytes;
+    }
+
+    @Override
+    public TermsEnum iterator(TermsEnum reuse) throws IOException {
+      final TVTermsEnum termsEnum;
+      if (reuse != null && reuse instanceof TVTermsEnum) {
+        termsEnum = (TVTermsEnum) reuse;
+      } else {
+        termsEnum = new TVTermsEnum();
+      }
+      termsEnum.reset(numTerms, flags, prefixLengths, suffixLengths, termFreqs, positionIndex, positions, startOffsets, lengths,
+          payloadIndex, payloadBytes,
+          new ByteArrayDataInput(termBytes.bytes, termBytes.offset, termBytes.length));
+      return termsEnum;
+    }
+
+    @Override
+    public long size() throws IOException {
+      return numTerms;
+    }
+
+    @Override
+    public long getSumTotalTermFreq() throws IOException {
+      return -1L;
+    }
+
+    @Override
+    public long getSumDocFreq() throws IOException {
+      return numTerms;
+    }
+
+    @Override
+    public int getDocCount() throws IOException {
+      return 1;
+    }
+
+    @Override
+    public boolean hasFreqs() {
+      return true;
+    }
+
+    @Override
+    public boolean hasOffsets() {
+      return (flags & OFFSETS) != 0;
+    }
+
+    @Override
+    public boolean hasPositions() {
+      return (flags & POSITIONS) != 0;
+    }
+
+    @Override
+    public boolean hasPayloads() {
+      return (flags & PAYLOADS) != 0;
+    }
+
+  }
+
+  private static class TVTermsEnum extends TermsEnum {
+
+    private int numTerms, startPos, ord;
+    private int[] prefixLengths, suffixLengths, termFreqs, positionIndex, positions, startOffsets, lengths, payloadIndex;
+    private ByteArrayDataInput in;
+    private BytesRef payloads;
+    private final BytesRef term;
+
+    private TVTermsEnum() {
+      term = new BytesRef(16);
+    }
+
+    void reset(int numTerms, int flags, int[] prefixLengths, int[] suffixLengths, int[] termFreqs, int[] positionIndex, int[] positions, int[] startOffsets, int[] lengths,
+        int[] payloadIndex, BytesRef payloads, ByteArrayDataInput in) {
+      this.numTerms = numTerms;
+      this.prefixLengths = prefixLengths;
+      this.suffixLengths = suffixLengths;
+      this.termFreqs = termFreqs;
+      this.positionIndex = positionIndex;
+      this.positions = positions;
+      this.startOffsets = startOffsets;
+      this.lengths = lengths;
+      this.payloadIndex = payloadIndex;
+      this.payloads = payloads;
+      this.in = in;
+      startPos = in.getPosition();
+      reset();
+    }
+
+    void reset() {
+      term.length = 0;
+      in.setPosition(startPos);
+      ord = -1;
+    }
+
+    @Override
+    public BytesRef next() throws IOException {
+      if (ord == numTerms - 1) {
+        return null;
+      } else {
+        assert ord < numTerms;
+        ++ord;
+      }
+
+      // read term
+      term.offset = 0;
+      term.length = prefixLengths[ord] + suffixLengths[ord];
+      if (term.length > term.bytes.length) {
+        term.bytes = ArrayUtil.grow(term.bytes, term.length);
+      }
+      in.readBytes(term.bytes, prefixLengths[ord], suffixLengths[ord]);
+
+      return term;
+    }
+
+    @Override
+    public SeekStatus seekCeil(BytesRef text)
+        throws IOException {
+      if (ord < numTerms && ord >= 0) {
+        final int cmp = term().compareTo(text);
+        if (cmp == 0) {
+          return SeekStatus.FOUND;
+        } else if (cmp > 0) {
+          reset();
+        }
+      }
+      // linear scan
+      while (true) {
+        final BytesRef term = next();
+        if (term == null) {
+          return SeekStatus.END;
+        }
+        final int cmp = term.compareTo(text);
+        if (cmp > 0) {
+          return SeekStatus.NOT_FOUND;
+        } else if (cmp == 0) {
+          return SeekStatus.FOUND;
+        }
+      }
+    }
+
+    @Override
+    public void seekExact(long ord) throws IOException {
+      throw new UnsupportedOperationException();
+    }
+
+    @Override
+    public BytesRef term() throws IOException {
+      return term;
+    }
+
+    @Override
+    public long ord() throws IOException {
+      throw new UnsupportedOperationException();
+    }
+
+    @Override
+    public int docFreq() throws IOException {
+      return 1;
+    }
+
+    @Override
+    public long totalTermFreq() throws IOException {
+      return termFreqs[ord];
+    }
+
+    @Override
+    public final DocsEnum docs(Bits liveDocs, DocsEnum reuse, int flags) throws IOException {
+      final TVDocsEnum docsEnum;
+      if (reuse != null && reuse instanceof TVDocsEnum) {
+        docsEnum = (TVDocsEnum) reuse;
+      } else {
+        docsEnum = new TVDocsEnum();
+      }
+
+      docsEnum.reset(liveDocs, termFreqs[ord], positionIndex[ord], positions, startOffsets, lengths, payloads, payloadIndex);
+      return docsEnum;
+    }
+
+    @Override
+    public DocsAndPositionsEnum docsAndPositions(Bits liveDocs, DocsAndPositionsEnum reuse, int flags) throws IOException {
+      if (positions == null && startOffsets == null) {
+        return null;
+      }
+      // TODO: slightly sheisty
+      return (DocsAndPositionsEnum) docs(liveDocs, reuse, flags);
+    }
+
+  }
+
+  private static class TVDocsEnum extends DocsAndPositionsEnum {
+
+    private Bits liveDocs;
+    private int doc = -1;
+    private int termFreq;
+    private int positionIndex;
+    private int[] positions;
+    private int[] startOffsets;
+    private int[] lengths;
+    private final BytesRef payload;
+    private int[] payloadIndex;
+    private int basePayloadOffset;
+    private int i;
+
+    TVDocsEnum() {
+      payload = new BytesRef();
+    }
+
+    public void reset(Bits liveDocs, int freq, int positionIndex, int[] positions,
+        int[] startOffsets, int[] lengths, BytesRef payloads,
+        int[] payloadIndex) {
+      this.liveDocs = liveDocs;
+      this.termFreq = freq;
+      this.positionIndex = positionIndex;
+      this.positions = positions;
+      this.startOffsets = startOffsets;
+      this.lengths = lengths;
+      this.basePayloadOffset = payloads.offset;
+      this.payload.bytes = payloads.bytes;
+      payload.offset = payload.length = 0;
+      this.payloadIndex = payloadIndex;
+
+      doc = i = -1;
+    }
+
+    private void checkDoc() {
+      if (doc == NO_MORE_DOCS) {
+        throw new IllegalStateException("DocsEnum exhausted");
+      } else if (doc == -1) {
+        throw new IllegalStateException("DocsEnum not started");
+      }
+    }
+
+    private void checkPosition() {
+      checkDoc();
+      if (i < 0) {
+        throw new IllegalStateException("Position enum not started");
+      } else if (i >= termFreq) {
+        throw new IllegalStateException("Read past last position");
+      }
+    }
+
+    @Override
+    public int nextPosition() throws IOException {
+      if (doc != 0) {
+        throw new IllegalStateException();
+      } else if (i >= termFreq - 1) {
+        throw new IllegalStateException("Read past last position");
+      }
+
+      ++i;
+
+      if (payloadIndex != null) {
+        payload.offset = basePayloadOffset + payloadIndex[positionIndex + i];
+        payload.length = payloadIndex[positionIndex + i + 1] - payloadIndex[positionIndex + i];
+      }
+
+      if (positions == null) {
+        return -1;
+      } else {
+        return positions[positionIndex + i];
+      }
+    }
+
+    @Override
+    public int startOffset() throws IOException {
+      checkPosition();
+      if (startOffsets == null) {
+        return -1;
+      } else {
+        return startOffsets[positionIndex + i];
+      }
+    }
+
+    @Override
+    public int endOffset() throws IOException {
+      checkPosition();
+      if (startOffsets == null) {
+        return -1;
+      } else {
+        return startOffsets[positionIndex + i] + lengths[positionIndex + i];
+      }
+    }
+
+    @Override
+    public BytesRef getPayload() throws IOException {
+      checkPosition();
+      if (payloadIndex == null || payload.length == 0) {
+        return null;
+      } else {
+        return payload;
+      }
+    }
+
+    @Override
+    public int freq() throws IOException {
+      checkDoc();
+      return termFreq;
+    }
+
+    @Override
+    public int docID() {
+      return doc;
+    }
+
+    @Override
+    public int nextDoc() throws IOException {
+      if (doc == -1 && (liveDocs == null || liveDocs.get(0))) {
+        return (doc = 0);
+      } else {
+        return (doc = NO_MORE_DOCS);
+      }
+    }
+
+    @Override
+    public int advance(int target) throws IOException {
+      return slowAdvance(target);
+    }
+
+    @Override
+    public long cost() {
+      return 1;
+    }
+  }
+
+  private static int sum(int[] arr) {
+    int sum = 0;
+    for (int el : arr) {
+      sum += el;
+    }
+    return sum;
+  }
+
+  @Override
+  public long ramBytesUsed() {
+    return indexReader.ramBytesUsed();
+  }
+  
+  @Override
+  public Iterable<? extends Accountable> getChildResources() {
+    return Collections.singleton(Accountables.namedAccountable("term vector index", indexReader));
+  }
+  
+  @Override
+  public void checkIntegrity() throws IOException {
+    if (version >= VERSION_CHECKSUM) {
+      CodecUtil.checksumEntireFile(vectorsStream);
+    }
+  }
+
+  @Override
+  public String toString() {
+    return getClass().getSimpleName() + "(mode=" + compressionMode + ",chunksize=" + chunkSize + ")";
+  }
+}

Property changes on: lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene42/Lucene42TermVectorsReader.java
___________________________________________________________________
Added: svn:eol-style
## -0,0 +1 ##
+native
\ No newline at end of property
Index: lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene45/Lucene45Codec.java
===================================================================
--- lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene45/Lucene45Codec.java	(revision 1629405)
+++ lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene45/Lucene45Codec.java	(working copy)
@@ -20,9 +20,9 @@
 import java.io.IOException;
 
 import org.apache.lucene.codecs.Codec;
+import org.apache.lucene.codecs.CompoundFormat;
 import org.apache.lucene.codecs.DocValuesFormat;
 import org.apache.lucene.codecs.FieldInfosFormat;
-import org.apache.lucene.codecs.FilterCodec;
 import org.apache.lucene.codecs.LiveDocsFormat;
 import org.apache.lucene.codecs.NormsConsumer;
 import org.apache.lucene.codecs.NormsFormat;
@@ -30,6 +30,7 @@
 import org.apache.lucene.codecs.SegmentInfoFormat;
 import org.apache.lucene.codecs.StoredFieldsFormat;
 import org.apache.lucene.codecs.TermVectorsFormat;
+import org.apache.lucene.codecs.lucene40.Lucene40CompoundFormat;
 import org.apache.lucene.codecs.lucene40.Lucene40LiveDocsFormat;
 import org.apache.lucene.codecs.lucene40.Lucene40SegmentInfoFormat;
 import org.apache.lucene.codecs.lucene41.Lucene41StoredFieldsFormat;
@@ -41,19 +42,9 @@
 import org.apache.lucene.index.SegmentWriteState;
 
 /**
- * Implements the Lucene 4.5 index format, with configurable per-field postings
- * and docvalues formats.
- * <p>
- * If you want to reuse functionality of this codec in another codec, extend
- * {@link FilterCodec}.
- *
- * @see org.apache.lucene.codecs.lucene45 package documentation for file format details.
- * @lucene.experimental
+ * Implements the Lucene 4.5 index format
  * @deprecated Only for reading old 4.3-4.5 segments
  */
-// NOTE: if we make largish changes in a minor release, easier to just make Lucene46Codec or whatever
-// if they are backwards compatible or smallish we can probably do the backwards in the postingsreader
-// (it writes a minor version, etc).
 @Deprecated
 public class Lucene45Codec extends Codec {
   private final StoredFieldsFormat fieldsFormat = new Lucene41StoredFieldsFormat();
@@ -61,6 +52,7 @@
   private final FieldInfosFormat fieldInfosFormat = new Lucene42FieldInfosFormat();
   private final SegmentInfoFormat infosFormat = new Lucene40SegmentInfoFormat();
   private final LiveDocsFormat liveDocsFormat = new Lucene40LiveDocsFormat();
+  private final CompoundFormat compoundFormat = new Lucene40CompoundFormat();
   
   private final PostingsFormat postingsFormat = new PerFieldPostingsFormat() {
     @Override
@@ -83,12 +75,12 @@
   }
   
   @Override
-  public final StoredFieldsFormat storedFieldsFormat() {
+  public StoredFieldsFormat storedFieldsFormat() {
     return fieldsFormat;
   }
   
   @Override
-  public final TermVectorsFormat termVectorsFormat() {
+  public TermVectorsFormat termVectorsFormat() {
     return vectorsFormat;
   }
 
@@ -111,6 +103,11 @@
   public final LiveDocsFormat liveDocsFormat() {
     return liveDocsFormat;
   }
+  
+  @Override
+  public CompoundFormat compoundFormat() {
+    return compoundFormat;
+  }
 
   /** Returns the postings format that should be used for writing 
    *  new segments of <code>field</code>.
Index: lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene45/Lucene45DocValuesConsumer.java
===================================================================
--- lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene45/Lucene45DocValuesConsumer.java	(revision 1629405)
+++ lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene45/Lucene45DocValuesConsumer.java	(working copy)
@@ -39,7 +39,11 @@
 import org.apache.lucene.util.packed.MonotonicBlockPackedWriter;
 import org.apache.lucene.util.packed.PackedInts;
 
-/** writer for {@link Lucene45DocValuesFormat} */
+/** 
+ * writer for 4.5 docvalues format
+ * @deprecated only for old 4.x segments
+ */
+@Deprecated
 class Lucene45DocValuesConsumer extends DocValuesConsumer implements Closeable {
 
   static final int BLOCK_SIZE = 16384;
Index: lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene45/Lucene45DocValuesFormat.java
===================================================================
--- lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene45/Lucene45DocValuesFormat.java	(revision 1629405)
+++ lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene45/Lucene45DocValuesFormat.java	(working copy)
@@ -19,148 +19,15 @@
 
 import java.io.IOException;
 
-import org.apache.lucene.codecs.CodecUtil;
 import org.apache.lucene.codecs.DocValuesConsumer;
 import org.apache.lucene.codecs.DocValuesProducer;
 import org.apache.lucene.codecs.DocValuesFormat;
 import org.apache.lucene.index.SegmentReadState;
 import org.apache.lucene.index.SegmentWriteState;
-import org.apache.lucene.index.FieldInfo.DocValuesType;
-import org.apache.lucene.store.DataOutput;
-import org.apache.lucene.util.SmallFloat;
-import org.apache.lucene.util.fst.FST;
-import org.apache.lucene.util.packed.BlockPackedWriter;
-import org.apache.lucene.util.packed.MonotonicBlockPackedWriter;
-import org.apache.lucene.util.packed.PackedInts;
 
 /**
  * Lucene 4.5 DocValues format.
- * <p>
- * Encodes the four per-document value types (Numeric,Binary,Sorted,SortedSet) with these strategies:
- * <p>
- * {@link DocValuesType#NUMERIC NUMERIC}:
- * <ul>
- *    <li>Delta-compressed: per-document integers written in blocks of 16k. For each block
- *        the minimum value in that block is encoded, and each entry is a delta from that 
- *        minimum value. Each block of deltas is compressed with bitpacking. For more 
- *        information, see {@link BlockPackedWriter}.
- *    <li>Table-compressed: when the number of unique values is very small (&lt; 256), and
- *        when there are unused "gaps" in the range of values used (such as {@link SmallFloat}), 
- *        a lookup table is written instead. Each per-document entry is instead the ordinal 
- *        to this table, and those ordinals are compressed with bitpacking ({@link PackedInts}). 
- *    <li>GCD-compressed: when all numbers share a common divisor, such as dates, the greatest
- *        common denominator (GCD) is computed, and quotients are stored using Delta-compressed Numerics.
- * </ul>
- * <p>
- * {@link DocValuesType#BINARY BINARY}:
- * <ul>
- *    <li>Fixed-width Binary: one large concatenated byte[] is written, along with the fixed length.
- *        Each document's value can be addressed directly with multiplication ({@code docID * length}). 
- *    <li>Variable-width Binary: one large concatenated byte[] is written, along with end addresses 
- *        for each document. The addresses are written in blocks of 16k, with the current absolute
- *        start for the block, and the average (expected) delta per entry. For each document the 
- *        deviation from the delta (actual - expected) is written.
- *    <li>Prefix-compressed Binary: values are written in chunks of 16, with the first value written
- *        completely and other values sharing prefixes. chunk addresses are written in blocks of 16k,
- *        with the current absolute start for the block, and the average (expected) delta per entry. 
- *        For each chunk the deviation from the delta (actual - expected) is written.
- * </ul>
- * <p>
- * {@link DocValuesType#SORTED SORTED}:
- * <ul>
- *    <li>Sorted: a mapping of ordinals to deduplicated terms is written as Prefix-Compressed Binary, 
- *        along with the per-document ordinals written using one of the numeric strategies above.
- * </ul>
- * <p>
- * {@link DocValuesType#SORTED_SET SORTED_SET}:
- * <ul>
- *    <li>SortedSet: a mapping of ordinals to deduplicated terms is written as Prefix-Compressed Binary, 
- *        an ordinal list and per-document index into this list are written using the numeric strategies 
- *        above. 
- * </ul>
- * <p>
- * Files:
- * <ol>
- *   <li><tt>.dvd</tt>: DocValues data</li>
- *   <li><tt>.dvm</tt>: DocValues metadata</li>
- * </ol>
- * <ol>
- *   <li><a name="dvm" id="dvm"></a>
- *   <p>The DocValues metadata or .dvm file.</p>
- *   <p>For DocValues field, this stores metadata, such as the offset into the 
- *      DocValues data (.dvd)</p>
- *   <p>DocValues metadata (.dvm) --&gt; Header,&lt;Entry&gt;<sup>NumFields</sup>,Footer</p>
- *   <ul>
- *     <li>Entry --&gt; NumericEntry | BinaryEntry | SortedEntry | SortedSetEntry</li>
- *     <li>NumericEntry --&gt; GCDNumericEntry | TableNumericEntry | DeltaNumericEntry</li>
- *     <li>GCDNumericEntry --&gt; NumericHeader,MinValue,GCD</li>
- *     <li>TableNumericEntry --&gt; NumericHeader,TableSize,{@link DataOutput#writeLong Int64}<sup>TableSize</sup></li>
- *     <li>DeltaNumericEntry --&gt; NumericHeader</li>
- *     <li>NumericHeader --&gt; FieldNumber,EntryType,NumericType,MissingOffset,PackedVersion,DataOffset,Count,BlockSize</li>
- *     <li>BinaryEntry --&gt; FixedBinaryEntry | VariableBinaryEntry | PrefixBinaryEntry</li>
- *     <li>FixedBinaryEntry --&gt; BinaryHeader</li>
- *     <li>VariableBinaryEntry --&gt; BinaryHeader,AddressOffset,PackedVersion,BlockSize</li>
- *     <li>PrefixBinaryEntry --&gt; BinaryHeader,AddressInterval,AddressOffset,PackedVersion,BlockSize</li>
- *     <li>BinaryHeader --&gt; FieldNumber,EntryType,BinaryType,MissingOffset,MinLength,MaxLength,DataOffset</li>
- *     <li>SortedEntry --&gt; FieldNumber,EntryType,BinaryEntry,NumericEntry</li>
- *     <li>SortedSetEntry --&gt; EntryType,BinaryEntry,NumericEntry,NumericEntry</li>
- *     <li>FieldNumber,PackedVersion,MinLength,MaxLength,BlockSize,ValueCount --&gt; {@link DataOutput#writeVInt VInt}</li>
- *     <li>EntryType,CompressionType --&gt; {@link DataOutput#writeByte Byte}</li>
- *     <li>Header --&gt; {@link CodecUtil#writeHeader CodecHeader}</li>
- *     <li>MinValue,GCD,MissingOffset,AddressOffset,DataOffset --&gt; {@link DataOutput#writeLong Int64}</li>
- *     <li>TableSize --&gt; {@link DataOutput#writeVInt vInt}</li>
- *     <li>Footer --&gt; {@link CodecUtil#writeFooter CodecFooter}</li>
- *   </ul>
- *   <p>Sorted fields have two entries: a BinaryEntry with the value metadata,
- *      and an ordinary NumericEntry for the document-to-ord metadata.</p>
- *   <p>SortedSet fields have three entries: a BinaryEntry with the value metadata,
- *      and two NumericEntries for the document-to-ord-index and ordinal list metadata.</p>
- *   <p>FieldNumber of -1 indicates the end of metadata.</p>
- *   <p>EntryType is a 0 (NumericEntry) or 1 (BinaryEntry)</p>
- *   <p>DataOffset is the pointer to the start of the data in the DocValues data (.dvd)</p>
- *   <p>NumericType indicates how Numeric values will be compressed:
- *      <ul>
- *         <li>0 --&gt; delta-compressed. For each block of 16k integers, every integer is delta-encoded
- *             from the minimum value within the block. 
- *         <li>1 --&gt, gcd-compressed. When all integers share a common divisor, only quotients are stored
- *             using blocks of delta-encoded ints.
- *         <li>2 --&gt; table-compressed. When the number of unique numeric values is small and it would save space,
- *             a lookup table of unique values is written, followed by the ordinal for each document.
- *      </ul>
- *   <p>BinaryType indicates how Binary values will be stored:
- *      <ul>
- *         <li>0 --&gt; fixed-width. All values have the same length, addressing by multiplication. 
- *         <li>1 --&gt, variable-width. An address for each value is stored.
- *         <li>2 --&gt; prefix-compressed. An address to the start of every interval'th value is stored.
- *      </ul>
- *   <p>MinLength and MaxLength represent the min and max byte[] value lengths for Binary values.
- *      If they are equal, then all values are of a fixed size, and can be addressed as DataOffset + (docID * length).
- *      Otherwise, the binary values are of variable size, and packed integer metadata (PackedVersion,BlockSize)
- *      is written for the addresses.
- *   <p>MissingOffset points to a byte[] containing a bitset of all documents that had a value for the field.
- *      If its -1, then there are no missing values.
- *   <p>Checksum contains the CRC32 checksum of all bytes in the .dvm file up
- *      until the checksum. This is used to verify integrity of the file on opening the
- *      index.
- *   <li><a name="dvd" id="dvd"></a>
- *   <p>The DocValues data or .dvd file.</p>
- *   <p>For DocValues field, this stores the actual per-document data (the heavy-lifting)</p>
- *   <p>DocValues data (.dvd) --&gt; Header,&lt;NumericData | BinaryData | SortedData&gt;<sup>NumFields</sup>,Footer</p>
- *   <ul>
- *     <li>NumericData --&gt; DeltaCompressedNumerics | TableCompressedNumerics | GCDCompressedNumerics</li>
- *     <li>BinaryData --&gt;  {@link DataOutput#writeByte Byte}<sup>DataLength</sup>,Addresses</li>
- *     <li>SortedData --&gt; {@link FST FST&lt;Int64&gt;}</li>
- *     <li>DeltaCompressedNumerics --&gt; {@link BlockPackedWriter BlockPackedInts(blockSize=16k)}</li>
- *     <li>TableCompressedNumerics --&gt; {@link PackedInts PackedInts}</li>
- *     <li>GCDCompressedNumerics --&gt; {@link BlockPackedWriter BlockPackedInts(blockSize=16k)}</li>
- *     <li>Addresses --&gt; {@link MonotonicBlockPackedWriter MonotonicBlockPackedInts(blockSize=16k)}</li>
- *     <li>Footer --&gt; {@link CodecUtil#writeFooter CodecFooter}</li>
- *   </ul>
- *   <p>SortedSet entries store the list of ordinals in their BinaryData as a
- *      sequences of increasing {@link DataOutput#writeVLong vLong}s, delta-encoded.</p>
- * </ol>
  * @deprecated Only for reading old 4.3-4.5 segments
- * @lucene.experimental
  */
 @Deprecated
 public class Lucene45DocValuesFormat extends DocValuesFormat {
@@ -177,7 +44,7 @@
   }
 
   @Override
-  public DocValuesProducer fieldsProducer(SegmentReadState state) throws IOException {
+  public final DocValuesProducer fieldsProducer(SegmentReadState state) throws IOException {
     return new Lucene45DocValuesProducer(state, DATA_CODEC, DATA_EXTENSION, META_CODEC, META_EXTENSION);
   }
   
Index: lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene45/Lucene45DocValuesProducer.java
===================================================================
--- lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene45/Lucene45DocValuesProducer.java	(revision 1629405)
+++ lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene45/Lucene45DocValuesProducer.java	(working copy)
@@ -63,13 +63,16 @@
 import org.apache.lucene.util.IOUtils;
 import org.apache.lucene.util.LongValues;
 import org.apache.lucene.util.RamUsageEstimator;
-import org.apache.lucene.util.StringHelper;
 import org.apache.lucene.util.Version;
 import org.apache.lucene.util.packed.BlockPackedReader;
 import org.apache.lucene.util.packed.MonotonicBlockPackedReader;
 import org.apache.lucene.util.packed.PackedInts;
 
-/** reader for {@link Lucene45DocValuesFormat} */
+/** 
+ * reader for 4.5 docvalues format
+ * @deprecated only for reading old 4.x segments
+ */
+@Deprecated
 class Lucene45DocValuesProducer extends DocValuesProducer implements Closeable {
   private final Map<Integer,NumericEntry> numerics;
   private final Map<Integer,BinaryEntry> binaries;
@@ -94,6 +97,27 @@
   private final Map<Integer,MonotonicBlockPackedReader> addressInstances = new HashMap<>();
   private final Map<Integer,MonotonicBlockPackedReader> ordIndexInstances = new HashMap<>();
   
+  private final boolean merging;
+  
+  // clone for merge: when merging we don't do any instances.put()s
+  Lucene45DocValuesProducer(Lucene45DocValuesProducer original) throws IOException {
+    assert Thread.holdsLock(original);
+    numerics = original.numerics;
+    binaries = original.binaries;
+    sortedSets = original.sortedSets;
+    ords = original.ords;
+    ordIndexes = original.ordIndexes;
+    ramBytesUsed = new AtomicLong(original.ramBytesUsed.get());
+    data = original.data.clone();
+    maxDoc = original.maxDoc;
+    version = original.version;
+    numFields = original.numFields;
+    lenientFieldInfoCheck = original.lenientFieldInfoCheck;
+    addressInstances.putAll(original.addressInstances);
+    ordIndexInstances.putAll(original.ordIndexInstances);
+    merging = true;
+  }
+  
   /** expert: instantiates a new reader */
   @SuppressWarnings("deprecation")
   protected Lucene45DocValuesProducer(SegmentReadState state, String dataCodec, String dataExtension, String metaCodec, String metaExtension) throws IOException {
@@ -103,6 +127,7 @@
     // read in the entries from the metadata file.
     ChecksumIndexInput in = state.directory.openChecksumInput(metaName, state.context);
     this.maxDoc = state.segmentInfo.getDocCount();
+    merging = false;
     boolean success = false;
     try {
       version = CodecUtil.checkHeader(in, metaCodec, 
@@ -439,8 +464,10 @@
     if (addrInstance == null) {
       data.seek(bytes.addressesOffset);
       addrInstance = MonotonicBlockPackedReader.of(data, bytes.packedIntsVersion, bytes.blockSize, bytes.count, false);
-      addressInstances.put(field.number, addrInstance);
-      ramBytesUsed.addAndGet(addrInstance.ramBytesUsed() + RamUsageEstimator.NUM_BYTES_INT);
+      if (!merging) {
+        addressInstances.put(field.number, addrInstance);
+        ramBytesUsed.addAndGet(addrInstance.ramBytesUsed() + RamUsageEstimator.NUM_BYTES_INT);
+      }
     }
     addresses = addrInstance;
     return addresses;
@@ -486,8 +513,10 @@
         size = 1L + bytes.count / interval;
       }
       addrInstance = MonotonicBlockPackedReader.of(data, bytes.packedIntsVersion, bytes.blockSize, size, false);
-      addressInstances.put(field.number, addrInstance);
-      ramBytesUsed.addAndGet(addrInstance.ramBytesUsed() + RamUsageEstimator.NUM_BYTES_INT);
+      if (!merging) {
+        addressInstances.put(field.number, addrInstance);
+        ramBytesUsed.addAndGet(addrInstance.ramBytesUsed() + RamUsageEstimator.NUM_BYTES_INT);
+      }
     }
     addresses = addrInstance;
     return addresses;
@@ -556,8 +585,10 @@
     if (ordIndexInstance == null) {
       data.seek(entry.offset);
       ordIndexInstance = MonotonicBlockPackedReader.of(data, entry.packedIntsVersion, entry.blockSize, entry.count, false);
-      ordIndexInstances.put(field.number, ordIndexInstance);
-      ramBytesUsed.addAndGet(ordIndexInstance.ramBytesUsed() + RamUsageEstimator.NUM_BYTES_INT);
+      if (!merging) {
+        ordIndexInstances.put(field.number, ordIndexInstance);
+        ramBytesUsed.addAndGet(ordIndexInstance.ramBytesUsed() + RamUsageEstimator.NUM_BYTES_INT);
+      }
     }
     ordIndex = ordIndexInstance;
     return ordIndex;
@@ -692,6 +723,11 @@
   }
 
   @Override
+  public synchronized DocValuesProducer getMergeInstance() throws IOException {
+    return new Lucene45DocValuesProducer(this);
+  }
+
+  @Override
   public void close() throws IOException {
     data.close();
   }
Index: lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene46/Lucene46Codec.java
===================================================================
--- lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene46/Lucene46Codec.java	(revision 1629405)
+++ lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene46/Lucene46Codec.java	(working copy)
@@ -20,9 +20,9 @@
 import java.io.IOException;
 
 import org.apache.lucene.codecs.Codec;
+import org.apache.lucene.codecs.CompoundFormat;
 import org.apache.lucene.codecs.DocValuesFormat;
 import org.apache.lucene.codecs.FieldInfosFormat;
-import org.apache.lucene.codecs.FilterCodec;
 import org.apache.lucene.codecs.LiveDocsFormat;
 import org.apache.lucene.codecs.NormsConsumer;
 import org.apache.lucene.codecs.NormsFormat;
@@ -30,6 +30,7 @@
 import org.apache.lucene.codecs.SegmentInfoFormat;
 import org.apache.lucene.codecs.StoredFieldsFormat;
 import org.apache.lucene.codecs.TermVectorsFormat;
+import org.apache.lucene.codecs.lucene40.Lucene40CompoundFormat;
 import org.apache.lucene.codecs.lucene40.Lucene40LiveDocsFormat;
 import org.apache.lucene.codecs.lucene41.Lucene41StoredFieldsFormat;
 import org.apache.lucene.codecs.lucene42.Lucene42NormsFormat;
@@ -39,19 +40,9 @@
 import org.apache.lucene.index.SegmentWriteState;
 
 /**
- * Implements the Lucene 4.6 index format, with configurable per-field postings
- * and docvalues formats.
- * <p>
- * If you want to reuse functionality of this codec in another codec, extend
- * {@link FilterCodec}.
- *
- * @see org.apache.lucene.codecs.lucene46 package documentation for file format details.
- * @lucene.experimental
+ * Implements the Lucene 4.6 index format
  * @deprecated Only for reading old 4.6-4.8 segments
  */
-// NOTE: if we make largish changes in a minor release, easier to just make Lucene46Codec or whatever
-// if they are backwards compatible or smallish we can probably do the backwards in the postingsreader
-// (it writes a minor version, etc).
 @Deprecated
 public class Lucene46Codec extends Codec {
   private final StoredFieldsFormat fieldsFormat = new Lucene41StoredFieldsFormat();
@@ -59,6 +50,7 @@
   private final FieldInfosFormat fieldInfosFormat = new Lucene46FieldInfosFormat();
   private final SegmentInfoFormat segmentInfosFormat = new Lucene46SegmentInfoFormat();
   private final LiveDocsFormat liveDocsFormat = new Lucene40LiveDocsFormat();
+  private final CompoundFormat compoundFormat = new Lucene40CompoundFormat();
   
   private final PostingsFormat postingsFormat = new PerFieldPostingsFormat() {
     @Override
@@ -80,12 +72,12 @@
   }
   
   @Override
-  public final StoredFieldsFormat storedFieldsFormat() {
+  public StoredFieldsFormat storedFieldsFormat() {
     return fieldsFormat;
   }
   
   @Override
-  public final TermVectorsFormat termVectorsFormat() {
+  public TermVectorsFormat termVectorsFormat() {
     return vectorsFormat;
   }
 
@@ -108,6 +100,11 @@
   public final LiveDocsFormat liveDocsFormat() {
     return liveDocsFormat;
   }
+  
+  @Override
+  public CompoundFormat compoundFormat() {
+    return compoundFormat;
+  }
 
   /** Returns the postings format that should be used for writing 
    *  new segments of <code>field</code>.
Index: lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene46/Lucene46FieldInfosFormat.java
===================================================================
--- lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene46/Lucene46FieldInfosFormat.java	(revision 1629405)
+++ lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene46/Lucene46FieldInfosFormat.java	(working copy)
@@ -19,77 +19,15 @@
 
 import java.io.IOException;
 
-import org.apache.lucene.codecs.CodecUtil;
-import org.apache.lucene.codecs.DocValuesFormat;
 import org.apache.lucene.codecs.FieldInfosFormat;
 import org.apache.lucene.codecs.FieldInfosReader;
 import org.apache.lucene.codecs.FieldInfosWriter;
-import org.apache.lucene.index.FieldInfo.DocValuesType;
-import org.apache.lucene.store.DataOutput;
 
 /**
  * Lucene 4.6 Field Infos format.
- * <p>
- * <p>Field names are stored in the field info file, with suffix <tt>.fnm</tt>.</p>
- * <p>FieldInfos (.fnm) --&gt; Header,FieldsCount, &lt;FieldName,FieldNumber,
- * FieldBits,DocValuesBits,DocValuesGen,Attributes&gt; <sup>FieldsCount</sup>,Footer</p>
- * <p>Data types:
- * <ul>
- *   <li>Header --&gt; {@link CodecUtil#checkHeader CodecHeader}</li>
- *   <li>FieldsCount --&gt; {@link DataOutput#writeVInt VInt}</li>
- *   <li>FieldName --&gt; {@link DataOutput#writeString String}</li>
- *   <li>FieldBits, DocValuesBits --&gt; {@link DataOutput#writeByte Byte}</li>
- *   <li>FieldNumber --&gt; {@link DataOutput#writeInt VInt}</li>
- *   <li>Attributes --&gt; {@link DataOutput#writeStringStringMap Map&lt;String,String&gt;}</li>
- *   <li>DocValuesGen --&gt; {@link DataOutput#writeLong(long) Int64}</li>
- *   <li>Footer --&gt; {@link CodecUtil#writeFooter CodecFooter}</li>
- * </ul>
- * </p>
- * Field Descriptions:
- * <ul>
- *   <li>FieldsCount: the number of fields in this file.</li>
- *   <li>FieldName: name of the field as a UTF-8 String.</li>
- *   <li>FieldNumber: the field's number. Note that unlike previous versions of
- *       Lucene, the fields are not numbered implicitly by their order in the
- *       file, instead explicitly.</li>
- *   <li>FieldBits: a byte containing field options.
- *       <ul>
- *         <li>The low-order bit is one for indexed fields, and zero for non-indexed
- *             fields.</li>
- *         <li>The second lowest-order bit is one for fields that have term vectors
- *             stored, and zero for fields without term vectors.</li>
- *         <li>If the third lowest order-bit is set (0x4), offsets are stored into
- *             the postings list in addition to positions.</li>
- *         <li>Fourth bit is unused.</li>
- *         <li>If the fifth lowest-order bit is set (0x10), norms are omitted for the
- *             indexed field.</li>
- *         <li>If the sixth lowest-order bit is set (0x20), payloads are stored for the
- *             indexed field.</li>
- *         <li>If the seventh lowest-order bit is set (0x40), term frequencies and
- *             positions omitted for the indexed field.</li>
- *         <li>If the eighth lowest-order bit is set (0x80), positions are omitted for the
- *             indexed field.</li>
- *       </ul>
- *    </li>
- *    <li>DocValuesBits: a byte containing per-document value types. The type
- *        recorded as two four-bit integers, with the high-order bits representing
- *        <code>norms</code> options, and the low-order bits representing 
- *        {@code DocValues} options. Each four-bit integer can be decoded as such:
- *        <ul>
- *          <li>0: no DocValues for this field.</li>
- *          <li>1: NumericDocValues. ({@link DocValuesType#NUMERIC})</li>
- *          <li>2: BinaryDocValues. ({@code DocValuesType#BINARY})</li>
- *          <li>3: SortedDocValues. ({@code DocValuesType#SORTED})</li>
- *        </ul>
- *    </li>
- *    <li>DocValuesGen is the generation count of the field's DocValues. If this is -1,
- *        there are no DocValues updates to that field. Anything above zero means there 
- *        are updates stored by {@link DocValuesFormat}.</li>
- *    <li>Attributes: a key-value map of codec-private attributes.</li>
- * </ul>
- *
- * @lucene.experimental
+ * @deprecated only for old 4.x segments
  */
+@Deprecated
 public final class Lucene46FieldInfosFormat extends FieldInfosFormat {
   private final FieldInfosReader reader = new Lucene46FieldInfosReader();
   private final FieldInfosWriter writer = new Lucene46FieldInfosWriter();
@@ -99,7 +37,7 @@
   }
 
   @Override
-  public FieldInfosReader getFieldInfosReader() throws IOException {
+  public final FieldInfosReader getFieldInfosReader() throws IOException {
     return reader;
   }
 
Index: lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene46/Lucene46FieldInfosReader.java
===================================================================
--- lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene46/Lucene46FieldInfosReader.java	(revision 1629405)
+++ lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene46/Lucene46FieldInfosReader.java	(working copy)
@@ -38,9 +38,9 @@
 /**
  * Lucene 4.6 FieldInfos reader.
  * 
- * @lucene.experimental
- * @see Lucene46FieldInfosFormat
+ * @deprecated only for old 4.x segments
  */
+@Deprecated
 final class Lucene46FieldInfosReader extends FieldInfosReader {
 
   /** Sole constructor. */
Index: lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene46/Lucene46FieldInfosWriter.java
===================================================================
--- lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene46/Lucene46FieldInfosWriter.java	(revision 1629405)
+++ lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene46/Lucene46FieldInfosWriter.java	(working copy)
@@ -34,9 +34,9 @@
 /**
  * Lucene 4.6 FieldInfos writer.
  * 
- * @see Lucene46FieldInfosFormat
- * @lucene.experimental
+ * @deprecated only for old 4.x segments
  */
+@Deprecated
 final class Lucene46FieldInfosWriter extends FieldInfosWriter {
   
   /** Sole constructor. */
Index: lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene46/Lucene46SegmentInfoFormat.java
===================================================================
--- lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene46/Lucene46SegmentInfoFormat.java	(revision 1629405)
+++ lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene46/Lucene46SegmentInfoFormat.java	(working copy)
@@ -17,54 +17,16 @@
  * limitations under the License.
  */
 
-import org.apache.lucene.codecs.CodecUtil;
 import org.apache.lucene.codecs.SegmentInfoFormat;
 import org.apache.lucene.codecs.SegmentInfoReader;
 import org.apache.lucene.codecs.SegmentInfoWriter;
-import org.apache.lucene.index.IndexWriter; // javadocs
-import org.apache.lucene.index.SegmentInfo; // javadocs
-import org.apache.lucene.index.SegmentInfos; // javadocs
-import org.apache.lucene.store.DataOutput; // javadocs
+import org.apache.lucene.index.SegmentInfo;
 
 /**
  * Lucene 4.6 Segment info format.
- * <p>
- * Files:
- * <ul>
- *   <li><tt>.si</tt>: Header, SegVersion, SegSize, IsCompoundFile, Diagnostics, Files, Footer
- * </ul>
- * </p>
- * Data types:
- * <p>
- * <ul>
- *   <li>Header --&gt; {@link CodecUtil#writeHeader CodecHeader}</li>
- *   <li>SegSize --&gt; {@link DataOutput#writeInt Int32}</li>
- *   <li>SegVersion --&gt; {@link DataOutput#writeString String}</li>
- *   <li>Files --&gt; {@link DataOutput#writeStringSet Set&lt;String&gt;}</li>
- *   <li>Diagnostics --&gt; {@link DataOutput#writeStringStringMap Map&lt;String,String&gt;}</li>
- *   <li>IsCompoundFile --&gt; {@link DataOutput#writeByte Int8}</li>
- *   <li>Footer --&gt; {@link CodecUtil#writeFooter CodecFooter}</li>
- * </ul>
- * </p>
- * Field Descriptions:
- * <p>
- * <ul>
- *   <li>SegVersion is the code version that created the segment.</li>
- *   <li>SegSize is the number of documents contained in the segment index.</li>
- *   <li>IsCompoundFile records whether the segment is written as a compound file or
- *       not. If this is -1, the segment is not a compound file. If it is 1, the segment
- *       is a compound file.</li>
- *   <li>The Diagnostics Map is privately written by {@link IndexWriter}, as a debugging aid,
- *       for each segment it creates. It includes metadata like the current Lucene
- *       version, OS, Java version, why the segment was created (merge, flush,
- *       addIndexes), etc.</li>
- *   <li>Files is a list of files referred to by this segment.</li>
- * </ul>
- * </p>
- * 
- * @see SegmentInfos
- * @lucene.experimental
+ * @deprecated only for old 4.x segments
  */
+@Deprecated
 public class Lucene46SegmentInfoFormat extends SegmentInfoFormat {
   private final SegmentInfoReader reader = new Lucene46SegmentInfoReader();
 
@@ -73,7 +35,7 @@
   }
   
   @Override
-  public SegmentInfoReader getSegmentInfoReader() {
+  public final SegmentInfoReader getSegmentInfoReader() {
     return reader;
   }
 
@@ -83,7 +45,7 @@
   }
 
   /** File extension used to store {@link SegmentInfo}. */
-  public final static String SI_EXTENSION = "si";
+  final static String SI_EXTENSION = "si";
   static final String CODEC_NAME = "Lucene46SegmentInfo";
   static final int VERSION_START = 0;
   static final int VERSION_CHECKSUM = 1;
Index: lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene46/Lucene46SegmentInfoReader.java
===================================================================
--- lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene46/Lucene46SegmentInfoReader.java	(revision 1629405)
+++ lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene46/Lucene46SegmentInfoReader.java	(working copy)
@@ -33,12 +33,11 @@
 import org.apache.lucene.util.Version;
 
 /**
- * Lucene 4.6 implementation of {@link SegmentInfoReader}.
- * 
- * @see Lucene46SegmentInfoFormat
- * @lucene.experimental
+ * Lucene 4.6 segment infos reader
+ * @deprecated only for old 4.x segments
  */
-public class Lucene46SegmentInfoReader extends SegmentInfoReader {
+@Deprecated
+final class Lucene46SegmentInfoReader extends SegmentInfoReader {
 
   /** Sole constructor. */
   public Lucene46SegmentInfoReader() {
Index: lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene49/Lucene49Codec.java
===================================================================
--- lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene49/Lucene49Codec.java	(revision 1629405)
+++ lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene49/Lucene49Codec.java	(working copy)
@@ -20,9 +20,9 @@
 import java.io.IOException;
 
 import org.apache.lucene.codecs.Codec;
+import org.apache.lucene.codecs.CompoundFormat;
 import org.apache.lucene.codecs.DocValuesFormat;
 import org.apache.lucene.codecs.FieldInfosFormat;
-import org.apache.lucene.codecs.FilterCodec;
 import org.apache.lucene.codecs.LiveDocsFormat;
 import org.apache.lucene.codecs.NormsConsumer;
 import org.apache.lucene.codecs.NormsFormat;
@@ -30,6 +30,7 @@
 import org.apache.lucene.codecs.SegmentInfoFormat;
 import org.apache.lucene.codecs.StoredFieldsFormat;
 import org.apache.lucene.codecs.TermVectorsFormat;
+import org.apache.lucene.codecs.lucene40.Lucene40CompoundFormat;
 import org.apache.lucene.codecs.lucene40.Lucene40LiveDocsFormat;
 import org.apache.lucene.codecs.lucene41.Lucene41StoredFieldsFormat;
 import org.apache.lucene.codecs.lucene42.Lucene42TermVectorsFormat;
@@ -40,18 +41,10 @@
 import org.apache.lucene.index.SegmentWriteState;
 
 /**
- * Implements the Lucene 4.9 index format, with configurable per-field postings
- * and docvalues formats.
- * <p>
- * If you want to reuse functionality of this codec in another codec, extend
- * {@link FilterCodec}.
- *
- * @see org.apache.lucene.codecs.lucene49 package documentation for file format details.
- * @lucene.experimental
+ * Implements the Lucene 4.9 index format
+ * @deprecated only for old 4.x segments
  */
-// NOTE: if we make largish changes in a minor release, easier to just make Lucene410Codec or whatever
-// if they are backwards compatible or smallish we can probably do the backwards in the postingsreader
-// (it writes a minor version, etc).
+@Deprecated
 public class Lucene49Codec extends Codec {
   private final StoredFieldsFormat fieldsFormat = new Lucene41StoredFieldsFormat();
   private final TermVectorsFormat vectorsFormat = new Lucene42TermVectorsFormat();
@@ -58,6 +51,7 @@
   private final FieldInfosFormat fieldInfosFormat = new Lucene46FieldInfosFormat();
   private final SegmentInfoFormat segmentInfosFormat = new Lucene46SegmentInfoFormat();
   private final LiveDocsFormat liveDocsFormat = new Lucene40LiveDocsFormat();
+  private final CompoundFormat compoundFormat = new Lucene40CompoundFormat();
   
   private final PostingsFormat postingsFormat = new PerFieldPostingsFormat() {
     @Override
@@ -79,12 +73,12 @@
   }
   
   @Override
-  public final StoredFieldsFormat storedFieldsFormat() {
+  public StoredFieldsFormat storedFieldsFormat() {
     return fieldsFormat;
   }
   
   @Override
-  public final TermVectorsFormat termVectorsFormat() {
+  public TermVectorsFormat termVectorsFormat() {
     return vectorsFormat;
   }
 
@@ -107,6 +101,11 @@
   public final LiveDocsFormat liveDocsFormat() {
     return liveDocsFormat;
   }
+  
+  @Override
+  public CompoundFormat compoundFormat() {
+    return compoundFormat;
+  }
 
   /** Returns the postings format that should be used for writing 
    *  new segments of <code>field</code>.
Index: lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene49/Lucene49DocValuesConsumer.java
===================================================================
--- lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene49/Lucene49DocValuesConsumer.java	(revision 1629405)
+++ lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene49/Lucene49DocValuesConsumer.java	(working copy)
@@ -40,7 +40,11 @@
 import org.apache.lucene.util.packed.MonotonicBlockPackedWriter;
 import org.apache.lucene.util.packed.PackedInts;
 
-/** writer for {@link Lucene49DocValuesFormat} */
+/** 
+ * writer for 4.9 docvalues format
+ * @deprecated only for old 4.x segments
+ */
+@Deprecated
 class Lucene49DocValuesConsumer extends DocValuesConsumer implements Closeable {
 
   static final int BLOCK_SIZE = 16384;
Index: lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene49/Lucene49DocValuesFormat.java
===================================================================
--- lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene49/Lucene49DocValuesFormat.java	(revision 1629405)
+++ lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene49/Lucene49DocValuesFormat.java	(working copy)
@@ -19,151 +19,17 @@
 
 import java.io.IOException;
 
-import org.apache.lucene.codecs.CodecUtil;
 import org.apache.lucene.codecs.DocValuesConsumer;
 import org.apache.lucene.codecs.DocValuesProducer;
 import org.apache.lucene.codecs.DocValuesFormat;
 import org.apache.lucene.index.SegmentReadState;
 import org.apache.lucene.index.SegmentWriteState;
-import org.apache.lucene.index.FieldInfo.DocValuesType;
-import org.apache.lucene.store.DataOutput;
-import org.apache.lucene.util.SmallFloat;
-import org.apache.lucene.util.fst.FST;
-import org.apache.lucene.util.packed.DirectWriter;
-import org.apache.lucene.util.packed.MonotonicBlockPackedWriter;
 
 /**
  * Lucene 4.9 DocValues format.
- * <p>
- * Encodes the five per-document value types (Numeric,Binary,Sorted,SortedSet,SortedNumeric) with these strategies:
- * <p>
- * {@link DocValuesType#NUMERIC NUMERIC}:
- * <ul>
- *    <li>Delta-compressed: per-document integers written as deltas from the minimum value,
- *        compressed with bitpacking. For more information, see {@link DirectWriter}.
- *    <li>Table-compressed: when the number of unique values is very small (&lt; 256), and
- *        when there are unused "gaps" in the range of values used (such as {@link SmallFloat}), 
- *        a lookup table is written instead. Each per-document entry is instead the ordinal 
- *        to this table, and those ordinals are compressed with bitpacking ({@link DirectWriter}). 
- *    <li>GCD-compressed: when all numbers share a common divisor, such as dates, the greatest
- *        common denominator (GCD) is computed, and quotients are stored using Delta-compressed Numerics.
- *    <li>Monotonic-compressed: when all numbers are monotonically increasing offsets, they are written
- *        as blocks of bitpacked integers, encoding the deviation from the expected delta.
- * </ul>
- * <p>
- * {@link DocValuesType#BINARY BINARY}:
- * <ul>
- *    <li>Fixed-width Binary: one large concatenated byte[] is written, along with the fixed length.
- *        Each document's value can be addressed directly with multiplication ({@code docID * length}). 
- *    <li>Variable-width Binary: one large concatenated byte[] is written, along with end addresses 
- *        for each document. The addresses are written as Monotonic-compressed numerics.
- *    <li>Prefix-compressed Binary: values are written in chunks of 16, with the first value written
- *        completely and other values sharing prefixes. chunk addresses are written as Monotonic-compressed
- *        numerics.
- * </ul>
- * <p>
- * {@link DocValuesType#SORTED SORTED}:
- * <ul>
- *    <li>Sorted: a mapping of ordinals to deduplicated terms is written as Prefix-Compressed Binary, 
- *        along with the per-document ordinals written using one of the numeric strategies above.
- * </ul>
- * <p>
- * {@link DocValuesType#SORTED_SET SORTED_SET}:
- * <ul>
- *    <li>SortedSet: a mapping of ordinals to deduplicated terms is written as Prefix-Compressed Binary, 
- *        an ordinal list and per-document index into this list are written using the numeric strategies 
- *        above. 
- * </ul>
- * <p>
- * {@link DocValuesType#SORTED_NUMERIC SORTED_NUMERIC}:
- * <ul>
- *    <li>SortedNumeric: a value list and per-document index into this list are written using the numeric
- *        strategies above.
- * </ul>
- * <p>
- * Files:
- * <ol>
- *   <li><tt>.dvd</tt>: DocValues data</li>
- *   <li><tt>.dvm</tt>: DocValues metadata</li>
- * </ol>
- * <ol>
- *   <li><a name="dvm" id="dvm"></a>
- *   <p>The DocValues metadata or .dvm file.</p>
- *   <p>For DocValues field, this stores metadata, such as the offset into the 
- *      DocValues data (.dvd)</p>
- *   <p>DocValues metadata (.dvm) --&gt; Header,&lt;Entry&gt;<sup>NumFields</sup>,Footer</p>
- *   <ul>
- *     <li>Entry --&gt; NumericEntry | BinaryEntry | SortedEntry | SortedSetEntry | SortedNumericEntry</li>
- *     <li>NumericEntry --&gt; GCDNumericEntry | TableNumericEntry | DeltaNumericEntry</li>
- *     <li>GCDNumericEntry --&gt; NumericHeader,MinValue,GCD,BitsPerValue</li>
- *     <li>TableNumericEntry --&gt; NumericHeader,TableSize,{@link DataOutput#writeLong Int64}<sup>TableSize</sup>,BitsPerValue</li>
- *     <li>DeltaNumericEntry --&gt; NumericHeader,MinValue,BitsPerValue</li>
- *     <li>MonotonicNumericEntry --&gt; NumericHeader,PackedVersion,BlockSize</li>
- *     <li>NumericHeader --&gt; FieldNumber,EntryType,NumericType,MissingOffset,DataOffset,Count,EndOffset</li>
- *     <li>BinaryEntry --&gt; FixedBinaryEntry | VariableBinaryEntry | PrefixBinaryEntry</li>
- *     <li>FixedBinaryEntry --&gt; BinaryHeader</li>
- *     <li>VariableBinaryEntry --&gt; BinaryHeader,AddressOffset,PackedVersion,BlockSize</li>
- *     <li>PrefixBinaryEntry --&gt; BinaryHeader,AddressInterval,AddressOffset,PackedVersion,BlockSize</li>
- *     <li>BinaryHeader --&gt; FieldNumber,EntryType,BinaryType,MissingOffset,MinLength,MaxLength,DataOffset</li>
- *     <li>SortedEntry --&gt; FieldNumber,EntryType,BinaryEntry,NumericEntry</li>
- *     <li>SortedSetEntry --&gt; EntryType,BinaryEntry,NumericEntry,NumericEntry</li>
- *     <li>SortedNumericEntry --&gt; EntryType,NumericEntry,NumericEntry</li>
- *     <li>FieldNumber,PackedVersion,MinLength,MaxLength,BlockSize,ValueCount --&gt; {@link DataOutput#writeVInt VInt}</li>
- *     <li>EntryType,CompressionType --&gt; {@link DataOutput#writeByte Byte}</li>
- *     <li>Header --&gt; {@link CodecUtil#writeHeader CodecHeader}</li>
- *     <li>MinValue,GCD,MissingOffset,AddressOffset,DataOffset,EndOffset --&gt; {@link DataOutput#writeLong Int64}</li>
- *     <li>TableSize,BitsPerValue --&gt; {@link DataOutput#writeVInt vInt}</li>
- *     <li>Footer --&gt; {@link CodecUtil#writeFooter CodecFooter}</li>
- *   </ul>
- *   <p>Sorted fields have two entries: a BinaryEntry with the value metadata,
- *      and an ordinary NumericEntry for the document-to-ord metadata.</p>
- *   <p>SortedSet fields have three entries: a BinaryEntry with the value metadata,
- *      and two NumericEntries for the document-to-ord-index and ordinal list metadata.</p>
- *   <p>SortedNumeric fields have two entries: A NumericEntry with the value metadata,
- *      and a numeric entry with the document-to-value index.</p>
- *   <p>FieldNumber of -1 indicates the end of metadata.</p>
- *   <p>EntryType is a 0 (NumericEntry) or 1 (BinaryEntry)</p>
- *   <p>DataOffset is the pointer to the start of the data in the DocValues data (.dvd)</p>
- *   <p>EndOffset is the pointer to the end of the data in the DocValues data (.dvd)</p>
- *   <p>NumericType indicates how Numeric values will be compressed:
- *      <ul>
- *         <li>0 --&gt; delta-compressed. For each block of 16k integers, every integer is delta-encoded
- *             from the minimum value within the block. 
- *         <li>1 --&gt, gcd-compressed. When all integers share a common divisor, only quotients are stored
- *             using blocks of delta-encoded ints.
- *         <li>2 --&gt; table-compressed. When the number of unique numeric values is small and it would save space,
- *             a lookup table of unique values is written, followed by the ordinal for each document.
- *      </ul>
- *   <p>BinaryType indicates how Binary values will be stored:
- *      <ul>
- *         <li>0 --&gt; fixed-width. All values have the same length, addressing by multiplication. 
- *         <li>1 --&gt, variable-width. An address for each value is stored.
- *         <li>2 --&gt; prefix-compressed. An address to the start of every interval'th value is stored.
- *      </ul>
- *   <p>MinLength and MaxLength represent the min and max byte[] value lengths for Binary values.
- *      If they are equal, then all values are of a fixed size, and can be addressed as DataOffset + (docID * length).
- *      Otherwise, the binary values are of variable size, and packed integer metadata (PackedVersion,BlockSize)
- *      is written for the addresses.
- *   <p>MissingOffset points to a byte[] containing a bitset of all documents that had a value for the field.
- *      If its -1, then there are no missing values.
- *   <p>Checksum contains the CRC32 checksum of all bytes in the .dvm file up
- *      until the checksum. This is used to verify integrity of the file on opening the
- *      index.
- *   <li><a name="dvd" id="dvd"></a>
- *   <p>The DocValues data or .dvd file.</p>
- *   <p>For DocValues field, this stores the actual per-document data (the heavy-lifting)</p>
- *   <p>DocValues data (.dvd) --&gt; Header,&lt;NumericData | BinaryData | SortedData&gt;<sup>NumFields</sup>,Footer</p>
- *   <ul>
- *     <li>NumericData --&gt; DeltaCompressedNumerics | TableCompressedNumerics | GCDCompressedNumerics</li>
- *     <li>BinaryData --&gt;  {@link DataOutput#writeByte Byte}<sup>DataLength</sup>,Addresses</li>
- *     <li>SortedData --&gt; {@link FST FST&lt;Int64&gt;}</li>
- *     <li>DeltaCompressedNumerics,TableCompressedNumerics,GCDCompressedNumerics --&gt; {@link DirectWriter PackedInts}</li>
- *     <li>Addresses --&gt; {@link MonotonicBlockPackedWriter MonotonicBlockPackedInts(blockSize=16k)}</li>
- *     <li>Footer --&gt; {@link CodecUtil#writeFooter CodecFooter}</li>
- *   </ul>
- * </ol>
- * @lucene.experimental
+ * @deprecated only for old 4.x segments
  */
+@Deprecated
 public class Lucene49DocValuesFormat extends DocValuesFormat {
 
   /** Sole Constructor */
@@ -177,7 +43,7 @@
   }
 
   @Override
-  public DocValuesProducer fieldsProducer(SegmentReadState state) throws IOException {
+  public final DocValuesProducer fieldsProducer(SegmentReadState state) throws IOException {
     return new Lucene49DocValuesProducer(state, DATA_CODEC, DATA_EXTENSION, META_CODEC, META_EXTENSION);
   }
   
Index: lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene49/Lucene49DocValuesProducer.java
===================================================================
--- lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene49/Lucene49DocValuesProducer.java	(revision 1629405)
+++ lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene49/Lucene49DocValuesProducer.java	(working copy)
@@ -67,56 +67,75 @@
 import org.apache.lucene.util.packed.DirectReader;
 import org.apache.lucene.util.packed.MonotonicBlockPackedReader;
 
-/** reader for {@link Lucene49DocValuesFormat} */
+/** 
+ * reader for 4.9 docvalues format
+ * @deprecated only for 4.x segments 
+ */
+@Deprecated
 class Lucene49DocValuesProducer extends DocValuesProducer implements Closeable {
-  private final Map<String,NumericEntry> numerics;
-  private final Map<String,BinaryEntry> binaries;
-  private final Map<String,SortedSetEntry> sortedSets;
-  private final Map<String,SortedSetEntry> sortedNumerics;
-  private final Map<String,NumericEntry> ords;
-  private final Map<String,NumericEntry> ordIndexes;
+  private final Map<String,NumericEntry> numerics = new HashMap<>();
+  private final Map<String,BinaryEntry> binaries = new HashMap<>();
+  private final Map<String,SortedSetEntry> sortedSets = new HashMap<>();
+  private final Map<String,SortedSetEntry> sortedNumerics = new HashMap<>();
+  private final Map<String,NumericEntry> ords = new HashMap<>();
+  private final Map<String,NumericEntry> ordIndexes = new HashMap<>();
   private final AtomicLong ramBytesUsed;
   private final IndexInput data;
   private final int numFields;
   private final int maxDoc;
-  private final int version;
 
   // memory-resident structures
   private final Map<String,MonotonicBlockPackedReader> addressInstances = new HashMap<>();
   private final Map<String,MonotonicBlockPackedReader> ordIndexInstances = new HashMap<>();
   
+  private final boolean merging;
+  
+  // clone for merge: when merging we don't do any instances.put()s
+  Lucene49DocValuesProducer(Lucene49DocValuesProducer original) throws IOException {
+    assert Thread.holdsLock(original);
+    numerics.putAll(original.numerics);
+    binaries.putAll(original.binaries);
+    sortedSets.putAll(original.sortedSets);
+    sortedNumerics.putAll(original.sortedNumerics);
+    ords.putAll(original.ords);
+    ordIndexes.putAll(original.ordIndexes);
+    ramBytesUsed = new AtomicLong(original.ramBytesUsed());
+    data = original.data.clone();
+    numFields = original.numFields;
+    maxDoc = original.maxDoc;
+    addressInstances.putAll(original.addressInstances);
+    ordIndexInstances.putAll(original.ordIndexInstances);
+    merging = true;
+  }
+  
   /** expert: instantiates a new reader */
   Lucene49DocValuesProducer(SegmentReadState state, String dataCodec, String dataExtension, String metaCodec, String metaExtension) throws IOException {
     String metaName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, metaExtension);
+    this.maxDoc = state.segmentInfo.getDocCount();
+    merging = false;
+    
+    int version = -1;
+    int numFields = -1;
+    
     // read in the entries from the metadata file.
-    ChecksumIndexInput in = state.directory.openChecksumInput(metaName, state.context);
-    this.maxDoc = state.segmentInfo.getDocCount();
-    boolean success = false;
-    try {
-      version = CodecUtil.checkHeader(in, metaCodec, 
-                                      Lucene49DocValuesFormat.VERSION_START,
-                                      Lucene49DocValuesFormat.VERSION_CURRENT);
-      numerics = new HashMap<>();
-      ords = new HashMap<>();
-      ordIndexes = new HashMap<>();
-      binaries = new HashMap<>();
-      sortedSets = new HashMap<>();
-      sortedNumerics = new HashMap<>();
-      numFields = readFields(in, state.fieldInfos);
-
-      CodecUtil.checkFooter(in);
-      success = true;
-    } finally {
-      if (success) {
-        IOUtils.close(in);
-      } else {
-        IOUtils.closeWhileHandlingException(in);
+    try (ChecksumIndexInput in = state.directory.openChecksumInput(metaName, state.context)) {
+      Throwable priorE = null;
+      try {
+        version = CodecUtil.checkHeader(in, metaCodec, 
+                                            Lucene49DocValuesFormat.VERSION_START,
+                                            Lucene49DocValuesFormat.VERSION_CURRENT);
+        numFields = readFields(in, state.fieldInfos);
+      } catch (Throwable exception) {
+        priorE = exception;
+      } finally {
+        CodecUtil.checkFooter(in, priorE);
       }
     }
+    this.numFields = numFields;
 
     String dataName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, dataExtension);
     this.data = state.directory.openInput(dataName, state.context);
-    success = false;
+    boolean success = false;
     try {
       final int version2 = CodecUtil.checkHeader(data, dataCodec, 
                                                  Lucene49DocValuesFormat.VERSION_START,
@@ -443,8 +462,10 @@
     if (addrInstance == null) {
       data.seek(bytes.addressesOffset);
       addrInstance = MonotonicBlockPackedReader.of(data, bytes.packedIntsVersion, bytes.blockSize, bytes.count+1, false);
-      addressInstances.put(field.name, addrInstance);
-      ramBytesUsed.addAndGet(addrInstance.ramBytesUsed() + RamUsageEstimator.NUM_BYTES_INT);
+      if (!merging) {
+        addressInstances.put(field.name, addrInstance);
+        ramBytesUsed.addAndGet(addrInstance.ramBytesUsed() + RamUsageEstimator.NUM_BYTES_INT);
+      }
     }
     addresses = addrInstance;
     return addresses;
@@ -489,8 +510,10 @@
         size = 1L + bytes.count / interval;
       }
       addrInstance = MonotonicBlockPackedReader.of(data, bytes.packedIntsVersion, bytes.blockSize, size, false);
-      addressInstances.put(field.name, addrInstance);
-      ramBytesUsed.addAndGet(addrInstance.ramBytesUsed() + RamUsageEstimator.NUM_BYTES_INT);
+      if (!merging) {
+        addressInstances.put(field.name, addrInstance);
+        ramBytesUsed.addAndGet(addrInstance.ramBytesUsed() + RamUsageEstimator.NUM_BYTES_INT);
+      }
     }
     addresses = addrInstance;
     return addresses;
@@ -556,8 +579,10 @@
     if (ordIndexInstance == null) {
       data.seek(entry.offset);
       ordIndexInstance = MonotonicBlockPackedReader.of(data, entry.packedIntsVersion, entry.blockSize, entry.count+1, false);
-      ordIndexInstances.put(field.name, ordIndexInstance);
-      ramBytesUsed.addAndGet(ordIndexInstance.ramBytesUsed() + RamUsageEstimator.NUM_BYTES_INT);
+      if (!merging) {
+        ordIndexInstances.put(field.name, ordIndexInstance);
+        ramBytesUsed.addAndGet(ordIndexInstance.ramBytesUsed() + RamUsageEstimator.NUM_BYTES_INT);
+      }
     }
     ordIndex = ordIndexInstance;
     return ordIndex;
@@ -729,6 +754,11 @@
     data.close();
   }
   
+  @Override
+  public synchronized DocValuesProducer getMergeInstance() throws IOException {
+    return new Lucene49DocValuesProducer(this);
+  }
+
   /** metadata entry for a numeric docvalues field */
   static class NumericEntry {
     private NumericEntry() {}
Index: lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene49/Lucene49NormsFormat.java
===================================================================
--- lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene49/Lucene49NormsFormat.java	(revision 0)
+++ lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene49/Lucene49NormsFormat.java	(working copy)
@@ -0,0 +1,54 @@
+package org.apache.lucene.codecs.lucene49;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.codecs.NormsConsumer;
+import org.apache.lucene.codecs.NormsFormat;
+import org.apache.lucene.codecs.NormsProducer;
+import org.apache.lucene.index.SegmentReadState;
+import org.apache.lucene.index.SegmentWriteState;
+
+/**
+ * Lucene 4.9 Score normalization format.
+ * @deprecated only for reading 4.9/4.10 indexes
+ */
+@Deprecated
+public class Lucene49NormsFormat extends NormsFormat {
+
+  /** Sole Constructor */
+  public Lucene49NormsFormat() {}
+  
+  @Override
+  public NormsConsumer normsConsumer(SegmentWriteState state) throws IOException {
+    throw new UnsupportedOperationException("this codec can only be used for reading");
+  }
+
+  @Override
+  public final NormsProducer normsProducer(SegmentReadState state) throws IOException {
+    return new Lucene49NormsProducer(state, DATA_CODEC, DATA_EXTENSION, METADATA_CODEC, METADATA_EXTENSION);
+  }
+  
+  static final String DATA_CODEC = "Lucene49NormsData";
+  static final String DATA_EXTENSION = "nvd";
+  static final String METADATA_CODEC = "Lucene49NormsMetadata";
+  static final String METADATA_EXTENSION = "nvm";
+  static final int VERSION_START = 0;
+  static final int VERSION_CURRENT = VERSION_START;
+}

Property changes on: lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene49/Lucene49NormsFormat.java
___________________________________________________________________
Added: svn:eol-style
## -0,0 +1 ##
+native
\ No newline at end of property
Index: lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene49/Lucene49NormsProducer.java
===================================================================
--- lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene49/Lucene49NormsProducer.java	(revision 0)
+++ lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene49/Lucene49NormsProducer.java	(working copy)
@@ -0,0 +1,269 @@
+package org.apache.lucene.codecs.lucene49;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.HashMap;
+import java.util.Map;
+import java.util.concurrent.atomic.AtomicInteger;
+import java.util.concurrent.atomic.AtomicLong;
+
+import org.apache.lucene.codecs.CodecUtil;
+import org.apache.lucene.codecs.NormsProducer;
+import org.apache.lucene.index.CorruptIndexException;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.NumericDocValues;
+import org.apache.lucene.index.SegmentReadState;
+import org.apache.lucene.store.ChecksumIndexInput;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.util.Accountable;
+import org.apache.lucene.util.Accountables;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util.RamUsageEstimator;
+import org.apache.lucene.util.packed.BlockPackedReader;
+import org.apache.lucene.util.packed.PackedInts;
+
+import static org.apache.lucene.codecs.lucene49.Lucene49NormsFormat.VERSION_START;
+import static org.apache.lucene.codecs.lucene49.Lucene49NormsFormat.VERSION_CURRENT;
+
+/**
+ * Reader for 4.9 norms
+ * @deprecated only for reading 4.9/4.10 indexes
+ */
+@Deprecated
+final class Lucene49NormsProducer extends NormsProducer {
+  static final byte DELTA_COMPRESSED = 0;
+  static final byte TABLE_COMPRESSED = 1;
+  static final byte CONST_COMPRESSED = 2;
+  static final byte UNCOMPRESSED = 3;
+  static final int BLOCK_SIZE = 16384;
+  
+  // metadata maps (just file pointers and minimal stuff)
+  private final Map<String,NormsEntry> norms = new HashMap<>();
+  private final IndexInput data;
+  
+  // ram instances we have already loaded
+  final Map<String,NumericDocValues> instances = new HashMap<>();
+  final Map<String,Accountable> instancesInfo = new HashMap<>();
+  
+  private final int maxDoc;
+  private final AtomicLong ramBytesUsed;
+  private final AtomicInteger activeCount = new AtomicInteger();
+  
+  private final boolean merging;
+  
+  // clone for merge: when merging we don't do any instances.put()s
+  Lucene49NormsProducer(Lucene49NormsProducer original) {
+    assert Thread.holdsLock(original);
+    norms.putAll(original.norms);
+    data = original.data.clone();
+    instances.putAll(original.instances);
+    instancesInfo.putAll(original.instancesInfo);
+    maxDoc = original.maxDoc;
+    ramBytesUsed = new AtomicLong(original.ramBytesUsed.get());
+    activeCount.set(original.activeCount.get());
+    merging = true;
+  }
+    
+  Lucene49NormsProducer(SegmentReadState state, String dataCodec, String dataExtension, String metaCodec, String metaExtension) throws IOException {
+    maxDoc = state.segmentInfo.getDocCount();
+    merging = false;
+    String metaName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, metaExtension);
+    ramBytesUsed = new AtomicLong(RamUsageEstimator.shallowSizeOfInstance(getClass()));
+    int version = -1;
+    
+    // read in the entries from the metadata file.
+    try (ChecksumIndexInput in = state.directory.openChecksumInput(metaName, state.context)) {
+      Throwable priorE = null;
+      try {
+        version = CodecUtil.checkHeader(in, metaCodec, VERSION_START, VERSION_CURRENT);
+        readFields(in, state.fieldInfos);
+      } catch (Throwable exception) {
+        priorE = exception;
+      } finally {
+        CodecUtil.checkFooter(in, priorE);
+      }
+    }
+
+    String dataName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, dataExtension);
+    this.data = state.directory.openInput(dataName, state.context);
+    boolean success = false;
+    try {
+      final int version2 = CodecUtil.checkHeader(data, dataCodec, VERSION_START, VERSION_CURRENT);
+      if (version != version2) {
+        throw new CorruptIndexException("Format versions mismatch: meta=" + version + ",data=" + version2, data);
+      }
+      
+      // NOTE: data file is too costly to verify checksum against all the bytes on open,
+      // but for now we at least verify proper structure of the checksum footer: which looks
+      // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption
+      // such as file truncation.
+      CodecUtil.retrieveChecksum(data);
+
+      success = true;
+    } finally {
+      if (!success) {
+        IOUtils.closeWhileHandlingException(this.data);
+      }
+    }
+  }
+  
+  private void readFields(IndexInput meta, FieldInfos infos) throws IOException {
+    int fieldNumber = meta.readVInt();
+    while (fieldNumber != -1) {
+      FieldInfo info = infos.fieldInfo(fieldNumber);
+      if (info == null) {
+        throw new CorruptIndexException("Invalid field number: " + fieldNumber, meta);
+      } else if (!info.hasNorms()) {
+        throw new CorruptIndexException("Invalid field: " + info.name, meta);
+      }
+      NormsEntry entry = new NormsEntry();
+      entry.format = meta.readByte();
+      entry.offset = meta.readLong();
+      switch(entry.format) {
+        case CONST_COMPRESSED:
+        case UNCOMPRESSED:
+        case TABLE_COMPRESSED:
+        case DELTA_COMPRESSED:
+          break;
+        default:
+          throw new CorruptIndexException("Unknown format: " + entry.format, meta);
+      }
+      norms.put(info.name, entry);
+      fieldNumber = meta.readVInt();
+    }
+  }
+
+  @Override
+  public synchronized NumericDocValues getNorms(FieldInfo field) throws IOException {
+    NumericDocValues instance = instances.get(field.name);
+    if (instance == null) {
+      instance = loadNorms(field);
+      if (!merging) {
+        instances.put(field.name, instance);
+        activeCount.incrementAndGet();
+      }
+    }
+    return instance;
+  }
+  
+  @Override
+  public long ramBytesUsed() {
+    return ramBytesUsed.get();
+  }
+  
+  @Override
+  public synchronized Iterable<? extends Accountable> getChildResources() {
+    return Accountables.namedAccountables("field", instancesInfo);
+  }
+  
+  @Override
+  public void checkIntegrity() throws IOException {
+    CodecUtil.checksumEntireFile(data);
+  }
+
+  private NumericDocValues loadNorms(FieldInfo field) throws IOException {
+    NormsEntry entry = norms.get(field.name);
+    switch(entry.format) {
+      case CONST_COMPRESSED:
+        if (!merging) {
+          instancesInfo.put(field.name, Accountables.namedAccountable("constant", 8));
+          ramBytesUsed.addAndGet(8);
+        }
+        final long v = entry.offset;
+        return new NumericDocValues() {
+          @Override
+          public long get(int docID) {
+            return v;
+          }
+        };
+      case UNCOMPRESSED:
+        data.seek(entry.offset);
+        final byte bytes[] = new byte[maxDoc];
+        data.readBytes(bytes, 0, bytes.length);
+        if (!merging) {
+          ramBytesUsed.addAndGet(RamUsageEstimator.sizeOf(bytes));
+          instancesInfo.put(field.name, Accountables.namedAccountable("byte array", maxDoc));
+        }
+        return new NumericDocValues() {
+          @Override
+          public long get(int docID) {
+            return bytes[docID];
+          }
+        };
+      case DELTA_COMPRESSED:
+        data.seek(entry.offset);
+        int packedIntsVersion = data.readVInt();
+        int blockSize = data.readVInt();
+        final BlockPackedReader reader = new BlockPackedReader(data, packedIntsVersion, blockSize, maxDoc, false);
+        if (!merging) {
+          ramBytesUsed.addAndGet(reader.ramBytesUsed());
+          instancesInfo.put(field.name, Accountables.namedAccountable("delta compressed", reader));
+        }
+        return reader;
+      case TABLE_COMPRESSED:
+        data.seek(entry.offset);
+        int packedVersion = data.readVInt();
+        int size = data.readVInt();
+        if (size > 256) {
+          throw new CorruptIndexException("TABLE_COMPRESSED cannot have more than 256 distinct values, got=" + size, data);
+        }
+        final long decode[] = new long[size];
+        for (int i = 0; i < decode.length; i++) {
+          decode[i] = data.readLong();
+        }
+        final int formatID = data.readVInt();
+        final int bitsPerValue = data.readVInt();
+        final PackedInts.Reader ordsReader = PackedInts.getReaderNoHeader(data, PackedInts.Format.byId(formatID), packedVersion, maxDoc, bitsPerValue);
+        if (!merging) {
+          ramBytesUsed.addAndGet(RamUsageEstimator.sizeOf(decode) + ordsReader.ramBytesUsed());
+          instancesInfo.put(field.name, Accountables.namedAccountable("table compressed", ordsReader));
+        }
+        return new NumericDocValues() {
+          @Override
+          public long get(int docID) {
+            return decode[(int)ordsReader.get(docID)];
+          }
+        };
+      default:
+        throw new AssertionError();
+    }
+  }
+
+  @Override
+  public void close() throws IOException {
+    data.close();
+  }
+  
+  @Override
+  public synchronized NormsProducer getMergeInstance() throws IOException {
+    return new Lucene49NormsProducer(this);
+  }
+
+  static class NormsEntry {
+    byte format;
+    long offset;
+  }
+
+  @Override
+  public String toString() {
+    return getClass().getSimpleName() + "(fields=" + norms.size() + ",active=" + activeCount.get() + ")";
+  }
+}

Property changes on: lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene49/Lucene49NormsProducer.java
___________________________________________________________________
Added: svn:eol-style
## -0,0 +1 ##
+native
\ No newline at end of property
Index: lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/Lucene40DocValuesWriter.java
===================================================================
--- lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/Lucene40DocValuesWriter.java	(revision 1629405)
+++ lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/Lucene40DocValuesWriter.java	(working copy)
@@ -26,11 +26,9 @@
 import org.apache.lucene.codecs.DocValuesConsumer;
 import org.apache.lucene.codecs.MissingOrdRemapper;
 import org.apache.lucene.codecs.lucene40.Lucene40FieldInfosReader.LegacyDocValuesType;
-import org.apache.lucene.index.BinaryDocValues;
 import org.apache.lucene.index.FieldInfo;
 import org.apache.lucene.index.IndexFileNames;
 import org.apache.lucene.index.SegmentWriteState;
-import org.apache.lucene.store.CompoundFileDirectory;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.IndexOutput;
 import org.apache.lucene.util.BytesRef;
@@ -37,7 +35,12 @@
 import org.apache.lucene.util.IOUtils;
 import org.apache.lucene.util.packed.PackedInts;
 
-class Lucene40DocValuesWriter extends DocValuesConsumer {
+/**
+ * Writer for 4.0 docvalues format
+ * @deprecated for test purposes only
+ */
+@Deprecated
+final class Lucene40DocValuesWriter extends DocValuesConsumer {
   private final Directory dir;
   private final SegmentWriteState state;
   private final String legacyKey;
@@ -47,7 +50,7 @@
   Lucene40DocValuesWriter(SegmentWriteState state, String filename, String legacyKey) throws IOException {
     this.state = state;
     this.legacyKey = legacyKey;
-    this.dir = new CompoundFileDirectory(state.directory, filename, state.context, true);
+    this.dir = new Lucene40CompoundReader(state.directory, filename, state.context, true);
   }
   
   @Override
Index: lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/Lucene40FieldInfosWriter.java
===================================================================
--- lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/Lucene40FieldInfosWriter.java	(revision 1629405)
+++ lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/Lucene40FieldInfosWriter.java	(working copy)
@@ -33,13 +33,11 @@
 import org.apache.lucene.util.IOUtils;
 
 /**
- * Lucene 4.0 FieldInfos writer.
- * 
- * @see Lucene40FieldInfosFormat
- * @lucene.experimental
+ * Writer for 4.0 fieldinfos format
+ * @deprecated for test purposes only
  */
 @Deprecated
-public class Lucene40FieldInfosWriter extends FieldInfosWriter {
+public final class Lucene40FieldInfosWriter extends FieldInfosWriter {
 
   /** Sole constructor. */
   public Lucene40FieldInfosWriter() {
Index: lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/Lucene40PostingsWriter.java
===================================================================
--- lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/Lucene40PostingsWriter.java	(revision 1629405)
+++ lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/Lucene40PostingsWriter.java	(working copy)
@@ -37,12 +37,11 @@
 import org.apache.lucene.util.IOUtils;
 
 /**
- * Concrete class that writes the 4.0 frq/prx postings format.
- * 
- * @see Lucene40PostingsFormat
- * @lucene.experimental 
+ * Writer for 4.0 postings format
+ * @deprecated for test purposes only
  */
-public final class Lucene40PostingsWriter extends PushPostingsWriterBase {
+@Deprecated
+final class Lucene40PostingsWriter extends PushPostingsWriterBase {
 
   final IndexOutput freqOut;
   final IndexOutput proxOut;
Index: lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/Lucene40RWCodec.java
===================================================================
--- lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/Lucene40RWCodec.java	(revision 1629405)
+++ lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/Lucene40RWCodec.java	(working copy)
@@ -28,8 +28,11 @@
  * limitations under the License.
  */
 
-/** Read-write version of Lucene40Codec for testing */
-@SuppressWarnings("deprecation")
+/**
+ * Read-write version of 4.0 codec for testing
+ * @deprecated for test purposes only
+ */
+@Deprecated
 public final class Lucene40RWCodec extends Lucene40Codec {
   
   private final FieldInfosFormat fieldInfos = new Lucene40FieldInfosFormat() {
Index: lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/Lucene40RWDocValuesFormat.java
===================================================================
--- lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/Lucene40RWDocValuesFormat.java	(revision 1629405)
+++ lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/Lucene40RWDocValuesFormat.java	(working copy)
@@ -23,15 +23,18 @@
 import org.apache.lucene.index.IndexFileNames;
 import org.apache.lucene.index.SegmentWriteState;
 
-/** Read-write version of {@link Lucene40DocValuesFormat} for testing */
-@SuppressWarnings("deprecation")
-public class Lucene40RWDocValuesFormat extends Lucene40DocValuesFormat {
+/**
+ * Read-write version of 4.0 docvalues format for testing
+ * @deprecated for test purposes only
+ */
+@Deprecated
+public final class Lucene40RWDocValuesFormat extends Lucene40DocValuesFormat {
 
   @Override
   public DocValuesConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
     String filename = IndexFileNames.segmentFileName(state.segmentInfo.name, 
           "dv", 
-          IndexFileNames.COMPOUND_FILE_EXTENSION);
+          Lucene40CompoundFormat.COMPOUND_FILE_EXTENSION);
     return new Lucene40DocValuesWriter(state, filename, Lucene40FieldInfosReader.LEGACY_DV_TYPE_KEY);
   }
 }
Index: lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/Lucene40RWNormsFormat.java
===================================================================
--- lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/Lucene40RWNormsFormat.java	(revision 1629405)
+++ lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/Lucene40RWNormsFormat.java	(working copy)
@@ -24,15 +24,18 @@
 import org.apache.lucene.index.IndexFileNames;
 import org.apache.lucene.index.SegmentWriteState;
 
-/** Read-write version of {@link Lucene40NormsFormat} for testing */
-@SuppressWarnings("deprecation")
-public class Lucene40RWNormsFormat extends Lucene40NormsFormat {
+/**
+ * Read-write version of 4.0 norms format for testing
+ * @deprecated for test purposes only
+ */
+@Deprecated
+public final class Lucene40RWNormsFormat extends Lucene40NormsFormat {
 
   @Override
   public NormsConsumer normsConsumer(SegmentWriteState state) throws IOException {
     String filename = IndexFileNames.segmentFileName(state.segmentInfo.name, 
         "nrm", 
-        IndexFileNames.COMPOUND_FILE_EXTENSION);
+        Lucene40CompoundFormat.COMPOUND_FILE_EXTENSION);
     final Lucene40DocValuesWriter impl = new Lucene40DocValuesWriter(state, filename, Lucene40FieldInfosReader.LEGACY_NORM_TYPE_KEY);
     return new NormsConsumer() {
       @Override
Index: lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/Lucene40RWPostingsFormat.java
===================================================================
--- lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/Lucene40RWPostingsFormat.java	(revision 1629405)
+++ lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/Lucene40RWPostingsFormat.java	(working copy)
@@ -23,14 +23,19 @@
 import org.apache.lucene.codecs.PostingsWriterBase;
 import org.apache.lucene.codecs.blocktree.BlockTreeTermsWriter;
 import org.apache.lucene.index.SegmentWriteState;
-import org.apache.lucene.util.LuceneTestCase;
 
 /**
- * Read-write version of {@link Lucene40PostingsFormat} for testing.
+ * Read-write version of 4.0 postings format for testing
+ * @deprecated for test purposes only
  */
-@SuppressWarnings("deprecation")
-public class Lucene40RWPostingsFormat extends Lucene40PostingsFormat {
+@Deprecated
+public final class Lucene40RWPostingsFormat extends Lucene40PostingsFormat {
   
+  /** minimum items (terms or sub-blocks) per block for 4.0 BlockTree */
+  final static int MIN_BLOCK_SIZE = 25;
+  /** maximum items (terms or sub-blocks) per block for 4.0 BlockTree */
+  final static int MAX_BLOCK_SIZE = 48;
+
   @Override
   public FieldsConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
     PostingsWriterBase docs = new Lucene40PostingsWriter(state);
@@ -41,7 +46,7 @@
     // Or... you must make a new Codec for this?
     boolean success = false;
     try {
-      FieldsConsumer ret = new BlockTreeTermsWriter(state, docs, minBlockSize, maxBlockSize);
+      FieldsConsumer ret = new BlockTreeTermsWriter(state, docs, MIN_BLOCK_SIZE, MAX_BLOCK_SIZE);
       success = true;
       return ret;
     } finally {
Index: lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/Lucene40RWSegmentInfoFormat.java
===================================================================
--- lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/Lucene40RWSegmentInfoFormat.java	(revision 1629405)
+++ lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/Lucene40RWSegmentInfoFormat.java	(working copy)
@@ -19,8 +19,12 @@
 
 import org.apache.lucene.codecs.SegmentInfoWriter;
 
-/** read-write version of 4.0 segmentinfos for testing */
-public class Lucene40RWSegmentInfoFormat extends Lucene40SegmentInfoFormat {
+/**
+ * Read-write version of 4.0 segmentinfo format for testing
+ * @deprecated for test purposes only
+ */
+@Deprecated
+public final class Lucene40RWSegmentInfoFormat extends Lucene40SegmentInfoFormat {
 
   @Override
   public SegmentInfoWriter getSegmentInfoWriter() {
Index: lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/Lucene40RWStoredFieldsFormat.java
===================================================================
--- lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/Lucene40RWStoredFieldsFormat.java	(revision 1629405)
+++ lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/Lucene40RWStoredFieldsFormat.java	(working copy)
@@ -24,10 +24,12 @@
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.IOContext;
 
-/** 
- * Simulates writing Lucene 4.0 Stored Fields Format.
- */ 
-public class Lucene40RWStoredFieldsFormat extends Lucene40StoredFieldsFormat {
+/**
+ * Read-write version of 4.0 stored fields format for testing
+ * @deprecated for test purposes only
+ */
+@Deprecated
+final class Lucene40RWStoredFieldsFormat extends Lucene40StoredFieldsFormat {
 
   @Override
   public StoredFieldsWriter fieldsWriter(Directory directory, SegmentInfo si, IOContext context) throws IOException {
Index: lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/Lucene40RWTermVectorsFormat.java
===================================================================
--- lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/Lucene40RWTermVectorsFormat.java	(revision 1629405)
+++ lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/Lucene40RWTermVectorsFormat.java	(working copy)
@@ -23,12 +23,13 @@
 import org.apache.lucene.index.SegmentInfo;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.IOContext;
-import org.apache.lucene.util.LuceneTestCase;
 
-/** 
- * Simulates writing Lucene 4.0 Stored Fields Format.
- */ 
-public class Lucene40RWTermVectorsFormat extends Lucene40TermVectorsFormat {
+/**
+ * Read-write version of 4.0 term vectors format for testing
+ * @deprecated for test purposes only
+ */
+@Deprecated
+public final class Lucene40RWTermVectorsFormat extends Lucene40TermVectorsFormat {
 
   @Override
   public TermVectorsWriter vectorsWriter(Directory directory, SegmentInfo segmentInfo, IOContext context) throws IOException {
Index: lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/Lucene40SegmentInfoWriter.java
===================================================================
--- lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/Lucene40SegmentInfoWriter.java	(revision 1629405)
+++ lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/Lucene40SegmentInfoWriter.java	(working copy)
@@ -31,13 +31,11 @@
 import org.apache.lucene.util.IOUtils;
 
 /**
- * Lucene 4.0 implementation of {@link SegmentInfoWriter}.
- * 
- * @see Lucene40SegmentInfoFormat
- * @lucene.experimental
+ * writer for 4.0 segmentinfos for testing
+ * @deprecated for test purposes only
  */
 @Deprecated
-public class Lucene40SegmentInfoWriter extends SegmentInfoWriter {
+public final class Lucene40SegmentInfoWriter extends SegmentInfoWriter {
 
   /** Sole constructor. */
   public Lucene40SegmentInfoWriter() {
Index: lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/Lucene40SkipListWriter.java
===================================================================
--- lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/Lucene40SkipListWriter.java	(revision 1629405)
+++ lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/Lucene40SkipListWriter.java	(working copy)
@@ -25,14 +25,11 @@
 
 
 /**
- * Implements the skip list writer for the 4.0 posting list format
- * that stores positions and payloads.
- * 
- * @see Lucene40PostingsFormat
- * @deprecated Only for reading old 4.0 segments
+ * Writer of 4.0 skip lists for testing
+ * @deprecated for test purposes only
  */
 @Deprecated
-public class Lucene40SkipListWriter extends MultiLevelSkipListWriter {
+final class Lucene40SkipListWriter extends MultiLevelSkipListWriter {
   private int[] lastSkipDoc;
   private int[] lastSkipPayloadLength;
   private int[] lastSkipOffsetLength;
Index: lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/Lucene40StoredFieldsWriter.java
===================================================================
--- lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/Lucene40StoredFieldsWriter.java	(revision 1629405)
+++ lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/Lucene40StoredFieldsWriter.java	(working copy)
@@ -34,15 +34,12 @@
 import static org.apache.lucene.codecs.lucene40.Lucene40StoredFieldsReader.*;
 
 
-/** 
- * Class responsible for writing stored document fields.
- * <p/>
- * It uses &lt;segment&gt;.fdt and &lt;segment&gt;.fdx; files.
- * 
- * @see Lucene40StoredFieldsFormat
- * @lucene.experimental 
+/**
+ * Writer for 4.0 stored fields format for testing
+ * @deprecated for test purposes only
  */
-public final class Lucene40StoredFieldsWriter extends StoredFieldsWriter {
+@Deprecated
+final class Lucene40StoredFieldsWriter extends StoredFieldsWriter {
 
   private final Directory directory;
   private final String segment;
Index: lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/Lucene40TermVectorsWriter.java
===================================================================
--- lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/Lucene40TermVectorsWriter.java	(revision 1629405)
+++ lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/Lucene40TermVectorsWriter.java	(working copy)
@@ -36,24 +36,12 @@
 
 import static org.apache.lucene.codecs.lucene40.Lucene40TermVectorsReader.*;
 
-
-// TODO: make a new 4.0 TV format that encodes better
-//   - use startOffset (not endOffset) as base for delta on
-//     next startOffset because today for syns or ngrams or
-//     WDF or shingles etc. we are encoding negative vints
-//     (= slow, 5 bytes per)
-//   - if doc has no term vectors, write 0 into the tvx
-//     file; saves a seek to tvd only to read a 0 vint (and
-//     saves a byte in tvd)
-
 /**
- * Lucene 4.0 Term Vectors writer.
- * <p>
- * It writes .tvd, .tvf, and .tvx files.
- * 
- * @see Lucene40TermVectorsFormat
+ * Writer for 4.0 term vectors format for testing
+ * @deprecated for test purposes only
  */
-public final class Lucene40TermVectorsWriter extends TermVectorsWriter {
+@Deprecated
+final class Lucene40TermVectorsWriter extends TermVectorsWriter {
   private final Directory directory;
   private final String segment;
   private IndexOutput tvx = null, tvd = null, tvf = null;
Index: lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/TestBitVector.java
===================================================================
--- lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/TestBitVector.java	(revision 1629405)
+++ lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/TestBitVector.java	(working copy)
@@ -24,7 +24,6 @@
 import org.apache.lucene.store.RAMDirectory;
 import org.apache.lucene.util.LuceneTestCase;
 import org.apache.lucene.util.TestUtil;
-import org.apache.lucene.util.TestUtil;
 
 /**
  * <code>TestBitVector</code> tests the <code>BitVector</code>, obviously.
Index: lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/TestLucene40CompoundFormat.java
===================================================================
--- lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/TestLucene40CompoundFormat.java	(revision 0)
+++ lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/TestLucene40CompoundFormat.java	(working copy)
@@ -0,0 +1,157 @@
+package org.apache.lucene.codecs.lucene40;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.codecs.Codec;
+import org.apache.lucene.index.BaseCompoundFormatTestCase;
+import org.apache.lucene.store.MockDirectoryWrapper;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.store.IndexOutput;
+
+public class TestLucene40CompoundFormat extends BaseCompoundFormatTestCase {
+  private final Codec codec = new Lucene40RWCodec();
+
+  @Override
+  protected Codec getCodec() {
+    return codec;
+  }
+  
+  // LUCENE-3382 test that delegate compound files correctly.
+  public void testCompoundFileAppendTwice() throws IOException {
+    Directory newDir = newFSDirectory(createTempDir("testCompoundFileAppendTwice"));
+    Directory csw = new Lucene40CompoundReader(newDir, "d.cfs", newIOContext(random()), true);
+    createSequenceFile(newDir, "d1", (byte) 0, 15);
+    IndexOutput out = csw.createOutput("d.xyz", newIOContext(random()));
+    out.writeInt(0);
+    out.close();
+    assertEquals(1, csw.listAll().length);
+    assertEquals("d.xyz", csw.listAll()[0]);
+   
+    csw.close();
+
+    Directory cfr = new Lucene40CompoundReader(newDir, "d.cfs", newIOContext(random()), false);
+    assertEquals(1, cfr.listAll().length);
+    assertEquals("d.xyz", cfr.listAll()[0]);
+    cfr.close();
+    newDir.close();
+  }
+  
+  public void testReadNestedCFP() throws IOException {
+    Directory newDir = newDirectory();
+    // manually manipulates directory
+    if (newDir instanceof MockDirectoryWrapper) {
+      ((MockDirectoryWrapper)newDir).setEnableVirusScanner(false);
+    }
+    Lucene40CompoundReader csw = new Lucene40CompoundReader(newDir, "d.cfs", newIOContext(random()), true);
+    Lucene40CompoundReader nested = new Lucene40CompoundReader(newDir, "b.cfs", newIOContext(random()), true);
+    IndexOutput out = nested.createOutput("b.xyz", newIOContext(random()));
+    IndexOutput out1 = nested.createOutput("b_1.xyz", newIOContext(random()));
+    out.writeInt(0);
+    out1.writeInt(1);
+    out.close();
+    out1.close();
+    nested.close();
+    newDir.copy(csw, "b.cfs", "b.cfs", newIOContext(random()));
+    newDir.copy(csw, "b.cfe", "b.cfe", newIOContext(random()));
+    newDir.deleteFile("b.cfs");
+    newDir.deleteFile("b.cfe");
+    csw.close();
+    
+    assertEquals(2, newDir.listAll().length);
+    csw = new Lucene40CompoundReader(newDir, "d.cfs", newIOContext(random()), false);
+    
+    assertEquals(2, csw.listAll().length);
+    nested = new Lucene40CompoundReader(csw, "b.cfs", newIOContext(random()), false);
+    
+    assertEquals(2, nested.listAll().length);
+    IndexInput openInput = nested.openInput("b.xyz", newIOContext(random()));
+    assertEquals(0, openInput.readInt());
+    openInput.close();
+    openInput = nested.openInput("b_1.xyz", newIOContext(random()));
+    assertEquals(1, openInput.readInt());
+    openInput.close();
+    nested.close();
+    csw.close();
+    newDir.close();
+  }
+  
+  public void testAppend() throws IOException {
+    Directory dir = newDirectory();
+    Directory newDir = newDirectory();
+    Directory csw = new Lucene40CompoundReader(newDir, "d.cfs", newIOContext(random()), true);
+    int size = 5 + random().nextInt(128);
+    for (int j = 0; j < 2; j++) {
+      IndexOutput os = csw.createOutput("seg_" + j + "_foo.txt", newIOContext(random()));
+      for (int i = 0; i < size; i++) {
+        os.writeInt(i*j);
+      }
+      os.close();
+      String[] listAll = newDir.listAll();
+      assertEquals(1, listAll.length);
+      assertEquals("d.cfs", listAll[0]);
+    }
+    createSequenceFile(dir, "d1", (byte) 0, 15);
+    dir.copy(csw, "d1", "d1", newIOContext(random()));
+    String[] listAll = newDir.listAll();
+    assertEquals(1, listAll.length);
+    assertEquals("d.cfs", listAll[0]);
+    csw.close();
+    
+    Directory csr = new Lucene40CompoundReader(newDir, "d.cfs", newIOContext(random()), false);
+    for (int j = 0; j < 2; j++) {
+      IndexInput openInput = csr.openInput("seg_" + j + "_foo.txt", newIOContext(random()));
+      assertEquals(size * 4, openInput.length());
+      for (int i = 0; i < size; i++) {
+        assertEquals(i*j, openInput.readInt());
+      }
+      
+      openInput.close();
+    }
+    IndexInput expected = dir.openInput("d1", newIOContext(random()));
+    IndexInput actual = csr.openInput("d1", newIOContext(random()));
+    assertSameStreams("d1", expected, actual);
+    assertSameSeekBehavior("d1", expected, actual);
+    expected.close();
+    actual.close();
+    csr.close();
+    newDir.close();
+    dir.close();
+  }
+  
+  public void testAppendTwice() throws IOException {
+    Directory newDir = newDirectory();
+    Directory csw = new Lucene40CompoundReader(newDir, "d.cfs", newIOContext(random()), true);
+    createSequenceFile(newDir, "d1", (byte) 0, 15);
+    IndexOutput out = csw.createOutput("d.xyz", newIOContext(random()));
+    out.writeInt(0);
+    out.close();
+    assertEquals(1, csw.listAll().length);
+    assertEquals("d.xyz", csw.listAll()[0]);
+    
+    csw.close();
+    
+    Directory cfr = new Lucene40CompoundReader(newDir, "d.cfs", newIOContext(random()), false);
+    assertEquals(1, cfr.listAll().length);
+    assertEquals("d.xyz", cfr.listAll()[0]);
+    cfr.close();
+    newDir.close();
+  }
+}

Property changes on: lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/TestLucene40CompoundFormat.java
___________________________________________________________________
Added: svn:eol-style
## -0,0 +1 ##
+native
\ No newline at end of property
Index: lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/TestLucene40StoredFieldsFormat.java
===================================================================
--- lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/TestLucene40StoredFieldsFormat.java	(revision 1629405)
+++ lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/TestLucene40StoredFieldsFormat.java	(working copy)
@@ -19,7 +19,6 @@
 
 import org.apache.lucene.codecs.Codec;
 import org.apache.lucene.index.BaseStoredFieldsFormatTestCase;
-import org.junit.BeforeClass;
 
 public class TestLucene40StoredFieldsFormat extends BaseStoredFieldsFormatTestCase {
   
Index: lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/TestLucene40TermVectorsFormat.java
===================================================================
--- lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/TestLucene40TermVectorsFormat.java	(revision 1629405)
+++ lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/TestLucene40TermVectorsFormat.java	(working copy)
@@ -19,7 +19,6 @@
 
 import org.apache.lucene.codecs.Codec;
 import org.apache.lucene.index.BaseTermVectorsFormatTestCase;
-import org.junit.BeforeClass;
 
 public class TestLucene40TermVectorsFormat extends BaseTermVectorsFormatTestCase {
   
Index: lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene41/Lucene41RWCodec.java
===================================================================
--- lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene41/Lucene41RWCodec.java	(revision 1629405)
+++ lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene41/Lucene41RWCodec.java	(working copy)
@@ -15,7 +15,6 @@
 import org.apache.lucene.codecs.lucene40.Lucene40RWNormsFormat;
 import org.apache.lucene.codecs.lucene40.Lucene40RWSegmentInfoFormat;
 import org.apache.lucene.codecs.lucene40.Lucene40RWTermVectorsFormat;
-import org.apache.lucene.util.LuceneTestCase;
 
 /*
  * Licensed to the Apache Software Foundation (ASF) under one or more
@@ -35,11 +34,12 @@
  */
 
 /**
- * Read-write version of {@link Lucene41Codec} for testing.
+ * Read-write version of 4.1 codec for testing
+ * @deprecated for test purposes only
  */
-@SuppressWarnings("deprecation")
-public class Lucene41RWCodec extends Lucene41Codec {
-  private final StoredFieldsFormat fieldsFormat = new Lucene41StoredFieldsFormat();
+@Deprecated
+public final class Lucene41RWCodec extends Lucene41Codec {
+  private final StoredFieldsFormat fieldsFormat = new Lucene41RWStoredFieldsFormat();
   private final FieldInfosFormat fieldInfos = new Lucene40FieldInfosFormat() {
     @Override
     public FieldInfosWriter getFieldInfosWriter() throws IOException {
Index: lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene41/Lucene41RWStoredFieldsFormat.java
===================================================================
--- lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene41/Lucene41RWStoredFieldsFormat.java	(revision 0)
+++ lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene41/Lucene41RWStoredFieldsFormat.java	(working copy)
@@ -0,0 +1,37 @@
+package org.apache.lucene.codecs.lucene41;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.codecs.StoredFieldsWriter;
+import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+
+/**
+ * Read-write version of 4.1 stored fields format for testing
+ * @deprecated for test purposes only
+ */
+@Deprecated
+public final class Lucene41RWStoredFieldsFormat extends Lucene41StoredFieldsFormat {
+  @Override
+  public StoredFieldsWriter fieldsWriter(Directory directory, SegmentInfo si, IOContext context) throws IOException {
+    return new Lucene41StoredFieldsWriter(directory, si, SEGMENT_SUFFIX, context, FORMAT_NAME, COMPRESSION_MODE, CHUNK_SIZE);
+  }
+}

Property changes on: lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene41/Lucene41RWStoredFieldsFormat.java
___________________________________________________________________
Added: svn:eol-style
## -0,0 +1 ##
+native
\ No newline at end of property
Index: lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene41/Lucene41StoredFieldsIndexWriter.java
===================================================================
--- lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene41/Lucene41StoredFieldsIndexWriter.java	(revision 0)
+++ lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene41/Lucene41StoredFieldsIndexWriter.java	(working copy)
@@ -0,0 +1,171 @@
+package org.apache.lucene.codecs.lucene41;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import static org.apache.lucene.util.BitUtil.zigZagEncode;
+
+import java.io.Closeable;
+import java.io.IOException;
+
+import org.apache.lucene.codecs.CodecUtil;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.util.packed.PackedInts;
+
+/**
+ * Writer for 4.1 stored fields/term vectors index for testing
+ * @deprecated for test purposes only
+ */
+@Deprecated
+public final class Lucene41StoredFieldsIndexWriter implements Closeable {
+  
+  static final int BLOCK_SIZE = 1024; // number of chunks to serialize at once
+
+  final IndexOutput fieldsIndexOut;
+  int totalDocs;
+  int blockDocs;
+  int blockChunks;
+  long firstStartPointer;
+  long maxStartPointer;
+  final int[] docBaseDeltas;
+  final long[] startPointerDeltas;
+
+  public Lucene41StoredFieldsIndexWriter(IndexOutput indexOutput) throws IOException {
+    this.fieldsIndexOut = indexOutput;
+    reset();
+    totalDocs = 0;
+    docBaseDeltas = new int[BLOCK_SIZE];
+    startPointerDeltas = new long[BLOCK_SIZE];
+    fieldsIndexOut.writeVInt(PackedInts.VERSION_CURRENT);
+  }
+
+  private void reset() {
+    blockChunks = 0;
+    blockDocs = 0;
+    firstStartPointer = -1; // means unset
+  }
+
+  private void writeBlock() throws IOException {
+    assert blockChunks > 0;
+    fieldsIndexOut.writeVInt(blockChunks);
+
+    // The trick here is that we only store the difference from the average start
+    // pointer or doc base, this helps save bits per value.
+    // And in order to prevent a few chunks that would be far from the average to
+    // raise the number of bits per value for all of them, we only encode blocks
+    // of 1024 chunks at once
+    // See LUCENE-4512
+
+    // doc bases
+    final int avgChunkDocs;
+    if (blockChunks == 1) {
+      avgChunkDocs = 0;
+    } else {
+      avgChunkDocs = Math.round((float) (blockDocs - docBaseDeltas[blockChunks - 1]) / (blockChunks - 1));
+    }
+    fieldsIndexOut.writeVInt(totalDocs - blockDocs); // docBase
+    fieldsIndexOut.writeVInt(avgChunkDocs);
+    int docBase = 0;
+    long maxDelta = 0;
+    for (int i = 0; i < blockChunks; ++i) {
+      final int delta = docBase - avgChunkDocs * i;
+      maxDelta |= zigZagEncode(delta);
+      docBase += docBaseDeltas[i];
+    }
+
+    final int bitsPerDocBase = PackedInts.bitsRequired(maxDelta);
+    fieldsIndexOut.writeVInt(bitsPerDocBase);
+    PackedInts.Writer writer = PackedInts.getWriterNoHeader(fieldsIndexOut,
+        PackedInts.Format.PACKED, blockChunks, bitsPerDocBase, 1);
+    docBase = 0;
+    for (int i = 0; i < blockChunks; ++i) {
+      final long delta = docBase - avgChunkDocs * i;
+      assert PackedInts.bitsRequired(zigZagEncode(delta)) <= writer.bitsPerValue();
+      writer.add(zigZagEncode(delta));
+      docBase += docBaseDeltas[i];
+    }
+    writer.finish();
+
+    // start pointers
+    fieldsIndexOut.writeVLong(firstStartPointer);
+    final long avgChunkSize;
+    if (blockChunks == 1) {
+      avgChunkSize = 0;
+    } else {
+      avgChunkSize = (maxStartPointer - firstStartPointer) / (blockChunks - 1);
+    }
+    fieldsIndexOut.writeVLong(avgChunkSize);
+    long startPointer = 0;
+    maxDelta = 0;
+    for (int i = 0; i < blockChunks; ++i) {
+      startPointer += startPointerDeltas[i];
+      final long delta = startPointer - avgChunkSize * i;
+      maxDelta |= zigZagEncode(delta);
+    }
+
+    final int bitsPerStartPointer = PackedInts.bitsRequired(maxDelta);
+    fieldsIndexOut.writeVInt(bitsPerStartPointer);
+    writer = PackedInts.getWriterNoHeader(fieldsIndexOut, PackedInts.Format.PACKED,
+        blockChunks, bitsPerStartPointer, 1);
+    startPointer = 0;
+    for (int i = 0; i < blockChunks; ++i) {
+      startPointer += startPointerDeltas[i];
+      final long delta = startPointer - avgChunkSize * i;
+      assert PackedInts.bitsRequired(zigZagEncode(delta)) <= writer.bitsPerValue();
+      writer.add(zigZagEncode(delta));
+    }
+    writer.finish();
+  }
+
+  public void writeIndex(int numDocs, long startPointer) throws IOException {
+    if (blockChunks == BLOCK_SIZE) {
+      writeBlock();
+      reset();
+    }
+
+    if (firstStartPointer == -1) {
+      firstStartPointer = maxStartPointer = startPointer;
+    }
+    assert firstStartPointer > 0 && startPointer >= firstStartPointer;
+
+    docBaseDeltas[blockChunks] = numDocs;
+    startPointerDeltas[blockChunks] = startPointer - maxStartPointer;
+
+    ++blockChunks;
+    blockDocs += numDocs;
+    totalDocs += numDocs;
+    maxStartPointer = startPointer;
+  }
+
+  public void finish(int numDocs, long maxPointer) throws IOException {
+    if (numDocs != totalDocs) {
+      throw new IllegalStateException("Expected " + numDocs + " docs, but got " + totalDocs);
+    }
+    if (blockChunks > 0) {
+      writeBlock();
+    }
+    fieldsIndexOut.writeVInt(0); // end marker
+    fieldsIndexOut.writeVLong(maxPointer);
+    CodecUtil.writeFooter(fieldsIndexOut);
+  }
+
+  @Override
+  public void close() throws IOException {
+    fieldsIndexOut.close();
+  }
+
+}

Property changes on: lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene41/Lucene41StoredFieldsIndexWriter.java
___________________________________________________________________
Added: svn:eol-style
## -0,0 +1 ##
+native
\ No newline at end of property
Index: lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene41/Lucene41StoredFieldsWriter.java
===================================================================
--- lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene41/Lucene41StoredFieldsWriter.java	(revision 0)
+++ lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene41/Lucene41StoredFieldsWriter.java	(working copy)
@@ -0,0 +1,315 @@
+package org.apache.lucene.codecs.lucene41;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import static org.apache.lucene.codecs.lucene41.Lucene41StoredFieldsReader.BYTE_ARR;
+import static org.apache.lucene.codecs.lucene41.Lucene41StoredFieldsReader.CODEC_SFX_DAT;
+import static org.apache.lucene.codecs.lucene41.Lucene41StoredFieldsReader.CODEC_SFX_IDX;
+import static org.apache.lucene.codecs.lucene41.Lucene41StoredFieldsReader.FIELDS_EXTENSION;
+import static org.apache.lucene.codecs.lucene41.Lucene41StoredFieldsReader.FIELDS_INDEX_EXTENSION;
+import static org.apache.lucene.codecs.lucene41.Lucene41StoredFieldsReader.NUMERIC_DOUBLE;
+import static org.apache.lucene.codecs.lucene41.Lucene41StoredFieldsReader.NUMERIC_FLOAT;
+import static org.apache.lucene.codecs.lucene41.Lucene41StoredFieldsReader.NUMERIC_INT;
+import static org.apache.lucene.codecs.lucene41.Lucene41StoredFieldsReader.NUMERIC_LONG;
+import static org.apache.lucene.codecs.lucene41.Lucene41StoredFieldsReader.STRING;
+import static org.apache.lucene.codecs.lucene41.Lucene41StoredFieldsReader.TYPE_BITS;
+import static org.apache.lucene.codecs.lucene41.Lucene41StoredFieldsReader.VERSION_CURRENT;
+
+import java.io.IOException;
+import java.util.Arrays;
+
+import org.apache.lucene.codecs.CodecUtil;
+import org.apache.lucene.codecs.StoredFieldsWriter;
+import org.apache.lucene.codecs.compressing.CompressionMode;
+import org.apache.lucene.codecs.compressing.Compressor;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.index.StorableField;
+import org.apache.lucene.store.DataOutput;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.util.ArrayUtil;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.GrowableByteArrayDataOutput;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util.packed.PackedInts;
+
+/**
+ * Writer for 4.1 stored fields format for testing
+ * @deprecated for test purposes only
+ */
+@Deprecated
+final class Lucene41StoredFieldsWriter extends StoredFieldsWriter {
+
+  // hard limit on the maximum number of documents per chunk
+  static final int MAX_DOCUMENTS_PER_CHUNK = 128;
+
+  private final Directory directory;
+  private final String segment;
+  private final String segmentSuffix;
+  private Lucene41StoredFieldsIndexWriter indexWriter;
+  private IndexOutput fieldsStream;
+
+  private final Compressor compressor;
+  private final int chunkSize;
+
+  private final GrowableByteArrayDataOutput bufferedDocs;
+  private int[] numStoredFields; // number of stored fields
+  private int[] endOffsets; // end offsets in bufferedDocs
+  private int docBase; // doc ID at the beginning of the chunk
+  private int numBufferedDocs; // docBase + numBufferedDocs == current doc ID
+
+  /** Sole constructor. */
+  public Lucene41StoredFieldsWriter(Directory directory, SegmentInfo si, String segmentSuffix, IOContext context,
+      String formatName, CompressionMode compressionMode, int chunkSize) throws IOException {
+    assert directory != null;
+    this.directory = directory;
+    this.segment = si.name;
+    this.segmentSuffix = segmentSuffix;
+    this.compressor = compressionMode.newCompressor();
+    this.chunkSize = chunkSize;
+    this.docBase = 0;
+    this.bufferedDocs = new GrowableByteArrayDataOutput(chunkSize);
+    this.numStoredFields = new int[16];
+    this.endOffsets = new int[16];
+    this.numBufferedDocs = 0;
+
+    boolean success = false;
+    IndexOutput indexStream = directory.createOutput(IndexFileNames.segmentFileName(segment, segmentSuffix, FIELDS_INDEX_EXTENSION), 
+                                                                     context);
+    try {
+      fieldsStream = directory.createOutput(IndexFileNames.segmentFileName(segment, segmentSuffix, FIELDS_EXTENSION),
+                                                    context);
+
+      final String codecNameIdx = formatName + CODEC_SFX_IDX;
+      final String codecNameDat = formatName + CODEC_SFX_DAT;
+      CodecUtil.writeHeader(indexStream, codecNameIdx, VERSION_CURRENT);
+      CodecUtil.writeHeader(fieldsStream, codecNameDat, VERSION_CURRENT);
+      assert CodecUtil.headerLength(codecNameDat) == fieldsStream.getFilePointer();
+      assert CodecUtil.headerLength(codecNameIdx) == indexStream.getFilePointer();
+
+      indexWriter = new Lucene41StoredFieldsIndexWriter(indexStream);
+      indexStream = null;
+
+      fieldsStream.writeVInt(chunkSize);
+      fieldsStream.writeVInt(PackedInts.VERSION_CURRENT);
+
+      success = true;
+    } finally {
+      if (!success) {
+        IOUtils.closeWhileHandlingException(indexStream);
+        abort();
+      }
+    }
+  }
+
+  @Override
+  public void close() throws IOException {
+    try {
+      IOUtils.close(fieldsStream, indexWriter);
+    } finally {
+      fieldsStream = null;
+      indexWriter = null;
+    }
+  }
+
+  private int numStoredFieldsInDoc;
+
+  @Override
+  public void startDocument() throws IOException {
+  }
+
+  @Override
+  public void finishDocument() throws IOException {
+    if (numBufferedDocs == this.numStoredFields.length) {
+      final int newLength = ArrayUtil.oversize(numBufferedDocs + 1, 4);
+      this.numStoredFields = Arrays.copyOf(this.numStoredFields, newLength);
+      endOffsets = Arrays.copyOf(endOffsets, newLength);
+    }
+    this.numStoredFields[numBufferedDocs] = numStoredFieldsInDoc;
+    numStoredFieldsInDoc = 0;
+    endOffsets[numBufferedDocs] = bufferedDocs.length;
+    ++numBufferedDocs;
+    if (triggerFlush()) {
+      flush();
+    }
+  }
+
+  private static void saveInts(int[] values, int length, DataOutput out) throws IOException {
+    assert length > 0;
+    if (length == 1) {
+      out.writeVInt(values[0]);
+    } else {
+      boolean allEqual = true;
+      for (int i = 1; i < length; ++i) {
+        if (values[i] != values[0]) {
+          allEqual = false;
+          break;
+        }
+      }
+      if (allEqual) {
+        out.writeVInt(0);
+        out.writeVInt(values[0]);
+      } else {
+        long max = 0;
+        for (int i = 0; i < length; ++i) {
+          max |= values[i];
+        }
+        final int bitsRequired = PackedInts.bitsRequired(max);
+        out.writeVInt(bitsRequired);
+        final PackedInts.Writer w = PackedInts.getWriterNoHeader(out, PackedInts.Format.PACKED, length, bitsRequired, 1);
+        for (int i = 0; i < length; ++i) {
+          w.add(values[i]);
+        }
+        w.finish();
+      }
+    }
+  }
+
+  private void writeHeader(int docBase, int numBufferedDocs, int[] numStoredFields, int[] lengths) throws IOException {
+    // save docBase and numBufferedDocs
+    fieldsStream.writeVInt(docBase);
+    fieldsStream.writeVInt(numBufferedDocs);
+
+    // save numStoredFields
+    saveInts(numStoredFields, numBufferedDocs, fieldsStream);
+
+    // save lengths
+    saveInts(lengths, numBufferedDocs, fieldsStream);
+  }
+
+  private boolean triggerFlush() {
+    return bufferedDocs.length >= chunkSize || // chunks of at least chunkSize bytes
+        numBufferedDocs >= MAX_DOCUMENTS_PER_CHUNK;
+  }
+
+  private void flush() throws IOException {
+    indexWriter.writeIndex(numBufferedDocs, fieldsStream.getFilePointer());
+
+    // transform end offsets into lengths
+    final int[] lengths = endOffsets;
+    for (int i = numBufferedDocs - 1; i > 0; --i) {
+      lengths[i] = endOffsets[i] - endOffsets[i - 1];
+      assert lengths[i] >= 0;
+    }
+    writeHeader(docBase, numBufferedDocs, numStoredFields, lengths);
+
+    // compress stored fields to fieldsStream
+    if (bufferedDocs.length >= 2 * chunkSize) {
+      // big chunk, slice it
+      for (int compressed = 0; compressed < bufferedDocs.length; compressed += chunkSize) {
+        compressor.compress(bufferedDocs.bytes, compressed, Math.min(chunkSize, bufferedDocs.length - compressed), fieldsStream);
+      }
+    } else {
+      compressor.compress(bufferedDocs.bytes, 0, bufferedDocs.length, fieldsStream);
+    }
+
+    // reset
+    docBase += numBufferedDocs;
+    numBufferedDocs = 0;
+    bufferedDocs.length = 0;
+  }
+
+  @Override
+  public void writeField(FieldInfo info, StorableField field)
+      throws IOException {
+
+    ++numStoredFieldsInDoc;
+
+    int bits = 0;
+    final BytesRef bytes;
+    final String string;
+
+    Number number = field.numericValue();
+    if (number != null) {
+      if (number instanceof Byte || number instanceof Short || number instanceof Integer) {
+        bits = NUMERIC_INT;
+      } else if (number instanceof Long) {
+        bits = NUMERIC_LONG;
+      } else if (number instanceof Float) {
+        bits = NUMERIC_FLOAT;
+      } else if (number instanceof Double) {
+        bits = NUMERIC_DOUBLE;
+      } else {
+        throw new IllegalArgumentException("cannot store numeric type " + number.getClass());
+      }
+      string = null;
+      bytes = null;
+    } else {
+      bytes = field.binaryValue();
+      if (bytes != null) {
+        bits = BYTE_ARR;
+        string = null;
+      } else {
+        bits = STRING;
+        string = field.stringValue();
+        if (string == null) {
+          throw new IllegalArgumentException("field " + field.name() + " is stored but does not have binaryValue, stringValue nor numericValue");
+        }
+      }
+    }
+
+    final long infoAndBits = (((long) info.number) << TYPE_BITS) | bits;
+    bufferedDocs.writeVLong(infoAndBits);
+
+    if (bytes != null) {
+      bufferedDocs.writeVInt(bytes.length);
+      bufferedDocs.writeBytes(bytes.bytes, bytes.offset, bytes.length);
+    } else if (string != null) {
+      bufferedDocs.writeString(field.stringValue());
+    } else {
+      if (number instanceof Byte || number instanceof Short || number instanceof Integer) {
+        bufferedDocs.writeInt(number.intValue());
+      } else if (number instanceof Long) {
+        bufferedDocs.writeLong(number.longValue());
+      } else if (number instanceof Float) {
+        bufferedDocs.writeInt(Float.floatToIntBits(number.floatValue()));
+      } else if (number instanceof Double) {
+        bufferedDocs.writeLong(Double.doubleToLongBits(number.doubleValue()));
+      } else {
+        throw new AssertionError("Cannot get here");
+      }
+    }
+  }
+
+  @Override
+  public void abort() {
+    IOUtils.closeWhileHandlingException(this);
+    IOUtils.deleteFilesIgnoringExceptions(directory,
+        IndexFileNames.segmentFileName(segment, segmentSuffix, FIELDS_EXTENSION),
+        IndexFileNames.segmentFileName(segment, segmentSuffix, FIELDS_INDEX_EXTENSION));
+  }
+
+  @Override
+  public void finish(FieldInfos fis, int numDocs) throws IOException {
+    if (numBufferedDocs > 0) {
+      flush();
+    } else {
+      assert bufferedDocs.length == 0;
+    }
+    if (docBase != numDocs) {
+      throw new RuntimeException("Wrote " + docBase + " docs, finish called with numDocs=" + numDocs);
+    }
+    indexWriter.finish(numDocs, fieldsStream.getFilePointer());
+    CodecUtil.writeFooter(fieldsStream);
+    assert bufferedDocs.length == 0;
+  }
+}

Property changes on: lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene41/Lucene41StoredFieldsWriter.java
___________________________________________________________________
Added: svn:eol-style
## -0,0 +1 ##
+native
\ No newline at end of property
Index: lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene41/TestLucene41StoredFieldsFormat.java
===================================================================
--- lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene41/TestLucene41StoredFieldsFormat.java	(revision 0)
+++ lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene41/TestLucene41StoredFieldsFormat.java	(working copy)
@@ -0,0 +1,28 @@
+package org.apache.lucene.codecs.lucene41;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.codecs.Codec;
+import org.apache.lucene.index.BaseStoredFieldsFormatTestCase;
+
+public class TestLucene41StoredFieldsFormat extends BaseStoredFieldsFormatTestCase {
+  @Override
+  protected Codec getCodec() {
+    return new Lucene41RWCodec();
+  }
+}

Property changes on: lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene41/TestLucene41StoredFieldsFormat.java
___________________________________________________________________
Added: svn:eol-style
## -0,0 +1 ##
+native
\ No newline at end of property
Index: lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene42/Lucene42DocValuesConsumer.java
===================================================================
--- lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene42/Lucene42DocValuesConsumer.java	(revision 1629405)
+++ lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene42/Lucene42DocValuesConsumer.java	(working copy)
@@ -34,7 +34,6 @@
 import org.apache.lucene.util.ArrayUtil;
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.IntsRef;
 import org.apache.lucene.util.IntsRefBuilder;
 import org.apache.lucene.util.MathUtil;
 import org.apache.lucene.util.fst.Builder;
@@ -58,9 +57,11 @@
 import static org.apache.lucene.codecs.lucene42.Lucene42DocValuesProducer.UNCOMPRESSED;
 
 /**
- * Writer for {@link Lucene42DocValuesFormat}
+ * Writer for 4.2 docvalues format for testing
+ * @deprecated for test purposes only
  */
-class Lucene42DocValuesConsumer extends DocValuesConsumer {
+@Deprecated
+final class Lucene42DocValuesConsumer extends DocValuesConsumer {
   final IndexOutput data, meta;
   final int maxDoc;
   final float acceptableOverheadRatio;
Index: lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene42/Lucene42FieldInfosWriter.java
===================================================================
--- lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene42/Lucene42FieldInfosWriter.java	(revision 1629405)
+++ lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene42/Lucene42FieldInfosWriter.java	(working copy)
@@ -33,10 +33,8 @@
 import org.apache.lucene.util.IOUtils;
 
 /**
- * Lucene 4.2 FieldInfos writer.
- * 
- * @see Lucene42FieldInfosFormat
- * @lucene.experimental
+ * Writer for 4.2 fieldinfos format for testing
+ * @deprecated for test purposes only
  */
 @Deprecated
 public final class Lucene42FieldInfosWriter extends FieldInfosWriter {
Index: lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene42/Lucene42NormsConsumer.java
===================================================================
--- lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene42/Lucene42NormsConsumer.java	(revision 1629405)
+++ lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene42/Lucene42NormsConsumer.java	(working copy)
@@ -36,9 +36,11 @@
 import static org.apache.lucene.codecs.lucene42.Lucene42DocValuesProducer.VERSION_CURRENT;
 
 /**
- * Writer for {@link Lucene42NormsFormat}
+ * Writer for 4.2 norms format for testing
+ * @deprecated for test purposes only
  */
-class Lucene42NormsConsumer extends NormsConsumer { 
+@Deprecated
+final class Lucene42NormsConsumer extends NormsConsumer { 
   static final byte NUMBER = 0;
 
   static final int BLOCK_SIZE = 4096;
Index: lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene42/Lucene42RWCodec.java
===================================================================
--- lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene42/Lucene42RWCodec.java	(revision 1629405)
+++ lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene42/Lucene42RWCodec.java	(working copy)
@@ -24,17 +24,21 @@
 import org.apache.lucene.codecs.FieldInfosWriter;
 import org.apache.lucene.codecs.NormsFormat;
 import org.apache.lucene.codecs.SegmentInfoFormat;
+import org.apache.lucene.codecs.StoredFieldsFormat;
+import org.apache.lucene.codecs.TermVectorsFormat;
 import org.apache.lucene.codecs.lucene40.Lucene40RWSegmentInfoFormat;
-import org.apache.lucene.util.LuceneTestCase;
+import org.apache.lucene.codecs.lucene41.Lucene41RWStoredFieldsFormat;
 
 /**
- * Read-write version of {@link Lucene42Codec} for testing.
+ * Read-Write version of 4.2 codec for testing
+ * @deprecated for test purposes only
  */
-@SuppressWarnings("deprecation")
-public class Lucene42RWCodec extends Lucene42Codec {
+@Deprecated
+public final class Lucene42RWCodec extends Lucene42Codec {
 
   private static final DocValuesFormat dv = new Lucene42RWDocValuesFormat();
   private static final NormsFormat norms = new Lucene42RWNormsFormat();
+  private static final StoredFieldsFormat storedFields = new Lucene41RWStoredFieldsFormat();
 
   private final FieldInfosFormat fieldInfosFormat = new Lucene42FieldInfosFormat() {
     @Override
@@ -64,4 +68,16 @@
   public SegmentInfoFormat segmentInfoFormat() {
     return segmentInfos;
   }
+
+  @Override
+  public StoredFieldsFormat storedFieldsFormat() {
+    return storedFields;
+  }
+  
+  private final TermVectorsFormat vectorsFormat = new Lucene42RWTermVectorsFormat();
+
+  @Override
+  public TermVectorsFormat termVectorsFormat() {
+    return vectorsFormat;
+  }
 }
Index: lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene42/Lucene42RWDocValuesFormat.java
===================================================================
--- lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene42/Lucene42RWDocValuesFormat.java	(revision 1629405)
+++ lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene42/Lucene42RWDocValuesFormat.java	(working copy)
@@ -21,13 +21,13 @@
 
 import org.apache.lucene.codecs.DocValuesConsumer;
 import org.apache.lucene.index.SegmentWriteState;
-import org.apache.lucene.util.LuceneTestCase;
 
 /**
- * Read-write version of {@link Lucene42DocValuesFormat} for testing.
+ * Read-Write version of 4.2 docvalues format for testing
+ * @deprecated for test purposes only
  */
-@SuppressWarnings("deprecation")
-public class Lucene42RWDocValuesFormat extends Lucene42DocValuesFormat {
+@Deprecated
+public final class Lucene42RWDocValuesFormat extends Lucene42DocValuesFormat {
   
   @Override
   public DocValuesConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
Index: lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene42/Lucene42RWNormsFormat.java
===================================================================
--- lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene42/Lucene42RWNormsFormat.java	(revision 1629405)
+++ lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene42/Lucene42RWNormsFormat.java	(working copy)
@@ -21,12 +21,13 @@
 
 import org.apache.lucene.codecs.NormsConsumer;
 import org.apache.lucene.index.SegmentWriteState;
-import org.apache.lucene.util.LuceneTestCase;
 
 /**
- * Read-write version of {@link Lucene42NormsFormat}
+ * Read-write version of 4.2 norms format for testing
+ * @deprecated for test purposes only
  */
-public class Lucene42RWNormsFormat extends Lucene42NormsFormat {
+@Deprecated
+public final class Lucene42RWNormsFormat extends Lucene42NormsFormat {
 
   @Override
   public NormsConsumer normsConsumer(SegmentWriteState state) throws IOException {
Index: lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene42/Lucene42RWTermVectorsFormat.java
===================================================================
--- lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene42/Lucene42RWTermVectorsFormat.java	(revision 0)
+++ lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene42/Lucene42RWTermVectorsFormat.java	(working copy)
@@ -0,0 +1,38 @@
+package org.apache.lucene.codecs.lucene42;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.codecs.TermVectorsWriter;
+import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+
+/**
+ * Read-Write version of 4.2 term vectors format for testing
+ * @deprecated for test purposes only
+ */
+@Deprecated
+public final class Lucene42RWTermVectorsFormat extends Lucene42TermVectorsFormat {
+
+  @Override
+  public TermVectorsWriter vectorsWriter(Directory directory, SegmentInfo segmentInfo, IOContext context) throws IOException {
+    return new Lucene42TermVectorsWriter(directory, segmentInfo, SEGMENT_SUFFIX, context, FORMAT_NAME, COMPRESSION_MODE, CHUNK_SIZE);
+  }
+}

Property changes on: lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene42/Lucene42RWTermVectorsFormat.java
___________________________________________________________________
Added: svn:eol-style
## -0,0 +1 ##
+native
\ No newline at end of property
Index: lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene42/Lucene42TermVectorsWriter.java
===================================================================
--- lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene42/Lucene42TermVectorsWriter.java	(revision 0)
+++ lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene42/Lucene42TermVectorsWriter.java	(working copy)
@@ -0,0 +1,714 @@
+package org.apache.lucene.codecs.lucene42;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import static org.apache.lucene.codecs.lucene42.Lucene42TermVectorsReader.BLOCK_SIZE;
+import static org.apache.lucene.codecs.lucene42.Lucene42TermVectorsReader.CODEC_SFX_DAT;
+import static org.apache.lucene.codecs.lucene42.Lucene42TermVectorsReader.CODEC_SFX_IDX;
+import static org.apache.lucene.codecs.lucene42.Lucene42TermVectorsReader.FLAGS_BITS;
+import static org.apache.lucene.codecs.lucene42.Lucene42TermVectorsReader.OFFSETS;
+import static org.apache.lucene.codecs.lucene42.Lucene42TermVectorsReader.PAYLOADS;
+import static org.apache.lucene.codecs.lucene42.Lucene42TermVectorsReader.POSITIONS;
+import static org.apache.lucene.codecs.lucene42.Lucene42TermVectorsReader.VECTORS_EXTENSION;
+import static org.apache.lucene.codecs.lucene42.Lucene42TermVectorsReader.VECTORS_INDEX_EXTENSION;
+import static org.apache.lucene.codecs.lucene42.Lucene42TermVectorsReader.VERSION_CURRENT;
+
+import java.io.IOException;
+import java.util.ArrayDeque;
+import java.util.Arrays;
+import java.util.Deque;
+import java.util.Iterator;
+import java.util.SortedSet;
+import java.util.TreeSet;
+
+import org.apache.lucene.codecs.CodecUtil;
+import org.apache.lucene.codecs.TermVectorsWriter;
+import org.apache.lucene.codecs.compressing.CompressionMode;
+import org.apache.lucene.codecs.compressing.Compressor;
+import org.apache.lucene.codecs.lucene41.Lucene41StoredFieldsIndexWriter;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.store.DataInput;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.util.ArrayUtil;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.GrowableByteArrayDataOutput;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util.StringHelper;
+import org.apache.lucene.util.packed.BlockPackedWriter;
+import org.apache.lucene.util.packed.PackedInts;
+
+/**
+ * Writer for 4.2 term vectors format for testing
+ * @deprecated for test purposes only
+ */
+@Deprecated
+final class Lucene42TermVectorsWriter extends TermVectorsWriter {
+
+  // hard limit on the maximum number of documents per chunk
+  static final int MAX_DOCUMENTS_PER_CHUNK = 128;
+
+  private final Directory directory;
+  private final String segment;
+  private final String segmentSuffix;
+  private Lucene41StoredFieldsIndexWriter indexWriter;
+  private IndexOutput vectorsStream;
+
+  private final Compressor compressor;
+  private final int chunkSize;
+
+  /** a pending doc */
+  private class DocData {
+    final int numFields;
+    final Deque<FieldData> fields;
+    final int posStart, offStart, payStart;
+    DocData(int numFields, int posStart, int offStart, int payStart) {
+      this.numFields = numFields;
+      this.fields = new ArrayDeque<>(numFields);
+      this.posStart = posStart;
+      this.offStart = offStart;
+      this.payStart = payStart;
+    }
+    FieldData addField(int fieldNum, int numTerms, boolean positions, boolean offsets, boolean payloads) {
+      final FieldData field;
+      if (fields.isEmpty()) {
+        field = new FieldData(fieldNum, numTerms, positions, offsets, payloads, posStart, offStart, payStart);
+      } else {
+        final FieldData last = fields.getLast();
+        final int posStart = last.posStart + (last.hasPositions ? last.totalPositions : 0);
+        final int offStart = last.offStart + (last.hasOffsets ? last.totalPositions : 0);
+        final int payStart = last.payStart + (last.hasPayloads ? last.totalPositions : 0);
+        field = new FieldData(fieldNum, numTerms, positions, offsets, payloads, posStart, offStart, payStart);
+      }
+      fields.add(field);
+      return field;
+    }
+  }
+
+  private DocData addDocData(int numVectorFields) {
+    FieldData last = null;
+    for (Iterator<DocData> it = pendingDocs.descendingIterator(); it.hasNext(); ) {
+      final DocData doc = it.next();
+      if (!doc.fields.isEmpty()) {
+        last = doc.fields.getLast();
+        break;
+      }
+    }
+    final DocData doc;
+    if (last == null) {
+      doc = new DocData(numVectorFields, 0, 0, 0);
+    } else {
+      final int posStart = last.posStart + (last.hasPositions ? last.totalPositions : 0);
+      final int offStart = last.offStart + (last.hasOffsets ? last.totalPositions : 0);
+      final int payStart = last.payStart + (last.hasPayloads ? last.totalPositions : 0);
+      doc = new DocData(numVectorFields, posStart, offStart, payStart);
+    }
+    pendingDocs.add(doc);
+    return doc;
+  }
+
+  /** a pending field */
+  private class FieldData {
+    final boolean hasPositions, hasOffsets, hasPayloads;
+    final int fieldNum, flags, numTerms;
+    final int[] freqs, prefixLengths, suffixLengths;
+    final int posStart, offStart, payStart;
+    int totalPositions;
+    int ord;
+    FieldData(int fieldNum, int numTerms, boolean positions, boolean offsets, boolean payloads,
+        int posStart, int offStart, int payStart) {
+      this.fieldNum = fieldNum;
+      this.numTerms = numTerms;
+      this.hasPositions = positions;
+      this.hasOffsets = offsets;
+      this.hasPayloads = payloads;
+      this.flags = (positions ? POSITIONS : 0) | (offsets ? OFFSETS : 0) | (payloads ? PAYLOADS : 0);
+      this.freqs = new int[numTerms];
+      this.prefixLengths = new int[numTerms];
+      this.suffixLengths = new int[numTerms];
+      this.posStart = posStart;
+      this.offStart = offStart;
+      this.payStart = payStart;
+      totalPositions = 0;
+      ord = 0;
+    }
+    void addTerm(int freq, int prefixLength, int suffixLength) {
+      freqs[ord] = freq;
+      prefixLengths[ord] = prefixLength;
+      suffixLengths[ord] = suffixLength;
+      ++ord;
+    }
+    void addPosition(int position, int startOffset, int length, int payloadLength) {
+      if (hasPositions) {
+        if (posStart + totalPositions == positionsBuf.length) {
+          positionsBuf = ArrayUtil.grow(positionsBuf);
+        }
+        positionsBuf[posStart + totalPositions] = position;
+      }
+      if (hasOffsets) {
+        if (offStart + totalPositions == startOffsetsBuf.length) {
+          final int newLength = ArrayUtil.oversize(offStart + totalPositions, 4);
+          startOffsetsBuf = Arrays.copyOf(startOffsetsBuf, newLength);
+          lengthsBuf = Arrays.copyOf(lengthsBuf, newLength);
+        }
+        startOffsetsBuf[offStart + totalPositions] = startOffset;
+        lengthsBuf[offStart + totalPositions] = length;
+      }
+      if (hasPayloads) {
+        if (payStart + totalPositions == payloadLengthsBuf.length) {
+          payloadLengthsBuf = ArrayUtil.grow(payloadLengthsBuf);
+        }
+        payloadLengthsBuf[payStart + totalPositions] = payloadLength;
+      }
+      ++totalPositions;
+    }
+  }
+
+  private int numDocs; // total number of docs seen
+  private final Deque<DocData> pendingDocs; // pending docs
+  private DocData curDoc; // current document
+  private FieldData curField; // current field
+  private final BytesRef lastTerm;
+  private int[] positionsBuf, startOffsetsBuf, lengthsBuf, payloadLengthsBuf;
+  private final GrowableByteArrayDataOutput termSuffixes; // buffered term suffixes
+  private final GrowableByteArrayDataOutput payloadBytes; // buffered term payloads
+  private final BlockPackedWriter writer;
+
+  /** Sole constructor. */
+  public Lucene42TermVectorsWriter(Directory directory, SegmentInfo si, String segmentSuffix, IOContext context,
+      String formatName, CompressionMode compressionMode, int chunkSize) throws IOException {
+    assert directory != null;
+    this.directory = directory;
+    this.segment = si.name;
+    this.segmentSuffix = segmentSuffix;
+    this.compressor = compressionMode.newCompressor();
+    this.chunkSize = chunkSize;
+
+    numDocs = 0;
+    pendingDocs = new ArrayDeque<>();
+    termSuffixes = new GrowableByteArrayDataOutput(ArrayUtil.oversize(chunkSize, 1));
+    payloadBytes = new GrowableByteArrayDataOutput(ArrayUtil.oversize(1, 1));
+    lastTerm = new BytesRef(ArrayUtil.oversize(30, 1));
+
+    boolean success = false;
+    IndexOutput indexStream = directory.createOutput(IndexFileNames.segmentFileName(segment, segmentSuffix, VECTORS_INDEX_EXTENSION), 
+                                                                     context);
+    try {
+      vectorsStream = directory.createOutput(IndexFileNames.segmentFileName(segment, segmentSuffix, VECTORS_EXTENSION),
+                                                     context);
+
+      final String codecNameIdx = formatName + CODEC_SFX_IDX;
+      final String codecNameDat = formatName + CODEC_SFX_DAT;
+      CodecUtil.writeHeader(indexStream, codecNameIdx, VERSION_CURRENT);
+      CodecUtil.writeHeader(vectorsStream, codecNameDat, VERSION_CURRENT);
+      assert CodecUtil.headerLength(codecNameDat) == vectorsStream.getFilePointer();
+      assert CodecUtil.headerLength(codecNameIdx) == indexStream.getFilePointer();
+
+      indexWriter = new Lucene41StoredFieldsIndexWriter(indexStream);
+      indexStream = null;
+
+      vectorsStream.writeVInt(PackedInts.VERSION_CURRENT);
+      vectorsStream.writeVInt(chunkSize);
+      writer = new BlockPackedWriter(vectorsStream, BLOCK_SIZE);
+
+      positionsBuf = new int[1024];
+      startOffsetsBuf = new int[1024];
+      lengthsBuf = new int[1024];
+      payloadLengthsBuf = new int[1024];
+
+      success = true;
+    } finally {
+      if (!success) {
+        IOUtils.closeWhileHandlingException(indexStream);
+        abort();
+      }
+    }
+  }
+
+  @Override
+  public void close() throws IOException {
+    try {
+      IOUtils.close(vectorsStream, indexWriter);
+    } finally {
+      vectorsStream = null;
+      indexWriter = null;
+    }
+  }
+
+  @Override
+  public void abort() {
+    IOUtils.closeWhileHandlingException(this);
+    IOUtils.deleteFilesIgnoringExceptions(directory,
+        IndexFileNames.segmentFileName(segment, segmentSuffix, VECTORS_EXTENSION),
+        IndexFileNames.segmentFileName(segment, segmentSuffix, VECTORS_INDEX_EXTENSION));
+  }
+
+  @Override
+  public void startDocument(int numVectorFields) throws IOException {
+    curDoc = addDocData(numVectorFields);
+  }
+
+  @Override
+  public void finishDocument() throws IOException {
+    // append the payload bytes of the doc after its terms
+    termSuffixes.writeBytes(payloadBytes.bytes, payloadBytes.length);
+    payloadBytes.length = 0;
+    ++numDocs;
+    if (triggerFlush()) {
+      flush();
+    }
+    curDoc = null;
+  }
+
+  @Override
+  public void startField(FieldInfo info, int numTerms, boolean positions,
+      boolean offsets, boolean payloads) throws IOException {
+    curField = curDoc.addField(info.number, numTerms, positions, offsets, payloads);
+    lastTerm.length = 0;
+  }
+
+  @Override
+  public void finishField() throws IOException {
+    curField = null;
+  }
+
+  @Override
+  public void startTerm(BytesRef term, int freq) throws IOException {
+    assert freq >= 1;
+    final int prefix = StringHelper.bytesDifference(lastTerm, term);
+    curField.addTerm(freq, prefix, term.length - prefix);
+    termSuffixes.writeBytes(term.bytes, term.offset + prefix, term.length - prefix);
+    // copy last term
+    if (lastTerm.bytes.length < term.length) {
+      lastTerm.bytes = new byte[ArrayUtil.oversize(term.length, 1)];
+    }
+    lastTerm.offset = 0;
+    lastTerm.length = term.length;
+    System.arraycopy(term.bytes, term.offset, lastTerm.bytes, 0, term.length);
+  }
+
+  @Override
+  public void addPosition(int position, int startOffset, int endOffset,
+      BytesRef payload) throws IOException {
+    assert curField.flags != 0;
+    curField.addPosition(position, startOffset, endOffset - startOffset, payload == null ? 0 : payload.length);
+    if (curField.hasPayloads && payload != null) {
+      payloadBytes.writeBytes(payload.bytes, payload.offset, payload.length);
+    }
+  }
+
+  private boolean triggerFlush() {
+    return termSuffixes.length >= chunkSize
+        || pendingDocs.size() >= MAX_DOCUMENTS_PER_CHUNK;
+  }
+
+  private void flush() throws IOException {
+    final int chunkDocs = pendingDocs.size();
+    assert chunkDocs > 0 : chunkDocs;
+
+    // write the index file
+    indexWriter.writeIndex(chunkDocs, vectorsStream.getFilePointer());
+
+    final int docBase = numDocs - chunkDocs;
+    vectorsStream.writeVInt(docBase);
+    vectorsStream.writeVInt(chunkDocs);
+
+    // total number of fields of the chunk
+    final int totalFields = flushNumFields(chunkDocs);
+
+    if (totalFields > 0) {
+      // unique field numbers (sorted)
+      final int[] fieldNums = flushFieldNums();
+      // offsets in the array of unique field numbers
+      flushFields(totalFields, fieldNums);
+      // flags (does the field have positions, offsets, payloads?)
+      flushFlags(totalFields, fieldNums);
+      // number of terms of each field
+      flushNumTerms(totalFields);
+      // prefix and suffix lengths for each field
+      flushTermLengths();
+      // term freqs - 1 (because termFreq is always >=1) for each term
+      flushTermFreqs();
+      // positions for all terms, when enabled
+      flushPositions();
+      // offsets for all terms, when enabled
+      flushOffsets(fieldNums);
+      // payload lengths for all terms, when enabled
+      flushPayloadLengths();
+
+      // compress terms and payloads and write them to the output
+      compressor.compress(termSuffixes.bytes, 0, termSuffixes.length, vectorsStream);
+    }
+
+    // reset
+    pendingDocs.clear();
+    curDoc = null;
+    curField = null;
+    termSuffixes.length = 0;
+  }
+
+  private int flushNumFields(int chunkDocs) throws IOException {
+    if (chunkDocs == 1) {
+      final int numFields = pendingDocs.getFirst().numFields;
+      vectorsStream.writeVInt(numFields);
+      return numFields;
+    } else {
+      writer.reset(vectorsStream);
+      int totalFields = 0;
+      for (DocData dd : pendingDocs) {
+        writer.add(dd.numFields);
+        totalFields += dd.numFields;
+      }
+      writer.finish();
+      return totalFields;
+    }
+  }
+
+  /** Returns a sorted array containing unique field numbers */
+  private int[] flushFieldNums() throws IOException {
+    SortedSet<Integer> fieldNums = new TreeSet<>();
+    for (DocData dd : pendingDocs) {
+      for (FieldData fd : dd.fields) {
+        fieldNums.add(fd.fieldNum);
+      }
+    }
+
+    final int numDistinctFields = fieldNums.size();
+    assert numDistinctFields > 0;
+    final int bitsRequired = PackedInts.bitsRequired(fieldNums.last());
+    final int token = (Math.min(numDistinctFields - 1, 0x07) << 5) | bitsRequired;
+    vectorsStream.writeByte((byte) token);
+    if (numDistinctFields - 1 >= 0x07) {
+      vectorsStream.writeVInt(numDistinctFields - 1 - 0x07);
+    }
+    final PackedInts.Writer writer = PackedInts.getWriterNoHeader(vectorsStream, PackedInts.Format.PACKED, fieldNums.size(), bitsRequired, 1);
+    for (Integer fieldNum : fieldNums) {
+      writer.add(fieldNum);
+    }
+    writer.finish();
+
+    int[] fns = new int[fieldNums.size()];
+    int i = 0;
+    for (Integer key : fieldNums) {
+      fns[i++] = key;
+    }
+    return fns;
+  }
+
+  private void flushFields(int totalFields, int[] fieldNums) throws IOException {
+    final PackedInts.Writer writer = PackedInts.getWriterNoHeader(vectorsStream, PackedInts.Format.PACKED, totalFields, PackedInts.bitsRequired(fieldNums.length - 1), 1);
+    for (DocData dd : pendingDocs) {
+      for (FieldData fd : dd.fields) {
+        final int fieldNumIndex = Arrays.binarySearch(fieldNums, fd.fieldNum);
+        assert fieldNumIndex >= 0;
+        writer.add(fieldNumIndex);
+      }
+    }
+    writer.finish();
+  }
+
+  private void flushFlags(int totalFields, int[] fieldNums) throws IOException {
+    // check if fields always have the same flags
+    boolean nonChangingFlags = true;
+    int[] fieldFlags = new int[fieldNums.length];
+    Arrays.fill(fieldFlags, -1);
+    outer:
+    for (DocData dd : pendingDocs) {
+      for (FieldData fd : dd.fields) {
+        final int fieldNumOff = Arrays.binarySearch(fieldNums, fd.fieldNum);
+        assert fieldNumOff >= 0;
+        if (fieldFlags[fieldNumOff] == -1) {
+          fieldFlags[fieldNumOff] = fd.flags;
+        } else if (fieldFlags[fieldNumOff] != fd.flags) {
+          nonChangingFlags = false;
+          break outer;
+        }
+      }
+    }
+
+    if (nonChangingFlags) {
+      // write one flag per field num
+      vectorsStream.writeVInt(0);
+      final PackedInts.Writer writer = PackedInts.getWriterNoHeader(vectorsStream, PackedInts.Format.PACKED, fieldFlags.length, FLAGS_BITS, 1);
+      for (int flags : fieldFlags) {
+        assert flags >= 0;
+        writer.add(flags);
+      }
+      assert writer.ord() == fieldFlags.length - 1;
+      writer.finish();
+    } else {
+      // write one flag for every field instance
+      vectorsStream.writeVInt(1);
+      final PackedInts.Writer writer = PackedInts.getWriterNoHeader(vectorsStream, PackedInts.Format.PACKED, totalFields, FLAGS_BITS, 1);
+      for (DocData dd : pendingDocs) {
+        for (FieldData fd : dd.fields) {
+          writer.add(fd.flags);
+        }
+      }
+      assert writer.ord() == totalFields - 1;
+      writer.finish();
+    }
+  }
+
+  private void flushNumTerms(int totalFields) throws IOException {
+    int maxNumTerms = 0;
+    for (DocData dd : pendingDocs) {
+      for (FieldData fd : dd.fields) {
+        maxNumTerms |= fd.numTerms;
+      }
+    }
+    final int bitsRequired = PackedInts.bitsRequired(maxNumTerms);
+    vectorsStream.writeVInt(bitsRequired);
+    final PackedInts.Writer writer = PackedInts.getWriterNoHeader(
+        vectorsStream, PackedInts.Format.PACKED, totalFields, bitsRequired, 1);
+    for (DocData dd : pendingDocs) {
+      for (FieldData fd : dd.fields) {
+        writer.add(fd.numTerms);
+      }
+    }
+    assert writer.ord() == totalFields - 1;
+    writer.finish();
+  }
+
+  private void flushTermLengths() throws IOException {
+    writer.reset(vectorsStream);
+    for (DocData dd : pendingDocs) {
+      for (FieldData fd : dd.fields) {
+        for (int i = 0; i < fd.numTerms; ++i) {
+          writer.add(fd.prefixLengths[i]);
+        }
+      }
+    }
+    writer.finish();
+    writer.reset(vectorsStream);
+    for (DocData dd : pendingDocs) {
+      for (FieldData fd : dd.fields) {
+        for (int i = 0; i < fd.numTerms; ++i) {
+          writer.add(fd.suffixLengths[i]);
+        }
+      }
+    }
+    writer.finish();
+  }
+
+  private void flushTermFreqs() throws IOException {
+    writer.reset(vectorsStream);
+    for (DocData dd : pendingDocs) {
+      for (FieldData fd : dd.fields) {
+        for (int i = 0; i < fd.numTerms; ++i) {
+          writer.add(fd.freqs[i] - 1);
+        }
+      }
+    }
+    writer.finish();
+  }
+
+  private void flushPositions() throws IOException {
+    writer.reset(vectorsStream);
+    for (DocData dd : pendingDocs) {
+      for (FieldData fd : dd.fields) {
+        if (fd.hasPositions) {
+          int pos = 0;
+          for (int i = 0; i < fd.numTerms; ++i) {
+            int previousPosition = 0;
+            for (int j = 0; j < fd.freqs[i]; ++j) {
+              final int position = positionsBuf[fd .posStart + pos++];
+              writer.add(position - previousPosition);
+              previousPosition = position;
+            }
+          }
+          assert pos == fd.totalPositions;
+        }
+      }
+    }
+    writer.finish();
+  }
+
+  private void flushOffsets(int[] fieldNums) throws IOException {
+    boolean hasOffsets = false;
+    long[] sumPos = new long[fieldNums.length];
+    long[] sumOffsets = new long[fieldNums.length];
+    for (DocData dd : pendingDocs) {
+      for (FieldData fd : dd.fields) {
+        hasOffsets |= fd.hasOffsets;
+        if (fd.hasOffsets && fd.hasPositions) {
+          final int fieldNumOff = Arrays.binarySearch(fieldNums, fd.fieldNum);
+          int pos = 0;
+          for (int i = 0; i < fd.numTerms; ++i) {
+            int previousPos = 0;
+            int previousOff = 0;
+            for (int j = 0; j < fd.freqs[i]; ++j) {
+              final int position = positionsBuf[fd.posStart + pos];
+              final int startOffset = startOffsetsBuf[fd.offStart + pos];
+              sumPos[fieldNumOff] += position - previousPos;
+              sumOffsets[fieldNumOff] += startOffset - previousOff;
+              previousPos = position;
+              previousOff = startOffset;
+              ++pos;
+            }
+          }
+          assert pos == fd.totalPositions;
+        }
+      }
+    }
+
+    if (!hasOffsets) {
+      // nothing to do
+      return;
+    }
+
+    final float[] charsPerTerm = new float[fieldNums.length];
+    for (int i = 0; i < fieldNums.length; ++i) {
+      charsPerTerm[i] = (sumPos[i] <= 0 || sumOffsets[i] <= 0) ? 0 : (float) ((double) sumOffsets[i] / sumPos[i]);
+    }
+
+    // start offsets
+    for (int i = 0; i < fieldNums.length; ++i) {
+      vectorsStream.writeInt(Float.floatToRawIntBits(charsPerTerm[i]));
+    }
+
+    writer.reset(vectorsStream);
+    for (DocData dd : pendingDocs) {
+      for (FieldData fd : dd.fields) {
+        if ((fd.flags & OFFSETS) != 0) {
+          final int fieldNumOff = Arrays.binarySearch(fieldNums, fd.fieldNum);
+          final float cpt = charsPerTerm[fieldNumOff];
+          int pos = 0;
+          for (int i = 0; i < fd.numTerms; ++i) {
+            int previousPos = 0;
+            int previousOff = 0;
+            for (int j = 0; j < fd.freqs[i]; ++j) {
+              final int position = fd.hasPositions ? positionsBuf[fd.posStart + pos] : 0;
+              final int startOffset = startOffsetsBuf[fd.offStart + pos];
+              writer.add(startOffset - previousOff - (int) (cpt * (position - previousPos)));
+              previousPos = position;
+              previousOff = startOffset;
+              ++pos;
+            }
+          }
+        }
+      }
+    }
+    writer.finish();
+
+    // lengths
+    writer.reset(vectorsStream);
+    for (DocData dd : pendingDocs) {
+      for (FieldData fd : dd.fields) {
+        if ((fd.flags & OFFSETS) != 0) {
+          int pos = 0;
+          for (int i = 0; i < fd.numTerms; ++i) {
+            for (int j = 0; j < fd.freqs[i]; ++j) {
+              writer.add(lengthsBuf[fd.offStart + pos++] - fd.prefixLengths[i] - fd.suffixLengths[i]);
+            }
+          }
+          assert pos == fd.totalPositions;
+        }
+      }
+    }
+    writer.finish();
+  }
+
+  private void flushPayloadLengths() throws IOException {
+    writer.reset(vectorsStream);
+    for (DocData dd : pendingDocs) {
+      for (FieldData fd : dd.fields) {
+        if (fd.hasPayloads) {
+          for (int i = 0; i < fd.totalPositions; ++i) {
+            writer.add(payloadLengthsBuf[fd.payStart + i]);
+          }
+        }
+      }
+    }
+    writer.finish();
+  }
+
+  @Override
+  public void finish(FieldInfos fis, int numDocs) throws IOException {
+    if (!pendingDocs.isEmpty()) {
+      flush();
+    }
+    if (numDocs != this.numDocs) {
+      throw new RuntimeException("Wrote " + this.numDocs + " docs, finish called with numDocs=" + numDocs);
+    }
+    indexWriter.finish(numDocs, vectorsStream.getFilePointer());
+    CodecUtil.writeFooter(vectorsStream);
+  }
+
+  @Override
+  public void addProx(int numProx, DataInput positions, DataInput offsets)
+      throws IOException {
+    assert (curField.hasPositions) == (positions != null);
+    assert (curField.hasOffsets) == (offsets != null);
+
+    if (curField.hasPositions) {
+      final int posStart = curField.posStart + curField.totalPositions;
+      if (posStart + numProx > positionsBuf.length) {
+        positionsBuf = ArrayUtil.grow(positionsBuf, posStart + numProx);
+      }
+      int position = 0;
+      if (curField.hasPayloads) {
+        final int payStart = curField.payStart + curField.totalPositions;
+        if (payStart + numProx > payloadLengthsBuf.length) {
+          payloadLengthsBuf = ArrayUtil.grow(payloadLengthsBuf, payStart + numProx);
+        }
+        for (int i = 0; i < numProx; ++i) {
+          final int code = positions.readVInt();
+          if ((code & 1) != 0) {
+            // This position has a payload
+            final int payloadLength = positions.readVInt();
+            payloadLengthsBuf[payStart + i] = payloadLength;
+            payloadBytes.copyBytes(positions, payloadLength);
+          } else {
+            payloadLengthsBuf[payStart + i] = 0;
+          }
+          position += code >>> 1;
+          positionsBuf[posStart + i] = position;
+        }
+      } else {
+        for (int i = 0; i < numProx; ++i) {
+          position += (positions.readVInt() >>> 1);
+          positionsBuf[posStart + i] = position;
+        }
+      }
+    }
+
+    if (curField.hasOffsets) {
+      final int offStart = curField.offStart + curField.totalPositions;
+      if (offStart + numProx > startOffsetsBuf.length) {
+        final int newLength = ArrayUtil.oversize(offStart + numProx, 4);
+        startOffsetsBuf = Arrays.copyOf(startOffsetsBuf, newLength);
+        lengthsBuf = Arrays.copyOf(lengthsBuf, newLength);
+      }
+      int lastOffset = 0, startOffset, endOffset;
+      for (int i = 0; i < numProx; ++i) {
+        startOffset = lastOffset + offsets.readVInt();
+        endOffset = startOffset + offsets.readVInt();
+        lastOffset = endOffset;
+        startOffsetsBuf[offStart + i] = startOffset;
+        lengthsBuf[offStart + i] = endOffset - startOffset;
+      }
+    }
+
+    curField.totalPositions += numProx;
+  }
+}

Property changes on: lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene42/Lucene42TermVectorsWriter.java
___________________________________________________________________
Added: svn:eol-style
## -0,0 +1 ##
+native
\ No newline at end of property
Index: lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene42/TestLucene42TermVectorsFormat.java
===================================================================
--- lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene42/TestLucene42TermVectorsFormat.java	(revision 0)
+++ lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene42/TestLucene42TermVectorsFormat.java	(working copy)
@@ -0,0 +1,28 @@
+package org.apache.lucene.codecs.lucene42;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.codecs.Codec;
+import org.apache.lucene.index.BaseTermVectorsFormatTestCase;
+
+public class TestLucene42TermVectorsFormat extends BaseTermVectorsFormatTestCase {
+  @Override
+  protected Codec getCodec() {
+    return new Lucene42RWCodec();
+  }
+}

Property changes on: lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene42/TestLucene42TermVectorsFormat.java
___________________________________________________________________
Added: svn:eol-style
## -0,0 +1 ##
+native
\ No newline at end of property
Index: lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene45/Lucene45RWCodec.java
===================================================================
--- lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene45/Lucene45RWCodec.java	(revision 1629405)
+++ lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene45/Lucene45RWCodec.java	(working copy)
@@ -24,16 +24,20 @@
 import org.apache.lucene.codecs.FieldInfosWriter;
 import org.apache.lucene.codecs.NormsFormat;
 import org.apache.lucene.codecs.SegmentInfoFormat;
+import org.apache.lucene.codecs.StoredFieldsFormat;
+import org.apache.lucene.codecs.TermVectorsFormat;
 import org.apache.lucene.codecs.lucene40.Lucene40RWSegmentInfoFormat;
+import org.apache.lucene.codecs.lucene41.Lucene41RWStoredFieldsFormat;
 import org.apache.lucene.codecs.lucene42.Lucene42FieldInfosFormat;
 import org.apache.lucene.codecs.lucene42.Lucene42FieldInfosWriter;
 import org.apache.lucene.codecs.lucene42.Lucene42RWNormsFormat;
+import org.apache.lucene.codecs.lucene42.Lucene42RWTermVectorsFormat;
 
 /**
  * Read-write version of {@link Lucene45Codec} for testing.
  */
 @SuppressWarnings("deprecation")
-public class Lucene45RWCodec extends Lucene45Codec {
+public final class Lucene45RWCodec extends Lucene45Codec {
   
   private final FieldInfosFormat fieldInfosFormat = new Lucene42FieldInfosFormat() {
     @Override
@@ -67,4 +71,18 @@
   public SegmentInfoFormat segmentInfoFormat() {
     return segmentInfos;
   }
+  
+  private static final StoredFieldsFormat storedFields = new Lucene41RWStoredFieldsFormat();
+
+  @Override
+  public StoredFieldsFormat storedFieldsFormat() {
+    return storedFields;
+  }
+  
+  private final TermVectorsFormat vectorsFormat = new Lucene42RWTermVectorsFormat();
+
+  @Override
+  public TermVectorsFormat termVectorsFormat() {
+    return vectorsFormat;
+  }
 }
Index: lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene45/Lucene45RWDocValuesFormat.java
===================================================================
--- lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene45/Lucene45RWDocValuesFormat.java	(revision 1629405)
+++ lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene45/Lucene45RWDocValuesFormat.java	(working copy)
@@ -24,9 +24,11 @@
 import org.apache.lucene.index.SegmentWriteState;
 
 /**
- * Read-write version of {@link Lucene45DocValuesFormat} for testing.
+ * Read-write version of 4.5 docvalues format for testing
+ * @deprecated for test purposes only
  */
-public class Lucene45RWDocValuesFormat extends Lucene45DocValuesFormat {
+@Deprecated
+public final class Lucene45RWDocValuesFormat extends Lucene45DocValuesFormat {
 
   @Override
   public DocValuesConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
Index: lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene45/TestLucene45DocValuesFormat.java
===================================================================
--- lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene45/TestLucene45DocValuesFormat.java	(revision 1629405)
+++ lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene45/TestLucene45DocValuesFormat.java	(working copy)
@@ -19,7 +19,6 @@
 
 import org.apache.lucene.codecs.Codec;
 import org.apache.lucene.index.BaseCompressingDocValuesFormatTestCase;
-import org.junit.BeforeClass;
 
 /**
  * Tests Lucene45DocValuesFormat
Index: lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene46/Lucene46RWCodec.java
===================================================================
--- lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene46/Lucene46RWCodec.java	(revision 1629405)
+++ lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene46/Lucene46RWCodec.java	(working copy)
@@ -20,15 +20,19 @@
 import org.apache.lucene.codecs.DocValuesFormat;
 import org.apache.lucene.codecs.NormsFormat;
 import org.apache.lucene.codecs.SegmentInfoFormat;
-import org.apache.lucene.codecs.SegmentInfoWriter;
+import org.apache.lucene.codecs.StoredFieldsFormat;
+import org.apache.lucene.codecs.TermVectorsFormat;
+import org.apache.lucene.codecs.lucene41.Lucene41RWStoredFieldsFormat;
 import org.apache.lucene.codecs.lucene42.Lucene42RWNormsFormat;
+import org.apache.lucene.codecs.lucene42.Lucene42RWTermVectorsFormat;
 import org.apache.lucene.codecs.lucene45.Lucene45RWDocValuesFormat;
 
 /**
- * Read-write version of {@link Lucene46Codec} for testing.
+ * Read-write version of 4.6 codec for testing
+ * @deprecated for test purposes only
  */
-@SuppressWarnings("deprecation")
-public class Lucene46RWCodec extends Lucene46Codec {
+@Deprecated
+public final class Lucene46RWCodec extends Lucene46Codec {
   
   private static final DocValuesFormat docValues = new Lucene45RWDocValuesFormat();
   
@@ -50,4 +54,18 @@
   public SegmentInfoFormat segmentInfoFormat() {
     return segmentInfos;
   }
+  
+  private static final StoredFieldsFormat storedFields = new Lucene41RWStoredFieldsFormat();
+
+  @Override
+  public StoredFieldsFormat storedFieldsFormat() {
+    return storedFields;
+  }
+  
+  private final TermVectorsFormat vectorsFormat = new Lucene42RWTermVectorsFormat();
+
+  @Override
+  public TermVectorsFormat termVectorsFormat() {
+    return vectorsFormat;
+  }
 }
Index: lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene46/Lucene46RWSegmentInfoFormat.java
===================================================================
--- lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene46/Lucene46RWSegmentInfoFormat.java	(revision 1629405)
+++ lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene46/Lucene46RWSegmentInfoFormat.java	(working copy)
@@ -19,8 +19,12 @@
 
 import org.apache.lucene.codecs.SegmentInfoWriter;
 
-/** read-write version of 4.6 segmentinfos for testing */
-public class Lucene46RWSegmentInfoFormat extends Lucene46SegmentInfoFormat {
+/**
+ * Read-Write version of 4.6 segmentinfo format for testing
+ * @deprecated for test purposes only
+ */
+@Deprecated
+public final class Lucene46RWSegmentInfoFormat extends Lucene46SegmentInfoFormat {
   @Override
   public SegmentInfoWriter getSegmentInfoWriter() {
     return new Lucene46SegmentInfoWriter();
Index: lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene46/Lucene46SegmentInfoWriter.java
===================================================================
--- lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene46/Lucene46SegmentInfoWriter.java	(revision 1629405)
+++ lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene46/Lucene46SegmentInfoWriter.java	(working copy)
@@ -31,12 +31,11 @@
 import org.apache.lucene.util.Version;
 
 /**
- * Lucene 4.0 implementation of {@link SegmentInfoWriter}.
- * 
- * @see Lucene46SegmentInfoFormat
- * @lucene.experimental
+ * Writer for 4.0 segmentinfo format for testing
+ * @deprecated for test purposes only
  */
-public class Lucene46SegmentInfoWriter extends SegmentInfoWriter {
+@Deprecated
+final class Lucene46SegmentInfoWriter extends SegmentInfoWriter {
 
   /** Sole constructor. */
   public Lucene46SegmentInfoWriter() {
Index: lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene49/Lucene49NormsConsumer.java
===================================================================
--- lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene49/Lucene49NormsConsumer.java	(revision 0)
+++ lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene49/Lucene49NormsConsumer.java	(working copy)
@@ -0,0 +1,251 @@
+package org.apache.lucene.codecs.lucene49;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import static org.apache.lucene.codecs.lucene49.Lucene49NormsFormat.VERSION_CURRENT;
+import static org.apache.lucene.codecs.lucene49.Lucene49NormsProducer.BLOCK_SIZE;
+import static org.apache.lucene.codecs.lucene49.Lucene49NormsProducer.CONST_COMPRESSED;
+import static org.apache.lucene.codecs.lucene49.Lucene49NormsProducer.DELTA_COMPRESSED;
+import static org.apache.lucene.codecs.lucene49.Lucene49NormsProducer.TABLE_COMPRESSED;
+import static org.apache.lucene.codecs.lucene49.Lucene49NormsProducer.UNCOMPRESSED;
+
+import java.io.IOException;
+import java.util.Arrays;
+import java.util.HashMap;
+import java.util.Map;
+
+import org.apache.lucene.codecs.CodecUtil;
+import org.apache.lucene.codecs.NormsConsumer;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.SegmentWriteState;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util.packed.BlockPackedWriter;
+import org.apache.lucene.util.packed.PackedInts;
+
+/**
+ * Writer for 4.9 norms
+ * @deprecated for test purposes only
+ */
+@Deprecated
+final class Lucene49NormsConsumer extends NormsConsumer { 
+  IndexOutput data, meta;
+  final int maxDoc;
+  
+  Lucene49NormsConsumer(SegmentWriteState state, String dataCodec, String dataExtension, String metaCodec, String metaExtension) throws IOException {
+    maxDoc = state.segmentInfo.getDocCount();
+    boolean success = false;
+    try {
+      String dataName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, dataExtension);
+      data = state.directory.createOutput(dataName, state.context);
+      CodecUtil.writeHeader(data, dataCodec, VERSION_CURRENT);
+      String metaName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, metaExtension);
+      meta = state.directory.createOutput(metaName, state.context);
+      CodecUtil.writeHeader(meta, metaCodec, VERSION_CURRENT);
+      success = true;
+    } finally {
+      if (!success) {
+        IOUtils.closeWhileHandlingException(this);
+      }
+    }
+  }
+  
+  // we explicitly use only certain bits per value and a specified format, so we statically check this will work
+  static {
+    assert PackedInts.Format.PACKED_SINGLE_BLOCK.isSupported(1);
+    assert PackedInts.Format.PACKED_SINGLE_BLOCK.isSupported(2);
+    assert PackedInts.Format.PACKED_SINGLE_BLOCK.isSupported(4);
+  }
+
+  @Override
+  public void addNormsField(FieldInfo field, Iterable<Number> values) throws IOException {
+    meta.writeVInt(field.number);
+    long minValue = Long.MAX_VALUE;
+    long maxValue = Long.MIN_VALUE;
+    // TODO: more efficient?
+    NormMap uniqueValues = new NormMap();
+    
+    long count = 0;
+    for (Number nv : values) {
+      if (nv == null) {
+        throw new IllegalStateException("illegal norms data for field " + field.name + ", got null for value: " + count);
+      }
+      final long v = nv.longValue();
+      
+      minValue = Math.min(minValue, v);
+      maxValue = Math.max(maxValue, v);
+      
+      if (uniqueValues != null) {
+        if (uniqueValues.add(v)) {
+          if (uniqueValues.size > 256) {
+            uniqueValues = null;
+          }
+        }
+      }
+      ++count;
+    }
+    
+    if (count != maxDoc) {
+      throw new IllegalStateException("illegal norms data for field " + field.name + ", expected " + maxDoc + " values, got " + count);
+    }
+    
+    if (uniqueValues != null && uniqueValues.size == 1) {
+      // 0 bpv
+      meta.writeByte(CONST_COMPRESSED);
+      meta.writeLong(minValue);
+    } else if (uniqueValues != null) {
+      // small number of unique values: this is the typical case:
+      // we only use bpv=1,2,4,8     
+      PackedInts.Format format = PackedInts.Format.PACKED_SINGLE_BLOCK;
+      int bitsPerValue = PackedInts.bitsRequired(uniqueValues.size-1);
+      if (bitsPerValue == 3) {
+        bitsPerValue = 4;
+      } else if (bitsPerValue > 4) {
+        bitsPerValue = 8;
+      }
+      
+      if (bitsPerValue == 8 && minValue >= Byte.MIN_VALUE && maxValue <= Byte.MAX_VALUE) {
+        meta.writeByte(UNCOMPRESSED); // uncompressed byte[]
+        meta.writeLong(data.getFilePointer());
+        for (Number nv : values) {
+          data.writeByte(nv == null ? 0 : (byte) nv.longValue());
+        }
+      } else {
+        meta.writeByte(TABLE_COMPRESSED); // table-compressed
+        meta.writeLong(data.getFilePointer());
+        data.writeVInt(PackedInts.VERSION_CURRENT);
+        
+        long[] decode = uniqueValues.getDecodeTable();
+        // upgrade to power of two sized array
+        int size = 1 << bitsPerValue;
+        data.writeVInt(size);
+        for (int i = 0; i < decode.length; i++) {
+          data.writeLong(decode[i]);
+        }
+        for (int i = decode.length; i < size; i++) {
+          data.writeLong(0);
+        }
+
+        data.writeVInt(format.getId());
+        data.writeVInt(bitsPerValue);
+
+        final PackedInts.Writer writer = PackedInts.getWriterNoHeader(data, format, maxDoc, bitsPerValue, PackedInts.DEFAULT_BUFFER_SIZE);
+        for(Number nv : values) {
+          writer.add(uniqueValues.getOrd(nv.longValue()));
+        }
+        writer.finish();
+      }
+    } else {
+      meta.writeByte(DELTA_COMPRESSED); // delta-compressed
+      meta.writeLong(data.getFilePointer());
+      data.writeVInt(PackedInts.VERSION_CURRENT);
+      data.writeVInt(BLOCK_SIZE);
+
+      final BlockPackedWriter writer = new BlockPackedWriter(data, BLOCK_SIZE);
+      for (Number nv : values) {
+        writer.add(nv.longValue());
+      }
+      writer.finish();
+    }
+  }
+  
+  @Override
+  public void close() throws IOException {
+    boolean success = false;
+    try {
+      if (meta != null) {
+        meta.writeVInt(-1); // write EOF marker
+        CodecUtil.writeFooter(meta); // write checksum
+      }
+      if (data != null) {
+        CodecUtil.writeFooter(data); // write checksum
+      }
+      success = true;
+    } finally {
+      if (success) {
+        IOUtils.close(data, meta);
+      } else {
+        IOUtils.closeWhileHandlingException(data, meta);
+      }
+      meta = data = null;
+    }
+  }
+  
+  // specialized deduplication of long->ord for norms: 99.99999% of the time this will be a single-byte range.
+  static class NormMap {
+    // we use short: at most we will add 257 values to this map before its rejected as too big above.
+    final short[] singleByteRange = new short[256];
+    final Map<Long,Short> other = new HashMap<Long,Short>();
+    int size;
+    
+    {
+      Arrays.fill(singleByteRange, (short)-1);
+    }
+
+    /** adds an item to the mapping. returns true if actually added */
+    public boolean add(long l) {
+      assert size <= 256; // once we add > 256 values, we nullify the map in addNumericField and don't use this strategy
+      if (l >= Byte.MIN_VALUE && l <= Byte.MAX_VALUE) {
+        int index = (int) (l + 128);
+        short previous = singleByteRange[index];
+        if (previous < 0) {
+          singleByteRange[index] = (short) size;
+          size++;
+          return true;
+        } else {
+          return false;
+        }
+      } else {
+        if (!other.containsKey(l)) {
+          other.put(l, (short)size);
+          size++;
+          return true;
+        } else {
+          return false;
+        }
+      }
+    }
+    
+    /** gets the ordinal for a previously added item */
+    public int getOrd(long l) {
+      if (l >= Byte.MIN_VALUE && l <= Byte.MAX_VALUE) {
+        int index = (int) (l + 128);
+        return singleByteRange[index];
+      } else {
+        // NPE if something is screwed up
+        return other.get(l);
+      }
+    }
+    
+    /** retrieves the ordinal table for previously added items */
+    public long[] getDecodeTable() {
+      long decode[] = new long[size];
+      for (int i = 0; i < singleByteRange.length; i++) {
+        short s = singleByteRange[i];
+        if (s >= 0) {
+          decode[s] = i - 128;
+        }
+      }
+      for (Map.Entry<Long,Short> entry : other.entrySet()) {
+        decode[entry.getValue()] = entry.getKey();
+      }
+      return decode;
+    }
+  }
+}

Property changes on: lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene49/Lucene49NormsConsumer.java
___________________________________________________________________
Added: svn:eol-style
## -0,0 +1 ##
+native
\ No newline at end of property
Index: lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene49/Lucene49RWCodec.java
===================================================================
--- lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene49/Lucene49RWCodec.java	(revision 1629405)
+++ lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene49/Lucene49RWCodec.java	(working copy)
@@ -20,13 +20,18 @@
 import org.apache.lucene.codecs.DocValuesFormat;
 import org.apache.lucene.codecs.NormsFormat;
 import org.apache.lucene.codecs.SegmentInfoFormat;
+import org.apache.lucene.codecs.StoredFieldsFormat;
+import org.apache.lucene.codecs.TermVectorsFormat;
+import org.apache.lucene.codecs.lucene41.Lucene41RWStoredFieldsFormat;
+import org.apache.lucene.codecs.lucene42.Lucene42RWTermVectorsFormat;
 import org.apache.lucene.codecs.lucene46.Lucene46RWSegmentInfoFormat;
 
 /**
- * Read-write version of {@link Lucene49Codec} for testing.
+ * Read-Write version of 4.9 codec for testing
+ * @deprecated for test purposes only
  */
-@SuppressWarnings("deprecation")
-public class Lucene49RWCodec extends Lucene49Codec {
+@Deprecated
+public final class Lucene49RWCodec extends Lucene49Codec {
   
   private static final DocValuesFormat docValues = new Lucene49RWDocValuesFormat();
   
@@ -35,7 +40,7 @@
     return docValues;
   }
   
-  private static final NormsFormat norms = new Lucene49NormsFormat();
+  private static final NormsFormat norms = new Lucene49RWNormsFormat();
 
   @Override
   public NormsFormat normsFormat() {
@@ -48,4 +53,18 @@
   public SegmentInfoFormat segmentInfoFormat() {
     return segmentInfos;
   }
+  
+  private static final StoredFieldsFormat storedFields = new Lucene41RWStoredFieldsFormat();
+
+  @Override
+  public StoredFieldsFormat storedFieldsFormat() {
+    return storedFields;
+  }
+  
+  private final TermVectorsFormat vectorsFormat = new Lucene42RWTermVectorsFormat();
+
+  @Override
+  public TermVectorsFormat termVectorsFormat() {
+    return vectorsFormat;
+  }
 }
Index: lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene49/Lucene49RWDocValuesFormat.java
===================================================================
--- lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene49/Lucene49RWDocValuesFormat.java	(revision 1629405)
+++ lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene49/Lucene49RWDocValuesFormat.java	(working copy)
@@ -22,10 +22,13 @@
 import org.apache.lucene.codecs.DocValuesConsumer;
 import org.apache.lucene.index.FieldInfo;
 import org.apache.lucene.index.SegmentWriteState;
-import org.apache.lucene.util.LuceneTestCase;
 
-/** Read-write version of {@link Lucene49DocValuesFormat} for testing */
-public class Lucene49RWDocValuesFormat extends Lucene49DocValuesFormat {
+/**
+ * Read-Write version of 4.9 docvalues format for testing
+ * @deprecated for test purposes only
+ */
+@Deprecated
+public final class Lucene49RWDocValuesFormat extends Lucene49DocValuesFormat {
 
   @Override
   public DocValuesConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
Index: lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene49/Lucene49RWNormsFormat.java
===================================================================
--- lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene49/Lucene49RWNormsFormat.java	(revision 0)
+++ lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene49/Lucene49RWNormsFormat.java	(working copy)
@@ -0,0 +1,35 @@
+package org.apache.lucene.codecs.lucene49;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.codecs.NormsConsumer;
+import org.apache.lucene.index.SegmentWriteState;
+
+/**
+ * Read-Write version of 4.9 norms format for testing
+ * @deprecated for test purposes only
+ */
+@Deprecated
+public class Lucene49RWNormsFormat extends Lucene49NormsFormat {
+  @Override
+  public NormsConsumer normsConsumer(SegmentWriteState state) throws IOException {
+    return new Lucene49NormsConsumer(state, DATA_CODEC, DATA_EXTENSION, METADATA_CODEC, METADATA_EXTENSION);
+  }
+}

Property changes on: lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene49/Lucene49RWNormsFormat.java
___________________________________________________________________
Added: svn:eol-style
## -0,0 +1 ##
+native
\ No newline at end of property
Index: lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene49/TestLucene49DocValuesFormat.java
===================================================================
--- lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene49/TestLucene49DocValuesFormat.java	(revision 1629405)
+++ lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene49/TestLucene49DocValuesFormat.java	(working copy)
@@ -19,7 +19,6 @@
 
 import org.apache.lucene.codecs.Codec;
 import org.apache.lucene.index.BaseCompressingDocValuesFormatTestCase;
-import org.junit.BeforeClass;
 
 /**
  * Tests Lucene49DocValuesFormat
Index: lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene49/TestLucene49NormsFormat.java
===================================================================
--- lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene49/TestLucene49NormsFormat.java	(revision 0)
+++ lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene49/TestLucene49NormsFormat.java	(working copy)
@@ -0,0 +1,33 @@
+package org.apache.lucene.codecs.lucene49;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.codecs.Codec;
+import org.apache.lucene.index.BaseNormsFormatTestCase;
+
+/**
+ * Tests Lucene49NormsFormat
+ */
+public class TestLucene49NormsFormat extends BaseNormsFormatTestCase {
+  private final Codec codec = new Lucene49RWCodec();
+  
+  @Override
+  protected Codec getCodec() {
+    return codec;
+  } 
+}

Property changes on: lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene49/TestLucene49NormsFormat.java
___________________________________________________________________
Added: svn:eol-style
## -0,0 +1 ##
+native
\ No newline at end of property
Index: lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/BlockTermsReader.java
===================================================================
--- lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/BlockTermsReader.java	(revision 1629405)
+++ lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/BlockTermsReader.java	(working copy)
@@ -33,15 +33,12 @@
 import org.apache.lucene.index.DocsEnum;
 import org.apache.lucene.index.FieldInfo.IndexOptions;
 import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.FieldInfos;
 import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.index.SegmentReadState;
 import org.apache.lucene.index.TermState;
 import org.apache.lucene.index.Terms;
 import org.apache.lucene.index.TermsEnum;
 import org.apache.lucene.store.ByteArrayDataInput;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
 import org.apache.lucene.store.IndexInput;
 import org.apache.lucene.util.Accountable;
 import org.apache.lucene.util.Accountables;
@@ -77,12 +74,7 @@
 
   // Reads the terms index
   private TermsIndexReaderBase indexReader;
-
-  // keeps the dirStart offset
-  private long dirOffset;
   
-  private final int version; 
-
   // Used as key for the terms cache
   private static class FieldAndTerm extends DoubleBarrelLRUCache.CloneableKey {
     String field;
@@ -113,35 +105,31 @@
     }
   }
   
-  // private String segment;
-  
-  public BlockTermsReader(TermsIndexReaderBase indexReader, Directory dir, FieldInfos fieldInfos, SegmentInfo info, PostingsReaderBase postingsReader, IOContext context,
-                          String segmentSuffix)
-    throws IOException {
+  public BlockTermsReader(TermsIndexReaderBase indexReader, PostingsReaderBase postingsReader, SegmentReadState state) throws IOException {
     
     this.postingsReader = postingsReader;
+    
+    String filename = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, BlockTermsWriter.TERMS_EXTENSION);
+    in = state.directory.openInput(filename, state.context);
 
-    // this.segment = segment;
-    in = dir.openInput(IndexFileNames.segmentFileName(info.name, segmentSuffix, BlockTermsWriter.TERMS_EXTENSION),
-                       context);
-
     boolean success = false;
     try {
-      version = readHeader(in);
+      CodecUtil.checkSegmentHeader(in, BlockTermsWriter.CODEC_NAME, 
+                                       BlockTermsWriter.VERSION_START,
+                                       BlockTermsWriter.VERSION_CURRENT,
+                                       state.segmentInfo.getId(), state.segmentSuffix);
 
       // Have PostingsReader init itself
       postingsReader.init(in);
       
-      if (version >= BlockTermsWriter.VERSION_CHECKSUM) {      
-        // NOTE: data file is too costly to verify checksum against all the bytes on open,
-        // but for now we at least verify proper structure of the checksum footer: which looks
-        // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption
-        // such as file truncation.
-        CodecUtil.retrieveChecksum(in);
-      }
+      // NOTE: data file is too costly to verify checksum against all the bytes on open,
+      // but for now we at least verify proper structure of the checksum footer: which looks
+      // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption
+      // such as file truncation.
+      CodecUtil.retrieveChecksum(in);
 
       // Read per-field details
-      seekDir(in, dirOffset);
+      seekDir(in);
 
       final int numFields = in.readVInt();
       if (numFields < 0) {
@@ -152,13 +140,13 @@
         final long numTerms = in.readVLong();
         assert numTerms >= 0;
         final long termsStartPointer = in.readVLong();
-        final FieldInfo fieldInfo = fieldInfos.fieldInfo(field);
+        final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);
         final long sumTotalTermFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS_ONLY ? -1 : in.readVLong();
         final long sumDocFreq = in.readVLong();
         final int docCount = in.readVInt();
-        final int longsSize = version >= BlockTermsWriter.VERSION_META_ARRAY ? in.readVInt() : 0;
-        if (docCount < 0 || docCount > info.getDocCount()) { // #docs with field must be <= #docs
-          throw new CorruptIndexException("invalid docCount: " + docCount + " maxDoc: " + info.getDocCount(), in);
+        final int longsSize = in.readVInt();
+        if (docCount < 0 || docCount > state.segmentInfo.getDocCount()) { // #docs with field must be <= #docs
+          throw new CorruptIndexException("invalid docCount: " + docCount + " maxDoc: " + state.segmentInfo.getDocCount(), in);
         }
         if (sumDocFreq < docCount) {  // #postings must be >= #docs with field
           throw new CorruptIndexException("invalid sumDocFreq: " + sumDocFreq + " docCount: " + docCount, in);
@@ -180,25 +168,10 @@
 
     this.indexReader = indexReader;
   }
-
-  private int readHeader(IndexInput input) throws IOException {
-    int version = CodecUtil.checkHeader(input, BlockTermsWriter.CODEC_NAME,
-                          BlockTermsWriter.VERSION_START,
-                          BlockTermsWriter.VERSION_CURRENT);
-    if (version < BlockTermsWriter.VERSION_APPEND_ONLY) {
-      dirOffset = input.readLong();
-    }
-    return version;
-  }
   
-  private void seekDir(IndexInput input, long dirOffset) throws IOException {
-    if (version >= BlockTermsWriter.VERSION_CHECKSUM) {
-      input.seek(input.length() - CodecUtil.footerLength() - 8);
-      dirOffset = input.readLong();
-    } else if (version >= BlockTermsWriter.VERSION_APPEND_ONLY) {
-      input.seek(input.length() - 8);
-      dirOffset = input.readLong();
-    }
+  private void seekDir(IndexInput input) throws IOException {
+    input.seek(input.length() - CodecUtil.footerLength() - 8);
+    long dirOffset = input.readLong();
     input.seek(dirOffset);
   }
   
@@ -906,9 +879,8 @@
   @Override
   public void checkIntegrity() throws IOException {   
     // verify terms
-    if (version >= BlockTermsWriter.VERSION_CHECKSUM) {
-      CodecUtil.checksumEntireFile(in);
-    }
+    CodecUtil.checksumEntireFile(in);
+
     // verify postings
     postingsReader.checkIntegrity();
   }
Index: lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/BlockTermsWriter.java
===================================================================
--- lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/BlockTermsWriter.java	(revision 1629405)
+++ lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/BlockTermsWriter.java	(working copy)
@@ -59,14 +59,11 @@
 
 public class BlockTermsWriter extends FieldsConsumer implements Closeable {
 
-  final static String CODEC_NAME = "BLOCK_TERMS_DICT";
+  final static String CODEC_NAME = "BlockTermsWriter";
 
   // Initial format
-  public static final int VERSION_START = 0;
-  public static final int VERSION_APPEND_ONLY = 1;
-  public static final int VERSION_META_ARRAY = 2;
-  public static final int VERSION_CHECKSUM = 3;
-  public static final int VERSION_CURRENT = VERSION_CHECKSUM;
+  public static final int VERSION_START = 4;
+  public static final int VERSION_CURRENT = VERSION_START;
 
   /** Extension of terms file */
   static final String TERMS_EXTENSION = "tib";
@@ -113,7 +110,7 @@
     boolean success = false;
     try {
       fieldInfos = state.fieldInfos;
-      writeHeader(out);
+      CodecUtil.writeSegmentHeader(out, CODEC_NAME, VERSION_CURRENT, state.segmentInfo.getId(), state.segmentSuffix);
       currentField = null;
       this.postingsWriter = postingsWriter;
       // segment = state.segmentName;
@@ -128,10 +125,6 @@
       }
     }
   }
-  
-  private void writeHeader(IndexOutput out) throws IOException {
-    CodecUtil.writeHeader(out, CODEC_NAME, VERSION_CURRENT);     
-  }
 
   @Override
   public void write(Fields fields) throws IOException {
@@ -184,9 +177,7 @@
           }
           out.writeVLong(field.sumDocFreq);
           out.writeVInt(field.docCount);
-          if (VERSION_CURRENT >= VERSION_META_ARRAY) {
-            out.writeVInt(field.longsSize);
-          }
+          out.writeVInt(field.longsSize);
         }
         writeTrailer(dirStart);
         CodecUtil.writeFooter(out);
@@ -290,9 +281,6 @@
       // EOF marker:
       out.writeVInt(0);
 
-      this.sumTotalTermFreq = sumTotalTermFreq;
-      this.sumDocFreq = sumDocFreq;
-      this.docCount = docCount;
       fieldIndexWriter.finish(out.getFilePointer());
       if (numTerms > 0) {
         fields.add(new FieldMetaData(fieldInfo,
Index: lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/FixedGapTermsIndexReader.java
===================================================================
--- lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/FixedGapTermsIndexReader.java	(revision 1629405)
+++ lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/FixedGapTermsIndexReader.java	(working copy)
@@ -17,13 +17,11 @@
  * limitations under the License.
  */
 
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
 import org.apache.lucene.store.IndexInput;
 import org.apache.lucene.codecs.CodecUtil;
 import org.apache.lucene.index.CorruptIndexException;
-import org.apache.lucene.index.FieldInfos;
 import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.SegmentReadState;
 import org.apache.lucene.util.Accountable;
 import org.apache.lucene.util.Accountables;
 import org.apache.lucene.util.BytesRef;
@@ -34,7 +32,6 @@
 import java.util.ArrayList;
 import java.util.Collections;
 import java.util.HashMap;
-import java.util.Comparator;
 import java.util.List;
 import java.io.IOException;
 
@@ -58,8 +55,6 @@
   private final int packedIntsVersion;
   private final int blocksize;
 
-  private final Comparator<BytesRef> termComp;
-
   private final static int PAGED_BYTES_BITS = 15;
 
   // all fields share this single logical byte[]
@@ -67,28 +62,24 @@
 
   final HashMap<String,FieldIndexData> fields = new HashMap<>();
   
-  // start of the field info data
-  private long dirOffset;
-  
-  private int version;
-  
-  public FixedGapTermsIndexReader(Directory dir, FieldInfos fieldInfos, String segment, Comparator<BytesRef> termComp, String segmentSuffix, IOContext context)
-    throws IOException {
+  public FixedGapTermsIndexReader(SegmentReadState state) throws IOException {
     final PagedBytes termBytes = new PagedBytes(PAGED_BYTES_BITS);
-
-    this.termComp = termComp;
     
-    final IndexInput in = dir.openInput(IndexFileNames.segmentFileName(segment, segmentSuffix, FixedGapTermsIndexWriter.TERMS_INDEX_EXTENSION), context);
+    String fileName = IndexFileNames.segmentFileName(state.segmentInfo.name, 
+                                                     state.segmentSuffix, 
+                                                     FixedGapTermsIndexWriter.TERMS_INDEX_EXTENSION);
+    final IndexInput in = state.directory.openInput(fileName, state.context);
     
     boolean success = false;
 
     try {
       
-      readHeader(in);
+      CodecUtil.checkSegmentHeader(in, FixedGapTermsIndexWriter.CODEC_NAME,
+                                       FixedGapTermsIndexWriter.VERSION_CURRENT, 
+                                       FixedGapTermsIndexWriter.VERSION_CURRENT,
+                                       state.segmentInfo.getId(), state.segmentSuffix);
       
-      if (version >= FixedGapTermsIndexWriter.VERSION_CHECKSUM) {
-        CodecUtil.checksumEntireFile(in);
-      }
+      CodecUtil.checksumEntireFile(in);
       
       indexInterval = in.readVInt();
       if (indexInterval < 1) {
@@ -97,7 +88,7 @@
       packedIntsVersion = in.readVInt();
       blocksize = in.readVInt();
       
-      seekDir(in, dirOffset);
+      seekDir(in);
 
       // Read directory
       final int numFields = in.readVInt();     
@@ -118,7 +109,7 @@
         if (packedIndexStart < indexStart) {
           throw new CorruptIndexException("invalid packedIndexStart: " + packedIndexStart + " indexStart: " + indexStart + "numIndexTerms: " + numIndexTerms, in);
         }
-        final FieldInfo fieldInfo = fieldInfos.fieldInfo(field);
+        final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);
         FieldIndexData previous = fields.put(fieldInfo.name, new FieldIndexData(in, termBytes, indexStart, termsStart, packedIndexStart, packedOffsetsStart, numIndexTerms));
         if (previous != null) {
           throw new CorruptIndexException("duplicate field: " + fieldInfo.name, in);
@@ -135,11 +126,6 @@
     }
   }
 
-  private void readHeader(IndexInput input) throws IOException {
-    version = CodecUtil.checkHeader(input, FixedGapTermsIndexWriter.CODEC_NAME,
-      FixedGapTermsIndexWriter.VERSION_CURRENT, FixedGapTermsIndexWriter.VERSION_CURRENT);
-  }
-
   private class IndexEnum extends FieldIndexEnum {
     private final FieldIndexData fieldIndex;
     private final BytesRef term = new BytesRef();
@@ -166,7 +152,7 @@
         final int length = (int) (fieldIndex.termOffsets.get(1+mid) - offset);
         termBytesReader.fillSlice(term, fieldIndex.termBytesStart + offset, length);
 
-        int delta = termComp.compare(target, term);
+        int delta = target.compareTo(term);
         if (delta < 0) {
           hi = mid - 1;
         } else if (delta > 0) {
@@ -301,13 +287,9 @@
   @Override
   public void close() throws IOException {}
 
-  private void seekDir(IndexInput input, long dirOffset) throws IOException {
-    if (version >= FixedGapTermsIndexWriter.VERSION_CHECKSUM) {
-      input.seek(input.length() - CodecUtil.footerLength() - 8);
-    } else {
-      input.seek(input.length() - 8);
-    }
-    dirOffset = input.readLong();
+  private void seekDir(IndexInput input) throws IOException {
+    input.seek(input.length() - CodecUtil.footerLength() - 8);
+    long dirOffset = input.readLong();
     input.seek(dirOffset);
   }
 
Index: lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/FixedGapTermsIndexWriter.java
===================================================================
--- lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/FixedGapTermsIndexWriter.java	(revision 1629405)
+++ lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/FixedGapTermsIndexWriter.java	(working copy)
@@ -49,12 +49,9 @@
   /** Extension of terms index file */
   static final String TERMS_INDEX_EXTENSION = "tii";
 
-  final static String CODEC_NAME = "SIMPLE_STANDARD_TERMS_INDEX";
-  final static int VERSION_START = 0;
-  final static int VERSION_APPEND_ONLY = 1;
-  final static int VERSION_MONOTONIC_ADDRESSING = 2;
-  final static int VERSION_CHECKSUM = 3;
-  final static int VERSION_CURRENT = VERSION_CHECKSUM;
+  final static String CODEC_NAME = "FixedGapTermsIndex";
+  final static int VERSION_START = 4;
+  final static int VERSION_CURRENT = VERSION_START;
 
   final static int BLOCKSIZE = 4096;
   final private int termIndexInterval;
@@ -75,7 +72,7 @@
     out = state.directory.createOutput(indexFileName, state.context);
     boolean success = false;
     try {
-      writeHeader(out);
+      CodecUtil.writeSegmentHeader(out, CODEC_NAME, VERSION_CURRENT, state.segmentInfo.getId(), state.segmentSuffix);
       out.writeVInt(termIndexInterval);
       out.writeVInt(PackedInts.VERSION_CURRENT);
       out.writeVInt(BLOCKSIZE);
@@ -86,10 +83,6 @@
       }
     }
   }
-  
-  private void writeHeader(IndexOutput out) throws IOException {
-    CodecUtil.writeHeader(out, CODEC_NAME, VERSION_CURRENT);
-  }
 
   @Override
   public FieldWriter addField(FieldInfo field, long termsFilePointer) {
Index: lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/VariableGapTermsIndexReader.java
===================================================================
--- lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/VariableGapTermsIndexReader.java	(revision 1629405)
+++ lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/VariableGapTermsIndexReader.java	(working copy)
@@ -24,9 +24,8 @@
 import org.apache.lucene.codecs.CodecUtil;
 import org.apache.lucene.index.CorruptIndexException;
 import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.FieldInfos;
 import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.store.Directory;
+import org.apache.lucene.index.SegmentReadState;
 import org.apache.lucene.store.IOContext;
 import org.apache.lucene.store.IndexInput;
 import org.apache.lucene.util.Accountable;
@@ -46,27 +45,23 @@
 
   final HashMap<String,FieldIndexData> fields = new HashMap<>();
   
-  // start of the field info data
-  private long dirOffset;
-  
-  private final int version;
-
-  final String segment;
-  public VariableGapTermsIndexReader(Directory dir, FieldInfos fieldInfos, String segment, String segmentSuffix, IOContext context)
-    throws IOException {
-    final IndexInput in = dir.openInput(IndexFileNames.segmentFileName(segment, segmentSuffix, VariableGapTermsIndexWriter.TERMS_INDEX_EXTENSION), new IOContext(context, true));
-    this.segment = segment;
+  public VariableGapTermsIndexReader(SegmentReadState state) throws IOException {
+    String fileName = IndexFileNames.segmentFileName(state.segmentInfo.name, 
+                                                     state.segmentSuffix, 
+                                                     VariableGapTermsIndexWriter.TERMS_INDEX_EXTENSION);
+    final IndexInput in = state.directory.openInput(fileName, new IOContext(state.context, true));
     boolean success = false;
 
     try {
       
-      version = readHeader(in);
+      CodecUtil.checkSegmentHeader(in, VariableGapTermsIndexWriter.CODEC_NAME,
+                                       VariableGapTermsIndexWriter.VERSION_START,
+                                       VariableGapTermsIndexWriter.VERSION_CURRENT,
+                                       state.segmentInfo.getId(), state.segmentSuffix);
       
-      if (version >= VariableGapTermsIndexWriter.VERSION_CHECKSUM) {
-        CodecUtil.checksumEntireFile(in);
-      }
+      CodecUtil.checksumEntireFile(in);
 
-      seekDir(in, dirOffset);
+      seekDir(in);
 
       // Read directory
       final int numFields = in.readVInt();
@@ -77,7 +72,7 @@
       for(int i=0;i<numFields;i++) {
         final int field = in.readVInt();
         final long indexStart = in.readVLong();
-        final FieldInfo fieldInfo = fieldInfos.fieldInfo(field);
+        final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);
         FieldIndexData previous = fields.put(fieldInfo.name, new FieldIndexData(in, fieldInfo, indexStart));
         if (previous != null) {
           throw new CorruptIndexException("duplicate field: " + fieldInfo.name, in);
@@ -92,15 +87,6 @@
       }
     }
   }
-  
-  private int readHeader(IndexInput input) throws IOException {
-    int version = CodecUtil.checkHeader(input, VariableGapTermsIndexWriter.CODEC_NAME,
-      VariableGapTermsIndexWriter.VERSION_START, VariableGapTermsIndexWriter.VERSION_CURRENT);
-    if (version < VariableGapTermsIndexWriter.VERSION_APPEND_ONLY) {
-      dirOffset = input.readLong();
-    }
-    return version;
-  }
 
   private static class IndexEnum extends FieldIndexEnum {
     private final BytesRefFSTEnum<Long> fstEnum;
@@ -206,14 +192,9 @@
   @Override
   public void close() throws IOException {}
 
-  private void seekDir(IndexInput input, long dirOffset) throws IOException {
-    if (version >= VariableGapTermsIndexWriter.VERSION_CHECKSUM) {
-      input.seek(input.length() - CodecUtil.footerLength() - 8);
-      dirOffset = input.readLong();
-    } else if (version >= VariableGapTermsIndexWriter.VERSION_APPEND_ONLY) {
-      input.seek(input.length() - 8);
-      dirOffset = input.readLong();
-    }
+  private void seekDir(IndexInput input) throws IOException {
+    input.seek(input.length() - CodecUtil.footerLength() - 8);
+    long dirOffset = input.readLong();
     input.seek(dirOffset);
   }
 
Index: lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/VariableGapTermsIndexWriter.java
===================================================================
--- lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/VariableGapTermsIndexWriter.java	(revision 1629405)
+++ lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/VariableGapTermsIndexWriter.java	(working copy)
@@ -52,11 +52,9 @@
   /** Extension of terms index file */
   static final String TERMS_INDEX_EXTENSION = "tiv";
 
-  final static String CODEC_NAME = "VARIABLE_GAP_TERMS_INDEX";
-  final static int VERSION_START = 0;
-  final static int VERSION_APPEND_ONLY = 1;
-  final static int VERSION_CHECKSUM = 2;
-  final static int VERSION_CURRENT = VERSION_CHECKSUM;
+  final static String CODEC_NAME = "VariableGapTermsIndex";
+  final static int VERSION_START = 3;
+  final static int VERSION_CURRENT = VERSION_START;
 
   private final List<FSTFieldWriter> fields = new ArrayList<>();
   
@@ -184,7 +182,7 @@
     try {
       fieldInfos = state.fieldInfos;
       this.policy = policy;
-      writeHeader(out);
+      CodecUtil.writeSegmentHeader(out, CODEC_NAME, VERSION_CURRENT, state.segmentInfo.getId(), state.segmentSuffix);
       success = true;
     } finally {
       if (!success) {
@@ -192,10 +190,6 @@
       }
     }
   }
-  
-  private void writeHeader(IndexOutput out) throws IOException {
-    CodecUtil.writeHeader(out, CODEC_NAME, VERSION_CURRENT);
-  }
 
   @Override
   public FieldWriter addField(FieldInfo field, long termsFilePointer) throws IOException {
Index: lucene/codecs/src/java/org/apache/lucene/codecs/blocktreeords/Ords41PostingsFormat.java
===================================================================
--- lucene/codecs/src/java/org/apache/lucene/codecs/blocktreeords/Ords41PostingsFormat.java	(revision 1629405)
+++ lucene/codecs/src/java/org/apache/lucene/codecs/blocktreeords/Ords41PostingsFormat.java	(working copy)
@@ -94,12 +94,7 @@
                                                                    state.segmentSuffix);
     boolean success = false;
     try {
-      FieldsProducer ret = new OrdsBlockTreeTermsReader(state.directory,
-                                                        state.fieldInfos,
-                                                        state.segmentInfo,
-                                                        postingsReader,
-                                                        state.context,
-                                                        state.segmentSuffix);
+      FieldsProducer ret = new OrdsBlockTreeTermsReader(postingsReader, state);
       success = true;
       return ret;
     } finally {
Index: lucene/codecs/src/java/org/apache/lucene/codecs/blocktreeords/OrdsBlockTreeTermsReader.java
===================================================================
--- lucene/codecs/src/java/org/apache/lucene/codecs/blocktreeords/OrdsBlockTreeTermsReader.java	(revision 1629405)
+++ lucene/codecs/src/java/org/apache/lucene/codecs/blocktreeords/OrdsBlockTreeTermsReader.java	(working copy)
@@ -31,12 +31,9 @@
 import org.apache.lucene.index.CorruptIndexException;
 import org.apache.lucene.index.FieldInfo.IndexOptions;
 import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.FieldInfos;
 import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.index.SegmentReadState;
 import org.apache.lucene.index.Terms;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
 import org.apache.lucene.store.IndexInput;
 import org.apache.lucene.util.Accountable;
 import org.apache.lucene.util.Accountables;
@@ -62,42 +59,33 @@
 
   private final TreeMap<String,OrdsFieldReader> fields = new TreeMap<>();
 
-  /** File offset where the directory starts in the terms file. */
-  private long dirOffset;
-
-  /** File offset where the directory starts in the index file. */
-  private long indexDirOffset;
-
-  final String segment;
-  
-  private final int version;
-
   /** Sole constructor. */
-  public OrdsBlockTreeTermsReader(Directory dir, FieldInfos fieldInfos, SegmentInfo info,
-                                  PostingsReaderBase postingsReader, IOContext ioContext,
-                                  String segmentSuffix)
-    throws IOException {
+  public OrdsBlockTreeTermsReader(PostingsReaderBase postingsReader, SegmentReadState state) throws IOException {
     
     this.postingsReader = postingsReader;
 
-    this.segment = info.name;
-    in = dir.openInput(IndexFileNames.segmentFileName(segment, segmentSuffix, OrdsBlockTreeTermsWriter.TERMS_EXTENSION),
-                       ioContext);
+    String termsFile = IndexFileNames.segmentFileName(state.segmentInfo.name, 
+                                                      state.segmentSuffix, 
+                                                      OrdsBlockTreeTermsWriter.TERMS_EXTENSION);
+    in = state.directory.openInput(termsFile, state.context);
 
     boolean success = false;
     IndexInput indexIn = null;
 
     try {
-      version = CodecUtil.checkHeader(in,
-                                      OrdsBlockTreeTermsWriter.TERMS_CODEC_NAME,
-                                      OrdsBlockTreeTermsWriter.VERSION_START,
-                                      OrdsBlockTreeTermsWriter.VERSION_CURRENT);
-      indexIn = dir.openInput(IndexFileNames.segmentFileName(segment, segmentSuffix, OrdsBlockTreeTermsWriter.TERMS_INDEX_EXTENSION),
-                              ioContext);
-      int indexVersion = CodecUtil.checkHeader(indexIn,
-                                               OrdsBlockTreeTermsWriter.TERMS_INDEX_CODEC_NAME,
-                                               OrdsBlockTreeTermsWriter.VERSION_START,
-                                               OrdsBlockTreeTermsWriter.VERSION_CURRENT);
+      int version = CodecUtil.checkSegmentHeader(in, OrdsBlockTreeTermsWriter.TERMS_CODEC_NAME,
+                                                     OrdsBlockTreeTermsWriter.VERSION_START,
+                                                     OrdsBlockTreeTermsWriter.VERSION_CURRENT,
+                                                     state.segmentInfo.getId(), state.segmentSuffix);
+      
+      String indexFile = IndexFileNames.segmentFileName(state.segmentInfo.name, 
+                                                        state.segmentSuffix, 
+                                                        OrdsBlockTreeTermsWriter.TERMS_INDEX_EXTENSION);
+      indexIn = state.directory.openInput(indexFile, state.context);
+      int indexVersion = CodecUtil.checkSegmentHeader(indexIn, OrdsBlockTreeTermsWriter.TERMS_INDEX_CODEC_NAME,
+                                                               OrdsBlockTreeTermsWriter.VERSION_START,
+                                                               OrdsBlockTreeTermsWriter.VERSION_CURRENT,
+                                                               state.segmentInfo.getId(), state.segmentSuffix);
       if (indexVersion != version) {
         throw new CorruptIndexException("mixmatched version files: " + in + "=" + version + "," + indexIn + "=" + indexVersion, indexIn);
       }
@@ -116,8 +104,8 @@
       CodecUtil.retrieveChecksum(in);
 
       // Read per-field details
-      seekDir(in, dirOffset);
-      seekDir(indexIn, indexDirOffset);
+      seekDir(in);
+      seekDir(indexIn);
 
       final int numFields = in.readVInt();
       if (numFields < 0) {
@@ -134,7 +122,7 @@
         in.readBytes(code.bytes, 0, numBytes);
         code.length = numBytes;
         final Output rootCode = OrdsBlockTreeTermsWriter.FST_OUTPUTS.newOutput(code, 0, numTerms);
-        final FieldInfo fieldInfo = fieldInfos.fieldInfo(field);
+        final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);
         assert fieldInfo != null: "field=" + field;
         assert numTerms <= Integer.MAX_VALUE;
         final long sumTotalTermFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS_ONLY ? -1 : in.readVLong();
@@ -145,8 +133,8 @@
 
         BytesRef minTerm = readBytesRef(in);
         BytesRef maxTerm = readBytesRef(in);
-        if (docCount < 0 || docCount > info.getDocCount()) { // #docs with field must be <= #docs
-          throw new CorruptIndexException("invalid docCount: " + docCount + " maxDoc: " + info.getDocCount(), in);
+        if (docCount < 0 || docCount > state.segmentInfo.getDocCount()) { // #docs with field must be <= #docs
+          throw new CorruptIndexException("invalid docCount: " + docCount + " maxDoc: " + state.segmentInfo.getDocCount(), in);
         }
         if (sumDocFreq < docCount) {  // #postings must be >= #docs with field
           throw new CorruptIndexException("invalid sumDocFreq: " + sumDocFreq + " docCount: " + docCount, in);
@@ -182,10 +170,9 @@
   }
 
   /** Seek {@code input} to the directory offset. */
-  private void seekDir(IndexInput input, long dirOffset)
-      throws IOException {
+  private void seekDir(IndexInput input) throws IOException {
     input.seek(input.length() - CodecUtil.footerLength() - 8);
-    dirOffset = input.readLong();
+    long dirOffset = input.readLong();
     input.seek(dirOffset);
   }
 
Index: lucene/codecs/src/java/org/apache/lucene/codecs/blocktreeords/OrdsBlockTreeTermsWriter.java
===================================================================
--- lucene/codecs/src/java/org/apache/lucene/codecs/blocktreeords/OrdsBlockTreeTermsWriter.java	(revision 1629405)
+++ lucene/codecs/src/java/org/apache/lucene/codecs/blocktreeords/OrdsBlockTreeTermsWriter.java	(working copy)
@@ -42,7 +42,6 @@
 import org.apache.lucene.util.BytesRefBuilder;
 import org.apache.lucene.util.FixedBitSet;
 import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.IntsRef;
 import org.apache.lucene.util.IntsRefBuilder;
 import org.apache.lucene.util.StringHelper;
 import org.apache.lucene.util.fst.Builder;
@@ -115,10 +114,10 @@
 
   /** Extension of terms file */
   static final String TERMS_EXTENSION = "tio";
-  final static String TERMS_CODEC_NAME = "BLOCK_TREE_ORDS_TERMS_DICT";
+  final static String TERMS_CODEC_NAME = "OrdsBlockTreeTerms";
 
   /** Initial terms format. */
-  public static final int VERSION_START = 0;
+  public static final int VERSION_START = 1;
 
   /** Current terms format. */
   public static final int VERSION_CURRENT = VERSION_START;
@@ -125,7 +124,7 @@
 
   /** Extension of terms index file */
   static final String TERMS_INDEX_EXTENSION = "tipo";
-  final static String TERMS_INDEX_CODEC_NAME = "BLOCK_TREE_ORDS_TERMS_INDEX";
+  final static String TERMS_INDEX_CODEC_NAME = "OrdsBlockTreeIndex";
 
   private final IndexOutput out;
   private final IndexOutput indexOut;
@@ -204,11 +203,11 @@
       fieldInfos = state.fieldInfos;
       this.minItemsInBlock = minItemsInBlock;
       this.maxItemsInBlock = maxItemsInBlock;
-      CodecUtil.writeHeader(out, TERMS_CODEC_NAME, VERSION_CURRENT);   
+      CodecUtil.writeSegmentHeader(out, TERMS_CODEC_NAME, VERSION_CURRENT, state.segmentInfo.getId(), state.segmentSuffix);
 
       final String termsIndexFileName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, TERMS_INDEX_EXTENSION);
       indexOut = state.directory.createOutput(termsIndexFileName, state.context);
-      CodecUtil.writeHeader(indexOut, TERMS_INDEX_CODEC_NAME, VERSION_CURRENT); 
+      CodecUtil.writeSegmentHeader(indexOut, TERMS_INDEX_CODEC_NAME, VERSION_CURRENT, state.segmentInfo.getId(), state.segmentSuffix);
 
       this.postingsWriter = postingsWriter;
       // segment = state.segmentInfo.name;
Index: lucene/codecs/src/java/org/apache/lucene/codecs/blocktreeords/OrdsSegmentTermsEnum.java
===================================================================
--- lucene/codecs/src/java/org/apache/lucene/codecs/blocktreeords/OrdsSegmentTermsEnum.java	(revision 1629405)
+++ lucene/codecs/src/java/org/apache/lucene/codecs/blocktreeords/OrdsSegmentTermsEnum.java	(working copy)
@@ -1030,7 +1030,7 @@
 
   @Override
   public String toString() {
-    return "OrdsSegmentTermsEnum(seg=" + fr.parent.segment + ")";
+    return "OrdsSegmentTermsEnum(seg=" + fr.parent + ")";
   }
 
   /** Holds a single input (IntsRef) + output pair. */
Index: lucene/codecs/src/java/org/apache/lucene/codecs/bloom/BloomFilteringPostingsFormat.java
===================================================================
--- lucene/codecs/src/java/org/apache/lucene/codecs/bloom/BloomFilteringPostingsFormat.java	(revision 1629405)
+++ lucene/codecs/src/java/org/apache/lucene/codecs/bloom/BloomFilteringPostingsFormat.java	(working copy)
@@ -72,7 +72,7 @@
  * NumFilteredFields, Filter<sup>NumFilteredFields</sup>, Footer</li>
  * <li>Filter --&gt; FieldNumber, FuzzySet</li>
  * <li>FuzzySet --&gt;See {@link FuzzySet#serialize(DataOutput)}</li>
- * <li>Header --&gt; {@link CodecUtil#writeHeader CodecHeader}</li>
+ * <li>Header --&gt; {@link CodecUtil#writeSegmentHeader SegmentHeader}</li>
  * <li>DelegatePostingsFormatName --&gt; {@link DataOutput#writeString(String)
  * String} The name of a ServiceProvider registered {@link PostingsFormat}</li>
  * <li>NumFilteredFields --&gt; {@link DataOutput#writeInt Uint32}</li>
@@ -85,9 +85,8 @@
 public final class BloomFilteringPostingsFormat extends PostingsFormat {
   
   public static final String BLOOM_CODEC_NAME = "BloomFilter";
-  public static final int VERSION_START = 1;
-  public static final int VERSION_CHECKSUM = 2;
-  public static final int VERSION_CURRENT = VERSION_CHECKSUM;
+  public static final int VERSION_START = 3;
+  public static final int VERSION_CURRENT = VERSION_START;
   
   /** Extension of Bloom Filters file */
   static final String BLOOM_EXTENSION = "blm";
@@ -167,7 +166,7 @@
       boolean success = false;
       try {
         bloomIn = state.directory.openChecksumInput(bloomFileName, state.context);
-        int version = CodecUtil.checkHeader(bloomIn, BLOOM_CODEC_NAME, VERSION_START, VERSION_CURRENT);
+        CodecUtil.checkSegmentHeader(bloomIn, BLOOM_CODEC_NAME, VERSION_START, VERSION_CURRENT, state.segmentInfo.getId(), state.segmentSuffix);
         // // Load the hash function used in the BloomFilter
         // hashFunction = HashFunction.forName(bloomIn.readString());
         // Load the delegate postings format
@@ -183,11 +182,7 @@
           FieldInfo fieldInfo = state.fieldInfos.fieldInfo(fieldNum);
           bloomsByFieldName.put(fieldInfo.name, bloom);
         }
-        if (version >= VERSION_CHECKSUM) {
-          CodecUtil.checkFooter(bloomIn);
-        } else {
-          CodecUtil.checkEOF(bloomIn);
-        }
+        CodecUtil.checkFooter(bloomIn);
         IOUtils.close(bloomIn);
         success = true;
       } finally {
@@ -507,7 +502,7 @@
       IndexOutput bloomOutput = null;
       try {
         bloomOutput = state.directory.createOutput(bloomFileName, state.context);
-        CodecUtil.writeHeader(bloomOutput, BLOOM_CODEC_NAME, VERSION_CURRENT);
+        CodecUtil.writeSegmentHeader(bloomOutput, BLOOM_CODEC_NAME, VERSION_CURRENT, state.segmentInfo.getId(), state.segmentSuffix);
         // remember the name of the postings format we will delegate to
         bloomOutput.writeString(delegatePostingsFormat.getName());
         
Index: lucene/codecs/src/java/org/apache/lucene/codecs/memory/DirectDocValuesConsumer.java
===================================================================
--- lucene/codecs/src/java/org/apache/lucene/codecs/memory/DirectDocValuesConsumer.java	(revision 1629405)
+++ lucene/codecs/src/java/org/apache/lucene/codecs/memory/DirectDocValuesConsumer.java	(working copy)
@@ -52,10 +52,10 @@
     try {
       String dataName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, dataExtension);
       data = state.directory.createOutput(dataName, state.context);
-      CodecUtil.writeHeader(data, dataCodec, VERSION_CURRENT);
+      CodecUtil.writeSegmentHeader(data, dataCodec, VERSION_CURRENT, state.segmentInfo.getId(), state.segmentSuffix);
       String metaName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, metaExtension);
       meta = state.directory.createOutput(metaName, state.context);
-      CodecUtil.writeHeader(meta, metaCodec, VERSION_CURRENT);
+      CodecUtil.writeSegmentHeader(meta, metaCodec, VERSION_CURRENT, state.segmentInfo.getId(), state.segmentSuffix);
       success = true;
     } finally {
       if (!success) {
Index: lucene/codecs/src/java/org/apache/lucene/codecs/memory/DirectDocValuesProducer.java
===================================================================
--- lucene/codecs/src/java/org/apache/lucene/codecs/memory/DirectDocValuesProducer.java	(revision 1629405)
+++ lucene/codecs/src/java/org/apache/lucene/codecs/memory/DirectDocValuesProducer.java	(working copy)
@@ -76,6 +76,8 @@
   private final AtomicLong ramBytesUsed;
   private final int version;
   
+  private final boolean merging;
+  
   static final byte NUMBER = 0;
   static final byte BYTES = 1;
   static final byte SORTED = 2;
@@ -84,11 +86,36 @@
   static final byte SORTED_NUMERIC = 5;
   static final byte SORTED_NUMERIC_SINGLETON = 6;
 
-  static final int VERSION_START = 2;
+  static final int VERSION_START = 3;
   static final int VERSION_CURRENT = VERSION_START;
+  
+  // clone for merge: when merging we don't do any instances.put()s
+  DirectDocValuesProducer(DirectDocValuesProducer original) throws IOException {
+    assert Thread.holdsLock(original);
+    numerics.putAll(original.numerics);
+    binaries.putAll(original.binaries);
+    sorteds.putAll(original.sorteds);
+    sortedSets.putAll(original.sortedSets);
+    sortedNumerics.putAll(original.sortedNumerics);
+    data = original.data.clone();
     
+    numericInstances.putAll(original.numericInstances);
+    binaryInstances.putAll(original.binaryInstances);
+    sortedInstances.putAll(original.sortedInstances);
+    sortedSetInstances.putAll(original.sortedSetInstances);
+    sortedNumericInstances.putAll(original.sortedNumericInstances);
+    docsWithFieldInstances.putAll(original.docsWithFieldInstances);
+    
+    numEntries = original.numEntries;
+    maxDoc = original.maxDoc;
+    ramBytesUsed = new AtomicLong(original.ramBytesUsed.get());
+    version = original.version;
+    merging = true;
+  }
+    
   DirectDocValuesProducer(SegmentReadState state, String dataCodec, String dataExtension, String metaCodec, String metaExtension) throws IOException {
     maxDoc = state.segmentInfo.getDocCount();
+    merging = false;
     String metaName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, metaExtension);
     // read in the entries from the metadata file.
     ChecksumIndexInput in = state.directory.openChecksumInput(metaName, state.context);
@@ -95,9 +122,8 @@
     ramBytesUsed = new AtomicLong(RamUsageEstimator.shallowSizeOfInstance(getClass()));
     boolean success = false;
     try {
-      version = CodecUtil.checkHeader(in, metaCodec, 
-                                      VERSION_START,
-                                      VERSION_CURRENT);
+      version = CodecUtil.checkSegmentHeader(in, metaCodec, VERSION_START, VERSION_CURRENT, 
+                                                 state.segmentInfo.getId(), state.segmentSuffix);
       numEntries = readFields(in, state.fieldInfos);
 
       CodecUtil.checkFooter(in);
@@ -114,9 +140,8 @@
     this.data = state.directory.openInput(dataName, state.context);
     success = false;
     try {
-      final int version2 = CodecUtil.checkHeader(data, dataCodec, 
-                                                 VERSION_START,
-                                                 VERSION_CURRENT);
+      final int version2 = CodecUtil.checkSegmentHeader(data, dataCodec, VERSION_START, VERSION_CURRENT,
+                                                              state.segmentInfo.getId(), state.segmentSuffix);
       if (version != version2) {
         throw new CorruptIndexException("Format versions mismatch: meta=" + version + ", data=" + version2, data);
       }
@@ -261,7 +286,10 @@
     if (instance == null) {
       // Lazy load
       instance = loadNumeric(numerics.get(field.name));
-      numericInstances.put(field.name, instance);
+      if (!merging) {
+        numericInstances.put(field.name, instance);
+        ramBytesUsed.addAndGet(instance.ramBytesUsed());
+      }
     }
     return instance.numerics;
   }
@@ -275,7 +303,6 @@
         final byte[] values = new byte[entry.count];
         data.readBytes(values, 0, entry.count);
         ret.bytesUsed = RamUsageEstimator.sizeOf(values);
-        ramBytesUsed.addAndGet(ret.bytesUsed);
         ret.numerics = new NumericDocValues() {
           @Override
           public long get(int idx) {
@@ -292,7 +319,6 @@
           values[i] = data.readShort();
         }
         ret.bytesUsed = RamUsageEstimator.sizeOf(values);
-        ramBytesUsed.addAndGet(ret.bytesUsed);
         ret.numerics = new NumericDocValues() {
           @Override
           public long get(int idx) {
@@ -309,7 +335,6 @@
           values[i] = data.readInt();
         }
         ret.bytesUsed = RamUsageEstimator.sizeOf(values);
-        ramBytesUsed.addAndGet(ret.bytesUsed);
         ret.numerics = new NumericDocValues() {
           @Override
           public long get(int idx) {
@@ -326,7 +351,6 @@
           values[i] = data.readLong();
         }
         ret.bytesUsed = RamUsageEstimator.sizeOf(values);
-        ramBytesUsed.addAndGet(ret.bytesUsed);
         ret.numerics = new NumericDocValues() {
           @Override
           public long get(int idx) {
@@ -347,7 +371,10 @@
     if (instance == null) {
       // Lazy load
       instance = loadBinary(binaries.get(field.name));
-      binaryInstances.put(field.name, instance);
+      if (!merging) {
+        binaryInstances.put(field.name, instance);
+        ramBytesUsed.addAndGet(instance.ramBytesUsed());
+      }
     }
     final byte[] bytes = instance.bytes;
     final int[] address = instance.address;
@@ -377,8 +404,6 @@
     }
     address[entry.count] = data.readInt();
 
-    ramBytesUsed.addAndGet(RamUsageEstimator.sizeOf(bytes) + RamUsageEstimator.sizeOf(address));
-
     BinaryRawValues values = new BinaryRawValues();
     values.bytes = bytes;
     values.address = address;
@@ -394,7 +419,10 @@
       if (instance == null) {
         // Lazy load
         instance = loadSorted(field);
-        sortedInstances.put(field.name, instance);
+        if (!merging) {
+          sortedInstances.put(field.name, instance);
+          ramBytesUsed.addAndGet(instance.ramBytesUsed());
+        }
       }
     }
     return newSortedInstance(instance.docToOrd.numerics, getBinary(field), entry.values.count);
@@ -439,7 +467,10 @@
     if (instance == null) {
       // Lazy load
       instance = loadSortedNumeric(entry);
-      sortedNumericInstances.put(field.name, instance);
+      if (!merging) {
+        sortedNumericInstances.put(field.name, instance);
+        ramBytesUsed.addAndGet(instance.ramBytesUsed());
+      }
     }
     
     if (entry.docToAddress == null) {
@@ -489,7 +520,10 @@
     if (instance == null) {
       // Lazy load
       instance = loadSortedSet(entry);
-      sortedSetInstances.put(field.name, instance);
+      if (!merging) {
+        sortedSetInstances.put(field.name, instance);
+        ramBytesUsed.addAndGet(instance.ramBytesUsed());
+      }
     }
 
     if (instance.docToOrdAddress == null) {
@@ -573,7 +607,10 @@
             bits[i] = data.readLong();
           }
           instance = new FixedBitSet(bits, maxDoc);
-          docsWithFieldInstances.put(field.name, instance);
+          if (!merging) {
+            docsWithFieldInstances.put(field.name, instance);
+            ramBytesUsed.addAndGet(instance.ramBytesUsed());
+          }
         }
       }
       return instance;
@@ -601,6 +638,11 @@
   }
 
   @Override
+  public synchronized DocValuesProducer getMergeInstance() throws IOException {
+    return new DirectDocValuesProducer(this);
+  }
+
+  @Override
   public void close() throws IOException {
     data.close();
   }
Index: lucene/codecs/src/java/org/apache/lucene/codecs/memory/FSTOrdTermsReader.java
===================================================================
--- lucene/codecs/src/java/org/apache/lucene/codecs/memory/FSTOrdTermsReader.java	(revision 1629405)
+++ lucene/codecs/src/java/org/apache/lucene/codecs/memory/FSTOrdTermsReader.java	(working copy)
@@ -75,7 +75,6 @@
   static final int INTERVAL = FSTOrdTermsWriter.SKIP_INTERVAL;
   final TreeMap<String, TermsReader> fields = new TreeMap<>();
   final PostingsReaderBase postingsReader;
-  int version;
   //static final boolean TEST = false;
 
   public FSTOrdTermsReader(SegmentReadState state, PostingsReaderBase postingsReader) throws IOException {
@@ -89,11 +88,20 @@
     try {
       indexIn = state.directory.openChecksumInput(termsIndexFileName, state.context);
       blockIn = state.directory.openInput(termsBlockFileName, state.context);
-      version = readHeader(indexIn);
-      readHeader(blockIn);
-      if (version >= FSTOrdTermsWriter.TERMS_VERSION_CHECKSUM) {
-        CodecUtil.checksumEntireFile(blockIn);
+      int version = CodecUtil.checkSegmentHeader(indexIn, FSTOrdTermsWriter.TERMS_INDEX_CODEC_NAME, 
+                                                          FSTOrdTermsWriter.VERSION_START, 
+                                                          FSTOrdTermsWriter.VERSION_CURRENT, 
+                                                          state.segmentInfo.getId(), state.segmentSuffix);
+      int version2 = CodecUtil.checkSegmentHeader(blockIn, FSTOrdTermsWriter.TERMS_CODEC_NAME, 
+                                                           FSTOrdTermsWriter.VERSION_START, 
+                                                           FSTOrdTermsWriter.VERSION_CURRENT, 
+                                                           state.segmentInfo.getId(), state.segmentSuffix);
+      
+      if (version != version2) {
+        throw new CorruptIndexException("Format versions mismatch: index=" + version + ", terms=" + version2, blockIn);
       }
+
+      CodecUtil.checksumEntireFile(blockIn);
       
       this.postingsReader.init(blockIn);
       seekDir(blockIn);
@@ -114,11 +122,7 @@
         TermsReader previous = fields.put(fieldInfo.name, current);
         checkFieldSummary(state.segmentInfo, indexIn, blockIn, current, previous);
       }
-      if (version >= FSTOrdTermsWriter.TERMS_VERSION_CHECKSUM) {
-        CodecUtil.checkFooter(indexIn);
-      } else {
-        CodecUtil.checkEOF(indexIn);
-      }
+      CodecUtil.checkFooter(indexIn);
       success = true;
     } finally {
       if (success) {
@@ -129,17 +133,8 @@
     }
   }
 
-  private int readHeader(IndexInput in) throws IOException {
-    return CodecUtil.checkHeader(in, FSTOrdTermsWriter.TERMS_CODEC_NAME,
-                                     FSTOrdTermsWriter.TERMS_VERSION_START,
-                                     FSTOrdTermsWriter.TERMS_VERSION_CURRENT);
-  }
   private void seekDir(IndexInput in) throws IOException {
-    if (version >= FSTOrdTermsWriter.TERMS_VERSION_CHECKSUM) {
-      in.seek(in.length() - CodecUtil.footerLength() - 8);
-    } else {
-      in.seek(in.length() - 8);
-    }
+    in.seek(in.length() - CodecUtil.footerLength() - 8);
     in.seek(in.readLong());
   }
   private void checkFieldSummary(SegmentInfo info, IndexInput indexIn, IndexInput blockIn, TermsReader field, TermsReader previous) throws IOException {
Index: lucene/codecs/src/java/org/apache/lucene/codecs/memory/FSTOrdTermsWriter.java
===================================================================
--- lucene/codecs/src/java/org/apache/lucene/codecs/memory/FSTOrdTermsWriter.java	(revision 1629405)
+++ lucene/codecs/src/java/org/apache/lucene/codecs/memory/FSTOrdTermsWriter.java	(working copy)
@@ -39,7 +39,6 @@
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.FixedBitSet;
 import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.IntsRef;
 import org.apache.lucene.util.IntsRefBuilder;
 import org.apache.lucene.util.fst.Builder;
 import org.apache.lucene.util.fst.FST;
@@ -76,7 +75,7 @@
  * <ul>
  *  <li>TermIndex(.tix) --&gt; Header, TermFST<sup>NumFields</sup>, Footer</li>
  *  <li>TermFST --&gt; {@link FST FST&lt;long&gt;}</li>
- *  <li>Header --&gt; {@link CodecUtil#writeHeader CodecHeader}</li>
+ *  <li>Header --&gt; {@link CodecUtil#writeSegmentHeader SegmentHeader}</li>
  *  <li>Footer --&gt; {@link CodecUtil#writeFooter CodecFooter}</li>
  * </ul>
  *
@@ -114,7 +113,7 @@
  *  <li>StatsBlock --&gt; &lt; DocFreq[Same?], (TotalTermFreq-DocFreq) ? &gt; <sup>NumTerms</sup>
  *  <li>MetaLongsBlock --&gt; &lt; LongDelta<sup>LongsSize</sup>, BytesSize &gt; <sup>NumTerms</sup>
  *  <li>MetaBytesBlock --&gt; Byte <sup>MetaBytesBlockLength</sup>
- *  <li>Header --&gt; {@link CodecUtil#writeHeader CodecHeader}</li>
+ *  <li>Header --&gt; {@link CodecUtil#writeSegmentHeader CodecHeader}</li>
  *  <li>DirOffset --&gt; {@link DataOutput#writeLong Uint64}</li>
  *  <li>NumFields, FieldNumber, DocCount, DocFreq, LongsSize, 
  *        FieldNumber, DocCount --&gt; {@link DataOutput#writeVInt VInt}</li>
@@ -149,10 +148,11 @@
 public class FSTOrdTermsWriter extends FieldsConsumer {
   static final String TERMS_INDEX_EXTENSION = "tix";
   static final String TERMS_BLOCK_EXTENSION = "tbk";
-  static final String TERMS_CODEC_NAME = "FST_ORD_TERMS_DICT";
-  public static final int TERMS_VERSION_START = 0;
-  public static final int TERMS_VERSION_CHECKSUM = 1;
-  public static final int TERMS_VERSION_CURRENT = TERMS_VERSION_CHECKSUM;
+  static final String TERMS_CODEC_NAME = "FSTOrdTerms";
+  static final String TERMS_INDEX_CODEC_NAME = "FSTOrdIndex";
+
+  public static final int VERSION_START = 2;
+  public static final int VERSION_CURRENT = VERSION_START;
   public static final int SKIP_INTERVAL = 8;
   
   final PostingsWriterBase postingsWriter;
@@ -174,8 +174,10 @@
     try {
       this.indexOut = state.directory.createOutput(termsIndexFileName, state.context);
       this.blockOut = state.directory.createOutput(termsBlockFileName, state.context);
-      writeHeader(indexOut);
-      writeHeader(blockOut);
+      CodecUtil.writeSegmentHeader(indexOut, TERMS_INDEX_CODEC_NAME, VERSION_CURRENT, 
+                                             state.segmentInfo.getId(), state.segmentSuffix);
+      CodecUtil.writeSegmentHeader(blockOut, TERMS_CODEC_NAME, VERSION_CURRENT, 
+                                             state.segmentInfo.getId(), state.segmentSuffix);
       this.postingsWriter.init(blockOut); 
       success = true;
     } finally {
@@ -260,9 +262,6 @@
     }
   }
 
-  private void writeHeader(IndexOutput out) throws IOException {
-    CodecUtil.writeHeader(out, TERMS_CODEC_NAME, TERMS_VERSION_CURRENT);   
-  }
   private void writeTrailer(IndexOutput out, long dirStart) throws IOException {
     out.writeLong(dirStart);
   }
Index: lucene/codecs/src/java/org/apache/lucene/codecs/memory/FSTTermsReader.java
===================================================================
--- lucene/codecs/src/java/org/apache/lucene/codecs/memory/FSTTermsReader.java	(revision 1629405)
+++ lucene/codecs/src/java/org/apache/lucene/codecs/memory/FSTTermsReader.java	(working copy)
@@ -72,7 +72,6 @@
   final TreeMap<String, TermsReader> fields = new TreeMap<>();
   final PostingsReaderBase postingsReader;
   //static boolean TEST = false;
-  final int version;
 
   public FSTTermsReader(SegmentReadState state, PostingsReaderBase postingsReader) throws IOException {
     final String termsFileName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, FSTTermsWriter.TERMS_EXTENSION);
@@ -82,10 +81,11 @@
 
     boolean success = false;
     try {
-      version = readHeader(in);
-      if (version >= FSTTermsWriter.TERMS_VERSION_CHECKSUM) {
-        CodecUtil.checksumEntireFile(in);
-      }
+      CodecUtil.checkSegmentHeader(in, FSTTermsWriter.TERMS_CODEC_NAME,
+                                       FSTTermsWriter.TERMS_VERSION_START,
+                                       FSTTermsWriter.TERMS_VERSION_CURRENT,
+                                       state.segmentInfo.getId(), state.segmentSuffix);
+      CodecUtil.checksumEntireFile(in);
       this.postingsReader.init(in);
       seekDir(in);
 
@@ -113,17 +113,8 @@
     }
   }
 
-  private int readHeader(IndexInput in) throws IOException {
-    return CodecUtil.checkHeader(in, FSTTermsWriter.TERMS_CODEC_NAME,
-                                     FSTTermsWriter.TERMS_VERSION_START,
-                                     FSTTermsWriter.TERMS_VERSION_CURRENT);
-  }
   private void seekDir(IndexInput in) throws IOException {
-    if (version >= FSTTermsWriter.TERMS_VERSION_CHECKSUM) {
-      in.seek(in.length() - CodecUtil.footerLength() - 8);
-    } else {
-      in.seek(in.length() - 8);
-    }
+    in.seek(in.length() - CodecUtil.footerLength() - 8);
     in.seek(in.readLong());
   }
   private void checkFieldSummary(SegmentInfo info, IndexInput in, TermsReader field, TermsReader previous) throws IOException {
Index: lucene/codecs/src/java/org/apache/lucene/codecs/memory/FSTTermsWriter.java
===================================================================
--- lucene/codecs/src/java/org/apache/lucene/codecs/memory/FSTTermsWriter.java	(revision 1629405)
+++ lucene/codecs/src/java/org/apache/lucene/codecs/memory/FSTTermsWriter.java	(working copy)
@@ -39,7 +39,6 @@
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.FixedBitSet;
 import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.IntsRef;
 import org.apache.lucene.util.IntsRefBuilder;
 import org.apache.lucene.util.fst.Builder;
 import org.apache.lucene.util.fst.FST;
@@ -91,7 +90,7 @@
  *  <li>TermFST --&gt; {@link FST FST&lt;TermData&gt;}</li>
  *  <li>TermData --&gt; Flag, BytesSize?, LongDelta<sup>LongsSize</sup>?, Byte<sup>BytesSize</sup>?, 
  *                      &lt; DocFreq[Same?], (TotalTermFreq-DocFreq) &gt; ? </li>
- *  <li>Header --&gt; {@link CodecUtil#writeHeader CodecHeader}</li>
+ *  <li>Header --&gt; {@link CodecUtil#writeSegmentHeader SegmentHeader}</li>
  *  <li>DirOffset --&gt; {@link DataOutput#writeLong Uint64}</li>
  *  <li>DocFreq, LongsSize, BytesSize, NumFields,
  *        FieldNumber, DocCount --&gt; {@link DataOutput#writeVInt VInt}</li>
@@ -123,10 +122,9 @@
 
 public class FSTTermsWriter extends FieldsConsumer {
   static final String TERMS_EXTENSION = "tmp";
-  static final String TERMS_CODEC_NAME = "FST_TERMS_DICT";
-  public static final int TERMS_VERSION_START = 0;
-  public static final int TERMS_VERSION_CHECKSUM = 1;
-  public static final int TERMS_VERSION_CURRENT = TERMS_VERSION_CHECKSUM;
+  static final String TERMS_CODEC_NAME = "FSTTerms";
+  public static final int TERMS_VERSION_START = 2;
+  public static final int TERMS_VERSION_CURRENT = TERMS_VERSION_START;
   
   final PostingsWriterBase postingsWriter;
   final FieldInfos fieldInfos;
@@ -144,7 +142,9 @@
 
     boolean success = false;
     try {
-      writeHeader(out);
+      CodecUtil.writeSegmentHeader(out, TERMS_CODEC_NAME, TERMS_VERSION_CURRENT,
+                                        state.segmentInfo.getId(), state.segmentSuffix);   
+
       this.postingsWriter.init(out); 
       success = true;
     } finally {
@@ -154,10 +154,6 @@
     }
   }
 
-  private void writeHeader(IndexOutput out) throws IOException {
-    CodecUtil.writeHeader(out, TERMS_CODEC_NAME, TERMS_VERSION_CURRENT);   
-  }
-
   private void writeTrailer(IndexOutput out, long dirStart) throws IOException {
     out.writeLong(dirStart);
   }
Index: lucene/codecs/src/java/org/apache/lucene/codecs/memory/MemoryDocValuesConsumer.java
===================================================================
--- lucene/codecs/src/java/org/apache/lucene/codecs/memory/MemoryDocValuesConsumer.java	(revision 1629405)
+++ lucene/codecs/src/java/org/apache/lucene/codecs/memory/MemoryDocValuesConsumer.java	(working copy)
@@ -33,7 +33,6 @@
 import org.apache.lucene.util.ArrayUtil;
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.IntsRef;
 import org.apache.lucene.util.IntsRefBuilder;
 import org.apache.lucene.util.MathUtil;
 import org.apache.lucene.util.fst.Builder;
@@ -75,10 +74,10 @@
     try {
       String dataName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, dataExtension);
       data = state.directory.createOutput(dataName, state.context);
-      CodecUtil.writeHeader(data, dataCodec, VERSION_CURRENT);
+      CodecUtil.writeSegmentHeader(data, dataCodec, VERSION_CURRENT, state.segmentInfo.getId(), state.segmentSuffix);
       String metaName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, metaExtension);
       meta = state.directory.createOutput(metaName, state.context);
-      CodecUtil.writeHeader(meta, metaCodec, VERSION_CURRENT);
+      CodecUtil.writeSegmentHeader(meta, metaCodec, VERSION_CURRENT, state.segmentInfo.getId(), state.segmentSuffix);
       success = true;
     } finally {
       if (!success) {
Index: lucene/codecs/src/java/org/apache/lucene/codecs/memory/MemoryDocValuesProducer.java
===================================================================
--- lucene/codecs/src/java/org/apache/lucene/codecs/memory/MemoryDocValuesProducer.java	(revision 1629405)
+++ lucene/codecs/src/java/org/apache/lucene/codecs/memory/MemoryDocValuesProducer.java	(working copy)
@@ -93,6 +93,8 @@
   private final AtomicLong ramBytesUsed;
   private final int version;
   
+  private final boolean merging;
+  
   static final byte NUMBER = 0;
   static final byte BYTES = 1;
   static final byte FST = 2;
@@ -108,19 +110,44 @@
   static final byte BLOCK_COMPRESSED = 2;
   static final byte GCD_COMPRESSED = 3;
   
-  static final int VERSION_START = 3;
+  static final int VERSION_START = 4;
   static final int VERSION_CURRENT = VERSION_START;
+  
+  // clone for merge: when merging we don't do any instances.put()s
+  MemoryDocValuesProducer(MemoryDocValuesProducer original) throws IOException {
+    assert Thread.holdsLock(original);
+    numerics.putAll(original.numerics);
+    binaries.putAll(original.binaries);
+    fsts.putAll(original.fsts);
+    sortedSets.putAll(original.sortedSets);
+    sortedNumerics.putAll(original.sortedNumerics);
+    data = original.data.clone();
     
+    numericInstances.putAll(original.numericInstances);
+    pagedBytesInstances.putAll(original.pagedBytesInstances);
+    fstInstances.putAll(original.fstInstances);
+    docsWithFieldInstances.putAll(original.docsWithFieldInstances);
+    addresses.putAll(original.addresses);
+    
+    numericInfo.putAll(original.numericInfo);
+    
+    numEntries = original.numEntries;
+    maxDoc = original.maxDoc;
+    ramBytesUsed = new AtomicLong(original.ramBytesUsed.get());
+    version = original.version;
+    merging = true;
+  }
+    
   MemoryDocValuesProducer(SegmentReadState state, String dataCodec, String dataExtension, String metaCodec, String metaExtension) throws IOException {
     maxDoc = state.segmentInfo.getDocCount();
+    merging = false;
     String metaName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, metaExtension);
     // read in the entries from the metadata file.
     ChecksumIndexInput in = state.directory.openChecksumInput(metaName, state.context);
     boolean success = false;
     try {
-      version = CodecUtil.checkHeader(in, metaCodec, 
-                                      VERSION_START,
-                                      VERSION_CURRENT);
+      version = CodecUtil.checkSegmentHeader(in, metaCodec, VERSION_START, VERSION_CURRENT,
+                                                 state.segmentInfo.getId(), state.segmentSuffix);
       numEntries = readFields(in, state.fieldInfos);
       CodecUtil.checkFooter(in);
       ramBytesUsed = new AtomicLong(RamUsageEstimator.shallowSizeOfInstance(getClass()));
@@ -137,9 +164,8 @@
     this.data = state.directory.openInput(dataName, state.context);
     success = false;
     try {
-      final int version2 = CodecUtil.checkHeader(data, dataCodec, 
-                                                 VERSION_START,
-                                                 VERSION_CURRENT);
+      final int version2 = CodecUtil.checkSegmentHeader(data, dataCodec, VERSION_START, VERSION_CURRENT,
+                                                              state.segmentInfo.getId(), state.segmentSuffix);
       if (version != version2) {
         throw new CorruptIndexException("Format versions mismatch: meta=" + version + ", data=" + version2, data);
       }
@@ -257,7 +283,9 @@
     NumericDocValues instance = numericInstances.get(field.name);
     if (instance == null) {
       instance = loadNumeric(field);
-      numericInstances.put(field.name, instance);
+      if (!merging) {
+        numericInstances.put(field.name, instance);
+      }
     }
     return instance;
   }
@@ -284,6 +312,11 @@
   }
   
   @Override
+  public synchronized DocValuesProducer getMergeInstance() throws IOException {
+    return new MemoryDocValuesProducer(this);
+  }
+
+  @Override
   public String toString() {
     return getClass().getSimpleName() + "(entries=" + numEntries + ")";
   }
@@ -304,8 +337,10 @@
         final int formatID = data.readVInt();
         final int bitsPerValue = data.readVInt();
         final PackedInts.Reader ordsReader = PackedInts.getReaderNoHeader(data, PackedInts.Format.byId(formatID), entry.packedIntsVersion, (int)entry.count, bitsPerValue);
-        ramBytesUsed.addAndGet(RamUsageEstimator.sizeOf(decode) + ordsReader.ramBytesUsed());
-        numericInfo.put(field.name, Accountables.namedAccountable("table compressed", ordsReader));
+        if (!merging) {
+          ramBytesUsed.addAndGet(RamUsageEstimator.sizeOf(decode) + ordsReader.ramBytesUsed());
+          numericInfo.put(field.name, Accountables.namedAccountable("table compressed", ordsReader));
+        }
         return new NumericDocValues() {
           @Override
           public long get(int docID) {
@@ -317,8 +352,10 @@
         final int formatIDDelta = data.readVInt();
         final int bitsPerValueDelta = data.readVInt();
         final PackedInts.Reader deltaReader = PackedInts.getReaderNoHeader(data, PackedInts.Format.byId(formatIDDelta), entry.packedIntsVersion, (int)entry.count, bitsPerValueDelta);
-        ramBytesUsed.addAndGet(deltaReader.ramBytesUsed());
-        numericInfo.put(field.name, Accountables.namedAccountable("delta compressed", deltaReader));
+        if (!merging) {
+          ramBytesUsed.addAndGet(deltaReader.ramBytesUsed());
+          numericInfo.put(field.name, Accountables.namedAccountable("delta compressed", deltaReader));
+        }
         return new NumericDocValues() {
           @Override
           public long get(int docID) {
@@ -328,8 +365,10 @@
       case BLOCK_COMPRESSED:
         final int blockSize = data.readVInt();
         final BlockPackedReader reader = new BlockPackedReader(data, entry.packedIntsVersion, blockSize, entry.count, false);
-        ramBytesUsed.addAndGet(reader.ramBytesUsed());
-        numericInfo.put(field.name, Accountables.namedAccountable("block compressed", reader));
+        if (!merging) {
+          ramBytesUsed.addAndGet(reader.ramBytesUsed());
+          numericInfo.put(field.name, Accountables.namedAccountable("block compressed", reader));
+        }
         return reader;
       case GCD_COMPRESSED:
         final long min = data.readLong();
@@ -337,8 +376,10 @@
         final int formatIDGCD = data.readVInt();
         final int bitsPerValueGCD = data.readVInt();
         final PackedInts.Reader quotientReader = PackedInts.getReaderNoHeader(data, PackedInts.Format.byId(formatIDGCD), entry.packedIntsVersion, (int)entry.count, bitsPerValueGCD);
-        ramBytesUsed.addAndGet(quotientReader.ramBytesUsed());
-        numericInfo.put(field.name, Accountables.namedAccountable("gcd compressed", quotientReader));
+        if (!merging) {
+          ramBytesUsed.addAndGet(quotientReader.ramBytesUsed());
+          numericInfo.put(field.name, Accountables.namedAccountable("gcd compressed", quotientReader));
+        }
         return new NumericDocValues() {
           @Override
           public long get(int docID) {
@@ -359,7 +400,9 @@
       instance = pagedBytesInstances.get(field.name);
       if (instance == null) {
         instance = loadBinary(field);
-        pagedBytesInstances.put(field.name, instance);
+        if (!merging) {
+          pagedBytesInstances.put(field.name, instance);
+        }
       }
     }
     final PagedBytes.Reader bytesReader = instance.reader;
@@ -399,11 +442,15 @@
     PagedBytes bytes = new PagedBytes(16);
     bytes.copy(data, entry.numBytes);
     bytesAndAddresses.reader = bytes.freeze(true);
-    ramBytesUsed.addAndGet(bytesAndAddresses.reader.ramBytesUsed());
+    if (!merging) {
+      ramBytesUsed.addAndGet(bytesAndAddresses.reader.ramBytesUsed());
+    }
     if (entry.minLength != entry.maxLength) {
       data.seek(data.getFilePointer() + entry.missingBytes);
       bytesAndAddresses.addresses = MonotonicBlockPackedReader.of(data, entry.packedIntsVersion, entry.blockSize, maxDoc, false);
-      ramBytesUsed.addAndGet(bytesAndAddresses.addresses.ramBytesUsed());
+      if (!merging) {
+        ramBytesUsed.addAndGet(bytesAndAddresses.addresses.ramBytesUsed());
+      }
     }
     return bytesAndAddresses;
   }
@@ -420,8 +467,10 @@
       if (instance == null) {
         data.seek(entry.offset);
         instance = new FST<>(data, PositiveIntOutputs.getSingleton());
-        ramBytesUsed.addAndGet(instance.ramBytesUsed());
-        fstInstances.put(field.name, instance);
+        if (!merging) {
+          ramBytesUsed.addAndGet(instance.ramBytesUsed());
+          fstInstances.put(field.name, instance);
+        }
       }
     }
     final NumericDocValues docToOrd = getNumeric(field);
@@ -498,7 +547,10 @@
         if (res == null) {
           data.seek(entry.addressOffset);
           res = MonotonicBlockPackedReader.of(data, entry.packedIntsVersion, entry.blockSize, entry.valueCount, false);
-          addresses.put(field.name, res);
+          if (!merging) {
+            addresses.put(field.name, res);
+            ramBytesUsed.addAndGet(res.ramBytesUsed());
+          }
         }
         addr = res;
       }
@@ -567,8 +619,10 @@
       if (instance == null) {
         data.seek(entry.offset);
         instance = new FST<>(data, PositiveIntOutputs.getSingleton());
-        ramBytesUsed.addAndGet(instance.ramBytesUsed());
-        fstInstances.put(field.name, instance);
+        if (!merging) {
+          ramBytesUsed.addAndGet(instance.ramBytesUsed());
+          fstInstances.put(field.name, instance);
+        }
       }
     }
     final BinaryDocValues docToOrds = getBinary(field);
@@ -659,7 +713,10 @@
             bits[i] = data.readLong();
           }
           instance = new FixedBitSet(bits, maxDoc);
-          docsWithFieldInstances.put(field.name, instance);
+          if (!merging) {
+            docsWithFieldInstances.put(field.name, instance);
+            ramBytesUsed.addAndGet(instance.ramBytesUsed());
+          }
         }
       }
       return instance;
Index: lucene/codecs/src/java/org/apache/lucene/codecs/memory/MemoryPostingsFormat.java
===================================================================
--- lucene/codecs/src/java/org/apache/lucene/codecs/memory/MemoryPostingsFormat.java	(revision 1629405)
+++ lucene/codecs/src/java/org/apache/lucene/codecs/memory/MemoryPostingsFormat.java	(working copy)
@@ -29,6 +29,7 @@
 import org.apache.lucene.codecs.FieldsProducer;
 import org.apache.lucene.codecs.PostingsFormat;
 import org.apache.lucene.codecs.TermStats;
+import org.apache.lucene.index.CorruptIndexException;
 import org.apache.lucene.index.DocsAndPositionsEnum;
 import org.apache.lucene.index.DocsEnum;
 import org.apache.lucene.index.FieldInfo;
@@ -53,7 +54,6 @@
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.FixedBitSet;
 import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.IntsRef;
 import org.apache.lucene.util.IntsRefBuilder;
 import org.apache.lucene.util.RamUsageEstimator;
 import org.apache.lucene.util.fst.Builder;
@@ -276,7 +276,7 @@
 
   private static String EXTENSION = "ram";
   private static final String CODEC_NAME = "MemoryPostings";
-  private static final int VERSION_START = 0;
+  private static final int VERSION_START = 1;
   private static final int VERSION_CURRENT = VERSION_START;
 
   private class MemoryFieldsConsumer extends FieldsConsumer {
@@ -288,7 +288,7 @@
       out = state.directory.createOutput(fileName, state.context);
       boolean success = false;
       try {
-        CodecUtil.writeHeader(out, CODEC_NAME, VERSION_CURRENT);
+        CodecUtil.writeSegmentHeader(out, CODEC_NAME, VERSION_CURRENT, state.segmentInfo.getId(), state.segmentSuffix);
         success = true;
       } finally {
         if (!success) {
@@ -894,7 +894,9 @@
       this.termCount = termCount;
       final int fieldNumber = in.readVInt();
       field = fieldInfos.fieldInfo(fieldNumber);
-      if (field.getIndexOptions() != IndexOptions.DOCS_ONLY) {
+      if (field == null) {
+        throw new CorruptIndexException("invalid field number: " + fieldNumber, in);
+      } else if (field.getIndexOptions() != IndexOptions.DOCS_ONLY) {
         sumTotalTermFreq = in.readVLong();
       } else {
         sumTotalTermFreq = -1;
@@ -973,24 +975,27 @@
   @Override
   public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {
     final String fileName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, EXTENSION);
-    final ChecksumIndexInput in = state.directory.openChecksumInput(fileName, IOContext.READONCE);
 
     final SortedMap<String,TermsReader> fields = new TreeMap<>();
 
-    try {
-      CodecUtil.checkHeader(in, CODEC_NAME, VERSION_START, VERSION_CURRENT);
-      while(true) {
-        final int termCount = in.readVInt();
-        if (termCount == 0) {
-          break;
+    try (ChecksumIndexInput in = state.directory.openChecksumInput(fileName, IOContext.READONCE)) {
+      Throwable priorE = null;
+      try {
+        CodecUtil.checkSegmentHeader(in, CODEC_NAME, VERSION_START, VERSION_CURRENT, state.segmentInfo.getId(), state.segmentSuffix);
+        while(true) {
+          final int termCount = in.readVInt();
+          if (termCount == 0) {
+            break;
+          }
+          final TermsReader termsReader = new TermsReader(state.fieldInfos, in, termCount);
+          // System.out.println("load field=" + termsReader.field.name);
+          fields.put(termsReader.field.name, termsReader);
         }
-        final TermsReader termsReader = new TermsReader(state.fieldInfos, in, termCount);
-        // System.out.println("load field=" + termsReader.field.name);
-        fields.put(termsReader.field.name, termsReader);
+      } catch (Throwable exception) {
+        priorE = exception;
+      } finally {
+        CodecUtil.checkFooter(in, priorE);
       }
-      CodecUtil.checkFooter(in);
-    } finally {
-      in.close();
     }
 
     return new FieldsProducer() {
Index: lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextCodec.java
===================================================================
--- lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextCodec.java	(revision 1629405)
+++ lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextCodec.java	(working copy)
@@ -18,6 +18,7 @@
  */
 
 import org.apache.lucene.codecs.Codec;
+import org.apache.lucene.codecs.CompoundFormat;
 import org.apache.lucene.codecs.FieldInfosFormat;
 import org.apache.lucene.codecs.LiveDocsFormat;
 import org.apache.lucene.codecs.PostingsFormat;
@@ -42,6 +43,7 @@
   private final NormsFormat normsFormat = new SimpleTextNormsFormat();
   private final LiveDocsFormat liveDocs = new SimpleTextLiveDocsFormat();
   private final DocValuesFormat dvFormat = new SimpleTextDocValuesFormat();
+  private final CompoundFormat compoundFormat = new SimpleTextCompoundFormat();
   
   public SimpleTextCodec() {
     super("SimpleText");
@@ -86,4 +88,9 @@
   public DocValuesFormat docValuesFormat() {
     return dvFormat;
   }
+  
+  @Override
+  public CompoundFormat compoundFormat() {
+    return compoundFormat;
+  }
 }
Index: lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextCompoundFormat.java
===================================================================
--- lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextCompoundFormat.java	(revision 0)
+++ lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextCompoundFormat.java	(working copy)
@@ -0,0 +1,250 @@
+package org.apache.lucene.codecs.simpletext;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.FileNotFoundException;
+import java.io.IOException;
+import java.nio.charset.StandardCharsets;
+import java.text.DecimalFormat;
+import java.text.DecimalFormatSymbols;
+import java.text.ParseException;
+import java.util.Arrays;
+import java.util.Collection;
+import java.util.Locale;
+
+import org.apache.lucene.codecs.CompoundFormat;
+import org.apache.lucene.index.CorruptIndexException;
+import org.apache.lucene.index.MergeState.CheckAbort;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.store.BaseDirectory;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.store.Lock;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.BytesRefBuilder;
+import org.apache.lucene.util.StringHelper;
+
+/**
+ * plain text compound format.
+ * <p>
+ * <b><font color="red">FOR RECREATIONAL USE ONLY</font></B>
+ * @lucene.experimental
+ */
+public class SimpleTextCompoundFormat extends CompoundFormat {
+  
+  /** Sole constructor. */
+  public SimpleTextCompoundFormat() {
+  }
+
+  @Override
+  public Directory getCompoundReader(Directory dir, SegmentInfo si, IOContext context) throws IOException {
+    String dataFile = IndexFileNames.segmentFileName(si.name, "", DATA_EXTENSION);
+    final IndexInput in = dir.openInput(dataFile, context);
+    
+    BytesRefBuilder scratch = new BytesRefBuilder();
+
+    // first get to TOC:
+    DecimalFormat df = new DecimalFormat(OFFSETPATTERN, DecimalFormatSymbols.getInstance(Locale.ROOT));
+    long pos = in.length() - TABLEPOS.length - OFFSETPATTERN.length() - 1;
+    in.seek(pos);
+    SimpleTextUtil.readLine(in, scratch);
+    assert StringHelper.startsWith(scratch.get(), TABLEPOS);
+    long tablePos = -1; 
+    try {
+      tablePos = df.parse(stripPrefix(scratch, TABLEPOS)).longValue();
+    } catch (ParseException e) {
+      throw new CorruptIndexException("can't parse CFS trailer, got: " + scratch.get().utf8ToString(), in);
+    }
+    
+    // seek to TOC and read it
+    in.seek(tablePos);
+    SimpleTextUtil.readLine(in, scratch);
+    assert StringHelper.startsWith(scratch.get(), TABLE);
+    int numEntries = Integer.parseInt(stripPrefix(scratch, TABLE));
+    
+    final String fileNames[] = new String[numEntries];
+    final long startOffsets[] = new long[numEntries];
+    final long endOffsets[] = new long[numEntries];
+    
+    for (int i = 0; i < numEntries; i++) {
+      SimpleTextUtil.readLine(in, scratch);
+      assert StringHelper.startsWith(scratch.get(), TABLENAME);
+      fileNames[i] = si.name + IndexFileNames.stripSegmentName(stripPrefix(scratch, TABLENAME));
+      
+      if (i > 0) {
+        // files must be unique and in sorted order
+        assert fileNames[i].compareTo(fileNames[i-1]) > 0;
+      }
+      
+      SimpleTextUtil.readLine(in, scratch);
+      assert StringHelper.startsWith(scratch.get(), TABLESTART);
+      startOffsets[i] = Long.parseLong(stripPrefix(scratch, TABLESTART));
+      
+      SimpleTextUtil.readLine(in, scratch);
+      assert StringHelper.startsWith(scratch.get(), TABLEEND);
+      endOffsets[i] = Long.parseLong(stripPrefix(scratch, TABLEEND));
+    }
+    
+    return new BaseDirectory() {
+      
+      private int getIndex(String name) throws IOException {
+        int index = Arrays.binarySearch(fileNames, name);
+        if (index < 0) {
+          throw new FileNotFoundException("No sub-file found (fileName=" + name + " files: " + Arrays.toString(fileNames) + ")");
+        }
+        return index;
+      }
+      
+      @Override
+      public String[] listAll() throws IOException {
+        ensureOpen();
+        return fileNames.clone();
+      }
+      
+      @Override
+      public long fileLength(String name) throws IOException {
+        ensureOpen();
+        int index = getIndex(name);
+        return endOffsets[index] - startOffsets[index];
+      }
+      
+      @Override
+      public IndexInput openInput(String name, IOContext context) throws IOException {
+        ensureOpen();
+        int index = getIndex(name);
+        return in.slice(name, startOffsets[index], endOffsets[index] - startOffsets[index]);
+      }
+      
+      @Override
+      public void close() throws IOException {
+        isOpen = false;
+        in.close();
+      }
+      
+      // write methods: disabled
+      
+      @Override
+      public IndexOutput createOutput(String name, IOContext context) { throw new UnsupportedOperationException(); }
+      
+      @Override
+      public void sync(Collection<String> names) { throw new UnsupportedOperationException(); }
+      
+      @Override
+      public void deleteFile(String name) { throw new UnsupportedOperationException(); }
+      
+      @Override
+      public void renameFile(String source, String dest) { throw new UnsupportedOperationException(); }
+      
+      @Override
+      public Lock makeLock(String name) { throw new UnsupportedOperationException(); }
+      
+      @Override
+      public void clearLock(String name) { throw new UnsupportedOperationException(); }
+    };
+  }
+
+  @Override
+  public void write(Directory dir, SegmentInfo si, Collection<String> files, CheckAbort checkAbort, IOContext context) throws IOException {
+    String dataFile = IndexFileNames.segmentFileName(si.name, "", DATA_EXTENSION);
+    
+    int numFiles = files.size();
+    String names[] = files.toArray(new String[numFiles]);
+    Arrays.sort(names);
+    long startOffsets[] = new long[numFiles];
+    long endOffsets[] = new long[numFiles];
+    
+    BytesRefBuilder scratch = new BytesRefBuilder();
+    
+    try (IndexOutput out = dir.createOutput(dataFile, context)) { 
+      for (int i = 0; i < names.length; i++) {
+        // write header for file
+        SimpleTextUtil.write(out, HEADER);
+        SimpleTextUtil.write(out, names[i], scratch);
+        SimpleTextUtil.writeNewline(out);
+        
+        // write bytes for file
+        startOffsets[i] = out.getFilePointer();
+        try (IndexInput in = dir.openInput(names[i], IOContext.READONCE)) {
+          out.copyBytes(in, in.length());
+        }
+        endOffsets[i] = out.getFilePointer();
+        
+        checkAbort.work(endOffsets[i] - startOffsets[i]);
+      }
+      
+      long tocPos = out.getFilePointer();
+      
+      // write CFS table
+      SimpleTextUtil.write(out, TABLE);
+      SimpleTextUtil.write(out, Integer.toString(numFiles), scratch);
+      SimpleTextUtil.writeNewline(out);
+     
+      for (int i = 0; i < names.length; i++) {
+        SimpleTextUtil.write(out, TABLENAME);
+        SimpleTextUtil.write(out, names[i], scratch);
+        SimpleTextUtil.writeNewline(out);
+        
+        SimpleTextUtil.write(out, TABLESTART);
+        SimpleTextUtil.write(out, Long.toString(startOffsets[i]), scratch);
+        SimpleTextUtil.writeNewline(out);
+
+        SimpleTextUtil.write(out, TABLEEND);
+        SimpleTextUtil.write(out, Long.toString(endOffsets[i]), scratch);
+        SimpleTextUtil.writeNewline(out);
+      }
+      
+      DecimalFormat df = new DecimalFormat(OFFSETPATTERN, DecimalFormatSymbols.getInstance(Locale.ROOT));
+      SimpleTextUtil.write(out, TABLEPOS);
+      SimpleTextUtil.write(out, df.format(tocPos), scratch);
+      SimpleTextUtil.writeNewline(out);
+    }
+  }
+
+  @Override
+  public String[] files(SegmentInfo si) {
+    return new String[] { IndexFileNames.segmentFileName(si.name, "", DATA_EXTENSION) };
+  }
+  
+  // helper method to strip strip away 'prefix' from 'scratch' and return as String
+  private String stripPrefix(BytesRefBuilder scratch, BytesRef prefix) throws IOException {
+    return new String(scratch.bytes(), prefix.length, scratch.length() - prefix.length, StandardCharsets.UTF_8);
+  }
+  
+  /** Extension of compound file */
+  static final String DATA_EXTENSION = "scf";
+  
+  final static BytesRef HEADER  = new BytesRef("cfs entry for: ");
+  
+  final static BytesRef TABLE =      new BytesRef("table of contents, size: ");
+  final static BytesRef TABLENAME =  new BytesRef("  filename: ");
+  final static BytesRef TABLESTART = new BytesRef("    start: ");
+  final static BytesRef TABLEEND =   new BytesRef("    end: ");
+  
+  final static BytesRef TABLEPOS = new BytesRef("table of contents begins at offset: ");
+  
+  final static String OFFSETPATTERN;
+  static {
+    int numDigits = Long.toString(Long.MAX_VALUE).length();
+    char pattern[] = new char[numDigits];
+    Arrays.fill(pattern, '0');
+    OFFSETPATTERN = new String(pattern);
+  }
+}

Property changes on: lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextCompoundFormat.java
___________________________________________________________________
Added: svn:eol-style
## -0,0 +1 ##
+native
\ No newline at end of property
Index: lucene/codecs/src/test/org/apache/lucene/codecs/simpletext/TestSimpleTextCompoundFormat.java
===================================================================
--- lucene/codecs/src/test/org/apache/lucene/codecs/simpletext/TestSimpleTextCompoundFormat.java	(revision 0)
+++ lucene/codecs/src/test/org/apache/lucene/codecs/simpletext/TestSimpleTextCompoundFormat.java	(working copy)
@@ -0,0 +1,30 @@
+package org.apache.lucene.codecs.simpletext;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.codecs.Codec;
+import org.apache.lucene.index.BaseCompoundFormatTestCase;
+
+public class TestSimpleTextCompoundFormat extends BaseCompoundFormatTestCase {
+  private final Codec codec = new SimpleTextCodec();
+
+  @Override
+  protected Codec getCodec() {
+    return codec;
+  }
+}

Property changes on: lucene/codecs/src/test/org/apache/lucene/codecs/simpletext/TestSimpleTextCompoundFormat.java
___________________________________________________________________
Added: svn:eol-style
## -0,0 +1 ##
+native
\ No newline at end of property
Index: lucene/core/src/java/org/apache/lucene/codecs/Codec.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/codecs/Codec.java	(revision 1629405)
+++ lucene/core/src/java/org/apache/lucene/codecs/Codec.java	(working copy)
@@ -86,6 +86,9 @@
   /** Encodes/decodes live docs */
   public abstract LiveDocsFormat liveDocsFormat();
   
+  /** Encodes/decodes compound files */
+  public abstract CompoundFormat compoundFormat();
+  
   /** looks up a codec by name */
   public static Codec forName(String name) {
     if (loader == null) {
Index: lucene/core/src/java/org/apache/lucene/codecs/CodecUtil.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/codecs/CodecUtil.java	(revision 1629405)
+++ lucene/core/src/java/org/apache/lucene/codecs/CodecUtil.java	(working copy)
@@ -19,6 +19,7 @@
 
 
 import java.io.IOException;
+import java.nio.charset.StandardCharsets;
 import java.util.Arrays;
 
 import org.apache.lucene.index.CorruptIndexException;
@@ -96,34 +97,46 @@
    * Writes a codec header for a per-segment, which records both a string to
    * identify the file, a version number, and the unique ID of the segment. 
    * This header can be parsed and validated with 
-   * {@link #checkSegmentHeader(DataInput, String, int, int, byte[]) checkSegmentHeader()}.
+   * {@link #checkSegmentHeader(DataInput, String, int, int, byte[], String) checkSegmentHeader()}.
    * <p>
-   * CodecSegmentHeader --&gt; CodecHeader,SegmentID
+   * CodecSegmentHeader --&gt; CodecHeader,SegmentID,SegmentSuffix
    * <ul>
-   *    <li>CodecHeader --&gt; {@link #writeHeader}
-   *    <li>SegmentID   --&gt; {@link DataOutput#writeByte byte}<sup>16</sup>.
-   *        Unique identifier for the segment.
+   *    <li>CodecHeader   --&gt; {@link #writeHeader}
+   *    <li>SegmentID     --&gt; {@link DataOutput#writeByte byte}<sup>16</sup>
+   *    <li>SegmentSuffix --&gt; SuffixLength,SuffixBytes
+   *    <li>SuffixLength  --&gt; {@link DataOutput#writeByte byte}
+   *    <li>SuffixBytes   --&gt; {@link DataOutput#writeByte byte}<sup>SuffixLength</sup>
    * </ul>
    * <p>
    * Note that the length of a segment header depends only upon the
-   * name of the codec, so this length can be computed at any time
-   * with {@link #headerLength(String)}.
+   * name of the codec and suffix, so this length can be computed at any time
+   * with {@link #segmentHeaderLength(String,String)}.
    * 
    * @param out Output stream
    * @param codec String to identify this file. It should be simple ASCII, 
    *              less than 128 characters in length.
    * @param segmentID Unique identifier for the segment
+   * @param segmentSuffix auxiliary suffix for the file. It should be simple ASCII,
+   *              less than 256 characters in length.
    * @param version Version number
    * @throws IOException If there is an I/O error writing to the underlying medium.
    * @throws IllegalArgumentException If the codec name is not simple ASCII, or 
-   *         is more than 127 characters in length, or if segmentID is invalid.
+   *         is more than 127 characters in length, or if segmentID is invalid,
+   *         or if the segmentSuffix is not simple ASCII, or more than 255 characters
+   *         in length.
    */
-  public static void writeSegmentHeader(DataOutput out, String codec, int version, byte[] segmentID) throws IOException {
+  public static void writeSegmentHeader(DataOutput out, String codec, int version, byte[] segmentID, String segmentSuffix) throws IOException {
     if (segmentID.length != StringHelper.ID_LENGTH) {
       throw new IllegalArgumentException("Invalid id: " + StringHelper.idToString(segmentID));
     }
     writeHeader(out, codec, version);
     out.writeBytes(segmentID, 0, segmentID.length);
+    BytesRef suffixBytes = new BytesRef(segmentSuffix);
+    if (suffixBytes.length != segmentSuffix.length() || suffixBytes.length >= 256) {
+      throw new IllegalArgumentException("codec must be simple ASCII, less than 256 characters in length [got " + segmentSuffix + "]");
+    }
+    out.writeByte((byte)suffixBytes.length);
+    out.writeBytes(suffixBytes.bytes, suffixBytes.offset, suffixBytes.length);
   }
 
   /**
@@ -142,10 +155,10 @@
    * 
    * @param codec Codec name.
    * @return length of the entire segment header.
-   * @see #writeSegmentHeader(DataOutput, String, int, byte[])
+   * @see #writeSegmentHeader(DataOutput, String, int, byte[], String)
    */
-  public static int segmentHeaderLength(String codec) {
-    return headerLength(codec) + StringHelper.ID_LENGTH;
+  public static int segmentHeaderLength(String codec, String segmentSuffix) {
+    return headerLength(codec) + StringHelper.ID_LENGTH + 1 + segmentSuffix.length();
   }
 
   /**
@@ -207,7 +220,7 @@
   
   /**
    * Reads and validates a header previously written with 
-   * {@link #writeSegmentHeader(DataOutput, String, int, byte[])}.
+   * {@link #writeSegmentHeader(DataOutput, String, int, byte[], String)}.
    * <p>
    * When reading a file, supply the expected <code>codec</code>,
    * expected version range (<code>minVersion to maxVersion</code>),
@@ -220,6 +233,7 @@
    * @param minVersion The minimum supported expected version number.
    * @param maxVersion The maximum supported expected version number.
    * @param segmentID The expected segment this file belongs to.
+   * @param segmentSuffix The expected auxiliary segment suffix for this file.
    * @return The actual version found, when a valid header is found 
    *         that matches <code>codec</code>, with an actual version 
    *         where <code>minVersion <= actual <= maxVersion</code>, 
@@ -228,15 +242,15 @@
    * @throws CorruptIndexException If the first four bytes are not
    *         {@link #CODEC_MAGIC}, or if the actual codec found is
    *         not <code>codec</code>, or if the <code>segmentID</code>
-   *         does not match.
+   *         or <code>segmentSuffix</code> do not match.
    * @throws IndexFormatTooOldException If the actual version is less 
    *         than <code>minVersion</code>.
    * @throws IndexFormatTooNewException If the actual version is greater 
    *         than <code>maxVersion</code>.
    * @throws IOException If there is an I/O error reading from the underlying medium.
-   * @see #writeSegmentHeader(DataOutput, String, int, byte[])
+   * @see #writeSegmentHeader(DataOutput, String, int, byte[],String)
    */
-  public static int checkSegmentHeader(DataInput in, String codec, int minVersion, int maxVersion, byte[] segmentID) throws IOException {
+  public static int checkSegmentHeader(DataInput in, String codec, int minVersion, int maxVersion, byte[] segmentID, String segmentSuffix) throws IOException {
     int version = checkHeader(in, codec, minVersion, maxVersion);
     byte id[] = new byte[StringHelper.ID_LENGTH];
     in.readBytes(id, 0, id.length);
@@ -244,6 +258,14 @@
       throw new CorruptIndexException("file mismatch, expected segment id=" + StringHelper.idToString(segmentID) 
                                                                  + ", got=" + StringHelper.idToString(id), in);
     }
+    int suffixLength = in.readByte() & 0xFF;
+    byte suffixBytes[] = new byte[suffixLength];
+    in.readBytes(suffixBytes, 0, suffixBytes.length);
+    String suffix = new String(suffixBytes, 0, suffixBytes.length, StandardCharsets.UTF_8);
+    if (!suffix.equals(segmentSuffix)) {
+      throw new CorruptIndexException("file mismatch, expected segment suffix=" + segmentSuffix
+                                                                     + ", got=" + suffix, in);
+    }
     return version;
   }
   
Index: lucene/core/src/java/org/apache/lucene/codecs/CompoundFormat.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/codecs/CompoundFormat.java	(revision 0)
+++ lucene/core/src/java/org/apache/lucene/codecs/CompoundFormat.java	(working copy)
@@ -0,0 +1,59 @@
+package org.apache.lucene.codecs;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Collection;
+
+import org.apache.lucene.index.MergeState.CheckAbort;
+import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+
+/**
+ * Encodes/decodes compound files
+ * @lucene.experimental
+ */
+public abstract class CompoundFormat {
+
+  /** Sole constructor. (For invocation by subclass 
+   *  constructors, typically implicit.) */
+  public CompoundFormat() {
+  }
+  
+  // TODO: this is very minimal. If we need more methods,
+  // we can add 'producer' classes.
+  
+  /**
+   * Returns a Directory view (read-only) for the compound files in this segment
+   */
+  public abstract Directory getCompoundReader(Directory dir, SegmentInfo si, IOContext context) throws IOException;
+  
+  /**
+   * Packs the provided files into a compound format.
+   */
+  // TODO: get checkAbort out of here, and everywhere, and have iw do it at a higher level
+  public abstract void write(Directory dir, SegmentInfo si, Collection<String> files, CheckAbort checkAbort, IOContext context) throws IOException;
+
+  /**
+   * Returns the compound file names used by this segment.
+   */
+  // TODO: get this out of here, and use trackingdirwrapper. but this is really scary in IW right now...
+  // NOTE: generally si.useCompoundFile is not even yet 'set' when this is called.
+  public abstract String[] files(SegmentInfo si);
+}

Property changes on: lucene/core/src/java/org/apache/lucene/codecs/CompoundFormat.java
___________________________________________________________________
Added: svn:eol-style
## -0,0 +1 ##
+native
\ No newline at end of property
Index: lucene/core/src/java/org/apache/lucene/codecs/DocValuesConsumer.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/codecs/DocValuesConsumer.java	(revision 1629405)
+++ lucene/core/src/java/org/apache/lucene/codecs/DocValuesConsumer.java	(working copy)
@@ -24,7 +24,6 @@
 import java.util.List;
 import java.util.NoSuchElementException;
 
-import org.apache.lucene.index.LeafReader;
 import org.apache.lucene.index.BinaryDocValues;
 import org.apache.lucene.index.FieldInfo;
 import org.apache.lucene.index.FilteredTermsEnum;
@@ -131,67 +130,110 @@
    *  Implementations can override this method 
    *  for more sophisticated merging (bulk-byte copying, etc). */
   public void merge(MergeState mergeState) throws IOException {
-    for (FieldInfo field : mergeState.fieldInfos) {
-      DocValuesType type = field.getDocValuesType();
+    for(DocValuesProducer docValuesProducer : mergeState.docValuesProducers) {
+      if (docValuesProducer != null) {
+        docValuesProducer.checkIntegrity();
+      }
+    }
+
+    for (FieldInfo mergeFieldInfo : mergeState.mergeFieldInfos) {
+      DocValuesType type = mergeFieldInfo.getDocValuesType();
       if (type != null) {
         if (type == DocValuesType.NUMERIC) {
           List<NumericDocValues> toMerge = new ArrayList<>();
           List<Bits> docsWithField = new ArrayList<>();
-          for (LeafReader reader : mergeState.readers) {
-            NumericDocValues values = reader.getNumericDocValues(field.name);
-            Bits bits = reader.getDocsWithField(field.name);
+          for (int i=0;i<mergeState.docValuesProducers.length;i++) {
+            NumericDocValues values = null;
+            Bits bits = null;
+            DocValuesProducer docValuesProducer = mergeState.docValuesProducers[i];
+            if (docValuesProducer != null) {
+              FieldInfo fieldInfo = mergeState.fieldInfos[i].fieldInfo(mergeFieldInfo.name);
+              if (fieldInfo != null && fieldInfo.getDocValuesType() == DocValuesType.NUMERIC) {
+                values = docValuesProducer.getNumeric(fieldInfo);
+                bits = docValuesProducer.getDocsWithField(fieldInfo);
+              }
+            }
             if (values == null) {
               values = DocValues.emptyNumeric();
-              bits = new Bits.MatchNoBits(reader.maxDoc());
+              bits = new Bits.MatchNoBits(mergeState.maxDocs[i]);
             }
             toMerge.add(values);
             docsWithField.add(bits);
           }
-          mergeNumericField(field, mergeState, toMerge, docsWithField);
+          mergeNumericField(mergeFieldInfo, mergeState, toMerge, docsWithField);
         } else if (type == DocValuesType.BINARY) {
           List<BinaryDocValues> toMerge = new ArrayList<>();
           List<Bits> docsWithField = new ArrayList<>();
-          for (LeafReader reader : mergeState.readers) {
-            BinaryDocValues values = reader.getBinaryDocValues(field.name);
-            Bits bits = reader.getDocsWithField(field.name);
+          for (int i=0;i<mergeState.docValuesProducers.length;i++) {
+            BinaryDocValues values = null;
+            Bits bits = null;
+            DocValuesProducer docValuesProducer = mergeState.docValuesProducers[i];
+            if (docValuesProducer != null) {
+              FieldInfo fieldInfo = mergeState.fieldInfos[i].fieldInfo(mergeFieldInfo.name);
+              if (fieldInfo != null && fieldInfo.getDocValuesType() == DocValuesType.BINARY) {
+                values = docValuesProducer.getBinary(fieldInfo);
+                bits = docValuesProducer.getDocsWithField(fieldInfo);
+              }
+            }
             if (values == null) {
               values = DocValues.emptyBinary();
-              bits = new Bits.MatchNoBits(reader.maxDoc());
+              bits = new Bits.MatchNoBits(mergeState.maxDocs[i]);
             }
             toMerge.add(values);
             docsWithField.add(bits);
           }
-          mergeBinaryField(field, mergeState, toMerge, docsWithField);
+          mergeBinaryField(mergeFieldInfo, mergeState, toMerge, docsWithField);
         } else if (type == DocValuesType.SORTED) {
           List<SortedDocValues> toMerge = new ArrayList<>();
-          for (LeafReader reader : mergeState.readers) {
-            SortedDocValues values = reader.getSortedDocValues(field.name);
+          for (int i=0;i<mergeState.docValuesProducers.length;i++) {
+            SortedDocValues values = null;
+            DocValuesProducer docValuesProducer = mergeState.docValuesProducers[i];
+            if (docValuesProducer != null) {
+              FieldInfo fieldInfo = mergeState.fieldInfos[i].fieldInfo(mergeFieldInfo.name);
+              if (fieldInfo != null && fieldInfo.getDocValuesType() == DocValuesType.SORTED) {
+                values = docValuesProducer.getSorted(fieldInfo);
+              }
+            }
             if (values == null) {
               values = DocValues.emptySorted();
             }
             toMerge.add(values);
           }
-          mergeSortedField(field, mergeState, toMerge);
+          mergeSortedField(mergeFieldInfo, mergeState, toMerge);
         } else if (type == DocValuesType.SORTED_SET) {
           List<SortedSetDocValues> toMerge = new ArrayList<>();
-          for (LeafReader reader : mergeState.readers) {
-            SortedSetDocValues values = reader.getSortedSetDocValues(field.name);
+          for (int i=0;i<mergeState.docValuesProducers.length;i++) {
+            SortedSetDocValues values = null;
+            DocValuesProducer docValuesProducer = mergeState.docValuesProducers[i];
+            if (docValuesProducer != null) {
+              FieldInfo fieldInfo = mergeState.fieldInfos[i].fieldInfo(mergeFieldInfo.name);
+              if (fieldInfo != null && fieldInfo.getDocValuesType() == DocValuesType.SORTED_SET) {
+                values = docValuesProducer.getSortedSet(fieldInfo);
+              }
+            }
             if (values == null) {
               values = DocValues.emptySortedSet();
             }
             toMerge.add(values);
           }
-          mergeSortedSetField(field, mergeState, toMerge);
+          mergeSortedSetField(mergeFieldInfo, mergeState, toMerge);
         } else if (type == DocValuesType.SORTED_NUMERIC) {
           List<SortedNumericDocValues> toMerge = new ArrayList<>();
-          for (LeafReader reader : mergeState.readers) {
-            SortedNumericDocValues values = reader.getSortedNumericDocValues(field.name);
+          for (int i=0;i<mergeState.docValuesProducers.length;i++) {
+            SortedNumericDocValues values = null;
+            DocValuesProducer docValuesProducer = mergeState.docValuesProducers[i];
+            if (docValuesProducer != null) {
+              FieldInfo fieldInfo = mergeState.fieldInfos[i].fieldInfo(mergeFieldInfo.name);
+              if (fieldInfo != null && fieldInfo.getDocValuesType() == DocValuesType.SORTED_NUMERIC) {
+                values = docValuesProducer.getSortedNumeric(fieldInfo);
+              }
+            }
             if (values == null) {
-              values = DocValues.emptySortedNumeric(reader.maxDoc());
+              values = DocValues.emptySortedNumeric(mergeState.maxDocs[i]);
             }
             toMerge.add(values);
           }
-          mergeSortedNumericField(field, mergeState, toMerge);
+          mergeSortedNumericField(mergeFieldInfo, mergeState, toMerge);
         } else {
           throw new AssertionError("type=" + type);
         }
@@ -216,7 +258,7 @@
                           int docIDUpto;
                           long nextValue;
                           boolean nextHasValue;
-                          LeafReader currentReader;
+                          int currentMaxDoc;
                           NumericDocValues currentValues;
                           Bits currentLiveDocs;
                           Bits currentDocsWithField;
@@ -248,13 +290,13 @@
                                 return false;
                               }
 
-                              if (currentReader == null || docIDUpto == currentReader.maxDoc()) {
+                              if (docIDUpto == currentMaxDoc) {
                                 readerUpto++;
                                 if (readerUpto < toMerge.size()) {
-                                  currentReader = mergeState.readers.get(readerUpto);
                                   currentValues = toMerge.get(readerUpto);
-                                  currentLiveDocs = currentReader.getLiveDocs();
                                   currentDocsWithField = docsWithField.get(readerUpto);
+                                  currentLiveDocs = mergeState.liveDocs[readerUpto];
+                                  currentMaxDoc = mergeState.maxDocs[readerUpto];
                                 }
                                 docIDUpto = 0;
                                 continue;
@@ -297,7 +339,7 @@
                          int docIDUpto;
                          BytesRef nextValue;
                          BytesRef nextPointer; // points to null if missing, or nextValue
-                         LeafReader currentReader;
+                         int currentMaxDoc;
                          BinaryDocValues currentValues;
                          Bits currentLiveDocs;
                          Bits currentDocsWithField;
@@ -329,13 +371,13 @@
                                return false;
                              }
 
-                             if (currentReader == null || docIDUpto == currentReader.maxDoc()) {
+                             if (docIDUpto == currentMaxDoc) {
                                readerUpto++;
                                if (readerUpto < toMerge.size()) {
-                                 currentReader = mergeState.readers.get(readerUpto);
                                  currentValues = toMerge.get(readerUpto);
                                  currentDocsWithField = docsWithField.get(readerUpto);
-                                 currentLiveDocs = currentReader.getLiveDocs();
+                                 currentLiveDocs = mergeState.liveDocs[readerUpto];
+                                 currentMaxDoc = mergeState.maxDocs[readerUpto];
                                }
                                docIDUpto = 0;
                                continue;
@@ -368,8 +410,8 @@
    * iterables that filter deleted documents.
    */
   public void mergeSortedNumericField(FieldInfo fieldInfo, final MergeState mergeState, List<SortedNumericDocValues> toMerge) throws IOException {
-    final LeafReader readers[] = mergeState.readers.toArray(new LeafReader[toMerge.size()]);
-    final SortedNumericDocValues dvs[] = toMerge.toArray(new SortedNumericDocValues[toMerge.size()]);
+    final int numReaders = toMerge.size();
+    final SortedNumericDocValues dvs[] = toMerge.toArray(new SortedNumericDocValues[numReaders]);
     
     // step 3: add field
     addSortedNumericField(fieldInfo,
@@ -381,7 +423,7 @@
               int readerUpto = -1;
               int docIDUpto;
               int nextValue;
-              LeafReader currentReader;
+              int currentMaxDoc;
               Bits currentLiveDocs;
               boolean nextIsSet;
 
@@ -407,15 +449,15 @@
 
               private boolean setNext() {
                 while (true) {
-                  if (readerUpto == readers.length) {
+                  if (readerUpto == numReaders) {
                     return false;
                   }
 
-                  if (currentReader == null || docIDUpto == currentReader.maxDoc()) {
+                  if (docIDUpto == currentMaxDoc) {
                     readerUpto++;
-                    if (readerUpto < readers.length) {
-                      currentReader = readers[readerUpto];
-                      currentLiveDocs = currentReader.getLiveDocs();
+                    if (readerUpto < numReaders) {
+                      currentLiveDocs = mergeState.liveDocs[readerUpto];
+                      currentMaxDoc = mergeState.maxDocs[readerUpto];
                     }
                     docIDUpto = 0;
                     continue;
@@ -444,7 +486,7 @@
               int readerUpto = -1;
               int docIDUpto;
               long nextValue;
-              LeafReader currentReader;
+              int currentMaxDoc;
               Bits currentLiveDocs;
               boolean nextIsSet;
               int valueUpto;
@@ -472,7 +514,7 @@
 
               private boolean setNext() {
                 while (true) {
-                  if (readerUpto == readers.length) {
+                  if (readerUpto == numReaders) {
                     return false;
                   }
                   
@@ -483,11 +525,11 @@
                     return true;
                   }
 
-                  if (currentReader == null || docIDUpto == currentReader.maxDoc()) {
+                  if (docIDUpto == currentMaxDoc) {
                     readerUpto++;
-                    if (readerUpto < readers.length) {
-                      currentReader = readers[readerUpto];
-                      currentLiveDocs = currentReader.getLiveDocs();
+                    if (readerUpto < numReaders) {
+                      currentLiveDocs = mergeState.liveDocs[readerUpto];
+                      currentMaxDoc = mergeState.maxDocs[readerUpto];
                     }
                     docIDUpto = 0;
                     continue;
@@ -494,7 +536,7 @@
                   }
                   
                   if (currentLiveDocs == null || currentLiveDocs.get(docIDUpto)) {
-                    assert docIDUpto < currentReader.maxDoc();
+                    assert docIDUpto < currentMaxDoc;
                     SortedNumericDocValues dv = dvs[readerUpto];
                     dv.setDocument(docIDUpto);
                     valueUpto = 0;
@@ -519,22 +561,22 @@
    * an Iterable that merges ordinals and values and filters deleted documents .
    */
   public void mergeSortedField(FieldInfo fieldInfo, final MergeState mergeState, List<SortedDocValues> toMerge) throws IOException {
-    final LeafReader readers[] = mergeState.readers.toArray(new LeafReader[toMerge.size()]);
-    final SortedDocValues dvs[] = toMerge.toArray(new SortedDocValues[toMerge.size()]);
+    final int numReaders = toMerge.size();
+    final SortedDocValues dvs[] = toMerge.toArray(new SortedDocValues[numReaders]);
     
     // step 1: iterate thru each sub and mark terms still in use
     TermsEnum liveTerms[] = new TermsEnum[dvs.length];
     long[] weights = new long[liveTerms.length];
-    for (int sub = 0; sub < liveTerms.length; sub++) {
-      LeafReader reader = readers[sub];
+    for (int sub=0;sub<numReaders;sub++) {
       SortedDocValues dv = dvs[sub];
-      Bits liveDocs = reader.getLiveDocs();
+      Bits liveDocs = mergeState.liveDocs[sub];
+      int maxDoc = mergeState.maxDocs[sub];
       if (liveDocs == null) {
         liveTerms[sub] = dv.termsEnum();
         weights[sub] = dv.getValueCount();
       } else {
         LongBitSet bitset = new LongBitSet(dv.getValueCount());
-        for (int i = 0; i < reader.maxDoc(); i++) {
+        for (int i = 0; i < maxDoc; i++) {
           if (liveDocs.get(i)) {
             int ord = dv.getOrd(i);
             if (ord >= 0) {
@@ -591,7 +633,7 @@
               int readerUpto = -1;
               int docIDUpto;
               int nextValue;
-              LeafReader currentReader;
+              int currentMaxDoc;
               Bits currentLiveDocs;
               LongValues currentMap;
               boolean nextIsSet;
@@ -619,16 +661,16 @@
 
               private boolean setNext() {
                 while (true) {
-                  if (readerUpto == readers.length) {
+                  if (readerUpto == numReaders) {
                     return false;
                   }
 
-                  if (currentReader == null || docIDUpto == currentReader.maxDoc()) {
+                  if (docIDUpto == currentMaxDoc) {
                     readerUpto++;
-                    if (readerUpto < readers.length) {
-                      currentReader = readers[readerUpto];
-                      currentLiveDocs = currentReader.getLiveDocs();
+                    if (readerUpto < numReaders) {
                       currentMap = map.getGlobalOrds(readerUpto);
+                      currentLiveDocs = mergeState.liveDocs[readerUpto];
+                      currentMaxDoc = mergeState.maxDocs[readerUpto];
                     }
                     docIDUpto = 0;
                     continue;
@@ -658,22 +700,22 @@
    * an Iterable that merges ordinals and values and filters deleted documents .
    */
   public void mergeSortedSetField(FieldInfo fieldInfo, final MergeState mergeState, List<SortedSetDocValues> toMerge) throws IOException {
-    final LeafReader readers[] = mergeState.readers.toArray(new LeafReader[toMerge.size()]);
     final SortedSetDocValues dvs[] = toMerge.toArray(new SortedSetDocValues[toMerge.size()]);
-    
+    final int numReaders = mergeState.maxDocs.length;
+
     // step 1: iterate thru each sub and mark terms still in use
     TermsEnum liveTerms[] = new TermsEnum[dvs.length];
     long[] weights = new long[liveTerms.length];
     for (int sub = 0; sub < liveTerms.length; sub++) {
-      LeafReader reader = readers[sub];
       SortedSetDocValues dv = dvs[sub];
-      Bits liveDocs = reader.getLiveDocs();
+      Bits liveDocs = mergeState.liveDocs[sub];
+      int maxDoc = mergeState.maxDocs[sub];
       if (liveDocs == null) {
         liveTerms[sub] = dv.termsEnum();
         weights[sub] = dv.getValueCount();
       } else {
         LongBitSet bitset = new LongBitSet(dv.getValueCount());
-        for (int i = 0; i < reader.maxDoc(); i++) {
+        for (int i = 0; i < maxDoc; i++) {
           if (liveDocs.get(i)) {
             dv.setDocument(i);
             long ord;
@@ -731,7 +773,7 @@
               int readerUpto = -1;
               int docIDUpto;
               int nextValue;
-              LeafReader currentReader;
+              int currentMaxDoc;
               Bits currentLiveDocs;
               boolean nextIsSet;
 
@@ -758,15 +800,15 @@
 
               private boolean setNext() {
                 while (true) {
-                  if (readerUpto == readers.length) {
+                  if (readerUpto == numReaders) {
                     return false;
                   }
 
-                  if (currentReader == null || docIDUpto == currentReader.maxDoc()) {
+                  if (docIDUpto == currentMaxDoc) {
                     readerUpto++;
-                    if (readerUpto < readers.length) {
-                      currentReader = readers[readerUpto];
-                      currentLiveDocs = currentReader.getLiveDocs();
+                    if (readerUpto < numReaders) {
+                      currentLiveDocs = mergeState.liveDocs[readerUpto];
+                      currentMaxDoc = mergeState.maxDocs[readerUpto];
                     }
                     docIDUpto = 0;
                     continue;
@@ -798,7 +840,7 @@
               int readerUpto = -1;
               int docIDUpto;
               long nextValue;
-              LeafReader currentReader;
+              int currentMaxDoc;
               Bits currentLiveDocs;
               LongValues currentMap;
               boolean nextIsSet;
@@ -829,7 +871,7 @@
 
               private boolean setNext() {
                 while (true) {
-                  if (readerUpto == readers.length) {
+                  if (readerUpto == numReaders) {
                     return false;
                   }
                   
@@ -840,12 +882,12 @@
                     return true;
                   }
 
-                  if (currentReader == null || docIDUpto == currentReader.maxDoc()) {
+                  if (docIDUpto == currentMaxDoc) {
                     readerUpto++;
-                    if (readerUpto < readers.length) {
-                      currentReader = readers[readerUpto];
-                      currentLiveDocs = currentReader.getLiveDocs();
+                    if (readerUpto < numReaders) {
                       currentMap = map.getGlobalOrds(readerUpto);
+                      currentLiveDocs = mergeState.liveDocs[readerUpto];
+                      currentMaxDoc = mergeState.maxDocs[readerUpto];
                     }
                     docIDUpto = 0;
                     continue;
@@ -852,7 +894,7 @@
                   }
                   
                   if (currentLiveDocs == null || currentLiveDocs.get(docIDUpto)) {
-                    assert docIDUpto < currentReader.maxDoc();
+                    assert docIDUpto < currentMaxDoc;
                     SortedSetDocValues dv = dvs[readerUpto];
                     dv.setDocument(docIDUpto);
                     ordUpto = ordLength = 0;
Index: lucene/core/src/java/org/apache/lucene/codecs/DocValuesProducer.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/codecs/DocValuesProducer.java	(revision 1629405)
+++ lucene/core/src/java/org/apache/lucene/codecs/DocValuesProducer.java	(working copy)
@@ -79,4 +79,12 @@
    * @lucene.internal
    */
   public abstract void checkIntegrity() throws IOException;
+  
+  /** 
+   * Returns an instance optimized for merging.
+   * <p>
+   * The default implementation returns {@code this} */
+  public DocValuesProducer getMergeInstance() throws IOException {
+    return this;
+  }
 }
Index: lucene/core/src/java/org/apache/lucene/codecs/FieldsConsumer.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/codecs/FieldsConsumer.java	(revision 1629405)
+++ lucene/core/src/java/org/apache/lucene/codecs/FieldsConsumer.java	(working copy)
@@ -22,7 +22,6 @@
 import java.util.ArrayList;
 import java.util.List;
 
-import org.apache.lucene.index.LeafReader;
 import org.apache.lucene.index.Fields;
 import org.apache.lucene.index.MappedMultiFields;
 import org.apache.lucene.index.MergeState;
@@ -90,11 +89,12 @@
 
     int docBase = 0;
 
-    for(int readerIndex=0;readerIndex<mergeState.readers.size();readerIndex++) {
-      final LeafReader reader = mergeState.readers.get(readerIndex);
-      final Fields f = reader.fields();
-      final int maxDoc = reader.maxDoc();
+    for(int readerIndex=0;readerIndex<mergeState.fieldsProducers.length;readerIndex++) {
+      final FieldsProducer f = mergeState.fieldsProducers[readerIndex];
+
+      final int maxDoc = mergeState.maxDocs[readerIndex];
       if (f != null) {
+        f.checkIntegrity();
         slices.add(new ReaderSlice(docBase, maxDoc, readerIndex));
         fields.add(f);
       }
Index: lucene/core/src/java/org/apache/lucene/codecs/FieldsProducer.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/codecs/FieldsProducer.java	(revision 1629405)
+++ lucene/core/src/java/org/apache/lucene/codecs/FieldsProducer.java	(working copy)
@@ -46,4 +46,12 @@
    * @lucene.internal
    */
   public abstract void checkIntegrity() throws IOException;
+  
+  /** 
+   * Returns an instance optimized for merging.
+   * <p>
+   * The default implementation returns {@code this} */
+  public FieldsProducer getMergeInstance() throws IOException {
+    return this;
+  }
 }
Index: lucene/core/src/java/org/apache/lucene/codecs/FilterCodec.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/codecs/FilterCodec.java	(revision 1629405)
+++ lucene/core/src/java/org/apache/lucene/codecs/FilterCodec.java	(working copy)
@@ -99,4 +99,8 @@
     return delegate.termVectorsFormat();
   }
 
+  @Override
+  public CompoundFormat compoundFormat() {
+    return delegate.compoundFormat();
+  }
 }
Index: lucene/core/src/java/org/apache/lucene/codecs/NormsConsumer.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/codecs/NormsConsumer.java	(revision 1629405)
+++ lucene/core/src/java/org/apache/lucene/codecs/NormsConsumer.java	(working copy)
@@ -24,7 +24,6 @@
 import java.util.List;
 import java.util.NoSuchElementException;
 
-import org.apache.lucene.index.LeafReader;
 import org.apache.lucene.index.DocValues;
 import org.apache.lucene.index.FieldInfo;
 import org.apache.lucene.index.MergeState;
@@ -72,17 +71,29 @@
    *  Implementations can override this method 
    *  for more sophisticated merging (bulk-byte copying, etc). */
   public void merge(MergeState mergeState) throws IOException {
-    for (FieldInfo field : mergeState.fieldInfos) {
-      if (field.hasNorms()) {
+    for(NormsProducer normsProducer : mergeState.normsProducers) {
+      if (normsProducer != null) {
+        normsProducer.checkIntegrity();
+      }
+    }
+    for (FieldInfo mergeFieldInfo : mergeState.mergeFieldInfos) {
+      if (mergeFieldInfo.hasNorms()) {
         List<NumericDocValues> toMerge = new ArrayList<>();
-        for (LeafReader reader : mergeState.readers) {
-          NumericDocValues norms = reader.getNormValues(field.name);
+        for (int i=0;i<mergeState.normsProducers.length;i++) {
+          NormsProducer normsProducer = mergeState.normsProducers[i];
+          NumericDocValues norms = null;
+          if (normsProducer != null) {
+            FieldInfo fieldInfo = mergeState.fieldInfos[i].fieldInfo(mergeFieldInfo.name);
+            if (fieldInfo != null && fieldInfo.hasNorms()) {
+              norms = normsProducer.getNorms(fieldInfo);
+            }
+          }
           if (norms == null) {
             norms = DocValues.emptyNumeric();
           }
           toMerge.add(norms);
         }
-        mergeNormsField(field, mergeState, toMerge);
+        mergeNormsField(mergeFieldInfo, mergeState, toMerge);
       }
     }
   }
@@ -104,7 +115,7 @@
                           int readerUpto = -1;
                           int docIDUpto;
                           long nextValue;
-                          LeafReader currentReader;
+                          int maxDoc;
                           NumericDocValues currentValues;
                           Bits currentLiveDocs;
                           boolean nextIsSet;
@@ -135,12 +146,12 @@
                                 return false;
                               }
 
-                              if (currentReader == null || docIDUpto == currentReader.maxDoc()) {
+                              if (currentValues == null || docIDUpto == maxDoc) {
                                 readerUpto++;
                                 if (readerUpto < toMerge.size()) {
-                                  currentReader = mergeState.readers.get(readerUpto);
                                   currentValues = toMerge.get(readerUpto);
-                                  currentLiveDocs = currentReader.getLiveDocs();
+                                  currentLiveDocs = mergeState.liveDocs[readerUpto];
+                                  maxDoc = mergeState.maxDocs[readerUpto];
                                 }
                                 docIDUpto = 0;
                                 continue;
Index: lucene/core/src/java/org/apache/lucene/codecs/NormsProducer.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/codecs/NormsProducer.java	(revision 1629405)
+++ lucene/core/src/java/org/apache/lucene/codecs/NormsProducer.java	(working copy)
@@ -47,4 +47,12 @@
    * @lucene.internal
    */
   public abstract void checkIntegrity() throws IOException;
+  
+  /** 
+   * Returns an instance optimized for merging.
+   * <p>
+   * The default implementation returns {@code this} */
+  public NormsProducer getMergeInstance() throws IOException {
+    return this;
+  }
 }
Index: lucene/core/src/java/org/apache/lucene/codecs/StoredFieldsReader.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/codecs/StoredFieldsReader.java	(revision 1629405)
+++ lucene/core/src/java/org/apache/lucene/codecs/StoredFieldsReader.java	(working copy)
@@ -36,8 +36,8 @@
   protected StoredFieldsReader() {
   }
   
-  /** Visit the stored fields for document <code>n</code> */
-  public abstract void visitDocument(int n, StoredFieldVisitor visitor) throws IOException;
+  /** Visit the stored fields for document <code>docID</code> */
+  public abstract void visitDocument(int docID, StoredFieldVisitor visitor) throws IOException;
 
   @Override
   public abstract StoredFieldsReader clone();
@@ -50,4 +50,12 @@
    * @lucene.internal
    */
   public abstract void checkIntegrity() throws IOException;
+  
+  /** 
+   * Returns an instance optimized for merging.
+   * <p>
+   * The default implementation returns {@code this} */
+  public StoredFieldsReader getMergeInstance() throws IOException {
+    return this;
+  }
 }
Index: lucene/core/src/java/org/apache/lucene/codecs/StoredFieldsWriter.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/codecs/StoredFieldsWriter.java	(revision 1629405)
+++ lucene/core/src/java/org/apache/lucene/codecs/StoredFieldsWriter.java	(working copy)
@@ -19,6 +19,7 @@
 import java.io.Closeable;
 import java.io.IOException;
 
+import org.apache.lucene.document.DocumentStoredFieldVisitor;
 import org.apache.lucene.index.FieldInfo;
 import org.apache.lucene.index.FieldInfos;
 import org.apache.lucene.index.MergeState;
@@ -25,7 +26,6 @@
 import org.apache.lucene.index.StorableField;
 import org.apache.lucene.index.StoredDocument;
 import org.apache.lucene.util.Bits;
-import org.apache.lucene.index.LeafReader;
 
 /**
  * Codec API for writing stored fields:
@@ -82,11 +82,13 @@
    *  merging (bulk-byte copying, etc). */
   public int merge(MergeState mergeState) throws IOException {
     int docCount = 0;
-    for (LeafReader reader : mergeState.readers) {
-      final int maxDoc = reader.maxDoc();
-      final Bits liveDocs = reader.getLiveDocs();
-      for (int i = 0; i < maxDoc; i++) {
-        if (liveDocs != null && !liveDocs.get(i)) {
+    for (int i=0;i<mergeState.storedFieldsReaders.length;i++) {
+      StoredFieldsReader storedFieldsReader = mergeState.storedFieldsReaders[i];
+      storedFieldsReader.checkIntegrity();
+      int maxDoc = mergeState.maxDocs[i];
+      Bits liveDocs = mergeState.liveDocs[i];
+      for (int docID=0;docID<maxDoc;docID++) {
+        if (liveDocs != null && !liveDocs.get(docID)) {
           // skip deleted docs
           continue;
         }
@@ -96,13 +98,15 @@
         // on the fly?
         // NOTE: it's very important to first assign to doc then pass it to
         // fieldsWriter.addDocument; see LUCENE-1282
-        StoredDocument doc = reader.document(i);
-        addDocument(doc, mergeState.fieldInfos);
+        DocumentStoredFieldVisitor visitor = new DocumentStoredFieldVisitor();
+        storedFieldsReader.visitDocument(docID, visitor);
+        StoredDocument doc = visitor.getDocument();
+        addDocument(doc, mergeState.mergeFieldInfos);
         docCount++;
         mergeState.checkAbort.work(300);
       }
     }
-    finish(mergeState.fieldInfos, docCount);
+    finish(mergeState.mergeFieldInfos, docCount);
     return docCount;
   }
   
Index: lucene/core/src/java/org/apache/lucene/codecs/TermVectorsReader.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/codecs/TermVectorsReader.java	(revision 1629405)
+++ lucene/core/src/java/org/apache/lucene/codecs/TermVectorsReader.java	(working copy)
@@ -56,4 +56,12 @@
    *  read term vectors. */
   @Override
   public abstract TermVectorsReader clone();
+  
+  /** 
+   * Returns an instance optimized for merging.
+   * <p>
+   * The default implementation returns {@code this} */
+  public TermVectorsReader getMergeInstance() throws IOException {
+    return this;
+  }
 }
Index: lucene/core/src/java/org/apache/lucene/codecs/TermVectorsWriter.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/codecs/TermVectorsWriter.java	(revision 1629405)
+++ lucene/core/src/java/org/apache/lucene/codecs/TermVectorsWriter.java	(working copy)
@@ -21,7 +21,6 @@
 import java.io.IOException;
 import java.util.Iterator;
 
-import org.apache.lucene.index.LeafReader;
 import org.apache.lucene.index.DocsAndPositionsEnum;
 import org.apache.lucene.index.FieldInfo;
 import org.apache.lucene.index.FieldInfos;
@@ -177,12 +176,16 @@
    *  merging (bulk-byte copying, etc). */
   public int merge(MergeState mergeState) throws IOException {
     int docCount = 0;
-    for (int i = 0; i < mergeState.readers.size(); i++) {
-      final LeafReader reader = mergeState.readers.get(i);
-      final int maxDoc = reader.maxDoc();
-      final Bits liveDocs = reader.getLiveDocs();
+    int numReaders = mergeState.maxDocs.length;
+    for (int i = 0; i < numReaders; i++) {
+      int maxDoc = mergeState.maxDocs[i];
+      Bits liveDocs = mergeState.liveDocs[i];
+      TermVectorsReader termVectorsReader = mergeState.termVectorsReaders[i];
+      if (termVectorsReader != null) {
+        termVectorsReader.checkIntegrity();
+      }
 
-      for (int docID = 0; docID < maxDoc; docID++) {
+      for (int docID=0;docID<maxDoc;docID++) {
         if (liveDocs != null && !liveDocs.get(docID)) {
           // skip deleted docs
           continue;
@@ -189,13 +192,18 @@
         }
         // NOTE: it's very important to first assign to vectors then pass it to
         // termVectorsWriter.addAllDocVectors; see LUCENE-1282
-        Fields vectors = reader.getTermVectors(docID);
+        Fields vectors;
+        if (termVectorsReader == null) {
+          vectors = null;
+        } else {
+          vectors = termVectorsReader.get(docID);
+        }
         addAllDocVectors(vectors, mergeState);
         docCount++;
         mergeState.checkAbort.work(300);
       }
     }
-    finish(mergeState.fieldInfos, docCount);
+    finish(mergeState.mergeFieldInfos, docCount);
     return docCount;
   }
   
@@ -227,7 +235,7 @@
     int fieldCount = 0;
     for(String fieldName : vectors) {
       fieldCount++;
-      final FieldInfo fieldInfo = mergeState.fieldInfos.fieldInfo(fieldName);
+      final FieldInfo fieldInfo = mergeState.mergeFieldInfos.fieldInfo(fieldName);
 
       assert lastFieldName == null || fieldName.compareTo(lastFieldName) > 0: "lastFieldName=" + lastFieldName + " fieldName=" + fieldName;
       lastFieldName = fieldName;
Index: lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsFormat.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsFormat.java	(revision 1629405)
+++ lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsFormat.java	(working copy)
@@ -64,7 +64,7 @@
    * <p>
    * <code>formatName</code> is the name of the format. This name will be used
    * in the file formats to perform
-   * {@link CodecUtil#checkHeader(org.apache.lucene.store.DataInput, String, int, int) codec header checks}.
+   * {@link CodecUtil#checkSegmentHeader codec header checks}.
    * <p>
    * <code>segmentSuffix</code> is the segment suffix. This suffix is added to 
    * the result file name only if it's not the empty string.
Index: lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsReader.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsReader.java	(revision 1629405)
+++ lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsReader.java	(working copy)
@@ -27,8 +27,6 @@
 import static org.apache.lucene.codecs.compressing.CompressingStoredFieldsWriter.STRING;
 import static org.apache.lucene.codecs.compressing.CompressingStoredFieldsWriter.TYPE_BITS;
 import static org.apache.lucene.codecs.compressing.CompressingStoredFieldsWriter.TYPE_MASK;
-import static org.apache.lucene.codecs.compressing.CompressingStoredFieldsWriter.VERSION_BIG_CHUNKS;
-import static org.apache.lucene.codecs.compressing.CompressingStoredFieldsWriter.VERSION_CHECKSUM;
 import static org.apache.lucene.codecs.compressing.CompressingStoredFieldsWriter.VERSION_CURRENT;
 import static org.apache.lucene.codecs.compressing.CompressingStoredFieldsWriter.VERSION_START;
 import static org.apache.lucene.codecs.compressing.CompressingStoredFieldsWriter.FIELDS_EXTENSION;
@@ -109,66 +107,61 @@
     boolean success = false;
     fieldInfos = fn;
     numDocs = si.getDocCount();
-    ChecksumIndexInput indexStream = null;
-    try {
-      final String indexStreamFN = IndexFileNames.segmentFileName(segment, segmentSuffix, FIELDS_INDEX_EXTENSION);
-      final String fieldsStreamFN = IndexFileNames.segmentFileName(segment, segmentSuffix, FIELDS_EXTENSION);
-      // Load the index into memory
-      indexStream = d.openChecksumInput(indexStreamFN, context);
-      final String codecNameIdx = formatName + CODEC_SFX_IDX;
-      version = CodecUtil.checkHeader(indexStream, codecNameIdx, VERSION_START, VERSION_CURRENT);
-      assert CodecUtil.headerLength(codecNameIdx) == indexStream.getFilePointer();
-      indexReader = new CompressingStoredFieldsIndexReader(indexStream, si);
-
-      long maxPointer = -1;
-      
-      if (version >= VERSION_CHECKSUM) {
+    
+    int version = -1;
+    long maxPointer = -1;
+    CompressingStoredFieldsIndexReader indexReader = null;
+    
+    // Load the index into memory
+    final String indexName = IndexFileNames.segmentFileName(segment, segmentSuffix, FIELDS_INDEX_EXTENSION);    
+    try (ChecksumIndexInput indexStream = d.openChecksumInput(indexName, context)) {
+      Throwable priorE = null;
+      try {
+        final String codecNameIdx = formatName + CODEC_SFX_IDX;
+        version = CodecUtil.checkSegmentHeader(indexStream, codecNameIdx, VERSION_START, VERSION_CURRENT, si.getId(), segmentSuffix);
+        assert CodecUtil.segmentHeaderLength(codecNameIdx, segmentSuffix) == indexStream.getFilePointer();
+        indexReader = new CompressingStoredFieldsIndexReader(indexStream, si);
         maxPointer = indexStream.readVLong();
-        CodecUtil.checkFooter(indexStream);
-      } else {
-        CodecUtil.checkEOF(indexStream);
+      } catch (Throwable exception) {
+        priorE = exception;
+      } finally {
+        CodecUtil.checkFooter(indexStream, priorE);
       }
-      indexStream.close();
-      indexStream = null;
+    }
+    
+    this.version = version;
+    this.maxPointer = maxPointer;
+    this.indexReader = indexReader;
 
+    final String fieldsStreamFN = IndexFileNames.segmentFileName(segment, segmentSuffix, FIELDS_EXTENSION);
+    try {
       // Open the data file and read metadata
       fieldsStream = d.openInput(fieldsStreamFN, context);
-      if (version >= VERSION_CHECKSUM) {
-        if (maxPointer + CodecUtil.footerLength() != fieldsStream.length()) {
-          throw new CorruptIndexException("Invalid fieldsStream maxPointer (file truncated?): maxPointer=" + maxPointer + ", length=" + fieldsStream.length(), fieldsStream);
-        }
-      } else {
-        maxPointer = fieldsStream.length();
+      if (maxPointer + CodecUtil.footerLength() != fieldsStream.length()) {
+        throw new CorruptIndexException("Invalid fieldsStream maxPointer (file truncated?): maxPointer=" + maxPointer + ", length=" + fieldsStream.length(), fieldsStream);
       }
-      this.maxPointer = maxPointer;
       final String codecNameDat = formatName + CODEC_SFX_DAT;
-      final int fieldsVersion = CodecUtil.checkHeader(fieldsStream, codecNameDat, VERSION_START, VERSION_CURRENT);
+      final int fieldsVersion = CodecUtil.checkSegmentHeader(fieldsStream, codecNameDat, VERSION_START, VERSION_CURRENT, si.getId(), segmentSuffix);
       if (version != fieldsVersion) {
         throw new CorruptIndexException("Version mismatch between stored fields index and data: " + version + " != " + fieldsVersion, fieldsStream);
       }
-      assert CodecUtil.headerLength(codecNameDat) == fieldsStream.getFilePointer();
+      assert CodecUtil.segmentHeaderLength(codecNameDat, segmentSuffix) == fieldsStream.getFilePointer();
 
-      if (version >= VERSION_BIG_CHUNKS) {
-        chunkSize = fieldsStream.readVInt();
-      } else {
-        chunkSize = -1;
-      }
+      chunkSize = fieldsStream.readVInt();
       packedIntsVersion = fieldsStream.readVInt();
       decompressor = compressionMode.newDecompressor();
       this.bytes = new BytesRef();
       
-      if (version >= VERSION_CHECKSUM) {
-        // NOTE: data file is too costly to verify checksum against all the bytes on open,
-        // but for now we at least verify proper structure of the checksum footer: which looks
-        // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption
-        // such as file truncation.
-        CodecUtil.retrieveChecksum(fieldsStream);
-      }
+      // NOTE: data file is too costly to verify checksum against all the bytes on open,
+      // but for now we at least verify proper structure of the checksum footer: which looks
+      // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption
+      // such as file truncation.
+      CodecUtil.retrieveChecksum(fieldsStream);
 
       success = true;
     } finally {
       if (!success) {
-        IOUtils.closeWhileHandlingException(this, indexStream);
+        IOUtils.closeWhileHandlingException(this);
       }
     }
   }
@@ -310,7 +303,7 @@
     }
 
     final DataInput documentInput;
-    if (version >= VERSION_BIG_CHUNKS && totalLength >= 2 * chunkSize) {
+    if (totalLength >= 2 * chunkSize) {
       assert chunkSize > 0;
       assert offset < chunkSize;
 
@@ -497,7 +490,7 @@
     void decompress() throws IOException {
       // decompress data
       final int chunkSize = chunkSize();
-      if (version >= VERSION_BIG_CHUNKS && chunkSize >= 2 * CompressingStoredFieldsReader.this.chunkSize) {
+      if (chunkSize >= 2 * CompressingStoredFieldsReader.this.chunkSize) {
         bytes.offset = bytes.length = 0;
         for (int decompressed = 0; decompressed < chunkSize; ) {
           final int toDecompress = Math.min(chunkSize - decompressed, CompressingStoredFieldsReader.this.chunkSize);
@@ -519,10 +512,8 @@
      * Check integrity of the data. The iterator is not usable after this method has been called.
      */
     void checkIntegrity() throws IOException {
-      if (version >= VERSION_CHECKSUM) {
-        fieldsStream.seek(fieldsStream.length() - CodecUtil.footerLength());
-        CodecUtil.checkFooter(fieldsStream);
-      }
+      fieldsStream.seek(fieldsStream.length() - CodecUtil.footerLength());
+      CodecUtil.checkFooter(fieldsStream);
     }
 
   }
@@ -539,9 +530,7 @@
 
   @Override
   public void checkIntegrity() throws IOException {
-    if (version >= VERSION_CHECKSUM) {
-      CodecUtil.checksumEntireFile(fieldsStream);
-    }
+    CodecUtil.checksumEntireFile(fieldsStream);
   }
 
   @Override
Index: lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsWriter.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsWriter.java	(revision 1629405)
+++ lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsWriter.java	(working copy)
@@ -24,11 +24,13 @@
 import org.apache.lucene.codecs.StoredFieldsReader;
 import org.apache.lucene.codecs.StoredFieldsWriter;
 import org.apache.lucene.codecs.compressing.CompressingStoredFieldsReader.ChunkIterator;
-import org.apache.lucene.index.LeafReader;
+import org.apache.lucene.document.DocumentStoredFieldVisitor;
 import org.apache.lucene.index.CorruptIndexException;
 import org.apache.lucene.index.FieldInfo;
 import org.apache.lucene.index.FieldInfos;
 import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.LeafReader;
 import org.apache.lucene.index.MergeState;
 import org.apache.lucene.index.SegmentInfo;
 import org.apache.lucene.index.SegmentReader;
@@ -73,9 +75,7 @@
   static final String CODEC_SFX_IDX = "Index";
   static final String CODEC_SFX_DAT = "Data";
   static final int VERSION_START = 0;
-  static final int VERSION_BIG_CHUNKS = 1;
-  static final int VERSION_CHECKSUM = 2;
-  static final int VERSION_CURRENT = VERSION_CHECKSUM;
+  static final int VERSION_CURRENT = VERSION_START;
 
   private final Directory directory;
   private final String segment;
@@ -118,10 +118,10 @@
 
       final String codecNameIdx = formatName + CODEC_SFX_IDX;
       final String codecNameDat = formatName + CODEC_SFX_DAT;
-      CodecUtil.writeHeader(indexStream, codecNameIdx, VERSION_CURRENT);
-      CodecUtil.writeHeader(fieldsStream, codecNameDat, VERSION_CURRENT);
-      assert CodecUtil.headerLength(codecNameDat) == fieldsStream.getFilePointer();
-      assert CodecUtil.headerLength(codecNameIdx) == indexStream.getFilePointer();
+      CodecUtil.writeSegmentHeader(indexStream, codecNameIdx, VERSION_CURRENT, si.getId(), segmentSuffix);
+      CodecUtil.writeSegmentHeader(fieldsStream, codecNameDat, VERSION_CURRENT, si.getId(), segmentSuffix);
+      assert CodecUtil.segmentHeaderLength(codecNameDat, segmentSuffix) == fieldsStream.getFilePointer();
+      assert CodecUtil.segmentHeaderLength(codecNameIdx, segmentSuffix) == indexStream.getFilePointer();
 
       indexWriter = new CompressingStoredFieldsIndexWriter(indexStream);
       indexStream = null;
@@ -333,15 +333,14 @@
   @Override
   public int merge(MergeState mergeState) throws IOException {
     int docCount = 0;
-    int idx = 0;
+    int numReaders = mergeState.maxDocs.length;
     
     MatchingReaders matching = new MatchingReaders(mergeState);
     
-    for (LeafReader reader : mergeState.readers) {
-      final SegmentReader matchingSegmentReader = matching.matchingSegmentReaders[idx++];
+    for (int readerIndex=0;readerIndex<numReaders;readerIndex++) {
       CompressingStoredFieldsReader matchingFieldsReader = null;
-      if (matchingSegmentReader != null) {
-        final StoredFieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();
+      if (matching.matchingReaders[readerIndex]) {
+        final StoredFieldsReader fieldsReader = mergeState.storedFieldsReaders[readerIndex];
         // we can only bulk-copy if the matching reader is also a CompressingStoredFieldsReader
         if (fieldsReader != null && fieldsReader instanceof CompressingStoredFieldsReader) {
           matchingFieldsReader = (CompressingStoredFieldsReader) fieldsReader;
@@ -348,8 +347,8 @@
         }
       }
 
-      final int maxDoc = reader.maxDoc();
-      final Bits liveDocs = reader.getLiveDocs();
+      final int maxDoc = mergeState.maxDocs[readerIndex];
+      final Bits liveDocs = mergeState.liveDocs[readerIndex];
 
       if (matchingFieldsReader == null
           || matchingFieldsReader.getVersion() != VERSION_CURRENT // means reader version is not the same as the writer version
@@ -356,9 +355,14 @@
           || matchingFieldsReader.getCompressionMode() != compressionMode
           || matchingFieldsReader.getChunkSize() != chunkSize) { // the way data is decompressed depends on the chunk size
         // naive merge...
+        StoredFieldsReader storedFieldsReader = mergeState.storedFieldsReaders[readerIndex];
+        if (storedFieldsReader != null) {
+          storedFieldsReader.checkIntegrity();
+        }
         for (int i = nextLiveDoc(0, liveDocs, maxDoc); i < maxDoc; i = nextLiveDoc(i + 1, liveDocs, maxDoc)) {
-          StoredDocument doc = reader.document(i);
-          addDocument(doc, mergeState.fieldInfos);
+          DocumentStoredFieldVisitor visitor = new DocumentStoredFieldVisitor();
+          storedFieldsReader.visitDocument(i, visitor);
+          addDocument(visitor.getDocument(), mergeState.mergeFieldInfos);
           ++docCount;
           mergeState.checkAbort.work(300);
         }
@@ -400,7 +404,7 @@
         }
       }
     }
-    finish(mergeState.fieldInfos, docCount);
+    finish(mergeState.mergeFieldInfos, docCount);
     return docCount;
   }
 
Index: lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingTermVectorsFormat.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingTermVectorsFormat.java	(revision 1629405)
+++ lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingTermVectorsFormat.java	(working copy)
@@ -46,7 +46,7 @@
    * <p>
    * <code>formatName</code> is the name of the format. This name will be used
    * in the file formats to perform
-   * {@link CodecUtil#checkHeader(org.apache.lucene.store.DataInput, String, int, int) codec header checks}.
+   * {@link CodecUtil#checkSegmentHeader codec header checks}.
    * <p>
    * The <code>compressionMode</code> parameter allows you to choose between
    * compression algorithms that have various compression and decompression
Index: lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingTermVectorsReader.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingTermVectorsReader.java	(revision 1629405)
+++ lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingTermVectorsReader.java	(working copy)
@@ -28,7 +28,6 @@
 import static org.apache.lucene.codecs.compressing.CompressingTermVectorsWriter.VECTORS_INDEX_EXTENSION;
 import static org.apache.lucene.codecs.compressing.CompressingTermVectorsWriter.VERSION_CURRENT;
 import static org.apache.lucene.codecs.compressing.CompressingTermVectorsWriter.VERSION_START;
-import static org.apache.lucene.codecs.compressing.CompressingTermVectorsWriter.VERSION_CHECKSUM;
 
 import java.io.Closeable;
 import java.io.IOException;
@@ -106,44 +105,47 @@
     boolean success = false;
     fieldInfos = fn;
     numDocs = si.getDocCount();
-    ChecksumIndexInput indexStream = null;
-    try {
-      // Load the index into memory
-      final String indexStreamFN = IndexFileNames.segmentFileName(segment, segmentSuffix, VECTORS_INDEX_EXTENSION);
-      indexStream = d.openChecksumInput(indexStreamFN, context);
-      final String codecNameIdx = formatName + CODEC_SFX_IDX;
-      version = CodecUtil.checkHeader(indexStream, codecNameIdx, VERSION_START, VERSION_CURRENT);
-      assert CodecUtil.headerLength(codecNameIdx) == indexStream.getFilePointer();
-      indexReader = new CompressingStoredFieldsIndexReader(indexStream, si);
-      
-      if (version >= VERSION_CHECKSUM) {
-        indexStream.readVLong(); // the end of the data file
-        CodecUtil.checkFooter(indexStream);
-      } else {
-        CodecUtil.checkEOF(indexStream);
+    int version = -1;
+    CompressingStoredFieldsIndexReader indexReader = null;
+    
+    // Load the index into memory
+    final String indexName = IndexFileNames.segmentFileName(segment, segmentSuffix, VECTORS_INDEX_EXTENSION);
+    try (ChecksumIndexInput input = d.openChecksumInput(indexName, context)) {
+      Throwable priorE = null;
+      try {
+        final String codecNameIdx = formatName + CODEC_SFX_IDX;
+        version = CodecUtil.checkSegmentHeader(input, codecNameIdx, VERSION_START, VERSION_CURRENT, si.getId(), segmentSuffix);
+        assert CodecUtil.segmentHeaderLength(codecNameIdx, segmentSuffix) == input.getFilePointer();
+        indexReader = new CompressingStoredFieldsIndexReader(input, si);
+        input.readVLong(); // the end of the data file
+      } catch (Throwable exception) {
+        priorE = exception;
+      } finally {
+        CodecUtil.checkFooter(input, priorE);
       }
-      indexStream.close();
-      indexStream = null;
+    }
+    
+    this.version = version;
+    this.indexReader = indexReader;
 
+    try {
       // Open the data file and read metadata
       final String vectorsStreamFN = IndexFileNames.segmentFileName(segment, segmentSuffix, VECTORS_EXTENSION);
       vectorsStream = d.openInput(vectorsStreamFN, context);
       final String codecNameDat = formatName + CODEC_SFX_DAT;
-      int version2 = CodecUtil.checkHeader(vectorsStream, codecNameDat, VERSION_START, VERSION_CURRENT);
+      int version2 = CodecUtil.checkSegmentHeader(vectorsStream, codecNameDat, VERSION_START, VERSION_CURRENT, si.getId(), segmentSuffix);
       if (version != version2) {
         throw new CorruptIndexException("Version mismatch between stored fields index and data: " + version + " != " + version2, vectorsStream);
       }
-      assert CodecUtil.headerLength(codecNameDat) == vectorsStream.getFilePointer();
+      assert CodecUtil.segmentHeaderLength(codecNameDat, segmentSuffix) == vectorsStream.getFilePointer();
       
       long pos = vectorsStream.getFilePointer();
-      if (version >= VERSION_CHECKSUM) {
-        // NOTE: data file is too costly to verify checksum against all the bytes on open,
-        // but for now we at least verify proper structure of the checksum footer: which looks
-        // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption
-        // such as file truncation.
-        CodecUtil.retrieveChecksum(vectorsStream);
-        vectorsStream.seek(pos);
-      }
+      // NOTE: data file is too costly to verify checksum against all the bytes on open,
+      // but for now we at least verify proper structure of the checksum footer: which looks
+      // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption
+      // such as file truncation.
+      CodecUtil.retrieveChecksum(vectorsStream);
+      vectorsStream.seek(pos);
 
       packedIntsVersion = vectorsStream.readVInt();
       chunkSize = vectorsStream.readVInt();
@@ -153,7 +155,7 @@
       success = true;
     } finally {
       if (!success) {
-        IOUtils.closeWhileHandlingException(this, indexStream);
+        IOUtils.closeWhileHandlingException(this);
       }
     }
   }
@@ -1078,9 +1080,7 @@
   
   @Override
   public void checkIntegrity() throws IOException {
-    if (version >= VERSION_CHECKSUM) {
-      CodecUtil.checksumEntireFile(vectorsStream);
-    }
+    CodecUtil.checksumEntireFile(vectorsStream);
   }
 
   @Override
Index: lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingTermVectorsWriter.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingTermVectorsWriter.java	(revision 1629405)
+++ lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingTermVectorsWriter.java	(working copy)
@@ -28,11 +28,12 @@
 import org.apache.lucene.codecs.CodecUtil;
 import org.apache.lucene.codecs.TermVectorsReader;
 import org.apache.lucene.codecs.TermVectorsWriter;
-import org.apache.lucene.index.LeafReader;
 import org.apache.lucene.index.FieldInfo;
 import org.apache.lucene.index.FieldInfos;
 import org.apache.lucene.index.Fields;
 import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.LeafReader;
 import org.apache.lucene.index.MergeState;
 import org.apache.lucene.index.SegmentInfo;
 import org.apache.lucene.index.SegmentReader;
@@ -68,8 +69,7 @@
   static final String CODEC_SFX_DAT = "Data";
 
   static final int VERSION_START = 0;
-  static final int VERSION_CHECKSUM = 1;
-  static final int VERSION_CURRENT = VERSION_CHECKSUM;
+  static final int VERSION_CURRENT = VERSION_START;
 
   static final int BLOCK_SIZE = 64;
 
@@ -231,10 +231,10 @@
 
       final String codecNameIdx = formatName + CODEC_SFX_IDX;
       final String codecNameDat = formatName + CODEC_SFX_DAT;
-      CodecUtil.writeHeader(indexStream, codecNameIdx, VERSION_CURRENT);
-      CodecUtil.writeHeader(vectorsStream, codecNameDat, VERSION_CURRENT);
-      assert CodecUtil.headerLength(codecNameDat) == vectorsStream.getFilePointer();
-      assert CodecUtil.headerLength(codecNameIdx) == indexStream.getFilePointer();
+      CodecUtil.writeSegmentHeader(indexStream, codecNameIdx, VERSION_CURRENT, si.getId(), segmentSuffix);
+      CodecUtil.writeSegmentHeader(vectorsStream, codecNameDat, VERSION_CURRENT, si.getId(), segmentSuffix);
+      assert CodecUtil.segmentHeaderLength(codecNameDat, segmentSuffix) == vectorsStream.getFilePointer();
+      assert CodecUtil.segmentHeaderLength(codecNameIdx, segmentSuffix) == indexStream.getFilePointer();
 
       indexWriter = new CompressingStoredFieldsIndexWriter(indexStream);
       indexStream = null;
@@ -728,15 +728,14 @@
   @Override
   public int merge(MergeState mergeState) throws IOException {
     int docCount = 0;
-    int idx = 0;
+    int numReaders = mergeState.maxDocs.length;
 
     MatchingReaders matching = new MatchingReaders(mergeState);
     
-    for (LeafReader reader : mergeState.readers) {
-      final SegmentReader matchingSegmentReader = matching.matchingSegmentReaders[idx++];
+    for (int readerIndex=0;readerIndex<numReaders;readerIndex++) {
       CompressingTermVectorsReader matchingVectorsReader = null;
-      if (matchingSegmentReader != null) {
-        final TermVectorsReader vectorsReader = matchingSegmentReader.getTermVectorsReader();
+      final TermVectorsReader vectorsReader = mergeState.termVectorsReaders[readerIndex];
+      if (matching.matchingReaders[readerIndex]) {
         // we can only bulk-copy if the matching reader is also a CompressingTermVectorsReader
         if (vectorsReader != null && vectorsReader instanceof CompressingTermVectorsReader) {
           matchingVectorsReader = (CompressingTermVectorsReader) vectorsReader;
@@ -743,8 +742,8 @@
         }
       }
 
-      final int maxDoc = reader.maxDoc();
-      final Bits liveDocs = reader.getLiveDocs();
+      final int maxDoc = mergeState.maxDocs[readerIndex];
+      final Bits liveDocs = mergeState.liveDocs[readerIndex];
 
       if (matchingVectorsReader == null
           || matchingVectorsReader.getVersion() != VERSION_CURRENT
@@ -752,8 +751,16 @@
           || matchingVectorsReader.getChunkSize() != chunkSize
           || matchingVectorsReader.getPackedIntsVersion() != PackedInts.VERSION_CURRENT) {
         // naive merge...
+        if (vectorsReader != null) {
+          vectorsReader.checkIntegrity();
+        }
         for (int i = nextLiveDoc(0, liveDocs, maxDoc); i < maxDoc; i = nextLiveDoc(i + 1, liveDocs, maxDoc)) {
-          final Fields vectors = reader.getTermVectors(i);
+          Fields vectors;
+          if (vectorsReader == null) {
+            vectors = null;
+          } else {
+            vectors = vectorsReader.get(i);
+          }
           addAllDocVectors(vectors, mergeState);
           ++docCount;
           mergeState.checkAbort.work(300);
@@ -775,8 +782,8 @@
               && (i == 0 || index.getStartPointer(i - 1) < startPointer)) { // start of a chunk
             final int docBase = vectorsStream.readVInt();
             final int chunkDocs = vectorsStream.readVInt();
-            assert docBase + chunkDocs <= matchingSegmentReader.maxDoc();
-            if (docBase + chunkDocs < matchingSegmentReader.maxDoc()
+            assert docBase + chunkDocs <= maxDoc;
+            if (docBase + chunkDocs < maxDoc
                 && nextDeletedDoc(docBase, liveDocs, docBase + chunkDocs) == docBase + chunkDocs) {
               final long chunkEnd = index.getStartPointer(docBase + chunkDocs);
               final long chunkLength = chunkEnd - vectorsStream.getFilePointer();
@@ -790,7 +797,12 @@
               i = nextLiveDoc(docBase + chunkDocs, liveDocs, maxDoc);
             } else {
               for (; i < docBase + chunkDocs; i = nextLiveDoc(i + 1, liveDocs, maxDoc)) {
-                final Fields vectors = reader.getTermVectors(i);
+                Fields vectors;
+                if (vectorsReader == null) {
+                  vectors = null;
+                } else {
+                  vectors = vectorsReader.get(i);
+                }
                 addAllDocVectors(vectors, mergeState);
                 ++docCount;
                 mergeState.checkAbort.work(300);
@@ -797,7 +809,12 @@
               }
             }
           } else {
-            final Fields vectors = reader.getTermVectors(i);
+            Fields vectors;
+            if (vectorsReader == null) {
+              vectors = null;
+            } else {
+              vectors = vectorsReader.get(i);
+            }
             addAllDocVectors(vectors, mergeState);
             ++docCount;
             mergeState.checkAbort.work(300);
@@ -809,7 +826,7 @@
         CodecUtil.checkFooter(vectorsStream);
       }
     }
-    finish(mergeState.fieldInfos, docCount);
+    finish(mergeState.mergeFieldInfos, docCount);
     return docCount;
   }
 
Index: lucene/core/src/java/org/apache/lucene/codecs/compressing/MatchingReaders.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/codecs/compressing/MatchingReaders.java	(revision 1629405)
+++ lucene/core/src/java/org/apache/lucene/codecs/compressing/MatchingReaders.java	(working copy)
@@ -17,9 +17,7 @@
  * limitations under the License.
  */
 
-import org.apache.lucene.index.LeafReader;
 import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.FieldInfos;
 import org.apache.lucene.index.MergeState;
 import org.apache.lucene.index.SegmentReader;
 
@@ -32,9 +30,9 @@
   /** {@link SegmentReader}s that have identical field
    * name/number mapping, so their stored fields and term
    * vectors may be bulk merged. */
-  final SegmentReader[] matchingSegmentReaders;
+  final boolean[] matchingReaders;
 
-  /** How many {@link #matchingSegmentReaders} are set. */
+  /** How many {@link #matchingReaders} are set. */
   final int count;
   
   MatchingReaders(MergeState mergeState) {
@@ -41,45 +39,33 @@
     // If the i'th reader is a SegmentReader and has
     // identical fieldName -> number mapping, then this
     // array will be non-null at position i:
-    int numReaders = mergeState.readers.size();
+    int numReaders = mergeState.maxDocs.length;
     int matchedCount = 0;
-    matchingSegmentReaders = new SegmentReader[numReaders];
+    matchingReaders = new boolean[numReaders];
 
     // If this reader is a SegmentReader, and all of its
     // field name -> number mappings match the "merged"
     // FieldInfos, then we can do a bulk copy of the
     // stored fields:
+
+    nextReader:
     for (int i = 0; i < numReaders; i++) {
-      LeafReader reader = mergeState.readers.get(i);
-      // TODO: we may be able to broaden this to
-      // non-SegmentReaders, since FieldInfos is now
-      // required?  But... this'd also require exposing
-      // bulk-copy (TVs and stored fields) API in foreign
-      // readers..
-      if (reader instanceof SegmentReader) {
-        SegmentReader segmentReader = (SegmentReader) reader;
-        boolean same = true;
-        FieldInfos segmentFieldInfos = segmentReader.getFieldInfos();
-        for (FieldInfo fi : segmentFieldInfos) {
-          FieldInfo other = mergeState.fieldInfos.fieldInfo(fi.number);
-          if (other == null || !other.name.equals(fi.name)) {
-            same = false;
-            break;
-          }
+      for (FieldInfo fi : mergeState.fieldInfos[i]) {
+        FieldInfo other = mergeState.mergeFieldInfos.fieldInfo(fi.number);
+        if (other == null || !other.name.equals(fi.name)) {
+          continue nextReader;
         }
-        if (same) {
-          matchingSegmentReaders[i] = segmentReader;
-          matchedCount++;
-        }
       }
+      matchingReaders[i] = true;
+      matchedCount++;
     }
     
     this.count = matchedCount;
 
     if (mergeState.infoStream.isEnabled("SM")) {
-      mergeState.infoStream.message("SM", "merge store matchedCount=" + count + " vs " + mergeState.readers.size());
-      if (count != mergeState.readers.size()) {
-        mergeState.infoStream.message("SM", "" + (mergeState.readers.size() - count) + " non-bulk merges");
+      mergeState.infoStream.message("SM", "merge store matchedCount=" + count + " vs " + numReaders);
+      if (count != numReaders) {
+        mergeState.infoStream.message("SM", "" + (numReaders - count) + " non-bulk merges");
       }
     }
   }
Index: lucene/core/src/java/org/apache/lucene/codecs/lucene41/Lucene41StoredFieldsFormat.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/codecs/lucene41/Lucene41StoredFieldsFormat.java	(revision 1629405)
+++ lucene/core/src/java/org/apache/lucene/codecs/lucene41/Lucene41StoredFieldsFormat.java	(working copy)
@@ -1,124 +0,0 @@
-package org.apache.lucene.codecs.lucene41;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.codecs.CodecUtil;
-import org.apache.lucene.codecs.StoredFieldsFormat;
-import org.apache.lucene.codecs.compressing.CompressingStoredFieldsFormat;
-import org.apache.lucene.codecs.compressing.CompressingStoredFieldsIndexWriter;
-import org.apache.lucene.codecs.compressing.CompressionMode;
-import org.apache.lucene.index.StoredFieldVisitor;
-import org.apache.lucene.store.DataOutput;
-import org.apache.lucene.util.packed.PackedInts;
-
-/**
- * Lucene 4.1 stored fields format.
- *
- * <p><b>Principle</b></p>
- * <p>This {@link StoredFieldsFormat} compresses blocks of 16KB of documents in
- * order to improve the compression ratio compared to document-level
- * compression. It uses the <a href="http://code.google.com/p/lz4/">LZ4</a>
- * compression algorithm, which is fast to compress and very fast to decompress
- * data. Although the compression method that is used focuses more on speed
- * than on compression ratio, it should provide interesting compression ratios
- * for redundant inputs (such as log files, HTML or plain text).</p>
- * <p><b>File formats</b></p>
- * <p>Stored fields are represented by two files:</p>
- * <ol>
- * <li><a name="field_data" id="field_data"></a>
- * <p>A fields data file (extension <tt>.fdt</tt>). This file stores a compact
- * representation of documents in compressed blocks of 16KB or more. When
- * writing a segment, documents are appended to an in-memory <tt>byte[]</tt>
- * buffer. When its size reaches 16KB or more, some metadata about the documents
- * is flushed to disk, immediately followed by a compressed representation of
- * the buffer using the
- * <a href="http://code.google.com/p/lz4/">LZ4</a>
- * <a href="http://fastcompression.blogspot.fr/2011/05/lz4-explained.html">compression format</a>.</p>
- * <p>Here is a more detailed description of the field data file format:</p>
- * <ul>
- * <li>FieldData (.fdt) --&gt; &lt;Header&gt;, PackedIntsVersion, &lt;Chunk&gt;<sup>ChunkCount</sup></li>
- * <li>Header --&gt; {@link CodecUtil#writeHeader CodecHeader}</li>
- * <li>PackedIntsVersion --&gt; {@link PackedInts#VERSION_CURRENT} as a {@link DataOutput#writeVInt VInt}</li>
- * <li>ChunkCount is not known in advance and is the number of chunks necessary to store all document of the segment</li>
- * <li>Chunk --&gt; DocBase, ChunkDocs, DocFieldCounts, DocLengths, &lt;CompressedDocs&gt;</li>
- * <li>DocBase --&gt; the ID of the first document of the chunk as a {@link DataOutput#writeVInt VInt}</li>
- * <li>ChunkDocs --&gt; the number of documents in the chunk as a {@link DataOutput#writeVInt VInt}</li>
- * <li>DocFieldCounts --&gt; the number of stored fields of every document in the chunk, encoded as followed:<ul>
- *   <li>if chunkDocs=1, the unique value is encoded as a {@link DataOutput#writeVInt VInt}</li>
- *   <li>else read a {@link DataOutput#writeVInt VInt} (let's call it <tt>bitsRequired</tt>)<ul>
- *     <li>if <tt>bitsRequired</tt> is <tt>0</tt> then all values are equal, and the common value is the following {@link DataOutput#writeVInt VInt}</li>
- *     <li>else <tt>bitsRequired</tt> is the number of bits required to store any value, and values are stored in a {@link PackedInts packed} array where every value is stored on exactly <tt>bitsRequired</tt> bits</li>
- *   </ul></li>
- * </ul></li>
- * <li>DocLengths --&gt; the lengths of all documents in the chunk, encoded with the same method as DocFieldCounts</li>
- * <li>CompressedDocs --&gt; a compressed representation of &lt;Docs&gt; using the LZ4 compression format</li>
- * <li>Docs --&gt; &lt;Doc&gt;<sup>ChunkDocs</sup></li>
- * <li>Doc --&gt; &lt;FieldNumAndType, Value&gt;<sup>DocFieldCount</sup></li>
- * <li>FieldNumAndType --&gt; a {@link DataOutput#writeVLong VLong}, whose 3 last bits are Type and other bits are FieldNum</li>
- * <li>Type --&gt;<ul>
- *   <li>0: Value is String</li>
- *   <li>1: Value is BinaryValue</li>
- *   <li>2: Value is Int</li>
- *   <li>3: Value is Float</li>
- *   <li>4: Value is Long</li>
- *   <li>5: Value is Double</li>
- *   <li>6, 7: unused</li>
- * </ul></li>
- * <li>FieldNum --&gt; an ID of the field</li>
- * <li>Value --&gt; {@link DataOutput#writeString(String) String} | BinaryValue | Int | Float | Long | Double depending on Type</li>
- * <li>BinaryValue --&gt; ValueLength &lt;Byte&gt;<sup>ValueLength</sup></li>
- * </ul>
- * <p>Notes</p>
- * <ul>
- * <li>If documents are larger than 16KB then chunks will likely contain only
- * one document. However, documents can never spread across several chunks (all
- * fields of a single document are in the same chunk).</li>
- * <li>When at least one document in a chunk is large enough so that the chunk
- * is larger than 32KB, the chunk will actually be compressed in several LZ4
- * blocks of 16KB. This allows {@link StoredFieldVisitor}s which are only
- * interested in the first fields of a document to not have to decompress 10MB
- * of data if the document is 10MB, but only 16KB.</li>
- * <li>Given that the original lengths are written in the metadata of the chunk,
- * the decompressor can leverage this information to stop decoding as soon as
- * enough data has been decompressed.</li>
- * <li>In case documents are incompressible, CompressedDocs will be less than
- * 0.5% larger than Docs.</li>
- * </ul>
- * </li>
- * <li><a name="field_index" id="field_index"></a>
- * <p>A fields index file (extension <tt>.fdx</tt>).</p>
- * <ul>
- * <li>FieldsIndex (.fdx) --&gt; &lt;Header&gt;, &lt;ChunkIndex&gt;</li>
- * <li>Header --&gt; {@link CodecUtil#writeHeader CodecHeader}</li>
- * <li>ChunkIndex: See {@link CompressingStoredFieldsIndexWriter}</li>
- * </ul>
- * </li>
- * </ol>
- * <p><b>Known limitations</b></p>
- * <p>This {@link StoredFieldsFormat} does not support individual documents
- * larger than (<tt>2<sup>31</sup> - 2<sup>14</sup></tt>) bytes.</p>
- * @lucene.experimental
- */
-public final class Lucene41StoredFieldsFormat extends CompressingStoredFieldsFormat {
-
-  /** Sole constructor. */
-  public Lucene41StoredFieldsFormat() {
-    super("Lucene41StoredFields", CompressionMode.FAST, 1 << 14);
-  }
-
-}
Index: lucene/core/src/java/org/apache/lucene/codecs/lucene410/Lucene410DocValuesProducer.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/codecs/lucene410/Lucene410DocValuesProducer.java	(revision 1629405)
+++ lucene/core/src/java/org/apache/lucene/codecs/lucene410/Lucene410DocValuesProducer.java	(working copy)
@@ -92,10 +92,33 @@
   private final Map<String,MonotonicBlockPackedReader> ordIndexInstances = new HashMap<>();
   private final Map<String,ReverseTermsIndex> reverseIndexInstances = new HashMap<>();
   
+  private final boolean merging;
+  
+  // clone for merge: when merging we don't do any instances.put()s
+  Lucene410DocValuesProducer(Lucene410DocValuesProducer original) throws IOException {
+    assert Thread.holdsLock(original);
+    numerics.putAll(original.numerics);
+    binaries.putAll(original.binaries);
+    sortedSets.putAll(original.sortedSets);
+    sortedNumerics.putAll(original.sortedNumerics);
+    ords.putAll(original.ords);
+    ordIndexes.putAll(original.ordIndexes);
+    numFields = original.numFields;
+    ramBytesUsed = new AtomicLong(original.ramBytesUsed.get());
+    data = original.data.clone();
+    maxDoc = original.maxDoc;
+    
+    addressInstances.putAll(original.addressInstances);
+    ordIndexInstances.putAll(original.ordIndexInstances);
+    reverseIndexInstances.putAll(original.reverseIndexInstances);
+    merging = true;
+  }
+  
   /** expert: instantiates a new reader */
   Lucene410DocValuesProducer(SegmentReadState state, String dataCodec, String dataExtension, String metaCodec, String metaExtension) throws IOException {
     String metaName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, metaExtension);
     this.maxDoc = state.segmentInfo.getDocCount();
+    merging = false;
     
     int version = -1;
     int numFields = -1;
@@ -441,8 +464,10 @@
     if (addresses == null) {
       data.seek(bytes.addressesOffset);
       addresses = MonotonicBlockPackedReader.of(data, bytes.packedIntsVersion, bytes.blockSize, bytes.count+1, false);
-      addressInstances.put(field.name, addresses);
-      ramBytesUsed.addAndGet(addresses.ramBytesUsed() + RamUsageEstimator.NUM_BYTES_INT);
+      if (!merging) {
+        addressInstances.put(field.name, addresses);
+        ramBytesUsed.addAndGet(addresses.ramBytesUsed() + RamUsageEstimator.NUM_BYTES_INT);
+      }
     }
     return addresses;
   }
@@ -479,8 +504,10 @@
       data.seek(bytes.addressesOffset);
       final long size = (bytes.count + INTERVAL_MASK) >>> INTERVAL_SHIFT;
       addresses = MonotonicBlockPackedReader.of(data, bytes.packedIntsVersion, bytes.blockSize, size, false);
-      addressInstances.put(field.name, addresses);
-      ramBytesUsed.addAndGet(addresses.ramBytesUsed() + RamUsageEstimator.NUM_BYTES_INT);
+      if (!merging) {
+        addressInstances.put(field.name, addresses);
+        ramBytesUsed.addAndGet(addresses.ramBytesUsed() + RamUsageEstimator.NUM_BYTES_INT);
+      }
     }
     return addresses;
   }
@@ -497,8 +524,10 @@
       PagedBytes pagedBytes = new PagedBytes(15);
       pagedBytes.copy(data, dataSize);
       index.terms = pagedBytes.freeze(true);
-      reverseIndexInstances.put(field.name, index);
-      ramBytesUsed.addAndGet(index.ramBytesUsed());
+      if (!merging) {
+        reverseIndexInstances.put(field.name, index);
+        ramBytesUsed.addAndGet(index.ramBytesUsed());
+      }
     }
     return index;
   }
@@ -560,8 +589,10 @@
     if (instance == null) {
       data.seek(entry.offset);
       instance = MonotonicBlockPackedReader.of(data, entry.packedIntsVersion, entry.blockSize, entry.count+1, false);
-      ordIndexInstances.put(field.name, instance);
-      ramBytesUsed.addAndGet(instance.ramBytesUsed() + RamUsageEstimator.NUM_BYTES_INT);
+      if (!merging) {
+        ordIndexInstances.put(field.name, instance);
+        ramBytesUsed.addAndGet(instance.ramBytesUsed() + RamUsageEstimator.NUM_BYTES_INT);
+      }
     }
     return instance;
   }
@@ -726,6 +757,11 @@
   }
 
   @Override
+  public synchronized DocValuesProducer getMergeInstance() throws IOException {
+    return new Lucene410DocValuesProducer(this);
+  }
+
+  @Override
   public void close() throws IOException {
     data.close();
   }
Index: lucene/core/src/java/org/apache/lucene/codecs/lucene42/Lucene42TermVectorsFormat.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/codecs/lucene42/Lucene42TermVectorsFormat.java	(revision 1629405)
+++ lucene/core/src/java/org/apache/lucene/codecs/lucene42/Lucene42TermVectorsFormat.java	(working copy)
@@ -1,132 +0,0 @@
-package org.apache.lucene.codecs.lucene42;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.codecs.CodecUtil;
-import org.apache.lucene.codecs.TermVectorsFormat;
-import org.apache.lucene.codecs.compressing.CompressingStoredFieldsIndexWriter;
-import org.apache.lucene.codecs.compressing.CompressingTermVectorsFormat;
-import org.apache.lucene.codecs.compressing.CompressionMode;
-import org.apache.lucene.codecs.lucene41.Lucene41StoredFieldsFormat;
-import org.apache.lucene.store.DataOutput;
-import org.apache.lucene.util.packed.BlockPackedWriter;
-import org.apache.lucene.util.packed.PackedInts;
-
-/**
- * Lucene 4.2 {@link TermVectorsFormat term vectors format}.
- * <p>
- * Very similarly to {@link Lucene41StoredFieldsFormat}, this format is based
- * on compressed chunks of data, with document-level granularity so that a
- * document can never span across distinct chunks. Moreover, data is made as
- * compact as possible:<ul>
- * <li>textual data is compressed using the very light,
- * <a href="http://code.google.com/p/lz4/">LZ4</a> compression algorithm,
- * <li>binary data is written using fixed-size blocks of
- * {@link PackedInts packed ints}.
- * </ul>
- * <p>
- * Term vectors are stored using two files<ul>
- * <li>a data file where terms, frequencies, positions, offsets and payloads
- * are stored,
- * <li>an index file, loaded into memory, used to locate specific documents in
- * the data file.
- * </ul>
- * Looking up term vectors for any document requires at most 1 disk seek.
- * <p><b>File formats</b>
- * <ol>
- * <li><a name="vector_data" id="vector_data"></a>
- * <p>A vector data file (extension <tt>.tvd</tt>). This file stores terms,
- * frequencies, positions, offsets and payloads for every document. Upon writing
- * a new segment, it accumulates data into memory until the buffer used to store
- * terms and payloads grows beyond 4KB. Then it flushes all metadata, terms
- * and positions to disk using <a href="http://code.google.com/p/lz4/">LZ4</a>
- * compression for terms and payloads and
- * {@link BlockPackedWriter blocks of packed ints} for positions.</p>
- * <p>Here is a more detailed description of the field data file format:</p>
- * <ul>
- * <li>VectorData (.tvd) --&gt; &lt;Header&gt;, PackedIntsVersion, ChunkSize, &lt;Chunk&gt;<sup>ChunkCount</sup>, Footer</li>
- * <li>Header --&gt; {@link CodecUtil#writeHeader CodecHeader}</li>
- * <li>PackedIntsVersion --&gt; {@link PackedInts#VERSION_CURRENT} as a {@link DataOutput#writeVInt VInt}</li>
- * <li>ChunkSize is the number of bytes of terms to accumulate before flushing, as a {@link DataOutput#writeVInt VInt}</li>
- * <li>ChunkCount is not known in advance and is the number of chunks necessary to store all document of the segment</li>
- * <li>Chunk --&gt; DocBase, ChunkDocs, &lt; NumFields &gt;, &lt; FieldNums &gt;, &lt; FieldNumOffs &gt;, &lt; Flags &gt;,
- * &lt; NumTerms &gt;, &lt; TermLengths &gt;, &lt; TermFreqs &gt;, &lt; Positions &gt;, &lt; StartOffsets &gt;, &lt; Lengths &gt;,
- * &lt; PayloadLengths &gt;, &lt; TermAndPayloads &gt;</li>
- * <li>DocBase is the ID of the first doc of the chunk as a {@link DataOutput#writeVInt VInt}</li>
- * <li>ChunkDocs is the number of documents in the chunk</li>
- * <li>NumFields --&gt; DocNumFields<sup>ChunkDocs</sup></li>
- * <li>DocNumFields is the number of fields for each doc, written as a {@link DataOutput#writeVInt VInt} if ChunkDocs==1 and as a {@link PackedInts} array otherwise</li>
- * <li>FieldNums --&gt; FieldNumDelta<sup>TotalDistincFields</sup>, a delta-encoded list of the sorted unique field numbers present in the chunk</li>
- * <li>FieldNumOffs --&gt; FieldNumOff<sup>TotalFields</sup>, as a {@link PackedInts} array</li>
- * <li>FieldNumOff is the offset of the field number in FieldNums</li>
- * <li>TotalFields is the total number of fields (sum of the values of NumFields)</li>
- * <li>Flags --&gt; Bit &lt; FieldFlags &gt;</li>
- * <li>Bit  is a single bit which when true means that fields have the same options for every document in the chunk</li>
- * <li>FieldFlags --&gt; if Bit==1: Flag<sup>TotalDistinctFields</sup> else Flag<sup>TotalFields</sup></li>
- * <li>Flag: a 3-bits int where:<ul>
- * <li>the first bit means that the field has positions</li>
- * <li>the second bit means that the field has offsets</li>
- * <li>the third bit means that the field has payloads</li>
- * </ul></li>
- * <li>NumTerms --&gt; FieldNumTerms<sup>TotalFields</sup></li>
- * <li>FieldNumTerms: the number of terms for each field, using {@link BlockPackedWriter blocks of 64 packed ints}</li>
- * <li>TermLengths --&gt; PrefixLength<sup>TotalTerms</sup> SuffixLength<sup>TotalTerms</sup></li>
- * <li>TotalTerms: total number of terms (sum of NumTerms)</li>
- * <li>PrefixLength: 0 for the first term of a field, the common prefix with the previous term otherwise using {@link BlockPackedWriter blocks of 64 packed ints}</li>
- * <li>SuffixLength: length of the term minus PrefixLength for every term using {@link BlockPackedWriter blocks of 64 packed ints}</li>
- * <li>TermFreqs --&gt; TermFreqMinus1<sup>TotalTerms</sup></li>
- * <li>TermFreqMinus1: (frequency - 1) for each term using  {@link BlockPackedWriter blocks of 64 packed ints}</li>
- * <li>Positions --&gt; PositionDelta<sup>TotalPositions</sup></li>
- * <li>TotalPositions is the sum of frequencies of terms of all fields that have positions</li>
- * <li>PositionDelta: the absolute position for the first position of a term, and the difference with the previous positions for following positions using {@link BlockPackedWriter blocks of 64 packed ints}</li>
- * <li>StartOffsets --&gt; (AvgCharsPerTerm<sup>TotalDistinctFields</sup>) StartOffsetDelta<sup>TotalOffsets</sup></li>
- * <li>TotalOffsets is the sum of frequencies of terms of all fields that have offsets</li>
- * <li>AvgCharsPerTerm: average number of chars per term, encoded as a float on 4 bytes. They are not present if no field has both positions and offsets enabled.</li>
- * <li>StartOffsetDelta: (startOffset - previousStartOffset - AvgCharsPerTerm * PositionDelta). previousStartOffset is 0 for the first offset and AvgCharsPerTerm is 0 if the field has no positions using  {@link BlockPackedWriter blocks of 64 packed ints}</li>
- * <li>Lengths --&gt; LengthMinusTermLength<sup>TotalOffsets</sup></li>
- * <li>LengthMinusTermLength: (endOffset - startOffset - termLength) using  {@link BlockPackedWriter blocks of 64 packed ints}</li>
- * <li>PayloadLengths --&gt; PayloadLength<sup>TotalPayloads</sup></li>
- * <li>TotalPayloads is the sum of frequencies of terms of all fields that have payloads</li>
- * <li>PayloadLength is the payload length encoded using  {@link BlockPackedWriter blocks of 64 packed ints}</li>
- * <li>TermAndPayloads --&gt; LZ4-compressed representation of &lt; FieldTermsAndPayLoads &gt;<sup>TotalFields</sup></li>
- * <li>FieldTermsAndPayLoads --&gt; Terms (Payloads)</li>
- * <li>Terms: term bytes</li>
- * <li>Payloads: payload bytes (if the field has payloads)</li>
- * <li>Footer --&gt; {@link CodecUtil#writeFooter CodecFooter}</li>
- * </ul>
- * </li>
- * <li><a name="vector_index" id="vector_index"></a>
- * <p>An index file (extension <tt>.tvx</tt>).</p>
- * <ul>
- * <li>VectorIndex (.tvx) --&gt; &lt;Header&gt;, &lt;ChunkIndex&gt;, Footer</li>
- * <li>Header --&gt; {@link CodecUtil#writeHeader CodecHeader}</li>
- * <li>ChunkIndex: See {@link CompressingStoredFieldsIndexWriter}</li>
- * <li>Footer --&gt; {@link CodecUtil#writeFooter CodecFooter}</li>
- * </ul>
- * </li>
- * </ol>
- * @lucene.experimental
- */
-public final class Lucene42TermVectorsFormat extends CompressingTermVectorsFormat {
-
-  /** Sole constructor. */
-  public Lucene42TermVectorsFormat() {
-    super("Lucene41StoredFields", "", CompressionMode.FAST, 1 << 12);
-  }
-
-}
Index: lucene/core/src/java/org/apache/lucene/codecs/lucene42/package.html
===================================================================
--- lucene/core/src/java/org/apache/lucene/codecs/lucene42/package.html	(revision 1629405)
+++ lucene/core/src/java/org/apache/lucene/codecs/lucene42/package.html	(working copy)
@@ -1,25 +0,0 @@
-<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<html>
-<head>
-   <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
-</head>
-<body>
-Lucene 4.2 file format.
-</body>
-</html>
Index: lucene/core/src/java/org/apache/lucene/codecs/lucene49/Lucene49NormsConsumer.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/codecs/lucene49/Lucene49NormsConsumer.java	(revision 1629405)
+++ lucene/core/src/java/org/apache/lucene/codecs/lucene49/Lucene49NormsConsumer.java	(working copy)
@@ -1,250 +0,0 @@
-package org.apache.lucene.codecs.lucene49;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Arrays;
-import java.util.HashMap;
-import java.util.Map;
-
-import org.apache.lucene.codecs.CodecUtil;
-import org.apache.lucene.codecs.NormsConsumer;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.SegmentWriteState;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.packed.BlockPackedWriter;
-import org.apache.lucene.util.packed.PackedInts;
-
-import static org.apache.lucene.codecs.lucene49.Lucene49NormsFormat.VERSION_CURRENT;
-
-/**
- * Writer for {@link Lucene49NormsFormat}
- */
-class Lucene49NormsConsumer extends NormsConsumer { 
-  static final byte DELTA_COMPRESSED = 0;
-  static final byte TABLE_COMPRESSED = 1;
-  static final byte CONST_COMPRESSED = 2;
-  static final byte UNCOMPRESSED = 3;
-  static final int BLOCK_SIZE = 16384;
-
-  IndexOutput data, meta;
-  final int maxDoc;
-  
-  Lucene49NormsConsumer(SegmentWriteState state, String dataCodec, String dataExtension, String metaCodec, String metaExtension) throws IOException {
-    maxDoc = state.segmentInfo.getDocCount();
-    boolean success = false;
-    try {
-      String dataName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, dataExtension);
-      data = state.directory.createOutput(dataName, state.context);
-      CodecUtil.writeHeader(data, dataCodec, VERSION_CURRENT);
-      String metaName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, metaExtension);
-      meta = state.directory.createOutput(metaName, state.context);
-      CodecUtil.writeHeader(meta, metaCodec, VERSION_CURRENT);
-      success = true;
-    } finally {
-      if (!success) {
-        IOUtils.closeWhileHandlingException(this);
-      }
-    }
-  }
-  
-  // we explicitly use only certain bits per value and a specified format, so we statically check this will work
-  static {
-    assert PackedInts.Format.PACKED_SINGLE_BLOCK.isSupported(1);
-    assert PackedInts.Format.PACKED_SINGLE_BLOCK.isSupported(2);
-    assert PackedInts.Format.PACKED_SINGLE_BLOCK.isSupported(4);
-  }
-
-  @Override
-  public void addNormsField(FieldInfo field, Iterable<Number> values) throws IOException {
-    meta.writeVInt(field.number);
-    long minValue = Long.MAX_VALUE;
-    long maxValue = Long.MIN_VALUE;
-    // TODO: more efficient?
-    NormMap uniqueValues = new NormMap();
-    
-    long count = 0;
-    for (Number nv : values) {
-      if (nv == null) {
-        throw new IllegalStateException("illegal norms data for field " + field.name + ", got null for value: " + count);
-      }
-      final long v = nv.longValue();
-      
-      minValue = Math.min(minValue, v);
-      maxValue = Math.max(maxValue, v);
-      
-      if (uniqueValues != null) {
-        if (uniqueValues.add(v)) {
-          if (uniqueValues.size > 256) {
-            uniqueValues = null;
-          }
-        }
-      }
-      ++count;
-    }
-    
-    if (count != maxDoc) {
-      throw new IllegalStateException("illegal norms data for field " + field.name + ", expected " + maxDoc + " values, got " + count);
-    }
-    
-    if (uniqueValues != null && uniqueValues.size == 1) {
-      // 0 bpv
-      meta.writeByte(CONST_COMPRESSED);
-      meta.writeLong(minValue);
-    } else if (uniqueValues != null) {
-      // small number of unique values: this is the typical case:
-      // we only use bpv=1,2,4,8     
-      PackedInts.Format format = PackedInts.Format.PACKED_SINGLE_BLOCK;
-      int bitsPerValue = PackedInts.bitsRequired(uniqueValues.size-1);
-      if (bitsPerValue == 3) {
-        bitsPerValue = 4;
-      } else if (bitsPerValue > 4) {
-        bitsPerValue = 8;
-      }
-      
-      if (bitsPerValue == 8 && minValue >= Byte.MIN_VALUE && maxValue <= Byte.MAX_VALUE) {
-        meta.writeByte(UNCOMPRESSED); // uncompressed byte[]
-        meta.writeLong(data.getFilePointer());
-        for (Number nv : values) {
-          data.writeByte(nv == null ? 0 : (byte) nv.longValue());
-        }
-      } else {
-        meta.writeByte(TABLE_COMPRESSED); // table-compressed
-        meta.writeLong(data.getFilePointer());
-        data.writeVInt(PackedInts.VERSION_CURRENT);
-        
-        long[] decode = uniqueValues.getDecodeTable();
-        // upgrade to power of two sized array
-        int size = 1 << bitsPerValue;
-        data.writeVInt(size);
-        for (int i = 0; i < decode.length; i++) {
-          data.writeLong(decode[i]);
-        }
-        for (int i = decode.length; i < size; i++) {
-          data.writeLong(0);
-        }
-
-        data.writeVInt(format.getId());
-        data.writeVInt(bitsPerValue);
-
-        final PackedInts.Writer writer = PackedInts.getWriterNoHeader(data, format, maxDoc, bitsPerValue, PackedInts.DEFAULT_BUFFER_SIZE);
-        for(Number nv : values) {
-          writer.add(uniqueValues.getOrd(nv.longValue()));
-        }
-        writer.finish();
-      }
-    } else {
-      meta.writeByte(DELTA_COMPRESSED); // delta-compressed
-      meta.writeLong(data.getFilePointer());
-      data.writeVInt(PackedInts.VERSION_CURRENT);
-      data.writeVInt(BLOCK_SIZE);
-
-      final BlockPackedWriter writer = new BlockPackedWriter(data, BLOCK_SIZE);
-      for (Number nv : values) {
-        writer.add(nv.longValue());
-      }
-      writer.finish();
-    }
-  }
-  
-  @Override
-  public void close() throws IOException {
-    boolean success = false;
-    try {
-      if (meta != null) {
-        meta.writeVInt(-1); // write EOF marker
-        CodecUtil.writeFooter(meta); // write checksum
-      }
-      if (data != null) {
-        CodecUtil.writeFooter(data); // write checksum
-      }
-      success = true;
-    } finally {
-      if (success) {
-        IOUtils.close(data, meta);
-      } else {
-        IOUtils.closeWhileHandlingException(data, meta);
-      }
-      meta = data = null;
-    }
-  }
-  
-  // specialized deduplication of long->ord for norms: 99.99999% of the time this will be a single-byte range.
-  static class NormMap {
-    // we use short: at most we will add 257 values to this map before its rejected as too big above.
-    final short[] singleByteRange = new short[256];
-    final Map<Long,Short> other = new HashMap<Long,Short>();
-    int size;
-    
-    {
-      Arrays.fill(singleByteRange, (short)-1);
-    }
-
-    /** adds an item to the mapping. returns true if actually added */
-    public boolean add(long l) {
-      assert size <= 256; // once we add > 256 values, we nullify the map in addNumericField and don't use this strategy
-      if (l >= Byte.MIN_VALUE && l <= Byte.MAX_VALUE) {
-        int index = (int) (l + 128);
-        short previous = singleByteRange[index];
-        if (previous < 0) {
-          singleByteRange[index] = (short) size;
-          size++;
-          return true;
-        } else {
-          return false;
-        }
-      } else {
-        if (!other.containsKey(l)) {
-          other.put(l, (short)size);
-          size++;
-          return true;
-        } else {
-          return false;
-        }
-      }
-    }
-    
-    /** gets the ordinal for a previously added item */
-    public int getOrd(long l) {
-      if (l >= Byte.MIN_VALUE && l <= Byte.MAX_VALUE) {
-        int index = (int) (l + 128);
-        return singleByteRange[index];
-      } else {
-        // NPE if something is screwed up
-        return other.get(l);
-      }
-    }
-    
-    /** retrieves the ordinal table for previously added items */
-    public long[] getDecodeTable() {
-      long decode[] = new long[size];
-      for (int i = 0; i < singleByteRange.length; i++) {
-        short s = singleByteRange[i];
-        if (s >= 0) {
-          decode[s] = i - 128;
-        }
-      }
-      for (Map.Entry<Long,Short> entry : other.entrySet()) {
-        decode[entry.getValue()] = entry.getKey();
-      }
-      return decode;
-    }
-  }
-}
Index: lucene/core/src/java/org/apache/lucene/codecs/lucene49/Lucene49NormsFormat.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/codecs/lucene49/Lucene49NormsFormat.java	(revision 1629405)
+++ lucene/core/src/java/org/apache/lucene/codecs/lucene49/Lucene49NormsFormat.java	(working copy)
@@ -1,121 +0,0 @@
-package org.apache.lucene.codecs.lucene49;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.codecs.CodecUtil;
-import org.apache.lucene.codecs.NormsConsumer;
-import org.apache.lucene.codecs.NormsFormat;
-import org.apache.lucene.codecs.NormsProducer;
-import org.apache.lucene.index.SegmentReadState;
-import org.apache.lucene.index.SegmentWriteState;
-import org.apache.lucene.store.DataOutput;
-import org.apache.lucene.util.SmallFloat;
-import org.apache.lucene.util.packed.BlockPackedWriter;
-import org.apache.lucene.util.packed.PackedInts;
-
-/**
- * Lucene 4.9 Score normalization format.
- * <p>
- * Encodes normalization values with these strategies:
- * <p>
- * <ul>
- *    <li>Uncompressed: when values fit into a single byte and would require more than 4 bits
- *        per value, they are just encoded as an uncompressed byte array.
- *    <li>Constant: when there is only one value present for the entire field, no actual data
- *        is written: this constant is encoded in the metadata
- *    <li>Table-compressed: when the number of unique values is very small (&lt; 64), and
- *        when there are unused "gaps" in the range of values used (such as {@link SmallFloat}), 
- *        a lookup table is written instead. Each per-document entry is instead the ordinal 
- *        to this table, and those ordinals are compressed with bitpacking ({@link PackedInts}). 
- *    <li>Delta-compressed: per-document integers written as deltas from the minimum value,
- *        compressed with bitpacking. For more information, see {@link BlockPackedWriter}.
- *        This is only used when norms of larger than one byte are present.
- * </ul>
- * <p>
- * Files:
- * <ol>
- *   <li><tt>.nvd</tt>: Norms data</li>
- *   <li><tt>.nvm</tt>: Norms metadata</li>
- * </ol>
- * <ol>
- *   <li><a name="nvm" id="nvm"></a>
- *   <p>The Norms metadata or .nvm file.</p>
- *   <p>For each norms field, this stores metadata, such as the offset into the 
- *      Norms data (.nvd)</p>
- *   <p>Norms metadata (.dvm) --&gt; Header,&lt;Entry&gt;<sup>NumFields</sup>,Footer</p>
- *   <ul>
- *     <li>Header --&gt; {@link CodecUtil#writeHeader CodecHeader}</li>
- *     <li>Entry --&gt; FieldNumber,Type,Offset</li>
- *     <li>FieldNumber --&gt; {@link DataOutput#writeVInt vInt}</li>
- *     <li>Type --&gt; {@link DataOutput#writeByte Byte}</li>
- *     <li>Offset --&gt; {@link DataOutput#writeLong Int64}</li>
- *     <li>Footer --&gt; {@link CodecUtil#writeFooter CodecFooter}</li>
- *   </ul>
- *   <p>FieldNumber of -1 indicates the end of metadata.</p>
- *   <p>Offset is the pointer to the start of the data in the norms data (.nvd), or the singleton value for Constant</p>
- *   <p>Type indicates how Numeric values will be compressed:
- *      <ul>
- *         <li>0 --&gt; delta-compressed. For each block of 16k integers, every integer is delta-encoded
- *             from the minimum value within the block. 
- *         <li>1 --&gt; table-compressed. When the number of unique numeric values is small and it would save space,
- *             a lookup table of unique values is written, followed by the ordinal for each document.
- *         <li>2 --&gt; constant. When there is a single value for the entire field.
- *         <li>3 --&gt; uncompressed: Values written as a simple byte[].
- *      </ul>
- *   <li><a name="nvd" id="nvd"></a>
- *   <p>The Norms data or .nvd file.</p>
- *   <p>For each Norms field, this stores the actual per-document data (the heavy-lifting)</p>
- *   <p>Norms data (.nvd) --&gt; Header,&lt;Uncompressed | TableCompressed | DeltaCompressed&gt;<sup>NumFields</sup>,Footer</p>
- *   <ul>
- *     <li>Header --&gt; {@link CodecUtil#writeHeader CodecHeader}</li>
- *     <li>Uncompressed --&gt;  {@link DataOutput#writeByte Byte}<sup>maxDoc</sup></li>
- *     <li>TableCompressed --&gt; PackedIntsVersion,Table,BitPackedData</li>
- *     <li>Table --&gt; TableSize, {@link DataOutput#writeLong int64}<sup>TableSize</sup></li>
- *     <li>BitpackedData --&gt; {@link PackedInts}</li>
- *     <li>DeltaCompressed --&gt; PackedIntsVersion,BlockSize,DeltaCompressedData</li>
- *     <li>DeltaCompressedData --&gt; {@link BlockPackedWriter BlockPackedWriter(blockSize=16k)}</li>
- *     <li>PackedIntsVersion,BlockSize,TableSize --&gt; {@link DataOutput#writeVInt vInt}</li>
- *     <li>Footer --&gt; {@link CodecUtil#writeFooter CodecFooter}</li>
- *   </ul>
- * </ol>
- * @lucene.experimental
- */
-public class Lucene49NormsFormat extends NormsFormat {
-
-  /** Sole Constructor */
-  public Lucene49NormsFormat() {}
-  
-  @Override
-  public NormsConsumer normsConsumer(SegmentWriteState state) throws IOException {
-    return new Lucene49NormsConsumer(state, DATA_CODEC, DATA_EXTENSION, METADATA_CODEC, METADATA_EXTENSION);
-  }
-
-  @Override
-  public NormsProducer normsProducer(SegmentReadState state) throws IOException {
-    return new Lucene49NormsProducer(state, DATA_CODEC, DATA_EXTENSION, METADATA_CODEC, METADATA_EXTENSION);
-  }
-  
-  private static final String DATA_CODEC = "Lucene49NormsData";
-  private static final String DATA_EXTENSION = "nvd";
-  private static final String METADATA_CODEC = "Lucene49NormsMetadata";
-  private static final String METADATA_EXTENSION = "nvm";
-  static final int VERSION_START = 0;
-  static final int VERSION_CURRENT = VERSION_START;
-}
Index: lucene/core/src/java/org/apache/lucene/codecs/lucene49/Lucene49NormsProducer.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/codecs/lucene49/Lucene49NormsProducer.java	(revision 1629405)
+++ lucene/core/src/java/org/apache/lucene/codecs/lucene49/Lucene49NormsProducer.java	(working copy)
@@ -1,234 +0,0 @@
-package org.apache.lucene.codecs.lucene49;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.HashMap;
-import java.util.Map;
-import java.util.concurrent.atomic.AtomicInteger;
-import java.util.concurrent.atomic.AtomicLong;
-
-import org.apache.lucene.codecs.CodecUtil;
-import org.apache.lucene.codecs.NormsProducer;
-import org.apache.lucene.index.CorruptIndexException;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.NumericDocValues;
-import org.apache.lucene.index.SegmentReadState;
-import org.apache.lucene.store.ChecksumIndexInput;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.util.Accountable;
-import org.apache.lucene.util.Accountables;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.RamUsageEstimator;
-import org.apache.lucene.util.packed.BlockPackedReader;
-import org.apache.lucene.util.packed.PackedInts;
-
-import static org.apache.lucene.codecs.lucene49.Lucene49NormsFormat.VERSION_START;
-import static org.apache.lucene.codecs.lucene49.Lucene49NormsFormat.VERSION_CURRENT;
-import static org.apache.lucene.codecs.lucene49.Lucene49NormsConsumer.CONST_COMPRESSED;
-import static org.apache.lucene.codecs.lucene49.Lucene49NormsConsumer.DELTA_COMPRESSED;
-import static org.apache.lucene.codecs.lucene49.Lucene49NormsConsumer.TABLE_COMPRESSED;
-import static org.apache.lucene.codecs.lucene49.Lucene49NormsConsumer.UNCOMPRESSED;
-
-/**
- * Reader for {@link Lucene49NormsFormat}
- */
-class Lucene49NormsProducer extends NormsProducer {
-  // metadata maps (just file pointers and minimal stuff)
-  private final Map<String,NormsEntry> norms = new HashMap<>();
-  private final IndexInput data;
-  
-  // ram instances we have already loaded
-  final Map<String,NumericDocValues> instances = new HashMap<>();
-  final Map<String,Accountable> instancesInfo = new HashMap<>();
-  
-  private final int maxDoc;
-  private final AtomicLong ramBytesUsed;
-  private final AtomicInteger activeCount = new AtomicInteger();
-    
-  Lucene49NormsProducer(SegmentReadState state, String dataCodec, String dataExtension, String metaCodec, String metaExtension) throws IOException {
-    maxDoc = state.segmentInfo.getDocCount();
-    String metaName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, metaExtension);
-    ramBytesUsed = new AtomicLong(RamUsageEstimator.shallowSizeOfInstance(getClass()));
-    int version = -1;
-    
-    // read in the entries from the metadata file.
-    try (ChecksumIndexInput in = state.directory.openChecksumInput(metaName, state.context)) {
-      Throwable priorE = null;
-      try {
-        version = CodecUtil.checkHeader(in, metaCodec, VERSION_START, VERSION_CURRENT);
-        readFields(in, state.fieldInfos);
-      } catch (Throwable exception) {
-        priorE = exception;
-      } finally {
-        CodecUtil.checkFooter(in, priorE);
-      }
-    }
-
-    String dataName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, dataExtension);
-    this.data = state.directory.openInput(dataName, state.context);
-    boolean success = false;
-    try {
-      final int version2 = CodecUtil.checkHeader(data, dataCodec, VERSION_START, VERSION_CURRENT);
-      if (version != version2) {
-        throw new CorruptIndexException("Format versions mismatch: meta=" + version + ",data=" + version2, data);
-      }
-      
-      // NOTE: data file is too costly to verify checksum against all the bytes on open,
-      // but for now we at least verify proper structure of the checksum footer: which looks
-      // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption
-      // such as file truncation.
-      CodecUtil.retrieveChecksum(data);
-
-      success = true;
-    } finally {
-      if (!success) {
-        IOUtils.closeWhileHandlingException(this.data);
-      }
-    }
-  }
-  
-  private void readFields(IndexInput meta, FieldInfos infos) throws IOException {
-    int fieldNumber = meta.readVInt();
-    while (fieldNumber != -1) {
-      FieldInfo info = infos.fieldInfo(fieldNumber);
-      if (info == null) {
-        throw new CorruptIndexException("Invalid field number: " + fieldNumber, meta);
-      } else if (!info.hasNorms()) {
-        throw new CorruptIndexException("Invalid field: " + info.name, meta);
-      }
-      NormsEntry entry = new NormsEntry();
-      entry.format = meta.readByte();
-      entry.offset = meta.readLong();
-      switch(entry.format) {
-        case CONST_COMPRESSED:
-        case UNCOMPRESSED:
-        case TABLE_COMPRESSED:
-        case DELTA_COMPRESSED:
-          break;
-        default:
-          throw new CorruptIndexException("Unknown format: " + entry.format, meta);
-      }
-      norms.put(info.name, entry);
-      fieldNumber = meta.readVInt();
-    }
-  }
-
-  @Override
-  public synchronized NumericDocValues getNorms(FieldInfo field) throws IOException {
-    NumericDocValues instance = instances.get(field.name);
-    if (instance == null) {
-      instance = loadNorms(field);
-      instances.put(field.name, instance);
-      activeCount.incrementAndGet();
-    }
-    return instance;
-  }
-  
-  @Override
-  public long ramBytesUsed() {
-    return ramBytesUsed.get();
-  }
-  
-  @Override
-  public synchronized Iterable<? extends Accountable> getChildResources() {
-    return Accountables.namedAccountables("field", instancesInfo);
-  }
-  
-  @Override
-  public void checkIntegrity() throws IOException {
-    CodecUtil.checksumEntireFile(data);
-  }
-
-  private NumericDocValues loadNorms(FieldInfo field) throws IOException {
-    NormsEntry entry = norms.get(field.name);
-    switch(entry.format) {
-      case CONST_COMPRESSED:
-        instancesInfo.put(field.name, Accountables.namedAccountable("constant", 8));
-        ramBytesUsed.addAndGet(8);
-        final long v = entry.offset;
-        return new NumericDocValues() {
-          @Override
-          public long get(int docID) {
-            return v;
-          }
-        };
-      case UNCOMPRESSED:
-        data.seek(entry.offset);
-        final byte bytes[] = new byte[maxDoc];
-        data.readBytes(bytes, 0, bytes.length);
-        ramBytesUsed.addAndGet(RamUsageEstimator.sizeOf(bytes));
-        instancesInfo.put(field.name, Accountables.namedAccountable("byte array", maxDoc));
-        return new NumericDocValues() {
-          @Override
-          public long get(int docID) {
-            return bytes[docID];
-          }
-        };
-      case DELTA_COMPRESSED:
-        data.seek(entry.offset);
-        int packedIntsVersion = data.readVInt();
-        int blockSize = data.readVInt();
-        final BlockPackedReader reader = new BlockPackedReader(data, packedIntsVersion, blockSize, maxDoc, false);
-        ramBytesUsed.addAndGet(reader.ramBytesUsed());
-        instancesInfo.put(field.name, Accountables.namedAccountable("delta compressed", reader));
-        return reader;
-      case TABLE_COMPRESSED:
-        data.seek(entry.offset);
-        int packedVersion = data.readVInt();
-        int size = data.readVInt();
-        if (size > 256) {
-          throw new CorruptIndexException("TABLE_COMPRESSED cannot have more than 256 distinct values, got=" + size, data);
-        }
-        final long decode[] = new long[size];
-        for (int i = 0; i < decode.length; i++) {
-          decode[i] = data.readLong();
-        }
-        final int formatID = data.readVInt();
-        final int bitsPerValue = data.readVInt();
-        final PackedInts.Reader ordsReader = PackedInts.getReaderNoHeader(data, PackedInts.Format.byId(formatID), packedVersion, maxDoc, bitsPerValue);
-        ramBytesUsed.addAndGet(RamUsageEstimator.sizeOf(decode) + ordsReader.ramBytesUsed());
-        instancesInfo.put(field.name, Accountables.namedAccountable("table compressed", ordsReader));
-        return new NumericDocValues() {
-          @Override
-          public long get(int docID) {
-            return decode[(int)ordsReader.get(docID)];
-          }
-        };
-      default:
-        throw new AssertionError();
-    }
-  }
-
-  @Override
-  public void close() throws IOException {
-    data.close();
-  }
-  
-  static class NormsEntry {
-    byte format;
-    long offset;
-  }
-
-  @Override
-  public String toString() {
-    return getClass().getSimpleName() + "(fields=" + norms.size() + ",active=" + activeCount.get() + ")";
-  }
-}
Index: lucene/core/src/java/org/apache/lucene/codecs/lucene49/package.html
===================================================================
--- lucene/core/src/java/org/apache/lucene/codecs/lucene49/package.html	(revision 1629405)
+++ lucene/core/src/java/org/apache/lucene/codecs/lucene49/package.html	(working copy)
@@ -1,25 +0,0 @@
-<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<html>
-<head>
-   <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
-</head>
-<body>
-Lucene 4.9 file format.
-</body>
-</html>
Index: lucene/core/src/java/org/apache/lucene/codecs/lucene50/Lucene50Codec.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/codecs/lucene50/Lucene50Codec.java	(revision 1629405)
+++ lucene/core/src/java/org/apache/lucene/codecs/lucene50/Lucene50Codec.java	(working copy)
@@ -18,6 +18,7 @@
  */
 
 import org.apache.lucene.codecs.Codec;
+import org.apache.lucene.codecs.CompoundFormat;
 import org.apache.lucene.codecs.DocValuesFormat;
 import org.apache.lucene.codecs.FieldInfosFormat;
 import org.apache.lucene.codecs.FilterCodec;
@@ -27,9 +28,6 @@
 import org.apache.lucene.codecs.SegmentInfoFormat;
 import org.apache.lucene.codecs.StoredFieldsFormat;
 import org.apache.lucene.codecs.TermVectorsFormat;
-import org.apache.lucene.codecs.lucene41.Lucene41StoredFieldsFormat;
-import org.apache.lucene.codecs.lucene42.Lucene42TermVectorsFormat;
-import org.apache.lucene.codecs.lucene49.Lucene49NormsFormat;
 import org.apache.lucene.codecs.perfield.PerFieldDocValuesFormat;
 import org.apache.lucene.codecs.perfield.PerFieldPostingsFormat;
 
@@ -44,11 +42,12 @@
  * @lucene.experimental
  */
 public class Lucene50Codec extends Codec {
-  private final StoredFieldsFormat fieldsFormat = new Lucene41StoredFieldsFormat();
-  private final TermVectorsFormat vectorsFormat = new Lucene42TermVectorsFormat();
+  private final StoredFieldsFormat fieldsFormat = new Lucene50StoredFieldsFormat();
+  private final TermVectorsFormat vectorsFormat = new Lucene50TermVectorsFormat();
   private final FieldInfosFormat fieldInfosFormat = new Lucene50FieldInfosFormat();
   private final SegmentInfoFormat segmentInfosFormat = new Lucene50SegmentInfoFormat();
   private final LiveDocsFormat liveDocsFormat = new Lucene50LiveDocsFormat();
+  private final CompoundFormat compoundFormat = new Lucene50CompoundFormat();
   
   private final PostingsFormat postingsFormat = new PerFieldPostingsFormat() {
     @Override
@@ -99,6 +98,11 @@
     return liveDocsFormat;
   }
 
+  @Override
+  public final CompoundFormat compoundFormat() {
+    return compoundFormat;
+  }
+
   /** Returns the postings format that should be used for writing 
    *  new segments of <code>field</code>.
    *  
@@ -125,7 +129,7 @@
   private final PostingsFormat defaultFormat = PostingsFormat.forName("Lucene41");
   private final DocValuesFormat defaultDVFormat = DocValuesFormat.forName("Lucene410");
 
-  private final NormsFormat normsFormat = new Lucene49NormsFormat();
+  private final NormsFormat normsFormat = new Lucene50NormsFormat();
 
   @Override
   public final NormsFormat normsFormat() {
Index: lucene/core/src/java/org/apache/lucene/codecs/lucene50/Lucene50CompoundFormat.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/codecs/lucene50/Lucene50CompoundFormat.java	(revision 0)
+++ lucene/core/src/java/org/apache/lucene/codecs/lucene50/Lucene50CompoundFormat.java	(working copy)
@@ -0,0 +1,127 @@
+package org.apache.lucene.codecs.lucene50;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Collection;
+
+import org.apache.lucene.codecs.CodecUtil;
+import org.apache.lucene.codecs.CompoundFormat;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.MergeState.CheckAbort;
+import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.store.DataOutput;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.store.IndexOutput;
+
+/**
+ * Lucene 5.0 compound file format
+ * <p>
+ * Files:
+ * <ul>
+ *    <li><tt>.cfs</tt>: An optional "virtual" file consisting of all the other 
+ *    index files for systems that frequently run out of file handles.
+ *    <li><tt>.cfe</tt>: The "virtual" compound file's entry table holding all 
+ *    entries in the corresponding .cfs file.
+ * </ul>
+ * <p>Description:</p>
+ * <ul>
+ *   <li>Compound (.cfs) --&gt; Header, FileData <sup>FileCount</sup>, Footer</li>
+ *   <li>Compound Entry Table (.cfe) --&gt; Header, FileCount, &lt;FileName,
+ *       DataOffset, DataLength&gt; <sup>FileCount</sup></li>
+ *   <li>Header --&gt; {@link CodecUtil#writeSegmentHeader SegmentHeader}</li>
+ *   <li>FileCount --&gt; {@link DataOutput#writeVInt VInt}</li>
+ *   <li>DataOffset,DataLength,Checksum --&gt; {@link DataOutput#writeLong UInt64}</li>
+ *   <li>FileName --&gt; {@link DataOutput#writeString String}</li>
+ *   <li>FileData --&gt; raw file data</li>
+ *   <li>Footer --&gt; {@link CodecUtil#writeFooter CodecFooter}</li>
+ * </ul>
+ * <p>Notes:</p>
+ * <ul>
+ *   <li>FileCount indicates how many files are contained in this compound file. 
+ *       The entry table that follows has that many entries. 
+ *   <li>Each directory entry contains a long pointer to the start of this file's data
+ *       section, the files length, and a String with that file's name.
+ * </ul>
+ */
+public final class Lucene50CompoundFormat extends CompoundFormat {
+
+  /** Sole constructor. */
+  public Lucene50CompoundFormat() {
+  }
+  
+  @Override
+  public Directory getCompoundReader(Directory dir, SegmentInfo si, IOContext context) throws IOException {
+    return new Lucene50CompoundReader(dir, si, context);
+  }
+
+  @Override
+  public void write(Directory dir, SegmentInfo si, Collection<String> files, CheckAbort checkAbort, IOContext context) throws IOException {
+    String dataFile = IndexFileNames.segmentFileName(si.name, "", DATA_EXTENSION);
+    String entriesFile = IndexFileNames.segmentFileName(si.name, "", ENTRIES_EXTENSION);
+    
+    try (IndexOutput data =    dir.createOutput(dataFile, context);
+         IndexOutput entries = dir.createOutput(entriesFile, context)) {
+      CodecUtil.writeSegmentHeader(data,    DATA_CODEC, VERSION_CURRENT, si.getId(), "");
+      CodecUtil.writeSegmentHeader(entries, ENTRY_CODEC, VERSION_CURRENT, si.getId(), "");
+      
+      // write number of files
+      entries.writeVInt(files.size());
+      for (String file : files) {
+        
+        // write bytes for file
+        long startOffset = data.getFilePointer();
+        try (IndexInput in = dir.openInput(file, IOContext.READONCE)) {
+          data.copyBytes(in, in.length());
+        }
+        long endOffset = data.getFilePointer();
+        
+        long length = endOffset - startOffset;
+        
+        // write entry for file
+        entries.writeString(IndexFileNames.stripSegmentName(file));
+        entries.writeLong(startOffset);
+        entries.writeLong(length);
+        
+        checkAbort.work(length);
+      }
+      
+      CodecUtil.writeFooter(data);
+      CodecUtil.writeFooter(entries);
+    }
+  }
+
+  @Override
+  public String[] files(SegmentInfo si) {
+    return new String[] {
+      IndexFileNames.segmentFileName(si.name, "", DATA_EXTENSION),
+      IndexFileNames.segmentFileName(si.name, "", ENTRIES_EXTENSION)
+    };
+  }
+
+  /** Extension of compound file */
+  static final String DATA_EXTENSION = "cfs";
+  /** Extension of compound file entries */
+  static final String ENTRIES_EXTENSION = "cfe";
+  static final String DATA_CODEC = "Lucene50CompoundData";
+  static final String ENTRY_CODEC = "Lucene50CompoundEntries";
+  static final int VERSION_START = 0;
+  static final int VERSION_CURRENT = VERSION_START;
+}

Property changes on: lucene/core/src/java/org/apache/lucene/codecs/lucene50/Lucene50CompoundFormat.java
___________________________________________________________________
Added: svn:eol-style
## -0,0 +1 ##
+native
\ No newline at end of property
Index: lucene/core/src/java/org/apache/lucene/codecs/lucene50/Lucene50CompoundReader.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/codecs/lucene50/Lucene50CompoundReader.java	(revision 0)
+++ lucene/core/src/java/org/apache/lucene/codecs/lucene50/Lucene50CompoundReader.java	(working copy)
@@ -0,0 +1,198 @@
+package org.apache.lucene.codecs.lucene50;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.codecs.CodecUtil;
+import org.apache.lucene.index.CorruptIndexException;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.store.BaseDirectory;
+import org.apache.lucene.store.ChecksumIndexInput;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.store.Lock;
+import org.apache.lucene.util.IOUtils;
+
+import java.util.Collection;
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.Map;
+import java.io.FileNotFoundException;
+import java.io.IOException;
+
+/**
+ * Class for accessing a compound stream.
+ * This class implements a directory, but is limited to only read operations.
+ * Directory methods that would normally modify data throw an exception.
+ * @lucene.experimental
+ */
+final class Lucene50CompoundReader extends BaseDirectory {
+  
+  /** Offset/Length for a slice inside of a compound file */
+  public static final class FileEntry {
+    long offset;
+    long length;
+  }
+  
+  private final Directory directory;
+  private final String segmentName;
+  private final Map<String,FileEntry> entries;
+  private final IndexInput handle;
+  private int version;
+  
+  /**
+   * Create a new CompoundFileDirectory.
+   */
+  // TODO: we should just pre-strip "entries" and append segment name up-front like simpletext?
+  // this need not be a "general purpose" directory anymore (it only writes index files)
+  public Lucene50CompoundReader(Directory directory, SegmentInfo si, IOContext context) throws IOException {
+    this.directory = directory;
+    this.segmentName = si.name;
+    String dataFileName = IndexFileNames.segmentFileName(segmentName, "", Lucene50CompoundFormat.DATA_EXTENSION);
+    String entriesFileName = IndexFileNames.segmentFileName(segmentName, "", Lucene50CompoundFormat.ENTRIES_EXTENSION);
+    this.entries = readEntries(si.getId(), directory, entriesFileName);
+    boolean success = false;
+    handle = directory.openInput(dataFileName, context);
+    try {
+      CodecUtil.checkSegmentHeader(handle, Lucene50CompoundFormat.DATA_CODEC, version, version, si.getId(), "");
+      
+      // NOTE: data file is too costly to verify checksum against all the bytes on open,
+      // but for now we at least verify proper structure of the checksum footer: which looks
+      // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption
+      // such as file truncation.
+      CodecUtil.retrieveChecksum(handle);
+      success = true;
+    } finally {
+      if (!success) {
+        IOUtils.closeWhileHandlingException(handle);
+      }
+    }
+    this.isOpen = true;
+  }
+
+  /** Helper method that reads CFS entries from an input stream */
+  private final Map<String, FileEntry> readEntries(byte[] segmentID, Directory dir, String entriesFileName) throws IOException {
+    Map<String,FileEntry> mapping = null;
+    try (ChecksumIndexInput entriesStream = dir.openChecksumInput(entriesFileName, IOContext.READONCE)) {
+      Throwable priorE = null;
+      try {
+        version = CodecUtil.checkSegmentHeader(entriesStream, Lucene50CompoundFormat.ENTRY_CODEC, 
+                                                              Lucene50CompoundFormat.VERSION_START, 
+                                                              Lucene50CompoundFormat.VERSION_CURRENT, segmentID, "");
+        final int numEntries = entriesStream.readVInt();
+        mapping = new HashMap<>(numEntries);
+        for (int i = 0; i < numEntries; i++) {
+          final FileEntry fileEntry = new FileEntry();
+          final String id = entriesStream.readString();
+          FileEntry previous = mapping.put(id, fileEntry);
+          if (previous != null) {
+            throw new CorruptIndexException("Duplicate cfs entry id=" + id + " in CFS ", entriesStream);
+          }
+          fileEntry.offset = entriesStream.readLong();
+          fileEntry.length = entriesStream.readLong();
+        }
+      } catch (Throwable exception) {
+        priorE = exception;
+      } finally {
+        CodecUtil.checkFooter(entriesStream, priorE);
+      }
+    }
+    return Collections.unmodifiableMap(mapping);
+  }
+  
+  @Override
+  public void close() throws IOException {
+    isOpen = false;
+    IOUtils.close(handle);
+  }
+  
+  @Override
+  public IndexInput openInput(String name, IOContext context) throws IOException {
+    ensureOpen();
+    final String id = IndexFileNames.stripSegmentName(name);
+    final FileEntry entry = entries.get(id);
+    if (entry == null) {
+      throw new FileNotFoundException("No sub-file with id " + id + " found (fileName=" + name + " files: " + entries.keySet() + ")");
+    }
+    return handle.slice(name, entry.offset, entry.length);
+  }
+  
+  /** Returns an array of strings, one for each file in the directory. */
+  @Override
+  public String[] listAll() {
+    ensureOpen();
+    String[] res = entries.keySet().toArray(new String[entries.size()]);
+    
+    // Add the segment name
+    for (int i = 0; i < res.length; i++) {
+      res[i] = segmentName + res[i];
+    }
+    return res;
+  }
+  
+  /** Not implemented
+   * @throws UnsupportedOperationException always: not supported by CFS */
+  @Override
+  public void deleteFile(String name) {
+    throw new UnsupportedOperationException();
+  }
+  
+  /** Not implemented
+   * @throws UnsupportedOperationException always: not supported by CFS */
+  public void renameFile(String from, String to) {
+    throw new UnsupportedOperationException();
+  }
+  
+  /** Returns the length of a file in the directory.
+   * @throws IOException if the file does not exist */
+  @Override
+  public long fileLength(String name) throws IOException {
+    ensureOpen();
+    FileEntry e = entries.get(IndexFileNames.stripSegmentName(name));
+    if (e == null)
+      throw new FileNotFoundException(name);
+    return e.length;
+  }
+  
+  @Override
+  public IndexOutput createOutput(String name, IOContext context) throws IOException {
+    throw new UnsupportedOperationException();
+  }
+  
+  @Override
+  public void sync(Collection<String> names) {
+    throw new UnsupportedOperationException();
+  }
+  
+  @Override
+  public Lock makeLock(String name) {
+    throw new UnsupportedOperationException();
+  }
+
+  @Override
+  public void clearLock(String name) throws IOException {
+    throw new UnsupportedOperationException();
+  }
+
+  @Override
+  public String toString() {
+    return "CompoundFileDirectory(segment=\"" + segmentName + "\" in dir=" + directory + ")";
+  }
+}

Property changes on: lucene/core/src/java/org/apache/lucene/codecs/lucene50/Lucene50CompoundReader.java
___________________________________________________________________
Added: svn:eol-style
## -0,0 +1 ##
+native
\ No newline at end of property
Index: lucene/core/src/java/org/apache/lucene/codecs/lucene50/Lucene50FieldInfosFormat.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/codecs/lucene50/Lucene50FieldInfosFormat.java	(revision 1629405)
+++ lucene/core/src/java/org/apache/lucene/codecs/lucene50/Lucene50FieldInfosFormat.java	(working copy)
@@ -25,7 +25,6 @@
 import org.apache.lucene.codecs.FieldInfosReader;
 import org.apache.lucene.codecs.FieldInfosWriter;
 import org.apache.lucene.index.FieldInfo.DocValuesType;
-import org.apache.lucene.index.SegmentInfo;
 import org.apache.lucene.store.DataOutput;
 
 /**
Index: lucene/core/src/java/org/apache/lucene/codecs/lucene50/Lucene50FieldInfosReader.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/codecs/lucene50/Lucene50FieldInfosReader.java	(revision 1629405)
+++ lucene/core/src/java/org/apache/lucene/codecs/lucene50/Lucene50FieldInfosReader.java	(working copy)
@@ -57,7 +57,7 @@
         CodecUtil.checkSegmentHeader(input, Lucene50FieldInfosFormat.CODEC_NAME, 
                                      Lucene50FieldInfosFormat.FORMAT_START, 
                                      Lucene50FieldInfosFormat.FORMAT_CURRENT,
-                                     segmentInfo.getId());
+                                     segmentInfo.getId(), segmentSuffix);
         
         final int size = input.readVInt(); //read in the size
         infos = new FieldInfo[size];
Index: lucene/core/src/java/org/apache/lucene/codecs/lucene50/Lucene50FieldInfosWriter.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/codecs/lucene50/Lucene50FieldInfosWriter.java	(revision 1629405)
+++ lucene/core/src/java/org/apache/lucene/codecs/lucene50/Lucene50FieldInfosWriter.java	(working copy)
@@ -47,7 +47,7 @@
   public void write(Directory directory, SegmentInfo segmentInfo, String segmentSuffix, FieldInfos infos, IOContext context) throws IOException {
     final String fileName = IndexFileNames.segmentFileName(segmentInfo.name, segmentSuffix, Lucene50FieldInfosFormat.EXTENSION);
     try (IndexOutput output = directory.createOutput(fileName, context)) {
-      CodecUtil.writeSegmentHeader(output, Lucene50FieldInfosFormat.CODEC_NAME, Lucene50FieldInfosFormat.FORMAT_CURRENT, segmentInfo.getId());
+      CodecUtil.writeSegmentHeader(output, Lucene50FieldInfosFormat.CODEC_NAME, Lucene50FieldInfosFormat.FORMAT_CURRENT, segmentInfo.getId(), segmentSuffix);
       output.writeVInt(infos.size());
       for (FieldInfo fi : infos) {
         fi.checkConsistency();
Index: lucene/core/src/java/org/apache/lucene/codecs/lucene50/Lucene50LiveDocsFormat.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/codecs/lucene50/Lucene50LiveDocsFormat.java	(revision 1629405)
+++ lucene/core/src/java/org/apache/lucene/codecs/lucene50/Lucene50LiveDocsFormat.java	(working copy)
@@ -85,7 +85,7 @@
     try (ChecksumIndexInput input = dir.openChecksumInput(name, context)) {
       Throwable priorE = null;
       try {
-        CodecUtil.checkSegmentHeader(input, CODEC_NAME, VERSION_START, VERSION_CURRENT, info.info.getId());
+        CodecUtil.checkSegmentHeader(input, CODEC_NAME, VERSION_START, VERSION_CURRENT, info.info.getId(), "");
         long filegen = input.readLong();
         if (gen != filegen) {
           throw new CorruptIndexException("file mismatch, expected generation=" + gen + ", got=" + filegen, input);
@@ -120,7 +120,7 @@
     }
     long data[] = fbs.getBits();
     try (IndexOutput output = dir.createOutput(name, context)) {
-      CodecUtil.writeSegmentHeader(output, CODEC_NAME, VERSION_CURRENT, info.info.getId());
+      CodecUtil.writeSegmentHeader(output, CODEC_NAME, VERSION_CURRENT, info.info.getId(), "");
       output.writeLong(gen);
       for (int i = 0; i < data.length; i++) {
         output.writeLong(data[i]);
Index: lucene/core/src/java/org/apache/lucene/codecs/lucene50/Lucene50NormsConsumer.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/codecs/lucene50/Lucene50NormsConsumer.java	(revision 0)
+++ lucene/core/src/java/org/apache/lucene/codecs/lucene50/Lucene50NormsConsumer.java	(working copy)
@@ -0,0 +1,319 @@
+package org.apache.lucene.codecs.lucene50;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Arrays;
+import java.util.HashMap;
+import java.util.Iterator;
+import java.util.Map;
+
+import org.apache.lucene.codecs.CodecUtil;
+import org.apache.lucene.codecs.NormsConsumer;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.SegmentWriteState;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.util.FilterIterator;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util.packed.BlockPackedWriter;
+import org.apache.lucene.util.packed.MonotonicBlockPackedWriter;
+import org.apache.lucene.util.packed.PackedInts;
+import org.apache.lucene.util.packed.PackedInts.FormatAndBits;
+
+import static org.apache.lucene.codecs.lucene50.Lucene50NormsFormat.VERSION_CURRENT;
+
+/**
+ * Writer for {@link Lucene50NormsFormat}
+ */
+class Lucene50NormsConsumer extends NormsConsumer { 
+  static final byte DELTA_COMPRESSED = 0;
+  static final byte TABLE_COMPRESSED = 1;
+  static final byte CONST_COMPRESSED = 2;
+  static final byte UNCOMPRESSED = 3;
+  static final byte INDIRECT = 4;
+  static final int BLOCK_SIZE = 1 << 14;
+  
+  // threshold for indirect encoding, computed as 1 - 1/log2(maxint)
+  // norms are only read for matching postings... so this is the threshold
+  // where n log n operations < maxdoc (e.g. it performs similar to other fields)
+  static final float INDIRECT_THRESHOLD = 1 - 1 / 31F;
+
+  IndexOutput data, meta;
+  
+  Lucene50NormsConsumer(SegmentWriteState state, String dataCodec, String dataExtension, String metaCodec, String metaExtension) throws IOException {
+    boolean success = false;
+    try {
+      String dataName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, dataExtension);
+      data = state.directory.createOutput(dataName, state.context);
+      CodecUtil.writeSegmentHeader(data, dataCodec, VERSION_CURRENT, state.segmentInfo.getId(), state.segmentSuffix);
+      String metaName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, metaExtension);
+      meta = state.directory.createOutput(metaName, state.context);
+      CodecUtil.writeSegmentHeader(meta, metaCodec, VERSION_CURRENT, state.segmentInfo.getId(), state.segmentSuffix);
+      success = true;
+    } finally {
+      if (!success) {
+        IOUtils.closeWhileHandlingException(this);
+      }
+    }
+  }
+  
+  // we explicitly use only certain bits per value and a specified format, so we statically check this will work
+  static {
+    assert PackedInts.Format.PACKED_SINGLE_BLOCK.isSupported(1);
+    assert PackedInts.Format.PACKED_SINGLE_BLOCK.isSupported(2);
+    assert PackedInts.Format.PACKED_SINGLE_BLOCK.isSupported(4);
+  }
+
+  @Override
+  public void addNormsField(FieldInfo field, Iterable<Number> values) throws IOException {
+    meta.writeVInt(field.number);
+    long minValue = Long.MAX_VALUE;
+    long maxValue = Long.MIN_VALUE;
+    // TODO: more efficient?
+    NormMap uniqueValues = new NormMap();
+    
+    int count = 0;
+    int missingCount = 0;
+    
+    for (Number nv : values) {
+      if (nv == null) {
+        throw new IllegalStateException("illegal norms data for field " + field.name + ", got null for value: " + count);
+      }
+      final long v = nv.longValue();
+      if (v == 0) {
+        missingCount++;
+      }
+      
+      minValue = Math.min(minValue, v);
+      maxValue = Math.max(maxValue, v);
+      
+      if (uniqueValues != null) {
+        if (uniqueValues.add(v)) {
+          if (uniqueValues.size > 256) {
+            uniqueValues = null;
+          }
+        }
+      }
+      count++;
+    }
+    if (uniqueValues != null && uniqueValues.size == 1) {
+      // 0 bpv
+      addConstant(minValue);
+    } else if (count > 256 && missingCount > count * INDIRECT_THRESHOLD) {
+      // sparse encoding
+      addIndirect(field, values, count, missingCount);
+    } else if (uniqueValues != null) {
+      // small number of unique values: this is the typical case:
+      FormatAndBits compression = fastestFormatAndBits(uniqueValues.size-1);
+      
+      if (compression.bitsPerValue == 8 && minValue >= Byte.MIN_VALUE && maxValue <= Byte.MAX_VALUE) {
+        addUncompressed(values, count);
+      } else {
+        addTableCompressed(values, compression, count, uniqueValues);
+      }
+    } else {
+      addDeltaCompressed(values, count);
+    }
+  }
+  
+  private FormatAndBits fastestFormatAndBits(int max) {
+    // we only use bpv=1,2,4,8     
+    PackedInts.Format format = PackedInts.Format.PACKED_SINGLE_BLOCK;
+    int bitsPerValue = PackedInts.bitsRequired(max);
+    if (bitsPerValue == 3) {
+      bitsPerValue = 4;
+    } else if (bitsPerValue > 4) {
+      bitsPerValue = 8;
+    }
+    return new FormatAndBits(format, bitsPerValue);
+  }
+  
+  private void addConstant(long constant) throws IOException {
+    meta.writeVInt(0);
+    meta.writeByte(CONST_COMPRESSED);
+    meta.writeLong(constant);
+  }
+  
+  private void addUncompressed(Iterable<Number> values, int count) throws IOException {
+    meta.writeVInt(count);
+    meta.writeByte(UNCOMPRESSED); // uncompressed byte[]
+    meta.writeLong(data.getFilePointer());
+    for (Number nv : values) {
+      data.writeByte((byte) nv.longValue());
+    }
+  }
+  
+  private void addTableCompressed(Iterable<Number> values, FormatAndBits compression, int count, NormMap uniqueValues) throws IOException {
+    meta.writeVInt(count);
+    meta.writeByte(TABLE_COMPRESSED); // table-compressed
+    meta.writeLong(data.getFilePointer());
+    data.writeVInt(PackedInts.VERSION_CURRENT);
+    
+    long[] decode = uniqueValues.getDecodeTable();
+    // upgrade to power of two sized array
+    int size = 1 << compression.bitsPerValue;
+    data.writeVInt(size);
+    for (int i = 0; i < decode.length; i++) {
+      data.writeLong(decode[i]);
+    }
+    for (int i = decode.length; i < size; i++) {
+      data.writeLong(0);
+    }
+
+    data.writeVInt(compression.format.getId());
+    data.writeVInt(compression.bitsPerValue);
+
+    final PackedInts.Writer writer = PackedInts.getWriterNoHeader(data, compression.format, count, compression.bitsPerValue, PackedInts.DEFAULT_BUFFER_SIZE);
+    for(Number nv : values) {
+      writer.add(uniqueValues.getOrd(nv.longValue()));
+    }
+    writer.finish();
+  }
+  
+  private void addDeltaCompressed(Iterable<Number> values, int count) throws IOException {
+    meta.writeVInt(count);
+    meta.writeByte(DELTA_COMPRESSED); // delta-compressed
+    meta.writeLong(data.getFilePointer());
+    data.writeVInt(PackedInts.VERSION_CURRENT);
+    data.writeVInt(BLOCK_SIZE);
+
+    final BlockPackedWriter writer = new BlockPackedWriter(data, BLOCK_SIZE);
+    for (Number nv : values) {
+      writer.add(nv.longValue());
+    }
+    writer.finish();
+  }
+  
+  private void addIndirect(FieldInfo field, final Iterable<Number> values, int count, int missingCount) throws IOException {
+    meta.writeVInt(count - missingCount);
+    meta.writeByte(INDIRECT);
+    meta.writeLong(data.getFilePointer());
+    data.writeVInt(PackedInts.VERSION_CURRENT);
+    data.writeVInt(BLOCK_SIZE);
+    
+    // write docs with value
+    final MonotonicBlockPackedWriter writer = new MonotonicBlockPackedWriter(data, BLOCK_SIZE);
+    int doc = 0;
+    for (Number n : values) {
+      long v = n.longValue();
+      if (v != 0) {
+        writer.add(doc);
+      }
+      doc++;
+    }
+    writer.finish();
+    
+    // write actual values
+    addNormsField(field, new Iterable<Number>() {
+      @Override
+      public Iterator<Number> iterator() {
+        return new FilterIterator<Number,Number>(values.iterator()) {
+          @Override
+          protected boolean predicateFunction(Number value) {
+            return value.longValue() != 0;
+          }
+        };
+      }
+    });
+  }
+  
+  @Override
+  public void close() throws IOException {
+    boolean success = false;
+    try {
+      if (meta != null) {
+        meta.writeVInt(-1); // write EOF marker
+        CodecUtil.writeFooter(meta); // write checksum
+      }
+      if (data != null) {
+        CodecUtil.writeFooter(data); // write checksum
+      }
+      success = true;
+    } finally {
+      if (success) {
+        IOUtils.close(data, meta);
+      } else {
+        IOUtils.closeWhileHandlingException(data, meta);
+      }
+      meta = data = null;
+    }
+  }
+  
+  // specialized deduplication of long->ord for norms: 99.99999% of the time this will be a single-byte range.
+  static class NormMap {
+    // we use short: at most we will add 257 values to this map before its rejected as too big above.
+    final short[] singleByteRange = new short[256];
+    final Map<Long,Short> other = new HashMap<Long,Short>();
+    int size;
+    
+    {
+      Arrays.fill(singleByteRange, (short)-1);
+    }
+
+    /** adds an item to the mapping. returns true if actually added */
+    public boolean add(long l) {
+      assert size <= 256; // once we add > 256 values, we nullify the map in addNumericField and don't use this strategy
+      if (l >= Byte.MIN_VALUE && l <= Byte.MAX_VALUE) {
+        int index = (int) (l + 128);
+        short previous = singleByteRange[index];
+        if (previous < 0) {
+          singleByteRange[index] = (short) size;
+          size++;
+          return true;
+        } else {
+          return false;
+        }
+      } else {
+        if (!other.containsKey(l)) {
+          other.put(l, (short)size);
+          size++;
+          return true;
+        } else {
+          return false;
+        }
+      }
+    }
+    
+    /** gets the ordinal for a previously added item */
+    public int getOrd(long l) {
+      if (l >= Byte.MIN_VALUE && l <= Byte.MAX_VALUE) {
+        int index = (int) (l + 128);
+        return singleByteRange[index];
+      } else {
+        // NPE if something is screwed up
+        return other.get(l);
+      }
+    }
+    
+    /** retrieves the ordinal table for previously added items */
+    public long[] getDecodeTable() {
+      long decode[] = new long[size];
+      for (int i = 0; i < singleByteRange.length; i++) {
+        short s = singleByteRange[i];
+        if (s >= 0) {
+          decode[s] = i - 128;
+        }
+      }
+      for (Map.Entry<Long,Short> entry : other.entrySet()) {
+        decode[entry.getValue()] = entry.getKey();
+      }
+      return decode;
+    }
+  }
+}

Property changes on: lucene/core/src/java/org/apache/lucene/codecs/lucene50/Lucene50NormsConsumer.java
___________________________________________________________________
Added: svn:eol-style
## -0,0 +1 ##
+native
\ No newline at end of property
Index: lucene/core/src/java/org/apache/lucene/codecs/lucene50/Lucene50NormsFormat.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/codecs/lucene50/Lucene50NormsFormat.java	(revision 0)
+++ lucene/core/src/java/org/apache/lucene/codecs/lucene50/Lucene50NormsFormat.java	(working copy)
@@ -0,0 +1,125 @@
+package org.apache.lucene.codecs.lucene50;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.codecs.CodecUtil;
+import org.apache.lucene.codecs.NormsConsumer;
+import org.apache.lucene.codecs.NormsFormat;
+import org.apache.lucene.codecs.NormsProducer;
+import org.apache.lucene.index.SegmentReadState;
+import org.apache.lucene.index.SegmentWriteState;
+import org.apache.lucene.store.DataOutput;
+import org.apache.lucene.util.SmallFloat;
+import org.apache.lucene.util.packed.BlockPackedWriter;
+import org.apache.lucene.util.packed.PackedInts;
+
+/**
+ * Lucene 5.0 Score normalization format.
+ * <p>
+ * Encodes normalization values with these strategies:
+ * <p>
+ * <ul>
+ *    <li>Uncompressed: when values fit into a single byte and would require more than 4 bits
+ *        per value, they are just encoded as an uncompressed byte array.
+ *    <li>Constant: when there is only one value present for the entire field, no actual data
+ *        is written: this constant is encoded in the metadata
+ *    <li>Table-compressed: when the number of unique values is very small (&lt; 64), and
+ *        when there are unused "gaps" in the range of values used (such as {@link SmallFloat}), 
+ *        a lookup table is written instead. Each per-document entry is instead the ordinal 
+ *        to this table, and those ordinals are compressed with bitpacking ({@link PackedInts}). 
+ *    <li>Delta-compressed: per-document integers written as deltas from the minimum value,
+ *        compressed with bitpacking. For more information, see {@link BlockPackedWriter}.
+ *        This is only used when norms of larger than one byte are present.
+ *    <li>Indirect: when norms are extremely sparse, missing values are omitted.
+ *        Access to an individual value is slower, but missing norm values are never accessed
+ *        by search code.
+ * </ul>
+ * <p>
+ * Files:
+ * <ol>
+ *   <li><tt>.nvd</tt>: Norms data</li>
+ *   <li><tt>.nvm</tt>: Norms metadata</li>
+ * </ol>
+ * <ol>
+ *   <li><a name="nvm" id="nvm"></a>
+ *   <p>The Norms metadata or .nvm file.</p>
+ *   <p>For each norms field, this stores metadata, such as the offset into the 
+ *      Norms data (.nvd)</p>
+ *   <p>Norms metadata (.dvm) --&gt; Header,&lt;Entry&gt;<sup>NumFields</sup>,Footer</p>
+ *   <ul>
+ *     <li>Header --&gt; {@link CodecUtil#writeHeader CodecHeader}</li>
+ *     <li>Entry --&gt; FieldNumber,Type,Offset</li>
+ *     <li>FieldNumber --&gt; {@link DataOutput#writeVInt vInt}</li>
+ *     <li>Type --&gt; {@link DataOutput#writeByte Byte}</li>
+ *     <li>Offset --&gt; {@link DataOutput#writeLong Int64}</li>
+ *     <li>Footer --&gt; {@link CodecUtil#writeFooter CodecFooter}</li>
+ *   </ul>
+ *   <p>FieldNumber of -1 indicates the end of metadata.</p>
+ *   <p>Offset is the pointer to the start of the data in the norms data (.nvd), or the singleton value for Constant</p>
+ *   <p>Type indicates how Numeric values will be compressed:
+ *      <ul>
+ *         <li>0 --&gt; delta-compressed. For each block of 16k integers, every integer is delta-encoded
+ *             from the minimum value within the block. 
+ *         <li>1 --&gt; table-compressed. When the number of unique numeric values is small and it would save space,
+ *             a lookup table of unique values is written, followed by the ordinal for each document.
+ *         <li>2 --&gt; constant. When there is a single value for the entire field.
+ *         <li>3 --&gt; uncompressed: Values written as a simple byte[].
+ *         <li>4 --&gt; indirect. Only documents with a value are written with a sparse encoding.
+ *      </ul>
+ *   <li><a name="nvd" id="nvd"></a>
+ *   <p>The Norms data or .nvd file.</p>
+ *   <p>For each Norms field, this stores the actual per-document data (the heavy-lifting)</p>
+ *   <p>Norms data (.nvd) --&gt; Header,&lt;Uncompressed | TableCompressed | DeltaCompressed&gt;<sup>NumFields</sup>,Footer</p>
+ *   <ul>
+ *     <li>Header --&gt; {@link CodecUtil#writeHeader CodecHeader}</li>
+ *     <li>Uncompressed --&gt;  {@link DataOutput#writeByte Byte}<sup>maxDoc</sup></li>
+ *     <li>TableCompressed --&gt; PackedIntsVersion,Table,BitPackedData</li>
+ *     <li>Table --&gt; TableSize, {@link DataOutput#writeLong int64}<sup>TableSize</sup></li>
+ *     <li>BitpackedData --&gt; {@link PackedInts}</li>
+ *     <li>DeltaCompressed --&gt; PackedIntsVersion,BlockSize,DeltaCompressedData</li>
+ *     <li>DeltaCompressedData --&gt; {@link BlockPackedWriter BlockPackedWriter(blockSize=16k)}</li>
+ *     <li>PackedIntsVersion,BlockSize,TableSize --&gt; {@link DataOutput#writeVInt vInt}</li>
+ *     <li>Footer --&gt; {@link CodecUtil#writeFooter CodecFooter}</li>
+ *   </ul>
+ * </ol>
+ * @lucene.experimental
+ */
+public class Lucene50NormsFormat extends NormsFormat {
+
+  /** Sole Constructor */
+  public Lucene50NormsFormat() {}
+  
+  @Override
+  public NormsConsumer normsConsumer(SegmentWriteState state) throws IOException {
+    return new Lucene50NormsConsumer(state, DATA_CODEC, DATA_EXTENSION, METADATA_CODEC, METADATA_EXTENSION);
+  }
+
+  @Override
+  public NormsProducer normsProducer(SegmentReadState state) throws IOException {
+    return new Lucene50NormsProducer(state, DATA_CODEC, DATA_EXTENSION, METADATA_CODEC, METADATA_EXTENSION);
+  }
+  
+  private static final String DATA_CODEC = "Lucene50NormsData";
+  private static final String DATA_EXTENSION = "nvd";
+  private static final String METADATA_CODEC = "Lucene50NormsMetadata";
+  private static final String METADATA_EXTENSION = "nvm";
+  static final int VERSION_START = 0;
+  static final int VERSION_CURRENT = VERSION_START;
+}

Property changes on: lucene/core/src/java/org/apache/lucene/codecs/lucene50/Lucene50NormsFormat.java
___________________________________________________________________
Added: svn:eol-style
## -0,0 +1 ##
+native
\ No newline at end of property
Index: lucene/core/src/java/org/apache/lucene/codecs/lucene50/Lucene50NormsProducer.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/codecs/lucene50/Lucene50NormsProducer.java	(revision 0)
+++ lucene/core/src/java/org/apache/lucene/codecs/lucene50/Lucene50NormsProducer.java	(working copy)
@@ -0,0 +1,320 @@
+package org.apache.lucene.codecs.lucene50;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.HashMap;
+import java.util.Map;
+import java.util.concurrent.atomic.AtomicInteger;
+import java.util.concurrent.atomic.AtomicLong;
+
+import org.apache.lucene.codecs.CodecUtil;
+import org.apache.lucene.codecs.NormsProducer;
+import org.apache.lucene.index.CorruptIndexException;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.NumericDocValues;
+import org.apache.lucene.index.SegmentReadState;
+import org.apache.lucene.store.ChecksumIndexInput;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.util.Accountable;
+import org.apache.lucene.util.Accountables;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util.RamUsageEstimator;
+import org.apache.lucene.util.packed.BlockPackedReader;
+import org.apache.lucene.util.packed.MonotonicBlockPackedReader;
+import org.apache.lucene.util.packed.PackedInts;
+
+import static org.apache.lucene.codecs.lucene50.Lucene50NormsFormat.VERSION_START;
+import static org.apache.lucene.codecs.lucene50.Lucene50NormsFormat.VERSION_CURRENT;
+import static org.apache.lucene.codecs.lucene50.Lucene50NormsConsumer.CONST_COMPRESSED;
+import static org.apache.lucene.codecs.lucene50.Lucene50NormsConsumer.DELTA_COMPRESSED;
+import static org.apache.lucene.codecs.lucene50.Lucene50NormsConsumer.TABLE_COMPRESSED;
+import static org.apache.lucene.codecs.lucene50.Lucene50NormsConsumer.UNCOMPRESSED;
+import static org.apache.lucene.codecs.lucene50.Lucene50NormsConsumer.INDIRECT;
+
+/**
+ * Reader for {@link Lucene50NormsFormat}
+ */
+class Lucene50NormsProducer extends NormsProducer {
+  // metadata maps (just file pointers and minimal stuff)
+  private final Map<String,NormsEntry> norms = new HashMap<>();
+  private final IndexInput data;
+  
+  // ram instances we have already loaded
+  final Map<String,NumericDocValues> instances = new HashMap<>();
+  final Map<String,Accountable> instancesInfo = new HashMap<>();
+  
+  private final AtomicLong ramBytesUsed;
+  private final AtomicInteger activeCount = new AtomicInteger();
+  
+  private final boolean merging;
+  
+  // clone for merge: when merging we don't do any instances.put()s
+  Lucene50NormsProducer(Lucene50NormsProducer original) {
+    assert Thread.holdsLock(original);
+    norms.putAll(original.norms);
+    data = original.data.clone();
+    instances.putAll(original.instances);
+    instancesInfo.putAll(original.instancesInfo);
+    ramBytesUsed = new AtomicLong(original.ramBytesUsed.get());
+    activeCount.set(original.activeCount.get());
+    merging = true;
+  }
+    
+  Lucene50NormsProducer(SegmentReadState state, String dataCodec, String dataExtension, String metaCodec, String metaExtension) throws IOException {
+    merging = false;
+    String metaName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, metaExtension);
+    ramBytesUsed = new AtomicLong(RamUsageEstimator.shallowSizeOfInstance(getClass()));
+    int version = -1;
+    
+    // read in the entries from the metadata file.
+    try (ChecksumIndexInput in = state.directory.openChecksumInput(metaName, state.context)) {
+      Throwable priorE = null;
+      try {
+        version = CodecUtil.checkSegmentHeader(in, metaCodec, VERSION_START, VERSION_CURRENT, state.segmentInfo.getId(), state.segmentSuffix);
+        readFields(in, state.fieldInfos);
+      } catch (Throwable exception) {
+        priorE = exception;
+      } finally {
+        CodecUtil.checkFooter(in, priorE);
+      }
+    }
+
+    String dataName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, dataExtension);
+    this.data = state.directory.openInput(dataName, state.context);
+    boolean success = false;
+    try {
+      final int version2 = CodecUtil.checkSegmentHeader(data, dataCodec, VERSION_START, VERSION_CURRENT, state.segmentInfo.getId(), state.segmentSuffix);
+      if (version != version2) {
+        throw new CorruptIndexException("Format versions mismatch: meta=" + version + ",data=" + version2, data);
+      }
+      
+      // NOTE: data file is too costly to verify checksum against all the bytes on open,
+      // but for now we at least verify proper structure of the checksum footer: which looks
+      // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption
+      // such as file truncation.
+      CodecUtil.retrieveChecksum(data);
+
+      success = true;
+    } finally {
+      if (!success) {
+        IOUtils.closeWhileHandlingException(this.data);
+      }
+    }
+  }
+  
+  private void readFields(IndexInput meta, FieldInfos infos) throws IOException {
+    int fieldNumber = meta.readVInt();
+    while (fieldNumber != -1) {
+      FieldInfo info = infos.fieldInfo(fieldNumber);
+      if (info == null) {
+        throw new CorruptIndexException("Invalid field number: " + fieldNumber, meta);
+      } else if (!info.hasNorms()) {
+        throw new CorruptIndexException("Invalid field: " + info.name, meta);
+      }
+      NormsEntry entry = readEntry(info, meta);
+      norms.put(info.name, entry);
+      fieldNumber = meta.readVInt();
+    }
+  }
+  
+  private NormsEntry readEntry(FieldInfo info, IndexInput meta) throws IOException {
+    NormsEntry entry = new NormsEntry();
+    entry.count = meta.readVInt();
+    entry.format = meta.readByte();
+    entry.offset = meta.readLong();
+    switch(entry.format) {
+      case CONST_COMPRESSED:
+      case UNCOMPRESSED:
+      case TABLE_COMPRESSED:
+      case DELTA_COMPRESSED:
+        break;
+      case INDIRECT:
+        if (meta.readVInt() != info.number) {
+          throw new CorruptIndexException("indirect norms entry for field: " + info.name + " is corrupt", meta);
+        }
+        entry.nested = readEntry(info, meta);
+        break;
+      default:
+        throw new CorruptIndexException("Unknown format: " + entry.format, meta);
+    }
+    return entry;
+  }
+
+  @Override
+  public synchronized NumericDocValues getNorms(FieldInfo field) throws IOException {
+    NumericDocValues instance = instances.get(field.name);
+    if (instance == null) {
+      LoadedNorms loaded = loadNorms(norms.get(field.name));
+      instance = loaded.norms;
+      if (!merging) {
+        instances.put(field.name, instance);
+        activeCount.incrementAndGet();
+        ramBytesUsed.addAndGet(loaded.ramBytesUsed);
+        instancesInfo.put(field.name, loaded.info);
+      }
+    }
+    return instance;
+  }
+  
+  @Override
+  public long ramBytesUsed() {
+    return ramBytesUsed.get();
+  }
+  
+  @Override
+  public synchronized Iterable<? extends Accountable> getChildResources() {
+    return Accountables.namedAccountables("field", instancesInfo);
+  }
+  
+  @Override
+  public void checkIntegrity() throws IOException {
+    CodecUtil.checksumEntireFile(data);
+  }
+
+  private LoadedNorms loadNorms(NormsEntry entry) throws IOException {
+    LoadedNorms instance = new LoadedNorms();
+    switch(entry.format) {
+      case CONST_COMPRESSED: {
+        final long v = entry.offset;
+        instance.info = Accountables.namedAccountable("constant", 8);
+        instance.ramBytesUsed = 8;
+        instance.norms = new NumericDocValues() {
+          @Override
+          public long get(int docID) {
+            return v;
+          }
+        };
+        break;
+      }
+      case UNCOMPRESSED: {
+        data.seek(entry.offset);
+        final byte bytes[] = new byte[entry.count];
+        data.readBytes(bytes, 0, bytes.length);
+        instance.info = Accountables.namedAccountable("byte array", bytes.length);
+        instance.ramBytesUsed = RamUsageEstimator.sizeOf(bytes);
+        instance.norms = new NumericDocValues() {
+          @Override
+          public long get(int docID) {
+            return bytes[docID];
+          }
+        };
+        break;
+      }
+      case DELTA_COMPRESSED: {
+        data.seek(entry.offset);
+        int packedIntsVersion = data.readVInt();
+        int blockSize = data.readVInt();
+        final BlockPackedReader reader = new BlockPackedReader(data, packedIntsVersion, blockSize, entry.count, false);
+        instance.info = Accountables.namedAccountable("delta compressed", reader);
+        instance.ramBytesUsed = reader.ramBytesUsed();
+        instance.norms = reader;
+        break;
+      }
+      case TABLE_COMPRESSED: {
+        data.seek(entry.offset);
+        int packedIntsVersion = data.readVInt();
+        int size = data.readVInt();
+        if (size > 256) {
+          throw new CorruptIndexException("TABLE_COMPRESSED cannot have more than 256 distinct values, got=" + size, data);
+        }
+        final long decode[] = new long[size];
+        for (int i = 0; i < decode.length; i++) {
+          decode[i] = data.readLong();
+        }
+        final int formatID = data.readVInt();
+        final int bitsPerValue = data.readVInt();
+        final PackedInts.Reader ordsReader = PackedInts.getReaderNoHeader(data, PackedInts.Format.byId(formatID), packedIntsVersion, entry.count, bitsPerValue);
+        instance.info = Accountables.namedAccountable("table compressed", ordsReader);
+        instance.ramBytesUsed = RamUsageEstimator.sizeOf(decode) + ordsReader.ramBytesUsed();
+        instance.norms = new NumericDocValues() {
+          @Override
+          public long get(int docID) {
+            return decode[(int)ordsReader.get(docID)];
+          }
+        };
+        break;
+      }
+      case INDIRECT: {
+        data.seek(entry.offset);
+        int packedIntsVersion = data.readVInt();
+        int blockSize = data.readVInt();
+        final MonotonicBlockPackedReader live = MonotonicBlockPackedReader.of(data, packedIntsVersion, blockSize, entry.count, false);
+        LoadedNorms nestedInstance = loadNorms(entry.nested);
+        instance.ramBytesUsed = live.ramBytesUsed() + nestedInstance.ramBytesUsed;
+        instance.info = Accountables.namedAccountable("indirect -> " + nestedInstance.info, instance.ramBytesUsed);
+        final NumericDocValues values = nestedInstance.norms;
+        final int upperBound = entry.count-1;
+        instance.norms = new NumericDocValues() {
+          @Override
+          public long get(int docID) {
+            int low = 0;
+            int high = upperBound;
+            while (low <= high) {
+              int mid = (low + high) >>> 1;
+              long doc = live.get(mid);
+              
+              if (doc < docID) {
+                low = mid + 1;
+              } else if (doc > docID) {
+                high = mid - 1;
+              } else {
+                return values.get(mid);
+              }
+            }
+            return 0;
+          }
+        };
+        break;
+      }
+      default:
+        throw new AssertionError();
+    }
+    return instance;
+  }
+
+  @Override
+  public void close() throws IOException {
+    data.close();
+  }
+  
+  static class NormsEntry {
+    byte format;
+    long offset;
+    int count;
+    NormsEntry nested;
+  }
+  
+  static class LoadedNorms {
+    NumericDocValues norms;
+    long ramBytesUsed;
+    Accountable info;
+  }
+
+  @Override
+  public synchronized NormsProducer getMergeInstance() throws IOException {
+    return new Lucene50NormsProducer(this);
+  }
+
+  @Override
+  public String toString() {
+    return getClass().getSimpleName() + "(fields=" + norms.size() + ",active=" + activeCount.get() + ")";
+  }
+}

Property changes on: lucene/core/src/java/org/apache/lucene/codecs/lucene50/Lucene50NormsProducer.java
___________________________________________________________________
Added: svn:eol-style
## -0,0 +1 ##
+native
\ No newline at end of property
Index: lucene/core/src/java/org/apache/lucene/codecs/lucene50/Lucene50StoredFieldsFormat.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/codecs/lucene50/Lucene50StoredFieldsFormat.java	(revision 0)
+++ lucene/core/src/java/org/apache/lucene/codecs/lucene50/Lucene50StoredFieldsFormat.java	(working copy)
@@ -0,0 +1,124 @@
+package org.apache.lucene.codecs.lucene50;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.codecs.CodecUtil;
+import org.apache.lucene.codecs.StoredFieldsFormat;
+import org.apache.lucene.codecs.compressing.CompressingStoredFieldsFormat;
+import org.apache.lucene.codecs.compressing.CompressingStoredFieldsIndexWriter;
+import org.apache.lucene.codecs.compressing.CompressionMode;
+import org.apache.lucene.index.StoredFieldVisitor;
+import org.apache.lucene.store.DataOutput;
+import org.apache.lucene.util.packed.PackedInts;
+
+/**
+ * Lucene 5.0 stored fields format.
+ *
+ * <p><b>Principle</b></p>
+ * <p>This {@link StoredFieldsFormat} compresses blocks of 16KB of documents in
+ * order to improve the compression ratio compared to document-level
+ * compression. It uses the <a href="http://code.google.com/p/lz4/">LZ4</a>
+ * compression algorithm, which is fast to compress and very fast to decompress
+ * data. Although the compression method that is used focuses more on speed
+ * than on compression ratio, it should provide interesting compression ratios
+ * for redundant inputs (such as log files, HTML or plain text).</p>
+ * <p><b>File formats</b></p>
+ * <p>Stored fields are represented by two files:</p>
+ * <ol>
+ * <li><a name="field_data" id="field_data"></a>
+ * <p>A fields data file (extension <tt>.fdt</tt>). This file stores a compact
+ * representation of documents in compressed blocks of 16KB or more. When
+ * writing a segment, documents are appended to an in-memory <tt>byte[]</tt>
+ * buffer. When its size reaches 16KB or more, some metadata about the documents
+ * is flushed to disk, immediately followed by a compressed representation of
+ * the buffer using the
+ * <a href="http://code.google.com/p/lz4/">LZ4</a>
+ * <a href="http://fastcompression.blogspot.fr/2011/05/lz4-explained.html">compression format</a>.</p>
+ * <p>Here is a more detailed description of the field data file format:</p>
+ * <ul>
+ * <li>FieldData (.fdt) --&gt; &lt;Header&gt;, PackedIntsVersion, &lt;Chunk&gt;<sup>ChunkCount</sup></li>
+ * <li>Header --&gt; {@link CodecUtil#writeSegmentHeader SegmentHeader}</li>
+ * <li>PackedIntsVersion --&gt; {@link PackedInts#VERSION_CURRENT} as a {@link DataOutput#writeVInt VInt}</li>
+ * <li>ChunkCount is not known in advance and is the number of chunks necessary to store all document of the segment</li>
+ * <li>Chunk --&gt; DocBase, ChunkDocs, DocFieldCounts, DocLengths, &lt;CompressedDocs&gt;</li>
+ * <li>DocBase --&gt; the ID of the first document of the chunk as a {@link DataOutput#writeVInt VInt}</li>
+ * <li>ChunkDocs --&gt; the number of documents in the chunk as a {@link DataOutput#writeVInt VInt}</li>
+ * <li>DocFieldCounts --&gt; the number of stored fields of every document in the chunk, encoded as followed:<ul>
+ *   <li>if chunkDocs=1, the unique value is encoded as a {@link DataOutput#writeVInt VInt}</li>
+ *   <li>else read a {@link DataOutput#writeVInt VInt} (let's call it <tt>bitsRequired</tt>)<ul>
+ *     <li>if <tt>bitsRequired</tt> is <tt>0</tt> then all values are equal, and the common value is the following {@link DataOutput#writeVInt VInt}</li>
+ *     <li>else <tt>bitsRequired</tt> is the number of bits required to store any value, and values are stored in a {@link PackedInts packed} array where every value is stored on exactly <tt>bitsRequired</tt> bits</li>
+ *   </ul></li>
+ * </ul></li>
+ * <li>DocLengths --&gt; the lengths of all documents in the chunk, encoded with the same method as DocFieldCounts</li>
+ * <li>CompressedDocs --&gt; a compressed representation of &lt;Docs&gt; using the LZ4 compression format</li>
+ * <li>Docs --&gt; &lt;Doc&gt;<sup>ChunkDocs</sup></li>
+ * <li>Doc --&gt; &lt;FieldNumAndType, Value&gt;<sup>DocFieldCount</sup></li>
+ * <li>FieldNumAndType --&gt; a {@link DataOutput#writeVLong VLong}, whose 3 last bits are Type and other bits are FieldNum</li>
+ * <li>Type --&gt;<ul>
+ *   <li>0: Value is String</li>
+ *   <li>1: Value is BinaryValue</li>
+ *   <li>2: Value is Int</li>
+ *   <li>3: Value is Float</li>
+ *   <li>4: Value is Long</li>
+ *   <li>5: Value is Double</li>
+ *   <li>6, 7: unused</li>
+ * </ul></li>
+ * <li>FieldNum --&gt; an ID of the field</li>
+ * <li>Value --&gt; {@link DataOutput#writeString(String) String} | BinaryValue | Int | Float | Long | Double depending on Type</li>
+ * <li>BinaryValue --&gt; ValueLength &lt;Byte&gt;<sup>ValueLength</sup></li>
+ * </ul>
+ * <p>Notes</p>
+ * <ul>
+ * <li>If documents are larger than 16KB then chunks will likely contain only
+ * one document. However, documents can never spread across several chunks (all
+ * fields of a single document are in the same chunk).</li>
+ * <li>When at least one document in a chunk is large enough so that the chunk
+ * is larger than 32KB, the chunk will actually be compressed in several LZ4
+ * blocks of 16KB. This allows {@link StoredFieldVisitor}s which are only
+ * interested in the first fields of a document to not have to decompress 10MB
+ * of data if the document is 10MB, but only 16KB.</li>
+ * <li>Given that the original lengths are written in the metadata of the chunk,
+ * the decompressor can leverage this information to stop decoding as soon as
+ * enough data has been decompressed.</li>
+ * <li>In case documents are incompressible, CompressedDocs will be less than
+ * 0.5% larger than Docs.</li>
+ * </ul>
+ * </li>
+ * <li><a name="field_index" id="field_index"></a>
+ * <p>A fields index file (extension <tt>.fdx</tt>).</p>
+ * <ul>
+ * <li>FieldsIndex (.fdx) --&gt; &lt;Header&gt;, &lt;ChunkIndex&gt;</li>
+ * <li>Header --&gt; {@link CodecUtil#writeSegmentHeader SegmentHeader}</li>
+ * <li>ChunkIndex: See {@link CompressingStoredFieldsIndexWriter}</li>
+ * </ul>
+ * </li>
+ * </ol>
+ * <p><b>Known limitations</b></p>
+ * <p>This {@link StoredFieldsFormat} does not support individual documents
+ * larger than (<tt>2<sup>31</sup> - 2<sup>14</sup></tt>) bytes.</p>
+ * @lucene.experimental
+ */
+public final class Lucene50StoredFieldsFormat extends CompressingStoredFieldsFormat {
+
+  /** Sole constructor. */
+  public Lucene50StoredFieldsFormat() {
+    super("Lucene50StoredFields", CompressionMode.FAST, 1 << 14);
+  }
+
+}

Property changes on: lucene/core/src/java/org/apache/lucene/codecs/lucene50/Lucene50StoredFieldsFormat.java
___________________________________________________________________
Added: svn:eol-style
## -0,0 +1 ##
+native
\ No newline at end of property
Index: lucene/core/src/java/org/apache/lucene/codecs/lucene50/Lucene50TermVectorsFormat.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/codecs/lucene50/Lucene50TermVectorsFormat.java	(revision 0)
+++ lucene/core/src/java/org/apache/lucene/codecs/lucene50/Lucene50TermVectorsFormat.java	(working copy)
@@ -0,0 +1,131 @@
+package org.apache.lucene.codecs.lucene50;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.codecs.CodecUtil;
+import org.apache.lucene.codecs.TermVectorsFormat;
+import org.apache.lucene.codecs.compressing.CompressingStoredFieldsIndexWriter;
+import org.apache.lucene.codecs.compressing.CompressingTermVectorsFormat;
+import org.apache.lucene.codecs.compressing.CompressionMode;
+import org.apache.lucene.store.DataOutput;
+import org.apache.lucene.util.packed.BlockPackedWriter;
+import org.apache.lucene.util.packed.PackedInts;
+
+/**
+ * Lucene 5.0 {@link TermVectorsFormat term vectors format}.
+ * <p>
+ * Very similarly to {@link Lucene50StoredFieldsFormat}, this format is based
+ * on compressed chunks of data, with document-level granularity so that a
+ * document can never span across distinct chunks. Moreover, data is made as
+ * compact as possible:<ul>
+ * <li>textual data is compressed using the very light,
+ * <a href="http://code.google.com/p/lz4/">LZ4</a> compression algorithm,
+ * <li>binary data is written using fixed-size blocks of
+ * {@link PackedInts packed ints}.
+ * </ul>
+ * <p>
+ * Term vectors are stored using two files<ul>
+ * <li>a data file where terms, frequencies, positions, offsets and payloads
+ * are stored,
+ * <li>an index file, loaded into memory, used to locate specific documents in
+ * the data file.
+ * </ul>
+ * Looking up term vectors for any document requires at most 1 disk seek.
+ * <p><b>File formats</b>
+ * <ol>
+ * <li><a name="vector_data" id="vector_data"></a>
+ * <p>A vector data file (extension <tt>.tvd</tt>). This file stores terms,
+ * frequencies, positions, offsets and payloads for every document. Upon writing
+ * a new segment, it accumulates data into memory until the buffer used to store
+ * terms and payloads grows beyond 4KB. Then it flushes all metadata, terms
+ * and positions to disk using <a href="http://code.google.com/p/lz4/">LZ4</a>
+ * compression for terms and payloads and
+ * {@link BlockPackedWriter blocks of packed ints} for positions.</p>
+ * <p>Here is a more detailed description of the field data file format:</p>
+ * <ul>
+ * <li>VectorData (.tvd) --&gt; &lt;Header&gt;, PackedIntsVersion, ChunkSize, &lt;Chunk&gt;<sup>ChunkCount</sup>, Footer</li>
+ * <li>Header --&gt; {@link CodecUtil#writeSegmentHeader SegmentHeader}</li>
+ * <li>PackedIntsVersion --&gt; {@link PackedInts#VERSION_CURRENT} as a {@link DataOutput#writeVInt VInt}</li>
+ * <li>ChunkSize is the number of bytes of terms to accumulate before flushing, as a {@link DataOutput#writeVInt VInt}</li>
+ * <li>ChunkCount is not known in advance and is the number of chunks necessary to store all document of the segment</li>
+ * <li>Chunk --&gt; DocBase, ChunkDocs, &lt; NumFields &gt;, &lt; FieldNums &gt;, &lt; FieldNumOffs &gt;, &lt; Flags &gt;,
+ * &lt; NumTerms &gt;, &lt; TermLengths &gt;, &lt; TermFreqs &gt;, &lt; Positions &gt;, &lt; StartOffsets &gt;, &lt; Lengths &gt;,
+ * &lt; PayloadLengths &gt;, &lt; TermAndPayloads &gt;</li>
+ * <li>DocBase is the ID of the first doc of the chunk as a {@link DataOutput#writeVInt VInt}</li>
+ * <li>ChunkDocs is the number of documents in the chunk</li>
+ * <li>NumFields --&gt; DocNumFields<sup>ChunkDocs</sup></li>
+ * <li>DocNumFields is the number of fields for each doc, written as a {@link DataOutput#writeVInt VInt} if ChunkDocs==1 and as a {@link PackedInts} array otherwise</li>
+ * <li>FieldNums --&gt; FieldNumDelta<sup>TotalDistincFields</sup>, a delta-encoded list of the sorted unique field numbers present in the chunk</li>
+ * <li>FieldNumOffs --&gt; FieldNumOff<sup>TotalFields</sup>, as a {@link PackedInts} array</li>
+ * <li>FieldNumOff is the offset of the field number in FieldNums</li>
+ * <li>TotalFields is the total number of fields (sum of the values of NumFields)</li>
+ * <li>Flags --&gt; Bit &lt; FieldFlags &gt;</li>
+ * <li>Bit  is a single bit which when true means that fields have the same options for every document in the chunk</li>
+ * <li>FieldFlags --&gt; if Bit==1: Flag<sup>TotalDistinctFields</sup> else Flag<sup>TotalFields</sup></li>
+ * <li>Flag: a 3-bits int where:<ul>
+ * <li>the first bit means that the field has positions</li>
+ * <li>the second bit means that the field has offsets</li>
+ * <li>the third bit means that the field has payloads</li>
+ * </ul></li>
+ * <li>NumTerms --&gt; FieldNumTerms<sup>TotalFields</sup></li>
+ * <li>FieldNumTerms: the number of terms for each field, using {@link BlockPackedWriter blocks of 64 packed ints}</li>
+ * <li>TermLengths --&gt; PrefixLength<sup>TotalTerms</sup> SuffixLength<sup>TotalTerms</sup></li>
+ * <li>TotalTerms: total number of terms (sum of NumTerms)</li>
+ * <li>PrefixLength: 0 for the first term of a field, the common prefix with the previous term otherwise using {@link BlockPackedWriter blocks of 64 packed ints}</li>
+ * <li>SuffixLength: length of the term minus PrefixLength for every term using {@link BlockPackedWriter blocks of 64 packed ints}</li>
+ * <li>TermFreqs --&gt; TermFreqMinus1<sup>TotalTerms</sup></li>
+ * <li>TermFreqMinus1: (frequency - 1) for each term using  {@link BlockPackedWriter blocks of 64 packed ints}</li>
+ * <li>Positions --&gt; PositionDelta<sup>TotalPositions</sup></li>
+ * <li>TotalPositions is the sum of frequencies of terms of all fields that have positions</li>
+ * <li>PositionDelta: the absolute position for the first position of a term, and the difference with the previous positions for following positions using {@link BlockPackedWriter blocks of 64 packed ints}</li>
+ * <li>StartOffsets --&gt; (AvgCharsPerTerm<sup>TotalDistinctFields</sup>) StartOffsetDelta<sup>TotalOffsets</sup></li>
+ * <li>TotalOffsets is the sum of frequencies of terms of all fields that have offsets</li>
+ * <li>AvgCharsPerTerm: average number of chars per term, encoded as a float on 4 bytes. They are not present if no field has both positions and offsets enabled.</li>
+ * <li>StartOffsetDelta: (startOffset - previousStartOffset - AvgCharsPerTerm * PositionDelta). previousStartOffset is 0 for the first offset and AvgCharsPerTerm is 0 if the field has no positions using  {@link BlockPackedWriter blocks of 64 packed ints}</li>
+ * <li>Lengths --&gt; LengthMinusTermLength<sup>TotalOffsets</sup></li>
+ * <li>LengthMinusTermLength: (endOffset - startOffset - termLength) using  {@link BlockPackedWriter blocks of 64 packed ints}</li>
+ * <li>PayloadLengths --&gt; PayloadLength<sup>TotalPayloads</sup></li>
+ * <li>TotalPayloads is the sum of frequencies of terms of all fields that have payloads</li>
+ * <li>PayloadLength is the payload length encoded using  {@link BlockPackedWriter blocks of 64 packed ints}</li>
+ * <li>TermAndPayloads --&gt; LZ4-compressed representation of &lt; FieldTermsAndPayLoads &gt;<sup>TotalFields</sup></li>
+ * <li>FieldTermsAndPayLoads --&gt; Terms (Payloads)</li>
+ * <li>Terms: term bytes</li>
+ * <li>Payloads: payload bytes (if the field has payloads)</li>
+ * <li>Footer --&gt; {@link CodecUtil#writeFooter CodecFooter}</li>
+ * </ul>
+ * </li>
+ * <li><a name="vector_index" id="vector_index"></a>
+ * <p>An index file (extension <tt>.tvx</tt>).</p>
+ * <ul>
+ * <li>VectorIndex (.tvx) --&gt; &lt;Header&gt;, &lt;ChunkIndex&gt;, Footer</li>
+ * <li>Header --&gt; {@link CodecUtil#writeSegmentHeader SegmentHeader}</li>
+ * <li>ChunkIndex: See {@link CompressingStoredFieldsIndexWriter}</li>
+ * <li>Footer --&gt; {@link CodecUtil#writeFooter CodecFooter}</li>
+ * </ul>
+ * </li>
+ * </ol>
+ * @lucene.experimental
+ */
+public final class Lucene50TermVectorsFormat extends CompressingTermVectorsFormat {
+
+  /** Sole constructor. */
+  public Lucene50TermVectorsFormat() {
+    super("Lucene50TermVectors", "", CompressionMode.FAST, 1 << 12);
+  }
+
+}

Property changes on: lucene/core/src/java/org/apache/lucene/codecs/lucene50/Lucene50TermVectorsFormat.java
___________________________________________________________________
Added: svn:eol-style
## -0,0 +1 ##
+native
\ No newline at end of property
Index: lucene/core/src/java/org/apache/lucene/codecs/lucene50/package.html
===================================================================
--- lucene/core/src/java/org/apache/lucene/codecs/lucene50/package.html	(revision 1629405)
+++ lucene/core/src/java/org/apache/lucene/codecs/lucene50/package.html	(working copy)
@@ -147,7 +147,7 @@
    This contains the set of field names used in the index.
 </li>
 <li>
-{@link org.apache.lucene.codecs.lucene41.Lucene41StoredFieldsFormat Stored Field values}. 
+{@link org.apache.lucene.codecs.lucene50.Lucene50StoredFieldsFormat Stored Field values}. 
 This contains, for each document, a list of attribute-value pairs, where the attributes 
 are field names. These are used to store auxiliary information about the document, such as 
 its title, url, or an identifier to access a database. The set of stored fields are what is 
@@ -173,12 +173,12 @@
 all documents omit position data.
 </li>
 <li>
-{@link org.apache.lucene.codecs.lucene49.Lucene49NormsFormat Normalization factors}. 
+{@link org.apache.lucene.codecs.lucene50.Lucene50NormsFormat Normalization factors}. 
 For each field in each document, a value is stored
 that is multiplied into the score for hits on that field.
 </li>
 <li>
-{@link org.apache.lucene.codecs.lucene42.Lucene42TermVectorsFormat Term Vectors}. 
+{@link org.apache.lucene.codecs.lucene50.Lucene50TermVectorsFormat Term Vectors}. 
 For each field in each document, the term vector (sometimes
 called document vector) may be stored. A term vector consists of term text and
 term frequency. To add Term Vectors to your index see the 
@@ -243,7 +243,7 @@
 <td>Stores metadata about a segment</td>
 </tr>
 <tr>
-<td>{@link org.apache.lucene.store.CompoundFileDirectory Compound File}</td>
+<td>{@link org.apache.lucene.codecs.lucene50.Lucene50CompoundFormat Compound File}</td>
 <td>.cfs, .cfe</td>
 <td>An optional "virtual" file consisting of all the other index files for
 systems that frequently run out of file handles.</td>
@@ -254,12 +254,12 @@
 <td>Stores information about the fields</td>
 </tr>
 <tr>
-<td>{@link org.apache.lucene.codecs.lucene41.Lucene41StoredFieldsFormat Field Index}</td>
+<td>{@link org.apache.lucene.codecs.lucene50.Lucene50StoredFieldsFormat Field Index}</td>
 <td>.fdx</td>
 <td>Contains pointers to field data</td>
 </tr>
 <tr>
-<td>{@link org.apache.lucene.codecs.lucene41.Lucene41StoredFieldsFormat Field Data}</td>
+<td>{@link org.apache.lucene.codecs.lucene50.Lucene50StoredFieldsFormat Field Data}</td>
 <td>.fdt</td>
 <td>The stored fields for documents</td>
 </tr>
@@ -289,7 +289,7 @@
 <td>Stores additional per-position metadata information such as character offsets and user payloads</td>
 </tr>
 <tr>
-<td>{@link org.apache.lucene.codecs.lucene49.Lucene49NormsFormat Norms}</td>
+<td>{@link org.apache.lucene.codecs.lucene50.Lucene50NormsFormat Norms}</td>
 <td>.nvd, .nvm</td>
 <td>Encodes length and boost factors for docs and fields</td>
 </tr>
@@ -299,17 +299,17 @@
 <td>Encodes additional scoring factors or other per-document information.</td>
 </tr>
 <tr>
-<td>{@link org.apache.lucene.codecs.lucene42.Lucene42TermVectorsFormat Term Vector Index}</td>
+<td>{@link org.apache.lucene.codecs.lucene50.Lucene50TermVectorsFormat Term Vector Index}</td>
 <td>.tvx</td>
 <td>Stores offset into the document data file</td>
 </tr>
 <tr>
-<td>{@link org.apache.lucene.codecs.lucene42.Lucene42TermVectorsFormat Term Vector Documents}</td>
+<td>{@link org.apache.lucene.codecs.lucene50.Lucene50TermVectorsFormat Term Vector Documents}</td>
 <td>.tvd</td>
 <td>Contains information about each document that has term vectors</td>
 </tr>
 <tr>
-<td>{@link org.apache.lucene.codecs.lucene42.Lucene42TermVectorsFormat Term Vector Fields}</td>
+<td>{@link org.apache.lucene.codecs.lucene50.Lucene50TermVectorsFormat Term Vector Fields}</td>
 <td>.tvf</td>
 <td>The field level info about term vectors</td>
 </tr>
Index: lucene/core/src/java/org/apache/lucene/codecs/perfield/PerFieldDocValuesFormat.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/codecs/perfield/PerFieldDocValuesFormat.java	(revision 1629405)
+++ lucene/core/src/java/org/apache/lucene/codecs/perfield/PerFieldDocValuesFormat.java	(working copy)
@@ -19,7 +19,6 @@
 
 import java.io.Closeable;
 import java.io.IOException;
-import java.util.Collections;
 import java.util.HashMap;
 import java.util.IdentityHashMap;
 import java.util.Map;
@@ -148,7 +147,10 @@
       final String formatName = format.getName();
       
       String previousValue = field.putAttribute(PER_FIELD_FORMAT_KEY, formatName);
-      assert field.getDocValuesGen() != -1 || previousValue == null: "formatName=" + formatName + " prevValue=" + previousValue;
+      if (field.getDocValuesGen() == -1 && previousValue != null) {
+        throw new IllegalStateException("found existing value for " + PER_FIELD_FORMAT_KEY + 
+                                        ", field=" + field.name + ", old=" + previousValue + ", new=" + formatName);
+      }
       
       Integer suffix = null;
       
@@ -190,7 +192,10 @@
       }
       
       previousValue = field.putAttribute(PER_FIELD_SUFFIX_KEY, Integer.toString(suffix));
-      assert field.getDocValuesGen() != -1 || previousValue == null : "suffix=" + Integer.toString(suffix) + " prevValue=" + previousValue;
+      if (field.getDocValuesGen() == -1 && previousValue != null) {
+        throw new IllegalStateException("found existing value for " + PER_FIELD_SUFFIX_KEY + 
+                                        ", field=" + field.name + ", old=" + previousValue + ", new=" + suffix);
+      }
 
       // TODO: we should only provide the "slice" of FIS
       // that this DVF actually sees ...
@@ -220,7 +225,25 @@
 
     private final Map<String,DocValuesProducer> fields = new TreeMap<>();
     private final Map<String,DocValuesProducer> formats = new HashMap<>();
+    
+    // clone for merge
+    FieldsReader(FieldsReader other) throws IOException {
+      Map<DocValuesProducer,DocValuesProducer> oldToNew = new IdentityHashMap<>();
+      // First clone all formats
+      for(Map.Entry<String,DocValuesProducer> ent : other.formats.entrySet()) {
+        DocValuesProducer values = ent.getValue().getMergeInstance();
+        formats.put(ent.getKey(), values);
+        oldToNew.put(ent.getValue(), values);
+      }
 
+      // Then rebuild fields:
+      for(Map.Entry<String,DocValuesProducer> ent : other.fields.entrySet()) {
+        DocValuesProducer producer = oldToNew.get(ent.getValue());
+        assert producer != null;
+        fields.put(ent.getKey(), producer);
+      }
+    }
+
     public FieldsReader(final SegmentReadState readState) throws IOException {
 
       // Init each unique format:
@@ -234,7 +257,9 @@
             if (formatName != null) {
               // null formatName means the field is in fieldInfos, but has no docvalues!
               final String suffix = fi.getAttribute(PER_FIELD_SUFFIX_KEY);
-              assert suffix != null;
+              if (suffix == null) {
+                throw new IllegalStateException("missing attribute: " + PER_FIELD_SUFFIX_KEY + " for field: " + fieldName);
+              }
               DocValuesFormat format = DocValuesFormat.forName(formatName);
               String segmentSuffix = getFullSegmentSuffix(readState.segmentSuffix, getSuffix(formatName, suffix));
               if (!formats.containsKey(segmentSuffix)) {
@@ -252,24 +277,6 @@
       }
     }
 
-    private FieldsReader(FieldsReader other) {
-
-      Map<DocValuesProducer,DocValuesProducer> oldToNew = new IdentityHashMap<>();
-      // First clone all formats
-      for(Map.Entry<String,DocValuesProducer> ent : other.formats.entrySet()) {
-        DocValuesProducer values = ent.getValue();
-        formats.put(ent.getKey(), values);
-        oldToNew.put(ent.getValue(), values);
-      }
-
-      // Then rebuild fields:
-      for(Map.Entry<String,DocValuesProducer> ent : other.fields.entrySet()) {
-        DocValuesProducer producer = oldToNew.get(ent.getValue());
-        assert producer != null;
-        fields.put(ent.getKey(), producer);
-      }
-    }
-
     @Override
     public NumericDocValues getNumeric(FieldInfo field) throws IOException {
       DocValuesProducer producer = fields.get(field.name);
@@ -312,11 +319,6 @@
     }
 
     @Override
-    public DocValuesProducer clone() {
-      return new FieldsReader(this);
-    }
-
-    @Override
     public long ramBytesUsed() {
       long size = 0;
       for (Map.Entry<String,DocValuesProducer> entry : formats.entrySet()) {
@@ -339,6 +341,11 @@
     }
     
     @Override
+    public DocValuesProducer getMergeInstance() throws IOException {
+      return new FieldsReader(this);
+    }
+
+    @Override
     public String toString() {
       return "PerFieldDocValues(formats=" + formats.size() + ")";
     }
Index: lucene/core/src/java/org/apache/lucene/codecs/perfield/PerFieldPostingsFormat.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/codecs/perfield/PerFieldPostingsFormat.java	(revision 1629405)
+++ lucene/core/src/java/org/apache/lucene/codecs/perfield/PerFieldPostingsFormat.java	(working copy)
@@ -22,6 +22,7 @@
 import java.util.ArrayList;
 import java.util.Collections;
 import java.util.HashMap;
+import java.util.IdentityHashMap;
 import java.util.Iterator;
 import java.util.List;
 import java.util.Map;
@@ -156,16 +157,24 @@
           formatToGroups.put(format, group);
         } else {
           // we've already seen this format, so just grab its suffix
-          assert suffixes.containsKey(formatName);
+          if (!suffixes.containsKey(formatName)) {
+            throw new IllegalStateException("no suffix for format name: " + formatName + ", expected: " + group.suffix);
+          }
         }
 
         group.fields.add(field);
 
         String previousValue = fieldInfo.putAttribute(PER_FIELD_FORMAT_KEY, formatName);
-        assert previousValue == null;
+        if (previousValue != null) {
+          throw new IllegalStateException("found existing value for " + PER_FIELD_FORMAT_KEY + 
+                                          ", field=" + fieldInfo.name + ", old=" + previousValue + ", new=" + formatName);
+        }
 
         previousValue = fieldInfo.putAttribute(PER_FIELD_SUFFIX_KEY, Integer.toString(group.suffix));
-        assert previousValue == null;
+        if (previousValue != null) {
+          throw new IllegalStateException("found existing value for " + PER_FIELD_SUFFIX_KEY + 
+                                          ", field=" + fieldInfo.name + ", old=" + previousValue + ", new=" + group.suffix);
+        }
       }
 
       // Second pass: write postings
@@ -207,7 +216,25 @@
 
     private final Map<String,FieldsProducer> fields = new TreeMap<>();
     private final Map<String,FieldsProducer> formats = new HashMap<>();
+    
+    // clone for merge
+    FieldsReader(FieldsReader other) throws IOException {
+      Map<FieldsProducer,FieldsProducer> oldToNew = new IdentityHashMap<>();
+      // First clone all formats
+      for(Map.Entry<String,FieldsProducer> ent : other.formats.entrySet()) {
+        FieldsProducer values = ent.getValue().getMergeInstance();
+        formats.put(ent.getKey(), values);
+        oldToNew.put(ent.getValue(), values);
+      }
 
+      // Then rebuild fields:
+      for(Map.Entry<String,FieldsProducer> ent : other.fields.entrySet()) {
+        FieldsProducer producer = oldToNew.get(ent.getValue());
+        assert producer != null;
+        fields.put(ent.getKey(), producer);
+      }
+    }
+
     public FieldsReader(final SegmentReadState readState) throws IOException {
 
       // Read _X.per and init each format:
@@ -221,7 +248,9 @@
             if (formatName != null) {
               // null formatName means the field is in fieldInfos, but has no postings!
               final String suffix = fi.getAttribute(PER_FIELD_SUFFIX_KEY);
-              assert suffix != null;
+              if (suffix == null) {
+                throw new IllegalStateException("missing attribute: " + PER_FIELD_SUFFIX_KEY + " for field: " + fieldName);
+              }
               PostingsFormat format = PostingsFormat.forName(formatName);
               String segmentSuffix = getSuffix(formatName, suffix);
               if (!formats.containsKey(segmentSuffix)) {
@@ -284,6 +313,11 @@
     }
 
     @Override
+    public FieldsProducer getMergeInstance() throws IOException {
+      return new FieldsReader(this);
+    }
+
+    @Override
     public String toString() {
       return "PerFieldPostings(formats=" + formats.size() + ")";
     }
Index: lucene/core/src/java/org/apache/lucene/index/IndexFileNames.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/index/IndexFileNames.java	(revision 1629405)
+++ lucene/core/src/java/org/apache/lucene/index/IndexFileNames.java	(working copy)
@@ -53,12 +53,6 @@
   /** Name of the generation reference file name */
   public static final String OLD_SEGMENTS_GEN = "segments.gen";
 
-  /** Extension of compound file */
-  public static final String COMPOUND_FILE_EXTENSION = "cfs";
-  
-  /** Extension of compound file entries */
-  public static final String COMPOUND_FILE_ENTRIES_EXTENSION = "cfe";
-
   /**
    * Computes the full file name from base, extension and generation. If the
    * generation is -1, the file name is null. If it's 0, the file name is
Index: lucene/core/src/java/org/apache/lucene/index/IndexWriter.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/index/IndexWriter.java	(revision 1629405)
+++ lucene/core/src/java/org/apache/lucene/index/IndexWriter.java	(working copy)
@@ -41,6 +41,7 @@
 
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.codecs.Codec;
+import org.apache.lucene.codecs.FieldInfosReader;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.index.DocValuesUpdate.BinaryDocValuesUpdate;
 import org.apache.lucene.index.DocValuesUpdate.NumericDocValuesUpdate;
@@ -50,7 +51,6 @@
 import org.apache.lucene.index.MergeState.CheckAbort;
 import org.apache.lucene.search.Query;
 import org.apache.lucene.store.AlreadyClosedException;
-import org.apache.lucene.store.CompoundFileDirectory;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.IOContext;
 import org.apache.lucene.store.Lock;
@@ -865,6 +865,28 @@
       }
     }
   }
+  
+  // reads latest field infos for the commit
+  // this is used on IW init and addIndexes(Dir) to create/update the global field map.
+  // TODO: fix tests abusing this method!
+  static FieldInfos readFieldInfos(SegmentCommitInfo si) throws IOException {
+    Codec codec = si.info.getCodec();
+    FieldInfosReader reader = codec.fieldInfosFormat().getFieldInfosReader();
+    
+    if (si.hasFieldUpdates()) {
+      // there are updates, we read latest (always outside of CFS)
+      final String segmentSuffix = Long.toString(si.getFieldInfosGen(), Character.MAX_RADIX);
+      return reader.read(si.info.dir, si.info, segmentSuffix, IOContext.READONCE);
+    } else if (si.info.getUseCompoundFile()) {
+      // cfs
+      try (Directory cfs = codec.compoundFormat().getCompoundReader(si.info.dir, si.info, IOContext.DEFAULT)) {
+        return reader.read(cfs, si.info, "", IOContext.READONCE);
+      }
+    } else {
+      // no cfs
+      return reader.read(si.info.dir, si.info, "", IOContext.READONCE);
+    }
+  }
 
   /**
    * Loads or returns the already loaded the global field number map for this {@link SegmentInfos}.
@@ -874,7 +896,8 @@
     final FieldNumbers map = new FieldNumbers();
 
     for(SegmentCommitInfo info : segmentInfos) {
-      for(FieldInfo fi : SegmentReader.readFieldInfos(info)) {
+      FieldInfos fis = readFieldInfos(info);
+      for(FieldInfo fi : fis) {
         map.addOrGet(fi.name, fi.number, fi.getDocValuesType());
       }
     }
@@ -2388,7 +2411,8 @@
 
             IOContext context = new IOContext(new MergeInfo(info.info.getDocCount(), info.sizeInBytes(), true, -1));
 
-            for(FieldInfo fi : SegmentReader.readFieldInfos(info)) {
+            FieldInfos fis = readFieldInfos(info);
+            for(FieldInfo fi : fis) {
               globalFieldNumberMap.addOrGet(fi.name, fi.number, fi.getDocValuesType());
             }
             infos.add(copySegmentAsIs(info, newSegName, context));
@@ -2512,7 +2536,7 @@
 
       SegmentMerger merger = new SegmentMerger(mergeReaders, info, infoStream, trackingDir,
                                                MergeState.CheckAbort.NONE, globalFieldNumberMap, 
-                                               context, config.getCheckIntegrityAtMerge());
+                                               context);
       
       if (!merger.shouldMerge()) {
         return;
@@ -2570,7 +2594,7 @@
       // above:
       success = false;
       try {
-        codec.segmentInfoFormat().getSegmentInfoWriter().write(trackingDir, info, mergeState.fieldInfos, context);
+        codec.segmentInfoFormat().getSegmentInfoWriter().write(trackingDir, info, mergeState.mergeFieldInfos, context);
         success = true;
       } finally {
         if (!success) {
@@ -3909,19 +3933,15 @@
       final SegmentMerger merger = new SegmentMerger(merge.getMergeReaders(),
           merge.info.info, infoStream, dirWrapper,
           checkAbort, globalFieldNumberMap, 
-          context, config.getCheckIntegrityAtMerge());
+                                                     context);
 
       merge.checkAborted(directory);
 
       // This is where all the work happens:
-      MergeState mergeState;
       boolean success3 = false;
       try {
-        if (!merger.shouldMerge()) {
-          // would result in a 0 document segment: nothing to merge!
-          mergeState = new MergeState(new ArrayList<LeafReader>(), merge.info.info, infoStream, checkAbort);
-        } else {
-          mergeState = merger.merge();
+        if (merger.shouldMerge()) {
+          merger.merge();
         }
         success3 = true;
       } finally {
@@ -3931,6 +3951,7 @@
           }
         }
       }
+      MergeState mergeState = merger.mergeState;
       assert mergeState.segmentInfo == merge.info.info;
       merge.info.info.setFiles(new HashSet<>(dirWrapper.getCreatedFiles()));
 
@@ -3937,18 +3958,27 @@
       // Record which codec was used to write the segment
 
       if (infoStream.isEnabled("IW")) {
-        if (merge.info.info.getDocCount() == 0) {
-          infoStream.message("IW", "merge away fully deleted segments");
+        if (merger.shouldMerge()) {
+          infoStream.message("IW", "merge codec=" + codec + " docCount=" + merge.info.info.getDocCount() + "; merged segment has " +
+                           (mergeState.mergeFieldInfos.hasVectors() ? "vectors" : "no vectors") + "; " +
+                           (mergeState.mergeFieldInfos.hasNorms() ? "norms" : "no norms") + "; " + 
+                           (mergeState.mergeFieldInfos.hasDocValues() ? "docValues" : "no docValues") + "; " + 
+                           (mergeState.mergeFieldInfos.hasProx() ? "prox" : "no prox") + "; " + 
+                           (mergeState.mergeFieldInfos.hasProx() ? "freqs" : "no freqs"));
         } else {
-          infoStream.message("IW", "merge codec=" + codec + " docCount=" + merge.info.info.getDocCount() + "; merged segment has " +
-                           (mergeState.fieldInfos.hasVectors() ? "vectors" : "no vectors") + "; " +
-                           (mergeState.fieldInfos.hasNorms() ? "norms" : "no norms") + "; " + 
-                           (mergeState.fieldInfos.hasDocValues() ? "docValues" : "no docValues") + "; " + 
-                           (mergeState.fieldInfos.hasProx() ? "prox" : "no prox") + "; " + 
-                           (mergeState.fieldInfos.hasProx() ? "freqs" : "no freqs"));
+          infoStream.message("IW", "skip merging fully deleted segments");
         }
       }
 
+      if (merger.shouldMerge() == false) {
+        // Merge would produce a 0-doc segment, so we do nothing except commit the merge to remove all the 0-doc segments that we "merged":
+        assert merge.info.info.getDocCount() == 0;
+        commitMerge(merge, mergeState);
+        return 0;
+      }
+
+      assert merge.info.info.getDocCount() > 0;
+
       // Very important to do this before opening the reader
       // because codec must know if prox was written for
       // this segment:
@@ -3961,6 +3991,7 @@
       if (useCompoundFile) {
         success = false;
 
+        String cfsFiles[] = merge.info.info.getCodec().compoundFormat().files(merge.info.info);
         Collection<String> filesToRemove = merge.info.files();
 
         try {
@@ -3985,8 +4016,9 @@
             }
 
             synchronized(this) {
-              deleter.deleteFile(IndexFileNames.segmentFileName(mergedName, "", IndexFileNames.COMPOUND_FILE_EXTENSION));
-              deleter.deleteFile(IndexFileNames.segmentFileName(mergedName, "", IndexFileNames.COMPOUND_FILE_ENTRIES_EXTENSION));
+              for (String cfsFile : cfsFiles) {
+                deleter.deleteFile(cfsFile);
+              }
               deleter.deleteNewFiles(merge.info.files());
             }
           }
@@ -4007,8 +4039,9 @@
             if (infoStream.isEnabled("IW")) {
               infoStream.message("IW", "abort merge after building CFS");
             }
-            deleter.deleteFile(IndexFileNames.segmentFileName(mergedName, "", IndexFileNames.COMPOUND_FILE_EXTENSION));
-            deleter.deleteFile(IndexFileNames.segmentFileName(mergedName, "", IndexFileNames.COMPOUND_FILE_ENTRIES_EXTENSION));
+            for (String cfsFile : cfsFiles) {
+              deleter.deleteFile(cfsFile);
+            }
             return 0;
           }
         }
@@ -4027,7 +4060,7 @@
       // above:
       boolean success2 = false;
       try {
-        codec.segmentInfoFormat().getSegmentInfoWriter().write(directory, merge.info.info, mergeState.fieldInfos, context);
+        codec.segmentInfoFormat().getSegmentInfoWriter().write(directory, merge.info.info, mergeState.mergeFieldInfos, context);
         success2 = true;
       } finally {
         if (!success2) {
@@ -4046,7 +4079,7 @@
       }
 
       final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();
-      if (poolReaders && mergedSegmentWarmer != null && merge.info.info.getDocCount() != 0) {
+      if (poolReaders && mergedSegmentWarmer != null) {
         final ReadersAndUpdates rld = readerPool.get(merge.info, true);
         final SegmentReader sr = rld.getReader(IOContext.READ);
         try {
@@ -4059,8 +4092,6 @@
         }
       }
 
-      // Force READ context because we merge deletes onto
-      // this reader:
       if (!commitMerge(merge, mergeState)) {
         // commitMerge will return false if this merge was
         // aborted
@@ -4457,40 +4488,30 @@
   static final Collection<String> createCompoundFile(InfoStream infoStream, Directory directory, CheckAbort checkAbort, final SegmentInfo info, IOContext context)
           throws IOException {
 
-    final String fileName = IndexFileNames.segmentFileName(info.name, "", IndexFileNames.COMPOUND_FILE_EXTENSION);
+    // TODO: use trackingdirectorywrapper instead of files() to know which files to delete when things fail:
+    String cfsFiles[] = info.getCodec().compoundFormat().files(info);
+    
     if (infoStream.isEnabled("IW")) {
-      infoStream.message("IW", "create compound file " + fileName);
+      infoStream.message("IW", "create compound file");
     }
     // Now merge all added files
     Collection<String> files = info.files();
-    CompoundFileDirectory cfsDir = new CompoundFileDirectory(directory, fileName, context, true);
+    
     boolean success = false;
     try {
-      for (String file : files) {
-        directory.copy(cfsDir, file, file, context);
-        checkAbort.work(directory.fileLength(file));
-      }
+      info.getCodec().compoundFormat().write(directory, info, files, checkAbort, context);
       success = true;
     } finally {
-      if (success) {
-        IOUtils.close(cfsDir);
-      } else {
-        IOUtils.closeWhileHandlingException(cfsDir);
-        try {
-          directory.deleteFile(fileName);
-        } catch (Throwable t) {
-        }
-        try {
-          directory.deleteFile(IndexFileNames.segmentFileName(info.name, "", IndexFileNames.COMPOUND_FILE_ENTRIES_EXTENSION));
-        } catch (Throwable t) {
-        }
+      if (!success) {
+        IOUtils.deleteFilesIgnoringExceptions(directory, cfsFiles);
       }
     }
 
     // Replace all previous files with the CFS/CFE files:
     Set<String> siFiles = new HashSet<>();
-    siFiles.add(fileName);
-    siFiles.add(IndexFileNames.segmentFileName(info.name, "", IndexFileNames.COMPOUND_FILE_ENTRIES_EXTENSION));
+    for (String cfsFile : cfsFiles) {
+      siFiles.add(cfsFile);
+    };
     info.setFiles(siFiles);
 
     return files;
Index: lucene/core/src/java/org/apache/lucene/index/IndexWriterConfig.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/index/IndexWriterConfig.java	(revision 1629405)
+++ lucene/core/src/java/org/apache/lucene/index/IndexWriterConfig.java	(working copy)
@@ -110,11 +110,6 @@
    *  ram buffers use <code>false</code> */
   public final static boolean DEFAULT_USE_COMPOUND_FILE_SYSTEM = true;
   
-  /** Default value for calling {@link LeafReader#checkIntegrity()} before
-   *  merging segments (set to <code>false</code>). You can set this
-   *  to <code>true</code> for additional safety. */
-  public final static boolean DEFAULT_CHECK_INTEGRITY_AT_MERGE = false;
-
   /** Default value for whether calls to {@link IndexWriter#close()} include a commit. */
   public final static boolean DEFAULT_COMMIT_ON_CLOSE = true;
   
Index: lucene/core/src/java/org/apache/lucene/index/LiveIndexWriterConfig.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/index/LiveIndexWriterConfig.java	(revision 1629405)
+++ lucene/core/src/java/org/apache/lucene/index/LiveIndexWriterConfig.java	(working copy)
@@ -94,9 +94,6 @@
   /** True if segment flushes should use compound file format */
   protected volatile boolean useCompoundFile = IndexWriterConfig.DEFAULT_USE_COMPOUND_FILE_SYSTEM;
   
-  /** True if merging should check integrity of segments before merge */
-  protected volatile boolean checkIntegrityAtMerge = IndexWriterConfig.DEFAULT_CHECK_INTEGRITY_AT_MERGE;
-
   /** True if calls to {@link IndexWriter#close()} should first do a commit. */
   protected boolean commitOnClose = IndexWriterConfig.DEFAULT_COMMIT_ON_CLOSE;
 
@@ -464,26 +461,6 @@
   }
   
   /**
-   * Sets if {@link IndexWriter} should call {@link LeafReader#checkIntegrity()}
-   * on existing segments before merging them into a new one.
-   * <p>
-   * Use <code>true</code> to enable this safety check, which can help
-   * reduce the risk of propagating index corruption from older segments 
-   * into new ones, at the expense of slower merging.
-   * </p>
-   */
-  public LiveIndexWriterConfig setCheckIntegrityAtMerge(boolean checkIntegrityAtMerge) {
-    this.checkIntegrityAtMerge = checkIntegrityAtMerge;
-    return this;
-  }
-  
-  /** Returns true if {@link LeafReader#checkIntegrity()} is called before 
-   *  merging segments. */
-  public boolean getCheckIntegrityAtMerge() {
-    return checkIntegrityAtMerge;
-  }
-
-  /**
    * Returns <code>true</code> if {@link IndexWriter#close()} should first commit before closing.
    */
   public boolean getCommitOnClose() {
@@ -513,7 +490,6 @@
     sb.append("readerPooling=").append(getReaderPooling()).append("\n");
     sb.append("perThreadHardLimitMB=").append(getRAMPerThreadHardLimitMB()).append("\n");
     sb.append("useCompoundFile=").append(getUseCompoundFile()).append("\n");
-    sb.append("checkIntegrityAtMerge=").append(getCheckIntegrityAtMerge()).append("\n");
     sb.append("commitOnClose=").append(getCommitOnClose()).append("\n");
     return sb.toString();
   }
Index: lucene/core/src/java/org/apache/lucene/index/MergeState.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/index/MergeState.java	(revision 1629405)
+++ lucene/core/src/java/org/apache/lucene/index/MergeState.java	(working copy)
@@ -17,9 +17,18 @@
  * limitations under the License.
  */
 
+import java.io.IOException;
+import java.util.Collections;
+import java.util.Iterator;
 import java.util.List;
 
+import org.apache.lucene.codecs.DocValuesProducer;
+import org.apache.lucene.codecs.FieldsProducer;
+import org.apache.lucene.codecs.NormsProducer;
+import org.apache.lucene.codecs.StoredFieldsReader;
+import org.apache.lucene.codecs.TermVectorsReader;
 import org.apache.lucene.store.Directory;
+import org.apache.lucene.util.Accountable;
 import org.apache.lucene.util.Bits;
 import org.apache.lucene.util.InfoStream;
 import org.apache.lucene.util.packed.PackedInts;
@@ -30,7 +39,372 @@
  * @lucene.experimental */
 public class MergeState {
 
+  /** {@link SegmentInfo} of the newly merged segment. */
+  public final SegmentInfo segmentInfo;
+
+  /** {@link FieldInfos} of the newly merged segment. */
+  public FieldInfos mergeFieldInfos;
+
+  /** Stored field producers being merged */
+  public final StoredFieldsReader[] storedFieldsReaders;
+
+  /** Term vector producers being merged */
+  public final TermVectorsReader[] termVectorsReaders;
+
+  /** Norms producers being merged */
+  public final NormsProducer[] normsProducers;
+
+  /** DocValues producers being merged */
+  public final DocValuesProducer[] docValuesProducers;
+
+  /** FieldInfos being merged */
+  public final FieldInfos[] fieldInfos;
+
+  /** Live docs for each reader */
+  public final Bits[] liveDocs;
+
+  /** Maps docIDs around deletions. */
+  public final DocMap[] docMaps;
+
+  /** Postings to merge */
+  public final FieldsProducer[] fieldsProducers;
+
+  /** New docID base per reader. */
+  public final int[] docBase;
+
+  /** Max docs per reader */
+  public final int[] maxDocs;
+
+  /** Holds the CheckAbort instance, which is invoked
+   *  periodically to see if the merge has been aborted. */
+  public final CheckAbort checkAbort;
+
+  /** InfoStream for debugging messages. */
+  public final InfoStream infoStream;
+
+  /** Counter used for periodic calls to checkAbort
+   * @lucene.internal */
+  public int checkAbortCount;
+
+  /** Sole constructor. */
+  MergeState(List<LeafReader> readers, SegmentInfo segmentInfo, InfoStream infoStream, CheckAbort checkAbort) throws IOException {
+
+    int numReaders = readers.size();
+    docMaps = new DocMap[numReaders];
+    docBase = new int[numReaders];
+    maxDocs = new int[numReaders];
+    fieldsProducers = new FieldsProducer[numReaders];
+    normsProducers = new NormsProducer[numReaders];
+    storedFieldsReaders = new StoredFieldsReader[numReaders];
+    termVectorsReaders = new TermVectorsReader[numReaders];
+    docValuesProducers = new DocValuesProducer[numReaders];
+    fieldInfos = new FieldInfos[numReaders];
+    liveDocs = new Bits[numReaders];
+
+    for(int i=0;i<numReaders;i++) {
+      final LeafReader reader = readers.get(i);
+
+      maxDocs[i] = reader.maxDoc();
+      liveDocs[i] = reader.getLiveDocs();
+      fieldInfos[i] = reader.getFieldInfos();
+
+      NormsProducer normsProducer;
+      DocValuesProducer docValuesProducer;
+      StoredFieldsReader storedFieldsReader;
+      TermVectorsReader termVectorsReader;
+      FieldsProducer fieldsProducer;
+      if (reader instanceof SegmentReader) {
+        SegmentReader segmentReader = (SegmentReader) reader;
+        normsProducer = segmentReader.getNormsReader();
+        if (normsProducer != null) {
+          normsProducer = normsProducer.getMergeInstance();
+        }
+        docValuesProducer = segmentReader.getDocValuesReader();
+        if (docValuesProducer != null) {
+          docValuesProducer = docValuesProducer.getMergeInstance();
+        }
+        storedFieldsReader = segmentReader.getFieldsReader();
+        if (storedFieldsReader != null) {
+          storedFieldsReader = storedFieldsReader.getMergeInstance();
+        }
+        termVectorsReader = segmentReader.getTermVectorsReader();
+        if (termVectorsReader != null) {
+          termVectorsReader = termVectorsReader.getMergeInstance();
+        }
+        fieldsProducer = segmentReader.fields();
+        if (fieldsProducer != null) {
+          fieldsProducer = fieldsProducer.getMergeInstance();
+        }
+      } else {
+        // A "foreign" reader
+        normsProducer = readerToNormsProducer(reader);
+        docValuesProducer = readerToDocValuesProducer(reader);
+        storedFieldsReader = readerToStoredFieldsReader(reader);
+        termVectorsReader = readerToTermVectorsReader(reader);
+        fieldsProducer = readerToFieldsProducer(reader);
+      }
+
+      normsProducers[i] = normsProducer;
+      docValuesProducers[i] = docValuesProducer;
+      storedFieldsReaders[i] = storedFieldsReader;
+      termVectorsReaders[i] = termVectorsReader;
+      fieldsProducers[i] = fieldsProducer;
+    }
+
+    this.segmentInfo = segmentInfo;
+    this.infoStream = infoStream;
+    this.checkAbort = checkAbort;
+
+    setDocMaps(readers);
+  }
+
+  private NormsProducer readerToNormsProducer(final LeafReader reader) {
+    return new NormsProducer() {
+
+      @Override
+      public NumericDocValues getNorms(FieldInfo field) throws IOException {
+        return reader.getNormValues(field.name);
+      }
+
+      @Override
+      public void checkIntegrity() throws IOException {
+        // We already checkIntegrity the entire reader up front in SegmentMerger
+      }
+
+      @Override
+      public void close() {
+      }
+
+      @Override
+      public long ramBytesUsed() {
+        return 0;
+      }
+
+      @Override
+      public Iterable<? extends Accountable> getChildResources() {
+        return Collections.emptyList();
+      }
+    };
+  }
+
+  private DocValuesProducer readerToDocValuesProducer(final LeafReader reader) {
+    return new DocValuesProducer() {
+
+      @Override
+      public NumericDocValues getNumeric(FieldInfo field) throws IOException {  
+        return reader.getNumericDocValues(field.name);
+      }
+
+      @Override
+      public BinaryDocValues getBinary(FieldInfo field) throws IOException {
+        return reader.getBinaryDocValues(field.name);
+      }
+
+      @Override
+      public SortedDocValues getSorted(FieldInfo field) throws IOException {
+        return reader.getSortedDocValues(field.name);
+      }
+
+      @Override
+      public SortedNumericDocValues getSortedNumeric(FieldInfo field) throws IOException {
+        return reader.getSortedNumericDocValues(field.name);
+      }
+
+      @Override
+      public SortedSetDocValues getSortedSet(FieldInfo field) throws IOException {
+        return reader.getSortedSetDocValues(field.name);
+      }
+
+      @Override
+      public Bits getDocsWithField(FieldInfo field) throws IOException {
+        return reader.getDocsWithField(field.name);
+      }
+
+      @Override
+      public void checkIntegrity() throws IOException {
+        // We already checkIntegrity the entire reader up front in SegmentMerger
+      }
+
+      @Override
+      public void close() {
+      }
+
+      @Override
+      public long ramBytesUsed() {
+        return 0;
+      }
+
+      @Override
+      public Iterable<? extends Accountable> getChildResources() {
+        return Collections.emptyList();
+      }
+    };
+  }
+
+  private StoredFieldsReader readerToStoredFieldsReader(final LeafReader reader) {
+    return new StoredFieldsReader() {
+      @Override
+      public void visitDocument(int docID, StoredFieldVisitor visitor) throws IOException {
+        reader.document(docID, visitor);
+      }
+
+      @Override
+      public StoredFieldsReader clone() {
+        return readerToStoredFieldsReader(reader);
+      }
+
+      @Override
+      public void checkIntegrity() throws IOException {
+        // We already checkIntegrity the entire reader up front in SegmentMerger
+      }
+
+      @Override
+      public void close() {
+      }
+
+      @Override
+      public long ramBytesUsed() {
+        return 0;
+      }
+
+      @Override
+      public Iterable<? extends Accountable> getChildResources() {
+        return Collections.emptyList();
+      }
+    };
+  }
+
+  private TermVectorsReader readerToTermVectorsReader(final LeafReader reader) {
+    return new TermVectorsReader() {
+      @Override
+      public Fields get(int docID) throws IOException {
+        return reader.getTermVectors(docID);
+      }
+
+      @Override
+      public TermVectorsReader clone() {
+        return readerToTermVectorsReader(reader);
+      }
+
+      @Override
+      public void checkIntegrity() throws IOException {
+        // We already checkIntegrity the entire reader up front in SegmentMerger
+      }
+
+      @Override
+      public void close() {
+      }
+
+      @Override
+      public long ramBytesUsed() {
+        return 0;
+      }
+
+      @Override
+      public Iterable<? extends Accountable> getChildResources() {
+        return Collections.emptyList();
+      }
+    };
+  }
+
+  private FieldsProducer readerToFieldsProducer(final LeafReader reader) throws IOException {
+    final Fields fields = reader.fields();
+    return new FieldsProducer() {
+      @Override
+      public Iterator<String> iterator() {
+        return fields.iterator();
+      }
+
+      @Override
+      public Terms terms(String field) throws IOException {
+        return fields.terms(field);
+      }
+
+      @Override
+      public int size() {
+        return fields.size();
+      }
+
+      @Override
+      public void checkIntegrity() throws IOException {
+        // We already checkIntegrity the entire reader up front in SegmentMerger
+      }
+
+      @Override
+      public void close() {
+      }
+
+      @Override
+      public long ramBytesUsed() {
+        return 0;
+      }
+
+      @Override
+      public Iterable<? extends Accountable> getChildResources() {
+        return Collections.emptyList();
+      }
+    };
+  }
+
+  // NOTE: removes any "all deleted" readers from mergeState.readers
+  private void setDocMaps(List<LeafReader> readers) throws IOException {
+    final int numReaders = maxDocs.length;
+
+    // Remap docIDs
+    int docBase = 0;
+    for(int i=0;i<numReaders;i++) {
+      final LeafReader reader = readers.get(i);
+      this.docBase[i] = docBase;
+      final DocMap docMap = DocMap.build(reader);
+      docMaps[i] = docMap;
+      docBase += docMap.numDocs();
+    }
+
+    segmentInfo.setDocCount(docBase);
+  }
+
   /**
+   * Class for recording units of work when merging segments.
+   */
+  public static class CheckAbort {
+    private double workCount;
+    private final MergePolicy.OneMerge merge;
+    private final Directory dir;
+
+    /** Creates a #CheckAbort instance. */
+    public CheckAbort(MergePolicy.OneMerge merge, Directory dir) {
+      this.merge = merge;
+      this.dir = dir;
+    }
+
+    /**
+     * Records the fact that roughly units amount of work
+     * have been done since this method was last called.
+     * When adding time-consuming code into SegmentMerger,
+     * you should test different values for units to ensure
+     * that the time in between calls to merge.checkAborted
+     * is up to ~ 1 second.
+     */
+    public void work(double units) throws MergePolicy.MergeAbortedException {
+      workCount += units;
+      if (workCount >= 10000.0) {
+        merge.checkAborted(dir);
+        workCount = 0;
+      }
+    }
+
+    /** If you use this: IW.close(false) cannot abort your merge!
+     * @lucene.internal */
+    static final MergeState.CheckAbort NONE = new MergeState.CheckAbort(null, null) {
+      @Override
+      public void work(double units) {
+        // do nothing
+      }
+    };
+  }
+
+
+  /**
    * Remaps docids around deletes during merge
    */
   public static abstract class DocMap {
@@ -100,10 +474,8 @@
         public int numDeletedDocs() {
           return numDeletedDocs;
         }
-
       };
     }
-
   }
 
   private static final class NoDelDocMap extends DocMap {
@@ -129,78 +501,4 @@
       return 0;
     }
   }
-
-  /** {@link SegmentInfo} of the newly merged segment. */
-  public final SegmentInfo segmentInfo;
-
-  /** {@link FieldInfos} of the newly merged segment. */
-  public FieldInfos fieldInfos;
-
-  /** Readers being merged. */
-  public final List<LeafReader> readers;
-
-  /** Maps docIDs around deletions. */
-  public DocMap[] docMaps;
-
-  /** New docID base per reader. */
-  public int[] docBase;
-
-  /** Holds the CheckAbort instance, which is invoked
-   *  periodically to see if the merge has been aborted. */
-  public final CheckAbort checkAbort;
-
-  /** InfoStream for debugging messages. */
-  public final InfoStream infoStream;
-
-  /** Counter used for periodic calls to checkAbort
-   * @lucene.internal */
-  public int checkAbortCount;
-
-  /** Sole constructor. */
-  MergeState(List<LeafReader> readers, SegmentInfo segmentInfo, InfoStream infoStream, CheckAbort checkAbort) {
-    this.readers = readers;
-    this.segmentInfo = segmentInfo;
-    this.infoStream = infoStream;
-    this.checkAbort = checkAbort;
-  }
-
-  /**
-   * Class for recording units of work when merging segments.
-   */
-  public static class CheckAbort {
-    private double workCount;
-    private final MergePolicy.OneMerge merge;
-    private final Directory dir;
-
-    /** Creates a #CheckAbort instance. */
-    public CheckAbort(MergePolicy.OneMerge merge, Directory dir) {
-      this.merge = merge;
-      this.dir = dir;
-    }
-
-    /**
-     * Records the fact that roughly units amount of work
-     * have been done since this method was last called.
-     * When adding time-consuming code into SegmentMerger,
-     * you should test different values for units to ensure
-     * that the time in between calls to merge.checkAborted
-     * is up to ~ 1 second.
-     */
-    public void work(double units) throws MergePolicy.MergeAbortedException {
-      workCount += units;
-      if (workCount >= 10000.0) {
-        merge.checkAborted(dir);
-        workCount = 0;
-      }
-    }
-
-    /** If you use this: IW.close(false) cannot abort your merge!
-     * @lucene.internal */
-    static final MergeState.CheckAbort NONE = new MergeState.CheckAbort(null, null) {
-      @Override
-      public void work(double units) {
-        // do nothing
-      }
-    };
-  }
 }
Index: lucene/core/src/java/org/apache/lucene/index/NumericDocValues.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/index/NumericDocValues.java	(revision 1629405)
+++ lucene/core/src/java/org/apache/lucene/index/NumericDocValues.java	(working copy)
@@ -23,7 +23,7 @@
 public abstract class NumericDocValues {
   
   /** Sole constructor. (For invocation by subclass 
-   * constructors, typically implicit.) */
+   *  constructors, typically implicit.) */
   protected NumericDocValues() {}
 
   /**
Index: lucene/core/src/java/org/apache/lucene/index/SegmentCoreReaders.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/index/SegmentCoreReaders.java	(revision 1629405)
+++ lucene/core/src/java/org/apache/lucene/index/SegmentCoreReaders.java	(working copy)
@@ -33,7 +33,6 @@
 import org.apache.lucene.codecs.TermVectorsReader;
 import org.apache.lucene.index.LeafReader.CoreClosedListener;
 import org.apache.lucene.store.AlreadyClosedException;
-import org.apache.lucene.store.CompoundFileDirectory;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.IOContext;
 import org.apache.lucene.util.Accountable;
@@ -60,7 +59,12 @@
 
   final StoredFieldsReader fieldsReaderOrig;
   final TermVectorsReader termVectorsReaderOrig;
-  final CompoundFileDirectory cfsReader;
+  final Directory cfsReader;
+  /** 
+   * fieldinfos for this core: means gen=-1.
+   * this is the exact fieldinfos these codec components saw at write.
+   * in the case of DV updates, SR may hold a newer version. */
+  final FieldInfos coreFieldInfos;
 
   // TODO: make a single thread local w/ a
   // Thingy class holding fieldsReader, termVectorsReader,
@@ -99,15 +103,15 @@
     
     try {
       if (si.info.getUseCompoundFile()) {
-        cfsDir = cfsReader = new CompoundFileDirectory(dir, IndexFileNames.segmentFileName(si.info.name, "", IndexFileNames.COMPOUND_FILE_EXTENSION), context, false);
+        cfsDir = cfsReader = codec.compoundFormat().getCompoundReader(dir, si.info, context);
       } else {
         cfsReader = null;
         cfsDir = dir;
       }
 
-      final FieldInfos fieldInfos = owner.fieldInfos;
+      coreFieldInfos = codec.fieldInfosFormat().getFieldInfosReader().read(cfsDir, si.info, "", context);
       
-      final SegmentReadState segmentReadState = new SegmentReadState(cfsDir, si.info, fieldInfos, context);
+      final SegmentReadState segmentReadState = new SegmentReadState(cfsDir, si.info, coreFieldInfos, context);
       final PostingsFormat format = codec.postingsFormat();
       // Ask codec for its Fields
       fields = format.fieldsProducer(segmentReadState);
@@ -116,7 +120,7 @@
       // TODO: since we don't write any norms file if there are no norms,
       // kinda jaky to assume the codec handles the case of no norms file at all gracefully?!
 
-      if (fieldInfos.hasNorms()) {
+      if (coreFieldInfos.hasNorms()) {
         normsProducer = codec.normsFormat().normsProducer(segmentReadState);
         assert normsProducer != null;
       } else {
@@ -123,10 +127,10 @@
         normsProducer = null;
       }
   
-      fieldsReaderOrig = si.info.getCodec().storedFieldsFormat().fieldsReader(cfsDir, si.info, fieldInfos, context);
+      fieldsReaderOrig = si.info.getCodec().storedFieldsFormat().fieldsReader(cfsDir, si.info, coreFieldInfos, context);
 
-      if (fieldInfos.hasVectors()) { // open term vector files only as needed
-        termVectorsReaderOrig = si.info.getCodec().termVectorsFormat().vectorsReader(cfsDir, si.info, fieldInfos, context);
+      if (coreFieldInfos.hasVectors()) { // open term vector files only as needed
+        termVectorsReaderOrig = si.info.getCodec().termVectorsFormat().vectorsReader(cfsDir, si.info, coreFieldInfos, context);
       } else {
         termVectorsReaderOrig = null;
       }
Index: lucene/core/src/java/org/apache/lucene/index/SegmentInfo.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/index/SegmentInfo.java	(revision 1629405)
+++ lucene/core/src/java/org/apache/lucene/index/SegmentInfo.java	(working copy)
@@ -149,7 +149,7 @@
   // NOTE: leave package private
   void setDocCount(int docCount) {
     if (this.docCount != -1) {
-      throw new IllegalStateException("docCount was already set");
+      throw new IllegalStateException("docCount was already set: this.docCount=" + this.docCount + " vs docCount=" + docCount);
     }
     this.docCount = docCount;
   }
Index: lucene/core/src/java/org/apache/lucene/index/SegmentInfos.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/index/SegmentInfos.java	(revision 1629405)
+++ lucene/core/src/java/org/apache/lucene/index/SegmentInfos.java	(working copy)
@@ -263,7 +263,7 @@
   }
 
   /** Since Lucene 5.0, every commit (segments_N) writes a unique id.  This will
-   *  return that id, or null if this commit was 5.0. */
+   *  return that id, or null if this commit was prior to 5.0. */
   public byte[] getId() {
     return id == null ? null : id.clone();
   }
Index: lucene/core/src/java/org/apache/lucene/index/SegmentMerger.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/index/SegmentMerger.java	(revision 1629405)
+++ lucene/core/src/java/org/apache/lucene/index/SegmentMerger.java	(working copy)
@@ -46,24 +46,26 @@
   
   private final IOContext context;
   
-  private final MergeState mergeState;
+  final MergeState mergeState;
   private final FieldInfos.Builder fieldInfosBuilder;
 
   // note, just like in codec apis Directory 'dir' is NOT the same as segmentInfo.dir!!
   SegmentMerger(List<LeafReader> readers, SegmentInfo segmentInfo, InfoStream infoStream, Directory dir,
-                MergeState.CheckAbort checkAbort, FieldInfos.FieldNumbers fieldNumbers, IOContext context, boolean validate) throws IOException {
+                MergeState.CheckAbort checkAbort, FieldInfos.FieldNumbers fieldNumbers, IOContext context) throws IOException {
     // validate incoming readers
-    if (validate) {
-      for (LeafReader reader : readers) {
+    for (LeafReader reader : readers) {
+      if ((reader instanceof SegmentReader) == false) {
+        // We only validate foreign readers up front: each index component
+        // calls .checkIntegrity itself for each incoming producer
         reader.checkIntegrity();
       }
     }
+
     mergeState = new MergeState(readers, segmentInfo, infoStream, checkAbort);
     directory = dir;
     this.codec = segmentInfo.getCodec();
     this.context = context;
     this.fieldInfosBuilder = new FieldInfos.Builder(fieldNumbers);
-    mergeState.segmentInfo.setDocCount(setDocMaps());
   }
   
   /** True if any merging should happen */
@@ -86,7 +88,7 @@
     // method that will spend alot of time.  The frequency
     // of this check impacts how long
     // IndexWriter.close(false) takes to actually stop the
-    // threads.
+    // background merge threads.
     mergeFieldInfos();
     long t0 = 0;
     if (mergeState.infoStream.isEnabled("SM")) {
@@ -97,10 +99,10 @@
       long t1 = System.nanoTime();
       mergeState.infoStream.message("SM", ((t1-t0)/1000000) + " msec to merge stored fields [" + numMerged + " docs]");
     }
-    assert numMerged == mergeState.segmentInfo.getDocCount();
+    assert numMerged == mergeState.segmentInfo.getDocCount(): "numMerged=" + numMerged + " vs mergeState.segmentInfo.getDocCount()=" + mergeState.segmentInfo.getDocCount();
 
     final SegmentWriteState segmentWriteState = new SegmentWriteState(mergeState.infoStream, directory, mergeState.segmentInfo,
-                                                                      mergeState.fieldInfos, null, context);
+                                                                      mergeState.mergeFieldInfos, null, context);
     if (mergeState.infoStream.isEnabled("SM")) {
       t0 = System.nanoTime();
     }
@@ -113,7 +115,7 @@
     if (mergeState.infoStream.isEnabled("SM")) {
       t0 = System.nanoTime();
     }
-    if (mergeState.fieldInfos.hasDocValues()) {
+    if (mergeState.mergeFieldInfos.hasDocValues()) {
       mergeDocValues(segmentWriteState);
     }
     if (mergeState.infoStream.isEnabled("SM")) {
@@ -121,7 +123,7 @@
       mergeState.infoStream.message("SM", ((t1-t0)/1000000) + " msec to merge doc values [" + numMerged + " docs]");
     }
     
-    if (mergeState.fieldInfos.hasNorms()) {
+    if (mergeState.mergeFieldInfos.hasNorms()) {
       if (mergeState.infoStream.isEnabled("SM")) {
         t0 = System.nanoTime();
       }
@@ -132,7 +134,7 @@
       }
     }
 
-    if (mergeState.fieldInfos.hasVectors()) {
+    if (mergeState.mergeFieldInfos.hasVectors()) {
       if (mergeState.infoStream.isEnabled("SM")) {
         t0 = System.nanoTime();
       }
@@ -146,7 +148,7 @@
     
     // write the merged infos
     FieldInfosWriter fieldInfosWriter = codec.fieldInfosFormat().getFieldInfosWriter();
-    fieldInfosWriter.write(directory, mergeState.segmentInfo, "", mergeState.fieldInfos, context);
+    fieldInfosWriter.write(directory, mergeState.segmentInfo, "", mergeState.mergeFieldInfos, context);
 
     return mergeState;
   }
@@ -182,13 +184,12 @@
   }
   
   public void mergeFieldInfos() throws IOException {
-    for (LeafReader reader : mergeState.readers) {
-      FieldInfos readerFieldInfos = reader.getFieldInfos();
+    for (FieldInfos readerFieldInfos : mergeState.fieldInfos) {
       for (FieldInfo fi : readerFieldInfos) {
         fieldInfosBuilder.add(fi);
       }
     }
-    mergeState.fieldInfos = fieldInfosBuilder.finish();
+    mergeState.mergeFieldInfos = fieldInfosBuilder.finish();
   }
 
   /**
@@ -237,32 +238,6 @@
     return numDocs;
   }
 
-  // NOTE: removes any "all deleted" readers from mergeState.readers
-  private int setDocMaps() throws IOException {
-    final int numReaders = mergeState.readers.size();
-
-    // Remap docIDs
-    mergeState.docMaps = new MergeState.DocMap[numReaders];
-    mergeState.docBase = new int[numReaders];
-
-    int docBase = 0;
-
-    int i = 0;
-    while(i < mergeState.readers.size()) {
-
-      final LeafReader reader = mergeState.readers.get(i);
-
-      mergeState.docBase[i] = docBase;
-      final MergeState.DocMap docMap = MergeState.DocMap.build(reader);
-      mergeState.docMaps[i] = docMap;
-      docBase += docMap.numDocs();
-
-      i++;
-    }
-
-    return docBase;
-  }
-
   private void mergeTerms(SegmentWriteState segmentWriteState) throws IOException {
     FieldsConsumer consumer = codec.postingsFormat().fieldsConsumer(segmentWriteState);
     boolean success = false;
Index: lucene/core/src/java/org/apache/lucene/index/SegmentReader.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/index/SegmentReader.java	(revision 1629405)
+++ lucene/core/src/java/org/apache/lucene/index/SegmentReader.java	(working copy)
@@ -33,7 +33,6 @@
 import org.apache.lucene.codecs.StoredFieldsReader;
 import org.apache.lucene.codecs.TermVectorsReader;
 import org.apache.lucene.index.FieldInfo.DocValuesType;
-import org.apache.lucene.store.CompoundFileDirectory;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.IOContext;
 import org.apache.lucene.util.Accountable;
@@ -92,13 +91,6 @@
   // TODO: why is this public?
   public SegmentReader(SegmentCommitInfo si, IOContext context) throws IOException {
     this.si = si;
-    // TODO if the segment uses CFS, we may open the CFS file twice: once for
-    // reading the FieldInfos (if they are not gen'd) and second time by
-    // SegmentCoreReaders. We can open the CFS here and pass to SCR, but then it
-    // results in less readable code (resource not closed where it was opened).
-    // Best if we could somehow read FieldInfos in SCR but not keep it there, but
-    // constructors don't allow returning two things...
-    fieldInfos = readFieldInfos(si);
     core = new SegmentCoreReaders(this, si.info.dir, si, context);
     segDocValues = new SegmentDocValues();
     
@@ -113,13 +105,10 @@
         liveDocs = null;
       }
       numDocs = si.info.getDocCount() - si.getDelCount();
+      
+      fieldInfos = initFieldInfos();
+      docValuesProducer = initDocValuesProducer();
 
-      if (fieldInfos.hasDocValues()) {
-        docValuesProducer = initDocValuesProducer(codec);
-      } else {
-        docValuesProducer = null;
-      }
-
       success = true;
     } finally {
       // With lock-less commits, it's entirely possible (and
@@ -153,24 +142,11 @@
     this.core = sr.core;
     core.incRef();
     this.segDocValues = sr.segDocValues;
-    
-//    System.out.println("[" + Thread.currentThread().getName() + "] SR.init: sharing reader: " + sr + " for gens=" + sr.genDVProducers.keySet());
-    
-    // increment refCount of DocValuesProducers that are used by this reader
+
     boolean success = false;
     try {
-      final Codec codec = si.info.getCodec();
-      if (si.getFieldInfosGen() == -1) {
-        fieldInfos = sr.fieldInfos;
-      } else {
-        fieldInfos = readFieldInfos(si);
-      }
-      
-      if (fieldInfos.hasDocValues()) {
-        docValuesProducer = initDocValuesProducer(codec);
-      } else {
-        docValuesProducer = null;
-      }
+      fieldInfos = initFieldInfos();
+      docValuesProducer = initDocValuesProducer();
       success = true;
     } finally {
       if (!success) {
@@ -179,50 +155,35 @@
     }
   }
 
-  // initialize the per-field DocValuesProducer
-  private DocValuesProducer initDocValuesProducer(Codec codec) throws IOException {
+  /**
+   * init most recent DocValues for the current commit
+   */
+  private DocValuesProducer initDocValuesProducer() throws IOException {
     final Directory dir = core.cfsReader != null ? core.cfsReader : si.info.dir;
-    final DocValuesFormat dvFormat = codec.docValuesFormat();
+    final DocValuesFormat dvFormat = si.info.getCodec().docValuesFormat();
 
-    if (!si.hasFieldUpdates()) {
+    if (!fieldInfos.hasDocValues()) {
+      return null;
+    } else if (si.hasFieldUpdates()) {
+      return new SegmentDocValuesProducer(si, dir, fieldInfos, segDocValues, dvFormat);
+    } else {
       // simple case, no DocValues updates
       return segDocValues.getDocValuesProducer(-1L, si, IOContext.READ, dir, dvFormat, fieldInfos);
-    } else {
-      return new SegmentDocValuesProducer(si, dir, fieldInfos, segDocValues, dvFormat);
     }
   }
   
   /**
-   * Reads the most recent {@link FieldInfos} of the given segment info.
-   * 
-   * @lucene.internal
+   * init most recent FieldInfos for the current commit
    */
-  static FieldInfos readFieldInfos(SegmentCommitInfo info) throws IOException {
-    final Directory dir;
-    final boolean closeDir;
-    if (info.getFieldInfosGen() == -1 && info.info.getUseCompoundFile()) {
-      // no fieldInfos gen and segment uses a compound file
-      dir = new CompoundFileDirectory(info.info.dir,
-          IndexFileNames.segmentFileName(info.info.name, "", IndexFileNames.COMPOUND_FILE_EXTENSION),
-          IOContext.READONCE,
-          false);
-      closeDir = true;
+  private FieldInfos initFieldInfos() throws IOException {
+    if (!si.hasFieldUpdates()) {
+      return core.coreFieldInfos;
     } else {
-      // gen'd FIS are read outside CFS, or the segment doesn't use a compound file
-      dir = info.info.dir;
-      closeDir = false;
+      // updates always outside of CFS
+      FieldInfosFormat fisFormat = si.info.getCodec().fieldInfosFormat();
+      final String segmentSuffix = Long.toString(si.getFieldInfosGen(), Character.MAX_RADIX);
+      return fisFormat.getFieldInfosReader().read(si.info.dir, si.info, segmentSuffix, IOContext.READONCE);
     }
-    
-    try {
-      final String segmentSuffix = info.getFieldInfosGen() == -1 ? "" : Long.toString(info.getFieldInfosGen(), Character.MAX_RADIX);
-      Codec codec = info.info.getCodec();
-      FieldInfosFormat fisFormat = codec.fieldInfosFormat();
-      return fisFormat.getFieldInfosReader().read(dir, info.info, segmentSuffix, IOContext.READONCE);
-    } finally {
-      if (closeDir) {
-        dir.close();
-      }
-    }
   }
   
   @Override
@@ -566,7 +527,7 @@
   @Override
   public void checkIntegrity() throws IOException {
     ensureOpen();
-    
+
     // stored fields
     getFieldsReader().checkIntegrity();
     
Index: lucene/core/src/java/org/apache/lucene/store/CompoundFileDirectory.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/store/CompoundFileDirectory.java	(revision 1629405)
+++ lucene/core/src/java/org/apache/lucene/store/CompoundFileDirectory.java	(working copy)
@@ -1,273 +0,0 @@
-package org.apache.lucene.store;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.codecs.Codec; // javadocs
-import org.apache.lucene.codecs.CodecUtil;
-import org.apache.lucene.codecs.LiveDocsFormat; // javadocs
-import org.apache.lucene.index.CorruptIndexException;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.store.DataOutput; // javadocs
-import org.apache.lucene.util.IOUtils;
-
-import java.util.Collection;
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.Map;
-import java.io.FileNotFoundException;
-import java.io.IOException;
-
-/**
- * Class for accessing a compound stream.
- * This class implements a directory, but is limited to only read operations.
- * Directory methods that would normally modify data throw an exception.
- * <p>
- * All files belonging to a segment have the same name with varying extensions.
- * The extensions correspond to the different file formats used by the {@link Codec}. 
- * When using the Compound File format these files are collapsed into a 
- * single <tt>.cfs</tt> file (except for the {@link LiveDocsFormat}, with a 
- * corresponding <tt>.cfe</tt> file indexing its sub-files.
- * <p>
- * Files:
- * <ul>
- *    <li><tt>.cfs</tt>: An optional "virtual" file consisting of all the other 
- *    index files for systems that frequently run out of file handles.
- *    <li><tt>.cfe</tt>: The "virtual" compound file's entry table holding all 
- *    entries in the corresponding .cfs file.
- * </ul>
- * <p>Description:</p>
- * <ul>
- *   <li>Compound (.cfs) --&gt; Header, FileData <sup>FileCount</sup>, Footer</li>
- *   <li>Compound Entry Table (.cfe) --&gt; Header, FileCount, &lt;FileName,
- *       DataOffset, DataLength&gt; <sup>FileCount</sup></li>
- *   <li>Header --&gt; {@link CodecUtil#writeHeader CodecHeader}</li>
- *   <li>FileCount --&gt; {@link DataOutput#writeVInt VInt}</li>
- *   <li>DataOffset,DataLength,Checksum --&gt; {@link DataOutput#writeLong UInt64}</li>
- *   <li>FileName --&gt; {@link DataOutput#writeString String}</li>
- *   <li>FileData --&gt; raw file data</li>
- *   <li>Footer --&gt; {@link CodecUtil#writeFooter CodecFooter}</li>
- * </ul>
- * <p>Notes:</p>
- * <ul>
- *   <li>FileCount indicates how many files are contained in this compound file. 
- *       The entry table that follows has that many entries. 
- *   <li>Each directory entry contains a long pointer to the start of this file's data
- *       section, the files length, and a String with that file's name.
- * </ul>
- * 
- * @lucene.experimental
- */
-public final class CompoundFileDirectory extends BaseDirectory {
-  
-  /** Offset/Length for a slice inside of a compound file */
-  public static final class FileEntry {
-    long offset;
-    long length;
-  }
-  
-  private final Directory directory;
-  private final String fileName;
-  protected final int readBufferSize;  
-  private final Map<String,FileEntry> entries;
-  private final boolean openForWrite;
-  private static final Map<String,FileEntry> SENTINEL = Collections.emptyMap();
-  private final CompoundFileWriter writer;
-  private final IndexInput handle;
-  private int version;
-  
-  /**
-   * Create a new CompoundFileDirectory.
-   */
-  public CompoundFileDirectory(Directory directory, String fileName, IOContext context, boolean openForWrite) throws IOException {
-    this.directory = directory;
-    this.fileName = fileName;
-    this.readBufferSize = BufferedIndexInput.bufferSize(context);
-    this.isOpen = false;
-    this.openForWrite = openForWrite;
-    if (!openForWrite) {
-      boolean success = false;
-      handle = directory.openInput(fileName, context);
-      try {
-        this.entries = readEntries(directory, fileName);
-        if (version >= CompoundFileWriter.VERSION_CHECKSUM) {
-          CodecUtil.checkHeader(handle, CompoundFileWriter.DATA_CODEC, version, version);
-          // NOTE: data file is too costly to verify checksum against all the bytes on open,
-          // but for now we at least verify proper structure of the checksum footer: which looks
-          // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption
-          // such as file truncation.
-          CodecUtil.retrieveChecksum(handle);
-        }
-        success = true;
-      } finally {
-        if (!success) {
-          IOUtils.closeWhileHandlingException(handle);
-        }
-      }
-      this.isOpen = true;
-      writer = null;
-    } else {
-      assert !(directory instanceof CompoundFileDirectory) : "compound file inside of compound file: " + fileName;
-      this.entries = SENTINEL;
-      this.isOpen = true;
-      writer = new CompoundFileWriter(directory, fileName);
-      handle = null;
-    }
-  }
-
-  /** Helper method that reads CFS entries from an input stream */
-  private final Map<String, FileEntry> readEntries(Directory dir, String name) throws IOException {
-    ChecksumIndexInput entriesStream = null;
-    Map<String,FileEntry> mapping = null;
-    boolean success = false;
-    try {
-      final String entriesFileName = IndexFileNames.segmentFileName(
-                                            IndexFileNames.stripExtension(name), "",
-                                             IndexFileNames.COMPOUND_FILE_ENTRIES_EXTENSION);
-      entriesStream = dir.openChecksumInput(entriesFileName, IOContext.READONCE);
-      version = CodecUtil.checkHeader(entriesStream, CompoundFileWriter.ENTRY_CODEC, CompoundFileWriter.VERSION_START, CompoundFileWriter.VERSION_CURRENT);
-      final int numEntries = entriesStream.readVInt();
-      mapping = new HashMap<>(numEntries);
-      for (int i = 0; i < numEntries; i++) {
-        final FileEntry fileEntry = new FileEntry();
-        final String id = entriesStream.readString();
-        FileEntry previous = mapping.put(id, fileEntry);
-        if (previous != null) {
-          throw new CorruptIndexException("Duplicate cfs entry id=" + id + " in CFS ", entriesStream);
-        }
-        fileEntry.offset = entriesStream.readLong();
-        fileEntry.length = entriesStream.readLong();
-      }
-      if (version >= CompoundFileWriter.VERSION_CHECKSUM) {
-        CodecUtil.checkFooter(entriesStream);
-      } else {
-        CodecUtil.checkEOF(entriesStream);
-      }
-      success = true;
-    } finally {
-      if (success) {
-        IOUtils.close(entriesStream);
-      } else {
-        IOUtils.closeWhileHandlingException(entriesStream);
-      }
-    }
-    return mapping;
-  }
-  
-  public Directory getDirectory() {
-    return directory;
-  }
-  
-  public String getName() {
-    return fileName;
-  }
-  
-  @Override
-  public synchronized void close() throws IOException {
-    if (!isOpen) {
-      // allow double close - usually to be consistent with other closeables
-      return; // already closed
-     }
-    isOpen = false;
-    if (writer != null) {
-      assert openForWrite;
-      writer.close();
-    } else {
-      IOUtils.close(handle);
-    }
-  }
-  
-  @Override
-  public synchronized IndexInput openInput(String name, IOContext context) throws IOException {
-    ensureOpen();
-    assert !openForWrite;
-    final String id = IndexFileNames.stripSegmentName(name);
-    final FileEntry entry = entries.get(id);
-    if (entry == null) {
-      throw new FileNotFoundException("No sub-file with id " + id + " found (fileName=" + name + " files: " + entries.keySet() + ")");
-    }
-    return handle.slice(name, entry.offset, entry.length);
-  }
-  
-  /** Returns an array of strings, one for each file in the directory. */
-  @Override
-  public String[] listAll() {
-    ensureOpen();
-    String[] res;
-    if (writer != null) {
-      res = writer.listAll(); 
-    } else {
-      res = entries.keySet().toArray(new String[entries.size()]);
-      // Add the segment name
-      String seg = IndexFileNames.parseSegmentName(fileName);
-      for (int i = 0; i < res.length; i++) {
-        res[i] = seg + res[i];
-      }
-    }
-    return res;
-  }
-  
-  /** Not implemented
-   * @throws UnsupportedOperationException always: not supported by CFS */
-  @Override
-  public void deleteFile(String name) {
-    throw new UnsupportedOperationException();
-  }
-  
-  /** Not implemented
-   * @throws UnsupportedOperationException always: not supported by CFS */
-  public void renameFile(String from, String to) {
-    throw new UnsupportedOperationException();
-  }
-  
-  /** Returns the length of a file in the directory.
-   * @throws IOException if the file does not exist */
-  @Override
-  public long fileLength(String name) throws IOException {
-    ensureOpen();
-    if (this.writer != null) {
-      return writer.fileLength(name);
-    }
-    FileEntry e = entries.get(IndexFileNames.stripSegmentName(name));
-    if (e == null)
-      throw new FileNotFoundException(name);
-    return e.length;
-  }
-  
-  @Override
-  public IndexOutput createOutput(String name, IOContext context) throws IOException {
-    ensureOpen();
-    return writer.createOutput(name, context);
-  }
-  
-  @Override
-  public void sync(Collection<String> names) {
-    throw new UnsupportedOperationException();
-  }
-  
-  /** Not implemented
-   * @throws UnsupportedOperationException always: not supported by CFS */
-  @Override
-  public Lock makeLock(String name) {
-    throw new UnsupportedOperationException();
-  }
-
-  @Override
-  public String toString() {
-    return "CompoundFileDirectory(file=\"" + fileName + "\" in dir=" + directory + ")";
-  }
-}
Index: lucene/core/src/java/org/apache/lucene/store/CompoundFileWriter.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/store/CompoundFileWriter.java	(revision 1629405)
+++ lucene/core/src/java/org/apache/lucene/store/CompoundFileWriter.java	(working copy)
@@ -1,354 +0,0 @@
-package org.apache.lucene.store;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.Closeable;
-import java.io.FileNotFoundException;
-import java.io.IOException;
-import java.util.Collection;
-import java.util.HashMap;
-import java.util.HashSet;
-import java.util.LinkedList;
-import java.util.Map;
-import java.util.Queue;
-import java.util.Set;
-import java.util.concurrent.atomic.AtomicBoolean;
-
-import org.apache.lucene.codecs.CodecUtil;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.util.IOUtils;
-
-/**
- * Combines multiple files into a single compound file.
- * 
- * @see CompoundFileDirectory
- * @lucene.internal
- */
-final class CompoundFileWriter implements Closeable{
-
-  private static final class FileEntry {
-    /** source file */
-    String file;
-    long length;
-    /** temporary holder for the start of this file's data section */
-    long offset;
-    /** the directory which contains the file. */
-    Directory dir;
-  }
-
-  // versioning for the .cfs file
-  static final String DATA_CODEC = "CompoundFileWriterData";
-  static final int VERSION_START = 0;
-  static final int VERSION_CHECKSUM = 1;
-  static final int VERSION_CURRENT = VERSION_CHECKSUM;
-
-  // versioning for the .cfe file
-  static final String ENTRY_CODEC = "CompoundFileWriterEntries";
-
-  private final Directory directory;
-  private final Map<String, FileEntry> entries = new HashMap<>();
-  private final Set<String> seenIDs = new HashSet<>();
-  // all entries that are written to a sep. file but not yet moved into CFS
-  private final Queue<FileEntry> pendingEntries = new LinkedList<>();
-  private boolean closed = false;
-  private IndexOutput dataOut;
-  private final AtomicBoolean outputTaken = new AtomicBoolean(false);
-  final String entryTableName;
-  final String dataFileName;
-
-  /**
-   * Create the compound stream in the specified file. The file name is the
-   * entire name (no extensions are added).
-   * 
-   * @throws NullPointerException
-   *           if <code>dir</code> or <code>name</code> is null
-   */
-  CompoundFileWriter(Directory dir, String name) {
-    if (dir == null)
-      throw new NullPointerException("directory cannot be null");
-    if (name == null)
-      throw new NullPointerException("name cannot be null");
-    directory = dir;
-    entryTableName = IndexFileNames.segmentFileName(
-        IndexFileNames.stripExtension(name), "",
-        IndexFileNames.COMPOUND_FILE_ENTRIES_EXTENSION);
-    dataFileName = name;
-    
-  }
-  
-  private synchronized IndexOutput getOutput(IOContext context) throws IOException {
-    if (dataOut == null) {
-      boolean success = false;
-      try {
-        dataOut = directory.createOutput(dataFileName, context);
-        CodecUtil.writeHeader(dataOut, DATA_CODEC, VERSION_CURRENT);
-        success = true;
-      } finally {
-        if (!success) {
-          IOUtils.closeWhileHandlingException(dataOut);
-        }
-      }
-    } 
-    return dataOut;
-  }
-
-  /** Returns the directory of the compound file. */
-  Directory getDirectory() {
-    return directory;
-  }
-
-  /** Returns the name of the compound file. */
-  String getName() {
-    return dataFileName;
-  }
-
-  /**
-   * Closes all resources and writes the entry table
-   * 
-   * @throws IllegalStateException
-   *           if close() had been called before or if no file has been added to
-   *           this object
-   */
-  @Override
-  public void close() throws IOException {
-    if (closed) {
-      return;
-    }
-    IndexOutput entryTableOut = null;
-    // TODO this code should clean up after itself
-    // (remove partial .cfs/.cfe)
-    boolean success = false;
-    try {
-      if (!pendingEntries.isEmpty() || outputTaken.get()) {
-        throw new IllegalStateException("CFS has pending open files");
-      }
-      closed = true;
-      // open the compound stream; we can safely use IOContext.DEFAULT
-      // here because this will only open the output if no file was
-      // added to the CFS
-      getOutput(IOContext.DEFAULT);
-      assert dataOut != null;
-      CodecUtil.writeFooter(dataOut);
-      success = true;
-    } finally {
-      if (success) {
-        IOUtils.close(dataOut);
-      } else {
-        IOUtils.closeWhileHandlingException(dataOut);
-      }
-    }
-    success = false;
-    try {
-      entryTableOut = directory.createOutput(entryTableName, IOContext.DEFAULT);
-      writeEntryTable(entries.values(), entryTableOut);
-      success = true;
-    } finally {
-      if (success) {
-        IOUtils.close(entryTableOut);
-      } else {
-        IOUtils.closeWhileHandlingException(entryTableOut);
-      }
-    }
-  }
-
-  private final void ensureOpen() {
-    if (closed) {
-      throw new AlreadyClosedException("CFS Directory is already closed");
-    }
-  }
-
-  /**
-   * Copy the contents of the file with specified extension into the provided
-   * output stream.
-   */
-  private final long copyFileEntry(IndexOutput dataOut, FileEntry fileEntry)
-      throws IOException {
-    final IndexInput is = fileEntry.dir.openInput(fileEntry.file, IOContext.READONCE);
-    boolean success = false;
-    try {
-      final long startPtr = dataOut.getFilePointer();
-      final long length = fileEntry.length;
-      dataOut.copyBytes(is, length);
-      // Verify that the output length diff is equal to original file
-      long endPtr = dataOut.getFilePointer();
-      long diff = endPtr - startPtr;
-      if (diff != length)
-        throw new IOException("Difference in the output file offsets " + diff
-            + " does not match the original file length " + length);
-      fileEntry.offset = startPtr;
-      success = true;
-      return length;
-    } finally {
-      if (success) {
-        IOUtils.close(is);
-        // copy successful - delete file
-        // if we can't we rely on IFD to pick up and retry
-        IOUtils.deleteFilesIgnoringExceptions(fileEntry.dir, fileEntry.file);
-      } else {
-        IOUtils.closeWhileHandlingException(is);
-      }
-    }
-  }
-
-  protected void writeEntryTable(Collection<FileEntry> entries,
-      IndexOutput entryOut) throws IOException {
-    CodecUtil.writeHeader(entryOut, ENTRY_CODEC, VERSION_CURRENT);
-    entryOut.writeVInt(entries.size());
-    for (FileEntry fe : entries) {
-      entryOut.writeString(IndexFileNames.stripSegmentName(fe.file));
-      entryOut.writeLong(fe.offset);
-      entryOut.writeLong(fe.length);
-    }
-    CodecUtil.writeFooter(entryOut);
-  }
-
-  IndexOutput createOutput(String name, IOContext context) throws IOException {
-    ensureOpen();
-    boolean success = false;
-    boolean outputLocked = false;
-    try {
-      assert name != null : "name must not be null";
-      if (entries.containsKey(name)) {
-        throw new IllegalArgumentException("File " + name + " already exists");
-      }
-      final FileEntry entry = new FileEntry();
-      entry.file = name;
-      entries.put(name, entry);
-      final String id = IndexFileNames.stripSegmentName(name);
-      assert !seenIDs.contains(id): "file=\"" + name + "\" maps to id=\"" + id + "\", which was already written";
-      seenIDs.add(id);
-      final DirectCFSIndexOutput out;
-
-      if ((outputLocked = outputTaken.compareAndSet(false, true))) {
-        out = new DirectCFSIndexOutput(getOutput(context), entry, false);
-      } else {
-        entry.dir = this.directory;
-        out = new DirectCFSIndexOutput(directory.createOutput(name, context), entry,
-            true);
-      }
-      success = true;
-      return out;
-    } finally {
-      if (!success) {
-        entries.remove(name);
-        if (outputLocked) { // release the output lock if not successful
-          assert outputTaken.get();
-          releaseOutputLock();
-        }
-      }
-    }
-  }
-
-  final void releaseOutputLock() {
-    outputTaken.compareAndSet(true, false);
-  }
-
-  private final void prunePendingEntries() throws IOException {
-    // claim the output and copy all pending files in
-    if (outputTaken.compareAndSet(false, true)) {
-      try {
-        while (!pendingEntries.isEmpty()) {
-          FileEntry entry = pendingEntries.poll();
-          copyFileEntry(getOutput(new IOContext(new FlushInfo(0, entry.length))), entry);
-          entries.put(entry.file, entry);
-        }
-      } finally {
-        final boolean compareAndSet = outputTaken.compareAndSet(true, false);
-        assert compareAndSet;
-      }
-    }
-  }
-
-  long fileLength(String name) throws IOException {
-    FileEntry fileEntry = entries.get(name);
-    if (fileEntry == null) {
-      throw new FileNotFoundException(name + " does not exist");
-    }
-    return fileEntry.length;
-  }
-
-  boolean fileExists(String name) {
-    return entries.containsKey(name);
-  }
-
-  String[] listAll() {
-    return entries.keySet().toArray(new String[0]);
-  }
-
-  private final class DirectCFSIndexOutput extends IndexOutput {
-    private final IndexOutput delegate;
-    private final long offset;
-    private boolean closed;
-    private FileEntry entry;
-    private long writtenBytes;
-    private final boolean isSeparate;
-
-    DirectCFSIndexOutput(IndexOutput delegate, FileEntry entry,
-        boolean isSeparate) {
-      super();
-      this.delegate = delegate;
-      this.entry = entry;
-      entry.offset = offset = delegate.getFilePointer();
-      this.isSeparate = isSeparate;
-
-    }
-
-    @Override
-    public void close() throws IOException {
-      if (!closed) {
-        closed = true;
-        entry.length = writtenBytes;
-        if (isSeparate) {
-          delegate.close();
-          // we are a separate file - push into the pending entries
-          pendingEntries.add(entry);
-        } else {
-          // we have been written into the CFS directly - release the lock
-          releaseOutputLock();
-        }
-        // now prune all pending entries and push them into the CFS
-        prunePendingEntries();
-      }
-    }
-
-    @Override
-    public long getFilePointer() {
-      return delegate.getFilePointer() - offset;
-    }
-
-    @Override
-    public void writeByte(byte b) throws IOException {
-      assert !closed;
-      writtenBytes++;
-      delegate.writeByte(b);
-    }
-
-    @Override
-    public void writeBytes(byte[] b, int offset, int length) throws IOException {
-      assert !closed;
-      writtenBytes += length;
-      delegate.writeBytes(b, offset, length);
-    }
-
-    @Override
-    public long getChecksum() throws IOException {
-      return delegate.getChecksum();
-    }
-  }
-
-}
Index: lucene/core/src/java/org/apache/lucene/store/Directory.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/store/Directory.java	(revision 1629405)
+++ lucene/core/src/java/org/apache/lucene/store/Directory.java	(working copy)
@@ -100,12 +100,7 @@
    */
   public abstract void renameFile(String source, String dest) throws IOException;
   
-  /** Returns a stream reading an existing file, with the
-   * specified read buffer size.  The particular Directory
-   * implementation may ignore the buffer size.  Currently
-   * the only Directory implementations that respect this
-   * parameter are {@link FSDirectory} and {@link
-   * CompoundFileDirectory}.
+  /** Returns a stream reading an existing file.
    * <p>Throws {@link FileNotFoundException} or {@link NoSuchFileException}
    * if the file does not exist.
    */
Index: lucene/core/src/test/org/apache/lucene/codecs/lucene41/TestBlockPostingsFormat.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/codecs/lucene41/TestBlockPostingsFormat.java	(revision 1629405)
+++ lucene/core/src/test/org/apache/lucene/codecs/lucene41/TestBlockPostingsFormat.java	(working copy)
@@ -17,8 +17,17 @@
  * limitations under the License.
  */
 
+import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.codecs.Codec;
+import org.apache.lucene.codecs.blocktree.FieldReader;
+import org.apache.lucene.codecs.blocktree.Stats;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
 import org.apache.lucene.index.BasePostingsFormatTestCase;
+import org.apache.lucene.index.DirectoryReader;
+import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.index.IndexWriterConfig;
+import org.apache.lucene.store.Directory;
 import org.apache.lucene.util.TestUtil;
 
 /**
@@ -31,4 +40,28 @@
   protected Codec getCodec() {
     return codec;
   }
+  
+  /** Make sure the final sub-block(s) are not skipped. */
+  public void testFinalBlock() throws Exception {
+    Directory d = newDirectory();
+    IndexWriter w = new IndexWriter(d, new IndexWriterConfig(new MockAnalyzer(random())));
+    for(int i=0;i<25;i++) {
+      Document doc = new Document();
+      doc.add(newStringField("field", Character.toString((char) (97+i)), Field.Store.NO));
+      doc.add(newStringField("field", "z" + Character.toString((char) (97+i)), Field.Store.NO));
+      w.addDocument(doc);
+    }
+    w.forceMerge(1);
+
+    DirectoryReader r = DirectoryReader.open(w, true);
+    assertEquals(1, r.leaves().size());
+    FieldReader field = (FieldReader) r.leaves().get(0).reader().fields().terms("field");
+    // We should see exactly two blocks: one root block (prefix empty string) and one block for z* terms (prefix z):
+    Stats stats = field.computeStats();
+    assertEquals(0, stats.floorBlockCount);
+    assertEquals(2, stats.nonFloorBlockCount);
+    r.close();
+    w.close();
+    d.close();
+  }
 }
Index: lucene/core/src/test/org/apache/lucene/codecs/lucene41/TestLucene41PostingsFormat.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/codecs/lucene41/TestLucene41PostingsFormat.java	(revision 1629405)
+++ lucene/core/src/test/org/apache/lucene/codecs/lucene41/TestLucene41PostingsFormat.java	(working copy)
@@ -1,63 +0,0 @@
-package org.apache.lucene.codecs.lucene41;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.analysis.MockAnalyzer;
-import org.apache.lucene.codecs.Codec;
-import org.apache.lucene.codecs.blocktree.FieldReader;
-import org.apache.lucene.codecs.blocktree.Stats;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.document.Field;
-import org.apache.lucene.index.BasePostingsFormatTestCase;
-import org.apache.lucene.index.DirectoryReader;
-import org.apache.lucene.index.IndexWriter;
-import org.apache.lucene.index.IndexWriterConfig;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.util.TestUtil;
-
-public class TestLucene41PostingsFormat extends BasePostingsFormatTestCase {
-
-  @Override
-  protected Codec getCodec() {
-    return TestUtil.alwaysPostingsFormat(new Lucene41PostingsFormat());
-  }
-
-  /** Make sure the final sub-block(s) are not skipped. */
-  public void testFinalBlock() throws Exception {
-    Directory d = newDirectory();
-    IndexWriter w = new IndexWriter(d, new IndexWriterConfig(new MockAnalyzer(random())));
-    for(int i=0;i<25;i++) {
-      Document doc = new Document();
-      doc.add(newStringField("field", Character.toString((char) (97+i)), Field.Store.NO));
-      doc.add(newStringField("field", "z" + Character.toString((char) (97+i)), Field.Store.NO));
-      w.addDocument(doc);
-    }
-    w.forceMerge(1);
-
-    DirectoryReader r = DirectoryReader.open(w, true);
-    assertEquals(1, r.leaves().size());
-    FieldReader field = (FieldReader) r.leaves().get(0).reader().fields().terms("field");
-    // We should see exactly two blocks: one root block (prefix empty string) and one block for z* terms (prefix z):
-    Stats stats = field.computeStats();
-    assertEquals(0, stats.floorBlockCount);
-    assertEquals(2, stats.nonFloorBlockCount);
-    r.close();
-    w.close();
-    d.close();
-  }
-}
Index: lucene/core/src/test/org/apache/lucene/codecs/lucene41/TestLucene41StoredFieldsFormat.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/codecs/lucene41/TestLucene41StoredFieldsFormat.java	(revision 1629405)
+++ lucene/core/src/test/org/apache/lucene/codecs/lucene41/TestLucene41StoredFieldsFormat.java	(working copy)
@@ -1,29 +0,0 @@
-package org.apache.lucene.codecs.lucene41;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.codecs.Codec;
-import org.apache.lucene.index.BaseStoredFieldsFormatTestCase;
-import org.apache.lucene.util.TestUtil;
-
-public class TestLucene41StoredFieldsFormat extends BaseStoredFieldsFormatTestCase {
-  @Override
-  protected Codec getCodec() {
-    return TestUtil.getDefaultCodec();
-  }
-}
Index: lucene/core/src/test/org/apache/lucene/codecs/lucene49/TestLucene49NormsFormat.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/codecs/lucene49/TestLucene49NormsFormat.java	(revision 1629405)
+++ lucene/core/src/test/org/apache/lucene/codecs/lucene49/TestLucene49NormsFormat.java	(working copy)
@@ -1,34 +0,0 @@
-package org.apache.lucene.codecs.lucene49;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.codecs.Codec;
-import org.apache.lucene.index.BaseNormsFormatTestCase;
-import org.apache.lucene.util.TestUtil;
-
-/**
- * Tests Lucene49NormsFormat
- */
-public class TestLucene49NormsFormat extends BaseNormsFormatTestCase {
-  private final Codec codec = TestUtil.getDefaultCodec();
-  
-  @Override
-  protected Codec getCodec() {
-    return codec;
-  } 
-}
Index: lucene/core/src/test/org/apache/lucene/codecs/lucene50/TestLucene50CompoundFormat.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/codecs/lucene50/TestLucene50CompoundFormat.java	(revision 0)
+++ lucene/core/src/test/org/apache/lucene/codecs/lucene50/TestLucene50CompoundFormat.java	(working copy)
@@ -0,0 +1,31 @@
+package org.apache.lucene.codecs.lucene50;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.codecs.Codec;
+import org.apache.lucene.index.BaseCompoundFormatTestCase;
+import org.apache.lucene.util.TestUtil;
+
+public class TestLucene50CompoundFormat extends BaseCompoundFormatTestCase {
+  private final Codec codec = TestUtil.getDefaultCodec();
+  
+  @Override
+  protected Codec getCodec() {
+    return codec;
+  }
+}

Property changes on: lucene/core/src/test/org/apache/lucene/codecs/lucene50/TestLucene50CompoundFormat.java
___________________________________________________________________
Added: svn:eol-style
## -0,0 +1 ##
+native
\ No newline at end of property
Index: lucene/core/src/test/org/apache/lucene/codecs/lucene50/TestLucene50NormsFormat.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/codecs/lucene50/TestLucene50NormsFormat.java	(revision 0)
+++ lucene/core/src/test/org/apache/lucene/codecs/lucene50/TestLucene50NormsFormat.java	(working copy)
@@ -0,0 +1,34 @@
+package org.apache.lucene.codecs.lucene50;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.codecs.Codec;
+import org.apache.lucene.index.BaseNormsFormatTestCase;
+import org.apache.lucene.util.TestUtil;
+
+/**
+ * Tests Lucene49NormsFormat
+ */
+public class TestLucene50NormsFormat extends BaseNormsFormatTestCase {
+  private final Codec codec = TestUtil.getDefaultCodec();
+  
+  @Override
+  protected Codec getCodec() {
+    return codec;
+  } 
+}

Property changes on: lucene/core/src/test/org/apache/lucene/codecs/lucene50/TestLucene50NormsFormat.java
___________________________________________________________________
Added: svn:eol-style
## -0,0 +1 ##
+native
\ No newline at end of property
Index: lucene/core/src/test/org/apache/lucene/codecs/lucene50/TestLucene50StoredFieldsFormat.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/codecs/lucene50/TestLucene50StoredFieldsFormat.java	(revision 0)
+++ lucene/core/src/test/org/apache/lucene/codecs/lucene50/TestLucene50StoredFieldsFormat.java	(working copy)
@@ -0,0 +1,29 @@
+package org.apache.lucene.codecs.lucene50;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.codecs.Codec;
+import org.apache.lucene.index.BaseStoredFieldsFormatTestCase;
+import org.apache.lucene.util.TestUtil;
+
+public class TestLucene50StoredFieldsFormat extends BaseStoredFieldsFormatTestCase {
+  @Override
+  protected Codec getCodec() {
+    return TestUtil.getDefaultCodec();
+  }
+}

Property changes on: lucene/core/src/test/org/apache/lucene/codecs/lucene50/TestLucene50StoredFieldsFormat.java
___________________________________________________________________
Added: svn:eol-style
## -0,0 +1 ##
+native
\ No newline at end of property
Index: lucene/core/src/test/org/apache/lucene/codecs/lucene50/TestLucene50TermVectorsFormat.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/codecs/lucene50/TestLucene50TermVectorsFormat.java	(revision 0)
+++ lucene/core/src/test/org/apache/lucene/codecs/lucene50/TestLucene50TermVectorsFormat.java	(working copy)
@@ -0,0 +1,29 @@
+package org.apache.lucene.codecs.lucene50;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.codecs.Codec;
+import org.apache.lucene.index.BaseTermVectorsFormatTestCase;
+import org.apache.lucene.util.TestUtil;
+
+public class TestLucene50TermVectorsFormat extends BaseTermVectorsFormatTestCase {
+  @Override
+  protected Codec getCodec() {
+    return TestUtil.getDefaultCodec();
+  }
+}

Property changes on: lucene/core/src/test/org/apache/lucene/codecs/lucene50/TestLucene50TermVectorsFormat.java
___________________________________________________________________
Added: svn:eol-style
## -0,0 +1 ##
+native
\ No newline at end of property
Index: lucene/core/src/test/org/apache/lucene/index/TestAddIndexes.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/index/TestAddIndexes.java	(revision 1629405)
+++ lucene/core/src/test/org/apache/lucene/index/TestAddIndexes.java	(working copy)
@@ -1116,7 +1116,10 @@
     w3.close();
     // we should now see segments_X,
     // _Y.cfs,_Y.cfe, _Z.si
-    assertEquals("Only one compound segment should exist, but got: " + Arrays.toString(dir.listAll()), 4, dir.listAll().length);
+    SegmentInfos sis = new SegmentInfos();
+    sis.read(dir);
+    assertEquals("Only one compound segment should exist", 1, sis.size());
+    assertTrue(sis.info(0).info.getUseCompoundFile());
     dir.close();
   }
   
Index: lucene/core/src/test/org/apache/lucene/index/TestAllFilesHaveChecksumFooter.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/index/TestAllFilesHaveChecksumFooter.java	(revision 1629405)
+++ lucene/core/src/test/org/apache/lucene/index/TestAllFilesHaveChecksumFooter.java	(working copy)
@@ -24,11 +24,9 @@
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.NumericDocValuesField;
-import org.apache.lucene.store.CompoundFileDirectory;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.IndexInput;
 import org.apache.lucene.store.MockDirectoryWrapper;
-import org.apache.lucene.util.IOUtils;
 import org.apache.lucene.util.LuceneTestCase;
 import org.apache.lucene.util.TestUtil;
 
@@ -65,33 +63,32 @@
       }
     }
     riw.close();
-    checkHeaders(dir);
+    checkFooters(dir);
     dir.close();
   }
   
-  private void checkHeaders(Directory dir) throws IOException {
-    for (String file : dir.listAll()) {
-      if (file.equals(IndexWriter.WRITE_LOCK_NAME)) {
-        continue; // write.lock has no footer, thats ok
+  private void checkFooters(Directory dir) throws IOException {
+    SegmentInfos sis = new SegmentInfos();
+    sis.read(dir);
+    checkFooter(dir, sis.getSegmentsFileName());
+    
+    for (SegmentCommitInfo si : sis) {
+      for (String file : si.files()) {
+        checkFooter(dir, file);
       }
-      if (file.endsWith(IndexFileNames.COMPOUND_FILE_EXTENSION)) {
-        CompoundFileDirectory cfsDir = new CompoundFileDirectory(dir, file, newIOContext(random()), false);
-        checkHeaders(cfsDir); // recurse into cfs
-        cfsDir.close();
-      }
-      IndexInput in = null;
-      boolean success = false;
-      try {
-        in = dir.openInput(file, newIOContext(random()));
-        CodecUtil.checksumEntireFile(in);
-        success = true;
-      } finally {
-        if (success) {
-          IOUtils.close(in);
-        } else {
-          IOUtils.closeWhileHandlingException(in);
+      if (si.info.getUseCompoundFile()) {
+        try (Directory cfsDir = si.info.getCodec().compoundFormat().getCompoundReader(dir, si.info, newIOContext(random()))) {
+          for (String cfsFile : cfsDir.listAll()) {
+            checkFooter(cfsDir, cfsFile);
+          }
         }
       }
     }
   }
+  
+  private void checkFooter(Directory dir, String file) throws IOException {
+    try (IndexInput in = dir.openInput(file, newIOContext(random()))) {
+      CodecUtil.checksumEntireFile(in);
+    }
+  }
 }
Index: lucene/core/src/test/org/apache/lucene/index/TestAllFilesHaveCodecHeader.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/index/TestAllFilesHaveCodecHeader.java	(revision 1629405)
+++ lucene/core/src/test/org/apache/lucene/index/TestAllFilesHaveCodecHeader.java	(working copy)
@@ -28,11 +28,9 @@
 import org.apache.lucene.document.FieldType;
 import org.apache.lucene.document.NumericDocValuesField;
 import org.apache.lucene.document.TextField;
-import org.apache.lucene.store.CompoundFileDirectory;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.IndexInput;
 import org.apache.lucene.store.MockDirectoryWrapper;
-import org.apache.lucene.util.IOUtils;
 import org.apache.lucene.util.LuceneTestCase;
 import org.apache.lucene.util.TestUtil;
 
@@ -83,41 +81,39 @@
   }
   
   private void checkHeaders(Directory dir, Map<String,String> namesToExtensions) throws IOException {
-    for (String file : dir.listAll()) {
-      if (file.equals(IndexWriter.WRITE_LOCK_NAME)) {
-        continue; // write.lock has no header, thats ok
+    SegmentInfos sis = new SegmentInfos();
+    sis.read(dir);
+    checkHeader(dir, sis.getSegmentsFileName(), namesToExtensions);
+    
+    for (SegmentCommitInfo si : sis) {
+      for (String file : si.files()) {
+        checkHeader(dir, file, namesToExtensions);
       }
-      if (file.endsWith(IndexFileNames.COMPOUND_FILE_EXTENSION)) {
-        CompoundFileDirectory cfsDir = new CompoundFileDirectory(dir, file, newIOContext(random()), false);
-        checkHeaders(cfsDir, namesToExtensions); // recurse into cfs
-        cfsDir.close();
-      }
-      IndexInput in = null;
-      boolean success = false;
-      try {
-        in = dir.openInput(file, newIOContext(random()));
-        int val = in.readInt();
-        assertEquals(file + " has no codec header, instead found: " + val, CodecUtil.CODEC_MAGIC, val);
-        String codecName = in.readString();
-        assertFalse(codecName.isEmpty());
-        String extension = IndexFileNames.getExtension(file);
-        if (extension == null) {
-          assertTrue(file.startsWith(IndexFileNames.SEGMENTS));
-          extension = "<segments> (not a real extension, designates segments file)";
+      if (si.info.getUseCompoundFile()) {
+        try (Directory cfsDir = si.info.getCodec().compoundFormat().getCompoundReader(dir, si.info, newIOContext(random()))) {
+          for (String cfsFile : cfsDir.listAll()) {
+            checkHeader(cfsDir, cfsFile, namesToExtensions);
+          }
         }
-        String previous = namesToExtensions.put(codecName, extension);
-        if (previous != null && !previous.equals(extension)) {
-          //TODO: not yet 
-          // fail("extensions " + previous + " and " + extension + " share same codecName " + codecName);
-        }
-        success = true;
-      } finally {
-        if (success) {
-          IOUtils.close(in);
-        } else {
-          IOUtils.closeWhileHandlingException(in);
-        }
       }
     }
   }
+  
+  private void checkHeader(Directory dir, String file, Map<String,String> namesToExtensions) throws IOException {
+    try (IndexInput in = dir.openInput(file, newIOContext(random()))) {
+      int val = in.readInt();
+      assertEquals(file + " has no codec header, instead found: " + val, CodecUtil.CODEC_MAGIC, val);
+      String codecName = in.readString();
+      assertFalse(codecName.isEmpty());
+      String extension = IndexFileNames.getExtension(file);
+      if (extension == null) {
+        assertTrue(file.startsWith(IndexFileNames.SEGMENTS));
+        extension = "<segments> (not a real extension, designates segments file)";
+      }
+      String previous = namesToExtensions.put(codecName, extension);
+      if (previous != null && !previous.equals(extension)) {
+        fail("extensions " + previous + " and " + extension + " share same codecName " + codecName);
+      }
+    }
+  }
 }
Index: lucene/core/src/test/org/apache/lucene/index/TestCodecUtil.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/index/TestCodecUtil.java	(revision 1629405)
+++ lucene/core/src/test/org/apache/lucene/index/TestCodecUtil.java	(working copy)
@@ -26,6 +26,7 @@
 import org.apache.lucene.store.RAMInputStream;
 import org.apache.lucene.store.RAMOutputStream;
 import org.apache.lucene.util.LuceneTestCase;
+import org.apache.lucene.util.StringHelper;
 
 /** tests for codecutil methods */
 public class TestCodecUtil extends LuceneTestCase {
@@ -194,4 +195,61 @@
     }
     input.close();
   }
+  
+  public void testSegmentHeaderLength() throws Exception {
+    RAMFile file = new RAMFile();
+    IndexOutput output = new RAMOutputStream(file, true);
+    CodecUtil.writeSegmentHeader(output, "FooBar", 5, StringHelper.randomId(), "xyz");
+    output.writeString("this is the data");
+    output.close();
+    
+    IndexInput input = new RAMInputStream("file", file);
+    input.seek(CodecUtil.segmentHeaderLength("FooBar", "xyz"));
+    assertEquals("this is the data", input.readString());
+    input.close();
+  }
+  
+  public void testWriteTooLongSuffix() throws Exception {
+    StringBuilder tooLong = new StringBuilder();
+    for (int i = 0; i < 256; i++) {
+      tooLong.append('a');
+    }
+    RAMFile file = new RAMFile();
+    IndexOutput output = new RAMOutputStream(file, true);
+    try {
+      CodecUtil.writeSegmentHeader(output, "foobar", 5, StringHelper.randomId(), tooLong.toString());
+      fail("didn't get expected exception");
+    } catch (IllegalArgumentException expected) {
+      // expected
+    }
+  }
+  
+  public void testWriteVeryLongSuffix() throws Exception {
+    StringBuilder justLongEnough = new StringBuilder();
+    for (int i = 0; i < 255; i++) {
+      justLongEnough.append('a');
+    }
+    RAMFile file = new RAMFile();
+    IndexOutput output = new RAMOutputStream(file, true);
+    byte[] id = StringHelper.randomId();
+    CodecUtil.writeSegmentHeader(output, "foobar", 5, id, justLongEnough.toString());
+    output.close();
+    
+    IndexInput input = new RAMInputStream("file", file);
+    CodecUtil.checkSegmentHeader(input, "foobar", 5, 5, id, justLongEnough.toString());
+    assertEquals(input.getFilePointer(), input.length());
+    assertEquals(input.getFilePointer(), CodecUtil.segmentHeaderLength("foobar", justLongEnough.toString()));
+    input.close();
+  }
+  
+  public void testWriteNonAsciiSuffix() throws Exception {
+    RAMFile file = new RAMFile();
+    IndexOutput output = new RAMOutputStream(file, true);
+    try {
+      CodecUtil.writeSegmentHeader(output, "foobar", 5, StringHelper.randomId(), "\u1234");
+      fail("didn't get expected exception");
+    } catch (IllegalArgumentException expected) {
+      // expected
+    }
+  }
 }
Index: lucene/core/src/test/org/apache/lucene/index/TestCodecs.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/index/TestCodecs.java	(revision 1629405)
+++ lucene/core/src/test/org/apache/lucene/index/TestCodecs.java	(working copy)
@@ -247,10 +247,10 @@
     final FieldData[] fields = new FieldData[] {field};
     final FieldInfos fieldInfos = builder.finish();
     final Directory dir = newDirectory();
-    this.write(fieldInfos, dir, fields);
     Codec codec = Codec.getDefault();
     final SegmentInfo si = new SegmentInfo(dir, Version.LATEST, SEGMENT, 10000, false, codec, null, StringHelper.randomId());
-
+    
+    this.write(si, fieldInfos, dir, fields);
     final FieldsProducer reader = codec.postingsFormat().fieldsProducer(new SegmentReadState(dir, si, fieldInfos, newIOContext(random())));
 
     final Iterator<String> fieldsEnum = reader.iterator();
@@ -304,9 +304,9 @@
       System.out.println("TEST: now write postings");
     }
 
-    this.write(fieldInfos, dir, fields);
     Codec codec = Codec.getDefault();
     final SegmentInfo si = new SegmentInfo(dir, Version.LATEST, SEGMENT, 10000, false, codec, null, StringHelper.randomId());
+    this.write(si, fieldInfos, dir, fields);
 
     if (VERBOSE) {
       System.out.println("TEST: now read postings");
@@ -798,10 +798,9 @@
     }
   }
 
-  private void write(final FieldInfos fieldInfos, final Directory dir, final FieldData[] fields) throws Throwable {
+  private void write(SegmentInfo si, final FieldInfos fieldInfos, final Directory dir, final FieldData[] fields) throws Throwable {
 
-    final Codec codec = Codec.getDefault();
-    final SegmentInfo si = new SegmentInfo(dir, Version.LATEST, SEGMENT, 10000, false, codec, null, StringHelper.randomId());
+    final Codec codec = si.getCodec();
     final SegmentWriteState state = new SegmentWriteState(InfoStream.getDefault(), dir, si, fieldInfos, null, newIOContext(random()));
 
     Arrays.sort(fields);
Index: lucene/core/src/test/org/apache/lucene/index/TestCompoundFile.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/index/TestCompoundFile.java	(revision 1629405)
+++ lucene/core/src/test/org/apache/lucene/index/TestCompoundFile.java	(working copy)
@@ -1,853 +0,0 @@
-package org.apache.lucene.index;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.document.Document;
-import org.apache.lucene.document.Field;
-import org.apache.lucene.store.CompoundFileDirectory;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.store.MockDirectoryWrapper;
-import org.apache.lucene.store.SimpleFSDirectory;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.LuceneTestCase;
-import org.apache.lucene.util.TestUtil;
-
-import java.io.IOException;
-import java.nio.file.Path;
-
-public class TestCompoundFile extends LuceneTestCase
-{
-    private Directory dir;
-
-    @Override
-    public void setUp() throws Exception {
-       super.setUp();
-       Path file = createTempDir("testIndex");
-       // use a simple FSDir here, to be sure to have SimpleFSInputs
-       dir = new SimpleFSDirectory(file,null);
-    }
-
-    @Override
-    public void tearDown() throws Exception {
-       dir.close();
-       super.tearDown();
-    }
-
-    /** Creates a file of the specified size with random data. */
-    private void createRandomFile(Directory dir, String name, int size)
-    throws IOException
-    {
-        IndexOutput os = dir.createOutput(name, newIOContext(random()));
-        for (int i=0; i<size; i++) {
-            byte b = (byte) (Math.random() * 256);
-            os.writeByte(b);
-        }
-        os.close();
-    }
-
-    /** Creates a file of the specified size with sequential data. The first
-     *  byte is written as the start byte provided. All subsequent bytes are
-     *  computed as start + offset where offset is the number of the byte.
-     */
-    private void createSequenceFile(Directory dir,
-                                    String name,
-                                    byte start,
-                                    int size)
-    throws IOException
-    {
-        IndexOutput os = dir.createOutput(name, newIOContext(random()));
-        for (int i=0; i < size; i++) {
-            os.writeByte(start);
-            start ++;
-        }
-        os.close();
-    }
-
-
-    private void assertSameStreams(String msg,
-                                   IndexInput expected,
-                                   IndexInput test)
-    throws IOException
-    {
-        assertNotNull(msg + " null expected", expected);
-        assertNotNull(msg + " null test", test);
-        assertEquals(msg + " length", expected.length(), test.length());
-        assertEquals(msg + " position", expected.getFilePointer(),
-                                        test.getFilePointer());
-
-        byte expectedBuffer[] = new byte[512];
-        byte testBuffer[] = new byte[expectedBuffer.length];
-
-        long remainder = expected.length() - expected.getFilePointer();
-        while(remainder > 0) {
-            int readLen = (int) Math.min(remainder, expectedBuffer.length);
-            expected.readBytes(expectedBuffer, 0, readLen);
-            test.readBytes(testBuffer, 0, readLen);
-            assertEqualArrays(msg + ", remainder " + remainder, expectedBuffer,
-                testBuffer, 0, readLen);
-            remainder -= readLen;
-        }
-    }
-
-
-    private void assertSameStreams(String msg,
-                                   IndexInput expected,
-                                   IndexInput actual,
-                                   long seekTo)
-    throws IOException
-    {
-        if(seekTo >= 0 && seekTo < expected.length())
-        {
-            expected.seek(seekTo);
-            actual.seek(seekTo);
-            assertSameStreams(msg + ", seek(mid)", expected, actual);
-        }
-    }
-
-
-
-    private void assertSameSeekBehavior(String msg,
-                                        IndexInput expected,
-                                        IndexInput actual)
-    throws IOException
-    {
-        // seek to 0
-        long point = 0;
-        assertSameStreams(msg + ", seek(0)", expected, actual, point);
-
-        // seek to middle
-        point = expected.length() / 2l;
-        assertSameStreams(msg + ", seek(mid)", expected, actual, point);
-
-        // seek to end - 2
-        point = expected.length() - 2;
-        assertSameStreams(msg + ", seek(end-2)", expected, actual, point);
-
-        // seek to end - 1
-        point = expected.length() - 1;
-        assertSameStreams(msg + ", seek(end-1)", expected, actual, point);
-
-        // seek to the end
-        point = expected.length();
-        assertSameStreams(msg + ", seek(end)", expected, actual, point);
-
-        // seek past end
-        point = expected.length() + 1;
-        assertSameStreams(msg + ", seek(end+1)", expected, actual, point);
-    }
-
-
-    private void assertEqualArrays(String msg,
-                                   byte[] expected,
-                                   byte[] test,
-                                   int start,
-                                   int len)
-    {
-        assertNotNull(msg + " null expected", expected);
-        assertNotNull(msg + " null test", test);
-
-        for (int i=start; i<len; i++) {
-            assertEquals(msg + " " + i, expected[i], test[i]);
-        }
-    }
-
-
-    // ===========================================================
-    //  Tests of the basic CompoundFile functionality
-    // ===========================================================
-
-
-    /** This test creates compound file based on a single file.
-     *  Files of different sizes are tested: 0, 1, 10, 100 bytes.
-     */
-    public void testSingleFile() throws IOException {
-        int data[] = new int[] { 0, 1, 10, 100 };
-        for (int i=0; i<data.length; i++) {
-            String name = "t" + data[i];
-            createSequenceFile(dir, name, (byte) 0, data[i]);
-            CompoundFileDirectory csw = new CompoundFileDirectory(dir, name + ".cfs", newIOContext(random()), true);
-            dir.copy(csw, name, name, newIOContext(random()));
-            csw.close();
-
-            CompoundFileDirectory csr = new CompoundFileDirectory(dir, name + ".cfs", newIOContext(random()), false);
-            IndexInput expected = dir.openInput(name, newIOContext(random()));
-            IndexInput actual = csr.openInput(name, newIOContext(random()));
-            assertSameStreams(name, expected, actual);
-            assertSameSeekBehavior(name, expected, actual);
-            expected.close();
-            actual.close();
-            csr.close();
-        }
-    }
-
-
-    /** This test creates compound file based on two files.
-     *
-     */
-    public void testTwoFiles() throws IOException {
-        createSequenceFile(dir, "d1", (byte) 0, 15);
-        createSequenceFile(dir, "d2", (byte) 0, 114);
-
-        CompoundFileDirectory csw = new CompoundFileDirectory(dir, "d.cfs", newIOContext(random()), true);
-        dir.copy(csw, "d1", "d1", newIOContext(random()));
-        dir.copy(csw, "d2", "d2", newIOContext(random()));
-        csw.close();
-
-        CompoundFileDirectory csr = new CompoundFileDirectory(dir, "d.cfs", newIOContext(random()), false);
-        IndexInput expected = dir.openInput("d1", newIOContext(random()));
-        IndexInput actual = csr.openInput("d1", newIOContext(random()));
-        assertSameStreams("d1", expected, actual);
-        assertSameSeekBehavior("d1", expected, actual);
-        expected.close();
-        actual.close();
-
-        expected = dir.openInput("d2", newIOContext(random()));
-        actual = csr.openInput("d2", newIOContext(random()));
-        assertSameStreams("d2", expected, actual);
-        assertSameSeekBehavior("d2", expected, actual);
-        expected.close();
-        actual.close();
-        csr.close();
-    }
-
-    /** This test creates a compound file based on a large number of files of
-     *  various length. The file content is generated randomly. The sizes range
-     *  from 0 to 1Mb. Some of the sizes are selected to test the buffering
-     *  logic in the file reading code. For this the chunk variable is set to
-     *  the length of the buffer used internally by the compound file logic.
-     */
-    public void testRandomFiles() throws IOException {
-        // Setup the test segment
-        String segment = "test";
-        int chunk = 1024; // internal buffer size used by the stream
-        createRandomFile(dir, segment + ".zero", 0);
-        createRandomFile(dir, segment + ".one", 1);
-        createRandomFile(dir, segment + ".ten", 10);
-        createRandomFile(dir, segment + ".hundred", 100);
-        createRandomFile(dir, segment + ".big1", chunk);
-        createRandomFile(dir, segment + ".big2", chunk - 1);
-        createRandomFile(dir, segment + ".big3", chunk + 1);
-        createRandomFile(dir, segment + ".big4", 3 * chunk);
-        createRandomFile(dir, segment + ".big5", 3 * chunk - 1);
-        createRandomFile(dir, segment + ".big6", 3 * chunk + 1);
-        createRandomFile(dir, segment + ".big7", 1000 * chunk);
-
-        // Setup extraneous files
-        createRandomFile(dir, "onetwothree", 100);
-        createRandomFile(dir, segment + ".notIn", 50);
-        createRandomFile(dir, segment + ".notIn2", 51);
-
-        // Now test
-        CompoundFileDirectory csw = new CompoundFileDirectory(dir, "test.cfs", newIOContext(random()), true);
-        final String data[] = new String[] {
-            ".zero", ".one", ".ten", ".hundred", ".big1", ".big2", ".big3",
-            ".big4", ".big5", ".big6", ".big7"
-        };
-        for (int i=0; i<data.length; i++) {
-            String fileName = segment + data[i];
-            dir.copy(csw, fileName, fileName, newIOContext(random()));
-        }
-        csw.close();
-
-        CompoundFileDirectory csr = new CompoundFileDirectory(dir, "test.cfs", newIOContext(random()), false);
-        for (int i=0; i<data.length; i++) {
-            IndexInput check = dir.openInput(segment + data[i], newIOContext(random()));
-            IndexInput test = csr.openInput(segment + data[i], newIOContext(random()));
-            assertSameStreams(data[i], check, test);
-            assertSameSeekBehavior(data[i], check, test);
-            test.close();
-            check.close();
-        }
-        csr.close();
-    }
-
-
-    /** Setup a larger compound file with a number of components, each of
-     *  which is a sequential file (so that we can easily tell that we are
-     *  reading in the right byte). The methods sets up 20 files - f0 to f19,
-     *  the size of each file is 1000 bytes.
-     */
-    private void setUp_2() throws IOException {
-        CompoundFileDirectory cw = new CompoundFileDirectory(dir, "f.comp", newIOContext(random()), true);
-        for (int i=0; i<20; i++) {
-            createSequenceFile(dir, "f" + i, (byte) 0, 2000);
-            String fileName = "f" + i;
-            dir.copy(cw, fileName, fileName, newIOContext(random()));
-        }
-        cw.close();
-    }
-
-
-    public void testReadAfterClose() throws IOException {
-        demo_FSIndexInputBug(dir, "test");
-    }
-
-    private void demo_FSIndexInputBug(Directory fsdir, String file)
-    throws IOException
-    {
-        // Setup the test file - we need more than 1024 bytes
-        IndexOutput os = fsdir.createOutput(file, IOContext.DEFAULT);
-        for(int i=0; i<2000; i++) {
-            os.writeByte((byte) i);
-        }
-        os.close();
-
-        IndexInput in = fsdir.openInput(file, IOContext.DEFAULT);
-
-        // This read primes the buffer in IndexInput
-        in.readByte();
-
-        // Close the file
-        in.close();
-
-        // ERROR: this call should fail, but succeeds because the buffer
-        // is still filled
-        in.readByte();
-
-        // ERROR: this call should fail, but succeeds for some reason as well
-        in.seek(1099);
-
-        try {
-            // OK: this call correctly fails. We are now past the 1024 internal
-            // buffer, so an actual IO is attempted, which fails
-            in.readByte();
-            fail("expected readByte() to throw exception");
-        } catch (IOException e) {
-          // expected exception
-        }
-    }
-
-    public void testClonedStreamsClosing() throws IOException {
-        setUp_2();
-        CompoundFileDirectory cr = new CompoundFileDirectory(dir, "f.comp", newIOContext(random()), false);
-
-        // basic clone
-        IndexInput expected = dir.openInput("f11", newIOContext(random()));
-
-        IndexInput one = cr.openInput("f11", newIOContext(random()));
-
-        IndexInput two = one.clone();
-
-        assertSameStreams("basic clone one", expected, one);
-        expected.seek(0);
-        assertSameStreams("basic clone two", expected, two);
-
-        // Now close the first stream
-        one.close();
-
-        // The following should really fail since we couldn't expect to
-        // access a file once close has been called on it (regardless of
-        // buffering and/or clone magic)
-        expected.seek(0);
-        two.seek(0);
-        assertSameStreams("basic clone two/2", expected, two);
-
-
-        // Now close the compound reader
-        cr.close();
-
-        // The following may also fail since the compound stream is closed
-        expected.seek(0);
-        two.seek(0);
-        //assertSameStreams("basic clone two/3", expected, two);
-
-
-        // Now close the second clone
-        two.close();
-        expected.seek(0);
-        two.seek(0);
-        //assertSameStreams("basic clone two/4", expected, two);
-
-        expected.close();
-    }
-
-
-    /** This test opens two files from a compound stream and verifies that
-     *  their file positions are independent of each other.
-     */
-    public void testRandomAccess() throws IOException {
-        setUp_2();
-        CompoundFileDirectory cr = new CompoundFileDirectory(dir, "f.comp", newIOContext(random()), false);
-
-        // Open two files
-        IndexInput e1 = dir.openInput("f11", newIOContext(random()));
-        IndexInput e2 = dir.openInput("f3", newIOContext(random()));
-
-        IndexInput a1 = cr.openInput("f11", newIOContext(random()));
-        IndexInput a2 = dir.openInput("f3", newIOContext(random()));
-
-        // Seek the first pair
-        e1.seek(100);
-        a1.seek(100);
-        assertEquals(100, e1.getFilePointer());
-        assertEquals(100, a1.getFilePointer());
-        byte be1 = e1.readByte();
-        byte ba1 = a1.readByte();
-        assertEquals(be1, ba1);
-
-        // Now seek the second pair
-        e2.seek(1027);
-        a2.seek(1027);
-        assertEquals(1027, e2.getFilePointer());
-        assertEquals(1027, a2.getFilePointer());
-        byte be2 = e2.readByte();
-        byte ba2 = a2.readByte();
-        assertEquals(be2, ba2);
-
-        // Now make sure the first one didn't move
-        assertEquals(101, e1.getFilePointer());
-        assertEquals(101, a1.getFilePointer());
-        be1 = e1.readByte();
-        ba1 = a1.readByte();
-        assertEquals(be1, ba1);
-
-        // Now more the first one again, past the buffer length
-        e1.seek(1910);
-        a1.seek(1910);
-        assertEquals(1910, e1.getFilePointer());
-        assertEquals(1910, a1.getFilePointer());
-        be1 = e1.readByte();
-        ba1 = a1.readByte();
-        assertEquals(be1, ba1);
-
-        // Now make sure the second set didn't move
-        assertEquals(1028, e2.getFilePointer());
-        assertEquals(1028, a2.getFilePointer());
-        be2 = e2.readByte();
-        ba2 = a2.readByte();
-        assertEquals(be2, ba2);
-
-        // Move the second set back, again cross the buffer size
-        e2.seek(17);
-        a2.seek(17);
-        assertEquals(17, e2.getFilePointer());
-        assertEquals(17, a2.getFilePointer());
-        be2 = e2.readByte();
-        ba2 = a2.readByte();
-        assertEquals(be2, ba2);
-
-        // Finally, make sure the first set didn't move
-        // Now make sure the first one didn't move
-        assertEquals(1911, e1.getFilePointer());
-        assertEquals(1911, a1.getFilePointer());
-        be1 = e1.readByte();
-        ba1 = a1.readByte();
-        assertEquals(be1, ba1);
-
-        e1.close();
-        e2.close();
-        a1.close();
-        a2.close();
-        cr.close();
-    }
-
-    /** This test opens two files from a compound stream and verifies that
-     *  their file positions are independent of each other.
-     */
-    public void testRandomAccessClones() throws IOException {
-        setUp_2();
-        CompoundFileDirectory cr = new CompoundFileDirectory(dir, "f.comp", newIOContext(random()), false);
-
-        // Open two files
-        IndexInput e1 = cr.openInput("f11", newIOContext(random()));
-        IndexInput e2 = cr.openInput("f3", newIOContext(random()));
-
-        IndexInput a1 = e1.clone();
-        IndexInput a2 = e2.clone();
-
-        // Seek the first pair
-        e1.seek(100);
-        a1.seek(100);
-        assertEquals(100, e1.getFilePointer());
-        assertEquals(100, a1.getFilePointer());
-        byte be1 = e1.readByte();
-        byte ba1 = a1.readByte();
-        assertEquals(be1, ba1);
-
-        // Now seek the second pair
-        e2.seek(1027);
-        a2.seek(1027);
-        assertEquals(1027, e2.getFilePointer());
-        assertEquals(1027, a2.getFilePointer());
-        byte be2 = e2.readByte();
-        byte ba2 = a2.readByte();
-        assertEquals(be2, ba2);
-
-        // Now make sure the first one didn't move
-        assertEquals(101, e1.getFilePointer());
-        assertEquals(101, a1.getFilePointer());
-        be1 = e1.readByte();
-        ba1 = a1.readByte();
-        assertEquals(be1, ba1);
-
-        // Now more the first one again, past the buffer length
-        e1.seek(1910);
-        a1.seek(1910);
-        assertEquals(1910, e1.getFilePointer());
-        assertEquals(1910, a1.getFilePointer());
-        be1 = e1.readByte();
-        ba1 = a1.readByte();
-        assertEquals(be1, ba1);
-
-        // Now make sure the second set didn't move
-        assertEquals(1028, e2.getFilePointer());
-        assertEquals(1028, a2.getFilePointer());
-        be2 = e2.readByte();
-        ba2 = a2.readByte();
-        assertEquals(be2, ba2);
-
-        // Move the second set back, again cross the buffer size
-        e2.seek(17);
-        a2.seek(17);
-        assertEquals(17, e2.getFilePointer());
-        assertEquals(17, a2.getFilePointer());
-        be2 = e2.readByte();
-        ba2 = a2.readByte();
-        assertEquals(be2, ba2);
-
-        // Finally, make sure the first set didn't move
-        // Now make sure the first one didn't move
-        assertEquals(1911, e1.getFilePointer());
-        assertEquals(1911, a1.getFilePointer());
-        be1 = e1.readByte();
-        ba1 = a1.readByte();
-        assertEquals(be1, ba1);
-
-        e1.close();
-        e2.close();
-        a1.close();
-        a2.close();
-        cr.close();
-    }
-
-
-    public void testFileNotFound() throws IOException {
-        setUp_2();
-        CompoundFileDirectory cr = new CompoundFileDirectory(dir, "f.comp", newIOContext(random()), false);
-
-        // Open two files
-        try {
-            cr.openInput("bogus", newIOContext(random()));
-            fail("File not found");
-
-        } catch (IOException e) {
-            /* success */
-            //System.out.println("SUCCESS: File Not Found: " + e);
-        }
-
-        cr.close();
-    }
-
-
-    public void testReadPastEOF() throws IOException {
-        setUp_2();
-        CompoundFileDirectory cr = new CompoundFileDirectory(dir, "f.comp", newIOContext(random()), false);
-        IndexInput is = cr.openInput("f2", newIOContext(random()));
-        is.seek(is.length() - 10);
-        byte b[] = new byte[100];
-        is.readBytes(b, 0, 10);
-
-        try {
-            is.readByte();
-            fail("Single byte read past end of file");
-        } catch (IOException e) {
-            /* success */
-            //System.out.println("SUCCESS: single byte read past end of file: " + e);
-        }
-
-        is.seek(is.length() - 10);
-        try {
-            is.readBytes(b, 0, 50);
-            fail("Block read past end of file");
-        } catch (IOException e) {
-            /* success */
-            //System.out.println("SUCCESS: block read past end of file: " + e);
-        }
-
-        is.close();
-        cr.close();
-    }
-
-    /** This test that writes larger than the size of the buffer output
-     * will correctly increment the file pointer.
-     */
-    public void testLargeWrites() throws IOException {
-        IndexOutput os = dir.createOutput("testBufferStart.txt", newIOContext(random()));
-
-        byte[] largeBuf = new byte[2048];
-        for (int i=0; i<largeBuf.length; i++) {
-            largeBuf[i] = (byte) (Math.random() * 256);
-        }
-
-        long currentPos = os.getFilePointer();
-        os.writeBytes(largeBuf, largeBuf.length);
-
-        try {
-            assertEquals(currentPos + largeBuf.length, os.getFilePointer());
-        } finally {
-            os.close();
-        }
-
-    }
-    
-   public void testAddExternalFile() throws IOException {
-       createSequenceFile(dir, "d1", (byte) 0, 15);
-
-       Directory newDir = newDirectory();
-       CompoundFileDirectory csw = new CompoundFileDirectory(newDir, "d.cfs", newIOContext(random()), true);
-       dir.copy(csw, "d1", "d1", newIOContext(random()));
-       csw.close();
-
-       CompoundFileDirectory csr = new CompoundFileDirectory(newDir, "d.cfs", newIOContext(random()), false);
-       IndexInput expected = dir.openInput("d1", newIOContext(random()));
-       IndexInput actual = csr.openInput("d1", newIOContext(random()));
-       assertSameStreams("d1", expected, actual);
-       assertSameSeekBehavior("d1", expected, actual);
-       expected.close();
-       actual.close();
-       csr.close();
-       
-       newDir.close();
-   }
-   
-   
-  public void testAppend() throws IOException {
-    Directory newDir = newDirectory();
-    CompoundFileDirectory csw = new CompoundFileDirectory(newDir, "d.cfs", newIOContext(random()), true);
-    int size = 5 + random().nextInt(128);
-    for (int j = 0; j < 2; j++) {
-      IndexOutput os = csw.createOutput("seg_" + j + "_foo.txt", newIOContext(random()));
-      for (int i = 0; i < size; i++) {
-        os.writeInt(i*j);
-      }
-      os.close();
-      String[] listAll = newDir.listAll();
-      assertEquals(1, listAll.length);
-      assertEquals("d.cfs", listAll[0]);
-    }
-    createSequenceFile(dir, "d1", (byte) 0, 15);
-    dir.copy(csw, "d1", "d1", newIOContext(random()));
-    String[] listAll = newDir.listAll();
-    assertEquals(1, listAll.length);
-    assertEquals("d.cfs", listAll[0]);
-    csw.close();
-    CompoundFileDirectory csr = new CompoundFileDirectory(newDir, "d.cfs", newIOContext(random()), false);
-    for (int j = 0; j < 2; j++) {
-      IndexInput openInput = csr.openInput("seg_" + j + "_foo.txt", newIOContext(random()));
-      assertEquals(size * 4, openInput.length());
-      for (int i = 0; i < size; i++) {
-        assertEquals(i*j, openInput.readInt());
-      }
-
-      openInput.close();
-
-    }
-    IndexInput expected = dir.openInput("d1", newIOContext(random()));
-    IndexInput actual = csr.openInput("d1", newIOContext(random()));
-    assertSameStreams("d1", expected, actual);
-    assertSameSeekBehavior("d1", expected, actual);
-    expected.close();
-    actual.close();
-    csr.close();
-    newDir.close();
-  }
-  
-  public void testAppendTwice() throws IOException {
-    Directory newDir = newDirectory();
-    CompoundFileDirectory csw = new CompoundFileDirectory(newDir, "d.cfs", newIOContext(random()), true);
-    createSequenceFile(newDir, "d1", (byte) 0, 15);
-    IndexOutput out = csw.createOutput("d.xyz", newIOContext(random()));
-    out.writeInt(0);
-    out.close();
-    assertEquals(1, csw.listAll().length);
-    assertEquals("d.xyz", csw.listAll()[0]);
-   
-    csw.close();
-
-    CompoundFileDirectory cfr = new CompoundFileDirectory(newDir, "d.cfs", newIOContext(random()), false);
-    assertEquals(1, cfr.listAll().length);
-    assertEquals("d.xyz", cfr.listAll()[0]);
-    cfr.close();
-    newDir.close();
-  }
-  
-  public void testEmptyCFS() throws IOException {
-    Directory newDir = newDirectory();
-    CompoundFileDirectory csw = new CompoundFileDirectory(newDir, "d.cfs", newIOContext(random()), true);
-    csw.close();
-
-    CompoundFileDirectory csr = new CompoundFileDirectory(newDir, "d.cfs", newIOContext(random()), false);
-    assertEquals(0, csr.listAll().length);
-    csr.close();
-
-    newDir.close();
-  }
-  
-  public void testReadNestedCFP() throws IOException {
-    Directory newDir = newDirectory();
-    // manually manipulates directory
-    if (newDir instanceof MockDirectoryWrapper) {
-      ((MockDirectoryWrapper)newDir).setEnableVirusScanner(false);
-    }
-    CompoundFileDirectory csw = new CompoundFileDirectory(newDir, "d.cfs", newIOContext(random()), true);
-    CompoundFileDirectory nested = new CompoundFileDirectory(newDir, "b.cfs", newIOContext(random()), true);
-    IndexOutput out = nested.createOutput("b.xyz", newIOContext(random()));
-    IndexOutput out1 = nested.createOutput("b_1.xyz", newIOContext(random()));
-    out.writeInt(0);
-    out1.writeInt(1);
-    out.close();
-    out1.close();
-    nested.close();
-    newDir.copy(csw, "b.cfs", "b.cfs", newIOContext(random()));
-    newDir.copy(csw, "b.cfe", "b.cfe", newIOContext(random()));
-    newDir.deleteFile("b.cfs");
-    newDir.deleteFile("b.cfe");
-    csw.close();
-    
-    assertEquals(2, newDir.listAll().length);
-    csw = new CompoundFileDirectory(newDir, "d.cfs", newIOContext(random()), false);
-    
-    assertEquals(2, csw.listAll().length);
-    nested = new CompoundFileDirectory(csw, "b.cfs", newIOContext(random()), false);
-    
-    assertEquals(2, nested.listAll().length);
-    IndexInput openInput = nested.openInput("b.xyz", newIOContext(random()));
-    assertEquals(0, openInput.readInt());
-    openInput.close();
-    openInput = nested.openInput("b_1.xyz", newIOContext(random()));
-    assertEquals(1, openInput.readInt());
-    openInput.close();
-    nested.close();
-    csw.close();
-    newDir.close();
-  }
-  
-  public void testDoubleClose() throws IOException {
-    Directory newDir = newDirectory();
-    CompoundFileDirectory csw = new CompoundFileDirectory(newDir, "d.cfs", newIOContext(random()), true);
-    IndexOutput out = csw.createOutput("d.xyz", newIOContext(random()));
-    out.writeInt(0);
-    out.close();
-    
-    csw.close();
-    // close a second time - must have no effect according to Closeable
-    csw.close();
-    
-    csw = new CompoundFileDirectory(newDir, "d.cfs", newIOContext(random()), false);
-    IndexInput openInput = csw.openInput("d.xyz", newIOContext(random()));
-    assertEquals(0, openInput.readInt());
-    openInput.close();
-    csw.close();
-    // close a second time - must have no effect according to Closeable
-    csw.close();
-    
-    newDir.close();
-    
-  }
-
-  // Make sure we don't somehow use more than 1 descriptor
-  // when reading a CFS with many subs:
-  public void testManySubFiles() throws IOException {
-
-    final Directory d = newFSDirectory(createTempDir("CFSManySubFiles"));
-    final int FILE_COUNT = atLeast(500);
-
-    for(int fileIdx=0;fileIdx<FILE_COUNT;fileIdx++) {
-      IndexOutput out = d.createOutput("file." + fileIdx, newIOContext(random()));
-      out.writeByte((byte) fileIdx);
-      out.close();
-    }
-    
-    final CompoundFileDirectory cfd = new CompoundFileDirectory(d, "c.cfs", newIOContext(random()), true);
-    for(int fileIdx=0;fileIdx<FILE_COUNT;fileIdx++) {
-      final String fileName = "file." + fileIdx;
-      d.copy(cfd, fileName, fileName, newIOContext(random()));
-    }
-    cfd.close();
-
-    final IndexInput[] ins = new IndexInput[FILE_COUNT];
-    final CompoundFileDirectory cfr = new CompoundFileDirectory(d, "c.cfs", newIOContext(random()), false);
-    for(int fileIdx=0;fileIdx<FILE_COUNT;fileIdx++) {
-      ins[fileIdx] = cfr.openInput("file." + fileIdx, newIOContext(random()));
-    }
-
-    for(int fileIdx=0;fileIdx<FILE_COUNT;fileIdx++) {
-      assertEquals((byte) fileIdx, ins[fileIdx].readByte());
-    }
-
-    for(int fileIdx=0;fileIdx<FILE_COUNT;fileIdx++) {
-      ins[fileIdx].close();
-    }
-    cfr.close();
-    d.close();
-  }
-  
-  public void testListAll() throws Exception {
-    Directory dir = newDirectory();
-    if (dir instanceof MockDirectoryWrapper) {
-      // test lists files manually and tries to verify every .cfs it finds,
-      // but a virus scanner could leave some trash.
-      ((MockDirectoryWrapper)dir).setEnableVirusScanner(false);
-    }
-    // riw should sometimes create docvalues fields, etc
-    RandomIndexWriter riw = new RandomIndexWriter(random(), dir);
-    Document doc = new Document();
-    // these fields should sometimes get term vectors, etc
-    Field idField = newStringField("id", "", Field.Store.NO);
-    Field bodyField = newTextField("body", "", Field.Store.NO);
-    doc.add(idField);
-    doc.add(bodyField);
-    for (int i = 0; i < 100; i++) {
-      idField.setStringValue(Integer.toString(i));
-      bodyField.setStringValue(TestUtil.randomUnicodeString(random()));
-      riw.addDocument(doc);
-      if (random().nextInt(7) == 0) {
-        riw.commit();
-      }
-    }
-    riw.close();
-    checkFiles(dir);
-    dir.close();
-  }
-  
-  // checks that we can open all files returned by listAll!
-  private void checkFiles(Directory dir) throws IOException {
-    for (String file : dir.listAll()) {
-      if (file.endsWith(IndexFileNames.COMPOUND_FILE_EXTENSION)) {
-        CompoundFileDirectory cfsDir = new CompoundFileDirectory(dir, file, newIOContext(random()), false);
-        checkFiles(cfsDir); // recurse into cfs
-        cfsDir.close();
-      }
-      IndexInput in = null;
-      boolean success = false;
-      try {
-        in = dir.openInput(file, newIOContext(random()));
-        success = true;
-      } finally {
-        if (success) {
-          IOUtils.close(in);
-        } else {
-          IOUtils.closeWhileHandlingException(in);
-        }
-      }
-    }
-  }
-}
Index: lucene/core/src/test/org/apache/lucene/index/TestConsistentFieldNumbers.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/index/TestConsistentFieldNumbers.java	(revision 1629405)
+++ lucene/core/src/test/org/apache/lucene/index/TestConsistentFieldNumbers.java	(working copy)
@@ -68,8 +68,8 @@
       sis.read(dir);
       assertEquals(2, sis.size());
 
-      FieldInfos fis1 = SegmentReader.readFieldInfos(sis.info(0));
-      FieldInfos fis2 = SegmentReader.readFieldInfos(sis.info(1));
+      FieldInfos fis1 = IndexWriter.readFieldInfos(sis.info(0));
+      FieldInfos fis2 = IndexWriter.readFieldInfos(sis.info(1));
 
       assertEquals("f1", fis1.fieldInfo(0).name);
       assertEquals("f2", fis1.fieldInfo(1).name);
@@ -86,7 +86,7 @@
       sis.read(dir);
       assertEquals(1, sis.size());
 
-      FieldInfos fis3 = SegmentReader.readFieldInfos(sis.info(0));
+      FieldInfos fis3 = IndexWriter.readFieldInfos(sis.info(0));
 
       assertEquals("f1", fis3.fieldInfo(0).name);
       assertEquals("f2", fis3.fieldInfo(1).name);
@@ -134,8 +134,8 @@
     sis.read(dir1);
     assertEquals(2, sis.size());
 
-    FieldInfos fis1 = SegmentReader.readFieldInfos(sis.info(0));
-    FieldInfos fis2 = SegmentReader.readFieldInfos(sis.info(1));
+    FieldInfos fis1 = IndexWriter.readFieldInfos(sis.info(0));
+    FieldInfos fis2 = IndexWriter.readFieldInfos(sis.info(1));
 
     assertEquals("f1", fis1.fieldInfo(0).name);
     assertEquals("f2", fis1.fieldInfo(1).name);
@@ -164,7 +164,7 @@
         SegmentInfos sis = new SegmentInfos();
         sis.read(dir);
         assertEquals(1, sis.size());
-        FieldInfos fis1 = SegmentReader.readFieldInfos(sis.info(0));
+        FieldInfos fis1 = IndexWriter.readFieldInfos(sis.info(0));
         assertEquals("f1", fis1.fieldInfo(0).name);
         assertEquals("f2", fis1.fieldInfo(1).name);
       }
@@ -181,8 +181,8 @@
         SegmentInfos sis = new SegmentInfos();
         sis.read(dir);
         assertEquals(2, sis.size());
-        FieldInfos fis1 = SegmentReader.readFieldInfos(sis.info(0));
-        FieldInfos fis2 = SegmentReader.readFieldInfos(sis.info(1));
+        FieldInfos fis1 = IndexWriter.readFieldInfos(sis.info(0));
+        FieldInfos fis2 = IndexWriter.readFieldInfos(sis.info(1));
         assertEquals("f1", fis1.fieldInfo(0).name);
         assertEquals("f2", fis1.fieldInfo(1).name);
         assertEquals("f1", fis2.fieldInfo(0).name);
@@ -202,9 +202,9 @@
         SegmentInfos sis = new SegmentInfos();
         sis.read(dir);
         assertEquals(3, sis.size());
-        FieldInfos fis1 = SegmentReader.readFieldInfos(sis.info(0));
-        FieldInfos fis2 = SegmentReader.readFieldInfos(sis.info(1));
-        FieldInfos fis3 = SegmentReader.readFieldInfos(sis.info(2));
+        FieldInfos fis1 = IndexWriter.readFieldInfos(sis.info(0));
+        FieldInfos fis2 = IndexWriter.readFieldInfos(sis.info(1));
+        FieldInfos fis3 = IndexWriter.readFieldInfos(sis.info(2));
         assertEquals("f1", fis1.fieldInfo(0).name);
         assertEquals("f2", fis1.fieldInfo(1).name);
         assertEquals("f1", fis2.fieldInfo(0).name);
@@ -234,7 +234,7 @@
       SegmentInfos sis = new SegmentInfos();
       sis.read(dir);
       assertEquals(1, sis.size());
-      FieldInfos fis1 = SegmentReader.readFieldInfos(sis.info(0));
+      FieldInfos fis1 = IndexWriter.readFieldInfos(sis.info(0));
       assertEquals("f1", fis1.fieldInfo(0).name);
       assertEquals("f2", fis1.fieldInfo(1).name);
       assertEquals("f3", fis1.fieldInfo(2).name);
@@ -272,7 +272,7 @@
     SegmentInfos sis = new SegmentInfos();
     sis.read(dir);
     for (SegmentCommitInfo si : sis) {
-      FieldInfos fis = SegmentReader.readFieldInfos(si);
+      FieldInfos fis = IndexWriter.readFieldInfos(si);
 
       for (FieldInfo fi : fis) {
         Field expected = getField(Integer.parseInt(fi.name));
Index: lucene/core/src/test/org/apache/lucene/index/TestDoc.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/index/TestDoc.java	(revision 1629405)
+++ lucene/core/src/test/org/apache/lucene/index/TestDoc.java	(working copy)
@@ -223,7 +223,7 @@
 
       SegmentMerger merger = new SegmentMerger(Arrays.<LeafReader>asList(r1, r2),
           si, InfoStream.getDefault(), trackingDir,
-          MergeState.CheckAbort.NONE, new FieldInfos.FieldNumbers(), context, true);
+          MergeState.CheckAbort.NONE, new FieldInfos.FieldNumbers(), context);
 
       MergeState mergeState = merger.merge();
       r1.close();
Index: lucene/core/src/test/org/apache/lucene/index/TestIndexFileDeleter.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/index/TestIndexFileDeleter.java	(revision 1629405)
+++ lucene/core/src/test/org/apache/lucene/index/TestIndexFileDeleter.java	(working copy)
@@ -22,7 +22,6 @@
 import java.util.concurrent.atomic.AtomicBoolean;
 
 import org.apache.lucene.analysis.MockAnalyzer;
-import org.apache.lucene.codecs.Codec;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.index.IndexWriterConfig.OpenMode;
@@ -84,6 +83,13 @@
     Term searchTerm = new Term("id", "7");
     writer.deleteDocuments(searchTerm);
     writer.close();
+    
+    // read in index to try to not depend on codec-specific filenames so much
+    SegmentInfos sis = new SegmentInfos();
+    sis.read(dir);
+    SegmentInfo si0 = sis.info(0).info;
+    SegmentInfo si1 = sis.info(1).info;
+    SegmentInfo si3 = sis.info(3).info;
 
     // Now, artificially create an extra .del file & extra
     // .s0 file:
@@ -110,11 +116,13 @@
     // non-existent segment:
     copyFile(dir, "_0_1" + ext, "_188_1" + ext);
 
+    String cfsFiles0[] = si0.getCodec().compoundFormat().files(si0);
+    
     // Create a bogus segment file:
-    copyFile(dir, "_0.cfs", "_188.cfs");
+    copyFile(dir, cfsFiles0[0], "_188.cfs");
 
     // Create a bogus fnm file when the CFS already exists:
-    copyFile(dir, "_0.cfs", "_0.fnm");
+    copyFile(dir, cfsFiles0[0], "_0.fnm");
     
     // Create some old segments file:
     copyFile(dir, "segments_2", "segments");
@@ -124,9 +132,15 @@
     
     // TODO: assert is bogus (relies upon codec-specific filenames)
     assertTrue(slowFileExists(dir, "_3.fdt") || slowFileExists(dir, "_3.fld"));
-    assertTrue(!slowFileExists(dir, "_3.cfs"));
-    copyFile(dir, "_1.cfs", "_3.cfs");
     
+    String cfsFiles3[] = si3.getCodec().compoundFormat().files(si3);
+    for (String f : cfsFiles3) {
+      assertTrue(!slowFileExists(dir, f));
+    }
+    
+    String cfsFiles1[] = si1.getCodec().compoundFormat().files(si1);
+    copyFile(dir, cfsFiles1[0], "_3.cfs");
+    
     String[] filesPre = dir.listAll();
 
     // Open & close a writer: it should delete the above 4
Index: lucene/core/src/test/org/apache/lucene/index/TestIndexWriter.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/index/TestIndexWriter.java	(revision 1629405)
+++ lucene/core/src/test/org/apache/lucene/index/TestIndexWriter.java	(working copy)
@@ -1327,6 +1327,7 @@
 
 
   public void testDeleteUnusedFiles() throws Exception {
+    assumeFalse("test relies on exact filenames", Codec.getDefault() instanceof SimpleTextCodec);
     for(int iter=0;iter<2;iter++) {
       MockDirectoryWrapper dir = newMockDirectory(); // relies on windows semantics
       dir.setEnableVirusScanner(false); // but ensures files are actually deleted
Index: lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions.java	(revision 1629405)
+++ lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions.java	(working copy)
@@ -1231,18 +1231,20 @@
 
     long gen = SegmentInfos.getLastCommitGeneration(dir);
     assertTrue("segment generation should be > 0 but got " + gen, gen > 0);
-
-    String[] files = dir.listAll();
+    
     boolean corrupted = false;
-    for(int i=0;i<files.length;i++) {
-      if (files[i].endsWith(".cfs")) {
-        dir.deleteFile(files[i]);
-        corrupted = true;
-        break;
-      }
+    SegmentInfos sis = new SegmentInfos();
+    sis.read(dir);
+    for (SegmentCommitInfo si : sis) {
+      assertTrue(si.info.getUseCompoundFile());
+      String cfsFiles[] = si.info.getCodec().compoundFormat().files(si.info);
+      dir.deleteFile(cfsFiles[0]);
+      corrupted = true;
+      break;
     }
-    assertTrue("failed to find cfs file to remove", corrupted);
 
+    assertTrue("failed to find cfs file to remove: ", corrupted);
+
     IndexReader reader = null;
     try {
       reader = DirectoryReader.open(dir);
Index: lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger.java	(revision 1629405)
+++ lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger.java	(working copy)
@@ -83,7 +83,7 @@
 
     SegmentMerger merger = new SegmentMerger(Arrays.<LeafReader>asList(reader1, reader2),
         si, InfoStream.getDefault(), mergedDir,
-        MergeState.CheckAbort.NONE, new FieldInfos.FieldNumbers(), newIOContext(random()), true);
+        MergeState.CheckAbort.NONE, new FieldInfos.FieldNumbers(), newIOContext(random()));
     MergeState mergeState = merger.merge();
     int docsMerged = mergeState.segmentInfo.getDocCount();
     assertTrue(docsMerged == 2);
Index: lucene/core/src/test/org/apache/lucene/index/TestTermVectorsReader.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/index/TestTermVectorsReader.java	(revision 1629405)
+++ lucene/core/src/test/org/apache/lucene/index/TestTermVectorsReader.java	(working copy)
@@ -129,7 +129,7 @@
     seg = writer.newestSegment();
     writer.close();
 
-    fieldInfos = SegmentReader.readFieldInfos(seg);
+    fieldInfos = IndexWriter.readFieldInfos(seg);
   }
   
   @Override
Index: lucene/core/src/test/org/apache/lucene/store/TestNRTCachingDirectory.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/store/TestNRTCachingDirectory.java	(revision 1629405)
+++ lucene/core/src/test/org/apache/lucene/store/TestNRTCachingDirectory.java	(working copy)
@@ -122,21 +122,4 @@
     writer.close();
     cachedFSDir.close();
   }
-
-  // LUCENE-5724
-  public void testLargeCFS() throws IOException {
-    Directory dir = new NRTCachingDirectory(newFSDirectory(createTempDir()), 2.0, 25.0);
-    IOContext context = new IOContext(new FlushInfo(0, 512*1024*1024));
-    IndexOutput out = dir.createOutput("big.bin", context);
-    byte[] bytes = new byte[512];
-    for(int i=0;i<1024*1024;i++) {
-      out.writeBytes(bytes, 0, bytes.length);
-    }
-    out.close();
-
-    Directory cfsDir = new CompoundFileDirectory(dir, "big.cfs", context, true);
-    dir.copy(cfsDir, "big.bin", "big.bin", context);
-    cfsDir.close();
-    dir.close();
-  }
 }
Index: lucene/misc/src/java/org/apache/lucene/index/CompoundFileExtractor.java
===================================================================
--- lucene/misc/src/java/org/apache/lucene/index/CompoundFileExtractor.java	(revision 1629405)
+++ lucene/misc/src/java/org/apache/lucene/index/CompoundFileExtractor.java	(working copy)
@@ -1,134 +0,0 @@
-package org.apache.lucene.index;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * Prints the filename and size of each file within a given compound file.
- * Add the -extract flag to extract files to the current working directory.
- * In order to make the extracted version of the index work, you have to copy
- * the segments file from the compound index into the directory where the extracted files are stored.
- * @param args Usage: org.apache.lucene.index.IndexReader [-extract] &lt;cfsfile&gt;
- */
-
-import java.io.IOException;
-import java.io.OutputStream;
-import java.nio.file.Files;
-import java.nio.file.Path;
-import java.nio.file.Paths;
-
-import org.apache.lucene.store.CompoundFileDirectory;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.FSDirectory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.util.ArrayUtil;
-import org.apache.lucene.util.CommandLineUtil;
-
-/**
- * Command-line tool for extracting sub-files out of a compound file.
- */
-public class CompoundFileExtractor {
-
-  public static void main(String [] args) {
-    String filename = null;
-    boolean extract = false;
-    String dirImpl = null;
-
-    int j = 0;
-    while(j < args.length) {
-      String arg = args[j];
-      if ("-extract".equals(arg)) {
-        extract = true;
-      } else if ("-dir-impl".equals(arg)) {
-        if (j == args.length - 1) {
-          System.out.println("ERROR: missing value for -dir-impl option");
-          System.exit(1);
-        }
-        j++;
-        dirImpl = args[j];
-      } else if (filename == null) {
-        filename = arg;
-      }
-      j++;
-    }
-
-    if (filename == null) {
-      System.out.println("Usage: org.apache.lucene.index.CompoundFileExtractor [-extract] [-dir-impl X] <cfsfile>");
-      return;
-    }
-
-    Directory dir = null;
-    CompoundFileDirectory cfr = null;
-    IOContext context = IOContext.READ;
-
-    try {
-      Path file = Paths.get(filename);
-      Path directory = file.toAbsolutePath().getParent();
-      filename = file.getFileName().toString();
-      if (dirImpl == null) {
-        dir = FSDirectory.open(directory);
-      } else {
-        dir = CommandLineUtil.newFSDirectory(dirImpl, directory);
-      }
-      
-      cfr = new CompoundFileDirectory(dir, filename, IOContext.DEFAULT, false);
-
-      String [] files = cfr.listAll();
-      ArrayUtil.timSort(files);   // sort the array of filename so that the output is more readable
-
-      for (int i = 0; i < files.length; ++i) {
-        long len = cfr.fileLength(files[i]);
-
-        if (extract) {
-          System.out.println("extract " + files[i] + " with " + len + " bytes to local directory...");
-          IndexInput ii = cfr.openInput(files[i], context);
-
-          OutputStream f = Files.newOutputStream(Paths.get(files[i]));
-
-          // read and write with a small buffer, which is more effective than reading byte by byte
-          byte[] buffer = new byte[1024];
-          int chunk = buffer.length;
-          while(len > 0) {
-            final int bufLen = (int) Math.min(chunk, len);
-            ii.readBytes(buffer, 0, bufLen);
-            f.write(buffer, 0, bufLen);
-            len -= bufLen;
-          }
-
-          f.close();
-          ii.close();
-        }
-        else
-          System.out.println(files[i] + ": " + len + " bytes");
-      }
-    } catch (IOException ioe) {
-      ioe.printStackTrace();
-    }
-    finally {
-      try {
-        if (dir != null)
-          dir.close();
-        if (cfr != null)
-          cfr.close();
-      }
-      catch (IOException ioe) {
-        ioe.printStackTrace();
-      }
-    }
-  }
-}
Index: lucene/misc/src/test/org/apache/lucene/index/TestIndexSplitter.java
===================================================================
--- lucene/misc/src/test/org/apache/lucene/index/TestIndexSplitter.java	(revision 1629405)
+++ lucene/misc/src/test/org/apache/lucene/index/TestIndexSplitter.java	(working copy)
@@ -78,7 +78,9 @@
     Path destDir2 = createTempDir(LuceneTestCase.getTestClass().getSimpleName());
     IndexSplitter.main(new String[] {dir.toAbsolutePath().toString(), destDir2.toAbsolutePath().toString(), splitSegName});
     Directory fsDirDest2 = newFSDirectory(destDir2);
-    assertEquals(4, fsDirDest2.listAll().length);
+    SegmentInfos sis = new SegmentInfos();
+    sis.read(fsDirDest2);
+    assertEquals(1, sis.size());
     r = DirectoryReader.open(fsDirDest2);
     assertEquals(50, r.maxDoc());
     r.close();
Index: lucene/sandbox/src/java/org/apache/lucene/codecs/idversion/IDVersionPostingsFormat.java
===================================================================
--- lucene/sandbox/src/java/org/apache/lucene/codecs/idversion/IDVersionPostingsFormat.java	(revision 1629405)
+++ lucene/sandbox/src/java/org/apache/lucene/codecs/idversion/IDVersionPostingsFormat.java	(working copy)
@@ -99,15 +99,10 @@
 
   @Override
   public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {
-    PostingsReaderBase postingsReader = new IDVersionPostingsReader();
+    PostingsReaderBase postingsReader = new IDVersionPostingsReader(state);
     boolean success = false;
      try {
-       FieldsProducer ret = new VersionBlockTreeTermsReader(state.directory,
-                                                            state.fieldInfos,
-                                                            state.segmentInfo,
-                                                            postingsReader,
-                                                            state.context,
-                                                            state.segmentSuffix);
+       FieldsProducer ret = new VersionBlockTreeTermsReader(postingsReader, state);
        success = true;
        return ret;
      } finally {
Index: lucene/sandbox/src/java/org/apache/lucene/codecs/idversion/IDVersionPostingsReader.java
===================================================================
--- lucene/sandbox/src/java/org/apache/lucene/codecs/idversion/IDVersionPostingsReader.java	(revision 1629405)
+++ lucene/sandbox/src/java/org/apache/lucene/codecs/idversion/IDVersionPostingsReader.java	(working copy)
@@ -26,21 +26,27 @@
 import org.apache.lucene.index.DocsAndPositionsEnum;
 import org.apache.lucene.index.DocsEnum;
 import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.SegmentReadState;
 import org.apache.lucene.store.DataInput;
 import org.apache.lucene.store.IndexInput;
 import org.apache.lucene.util.Accountable;
-import org.apache.lucene.util.BitUtil;
 import org.apache.lucene.util.Bits;
 
 final class IDVersionPostingsReader extends PostingsReaderBase {
+  final SegmentReadState state;
+  
+  public IDVersionPostingsReader(SegmentReadState state) {
+    this.state = state;
+  }
 
   @Override
   public void init(IndexInput termsIn) throws IOException {
     // Make sure we are talking to the matching postings writer
-    CodecUtil.checkHeader(termsIn,
-                          IDVersionPostingsWriter.TERMS_CODEC,
-                          IDVersionPostingsWriter.VERSION_START,
-                          IDVersionPostingsWriter.VERSION_CURRENT);
+    CodecUtil.checkSegmentHeader(termsIn,
+                                 IDVersionPostingsWriter.TERMS_CODEC,
+                                 IDVersionPostingsWriter.VERSION_START,
+                                 IDVersionPostingsWriter.VERSION_CURRENT,
+                                 state.segmentInfo.getId(), state.segmentSuffix);
   }
 
   @Override
Index: lucene/sandbox/src/java/org/apache/lucene/codecs/idversion/IDVersionPostingsWriter.java
===================================================================
--- lucene/sandbox/src/java/org/apache/lucene/codecs/idversion/IDVersionPostingsWriter.java	(revision 1629405)
+++ lucene/sandbox/src/java/org/apache/lucene/codecs/idversion/IDVersionPostingsWriter.java	(working copy)
@@ -26,7 +26,6 @@
 import org.apache.lucene.index.SegmentWriteState;
 import org.apache.lucene.store.DataOutput;
 import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.util.BitUtil;
 import org.apache.lucene.util.BytesRef;
 
 final class IDVersionPostingsWriter extends PushPostingsWriterBase {
@@ -34,7 +33,7 @@
   final static String TERMS_CODEC = "IDVersionPostingsWriterTerms";
 
   // Increment version to change it
-  final static int VERSION_START = 0;
+  final static int VERSION_START = 1;
   final static int VERSION_CURRENT = VERSION_START;
 
   final static IDVersionTermState emptyState = new IDVersionTermState();
@@ -57,7 +56,7 @@
 
   @Override
   public void init(IndexOutput termsOut) throws IOException {
-    CodecUtil.writeHeader(termsOut, TERMS_CODEC, VERSION_CURRENT);
+    CodecUtil.writeSegmentHeader(termsOut, TERMS_CODEC, VERSION_CURRENT, state.segmentInfo.getId(), state.segmentSuffix);
   }
 
   @Override
Index: lucene/sandbox/src/java/org/apache/lucene/codecs/idversion/IDVersionSegmentTermsEnum.java
===================================================================
--- lucene/sandbox/src/java/org/apache/lucene/codecs/idversion/IDVersionSegmentTermsEnum.java	(revision 1629405)
+++ lucene/sandbox/src/java/org/apache/lucene/codecs/idversion/IDVersionSegmentTermsEnum.java	(working copy)
@@ -283,7 +283,7 @@
       targetUpto = 0;
 
       IDVersionSegmentTermsEnumFrame lastFrame = stack[0];
-      assert validIndexPrefix <= term.length(): "validIndexPrefix=" + validIndexPrefix + " term.length=" + term.length() + " seg=" + fr.parent.segment;
+      assert validIndexPrefix <= term.length(): "validIndexPrefix=" + validIndexPrefix + " term.length=" + term.length() + " seg=" + fr.parent;
 
       final int targetLimit = Math.min(target.length, validIndexPrefix);
 
@@ -1063,6 +1063,6 @@
 
   @Override
   public String toString() {
-    return "IDVersionSegmentTermsEnum(seg=" + fr.parent.segment + ")";
+    return "IDVersionSegmentTermsEnum(seg=" + fr.parent + ")";
   }
 }
Index: lucene/sandbox/src/java/org/apache/lucene/codecs/idversion/VersionBlockTreeTermsReader.java
===================================================================
--- lucene/sandbox/src/java/org/apache/lucene/codecs/idversion/VersionBlockTreeTermsReader.java	(revision 1629405)
+++ lucene/sandbox/src/java/org/apache/lucene/codecs/idversion/VersionBlockTreeTermsReader.java	(working copy)
@@ -29,12 +29,9 @@
 import org.apache.lucene.codecs.PostingsReaderBase;
 import org.apache.lucene.index.CorruptIndexException;
 import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.FieldInfos;
 import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.index.SegmentReadState;
 import org.apache.lucene.index.Terms;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
 import org.apache.lucene.store.IndexInput;
 import org.apache.lucene.util.Accountable;
 import org.apache.lucene.util.Accountables;
@@ -61,38 +58,36 @@
 
   private final TreeMap<String,VersionFieldReader> fields = new TreeMap<>();
 
-  /** File offset where the directory starts in the terms file. */
-  private long dirOffset;
-
-  /** File offset where the directory starts in the index file. */
-  private long indexDirOffset;
-
-  final String segment;
-  
-  private final int version;
-
   /** Sole constructor. */
-  public VersionBlockTreeTermsReader(Directory dir, FieldInfos fieldInfos, SegmentInfo info,
-                                     PostingsReaderBase postingsReader, IOContext ioContext,
-                                     String segmentSuffix)
-    throws IOException {
+  public VersionBlockTreeTermsReader(PostingsReaderBase postingsReader, SegmentReadState state) throws IOException {
     
     this.postingsReader = postingsReader;
 
-    this.segment = info.name;
-    in = dir.openInput(IndexFileNames.segmentFileName(segment, segmentSuffix, VersionBlockTreeTermsWriter.TERMS_EXTENSION),
-                       ioContext);
+    String termsFile = IndexFileNames.segmentFileName(state.segmentInfo.name, 
+                                                      state.segmentSuffix, 
+                                                      VersionBlockTreeTermsWriter.TERMS_EXTENSION);
+    in = state.directory.openInput(termsFile, state.context);
 
     boolean success = false;
     IndexInput indexIn = null;
 
     try {
-      version = readHeader(in);
-      indexIn = dir.openInput(IndexFileNames.segmentFileName(segment, segmentSuffix, VersionBlockTreeTermsWriter.TERMS_INDEX_EXTENSION),
-                                ioContext);
-      int indexVersion = readIndexHeader(indexIn);
-      if (indexVersion != version) {
-        throw new CorruptIndexException("mixmatched version files: " + in + "=" + version + "," + indexIn + "=" + indexVersion, indexIn);
+      int termsVersion = CodecUtil.checkSegmentHeader(in, VersionBlockTreeTermsWriter.TERMS_CODEC_NAME,
+                                                          VersionBlockTreeTermsWriter.VERSION_START,
+                                                          VersionBlockTreeTermsWriter.VERSION_CURRENT,
+                                                          state.segmentInfo.getId(), state.segmentSuffix);
+      
+      String indexFile = IndexFileNames.segmentFileName(state.segmentInfo.name, 
+                                                        state.segmentSuffix, 
+                                                        VersionBlockTreeTermsWriter.TERMS_INDEX_EXTENSION);
+      indexIn = state.directory.openInput(indexFile, state.context);
+      int indexVersion = CodecUtil.checkSegmentHeader(indexIn, VersionBlockTreeTermsWriter.TERMS_INDEX_CODEC_NAME,
+                                                               VersionBlockTreeTermsWriter.VERSION_START,
+                                                               VersionBlockTreeTermsWriter.VERSION_CURRENT,
+                                                               state.segmentInfo.getId(), state.segmentSuffix);
+      
+      if (indexVersion != termsVersion) {
+        throw new CorruptIndexException("mixmatched version files: " + in + "=" + termsVersion + "," + indexIn + "=" + indexVersion, indexIn);
       }
       
       // verify
@@ -108,8 +103,8 @@
       CodecUtil.retrieveChecksum(in);
 
       // Read per-field details
-      seekDir(in, dirOffset);
-      seekDir(indexIn, indexDirOffset);
+      seekDir(in);
+      seekDir(indexIn);
 
       final int numFields = in.readVInt();
       if (numFields < 0) {
@@ -126,7 +121,7 @@
         code.length = numBytes;
         final long version = in.readVLong();
         final Pair<BytesRef,Long> rootCode = VersionBlockTreeTermsWriter.FST_OUTPUTS.newPair(code, version);
-        final FieldInfo fieldInfo = fieldInfos.fieldInfo(field);
+        final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);
         assert fieldInfo != null: "field=" + field;
         final long sumTotalTermFreq = numTerms;
         final long sumDocFreq = numTerms;
@@ -136,8 +131,8 @@
 
         BytesRef minTerm = readBytesRef(in);
         BytesRef maxTerm = readBytesRef(in);
-        if (docCount < 0 || docCount > info.getDocCount()) { // #docs with field must be <= #docs
-          throw new CorruptIndexException("invalid docCount: " + docCount + " maxDoc: " + info.getDocCount(), in);
+        if (docCount < 0 || docCount > state.segmentInfo.getDocCount()) { // #docs with field must be <= #docs
+          throw new CorruptIndexException("invalid docCount: " + docCount + " maxDoc: " + state.segmentInfo.getDocCount(), in);
         }
         if (sumDocFreq < docCount) {  // #postings must be >= #docs with field
           throw new CorruptIndexException("invalid sumDocFreq: " + sumDocFreq + " docCount: " + docCount, in);
@@ -172,27 +167,10 @@
     return bytes;
   }
 
-  /** Reads terms file header. */
-  private int readHeader(IndexInput input) throws IOException {
-    int version = CodecUtil.checkHeader(input, VersionBlockTreeTermsWriter.TERMS_CODEC_NAME,
-                          VersionBlockTreeTermsWriter.VERSION_START,
-                          VersionBlockTreeTermsWriter.VERSION_CURRENT);
-    return version;
-  }
-
-  /** Reads index file header. */
-  private int readIndexHeader(IndexInput input) throws IOException {
-    int version = CodecUtil.checkHeader(input, VersionBlockTreeTermsWriter.TERMS_INDEX_CODEC_NAME,
-                          VersionBlockTreeTermsWriter.VERSION_START,
-                          VersionBlockTreeTermsWriter.VERSION_CURRENT);
-    return version;
-  }
-
   /** Seek {@code input} to the directory offset. */
-  private void seekDir(IndexInput input, long dirOffset)
-      throws IOException {
+  private void seekDir(IndexInput input) throws IOException {
     input.seek(input.length() - CodecUtil.footerLength() - 8);
-    dirOffset = input.readLong();
+    long dirOffset = input.readLong();
     input.seek(dirOffset);
   }
 
Index: lucene/sandbox/src/java/org/apache/lucene/codecs/idversion/VersionBlockTreeTermsWriter.java
===================================================================
--- lucene/sandbox/src/java/org/apache/lucene/codecs/idversion/VersionBlockTreeTermsWriter.java	(revision 1629405)
+++ lucene/sandbox/src/java/org/apache/lucene/codecs/idversion/VersionBlockTreeTermsWriter.java	(working copy)
@@ -121,7 +121,7 @@
   final static String TERMS_CODEC_NAME = "VERSION_BLOCK_TREE_TERMS_DICT";
 
   /** Initial terms format. */
-  public static final int VERSION_START = 0;
+  public static final int VERSION_START = 1;
 
   /** Current terms format. */
   public static final int VERSION_CURRENT = VERSION_START;
@@ -199,13 +199,13 @@
       fieldInfos = state.fieldInfos;
       this.minItemsInBlock = minItemsInBlock;
       this.maxItemsInBlock = maxItemsInBlock;
-      CodecUtil.writeHeader(out, TERMS_CODEC_NAME, VERSION_CURRENT);   
+      CodecUtil.writeSegmentHeader(out, TERMS_CODEC_NAME, VERSION_CURRENT, state.segmentInfo.getId(), state.segmentSuffix);   
 
       //DEBUG = state.segmentName.equals("_4a");
 
       final String termsIndexFileName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, TERMS_INDEX_EXTENSION);
       indexOut = state.directory.createOutput(termsIndexFileName, state.context);
-      CodecUtil.writeHeader(indexOut, TERMS_INDEX_CODEC_NAME, VERSION_CURRENT); 
+      CodecUtil.writeSegmentHeader(indexOut, TERMS_INDEX_CODEC_NAME, VERSION_CURRENT, state.segmentInfo.getId(), state.segmentSuffix); 
 
       this.postingsWriter = postingsWriter;
       // segment = state.segmentInfo.name;
Index: lucene/test-framework/src/java/org/apache/lucene/codecs/asserting/AssertingDocValuesFormat.java
===================================================================
--- lucene/test-framework/src/java/org/apache/lucene/codecs/asserting/AssertingDocValuesFormat.java	(revision 1629405)
+++ lucene/test-framework/src/java/org/apache/lucene/codecs/asserting/AssertingDocValuesFormat.java	(working copy)
@@ -301,6 +301,11 @@
     }
     
     @Override
+    public DocValuesProducer getMergeInstance() throws IOException {
+      return new AssertingDocValuesProducer(in.getMergeInstance(), maxDoc);
+    }
+
+    @Override
     public String toString() {
       return getClass().getSimpleName() + "(" + in.toString() + ")";
     }
Index: lucene/test-framework/src/java/org/apache/lucene/codecs/asserting/AssertingNormsFormat.java
===================================================================
--- lucene/test-framework/src/java/org/apache/lucene/codecs/asserting/AssertingNormsFormat.java	(revision 1629405)
+++ lucene/test-framework/src/java/org/apache/lucene/codecs/asserting/AssertingNormsFormat.java	(working copy)
@@ -124,6 +124,11 @@
     }
     
     @Override
+    public NormsProducer getMergeInstance() throws IOException {
+      return new AssertingNormsProducer(in.getMergeInstance(), maxDoc);
+    }
+    
+    @Override
     public String toString() {
       return getClass().getSimpleName() + "(" + in.toString() + ")";
     }
Index: lucene/test-framework/src/java/org/apache/lucene/codecs/asserting/AssertingPostingsFormat.java
===================================================================
--- lucene/test-framework/src/java/org/apache/lucene/codecs/asserting/AssertingPostingsFormat.java	(revision 1629405)
+++ lucene/test-framework/src/java/org/apache/lucene/codecs/asserting/AssertingPostingsFormat.java	(working copy)
@@ -111,6 +111,11 @@
     }
     
     @Override
+    public FieldsProducer getMergeInstance() throws IOException {
+      return new AssertingFieldsProducer(in.getMergeInstance());
+    }
+
+    @Override
     public String toString() {
       return getClass().getSimpleName() + "(" + in.toString() + ")";
     }
Index: lucene/test-framework/src/java/org/apache/lucene/codecs/asserting/AssertingStoredFieldsFormat.java
===================================================================
--- lucene/test-framework/src/java/org/apache/lucene/codecs/asserting/AssertingStoredFieldsFormat.java	(revision 1629405)
+++ lucene/test-framework/src/java/org/apache/lucene/codecs/asserting/AssertingStoredFieldsFormat.java	(working copy)
@@ -97,6 +97,11 @@
     }
 
     @Override
+    public StoredFieldsReader getMergeInstance() throws IOException {
+      return new AssertingStoredFieldsReader(in.getMergeInstance(), maxDoc);
+    }
+
+    @Override
     public String toString() {
       return getClass().getSimpleName() + "(" + in.toString() + ")";
     }
Index: lucene/test-framework/src/java/org/apache/lucene/codecs/asserting/AssertingTermVectorsFormat.java
===================================================================
--- lucene/test-framework/src/java/org/apache/lucene/codecs/asserting/AssertingTermVectorsFormat.java	(revision 1629405)
+++ lucene/test-framework/src/java/org/apache/lucene/codecs/asserting/AssertingTermVectorsFormat.java	(working copy)
@@ -96,6 +96,11 @@
     }
     
     @Override
+    public TermVectorsReader getMergeInstance() throws IOException {
+      return new AssertingTermVectorsReader(in.getMergeInstance());
+    }
+
+    @Override
     public String toString() {
       return getClass().getSimpleName() + "(" + in.toString() + ")";
     }
Index: lucene/test-framework/src/java/org/apache/lucene/codecs/cranky/CrankyCodec.java
===================================================================
--- lucene/test-framework/src/java/org/apache/lucene/codecs/cranky/CrankyCodec.java	(revision 1629405)
+++ lucene/test-framework/src/java/org/apache/lucene/codecs/cranky/CrankyCodec.java	(working copy)
@@ -20,6 +20,7 @@
 import java.util.Random;
 
 import org.apache.lucene.codecs.Codec;
+import org.apache.lucene.codecs.CompoundFormat;
 import org.apache.lucene.codecs.DocValuesFormat;
 import org.apache.lucene.codecs.FieldInfosFormat;
 import org.apache.lucene.codecs.FilterCodec;
@@ -86,6 +87,11 @@
   }
 
   @Override
+  public CompoundFormat compoundFormat() {
+    return new CrankyCompoundFormat(delegate.compoundFormat(), random);
+  }
+
+  @Override
   public String toString() {
     return "Cranky(" + delegate + ")";
   }
Index: lucene/test-framework/src/java/org/apache/lucene/codecs/cranky/CrankyCompoundFormat.java
===================================================================
--- lucene/test-framework/src/java/org/apache/lucene/codecs/cranky/CrankyCompoundFormat.java	(revision 0)
+++ lucene/test-framework/src/java/org/apache/lucene/codecs/cranky/CrankyCompoundFormat.java	(working copy)
@@ -0,0 +1,56 @@
+package org.apache.lucene.codecs.cranky;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Collection;
+import java.util.Random;
+
+import org.apache.lucene.codecs.CompoundFormat;
+import org.apache.lucene.index.MergeState.CheckAbort;
+import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+
+class CrankyCompoundFormat extends CompoundFormat {
+  CompoundFormat delegate;
+  Random random;
+  
+  CrankyCompoundFormat(CompoundFormat delegate, Random random) {
+    this.delegate = delegate;
+    this.random = random;
+  }
+  
+  @Override
+  public Directory getCompoundReader(Directory dir, SegmentInfo si, IOContext context) throws IOException {
+    return delegate.getCompoundReader(dir, si, context);
+  }
+  
+  @Override
+  public void write(Directory dir, SegmentInfo si, Collection<String> files, CheckAbort checkAbort, IOContext context) throws IOException {
+    if (random.nextInt(100) == 0) {
+      throw new IOException("Fake IOException from CompoundFormat.write()");
+    }
+    delegate.write(dir, si, files, checkAbort, context);
+  }
+  
+  @Override
+  public String[] files(SegmentInfo si) {
+    return delegate.files(si);
+  }
+}

Property changes on: lucene/test-framework/src/java/org/apache/lucene/codecs/cranky/CrankyCompoundFormat.java
___________________________________________________________________
Added: svn:eol-style
## -0,0 +1 ##
+native
\ No newline at end of property
Index: lucene/test-framework/src/java/org/apache/lucene/codecs/lucene41ords/Lucene41WithOrds.java
===================================================================
--- lucene/test-framework/src/java/org/apache/lucene/codecs/lucene41ords/Lucene41WithOrds.java	(revision 1629405)
+++ lucene/test-framework/src/java/org/apache/lucene/codecs/lucene41ords/Lucene41WithOrds.java	(working copy)
@@ -35,7 +35,6 @@
 import org.apache.lucene.codecs.lucene41.Lucene41PostingsWriter;
 import org.apache.lucene.index.SegmentReadState;
 import org.apache.lucene.index.SegmentWriteState;
-import org.apache.lucene.util.BytesRef;
 
 // TODO: we could make separate base class that can wrapp
 // any PostingsBaseFormat and make it ord-able...
@@ -100,11 +99,7 @@
 
     boolean success = false;
     try {
-      indexReader = new FixedGapTermsIndexReader(state.directory,
-                                                 state.fieldInfos,
-                                                 state.segmentInfo.name,
-                                                 BytesRef.getUTF8SortedAsUnicodeComparator(),
-                                                 state.segmentSuffix, state.context);
+      indexReader = new FixedGapTermsIndexReader(state);
       success = true;
     } finally {
       if (!success) {
@@ -114,13 +109,7 @@
 
     success = false;
     try {
-      FieldsProducer ret = new BlockTermsReader(indexReader,
-                                                state.directory,
-                                                state.fieldInfos,
-                                                state.segmentInfo,
-                                                postings,
-                                                state.context,
-                                                state.segmentSuffix);
+      FieldsProducer ret = new BlockTermsReader(indexReader, postings, state);
       success = true;
       return ret;
     } finally {
Index: lucene/test-framework/src/java/org/apache/lucene/codecs/lucene41vargap/Lucene41VarGapDocFreqInterval.java
===================================================================
--- lucene/test-framework/src/java/org/apache/lucene/codecs/lucene41vargap/Lucene41VarGapDocFreqInterval.java	(revision 1629405)
+++ lucene/test-framework/src/java/org/apache/lucene/codecs/lucene41vargap/Lucene41VarGapDocFreqInterval.java	(working copy)
@@ -103,10 +103,7 @@
 
     boolean success = false;
     try {
-      indexReader = new VariableGapTermsIndexReader(state.directory,
-                                                 state.fieldInfos,
-                                                 state.segmentInfo.name,
-                                                 state.segmentSuffix, state.context);
+      indexReader = new VariableGapTermsIndexReader(state);
       success = true;
     } finally {
       if (!success) {
@@ -116,13 +113,7 @@
 
     success = false;
     try {
-      FieldsProducer ret = new BlockTermsReader(indexReader,
-                                                state.directory,
-                                                state.fieldInfos,
-                                                state.segmentInfo,
-                                                postings,
-                                                state.context,
-                                                state.segmentSuffix);
+      FieldsProducer ret = new BlockTermsReader(indexReader, postings, state);
       success = true;
       return ret;
     } finally {
Index: lucene/test-framework/src/java/org/apache/lucene/codecs/lucene41vargap/Lucene41VarGapFixedInterval.java
===================================================================
--- lucene/test-framework/src/java/org/apache/lucene/codecs/lucene41vargap/Lucene41VarGapFixedInterval.java	(revision 1629405)
+++ lucene/test-framework/src/java/org/apache/lucene/codecs/lucene41vargap/Lucene41VarGapFixedInterval.java	(working copy)
@@ -100,10 +100,7 @@
 
     boolean success = false;
     try {
-      indexReader = new VariableGapTermsIndexReader(state.directory,
-                                                 state.fieldInfos,
-                                                 state.segmentInfo.name,
-                                                 state.segmentSuffix, state.context);
+      indexReader = new VariableGapTermsIndexReader(state);
       success = true;
     } finally {
       if (!success) {
@@ -113,13 +110,7 @@
 
     success = false;
     try {
-      FieldsProducer ret = new BlockTermsReader(indexReader,
-                                                state.directory,
-                                                state.fieldInfos,
-                                                state.segmentInfo,
-                                                postings,
-                                                state.context,
-                                                state.segmentSuffix);
+      FieldsProducer ret = new BlockTermsReader(indexReader, postings, state);
       success = true;
       return ret;
     } finally {
Index: lucene/test-framework/src/java/org/apache/lucene/codecs/mockrandom/MockRandomPostingsFormat.java
===================================================================
--- lucene/test-framework/src/java/org/apache/lucene/codecs/mockrandom/MockRandomPostingsFormat.java	(revision 1629405)
+++ lucene/test-framework/src/java/org/apache/lucene/codecs/mockrandom/MockRandomPostingsFormat.java	(working copy)
@@ -340,11 +340,7 @@
           if (LuceneTestCase.VERBOSE) {
             System.out.println("MockRandomCodec: fixed-gap terms index");
           }
-          indexReader = new FixedGapTermsIndexReader(state.directory,
-                                                     state.fieldInfos,
-                                                     state.segmentInfo.name,
-                                                     BytesRef.getUTF8SortedAsUnicodeComparator(),
-                                                     state.segmentSuffix, state.context);
+          indexReader = new FixedGapTermsIndexReader(state);
         } else {
           final int n2 = random.nextInt(3);
           if (n2 == 1) {
@@ -355,10 +351,7 @@
           if (LuceneTestCase.VERBOSE) {
             System.out.println("MockRandomCodec: variable-gap terms index");
           }
-          indexReader = new VariableGapTermsIndexReader(state.directory,
-                                                        state.fieldInfos,
-                                                        state.segmentInfo.name,
-                                                        state.segmentSuffix, state.context);
+          indexReader = new VariableGapTermsIndexReader(state);
 
         }
 
@@ -371,13 +364,7 @@
 
       success = false;
       try {
-        fields = new BlockTermsReader(indexReader,
-                                      state.directory,
-                                      state.fieldInfos,
-                                      state.segmentInfo,
-                                      postingsReader,
-                                      state.context,
-                                      state.segmentSuffix);
+        fields = new BlockTermsReader(indexReader, postingsReader, state);
         success = true;
       } finally {
         if (!success) {
@@ -396,12 +383,7 @@
 
       boolean success = false;
       try {
-        fields = new OrdsBlockTreeTermsReader(state.directory,
-                                              state.fieldInfos,
-                                              state.segmentInfo,
-                                              postingsReader,
-                                              state.context,
-                                              state.segmentSuffix);
+        fields = new OrdsBlockTreeTermsReader(postingsReader, state);
         success = true;
       } finally {
         if (!success) {
Index: lucene/test-framework/src/java/org/apache/lucene/index/BaseCompoundFormatTestCase.java
===================================================================
--- lucene/test-framework/src/java/org/apache/lucene/index/BaseCompoundFormatTestCase.java	(revision 0)
+++ lucene/test-framework/src/java/org/apache/lucene/index/BaseCompoundFormatTestCase.java	(working copy)
@@ -0,0 +1,787 @@
+package org.apache.lucene.index;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.Collections;
+import java.util.List;
+
+import org.apache.lucene.codecs.Codec;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.document.StoredField;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.FilterDirectory;
+import org.apache.lucene.store.FlushInfo;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.store.MockDirectoryWrapper;
+import org.apache.lucene.store.NRTCachingDirectory;
+import org.apache.lucene.util.StringHelper;
+import org.apache.lucene.util.TestUtil;
+import org.apache.lucene.util.Version;
+
+/**
+ * Abstract class to do basic tests for a compound format.
+ * NOTE: This test focuses on the compound impl, nothing else.
+ * The [stretch] goal is for this test to be
+ * so thorough in testing a new CompoundFormat that if this
+ * test passes, then all Lucene/Solr tests should also pass.  Ie,
+ * if there is some bug in a given CompoundFormat that this
+ * test fails to catch then this test needs to be improved! */
+public abstract class BaseCompoundFormatTestCase extends BaseIndexFileFormatTestCase {
+    
+  // test that empty CFS is empty
+  public void testEmpty() throws IOException {
+    Directory dir = newDirectory();
+    
+    SegmentInfo si = newSegmentInfo(dir, "_123");
+    si.getCodec().compoundFormat().write(dir, si, Collections.<String>emptyList(), MergeState.CheckAbort.NONE, IOContext.DEFAULT);
+    Directory cfs = si.getCodec().compoundFormat().getCompoundReader(dir, si, IOContext.DEFAULT);
+    assertEquals(0, cfs.listAll().length);
+    cfs.close();
+    dir.close();
+  }
+  
+  /** 
+   * This test creates compound file based on a single file.
+   * Files of different sizes are tested: 0, 1, 10, 100 bytes.
+   */
+  public void testSingleFile() throws IOException {
+    int data[] = new int[] { 0, 1, 10, 100 };
+    for (int i=0; i<data.length; i++) {
+      String testfile = "_" + i + ".test";
+      Directory dir = newDirectory();
+      createSequenceFile(dir, testfile, (byte) 0, data[i]);
+      
+      SegmentInfo si = newSegmentInfo(dir, "_" + i);
+      si.getCodec().compoundFormat().write(dir, si, Collections.singleton(testfile), MergeState.CheckAbort.NONE, IOContext.DEFAULT);
+      Directory cfs = si.getCodec().compoundFormat().getCompoundReader(dir, si, IOContext.DEFAULT);
+      
+      IndexInput expected = dir.openInput(testfile, newIOContext(random()));
+      IndexInput actual = cfs.openInput(testfile, newIOContext(random()));
+      assertSameStreams(testfile, expected, actual);
+      assertSameSeekBehavior(testfile, expected, actual);
+      expected.close();
+      actual.close();
+      cfs.close();
+      dir.close();
+    }
+  }
+  
+  /** 
+   * This test creates compound file based on two files.
+   */
+  public void testTwoFiles() throws IOException {
+    String files[] = { "_123.d1", "_123.d2" };
+    Directory dir = newDirectory();
+    createSequenceFile(dir, files[0], (byte) 0, 15);
+    createSequenceFile(dir, files[1], (byte) 0, 114);
+    
+    SegmentInfo si = newSegmentInfo(dir, "_123");
+    si.getCodec().compoundFormat().write(dir, si, Arrays.asList(files), MergeState.CheckAbort.NONE, IOContext.DEFAULT);
+    Directory cfs = si.getCodec().compoundFormat().getCompoundReader(dir, si, IOContext.DEFAULT);
+
+    for (String file : files) {
+      IndexInput expected = dir.openInput(file, newIOContext(random()));
+      IndexInput actual = cfs.openInput(file, newIOContext(random()));
+      assertSameStreams(file, expected, actual);
+      assertSameSeekBehavior(file, expected, actual);
+      expected.close();
+      actual.close();
+    }
+
+    cfs.close();
+    dir.close();
+  }
+  
+  // test that a second call to close() behaves according to Closeable
+  public void testDoubleClose() throws IOException {
+    final String testfile = "_123.test";
+
+    Directory dir = newDirectory();
+    IndexOutput out = dir.createOutput(testfile, IOContext.DEFAULT);
+    out.writeInt(3);
+    out.close();
+    
+    SegmentInfo si = newSegmentInfo(dir, "_123");
+    si.getCodec().compoundFormat().write(dir, si, Collections.singleton(testfile), MergeState.CheckAbort.NONE, IOContext.DEFAULT);
+    Directory cfs = si.getCodec().compoundFormat().getCompoundReader(dir, si, IOContext.DEFAULT);
+    assertEquals(1, cfs.listAll().length);
+    cfs.close();
+    cfs.close(); // second close should not throw exception
+    dir.close();
+  }
+  
+  // LUCENE-5724: things like NRTCachingDir rely upon IOContext being properly passed down
+  public void testPassIOContext() throws IOException {
+    final String testfile = "_123.test";
+    final IOContext myContext = new IOContext();
+
+    Directory dir = new FilterDirectory(newDirectory()) {
+      @Override
+      public IndexOutput createOutput(String name, IOContext context) throws IOException {
+        assertSame(myContext, context);
+        return super.createOutput(name, context);
+      }
+    };
+    IndexOutput out = dir.createOutput(testfile, myContext);
+    out.writeInt(3);
+    out.close();
+    
+    SegmentInfo si = newSegmentInfo(dir, "_123");
+    si.getCodec().compoundFormat().write(dir, si, Collections.singleton(testfile), MergeState.CheckAbort.NONE, myContext);
+    dir.close();
+  }
+  
+  // LUCENE-5724: actually test we play nice with NRTCachingDir and massive file
+  public void testLargeCFS() throws IOException {   
+    final String testfile = "_123.test";
+    IOContext context = new IOContext(new FlushInfo(0, 512*1024*1024));
+
+    Directory dir = new NRTCachingDirectory(newFSDirectory(createTempDir()), 2.0, 25.0);
+
+    IndexOutput out = dir.createOutput(testfile, context);
+    byte[] bytes = new byte[512];
+    for(int i=0;i<1024*1024;i++) {
+      out.writeBytes(bytes, 0, bytes.length);
+    }
+    out.close();
+    
+    SegmentInfo si = newSegmentInfo(dir, "_123");
+    si.getCodec().compoundFormat().write(dir, si, Collections.singleton(testfile), MergeState.CheckAbort.NONE, context);
+
+    dir.close();
+  }
+  
+  // Just tests that we can open all files returned by listAll
+  public void testListAll() throws Exception {
+    Directory dir = newDirectory();
+    if (dir instanceof MockDirectoryWrapper) {
+      // test lists files manually and tries to verify every .cfs it finds,
+      // but a virus scanner could leave some trash.
+      ((MockDirectoryWrapper)dir).setEnableVirusScanner(false);
+    }
+    // riw should sometimes create docvalues fields, etc
+    RandomIndexWriter riw = new RandomIndexWriter(random(), dir);
+    Document doc = new Document();
+    // these fields should sometimes get term vectors, etc
+    Field idField = newStringField("id", "", Field.Store.NO);
+    Field bodyField = newTextField("body", "", Field.Store.NO);
+    doc.add(idField);
+    doc.add(bodyField);
+    for (int i = 0; i < 100; i++) {
+      idField.setStringValue(Integer.toString(i));
+      bodyField.setStringValue(TestUtil.randomUnicodeString(random()));
+      riw.addDocument(doc);
+      if (random().nextInt(7) == 0) {
+        riw.commit();
+      }
+    }
+    riw.close();
+    SegmentInfos infos = new SegmentInfos();
+    infos.read(dir);
+    for (SegmentCommitInfo si : infos) {
+      if (si.info.getUseCompoundFile()) {
+        try (Directory cfsDir = si.info.getCodec().compoundFormat().getCompoundReader(dir, si.info, newIOContext(random()))) {
+          for (String cfsFile : cfsDir.listAll()) {
+            try (IndexInput cfsIn = cfsDir.openInput(cfsFile, IOContext.DEFAULT)) {}
+          }
+        }
+      }
+    }
+    dir.close();
+  }
+  
+  // test that cfs reader is read-only
+  public void testCreateOutputDisabled() throws IOException {
+    Directory dir = newDirectory();
+    
+    SegmentInfo si = newSegmentInfo(dir, "_123");
+    si.getCodec().compoundFormat().write(dir, si, Collections.<String>emptyList(), MergeState.CheckAbort.NONE, IOContext.DEFAULT);
+    Directory cfs = si.getCodec().compoundFormat().getCompoundReader(dir, si, IOContext.DEFAULT);
+    try {
+      cfs.createOutput("bogus", IOContext.DEFAULT);
+      fail("didn't get expected exception");
+    } catch (UnsupportedOperationException expected) {
+      // expected UOE
+    }
+    cfs.close();
+    dir.close();
+  }
+  
+  // test that cfs reader is read-only
+  public void testDeleteFileDisabled() throws IOException {
+    final String testfile = "_123.test";
+
+    Directory dir = newDirectory();
+    IndexOutput out = dir.createOutput(testfile, IOContext.DEFAULT);
+    out.writeInt(3);
+    out.close();
+ 
+    SegmentInfo si = newSegmentInfo(dir, "_123");
+    si.getCodec().compoundFormat().write(dir, si, Collections.<String>emptyList(), MergeState.CheckAbort.NONE, IOContext.DEFAULT);
+    Directory cfs = si.getCodec().compoundFormat().getCompoundReader(dir, si, IOContext.DEFAULT);
+    try {
+      cfs.deleteFile(testfile);
+      fail("didn't get expected exception");
+    } catch (UnsupportedOperationException expected) {
+      // expected UOE
+    }
+    cfs.close();
+    dir.close();
+  }
+  
+  // test that cfs reader is read-only
+  public void testRenameFileDisabled() throws IOException {
+    final String testfile = "_123.test";
+
+    Directory dir = newDirectory();
+    IndexOutput out = dir.createOutput(testfile, IOContext.DEFAULT);
+    out.writeInt(3);
+    out.close();
+ 
+    SegmentInfo si = newSegmentInfo(dir, "_123");
+    si.getCodec().compoundFormat().write(dir, si, Collections.<String>emptyList(), MergeState.CheckAbort.NONE, IOContext.DEFAULT);
+    Directory cfs = si.getCodec().compoundFormat().getCompoundReader(dir, si, IOContext.DEFAULT);
+    try {
+      cfs.renameFile(testfile, "bogus");
+      fail("didn't get expected exception");
+    } catch (UnsupportedOperationException expected) {
+      // expected UOE
+    }
+    cfs.close();
+    dir.close();
+  }
+  
+  // test that cfs reader is read-only
+  public void testSyncDisabled() throws IOException {
+    final String testfile = "_123.test";
+
+    Directory dir = newDirectory();
+    IndexOutput out = dir.createOutput(testfile, IOContext.DEFAULT);
+    out.writeInt(3);
+    out.close();
+ 
+    SegmentInfo si = newSegmentInfo(dir, "_123");
+    si.getCodec().compoundFormat().write(dir, si, Collections.<String>emptyList(), MergeState.CheckAbort.NONE, IOContext.DEFAULT);
+    Directory cfs = si.getCodec().compoundFormat().getCompoundReader(dir, si, IOContext.DEFAULT);
+    try {
+      cfs.sync(Collections.singleton(testfile));
+      fail("didn't get expected exception");
+    } catch (UnsupportedOperationException expected) {
+      // expected UOE
+    }
+    cfs.close();
+    dir.close();
+  }
+  
+  // test that cfs reader is read-only
+  public void testMakeLockDisabled() throws IOException {
+    final String testfile = "_123.test";
+
+    Directory dir = newDirectory();
+    IndexOutput out = dir.createOutput(testfile, IOContext.DEFAULT);
+    out.writeInt(3);
+    out.close();
+ 
+    SegmentInfo si = newSegmentInfo(dir, "_123");
+    si.getCodec().compoundFormat().write(dir, si, Collections.<String>emptyList(), MergeState.CheckAbort.NONE, IOContext.DEFAULT);
+    Directory cfs = si.getCodec().compoundFormat().getCompoundReader(dir, si, IOContext.DEFAULT);
+    try {
+      cfs.makeLock("foobar");
+      fail("didn't get expected exception");
+    } catch (UnsupportedOperationException expected) {
+      // expected UOE
+    }
+    cfs.close();
+    dir.close();
+  }
+  
+  // test that cfs reader is read-only
+  public void testClearLockDisabled() throws IOException {
+    final String testfile = "_123.test";
+
+    Directory dir = newDirectory();
+    IndexOutput out = dir.createOutput(testfile, IOContext.DEFAULT);
+    out.writeInt(3);
+    out.close();
+ 
+    SegmentInfo si = newSegmentInfo(dir, "_123");
+    si.getCodec().compoundFormat().write(dir, si, Collections.<String>emptyList(), MergeState.CheckAbort.NONE, IOContext.DEFAULT);
+    Directory cfs = si.getCodec().compoundFormat().getCompoundReader(dir, si, IOContext.DEFAULT);
+    try {
+      cfs.clearLock("foobar");
+      fail("didn't get expected exception");
+    } catch (UnsupportedOperationException expected) {
+      // expected UOE
+    }
+    cfs.close();
+    dir.close();
+  }
+  
+  /** 
+   * This test creates a compound file based on a large number of files of
+   * various length. The file content is generated randomly. The sizes range
+   * from 0 to 1Mb. Some of the sizes are selected to test the buffering
+   * logic in the file reading code. For this the chunk variable is set to
+   * the length of the buffer used internally by the compound file logic.
+   */
+  public void testRandomFiles() throws IOException {
+    Directory dir = newDirectory();
+    // Setup the test segment
+    String segment = "_123";
+    int chunk = 1024; // internal buffer size used by the stream
+    createRandomFile(dir, segment + ".zero", 0);
+    createRandomFile(dir, segment + ".one", 1);
+    createRandomFile(dir, segment + ".ten", 10);
+    createRandomFile(dir, segment + ".hundred", 100);
+    createRandomFile(dir, segment + ".big1", chunk);
+    createRandomFile(dir, segment + ".big2", chunk - 1);
+    createRandomFile(dir, segment + ".big3", chunk + 1);
+    createRandomFile(dir, segment + ".big4", 3 * chunk);
+    createRandomFile(dir, segment + ".big5", 3 * chunk - 1);
+    createRandomFile(dir, segment + ".big6", 3 * chunk + 1);
+    createRandomFile(dir, segment + ".big7", 1000 * chunk);
+    
+    String files[] = dir.listAll();
+    
+    SegmentInfo si = newSegmentInfo(dir, "_123");
+    si.getCodec().compoundFormat().write(dir, si, Arrays.asList(files), MergeState.CheckAbort.NONE, IOContext.DEFAULT);
+    Directory cfs = si.getCodec().compoundFormat().getCompoundReader(dir, si, IOContext.DEFAULT);
+    
+    for (int i = 0; i < files.length; i++) {
+      IndexInput check = dir.openInput(files[i], newIOContext(random()));
+      IndexInput test = cfs.openInput(files[i], newIOContext(random()));
+      assertSameStreams(files[i], check, test);
+      assertSameSeekBehavior(files[i], check, test);
+      test.close();
+      check.close();
+    }
+    cfs.close();
+    dir.close();
+  }
+  
+  // Make sure we don't somehow use more than 1 descriptor
+  // when reading a CFS with many subs:
+  public void testManySubFiles() throws IOException {
+    final MockDirectoryWrapper dir = newMockFSDirectory(createTempDir("CFSManySubFiles"));
+    
+    final int FILE_COUNT = atLeast(500);
+    
+    for (int fileIdx = 0; fileIdx < FILE_COUNT; fileIdx++) {
+      IndexOutput out = dir.createOutput("_123." + fileIdx, newIOContext(random()));
+      out.writeByte((byte) fileIdx);
+      out.close();
+    }
+    
+    assertEquals(0, dir.getFileHandleCount());
+    
+    SegmentInfo si = newSegmentInfo(dir, "_123");
+    si.getCodec().compoundFormat().write(dir, si, Arrays.asList(dir.listAll()), MergeState.CheckAbort.NONE, IOContext.DEFAULT);
+    Directory cfs = si.getCodec().compoundFormat().getCompoundReader(dir, si, IOContext.DEFAULT);
+    
+    final IndexInput[] ins = new IndexInput[FILE_COUNT];
+    for (int fileIdx = 0; fileIdx < FILE_COUNT; fileIdx++) {
+      ins[fileIdx] = cfs.openInput("_123." + fileIdx, newIOContext(random()));
+    }
+    
+    assertEquals(1, dir.getFileHandleCount());
+
+    for (int fileIdx = 0; fileIdx < FILE_COUNT; fileIdx++) {
+      assertEquals((byte) fileIdx, ins[fileIdx].readByte());
+    }
+    
+    assertEquals(1, dir.getFileHandleCount());
+    
+    for(int fileIdx=0;fileIdx<FILE_COUNT;fileIdx++) {
+      ins[fileIdx].close();
+    }
+    cfs.close();
+    
+    dir.close();
+  }
+  
+  public void testClonedStreamsClosing() throws IOException {
+    Directory dir = newDirectory();
+    Directory cr = createLargeCFS(dir);
+    
+    // basic clone
+    IndexInput expected = dir.openInput("_123.f11", newIOContext(random()));
+    
+    IndexInput one = cr.openInput("_123.f11", newIOContext(random()));
+    
+    IndexInput two = one.clone();
+    
+    assertSameStreams("basic clone one", expected, one);
+    expected.seek(0);
+    assertSameStreams("basic clone two", expected, two);
+    
+    // Now close the first stream
+    one.close();
+    
+    // The following should really fail since we couldn't expect to
+    // access a file once close has been called on it (regardless of
+    // buffering and/or clone magic)
+    expected.seek(0);
+    two.seek(0);
+    assertSameStreams("basic clone two/2", expected, two);
+    
+    // Now close the compound reader
+    cr.close();
+    
+    // The following may also fail since the compound stream is closed
+    expected.seek(0);
+    two.seek(0);
+    //assertSameStreams("basic clone two/3", expected, two);
+    
+    // Now close the second clone
+    two.close();
+    expected.seek(0);
+    //assertSameStreams("basic clone two/4", expected, two);
+    
+    expected.close();
+    dir.close();
+  }
+  
+  /** This test opens two files from a compound stream and verifies that
+   *  their file positions are independent of each other.
+   */
+  public void testRandomAccess() throws IOException {
+    Directory dir = newDirectory();
+    Directory cr = createLargeCFS(dir);
+    
+    // Open two files
+    IndexInput e1 = dir.openInput("_123.f11", newIOContext(random()));
+    IndexInput e2 = dir.openInput("_123.f3", newIOContext(random()));
+    
+    IndexInput a1 = cr.openInput("_123.f11", newIOContext(random()));
+    IndexInput a2 = dir.openInput("_123.f3", newIOContext(random()));
+    
+    // Seek the first pair
+    e1.seek(100);
+    a1.seek(100);
+    assertEquals(100, e1.getFilePointer());
+    assertEquals(100, a1.getFilePointer());
+    byte be1 = e1.readByte();
+    byte ba1 = a1.readByte();
+    assertEquals(be1, ba1);
+    
+    // Now seek the second pair
+    e2.seek(1027);
+    a2.seek(1027);
+    assertEquals(1027, e2.getFilePointer());
+    assertEquals(1027, a2.getFilePointer());
+    byte be2 = e2.readByte();
+    byte ba2 = a2.readByte();
+    assertEquals(be2, ba2);
+    
+    // Now make sure the first one didn't move
+    assertEquals(101, e1.getFilePointer());
+    assertEquals(101, a1.getFilePointer());
+    be1 = e1.readByte();
+    ba1 = a1.readByte();
+    assertEquals(be1, ba1);
+    
+    // Now more the first one again, past the buffer length
+    e1.seek(1910);
+    a1.seek(1910);
+    assertEquals(1910, e1.getFilePointer());
+    assertEquals(1910, a1.getFilePointer());
+    be1 = e1.readByte();
+    ba1 = a1.readByte();
+    assertEquals(be1, ba1);
+    
+    // Now make sure the second set didn't move
+    assertEquals(1028, e2.getFilePointer());
+    assertEquals(1028, a2.getFilePointer());
+    be2 = e2.readByte();
+    ba2 = a2.readByte();
+    assertEquals(be2, ba2);
+    
+    // Move the second set back, again cross the buffer size
+    e2.seek(17);
+    a2.seek(17);
+    assertEquals(17, e2.getFilePointer());
+    assertEquals(17, a2.getFilePointer());
+    be2 = e2.readByte();
+    ba2 = a2.readByte();
+    assertEquals(be2, ba2);
+    
+    // Finally, make sure the first set didn't move
+    // Now make sure the first one didn't move
+    assertEquals(1911, e1.getFilePointer());
+    assertEquals(1911, a1.getFilePointer());
+    be1 = e1.readByte();
+    ba1 = a1.readByte();
+    assertEquals(be1, ba1);
+    
+    e1.close();
+    e2.close();
+    a1.close();
+    a2.close();
+    cr.close();
+    dir.close();
+  }
+  
+  /** This test opens two files from a compound stream and verifies that
+   *  their file positions are independent of each other.
+   */
+  public void testRandomAccessClones() throws IOException {
+    Directory dir = newDirectory();
+    Directory cr = createLargeCFS(dir);
+    
+    // Open two files
+    IndexInput e1 = cr.openInput("_123.f11", newIOContext(random()));
+    IndexInput e2 = cr.openInput("_123.f3", newIOContext(random()));
+    
+    IndexInput a1 = e1.clone();
+    IndexInput a2 = e2.clone();
+    
+    // Seek the first pair
+    e1.seek(100);
+    a1.seek(100);
+    assertEquals(100, e1.getFilePointer());
+    assertEquals(100, a1.getFilePointer());
+    byte be1 = e1.readByte();
+    byte ba1 = a1.readByte();
+    assertEquals(be1, ba1);
+    
+    // Now seek the second pair
+    e2.seek(1027);
+    a2.seek(1027);
+    assertEquals(1027, e2.getFilePointer());
+    assertEquals(1027, a2.getFilePointer());
+    byte be2 = e2.readByte();
+    byte ba2 = a2.readByte();
+    assertEquals(be2, ba2);
+    
+    // Now make sure the first one didn't move
+    assertEquals(101, e1.getFilePointer());
+    assertEquals(101, a1.getFilePointer());
+    be1 = e1.readByte();
+    ba1 = a1.readByte();
+    assertEquals(be1, ba1);
+    
+    // Now more the first one again, past the buffer length
+    e1.seek(1910);
+    a1.seek(1910);
+    assertEquals(1910, e1.getFilePointer());
+    assertEquals(1910, a1.getFilePointer());
+    be1 = e1.readByte();
+    ba1 = a1.readByte();
+    assertEquals(be1, ba1);
+    
+    // Now make sure the second set didn't move
+    assertEquals(1028, e2.getFilePointer());
+    assertEquals(1028, a2.getFilePointer());
+    be2 = e2.readByte();
+    ba2 = a2.readByte();
+    assertEquals(be2, ba2);
+    
+    // Move the second set back, again cross the buffer size
+    e2.seek(17);
+    a2.seek(17);
+    assertEquals(17, e2.getFilePointer());
+    assertEquals(17, a2.getFilePointer());
+    be2 = e2.readByte();
+    ba2 = a2.readByte();
+    assertEquals(be2, ba2);
+    
+    // Finally, make sure the first set didn't move
+    // Now make sure the first one didn't move
+    assertEquals(1911, e1.getFilePointer());
+    assertEquals(1911, a1.getFilePointer());
+    be1 = e1.readByte();
+    ba1 = a1.readByte();
+    assertEquals(be1, ba1);
+    
+    e1.close();
+    e2.close();
+    a1.close();
+    a2.close();
+    cr.close();
+    dir.close();
+  }
+  
+  public void testFileNotFound() throws IOException {
+    Directory dir = newDirectory();
+    Directory cr = createLargeCFS(dir);
+    
+    // Open bogus file
+    try {
+      cr.openInput("bogus", newIOContext(random()));
+      fail("File not found");
+    } catch (IOException e) {
+      /* success */;
+    }
+    
+    cr.close();
+    dir.close();
+  }
+  
+  public void testReadPastEOF() throws IOException {
+    Directory dir = newDirectory();
+    Directory cr = createLargeCFS(dir);
+    IndexInput is = cr.openInput("_123.f2", newIOContext(random()));
+    is.seek(is.length() - 10);
+    byte b[] = new byte[100];
+    is.readBytes(b, 0, 10);
+    
+    try {
+      is.readByte();
+      fail("Single byte read past end of file");
+    } catch (IOException e) {
+      /* success */
+    }
+    
+    is.seek(is.length() - 10);
+    try {
+      is.readBytes(b, 0, 50);
+      fail("Block read past end of file");
+    } catch (IOException e) {
+      /* success */
+    }
+    
+    is.close();
+    cr.close();
+    dir.close();
+  }
+  
+  /** Returns a new fake segment */
+  protected static SegmentInfo newSegmentInfo(Directory dir, String name) {
+    return new SegmentInfo(dir, Version.LATEST, name, 10000, false, Codec.getDefault(), null, StringHelper.randomId());
+  }
+  
+  /** Creates a file of the specified size with random data. */
+  protected static void createRandomFile(Directory dir, String name, int size) throws IOException {
+    IndexOutput os = dir.createOutput(name, newIOContext(random()));
+    for (int i=0; i<size; i++) {
+      byte b = (byte) (Math.random() * 256);
+      os.writeByte(b);
+    }
+    os.close();
+  }
+  
+  /** Creates a file of the specified size with sequential data. The first
+   *  byte is written as the start byte provided. All subsequent bytes are
+   *  computed as start + offset where offset is the number of the byte.
+   */
+  protected static void createSequenceFile(Directory dir, String name, byte start, int size) throws IOException {
+    IndexOutput os = dir.createOutput(name, newIOContext(random()));
+    for (int i=0; i < size; i++) {
+      os.writeByte(start);
+      start ++;
+    }
+    os.close();
+  }
+  
+  protected static void assertSameStreams(String msg, IndexInput expected, IndexInput test) throws IOException {
+    assertNotNull(msg + " null expected", expected);
+    assertNotNull(msg + " null test", test);
+    assertEquals(msg + " length", expected.length(), test.length());
+    assertEquals(msg + " position", expected.getFilePointer(), test.getFilePointer());
+    
+    byte expectedBuffer[] = new byte[512];
+    byte testBuffer[] = new byte[expectedBuffer.length];
+    
+    long remainder = expected.length() - expected.getFilePointer();
+    while (remainder > 0) {
+      int readLen = (int) Math.min(remainder, expectedBuffer.length);
+      expected.readBytes(expectedBuffer, 0, readLen);
+      test.readBytes(testBuffer, 0, readLen);
+      assertEqualArrays(msg + ", remainder " + remainder, expectedBuffer, testBuffer, 0, readLen);
+      remainder -= readLen;
+    }
+  }
+  
+  protected static void assertSameStreams(String msg, IndexInput expected, IndexInput actual, long seekTo) throws IOException {
+    if (seekTo >= 0 && seekTo < expected.length()) {
+      expected.seek(seekTo);
+      actual.seek(seekTo);
+      assertSameStreams(msg + ", seek(mid)", expected, actual);
+    }
+  }
+  
+  protected static void assertSameSeekBehavior(String msg, IndexInput expected, IndexInput actual) throws IOException {
+    // seek to 0
+    long point = 0;
+    assertSameStreams(msg + ", seek(0)", expected, actual, point);
+    
+    // seek to middle
+    point = expected.length() / 2l;
+    assertSameStreams(msg + ", seek(mid)", expected, actual, point);
+    
+    // seek to end - 2
+    point = expected.length() - 2;
+    assertSameStreams(msg + ", seek(end-2)", expected, actual, point);
+    
+    // seek to end - 1
+    point = expected.length() - 1;
+    assertSameStreams(msg + ", seek(end-1)", expected, actual, point);
+    
+    // seek to the end
+    point = expected.length();
+    assertSameStreams(msg + ", seek(end)", expected, actual, point);
+    
+    // seek past end
+    point = expected.length() + 1;
+    assertSameStreams(msg + ", seek(end+1)", expected, actual, point);
+  }
+  
+  protected static void assertEqualArrays(String msg, byte[] expected, byte[] test, int start, int len) {
+    assertNotNull(msg + " null expected", expected);
+    assertNotNull(msg + " null test", test);
+    
+    for (int i=start; i<len; i++) {
+      assertEquals(msg + " " + i, expected[i], test[i]);
+    }
+  }
+  
+  /** 
+   * Setup a large compound file with a number of components, each of
+   * which is a sequential file (so that we can easily tell that we are
+   * reading in the right byte). The methods sets up 20 files - _123.0 to _123.19,
+   * the size of each file is 1000 bytes.
+   */
+  protected static Directory createLargeCFS(Directory dir) throws IOException {
+    List<String> files = new ArrayList<>();
+    for (int i = 0; i < 20; i++) {
+      createSequenceFile(dir, "_123.f" + i, (byte) 0, 2000);
+      files.add("_123.f" + i);
+    }
+    
+    SegmentInfo si = newSegmentInfo(dir, "_123");
+    si.getCodec().compoundFormat().write(dir, si, files, MergeState.CheckAbort.NONE, IOContext.DEFAULT);
+    Directory cfs = si.getCodec().compoundFormat().getCompoundReader(dir, si, IOContext.DEFAULT);
+    return cfs;
+  }
+
+  @Override
+  protected void addRandomFields(Document doc) {
+    doc.add(new StoredField("foobar", TestUtil.randomSimpleString(random())));
+  }
+
+  @Override
+  public void testMergeStability() throws Exception {
+    assumeTrue("test does not work with CFS", true);
+  }
+}

Property changes on: lucene/test-framework/src/java/org/apache/lucene/index/BaseCompoundFormatTestCase.java
___________________________________________________________________
Added: svn:eol-style
## -0,0 +1 ##
+native
\ No newline at end of property
Index: lucene/test-framework/src/java/org/apache/lucene/index/BaseNormsFormatTestCase.java
===================================================================
--- lucene/test-framework/src/java/org/apache/lucene/index/BaseNormsFormatTestCase.java	(revision 1629405)
+++ lucene/test-framework/src/java/org/apache/lucene/index/BaseNormsFormatTestCase.java	(working copy)
@@ -139,6 +139,19 @@
     }
   }
   
+  public void testSparse() throws Exception {
+    int iterations = atLeast(1);
+    final Random r = random();
+    for (int i = 0; i < iterations; i++) {
+      doTestNormsVersusStoredFields(new LongProducer() {
+        @Override
+        long next() {
+          return r.nextInt(100) == 0 ? TestUtil.nextLong(r, Byte.MIN_VALUE, Byte.MAX_VALUE) : 0;
+        }
+      });
+    }
+  }
+  
   private void doTestNormsVersusStoredFields(LongProducer longs) throws Exception {
     int numDocs = atLeast(500);
     long norms[] = new long[numDocs];
Index: lucene/test-framework/src/java/org/apache/lucene/store/BaseDirectoryTestCase.java
===================================================================
--- lucene/test-framework/src/java/org/apache/lucene/store/BaseDirectoryTestCase.java	(revision 1629405)
+++ lucene/test-framework/src/java/org/apache/lucene/store/BaseDirectoryTestCase.java	(working copy)
@@ -18,7 +18,6 @@
  */
 
 import java.io.EOFException;
-import java.io.File;
 import java.io.FileNotFoundException;
 import java.io.IOException;
 import java.nio.file.Files;
@@ -584,39 +583,6 @@
     dir.close();
   }
 
-  // LUCENE-3382 test that delegate compound files correctly.
-  public void testCompoundFileAppendTwice() throws IOException {
-    Directory newDir = getDirectory(createTempDir("testCompoundFileAppendTwice"));
-    CompoundFileDirectory csw = new CompoundFileDirectory(newDir, "d.cfs", newIOContext(random()), true);
-    createSequenceFile(newDir, "d1", (byte) 0, 15);
-    IndexOutput out = csw.createOutput("d.xyz", newIOContext(random()));
-    out.writeInt(0);
-    out.close();
-    assertEquals(1, csw.listAll().length);
-    assertEquals("d.xyz", csw.listAll()[0]);
-   
-    csw.close();
-
-    CompoundFileDirectory cfr = new CompoundFileDirectory(newDir, "d.cfs", newIOContext(random()), false);
-    assertEquals(1, cfr.listAll().length);
-    assertEquals("d.xyz", cfr.listAll()[0]);
-    cfr.close();
-    newDir.close();
-  }
-
-  /** Creates a file of the specified size with sequential data. The first
-   *  byte is written as the start byte provided. All subsequent bytes are
-   *  computed as start + offset where offset is the number of the byte.
-   */
-  private void createSequenceFile(Directory dir, String name, byte start, int size) throws IOException {
-    IndexOutput os = dir.createOutput(name, newIOContext(random()));
-    for (int i=0; i < size; i++) {
-      os.writeByte(start);
-      start ++;
-    }
-    os.close();
-  }
-
   public void testCopyBytes() throws Exception {
     testCopyBytes(getDirectory(createTempDir("testCopyBytes")));
   }
@@ -1041,5 +1007,29 @@
     input.close();
     dir.close();
   }
+  
+  /** 
+   * This test that writes larger than the size of the buffer output
+   * will correctly increment the file pointer.
+   */
+  public void testLargeWrites() throws IOException {
+    Directory dir = getDirectory(createTempDir("largeWrites"));
+    IndexOutput os = dir.createOutput("testBufferStart.txt", newIOContext(random()));
+    
+    byte[] largeBuf = new byte[2048];
+    for (int i=0; i<largeBuf.length; i++) {
+      largeBuf[i] = (byte) (Math.random() * 256);
+    }
+    
+    long currentPos = os.getFilePointer();
+    os.writeBytes(largeBuf, largeBuf.length);
+    
+    try {
+      assertEquals(currentPos + largeBuf.length, os.getFilePointer());
+    } finally {
+      os.close();
+    }
+    dir.close();
+  }
 }
 
Index: lucene/test-framework/src/java/org/apache/lucene/store/MockDirectoryWrapper.java
===================================================================
--- lucene/test-framework/src/java/org/apache/lucene/store/MockDirectoryWrapper.java	(revision 1629405)
+++ lucene/test-framework/src/java/org/apache/lucene/store/MockDirectoryWrapper.java	(working copy)
@@ -466,6 +466,11 @@
       }
     }
   }
+  
+  /** returns current open file handle count */
+  public synchronized long getFileHandleCount() {
+    return openFileHandles.size();
+  }
 
   @Override
   public synchronized void deleteFile(String name) throws IOException {
Index: lucene/test-framework/src/java/org/apache/lucene/util/LuceneTestCase.java
===================================================================
--- lucene/test-framework/src/java/org/apache/lucene/util/LuceneTestCase.java	(revision 1629405)
+++ lucene/test-framework/src/java/org/apache/lucene/util/LuceneTestCase.java	(working copy)
@@ -867,7 +867,6 @@
     }
     c.setUseCompoundFile(r.nextBoolean());
     c.setReaderPooling(r.nextBoolean());
-    c.setCheckIntegrityAtMerge(r.nextBoolean());
     return c;
   }
 
@@ -1041,12 +1040,6 @@
     }
     
     if (rarely(r)) {
-      // change merge integrity check parameters
-      c.setCheckIntegrityAtMerge(r.nextBoolean());
-      didChange = true;
-    }
-    
-    if (rarely(r)) {
       // change CMS merge parameters
       MergeScheduler ms = c.getMergeScheduler();
       if (ms instanceof ConcurrentMergeScheduler) {
Index: solr/core/src/java/org/apache/solr/update/SolrIndexConfig.java
===================================================================
--- solr/core/src/java/org/apache/solr/update/SolrIndexConfig.java	(revision 1629405)
+++ solr/core/src/java/org/apache/solr/update/SolrIndexConfig.java	(working copy)
@@ -232,8 +232,6 @@
       iwc.setMergedSegmentWarmer(warmer);
     }
 
-    iwc.setCheckIntegrityAtMerge(checkIntegrityAtMerge);
-
     return iwc;
   }
 
Index: solr/core/src/test/org/apache/solr/update/SolrIndexConfigTest.java
===================================================================
--- solr/core/src/test/org/apache/solr/update/SolrIndexConfigTest.java	(revision 1629405)
+++ solr/core/src/test/org/apache/solr/update/SolrIndexConfigTest.java	(working copy)
@@ -78,18 +78,6 @@
 
   }
 
-  @Test
-  public void testCheckIntegrityAtMerge() throws Exception {
-    SolrConfig solrConfig = new SolrConfig("solr" + File.separator
-        + "collection1", "solrconfig-indexconfig.xml", null);
-    SolrIndexConfig solrIndexConfig = new SolrIndexConfig(solrConfig, null, null);
-    assertNotNull(solrIndexConfig.checkIntegrityAtMerge);
-    assertTrue(solrIndexConfig.checkIntegrityAtMerge);
-    IndexSchema indexSchema = IndexSchemaFactory.buildIndexSchema("schema.xml", solrConfig);
-    IndexWriterConfig iwc = solrIndexConfig.toIndexWriterConfig(indexSchema);
-    assertTrue(iwc.getCheckIntegrityAtMerge());
-  }
-
   public void testMergedSegmentWarmerIndexConfigCreation() throws Exception {
     SolrConfig solrConfig = new SolrConfig("solr" + File.separator
         + "collection1", "solrconfig-warmer.xml", null);
