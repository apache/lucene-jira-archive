Index: solr/core/src/java/org/apache/solr/schema/TextField.java
===================================================================
--- solr/core/src/java/org/apache/solr/schema/TextField.java	(revision 1571317)
+++ solr/core/src/java/org/apache/solr/schema/TextField.java	(working copy)
@@ -17,10 +17,8 @@
 
 package org.apache.solr.schema;
 
-import org.apache.lucene.analysis.tokenattributes.TermToBytesRefAttribute;
 import org.apache.lucene.search.*;
 import org.apache.lucene.index.StorableField;
-import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.CharsRef;
@@ -133,25 +131,25 @@
   }
 
   public static BytesRef analyzeMultiTerm(String field, String part, Analyzer analyzerIn) {
+
     if (part == null || analyzerIn == null) return null;
 
-    try (TokenStream source = analyzerIn.tokenStream(field, part)){
-      source.reset();
+    BytesRef analyzed = null;
 
-      TermToBytesRefAttribute termAtt = source.getAttribute(TermToBytesRefAttribute.class);
-      BytesRef bytes = termAtt.getBytesRef();
-
-      if (!source.incrementToken())
+    //analyzerIn is not actually used in initialization
+    QueryBuilder builder = new QueryBuilder(analyzerIn);
+    try{
+      analyzed = builder.analyzeMultitermTerm(field, part, analyzerIn);
+    } catch (IllegalArgumentException e) {
+      if (e.getMessage().indexOf("returned nothing") > -1) {
         throw  new SolrException(SolrException.ErrorCode.BAD_REQUEST,"analyzer returned no terms for multiTerm term: " + part);
-      termAtt.fillBytesRef();
-      if (source.incrementToken())
+      } else {
         throw  new SolrException(SolrException.ErrorCode.BAD_REQUEST,"analyzer returned too many terms for multiTerm term: " + part);
-
-      source.end();
-      return BytesRef.deepCopyOf(bytes);
+      }
     } catch (IOException e) {
       throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,"error analyzing range part: " + part, e);
     }
+    return analyzed;
   }
 
 
Index: lucene/queryparser/src/test/org/apache/lucene/queryparser/analyzing/TestAnalyzingQueryParser.java
===================================================================
--- lucene/queryparser/src/test/org/apache/lucene/queryparser/analyzing/TestAnalyzingQueryParser.java	(revision 1571317)
+++ lucene/queryparser/src/test/org/apache/lucene/queryparser/analyzing/TestAnalyzingQueryParser.java	(working copy)
@@ -122,7 +122,7 @@
      
     AnalyzingQueryParser qp = new AnalyzingQueryParser(TEST_VERSION_CURRENT, FIELD, a);
     try{
-      qp.analyzeSingleChunk(FIELD, "", "not a single chunk");
+      qp.getWildcardQuery(FIELD, "not a single chunk");
     } catch (ParseException e){
       if (e.getMessage().contains("multiple terms")){
         ex = true;
@@ -295,4 +295,4 @@
     }
 
   }
-}
\ No newline at end of file
+}
Index: lucene/queryparser/src/java/org/apache/lucene/queryparser/classic/QueryParserBase.java
===================================================================
--- lucene/queryparser/src/java/org/apache/lucene/queryparser/classic/QueryParserBase.java	(revision 1571317)
+++ lucene/queryparser/src/java/org/apache/lucene/queryparser/classic/QueryParserBase.java	(working copy)
@@ -576,29 +576,15 @@
   }
 
   // TODO: Should this be protected instead?
+  // Can also throw IllegalArgumentException
   private BytesRef analyzeMultitermTerm(String field, String part) {
-    return analyzeMultitermTerm(field, part, getAnalyzer());
-  }
-
-  protected BytesRef analyzeMultitermTerm(String field, String part, Analyzer analyzerIn) {
-    if (analyzerIn == null) analyzerIn = getAnalyzer();
-
-    try (TokenStream source = analyzerIn.tokenStream(field, part)) {
-      source.reset();
-      
-      TermToBytesRefAttribute termAtt = source.getAttribute(TermToBytesRefAttribute.class);
-      BytesRef bytes = termAtt.getBytesRef();
-
-      if (!source.incrementToken())
-        throw new IllegalArgumentException("analyzer returned no terms for multiTerm term: " + part);
-      termAtt.fillBytesRef();
-      if (source.incrementToken())
-        throw new IllegalArgumentException("analyzer returned too many terms for multiTerm term: " + part);
-      source.end();
-      return BytesRef.deepCopyOf(bytes);
+    BytesRef analyzed = null;
+    try{
+      analyzed = analyzeMultitermTerm(field, part, getAnalyzer());
     } catch (IOException e) {
-      throw new RuntimeException("Error analyzing multiTerm term: " + part, e);
+      throw new RuntimeException(e.getMessage());
     }
+    return analyzed;
   }
 
   /**
Index: lucene/queryparser/src/java/org/apache/lucene/queryparser/analyzing/AnalyzingQueryParser.java
===================================================================
--- lucene/queryparser/src/java/org/apache/lucene/queryparser/analyzing/AnalyzingQueryParser.java	(revision 1571317)
+++ lucene/queryparser/src/java/org/apache/lucene/queryparser/analyzing/AnalyzingQueryParser.java	(working copy)
@@ -18,12 +18,8 @@
  */
 
 import java.io.IOException;
-import java.util.regex.Matcher;
-import java.util.regex.Pattern;
 
 import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
 import org.apache.lucene.queryparser.classic.ParseException;
 import org.apache.lucene.search.Query;
 import org.apache.lucene.util.Version;
@@ -40,8 +36,7 @@
  * using this parser will be no improvement over QueryParser in such cases). 
  */
 public class AnalyzingQueryParser extends org.apache.lucene.queryparser.classic.QueryParser {
-  // gobble escaped chars or find a wildcard character 
-  private final Pattern wildcardPattern = Pattern.compile("(\\.)|([?*]+)");
+
   public AnalyzingQueryParser(Version matchVersion, String field, Analyzer analyzer) {
     super(matchVersion, field, analyzer);
     setAnalyzeRangeTerms(true);
@@ -68,7 +63,7 @@
   @Override
   protected Query getWildcardQuery(String field, String termStr) throws ParseException {
 
-    if (termStr == null){
+    if (termStr == null) {
       //can't imagine this would ever happen
       throw new ParseException("Passed null value as term to getWildcardQuery");
     }
@@ -76,31 +71,15 @@
       throw new ParseException("'*' or '?' not allowed as first character in WildcardQuery"
                               + " unless getAllowLeadingWildcard() returns true");
     }
-    
-    Matcher wildcardMatcher = wildcardPattern.matcher(termStr);
-    StringBuilder sb = new StringBuilder();
-    int last = 0;
-  
-    while (wildcardMatcher.find()){
-      // continue if escaped char
-      if (wildcardMatcher.group(1) != null){
-        continue;
-      }
-     
-      if (wildcardMatcher.start() > 0){
-        String chunk = termStr.substring(last, wildcardMatcher.start());
-        String analyzed = analyzeSingleChunk(field, termStr, chunk);
-        sb.append(analyzed);
-      }
-      //append the wildcard character
-      sb.append(wildcardMatcher.group(2));
-     
-      last = wildcardMatcher.end();
+    String analyzed = null;
+    try{
+      analyzed = analyzeWildcard(field, termStr, getAnalyzer()).utf8ToString();
+    } catch (IOException e) {
+      throw new ParseException(e.getMessage());
+    } catch (IllegalArgumentException e) {
+      throw new ParseException(e.getMessage());
     }
-    if (last < termStr.length()){
-      sb.append(analyzeSingleChunk(field, termStr, termStr.substring(last)));
-    }
-    return super.getWildcardQuery(field, sb.toString());
+    return super.getWildcardQuery(field, analyzed);
   }
   
   /**
@@ -123,7 +102,15 @@
    */
   @Override
   protected Query getPrefixQuery(String field, String termStr) throws ParseException {
-    String analyzed = analyzeSingleChunk(field, termStr, termStr);
+    String analyzed = null;
+    try{
+      analyzed = analyzeMultitermTerm(field, termStr, getAnalyzer()).utf8ToString();
+    } catch (IOException e) {
+      throw new ParseException(e.getMessage());
+    } catch (IllegalArgumentException e) {
+      throw new ParseException(e.getMessage());
+    }
+
     return super.getPrefixQuery(field, analyzed);
   }
 
@@ -144,61 +131,14 @@
   protected Query getFuzzyQuery(String field, String termStr, float minSimilarity)
       throws ParseException {
    
-    String analyzed = analyzeSingleChunk(field, termStr, termStr);
-    return super.getFuzzyQuery(field, analyzed, minSimilarity);
-  }
-
-  /**
-   * Returns the analyzed form for the given chunk
-   * 
-   * If the analyzer produces more than one output token from the given chunk,
-   * a ParseException is thrown.
-   *
-   * @param field The target field
-   * @param termStr The full term from which the given chunk is excerpted
-   * @param chunk The portion of the given termStr to be analyzed
-   * @return The result of analyzing the given chunk
-   * @throws ParseException when analysis returns other than one output token
-   */
-  protected String analyzeSingleChunk(String field, String termStr, String chunk) throws ParseException{
     String analyzed = null;
-    try (TokenStream stream = getAnalyzer().tokenStream(field, chunk)) {
-      stream.reset();
-      CharTermAttribute termAtt = stream.getAttribute(CharTermAttribute.class);
-      // get first and hopefully only output token
-      if (stream.incrementToken()) {
-        analyzed = termAtt.toString();
-        
-        // try to increment again, there should only be one output token
-        StringBuilder multipleOutputs = null;
-        while (stream.incrementToken()) {
-          if (null == multipleOutputs) {
-            multipleOutputs = new StringBuilder();
-            multipleOutputs.append('"');
-            multipleOutputs.append(analyzed);
-            multipleOutputs.append('"');
-          }
-          multipleOutputs.append(',');
-          multipleOutputs.append('"');
-          multipleOutputs.append(termAtt.toString());
-          multipleOutputs.append('"');
-        }
-        stream.end();
-        if (null != multipleOutputs) {
-          throw new ParseException(
-              String.format(getLocale(),
-                  "Analyzer created multiple terms for \"%s\": %s", chunk, multipleOutputs.toString()));
-        }
-      } else {
-        // nothing returned by analyzer.  Was it a stop word and the user accidentally
-        // used an analyzer with stop words?
-        stream.end();
-        throw new ParseException(String.format(getLocale(), "Analyzer returned nothing for \"%s\"", chunk));
-      }
-    } catch (IOException e){
-      throw new ParseException(
-          String.format(getLocale(), "IO error while trying to analyze single term: \"%s\"", termStr));
+    try{
+      analyzed = analyzeMultitermTerm(field, termStr, getAnalyzer()).utf8ToString();
+    } catch (IOException e) {
+      throw new ParseException(e.getMessage());
+    } catch (IllegalArgumentException e) {
+      throw new ParseException(e.getMessage());
     }
-    return analyzed;
+    return super.getFuzzyQuery(field, analyzed, minSimilarity);
   }
 }
Index: lucene/core/src/test/org/apache/lucene/util/TestQueryBuilderMultitermAnalysis.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/util/TestQueryBuilderMultitermAnalysis.java	(revision 0)
+++ lucene/core/src/test/org/apache/lucene/util/TestQueryBuilderMultitermAnalysis.java	(revision 0)
@@ -0,0 +1,196 @@
+package org.apache.lucene.util; 
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */ 
+
+import java.io.IOException; 
+import java.util.LinkedHashMap; 
+import java.util.Map; 
+
+import org.apache.lucene.analysis.Analyzer; 
+import org.apache.lucene.analysis.MockRegexReplacementFilter; 
+import org.apache.lucene.analysis.MockTokenFilter; 
+import org.apache.lucene.analysis.MockTokenizer; 
+import org.apache.lucene.analysis.TokenFilter; 
+import org.apache.lucene.analysis.Tokenizer; 
+import org.apache.lucene.util.BytesRef; 
+import org.apache.lucene.util.LuceneTestCase; 
+import org.junit.AfterClass; 
+import org.junit.BeforeClass; 
+
+/**
+ * Tests the multiterm analysis methods in QueryBuilder.
+ * <p>
+ * This runs tests with three analyzers.  All analyzers replace vowels with upper
+ * case vowels to mock normalization.
+ *
+ */ 
+
+public class TestQueryBuilderMultitermAnalysis extends LuceneTestCase {
+  private static Analyzer stopAnalyzer;
+  private static Analyzer noStopAnalyzer;
+  private static Analyzer wsAnalyzer;
+  private static final String FIELD = "f1";
+
+  @BeforeClass
+  public static void beforeClass() throws Exception {
+    final Map<String, String> charReplacements = new LinkedHashMap<String, String>();
+    charReplacements.put("a", "A");
+    charReplacements.put("e", "E");
+    charReplacements.put("i", "I");
+    charReplacements.put("o", "O");
+    charReplacements.put("u", "U");
+    
+    noStopAnalyzer = new Analyzer() {
+      @Override
+      public TokenStreamComponents createComponents(String fieldName) {
+        Tokenizer tokenizer = new MockTokenizer(MockTokenizer.SIMPLE,
+            true);
+        TokenFilter replace = new MockRegexReplacementFilter(tokenizer);
+        ((MockRegexReplacementFilter)replace).setReplacements(charReplacements);
+        return new TokenStreamComponents(tokenizer, replace);
+      }
+    };
+
+    stopAnalyzer = new Analyzer() {
+      @Override
+      public TokenStreamComponents createComponents(String fieldName) {
+        Tokenizer tokenizer = new MockTokenizer(MockTokenizer.SIMPLE,
+            true);
+        TokenFilter filter = new MockTokenFilter(tokenizer, MockTokenFilter.ENGLISH_STOPSET);
+        TokenFilter replace = new MockRegexReplacementFilter(filter);
+        ((MockRegexReplacementFilter)replace).setReplacements(charReplacements);
+        return new TokenStreamComponents(tokenizer, replace);
+      }
+    };
+    
+    wsAnalyzer = new Analyzer() {
+      @Override
+      public TokenStreamComponents createComponents(String fieldName) {
+        Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE,
+            true);
+        TokenFilter replace = new MockRegexReplacementFilter(tokenizer);
+        ((MockRegexReplacementFilter)replace).setReplacements(charReplacements);
+        return new TokenStreamComponents(tokenizer, replace);
+      }
+    };
+  }
+
+  @AfterClass
+  public static void afterClass() throws Exception {
+    stopAnalyzer = null;
+    noStopAnalyzer = null;
+    wsAnalyzer = null;
+  }
+
+  public void testAnalyzeMultitermTerm() throws Exception {
+    String txt = "abc";
+    QueryBuilder b = new QueryBuilder(noStopAnalyzer);
+    BytesRef normed = b.analyzeMultitermTerm(FIELD, txt, noStopAnalyzer);
+    assertEquals("Abc", normed.utf8ToString());
+    
+    //analyzer must not have stop words
+    testExceptionMT("the", stopAnalyzer);
+    //analyzer will return two tokens
+    testExceptionMT("abc def", noStopAnalyzer);
+    //analyzer will return two tokens
+    testExceptionMT("the*abc", noStopAnalyzer);
+    //analyzer will return one token, but the offset will be > 1
+    testExceptionMT("the*abc", stopAnalyzer);
+    testExceptionMT("the?abc", stopAnalyzer);
+    //analyzer will return two tokens
+    testExceptionMT("abc*the", noStopAnalyzer);
+    //analyzer will return two tokens
+    testExceptionMT("the?abc", noStopAnalyzer);
+    
+    //TODO: how do we test for this?  OffsetAttribute didn't seem to help
+    //testException("abc*the", stopAnalyzer);
+    
+    //make absolutely sure there aren't any assertion
+    //problems with fully consuming the token stream and closing it down.
+    normed = b.analyzeMultitermTerm(FIELD, txt, noStopAnalyzer);
+    assertEquals("Abc", normed.utf8ToString());
+  }
+  
+  public void testAnalyzeWildcard() throws Exception {
+    QueryBuilder b = new QueryBuilder(noStopAnalyzer);
+    BytesRef normed = b.analyzeWildcard(FIELD, "abc", noStopAnalyzer);
+    assertEquals("Abc", normed.utf8ToString());
+    
+    normed = b.analyzeWildcard(FIELD, "abc*def", noStopAnalyzer);
+    assertEquals("Abc*dEf", normed.utf8ToString());
+    normed = b.analyzeWildcard(FIELD, "*def", noStopAnalyzer);
+    assertEquals("*dEf", normed.utf8ToString());
+    normed = b.analyzeWildcard(FIELD, "?def", noStopAnalyzer);
+    assertEquals("?dEf", normed.utf8ToString());
+    normed = b.analyzeWildcard(FIELD, "abc\\\\*def", noStopAnalyzer);
+    assertEquals("Abc*dEf", normed.utf8ToString());
+    
+    //note that wsAnalyzer is required for these.
+    //MockTokenizer.SIMPLE would return Abc
+    normed = b.analyzeWildcard(FIELD, "abc\\*", wsAnalyzer);
+    assertEquals("Abc\\*", normed.utf8ToString());
+    
+    normed = b.analyzeWildcard(FIELD, "\\*abc", wsAnalyzer);
+    assertEquals("\\*Abc", normed.utf8ToString());
+    //wildcard chars alone are ok for analyzeWildcard
+    normed = b.analyzeWildcard(FIELD, "*", noStopAnalyzer);
+    assertEquals("*", normed.utf8ToString());
+    
+    normed = b.analyzeWildcard(FIELD, "?", noStopAnalyzer);
+    assertEquals("?", normed.utf8ToString());
+    normed = b.analyzeWildcard(FIELD, "***", noStopAnalyzer);
+    assertEquals("***", normed.utf8ToString());
+    
+    normed = b.analyzeWildcard(FIELD, "?*?*?", noStopAnalyzer);
+    assertEquals("?*?*?", normed.utf8ToString());
+    //* is not escaped and therefore tokenizer will split
+    testExceptionMT("abc\\\\*def", noStopAnalyzer);
+    testExceptionWC("the\\\\*def", stopAnalyzer);
+    testExceptionWC("the*def", stopAnalyzer);
+    testExceptionWC("def*the", stopAnalyzer);
+    testExceptionWC("*the", stopAnalyzer);
+    testExceptionWC("the*", stopAnalyzer);
+    
+    //make absolutely sure there aren't any assertion
+    //problems caused by failing to consume the tokenstream and closing it down.
+    normed = b.analyzeMultitermTerm(FIELD, "abc", noStopAnalyzer);
+    assertEquals("Abc", normed.utf8ToString());
+  }
+  
+  private void testExceptionMT(String part, Analyzer analyzer) throws IOException {
+    boolean ex = false;
+    QueryBuilder b = new QueryBuilder(analyzer);
+    try{
+      b.analyzeMultitermTerm(FIELD, part, analyzer);
+    } catch (IllegalArgumentException e) {
+      ex = true;
+    }
+    assertTrue("Exception: "+part, ex);
+  }
+
+  private void testExceptionWC(String part, Analyzer analyzer) throws IOException {
+    boolean ex = false;
+    QueryBuilder b = new QueryBuilder(analyzer);
+    try{
+      b.analyzeWildcard(FIELD, part, analyzer);
+    } catch (IllegalArgumentException e) {
+      ex = true;
+    }
+    assertTrue("Exception: "+part, ex);
+  }
+}
Index: lucene/core/src/java/org/apache/lucene/util/QueryBuilder.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/util/QueryBuilder.java	(revision 1571317)
+++ lucene/core/src/java/org/apache/lucene/util/QueryBuilder.java	(working copy)
@@ -18,6 +18,8 @@
  */
 
 import java.io.IOException;
+import java.util.regex.Matcher;
+import java.util.regex.Pattern;
 import java.util.ArrayList;
 import java.util.List;
 
@@ -33,6 +35,7 @@
 import org.apache.lucene.search.PhraseQuery;
 import org.apache.lucene.search.Query;
 import org.apache.lucene.search.TermQuery;
+import org.apache.lucene.util.BytesRef;
 
 /**
  * Creates queries from the {@link Analyzer} chain.
@@ -50,6 +53,9 @@
  * are provided so that the generated queries can be customized.
  */
 public class QueryBuilder {
+  //pattern used in analyzeWildcard
+  private final static Pattern WILDCARD_PATTERN = Pattern.compile("(?s)(\\\\.)|([?*]+)");
+
   private Analyzer analyzer;
   private boolean enablePositionIncrements = true;
   
@@ -410,4 +416,149 @@
   protected MultiPhraseQuery newMultiPhraseQuery() {
     return new MultiPhraseQuery();
   }
+
+  /**
+   * Analysis of wildcards is a bit tricky.  This splits a term by wildcard
+   * characters, analyzes the subcomponents and puts the analyzed termText 
+   * back together. If the multiTermAnalyzer is null, this returns 
+   * the termText without changes. 
+   * <p>
+   * Please note: this will throw an IllegalArgumentException if 
+   * the analyzer returns zero terms or more than one term.  It will also 
+   * throw an IllegalArgument exception if the analyzer supports 
+   * position increments, and the increment is > 1 (a stop word
+   * has been skipped).   
+   * <p>
+   * See also:{@link #analyzeMultiterm(String, String, Analyzer)
+   * 
+   * @param field for query
+   * @param termText incoming wildcard query
+   * @param analyzer analyzer to use
+   * @return analyzed wildcard BytesRef
+   * @throws IOException for IO problems during analysis 
+   * and IllegalArgumentException if zero or more than one term 
+   * is returned by the analyzer
+   */
+  public BytesRef analyzeWildcard(String field, String termText, 
+      Analyzer analyzer) throws IOException, IllegalArgumentException {
+    
+    if (analyzer == null) {
+      return new BytesRef(termText);
+    }
+    
+    Matcher wildcardMatcher = WILDCARD_PATTERN.matcher(termText);
+    BytesRef bytes = new BytesRef();
+    int last = 0;
+
+    while (wildcardMatcher.find()) {
+      // continue if escaped char
+      if (wildcardMatcher.group(1) != null) {
+        continue;
+      }
+
+      if (wildcardMatcher.start() > 0) {
+        String chunk = termText.substring(last, wildcardMatcher.start());
+        BytesRef analyzed = analyzeMultitermTerm(field, chunk, analyzer);
+        bytes.append(analyzed);
+      }
+      // append the wildcard character
+      bytes.append(new BytesRef(wildcardMatcher.group(2)));
+
+      last = wildcardMatcher.end();
+    }
+    if (last < termText.length()) {
+      bytes.append(analyzeMultitermTerm(field, termText.substring(last), analyzer));
+    }
+    return BytesRef.deepCopyOf(bytes);
+  }
+  
+  /**
+   * Analyzes the part.  If the multiTermAnalyzer is null, this 
+   * returns the part without changes. 
+   * <p>
+   * Please note: this will throw an IllegalArgumentException if 
+   * the analyzer returns zero terms or more than one term.  It will 
+   * also throw an IllegalArgument exception if the analyzer supports 
+   * position increments, and the increment is > 1 (a stop word
+   * has been skipped).
+   * <p>
+   * To analyze a wildcard, use 
+   * {@link #analyzeWildcard(String, String, Analyzer) instead.
+   * 
+   * @param field field for term
+   * @param part the string to analyze
+   * @param multiTermAnalyzer analyzer 
+   * @return bytesRef to analyzed term part
+   * @throws IOException for IO problems during analysis and IllegalArgumentException
+   * if zero or more than one term is returned by the analyzer.
+   */
+  public BytesRef analyzeMultitermTerm(String field, String part, Analyzer multiTermAnalyzer) 
+      throws IOException, IllegalArgumentException {
+
+    if (multiTermAnalyzer == null) {
+      return new BytesRef(part);
+    }
+
+    TokenStream source;
+
+    try {
+      source = multiTermAnalyzer.tokenStream(field, part);
+      source.reset();
+    } catch (IOException e) {
+      throw new IOException("Unable to initialize TokenStream to analyze multiTerm term: " + part);
+    }
+    TermToBytesRefAttribute termAtt = source.getAttribute(TermToBytesRefAttribute.class);
+    PositionIncrementAttribute posAtt = null;
+    if (source.hasAttribute(PositionIncrementAttribute.class)) {
+      posAtt = source.getAttribute(PositionIncrementAttribute.class);
+    }
+    BytesRef bytes = termAtt.getBytesRef();
+    int incr = -1;
+    try {
+      // get first and hopefully only output token
+      if (source.incrementToken()) {
+        termAtt.fillBytesRef();
+        if (posAtt != null) {
+          incr += posAtt.getPositionIncrement();
+        }
+        // try to increment again, there should only be one output token
+        StringBuilder multipleOutputs = null;
+        String first = bytes.utf8ToString();
+        while (source.incrementToken()) {
+          termAtt.fillBytesRef();
+          if (null == multipleOutputs) {
+            multipleOutputs = new StringBuilder();
+            multipleOutputs.append('"');
+            multipleOutputs.append(first);
+            multipleOutputs.append('"');
+          }
+          multipleOutputs.append(',');
+          multipleOutputs.append('"');
+          multipleOutputs.append(bytes.utf8ToString());
+          multipleOutputs.append('"');
+        }
+        source.end();
+        source.close();
+        if (null != multipleOutputs) {
+          throw new IllegalArgumentException(
+              String.format(
+                  "Analyzer created multiple terms for \"%s\": %s", part, multipleOutputs.toString()));
+        } else if (incr > 0) {
+          throw new IllegalArgumentException(
+              String.format(
+                  "Analyzer created multiple terms for \"%s\"", part));          
+        } 
+      } else {
+        // nothing returned by analyzer.  Was it a stop word and the user accidentally
+        // used an analyzer with stop words?
+        source.end();
+        source.close();
+        throw new IllegalArgumentException(String.format("Analyzer returned nothing for \"%s\"", part));
+      }
+    } catch (IOException e) {
+      throw new IOException(
+          String.format("IO error while trying to analyze single term: \"%s\"", part));
+    }
+    return BytesRef.deepCopyOf(bytes);
+  }
 }
Index: lucene/test-framework/src/java/org/apache/lucene/analysis/MockRegexReplacementFilter.java
===================================================================
--- lucene/test-framework/src/java/org/apache/lucene/analysis/MockRegexReplacementFilter.java	(revision 0)
+++ lucene/test-framework/src/java/org/apache/lucene/analysis/MockRegexReplacementFilter.java	(revision 0)
@@ -0,0 +1,84 @@
+package org.apache.lucene.analysis;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.LinkedHashMap;
+import java.util.Map;
+import java.util.regex.Matcher;
+import java.util.regex.Pattern;
+
+import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
+
+/**
+ *   This allows for easy mocking of normalization filters
+ *   like ascii or icu normalization. 
+ *   <p>
+ *   If a token is reduced to zero-length through this process, 
+ *   the token will not be returned.
+ *   <p>
+ *   Under the hood, a LinkedHashMap is used to maintain the insertion order
+ *   of replacements.entrySet() that is passed in via setReplacements.
+ *   <p>
+ *   Regexes are case sensitive.  Make sure to add (?i) if you want 
+ *   case insensitivity.
+ */
+public class MockRegexReplacementFilter extends TokenFilter {
+  
+  private final CharTermAttribute termAtt;
+  private LinkedHashMap<Pattern, String> replacements;
+  
+  public MockRegexReplacementFilter(TokenStream in) {
+    super(in);
+    termAtt = addAttribute(CharTermAttribute.class);
+  }
+
+  public void setReplacements(Map<String, String> replacements) {
+    this.replacements = new LinkedHashMap<Pattern, String>();
+    for (Map.Entry<String, String> entry : replacements.entrySet()){
+      Pattern p = Pattern.compile(entry.getKey());
+      this.replacements.put(p,  entry.getValue());
+    }
+  }
+
+  @Override
+  public final boolean incrementToken() throws java.io.IOException {
+
+    while (input.incrementToken()){
+      String text = termAtt.toString().toLowerCase();
+      for (Map.Entry<Pattern, String> entry : replacements.entrySet()){
+        Matcher m = entry.getKey().matcher(text);
+        text = m.replaceAll(entry.getValue());
+      }
+      if (text.length() > 0){
+        termAtt.setEmpty().append(text);
+        return true;
+      }
+      //else go on to next token
+    }
+    return false;
+
+  }
+
+  @Override
+  public void reset() throws IOException {
+    super.reset();
+  }
+}
+
+
