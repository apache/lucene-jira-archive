Index: solr/core/src/java/org/apache/solr/schema/TextField.java
===================================================================
--- solr/core/src/java/org/apache/solr/schema/TextField.java	(revision 1573120)
+++ solr/core/src/java/org/apache/solr/schema/TextField.java	(working copy)
@@ -17,11 +17,10 @@
 
 package org.apache.solr.schema;
 
-import org.apache.lucene.analysis.tokenattributes.TermToBytesRefAttribute;
 import org.apache.lucene.search.*;
 import org.apache.lucene.index.StorableField;
-import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.queryparser.classic.QueryParserBase;
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.CharsRef;
 import org.apache.lucene.util.QueryBuilder;
@@ -133,25 +132,24 @@
   }
 
   public static BytesRef analyzeMultiTerm(String field, String part, Analyzer analyzerIn) {
+
     if (part == null || analyzerIn == null) return null;
 
-    try (TokenStream source = analyzerIn.tokenStream(field, part)){
-      source.reset();
+    BytesRef analyzed = null;
 
-      TermToBytesRefAttribute termAtt = source.getAttribute(TermToBytesRefAttribute.class);
-      BytesRef bytes = termAtt.getBytesRef();
-
-      if (!source.incrementToken())
+    //analyzerIn is not actually used in initialization
+    try{
+      analyzed = QueryParserBase.analyzeMultitermTerm(field, part, analyzerIn);
+    } catch (IllegalArgumentException e) {
+      if (e.getMessage().indexOf("returned nothing") > -1) {
         throw  new SolrException(SolrException.ErrorCode.BAD_REQUEST,"analyzer returned no terms for multiTerm term: " + part);
-      termAtt.fillBytesRef();
-      if (source.incrementToken())
+      } else {
         throw  new SolrException(SolrException.ErrorCode.BAD_REQUEST,"analyzer returned too many terms for multiTerm term: " + part);
-
-      source.end();
-      return BytesRef.deepCopyOf(bytes);
-    } catch (IOException e) {
+      }
+    } catch (RuntimeException e) {
       throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,"error analyzing range part: " + part, e);
     }
+    return analyzed;
   }
 
 
Index: lucene/queryparser/src/test/org/apache/lucene/queryparser/classic/TestQueryParserBaseMultitermAnalysis.java
===================================================================
--- lucene/queryparser/src/test/org/apache/lucene/queryparser/classic/TestQueryParserBaseMultitermAnalysis.java	(revision 0)
+++ lucene/queryparser/src/test/org/apache/lucene/queryparser/classic/TestQueryParserBaseMultitermAnalysis.java	(revision 0)
@@ -0,0 +1,138 @@
+package org.apache.lucene.util; 
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */ 
+
+import java.io.IOException; 
+import java.util.LinkedHashMap; 
+import java.util.Map; 
+
+import org.apache.lucene.analysis.Analyzer; 
+import org.apache.lucene.analysis.MockRegexReplacementFilter; 
+import org.apache.lucene.analysis.MockTokenFilter; 
+import org.apache.lucene.analysis.MockTokenizer; 
+import org.apache.lucene.analysis.TokenFilter; 
+import org.apache.lucene.analysis.Tokenizer; 
+import org.apache.lucene.queryparser.classic.QueryParserBase;
+import org.apache.lucene.util.BytesRef; 
+import org.apache.lucene.util.LuceneTestCase; 
+import org.junit.AfterClass; 
+import org.junit.BeforeClass; 
+
+/**
+ * Tests the multiterm analysis methods in QueryBuilder.
+ * <p>
+ * This runs tests with three analyzers.  All analyzers replace vowels with upper
+ * case vowels to mock normalization.
+ *
+ */ 
+
+public class TestQueryParserBaseMultitermAnalysis extends LuceneTestCase {
+  private static Analyzer stopAnalyzer;
+  private static Analyzer noStopAnalyzer;
+  private static Analyzer wsAnalyzer;
+  private static final String FIELD = "f1";
+
+  @BeforeClass
+  public static void beforeClass() throws Exception {
+    final Map<String, String> charReplacements = new LinkedHashMap<String, String>();
+    charReplacements.put("a", "A");
+    charReplacements.put("e", "E");
+    charReplacements.put("i", "I");
+    charReplacements.put("o", "O");
+    charReplacements.put("u", "U");
+    
+    noStopAnalyzer = new Analyzer() {
+      @Override
+      public TokenStreamComponents createComponents(String fieldName) {
+        Tokenizer tokenizer = new MockTokenizer(MockTokenizer.SIMPLE,
+            true);
+        TokenFilter replace = new MockRegexReplacementFilter(tokenizer);
+        ((MockRegexReplacementFilter)replace).setReplacements(charReplacements);
+        return new TokenStreamComponents(tokenizer, replace);
+      }
+    };
+
+    stopAnalyzer = new Analyzer() {
+      @Override
+      public TokenStreamComponents createComponents(String fieldName) {
+        Tokenizer tokenizer = new MockTokenizer(MockTokenizer.SIMPLE,
+            true);
+        TokenFilter filter = new MockTokenFilter(tokenizer, MockTokenFilter.ENGLISH_STOPSET);
+        TokenFilter replace = new MockRegexReplacementFilter(filter);
+        ((MockRegexReplacementFilter)replace).setReplacements(charReplacements);
+        return new TokenStreamComponents(tokenizer, replace);
+      }
+    };
+    
+    wsAnalyzer = new Analyzer() {
+      @Override
+      public TokenStreamComponents createComponents(String fieldName) {
+        Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE,
+            true);
+        TokenFilter replace = new MockRegexReplacementFilter(tokenizer);
+        ((MockRegexReplacementFilter)replace).setReplacements(charReplacements);
+        return new TokenStreamComponents(tokenizer, replace);
+      }
+    };
+  }
+
+  @AfterClass
+  public static void afterClass() throws Exception {
+    stopAnalyzer = null;
+    noStopAnalyzer = null;
+    wsAnalyzer = null;
+  }
+
+  public void testAnalyzeMultitermTerm() throws Exception {
+    String txt = "abc";
+    BytesRef normed = QueryParserBase.analyzeMultitermTerm(FIELD, txt, noStopAnalyzer);
+    assertEquals("Abc", normed.utf8ToString());
+    
+    //analyzer must not have stop words
+    testException("the", stopAnalyzer);
+    //analyzer will return two tokens
+    testException("abc def", noStopAnalyzer);
+    //analyzer will return two tokens
+    testException("the*abc", noStopAnalyzer);
+    //analyzer will return one token, but the offset will be > 1
+    testException("the*abc", stopAnalyzer);
+    testException("the?abc", stopAnalyzer);
+    //analyzer will return two tokens
+    testException("abc*the", noStopAnalyzer);
+    //analyzer will return two tokens
+    testException("the?abc", noStopAnalyzer);
+    
+    //TODO: how do we test for this?  OffsetAttribute didn't seem to help
+    //testException("abc*the", stopAnalyzer);
+    
+    //make absolutely sure there aren't any assertion
+    //problems with fully consuming the token stream and closing it down.
+    normed = QueryParserBase.analyzeMultitermTerm(FIELD, txt, noStopAnalyzer);
+    assertEquals("Abc", normed.utf8ToString());
+  }
+  
+  private void testException(String part, Analyzer analyzer) throws IOException {
+    boolean ex = false;
+    try{
+      QueryParserBase.analyzeMultitermTerm(FIELD, part, analyzer);
+    } catch (IllegalArgumentException e) {
+      ex = true;
+    }
+    assertTrue("Exception: "+part, ex);
+  }
+}
Index: lucene/queryparser/src/test/org/apache/lucene/queryparser/analyzing/TestAnalyzingQueryParser.java
===================================================================
--- lucene/queryparser/src/test/org/apache/lucene/queryparser/analyzing/TestAnalyzingQueryParser.java	(revision 1573120)
+++ lucene/queryparser/src/test/org/apache/lucene/queryparser/analyzing/TestAnalyzingQueryParser.java	(working copy)
@@ -112,10 +112,12 @@
         (random(), MockTokenizer.WHITESPACE, true, MockTokenFilter.ENGLISH_STOPSET);
     try {
       String q = parseWithAnalyzingQueryParser(termStr, stopsAnalyzer, true);     
-    } catch (ParseException e){
-      if (e.getMessage().contains("returned nothing")){
+    } catch (ParseException e) {
+      if (e.getMessage().contains("returned nothing")) {
         ex = true;
       }
+    } catch (IllegalArgumentException e) {
+      ex = true;
     }
     assertEquals("Should have returned nothing", true, ex);
     ex = false;
@@ -123,10 +125,12 @@
     AnalyzingQueryParser qp = new AnalyzingQueryParser(TEST_VERSION_CURRENT, FIELD, a);
     try{
       qp.analyzeSingleChunk(FIELD, "", "not a single chunk");
-    } catch (ParseException e){
-      if (e.getMessage().contains("multiple terms")){
+    } catch (ParseException e) {
+      if (e.getMessage().contains("multiple terms")) {
         ex = true;
       }
+    } catch (IllegalArgumentException e) {
+      ex = true;
     }
     assertEquals("Should have produced multiple terms", true, ex);
   }
@@ -145,7 +149,7 @@
     try {
       String qString = parseWithAnalyzingQueryParser("*", a, true);
       assertEquals("Every word", "*", qString);
-    } catch (ParseException e){
+    } catch (ParseException e) {
       pex = true;
     }
       
@@ -154,11 +158,11 @@
   }
   public void testWildCardEscapes() throws ParseException, IOException {
 
-    for (Map.Entry<String, String> entry : wildcardEscapeHits.entrySet()){
+    for (Map.Entry<String, String> entry : wildcardEscapeHits.entrySet()) {
       Query q = getAnalyzedQuery(entry.getKey(), a, false);
       assertEquals("WildcardEscapeHits: " + entry.getKey(), true, isAHit(q, entry.getValue(), a));
     }
-    for (Map.Entry<String, String> entry : wildcardEscapeMisses.entrySet()){
+    for (Map.Entry<String, String> entry : wildcardEscapeMisses.entrySet()) {
       Query q = getAnalyzedQuery(entry.getKey(), a, false);
       assertEquals("WildcardEscapeMisses: " + entry.getKey(), false, isAHit(q, entry.getValue(), a));
     }
@@ -169,7 +173,7 @@
     try{
       String q = parseWithAnalyzingQueryParser(wildcardInput[0], a, false);
 
-    } catch (ParseException e){
+    } catch (ParseException e) {
       ex = true;
     }
     assertEquals("Testing initial wildcard not allowed",
@@ -295,4 +299,4 @@
     }
 
   }
-}
\ No newline at end of file
+}
Index: lucene/queryparser/src/java/org/apache/lucene/queryparser/classic/QueryParserBase.java
===================================================================
--- lucene/queryparser/src/java/org/apache/lucene/queryparser/classic/QueryParserBase.java	(revision 1573120)
+++ lucene/queryparser/src/java/org/apache/lucene/queryparser/classic/QueryParserBase.java	(working copy)
@@ -24,6 +24,7 @@
 
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;
 import org.apache.lucene.analysis.tokenattributes.TermToBytesRefAttribute;
 import org.apache.lucene.document.DateTools;
 import org.apache.lucene.index.Term;
@@ -580,25 +581,93 @@
     return analyzeMultitermTerm(field, part, getAnalyzer());
   }
 
-  protected BytesRef analyzeMultitermTerm(String field, String part, Analyzer analyzerIn) {
-    if (analyzerIn == null) analyzerIn = getAnalyzer();
+  /**
+   * Analyzes the part.  If the multitermAnalyzer is null, this
+   * returns the part without changes.
+   * <p>
+   * Please note: this will throw an IllegalArgumentException if
+   * the analyzer returns zero terms or more than one term.  It will
+   * also throw an IllegalArgument exception if the analyzer supports
+   * position increments, and the increment is > 1 (a stop word
+   * has been skipped).  Finally, it will throw a RuntimeException
+   * if there is an IOException during analysis.
+   * <p>
+   * To analyze a wildcard, use
+   * {@link #analyzeWildcard(String, String, Analyzer) instead.
+   *
+   * @param field field for term
+   * @param part the string to analyze
+   * @param multitermAnalyzer analyzer
+   * @return bytesRef to analyzed term part
+   * @throws RuntimeException for IO problems during analysis and IllegalArgumentException
+   * if zero or more than one term is returned by the analyzer.
+   */
+  public static BytesRef analyzeMultitermTerm(String field, String part, Analyzer multitermAnalyzer) {
+    if (multitermAnalyzer == null) {
+      return new BytesRef(part);
+    }
 
-    try (TokenStream source = analyzerIn.tokenStream(field, part)) {
+    TokenStream source;
+
+    try {
+      source = multitermAnalyzer.tokenStream(field, part);
       source.reset();
-      
-      TermToBytesRefAttribute termAtt = source.getAttribute(TermToBytesRefAttribute.class);
-      BytesRef bytes = termAtt.getBytesRef();
-
-      if (!source.incrementToken())
-        throw new IllegalArgumentException("analyzer returned no terms for multiTerm term: " + part);
-      termAtt.fillBytesRef();
-      if (source.incrementToken())
-        throw new IllegalArgumentException("analyzer returned too many terms for multiTerm term: " + part);
-      source.end();
-      return BytesRef.deepCopyOf(bytes);
     } catch (IOException e) {
-      throw new RuntimeException("Error analyzing multiTerm term: " + part, e);
+      throw new RuntimeException("Unable to initialize TokenStream to analyze multiTerm term: " + part);
     }
+    TermToBytesRefAttribute termAtt = source.getAttribute(TermToBytesRefAttribute.class);
+    PositionIncrementAttribute posAtt = null;
+    if (source.hasAttribute(PositionIncrementAttribute.class)) {
+      posAtt = source.getAttribute(PositionIncrementAttribute.class);
+    }
+    BytesRef bytes = termAtt.getBytesRef();
+    int incr = -1;
+    try {
+      // get first and hopefully only output token
+      if (source.incrementToken()) {
+        termAtt.fillBytesRef();
+        if (posAtt != null) {
+          incr += posAtt.getPositionIncrement();
+        }
+        // try to increment again, there should only be one output token
+        StringBuilder multipleOutputs = null;
+        String first = bytes.utf8ToString();
+        while (source.incrementToken()) {
+          termAtt.fillBytesRef();
+          if (null == multipleOutputs) {
+            multipleOutputs = new StringBuilder();
+            multipleOutputs.append('"');
+            multipleOutputs.append(first);
+            multipleOutputs.append('"');
+          }
+          multipleOutputs.append(',');
+          multipleOutputs.append('"');
+          multipleOutputs.append(bytes.utf8ToString());
+          multipleOutputs.append('"');
+        }
+        source.end();
+        source.close();
+        if (null != multipleOutputs) {
+          throw new IllegalArgumentException(
+              String.format(
+                  "Analyzer created multiple terms for \"%s\": %s", part, multipleOutputs.toString()));
+        } else if (incr > 0) {
+          throw new IllegalArgumentException(
+              String.format(
+                  "Analyzer created multiple terms for \"%s\"", part));
+        }
+      } else {
+        // nothing returned by analyzer.  Was it a stop word and the user accidentally
+        // used an analyzer with stop words?
+        source.end();
+        source.close();
+        throw new IllegalArgumentException(String.format("Analyzer returned nothing for \"%s\"", part));
+      }
+    } catch (IOException e) {
+      throw new RuntimeException(
+          String.format("IO error while trying to analyze single term: \"%s\"", part));
+    }
+    return BytesRef.deepCopyOf(bytes);
   }
 
   /**
Index: lucene/queryparser/src/java/org/apache/lucene/queryparser/analyzing/AnalyzingQueryParser.java
===================================================================
--- lucene/queryparser/src/java/org/apache/lucene/queryparser/analyzing/AnalyzingQueryParser.java	(revision 1573120)
+++ lucene/queryparser/src/java/org/apache/lucene/queryparser/analyzing/AnalyzingQueryParser.java	(working copy)
@@ -22,8 +22,6 @@
 import java.util.regex.Pattern;
 
 import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
 import org.apache.lucene.queryparser.classic.ParseException;
 import org.apache.lucene.search.Query;
 import org.apache.lucene.util.Version;
@@ -162,42 +160,12 @@
    */
   protected String analyzeSingleChunk(String field, String termStr, String chunk) throws ParseException{
     String analyzed = null;
-    try (TokenStream stream = getAnalyzer().tokenStream(field, chunk)) {
-      stream.reset();
-      CharTermAttribute termAtt = stream.getAttribute(CharTermAttribute.class);
-      // get first and hopefully only output token
-      if (stream.incrementToken()) {
-        analyzed = termAtt.toString();
-        
-        // try to increment again, there should only be one output token
-        StringBuilder multipleOutputs = null;
-        while (stream.incrementToken()) {
-          if (null == multipleOutputs) {
-            multipleOutputs = new StringBuilder();
-            multipleOutputs.append('"');
-            multipleOutputs.append(analyzed);
-            multipleOutputs.append('"');
-          }
-          multipleOutputs.append(',');
-          multipleOutputs.append('"');
-          multipleOutputs.append(termAtt.toString());
-          multipleOutputs.append('"');
-        }
-        stream.end();
-        if (null != multipleOutputs) {
-          throw new ParseException(
-              String.format(getLocale(),
-                  "Analyzer created multiple terms for \"%s\": %s", chunk, multipleOutputs.toString()));
-        }
-      } else {
-        // nothing returned by analyzer.  Was it a stop word and the user accidentally
-        // used an analyzer with stop words?
-        stream.end();
-        throw new ParseException(String.format(getLocale(), "Analyzer returned nothing for \"%s\"", chunk));
-      }
-    } catch (IOException e){
-      throw new ParseException(
-          String.format(getLocale(), "IO error while trying to analyze single term: \"%s\"", termStr));
+    try {
+      analyzed = analyzeMultitermTerm(field, chunk, getAnalyzer()).utf8ToString();
+    } catch (IllegalArgumentException e) {
+      throw new ParseException(e.getMessage());
+    } catch (RuntimeException e) {
+      throw new ParseException("IOException during analysis");
     }
     return analyzed;
   }
Index: lucene/test-framework/src/java/org/apache/lucene/analysis/MockRegexReplacementFilter.java
===================================================================
--- lucene/test-framework/src/java/org/apache/lucene/analysis/MockRegexReplacementFilter.java	(revision 0)
+++ lucene/test-framework/src/java/org/apache/lucene/analysis/MockRegexReplacementFilter.java	(revision 0)
@@ -0,0 +1,86 @@
+package org.apache.lucene.analysis;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.LinkedHashMap;
+import java.util.Map;
+import java.util.regex.Matcher;
+import java.util.regex.Pattern;
+
+import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
+
+/**
+ *   This allows for easy mocking of normalization filters
+ *   like ascii or icu normalization. 
+ *   <p>
+ *   If a token is reduced to zero-length through this process, 
+ *   the token will not be returned.
+ *   <p>
+ *   Under the hood, a LinkedHashMap is used to maintain the insertion order
+ *   of replacements.entrySet() that is passed in via setReplacements.
+ *   <p>
+ *   Regexes are case sensitive.  Make sure to add (?i) if you want 
+ *   case insensitivity.
+ *   <p>
+ *   Make sure to call setReplacements() immediately after initialization.
+ */
+public class MockRegexReplacementFilter extends TokenFilter {
+  
+  private final CharTermAttribute termAtt;
+  private LinkedHashMap<Pattern, String> replacements = null;
+  
+  public MockRegexReplacementFilter(TokenStream in) {
+    super(in);
+    termAtt = addAttribute(CharTermAttribute.class);
+  }
+
+  public void setReplacements(Map<String, String> replacements) {
+    this.replacements = new LinkedHashMap<Pattern, String>();
+    for (Map.Entry<String, String> entry : replacements.entrySet()){
+      Pattern p = Pattern.compile(entry.getKey());
+      this.replacements.put(p, entry.getValue());
+    }
+  }
+
+  @Override
+  public final boolean incrementToken() throws java.io.IOException {
+    assert(replacements != null);
+    while (input.incrementToken()){
+      String text = termAtt.toString().toLowerCase();
+      for (Map.Entry<Pattern, String> entry : replacements.entrySet()){
+        Matcher m = entry.getKey().matcher(text);
+        text = m.replaceAll(entry.getValue());
+      }
+      if (text.length() > 0){
+        termAtt.setEmpty().append(text);
+        return true;
+      }
+      //else go on to next token
+    }
+    return false;
+
+  }
+
+  @Override
+  public void reset() throws IOException {
+    super.reset();
+  }
+}
+
+
