Index: src/test/org/apache/lucene/index/TestIndexWriterMergePolicy.java
===================================================================
--- src/test/org/apache/lucene/index/TestIndexWriterMergePolicy.java	(revision 523296)
+++ src/test/org/apache/lucene/index/TestIndexWriterMergePolicy.java	(working copy)
@@ -40,7 +40,7 @@
     for (int i = 0; i < 100; i++) {
       addDoc(writer);
       checkInvariants(writer);
-      if (writer.getRamSegmentCount() + writer.getSegmentCount() >= 18) {
+      if (writer.getSegmentCount() + writer.getSegmentCount() >= 18) {
         noOverMerge = true;
       }
     }
@@ -178,7 +178,7 @@
     int mergeFactor = writer.getMergeFactor();
     int maxMergeDocs = writer.getMaxMergeDocs();
 
-    int ramSegmentCount = writer.getRamSegmentCount();
+    int ramSegmentCount = writer.getNumBufferedDocuments();
     assertTrue(ramSegmentCount < maxBufferedDocs);
 
     int lowerBound = -1;
Index: src/test/org/apache/lucene/index/TestIndexWriterDelete.java
===================================================================
--- src/test/org/apache/lucene/index/TestIndexWriterDelete.java	(revision 523296)
+++ src/test/org/apache/lucene/index/TestIndexWriterDelete.java	(working copy)
@@ -93,7 +93,7 @@
       }
       modifier.flush();
 
-      assertEquals(0, modifier.getRamSegmentCount());
+      assertEquals(0, modifier.getNumBufferedDocuments());
       assertTrue(0 < modifier.getSegmentCount());
 
       if (!autoCommit) {
@@ -435,7 +435,7 @@
           String[] startFiles = dir.list();
           SegmentInfos infos = new SegmentInfos();
           infos.read(dir);
-          IndexFileDeleter d = new IndexFileDeleter(dir, new KeepOnlyLastCommitDeletionPolicy(), infos, null);
+          IndexFileDeleter d = new IndexFileDeleter(dir, new KeepOnlyLastCommitDeletionPolicy(), infos, null, null);
           String[] endFiles = dir.list();
 
           Arrays.sort(startFiles);
Index: src/test/org/apache/lucene/index/TestIndexReader.java
===================================================================
--- src/test/org/apache/lucene/index/TestIndexReader.java	(revision 523296)
+++ src/test/org/apache/lucene/index/TestIndexReader.java	(working copy)
@@ -803,7 +803,7 @@
           String[] startFiles = dir.list();
           SegmentInfos infos = new SegmentInfos();
           infos.read(dir);
-          IndexFileDeleter d = new IndexFileDeleter(dir, new KeepOnlyLastCommitDeletionPolicy(), infos, null);
+          IndexFileDeleter d = new IndexFileDeleter(dir, new KeepOnlyLastCommitDeletionPolicy(), infos, null, null);
           String[] endFiles = dir.list();
 
           Arrays.sort(startFiles);
Index: src/test/org/apache/lucene/index/TestIndexWriter.java
===================================================================
--- src/test/org/apache/lucene/index/TestIndexWriter.java	(revision 523296)
+++ src/test/org/apache/lucene/index/TestIndexWriter.java	(working copy)
@@ -461,7 +461,7 @@
       String[] startFiles = dir.list();
       SegmentInfos infos = new SegmentInfos();
       infos.read(dir);
-      IndexFileDeleter d = new IndexFileDeleter(dir, new KeepOnlyLastCommitDeletionPolicy(), infos, null);
+      IndexFileDeleter d = new IndexFileDeleter(dir, new KeepOnlyLastCommitDeletionPolicy(), infos, null, null);
       String[] endFiles = dir.list();
 
       Arrays.sort(startFiles);
@@ -842,6 +842,7 @@
     public void testCommitOnCloseAbort() throws IOException {
       Directory dir = new RAMDirectory();      
       IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(), true);
+      writer.setMaxBufferedDocs(10);
       for (int i = 0; i < 14; i++) {
         addDoc(writer);
       }
@@ -854,6 +855,7 @@
       searcher.close();
 
       writer = new IndexWriter(dir, false, new WhitespaceAnalyzer(), false);
+      writer.setMaxBufferedDocs(10);
       for(int j=0;j<17;j++) {
         addDoc(writer);
       }
@@ -878,6 +880,7 @@
       // Now make sure we can re-open the index, add docs,
       // and all is good:
       writer = new IndexWriter(dir, false, new WhitespaceAnalyzer(), false);
+      writer.setMaxBufferedDocs(10);
       for(int i=0;i<12;i++) {
         for(int j=0;j<17;j++) {
           addDoc(writer);
@@ -945,6 +948,7 @@
     public void testCommitOnCloseOptimize() throws IOException {
       RAMDirectory dir = new RAMDirectory();      
       IndexWriter writer  = new IndexWriter(dir, new WhitespaceAnalyzer(), true);
+      writer.setMaxBufferedDocs(10);
       for(int j=0;j<17;j++) {
         addDocWithIndex(writer, j);
       }
Index: src/test/org/apache/lucene/index/TestStressIndexing.java
===================================================================
--- src/test/org/apache/lucene/index/TestStressIndexing.java	(revision 523296)
+++ src/test/org/apache/lucene/index/TestStressIndexing.java	(working copy)
@@ -74,8 +74,6 @@
           count++;
         }
         
-        modifier.close();
-
       } catch (Exception e) {
         System.out.println(e.toString());
         e.printStackTrace();
@@ -125,6 +123,9 @@
     IndexerThread indexerThread = new IndexerThread(modifier);
     indexerThread.start();
       
+    IndexerThread indexerThread2 = new IndexerThread(modifier);
+    indexerThread2.start();
+      
     // Two searchers that constantly just re-instantiate the searcher:
     SearcherThread searcherThread1 = new SearcherThread(directory);
     searcherThread1.start();
@@ -133,9 +134,14 @@
     searcherThread2.start();
 
     indexerThread.join();
+    indexerThread2.join();
     searcherThread1.join();
     searcherThread2.join();
+
+    modifier.close();
+
     assertTrue("hit unexpected exception in indexer", !indexerThread.failed);
+    assertTrue("hit unexpected exception in indexer 2", !indexerThread2.failed);
     assertTrue("hit unexpected exception in search1", !searcherThread1.failed);
     assertTrue("hit unexpected exception in search2", !searcherThread2.failed);
     //System.out.println("    Writer: " + indexerThread.count + " iterations");
Index: src/test/org/apache/lucene/index/TestIndexFileDeleter.java
===================================================================
--- src/test/org/apache/lucene/index/TestIndexFileDeleter.java	(revision 523296)
+++ src/test/org/apache/lucene/index/TestIndexFileDeleter.java	(working copy)
@@ -34,6 +34,7 @@
     Directory dir = new RAMDirectory();
 
     IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(), true);
+    writer.setMaxBufferedDocs(10);
     int i;
     for(i=0;i<35;i++) {
       addDoc(writer, i);
Index: src/test/org/apache/lucene/index/TestDeletionPolicy.java
===================================================================
--- src/test/org/apache/lucene/index/TestDeletionPolicy.java	(revision 523296)
+++ src/test/org/apache/lucene/index/TestDeletionPolicy.java	(working copy)
@@ -254,6 +254,7 @@
       Directory dir = new RAMDirectory();
 
       IndexWriter writer = new IndexWriter(dir, autoCommit, new WhitespaceAnalyzer(), true, policy);
+      writer.setMaxBufferedDocs(10);
       writer.setUseCompoundFile(useCompoundFile);
       for(int i=0;i<107;i++) {
         addDoc(writer);
@@ -271,7 +272,7 @@
       } else {
         // If we are not auto committing then there should
         // be exactly 2 commits (one per close above):
-        assertEquals(2, policy.numOnCommit);
+        assertEquals(autoCommit?2:1, policy.numOnCommit);
       }
 
       // Simplistic check: just verify all segments_N's still
@@ -316,6 +317,7 @@
       Directory dir = new RAMDirectory();
 
       IndexWriter writer = new IndexWriter(dir, autoCommit, new WhitespaceAnalyzer(), true, policy);
+      writer.setMaxBufferedDocs(10);
       writer.setUseCompoundFile(useCompoundFile);
       for(int i=0;i<107;i++) {
         addDoc(writer);
@@ -333,13 +335,15 @@
       } else {
         // If we are not auto committing then there should
         // be exactly 2 commits (one per close above):
-        assertEquals(2, policy.numOnCommit);
+        assertEquals(autoCommit?2:1, policy.numOnCommit);
       }
 
-      // Simplistic check: just verify the index is in fact
-      // readable:
-      IndexReader reader = IndexReader.open(dir);
-      reader.close();
+      if (autoCommit) {
+        // Simplistic check: just verify the index is in fact
+        // readable:
+        IndexReader reader = IndexReader.open(dir);
+        reader.close();
+      }
 
       dir.close();
     }
@@ -363,6 +367,7 @@
 
       for(int j=0;j<N+1;j++) {
         IndexWriter writer = new IndexWriter(dir, autoCommit, new WhitespaceAnalyzer(), true, policy);
+        writer.setMaxBufferedDocs(10);
         writer.setUseCompoundFile(useCompoundFile);
         for(int i=0;i<17;i++) {
           addDoc(writer);
@@ -523,6 +528,7 @@
 
       Directory dir = new RAMDirectory();
       IndexWriter writer = new IndexWriter(dir, autoCommit, new WhitespaceAnalyzer(), true, policy);
+      writer.setMaxBufferedDocs(10);
       writer.setUseCompoundFile(useCompoundFile);
       writer.close();
       Term searchTerm = new Term("content", "aaa");        
@@ -531,6 +537,7 @@
       for(int i=0;i<N+1;i++) {
 
         writer = new IndexWriter(dir, autoCommit, new WhitespaceAnalyzer(), false, policy);
+        writer.setMaxBufferedDocs(10);
         writer.setUseCompoundFile(useCompoundFile);
         for(int j=0;j<17;j++) {
           addDoc(writer);
Index: src/test/org/apache/lucene/index/TestIndexModifier.java
===================================================================
--- src/test/org/apache/lucene/index/TestIndexModifier.java	(revision 523296)
+++ src/test/org/apache/lucene/index/TestIndexModifier.java	(working copy)
@@ -74,7 +74,7 @@
     //  Lucene defaults:
     assertNull(i.getInfoStream());
     assertTrue(i.getUseCompoundFile());
-    assertEquals(10, i.getMaxBufferedDocs());
+    assertEquals(0, i.getMaxBufferedDocs());
     assertEquals(10000, i.getMaxFieldLength());
     assertEquals(10, i.getMergeFactor());
     // test setting properties:
Index: src/test/org/apache/lucene/index/TestBackwardsCompatibility.java
===================================================================
--- src/test/org/apache/lucene/index/TestBackwardsCompatibility.java	(revision 523296)
+++ src/test/org/apache/lucene/index/TestBackwardsCompatibility.java	(working copy)
@@ -314,14 +314,21 @@
         reader.setNorm(21, "content", (float) 1.5);
         reader.close();
 
+        /*
+        String[] x = dir.list();
+        for(int i=0;i<x.length;i++) {
+          System.out.println("  " + i + ": " + x[i]);
+        }
+        */
+
         // The numbering of fields can vary depending on which
         // JRE is in use.  On some JREs we see content bound to
         // field 0; on others, field 1.  So, here we have to
         // figure out which field number corresponds to
         // "content", and then set our expected file names below
         // accordingly:
-        CompoundFileReader cfsReader = new CompoundFileReader(dir, "_2.cfs");
-        FieldInfos fieldInfos = new FieldInfos(cfsReader, "_2.fnm");
+        CompoundFileReader cfsReader = new CompoundFileReader(dir, "_0.cfs");
+        FieldInfos fieldInfos = new FieldInfos(cfsReader, "_0.fnm");
         int contentFieldIndex = -1;
         for(int i=0;i<fieldInfos.size();i++) {
           FieldInfo fi = fieldInfos.fieldInfo(i);
@@ -336,14 +343,11 @@
         // Now verify file names:
         String[] expected = {"_0.cfs",
                              "_0_1.del",
-                             "_1.cfs",
-                             "_2.cfs",
-                             "_2_1.s" + contentFieldIndex,
-                             "_3.cfs",
-                             "segments_a",
+                             "_0_1.s" + contentFieldIndex,
+                             "segments_4",
                              "segments.gen"};
         if (!autoCommit) {
-          expected[6] = "segments_3";
+          expected[3] = "segments_3";
         }
 
         String[] actual = dir.list();
Index: src/test/org/apache/lucene/index/TestLazyProxSkipping.java
===================================================================
--- src/test/org/apache/lucene/index/TestLazyProxSkipping.java	(revision 523296)
+++ src/test/org/apache/lucene/index/TestLazyProxSkipping.java	(working copy)
@@ -50,7 +50,7 @@
         
         Directory directory = new RAMDirectory();
         IndexWriter writer = new IndexWriter(directory, new WhitespaceAnalyzer(), true);
-        
+        writer.setMaxBufferedDocs(10);
         for (int i = 0; i < numDocs; i++) {
             Document doc = new Document();
             String content;
Index: src/java/org/apache/lucene/analysis/SimpleSpaceTokenizer.java
===================================================================
--- src/java/org/apache/lucene/analysis/SimpleSpaceTokenizer.java	(revision 0)
+++ src/java/org/apache/lucene/analysis/SimpleSpaceTokenizer.java	(revision 0)
@@ -0,0 +1,97 @@
+package org.apache.lucene.analysis;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.Reader;
+import java.io.IOException;
+
+public class SimpleSpaceTokenizer extends Tokenizer {
+
+  Token t;
+
+  public SimpleSpaceTokenizer() {
+    super(null);
+    t = new Token(null, 0, 0);
+  }
+
+  public void init(Reader input) {
+    this.input = input;
+    offset = 0;
+    bufferIndex = 0;
+    dataLen = 0;
+    t.termBuffer = buffer;
+    t.termBufferOffset = 0;
+  }
+
+  private int offset = 0, bufferIndex = 0, dataLen = 0;
+  private static final int MAX_WORD_LEN = 255;
+  private static final int IO_BUFFER_SIZE = 1024;
+  private final char[] buffer = new char[MAX_WORD_LEN];
+  private final char[] ioBuffer = new char[IO_BUFFER_SIZE];
+
+  /** Called on each token character to normalize it before it is added to the
+   * token.  The default implementation does nothing.  Subclasses may use this
+   * to, e.g., lowercase tokens. */
+  protected char normalize(char c) {
+    return c;
+  }
+
+  /** Returns the next token in the stream, or null at EOS. */
+  public final Token next() throws IOException {
+    int length = 0;
+    int start = offset;
+    while (true) {
+      final char c;
+
+      offset++;
+      if (bufferIndex >= dataLen) {
+        dataLen = input.read(ioBuffer);
+        bufferIndex = 0;
+      }
+
+      if (dataLen == -1) {
+        if (length > 0)
+          break;
+        else
+          return null;
+      } else
+        c = ioBuffer[bufferIndex++];
+
+      if (c != ' ') {               // if it's a token char
+
+        if (length == 0)			           // start of token
+          start = offset - 1;
+
+        buffer[length++] = c;
+
+        if (length == MAX_WORD_LEN)		   // buffer overflow!
+          break;
+
+      } else if (length > 0)             // at non-Letter w/ chars
+        break;                           // return 'em
+    }
+
+    // t.termText = new String(buffer, 0, length);
+    t.termBufferLength = length;
+    t.startOffset = start;
+    t.endOffset = start+length;
+
+    return t;
+  }
+}
+

Property changes on: src/java/org/apache/lucene/analysis/SimpleSpaceTokenizer.java
___________________________________________________________________
Name: svn:eol-style
   + native

Index: src/java/org/apache/lucene/analysis/SimpleSpaceAnalyzer.java
===================================================================
--- src/java/org/apache/lucene/analysis/SimpleSpaceAnalyzer.java	(revision 0)
+++ src/java/org/apache/lucene/analysis/SimpleSpaceAnalyzer.java	(revision 0)
@@ -0,0 +1,35 @@
+package org.apache.lucene.analysis;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.Reader;
+
+/** An Analyzer that uses SimpleSpaceTokenizer. */
+
+public final class SimpleSpaceAnalyzer extends Analyzer {
+  private ThreadLocal tokenizers = new ThreadLocal();
+  public TokenStream tokenStream(String fieldName, Reader reader) {
+    SimpleSpaceTokenizer s = (SimpleSpaceTokenizer) tokenizers.get();
+    if (s == null) {
+      s = new SimpleSpaceTokenizer();
+      tokenizers.set(s);
+    }
+    s.init(reader);
+    return s;
+  }
+}

Property changes on: src/java/org/apache/lucene/analysis/SimpleSpaceAnalyzer.java
___________________________________________________________________
Name: svn:eol-style
   + native

Index: src/java/org/apache/lucene/analysis/Token.java
===================================================================
--- src/java/org/apache/lucene/analysis/Token.java	(revision 523296)
+++ src/java/org/apache/lucene/analysis/Token.java	(working copy)
@@ -56,7 +56,13 @@
   String type = "word";				  // lexical type
   
   Payload payload;
-  
+
+  // For better indexing performance, set buffer & length
+  // instead of termText
+  char[] termBuffer;
+  int termBufferOffset;
+  int termBufferLength;
+
   private int positionIncrement = 1;
 
   /** Constructs a Token with the given term text, and start & end offsets.
@@ -67,6 +73,17 @@
     endOffset = end;
   }
 
+  /** Constructs a Token with the given term text buffer
+      starting at offset for length lenth, and start & end offsets.
+      The type defaults to "word." */
+  public Token(char[] text, int offset, int length, int start, int end) {
+    termBuffer = text;
+    termBufferOffset = offset;
+    termBufferLength = length;
+    startOffset = start;
+    endOffset = end;
+  }
+
   /** Constructs a Token with the given text, start and end offsets, & type. */
   public Token(String text, int start, int end, String typ) {
     termText = text;
@@ -75,6 +92,19 @@
     type = typ;
   }
 
+  /** Constructs a Token with the given term text buffer
+      starting at offset for length lenth, and start & end
+      offsets, & type. */
+  public Token(char[] text, int offset, int length, int start, int end, String typ) {
+    termBuffer = text;
+    termBufferOffset = offset;
+    termBufferLength = length;
+    startOffset = start;
+    endOffset = end;
+    type = typ;
+  }
+
+
   /** Set the position increment.  This determines the position of this token
    * relative to the previous Token in a {@link TokenStream}, used in phrase
    * searching.
@@ -119,7 +149,17 @@
 
   /** Returns the Token's term text. */
   public final String termText() { return termText; }
+  public final char[] termBuffer() { return termBuffer; }
+  public final int termBufferOffset() { return termBufferOffset; }
+  public final int termBufferLength() { return termBufferLength; }
 
+  public final void setTermBuffer(char[] buffer, int offset, int length) {
+    this.termBuffer = buffer;
+    this.termBufferOffset = offset;
+    this.termBufferLength = length;
+  }
+    
+
   /** Returns this Token's starting offset, the position of the first character
     corresponding to this token in the source text.
 
Index: src/java/org/apache/lucene/index/FieldInfos.java
===================================================================
--- src/java/org/apache/lucene/index/FieldInfos.java	(revision 523296)
+++ src/java/org/apache/lucene/index/FieldInfos.java	(working copy)
@@ -156,7 +156,7 @@
    * @param omitNorms true if the norms for the indexed field should be omitted
    */
   public void add(String name, boolean isIndexed, boolean storeTermVector,
-                  boolean storePositionWithTermVector, boolean storeOffsetWithTermVector, boolean omitNorms) {
+                       boolean storePositionWithTermVector, boolean storeOffsetWithTermVector, boolean omitNorms) {
     add(name, isIndexed, storeTermVector, storePositionWithTermVector,
         storeOffsetWithTermVector, omitNorms, false);
   }
@@ -174,12 +174,13 @@
    * @param omitNorms true if the norms for the indexed field should be omitted
    * @param storePayloads true if payloads should be stored for this field
    */
-  public void add(String name, boolean isIndexed, boolean storeTermVector,
-                  boolean storePositionWithTermVector, boolean storeOffsetWithTermVector,
-                  boolean omitNorms, boolean storePayloads) {
+  // nocommit: API change, but, not yet released
+  public FieldInfo add(String name, boolean isIndexed, boolean storeTermVector,
+                       boolean storePositionWithTermVector, boolean storeOffsetWithTermVector,
+                       boolean omitNorms, boolean storePayloads) {
     FieldInfo fi = fieldInfo(name);
     if (fi == null) {
-      addInternal(name, isIndexed, storeTermVector, storePositionWithTermVector, storeOffsetWithTermVector, omitNorms, storePayloads);
+      return addInternal(name, isIndexed, storeTermVector, storePositionWithTermVector, storeOffsetWithTermVector, omitNorms, storePayloads);
     } else {
       if (fi.isIndexed != isIndexed) {
         fi.isIndexed = true;                      // once indexed, always index
@@ -199,19 +200,20 @@
       if (fi.storePayloads != storePayloads) {
         fi.storePayloads = true;
       }
-
     }
+    return fi;
   }
 
-
-  private void addInternal(String name, boolean isIndexed,
-                           boolean storeTermVector, boolean storePositionWithTermVector, 
-                           boolean storeOffsetWithTermVector, boolean omitNorms, boolean storePayloads) {
+  // nocommit: API change
+  private FieldInfo addInternal(String name, boolean isIndexed,
+                                boolean storeTermVector, boolean storePositionWithTermVector, 
+                                boolean storeOffsetWithTermVector, boolean omitNorms, boolean storePayloads) {
     FieldInfo fi =
       new FieldInfo(name, isIndexed, byNumber.size(), storeTermVector, storePositionWithTermVector,
               storeOffsetWithTermVector, omitNorms, storePayloads);
     byNumber.add(fi);
     byName.put(name, fi);
+    return fi;
   }
 
   public int fieldNumber(String fieldName) {
Index: src/java/org/apache/lucene/index/IndexReader.java
===================================================================
--- src/java/org/apache/lucene/index/IndexReader.java	(revision 523296)
+++ src/java/org/apache/lucene/index/IndexReader.java	(working copy)
@@ -771,7 +771,7 @@
         // KeepOnlyLastCommitDeleter:
         IndexFileDeleter deleter =  new IndexFileDeleter(directory,
                                                          deletionPolicy == null ? new KeepOnlyLastCommitDeletionPolicy() : deletionPolicy,
-                                                         segmentInfos, null);
+                                                         segmentInfos, null, null);
 
         // Checkpoint the state we are about to change, in
         // case we have to roll back:
Index: src/java/org/apache/lucene/index/IndexFileNames.java
===================================================================
--- src/java/org/apache/lucene/index/IndexFileNames.java	(revision 523296)
+++ src/java/org/apache/lucene/index/IndexFileNames.java	(working copy)
@@ -50,6 +50,9 @@
   /** Extension of separate norms */
   static final String SEPARATE_NORMS_EXTENSION = "s";
 
+  /** Extension of flushed temp segment data */
+  static final String TEMP_FLUSH_EXTENSION = "tfp";
+
   /**
    * This array contains all filename extensions used by
    * Lucene's index files, with two exceptions, namely the
@@ -60,7 +63,7 @@
    */
   static final String INDEX_EXTENSIONS[] = new String[] {
       "cfs", "fnm", "fdx", "fdt", "tii", "tis", "frq", "prx", "del",
-      "tvx", "tvd", "tvf", "gen", "nrm" 
+      "tvx", "tvd", "tvf", "gen", "nrm", "tfp"
   };
 
   /** File extensions that are added to a compound file
Index: src/java/org/apache/lucene/index/SegmentTermDocs.java
===================================================================
--- src/java/org/apache/lucene/index/SegmentTermDocs.java	(revision 523296)
+++ src/java/org/apache/lucene/index/SegmentTermDocs.java	(working copy)
@@ -51,6 +51,7 @@
   }
 
   public void seek(Term term) throws IOException {
+    //System.out.println("std: seek term " + term);
     TermInfo ti = parent.tis.get(term);
     seek(ti, term);
   }
@@ -74,6 +75,7 @@
 
   void seek(TermInfo ti, Term term) throws IOException {
     count = 0;
+    //System.out.println("std: seek " + ti);
     payloadLengthAtLastSkip = 0;
     FieldInfo fi = parent.fieldInfos.fieldInfo(term.field);
     currentFieldStoresPayloads = (fi != null) ? fi.storePayloads : false;
@@ -81,6 +83,7 @@
       df = 0;
     } else {
       df = ti.docFreq;
+      //System.out.println("  df = " + df);
       doc = 0;
       skipDoc = 0;
       skipCount = 0;
@@ -90,6 +93,7 @@
       skipPointer = freqPointer + ti.skipOffset;
       freqStream.seek(freqPointer);
       haveSkipped = false;
+      //System.out.println("  freqP = " + freqPointer + "; proxP = " + proxPointer);
     }
   }
 
@@ -106,6 +110,7 @@
   }
 
   public boolean next() throws IOException {
+    //System.out.println("std: now do next");
     while (true) {
       if (count == df)
         return false;
@@ -118,6 +123,7 @@
         freq = freqStream.readVInt();		  // else read freq
 
       count++;
+      //System.out.println("std: read " + docCode + "; freq= " + freq);
 
       if (deletedDocs == null || !deletedDocs.get(doc))
         break;
@@ -131,18 +137,22 @@
           throws IOException {
     final int length = docs.length;
     int i = 0;
+    //System.out.println("read bulk: df =" + df + "; length=" + length);
     while (i < length && count < df) {
 
       // manually inlined call to next() for speed
       final int docCode = freqStream.readVInt();
+      //System.out.println("  read code: " + docCode);
       doc += docCode >>> 1;			  // shift off low bit
       if ((docCode & 1) != 0)			  // if low bit is set
         freq = 1;				  // freq is one
       else
         freq = freqStream.readVInt();		  // else read freq
       count++;
+      // //System.out.println("  read freq " + freq);
 
       if (deletedDocs == null || !deletedDocs.get(doc)) {
+        //System.out.println("  add " + doc + "; freq=" + freq);
         docs[i] = doc;
         freqs[i] = freq;
         ++i;
@@ -156,7 +166,9 @@
 
   /** Optimized implementation. */
   public boolean skipTo(int target) throws IOException {
+    //System.out.println("std skip to " + target);
     if (df >= skipInterval) {                      // optimized case
+      //System.out.println("  is frequent enough");
 
       if (skipStream == null)
         skipStream = (IndexInput) freqStream.clone(); // lazily clone
@@ -172,6 +184,7 @@
       long lastFreqPointer = freqStream.getFilePointer();
       long lastProxPointer = -1;
       int numSkipped = -1 - (count % skipInterval);
+      //System.out.println("  target " + target + "; skipDoc " + skipDoc);
 
       while (target > skipDoc) {
         lastSkipDoc = skipDoc;
@@ -203,11 +216,13 @@
         freqPointer += skipStream.readVInt();
         proxPointer += skipStream.readVInt();
 
+        //System.out.println("  now freq " + freqPointer + " prox " + proxPointer);
         skipCount++;
       }
       
       // if we found something to skip, then skip it
       if (lastFreqPointer > freqStream.getFilePointer()) {
+        //System.out.println("  do skip!  " + lastFreqPointer);
         freqStream.seek(lastFreqPointer);
         skipProx(lastProxPointer, lastPayloadLength);
 
@@ -219,6 +234,7 @@
 
     // done skipping, now just scan
     do {
+      //System.out.println("  now scan " + target + " " + doc);
       if (!next())
         return false;
     } while (target > doc);
Index: src/java/org/apache/lucene/index/FieldsWriter.java
===================================================================
--- src/java/org/apache/lucene/index/FieldsWriter.java	(revision 523296)
+++ src/java/org/apache/lucene/index/FieldsWriter.java	(working copy)
@@ -38,17 +38,90 @@
 
     private IndexOutput indexStream;
 
+    private boolean doClose;
+
     FieldsWriter(Directory d, String segment, FieldInfos fn) throws IOException {
         fieldInfos = fn;
         fieldsStream = d.createOutput(segment + ".fdt");
         indexStream = d.createOutput(segment + ".fdx");
+        doClose = true;
     }
 
+    FieldsWriter(IndexOutput fdx, IndexOutput fdt, FieldInfos fn) throws IOException {
+        fieldInfos = fn;
+        fieldsStream = fdt;
+        indexStream = fdx;
+        doClose = false;
+    }
+    IndexOutput getIndexStream() {
+      return indexStream;
+    }
+    IndexOutput getFieldsStream() {
+      return fieldsStream;
+    }
+
     final void close() throws IOException {
+      if (doClose) {
         fieldsStream.close();
         indexStream.close();
+      }
     }
 
+    final void writeField(FieldInfo fi, Fieldable field) throws IOException {
+      // if the field as an instanceof FieldsReader.FieldForMerge, we're in merge mode
+      // and field.binaryValue() already returns the compressed value for a field
+      // with isCompressed()==true, so we disable compression in that case
+      boolean disableCompression = (field instanceof FieldsReader.FieldForMerge);
+      fieldsStream.writeVInt(fi.number);
+      // System.out.println("  write field number " + fieldInfos.fieldNumber(field.name()) + " name " + field.name() + " to " + fieldsStream + " at " + fieldsStream.getFilePointer());
+      byte bits = 0;
+      if (field.isTokenized())
+        bits |= FieldsWriter.FIELD_IS_TOKENIZED;
+      if (field.isBinary())
+        bits |= FieldsWriter.FIELD_IS_BINARY;
+      if (field.isCompressed())
+        bits |= FieldsWriter.FIELD_IS_COMPRESSED;
+                
+      fieldsStream.writeByte(bits);
+                
+      if (field.isCompressed()) {
+        // compression is enabled for the current field
+        byte[] data = null;
+                  
+        if (disableCompression) {
+          // optimized case for merging, the data
+          // is already compressed
+          data = field.binaryValue();
+        } else {
+          // check if it is a binary field
+          if (field.isBinary()) {
+            data = compress(field.binaryValue());
+          }
+          else {
+            data = compress(field.stringValue().getBytes("UTF-8"));
+          }
+        }
+        final int len = data.length;
+        // System.out.println("    compressed: " + len);
+        fieldsStream.writeVInt(len);
+        fieldsStream.writeBytes(data, len);
+      }
+      else {
+        // compression is disabled for the current field
+        if (field.isBinary()) {
+          byte[] data = field.binaryValue();
+          final int len = data.length;
+          // System.out.println("    not compressed: " + len);
+          fieldsStream.writeVInt(len);
+          fieldsStream.writeBytes(data, len);
+        }
+        else {
+          fieldsStream.writeString(field.stringValue());
+        }
+      }
+      // System.out.println("    fieldsStream now at " + fieldsStream.getFilePointer());
+    }
+
     final void addDocument(Document doc) throws IOException {
         indexStream.writeLong(fieldsStream.getFilePointer());
 
@@ -59,62 +132,14 @@
             if (field.isStored())
                 storedCount++;
         }
+        // System.out.println("write " + storedCount + " fields to " + fieldsStream + " at " + fieldsStream.getFilePointer());
         fieldsStream.writeVInt(storedCount);
 
         fieldIterator = doc.getFields().iterator();
         while (fieldIterator.hasNext()) {
             Fieldable field = (Fieldable) fieldIterator.next();
-            // if the field as an instanceof FieldsReader.FieldForMerge, we're in merge mode
-            // and field.binaryValue() already returns the compressed value for a field
-            // with isCompressed()==true, so we disable compression in that case
-            boolean disableCompression = (field instanceof FieldsReader.FieldForMerge);
-            if (field.isStored()) {
-                fieldsStream.writeVInt(fieldInfos.fieldNumber(field.name()));
-
-                byte bits = 0;
-                if (field.isTokenized())
-                    bits |= FieldsWriter.FIELD_IS_TOKENIZED;
-                if (field.isBinary())
-                    bits |= FieldsWriter.FIELD_IS_BINARY;
-                if (field.isCompressed())
-                    bits |= FieldsWriter.FIELD_IS_COMPRESSED;
-                
-                fieldsStream.writeByte(bits);
-                
-                if (field.isCompressed()) {
-                  // compression is enabled for the current field
-                  byte[] data = null;
-                  
-                  if (disableCompression) {
-                      // optimized case for merging, the data
-                      // is already compressed
-                      data = field.binaryValue();
-                  } else {
-                      // check if it is a binary field
-                      if (field.isBinary()) {
-                        data = compress(field.binaryValue());
-                      }
-                      else {
-                        data = compress(field.stringValue().getBytes("UTF-8"));
-                      }
-                  }
-                  final int len = data.length;
-                  fieldsStream.writeVInt(len);
-                  fieldsStream.writeBytes(data, len);
-                }
-                else {
-                  // compression is disabled for the current field
-                  if (field.isBinary()) {
-                    byte[] data = field.binaryValue();
-                    final int len = data.length;
-                    fieldsStream.writeVInt(len);
-                    fieldsStream.writeBytes(data, len);
-                  }
-                  else {
-                    fieldsStream.writeString(field.stringValue());
-                  }
-                }
-            }
+            if (field.isStored())
+              writeField(fieldInfos.fieldInfo(field.name()), field);
         }
     }
 
Index: src/java/org/apache/lucene/index/IndexWriter.java
===================================================================
--- src/java/org/apache/lucene/index/IndexWriter.java	(revision 523296)
+++ src/java/org/apache/lucene/index/IndexWriter.java	(working copy)
@@ -60,10 +60,11 @@
   (which just deletes and then adds). When finished adding, deleting and updating documents, <a href="#close()"><b>close</b></a> should be called.</p>
 
   <p>These changes are buffered in memory and periodically
-  flushed to the {@link Directory} (during the above method calls).  A flush is triggered when there are
-  enough buffered deletes (see {@link
-  #setMaxBufferedDeleteTerms}) or enough added documents
-  (see {@link #setMaxBufferedDocs}) since the last flush,
+  flushed to the {@link Directory} (during the above method
+  calls).  A flush is triggered when there are enough
+  buffered deletes (see {@link #setMaxBufferedDeleteTerms})
+  or enough added documents (see {@link #setMaxBufferedDocs}
+  and {@link #setRAMBufferSizeMB}) since the last flush,
   whichever is sooner.  When a flush occurs, both pending
   deletes and added documents are flushed to the index.  A
   flush may also trigger one or more segment merges.</p>
@@ -171,11 +172,17 @@
   public final static int DEFAULT_MERGE_FACTOR = 10;
 
   /**
-   * Default value is 10. Change using {@link #setMaxBufferedDocs(int)}.
+   * Default value is 0 (meaning flush is based on RAM usage
+   * by default). Change using {@link #setMaxBufferedDocs}.
    */
-  public final static int DEFAULT_MAX_BUFFERED_DOCS = 10;
+  public final static int DEFAULT_MAX_BUFFERED_DOCS = 0;
 
   /**
+   * Default value is 16 MB.   Change using {@link #setRAMBufferSizeMB}.
+   */
+  public final static float DEFAULT_RAM_BUFFER_SIZE_MB = 16F;
+
+  /**
    * Default value is 1000. Change using {@link #setMaxBufferedDeleteTerms(int)}.
    */
   public final static int DEFAULT_MAX_BUFFERED_DELETE_TERMS = 1000;
@@ -208,8 +215,7 @@
   private boolean autoCommit = true;              // false if we should commit only on close
 
   SegmentInfos segmentInfos = new SegmentInfos();       // the segments
-  SegmentInfos ramSegmentInfos = new SegmentInfos();    // the segments in ramDirectory
-  private final RAMDirectory ramDirectory = new RAMDirectory(); // for temp segs
+  private MultiDocumentWriter docWriter;
   private IndexFileDeleter deleter;
 
   private Lock writeLock;
@@ -563,6 +569,10 @@
     }
   }
 
+  IndexFileDeleter getDeleter() {
+    return deleter;
+  }
+
   private void init(Directory d, Analyzer a, final boolean create, boolean closeDir, IndexDeletionPolicy deletionPolicy, boolean autoCommit)
     throws CorruptIndexException, LockObtainFailedException, IOException {
     this.closeDir = closeDir;
@@ -602,11 +612,14 @@
         rollbackSegmentInfos = (SegmentInfos) segmentInfos.clone();
       }
 
+      docWriter = new MultiDocumentWriter(newSegmentName(), directory, this, !autoCommit);
+      docWriter.setInfoStream(infoStream);
+
       // Default deleter (for backwards compatibility) is
       // KeepOnlyLastCommitDeleter:
       deleter = new IndexFileDeleter(directory,
                                      deletionPolicy == null ? new KeepOnlyLastCommitDeletionPolicy() : deletionPolicy,
-                                     segmentInfos, infoStream);
+                                     segmentInfos, infoStream, docWriter);
 
     } catch (IOException e) {
       this.writeLock.release();
@@ -661,19 +674,26 @@
   }
 
   /** Determines the minimal number of documents required before the buffered
-   * in-memory documents are merged and a new Segment is created.
+   * in-memory documents are flushed as a new Segment.
    * Since Documents are merged in a {@link org.apache.lucene.store.RAMDirectory},
    * large value gives faster indexing.  At the same time, mergeFactor limits
    * the number of files open in a FSDirectory.
    *
-   * <p> The default value is 10.
+   * <p>If this is 0, then the RAM buffer is flushed instead
+   * by overally RAM usage (see {@link
+   * #setRAMBufferSizeMB}).  If this is non-zero, then
+   * flushing is triggered by maxBufferedDocs and not by
+   * overall RAM usage.</p>
    *
-   * @throws IllegalArgumentException if maxBufferedDocs is smaller than 2
+   * <p> The default value is 0.</p>
+   *
+   * @throws IllegalArgumentException if maxBufferedDocs is
+   * non-zero and smaller than 2
    */
   public void setMaxBufferedDocs(int maxBufferedDocs) {
     ensureOpen();
-    if (maxBufferedDocs < 2)
-      throw new IllegalArgumentException("maxBufferedDocs must at least be 2");
+    if (maxBufferedDocs != 0 && maxBufferedDocs < 2)
+      throw new IllegalArgumentException("maxBufferedDocs must at least be 2 or 0 to disable");
     this.minMergeDocs = maxBufferedDocs;
   }
 
@@ -685,7 +705,28 @@
     return minMergeDocs;
   }
 
+  /** Determines the amount of RAM that may be used for
+   * buffering before the in-memory documents are flushed as
+   * a new Segment.  This only applies when maxBufferedDocs
+   * is set to 0.  Generally for faster indexing performance
+   * it's best to flush by RAM usage instead of document
+   * count.
+   */
+  public void setRAMBufferSizeMB(float mb) {
+    if (mb < 1)
+      throw new IllegalArgumentException("ramBufferSize must at least be 1 MB");
+    ramBufferSize = mb*1024F*1024F;
+    docWriter.setRAMBufferSizeMB(mb);
+  }
+
   /**
+   * @see #setRAMBufferSizeMB
+   */
+  public float getRAMBufferSizeMB() {
+    return ramBufferSize/1024F/1024F;
+  }
+
+  /**
    * <p>Determines the minimal number of delete terms required before the buffered
    * in-memory delete terms are applied and flushed. If there are documents
    * buffered in memory at the time, they are merged and a new segment is
@@ -756,7 +797,9 @@
   public void setInfoStream(PrintStream infoStream) {
     ensureOpen();
     this.infoStream = infoStream;
-    deleter.setInfoStream(infoStream);
+    docWriter.setInfoStream(infoStream);
+    // nocommit
+    //deleter.setInfoStream(infoStream);
   }
 
   /**
@@ -835,7 +878,7 @@
    */
   public synchronized void close() throws CorruptIndexException, IOException {
     if (!closed) {
-      flushRamSegments();
+      flush();
 
       if (commitPending) {
         segmentInfos.write(directory);         // now commit changes
@@ -844,7 +887,6 @@
         rollbackSegmentInfos = null;
       }
 
-      ramDirectory.close();
       if (writeLock != null) {
         writeLock.release();                          // release write lock
         writeLock = null;
@@ -884,7 +926,7 @@
   /** Returns the number of documents currently in this index. */
   public synchronized int docCount() {
     ensureOpen();
-    int count = ramSegmentInfos.size();
+    int count = docWriter.docID;
     for (int i = 0; i < segmentInfos.size(); i++) {
       SegmentInfo si = segmentInfos.info(i);
       count += si.docCount;
@@ -962,24 +1004,13 @@
    */
   public void addDocument(Document doc, Analyzer analyzer) throws CorruptIndexException, IOException {
     ensureOpen();
-    SegmentInfo newSegmentInfo = buildSingleDocSegment(doc, analyzer);
-    synchronized (this) {
-      ramSegmentInfos.addElement(newSegmentInfo);
-      maybeFlushRamSegments();
-    }
+    docWriter.addDocument(doc, analyzer);
+    // For the non-autoCommit case, MultiDocumentWriter
+    // takes care of flushing its pending state to disk
+    if (autoCommit)
+      maybeFlush();
   }
 
-  SegmentInfo buildSingleDocSegment(Document doc, Analyzer analyzer)
-      throws CorruptIndexException, IOException {
-    DocumentWriter dw = new DocumentWriter(ramDirectory, analyzer, this);
-    dw.setInfoStream(infoStream);
-    String segmentName = newRamSegmentName();
-    dw.addDocument(segmentName, doc);
-    SegmentInfo si = new SegmentInfo(segmentName, 1, ramDirectory, false, false);
-    si.setNumFields(dw.getNumFields());
-    return si;
-  }
-
   /**
    * Deletes the document(s) containing <code>term</code>.
    * @param term the term to identify the documents to be deleted
@@ -989,7 +1020,7 @@
   public synchronized void deleteDocuments(Term term) throws CorruptIndexException, IOException {
     ensureOpen();
     bufferDeleteTerm(term);
-    maybeFlushRamSegments();
+    maybeFlush();
   }
 
   /**
@@ -1005,7 +1036,7 @@
     for (int i = 0; i < terms.length; i++) {
       bufferDeleteTerm(terms[i]);
     }
-    maybeFlushRamSegments();
+    maybeFlush();
   }
 
   /**
@@ -1041,26 +1072,23 @@
   public void updateDocument(Term term, Document doc, Analyzer analyzer)
       throws CorruptIndexException, IOException {
     ensureOpen();
-    SegmentInfo newSegmentInfo = buildSingleDocSegment(doc, analyzer);
-    synchronized (this) {
-      bufferDeleteTerm(term);
-      ramSegmentInfos.addElement(newSegmentInfo);
-      maybeFlushRamSegments();
-    }
+    bufferDeleteTerm(term);
+    docWriter.addDocument(doc, analyzer);
+    // nocommit: what if we need to trigger on max delete terms?
+    // For the non-autoCommit case, MultiDocumentWriter
+    // takes care of flushing its pending state to disk
+    if (autoCommit)
+      maybeFlush();
   }
 
-  final synchronized String newRamSegmentName() {
-    return "_ram_" + Integer.toString(ramSegmentInfos.counter++, Character.MAX_RADIX);
-  }
-
   // for test purpose
   final synchronized int getSegmentCount(){
     return segmentInfos.size();
   }
 
   // for test purpose
-  final synchronized int getRamSegmentCount(){
-    return ramSegmentInfos.size();
+  final synchronized int getNumBufferedDocuments(){
+    return docWriter.docID;
   }
 
   // for test purpose
@@ -1089,6 +1117,7 @@
    */
   private int mergeFactor = DEFAULT_MERGE_FACTOR;
 
+  // nocommit fix javadocs
   /** Determines the minimal number of documents required before the buffered
    * in-memory documents are merging and a new Segment is created.
    * Since Documents are merged in a {@link org.apache.lucene.store.RAMDirectory},
@@ -1096,10 +1125,11 @@
    * the number of files open in a FSDirectory.
    *
    * <p> The default value is {@link #DEFAULT_MAX_BUFFERED_DOCS}.
-
    */
   private int minMergeDocs = DEFAULT_MAX_BUFFERED_DOCS;
 
+  // nocommit javadoc
+  private float ramBufferSize = DEFAULT_RAM_BUFFER_SIZE_MB*1024F*1024F;
 
   /** Determines the largest number of documents ever merged by addDocument().
    * Small values (e.g., less than 10,000) are best for interactive indexing,
@@ -1177,7 +1207,7 @@
   */
   public synchronized void optimize() throws CorruptIndexException, IOException {
     ensureOpen();
-    flushRamSegments();
+    flush();
     while (segmentInfos.size() > 1 ||
            (segmentInfos.size() == 1 &&
             (SegmentReader.hasDeletions(segmentInfos.info(0)) ||
@@ -1186,7 +1216,7 @@
              (useCompoundFile &&
               (!SegmentReader.usesCompoundFile(segmentInfos.info(0))))))) {
       int minSegment = segmentInfos.size() - mergeFactor;
-      mergeSegments(segmentInfos, minSegment < 0 ? 0 : minSegment, segmentInfos.size());
+      mergeSegments(minSegment < 0 ? 0 : minSegment, segmentInfos.size());
     }
   }
 
@@ -1203,7 +1233,7 @@
     localRollbackSegmentInfos = (SegmentInfos) segmentInfos.clone();
     localAutoCommit = autoCommit;
     if (localAutoCommit) {
-      flushRamSegments();
+      flush();
       // Turn off auto-commit during our local transaction:
       autoCommit = false;
     } else
@@ -1293,16 +1323,18 @@
       segmentInfos.clear();
       segmentInfos.addAll(rollbackSegmentInfos);
 
+      docWriter.abort();
+
       // Ask deleter to locate unreferenced files & remove
       // them:
       deleter.checkpoint(segmentInfos, false);
       deleter.refresh();
 
-      ramSegmentInfos = new SegmentInfos();
       bufferedDeleteTerms.clear();
       numBufferedDeleteTerms = 0;
 
       commitPending = false;
+      docWriter.abort();
       close();
 
     } else {
@@ -1397,7 +1429,7 @@
         for (int base = start; base < segmentInfos.size(); base++) {
           int end = Math.min(segmentInfos.size(), base+mergeFactor);
           if (end-base > 1) {
-            mergeSegments(segmentInfos, base, end);
+            mergeSegments(base, end);
           }
         }
       }
@@ -1437,7 +1469,7 @@
     // segments in S may not since they could come from multiple indexes.
     // Here is the merge algorithm for addIndexesNoOptimize():
     //
-    // 1 Flush ram segments.
+    // 1 Flush ram.
     // 2 Consider a combined sequence with segments from T followed
     //   by segments from S (same as current addIndexes(Directory[])).
     // 3 Assume the highest level for segments in S is h. Call
@@ -1458,14 +1490,18 @@
     // copy a segment, which may cause doc count to change because deleted
     // docs are garbage collected.
 
-    // 1 flush ram segments
+    // 1 flush ram
 
     ensureOpen();
-    flushRamSegments();
+    flush();
 
     // 2 copy segment infos and find the highest level from dirs
     int startUpperBound = minMergeDocs;
 
+    // nocommit: what to do?
+    if (startUpperBound == 0)
+      startUpperBound = 10;
+
     boolean success = false;
 
     startTransaction();
@@ -1524,7 +1560,7 @@
 
         // copy those segments from S
         for (int i = segmentCount - numSegmentsToCopy; i < segmentCount; i++) {
-          mergeSegments(segmentInfos, i, i + 1);
+          mergeSegments(i, i + 1);
         }
         if (checkNonDecreasingLevels(segmentCount - numSegmentsToCopy)) {
           success = true;
@@ -1533,7 +1569,7 @@
       }
 
       // invariants do not hold, simply merge those segments
-      mergeSegments(segmentInfos, segmentCount - numTailSegments, segmentCount);
+      mergeSegments(segmentCount - numTailSegments, segmentCount);
 
       // maybe merge segments again if necessary
       if (segmentInfos.info(segmentInfos.size() - 1).docCount > startUpperBound) {
@@ -1673,22 +1709,18 @@
     throws IOException {
   }
 
-  protected final void maybeFlushRamSegments() throws CorruptIndexException, IOException {
+  protected final synchronized void maybeFlush() throws CorruptIndexException, IOException {
     // A flush is triggered if enough new documents are buffered or
-    // if enough delete terms are buffered
-    if (ramSegmentInfos.size() >= minMergeDocs || numBufferedDeleteTerms >= maxBufferedDeleteTerms) {
-      flushRamSegments();
+    // if enough delete terms are buffered or enough RAM is
+    // being consumed
+    // nocommit
+    if (numBufferedDeleteTerms >= maxBufferedDeleteTerms ||
+        (autoCommit && ((minMergeDocs != 0 && docWriter.docID >= minMergeDocs) ||
+                        ((true || minMergeDocs == 0) && autoCommit && docWriter.getRAMUsed() > ramBufferSize)))) {
+      flush();
     }
   }
 
-  /** Expert:  Flushes all RAM-resident segments (buffered documents), then may merge segments. */
-  private final synchronized void flushRamSegments() throws CorruptIndexException, IOException {
-    if (ramSegmentInfos.size() > 0 || bufferedDeleteTerms.size() > 0) {
-      mergeSegments(ramSegmentInfos, 0, ramSegmentInfos.size());
-      maybeMergeSegments(minMergeDocs);
-    }
-  }
-
   /**
    * Flush all in-memory buffered updates (adds and deletes)
    * to the Directory. 
@@ -1699,7 +1731,86 @@
    */
   public final synchronized void flush() throws CorruptIndexException, IOException {
     ensureOpen();
-    flushRamSegments();
+
+    SegmentInfo newSegment = null;
+    boolean anything = false;
+
+    boolean flushDocs = docWriter.docID > 0;
+    boolean flushDeletes = bufferedDeleteTerms.size() > 0;
+    final int numDocs = docWriter.docID;
+
+    if (flushDocs || flushDeletes) {
+
+      SegmentInfos rollback = null;
+
+      if (flushDeletes)
+        rollback = (SegmentInfos) segmentInfos.clone();
+
+      boolean success = false;
+
+      try {
+        if (flushDocs) {
+          int mergedDocCount = docWriter.docID;
+          String segment = docWriter.segment;
+          docWriter.flush(newSegmentName());
+          newSegment = new SegmentInfo(segment,
+                                       mergedDocCount,
+                                       directory, false, true);
+          segmentInfos.addElement(newSegment);
+        }
+
+        if (flushDeletes) {
+          maybeApplyDeletes(flushDocs);
+          doAfterFlush();
+        }
+
+        checkpoint();
+        success = true;
+      } finally {
+        if (!success) {
+          if (flushDeletes) {
+            // Fully replace the segmentInfos since flushed
+            // deletes could have changed any of the
+            // SegmentInfo instances:
+            segmentInfos.clear();
+            segmentInfos.addAll(rollback);
+          } else {
+            // Remove segment we added, if any:
+            if (newSegment != null && 
+                segmentInfos.size() > 0 && 
+                segmentInfos.info(segmentInfos.size()-1) == newSegment)
+              segmentInfos.remove(segmentInfos.size()-1);
+            docWriter.abort();
+          }
+          deleter.checkpoint(segmentInfos, false);
+          deleter.refresh();
+        }
+      }
+
+      deleter.checkpoint(segmentInfos, autoCommit);
+
+      if (flushDocs && useCompoundFile) {
+        success = false;
+        try {
+          docWriter.createCompoundFile(newSegment.name);
+          newSegment.setUseCompoundFile(true);
+          checkpoint();
+          success = true;
+        } finally {
+          if (!success) {
+            newSegment.setUseCompoundFile(false);
+            deleter.refresh();
+          }
+        }
+
+        deleter.checkpoint(segmentInfos, autoCommit);
+      }
+
+      // nocommit
+      // maybeMergeSegments(mergeFactor * numDocs / 2);
+
+      maybeMergeSegments(minMergeDocs);
+    }
   }
 
   /** Expert:  Return the total size of all index files currently cached in memory.
@@ -1707,15 +1818,15 @@
    */
   public final long ramSizeInBytes() {
     ensureOpen();
-    return ramDirectory.sizeInBytes();
+    return docWriter.getRAMUsed();
   }
 
   /** Expert:  Return the number of documents whose segments are currently cached in memory.
-   * Useful when calling flushRamSegments()
+   * Useful when calling flush()
    */
   public final synchronized int numRamDocs() {
     ensureOpen();
-    return ramSegmentInfos.size();
+    return docWriter.docID;
   }
   
   /** Incremental segment merger.  */
@@ -1723,6 +1834,9 @@
     long lowerBound = -1;
     long upperBound = startUpperBound;
 
+    // nocommit
+    if (upperBound == 0) upperBound = 10;
+
     while (upperBound < maxMergeDocs) {
       int minSegment = segmentInfos.size();
       int maxSegment = -1;
@@ -1754,7 +1868,7 @@
         while (numSegments >= mergeFactor) {
           // merge the leftmost* mergeFactor segments
 
-          int docCount = mergeSegments(segmentInfos, minSegment, minSegment + mergeFactor);
+          int docCount = mergeSegments(minSegment, minSegment + mergeFactor);
           numSegments -= mergeFactor;
 
           if (docCount > upperBound) {
@@ -1783,39 +1897,29 @@
    * Merges the named range of segments, replacing them in the stack with a
    * single segment.
    */
-  private final int mergeSegments(SegmentInfos sourceSegments, int minSegment, int end)
+  private final int mergeSegments(int minSegment, int end)
     throws CorruptIndexException, IOException {
 
-    // We may be called solely because there are deletes
-    // pending, in which case doMerge is false:
-    boolean doMerge = end > 0;
     final String mergedName = newSegmentName();
+
     SegmentMerger merger = null;
-
-    final List ramSegmentsToDelete = new ArrayList();
-
     SegmentInfo newSegment = null;
 
     int mergedDocCount = 0;
-    boolean anyDeletes = (bufferedDeleteTerms.size() != 0);
 
     // This is try/finally to make sure merger's readers are closed:
     try {
 
-      if (doMerge) {
-        if (infoStream != null) infoStream.print("merging segments");
-        merger = new SegmentMerger(this, mergedName);
+      if (infoStream != null) infoStream.print("merging segments");
 
-        for (int i = minSegment; i < end; i++) {
-          SegmentInfo si = sourceSegments.info(i);
-          if (infoStream != null)
-            infoStream.print(" " + si.name + " (" + si.docCount + " docs)");
-          IndexReader reader = SegmentReader.get(si); // no need to set deleter (yet)
-          merger.add(reader);
-          if (reader.directory() == this.ramDirectory) {
-            ramSegmentsToDelete.add(si);
-          }
-        }
+      merger = new SegmentMerger(this, mergedName);
+
+      for (int i = minSegment; i < end; i++) {
+        SegmentInfo si = segmentInfos.info(i);
+        if (infoStream != null)
+          infoStream.print(" " + si.name + " (" + si.docCount + " docs)");
+        IndexReader reader = SegmentReader.get(si); // no need to set deleter (yet)
+        merger.add(reader);
       }
 
       SegmentInfos rollback = null;
@@ -1825,99 +1929,57 @@
       // if we hit exception when doing the merge:
       try {
 
-        if (doMerge) {
-          mergedDocCount = merger.merge();
+        mergedDocCount = merger.merge();
 
-          if (infoStream != null) {
-            infoStream.println(" into "+mergedName+" ("+mergedDocCount+" docs)");
-          }
+        if (infoStream != null) {
+          infoStream.println(" into "+mergedName+" ("+mergedDocCount+" docs)");
+        }
 
-          newSegment = new SegmentInfo(mergedName, mergedDocCount,
-                                       directory, false, true);
-        }
+        newSegment = new SegmentInfo(mergedName, mergedDocCount,
+                                     directory, false, true);
         
-        if (sourceSegments != ramSegmentInfos || anyDeletes) {
-          // Now save the SegmentInfo instances that
-          // we are replacing:
-          rollback = (SegmentInfos) segmentInfos.clone();
-        }
+        rollback = (SegmentInfos) segmentInfos.clone();
 
-        if (doMerge) {
-          if (sourceSegments == ramSegmentInfos) {
-            segmentInfos.addElement(newSegment);
-          } else {
-            for (int i = end-1; i > minSegment; i--)     // remove old infos & add new
-              sourceSegments.remove(i);
+        for (int i = end-1; i > minSegment; i--)     // remove old infos & add new
+          segmentInfos.remove(i);
 
-            segmentInfos.set(minSegment, newSegment);
-          }
-        }
+        segmentInfos.set(minSegment, newSegment);
 
-        if (sourceSegments == ramSegmentInfos) {
-          maybeApplyDeletes(doMerge);
-          doAfterFlush();
-        }
-        
         checkpoint();
 
         success = true;
 
       } finally {
 
-        if (success) {
-          // The non-ram-segments case is already committed
-          // (above), so all the remains for ram segments case
-          // is to clear the ram segments:
-          if (sourceSegments == ramSegmentInfos) {
-            ramSegmentInfos.removeAllElements();
-          }
-        } else {
+        if (!success && rollback != null) {
+          // Rollback the individual SegmentInfo
+          // instances, but keep original SegmentInfos
+          // instance (so we don't try to write again the
+          // same segments_N file -- write once):
+          segmentInfos.clear();
+          segmentInfos.addAll(rollback);
 
-          // Must rollback so our state matches index:
-          if (sourceSegments == ramSegmentInfos && !anyDeletes) {
-            // Simple case: newSegment may or may not have
-            // been added to the end of our segment infos,
-            // so just check & remove if so:
-            if (newSegment != null && 
-                segmentInfos.size() > 0 && 
-                segmentInfos.info(segmentInfos.size()-1) == newSegment) {
-              segmentInfos.remove(segmentInfos.size()-1);
-            }
-          } else if (rollback != null) {
-            // Rollback the individual SegmentInfo
-            // instances, but keep original SegmentInfos
-            // instance (so we don't try to write again the
-            // same segments_N file -- write once):
-            segmentInfos.clear();
-            segmentInfos.addAll(rollback);
-          }
-
           // Delete any partially created and now unreferenced files:
           deleter.refresh();
         }
       }
     } finally {
       // close readers before we attempt to delete now-obsolete segments
-      if (doMerge) merger.closeReaders();
+      merger.closeReaders();
     }
 
-    // Delete the RAM segments
-    deleter.deleteDirect(ramDirectory, ramSegmentsToDelete);
-
     // Give deleter a chance to remove files now.
     deleter.checkpoint(segmentInfos, autoCommit);
 
-    if (useCompoundFile && doMerge) {
+    if (useCompoundFile) {
 
       boolean success = false;
 
       try {
-
         merger.createCompoundFile(mergedName + ".cfs");
         newSegment.setUseCompoundFile(true);
         checkpoint();
         success = true;
-
       } finally {
         if (!success) {  
           // Must rollback:
@@ -1936,14 +1998,14 @@
   // Called during flush to apply any buffered deletes.  If
   // doMerge is true then a new segment was just created and
   // flushed from the ram segments.
-  private final void maybeApplyDeletes(boolean doMerge) throws CorruptIndexException, IOException {
+  private final void maybeApplyDeletes(boolean flushedNewSegment) throws CorruptIndexException, IOException {
 
     if (bufferedDeleteTerms.size() > 0) {
       if (infoStream != null)
         infoStream.println("flush " + numBufferedDeleteTerms + " buffered deleted terms on "
                            + segmentInfos.size() + " segments.");
 
-      if (doMerge) {
+      if (flushedNewSegment) {
         IndexReader reader = null;
         try {
           reader = SegmentReader.get(segmentInfos.info(segmentInfos.size() - 1));
@@ -1964,7 +2026,7 @@
       }
 
       int infosEnd = segmentInfos.size();
-      if (doMerge) {
+      if (flushedNewSegment) {
         infosEnd--;
       }
 
@@ -1996,6 +2058,8 @@
   private final boolean checkNonDecreasingLevels(int start) {
     int lowerBound = -1;
     int upperBound = minMergeDocs;
+    if (upperBound == 0)
+      upperBound = 10;
 
     for (int i = segmentInfos.size() - 1; i >= start; i--) {
       int docCount = segmentInfos.info(i).docCount;
@@ -2044,10 +2108,11 @@
   // well as the disk segments.
   private void bufferDeleteTerm(Term term) {
     Num num = (Num) bufferedDeleteTerms.get(term);
+    int numDoc = docWriter.docID;
     if (num == null) {
-      bufferedDeleteTerms.put(term, new Num(ramSegmentInfos.size()));
+      bufferedDeleteTerms.put(term, new Num(numDoc));
     } else {
-      num.setNum(ramSegmentInfos.size());
+      num.setNum(numDoc);
     }
     numBufferedDeleteTerms++;
   }
@@ -2057,17 +2122,20 @@
   // the documents buffered before it, not those buffered after it.
   private final void applyDeletesSelectively(HashMap deleteTerms,
       IndexReader reader) throws CorruptIndexException, IOException {
+    //System.out.println("now apply selective deletes");
     Iterator iter = deleteTerms.entrySet().iterator();
     while (iter.hasNext()) {
       Entry entry = (Entry) iter.next();
       Term term = (Term) entry.getKey();
-
+      //System.out.println("  term " + term);
+    
       TermDocs docs = reader.termDocs(term);
       if (docs != null) {
         int num = ((Num) entry.getValue()).getNum();
         try {
           while (docs.next()) {
             int doc = docs.doc();
+            //System.out.println("    doc " + doc + " vs " + num);
             if (doc >= num) {
               break;
             }
Index: src/java/org/apache/lucene/index/IndexFileDeleter.java
===================================================================
--- src/java/org/apache/lucene/index/IndexFileDeleter.java	(revision 523296)
+++ src/java/org/apache/lucene/index/IndexFileDeleter.java	(working copy)
@@ -97,6 +97,7 @@
   private PrintStream infoStream;
   private Directory directory;
   private IndexDeletionPolicy policy;
+  private MultiDocumentWriter docWriter;
 
   void setInfoStream(PrintStream infoStream) {
     this.infoStream = infoStream;
@@ -116,10 +117,12 @@
    * @throws CorruptIndexException if the index is corrupt
    * @throws IOException if there is a low-level IO error
    */
-  public IndexFileDeleter(Directory directory, IndexDeletionPolicy policy, SegmentInfos segmentInfos, PrintStream infoStream)
+  public IndexFileDeleter(Directory directory, IndexDeletionPolicy policy, SegmentInfos segmentInfos, PrintStream infoStream, MultiDocumentWriter docWriter)
     throws CorruptIndexException, IOException {
 
+    this.docWriter = docWriter;
     this.infoStream = infoStream;
+
     this.policy = policy;
     this.directory = directory;
 
@@ -310,6 +313,8 @@
 
     // Incref the files:
     incRef(segmentInfos, isCommit);
+    if (docWriter != null)
+      incRef(docWriter.files());
 
     if (isCommit) {
       // Append to our commits list:
@@ -325,9 +330,8 @@
     // DecRef old files from the last checkpoint, if any:
     int size = lastFiles.size();
     if (size > 0) {
-      for(int i=0;i<size;i++) {
+      for(int i=0;i<size;i++)
         decRef((List) lastFiles.get(i));
-      }
       lastFiles.clear();
     }
 
@@ -340,6 +344,8 @@
           lastFiles.add(segmentInfo.files());
         }
       }
+      if (docWriter != null)
+        lastFiles.add(docWriter.files());
     }
   }
 
Index: src/java/org/apache/lucene/index/MultiDocumentWriter.java
===================================================================
--- src/java/org/apache/lucene/index/MultiDocumentWriter.java	(revision 0)
+++ src/java/org/apache/lucene/index/MultiDocumentWriter.java	(revision 0)
@@ -0,0 +1,3567 @@
+package org.apache.lucene.index;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.lang.management.ManagementFactory;
+import java.lang.management.MemoryMXBean;
+
+import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.analysis.Token;
+import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Fieldable;
+import org.apache.lucene.search.Similarity;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.store.RAMOutputStream;
+
+import org.apache.lucene.util.PriorityQueue;
+import org.apache.lucene.util.StringHelper;
+
+import java.io.OutputStream;
+import java.io.IOException;
+import java.io.PrintStream;
+import java.io.Reader;
+import java.io.StringReader;
+import java.util.Arrays;
+import java.util.HashMap;
+import java.util.List;
+import java.util.ArrayList;
+import java.util.Vector;
+import java.util.Enumeration;
+import java.util.Map;
+import java.util.Iterator;
+import java.text.NumberFormat;
+
+/**
+ * This class accepts multiple added documents and directly
+ * writes a single segment file.  It does this more
+ * efficiently than creating a single segment per document
+ * (with DocumentWriter) and doing standard merges on the
+ * resulting segments.
+ *
+ * When a document is added, its stored fields (if any) and
+ * term vectors (if any) are immediately written to the
+ * Directory (ie these do not consume RAM).  The terms
+ * dictionary and freq/prox posting lists are written to RAM
+ * as a partial segment (which is one "file" in its own
+ * format that contains term info, freq and prox).
+ * Periodically these partial segments in RAM are merged
+ * into larger partial segments, which compacts them thus
+ * freeing up RAM.
+ *
+ * Eventually the partial egments exceed the allowed RAM
+ * buffer size (settable via IndexWriter) and we either
+ * create a true segment at that point (when writer has
+ * autoCommit=true) else we flush the partial segments to
+ * the Directory as a single flushed partial segment (*.tfp)
+ * and then only when IndexWriter is closed do we merge
+ * those flushed partial segments into a real segment.
+ *
+ * There are some other careful aspects:
+ *
+ *   * An efficient RAMFile that holds byte[] buffers
+ *     chained together in a linked list, in growing sizes
+ *     (128, 1024, 8192).
+ *
+ *   * Large shared int[] arrays to hold posting data for
+ *     one document while it's being processed.
+ *
+ *   * Large shared char[] arrays to text for each Posting.
+ *
+ *   * Recycle buffers, Postings, etc., when possible
+ */
+
+final class MultiDocumentWriter {
+
+  // Only applies when multiple threads call addDocument:
+  // max number of pending documents in line waiting to be
+  // written to the in-memory and on-disk segment files.
+  // Once we hit this max, the new incoming addDocument
+  // calls will wait until the line shrinks below this.
+  final public static int MAX_WAIT_QUEUE = 5;
+
+  final public static int DEFAULT_FLUSH_MERGE_FACTOR = 5;
+
+  final public static boolean REUSE_INT_ARRAYS = true;
+  final public static boolean REUSE_CHAR_ARRAYS = true;
+  final public static boolean REUSE_POSTING_ARRAYS = true;
+
+  // How much ram we are allowed to use:
+  final private static long DEFAULT_RAM_BUFFER_SIZE = 16*1024*1024;
+
+  private IndexWriter writer;
+  private Directory directory;                    // dir where final segment is written
+
+  private FieldInfos fieldInfos;                  // all fields we've seen
+
+  private IndexOutput tvx, tvf, tvd;              // to write term vectors
+
+  private FieldsWriter fieldsWriter;              // to write stored fields
+
+  private PrintStream infoStream;
+  String segment;                                 // current segment we are writing
+  int docID;                                      // next docID
+  int nextWriteDocID;                          // next docID to be written
+
+  // ASSERT
+  int maxDocID;
+
+  // nocommit
+  static MemoryMXBean bean = ManagementFactory.getMemoryMXBean();
+
+  private List ramSegments = new ArrayList();
+  private int[] levelCounts = new int[1];
+  private long[] levelSizes = new long[1];
+  private long totalSize;
+
+  private List flushedSegments = new ArrayList();
+  private int flushedCount;
+  private int[] flushedLevelCounts = new int[1];
+  private long[] flushedLevelSizes = new long[1];
+  private long totalFlushedSize;
+
+  // TODO: getter/setter
+  private int flushedMergeFactor = DEFAULT_FLUSH_MERGE_FACTOR;
+
+  private boolean hasNorms;
+  private boolean flushedVectors;
+  private boolean flushedNorms;
+  private boolean doSelfFlush;
+
+  private long ramBufferSize = DEFAULT_RAM_BUFFER_SIZE;
+
+  private List freeThreadStates = new ArrayList();
+  private ThreadState[] waitingThreadStates = new ThreadState[1];
+  private int numThreadState;
+  private boolean flushPending = false;
+
+  private int numWaiting = 0;
+
+  void setRAMBufferSizeMB(float mb) {
+    ramBufferSize = (long) (mb*1024*1024);
+  }
+
+  long startTime;
+
+  MultiDocumentWriter(String segment, Directory directory, IndexWriter writer, boolean doSelfFlush) throws IOException {
+    this.directory = directory;
+    this.writer = writer;
+    this.doSelfFlush = doSelfFlush;
+    reset(segment);
+    startTime = System.currentTimeMillis();
+  }
+
+  private List files = null;
+
+  /**
+   * Returns list of files in use by this instance,
+   * including any flushed segments.
+   */
+  public List files() {
+
+    if (files != null)
+      return files;
+
+    files = new ArrayList();
+    final int numFlushed = flushedSegments.size();
+    for(int i=0;i<numFlushed;i++) {
+      FlushedSegment fs = (FlushedSegment) flushedSegments.get(i);
+      files.add(tempFileName(fs.segment));
+    }
+
+    // Stored fields:
+    if (fieldsWriter != null) {
+      files.add(segment + ".fdt");
+      files.add(segment + ".fdx");
+    }
+
+    // Vectors:
+    if (tvx != null) {
+      files.add(segment + ".tvx");
+      files.add(segment + ".tvf");
+      files.add(segment + ".tvd");
+    }
+    return files;
+  }
+
+  /**
+   * Reset ourselves, discarding any updates we've
+   * accumulated.
+  */
+  public void abort() throws IOException {
+
+    totalSize = 0;
+    totalFlushedSize = 0;
+    flushedCount = 0;
+    Arrays.fill(levelSizes, 0);
+    Arrays.fill(levelCounts, 0);
+    Arrays.fill(flushedLevelSizes, 0);
+    Arrays.fill(flushedLevelCounts, 0);
+    ramSegments.clear();
+    flushedSegments.clear();
+
+    // Discard pending norms:
+    final int numField = fieldInfos.size();
+    for (int i=0;i<numField;i++) {
+      FieldInfo fi = fieldInfos.fieldInfo(i);
+      if (fi.isIndexed && !fi.omitNorms) {
+        BufferedNorms n = norms[i];
+        n.out.reset();
+        n.upto = 0;
+      }
+    }
+
+    if (tvx != null) {
+      tvx.close();
+      tvf.close();
+      tvd.close();
+    }
+
+    final int size = freeThreadStates.size();
+    for(int i=0;i<size;i++) {
+      ThreadState state = (ThreadState) freeThreadStates.get(i);
+      if (state.localFieldsWriter != null) {
+        state.localFieldsWriter.close();
+        state.localFieldsWriter = null;
+      }
+    }
+
+    if (fieldsWriter != null) {
+      fieldsWriter.close();
+      fieldsWriter = null;
+    }
+
+    flushedNorms = false;
+    flushedVectors = false;
+
+    files = null;
+    docID = 0;
+    nextWriteDocID = 0;
+  }
+
+  // flush all changes to a real segment
+  int netFlushCount = 0;
+  int netFlushTimes = 0;
+  int lastFlushDocID;
+
+  synchronized void flush(String newSegmentName) throws IOException {
+    long t0 = System.currentTimeMillis();
+    System.out.println("FLUSH @ " + docID + ": RAM=" + totalSize);
+    System.out.println("  mem now: " + bean.getHeapMemoryUsage().getUsed());
+
+    // Must wait for all in-flight documents to finish:
+    if (numThreadState > freeThreadStates.size()) {
+      // System.out.println("  mark pending & wait..." + numThreadState + " vs " + freeThreadStates.size());
+      flushPending = true;
+      while (numThreadState > freeThreadStates.size()) {
+        // System.out.println("flush wait: " + numThreadState + " vs " + freeThreadStates.size());
+        try {
+          wait();
+        } catch (InterruptedException e) {
+        }
+      }
+      // System.out.println("  wait done...");
+    }
+
+    // nocommit: what if we hit exception before notifyAll?
+    netFlushCount += docID;
+    netFlushTimes++;
+    // System.out.println("  FLUSH avg # docs=" + (((float) netFlushCount)/netFlushTimes));
+
+    fieldInfos.write(directory, segment + ".fnm");
+
+    // nocommit: not clean!!
+    ThreadState state = (ThreadState) freeThreadStates.get(0);
+
+    state.flushTerms();
+
+    // write norms of indexed fields
+    writeNorms();
+
+    assert fieldInfos.hasVectors() == (tvx != null);
+
+    if (tvx != null) {
+      flushedVectors = true;
+      tvx.close();
+      tvf.close();
+      tvd.close();
+      tvx = null;
+    } else {
+      flushedVectors = false;
+    }
+
+    if (fieldsWriter != null) {
+      fieldsWriter.close();
+      fieldsWriter = null;
+    }
+
+    final int size = freeThreadStates.size();
+    for(int i=0;i<size;i++) {
+      ThreadState state2 = (ThreadState) freeThreadStates.get(i);
+      if (state2.localFieldsWriter != null) {
+        state2.localFieldsWriter.close();
+        state2.localFieldsWriter = null;
+      }
+    }
+
+    flushedNorms = hasNorms;
+    files = null;
+
+    if (newSegmentName != null)
+      reset(newSegmentName);
+    flushPending = false;
+    notifyAll();
+    netSegmentTime += (System.currentTimeMillis()-t0);
+    if (infoStream != null) {
+      System.out.println("TIME:         doc " + (netDocTime/1000.0) + " sec");
+      System.out.println("TIME:      merge0 " + (netMerge0Time/1000.0) + " sec");
+      System.out.println("TIME:      merge1 " + (netMerge1Time/1000.0) + " sec");
+      System.out.println("TIME:       flush " + (netFlushTime/1000.0) + " sec");
+      System.out.println("TIME: merge flush " + (netFlushedMergeTime/1000.0) + " sec");
+      System.out.println("TIME:     segment " + (netSegmentTime/1000.0) + " sec");
+    }
+  }
+
+  // start a new segment
+  void reset(String segment) throws IOException {
+    this.segment = segment;
+    docID = 0;
+    nextWriteDocID = 0;
+    hasNorms = false;
+    fieldInfos = new FieldInfos();
+    flushedCount = 0;
+    Arrays.fill(levelCounts, 0);
+    Arrays.fill(levelSizes, 0);
+    Arrays.fill(flushedLevelCounts, 0);
+    Arrays.fill(flushedLevelSizes, 0);
+    files = null;
+  }
+
+  private BufferedNorms[] norms = new BufferedNorms[0];
+
+  // Per-thread items
+  private final class ThreadState {
+    int docID;
+
+    // We write term vectors to this, privately to one
+    // thread, and then flush the data to the real term
+    // vectors file, synchronized
+    RAMWriter tvfLocal = new RAMWriter();
+
+    // We write stored fields to this, privately to on
+    // thread
+    RAMWriter fdxLocal = new RAMWriter();
+    RAMWriter fdtLocal = new RAMWriter();
+
+    // Used to write to a new RAM segment
+    RAMWriter out = new RAMWriter();
+
+    private long[] vectorFieldPointers = new long[0];
+    private int[] vectorFieldNumbers = new int[0];
+
+    private FieldData[] docFieldData = new FieldData[10];
+    private int numStoredFields;
+
+    FieldData[] fieldDataArray;
+    int numFieldData;
+    int numFieldDataLast;
+    FieldData[] fieldDataHash;
+    int fieldDataHashMask;
+
+    //  FieldsWriter that's temporarily private to this
+    //  thread.  We write fields here and then copy the
+    //  output to the real FieldsWriter.
+    private FieldsWriter localFieldsWriter;
+
+    public ThreadState() {
+      fieldDataArray = new FieldData[21];
+      fieldDataHash = new FieldData[32];
+      fieldDataHashMask = 31;
+      vectorFieldPointers = new long[16];
+      vectorFieldNumbers = new int[16];
+      initPostingArrays();
+      initIntBlocks();
+      initCharBlocks();
+    }
+
+    void initPostingArrays() {
+      postingsArrayLimit = 16;
+      postingsArray = new Posting[postingsArrayLimit];
+      // NOTE: must be a power of two for hash collision
+      // strategy to work correctly
+      postingsHashSize = 32;
+      postingsHashMask = postingsHashSize-1;
+      postingsHash = new Posting[postingsHashSize];
+    }
+
+    private int gen;
+    boolean anyOldFields;
+
+    int docCode;
+    int docCodeFreq1;
+
+    final byte[] docCodeBytes = new byte[5];
+    final byte[] docCodeFreq1Bytes = new byte[5];
+
+    int numBytesDocCode;
+    int numBytesDocCodeFreq1;
+
+    private boolean docHasReader;
+    private int netIndexedFieldLength;
+
+    float docBoost;
+
+    // Initializes shared state for this new document.
+    void init(Document doc, int docID) throws IOException {
+
+      this.docID = docID;
+      // ASSERT
+      maxDocID = docID;
+      docBoost = doc.getBoost();
+
+      List docFields = doc.getFields();
+      numStoredFields = 0;
+      final int numDocFields = docFields.size();
+
+      // Maybe grow our docFieldData
+      if (docFieldData.length < numDocFields) {
+        int newSize = (int) (numDocFields*1.25);
+        docFieldData = new FieldData[newSize];
+      }
+
+      boolean docHasVectors = false;
+
+      docHasReader = false;
+      netIndexedFieldLength = 0;
+
+      final int lastGen = gen++;
+
+      numFieldData = 0;
+      anyOldFields = false;
+
+      // Absorb any new fields first seen in this document.
+      // Also absorb any changes to fields we had already
+      // seen before (eg suddenly turning on norms or
+      // vectors, etc.):
+      for(int i=0;i<numDocFields;i++) {
+        Fieldable field = (Fieldable) docFields.get(i);
+        FieldInfo fi = fieldInfos.add(field.name(), field.isIndexed(), field.isTermVectorStored(),
+                                      field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),
+                                      field.getOmitNorms(), false);
+        numStoredFields += field.isStored() ? 1:0;
+        docHasVectors |= field.isTermVectorStored();
+        if (field.isIndexed()) {
+          if (field.readerValue() != null)
+            docHasReader = true;
+          else {
+            String s = field.stringValue();
+            if (s != null)
+              netIndexedFieldLength += s.length();
+            else
+              throw new IllegalArgumentException
+                ("field must have either String or Reader value");
+          }
+        }
+
+        // TODO: really we should not do this while holding
+        // the lock?  Only fieldInfos.add must hold the
+        // lock?  (Maybe we should just make fieldInfos.add
+        // synchronized?  Surely that wouldn't hurt
+        // performance?)
+
+        // Make sure we have a FieldData allocated
+        int hashPos = fi.name.hashCode() & fieldDataHashMask;
+        FieldData fp = fieldDataHash[hashPos];
+        while(fp != null && !fp.fieldInfo.name.equals(fi.name))
+          fp = fp.next;
+
+        if (fp == null) {
+          // TODO: recycle?
+          //System.out.println("now make new fieldData: " + fi.name + " " + fi + " hashPos=" + hashPos);
+          fp = new FieldData(fi);
+          fp.next = fieldDataHash[hashPos];
+          fieldDataHash[hashPos] = fp;
+        } else
+          fp.fieldInfo = fi;
+
+        docFieldData[i] = fp;
+
+        if (gen != fp.gen) {
+
+          if (fp.gen != lastGen)
+            anyOldFields = true;
+
+          // First time we're seeing this field for this
+          // doc.
+          fp.gen = gen;
+          fp.fieldCount = 0;
+          fp.doVectors = fp.doVectorPositions = fp.doVectorOffsets = false;
+
+          if (numFieldData == fieldDataArray.length) {
+            int newSize = fieldDataArray.length*2;
+            FieldData newArray[] = new FieldData[newSize];
+            System.arraycopy(fieldDataArray, 0, newArray, 0, numFieldData);
+            fieldDataArray = newArray;
+
+            // Rehash
+            newSize = 2*fieldDataHash.length*2;
+            newArray = new FieldData[newSize];
+            fieldDataHashMask = newSize-1;
+            for(int j=0;j<fieldDataHash.length;j++) {
+              FieldData fp0 = fieldDataHash[j];
+              while(fp0 != null) {
+                hashPos = fp0.fieldInfo.name.hashCode() & fieldDataHashMask;
+                FieldData nextFP0 = fp0.next;
+                fp0.next = newArray[hashPos];
+                newArray[hashPos] = fp0;
+                fp0 = nextFP0;
+              }
+            }
+            fieldDataHash = newArray;
+          }
+          fieldDataArray[numFieldData++] = fp;
+        }
+
+        fp.doVectors |= field.isTermVectorStored();
+        if (fp.doVectors) {
+          fp.doVectorPositions |= field.isStorePositionWithTermVector();
+          fp.doVectorOffsets |= field.isStoreOffsetWithTermVector();
+        }
+
+        if (fp.fieldCount == fp.docFields.length) {
+          Fieldable[] newArray = new Fieldable[fp.docFields.length*2];
+          System.arraycopy(fp.docFields, 0, newArray, 0, fp.docFields.length);
+          fp.docFields = newArray;
+        }
+        fp.docFields[fp.fieldCount++] = field;
+      }
+
+      final int numFields = fieldInfos.size();
+
+      // Maybe grow our buffered norms
+      if (norms.length < numFields) {
+        int newSize = (int) (numFields*1.25);
+        BufferedNorms[] newNorms = new BufferedNorms[newSize];
+        System.arraycopy(norms, 0, newNorms, 0, norms.length);
+        for(int i=norms.length;i<newNorms.length;i++)
+          newNorms[i] = new BufferedNorms();
+        norms = newNorms;
+      }
+
+      // Maybe init the local & global vectors writer
+      if (docHasVectors && tvx == null) {
+        tvx = directory.createOutput(segment + TermVectorsWriter.TVX_EXTENSION);
+        tvx.writeInt(TermVectorsWriter.FORMAT_VERSION);
+        tvd = directory.createOutput(segment +  TermVectorsWriter.TVD_EXTENSION);
+        tvd.writeInt(TermVectorsWriter.FORMAT_VERSION);
+        tvf = directory.createOutput(segment +  TermVectorsWriter.TVF_EXTENSION);
+        tvf.writeInt(TermVectorsWriter.FORMAT_VERSION);
+        files = null;
+
+        // TODO: need unit test to catch this:
+        // Catch up for all previous docs null term vectors:
+        for(int i=0;i<docID;i++)
+          tvx.writeLong(0);
+      }
+
+      // Maybe init the local & global fieldsWriter
+      if (localFieldsWriter == null) {
+        if (fieldsWriter == null) {
+          fieldsWriter = new FieldsWriter(directory, segment, fieldInfos);
+          files = null;
+        }
+        localFieldsWriter = new FieldsWriter(fdxLocal, fdtLocal, fieldInfos);
+      }
+    }
+
+    private final void quickSort(Posting[] postings, int lo, int hi) {
+      if (lo >= hi)
+        return;
+
+      int mid = (lo + hi) / 2;
+
+      if (postings[lo].compareTo(postings[mid]) > 0) {
+        Posting tmp = postings[lo];
+        postings[lo] = postings[mid];
+        postings[mid] = tmp;
+      }
+
+      if (postings[mid].compareTo(postings[hi]) > 0) {
+        Posting tmp = postings[mid];
+        postings[mid] = postings[hi];
+        postings[hi] = tmp;
+
+        if (postings[lo].compareTo(postings[mid]) > 0) {
+          Posting tmp2 = postings[lo];
+          postings[lo] = postings[mid];
+          postings[mid] = tmp2;
+        }
+      }
+
+      int left = lo + 1;
+      int right = hi - 1;
+
+      if (left >= right)
+        return;
+
+      Posting partition = postings[mid];
+
+      for (; ;) {
+        while (postings[right].compareTo(partition) > 0)
+          --right;
+
+        while (left < right && postings[left].compareTo(partition) <= 0)
+          ++left;
+
+        if (left < right) {
+          Posting tmp = postings[left];
+          postings[left] = postings[right];
+          postings[right] = tmp;
+          --right;
+        } else {
+          break;
+        }
+      }
+
+      quickSort(postings, lo, left);
+      quickSort(postings, left + 1, hi);
+    }
+
+    private Posting[] postingsArray;
+    private int postingsArrayLimit;
+    private int numPostings;
+ 
+    private Posting[] postingsHash;
+    private int postingsHashSize;
+    private int postingsHashMask;
+
+    int length;
+    int position;
+    int offset;
+    float boost;
+    FieldInfo fieldInfo;
+    
+    // Tokenizes the fields of a document into Postings.
+    final void processDocument(Analyzer analyzer)
+      throws IOException {
+
+      // System.out.println("process doc");
+
+      final int maxFieldLength = writer.getMaxFieldLength();
+      final int numFields = numFieldData;
+
+      docCode = docID << 1;
+      docCodeFreq1 = docCode+1;
+
+      numBytesDocCode = writeVInt(docCodeBytes, docCode);
+      numBytesDocCodeFreq1 = writeVInt(docCodeFreq1Bytes, docCodeFreq1);
+      numVectorFields = 0;
+
+      assert out.buffer == null;
+
+      // Make coarse guess on how large initial buffers
+      // should be.  This helps most in the multithreaded
+      // case since allocating a new RAM buffer is
+      // synchronized.  If we guess to large it's not really
+      // a problem because the level 1 merge will compress
+      // it.
+      final int startLevel;
+      if (docHasReader) netIndexedFieldLength += 4096;
+      if (netIndexedFieldLength < 2048)
+        startLevel = 0;
+      else if (netIndexedFieldLength < 4096)
+        startLevel = 1;
+      else if (netIndexedFieldLength < 8192)
+        startLevel = 2;
+      else if (netIndexedFieldLength < 16384)
+        startLevel = 3;
+      else
+        startLevel = 4;
+      
+      // System.out.println("  doc startLevel=" + startLevel);
+      
+      out.setStartLevel(startLevel);
+
+      // Sort by field name only if it's not the same set
+      // of fields we just saw
+      if (true || numFieldDataLast != numFields || anyOldFields)
+        Arrays.sort(fieldDataArray, 0, numFields);
+      numFieldDataLast = numFields;
+      
+      fdtLocal.writeVInt(numStoredFields);
+
+      for(int i=0;i<numFields;i++) {
+        FieldData fp = fieldDataArray[i];
+        fieldInfo = fp.fieldInfo;
+
+        final int limit = fp.fieldCount;
+        final Fieldable[] docFields = fp.docFields;
+
+        length = 0;
+        position = 0;
+        offset = 0;
+        boost = docBoost;
+        numPostings = 0;
+        boolean isIndexed = false;
+        // System.out.println("invert field " + fp.fieldInfo.name);
+        for(int j=0;j<fp.fieldCount;j++) {
+          Fieldable field = docFields[j];
+          if (field.isIndexed()) {
+            isIndexed = true;
+            invertField(field, analyzer, maxFieldLength);
+          }
+          if (field.isStored())
+            // System.out.println("now write field " + field);
+            localFieldsWriter.writeField(fp.fieldInfo, field);
+          docFields[j] = null;
+        }
+
+        if (isIndexed) {
+          quickSort(postingsArray, 0, numPostings-1);
+          long t0 = System.currentTimeMillis();
+          addPostingsAndVectors(fp);
+          netDocTime += (System.currentTimeMillis()-t0);
+        }
+
+        if (fieldInfo.isIndexed && !fieldInfo.omitNorms)
+          fp.norm = boost * writer.getSimilarity().lengthNorm(fieldInfo.name, length);
+      }
+    
+      if (!REUSE_POSTING_ARRAYS)
+        initPostingArrays();
+
+      // mark end
+      out.writeVInt(Integer.MAX_VALUE);
+
+      ramSegment = new RAMSegment(1, out);
+
+      //System.out.println("  doc: size=" + ramSegment.size);
+      //printAlloc("    doc", ramSegment.data, ramSegment.dataLimit);
+    }
+
+    private Token token;
+    private Token localToken = new Token("", 0, 0);
+    private int offsetStart;
+    private int offsetEnd;
+    private boolean storeOffsets;
+
+    public void invertField(Fieldable field, Analyzer analyzer, final int maxFieldLength) throws IOException {
+
+      if (length>0) position += analyzer.getPositionIncrementGap(fieldInfo.name);
+
+      storeOffsets = field.isStoreOffsetWithTermVector();
+
+      netDocCount++;
+
+      if (!field.isTokenized()) {		  // un-tokenized field
+        token = localToken;
+        String stringValue = field.stringValue();
+        token.setTermText(stringValue);
+        if (storeOffsets) {
+          offsetStart = offset;
+          offsetEnd = offset + stringValue.length();
+          addPosition();
+        } else {
+          offsetStart = offsetEnd = -1;
+          addPosition();
+        }
+        offset += stringValue.length();
+        length++;
+        // nocommit
+        netDocSize += stringValue.length();
+      } else {
+        Reader reader;			  // find or make Reader
+        if (field.readerValue() != null)
+          reader = field.readerValue();
+        else {
+          stringReader.init(field.stringValue());
+          // nocommit
+          netDocSize += field.stringValue().length();
+          reader = stringReader;
+        }
+          
+        // Tokenize field and add to postingTable
+        TokenStream stream = analyzer.tokenStream(fieldInfo.name, reader);
+        try {
+          offsetEnd = offset-1;
+          if (storeOffsets) {
+            for (token = stream.next(); token != null; token = stream.next()) {
+              position += (token.getPositionIncrement() - 1);
+              offsetStart = offset + token.startOffset();
+              offsetEnd = offset + token.endOffset();
+              addPosition();
+              if (++length >= maxFieldLength) {
+                if (infoStream != null)
+                  infoStream.println("maxFieldLength " +maxFieldLength+ " reached, ignoring following tokens");
+                break;
+              }
+            }
+            offset = offsetEnd+1;
+          } else {
+            offsetStart = offsetEnd = -1;
+            for (token = stream.next(); token != null; token = stream.next()) {
+              position += (token.getPositionIncrement() - 1);
+              addPosition();
+              if (++length >= maxFieldLength) {
+                if (infoStream != null)
+                  infoStream.println("maxFieldLength " +maxFieldLength+ " reached, ignoring following tokens");
+                break;
+              }
+            }
+          }
+            
+        } finally {
+          stream.close();
+        }
+      }
+
+      boost *= field.getBoost();
+    }
+
+    private final class ReusableStringReader extends Reader {
+      int upto;
+      int left;
+      String s;
+      void init(String s) {
+        this.s = s;
+        left = s.length();
+        this.upto = 0;
+      }
+      public int read(char[] c) {
+        return read(c, 0, c.length);
+      }
+      public int read(char[] c, int off, int len) {
+        if (left > len) {
+          s.getChars(upto, upto+len, c, off);
+          upto += len;
+          left -= len;
+          return len;
+        } else if (0 == left) {
+          return -1;
+        } else {
+          s.getChars(upto, upto+left, c, off);
+          int r = left;
+          left = 0;
+          upto = s.length();
+          return r;
+        }
+      }
+      public void close() {};
+    }
+
+    ReusableStringReader stringReader = new ReusableStringReader();
+
+    private final class CharBlock {
+      char[] buffer;
+      CharBlock next;
+    }
+
+    CharBlock charBlockStart;
+    CharBlock currentCharBlock;
+    char[] currentCharBuffer;
+    int currentCharUpto;
+    int currentCharLimit;
+    
+    void initCharBlocks() {
+      currentCharBlock = charBlockStart = new CharBlock();
+      // TODO: tune
+      currentCharLimit = 65536;
+      currentCharBuffer = currentCharBlock.buffer = new char[currentCharLimit];
+      currentCharUpto = 0;
+    }
+
+    void newCharBlock() {
+      if (currentCharBlock.next == null) {
+        CharBlock newBlock = new CharBlock();
+        currentCharBlock.next = newBlock;
+        newBlock.next = null;
+        newBlock.buffer = new char[65536];
+        currentCharBlock = newBlock;
+      } else
+        currentCharBlock = currentCharBlock.next;
+      currentCharBuffer = currentCharBlock.buffer;
+      currentCharUpto = 0;
+      currentCharLimit = currentCharBuffer.length;
+    }
+
+    void resetCharBlocks() {
+      currentCharBlock = charBlockStart;
+      currentCharBuffer = currentCharBlock.buffer;
+      currentCharUpto = 0;
+      currentCharLimit = currentCharBuffer.length;
+    }
+
+    private final class IntBlock {
+      int[] buffer;
+      IntBlock next;
+      int offset;
+    }
+
+    IntBlock intBlockStart;
+    IntBlock currentIntBlock;
+    int[] currentIntBuffer;
+    int currentIntUpto;
+    int currentIntOffset;
+    int currentIntLimit;
+    
+    void initIntBlocks() {
+      currentIntBlock = intBlockStart = new IntBlock();
+      // System.out.println("ALLOC start int block");
+      currentIntLimit = 65536;
+      currentIntBuffer = currentIntBlock.buffer = new int[currentIntLimit];
+      currentIntUpto = 0;
+    }
+
+    int newIntSlice(Posting p, final int level, final int mod) {
+      final int newSize;
+      final int newLevel;
+
+      // TODO: tune
+      switch(level) {
+      case 0:
+        newSize = 2+mod*2;
+        newLevel = 1;
+        break;
+      case 1:
+        newSize = 2+mod*4;
+        newLevel = 2;
+        break;
+      case 2:
+        newSize = 2+mod*8;
+        newLevel = 3;
+        break;
+      case 3:
+        newSize = 2+mod*16;
+        newLevel = 4;
+        break;
+      case 4:
+        newSize = 2+mod*32;
+        newLevel = 5;
+        break;
+      case 5:
+        newSize = 2+mod*64;
+        newLevel = 6;
+        break;
+      case 6:
+        newSize = 2+mod*128;
+        newLevel = 7;
+        break;
+      default:
+        newSize = 2+mod*256;
+        newLevel = 8;
+        break;
+      }
+
+      if (currentIntUpto + newSize > currentIntLimit)
+        nextIntBlock();
+      
+      // Put forwarding address at end of last chunk
+      p.buffer[p.limit] = currentIntOffset+currentIntUpto;
+      //System.out.println("    new int slice: upto=" + p.buffer[p.limit] + " saved @ upto=" + p.limit);
+      p.buffer = currentIntBuffer;
+      p.limit = currentIntUpto+newSize-1;
+      final int lastUpto = currentIntUpto;
+      p.buffer[p.limit] = newLevel;
+      currentIntUpto += newSize;
+      return lastUpto;
+    }
+
+    void nextIntBlock() {
+      if (currentIntBlock.next == null) {
+        //System.out.println("ALLOC new int block");
+        IntBlock newBlock = new IntBlock();
+        currentIntBlock.next = newBlock;
+        newBlock.next = null;
+        newBlock.buffer = new int[65536];
+        newBlock.offset = currentIntBlock.offset+currentIntBuffer.length;
+        currentIntBlock = newBlock;
+      } else
+        currentIntBlock = currentIntBlock.next;
+      currentIntOffset = currentIntBlock.offset;
+      currentIntBuffer = currentIntBlock.buffer;
+      currentIntUpto = 0;
+      currentIntLimit = currentIntBuffer.length;
+    }
+
+    void resetIntBlocks() {
+      currentIntBlock = intBlockStart;
+      currentIntOffset = currentIntBlock.offset;
+      currentIntBuffer = currentIntBlock.buffer;
+      currentIntUpto = 0;
+      currentIntLimit = currentIntBuffer.length;
+    }
+
+    void initIntSlice(Posting p, final int size) {
+      if (currentIntUpto + size > currentIntLimit)
+        nextIntBlock();
+      p.buffer = currentIntBuffer;
+      p.limit = currentIntUpto+size-1;
+      p.startIndex = currentIntOffset+currentIntUpto;
+      currentIntUpto += size;
+      //System.out.println("  init int slice: startIndex=" + p.startIndex);
+    }
+
+    final class Posting {				  // info about a Term in a doc
+      char[] text;
+      int textStart;
+      int hashCode;
+      int startIndex;                             // Location of first int slice
+      int[] buffer;                               // Current int slice
+      int limit;                                  // End point (buffer[limit]) of current int slice
+      Payload[] payloads;
+
+      // ONLY USE FOR DEBUGGING!
+      public String getText() {
+        int upto = textStart;
+        while(text[upto] != 0xffff)
+          upto++;
+        return new String(text, textStart, upto-textStart);
+      }
+
+      boolean equals(String otherText) {
+        final int len = otherText.length();
+        int pos = textStart;
+        int i=0;
+        for(;i<len;i++,pos++)
+          if (otherText.charAt(i) != text[pos])
+            return false;
+        return text[pos] == 0xffff;
+      }
+
+      boolean equals(char[] otherText, int offset, int len) {
+        int pos = textStart;
+        int otherPos = offset;
+        final int stopAt = len+otherPos;
+        for(;otherPos<stopAt;pos++,otherPos++)
+          if (otherText[otherPos] != text[pos])
+            return false;
+        return text[pos] == 0xffff;
+      }
+
+      int compareTo(Posting other) {
+        final char[] text1 = text;
+        int pos1 = textStart;
+        final char[] text2 = other.text;
+        int pos2 = other.textStart;
+        while(true) {
+          final char c1 = text1[pos1++];
+          final char c2 = text2[pos2++];
+          if (c1 < c2)
+            if (c2 == 0xffff)
+              return 1;
+            else
+              return -1;
+          else if (c2 < c1)
+            if (c1 == 0xffff)
+              return -1;
+            else
+              return 1;
+          else if (c1 == 0xffff)
+            return 0;
+        }
+      }
+    }
+
+    char[] localTextBuffer = new char[10];
+
+    private final void addPosition() {
+
+      // See if this text already has a posting in the hash.  On conflict in the hash we re-hash to another position.
+      final char[] tokenText;
+      final int tokenTextLen;
+      final int tokenTextOffset;
+
+      char[] t = token.termBuffer();
+      if (t == null) {
+        // Fallback to String token
+        String s = token.termText();
+        tokenTextLen = s.length();
+        if (tokenTextLen > localTextBuffer.length)
+          localTextBuffer = new char[(int)(1.25*tokenTextLen)];
+        s.getChars(0, tokenTextLen, localTextBuffer, 0);
+        tokenText = localTextBuffer;
+        tokenTextOffset = 0;
+        //System.out.println("  addPosition: string=" + s + " pos=" + position + " offsetStart=" + offsetStart + " offsetEnd=" + offsetEnd);
+      } else {
+        tokenText = t;
+        tokenTextLen = token.termBufferLength();
+        tokenTextOffset = token.termBufferOffset();
+        //System.out.println("  addPosition: buffer=" + new String(tokenText, tokenTextOffset, tokenTextLen) + " pos=" + position + " offsetStart=" + offsetStart + " offsetEnd=" + offsetEnd);
+      }
+
+      int code = 0;
+      for(int i=tokenTextLen-1;i>=0;i--)
+        code = (code*37) + tokenText[i];
+      int hashPos = code & postingsHashMask;
+      //System.out.println("    hashCode=" + code);
+
+      Posting p = postingsHash[hashPos];
+      if (p != null && (code != p.hashCode || !p.equals(tokenText, tokenTextOffset, tokenTextLen))) {
+        int code2 = code|1;
+        do {
+          hashPos = code2 & postingsHashMask;
+          code2 += code|1;
+          p = postingsHash[hashPos];
+        } while (p != null && (code != p.hashCode || !p.equals(tokenText, tokenTextOffset, tokenTextLen)));
+      }
+
+      if (p != null) {           // term seen before        
+
+        int[] buffer = p.buffer;
+        int limit = p.limit;
+        int upto = buffer[limit]>>>4;
+        int level = buffer[limit]&15;
+        final int freq = buffer[limit-1];
+
+        //System.out.println("    already seen: upto=" + upto + " level=" + level + " freq=" + buffer[p.limit-1]);
+
+        if (storeOffsets) {
+          if (upto == limit-1) {
+            // Need more space
+            buffer[upto] = position;
+            //System.out.println("    make new space1 pre: upto=" + upto + " limit=" + p.limit);
+            upto = newIntSlice(p, level, 3);
+            //System.out.println("    make new space1 post: upto=" + upto + " limit=" + p.limit);
+            buffer = p.buffer;
+            limit = p.limit;
+            level = buffer[limit];
+            buffer[upto] = offsetStart;
+            buffer[upto+1] = offsetEnd;
+            buffer[limit] = ((upto+2)<<4)+level;
+          } else if (upto == limit-2) {
+            // Need more space
+            buffer[upto] = position;
+            buffer[upto+1] = offsetStart;
+            //System.out.println("    make new space2 pre: upto=" + upto + " limit=" + p.limit);
+            upto = newIntSlice(p, level, 3);
+            //System.out.println("    make new space2 post: upto=" + upto + " limit=" + p.limit);
+            buffer = p.buffer;
+            limit = p.limit;
+            level = buffer[limit];
+            buffer[upto] = offsetEnd;
+            buffer[limit] = ((upto+1)<<4)+level;
+          } else if (upto == limit-3) {
+            // Need more space
+            buffer[upto] = position;
+            buffer[upto+1] = offsetStart;
+            buffer[upto+2] = offsetEnd;
+            //System.out.println("    make new space3 pre: upto=" + upto + " limit=" + p.limit);
+            upto = newIntSlice(p, level, 3);
+            //System.out.println("    make new space3 post: upto=" + upto + " limit=" + p.limit);
+            buffer = p.buffer;
+            limit = p.limit;
+            level = buffer[limit];
+            buffer[limit] = (upto<<4)+level;
+          } else {
+            //System.out.println("    no space needed: upto=" + upto + " limit=" + p.limit);
+            buffer[upto] = position;
+            buffer[upto+1] = offsetStart;
+            buffer[upto+2] = offsetEnd;
+            buffer[limit] = ((upto+3)<<4)+level;
+          }
+        } else {
+          if (upto == limit-1) {
+            // Need more space
+            //System.out.println("    make new space pre: upto=" + upto + " limit=" + p.limit);
+            buffer[upto] = position;
+            upto = newIntSlice(p, level, 1);
+            limit = p.limit;
+            buffer = p.buffer;
+            level = buffer[limit];
+            buffer[limit] = (upto<<4)+level;
+            //System.out.println("    make new space post: upto=" + upto + " limit=" + p.limit);
+          } else {
+            //System.out.println("    write pos " + position + " to upto=" + upto + " vs limit=" + p.limit + " level=" + level);
+            buffer[upto] = position;
+            buffer[limit] = ((upto+1)<<4)+level;
+          }
+        }
+
+        final Payload payload = token.getPayload();
+
+        if (payload != null) {
+          if (p.payloads == null) {
+            // lazily allocate payload array
+            p.payloads = new Payload[freq*2];
+          } else if (p.payloads.length <= freq) {
+            Payload[] newPayloads = new Payload[freq * 2];  // grow payloads array
+            System.arraycopy(p.payloads, 0, newPayloads, 0, p.payloads.length);
+            p.payloads = newPayloads;
+          }
+          p.payloads[freq] = payload;
+          fieldInfo.storePayloads = true;
+        }
+        buffer[limit-1] = 1+freq;               // update frequency
+
+        // }
+
+      } else {					  // word not seen before
+
+        // System.out.println("    new posting");
+        p = postingsArray[numPostings];
+        if (p == null)
+          p = postingsArray[numPostings] = new Posting();
+        numPostings++;
+        
+        final int textLen1 = 1+tokenTextLen;
+
+        if (textLen1 > 1024) {
+          p.text = new char[textLen1];
+          p.textStart = 0;
+        } else {
+          if (textLen1 + currentCharUpto > currentCharLimit)
+            newCharBlock();
+          p.text = currentCharBuffer;
+          p.textStart = currentCharUpto;
+          currentCharUpto += textLen1;
+        }
+        System.arraycopy(tokenText, tokenTextOffset, p.text, p.textStart, tokenTextLen);
+        // System.out.println("      posting=" + p + " textStart=" + p.textStart + " text=" + p.text);
+        p.text[p.textStart+tokenTextLen] = 0xffff;
+        p.hashCode = code;
+
+        postingsHash[hashPos] = p;
+
+        if (numPostings == postingsArrayLimit) {
+          // Resize postings array
+          int newSize = postingsArrayLimit*2;
+          Posting[] newPostings = new Posting[newSize];
+          System.arraycopy(postingsArray, 0, newPostings, 0, postingsArrayLimit);
+          postingsArray = newPostings;
+          postingsArrayLimit = newSize;
+
+          // Resize hash
+          newSize = postingsHashSize*2;
+          postingsHashMask = newSize-1;
+          Posting[] newHash = new Posting[newSize];
+          for(int i=0;i<postingsHashSize;i++) {
+            Posting p0 = postingsHash[i];
+            if (p0 != null) {
+              final int codex = p0.hashCode;
+              int hashPosx = codex & postingsHashMask;
+              assert hashPosx >= 0;
+              if (newHash[hashPosx] != null) {
+                int codex2 = codex|1;
+                do {
+                  hashPosx = codex2 & postingsHashMask;
+                  codex2 += codex|1;
+                } while (newHash[hashPosx] != null);
+              }
+              newHash[hashPosx] = p0;
+            }
+          }
+
+          postingsHash = newHash;
+          postingsHashSize = newSize;
+        }
+
+        final int[] buffer;
+        final int limit;
+        if (storeOffsets) {
+          initIntSlice(p, 5);
+          buffer = p.buffer;
+          limit = p.limit;
+          //System.out.println("    has offset: initIntSlice(4): limit=" + p.limit);
+          buffer[limit-4] = position;
+          buffer[limit-3] = offsetStart;
+          buffer[limit-2] = offsetEnd;
+        } else {
+          initIntSlice(p, 3);
+          buffer = p.buffer;
+          limit = p.limit;
+          //System.out.println("    no offset: initIntSlice(2): limit=" + p.limit);
+          buffer[limit-2] = position;
+          // System.out.println("    write pos " + position + " to upto=" + p.limit);
+        }
+        // TODO: for low freq we could encode into
+        // buffer[limit] then upgrade to separate int once
+        // freq is high
+        buffer[limit-1] = 1;
+        buffer[limit] = (limit-1)<<4;
+        
+        final Payload payload = token.getPayload();
+        if (payload != null) {
+          p.payloads = new Payload[1];
+          p.payloads[0] = payload;
+          fieldInfo.storePayloads = true;
+        } else
+          p.payloads = null;
+      }
+      position++;
+    }
+
+    private RAMSegment ramSegment;
+    private int numVectorFields;
+
+    public int writeVInt(byte[] b, int i) {
+      int upto = 0;
+      while ((i & ~0x7F) != 0) {
+        b[upto++] = ((byte)((i & 0x7f) | 0x80));
+        i >>>= 7;
+      }
+      b[upto++] = ((byte)i);
+      return upto;
+    }
+
+    private final class IntSliceReader {
+      IntBlock next;
+      IntBlock intBlock;
+      int[] buffer;
+      int upto;
+      int limit;
+      int mod;
+      int level;
+
+      public void init(int startIndex, int mod) {
+        // TODO: we could do binary search
+        // Seek to the starting intBlock
+        intBlock = intBlockStart;
+        buffer = intBlock.buffer;
+        next = intBlock.next;
+        while(intBlock.offset + intBlock.buffer.length <= startIndex) {
+          intBlock = next;
+          buffer = intBlock.buffer;
+          next = next.next;
+        }
+        level = 0;
+        this.mod = mod;
+        upto = startIndex-intBlock.offset;
+        limit = upto+mod+1;
+        //System.out.println("  reader.init: startIndex=" + startIndex + " upto=" + upto + " limit=" + limit);
+      }
+
+      public int next() {
+        // System.out.println("    next upto=" + upto + " limit=" + limit + " buffer=" + buffer);
+        if (upto == limit) {
+          // Skip to our next slice
+          final int nextIndex = buffer[upto];
+          //System.out.println("      seek to " + nextIndex + " vs end=" + (intBlock.offset+intBlock.buffer.length) + " fwd address=" + upto);
+          while(intBlock.offset + intBlock.buffer.length <= nextIndex) {
+            intBlock = next;
+            buffer = intBlock.buffer;
+            next = next.next;
+          }
+
+          final int newSize;
+
+          // TODO: tune
+          switch(level) {
+          case 0:
+            newSize = 2+mod*2;
+            level = 1;
+            break;
+          case 1:
+            newSize = 2+mod*4;
+            level = 2;
+            break;
+          case 2:
+            newSize = 2+mod*8;
+            level = 3;
+            break;
+          case 3:
+            newSize = 2+mod*16;
+            level = 4;
+            break;
+          case 4:
+            newSize = 2+mod*32;
+            level = 5;
+            break;
+          case 5:
+            newSize = 2+mod*64;
+            level = 6;
+            break;
+          case 6:
+            newSize = 2+mod*128;
+            level = 7;
+            break;
+          default:
+            newSize = 2+mod*256;
+            level = 8;
+            break;
+          }
+          upto = nextIndex-intBlock.offset;
+          limit = upto + newSize - 1;
+          // System.out.println("      skip to upto=" + upto + " limit=" + limit);
+        }
+        //System.out.println("    now return " + buffer[upto]);
+        return buffer[upto++];
+      }
+    }
+
+    IntSliceReader intSliceReader = new IntSliceReader();
+
+    /*
+     * Walk through all unique text tokens (Posting
+     * instances) found in this field and serialize them
+     * into a single RAM segment.
+     */
+    private void addPostingsAndVectors(FieldData fp)
+      throws CorruptIndexException, IOException {
+
+      final FieldInfo currentField = fp.fieldInfo;
+        
+      final Posting[] postings = postingsArray;
+      final int numTerms = numPostings;
+      final int fieldNumber = fp.fieldInfo.number;
+      final int postingsHashSize = postingsHash.length;
+
+      final boolean doVectors = fp.doVectors;
+      final boolean doPositions = fp.doVectorPositions;
+      final boolean doOffsets = fp.doVectorOffsets;
+      Posting lastPosting = null;
+
+      if (doVectors) {
+        if (numVectorFields == vectorFieldPointers.length) {
+          final int newSize = (int) (vectorFieldPointers.length*1.25);
+          vectorFieldPointers = MultiDocumentWriter.realloc(vectorFieldPointers, newSize);
+          vectorFieldNumbers = MultiDocumentWriter.realloc(vectorFieldNumbers, newSize);
+        }
+        vectorFieldNumbers[numVectorFields] = fieldNumber;
+        vectorFieldPointers[numVectorFields++] = tvfLocal.getFilePointer();
+        tvfLocal.writeVInt(numTerms);
+        byte bits = 0x0;
+        if (doPositions)
+          bits |= TermVectorsWriter.STORE_POSITIONS_WITH_TERMVECTOR;
+        if (doOffsets) 
+          bits |= TermVectorsWriter.STORE_OFFSET_WITH_TERMVECTOR;
+        tvfLocal.writeByte(bits);
+      }
+
+      final int intBlockMod = doOffsets ? 3:1;
+
+      // System.out.println("add postings");
+
+      for(int i=0;i<numTerms;i++) {
+        final Posting posting = postings[i];
+
+        // add an entry to the freq file
+        final int termFreq = posting.buffer[posting.limit-1];
+
+        // System.out.println(i + " of " + numTerms + ": text=" + posting.getText() + " freq=" + termFreq);
+
+        if (REUSE_POSTING_ARRAYS) {
+          // Fully clear hash as we go
+          final int code = posting.hashCode;
+          int code2 = code|1;
+          int hashPos = code & postingsHashMask;
+          // System.out.println("  pos=" + hashPos);
+          while (postingsHash[hashPos] != posting) {
+            hashPos = code2 & postingsHashMask;
+            code2 += code|1;
+          }
+          assert postingsHash[hashPos] == posting;
+          postingsHash[hashPos] = null;
+        }
+
+        intSliceReader.init(posting.startIndex, intBlockMod);
+
+        // Write term
+        final int prefix;
+        final char[] text2 = posting.text;
+        int offset2 = posting.textStart;
+        // System.out.println("  now parse posting=" + posting + " textStart=" + posting.textStart);
+        if (lastPosting == null)
+          prefix = 0;
+        else {
+          final char[] text1 = lastPosting.text;
+          int offset1 = lastPosting.textStart;
+          while(true) {
+            final char c1 = text1[offset1];
+            final char c2 = text2[offset2];
+            if (c1 != c2 || c1 == 0xffff) {
+              prefix = offset1-lastPosting.textStart;
+              break;
+            }
+            offset1++;
+            offset2++;
+          }
+        }
+        lastPosting = posting;
+
+        while(text2[offset2] != 0xffff) {
+          // System.out.println("     " + offset2 + ": " + text2[offset2]);
+          offset2++;
+        }
+
+        final int suffix = (offset2 - posting.textStart)-prefix;
+        // System.out.println("    prefix=" + prefix + " suffix=" + suffix);
+
+        out.writeVInt(prefix);                  // write shared prefix length
+        out.writeVInt(suffix);                  // write delta length
+
+        out.writeChars(text2, posting.textStart+prefix, suffix);    // write delta chars
+        
+        out.writeVInt(fieldNumber);             // write field num
+
+
+        out.writeByte((byte) 1);                       // write doc freq (1 as vint = 1)
+
+        if (doVectors) {
+          // TODO: this is a dup of above, can we share
+          // somehow?
+          tvfLocal.writeVInt(prefix);
+          tvfLocal.writeVInt(suffix);
+          tvfLocal.writeChars(text2, posting.textStart+prefix, suffix);
+          tvfLocal.writeVInt(termFreq);
+        }
+
+        // Placeholder for proxSize:
+        RAMCell proxBuffer = out.tail;
+        int proxUpto = out.upto;
+        out.writeInt(0);
+        final long pos = out.getFilePointer();
+
+        final Payload[] payloads = posting.payloads;
+
+        int lastPosition = 0;
+        for(int k=0;k<termFreq;k++) {
+          final int position = intSliceReader.next();
+          final int deltaCode = (position - lastPosition)<<1;
+          assert position > lastPosition || k == 0;
+
+          if (doPositions)
+            tvfLocal.writeVInt(position-lastPosition);
+
+          lastPosition = position;
+
+          if (doOffsets) {
+            // TODO: more efficient!
+            intSliceReader.next();
+            intSliceReader.next();
+          }
+
+          if (payloads != null) {
+            // We use a simpler but more space-consuming
+            // intermediate representation of payloads.  We
+            // have to do this so this data is independently
+            // concatenable.
+            final Payload payload = payloads[k];
+            if (payload != null) {
+              out.writeVInt(deltaCode|1);
+              //System.out.println("    write code=" + (deltaCode|1));
+              out.writeVInt(payload.length);
+              out.writeBytes(payload.data, payload.offset, payload.length);
+            } else {
+              //System.out.println("    write code=" + deltaCode);
+              out.writeVInt(deltaCode);
+            }
+          } else {
+            //System.out.println("    write code=" + deltaCode);
+            out.writeVInt(deltaCode);
+          }
+        }
+
+        // Go back and write actual prox size:
+        out.writeInt(proxBuffer, proxUpto, (int)(out.getFilePointer()-pos));
+
+        // ASSERT
+        //long pos1 = out.getFilePointer();
+        //long pos = proxOut.getFilePointer();
+
+        //assert pos1+pos == out.getFilePointer();
+
+        // Write freq (1 entry because this is 1 doc)
+        if (1 == termFreq)				  // optimize freq=1
+          out.writeBytes(docCodeFreq1Bytes, numBytesDocCodeFreq1);                 // set low bit of doc num.
+        else {
+          out.writeBytes(docCodeBytes, numBytesDocCode);                 // the document number
+          out.writeVInt(termFreq);		  // frequency in doc
+        }
+
+        if (doOffsets) {
+          intSliceReader.init(posting.startIndex, intBlockMod);
+          int lastOffset = 0;
+          for(int k=0;k<termFreq;k++) {
+            // Skip position
+            intSliceReader.next();
+            int startOffset = intSliceReader.next();
+            int endOffset = intSliceReader.next();
+            tvfLocal.writeVInt(startOffset-lastOffset);
+            tvfLocal.writeVInt(endOffset-startOffset);
+            lastOffset = endOffset;
+          }
+        }
+      }
+
+      // fp.numPostings = newNumPostings;
+      // System.out.println("  recycled " + newNumPostings + " out of " + limit + " postings");
+      if (REUSE_INT_ARRAYS)
+        resetIntBlocks();
+      else
+        initIntBlocks();
+
+      if (REUSE_CHAR_ARRAYS)
+        resetCharBlocks();
+      else
+        initCharBlocks();
+    }
+
+    private final void addNorms() throws IOException { 
+      for(int n=0;n<numFieldData;n++) {
+        FieldData fp = docFieldData[n];
+        FieldInfo fi = fp.fieldInfo;
+        // System.out.println("now check " + fi.name + ": " + norms[fi.number].upto + " vs " + docID);
+        if (fi.isIndexed && !fi.omitNorms) {
+          assert norms[fi.number].upto <= docID;
+          // System.out.println(docID + " add norm for field " + fi.name + ": " + fieldBoosts[fi.number] + " " + fieldLengths[fi.number] + " " + sim.lengthNorm(fi.name, fieldLengths[fi.number]));
+          norms[fi.number].fill(docID);
+          norms[fi.number].add(fp.norm);
+        }
+      }
+    }
+
+    /*
+      Holds data associated with a single field.  A document
+      may have many occurrences of a given field name; we
+      gather all such occurrences here (docFields) so that
+      we can process the entire field at once.
+    */
+    private class FieldData implements Comparable {
+
+      FieldInfo fieldInfo;
+
+      int fieldCount;
+      Fieldable[] docFields = new Fieldable[1];
+
+      // private int fieldNumber;
+      float norm;
+      int gen;
+      FieldData next;
+
+      boolean doVectors;
+      boolean doVectorPositions;
+      boolean doVectorOffsets;
+
+      public FieldData(FieldInfo fieldInfo) {
+        this.fieldInfo = fieldInfo;
+      }
+
+      public int compareTo(Object o) {
+        return fieldInfo.name.compareTo(((FieldData) o).fieldInfo.name);
+      }
+    }
+
+    /**
+     * Holds details for one segment being merged.
+     */
+    final class SegmentMergeInfo {
+      SegmentMergeInfo next;
+      SegmentMergeInfo hashNext;
+      int idx;
+
+      char textBuffer[] = new char[10];
+      int textLength;
+      int fieldNumber;
+
+      IndexInput in;
+      IndexInput in2;
+      IndexOutput out;
+
+      private long proxSize;
+
+      int totDF;
+      long totProxSize;
+
+      int df;
+      int hashCode;
+      private boolean proxSizeIsVariable;
+
+      SegmentMergeInfo(int idx) {
+        this.idx = idx;
+      }
+
+      public void setInput(IndexInput in, IndexInput in2) {
+        this.in = in;
+        this.in2 = in2;
+      }
+      public void setOutput(IndexOutput out) {
+        this.out = out;
+      }
+
+      void setProxSizeVariable(boolean v) {
+        proxSizeIsVariable = v;
+      }
+
+      public int sort(int[] result) {
+        int num = 0;
+        SegmentMergeInfo smi2 = this;
+        totDF = 0;
+        totProxSize = 0;
+        while(smi2 != null) {
+          totDF += smi2.df;
+          totProxSize += smi2.proxSize;
+          result[num++] = smi2.idx;
+          smi2 = smi2.next;
+        }
+
+        if (2 == num) {
+          if (result[0] > result[1]) {
+            final int t = result[0];
+            result[0] = result[1];
+            result[1] = t;
+          }
+        } else
+          // TODO: maybe radix sort here?
+          Arrays.sort(result, 0, num);
+
+        return num;
+      }
+
+      public boolean next() throws IOException {
+
+        // ASSERT
+        long pos = in.getFilePointer();
+
+        final int start = in.readVInt();
+        if (start == Integer.MAX_VALUE)
+          return false;
+
+        //System.out.println("    next: idx=" + idx + " start=" + start + " textLength=" + textLength + " pos=" + pos + " buffer=" + ((RAMReader) in).buffer);
+        assert start <= textLength;
+        final int length = in.readVInt();
+        textLength = start + length;
+        if (textLength > textBuffer.length) {
+          char[] newTextBuffer = new char[(int) (textLength*1.5)];
+          System.arraycopy(textBuffer, 0, newTextBuffer, 0, start);
+          textBuffer = newTextBuffer;
+        }
+        in.readChars(textBuffer, start, length);
+        //System.out.println("      next term=" + new String(textBuffer, 0, textLength) + " len=" + textLength + " field=" + fieldNumber);
+
+        fieldNumber = in.readVInt();
+        df = in.readVInt();
+        if (proxSizeIsVariable)
+          proxSize = in.readVLong();
+        else
+          proxSize = in.readInt();
+        //System.out.println("      next: df=" + df + " proxSize=" + proxSize + " after term pos=" + in.getFilePointer());
+
+        hashCode = 0;
+        for(int i=textLength-1;i>=0;i--)
+          hashCode = (hashCode * 37) + textBuffer[i];
+        hashCode += fieldNumber;
+
+        return true;
+      }
+
+      public void flushFreq() throws IOException {
+        // We can't copyBytes with freq because we must
+        // re-encode the deltas between docIDs.
+        int docID = 0;
+        for(int i=0;i<df;i++) {
+          final int code = in.readVInt();
+          docID += code >>> 1;
+          assert docID <= maxDocID;
+          assert docID > lastDocID || 0==lastDocID;
+          final int newCode = (docID-lastDocID)<<1;
+          lastDocID = docID;
+          if ((code & 1) != 0)
+            out.writeVInt(newCode|1);
+          else {
+            out.writeVInt(newCode);
+            out.writeVInt(in.readVInt());
+          }
+        }
+      }
+
+      public void close() throws IOException {
+        MultiDocumentWriter.close(in, in2);
+      }
+
+      public boolean equals(SegmentMergeInfo other) {
+        if (other.fieldNumber == fieldNumber &&
+            other.textLength == textLength) {
+          final char[] textA = textBuffer;
+          final char[] textB = other.textBuffer;
+          for(int i=0;i<textLength;i++)
+            if (textA[i] != textB[i])
+              return false;
+          return true;
+        } else
+          return false;
+      }
+    }
+
+    /*
+      This queue is used for merging RAM and Flushed
+      segments.  It's modified from the PriorityQueue used
+      for main segment merging: it has two tiers.  The first
+      tier, using a priority queue, keeps track of each
+      unique term that's we've seen.  The second tier, using
+      linked list inside SMI, keeps track of all SMIs that
+      have this term.  This "de-dupping" is a good
+      performance gain when you are merging a very large
+      number of segments since the "lessThan" method is
+      quite costly.
+     */
+
+    // Shared merge queue
+    MergeQueue mergeQueue = new MergeQueue();
+
+    final class MergeQueue {
+
+      // Records all idx's that are pending for a given field+text:
+      private SegmentMergeInfo[] heap;
+      private SegmentMergeInfo[] hash;
+      private int size;
+      private int maxSize;
+      private int hashMask;
+      
+      void init(int newMaxSize) {
+        size = 0;
+        if (maxSize < newMaxSize) {
+          if (newMaxSize < 32)
+            maxSize = 32;
+          else
+            maxSize = (int) (1.25*newMaxSize);
+          int heapSize = maxSize + 1;
+          heap = new SegmentMergeInfo[heapSize];
+          this.maxSize = maxSize;
+          int hashSize = 32;
+          int target = 3*maxSize;
+          while(hashSize < target)
+            hashSize *= 2;
+          hash = new SegmentMergeInfo[hashSize];
+          hashMask = hashSize-1;
+        }
+      }
+
+      /**
+       * Adds a SegmentMergeInfo to a PriorityQueue in log(size) time.
+       * If one tries to add more objects than maxSize from initialize
+       * a RuntimeException (ArrayIndexOutOfBound) is thrown.
+       */
+      public void put(SegmentMergeInfo smi) {
+
+        //System.out.println("Q: put text=" + new String(smi.textBuffer, 0, smi.textLength) + " field=" + smi.fieldNumber + " idx=" + smi.idx + " smi=" + smi + " hash=" + smi.hashCode);
+
+        // See if the term for this SMI is already hashed
+        int hashPos = smi.hashCode & hashMask;
+        SegmentMergeInfo smi2 = hash[hashPos];
+        //System.out.println("  hash[" + hashPos + "] = " + smi2);
+        while(smi2 != null && (smi2.hashCode != smi.hashCode || !smi.equals(smi2)))
+          smi2 = smi2.hashNext;
+
+        if (smi2 != null) {
+          // This term is already in the queue, so we don't
+          // add it again.  Instead, we chain it (linked
+          // list) to the SMI already enrolled.
+          smi.next = smi2.next;
+          smi2.next = smi;
+          // System.out.println("    already seen");
+        } else {
+          // First time we are seeing this field+text, so
+          // enroll into hash & priority queue:
+          heap[++size] = smi;
+          smi.next = null;
+          smi.hashNext = hash[hashPos];
+          hash[hashPos] = smi;
+          upHeap();
+          // System.out.println("    not yet seen; set hash[" + hashPos + "]=" + smi + "; set smi.hashNext=" + smi.hashNext);
+        }
+      }
+
+      /** Removes and returns the least element of the PriorityQueue in log(size)
+          time. */
+      public SegmentMergeInfo pop() {
+        SegmentMergeInfo smi = heap[1];			  // save first value
+        // System.out.println("Q: pop text=" + new String(smi.textBuffer, 0, smi.textLength));
+        heap[1] = heap[size];			  // move last to first
+        size--;
+        downHeap();				  // adjust heap
+
+        // Also remove from hash:
+        int hashPos = smi.hashCode & hashMask;
+        SegmentMergeInfo lastSmi2 = null;
+        SegmentMergeInfo smi2 = hash[hashPos];
+        while(smi2 != smi) {
+          lastSmi2 = smi2;
+          smi2 = smi2.hashNext;
+        }
+        assert smi2 != null;
+        if (lastSmi2 == null)
+          hash[hashPos] = smi.hashNext;
+        else
+          lastSmi2.hashNext = smi.hashNext;
+        return smi;
+      }
+
+      private void upHeap() {
+        int i = size;
+        SegmentMergeInfo node = heap[i];			  // save bottom node
+        int j = i >>> 1;
+        while (j > 0 && lessThan(node, heap[j])) {
+          heap[i] = heap[j];			  // shift parents down
+          i = j;
+          j = j >>> 1;
+        }
+        heap[i] = node;				  // install saved node
+      }
+
+      private void downHeap() {
+        int i = 1;
+        SegmentMergeInfo node = heap[i];			  // save top node
+        int j = i << 1;				  // find smaller child
+        int k = j + 1;
+        if (k <= size && lessThan(heap[k], heap[j])) {
+          j = k;
+        }
+        while (j <= size && lessThan(heap[j], node)) {
+          heap[i] = heap[j];			  // shift up child
+          i = j;
+          j = i << 1;
+          k = j + 1;
+          if (k <= size && lessThan(heap[k], heap[j])) {
+            j = k;
+          }
+        }
+        heap[i] = node;				  // install saved node
+      }
+
+      // return true if a < b
+      protected boolean lessThan(SegmentMergeInfo stiA, SegmentMergeInfo stiB) {
+
+        // first by field
+        if (stiA.fieldNumber == stiB.fieldNumber) {
+
+          // then by text
+
+          // TODO: most of the time we are comparing things
+          // with long shared prefixes; is there some way to
+          // optimize for this fact?
+          final char[] textA = stiA.textBuffer;
+          final char[] textB = stiB.textBuffer;
+          final int len = stiA.textLength < stiB.textLength ? stiA.textLength : stiB.textLength;
+          for(int i=0;i<len;i++) {
+            final char charA = textA[i];
+            final char charB = textB[i];
+            if (charA < charB)
+              return true;
+            else if (charA > charB)
+              return false;
+          }
+
+          if (stiA.textLength < stiB.textLength)
+            return true;
+          else if (stiA.textLength > stiB.textLength)
+            return false;
+
+          // Should never get here because dups are handled by
+          // first tier hash:
+          System.out.println("  failed text=" + new String(stiA.textBuffer, 0, stiA.textLength));
+          assert false;
+          return false;
+
+        } else {
+          // fields differ:
+          String fieldA = fieldInfos.fieldName(stiA.fieldNumber);
+          String fieldB = fieldInfos.fieldName(stiB.fieldNumber);
+          return fieldA.compareTo(fieldB) < 0;
+        }
+      }
+    }
+
+    int lastDocID;
+    char[] lastChars = new char[10];
+
+    // Merges RAM segments into a single segment, which may be
+    // in RAM or in the real directory.  Input segments for
+    // merging should be placed in mergeInputs already.
+    final void mergeTerms(int numSegmentsIn, IndexOutput out) throws IOException {
+    
+      MergeQueue queue = null;
+
+      // nocommit
+      //boolean debug = false;
+
+      queue = mergeQueue;
+      queue.init(numSegmentsIn);
+
+      // initialize queue
+      for (int i=0;i<numSegmentsIn;i++) {
+        ThreadState.SegmentMergeInfo smi = mergeInputs[i];
+        smi.setOutput(out);
+        if (smi.next())
+          queue.put(smi);				  // initialize queue
+      }
+
+      int lastCharsLength = 0;
+
+      long lastProx = 0;
+
+      while (queue.size > 0) {
+
+        final SegmentMergeInfo smi = queue.pop();
+
+        //System.out.println("\n  merge term " + fieldInfos.fieldName(smi.fieldNumber) + ":" + new String(smi.textBuffer, 0, smi.textLength));
+
+        // TODO: now do I still need my own lastText buffer?
+        int len = lastCharsLength < smi.textLength ? lastCharsLength : smi.textLength;
+        int prefix = len;
+        for(int i=0;i<len;i++) {
+          if (lastChars[i] != smi.textBuffer[i]) {
+            prefix = i;
+            break;
+          }
+        }
+
+        final int length = smi.textLength - prefix;
+
+        if (lastChars.length < prefix + length) {
+          char[] n = new char[prefix+length];
+          System.arraycopy(lastChars, 0, n, 0, lastCharsLength);
+          lastChars = n;
+        }
+        System.arraycopy(smi.textBuffer, prefix, lastChars, prefix, length);
+        lastCharsLength = smi.textLength;
+
+        //System.out.println("    write prefix=" + prefix + " length=" + length + " pos=" + out.getFilePointer() + " buffer=" + ((RAMWriter) out).buffer);
+
+        out.writeVInt(prefix);                   // write shared prefix length
+
+        out.writeVInt(length);                  // write delta length
+
+        out.writeChars(lastChars, prefix, length);    // write delta chars
+
+        // TODO: better encoding of this: encode 'new field'
+        // transition?  stuff it as lower bit of prefix above?
+        final int fieldNumber = smi.fieldNumber;
+        out.writeVInt(fieldNumber);      // write field num
+
+        // First pass: pull all SMIs that have the same term,
+        // sum up total df (number of docs that have this
+        // term) and proxSize (number of bytes required to
+        // hold all prox data).
+
+        lastDocID = 0;
+
+        if (smi.next == null) {
+          // Optimize singleton
+          out.writeVInt(smi.df);                 // write doc freq
+          out.writeVLong(smi.proxSize);
+          copyBytes(smi.in, out, smi.proxSize);
+          smi.flushFreq();
+          if (smi.next())
+            queue.put(smi);
+        } else {
+          final int numToMerge = smi.sort(mergeIDXArray);
+          // System.out.println("    " + numToMerge + " to merge");
+
+          // Now write term entry:
+          out.writeVInt(smi.totDF);                 // write doc freq
+          out.writeVLong(smi.totProxSize);
+          //System.out.println("      write: after term pos=" + out.getFilePointer());
+
+          // 2nd pass: write all prox's
+          long lastPos = out.getFilePointer();
+          for(int i=0;i<numToMerge;i++) {
+            final SegmentMergeInfo smi2 = mergeInputs[mergeIDXArray[i]];
+            copyBytes(smi2.in, out, smi2.proxSize);
+          }
+          assert out.getFilePointer() == lastPos+smi.totProxSize;
+
+          // 3rd pass: write all freqs, and push back into
+          // queue if each has more data
+          for(int i=0;i<numToMerge;i++) {
+            final SegmentMergeInfo smi2 = mergeInputs[mergeIDXArray[i]];
+            smi2.flushFreq();
+            if (smi2.next())
+              queue.put(smi2);
+          }
+        }
+      }
+
+      // mark end
+      out.writeVInt(Integer.MAX_VALUE);
+      // debug = false;
+      if (REUSE_INT_ARRAYS)
+        resetIntBlocks();
+      else
+        initIntBlocks();
+    }
+
+    private final TermInfo termInfo = new TermInfo(); // minimize consing
+    private IndexOutput freqOutput;
+    private IndexOutput proxOutput;
+    private int skipInterval;
+    private int lastDoc;
+    private int lastPayloadLength;
+    private int df;
+    private boolean currentFieldStorePayloads;
+
+    // Write out the postings & dictionary to real output
+    // files, in the "real" lucene file format.  This is to
+    // finalize a segment.
+    void flushTerms() throws IOException {
+
+      if (infoStream != null)
+        infoStream.println("flush postings as segment " + segment + " docID=" + MultiDocumentWriter.this.docID);
+
+      // First we must pre-merge flushed segments:
+      /*
+      boolean any;
+      do {
+        any = false;
+        for(int i=flushedLevelCounts.length-1;i>=0;i--)
+          if (flushedLevelCounts[i] > 0) {
+            // Merge up all levels below the current max level:
+            for(int j=0;j<i;j++)
+              while(flushedLevelCounts[j] > 0) {
+                mergeFlushedSegments(this, j);
+                any = true;
+              }
+
+            // Do one more merge if we have too many flushed
+            // segments at the max level:
+            if (flushedLevelCounts[i] > flushedMergeFactor) {
+              mergeFlushedSegments(this, i);
+              any  = true;
+            }
+            break;
+          }
+      } while(any);
+      */
+      while(flushedSegments.size() > flushedMergeFactor) {
+        if (infoStream != null)
+          infoStream.println("  merge flushed segments before flushing terms: now " + flushedSegments.size() + " flushed segments");
+        mergeFlushedSegments(this, flushedSegments.size()-flushedMergeFactor, flushedSegments.size(), -1);
+      }
+      
+      TermInfosWriter termInfosWriter = null;
+
+      final int numRAMSegments = ramSegments.size();
+      final int numFlushedSegments = flushedSegments.size();
+      final int numSegmentsIn = numRAMSegments + numFlushedSegments;
+
+      resizeMergeInputs(numSegmentsIn);
+
+      int numDoc = 0;
+      long oldSize = 0;
+      long newSize = 0;
+
+      try {
+        freqOutput = directory.createOutput(segment + ".frq");
+        proxOutput = directory.createOutput(segment + ".prx");
+        termInfosWriter = new TermInfosWriter(directory, segment, fieldInfos,
+                                              writer.getTermIndexInterval());
+        skipInterval = termInfosWriter.skipInterval;
+
+        MergeQueue queue = mergeQueue;
+        queue.init(numSegmentsIn);
+
+        int i=0;
+        for (;i<numFlushedSegments;i++) {
+          FlushedSegment fs = (FlushedSegment) flushedSegments.get(i);
+          oldSize += fs.size;
+          SegmentMergeInfo smi = mergeInputs[i];
+          smi.setProxSizeVariable(true);
+          // TODO: try cloning instead?
+          smi.setInput(directory.openInput(tempFileName(fs.segment)),
+                       directory.openInput(tempFileName(fs.segment)));
+          //System.out.println("  FLUSHED idx " + i + " numDoc=" + fs.numDoc + " size=" + fs.size);
+          if (smi.next())
+            queue.put(smi);				  // initialize queue
+          numDoc += fs.numDoc;
+        }
+
+        for (;i<numSegmentsIn;i++) {
+          RAMSegment rs = (RAMSegment) ramSegments.get(i-numFlushedSegments);
+          // System.out.println("  RAM idx " + i + " numDoc= " + rs.numDoc + " size=" + rs.size);
+          //printAlloc("    data", rs.data, rs.dataLimit);
+          oldSize += rs.size;
+          SegmentMergeInfo smi = mergeInputs[i];
+          if (1 == rs.numDoc)
+            smi.setProxSizeVariable(false);
+          else
+            smi.setProxSizeVariable(true);
+
+          // Tell the "term+freq" stream NOT to free buffers
+          // as it goes:
+          RAMReader in = getReader(rs.data, rs.dataLimit);
+          in.doFreeBuffers = false;
+          smi.setInput(in,getReader(rs.data, rs.dataLimit));
+          if (smi.next())
+            queue.put(smi);				  // initialize queue
+          numDoc += rs.numDoc;
+        }
+
+        char[] lastChars = new char[10];
+        int lastCharsLength = 0;
+        int currentField = -1;
+        String currentFieldName = null; 
+
+        while (queue.size > 0) {
+
+          final SegmentMergeInfo smi = queue.pop();
+
+          long freqPointer = freqOutput.getFilePointer();
+          long proxPointer = proxOutput.getFilePointer();
+
+          final int fieldNumber = smi.fieldNumber;
+
+          if (currentField != fieldNumber) {
+            currentField = fieldNumber;
+            currentFieldName = fieldInfos.fieldName(fieldNumber);
+            currentFieldStorePayloads = fieldInfos.fieldInfo(fieldNumber).storePayloads;
+          }
+
+          // TODO: can we avoid cons'ing here?
+          Term term = new Term(currentFieldName, new String(smi.textBuffer, 0, smi.textLength));
+
+          df = 0;
+          lastDoc = 0;
+          lastPayloadLength = -1;
+          lastDocID = 0;
+
+          // System.out.println("  " + term);
+          resetSkip();
+
+          final int numToMerge = smi.sort(mergeIDXArray);        
+
+          for(int j=0;j<numToMerge;j++) {
+            final SegmentMergeInfo smi2 = mergeInputs[mergeIDXArray[j]];
+            // System.out.println("    idx " + smi2.idx);
+            appendPostings(smi2);
+            if (smi2.next())
+              queue.put(smi2);
+          }
+
+          long skipPointer = writeSkip();
+
+          assert df == smi.totDF;
+
+          termInfo.set(df, freqPointer, proxPointer, (int) (skipPointer - freqPointer));
+          termInfosWriter.add(term, termInfo);
+        }
+      } finally {
+        newSize = freqOutput.length() + proxOutput.length() + directory.fileLength(segment + ".tis") + directory.fileLength(segment + ".tii");
+        close(freqOutput, proxOutput, termInfosWriter);
+        for (int i=0;i<numSegmentsIn;i++) {
+          // Necessary so final buffer is freed:
+          if (mergeInputs[i].in2 != null)
+            mergeInputs[i].in2.seek(mergeInputs[i].in.getFilePointer());
+          mergeInputs[i].close();
+        }
+      }
+
+      totalSize = 0;
+      totalFlushedSize = 0;
+      flushedCount = 0;
+      Arrays.fill(levelSizes, 0);
+      Arrays.fill(levelCounts, 0);
+      Arrays.fill(flushedLevelSizes, 0);
+      Arrays.fill(flushedLevelCounts, 0);
+      ramSegments.clear();
+      flushedSegments.clear();
+
+      if (infoStream != null)
+        infoStream.println("  oldSize=" + oldSize + " newSize=" + newSize + " new/old=" + ((int)(100.0*newSize/oldSize)) + "%");
+
+      files = null;
+    
+      // NOTE: if we merged flushed segments, we have now just
+      // obsoleted some files.  But we don't call deleter
+      // checkpoint here because our caller (IndexWriter) will
+      // do so shortly after calling us.
+      if (REUSE_INT_ARRAYS)
+        resetIntBlocks();
+      else
+        initIntBlocks();
+    }
+
+    /* Called only by flushTerms, to append all postings for
+     * a given term into the main freq/prox postings
+     * output. */
+    void appendPostings(SegmentMergeInfo smi) throws IOException {
+      final int termFreq = smi.df;
+
+      final IndexInput freq = smi.in;
+      long pos = freq.getFilePointer();
+      freq.seek(pos+smi.proxSize);
+
+      final IndexInput prox = smi.in2;
+      prox.seek(pos);
+    
+      int doc = 0;
+
+      for(int i=0;i<termFreq;i++) {
+
+        if ((++df % skipInterval) == 0)
+          bufferSkip(lastDoc, lastPayloadLength);
+
+        final int docCode = freq.readVInt();
+        doc += docCode >>> 1;
+
+        // System.out.println("      doc=" + doc + " lastDoc=" + lastDoc + " df=" + df);
+
+        assert doc <= maxDocID;
+      
+        //if (!(doc > lastDoc || df == 1))
+        //System.out.println("doc=" + doc + " lastDoc=" + lastDoc + " df=" + df);
+
+        assert doc > lastDoc || df == 1;
+
+        final int termDocFreq;
+        final int newDocCode = (doc-lastDoc)<<1;
+        lastDoc = doc;
+
+        if ((docCode & 1) != 0) {
+          freqOutput.writeVInt(newDocCode|1);
+          termDocFreq = 1;
+          //System.out.println("      doc " + doc + " freq 1");
+          //System.out.println("        write " + (newDocCode|1));
+        } else {
+          freqOutput.writeVInt(newDocCode);
+          termDocFreq = freq.readVInt();
+          //System.out.println("      doc " + doc + " freq " + termDocFreq);
+          //System.out.println("        write " + newDocCode + " then " + termDocFreq);
+          freqOutput.writeVInt(termDocFreq);
+        }
+        //System.out.println("      #pos=" + termDocFreq);
+
+        /** See {@link DocumentWriter#writePostings(Posting[], String) for 
+         *  documentation about the encoding of positions and payloads
+         */
+        for(int j=0;j<termDocFreq;j++) {
+          final int deltaCode = prox.readVInt();
+          //System.out.println("        " + j + ": code=" + deltaCode + " upto=" + ((RAMReader) prox).upto + " buffer=" + ((RAMReader) prox).buffer);
+          if (currentFieldStorePayloads) {
+            // Current field has seen at least one payload in this session
+            final int payloadLength;
+            if ((deltaCode & 1) != 0)
+              // This token has a payload
+              payloadLength = prox.readVInt();
+            else
+              // This token does not have a payload
+              payloadLength = 0;
+            if (payloadLength != lastPayloadLength) {
+              proxOutput.writeVInt(deltaCode | 1);
+              proxOutput.writeVInt(payloadLength);
+              lastPayloadLength = payloadLength;
+            } else
+              proxOutput.writeVInt(deltaCode & ~1);
+
+            if (payloadLength > 0)
+              copyBytes(prox, proxOutput, payloadLength);
+          } else {
+            assert 0 == (deltaCode&1);
+            proxOutput.writeVInt(deltaCode>>1);
+          }
+        }
+      }
+    
+      assert prox.getFilePointer() == pos + smi.proxSize;
+    }
+
+    private RAMWriter skipBuffer = new RAMWriter();
+    private int lastSkipDoc;
+    private int lastSkipPayloadLength;
+    private long lastSkipFreqPointer;
+    private long lastSkipProxPointer;
+
+    private void resetSkip() {
+      lastSkipDoc = 0;
+      lastSkipPayloadLength = -1;  // we don't have to write the first length in the skip list
+      lastSkipFreqPointer = freqOutput.getFilePointer();
+      lastSkipProxPointer = proxOutput.getFilePointer();
+    }
+
+    private void bufferSkip(int doc, int payloadLength) throws IOException {
+      //System.out.println("    buffer skip: freq ptr " + freqPointer + " prox " + proxPointer);
+      //System.out.println("      vs last freq ptr " + lastSkipFreqPointer + " prox " + lastSkipProxPointer);
+
+      // To efficiently store payloads in the posting lists we do not store the length of
+      // every payload. Instead we omit the length for a payload if the previous payload had
+      // the same length.
+      // However, in order to support skipping the payload length at every skip point must be known.
+      // So we use the same length encoding that we use for the posting lists for the skip data as well:
+      // Case 1: current field does not store payloads
+      //           SkipDatum                 --> DocSkip, FreqSkip, ProxSkip
+      //           DocSkip,FreqSkip,ProxSkip --> VInt
+      //           DocSkip records the document number before every SkipInterval th  document in TermFreqs. 
+      //           Document numbers are represented as differences from the previous value in the sequence.
+      // Case 2: current field stores payloads
+      //           SkipDatum                 --> DocSkip, PayloadLength?, FreqSkip,ProxSkip
+      //           DocSkip,FreqSkip,ProxSkip --> VInt
+      //           PayloadLength             --> VInt    
+      //         In this case DocSkip/2 is the difference between
+      //         the current and the previous value. If DocSkip
+      //         is odd, then a PayloadLength encoded as VInt follows,
+      //         if DocSkip is even, then it is assumed that the
+      //         current payload length equals the length at the previous
+      //         skip point
+
+      final int delta = doc - lastSkipDoc;
+      if (currentFieldStorePayloads) {
+        if (payloadLength == lastSkipPayloadLength)
+          // the current payload length equals the length at the previous skip point,
+          // so we don't store the length again
+          skipBuffer.writeVInt(delta << 1);
+        else {
+          // the payload length is different from the previous one. We shift the DocSkip, 
+          // set the lowest bit and store the current payload length as VInt.
+          skipBuffer.writeVInt((delta << 1) + 1);
+          skipBuffer.writeVInt(payloadLength);
+          lastSkipPayloadLength = payloadLength;
+        }
+      } else
+        // current field does not store payloads
+        skipBuffer.writeVInt(delta);
+
+      long freqPointer = freqOutput.getFilePointer();
+      long proxPointer = proxOutput.getFilePointer();
+      skipBuffer.writeVInt((int) (freqPointer - lastSkipFreqPointer));
+      skipBuffer.writeVInt((int) (proxPointer - lastSkipProxPointer));
+      lastSkipFreqPointer = freqPointer;
+      lastSkipProxPointer = proxPointer;
+
+      lastSkipDoc = doc;
+    }
+
+    long writeSkip() throws IOException {
+      long skipPointer = freqOutput.getFilePointer();
+      skipBuffer.writeTo(freqOutput);
+      return skipPointer;
+    }
+
+    SegmentMergeInfo mergeInputs[] = new SegmentMergeInfo[0];
+    int[] mergeIDXArray;
+
+    final void resizeMergeInputs(final int minSize) {
+      if (mergeInputs.length < minSize) {
+        int size = (int) (minSize*1.25);
+        SegmentMergeInfo[] newMergeInputs = new SegmentMergeInfo[size];
+        System.arraycopy(mergeInputs, 0, newMergeInputs, 0, mergeInputs.length);
+        for(int i=mergeInputs.length;i<size;i++)
+          newMergeInputs[i] = new SegmentMergeInfo(i);
+        mergeInputs = newMergeInputs;
+        mergeIDXArray = new int[size];
+      }
+    }
+  }
+
+  // nocommit: recycle
+  RAMReader getReader(RAMCell head, int finalLimit) {
+    return new RAMReader(head, finalLimit);
+  }
+
+  synchronized ThreadState getThreadState(Document doc) throws IOException {
+    ThreadState state = null;
+    while(true) {
+      final int size = freeThreadStates.size();
+      if (flushPending) {
+        try {
+          wait();
+        } catch (InterruptedException e) {
+        }
+      } else if (0 == size) {
+
+        // There are no free thread states, or, a flush is
+        // trying to happen
+        if (numWaiting >= MAX_WAIT_QUEUE) {
+          // System.out.println("do wait");
+
+          // There are too many thread states in line write
+          // to the index so we now pause to give them a
+          // chance to get scheduled by the JVM and finish
+          // their documents.  Once we wake up again, a
+          // recycled ThreadState should be available else
+          // we wait again.
+          // System.out.println("w " + Thread.currentThread().getName());
+          try {
+            wait();
+          } catch (InterruptedException e) {
+          }
+          // System.out.println("  wd " + Thread.currentThread().getName());
+
+        } else {
+          // OK, just create a new thread state
+          state = new ThreadState();
+          numThreadState++;
+          break;
+        }
+      } else {
+        // Use recycled thread state
+        state = (ThreadState) freeThreadStates.get(size-1);
+        freeThreadStates.remove(size-1);
+        break;
+      }
+    }
+
+    boolean success = false;
+    try {
+      state.init(doc, docID++);
+      success = true;
+    } finally {
+      if (!success)
+        freeThreadStates.add(state);
+    }
+
+    return state;
+  }
+
+  void addDocument(Document doc, Analyzer analyzer)
+          throws CorruptIndexException, IOException {
+
+    // First pass: go through all fields in doc, updating
+    // shared FieldInfos and writing any stored fields:
+    final ThreadState state = getThreadState(doc);
+    boolean success = false;
+    try {
+      state.processDocument(analyzer);
+      success = true;
+    } finally {
+      if (success)
+        finishDocument(state);
+      else {
+        // nocommit: need to do some cleanup of the thread state?
+        freeThreadStates.add(state);
+      }
+    }
+  }
+
+  long netMerge0Time;
+  long netMerge1Time;
+  long netFlushedMergeTime;
+  long netDocTime;
+  long netFlushTime;
+  long netSegmentTime;
+
+  /*
+   * Does the synchronized work to finish/flush the inverted document.
+   */
+  private synchronized void finishDocument(ThreadState state) throws IOException {
+
+    // Now write the indexed document to the real files.
+    if (nextWriteDocID == state.docID) {
+      // It's my turn, so write everything now:
+      try {
+        writeDocument(state);
+      } finally {
+        nextWriteDocID++;
+        // Recycle our thread state back in the free pool
+        freeThreadStates.add(state);
+      }
+
+      // If any states were waiting on me, sweep through and
+      // flush those that are enabled by my write.
+      boolean doNotify = numWaiting >= MAX_WAIT_QUEUE || flushPending;
+      if (numWaiting > 0) {
+        while(true) {
+          int upto = 0;
+          for(int i=0;i<numWaiting;i++) {
+            ThreadState s = waitingThreadStates[i];
+            if (s.docID == nextWriteDocID) {
+              writeDocument(s);
+              nextWriteDocID++;
+
+              // Recycle thread state back in the free pool
+              freeThreadStates.add(s);
+
+            } else {
+              // Compact as we go
+              waitingThreadStates[upto++] = waitingThreadStates[i];
+            }
+          }
+          if (upto == numWaiting) 
+            break;
+          numWaiting = upto;
+        }
+      }
+
+      // Now notify any incoming calls to addDocument
+      // (above) that are waiting on our line to
+      // shrink
+      if (doNotify) notifyAll();
+
+    } else {
+      // Another thread got a docID before me, but, it
+      // hasn't finished its processing.  So add myself to
+      // the line but don't hold up this thread.
+      if (numWaiting == waitingThreadStates.length) {
+        ThreadState[] newWaiting = new ThreadState[2*waitingThreadStates.length];
+        System.arraycopy(waitingThreadStates, 0, newWaiting, 0, numWaiting);
+        waitingThreadStates = newWaiting;
+      }
+      waitingThreadStates[numWaiting++] = state;
+      // System.out.println("now wait: numWaiting=" + numWaiting);
+    }
+  }
+
+  // Move all per-document state that was accumulated in the
+  // ThreadState into the "real" stores
+  private void writeDocument(ThreadState state) throws IOException {
+
+    ramSegments.add(state.ramSegment);
+
+    // Append stored fields to the real FieldsWriter:
+    fieldsWriter.getIndexStream().writeLong(fieldsWriter.getFieldsStream().getFilePointer());
+    state.fdxLocal.free();
+    state.fdtLocal.writeTo(fieldsWriter.getFieldsStream());
+
+    // Append buffered term vectors output to the real outputs:
+    if (tvx != null) {
+      tvx.writeLong(tvd.getFilePointer());
+      if (state.numVectorFields > 0) {
+        tvd.writeVInt(state.numVectorFields);
+        for(int i=0;i<state.numVectorFields;i++)
+          tvd.writeVInt(state.vectorFieldNumbers[i]);
+        assert 0 == state.vectorFieldPointers[0];
+        tvd.writeVLong(tvf.getFilePointer());
+        long lastPos = state.vectorFieldPointers[0];
+        for(int i=1;i<state.numVectorFields;i++) {
+          long pos = state.vectorFieldPointers[i];
+          tvd.writeVLong(pos-lastPos);
+          lastPos = pos;
+        }
+        state.tvfLocal.writeTo(tvf);
+      }
+    }
+
+    state.addNorms();
+
+    long size = state.ramSegment.size;
+    
+    totalSize += size;
+
+    // just created a new level 0 segment:
+    levelSizes[0] += size;
+    levelCounts[0]++;
+
+    // System.out.println("  after add: " + levelSizes[0] + " vs " + (ramBufferSize/20));
+
+    if (levelSizes[0] > ramBufferSize/14) {
+      long t0 = System.currentTimeMillis();
+      mergeRAMSegments(state, 0);
+      netMerge0Time += (System.currentTimeMillis()-t0);
+      if (levelSizes[1] > ramBufferSize/7 && level0Compression < 0.7) {
+        t0 = System.currentTimeMillis();
+        mergeRAMSegments(state, 1);
+        netMerge1Time += (System.currentTimeMillis()-t0);
+      }
+    }
+
+    if (doSelfFlush && totalSize > ramBufferSize) {
+      long t0 = System.currentTimeMillis();
+      flushRAMSegments(state);
+      netFlushTime += (System.currentTimeMillis()-t0);
+    }
+  }
+
+  // For debugging
+  public void printAlloc(String prefix, RAMCell head, int limit) {
+    RAMCell c = head;
+    System.out.print(prefix + ":");
+    if (c == null)
+      System.out.println(" null");
+    else
+      while(c != null) {
+        if (c.next == null) {
+          System.out.println(" " + c.buffer.length + "(" + limit + ")");
+          break;
+        } else {
+          System.out.print(" " + c.buffer.length);
+          c = c.next;
+        }
+      }
+  }
+
+  long getRAMUsed() {
+    return totalSize;
+  }
+
+  private final String tempFileName(int count) {
+    return segment + "x" + count + "." + IndexFileNames.TEMP_FLUSH_EXTENSION;
+  }
+
+  long netDocSize;
+  int netDocCount;
+
+  // Called when RAM buffer is full; we now merge all RAM
+  // segments to a single flushed segment:
+  final synchronized void flushRAMSegments(ThreadState state) throws IOException {
+
+    if (infoStream != null) {
+      String name = tempFileName(flushedCount);
+      infoStream.println("\n" + getElapsedTime() + ": flush ram segments at docID " + docID + ", to " + name.substring(0, name.length()-4) + ": totalRam=" + (totalSize/1024/1024) + " MB");
+    }
+    System.out.println("FLUSH TEMP @ docID=" + docID + " numDoc=" + (docID-lastFlushDocID) + "; RAM=" + totalSize);
+    System.out.println("  mem now: " + bean.getHeapMemoryUsage().getUsed());
+    System.out.println("  avg doc=" + (netDocSize/netDocCount) + " bytes");
+    lastFlushDocID = docID;
+
+    // nocommit
+    netDocSize = 0;
+    netDocCount = 0;
+
+    IndexOutput out = directory.createOutput(tempFileName(flushedCount));
+
+    final int numSegmentsIn = ramSegments.size();
+    long newSize;
+    long oldSize = totalSize;
+
+    state.resizeMergeInputs(numSegmentsIn);
+    
+    int numDoc = 0;
+    for(int i=0;i<numSegmentsIn;i++) {
+      RAMSegment rs = (RAMSegment) ramSegments.get(i);
+      numDoc += rs.numDoc;
+      //System.out.println("  idx " + mergeInputs[i].idx + " numDoc=" + rs.numDoc + " freqSize=" + rs.freq.size + " proxSize=" + rs.prox.size + " termSize=" + rs.terms.size);
+      //rs.terms.printAlloc("    terms");
+      //rs.freq.printAlloc("     freq");
+      //rs.prox.printAlloc("     prox");
+      state.mergeInputs[i].setInput(getReader(rs.data, rs.dataLimit), null);
+      if (1 == rs.numDoc)
+        state.mergeInputs[i].setProxSizeVariable(false);
+      else
+        state.mergeInputs[i].setProxSizeVariable(true);
+    }
+
+    try {
+      state.mergeTerms(numSegmentsIn, out);
+      newSize = out.getFilePointer();
+    } finally {
+      close(out, null);
+      for (int i=0;i<numSegmentsIn;i++)
+        state.mergeInputs[i].close();
+    }
+
+    //for(int i=0;i<numSegmentsIn;i++)
+    //RAMSegment rs = (RAMSegment) ramSegments.get(i);
+    
+    ramSegments.clear();
+    Arrays.fill(levelCounts, 0);
+    Arrays.fill(levelSizes, 0);
+
+    FlushedSegment seg = new FlushedSegment(numDoc, flushedCount++, newSize);
+    flushedSegments.add(seg);
+    flushedLevelCounts[0]++;
+    flushedLevelSizes[0] += newSize;
+    totalFlushedSize += totalSize;
+    totalSize = 0;
+
+    if (infoStream != null) {
+      infoStream.println("  oldRAMSize=" + oldSize + " newFlushedSize=" + newSize + " new/old=" + ((int)(100.0*newSize/oldSize)) + "% totalRAM=0 totalFlushed=" + (totalFlushedSize/1024/1024) + " MB");
+    }
+    System.out.println("after flush:");
+    for(int i=flushedLevelCounts.length-1;i>=0;i--)
+      System.out.println("  level " + i + ": count=" + flushedLevelCounts[i]);
+
+    files = null;
+
+    // Must "register" our newly created files with the
+    // deleter so that when they later decref they get
+    // deleted:
+    synchronized(writer) {
+      writer.getDeleter().checkpoint(writer.segmentInfos, false);
+    }
+
+    // Maybe cascade merges.  We do slightly different
+    // policy than normal segment merges: we let 20 level 0
+    // segments accumulate first, then we merge the first 10
+    // into a level 1 segment.  After another 10 level 0
+    // segments we merge the first 10 level 0's into another
+    // level 1, etc.  This better "spreads" / "postpones"
+    // the merge work so we don't pay a massive wasted merge
+    // price only to find it's time to flush a real segment.
+    int mergeLevel = 0;
+    while(mergeLevel < flushedLevelCounts.length && flushedLevelCounts[mergeLevel] == 2*flushedMergeFactor)
+      mergeFlushedSegments(state, mergeLevel++);
+  }
+
+  // Merge flushed segments into a single new flushed segment
+  final void mergeFlushedSegments(ThreadState state, int level) throws IOException {
+
+    int start = 0;
+    int end = 0;
+    for(int i=flushedLevelCounts.length-1;i>=level;i--) {
+      start = end;
+      end += flushedLevelCounts[i];
+    }
+
+    if (end-start > flushedMergeFactor)
+      end = start+flushedMergeFactor;
+
+    if (infoStream != null)
+      infoStream.println("merge flushed segments: level " + level);
+
+    mergeFlushedSegments(state, start, end, level);
+  }
+
+  final void mergeFlushedSegments(ThreadState state, int start, int end, int level) throws IOException {
+    long t0 = System.currentTimeMillis();
+    if (infoStream != null) {
+      String name = tempFileName(flushedCount);
+      infoStream.println("merge flushed segments to " + name.substring(0, name.length()-4) + ": start " + start + " to end " + end);
+    }
+
+    long newSize;
+    int numDoc;
+    long oldSize;
+
+    FlushedSegment newSegment;
+
+    if (1 == end-start) {
+      // Degenerate case
+      newSegment = (FlushedSegment) flushedSegments.get(start);
+      numDoc = newSegment.numDoc;
+      oldSize = newSize = newSegment.size;
+    } else {
+
+      // maybe reallocate
+      state.resizeMergeInputs(end-start);
+      numDoc = 0;
+      oldSize = 0;
+      IndexOutput out = directory.createOutput(tempFileName(flushedCount));
+
+      try {
+        int upto = 0;
+        for (int i=start;i<end;i++) {
+          FlushedSegment fs = (FlushedSegment) flushedSegments.get(i);
+          IndexInput in = directory.openInput(tempFileName(fs.segment));
+          ThreadState.SegmentMergeInfo smi = state.mergeInputs[upto++];
+          smi.setInput(in, null);
+          smi.setProxSizeVariable(true);
+          numDoc += fs.numDoc;
+          oldSize += in.length();
+        }
+
+        state.mergeTerms(end-start, out);
+        newSize = out.getFilePointer();
+      } finally {
+        close(out, null);
+        for(int i=0;i<end-start;i++)
+          state.mergeInputs[i].close();
+      }
+
+      // TODO: transactional here:
+      for (int i = end-1; i > start; i--)    // remove old infos & add new
+        flushedSegments.remove(i);
+      
+      newSegment = new FlushedSegment(numDoc, flushedCount++, newSize);
+      flushedSegments.set(start, newSegment);
+    }
+
+
+    if (level != -1) {
+      if (flushedLevelSizes.length == level+1) {
+        flushedLevelSizes = realloc(flushedLevelSizes, 1+flushedLevelSizes.length);
+        flushedLevelCounts = realloc(flushedLevelCounts, 1+flushedLevelCounts.length);
+      }
+
+      flushedLevelSizes[level] -= oldSize;
+      flushedLevelSizes[1+level] += newSize;
+
+      flushedLevelCounts[level] -= (end-start);
+      flushedLevelCounts[1+level]++;
+    }
+
+    totalFlushedSize += newSize - oldSize;
+
+    if (infoStream != null) {
+      infoStream.println("  done: oldSize=" + oldSize + " newSize=" + newSize + " new/old=" + ((int)(100.0*newSize/oldSize)) + "% totalFlushed=" + (totalFlushedSize/1024/1024) + " MB");
+      if (level != -1)
+        for(int i=flushedLevelCounts.length-1;i>=0;i--)
+          System.out.println("  level " + i + ": count=" + flushedLevelCounts[i]);
+    }
+
+    files = null;
+
+    // nocommit: should I just give IFD list of files to delete?
+    // Have deleter remove our now unreferenced files:
+    synchronized(writer) {
+      writer.getDeleter().checkpoint(writer.segmentInfos, false);
+    }
+    netFlushedMergeTime += System.currentTimeMillis()-t0;
+  }
+
+  static void close(IndexOutput f0, IndexOutput f1) throws IOException {
+    IOException keep = null;
+    try {
+      if (f0 != null) f0.close();
+    } catch (IOException e) {
+      keep = e;
+    } finally {
+      try {
+        if (f1 != null) f1.close();
+      } catch (IOException e) {
+        if (keep != null) throw keep;
+      }
+    }
+  }
+
+  static void close(IndexInput f0, IndexInput f1) throws IOException {
+    IOException keep = null;
+    try {
+      if (f0 != null) f0.close();
+    } catch (IOException e) {
+      keep = e;
+    } finally {
+      try {
+        if (f1 != null) f1.close();
+      } catch (IOException e) {
+        if (keep != null) throw keep;
+      }
+    }
+  }
+
+  static void close(IndexOutput freq, IndexOutput prox, TermInfosWriter terms) throws IOException {
+    IOException keep = null;
+    try {
+      if (freq != null) freq.close();
+    } catch (IOException e) {
+      keep = e;
+    } finally {
+      try {
+        if (prox != null) prox.close();
+      } catch (IOException e) {
+        if (keep == null) keep = e;
+      } finally {
+        try {
+          if (terms != null) terms.close();
+        } catch (IOException e) {
+          if (keep == null) keep = e;
+        } finally {
+          if (keep != null) throw keep;
+        }
+      }
+    }
+  }
+  
+  float level0Compression;
+
+  NumberFormat nf = NumberFormat.getInstance();
+  String getElapsedTime() {
+    long t = System.currentTimeMillis();
+    nf.setMaximumFractionDigits(1);
+    nf.setMinimumFractionDigits(1);
+    return nf.format((t-startTime)/1000.0) + " sec";
+  }
+
+  // In-memory merge: reads multiple ram segments (in the
+  // modified format) and replaces with a single ram segment.
+  final void mergeRAMSegments(ThreadState state, int level) throws IOException {
+
+    int start = 0;
+    int end = 0;
+    for(int i=levelCounts.length-1;i>=level;i--) {
+      start = end;
+      end += levelCounts[i];
+    }
+
+    if (infoStream != null) {
+      infoStream.println("\n" + getElapsedTime() + ": merge ram segments: level " + level + ": start idx " + start + " to end idx " + end + " docID=" + docID);
+      System.out.println("  RAM: " + totalSize);
+    }
+    long oldSize;
+    //long oldTermsSize;
+    //long oldFreqSize;
+    //long oldProxSize;
+    long newSize;
+
+    int numDoc;
+    RAMSegment newRAMSegment;
+
+    if (end == start+1) {
+      // Degenerate case, if suddenly an immense document
+      // comes through
+      newRAMSegment = (RAMSegment) ramSegments.get(start);
+      //oldTermsSize = newRAMSegment.terms.size;
+      //oldFreqSize = newRAMSegment.freq.size;
+      //oldProxSize = newRAMSegment.prox.size;
+      newSize = oldSize = newRAMSegment.size;
+      numDoc = newRAMSegment.numDoc;
+    } else {
+
+      state.resizeMergeInputs(end-start);
+      final int numSegmentsIn = end-start;
+
+      oldSize = 0;
+      //oldTermsSize = 0;
+      //oldFreqSize = 0;
+      //oldProxSize = 0;
+      int upto = 0;
+      numDoc = 0;
+      for(int i=start;i<end;i++) {
+        RAMSegment rs = (RAMSegment) ramSegments.get(i);
+        numDoc += rs.numDoc;
+        oldSize += rs.size;
+        //oldTermsSize += rs.terms.size;
+        //oldFreqSize += rs.freq.size;
+        //oldProxSize += rs.prox.size;
+        ThreadState.SegmentMergeInfo smi = state.mergeInputs[upto++];
+        //System.out.println("  idx " + smi.idx + " numDoc=" + rs.numDoc + " size=" + rs.size);
+        //rs.terms.printAlloc("    terms");
+        //rs.freq.printAlloc("     freq");
+        //rs.prox.printAlloc("     prox");
+        smi.setInput(getReader(rs.data, rs.dataLimit), null);
+        if (1 == rs.numDoc)
+          smi.setProxSizeVariable(false);
+        else
+          smi.setProxSizeVariable(true);
+      }
+
+      final int startLevel;
+      if (oldSize < 256)
+        startLevel = 0;
+      else if (oldSize < 1024)
+        startLevel = 1;
+      else if (oldSize < 4096)
+        startLevel = 2;
+      else if (oldSize < 16384)
+        startLevel = 3;
+      else
+        startLevel = 4;
+
+      assert state.out.buffer == null;
+      state.out.setStartLevel(startLevel);
+
+      try {
+        state.mergeTerms(end-start, state.out);
+      } finally {
+        for (int i=0;i<numSegmentsIn;i++)
+          state.mergeInputs[i].close();
+      }
+
+      // TODO: transactional here:
+      for (int i = end-1; i > start; i--) {    // remove old infos & add new
+        RAMSegment rs = (RAMSegment) ramSegments.get(i);
+        ramSegments.remove(i);
+      }
+
+      newRAMSegment = new RAMSegment(numDoc, state.out);
+      newSize = newRAMSegment.size;
+      ramSegments.set(start, newRAMSegment);
+    }
+
+    if (levelSizes.length == level+1) {
+      levelSizes = realloc(levelSizes, 1+levelSizes.length); 
+      levelCounts = realloc(levelCounts, 1+levelCounts.length); 
+    }
+
+    levelSizes[level] -= oldSize;
+    levelSizes[1+level] += newSize;
+
+    levelCounts[level] -= (end-start);
+    levelCounts[1+level]++;
+
+    totalSize += newSize - oldSize;
+    if (0 == level)
+      level0Compression = ((float) newSize)/oldSize;
+
+    if (infoStream != null) {
+      infoStream.println("  oldSize=" + oldSize + " newSize=" + newSize + " new/old=" + ((int)(100.0*newSize/oldSize)) + "% totalRAM=" + (totalSize/1024/1024) + " MB");
+      //infoStream.println("  termsSize=" + termsOut.size + " freqSize=" + freqOut.size + " proxSize=" + proxOut.size);
+      //infoStream.println("  oldTermsSize=" + oldTermsSize + " oldFreqSize=" + oldFreqSize + " oldProxSize=" + oldProxSize);
+    }    
+  }
+
+  final void createCompoundFile(String segment)
+          throws IOException {
+
+    CompoundFileWriter cfsWriter = new CompoundFileWriter(directory, segment + ".cfs");
+
+    // Basic files
+    for (int i = 0; i < IndexFileNames.COMPOUND_EXTENSIONS.length; i++)
+      cfsWriter.addFile(segment + "." + IndexFileNames.COMPOUND_EXTENSIONS[i]);
+
+    // Fieldable norm files
+    if (flushedNorms)
+      cfsWriter.addFile(segment + "." + IndexFileNames.NORMS_EXTENSION);
+
+    // Vector files
+    if (flushedVectors)
+      for (int i = 0; i < IndexFileNames.VECTOR_EXTENSIONS.length; i++)
+        cfsWriter.addFile(segment + "." + IndexFileNames.VECTOR_EXTENSIONS[i]);
+
+    // Perform the merge
+    cfsWriter.close();
+  }
+
+  private void writeNorms() throws IOException {
+    IndexOutput output = null;
+    try {
+      final int numField = fieldInfos.size();
+      for (int i=0;i<numField;i++) {
+        FieldInfo fi = fieldInfos.fieldInfo(i);
+        if (fi.isIndexed && !fi.omitNorms) {
+          BufferedNorms n = norms[i];
+          n.fill(docID);
+          if (output == null) { 
+            output = directory.createOutput(segment + "." + IndexFileNames.NORMS_EXTENSION);
+            output.writeBytes(SegmentMerger.NORMS_HEADER, SegmentMerger.NORMS_HEADER.length);
+          }
+          n.out.writeTo(output);
+          n.upto = 0;
+          hasNorms = true;
+        }
+      }
+    } finally {
+      if (output != null) { 
+        output.close();
+      }
+    }
+  }
+
+  // NOTE: not thread safe once we allow merges to run concurrently
+  byte[] byteBuffer = new byte[1024];
+
+  /** Copy numBytes from srcIn to destIn */
+  void copyBytes(IndexInput srcIn, IndexOutput destIn, long numBytes) throws IOException {
+    assert numBytes > 0;
+
+    if (srcIn instanceof RAMReader) {
+      RAMReader src = (RAMReader) srcIn;
+      while(true) {
+        final int chunk = src.limit - src.upto;
+        if (chunk < numBytes) {
+          // Src is the limit
+          destIn.writeBytes(src.buffer, src.upto, chunk);
+          src.nextBuffer();
+          numBytes -= chunk;
+        } else if (chunk == numBytes) {
+          // Matched
+          destIn.writeBytes(src.buffer, src.upto, chunk);
+          src.nextBuffer();
+          break;
+        } else {
+          // numBytes is the limit
+          destIn.writeBytes(src.buffer, src.upto, (int) numBytes);
+          src.upto += numBytes;
+          break;
+        }
+      }
+    } else {
+      // Use intermediate buffer
+      while(numBytes > 0) {
+        final int chunk;
+        if (numBytes > 1024) {
+          chunk = 1024;
+        } else {
+          chunk = (int) numBytes;
+        }
+        srcIn.readBytes(byteBuffer, 0, chunk);
+        destIn.writeBytes(byteBuffer, chunk);
+        numBytes -= chunk;
+      }
+    }
+  }
+
+  /** If non-null, a message will be printed to this if maxFieldLength is reached.
+   */
+  void setInfoStream(PrintStream infoStream) {
+    this.infoStream = infoStream;
+    // nocommit
+    // this.infoStream = System.out;
+  }
+
+  private class RAMSegment {
+    int numDoc;
+    RAMCell data;
+    int dataLimit;
+    long size;
+    public RAMSegment(int numDoc, RAMWriter out) {
+      this.numDoc = numDoc;
+      size = out.size;
+
+      this.data = out.head;
+      this.dataLimit = out.upto;
+      out.reset();
+    }
+  }
+
+  private class FlushedSegment {
+    int numDoc;
+    int segment;
+    long size;
+    public FlushedSegment(int numDoc, int segment, long size) {
+      this.numDoc = numDoc;
+      this.segment = segment;
+      this.size = size;
+    }
+  }
+
+  final static int MAX_RAM_CELL_LEVEL = 4;
+  RAMCell freeCells[] = new RAMCell[1+MAX_RAM_CELL_LEVEL];
+
+  synchronized void recycle(RAMCell cell) {
+    cell.next = freeCells[cell.level];
+    freeCells[cell.level] = cell;
+  }
+
+  public RAMCell alloc(final int level, final int subLevel) {
+    RAMCell r;
+    synchronized(this) {
+      r = freeCells[level];
+      if (r != null)
+        freeCells[level] = r.next;
+      else
+        r = null;
+    }
+
+    if (r == null)
+      r = new RAMCell(level, subLevel);
+    else {
+      r.next = null;
+      r.subLevel = (byte) subLevel;
+    }
+    return r;
+  }
+
+  private static final class RAMCell {
+
+    byte[] buffer;
+    RAMCell next;
+    byte level;
+    byte subLevel;
+
+    public RAMCell(final int level, final int subLevel) {
+      this.level = (byte) level;
+      this.subLevel = (byte) subLevel;
+      int size = 0;
+      switch(this.level) {
+      case 0:
+        size = 64;
+        break;
+      case 1:
+        size = 256;
+        break;
+      case 2:
+        size = 1024;
+        break;
+      case 3:
+        size = 4096;
+        break;
+      case 4:
+        size = 16384;
+        break;
+      }
+      buffer = new byte[size];
+    }
+  }
+
+  private final class RAMWriter extends IndexOutput {
+
+    RAMCell head;
+    RAMCell tail;
+    int upto;
+    int limit;
+    byte[] buffer;
+    long size;
+
+    boolean isFree = false;
+
+    void setStartLevel(int level) {
+      assert head == null;
+      head = tail = alloc(level, 0);
+      buffer = head.buffer;
+      upto = 0;
+      limit = head.buffer.length;
+      size = limit;
+    }
+
+    public void writeByte(byte b) {
+      assert !isFree;
+      if (upto == limit)
+        nextBuffer();
+
+      buffer[upto++] = b;
+    }
+
+    // Move all of our bytes to out and reset
+    public void writeTo(IndexOutput out) throws IOException {
+      assert !isFree;
+      while(head != null) {
+        final int numBytes;
+        if (head.next == null)
+          numBytes = upto;
+        else
+          numBytes = head.buffer.length;
+        out.writeBytes(head.buffer, numBytes);
+        RAMCell next = head.next;
+        recycle(head);
+        head = next;
+      }
+      reset();
+    }
+
+    private void reset() {
+      assert !isFree;
+      head = tail = null;
+      buffer = null;
+      limit = upto = 0;
+      size = 0;
+    }
+
+    private void free() {
+      assert !isFree;
+      while(head != null) {
+        RAMCell c = head.next;
+        recycle(head);
+        head = c;
+      }
+      reset();
+    }
+
+    // Write an int at a specific spot:
+    public void writeInt(RAMCell cell, int upto, int v) {
+      byte[] buffer = cell.buffer;
+      int limit = buffer.length;
+      if (upto == limit) {
+        buffer = cell.next.buffer;
+        limit = buffer.length;
+        upto = 0;
+      }
+      assert 0 == buffer[upto];
+      buffer[upto++] = (byte) (v >> 24);
+      if (upto == limit) {
+        buffer = cell.next.buffer;
+        limit = buffer.length;
+        upto = 0;
+      }
+      assert 0 == buffer[upto];
+      buffer[upto++] = (byte) (v >> 16);
+      if (upto == limit) {
+        buffer = cell.next.buffer;
+        limit = buffer.length;
+        upto = 0;
+      }
+      assert 0 == buffer[upto];
+      buffer[upto++] = (byte) (v >> 8);
+      if (upto == limit) {
+        buffer = cell.next.buffer;
+        limit = buffer.length;
+        upto = 0;
+      }
+      assert 0 == buffer[upto];
+      buffer[upto++] = (byte) v;
+    }
+
+    public void writeBytes(byte[] b, int offset, int numBytes) {
+      assert !isFree;
+      assert numBytes > 0;
+      switch(numBytes) {
+      case 4:
+        writeByte(b[offset++]);
+      case 3:
+        writeByte(b[offset++]);
+      case 2:
+        writeByte(b[offset++]);
+      case 1:
+        writeByte(b[offset++]);
+        break;
+      default:
+        if (upto == limit)
+          nextBuffer();
+        // System.out.println(" writeBytes: buffer=" + buffer + " head=" + head + " offset=" + offset + " nB=" + numBytes);
+        while(true) {
+          int chunk = limit - upto;
+          if (chunk >= numBytes) {
+            System.arraycopy(b, offset, buffer, upto, numBytes);
+            upto += numBytes;
+            break;
+          } else {
+            System.arraycopy(b, offset, buffer, upto, chunk);
+            offset += chunk;
+            numBytes -= chunk;
+            nextBuffer();
+          }
+        }
+      }
+    }
+
+    public void nextBuffer() {
+      assert !isFree;
+
+      final int level;
+      final int subLevel;
+      if (tail == null) {
+        level = 0;
+        subLevel = 0;
+      } else if (tail.level < MAX_RAM_CELL_LEVEL) {
+        if (7 == tail.subLevel) {
+          level = 1+tail.level;
+          subLevel = 0;
+        } else {
+          level = tail.level;
+          subLevel = 1+tail.subLevel;
+        }
+      } else {
+        subLevel = 0;
+        level = MAX_RAM_CELL_LEVEL;
+      }
+
+      RAMCell c = alloc(level, subLevel);
+
+      if (head == null)
+        head = tail = c;
+      else {
+        tail.next = c;
+        tail = c;
+      }
+
+      limit = c.buffer.length;
+      size += limit;
+      buffer = c.buffer;
+
+      upto = 0;
+    }
+
+    public long getFilePointer() {
+      assert !isFree;
+      return size - (limit-upto);
+    }
+
+    public long length() {
+      assert !isFree;
+      return getFilePointer();
+    }
+
+    public void close() {}
+    public void flush() {throw new RuntimeException("not implemented");}
+    public void seek(long pos) {throw new RuntimeException("not implemented");}
+  }
+
+  // Limited IndexInput for "read once".  This frees each
+  // buffer from the head once it's been read.
+  private final class RAMReader extends IndexInput {
+
+    boolean doFreeBuffers;
+    int readLimit;
+    int upto;
+    int limit;
+    RAMCell head;
+    byte[] buffer;
+    long pos;
+
+    // ASSERT
+    boolean finished = true;
+    
+    RAMReader(RAMCell head, int limit) {
+      reset(head, limit);
+    }
+
+    public void reset(RAMCell head, int limit) {
+      // Make sure we were fully read
+      assert finished;
+      doFreeBuffers = true;
+      finished = false;
+      readLimit = limit;
+      this.head = head;
+      upto = 0;
+      pos = 0;
+
+      if (head == null) {
+        assert 0 == readLimit;
+        buffer = null;
+      } else {
+        buffer = head.buffer;
+        if (head.next == null) {
+          this.limit = readLimit;
+          assert this.limit > 0 && this.limit <= buffer.length;
+        } else
+          this.limit = buffer.length;
+      }
+    }
+
+    public byte readByte() {
+      byte b = buffer[upto++];
+      if (upto == limit)
+        nextBuffer();
+      return b;
+    }
+
+    public void nextBuffer() {
+      RAMCell c = head.next;
+      pos += limit;
+      if (doFreeBuffers)
+        recycle(head);
+      head = c;
+      upto = 0;
+      if (head != null) {
+        buffer = head.buffer;
+        if (head.next == null) {
+          limit = readLimit;
+          assert limit > 0 && limit <= buffer.length;
+        } else
+          limit = buffer.length;
+      } else {
+        // ASSERT
+        finished = true;
+        buffer = null;
+      }
+    }
+
+    public void seek(long toPos) {
+      assert toPos > (pos+upto);
+      while(true) {
+        if (pos+limit > toPos) {
+          // Seek within current buffer
+          upto = (int) (toPos-pos);
+          break;
+        } else
+          nextBuffer();
+      }
+    }
+
+    public long getFilePointer() {
+      return pos+upto;
+    }
+
+    public void readBytes(byte[] b, int offset, int len) {throw new RuntimeException("not implemented");}
+    public void close() {}
+    public long length() {throw new RuntimeException("not implemented");}
+  }
+
+  static final byte defaultNorm = Similarity.encodeNorm(1.0f);
+
+  private class BufferedNorms {
+
+    RAMWriter out = new RAMWriter();
+    int upto;
+
+    void add(float norm) {
+      byte b = Similarity.encodeNorm(norm);
+      out.writeByte(b);
+      upto++;
+    }
+
+    void fill(int docID) {
+      // System.out.println("  now fill: " + upto + " vs " + docID);
+      while(upto < docID) {
+        // fill in docs that didn't have this field:
+        out.writeByte(defaultNorm);
+        upto++;
+      }
+    }
+  }
+
+  static long[] realloc(long[] array, int newSize) {
+    long[] newArray = new long[newSize];
+    System.arraycopy(array, 0, newArray, 0, array.length);
+    return newArray;
+  }
+
+  static int[] realloc(int[] array, int newSize) {
+    int[] newArray = new int[newSize];
+    System.arraycopy(array, 0, newArray, 0, array.length);
+    return newArray;
+  }
+}

Property changes on: src/java/org/apache/lucene/index/MultiDocumentWriter.java
___________________________________________________________________
Name: svn:eol-style
   + native

Index: src/java/org/apache/lucene/index/SegmentMergeInfo.java
===================================================================
--- src/java/org/apache/lucene/index/SegmentMergeInfo.java	(revision 523296)
+++ src/java/org/apache/lucene/index/SegmentMergeInfo.java	(working copy)
@@ -73,9 +73,8 @@
 
   final void close() throws IOException {
     termEnum.close();
-    if (postings != null) {
-    postings.close();
+    if (postings != null)
+      postings.close();
   }
 }
-}
 
Index: src/java/org/apache/lucene/store/IndexOutput.java
===================================================================
--- src/java/org/apache/lucene/store/IndexOutput.java	(revision 523296)
+++ src/java/org/apache/lucene/store/IndexOutput.java	(working copy)
@@ -125,6 +125,24 @@
     }
   }
 
+  public void writeChars(char[] s, int start, int length)
+       throws IOException {
+    final int end = start + length;
+    for (int i = start; i < end; i++) {
+      final int code = (int)s[i];
+      if (code >= 0x01 && code <= 0x7F)
+	writeByte((byte)code);
+      else if (((code >= 0x80) && (code <= 0x7FF)) || code == 0) {
+	writeByte((byte)(0xC0 | (code >> 6)));
+	writeByte((byte)(0x80 | (code & 0x3F)));
+      } else {
+	writeByte((byte)(0xE0 | (code >>> 12)));
+	writeByte((byte)(0x80 | ((code >> 6) & 0x3F)));
+	writeByte((byte)(0x80 | (code & 0x3F)));
+      }
+    }
+  }
+
   /** Forces any buffered output to be written. */
   public abstract void flush() throws IOException;
 
Index: src/demo/org/apache/lucene/demo/IndexLineFiles.java
===================================================================
--- src/demo/org/apache/lucene/demo/IndexLineFiles.java	(revision 0)
+++ src/demo/org/apache/lucene/demo/IndexLineFiles.java	(revision 0)
@@ -0,0 +1,193 @@
+package org.apache.lucene.demo;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.analysis.standard.StandardAnalyzer;
+import org.apache.lucene.analysis.SimpleSpaceAnalyzer;
+import org.apache.lucene.analysis.WhitespaceAnalyzer;
+import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.store.FSDirectory;
+import org.apache.lucene.document.DateTools;
+
+import java.io.File;
+import java.io.FileReader;
+import java.io.BufferedReader;
+import java.io.FileNotFoundException;
+import java.io.IOException;
+import java.util.Date;
+
+import java.util.concurrent.atomic.AtomicInteger;
+
+/** Index all text files under a directory. */
+public class IndexLineFiles {
+  
+  private IndexLineFiles() {}
+
+  static final File INDEX_DIR = new File("index");
+
+  static final AtomicInteger allCount = new AtomicInteger();
+  
+  static int bufferSize;
+  static String fileName;
+
+  private static class Indexer extends Thread {
+
+    public void run() {
+      int iter = 0;
+      Document doc = new Document();
+      while (true) {
+
+        try {
+          BufferedReader input = new BufferedReader(new FileReader(fileName));
+          String line = null;
+
+          try {
+            while ((line = input.readLine()) != null) {
+
+              if (doStoredFields && 0 == iter) {
+                // Add the path of the file as a field named "path".  Use a field that is 
+                // indexed (i.e. searchable), but don't tokenize the field into words.
+                doc.add(new Field("path", fileName, Field.Store.YES, Field.Index.NO));
+          
+                // Add the last modified date of the file a field named "modified".  Use 
+                // a field that is indexed (i.e. searchable), but don't tokenize the field
+                // into words.
+                doc.add(new Field("modified",
+                                  "200703161637",
+                                  Field.Store.YES, Field.Index.NO));
+              }
+
+              doc.add(new Field("contents", line, Field.Store.NO, Field.Index.TOKENIZED, tvMode));
+
+              if (++iter == mult) {
+                writer.addDocument(doc);
+                doc.getFields().clear();
+                iter = 0;
+                if (allCount.getAndIncrement() >= (numDoc-1)) {
+                  System.out.println("THREAD DONE");
+                  return;
+                }
+              }
+            }
+          } finally {
+            input.close();
+          }
+        } catch (IOException e) {
+          throw new RuntimeException(e);
+        }
+      }
+    }
+  }
+
+  static Field.TermVector tvMode;
+  static boolean doStoredFields;
+  static int mult;
+  static IndexWriter writer;
+  static int numDoc;
+
+  /** Index all text files under a directory. */
+  public static void main(String[] args) throws IOException {
+    String usage = "java org.apache.lucene.demo.IndexFiles <lineFile> <autoCommit:yes|no> <bufferSizeMB> <docLimit> <maxBufferedDocs> <termVectors:no,pos,posoffs> <storedFields:yes|no} <multiplier> <numThread> <mergeFactor>";
+
+    if (args.length == 0) {
+      System.err.println("Usage: " + usage);
+      System.exit(1);
+    }
+
+    fileName = args[0];
+    boolean autoCommit = args[1].equals("yes");
+    bufferSize = Integer.parseInt(args[2]);
+    numDoc = Integer.parseInt(args[3]);
+    int maxBufferedDocs = Integer.parseInt(args[4]);
+    boolean optimize = args[5].equals("yes");
+    
+    if (args[6].equals("no"))
+      tvMode = Field.TermVector.NO;
+    else if (args[6].equals("yes"))
+      tvMode = Field.TermVector.YES;
+    else if (args[6].equals("pos"))
+      tvMode = Field.TermVector.WITH_POSITIONS;
+    else if (args[6].equals("posoffs"))
+      tvMode = Field.TermVector.WITH_POSITIONS_OFFSETS;
+    else
+      throw new RuntimeException("bad term vector mode: " + args[6]);
+
+    doStoredFields = args[7].equals("yes");
+    mult = Integer.parseInt(args[8]);
+    int numThread = Integer.parseInt(args[9]);
+    int mergeFactor = Integer.parseInt(args[10]);
+
+    System.out.println("\nFAST BLOCK: autoCommit=" + autoCommit + " bufferSize=" + bufferSize + "MB docLimit=" + numDoc + " optimize=" + optimize + " termVectors=" + args[6] + " storedFields=" + doStoredFields + " multiplier=" + mult + " numThread=" + numThread + " mergeFactor=" + mergeFactor);
+    System.out.println("  NO MERGING");
+
+    if (INDEX_DIR.exists()) {
+      System.out.println("Cannot save index to '" +INDEX_DIR+ "' directory, please delete it first");
+      System.exit(1);
+    }
+    
+    Date start = new Date();
+    try {
+      // IndexWriter writer = new IndexWriter(INDEX_DIR, new StandardAnalyzer(), true);
+      writer = new IndexWriter(FSDirectory.getDirectory(INDEX_DIR), autoCommit, new SimpleSpaceAnalyzer(), true);
+      //writer = new IndexWriter(FSDirectory.getDirectory(INDEX_DIR), autoCommit, new WhitespaceAnalyzer(), true);
+      writer.setMaxBufferedDocs(maxBufferedDocs);
+      writer.setMaxFieldLength(100000000);
+      writer.setRAMBufferSizeMB(bufferSize);
+      writer.setUseCompoundFile(false);
+      writer.setInfoStream(System.out);
+      writer.setMergeFactor(mergeFactor);
+      // writer.setMaxFieldLength(10000000);
+      //writer.setMaxFieldLength(1000);
+
+      Indexer[] indexers = new Indexer[numThread];
+      for(int i=0;i<numThread-1;i++) {
+        indexers[i] = new Indexer();
+        indexers[i].start();
+      }
+      indexers[numThread-1] = new Indexer();
+      indexers[numThread-1].run();
+      System.out.println("done run primary");
+
+      for(int i=0;i<numThread-1;i++) {
+        try {
+          indexers[i].join();
+        } catch (InterruptedException e) {
+          throw new RuntimeException(e);
+        }
+        System.out.println("done join " + i);
+      }
+      if (optimize) {
+        System.out.println("Optimize...");
+        writer.optimize();
+      }
+      writer.close();
+
+      Date end = new Date();
+      System.out.println(allCount.get() + " docs; " + (end.getTime() - start.getTime()) + " total milliseconds");
+      System.out.println("FINISHED");
+
+    } catch (IOException e) {
+      e.printStackTrace(System.out);
+      System.out.println(" caught a " + e.getClass() +
+       "\n with message: " + e.getMessage());
+    }
+  }
+}

Property changes on: src/demo/org/apache/lucene/demo/IndexLineFiles.java
___________________________________________________________________
Name: svn:eol-style
   + native

