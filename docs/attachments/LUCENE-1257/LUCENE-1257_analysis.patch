Index: src/java/org/apache/lucene/analysis/Analyzer.java
===================================================================
--- src/java/org/apache/lucene/analysis/Analyzer.java	(revision 826550)
+++ src/java/org/apache/lucene/analysis/Analyzer.java	(working copy)
@@ -52,7 +52,7 @@
     return tokenStream(fieldName, reader);
   }
 
-  private CloseableThreadLocal tokenStreams = new CloseableThreadLocal();
+  private CloseableThreadLocal<Object> tokenStreams = new CloseableThreadLocal<Object>();
 
   /** Used by Analyzers that implement reusableTokenStream
    *  to retrieve previously saved TokenStreams for re-use
Index: src/java/org/apache/lucene/analysis/BaseCharFilter.java
===================================================================
--- src/java/org/apache/lucene/analysis/BaseCharFilter.java	(revision 826550)
+++ src/java/org/apache/lucene/analysis/BaseCharFilter.java	(working copy)
@@ -33,8 +33,7 @@
  */
 public abstract class BaseCharFilter extends CharFilter {
 
-  //private List<OffCorrectMap> pcmList;
-  private List pcmList;
+  private List<OffCorrectMap> pcmList;
   
   public BaseCharFilter(CharStream in) {
     super(in);
@@ -64,7 +63,7 @@
 
   protected void addOffCorrectMap(int off, int cumulativeDiff) {
     if (pcmList == null) {
-      pcmList = new ArrayList();
+      pcmList = new ArrayList<OffCorrectMap>();
     }
     pcmList.add(new OffCorrectMap(off, cumulativeDiff));
   }
Index: src/java/org/apache/lucene/analysis/CachingTokenFilter.java
===================================================================
--- src/java/org/apache/lucene/analysis/CachingTokenFilter.java	(revision 826550)
+++ src/java/org/apache/lucene/analysis/CachingTokenFilter.java	(working copy)
@@ -34,8 +34,8 @@
  * stream to the first Token. 
  */
 public final class CachingTokenFilter extends TokenFilter {
-  private List cache = null;
-  private Iterator iterator = null; 
+  private List<AttributeSource.State> cache = null;
+  private Iterator<AttributeSource.State> iterator = null; 
   private AttributeSource.State finalState;
   
   public CachingTokenFilter(TokenStream input) {
@@ -45,7 +45,7 @@
   public final boolean incrementToken() throws IOException {
     if (cache == null) {
       // fill cache lazily
-      cache = new LinkedList();
+      cache = new LinkedList<AttributeSource.State>();
       fillCache();
       iterator = cache.iterator();
     }
@@ -55,7 +55,7 @@
       return false;
     }
     // Since the TokenFilter can be reset, the tokens need to be preserved as immutable.
-    restoreState((AttributeSource.State) iterator.next());
+    restoreState(iterator.next());
     return true;
   }
   
Index: src/java/org/apache/lucene/analysis/PerFieldAnalyzerWrapper.java
===================================================================
--- src/java/org/apache/lucene/analysis/PerFieldAnalyzerWrapper.java	(revision 826550)
+++ src/java/org/apache/lucene/analysis/PerFieldAnalyzerWrapper.java	(working copy)
@@ -44,7 +44,7 @@
  */
 public class PerFieldAnalyzerWrapper extends Analyzer {
   private Analyzer defaultAnalyzer;
-  private Map analyzerMap = new HashMap();
+  private Map<String,Analyzer> analyzerMap = new HashMap<String,Analyzer>();
 
 
   /**
@@ -67,7 +67,7 @@
    * used for those fields 
    */
   public PerFieldAnalyzerWrapper(Analyzer defaultAnalyzer, 
-      Map /*<String, Analyzer>*/ fieldAnalyzers) {
+      Map<String,Analyzer> fieldAnalyzers) {
     this.defaultAnalyzer = defaultAnalyzer;
     if (fieldAnalyzers != null) {
       analyzerMap.putAll(fieldAnalyzers);
@@ -87,7 +87,7 @@
   }
 
   public TokenStream tokenStream(String fieldName, Reader reader) {
-    Analyzer analyzer = (Analyzer) analyzerMap.get(fieldName);
+    Analyzer analyzer = analyzerMap.get(fieldName);
     if (analyzer == null) {
       analyzer = defaultAnalyzer;
     }
@@ -102,7 +102,7 @@
       // tokenStream but not reusableTokenStream
       return tokenStream(fieldName, reader);
     }
-    Analyzer analyzer = (Analyzer) analyzerMap.get(fieldName);
+    Analyzer analyzer = analyzerMap.get(fieldName);
     if (analyzer == null)
       analyzer = defaultAnalyzer;
 
@@ -111,7 +111,7 @@
   
   /** Return the positionIncrementGap from the analyzer assigned to fieldName */
   public int getPositionIncrementGap(String fieldName) {
-    Analyzer analyzer = (Analyzer) analyzerMap.get(fieldName);
+    Analyzer analyzer = analyzerMap.get(fieldName);
     if (analyzer == null)
       analyzer = defaultAnalyzer;
     return analyzer.getPositionIncrementGap(fieldName);
Index: src/java/org/apache/lucene/analysis/TeeSinkTokenFilter.java
===================================================================
--- src/java/org/apache/lucene/analysis/TeeSinkTokenFilter.java	(revision 826550)
+++ src/java/org/apache/lucene/analysis/TeeSinkTokenFilter.java	(working copy)
@@ -73,7 +73,7 @@
  * <p>Note, the EntityDetect and URLDetect TokenStreams are for the example and do not currently exist in Lucene.
  */
 public final class TeeSinkTokenFilter extends TokenFilter {
-  private final List sinks = new LinkedList();
+  private final List<WeakReference<SinkTokenStream>> sinks = new LinkedList<WeakReference<SinkTokenStream>>();
   
   /**
    * Instantiates a new TeeSinkTokenFilter.
@@ -96,7 +96,7 @@
    */
   public SinkTokenStream newSinkTokenStream(SinkFilter filter) {
     SinkTokenStream sink = new SinkTokenStream(this.cloneAttributes(), filter);
-    this.sinks.add(new WeakReference(sink));
+    this.sinks.add(new WeakReference<SinkTokenStream>(sink));
     return sink;
   }
   
@@ -111,10 +111,10 @@
       throw new IllegalArgumentException("The supplied sink is not compatible to this tee");
     }
     // add eventually missing attribute impls to the existing sink
-    for (Iterator it = this.cloneAttributes().getAttributeImplsIterator(); it.hasNext(); ) {
-      sink.addAttributeImpl((AttributeImpl) it.next());
+    for (Iterator<AttributeImpl> it = this.cloneAttributes().getAttributeImplsIterator(); it.hasNext(); ) {
+      sink.addAttributeImpl(it.next());
     }
-    this.sinks.add(new WeakReference(sink));
+    this.sinks.add(new WeakReference<SinkTokenStream>(sink));
   }
   
   /**
@@ -131,8 +131,8 @@
     if (input.incrementToken()) {
       // capture state lazily - maybe no SinkFilter accepts this state
       AttributeSource.State state = null;
-      for (Iterator it = sinks.iterator(); it.hasNext(); ) {
-        final SinkTokenStream sink = (SinkTokenStream) ((WeakReference) it.next()).get();
+      for (WeakReference<SinkTokenStream> ref : sinks) {
+        final SinkTokenStream sink = ref.get();
         if (sink != null) {
           if (sink.accept(this)) {
             if (state == null) {
@@ -151,8 +151,8 @@
   public final void end() throws IOException {
     super.end();
     AttributeSource.State finalState = captureState();
-    for (Iterator it = sinks.iterator(); it.hasNext(); ) {
-      final SinkTokenStream sink = (SinkTokenStream) ((WeakReference) it.next()).get();
+    for (WeakReference<SinkTokenStream> ref : sinks) {
+      final SinkTokenStream sink = ref.get();
       if (sink != null) {
         sink.setFinalState(finalState);
       }
@@ -179,9 +179,9 @@
   }
   
   public static final class SinkTokenStream extends TokenStream {
-    private final List cachedStates = new LinkedList();
+    private final List<AttributeSource.State> cachedStates = new LinkedList<AttributeSource.State>();
     private AttributeSource.State finalState;
-    private Iterator it = null;
+    private Iterator<AttributeSource.State> it = null;
     private SinkFilter filter;
     
     private SinkTokenStream(AttributeSource source, SinkFilter filter) {
@@ -214,7 +214,7 @@
         return false;
       }
       
-      AttributeSource.State state = (State) it.next();
+      AttributeSource.State state = it.next();
       restoreState(state);
       return true;
     }
