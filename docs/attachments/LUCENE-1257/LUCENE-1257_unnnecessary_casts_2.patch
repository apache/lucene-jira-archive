Index: src/java/org/apache/lucene/analysis/standard/StandardFilter.java
===================================================================
--- src/java/org/apache/lucene/analysis/standard/StandardFilter.java	(revision 828836)
+++ src/java/org/apache/lucene/analysis/standard/StandardFilter.java	(working copy)
@@ -17,7 +17,6 @@
  * limitations under the License.
  */
 
-import org.apache.lucene.analysis.Token;
 import org.apache.lucene.analysis.TokenFilter;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.tokenattributes.TermAttribute;
Index: src/java/org/apache/lucene/index/FieldInfos.java
===================================================================
--- src/java/org/apache/lucene/index/FieldInfos.java	(revision 828836)
+++ src/java/org/apache/lucene/index/FieldInfos.java	(working copy)
@@ -102,7 +102,7 @@
     FieldInfos fis = new FieldInfos();
     final int numField = byNumber.size();
     for(int i=0;i<numField;i++) {
-      FieldInfo fi = (FieldInfo) ((FieldInfo) byNumber.get(i)).clone();
+      FieldInfo fi = (FieldInfo) ( byNumber.get(i)).clone();
       fis.byNumber.add(fi);
       fis.byName.put(fi.name, fi);
     }
@@ -259,7 +259,7 @@
   }
 
   public FieldInfo fieldInfo(String fieldName) {
-    return (FieldInfo) byName.get(fieldName);
+    return  byName.get(fieldName);
   }
 
   /**
Index: src/java/org/apache/lucene/index/SegmentInfo.java
===================================================================
--- src/java/org/apache/lucene/index/SegmentInfo.java	(revision 828836)
+++ src/java/org/apache/lucene/index/SegmentInfo.java	(working copy)
@@ -264,7 +264,7 @@
       final int size = files.size();
       sizeInBytes = 0;
       for(int i=0;i<size;i++) {
-        final String fileName = (String) files.get(i);
+        final String fileName = files.get(i);
         // We don't count bytes used by a shared doc store
         // against this segment:
         if (docStoreOffset == -1 || !IndexFileNames.isDocStoreFile(fileName))
Index: src/java/org/apache/lucene/index/ParallelReader.java
===================================================================
--- src/java/org/apache/lucene/index/ParallelReader.java	(revision 828836)
+++ src/java/org/apache/lucene/index/ParallelReader.java	(working copy)
@@ -195,8 +195,8 @@
       List<Boolean> newDecrefOnClose = new ArrayList<Boolean>();
       ParallelReader pr = new ParallelReader();
       for (int i = 0; i < readers.size(); i++) {
-        IndexReader oldReader = (IndexReader) readers.get(i);
-        IndexReader newReader = (IndexReader) newReaders.get(i);
+        IndexReader oldReader = readers.get(i);
+        IndexReader newReader = newReaders.get(i);
         if (newReader == oldReader) {
           newDecrefOnClose.add(Boolean.TRUE);
           newReader.incRef();
@@ -293,8 +293,7 @@
       if (vector != null)
         results.add(vector);
     }
-    return (TermFreqVector[])
-      results.toArray(new TermFreqVector[results.size()]);
+    return results.toArray(new TermFreqVector[results.size()]);
   }
 
   public TermFreqVector getTermFreqVector(int n, String field)
Index: src/java/org/apache/lucene/index/FormatPostingsPositionsWriter.java
===================================================================
--- src/java/org/apache/lucene/index/FormatPostingsPositionsWriter.java	(revision 828836)
+++ src/java/org/apache/lucene/index/FormatPostingsPositionsWriter.java	(working copy)
@@ -18,8 +18,8 @@
  */
 
 import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.store.IndexInput;
 
+
 import java.io.IOException;
 
 final class FormatPostingsPositionsWriter extends FormatPostingsPositionsConsumer {
Index: src/java/org/apache/lucene/index/CompoundFileReader.java
===================================================================
--- src/java/org/apache/lucene/index/CompoundFileReader.java	(revision 828836)
+++ src/java/org/apache/lucene/index/CompoundFileReader.java	(working copy)
@@ -129,7 +129,7 @@
         if (stream == null)
             throw new IOException("Stream closed");
 
-        FileEntry entry = (FileEntry) entries.get(id);
+        FileEntry entry = entries.get(id);
         if (entry == null)
             throw new IOException("No sub-file with id " + id + " found");
 
@@ -139,7 +139,7 @@
     /** Returns an array of strings, one for each file in the directory. */
     public String[] listAll() {
         String res[] = new String[entries.size()];
-        return (String[]) entries.keySet().toArray(res);
+        return entries.keySet().toArray(res);
     }
 
     /** Returns true iff a file with the given name exists. */
@@ -176,7 +176,7 @@
     public long fileLength(String name)
     throws IOException
     {
-        FileEntry e = (FileEntry) entries.get(name);
+        FileEntry e = entries.get(name);
         if (e == null)
             throw new IOException("File " + name + " does not exist");
         return e.length;
Index: src/java/org/apache/lucene/index/DocFieldProcessorPerThread.java
===================================================================
--- src/java/org/apache/lucene/index/DocFieldProcessorPerThread.java	(revision 828836)
+++ src/java/org/apache/lucene/index/DocFieldProcessorPerThread.java	(working copy)
@@ -128,7 +128,7 @@
   }
 
   private void rehash() {
-    final int newHashSize = (int) (fieldHash.length*2);
+    final int newHashSize = (fieldHash.length*2);
     assert newHashSize > fieldHash.length;
 
     final DocFieldProcessorPerField newHashArray[] = new DocFieldProcessorPerField[newHashSize];
@@ -172,7 +172,7 @@
     // vectors, etc.):
 
     for(int i=0;i<numDocFields;i++) {
-      Fieldable field = (Fieldable) docFields.get(i);
+      Fieldable field = docFields.get(i);
       final String fieldName = field.name();
 
       // Make sure we have a PerField allocated
Index: src/java/org/apache/lucene/index/MergePolicy.java
===================================================================
--- src/java/org/apache/lucene/index/MergePolicy.java	(revision 828836)
+++ src/java/org/apache/lucene/index/MergePolicy.java	(working copy)
@@ -163,7 +163,7 @@
       b.append("MergeSpec:\n");
       final int count = merges.size();
       for(int i=0;i<count;i++)
-        b.append("  ").append(1 + i).append(": ").append(((OneMerge) merges.get(i)).segString(dir));
+        b.append("  ").append(1 + i).append(": ").append(merges.get(i).segString(dir));
       return b.toString();
     }
   }
Index: src/java/org/apache/lucene/index/ConcurrentMergeScheduler.java
===================================================================
--- src/java/org/apache/lucene/index/ConcurrentMergeScheduler.java	(revision 828836)
+++ src/java/org/apache/lucene/index/ConcurrentMergeScheduler.java	(working copy)
@@ -88,7 +88,7 @@
 
     final int numThreads = mergeThreadCount();
     for(int i=0;i<numThreads;i++) {
-      MergeThread merge = (MergeThread) mergeThreads.get(i);
+      MergeThread merge = mergeThreads.get(i);
       merge.setThreadPriority(pri);
     }
   }
@@ -123,7 +123,7 @@
       final int count = mergeThreads.size();
       if (verbose()) {
         for(int i=0;i<count;i++)
-          message("    " + i + ": " + ((MergeThread) mergeThreads.get(i)));
+          message("    " + i + ": " + mergeThreads.get(i));
       }
       
       try {
@@ -141,7 +141,7 @@
     int count = 0;
     final int numThreads = mergeThreads.size();
     for(int i=0;i<numThreads;i++)
-      if (((MergeThread) mergeThreads.get(i)).isAlive())
+      if (mergeThreads.get(i).isAlive())
         count++;
     return count;
   }
Index: src/java/org/apache/lucene/index/MultiReader.java
===================================================================
--- src/java/org/apache/lucene/index/MultiReader.java	(revision 828836)
+++ src/java/org/apache/lucene/index/MultiReader.java	(working copy)
@@ -67,7 +67,7 @@
   }
   
   private void initialize(IndexReader[] subReaders, boolean closeSubReaders) {
-    this.subReaders = (IndexReader[]) subReaders.clone();
+    this.subReaders =  subReaders.clone();
     starts = new int[subReaders.length + 1];    // build starts array
     decrefOnClose = new boolean[subReaders.length];
     for (int i = 0; i < subReaders.length; i++) {
Index: src/java/org/apache/lucene/index/SortedTermVectorMapper.java
===================================================================
--- src/java/org/apache/lucene/index/SortedTermVectorMapper.java	(revision 828836)
+++ src/java/org/apache/lucene/index/SortedTermVectorMapper.java	(working copy)
@@ -61,7 +61,7 @@
    */
   //We need to combine any previous mentions of the term
   public void map(String term, int frequency, TermVectorOffsetInfo[] offsets, int[] positions) {
-    TermVectorEntry entry = (TermVectorEntry) termToTVE.get(term);
+    TermVectorEntry entry =  termToTVE.get(term);
     if (entry == null) {
       entry = new TermVectorEntry(ALL, term, frequency, 
               storeOffsets == true ? offsets : null,
Index: src/java/org/apache/lucene/index/PositionBasedTermVectorMapper.java
===================================================================
--- src/java/org/apache/lucene/index/PositionBasedTermVectorMapper.java	(revision 828836)
+++ src/java/org/apache/lucene/index/PositionBasedTermVectorMapper.java	(working copy)
@@ -70,7 +70,7 @@
   public void map(String term, int frequency, TermVectorOffsetInfo[] offsets, int[] positions) {
     for (int i = 0; i < positions.length; i++) {
       Integer posVal = Integer.valueOf(positions[i]);
-      TVPositionInfo pos = (TVPositionInfo) currentPositions.get(posVal);
+      TVPositionInfo pos = currentPositions.get(posVal);
       if (pos == null) {
         pos = new TVPositionInfo(positions[i], storeOffsets);
         currentPositions.put(posVal, pos);
Index: src/java/org/apache/lucene/index/DirectoryReader.java
===================================================================
--- src/java/org/apache/lucene/index/DirectoryReader.java	(revision 828836)
+++ src/java/org/apache/lucene/index/DirectoryReader.java	(working copy)
@@ -207,7 +207,7 @@
     
     for (int i = infos.size() - 1; i>=0; i--) {
       // find SegmentReader for this segment
-      Integer oldReaderIndex = (Integer) segmentReaders.get(infos.info(i).name);
+      Integer oldReaderIndex = segmentReaders.get(infos.info(i).name);
       if (oldReaderIndex == null) {
         // this is a new segment, no old SegmentReader can be reused
         newReaders[i] = null;
@@ -268,17 +268,17 @@
     // try to copy unchanged norms from the old normsCache to the new one
     if (oldNormsCache != null) {
       for (Map.Entry<String,byte[]> entry: oldNormsCache.entrySet()) {
-        String field = (String) entry.getKey();
+        String field = entry.getKey();
         if (!hasNorms(field)) {
           continue;
         }
 
-        byte[] oldBytes = (byte[]) entry.getValue();
+        byte[] oldBytes = entry.getValue();
 
         byte[] bytes = new byte[maxDoc()];
 
         for (int i = 0; i < subReaders.length; i++) {
-          Integer oldReaderIndex = ((Integer) segmentReaders.get(subReaders[i].getSegmentName()));
+          Integer oldReaderIndex = segmentReaders.get(subReaders[i].getSegmentName());
 
           // this SegmentReader was not re-opened, we can copy all of its norms 
           if (oldReaderIndex != null &&
@@ -394,14 +394,14 @@
         assert isCurrent();
 
         if (openReadOnly) {
-          return (IndexReader) clone(openReadOnly);
+          return clone(openReadOnly);
         } else {
           return this;
         }
       } else if (isCurrent()) {
         if (openReadOnly != readOnly) {
           // Just fallback to clone
-          return (IndexReader) clone(openReadOnly);
+          return clone(openReadOnly);
         } else {
           return this;
         }
@@ -412,7 +412,7 @@
       if (segmentInfos != null && commit.getSegmentsFileName().equals(segmentInfos.getCurrentSegmentFileName())) {
         if (readOnly != openReadOnly) {
           // Just fallback to clone
-          return (IndexReader) clone(openReadOnly);
+          return clone(openReadOnly);
         } else {
           return this;
         }
@@ -563,7 +563,7 @@
 
   public synchronized byte[] norms(String field) throws IOException {
     ensureOpen();
-    byte[] bytes = (byte[])normsCache.get(field);
+    byte[] bytes = normsCache.get(field);
     if (bytes != null)
       return bytes;          // cache hit
     if (!hasNorms(field))
@@ -579,7 +579,7 @@
   public synchronized void norms(String field, byte[] result, int offset)
     throws IOException {
     ensureOpen();
-    byte[] bytes = (byte[])normsCache.get(field);
+    byte[] bytes = normsCache.get(field);
     if (bytes==null && !hasNorms(field)) {
       Arrays.fill(result, offset, result.length, DefaultSimilarity.encodeNorm(1.0f));
     } else if (bytes != null) {                           // cache hit
@@ -977,7 +977,7 @@
       int numMatchingSegments = 0;
       matchingSegments[0] = null;
 
-      SegmentMergeInfo top = (SegmentMergeInfo)queue.top();
+      SegmentMergeInfo top = queue.top();
 
       if (top == null) {
         term = null;
@@ -991,7 +991,7 @@
         matchingSegments[numMatchingSegments++] = top;
         queue.pop();
         docFreq += top.termEnum.docFreq();    // increment freq
-        top = (SegmentMergeInfo)queue.top();
+        top = queue.top();
       }
 
       matchingSegments[numMatchingSegments] = null;
@@ -1168,7 +1168,7 @@
     }
   
     protected TermDocs termDocs(IndexReader reader) throws IOException {
-      return (TermDocs)reader.termPositions();
+      return reader.termPositions();
     }
   
     public int nextPosition() throws IOException {
Index: src/java/org/apache/lucene/index/BufferedDeletes.java
===================================================================
--- src/java/org/apache/lucene/index/BufferedDeletes.java	(revision 828836)
+++ src/java/org/apache/lucene/index/BufferedDeletes.java	(working copy)
@@ -136,7 +136,7 @@
     if (queries.size() > 0) {
       newDeleteQueries = new HashMap<Query, Integer>(queries.size());
       for(Entry<Query,Integer> entry: queries.entrySet()) {
-        Integer num = (Integer) entry.getValue();
+        Integer num = entry.getValue();
         newDeleteQueries.put(entry.getKey(),
                              Integer.valueOf(mapper.remap(num.intValue())));
       }
Index: src/java/org/apache/lucene/index/MultipleTermPositions.java
===================================================================
--- src/java/org/apache/lucene/index/MultipleTermPositions.java	(revision 828836)
+++ src/java/org/apache/lucene/index/MultipleTermPositions.java	(working copy)
@@ -143,7 +143,7 @@
 
   public final boolean skipTo(int target) throws IOException {
     while (_termPositionsQueue.peek() != null && target > _termPositionsQueue.peek().doc()) {
-      TermPositions tp = (TermPositions) _termPositionsQueue.pop();
+      TermPositions tp =  _termPositionsQueue.pop();
       if (tp.skipTo(target))
         _termPositionsQueue.add(tp);
       else
@@ -162,7 +162,7 @@
 
   public final void close() throws IOException {
     while (_termPositionsQueue.size() > 0)
-      ((TermPositions) _termPositionsQueue.pop()).close();
+      _termPositionsQueue.pop().close();
   }
 
   /**
Index: src/java/org/apache/lucene/index/SegmentMerger.java
===================================================================
--- src/java/org/apache/lucene/index/SegmentMerger.java	(revision 828836)
+++ src/java/org/apache/lucene/index/SegmentMerger.java	(working copy)
@@ -115,7 +115,7 @@
    * @return The ith reader to be merged
    */
   final IndexReader segmentReader(int i) {
-    return (IndexReader) readers.get(i);
+    return readers.get(i);
   }
 
   /**
@@ -244,7 +244,7 @@
     // FieldInfos, then we can do a bulk copy of the
     // stored fields:
     for (int i = 0; i < numReaders; i++) {
-      IndexReader reader = (IndexReader) readers.get(i);
+      IndexReader reader = readers.get(i);
       if (reader instanceof SegmentReader) {
         SegmentReader segmentReader = (SegmentReader) reader;
         boolean same = true;
@@ -576,7 +576,7 @@
     int base = 0;
     final int readerCount = readers.size();
     for (int i = 0; i < readerCount; i++) {
-      IndexReader reader = (IndexReader) readers.get(i);
+      IndexReader reader = readers.get(i);
       TermEnum termEnum = reader.terms();
       SegmentMergeInfo smi = new SegmentMergeInfo(base, termEnum, reader);
       int[] docMap  = smi.getDocMap();
@@ -606,13 +606,13 @@
 
     while (queue.size() > 0) {
       int matchSize = 0;			  // pop matching terms
-      match[matchSize++] = (SegmentMergeInfo) queue.pop();
+      match[matchSize++] = queue.pop();
       Term term = match[0].term;
-      SegmentMergeInfo top = (SegmentMergeInfo) queue.top();
+      SegmentMergeInfo top = queue.top();
 
       while (top != null && term.compareTo(top.term) == 0) {
-        match[matchSize++] = (SegmentMergeInfo) queue.pop();
-        top = (SegmentMergeInfo) queue.top();
+        match[matchSize++] =  queue.pop();
+        top =  queue.top();
       }
 
       if (currentField != term.field) {
Index: src/java/org/apache/lucene/index/SegmentMergeQueue.java
===================================================================
--- src/java/org/apache/lucene/index/SegmentMergeQueue.java	(revision 828836)
+++ src/java/org/apache/lucene/index/SegmentMergeQueue.java	(working copy)
@@ -35,7 +35,7 @@
 
   final void close() throws IOException {
     while (top() != null)
-      ((SegmentMergeInfo)pop()).close();
+      pop().close();
   }
 
 }
Index: src/java/org/apache/lucene/index/DocumentsWriter.java
===================================================================
--- src/java/org/apache/lucene/index/DocumentsWriter.java	(revision 828836)
+++ src/java/org/apache/lucene/index/DocumentsWriter.java	(working copy)
@@ -666,7 +666,7 @@
     // First, find a thread state.  If this thread already
     // has affinity to a specific ThreadState, use that one
     // again.
-    DocumentsWriterThreadState state = (DocumentsWriterThreadState) threadBindings.get(Thread.currentThread());
+    DocumentsWriterThreadState state = threadBindings.get(Thread.currentThread());
     if (state == null) {
 
       // First time this thread has called us since last
Index: src/java/org/apache/lucene/index/FormatPostingsPositionsConsumer.java
===================================================================
--- src/java/org/apache/lucene/index/FormatPostingsPositionsConsumer.java	(revision 828836)
+++ src/java/org/apache/lucene/index/FormatPostingsPositionsConsumer.java	(working copy)
@@ -19,7 +19,6 @@
 
 import java.io.IOException;
 
-import org.apache.lucene.store.IndexInput;
 
 abstract class FormatPostingsPositionsConsumer {
 
Index: src/java/org/apache/lucene/index/IndexWriter.java
===================================================================
--- src/java/org/apache/lucene/index/IndexWriter.java	(revision 828836)
+++ src/java/org/apache/lucene/index/IndexWriter.java	(working copy)
@@ -590,7 +590,7 @@
         readBufferSize = BufferedIndexInput.BUFFER_SIZE;
       }
 
-      SegmentReader sr = (SegmentReader) readerMap.get(info);
+      SegmentReader sr = readerMap.get(info);
       if (sr == null) {
         // TODO: we may want to avoid doing this while
         // synchronized
@@ -619,7 +619,7 @@
 
     // Returns a ref
     public synchronized SegmentReader getIfExists(SegmentInfo info) throws IOException {
-      SegmentReader sr = (SegmentReader) readerMap.get(info);
+      SegmentReader sr = readerMap.get(info);
       if (sr != null) {
         sr.incRef();
       }
@@ -2364,7 +2364,7 @@
       if (spec != null) {
         final int numMerges = spec.merges.size();
         for(int i=0;i<numMerges;i++)
-          registerMerge((MergePolicy.OneMerge) spec.merges.get(i));
+          registerMerge(spec.merges.get(i));
       }
     }
 
@@ -2385,7 +2385,7 @@
           // if any of them have hit an exception.
           running = false;
           for(int i=0;i<numMerges;i++) {
-            final MergePolicy.OneMerge merge = (MergePolicy.OneMerge) spec.merges.get(i);
+            final MergePolicy.OneMerge merge = spec.merges.get(i);
             if (pendingMerges.contains(merge) || runningMerges.contains(merge))
               running = true;
             Throwable t = merge.getException();
@@ -2480,7 +2480,7 @@
       if (spec != null) {
         final int numMerges = spec.merges.size();
         for(int i=0;i<numMerges;i++) {
-          final MergePolicy.OneMerge merge = ((MergePolicy.OneMerge) spec.merges.get(i));
+          final MergePolicy.OneMerge merge = ( spec.merges.get(i));
           merge.optimize = true;
           merge.maxNumSegmentsOptimize = maxNumSegmentsOptimize;
         }
@@ -2492,7 +2492,7 @@
     if (spec != null) {
       final int numMerges = spec.merges.size();
       for(int i=0;i<numMerges;i++)
-        registerMerge((MergePolicy.OneMerge) spec.merges.get(i));
+        registerMerge(spec.merges.get(i));
     }
   }
 
@@ -2504,7 +2504,7 @@
       return null;
     else {
       // Advance the merge from pending to running
-      MergePolicy.OneMerge merge = (MergePolicy.OneMerge) pendingMerges.removeFirst();
+      MergePolicy.OneMerge merge = pendingMerges.removeFirst();
       runningMerges.add(merge);
       return merge;
     }
@@ -2518,7 +2518,7 @@
     else {
       Iterator<MergePolicy.OneMerge> it = pendingMerges.iterator();
       while(it.hasNext()) {
-        MergePolicy.OneMerge merge = (MergePolicy.OneMerge) it.next();
+        MergePolicy.OneMerge merge = it.next();
         if (merge.isExternal) {
           // Advance the merge from pending to running
           it.remove();
Index: src/java/org/apache/lucene/index/IndexFileDeleter.java
===================================================================
--- src/java/org/apache/lucene/index/IndexFileDeleter.java	(revision 828836)
+++ src/java/org/apache/lucene/index/IndexFileDeleter.java	(working copy)
@@ -254,7 +254,7 @@
       // First decref all files that had been referred to by
       // the now-deleted commits:
       for(int i=0;i<size;i++) {
-        CommitPoint commit = (CommitPoint) commitsToDelete.get(i);
+        CommitPoint commit = commitsToDelete.get(i);
         if (infoStream != null) {
           message("deleteCommits: now decRef commit \"" + commit.getSegmentsFileName() + "\"");
         }
@@ -269,7 +269,7 @@
       int readFrom = 0;
       int writeTo = 0;
       while(readFrom < size) {
-        CommitPoint commit = (CommitPoint) commits.get(readFrom);
+        CommitPoint commit = commits.get(readFrom);
         if (!commit.deleted) {
           if (writeTo != readFrom) {
             commits.set(writeTo, commits.get(readFrom));
