Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestBugInSomething.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestBugInSomething.java	(revision 1579142)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestBugInSomething.java	(working copy)
@@ -262,6 +262,67 @@
     checkRandomData(random(), analyzer, 2000);
   }
   
+  /* nocommit: failing seeds from TestRandomChains
+  
+   [junit4] Suite: org.apache.lucene.analysis.core.TestRandomChains
+   [junit4]   2> TEST FAIL: useCharFilter=true text='</Br</script \u2979\u2974'
+   [junit4]   2> Exception from random analyzer: 
+   [junit4]   2> charfilters=
+   [junit4]   2>   org.apache.lucene.analysis.fa.PersianCharFilter(java.io.StringReader@343def7c)
+   [junit4]   2>   org.apache.lucene.analysis.MockCharFilter(org.apache.lucene.analysis.fa.PersianCharFilter@4e35d0af)
+   [junit4]   2> tokenizer=
+   [junit4]   2>   org.apache.lucene.analysis.path.PathHierarchyTokenizer()
+   [junit4]   2> filters=
+   [junit4]   2>   org.apache.lucene.analysis.cjk.CJKWidthFilter(ValidatingTokenFilter@2f17b946 term=,bytes=[],startOffset=0,endOffset=0,positionIncrement=1,positionLength=1)
+   [junit4]   2>   org.apache.lucene.analysis.miscellaneous.WordDelimiterFilter(LUCENE_50, ValidatingTokenFilter@3e84bad1 term=,bytes=[],startOffset=0,endOffset=0,positionIncrement=1,positionLength=1,type=word, -19, [vhc, sinj, nztsl, rkwj, hjiqwl])
+   [junit4]   2>   org.apache.lucene.analysis.fr.FrenchMinimalStemFilter(ValidatingTokenFilter@7c050f67 term=,bytes=[],startOffset=0,endOffset=0,positionIncrement=1,positionLength=1,type=word,keyword=false)
+   [junit4]   2>   org.apache.lucene.analysis.pattern.PatternCaptureGroupTokenFilter(ValidatingTokenFilter@303672cd term=,bytes=[],startOffset=0,endOffset=0,positionIncrement=1,positionLength=1,type=word,keyword=false, false, [Ljava.util.regex.Pattern;@46ab0975)
+   [junit4]   2> offsetsAreCorrect=false
+   [junit4]   2> NOTE: reproduce with: ant test  -Dtestcase=TestRandomChains -Dtests.method=testRandomChains -Dtests.seed=7B2834E90023CE3F -Dtests.locale=en -Dtests.timezone=Pacific/Majuro -Dtests.file.encoding=UTF-8
+   [junit4] FAILURE 2.18s | TestRandomChains.testRandomChains <<<
+
+   [junit4]   2> TEST FAIL: useCharFilter=false text='ickike ha&#\n'\\'</  M jlcr oobtexkc \ud83c\udd31\ud83c\uddc5\ud83c\udd4b\ud83c\uddfe\ud83c\udd42 q\u429bx\u18bb\u6376 \uf6c2=/ \ud969\udf82\u0762\ud807\ude05m\ua56c\ue156\u0745 hdlbaex \uf6a6\u017d\ud926\ude34\uf4b3\ua105 &#x53e cdbqr  . \u0006 \u2c6b\u2c6d\u2c75\u2c63 \ue0ddC\ud9db\udfbb\ue2fc\u0004\uf95d\uf24c\u4191 \ufc2e\ufcb7\ufbcf\ufd27\ufd5c bcxz \u9408\u8bd1\u8b49\u8083\u50bd\u8e79 e \u0006\uf4a1e \ud7b9\ud7cd\ud7e1\ud7fe\ud7d1 \u02cb\uf1e9\u3fbb1\u5e84z qvzux'
+   [junit4]   2> Exception from random analyzer: 
+   [junit4]   2> charfilters=
+   [junit4]   2>   org.apache.lucene.analysis.MockCharFilter(java.io.StringReader@3cbc8dde)
+   [junit4]   2> tokenizer=
+   [junit4]   2>   org.apache.lucene.analysis.core.WhitespaceTokenizer(LUCENE_50, org.apache.lucene.util.AttributeSource$AttributeFactory$DefaultAttributeFactory@41c65839)
+   [junit4]   2> filters=
+   [junit4]   2>   org.apache.lucene.analysis.miscellaneous.WordDelimiterFilter(LUCENE_50, ValidatingTokenFilter@7f0608f8 term=,bytes=[],startOffset=0,endOffset=0,positionIncrement=1,type=word, -12, [eakwlx])
+   [junit4]   2>   org.apache.lucene.analysis.miscellaneous.WordDelimiterFilter(LUCENE_50, ValidatingTokenFilter@653712ee term=,bytes=[],startOffset=0,endOffset=0,positionIncrement=1,type=word, -35, [gaqhxdmjir, adapcq, trdsqbh, tqqdg])
+   [junit4]   2>   org.apache.lucene.analysis.de.GermanLightStemFilter(ValidatingTokenFilter@2bbfb2c term=,bytes=[],startOffset=0,endOffset=0,positionIncrement=1,type=word,keyword=false)
+   [junit4]   2> offsetsAreCorrect=false
+   [junit4]   2> NOTE: reproduce with: ant test  -Dtestcase=TestRandomChains -Dtests.method=testRandomChainsWithLargeStrings -Dtests.seed=475928F2B7280FFC -Dtests.locale=ro -Dtests.timezone=IST -Dtests.file.encoding=UTF-8
+   [junit4] FAILURE 2.08s | TestRandomChains.testRandomChainsWithLargeStrings <<<
+
+   [junit4] OK      4.02s | TestRandomChains.testRandomChainsWithLargeStrings
+   [junit4]   2> TEST FAIL: useCharFilter=false text='efgugj </p></'
+   [junit4]   2> Exception from random analyzer: 
+   [junit4]   2> charfilters=
+   [junit4]   2> tokenizer=
+   [junit4]   2>   org.apache.lucene.analysis.path.PathHierarchyTokenizer()
+   [junit4]   2> filters=
+   [junit4]   2>   org.apache.lucene.analysis.miscellaneous.WordDelimiterFilter(LUCENE_50, ValidatingTokenFilter@3a5a1476 term=,bytes=[],startOffset=0,endOffset=0,positionIncrement=1,type=word, -39, [pp, wqiirjwuo, qmza, hki, vm, mqsam, hxk, zi])
+   [junit4]   2>   org.apache.lucene.analysis.hu.HungarianLightStemFilter(ValidatingTokenFilter@6ff56ce7 term=,bytes=[],startOffset=0,endOffset=0,positionIncrement=1,type=word,keyword=false)
+   [junit4]   2>   org.apache.lucene.analysis.cjk.CJKBigramFilter(ValidatingTokenFilter@59c0eb1d term=,bytes=[],startOffset=0,endOffset=0,positionIncrement=1,type=word,keyword=false,positionLength=1, -20, false)
+   [junit4]   2> offsetsAreCorrect=false
+   [junit4]   2> NOTE: reproduce with: ant test  -Dtestcase=TestRandomChains -Dtests.method=testRandomChains -Dtests.seed=F00594EF54E53E4D -Dtests.locale=ms -Dtests.timezone=America/Glace_Bay -Dtests.file.encoding=US-ASCII
+
+   [junit4]   2> TEST FAIL: useCharFilter=true text='ybe \u0018\u02d6\ub4d8 nydx) \uf1fc\uf8e6\uf2af\uee9f\uea13 @\u28e7\ue218_s  \uabdc\uabe1\uabc5\uabdc\uabd9 \ube72\uf52eg\u0015\u036a \u16a5\u16ef\u16f2\u16cc \u20d8\u20d5\u20d6\u20f6 dkz ba '
+   [junit4]   2> Exception from random analyzer: 
+   [junit4]   2> charfilters=
+   [junit4]   2>   org.apache.lucene.analysis.charfilter.HTMLStripCharFilter(java.io.StringReader@4bc07ab1, [<HOST>, <KATAKANA>])
+   [junit4]   2> tokenizer=
+   [junit4]   2>   org.apache.lucene.analysis.path.ReversePathHierarchyTokenizer(茐, 銙)
+   [junit4]   2> filters=
+   [junit4]   2>   org.apache.lucene.analysis.miscellaneous.WordDelimiterFilter(LUCENE_50, ValidatingTokenFilter@49b72151 term=,bytes=[],startOffset=0,endOffset=0,positionIncrement=1,keyword=false,type=word, -13, [zoea, prjzfxsm])
+   [junit4]   2>   org.apache.lucene.analysis.miscellaneous.WordDelimiterFilter(LUCENE_50, ValidatingTokenFilter@15ec72a6 term=,bytes=[],startOffset=0,endOffset=0,positionIncrement=1,keyword=false,type=word, 91, [])
+   [junit4]   2>   org.apache.lucene.analysis.in.IndicNormalizationFilter(ValidatingTokenFilter@565afb88 term=,bytes=[],startOffset=0,endOffset=0,positionIncrement=1,keyword=false,type=word)
+   [junit4]   2>   org.apache.lucene.analysis.standard.StandardFilter(LUCENE_50, ValidatingTokenFilter@6f37ca02 term=,bytes=[],startOffset=0,endOffset=0,positionIncrement=1,keyword=false,type=word)
+   [junit4]   2> offsetsAreCorrect=false
+   [junit4]   2> NOTE: reproduce with: ant test  -Dtestcase=TestRandomChains -Dtests.method=testRandomChainsWithLargeStrings -Dtests.seed=D981AB6248EFCEE0 -Dtests.locale=es_MX -Dtests.timezone=Africa/Dakar -Dtests.file.encoding=UTF-8
+*/
+  
   public void testCuriousWikipediaString() throws Exception {
     final CharArraySet protWords = new CharArraySet(TEST_VERSION_CURRENT, new HashSet<>(
         Arrays.asList("rrdpafa", "pupmmlu", "xlq", "dyy", "zqrxrrck", "o", "hsrlfvcha")), false);
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestLucene47WordDelimiterFilter.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestLucene47WordDelimiterFilter.java	(revision 1579142)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestLucene47WordDelimiterFilter.java	(working copy)
@@ -65,19 +65,19 @@
     // the correct offsets.
     TokenFilter wdf = new Lucene47WordDelimiterFilter(new SingleTokenTokenStream(new Token("foo-bar", 5, 12)), DEFAULT_WORD_DELIM_TABLE, flags, null);
 
-    assertTokenStreamContents(wdf, 
+    assertBrokenTokenStreamContents(wdf, 
         new String[] { "foo", "bar", "foobar" },
         new int[] { 5, 9, 5 }, 
         new int[] { 8, 12, 12 },
-        null, null, null, null, false);
+        new int[] { 1, 1, 0 });
 
     wdf = new Lucene47WordDelimiterFilter(new SingleTokenTokenStream(new Token("foo-bar", 5, 6)), DEFAULT_WORD_DELIM_TABLE, flags, null);
     
-    assertTokenStreamContents(wdf,
+    assertBrokenTokenStreamContents(wdf,
         new String[] { "foo", "bar", "foobar" },
         new int[] { 5, 5, 5 },
         new int[] { 6, 6, 6 },
-        null, null, null, null, false);
+        new int[] { 1, 1, 0 });
   }
   
   @Test
@@ -85,10 +85,11 @@
     int flags = GENERATE_WORD_PARTS | GENERATE_NUMBER_PARTS | CATENATE_ALL | SPLIT_ON_CASE_CHANGE | SPLIT_ON_NUMERICS | STEM_ENGLISH_POSSESSIVE;
     TokenFilter wdf = new Lucene47WordDelimiterFilter(new SingleTokenTokenStream(new Token("übelkeit)", 7, 16)), DEFAULT_WORD_DELIM_TABLE, flags, null);
     
-    assertTokenStreamContents(wdf,
+    assertBrokenTokenStreamContents(wdf,
         new String[] { "übelkeit" },
         new int[] { 7 },
-        new int[] { 15 });
+        new int[] { 15 },
+        new int[] { 1 });
   }
   
   @Test
@@ -96,10 +97,11 @@
     int flags = GENERATE_WORD_PARTS | GENERATE_NUMBER_PARTS | CATENATE_ALL | SPLIT_ON_CASE_CHANGE | SPLIT_ON_NUMERICS | STEM_ENGLISH_POSSESSIVE;
     TokenFilter wdf = new Lucene47WordDelimiterFilter(new SingleTokenTokenStream(new Token("(übelkeit", 7, 17)), DEFAULT_WORD_DELIM_TABLE, flags, null);
     
-    assertTokenStreamContents(wdf,
+    assertBrokenTokenStreamContents(wdf,
         new String[] { "übelkeit" },
         new int[] { 8 },
-        new int[] { 17 });
+        new int[] { 17 },
+        new int[] { 1 });
   }
   
   @Test
@@ -107,10 +109,11 @@
     int flags = GENERATE_WORD_PARTS | GENERATE_NUMBER_PARTS | CATENATE_ALL | SPLIT_ON_CASE_CHANGE | SPLIT_ON_NUMERICS | STEM_ENGLISH_POSSESSIVE;
     TokenFilter wdf = new Lucene47WordDelimiterFilter(new SingleTokenTokenStream(new Token("(übelkeit", 7, 16)), DEFAULT_WORD_DELIM_TABLE, flags, null);
     
-    assertTokenStreamContents(wdf,
+    assertBrokenTokenStreamContents(wdf,
         new String[] { "übelkeit" },
         new int[] { 8 },
-        new int[] { 16 });
+        new int[] { 16 },
+        new int[] { 1 });
   }
   
   @Test
@@ -118,11 +121,11 @@
     int flags = GENERATE_WORD_PARTS | GENERATE_NUMBER_PARTS | CATENATE_ALL | SPLIT_ON_CASE_CHANGE | SPLIT_ON_NUMERICS | STEM_ENGLISH_POSSESSIVE;
     TokenFilter wdf = new Lucene47WordDelimiterFilter(new SingleTokenTokenStream(new Token("(foo,bar)", 7, 16)), DEFAULT_WORD_DELIM_TABLE, flags, null);
     
-    assertTokenStreamContents(wdf,
+    assertBrokenTokenStreamContents(wdf,
         new String[] { "foo", "bar", "foobar"},
         new int[] { 8, 12, 8 },
         new int[] { 11, 15, 15 },
-        null, null, null, null, false);
+        new int[] { 1, 1, 0 });
   }
 
   public void doSplit(final String input, String... output) throws Exception {
@@ -130,7 +133,7 @@
     TokenFilter wdf = new Lucene47WordDelimiterFilter(keywordMockTokenizer(input),
         WordDelimiterIterator.DEFAULT_WORD_DELIM_TABLE, flags, null);
     
-    assertTokenStreamContents(wdf, output);
+    assertBrokenTokenStreamContents(wdf, output, null, null, null);
   }
 
   @Test
@@ -173,7 +176,7 @@
     flags |= (stemPossessive == 1) ? STEM_ENGLISH_POSSESSIVE : 0;
     TokenFilter wdf = new Lucene47WordDelimiterFilter(keywordMockTokenizer(input), flags, null);
 
-    assertTokenStreamContents(wdf, output);
+    assertBrokenTokenStreamContents(wdf, output, null, null, null);
   }
   
   /*
@@ -225,30 +228,24 @@
     };
 
     /* in this case, works as expected. */
-    assertAnalyzesTo(a, "LUCENE / SOLR", new String[] { "LUCENE", "SOLR" },
+    assertBrokenTokenStreamContents(a.tokenStream("bogus", "LUCENE / SOLR"), 
+        new String[] { "LUCENE", "SOLR" },
         new int[] { 0, 9 },
         new int[] { 6, 13 },
-        null,
-        new int[] { 1, 1 },
-        null,
-        false);
+        new int[] { 1, 1 });
     
     /* only in this case, posInc of 2 ?! */
-    assertAnalyzesTo(a, "LUCENE / solR", new String[] { "LUCENE", "sol", "R", "solR" },
+    assertBrokenTokenStreamContents(a.tokenStream("bogus", "LUCENE / solR"), 
+        new String[] { "LUCENE", "sol", "R", "solR" },
         new int[] { 0, 9, 12, 9 },
         new int[] { 6, 12, 13, 13 },
-        null,
-        new int[] { 1, 1, 1, 0 },
-        null,
-        false);
+        new int[] { 1, 1, 1, 0 });
     
-    assertAnalyzesTo(a, "LUCENE / NUTCH SOLR", new String[] { "LUCENE", "NUTCH", "SOLR" },
+    assertBrokenTokenStreamContents(a.tokenStream("bogus", "LUCENE / NUTCH SOLR"), 
+        new String[] { "LUCENE", "NUTCH", "SOLR" },
         new int[] { 0, 9, 15 },
         new int[] { 6, 14, 19 },
-        null,
-        new int[] { 1, 1, 1 },
-        null,
-        false);
+        new int[] { 1, 1, 1 });
     
     /* analyzer that will consume tokens with large position increments */
     Analyzer a2 = new Analyzer() {
@@ -262,39 +259,31 @@
     };
     
     /* increment of "largegap" is preserved */
-    assertAnalyzesTo(a2, "LUCENE largegap SOLR", new String[] { "LUCENE", "largegap", "SOLR" },
+    assertBrokenTokenStreamContents(a2.tokenStream("bogus", "LUCENE largegap SOLR"), 
+        new String[] { "LUCENE", "largegap", "SOLR" },
         new int[] { 0, 7, 16 },
         new int[] { 6, 15, 20 },
-        null,
-        new int[] { 1, 10, 1 },
-        null,
-        false);
+        new int[] { 1, 10, 1 });
     
     /* the "/" had a position increment of 10, where did it go?!?!! */
-    assertAnalyzesTo(a2, "LUCENE / SOLR", new String[] { "LUCENE", "SOLR" },
+    assertBrokenTokenStreamContents(a2.tokenStream("bogus", "LUCENE / SOLR"), 
+        new String[] { "LUCENE", "SOLR" },
         new int[] { 0, 9 },
         new int[] { 6, 13 },
-        null,
-        new int[] { 1, 11 },
-        null,
-        false);
+        new int[] { 1, 11 });
     
     /* in this case, the increment of 10 from the "/" is carried over */
-    assertAnalyzesTo(a2, "LUCENE / solR", new String[] { "LUCENE", "sol", "R", "solR" },
+    assertBrokenTokenStreamContents(a2.tokenStream("bogus", "LUCENE / solR"), 
+        new String[] { "LUCENE", "sol", "R", "solR" },
         new int[] { 0, 9, 12, 9 },
         new int[] { 6, 12, 13, 13 },
-        null,
-        new int[] { 1, 11, 1, 0 },
-        null,
-        false);
+        new int[] { 1, 11, 1, 0 });
     
-    assertAnalyzesTo(a2, "LUCENE / NUTCH SOLR", new String[] { "LUCENE", "NUTCH", "SOLR" },
+    assertBrokenTokenStreamContents(a2.tokenStream("bogus", "LUCENE / NUTCH SOLR"), 
+        new String[] { "LUCENE", "NUTCH", "SOLR" },
         new int[] { 0, 9, 15 },
         new int[] { 6, 14, 19 },
-        null,
-        new int[] { 1, 11, 1 },
-        null,
-        false);
+        new int[] { 1, 11, 1 });
 
     Analyzer a3 = new Analyzer() {
       @Override
@@ -306,70 +295,17 @@
       }
     };
 
-    assertAnalyzesTo(a3, "lucene.solr", 
+    assertBrokenTokenStreamContents(a3.tokenStream("bogus", "lucene.solr"), 
         new String[] { "lucene", "solr", "lucenesolr" },
         new int[] { 0, 7, 0 },
         new int[] { 6, 11, 11 },
-        null,
-        new int[] { 1, 1, 0 },
-        null,
-        false);
+        new int[] { 1, 1, 0 });
 
     /* the stopword should add a gap here */
-    assertAnalyzesTo(a3, "the lucene.solr", 
+   assertBrokenTokenStreamContents(a3.tokenStream("bogus", "the lucene.solr"), 
         new String[] { "lucene", "solr", "lucenesolr" }, 
         new int[] { 4, 11, 4 }, 
         new int[] { 10, 15, 15 },
-        null,
-        new int[] { 2, 1, 0 },
-        null,
-        false);
+        new int[] { 2, 1, 0 });
   }
-  
-  /** blast some random strings through the analyzer */
-  public void testRandomStrings() throws Exception {
-    int numIterations = atLeast(5);
-    for (int i = 0; i < numIterations; i++) {
-      final int flags = random().nextInt(512);
-      final CharArraySet protectedWords;
-      if (random().nextBoolean()) {
-        protectedWords = new CharArraySet(TEST_VERSION_CURRENT, new HashSet<>(Arrays.asList("a", "b", "cd")), false);
-      } else {
-        protectedWords = null;
-      }
-      
-      Analyzer a = new Analyzer() {
-        
-        @Override
-        protected TokenStreamComponents createComponents(String fieldName) {
-          Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);
-          return new TokenStreamComponents(tokenizer, new Lucene47WordDelimiterFilter(tokenizer, flags, protectedWords));
-        }
-      };
-      checkRandomData(random(), a, 200, 20, false, false);
-    }
-  }
-  
-  public void testEmptyTerm() throws IOException {
-    Random random = random();
-    for (int i = 0; i < 512; i++) {
-      final int flags = i;
-      final CharArraySet protectedWords;
-      if (random.nextBoolean()) {
-        protectedWords = new CharArraySet(TEST_VERSION_CURRENT, new HashSet<>(Arrays.asList("a", "b", "cd")), false);
-      } else {
-        protectedWords = null;
-      }
-    
-      Analyzer a = new Analyzer() { 
-        @Override
-        protected TokenStreamComponents createComponents(String fieldName) {
-          Tokenizer tokenizer = new KeywordTokenizer();
-          return new TokenStreamComponents(tokenizer, new Lucene47WordDelimiterFilter(tokenizer, flags, protectedWords));
-        }
-      };
-      // depending upon options, this thing may or may not preserve the empty term
-      checkAnalysisConsistency(random, a, random.nextBoolean(), "");
-    }
-  }
 }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/ngram/NGramTokenFilterTest.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/ngram/NGramTokenFilterTest.java	(revision 1579142)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/ngram/NGramTokenFilterTest.java	(working copy)
@@ -174,14 +174,11 @@
 
   public void testLucene43() throws IOException {
     NGramTokenFilter filter = new NGramTokenFilter(Version.LUCENE_43, input, 2, 3);
-    assertTokenStreamContents(filter,
+    assertBrokenTokenStreamContents(filter,
         new String[]{"ab","bc","cd","de","abc","bcd","cde"},
         new int[]{0,1,2,3,0,1,2},
         new int[]{2,3,4,5,3,4,5},
-        null,
-        new int[]{1,1,1,1,1,1,1},
-        null, null, false
-        );
+        new int[]{1,1,1,1,1,1,1});
   }
 
   public void testSupplementaryCharacters() throws IOException {
Index: lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase.java
===================================================================
--- lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase.java	(revision 1579142)
+++ lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase.java	(working copy)
@@ -105,15 +105,13 @@
     }
   }
 
-  // offsetsAreCorrect also validates:
+  // graphOffsetsAreCorrect validates:
   //   - graph offsets are correct (all tokens leaving from
   //     pos X have the same startOffset; all tokens
   //     arriving to pos Y have the same endOffset)
-  //   - offsets only move forwards (startOffset >=
-  //     lastStartOffset)
   public static void assertTokenStreamContents(TokenStream ts, String[] output, int startOffsets[], int endOffsets[], String types[], int posIncrements[],
                                                int posLengths[], Integer finalOffset, Integer finalPosInc, boolean[] keywordAtts,
-                                               boolean offsetsAreCorrect) throws IOException {
+                                               boolean graphOffsetsAreCorrect) throws IOException {
     assertNotNull(output);
     CheckClearAttributesAttribute checkClearAtt = ts.addAttribute(CheckClearAttributesAttribute.class);
     
@@ -204,12 +202,11 @@
                      endOffset <= finalOffset.intValue());
         }
 
-        if (offsetsAreCorrect) {
-          assertTrue("offsets must not go backwards startOffset=" + startOffset + " is < lastStartOffset=" + lastStartOffset, offsetAtt.startOffset() >= lastStartOffset);
-          lastStartOffset = offsetAtt.startOffset();
-        }
 
-        if (offsetsAreCorrect && posLengthAtt != null && posIncrAtt != null) {
+        assertTrue("offsets must not go backwards startOffset=" + startOffset + " is < lastStartOffset=" + lastStartOffset, offsetAtt.startOffset() >= lastStartOffset);
+        lastStartOffset = offsetAtt.startOffset();
+
+        if (graphOffsetsAreCorrect && posLengthAtt != null && posIncrAtt != null) {
           // Validate offset consistency in the graph, ie
           // all tokens leaving from a certain pos have the
           // same startOffset, and all tokens arriving to a
@@ -288,12 +285,12 @@
   
   public static void assertTokenStreamContents(TokenStream ts, String[] output, int startOffsets[], int endOffsets[], String types[], int posIncrements[],
                                                int posLengths[], Integer finalOffset, boolean[] keywordAtts,
-                                               boolean offsetsAreCorrect) throws IOException {
-    assertTokenStreamContents(ts, output, startOffsets, endOffsets, types, posIncrements, posLengths, finalOffset, null, null, offsetsAreCorrect);
+                                               boolean graphOffsetsAreCorrect) throws IOException {
+    assertTokenStreamContents(ts, output, startOffsets, endOffsets, types, posIncrements, posLengths, finalOffset, null, null, graphOffsetsAreCorrect);
   }
 
-  public static void assertTokenStreamContents(TokenStream ts, String[] output, int startOffsets[], int endOffsets[], String types[], int posIncrements[], int posLengths[], Integer finalOffset, boolean offsetsAreCorrect) throws IOException {
-    assertTokenStreamContents(ts, output, startOffsets, endOffsets, types, posIncrements, posLengths, finalOffset, null, offsetsAreCorrect);
+  public static void assertTokenStreamContents(TokenStream ts, String[] output, int startOffsets[], int endOffsets[], String types[], int posIncrements[], int posLengths[], Integer finalOffset, boolean graphOffsetsAreCorrect) throws IOException {
+    assertTokenStreamContents(ts, output, startOffsets, endOffsets, types, posIncrements, posLengths, finalOffset, null, graphOffsetsAreCorrect);
   }
 
   public static void assertTokenStreamContents(TokenStream ts, String[] output, int startOffsets[], int endOffsets[], String types[], int posIncrements[], int posLengths[], Integer finalOffset) throws IOException {
@@ -340,6 +337,37 @@
     assertTokenStreamContents(ts, output, startOffsets, endOffsets, null, posIncrements, posLengths, finalOffset);
   }
   
+  /** 
+   * wimpy version of assertTokenStreamContents that allows for various forms of brokenness
+   * @deprecated only use this for testing back compat components */
+  @Deprecated
+  public static void assertBrokenTokenStreamContents(TokenStream ts, String[] output, int startOffsets[], int endOffsets[], int posIncrements[]) throws IOException {
+    ts.reset();
+    CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);
+    OffsetAttribute offsetAtt = ts.addAttribute(OffsetAttribute.class);
+    PositionIncrementAttribute posIncAtt = ts.addAttribute(PositionIncrementAttribute.class);
+    int currentIndex = 0;
+    while (ts.incrementToken()) {
+      if (currentIndex >= output.length) {
+        fail("too many tokens from stream");
+      }
+      assertEquals(output[currentIndex], termAtt.toString());
+      if (startOffsets != null) {
+        assertEquals(startOffsets[currentIndex], offsetAtt.startOffset());
+      }
+      if (endOffsets != null) {
+        assertEquals(endOffsets[currentIndex], offsetAtt.endOffset());
+      }
+      if (posIncrements != null) {
+        assertEquals(posIncrements[currentIndex], posIncAtt.getPositionIncrement());
+      }
+      currentIndex++;
+    }
+    assertEquals("too few tokens from stream", currentIndex, output.length);
+    ts.end();
+    ts.close();
+  }
+  
   public static void assertAnalyzesTo(Analyzer a, String input, String[] output, int startOffsets[], int endOffsets[], String types[], int posIncrements[]) throws IOException {
     checkResetException(a, input);
     assertTokenStreamContents(a.tokenStream("dummy", input), output, startOffsets, endOffsets, types, posIncrements, null, input.length());
@@ -350,9 +378,9 @@
     assertTokenStreamContents(a.tokenStream("dummy", input), output, startOffsets, endOffsets, types, posIncrements, posLengths, input.length());
   }
 
-  public static void assertAnalyzesTo(Analyzer a, String input, String[] output, int startOffsets[], int endOffsets[], String types[], int posIncrements[], int posLengths[], boolean offsetsAreCorrect) throws IOException {
+  public static void assertAnalyzesTo(Analyzer a, String input, String[] output, int startOffsets[], int endOffsets[], String types[], int posIncrements[], int posLengths[], boolean graphOffsetsAreCorrect) throws IOException {
     checkResetException(a, input);
-    assertTokenStreamContents(a.tokenStream("dummy", input), output, startOffsets, endOffsets, types, posIncrements, posLengths, input.length(), offsetsAreCorrect);
+    assertTokenStreamContents(a.tokenStream("dummy", input), output, startOffsets, endOffsets, types, posIncrements, posLengths, input.length(), graphOffsetsAreCorrect);
   }
   
   public static void assertAnalyzesTo(Analyzer a, String input, String[] output) throws IOException {
@@ -448,7 +476,7 @@
     final Analyzer a;
     final boolean useCharFilter;
     final boolean simple;
-    final boolean offsetsAreCorrect;
+    final boolean graphOffsetsAreCorrect;
     final RandomIndexWriter iw;
 
     // NOTE: not volatile because we don't want the tests to
@@ -456,7 +484,7 @@
     // interact)... so this is just "best effort":
     public boolean failed;
     
-    AnalysisThread(long seed, Analyzer a, int iterations, int maxWordLength, boolean useCharFilter, boolean simple, boolean offsetsAreCorrect, RandomIndexWriter iw) {
+    AnalysisThread(long seed, Analyzer a, int iterations, int maxWordLength, boolean useCharFilter, boolean simple, boolean graphOffsetsAreCorrect, RandomIndexWriter iw) {
       this.seed = seed;
       this.a = a;
       this.iterations = iterations;
@@ -463,7 +491,7 @@
       this.maxWordLength = maxWordLength;
       this.useCharFilter = useCharFilter;
       this.simple = simple;
-      this.offsetsAreCorrect = offsetsAreCorrect;
+      this.graphOffsetsAreCorrect = graphOffsetsAreCorrect;
       this.iw = iw;
     }
     
@@ -473,7 +501,7 @@
       try {
         // see the part in checkRandomData where it replays the same text again
         // to verify reproducability/reuse: hopefully this would catch thread hazards.
-        checkRandomData(new Random(seed), a, iterations, maxWordLength, useCharFilter, simple, offsetsAreCorrect, iw);
+        checkRandomData(new Random(seed), a, iterations, maxWordLength, useCharFilter, simple, graphOffsetsAreCorrect, iw);
         success = true;
       } catch (IOException e) {
         Rethrow.rethrow(e);
@@ -487,7 +515,7 @@
     checkRandomData(random, a, iterations, maxWordLength, simple, true);
   }
 
-  public static void checkRandomData(Random random, Analyzer a, int iterations, int maxWordLength, boolean simple, boolean offsetsAreCorrect) throws IOException {
+  public static void checkRandomData(Random random, Analyzer a, int iterations, int maxWordLength, boolean simple, boolean graphOffsetsAreCorrect) throws IOException {
     checkResetException(a, "best effort");
     long seed = random.nextLong();
     boolean useCharFilter = random.nextBoolean();
@@ -503,13 +531,13 @@
     }
     boolean success = false;
     try {
-      checkRandomData(new Random(seed), a, iterations, maxWordLength, useCharFilter, simple, offsetsAreCorrect, iw);
+      checkRandomData(new Random(seed), a, iterations, maxWordLength, useCharFilter, simple, graphOffsetsAreCorrect, iw);
       // now test with multiple threads: note we do the EXACT same thing we did before in each thread,
       // so this should only really fail from another thread if its an actual thread problem
       int numThreads = TestUtil.nextInt(random, 2, 4);
       AnalysisThread threads[] = new AnalysisThread[numThreads];
       for (int i = 0; i < threads.length; i++) {
-        threads[i] = new AnalysisThread(seed, a, iterations, maxWordLength, useCharFilter, simple, offsetsAreCorrect, iw);
+        threads[i] = new AnalysisThread(seed, a, iterations, maxWordLength, useCharFilter, simple, graphOffsetsAreCorrect, iw);
       }
       for (int i = 0; i < threads.length; i++) {
         threads[i].start();
@@ -536,7 +564,7 @@
     }
   }
 
-  private static void checkRandomData(Random random, Analyzer a, int iterations, int maxWordLength, boolean useCharFilter, boolean simple, boolean offsetsAreCorrect, RandomIndexWriter iw) throws IOException {
+  private static void checkRandomData(Random random, Analyzer a, int iterations, int maxWordLength, boolean useCharFilter, boolean simple, boolean graphOffsetsAreCorrect, RandomIndexWriter iw) throws IOException {
 
     final LineFileDocs docs = new LineFileDocs(random);
     Document doc = null;
@@ -563,7 +591,7 @@
         case 1: ft.setIndexOptions(IndexOptions.DOCS_AND_FREQS); break;
         case 2: ft.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS); break;
         default:
-                if (supportsOffsets && offsetsAreCorrect) {
+                if (supportsOffsets) {
                   ft.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);
                 } else {
                   ft.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);
@@ -602,7 +630,7 @@
         }
         
         try {
-          checkAnalysisConsistency(random, a, useCharFilter, text, offsetsAreCorrect, currentField);
+          checkAnalysisConsistency(random, a, useCharFilter, text, graphOffsetsAreCorrect, currentField);
           if (iw != null) {
             if (random.nextInt(7) == 0) {
               // pile up a multivalued field
@@ -664,11 +692,11 @@
     checkAnalysisConsistency(random, a, useCharFilter, text, true);
   }
 
-  public static void checkAnalysisConsistency(Random random, Analyzer a, boolean useCharFilter, String text, boolean offsetsAreCorrect) throws IOException {
-    checkAnalysisConsistency(random, a, useCharFilter, text, offsetsAreCorrect, null);
+  public static void checkAnalysisConsistency(Random random, Analyzer a, boolean useCharFilter, String text, boolean graphOffsetsAreCorrect) throws IOException {
+    checkAnalysisConsistency(random, a, useCharFilter, text, graphOffsetsAreCorrect, null);
   }
   
-  private static void checkAnalysisConsistency(Random random, Analyzer a, boolean useCharFilter, String text, boolean offsetsAreCorrect, Field field) throws IOException {
+  private static void checkAnalysisConsistency(Random random, Analyzer a, boolean useCharFilter, String text, boolean graphOffsetsAreCorrect, Field field) throws IOException {
 
     if (VERBOSE) {
       System.out.println(Thread.currentThread().getName() + ": NOTE: BaseTokenStreamTestCase: get first token stream now text=" + text);
@@ -811,7 +839,7 @@
                                 toIntArray(positions),
                                 toIntArray(positionLengths),
                                 text.length(),
-                                offsetsAreCorrect);
+                                graphOffsetsAreCorrect);
     } else if (typeAtt != null && posIncAtt != null && offsetAtt != null) {
       // offset + pos + type
       assertTokenStreamContents(ts, 
@@ -822,7 +850,7 @@
                                 toIntArray(positions),
                                 null,
                                 text.length(),
-                                offsetsAreCorrect);
+                                graphOffsetsAreCorrect);
     } else if (posIncAtt != null && posLengthAtt != null && offsetAtt != null) {
       // offset + pos + posLength
       assertTokenStreamContents(ts, 
@@ -833,7 +861,7 @@
                                 toIntArray(positions),
                                 toIntArray(positionLengths),
                                 text.length(),
-                                offsetsAreCorrect);
+                                graphOffsetsAreCorrect);
     } else if (posIncAtt != null && offsetAtt != null) {
       // offset + pos
       assertTokenStreamContents(ts, 
@@ -844,7 +872,7 @@
                                 toIntArray(positions),
                                 null,
                                 text.length(),
-                                offsetsAreCorrect);
+                                graphOffsetsAreCorrect);
     } else if (offsetAtt != null) {
       // offset
       assertTokenStreamContents(ts, 
@@ -855,7 +883,7 @@
                                 null,
                                 null,
                                 text.length(),
-                                offsetsAreCorrect);
+                                graphOffsetsAreCorrect);
     } else {
       // terms only
       assertTokenStreamContents(ts, 
Index: lucene/test-framework/src/java/org/apache/lucene/analysis/ValidatingTokenFilter.java
===================================================================
--- lucene/test-framework/src/java/org/apache/lucene/analysis/ValidatingTokenFilter.java	(revision 1579142)
+++ lucene/test-framework/src/java/org/apache/lucene/analysis/ValidatingTokenFilter.java	(working copy)
@@ -51,7 +51,7 @@
   private final PositionLengthAttribute posLenAtt = getAttrIfExists(PositionLengthAttribute.class);
   private final OffsetAttribute offsetAtt = getAttrIfExists(OffsetAttribute.class);
   private final CharTermAttribute termAtt = getAttrIfExists(CharTermAttribute.class);
-  private final boolean offsetsAreCorrect;
+  private final boolean graphOffsetsAreCorrect;
 
   private final String name;
 
@@ -67,10 +67,10 @@
   /** The name arg is used to identify this stage when
    *  throwing exceptions (useful if you have more than one
    *  instance in your chain). */
-  public ValidatingTokenFilter(TokenStream in, String name, boolean offsetsAreCorrect) {
+  public ValidatingTokenFilter(TokenStream in, String name, boolean graphOffsetsAreCorrect) {
     super(in);
     this.name = name;
-    this.offsetsAreCorrect = offsetsAreCorrect;
+    this.graphOffsetsAreCorrect = graphOffsetsAreCorrect;
   }
 
   @Override
@@ -90,13 +90,13 @@
       }
     }
 
-    // System.out.println("  got token=" + termAtt + " pos=" + pos);
+    System.out.println(this);
     
     if (offsetAtt != null) {
       startOffset = offsetAtt.startOffset();
       endOffset = offsetAtt.endOffset();
 
-      if (offsetsAreCorrect && offsetAtt.startOffset() < lastStartOffset) {
+      if (offsetAtt.startOffset() < lastStartOffset) {
         throw new IllegalStateException(name + ": offsets must not go backwards startOffset=" + startOffset + " is < lastStartOffset=" + lastStartOffset);
       }
       lastStartOffset = offsetAtt.startOffset();
@@ -104,7 +104,7 @@
     
     posLen = posLenAtt == null ? 1 : posLenAtt.getPositionLength();
     
-    if (offsetAtt != null && posIncAtt != null && offsetsAreCorrect) {
+    if (offsetAtt != null && posIncAtt != null && graphOffsetsAreCorrect) {
 
       if (!posToStartOffset.containsKey(pos)) {
         // First time we've seen a token leaving from this position:
