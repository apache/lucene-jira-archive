Index: lucene/core/src/java/org/apache/lucene/search/BooleanQuery.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/search/BooleanQuery.java	(revision 1598989)
+++ lucene/core/src/java/org/apache/lucene/search/BooleanQuery.java	(working copy)
@@ -20,6 +20,7 @@
 import java.io.IOException;
 import java.util.ArrayList;
 import java.util.Arrays;
+import java.util.Collection;
 import java.util.Iterator;
 import java.util.List;
 import java.util.Set;
@@ -579,6 +580,15 @@
     }
   }
 
+  @Override
+  public void extractQueries(Collection<Query> queries) {
+    for (BooleanClause clause : clauses) {
+      if (clause.getOccur() != Occur.MUST_NOT) {
+        clause.getQuery().extractQueries(queries);
+      }
+    }
+  }
+
   @Override @SuppressWarnings("unchecked")
   public BooleanQuery clone() {
     BooleanQuery clone = (BooleanQuery)super.clone();
Index: lucene/core/src/java/org/apache/lucene/search/ConstantScoreQuery.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/search/ConstantScoreQuery.java	(revision 1598989)
+++ lucene/core/src/java/org/apache/lucene/search/ConstantScoreQuery.java	(working copy)
@@ -103,6 +103,13 @@
       query.extractTerms(terms);
   }
 
+  @Override
+  public void extractQueries(Collection<Query> queries) {
+    if (query != null) {
+      query.extractQueries(queries);
+    }
+  }
+
   protected class ConstantWeight extends Weight {
     private final Weight innerWeight;
     private float queryNorm;
Index: lucene/core/src/java/org/apache/lucene/search/DisjunctionMaxQuery.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/search/DisjunctionMaxQuery.java	(revision 1598989)
+++ lucene/core/src/java/org/apache/lucene/search/DisjunctionMaxQuery.java	(working copy)
@@ -246,6 +246,13 @@
     }
   }
 
+  @Override
+  public void extractQueries(Collection<Query> queries) {
+    for (Query query : disjuncts) {
+      query.extractQueries(queries);
+    }
+  }
+
   /** Prettyprint us.
    * @param field the field to which we are applied
    * @return a string that shows what we do, of the form "(disjunct1 | disjunct2 | ... | disjunctn)^boost"
Index: lucene/core/src/java/org/apache/lucene/search/FilteredQuery.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/search/FilteredQuery.java	(revision 1598989)
+++ lucene/core/src/java/org/apache/lucene/search/FilteredQuery.java	(working copy)
@@ -383,6 +383,11 @@
     getQuery().extractTerms(terms);
   }
 
+  @Override
+  public void extractQueries(Collection<Query> queries) {
+    getQuery().extractQueries(queries);
+  }
+
   /** Prints a user-readable version of this query. */
   @Override
   public String toString (String s) {
Index: lucene/core/src/java/org/apache/lucene/search/Query.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/search/Query.java	(revision 1598989)
+++ lucene/core/src/java/org/apache/lucene/search/Query.java	(working copy)
@@ -19,6 +19,7 @@
 
 import java.io.IOException;
 
+import java.util.Collection;
 import java.util.Set;
 
 import org.apache.lucene.index.IndexReader;
@@ -99,6 +100,15 @@
     throw new UnsupportedOperationException();
   }
 
+  /**
+   * Expert: adds all sub-queries that are part of this query to the queries
+   * collection. No need to override if the query is not a compound one,  as
+   * the query itself gets returned by default.
+   */
+  public void extractQueries(Collection<Query> queries) {
+    queries.add(this);
+  }
+
   /** Returns a clone of this query. */
   @Override
   public Query clone() {
Index: lucene/core/src/java/org/apache/lucene/search/spans/FieldMaskingSpanQuery.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/search/spans/FieldMaskingSpanQuery.java	(revision 1598989)
+++ lucene/core/src/java/org/apache/lucene/search/spans/FieldMaskingSpanQuery.java	(working copy)
@@ -18,6 +18,7 @@
  */
 
 import java.io.IOException;
+import java.util.Collection;
 import java.util.Map;
 import java.util.Set;
 
@@ -103,9 +104,14 @@
   @Override
   public void extractTerms(Set<Term> terms) {
     maskedQuery.extractTerms(terms);
-  }  
+  }
 
   @Override
+  public void extractQueries(Collection<Query> queries) {
+    maskedQuery.extractQueries(queries);
+  }
+
+  @Override
   public Weight createWeight(IndexSearcher searcher) throws IOException {
     return maskedQuery.createWeight(searcher);
   }
Index: lucene/core/src/java/org/apache/lucene/search/spans/SpanMultiTermQueryWrapper.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/search/spans/SpanMultiTermQueryWrapper.java	(revision 1598989)
+++ lucene/core/src/java/org/apache/lucene/search/spans/SpanMultiTermQueryWrapper.java	(working copy)
@@ -18,6 +18,7 @@
  */
 
 import java.io.IOException;
+import java.util.Collection;
 import java.util.Map;
 
 import org.apache.lucene.index.AtomicReaderContext;
@@ -120,6 +121,11 @@
   }
 
   @Override
+  public void extractQueries(Collection<Query> queries) {
+    query.extractQueries(queries);
+  }
+
+  @Override
   public Query rewrite(IndexReader reader) throws IOException {
     final Query q = query.rewrite(reader);
     if (!(q instanceof SpanQuery))
Index: lucene/core/src/java/org/apache/lucene/search/spans/SpanNearQuery.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/search/spans/SpanNearQuery.java	(revision 1598989)
+++ lucene/core/src/java/org/apache/lucene/search/spans/SpanNearQuery.java	(working copy)
@@ -20,6 +20,7 @@
 import java.io.IOException;
 
 
+import java.util.Collection;
 import java.util.List;
 import java.util.ArrayList;
 import java.util.Iterator;
@@ -95,10 +96,16 @@
     for (final SpanQuery clause : clauses) {
       clause.extractTerms(terms);
     }
-  }  
-  
+  }
 
   @Override
+  public void extractQueries(Collection<Query> queries) {
+    for (final SpanQuery clause : clauses) {
+      clause.extractQueries(queries);
+    }
+  }
+
+  @Override
   public String toString(String field) {
     StringBuilder buffer = new StringBuilder();
     buffer.append("spanNear([");
Index: lucene/core/src/java/org/apache/lucene/search/spans/SpanNotQuery.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/search/spans/SpanNotQuery.java	(revision 1598989)
+++ lucene/core/src/java/org/apache/lucene/search/spans/SpanNotQuery.java	(working copy)
@@ -79,6 +79,11 @@
   public void extractTerms(Set<Term> terms) { include.extractTerms(terms); }
 
   @Override
+  public void extractQueries(Collection<Query> queries) {
+    include.extractQueries(queries);
+  }
+
+  @Override
   public String toString(String field) {
     StringBuilder buffer = new StringBuilder();
     buffer.append("spanNot(");
Index: lucene/core/src/java/org/apache/lucene/search/spans/SpanOrQuery.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/search/spans/SpanOrQuery.java	(revision 1598989)
+++ lucene/core/src/java/org/apache/lucene/search/spans/SpanOrQuery.java	(working copy)
@@ -74,8 +74,15 @@
       clause.extractTerms(terms);
     }
   }
-  
+
   @Override
+  public void extractQueries(Collection<Query> queries) {
+    for(final SpanQuery clause: clauses) {
+      clause.extractQueries(queries);
+    }
+  }
+
+  @Override
   public SpanOrQuery clone() {
     int sz = clauses.size();
     SpanQuery[] newClauses = new SpanQuery[sz];
Index: lucene/core/src/java/org/apache/lucene/search/spans/SpanPositionCheckQuery.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/search/spans/SpanPositionCheckQuery.java	(revision 1598989)
+++ lucene/core/src/java/org/apache/lucene/search/spans/SpanPositionCheckQuery.java	(working copy)
@@ -60,7 +60,12 @@
     match.extractTerms(terms);
   }
 
-  /** 
+  @Override
+  public void extractQueries(Collection<Query> queries) {
+    match.extractQueries(queries);
+  }
+
+  /**
    * Return value for {@link SpanPositionCheckQuery#acceptPosition(Spans)}.
    */
   protected static enum AcceptStatus {
Index: lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/MultiTermHighlighting.java
===================================================================
--- lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/MultiTermHighlighting.java	(revision 1598989)
+++ lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/MultiTermHighlighting.java	(working copy)
@@ -62,37 +62,17 @@
    * automata that will match terms.
    */
   static CharacterRunAutomaton[] extractAutomata(Query query, String field) {
+    List<Query> queries = new ArrayList<>();
+    query.extractQueries(queries);
     List<CharacterRunAutomaton> list = new ArrayList<>();
-    if (query instanceof BooleanQuery) {
-      BooleanClause clauses[] = ((BooleanQuery) query).getClauses();
-      for (BooleanClause clause : clauses) {
-        if (!clause.isProhibited()) {
-          list.addAll(Arrays.asList(extractAutomata(clause.getQuery(), field)));
-        }
-      }
-    } else if (query instanceof FilteredQuery) {
-      list.addAll(Arrays.asList(extractAutomata(((FilteredQuery) query).getQuery(), field)));
-    } else if (query instanceof ConstantScoreQuery) {
-      list.addAll(Arrays.asList(extractAutomata(((ConstantScoreQuery) query).getQuery(), field)));
-    } else if (query instanceof DisjunctionMaxQuery) {
-      for (Query sub : ((DisjunctionMaxQuery) query).getDisjuncts()) {
-        list.addAll(Arrays.asList(extractAutomata(sub, field)));
-      }
-    } else if (query instanceof SpanOrQuery) {
-      for (Query sub : ((SpanOrQuery) query).getClauses()) {
-        list.addAll(Arrays.asList(extractAutomata(sub, field)));
-      }
-    } else if (query instanceof SpanNearQuery) {
-      for (Query sub : ((SpanNearQuery) query).getClauses()) {
-        list.addAll(Arrays.asList(extractAutomata(sub, field)));
-      }
-    } else if (query instanceof SpanNotQuery) {
-      list.addAll(Arrays.asList(extractAutomata(((SpanNotQuery) query).getInclude(), field)));
-    } else if (query instanceof SpanPositionCheckQuery) {
-      list.addAll(Arrays.asList(extractAutomata(((SpanPositionCheckQuery) query).getMatch(), field)));
-    } else if (query instanceof SpanMultiTermQueryWrapper) {
-      list.addAll(Arrays.asList(extractAutomata(((SpanMultiTermQueryWrapper<?>) query).getWrappedQuery(), field)));
-    } else if (query instanceof AutomatonQuery) {
+    for (Query subQuery : queries) {
+      extractAutomata(subQuery, field, list);
+    }
+    return list.toArray(new CharacterRunAutomaton[list.size()]);
+  }
+
+  private static void extractAutomata(Query query, String field, List<CharacterRunAutomaton> list) {
+    if (query instanceof AutomatonQuery) {
       final AutomatonQuery aq = (AutomatonQuery) query;
       if (aq.getField().equals(field)) {
         list.add(new CharacterRunAutomaton(aq.getAutomaton()) {
@@ -106,7 +86,7 @@
       final PrefixQuery pq = (PrefixQuery) query;
       Term prefix = pq.getPrefix();
       if (prefix.field().equals(field)) {
-        list.add(new CharacterRunAutomaton(BasicOperations.concatenate(BasicAutomata.makeString(prefix.text()), 
+        list.add(new CharacterRunAutomaton(BasicOperations.concatenate(BasicAutomata.makeString(prefix.text()),
                                                                        BasicAutomata.makeAnyString())) {
           @Override
           public String toString() {
@@ -191,7 +171,6 @@
         });
       }
     }
-    return list.toArray(new CharacterRunAutomaton[list.size()]);
   }
   
   /** 
Index: lucene/highlighter/src/test/org/apache/lucene/search/postingshighlight/TestMultiTermHighlighting.java
===================================================================
--- lucene/highlighter/src/test/org/apache/lucene/search/postingshighlight/TestMultiTermHighlighting.java	(revision 1598989)
+++ lucene/highlighter/src/test/org/apache/lucene/search/postingshighlight/TestMultiTermHighlighting.java	(working copy)
@@ -29,22 +29,10 @@
 import org.apache.lucene.index.RandomIndexWriter;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.index.FieldInfo.IndexOptions;
+import org.apache.lucene.queries.BoostingQuery;
+import org.apache.lucene.queries.CustomScoreQuery;
 import org.apache.lucene.queries.TermFilter;
-import org.apache.lucene.search.BooleanClause;
-import org.apache.lucene.search.BooleanQuery;
-import org.apache.lucene.search.ConstantScoreQuery;
-import org.apache.lucene.search.DisjunctionMaxQuery;
-import org.apache.lucene.search.FilteredQuery;
-import org.apache.lucene.search.FuzzyQuery;
-import org.apache.lucene.search.IndexSearcher;
-import org.apache.lucene.search.MatchAllDocsQuery;
-import org.apache.lucene.search.PrefixQuery;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.RegexpQuery;
-import org.apache.lucene.search.Sort;
-import org.apache.lucene.search.TermRangeQuery;
-import org.apache.lucene.search.TopDocs;
-import org.apache.lucene.search.WildcardQuery;
+import org.apache.lucene.search.*;
 import org.apache.lucene.search.spans.SpanFirstQuery;
 import org.apache.lucene.search.spans.SpanMultiTermQueryWrapper;
 import org.apache.lucene.search.spans.SpanNearQuery;
@@ -55,111 +43,50 @@
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.util.LuceneTestCase;
 
+import java.util.Arrays;
+
 /** 
  * Some tests that override {@link PostingsHighlighter#getIndexAnalyzer} to
- * highlight wilcard, fuzzy, etc queries.
+ * highlight wildcard, fuzzy, etc queries.
  */
 public class TestMultiTermHighlighting extends LuceneTestCase {
-  
+
   public void testWildcards() throws Exception {
     Directory dir = newDirectory();
-    // use simpleanalyzer for more natural tokenization (else "test." is a token)
-    final Analyzer analyzer = new MockAnalyzer(random(), MockTokenizer.SIMPLE, true);
-    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);
-    iwc.setMergePolicy(newLogMergePolicy());
-    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwc);
-    
-    FieldType offsetsType = new FieldType(TextField.TYPE_STORED);
-    offsetsType.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);
-    Field body = new Field("body", "", offsetsType);
-    Document doc = new Document();
-    doc.add(body);
-    
-    body.setStringValue("This is a test.");
-    iw.addDocument(doc);
-    body.setStringValue("Test a one sentence document.");
-    iw.addDocument(doc);
-    
-    IndexReader ir = iw.getReader();
-    iw.shutdown();
-    
+    final Analyzer analyzer = newAnalyzer();
+    IndexReader ir = indexDocument(dir, analyzer);
+
     IndexSearcher searcher = newSearcher(ir);
-    PostingsHighlighter highlighter = new PostingsHighlighter() {
-      @Override
-      protected Analyzer getIndexAnalyzer(String field) {
-        return analyzer;
-      }
-    };
+    PostingsHighlighter highlighter = newPostingsHighlighter(analyzer);
+
     Query query = new WildcardQuery(new Term("body", "te*"));
-    TopDocs topDocs = searcher.search(query, null, 10, Sort.INDEXORDER);
-    assertEquals(2, topDocs.totalHits);
-    String snippets[] = highlighter.highlight("body", query, searcher, topDocs);
-    assertEquals(2, snippets.length);
-    assertEquals("This is a <b>test</b>.", snippets[0]);
-    assertEquals("<b>Test</b> a one sentence document.", snippets[1]);
-    
+    assertHighlight(searcher, query, highlighter);
+
     // wrong field
     BooleanQuery bq = new BooleanQuery();
     bq.add(new MatchAllDocsQuery(), BooleanClause.Occur.SHOULD);
     bq.add(new WildcardQuery(new Term("bogus", "te*")), BooleanClause.Occur.SHOULD);
-    topDocs = searcher.search(bq, null, 10, Sort.INDEXORDER);
-    assertEquals(2, topDocs.totalHits);
-    snippets = highlighter.highlight("body", bq, searcher, topDocs);
-    assertEquals(2, snippets.length);
-    assertEquals("This is a test.", snippets[0]);
-    assertEquals("Test a one sentence document.", snippets[1]);
-    
+    assertNoHighlight(searcher, bq, highlighter);
+
     ir.close();
     dir.close();
   }
   
   public void testOnePrefix() throws Exception {
     Directory dir = newDirectory();
-    // use simpleanalyzer for more natural tokenization (else "test." is a token)
-    final Analyzer analyzer = new MockAnalyzer(random(), MockTokenizer.SIMPLE, true);
-    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);
-    iwc.setMergePolicy(newLogMergePolicy());
-    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwc);
+    final Analyzer analyzer = newAnalyzer();
+    IndexReader ir = indexDocument(dir, analyzer);
     
-    FieldType offsetsType = new FieldType(TextField.TYPE_STORED);
-    offsetsType.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);
-    Field body = new Field("body", "", offsetsType);
-    Document doc = new Document();
-    doc.add(body);
-    
-    body.setStringValue("This is a test.");
-    iw.addDocument(doc);
-    body.setStringValue("Test a one sentence document.");
-    iw.addDocument(doc);
-    
-    IndexReader ir = iw.getReader();
-    iw.shutdown();
-    
     IndexSearcher searcher = newSearcher(ir);
-    PostingsHighlighter highlighter = new PostingsHighlighter() {
-      @Override
-      protected Analyzer getIndexAnalyzer(String field) {
-        return analyzer;
-      }
-    };
+    PostingsHighlighter highlighter = newPostingsHighlighter(analyzer);
     Query query = new PrefixQuery(new Term("body", "te"));
-    TopDocs topDocs = searcher.search(query, null, 10, Sort.INDEXORDER);
-    assertEquals(2, topDocs.totalHits);
-    String snippets[] = highlighter.highlight("body", query, searcher, topDocs);
-    assertEquals(2, snippets.length);
-    assertEquals("This is a <b>test</b>.", snippets[0]);
-    assertEquals("<b>Test</b> a one sentence document.", snippets[1]);
+    assertHighlight(searcher, query, highlighter);
     
     // wrong field
     BooleanQuery bq = new BooleanQuery();
     bq.add(new MatchAllDocsQuery(), BooleanClause.Occur.SHOULD);
     bq.add(new PrefixQuery(new Term("bogus", "te")), BooleanClause.Occur.SHOULD);
-    topDocs = searcher.search(bq, null, 10, Sort.INDEXORDER);
-    assertEquals(2, topDocs.totalHits);
-    snippets = highlighter.highlight("body", bq, searcher, topDocs);
-    assertEquals(2, snippets.length);
-    assertEquals("This is a test.", snippets[0]);
-    assertEquals("Test a one sentence document.", snippets[1]);
+    assertNoHighlight(searcher, bq, highlighter);
     
     ir.close();
     dir.close();
@@ -167,51 +94,19 @@
   
   public void testOneRegexp() throws Exception {
     Directory dir = newDirectory();
-    // use simpleanalyzer for more natural tokenization (else "test." is a token)
-    final Analyzer analyzer = new MockAnalyzer(random(), MockTokenizer.SIMPLE, true);
-    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);
-    iwc.setMergePolicy(newLogMergePolicy());
-    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwc);
+    final Analyzer analyzer = newAnalyzer();
+    IndexReader ir = indexDocument(dir, analyzer);
     
-    FieldType offsetsType = new FieldType(TextField.TYPE_STORED);
-    offsetsType.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);
-    Field body = new Field("body", "", offsetsType);
-    Document doc = new Document();
-    doc.add(body);
-    
-    body.setStringValue("This is a test.");
-    iw.addDocument(doc);
-    body.setStringValue("Test a one sentence document.");
-    iw.addDocument(doc);
-    
-    IndexReader ir = iw.getReader();
-    iw.shutdown();
-    
     IndexSearcher searcher = newSearcher(ir);
-    PostingsHighlighter highlighter = new PostingsHighlighter() {
-      @Override
-      protected Analyzer getIndexAnalyzer(String field) {
-        return analyzer;
-      }
-    };
+    PostingsHighlighter highlighter = newPostingsHighlighter(analyzer);
     Query query = new RegexpQuery(new Term("body", "te.*"));
-    TopDocs topDocs = searcher.search(query, null, 10, Sort.INDEXORDER);
-    assertEquals(2, topDocs.totalHits);
-    String snippets[] = highlighter.highlight("body", query, searcher, topDocs);
-    assertEquals(2, snippets.length);
-    assertEquals("This is a <b>test</b>.", snippets[0]);
-    assertEquals("<b>Test</b> a one sentence document.", snippets[1]);
+    assertHighlight(searcher, query, highlighter);
     
     // wrong field
     BooleanQuery bq = new BooleanQuery();
     bq.add(new MatchAllDocsQuery(), BooleanClause.Occur.SHOULD);
     bq.add(new RegexpQuery(new Term("bogus", "te.*")), BooleanClause.Occur.SHOULD);
-    topDocs = searcher.search(bq, null, 10, Sort.INDEXORDER);
-    assertEquals(2, topDocs.totalHits);
-    snippets = highlighter.highlight("body", bq, searcher, topDocs);
-    assertEquals(2, snippets.length);
-    assertEquals("This is a test.", snippets[0]);
-    assertEquals("Test a one sentence document.", snippets[1]);
+    assertNoHighlight(searcher, bq, highlighter);
     
     ir.close();
     dir.close();
@@ -219,60 +114,23 @@
   
   public void testOneFuzzy() throws Exception {
     Directory dir = newDirectory();
-    // use simpleanalyzer for more natural tokenization (else "test." is a token)
-    final Analyzer analyzer = new MockAnalyzer(random(), MockTokenizer.SIMPLE, true);
-    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);
-    iwc.setMergePolicy(newLogMergePolicy());
-    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwc);
+    final Analyzer analyzer = newAnalyzer();
+    IndexReader ir = indexDocument(dir, analyzer);
     
-    FieldType offsetsType = new FieldType(TextField.TYPE_STORED);
-    offsetsType.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);
-    Field body = new Field("body", "", offsetsType);
-    Document doc = new Document();
-    doc.add(body);
-    
-    body.setStringValue("This is a test.");
-    iw.addDocument(doc);
-    body.setStringValue("Test a one sentence document.");
-    iw.addDocument(doc);
-    
-    IndexReader ir = iw.getReader();
-    iw.shutdown();
-    
     IndexSearcher searcher = newSearcher(ir);
-    PostingsHighlighter highlighter = new PostingsHighlighter() {
-      @Override
-      protected Analyzer getIndexAnalyzer(String field) {
-        return analyzer;
-      }
-    };
+    PostingsHighlighter highlighter = newPostingsHighlighter(analyzer);
     Query query = new FuzzyQuery(new Term("body", "tets"), 1);
-    TopDocs topDocs = searcher.search(query, null, 10, Sort.INDEXORDER);
-    assertEquals(2, topDocs.totalHits);
-    String snippets[] = highlighter.highlight("body", query, searcher, topDocs);
-    assertEquals(2, snippets.length);
-    assertEquals("This is a <b>test</b>.", snippets[0]);
-    assertEquals("<b>Test</b> a one sentence document.", snippets[1]);
+    assertHighlight(searcher, query, highlighter);
     
     // with prefix
     query = new FuzzyQuery(new Term("body", "tets"), 1, 2);
-    topDocs = searcher.search(query, null, 10, Sort.INDEXORDER);
-    assertEquals(2, topDocs.totalHits);
-    snippets = highlighter.highlight("body", query, searcher, topDocs);
-    assertEquals(2, snippets.length);
-    assertEquals("This is a <b>test</b>.", snippets[0]);
-    assertEquals("<b>Test</b> a one sentence document.", snippets[1]);
+    assertHighlight(searcher, query, highlighter);
     
     // wrong field
     BooleanQuery bq = new BooleanQuery();
     bq.add(new MatchAllDocsQuery(), BooleanClause.Occur.SHOULD);
     bq.add(new FuzzyQuery(new Term("bogus", "tets"), 1), BooleanClause.Occur.SHOULD);
-    topDocs = searcher.search(bq, null, 10, Sort.INDEXORDER);
-    assertEquals(2, topDocs.totalHits);
-    snippets = highlighter.highlight("body", bq, searcher, topDocs);
-    assertEquals(2, snippets.length);
-    assertEquals("This is a test.", snippets[0]);
-    assertEquals("Test a one sentence document.", snippets[1]);
+    assertNoHighlight(searcher, bq, highlighter);
     
     ir.close();
     dir.close();
@@ -280,109 +138,53 @@
   
   public void testRanges() throws Exception {
     Directory dir = newDirectory();
-    // use simpleanalyzer for more natural tokenization (else "test." is a token)
-    final Analyzer analyzer = new MockAnalyzer(random(), MockTokenizer.SIMPLE, true);
-    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);
-    iwc.setMergePolicy(newLogMergePolicy());
-    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwc);
+    final Analyzer analyzer = newAnalyzer();
+    IndexReader ir = indexDocument(dir, analyzer);
     
-    FieldType offsetsType = new FieldType(TextField.TYPE_STORED);
-    offsetsType.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);
-    Field body = new Field("body", "", offsetsType);
-    Document doc = new Document();
-    doc.add(body);
-    
-    body.setStringValue("This is a test.");
-    iw.addDocument(doc);
-    body.setStringValue("Test a one sentence document.");
-    iw.addDocument(doc);
-    
-    IndexReader ir = iw.getReader();
-    iw.shutdown();
-    
     IndexSearcher searcher = newSearcher(ir);
-    PostingsHighlighter highlighter = new PostingsHighlighter() {
-      @Override
-      protected Analyzer getIndexAnalyzer(String field) {
-        return analyzer;
-      }
-    };
+    PostingsHighlighter highlighter = newPostingsHighlighter(analyzer);
     Query query = TermRangeQuery.newStringRange("body", "ta", "tf", true, true);
-    TopDocs topDocs = searcher.search(query, null, 10, Sort.INDEXORDER);
-    assertEquals(2, topDocs.totalHits);
-    String snippets[] = highlighter.highlight("body", query, searcher, topDocs);
-    assertEquals(2, snippets.length);
-    assertEquals("This is a <b>test</b>.", snippets[0]);
-    assertEquals("<b>Test</b> a one sentence document.", snippets[1]);
+    assertHighlight(searcher, query, highlighter);
     
     // null start
     query = TermRangeQuery.newStringRange("body", null, "tf", true, true);
-    topDocs = searcher.search(query, null, 10, Sort.INDEXORDER);
-    assertEquals(2, topDocs.totalHits);
-    snippets = highlighter.highlight("body", query, searcher, topDocs);
+    String[] snippets = highlight(searcher, query, highlighter);
     assertEquals(2, snippets.length);
     assertEquals("This <b>is</b> <b>a</b> <b>test</b>.", snippets[0]);
     assertEquals("<b>Test</b> <b>a</b> <b>one</b> <b>sentence</b> <b>document</b>.", snippets[1]);
     
     // null end
     query = TermRangeQuery.newStringRange("body", "ta", null, true, true);
-    topDocs = searcher.search(query, null, 10, Sort.INDEXORDER);
-    assertEquals(2, topDocs.totalHits);
-    snippets = highlighter.highlight("body", query, searcher, topDocs);
+    snippets = highlight(searcher, query, highlighter);
     assertEquals(2, snippets.length);
     assertEquals("<b>This</b> is a <b>test</b>.", snippets[0]);
     assertEquals("<b>Test</b> a one sentence document.", snippets[1]);
     
     // exact start inclusive
     query = TermRangeQuery.newStringRange("body", "test", "tf", true, true);
-    topDocs = searcher.search(query, null, 10, Sort.INDEXORDER);
-    assertEquals(2, topDocs.totalHits);
-    snippets = highlighter.highlight("body", query, searcher, topDocs);
-    assertEquals(2, snippets.length);
-    assertEquals("This is a <b>test</b>.", snippets[0]);
-    assertEquals("<b>Test</b> a one sentence document.", snippets[1]);
+    assertHighlight(searcher, query, highlighter);
     
     // exact end inclusive
     query = TermRangeQuery.newStringRange("body", "ta", "test", true, true);
-    topDocs = searcher.search(query, null, 10, Sort.INDEXORDER);
-    assertEquals(2, topDocs.totalHits);
-    snippets = highlighter.highlight("body", query, searcher, topDocs);
-    assertEquals(2, snippets.length);
-    assertEquals("This is a <b>test</b>.", snippets[0]);
-    assertEquals("<b>Test</b> a one sentence document.", snippets[1]);
+    assertHighlight(searcher, query, highlighter);
     
     // exact start exclusive
     BooleanQuery bq = new BooleanQuery();
     bq.add(new MatchAllDocsQuery(), BooleanClause.Occur.SHOULD);
     bq.add(TermRangeQuery.newStringRange("body", "test", "tf", false, true), BooleanClause.Occur.SHOULD);
-    topDocs = searcher.search(bq, null, 10, Sort.INDEXORDER);
-    assertEquals(2, topDocs.totalHits);
-    snippets = highlighter.highlight("body", bq, searcher, topDocs);
-    assertEquals(2, snippets.length);
-    assertEquals("This is a test.", snippets[0]);
-    assertEquals("Test a one sentence document.", snippets[1]);
+    assertNoHighlight(searcher, bq, highlighter);
     
     // exact end exclusive
     bq = new BooleanQuery();
     bq.add(new MatchAllDocsQuery(), BooleanClause.Occur.SHOULD);
     bq.add(TermRangeQuery.newStringRange("body", "ta", "test", true, false), BooleanClause.Occur.SHOULD);
-    topDocs = searcher.search(bq, null, 10, Sort.INDEXORDER);
-    assertEquals(2, topDocs.totalHits);
-    snippets = highlighter.highlight("body", bq, searcher, topDocs);
-    assertEquals(2, snippets.length);
-    assertEquals("This is a test.", snippets[0]);
-    assertEquals("Test a one sentence document.", snippets[1]);
+    assertNoHighlight(searcher, bq, highlighter);
     
     // wrong field
     bq = new BooleanQuery();
     bq.add(new MatchAllDocsQuery(), BooleanClause.Occur.SHOULD);
     bq.add(TermRangeQuery.newStringRange("bogus", "ta", "tf", true, true), BooleanClause.Occur.SHOULD);
-    topDocs = searcher.search(bq, null, 10, Sort.INDEXORDER);
-    assertEquals(2, topDocs.totalHits);
-    snippets = highlighter.highlight("body", bq, searcher, topDocs);
-    assertEquals(2, snippets.length);
-    assertEquals("This is a test.", snippets[0]);
-    assertEquals("Test a one sentence document.", snippets[1]);
+    assertNoHighlight(searcher, bq, highlighter);
     
     ir.close();
     dir.close();
@@ -390,52 +192,20 @@
   
   public void testWildcardInBoolean() throws Exception {
     Directory dir = newDirectory();
-    // use simpleanalyzer for more natural tokenization (else "test." is a token)
-    final Analyzer analyzer = new MockAnalyzer(random(), MockTokenizer.SIMPLE, true);
-    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);
-    iwc.setMergePolicy(newLogMergePolicy());
-    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwc);
+    final Analyzer analyzer = newAnalyzer();
+    IndexReader ir = indexDocument(dir, analyzer);
     
-    FieldType offsetsType = new FieldType(TextField.TYPE_STORED);
-    offsetsType.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);
-    Field body = new Field("body", "", offsetsType);
-    Document doc = new Document();
-    doc.add(body);
-    
-    body.setStringValue("This is a test.");
-    iw.addDocument(doc);
-    body.setStringValue("Test a one sentence document.");
-    iw.addDocument(doc);
-    
-    IndexReader ir = iw.getReader();
-    iw.shutdown();
-    
     IndexSearcher searcher = newSearcher(ir);
-    PostingsHighlighter highlighter = new PostingsHighlighter() {
-      @Override
-      protected Analyzer getIndexAnalyzer(String field) {
-        return analyzer;
-      }
-    };
+    PostingsHighlighter highlighter = newPostingsHighlighter(analyzer);
     BooleanQuery query = new BooleanQuery();
     query.add(new WildcardQuery(new Term("body", "te*")), BooleanClause.Occur.SHOULD);
-    TopDocs topDocs = searcher.search(query, null, 10, Sort.INDEXORDER);
-    assertEquals(2, topDocs.totalHits);
-    String snippets[] = highlighter.highlight("body", query, searcher, topDocs);
-    assertEquals(2, snippets.length);
-    assertEquals("This is a <b>test</b>.", snippets[0]);
-    assertEquals("<b>Test</b> a one sentence document.", snippets[1]);
+    assertHighlight(searcher, query, highlighter);
     
     // must not
     query = new BooleanQuery();
     query.add(new MatchAllDocsQuery(), BooleanClause.Occur.SHOULD);
     query.add(new WildcardQuery(new Term("bogus", "te*")), BooleanClause.Occur.MUST_NOT);
-    topDocs = searcher.search(query, null, 10, Sort.INDEXORDER);
-    assertEquals(2, topDocs.totalHits);
-    snippets = highlighter.highlight("body", query, searcher, topDocs);
-    assertEquals(2, snippets.length);
-    assertEquals("This is a test.", snippets[0]);
-    assertEquals("Test a one sentence document.", snippets[1]);
+    assertNoHighlight(searcher, query, highlighter);
     
     ir.close();
     dir.close();
@@ -443,42 +213,15 @@
 
   public void testWildcardInFiltered() throws Exception {
     Directory dir = newDirectory();
-    // use simpleanalyzer for more natural tokenization (else "test." is a token)
-    final Analyzer analyzer = new MockAnalyzer(random(), MockTokenizer.SIMPLE, true);
-    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);
-    iwc.setMergePolicy(newLogMergePolicy());
-    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwc);
+    final Analyzer analyzer = newAnalyzer();
+    IndexReader ir = indexDocument(dir, analyzer);
 
-    FieldType offsetsType = new FieldType(TextField.TYPE_STORED);
-    offsetsType.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);
-    Field body = new Field("body", "", offsetsType);
-    Document doc = new Document();
-    doc.add(body);
-
-    body.setStringValue("This is a test.");
-    iw.addDocument(doc);
-    body.setStringValue("Test a one sentence document.");
-    iw.addDocument(doc);
-
-    IndexReader ir = iw.getReader();
-    iw.shutdown();
-
     IndexSearcher searcher = newSearcher(ir);
-    PostingsHighlighter highlighter = new PostingsHighlighter() {
-      @Override
-      protected Analyzer getIndexAnalyzer(String field) {
-        return analyzer;
-      }
-    };
+    PostingsHighlighter highlighter = newPostingsHighlighter(analyzer);
     FilteredQuery query = new FilteredQuery(
         new WildcardQuery(new Term("body", "te*")),
         new TermFilter(new Term("body", "test")));
-    TopDocs topDocs = searcher.search(query, null, 10, Sort.INDEXORDER);
-    assertEquals(2, topDocs.totalHits);
-    String snippets[] = highlighter.highlight("body", query, searcher, topDocs);
-    assertEquals(2, snippets.length);
-    assertEquals("This is a <b>test</b>.", snippets[0]);
-    assertEquals("<b>Test</b> a one sentence document.", snippets[1]);
+    assertHighlight(searcher, query, highlighter);
 
     ir.close();
     dir.close();
@@ -486,40 +229,13 @@
 
   public void testWildcardInConstantScore() throws Exception {
     Directory dir = newDirectory();
-    // use simpleanalyzer for more natural tokenization (else "test." is a token)
-    final Analyzer analyzer = new MockAnalyzer(random(), MockTokenizer.SIMPLE, true);
-    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);
-    iwc.setMergePolicy(newLogMergePolicy());
-    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwc);
+    final Analyzer analyzer = newAnalyzer();
+    IndexReader ir = indexDocument(dir, analyzer);
 
-    FieldType offsetsType = new FieldType(TextField.TYPE_STORED);
-    offsetsType.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);
-    Field body = new Field("body", "", offsetsType);
-    Document doc = new Document();
-    doc.add(body);
-
-    body.setStringValue("This is a test.");
-    iw.addDocument(doc);
-    body.setStringValue("Test a one sentence document.");
-    iw.addDocument(doc);
-
-    IndexReader ir = iw.getReader();
-    iw.shutdown();
-
     IndexSearcher searcher = newSearcher(ir);
-    PostingsHighlighter highlighter = new PostingsHighlighter() {
-      @Override
-      protected Analyzer getIndexAnalyzer(String field) {
-        return analyzer;
-      }
-    };
+    PostingsHighlighter highlighter = newPostingsHighlighter(analyzer);
     ConstantScoreQuery query = new ConstantScoreQuery(new WildcardQuery(new Term("body", "te*")));
-    TopDocs topDocs = searcher.search(query, null, 10, Sort.INDEXORDER);
-    assertEquals(2, topDocs.totalHits);
-    String snippets[] = highlighter.highlight("body", query, searcher, topDocs);
-    assertEquals(2, snippets.length);
-    assertEquals("This is a <b>test</b>.", snippets[0]);
-    assertEquals("<b>Test</b> a one sentence document.", snippets[1]);
+    assertHighlight(searcher, query, highlighter);
 
     ir.close();
     dir.close();
@@ -527,41 +243,14 @@
   
   public void testWildcardInDisjunctionMax() throws Exception {
     Directory dir = newDirectory();
-    // use simpleanalyzer for more natural tokenization (else "test." is a token)
-    final Analyzer analyzer = new MockAnalyzer(random(), MockTokenizer.SIMPLE, true);
-    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);
-    iwc.setMergePolicy(newLogMergePolicy());
-    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwc);
+    final Analyzer analyzer = newAnalyzer();
+    IndexReader ir = indexDocument(dir, analyzer);
     
-    FieldType offsetsType = new FieldType(TextField.TYPE_STORED);
-    offsetsType.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);
-    Field body = new Field("body", "", offsetsType);
-    Document doc = new Document();
-    doc.add(body);
-    
-    body.setStringValue("This is a test.");
-    iw.addDocument(doc);
-    body.setStringValue("Test a one sentence document.");
-    iw.addDocument(doc);
-    
-    IndexReader ir = iw.getReader();
-    iw.shutdown();
-    
     IndexSearcher searcher = newSearcher(ir);
-    PostingsHighlighter highlighter = new PostingsHighlighter() {
-      @Override
-      protected Analyzer getIndexAnalyzer(String field) {
-        return analyzer;
-      }
-    };
+    PostingsHighlighter highlighter = newPostingsHighlighter(analyzer);
     DisjunctionMaxQuery query = new DisjunctionMaxQuery(0);
     query.add(new WildcardQuery(new Term("body", "te*")));
-    TopDocs topDocs = searcher.search(query, null, 10, Sort.INDEXORDER);
-    assertEquals(2, topDocs.totalHits);
-    String snippets[] = highlighter.highlight("body", query, searcher, topDocs);
-    assertEquals(2, snippets.length);
-    assertEquals("This is a <b>test</b>.", snippets[0]);
-    assertEquals("<b>Test</b> a one sentence document.", snippets[1]);
+    assertHighlight(searcher, query, highlighter);
     
     ir.close();
     dir.close();
@@ -569,40 +258,14 @@
   
   public void testSpanWildcard() throws Exception {
     Directory dir = newDirectory();
-    // use simpleanalyzer for more natural tokenization (else "test." is a token)
-    final Analyzer analyzer = new MockAnalyzer(random(), MockTokenizer.SIMPLE, true);
-    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);
-    iwc.setMergePolicy(newLogMergePolicy());
-    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwc);
+    final Analyzer analyzer = newAnalyzer();
+    IndexReader ir = indexDocument(dir, analyzer);
     
-    FieldType offsetsType = new FieldType(TextField.TYPE_STORED);
-    offsetsType.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);
-    Field body = new Field("body", "", offsetsType);
-    Document doc = new Document();
-    doc.add(body);
-    
-    body.setStringValue("This is a test.");
-    iw.addDocument(doc);
-    body.setStringValue("Test a one sentence document.");
-    iw.addDocument(doc);
-    
-    IndexReader ir = iw.getReader();
-    iw.shutdown();
-    
     IndexSearcher searcher = newSearcher(ir);
-    PostingsHighlighter highlighter = new PostingsHighlighter() {
-      @Override
-      protected Analyzer getIndexAnalyzer(String field) {
-        return analyzer;
-      }
-    };
+    PostingsHighlighter highlighter = newPostingsHighlighter(analyzer);
     Query query = new SpanMultiTermQueryWrapper<>(new WildcardQuery(new Term("body", "te*")));
     TopDocs topDocs = searcher.search(query, null, 10, Sort.INDEXORDER);
-    assertEquals(2, topDocs.totalHits);
-    String snippets[] = highlighter.highlight("body", query, searcher, topDocs);
-    assertEquals(2, snippets.length);
-    assertEquals("This is a <b>test</b>.", snippets[0]);
-    assertEquals("<b>Test</b> a one sentence document.", snippets[1]);
+    assertHighlight(searcher, query, highlighter);
     
     ir.close();
     dir.close();
@@ -610,41 +273,14 @@
   
   public void testSpanOr() throws Exception {
     Directory dir = newDirectory();
-    // use simpleanalyzer for more natural tokenization (else "test." is a token)
-    final Analyzer analyzer = new MockAnalyzer(random(), MockTokenizer.SIMPLE, true);
-    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);
-    iwc.setMergePolicy(newLogMergePolicy());
-    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwc);
+    final Analyzer analyzer = newAnalyzer();
+    IndexReader ir = indexDocument(dir, analyzer);
     
-    FieldType offsetsType = new FieldType(TextField.TYPE_STORED);
-    offsetsType.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);
-    Field body = new Field("body", "", offsetsType);
-    Document doc = new Document();
-    doc.add(body);
-    
-    body.setStringValue("This is a test.");
-    iw.addDocument(doc);
-    body.setStringValue("Test a one sentence document.");
-    iw.addDocument(doc);
-    
-    IndexReader ir = iw.getReader();
-    iw.shutdown();
-    
     IndexSearcher searcher = newSearcher(ir);
-    PostingsHighlighter highlighter = new PostingsHighlighter() {
-      @Override
-      protected Analyzer getIndexAnalyzer(String field) {
-        return analyzer;
-      }
-    };
+    PostingsHighlighter highlighter = newPostingsHighlighter(analyzer);
     SpanQuery childQuery = new SpanMultiTermQueryWrapper<>(new WildcardQuery(new Term("body", "te*")));
-    Query query = new SpanOrQuery(new SpanQuery[] { childQuery });
-    TopDocs topDocs = searcher.search(query, null, 10, Sort.INDEXORDER);
-    assertEquals(2, topDocs.totalHits);
-    String snippets[] = highlighter.highlight("body", query, searcher, topDocs);
-    assertEquals(2, snippets.length);
-    assertEquals("This is a <b>test</b>.", snippets[0]);
-    assertEquals("<b>Test</b> a one sentence document.", snippets[1]);
+    Query query = new SpanOrQuery(childQuery);
+    assertHighlight(searcher, query, highlighter);
     
     ir.close();
     dir.close();
@@ -652,41 +288,14 @@
   
   public void testSpanNear() throws Exception {
     Directory dir = newDirectory();
-    // use simpleanalyzer for more natural tokenization (else "test." is a token)
-    final Analyzer analyzer = new MockAnalyzer(random(), MockTokenizer.SIMPLE, true);
-    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);
-    iwc.setMergePolicy(newLogMergePolicy());
-    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwc);
+    final Analyzer analyzer = newAnalyzer();
+    IndexReader ir = indexDocument(dir, analyzer);
     
-    FieldType offsetsType = new FieldType(TextField.TYPE_STORED);
-    offsetsType.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);
-    Field body = new Field("body", "", offsetsType);
-    Document doc = new Document();
-    doc.add(body);
-    
-    body.setStringValue("This is a test.");
-    iw.addDocument(doc);
-    body.setStringValue("Test a one sentence document.");
-    iw.addDocument(doc);
-    
-    IndexReader ir = iw.getReader();
-    iw.shutdown();
-    
     IndexSearcher searcher = newSearcher(ir);
-    PostingsHighlighter highlighter = new PostingsHighlighter() {
-      @Override
-      protected Analyzer getIndexAnalyzer(String field) {
-        return analyzer;
-      }
-    };
+    PostingsHighlighter highlighter = newPostingsHighlighter(analyzer);
     SpanQuery childQuery = new SpanMultiTermQueryWrapper<>(new WildcardQuery(new Term("body", "te*")));
     Query query = new SpanNearQuery(new SpanQuery[] { childQuery }, 0, true);
-    TopDocs topDocs = searcher.search(query, null, 10, Sort.INDEXORDER);
-    assertEquals(2, topDocs.totalHits);
-    String snippets[] = highlighter.highlight("body", query, searcher, topDocs);
-    assertEquals(2, snippets.length);
-    assertEquals("This is a <b>test</b>.", snippets[0]);
-    assertEquals("<b>Test</b> a one sentence document.", snippets[1]);
+    assertHighlight(searcher, query, highlighter);
     
     ir.close();
     dir.close();
@@ -694,42 +303,15 @@
   
   public void testSpanNot() throws Exception {
     Directory dir = newDirectory();
-    // use simpleanalyzer for more natural tokenization (else "test." is a token)
-    final Analyzer analyzer = new MockAnalyzer(random(), MockTokenizer.SIMPLE, true);
-    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);
-    iwc.setMergePolicy(newLogMergePolicy());
-    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwc);
+    final Analyzer analyzer = newAnalyzer();
+    IndexReader ir = indexDocument(dir, analyzer);
     
-    FieldType offsetsType = new FieldType(TextField.TYPE_STORED);
-    offsetsType.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);
-    Field body = new Field("body", "", offsetsType);
-    Document doc = new Document();
-    doc.add(body);
-    
-    body.setStringValue("This is a test.");
-    iw.addDocument(doc);
-    body.setStringValue("Test a one sentence document.");
-    iw.addDocument(doc);
-    
-    IndexReader ir = iw.getReader();
-    iw.shutdown();
-    
     IndexSearcher searcher = newSearcher(ir);
-    PostingsHighlighter highlighter = new PostingsHighlighter() {
-      @Override
-      protected Analyzer getIndexAnalyzer(String field) {
-        return analyzer;
-      }
-    };
+    PostingsHighlighter highlighter = newPostingsHighlighter(analyzer);
     SpanQuery include = new SpanMultiTermQueryWrapper<>(new WildcardQuery(new Term("body", "te*")));
     SpanQuery exclude = new SpanTermQuery(new Term("body", "bogus"));
     Query query = new SpanNotQuery(include, exclude);
-    TopDocs topDocs = searcher.search(query, null, 10, Sort.INDEXORDER);
-    assertEquals(2, topDocs.totalHits);
-    String snippets[] = highlighter.highlight("body", query, searcher, topDocs);
-    assertEquals(2, snippets.length);
-    assertEquals("This is a <b>test</b>.", snippets[0]);
-    assertEquals("<b>Test</b> a one sentence document.", snippets[1]);
+    assertHighlight(searcher, query, highlighter);
     
     ir.close();
     dir.close();
@@ -737,86 +319,86 @@
   
   public void testSpanPositionCheck() throws Exception {
     Directory dir = newDirectory();
-    // use simpleanalyzer for more natural tokenization (else "test." is a token)
-    final Analyzer analyzer = new MockAnalyzer(random(), MockTokenizer.SIMPLE, true);
-    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);
-    iwc.setMergePolicy(newLogMergePolicy());
-    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwc);
+    final Analyzer analyzer = newAnalyzer();
+    IndexReader ir = indexDocument(dir, analyzer);
     
-    FieldType offsetsType = new FieldType(TextField.TYPE_STORED);
-    offsetsType.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);
-    Field body = new Field("body", "", offsetsType);
-    Document doc = new Document();
-    doc.add(body);
-    
-    body.setStringValue("This is a test.");
-    iw.addDocument(doc);
-    body.setStringValue("Test a one sentence document.");
-    iw.addDocument(doc);
-    
-    IndexReader ir = iw.getReader();
-    iw.shutdown();
-    
     IndexSearcher searcher = newSearcher(ir);
-    PostingsHighlighter highlighter = new PostingsHighlighter() {
-      @Override
-      protected Analyzer getIndexAnalyzer(String field) {
-        return analyzer;
-      }
-    };
+    PostingsHighlighter highlighter = newPostingsHighlighter(analyzer);
     SpanQuery childQuery = new SpanMultiTermQueryWrapper<>(new WildcardQuery(new Term("body", "te*")));
     Query query = new SpanFirstQuery(childQuery, 1000000);
-    TopDocs topDocs = searcher.search(query, null, 10, Sort.INDEXORDER);
-    assertEquals(2, topDocs.totalHits);
-    String snippets[] = highlighter.highlight("body", query, searcher, topDocs);
+    assertHighlight(searcher, query, highlighter);
+    
+    ir.close();
+    dir.close();
+  }
+
+  public void testSpanMultiTermQueryWrapper() throws Exception {
+    Directory dir = newDirectory();
+    final Analyzer analyzer = newAnalyzer();
+    IndexReader ir = indexDocument(dir, analyzer);
+
+    IndexSearcher searcher = newSearcher(ir);
+    PostingsHighlighter highlighter = newPostingsHighlighter(analyzer);
+    SpanQuery query = new SpanMultiTermQueryWrapper<>(new WildcardQuery(new Term("body", "te*")));
+    assertHighlight(searcher, query, highlighter);
+
+    ir.close();
+    dir.close();
+  }
+
+  public void testCustomScoreQuery() throws Exception {
+    Directory dir = newDirectory();
+    final Analyzer analyzer = newAnalyzer();
+    IndexReader ir = indexDocument(dir, analyzer);
+
+    IndexSearcher searcher = newSearcher(ir);
+    PostingsHighlighter highlighter = newPostingsHighlighter(analyzer);
+    CustomScoreQuery query = new CustomScoreQuery(new WildcardQuery(new Term("body", "te*")));
+    assertHighlight(searcher, query, highlighter);
+
+    ir.close();
+    dir.close();
+  }
+
+  public void testBoostingQuery() throws Exception {
+    Directory dir = newDirectory();
+    final Analyzer analyzer = newAnalyzer();
+    IndexReader ir = indexDocument(dir, analyzer);
+
+    IndexSearcher searcher = newSearcher(ir);
+    PostingsHighlighter highlighter = newPostingsHighlighter(analyzer);
+    BoostingQuery query = new BoostingQuery(new WildcardQuery(new Term("body", "te*")), new TermQuery(new Term("body", "not here")), 2f);
+    assertHighlight(searcher, query, highlighter);
+
+    query = new BoostingQuery(new WildcardQuery(new Term("body", "te*")), new WildcardQuery(new Term("body", "th*")), 2f);
+    String[] snippets = highlight(searcher, query, highlighter);
     assertEquals(2, snippets.length);
-    assertEquals("This is a <b>test</b>.", snippets[0]);
+    assertEquals("<b>This</b> is a <b>test</b>.", snippets[0]);
     assertEquals("<b>Test</b> a one sentence document.", snippets[1]);
-    
+
     ir.close();
     dir.close();
   }
-  
+
   /** Runs a query with two MTQs and confirms the formatter
    *  can tell which query matched which hit. */
   public void testWhichMTQMatched() throws Exception {
     Directory dir = newDirectory();
-    // use simpleanalyzer for more natural tokenization (else "test." is a token)
-    final Analyzer analyzer = new MockAnalyzer(random(), MockTokenizer.SIMPLE, true);
-    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);
-    iwc.setMergePolicy(newLogMergePolicy());
-    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwc);
+    final Analyzer analyzer = newAnalyzer();
+    IndexReader ir = indexDocument(dir, analyzer);
     
-    FieldType offsetsType = new FieldType(TextField.TYPE_STORED);
-    offsetsType.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);
-    Field body = new Field("body", "", offsetsType);
-    Document doc = new Document();
-    doc.add(body);
-    
-    body.setStringValue("Test a one sentence document.");
-    iw.addDocument(doc);
-    
-    IndexReader ir = iw.getReader();
-    iw.shutdown();
-    
     IndexSearcher searcher = newSearcher(ir);
-    PostingsHighlighter highlighter = new PostingsHighlighter() {
-      @Override
-      protected Analyzer getIndexAnalyzer(String field) {
-        return analyzer;
-      }
-    };
+    PostingsHighlighter highlighter = newPostingsHighlighter(analyzer);
     BooleanQuery query = new BooleanQuery();
     query.add(new WildcardQuery(new Term("body", "te*")), BooleanClause.Occur.SHOULD);
     query.add(new WildcardQuery(new Term("body", "one")), BooleanClause.Occur.SHOULD);
     query.add(new WildcardQuery(new Term("body", "se*")), BooleanClause.Occur.SHOULD);
-    TopDocs topDocs = searcher.search(query, null, 10, Sort.INDEXORDER);
-    assertEquals(1, topDocs.totalHits);
-    String snippets[] = highlighter.highlight("body", query, searcher, topDocs);
-    assertEquals(1, snippets.length);
-    
+    String[] snippets = highlight(searcher, query, highlighter);
+
     // Default formatter just bolds each hit:
-    assertEquals("<b>Test</b> a <b>one</b> <b>sentence</b> document.", snippets[0]);
+    assertEquals(2, snippets.length);
+    assertEquals("This is a <b>test</b>.", snippets[0]);
+    assertEquals("<b>Test</b> a <b>one</b> <b>sentence</b> document.", snippets[1]);
     
     // Now use our own formatter, that also stuffs the
     // matching term's text into the result:
@@ -868,15 +450,69 @@
         };
       }
     };
-    
-    assertEquals(1, topDocs.totalHits);
-    snippets = highlighter.highlight("body", query, searcher, topDocs);
-    assertEquals(1, snippets.length);
-    
+
+    snippets = highlight(searcher, query, highlighter);
     // Default formatter bolds each hit:
-    assertEquals("<b>Test(body:te*)</b> a <b>one(body:one)</b> <b>sentence(body:se*)</b> document.", snippets[0]);
+    assertEquals(2, snippets.length);
+    assertEquals("This is a <b>test(body:te*)</b>.", snippets[0]);
+    assertEquals("<b>Test(body:te*)</b> a <b>one(body:one)</b> <b>sentence(body:se*)</b> document.", snippets[1]);
     
     ir.close();
     dir.close();
   }
+
+  private static Analyzer newAnalyzer() {
+    // use simpleanalyzer for more natural tokenization (else "test." is a token)
+    return new MockAnalyzer(random(), MockTokenizer.SIMPLE, true);
+  }
+
+  private static IndexReader indexDocument(Directory dir, Analyzer analyzer) throws Exception {
+    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);
+    iwc.setMergePolicy(newLogMergePolicy());
+    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwc);
+
+    FieldType offsetsType = new FieldType(TextField.TYPE_STORED);
+    offsetsType.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);
+    Field body = new Field("body", "", offsetsType);
+    Document doc = new Document();
+    doc.add(body);
+
+    body.setStringValue("This is a test.");
+    iw.addDocument(doc);
+    body.setStringValue("Test a one sentence document.");
+    iw.addDocument(doc);
+
+    IndexReader ir = iw.getReader();
+    iw.shutdown();
+    return ir;
+  }
+
+  private static PostingsHighlighter newPostingsHighlighter(final Analyzer analyzer) {
+    return new PostingsHighlighter() {
+      @Override
+      protected Analyzer getIndexAnalyzer(String field) {
+        return analyzer;
+      }
+    };
+  }
+
+  private static void assertHighlight(IndexSearcher searcher, Query query, PostingsHighlighter highlighter) throws Exception {
+    String[] snippets = highlight(searcher, query, highlighter);
+    assertEquals(2, snippets.length);
+    assertEquals("This is a <b>test</b>.", snippets[0]);
+    assertEquals("<b>Test</b> a one sentence document.", snippets[1]);
+  }
+
+  private static void assertNoHighlight(IndexSearcher searcher, Query query, PostingsHighlighter highlighter) throws Exception {
+    String[] snippets = highlight(searcher, query, highlighter);
+    assertEquals(2, snippets.length);
+    assertEquals("This is a test.", snippets[0]);
+    assertEquals("Test a one sentence document.", snippets[1]);
+  }
+
+  private static String[] highlight(IndexSearcher searcher, Query query, PostingsHighlighter highlighter) throws Exception {
+    TopDocs topDocs = searcher.search(query, null, 10, Sort.INDEXORDER);
+    assertEquals(2, topDocs.totalHits);
+    return highlighter.highlight("body", query, searcher, topDocs);
+  }
 }
Index: lucene/join/src/java/org/apache/lucene/search/join/TermsIncludingScoreQuery.java
===================================================================
--- lucene/join/src/java/org/apache/lucene/search/join/TermsIncludingScoreQuery.java	(revision 1598989)
+++ lucene/join/src/java/org/apache/lucene/search/join/TermsIncludingScoreQuery.java	(working copy)
@@ -18,6 +18,7 @@
  */
 
 import java.io.IOException;
+import java.util.Collection;
 import java.util.Locale;
 import java.util.Set;
 
@@ -28,7 +29,6 @@
 import org.apache.lucene.index.Terms;
 import org.apache.lucene.index.TermsEnum;
 import org.apache.lucene.search.LeafCollector;
-import org.apache.lucene.search.Collector;
 import org.apache.lucene.search.ComplexExplanation;
 import org.apache.lucene.search.DocIdSetIterator;
 import org.apache.lucene.search.Explanation;
@@ -83,6 +83,11 @@
   }
 
   @Override
+  public void extractQueries(Collection<Query> queries) {
+    originalQuery.extractQueries(queries);
+  }
+
+  @Override
   public Query rewrite(IndexReader reader) throws IOException {
     final Query originalQueryRewrite = originalQuery.rewrite(reader);
     if (originalQueryRewrite != originalQuery) {
Index: lucene/join/src/java/org/apache/lucene/search/join/ToChildBlockJoinQuery.java
===================================================================
--- lucene/join/src/java/org/apache/lucene/search/join/ToChildBlockJoinQuery.java	(revision 1598989)
+++ lucene/join/src/java/org/apache/lucene/search/join/ToChildBlockJoinQuery.java	(working copy)
@@ -334,6 +334,11 @@
   }
 
   @Override
+  public void extractQueries(Collection<Query> queries) {
+    parentQuery.extractQueries(queries);
+  }
+
+  @Override
   public Query rewrite(IndexReader reader) throws IOException {
     final Query parentRewrite = parentQuery.rewrite(reader);
     if (parentRewrite != parentQuery) {
Index: lucene/join/src/java/org/apache/lucene/search/join/ToParentBlockJoinQuery.java
===================================================================
--- lucene/join/src/java/org/apache/lucene/search/join/ToParentBlockJoinQuery.java	(revision 1598989)
+++ lucene/join/src/java/org/apache/lucene/search/join/ToParentBlockJoinQuery.java	(working copy)
@@ -447,6 +447,11 @@
   }
 
   @Override
+  public void extractQueries(Collection<Query> queries) {
+    childQuery.extractQueries(queries);
+  }
+
+  @Override
   public Query rewrite(IndexReader reader) throws IOException {
     final Query childRewrite = childQuery.rewrite(reader);
     if (childRewrite != childQuery) {
Index: lucene/queries/src/java/org/apache/lucene/queries/BoostingQuery.java
===================================================================
--- lucene/queries/src/java/org/apache/lucene/queries/BoostingQuery.java	(revision 1598989)
+++ lucene/queries/src/java/org/apache/lucene/queries/BoostingQuery.java	(working copy)
@@ -18,6 +18,7 @@
  */
 
 import java.io.IOException;
+import java.util.Collection;
 
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.search.*;
@@ -81,7 +82,13 @@
       return result;
     }
 
-    @Override
+  @Override
+  public void extractQueries(Collection<Query> queries) {
+    match.extractQueries(queries);
+    context.extractQueries(queries);
+  }
+
+  @Override
     public int hashCode() {
       final int prime = 31;
       int result = super.hashCode();
Index: lucene/queries/src/java/org/apache/lucene/queries/CustomScoreQuery.java
===================================================================
--- lucene/queries/src/java/org/apache/lucene/queries/CustomScoreQuery.java	(revision 1598989)
+++ lucene/queries/src/java/org/apache/lucene/queries/CustomScoreQuery.java	(working copy)
@@ -116,6 +116,14 @@
     }
   }
 
+  @Override
+  public void extractQueries(Collection<Query> queries) {
+    subQuery.extractQueries(queries);
+    for (Query scoringQuery : scoringQueries) {
+      scoringQuery.extractQueries(queries);
+    }
+  }
+
   /*(non-Javadoc) @see org.apache.lucene.search.Query#clone() */
   @Override
   public CustomScoreQuery clone() {
Index: lucene/queries/src/java/org/apache/lucene/queries/function/BoostedQuery.java
===================================================================
--- lucene/queries/src/java/org/apache/lucene/queries/function/BoostedQuery.java	(revision 1598989)
+++ lucene/queries/src/java/org/apache/lucene/queries/function/BoostedQuery.java	(working copy)
@@ -62,6 +62,11 @@
   }
 
   @Override
+  public void extractQueries(Collection<Query> queries) {
+    q.extractQueries(queries);
+  }
+
+  @Override
   public Weight createWeight(IndexSearcher searcher) throws IOException {
     return new BoostedQuery.BoostedWeight(searcher);
   }
Index: lucene/test-framework/src/java/org/apache/lucene/search/AssertingQuery.java
===================================================================
--- lucene/test-framework/src/java/org/apache/lucene/search/AssertingQuery.java	(revision 1598989)
+++ lucene/test-framework/src/java/org/apache/lucene/search/AssertingQuery.java	(working copy)
@@ -18,6 +18,7 @@
  */
 
 import java.io.IOException;
+import java.util.Collection;
 import java.util.Random;
 import java.util.Set;
 
@@ -52,6 +53,11 @@
   }
 
   @Override
+  public void extractQueries(Collection<Query> queries) {
+    in.extractQueries(queries);
+  }
+
+  @Override
   public String toString(String field) {
     return in.toString(field);
   }
Index: solr/core/src/java/org/apache/solr/search/WrappedQuery.java
===================================================================
--- solr/core/src/java/org/apache/solr/search/WrappedQuery.java	(revision 1598989)
+++ solr/core/src/java/org/apache/solr/search/WrappedQuery.java	(working copy)
@@ -24,6 +24,7 @@
 import org.apache.lucene.search.Weight;
 
 import java.io.IOException;
+import java.util.Collection;
 import java.util.Set;
 
 /** A simple query that wraps another query and implements ExtendedQuery. */
@@ -69,6 +70,11 @@
   }
 
   @Override
+  public void extractQueries(Collection<Query> queries) {
+    q.extractQueries(queries);
+  }
+
+  @Override
   public WrappedQuery clone() {
     WrappedQuery newQ = (WrappedQuery)super.clone();
     newQ.q = (Query) q.clone();
Index: solr/core/src/java/org/apache/solr/search/join/IgnoreAcceptDocsQuery.java
===================================================================
--- solr/core/src/java/org/apache/solr/search/join/IgnoreAcceptDocsQuery.java	(revision 1598989)
+++ solr/core/src/java/org/apache/solr/search/join/IgnoreAcceptDocsQuery.java	(working copy)
@@ -28,6 +28,7 @@
 import org.apache.lucene.util.Bits;
 
 import java.io.IOException;
+import java.util.Collection;
 import java.util.Set;
 
 public class IgnoreAcceptDocsQuery extends Query {
@@ -104,6 +105,11 @@
   }
 
   @Override
+  public void extractQueries(Collection<Query> queries) {
+    q.extractQueries(queries);
+  }
+
+  @Override
   public int hashCode() {
     return q.hashCode()*31;
   }
