diff --git a/lucene/core/src/java/org/apache/lucene/codecs/StoredFieldsWriter.java b/lucene/core/src/java/org/apache/lucene/codecs/StoredFieldsWriter.java
index 39ade4242b..e7de14e05b 100644
--- a/lucene/core/src/java/org/apache/lucene/codecs/StoredFieldsWriter.java
+++ b/lucene/core/src/java/org/apache/lucene/codecs/StoredFieldsWriter.java
@@ -125,7 +125,7 @@ public abstract class StoredFieldsWriter implements Closeable {
       if (sub == null) {
         break;
       }
-      assert sub.mappedDocID == docCount;
+      assert sub.mappedDocID == docCount : sub.mappedDocID + " != " + docCount;
       startDocument();
       sub.reader.visitDocument(sub.docID, sub.visitor);
       finishDocument();
diff --git a/lucene/core/src/java/org/apache/lucene/index/FrozenBufferedUpdates.java b/lucene/core/src/java/org/apache/lucene/index/FrozenBufferedUpdates.java
index 202bf2cc49..ed01c1c4cf 100644
--- a/lucene/core/src/java/org/apache/lucene/index/FrozenBufferedUpdates.java
+++ b/lucene/core/src/java/org/apache/lucene/index/FrozenBufferedUpdates.java
@@ -412,7 +412,7 @@ class FrozenBufferedUpdates {
         writer.checkpoint();
       }
 
-      if (writer.keepFullyDeletedSegments == false && result.allDeleted != null) {
+      if (result.allDeleted != null) {
         if (infoStream.isEnabled("IW")) {
           infoStream.message("IW", "drop 100% deleted segments: " + writer.segString(result.allDeleted));
         }
diff --git a/lucene/core/src/java/org/apache/lucene/index/IndexWriter.java b/lucene/core/src/java/org/apache/lucene/index/IndexWriter.java
index dcc3be5d07..153c788397 100644
--- a/lucene/core/src/java/org/apache/lucene/index/IndexWriter.java
+++ b/lucene/core/src/java/org/apache/lucene/index/IndexWriter.java
@@ -46,6 +46,7 @@ import org.apache.lucene.index.DocValuesUpdate.BinaryDocValuesUpdate;
 import org.apache.lucene.index.DocValuesUpdate.NumericDocValuesUpdate;
 import org.apache.lucene.index.FieldInfos.FieldNumbers;
 import org.apache.lucene.index.IndexWriterConfig.OpenMode;
+import org.apache.lucene.search.DocIdSetIterator;
 import org.apache.lucene.search.MatchAllDocsQuery;
 import org.apache.lucene.search.Query;
 import org.apache.lucene.search.Sort;
@@ -881,6 +882,27 @@ public class IndexWriter implements Closeable, TwoPhaseCommit, Accountable {
     return delCount;
   }
 
+  /**
+   * Returns true if the pooled reader for the segmentInfo has one or more persistent deletes.
+   */
+  final boolean hasPersistentDeletedDocs(SegmentCommitInfo info) throws IOException {
+    ensureOpen(false);
+    if (config.getPersistentDeletesProducer() != null) {
+      final ReadersAndUpdates rld = readerPool.get(info, true);
+      try {
+        final SegmentReader reader = rld.getReader(IOContext.READ);
+        try {
+          return hasPersistentDeletes(reader, rld.getLiveDocs());
+        } finally {
+          rld.release(reader);
+        }
+      } finally {
+        readerPool.release(rld);
+      }
+    }
+    return false;
+  }
+
   /**
    * Used internally to throw an {@link AlreadyClosedException} if this
    * IndexWriter has been closed or is in the process of closing.
@@ -1612,6 +1634,26 @@ public class IndexWriter implements Closeable, TwoPhaseCommit, Accountable {
     return -1;
   }
 
+  /**
+   * Returns true iff the given reader has live docs and at least one the deleted docs in the reader should be persistent
+   */
+  boolean hasPersistentDeletes(CodecReader reader, Bits liveDocs) throws IOException {
+    LiveIndexWriterConfig.PersistentDeletesProducer persistentDeletesProducer = config.getPersistentDeletesProducer();
+    if (liveDocs != null && persistentDeletesProducer != null) {
+      DocIdSetIterator persistentDeletes = persistentDeletesProducer.get(reader);
+      if (persistentDeletes != null) {
+        // just check if there is one document, in that case the reader is not fully deleted
+        int doc = -1;
+        while ((doc = persistentDeletes.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {
+          if (liveDocs.get(doc) == false) {
+            return true;
+          }
+        }
+      }
+    }
+    return false;
+  }
+
   /** Drops a segment that has 100% deleted documents. */
   synchronized void dropDeletedSegment(SegmentCommitInfo info) throws IOException {
     // If a merge has already registered for this
@@ -1619,6 +1661,30 @@ public class IndexWriter implements Closeable, TwoPhaseCommit, Accountable {
     // merge will skip merging it and will then drop
     // it once it's done:
     if (mergingSegments.contains(info) == false) {
+      LiveIndexWriterConfig.PersistentDeletesProducer persistentDeletesProducer = config.getPersistentDeletesProducer();
+      if (segmentInfos.contains(info) && persistentDeletesProducer != null) {
+        // this info might have been dropped already by an earlier call to this method, only check the pool if
+        // it's still there ie. a valid segment
+        ReadersAndUpdates readersAndUpdates = readerPool.get(info, true);
+        if (readersAndUpdates != null) {
+          try {
+            SegmentReader reader = readersAndUpdates.getReader(IOContext.READ);
+            try {
+              if (hasPersistentDeletes(reader, new Bits.MatchNoBits(reader.maxDoc()))) {
+                // only drop it if it's really fully deleted and has no persistent deletes
+                if (infoStream.isEnabled("IW")) {
+                  infoStream.message("IW", "keep fully deleted segment " + segString(info) + " found persistent deletes");
+                }
+                return;
+              }
+            } finally {
+              readersAndUpdates.release(reader);
+            }
+          } finally {
+            readerPool.release(readersAndUpdates);
+          }
+        }
+      }
       // it's possible that we invoke this method more than once for the same SCI
       // we must only remove the docs once!
       boolean dropPendingDocs = segmentInfos.remove(info);
@@ -2995,8 +3061,6 @@ public class IndexWriter implements Closeable, TwoPhaseCommit, Accountable {
     // long so we can detect int overflow:
     long numDocs = 0;
 
-    Sort indexSort = config.getIndexSort();
-
     long seqNo;
 
     try {
@@ -3027,7 +3091,7 @@ public class IndexWriter implements Closeable, TwoPhaseCommit, Accountable {
 
       SegmentMerger merger = new SegmentMerger(Arrays.asList(readers), info, infoStream, trackingDir,
                                                globalFieldNumberMap, 
-                                               context);
+                                               context, null);
 
       if (!merger.shouldMerge()) {
         return docWriter.deleteQueue.getNextSequenceNumber();
@@ -3693,18 +3757,6 @@ public class IndexWriter implements Closeable, TwoPhaseCommit, Accountable {
     }
   }
   
-  private static class MergedDeletesAndUpdates {
-    ReadersAndUpdates mergedDeletesAndUpdates = null;
-    
-    MergedDeletesAndUpdates() {}
-    
-    final void init(ReaderPool readerPool, MergePolicy.OneMerge merge) throws IOException {
-      if (mergedDeletesAndUpdates == null) {
-        mergedDeletesAndUpdates = readerPool.get(merge.info, true);
-      }
-    }
-  }
-
   /**
    * Carefully merges deletes and updates for the segments we just merged. This
    * is tricky because, although merging will clear all deletes (compacts the
@@ -3760,6 +3812,7 @@ public class IndexWriter implements Closeable, TwoPhaseCommit, Accountable {
         assert currentLiveDocs != null;
         assert prevLiveDocs.length() == maxDoc;
         assert currentLiveDocs.length() == maxDoc;
+        Bits persistentDeletes = mergeState.persistentDeletes[i];
 
         // There were deletes on this segment when the merge
         // started.  The merge has collapsed away those
@@ -3781,11 +3834,24 @@ public class IndexWriter implements Closeable, TwoPhaseCommit, Accountable {
             if (prevLiveDocs.get(j) == false) {
               // if the document was deleted before, it better still be deleted!
               assert currentLiveDocs.get(j) == false;
+              if (persistentDeletes != null && persistentDeletes.get(j)) {
+                mergedDeletesAndUpdates.delete(segDocMap.get(segLeafDocMap.get(j)));
+              }
             } else if (currentLiveDocs.get(j) == false) {
               // the document was deleted while we were merging:
               mergedDeletesAndUpdates.delete(segDocMap.get(segLeafDocMap.get(j)));
             }
           }
+        } else {
+          if (persistentDeletes != null) {
+            for (int j = 0; j < maxDoc; j++) {
+              if (persistentDeletes.get(j)) {
+                // if the document was deleted before, it better still be deleted!
+                assert currentLiveDocs.get(j) == false;
+                mergedDeletesAndUpdates.delete(segDocMap.get(segLeafDocMap.get(j)));
+              }
+            }
+          }
         }
       } else if (currentLiveDocs != null) {
         assert currentLiveDocs.length() == maxDoc;
@@ -3932,20 +3998,20 @@ public class IndexWriter implements Closeable, TwoPhaseCommit, Accountable {
       merge.info.info.maxDoc() == 0 ||
       (mergedUpdates != null &&
        mergedUpdates.getPendingDeleteCount() == merge.info.info.maxDoc());
-
+    boolean hasPersistentDeletes = mergeState.hasPersistentDeletes();
     if (infoStream.isEnabled("IW")) {
       if (allDeleted) {
-        infoStream.message("IW", "merged segment " + merge.info + " is 100% deleted" +  (keepFullyDeletedSegments ? "" : "; skipping insert"));
+        infoStream.message("IW", "merged segment " + merge.info + " is 100% deleted" +  (hasPersistentDeletes ? "" : "; skipping insert"));
       }
     }
 
-    final boolean dropSegment = allDeleted && !keepFullyDeletedSegments;
+    final boolean dropSegment = allDeleted && hasPersistentDeletes == false;
 
     // If we merged no segments then we better be dropping
     // the new segment:
     assert merge.segments.size() > 0 || dropSegment;
 
-    assert merge.info.info.maxDoc() != 0 || keepFullyDeletedSegments || dropSegment;
+    assert merge.info.info.maxDoc() != 0 || hasPersistentDeletes || dropSegment;
 
     if (mergedUpdates != null) {
       boolean success = false;
@@ -4430,7 +4496,7 @@ public class IndexWriter implements Closeable, TwoPhaseCommit, Accountable {
       final SegmentMerger merger = new SegmentMerger(mergeReaders,
                                                      merge.info.info, infoStream, dirWrapper,
                                                      globalFieldNumberMap, 
-                                                     context);
+                                                     context, config.getPersistentDeletesProducer());
 
       merge.checkAborted();
 
@@ -4680,19 +4746,6 @@ public class IndexWriter implements Closeable, TwoPhaseCommit, Accountable {
     }
   }
 
-  boolean keepFullyDeletedSegments;
-
-  /** Only for testing.
-   *
-   * @lucene.internal */
-  void setKeepFullyDeletedSegments(boolean v) {
-    keepFullyDeletedSegments = v;
-  }
-
-  boolean getKeepFullyDeletedSegments() {
-    return keepFullyDeletedSegments;
-  }
-
   // called only from assert
   private boolean filesExist(SegmentInfos toSync) throws IOException {
     
diff --git a/lucene/core/src/java/org/apache/lucene/index/IndexWriterConfig.java b/lucene/core/src/java/org/apache/lucene/index/IndexWriterConfig.java
index 997a686252..41974983e3 100644
--- a/lucene/core/src/java/org/apache/lucene/index/IndexWriterConfig.java
+++ b/lucene/core/src/java/org/apache/lucene/index/IndexWriterConfig.java
@@ -17,6 +17,7 @@
 package org.apache.lucene.index;
 
 
+import java.io.IOException;
 import java.io.PrintStream;
 import java.util.Arrays;
 import java.util.EnumSet;
@@ -26,6 +27,7 @@ import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.standard.StandardAnalyzer;
 import org.apache.lucene.codecs.Codec;
 import org.apache.lucene.index.IndexWriter.IndexReaderWarmer;
+import org.apache.lucene.search.DocIdSetIterator;
 import org.apache.lucene.search.Sort;
 import org.apache.lucene.search.SortField;
 import org.apache.lucene.search.similarities.Similarity;
@@ -484,5 +486,14 @@ public final class IndexWriterConfig extends LiveIndexWriterConfig {
   public IndexWriterConfig setCheckPendingFlushUpdate(boolean checkPendingFlushOnUpdate) {
     return (IndexWriterConfig) super.setCheckPendingFlushUpdate(checkPendingFlushOnUpdate);
   }
-  
+
+  /**
+   * Set the {@link org.apache.lucene.index.LiveIndexWriterConfig.PersistentDeletesProducer}. Use <code>null</code>
+   * to disable persistent deletes
+   */
+  public IndexWriterConfig setPersistentDeletesProducer(PersistentDeletesProducer persistentDeletesProducer) {
+    this.persistentDeletesProducer = persistentDeletesProducer;
+    return this;
+  }
+
 }
diff --git a/lucene/core/src/java/org/apache/lucene/index/LiveIndexWriterConfig.java b/lucene/core/src/java/org/apache/lucene/index/LiveIndexWriterConfig.java
index af8ff1531f..1284f67e71 100644
--- a/lucene/core/src/java/org/apache/lucene/index/LiveIndexWriterConfig.java
+++ b/lucene/core/src/java/org/apache/lucene/index/LiveIndexWriterConfig.java
@@ -17,6 +17,7 @@
 package org.apache.lucene.index;
 
 
+import java.io.IOException;
 import java.util.Collections;
 import java.util.Set;
 
@@ -25,6 +26,7 @@ import org.apache.lucene.codecs.Codec;
 import org.apache.lucene.index.DocumentsWriterPerThread.IndexingChain;
 import org.apache.lucene.index.IndexWriter.IndexReaderWarmer;
 import org.apache.lucene.index.IndexWriterConfig.OpenMode;
+import org.apache.lucene.search.DocIdSetIterator;
 import org.apache.lucene.search.IndexSearcher;
 import org.apache.lucene.search.Sort;
 import org.apache.lucene.search.similarities.Similarity;
@@ -106,6 +108,11 @@ public class LiveIndexWriterConfig {
   /** if an indexing thread should check for pending flushes on update in order to help out on a full flush*/
   protected volatile boolean checkPendingFlushOnUpdate = true;
 
+  /** A persistent deletes producer */
+  protected PersistentDeletesProducer persistentDeletesProducer = null;
+
+
+
   // used by IndexWriterConfig
   LiveIndexWriterConfig(Analyzer analyzer) {
     this.analyzer = analyzer;
@@ -452,6 +459,14 @@ public class LiveIndexWriterConfig {
     return this;
   }
 
+  /**
+   * Returns a new PersistentDeletesProducer or <code>null</code> if persistent deletes are not supported
+   * @lucene.experimental
+   */
+  public PersistentDeletesProducer getPersistentDeletesProducer() {
+    return persistentDeletesProducer;
+  }
+
   @Override
   public String toString() {
     StringBuilder sb = new StringBuilder();
@@ -475,6 +490,22 @@ public class LiveIndexWriterConfig {
     sb.append("commitOnClose=").append(getCommitOnClose()).append("\n");
     sb.append("indexSort=").append(getIndexSort()).append("\n");
     sb.append("checkPendingFlushOnUpdate=").append(isCheckPendingFlushOnUpdate()).append("\n");
+    sb.append("persistentDeletesProducer=").append(getPersistentDeletesProducer()).append("\n");
     return sb.toString();
   }
+
+  /**
+   * Expert: allows filtering documents that should survive merges even though they are marked as deleted.
+   * This is useful if a document should be kept around even when they have been explicitly deleted or updated. This can
+   * be useful if a history of updates should be maintained in the index or for testing purposes.
+   * @lucene.experimental
+   */
+  @FunctionalInterface
+  public interface PersistentDeletesProducer {
+    /**
+     * Returns a {@link DocIdSetIterator} that marks all documents that should survive a merge even though they are
+     * marked as deleted. This method will return <code>null</code> if there are no persistent deletes.
+     */
+    DocIdSetIterator get(CodecReader reader) throws IOException;
+  }
 }
diff --git a/lucene/core/src/java/org/apache/lucene/index/MergePolicy.java b/lucene/core/src/java/org/apache/lucene/index/MergePolicy.java
index d9a0ab83ee..561ddf8085 100644
--- a/lucene/core/src/java/org/apache/lucene/index/MergePolicy.java
+++ b/lucene/core/src/java/org/apache/lucene/index/MergePolicy.java
@@ -562,7 +562,7 @@ public abstract class MergePolicy {
    *  writer, and matches the current compound file setting */
   protected final boolean isMerged(SegmentInfos infos, SegmentCommitInfo info, IndexWriter writer) throws IOException {
     assert writer != null;
-    boolean hasDeletions = writer.numDeletedDocs(info) > 0;
+    boolean hasDeletions = writer.numDeletedDocs(info) > 0 && writer.hasPersistentDeletedDocs(info) == false;
     return !hasDeletions &&
       info.info.dir == writer.getDirectory() &&
       useCompoundFile(infos, info, writer) == info.info.getUseCompoundFile();
diff --git a/lucene/core/src/java/org/apache/lucene/index/MergePolicyWrapper.java b/lucene/core/src/java/org/apache/lucene/index/MergePolicyWrapper.java
index c51cd00d7c..77c8b755dc 100644
--- a/lucene/core/src/java/org/apache/lucene/index/MergePolicyWrapper.java
+++ b/lucene/core/src/java/org/apache/lucene/index/MergePolicyWrapper.java
@@ -85,5 +85,4 @@ public class MergePolicyWrapper extends MergePolicy {
   public String toString() {
     return getClass().getSimpleName() + "(" + in + ")";
   }
-
 }
diff --git a/lucene/core/src/java/org/apache/lucene/index/MergeState.java b/lucene/core/src/java/org/apache/lucene/index/MergeState.java
index 9ad69f6b02..9b08cd8a84 100644
--- a/lucene/core/src/java/org/apache/lucene/index/MergeState.java
+++ b/lucene/core/src/java/org/apache/lucene/index/MergeState.java
@@ -28,9 +28,13 @@ import org.apache.lucene.codecs.NormsProducer;
 import org.apache.lucene.codecs.PointsReader;
 import org.apache.lucene.codecs.StoredFieldsReader;
 import org.apache.lucene.codecs.TermVectorsReader;
+import org.apache.lucene.search.DocIdSetIterator;
 import org.apache.lucene.search.Sort;
+import org.apache.lucene.util.BitSet;
 import org.apache.lucene.util.Bits;
+import org.apache.lucene.util.FixedBitSet;
 import org.apache.lucene.util.InfoStream;
+import org.apache.lucene.util.SparseFixedBitSet;
 import org.apache.lucene.util.packed.PackedInts;
 import org.apache.lucene.util.packed.PackedLongValues;
 
@@ -69,6 +73,9 @@ public class MergeState {
   /** Live docs for each reader */
   public final Bits[] liveDocs;
 
+  /** Persistent deletes */
+  public final Bits[] persistentDeletes;
+
   /** Postings to merge */
   public final FieldsProducer[] fieldsProducers;
 
@@ -85,7 +92,8 @@ public class MergeState {
   public boolean needsIndexSort;
 
   /** Sole constructor. */
-  MergeState(List<CodecReader> originalReaders, SegmentInfo segmentInfo, InfoStream infoStream) throws IOException {
+  MergeState(List<CodecReader> originalReaders, SegmentInfo segmentInfo, InfoStream infoStream,
+             LiveIndexWriterConfig.PersistentDeletesProducer persistentDeletesProducer) throws IOException {
 
     this.infoStream = infoStream;
 
@@ -103,6 +111,8 @@ public class MergeState {
     pointsReaders = new PointsReader[numReaders];
     fieldInfos = new FieldInfos[numReaders];
     liveDocs = new Bits[numReaders];
+    persistentDeletes = new Bits[numReaders];
+    int[] numDocsPerReader = new int[numReaders];
 
     int numDocs = 0;
     for(int i=0;i<numReaders;i++) {
@@ -110,6 +120,7 @@ public class MergeState {
 
       maxDocs[i] = reader.maxDoc();
       liveDocs[i] = reader.getLiveDocs();
+
       fieldInfos[i] = reader.getFieldInfos();
 
       normsProducers[i] = reader.getNormsReader();
@@ -137,17 +148,56 @@ public class MergeState {
       if (pointsReaders[i] != null) {
         pointsReaders[i] = pointsReaders[i].getMergeInstance();
       }
-      numDocs += reader.numDocs();
+      numDocsPerReader[i] = buildPersistentDeletes(reader, i, liveDocs, persistentDeletes, persistentDeletesProducer);
+      numDocs += numDocsPerReader[i];
     }
 
     segmentInfo.setMaxDoc(numDocs);
 
     this.segmentInfo = segmentInfo;
-    this.docMaps = buildDocMaps(readers, indexSort);
+    this.docMaps = buildDocMaps(readers, liveDocs, numDocsPerReader, indexSort);
+  }
+
+  private int buildPersistentDeletes(CodecReader reader, int i, Bits[] liveDocs, Bits[] persistentDeletes,
+                                     LiveIndexWriterConfig.PersistentDeletesProducer persistentDeletesProducer) throws IOException {
+    int numDocs = reader.numDocs();
+    if (liveDocs[i] != null  && persistentDeletesProducer != null) {
+      Bits currentLiveDocs = liveDocs[i];
+      DocIdSetIterator iter = persistentDeletesProducer.get(reader);
+      if (iter != null) {
+        boolean useSpareBitSet = BitSet.useSparseBitSet(reader.maxDoc() - numDocs, reader.maxDoc());
+        BitSet persistentDeleteBits = useSpareBitSet ?
+            new SparseFixedBitSet(reader.maxDoc()) : new FixedBitSet(reader.maxDoc());
+        for (int doc = iter.nextDoc(); doc != DocIdSetIterator.NO_MORE_DOCS; doc = iter.nextDoc()) {
+          if (currentLiveDocs.get(doc) == false) {
+            persistentDeleteBits.set(doc);
+            numDocs++;
+          }
+        }
+        if (numDocs == reader.numDocs()) {
+          assert persistentDeletes[i] == null;
+          // we didn't bring anything back to life here.
+          return reader.numDocs();
+        }
+        liveDocs[i] = new Bits() {
+          @Override
+          public boolean get(int index) {
+            return currentLiveDocs.get(index) || persistentDeleteBits.get(index);
+          }
+
+          @Override
+          public int length() {
+            return currentLiveDocs.length();
+          }
+        };
+        persistentDeletes[i] = persistentDeleteBits;
+      }
+    }
+    return numDocs;
   }
 
   // Remap docIDs around deletions
-  private DocMap[] buildDeletionDocMaps(List<CodecReader> readers) {
+  private DocMap[] buildDeletionDocMaps(List<CodecReader> readers, Bits[] readerLiveDocs, int[] numDocsPerReader) {
 
     int totalDocs = 0;
     int numReaders = readers.size();
@@ -155,7 +205,7 @@ public class MergeState {
 
     for (int i = 0; i < numReaders; i++) {
       LeafReader reader = readers.get(i);
-      Bits liveDocs = reader.getLiveDocs();
+      Bits liveDocs = readerLiveDocs[i];
 
       final PackedLongValues delDocMap;
       if (liveDocs != null) {
@@ -177,24 +227,25 @@ public class MergeState {
           }
         }
       };
-      totalDocs += reader.numDocs();
+      totalDocs += numDocsPerReader[i];
     }
 
     return docMaps;
   }
 
-  private DocMap[] buildDocMaps(List<CodecReader> readers, Sort indexSort) throws IOException {
+  private DocMap[] buildDocMaps(List<CodecReader> readers, Bits[] liveDocs, int[] numDocsPerReader,
+                                Sort indexSort) throws IOException {
 
     if (indexSort == null) {
       // no index sort ... we only must map around deletions, and rebase to the merged segment's docID space
-      return buildDeletionDocMaps(readers);
+      return buildDeletionDocMaps(readers, liveDocs, numDocsPerReader);
     } else {
       // do a merge sort of the incoming leaves:
       long t0 = System.nanoTime();
-      DocMap[] result = MultiSorter.sort(indexSort, readers);
+      DocMap[] result = MultiSorter.sort(indexSort, readers, liveDocs);
       if (result == null) {
         // already sorted so we can switch back to map around deletions
-        return buildDeletionDocMaps(readers);
+        return buildDeletionDocMaps(readers, liveDocs, numDocsPerReader);
       } else {
         needsIndexSort = true;
       }
@@ -273,6 +324,15 @@ public class MergeState {
     return readers;
   }
 
+  public boolean hasPersistentDeletes() {
+    for (int i = 0; i < persistentDeletes.length; i++) {
+      if (persistentDeletes[i] != null) {
+        return true;
+      }
+    }
+    return false;
+  }
+
   /** A map of doc IDs. */
   public static abstract class DocMap {
     /** Sole constructor */
diff --git a/lucene/core/src/java/org/apache/lucene/index/MultiSorter.java b/lucene/core/src/java/org/apache/lucene/index/MultiSorter.java
index b484228bfc..6e6c476dc0 100644
--- a/lucene/core/src/java/org/apache/lucene/index/MultiSorter.java
+++ b/lucene/core/src/java/org/apache/lucene/index/MultiSorter.java
@@ -36,7 +36,7 @@ final class MultiSorter {
    *  documents into the merged segment.  The documents for each incoming leaf reader must already be sorted by the same sort!
    *  Returns null if the merge sort is not needed (segments are already in index sort order).
    **/
-  static MergeState.DocMap[] sort(Sort sort, List<CodecReader> readers) throws IOException {
+  static MergeState.DocMap[] sort(Sort sort, List<CodecReader> readers, Bits[] readerLiveDocs) throws IOException {
 
     // TODO: optimize if only 1 reader is incoming, though that's a rare case
 
@@ -71,7 +71,7 @@ final class MultiSorter {
 
     for(int i=0;i<leafCount;i++) {
       CodecReader reader = readers.get(i);
-      LeafAndDocID leaf = new LeafAndDocID(i, reader.getLiveDocs(), reader.maxDoc(), comparables.length);
+      LeafAndDocID leaf = new LeafAndDocID(i, readerLiveDocs[i], reader.maxDoc(), comparables.length);
       for(int j=0;j<comparables.length;j++) {
         leaf.values[j] = comparables[j][i].getComparable(leaf.docID);
         assert leaf.values[j] != null;
@@ -113,7 +113,7 @@ final class MultiSorter {
     MergeState.DocMap[] docMaps = new MergeState.DocMap[leafCount];
     for(int i=0;i<leafCount;i++) {
       final PackedLongValues remapped = builders[i].build();
-      final Bits liveDocs = readers.get(i).getLiveDocs();
+      final Bits liveDocs = readerLiveDocs[i];
       docMaps[i] = new MergeState.DocMap() {
           @Override
           public int get(int docID) {
diff --git a/lucene/core/src/java/org/apache/lucene/index/NoMergePolicy.java b/lucene/core/src/java/org/apache/lucene/index/NoMergePolicy.java
index ec309b8a29..769f0040d3 100644
--- a/lucene/core/src/java/org/apache/lucene/index/NoMergePolicy.java
+++ b/lucene/core/src/java/org/apache/lucene/index/NoMergePolicy.java
@@ -67,9 +67,10 @@ public final class NoMergePolicy extends MergePolicy {
   public void setNoCFSRatio(double noCFSRatio) {
     super.setNoCFSRatio(noCFSRatio);
   }
-  
+
   @Override
   public String toString() {
     return "NoMergePolicy";
   }
+
 }
diff --git a/lucene/core/src/java/org/apache/lucene/index/ReadersAndUpdates.java b/lucene/core/src/java/org/apache/lucene/index/ReadersAndUpdates.java
index 0e32256226..27c1332b3d 100644
--- a/lucene/core/src/java/org/apache/lucene/index/ReadersAndUpdates.java
+++ b/lucene/core/src/java/org/apache/lucene/index/ReadersAndUpdates.java
@@ -291,11 +291,6 @@ class ReadersAndUpdates {
     return liveDocs;
   }
 
-  public synchronized Bits getReadOnlyLiveDocs() {
-    liveDocsShared = true;
-    return liveDocs;
-  }
-
   public synchronized void dropChanges() {
     assert Thread.holdsLock(writer);
     // Discard (don't save) changes when we are dropping
diff --git a/lucene/core/src/java/org/apache/lucene/index/SegmentMerger.java b/lucene/core/src/java/org/apache/lucene/index/SegmentMerger.java
index ad60a94298..7393bd6288 100644
--- a/lucene/core/src/java/org/apache/lucene/index/SegmentMerger.java
+++ b/lucene/core/src/java/org/apache/lucene/index/SegmentMerger.java
@@ -52,11 +52,12 @@ final class SegmentMerger {
 
   // note, just like in codec apis Directory 'dir' is NOT the same as segmentInfo.dir!!
   SegmentMerger(List<CodecReader> readers, SegmentInfo segmentInfo, InfoStream infoStream, Directory dir,
-                FieldInfos.FieldNumbers fieldNumbers, IOContext context) throws IOException {
+                FieldInfos.FieldNumbers fieldNumbers, IOContext context,
+                LiveIndexWriterConfig.PersistentDeletesProducer persistentDeletesProducer) throws IOException {
     if (context.context != IOContext.Context.MERGE) {
       throw new IllegalArgumentException("IOContext.context should be MERGE; got: " + context.context);
     }
-    mergeState = new MergeState(readers, segmentInfo, infoStream);
+    mergeState = new MergeState(readers, segmentInfo, infoStream, persistentDeletesProducer);
     directory = dir;
     this.codec = segmentInfo.getCodec();
     this.context = context;
diff --git a/lucene/core/src/java/org/apache/lucene/index/StandardDirectoryReader.java b/lucene/core/src/java/org/apache/lucene/index/StandardDirectoryReader.java
index a605c55d9b..0188b4e7ec 100644
--- a/lucene/core/src/java/org/apache/lucene/index/StandardDirectoryReader.java
+++ b/lucene/core/src/java/org/apache/lucene/index/StandardDirectoryReader.java
@@ -31,6 +31,7 @@ import java.util.concurrent.CopyOnWriteArraySet;
 import org.apache.lucene.store.AlreadyClosedException;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.IOContext;
+import org.apache.lucene.util.Bits;
 import org.apache.lucene.util.IOUtils;
 
 /** Default implementation of {@link DirectoryReader}. */
@@ -103,7 +104,8 @@ public final class StandardDirectoryReader extends DirectoryReader {
         final ReadersAndUpdates rld = writer.readerPool.get(info, true);
         try {
           final SegmentReader reader = rld.getReadOnlyClone(IOContext.READ);
-          if (reader.numDocs() > 0 || writer.getKeepFullyDeletedSegments()) {
+          if (reader.numDocs() > 0
+              || writer.hasPersistentDeletes(reader, new Bits.MatchNoBits(reader.maxDoc()))) {
             // Steal the ref:
             readers.add(reader);
             infosUpto++;
diff --git a/lucene/core/src/java/org/apache/lucene/util/BitSet.java b/lucene/core/src/java/org/apache/lucene/util/BitSet.java
index b86219bb97..3959f995f7 100644
--- a/lucene/core/src/java/org/apache/lucene/util/BitSet.java
+++ b/lucene/core/src/java/org/apache/lucene/util/BitSet.java
@@ -31,9 +31,8 @@ public abstract class BitSet implements MutableBits, Accountable {
    *  NOTE: this will fully consume the {@link DocIdSetIterator}. */
   public static BitSet of(DocIdSetIterator it, int maxDoc) throws IOException {
     final long cost = it.cost();
-    final int threshold = maxDoc >>> 7;
     BitSet set;
-    if (cost < threshold) {
+    if (useSparseBitSet(cost, maxDoc)) {
       set = new SparseFixedBitSet(maxDoc);
     } else {
       set = new FixedBitSet(maxDoc);
@@ -42,6 +41,14 @@ public abstract class BitSet implements MutableBits, Accountable {
     return set;
   }
 
+  /**
+   * Returns true iff the cost given the max number of documents warrants a sparse bit set.
+   */
+  public static boolean useSparseBitSet(long cost, int maxDoc) {
+    final int threshold = maxDoc >>> 7;
+    return cost < threshold;
+  }
+
   /** Set the bit at <code>i</code>. */
   public abstract void set(int i);
 
diff --git a/lucene/core/src/test/org/apache/lucene/TestExternalCodecs.java b/lucene/core/src/test/org/apache/lucene/TestExternalCodecs.java
index 2795679d0f..a5c1a7f393 100644
--- a/lucene/core/src/test/org/apache/lucene/TestExternalCodecs.java
+++ b/lucene/core/src/test/org/apache/lucene/TestExternalCodecs.java
@@ -70,9 +70,10 @@ public class TestExternalCodecs extends LuceneTestCase {
     dir.setCheckIndexOnClose(false); // we use a custom codec provider
     IndexWriter w = new IndexWriter(
         dir,
-        newIndexWriterConfig(new MockAnalyzer(random())).
-        setCodec(new CustomPerFieldCodec()).
-            setMergePolicy(newLogMergePolicy(3))
+        newIndexWriterConfig(new MockAnalyzer(random()))
+            .setCodec(new CustomPerFieldCodec())
+            .setMergePolicy(newLogMergePolicy(3))
+            .setPersistentDeletesProducer(null)
     );
     Document doc = new Document();
     // uses default codec:
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestAddIndexes.java b/lucene/core/src/test/org/apache/lucene/index/TestAddIndexes.java
index 876328a4a4..12a05b5b83 100644
--- a/lucene/core/src/test/org/apache/lucene/index/TestAddIndexes.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestAddIndexes.java
@@ -164,7 +164,9 @@ public class TestAddIndexes extends LuceneTestCase {
     Directory aux = newDirectory();
 
     setUpDirs(dir, aux);
-    IndexWriter writer = newWriter(dir, newIndexWriterConfig(new MockAnalyzer(random())).setOpenMode(OpenMode.APPEND));
+    IndexWriter writer = newWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))
+        .setPersistentDeletesProducer(null)
+        .setOpenMode(OpenMode.APPEND));
     writer.addIndexes(aux);
 
     // Adds 10 docs, then replaces them with another 10
@@ -201,7 +203,9 @@ public class TestAddIndexes extends LuceneTestCase {
     Directory aux = newDirectory();
 
     setUpDirs(dir, aux);
-    IndexWriter writer = newWriter(dir, newIndexWriterConfig(new MockAnalyzer(random())).setOpenMode(OpenMode.APPEND));
+    IndexWriter writer = newWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))
+        .setPersistentDeletesProducer(null)
+        .setOpenMode(OpenMode.APPEND));
 
     // Adds 10 docs, then replaces them with another 10
     // docs, so 10 pending deletes:
@@ -240,7 +244,9 @@ public class TestAddIndexes extends LuceneTestCase {
     Directory aux = newDirectory();
 
     setUpDirs(dir, aux);
-    IndexWriter writer = newWriter(dir, newIndexWriterConfig(new MockAnalyzer(random())).setOpenMode(OpenMode.APPEND));
+    IndexWriter writer = newWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))
+        .setPersistentDeletesProducer(null)
+        .setOpenMode(OpenMode.APPEND));
 
     // Adds 10 docs, then replaces them with another 10
     // docs, so 10 pending deletes:
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestBinaryDocValuesUpdates.java b/lucene/core/src/test/org/apache/lucene/index/TestBinaryDocValuesUpdates.java
index 80ae80423e..c3ab1cb985 100644
--- a/lucene/core/src/test/org/apache/lucene/index/TestBinaryDocValuesUpdates.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestBinaryDocValuesUpdates.java
@@ -534,6 +534,7 @@ public class TestBinaryDocValuesUpdates extends LuceneTestCase {
     Directory dir = newDirectory();
     Random random = random();
     IndexWriterConfig conf = newIndexWriterConfig(new MockAnalyzer(random));
+    conf.setPersistentDeletesProducer(null);
     IndexWriter writer = new IndexWriter(dir, conf);
 
     int docid = 0;
@@ -562,6 +563,7 @@ public class TestBinaryDocValuesUpdates extends LuceneTestCase {
       } else if (random.nextDouble() < 0.1) {
         writer.close();
         conf = newIndexWriterConfig(new MockAnalyzer(random));
+        conf.setPersistentDeletesProducer(null);
         writer = new IndexWriter(dir, conf);
       }
 
@@ -1376,7 +1378,8 @@ public class TestBinaryDocValuesUpdates extends LuceneTestCase {
   
   public void testUpdateAllDeletedSegment() throws Exception {
     Directory dir = newDirectory();
-    IndexWriterConfig conf = newIndexWriterConfig(new MockAnalyzer(random()));
+    IndexWriterConfig conf = newIndexWriterConfig(new MockAnalyzer(random()))
+        .setPersistentDeletesProducer(null);
     IndexWriter writer = new IndexWriter(dir, conf);
     
     Document doc = new Document();
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestDoc.java b/lucene/core/src/test/org/apache/lucene/index/TestDoc.java
index 37761d3f68..a30c9f4491 100644
--- a/lucene/core/src/test/org/apache/lucene/index/TestDoc.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestDoc.java
@@ -222,7 +222,7 @@ public class TestDoc extends LuceneTestCase {
 
     SegmentMerger merger = new SegmentMerger(Arrays.<CodecReader>asList(r1, r2),
                                              si, InfoStream.getDefault(), trackingDir,
-                                             new FieldInfos.FieldNumbers(), context);
+                                             new FieldInfos.FieldNumbers(), context, null);
 
     MergeState mergeState = merger.merge();
     r1.close();
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestIndexTooManyDocs.java b/lucene/core/src/test/org/apache/lucene/index/TestIndexTooManyDocs.java
index 78305f43cb..f3cf53e6cc 100644
--- a/lucene/core/src/test/org/apache/lucene/index/TestIndexTooManyDocs.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestIndexTooManyDocs.java
@@ -52,29 +52,32 @@ public class TestIndexTooManyDocs extends LuceneTestCase {
       for (int i = 0; i < numThreads; i++) {
         if (i >= 2) {
           threads[i] = new Thread(() -> {
-            latch.countDown();
             try {
-              latch.await();
-            } catch (InterruptedException e) {
-              throw new AssertionError(e);
-            }
-            for (int d = 0; d < 100; d++) {
-              Document doc = new Document();
-              String id = Integer.toString(random().nextInt(numMaxDoc * 2));
-              doc.add(new StringField("id", id, Field.Store.NO));
+              latch.countDown();
               try {
-                Term t = new Term("id", id);
-                if (random().nextInt(5) == 0) {
-                  writer.deleteDocuments(new TermQuery(t));
-                }
-                writer.updateDocument(t, doc);
-              } catch (IOException e) {
+                latch.await();
+              } catch (InterruptedException e) {
                 throw new AssertionError(e);
-              } catch (IllegalArgumentException e) {
-                assertEquals("number of documents in the index cannot exceed " + IndexWriter.getActualMaxDocs(), e.getMessage());
               }
+              for (int d = 0; d < 100; d++) {
+                Document doc = new Document();
+                String id = Integer.toString(random().nextInt(numMaxDoc * 2));
+                doc.add(new StringField("id", id, Field.Store.NO));
+                try {
+                  Term t = new Term("id", id);
+                  if (random().nextInt(5) == 0) {
+                    writer.deleteDocuments(new TermQuery(t));
+                  }
+                  writer.updateDocument(t, doc);
+                } catch (IOException e) {
+                  throw new AssertionError(e);
+                } catch (IllegalArgumentException e) {
+                  assertEquals("number of documents in the index cannot exceed " + IndexWriter.getActualMaxDocs(), e.getMessage());
+                }
+              }
+            } finally {
+              indexingDone.countDown();
             }
-            indexingDone.countDown();
           });
         } else {
           threads[i] = new Thread(() -> {
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestIndexWriter.java b/lucene/core/src/test/org/apache/lucene/index/TestIndexWriter.java
index 983581ac3b..d9a3b7bed2 100644
--- a/lucene/core/src/test/org/apache/lucene/index/TestIndexWriter.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestIndexWriter.java
@@ -39,6 +39,7 @@ import java.util.Random;
 import java.util.Set;
 import java.util.concurrent.CountDownLatch;
 import java.util.concurrent.atomic.AtomicBoolean;
+import java.util.concurrent.atomic.AtomicInteger;
 
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.CannedTokenStream;
@@ -55,6 +56,7 @@ import org.apache.lucene.document.BinaryDocValuesField;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.FieldType;
+import org.apache.lucene.document.IntPoint;
 import org.apache.lucene.document.NumericDocValuesField;
 import org.apache.lucene.document.SortedDocValuesField;
 import org.apache.lucene.document.SortedNumericDocValuesField;
@@ -69,7 +71,10 @@ import org.apache.lucene.search.DocIdSetIterator;
 import org.apache.lucene.search.IndexSearcher;
 import org.apache.lucene.search.MatchAllDocsQuery;
 import org.apache.lucene.search.PhraseQuery;
+import org.apache.lucene.search.Query;
 import org.apache.lucene.search.ScoreDoc;
+import org.apache.lucene.search.ScoreMode;
+import org.apache.lucene.search.Scorer;
 import org.apache.lucene.search.TermQuery;
 import org.apache.lucene.store.AlreadyClosedException;
 import org.apache.lucene.store.BaseDirectoryWrapper;
@@ -92,7 +97,6 @@ import org.apache.lucene.util.Constants;
 import org.apache.lucene.util.IOUtils;
 import org.apache.lucene.util.InfoStream;
 import org.apache.lucene.util.LuceneTestCase;
-import org.apache.lucene.util.SetOnce;
 import org.apache.lucene.util.StringHelper;
 import org.apache.lucene.util.TestUtil;
 import org.apache.lucene.util.ThreadInterruptedException;
@@ -135,7 +139,8 @@ public class TestIndexWriter extends LuceneTestCase {
     reader.close();
 
     // merge the index down and check that the new doc count is correct
-    writer = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random())));
+    writer = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))
+        .setPersistentDeletesProducer(null));
     assertEquals(60, writer.numDocs());
     writer.forceMerge(1);
     assertEquals(60, writer.maxDoc());
@@ -1493,7 +1498,8 @@ public class TestIndexWriter extends LuceneTestCase {
    */
   public void testWickedLongTerm() throws IOException {
     Directory dir = newDirectory();
-    RandomIndexWriter w = new RandomIndexWriter(random(), dir, new StringSplitAnalyzer());
+    RandomIndexWriter w = new RandomIndexWriter(random(), dir,
+        newIndexWriterConfig(random(), new StringSplitAnalyzer()).setPersistentDeletesProducer(null));
 
     char[] chars = new char[DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8];
     Arrays.fill(chars, 'x');
@@ -2219,19 +2225,20 @@ public class TestIndexWriter extends LuceneTestCase {
   
   public void testMergeAllDeleted() throws IOException {
     Directory dir = newDirectory();
+    AtomicBoolean keepFullyDeletedSegments = new AtomicBoolean(false);
     IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(random()));
-    final SetOnce<IndexWriter> iwRef = new SetOnce<>();
+    iwc.setPersistentDeletesProducer(
+        reader -> keepFullyDeletedSegments.get() ? DocIdSetIterator.all(reader.maxDoc()) : DocIdSetIterator.empty());
     IndexWriter evilWriter = RandomIndexWriter.mockIndexWriter(random(), dir, iwc, new RandomIndexWriter.TestPoint() {
       @Override
       public void apply(String message) {
         if ("startCommitMerge".equals(message)) {
-          iwRef.get().setKeepFullyDeletedSegments(false);
+          keepFullyDeletedSegments.set(false);
         } else if ("startMergeInit".equals(message)) {
-          iwRef.get().setKeepFullyDeletedSegments(true);
+          keepFullyDeletedSegments.set(true);
         }
       }
     });
-    iwRef.set(evilWriter);
     for (int i = 0; i < 1000; i++) {
       addDoc(evilWriter);
       if (random().nextInt(17) == 0) {
@@ -2956,4 +2963,130 @@ public class TestIndexWriter extends LuceneTestCase {
     }
   }
 
+  public void testPersistentDeletes() throws IOException {
+    Directory dir = newDirectory();
+    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig()
+        .setPersistentDeletesProducer(reader -> DocIdSetIterator.all(reader.maxDoc())));
+
+    Document doc = new Document();
+    doc.add(new StringField("id", "1", Field.Store.YES));
+    doc.add(newTextField(random(), "text", "hello world", Field.Store.YES));
+    doc.add(new IntPoint("point", 1));
+    doc.add(new NumericDocValuesField("point", 1));
+    w.addDocument(doc);
+    doc = new Document();
+    doc.add(new StringField("id", "2", Field.Store.YES));
+    doc.add(newTextField(random(), "text", "hello world", Field.Store.YES));
+    doc.add(new IntPoint("point", 1));
+    doc.add(new NumericDocValuesField("point", 2));
+    w.addDocument(doc);
+    w.commit();
+    w.updateDocument(new Term("id", "2"), doc);
+    w.commit();
+    w.forceMerge(1);
+    w.commit();
+    assertEquals(2, w.numDocs());
+    assertEquals(3, w.maxDoc());
+    assertTrue(w.hasDeletions());
+    DirectoryReader open = DirectoryReader.open(w);
+    assertEquals(open.leaves().size(), 1);
+    LeafReader leafReader = open.leaves().get(0).reader();
+    TermsEnum id = leafReader.terms("id").iterator();
+    assertTrue(id.seekExact(new BytesRef("2")));
+    PostingsEnum postings = id.postings(null);
+    assertNotSame(postings.nextDoc(), DocIdSetIterator.NO_MORE_DOCS);
+    assertNotSame(postings.nextDoc(), DocIdSetIterator.NO_MORE_DOCS);
+    assertEquals(postings.nextDoc(), DocIdSetIterator.NO_MORE_DOCS);
+
+    // check if fully deleted segments are kept
+    w.updateDocument(new Term("id", "2"), doc); // add another segment
+    w.deleteDocuments(new Term("id", "1"));
+    w.deleteDocuments(new Term("id", "2"));
+    w.commit();
+    if (random().nextBoolean()) {
+      w.forceMerge(1);
+    }
+
+    assertEquals(0, w.numDocs());
+    assertEquals(4, w.maxDoc());
+    assertTrue(w.hasDeletions());
+
+    DirectoryReader newReader = DirectoryReader.open(w);
+    assertNotSame(newReader, open);
+    assertEquals(0, newReader.numDocs());
+    assertEquals(4, newReader.maxDoc());
+    IOUtils.close(open, newReader, w, dir);
+  }
+
+  public void testFlushFullyDeletedSegment() throws IOException {
+    Directory dir = newDirectory();
+    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig()
+        .setPersistentDeletesProducer(reader -> DocIdSetIterator.all(reader.maxDoc())));
+
+    Document doc = new Document();
+    doc.add(new StringField("id", "1", Field.Store.YES));
+    doc.add(newTextField(random(), "text", "hello world", Field.Store.YES));
+    doc.add(new IntPoint("point", 1));
+    doc.add(new NumericDocValuesField("point", 1));
+    w.addDocument(doc);
+    w.deleteDocuments(new Term("id", "1"));
+    w.commit();
+    assertEquals(0, w.numDocs());
+    assertEquals(1, w.maxDoc());
+    DirectoryReader newReader = DirectoryReader.open(w);
+    assertEquals(0, newReader.numDocs());
+    assertEquals(1, newReader.maxDoc());
+    IOUtils.close(newReader, w, dir);
+  }
+
+  public void testRandomPersistentDeletes() throws IOException {
+    Directory dir = newDirectory();
+    AtomicInteger maxSeqId = new AtomicInteger(0);
+
+    RandomIndexWriter w = new RandomIndexWriter(random(), dir, newIndexWriterConfig()
+        .setPersistentDeletesProducer(reader -> {
+          Query seq_id = IntPoint.newRangeQuery("seq_id", maxSeqId.intValue() - 50, Integer.MAX_VALUE);
+          IndexSearcher s = new IndexSearcher(reader);
+          Scorer scorer = s.createNormalizedWeight(seq_id, ScoreMode.COMPLETE_NO_SCORES).scorer(reader.getContext());
+          if (scorer != null) {
+            return scorer.iterator();
+          } else {
+            return null;
+          }
+        }));
+    int numDocs = atLeast(100);
+    for (int i = 0; i < numDocs; i++) {
+      int id = random().nextInt(5);
+      Document doc = new Document();
+      doc.add(new StringField("id", id  + "", Field.Store.YES));
+      int seqId = maxSeqId.incrementAndGet();
+      doc.add(new IntPoint("seq_id", seqId));
+      doc.add(new NumericDocValuesField("seq_id", seqId));
+      w.updateDocument(new Term("id", id  + ""), doc);
+    }
+    w.forceMerge(1);
+    w.commit();
+    w.close();
+
+    DirectoryReader reader = DirectoryReader.open(dir);
+    assertEquals(1, reader.leaves().size());
+    LeafReader leafReader = reader.leaves().get(0).reader();
+    NumericDocValues seq_id = leafReader.getNumericDocValues("seq_id");
+    long[] seqIds = new long[leafReader.maxDoc()];
+    int j = 0;
+    while(seq_id.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {
+      seqIds[j++] = seq_id.longValue();
+    }
+    assertEquals(j, seqIds.length);
+    Arrays.sort(seqIds);
+    int last = seqIds.length-1;
+    for (int i = 0; i < Math.min(50, seqIds.length); i++) {
+      assertEquals(seqIds[last-i]-1, seqIds[last-(i+1)]);
+    }
+    reader.close();
+    dir.close();
+
+
+  }
+
 }
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestIndexWriterConfig.java b/lucene/core/src/test/org/apache/lucene/index/TestIndexWriterConfig.java
index 063045ef2b..77b59a3f7c 100644
--- a/lucene/core/src/test/org/apache/lucene/index/TestIndexWriterConfig.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestIndexWriterConfig.java
@@ -74,6 +74,7 @@ public class TestIndexWriterConfig extends LuceneTestCase {
     assertEquals(Codec.getDefault(), conf.getCodec());
     assertEquals(InfoStream.getDefault(), conf.getInfoStream());
     assertEquals(IndexWriterConfig.DEFAULT_USE_COMPOUND_FILE_SYSTEM, conf.getUseCompoundFile());
+    assertNull(conf.getPersistentDeletesProducer());
     assertTrue(conf.isCheckPendingFlushOnUpdate());
     // Sanity check - validate that all getters are covered.
     Set<String> getters = new HashSet<>();
@@ -100,6 +101,7 @@ public class TestIndexWriterConfig extends LuceneTestCase {
     getters.add("getInfoStream");
     getters.add("getUseCompoundFile");
     getters.add("isCheckPendingFlushOnUpdate");
+    getters.add("getPersistentDeletesProducer");
     
     for (Method m : IndexWriterConfig.class.getDeclaredMethods()) {
       if (m.getDeclaringClass() == IndexWriterConfig.class && m.getName().startsWith("get")) {
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions.java b/lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions.java
index 61bf1fc940..afe1bd9773 100644
--- a/lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions.java
@@ -684,6 +684,7 @@ public class TestIndexWriterExceptions extends LuceneTestCase {
       reader.close();
 
       writer = new IndexWriter(dir, newIndexWriterConfig(analyzer)
+                                      .setPersistentDeletesProducer(null)
                                       .setMaxBufferedDocs(10));
       doc = new Document();
       doc.add(newField("contents", "here are some contents", DocCopyIterator.custom5));
@@ -794,6 +795,7 @@ public class TestIndexWriterExceptions extends LuceneTestCase {
       assertEquals(NUM_THREAD*NUM_ITER, numDel);
 
       IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(analyzer)
+                                                  .setPersistentDeletesProducer(null)
                                                   .setMaxBufferedDocs(10));
       Document doc = new Document();
       doc.add(newField("contents", "here are some contents", DocCopyIterator.custom5));
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestIndexWriterMergePolicy.java b/lucene/core/src/test/org/apache/lucene/index/TestIndexWriterMergePolicy.java
index 2d5680c15d..b8555315b1 100644
--- a/lucene/core/src/test/org/apache/lucene/index/TestIndexWriterMergePolicy.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestIndexWriterMergePolicy.java
@@ -211,6 +211,7 @@ public class TestIndexWriterMergePolicy extends LuceneTestCase {
                                     .setOpenMode(OpenMode.APPEND)
                                     .setMaxBufferedDocs(10)
                                     .setMergePolicy(ldmp)
+                                    .setPersistentDeletesProducer(null)
                                     .setMergeScheduler(new ConcurrentMergeScheduler()));
 
     // merge factor is changed, so check invariants after all adds
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestIndexWriterMerging.java b/lucene/core/src/test/org/apache/lucene/index/TestIndexWriterMerging.java
index 6931efaed8..66fae740b3 100644
--- a/lucene/core/src/test/org/apache/lucene/index/TestIndexWriterMerging.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestIndexWriterMerging.java
@@ -163,7 +163,8 @@ public class TestIndexWriterMerging extends LuceneTestCase {
     ir.close();
 
     writer = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))
-                                    .setMergePolicy(newLogMergePolicy()));
+        .setPersistentDeletesProducer(null)
+        .setMergePolicy(newLogMergePolicy()));
     assertEquals(8, writer.numDocs());
     assertEquals(10, writer.maxDoc());
     writer.forceMergeDeletes();
@@ -230,6 +231,7 @@ public class TestIndexWriterMerging extends LuceneTestCase {
     writer = new IndexWriter(
         dir,
         newIndexWriterConfig(new MockAnalyzer(random()))
+          .setPersistentDeletesProducer(null)
           .setMergePolicy(newLogMergePolicy(3))
     );
     assertEquals(49, writer.numDocs());
@@ -295,7 +297,8 @@ public class TestIndexWriterMerging extends LuceneTestCase {
     writer = new IndexWriter(
         dir,
         newIndexWriterConfig(new MockAnalyzer(random()))
-           .setMergePolicy(newLogMergePolicy(3))
+            .setPersistentDeletesProducer(null)
+            .setMergePolicy(newLogMergePolicy(3))
     );
     writer.forceMergeDeletes(false);
     writer.close();
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull.java b/lucene/core/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull.java
index be862ef060..7541f55ffa 100644
--- a/lucene/core/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull.java
@@ -28,6 +28,7 @@ import org.apache.lucene.document.IntPoint;
 import org.apache.lucene.document.NumericDocValuesField;
 import org.apache.lucene.document.TextField;
 import org.apache.lucene.index.IndexWriterConfig.OpenMode;
+import org.apache.lucene.search.DocIdSetIterator;
 import org.apache.lucene.search.IndexSearcher;
 import org.apache.lucene.search.ScoreDoc;
 import org.apache.lucene.search.TermQuery;
@@ -502,10 +503,9 @@ public class TestIndexWriterOnDiskFull extends LuceneTestCase {
           .setMergeScheduler(new SerialMergeScheduler())
           .setReaderPooling(true)
           .setMergePolicy(newLogMergePolicy(2))
+          // we can do this because we add/delete/add (and dont merge to "nothing")
+          .setPersistentDeletesProducer(r -> DocIdSetIterator.all(r.maxDoc()))
     );
-    // we can do this because we add/delete/add (and dont merge to "nothing")
-    w.setKeepFullyDeletedSegments(true);
-
     Document doc = new Document();
 
     doc.add(newTextField("f", "doctor who", Field.Store.NO));
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestIndexWriterReader.java b/lucene/core/src/test/org/apache/lucene/index/TestIndexWriterReader.java
index 86881acff1..dddf5ecad6 100644
--- a/lucene/core/src/test/org/apache/lucene/index/TestIndexWriterReader.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestIndexWriterReader.java
@@ -893,7 +893,7 @@ public class TestIndexWriterReader extends LuceneTestCase {
   public void testForceMergeDeletes() throws Throwable {
     Directory dir = newDirectory();
     final IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))
-                                                 .setMergePolicy(newLogMergePolicy()));
+        .setPersistentDeletesProducer(null).setMergePolicy(newLogMergePolicy()));
     Document doc = new Document();
     doc.add(newTextField("field", "a b c", Field.Store.NO));
     Field id = newStringField("id", "", Field.Store.NO);
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestMultiFields.java b/lucene/core/src/test/org/apache/lucene/index/TestMultiFields.java
index 27f2f1a369..5b7122ff13 100644
--- a/lucene/core/src/test/org/apache/lucene/index/TestMultiFields.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestMultiFields.java
@@ -49,10 +49,9 @@ public class TestMultiFields extends LuceneTestCase {
       Directory dir = newDirectory();
 
       IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))
-                                             .setMergePolicy(NoMergePolicy.INSTANCE));
-      // we can do this because we use NoMergePolicy (and dont merge to "nothing")
-      w.setKeepFullyDeletedSegments(true);
-
+          .setMergePolicy(new MergePolicyWrapper(NoMergePolicy.INSTANCE))
+          // we keep fully deleted segments
+          .setPersistentDeletesProducer(reader -> DocIdSetIterator.all(reader.maxDoc())));
       Map<BytesRef,List<Integer>> docs = new HashMap<>();
       Set<Integer> deleted = new HashSet<>();
       List<BytesRef> terms = new ArrayList<>();
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestNumericDocValuesUpdates.java b/lucene/core/src/test/org/apache/lucene/index/TestNumericDocValuesUpdates.java
index f6a328abb8..21e87ad2da 100644
--- a/lucene/core/src/test/org/apache/lucene/index/TestNumericDocValuesUpdates.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestNumericDocValuesUpdates.java
@@ -633,7 +633,8 @@ public class TestNumericDocValuesUpdates extends LuceneTestCase {
   public void testSegmentMerges() throws Exception {
     Directory dir = newDirectory();
     Random random = random();
-    IndexWriterConfig conf = newIndexWriterConfig(new MockAnalyzer(random));
+    IndexWriterConfig conf = newIndexWriterConfig(new MockAnalyzer(random))
+        .setPersistentDeletesProducer(null);
     IndexWriter writer = new IndexWriter(dir, conf);
     
     int docid = 0;
@@ -688,6 +689,7 @@ public class TestNumericDocValuesUpdates extends LuceneTestCase {
         }
         writer.close();
         conf = newIndexWriterConfig(new MockAnalyzer(random));
+        conf.setPersistentDeletesProducer(null);
         writer = new IndexWriter(dir, conf);
       }
 
@@ -1612,7 +1614,8 @@ public class TestNumericDocValuesUpdates extends LuceneTestCase {
   
   public void testUpdateAllDeletedSegment() throws Exception {
     Directory dir = newDirectory();
-    IndexWriterConfig conf = newIndexWriterConfig(new MockAnalyzer(random()));
+    IndexWriterConfig conf = newIndexWriterConfig(new MockAnalyzer(random()))
+        .setPersistentDeletesProducer(null);
     IndexWriter writer = new IndexWriter(dir, conf);
     
     Document doc = new Document();
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestParallelReaderEmptyIndex.java b/lucene/core/src/test/org/apache/lucene/index/TestParallelReaderEmptyIndex.java
index 61c84dccab..a626b047fb 100644
--- a/lucene/core/src/test/org/apache/lucene/index/TestParallelReaderEmptyIndex.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestParallelReaderEmptyIndex.java
@@ -112,7 +112,7 @@ public class TestParallelReaderEmptyIndex extends LuceneTestCase {
       ir.close();
 
       iw = new IndexWriter(rd1, newIndexWriterConfig(new MockAnalyzer(random()))
-                                  .setOpenMode(OpenMode.APPEND));
+          .setPersistentDeletesProducer(null).setOpenMode(OpenMode.APPEND));
       iw.forceMerge(1);
       iw.close();
     }
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestPointValues.java b/lucene/core/src/test/org/apache/lucene/index/TestPointValues.java
index 7228f37600..409ce1bc03 100644
--- a/lucene/core/src/test/org/apache/lucene/index/TestPointValues.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestPointValues.java
@@ -540,7 +540,7 @@ public class TestPointValues extends LuceneTestCase {
 
   public void testDeleteAllPointDocs() throws Exception {
     Directory dir = newDirectory();
-    IndexWriterConfig iwc = newIndexWriterConfig();
+    IndexWriterConfig iwc = newIndexWriterConfig().setPersistentDeletesProducer(null);
     IndexWriter w = new IndexWriter(dir, iwc);
     Document doc = new Document();
     doc.add(new StringField("id", "0", Field.Store.NO));
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger.java b/lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger.java
index 6d0e04bbb2..af17152c3e 100644
--- a/lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger.java
@@ -89,7 +89,7 @@ public class TestSegmentMerger extends LuceneTestCase {
     SegmentMerger merger = new SegmentMerger(Arrays.<CodecReader>asList(reader1, reader2),
                                              si, InfoStream.getDefault(), mergedDir,
                                              new FieldInfos.FieldNumbers(),
-                                             newIOContext(random(), new IOContext(new MergeInfo(-1, -1, false, -1))));
+                                             newIOContext(random(), new IOContext(new MergeInfo(-1, -1, false, -1))), null);
     MergeState mergeState = merger.merge();
     int docsMerged = mergeState.segmentInfo.maxDoc();
     assertTrue(docsMerged == 2);
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestThreadedForceMerge.java b/lucene/core/src/test/org/apache/lucene/index/TestThreadedForceMerge.java
index dae20b5b85..d2d827e730 100644
--- a/lucene/core/src/test/org/apache/lucene/index/TestThreadedForceMerge.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestThreadedForceMerge.java
@@ -61,7 +61,8 @@ public class TestThreadedForceMerge extends LuceneTestCase {
         newIndexWriterConfig(ANALYZER).
             setOpenMode(OpenMode.CREATE).
             setMaxBufferedDocs(2).
-            setMergePolicy(newLogMergePolicy())
+            setMergePolicy(newLogMergePolicy()).
+            setPersistentDeletesProducer(null)
     );
 
     for(int iter=0;iter<NUM_ITER;iter++) {
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestTieredMergePolicy.java b/lucene/core/src/test/org/apache/lucene/index/TestTieredMergePolicy.java
index c8a87d3793..db4ea37ba8 100644
--- a/lucene/core/src/test/org/apache/lucene/index/TestTieredMergePolicy.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestTieredMergePolicy.java
@@ -32,6 +32,7 @@ public class TestTieredMergePolicy extends BaseMergePolicyTestCase {
   public void testForceMergeDeletes() throws Exception {
     Directory dir = newDirectory();
     IndexWriterConfig conf = newIndexWriterConfig(new MockAnalyzer(random()));
+    conf.setPersistentDeletesProducer(null);
     TieredMergePolicy tmp = newTieredMergePolicy();
     conf.setMergePolicy(tmp);
     conf.setMaxBufferedDocs(4);
@@ -116,6 +117,7 @@ public class TestTieredMergePolicy extends BaseMergePolicyTestCase {
     tmp.setMaxMergedSegmentMB(0.01);
     tmp.setForceMergeDeletesPctAllowed(0.0);
     conf.setMergePolicy(tmp);
+    conf.setPersistentDeletesProducer(null);
 
     final IndexWriter w = new IndexWriter(dir, conf);
 
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestUpgradeIndexMergePolicy.java b/lucene/core/src/test/org/apache/lucene/index/TestUpgradeIndexMergePolicy.java
index 0ab13b42de..54436f1dca 100644
--- a/lucene/core/src/test/org/apache/lucene/index/TestUpgradeIndexMergePolicy.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestUpgradeIndexMergePolicy.java
@@ -20,7 +20,7 @@ package org.apache.lucene.index;
 public class TestUpgradeIndexMergePolicy extends BaseMergePolicyTestCase {
 
   public MergePolicy mergePolicy() {
-    return new UpgradeIndexMergePolicy(newMergePolicy(random()));
+    return new UpgradeIndexMergePolicy(newMergePolicy());
   }
 
 }
diff --git a/lucene/sandbox/src/test/org/apache/lucene/codecs/idversion/TestIDVersionPostingsFormat.java b/lucene/sandbox/src/test/org/apache/lucene/codecs/idversion/TestIDVersionPostingsFormat.java
index 0574e70d26..e1ef1f5c1d 100644
--- a/lucene/sandbox/src/test/org/apache/lucene/codecs/idversion/TestIDVersionPostingsFormat.java
+++ b/lucene/sandbox/src/test/org/apache/lucene/codecs/idversion/TestIDVersionPostingsFormat.java
@@ -507,7 +507,7 @@ public class TestIDVersionPostingsFormat extends LuceneTestCase {
 
   public void testMoreThanOneDocPerIDWithDeletesAcrossSegments() throws IOException {
     Directory dir = newDirectory();
-    IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(random()));
+    IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(random())).setPersistentDeletesProducer(null);
     iwc.setCodec(TestUtil.alwaysPostingsFormat(new IDVersionPostingsFormat()));
     RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);
     Document doc = new Document();
@@ -609,6 +609,7 @@ public class TestIDVersionPostingsFormat extends LuceneTestCase {
   public void testGlobalVersions() throws Exception {
     Directory dir = newDirectory();
     IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(random()));
+    iwc.setPersistentDeletesProducer(null);
     iwc.setCodec(TestUtil.alwaysPostingsFormat(new IDVersionPostingsFormat()));
     final RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);
 
diff --git a/lucene/test-framework/src/java/org/apache/lucene/index/BaseDocValuesFormatTestCase.java b/lucene/test-framework/src/java/org/apache/lucene/index/BaseDocValuesFormatTestCase.java
index 28ab3b6d01..6ed46666ee 100644
--- a/lucene/test-framework/src/java/org/apache/lucene/index/BaseDocValuesFormatTestCase.java
+++ b/lucene/test-framework/src/java/org/apache/lucene/index/BaseDocValuesFormatTestCase.java
@@ -581,6 +581,7 @@ public abstract class BaseDocValuesFormatTestCase extends BaseIndexFileFormatTes
     Analyzer analyzer = new MockAnalyzer(random());
     IndexWriterConfig iwconfig = newIndexWriterConfig(analyzer);
     iwconfig.setMergePolicy(newLogMergePolicy());
+    iwconfig.setPersistentDeletesProducer(null);
     RandomIndexWriter iwriter = new RandomIndexWriter(random(), directory, iwconfig);
     
     Document doc = new Document();
@@ -767,6 +768,7 @@ public abstract class BaseDocValuesFormatTestCase extends BaseIndexFileFormatTes
     Analyzer analyzer = new MockAnalyzer(random());
     IndexWriterConfig iwconfig = newIndexWriterConfig(analyzer);
     iwconfig.setMergePolicy(newLogMergePolicy());
+    iwconfig.setPersistentDeletesProducer(null);
     RandomIndexWriter iwriter = new RandomIndexWriter(random(), directory, iwconfig);
     
     Document doc = new Document();
@@ -1991,6 +1993,7 @@ public abstract class BaseDocValuesFormatTestCase extends BaseIndexFileFormatTes
     Analyzer analyzer = new MockAnalyzer(random());
     IndexWriterConfig iwconfig = newIndexWriterConfig(analyzer);
     iwconfig.setMergePolicy(newLogMergePolicy());
+    iwconfig.setPersistentDeletesProducer(null);
     RandomIndexWriter iwriter = new RandomIndexWriter(random(), directory, iwconfig);
     
     Document doc = new Document();
@@ -2931,6 +2934,7 @@ public abstract class BaseDocValuesFormatTestCase extends BaseIndexFileFormatTes
     Analyzer analyzer = new MockAnalyzer(random());
     IndexWriterConfig iwconfig = newIndexWriterConfig(analyzer);
     iwconfig.setMergePolicy(newLogMergePolicy());
+    iwconfig.setPersistentDeletesProducer(null);
     RandomIndexWriter iwriter = new RandomIndexWriter(random(), directory, iwconfig);
     
     Document doc = new Document();
@@ -3057,6 +3061,7 @@ public abstract class BaseDocValuesFormatTestCase extends BaseIndexFileFormatTes
     Analyzer analyzer = new MockAnalyzer(random());
     IndexWriterConfig iwconfig = newIndexWriterConfig(analyzer);
     iwconfig.setMergePolicy(newLogMergePolicy());
+    iwconfig.setPersistentDeletesProducer(null);
     RandomIndexWriter iwriter = new RandomIndexWriter(random(), directory, iwconfig);
     
     Document doc = new Document();
@@ -3222,6 +3227,7 @@ public abstract class BaseDocValuesFormatTestCase extends BaseIndexFileFormatTes
     Analyzer analyzer = new MockAnalyzer(random());
     IndexWriterConfig iwconfig = newIndexWriterConfig(analyzer);
     iwconfig.setMergePolicy(newLogMergePolicy());
+    iwconfig.setPersistentDeletesProducer(null);
     RandomIndexWriter iwriter = new RandomIndexWriter(random(), directory, iwconfig);
 
     Document doc = new Document();
@@ -3252,6 +3258,7 @@ public abstract class BaseDocValuesFormatTestCase extends BaseIndexFileFormatTes
     Analyzer analyzer = new MockAnalyzer(random());
     IndexWriterConfig iwconfig = newIndexWriterConfig(analyzer);
     iwconfig.setMergePolicy(newLogMergePolicy());
+    iwconfig.setPersistentDeletesProducer(null);
     RandomIndexWriter iwriter = new RandomIndexWriter(random(), directory, iwconfig);
 
     Document doc = new Document();
@@ -3282,6 +3289,7 @@ public abstract class BaseDocValuesFormatTestCase extends BaseIndexFileFormatTes
     Analyzer analyzer = new MockAnalyzer(random());
     IndexWriterConfig iwconfig = newIndexWriterConfig(analyzer);
     iwconfig.setMergePolicy(newLogMergePolicy());
+    iwconfig.setPersistentDeletesProducer(null);
     RandomIndexWriter iwriter = new RandomIndexWriter(random(), directory, iwconfig);
 
     Document doc = new Document();
@@ -3312,6 +3320,7 @@ public abstract class BaseDocValuesFormatTestCase extends BaseIndexFileFormatTes
     Analyzer analyzer = new MockAnalyzer(random());
     IndexWriterConfig iwconfig = newIndexWriterConfig(analyzer);
     iwconfig.setMergePolicy(newLogMergePolicy());
+    iwconfig.setPersistentDeletesProducer(null);
     RandomIndexWriter iwriter = new RandomIndexWriter(random(), directory, iwconfig);
 
     Document doc = new Document();
@@ -3342,6 +3351,7 @@ public abstract class BaseDocValuesFormatTestCase extends BaseIndexFileFormatTes
     Analyzer analyzer = new MockAnalyzer(random());
     IndexWriterConfig iwconfig = newIndexWriterConfig(analyzer);
     iwconfig.setMergePolicy(newLogMergePolicy());
+    iwconfig.setPersistentDeletesProducer(null);
     RandomIndexWriter iwriter = new RandomIndexWriter(random(), directory, iwconfig);
 
     Document doc = new Document();
diff --git a/lucene/test-framework/src/java/org/apache/lucene/index/BaseNormsFormatTestCase.java b/lucene/test-framework/src/java/org/apache/lucene/index/BaseNormsFormatTestCase.java
index 6b301f96dc..33a5acc768 100644
--- a/lucene/test-framework/src/java/org/apache/lucene/index/BaseNormsFormatTestCase.java
+++ b/lucene/test-framework/src/java/org/apache/lucene/index/BaseNormsFormatTestCase.java
@@ -589,7 +589,8 @@ public abstract class BaseNormsFormatTestCase extends BaseIndexFileFormatTestCas
    */
   public void testUndeadNorms() throws Exception {
     Directory dir = applyCreatedVersionMajor(newDirectory());
-    RandomIndexWriter w = new RandomIndexWriter(random(), dir);
+    RandomIndexWriter w = new RandomIndexWriter(random(), dir,
+        newIndexWriterConfig(random(), new MockAnalyzer(random())).setPersistentDeletesProducer(null));
     int numDocs = atLeast(500);
     List<Integer> toDelete = new ArrayList<>();
     for(int i=0;i<numDocs;i++) {
diff --git a/lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase.java b/lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase.java
index 51b418e41e..f5fc52cfd9 100644
--- a/lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase.java
+++ b/lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase.java
@@ -281,6 +281,7 @@ public abstract class BasePostingsFormatTestCase extends BaseIndexFileFormatTest
     IndexWriterConfig iwc = newIndexWriterConfig(null);
     iwc.setCodec(getCodec());
     iwc.setMergePolicy(newLogMergePolicy());
+    iwc.setPersistentDeletesProducer(null);
     RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwc);
     Document doc = new Document();
     iw.addDocument(doc);
diff --git a/lucene/test-framework/src/java/org/apache/lucene/util/LuceneTestCase.java b/lucene/test-framework/src/java/org/apache/lucene/util/LuceneTestCase.java
index 1b7ee618cb..a775bd5e86 100644
--- a/lucene/test-framework/src/java/org/apache/lucene/util/LuceneTestCase.java
+++ b/lucene/test-framework/src/java/org/apache/lucene/util/LuceneTestCase.java
@@ -987,7 +987,9 @@ public abstract class LuceneTestCase extends Assert {
     }
 
     c.setMergePolicy(newMergePolicy(r));
-
+    if (rarely(r)) {
+      c.setPersistentDeletesProducer(reader -> DocIdSetIterator.all(reader.maxDoc()));
+    }
     avoidPathologicalMerging(c);
 
     if (rarely(r)) {
@@ -1053,7 +1055,7 @@ public abstract class LuceneTestCase extends Assert {
       return new MockRandomMergePolicy(r);
     } else if (r.nextBoolean()) {
       return newTieredMergePolicy(r);
-    } else if (r.nextInt(5) == 0) { 
+    } else if (r.nextInt(5) == 0) {
       return newAlcoholicMergePolicy(r, classEnvRule.timeZone);
     }
     return newLogMergePolicy(r);
