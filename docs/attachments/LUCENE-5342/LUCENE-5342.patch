Index: lucene/CHANGES.txt
===================================================================
--- lucene/CHANGES.txt	(revision 1542296)
+++ lucene/CHANGES.txt	(working copy)
@@ -158,6 +158,10 @@
   deleted at a later point in time. This could cause short-term disk
   pollution or OOM if in-memory directories are used. (Simon Willnauer)
 
+* LUCENE-5342: Fixed bulk-merge issue in CompressingStoredFieldsFormat which
+  created corrupted segments when mixing chunk sizes.
+  Lucene41StoredFieldsFormat is not impacted. (Adrien Grand, Robert Muir)
+
 API Changes
 
 * LUCENE-5222: Add SortField.needsScores(). Previously it was not possible
Index: lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsReader.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsReader.java	(revision 1542296)
+++ lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsReader.java	(working copy)
@@ -373,6 +373,10 @@
     return compressionMode;
   }
 
+  int getChunkSize() {
+    return chunkSize;
+  }
+
   ChunkIterator chunkIterator(int startDocID) throws IOException {
     ensureOpen();
     fieldsStream.seek(indexReader.getStartPointer(startDocID));
Index: lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsWriter.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsWriter.java	(revision 1542296)
+++ lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsWriter.java	(working copy)
@@ -337,7 +337,9 @@
       final Bits liveDocs = reader.getLiveDocs();
 
       if (matchingFieldsReader == null
-          || matchingFieldsReader.getVersion() != VERSION_CURRENT) { // means reader version is not the same as the writer version
+          || matchingFieldsReader.getVersion() != VERSION_CURRENT // means reader version is not the same as the writer version
+          || matchingFieldsReader.getCompressionMode() != compressionMode
+          || matchingFieldsReader.getChunkSize() != chunkSize) { // the way data is decompressed depends on the chunk size
         // naive merge...
         for (int i = nextLiveDoc(0, liveDocs, maxDoc); i < maxDoc; i = nextLiveDoc(i + 1, liveDocs, maxDoc)) {
           Document doc = reader.document(i);
@@ -362,8 +364,7 @@
               startOffsets[i] = startOffsets[i - 1] + it.lengths[i - 1];
             }
 
-            if (compressionMode == matchingFieldsReader.getCompressionMode() // same compression mode
-                && numBufferedDocs == 0 // starting a new chunk
+            if (numBufferedDocs == 0 // starting a new chunk
                 && startOffsets[it.chunkDocs - 1] < chunkSize // chunk is small enough
                 && startOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] >= chunkSize // chunk is large enough
                 && nextDeletedDoc(it.docBase, liveDocs, it.docBase + it.chunkDocs) == it.docBase + it.chunkDocs) { // no deletion in the chunk
