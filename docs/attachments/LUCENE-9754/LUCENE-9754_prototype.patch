diff --git a/lucene/analysis/icu/src/java/org/apache/lucene/analysis/icu/segmentation/ICUTokenizer.java b/lucene/analysis/icu/src/java/org/apache/lucene/analysis/icu/segmentation/ICUTokenizer.java
index 5e35f6af24e..46e4b95a93b 100644
--- a/lucene/analysis/icu/src/java/org/apache/lucene/analysis/icu/segmentation/ICUTokenizer.java
+++ b/lucene/analysis/icu/src/java/org/apache/lucene/analysis/icu/segmentation/ICUTokenizer.java
@@ -16,15 +16,14 @@
  */
 package org.apache.lucene.analysis.icu.segmentation;
 
-import com.ibm.icu.lang.UCharacter;
 import com.ibm.icu.text.BreakIterator;
 import java.io.IOException;
-import java.io.Reader;
-import org.apache.lucene.analysis.Tokenizer;
+import java.util.Locale;
 import org.apache.lucene.analysis.icu.tokenattributes.ScriptAttribute;
 import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
 import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
 import org.apache.lucene.analysis.tokenattributes.TypeAttribute;
+import org.apache.lucene.analysis.util.SegmentingTokenizerBase;
 import org.apache.lucene.util.AttributeFactory;
 
 /**
@@ -37,16 +36,7 @@ import org.apache.lucene.util.AttributeFactory;
  * @see ICUTokenizerConfig
  * @lucene.experimental
  */
-public final class ICUTokenizer extends Tokenizer {
-  private static final int IOBUFFER = 4096;
-  private final char buffer[] = new char[IOBUFFER];
-  /** true length of text in the buffer */
-  private int length = 0;
-  /** length in buffer that can be evaluated safely, up to a safe end point */
-  private int usableLength = 0;
-  /** accumulated offset of previous buffers for this reader, for offsetAtt */
-  private int offset = 0;
-
+public final class ICUTokenizer extends SegmentingTokenizerBase {
   private final CompositeBreakIterator breaker; /* tokenizes a char[] of text */
   private final ICUTokenizerConfig config;
   private final OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);
@@ -87,119 +77,29 @@ public final class ICUTokenizer extends Tokenizer {
    * @param config Tailored BreakIterator configuration
    */
   public ICUTokenizer(AttributeFactory factory, ICUTokenizerConfig config) {
-    super(factory);
+    // TODO: allow customizing this sentence iterator, make it an ICU one
+    super(factory, java.text.BreakIterator.getSentenceInstance(Locale.ROOT));
     this.config = config;
     breaker = new CompositeBreakIterator(config);
   }
 
   @Override
-  public boolean incrementToken() throws IOException {
-    clearAttributes();
-    if (length == 0) refill();
-    while (!incrementTokenBuffer()) {
-      refill();
-      if (length <= 0) // no more bytes to read;
-      return false;
-    }
-    return true;
+  protected void setNextSentence(int sentenceStart, int sentenceEnd) {
+    breaker.setText(buffer, sentenceStart, sentenceEnd - sentenceStart);
   }
 
   @Override
   public void reset() throws IOException {
     super.reset();
     breaker.setText(buffer, 0, 0);
-    length = usableLength = offset = 0;
-  }
-
-  @Override
-  public void end() throws IOException {
-    super.end();
-    final int finalOffset = (length < 0) ? offset : offset + length;
-    offsetAtt.setOffset(correctOffset(finalOffset), correctOffset(finalOffset));
-  }
-
-  /*
-   * This tokenizes text based upon the longest matching rule, and because of
-   * this, isn't friendly to a Reader.
-   *
-   * Text is read from the input stream in 4kB chunks. Within a 4kB chunk of
-   * text, the last unambiguous break point is found (in this implementation:
-   * white space character) Any remaining characters represent possible partial
-   * words, so are appended to the front of the next chunk.
-   *
-   * There is the possibility that there are no unambiguous break points within
-   * an entire 4kB chunk of text (binary data). So there is a maximum word limit
-   * of 4kB since it will not try to grow the buffer in this case.
-   */
-
-  /**
-   * Returns the last unambiguous break position in the text.
-   *
-   * @return position of character, or -1 if one does not exist
-   */
-  private int findSafeEnd() {
-    for (int i = length - 1; i >= 0; i--) {
-      if (UCharacter.isWhitespace(buffer[i])) {
-        return i + 1;
-      }
-    }
-    return -1;
-  }
-
-  /**
-   * Refill the buffer, accumulating the offset and setting usableLength to the last unambiguous
-   * break position
-   *
-   * @throws IOException If there is a low-level I/O error.
-   */
-  private void refill() throws IOException {
-    offset += usableLength;
-    int leftover = length - usableLength;
-    System.arraycopy(buffer, usableLength, buffer, 0, leftover);
-    int requested = buffer.length - leftover;
-    int returned = read(input, buffer, leftover, requested);
-    length = returned + leftover;
-    if (returned < requested) {
-      /* reader has been emptied, process the rest */
-      usableLength = length;
-    } else {
-      /* still more data to be read, find a safe-stopping place */
-      usableLength = findSafeEnd();
-      if (usableLength < 0) {
-        /*
-         * more than IOBUFFER of text without space,
-         * gonna possibly truncate tokens
-         */
-        usableLength = length;
-      }
-    }
-
-    breaker.setText(buffer, 0, Math.max(0, usableLength));
-  }
-
-  // TODO: refactor to a shared readFully somewhere
-  // (NGramTokenizer does this too):
-  /** commons-io's readFully, but without bugs if offset != 0 */
-  private static int read(Reader input, char[] buffer, int offset, int length) throws IOException {
-    assert length >= 0 : "length must not be negative: " + length;
-
-    int remaining = length;
-    while (remaining > 0) {
-      int location = length - remaining;
-      int count = input.read(buffer, offset + location, remaining);
-      if (-1 == count) { // EOF
-        break;
-      }
-      remaining -= count;
-    }
-    return length - remaining;
   }
 
   /*
    * return true if there is a token from the buffer, or null if it is
    * exhausted.
    */
-  private boolean incrementTokenBuffer() {
+  @Override
+  protected boolean incrementWord() {
     int start = breaker.current();
     assert start != BreakIterator.DONE;
 
@@ -214,6 +114,7 @@ public final class ICUTokenizer extends Tokenizer {
       return false; // BreakIterator exhausted
     }
 
+    clearAttributes();
     termAtt.copyBuffer(buffer, start, end - start);
     offsetAtt.setOffset(correctOffset(offset + start), correctOffset(offset + end));
     typeAtt.setType(config.getType(breaker.getScriptCode(), breaker.getRuleStatus()));
diff --git a/lucene/analysis/icu/src/test/org/apache/lucene/analysis/icu/segmentation/TestICUTokenizer.java b/lucene/analysis/icu/src/test/org/apache/lucene/analysis/icu/segmentation/TestICUTokenizer.java
index 1dd5ca0de3a..2602408bb2a 100644
--- a/lucene/analysis/icu/src/test/org/apache/lucene/analysis/icu/segmentation/TestICUTokenizer.java
+++ b/lucene/analysis/icu/src/test/org/apache/lucene/analysis/icu/segmentation/TestICUTokenizer.java
@@ -18,9 +18,7 @@ package org.apache.lucene.analysis.icu.segmentation;
 
 import com.ibm.icu.lang.UScript;
 import com.ibm.icu.text.UnicodeSet;
-import java.io.IOException;
 import java.io.StringReader;
-import java.util.Arrays;
 import java.util.Random;
 import java.util.concurrent.CountDownLatch;
 import org.apache.lucene.analysis.Analyzer;
@@ -30,41 +28,6 @@ import org.apache.lucene.analysis.Tokenizer;
 import org.apache.lucene.analysis.icu.tokenattributes.ScriptAttribute;
 
 public class TestICUTokenizer extends BaseTokenStreamTestCase {
-
-  public void testHugeDoc() throws IOException {
-    StringBuilder sb = new StringBuilder();
-    char whitespace[] = new char[4094];
-    Arrays.fill(whitespace, ' ');
-    sb.append(whitespace);
-    sb.append("testing 1234");
-    String input = sb.toString();
-    ICUTokenizer tokenizer =
-        new ICUTokenizer(newAttributeFactory(), new DefaultICUTokenizerConfig(false, true));
-    tokenizer.setReader(new StringReader(input));
-    assertTokenStreamContents(tokenizer, new String[] {"testing", "1234"});
-  }
-
-  public void testHugeTerm2() throws IOException {
-    StringBuilder sb = new StringBuilder();
-    for (int i = 0; i < 40960; i++) {
-      sb.append('a');
-    }
-    String input = sb.toString();
-    ICUTokenizer tokenizer =
-        new ICUTokenizer(newAttributeFactory(), new DefaultICUTokenizerConfig(false, true));
-    tokenizer.setReader(new StringReader(input));
-    char token[] = new char[4096];
-    Arrays.fill(token, 'a');
-    String expectedToken = new String(token);
-    String expected[] = {
-      expectedToken, expectedToken, expectedToken,
-      expectedToken, expectedToken, expectedToken,
-      expectedToken, expectedToken, expectedToken,
-      expectedToken
-    };
-    assertTokenStreamContents(tokenizer, expected);
-  }
-
   private Analyzer a;
 
   @Override
@@ -517,7 +480,10 @@ public class TestICUTokenizer extends BaseTokenStreamTestCase {
         new String[] {value + '\u200D' + value},
         new String[] {"<EMOJI>"});
   }
-
+  /** test sentence chunking */
+  public void testTwoSentences() throws Exception {
+    assertAnalyzesTo(a, "Word1 word2. Word3 word4", new String[] { "Word1", "word2", "Word3", "word4" });
+  }
   /** blast some random strings through the analyzer */
   public void testRandomStrings() throws Exception {
     checkRandomData(random(), a, 200 * RANDOM_MULTIPLIER);
diff --git a/lucene/analysis/icu/src/test/org/apache/lucene/analysis/icu/segmentation/TestICUTokenizerFactory.java b/lucene/analysis/icu/src/test/org/apache/lucene/analysis/icu/segmentation/TestICUTokenizerFactory.java
index 4ea9542b676..b91974049ba 100644
--- a/lucene/analysis/icu/src/test/org/apache/lucene/analysis/icu/segmentation/TestICUTokenizerFactory.java
+++ b/lucene/analysis/icu/src/test/org/apache/lucene/analysis/icu/segmentation/TestICUTokenizerFactory.java
@@ -44,7 +44,7 @@ public class TestICUTokenizerFactory extends BaseTokenStreamTestCase {
     // “ U+201C LEFT DOUBLE QUOTATION MARK; ” U+201D RIGHT DOUBLE QUOTATION MARK
     Reader reader =
         new StringReader(
-            "  Don't,break.at?/(punct)!  \u201Cnice\u201D\r\n\r\n85_At:all; `really\" +2=3$5,&813 !@#%$^)(*@#$   ");
+            "  Don't,break.at-/(punct)-  \u201Cnice\u201D\r\n\r\n85_At:all; `really\" +2=3$5,&813 -@#%$^)(*@#$   ");
     final Map<String, String> args = new HashMap<>();
     args.put(ICUTokenizerFactory.RULEFILES, "Latn:Latin-break-only-on-whitespace.rbbi");
     ICUTokenizerFactory factory = new ICUTokenizerFactory(args);
@@ -54,12 +54,12 @@ public class TestICUTokenizerFactory extends BaseTokenStreamTestCase {
     assertTokenStreamContents(
         stream,
         new String[] {
-          "Don't,break.at?/(punct)!",
+          "Don't,break.at-/(punct)-",
           "\u201Cnice\u201D",
           "85_At:all;",
           "`really\"",
           "+2=3$5,&813",
-          "!@#%$^)(*@#$"
+          "-@#%$^)(*@#$"
         },
         new String[] {"<ALPHANUM>", "<ALPHANUM>", "<ALPHANUM>", "<ALPHANUM>", "<NUM>", "<OTHER>"});
   }
