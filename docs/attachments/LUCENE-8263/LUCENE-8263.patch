diff --git a/lucene/core/src/java/org/apache/lucene/index/TieredMergePolicy.java b/lucene/core/src/java/org/apache/lucene/index/TieredMergePolicy.java
index 7f917bb..576025a 100644
--- a/lucene/core/src/java/org/apache/lucene/index/TieredMergePolicy.java
+++ b/lucene/core/src/java/org/apache/lucene/index/TieredMergePolicy.java
@@ -100,10 +100,7 @@ public class TieredMergePolicy extends MergePolicy {
   private long floorSegmentBytes = 2*1024*1024L;
   private double segsPerTier = 10.0;
   private double forceMergeDeletesPctAllowed = 10.0;
-  private double reclaimDeletesWeight = 2.0;
-
-  //TODO breaking this up into two JIRAs, see LUCENE-8263
-  //private double indexPctDeletedTarget = 20.0;
+  private double deletesPctAllowed = 33.0;
 
   /** Sole constructor, setting all settings to their
    *  defaults. */
@@ -168,51 +165,30 @@ public class TieredMergePolicy extends MergePolicy {
     return this;
   }
 
-
-  //TODO: See LUCENE-8263
-//  /** Returns the current setIndexPctDeletedTarget setting.
-//   *
-//   * @see #setIndexPctDeletedTarget */
-//  public double getIndexPctDeletedTarget() {
-//    return indexPctDeletedTarget;
-//  }
-//
-//  /** Controls what percentage of documents in the index need to be deleted before
-//   * regular merging considers max segments with more than 50% live documents
-//   * for merging*/
-//  public TieredMergePolicy setIndexPctDeletedTarget(double v) {
-//    if (v < 10.0) {
-//      throw new IllegalArgumentException("indexPctDeletedTarget must be >= 10.0 (got " + v + ")");
-//    }
-//    indexPctDeletedTarget = v;
-//    return this;
-//  }
-
-  /** Returns the current maxMergedSegmentMB setting.
+  /** Returns the current deletesPctAllowed setting.
    *
-   * @see #setMaxMergedSegmentMB */
-  public double getMaxMergedSegmentMB() {
-    return maxMergedSegmentBytes/1024/1024.;
+   * @see #setDeletesPctAllowed */
+  public double getDeletesPctAllowed() {
+    return deletesPctAllowed;
   }
 
-  /** Controls how aggressively merges that reclaim more
-   *  deletions are favored.  Higher values will more
-   *  aggressively target merges that reclaim deletions, but
-   *  be careful not to go so high that way too much merging
-   *  takes place; a value of 3.0 is probably nearly too
-   *  high.  A value of 0.0 means deletions don't impact
-   *  merge selection. */ 
-  public TieredMergePolicy setReclaimDeletesWeight(double v) {
-    if (v < 0.0) {
-      throw new IllegalArgumentException("reclaimDeletesWeight must be >= 0.0 (got " + v + ")");
+  /** Controls the maximum percentage of deleted documents that is tolerated in
+   *  the index. Lower values make the index more space efficient at the
+   *  expense of increased CPU and I/O activity. Values must be between 20 and
+   *  50. Default value is 33. */
+  public TieredMergePolicy setDeletesPctAllowed(double v) {
+    if (v < 20 || v > 50) {
+      throw new IllegalArgumentException("indexPctDeletedTarget must be >= 20.0 and <= 50 (got " + v + ")");
     }
-    reclaimDeletesWeight = v;
+    deletesPctAllowed = v;
     return this;
   }
 
-  /** See {@link #setReclaimDeletesWeight}. */
-  public double getReclaimDeletesWeight() {
-    return reclaimDeletesWeight;
+  /** Returns the current maxMergedSegmentMB setting.
+   *
+   * @see #setMaxMergedSegmentMB */
+  public double getMaxMergedSegmentMB() {
+    return maxMergedSegmentBytes/1024/1024.;
   }
 
   /** Segments smaller than this are "rounded up" to this
@@ -361,6 +337,9 @@ public class TieredMergePolicy extends MergePolicy {
       if (merging.contains(segSizeDocs.segInfo)) {
         mergingBytes += segSizeDocs.sizeInBytes;
         iter.remove();
+        // if this segment is merging, then its deletes are being reclaimed already.
+        // only count live docs in the total max doc
+        totalMaxDoc += segSizeDocs.maxDoc - segSizeDocs.delCount;
       } else {
         totalDelDocs += segSizeDocs.delCount;
         totalMaxDoc += segSizeDocs.maxDoc;
@@ -372,12 +351,11 @@ public class TieredMergePolicy extends MergePolicy {
     assert totalMaxDoc >= 0;
     assert totalDelDocs >= 0;
 
+    final double totalDelPct = 100 * (double) totalDelDocs / totalMaxDoc;
+    int allowedDelCount = (int) (deletesPctAllowed * totalMaxDoc / 100);
+
     // If we have too-large segments, grace them out of the maximum segment count
-    // If we're above certain thresholds, we can merge very large segments.
-    double totalDelPct = (double) totalDelDocs / (double) totalMaxDoc;
-    //TODO: See LUCENE-8263
-    //double targetAsPct = indexPctDeletedTarget / 100.0;
-    double targetAsPct = 0.5;
+    // If we're above certain thresholds of deleted docs, we can merge very large segments.
     int tooBigCount = 0;
     iter = sortedInfos.iterator();
 
@@ -387,13 +365,15 @@ public class TieredMergePolicy extends MergePolicy {
 
     while (iter.hasNext()) {
       SegmentSizeAndDocs segSizeDocs = iter.next();
-      double segDelPct = (double) segSizeDocs.delCount / (double) segSizeDocs.maxDoc;
-      if (segSizeDocs.sizeInBytes > maxMergedSegmentBytes / 2 && (totalDelPct < targetAsPct || segDelPct < targetAsPct)) {
+      double segDelPct = 100 * (double) segSizeDocs.delCount / (double) segSizeDocs.maxDoc;
+      if (segSizeDocs.sizeInBytes > maxMergedSegmentBytes / 2 && (totalDelPct <= deletesPctAllowed || segDelPct <= deletesPctAllowed)) {
         iter.remove();
         tooBigCount++; // Just for reporting purposes.
         totIndexBytes -= segSizeDocs.sizeInBytes;
+        allowedDelCount -= segSizeDocs.delCount;
       }
     }
+    allowedDelCount = Math.max(0, allowedDelCount);
 
     final int mergeFactor = (int) Math.min(maxMergeAtOnce, segsPerTier);
     // Compute max allowed segments in the index
@@ -418,14 +398,14 @@ public class TieredMergePolicy extends MergePolicy {
       message("  allowedSegmentCount=" + allowedSegCount + " vs count=" + infos.size() +
           " (eligible count=" + sortedInfos.size() + ") tooBigCount= " + tooBigCount, mergeContext);
     }
-    return doFindMerges(sortedInfos, maxMergedSegmentBytes, mergeFactor, (int) allowedSegCount, MERGE_TYPE.NATURAL,
+    return doFindMerges(sortedInfos, maxMergedSegmentBytes, mergeFactor, (int) allowedSegCount, allowedDelCount, MERGE_TYPE.NATURAL,
         mergeContext, mergingBytes >= maxMergedSegmentBytes);
   }
 
   private MergeSpecification doFindMerges(List<SegmentSizeAndDocs> sortedEligibleInfos,
                                           final long maxMergedSegmentBytes,
                                           final int mergeFactor, final int allowedSegCount,
-                                          final MERGE_TYPE mergeType,
+                                          final int allowedDelCount, final MERGE_TYPE mergeType,
                                           MergeContext mergeContext,
                                           boolean maxMergeIsRunning) throws IOException {
 
@@ -477,7 +457,10 @@ public class TieredMergePolicy extends MergePolicy {
         return spec;
       }
 
-      if (allowedSegCount != Integer.MAX_VALUE && sortedEligible.size() <= allowedSegCount && mergeType == MERGE_TYPE.NATURAL) {
+      final int remainingDelCount = sortedEligible.stream().mapToInt(c -> c.delCount).sum();
+      if (mergeType == MERGE_TYPE.NATURAL &&
+          sortedEligible.size() <= allowedSegCount &&
+          remainingDelCount <= allowedDelCount) {
         return spec;
       }
 
@@ -487,17 +470,7 @@ public class TieredMergePolicy extends MergePolicy {
       boolean bestTooLarge = false;
       long bestMergeBytes = 0;
 
-      // Consider all merge starts.
-      int lim = sortedEligible.size() - mergeFactor; // assume the usual case of background merging.
-
-      if (mergeType != MERGE_TYPE.NATURAL) { // The unusual case of forceMerge or expungeDeletes.
-        // The incoming eligible list will have only segments with > forceMergeDeletesPctAllowed in the case of
-        // findForcedDeletesMerges and segments with < max allowed size in the case of optimize.
-        // If forcing, we must allow singleton merges.
-        lim = sortedEligible.size() - 1;
-      }
-
-      for (int startIdx = 0; startIdx <= lim; startIdx++) {
+      for (int startIdx = 0; startIdx < sortedEligible.size(); startIdx++) {
 
         long totAfterMergeBytes = 0;
 
@@ -540,6 +513,16 @@ public class TieredMergePolicy extends MergePolicy {
           }
         }
 
+        // If we didn't find a too-large merge and have a list of candidates
+        // whose length is less than the merge factor, it means we are reaching
+        // the tail of the list of segments and will only find smaller merges.
+        // Stop here.
+        if (bestScore != null &&
+            hitTooLarge == false &&
+            candidate.size() < mergeFactor) {
+          break;
+        }
+
         final MergeScore score = score(candidate, hitTooLarge, segInfosSizes);
         if (verbose(mergeContext)) {
           message("  maybe=" + segString(mergeContext, candidate) + " score=" + score.getScore() + " " + score.getExplanation() + " tooLarge=" + hitTooLarge + " size=" + String.format(Locale.ROOT, "%.3f MB", totAfterMergeBytes/1024./1024.), mergeContext);
@@ -621,7 +604,7 @@ public class TieredMergePolicy extends MergePolicy {
 
     // Strongly favor merges that reclaim deletes:
     final double nonDelRatio = ((double) totAfterMergeBytes)/totBeforeMergeBytes;
-    mergeScore *= Math.pow(nonDelRatio, reclaimDeletesWeight);
+    mergeScore *= Math.pow(nonDelRatio, 2);
 
     final double finalMergeScore = mergeScore;
 
@@ -736,7 +719,7 @@ public class TieredMergePolicy extends MergePolicy {
     }
 
     MergeSpecification spec = doFindMerges(sortedSizeAndDocs, maxMergeBytes, maxMergeAtOnceExplicit,
-        maxSegmentCount, MERGE_TYPE.FORCE_MERGE, mergeContext, false);
+        maxSegmentCount, 0, MERGE_TYPE.FORCE_MERGE, mergeContext, false);
 
     return spec;
   }
@@ -781,7 +764,7 @@ public class TieredMergePolicy extends MergePolicy {
       message("eligible=" + sortedInfos, mergeContext);
     }
     return doFindMerges(sortedInfos, maxMergedSegmentBytes,
-        maxMergeAtOnceExplicit, Integer.MAX_VALUE, MERGE_TYPE.FORCE_MERGE_DELETES, mergeContext, false);
+        maxMergeAtOnceExplicit, Integer.MAX_VALUE, 0, MERGE_TYPE.FORCE_MERGE_DELETES, mergeContext, false);
 
   }
 
@@ -800,9 +783,7 @@ public class TieredMergePolicy extends MergePolicy {
     sb.append("segmentsPerTier=").append(segsPerTier).append(", ");
     sb.append("maxCFSSegmentSizeMB=").append(getMaxCFSSegmentSizeMB()).append(", ");
     sb.append("noCFSRatio=").append(noCFSRatio).append(", ");
-    sb.append("reclaimDeletesWeight=").append(reclaimDeletesWeight);
-    //TODO: See LUCENE-8263
-    //sb.append("indexPctDeletedTarget=").append(indexPctDeletedTarget);
+    sb.append("deletesPctAllowed=").append(deletesPctAllowed);
     return sb.toString();
   }
 }
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestBinaryDocValuesUpdates.java b/lucene/core/src/test/org/apache/lucene/index/TestBinaryDocValuesUpdates.java
index e9942e4..a6632d8 100644
--- a/lucene/core/src/test/org/apache/lucene/index/TestBinaryDocValuesUpdates.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestBinaryDocValuesUpdates.java
@@ -286,7 +286,8 @@ public class TestBinaryDocValuesUpdates extends LuceneTestCase {
   public void testUpdatesWithDeletes() throws Exception {
     // update and delete different documents in the same commit session
     Directory dir = newDirectory();
-    IndexWriterConfig conf = newIndexWriterConfig(new MockAnalyzer(random()));
+    IndexWriterConfig conf = newIndexWriterConfig(new MockAnalyzer(random()))
+        .setMergePolicy(NoMergePolicy.INSTANCE); // otherwise the delete might force a merge
     conf.setMaxBufferedDocs(10); // control segment flushing
     IndexWriter writer = new IndexWriter(dir, conf);
     
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestDirectoryReaderReopen.java b/lucene/core/src/test/org/apache/lucene/index/TestDirectoryReaderReopen.java
index 468e8e2..8d1b99a 100644
--- a/lucene/core/src/test/org/apache/lucene/index/TestDirectoryReaderReopen.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestDirectoryReaderReopen.java
@@ -690,7 +690,7 @@ public class TestDirectoryReaderReopen extends LuceneTestCase {
   public void testNPEAfterInvalidReindex1() throws Exception {
     Directory dir = new RAMDirectory();
 
-    IndexWriter w = new IndexWriter(dir, new IndexWriterConfig(new MockAnalyzer(random())));
+    IndexWriter w = new IndexWriter(dir, new IndexWriterConfig(new MockAnalyzer(random())).setMergePolicy(NoMergePolicy.INSTANCE));
     Document doc = new Document();
     doc.add(newStringField("id", "id", Field.Store.NO));
     w.addDocument(doc);
@@ -737,7 +737,7 @@ public class TestDirectoryReaderReopen extends LuceneTestCase {
   public void testNPEAfterInvalidReindex2() throws Exception {
     Directory dir = new RAMDirectory();
 
-    IndexWriter w = new IndexWriter(dir, new IndexWriterConfig(new MockAnalyzer(random())));
+    IndexWriter w = new IndexWriter(dir, new IndexWriterConfig(new MockAnalyzer(random())).setMergePolicy(NoMergePolicy.INSTANCE));
     Document doc = new Document();
     doc.add(newStringField("id", "id", Field.Store.NO));
     w.addDocument(doc);
@@ -991,7 +991,7 @@ public class TestDirectoryReaderReopen extends LuceneTestCase {
     }
  
     w = new IndexWriter(dir,
-                        new IndexWriterConfig(new MockAnalyzer(random())));
+                        new IndexWriterConfig(new MockAnalyzer(random())).setMergePolicy(NoMergePolicy.INSTANCE));
     doc = new Document();
     doc.add(newStringField("field", "value", Field.Store.NO));
     w.addDocument(doc);
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestIndexWriterDelete.java b/lucene/core/src/test/org/apache/lucene/index/TestIndexWriterDelete.java
index 660f88b..2f15cbb 100644
--- a/lucene/core/src/test/org/apache/lucene/index/TestIndexWriterDelete.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestIndexWriterDelete.java
@@ -1146,6 +1146,7 @@ public class TestIndexWriterDelete extends LuceneTestCase {
   public void testDeletesCheckIndexOutput() throws Exception {
     Directory dir = newDirectory();
     IndexWriterConfig iwc = new IndexWriterConfig(new MockAnalyzer(random()));
+    iwc.setMergePolicy(NoMergePolicy.INSTANCE);
     iwc.setMaxBufferedDocs(2);
     IndexWriter w = new IndexWriter(dir, iwc);
     Document doc = new Document();
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestNumericDocValuesUpdates.java b/lucene/core/src/test/org/apache/lucene/index/TestNumericDocValuesUpdates.java
index f6a328a..c6b9340 100644
--- a/lucene/core/src/test/org/apache/lucene/index/TestNumericDocValuesUpdates.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestNumericDocValuesUpdates.java
@@ -375,6 +375,7 @@ public class TestNumericDocValuesUpdates extends LuceneTestCase {
     // update and delete different documents in the same commit session
     Directory dir = newDirectory();
     IndexWriterConfig conf = newIndexWriterConfig(new MockAnalyzer(random()));
+    conf.setMergePolicy(NoMergePolicy.INSTANCE); // otherwise a singleton merge could get rid of the delete
     conf.setMaxBufferedDocs(10); // control segment flushing
     IndexWriter writer = new IndexWriter(dir, conf);
     
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestTieredMergePolicy.java b/lucene/core/src/test/org/apache/lucene/index/TestTieredMergePolicy.java
index 01432c5..02a919f 100644
--- a/lucene/core/src/test/org/apache/lucene/index/TestTieredMergePolicy.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestTieredMergePolicy.java
@@ -46,8 +46,12 @@ public class TestTieredMergePolicy extends BaseMergePolicyTestCase {
     final long maxMergedSegmentBytes = (long) (tmp.getMaxMergedSegmentMB() * 1024 * 1024);
 
     long minSegmentBytes = Long.MAX_VALUE;
+    int totalDelCount = 0;
+    int totalMaxDoc = 0;
     long totalBytes = 0;
     for (SegmentCommitInfo sci : infos) {
+      totalDelCount += sci.getDelCount();
+      totalMaxDoc += sci.info.maxDoc();
       long byteSize = sci.sizeInBytes();
       double liveRatio = 1 - (double) sci.getDelCount() / sci.info.maxDoc();
       long weightedByteSize = (long) Math.round(liveRatio * byteSize);
@@ -55,6 +59,10 @@ public class TestTieredMergePolicy extends BaseMergePolicyTestCase {
       minSegmentBytes = Math.min(minSegmentBytes, weightedByteSize);
     }
 
+    final double delPercentage = 100.0 * totalDelCount / totalMaxDoc;
+    assertTrue("Percentage of deleted docs " + delPercentage + " is larger than the target: " + tmp.getDeletesPctAllowed(),
+        delPercentage <= tmp.getDeletesPctAllowed());
+
     long levelSize = Math.max(minSegmentBytes, (long) (tmp.getFloorSegmentMB() * 1024 * 1024));
     long bytesLeft = totalBytes;
     double allowedSegCount = 0;
@@ -63,7 +71,7 @@ public class TestTieredMergePolicy extends BaseMergePolicyTestCase {
     int mergeFactor = (int) Math.min(tmp.getSegmentsPerTier(), tmp.getMaxMergeAtOnce());
     while (true) {
       final double segCountLevel = bytesLeft / (double) levelSize;
-      if (segCountLevel < tmp.getSegmentsPerTier() || levelSize > maxMergedSegmentBytes / 2) {
+      if (segCountLevel < tmp.getSegmentsPerTier() || levelSize >= maxMergedSegmentBytes / 2) {
         allowedSegCount += Math.ceil(segCountLevel);
         break;
       }
@@ -80,8 +88,9 @@ public class TestTieredMergePolicy extends BaseMergePolicyTestCase {
   @Override
   protected void assertMerge(MergePolicy policy, MergeSpecification merges) {
     TieredMergePolicy tmp = (TieredMergePolicy) policy;
+    final int mergeFactor = (int) Math.min(tmp.getMaxMergeAtOnce(), tmp.getSegmentsPerTier());
     for (OneMerge merge : merges.merges) {
-      assertTrue(merge.segments.size() <= tmp.getMaxMergeAtOnce());
+      assertTrue(merge.segments.size() <= mergeFactor);
     }
   }
 
@@ -93,6 +102,7 @@ public class TestTieredMergePolicy extends BaseMergePolicyTestCase {
     conf.setMaxBufferedDocs(4);
     tmp.setMaxMergeAtOnce(100);
     tmp.setSegmentsPerTier(100);
+    tmp.setDeletesPctAllowed(50.0);
     tmp.setForceMergeDeletesPctAllowed(30.0);
     IndexWriter w = new IndexWriter(dir, conf);
     for(int i=0;i<80;i++) {
@@ -572,6 +582,27 @@ public class TestTieredMergePolicy extends BaseMergePolicyTestCase {
     assertEquals(10, merge.segments.size());
   }
 
+  /**
+   * Make sure that singleton merges are considered when the max number of deletes is crossed.
+   */
+  public void testMergePurelyToReclaimDeletes() throws IOException {
+    TieredMergePolicy mergePolicy = mergePolicy();
+    SegmentInfos infos = new SegmentInfos(Version.LATEST.major);
+    // single 1GB segment with no deletes
+    infos.add(makeSegmentCommitInfo("_0", 1_000_000, 0, 1024, IndexWriter.SOURCE_MERGE));
+
+    // not eligible for merging
+    assertNull(mergePolicy.findMerges(MergeTrigger.EXPLICIT, infos, new MockMergeContext(SegmentCommitInfo::getDelCount)));
+
+    // introduce 15% deletes, still not eligible
+    infos = applyDeletes(infos, (int) (0.15 * 1_000_000));
+    assertNull(mergePolicy.findMerges(MergeTrigger.EXPLICIT, infos, new MockMergeContext(SegmentCommitInfo::getDelCount)));
+
+    // now cross the delete threshold, becomes eligible
+    infos = applyDeletes(infos, (int) ((mergePolicy.getDeletesPctAllowed() - 15 + 1) / 100 * 1_000_000));
+    assertNotNull(mergePolicy.findMerges(MergeTrigger.EXPLICIT, infos, new MockMergeContext(SegmentCommitInfo::getDelCount)));
+  }
+
   @Override
   public void testSimulateAppendOnly() throws IOException {
     TieredMergePolicy mergePolicy = mergePolicy();
@@ -587,4 +618,5 @@ public class TestTieredMergePolicy extends BaseMergePolicyTestCase {
     mergePolicy.setMaxMergedSegmentMB(TestUtil.nextInt(random(), 1024, 10 * 1024));
     doTestSimulateUpdates(mergePolicy, 10_000_000, 2500);
   }
+
 }
diff --git a/lucene/test-framework/src/java/org/apache/lucene/index/BaseMergePolicyTestCase.java b/lucene/test-framework/src/java/org/apache/lucene/index/BaseMergePolicyTestCase.java
index 8ec9248..c922f02 100644
--- a/lucene/test-framework/src/java/org/apache/lucene/index/BaseMergePolicyTestCase.java
+++ b/lucene/test-framework/src/java/org/apache/lucene/index/BaseMergePolicyTestCase.java
@@ -359,6 +359,7 @@ public abstract class BaseMergePolicyTestCase extends LuceneTestCase {
 
       MergeSpecification merges = mergePolicy.findMerges(MergeTrigger.SEGMENT_FLUSH, segmentInfos, mergeContext);
       while (merges != null) {
+        assertTrue(merges.merges.size() > 0);
         assertMerge(mergePolicy, merges);
         for (OneMerge oneMerge : merges.merges) {
           segmentInfos = applyMerge(segmentInfos, oneMerge, "_" + segNameGenerator.getAndIncrement(), stats);
diff --git a/lucene/test-framework/src/java/org/apache/lucene/util/LuceneTestCase.java b/lucene/test-framework/src/java/org/apache/lucene/util/LuceneTestCase.java
index d6c0e7e..6be8d03 100644
--- a/lucene/test-framework/src/java/org/apache/lucene/util/LuceneTestCase.java
+++ b/lucene/test-framework/src/java/org/apache/lucene/util/LuceneTestCase.java
@@ -1131,7 +1131,7 @@ public abstract class LuceneTestCase extends Assert {
       tmp.setSegmentsPerTier(TestUtil.nextInt(r, 10, 50));
     }
     configureRandom(r, tmp);
-    tmp.setReclaimDeletesWeight(r.nextDouble()*4);
+    tmp.setDeletesPctAllowed(20 + random().nextDouble() * 30);
     return tmp;
   }
 
@@ -1266,7 +1266,7 @@ public abstract class LuceneTestCase extends Assert {
           tmp.setSegmentsPerTier(TestUtil.nextInt(r, 10, 50));
         }
         configureRandom(r, tmp);
-        tmp.setReclaimDeletesWeight(r.nextDouble()*4);
+        tmp.setDeletesPctAllowed(20 + random().nextDouble() * 30);
       }
       didChange = true;
     }
