diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/WordDelimiterFilter.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/WordDelimiterFilter.java
index 4fa2640..ff79304 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/WordDelimiterFilter.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/WordDelimiterFilter.java
@@ -179,6 +179,13 @@ public final class WordDelimiterFilter extends TokenFilter {
   // used for catenate all
   private final WordDelimiterConcatenation concatAll = new WordDelimiterConcatenation();
 
+  /** The following variable is used to remember the original token length
+   * generated, in order to prevent from generating it again later (because of
+   * CATENATE_WORDS|CATENATE_ALL)
+   * We ensure that !has(PRESERVE_ORIGINAL) => original_token_length = -1
+   */
+  private int original_token_length = -1;
+
   // used for accumulating position increment gaps
   private int accumPosInc = 0;
 
@@ -251,7 +258,7 @@ public final class WordDelimiterFilter extends TokenFilter {
           return true;
         }
         
-        // word of simply delimiters
+        // word of simply delimiters that we should skip if not PRESERVE_ORIGINAL
         if (iterator.end == WordDelimiterIterator.DONE && !has(PRESERVE_ORIGINAL)) {
           // if the posInc is 1, simply ignore it in the accumulation
           // TODO: proper hole adjustment (FilteringTokenFilter-like) instead of this previous logic!
@@ -271,6 +278,7 @@ public final class WordDelimiterFilter extends TokenFilter {
           posIncAttribute.setPositionIncrement(accumPosInc);
           accumPosInc = 0;
           first = false;
+          original_token_length = termLength;
           return true;
         }
       }
@@ -287,9 +295,10 @@ public final class WordDelimiterFilter extends TokenFilter {
         if (!concatAll.isEmpty()) {
           // only if we haven't output this same combo above!
           if (concatAll.subwordCount > lastConcatCount) {
-            concatAll.writeAndClear();
-            buffer();
-            continue;
+            if (flushConcatenation(concatAll)) {
+              buffer();
+              continue;
+            }
           }
           concatAll.clear();
         }
@@ -365,6 +374,7 @@ public final class WordDelimiterFilter extends TokenFilter {
     concatAll.clear();
     accumPosInc = bufferedPos = bufferedLen = 0;
     first = true;
+    original_token_length = -1;
   }
 
   // ================================================= Helper Methods ================================================
@@ -442,12 +452,21 @@ public final class WordDelimiterFilter extends TokenFilter {
   /**
    * Flushes the given WordDelimiterConcatenation by either writing its concat and then clearing, or just clearing.
    *
+   * To prevent duplicates from PRESERVE_ORIGINAL and (SPLIT_ON_CASE_CHANGE|CATENATE_ALL),
+   * we check that the created token is not of the same length as the last preserved
+   * original token.
+   *
    * @param concatenation WordDelimiterConcatenation that will be flushed
    * @return {@code true} if the concatenation was written before it was cleared, {@code false} otherwise
    */
   private boolean flushConcatenation(WordDelimiterConcatenation concatenation) {
     lastConcatCount = concatenation.subwordCount;
-    if (concatenation.subwordCount != 1 || !shouldGenerateParts(concatenation.type)) {
+    /** We remind the reader that:
+     * 1) !has(PRESERVE_ORIGINAL) => original_token_length = -1
+     *  so (concatenation.lenght() != original_token_length) also checks has(PRESERVE_ORIGINAL)
+     */
+    if ((concatenation.subwordCount != 1 || !shouldGenerateParts(concatenation.type))
+        && (concatenation.length() != original_token_length) ){
       concatenation.writeAndClear();
       return true;
     }
@@ -603,6 +622,9 @@ public final class WordDelimiterFilter extends TokenFilter {
     int type;
     int subwordCount;
 
+    int length() {
+      return buffer.length();
+    }
     /**
      * Appends the given text of the given length, to the concetenation at the given offset
      *
diff --git a/lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestWordDelimiterFilter.java b/lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestWordDelimiterFilter.java
index 235970b..8d6b463 100644
--- a/lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestWordDelimiterFilter.java
+++ b/lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestWordDelimiterFilter.java
@@ -388,9 +388,9 @@ public class TestWordDelimiterFilter extends BaseTokenStreamTestCase {
     doSplit("abcDef", SPLIT_ON_CASE_CHANGE | GENERATE_WORD_PARTS, "abc", "Def");
     doSplit("abcDef", SPLIT_ON_CASE_CHANGE | GENERATE_WORD_PARTS | CATENATE_ALL, "abc", "abcDef", "Def");
     /* The following is bugued? Should be "abcDef" */
-    doSplit("abcDef", SPLIT_ON_CASE_CHANGE | PRESERVE_ORIGINAL | CATENATE_ALL, "abcDef", "abcDef");
+    doSplit("abcDef", SPLIT_ON_CASE_CHANGE | PRESERVE_ORIGINAL | CATENATE_ALL, "abcDef");
     /* The following is bugued? Should not be "abcDef", "abc", "Def" */
-    doSplit("abcDef", SPLIT_ON_CASE_CHANGE | PRESERVE_ORIGINAL | CATENATE_ALL | GENERATE_WORD_PARTS, "abcDef", "abc", "abcDef", "Def");
+    doSplit("abcDef", SPLIT_ON_CASE_CHANGE | PRESERVE_ORIGINAL | CATENATE_ALL | GENERATE_WORD_PARTS, "abcDef", "abc", "Def");
   }
 
   public void doSplitPossessive(int stemPossessive, final String input, final String... output) throws Exception {
@@ -410,7 +410,7 @@ public class TestWordDelimiterFilter extends BaseTokenStreamTestCase {
     doSplitPossessive(0, "ra's", "ra", "s");
   }
 
-  /** We should not generate the same token twice with PRESERVE_ORIGINAL and CONCATENATE_ALL|CATENATE_WORDS */
+  /** We should not generate the same token twice with PRESERVE_ORIGINAL and SPLIT_ON_CASE_CHANGE|GENERATE_WORD_PARTS */
   public void checkNoDuplicates(String input_string) throws Exception {
     final int flags = PRESERVE_ORIGINAL | GENERATE_WORD_PARTS | GENERATE_NUMBER_PARTS | CATENATE_WORDS | CATENATE_NUMBERS | CATENATE_ALL | SPLIT_ON_CASE_CHANGE | SPLIT_ON_NUMERICS | STEM_ENGLISH_POSSESSIVE; 
 
@@ -445,7 +445,7 @@ public class TestWordDelimiterFilter extends BaseTokenStreamTestCase {
    * duplicate tokens.
    */
   public void testNoDuplicates() throws Exception  {
-    // checkNoDuplicates("abcDef abcDef");
+    checkNoDuplicates("abcDef abcDef");
   }
   /*
    * Set a large position increment gap of 10 if the token is "largegap" or "/"
