
Property changes on: .
___________________________________________________________________
Modified: svn:mergeinfo
   Merged /lucene/java/branches/lucene_2_9:r909334
   Merged /lucene/java/trunk:r889614-889621,889623-889666,889668-889865,899002-916543

Index: common-build.xml
===================================================================
--- common-build.xml	(revision 916567)
+++ common-build.xml	(working copy)
@@ -113,6 +113,9 @@
   <property name="svnversion.exe" value="svnversion" />
   <property name="svn.exe" value="svn" />
   
+  <property name="gpg.exe" value="gpg" />
+  <property name="gpg.key" value="CODE SIGNING KEY" />
+
   <condition property="build-1-5-contrib">
      <equals arg1="1.5" arg2="${ant.java.version}" />
   </condition>
@@ -633,8 +636,10 @@
           doctitle="@{title}"
           maxmemory="${javadoc.maxmemory}"
           bottom="Copyright &amp;copy; ${year} Apache Software Foundation.  All Rights Reserved.">
-        <tag name="todo" description="To Do:"/>
-        <tag name="uml.property" description="UML Property:"/>
+        <tag name="lucene.experimental" 
+      	description="WARNING: This API is experimental and might change in incompatible ways in the next release."/>
+        <tag name="lucene.internal"
+        description="NOTE: This API is for Lucene internal purposes only and might change in incompatible ways in the next release."/>
       	<link offline="true" packagelistLoc="${javadoc.dir}"/>
       	
       	<sources />
Index: CHANGES.txt
===================================================================
--- CHANGES.txt	(revision 916567)
+++ CHANGES.txt	(working copy)
@@ -1,12 +1,26 @@
 Lucene Change Log
-$Id$
 
-Bug fixes
+======================= Flexible Indexing Branch =======================
 
- * LUCENE-2222: FixedIntBlockIndexInput incorrectly read one block of
-   0s before the actual data.  (Renaud Delbru via Mike McCandless)
+Changes in backwards compatibility policy
 
+* LUCENE-2111: UnicodeUtil now uses BytesRef for UTF-8 output, and
+  some method signatures have changed to CharSequence.  These are
+  advanced APIs and subject to change suddenly.
+  (Robert Muir, Mike McCandless)
+
+* LUCENE-1458: Flex API changes:
+    Directory.copy now copies all files (not just index files), since
+    what is and isn't and index file is now dependent on the codec
+    used. (Mike McCandless)
+
+Bug Fixes
+
+* LUCENE-2222: FixedIntBlockIndexInput incorrectly read one block of
+  0s before the actual data.  (Renaud Delbru via Mike McCandless)
+
 ======================= Trunk (not yet released) =======================
+
 Changes in backwards compatibility policy
 
 * LUCENE-1483: Removed utility class oal.util.SorterTemplate; this
@@ -25,16 +39,9 @@
   toString.  These are advanced APIs and subject to change suddenly.
   (Tim Smith via Mike McCandless)
 
-* LUCENE-2111: UnicodeUtil now uses BytesRef for UTF-8 output, and
-  some method signatures have changed to CharSequence.  These are
-  advanced APIs and subject to change suddenly.
-  (Robert Muir, Mike McCandless)
+* LUCENE-2190: Removed deprecated customScore() and customExplain()
+  methods from experimental CustomScoreQuery.  (Uwe Schindler)
 
-* LUCENE-1458: Flex API changes:
-  - Directory.copy now copies all files (not just index files), since
-    what is and isn't and index file is now dependent on the codec
-    used.
-
 Changes in runtime behavior
 
 * LUCENE-1923: Made IndexReader.toString() produce something
@@ -58,58 +65,52 @@
   until Lucene 4.0 the default one will be deprecated.
   (Shai Erera via Uwe Schindler) 
 
-* LUCENE-1609: Restore IndexReader.getTermInfosIndexDivisor (it was
-  accidentally removed in 3.0.0)  (Mike McCandless)
-
-* LUCENE-1972: Restore SortField.getComparatorSource (it was
-  accidentally removed in 3.0.0)  (John Wang via Uwe Schindler)
-
 * LUCENE-2177: Deprecate the Field ctors that take byte[] and Store.
   Since the removal of compressed fields, Store can only be YES, so
   it's not necessary to specify.  (Erik Hatcher via Mike McCandless)
 
-* LUCENE-2190: Added setNextReader method to CustomScoreQuery, which
-  is necessary with per-segment searching to notify the subclass
-  which reader the int doc, passed to customScore, refers to.  (Paul
-  chez Jamespot via Mike McCandless)
-
 * LUCENE-2200: Several final classes had non-overriding protected
   members. These were converted to private and unused protected
   constructors removed.  (Steven Rowe via Robert Muir)
 
-Bug fixes
+* LUCENE-2240: SimpleAnalyzer and WhitespaceAnalyzer now have
+  Version ctors.  (Simon Willnauer via Uwe Schindler)
 
-* LUCENE-2092: BooleanQuery was ignoring disableCoord in its hashCode
-  and equals methods, cause bad things to happen when caching
-  BooleanQueries.  (Chris Hostetter, Mike McCandless)
+* LUCENE-2259: Add IndexWriter.deleteUnusedFiles, to attempt removing
+  unused files.  This is only useful on Windows, which prevents
+  deletion of open files. IndexWriter will eventually remove these
+  files itself; this method just lets you do so when you know the
+  files are no longer open by IndexReaders. (luocanrao via Mike
+  McCandless)
 
-* LUCENE-2095: Fixes: when two threads call IndexWriter.commit() at
-  the same time, it's possible for commit to return control back to
-  one of the threads before all changes are actually committed.
-  (Sanne Grinovero via Mike McCandless)
+* LUCENE-2281: added doBeforeFlush to IndexWriter to allow extensions to perform
+  operations before flush starts. Also exposed doAfterFlush as protected instead
+  of package-private. (Shai Erera via Mike McCandless)
 
+Bug fixes
+
 * LUCENE-2119: Don't throw NegativeArraySizeException if you pass
   Integer.MAX_VALUE as nDocs to IndexSearcher search methods.  (Paul
   Taylor via Mike McCandless)
 
-* LUCENE-2132: Fix the demo result.jsp to use QueryParser with a 
-  Version argument.  (Brian Li via Robert Muir)
-
 * LUCENE-2142: FieldCacheImpl.getStringIndex no longer throws an
   exception when term count exceeds doc count.  (Mike McCandless)
 
-* LUCENE-2166: Don't incorrectly keep warning about the same immense
-  term, when IndexWriter.infoStream is on.  (Mike McCandless)
-
 * LUCENE-2104: NativeFSLock.release() would silently fail if the lock is held by 
   another thread/process.  (Shai Erera via Uwe Schindler)
 
-* LUCENE-2158: At high indexing rates, NRT reader could temporarily
-  lose deletions.  (Mike McCandless)
+* LUCENE-2216: OpenBitSet.hashCode returned different hash codes for
+  sets that only differed by trailing zeros. (Dawid Weiss, yonik)
+
+* LUCENE-2235: Implement missing PerFieldAnalyzerWrapper.getOffsetGap().
+  (Javier Godoy via Uwe Schindler)
+
+* LUCENE-2249: ParallelMultiSearcher should shut down thread pool on
+  close.  (Martin Traverso via Uwe Schindler)
   
-* LUCENE-2182: DEFAULT_ATTRIBUTE_FACTORY was failing to load
-  implementation class when interface was loaded by a different
-  class loader.  (Uwe Schindler, reported on java-user by Ahmed El-dawy)
+* LUCENE-2273: FieldCacheImpl.getCacheEntries() used WeakHashMap
+  incorrectly and lead to ConcurrentModificationException.
+  (Uwe Schindler, Robert Muir)
   
 New features
 
@@ -135,11 +136,20 @@
   stopwords, and implement many analyzers in contrib with it.  
   (Simon Willnauer via Robert Muir)
   
+* LUCENE-2198: Support protected words in stemming TokenFilters using a
+  new KeywordAttribute.  (Simon Willnauer via Uwe Schindler)
+  
+* LUCENE-2183, LUCENE-2240, LUCENE-2241: Added Unicode 4 support
+  to CharTokenizer and its subclasses. CharTokenizer now has new
+  int-API which is conditionally preferred to the old char-API depending
+  on the provided Version. Version < 3.1 will use the char-API.
+  (Simon Willnauer via Uwe Schindler)
+
+* LUCENE-2247: Added a CharArrayMap<V> for performance improvements
+  in some stemmers and synonym filters. (Uwe Schindler)
+
 Optimizations
 
-* LUCENE-2086: When resolving deleted terms, do so in term sort order
-  for better performance. (Bogdan Ghidireac via Mike McCandless)
-
 * LUCENE-2075: Terms dict cache is now shared across threads instead
   of being stored separately in thread local storage.  Also fixed
   terms dict so that the cache is used when seeking the thread local
@@ -155,13 +165,12 @@
 * LUCENE-2137: Switch to AtomicInteger for some ref counting (Earwin
   Burrfoot via Mike McCandless)
 
-* LUCENE-2123: Move FuzzyQuery rewrite as separate RewriteMode into
-  MTQ. This also fixes a slowdown / memory issue added by LUCENE-504.
+* LUCENE-2123, LUCENE-2261: Move FuzzyQuery rewrite to separate RewriteMode 
+  into MultiTermQuery. The number of fuzzy expansions can be specified with
+  the maxExpansions parameter to FuzzyQuery, but the default is limited to
+  BooleanQuery.maxClauseCount() as before. 
   (Uwe Schindler, Robert Muir, Mike McCandless)
 
-* LUCENE-2137: Switch to AtomicInteger for some ref counting (Earwin
-  Burrfoot via Mike McCandless)
-
 * LUCENE-2135: On IndexReader.close, forcefully evict any entries from
   the FieldCache rather than waiting for the WeakHashMap to release
   the reference (Mike McCandless)
@@ -188,6 +197,9 @@
 * LUCENE-2188: Add a utility class for tracking deprecated overridden
   methods in non-final subclasses.
   (Uwe Schindler, Robert Muir)
+
+* LUCENE-2195: Speedup CharArraySet if set is empty.
+  (Simon Willnauer via Robert Muir)
    
 Build
 
@@ -206,15 +218,118 @@
 * LUCENE-2065: Use Java 5 generics throughout our unit tests.  (Kay
   Kay via Mike McCandless)
 
-* LUCENE-2114: Change TestFilteredSearch to test on multi-segment
-  index as well; improve javadocs of Filter to call out that the
-  provided reader is per-segment (Simon Willnauer via Mike McCandless)
-
 * LUCENE-2155: Fix time and zone dependent localization test failures
   in queryparser tests. (Uwe Schindler, Chris Male, Robert Muir)
 
 * LUCENE-2170: Fix thread starvation problems.  (Uwe Schindler)
 
+* LUCENE-2248, LUCENE-2251: Refactor tests to not use Version.LUCENE_CURRENT,
+  but instead use a global static value from LuceneTestCase(J4), that
+  contains the release version.  (Uwe Schindler, Simon Willnauer)
+  
+================== Release 2.9.2 / 3.0.1 2010-02-26 ====================
+
+Changes in backwards compatibility policy
+
+* LUCENE-2123 (3.0.1 only): Removed the protected inner class ScoreTerm
+  from FuzzyQuery. The change was needed because the comparator of this
+  class had to be changed in an incompatible way. The class was never
+  intended to be public.  (Uwe Schindler, Mike McCandless)
+  
+Bug fixes
+
+ * LUCENE-2092: BooleanQuery was ignoring disableCoord in its hashCode
+   and equals methods, cause bad things to happen when caching
+   BooleanQueries.  (Chris Hostetter, Mike McCandless)
+
+ * LUCENE-2095: Fixes: when two threads call IndexWriter.commit() at
+   the same time, it's possible for commit to return control back to
+   one of the threads before all changes are actually committed.
+   (Sanne Grinovero via Mike McCandless)
+
+ * LUCENE-2132 (3.0.1 only): Fix the demo result.jsp to use QueryParser
+   with a Version argument.  (Brian Li via Robert Muir)
+
+ * LUCENE-2166: Don't incorrectly keep warning about the same immense
+   term, when IndexWriter.infoStream is on.  (Mike McCandless)
+
+ * LUCENE-2158: At high indexing rates, NRT reader could temporarily
+   lose deletions.  (Mike McCandless)
+  
+ * LUCENE-2182: DEFAULT_ATTRIBUTE_FACTORY was failing to load
+   implementation class when interface was loaded by a different
+   class loader.  (Uwe Schindler, reported on java-user by Ahmed El-dawy)
+
+ * LUCENE-2257: Increase max number of unique terms in one segment to
+   termIndexInterval (default 128) * ~2.1 billion = ~274 billion.
+   (Tom Burton-West via Mike McCandless)
+  
+ * LUCENE-2260: Fixed AttributeSource to not hold a strong
+   reference to the Attribute/AttributeImpl classes which prevents
+   unloading of custom attributes loaded by other classloaders
+   (e.g. in Solr plugins).  (Uwe Schindler)
+ 
+ * LUCENE-1941: Fix Min/MaxPayloadFunction returns 0 when
+   only one payload is present.  (Erik Hatcher, Mike McCandless
+   via Uwe Schindler)
+
+ * LUCENE-2270: Queries consisting of all zero-boost clauses
+   (for example, text:foo^0) sorted incorrectly and produced
+   invalid docids. (yonik)
+
+API Changes
+
+ * LUCENE-1609 (3.0.1 only): Restore IndexReader.getTermInfosIndexDivisor
+   (it was accidentally removed in 3.0.0)  (Mike McCandless)
+
+ * LUCENE-1972 (3.0.1 only): Restore SortField.getComparatorSource
+   (it was accidentally removed in 3.0.0)  (John Wang via Uwe Schindler)
+
+ * LUCENE-2190: Added a new class CustomScoreProvider to function package
+   that can be subclassed to provide custom scoring to CustomScoreQuery.
+   The methods in CustomScoreQuery that did this before were deprecated
+   and replaced by a method getCustomScoreProvider(IndexReader) that
+   returns a custom score implementation using the above class. The change
+   is necessary with per-segment searching, as CustomScoreQuery is
+   a stateless class (like all other Queries) and does not know about
+   the currently searched segment. This API works similar to Filter's
+   getDocIdSet(IndexReader).  (Paul chez Jamespot via Mike McCandless,
+   Uwe Schindler)
+
+ * LUCENE-2080: Deprecate Version.LUCENE_CURRENT, as using this constant
+   will cause backwards compatibility problems when upgrading Lucene. See
+   the Version javadocs for additional information.
+   (Robert Muir)
+
+Optimizations
+
+ * LUCENE-2086: When resolving deleted terms, do so in term sort order
+   for better performance (Bogdan Ghidireac via Mike McCandless)
+
+ * LUCENE-2123 (partly, 3.0.1 only): Fixes a slowdown / memory issue
+   added by LUCENE-504.  (Uwe Schindler, Robert Muir, Mike McCandless)
+
+ * LUCENE-2258: Remove unneeded synchronization in FuzzyTermEnum.
+   (Uwe Schindler, Robert Muir)
+
+Test Cases
+
+ * LUCENE-2114: Change TestFilteredSearch to test on multi-segment
+   index as well. (Simon Willnauer via Mike McCandless)
+
+ * LUCENE-2211: Improves BaseTokenStreamTestCase to use a fake attribute
+   that checks if clearAttributes() was called correctly.
+   (Uwe Schindler, Robert Muir)
+
+ * LUCENE-2207, LUCENE-2219: Improve BaseTokenStreamTestCase to check if 
+   end() is implemented correctly.  (Koji Sekiguchi, Robert Muir)
+
+Documentation
+
+ * LUCENE-2114: Improve javadocs of Filter to call out that the
+   provided reader is per-segment (Simon Willnauer via Mike
+   McCandless)
+ 
 ======================= Release 3.0.0 2009-11-25 =======================
 
 Changes in backwards compatibility policy
@@ -520,10 +635,10 @@
     code to implement this method.  If you already extend
     IndexSearcher, no further changes are needed to use Collector.
     
-    Finally, the values Float.NaN, Float.NEGATIVE_INFINITY and
-    Float.POSITIVE_INFINITY are not valid scores.  Lucene uses these
-    values internally in certain places, so if you have hits with such
-    scores, it will cause problems. (Shai Erera via Mike McCandless)
+    Finally, the values Float.NaN and Float.NEGATIVE_INFINITY are not
+    valid scores.  Lucene uses these values internally in certain
+    places, so if you have hits with such scores, it will cause
+    problems. (Shai Erera via Mike McCandless)
 
  * LUCENE-1687: All methods and parsers from the interface ExtendedFieldCache
     have been moved into FieldCache. ExtendedFieldCache is now deprecated and
@@ -601,7 +716,7 @@
     
  * LUCENE-1575: As of 2.9, the core collectors as well as
     IndexSearcher's search methods that return top N results, no
-    longer filter out zero scoring documents. If you rely on this
+    longer filter documents with scores <= 0.0. If you rely on this
     functionality you can use PositiveScoresOnlyCollector like this:
 
     <code>
Index: src/test/org/apache/lucene/TestMergeSchedulerExternal.java
===================================================================
--- src/test/org/apache/lucene/TestMergeSchedulerExternal.java	(revision 916567)
+++ src/test/org/apache/lucene/TestMergeSchedulerExternal.java	(working copy)
@@ -95,7 +95,7 @@
     Field idField = new Field("id", "", Field.Store.YES, Field.Index.NOT_ANALYZED);
     doc.add(idField);
     
-    IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
+    IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
     MyMergeScheduler ms = new MyMergeScheduler();
     writer.setMergeScheduler(ms);
     writer.setMaxBufferedDocs(2);
Index: src/test/org/apache/lucene/queryParser/TestMultiAnalyzer.java
===================================================================
--- src/test/org/apache/lucene/queryParser/TestMultiAnalyzer.java	(revision 916567)
+++ src/test/org/apache/lucene/queryParser/TestMultiAnalyzer.java	(working copy)
@@ -44,7 +44,7 @@
 
   public void testMultiAnalyzer() throws ParseException {
     
-    QueryParser qp = new QueryParser(Version.LUCENE_CURRENT, "", new MultiAnalyzer());
+    QueryParser qp = new QueryParser(TEST_VERSION_CURRENT, "", new MultiAnalyzer());
 
     // trivial, no multiple tokens:
     assertEquals("foo", qp.parse("foo").toString());
@@ -135,9 +135,9 @@
 
     @Override
     public TokenStream tokenStream(String fieldName, Reader reader) {
-      TokenStream result = new StandardTokenizer(Version.LUCENE_CURRENT, reader);
+      TokenStream result = new StandardTokenizer(TEST_VERSION_CURRENT, reader);
       result = new TestFilter(result);
-      result = new LowerCaseFilter(Version.LUCENE_CURRENT, result);
+      result = new LowerCaseFilter(TEST_VERSION_CURRENT, result);
       return result;
     }
   }
@@ -203,9 +203,9 @@
 
     @Override
     public TokenStream tokenStream(String fieldName, Reader reader) {
-      TokenStream result = new StandardTokenizer(Version.LUCENE_CURRENT, reader);
+      TokenStream result = new StandardTokenizer(TEST_VERSION_CURRENT, reader);
       result = new TestPosIncrementFilter(result);
-      result = new LowerCaseFilter(Version.LUCENE_CURRENT, result);
+      result = new LowerCaseFilter(TEST_VERSION_CURRENT, result);
       return result;
     }
   }
@@ -242,7 +242,7 @@
     private final static class DumbQueryParser extends QueryParser {
         
         public DumbQueryParser(String f, Analyzer a) {
-            super(Version.LUCENE_CURRENT, f, a);
+            super(TEST_VERSION_CURRENT, f, a);
         }
 
         /** expose super's version */
Index: src/test/org/apache/lucene/queryParser/TestMultiFieldQueryParser.java
===================================================================
--- src/test/org/apache/lucene/queryParser/TestMultiFieldQueryParser.java	(revision 916567)
+++ src/test/org/apache/lucene/queryParser/TestMultiFieldQueryParser.java	(working copy)
@@ -36,7 +36,6 @@
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.RAMDirectory;
 import org.apache.lucene.util.LuceneTestCase;
-import org.apache.lucene.util.Version;
 
 /**
  * Tests QueryParser.
@@ -60,18 +59,18 @@
     String[] fields = {"b", "t"};
     Occur occur[] = {Occur.SHOULD, Occur.SHOULD};
     TestQueryParser.QPTestAnalyzer a = new TestQueryParser.QPTestAnalyzer();
-    MultiFieldQueryParser mfqp = new MultiFieldQueryParser(Version.LUCENE_CURRENT, fields, a);
+    MultiFieldQueryParser mfqp = new MultiFieldQueryParser(TEST_VERSION_CURRENT, fields, a);
     
     Query q = mfqp.parse(qtxt);
     assertEquals(expectedRes, q.toString());
     
-    q = MultiFieldQueryParser.parse(Version.LUCENE_CURRENT, qtxt, fields, occur, a);
+    q = MultiFieldQueryParser.parse(TEST_VERSION_CURRENT, qtxt, fields, occur, a);
     assertEquals(expectedRes, q.toString());
   }
   
   public void testSimple() throws Exception {
     String[] fields = {"b", "t"};
-    MultiFieldQueryParser mfqp = new MultiFieldQueryParser(Version.LUCENE_CURRENT, fields, new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT));
+    MultiFieldQueryParser mfqp = new MultiFieldQueryParser(TEST_VERSION_CURRENT, fields, new StandardAnalyzer(TEST_VERSION_CURRENT));
     
     Query q = mfqp.parse("one");
     assertEquals("b:one t:one", q.toString());
@@ -134,7 +133,7 @@
       boosts.put("b", Float.valueOf(5));
       boosts.put("t", Float.valueOf(10));
       String[] fields = {"b", "t"};
-      MultiFieldQueryParser mfqp = new MultiFieldQueryParser(Version.LUCENE_CURRENT, fields, new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT), boosts);
+      MultiFieldQueryParser mfqp = new MultiFieldQueryParser(TEST_VERSION_CURRENT, fields, new StandardAnalyzer(TEST_VERSION_CURRENT), boosts);
       
       
       //Check for simple
@@ -160,24 +159,24 @@
   public void testStaticMethod1() throws ParseException {
     String[] fields = {"b", "t"};
     String[] queries = {"one", "two"};
-    Query q = MultiFieldQueryParser.parse(Version.LUCENE_CURRENT, queries, fields, new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT));
+    Query q = MultiFieldQueryParser.parse(TEST_VERSION_CURRENT, queries, fields, new StandardAnalyzer(TEST_VERSION_CURRENT));
     assertEquals("b:one t:two", q.toString());
 
     String[] queries2 = {"+one", "+two"};
-    q = MultiFieldQueryParser.parse(Version.LUCENE_CURRENT, queries2, fields, new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT));
+    q = MultiFieldQueryParser.parse(TEST_VERSION_CURRENT, queries2, fields, new StandardAnalyzer(TEST_VERSION_CURRENT));
     assertEquals("(+b:one) (+t:two)", q.toString());
 
     String[] queries3 = {"one", "+two"};
-    q = MultiFieldQueryParser.parse(Version.LUCENE_CURRENT, queries3, fields, new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT));
+    q = MultiFieldQueryParser.parse(TEST_VERSION_CURRENT, queries3, fields, new StandardAnalyzer(TEST_VERSION_CURRENT));
     assertEquals("b:one (+t:two)", q.toString());
 
     String[] queries4 = {"one +more", "+two"};
-    q = MultiFieldQueryParser.parse(Version.LUCENE_CURRENT, queries4, fields, new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT));
+    q = MultiFieldQueryParser.parse(TEST_VERSION_CURRENT, queries4, fields, new StandardAnalyzer(TEST_VERSION_CURRENT));
     assertEquals("(b:one +b:more) (+t:two)", q.toString());
 
     String[] queries5 = {"blah"};
     try {
-      q = MultiFieldQueryParser.parse(Version.LUCENE_CURRENT, queries5, fields, new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT));
+      q = MultiFieldQueryParser.parse(TEST_VERSION_CURRENT, queries5, fields, new StandardAnalyzer(TEST_VERSION_CURRENT));
       fail();
     } catch(IllegalArgumentException e) {
       // expected exception, array length differs
@@ -187,11 +186,11 @@
     TestQueryParser.QPTestAnalyzer stopA = new TestQueryParser.QPTestAnalyzer();
     
     String[] queries6 = {"((+stop))", "+((stop))"};
-    q = MultiFieldQueryParser.parse(Version.LUCENE_CURRENT, queries6, fields, stopA);
+    q = MultiFieldQueryParser.parse(TEST_VERSION_CURRENT, queries6, fields, stopA);
     assertEquals("", q.toString());
     
     String[] queries7 = {"one ((+stop)) +more", "+((stop)) +two"};
-    q = MultiFieldQueryParser.parse(Version.LUCENE_CURRENT, queries7, fields, stopA);
+    q = MultiFieldQueryParser.parse(TEST_VERSION_CURRENT, queries7, fields, stopA);
     assertEquals("(b:one +b:more) (+t:two)", q.toString());
 
   }
@@ -199,15 +198,15 @@
   public void testStaticMethod2() throws ParseException {
     String[] fields = {"b", "t"};
     BooleanClause.Occur[] flags = {BooleanClause.Occur.MUST, BooleanClause.Occur.MUST_NOT};
-    Query q = MultiFieldQueryParser.parse(Version.LUCENE_CURRENT, "one", fields, flags, new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT));
+    Query q = MultiFieldQueryParser.parse(TEST_VERSION_CURRENT, "one", fields, flags, new StandardAnalyzer(TEST_VERSION_CURRENT));
     assertEquals("+b:one -t:one", q.toString());
 
-    q = MultiFieldQueryParser.parse(Version.LUCENE_CURRENT, "one two", fields, flags, new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT));
+    q = MultiFieldQueryParser.parse(TEST_VERSION_CURRENT, "one two", fields, flags, new StandardAnalyzer(TEST_VERSION_CURRENT));
     assertEquals("+(b:one b:two) -(t:one t:two)", q.toString());
 
     try {
       BooleanClause.Occur[] flags2 = {BooleanClause.Occur.MUST};
-      q = MultiFieldQueryParser.parse(Version.LUCENE_CURRENT, "blah", fields, flags2, new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT));
+      q = MultiFieldQueryParser.parse(TEST_VERSION_CURRENT, "blah", fields, flags2, new StandardAnalyzer(TEST_VERSION_CURRENT));
       fail();
     } catch(IllegalArgumentException e) {
       // expected exception, array length differs
@@ -219,15 +218,15 @@
     //int[] flags = {MultiFieldQueryParser.REQUIRED_FIELD, MultiFieldQueryParser.PROHIBITED_FIELD};
       BooleanClause.Occur[] flags = {BooleanClause.Occur.MUST, BooleanClause.Occur.MUST_NOT};
 
-    Query q = MultiFieldQueryParser.parse(Version.LUCENE_CURRENT, "one", fields, flags, new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT));//, fields, flags, new StandardAnalyzer());
+    Query q = MultiFieldQueryParser.parse(TEST_VERSION_CURRENT, "one", fields, flags, new StandardAnalyzer(TEST_VERSION_CURRENT));//, fields, flags, new StandardAnalyzer());
     assertEquals("+b:one -t:one", q.toString());
 
-    q = MultiFieldQueryParser.parse(Version.LUCENE_CURRENT, "one two", fields, flags, new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT));
+    q = MultiFieldQueryParser.parse(TEST_VERSION_CURRENT, "one two", fields, flags, new StandardAnalyzer(TEST_VERSION_CURRENT));
     assertEquals("+(b:one b:two) -(t:one t:two)", q.toString());
 
     try {
       BooleanClause.Occur[] flags2 = {BooleanClause.Occur.MUST};
-      q = MultiFieldQueryParser.parse(Version.LUCENE_CURRENT, "blah", fields, flags2, new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT));
+      q = MultiFieldQueryParser.parse(TEST_VERSION_CURRENT, "blah", fields, flags2, new StandardAnalyzer(TEST_VERSION_CURRENT));
       fail();
     } catch(IllegalArgumentException e) {
       // expected exception, array length differs
@@ -239,12 +238,12 @@
     String[] fields = {"f1", "f2", "f3"};
     BooleanClause.Occur[] flags = {BooleanClause.Occur.MUST,
         BooleanClause.Occur.MUST_NOT, BooleanClause.Occur.SHOULD};
-    Query q = MultiFieldQueryParser.parse(Version.LUCENE_CURRENT, queries, fields, flags, new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT));
+    Query q = MultiFieldQueryParser.parse(TEST_VERSION_CURRENT, queries, fields, flags, new StandardAnalyzer(TEST_VERSION_CURRENT));
     assertEquals("+f1:one -f2:two f3:three", q.toString());
 
     try {
       BooleanClause.Occur[] flags2 = {BooleanClause.Occur.MUST};
-      q = MultiFieldQueryParser.parse(Version.LUCENE_CURRENT, queries, fields, flags2, new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT));
+      q = MultiFieldQueryParser.parse(TEST_VERSION_CURRENT, queries, fields, flags2, new StandardAnalyzer(TEST_VERSION_CURRENT));
       fail();
     } catch(IllegalArgumentException e) {
       // expected exception, array length differs
@@ -255,12 +254,12 @@
     String[] queries = {"one", "two"};
     String[] fields = {"b", "t"};
       BooleanClause.Occur[] flags = {BooleanClause.Occur.MUST, BooleanClause.Occur.MUST_NOT};
-    Query q = MultiFieldQueryParser.parse(Version.LUCENE_CURRENT, queries, fields, flags, new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT));
+    Query q = MultiFieldQueryParser.parse(TEST_VERSION_CURRENT, queries, fields, flags, new StandardAnalyzer(TEST_VERSION_CURRENT));
     assertEquals("+b:one -t:two", q.toString());
 
     try {
       BooleanClause.Occur[] flags2 = {BooleanClause.Occur.MUST};
-      q = MultiFieldQueryParser.parse(Version.LUCENE_CURRENT, queries, fields, flags2, new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT));
+      q = MultiFieldQueryParser.parse(TEST_VERSION_CURRENT, queries, fields, flags2, new StandardAnalyzer(TEST_VERSION_CURRENT));
       fail();
     } catch(IllegalArgumentException e) {
       // expected exception, array length differs
@@ -269,7 +268,7 @@
 
   public void testAnalyzerReturningNull() throws ParseException {
     String[] fields = new String[] { "f1", "f2", "f3" };
-    MultiFieldQueryParser parser = new MultiFieldQueryParser(Version.LUCENE_CURRENT, fields, new AnalyzerReturningNull());
+    MultiFieldQueryParser parser = new MultiFieldQueryParser(TEST_VERSION_CURRENT, fields, new AnalyzerReturningNull());
     Query q = parser.parse("bla AND blo");
     assertEquals("+(f2:bla f3:bla) +(f2:blo f3:blo)", q.toString());
     // the following queries are not affected as their terms are not analyzed anyway:
@@ -282,7 +281,7 @@
   }
 
   public void testStopWordSearching() throws Exception {
-    Analyzer analyzer = new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT);
+    Analyzer analyzer = new StandardAnalyzer(TEST_VERSION_CURRENT);
     Directory ramDir = new RAMDirectory();
     IndexWriter iw =  new IndexWriter(ramDir, analyzer, true, IndexWriter.MaxFieldLength.LIMITED);
     Document doc = new Document();
@@ -291,7 +290,7 @@
     iw.close();
     
     MultiFieldQueryParser mfqp = 
-      new MultiFieldQueryParser(Version.LUCENE_CURRENT, new String[] {"body"}, analyzer);
+      new MultiFieldQueryParser(TEST_VERSION_CURRENT, new String[] {"body"}, analyzer);
     mfqp.setDefaultOperator(QueryParser.Operator.AND);
     Query q = mfqp.parse("the footest");
     IndexSearcher is = new IndexSearcher(ramDir, true);
@@ -304,7 +303,7 @@
    * Return empty tokens for field "f1".
    */
   private static class AnalyzerReturningNull extends Analyzer {
-    StandardAnalyzer stdAnalyzer = new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT);
+    StandardAnalyzer stdAnalyzer = new StandardAnalyzer(TEST_VERSION_CURRENT);
 
     public AnalyzerReturningNull() {
     }
Index: src/test/org/apache/lucene/queryParser/TestQueryParser.java
===================================================================
--- src/test/org/apache/lucene/queryParser/TestQueryParser.java	(revision 916567)
+++ src/test/org/apache/lucene/queryParser/TestQueryParser.java	(working copy)
@@ -64,7 +64,6 @@
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.MockRAMDirectory;
 import org.apache.lucene.util.LocalizedTestCase;
-import org.apache.lucene.util.Version;
 
 /**
  * Tests QueryParser.
@@ -102,6 +101,7 @@
     public boolean incrementToken() throws IOException {
       if (inPhrase) {
         inPhrase = false;
+        clearAttributes();
         termAtt.setTermBuffer("phrase2");
         offsetAtt.setOffset(savedStart, savedEnd);
         return true;
@@ -127,13 +127,13 @@
     /** Filters LowerCaseTokenizer with StopFilter. */
     @Override
     public final TokenStream tokenStream(String fieldName, Reader reader) {
-      return new QPTestFilter(new LowerCaseTokenizer(reader));
+      return new QPTestFilter(new LowerCaseTokenizer(TEST_VERSION_CURRENT, reader));
     }
   }
 
   public static class QPTestParser extends QueryParser {
     public QPTestParser(String f, Analyzer a) {
-      super(Version.LUCENE_CURRENT, f, a);
+      super(TEST_VERSION_CURRENT, f, a);
     }
 
     @Override
@@ -157,8 +157,8 @@
 
   public QueryParser getParser(Analyzer a) throws Exception {
     if (a == null)
-      a = new SimpleAnalyzer();
-    QueryParser qp = new QueryParser(Version.LUCENE_CURRENT, "field", a);
+      a = new SimpleAnalyzer(TEST_VERSION_CURRENT);
+    QueryParser qp = new QueryParser(TEST_VERSION_CURRENT, "field", a);
     qp.setDefaultOperator(QueryParser.OR_OPERATOR);
     return qp;
   }
@@ -227,8 +227,8 @@
   public Query getQueryDOA(String query, Analyzer a)
     throws Exception {
     if (a == null)
-      a = new SimpleAnalyzer();
-    QueryParser qp = new QueryParser(Version.LUCENE_CURRENT, "field", a);
+      a = new SimpleAnalyzer(TEST_VERSION_CURRENT);
+    QueryParser qp = new QueryParser(TEST_VERSION_CURRENT, "field", a);
     qp.setDefaultOperator(QueryParser.AND_OPERATOR);
     return qp.parse(query);
   }
@@ -252,8 +252,8 @@
   
   public void testSimple() throws Exception {
     assertQueryEquals("term term term", null, "term term term");
-    assertQueryEquals("türm term term", new WhitespaceAnalyzer(), "türm term term");
-    assertQueryEquals("ümlaut", new WhitespaceAnalyzer(), "ümlaut");
+    assertQueryEquals("türm term term", new WhitespaceAnalyzer(TEST_VERSION_CURRENT), "türm term term");
+    assertQueryEquals("ümlaut", new WhitespaceAnalyzer(TEST_VERSION_CURRENT), "ümlaut");
 
     assertQueryEquals("\"\"", new KeywordAnalyzer(), "");
     assertQueryEquals("foo:\"\"", new KeywordAnalyzer(), "foo:");
@@ -300,7 +300,7 @@
     assertQueryEquals("+title:(dog OR cat) -author:\"bob dole\"", null,
                       "+(title:dog title:cat) -author:\"bob dole\"");
     
-    QueryParser qp = new QueryParser(Version.LUCENE_CURRENT, "field", new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT));
+    QueryParser qp = new QueryParser(TEST_VERSION_CURRENT, "field", new StandardAnalyzer(TEST_VERSION_CURRENT));
     // make sure OR is the default:
     assertEquals(QueryParser.OR_OPERATOR, qp.getDefaultOperator());
     qp.setDefaultOperator(QueryParser.AND_OPERATOR);
@@ -310,7 +310,7 @@
   }
 
   public void testPunct() throws Exception {
-    Analyzer a = new WhitespaceAnalyzer();
+    Analyzer a = new WhitespaceAnalyzer(TEST_VERSION_CURRENT);
     assertQueryEquals("a&b", a, "a&b");
     assertQueryEquals("a&&b", a, "a&&b");
     assertQueryEquals(".NET", a, ".NET");
@@ -330,7 +330,7 @@
     assertQueryEquals("term 1.0 1 2", null, "term");
     assertQueryEquals("term term1 term2", null, "term term term");
 
-    Analyzer a = new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT);
+    Analyzer a = new StandardAnalyzer(TEST_VERSION_CURRENT);
     assertQueryEquals("3", a, "3");
     assertQueryEquals("term 1.0 1 2", a, "term 1.0 1 2");
     assertQueryEquals("term term1 term2", a, "term term1 term2");
@@ -455,7 +455,7 @@
     assertQueryEquals("[ a TO z]", null, "[a TO z]");
     assertEquals(MultiTermQuery.CONSTANT_SCORE_AUTO_REWRITE_DEFAULT, ((TermRangeQuery)getQuery("[ a TO z]", null)).getRewriteMethod());
 
-    QueryParser qp = new QueryParser(Version.LUCENE_CURRENT, "field", new SimpleAnalyzer());
+    QueryParser qp = new QueryParser(TEST_VERSION_CURRENT, "field", new SimpleAnalyzer(TEST_VERSION_CURRENT));
     qp.setMultiTermRewriteMethod(MultiTermQuery.SCORING_BOOLEAN_QUERY_REWRITE);
     assertEquals(MultiTermQuery.SCORING_BOOLEAN_QUERY_REWRITE,((TermRangeQuery)qp.parse("[ a TO z]")).getRewriteMethod());
     
@@ -472,7 +472,7 @@
   public void testFarsiRangeCollating() throws Exception {
     
     RAMDirectory ramDir = new RAMDirectory();
-    IndexWriter iw = new IndexWriter(ramDir, new WhitespaceAnalyzer(), true, 
+    IndexWriter iw = new IndexWriter(ramDir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, 
                                      IndexWriter.MaxFieldLength.LIMITED);
     Document doc = new Document();
     doc.add(new Field("content","\u0633\u0627\u0628", 
@@ -481,7 +481,7 @@
     iw.close();
     IndexSearcher is = new IndexSearcher(ramDir, true);
 
-    QueryParser qp = new QueryParser(Version.LUCENE_CURRENT, "content", new WhitespaceAnalyzer());
+    QueryParser qp = new QueryParser(TEST_VERSION_CURRENT, "content", new WhitespaceAnalyzer(TEST_VERSION_CURRENT));
 
     // Neither Java 1.4.2 nor 1.5.0 has Farsi Locale collation available in
     // RuleBasedCollator.  However, the Arabic Locale seems to order the Farsi
@@ -579,7 +579,7 @@
     final String defaultField = "default";
     final String monthField = "month";
     final String hourField = "hour";
-    QueryParser qp = new QueryParser(Version.LUCENE_CURRENT, "field", new SimpleAnalyzer());
+    QueryParser qp = new QueryParser(TEST_VERSION_CURRENT, "field", new SimpleAnalyzer(TEST_VERSION_CURRENT));
     
     // Don't set any date resolution and verify if DateField is used
     assertDateRangeQueryEquals(qp, defaultField, startDate, endDate, 
@@ -620,7 +620,7 @@
   }
 
   public void testEscaped() throws Exception {
-    Analyzer a = new WhitespaceAnalyzer();
+    Analyzer a = new WhitespaceAnalyzer(TEST_VERSION_CURRENT);
     
     /*assertQueryEquals("\\[brackets", a, "\\[brackets");
     assertQueryEquals("\\[brackets", null, "brackets");
@@ -714,7 +714,7 @@
   }
 
   public void testQueryStringEscaping() throws Exception {
-    Analyzer a = new WhitespaceAnalyzer();
+    Analyzer a = new WhitespaceAnalyzer(TEST_VERSION_CURRENT);
 
     assertEscapedQueryEquals("a-b:c", a, "a\\-b\\:c");
     assertEscapedQueryEquals("a+b:c", a, "a\\+b\\:c");
@@ -801,8 +801,8 @@
     throws Exception {
     Set<Object> stopWords = new HashSet<Object>(1);
     stopWords.add("on");
-    StandardAnalyzer oneStopAnalyzer = new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT, stopWords);
-    QueryParser qp = new QueryParser(Version.LUCENE_CURRENT, "field", oneStopAnalyzer);
+    StandardAnalyzer oneStopAnalyzer = new StandardAnalyzer(TEST_VERSION_CURRENT, stopWords);
+    QueryParser qp = new QueryParser(TEST_VERSION_CURRENT, "field", oneStopAnalyzer);
     Query q = qp.parse("on^1.0");
     assertNotNull(q);
     q = qp.parse("\"hello\"^2.0");
@@ -814,7 +814,7 @@
     q = qp.parse("\"on\"^1.0");
     assertNotNull(q);
 
-    QueryParser qp2 = new QueryParser(Version.LUCENE_CURRENT, "field", new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT));
+    QueryParser qp2 = new QueryParser(TEST_VERSION_CURRENT, "field", new StandardAnalyzer(TEST_VERSION_CURRENT));
     q = qp2.parse("the^3");
     // "the" is a stop word so the result is an empty query:
     assertNotNull(q);
@@ -843,7 +843,7 @@
 
   public void testCustomQueryParserWildcard() {
     try {
-      new QPTestParser("contents", new WhitespaceAnalyzer()).parse("a?t");
+      new QPTestParser("contents", new WhitespaceAnalyzer(TEST_VERSION_CURRENT)).parse("a?t");
       fail("Wildcard queries should not be allowed");
     } catch (ParseException expected) {
       // expected exception
@@ -852,7 +852,7 @@
 
   public void testCustomQueryParserFuzzy() throws Exception {
     try {
-      new QPTestParser("contents", new WhitespaceAnalyzer()).parse("xunit~");
+      new QPTestParser("contents", new WhitespaceAnalyzer(TEST_VERSION_CURRENT)).parse("xunit~");
       fail("Fuzzy queries should not be allowed");
     } catch (ParseException expected) {
       // expected exception
@@ -862,7 +862,7 @@
   public void testBooleanQuery() throws Exception {
     BooleanQuery.setMaxClauseCount(2);
     try {
-      QueryParser qp = new QueryParser(Version.LUCENE_CURRENT, "field", new WhitespaceAnalyzer());
+      QueryParser qp = new QueryParser(TEST_VERSION_CURRENT, "field", new WhitespaceAnalyzer(TEST_VERSION_CURRENT));
       qp.parse("one two three");
       fail("ParseException expected due to too many boolean clauses");
     } catch (ParseException expected) {
@@ -874,7 +874,7 @@
    * This test differs from TestPrecedenceQueryParser
    */
   public void testPrecedence() throws Exception {
-    QueryParser qp = new QueryParser(Version.LUCENE_CURRENT, "field", new WhitespaceAnalyzer());
+    QueryParser qp = new QueryParser(TEST_VERSION_CURRENT, "field", new WhitespaceAnalyzer(TEST_VERSION_CURRENT));
     Query query1 = qp.parse("A AND B OR C AND D");
     Query query2 = qp.parse("+A +B +C +D");
     assertEquals(query1, query2);
@@ -882,7 +882,7 @@
 
   public void testLocalDateFormat() throws IOException, ParseException {
     RAMDirectory ramDir = new RAMDirectory();
-    IndexWriter iw = new IndexWriter(ramDir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
+    IndexWriter iw = new IndexWriter(ramDir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
     addDateDoc("a", 2005, 12, 2, 10, 15, 33, iw);
     addDateDoc("b", 2005, 12, 4, 22, 15, 00, iw);
     iw.close();
@@ -898,7 +898,7 @@
 
   public void testStarParsing() throws Exception {
     final int[] type = new int[1];
-    QueryParser qp = new QueryParser(Version.LUCENE_CURRENT, "field", new WhitespaceAnalyzer()) {
+    QueryParser qp = new QueryParser(TEST_VERSION_CURRENT, "field", new WhitespaceAnalyzer(TEST_VERSION_CURRENT)) {
       @Override
       protected Query getWildcardQuery(String field, String termStr) throws ParseException {
         // override error checking of superclass
@@ -957,7 +957,7 @@
   }
 
   public void testStopwords() throws Exception {
-    QueryParser qp = new QueryParser(Version.LUCENE_CURRENT, "a", new StopAnalyzer(Version.LUCENE_CURRENT, StopFilter.makeStopSet(Version.LUCENE_CURRENT, "the", "foo")));
+    QueryParser qp = new QueryParser(TEST_VERSION_CURRENT, "a", new StopAnalyzer(TEST_VERSION_CURRENT, StopFilter.makeStopSet(TEST_VERSION_CURRENT, "the", "foo")));
     Query result = qp.parse("a:the OR a:foo");
     assertNotNull("result is null and it shouldn't be", result);
     assertTrue("result is not a BooleanQuery", result instanceof BooleanQuery);
@@ -973,7 +973,7 @@
   }
 
   public void testPositionIncrement() throws Exception {
-    QueryParser qp = new QueryParser(Version.LUCENE_CURRENT, "a", new StopAnalyzer(Version.LUCENE_CURRENT, StopFilter.makeStopSet(Version.LUCENE_CURRENT, "the", "in", "are", "this")));
+    QueryParser qp = new QueryParser(TEST_VERSION_CURRENT, "a", new StopAnalyzer(TEST_VERSION_CURRENT, StopFilter.makeStopSet(TEST_VERSION_CURRENT, "the", "in", "are", "this")));
     qp.setEnablePositionIncrements(true);
     String qtxt = "\"the words in poisitions pos02578 are stopped in this phrasequery\"";
     //               0         2                      5           7  8
@@ -990,7 +990,7 @@
   }
 
   public void testMatchAllDocs() throws Exception {
-    QueryParser qp = new QueryParser(Version.LUCENE_CURRENT, "field", new WhitespaceAnalyzer());
+    QueryParser qp = new QueryParser(TEST_VERSION_CURRENT, "field", new WhitespaceAnalyzer(TEST_VERSION_CURRENT));
     assertEquals(new MatchAllDocsQuery(), qp.parse("*:*"));
     assertEquals(new MatchAllDocsQuery(), qp.parse("(*:*)"));
     BooleanQuery bq = (BooleanQuery)qp.parse("+*:* -*:*");
@@ -999,7 +999,7 @@
   }
   
   private void assertHits(int expected, String query, IndexSearcher is) throws ParseException, IOException {
-    QueryParser qp = new QueryParser(Version.LUCENE_CURRENT, "date", new WhitespaceAnalyzer());
+    QueryParser qp = new QueryParser(TEST_VERSION_CURRENT, "date", new WhitespaceAnalyzer(TEST_VERSION_CURRENT));
     qp.setLocale(Locale.ENGLISH);
     Query q = qp.parse(query);
     ScoreDoc[] hits = is.search(q, null, 1000).scoreDocs;
@@ -1027,7 +1027,7 @@
   // "match"
   public void testPositionIncrements() throws Exception {
     Directory dir = new MockRAMDirectory();
-    Analyzer a = new StandardAnalyzer(Version.LUCENE_CURRENT);
+    Analyzer a = new StandardAnalyzer(TEST_VERSION_CURRENT);
     IndexWriter w = new IndexWriter(dir, a, IndexWriter.MaxFieldLength.UNLIMITED);
     Document doc = new Document();
     doc.add(new Field("f", "the wizard of ozzy", Field.Store.NO, Field.Index.ANALYZED));
@@ -1035,7 +1035,7 @@
     IndexReader r = w.getReader();
     w.close();
     IndexSearcher s = new IndexSearcher(r);
-    QueryParser qp = new QueryParser(Version.LUCENE_CURRENT, "f", a);
+    QueryParser qp = new QueryParser(TEST_VERSION_CURRENT, "f", a);
     Query q = qp.parse("\"wizard of ozzy\"");
     assertEquals(1, s.search(q, 1).totalHits);
     r.close();
Index: src/test/org/apache/lucene/TestSnapshotDeletionPolicy.java
===================================================================
--- src/test/org/apache/lucene/TestSnapshotDeletionPolicy.java	(revision 916567)
+++ src/test/org/apache/lucene/TestSnapshotDeletionPolicy.java	(working copy)
@@ -67,7 +67,7 @@
     Directory dir = new MockRAMDirectory();
 
     SnapshotDeletionPolicy dp = new SnapshotDeletionPolicy(new KeepOnlyLastCommitDeletionPolicy());
-    IndexWriter writer = new IndexWriter(dir, new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT), dp, IndexWriter.MaxFieldLength.UNLIMITED);
+    IndexWriter writer = new IndexWriter(dir, new StandardAnalyzer(TEST_VERSION_CURRENT), dp, IndexWriter.MaxFieldLength.UNLIMITED);
     // Force frequent flushes
     writer.setMaxBufferedDocs(2);
     Document doc = new Document();
@@ -83,7 +83,7 @@
     writer.close();
     copyFiles(dir, cp);
     
-    writer = new IndexWriter(dir, new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT), dp, IndexWriter.MaxFieldLength.UNLIMITED);
+    writer = new IndexWriter(dir, new StandardAnalyzer(TEST_VERSION_CURRENT), dp, IndexWriter.MaxFieldLength.UNLIMITED);
     copyFiles(dir, cp);
     for(int i=0;i<7;i++) {
       writer.addDocument(doc);
@@ -95,7 +95,7 @@
     writer.close();
     copyFiles(dir, cp);
     dp.release();
-    writer = new IndexWriter(dir, new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT), dp, IndexWriter.MaxFieldLength.UNLIMITED);
+    writer = new IndexWriter(dir, new StandardAnalyzer(TEST_VERSION_CURRENT), dp, IndexWriter.MaxFieldLength.UNLIMITED);
     writer.close();
     try {
       copyFiles(dir, cp);
@@ -111,7 +111,7 @@
     final long stopTime = System.currentTimeMillis() + 1000;
 
     SnapshotDeletionPolicy dp = new SnapshotDeletionPolicy(new KeepOnlyLastCommitDeletionPolicy());
-    final IndexWriter writer = new IndexWriter(dir, new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT), dp, IndexWriter.MaxFieldLength.UNLIMITED);
+    final IndexWriter writer = new IndexWriter(dir, new StandardAnalyzer(TEST_VERSION_CURRENT), dp, IndexWriter.MaxFieldLength.UNLIMITED);
 
     // Force frequent flushes
     writer.setMaxBufferedDocs(2);
Index: src/test/org/apache/lucene/analysis/TestMappingCharFilter.java
===================================================================
--- src/test/org/apache/lucene/analysis/TestMappingCharFilter.java	(revision 916567)
+++ src/test/org/apache/lucene/analysis/TestMappingCharFilter.java	(working copy)
@@ -58,55 +58,55 @@
 
   public void testNothingChange() throws Exception {
     CharStream cs = new MappingCharFilter( normMap, new StringReader( "x" ) );
-    TokenStream ts = new WhitespaceTokenizer( cs );
+    TokenStream ts = new WhitespaceTokenizer(TEST_VERSION_CURRENT, cs );
     assertTokenStreamContents(ts, new String[]{"x"}, new int[]{0}, new int[]{1});
   }
 
   public void test1to1() throws Exception {
     CharStream cs = new MappingCharFilter( normMap, new StringReader( "h" ) );
-    TokenStream ts = new WhitespaceTokenizer( cs );
+    TokenStream ts = new WhitespaceTokenizer( TEST_VERSION_CURRENT, cs );
     assertTokenStreamContents(ts, new String[]{"i"}, new int[]{0}, new int[]{1});
   }
 
   public void test1to2() throws Exception {
     CharStream cs = new MappingCharFilter( normMap, new StringReader( "j" ) );
-    TokenStream ts = new WhitespaceTokenizer( cs );
+    TokenStream ts = new WhitespaceTokenizer( TEST_VERSION_CURRENT, cs );
     assertTokenStreamContents(ts, new String[]{"jj"}, new int[]{0}, new int[]{1});
   }
 
   public void test1to3() throws Exception {
     CharStream cs = new MappingCharFilter( normMap, new StringReader( "k" ) );
-    TokenStream ts = new WhitespaceTokenizer( cs );
+    TokenStream ts = new WhitespaceTokenizer( TEST_VERSION_CURRENT, cs );
     assertTokenStreamContents(ts, new String[]{"kkk"}, new int[]{0}, new int[]{1});
   }
 
   public void test2to4() throws Exception {
     CharStream cs = new MappingCharFilter( normMap, new StringReader( "ll" ) );
-    TokenStream ts = new WhitespaceTokenizer( cs );
+    TokenStream ts = new WhitespaceTokenizer( TEST_VERSION_CURRENT, cs );
     assertTokenStreamContents(ts, new String[]{"llll"}, new int[]{0}, new int[]{2});
   }
 
   public void test2to1() throws Exception {
     CharStream cs = new MappingCharFilter( normMap, new StringReader( "aa" ) );
-    TokenStream ts = new WhitespaceTokenizer( cs );
+    TokenStream ts = new WhitespaceTokenizer( TEST_VERSION_CURRENT, cs );
     assertTokenStreamContents(ts, new String[]{"a"}, new int[]{0}, new int[]{2});
   }
 
   public void test3to1() throws Exception {
     CharStream cs = new MappingCharFilter( normMap, new StringReader( "bbb" ) );
-    TokenStream ts = new WhitespaceTokenizer( cs );
+    TokenStream ts = new WhitespaceTokenizer( TEST_VERSION_CURRENT, cs );
     assertTokenStreamContents(ts, new String[]{"b"}, new int[]{0}, new int[]{3});
   }
 
   public void test4to2() throws Exception {
     CharStream cs = new MappingCharFilter( normMap, new StringReader( "cccc" ) );
-    TokenStream ts = new WhitespaceTokenizer( cs );
+    TokenStream ts = new WhitespaceTokenizer( TEST_VERSION_CURRENT, cs );
     assertTokenStreamContents(ts, new String[]{"cc"}, new int[]{0}, new int[]{4});
   }
 
   public void test5to0() throws Exception {
     CharStream cs = new MappingCharFilter( normMap, new StringReader( "empty" ) );
-    TokenStream ts = new WhitespaceTokenizer( cs );
+    TokenStream ts = new WhitespaceTokenizer( TEST_VERSION_CURRENT, cs );
     assertTokenStreamContents(ts, new String[0]);
   }
 
@@ -130,7 +130,7 @@
   //
   public void testTokenStream() throws Exception {
     CharStream cs = new MappingCharFilter( normMap, CharReader.get( new StringReader( "h i j k ll cccc bbb aa" ) ) );
-    TokenStream ts = new WhitespaceTokenizer( cs );
+    TokenStream ts = new WhitespaceTokenizer( TEST_VERSION_CURRENT, cs );
     assertTokenStreamContents(ts,
       new String[]{"i","i","jj","kkk","llll","cc","b","a"},
       new int[]{0,2,4,6,8,11,16,20},
@@ -151,7 +151,7 @@
   public void testChained() throws Exception {
     CharStream cs = new MappingCharFilter( normMap,
         new MappingCharFilter( normMap, CharReader.get( new StringReader( "aaaa ll h" ) ) ) );
-    TokenStream ts = new WhitespaceTokenizer( cs );
+    TokenStream ts = new WhitespaceTokenizer( TEST_VERSION_CURRENT, cs );
     assertTokenStreamContents(ts,
       new String[]{"a","llllllll","i"},
       new int[]{0,5,8},
Index: src/test/org/apache/lucene/analysis/TestPerFieldAnalzyerWrapper.java
===================================================================
--- src/test/org/apache/lucene/analysis/TestPerFieldAnalzyerWrapper.java	(revision 916567)
+++ src/test/org/apache/lucene/analysis/TestPerFieldAnalzyerWrapper.java	(working copy)
@@ -25,8 +25,8 @@
   public void testPerField() throws Exception {
     String text = "Qwerty";
     PerFieldAnalyzerWrapper analyzer =
-              new PerFieldAnalyzerWrapper(new WhitespaceAnalyzer());
-    analyzer.addAnalyzer("special", new SimpleAnalyzer());
+              new PerFieldAnalyzerWrapper(new WhitespaceAnalyzer(TEST_VERSION_CURRENT));
+    analyzer.addAnalyzer("special", new SimpleAnalyzer(TEST_VERSION_CURRENT));
 
     TokenStream tokenStream = analyzer.tokenStream("field",
                                             new StringReader(text));
Index: src/test/org/apache/lucene/analysis/TestASCIIFoldingFilter.java
===================================================================
--- src/test/org/apache/lucene/analysis/TestASCIIFoldingFilter.java	(revision 916567)
+++ src/test/org/apache/lucene/analysis/TestASCIIFoldingFilter.java	(working copy)
@@ -18,7 +18,6 @@
  */
 
 import org.apache.lucene.analysis.tokenattributes.TermAttribute;
-
 import java.io.StringReader;
 import java.util.List;
 import java.util.ArrayList;
@@ -28,7 +27,7 @@
 
   // testLain1Accents() is a copy of TestLatin1AccentFilter.testU().
   public void testLatin1Accents() throws Exception {
-    TokenStream stream = new WhitespaceTokenizer(new StringReader
+    TokenStream stream = new WhitespaceTokenizer(TEST_VERSION_CURRENT, new StringReader
       ("Des mot clés À LA CHAÎNE À Á Â Ã Ä Å Æ Ç È É Ê Ë Ì Í Î Ï Ĳ Ð Ñ"
       +" Ò Ó Ô Õ Ö Ø Œ Þ Ù Ú Û Ü Ý Ÿ à á â ã ä å æ ç è é ê ë ì í î ï ĳ"
       +" ð ñ ò ó ô õ ö ø œ ß þ ù ú û ü ý ÿ ﬁ ﬂ"));
@@ -1889,7 +1888,7 @@
       expectedOutputTokens.add(expected.toString());
     }
 
-    TokenStream stream = new WhitespaceTokenizer(new StringReader(inputText.toString()));
+    TokenStream stream = new WhitespaceTokenizer(TEST_VERSION_CURRENT, new StringReader(inputText.toString()));
     ASCIIFoldingFilter filter = new ASCIIFoldingFilter(stream);
     TermAttribute termAtt = filter.getAttribute(TermAttribute.class);
     Iterator<String> expectedIter = expectedOutputTokens.iterator();
Index: src/test/org/apache/lucene/analysis/TestCachingTokenFilter.java
===================================================================
--- src/test/org/apache/lucene/analysis/TestCachingTokenFilter.java	(revision 916567)
+++ src/test/org/apache/lucene/analysis/TestCachingTokenFilter.java	(working copy)
@@ -37,7 +37,7 @@
   
   public void testCaching() throws IOException {
     Directory dir = new RAMDirectory();
-    IndexWriter writer = new IndexWriter(dir, new SimpleAnalyzer(), IndexWriter.MaxFieldLength.LIMITED);
+    IndexWriter writer = new IndexWriter(dir, new SimpleAnalyzer(TEST_VERSION_CURRENT), IndexWriter.MaxFieldLength.LIMITED);
     Document doc = new Document();
     TokenStream stream = new TokenStream() {
       private int index = 0;
@@ -49,6 +49,7 @@
         if (index == tokens.length) {
           return false;
         } else {
+          clearAttributes();
           termAtt.setTermBuffer(tokens[index++]);
           offsetAtt.setOffset(0,0);
           return true;
Index: src/test/org/apache/lucene/analysis/TestStandardAnalyzer.java
===================================================================
--- src/test/org/apache/lucene/analysis/TestStandardAnalyzer.java	(revision 916567)
+++ src/test/org/apache/lucene/analysis/TestStandardAnalyzer.java	(working copy)
@@ -23,16 +23,16 @@
 
 public class TestStandardAnalyzer extends BaseTokenStreamTestCase {
 
-  private Analyzer a = new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT);
+  private Analyzer a = new StandardAnalyzer(TEST_VERSION_CURRENT);
 
   public void testMaxTermLength() throws Exception {
-    StandardAnalyzer sa = new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT);
+    StandardAnalyzer sa = new StandardAnalyzer(TEST_VERSION_CURRENT);
     sa.setMaxTokenLength(5);
     assertAnalyzesTo(sa, "ab cd toolong xy z", new String[]{"ab", "cd", "xy", "z"});
   }
 
   public void testMaxTermLength2() throws Exception {
-    StandardAnalyzer sa = new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT);
+    StandardAnalyzer sa = new StandardAnalyzer(TEST_VERSION_CURRENT);
     assertAnalyzesTo(sa, "ab cd toolong xy z", new String[]{"ab", "cd", "toolong", "xy", "z"});
     sa.setMaxTokenLength(5);
     
@@ -96,7 +96,7 @@
 
   public void testLucene1140() throws Exception {
     try {
-      StandardAnalyzer analyzer = new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT);
+      StandardAnalyzer analyzer = new StandardAnalyzer(TEST_VERSION_CURRENT);
       assertAnalyzesTo(analyzer, "www.nutch.org.", new String[]{ "www.nutch.org" }, new String[] { "<HOST>" });
     } catch (NullPointerException e) {
       fail("Should not throw an NPE and it did");
@@ -106,7 +106,7 @@
 
   public void testDomainNames() throws Exception {
     // Current lucene should not show the bug
-    StandardAnalyzer a2 = new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT);
+    StandardAnalyzer a2 = new StandardAnalyzer(TEST_VERSION_CURRENT);
 
     // domain names
     assertAnalyzesTo(a2, "www.nutch.org", new String[]{"www.nutch.org"});
Index: src/test/org/apache/lucene/analysis/tokenattributes/TestSimpleAttributeImpls.java
===================================================================
--- src/test/org/apache/lucene/analysis/tokenattributes/TestSimpleAttributeImpls.java	(revision 916567)
+++ src/test/org/apache/lucene/analysis/tokenattributes/TestSimpleAttributeImpls.java	(working copy)
@@ -20,6 +20,7 @@
 import org.apache.lucene.index.Payload;
 import org.apache.lucene.util.AttributeImpl;
 import org.apache.lucene.util.LuceneTestCase;
+import org.apache.lucene.util.AttributeSource.AttributeFactory;
 
 public class TestSimpleAttributeImpls extends LuceneTestCase {
 
@@ -118,6 +119,25 @@
     assertEquals(0, att.endOffset());
   }
   
+  public void testKeywordAttribute() {
+    AttributeImpl attrImpl = AttributeFactory.DEFAULT_ATTRIBUTE_FACTORY.createAttributeInstance(KeywordAttribute.class);
+    assertSame(KeywordAttributeImpl.class, attrImpl.getClass());
+    KeywordAttributeImpl att = (KeywordAttributeImpl) attrImpl;
+    assertFalse(att.isKeyword());
+    att.setKeyword(true);
+    assertTrue(att.isKeyword());
+    
+    KeywordAttributeImpl assertCloneIsEqual = (KeywordAttributeImpl) assertCloneIsEqual(att);
+    assertTrue(assertCloneIsEqual.isKeyword());
+    assertCloneIsEqual.clear();
+    assertFalse(assertCloneIsEqual.isKeyword());
+    assertTrue(att.isKeyword());
+    
+    att.copyTo(assertCloneIsEqual);
+    assertTrue(assertCloneIsEqual.isKeyword());
+    assertTrue(att.isKeyword());
+  }
+  
   public static final AttributeImpl assertCloneIsEqual(AttributeImpl att) {
     AttributeImpl clone = (AttributeImpl) att.clone();
     assertEquals("Clone must be equal", att, clone);
Index: src/test/org/apache/lucene/analysis/tokenattributes/TestTermAttributeImpl.java
===================================================================
--- src/test/org/apache/lucene/analysis/tokenattributes/TestTermAttributeImpl.java	(revision 916567)
+++ src/test/org/apache/lucene/analysis/tokenattributes/TestTermAttributeImpl.java	(working copy)
@@ -49,7 +49,6 @@
       buf.append(buf.toString());
     }
     assertEquals(1048576, t.termLength());
-    assertEquals(1179654, t.termBuffer().length);
 
     // now as a string, first variant
     t = new TermAttributeImpl();
@@ -63,7 +62,6 @@
       buf.append(content);
     }
     assertEquals(1048576, t.termLength());
-    assertEquals(1179654, t.termBuffer().length);
 
     // now as a string, second variant
     t = new TermAttributeImpl();
@@ -77,7 +75,6 @@
       buf.append(content);
     }
     assertEquals(1048576, t.termLength());
-    assertEquals(1179654, t.termBuffer().length);
 
     // Test for slow growth to a long term
     t = new TermAttributeImpl();
@@ -91,7 +88,6 @@
       buf.append("a");
     }
     assertEquals(20000, t.termLength());
-    assertEquals(20167, t.termBuffer().length);
 
     // Test for slow growth to a long term
     t = new TermAttributeImpl();
@@ -105,7 +101,6 @@
       buf.append("a");
     }
     assertEquals(20000, t.termLength());
-    assertEquals(20167, t.termBuffer().length);
   }
 
   public void testToString() throws Exception {
Index: src/test/org/apache/lucene/analysis/TestAnalyzers.java
===================================================================
--- src/test/org/apache/lucene/analysis/TestAnalyzers.java	(revision 916567)
+++ src/test/org/apache/lucene/analysis/TestAnalyzers.java	(working copy)
@@ -26,7 +26,6 @@
 import org.apache.lucene.analysis.tokenattributes.PayloadAttribute;
 import org.apache.lucene.analysis.tokenattributes.TermAttribute;
 import org.apache.lucene.index.Payload;
-import org.apache.lucene.util.Version;
 
 public class TestAnalyzers extends BaseTokenStreamTestCase {
 
@@ -35,7 +34,7 @@
    }
 
   public void testSimple() throws Exception {
-    Analyzer a = new SimpleAnalyzer();
+    Analyzer a = new SimpleAnalyzer(TEST_VERSION_CURRENT);
     assertAnalyzesTo(a, "foo bar FOO BAR", 
                      new String[] { "foo", "bar", "foo", "bar" });
     assertAnalyzesTo(a, "foo      bar .  FOO <> BAR", 
@@ -55,7 +54,7 @@
   }
 
   public void testNull() throws Exception {
-    Analyzer a = new WhitespaceAnalyzer();
+    Analyzer a = new WhitespaceAnalyzer(TEST_VERSION_CURRENT);
     assertAnalyzesTo(a, "foo bar FOO BAR", 
                      new String[] { "foo", "bar", "FOO", "BAR" });
     assertAnalyzesTo(a, "foo      bar .  FOO <> BAR", 
@@ -75,7 +74,7 @@
   }
 
   public void testStop() throws Exception {
-    Analyzer a = new StopAnalyzer(Version.LUCENE_CURRENT);
+    Analyzer a = new StopAnalyzer(TEST_VERSION_CURRENT);
     assertAnalyzesTo(a, "foo bar FOO BAR", 
                      new String[] { "foo", "bar", "foo", "bar" });
     assertAnalyzesTo(a, "foo a bar such FOO THESE BAR", 
@@ -97,11 +96,11 @@
   public void testPayloadCopy() throws IOException {
     String s = "how now brown cow";
     TokenStream ts;
-    ts = new WhitespaceTokenizer(new StringReader(s));
+    ts = new WhitespaceTokenizer(TEST_VERSION_CURRENT, new StringReader(s));
     ts = new PayloadSetter(ts);
     verifyPayload(ts);
 
-    ts = new WhitespaceTokenizer(new StringReader(s));
+    ts = new WhitespaceTokenizer(TEST_VERSION_CURRENT, new StringReader(s));
     ts = new PayloadSetter(ts);
     verifyPayload(ts);
   }
@@ -122,12 +121,12 @@
 
   private static class MyStandardAnalyzer extends StandardAnalyzer {
     public MyStandardAnalyzer() {
-      super(org.apache.lucene.util.Version.LUCENE_CURRENT);
+      super(TEST_VERSION_CURRENT);
     }
   
     @Override
     public TokenStream tokenStream(String field, Reader reader) {
-      return new WhitespaceAnalyzer().tokenStream(field, reader);
+      return new WhitespaceAnalyzer(TEST_VERSION_CURRENT).tokenStream(field, reader);
     }
   }
 
@@ -144,8 +143,8 @@
 
     @Override
     public TokenStream tokenStream(String fieldName, Reader reader) {
-      return new LowerCaseFilter(Version.LUCENE_CURRENT,
-          new WhitespaceTokenizer(reader));
+      return new LowerCaseFilter(TEST_VERSION_CURRENT,
+          new WhitespaceTokenizer(TEST_VERSION_CURRENT, reader));
     }
     
   }
@@ -192,9 +191,9 @@
   public void testLowerCaseFilterLowSurrogateLeftover() throws IOException {
     // test if the limit of the termbuffer is correctly used with supplementary
     // chars
-    WhitespaceTokenizer tokenizer = new WhitespaceTokenizer(new StringReader(
-        "BogustermBogusterm\udc16"));
-    LowerCaseFilter filter = new LowerCaseFilter(Version.LUCENE_CURRENT,
+    WhitespaceTokenizer tokenizer = new WhitespaceTokenizer(TEST_VERSION_CURRENT, 
+        new StringReader("BogustermBogusterm\udc16"));
+    LowerCaseFilter filter = new LowerCaseFilter(TEST_VERSION_CURRENT,
         tokenizer);
     assertTokenStreamContents(filter, new String[] {"bogustermbogusterm\udc16"});
     filter.reset();
Index: src/test/org/apache/lucene/analysis/TestTeeSinkTokenFilter.java
===================================================================
--- src/test/org/apache/lucene/analysis/TestTeeSinkTokenFilter.java	(revision 916567)
+++ src/test/org/apache/lucene/analysis/TestTeeSinkTokenFilter.java	(working copy)
@@ -22,8 +22,6 @@
 import org.apache.lucene.analysis.tokenattributes.TermAttribute;
 import org.apache.lucene.util.AttributeSource;
 import org.apache.lucene.util.English;
-import org.apache.lucene.util.Version;
-
 import java.io.IOException;
 import java.io.StringReader;
 
@@ -76,83 +74,46 @@
 
   
   public void testGeneral() throws IOException {
-    final TeeSinkTokenFilter source = new TeeSinkTokenFilter(new WhitespaceTokenizer(new StringReader(buffer1.toString())));
+    final TeeSinkTokenFilter source = new TeeSinkTokenFilter(new WhitespaceTokenizer(TEST_VERSION_CURRENT, new StringReader(buffer1.toString())));
     final TokenStream sink1 = source.newSinkTokenStream();
     final TokenStream sink2 = source.newSinkTokenStream(theFilter);
-    int i = 0;
-    TermAttribute termAtt = source.getAttribute(TermAttribute.class);
-    while (source.incrementToken()) {
-      assertEquals(tokens1[i], termAtt.term());
-      i++;
-    }
-    assertEquals(tokens1.length, i);
     
-    i = 0;
-    termAtt = sink1.getAttribute(TermAttribute.class);
-    while (sink1.incrementToken()) {
-      assertEquals(tokens1[i], termAtt.term());
-      i++;
-    }
-    assertEquals(tokens1.length, i);
+    source.addAttribute(CheckClearAttributesAttribute.class);
+    sink1.addAttribute(CheckClearAttributesAttribute.class);
+    sink2.addAttribute(CheckClearAttributesAttribute.class);
     
-    i = 0;
-    termAtt = sink2.getAttribute(TermAttribute.class);
-    while (sink2.incrementToken()) {
-      assertTrue(termAtt.term().equalsIgnoreCase("The"));
-      i++;
-    }
-    assertEquals("there should be two times 'the' in the stream", 2, i);
+    assertTokenStreamContents(source, tokens1);
+    assertTokenStreamContents(sink1, tokens1);
+    assertTokenStreamContents(sink2, new String[]{"The", "the"});
   }
 
   public void testMultipleSources() throws Exception {
-    final TeeSinkTokenFilter tee1 = new TeeSinkTokenFilter(new WhitespaceTokenizer(new StringReader(buffer1.toString())));
+    final TeeSinkTokenFilter tee1 = new TeeSinkTokenFilter(new WhitespaceTokenizer(TEST_VERSION_CURRENT, new StringReader(buffer1.toString())));
     final TeeSinkTokenFilter.SinkTokenStream dogDetector = tee1.newSinkTokenStream(dogFilter);
     final TeeSinkTokenFilter.SinkTokenStream theDetector = tee1.newSinkTokenStream(theFilter);
     final TokenStream source1 = new CachingTokenFilter(tee1);
+    
+    tee1.addAttribute(CheckClearAttributesAttribute.class);
+    dogDetector.addAttribute(CheckClearAttributesAttribute.class);
+    theDetector.addAttribute(CheckClearAttributesAttribute.class);
 
-    final TeeSinkTokenFilter tee2 = new TeeSinkTokenFilter(new WhitespaceTokenizer(new StringReader(buffer2.toString())));
+    final TeeSinkTokenFilter tee2 = new TeeSinkTokenFilter(new WhitespaceTokenizer(TEST_VERSION_CURRENT, new StringReader(buffer2.toString())));
     tee2.addSinkTokenStream(dogDetector);
     tee2.addSinkTokenStream(theDetector);
     final TokenStream source2 = tee2;
 
-    int i = 0;
-    TermAttribute termAtt = source1.getAttribute(TermAttribute.class);
-    while (source1.incrementToken()) {
-      assertEquals(tokens1[i], termAtt.term());
-      i++;
-    }
-    assertEquals(tokens1.length, i);
-    i = 0;
-    termAtt = source2.getAttribute(TermAttribute.class);
-    while (source2.incrementToken()) {
-      assertEquals(tokens2[i], termAtt.term());
-      i++;
-    }
-    assertEquals(tokens2.length, i);
-    i = 0;
-    termAtt = theDetector.getAttribute(TermAttribute.class);
-    while (theDetector.incrementToken()) {
-      assertTrue("'" + termAtt.term() + "' is not equal to 'The'", termAtt.term().equalsIgnoreCase("The"));
-      i++;
-    }
-    assertEquals("there must be 4 times 'The' in the stream", 4, i);
-    i = 0;
-    termAtt = dogDetector.getAttribute(TermAttribute.class);
-    while (dogDetector.incrementToken()) {
-      assertTrue("'" + termAtt.term() + "' is not equal to 'Dogs'", termAtt.term().equalsIgnoreCase("Dogs"));
-      i++;
-    }
-    assertEquals("there must be 2 times 'Dog' in the stream", 2, i);
+    assertTokenStreamContents(source1, tokens1);
+    assertTokenStreamContents(source2, tokens2);
+
+    assertTokenStreamContents(theDetector, new String[]{"The", "the", "The", "the"});
+    assertTokenStreamContents(dogDetector, new String[]{"Dogs", "Dogs"});
     
     source1.reset();
-    TokenStream lowerCasing = new LowerCaseFilter(Version.LUCENE_CURRENT, source1);
-    i = 0;
-    termAtt = lowerCasing.getAttribute(TermAttribute.class);
-    while (lowerCasing.incrementToken()) {
-      assertEquals(tokens1[i].toLowerCase(), termAtt.term());
-      i++;
-    }
-    assertEquals(i, tokens1.length);
+    TokenStream lowerCasing = new LowerCaseFilter(TEST_VERSION_CURRENT, source1);
+    String[] lowerCaseTokens = new String[tokens1.length];
+    for (int i = 0; i < tokens1.length; i++)
+      lowerCaseTokens[i] = tokens1[i].toLowerCase();
+    assertTokenStreamContents(lowerCasing, lowerCaseTokens);
   }
 
   /**
@@ -170,10 +131,10 @@
         buffer.append(English.intToEnglish(i).toUpperCase()).append(' ');
       }
       //make sure we produce the same tokens
-      TeeSinkTokenFilter teeStream = new TeeSinkTokenFilter(new StandardFilter(new StandardTokenizer(Version.LUCENE_CURRENT, new StringReader(buffer.toString()))));
+      TeeSinkTokenFilter teeStream = new TeeSinkTokenFilter(new StandardFilter(new StandardTokenizer(TEST_VERSION_CURRENT, new StringReader(buffer.toString()))));
       TokenStream sink = teeStream.newSinkTokenStream(new ModuloSinkFilter(100));
       teeStream.consumeAllTokens();
-      TokenStream stream = new ModuloTokenFilter(new StandardFilter(new StandardTokenizer(Version.LUCENE_CURRENT, new StringReader(buffer.toString()))), 100);
+      TokenStream stream = new ModuloTokenFilter(new StandardFilter(new StandardTokenizer(TEST_VERSION_CURRENT, new StringReader(buffer.toString()))), 100);
       TermAttribute tfTok = stream.addAttribute(TermAttribute.class);
       TermAttribute sinkTok = sink.addAttribute(TermAttribute.class);
       for (int i=0; stream.incrementToken(); i++) {
@@ -186,12 +147,12 @@
         int tfPos = 0;
         long start = System.currentTimeMillis();
         for (int i = 0; i < 20; i++) {
-          stream = new StandardFilter(new StandardTokenizer(Version.LUCENE_CURRENT, new StringReader(buffer.toString())));
+          stream = new StandardFilter(new StandardTokenizer(TEST_VERSION_CURRENT, new StringReader(buffer.toString())));
           PositionIncrementAttribute posIncrAtt = stream.getAttribute(PositionIncrementAttribute.class);
           while (stream.incrementToken()) {
             tfPos += posIncrAtt.getPositionIncrement();
           }
-          stream = new ModuloTokenFilter(new StandardFilter(new StandardTokenizer(Version.LUCENE_CURRENT, new StringReader(buffer.toString()))), modCounts[j]);
+          stream = new ModuloTokenFilter(new StandardFilter(new StandardTokenizer(TEST_VERSION_CURRENT, new StringReader(buffer.toString()))), modCounts[j]);
           posIncrAtt = stream.getAttribute(PositionIncrementAttribute.class);
           while (stream.incrementToken()) {
             tfPos += posIncrAtt.getPositionIncrement();
@@ -203,7 +164,7 @@
         //simulate one field with one sink
         start = System.currentTimeMillis();
         for (int i = 0; i < 20; i++) {
-          teeStream = new TeeSinkTokenFilter(new StandardFilter(new StandardTokenizer(Version.LUCENE_CURRENT, new StringReader(buffer.toString()))));
+          teeStream = new TeeSinkTokenFilter(new StandardFilter(new StandardTokenizer(TEST_VERSION_CURRENT, new StringReader(buffer.toString()))));
           sink = teeStream.newSinkTokenStream(new ModuloSinkFilter(modCounts[j]));
           PositionIncrementAttribute posIncrAtt = teeStream.getAttribute(PositionIncrementAttribute.class);
           while (teeStream.incrementToken()) {
Index: src/test/org/apache/lucene/analysis/TestToken.java
===================================================================
--- src/test/org/apache/lucene/analysis/TestToken.java	(revision 916567)
+++ src/test/org/apache/lucene/analysis/TestToken.java	(working copy)
@@ -85,7 +85,6 @@
       buf.append(buf.toString());
     }
     assertEquals(1048576, t.termLength());
-    assertEquals(1179654, t.termBuffer().length);
 
     // now as a string, first variant
     t = new Token();
@@ -99,7 +98,6 @@
       buf.append(content);
     }
     assertEquals(1048576, t.termLength());
-    assertEquals(1179654, t.termBuffer().length);
 
     // now as a string, second variant
     t = new Token();
@@ -113,7 +111,6 @@
       buf.append(content);
     }
     assertEquals(1048576, t.termLength());
-    assertEquals(1179654, t.termBuffer().length);
 
     // Test for slow growth to a long term
     t = new Token();
@@ -127,7 +124,6 @@
       buf.append("a");
     }
     assertEquals(20000, t.termLength());
-    assertEquals(20167, t.termBuffer().length);
 
     // Test for slow growth to a long term
     t = new Token();
@@ -141,7 +137,6 @@
       buf.append("a");
     }
     assertEquals(20000, t.termLength());
-    assertEquals(20167, t.termBuffer().length);
   }
 
   public void testToString() throws Exception {
Index: src/test/org/apache/lucene/analysis/BaseTokenStreamTestCase.java
===================================================================
--- src/test/org/apache/lucene/analysis/BaseTokenStreamTestCase.java	(revision 916567)
+++ src/test/org/apache/lucene/analysis/BaseTokenStreamTestCase.java	(working copy)
@@ -21,6 +21,8 @@
 import java.io.IOException;
  
 import org.apache.lucene.analysis.tokenattributes.*;
+import org.apache.lucene.util.Attribute;
+import org.apache.lucene.util.AttributeImpl;
 import org.apache.lucene.util.LuceneTestCase;
 
 /** 
@@ -38,26 +40,67 @@
   
   // some helpers to test Analyzers and TokenStreams:
   
-  public static void assertTokenStreamContents(TokenStream ts, String[] output, int startOffsets[], int endOffsets[], String types[], int posIncrements[]) throws IOException {
+  public static interface CheckClearAttributesAttribute extends Attribute {
+    boolean getAndResetClearCalled();
+  }
+
+  public static final class CheckClearAttributesAttributeImpl extends AttributeImpl implements CheckClearAttributesAttribute {
+    private boolean clearCalled = false;
+    
+    public boolean getAndResetClearCalled() {
+      try {
+        return clearCalled;
+      } finally {
+        clearCalled = false;
+      }
+    }
+
+    @Override
+    public void clear() {
+      clearCalled = true;
+    }
+
+    @Override
+    public boolean equals(Object other) {
+      return (
+        other instanceof CheckClearAttributesAttributeImpl &&
+        ((CheckClearAttributesAttributeImpl) other).clearCalled == this.clearCalled
+      );
+    }
+
+    @Override
+    public int hashCode() {
+      return 76137213 ^ Boolean.valueOf(clearCalled).hashCode();
+    }
+    
+    @Override
+    public void copyTo(AttributeImpl target) {
+      ((CheckClearAttributesAttributeImpl) target).clear();
+    }
+  }
+
+  public static void assertTokenStreamContents(TokenStream ts, String[] output, int startOffsets[], int endOffsets[], String types[], int posIncrements[], Integer finalOffset) throws IOException {
     assertNotNull(output);
-    assertTrue("has TermAttribute", ts.hasAttribute(TermAttribute.class));
+    CheckClearAttributesAttribute checkClearAtt = ts.addAttribute(CheckClearAttributesAttribute.class);
+    
+    assertTrue("has no TermAttribute", ts.hasAttribute(TermAttribute.class));
     TermAttribute termAtt = ts.getAttribute(TermAttribute.class);
     
     OffsetAttribute offsetAtt = null;
-    if (startOffsets != null || endOffsets != null) {
-      assertTrue("has OffsetAttribute", ts.hasAttribute(OffsetAttribute.class));
+    if (startOffsets != null || endOffsets != null || finalOffset != null) {
+      assertTrue("has no OffsetAttribute", ts.hasAttribute(OffsetAttribute.class));
       offsetAtt = ts.getAttribute(OffsetAttribute.class);
     }
     
     TypeAttribute typeAtt = null;
     if (types != null) {
-      assertTrue("has TypeAttribute", ts.hasAttribute(TypeAttribute.class));
+      assertTrue("has no TypeAttribute", ts.hasAttribute(TypeAttribute.class));
       typeAtt = ts.getAttribute(TypeAttribute.class);
     }
     
     PositionIncrementAttribute posIncrAtt = null;
     if (posIncrements != null) {
-      assertTrue("has PositionIncrementAttribute", ts.hasAttribute(PositionIncrementAttribute.class));
+      assertTrue("has no PositionIncrementAttribute", ts.hasAttribute(PositionIncrementAttribute.class));
       posIncrAtt = ts.getAttribute(PositionIncrementAttribute.class);
     }
     
@@ -70,7 +113,10 @@
       if (typeAtt != null) typeAtt.setType("bogusType");
       if (posIncrAtt != null) posIncrAtt.setPositionIncrement(45987657);
       
-      assertTrue("token "+i+" exists", ts.incrementToken());
+      checkClearAtt.getAndResetClearCalled(); // reset it, because we called clearAttribute() before
+      assertTrue("token "+i+" does not exist", ts.incrementToken());
+      assertTrue("clearAttributes() was not called correctly in TokenStream chain", checkClearAtt.getAndResetClearCalled());
+      
       assertEquals("term "+i, output[i], termAtt.term());
       if (startOffsets != null)
         assertEquals("startOffset "+i, startOffsets[i], offsetAtt.startOffset());
@@ -83,32 +129,45 @@
     }
     assertFalse("end of stream", ts.incrementToken());
     ts.end();
+    if (finalOffset != null)
+      assertEquals("finalOffset ", finalOffset.intValue(), offsetAtt.endOffset());
     ts.close();
   }
   
+  public static void assertTokenStreamContents(TokenStream ts, String[] output, int startOffsets[], int endOffsets[], String types[], int posIncrements[]) throws IOException {
+    assertTokenStreamContents(ts, output, startOffsets, endOffsets, types, posIncrements, null);
+  }
+
   public static void assertTokenStreamContents(TokenStream ts, String[] output) throws IOException {
-    assertTokenStreamContents(ts, output, null, null, null, null);
+    assertTokenStreamContents(ts, output, null, null, null, null, null);
   }
   
   public static void assertTokenStreamContents(TokenStream ts, String[] output, String[] types) throws IOException {
-    assertTokenStreamContents(ts, output, null, null, types, null);
+    assertTokenStreamContents(ts, output, null, null, types, null, null);
   }
   
   public static void assertTokenStreamContents(TokenStream ts, String[] output, int[] posIncrements) throws IOException {
-    assertTokenStreamContents(ts, output, null, null, null, posIncrements);
+    assertTokenStreamContents(ts, output, null, null, null, posIncrements, null);
   }
   
   public static void assertTokenStreamContents(TokenStream ts, String[] output, int startOffsets[], int endOffsets[]) throws IOException {
-    assertTokenStreamContents(ts, output, startOffsets, endOffsets, null, null);
+    assertTokenStreamContents(ts, output, startOffsets, endOffsets, null, null, null);
   }
   
+  public static void assertTokenStreamContents(TokenStream ts, String[] output, int startOffsets[], int endOffsets[], Integer finalOffset) throws IOException {
+    assertTokenStreamContents(ts, output, startOffsets, endOffsets, null, null, finalOffset);
+  }
+  
   public static void assertTokenStreamContents(TokenStream ts, String[] output, int startOffsets[], int endOffsets[], int[] posIncrements) throws IOException {
-    assertTokenStreamContents(ts, output, startOffsets, endOffsets, null, posIncrements);
+    assertTokenStreamContents(ts, output, startOffsets, endOffsets, null, posIncrements, null);
   }
 
+  public static void assertTokenStreamContents(TokenStream ts, String[] output, int startOffsets[], int endOffsets[], int[] posIncrements, Integer finalOffset) throws IOException {
+    assertTokenStreamContents(ts, output, startOffsets, endOffsets, null, posIncrements, finalOffset);
+  }
   
   public static void assertAnalyzesTo(Analyzer a, String input, String[] output, int startOffsets[], int endOffsets[], String types[], int posIncrements[]) throws IOException {
-    assertTokenStreamContents(a.tokenStream("dummy", new StringReader(input)), output, startOffsets, endOffsets, types, posIncrements);
+    assertTokenStreamContents(a.tokenStream("dummy", new StringReader(input)), output, startOffsets, endOffsets, types, posIncrements, input.length());
   }
   
   public static void assertAnalyzesTo(Analyzer a, String input, String[] output) throws IOException {
@@ -133,7 +192,7 @@
   
 
   public static void assertAnalyzesToReuse(Analyzer a, String input, String[] output, int startOffsets[], int endOffsets[], String types[], int posIncrements[]) throws IOException {
-    assertTokenStreamContents(a.reusableTokenStream("dummy", new StringReader(input)), output, startOffsets, endOffsets, types, posIncrements);
+    assertTokenStreamContents(a.reusableTokenStream("dummy", new StringReader(input)), output, startOffsets, endOffsets, types, posIncrements, input.length());
   }
   
   public static void assertAnalyzesToReuse(Analyzer a, String input, String[] output) throws IOException {
Index: src/test/org/apache/lucene/analysis/TestLengthFilter.java
===================================================================
--- src/test/org/apache/lucene/analysis/TestLengthFilter.java	(revision 916567)
+++ src/test/org/apache/lucene/analysis/TestLengthFilter.java	(working copy)
@@ -18,13 +18,12 @@
  */
 
 import org.apache.lucene.analysis.tokenattributes.TermAttribute;
-
 import java.io.StringReader;
 
 public class TestLengthFilter extends BaseTokenStreamTestCase {
   
   public void testFilter() throws Exception {
-    TokenStream stream = new WhitespaceTokenizer(
+    TokenStream stream = new WhitespaceTokenizer(TEST_VERSION_CURRENT, 
         new StringReader("short toolong evenmuchlongertext a ab toolong foo"));
     LengthFilter filter = new LengthFilter(stream, 2, 6);
     TermAttribute termAtt = filter.getAttribute(TermAttribute.class);
Index: src/test/org/apache/lucene/analysis/TestISOLatin1AccentFilter.java
===================================================================
--- src/test/org/apache/lucene/analysis/TestISOLatin1AccentFilter.java	(revision 916567)
+++ src/test/org/apache/lucene/analysis/TestISOLatin1AccentFilter.java	(working copy)
@@ -18,12 +18,11 @@
  */
 
 import org.apache.lucene.analysis.tokenattributes.TermAttribute;
-
 import java.io.StringReader;
 
 public class TestISOLatin1AccentFilter extends BaseTokenStreamTestCase {
   public void testU() throws Exception {
-    TokenStream stream = new WhitespaceTokenizer(new StringReader("Des mot clés À LA CHAÎNE À Á Â Ã Ä Å Æ Ç È É Ê Ë Ì Í Î Ï Ĳ Ð Ñ Ò Ó Ô Õ Ö Ø Œ Þ Ù Ú Û Ü Ý Ÿ à á â ã ä å æ ç è é ê ë ì í î ï ĳ ð ñ ò ó ô õ ö ø œ ß þ ù ú û ü ý ÿ ﬁ ﬂ"));
+    TokenStream stream = new WhitespaceTokenizer(TEST_VERSION_CURRENT, new StringReader("Des mot clés À LA CHAÎNE À Á Â Ã Ä Å Æ Ç È É Ê Ë Ì Í Î Ï Ĳ Ð Ñ Ò Ó Ô Õ Ö Ø Œ Þ Ù Ú Û Ü Ý Ÿ à á â ã ä å æ ç è é ê ë ì í î ï ĳ ð ñ ò ó ô õ ö ø œ ß þ ù ú û ü ý ÿ ﬁ ﬂ"));
     ISOLatin1AccentFilter filter = new ISOLatin1AccentFilter(stream);
     TermAttribute termAtt = filter.getAttribute(TermAttribute.class);
     assertTermEquals("Des", filter, termAtt);

Property changes on: src\test\org\apache\lucene\analysis\TestISOLatin1AccentFilter.java
___________________________________________________________________
Modified: svn:mergeinfo
   Merged /lucene/java/trunk/src/test/org/apache/lucene/analysis/TestISOLatin1AccentFilter.java:r889614-889621,889623-889666,889668-889865,899002-916543
   Merged /lucene/java/branches/lucene_2_9/src/test/org/apache/lucene/analysis/TestISOLatin1AccentFilter.java:r909334

Index: src/test/org/apache/lucene/analysis/TestCharArraySet.java
===================================================================
--- src/test/org/apache/lucene/analysis/TestCharArraySet.java	(revision 916567)
+++ src/test/org/apache/lucene/analysis/TestCharArraySet.java	(working copy)
@@ -19,9 +19,11 @@
 
 import java.util.ArrayList;
 import java.util.Arrays;
+import java.util.Collections;
 import java.util.HashSet;
 import java.util.List;
 import java.util.Set;
+import java.util.Iterator;
 
 import org.apache.lucene.util.LuceneTestCase;
 import org.apache.lucene.util.Version;
@@ -39,7 +41,7 @@
   
   
   public void testRehash() throws Exception {
-    CharArraySet cas = new CharArraySet(Version.LUCENE_CURRENT, 0, true);
+    CharArraySet cas = new CharArraySet(TEST_VERSION_CURRENT, 0, true);
     for(int i=0;i<TEST_STOP_WORDS.length;i++)
       cas.add(TEST_STOP_WORDS[i]);
     assertEquals(TEST_STOP_WORDS.length, cas.size());
@@ -50,7 +52,7 @@
   public void testNonZeroOffset() {
     String[] words={"Hello","World","this","is","a","test"};
     char[] findme="xthisy".toCharArray();   
-    CharArraySet set=new CharArraySet(Version.LUCENE_CURRENT, 10,true);
+    CharArraySet set=new CharArraySet(TEST_VERSION_CURRENT, 10,true);
     set.addAll(Arrays.asList(words));
     assertTrue(set.contains(findme, 1, 4));
     assertTrue(set.contains(new String(findme,1,4)));
@@ -62,7 +64,7 @@
   }
   
   public void testObjectContains() {
-    CharArraySet set = new CharArraySet(Version.LUCENE_CURRENT, 10, true);
+    CharArraySet set = new CharArraySet(TEST_VERSION_CURRENT, 10, true);
     Integer val = Integer.valueOf(1);
     set.add(val);
     assertTrue(set.contains(val));
@@ -78,7 +80,7 @@
   }
   
   public void testClear(){
-    CharArraySet set=new CharArraySet(Version.LUCENE_CURRENT, 10,true);
+    CharArraySet set=new CharArraySet(TEST_VERSION_CURRENT, 10,true);
     set.addAll(Arrays.asList(TEST_STOP_WORDS));
     assertEquals("Not all words added", TEST_STOP_WORDS.length, set.size());
     set.clear();
@@ -92,7 +94,7 @@
   }
   
   public void testModifyOnUnmodifiable(){
-    CharArraySet set=new CharArraySet(Version.LUCENE_CURRENT, 10,true);
+    CharArraySet set=new CharArraySet(TEST_VERSION_CURRENT, 10, true);
     set.addAll(Arrays.asList(TEST_STOP_WORDS));
     final int size = set.size();
     set = CharArraySet.unmodifiableSet(set);
@@ -143,8 +145,12 @@
       assertFalse("Test String has been added to unmodifiable set", set.contains(NOT_IN_SET));
       assertEquals("Size of unmodifiable set has changed", size, set.size());
     }
+    
+    // This test was changed in 3.1, as a contains() call on the given Collection using the "correct" iterator's
+    // current key (now a char[]) on a Set<String> would not hit any element of the CAS and therefor never call
+    // remove() on the iterator
     try{
-      set.removeAll(Arrays.asList(TEST_STOP_WORDS));  
+      set.removeAll(new CharArraySet(TEST_VERSION_CURRENT, Arrays.asList(TEST_STOP_WORDS), true));  
       fail("Modified unmodifiable set");
     }catch (UnsupportedOperationException e) {
       // expected
@@ -152,7 +158,7 @@
     }
     
     try{
-      set.retainAll(Arrays.asList(new String[]{NOT_IN_SET}));  
+      set.retainAll(new CharArraySet(TEST_VERSION_CURRENT, Arrays.asList(NOT_IN_SET), true));  
       fail("Modified unmodifiable set");
     }catch (UnsupportedOperationException e) {
       // expected
@@ -173,7 +179,7 @@
   }
   
   public void testUnmodifiableSet(){
-    CharArraySet set = new CharArraySet(Version.LUCENE_CURRENT, 10,true);
+    CharArraySet set = new CharArraySet(TEST_VERSION_CURRENT, 10,true);
     set.addAll(Arrays.asList(TEST_STOP_WORDS));
     set.add(Integer.valueOf(1));
     final int size = set.size();
@@ -203,7 +209,7 @@
         "\ud801\udc1c\ud801\udc1cCDE", "A\ud801\udc1cB"};
     String[] lowerArr = new String[] {"abc\ud801\udc44",
         "\ud801\udc44\ud801\udc44cde", "a\ud801\udc44b"};
-    CharArraySet set = new CharArraySet(Version.LUCENE_31, Arrays.asList(TEST_STOP_WORDS), true);
+    CharArraySet set = new CharArraySet(TEST_VERSION_CURRENT, Arrays.asList(TEST_STOP_WORDS), true);
     for (String upper : upperArr) {
       set.add(upper);
     }
@@ -211,7 +217,7 @@
       assertTrue(String.format(missing, upperArr[i]), set.contains(upperArr[i]));
       assertTrue(String.format(missing, lowerArr[i]), set.contains(lowerArr[i]));
     }
-    set = new CharArraySet(Version.LUCENE_31, Arrays.asList(TEST_STOP_WORDS), false);
+    set = new CharArraySet(TEST_VERSION_CURRENT, Arrays.asList(TEST_STOP_WORDS), false);
     for (String upper : upperArr) {
       set.add(upper);
     }
@@ -229,7 +235,7 @@
 
     String[] lowerArr = new String[] { "abc\uD800", "abc\uD800efg",
         "\uD800efg", "\uD800\ud801\udc44b" };
-    CharArraySet set = new CharArraySet(Version.LUCENE_31, Arrays
+    CharArraySet set = new CharArraySet(TEST_VERSION_CURRENT, Arrays
         .asList(TEST_STOP_WORDS), true);
     for (String upper : upperArr) {
       set.add(upper);
@@ -238,7 +244,7 @@
       assertTrue(String.format(missing, upperArr[i]), set.contains(upperArr[i]));
       assertTrue(String.format(missing, lowerArr[i]), set.contains(lowerArr[i]));
     }
-    set = new CharArraySet(Version.LUCENE_31, Arrays.asList(TEST_STOP_WORDS),
+    set = new CharArraySet(TEST_VERSION_CURRENT, Arrays.asList(TEST_STOP_WORDS),
         false);
     for (String upper : upperArr) {
       set.add(upper);
@@ -321,12 +327,56 @@
     }
   }
   
+  public void testCopyCharArraySetBWCompat() {
+    CharArraySet setIngoreCase = new CharArraySet(TEST_VERSION_CURRENT, 10, true);
+    CharArraySet setCaseSensitive = new CharArraySet(TEST_VERSION_CURRENT, 10, false);
+
+    List<String> stopwords = Arrays.asList(TEST_STOP_WORDS);
+    List<String> stopwordsUpper = new ArrayList<String>();
+    for (String string : stopwords) {
+      stopwordsUpper.add(string.toUpperCase());
+    }
+    setIngoreCase.addAll(Arrays.asList(TEST_STOP_WORDS));
+    setIngoreCase.add(Integer.valueOf(1));
+    setCaseSensitive.addAll(Arrays.asList(TEST_STOP_WORDS));
+    setCaseSensitive.add(Integer.valueOf(1));
+
+    CharArraySet copy = CharArraySet.copy(setIngoreCase);
+    CharArraySet copyCaseSens = CharArraySet.copy(setCaseSensitive);
+
+    assertEquals(setIngoreCase.size(), copy.size());
+    assertEquals(setCaseSensitive.size(), copy.size());
+
+    assertTrue(copy.containsAll(stopwords));
+    assertTrue(copy.containsAll(stopwordsUpper));
+    assertTrue(copyCaseSens.containsAll(stopwords));
+    for (String string : stopwordsUpper) {
+      assertFalse(copyCaseSens.contains(string));
+    }
+    // test adding terms to the copy
+    List<String> newWords = new ArrayList<String>();
+    for (String string : stopwords) {
+      newWords.add(string+"_1");
+    }
+    copy.addAll(newWords);
+    
+    assertTrue(copy.containsAll(stopwords));
+    assertTrue(copy.containsAll(stopwordsUpper));
+    assertTrue(copy.containsAll(newWords));
+    // new added terms are not in the source set
+    for (String string : newWords) {
+      assertFalse(setIngoreCase.contains(string));  
+      assertFalse(setCaseSensitive.contains(string));  
+
+    }
+  }
+  
   /**
    * Test the static #copy() function with a CharArraySet as a source
    */
   public void testCopyCharArraySet() {
-    CharArraySet setIngoreCase = new CharArraySet(Version.LUCENE_CURRENT, 10, true);
-    CharArraySet setCaseSensitive = new CharArraySet(Version.LUCENE_CURRENT, 10, false);
+    CharArraySet setIngoreCase = new CharArraySet(TEST_VERSION_CURRENT, 10, true);
+    CharArraySet setCaseSensitive = new CharArraySet(TEST_VERSION_CURRENT, 10, false);
 
     List<String> stopwords = Arrays.asList(TEST_STOP_WORDS);
     List<String> stopwordsUpper = new ArrayList<String>();
@@ -338,8 +388,8 @@
     setCaseSensitive.addAll(Arrays.asList(TEST_STOP_WORDS));
     setCaseSensitive.add(Integer.valueOf(1));
 
-    CharArraySet copy = CharArraySet.copy(Version.LUCENE_CURRENT, setIngoreCase);
-    CharArraySet copyCaseSens = CharArraySet.copy(Version.LUCENE_CURRENT, setCaseSensitive);
+    CharArraySet copy = CharArraySet.copy(TEST_VERSION_CURRENT, setIngoreCase);
+    CharArraySet copyCaseSens = CharArraySet.copy(TEST_VERSION_CURRENT, setCaseSensitive);
 
     assertEquals(setIngoreCase.size(), copy.size());
     assertEquals(setCaseSensitive.size(), copy.size());
@@ -381,7 +431,7 @@
     }
     set.addAll(Arrays.asList(TEST_STOP_WORDS));
 
-    CharArraySet copy = CharArraySet.copy(Version.LUCENE_CURRENT, set);
+    CharArraySet copy = CharArraySet.copy(TEST_VERSION_CURRENT, set);
 
     assertEquals(set.size(), copy.size());
     assertEquals(set.size(), copy.size());
@@ -404,4 +454,85 @@
       assertFalse(set.contains(string));  
     }
   }
+  
+  /**
+   * Tests a special case of {@link CharArraySet#copy(Version, Set)} where the
+   * set to copy is the {@link CharArraySet#EMPTY_SET}
+   */
+  public void testCopyEmptySet() {
+    assertSame(CharArraySet.EMPTY_SET, 
+        CharArraySet.copy(TEST_VERSION_CURRENT, CharArraySet.EMPTY_SET));
+  }
+
+  /**
+   * Smoketests the static empty set
+   */
+  public void testEmptySet() {
+    assertEquals(0, CharArraySet.EMPTY_SET.size());
+    
+    assertTrue(CharArraySet.EMPTY_SET.isEmpty());
+    for (String stopword : TEST_STOP_WORDS) {
+      assertFalse(CharArraySet.EMPTY_SET.contains(stopword));
+    }
+    assertFalse(CharArraySet.EMPTY_SET.contains((Object) "foo"));
+    assertFalse(CharArraySet.EMPTY_SET.contains((Object) "foo".toCharArray()));
+    assertFalse(CharArraySet.EMPTY_SET.contains("foo".toCharArray(),0,3));
+  }
+  
+  /**
+   * Test for NPE
+   */
+  public void testContainsWithNull() {
+    CharArraySet set = new CharArraySet(TEST_VERSION_CURRENT, 1, true);
+    try {
+      set.contains((char[]) null, 0, 10);
+      fail("null value must raise NPE");
+    } catch (NullPointerException e) {}
+    try {
+      set.contains((CharSequence) null);
+      fail("null value must raise NPE");
+    } catch (NullPointerException e) {}
+    try {
+      set.contains((Object) null);
+      fail("null value must raise NPE");
+    } catch (NullPointerException e) {}
+  }
+  
+  @Deprecated @SuppressWarnings("unchecked")
+  public void testIterator() {
+    HashSet<String> hset = new HashSet<String>();
+    hset.addAll(Arrays.asList(TEST_STOP_WORDS));
+
+    assertTrue("in 3.0 version, iterator should be CharArraySetIterator",
+      ((Iterator) CharArraySet.copy(Version.LUCENE_30, hset).iterator()) instanceof CharArraySet.CharArraySetIterator);
+
+    CharArraySet set = CharArraySet.copy(TEST_VERSION_CURRENT, hset);
+    assertFalse("in current version, iterator should not be CharArraySetIterator",
+      ((Iterator) set.iterator()) instanceof CharArraySet.CharArraySetIterator);
+    
+    Iterator<String> it = set.stringIterator();
+    assertTrue(it instanceof CharArraySet.CharArraySetIterator);
+    while (it.hasNext()) {
+      // as the set returns String instances, this must work:
+      assertTrue(hset.contains(it.next()));
+      try {
+        it.remove();
+        fail("remove() should not work on CharArraySetIterator");
+      } catch (UnsupportedOperationException uoe) {
+        // pass
+      }
+    }
+  }
+  
+  public void testToString() {
+    CharArraySet set = CharArraySet.copy(TEST_VERSION_CURRENT, Collections.singleton("test"));
+    assertEquals("[test]", set.toString());
+    set.add("test2");
+    assertTrue(set.toString().contains(", "));
+    
+    set = CharArraySet.copy(Version.LUCENE_30, Collections.singleton("test"));
+    assertEquals("[test]", set.toString());
+    set.add("test2");
+    assertTrue(set.toString().contains(", "));
+  }
 }
Index: src/test/org/apache/lucene/analysis/TestKeywordAnalyzer.java
===================================================================
--- src/test/org/apache/lucene/analysis/TestKeywordAnalyzer.java	(revision 916567)
+++ src/test/org/apache/lucene/analysis/TestKeywordAnalyzer.java	(working copy)
@@ -31,7 +31,6 @@
 import org.apache.lucene.search.Query;
 import org.apache.lucene.search.ScoreDoc;
 import org.apache.lucene.store.RAMDirectory;
-import org.apache.lucene.util.Version;
 
 public class TestKeywordAnalyzer extends BaseTokenStreamTestCase {
   
@@ -43,7 +42,7 @@
     super.setUp();
     directory = new RAMDirectory();
     IndexWriter writer = new IndexWriter(directory,
-                                         new SimpleAnalyzer(),
+                                         new SimpleAnalyzer(TEST_VERSION_CURRENT),
                                          true, IndexWriter.MaxFieldLength.LIMITED);
 
     Document doc = new Document();
@@ -57,10 +56,10 @@
   }
 
   public void testPerFieldAnalyzer() throws Exception {
-    PerFieldAnalyzerWrapper analyzer = new PerFieldAnalyzerWrapper(new SimpleAnalyzer());
+    PerFieldAnalyzerWrapper analyzer = new PerFieldAnalyzerWrapper(new SimpleAnalyzer(TEST_VERSION_CURRENT));
     analyzer.addAnalyzer("partnum", new KeywordAnalyzer());
 
-    QueryParser queryParser = new QueryParser(Version.LUCENE_CURRENT, "description", analyzer);
+    QueryParser queryParser = new QueryParser(TEST_VERSION_CURRENT, "description", analyzer);
     Query query = queryParser.parse("partnum:Q36 AND SPACE");
 
     ScoreDoc[] hits = searcher.search(query, null, 1000).scoreDocs;
Index: src/test/org/apache/lucene/analysis/TestStopFilter.java
===================================================================
--- src/test/org/apache/lucene/analysis/TestStopFilter.java	(revision 916567)
+++ src/test/org/apache/lucene/analysis/TestStopFilter.java	(working copy)
@@ -38,7 +38,7 @@
   public void testExactCase() throws IOException {
     StringReader reader = new StringReader("Now is The Time");
     Set<String> stopWords = new HashSet<String>(Arrays.asList("is", "the", "Time"));
-    TokenStream stream = new StopFilter(Version.LUCENE_CURRENT, new WhitespaceTokenizer(reader), stopWords, false);
+    TokenStream stream = new StopFilter(TEST_VERSION_CURRENT, new WhitespaceTokenizer(TEST_VERSION_CURRENT, reader), stopWords, false);
     final TermAttribute termAtt = stream.getAttribute(TermAttribute.class);
     assertTrue(stream.incrementToken());
     assertEquals("Now", termAtt.term());
@@ -50,7 +50,7 @@
   public void testIgnoreCase() throws IOException {
     StringReader reader = new StringReader("Now is The Time");
     Set<Object> stopWords = new HashSet<Object>(Arrays.asList( "is", "the", "Time" ));
-    TokenStream stream = new StopFilter(Version.LUCENE_CURRENT, new WhitespaceTokenizer(reader), stopWords, true);
+    TokenStream stream = new StopFilter(TEST_VERSION_CURRENT, new WhitespaceTokenizer(TEST_VERSION_CURRENT, reader), stopWords, true);
     final TermAttribute termAtt = stream.getAttribute(TermAttribute.class);
     assertTrue(stream.incrementToken());
     assertEquals("Now", termAtt.term());
@@ -60,8 +60,8 @@
   public void testStopFilt() throws IOException {
     StringReader reader = new StringReader("Now is The Time");
     String[] stopWords = new String[] { "is", "the", "Time" };
-    Set<Object> stopSet = StopFilter.makeStopSet(Version.LUCENE_CURRENT, stopWords);
-    TokenStream stream = new StopFilter(Version.LUCENE_CURRENT, new WhitespaceTokenizer(reader), stopSet);
+    Set<Object> stopSet = StopFilter.makeStopSet(TEST_VERSION_CURRENT, stopWords);
+    TokenStream stream = new StopFilter(TEST_VERSION_CURRENT, new WhitespaceTokenizer(TEST_VERSION_CURRENT, reader), stopSet);
     final TermAttribute termAtt = stream.getAttribute(TermAttribute.class);
     assertTrue(stream.incrementToken());
     assertEquals("Now", termAtt.term());
@@ -84,14 +84,14 @@
     log(sb.toString());
     String stopWords[] = a.toArray(new String[0]);
     for (int i=0; i<a.size(); i++) log("Stop: "+stopWords[i]);
-    Set<Object> stopSet = StopFilter.makeStopSet(Version.LUCENE_CURRENT, stopWords);
+    Set<Object> stopSet = StopFilter.makeStopSet(TEST_VERSION_CURRENT, stopWords);
     // with increments
     StringReader reader = new StringReader(sb.toString());
-    StopFilter stpf = new StopFilter(Version.LUCENE_24, new WhitespaceTokenizer(reader), stopSet);
+    StopFilter stpf = new StopFilter(Version.LUCENE_24, new WhitespaceTokenizer(TEST_VERSION_CURRENT, reader), stopSet);
     doTestStopPositons(stpf,true);
     // without increments
     reader = new StringReader(sb.toString());
-    stpf = new StopFilter(Version.LUCENE_CURRENT, new WhitespaceTokenizer(reader), stopSet);
+    stpf = new StopFilter(TEST_VERSION_CURRENT, new WhitespaceTokenizer(TEST_VERSION_CURRENT, reader), stopSet);
     doTestStopPositons(stpf,false);
     // with increments, concatenating two stop filters
     ArrayList<String> a0 = new ArrayList<String>();
@@ -107,12 +107,12 @@
     for (int i=0; i<a0.size(); i++) log("Stop0: "+stopWords0[i]);
     String stopWords1[] =  a1.toArray(new String[0]);
     for (int i=0; i<a1.size(); i++) log("Stop1: "+stopWords1[i]);
-    Set<Object> stopSet0 = StopFilter.makeStopSet(Version.LUCENE_CURRENT, stopWords0);
-    Set<Object> stopSet1 = StopFilter.makeStopSet(Version.LUCENE_CURRENT, stopWords1);
+    Set<Object> stopSet0 = StopFilter.makeStopSet(TEST_VERSION_CURRENT, stopWords0);
+    Set<Object> stopSet1 = StopFilter.makeStopSet(TEST_VERSION_CURRENT, stopWords1);
     reader = new StringReader(sb.toString());
-    StopFilter stpf0 = new StopFilter(Version.LUCENE_CURRENT, new WhitespaceTokenizer(reader), stopSet0); // first part of the set
+    StopFilter stpf0 = new StopFilter(TEST_VERSION_CURRENT, new WhitespaceTokenizer(TEST_VERSION_CURRENT, reader), stopSet0); // first part of the set
     stpf0.setEnablePositionIncrements(true);
-    StopFilter stpf01 = new StopFilter(Version.LUCENE_CURRENT, stpf0, stopSet1); // two stop filters concatenated!
+    StopFilter stpf01 = new StopFilter(TEST_VERSION_CURRENT, stpf0, stopSet1); // two stop filters concatenated!
     doTestStopPositons(stpf01,true);
   }
   
Index: src/test/org/apache/lucene/analysis/TestStopAnalyzer.java
===================================================================
--- src/test/org/apache/lucene/analysis/TestStopAnalyzer.java	(revision 916567)
+++ src/test/org/apache/lucene/analysis/TestStopAnalyzer.java	(working copy)
@@ -29,7 +29,7 @@
 
 public class TestStopAnalyzer extends BaseTokenStreamTestCase {
   
-  private StopAnalyzer stop = new StopAnalyzer(Version.LUCENE_CURRENT);
+  private StopAnalyzer stop = new StopAnalyzer(TEST_VERSION_CURRENT);
   private Set<Object> inValidTokens = new HashSet<Object>();
   
   public TestStopAnalyzer(String s) {
@@ -82,7 +82,7 @@
     stopWordsSet.add("good");
     stopWordsSet.add("test");
     stopWordsSet.add("analyzer");
-    StopAnalyzer newStop = new StopAnalyzer(Version.LUCENE_CURRENT, stopWordsSet);
+    StopAnalyzer newStop = new StopAnalyzer(TEST_VERSION_CURRENT, stopWordsSet);
     StringReader reader = new StringReader("This is a good test of the english stop analyzer with positions");
     int expectedIncr[] =                  { 1,   1, 1,          3, 1,  1,      1,            2,   1};
     TokenStream stream = newStop.tokenStream("test", reader);
Index: src/test/org/apache/lucene/collation/CollationTestBase.java
===================================================================
--- src/test/org/apache/lucene/collation/CollationTestBase.java	(revision 916567)
+++ src/test/org/apache/lucene/collation/CollationTestBase.java	(working copy)
@@ -18,7 +18,6 @@
  */
 
 
-import junit.framework.TestCase;
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.PerFieldAnalyzerWrapper;
 import org.apache.lucene.analysis.WhitespaceAnalyzer;
@@ -38,13 +37,14 @@
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.util.IndexableBinaryStringTools;
+import org.apache.lucene.util.LuceneTestCase;
 
 import java.io.IOException;
 import java.nio.CharBuffer;
 import java.nio.ByteBuffer;
 
 
-public class CollationTestBase extends TestCase {
+public class CollationTestBase extends LuceneTestCase {
 
   protected String firstRangeBeginningOriginal = "\u062F";
   protected String firstRangeEndOriginal = "\u0698";
@@ -178,7 +178,7 @@
                                    String usResult) throws Exception {
     RAMDirectory indexStore = new RAMDirectory();
     PerFieldAnalyzerWrapper analyzer
-      = new PerFieldAnalyzerWrapper(new WhitespaceAnalyzer());
+      = new PerFieldAnalyzerWrapper(new WhitespaceAnalyzer(TEST_VERSION_CURRENT));
     analyzer.addAnalyzer("US", usAnalyzer);
     analyzer.addAnalyzer("France", franceAnalyzer);
     analyzer.addAnalyzer("Sweden", swedenAnalyzer);
@@ -229,7 +229,7 @@
     Sort sort = new Sort();
     Query queryX = new TermQuery(new Term ("contents", "x"));
     Query queryY = new TermQuery(new Term ("contents", "y"));
-
+    
     sort.setSort(new SortField("US", SortField.STRING));
     assertMatches(searcher, queryY, sort, usResult);
 
Index: src/test/org/apache/lucene/TestSearch.java
===================================================================
--- src/test/org/apache/lucene/TestSearch.java	(revision 916567)
+++ src/test/org/apache/lucene/TestSearch.java	(working copy)
@@ -22,7 +22,6 @@
 import java.io.StringWriter;
 
 import org.apache.lucene.util.LuceneTestCase;
-import org.apache.lucene.util.Version;
 import junit.framework.TestSuite;
 import junit.textui.TestRunner;
 
@@ -74,7 +73,7 @@
     throws Exception
     {
       Directory directory = new RAMDirectory();
-      Analyzer analyzer = new SimpleAnalyzer();
+      Analyzer analyzer = new SimpleAnalyzer(TEST_VERSION_CURRENT);
       IndexWriter writer = new IndexWriter(directory, analyzer, true, 
                                            IndexWriter.MaxFieldLength.LIMITED);
 
@@ -108,7 +107,7 @@
       };
       ScoreDoc[] hits = null;
 
-      QueryParser parser = new QueryParser(Version.LUCENE_CURRENT, "contents", analyzer);
+      QueryParser parser = new QueryParser(TEST_VERSION_CURRENT, "contents", analyzer);
       parser.setPhraseSlop(4);
       for (int j = 0; j < queries.length; j++) {
         Query query = parser.parse(queries[j]);
Index: src/test/org/apache/lucene/store/TestLockFactory.java
===================================================================
--- src/test/org/apache/lucene/store/TestLockFactory.java	(revision 916567)
+++ src/test/org/apache/lucene/store/TestLockFactory.java	(working copy)
@@ -48,7 +48,7 @@
         // Lock prefix should have been set:
         assertTrue("lock prefix was not set by the RAMDirectory", lf.lockPrefixSet);
 
-        IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(), true,
+        IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true,
                                              IndexWriter.MaxFieldLength.LIMITED);
 
         // add 100 documents (so that commit lock is used)
@@ -81,14 +81,14 @@
         assertTrue("RAMDirectory.setLockFactory did not take",
                    NoLockFactory.class.isInstance(dir.getLockFactory()));
 
-        IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(), true,
+        IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true,
                                              IndexWriter.MaxFieldLength.LIMITED);
 
         // Create a 2nd IndexWriter.  This is normally not allowed but it should run through since we're not
         // using any locks:
         IndexWriter writer2 = null;
         try {
-            writer2 = new IndexWriter(dir, new WhitespaceAnalyzer(), false,
+            writer2 = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), false,
                                       IndexWriter.MaxFieldLength.LIMITED);
         } catch (Exception e) {
             e.printStackTrace(System.out);
@@ -109,13 +109,13 @@
         assertTrue("RAMDirectory did not use correct LockFactory: got " + dir.getLockFactory(),
                    SingleInstanceLockFactory.class.isInstance(dir.getLockFactory()));
 
-        IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(), true,
+        IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true,
                                              IndexWriter.MaxFieldLength.LIMITED);
 
         // Create a 2nd IndexWriter.  This should fail:
         IndexWriter writer2 = null;
         try {
-            writer2 = new IndexWriter(dir, new WhitespaceAnalyzer(), false,
+            writer2 = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), false,
                                       IndexWriter.MaxFieldLength.LIMITED);
             fail("Should have hit an IOException with two IndexWriters on default SingleInstanceLockFactory");
         } catch (IOException e) {
@@ -152,7 +152,7 @@
         FSDirectory fs1 = FSDirectory.open(indexDir, lockFactory);
 
         // First create a 1 doc index:
-        IndexWriter w = new IndexWriter(fs1, new WhitespaceAnalyzer(), true,
+        IndexWriter w = new IndexWriter(fs1, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true,
                                         IndexWriter.MaxFieldLength.LIMITED);
         addDoc(w);
         w.close();
@@ -262,7 +262,7 @@
         }
         @Override
         public void run() {
-            WhitespaceAnalyzer analyzer = new WhitespaceAnalyzer();
+            WhitespaceAnalyzer analyzer = new WhitespaceAnalyzer(TEST_VERSION_CURRENT);
             IndexWriter writer = null;
             for(int i=0;i<this.numIteration;i++) {
                 try {
Index: src/test/org/apache/lucene/store/TestRAMDirectory.java
===================================================================
--- src/test/org/apache/lucene/store/TestRAMDirectory.java	(revision 916567)
+++ src/test/org/apache/lucene/store/TestRAMDirectory.java	(working copy)
@@ -25,7 +25,6 @@
 
 
 import org.apache.lucene.util.LuceneTestCase;
-
 import org.apache.lucene.analysis.WhitespaceAnalyzer;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
@@ -55,7 +54,7 @@
     indexDir = new File(tempDir, "RAMDirIndex");
     
     Directory dir = FSDirectory.open(indexDir);
-    IndexWriter writer  = new IndexWriter(dir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
+    IndexWriter writer  = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
     // add some documents
     Document doc = null;
     for (int i = 0; i < docsToAdd; i++) {
@@ -106,7 +105,7 @@
     final MockRAMDirectory ramDir = new MockRAMDirectory(dir);
     dir.close();
     
-    final IndexWriter writer  = new IndexWriter(ramDir, new WhitespaceAnalyzer(), false, IndexWriter.MaxFieldLength.LIMITED);
+    final IndexWriter writer  = new IndexWriter(ramDir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), false, IndexWriter.MaxFieldLength.LIMITED);
     writer.optimize();
     
     assertEquals(ramDir.sizeInBytes(), ramDir.getRecomputedSizeInBytes());
Index: src/test/org/apache/lucene/store/TestWindowsMMap.java
===================================================================
--- src/test/org/apache/lucene/store/TestWindowsMMap.java	(revision 916567)
+++ src/test/org/apache/lucene/store/TestWindowsMMap.java	(working copy)
@@ -69,7 +69,7 @@
 
     // plan to add a set of useful stopwords, consider changing some of the
     // interior filters.
-    StandardAnalyzer analyzer = new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT, Collections.emptySet());
+    StandardAnalyzer analyzer = new StandardAnalyzer(TEST_VERSION_CURRENT, Collections.emptySet());
     // TODO: something about lock timeouts and leftover locks.
     IndexWriter writer = new IndexWriter(storeDirectory, analyzer, true, IndexWriter.MaxFieldLength.LIMITED);
     IndexSearcher searcher = new IndexSearcher(storeDirectory, true);
Index: src/test/org/apache/lucene/store/TestFileSwitchDirectory.java
===================================================================
--- src/test/org/apache/lucene/store/TestFileSwitchDirectory.java	(revision 916567)
+++ src/test/org/apache/lucene/store/TestFileSwitchDirectory.java	(working copy)
@@ -41,7 +41,7 @@
     RAMDirectory secondaryDir = new MockRAMDirectory();
     
     FileSwitchDirectory fsd = new FileSwitchDirectory(fileExtensions, primaryDir, secondaryDir, true);
-    IndexWriter writer = new IndexWriter(fsd, new WhitespaceAnalyzer(),
+    IndexWriter writer = new IndexWriter(fsd, new WhitespaceAnalyzer(TEST_VERSION_CURRENT),
         IndexWriter.MaxFieldLength.LIMITED);
     writer.setUseCompoundFile(false);
     TestIndexWriterReader.createIndexNoClose(true, "ram", writer);
Index: src/test/org/apache/lucene/store/TestBufferedIndexInput.java
===================================================================
--- src/test/org/apache/lucene/store/TestBufferedIndexInput.java	(revision 916567)
+++ src/test/org/apache/lucene/store/TestBufferedIndexInput.java	(working copy)
@@ -242,7 +242,7 @@
       File indexDir = new File(System.getProperty("tempDir"), "testSetBufferSize");
       MockFSDirectory dir = new MockFSDirectory(indexDir, newRandom());
       try {
-        IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
+        IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
         writer.setUseCompoundFile(false);
         for(int i=0;i<37;i++) {
           Document doc = new Document();
Index: src/test/org/apache/lucene/TestDemo.java
===================================================================
--- src/test/org/apache/lucene/TestDemo.java	(revision 916567)
+++ src/test/org/apache/lucene/TestDemo.java	(working copy)
@@ -34,7 +34,6 @@
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.RAMDirectory;
 import org.apache.lucene.util.LuceneTestCase;
-import org.apache.lucene.util.Version;
 
 /**
  * A very simple demo used in the API documentation (src/java/overview.html).
@@ -46,7 +45,7 @@
 
   public void testDemo() throws IOException, ParseException {
 
-    Analyzer analyzer = new StandardAnalyzer(Version.LUCENE_CURRENT);
+    Analyzer analyzer = new StandardAnalyzer(TEST_VERSION_CURRENT);
 
     // Store the index in memory:
     Directory directory = new RAMDirectory();
@@ -67,7 +66,7 @@
 
     assertEquals(1, isearcher.search(new TermQuery(new Term("fieldname", longTerm)), 1).totalHits);
     // Parse a simple query that searches for "text":
-    QueryParser parser = new QueryParser(Version.LUCENE_CURRENT, "fieldname", analyzer);
+    QueryParser parser = new QueryParser(TEST_VERSION_CURRENT, "fieldname", analyzer);
     Query query = parser.parse("text");
     ScoreDoc[] hits = isearcher.search(query, null, 1).scoreDocs;
     assertEquals(1, hits.length);
Index: src/test/org/apache/lucene/search/TestNot.java
===================================================================
--- src/test/org/apache/lucene/search/TestNot.java	(revision 916567)
+++ src/test/org/apache/lucene/search/TestNot.java	(working copy)
@@ -25,7 +25,6 @@
 import org.apache.lucene.analysis.SimpleAnalyzer;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
-import org.apache.lucene.util.Version;
 
 /** Similarity unit test.
  *
@@ -39,7 +38,7 @@
 
   public void testNot() throws Exception {
     RAMDirectory store = new RAMDirectory();
-    IndexWriter writer = new IndexWriter(store, new SimpleAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
+    IndexWriter writer = new IndexWriter(store, new SimpleAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
 
     Document d1 = new Document();
     d1.add(new Field("field", "a b", Field.Store.YES, Field.Index.ANALYZED));
@@ -49,7 +48,7 @@
     writer.close();
 
     Searcher searcher = new IndexSearcher(store, true);
-      QueryParser parser = new QueryParser(Version.LUCENE_CURRENT, "field", new SimpleAnalyzer());
+      QueryParser parser = new QueryParser(TEST_VERSION_CURRENT, "field", new SimpleAnalyzer(TEST_VERSION_CURRENT));
     Query query = parser.parse("a NOT b");
     //System.out.println(query);
     ScoreDoc[] hits = searcher.search(query, null, 1000).scoreDocs;
Index: src/test/org/apache/lucene/search/TestTimeLimitingCollector.java
===================================================================
--- src/test/org/apache/lucene/search/TestTimeLimitingCollector.java	(revision 916567)
+++ src/test/org/apache/lucene/search/TestTimeLimitingCollector.java	(working copy)
@@ -31,7 +31,6 @@
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.RAMDirectory;
 import org.apache.lucene.util.LuceneTestCase;
-import org.apache.lucene.util.Version;
 import org.apache.lucene.util.ThreadInterruptedException;
 
 /**
@@ -76,7 +75,7 @@
         "blueberry pizza",
     };
     Directory directory = new RAMDirectory();
-    IndexWriter iw = new IndexWriter(directory, new WhitespaceAnalyzer(), true, MaxFieldLength.UNLIMITED);
+    IndexWriter iw = new IndexWriter(directory, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, MaxFieldLength.UNLIMITED);
     
     for (int i=0; i<N_DOCS; i++) {
       add(docText[i%docText.length], iw);
@@ -89,7 +88,7 @@
     for (int i = 1; i < docText.length; i++) {
       qtxt += ' ' + docText[i]; // large query so that search will be longer
     }
-    QueryParser queryParser = new QueryParser(Version.LUCENE_CURRENT, FIELD_NAME, new WhitespaceAnalyzer());
+    QueryParser queryParser = new QueryParser(TEST_VERSION_CURRENT, FIELD_NAME, new WhitespaceAnalyzer(TEST_VERSION_CURRENT));
     query = queryParser.parse(qtxt);
     
     // warm the searcher
Index: src/test/org/apache/lucene/search/TestPrefixFilter.java
===================================================================
--- src/test/org/apache/lucene/search/TestPrefixFilter.java	(revision 916567)
+++ src/test/org/apache/lucene/search/TestPrefixFilter.java	(working copy)
@@ -37,7 +37,7 @@
                                         "/Computers/Mac/One",
                                         "/Computers/Mac/Two",
                                         "/Computers/Windows"};
-    IndexWriter writer = new IndexWriter(directory, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
+    IndexWriter writer = new IndexWriter(directory, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
     for (int i = 0; i < categories.length; i++) {
       Document doc = new Document();
       doc.add(new Field("category", categories[i], Field.Store.YES, Field.Index.NOT_ANALYZED));
Index: src/test/org/apache/lucene/search/TestCachingWrapperFilter.java
===================================================================
--- src/test/org/apache/lucene/search/TestCachingWrapperFilter.java	(revision 916567)
+++ src/test/org/apache/lucene/search/TestCachingWrapperFilter.java	(working copy)
@@ -32,7 +32,7 @@
 public class TestCachingWrapperFilter extends LuceneTestCase {
   public void testCachingWorks() throws Exception {
     Directory dir = new RAMDirectory();
-    IndexWriter writer = new IndexWriter(dir, new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
+    IndexWriter writer = new IndexWriter(dir, new StandardAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
     writer.close();
 
     IndexReader reader = IndexReader.open(dir, true);
@@ -71,7 +71,7 @@
   
   public void testIsCacheAble() throws Exception {
     Directory dir = new RAMDirectory();
-    IndexWriter writer = new IndexWriter(dir, new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
+    IndexWriter writer = new IndexWriter(dir, new StandardAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
     writer.close();
 
     IndexReader reader = IndexReader.open(dir, true);
Index: src/test/org/apache/lucene/search/TestBooleanQuery.java
===================================================================
--- src/test/org/apache/lucene/search/TestBooleanQuery.java	(revision 916567)
+++ src/test/org/apache/lucene/search/TestBooleanQuery.java	(working copy)
@@ -60,7 +60,7 @@
   // LUCENE-1630
   public void testNullOrSubScorer() throws Throwable {
     Directory dir = new MockRAMDirectory();
-    IndexWriter w = new IndexWriter(dir, new WhitespaceAnalyzer(), IndexWriter.MaxFieldLength.UNLIMITED);
+    IndexWriter w = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), IndexWriter.MaxFieldLength.UNLIMITED);
     Document doc = new Document();
     doc.add(new Field("field", "a b c d", Field.Store.NO, Field.Index.ANALYZED));
     w.addDocument(doc);
Index: src/test/org/apache/lucene/search/TestPhraseQuery.java
===================================================================
--- src/test/org/apache/lucene/search/TestPhraseQuery.java	(revision 916567)
+++ src/test/org/apache/lucene/search/TestPhraseQuery.java	(working copy)
@@ -51,7 +51,7 @@
     Analyzer analyzer = new Analyzer() {
       @Override
       public TokenStream tokenStream(String fieldName, Reader reader) {
-        return new WhitespaceTokenizer(reader);
+        return new WhitespaceTokenizer(TEST_VERSION_CURRENT, reader);
       }
 
       @Override
@@ -239,7 +239,7 @@
   
   public void testPhraseQueryInConjunctionScorer() throws Exception {
     RAMDirectory directory = new RAMDirectory();
-    IndexWriter writer = new IndexWriter(directory, new WhitespaceAnalyzer(), true, 
+    IndexWriter writer = new IndexWriter(directory, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, 
                                          IndexWriter.MaxFieldLength.LIMITED);
     
     Document doc = new Document();
@@ -275,7 +275,7 @@
     
     searcher.close();
     
-    writer = new IndexWriter(directory, new WhitespaceAnalyzer(), true, 
+    writer = new IndexWriter(directory, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, 
                              IndexWriter.MaxFieldLength.LIMITED);
     doc = new Document();
     doc.add(new Field("contents", "map entry woo", Field.Store.YES, Field.Index.ANALYZED));
@@ -325,7 +325,7 @@
   
   public void testSlopScoring() throws IOException {
     Directory directory = new RAMDirectory();
-    IndexWriter writer = new IndexWriter(directory, new WhitespaceAnalyzer(), true, 
+    IndexWriter writer = new IndexWriter(directory, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, 
                                          IndexWriter.MaxFieldLength.LIMITED);
 
     Document doc = new Document();
@@ -362,8 +362,8 @@
   }
   
   public void testToString() throws Exception {
-    StopAnalyzer analyzer = new StopAnalyzer(Version.LUCENE_CURRENT);
-    QueryParser qp = new QueryParser(Version.LUCENE_CURRENT, "field", analyzer);
+    StopAnalyzer analyzer = new StopAnalyzer(TEST_VERSION_CURRENT);
+    QueryParser qp = new QueryParser(TEST_VERSION_CURRENT, "field", analyzer);
     qp.setEnablePositionIncrements(true);
     PhraseQuery q = (PhraseQuery)qp.parse("\"this hi this is a test is\"");
     assertEquals("field:\"? hi ? ? ? test\"", q.toString());
Index: src/test/org/apache/lucene/search/TestPositionIncrement.java
===================================================================
--- src/test/org/apache/lucene/search/TestPositionIncrement.java	(revision 916567)
+++ src/test/org/apache/lucene/search/TestPositionIncrement.java	(working copy)
@@ -80,6 +80,7 @@
           public boolean incrementToken() {
             if (i == TOKENS.length)
               return false;
+            clearAttributes();
             termAtt.setTermBuffer(TOKENS[i]);
             offsetAtt.setOffset(i,i);
             posIncrAtt.setPositionIncrement(INCREMENTS[i]);
@@ -193,7 +194,7 @@
     assertEquals(0, hits.length);
 
     // should not find "1 2" because there is a gap of 1 in the index
-    QueryParser qp = new QueryParser(Version.LUCENE_CURRENT, "field",
+    QueryParser qp = new QueryParser(TEST_VERSION_CURRENT, "field",
                                      new StopWhitespaceAnalyzer(false));
     q = (PhraseQuery) qp.parse("\"1 2\"");
     hits = searcher.search(q, null, 1000).scoreDocs;
@@ -217,7 +218,7 @@
     assertEquals(0, hits.length);
       
     // when both qp qnd stopFilter propagate increments, we should find the doc.
-    qp = new QueryParser(Version.LUCENE_CURRENT, "field",
+    qp = new QueryParser(TEST_VERSION_CURRENT, "field",
                          new StopWhitespaceAnalyzer(true));
     qp.setEnablePositionIncrements(true);
     q = (PhraseQuery) qp.parse("\"1 stop 2\"");
@@ -227,15 +228,15 @@
 
   private static class StopWhitespaceAnalyzer extends Analyzer {
     boolean enablePositionIncrements;
-    final WhitespaceAnalyzer a = new WhitespaceAnalyzer();
+    final WhitespaceAnalyzer a = new WhitespaceAnalyzer(TEST_VERSION_CURRENT);
     public StopWhitespaceAnalyzer(boolean enablePositionIncrements) {
       this.enablePositionIncrements = enablePositionIncrements;
     }
     @Override
     public TokenStream tokenStream(String fieldName, Reader reader) {
       TokenStream ts = a.tokenStream(fieldName,reader);
-      return new StopFilter(enablePositionIncrements?Version.LUCENE_CURRENT:Version.LUCENE_24, ts,
-          new CharArraySet(Version.LUCENE_CURRENT, Collections.singleton("stop"), true));
+      return new StopFilter(enablePositionIncrements?TEST_VERSION_CURRENT:Version.LUCENE_24, ts,
+          new CharArraySet(TEST_VERSION_CURRENT, Collections.singleton("stop"), true));
     }
   }
   
@@ -332,7 +333,7 @@
 
   @Override
   public TokenStream tokenStream(String fieldName, Reader reader) {
-    TokenStream result = new LowerCaseTokenizer(reader);
+    TokenStream result = new LowerCaseTokenizer(LuceneTestCase.TEST_VERSION_CURRENT, reader);
     return new PayloadFilter(result, fieldName);
   }
 }
Index: src/test/org/apache/lucene/search/TestTermRangeQuery.java
===================================================================
--- src/test/org/apache/lucene/search/TestTermRangeQuery.java	(revision 916567)
+++ src/test/org/apache/lucene/search/TestTermRangeQuery.java	(working copy)
@@ -122,7 +122,7 @@
   }
   
   private void checkBooleanTerms(Searcher searcher, TermRangeQuery query, String... terms) throws IOException {
-    query.setRewriteMethod(MultiTermQuery.TOP_TERMS_SCORING_BOOLEAN_REWRITE);
+    query.setRewriteMethod(new MultiTermQuery.TopTermsScoringBooleanQueryRewrite());
     final BooleanQuery bq = (BooleanQuery) searcher.rewrite(query);
     final Set<String> allowedTerms = new HashSet<String>(Arrays.asList(terms));
     assertEquals(allowedTerms.size(), bq.clauses().size());
@@ -284,6 +284,7 @@
         if (done)
           return false;
         else {
+          clearAttributes();
           done = true;
           if (count == 1) {
             termAtt.termBuffer()[0] = buffer[0];
@@ -319,7 +320,7 @@
   }
 
   private void initializeIndex(String[] values) throws IOException {
-    initializeIndex(values, new WhitespaceAnalyzer());
+    initializeIndex(values, new WhitespaceAnalyzer(TEST_VERSION_CURRENT));
   }
 
   private void initializeIndex(String[] values, Analyzer analyzer) throws IOException {
@@ -331,7 +332,7 @@
   }
 
   private void addDoc(String content) throws IOException {
-    IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(), false, IndexWriter.MaxFieldLength.LIMITED);
+    IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), false, IndexWriter.MaxFieldLength.LIMITED);
     insertDoc(writer, content);
     writer.close();
   }
Index: src/test/org/apache/lucene/search/TestSloppyPhraseQuery.java
===================================================================
--- src/test/org/apache/lucene/search/TestSloppyPhraseQuery.java	(revision 916567)
+++ src/test/org/apache/lucene/search/TestSloppyPhraseQuery.java	(working copy)
@@ -18,7 +18,6 @@
  */
 
 import org.apache.lucene.util.LuceneTestCase;
-
 import org.apache.lucene.analysis.WhitespaceAnalyzer;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
@@ -117,7 +116,7 @@
     query.setSlop(slop);
 
     RAMDirectory ramDir = new RAMDirectory();
-    WhitespaceAnalyzer analyzer = new WhitespaceAnalyzer();
+    WhitespaceAnalyzer analyzer = new WhitespaceAnalyzer(TEST_VERSION_CURRENT);
     IndexWriter writer = new IndexWriter(ramDir, analyzer, MaxFieldLength.UNLIMITED);
     writer.addDocument(doc);
     writer.close();
Index: src/test/org/apache/lucene/search/TestBooleanMinShouldMatch.java
===================================================================
--- src/test/org/apache/lucene/search/TestBooleanMinShouldMatch.java	(revision 916567)
+++ src/test/org/apache/lucene/search/TestBooleanMinShouldMatch.java	(working copy)
@@ -59,7 +59,7 @@
 
         index = new RAMDirectory();
         IndexWriter writer = new IndexWriter(index,
-                                             new WhitespaceAnalyzer(),
+                                             new WhitespaceAnalyzer(TEST_VERSION_CURRENT),
                                              true, IndexWriter.MaxFieldLength.LIMITED);
 
         for (int i = 0; i < data.length; i++) {
Index: src/test/org/apache/lucene/search/TestBooleanOr.java
===================================================================
--- src/test/org/apache/lucene/search/TestBooleanOr.java	(revision 916567)
+++ src/test/org/apache/lucene/search/TestBooleanOr.java	(working copy)
@@ -135,7 +135,7 @@
     RAMDirectory rd = new RAMDirectory();
 
     //
-    IndexWriter writer = new IndexWriter(rd, new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
+    IndexWriter writer = new IndexWriter(rd, new StandardAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
 
     //
     Document d = new Document();
Index: src/test/org/apache/lucene/search/TestDateSort.java
===================================================================
--- src/test/org/apache/lucene/search/TestDateSort.java	(revision 916567)
+++ src/test/org/apache/lucene/search/TestDateSort.java	(working copy)
@@ -33,7 +33,6 @@
 import org.apache.lucene.search.SortField;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.RAMDirectory;
-import org.apache.lucene.util.Version;
 
 /**
  * Test date sorting, i.e. auto-sorting of fields with type "long".
@@ -51,7 +50,7 @@
     super.setUp();
     // Create an index writer.
     directory = new RAMDirectory();
-    IndexWriter writer = new IndexWriter(directory, new WhitespaceAnalyzer(), true,
+    IndexWriter writer = new IndexWriter(directory, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true,
                                          IndexWriter.MaxFieldLength.LIMITED);
 
     // oldest doc:
@@ -76,7 +75,7 @@
 
     Sort sort = new Sort(new SortField(DATE_TIME_FIELD, SortField.STRING, true));
 
-    QueryParser queryParser = new QueryParser(Version.LUCENE_CURRENT, TEXT_FIELD, new WhitespaceAnalyzer());
+    QueryParser queryParser = new QueryParser(TEST_VERSION_CURRENT, TEXT_FIELD, new WhitespaceAnalyzer(TEST_VERSION_CURRENT));
     Query query = queryParser.parse("Document");
 
     // Execute the search and process the search results.
Index: src/test/org/apache/lucene/search/TestSort.java
===================================================================
--- src/test/org/apache/lucene/search/TestSort.java	(revision 916567)
+++ src/test/org/apache/lucene/search/TestSort.java	(working copy)
@@ -104,7 +104,7 @@
   private Searcher getIndex (boolean even, boolean odd)
   throws IOException {
     RAMDirectory indexStore = new RAMDirectory ();
-    IndexWriter writer = new IndexWriter (indexStore, new SimpleAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
+    IndexWriter writer = new IndexWriter (indexStore, new SimpleAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
     writer.setMaxBufferedDocs(2);
     writer.setMergeFactor(1000);
     for (int i=0; i<data.length; ++i) {
@@ -140,7 +140,7 @@
   
   private IndexSearcher getFullStrings() throws CorruptIndexException, LockObtainFailedException, IOException {
     RAMDirectory indexStore = new RAMDirectory ();
-    IndexWriter writer = new IndexWriter (indexStore, new SimpleAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
+    IndexWriter writer = new IndexWriter (indexStore, new SimpleAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
     writer.setMaxBufferedDocs(4);
     writer.setMergeFactor(97);
     for (int i=0; i<NUM_STRINGS; i++) {
Index: src/test/org/apache/lucene/search/TestMultiTermConstantScore.java
===================================================================
--- src/test/org/apache/lucene/search/TestMultiTermConstantScore.java	(revision 916567)
+++ src/test/org/apache/lucene/search/TestMultiTermConstantScore.java	(working copy)
@@ -26,7 +26,6 @@
 import org.apache.lucene.index.Term;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.RAMDirectory;
-
 import java.io.IOException;
 import java.text.Collator;
 import java.util.Locale;
@@ -65,7 +64,7 @@
         "X       4 5 6" };
 
     small = new RAMDirectory();
-    IndexWriter writer = new IndexWriter(small, new WhitespaceAnalyzer(), true,
+    IndexWriter writer = new IndexWriter(small, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true,
         IndexWriter.MaxFieldLength.LIMITED);
 
     for (int i = 0; i < data.length; i++) {
@@ -616,7 +615,7 @@
 
     /* build an index */
     RAMDirectory farsiIndex = new RAMDirectory();
-    IndexWriter writer = new IndexWriter(farsiIndex, new SimpleAnalyzer(), T,
+    IndexWriter writer = new IndexWriter(farsiIndex, new SimpleAnalyzer(TEST_VERSION_CURRENT), T,
         IndexWriter.MaxFieldLength.LIMITED);
     Document doc = new Document();
     doc.add(new Field("content", "\u0633\u0627\u0628", Field.Store.YES,
@@ -656,7 +655,7 @@
 
     /* build an index */
     RAMDirectory danishIndex = new RAMDirectory();
-    IndexWriter writer = new IndexWriter(danishIndex, new SimpleAnalyzer(), T,
+    IndexWriter writer = new IndexWriter(danishIndex, new SimpleAnalyzer(TEST_VERSION_CURRENT), T,
                                          IndexWriter.MaxFieldLength.LIMITED);
 
     // Danish collation orders the words below in the given order
Index: src/test/org/apache/lucene/search/payloads/TestPayloadNearQuery.java
===================================================================
--- src/test/org/apache/lucene/search/payloads/TestPayloadNearQuery.java	(revision 916567)
+++ src/test/org/apache/lucene/search/payloads/TestPayloadNearQuery.java	(working copy)
@@ -37,6 +37,7 @@
 import org.apache.lucene.search.TopDocs;
 import org.apache.lucene.search.spans.SpanQuery;
 import org.apache.lucene.search.spans.SpanNearQuery;
+import org.apache.lucene.search.spans.SpanTermQuery;
 import org.apache.lucene.store.RAMDirectory;
 import org.apache.lucene.util.English;
 import org.apache.lucene.util.LuceneTestCase;
@@ -56,20 +57,18 @@
   private class PayloadAnalyzer extends Analyzer {
     @Override
     public TokenStream tokenStream(String fieldName, Reader reader) {
-      TokenStream result = new LowerCaseTokenizer(reader);
+      TokenStream result = new LowerCaseTokenizer(TEST_VERSION_CURRENT, reader);
       result = new PayloadFilter(result, fieldName);
       return result;
     }
   }
 
   private class PayloadFilter extends TokenFilter {
-    String fieldName;
     int numSeen = 0;
     protected PayloadAttribute payAtt;
 
     public PayloadFilter(TokenStream input, String fieldName) {
       super(input);
-      this.fieldName = fieldName;
       payAtt = addAttribute(PayloadAttribute.class);
     }
 
@@ -93,7 +92,7 @@
     String[] words = phrase.split("[\\s]+");
     SpanQuery clauses[] = new SpanQuery[words.length];
     for (int i=0;i<clauses.length;i++) {
-      clauses[i] = new PayloadTermQuery(new Term(fieldName, words[i]), new AveragePayloadFunction());  
+      clauses[i] = new SpanTermQuery(new Term(fieldName, words[i]));  
     } 
     return new PayloadNearQuery(clauses, 0, inOrder);
   }
Index: src/test/org/apache/lucene/search/payloads/TestPayloadTermQuery.java
===================================================================
--- src/test/org/apache/lucene/search/payloads/TestPayloadTermQuery.java	(revision 916567)
+++ src/test/org/apache/lucene/search/payloads/TestPayloadTermQuery.java	(working copy)
@@ -66,7 +66,7 @@
 
     @Override
     public TokenStream tokenStream(String fieldName, Reader reader) {
-      TokenStream result = new LowerCaseTokenizer(reader);
+      TokenStream result = new LowerCaseTokenizer(TEST_VERSION_CURRENT, reader);
       result = new PayloadFilter(result, fieldName);
       return result;
     }
Index: src/test/org/apache/lucene/search/payloads/PayloadHelper.java
===================================================================
--- src/test/org/apache/lucene/search/payloads/PayloadHelper.java	(revision 916567)
+++ src/test/org/apache/lucene/search/payloads/PayloadHelper.java	(working copy)
@@ -27,6 +27,7 @@
 import org.apache.lucene.util.English;
 import org.apache.lucene.search.IndexSearcher;
 import org.apache.lucene.search.Similarity;
+import static org.apache.lucene.util.LuceneTestCaseJ4.TEST_VERSION_CURRENT;
 
 import java.io.Reader;
 import java.io.IOException;
@@ -50,7 +51,7 @@
 
     @Override
     public TokenStream tokenStream(String fieldName, Reader reader) {
-      TokenStream result = new LowerCaseTokenizer(reader);
+      TokenStream result = new LowerCaseTokenizer(TEST_VERSION_CURRENT, reader);
       result = new PayloadFilter(result, fieldName);
       return result;
     }
Index: src/test/org/apache/lucene/search/TestBooleanScorer.java
===================================================================
--- src/test/org/apache/lucene/search/TestBooleanScorer.java	(revision 916567)
+++ src/test/org/apache/lucene/search/TestBooleanScorer.java	(working copy)
@@ -44,7 +44,7 @@
     String[] values = new String[] { "1", "2", "3", "4" };
 
     try {
-      IndexWriter writer = new IndexWriter(directory, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
+      IndexWriter writer = new IndexWriter(directory, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
       for (int i = 0; i < values.length; i++) {
         Document doc = new Document();
         doc.add(new Field(FIELD, values[i], Field.Store.YES, Field.Index.NOT_ANALYZED));
Index: src/test/org/apache/lucene/search/TestDocIdSet.java
===================================================================
--- src/test/org/apache/lucene/search/TestDocIdSet.java	(revision 916567)
+++ src/test/org/apache/lucene/search/TestDocIdSet.java	(working copy)
@@ -105,7 +105,7 @@
     // Tests that if a Filter produces a null DocIdSet, which is given to
     // IndexSearcher, everything works fine. This came up in LUCENE-1754.
     Directory dir = new RAMDirectory();
-    IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(), MaxFieldLength.UNLIMITED);
+    IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), MaxFieldLength.UNLIMITED);
     Document doc = new Document();
     doc.add(new Field("c", "val", Store.NO, Index.NOT_ANALYZED_NO_NORMS));
     writer.addDocument(doc);
Index: src/test/org/apache/lucene/search/spans/TestNearSpansOrdered.java
===================================================================
--- src/test/org/apache/lucene/search/spans/TestNearSpansOrdered.java	(revision 916567)
+++ src/test/org/apache/lucene/search/spans/TestNearSpansOrdered.java	(working copy)
@@ -30,14 +30,13 @@
 import org.apache.lucene.search.Scorer;
 import org.apache.lucene.store.RAMDirectory;
 import org.apache.lucene.util.LuceneTestCase;
-import org.apache.lucene.util.Version;
 
 public class TestNearSpansOrdered extends LuceneTestCase {
   protected IndexSearcher searcher;
 
   public static final String FIELD = "field";
   public static final QueryParser qp =
-    new QueryParser(Version.LUCENE_CURRENT, FIELD, new WhitespaceAnalyzer());
+    new QueryParser(TEST_VERSION_CURRENT, FIELD, new WhitespaceAnalyzer(TEST_VERSION_CURRENT));
 
   @Override
   public void tearDown() throws Exception {
@@ -49,7 +48,7 @@
   public void setUp() throws Exception {
     super.setUp();
     RAMDirectory directory = new RAMDirectory();
-    IndexWriter writer= new IndexWriter(directory, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
+    IndexWriter writer= new IndexWriter(directory, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
     for (int i = 0; i < docFields.length; i++) {
       Document doc = new Document();
       doc.add(new Field(FIELD, docFields[i], Field.Store.NO, Field.Index.ANALYZED));
Index: src/test/org/apache/lucene/search/spans/TestBasics.java
===================================================================
--- src/test/org/apache/lucene/search/spans/TestBasics.java	(revision 916567)
+++ src/test/org/apache/lucene/search/spans/TestBasics.java	(working copy)
@@ -55,7 +55,7 @@
   public void setUp() throws Exception {
     super.setUp();
     RAMDirectory directory = new RAMDirectory();
-    IndexWriter writer = new IndexWriter(directory, new SimpleAnalyzer(), true, 
+    IndexWriter writer = new IndexWriter(directory, new SimpleAnalyzer(TEST_VERSION_CURRENT), true, 
                                          IndexWriter.MaxFieldLength.LIMITED);
     //writer.infoStream = System.out;
     for (int i = 0; i < 1000; i++) {
Index: src/test/org/apache/lucene/search/spans/TestSpans.java
===================================================================
--- src/test/org/apache/lucene/search/spans/TestSpans.java	(revision 916567)
+++ src/test/org/apache/lucene/search/spans/TestSpans.java	(working copy)
@@ -37,7 +37,6 @@
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.util.LuceneTestCase;
-
 import java.io.IOException;
 import java.util.Collections;
 
@@ -50,7 +49,7 @@
   public void setUp() throws Exception {
     super.setUp();
     RAMDirectory directory = new RAMDirectory();
-    IndexWriter writer= new IndexWriter(directory, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
+    IndexWriter writer= new IndexWriter(directory, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
     for (int i = 0; i < docFields.length; i++) {
       Document doc = new Document();
       doc.add(new Field(field, docFields[i], Field.Store.YES, Field.Index.ANALYZED));
@@ -452,7 +451,7 @@
   // LUCENE-1404
   public void testNPESpanQuery() throws Throwable {
     final Directory dir = new MockRAMDirectory();
-    final IndexWriter writer = new IndexWriter(dir, new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT, Collections.emptySet()), IndexWriter.MaxFieldLength.LIMITED);
+    final IndexWriter writer = new IndexWriter(dir, new StandardAnalyzer(TEST_VERSION_CURRENT, Collections.emptySet()), IndexWriter.MaxFieldLength.LIMITED);
 
     // Add documents
     addDoc(writer, "1", "the big dogs went running to the market");
Index: src/test/org/apache/lucene/search/spans/TestFieldMaskingSpanQuery.java
===================================================================
--- src/test/org/apache/lucene/search/spans/TestFieldMaskingSpanQuery.java	(revision 916567)
+++ src/test/org/apache/lucene/search/spans/TestFieldMaskingSpanQuery.java	(working copy)
@@ -54,7 +54,7 @@
     super.setUp();
     RAMDirectory directory = new RAMDirectory();
     IndexWriter writer= new IndexWriter(directory,
-                                        new WhitespaceAnalyzer(), true,
+                                        new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true,
                                         IndexWriter.MaxFieldLength.LIMITED);
     
     writer.addDocument(doc(new Field[] { field("id", "0")
Index: src/test/org/apache/lucene/search/spans/TestSpansAdvanced.java
===================================================================
--- src/test/org/apache/lucene/search/spans/TestSpansAdvanced.java	(revision 916567)
+++ src/test/org/apache/lucene/search/spans/TestSpansAdvanced.java	(working copy)
@@ -56,7 +56,7 @@
 
         // create test index
         mDirectory = new RAMDirectory();
-        final IndexWriter writer = new IndexWriter(mDirectory, new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
+        final IndexWriter writer = new IndexWriter(mDirectory, new StandardAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
         addDocument(writer, "1", "I think it should work.");
         addDocument(writer, "2", "I think it should work.");
         addDocument(writer, "3", "I think it should work.");
Index: src/test/org/apache/lucene/search/spans/TestSpansAdvanced2.java
===================================================================
--- src/test/org/apache/lucene/search/spans/TestSpansAdvanced2.java	(revision 916567)
+++ src/test/org/apache/lucene/search/spans/TestSpansAdvanced2.java	(working copy)
@@ -40,7 +40,7 @@
         super.setUp();
 
         // create test index
-        final IndexWriter writer = new IndexWriter(mDirectory, new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT), false, IndexWriter.MaxFieldLength.LIMITED);
+        final IndexWriter writer = new IndexWriter(mDirectory, new StandardAnalyzer(TEST_VERSION_CURRENT), false, IndexWriter.MaxFieldLength.LIMITED);
         addDocument(writer, "A", "Should we, could we, would we?");
         addDocument(writer, "B", "It should.  Should it?");
         addDocument(writer, "C", "It shouldn't.");
Index: src/test/org/apache/lucene/search/spans/TestPayloadSpans.java
===================================================================
--- src/test/org/apache/lucene/search/spans/TestPayloadSpans.java	(revision 916567)
+++ src/test/org/apache/lucene/search/spans/TestPayloadSpans.java	(working copy)
@@ -468,7 +468,7 @@
 
     @Override
     public TokenStream tokenStream(String fieldName, Reader reader) {
-      TokenStream result = new LowerCaseTokenizer(reader);
+      TokenStream result = new LowerCaseTokenizer(TEST_VERSION_CURRENT, reader);
       result = new PayloadFilter(result, fieldName);
       return result;
     }
@@ -520,7 +520,7 @@
 
     @Override
     public TokenStream tokenStream(String fieldName, Reader reader) {
-      TokenStream result = new LowerCaseTokenizer(reader);
+      TokenStream result = new LowerCaseTokenizer(TEST_VERSION_CURRENT, reader);
       result = new PayloadFilter(result, fieldName);
       return result;
     }
Index: src/test/org/apache/lucene/search/TestMultiSearcher.java
===================================================================
--- src/test/org/apache/lucene/search/TestMultiSearcher.java	(revision 916567)
+++ src/test/org/apache/lucene/search/TestMultiSearcher.java	(working copy)
@@ -30,8 +30,6 @@
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.RAMDirectory;
 import org.apache.lucene.store.MockRAMDirectory;
-import org.apache.lucene.util.Version;
-
 import java.io.IOException;
 import java.util.Collections;
 import java.util.HashSet;
@@ -84,9 +82,9 @@
         lDoc3.add(new Field("handle", "1", Field.Store.YES, Field.Index.NOT_ANALYZED));
 
         // creating an index writer for the first index
-        IndexWriter writerA = new IndexWriter(indexStoreA, new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
+        IndexWriter writerA = new IndexWriter(indexStoreA, new StandardAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
         // creating an index writer for the second index, but writing nothing
-        IndexWriter writerB = new IndexWriter(indexStoreB, new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
+        IndexWriter writerB = new IndexWriter(indexStoreB, new StandardAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
 
         //--------------------------------------------------------------------
         // scenario 1
@@ -103,7 +101,7 @@
         writerB.close();
 
         // creating the query
-        QueryParser parser = new QueryParser(Version.LUCENE_CURRENT, "fulltext", new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT));
+        QueryParser parser = new QueryParser(TEST_VERSION_CURRENT, "fulltext", new StandardAnalyzer(TEST_VERSION_CURRENT));
         Query query = parser.parse("handle:1");
 
         // building the searchables
@@ -130,7 +128,7 @@
         //--------------------------------------------------------------------
 
         // adding one document to the empty index
-        writerB = new IndexWriter(indexStoreB, new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT), false, IndexWriter.MaxFieldLength.LIMITED);
+        writerB = new IndexWriter(indexStoreB, new StandardAnalyzer(TEST_VERSION_CURRENT), false, IndexWriter.MaxFieldLength.LIMITED);
         writerB.addDocument(lDoc);
         writerB.optimize();
         writerB.close();
@@ -176,7 +174,7 @@
         readerB.close();
 
         // optimizing the index with the writer
-        writerB = new IndexWriter(indexStoreB, new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT), false, IndexWriter.MaxFieldLength.LIMITED);
+        writerB = new IndexWriter(indexStoreB, new StandardAnalyzer(TEST_VERSION_CURRENT), false, IndexWriter.MaxFieldLength.LIMITED);
         writerB.optimize();
         writerB.close();
 
Index: src/test/org/apache/lucene/search/TestQueryWrapperFilter.java
===================================================================
--- src/test/org/apache/lucene/search/TestQueryWrapperFilter.java	(revision 916567)
+++ src/test/org/apache/lucene/search/TestQueryWrapperFilter.java	(working copy)
@@ -33,7 +33,7 @@
 
   public void testBasic() throws Exception {
     Directory dir = new RAMDirectory();
-    IndexWriter writer = new IndexWriter(dir, new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT), true,
+    IndexWriter writer = new IndexWriter(dir, new StandardAnalyzer(TEST_VERSION_CURRENT), true,
         IndexWriter.MaxFieldLength.LIMITED);
     Document doc = new Document();
     doc.add(new Field("field", "value", Store.NO, Index.ANALYZED));
Index: src/test/org/apache/lucene/search/TestTermVectors.java
===================================================================
--- src/test/org/apache/lucene/search/TestTermVectors.java	(revision 916567)
+++ src/test/org/apache/lucene/search/TestTermVectors.java	(working copy)
@@ -41,7 +41,7 @@
   @Override
   public void setUp() throws Exception {                  
     super.setUp();
-    IndexWriter writer = new IndexWriter(directory, new SimpleAnalyzer(), true,
+    IndexWriter writer = new IndexWriter(directory, new SimpleAnalyzer(TEST_VERSION_CURRENT), true,
                                          IndexWriter.MaxFieldLength.LIMITED);
     //writer.setUseCompoundFile(true);
     //writer.infoStream = System.out;
@@ -93,7 +93,7 @@
   
   public void testTermVectorsFieldOrder() throws IOException {
     Directory dir = new MockRAMDirectory();
-    IndexWriter writer = new IndexWriter(dir, new SimpleAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
+    IndexWriter writer = new IndexWriter(dir, new SimpleAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
     Document doc = new Document();
     doc.add(new Field("c", "some content here", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));
     doc.add(new Field("a", "some content here", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));
@@ -231,7 +231,7 @@
     Directory dir = new MockRAMDirectory();
     
     try {
-      IndexWriter writer = new IndexWriter(dir, new SimpleAnalyzer(), true, 
+      IndexWriter writer = new IndexWriter(dir, new SimpleAnalyzer(TEST_VERSION_CURRENT), true, 
                                            IndexWriter.MaxFieldLength.LIMITED);
       assertTrue(writer != null);
       writer.addDocument(testDoc1);
@@ -347,7 +347,7 @@
 
   // Test only a few docs having vectors
   public void testRareVectors() throws IOException {
-    IndexWriter writer = new IndexWriter(directory, new SimpleAnalyzer(), true, 
+    IndexWriter writer = new IndexWriter(directory, new SimpleAnalyzer(TEST_VERSION_CURRENT), true, 
                                          IndexWriter.MaxFieldLength.LIMITED);
     for(int i=0;i<100;i++) {
       Document doc = new Document();
@@ -379,7 +379,7 @@
   // In a single doc, for the same field, mix the term
   // vectors up
   public void testMixedVectrosVectors() throws IOException {
-    IndexWriter writer = new IndexWriter(directory, new SimpleAnalyzer(), true, 
+    IndexWriter writer = new IndexWriter(directory, new SimpleAnalyzer(TEST_VERSION_CURRENT), true, 
                                          IndexWriter.MaxFieldLength.LIMITED);
     Document doc = new Document();
     doc.add(new Field("field", "one",
Index: src/test/org/apache/lucene/search/TestSimpleExplanations.java
===================================================================
--- src/test/org/apache/lucene/search/TestSimpleExplanations.java	(revision 916567)
+++ src/test/org/apache/lucene/search/TestSimpleExplanations.java	(working copy)
@@ -28,7 +28,6 @@
 import org.apache.lucene.search.spans.SpanTermQuery;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.MockRAMDirectory;
-import org.apache.lucene.util.Version;
 
 
 /**
@@ -317,8 +316,8 @@
     Document lDoc3 = new Document();
     lDoc3.add(new Field("handle", "1 2", Field.Store.YES, Field.Index.ANALYZED));
 
-    IndexWriter writerA = new IndexWriter(indexStoreA, new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
-    IndexWriter writerB = new IndexWriter(indexStoreB, new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
+    IndexWriter writerA = new IndexWriter(indexStoreA, new StandardAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
+    IndexWriter writerB = new IndexWriter(indexStoreB, new StandardAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
 
     writerA.addDocument(lDoc);
     writerA.addDocument(lDoc2);
@@ -328,7 +327,7 @@
     writerB.addDocument(lDoc3);
     writerB.close();
 
-    QueryParser parser = new QueryParser(Version.LUCENE_CURRENT, "fulltext", new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT));
+    QueryParser parser = new QueryParser(TEST_VERSION_CURRENT, "fulltext", new StandardAnalyzer(TEST_VERSION_CURRENT));
     Query query = parser.parse("handle:1");
 
     Searcher[] searchers = new Searcher[2];
Index: src/test/org/apache/lucene/search/TestQueryTermVector.java
===================================================================
--- src/test/org/apache/lucene/search/TestQueryTermVector.java	(revision 916567)
+++ src/test/org/apache/lucene/search/TestQueryTermVector.java	(working copy)
@@ -42,7 +42,7 @@
     result = new QueryTermVector(null);
     assertTrue(result.getTerms().length == 0);
     
-    result = new QueryTermVector("foo bar foo again foo bar go go go", new WhitespaceAnalyzer());
+    result = new QueryTermVector("foo bar foo again foo bar go go go", new WhitespaceAnalyzer(TEST_VERSION_CURRENT));
     assertTrue(result != null);
     terms = result.getTerms();
     assertTrue(terms.length == 4);
Index: src/test/org/apache/lucene/search/TestMultiValuedNumericRangeQuery.java
===================================================================
--- src/test/org/apache/lucene/search/TestMultiValuedNumericRangeQuery.java	(revision 916567)
+++ src/test/org/apache/lucene/search/TestMultiValuedNumericRangeQuery.java	(working copy)
@@ -43,7 +43,7 @@
     final Random rnd = newRandom();
 
     RAMDirectory directory = new RAMDirectory();
-    IndexWriter writer = new IndexWriter(directory, new WhitespaceAnalyzer(), true, MaxFieldLength.UNLIMITED);
+    IndexWriter writer = new IndexWriter(directory, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, MaxFieldLength.UNLIMITED);
     
     DecimalFormat format = new DecimalFormat("00000000000", new DecimalFormatSymbols(Locale.US));
     
Index: src/test/org/apache/lucene/search/TestTermScorer.java
===================================================================
--- src/test/org/apache/lucene/search/TestTermScorer.java	(revision 916567)
+++ src/test/org/apache/lucene/search/TestTermScorer.java	(working copy)
@@ -51,7 +51,7 @@
         directory = new RAMDirectory();
 
 
-        IndexWriter writer = new IndexWriter(directory, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
+        IndexWriter writer = new IndexWriter(directory, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
         for (int i = 0; i < values.length; i++)
         {
             Document doc = new Document();
Index: src/test/org/apache/lucene/search/TestPhrasePrefixQuery.java
===================================================================
--- src/test/org/apache/lucene/search/TestPhrasePrefixQuery.java	(revision 916567)
+++ src/test/org/apache/lucene/search/TestPhrasePrefixQuery.java	(working copy)
@@ -48,7 +48,7 @@
         throws IOException
     {
         RAMDirectory indexStore = new RAMDirectory();
-        IndexWriter writer = new IndexWriter(indexStore, new SimpleAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
+        IndexWriter writer = new IndexWriter(indexStore, new SimpleAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
         Document doc1 = new Document();
         Document doc2 = new Document();
         Document doc3 = new Document();
Index: src/test/org/apache/lucene/search/TestSetNorm.java
===================================================================
--- src/test/org/apache/lucene/search/TestSetNorm.java	(revision 916567)
+++ src/test/org/apache/lucene/search/TestSetNorm.java	(working copy)
@@ -39,7 +39,7 @@
 
   public void testSetNorm() throws Exception {
     RAMDirectory store = new RAMDirectory();
-    IndexWriter writer = new IndexWriter(store, new SimpleAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
+    IndexWriter writer = new IndexWriter(store, new SimpleAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
 
     // add the same document four times
     Fieldable f1 = new Field("field", "word", Field.Store.YES, Field.Index.ANALYZED);
Index: src/test/org/apache/lucene/search/TestWildcard.java
===================================================================
--- src/test/org/apache/lucene/search/TestWildcard.java	(revision 916567)
+++ src/test/org/apache/lucene/search/TestWildcard.java	(working copy)
@@ -29,7 +29,6 @@
 import org.apache.lucene.index.Term;
 import org.apache.lucene.queryParser.QueryParser;
 import org.apache.lucene.store.RAMDirectory;
-import org.apache.lucene.util.Version;
 
 import java.io.IOException;
 
@@ -195,7 +194,7 @@
   private RAMDirectory getIndexStore(String field, String[] contents)
       throws IOException {
     RAMDirectory indexStore = new RAMDirectory();
-    IndexWriter writer = new IndexWriter(indexStore, new SimpleAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
+    IndexWriter writer = new IndexWriter(indexStore, new SimpleAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
     for (int i = 0; i < contents.length; ++i) {
       Document doc = new Document();
       doc.add(new Field(field, contents[i], Field.Store.YES, Field.Index.ANALYZED));
@@ -223,7 +222,7 @@
   public void testParsingAndSearching() throws Exception {
     String field = "content";
     boolean dbg = false;
-    QueryParser qp = new QueryParser(Version.LUCENE_CURRENT, field, new WhitespaceAnalyzer());
+    QueryParser qp = new QueryParser(TEST_VERSION_CURRENT, field, new WhitespaceAnalyzer(TEST_VERSION_CURRENT));
     qp.setAllowLeadingWildcard(true);
     String docs[] = {
         "\\ abcdefg1",
@@ -253,7 +252,7 @@
 
     // prepare the index
     RAMDirectory dir = new RAMDirectory();
-    IndexWriter iw = new IndexWriter(dir, new WhitespaceAnalyzer(), IndexWriter.MaxFieldLength.LIMITED);
+    IndexWriter iw = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), IndexWriter.MaxFieldLength.LIMITED);
     for (int i = 0; i < docs.length; i++) {
       Document doc = new Document();
       doc.add(new Field(field,docs[i],Store.NO,Index.ANALYZED));
Index: src/test/org/apache/lucene/search/BaseTestRangeFilter.java
===================================================================
--- src/test/org/apache/lucene/search/BaseTestRangeFilter.java	(revision 916567)
+++ src/test/org/apache/lucene/search/BaseTestRangeFilter.java	(working copy)
@@ -20,7 +20,6 @@
 import java.util.Random;
 
 import org.apache.lucene.util.LuceneTestCase;
-
 import org.apache.lucene.analysis.SimpleAnalyzer;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
@@ -97,7 +96,7 @@
         try {
             
             /* build an index */
-            IndexWriter writer = new IndexWriter(index.index, new SimpleAnalyzer(), T, 
+            IndexWriter writer = new IndexWriter(index.index, new SimpleAnalyzer(TEST_VERSION_CURRENT), T, 
                                                  IndexWriter.MaxFieldLength.LIMITED);
 
           for (int d = minId; d <= maxId; d++) {
Index: src/test/org/apache/lucene/search/TestBoolean2.java
===================================================================
--- src/test/org/apache/lucene/search/TestBoolean2.java	(revision 916567)
+++ src/test/org/apache/lucene/search/TestBoolean2.java	(working copy)
@@ -32,7 +32,6 @@
 import org.apache.lucene.store.MockRAMDirectory;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.util.LuceneTestCase;
-import org.apache.lucene.util.Version;
 
 /** Test BooleanQuery2 against BooleanQuery by overriding the standard query parser.
  * This also tests the scoring order of BooleanQuery.
@@ -51,7 +50,7 @@
   public void setUp() throws Exception {
     super.setUp();
     RAMDirectory directory = new RAMDirectory();
-    IndexWriter writer= new IndexWriter(directory, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
+    IndexWriter writer= new IndexWriter(directory, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
     for (int i = 0; i < docFields.length; i++) {
       Document doc = new Document();
       doc.add(new Field(field, docFields[i], Field.Store.NO, Field.Index.ANALYZED));
@@ -68,14 +67,14 @@
     int docCount = 0;
     do {
       final Directory copy = new RAMDirectory(dir2);
-      IndexWriter w = new IndexWriter(dir2, new WhitespaceAnalyzer(), IndexWriter.MaxFieldLength.UNLIMITED);
+      IndexWriter w = new IndexWriter(dir2, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), IndexWriter.MaxFieldLength.UNLIMITED);
       w.addIndexesNoOptimize(new Directory[] {copy});
       docCount = w.maxDoc();
       w.close();
       mulFactor *= 2;
     } while(docCount < 3000);
 
-    IndexWriter w = new IndexWriter(dir2, new WhitespaceAnalyzer(), IndexWriter.MaxFieldLength.UNLIMITED);
+    IndexWriter w = new IndexWriter(dir2, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), IndexWriter.MaxFieldLength.UNLIMITED);
     Document doc = new Document();
     doc.add(new Field("field2", "xxx", Field.Store.NO, Field.Index.ANALYZED));
     for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {
@@ -107,7 +106,7 @@
   };
 
   public Query makeQuery(String queryText) throws ParseException {
-    Query q = (new QueryParser(Version.LUCENE_CURRENT, field, new WhitespaceAnalyzer())).parse(queryText);
+    Query q = (new QueryParser(TEST_VERSION_CURRENT, field, new WhitespaceAnalyzer(TEST_VERSION_CURRENT))).parse(queryText);
     return q;
   }
 
Index: src/test/org/apache/lucene/search/TestExplanations.java
===================================================================
--- src/test/org/apache/lucene/search/TestExplanations.java	(revision 916567)
+++ src/test/org/apache/lucene/search/TestExplanations.java	(working copy)
@@ -32,7 +32,6 @@
 import org.apache.lucene.search.spans.SpanTermQuery;
 import org.apache.lucene.store.RAMDirectory;
 import org.apache.lucene.util.LuceneTestCase;
-import org.apache.lucene.util.Version;
 
 /**
  * Tests primitive queries (ie: that rewrite to themselves) to
@@ -52,7 +51,7 @@
   public static final String KEY = "KEY";
   public static final String FIELD = "field";
   public static final QueryParser qp =
-    new QueryParser(Version.LUCENE_CURRENT, FIELD, new WhitespaceAnalyzer());
+    new QueryParser(TEST_VERSION_CURRENT, FIELD, new WhitespaceAnalyzer(TEST_VERSION_CURRENT));
 
   @Override
   public void tearDown() throws Exception {
@@ -64,7 +63,7 @@
   public void setUp() throws Exception {
     super.setUp();
     RAMDirectory directory = new RAMDirectory();
-    IndexWriter writer= new IndexWriter(directory, new WhitespaceAnalyzer(), true,
+    IndexWriter writer= new IndexWriter(directory, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true,
                                         IndexWriter.MaxFieldLength.LIMITED);
     for (int i = 0; i < docFields.length; i++) {
       Document doc = new Document();
Index: src/test/org/apache/lucene/search/function/FunctionTestSetup.java
===================================================================
--- src/test/org/apache/lucene/search/function/FunctionTestSetup.java	(revision 916567)
+++ src/test/org/apache/lucene/search/function/FunctionTestSetup.java	(working copy)
@@ -96,7 +96,7 @@
     // prepare a small index with just a few documents.  
     super.setUp();
     dir = new RAMDirectory();
-    anlzr = new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT);
+    anlzr = new StandardAnalyzer(TEST_VERSION_CURRENT);
     IndexWriter iw = new IndexWriter(dir, anlzr,
             IndexWriter.MaxFieldLength.LIMITED);
     // add docs not exactly in natural ID order, to verify we do check the order of docs by scores
Index: src/test/org/apache/lucene/search/function/TestCustomScoreQuery.java
===================================================================
--- src/test/org/apache/lucene/search/function/TestCustomScoreQuery.java	(revision 916567)
+++ src/test/org/apache/lucene/search/function/TestCustomScoreQuery.java	(working copy)
@@ -20,7 +20,6 @@
 import org.apache.lucene.queryParser.QueryParser;
 import org.apache.lucene.queryParser.ParseException;
 import org.apache.lucene.search.*;
-import org.apache.lucene.util.Version;
 import org.junit.Test;
 import static org.junit.Assert.assertEquals;
 import static org.junit.Assert.assertTrue;
@@ -30,11 +29,11 @@
 import java.util.Map;
 
 import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.Term;
 
 /**
  * Test CustomScoreQuery search.
  */
-@SuppressWarnings({"MagicNumber"})
 public class TestCustomScoreQuery extends FunctionTestSetup {
 
   /* @override constructor */
@@ -97,23 +96,26 @@
     public String name() {
       return "customAdd";
     }
-
-    /*(non-Javadoc) @see org.apache.lucene.search.function.CustomScoreQuery#customScore(int, float, float) */
+    
     @Override
-    public float customScore(int doc, float subQueryScore, float valSrcScore) {
-      return subQueryScore + valSrcScore;
-    }
+    protected CustomScoreProvider getCustomScoreProvider(IndexReader reader) {
+      return new CustomScoreProvider(reader) {
+        @Override
+        public float customScore(int doc, float subQueryScore, float valSrcScore) {
+          return subQueryScore + valSrcScore;
+        }
 
-    /* (non-Javadoc)@see org.apache.lucene.search.function.CustomScoreQuery#customExplain(int, org.apache.lucene.search.Explanation, org.apache.lucene.search.Explanation)*/
-    @Override
-    public Explanation customExplain(int doc, Explanation subQueryExpl, Explanation valSrcExpl) {
-      float valSrcScore = valSrcExpl == null ? 0 : valSrcExpl.getValue();
-      Explanation exp = new Explanation(valSrcScore + subQueryExpl.getValue(), "custom score: sum of:");
-      exp.addDetail(subQueryExpl);
-      if (valSrcExpl != null) {
-        exp.addDetail(valSrcExpl);
-      }
-      return exp;
+        @Override
+        public Explanation customExplain(int doc, Explanation subQueryExpl, Explanation valSrcExpl) {
+          float valSrcScore = valSrcExpl == null ? 0 : valSrcExpl.getValue();
+          Explanation exp = new Explanation(valSrcScore + subQueryExpl.getValue(), "custom score: sum of:");
+          exp.addDetail(subQueryExpl);
+          if (valSrcExpl != null) {
+            exp.addDetail(valSrcExpl);
+          }
+          return exp;
+        }
+      };
     }
   }
 
@@ -131,61 +133,65 @@
       return "customMulAdd";
     }
 
-    /*(non-Javadoc) @see org.apache.lucene.search.function.CustomScoreQuery#customScore(int, float, float) */
     @Override
-    public float customScore(int doc, float subQueryScore, float valSrcScores[]) {
-      if (valSrcScores.length == 0) {
-        return subQueryScore;
-      }
-      if (valSrcScores.length == 1) {
-        return subQueryScore + valSrcScores[0];
-        // confirm that skipping beyond the last doc, on the
-        // previous reader, hits NO_MORE_DOCS
-      }
-      return (subQueryScore + valSrcScores[0]) * valSrcScores[1]; // we know there are two
-    }
+    protected CustomScoreProvider getCustomScoreProvider(IndexReader reader) {
+      return new CustomScoreProvider(reader) {
+        @Override
+        public float customScore(int doc, float subQueryScore, float valSrcScores[]) {
+          if (valSrcScores.length == 0) {
+            return subQueryScore;
+          }
+          if (valSrcScores.length == 1) {
+            return subQueryScore + valSrcScores[0];
+            // confirm that skipping beyond the last doc, on the
+            // previous reader, hits NO_MORE_DOCS
+          }
+          return (subQueryScore + valSrcScores[0]) * valSrcScores[1]; // we know there are two
+        }
 
-    /* (non-Javadoc)@see org.apache.lucene.search.function.CustomScoreQuery#customExplain(int, org.apache.lucene.search.Explanation, org.apache.lucene.search.Explanation)*/
-    @Override
-    public Explanation customExplain(int doc, Explanation subQueryExpl, Explanation valSrcExpls[]) {
-      if (valSrcExpls.length == 0) {
-        return subQueryExpl;
-      }
-      Explanation exp = new Explanation(valSrcExpls[0].getValue() + subQueryExpl.getValue(), "sum of:");
-      exp.addDetail(subQueryExpl);
-      exp.addDetail(valSrcExpls[0]);
-      if (valSrcExpls.length == 1) {
-        exp.setDescription("CustomMulAdd, sum of:");
-        return exp;
-      }
-      Explanation exp2 = new Explanation(valSrcExpls[1].getValue() * exp.getValue(), "custom score: product of:");
-      exp2.addDetail(valSrcExpls[1]);
-      exp2.addDetail(exp);
-      return exp2;
+        @Override
+        public Explanation customExplain(int doc, Explanation subQueryExpl, Explanation valSrcExpls[]) {
+          if (valSrcExpls.length == 0) {
+            return subQueryExpl;
+          }
+          Explanation exp = new Explanation(valSrcExpls[0].getValue() + subQueryExpl.getValue(), "sum of:");
+          exp.addDetail(subQueryExpl);
+          exp.addDetail(valSrcExpls[0]);
+          if (valSrcExpls.length == 1) {
+            exp.setDescription("CustomMulAdd, sum of:");
+            return exp;
+          }
+          Explanation exp2 = new Explanation(valSrcExpls[1].getValue() * exp.getValue(), "custom score: product of:");
+          exp2.addDetail(valSrcExpls[1]);
+          exp2.addDetail(exp);
+          return exp2;
+        }
+      };
     }
   }
 
   private final class CustomExternalQuery extends CustomScoreQuery {
-    private IndexReader reader;
-    private int[] values;
 
-    public float customScore(int doc, float subScore, float valSrcScore) {
-      assertTrue(doc <= reader.maxDoc());
-      return (float) values[doc];
+    @Override
+    protected CustomScoreProvider getCustomScoreProvider(IndexReader reader) throws IOException {
+      final int[] values = FieldCache.DEFAULT.getInts(reader, INT_FIELD);
+      return new CustomScoreProvider(reader) {
+        @Override
+        public float customScore(int doc, float subScore, float valSrcScore) throws IOException {
+          assertTrue(doc <= reader.maxDoc());
+          return (float) values[doc];
+        }
+      };
     }
 
-    public void setNextReader(IndexReader r) throws IOException {
-      reader = r;
-      values = FieldCache.DEFAULT.getInts(r, INT_FIELD);
-    }
-
     public CustomExternalQuery(Query q) {
       super(q);
     }
   }
 
+  @Test
   public void testCustomExternalQuery() throws Exception {
-    QueryParser qp = new QueryParser(Version.LUCENE_CURRENT, TEXT_FIELD,anlzr); 
+    QueryParser qp = new QueryParser(TEST_VERSION_CURRENT, TEXT_FIELD,anlzr); 
     String qtxt = "first aid text"; // from the doc texts in FunctionQuerySetup.
     Query q1 = qp.parse(qtxt); 
     
@@ -203,12 +209,35 @@
     s.close();
   }
   
+  @Test
+  public void testRewrite() throws Exception {
+    final IndexSearcher s = new IndexSearcher(dir, true);
+
+    Query q = new TermQuery(new Term(TEXT_FIELD, "first"));
+    CustomScoreQuery original = new CustomScoreQuery(q);
+    CustomScoreQuery rewritten = (CustomScoreQuery) original.rewrite(s.getIndexReader());
+    assertTrue("rewritten query should be identical, as TermQuery does not rewrite", original == rewritten);
+    assertTrue("no hits for query", s.search(rewritten,1).totalHits > 0);
+    assertEquals(s.search(q,1).totalHits, s.search(rewritten,1).totalHits);
+
+    q = new TermRangeQuery(TEXT_FIELD, null, null, true, true); // everything
+    original = new CustomScoreQuery(q);
+    rewritten = (CustomScoreQuery) original.rewrite(s.getIndexReader());
+    assertTrue("rewritten query should not be identical, as TermRangeQuery rewrites", original != rewritten);
+    assertTrue("rewritten query should be a CustomScoreQuery", rewritten instanceof CustomScoreQuery);
+    assertTrue("no hits for query", s.search(rewritten,1).totalHits > 0);
+    assertEquals(s.search(q,1).totalHits, s.search(original,1).totalHits);
+    assertEquals(s.search(q,1).totalHits, s.search(rewritten,1).totalHits);
+    
+    s.close();
+  }
+  
   // Test that FieldScoreQuery returns docs with expected score.
   private void doTestCustomScore(String field, FieldScoreQuery.Type tp, double dboost) throws Exception, ParseException {
     float boost = (float) dboost;
     IndexSearcher s = new IndexSearcher(dir, true);
     FieldScoreQuery qValSrc = new FieldScoreQuery(field, tp); // a query that would score by the field
-    QueryParser qp = new QueryParser(Version.LUCENE_CURRENT, TEXT_FIELD, anlzr);
+    QueryParser qp = new QueryParser(TEST_VERSION_CURRENT, TEXT_FIELD, anlzr);
     String qtxt = "first aid text"; // from the doc texts in FunctionQuerySetup.
 
     // regular (boolean) query.
Index: src/test/org/apache/lucene/search/TestThreadSafe.java
===================================================================
--- src/test/org/apache/lucene/search/TestThreadSafe.java	(revision 916567)
+++ src/test/org/apache/lucene/search/TestThreadSafe.java	(working copy)
@@ -105,7 +105,7 @@
   String[] words = "now is the time for all good men to come to the aid of their country".split(" ");
 
   void buildDir(Directory dir, int nDocs, int maxFields, int maxFieldLen) throws IOException {
-    IndexWriter iw = new IndexWriter(dir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
+    IndexWriter iw = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
     iw.setMaxBufferedDocs(10);
     for (int j=0; j<nDocs; j++) {
       Document d = new Document();
Index: src/test/org/apache/lucene/search/TestFieldCache.java
===================================================================
--- src/test/org/apache/lucene/search/TestFieldCache.java	(revision 916567)
+++ src/test/org/apache/lucene/search/TestFieldCache.java	(working copy)
@@ -23,7 +23,6 @@
 import org.apache.lucene.index.IndexWriter;
 import org.apache.lucene.store.RAMDirectory;
 import org.apache.lucene.util.LuceneTestCase;
-
 import java.io.IOException;
 import java.io.ByteArrayOutputStream;
 import java.io.PrintStream;
@@ -40,7 +39,7 @@
   protected void setUp() throws Exception {
     super.setUp();
     RAMDirectory directory = new RAMDirectory();
-    IndexWriter writer= new IndexWriter(directory, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
+    IndexWriter writer= new IndexWriter(directory, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
     long theLong = Long.MAX_VALUE;
     double theDouble = Double.MAX_VALUE;
     byte theByte = Byte.MAX_VALUE;
Index: src/test/org/apache/lucene/search/TestScorerPerf.java
===================================================================
--- src/test/org/apache/lucene/search/TestScorerPerf.java	(revision 916567)
+++ src/test/org/apache/lucene/search/TestScorerPerf.java	(working copy)
@@ -2,7 +2,6 @@
 
 import org.apache.lucene.util.DocIdBitSet;
 import org.apache.lucene.util.LuceneTestCase;
-
 import java.util.Random;
 import java.util.BitSet;
 import java.io.IOException;
@@ -33,10 +32,6 @@
  * limitations under the License.
  */
 
-/**
- *
- * @version $Id$
- */
 public class TestScorerPerf extends LuceneTestCase {
   Random r;
   boolean validate = true;  // set to false when doing performance testing
@@ -49,7 +44,7 @@
       // Create a dummy index with nothing in it.
     // This could possibly fail if Lucene starts checking for docid ranges...
     RAMDirectory rd = new RAMDirectory();
-    IndexWriter iw = new IndexWriter(rd,new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
+    IndexWriter iw = new IndexWriter(rd,new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
     iw.addDocument(new Document());
     iw.close();
     s = new IndexSearcher(rd, true);
@@ -64,7 +59,7 @@
       terms[i] = new Term("f",Character.toString((char)('A'+i)));
     }
 
-    IndexWriter iw = new IndexWriter(dir,new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
+    IndexWriter iw = new IndexWriter(dir,new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
     for (int i=0; i<nDocs; i++) {
       Document d = new Document();
       for (int j=0; j<nTerms; j++) {
Index: src/test/org/apache/lucene/search/TestFuzzyQuery.java
===================================================================
--- src/test/org/apache/lucene/search/TestFuzzyQuery.java	(revision 916567)
+++ src/test/org/apache/lucene/search/TestFuzzyQuery.java	(working copy)
@@ -33,7 +33,6 @@
 import org.apache.lucene.store.MockRAMDirectory;
 import org.apache.lucene.store.RAMDirectory;
 import org.apache.lucene.util.LuceneTestCase;
-import org.apache.lucene.util.Version;
 
 /**
  * Tests {@link FuzzyQuery}.
@@ -43,7 +42,7 @@
 
   public void testFuzziness() throws Exception {
     RAMDirectory directory = new RAMDirectory();
-    IndexWriter writer = new IndexWriter(directory, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
+    IndexWriter writer = new IndexWriter(directory, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
     addDoc("aaaaa", writer);
     addDoc("aaaab", writer);
     addDoc("aaabb", writer);
@@ -90,22 +89,16 @@
       assertEquals(order.get(i), term);
     }
 
-    // test BooleanQuery.maxClauseCount
-    int savedClauseCount = BooleanQuery.getMaxClauseCount();
-    try {
-      BooleanQuery.setMaxClauseCount(2);
-      // This query would normally return 3 documents, because 3 terms match (see above):
-      query = new FuzzyQuery(new Term("field", "bbbbb"), FuzzyQuery.defaultMinSimilarity, 0);   
-      hits = searcher.search(query, null, 1000).scoreDocs;
-      assertEquals("only 2 documents should match", 2, hits.length);
-      order = Arrays.asList("bbbbb","abbbb");
-      for (int i = 0; i < hits.length; i++) {
-        final String term = searcher.doc(hits[i].doc).get("field");
-        //System.out.println(hits[i].score);
-        assertEquals(order.get(i), term);
-      }
-    } finally {
-      BooleanQuery.setMaxClauseCount(savedClauseCount);
+    // test pq size by supplying maxExpansions=2
+    // This query would normally return 3 documents, because 3 terms match (see above):
+    query = new FuzzyQuery(new Term("field", "bbbbb"), FuzzyQuery.defaultMinSimilarity, 0, 2); 
+    hits = searcher.search(query, null, 1000).scoreDocs;
+    assertEquals("only 2 documents should match", 2, hits.length);
+    order = Arrays.asList("bbbbb","abbbb");
+    for (int i = 0; i < hits.length; i++) {
+      final String term = searcher.doc(hits[i].doc).get("field");
+      //System.out.println(hits[i].score);
+      assertEquals(order.get(i), term);
     }
 
     // not similar enough:
@@ -200,7 +193,7 @@
 
   public void testFuzzinessLong() throws Exception {
     RAMDirectory directory = new RAMDirectory();
-    IndexWriter writer = new IndexWriter(directory, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
+    IndexWriter writer = new IndexWriter(directory, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
     addDoc("aaaaaaa", writer);
     addDoc("segment", writer);
     writer.optimize();
@@ -288,7 +281,7 @@
   
   public void testTokenLengthOpt() throws IOException {
     RAMDirectory directory = new RAMDirectory();
-    IndexWriter writer = new IndexWriter(directory, new WhitespaceAnalyzer(),
+    IndexWriter writer = new IndexWriter(directory, new WhitespaceAnalyzer(TEST_VERSION_CURRENT),
         true, IndexWriter.MaxFieldLength.LIMITED);
     addDoc("12345678911", writer);
     addDoc("segment", writer);
@@ -320,7 +313,7 @@
   
   public void testGiga() throws Exception {
 
-    StandardAnalyzer analyzer = new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT);
+    StandardAnalyzer analyzer = new StandardAnalyzer(TEST_VERSION_CURRENT);
 
     Directory index = new MockRAMDirectory();
     IndexWriter w = new IndexWriter(index, analyzer, true, IndexWriter.MaxFieldLength.UNLIMITED);
@@ -345,7 +338,7 @@
     IndexReader r = w.getReader();
     w.close();
 
-    Query q = new QueryParser(Version.LUCENE_CURRENT, "field", analyzer).parse( "giga~0.9" );
+    Query q = new QueryParser(TEST_VERSION_CURRENT, "field", analyzer).parse( "giga~0.9" );
 
     // 3. search
     IndexSearcher searcher = new IndexSearcher(r);
Index: src/test/org/apache/lucene/search/TestMultiThreadTermVectors.java
===================================================================
--- src/test/org/apache/lucene/search/TestMultiThreadTermVectors.java	(revision 916567)
+++ src/test/org/apache/lucene/search/TestMultiThreadTermVectors.java	(working copy)
@@ -41,7 +41,7 @@
   public void setUp() throws Exception {
     super.setUp();
     IndexWriter writer
-            = new IndexWriter(directory, new SimpleAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
+            = new IndexWriter(directory, new SimpleAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
     //writer.setUseCompoundFile(false);
     //writer.infoStream = System.out;
     for (int i = 0; i < numDocs; i++) {
Index: src/test/org/apache/lucene/search/TestDateFilter.java
===================================================================
--- src/test/org/apache/lucene/search/TestDateFilter.java	(revision 916567)
+++ src/test/org/apache/lucene/search/TestDateFilter.java	(working copy)
@@ -50,7 +50,7 @@
     {
 	// create an index
         RAMDirectory indexStore = new RAMDirectory();
-        IndexWriter writer = new IndexWriter(indexStore, new SimpleAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
+        IndexWriter writer = new IndexWriter(indexStore, new SimpleAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
 
  	long now = System.currentTimeMillis();
 
@@ -111,7 +111,7 @@
     {
 	// create an index
         RAMDirectory indexStore = new RAMDirectory();
-        IndexWriter writer = new IndexWriter(indexStore, new SimpleAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
+        IndexWriter writer = new IndexWriter(indexStore, new SimpleAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
 
  	long now = System.currentTimeMillis();
 
Index: src/test/org/apache/lucene/search/TestMatchAllDocsQuery.java
===================================================================
--- src/test/org/apache/lucene/search/TestMatchAllDocsQuery.java	(revision 916567)
+++ src/test/org/apache/lucene/search/TestMatchAllDocsQuery.java	(working copy)
@@ -29,14 +29,13 @@
 import org.apache.lucene.store.RAMDirectory;
 
 import org.apache.lucene.util.LuceneTestCase;
-import org.apache.lucene.util.Version;
 
 /**
  * Tests MatchAllDocsQuery.
  *
  */
 public class TestMatchAllDocsQuery extends LuceneTestCase {
-  private Analyzer analyzer = new StandardAnalyzer(Version.LUCENE_CURRENT);
+  private Analyzer analyzer = new StandardAnalyzer(TEST_VERSION_CURRENT);
   
   public void testQuery() throws Exception {
 
@@ -100,7 +99,7 @@
     assertEquals(2, hits.length);
     
     // test parsable toString()
-    QueryParser qp = new QueryParser(Version.LUCENE_CURRENT, "key", analyzer);
+    QueryParser qp = new QueryParser(TEST_VERSION_CURRENT, "key", analyzer);
     hits = is.search(qp.parse(new MatchAllDocsQuery().toString()), null, 1000).scoreDocs;
     assertEquals(2, hits.length);
 
Index: src/test/org/apache/lucene/search/TestFilteredSearch.java
===================================================================
--- src/test/org/apache/lucene/search/TestFilteredSearch.java	(revision 916567)
+++ src/test/org/apache/lucene/search/TestFilteredSearch.java	(working copy)
@@ -20,7 +20,6 @@
 import java.io.IOException;
 
 import org.apache.lucene.util.LuceneTestCase;
-
 import org.apache.lucene.analysis.WhitespaceAnalyzer;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
@@ -50,13 +49,13 @@
     RAMDirectory directory = new RAMDirectory();
     int[] filterBits = {1, 36};
     SimpleDocIdSetFilter filter = new SimpleDocIdSetFilter(filterBits);
-    IndexWriter writer = new IndexWriter(directory, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
+    IndexWriter writer = new IndexWriter(directory, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
     searchFiltered(writer, directory, filter, enforceSingleSegment);
     // run the test on more than one segment
     enforceSingleSegment = false;
     // reset - it is stateful
     filter.reset();
-    writer = new IndexWriter(directory, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
+    writer = new IndexWriter(directory, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
     // we index 60 docs - this will create 6 segments
     writer.setMaxBufferedDocs(10);
     searchFiltered(writer, directory, filter, enforceSingleSegment);
Index: src/test/org/apache/lucene/search/TestDisjunctionMaxQuery.java
===================================================================
--- src/test/org/apache/lucene/search/TestDisjunctionMaxQuery.java	(revision 916567)
+++ src/test/org/apache/lucene/search/TestDisjunctionMaxQuery.java	(working copy)
@@ -79,7 +79,7 @@
 
         index = new RAMDirectory();
         IndexWriter writer = new IndexWriter(index,
-                                             new WhitespaceAnalyzer(),
+                                             new WhitespaceAnalyzer(TEST_VERSION_CURRENT),
                                              true, IndexWriter.MaxFieldLength.LIMITED);
         writer.setSimilarity(sim);
 
Index: src/test/org/apache/lucene/search/TestSimilarity.java
===================================================================
--- src/test/org/apache/lucene/search/TestSimilarity.java	(revision 916567)
+++ src/test/org/apache/lucene/search/TestSimilarity.java	(working copy)
@@ -18,7 +18,6 @@
  */
 
 import org.apache.lucene.util.LuceneTestCase;
-
 import java.io.IOException;
 import java.util.Collection;
 
@@ -64,7 +63,7 @@
 
   public void testSimilarity() throws Exception {
     RAMDirectory store = new RAMDirectory();
-    IndexWriter writer = new IndexWriter(store, new SimpleAnalyzer(), true, 
+    IndexWriter writer = new IndexWriter(store, new SimpleAnalyzer(TEST_VERSION_CURRENT), true, 
                                          IndexWriter.MaxFieldLength.LIMITED);
     writer.setSimilarity(new SimpleSimilarity());
     
Index: src/test/org/apache/lucene/search/TestFieldCacheRangeFilter.java
===================================================================
--- src/test/org/apache/lucene/search/TestFieldCacheRangeFilter.java	(revision 916567)
+++ src/test/org/apache/lucene/search/TestFieldCacheRangeFilter.java	(working copy)
@@ -531,7 +531,7 @@
   // test using a sparse index (with deleted docs). The DocIdSet should be not cacheable, as it uses TermDocs if the range contains 0
   public void testSparseIndex() throws IOException {
     RAMDirectory dir = new RAMDirectory();
-    IndexWriter writer = new IndexWriter(dir, new SimpleAnalyzer(), T, IndexWriter.MaxFieldLength.LIMITED);
+    IndexWriter writer = new IndexWriter(dir, new SimpleAnalyzer(TEST_VERSION_CURRENT), T, IndexWriter.MaxFieldLength.LIMITED);
 
     for (int d = -20; d <= 20; d++) {
       Document doc = new Document();
Index: src/test/org/apache/lucene/search/TestSpanQueryFilter.java
===================================================================
--- src/test/org/apache/lucene/search/TestSpanQueryFilter.java	(revision 916567)
+++ src/test/org/apache/lucene/search/TestSpanQueryFilter.java	(working copy)
@@ -39,7 +39,7 @@
 
   public void testFilterWorks() throws Exception {
     Directory dir = new RAMDirectory();
-    IndexWriter writer = new IndexWriter(dir, new SimpleAnalyzer(), true, 
+    IndexWriter writer = new IndexWriter(dir, new SimpleAnalyzer(TEST_VERSION_CURRENT), true, 
                                          IndexWriter.MaxFieldLength.LIMITED);
     for (int i = 0; i < 500; i++) {
       Document document = new Document();
Index: src/test/org/apache/lucene/search/TestCustomSearcherSort.java
===================================================================
--- src/test/org/apache/lucene/search/TestCustomSearcherSort.java	(revision 916567)
+++ src/test/org/apache/lucene/search/TestCustomSearcherSort.java	(working copy)
@@ -70,7 +70,7 @@
   private Directory getIndex()
   throws IOException {
           RAMDirectory indexStore = new RAMDirectory ();
-          IndexWriter writer = new IndexWriter (indexStore, new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
+          IndexWriter writer = new IndexWriter (indexStore, new StandardAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
           RandomGen random = new RandomGen(newRandom());
           for (int i=0; i<INDEX_SIZE; ++i) { // don't decrease; if to low the problem doesn't show up
           Document doc = new Document();
Index: src/test/org/apache/lucene/search/TestNumericRangeQuery32.java
===================================================================
--- src/test/org/apache/lucene/search/TestNumericRangeQuery32.java	(revision 916567)
+++ src/test/org/apache/lucene/search/TestNumericRangeQuery32.java	(working copy)
@@ -28,10 +28,15 @@
 import org.apache.lucene.index.IndexWriter.MaxFieldLength;
 import org.apache.lucene.index.TermsEnum;
 import org.apache.lucene.store.RAMDirectory;
-import org.apache.lucene.util.LuceneTestCase;
+import org.apache.lucene.util.LuceneTestCaseJ4;
 import org.apache.lucene.util.NumericUtils;
 
-public class TestNumericRangeQuery32 extends LuceneTestCase {
+import org.junit.Test;
+import org.junit.AfterClass;
+import org.junit.BeforeClass;
+import static org.junit.Assert.*;
+
+public class TestNumericRangeQuery32 extends LuceneTestCaseJ4 {
   // distance of entries
   private static final int distance = 6666;
   // shift the starting of the values to the left, to also have negative values:
@@ -39,55 +44,66 @@
   // number of docs to generate for testing
   private static final int noDocs = 10000;
   
-  private static final RAMDirectory directory;
-  private static final IndexSearcher searcher;
-  static {
-    try {    
-      // set the theoretical maximum term count for 8bit (see docs for the number)
-      BooleanQuery.setMaxClauseCount(3*255*2 + 255);
-      
-      directory = new RAMDirectory();
-      IndexWriter writer = new IndexWriter(directory, new WhitespaceAnalyzer(),
-      true, MaxFieldLength.UNLIMITED);
-      
-      NumericField
-        field8 = new NumericField("field8", 8, Field.Store.YES, true),
-        field4 = new NumericField("field4", 4, Field.Store.YES, true),
-        field2 = new NumericField("field2", 2, Field.Store.YES, true),
-        fieldNoTrie = new NumericField("field"+Integer.MAX_VALUE, Integer.MAX_VALUE, Field.Store.YES, true),
-        ascfield8 = new NumericField("ascfield8", 8, Field.Store.NO, true),
-        ascfield4 = new NumericField("ascfield4", 4, Field.Store.NO, true),
-        ascfield2 = new NumericField("ascfield2", 2, Field.Store.NO, true);
-      
-      Document doc = new Document();
-      // add fields, that have a distance to test general functionality
-      doc.add(field8); doc.add(field4); doc.add(field2); doc.add(fieldNoTrie);
-      // add ascending fields with a distance of 1, beginning at -noDocs/2 to test the correct splitting of range and inclusive/exclusive
-      doc.add(ascfield8); doc.add(ascfield4); doc.add(ascfield2);
-      
-      // Add a series of noDocs docs with increasing int values
-      for (int l=0; l<noDocs; l++) {
-        int val=distance*l+startOffset;
-        field8.setIntValue(val);
-        field4.setIntValue(val);
-        field2.setIntValue(val);
-        fieldNoTrie.setIntValue(val);
+  private static RAMDirectory directory = null;
+  private static IndexSearcher searcher = null;
+  
+  @BeforeClass
+  public static void beforeClass() throws Exception {
+    directory = new RAMDirectory();
+    IndexWriter writer = new IndexWriter(directory, new WhitespaceAnalyzer(TEST_VERSION_CURRENT),
+    true, MaxFieldLength.UNLIMITED);
+    
+    NumericField
+      field8 = new NumericField("field8", 8, Field.Store.YES, true),
+      field4 = new NumericField("field4", 4, Field.Store.YES, true),
+      field2 = new NumericField("field2", 2, Field.Store.YES, true),
+      fieldNoTrie = new NumericField("field"+Integer.MAX_VALUE, Integer.MAX_VALUE, Field.Store.YES, true),
+      ascfield8 = new NumericField("ascfield8", 8, Field.Store.NO, true),
+      ascfield4 = new NumericField("ascfield4", 4, Field.Store.NO, true),
+      ascfield2 = new NumericField("ascfield2", 2, Field.Store.NO, true);
+    
+    Document doc = new Document();
+    // add fields, that have a distance to test general functionality
+    doc.add(field8); doc.add(field4); doc.add(field2); doc.add(fieldNoTrie);
+    // add ascending fields with a distance of 1, beginning at -noDocs/2 to test the correct splitting of range and inclusive/exclusive
+    doc.add(ascfield8); doc.add(ascfield4); doc.add(ascfield2);
+    
+    // Add a series of noDocs docs with increasing int values
+    for (int l=0; l<noDocs; l++) {
+      int val=distance*l+startOffset;
+      field8.setIntValue(val);
+      field4.setIntValue(val);
+      field2.setIntValue(val);
+      fieldNoTrie.setIntValue(val);
 
-        val=l-(noDocs/2);
-        ascfield8.setIntValue(val);
-        ascfield4.setIntValue(val);
-        ascfield2.setIntValue(val);
-        writer.addDocument(doc);
-      }
-    
-      writer.optimize();
-      writer.close();
-      searcher=new IndexSearcher(directory, true);
-    } catch (Exception e) {
-      throw new Error(e);
+      val=l-(noDocs/2);
+      ascfield8.setIntValue(val);
+      ascfield4.setIntValue(val);
+      ascfield2.setIntValue(val);
+      writer.addDocument(doc);
     }
+  
+    writer.optimize();
+    writer.close();
+    searcher=new IndexSearcher(directory, true);
   }
   
+  @AfterClass
+  public static void afterClass() throws Exception {
+    searcher.close();
+    searcher = null;
+    directory.close();
+    directory = null;
+  }
+  
+  @Override
+  public void setUp() throws Exception {
+    super.setUp();
+    // set the theoretical maximum term count for 8bit (see docs for the number)
+    // super.tearDown will restore the default
+    BooleanQuery.setMaxClauseCount(3*255*2 + 255);
+  }
+  
   /** test for both constant score and boolean query, the other tests only use the constant score mode */
   private void testRange(int precisionStep) throws Exception {
     String field="field"+precisionStep;
@@ -138,18 +154,22 @@
     }
   }
 
+  @Test
   public void testRange_8bit() throws Exception {
     testRange(8);
   }
   
+  @Test
   public void testRange_4bit() throws Exception {
     testRange(4);
   }
   
+  @Test
   public void testRange_2bit() throws Exception {
     testRange(2);
   }
   
+  @Test
   public void testInverseRange() throws Exception {
     NumericRangeFilter<Integer> f = NumericRangeFilter.newIntRange("field8", 8, 1000, -1000, true, true);
     assertSame("A inverse range should return the EMPTY_DOCIDSET instance", DocIdSet.EMPTY_DOCIDSET, f.getDocIdSet(searcher.getIndexReader()));
@@ -161,6 +181,7 @@
       DocIdSet.EMPTY_DOCIDSET, f.getDocIdSet(searcher.getIndexReader()));
   }
   
+  @Test
   public void testOneMatchQuery() throws Exception {
     NumericRangeQuery<Integer> q = NumericRangeQuery.newIntRange("ascfield8", 8, 1000, 1000, true, true);
     assertSame(MultiTermQuery.CONSTANT_SCORE_BOOLEAN_QUERY_REWRITE, q.getRewriteMethod());
@@ -186,14 +207,17 @@
     assertEquals("Last doc", (count-1)*distance+startOffset, Integer.parseInt(doc.get(field)) );
   }
   
+  @Test
   public void testLeftOpenRange_8bit() throws Exception {
     testLeftOpenRange(8);
   }
   
+  @Test
   public void testLeftOpenRange_4bit() throws Exception {
     testLeftOpenRange(4);
   }
   
+  @Test
   public void testLeftOpenRange_2bit() throws Exception {
     testLeftOpenRange(2);
   }
@@ -214,14 +238,17 @@
     assertEquals("Last doc", (noDocs-1)*distance+startOffset, Integer.parseInt(doc.get(field)) );
   }
   
+  @Test
   public void testRightOpenRange_8bit() throws Exception {
     testRightOpenRange(8);
   }
   
+  @Test
   public void testRightOpenRange_4bit() throws Exception {
     testRightOpenRange(4);
   }
   
+  @Test
   public void testRightOpenRange_2bit() throws Exception {
     testRightOpenRange(2);
   }
@@ -278,18 +305,22 @@
     }
   }
   
+  @Test
   public void testRandomTrieAndClassicRangeQuery_8bit() throws Exception {
     testRandomTrieAndClassicRangeQuery(8);
   }
   
+  @Test
   public void testRandomTrieAndClassicRangeQuery_4bit() throws Exception {
     testRandomTrieAndClassicRangeQuery(4);
   }
   
+  @Test
   public void testRandomTrieAndClassicRangeQuery_2bit() throws Exception {
     testRandomTrieAndClassicRangeQuery(2);
   }
   
+  @Test
   public void testRandomTrieAndClassicRangeQuery_NoTrie() throws Exception {
     testRandomTrieAndClassicRangeQuery(Integer.MAX_VALUE);
   }
@@ -323,14 +354,17 @@
     }
   }
 
+  @Test
   public void testRangeSplit_8bit() throws Exception {
     testRangeSplit(8);
   }
   
+  @Test
   public void testRangeSplit_4bit() throws Exception {
     testRangeSplit(4);
   }
   
+  @Test
   public void testRangeSplit_2bit() throws Exception {
     testRangeSplit(2);
   }
@@ -351,14 +385,17 @@
     assertEquals("Returned count of range filter must be equal to inclusive range length", upper-lower+1, tTopDocs.totalHits );
   }
 
+  @Test
   public void testFloatRange_8bit() throws Exception {
     testFloatRange(8);
   }
   
+  @Test
   public void testFloatRange_4bit() throws Exception {
     testFloatRange(4);
   }
   
+  @Test
   public void testFloatRange_2bit() throws Exception {
     testFloatRange(2);
   }
@@ -388,18 +425,22 @@
     }
   }
 
+  @Test
   public void testSorting_8bit() throws Exception {
     testSorting(8);
   }
   
+  @Test
   public void testSorting_4bit() throws Exception {
     testSorting(4);
   }
   
+  @Test
   public void testSorting_2bit() throws Exception {
     testSorting(2);
   }
   
+  @Test
   public void testEqualsAndHash() throws Exception {
     QueryUtils.checkHashEquals(NumericRangeQuery.newIntRange("test1", 4, 10, 20, true, true));
     QueryUtils.checkHashEquals(NumericRangeQuery.newIntRange("test2", 4, 10, 20, false, true));
@@ -460,6 +501,7 @@
 
   }
   
+  @Test
   public void testEnum() throws Exception {
     int count=3000;
     int lower=(distance*3/2)+startOffset, upper=lower + count*distance + (distance/3);
Index: src/test/org/apache/lucene/search/TestPrefixInBooleanQuery.java
===================================================================
--- src/test/org/apache/lucene/search/TestPrefixInBooleanQuery.java	(revision 916567)
+++ src/test/org/apache/lucene/search/TestPrefixInBooleanQuery.java	(working copy)
@@ -18,7 +18,6 @@
  */
 
 import org.apache.lucene.util.LuceneTestCase;
-
 import org.apache.lucene.analysis.WhitespaceAnalyzer;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
@@ -46,7 +45,7 @@
     super.setUp();
     
     IndexWriter writer = new IndexWriter(directory,
-                                         new WhitespaceAnalyzer(), true,
+                                         new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true,
                                          IndexWriter.MaxFieldLength.LIMITED);
 
     for (int i = 0; i < 5137; ++i) {
Index: src/test/org/apache/lucene/search/TestDocBoost.java
===================================================================
--- src/test/org/apache/lucene/search/TestDocBoost.java	(revision 916567)
+++ src/test/org/apache/lucene/search/TestDocBoost.java	(working copy)
@@ -39,7 +39,7 @@
 
   public void testDocBoost() throws Exception {
     RAMDirectory store = new RAMDirectory();
-    IndexWriter writer = new IndexWriter(store, new SimpleAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
+    IndexWriter writer = new IndexWriter(store, new SimpleAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
 
     Fieldable f1 = new Field("field", "word", Field.Store.YES, Field.Index.ANALYZED);
     Fieldable f2 = new Field("field", "word", Field.Store.YES, Field.Index.ANALYZED);
Index: src/test/org/apache/lucene/search/TestElevationComparator.java
===================================================================
--- src/test/org/apache/lucene/search/TestElevationComparator.java	(revision 916567)
+++ src/test/org/apache/lucene/search/TestElevationComparator.java	(working copy)
@@ -23,7 +23,6 @@
 import org.apache.lucene.index.*;
 import org.apache.lucene.store.*;
 import org.apache.lucene.util.LuceneTestCase;
-
 import java.io.IOException;
 import java.util.HashMap;
 import java.util.Map;
@@ -35,7 +34,7 @@
   //@Test
   public void testSorting() throws Throwable {
     Directory directory = new MockRAMDirectory();
-    IndexWriter writer = new IndexWriter(directory, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
+    IndexWriter writer = new IndexWriter(directory, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
     writer.setMaxBufferedDocs(2);
     writer.setMergeFactor(1000);
     writer.addDocument(adoc(new String[] {"id", "a", "title", "ipod", "str_s", "a"}));
Index: src/test/org/apache/lucene/search/TestNumericRangeQuery64.java
===================================================================
--- src/test/org/apache/lucene/search/TestNumericRangeQuery64.java	(revision 916567)
+++ src/test/org/apache/lucene/search/TestNumericRangeQuery64.java	(working copy)
@@ -26,10 +26,15 @@
 import org.apache.lucene.index.IndexWriter;
 import org.apache.lucene.index.IndexWriter.MaxFieldLength;
 import org.apache.lucene.store.RAMDirectory;
-import org.apache.lucene.util.LuceneTestCase;
+import org.apache.lucene.util.LuceneTestCaseJ4;
 import org.apache.lucene.util.NumericUtils;
 
-public class TestNumericRangeQuery64 extends LuceneTestCase {
+import org.junit.Test;
+import org.junit.AfterClass;
+import org.junit.BeforeClass;
+import static org.junit.Assert.*;
+
+public class TestNumericRangeQuery64 extends LuceneTestCaseJ4 {
   // distance of entries
   private static final long distance = 66666L;
   // shift the starting of the values to the left, to also have negative values:
@@ -37,59 +42,70 @@
   // number of docs to generate for testing
   private static final int noDocs = 10000;
   
-  private static final RAMDirectory directory;
-  private static final IndexSearcher searcher;
-  static {
-    try {
-      // set the theoretical maximum term count for 8bit (see docs for the number)
-      BooleanQuery.setMaxClauseCount(7*255*2 + 255);
-      
-      directory = new RAMDirectory();
-      IndexWriter writer = new IndexWriter(directory, new WhitespaceAnalyzer(),
-      true, MaxFieldLength.UNLIMITED);
-      
-      NumericField
-        field8 = new NumericField("field8", 8, Field.Store.YES, true),
-        field6 = new NumericField("field6", 6, Field.Store.YES, true),
-        field4 = new NumericField("field4", 4, Field.Store.YES, true),
-        field2 = new NumericField("field2", 2, Field.Store.YES, true),
-        fieldNoTrie = new NumericField("field"+Integer.MAX_VALUE, Integer.MAX_VALUE, Field.Store.YES, true),
-        ascfield8 = new NumericField("ascfield8", 8, Field.Store.NO, true),
-        ascfield6 = new NumericField("ascfield6", 6, Field.Store.NO, true),
-        ascfield4 = new NumericField("ascfield4", 4, Field.Store.NO, true),
-        ascfield2 = new NumericField("ascfield2", 2, Field.Store.NO, true);
-      
-      Document doc = new Document();
-      // add fields, that have a distance to test general functionality
-      doc.add(field8); doc.add(field6); doc.add(field4); doc.add(field2); doc.add(fieldNoTrie);
-      // add ascending fields with a distance of 1, beginning at -noDocs/2 to test the correct splitting of range and inclusive/exclusive
-      doc.add(ascfield8); doc.add(ascfield6); doc.add(ascfield4); doc.add(ascfield2);
-      
-      // Add a series of noDocs docs with increasing long values, by updating the fields
-      for (int l=0; l<noDocs; l++) {
-        long val=distance*l+startOffset;
-        field8.setLongValue(val);
-        field6.setLongValue(val);
-        field4.setLongValue(val);
-        field2.setLongValue(val);
-        fieldNoTrie.setLongValue(val);
+  private static RAMDirectory directory = null;
+  private static IndexSearcher searcher = null;
+  
+  @BeforeClass
+  public static void beforeClass() throws Exception {
+    directory = new RAMDirectory();
+    IndexWriter writer = new IndexWriter(directory, new WhitespaceAnalyzer(TEST_VERSION_CURRENT),
+    true, MaxFieldLength.UNLIMITED);
+    
+    NumericField
+      field8 = new NumericField("field8", 8, Field.Store.YES, true),
+      field6 = new NumericField("field6", 6, Field.Store.YES, true),
+      field4 = new NumericField("field4", 4, Field.Store.YES, true),
+      field2 = new NumericField("field2", 2, Field.Store.YES, true),
+      fieldNoTrie = new NumericField("field"+Integer.MAX_VALUE, Integer.MAX_VALUE, Field.Store.YES, true),
+      ascfield8 = new NumericField("ascfield8", 8, Field.Store.NO, true),
+      ascfield6 = new NumericField("ascfield6", 6, Field.Store.NO, true),
+      ascfield4 = new NumericField("ascfield4", 4, Field.Store.NO, true),
+      ascfield2 = new NumericField("ascfield2", 2, Field.Store.NO, true);
+    
+    Document doc = new Document();
+    // add fields, that have a distance to test general functionality
+    doc.add(field8); doc.add(field6); doc.add(field4); doc.add(field2); doc.add(fieldNoTrie);
+    // add ascending fields with a distance of 1, beginning at -noDocs/2 to test the correct splitting of range and inclusive/exclusive
+    doc.add(ascfield8); doc.add(ascfield6); doc.add(ascfield4); doc.add(ascfield2);
+    
+    // Add a series of noDocs docs with increasing long values, by updating the fields
+    for (int l=0; l<noDocs; l++) {
+      long val=distance*l+startOffset;
+      field8.setLongValue(val);
+      field6.setLongValue(val);
+      field4.setLongValue(val);
+      field2.setLongValue(val);
+      fieldNoTrie.setLongValue(val);
 
-        val=l-(noDocs/2);
-        ascfield8.setLongValue(val);
-        ascfield6.setLongValue(val);
-        ascfield4.setLongValue(val);
-        ascfield2.setLongValue(val);
-        writer.addDocument(doc);
-      }
-    
-      writer.optimize();
-      writer.close();
-      searcher=new IndexSearcher(directory, true);
-    } catch (Exception e) {
-      throw new Error(e);
+      val=l-(noDocs/2);
+      ascfield8.setLongValue(val);
+      ascfield6.setLongValue(val);
+      ascfield4.setLongValue(val);
+      ascfield2.setLongValue(val);
+      writer.addDocument(doc);
     }
+  
+    writer.optimize();
+    writer.close();
+    searcher=new IndexSearcher(directory, true);
   }
   
+  @AfterClass
+  public static void afterClass() throws Exception {
+    searcher.close();
+    searcher = null;
+    directory.close();
+    directory = null;
+  }
+  
+  @Override
+  public void setUp() throws Exception {
+    super.setUp();
+    // set the theoretical maximum term count for 8bit (see docs for the number)
+    // super.tearDown will restore the default
+    BooleanQuery.setMaxClauseCount(7*255*2 + 255);
+  }
+  
   /** test for constant score + boolean query + filter, the other tests only use the constant score mode */
   private void testRange(int precisionStep) throws Exception {
     String field="field"+precisionStep;
@@ -140,22 +156,27 @@
     }
   }
 
+  @Test
   public void testRange_8bit() throws Exception {
     testRange(8);
   }
   
+  @Test
   public void testRange_6bit() throws Exception {
     testRange(6);
   }
   
+  @Test
   public void testRange_4bit() throws Exception {
     testRange(4);
   }
   
+  @Test
   public void testRange_2bit() throws Exception {
     testRange(2);
   }
   
+  @Test
   public void testInverseRange() throws Exception {
     NumericRangeFilter<Long> f = NumericRangeFilter.newLongRange("field8", 8, 1000L, -1000L, true, true);
     assertSame("A inverse range should return the EMPTY_DOCIDSET instance", DocIdSet.EMPTY_DOCIDSET, f.getDocIdSet(searcher.getIndexReader()));
@@ -167,6 +188,7 @@
       DocIdSet.EMPTY_DOCIDSET, f.getDocIdSet(searcher.getIndexReader()));
   }
   
+  @Test
   public void testOneMatchQuery() throws Exception {
     NumericRangeQuery<Long> q = NumericRangeQuery.newLongRange("ascfield8", 8, 1000L, 1000L, true, true);
     assertSame(MultiTermQuery.CONSTANT_SCORE_BOOLEAN_QUERY_REWRITE, q.getRewriteMethod());
@@ -192,18 +214,22 @@
     assertEquals("Last doc", (count-1)*distance+startOffset, Long.parseLong(doc.get(field)) );
   }
   
+  @Test
   public void testLeftOpenRange_8bit() throws Exception {
     testLeftOpenRange(8);
   }
   
+  @Test
   public void testLeftOpenRange_6bit() throws Exception {
     testLeftOpenRange(6);
   }
   
+  @Test
   public void testLeftOpenRange_4bit() throws Exception {
     testLeftOpenRange(4);
   }
   
+  @Test
   public void testLeftOpenRange_2bit() throws Exception {
     testLeftOpenRange(2);
   }
@@ -224,18 +250,22 @@
     assertEquals("Last doc", (noDocs-1)*distance+startOffset, Long.parseLong(doc.get(field)) );
   }
   
+  @Test
   public void testRightOpenRange_8bit() throws Exception {
     testRightOpenRange(8);
   }
   
+  @Test
   public void testRightOpenRange_6bit() throws Exception {
     testRightOpenRange(6);
   }
   
+  @Test
   public void testRightOpenRange_4bit() throws Exception {
     testRightOpenRange(4);
   }
   
+  @Test
   public void testRightOpenRange_2bit() throws Exception {
     testRightOpenRange(2);
   }
@@ -292,22 +322,27 @@
     }
   }
   
+  @Test
   public void testRandomTrieAndClassicRangeQuery_8bit() throws Exception {
     testRandomTrieAndClassicRangeQuery(8);
   }
   
+  @Test
   public void testRandomTrieAndClassicRangeQuery_6bit() throws Exception {
     testRandomTrieAndClassicRangeQuery(6);
   }
   
+  @Test
   public void testRandomTrieAndClassicRangeQuery_4bit() throws Exception {
     testRandomTrieAndClassicRangeQuery(4);
   }
   
+  @Test
   public void testRandomTrieAndClassicRangeQuery_2bit() throws Exception {
     testRandomTrieAndClassicRangeQuery(2);
   }
   
+  @Test
   public void testRandomTrieAndClassicRangeQuery_NoTrie() throws Exception {
     testRandomTrieAndClassicRangeQuery(Integer.MAX_VALUE);
   }
@@ -341,18 +376,22 @@
     }
   }
 
+  @Test
   public void testRangeSplit_8bit() throws Exception {
     testRangeSplit(8);
   }
   
+  @Test
   public void testRangeSplit_6bit() throws Exception {
     testRangeSplit(6);
   }
   
+  @Test
   public void testRangeSplit_4bit() throws Exception {
     testRangeSplit(4);
   }
   
+  @Test
   public void testRangeSplit_2bit() throws Exception {
     testRangeSplit(2);
   }
@@ -373,18 +412,22 @@
     assertEquals("Returned count of range filter must be equal to inclusive range length", upper-lower+1, tTopDocs.totalHits );
   }
 
+  @Test
   public void testDoubleRange_8bit() throws Exception {
     testDoubleRange(8);
   }
   
+  @Test
   public void testDoubleRange_6bit() throws Exception {
     testDoubleRange(6);
   }
   
+  @Test
   public void testDoubleRange_4bit() throws Exception {
     testDoubleRange(4);
   }
   
+  @Test
   public void testDoubleRange_2bit() throws Exception {
     testDoubleRange(2);
   }
@@ -414,22 +457,27 @@
     }
   }
 
+  @Test
   public void testSorting_8bit() throws Exception {
     testSorting(8);
   }
   
+  @Test
   public void testSorting_6bit() throws Exception {
     testSorting(6);
   }
   
+  @Test
   public void testSorting_4bit() throws Exception {
     testSorting(4);
   }
   
+  @Test
   public void testSorting_2bit() throws Exception {
     testSorting(2);
   }
   
+  @Test
   public void testEqualsAndHash() throws Exception {
     QueryUtils.checkHashEquals(NumericRangeQuery.newLongRange("test1", 4, 10L, 20L, true, true));
     QueryUtils.checkHashEquals(NumericRangeQuery.newLongRange("test2", 4, 10L, 20L, false, true));
Index: src/test/org/apache/lucene/search/TestMultiSearcherRanking.java
===================================================================
--- src/test/org/apache/lucene/search/TestMultiSearcherRanking.java	(revision 916567)
+++ src/test/org/apache/lucene/search/TestMultiSearcherRanking.java	(working copy)
@@ -26,8 +26,6 @@
 import org.apache.lucene.queryParser.QueryParser;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.RAMDirectory;
-import org.apache.lucene.util.Version;
-
 import java.io.IOException;
 
 /**
@@ -88,7 +86,7 @@
   private void checkQuery(String queryStr) throws IOException, ParseException {
     // check result hit ranking
     if(verbose) System.out.println("Query: " + queryStr);
-      QueryParser queryParser = new QueryParser(Version.LUCENE_CURRENT, FIELD_NAME, new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT));
+      QueryParser queryParser = new QueryParser(TEST_VERSION_CURRENT, FIELD_NAME, new StandardAnalyzer(TEST_VERSION_CURRENT));
     Query query = queryParser.parse(queryStr);
     ScoreDoc[] multiSearcherHits = multiSearcher.search(query, null, 1000).scoreDocs;
     ScoreDoc[] singleSearcherHits = singleSearcher.search(query, null, 1000).scoreDocs;
@@ -115,12 +113,12 @@
     super.setUp();
     // create MultiSearcher from two seperate searchers
     Directory d1 = new RAMDirectory();
-    IndexWriter iw1 = new IndexWriter(d1, new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT), true,
+    IndexWriter iw1 = new IndexWriter(d1, new StandardAnalyzer(TEST_VERSION_CURRENT), true,
                                       IndexWriter.MaxFieldLength.LIMITED);
     addCollection1(iw1);
     iw1.close();
     Directory d2 = new RAMDirectory();
-    IndexWriter iw2 = new IndexWriter(d2, new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT), true,
+    IndexWriter iw2 = new IndexWriter(d2, new StandardAnalyzer(TEST_VERSION_CURRENT), true,
                                       IndexWriter.MaxFieldLength.LIMITED);
     addCollection2(iw2);
     iw2.close();
@@ -132,7 +130,7 @@
 
     // create IndexSearcher which contains all documents
     Directory d = new RAMDirectory();
-    IndexWriter iw = new IndexWriter(d, new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT), true,
+    IndexWriter iw = new IndexWriter(d, new StandardAnalyzer(TEST_VERSION_CURRENT), true,
                                      IndexWriter.MaxFieldLength.LIMITED);
     addCollection1(iw);
     addCollection2(iw);
Index: src/test/org/apache/lucene/search/TestPrefixQuery.java
===================================================================
--- src/test/org/apache/lucene/search/TestPrefixQuery.java	(revision 916567)
+++ src/test/org/apache/lucene/search/TestPrefixQuery.java	(working copy)
@@ -36,7 +36,7 @@
     String[] categories = new String[] {"/Computers",
                                         "/Computers/Mac",
                                         "/Computers/Windows"};
-    IndexWriter writer = new IndexWriter(directory, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
+    IndexWriter writer = new IndexWriter(directory, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
     for (int i = 0; i < categories.length; i++) {
       Document doc = new Document();
       doc.add(new Field("category", categories[i], Field.Store.YES, Field.Index.NOT_ANALYZED));
Index: src/test/org/apache/lucene/search/TestMultiPhraseQuery.java
===================================================================
--- src/test/org/apache/lucene/search/TestMultiPhraseQuery.java	(revision 916567)
+++ src/test/org/apache/lucene/search/TestMultiPhraseQuery.java	(working copy)
@@ -48,7 +48,7 @@
 
     public void testPhrasePrefix() throws IOException {
         MockRAMDirectory indexStore = new MockRAMDirectory();
-        IndexWriter writer = new IndexWriter(indexStore, new SimpleAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
+        IndexWriter writer = new IndexWriter(indexStore, new SimpleAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
         add("blueberry pie", writer);
         add("blueberry strudel", writer);
         add("blueberry pizza", writer);
@@ -143,7 +143,7 @@
       // The contained PhraseMultiQuery must contain exactly one term array.
 
       MockRAMDirectory indexStore = new MockRAMDirectory();
-      IndexWriter writer = new IndexWriter(indexStore, new SimpleAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
+      IndexWriter writer = new IndexWriter(indexStore, new SimpleAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
       add("blueberry pie", writer);
       add("blueberry chewing gum", writer);
       add("blue raspberry pie", writer);
@@ -172,7 +172,7 @@
     
   public void testPhrasePrefixWithBooleanQuery() throws IOException {
     MockRAMDirectory indexStore = new MockRAMDirectory();
-    IndexWriter writer = new IndexWriter(indexStore, new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT, Collections.emptySet()), true, IndexWriter.MaxFieldLength.LIMITED);
+    IndexWriter writer = new IndexWriter(indexStore, new StandardAnalyzer(TEST_VERSION_CURRENT, Collections.emptySet()), true, IndexWriter.MaxFieldLength.LIMITED);
     add("This is a test", "object", writer);
     add("a note", "note", writer);
     writer.close();
Index: src/test/org/apache/lucene/search/TestBooleanPrefixQuery.java
===================================================================
--- src/test/org/apache/lucene/search/TestBooleanPrefixQuery.java	(revision 916567)
+++ src/test/org/apache/lucene/search/TestBooleanPrefixQuery.java	(working copy)
@@ -79,8 +79,7 @@
     Query rw2 = null;
     IndexReader reader = null;
     try {
-      IndexWriter writer = new IndexWriter(directory, new
-                                           WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
+      IndexWriter writer = new IndexWriter(directory, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
       for (int i = 0; i < categories.length; i++) {
         Document doc = new Document();
         doc.add(new Field("category", categories[i], Field.Store.YES, Field.Index.NOT_ANALYZED));
Index: src/test/org/apache/lucene/search/TestFilteredQuery.java
===================================================================
--- src/test/org/apache/lucene/search/TestFilteredQuery.java	(revision 916567)
+++ src/test/org/apache/lucene/search/TestFilteredQuery.java	(working copy)
@@ -27,7 +27,6 @@
 import org.apache.lucene.store.RAMDirectory;
 import org.apache.lucene.util.LuceneTestCase;
 import org.apache.lucene.util.DocIdBitSet;
-
 import java.util.BitSet;
 
 /**
@@ -49,7 +48,7 @@
   public void setUp() throws Exception {
     super.setUp();
     directory = new RAMDirectory();
-    IndexWriter writer = new IndexWriter (directory, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
+    IndexWriter writer = new IndexWriter (directory, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
 
     Document doc = new Document();
     doc.add (new Field("field", "one two three four five", Field.Store.YES, Field.Index.ANALYZED));
Index: src/test/org/apache/lucene/search/QueryUtils.java
===================================================================
--- src/test/org/apache/lucene/search/QueryUtils.java	(revision 916567)
+++ src/test/org/apache/lucene/search/QueryUtils.java	(working copy)
@@ -15,6 +15,7 @@
 import org.apache.lucene.index.MultiReader;
 import org.apache.lucene.index.IndexWriter.MaxFieldLength;
 import org.apache.lucene.store.RAMDirectory;
+import static org.apache.lucene.util.LuceneTestCaseJ4.TEST_VERSION_CURRENT;
 
 /**
  * Copyright 2005 Apache Software Foundation
@@ -199,7 +200,7 @@
   private static RAMDirectory makeEmptyIndex(final int numDeletedDocs) 
     throws IOException {
       RAMDirectory d = new RAMDirectory();
-      IndexWriter w = new IndexWriter(d, new WhitespaceAnalyzer(), true,
+      IndexWriter w = new IndexWriter(d, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true,
                                       MaxFieldLength.LIMITED);
       for (int i = 0; i < numDeletedDocs; i++) {
         w.addDocument(new Document());
Index: src/test/org/apache/lucene/search/TestTermRangeFilter.java
===================================================================
--- src/test/org/apache/lucene/search/TestTermRangeFilter.java	(revision 916567)
+++ src/test/org/apache/lucene/search/TestTermRangeFilter.java	(working copy)
@@ -339,7 +339,7 @@
             
         /* build an index */
         RAMDirectory farsiIndex = new RAMDirectory();
-        IndexWriter writer = new IndexWriter(farsiIndex, new SimpleAnalyzer(), T, 
+        IndexWriter writer = new IndexWriter(farsiIndex, new SimpleAnalyzer(TEST_VERSION_CURRENT), T, 
                                              IndexWriter.MaxFieldLength.LIMITED);
         Document doc = new Document();
         doc.add(new Field("content","\u0633\u0627\u0628", 
@@ -379,7 +379,7 @@
         /* build an index */
         RAMDirectory danishIndex = new RAMDirectory();
         IndexWriter writer = new IndexWriter
-            (danishIndex, new SimpleAnalyzer(), T, 
+            (danishIndex, new SimpleAnalyzer(TEST_VERSION_CURRENT), T, 
              IndexWriter.MaxFieldLength.LIMITED);
         // Danish collation orders the words below in the given order
         // (example taken from TestSort.testInternationalSort() ).
Index: src/test/org/apache/lucene/index/TestDoc.java
===================================================================
--- src/test/org/apache/lucene/index/TestDoc.java	(revision 916567)
+++ src/test/org/apache/lucene/index/TestDoc.java	(working copy)
@@ -110,7 +110,7 @@
       PrintWriter out = new PrintWriter(sw, true);
 
       Directory directory = FSDirectory.open(indexDir);
-      IndexWriter writer = new IndexWriter(directory, new SimpleAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
+      IndexWriter writer = new IndexWriter(directory, new SimpleAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
 
       SegmentInfo si1 = indexDoc(writer, "test.txt");
       printSegment(out, si1);
@@ -138,7 +138,7 @@
       out = new PrintWriter(sw, true);
 
       directory = FSDirectory.open(indexDir);
-      writer = new IndexWriter(directory, new SimpleAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
+      writer = new IndexWriter(directory, new SimpleAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
 
       si1 = indexDoc(writer, "test.txt");
       printSegment(out, si1);
Index: src/test/org/apache/lucene/index/TestParallelTermEnum.java
===================================================================
--- src/test/org/apache/lucene/index/TestParallelTermEnum.java	(revision 916567)
+++ src/test/org/apache/lucene/index/TestParallelTermEnum.java	(working copy)
@@ -20,7 +20,6 @@
 import java.io.IOException;
 
 import org.apache.lucene.util.LuceneTestCase;
-
 import org.apache.lucene.analysis.SimpleAnalyzer;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
@@ -38,7 +37,7 @@
         Document doc;
 
         RAMDirectory rd1 = new RAMDirectory();
-        IndexWriter iw1 = new IndexWriter(rd1, new SimpleAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
+        IndexWriter iw1 = new IndexWriter(rd1, new SimpleAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
 
         doc = new Document();
         doc.add(new Field("field1", "the quick brown fox jumps", Store.YES,
@@ -50,7 +49,7 @@
 
         iw1.close();
         RAMDirectory rd2 = new RAMDirectory();
-        IndexWriter iw2 = new IndexWriter(rd2, new SimpleAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
+        IndexWriter iw2 = new IndexWriter(rd2, new SimpleAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
 
         doc = new Document();
         doc.add(new Field("field0", "", Store.NO, Index.ANALYZED));
Index: src/test/org/apache/lucene/index/TestIndexReaderClone.java
===================================================================
--- src/test/org/apache/lucene/index/TestIndexReaderClone.java	(revision 916567)
+++ src/test/org/apache/lucene/index/TestIndexReaderClone.java	(working copy)
@@ -197,7 +197,7 @@
 
     TestIndexReaderReopen.createIndex(dir1, true);
     IndexReader reader1 = IndexReader.open(dir1, false);
-    IndexWriter w = new IndexWriter(dir1, new SimpleAnalyzer(), IndexWriter.MaxFieldLength.LIMITED);
+    IndexWriter w = new IndexWriter(dir1, new SimpleAnalyzer(TEST_VERSION_CURRENT), IndexWriter.MaxFieldLength.LIMITED);
     w.optimize();
     w.close();
     IndexReader reader2 = reader1.clone(true);
@@ -484,7 +484,7 @@
 
   public void testCloseStoredFields() throws Exception {
     final Directory dir = new MockRAMDirectory();
-    IndexWriter w = new IndexWriter(dir, new SimpleAnalyzer(), IndexWriter.MaxFieldLength.UNLIMITED);
+    IndexWriter w = new IndexWriter(dir, new SimpleAnalyzer(TEST_VERSION_CURRENT), IndexWriter.MaxFieldLength.UNLIMITED);
     w.setUseCompoundFile(false);
     Document doc = new Document();
     doc.add(new Field("field", "yes it's stored", Field.Store.YES, Field.Index.ANALYZED));
Index: src/test/org/apache/lucene/index/TestSegmentTermEnum.java
===================================================================
--- src/test/org/apache/lucene/index/TestSegmentTermEnum.java	(revision 916567)
+++ src/test/org/apache/lucene/index/TestSegmentTermEnum.java	(working copy)
@@ -20,10 +20,10 @@
 import java.io.IOException;
 
 import org.apache.lucene.util.LuceneTestCase;
-
 import org.apache.lucene.analysis.WhitespaceAnalyzer;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
+import org.apache.lucene.index.codecs.preflex.SegmentTermEnum;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.RAMDirectory;
 import org.apache.lucene.store.MockRAMDirectory;
@@ -37,7 +37,7 @@
   {
     IndexWriter writer = null;
 
-    writer  = new IndexWriter(dir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
+    writer  = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
 
     // ADD 100 documents with term : aaa
     // add 100 documents with terms: aaa bbb
@@ -53,7 +53,7 @@
     verifyDocFreq();
 
     // merge segments by optimizing the index
-    writer = new IndexWriter(dir, new WhitespaceAnalyzer(), false, IndexWriter.MaxFieldLength.LIMITED);
+    writer = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), false, IndexWriter.MaxFieldLength.LIMITED);
     writer.optimize();
     writer.close();
 
@@ -61,6 +61,25 @@
     verifyDocFreq();
   }
 
+  // nocommit
+  /*
+  public void testPrevTermAtEnd() throws IOException
+  {
+    Directory dir = new MockRAMDirectory();
+    IndexWriter writer  = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
+    addDoc(writer, "aaa bbb");
+    writer.close();
+    SegmentReader reader = SegmentReader.getOnlySegmentReader(dir);
+    SegmentTermEnum termEnum = (SegmentTermEnum) reader.terms();
+    assertTrue(termEnum.next());
+    assertEquals("aaa", termEnum.term().text());
+    assertTrue(termEnum.next());
+    assertEquals("aaa", termEnum.prev().text());
+    assertEquals("bbb", termEnum.term().text());
+    assertFalse(termEnum.next());
+    assertEquals("bbb", termEnum.prev().text());
+  }
+  */
   private void verifyDocFreq()
       throws IOException
   {
Index: src/test/org/apache/lucene/index/TestIndexReader.java
===================================================================
--- src/test/org/apache/lucene/index/TestIndexReader.java	(revision 916567)
+++ src/test/org/apache/lucene/index/TestIndexReader.java	(working copy)
@@ -80,7 +80,7 @@
       commitUserData.put("foo", "fighters");
       
       // set up writer
-      IndexWriter writer = new IndexWriter(d, new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
+      IndexWriter writer = new IndexWriter(d, new StandardAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
       writer.setMaxBufferedDocs(2);
       for(int i=0;i<27;i++)
         addDocumentWithFields(writer);
@@ -102,7 +102,7 @@
       assertTrue(c.equals(r.getIndexCommit()));
 
       // Change the index
-      writer = new IndexWriter(d, new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT), false, IndexWriter.MaxFieldLength.LIMITED);
+      writer = new IndexWriter(d, new StandardAnalyzer(TEST_VERSION_CURRENT), false, IndexWriter.MaxFieldLength.LIMITED);
       writer.setMaxBufferedDocs(2);
       for(int i=0;i<7;i++)
         addDocumentWithFields(writer);
@@ -113,7 +113,7 @@
       assertFalse(r2.getIndexCommit().isOptimized());
       r3.close();
 
-      writer = new IndexWriter(d, new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT), false, IndexWriter.MaxFieldLength.LIMITED);
+      writer = new IndexWriter(d, new StandardAnalyzer(TEST_VERSION_CURRENT), false, IndexWriter.MaxFieldLength.LIMITED);
       writer.optimize();
       writer.close();
 
@@ -127,19 +127,19 @@
     public void testIsCurrent() throws Exception
     {
       RAMDirectory d = new MockRAMDirectory();
-      IndexWriter writer = new IndexWriter(d, new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
+      IndexWriter writer = new IndexWriter(d, new StandardAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
       addDocumentWithFields(writer);
       writer.close();
       // set up reader:
       IndexReader reader = IndexReader.open(d, false);
       assertTrue(reader.isCurrent());
       // modify index by adding another document:
-      writer = new IndexWriter(d, new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT), false, IndexWriter.MaxFieldLength.LIMITED);
+      writer = new IndexWriter(d, new StandardAnalyzer(TEST_VERSION_CURRENT), false, IndexWriter.MaxFieldLength.LIMITED);
       addDocumentWithFields(writer);
       writer.close();
       assertFalse(reader.isCurrent());
       // re-create index:
-      writer = new IndexWriter(d, new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
+      writer = new IndexWriter(d, new StandardAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
       addDocumentWithFields(writer);
       writer.close();
       assertFalse(reader.isCurrent());
@@ -155,7 +155,7 @@
     {
         RAMDirectory d = new MockRAMDirectory();
         // set up writer
-        IndexWriter writer = new IndexWriter(d, new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
+        IndexWriter writer = new IndexWriter(d, new StandardAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
         addDocumentWithFields(writer);
         writer.close();
         // set up reader
@@ -167,7 +167,7 @@
         assertTrue(fieldNames.contains("unstored"));
         reader.close();
         // add more documents
-        writer = new IndexWriter(d, new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT), false, IndexWriter.MaxFieldLength.LIMITED);
+        writer = new IndexWriter(d, new StandardAnalyzer(TEST_VERSION_CURRENT), false, IndexWriter.MaxFieldLength.LIMITED);
         // want to get some more segments here
         for (int i = 0; i < 5*writer.getMergeFactor(); i++)
         {
@@ -247,7 +247,7 @@
   public void testTermVectors() throws Exception {
     RAMDirectory d = new MockRAMDirectory();
     // set up writer
-    IndexWriter writer = new IndexWriter(d, new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
+    IndexWriter writer = new IndexWriter(d, new StandardAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
     // want to get some more segments here
     // new termvector fields
     for (int i = 0; i < 5 * writer.getMergeFactor(); i++) {
@@ -315,7 +315,7 @@
         Term searchTerm = new Term("content", "aaa");
 
         //  add 100 documents with term : aaa
-        writer  = new IndexWriter(dir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
+        writer  = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
         for (int i = 0; i < 100; i++)
         {
             addDoc(writer, searchTerm.text());
@@ -357,7 +357,7 @@
         Directory dir = new RAMDirectory();
         byte[] bin = new byte[]{0, 1, 2, 3, 4, 5, 6, 7, 8, 9};
         
-        IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.UNLIMITED);
+        IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.UNLIMITED);
         
         for (int i = 0; i < 10; i++) {
           addDoc(writer, "document number " + (i + 1));
@@ -366,7 +366,7 @@
           addDocumentWithTermVectorFields(writer);
         }
         writer.close();
-        writer = new IndexWriter(dir, new WhitespaceAnalyzer(), false, IndexWriter.MaxFieldLength.LIMITED);
+        writer = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), false, IndexWriter.MaxFieldLength.LIMITED);
         Document doc = new Document();
         doc.add(new Field("bin1", bin, Field.Store.YES));
         doc.add(new Field("junk", "junk text", Field.Store.NO, Field.Index.ANALYZED));
@@ -403,7 +403,7 @@
         // force optimize
 
 
-        writer = new IndexWriter(dir, new WhitespaceAnalyzer(), false, IndexWriter.MaxFieldLength.LIMITED);
+        writer = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), false, IndexWriter.MaxFieldLength.LIMITED);
         writer.optimize();
         writer.close();
         reader = IndexReader.open(dir, false);
@@ -432,7 +432,7 @@
         Term searchTerm = new Term("content", "aaa");
 
         //  add 11 documents with term : aaa
-        writer  = new IndexWriter(dir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
+        writer  = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
         for (int i = 0; i < 11; i++)
         {
             addDoc(writer, searchTerm.text());
@@ -477,7 +477,7 @@
         Term searchTerm = new Term("content", "aaa");
 
         //  add 11 documents with term : aaa
-        writer  = new IndexWriter(dir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
+        writer  = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
         for (int i = 0; i < 11; i++)
         {
             addDoc(writer, searchTerm.text());
@@ -526,7 +526,7 @@
         Term searchTerm = new Term("content", "aaa");
 
         //  add 1 documents with term : aaa
-        writer  = new IndexWriter(dir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
+        writer  = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
         addDoc(writer, searchTerm.text());
         writer.close();
 
@@ -571,7 +571,7 @@
         Term searchTerm = new Term("content", "aaa");
 
         //  add 1 documents with term : aaa
-        writer  = new IndexWriter(dir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
+        writer  = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
         writer.setUseCompoundFile(false);
         addDoc(writer, searchTerm.text());
         writer.close();
@@ -625,7 +625,7 @@
         Term searchTerm2 = new Term("content", "bbb");
 
         //  add 100 documents with term : aaa
-        IndexWriter writer  = new IndexWriter(dir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
+        IndexWriter writer  = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
         for (int i = 0; i < 100; i++)
         {
             addDoc(writer, searchTerm.text());
@@ -641,7 +641,7 @@
         assertTermDocsCount("first reader", reader, searchTerm2, 0);
 
         // add 100 documents with term : bbb
-        writer  = new IndexWriter(dir, new WhitespaceAnalyzer(), false, IndexWriter.MaxFieldLength.LIMITED);
+        writer  = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), false, IndexWriter.MaxFieldLength.LIMITED);
         for (int i = 0; i < 100; i++)
         {
             addDoc(writer, searchTerm2.text());
@@ -708,7 +708,7 @@
         // Create initial data set
         File dirFile = new File(System.getProperty("tempDir"), "testIndex");
         Directory dir = getDirectory();
-        IndexWriter writer  = new IndexWriter(dir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
+        IndexWriter writer  = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
         addDoc(writer, "test");
         writer.close();
         dir.close();
@@ -718,7 +718,7 @@
         dir = getDirectory();
 
         // Now create the data set again, just as before
-        writer  = new IndexWriter(dir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
+        writer  = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
         addDoc(writer, "test");
         writer.close();
         dir.close();
@@ -744,7 +744,7 @@
           else
             dir = getDirectory();
           assertFalse(IndexReader.indexExists(dir));
-          IndexWriter writer  = new IndexWriter(dir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
+          IndexWriter writer  = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
           addDocumentWithFields(writer);
           assertTrue(IndexWriter.isLocked(dir));		// writer open, so dir is locked
           writer.close();
@@ -761,7 +761,7 @@
           // incremented:
           Thread.sleep(1000);
 
-          writer  = new IndexWriter(dir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
+          writer  = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
           addDocumentWithFields(writer);
           writer.close();
           reader = IndexReader.open(dir, false);
@@ -778,7 +778,7 @@
     public void testVersion() throws IOException {
       Directory dir = new MockRAMDirectory();
       assertFalse(IndexReader.indexExists(dir));
-      IndexWriter writer  = new IndexWriter(dir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
+      IndexWriter writer  = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
       addDocumentWithFields(writer);
       assertTrue(IndexWriter.isLocked(dir));		// writer open, so dir is locked
       writer.close();
@@ -789,7 +789,7 @@
       reader.close();
       // modify index and check version has been
       // incremented:
-      writer  = new IndexWriter(dir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
+      writer  = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
       addDocumentWithFields(writer);
       writer.close();
       reader = IndexReader.open(dir, false);
@@ -800,10 +800,10 @@
 
     public void testLock() throws IOException {
       Directory dir = new MockRAMDirectory();
-      IndexWriter writer  = new IndexWriter(dir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
+      IndexWriter writer  = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
       addDocumentWithFields(writer);
       writer.close();
-      writer  = new IndexWriter(dir, new WhitespaceAnalyzer(), false, IndexWriter.MaxFieldLength.LIMITED);
+      writer  = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), false, IndexWriter.MaxFieldLength.LIMITED);
       IndexReader reader = IndexReader.open(dir, false);
       try {
         reader.deleteDocument(0);
@@ -820,7 +820,7 @@
 
     public void testUndeleteAll() throws IOException {
       Directory dir = new MockRAMDirectory();
-      IndexWriter writer  = new IndexWriter(dir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
+      IndexWriter writer  = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
       addDocumentWithFields(writer);
       addDocumentWithFields(writer);
       writer.close();
@@ -837,7 +837,7 @@
 
     public void testUndeleteAllAfterClose() throws IOException {
       Directory dir = new MockRAMDirectory();
-      IndexWriter writer  = new IndexWriter(dir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
+      IndexWriter writer  = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
       addDocumentWithFields(writer);
       addDocumentWithFields(writer);
       writer.close();
@@ -854,7 +854,7 @@
 
     public void testUndeleteAllAfterCloseThenReopen() throws IOException {
       Directory dir = new MockRAMDirectory();
-      IndexWriter writer  = new IndexWriter(dir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
+      IndexWriter writer  = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
       addDocumentWithFields(writer);
       addDocumentWithFields(writer);
       writer.close();
@@ -892,7 +892,7 @@
       
       // First build up a starting index:
       RAMDirectory startDir = new MockRAMDirectory();
-      IndexWriter writer = new IndexWriter(startDir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
+      IndexWriter writer = new IndexWriter(startDir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
       for(int i=0;i<157;i++) {
         Document d = new Document();
         d.add(new Field("id", Integer.toString(i), Field.Store.YES, Field.Index.NOT_ANALYZED));
@@ -1066,7 +1066,7 @@
 
     public void testDocsOutOfOrderJIRA140() throws IOException {
       Directory dir = new MockRAMDirectory();      
-      IndexWriter writer  = new IndexWriter(dir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
+      IndexWriter writer  = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
       for(int i=0;i<11;i++) {
         addDoc(writer, "aaa");
       }
@@ -1084,7 +1084,7 @@
       }
       reader.close();
 
-      writer = new IndexWriter(dir, new WhitespaceAnalyzer(), false, IndexWriter.MaxFieldLength.LIMITED);
+      writer = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), false, IndexWriter.MaxFieldLength.LIMITED);
 
       // We must add more docs to get a new segment written
       for(int i=0;i<11;i++) {
@@ -1106,7 +1106,7 @@
     public void testExceptionReleaseWriteLockJIRA768() throws IOException {
 
       Directory dir = new MockRAMDirectory();      
-      IndexWriter writer  = new IndexWriter(dir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
+      IndexWriter writer  = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
       addDoc(writer, "aaa");
       writer.close();
 
@@ -1182,7 +1182,7 @@
         //  add 100 documents with term : aaa
         //  add 100 documents with term : bbb
         //  add 100 documents with term : ccc
-        IndexWriter writer  = new IndexWriter(dir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
+        IndexWriter writer  = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
         for (int i = 0; i < 100; i++)
         {
             addDoc(writer, searchTerm1.text());
@@ -1406,7 +1406,7 @@
       RAMDirectory d = new MockRAMDirectory();
 
       // set up writer
-      IndexWriter writer = new IndexWriter(d, new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
+      IndexWriter writer = new IndexWriter(d, new StandardAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
       writer.setMaxBufferedDocs(2);
       for(int i=0;i<27;i++)
         addDocumentWithFields(writer);
@@ -1422,7 +1422,7 @@
       assertTrue(c.equals(r.getIndexCommit()));
 
       // Change the index
-      writer = new IndexWriter(d, new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT), false, IndexWriter.MaxFieldLength.LIMITED);
+      writer = new IndexWriter(d, new StandardAnalyzer(TEST_VERSION_CURRENT), false, IndexWriter.MaxFieldLength.LIMITED);
       writer.setMaxBufferedDocs(2);
       for(int i=0;i<7;i++)
         addDocumentWithFields(writer);
@@ -1433,7 +1433,7 @@
       assertFalse(r2.getIndexCommit().isOptimized());
       r2.close();
 
-      writer = new IndexWriter(d, new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT), false, IndexWriter.MaxFieldLength.LIMITED);
+      writer = new IndexWriter(d, new StandardAnalyzer(TEST_VERSION_CURRENT), false, IndexWriter.MaxFieldLength.LIMITED);
       writer.optimize();
       writer.close();
 
@@ -1447,7 +1447,7 @@
 
     public void testReadOnly() throws Throwable {
       RAMDirectory d = new MockRAMDirectory();
-      IndexWriter writer = new IndexWriter(d, new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
+      IndexWriter writer = new IndexWriter(d, new StandardAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
       addDocumentWithFields(writer);
       writer.commit();
       addDocumentWithFields(writer);
@@ -1461,7 +1461,7 @@
         // expected
       }
       
-      writer = new IndexWriter(d, new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT), false, IndexWriter.MaxFieldLength.LIMITED);
+      writer = new IndexWriter(d, new StandardAnalyzer(TEST_VERSION_CURRENT), false, IndexWriter.MaxFieldLength.LIMITED);
       addDocumentWithFields(writer);
       writer.close();
 
@@ -1478,7 +1478,7 @@
         // expected
       }
 
-      writer = new IndexWriter(d, new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT), false, IndexWriter.MaxFieldLength.LIMITED);
+      writer = new IndexWriter(d, new StandardAnalyzer(TEST_VERSION_CURRENT), false, IndexWriter.MaxFieldLength.LIMITED);
       writer.optimize();
       writer.close();
 
@@ -1496,7 +1496,7 @@
       }
 
       // Make sure write lock isn't held
-      writer = new IndexWriter(d, new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT), false, IndexWriter.MaxFieldLength.LIMITED);
+      writer = new IndexWriter(d, new StandardAnalyzer(TEST_VERSION_CURRENT), false, IndexWriter.MaxFieldLength.LIMITED);
       writer.close();
 
       r3.close();
@@ -1506,7 +1506,7 @@
   // LUCENE-1474
   public void testIndexReader() throws Exception {
     Directory dir = new RAMDirectory();
-    IndexWriter writer = new IndexWriter(dir, new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT),
+    IndexWriter writer = new IndexWriter(dir, new StandardAnalyzer(TEST_VERSION_CURRENT),
                                          IndexWriter.MaxFieldLength.UNLIMITED);
     writer.addDocument(createDocument("a"));
     writer.addDocument(createDocument("b"));
@@ -1524,7 +1524,7 @@
   public void testIndexReaderUnDeleteAll() throws Exception {
     MockRAMDirectory dir = new MockRAMDirectory();
     dir.setPreventDoubleWrite(false);
-    IndexWriter writer = new IndexWriter(dir, new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT),
+    IndexWriter writer = new IndexWriter(dir, new StandardAnalyzer(TEST_VERSION_CURRENT),
                                          IndexWriter.MaxFieldLength.UNLIMITED);
     writer.addDocument(createDocument("a"));
     writer.addDocument(createDocument("b"));
@@ -1566,7 +1566,7 @@
 
     Directory dir = new MockRAMDirectory();
     
-    IndexWriter writer = new IndexWriter(dir, new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT),
+    IndexWriter writer = new IndexWriter(dir, new StandardAnalyzer(TEST_VERSION_CURRENT),
                                          IndexWriter.MaxFieldLength.LIMITED);
 
     writer.setMaxBufferedDocs(2);
@@ -1593,7 +1593,7 @@
   public void testFieldCacheReuseAfterClone() throws Exception {
     //Codec.DEBUG = true;
     Directory dir = new MockRAMDirectory();
-    IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(), IndexWriter.MaxFieldLength.UNLIMITED);
+    IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), IndexWriter.MaxFieldLength.UNLIMITED);
     Document doc = new Document();
     doc.add(new Field("number", "17", Field.Store.NO, Field.Index.NOT_ANALYZED));
     writer.addDocument(doc);
@@ -1624,7 +1624,7 @@
   // FieldCache
   public void testFieldCacheReuseAfterReopen() throws Exception {
     Directory dir = new MockRAMDirectory();
-    IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(), IndexWriter.MaxFieldLength.UNLIMITED);
+    IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), IndexWriter.MaxFieldLength.UNLIMITED);
     Document doc = new Document();
     doc.add(new Field("number", "17", Field.Store.NO, Field.Index.NOT_ANALYZED));
     writer.addDocument(doc);
@@ -1656,7 +1656,7 @@
   // reopen switches readOnly
   public void testReopenChangeReadonly() throws Exception {
     Directory dir = new MockRAMDirectory();
-    IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(), IndexWriter.MaxFieldLength.UNLIMITED);
+    IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), IndexWriter.MaxFieldLength.UNLIMITED);
     Document doc = new Document();
     doc.add(new Field("number", "17", Field.Store.NO, Field.Index.NOT_ANALYZED));
     writer.addDocument(doc);
@@ -1697,7 +1697,7 @@
   // LUCENE-1586: getUniqueTermCount
   public void testUniqueTermCount() throws Exception {
     Directory dir = new MockRAMDirectory();
-    IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(), IndexWriter.MaxFieldLength.UNLIMITED);
+    IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), IndexWriter.MaxFieldLength.UNLIMITED);
     Document doc = new Document();
     doc.add(new Field("field", "a b c d e f g h i j k l m n o p q r s t u v w x y z", Field.Store.NO, Field.Index.ANALYZED));
     doc.add(new Field("number", "0 1 2 3 4 5 6 7 8 9", Field.Store.NO, Field.Index.ANALYZED));
@@ -1730,7 +1730,7 @@
   // LUCENE-1609: don't load terms index
   public void testNoTermsIndex() throws Throwable {
     Directory dir = new MockRAMDirectory();
-    IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(), IndexWriter.MaxFieldLength.UNLIMITED);
+    IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), IndexWriter.MaxFieldLength.UNLIMITED);
     Document doc = new Document();
     doc.add(new Field("field", "a b c d e f g h i j k l m n o p q r s t u v w x y z", Field.Store.NO, Field.Index.ANALYZED));
     doc.add(new Field("number", "0 1 2 3 4 5 6 7 8 9", Field.Store.NO, Field.Index.ANALYZED));
@@ -1747,7 +1747,7 @@
     }
 
     assertEquals(-1, ((SegmentReader) r.getSequentialSubReaders()[0]).getTermInfosIndexDivisor());
-    writer = new IndexWriter(dir, new WhitespaceAnalyzer(), IndexWriter.MaxFieldLength.UNLIMITED);
+    writer = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), IndexWriter.MaxFieldLength.UNLIMITED);
     writer.addDocument(doc);
     writer.close();
 
@@ -1769,7 +1769,7 @@
   // LUCENE-2046
   public void testPrepareCommitIsCurrent() throws Throwable {
     Directory dir = new MockRAMDirectory();
-    IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(), IndexWriter.MaxFieldLength.UNLIMITED);
+    IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), IndexWriter.MaxFieldLength.UNLIMITED);
     Document doc = new Document();
     writer.addDocument(doc);
     IndexReader r = IndexReader.open(dir, true);
Index: src/test/org/apache/lucene/index/TestStressIndexing2.java
===================================================================
--- src/test/org/apache/lucene/index/TestStressIndexing2.java	(revision 916567)
+++ src/test/org/apache/lucene/index/TestStressIndexing2.java	(working copy)
@@ -138,7 +138,7 @@
   
   public DocsAndWriter indexRandomIWReader(int nThreads, int iterations, int range, Directory dir) throws IOException, InterruptedException {
     Map<String,Document> docs = new HashMap<String,Document>();
-    IndexWriter w = new MockIndexWriter(dir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.UNLIMITED);
+    IndexWriter w = new MockIndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.UNLIMITED);
     w.setUseCompoundFile(false);
 
     /***
@@ -190,7 +190,7 @@
   public Map<String,Document> indexRandom(int nThreads, int iterations, int range, Directory dir) throws IOException, InterruptedException {
     Map<String,Document> docs = new HashMap<String,Document>();
     for(int iter=0;iter<3;iter++) {
-      IndexWriter w = new MockIndexWriter(dir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.UNLIMITED);
+      IndexWriter w = new MockIndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.UNLIMITED);
       w.setUseCompoundFile(false);
 
       // force many merges
@@ -234,7 +234,7 @@
 
   
   public static void indexSerial(Map<String,Document> docs, Directory dir) throws IOException {
-    IndexWriter w = new IndexWriter(dir, new WhitespaceAnalyzer(), IndexWriter.MaxFieldLength.UNLIMITED);
+    IndexWriter w = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), IndexWriter.MaxFieldLength.UNLIMITED);
 
     // index all docs in a single thread
     Iterator<Document> iter = docs.values().iterator();
Index: src/test/org/apache/lucene/index/TestWordlistLoader.java
===================================================================
--- src/test/org/apache/lucene/index/TestWordlistLoader.java	(revision 916567)
+++ src/test/org/apache/lucene/index/TestWordlistLoader.java	(working copy)
@@ -21,6 +21,7 @@
 import java.io.IOException;
 import java.io.StringReader;
 import java.util.HashSet;
+import java.util.Set;
 
 import org.apache.lucene.util.LuceneTestCase;
 
@@ -53,4 +54,28 @@
     assertFalse(wordset.contains("four"));
   }
 
+  /**
+   * Test stopwords in snowball format
+   */
+  public void testSnowballListLoading() throws IOException {
+    String s = 
+      "|comment\n" + // commented line
+      " |comment\n" + // commented line with leading whitespace
+      "\n" + // blank line
+      "  \t\n" + // line with only whitespace
+      " |comment | comment\n" + // commented line with comment
+      "ONE\n" + // stopword, in uppercase
+      "   two   \n" + // stopword with leading/trailing space
+      " three   four five \n" + // multiple stopwords
+      "six seven | comment\n"; //multiple stopwords + comment
+    Set<String> wordset = WordlistLoader.getSnowballWordSet(new StringReader(s));
+    assertEquals(7, wordset.size());
+    assertTrue(wordset.contains("ONE"));
+    assertTrue(wordset.contains("two"));
+    assertTrue(wordset.contains("three"));
+    assertTrue(wordset.contains("four"));
+    assertTrue(wordset.contains("five"));
+    assertTrue(wordset.contains("six"));
+    assertTrue(wordset.contains("seven"));
+  }
 }
Index: src/test/org/apache/lucene/index/TestTermVectorsReader.java
===================================================================
--- src/test/org/apache/lucene/index/TestTermVectorsReader.java	(revision 916567)
+++ src/test/org/apache/lucene/index/TestTermVectorsReader.java	(working copy)
@@ -138,6 +138,7 @@
         return false;
       else {
         final TestToken testToken = tokens[tokenUpto++];
+        clearAttributes();
         termAtt.setTermBuffer(testToken.text);
         offsetAtt.setOffset(testToken.startOffset, testToken.endOffset);
         if (tokenUpto > 1) {
Index: src/test/org/apache/lucene/index/TestSegmentTermDocs.java
===================================================================
--- src/test/org/apache/lucene/index/TestSegmentTermDocs.java	(revision 916567)
+++ src/test/org/apache/lucene/index/TestSegmentTermDocs.java	(working copy)
@@ -101,7 +101,7 @@
 
   public void testSkipTo(int indexDivisor) throws IOException {
     Directory dir = new RAMDirectory();
-    IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(), true, 
+    IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, 
                                          IndexWriter.MaxFieldLength.LIMITED);
     
     Term ta = new Term("content","aaa");
Index: src/test/org/apache/lucene/index/TestIndexWriter.java
===================================================================
--- src/test/org/apache/lucene/index/TestIndexWriter.java	(revision 916567)
+++ src/test/org/apache/lucene/index/TestIndexWriter.java	(working copy)
@@ -70,7 +70,6 @@
 import org.apache.lucene.store.SingleInstanceLockFactory;
 import org.apache.lucene.util.UnicodeUtil;
 import org.apache.lucene.util._TestUtil;
-import org.apache.lucene.util.Version;
 import org.apache.lucene.util.ThreadInterruptedException;
 import org.apache.lucene.util.BytesRef;
 
@@ -87,12 +86,16 @@
         IndexReader reader = null;
         int i;
 
-        IndexWriter.setDefaultWriteLockTimeout(2000);
-        assertEquals(2000, IndexWriter.getDefaultWriteLockTimeout());
+        long savedWriteLockTimeout = IndexWriter.getDefaultWriteLockTimeout();
+        try {
+          IndexWriter.setDefaultWriteLockTimeout(2000);
+          assertEquals(2000, IndexWriter.getDefaultWriteLockTimeout());
 
-        writer  = new IndexWriter(dir, new WhitespaceAnalyzer(), IndexWriter.MaxFieldLength.LIMITED);
+          writer  = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), IndexWriter.MaxFieldLength.LIMITED);
 
-        IndexWriter.setDefaultWriteLockTimeout(1000);
+        } finally {
+          IndexWriter.setDefaultWriteLockTimeout(savedWriteLockTimeout);
+        }
 
         // add 100 documents
         for (i = 0; i < 100; i++) {
@@ -109,7 +112,7 @@
         reader.close();
 
         // test doc count before segments are merged/index is optimized
-        writer = new IndexWriter(dir, new WhitespaceAnalyzer(), IndexWriter.MaxFieldLength.LIMITED);
+        writer = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), IndexWriter.MaxFieldLength.LIMITED);
         assertEquals(100, writer.maxDoc());
         writer.close();
 
@@ -119,7 +122,7 @@
         reader.close();
 
         // optimize the index and check that the new doc count is correct
-        writer = new IndexWriter(dir, new WhitespaceAnalyzer(), IndexWriter.MaxFieldLength.UNLIMITED);
+        writer = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), IndexWriter.MaxFieldLength.UNLIMITED);
         assertEquals(100, writer.maxDoc());
         assertEquals(60, writer.numDocs());
         writer.optimize();
@@ -135,7 +138,7 @@
 
         // make sure opening a new index for create over
         // this existing one works correctly:
-        writer = new IndexWriter(dir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
+        writer = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
         assertEquals(0, writer.maxDoc());
         assertEquals(0, writer.numDocs());
         writer.close();
@@ -178,7 +181,7 @@
       long inputDiskUsage = 0;
       for(int i=0;i<NUM_DIR;i++) {
         dirs[i] = new RAMDirectory();
-        IndexWriter writer  = new IndexWriter(dirs[i], new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
+        IndexWriter writer  = new IndexWriter(dirs[i], new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
         for(int j=0;j<25;j++) {
           addDocWithIndex(writer, 25*i+j);
         }
@@ -192,7 +195,7 @@
       // Now, build a starting index that has START_COUNT docs.  We
       // will then try to addIndexesNoOptimize into a copy of this:
       RAMDirectory startDir = new RAMDirectory();
-      IndexWriter writer = new IndexWriter(startDir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
+      IndexWriter writer = new IndexWriter(startDir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
       for(int j=0;j<START_COUNT;j++) {
         addDocWithIndex(writer, j);
       }
@@ -255,7 +258,7 @@
 
           // Make a new dir that will enforce disk usage:
           MockRAMDirectory dir = new MockRAMDirectory(startDir);
-          writer = new IndexWriter(dir, new WhitespaceAnalyzer(), false, IndexWriter.MaxFieldLength.UNLIMITED);
+          writer = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), false, IndexWriter.MaxFieldLength.UNLIMITED);
           IOException err = null;
 
           MergeScheduler ms = writer.getMergeScheduler();
@@ -467,7 +470,7 @@
             System.out.println("TEST: cycle: diskFree=" + diskFree);
           MockRAMDirectory dir = new MockRAMDirectory();
           dir.setMaxSizeInBytes(diskFree);
-          IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.UNLIMITED);
+          IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.UNLIMITED);
 
           MergeScheduler ms = writer.getMergeScheduler();
           if (ms instanceof ConcurrentMergeScheduler)
@@ -545,7 +548,7 @@
     */
     public void testWickedLongTerm() throws IOException {
       RAMDirectory dir = new RAMDirectory();
-      IndexWriter writer  = new IndexWriter(dir, new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
+      IndexWriter writer  = new IndexWriter(dir, new StandardAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
 
       char[] chars = new char[DocumentsWriter.MAX_TERM_LENGTH_UTF8];
       Arrays.fill(chars, 'x');
@@ -588,7 +591,7 @@
       // maximum length term, and search on that term:
       doc = new Document();
       doc.add(new Field("content", bigTerm, Field.Store.NO, Field.Index.ANALYZED));
-      StandardAnalyzer sa = new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT);
+      StandardAnalyzer sa = new StandardAnalyzer(TEST_VERSION_CURRENT);
       sa.setMaxTokenLength(100000);
       writer  = new IndexWriter(dir, sa, IndexWriter.MaxFieldLength.LIMITED);
       writer.addDocument(doc);
@@ -608,7 +611,7 @@
       doc.add(new Field("content", "aaa", Field.Store.YES, Field.Index.ANALYZED));
 
       for(int numDocs=38;numDocs<500;numDocs += 38) {
-        IndexWriter writer  = new IndexWriter(dir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
+        IndexWriter writer  = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
         LogDocMergePolicy ldmp = new LogDocMergePolicy(writer);
         ldmp.setMinMergeDocs(1);
         writer.setMergePolicy(ldmp);
@@ -622,7 +625,7 @@
         sis.read(dir);
         final int segCount = sis.size();
 
-        writer  = new IndexWriter(dir, new WhitespaceAnalyzer(), IndexWriter.MaxFieldLength.LIMITED);
+        writer  = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), IndexWriter.MaxFieldLength.LIMITED);
         writer.setMergePolicy(ldmp);
         writer.setMergeFactor(5);
         writer.optimize(3);
@@ -645,7 +648,7 @@
       final Document doc = new Document();
       doc.add(new Field("content", "aaa", Field.Store.YES, Field.Index.ANALYZED));
 
-      IndexWriter writer  = new IndexWriter(dir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
+      IndexWriter writer  = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
       LogDocMergePolicy ldmp = new LogDocMergePolicy(writer);
       ldmp.setMinMergeDocs(1);
       writer.setMergePolicy(ldmp);
@@ -687,7 +690,7 @@
     public void testOptimizeTempSpaceUsage() throws IOException {
     
       MockRAMDirectory dir = new MockRAMDirectory();
-      IndexWriter writer  = new IndexWriter(dir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
+      IndexWriter writer  = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
       for(int j=0;j<500;j++) {
         addDocWithIndex(writer, j);
       }
@@ -700,7 +703,7 @@
       }
 
       dir.resetMaxUsedSizeInBytes();
-      writer  = new IndexWriter(dir, new WhitespaceAnalyzer(), false, IndexWriter.MaxFieldLength.LIMITED);
+      writer  = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), false, IndexWriter.MaxFieldLength.LIMITED);
       writer.optimize();
       writer.close();
       long maxDiskUsage = dir.getMaxUsedSizeInBytes();
@@ -731,7 +734,7 @@
           Directory dir = FSDirectory.open(indexDir);
 
           // add one document & close writer
-          IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
+          IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
           addDoc(writer);
           writer.close();
 
@@ -740,7 +743,7 @@
           assertEquals("should be one document", reader.numDocs(), 1);
 
           // now open index for create:
-          writer = new IndexWriter(dir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
+          writer = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
           assertEquals("should be zero documents", writer.maxDoc(), 0);
           addDoc(writer);
           writer.close();
@@ -764,7 +767,7 @@
 
         IndexWriter writer = null;
 
-        writer  = new IndexWriter(dir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
+        writer  = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
 
         // add 100 documents
         for (int i = 0; i < 100; i++) {
@@ -802,7 +805,7 @@
         reader.close();
 
         try {
-          writer  = new IndexWriter(dir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
+          writer  = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
         } catch (Exception e) {
           fail("writer failed to open on a crashed index");
         }
@@ -824,7 +827,7 @@
 
         IndexWriter writer = null;
 
-        writer  = new IndexWriter(dir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
+        writer  = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
 
         // add 100 documents
         for (int i = 0; i < 100; i++) {
@@ -867,7 +870,7 @@
 
         IndexWriter writer = null;
 
-        writer  = new IndexWriter(dir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
+        writer  = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
         addDoc(writer);
 
         // close
@@ -889,7 +892,7 @@
 
         IndexWriter writer = null;
 
-        writer  = new IndexWriter(dir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
+        writer  = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
 
         // add 100 documents
         for (int i = 0; i < 100; i++) {
@@ -928,7 +931,7 @@
      */
     public void testCommitOnClose() throws IOException {
         Directory dir = new RAMDirectory();      
-        IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
+        IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
         for (int i = 0; i < 14; i++) {
           addDoc(writer);
         }
@@ -942,7 +945,7 @@
 
         IndexReader reader = IndexReader.open(dir, true);
 
-        writer = new IndexWriter(dir, new WhitespaceAnalyzer(), IndexWriter.MaxFieldLength.LIMITED);
+        writer = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), IndexWriter.MaxFieldLength.LIMITED);
         for(int i=0;i<3;i++) {
           for(int j=0;j<11;j++) {
             addDoc(writer);
@@ -974,7 +977,7 @@
      */
     public void testCommitOnCloseAbort() throws IOException {
       MockRAMDirectory dir = new MockRAMDirectory();      
-      IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
+      IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
       writer.setMaxBufferedDocs(10);
       for (int i = 0; i < 14; i++) {
         addDoc(writer);
@@ -987,7 +990,7 @@
       assertEquals("first number of hits", 14, hits.length);
       searcher.close();
 
-      writer = new IndexWriter(dir, new WhitespaceAnalyzer(), false, IndexWriter.MaxFieldLength.LIMITED);
+      writer = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), false, IndexWriter.MaxFieldLength.LIMITED);
       writer.setMaxBufferedDocs(10);
       for(int j=0;j<17;j++) {
         addDoc(writer);
@@ -1012,7 +1015,7 @@
           
       // Now make sure we can re-open the index, add docs,
       // and all is good:
-      writer = new IndexWriter(dir, new WhitespaceAnalyzer(), false, IndexWriter.MaxFieldLength.LIMITED);
+      writer = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), false, IndexWriter.MaxFieldLength.LIMITED);
       writer.setMaxBufferedDocs(10);
 
       // On abort, writer in fact may write to the same
@@ -1047,7 +1050,7 @@
      */
     public void testCommitOnCloseDiskUsage() throws IOException {
       MockRAMDirectory dir = new MockRAMDirectory();      
-      IndexWriter writer  = new IndexWriter(dir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
+      IndexWriter writer  = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
       for(int j=0;j<30;j++) {
         addDocWithIndex(writer, j);
       }
@@ -1055,7 +1058,7 @@
       dir.resetMaxUsedSizeInBytes();
 
       long startDiskUsage = dir.getMaxUsedSizeInBytes();
-      writer = new IndexWriter(dir, new WhitespaceAnalyzer(), false, IndexWriter.MaxFieldLength.LIMITED);
+      writer = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), false, IndexWriter.MaxFieldLength.LIMITED);
       writer.setMaxBufferedDocs(10);
       writer.setMergeScheduler(new SerialMergeScheduler());
       for(int j=0;j<1470;j++) {
@@ -1090,14 +1093,14 @@
      */
     public void testCommitOnCloseOptimize() throws IOException {
       RAMDirectory dir = new RAMDirectory();      
-      IndexWriter writer  = new IndexWriter(dir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
+      IndexWriter writer  = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
       writer.setMaxBufferedDocs(10);
       for(int j=0;j<17;j++) {
         addDocWithIndex(writer, j);
       }
       writer.close();
 
-      writer  = new IndexWriter(dir, new WhitespaceAnalyzer(), false, IndexWriter.MaxFieldLength.LIMITED);
+      writer  = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), false, IndexWriter.MaxFieldLength.LIMITED);
       writer.optimize();
 
       // Open a reader before closing (commiting) the writer:
@@ -1119,7 +1122,7 @@
       assertFalse("Reader incorrectly sees that the index is optimized", reader.isOptimized());
       reader.close();
 
-      writer  = new IndexWriter(dir, new WhitespaceAnalyzer(), false, IndexWriter.MaxFieldLength.LIMITED);
+      writer  = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), false, IndexWriter.MaxFieldLength.LIMITED);
       writer.optimize();
       writer.close();
       assertNoUnreferencedFiles(dir, "aborted writer after optimize");
@@ -1134,7 +1137,7 @@
 
     public void testIndexNoDocuments() throws IOException {
       RAMDirectory dir = new RAMDirectory();      
-      IndexWriter writer  = new IndexWriter(dir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
+      IndexWriter writer  = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
       writer.commit();
       writer.close();
 
@@ -1143,7 +1146,7 @@
       assertEquals(0, reader.numDocs());
       reader.close();
 
-      writer  = new IndexWriter(dir, new WhitespaceAnalyzer(), false, IndexWriter.MaxFieldLength.LIMITED);
+      writer  = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), false, IndexWriter.MaxFieldLength.LIMITED);
       writer.commit();
       writer.close();
 
@@ -1155,7 +1158,7 @@
 
     public void testManyFields() throws IOException {
       RAMDirectory dir = new RAMDirectory();      
-      IndexWriter writer  = new IndexWriter(dir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
+      IndexWriter writer  = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
       writer.setMaxBufferedDocs(10);
       for(int j=0;j<100;j++) {
         Document doc = new Document();
@@ -1186,7 +1189,7 @@
 
     public void testSmallRAMBuffer() throws IOException {
       RAMDirectory dir = new RAMDirectory();      
-      IndexWriter writer  = new IndexWriter(dir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
+      IndexWriter writer  = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
       writer.setRAMBufferSizeMB(0.000001);
       int lastNumFile = dir.listAll().length;
       for(int j=0;j<9;j++) {
@@ -1207,7 +1210,7 @@
     // maxBufferedDocs in a write session
     public void testChangingRAMBuffer() throws IOException {
       RAMDirectory dir = new RAMDirectory();      
-      IndexWriter writer  = new IndexWriter(dir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
+      IndexWriter writer  = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
       writer.setMaxBufferedDocs(10);
       writer.setRAMBufferSizeMB(IndexWriter.DISABLE_AUTO_FLUSH);
 
@@ -1261,7 +1264,7 @@
 
     public void testChangingRAMBuffer2() throws IOException {
       RAMDirectory dir = new RAMDirectory();      
-      IndexWriter writer  = new IndexWriter(dir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
+      IndexWriter writer  = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
       writer.setMaxBufferedDocs(10);
       writer.setMaxBufferedDeleteTerms(10);
       writer.setRAMBufferSizeMB(IndexWriter.DISABLE_AUTO_FLUSH);
@@ -1321,7 +1324,7 @@
 
     public void testDiverseDocs() throws IOException {
       RAMDirectory dir = new RAMDirectory();      
-      IndexWriter writer  = new IndexWriter(dir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
+      IndexWriter writer  = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
       writer.setRAMBufferSizeMB(0.5);
       Random rand = newRandom();
       for(int i=0;i<3;i++) {
@@ -1370,7 +1373,7 @@
 
     public void testEnablingNorms() throws IOException {
       RAMDirectory dir = new RAMDirectory();      
-      IndexWriter writer  = new IndexWriter(dir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
+      IndexWriter writer  = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
       writer.setMaxBufferedDocs(10);
       // Enable norms for only 1 doc, pre flush
       for(int j=0;j<10;j++) {
@@ -1391,7 +1394,7 @@
       assertEquals(10, hits.length);
       searcher.close();
 
-      writer  = new IndexWriter(dir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
+      writer  = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
       writer.setMaxBufferedDocs(10);
       // Enable norms for only 1 doc, post flush
       for(int j=0;j<27;j++) {
@@ -1417,7 +1420,7 @@
 
     public void testHighFreqTerm() throws IOException {
       RAMDirectory dir = new RAMDirectory();      
-      IndexWriter writer  = new IndexWriter(dir, new WhitespaceAnalyzer(), true, new IndexWriter.MaxFieldLength(100000000));
+      IndexWriter writer  = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, new IndexWriter.MaxFieldLength(100000000));
       writer.setRAMBufferSizeMB(0.01);
       // Massive doc that has 128 K a's
       StringBuilder b = new StringBuilder(1024*1024);
@@ -1464,7 +1467,7 @@
       }
       
       Directory dir = new MyRAMDirectory();
-      IndexWriter writer  = new IndexWriter(dir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
+      IndexWriter writer  = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
       for (int i = 0; i < 100; i++) {
         addDoc(writer);
       }
@@ -1475,7 +1478,7 @@
       assertEquals("did not get right number of hits", 100, hits.length);
       writer.close();
 
-      writer  = new IndexWriter(dir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
+      writer  = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
       writer.close();
 
       dir.close();
@@ -1483,7 +1486,7 @@
 
     public void testFlushWithNoMerging() throws IOException {
       Directory dir = new RAMDirectory();
-      IndexWriter writer  = new IndexWriter(dir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
+      IndexWriter writer  = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
       writer.setMaxBufferedDocs(2);
       Document doc = new Document();
       doc.add(new Field("field", "aaa", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));
@@ -1502,7 +1505,7 @@
     // empty doc (no norms) and flush
     public void testEmptyDocAfterFlushingRealDoc() throws IOException {
       Directory dir = new RAMDirectory();
-      IndexWriter writer  = new IndexWriter(dir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
+      IndexWriter writer  = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
       Document doc = new Document();
       doc.add(new Field("field", "aaa", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));
       writer.addDocument(doc);
@@ -1521,7 +1524,7 @@
 
       Directory dir = new MockRAMDirectory();
       for(int pass=0;pass<2;pass++) {
-        IndexWriter writer  = new IndexWriter(dir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
+        IndexWriter writer  = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
         writer.setMergeScheduler(new ConcurrentMergeScheduler());
         Document doc = new Document();
         doc.add(new Field("field", "aaa", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));
@@ -1575,7 +1578,7 @@
    */
   public void testBadSegment() throws IOException {
     MockRAMDirectory dir = new MockRAMDirectory();
-    IndexWriter ir = new IndexWriter(dir, new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
+    IndexWriter ir = new IndexWriter(dir, new StandardAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
     
     Document document = new Document();
     document.add(new Field("tvtest", "", Field.Store.NO, Field.Index.ANALYZED,
@@ -1588,7 +1591,7 @@
   // LUCENE-1008
   public void testNoTermVectorAfterTermVector() throws IOException {
     MockRAMDirectory dir = new MockRAMDirectory();
-    IndexWriter iw = new IndexWriter(dir, new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
+    IndexWriter iw = new IndexWriter(dir, new StandardAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
     Document document = new Document();
     document.add(new Field("tvtest", "a b c", Field.Store.NO, Field.Index.ANALYZED,
         Field.TermVector.YES));
@@ -1614,7 +1617,7 @@
   // LUCENE-1010
   public void testNoTermVectorAfterTermVectorMerge() throws IOException {
     MockRAMDirectory dir = new MockRAMDirectory();
-    IndexWriter iw = new IndexWriter(dir, new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
+    IndexWriter iw = new IndexWriter(dir, new StandardAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
     Document document = new Document();
     document.add(new Field("tvtest", "a b c", Field.Store.NO, Field.Index.ANALYZED,
         Field.TermVector.YES));
@@ -1646,7 +1649,7 @@
     int pri = Thread.currentThread().getPriority();
     try {
       MockRAMDirectory dir = new MockRAMDirectory();
-      IndexWriter iw = new IndexWriter(dir, new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
+      IndexWriter iw = new IndexWriter(dir, new StandardAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
       Document document = new Document();
       document.add(new Field("tvtest", "a b c", Field.Store.NO, Field.Index.ANALYZED,
                              Field.TermVector.YES));
@@ -1686,7 +1689,7 @@
   // LUCENE-1013
   public void testSetMaxMergeDocs() throws IOException {
     MockRAMDirectory dir = new MockRAMDirectory();
-    IndexWriter iw = new IndexWriter(dir, new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
+    IndexWriter iw = new IndexWriter(dir, new StandardAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
     iw.setMergeScheduler(new MyMergeScheduler());
     iw.setMaxMergeDocs(20);
     iw.setMaxBufferedDocs(2);
@@ -1706,7 +1709,7 @@
 
       @Override
       public TokenStream tokenStream(String fieldName, Reader reader) {
-        return new TokenFilter(new StandardTokenizer(Version.LUCENE_CURRENT, reader)) {
+        return new TokenFilter(new StandardTokenizer(TEST_VERSION_CURRENT, reader)) {
           private int count = 0;
 
           @Override
@@ -1804,7 +1807,7 @@
     failure.setDoFail();
     dir.failOn(failure);
 
-    IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(), IndexWriter.MaxFieldLength.LIMITED);
+    IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), IndexWriter.MaxFieldLength.LIMITED);
     writer.setMaxBufferedDocs(2);
     Document doc = new Document();
     String contents = "aa bb cc dd ee ff gg hh ii jj kk";
@@ -1854,7 +1857,7 @@
     Analyzer analyzer = new Analyzer() {
       @Override
       public TokenStream tokenStream(String fieldName, Reader reader) {
-        return new CrashingFilter(fieldName, new WhitespaceTokenizer(reader));
+        return new CrashingFilter(fieldName, new WhitespaceTokenizer(TEST_VERSION_CURRENT, reader));
       }
     };
 
@@ -1937,7 +1940,7 @@
     Analyzer analyzer = new Analyzer() {
       @Override
       public TokenStream tokenStream(String fieldName, Reader reader) {
-        return new CrashingFilter(fieldName, new WhitespaceTokenizer(reader));
+        return new CrashingFilter(fieldName, new WhitespaceTokenizer(TEST_VERSION_CURRENT, reader));
       }
     };
 
@@ -2051,7 +2054,7 @@
     MockRAMDirectory dir = new MockRAMDirectory();
     int delID = 0;
     for(int i=0;i<20;i++) {
-      IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(), IndexWriter.MaxFieldLength.LIMITED);
+      IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), IndexWriter.MaxFieldLength.LIMITED);
       writer.setMaxBufferedDocs(2);
       writer.setMergeFactor(2);
       writer.setUseCompoundFile(false);
@@ -2087,7 +2090,7 @@
       reader.close();
 
       if (0 == i % 4) {
-        writer = new IndexWriter(dir, new WhitespaceAnalyzer(), IndexWriter.MaxFieldLength.LIMITED);
+        writer = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), IndexWriter.MaxFieldLength.LIMITED);
         writer.setUseCompoundFile(false);
         writer.optimize();
         writer.close();
@@ -2104,7 +2107,7 @@
 
     for(int pass=0;pass<2;pass++) {
 
-      IndexWriter writer = new IndexWriter(directory, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.UNLIMITED);
+      IndexWriter writer = new IndexWriter(directory, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.UNLIMITED);
 
       //System.out.println("TEST: pass=" + pass + " cms=" + (pass >= 2));
       for(int iter=0;iter<10;iter++) {
@@ -2176,7 +2179,7 @@
         reader.close();
 
         // Reopen
-        writer = new IndexWriter(directory, new WhitespaceAnalyzer(), false, IndexWriter.MaxFieldLength.UNLIMITED);
+        writer = new IndexWriter(directory, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), false, IndexWriter.MaxFieldLength.UNLIMITED);
       }
       writer.close();
     }
@@ -2256,7 +2259,7 @@
 
     for(int iter=0;iter<7;iter++) {
       MockRAMDirectory dir = new MockRAMDirectory();
-      IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(), IndexWriter.MaxFieldLength.LIMITED);
+      IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), IndexWriter.MaxFieldLength.LIMITED);
       ConcurrentMergeScheduler cms = new ConcurrentMergeScheduler();
 
       // We expect AlreadyClosedException
@@ -2315,7 +2318,7 @@
   // OK:
   public void testImmediateDiskFull() throws IOException {
     MockRAMDirectory dir = new MockRAMDirectory();
-    IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(), IndexWriter.MaxFieldLength.LIMITED);
+    IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), IndexWriter.MaxFieldLength.LIMITED);
     dir.setMaxSizeInBytes(dir.getRecomputedActualSizeInBytes());
     writer.setMaxBufferedDocs(2);
     final Document doc = new Document();
@@ -2353,7 +2356,7 @@
 
     for(int iter=0;iter<10;iter++) {
       MockRAMDirectory dir = new MockRAMDirectory();
-      IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(), IndexWriter.MaxFieldLength.UNLIMITED);
+      IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), IndexWriter.MaxFieldLength.UNLIMITED);
       ConcurrentMergeScheduler cms = new ConcurrentMergeScheduler();
       // We expect disk full exceptions in the merge threads
       cms.setSuppressExceptions();
@@ -2414,7 +2417,7 @@
   public void _testSingleThreadFailure(MockRAMDirectory.Failure failure) throws IOException {
     MockRAMDirectory dir = new MockRAMDirectory();
 
-    IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(), IndexWriter.MaxFieldLength.UNLIMITED);
+    IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), IndexWriter.MaxFieldLength.UNLIMITED);
     writer.setMaxBufferedDocs(2);
     final Document doc = new Document();
     doc.add(new Field("field", "aaa bbb ccc ddd eee fff ggg hhh iii jjj", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));
@@ -2444,7 +2447,7 @@
 
     for(int iter=0;iter<2;iter++) {
       MockRAMDirectory dir = new MockRAMDirectory();
-      IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(), IndexWriter.MaxFieldLength.LIMITED);
+      IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), IndexWriter.MaxFieldLength.LIMITED);
       ConcurrentMergeScheduler cms = new ConcurrentMergeScheduler();
       // We expect disk full exceptions in the merge threads
       cms.setSuppressExceptions();
@@ -2604,7 +2607,7 @@
   public void testUnlimitedMaxFieldLength() throws IOException {
     Directory dir = new MockRAMDirectory();
 
-    IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(), IndexWriter.MaxFieldLength.UNLIMITED);
+    IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), IndexWriter.MaxFieldLength.UNLIMITED);
 
     Document doc = new Document();
     StringBuilder b = new StringBuilder();
@@ -2628,7 +2631,7 @@
 
     IndexWriter writer = null;
 
-    writer  = new IndexWriter(dir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
+    writer  = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
 
     // add 100 documents
     for (int i = 0; i < 100; i++) {
@@ -2664,7 +2667,7 @@
   public void testForceCommit() throws IOException {
     Directory dir = new MockRAMDirectory();
 
-    IndexWriter writer  = new IndexWriter(dir, new WhitespaceAnalyzer(), IndexWriter.MaxFieldLength.LIMITED);
+    IndexWriter writer  = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), IndexWriter.MaxFieldLength.LIMITED);
     writer.setMaxBufferedDocs(2);
     writer.setMergeFactor(5);
 
@@ -2718,7 +2721,7 @@
     FailOnlyInSync failure = new FailOnlyInSync();
     dir.failOn(failure);
 
-    IndexWriter writer  = new IndexWriter(dir, new WhitespaceAnalyzer(), IndexWriter.MaxFieldLength.UNLIMITED);
+    IndexWriter writer  = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), IndexWriter.MaxFieldLength.UNLIMITED);
     failure.setDoFail();
 
     ConcurrentMergeScheduler cms = new ConcurrentMergeScheduler();
@@ -2754,7 +2757,7 @@
     Directory dir = new MockRAMDirectory();
     for(int iter=0;iter<2;iter++) {
       IndexWriter writer = new IndexWriter(dir,
-                                           new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT), IndexWriter.MaxFieldLength.UNLIMITED);
+                                           new StandardAnalyzer(TEST_VERSION_CURRENT), IndexWriter.MaxFieldLength.UNLIMITED);
       writer.setMaxBufferedDocs(2);
       writer.setRAMBufferSizeMB(IndexWriter.DISABLE_AUTO_FLUSH);
       writer.setMergeScheduler(new SerialMergeScheduler());
@@ -2787,7 +2790,7 @@
       reader.close();
 
       writer = new IndexWriter(dir,
-                               new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT), IndexWriter.MaxFieldLength.UNLIMITED);
+                               new StandardAnalyzer(TEST_VERSION_CURRENT), IndexWriter.MaxFieldLength.UNLIMITED);
       writer.setMaxBufferedDocs(2);
       writer.setRAMBufferSizeMB(IndexWriter.DISABLE_AUTO_FLUSH);
       writer.setMergeScheduler(new SerialMergeScheduler());
@@ -2806,7 +2809,7 @@
     Directory dir = new MockRAMDirectory();
     for(int iter=0;iter<2;iter++) {
       IndexWriter writer = new IndexWriter(dir,
-                                           new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT), IndexWriter.MaxFieldLength.UNLIMITED);
+                                           new StandardAnalyzer(TEST_VERSION_CURRENT), IndexWriter.MaxFieldLength.UNLIMITED);
       writer.setMaxBufferedDocs(2);
       writer.setRAMBufferSizeMB(IndexWriter.DISABLE_AUTO_FLUSH);
       writer.setMergeScheduler(new SerialMergeScheduler());
@@ -2843,7 +2846,7 @@
   public void testTermVectorCorruption3() throws IOException {
     Directory dir = new MockRAMDirectory();
     IndexWriter writer = new IndexWriter(dir,
-                                         new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT),
+                                         new StandardAnalyzer(TEST_VERSION_CURRENT),
                                          IndexWriter.MaxFieldLength.LIMITED);
     writer.setMaxBufferedDocs(2);
     writer.setRAMBufferSizeMB(IndexWriter.DISABLE_AUTO_FLUSH);
@@ -2865,7 +2868,7 @@
     writer.close();
 
     writer = new IndexWriter(dir,
-                             new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT),
+                             new StandardAnalyzer(TEST_VERSION_CURRENT),
                              IndexWriter.MaxFieldLength.LIMITED);
     writer.setMaxBufferedDocs(2);
     writer.setRAMBufferSizeMB(IndexWriter.DISABLE_AUTO_FLUSH);
@@ -2890,7 +2893,7 @@
   public void testUserSpecifiedMaxFieldLength() throws IOException {
     Directory dir = new MockRAMDirectory();
 
-    IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(), new IndexWriter.MaxFieldLength(100000));
+    IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), new IndexWriter.MaxFieldLength(100000));
 
     Document doc = new Document();
     StringBuilder b = new StringBuilder();
@@ -2913,7 +2916,7 @@
   public void testExpungeDeletes() throws IOException {
     Directory dir = new MockRAMDirectory();
     IndexWriter writer = new IndexWriter(dir,
-                                         new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT),
+                                         new StandardAnalyzer(TEST_VERSION_CURRENT),
                                          IndexWriter.MaxFieldLength.LIMITED);
     writer.setMaxBufferedDocs(2);
     writer.setRAMBufferSizeMB(IndexWriter.DISABLE_AUTO_FLUSH);
@@ -2941,7 +2944,7 @@
     ir.close();
 
     writer = new IndexWriter(dir,
-                             new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT),
+                             new StandardAnalyzer(TEST_VERSION_CURRENT),
                              IndexWriter.MaxFieldLength.LIMITED);
     assertEquals(8, writer.numDocs());
     assertEquals(10, writer.maxDoc());
@@ -2959,7 +2962,7 @@
   public void testExpungeDeletes2() throws IOException {
     Directory dir = new MockRAMDirectory();
     IndexWriter writer = new IndexWriter(dir,
-                                         new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT),
+                                         new StandardAnalyzer(TEST_VERSION_CURRENT),
                                          IndexWriter.MaxFieldLength.LIMITED);
     writer.setMaxBufferedDocs(2);
     writer.setMergeFactor(50);
@@ -2988,7 +2991,7 @@
     ir.close();
 
     writer = new IndexWriter(dir,
-                             new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT),
+                             new StandardAnalyzer(TEST_VERSION_CURRENT),
                              IndexWriter.MaxFieldLength.LIMITED);
     writer.setMergeFactor(3);
     assertEquals(49, writer.numDocs());
@@ -3006,7 +3009,7 @@
   public void testExpungeDeletes3() throws IOException {
     Directory dir = new MockRAMDirectory();
     IndexWriter writer = new IndexWriter(dir,
-                                         new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT),
+                                         new StandardAnalyzer(TEST_VERSION_CURRENT),
                                          IndexWriter.MaxFieldLength.LIMITED);
     writer.setMaxBufferedDocs(2);
     writer.setMergeFactor(50);
@@ -3035,7 +3038,7 @@
     ir.close();
 
     writer = new IndexWriter(dir,
-                             new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT),
+                             new StandardAnalyzer(TEST_VERSION_CURRENT),
                              IndexWriter.MaxFieldLength.LIMITED);
     // Force many merges to happen
     writer.setMergeFactor(3);
@@ -3051,7 +3054,7 @@
   // LUCENE-1179
   public void testEmptyFieldName() throws IOException {
     MockRAMDirectory dir = new MockRAMDirectory();
-    IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(), IndexWriter.MaxFieldLength.UNLIMITED);
+    IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), IndexWriter.MaxFieldLength.UNLIMITED);
     Document doc = new Document();
     doc.add(new Field("", "a b c", Field.Store.NO, Field.Index.ANALYZED));
     writer.addDocument(doc);
@@ -3077,7 +3080,7 @@
 
   public void testExceptionDocumentsWriterInit() throws IOException {
     MockRAMDirectory dir = new MockRAMDirectory();
-    MockIndexWriter w = new MockIndexWriter(dir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.UNLIMITED);
+    MockIndexWriter w = new MockIndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.UNLIMITED);
     Document doc = new Document();
     doc.add(new Field("field", "a field", Field.Store.YES,
                       Field.Index.ANALYZED));
@@ -3097,7 +3100,7 @@
   // LUCENE-1208
   public void testExceptionJustBeforeFlush() throws IOException {
     MockRAMDirectory dir = new MockRAMDirectory();
-    MockIndexWriter w = new MockIndexWriter(dir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.UNLIMITED);
+    MockIndexWriter w = new MockIndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.UNLIMITED);
     w.setMaxBufferedDocs(2);
     Document doc = new Document();
     doc.add(new Field("field", "a field", Field.Store.YES,
@@ -3107,7 +3110,7 @@
     Analyzer analyzer = new Analyzer() {
       @Override
       public TokenStream tokenStream(String fieldName, Reader reader) {
-        return new CrashingFilter(fieldName, new WhitespaceTokenizer(reader));
+        return new CrashingFilter(fieldName, new WhitespaceTokenizer(TEST_VERSION_CURRENT, reader));
       }
     };
 
@@ -3147,7 +3150,7 @@
   // LUCENE-1210
   public void testExceptionOnMergeInit() throws IOException {
     MockRAMDirectory dir = new MockRAMDirectory();
-    MockIndexWriter2 w = new MockIndexWriter2(dir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.UNLIMITED);
+    MockIndexWriter2 w = new MockIndexWriter2(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.UNLIMITED);
     w.setMaxBufferedDocs(2);
     w.setMergeFactor(2);
     w.doFail = true;
@@ -3174,28 +3177,37 @@
       super(dir, a, create, mfl);
     }
 
-    boolean wasCalled;
+    boolean afterWasCalled;
+    boolean beforeWasCalled;
 
     @Override
     public void doAfterFlush() {
-      wasCalled = true;
+      afterWasCalled = true;
     }
+    
+    @Override
+    protected void doBeforeFlush() throws IOException {
+      beforeWasCalled = true;
+    }
   }
 
   // LUCENE-1222
-  public void testDoAfterFlush() throws IOException {
+  public void testDoBeforeAfterFlush() throws IOException {
     MockRAMDirectory dir = new MockRAMDirectory();
-    MockIndexWriter3 w = new MockIndexWriter3(dir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
+    MockIndexWriter3 w = new MockIndexWriter3(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
     Document doc = new Document();
     doc.add(new Field("field", "a field", Field.Store.YES,
                       Field.Index.ANALYZED));
     w.addDocument(doc);
     w.commit();
-    assertTrue(w.wasCalled);
-    w.wasCalled = true;
+    assertTrue(w.beforeWasCalled);
+    assertTrue(w.afterWasCalled);
+    w.beforeWasCalled = false;
+    w.afterWasCalled = false;
     w.deleteDocuments(new Term("field", "field"));
     w.commit();
-    assertTrue(w.wasCalled);
+    assertTrue(w.beforeWasCalled);
+    assertTrue(w.afterWasCalled);
     w.close();
 
     IndexReader ir = IndexReader.open(dir, true);
@@ -3238,7 +3250,7 @@
   public void testExceptionsDuringCommit() throws Throwable {
     MockRAMDirectory dir = new MockRAMDirectory();
     FailOnlyInCommit failure = new FailOnlyInCommit();
-    IndexWriter w = new IndexWriter(dir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.UNLIMITED);
+    IndexWriter w = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.UNLIMITED);
     Document doc = new Document();
     doc.add(new Field("field", "a field", Field.Store.YES,
                       Field.Index.ANALYZED));
@@ -3286,7 +3298,7 @@
   // LUCENE-510
   public void testInvalidUTF16() throws Throwable {
     MockRAMDirectory dir = new MockRAMDirectory();
-    IndexWriter w = new IndexWriter(dir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.UNLIMITED);
+    IndexWriter w = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.UNLIMITED);
     Document doc = new Document();
 
     final int count = utf8Data.length/2;
@@ -3499,7 +3511,7 @@
     };
 
     MockRAMDirectory dir = new MockRAMDirectory();
-    IndexWriter w = new IndexWriter(dir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.UNLIMITED);
+    IndexWriter w = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.UNLIMITED);
     Document doc = new Document();
     doc.add(new Field("field", tokens));
     w.addDocument(doc);
@@ -3531,7 +3543,7 @@
   public void testPrepareCommit() throws IOException {
     Directory dir = new MockRAMDirectory();
 
-    IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(), IndexWriter.MaxFieldLength.LIMITED);
+    IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), IndexWriter.MaxFieldLength.LIMITED);
     writer.setMaxBufferedDocs(2);
     writer.setMergeFactor(5);
 
@@ -3583,7 +3595,7 @@
     MockRAMDirectory dir = new MockRAMDirectory();
     dir.setPreventDoubleWrite(false);
 
-    IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(), IndexWriter.MaxFieldLength.LIMITED);
+    IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), IndexWriter.MaxFieldLength.LIMITED);
 
     writer.setMaxBufferedDocs(2);
     writer.setMergeFactor(5);
@@ -3608,7 +3620,7 @@
     reader.close();
     reader2.close();
 
-    writer = new IndexWriter(dir, new WhitespaceAnalyzer(), IndexWriter.MaxFieldLength.LIMITED);
+    writer = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), IndexWriter.MaxFieldLength.LIMITED);
     for (int i = 0; i < 17; i++)
       addDoc(writer);
 
@@ -3636,7 +3648,7 @@
   public void testPrepareCommitNoChanges() throws IOException {
     MockRAMDirectory dir = new MockRAMDirectory();
 
-    IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(), IndexWriter.MaxFieldLength.LIMITED);
+    IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), IndexWriter.MaxFieldLength.LIMITED);
     writer.prepareCommit();
     writer.commit();
     writer.close();
@@ -3663,14 +3675,14 @@
     public RunAddIndexesThreads(int numCopy) throws Throwable {
       NUM_COPY = numCopy;
       dir = new MockRAMDirectory();
-      IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(), IndexWriter.MaxFieldLength.LIMITED);
+      IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), IndexWriter.MaxFieldLength.LIMITED);
       writer.setMaxBufferedDocs(2);
       for (int i = 0; i < NUM_INIT_DOCS; i++)
         addDoc(writer);
       writer.close();
 
       dir2 = new MockRAMDirectory();
-      writer2 = new IndexWriter(dir2, new WhitespaceAnalyzer(), IndexWriter.MaxFieldLength.LIMITED);
+      writer2 = new IndexWriter(dir2, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), IndexWriter.MaxFieldLength.LIMITED);
       cms = (ConcurrentMergeScheduler) writer2.getMergeScheduler();
 
       readers = new IndexReader[NUM_COPY];
@@ -3935,7 +3947,7 @@
   // LUCENE-1347
   public void testRollbackExceptionHang() throws Throwable {
     MockRAMDirectory dir = new MockRAMDirectory();
-    MockIndexWriter4 w = new MockIndexWriter4(dir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
+    MockIndexWriter4 w = new MockIndexWriter4(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
 
     addDoc(w);
     w.doFail = true;
@@ -3954,7 +3966,7 @@
   // LUCENE-1219
   public void testBinaryFieldOffsetLength() throws IOException {
     MockRAMDirectory dir = new MockRAMDirectory();
-    IndexWriter w = new IndexWriter(dir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.UNLIMITED);
+    IndexWriter w = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.UNLIMITED);
     byte[] b = new byte[50];
     for(int i=0;i<50;i++)
       b[i] = (byte) (i+77);
@@ -3984,7 +3996,7 @@
   // LUCENE-1382
   public void testCommitUserData() throws IOException {
     Directory dir = new MockRAMDirectory();
-    IndexWriter w = new IndexWriter(dir, new WhitespaceAnalyzer(), IndexWriter.MaxFieldLength.LIMITED);
+    IndexWriter w = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), IndexWriter.MaxFieldLength.LIMITED);
     w.setMaxBufferedDocs(2);
     for(int j=0;j<17;j++)
       addDoc(w);
@@ -3997,7 +4009,7 @@
     assertEquals(0, r.getCommitUserData().size());
     r.close();
       
-    w = new IndexWriter(dir, new WhitespaceAnalyzer(), IndexWriter.MaxFieldLength.LIMITED);
+    w = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), IndexWriter.MaxFieldLength.LIMITED);
     w.setMaxBufferedDocs(2);
     for(int j=0;j<17;j++)
       addDoc(w);
@@ -4012,7 +4024,7 @@
     assertEquals("test1", r.getCommitUserData().get("label"));
     r.close();
 
-    w = new IndexWriter(dir, new WhitespaceAnalyzer(), IndexWriter.MaxFieldLength.LIMITED);
+    w = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), IndexWriter.MaxFieldLength.LIMITED);
     w.optimize();
     w.close();
 
@@ -4023,7 +4035,7 @@
 
   public void testOptimizeExceptions() throws IOException {
     RAMDirectory startDir = new MockRAMDirectory();
-    IndexWriter w = new IndexWriter(startDir, new WhitespaceAnalyzer(), IndexWriter.MaxFieldLength.UNLIMITED);    
+    IndexWriter w = new IndexWriter(startDir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), IndexWriter.MaxFieldLength.UNLIMITED);    
     w.setMaxBufferedDocs(2);
     w.setMergeFactor(100);
     for(int i=0;i<27;i++)
@@ -4032,7 +4044,7 @@
 
     for(int i=0;i<200;i++) {
       MockRAMDirectory dir = new MockRAMDirectory(startDir);
-      w = new IndexWriter(dir, new WhitespaceAnalyzer(), IndexWriter.MaxFieldLength.UNLIMITED);
+      w = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), IndexWriter.MaxFieldLength.UNLIMITED);
       ((ConcurrentMergeScheduler) w.getMergeScheduler()).setSuppressExceptions();
       dir.setRandomIOExceptionRate(0.5, 100);
       try {
@@ -4051,7 +4063,7 @@
 
     final List<Throwable> thrown = new ArrayList<Throwable>();
 
-    final IndexWriter writer = new IndexWriter(new MockRAMDirectory(), new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT), IndexWriter.MaxFieldLength.UNLIMITED) {
+    final IndexWriter writer = new IndexWriter(new MockRAMDirectory(), new StandardAnalyzer(TEST_VERSION_CURRENT), IndexWriter.MaxFieldLength.UNLIMITED) {
         @Override
         public void message(final String message) {
           if (message.startsWith("now flush at close") && 0 == thrown.size()) {
@@ -4076,7 +4088,7 @@
   // LUCENE-1442
   public void testDoubleOffsetCounting() throws Exception {
     MockRAMDirectory dir = new MockRAMDirectory();
-    IndexWriter w = new IndexWriter(dir, new WhitespaceAnalyzer(), IndexWriter.MaxFieldLength.LIMITED);
+    IndexWriter w = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), IndexWriter.MaxFieldLength.LIMITED);
     Document doc = new Document();
     Field f = new Field("field", "abcd", Field.Store.NO, Field.Index.NOT_ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS);
     doc.add(f);
@@ -4111,7 +4123,7 @@
   // LUCENE-1442
   public void testDoubleOffsetCounting2() throws Exception {
     MockRAMDirectory dir = new MockRAMDirectory();
-    IndexWriter w = new IndexWriter(dir, new SimpleAnalyzer(), IndexWriter.MaxFieldLength.LIMITED);
+    IndexWriter w = new IndexWriter(dir, new SimpleAnalyzer(TEST_VERSION_CURRENT), IndexWriter.MaxFieldLength.LIMITED);
     Document doc = new Document();
     Field f = new Field("field", "abcd", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS);
     doc.add(f);
@@ -4133,7 +4145,7 @@
   // LUCENE-1448
   public void testEndOffsetPositionCharAnalyzer() throws Exception {
     MockRAMDirectory dir = new MockRAMDirectory();
-    IndexWriter w = new IndexWriter(dir, new WhitespaceAnalyzer(), IndexWriter.MaxFieldLength.LIMITED);
+    IndexWriter w = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), IndexWriter.MaxFieldLength.LIMITED);
     Document doc = new Document();
     Field f = new Field("field", "abcd   ", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS);
     doc.add(f);
@@ -4155,7 +4167,7 @@
   // LUCENE-1448
   public void testEndOffsetPositionWithCachingTokenFilter() throws Exception {
     MockRAMDirectory dir = new MockRAMDirectory();
-    Analyzer analyzer = new WhitespaceAnalyzer();
+    Analyzer analyzer = new WhitespaceAnalyzer(TEST_VERSION_CURRENT);
     IndexWriter w = new IndexWriter(dir, analyzer, IndexWriter.MaxFieldLength.LIMITED);
     Document doc = new Document();
     TokenStream stream = new CachingTokenFilter(analyzer.tokenStream("field", new StringReader("abcd   ")));
@@ -4179,7 +4191,7 @@
   // LUCENE-1448
   public void testEndOffsetPositionWithTeeSinkTokenFilter() throws Exception {
     MockRAMDirectory dir = new MockRAMDirectory();
-    Analyzer analyzer = new WhitespaceAnalyzer();
+    Analyzer analyzer = new WhitespaceAnalyzer(TEST_VERSION_CURRENT);
     IndexWriter w = new IndexWriter(dir, analyzer, IndexWriter.MaxFieldLength.LIMITED);
     Document doc = new Document();
     TeeSinkTokenFilter tee = new TeeSinkTokenFilter(analyzer.tokenStream("field", new StringReader("abcd   ")));
@@ -4205,7 +4217,7 @@
   // LUCENE-1448
   public void testEndOffsetPositionStopFilter() throws Exception {
     MockRAMDirectory dir = new MockRAMDirectory();
-    IndexWriter w = new IndexWriter(dir, new StopAnalyzer(Version.LUCENE_CURRENT), IndexWriter.MaxFieldLength.LIMITED);
+    IndexWriter w = new IndexWriter(dir, new StopAnalyzer(TEST_VERSION_CURRENT), IndexWriter.MaxFieldLength.LIMITED);
     Document doc = new Document();
     Field f = new Field("field", "abcd the", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS);
     doc.add(f);
@@ -4227,7 +4239,7 @@
   // LUCENE-1448
   public void testEndOffsetPositionStandard() throws Exception {
     MockRAMDirectory dir = new MockRAMDirectory();
-    IndexWriter w = new IndexWriter(dir, new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT), IndexWriter.MaxFieldLength.LIMITED);
+    IndexWriter w = new IndexWriter(dir, new StandardAnalyzer(TEST_VERSION_CURRENT), IndexWriter.MaxFieldLength.LIMITED);
     Document doc = new Document();
     Field f = new Field("field", "abcd the  ", Field.Store.NO,
         Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS);
@@ -4257,7 +4269,7 @@
   // LUCENE-1448
   public void testEndOffsetPositionStandardEmptyField() throws Exception {
     MockRAMDirectory dir = new MockRAMDirectory();
-    IndexWriter w = new IndexWriter(dir, new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT), IndexWriter.MaxFieldLength.LIMITED);
+    IndexWriter w = new IndexWriter(dir, new StandardAnalyzer(TEST_VERSION_CURRENT), IndexWriter.MaxFieldLength.LIMITED);
     Document doc = new Document();
     Field f = new Field("field", "", Field.Store.NO,
                         Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS);
@@ -4284,7 +4296,7 @@
   // LUCENE-1448
   public void testEndOffsetPositionStandardEmptyField2() throws Exception {
     MockRAMDirectory dir = new MockRAMDirectory();
-    IndexWriter w = new IndexWriter(dir, new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT), IndexWriter.MaxFieldLength.LIMITED);
+    IndexWriter w = new IndexWriter(dir, new StandardAnalyzer(TEST_VERSION_CURRENT), IndexWriter.MaxFieldLength.LIMITED);
     Document doc = new Document();
 
     Field f = new Field("field", "abcd", Field.Store.NO,
@@ -4326,7 +4338,7 @@
       out.writeByte((byte) 42);
       out.close();
 
-      new IndexWriter(dir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED).close();
+      new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED).close();
 
       assertTrue(dir.fileExists("myrandomfile"));
 
@@ -4344,7 +4356,7 @@
 
   public void testDeadlock() throws Exception {
     MockRAMDirectory dir = new MockRAMDirectory();
-    IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(), IndexWriter.MaxFieldLength.UNLIMITED);
+    IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), IndexWriter.MaxFieldLength.UNLIMITED);
     writer.setMaxBufferedDocs(2);
     Document doc = new Document();
     doc.add(new Field("content", "aaa bbb ccc ddd eee fff ggg hhh iii", Field.Store.YES,
@@ -4356,7 +4368,7 @@
     // index has 2 segments
 
     MockRAMDirectory dir2 = new MockRAMDirectory();
-    IndexWriter writer2 = new IndexWriter(dir2, new WhitespaceAnalyzer(), IndexWriter.MaxFieldLength.LIMITED);
+    IndexWriter writer2 = new IndexWriter(dir2, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), IndexWriter.MaxFieldLength.LIMITED);
     writer2.addDocument(doc);
     writer2.close();
 
@@ -4394,7 +4406,7 @@
             if (w != null) {
               w.close();
             }
-            w = new IndexWriter(dir, new WhitespaceAnalyzer(), IndexWriter.MaxFieldLength.UNLIMITED);
+            w = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), IndexWriter.MaxFieldLength.UNLIMITED);
 
             //((ConcurrentMergeScheduler) w.getMergeScheduler()).setSuppressExceptions();
             if (!first && !allowInterrupt) {
@@ -4503,30 +4515,30 @@
 
   public void testIndexStoreCombos() throws Exception {
     MockRAMDirectory dir = new MockRAMDirectory();
-    IndexWriter w = new IndexWriter(dir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.UNLIMITED);
+    IndexWriter w = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.UNLIMITED);
     byte[] b = new byte[50];
     for(int i=0;i<50;i++)
       b[i] = (byte) (i+77);
 
     Document doc = new Document();
     Field f = new Field("binary", b, 10, 17, Field.Store.YES);
-    f.setTokenStream(new WhitespaceTokenizer(new StringReader("doc1field1")));
+    f.setTokenStream(new WhitespaceTokenizer(TEST_VERSION_CURRENT, new StringReader("doc1field1")));
     Field f2 = new Field("string", "value", Field.Store.YES,Field.Index.ANALYZED);
-    f2.setTokenStream(new WhitespaceTokenizer(new StringReader("doc1field2")));
+    f2.setTokenStream(new WhitespaceTokenizer(TEST_VERSION_CURRENT, new StringReader("doc1field2")));
     doc.add(f);
     doc.add(f2);
     w.addDocument(doc);
     
     // add 2 docs to test in-memory merging
-    f.setTokenStream(new WhitespaceTokenizer(new StringReader("doc2field1")));
-    f2.setTokenStream(new WhitespaceTokenizer(new StringReader("doc2field2")));
+    f.setTokenStream(new WhitespaceTokenizer(TEST_VERSION_CURRENT, new StringReader("doc2field1")));
+    f2.setTokenStream(new WhitespaceTokenizer(TEST_VERSION_CURRENT, new StringReader("doc2field2")));
     w.addDocument(doc);
   
     // force segment flush so we can force a segment merge with doc3 later.
     w.commit();
 
-    f.setTokenStream(new WhitespaceTokenizer(new StringReader("doc3field1")));
-    f2.setTokenStream(new WhitespaceTokenizer(new StringReader("doc3field2")));
+    f.setTokenStream(new WhitespaceTokenizer(TEST_VERSION_CURRENT, new StringReader("doc3field1")));
+    f2.setTokenStream(new WhitespaceTokenizer(TEST_VERSION_CURRENT, new StringReader("doc3field2")));
 
     w.addDocument(doc);
     w.commit();
@@ -4565,7 +4577,7 @@
   // LUCENE-1727: make sure doc fields are stored in order
   public void testStoredFieldsOrder() throws Throwable {
     Directory d = new MockRAMDirectory();
-    IndexWriter w = new IndexWriter(d, new WhitespaceAnalyzer(), IndexWriter.MaxFieldLength.UNLIMITED);
+    IndexWriter w = new IndexWriter(d, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), IndexWriter.MaxFieldLength.UNLIMITED);
     Document doc = new Document();
     doc.add(new Field("zzz", "a b c", Field.Store.YES, Field.Index.NO));
     doc.add(new Field("aaa", "a b c", Field.Store.YES, Field.Index.NO));
@@ -4597,7 +4609,7 @@
   public void testEmbeddedFFFF() throws Throwable {
 
     Directory d = new MockRAMDirectory();
-    IndexWriter w = new IndexWriter(d, new WhitespaceAnalyzer(), IndexWriter.MaxFieldLength.UNLIMITED);
+    IndexWriter w = new IndexWriter(d, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), IndexWriter.MaxFieldLength.UNLIMITED);
     Document doc = new Document();
     doc.add(new Field("field", "a a\uffffb", Field.Store.NO, Field.Index.ANALYZED));
     w.addDocument(doc);
@@ -4614,7 +4626,7 @@
 
   public void testNoDocsIndex() throws Throwable {
     Directory dir = new MockRAMDirectory();
-    IndexWriter writer = new IndexWriter(dir, new SimpleAnalyzer(), IndexWriter.MaxFieldLength.UNLIMITED);
+    IndexWriter writer = new IndexWriter(dir, new SimpleAnalyzer(TEST_VERSION_CURRENT), IndexWriter.MaxFieldLength.UNLIMITED);
     writer.setUseCompoundFile(false);
     ByteArrayOutputStream bos = new ByteArrayOutputStream(1024);
     writer.setInfoStream(new PrintStream(bos));
@@ -4632,7 +4644,7 @@
     final int NUM_THREADS = 5;
     final double RUN_SEC = 0.5;
     final Directory dir = new MockRAMDirectory();
-    final IndexWriter w = new IndexWriter(dir, new SimpleAnalyzer(), IndexWriter.MaxFieldLength.UNLIMITED);
+    final IndexWriter w = new IndexWriter(dir, new SimpleAnalyzer(TEST_VERSION_CURRENT), IndexWriter.MaxFieldLength.UNLIMITED);
     w.commit();
     final AtomicBoolean failed = new AtomicBoolean();
     Thread[] threads = new Thread[NUM_THREADS];
@@ -4844,4 +4856,62 @@
     w.close();
     dir.close();
   }
+
+  public void testDeleteUnusedFiles() throws Exception {
+
+    for(int iter=0;iter<2;iter++) {
+      Directory dir = new MockRAMDirectory();
+      IndexWriter w = new IndexWriter(dir, new WhitespaceAnalyzer(), IndexWriter.MaxFieldLength.UNLIMITED);
+      Document doc = new Document();
+      doc.add(new Field("field", "go", Field.Store.NO, Field.Index.ANALYZED));
+      w.addDocument(doc);
+      IndexReader r;
+      if (iter == 0) {
+        // use NRT
+        r = w.getReader();
+      } else {
+        // don't use NRT
+        w.commit();
+        r = IndexReader.open(dir);
+      }
+
+      List<String> files = Arrays.asList(dir.listAll());
+      assertTrue(files.contains("_0.cfs"));
+      w.addDocument(doc);
+      w.optimize();
+      if (iter == 1) {
+        w.commit();
+      }
+      IndexReader r2 = r.reopen();
+      assertTrue(r != r2);
+      files = Arrays.asList(dir.listAll());
+      assertTrue(files.contains("_0.cfs"));
+      // optimize created this
+      assertTrue(files.contains("_2.cfs"));
+      w.deleteUnusedFiles();
+
+      files = Arrays.asList(dir.listAll());
+      // r still holds this file open
+      assertTrue(files.contains("_0.cfs"));
+      assertTrue(files.contains("_2.cfs"));
+
+      r.close();
+      if (iter == 0) {
+        // on closing NRT reader, it calls writer.deleteUnusedFiles
+        files = Arrays.asList(dir.listAll());
+        assertFalse(files.contains("_0.cfs"));
+      } else {
+        // now writer can remove it
+        w.deleteUnusedFiles();
+        files = Arrays.asList(dir.listAll());
+        assertFalse(files.contains("_0.cfs"));
+      }
+      assertTrue(files.contains("_2.cfs"));
+
+      w.close();
+      r2.close();
+
+      dir.close();
+    }
+  }
 }
Index: src/test/org/apache/lucene/index/TestMultiLevelSkipList.java
===================================================================
--- src/test/org/apache/lucene/index/TestMultiLevelSkipList.java	(revision 916567)
+++ src/test/org/apache/lucene/index/TestMultiLevelSkipList.java	(working copy)
@@ -101,7 +101,7 @@
   private static class PayloadAnalyzer extends Analyzer {
     @Override
     public TokenStream tokenStream(String fieldName, Reader reader) {
-      return new PayloadFilter(new LowerCaseTokenizer(reader));
+      return new PayloadFilter(new LowerCaseTokenizer(TEST_VERSION_CURRENT, reader));
     }
 
   }
Index: src/test/org/apache/lucene/index/TestCheckIndex.java
===================================================================
--- src/test/org/apache/lucene/index/TestCheckIndex.java	(revision 916567)
+++ src/test/org/apache/lucene/index/TestCheckIndex.java	(working copy)
@@ -34,7 +34,7 @@
 
   public void testDeletedDocs() throws IOException {
     MockRAMDirectory dir = new MockRAMDirectory();
-    IndexWriter writer  = new IndexWriter(dir, new WhitespaceAnalyzer(), true, 
+    IndexWriter writer  = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, 
                                           IndexWriter.MaxFieldLength.LIMITED);      
     writer.setMaxBufferedDocs(2);
     Document doc = new Document();
Index: src/test/org/apache/lucene/index/TestIndexWriterExceptions.java
===================================================================
--- src/test/org/apache/lucene/index/TestIndexWriterExceptions.java	(revision 916567)
+++ src/test/org/apache/lucene/index/TestIndexWriterExceptions.java	(working copy)
@@ -134,7 +134,7 @@
   public void testRandomExceptions() throws Throwable {
     MockRAMDirectory dir = new MockRAMDirectory();
 
-    MockIndexWriter writer  = new MockIndexWriter(dir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
+    MockIndexWriter writer  = new MockIndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
     ((ConcurrentMergeScheduler) writer.getMergeScheduler()).setSuppressExceptions();
     //writer.setMaxBufferedDocs(10);
     writer.setRAMBufferSizeMB(0.1);
@@ -172,7 +172,7 @@
   public void testRandomExceptionsThreads() throws Throwable {
 
     MockRAMDirectory dir = new MockRAMDirectory();
-    MockIndexWriter writer  = new MockIndexWriter(dir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
+    MockIndexWriter writer  = new MockIndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
     ((ConcurrentMergeScheduler) writer.getMergeScheduler()).setSuppressExceptions();
     //writer.setMaxBufferedDocs(10);
     writer.setRAMBufferSizeMB(0.2);
Index: src/test/org/apache/lucene/index/TestTransactions.java
===================================================================
--- src/test/org/apache/lucene/index/TestTransactions.java	(revision 916567)
+++ src/test/org/apache/lucene/index/TestTransactions.java	(working copy)
@@ -88,12 +88,12 @@
     @Override
     public void doWork() throws Throwable {
 
-      IndexWriter writer1 = new IndexWriter(dir1, new WhitespaceAnalyzer(), IndexWriter.MaxFieldLength.LIMITED);
+      IndexWriter writer1 = new IndexWriter(dir1, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), IndexWriter.MaxFieldLength.LIMITED);
       writer1.setMaxBufferedDocs(3);
       writer1.setMergeFactor(2);
       ((ConcurrentMergeScheduler) writer1.getMergeScheduler()).setSuppressExceptions();
 
-      IndexWriter writer2 = new IndexWriter(dir2, new WhitespaceAnalyzer(), IndexWriter.MaxFieldLength.LIMITED);
+      IndexWriter writer2 = new IndexWriter(dir2, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), IndexWriter.MaxFieldLength.LIMITED);
       // Intentionally use different params so flush/merge
       // happen @ different times
       writer2.setMaxBufferedDocs(2);
@@ -178,7 +178,7 @@
   }
 
   public void initIndex(Directory dir) throws Throwable {
-    IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(), IndexWriter.MaxFieldLength.LIMITED);
+    IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), IndexWriter.MaxFieldLength.LIMITED);
     for(int j=0; j<7; j++) {
       Document d = new Document();
       int n = RANDOM.nextInt();
Index: src/test/org/apache/lucene/index/TestDocumentWriter.java
===================================================================
--- src/test/org/apache/lucene/index/TestDocumentWriter.java	(revision 916567)
+++ src/test/org/apache/lucene/index/TestDocumentWriter.java	(working copy)
@@ -61,7 +61,7 @@
   public void testAddDocument() throws Exception {
     Document testDoc = new Document();
     DocHelper.setupDoc(testDoc);
-    Analyzer analyzer = new WhitespaceAnalyzer();
+    Analyzer analyzer = new WhitespaceAnalyzer(TEST_VERSION_CURRENT);
     IndexWriter writer = new IndexWriter(dir, analyzer, true, IndexWriter.MaxFieldLength.LIMITED);
     writer.addDocument(testDoc);
     writer.commit();
@@ -110,7 +110,7 @@
     Analyzer analyzer = new Analyzer() {
       @Override
       public TokenStream tokenStream(String fieldName, Reader reader) {
-        return new WhitespaceTokenizer(reader);
+        return new WhitespaceTokenizer(TEST_VERSION_CURRENT, reader);
       }
 
       @Override
@@ -143,7 +143,7 @@
     Analyzer analyzer = new Analyzer() {
       @Override
       public TokenStream tokenStream(String fieldName, Reader reader) {
-        return new TokenFilter(new WhitespaceTokenizer(reader)) {
+        return new TokenFilter(new WhitespaceTokenizer(TEST_VERSION_CURRENT, reader)) {
           boolean first=true;
           AttributeSource.State state;
 
@@ -207,7 +207,7 @@
 
 
   public void testPreAnalyzedField() throws IOException {
-    IndexWriter writer = new IndexWriter(dir, new SimpleAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
+    IndexWriter writer = new IndexWriter(dir, new SimpleAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
     Document doc = new Document();
     
     doc.add(new Field("preanalyzed", new TokenStream() {
@@ -221,6 +221,7 @@
         if (index == tokens.length) {
           return false;
         } else {
+          clearAttributes();
           termAtt.setTermBuffer(tokens[index++]);
           return true;
         }        
@@ -265,7 +266,7 @@
     doc.add(new Field("f2", "v1", Store.YES, Index.NOT_ANALYZED, TermVector.WITH_POSITIONS_OFFSETS));
     doc.add(new Field("f2", "v2", Store.YES, Index.NOT_ANALYZED, TermVector.NO));
 
-    IndexWriter writer = new IndexWriter(dir, new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
+    IndexWriter writer = new IndexWriter(dir, new StandardAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
     writer.addDocument(doc);
     writer.close();
 
@@ -298,7 +299,7 @@
     doc.add(f);
     doc.add(new Field("f2", "v2", Store.YES, Index.NO));
 
-    IndexWriter writer = new IndexWriter(dir, new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
+    IndexWriter writer = new IndexWriter(dir, new StandardAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
     writer.addDocument(doc);
     writer.optimize(); // be sure to have a single segment
     writer.close();
Index: src/test/org/apache/lucene/index/TestIndexFileDeleter.java
===================================================================
--- src/test/org/apache/lucene/index/TestIndexFileDeleter.java	(revision 916567)
+++ src/test/org/apache/lucene/index/TestIndexFileDeleter.java	(working copy)
@@ -18,7 +18,6 @@
  */
 
 import org.apache.lucene.util.LuceneTestCase;
-
 import org.apache.lucene.analysis.WhitespaceAnalyzer;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.IndexInput;
@@ -40,7 +39,7 @@
 
     Directory dir = new RAMDirectory();
 
-    IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
+    IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
     writer.setMaxBufferedDocs(10);
     int i;
     for(i=0;i<35;i++) {
@@ -145,7 +144,7 @@
 
     // Open & close a writer: it should delete the above 4
     // files and nothing more:
-    writer = new IndexWriter(dir, new WhitespaceAnalyzer(), false, IndexWriter.MaxFieldLength.LIMITED);
+    writer = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), false, IndexWriter.MaxFieldLength.LIMITED);
     writer.close();
 
     String[] files2 = dir.listAll();
Index: src/test/org/apache/lucene/index/TestIndexWriterLockRelease.java
===================================================================
--- src/test/org/apache/lucene/index/TestIndexWriterLockRelease.java	(revision 916567)
+++ src/test/org/apache/lucene/index/TestIndexWriterLockRelease.java	(working copy)
@@ -28,8 +28,6 @@
  * This tests the patch for issue #LUCENE-715 (IndexWriter does not
  * release its write lock when trying to open an index which does not yet
  * exist).
- *
- * @version $Id$
  */
 
 public class TestIndexWriterLockRelease extends LuceneTestCase {
@@ -77,10 +75,10 @@
         IndexWriter im;
         FSDirectory dir = FSDirectory.open(this.__test_dir);
         try {
-            im = new IndexWriter(dir, new org.apache.lucene.analysis.standard.StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT), false, IndexWriter.MaxFieldLength.LIMITED);
+            im = new IndexWriter(dir, new org.apache.lucene.analysis.standard.StandardAnalyzer(TEST_VERSION_CURRENT), false, IndexWriter.MaxFieldLength.LIMITED);
         } catch (FileNotFoundException e) {
             try {
-                im = new IndexWriter(dir, new org.apache.lucene.analysis.standard.StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT), false, IndexWriter.MaxFieldLength.LIMITED);
+                im = new IndexWriter(dir, new org.apache.lucene.analysis.standard.StandardAnalyzer(TEST_VERSION_CURRENT), false, IndexWriter.MaxFieldLength.LIMITED);
             } catch (FileNotFoundException e1) {
             }
         } finally {
Index: src/test/org/apache/lucene/index/TestAddIndexesNoOptimize.java
===================================================================
--- src/test/org/apache/lucene/index/TestAddIndexesNoOptimize.java	(revision 916567)
+++ src/test/org/apache/lucene/index/TestAddIndexesNoOptimize.java	(working copy)
@@ -20,7 +20,6 @@
 import java.io.IOException;
 
 import org.apache.lucene.util.LuceneTestCase;
-
 import org.apache.lucene.analysis.WhitespaceAnalyzer;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
@@ -432,7 +431,7 @@
 
   private IndexWriter newWriter(Directory dir, boolean create)
       throws IOException {
-    final IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(), create, IndexWriter.MaxFieldLength.UNLIMITED);
+    final IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), create, IndexWriter.MaxFieldLength.UNLIMITED);
     writer.setMergePolicy(new LogDocMergePolicy(writer));
     return writer;
   }
@@ -506,7 +505,7 @@
   public void testHangOnClose() throws IOException {
 
     Directory dir = new MockRAMDirectory();
-    IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
+    IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
     writer.setMergePolicy(new LogByteSizeMergePolicy(writer));
     writer.setMaxBufferedDocs(5);
     writer.setUseCompoundFile(false);
@@ -532,7 +531,7 @@
     writer.close();
 
     Directory dir2 = new MockRAMDirectory();
-    writer = new IndexWriter(dir2, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
+    writer = new IndexWriter(dir2, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
     LogByteSizeMergePolicy lmp = new LogByteSizeMergePolicy(writer);
     lmp.setMinMergeMB(0.0001);
     writer.setMergePolicy(lmp);
Index: src/test/org/apache/lucene/index/TestIndexReaderCloneNorms.java
===================================================================
--- src/test/org/apache/lucene/index/TestIndexReaderCloneNorms.java	(revision 916567)
+++ src/test/org/apache/lucene/index/TestIndexReaderCloneNorms.java	(working copy)
@@ -72,7 +72,7 @@
   protected void setUp() throws Exception {
     super.setUp();
     similarityOne = new SimilarityOne();
-    anlzr = new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT);
+    anlzr = new StandardAnalyzer(TEST_VERSION_CURRENT);
   }
   
   /**
Index: src/test/org/apache/lucene/index/TestCrash.java
===================================================================
--- src/test/org/apache/lucene/index/TestCrash.java	(revision 916567)
+++ src/test/org/apache/lucene/index/TestCrash.java	(working copy)
@@ -35,7 +35,7 @@
   private IndexWriter initIndex(MockRAMDirectory dir) throws IOException {
     dir.setLockFactory(NoLockFactory.getNoLockFactory());
 
-    IndexWriter writer  = new IndexWriter(dir, new WhitespaceAnalyzer(), IndexWriter.MaxFieldLength.UNLIMITED);
+    IndexWriter writer  = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), IndexWriter.MaxFieldLength.UNLIMITED);
     //writer.setMaxBufferedDocs(2);
     writer.setMaxBufferedDocs(10);
     ((ConcurrentMergeScheduler) writer.getMergeScheduler()).setSuppressExceptions();
Index: src/test/org/apache/lucene/index/TestConcurrentMergeScheduler.java
===================================================================
--- src/test/org/apache/lucene/index/TestConcurrentMergeScheduler.java	(revision 916567)
+++ src/test/org/apache/lucene/index/TestConcurrentMergeScheduler.java	(working copy)
@@ -25,12 +25,11 @@
 import org.apache.lucene.document.Field;
 
 import org.apache.lucene.util.LuceneTestCase;
-
 import java.io.IOException;
 
 public class TestConcurrentMergeScheduler extends LuceneTestCase {
   
-  private static final Analyzer ANALYZER = new SimpleAnalyzer();
+  private static final Analyzer ANALYZER = new SimpleAnalyzer(TEST_VERSION_CURRENT);
 
   private static class FailOnlyOnFlush extends MockRAMDirectory.Failure {
     boolean doFail;
Index: src/test/org/apache/lucene/index/TestThreadedOptimize.java
===================================================================
--- src/test/org/apache/lucene/index/TestThreadedOptimize.java	(revision 916567)
+++ src/test/org/apache/lucene/index/TestThreadedOptimize.java	(working copy)
@@ -34,7 +34,7 @@
 
 public class TestThreadedOptimize extends LuceneTestCase {
   
-  private static final Analyzer ANALYZER = new SimpleAnalyzer();
+  private static final Analyzer ANALYZER = new SimpleAnalyzer(TEST_VERSION_CURRENT);
 
   private final static int NUM_THREADS = 3;
   //private final static int NUM_THREADS = 5;
Index: src/test/org/apache/lucene/index/TestOmitTf.java
===================================================================
--- src/test/org/apache/lucene/index/TestOmitTf.java	(revision 916567)
+++ src/test/org/apache/lucene/index/TestOmitTf.java	(working copy)
@@ -61,7 +61,7 @@
   // omitTermFreqAndPositions bit in the FieldInfo
   public void testOmitTermFreqAndPositions() throws Exception {
     Directory ram = new MockRAMDirectory();
-    Analyzer analyzer = new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT);
+    Analyzer analyzer = new StandardAnalyzer(TEST_VERSION_CURRENT);
     IndexWriter writer = new IndexWriter(ram, analyzer, true, IndexWriter.MaxFieldLength.LIMITED);
     Document d = new Document();
         
@@ -112,7 +112,7 @@
   // omitTermFreqAndPositions for the same field works
   public void testMixedMerge() throws Exception {
     Directory ram = new MockRAMDirectory();
-    Analyzer analyzer = new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT);
+    Analyzer analyzer = new StandardAnalyzer(TEST_VERSION_CURRENT);
     IndexWriter writer = new IndexWriter(ram, analyzer, true, IndexWriter.MaxFieldLength.LIMITED);
     writer.setMaxBufferedDocs(3);
     writer.setMergeFactor(2);
@@ -169,7 +169,7 @@
   // field, 
   public void testMixedRAM() throws Exception {
     Directory ram = new MockRAMDirectory();
-    Analyzer analyzer = new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT);
+    Analyzer analyzer = new StandardAnalyzer(TEST_VERSION_CURRENT);
     IndexWriter writer = new IndexWriter(ram, analyzer, true, IndexWriter.MaxFieldLength.LIMITED);
     writer.setMaxBufferedDocs(10);
     writer.setMergeFactor(2);
@@ -217,7 +217,7 @@
   // Verifies no *.prx exists when all fields omit term freq:
   public void testNoPrxFile() throws Throwable {
     Directory ram = new MockRAMDirectory();
-    Analyzer analyzer = new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT);
+    Analyzer analyzer = new StandardAnalyzer(TEST_VERSION_CURRENT);
     IndexWriter writer = new IndexWriter(ram, analyzer, true, IndexWriter.MaxFieldLength.LIMITED);
     writer.setMaxBufferedDocs(3);
     writer.setMergeFactor(2);
@@ -248,7 +248,7 @@
   // Test scores with one field with Term Freqs and one without, otherwise with equal content 
   public void testBasic() throws Exception {
     Directory dir = new MockRAMDirectory();  
-    Analyzer analyzer = new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT);
+    Analyzer analyzer = new StandardAnalyzer(TEST_VERSION_CURRENT);
     IndexWriter writer = new IndexWriter(dir, analyzer, true, IndexWriter.MaxFieldLength.LIMITED);
     writer.setMergeFactor(2);
     writer.setMaxBufferedDocs(2);
Index: src/test/org/apache/lucene/index/TestIndexWriterMergePolicy.java
===================================================================
--- src/test/org/apache/lucene/index/TestIndexWriterMergePolicy.java	(revision 916567)
+++ src/test/org/apache/lucene/index/TestIndexWriterMergePolicy.java	(working copy)
@@ -34,7 +34,7 @@
   public void testNormalCase() throws IOException {
     Directory dir = new RAMDirectory();
 
-    IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
+    IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
     writer.setMaxBufferedDocs(10);
     writer.setMergeFactor(10);
     writer.setMergePolicy(new LogDocMergePolicy(writer));
@@ -51,7 +51,7 @@
   public void testNoOverMerge() throws IOException {
     Directory dir = new RAMDirectory();
 
-    IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
+    IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
     writer.setMaxBufferedDocs(10);
     writer.setMergeFactor(10);
     writer.setMergePolicy(new LogDocMergePolicy(writer));
@@ -73,7 +73,7 @@
   public void testForceFlush() throws IOException {
     Directory dir = new RAMDirectory();
 
-    IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
+    IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
     writer.setMaxBufferedDocs(10);
     writer.setMergeFactor(10);
     LogDocMergePolicy mp = new LogDocMergePolicy(writer);
@@ -84,7 +84,7 @@
       addDoc(writer);
       writer.close();
 
-      writer = new IndexWriter(dir, new WhitespaceAnalyzer(), false, IndexWriter.MaxFieldLength.LIMITED);
+      writer = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), false, IndexWriter.MaxFieldLength.LIMITED);
       writer.setMaxBufferedDocs(10);
       writer.setMergePolicy(mp);
       mp.setMinMergeDocs(100);
@@ -99,7 +99,7 @@
   public void testMergeFactorChange() throws IOException {
     Directory dir = new RAMDirectory();
 
-    IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
+    IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
     writer.setMaxBufferedDocs(10);
     writer.setMergeFactor(100);
     writer.setMergePolicy(new LogDocMergePolicy(writer));
@@ -125,7 +125,7 @@
   public void testMaxBufferedDocsChange() throws IOException {
     Directory dir = new RAMDirectory();
 
-    IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.UNLIMITED);
+    IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.UNLIMITED);
     writer.setMaxBufferedDocs(101);
     writer.setMergeFactor(101);
     writer.setMergePolicy(new LogDocMergePolicy(writer));
@@ -139,7 +139,7 @@
       }
       writer.close();
 
-      writer = new IndexWriter(dir, new WhitespaceAnalyzer(), false, IndexWriter.MaxFieldLength.UNLIMITED);
+      writer = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), false, IndexWriter.MaxFieldLength.UNLIMITED);
       writer.setMaxBufferedDocs(101);
       writer.setMergeFactor(101);
       writer.setMergePolicy(new LogDocMergePolicy(writer));
@@ -170,7 +170,7 @@
   public void testMergeDocCount0() throws IOException {
     Directory dir = new RAMDirectory();
 
-    IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.UNLIMITED);
+    IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.UNLIMITED);
     writer.setMergePolicy(new LogDocMergePolicy(writer));
     writer.setMaxBufferedDocs(10);
     writer.setMergeFactor(100);
@@ -185,7 +185,7 @@
     reader.deleteDocuments(new Term("content", "aaa"));
     reader.close();
 
-    writer = new IndexWriter(dir, new WhitespaceAnalyzer(), false, IndexWriter.MaxFieldLength.UNLIMITED);
+    writer = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), false, IndexWriter.MaxFieldLength.UNLIMITED);
     writer.setMergePolicy(new LogDocMergePolicy(writer));
     writer.setMaxBufferedDocs(10);
     writer.setMergeFactor(5);
Index: src/test/org/apache/lucene/index/DocHelper.java
===================================================================
--- src/test/org/apache/lucene/index/DocHelper.java	(revision 916567)
+++ src/test/org/apache/lucene/index/DocHelper.java	(working copy)
@@ -29,6 +29,7 @@
 import org.apache.lucene.document.Fieldable;
 import org.apache.lucene.search.Similarity;
 import org.apache.lucene.store.Directory;
+import static org.apache.lucene.util.LuceneTestCaseJ4.TEST_VERSION_CURRENT;
 
 class DocHelper {
   public static final String FIELD_1_TEXT = "field one text";
@@ -218,7 +219,7 @@
    */ 
   public static SegmentInfo writeDoc(Directory dir, Document doc) throws IOException
   {
-    return writeDoc(dir, new WhitespaceAnalyzer(), Similarity.getDefault(), doc);
+    return writeDoc(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), Similarity.getDefault(), doc);
   }
 
   /**
Index: src/test/org/apache/lucene/index/TestAtomicUpdate.java
===================================================================
--- src/test/org/apache/lucene/index/TestAtomicUpdate.java	(revision 916567)
+++ src/test/org/apache/lucene/index/TestAtomicUpdate.java	(working copy)
@@ -26,7 +26,7 @@
 import java.io.IOException;
 
 public class TestAtomicUpdate extends LuceneTestCase {
-  private static final Analyzer ANALYZER = new SimpleAnalyzer();
+  private static final Analyzer ANALYZER = new SimpleAnalyzer(TEST_VERSION_CURRENT);
   private Random RANDOM;
 
   public class MockIndexWriter extends IndexWriter {
Index: src/test/org/apache/lucene/index/TestIndexWriterDelete.java
===================================================================
--- src/test/org/apache/lucene/index/TestIndexWriterDelete.java	(revision 916567)
+++ src/test/org/apache/lucene/index/TestIndexWriterDelete.java	(working copy)
@@ -41,7 +41,7 @@
 
     Directory dir = new MockRAMDirectory();
     IndexWriter modifier = new IndexWriter(dir,
-                                           new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.UNLIMITED);
+                                           new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.UNLIMITED);
     modifier.setUseCompoundFile(true);
     modifier.setMaxBufferedDeleteTerms(1);
 
@@ -78,7 +78,7 @@
 
     Directory dir = new MockRAMDirectory();
     IndexWriter modifier = new IndexWriter(dir,
-                                           new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.UNLIMITED);
+                                           new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.UNLIMITED);
     modifier.setMaxBufferedDocs(2);
     modifier.setMaxBufferedDeleteTerms(2);
 
@@ -113,7 +113,7 @@
   public void testMaxBufferedDeletes() throws IOException {
     Directory dir = new MockRAMDirectory();
     IndexWriter writer = new IndexWriter(dir,
-                                         new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.UNLIMITED);
+                                         new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.UNLIMITED);
     writer.setMaxBufferedDeleteTerms(1);
     writer.deleteDocuments(new Term("foobar", "1"));
     writer.deleteDocuments(new Term("foobar", "1"));
@@ -128,7 +128,7 @@
     for(int t=0;t<2;t++) {
       Directory dir = new MockRAMDirectory();
       IndexWriter modifier = new IndexWriter(dir,
-                                             new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.UNLIMITED);
+                                             new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.UNLIMITED);
       modifier.setMaxBufferedDocs(4);
       modifier.setMaxBufferedDeleteTerms(4);
 
@@ -170,7 +170,7 @@
   public void testBothDeletes() throws IOException {
     Directory dir = new MockRAMDirectory();
     IndexWriter modifier = new IndexWriter(dir,
-                                           new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.UNLIMITED);
+                                           new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.UNLIMITED);
     modifier.setMaxBufferedDocs(100);
     modifier.setMaxBufferedDeleteTerms(100);
 
@@ -203,7 +203,7 @@
   public void testBatchDeletes() throws IOException {
     Directory dir = new MockRAMDirectory();
     IndexWriter modifier = new IndexWriter(dir,
-                                           new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.UNLIMITED);
+                                           new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.UNLIMITED);
     modifier.setMaxBufferedDocs(2);
     modifier.setMaxBufferedDeleteTerms(2);
 
@@ -247,7 +247,7 @@
   public void testDeleteAll() throws IOException {
     Directory dir = new MockRAMDirectory();
     IndexWriter modifier = new IndexWriter(dir,
-                                           new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.UNLIMITED);
+                                           new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.UNLIMITED);
     modifier.setMaxBufferedDocs(2);
     modifier.setMaxBufferedDeleteTerms(2);
 
@@ -294,7 +294,7 @@
   public void testDeleteAllRollback() throws IOException {
     Directory dir = new MockRAMDirectory();
     IndexWriter modifier = new IndexWriter(dir,
-                                           new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.UNLIMITED);
+                                           new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.UNLIMITED);
     modifier.setMaxBufferedDocs(2);
     modifier.setMaxBufferedDeleteTerms(2);
     
@@ -332,7 +332,7 @@
   public void testDeleteAllNRT() throws IOException {
     Directory dir = new MockRAMDirectory();
     IndexWriter modifier = new IndexWriter(dir,
-                                           new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.UNLIMITED);
+                                           new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.UNLIMITED);
     modifier.setMaxBufferedDocs(2);
     modifier.setMaxBufferedDeleteTerms(2);
     
@@ -424,7 +424,7 @@
     // First build up a starting index:
     MockRAMDirectory startDir = new MockRAMDirectory();
     IndexWriter writer = new IndexWriter(startDir,
-                                         new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.UNLIMITED);
+                                         new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.UNLIMITED);
     for (int i = 0; i < 157; i++) {
       Document d = new Document();
       d.add(new Field("id", Integer.toString(i), Field.Store.YES,
@@ -447,7 +447,7 @@
       MockRAMDirectory dir = new MockRAMDirectory(startDir);
       dir.setPreventDoubleWrite(false);
       IndexWriter modifier = new IndexWriter(dir,
-                                             new WhitespaceAnalyzer(), IndexWriter.MaxFieldLength.UNLIMITED);
+                                             new WhitespaceAnalyzer(TEST_VERSION_CURRENT), IndexWriter.MaxFieldLength.UNLIMITED);
 
       modifier.setMaxBufferedDocs(1000); // use flush or close
       modifier.setMaxBufferedDeleteTerms(1000); // use flush or close
@@ -653,7 +653,7 @@
 
     MockRAMDirectory dir = new MockRAMDirectory();
     IndexWriter modifier = new IndexWriter(dir,
-                                           new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.UNLIMITED);
+                                           new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.UNLIMITED);
     modifier.setUseCompoundFile(true);
     modifier.setMaxBufferedDeleteTerms(2);
 
@@ -762,7 +762,7 @@
 
     MockRAMDirectory dir = new MockRAMDirectory();
     IndexWriter modifier = new IndexWriter(dir,
-                                           new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.UNLIMITED);
+                                           new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.UNLIMITED);
 
     dir.failOn(failure.reset());
 
Index: src/test/org/apache/lucene/index/TestDirectoryReader.java
===================================================================
--- src/test/org/apache/lucene/index/TestDirectoryReader.java	(revision 916567)
+++ src/test/org/apache/lucene/index/TestDirectoryReader.java	(working copy)
@@ -194,7 +194,7 @@
   }
 
   private void addDoc(RAMDirectory ramDir1, String s, boolean create) throws IOException {
-    IndexWriter iw = new IndexWriter(ramDir1, new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT), create, IndexWriter.MaxFieldLength.LIMITED);
+    IndexWriter iw = new IndexWriter(ramDir1, new StandardAnalyzer(TEST_VERSION_CURRENT), create, IndexWriter.MaxFieldLength.LIMITED);
     Document doc = new Document();
     doc.add(new Field("body", s, Field.Store.YES, Field.Index.ANALYZED));
     iw.addDocument(doc);
Index: src/test/org/apache/lucene/index/TestIndexWriterReader.java
===================================================================
--- src/test/org/apache/lucene/index/TestIndexWriterReader.java	(revision 916567)
+++ src/test/org/apache/lucene/index/TestIndexWriterReader.java	(working copy)
@@ -76,7 +76,7 @@
     boolean optimize = true;
 
     Directory dir1 = new MockRAMDirectory();
-    IndexWriter writer = new IndexWriter(dir1, new WhitespaceAnalyzer(),
+    IndexWriter writer = new IndexWriter(dir1, new WhitespaceAnalyzer(TEST_VERSION_CURRENT),
         IndexWriter.MaxFieldLength.LIMITED);
 
     // create the index
@@ -99,12 +99,26 @@
     assertEquals(1, count(new Term("id", Integer.toString(8000)), r2));
     
     r1.close();
-    r2.close();
     writer.close();
     
     IndexReader r3 = IndexReader.open(dir1, true);
     assertEquals(0, count(new Term("id", id10), r3));
     assertEquals(1, count(new Term("id", Integer.toString(8000)), r3));
+
+    writer = new IndexWriter(dir1, new WhitespaceAnalyzer(TEST_VERSION_CURRENT),
+                             IndexWriter.MaxFieldLength.LIMITED);
+    Document doc = new Document();
+    doc.add(new Field("field", "a b c", Field.Store.NO, Field.Index.ANALYZED));
+    writer.addDocument(doc);
+    assertTrue(r2.isCurrent());
+    assertTrue(r3.isCurrent());
+
+    writer.close();
+
+    assertFalse(r2.isCurrent());
+    assertTrue(!r3.isCurrent());
+
+    r2.close();
     r3.close();
     
     dir1.close();
@@ -119,7 +133,7 @@
     boolean optimize = false;
 
     Directory dir1 = new MockRAMDirectory();
-    IndexWriter writer = new IndexWriter(dir1, new WhitespaceAnalyzer(),
+    IndexWriter writer = new IndexWriter(dir1, new WhitespaceAnalyzer(TEST_VERSION_CURRENT),
         IndexWriter.MaxFieldLength.LIMITED);
     writer.setInfoStream(infoStream);
     // create the index
@@ -128,7 +142,7 @@
 
     // create a 2nd index
     Directory dir2 = new MockRAMDirectory();
-    IndexWriter writer2 = new IndexWriter(dir2, new WhitespaceAnalyzer(),
+    IndexWriter writer2 = new IndexWriter(dir2, new WhitespaceAnalyzer(TEST_VERSION_CURRENT),
         IndexWriter.MaxFieldLength.LIMITED);
     writer2.setInfoStream(infoStream);
     createIndexNoClose(!optimize, "index2", writer2);
@@ -157,13 +171,13 @@
     boolean optimize = false;
 
     Directory dir1 = new MockRAMDirectory();
-    IndexWriter writer = new IndexWriter(dir1, new WhitespaceAnalyzer(),
+    IndexWriter writer = new IndexWriter(dir1, new WhitespaceAnalyzer(TEST_VERSION_CURRENT),
         IndexWriter.MaxFieldLength.LIMITED);
     writer.setInfoStream(infoStream);
 
     // create a 2nd index
     Directory dir2 = new MockRAMDirectory();
-    IndexWriter writer2 = new IndexWriter(dir2, new WhitespaceAnalyzer(),
+    IndexWriter writer2 = new IndexWriter(dir2, new WhitespaceAnalyzer(TEST_VERSION_CURRENT),
         IndexWriter.MaxFieldLength.LIMITED);
     writer2.setInfoStream(infoStream);
     createIndexNoClose(!optimize, "index2", writer2);
@@ -192,7 +206,7 @@
     boolean optimize = true;
 
     Directory dir1 = new MockRAMDirectory();
-    IndexWriter writer = new IndexWriter(dir1, new WhitespaceAnalyzer(),
+    IndexWriter writer = new IndexWriter(dir1, new WhitespaceAnalyzer(TEST_VERSION_CURRENT),
         IndexWriter.MaxFieldLength.LIMITED);
     writer.setInfoStream(infoStream);
     // create the index
@@ -231,7 +245,7 @@
     writer.close();
         
     // reopen the writer to verify the delete made it to the directory
-    writer = new IndexWriter(dir1, new WhitespaceAnalyzer(),
+    writer = new IndexWriter(dir1, new WhitespaceAnalyzer(TEST_VERSION_CURRENT),
         IndexWriter.MaxFieldLength.LIMITED);
     writer.setInfoStream(infoStream);
     IndexReader w2r1 = writer.getReader();
@@ -246,7 +260,7 @@
     int numDirs = 3;
     
     Directory mainDir = new MockRAMDirectory();
-    IndexWriter mainWriter = new IndexWriter(mainDir, new WhitespaceAnalyzer(),
+    IndexWriter mainWriter = new IndexWriter(mainDir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT),
         IndexWriter.MaxFieldLength.LIMITED);
     mainWriter.setInfoStream(infoStream);
     AddDirectoriesThreads addDirThreads = new AddDirectoriesThreads(numIter, mainWriter);
@@ -354,7 +368,7 @@
       this.numDirs = numDirs;
       this.mainWriter = mainWriter;
       addDir = new MockRAMDirectory();
-      IndexWriter writer = new IndexWriter(addDir, new WhitespaceAnalyzer(),
+      IndexWriter writer = new IndexWriter(addDir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT),
           IndexWriter.MaxFieldLength.LIMITED);
       writer.setMaxBufferedDocs(2);
       for (int i = 0; i < NUM_INIT_DOCS; i++) {
@@ -462,7 +476,7 @@
    */
   public void doTestIndexWriterReopenSegment(boolean optimize) throws Exception {
     Directory dir1 = new MockRAMDirectory();
-    IndexWriter writer = new IndexWriter(dir1, new WhitespaceAnalyzer(),
+    IndexWriter writer = new IndexWriter(dir1, new WhitespaceAnalyzer(TEST_VERSION_CURRENT),
         IndexWriter.MaxFieldLength.LIMITED);
     writer.setInfoStream(infoStream);
     IndexReader r1 = writer.getReader();
@@ -500,7 +514,7 @@
     writer.close();
 
     // test whether the changes made it to the directory
-    writer = new IndexWriter(dir1, new WhitespaceAnalyzer(),
+    writer = new IndexWriter(dir1, new WhitespaceAnalyzer(TEST_VERSION_CURRENT),
         IndexWriter.MaxFieldLength.LIMITED);
     IndexReader w2r1 = writer.getReader();
     // insure the deletes were actually flushed to the directory
@@ -541,7 +555,7 @@
    */
   public static void createIndex(Directory dir1, String indexName,
       boolean multiSegment) throws IOException {
-    IndexWriter w = new IndexWriter(dir1, new WhitespaceAnalyzer(),
+    IndexWriter w = new IndexWriter(dir1, new WhitespaceAnalyzer(TEST_VERSION_CURRENT),
         IndexWriter.MaxFieldLength.LIMITED);
     w.setMergePolicy(new LogDocMergePolicy(w));
     for (int i = 0; i < 100; i++) {
@@ -576,7 +590,7 @@
   public void testMergeWarmer() throws Exception {
 
     Directory dir1 = new MockRAMDirectory();
-    IndexWriter writer = new IndexWriter(dir1, new WhitespaceAnalyzer(),
+    IndexWriter writer = new IndexWriter(dir1, new WhitespaceAnalyzer(TEST_VERSION_CURRENT),
                                          IndexWriter.MaxFieldLength.LIMITED);
     writer.setInfoStream(infoStream);
 
@@ -611,7 +625,7 @@
 
   public void testAfterCommit() throws Exception {
     Directory dir1 = new MockRAMDirectory();
-    IndexWriter writer = new IndexWriter(dir1, new WhitespaceAnalyzer(),
+    IndexWriter writer = new IndexWriter(dir1, new WhitespaceAnalyzer(TEST_VERSION_CURRENT),
                                          IndexWriter.MaxFieldLength.LIMITED);
     writer.setInfoStream(infoStream);
 
@@ -644,7 +658,7 @@
   // Make sure reader remains usable even if IndexWriter closes
   public void testAfterClose() throws Exception {
     Directory dir1 = new MockRAMDirectory();
-    IndexWriter writer = new IndexWriter(dir1, new WhitespaceAnalyzer(),
+    IndexWriter writer = new IndexWriter(dir1, new WhitespaceAnalyzer(TEST_VERSION_CURRENT),
                                          IndexWriter.MaxFieldLength.LIMITED);
     writer.setInfoStream(infoStream);
 
@@ -674,7 +688,7 @@
   // Stress test reopen during addIndexes
   public void testDuringAddIndexes() throws Exception {
     Directory dir1 = new MockRAMDirectory();
-    final IndexWriter writer = new IndexWriter(dir1, new WhitespaceAnalyzer(),
+    final IndexWriter writer = new IndexWriter(dir1, new WhitespaceAnalyzer(TEST_VERSION_CURRENT),
                                                IndexWriter.MaxFieldLength.LIMITED);
     writer.setInfoStream(infoStream);
     writer.setMergeFactor(2);
@@ -752,7 +766,7 @@
   // Stress test reopen during add/delete
   public void testDuringAddDelete() throws Exception {
     Directory dir1 = new MockRAMDirectory();
-    final IndexWriter writer = new IndexWriter(dir1, new WhitespaceAnalyzer(),
+    final IndexWriter writer = new IndexWriter(dir1, new WhitespaceAnalyzer(TEST_VERSION_CURRENT),
                                                IndexWriter.MaxFieldLength.LIMITED);
     writer.setInfoStream(infoStream);
     writer.setMergeFactor(2);
@@ -833,7 +847,7 @@
 
   public void testExpungeDeletes() throws Throwable {
     Directory dir = new MockRAMDirectory();
-    final IndexWriter w = new IndexWriter(dir, new WhitespaceAnalyzer(),
+    final IndexWriter w = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT),
                                                IndexWriter.MaxFieldLength.LIMITED);
     Document doc = new Document();
     doc.add(new Field("field", "a b c", Field.Store.NO, Field.Index.ANALYZED));
@@ -858,7 +872,7 @@
 
   public void testDeletesNumDocs() throws Throwable {
     Directory dir = new MockRAMDirectory();
-    final IndexWriter w = new IndexWriter(dir, new WhitespaceAnalyzer(),
+    final IndexWriter w = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT),
                                                IndexWriter.MaxFieldLength.LIMITED);
     Document doc = new Document();
     doc.add(new Field("field", "a b c", Field.Store.NO, Field.Index.ANALYZED));
Index: src/test/org/apache/lucene/index/TestFieldsReader.java
===================================================================
--- src/test/org/apache/lucene/index/TestFieldsReader.java	(revision 916567)
+++ src/test/org/apache/lucene/index/TestFieldsReader.java	(working copy)
@@ -50,7 +50,7 @@
     fieldInfos = new FieldInfos();
     DocHelper.setupDoc(testDoc);
     fieldInfos.add(testDoc);
-    IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
+    IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
     writer.setUseCompoundFile(false);
     writer.addDocument(testDoc);
     writer.close();
@@ -211,7 +211,7 @@
     FSDirectory tmpDir = FSDirectory.open(file);
     assertTrue(tmpDir != null);
 
-    IndexWriter writer = new IndexWriter(tmpDir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
+    IndexWriter writer = new IndexWriter(tmpDir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
     writer.setUseCompoundFile(false);
     writer.addDocument(testDoc);
     writer.close();
@@ -392,7 +392,7 @@
 
     try {
       Directory dir = new FaultyFSDirectory(indexDir);
-      IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
+      IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
       for(int i=0;i<2;i++)
         writer.addDocument(testDoc);
       writer.optimize();
Index: src/test/org/apache/lucene/index/TestNRTReaderWithThreads.java
===================================================================
--- src/test/org/apache/lucene/index/TestNRTReaderWithThreads.java	(revision 916567)
+++ src/test/org/apache/lucene/index/TestNRTReaderWithThreads.java	(working copy)
@@ -32,7 +32,7 @@
 
   public void testIndexing() throws Exception {
     Directory mainDir = new MockRAMDirectory();
-    IndexWriter writer = new IndexWriter(mainDir, new WhitespaceAnalyzer(),
+    IndexWriter writer = new IndexWriter(mainDir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT),
         IndexWriter.MaxFieldLength.LIMITED);
     writer.setUseCompoundFile(false);
     IndexReader reader = writer.getReader(); // start pooling readers
@@ -54,13 +54,16 @@
     int addCount = 0;
     for (int x=0; x < indexThreads.length; x++) {
       indexThreads[x].run = false;
-      assertTrue(indexThreads[x].ex == null);
+      assertNull("Exception thrown: "+indexThreads[x].ex, indexThreads[x].ex);
       addCount += indexThreads[x].addCount;
       delCount += indexThreads[x].delCount;
     }
     for (int x=0; x < indexThreads.length; x++) {
       indexThreads[x].join();
     }
+    for (int x=0; x < indexThreads.length; x++) {
+      assertNull("Exception thrown: "+indexThreads[x].ex, indexThreads[x].ex);
+    }
     //System.out.println("addCount:"+addCount);
     //System.out.println("delCount:"+delCount);
     writer.close();
@@ -69,8 +72,8 @@
 
   public class RunThread extends Thread {
     IndexWriter writer;
-    boolean run = true;
-    Throwable ex;
+    volatile boolean run = true;
+    volatile Throwable ex;
     int delCount = 0;
     int addCount = 0;
     int type;
Index: src/test/org/apache/lucene/index/TestTransactionRollback.java
===================================================================
--- src/test/org/apache/lucene/index/TestTransactionRollback.java	(revision 916567)
+++ src/test/org/apache/lucene/index/TestTransactionRollback.java	(working copy)
@@ -27,7 +27,6 @@
 import java.util.HashMap;
 
 import org.apache.lucene.util.LuceneTestCase;
-
 import org.apache.lucene.analysis.WhitespaceAnalyzer;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
@@ -66,7 +65,7 @@
     if (last==null)
       throw new RuntimeException("Couldn't find commit point "+id);
 		
-    IndexWriter w = new IndexWriter(dir, new WhitespaceAnalyzer(),
+    IndexWriter w = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT),
                                     new RollbackDeletionPolicy(id), MaxFieldLength.UNLIMITED, last);
     Map<String,String> data = new HashMap<String,String>();
     data.put("index", "Rolled back to 1-"+id);
@@ -128,7 +127,7 @@
 		
     //Build index, of records 1 to 100, committing after each batch of 10
     IndexDeletionPolicy sdp=new KeepAllDeletionPolicy();
-    IndexWriter w=new IndexWriter(dir,new WhitespaceAnalyzer(),sdp,MaxFieldLength.UNLIMITED);
+    IndexWriter w=new IndexWriter(dir,new WhitespaceAnalyzer(TEST_VERSION_CURRENT),sdp,MaxFieldLength.UNLIMITED);
     for(int currentRecordId=1;currentRecordId<=100;currentRecordId++) {
       Document doc=new Document();
       doc.add(new Field(FIELD_RECORD_ID,""+currentRecordId,Field.Store.YES,Field.Index.ANALYZED));
@@ -196,7 +195,7 @@
     for(int i=0;i<2;i++) {
       // Unless you specify a prior commit point, rollback
       // should not work:
-      new IndexWriter(dir,new WhitespaceAnalyzer(),
+      new IndexWriter(dir,new WhitespaceAnalyzer(TEST_VERSION_CURRENT),
                       new DeleteLastCommitPolicy(),
                       MaxFieldLength.UNLIMITED).close();
       IndexReader r = IndexReader.open(dir, true);
Index: src/test/org/apache/lucene/index/TestFilterIndexReader.java
===================================================================
--- src/test/org/apache/lucene/index/TestFilterIndexReader.java	(revision 916567)
+++ src/test/org/apache/lucene/index/TestFilterIndexReader.java	(working copy)
@@ -97,7 +97,7 @@
    */
   public void testFilterIndexReader() throws Exception {
     RAMDirectory directory = new MockRAMDirectory();
-    IndexWriter writer = new IndexWriter(directory, new WhitespaceAnalyzer(), true,
+    IndexWriter writer = new IndexWriter(directory, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true,
                                          IndexWriter.MaxFieldLength.LIMITED);
 
     Document d1 = new Document();
Index: src/test/org/apache/lucene/index/TestIndexReaderReopen.java
===================================================================
--- src/test/org/apache/lucene/index/TestIndexReaderReopen.java	(revision 916567)
+++ src/test/org/apache/lucene/index/TestIndexReaderReopen.java	(working copy)
@@ -702,7 +702,7 @@
     final Directory dir = new MockRAMDirectory();
     final int n = 30;
 
-    IndexWriter writer = new IndexWriter(dir, new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT), IndexWriter.MaxFieldLength.LIMITED);
+    IndexWriter writer = new IndexWriter(dir, new StandardAnalyzer(TEST_VERSION_CURRENT), IndexWriter.MaxFieldLength.LIMITED);
     for (int i = 0; i < n; i++) {
       writer.addDocument(createDocument(i, 3));
     }
@@ -721,7 +721,7 @@
           modifier.deleteDocument(i % modifier.maxDoc());
           modifier.close();
         } else {
-          IndexWriter modifier = new IndexWriter(dir, new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT), IndexWriter.MaxFieldLength.LIMITED);
+          IndexWriter modifier = new IndexWriter(dir, new StandardAnalyzer(TEST_VERSION_CURRENT), IndexWriter.MaxFieldLength.LIMITED);
           modifier.addDocument(createDocument(n + i, 6));
           modifier.close();
         }
@@ -948,7 +948,7 @@
   
   public static void createIndex(Directory dir, boolean multiSegment) throws IOException {
     IndexWriter.unlock(dir);
-    IndexWriter w = new IndexWriter(dir, new WhitespaceAnalyzer(), IndexWriter.MaxFieldLength.LIMITED);
+    IndexWriter w = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), IndexWriter.MaxFieldLength.LIMITED);
 
     w.setMergePolicy(new LogDocMergePolicy(w));
     
@@ -993,7 +993,7 @@
   static void modifyIndex(int i, Directory dir) throws IOException {
     switch (i) {
       case 0: {
-        IndexWriter w = new IndexWriter(dir, new WhitespaceAnalyzer(), IndexWriter.MaxFieldLength.LIMITED);
+        IndexWriter w = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), IndexWriter.MaxFieldLength.LIMITED);
         w.deleteDocuments(new Term("field2", "a11"));
         w.deleteDocuments(new Term("field2", "b30"));
         w.close();
@@ -1008,13 +1008,13 @@
         break;
       }
       case 2: {
-        IndexWriter w = new IndexWriter(dir, new WhitespaceAnalyzer(), IndexWriter.MaxFieldLength.LIMITED);
+        IndexWriter w = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), IndexWriter.MaxFieldLength.LIMITED);
         w.optimize();
         w.close();
         break;
       }
       case 3: {
-        IndexWriter w = new IndexWriter(dir, new WhitespaceAnalyzer(), IndexWriter.MaxFieldLength.LIMITED);
+        IndexWriter w = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), IndexWriter.MaxFieldLength.LIMITED);
         w.addDocument(createDocument(101, 4));
         w.optimize();
         w.addDocument(createDocument(102, 4));
@@ -1030,7 +1030,7 @@
         break;
       }
       case 5: {
-        IndexWriter w = new IndexWriter(dir, new WhitespaceAnalyzer(), IndexWriter.MaxFieldLength.LIMITED);
+        IndexWriter w = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), IndexWriter.MaxFieldLength.LIMITED);
         w.addDocument(createDocument(101, 4));
         w.close();
         break;
@@ -1194,7 +1194,7 @@
 
   public void testReopenOnCommit() throws Throwable {
     Directory dir = new MockRAMDirectory();
-    IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(), new KeepAllCommits(), IndexWriter.MaxFieldLength.UNLIMITED);
+    IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), new KeepAllCommits(), IndexWriter.MaxFieldLength.UNLIMITED);
     for(int i=0;i<4;i++) {
       Document doc = new Document();
       doc.add(new Field("id", ""+i, Field.Store.NO, Field.Index.NOT_ANALYZED));
Index: src/test/org/apache/lucene/index/TestStressIndexing.java
===================================================================
--- src/test/org/apache/lucene/index/TestStressIndexing.java	(revision 916567)
+++ src/test/org/apache/lucene/index/TestStressIndexing.java	(working copy)
@@ -26,7 +26,7 @@
 import java.io.File;
 
 public class TestStressIndexing extends LuceneTestCase {
-  private static final Analyzer ANALYZER = new SimpleAnalyzer();
+  private static final Analyzer ANALYZER = new SimpleAnalyzer(TEST_VERSION_CURRENT);
   private Random RANDOM;
 
   private static abstract class TimedThread extends Thread {
Index: src/test/org/apache/lucene/index/TestTermdocPerf.java
===================================================================
--- src/test/org/apache/lucene/index/TestTermdocPerf.java	(revision 916567)
+++ src/test/org/apache/lucene/index/TestTermdocPerf.java	(working copy)
@@ -30,10 +30,6 @@
 import org.apache.lucene.store.RAMDirectory;
 import org.apache.lucene.util.LuceneTestCase;
 
-/**
- * @version $Id$
- */
-
 class RepeatingTokenStream extends TokenStream {
   public int num;
   TermAttribute termAtt;
@@ -48,6 +44,7 @@
    public boolean incrementToken() throws IOException {
      num--;
      if (num >= 0) {
+       clearAttributes();
        termAtt.setTermBuffer(value);
        return true;
      }
Index: src/test/org/apache/lucene/index/TestPayloads.java
===================================================================
--- src/test/org/apache/lucene/index/TestPayloads.java	(revision 916567)
+++ src/test/org/apache/lucene/index/TestPayloads.java	(working copy)
@@ -410,7 +410,7 @@
         @Override
         public TokenStream tokenStream(String fieldName, Reader reader) {
             PayloadData payload =  fieldToData.get(fieldName);
-            TokenStream ts = new WhitespaceTokenizer(reader);
+            TokenStream ts = new WhitespaceTokenizer(TEST_VERSION_CURRENT, reader);
             if (payload != null) {
                 if (payload.numFieldInstancesToSkip == 0) {
                     ts = new PayloadFilter(ts, payload.data, payload.offset, payload.length);
@@ -483,7 +483,7 @@
         final ByteArrayPool pool = new ByteArrayPool(numThreads, 5);
         
         Directory dir = new MockRAMDirectory();
-        final IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(), IndexWriter.MaxFieldLength.LIMITED);
+        final IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), IndexWriter.MaxFieldLength.LIMITED);
         final String field = "test";
         
         Thread[] ingesters = new Thread[numThreads];
@@ -552,6 +552,7 @@
         public boolean incrementToken() throws IOException {
             if (!first) return false;
             first = false;
+            clearAttributes();
             termAtt.setTermBuffer(term);
             payloadAtt.setPayload(new Payload(payload));
             return true;
Index: src/test/org/apache/lucene/index/TestLazyBug.java
===================================================================
--- src/test/org/apache/lucene/index/TestLazyBug.java	(revision 916567)
+++ src/test/org/apache/lucene/index/TestLazyBug.java	(working copy)
@@ -63,7 +63,7 @@
     Directory dir = new RAMDirectory();
     try {
       Random r = newRandom();
-      Analyzer analyzer = new SimpleAnalyzer();
+      Analyzer analyzer = new SimpleAnalyzer(TEST_VERSION_CURRENT);
       IndexWriter writer = new IndexWriter(dir, analyzer, true, IndexWriter.MaxFieldLength.LIMITED);
       
       writer.setUseCompoundFile(false);
Index: src/test/org/apache/lucene/index/TestIndexWriterMerging.java
===================================================================
--- src/test/org/apache/lucene/index/TestIndexWriterMerging.java	(revision 916567)
+++ src/test/org/apache/lucene/index/TestIndexWriterMerging.java	(working copy)
@@ -56,7 +56,7 @@
 
     Directory merged = new MockRAMDirectory();
 
-    IndexWriter writer = new IndexWriter(merged, new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
+    IndexWriter writer = new IndexWriter(merged, new StandardAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
     writer.setMergeFactor(2);
 
     writer.addIndexesNoOptimize(new Directory[]{indexA, indexB});
@@ -93,7 +93,7 @@
   private void fillIndex(Directory dir, int start, int numDocs) throws IOException
   {
 
-    IndexWriter writer = new IndexWriter(dir, new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
+    IndexWriter writer = new IndexWriter(dir, new StandardAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
     writer.setMergeFactor(2);
     writer.setMaxBufferedDocs(2);
 
Index: src/test/org/apache/lucene/index/TestParallelReader.java
===================================================================
--- src/test/org/apache/lucene/index/TestParallelReader.java	(revision 916567)
+++ src/test/org/apache/lucene/index/TestParallelReader.java	(working copy)
@@ -106,7 +106,7 @@
 
     // one document only:
     Directory dir2 = new MockRAMDirectory();
-    IndexWriter w2 = new IndexWriter(dir2, new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
+    IndexWriter w2 = new IndexWriter(dir2, new StandardAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
     Document d3 = new Document();
     d3.add(new Field("f3", "v1", Field.Store.YES, Field.Index.ANALYZED));
     w2.addDocument(d3);
@@ -151,13 +151,13 @@
     Directory dir2 = getDir2();
     
     // add another document to ensure that the indexes are not optimized
-    IndexWriter modifier = new IndexWriter(dir1, new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT), IndexWriter.MaxFieldLength.LIMITED);
+    IndexWriter modifier = new IndexWriter(dir1, new StandardAnalyzer(TEST_VERSION_CURRENT), IndexWriter.MaxFieldLength.LIMITED);
     Document d = new Document();
     d.add(new Field("f1", "v1", Field.Store.YES, Field.Index.ANALYZED));
     modifier.addDocument(d);
     modifier.close();
     
-    modifier = new IndexWriter(dir2, new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT), IndexWriter.MaxFieldLength.LIMITED);
+    modifier = new IndexWriter(dir2, new StandardAnalyzer(TEST_VERSION_CURRENT), IndexWriter.MaxFieldLength.LIMITED);
     d = new Document();
     d.add(new Field("f2", "v2", Field.Store.YES, Field.Index.ANALYZED));
     modifier.addDocument(d);
@@ -170,7 +170,7 @@
     assertFalse(pr.isOptimized());
     pr.close();
     
-    modifier = new IndexWriter(dir1, new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT), IndexWriter.MaxFieldLength.LIMITED);
+    modifier = new IndexWriter(dir1, new StandardAnalyzer(TEST_VERSION_CURRENT), IndexWriter.MaxFieldLength.LIMITED);
     modifier.optimize();
     modifier.close();
     
@@ -182,7 +182,7 @@
     pr.close();
 
     
-    modifier = new IndexWriter(dir2, new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT), IndexWriter.MaxFieldLength.LIMITED);
+    modifier = new IndexWriter(dir2, new StandardAnalyzer(TEST_VERSION_CURRENT), IndexWriter.MaxFieldLength.LIMITED);
     modifier.optimize();
     modifier.close();
     
@@ -233,7 +233,7 @@
   // Fields 1-4 indexed together:
   private Searcher single() throws IOException {
     Directory dir = new MockRAMDirectory();
-    IndexWriter w = new IndexWriter(dir, new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
+    IndexWriter w = new IndexWriter(dir, new StandardAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
     Document d1 = new Document();
     d1.add(new Field("f1", "v1", Field.Store.YES, Field.Index.ANALYZED));
     d1.add(new Field("f2", "v1", Field.Store.YES, Field.Index.ANALYZED));
@@ -263,7 +263,7 @@
 
   private Directory getDir1() throws IOException {
     Directory dir1 = new MockRAMDirectory();
-    IndexWriter w1 = new IndexWriter(dir1, new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
+    IndexWriter w1 = new IndexWriter(dir1, new StandardAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
     Document d1 = new Document();
     d1.add(new Field("f1", "v1", Field.Store.YES, Field.Index.ANALYZED));
     d1.add(new Field("f2", "v1", Field.Store.YES, Field.Index.ANALYZED));
@@ -278,7 +278,7 @@
 
   private Directory getDir2() throws IOException {
     Directory dir2 = new RAMDirectory();
-    IndexWriter w2 = new IndexWriter(dir2, new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
+    IndexWriter w2 = new IndexWriter(dir2, new StandardAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
     Document d3 = new Document();
     d3.add(new Field("f3", "v1", Field.Store.YES, Field.Index.ANALYZED));
     d3.add(new Field("f4", "v1", Field.Store.YES, Field.Index.ANALYZED));
Index: src/test/org/apache/lucene/index/TestDeletionPolicy.java
===================================================================
--- src/test/org/apache/lucene/index/TestDeletionPolicy.java	(revision 916567)
+++ src/test/org/apache/lucene/index/TestDeletionPolicy.java	(working copy)
@@ -201,7 +201,7 @@
 
     Directory dir = new RAMDirectory();
     ExpirationTimeDeletionPolicy policy = new ExpirationTimeDeletionPolicy(dir, SECONDS);
-    IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(), true, policy, IndexWriter.MaxFieldLength.UNLIMITED);
+    IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, policy, IndexWriter.MaxFieldLength.UNLIMITED);
     writer.setUseCompoundFile(useCompoundFile);
     writer.close();
 
@@ -210,7 +210,7 @@
       // Record last time when writer performed deletes of
       // past commits
       lastDeleteTime = System.currentTimeMillis();
-      writer = new IndexWriter(dir, new WhitespaceAnalyzer(), false, policy, IndexWriter.MaxFieldLength.UNLIMITED);
+      writer = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), false, policy, IndexWriter.MaxFieldLength.UNLIMITED);
       writer.setUseCompoundFile(useCompoundFile);
       for(int j=0;j<17;j++) {
         addDoc(writer);
@@ -271,7 +271,7 @@
       Directory dir = new RAMDirectory();
       policy.dir = dir;
 
-      IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(), true, policy, IndexWriter.MaxFieldLength.UNLIMITED);
+      IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, policy, IndexWriter.MaxFieldLength.UNLIMITED);
       writer.setMaxBufferedDocs(10);
       writer.setUseCompoundFile(useCompoundFile);
       writer.setMergeScheduler(new SerialMergeScheduler());
@@ -280,7 +280,7 @@
       }
       writer.close();
 
-      writer = new IndexWriter(dir, new WhitespaceAnalyzer(), false, policy, IndexWriter.MaxFieldLength.UNLIMITED);
+      writer = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), false, policy, IndexWriter.MaxFieldLength.UNLIMITED);
       writer.setUseCompoundFile(useCompoundFile);
       writer.optimize();
       writer.close();
@@ -318,7 +318,7 @@
           // Open & close a writer and assert that it
           // actually removed something:
           int preCount = dir.listAll().length;
-          writer = new IndexWriter(dir, new WhitespaceAnalyzer(), false, policy, IndexWriter.MaxFieldLength.LIMITED);
+          writer = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), false, policy, IndexWriter.MaxFieldLength.LIMITED);
           writer.close();
           int postCount = dir.listAll().length;
           assertTrue(postCount < preCount);
@@ -340,7 +340,7 @@
     Directory dir = new MockRAMDirectory();
     policy.dir = dir;
 
-    IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(), policy, IndexWriter.MaxFieldLength.LIMITED);
+    IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), policy, IndexWriter.MaxFieldLength.LIMITED);
     writer.setMaxBufferedDocs(2);
     for(int i=0;i<10;i++) {
       addDoc(writer);
@@ -359,7 +359,7 @@
     assertTrue(lastCommit != null);
 
     // Now add 1 doc and optimize
-    writer = new IndexWriter(dir, new WhitespaceAnalyzer(), policy, IndexWriter.MaxFieldLength.LIMITED);
+    writer = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), policy, IndexWriter.MaxFieldLength.LIMITED);
     addDoc(writer);
     assertEquals(11, writer.numDocs());
     writer.optimize();
@@ -368,7 +368,7 @@
     assertEquals(7, IndexReader.listCommits(dir).size());
 
     // Now open writer on the commit just before optimize:
-    writer = new IndexWriter(dir, new WhitespaceAnalyzer(), policy, IndexWriter.MaxFieldLength.LIMITED, lastCommit);
+    writer = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), policy, IndexWriter.MaxFieldLength.LIMITED, lastCommit);
     assertEquals(10, writer.numDocs());
 
     // Should undo our rollback:
@@ -380,7 +380,7 @@
     assertEquals(11, r.numDocs());
     r.close();
 
-    writer = new IndexWriter(dir, new WhitespaceAnalyzer(), policy, IndexWriter.MaxFieldLength.LIMITED, lastCommit);
+    writer = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), policy, IndexWriter.MaxFieldLength.LIMITED, lastCommit);
     assertEquals(10, writer.numDocs());
     // Commits the rollback:
     writer.close();
@@ -396,7 +396,7 @@
     r.close();
 
     // Reoptimize
-    writer = new IndexWriter(dir, new WhitespaceAnalyzer(), policy, IndexWriter.MaxFieldLength.LIMITED);
+    writer = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), policy, IndexWriter.MaxFieldLength.LIMITED);
     writer.optimize();
     writer.close();
 
@@ -407,7 +407,7 @@
 
     // Now open writer on the commit just before optimize,
     // but this time keeping only the last commit:
-    writer = new IndexWriter(dir, new WhitespaceAnalyzer(), new KeepOnlyLastCommitDeletionPolicy(), IndexWriter.MaxFieldLength.LIMITED, lastCommit);
+    writer = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), new KeepOnlyLastCommitDeletionPolicy(), IndexWriter.MaxFieldLength.LIMITED, lastCommit);
     assertEquals(10, writer.numDocs());
     
     // Reader still sees optimized index, because writer
@@ -443,7 +443,7 @@
 
       Directory dir = new RAMDirectory();
 
-      IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(), true, policy, IndexWriter.MaxFieldLength.UNLIMITED);
+      IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, policy, IndexWriter.MaxFieldLength.UNLIMITED);
       writer.setMaxBufferedDocs(10);
       writer.setUseCompoundFile(useCompoundFile);
       for(int i=0;i<107;i++) {
@@ -451,7 +451,7 @@
       }
       writer.close();
 
-      writer = new IndexWriter(dir, new WhitespaceAnalyzer(), false, policy, IndexWriter.MaxFieldLength.UNLIMITED);
+      writer = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), false, policy, IndexWriter.MaxFieldLength.UNLIMITED);
       writer.setUseCompoundFile(useCompoundFile);
       writer.optimize();
       writer.close();
@@ -486,7 +486,7 @@
       KeepLastNDeletionPolicy policy = new KeepLastNDeletionPolicy(N);
 
       for(int j=0;j<N+1;j++) {
-        IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(), true, policy, IndexWriter.MaxFieldLength.UNLIMITED);
+        IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, policy, IndexWriter.MaxFieldLength.UNLIMITED);
         writer.setMaxBufferedDocs(10);
         writer.setUseCompoundFile(useCompoundFile);
         for(int i=0;i<17;i++) {
@@ -541,14 +541,14 @@
       KeepLastNDeletionPolicy policy = new KeepLastNDeletionPolicy(N);
 
       Directory dir = new RAMDirectory();
-      IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(), true, policy, IndexWriter.MaxFieldLength.UNLIMITED);
+      IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, policy, IndexWriter.MaxFieldLength.UNLIMITED);
       writer.setUseCompoundFile(useCompoundFile);
       writer.close();
       Term searchTerm = new Term("content", "aaa");        
       Query query = new TermQuery(searchTerm);
 
       for(int i=0;i<N+1;i++) {
-        writer = new IndexWriter(dir, new WhitespaceAnalyzer(), false, policy, IndexWriter.MaxFieldLength.UNLIMITED);
+        writer = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), false, policy, IndexWriter.MaxFieldLength.UNLIMITED);
         writer.setUseCompoundFile(useCompoundFile);
         for(int j=0;j<17;j++) {
           addDoc(writer);
@@ -565,7 +565,7 @@
         reader.close();
         searcher.close();
       }
-      writer = new IndexWriter(dir, new WhitespaceAnalyzer(), false, policy, IndexWriter.MaxFieldLength.UNLIMITED);
+      writer = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), false, policy, IndexWriter.MaxFieldLength.UNLIMITED);
       writer.setUseCompoundFile(useCompoundFile);
       writer.optimize();
       // this is a commit
@@ -636,7 +636,7 @@
       KeepLastNDeletionPolicy policy = new KeepLastNDeletionPolicy(N);
 
       Directory dir = new RAMDirectory();
-      IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(), true, policy, IndexWriter.MaxFieldLength.UNLIMITED);
+      IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, policy, IndexWriter.MaxFieldLength.UNLIMITED);
       writer.setMaxBufferedDocs(10);
       writer.setUseCompoundFile(useCompoundFile);
       writer.close();
@@ -645,7 +645,7 @@
 
       for(int i=0;i<N+1;i++) {
 
-        writer = new IndexWriter(dir, new WhitespaceAnalyzer(), false, policy, IndexWriter.MaxFieldLength.UNLIMITED);
+        writer = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), false, policy, IndexWriter.MaxFieldLength.UNLIMITED);
         writer.setMaxBufferedDocs(10);
         writer.setUseCompoundFile(useCompoundFile);
         for(int j=0;j<17;j++) {
@@ -663,7 +663,7 @@
         reader.close();
         searcher.close();
 
-        writer = new IndexWriter(dir, new WhitespaceAnalyzer(), true, policy, IndexWriter.MaxFieldLength.UNLIMITED);
+        writer = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, policy, IndexWriter.MaxFieldLength.UNLIMITED);
         // This will not commit: there are no changes
         // pending because we opened for "create":
         writer.close();
Index: src/test/org/apache/lucene/index/TestParallelReaderEmptyIndex.java
===================================================================
--- src/test/org/apache/lucene/index/TestParallelReaderEmptyIndex.java	(revision 916567)
+++ src/test/org/apache/lucene/index/TestParallelReaderEmptyIndex.java	(working copy)
@@ -47,7 +47,7 @@
    */
   public void testEmptyIndex() throws IOException {
     RAMDirectory rd1 = new MockRAMDirectory();
-    IndexWriter iw = new IndexWriter(rd1, new SimpleAnalyzer(), true,
+    IndexWriter iw = new IndexWriter(rd1, new SimpleAnalyzer(TEST_VERSION_CURRENT), true,
                                      MaxFieldLength.UNLIMITED);
     iw.close();
 
@@ -55,7 +55,7 @@
 
     RAMDirectory rdOut = new MockRAMDirectory();
 
-    IndexWriter iwOut = new IndexWriter(rdOut, new SimpleAnalyzer(), true,
+    IndexWriter iwOut = new IndexWriter(rdOut, new SimpleAnalyzer(TEST_VERSION_CURRENT), true,
                                         MaxFieldLength.UNLIMITED);
     ParallelReader pr = new ParallelReader();
     pr.add(IndexReader.open(rd1,true));
@@ -80,7 +80,7 @@
   public void testEmptyIndexWithVectors() throws IOException {
     RAMDirectory rd1 = new MockRAMDirectory();
     {
-      IndexWriter iw = new IndexWriter(rd1, new SimpleAnalyzer(), true,
+      IndexWriter iw = new IndexWriter(rd1, new SimpleAnalyzer(TEST_VERSION_CURRENT), true,
                                        MaxFieldLength.UNLIMITED);
       Document doc = new Document();
       doc.add(new Field("test", "", Store.NO, Index.ANALYZED,
@@ -95,7 +95,7 @@
       ir.deleteDocument(0);
       ir.close();
 
-      iw = new IndexWriter(rd1, new SimpleAnalyzer(), false,
+      iw = new IndexWriter(rd1, new SimpleAnalyzer(TEST_VERSION_CURRENT), false,
                            MaxFieldLength.UNLIMITED);
       iw.optimize();
       iw.close();
@@ -103,7 +103,7 @@
 
     RAMDirectory rd2 = new MockRAMDirectory();
     {
-      IndexWriter iw = new IndexWriter(rd2, new SimpleAnalyzer(), true,
+      IndexWriter iw = new IndexWriter(rd2, new SimpleAnalyzer(TEST_VERSION_CURRENT), true,
                                        MaxFieldLength.UNLIMITED);
       Document doc = new Document();
       iw.addDocument(doc);
@@ -112,7 +112,7 @@
 
     RAMDirectory rdOut = new MockRAMDirectory();
 
-    IndexWriter iwOut = new IndexWriter(rdOut, new SimpleAnalyzer(), true,
+    IndexWriter iwOut = new IndexWriter(rdOut, new SimpleAnalyzer(TEST_VERSION_CURRENT), true,
                                         MaxFieldLength.UNLIMITED);
     ParallelReader pr = new ParallelReader();
     pr.add(IndexReader.open(rd1,true));
Index: src/test/org/apache/lucene/index/TestBackwardsCompatibility.java
===================================================================
--- src/test/org/apache/lucene/index/TestBackwardsCompatibility.java	(revision 916567)
+++ src/test/org/apache/lucene/index/TestBackwardsCompatibility.java	(working copy)
@@ -224,7 +224,7 @@
         hasTested29++;
       }
 
-      IndexWriter w = new IndexWriter(dir, new WhitespaceAnalyzer(), IndexWriter.MaxFieldLength.LIMITED);
+      IndexWriter w = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), IndexWriter.MaxFieldLength.LIMITED);
       w.optimize();
       FlexTestUtil.verifyFlexVsPreFlex(rand, w);
       w.close();
@@ -365,7 +365,7 @@
     Directory dir = FSDirectory.open(new File(dirName));
 
     // open writer
-    IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(), false, IndexWriter.MaxFieldLength.UNLIMITED);
+    IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), false, IndexWriter.MaxFieldLength.UNLIMITED);
 
     // add 10 docs
     for(int i=0;i<10;i++) {
@@ -409,7 +409,7 @@
     searcher.close();
 
     // optimize
-    writer = new IndexWriter(dir, new WhitespaceAnalyzer(), false, IndexWriter.MaxFieldLength.UNLIMITED);
+    writer = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), false, IndexWriter.MaxFieldLength.UNLIMITED);
     writer.optimize();
     writer.close();
 
@@ -459,7 +459,7 @@
     searcher.close();
 
     // optimize
-    IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(), false, IndexWriter.MaxFieldLength.UNLIMITED);
+    IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), false, IndexWriter.MaxFieldLength.UNLIMITED);
     writer.optimize();
     writer.close();
 
@@ -481,7 +481,7 @@
     dirName = fullDir(dirName);
 
     Directory dir = FSDirectory.open(new File(dirName));
-    IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
+    IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
     writer.setUseCompoundFile(doCFS);
     writer.setMaxBufferedDocs(10);
     
@@ -492,7 +492,7 @@
     writer.close();
 
     // open fresh writer so we get no prx file in the added segment
-    writer = new IndexWriter(dir, new WhitespaceAnalyzer(), IndexWriter.MaxFieldLength.LIMITED);
+    writer = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), IndexWriter.MaxFieldLength.LIMITED);
     writer.setUseCompoundFile(doCFS);
     writer.setMaxBufferedDocs(10);
     addNoProxDoc(writer);
@@ -519,7 +519,7 @@
     try {
       Directory dir = FSDirectory.open(new File(fullDir(outputDir)));
 
-      IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.UNLIMITED);
+      IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.UNLIMITED);
       writer.setRAMBufferSizeMB(16.0);
       for(int i=0;i<35;i++) {
         addDoc(writer, i);

Property changes on: src\test\org\apache\lucene\index\TestBackwardsCompatibility.java
___________________________________________________________________
Modified: svn:mergeinfo
   Merged /lucene/java/branches/lucene_2_9/src/test/org/apache/lucene/index/TestBackwardsCompatibility.java:r909334
   Merged /lucene/java/trunk/src/test/org/apache/lucene/index/TestBackwardsCompatibility.java:r889614-889621,889623-889666,889668-889865,899002-916543

Index: src/test/org/apache/lucene/index/TestLazyProxSkipping.java
===================================================================
--- src/test/org/apache/lucene/index/TestLazyProxSkipping.java	(revision 916567)
+++ src/test/org/apache/lucene/index/TestLazyProxSkipping.java	(working copy)
@@ -61,7 +61,7 @@
         int numDocs = 500;
         
         Directory directory = new SeekCountingDirectory();
-        IndexWriter writer = new IndexWriter(directory, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
+        IndexWriter writer = new IndexWriter(directory, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
         writer.setUseCompoundFile(false);
         writer.setMaxBufferedDocs(10);
         for (int i = 0; i < numDocs; i++) {
@@ -119,7 +119,7 @@
     
     public void testSeek() throws IOException {
         Directory directory = new RAMDirectory();
-        IndexWriter writer = new IndexWriter(directory, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
+        IndexWriter writer = new IndexWriter(directory, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
         for (int i = 0; i < 10; i++) {
             Document doc = new Document();
             doc.add(new Field(this.field, "a b", Field.Store.YES, Field.Index.ANALYZED));
Index: src/test/org/apache/lucene/index/TestNorms.java
===================================================================
--- src/test/org/apache/lucene/index/TestNorms.java	(revision 916567)
+++ src/test/org/apache/lucene/index/TestNorms.java	(working copy)
@@ -28,10 +28,8 @@
 import org.apache.lucene.search.DefaultSimilarity;
 import org.apache.lucene.search.Similarity;
 import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.FSDirectory;
-import org.apache.lucene.util._TestUtil;
+import org.apache.lucene.store.RAMDirectory;
 
-import java.io.File;
 import java.io.IOException;
 import java.util.ArrayList;
 
@@ -66,7 +64,7 @@
   protected void setUp() throws Exception {
     super.setUp();
     similarityOne = new SimilarityOne();
-    anlzr = new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT);
+    anlzr = new StandardAnalyzer(TEST_VERSION_CURRENT);
   }
 
   /**
@@ -76,9 +74,7 @@
    * Including optimize. 
    */
   public void testNorms() throws IOException {
-    // test with a single index: index1
-    File indexDir1 = _TestUtil.getTempDir("lucenetestindex1");
-    Directory dir1 = FSDirectory.open(indexDir1);
+    Directory dir1 = new RAMDirectory();
 
     norms = new ArrayList<Float>();
     modifiedNorms = new ArrayList<Float>();
@@ -95,15 +91,13 @@
     modifiedNorms = new ArrayList<Float>();
     numDocNorms = 0;
     
-    File indexDir2 = _TestUtil.getTempDir("lucenetestindex2");
-    Directory dir2 = FSDirectory.open(indexDir2);
+    Directory dir2 = new RAMDirectory();
 
     createIndex(dir2);
     doTestNorms(dir2);
 
     // add index1 and index2 to a third index: index3
-    File indexDir3 = _TestUtil.getTempDir("lucenetestindex3");
-    Directory dir3 = FSDirectory.open(indexDir3);
+    Directory dir3 = new RAMDirectory();
 
     createIndex(dir3);
     IndexWriter iw = new IndexWriter(dir3,anlzr,false, IndexWriter.MaxFieldLength.LIMITED);
@@ -134,9 +128,6 @@
     dir1.close();
     dir2.close();
     dir3.close();
-    _TestUtil.rmDir(indexDir1);
-    _TestUtil.rmDir(indexDir2);
-    _TestUtil.rmDir(indexDir3);
   }
 
   private void doTestNorms(Directory dir) throws IOException {
Index: src/test/org/apache/lucene/TestSearchForDuplicates.java
===================================================================
--- src/test/org/apache/lucene/TestSearchForDuplicates.java	(revision 916567)
+++ src/test/org/apache/lucene/TestSearchForDuplicates.java	(working copy)
@@ -27,8 +27,6 @@
 import org.apache.lucene.index.*;
 import org.apache.lucene.search.*;
 import org.apache.lucene.queryParser.*;
-import org.apache.lucene.util.Version;
-
 import org.apache.lucene.util.LuceneTestCase;
 import junit.framework.TestSuite;
 import junit.textui.TestRunner;
@@ -79,7 +77,7 @@
 
   private void doTest(PrintWriter out, boolean useCompoundFiles) throws Exception {
       Directory directory = new RAMDirectory();
-      Analyzer analyzer = new SimpleAnalyzer();
+      Analyzer analyzer = new SimpleAnalyzer(TEST_VERSION_CURRENT);
       IndexWriter writer = new IndexWriter(directory, analyzer, true,
                                            IndexWriter.MaxFieldLength.LIMITED);
 
@@ -101,7 +99,7 @@
       // try a search without OR
       Searcher searcher = new IndexSearcher(directory, true);
 
-      QueryParser parser = new QueryParser(Version.LUCENE_CURRENT, PRIORITY_FIELD, analyzer);
+      QueryParser parser = new QueryParser(TEST_VERSION_CURRENT, PRIORITY_FIELD, analyzer);
 
       Query query = parser.parse(HIGH_PRIORITY);
       out.println("Query: " + query.toString(PRIORITY_FIELD));
@@ -116,7 +114,7 @@
       searcher = new IndexSearcher(directory, true);
       hits = null;
 
-      parser = new QueryParser(Version.LUCENE_CURRENT, PRIORITY_FIELD, analyzer);
+      parser = new QueryParser(TEST_VERSION_CURRENT, PRIORITY_FIELD, analyzer);
 
       query = parser.parse(HIGH_PRIORITY + " OR " + MED_PRIORITY);
       out.println("Query: " + query.toString(PRIORITY_FIELD));
Index: src/test/org/apache/lucene/util/LuceneTestCaseJ4.java
===================================================================
--- src/test/org/apache/lucene/util/LuceneTestCaseJ4.java	(revision 916567)
+++ src/test/org/apache/lucene/util/LuceneTestCaseJ4.java	(working copy)
@@ -18,6 +18,7 @@
  */
 
 import org.apache.lucene.index.ConcurrentMergeScheduler;
+import org.apache.lucene.search.BooleanQuery;
 import org.apache.lucene.search.FieldCache;
 import org.apache.lucene.search.FieldCache.CacheEntry;
 import org.apache.lucene.util.FieldCacheSanityChecker.Insanity;
@@ -30,6 +31,9 @@
 import java.util.Arrays;
 import java.util.Iterator;
 import java.util.Random;
+import java.util.ArrayList;
+import java.util.List;
+import java.util.Collections;
 
 import static org.junit.Assert.assertEquals;
 import static org.junit.Assert.fail;
@@ -73,6 +77,24 @@
 //@RunWith(RunBareWrapper.class)
 public class LuceneTestCaseJ4 extends TestWatchman {
 
+  /** Change this when development starts for new Lucene version: */
+  public static final Version TEST_VERSION_CURRENT = Version.LUCENE_31;
+
+  private int savedBoolMaxClauseCount;
+
+  private volatile Thread.UncaughtExceptionHandler savedUncaughtExceptionHandler = null;
+  
+  private static class UncaughtExceptionEntry {
+    public final Thread thread;
+    public final Throwable exception;
+    
+    public UncaughtExceptionEntry(Thread thread, Throwable exception) {
+      this.thread = thread;
+      this.exception = exception;
+    }
+  }
+  private List<UncaughtExceptionEntry> uncaughtExceptions = Collections.synchronizedList(new ArrayList<UncaughtExceptionEntry>());
+
   // This is how we get control when errors occur.
   // Think of this as start/end/success/failed
   // events.
@@ -88,7 +110,17 @@
 
   @Before
   public void setUp() throws Exception {
+    savedUncaughtExceptionHandler = Thread.getDefaultUncaughtExceptionHandler();
+    Thread.setDefaultUncaughtExceptionHandler(new Thread.UncaughtExceptionHandler() {
+      public void uncaughtException(Thread t, Throwable e) {
+        uncaughtExceptions.add(new UncaughtExceptionEntry(t, e));
+        if (savedUncaughtExceptionHandler != null)
+          savedUncaughtExceptionHandler.uncaughtException(t, e);
+      }
+    });
+    
     ConcurrentMergeScheduler.setTestMode();
+    savedBoolMaxClauseCount = BooleanQuery.getMaxClauseCount();
     seed = null;
   }
 
@@ -114,6 +146,7 @@
 
   @After
   public void tearDown() throws Exception {
+    BooleanQuery.setMaxClauseCount(savedBoolMaxClauseCount);
     try {
       // this isn't as useful as calling directly from the scope where the
       // index readers are used, because they could be gc'ed just before
@@ -130,6 +163,16 @@
     } finally {
       purgeFieldCache(FieldCache.DEFAULT);
     }
+    
+    Thread.setDefaultUncaughtExceptionHandler(savedUncaughtExceptionHandler);
+    if (!uncaughtExceptions.isEmpty()) {
+      System.err.println("The following exceptions were thrown by threads:");
+      for (UncaughtExceptionEntry entry : uncaughtExceptions) {
+        System.err.println("*** Thread: " + entry.thread.getName() + " ***");
+        entry.exception.printStackTrace(System.err);
+      }
+      fail("Some threads throwed uncaught exceptions!");
+    }
   }
 
   /**
@@ -214,7 +257,7 @@
    */
   public Random newRandom() {
     if (seed != null) {
-      throw new IllegalStateException("please call LuceneTestCase.newRandom only once per test");
+      throw new IllegalStateException("please call LuceneTestCaseJ4.newRandom only once per test");
     }
     return newRandom(seedRnd.nextLong());
   }
@@ -226,7 +269,7 @@
    */
   public Random newRandom(long seed) {
     if (this.seed != null) {
-      throw new IllegalStateException("please call LuceneTestCase.newRandom only once per test");
+      throw new IllegalStateException("please call LuceneTestCaseJ4.newRandom only once per test");
     }
     this.seed = Long.valueOf(seed);
     return new Random(seed);
Index: src/test/org/apache/lucene/util/LuceneTestCase.java
===================================================================
--- src/test/org/apache/lucene/util/LuceneTestCase.java	(revision 916567)
+++ src/test/org/apache/lucene/util/LuceneTestCase.java	(working copy)
@@ -21,10 +21,14 @@
 import java.util.Arrays;
 import java.util.Iterator;
 import java.util.Random;
+import java.util.ArrayList;
+import java.util.List;
+import java.util.Collections;
 
 import junit.framework.TestCase;
 
 import org.apache.lucene.index.ConcurrentMergeScheduler;
+import org.apache.lucene.search.BooleanQuery;
 import org.apache.lucene.search.FieldCache;
 import org.apache.lucene.search.FieldCache.CacheEntry;
 import org.apache.lucene.util.FieldCacheSanityChecker.Insanity;
@@ -53,6 +57,23 @@
 @Deprecated
 public abstract class LuceneTestCase extends TestCase {
 
+  public static final Version TEST_VERSION_CURRENT = LuceneTestCaseJ4.TEST_VERSION_CURRENT;
+
+  private int savedBoolMaxClauseCount;
+  
+  private volatile Thread.UncaughtExceptionHandler savedUncaughtExceptionHandler = null;
+  
+  private static class UncaughtExceptionEntry {
+    public final Thread thread;
+    public final Throwable exception;
+    
+    public UncaughtExceptionEntry(Thread thread, Throwable exception) {
+      this.thread = thread;
+      this.exception = exception;
+    }
+  }
+  private List<UncaughtExceptionEntry> uncaughtExceptions = Collections.synchronizedList(new ArrayList<UncaughtExceptionEntry>());
+
   public LuceneTestCase() {
     super();
   }
@@ -64,7 +85,18 @@
   @Override
   protected void setUp() throws Exception {
     super.setUp();
+    
+    savedUncaughtExceptionHandler = Thread.getDefaultUncaughtExceptionHandler();
+    Thread.setDefaultUncaughtExceptionHandler(new Thread.UncaughtExceptionHandler() {
+      public void uncaughtException(Thread t, Throwable e) {
+        uncaughtExceptions.add(new UncaughtExceptionEntry(t, e));
+        if (savedUncaughtExceptionHandler != null)
+          savedUncaughtExceptionHandler.uncaughtException(t, e);
+      }
+    });
+    
     ConcurrentMergeScheduler.setTestMode();
+    savedBoolMaxClauseCount = BooleanQuery.getMaxClauseCount();
   }
 
   /**
@@ -87,6 +119,7 @@
 
   @Override
   protected void tearDown() throws Exception {
+    BooleanQuery.setMaxClauseCount(savedBoolMaxClauseCount);
     try {
       // this isn't as useful as calling directly from the scope where the 
       // index readers are used, because they could be gc'ed just before
@@ -104,6 +137,16 @@
       purgeFieldCache(FieldCache.DEFAULT);
     }
     
+    Thread.setDefaultUncaughtExceptionHandler(savedUncaughtExceptionHandler);
+    if (!uncaughtExceptions.isEmpty()) {
+      System.err.println("The following exceptions were thrown by threads:");
+      for (UncaughtExceptionEntry entry : uncaughtExceptions) {
+        System.err.println("*** Thread: " + entry.thread.getName() + " ***");
+        entry.exception.printStackTrace(System.err);
+      }
+      fail("Some threads throwed uncaught exceptions!");
+    }
+    
     super.tearDown();
   }
 
Index: src/test/org/apache/lucene/util/TestIndexableBinaryStringTools.java
===================================================================
--- src/test/org/apache/lucene/util/TestIndexableBinaryStringTools.java	(revision 916567)
+++ src/test/org/apache/lucene/util/TestIndexableBinaryStringTools.java	(working copy)
@@ -164,14 +164,14 @@
       int encodedLen1 = IndexableBinaryStringTools.getEncodedLength(
           originalArray1, 0, numBytes1);
       if (encodedLen1 > encoded1.length)
-        encoded1 = new char[ArrayUtil.getNextSize(encodedLen1)];
+        encoded1 = new char[ArrayUtil.oversize(encodedLen1, RamUsageEstimator.NUM_BYTES_CHAR)];
       IndexableBinaryStringTools.encode(originalArray1, 0, numBytes1, encoded1,
           0, encodedLen1);
 
       int encodedLen2 = IndexableBinaryStringTools.getEncodedLength(original2,
           0, numBytes2);
       if (encodedLen2 > encoded2.length)
-        encoded2 = new char[ArrayUtil.getNextSize(encodedLen2)];
+        encoded2 = new char[ArrayUtil.oversize(encodedLen2, RamUsageEstimator.NUM_BYTES_CHAR)];
       IndexableBinaryStringTools.encode(original2, 0, numBytes2, encoded2, 0,
           encodedLen2);
 
@@ -308,7 +308,7 @@
       int encodedLen = IndexableBinaryStringTools.getEncodedLength(binary, 0,
           numBytes);
       if (encoded.length < encodedLen)
-        encoded = new char[ArrayUtil.getNextSize(encodedLen)];
+        encoded = new char[ArrayUtil.oversize(encodedLen, RamUsageEstimator.NUM_BYTES_CHAR)];
       IndexableBinaryStringTools.encode(binary, 0, numBytes, encoded, 0,
           encodedLen);
 
Index: src/test/org/apache/lucene/util/ArrayUtilTest.java
===================================================================
--- src/test/org/apache/lucene/util/ArrayUtilTest.java	(revision 916567)
+++ src/test/org/apache/lucene/util/ArrayUtilTest.java	(working copy)
@@ -1,74 +0,0 @@
-package org.apache.lucene.util;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.util.LuceneTestCase;
-
-
-/**
- *
- *
- **/
-public class ArrayUtilTest extends LuceneTestCase {
-
-  public void testParseInt() throws Exception {
-    int test;
-    try {
-      test = ArrayUtil.parseInt("".toCharArray());
-      assertTrue(false);
-    } catch (NumberFormatException e) {
-      //expected
-    }
-    try {
-      test = ArrayUtil.parseInt("foo".toCharArray());
-      assertTrue(false);
-    } catch (NumberFormatException e) {
-      //expected
-    }
-    try {
-      test = ArrayUtil.parseInt(String.valueOf(Long.MAX_VALUE).toCharArray());
-      assertTrue(false);
-    } catch (NumberFormatException e) {
-      //expected
-    }
-    try {
-      test = ArrayUtil.parseInt("0.34".toCharArray());
-      assertTrue(false);
-    } catch (NumberFormatException e) {
-      //expected
-    }
-
-    try {
-      test = ArrayUtil.parseInt("1".toCharArray());
-      assertTrue(test + " does not equal: " + 1, test == 1);
-      test = ArrayUtil.parseInt("-10000".toCharArray());
-      assertTrue(test + " does not equal: " + -10000, test == -10000);
-      test = ArrayUtil.parseInt("1923".toCharArray());
-      assertTrue(test + " does not equal: " + 1923, test == 1923);
-      test = ArrayUtil.parseInt("-1".toCharArray());
-      assertTrue(test + " does not equal: " + -1, test == -1);
-      test = ArrayUtil.parseInt("foo 1923 bar".toCharArray(), 4, 4);
-      assertTrue(test + " does not equal: " + 1923, test == 1923);
-    } catch (NumberFormatException e) {
-      e.printStackTrace();
-      assertTrue(false);
-    }
-
-  }
-
-}
Index: src/test/org/apache/lucene/util/TestOpenBitSet.java
===================================================================
--- src/test/org/apache/lucene/util/TestOpenBitSet.java	(revision 916567)
+++ src/test/org/apache/lucene/util/TestOpenBitSet.java	(working copy)
@@ -22,9 +22,6 @@
 
 import org.apache.lucene.search.DocIdSetIterator;
 
-/**
- * @version $Id$
- */
 public class TestOpenBitSet extends LuceneTestCase {
   Random rand;
 
@@ -233,7 +230,14 @@
     }
   }
 
-  
+  public void testHashCodeEquals() {
+    OpenBitSet bs1 = new OpenBitSet(200);
+    OpenBitSet bs2 = new OpenBitSet(64);
+    bs1.set(3);
+    bs2.set(3);       
+    assertEquals(bs1, bs2);
+    assertEquals(bs1.hashCode(), bs2.hashCode());
+  } 
 }
 
 

Property changes on: src\test\org\apache\lucene\util\TestAttributeSource.java
___________________________________________________________________
Added: svn:mergeinfo
   Merged /lucene/java/branches/lucene_2_9/src/test/org/apache/lucene/util/TestAttributeSource.java:r909334

Index: src/test/org/apache/lucene/util/TestFieldCacheSanityChecker.java
===================================================================
--- src/test/org/apache/lucene/util/TestFieldCacheSanityChecker.java	(revision 916567)
+++ src/test/org/apache/lucene/util/TestFieldCacheSanityChecker.java	(working copy)
@@ -45,9 +45,9 @@
     RAMDirectory dirA = new RAMDirectory();
     RAMDirectory dirB = new RAMDirectory();
 
-    IndexWriter wA = new IndexWriter(dirA, new WhitespaceAnalyzer(), true, 
+    IndexWriter wA = new IndexWriter(dirA, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, 
                                      IndexWriter.MaxFieldLength.LIMITED);
-    IndexWriter wB = new IndexWriter(dirB, new WhitespaceAnalyzer(), true, 
+    IndexWriter wB = new IndexWriter(dirB, new WhitespaceAnalyzer(TEST_VERSION_CURRENT), true, 
                                      IndexWriter.MaxFieldLength.LIMITED);
 
     long theLong = Long.MAX_VALUE;
Index: src/test/org/apache/lucene/util/English.java
===================================================================
--- src/test/org/apache/lucene/util/English.java	(revision 916567)
+++ src/test/org/apache/lucene/util/English.java	(working copy)
@@ -1,186 +0,0 @@
-package org.apache.lucene.util;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- * <p/>
- * http://www.apache.org/licenses/LICENSE-2.0
- * <p/>
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-
-public class English {
-
-  public static String longToEnglish(long i) {
-    StringBuilder result = new StringBuilder();
-    longToEnglish(i, result);
-    return result.toString();
-  }
-
-  public static void longToEnglish(long i, StringBuilder result) {
-    if (i == 0) {
-      result.append("zero");
-      return;
-    }
-    if (i < 0) {
-      result.append("minus ");
-      i = -i;
-    }
-    if (i >= 1000000000000000000l) {        // quadrillion
-      longToEnglish(i / 1000000000000000000l, result);
-      result.append("quintillion, ");
-      i = i % 1000000000000000000l;
-    }
-    if (i >= 1000000000000000l) {        // quadrillion
-      longToEnglish(i / 1000000000000000l, result);
-      result.append("quadrillion, ");
-      i = i % 1000000000000000l;
-    }
-    if (i >= 1000000000000l) {        // trillions
-      longToEnglish(i / 1000000000000l, result);
-      result.append("trillion, ");
-      i = i % 1000000000000l;
-    }
-    if (i >= 1000000000) {        // billions
-      longToEnglish(i / 1000000000, result);
-      result.append("billion, ");
-      i = i % 1000000000;
-    }
-    if (i >= 1000000) {          // millions
-      longToEnglish(i / 1000000, result);
-      result.append("million, ");
-      i = i % 1000000;
-    }
-    if (i >= 1000) {          // thousands
-      longToEnglish(i / 1000, result);
-      result.append("thousand, ");
-      i = i % 1000;
-    }
-    if (i >= 100) {          // hundreds
-      longToEnglish(i / 100, result);
-      result.append("hundred ");
-      i = i % 100;
-    }
-    //we know we are smaller here so we can cast
-    if (i >= 20) {
-      switch (((int) i) / 10) {
-        case 9:
-          result.append("ninety");
-          break;
-        case 8:
-          result.append("eighty");
-          break;
-        case 7:
-          result.append("seventy");
-          break;
-        case 6:
-          result.append("sixty");
-          break;
-        case 5:
-          result.append("fifty");
-          break;
-        case 4:
-          result.append("forty");
-          break;
-        case 3:
-          result.append("thirty");
-          break;
-        case 2:
-          result.append("twenty");
-          break;
-      }
-      i = i % 10;
-      if (i == 0)
-        result.append(" ");
-      else
-        result.append("-");
-    }
-    switch ((int) i) {
-      case 19:
-        result.append("nineteen ");
-        break;
-      case 18:
-        result.append("eighteen ");
-        break;
-      case 17:
-        result.append("seventeen ");
-        break;
-      case 16:
-        result.append("sixteen ");
-        break;
-      case 15:
-        result.append("fifteen ");
-        break;
-      case 14:
-        result.append("fourteen ");
-        break;
-      case 13:
-        result.append("thirteen ");
-        break;
-      case 12:
-        result.append("twelve ");
-        break;
-      case 11:
-        result.append("eleven ");
-        break;
-      case 10:
-        result.append("ten ");
-        break;
-      case 9:
-        result.append("nine ");
-        break;
-      case 8:
-        result.append("eight ");
-        break;
-      case 7:
-        result.append("seven ");
-        break;
-      case 6:
-        result.append("six ");
-        break;
-      case 5:
-        result.append("five ");
-        break;
-      case 4:
-        result.append("four ");
-        break;
-      case 3:
-        result.append("three ");
-        break;
-      case 2:
-        result.append("two ");
-        break;
-      case 1:
-        result.append("one ");
-        break;
-      case 0:
-        result.append("");
-        break;
-    }
-  }
-
-
-  public static String intToEnglish(int i) {
-    StringBuilder result = new StringBuilder();
-    longToEnglish(i, result);
-    return result.toString();
-  }
-
-  public static void intToEnglish(int i, StringBuilder result) {
-    longToEnglish(i, result);
-  }
-
-  public static void main(String[] args) {
-    System.out.println(longToEnglish(Long.parseLong(args[0])));
-  }
-
-}
Index: src/test/org/apache/lucene/util/TestRamUsageEstimator.java
===================================================================
--- src/test/org/apache/lucene/util/TestRamUsageEstimator.java	(revision 916567)
+++ src/test/org/apache/lucene/util/TestRamUsageEstimator.java	(working copy)
@@ -25,20 +25,20 @@
     String string = new String("test str");
     RamUsageEstimator rue = new RamUsageEstimator();
     long size = rue.estimateRamUsage(string);
-    System.out.println("size:" + size);
+    //System.out.println("size:" + size);
     
     string = new String("test strin");
     size = rue.estimateRamUsage(string);
-    System.out.println("size:" + size);
+    //System.out.println("size:" + size);
     
     Holder holder = new Holder();
     holder.holder = new Holder("string2", 5000L);
     size = rue.estimateRamUsage(holder);
-    System.out.println("size:" + size);
+    //System.out.println("size:" + size);
     
     String[] strings = new String[]{new String("test strin"), new String("hollow"), new String("catchmaster")};
     size = rue.estimateRamUsage(strings);
-    System.out.println("size:" + size);
+    //System.out.println("size:" + size);
   }
   
   private static final class Holder {
Index: src/test/org/apache/lucene/util/TestSortedVIntList.java
===================================================================
--- src/test/org/apache/lucene/util/TestSortedVIntList.java	(revision 916567)
+++ src/test/org/apache/lucene/util/TestSortedVIntList.java	(working copy)
@@ -20,8 +20,6 @@
 import java.io.IOException;
 import java.util.BitSet;
 
-import org.apache.lucene.util.LuceneTestCase;
-
 import junit.framework.TestSuite;
 import junit.textui.TestRunner;
 
@@ -193,4 +191,11 @@
   public void test12() {
    tstIllegalArgExc(new int[] {0,1,1,2,3,5,8,0});
   }
+  public void test13Allocation() throws Exception {
+    int [] a = new int[2000]; // SortedVIntList initial byte size is 128
+    for (int i = 0; i < a.length; i++) {
+      a[i] = (107 + i) * i;
+    }
+    tstIterator(new SortedVIntList(a), a);
+  }
 }
Index: src/test/org/apache/lucene/util/TestSmallFloat.java
===================================================================
--- src/test/org/apache/lucene/util/TestSmallFloat.java	(revision 916567)
+++ src/test/org/apache/lucene/util/TestSmallFloat.java	(working copy)
@@ -18,9 +18,6 @@
 
 import java.util.Random;
 
-/**
- * @version $Id$
- */
 public class TestSmallFloat extends LuceneTestCase {
 
   // original lucene byteToFloat
Index: src/test/org/apache/lucene/document/TestBinaryDocument.java
===================================================================
--- src/test/org/apache/lucene/document/TestBinaryDocument.java	(revision 916567)
+++ src/test/org/apache/lucene/document/TestBinaryDocument.java	(working copy)
@@ -59,7 +59,7 @@
     
     /** add the doc to a ram index */
     MockRAMDirectory dir = new MockRAMDirectory();
-    IndexWriter writer = new IndexWriter(dir, new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
+    IndexWriter writer = new IndexWriter(dir, new StandardAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
     writer.addDocument(doc);
     writer.close();
     
@@ -97,7 +97,7 @@
     
     /** add the doc to a ram index */
     MockRAMDirectory dir = new MockRAMDirectory();
-    IndexWriter writer = new IndexWriter(dir, new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
+    IndexWriter writer = new IndexWriter(dir, new StandardAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
     writer.addDocument(doc);
     writer.close();
     

Property changes on: src\test\org\apache\lucene\document\TestNumberTools.java
___________________________________________________________________
Modified: svn:mergeinfo
   Merged /lucene/java/branches/lucene_2_9/src/test/org/apache/lucene/document/TestNumberTools.java:r909334
   Merged /lucene/java/trunk/src/test/org/apache/lucene/document/TestNumberTools.java:r889614-889621,889623-889666,889668-889865,899002-916543


Property changes on: src\test\org\apache\lucene\document\TestDateTools.java
___________________________________________________________________
Modified: svn:mergeinfo
   Merged /lucene/java/trunk/src/test/org/apache/lucene/document/TestDateTools.java:r889614-889621,889623-889666,889668-889865,899002-916543
   Merged /lucene/java/branches/lucene_2_9/src/test/org/apache/lucene/document/TestDateTools.java:r909334

Index: src/test/org/apache/lucene/document/TestDocument.java
===================================================================
--- src/test/org/apache/lucene/document/TestDocument.java	(revision 916567)
+++ src/test/org/apache/lucene/document/TestDocument.java	(working copy)
@@ -154,7 +154,7 @@
     public void testGetValuesForIndexedDocument() throws Exception
     {
         RAMDirectory dir = new RAMDirectory();
-        IndexWriter writer = new IndexWriter(dir, new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
+        IndexWriter writer = new IndexWriter(dir, new StandardAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
         writer.addDocument(makeDocumentWithFields());
         writer.close();
 
@@ -225,7 +225,7 @@
       doc.add(new Field("keyword", "test", Field.Store.YES, Field.Index.NOT_ANALYZED));
 
       RAMDirectory dir = new RAMDirectory();
-      IndexWriter writer = new IndexWriter(dir, new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
+      IndexWriter writer = new IndexWriter(dir, new StandardAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
       writer.addDocument(doc);
       field.setValue("id2");
       writer.addDocument(doc);
Index: src/java/org/apache/lucene/analysis/SimpleAnalyzer.java
===================================================================
--- src/java/org/apache/lucene/analysis/SimpleAnalyzer.java	(revision 916567)
+++ src/java/org/apache/lucene/analysis/SimpleAnalyzer.java	(working copy)
@@ -19,14 +19,42 @@
 
 import java.io.Reader;
 
+import org.apache.lucene.util.Version;
+
 /** An {@link Analyzer} that filters {@link LetterTokenizer} 
- *  with {@link LowerCaseFilter} */
-
+ *  with {@link LowerCaseFilter} 
+ * <p>
+ * <a name="version">You must specify the required {@link Version} compatibility
+ * when creating {@link CharTokenizer}:
+ * <ul>
+ * <li>As of 3.1, {@link LowerCaseTokenizer} uses an int based API to normalize and
+ * detect token codepoints. See {@link CharTokenizer#isTokenChar(int)} and
+ * {@link CharTokenizer#normalize(int)} for details.</li>
+ * </ul>
+ * <p>
+ **/
 public final class SimpleAnalyzer extends ReusableAnalyzerBase {
 
+  private final Version matchVersion;
+  
+  /**
+   * Creates a new {@link SimpleAnalyzer}
+   * @param matchVersion Lucene version to match See {@link <a href="#version">above</a>}
+   */
+  public SimpleAnalyzer(Version matchVersion) {
+    this.matchVersion = matchVersion;
+  }
+  
+  /**
+   * Creates a new {@link SimpleAnalyzer}
+   * @deprecated use {@link #SimpleAnalyzer(Version)} instead 
+   */
+  @Deprecated  public SimpleAnalyzer() {
+    this(Version.LUCENE_30);
+  }
   @Override
   protected TokenStreamComponents createComponents(final String fieldName,
       final Reader reader) {
-    return new TokenStreamComponents(new LowerCaseTokenizer(reader));
+    return new TokenStreamComponents(new LowerCaseTokenizer(matchVersion, reader));
   }
 }
Index: src/java/org/apache/lucene/analysis/ASCIIFoldingFilter.java
===================================================================
--- src/java/org/apache/lucene/analysis/ASCIIFoldingFilter.java	(revision 916567)
+++ src/java/org/apache/lucene/analysis/ASCIIFoldingFilter.java	(working copy)
@@ -1,10 +1,5 @@
 package org.apache.lucene.analysis;
 
-import java.io.IOException;
-
-import org.apache.lucene.analysis.tokenattributes.TermAttribute;
-import org.apache.lucene.util.ArrayUtil;
-
 /**
  * Licensed to the Apache Software Foundation (ASF) under one or more
  * contributor license agreements.  See the NOTICE file distributed with
@@ -22,6 +17,12 @@
  * limitations under the License.
  */
 
+import java.io.IOException;
+
+import org.apache.lucene.analysis.tokenattributes.TermAttribute;
+import org.apache.lucene.util.ArrayUtil;
+import org.apache.lucene.util.RamUsageEstimator;
+
 /**
  * This class converts alphabetic, numeric, and symbolic Unicode characters
  * which are not in the first 127 ASCII characters (the "Basic Latin" Unicode
@@ -101,7 +102,7 @@
     // Worst-case length required:
     final int maxSizeNeeded = 4 * length;
     if (output.length < maxSizeNeeded) {
-      output = new char[ArrayUtil.getNextSize(maxSizeNeeded)];
+      output = new char[ArrayUtil.oversize(maxSizeNeeded, RamUsageEstimator.NUM_BYTES_CHAR)];
     }
 
     outputPos = 0;
Index: src/java/org/apache/lucene/analysis/WhitespaceAnalyzer.java
===================================================================
--- src/java/org/apache/lucene/analysis/WhitespaceAnalyzer.java	(revision 916567)
+++ src/java/org/apache/lucene/analysis/WhitespaceAnalyzer.java	(working copy)
@@ -19,13 +19,44 @@
 
 import java.io.Reader;
 
-/** An Analyzer that uses {@link WhitespaceTokenizer}. */
+import org.apache.lucene.util.Version;
 
+/**
+ * An Analyzer that uses {@link WhitespaceTokenizer}.
+ * <p>
+ * <a name="version">You must specify the required {@link Version} compatibility
+ * when creating {@link CharTokenizer}:
+ * <ul>
+ * <li>As of 3.1, {@link WhitespaceTokenizer} uses an int based API to normalize and
+ * detect token codepoints. See {@link CharTokenizer#isTokenChar(int)} and
+ * {@link CharTokenizer#normalize(int)} for details.</li>
+ * </ul>
+ * <p>
+ **/
 public final class WhitespaceAnalyzer extends ReusableAnalyzerBase {
-
+  
+  private final Version matchVersion;
+  
+  /**
+   * Creates a new {@link WhitespaceAnalyzer}
+   * @param matchVersion Lucene version to match See {@link <a href="#version">above</a>}
+   */
+  public WhitespaceAnalyzer(Version matchVersion) {
+    this.matchVersion = matchVersion;
+  }
+  
+  /**
+   * Creates a new {@link WhitespaceAnalyzer}
+   * @deprecated use {@link #WhitespaceAnalyzer(Version)} instead 
+   */
+  @Deprecated
+  public WhitespaceAnalyzer() {
+    this(Version.LUCENE_30);
+  }
+  
   @Override
   protected TokenStreamComponents createComponents(final String fieldName,
       final Reader reader) {
-    return new TokenStreamComponents(new WhitespaceTokenizer(reader));
+    return new TokenStreamComponents(new WhitespaceTokenizer(matchVersion, reader));
   }
 }
Index: src/java/org/apache/lucene/analysis/NumericTokenStream.java
===================================================================
--- src/java/org/apache/lucene/analysis/NumericTokenStream.java	(revision 916567)
+++ src/java/org/apache/lucene/analysis/NumericTokenStream.java	(working copy)
@@ -82,8 +82,7 @@
  * href="../search/NumericRangeQuery.html#precisionStepDesc"><code>precisionStep</code></a>
  * parameter as well as how numeric fields work under the hood.</p>
  *
- * <p><font color="red"><b>NOTE:</b> This API is experimental and
- * might change in incompatible ways in the next release.</font>
+ * @lucene.experimental
  *
  * @since 2.9
  */
Index: src/java/org/apache/lucene/analysis/CharFilter.java
===================================================================
--- src/java/org/apache/lucene/analysis/CharFilter.java	(revision 916567)
+++ src/java/org/apache/lucene/analysis/CharFilter.java	(working copy)
@@ -24,9 +24,6 @@
  * They can be used as {@link java.io.Reader} with additional offset
  * correction. {@link Tokenizer}s will automatically use {@link #correctOffset}
  * if a CharFilter/CharStream subclass is used.
- *
- * @version $Id$
- *
  */
 public abstract class CharFilter extends CharStream {
 
Index: src/java/org/apache/lucene/analysis/WordlistLoader.java
===================================================================
--- src/java/org/apache/lucene/analysis/WordlistLoader.java	(revision 916567)
+++ src/java/org/apache/lucene/analysis/WordlistLoader.java	(working copy)
@@ -191,6 +191,66 @@
     return result;
   }
 
+  /**
+   * Loads a text file in Snowball format associated with a given class (See
+   * {@link Class#getResourceAsStream(String)}) and adds all words as entries to
+   * a {@link Set}. The words need to be in lower-case if you make use of an
+   * Analyzer which uses LowerCaseFilter (like StandardAnalyzer).
+   * 
+   * @param aClass a class that is associated with the given stopwordResource
+   * @param stopwordResource name of the resource file associated with the given
+   *          class
+   * @return a {@link Set} with the file's words
+   * @see #getSnowballWordSet(Reader)
+   */
+  public static Set<String> getSnowballWordSet(Class<?> aClass,
+      String stopwordResource) throws IOException {
+    final Reader reader = new BufferedReader(new InputStreamReader(aClass
+        .getResourceAsStream(stopwordResource), "UTF-8"));
+    try {
+      return getSnowballWordSet(reader);
+    } finally {
+      reader.close();
+    }
+  }
+  
+  /**
+   * Reads stopwords from a stopword list in Snowball format.
+   * <p>
+   * The snowball format is the following:
+   * <ul>
+   * <li>Lines may contain multiple words separated by whitespace.
+   * <li>The comment character is the vertical line (&#124;).
+   * <li>Lines may contain trailing comments.
+   * </ul>
+   * </p>
+   * 
+   * @param reader Reader containing a Snowball stopword list
+   * @return A Set with the reader's words
+   */
+  public static Set<String> getSnowballWordSet(Reader reader)
+      throws IOException {
+    final Set<String> result = new HashSet<String>();
+    BufferedReader br = null;
+    try {
+      if (reader instanceof BufferedReader) {
+        br = (BufferedReader) reader;
+      } else {
+        br = new BufferedReader(reader);
+      }
+      String line = null;
+      while ((line = br.readLine()) != null) {
+        int comment = line.indexOf('|');
+        if (comment >= 0) line = line.substring(0, comment);
+        String words[] = line.split("\\s+");
+        for (int i = 0; i < words.length; i++)
+          if (words[i].length() > 0) result.add(words[i]);
+      }
+    } finally {
+      if (br != null) br.close();
+    }
+    return result;
+  }
 
 
   /**
Index: src/java/org/apache/lucene/analysis/PerFieldAnalyzerWrapper.java
===================================================================
--- src/java/org/apache/lucene/analysis/PerFieldAnalyzerWrapper.java	(revision 916567)
+++ src/java/org/apache/lucene/analysis/PerFieldAnalyzerWrapper.java	(working copy)
@@ -17,6 +17,8 @@
  * limitations under the License.
  */
 
+import org.apache.lucene.document.Fieldable;
+
 import java.io.Reader;
 import java.io.IOException;
 import java.util.Map;
@@ -118,6 +120,15 @@
       analyzer = defaultAnalyzer;
     return analyzer.getPositionIncrementGap(fieldName);
   }
+
+  /** Return the offsetGap from the analyzer assigned to field */
+  @Override
+  public int getOffsetGap(Fieldable field) {
+    Analyzer analyzer = analyzerMap.get(field.name());
+    if (analyzer == null)
+      analyzer = defaultAnalyzer;
+    return analyzer.getOffsetGap(field);
+  }
   
   @Override
   public String toString() {
Index: src/java/org/apache/lucene/analysis/tokenattributes/FlagsAttribute.java
===================================================================
--- src/java/org/apache/lucene/analysis/tokenattributes/FlagsAttribute.java	(revision 916567)
+++ src/java/org/apache/lucene/analysis/tokenattributes/FlagsAttribute.java	(working copy)
@@ -23,10 +23,10 @@
 /**
  * This attribute can be used to pass different flags down the {@link Tokenizer} chain,
  * eg from one TokenFilter to another one. 
+ * @lucene.experimental While we think this is here to stay, we may want to change it to be a long.
  */
 public interface FlagsAttribute extends Attribute {
   /**
-   * EXPERIMENTAL:  While we think this is here to stay, we may want to change it to be a long.
    * <p/>
    *
    * Get the bitset for any bits that have been set.  This is completely distinct from {@link TypeAttribute#type()}, although they do share similar purposes.
Index: src/java/org/apache/lucene/analysis/tokenattributes/FlagsAttributeImpl.java
===================================================================
--- src/java/org/apache/lucene/analysis/tokenattributes/FlagsAttributeImpl.java	(revision 916567)
+++ src/java/org/apache/lucene/analysis/tokenattributes/FlagsAttributeImpl.java	(working copy)
@@ -24,12 +24,12 @@
 /**
  * This attribute can be used to pass different flags down the tokenizer chain,
  * eg from one TokenFilter to another one. 
+ * @lucene.experimental While we think this is here to stay, we may want to change it to be a long.
  */
 public class FlagsAttributeImpl extends AttributeImpl implements FlagsAttribute, Cloneable, Serializable {
   private int flags = 0;
   
   /**
-   * EXPERIMENTAL:  While we think this is here to stay, we may want to change it to be a long.
    * <p/>
    *
    * Get the bitset for any bits that have been set.  This is completely distinct from {@link TypeAttribute#type()}, although they do share similar purposes.
Index: src/java/org/apache/lucene/analysis/tokenattributes/TermAttributeImpl.java
===================================================================
--- src/java/org/apache/lucene/analysis/tokenattributes/TermAttributeImpl.java	(revision 916567)
+++ src/java/org/apache/lucene/analysis/tokenattributes/TermAttributeImpl.java	(working copy)
@@ -21,6 +21,7 @@
 
 import org.apache.lucene.util.ArrayUtil;
 import org.apache.lucene.util.AttributeImpl;
+import org.apache.lucene.util.RamUsageEstimator;
 
 /**
  * The term text of a Token.
@@ -106,12 +107,12 @@
   public char[] resizeTermBuffer(int newSize) {
     if (termBuffer == null) {
       // The buffer is always at least MIN_BUFFER_SIZE
-      termBuffer = new char[ArrayUtil.getNextSize(newSize < MIN_BUFFER_SIZE ? MIN_BUFFER_SIZE : newSize)]; 
+      termBuffer = new char[ArrayUtil.oversize(newSize < MIN_BUFFER_SIZE ? MIN_BUFFER_SIZE : newSize, RamUsageEstimator.NUM_BYTES_CHAR)]; 
     } else {
       if(termBuffer.length < newSize){
         // Not big enough; create a new array with slight
         // over allocation and preserve content
-        final char[] newCharBuffer = new char[ArrayUtil.getNextSize(newSize)];
+        final char[] newCharBuffer = new char[ArrayUtil.oversize(newSize, RamUsageEstimator.NUM_BYTES_CHAR)];
         System.arraycopy(termBuffer, 0, newCharBuffer, 0, termBuffer.length);
         termBuffer = newCharBuffer;
       }
@@ -127,19 +128,19 @@
   private void growTermBuffer(int newSize) {
     if (termBuffer == null) {
       // The buffer is always at least MIN_BUFFER_SIZE
-      termBuffer = new char[ArrayUtil.getNextSize(newSize < MIN_BUFFER_SIZE ? MIN_BUFFER_SIZE : newSize)];   
+      termBuffer = new char[ArrayUtil.oversize(newSize < MIN_BUFFER_SIZE ? MIN_BUFFER_SIZE : newSize, RamUsageEstimator.NUM_BYTES_CHAR)];   
     } else {
       if(termBuffer.length < newSize){
         // Not big enough; create a new array with slight
         // over allocation:
-        termBuffer = new char[ArrayUtil.getNextSize(newSize)];
+        termBuffer = new char[ArrayUtil.oversize(newSize, RamUsageEstimator.NUM_BYTES_CHAR)];
       }
     } 
   }
   
   private void initTermBuffer() {
     if (termBuffer == null) {
-      termBuffer = new char[ArrayUtil.getNextSize(MIN_BUFFER_SIZE)];
+      termBuffer = new char[ArrayUtil.oversize(MIN_BUFFER_SIZE, RamUsageEstimator.NUM_BYTES_CHAR)];
       termLength = 0;
     }
   }
Index: src/java/org/apache/lucene/analysis/CharTokenizer.java
===================================================================
--- src/java/org/apache/lucene/analysis/CharTokenizer.java	(revision 916567)
+++ src/java/org/apache/lucene/analysis/CharTokenizer.java	(working copy)
@@ -23,59 +23,310 @@
 import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
 import org.apache.lucene.analysis.tokenattributes.TermAttribute;
 import org.apache.lucene.util.AttributeSource;
+import org.apache.lucene.util.CharacterUtils;
+import org.apache.lucene.util.Version;
+import org.apache.lucene.util.VirtualMethod;
+import org.apache.lucene.util.CharacterUtils.CharacterBuffer;
 
-/** An abstract base class for simple, character-oriented tokenizers.*/
+/**
+ * An abstract base class for simple, character-oriented tokenizers. 
+ * <p>
+ * <a name="version">You must specify the required {@link Version} compatibility
+ * when creating {@link CharTokenizer}:
+ * <ul>
+ * <li>As of 3.1, {@link CharTokenizer} uses an int based API to normalize and
+ * detect token codepoints. See {@link #isTokenChar(int)} and
+ * {@link #normalize(int)} for details.</li>
+ * </ul>
+ * <p>
+ * A new {@link CharTokenizer} API has been introduced with Lucene 3.1. This API
+ * moved from UTF-16 code units to UTF-32 codepoints to eventually add support
+ * for <a href=
+ * "http://java.sun.com/j2se/1.5.0/docs/api/java/lang/Character.html#supplementary"
+ * >supplementary characters</a>. The old <i>char</i> based API has been
+ * deprecated and should be replaced with the <i>int</i> based methods
+ * {@link #isTokenChar(int)} and {@link #normalize(int)}.
+ * </p>
+ * <p>
+ * As of Lucene 3.1 each {@link CharTokenizer} - constructor expects a
+ * {@link Version} argument. Based on the given {@link Version} either the new
+ * API or a backwards compatibility layer is used at runtime. For
+ * {@link Version} < 3.1 the backwards compatibility layer ensures correct
+ * behavior even for indexes build with previous versions of Lucene. If a
+ * {@link Version} >= 3.1 is used {@link CharTokenizer} requires the new API to
+ * be implemented by the instantiated class. Yet, the old <i>char</i> based API
+ * is not required anymore even if backwards compatibility must be preserved.
+ * {@link CharTokenizer} subclasses implementing the new API are fully backwards
+ * compatible if instantiated with {@link Version} < 3.1.
+ * </p>
+ * <p>
+ * <strong>Note:</strong> If you use a subclass of {@link CharTokenizer} with {@link Version} >=
+ * 3.1 on an index build with a version < 3.1, created tokens might not be
+ * compatible with the terms in your index.
+ * </p>
+ **/
 public abstract class CharTokenizer extends Tokenizer {
-  public CharTokenizer(Reader input) {
+  
+  /**
+   * Creates a new {@link CharTokenizer} instance
+   * 
+   * @param matchVersion
+   *          Lucene version to match See {@link <a href="#version">above</a>}
+   * @param input
+   *          the input to split up into tokens
+   */
+  public CharTokenizer(Version matchVersion, Reader input) {
     super(input);
+    charUtils = CharacterUtils.getInstance(matchVersion);
     offsetAtt = addAttribute(OffsetAttribute.class);
     termAtt = addAttribute(TermAttribute.class);
+    useOldAPI = useOldAPI(matchVersion);
+    ioBuffer = CharacterUtils.newCharacterBuffer(IO_BUFFER_SIZE);
+
   }
-
-  public CharTokenizer(AttributeSource source, Reader input) {
+  
+  /**
+   * Creates a new {@link CharTokenizer} instance
+   * 
+   * @param matchVersion
+   *          Lucene version to match See {@link <a href="#version">above</a>}
+   * @param source
+   *          the attribute source to use for this {@link Tokenizer}
+   * @param input
+   *          the input to split up into tokens
+   */
+  public CharTokenizer(Version matchVersion, AttributeSource source,
+      Reader input) {
     super(source, input);
+    charUtils = CharacterUtils.getInstance(matchVersion);
     offsetAtt = addAttribute(OffsetAttribute.class);
     termAtt = addAttribute(TermAttribute.class);
+    useOldAPI = useOldAPI(matchVersion);
+    ioBuffer = CharacterUtils.newCharacterBuffer(IO_BUFFER_SIZE);
   }
-
-  public CharTokenizer(AttributeFactory factory, Reader input) {
+  
+  /**
+   * Creates a new {@link CharTokenizer} instance
+   * 
+   * @param matchVersion
+   *          Lucene version to match See {@link <a href="#version">above</a>}
+   * @param factory
+   *          the attribute factory to use for this {@link Tokenizer}
+   * @param input
+   *          the input to split up into tokens
+   */
+  public CharTokenizer(Version matchVersion, AttributeFactory factory,
+      Reader input) {
     super(factory, input);
+    charUtils = CharacterUtils.getInstance(matchVersion);
     offsetAtt = addAttribute(OffsetAttribute.class);
     termAtt = addAttribute(TermAttribute.class);
+    useOldAPI = useOldAPI(matchVersion);
+    ioBuffer = CharacterUtils.newCharacterBuffer(IO_BUFFER_SIZE);
   }
   
+  /**
+   * Creates a new {@link CharTokenizer} instance
+   * @param input the input to split up into tokens
+   * @deprecated use {@link #CharTokenizer(Version, Reader)} instead. This will be
+   *             removed in Lucene 4.0.
+   */
+  @Deprecated
+  public CharTokenizer(Reader input) {
+    this(Version.LUCENE_30, input);
+  }
+
+  /**
+   * Creates a new {@link CharTokenizer} instance
+   * @param input the input to split up into tokens
+   * @param source the attribute source to use for this {@link Tokenizer}
+   * @deprecated use {@link #CharTokenizer(Version, AttributeSource, Reader)} instead. This will be
+   *             removed in Lucene 4.0.
+   */
+  @Deprecated
+  public CharTokenizer(AttributeSource source, Reader input) {
+    this(Version.LUCENE_30, source, input);
+  }
+
+  /**
+   * Creates a new {@link CharTokenizer} instance
+   * @param input the input to split up into tokens
+   * @param factory the attribute factory to use for this {@link Tokenizer}
+   * @deprecated use {@link #CharTokenizer(Version, AttributeSource.AttributeFactory, Reader)} instead. This will be
+   *             removed in Lucene 4.0.
+   */
+  @Deprecated
+  public CharTokenizer(AttributeFactory factory, Reader input) {
+    this(Version.LUCENE_30, factory, input);
+  }
+  
   private int offset = 0, bufferIndex = 0, dataLen = 0;
   private static final int MAX_WORD_LEN = 255;
   private static final int IO_BUFFER_SIZE = 4096;
-  private final char[] ioBuffer = new char[IO_BUFFER_SIZE];
   
-  private TermAttribute termAtt;
-  private OffsetAttribute offsetAtt;
+  private final TermAttribute termAtt;
+  private final OffsetAttribute offsetAtt;
+  
+  private final CharacterUtils charUtils;
+  private final CharacterBuffer ioBuffer;
+  
+  /**
+   * @deprecated this will be removed in lucene 4.0
+   */
+  @Deprecated
+  private final boolean useOldAPI;
+  
+  /**
+   * @deprecated this will be removed in lucene 4.0
+   */
+  @Deprecated
+  private static final VirtualMethod<CharTokenizer> isTokenCharMethod =
+    new VirtualMethod<CharTokenizer>(CharTokenizer.class, "isTokenChar", char.class);
+  
+  /**
+   * @deprecated this will be removed in lucene 4.0
+   */
+  @Deprecated
+  private static final VirtualMethod<CharTokenizer> normalizeMethod =
+    new VirtualMethod<CharTokenizer>(CharTokenizer.class, "normalize", char.class);
 
-  /** Returns true iff a character should be included in a token.  This
+  /**
+   * Returns true iff a UTF-16 code unit should be included in a token. This
    * tokenizer generates as tokens adjacent sequences of characters which
-   * satisfy this predicate.  Characters for which this is false are used to
-   * define token boundaries and are not included in tokens. */
-  protected abstract boolean isTokenChar(char c);
+   * satisfy this predicate. Characters for which this is <code>false</code> are
+   * used to define token boundaries and are not included in tokens.
+   * <p>
+   * Note: This method cannot handle <a href=
+   * "http://java.sun.com/j2se/1.5.0/docs/api/java/lang/Character.html#supplementary"
+   * >supplementary characters</a>. To support all Unicode characters, including
+   * supplementary characters, use the {@link #isTokenChar(int)} method.
+   * </p>
+   * 
+   * @deprecated use {@link #isTokenChar(int)} instead. This method will be
+   *             removed in Lucene 4.0.
+   */
+  @Deprecated  
+  protected boolean isTokenChar(char c) {
+    return isTokenChar((int)c); 
+  }
 
-  /** Called on each token character to normalize it before it is added to the
-   * token.  The default implementation does nothing.  Subclasses may use this
-   * to, e.g., lowercase tokens. */
+  /**
+   * Called on each token UTF-16 code unit to normalize it before it is added to the
+   * token. The default implementation does nothing. Subclasses may use this to,
+   * e.g., lowercase tokens.
+   * <p>
+   * Note: This method cannot handle <a href=
+   * "http://java.sun.com/j2se/1.5.0/docs/api/java/lang/Character.html#supplementary"
+   * >supplementary characters</a>. To support all Unicode characters, including
+   * supplementary characters, use the {@link #normalize(int)} method.
+   * </p>
+   * 
+   * @deprecated use {@link #normalize(int)} instead. This method will be
+   *             removed in Lucene 4.0.
+   */
+  @Deprecated 
   protected char normalize(char c) {
+    return (char) normalize((int) c);
+  }
+
+  /**
+   * Returns true iff a codepoint should be included in a token. This tokenizer
+   * generates as tokens adjacent sequences of codepoints which satisfy this
+   * predicate. Codepoints for which this is false are used to define token
+   * boundaries and are not included in tokens.
+   * <p>
+   * As of Lucene 3.1 the char based API ({@link #isTokenChar(char)} and
+   * {@link #normalize(char)}) has been depreciated in favor of a Unicode 4.0
+   * compatible int based API to support codepoints instead of UTF-16 code
+   * units. Subclasses of {@link CharTokenizer} must not override the char based
+   * methods if a {@link Version} >= 3.1 is passed to the constructor.
+   * <p>
+   * <p>
+   * NOTE: This method will be marked <i>abstract</i> in Lucene 4.0.
+   * </p>
+   */
+  protected boolean isTokenChar(int c) {
+    throw new UnsupportedOperationException("since LUCENE_3_1 subclasses of CharTokenizer must implement isTokenChar(int)");
+  }
+
+  /**
+   * Called on each token character to normalize it before it is added to the
+   * token. The default implementation does nothing. Subclasses may use this to,
+   * e.g., lowercase tokens.
+   * <p>
+   * As of Lucene 3.1 the char based API ({@link #isTokenChar(char)} and
+   * {@link #normalize(char)}) has been depreciated in favor of a Unicode 4.0
+   * compatible int based API to support codepoints instead of UTF-16 code
+   * units. Subclasses of {@link CharTokenizer} must not override the char based
+   * methods if a {@link Version} >= 3.1 is passed to the constructor.
+   * <p>
+   * <p>
+   * NOTE: This method will be marked <i>abstract</i> in Lucene 4.0.
+   * </p>
+   */
+  protected int normalize(int c) {
     return c;
   }
 
   @Override
   public final boolean incrementToken() throws IOException {
     clearAttributes();
+    if(useOldAPI) // TODO remove this in LUCENE 4.0
+      return incrementTokenOld();
     int length = 0;
     int start = bufferIndex;
     char[] buffer = termAtt.termBuffer();
     while (true) {
+      if (bufferIndex >= dataLen) {
+        offset += dataLen;
+        if(!charUtils.fill(ioBuffer, input)) { // read supplementary char aware with CharacterUtils
+          dataLen = 0; // so next offset += dataLen won't decrement offset
+          if (length > 0)
+            break;
+          else
+            return false;
+        }
+        dataLen = ioBuffer.getLength();
+        bufferIndex = 0;
+      }
+      // use CharacterUtils here to support < 3.1 UTF-16 code unit behavior if the char based methods are gone
+      final int c = charUtils.codePointAt(ioBuffer.getBuffer(), bufferIndex);
+      bufferIndex += Character.charCount(c);
 
+      if (isTokenChar(c)) {               // if it's a token char
+        if (length == 0)                 // start of token
+          start = offset + bufferIndex - 1;
+        else if (length >= buffer.length-1) // check if a supplementary could run out of bounds
+          buffer = termAtt.resizeTermBuffer(2+length); // make sure a supplementary fits in the buffer
+        length += Character.toChars(normalize(c), buffer, length); // buffer it, normalized
+        if (length >= MAX_WORD_LEN) // buffer overflow! make sure to check for >= surrogate pair could break == test
+          break;
+      } else if (length > 0)             // at non-Letter w/ chars
+        break;                           // return 'em
+    }
+
+    termAtt.setTermLength(length);
+    offsetAtt.setOffset(correctOffset(start), correctOffset(start+length));
+    return true;
+    
+  }
+  
+  /**
+   * The <= 3.0 version of incrementToken. This is a backwards compat implementation used
+   * if a version <= 3.0 is provided to the ctor. 
+   * @deprecated remove in 4.0
+   */
+  @Deprecated
+  private boolean incrementTokenOld() throws IOException {
+    int length = 0;
+    int start = bufferIndex;
+    char[] buffer = termAtt.termBuffer();
+    final char[] oldIoBuffer = ioBuffer.getBuffer();
+    while (true) {
+
       if (bufferIndex >= dataLen) {
         offset += dataLen;
-        dataLen = input.read(ioBuffer);
+        dataLen = input.read(oldIoBuffer);
         if (dataLen == -1) {
           dataLen = 0;                            // so next offset += dataLen won't decrement offset
           if (length > 0)
@@ -86,7 +337,7 @@
         bufferIndex = 0;
       }
 
-      final char c = ioBuffer[bufferIndex++];
+      final char c = oldIoBuffer[bufferIndex++];
 
       if (isTokenChar(c)) {               // if it's a token char
 
@@ -107,12 +358,14 @@
     termAtt.setTermLength(length);
     offsetAtt.setOffset(correctOffset(start), correctOffset(start+length));
     return true;
-  }
+  }  
   
+  
+  
   @Override
   public final void end() {
     // set final offset
-    int finalOffset = correctOffset(offset);
+    final int finalOffset = correctOffset(offset);
     offsetAtt.setOffset(finalOffset, finalOffset);
   }
 
@@ -122,5 +375,19 @@
     bufferIndex = 0;
     offset = 0;
     dataLen = 0;
+    ioBuffer.reset(); // make sure to reset the IO buffer!!
   }
-}
+
+  /**
+   * @deprecated this will be removed in lucene 4.0
+   */
+  @Deprecated
+  private boolean useOldAPI(Version matchVersion) {
+    final Class<? extends CharTokenizer> clazz = this.getClass();
+    if (matchVersion.onOrAfter(Version.LUCENE_31)
+        && (isTokenCharMethod.isOverriddenAsOf(clazz) || normalizeMethod
+            .isOverriddenAsOf(clazz))) throw new IllegalArgumentException(
+        "For matchVersion >= LUCENE_31, CharTokenizer subclasses must not override isTokenChar(char) or normalize(char).");
+    return !matchVersion.onOrAfter(Version.LUCENE_31);
+  } 
+}
\ No newline at end of file
Index: src/java/org/apache/lucene/analysis/PorterStemmer.java
===================================================================
--- src/java/org/apache/lucene/analysis/PorterStemmer.java	(revision 916567)
+++ src/java/org/apache/lucene/analysis/PorterStemmer.java	(working copy)
@@ -44,8 +44,13 @@
 */
 
 
-import java.io.*;
+import java.io.IOException;
+import java.io.InputStream;
+import java.io.FileInputStream;
 
+import static org.apache.lucene.util.RamUsageEstimator.NUM_BYTES_CHAR;
+import org.apache.lucene.util.ArrayUtil;
+
 /**
  *
  * Stemmer, implementing the Porter Stemming Algorithm
@@ -61,11 +66,10 @@
   private int i,    /* offset into b */
     j, k, k0;
   private boolean dirty = false;
-  private static final int INC = 50; /* unit of size whereby b is increased */
-  private static final int EXTRA = 1;
+  private static final int INITIAL_SIZE = 50;
 
   public PorterStemmer() {
-    b = new char[INC];
+    b = new char[INITIAL_SIZE];
     i = 0;
   }
 
@@ -81,10 +85,8 @@
    * adding characters, you can call stem(void) to process the word.
    */
   public void add(char ch) {
-    if (b.length <= i + EXTRA) {
-      char[] new_b = new char[b.length+INC];
-      System.arraycopy(b, 0, new_b, 0, b.length);
-      b = new_b;
+    if (b.length <= i) {
+      b = ArrayUtil.grow(b, i+1);
     }
     b[i++] = ch;
   }
@@ -451,8 +453,7 @@
   public boolean stem(char[] wordBuffer, int offset, int wordLen) {
     reset();
     if (b.length < wordLen) {
-      char[] new_b = new char[wordLen + EXTRA];
-      b = new_b;
+      b = new char[ArrayUtil.oversize(wordLen, NUM_BYTES_CHAR)];
     }
     System.arraycopy(wordBuffer, offset, b, 0, wordLen);
     i = wordLen;
Index: src/java/org/apache/lucene/analysis/PorterStemFilter.java
===================================================================
--- src/java/org/apache/lucene/analysis/PorterStemFilter.java	(revision 916567)
+++ src/java/org/apache/lucene/analysis/PorterStemFilter.java	(working copy)
@@ -19,6 +19,7 @@
 
 import java.io.IOException;
 
+import org.apache.lucene.analysis.tokenattributes.KeywordAttribute;
 import org.apache.lucene.analysis.tokenattributes.TermAttribute;
 
 /** Transforms the token stream as per the Porter stemming algorithm.
@@ -38,15 +39,23 @@
       }
     }
     </PRE>
+    <p>
+    Note: This filter is aware of the {@link KeywordAttribute}. To prevent
+    certain terms from being passed to the stemmer
+    {@link KeywordAttribute#isKeyword()} should be set to <code>true</code>
+    in a previous {@link TokenStream}.
+    </p>
 */
 public final class PorterStemFilter extends TokenFilter {
-  private PorterStemmer stemmer;
-  private TermAttribute termAtt;
+  private final PorterStemmer stemmer;
+  private final TermAttribute termAtt;
+  private final KeywordAttribute keywordAttr;
 
   public PorterStemFilter(TokenStream in) {
     super(in);
     stemmer = new PorterStemmer();
     termAtt = addAttribute(TermAttribute.class);
+    keywordAttr = addAttribute(KeywordAttribute.class);
   }
 
   @Override
@@ -54,7 +63,7 @@
     if (!input.incrementToken())
       return false;
 
-    if (stemmer.stem(termAtt.termBuffer(), 0, termAtt.termLength()))
+    if ((!keywordAttr.isKeyword()) && stemmer.stem(termAtt.termBuffer(), 0, termAtt.termLength()))
       termAtt.setTermBuffer(stemmer.getResultBuffer(), 0, stemmer.getResultLength());
     return true;
   }

Property changes on: src\java\org\apache\lucene\analysis\Tokenizer.java
___________________________________________________________________
Added: svn:mergeinfo
   Merged /lucene/java/branches/lucene_2_9/src/java/org/apache/lucene/analysis/Tokenizer.java:r909334

Index: src/java/org/apache/lucene/analysis/LowerCaseTokenizer.java
===================================================================
--- src/java/org/apache/lucene/analysis/LowerCaseTokenizer.java	(revision 916567)
+++ src/java/org/apache/lucene/analysis/LowerCaseTokenizer.java	(working copy)
@@ -20,6 +20,7 @@
 import java.io.Reader;
 
 import org.apache.lucene.util.AttributeSource;
+import org.apache.lucene.util.Version;
 
 /**
  * LowerCaseTokenizer performs the function of LetterTokenizer
@@ -30,27 +31,98 @@
  * <P>
  * Note: this does a decent job for most European languages, but does a terrible
  * job for some Asian languages, where words are not separated by spaces.
+ * </p>
+ * <p>
+ * <a name="version"/>
+ * You must specify the required {@link Version} compatibility when creating
+ * {@link LowerCaseTokenizer}:
+ * <ul>
+ * <li>As of 3.1, {@link CharTokenizer} uses an int based API to normalize and
+ * detect token characters. See {@link CharTokenizer#isTokenChar(int)} and
+ * {@link CharTokenizer#normalize(int)} for details.</li>
+ * </ul>
+ * </p>
  */
 public final class LowerCaseTokenizer extends LetterTokenizer {
-  /** Construct a new LowerCaseTokenizer. */
+  
+  /**
+   * Construct a new LowerCaseTokenizer.
+   * 
+   * @param matchVersion
+   *          Lucene version to match See {@link <a href="#version">above</a>}
+   * 
+   * @param in
+   *          the input to split up into tokens
+   */
+  public LowerCaseTokenizer(Version matchVersion, Reader in) {
+    super(matchVersion, in);
+  }
+
+  /** 
+   * Construct a new LowerCaseTokenizer using a given {@link AttributeSource}.
+   *
+   * @param matchVersion
+   *          Lucene version to match See {@link <a href="#version">above</a>}
+   * @param source
+   *          the attribute source to use for this {@link Tokenizer}
+   * @param in
+   *          the input to split up into tokens
+   */
+  public LowerCaseTokenizer(Version matchVersion, AttributeSource source, Reader in) {
+    super(matchVersion, source, in);
+  }
+
+  /**
+   * Construct a new LowerCaseTokenizer using a given
+   * {@link org.apache.lucene.util.AttributeSource.AttributeFactory}.
+   *
+   * @param matchVersion
+   *          Lucene version to match See {@link <a href="#version">above</a>}
+   * @param factory
+   *          the attribute factory to use for this {@link Tokenizer}
+   * @param in
+   *          the input to split up into tokens
+   */
+  public LowerCaseTokenizer(Version matchVersion, AttributeFactory factory, Reader in) {
+    super(matchVersion, factory, in);
+  }
+  
+  /**
+   * Construct a new LowerCaseTokenizer.
+   * 
+   * @deprecated use {@link #LowerCaseTokenizer(Reader)} instead. This will be
+   *             removed in Lucene 4.0.
+   */
+  @Deprecated
   public LowerCaseTokenizer(Reader in) {
-    super(in);
+    super(Version.LUCENE_30, in);
   }
 
-  /** Construct a new LowerCaseTokenizer using a given {@link AttributeSource}. */
+  /**
+   * Construct a new LowerCaseTokenizer using a given {@link AttributeSource}.
+   * 
+   * @deprecated use {@link #LowerCaseTokenizer(AttributeSource, Reader)}
+   *             instead. This will be removed in Lucene 4.0.
+   */
   public LowerCaseTokenizer(AttributeSource source, Reader in) {
-    super(source, in);
+    super(Version.LUCENE_30, source, in);
   }
 
-  /** Construct a new LowerCaseTokenizer using a given {@link org.apache.lucene.util.AttributeSource.AttributeFactory}. */
+  /**
+   * Construct a new LowerCaseTokenizer using a given
+   * {@link org.apache.lucene.util.AttributeSource.AttributeFactory}.
+   * 
+   * @deprecated use {@link #LowerCaseTokenizer(AttributeSource.AttributeFactory, Reader)}
+   *             instead. This will be removed in Lucene 4.0.
+   */
   public LowerCaseTokenizer(AttributeFactory factory, Reader in) {
-    super(factory, in);
+    super(Version.LUCENE_30, factory, in);
   }
   
   /** Converts char to lower case
-   * {@link Character#toLowerCase(char)}.*/
+   * {@link Character#toLowerCase(int)}.*/
   @Override
-  protected char normalize(char c) {
+  protected int normalize(int c) {
     return Character.toLowerCase(c);
   }
 }
Index: src/java/org/apache/lucene/analysis/Token.java
===================================================================
--- src/java/org/apache/lucene/analysis/Token.java	(revision 916567)
+++ src/java/org/apache/lucene/analysis/Token.java	(working copy)
@@ -29,6 +29,7 @@
 import org.apache.lucene.util.Attribute;
 import org.apache.lucene.util.AttributeSource;
 import org.apache.lucene.util.AttributeImpl;
+import org.apache.lucene.util.RamUsageEstimator;
 
 /** 
   A Token is an occurrence of a term from the text of a field.  It consists of
@@ -347,12 +348,12 @@
   public char[] resizeTermBuffer(int newSize) {
     if (termBuffer == null) {
       // The buffer is always at least MIN_BUFFER_SIZE
-      termBuffer = new char[ArrayUtil.getNextSize(newSize < MIN_BUFFER_SIZE ? MIN_BUFFER_SIZE : newSize)]; 
+      termBuffer = new char[ArrayUtil.oversize(newSize < MIN_BUFFER_SIZE ? MIN_BUFFER_SIZE : newSize, RamUsageEstimator.NUM_BYTES_CHAR)]; 
     } else {
       if(termBuffer.length < newSize){
         // Not big enough; create a new array with slight
         // over allocation and preserve content
-        final char[] newCharBuffer = new char[ArrayUtil.getNextSize(newSize)];
+        final char[] newCharBuffer = new char[ArrayUtil.oversize(newSize, RamUsageEstimator.NUM_BYTES_CHAR)];
         System.arraycopy(termBuffer, 0, newCharBuffer, 0, termBuffer.length);
         termBuffer = newCharBuffer;
       }
@@ -367,19 +368,19 @@
   private void growTermBuffer(int newSize) {
     if (termBuffer == null) {
       // The buffer is always at least MIN_BUFFER_SIZE    
-      termBuffer = new char[ArrayUtil.getNextSize(newSize < MIN_BUFFER_SIZE ? MIN_BUFFER_SIZE : newSize)];   
+      termBuffer = new char[ArrayUtil.oversize(newSize < MIN_BUFFER_SIZE ? MIN_BUFFER_SIZE : newSize, RamUsageEstimator.NUM_BYTES_CHAR)];
     } else {
       if(termBuffer.length < newSize){
         // Not big enough; create a new array with slight
         // over allocation:
-        termBuffer = new char[ArrayUtil.getNextSize(newSize)];
+        termBuffer = new char[ArrayUtil.oversize(newSize, RamUsageEstimator.NUM_BYTES_CHAR)];
       }
     } 
   }
   
   private void initTermBuffer() {
     if (termBuffer == null) {
-      termBuffer = new char[ArrayUtil.getNextSize(MIN_BUFFER_SIZE)];
+      termBuffer = new char[ArrayUtil.oversize(MIN_BUFFER_SIZE, RamUsageEstimator.NUM_BYTES_CHAR)];
       termLength = 0;
     }
   }
@@ -453,14 +454,14 @@
   }
 
   /**
-   * EXPERIMENTAL:  While we think this is here to stay, we may want to change it to be a long.
    * <p/>
    *
    * Get the bitset for any bits that have been set.  This is completely distinct from {@link #type()}, although they do share similar purposes.
    * The flags can be used to encode information about the token for use by other {@link org.apache.lucene.analysis.TokenFilter}s.
    *
-   *
+   * 
    * @return The bits
+   * @lucene.experimental While we think this is here to stay, we may want to change it to be a long.
    */
   public int getFlags() {
     return flags;
Index: src/java/org/apache/lucene/analysis/WhitespaceTokenizer.java
===================================================================
--- src/java/org/apache/lucene/analysis/WhitespaceTokenizer.java	(revision 916567)
+++ src/java/org/apache/lucene/analysis/WhitespaceTokenizer.java	(working copy)
@@ -20,30 +20,102 @@
 import java.io.Reader;
 
 import org.apache.lucene.util.AttributeSource;
+import org.apache.lucene.util.Version;
 
-/** A WhitespaceTokenizer is a tokenizer that divides text at whitespace.
- * Adjacent sequences of non-Whitespace characters form tokens. */
+/**
+ * A WhitespaceTokenizer is a tokenizer that divides text at whitespace.
+ * Adjacent sequences of non-Whitespace characters form tokens. <a
+ * name="version"/>
+ * <p>
+ * You must specify the required {@link Version} compatibility when creating
+ * {@link WhitespaceTokenizer}:
+ * <ul>
+ * <li>As of 3.1, {@link CharTokenizer} uses an int based API to normalize and
+ * detect token characters. See {@link CharTokenizer#isTokenChar(int)} and
+ * {@link CharTokenizer#normalize(int)} for details.</li>
+ * </ul>
+ */
+public class WhitespaceTokenizer extends CharTokenizer {
+  
+  /**
+   * Construct a new WhitespaceTokenizer. * @param matchVersion Lucene version
+   * to match See {@link <a href="#version">above</a>}
+   * 
+   * @param in
+   *          the input to split up into tokens
+   */
+  public WhitespaceTokenizer(Version matchVersion, Reader in) {
+    super(matchVersion, in);
+  }
 
-public class WhitespaceTokenizer extends CharTokenizer {
-  /** Construct a new WhitespaceTokenizer. */
+  /**
+   * Construct a new WhitespaceTokenizer using a given {@link AttributeSource}.
+   * 
+   * @param matchVersion
+   *          Lucene version to match See {@link <a href="#version">above</a>}
+   * @param source
+   *          the attribute source to use for this {@link Tokenizer}
+   * @param in
+   *          the input to split up into tokens
+   */
+  public WhitespaceTokenizer(Version matchVersion, AttributeSource source, Reader in) {
+    super(matchVersion, source, in);
+  }
+
+  /**
+   * Construct a new WhitespaceTokenizer using a given
+   * {@link org.apache.lucene.util.AttributeSource.AttributeFactory}.
+   *
+   * @param
+   *          matchVersion Lucene version to match See
+   *          {@link <a href="#version">above</a>}
+   * @param factory
+   *          the attribute factory to use for this {@link Tokenizer}
+   * @param in
+   *          the input to split up into tokens
+   */
+  public WhitespaceTokenizer(Version matchVersion, AttributeFactory factory, Reader in) {
+    super(matchVersion, factory, in);
+  }
+  
+  /**
+   * Construct a new WhitespaceTokenizer.
+   * 
+   * @deprecated use {@link #WhitespaceTokenizer(Version, Reader)} instead. This will
+   *             be removed in Lucene 4.0.
+   */
+  @Deprecated
   public WhitespaceTokenizer(Reader in) {
     super(in);
   }
 
-  /** Construct a new WhitespaceTokenizer using a given {@link AttributeSource}. */
+  /**
+   * Construct a new WhitespaceTokenizer using a given {@link AttributeSource}.
+   * 
+   * @deprecated use {@link #WhitespaceTokenizer(Version, AttributeSource, Reader)}
+   *             instead. This will be removed in Lucene 4.0.
+   */
+  @Deprecated
   public WhitespaceTokenizer(AttributeSource source, Reader in) {
     super(source, in);
   }
 
-  /** Construct a new WhitespaceTokenizer using a given {@link org.apache.lucene.util.AttributeSource.AttributeFactory}. */
+  /**
+   * Construct a new WhitespaceTokenizer using a given
+   * {@link org.apache.lucene.util.AttributeSource.AttributeFactory}.
+   * 
+   * @deprecated use {@link #WhitespaceTokenizer(Version, AttributeSource.AttributeFactory, Reader)}
+   *             instead. This will be removed in Lucene 4.0.
+   */
+  @Deprecated
   public WhitespaceTokenizer(AttributeFactory factory, Reader in) {
     super(factory, in);
   }
   
   /** Collects only characters which do not satisfy
-   * {@link Character#isWhitespace(char)}.*/
+   * {@link Character#isWhitespace(int)}.*/
   @Override
-  protected boolean isTokenChar(char c) {
+  protected boolean isTokenChar(int c) {
     return !Character.isWhitespace(c);
   }
 }
Index: src/java/org/apache/lucene/analysis/CharArraySet.java
===================================================================
--- src/java/org/apache/lucene/analysis/CharArraySet.java	(revision 916567)
+++ src/java/org/apache/lucene/analysis/CharArraySet.java	(working copy)
@@ -1,15 +1,5 @@
 package org.apache.lucene.analysis;
 
-import java.util.Arrays;
-import java.util.AbstractSet;
-import java.util.Collection;
-import java.util.Collections;
-import java.util.Iterator;
-import java.util.Set;
-
-import org.apache.lucene.util.CharacterUtils;
-import org.apache.lucene.util.Version;
-
 /**
  * Licensed to the Apache Software Foundation (ASF) under one or more
  * contributor license agreements.  See the NOTICE file distributed with
@@ -27,7 +17,14 @@
  * limitations under the License.
  */
 
+import java.util.Arrays;
+import java.util.AbstractSet;
+import java.util.Collection;
+import java.util.Iterator;
+import java.util.Set;
 
+import org.apache.lucene.util.Version;
+
 /**
  * A simple class that stores Strings as char[]'s in a
  * hash table.  Note that this is not a general purpose
@@ -58,16 +55,11 @@
  * For type safety also {@link #stringIterator()} is provided.
  */
 public class CharArraySet extends AbstractSet<Object> {
-  private final static int INIT_SIZE = 8;
-  private char[][] entries;
-  private int count;
-  private final boolean ignoreCase;
-  public static final CharArraySet EMPTY_SET = CharArraySet.unmodifiableSet(
-      new CharArraySet(Version.LUCENE_CURRENT, 0, false));
+  public static final CharArraySet EMPTY_SET = new CharArraySet(CharArrayMap.<Object>emptyMap());
+  private static final Object PLACEHOLDER = new Object();
   
-  private final CharacterUtils charUtils;
-  private final Version matchVersion;
-
+  private final CharArrayMap<Object> map;
+  
   /**
    * Create set with enough capacity to hold startSize terms
    * 
@@ -81,13 +73,7 @@
    *          otherwise <code>true</code>.
    */
   public CharArraySet(Version matchVersion, int startSize, boolean ignoreCase) {
-    this.ignoreCase = ignoreCase;
-    int size = INIT_SIZE;
-    while(startSize + (startSize>>2) > size)
-      size <<= 1;
-    entries = new char[size][];
-    this.charUtils = CharacterUtils.getInstance(matchVersion);
-    this.matchVersion = matchVersion;
+    this(new CharArrayMap<Object>(matchVersion, startSize, ignoreCase));
   }
 
   /**
@@ -102,7 +88,7 @@
    *          <code>false</code> if and only if the set should be case sensitive
    *          otherwise <code>true</code>.
    */
-  public CharArraySet(Version matchVersion, Collection<? extends Object> c, boolean ignoreCase) {
+  public CharArraySet(Version matchVersion, Collection<?> c, boolean ignoreCase) {
     this(matchVersion, c.size(), ignoreCase);
     addAll(c);
   }
@@ -133,77 +119,51 @@
    * @deprecated use {@link #CharArraySet(Version, Collection, boolean)} instead         
    */  
   @Deprecated
-  public CharArraySet(Collection<? extends Object> c, boolean ignoreCase) {
+  public CharArraySet(Collection<?> c, boolean ignoreCase) {
     this(Version.LUCENE_30, c.size(), ignoreCase);
     addAll(c);
   }
   
-  /** Create set from entries */
-  private CharArraySet(Version matchVersion, char[][] entries, boolean ignoreCase, int count){
-    this.entries = entries;
-    this.ignoreCase = ignoreCase;
-    this.count = count;
-    this.charUtils = CharacterUtils.getInstance(matchVersion);
-    this.matchVersion = matchVersion;
+  /** Create set from the specified map (internal only), used also by {@link CharArrayMap#keySet()} */
+  CharArraySet(final CharArrayMap<Object> map){
+    this.map = map;
   }
   
   /** Clears all entries in this set. This method is supported for reusing, but not {@link Set#remove}. */
   @Override
   public void clear() {
-    count = 0;
-    Arrays.fill(entries, null);
+    map.clear();
   }
 
   /** true if the <code>len</code> chars of <code>text</code> starting at <code>off</code>
    * are in the set */
   public boolean contains(char[] text, int off, int len) {
-    return entries[getSlot(text, off, len)] != null;
+    return map.containsKey(text, off, len);
   }
 
   /** true if the <code>CharSequence</code> is in the set */
   public boolean contains(CharSequence cs) {
-    return entries[getSlot(cs)] != null;
+    return map.containsKey(cs);
   }
 
-  private int getSlot(char[] text, int off, int len) {
-    int code = getHashCode(text, off, len);
-    int pos = code & (entries.length-1);
-    char[] text2 = entries[pos];
-    if (text2 != null && !equals(text, off, len, text2)) {
-      final int inc = ((code>>8)+code)|1;
-      do {
-        code += inc;
-        pos = code & (entries.length-1);
-        text2 = entries[pos];
-      } while (text2 != null && !equals(text, off, len, text2));
-    }
-    return pos;
+  @Override
+  public boolean contains(Object o) {
+    return map.containsKey(o);
   }
 
-  /** Returns true if the String is in the set */  
-  private int getSlot(CharSequence text) {
-    int code = getHashCode(text);
-    int pos = code & (entries.length-1);
-    char[] text2 = entries[pos];
-    if (text2 != null && !equals(text, text2)) {
-      final int inc = ((code>>8)+code)|1;
-      do {
-        code += inc;
-        pos = code & (entries.length-1);
-        text2 = entries[pos];
-      } while (text2 != null && !equals(text, text2));
-    }
-    return pos;
+  @Override
+  public boolean add(Object o) {
+    return map.put(o, PLACEHOLDER) == null;
   }
 
   /** Add this CharSequence into the set */
   public boolean add(CharSequence text) {
-    return add(text.toString()); // could be more efficient
+    return map.put(text, PLACEHOLDER) == null;
   }
   
   /** Add this String into the set */
   public boolean add(String text) {
-    return add(text.toCharArray());
+    return map.put(text, PLACEHOLDER) == null;
   }
 
   /** Add this char[] directly to the set.
@@ -211,141 +171,13 @@
    * The user should never modify this text array after calling this method.
    */
   public boolean add(char[] text) {
-    if (ignoreCase)
-      for(int i=0;i<text.length;){
-        i += Character.toChars(
-              Character.toLowerCase(
-                  charUtils.codePointAt(text, i)), text, i);
-      }
-    int slot = getSlot(text, 0, text.length);
-    if (entries[slot] != null) return false;
-    entries[slot] = text;
-    count++;
-
-    if (count + (count>>2) > entries.length) {
-      rehash();
-    }
-
-    return true;
+    return map.put(text, PLACEHOLDER) == null;
   }
 
-  private boolean equals(char[] text1, int off, int len, char[] text2) {
-    if (len != text2.length)
-      return false;
-    final int limit = off+len;
-    if (ignoreCase) {
-      for(int i=0;i<len;) {
-        final int codePointAt = charUtils.codePointAt(text1, off+i, limit);
-        if (Character.toLowerCase(codePointAt) != charUtils.codePointAt(text2, i))
-          return false;
-        i += Character.charCount(codePointAt); 
-      }
-    } else {
-      for(int i=0;i<len;i++) {
-        if (text1[off+i] != text2[i])
-          return false;
-      }
-    }
-    return true;
-  }
-
-  private boolean equals(CharSequence text1, char[] text2) {
-    int len = text1.length();
-    if (len != text2.length)
-      return false;
-    if (ignoreCase) {
-      for(int i=0;i<len;) {
-        final int codePointAt = charUtils.codePointAt(text1, i);
-        if (Character.toLowerCase(codePointAt) != charUtils.codePointAt(text2, i))
-          return false;
-        i += Character.charCount(codePointAt);
-      }
-    } else {
-      for(int i=0;i<len;i++) {
-        if (text1.charAt(i) != text2[i])
-          return false;
-      }
-    }
-    return true;
-  }
-  
-
-
-  private void rehash() {
-    final int newSize = 2*entries.length;
-    char[][] oldEntries = entries;
-    entries = new char[newSize][];
-
-    for(int i=0;i<oldEntries.length;i++) {
-      char[] text = oldEntries[i];
-      if (text != null) {
-        // todo: could be faster... no need to compare strings on collision
-        entries[getSlot(text,0,text.length)] = text;
-      }
-    }
-  }
-  
-  private int getHashCode(char[] text, int offset, int len) {
-    int code = 0;
-    final int stop = offset + len;
-    if (ignoreCase) {
-      for (int i=offset; i<stop;) {
-        final int codePointAt = charUtils.codePointAt(text, i, stop);
-        code = code*31 + Character.toLowerCase(codePointAt);
-        i += Character.charCount(codePointAt);
-      }
-    } else {
-      for (int i=offset; i<stop; i++) {
-        code = code*31 + text[i];
-      }
-    }
-    return code;
-  }
-
-  private int getHashCode(CharSequence text) {
-    int code = 0;
-    int len = text.length();
-    if (ignoreCase) {
-      for (int i=0; i<len;) {
-        int codePointAt = charUtils.codePointAt(text, i);
-        code = code*31 + Character.toLowerCase(codePointAt);
-        i += Character.charCount(codePointAt);
-      }
-    } else {
-      for (int i=0; i<len; i++) {
-        code = code*31 + text.charAt(i);
-      }
-    }
-    return code;
-  }
-
-
   @Override
   public int size() {
-    return count;
+    return map.size();
   }
-
-  @Override
-  public boolean isEmpty() {
-    return count==0;
-  }
-
-  @Override
-  public boolean contains(Object o) {
-    if (o instanceof char[]) {
-      final char[] text = (char[])o;
-      return contains(text, 0, text.length);
-    } 
-    return contains(o.toString());
-  }
-
-  @Override
-  public boolean add(Object o) {
-    if (o instanceof char[]) {
-      return add((char[])o);
-    }
-    return add(o.toString());
-  }
   
   /**
    * Returns an unmodifiable {@link CharArraySet}. This allows to provide
@@ -362,14 +194,9 @@
       throw new NullPointerException("Given set is null");
     if (set == EMPTY_SET)
       return EMPTY_SET;
-    if (set instanceof UnmodifiableCharArraySet)
+    if (set.map instanceof CharArrayMap.UnmodifiableCharArrayMap)
       return set;
-
-    /*
-     * Instead of delegating calls to the given set copy the low-level values to
-     * the unmodifiable Subclass
-     */
-    return new UnmodifiableCharArraySet(set.matchVersion, set.entries, set.ignoreCase, set.count);
+    return new CharArraySet(CharArrayMap.unmodifiableMap(set.map));
   }
 
   /**
@@ -387,7 +214,7 @@
   public static CharArraySet copy(final Set<?> set) {
     if(set == EMPTY_SET)
       return EMPTY_SET;
-    return (set instanceof CharArraySet) ? copy((CharArraySet) set) : copy(Version.LUCENE_30, set);
+    return copy(Version.LUCENE_30, set);
   }
   
   /**
@@ -416,29 +243,27 @@
       return EMPTY_SET;
     if(set instanceof CharArraySet) {
       final CharArraySet source = (CharArraySet) set;
-      // use fast path instead of iterating all values
-      // this is even on very small sets ~10 times faster than iterating
-      final char[][] entries = new char[source.entries.length][];
-      System.arraycopy(source.entries, 0, entries, 0, entries.length);
-      return new CharArraySet(source.matchVersion, entries, source.ignoreCase, source.count);
+      return new CharArraySet(CharArrayMap.copy(source.map.matchVersion, source.map));
     }
     return new CharArraySet(matchVersion, set, false);
   }
   
-
   /** The Iterator<String> for this set.  Strings are constructed on the fly, so
-   * use <code>nextCharArray</code> for more efficient access. */
+   * use <code>nextCharArray</code> for more efficient access.
+   * @deprecated Use the standard iterator, which returns {@code char[]} instances.
+   */
+  @Deprecated
   public class CharArraySetIterator implements Iterator<String> {
     int pos=-1;
     char[] next;
-    CharArraySetIterator() {
+    private CharArraySetIterator() {
       goNext();
     }
 
     private void goNext() {
       next = null;
       pos++;
-      while (pos < entries.length && (next=entries[pos]) == null) pos++;
+      while (pos < map.keys.length && (next=map.keys[pos]) == null) pos++;
     }
 
     public boolean hasNext() {
@@ -463,61 +288,41 @@
     }
   }
 
-  /** returns an iterator of new allocated Strings */
+  /** returns an iterator of new allocated Strings (an instance of {@link CharArraySetIterator}).
+   * @deprecated Use {@link #iterator}, which returns {@code char[]} instances.
+   */
+  @Deprecated
   public Iterator<String> stringIterator() {
     return new CharArraySetIterator();
   }
 
-  /** returns an iterator of new allocated Strings, this method violates the Set interface */
-  @Override
-  @SuppressWarnings("unchecked")
+  /** Returns an {@link Iterator} depending on the version used:
+   * <ul>
+   * <li>if {@code matchVersion} &ge; 3.1, it returns {@code char[]} instances in this set.</li>
+   * <li>if {@code matchVersion} is 3.0 or older, it returns new
+   * allocated Strings, so this method violates the Set interface.
+   * It is kept this way for backwards compatibility, normally it should
+   * return {@code char[]} on {@code next()}</li>
+   * </ul>
+   */
+  @Override @SuppressWarnings("unchecked")
   public Iterator<Object> iterator() {
-    return (Iterator) stringIterator();
+    // use the AbstractSet#keySet()'s iterator (to not produce endless recursion)
+    return map.matchVersion.onOrAfter(Version.LUCENE_31) ?
+      map.originalKeySet().iterator() : (Iterator) stringIterator();
   }
   
-  /**
-   * Efficient unmodifiable {@link CharArraySet}. This implementation does not
-   * delegate calls to a give {@link CharArraySet} like
-   * {@link Collections#unmodifiableSet(java.util.Set)} does. Instead is passes
-   * the internal representation of a {@link CharArraySet} to a super
-   * constructor and overrides all mutators. 
-   */
-  private static final class UnmodifiableCharArraySet extends CharArraySet {
-
-    private UnmodifiableCharArraySet(Version matchVersion, char[][] entries, boolean ignoreCase,
-        int count) {
-      super(matchVersion, entries, ignoreCase, count);
+  @Override
+  public String toString() {
+    final StringBuilder sb = new StringBuilder("[");
+    for (Object item : this) {
+      if (sb.length()>1) sb.append(", ");
+      if (item instanceof char[]) {
+        sb.append((char[]) item);
+      } else {
+        sb.append(item);
+      }
     }
-
-    @Override
-    public void clear() {
-      throw new UnsupportedOperationException();
-    }
-
-    @Override
-    public boolean add(Object o){
-      throw new UnsupportedOperationException();
-    }
-    
-    @Override
-    public boolean addAll(Collection<? extends Object> coll) {
-      throw new UnsupportedOperationException();
-    }
-    
-    @Override
-    public boolean add(char[] text) {
-      throw new UnsupportedOperationException();
-    }
-
-    @Override
-    public boolean add(CharSequence text) {
-      throw new UnsupportedOperationException();
-    }
-
-    @Override
-    public boolean add(String text) {
-      throw new UnsupportedOperationException();
-    }
+    return sb.append(']').toString();
   }
-
 }
Index: src/java/org/apache/lucene/analysis/LetterTokenizer.java
===================================================================
--- src/java/org/apache/lucene/analysis/LetterTokenizer.java	(revision 916567)
+++ src/java/org/apache/lucene/analysis/LetterTokenizer.java	(working copy)
@@ -20,34 +20,106 @@
 import java.io.Reader;
 
 import org.apache.lucene.util.AttributeSource;
+import org.apache.lucene.util.Version;
 
-/** A LetterTokenizer is a tokenizer that divides text at non-letters.  That's
-  to say, it defines tokens as maximal strings of adjacent letters, as defined
-  by java.lang.Character.isLetter() predicate.
+/**
+ * A LetterTokenizer is a tokenizer that divides text at non-letters. That's to
+ * say, it defines tokens as maximal strings of adjacent letters, as defined by
+ * java.lang.Character.isLetter() predicate.
+ * <p>
+ * Note: this does a decent job for most European languages, but does a terrible
+ * job for some Asian languages, where words are not separated by spaces.
+ * </p>
+ * <p>
+ * <a name="version"/>
+ * You must specify the required {@link Version} compatibility when creating
+ * {@link LetterTokenizer}:
+ * <ul>
+ * <li>As of 3.1, {@link CharTokenizer} uses an int based API to normalize and
+ * detect token characters. See {@link CharTokenizer#isTokenChar(int)} and
+ * {@link CharTokenizer#normalize(int)} for details.</li>
+ * </ul>
+ * </p>
+ */
 
-  Note: this does a decent job for most European languages, but does a terrible
-  job for some Asian languages, where words are not separated by spaces. */
-
 public class LetterTokenizer extends CharTokenizer {
-  /** Construct a new LetterTokenizer. */
+  
+  /**
+   * Construct a new LetterTokenizer.
+   * 
+   * @param matchVersion
+   *          Lucene version to match See {@link <a href="#version">above</a>}
+   * @param in
+   *          the input to split up into tokens
+   */
+  public LetterTokenizer(Version matchVersion, Reader in) {
+    super(matchVersion, in);
+  }
+  
+  /**
+   * Construct a new LetterTokenizer using a given {@link AttributeSource}.
+   * 
+   * @param matchVersion
+   *          Lucene version to match See {@link <a href="#version">above</a>}
+   * @param source
+   *          the attribute source to use for this {@link Tokenizer}
+   * @param in
+   *          the input to split up into tokens
+   */
+  public LetterTokenizer(Version matchVersion, AttributeSource source, Reader in) {
+    super(matchVersion, source, in);
+  }
+  
+  /**
+   * Construct a new LetterTokenizer using a given
+   * {@link org.apache.lucene.util.AttributeSource.AttributeFactory}.
+   * 
+   * @param matchVersion
+   *          Lucene version to match See {@link <a href="#version">above</a>}
+   * @param factory
+   *          the attribute factory to use for this {@link Tokenizer}
+   * @param in
+   *          the input to split up into tokens
+   */
+  public LetterTokenizer(Version matchVersion, AttributeFactory factory, Reader in) {
+    super(matchVersion, factory, in);
+  }
+  
+  /**
+   * Construct a new LetterTokenizer.
+   * 
+   * @deprecated use {@link #LetterTokenizer(Version, Reader)} instead. This
+   *             will be removed in Lucene 4.0.
+   */
   public LetterTokenizer(Reader in) {
-    super(in);
+    super(Version.LUCENE_30, in);
   }
   
-  /** Construct a new LetterTokenizer using a given {@link AttributeSource}. */
+  /**
+   * Construct a new LetterTokenizer using a given {@link AttributeSource}. 
+   * @deprecated
+   * use {@link #LetterTokenizer(Version, AttributeSource, Reader)} instead.
+   * This will be removed in Lucene 4.0.
+   */
   public LetterTokenizer(AttributeSource source, Reader in) {
-    super(source, in);
+    super(Version.LUCENE_30, source, in);
   }
   
-  /** Construct a new LetterTokenizer using a given {@link org.apache.lucene.util.AttributeSource.AttributeFactory}. */
+  /**
+   * Construct a new LetterTokenizer using a given
+   * {@link org.apache.lucene.util.AttributeSource.AttributeFactory}.
+   * 
+   * @deprecated use {@link #LetterTokenizer(Version, AttributeSource.AttributeFactory, Reader)}
+   *             instead. This will be removed in Lucene 4.0.
+   */
   public LetterTokenizer(AttributeFactory factory, Reader in) {
-    super(factory, in);
+    super(Version.LUCENE_30, factory, in);
   }
-
+  
   /** Collects only characters which satisfy
-   * {@link Character#isLetter(char)}.*/
+   * {@link Character#isLetter(int)}.*/
   @Override
-  protected boolean isTokenChar(char c) {
+  protected boolean isTokenChar(int c) {
     return Character.isLetter(c);
   }
 }
Index: src/java/org/apache/lucene/analysis/StopAnalyzer.java
===================================================================
--- src/java/org/apache/lucene/analysis/StopAnalyzer.java	(revision 916567)
+++ src/java/org/apache/lucene/analysis/StopAnalyzer.java	(working copy)
@@ -99,7 +99,7 @@
   @Override
   protected TokenStreamComponents createComponents(String fieldName,
       Reader reader) {
-    final Tokenizer source = new LowerCaseTokenizer(reader);
+    final Tokenizer source = new LowerCaseTokenizer(matchVersion, reader);
     return new TokenStreamComponents(source, new StopFilter(matchVersion,
           source, stopwords));
   }
Index: src/java/org/apache/lucene/search/FieldCache.java
===================================================================
--- src/java/org/apache/lucene/search/FieldCache.java	(revision 916567)
+++ src/java/org/apache/lucene/search/FieldCache.java	(working copy)
@@ -516,12 +516,7 @@
   /**
    * EXPERT: A unique Identifier/Description for each item in the FieldCache. 
    * Can be useful for logging/debugging.
-   * <p>
-   * <b>EXPERIMENTAL API:</b> This API is considered extremely advanced 
-   * and experimental.  It may be removed or altered w/o warning in future 
-   * releases 
-   * of Lucene.
-   * </p>
+   * @lucene.experimental
    */
   public static abstract class CacheEntry {
     public abstract Object getReaderKey();
@@ -586,12 +581,7 @@
    * associated with it has garbage collected will prevent the Value itself
    * from being garbage collected when the Cache drops the WeakReference.
    * </p>
-   * <p>
-   * <b>EXPERIMENTAL API:</b> This API is considered extremely advanced 
-   * and experimental.  It may be removed or altered w/o warning in future 
-   * releases 
-   * of Lucene.
-   * </p>
+   * @lucene.experimental
    */
   public abstract CacheEntry[] getCacheEntries();
 
@@ -604,12 +594,7 @@
    * It should not be relied on for "Cache maintenance" in general 
    * application code.
    * </p>
-   * <p>
-   * <b>EXPERIMENTAL API:</b> This API is considered extremely advanced 
-   * and experimental.  It may be removed or altered w/o warning in future 
-   * releases 
-   * of Lucene.
-   * </p>
+   * @lucene.experimental
    */
   public abstract void purgeAllCaches();
 
Index: src/java/org/apache/lucene/search/MultiTermQuery.java
===================================================================
--- src/java/org/apache/lucene/search/MultiTermQuery.java	(revision 916567)
+++ src/java/org/apache/lucene/search/MultiTermQuery.java	(working copy)
@@ -55,7 +55,7 @@
  * computing unhelpful scores, and it tries to pick the most
  * performant rewrite method given the query. If you
  * need scoring (like {@link FuzzyQuery}, use
- * {@link #TOP_TERMS_SCORING_BOOLEAN_REWRITE} which uses
+ * {@link TopTermsScoringBooleanQueryRewrite} which uses
  * a priority queue to only collect competitive terms
  * and not hit this limitation.
  *
@@ -268,10 +268,41 @@
    *  @see #setRewriteMethod */
   public final static RewriteMethod SCORING_BOOLEAN_QUERY_REWRITE = new ScoringBooleanQueryRewrite();
 
-  private static final class TopTermsScoringBooleanQueryRewrite extends BooleanQueryRewrite {
+  /** A rewrite method that first translates each term into
+   *  {@link BooleanClause.Occur#SHOULD} clause in a
+   *  BooleanQuery, and keeps the scores as computed by the
+   *  query.
+   *
+   * <p>This rewrite mode only uses the top scoring terms
+   * so it will not overflow the boolean max clause count.
+   * It is the default rewrite mode for {@link FuzzyQuery}.
+   *
+   *  @see #setRewriteMethod */
+  public static final class TopTermsScoringBooleanQueryRewrite extends BooleanQueryRewrite {
+    private final int size;
+    
+    /** 
+     * Create a TopTermsScoringBooleanQueryRewrite for 
+     * at most <code>size</code> terms.
+     * <p>
+     * NOTE: if {@link BooleanQuery#getMaxClauseCount} is smaller than 
+     * <code>size</code>, then it will be used instead. 
+     */
+    public TopTermsScoringBooleanQueryRewrite(int size) {
+      this.size = size;
+    }
+    
+    /** 
+     * Create a TopTermsScoringBooleanQueryRewrite that is limited
+     * to at most {@link BooleanQuery#getMaxClauseCount} terms. 
+     */
+    public TopTermsScoringBooleanQueryRewrite() {
+      this(Integer.MAX_VALUE);
+    }
+    
     @Override
     public Query rewrite(IndexReader reader, MultiTermQuery query) throws IOException {
-      final int maxSize = BooleanQuery.getMaxClauseCount();
+      final int maxSize = Math.min(size, BooleanQuery.getMaxClauseCount());
       final PriorityQueue<ScoreTerm> stQueue = new PriorityQueue<ScoreTerm>();
       collectTerms(reader, query, new TermCollector() {
         public boolean collect(Term t, float boost) {
@@ -301,10 +332,23 @@
       query.incTotalNumberOfTerms(bq.clauses().size());
       return bq;
     }
+  
+    @Override
+    public int hashCode() {
+      final int prime = 17;
+      int result = 1;
+      result = prime * result + size;
+      return result;
+    }
 
-    // Make sure we are still a singleton even after deserializing
-    protected Object readResolve() {
-      return TOP_TERMS_SCORING_BOOLEAN_REWRITE;
+    @Override
+    public boolean equals(Object obj) {
+      if (this == obj) return true;
+      if (obj == null) return false;
+      if (getClass() != obj.getClass()) return false;
+      TopTermsScoringBooleanQueryRewrite other = (TopTermsScoringBooleanQueryRewrite) obj;
+      if (size != other.size) return false;
+      return true;
     }
   
     private static class ScoreTerm implements Comparable<ScoreTerm> {
@@ -319,18 +363,6 @@
       }
     }
   }
-  
-  /** A rewrite method that first translates each term into
-   *  {@link BooleanClause.Occur#SHOULD} clause in a
-   *  BooleanQuery, and keeps the scores as computed by the
-   *  query.
-   *
-   * <p>This rewrite mode only uses the top scoring terms
-   * so it will not overflow the boolean max clause count.
-   * It is the default rewrite mode for {@link FuzzyQuery}.
-   *
-   *  @see #setRewriteMethod */
-  public final static RewriteMethod TOP_TERMS_SCORING_BOOLEAN_REWRITE = new TopTermsScoringBooleanQueryRewrite();
 
   private static class ConstantScoreBooleanQueryRewrite extends ScoringBooleanQueryRewrite implements Serializable {
     @Override
Index: src/java/org/apache/lucene/search/FuzzyQuery.java
===================================================================
--- src/java/org/apache/lucene/search/FuzzyQuery.java	(revision 916567)
+++ src/java/org/apache/lucene/search/FuzzyQuery.java	(working copy)
@@ -31,7 +31,7 @@
  * length of 0 - in this case, *every* term will be enumerated and
  * cause an edit score calculation.
  * 
- * <p>This query uses {@link MultiTermQuery#TOP_TERMS_SCORING_BOOLEAN_REWRITE}
+ * <p>This query uses {@link MultiTermQuery.TopTermsScoringBooleanQueryRewrite}
  * as default. So terms will be collected and scored according to their
  * edit distance. Only the top terms are used for building the {@link BooleanQuery}.
  * It is not recommended to change the rewrite mode for fuzzy queries.
@@ -40,6 +40,7 @@
   
   public final static float defaultMinSimilarity = 0.5f;
   public final static int defaultPrefixLength = 0;
+  public final static int defaultMaxExpansions = Integer.MAX_VALUE;
   
   private float minimumSimilarity;
   private int prefixLength;
@@ -60,13 +61,16 @@
    *  as the query term is considered similar to the query term if the edit distance
    *  between both terms is less than <code>length(term)*0.5</code>
    * @param prefixLength length of common (non-fuzzy) prefix
+   * @param maxExpansions the maximum number of terms to match. If this number is
+   *  greater than {@link BooleanQuery#getMaxClauseCount} when the query is rewritten, 
+   *  then the maxClauseCount will be used instead.
    * @throws IllegalArgumentException if minimumSimilarity is &gt;= 1 or &lt; 0
    * or if prefixLength &lt; 0
    */
-  public FuzzyQuery(Term term, float minimumSimilarity, int prefixLength) throws IllegalArgumentException {
+  public FuzzyQuery(Term term, float minimumSimilarity, int prefixLength,
+      int maxExpansions) {
     super(term.field());
     this.term = term;
-    setRewriteMethod(TOP_TERMS_SCORING_BOOLEAN_REWRITE);
     
     if (minimumSimilarity >= 1.0f)
       throw new IllegalArgumentException("minimumSimilarity >= 1");
@@ -74,7 +78,11 @@
       throw new IllegalArgumentException("minimumSimilarity < 0");
     if (prefixLength < 0)
       throw new IllegalArgumentException("prefixLength < 0");
+    if (maxExpansions < 0)
+      throw new IllegalArgumentException("maxExpansions < 0");
     
+    setRewriteMethod(new MultiTermQuery.TopTermsScoringBooleanQueryRewrite(maxExpansions));
+    
     if (term.text().length() > 1.0f / (1.0f - minimumSimilarity)) {
       this.termLongEnough = true;
     }
@@ -84,17 +92,24 @@
   }
   
   /**
-   * Calls {@link #FuzzyQuery(Term, float) FuzzyQuery(term, minimumSimilarity, 0)}.
+   * Calls {@link #FuzzyQuery(Term, float) FuzzyQuery(term, minimumSimilarity, prefixLength, Integer.MAX_VALUE)}.
    */
-  public FuzzyQuery(Term term, float minimumSimilarity) throws IllegalArgumentException {
-    this(term, minimumSimilarity, defaultPrefixLength);
+  public FuzzyQuery(Term term, float minimumSimilarity, int prefixLength) {
+    this(term, minimumSimilarity, prefixLength, defaultMaxExpansions);
   }
+  
+  /**
+   * Calls {@link #FuzzyQuery(Term, float) FuzzyQuery(term, minimumSimilarity, 0, Integer.MAX_VALUE)}.
+   */
+  public FuzzyQuery(Term term, float minimumSimilarity) {
+    this(term, minimumSimilarity, defaultPrefixLength, defaultMaxExpansions);
+  }
 
   /**
-   * Calls {@link #FuzzyQuery(Term, float) FuzzyQuery(term, 0.5f, 0)}.
+   * Calls {@link #FuzzyQuery(Term, float) FuzzyQuery(term, 0.5f, 0, Integer.MAX_VALUE)}.
    */
   public FuzzyQuery(Term term) {
-    this(term, defaultMinSimilarity, defaultPrefixLength);
+    this(term, defaultMinSimilarity, defaultPrefixLength, defaultMaxExpansions);
   }
   
   /**

Property changes on: src\java\org\apache\lucene\search\MultiTermQueryWrapperFilter.java
___________________________________________________________________
Added: svn:mergeinfo
   Merged /lucene/java/branches/lucene_2_9/src/java/org/apache/lucene/search/MultiTermQueryWrapperFilter.java:r909334

Index: src/java/org/apache/lucene/search/NumericRangeFilter.java
===================================================================
--- src/java/org/apache/lucene/search/NumericRangeFilter.java	(revision 916567)
+++ src/java/org/apache/lucene/search/NumericRangeFilter.java	(working copy)
@@ -39,9 +39,7 @@
  * See {@link NumericRangeQuery} for details on how Lucene
  * indexes and searches numeric valued fields.
  *
- * <p><font color="red"><b>NOTE:</b> This API is experimental and
- * might change in incompatible ways in the next
- * release.</font>
+ * @lucene.experimental
  *
  * @since 2.9
  **/
Index: src/java/org/apache/lucene/search/Filter.java
===================================================================
--- src/java/org/apache/lucene/search/Filter.java	(revision 916567)
+++ src/java/org/apache/lucene/search/Filter.java	(working copy)
@@ -23,21 +23,20 @@
 import org.apache.lucene.util.DocIdBitSet;
 
 /** 
- *  Abstract base class providing a mechanism to use a subset of an index
- *  for restriction or permission of index search results.
- *  <p>
+ *  Abstract base class for restricting which documents may
+ *  be returned during searching.
  */
 public abstract class Filter implements java.io.Serializable {
   
   /**
-   * Creates a {@link DocIdSet} that provides the documents which should be
-   * permitted or prohibited in search results. <b>NOTE:</b> null can be
-   * returned if no documents will be accepted by this Filter.
+   * Creates a {@link DocIdSet} enumerating the documents that should be
+   * permitted in search results. <b>NOTE:</b> null can be
+   * returned if no documents are accepted by this Filter.
    * <p>
-   * Note: This method might be called more than once during a search if the
-   * index has more than one segment. In such a case the {@link DocIdSet}
-   * must be relative to the document base of the given reader. Yet, the
-   * segment readers are passed in increasing document base order.
+   * Note: This method will be called once per segment in
+   * the index during searching.  The returned {@link DocIdSet}
+   * must refer to document IDs for that segment, not for
+   * the top-level reader.
    * 
    * @param reader a {@link IndexReader} instance opened on the index currently
    *         searched on. Note, it is likely that the provided reader does not
Index: src/java/org/apache/lucene/search/Similarity.java
===================================================================
--- src/java/org/apache/lucene/search/Similarity.java	(revision 916567)
+++ src/java/org/apache/lucene/search/Similarity.java	(working copy)
@@ -602,8 +602,7 @@
    * {@link FieldInvertState#getLength()} as the second argument, and
    * then multiplies this value by {@link FieldInvertState#getBoost()}.</p>
    * 
-   * <p><b>WARNING</b>: This API is new and experimental and may
-   * suddenly change.</p>
+   * @lucene.experimental
    * 
    * @param field field name
    * @param state current processing state for this field
Index: src/java/org/apache/lucene/search/FieldValueHitQueue.java
===================================================================
--- src/java/org/apache/lucene/search/FieldValueHitQueue.java	(revision 916567)
+++ src/java/org/apache/lucene/search/FieldValueHitQueue.java	(working copy)
@@ -26,11 +26,8 @@
  * Uses <code>FieldCache.DEFAULT</code> for maintaining
  * internal term lookup tables.
  * 
- * <b>NOTE:</b> This API is experimental and might change in
- * incompatible ways in the next release.
- *
+ * @lucene.experimental
  * @since 2.9
- * @version $Id:
  * @see Searcher#search(Query,Filter,int,Sort)
  * @see FieldCache
  */
Index: src/java/org/apache/lucene/search/TopScoreDocCollector.java
===================================================================
--- src/java/org/apache/lucene/search/TopScoreDocCollector.java	(revision 916567)
+++ src/java/org/apache/lucene/search/TopScoreDocCollector.java	(working copy)
@@ -29,10 +29,10 @@
  * instance of this collector you should know in advance whether documents are
  * going to be collected in doc Id order or not.
  *
- * <p><b>NOTE</b>: The values Float.Nan,
- * Float.NEGATIVE_INFINITY and Float.POSITIVE_INFINITY are
- * not valid scores.  This collector will not properly
- * collect hits with such scores.
+ * <p><b>NOTE</b>: The values {@link Float#NaN} and
+ * {Float#NEGATIVE_INFINITY} are not valid scores.  This
+ * collector will not properly collect hits with such
+ * scores.
  */
 public abstract class TopScoreDocCollector extends TopDocsCollector<ScoreDoc> {
 
@@ -45,6 +45,11 @@
     @Override
     public void collect(int doc) throws IOException {
       float score = scorer.score();
+
+      // This collector cannot handle these scores:
+      assert score != Float.NEGATIVE_INFINITY;
+      assert !Float.isNaN(score);
+
       totalHits++;
       if (score <= pqTop.score) {
         // Since docs are returned in-order (i.e., increasing doc Id), a document
@@ -72,6 +77,10 @@
     @Override
     public void collect(int doc) throws IOException {
       float score = scorer.score();
+
+      // This collector cannot handle NaN
+      assert !Float.isNaN(score);
+
       totalHits++;
       doc += docBase;
       if (score < pqTop.score || (score == pqTop.score && doc > pqTop.doc)) {
Index: src/java/org/apache/lucene/search/payloads/PayloadFunction.java
===================================================================
--- src/java/org/apache/lucene/search/payloads/PayloadFunction.java	(revision 916567)
+++ src/java/org/apache/lucene/search/payloads/PayloadFunction.java	(working copy)
@@ -18,16 +18,15 @@
 
 import java.io.Serializable;
 
-
 /**
- * An abstract class that defines a way for Payload*Query instances
- * to transform the cumulative effects of payload scores for a document.
- *
+ * An abstract class that defines a way for Payload*Query instances to transform
+ * the cumulative effects of payload scores for a document.
+ * 
  * @see org.apache.lucene.search.payloads.PayloadTermQuery for more information
- *
- * <p/>
- * This class and its derivations are experimental and subject to change
- *
+ * 
+ * @lucene.experimental This class and its derivations are experimental and subject to
+ *               change
+ * 
  **/
 public abstract class PayloadFunction implements Serializable {
 
Index: src/java/org/apache/lucene/search/payloads/MinPayloadFunction.java
===================================================================
--- src/java/org/apache/lucene/search/payloads/MinPayloadFunction.java	(revision 916567)
+++ src/java/org/apache/lucene/search/payloads/MinPayloadFunction.java	(working copy)
@@ -23,9 +23,13 @@
  **/
 public class MinPayloadFunction extends PayloadFunction {
 
-    @Override
-    public float currentScore(int docId, String field, int start, int end, int numPayloadsSeen, float currentScore, float currentPayloadScore) {
-    return Math.min(currentPayloadScore, currentScore);
+  @Override
+  public float currentScore(int docId, String field, int start, int end, int numPayloadsSeen, float currentScore, float currentPayloadScore) {
+    if (numPayloadsSeen == 0) {
+      return currentPayloadScore;
+    } else {
+      return Math.min(currentPayloadScore, currentScore);
+    }
   }
 
   @Override
Index: src/java/org/apache/lucene/search/payloads/MaxPayloadFunction.java
===================================================================
--- src/java/org/apache/lucene/search/payloads/MaxPayloadFunction.java	(revision 916567)
+++ src/java/org/apache/lucene/search/payloads/MaxPayloadFunction.java	(working copy)
@@ -27,7 +27,11 @@
 public class MaxPayloadFunction extends PayloadFunction {
   @Override
   public float currentScore(int docId, String field, int start, int end, int numPayloadsSeen, float currentScore, float currentPayloadScore) {
-    return Math.max(currentPayloadScore, currentScore);
+    if (numPayloadsSeen == 0) {
+      return currentPayloadScore;
+    } else {
+      return Math.max(currentPayloadScore, currentScore);
+    }
   }
 
   @Override
Index: src/java/org/apache/lucene/search/payloads/PayloadSpanUtil.java
===================================================================
--- src/java/org/apache/lucene/search/payloads/PayloadSpanUtil.java	(revision 916567)
+++ src/java/org/apache/lucene/search/payloads/PayloadSpanUtil.java	(working copy)
@@ -44,11 +44,7 @@
  * Operates like Highlighter - IndexReader should only contain doc of interest,
  * best to use MemoryIndex.
  *
- * <p/>
- * <font color="#FF0000">
-   * WARNING: The status of the <b>Payloads</b> feature is experimental.
-   * The APIs introduced here might change in the future and will not be
-   * supported anymore in such a case.</font>
+ * @lucene.experimental
  * 
  */
 public class PayloadSpanUtil {
Index: src/java/org/apache/lucene/search/SpanQueryFilter.java
===================================================================
--- src/java/org/apache/lucene/search/SpanQueryFilter.java	(revision 916567)
+++ src/java/org/apache/lucene/search/SpanQueryFilter.java	(working copy)
@@ -34,9 +34,6 @@
  * <p/>
  * This filter does not cache.  See the {@link org.apache.lucene.search.CachingSpanFilter} for a wrapper that
  * caches.
- *
- *
- * @version $Id:$
  */
 public class SpanQueryFilter extends SpanFilter {
   protected SpanQuery query;
Index: src/java/org/apache/lucene/search/spans/Spans.java
===================================================================
--- src/java/org/apache/lucene/search/spans/Spans.java	(revision 916567)
+++ src/java/org/apache/lucene/search/spans/Spans.java	(working copy)
@@ -66,10 +66,7 @@
    * <br>
     * Note that the return type is a collection, thus the ordering should not be relied upon.
     * <br/>
-   * <p><font color="#FF0000">
-   * WARNING: The status of the <b>Payloads</b> feature is experimental.
-   * The APIs introduced here might change in the future and will not be
-   * supported anymore in such a case.</font>
+   * @lucene.experimental
    *
    * @return a List of byte arrays containing the data of this payload, otherwise null if isPayloadAvailable is false
    * @throws java.io.IOException
Index: src/java/org/apache/lucene/search/FuzzyTermEnum.java
===================================================================
--- src/java/org/apache/lucene/search/FuzzyTermEnum.java	(revision 916567)
+++ src/java/org/apache/lucene/search/FuzzyTermEnum.java	(working copy)
@@ -193,7 +193,7 @@
    * @return the similarity,  0.0 or less indicates that it matches less than the required
    * threshold and 1.0 indicates that the text and target are identical
    */
-  private synchronized final float similarity(final String target) {
+  private float similarity(final String target) {
     final int m = target.length();
     final int n = text.length();
     if (n == 0)  {
Index: src/java/org/apache/lucene/search/SpanFilterResult.java
===================================================================
--- src/java/org/apache/lucene/search/SpanFilterResult.java	(revision 916567)
+++ src/java/org/apache/lucene/search/SpanFilterResult.java	(working copy)
@@ -23,8 +23,7 @@
 /**
  *  The results of a SpanQueryFilter.  Wraps the BitSet and the position information from the SpanQuery
  *
- *<p/>
- * NOTE: This API is still experimental and subject to change. 
+ * @lucene.experimental 
  *
  **/
 public class SpanFilterResult {
Index: src/java/org/apache/lucene/search/Collector.java
===================================================================
--- src/java/org/apache/lucene/search/Collector.java	(revision 916567)
+++ src/java/org/apache/lucene/search/Collector.java	(working copy)
@@ -116,8 +116,7 @@
  * you need that behavior, use {@link
  * PositiveScoresOnlyCollector}.</p>
  *
- * <p><b>NOTE:</b> This API is experimental and might change
- * in incompatible ways in the next release.</p>
+ * @lucene.experimental
  * 
  * @since 2.9
  */
Index: src/java/org/apache/lucene/search/function/FieldScoreQuery.java
===================================================================
--- src/java/org/apache/lucene/search/function/FieldScoreQuery.java	(revision 916567)
+++ src/java/org/apache/lucene/search/function/FieldScoreQuery.java	(working copy)
@@ -52,10 +52,7 @@
  * otherwise, for instance if for each query a new index reader is opened, large penalties would be 
  * paid for loading the field values into memory over and over again!
  * 
- * <p><font color="#FF0000">
- * WARNING: The status of the <b>search.function</b> package is experimental. 
- * The APIs introduced here might change in the future and will not be 
- * supported anymore in such a case.</font>
+ * @lucene.experimental
  */
 public class FieldScoreQuery extends ValueSourceQuery {
 
Index: src/java/org/apache/lucene/search/function/ReverseOrdFieldSource.java
===================================================================
--- src/java/org/apache/lucene/search/function/ReverseOrdFieldSource.java	(revision 916567)
+++ src/java/org/apache/lucene/search/function/ReverseOrdFieldSource.java	(working copy)
@@ -40,10 +40,7 @@
  * when other documents are inserted or deleted,
  * or if a MultiSearcher is used. 
  * 
- * <p><font color="#FF0000">
- * WARNING: The status of the <b>search.function</b> package is experimental. 
- * The APIs introduced here might change in the future and will not be 
- * supported anymore in such a case.</font>
+ * @lucene.experimental
  *
  * <p><b>NOTE</b>: with the switch in 2.9 to segment-based
  * searching, if {@link #getValues} is invoked with a
Index: src/java/org/apache/lucene/search/function/OrdFieldSource.java
===================================================================
--- src/java/org/apache/lucene/search/function/OrdFieldSource.java	(revision 916567)
+++ src/java/org/apache/lucene/search/function/OrdFieldSource.java	(working copy)
@@ -39,10 +39,7 @@
  * when other documents are inserted or deleted,
  * or if a MultiSearcher is used. 
  *
- * <p><font color="#FF0000">
- * WARNING: The status of the <b>search.function</b> package is experimental. 
- * The APIs introduced here might change in the future and will not be 
- * supported anymore in such a case.</font>
+ * @lucene.experimental
  *
  * <p><b>NOTE</b>: with the switch in 2.9 to segment-based
  * searching, if {@link #getValues} is invoked with a
Index: src/java/org/apache/lucene/search/function/package.html
===================================================================
--- src/java/org/apache/lucene/search/function/package.html	(revision 916567)
+++ src/java/org/apache/lucene/search/function/package.html	(working copy)
@@ -26,11 +26,7 @@
   The <code>function</code> package provides tight control over documents scores.
 </DIV>
 <DIV>
-<font color="#FF0000">
-WARNING: The status of the <b>search.function</b> package is experimental. The APIs
-introduced here might change in the future and will not be supported anymore
-in such a case.
-</font>
+@lucene.experimental
 </DIV>
 <DIV>
   Two types of queries are available in this package:
Index: src/java/org/apache/lucene/search/function/IntFieldSource.java
===================================================================
--- src/java/org/apache/lucene/search/function/IntFieldSource.java	(revision 916567)
+++ src/java/org/apache/lucene/search/function/IntFieldSource.java	(working copy)
@@ -29,10 +29,7 @@
  * using <code>getInts()</code> and makes those values 
  * available as other numeric types, casting as needed.
  * 
- * <p><font color="#FF0000">
- * WARNING: The status of the <b>search.function</b> package is experimental. 
- * The APIs introduced here might change in the future and will not be 
- * supported anymore in such a case.</font>
+ * @lucene.experimental
  * 
  * @see org.apache.lucene.search.function.FieldCacheSource for requirements 
  * on the field.
Index: src/java/org/apache/lucene/search/function/ValueSourceQuery.java
===================================================================
--- src/java/org/apache/lucene/search/function/ValueSourceQuery.java	(revision 916567)
+++ src/java/org/apache/lucene/search/function/ValueSourceQuery.java	(working copy)
@@ -37,10 +37,7 @@
  * <p>
  * Score is set as: Score(doc,query) = query.getBoost()<sup>2</sup> * valueSource(doc).  
  *
- * <p><font color="#FF0000">
- * WARNING: The status of the <b>search.function</b> package is experimental. 
- * The APIs introduced here might change in the future and will not be 
- * supported anymore in such a case.</font>
+ * @lucene.experimental
  */
 public class ValueSourceQuery extends Query {
   ValueSource valSrc;
Index: src/java/org/apache/lucene/search/function/ShortFieldSource.java
===================================================================
--- src/java/org/apache/lucene/search/function/ShortFieldSource.java	(revision 916567)
+++ src/java/org/apache/lucene/search/function/ShortFieldSource.java	(working copy)
@@ -29,10 +29,7 @@
  * using <code>getShorts()</code> and makes those values 
  * available as other numeric types, casting as needed.
  * 
- * <p><font color="#FF0000">
- * WARNING: The status of the <b>search.function</b> package is experimental. 
- * The APIs introduced here might change in the future and will not be 
- * supported anymore in such a case.</font>
+ * @lucene.experimental
  * 
  * @see org.apache.lucene.search.function.FieldCacheSource for requirements 
  * on the field.
Index: src/java/org/apache/lucene/search/function/DocValues.java
===================================================================
--- src/java/org/apache/lucene/search/function/DocValues.java	(revision 916567)
+++ src/java/org/apache/lucene/search/function/DocValues.java	(working copy)
@@ -25,10 +25,7 @@
  * {@link org.apache.lucene.search.function.ValueSource ValueSuorce} 
  * for a particular field and reader.
  *
- * <p><font color="#FF0000">
- * WARNING: The status of the <b>search.function</b> package is experimental. 
- * The APIs introduced here might change in the future and will not be 
- * supported anymore in such a case.</font>
+ * @lucene.experimental
  * 
  *
  */
Index: src/java/org/apache/lucene/search/function/ByteFieldSource.java
===================================================================
--- src/java/org/apache/lucene/search/function/ByteFieldSource.java	(revision 916567)
+++ src/java/org/apache/lucene/search/function/ByteFieldSource.java	(working copy)
@@ -29,10 +29,7 @@
  * using <code>getBytes()</code> and makes those values 
  * available as other numeric types, casting as needed.
  * 
- * <p><font color="#FF0000">
- * WARNING: The status of the <b>search.function</b> package is experimental. 
- * The APIs introduced here might change in the future and will not be 
- * supported anymore in such a case.</font>
+ * @lucene.experimental
  * 
  * @see org.apache.lucene.search.function.FieldCacheSource for requirements 
  * on the field. 
Index: src/java/org/apache/lucene/search/function/FieldCacheSource.java
===================================================================
--- src/java/org/apache/lucene/search/function/FieldCacheSource.java	(revision 916567)
+++ src/java/org/apache/lucene/search/function/FieldCacheSource.java	(working copy)
@@ -34,10 +34,7 @@
  * <p>
  * Document with no tokens in this field are assigned the <code>Zero</code> value.    
  * 
- * <p><font color="#FF0000">
- * WARNING: The status of the <b>search.function</b> package is experimental. 
- * The APIs introduced here might change in the future and will not be 
- * supported anymore in such a case.</font>
+ * @lucene.experimental
  *
  * <p><b>NOTE</b>: with the switch in 2.9 to segment-based
  * searching, if {@link #getValues} is invoked with a
Index: src/java/org/apache/lucene/search/function/FloatFieldSource.java
===================================================================
--- src/java/org/apache/lucene/search/function/FloatFieldSource.java	(revision 916567)
+++ src/java/org/apache/lucene/search/function/FloatFieldSource.java	(working copy)
@@ -29,10 +29,7 @@
  * using <code>getFloats()</code> and makes those values 
  * available as other numeric types, casting as needed.
  * 
- * <p><font color="#FF0000">
- * WARNING: The status of the <b>search.function</b> package is experimental. 
- * The APIs introduced here might change in the future and will not be 
- * supported anymore in such a case.</font>
+ * @lucene.experimental
  * 
  * @see org.apache.lucene.search.function.FieldCacheSource for requirements 
  * on the field.
Index: src/java/org/apache/lucene/search/function/CustomScoreQuery.java
===================================================================
--- src/java/org/apache/lucene/search/function/CustomScoreQuery.java	(revision 916567)
+++ src/java/org/apache/lucene/search/function/CustomScoreQuery.java	(working copy)
@@ -40,12 +40,9 @@
  *        For most simple/convenient use cases this query is likely to be a 
  *        {@link org.apache.lucene.search.function.FieldScoreQuery FieldScoreQuery}</li>
  * </ol>
- * Subclasses can modify the computation by overriding {@link #customScore(int, float, float)}.
+ * Subclasses can modify the computation by overriding {@link #getCustomScoreProvider}.
  * 
- * <p><font color="#FF0000">
- * WARNING: The status of the <b>search.function</b> package is experimental. 
- * The APIs introduced here might change in the future and will not be 
- * supported anymore in such a case.</font>
+ * @lucene.experimental
  */
 public class CustomScoreQuery extends Query {
 
@@ -83,7 +80,6 @@
    * This parameter is optional - it can be null or even an empty array.
    */
   public CustomScoreQuery(Query subQuery, ValueSourceQuery... valSrcQueries) {
-    super();
     this.subQuery = subQuery;
     this.valSrcQueries = valSrcQueries!=null?
         valSrcQueries : new ValueSourceQuery[0];
@@ -93,11 +89,23 @@
   /*(non-Javadoc) @see org.apache.lucene.search.Query#rewrite(org.apache.lucene.index.IndexReader) */
   @Override
   public Query rewrite(IndexReader reader) throws IOException {
-    subQuery = subQuery.rewrite(reader);
+    CustomScoreQuery clone = null;
+    
+    final Query sq = subQuery.rewrite(reader);
+    if (sq != subQuery) {
+      clone = (CustomScoreQuery) clone();
+      clone.subQuery = sq;
+    }
+
     for(int i = 0; i < valSrcQueries.length; i++) {
-      valSrcQueries[i] = (ValueSourceQuery) valSrcQueries[i].rewrite(reader);
+      final ValueSourceQuery v = (ValueSourceQuery) valSrcQueries[i].rewrite(reader);
+      if (v != valSrcQueries[i]) {
+        if (clone == null) clone = (CustomScoreQuery) clone();
+        clone.valSrcQueries[i] = v;
+      }
     }
-    return this;
+    
+    return (clone == null) ? this : clone;
   }
 
   /*(non-Javadoc) @see org.apache.lucene.search.Query#extractTerms(java.util.Set) */
@@ -142,7 +150,8 @@
     }
     CustomScoreQuery other = (CustomScoreQuery)o;
     if (this.getBoost() != other.getBoost() ||
-        !this.subQuery.equals(other.subQuery)||
+        !this.subQuery.equals(other.subQuery) ||
+        this.strict != other.strict ||
         this.valSrcQueries.length != other.valSrcQueries.length) {
       return false;
     }
@@ -153,134 +162,19 @@
   @Override
   public int hashCode() {
     return (getClass().hashCode() + subQuery.hashCode() + Arrays.hashCode(valSrcQueries))
-      ^ Float.floatToIntBits(getBoost());
-  }  
+      ^ Float.floatToIntBits(getBoost()) ^ (strict ? 1234 : 4321);
+  }
   
   /**
-   * Compute a custom score by the subQuery score and a number of 
-   * ValueSourceQuery scores.
-   * <p> 
-   * Subclasses can override this method to modify the custom score.  
-   * <p>
-   * If your custom scoring is different than the default herein you 
-   * should override at least one of the two customScore() methods.
-   * If the number of ValueSourceQueries is always &lt; 2 it is 
-   * sufficient to override the other 
-   * {@link #customScore(int, float, float) customScore()} 
-   * method, which is simpler. 
-   * <p>
-   * The default computation herein is a multiplication of given scores:
-   * <pre>
-   *     ModifiedScore = valSrcScore * valSrcScores[0] * valSrcScores[1] * ...
-   * </pre>
-   * 
-   * @param doc id of scored doc. 
-   * @param subQueryScore score of that doc by the subQuery.
-   * @param valSrcScores scores of that doc by the ValueSourceQuery.
-   * @return custom score.
+   * Returns a {@link CustomScoreProvider} that calculates the custom scores
+   * for the given {@link IndexReader}. The default implementation returns a default
+   * implementation as specified in the docs of {@link CustomScoreProvider}.
+   * @since 2.9.2
    */
-  public float customScore(int doc, float subQueryScore, float valSrcScores[]) {
-    if (valSrcScores.length == 1) {
-      return customScore(doc, subQueryScore, valSrcScores[0]);
-    }
-    if (valSrcScores.length == 0) {
-      return customScore(doc, subQueryScore, 1);
-    }
-    float score = subQueryScore;
-    for(int i = 0; i < valSrcScores.length; i++) {
-      score *= valSrcScores[i];
-    }
-    return score;
+  protected CustomScoreProvider getCustomScoreProvider(IndexReader reader) throws IOException {
+    return new CustomScoreProvider(reader);
   }
 
-  /**
-   * Compute a custom score by the subQuery score and the ValueSourceQuery score.
-   * <p> 
-   * Subclasses can override this method to modify the custom score.
-   * <p>
-   * If your custom scoring is different than the default herein you 
-   * should override at least one of the two customScore() methods.
-   * If the number of ValueSourceQueries is always &lt; 2 it is 
-   * sufficient to override this customScore() method, which is simpler. 
-   * <p>
-   * The default computation herein is a multiplication of the two scores:
-   * <pre>
-   *     ModifiedScore = subQueryScore * valSrcScore
-   * </pre>
-   *
-   * <p><b>NOTE</b>: The doc is relative to the current
-   * reader, last passed to {@link #setNextReader}.
-   * 
-   * @param doc id of scored doc. 
-   * @param subQueryScore score of that doc by the subQuery.
-   * @param valSrcScore score of that doc by the ValueSourceQuery.
-   * @return custom score.
-   */
-  public float customScore(int doc, float subQueryScore, float valSrcScore) {
-    return subQueryScore * valSrcScore;
-  }
-
-  /**
-   * Called when the scoring switches to another reader.
-   * 
-   * @param reader
-   *          next IndexReader
-   */
-  public void setNextReader(IndexReader reader) throws IOException {
-  }
-
-  /**
-   * Explain the custom score.
-   * Whenever overriding {@link #customScore(int, float, float[])}, 
-   * this method should also be overridden to provide the correct explanation
-   * for the part of the custom scoring.
-   *  
-   * @param doc doc being explained.
-   * @param subQueryExpl explanation for the sub-query part.
-   * @param valSrcExpls explanation for the value source part.
-   * @return an explanation for the custom score
-   */
-  public Explanation customExplain(int doc, Explanation subQueryExpl, Explanation valSrcExpls[]) {
-    if (valSrcExpls.length == 1) {
-      return customExplain(doc, subQueryExpl, valSrcExpls[0]);
-    }
-    if (valSrcExpls.length == 0) {
-      return subQueryExpl;
-    }
-    float valSrcScore = 1;
-    for (int i = 0; i < valSrcExpls.length; i++) {
-      valSrcScore *= valSrcExpls[i].getValue();
-    }
-    Explanation exp = new Explanation( valSrcScore * subQueryExpl.getValue(), "custom score: product of:");
-    exp.addDetail(subQueryExpl);
-    for (int i = 0; i < valSrcExpls.length; i++) {
-      exp.addDetail(valSrcExpls[i]);
-    }
-    return exp;
-  }
-
-  /**
-   * Explain the custom score.
-   * Whenever overriding {@link #customScore(int, float, float)}, 
-   * this method should also be overridden to provide the correct explanation
-   * for the part of the custom scoring.
-   *  
-   * @param doc doc being explained.
-   * @param subQueryExpl explanation for the sub-query part.
-   * @param valSrcExpl explanation for the value source part.
-   * @return an explanation for the custom score
-   */
-  public Explanation customExplain(int doc, Explanation subQueryExpl, Explanation valSrcExpl) {
-    float valSrcScore = 1;
-    if (valSrcExpl != null) {
-      valSrcScore *= valSrcExpl.getValue();
-    }
-    Explanation exp = new Explanation( valSrcScore * subQueryExpl.getValue(), "custom score: product of:");
-    exp.addDetail(subQueryExpl);
-    exp.addDetail(valSrcExpl);
-    return exp;
-  }
-
   //=========================== W E I G H T ============================
   
   private class CustomWeight extends Weight {
@@ -374,7 +268,7 @@
       for(int i = 0; i < valSrcWeights.length; i++) {
         valSrcExpls[i] = valSrcWeights[i].explain(reader, doc);
       }
-      Explanation customExp = customExplain(doc,subQueryExpl,valSrcExpls);
+      Explanation customExp = CustomScoreQuery.this.getCustomScoreProvider(reader).customExplain(doc,subQueryExpl,valSrcExpls);
       float sc = getValue() * customExp.getValue();
       Explanation res = new ComplexExplanation(
         true, sc, CustomScoreQuery.this.toString() + ", product of:");
@@ -401,6 +295,7 @@
     private Scorer subQueryScorer;
     private Scorer[] valSrcScorers;
     private IndexReader reader;
+    private final CustomScoreProvider provider;
     private float vScores[]; // reused in score() to avoid allocating this array for each doc 
 
     // constructor
@@ -412,7 +307,7 @@
       this.valSrcScorers = valSrcScorers;
       this.reader = reader;
       this.vScores = new float[valSrcScorers.length];
-      setNextReader(reader);
+      this.provider = CustomScoreQuery.this.getCustomScoreProvider(reader);
     }
 
     @Override
@@ -437,7 +332,7 @@
       for (int i = 0; i < valSrcScorers.length; i++) {
         vScores[i] = valSrcScorers[i].score();
       }
-      return qWeight * customScore(subQueryScorer.docID(), subQueryScorer.score(), vScores);
+      return qWeight * provider.customScore(subQueryScorer.docID(), subQueryScorer.score(), vScores);
     }
 
     @Override
Index: src/java/org/apache/lucene/search/function/ValueSource.java
===================================================================
--- src/java/org/apache/lucene/search/function/ValueSource.java	(revision 916567)
+++ src/java/org/apache/lucene/search/function/ValueSource.java	(working copy)
@@ -31,10 +31,7 @@
  * <P>ValueSource implementations differ in RAM requirements: it would always be a factor
  * of the number of documents, but for each document the number of bytes can be 1, 2, 4, or 8. 
  *
- * <p><font color="#FF0000">
- * WARNING: The status of the <b>search.function</b> package is experimental. 
- * The APIs introduced here might change in the future and will not be 
- * supported anymore in such a case.</font>
+ * @lucene.experimental
  *
  *
  */
Index: src/java/org/apache/lucene/search/NumericRangeQuery.java
===================================================================
--- src/java/org/apache/lucene/search/NumericRangeQuery.java	(revision 916567)
+++ src/java/org/apache/lucene/search/NumericRangeQuery.java	(working copy)
@@ -74,8 +74,7 @@
  * BooleanQuery rewrite methods without changing
  * BooleanQuery's default max clause count.
  *
- * <p><font color="red"><b>NOTE:</b> This API is experimental and
- * might change in incompatible ways in the next release.</font>
+ * @lucene.experimental
  *
  * <br><h3>How it works</h3>
  *
Index: src/java/org/apache/lucene/search/Query.java
===================================================================
--- src/java/org/apache/lucene/search/Query.java	(revision 916567)
+++ src/java/org/apache/lucene/search/Query.java	(working copy)
@@ -101,6 +101,8 @@
     Weight weight = query.createWeight(searcher);
     float sum = weight.sumOfSquaredWeights();
     float norm = getSimilarity(searcher).queryNorm(sum);
+    if (Float.isInfinite(norm) || Float.isNaN(norm))
+      norm = 1.0f;
     weight.normalize(norm);
     return weight;
   }
Index: src/java/org/apache/lucene/search/FieldComparator.java
===================================================================
--- src/java/org/apache/lucene/search/FieldComparator.java	(revision 916567)
+++ src/java/org/apache/lucene/search/FieldComparator.java	(working copy)
@@ -79,8 +79,7 @@
  *       FieldDoc#fields} when returning the top results.
  * </ul>
  *
- * <b>NOTE:</b> This API is experimental and might change in
- * incompatible ways in the next release.
+ * @lucene.experimental
  */
 public abstract class FieldComparator {
 
Index: src/java/org/apache/lucene/search/SortField.java
===================================================================
--- src/java/org/apache/lucene/search/SortField.java	(revision 916567)
+++ src/java/org/apache/lucene/search/SortField.java	(working copy)
@@ -365,8 +365,7 @@
   /** Returns the {@link FieldComparator} to use for
    * sorting.
    *
-   * <b>NOTE:</b> This API is experimental and might change in
-   * incompatible ways in the next release.
+   * @lucene.experimental
    *
    * @param numHits number of top hits the queue will store
    * @param sortPos position of this SortField within {@link
Index: src/java/org/apache/lucene/search/QueryWrapperFilter.java
===================================================================
--- src/java/org/apache/lucene/search/QueryWrapperFilter.java	(revision 916567)
+++ src/java/org/apache/lucene/search/QueryWrapperFilter.java	(working copy)
@@ -31,8 +31,6 @@
  * QueryFilter that matches, e.g., only documents modified within the last
  * week.  The QueryFilter and TermRangeQuery would only need to be reconstructed
  * once per day.
- *
- * @version $Id:$
  */
 public class QueryWrapperFilter extends Filter {
   private Query query;
Index: src/java/org/apache/lucene/search/TopFieldCollector.java
===================================================================
--- src/java/org/apache/lucene/search/TopFieldCollector.java	(revision 916567)
+++ src/java/org/apache/lucene/search/TopFieldCollector.java	(working copy)
@@ -30,8 +30,7 @@
  * See the {@link #create(org.apache.lucene.search.Sort, int, boolean, boolean, boolean, boolean)} method
  * for instantiating a TopFieldCollector.
  * 
- * <p><b>NOTE:</b> This API is experimental and might change in
- * incompatible ways in the next release.</p>
+ * @lucene.experimental
  */
 public abstract class TopFieldCollector extends TopDocsCollector<Entry> {
   
Index: src/java/org/apache/lucene/search/ParallelMultiSearcher.java
===================================================================
--- src/java/org/apache/lucene/search/ParallelMultiSearcher.java	(revision 916567)
+++ src/java/org/apache/lucene/search/ParallelMultiSearcher.java	(working copy)
@@ -175,8 +175,14 @@
      searchables[i].search(weight, filter, hc);
    }
   }
-  
+
   @Override
+  public void close() throws IOException {
+    executor.shutdown();
+    super.close();
+  }
+
+  @Override
   HashMap<Term, Integer> createDocFrequencyMap(Set<Term> terms) throws IOException {
     final Term[] allTermsArray = terms.toArray(new Term[terms.size()]);
     final int[] aggregatedDocFreqs = new int[terms.size()];
Index: src/java/org/apache/lucene/search/IndexSearcher.java
===================================================================
--- src/java/org/apache/lucene/search/IndexSearcher.java	(revision 916567)
+++ src/java/org/apache/lucene/search/IndexSearcher.java	(working copy)
@@ -86,9 +86,7 @@
   /** Expert: directly specify the reader, subReaders and
    *  their docID starts.
    * 
-   * <p><b>NOTE:</b> This API is experimental and
-   * might change in incompatible ways in the next
-   * release.</font></p> */
+   * @lucene.experimental */
   public IndexSearcher(IndexReader reader, IndexReader[] subReaders, int[] docStarts) {
     this.reader = reader;
     this.subReaders = subReaders;
Index: src/java/org/apache/lucene/search/FieldCacheImpl.java
===================================================================
--- src/java/org/apache/lucene/search/FieldCacheImpl.java	(revision 916567)
+++ src/java/org/apache/lucene/search/FieldCacheImpl.java	(working copy)
@@ -61,27 +61,26 @@
     caches.put(StringIndex.class, new StringIndexCache(this));
   }
 
-  public void purgeAllCaches() {
+  public synchronized void purgeAllCaches() {
     init();
   }
 
-  public void purge(IndexReader r) {
+  public synchronized void purge(IndexReader r) {
     for(Cache c : caches.values()) {
       c.purge(r);
     }
   }
   
-  public CacheEntry[] getCacheEntries() {
+  public synchronized CacheEntry[] getCacheEntries() {
     List<CacheEntry> result = new ArrayList<CacheEntry>(17);
-    for(final Class<?> cacheType: caches.keySet()) {
-      Cache cache = caches.get(cacheType);
-      for (final Object readerKey : cache.readerCache.keySet()) {
-        // we've now materialized a hard ref
-        
-        // innerKeys was backed by WeakHashMap, sanity check
-        // that it wasn't GCed before we made hard ref
-        if (null != readerKey && cache.readerCache.containsKey(readerKey)) {
-          Map<Entry, Object> innerCache = cache.readerCache.get(readerKey);
+    for(final Map.Entry<Class<?>,Cache> cacheEntry: caches.entrySet()) {
+      final Cache cache = cacheEntry.getValue();
+      final Class<?> cacheType = cacheEntry.getKey();
+      synchronized(cache.readerCache) {
+        for (final Map.Entry<Object,Map<Entry, Object>> readerCacheEntry : cache.readerCache.entrySet()) {
+          final Object readerKey = readerCacheEntry.getKey();
+          if (readerKey == null) continue;
+          final Map<Entry, Object> innerCache = readerCacheEntry.getValue();
           for (final Map.Entry<Entry, Object> mapEntry : innerCache.entrySet()) {
             Entry entry = mapEntry.getKey();
             result.add(new CacheEntryImpl(readerKey, entry.field,
Index: src/java/org/apache/lucene/search/DefaultSimilarity.java
===================================================================
--- src/java/org/apache/lucene/search/DefaultSimilarity.java	(revision 916567)
+++ src/java/org/apache/lucene/search/DefaultSimilarity.java	(working copy)
@@ -29,8 +29,7 @@
    *  FieldInvertState#getLength()} - {@link
    *  FieldInvertState#getNumOverlap()}.
    *
-   *  <p><b>WARNING</b>: This API is new and experimental, and may suddenly
-   *  change.</p> */
+   *  @lucene.experimental */
   @Override
   public float computeNorm(String field, FieldInvertState state) {
     final int numTerms;
@@ -85,8 +84,7 @@
    *  norm.  By default this is false, meaning overlap
    *  tokens are counted just like non-overlap tokens.
    *
-   *  <p><b>WARNING</b>: This API is new and experimental, and may suddenly
-   *  change.</p>
+   *  @lucene.experimental
    *
    *  @see #computeNorm
    */
Index: src/java/org/apache/lucene/search/FieldComparatorSource.java
===================================================================
--- src/java/org/apache/lucene/search/FieldComparatorSource.java	(revision 916567)
+++ src/java/org/apache/lucene/search/FieldComparatorSource.java	(working copy)
@@ -23,8 +23,7 @@
 /**
  * Provides a {@link FieldComparator} for custom field sorting.
  *
- * <b>NOTE:</b> This API is experimental and might change in
- * incompatible ways in the next release.
+ * @lucene.experimental
  *
  */
 public abstract class FieldComparatorSource implements Serializable {
Index: src/java/org/apache/lucene/index/NormsWriterPerField.java
===================================================================
--- src/java/org/apache/lucene/index/NormsWriterPerField.java	(revision 916567)
+++ src/java/org/apache/lucene/index/NormsWriterPerField.java	(working copy)
@@ -63,11 +63,13 @@
   
   @Override
   void finish() {
-    assert docIDs.length == norms.length;
     if (fieldInfo.isIndexed && !fieldInfo.omitNorms) {
       if (docIDs.length <= upto) {
         assert docIDs.length == upto;
         docIDs = ArrayUtil.grow(docIDs, 1+upto);
+      }
+      if (norms.length <= upto) {
+        assert norms.length == upto;
         norms = ArrayUtil.grow(norms, 1+upto);
       }
       final float norm = docState.similarity.computeNorm(fieldInfo.name, fieldState);
Index: src/java/org/apache/lucene/index/ByteBlockPool.java
===================================================================
--- src/java/org/apache/lucene/index/ByteBlockPool.java	(revision 916567)
+++ src/java/org/apache/lucene/index/ByteBlockPool.java	(working copy)
@@ -35,6 +35,8 @@
 
 import java.util.Arrays;
 import org.apache.lucene.util.BytesRef;
+import static org.apache.lucene.util.RamUsageEstimator.NUM_BYTES_OBJECT_REF;
+import org.apache.lucene.util.ArrayUtil;
 
 final class ByteBlockPool {
 
@@ -84,7 +86,8 @@
 
   public void nextBuffer() {
     if (1+bufferUpto == buffers.length) {
-      byte[][] newBuffers = new byte[(int) (buffers.length*1.5)][];
+      byte[][] newBuffers = new byte[ArrayUtil.oversize(buffers.length+1,
+                                                        NUM_BYTES_OBJECT_REF)][];
       System.arraycopy(buffers, 0, newBuffers, 0, buffers.length);
       buffers = newBuffers;
     }
Index: src/java/org/apache/lucene/index/SegmentInfo.java
===================================================================
--- src/java/org/apache/lucene/index/SegmentInfo.java	(revision 916567)
+++ src/java/org/apache/lucene/index/SegmentInfo.java	(working copy)
@@ -34,8 +34,7 @@
  * Information about a segment such as it's name, directory, and files related
  * to the segment.
  * 
- * * <p><b>NOTE:</b> This API is new and still experimental
- * (subject to change suddenly in the next release)</p>
+ * @lucene.experimental
  */
 public final class SegmentInfo {
 
@@ -558,6 +557,7 @@
    */
   void write(IndexOutput output)
     throws IOException {
+    assert delCount <= docCount: "delCount=" + delCount + " docCount=" + docCount + " segment=" + name;
     output.writeString(name);
     output.writeInt(docCount);
     output.writeLong(delGen);
Index: src/java/org/apache/lucene/index/SegmentReader.java
===================================================================
--- src/java/org/apache/lucene/index/SegmentReader.java	(revision 916567)
+++ src/java/org/apache/lucene/index/SegmentReader.java	(working copy)
@@ -49,10 +49,8 @@
 import org.apache.lucene.search.FieldCache; // not great (circular); used only to purge FieldCache entry on close
 import org.apache.lucene.util.BytesRef;
 
-/** @version $Id */
 /**
- * <p><b>NOTE:</b> This API is new and still experimental
- * (subject to change suddenly in the next release)</p>
+ * @lucene.experimental
  */
 public class SegmentReader extends IndexReader implements Cloneable {
   protected boolean readOnly;
@@ -205,7 +203,11 @@
           storeCFSReader.close();
         }
 
-        // Force FieldCache to evict our entries at this point
+        // Force FieldCache to evict our entries at this
+        // point.  If the exception occurred while
+        // initializing the core readers, then
+        // origInstance will be null, and we don't want
+        // to call FieldCache.purge (it leads to NPE):
         if (origInstance != null) {
           FieldCache.DEFAULT.purge(origInstance);
         }
@@ -583,7 +585,7 @@
       deletedDocsRef = new AtomicInteger(1);
       assert checkDeletedCounts();
       if (deletedDocs.size() != si.docCount) {
-        throw new CorruptIndexException("document count mismatch: deleted docs count " + deletedDocs.size() + " vs segment doc count " + si.docCount);
+        throw new CorruptIndexException("document count mismatch: deleted docs count " + deletedDocs.size() + " vs segment doc count " + si.docCount + " segment=" + si.name);
       }
     } else
       assert si.getDelCount() == 0;
Index: src/java/org/apache/lucene/index/SnapshotDeletionPolicy.java
===================================================================
--- src/java/org/apache/lucene/index/SnapshotDeletionPolicy.java	(revision 916567)
+++ src/java/org/apache/lucene/index/SnapshotDeletionPolicy.java	(working copy)
@@ -39,8 +39,7 @@
  *  snapshot held when a writer is closed will "survive"
  *  when the next writer is opened.
  *
- * <p><b>WARNING</b>: This API is a new and experimental and
- * may suddenly change.</p> */
+ * @lucene.experimental */
 
 public class SnapshotDeletionPolicy implements IndexDeletionPolicy {
 
Index: src/java/org/apache/lucene/index/DocFieldProcessorPerThread.java
===================================================================
--- src/java/org/apache/lucene/index/DocFieldProcessorPerThread.java	(revision 916567)
+++ src/java/org/apache/lucene/index/DocFieldProcessorPerThread.java	(working copy)
@@ -24,6 +24,7 @@
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Fieldable;
 import org.apache.lucene.util.ArrayUtil;
+import org.apache.lucene.util.RamUsageEstimator;
 
 /**
  * Gathers all Fieldables for a document under the same
@@ -340,7 +341,7 @@
         // enough space to recycle all outstanding PerDoc
         // instances
         assert allocCount == 1+docFreeList.length;
-        docFreeList = new PerDoc[ArrayUtil.getNextSize(allocCount)];
+        docFreeList = new PerDoc[ArrayUtil.oversize(allocCount, RamUsageEstimator.NUM_BYTES_OBJECT_REF)];
       }
       return new PerDoc();
     } else
Index: src/java/org/apache/lucene/index/DocFieldConsumers.java
===================================================================
--- src/java/org/apache/lucene/index/DocFieldConsumers.java	(revision 916567)
+++ src/java/org/apache/lucene/index/DocFieldConsumers.java	(working copy)
@@ -25,6 +25,7 @@
 import java.io.IOException;
 
 import org.apache.lucene.util.ArrayUtil;
+import org.apache.lucene.util.RamUsageEstimator;
 
 /** This is just a "splitter" class: it lets you wrap two
  *  DocFieldConsumer instances as a single consumer. */
@@ -117,7 +118,7 @@
         // enough space to recycle all outstanding PerDoc
         // instances
         assert allocCount == 1+docFreeList.length;
-        docFreeList = new PerDoc[ArrayUtil.getNextSize(allocCount)];
+        docFreeList = new PerDoc[ArrayUtil.oversize(allocCount, RamUsageEstimator.NUM_BYTES_OBJECT_REF)];
       }
       return new PerDoc();
     } else
Index: src/java/org/apache/lucene/index/MergePolicy.java
===================================================================
--- src/java/org/apache/lucene/index/MergePolicy.java	(revision 916567)
+++ src/java/org/apache/lucene/index/MergePolicy.java	(working copy)
@@ -51,8 +51,7 @@
  * <p>The default MergePolicy is {@link
  * LogByteSizeMergePolicy}.</p>
  *
- * <p><b>NOTE:</b> This API is new and still experimental
- * (subject to change suddenly in the next release)</p>
+ * @lucene.experimental
  *
  * <p><b>NOTE</b>: This class typically requires access to
  * package-private APIs (e.g. <code>SegmentInfos</code>) to do its job;
Index: src/java/org/apache/lucene/index/StoredFieldsWriter.java
===================================================================
--- src/java/org/apache/lucene/index/StoredFieldsWriter.java	(revision 916567)
+++ src/java/org/apache/lucene/index/StoredFieldsWriter.java	(working copy)
@@ -20,6 +20,7 @@
 import java.io.IOException;
 import org.apache.lucene.store.RAMOutputStream;
 import org.apache.lucene.util.ArrayUtil;
+import org.apache.lucene.util.RamUsageEstimator;
 
 /** This is a DocFieldConsumer that writes stored fields. */
 final class StoredFieldsWriter {
@@ -108,7 +109,7 @@
         // enough space to recycle all outstanding PerDoc
         // instances
         assert allocCount == 1+docFreeList.length;
-        docFreeList = new PerDoc[ArrayUtil.getNextSize(allocCount)];
+        docFreeList = new PerDoc[ArrayUtil.oversize(allocCount, RamUsageEstimator.NUM_BYTES_OBJECT_REF)];
       }
       return new PerDoc();
     } else
Index: src/java/org/apache/lucene/index/SegmentInfos.java
===================================================================
--- src/java/org/apache/lucene/index/SegmentInfos.java	(revision 916567)
+++ src/java/org/apache/lucene/index/SegmentInfos.java	(working copy)
@@ -41,8 +41,7 @@
  * A collection of segmentInfo objects with methods for operating on
  * those segments in relation to the file system.
  * 
- * <p><b>NOTE:</b> This API is new and still experimental
- * (subject to change suddenly in the next release)</p>
+ * @lucene.experimental
  */
 public final class SegmentInfos extends Vector<SegmentInfo> {
 
Index: src/java/org/apache/lucene/index/CheckIndex.java
===================================================================
--- src/java/org/apache/lucene/index/CheckIndex.java	(revision 916567)
+++ src/java/org/apache/lucene/index/CheckIndex.java	(working copy)
@@ -44,9 +44,7 @@
  * <p>As this tool checks every byte in the index, on a large
  * index it can take quite a long time to run.
  *
- * <p><b>WARNING</b>: this tool and API is new and
- * experimental and is subject to suddenly change in the
- * next release.  Please make a complete backup of your
+ * @lucene.experimental Please make a complete backup of your
  * index before using this to fix your index!
  */
 public class CheckIndex {
@@ -57,8 +55,7 @@
   /**
    * Returned from {@link #checkIndex()} detailing the health and status of the index.
    *
-   * <p><b>WARNING</b>: this API is new and experimental and is
-   * subject to suddenly change in the next release.
+   * @lucene.experimental
    **/
 
   public static class Status {
Index: src/java/org/apache/lucene/index/MergeScheduler.java
===================================================================
--- src/java/org/apache/lucene/index/MergeScheduler.java	(revision 916567)
+++ src/java/org/apache/lucene/index/MergeScheduler.java	(working copy)
@@ -24,8 +24,7 @@
  *  selected by a {@link MergePolicy}.  The default
  *  MergeScheduler is {@link ConcurrentMergeScheduler}.</p>
  *
- * <p><b>NOTE:</b> This API is new and still experimental
- * (subject to change suddenly in the next release)</p>
+ * @lucene.experimental
  *
  * <p><b>NOTE</b>: This class typically requires access to
  * package-private APIs (eg, SegmentInfos) to do its job;
Index: src/java/org/apache/lucene/index/IndexCommit.java
===================================================================
--- src/java/org/apache/lucene/index/IndexCommit.java	(revision 916567)
+++ src/java/org/apache/lucene/index/IndexCommit.java	(working copy)
@@ -37,8 +37,7 @@
  * associated with it. The segments file associated with a
  * later index commit point would have a larger N.</p>
  *
- * <p><b>WARNING</b>: This API is a new and experimental and
- * may suddenly change. </p>
+ * @lucene.experimental
 */
 
 public abstract class IndexCommit {
Index: src/java/org/apache/lucene/index/DirectoryReader.java
===================================================================
--- src/java/org/apache/lucene/index/DirectoryReader.java	(revision 916567)
+++ src/java/org/apache/lucene/index/DirectoryReader.java	(working copy)
@@ -985,6 +985,12 @@
     // not a good idea):
     FieldCache.DEFAULT.purge(this);
 
+    if (writer != null) {
+      // Since we just closed, writer may now be able to
+      // delete unused files:
+      writer.deleteUnusedFiles();
+    }
+
     // throw the first exception
     if (ioe != null) throw ioe;
   }
@@ -1032,7 +1038,7 @@
   /**
    * Expert: return the IndexCommit that this reader has opened.
    * <p/>
-   * <p><b>WARNING</b>: this API is new and experimental and may suddenly change.</p>
+   * @lucene.experimental
    */
   @Override
   public IndexCommit getIndexCommit() throws IOException {
Index: src/java/org/apache/lucene/index/MultipleTermPositions.java
===================================================================
--- src/java/org/apache/lucene/index/MultipleTermPositions.java	(revision 916567)
+++ src/java/org/apache/lucene/index/MultipleTermPositions.java	(working copy)
@@ -17,14 +17,14 @@
  * limitations under the License.
  */
 
-import org.apache.lucene.util.PriorityQueue;
-
 import java.io.IOException;
 import java.util.Arrays;
-
 import java.util.LinkedList;
 import java.util.List;
 
+import org.apache.lucene.util.PriorityQueue;
+import org.apache.lucene.util.ArrayUtil;
+
 /**
  * Allows you to iterate over the {@link TermPositions} for multiple {@link Term}s as
  * a single {@link TermPositions}.
@@ -85,10 +85,8 @@
     }
 
     private void growArray() {
-      int[] newArray = new int[_arraySize * 2];
-      System.arraycopy(_array, 0, newArray, 0, _arraySize);
-      _array = newArray;
-      _arraySize *= 2;
+      _array = ArrayUtil.grow(_array, _arraySize+1);
+      _arraySize = _array.length;
     }
   }
 
Index: src/java/org/apache/lucene/index/TermsHash.java
===================================================================
--- src/java/org/apache/lucene/index/TermsHash.java	(revision 916567)
+++ src/java/org/apache/lucene/index/TermsHash.java	(working copy)
@@ -26,6 +26,7 @@
 import java.io.IOException;
 
 import org.apache.lucene.util.ArrayUtil;
+import org.apache.lucene.util.RamUsageEstimator;
 
 /** This class implements {@link InvertedDocConsumer}, which
  *  is passed each token produced by the analyzer on each
@@ -92,7 +93,7 @@
 
     assert postingsFreeCount == postingsAllocCount: Thread.currentThread().getName() + ": postingsFreeCount=" + postingsFreeCount + " postingsAllocCount=" + postingsAllocCount + " consumer=" + consumer;
 
-    final int newSize = ArrayUtil.getShrinkSize(postingsFreeList.length, postingsAllocCount);
+    final int newSize = ArrayUtil.getShrinkSize(postingsFreeList.length, postingsAllocCount, RamUsageEstimator.NUM_BYTES_OBJECT_REF);
     if (newSize != postingsFreeList.length) {
       RawPostingList[] newArray = new RawPostingList[newSize];
       System.arraycopy(postingsFreeList, 0, newArray, 0, postingsFreeCount);
@@ -225,7 +226,7 @@
       if (newPostingsAllocCount > postingsFreeList.length)
         // Pre-allocate the postingsFreeList so it's large
         // enough to hold all postings we've given out
-        postingsFreeList = new RawPostingList[ArrayUtil.getNextSize(newPostingsAllocCount)];
+        postingsFreeList = new RawPostingList[ArrayUtil.oversize(newPostingsAllocCount, RamUsageEstimator.NUM_BYTES_OBJECT_REF)];
     }
 
     postingsFreeCount -= numToCopy;
Index: src/java/org/apache/lucene/index/FieldInvertState.java
===================================================================
--- src/java/org/apache/lucene/index/FieldInvertState.java	(revision 916567)
+++ src/java/org/apache/lucene/index/FieldInvertState.java	(working copy)
@@ -23,8 +23,7 @@
  * being added to the index. The information collected in this class is
  * also used to calculate the normalization factor for a field.
  * 
- * <p><b>WARNING</b>: This API is new and experimental, and may suddenly
- * change.</p>
+ * @lucene.experimental
  */
 public final class FieldInvertState {
   int position;
Index: src/java/org/apache/lucene/index/TermVectorsTermsWriter.java
===================================================================
--- src/java/org/apache/lucene/index/TermVectorsTermsWriter.java	(revision 916567)
+++ src/java/org/apache/lucene/index/TermVectorsTermsWriter.java	(working copy)
@@ -20,6 +20,7 @@
 import org.apache.lucene.store.IndexOutput;
 import org.apache.lucene.store.RAMOutputStream;
 import org.apache.lucene.util.ArrayUtil;
+import org.apache.lucene.util.RamUsageEstimator;
 
 import java.io.IOException;
 import java.util.Collection;
@@ -117,7 +118,7 @@
         // enough space to recycle all outstanding PerDoc
         // instances
         assert allocCount == 1+docFreeList.length;
-        docFreeList = new PerDoc[ArrayUtil.getNextSize(allocCount)];
+        docFreeList = new PerDoc[ArrayUtil.oversize(allocCount, RamUsageEstimator.NUM_BYTES_OBJECT_REF)];
       }
       return new PerDoc();
     } else
@@ -266,6 +267,8 @@
     void addField(final int fieldNumber) {
       if (numVectorFields == fieldNumbers.length) {
         fieldNumbers = ArrayUtil.grow(fieldNumbers);
+      }
+      if (numVectorFields == fieldPointers.length) {
         fieldPointers = ArrayUtil.grow(fieldPointers);
       }
       fieldNumbers[numVectorFields] = fieldNumber;
Index: src/java/org/apache/lucene/index/IndexReader.java
===================================================================
--- src/java/org/apache/lucene/index/IndexReader.java	(revision 916567)
+++ src/java/org/apache/lucene/index/IndexReader.java	(working copy)
@@ -1266,8 +1266,7 @@
    * readers that correspond to a Directory with its own
    * segments_N file.
    *
-   * <p><b>WARNING</b>: this API is new and experimental and
-   * may suddenly change.</p>
+   * @lucene.experimental
    */
   public IndexCommit getIndexCommit() throws IOException {
     throw new UnsupportedOperationException("This reader does not support this method.");
Index: src/java/org/apache/lucene/index/DocumentsWriter.java
===================================================================
--- src/java/org/apache/lucene/index/DocumentsWriter.java	(revision 916567)
+++ src/java/org/apache/lucene/index/DocumentsWriter.java	(working copy)
@@ -42,6 +42,7 @@
 import org.apache.lucene.util.Constants;
 import org.apache.lucene.util.ThreadInterruptedException;
 import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.RamUsageEstimator;
 
 /**
  * This class accepts multiple added documents and directly
@@ -999,57 +1000,58 @@
     assert checkDeleteTerm(null);
 
     // Delete by term
-    try {
-      Fields fields = reader.fields();
-      TermsEnum termsEnum = null;
-
-      String currentField = null;
-      BytesRef termRef = new BytesRef();
-      DocsEnum docs = null;
-
-      for (Entry<Term, BufferedDeletes.Num> entry: deletesFlushed.terms.entrySet()) {
-        Term term = entry.getKey();
-        // Since we visit terms sorted, we gain performance
-        // by re-using the same TermsEnum and seeking only
-        // forwards
-        if (term.field() != currentField) {
-          // nocommit -- once we sync up branch again, add
-          // assert that this field is always > last one
-          currentField = term.field();
-          Terms terms = fields.terms(currentField);
-          if (terms != null) {
-            termsEnum = terms.iterator();
-          } else {
-            termsEnum = null;
+    if (deletesFlushed.terms.size() > 0) {
+      try {
+        Fields fields = reader.fields();
+        TermsEnum termsEnum = null;
+        
+        String currentField = null;
+        BytesRef termRef = new BytesRef();
+        DocsEnum docs = null;
+        
+        for (Entry<Term, BufferedDeletes.Num> entry: deletesFlushed.terms.entrySet()) {
+          Term term = entry.getKey();
+          // Since we visit terms sorted, we gain performance
+          // by re-using the same TermsEnum and seeking only
+          // forwards
+          if (term.field() != currentField) {
+            // nocommit -- once we sync up branch again, add
+            // assert that this field is always > last one
+            currentField = term.field();
+            Terms terms = fields.terms(currentField);
+            if (terms != null) {
+              termsEnum = terms.iterator();
+            } else {
+              termsEnum = null;
+            }
           }
-        }
-
-        if (termsEnum == null) {
-          continue;
-        }
-
-        termRef.copy(term.text());
-        if (termsEnum.seek(termRef) == TermsEnum.SeekStatus.FOUND) {
-          DocsEnum docsEnum = termsEnum.docs(reader.getDeletedDocs(), docs);
-
-          if (docsEnum != null) {
-            docs = docsEnum;
-            int limit = entry.getValue().getNum();
-            while (true) {
-              final int docID = docs.nextDoc();
-              if (docID == DocsEnum.NO_MORE_DOCS || docIDStart+docID >= limit) {
-                break;
+          
+          if (termsEnum == null) {
+            continue;
+          }
+          
+          termRef.copy(term.text());
+          if (termsEnum.seek(termRef) == TermsEnum.SeekStatus.FOUND) {
+            DocsEnum docsEnum = termsEnum.docs(reader.getDeletedDocs(), docs);
+            
+            if (docsEnum != null) {
+              docs = docsEnum;
+              int limit = entry.getValue().getNum();
+              while (true) {
+                final int docID = docs.nextDoc();
+                if (docID == DocsEnum.NO_MORE_DOCS || docIDStart+docID >= limit) {
+                  break;
+                }
+                reader.deleteDocument(docID);
+                any = true;
               }
-              reader.deleteDocument(docID);
-              any = true;
             }
           }
         }
+      } finally {
+        //docs.close();
       }
-    } finally {
-      //docs.close();
     }
-
     // Delete by docID
     for (Integer docIdInt : deletesFlushed.docIDs) {
       int docID = docIdInt.intValue();
@@ -1060,23 +1062,28 @@
     }
 
     // Delete by query
-    IndexSearcher searcher = new IndexSearcher(reader);
-    for (Entry<Query, Integer> entry : deletesFlushed.queries.entrySet()) {
-      Query query = entry.getKey();
-      int limit = entry.getValue().intValue();
-      Weight weight = query.weight(searcher);
-      Scorer scorer = weight.scorer(reader, true, false);
-      if (scorer != null) {
-        while(true)  {
-          int doc = scorer.nextDoc();
-          if (((long) docIDStart) + doc >= limit)
-            break;
-          reader.deleteDocument(doc);
-          any = true;
+    if (deletesFlushed.queries.size() > 0) {
+      IndexSearcher searcher = new IndexSearcher(reader);
+      try {
+        for (Entry<Query, Integer> entry : deletesFlushed.queries.entrySet()) {
+          Query query = entry.getKey();
+          int limit = entry.getValue().intValue();
+          Weight weight = query.weight(searcher);
+          Scorer scorer = weight.scorer(reader, true, false);
+          if (scorer != null) {
+            while(true)  {
+              int doc = scorer.nextDoc();
+              if (((long) docIDStart) + doc >= limit)
+                break;
+              reader.deleteDocument(doc);
+              any = true;
+            }
+          }
         }
+      } finally {
+        searcher.close();
       }
     }
-    searcher.close();
     return any;
   }
 
@@ -1507,7 +1514,7 @@
         int gap = doc.docID - nextWriteDocID;
         if (gap >= waiting.length) {
           // Grow queue
-          DocWriter[] newArray = new DocWriter[ArrayUtil.getNextSize(gap)];
+          DocWriter[] newArray = new DocWriter[ArrayUtil.oversize(gap, RamUsageEstimator.NUM_BYTES_OBJECT_REF)];
           assert nextWriteLoc >= 0;
           System.arraycopy(waiting, nextWriteLoc, newArray, 0, waiting.length-nextWriteLoc);
           System.arraycopy(waiting, 0, newArray, waiting.length-nextWriteLoc, nextWriteLoc);
Index: src/java/org/apache/lucene/index/TermVectorsReader.java
===================================================================
--- src/java/org/apache/lucene/index/TermVectorsReader.java	(revision 916567)
+++ src/java/org/apache/lucene/index/TermVectorsReader.java	(working copy)
@@ -20,6 +20,7 @@
 import org.apache.lucene.store.BufferedIndexInput;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.util.ArrayUtil;
 
 import java.io.IOException;
 import java.util.Arrays;
@@ -460,18 +461,14 @@
       if (preUTF8) {
         // Term stored as java chars
         if (charBuffer.length < totalLength) {
-          char[] newCharBuffer = new char[(int) (1.5*totalLength)];
-          System.arraycopy(charBuffer, 0, newCharBuffer, 0, start);
-          charBuffer = newCharBuffer;
+          charBuffer = ArrayUtil.grow(charBuffer, totalLength);
         }
         tvf.readChars(charBuffer, start, deltaLength);
         term = new String(charBuffer, 0, totalLength);
       } else {
         // Term stored as utf8 bytes
         if (byteBuffer.length < totalLength) {
-          byte[] newByteBuffer = new byte[(int) (1.5*totalLength)];
-          System.arraycopy(byteBuffer, 0, newByteBuffer, 0, start);
-          byteBuffer = newByteBuffer;
+          byteBuffer = ArrayUtil.grow(byteBuffer, totalLength);
         }
         tvf.readBytes(byteBuffer, start, deltaLength);
         term = new String(byteBuffer, 0, totalLength, "UTF-8");
Index: src/java/org/apache/lucene/index/IndexWriter.java
===================================================================
--- src/java/org/apache/lucene/index/IndexWriter.java	(revision 916567)
+++ src/java/org/apache/lucene/index/IndexWriter.java	(working copy)
@@ -362,8 +362,7 @@
    * if you attempt to reopen any of those readers, you'll
    * hit an {@link AlreadyClosedException}.</p>
    *
-   * <p><b>NOTE:</b> This API is experimental and might
-   * change in incompatible ways in the next release.</p>
+   * @lucene.experimental
    *
    * @return IndexReader that covers entire index plus all
    * changes made so far by this IndexWriter instance
@@ -3327,13 +3326,19 @@
     }
   }
 
-  // This is called after pending added and deleted
-  // documents have been flushed to the Directory but before
-  // the change is committed (new segments_N file written).
-  void doAfterFlush()
-    throws IOException {
-  }
+  /**
+   * A hook for extending classes to execute operations after pending added and
+   * deleted documents have been flushed to the Directory but before the change
+   * is committed (new segments_N file written).
+   */
+  protected void doAfterFlush() throws IOException {}
 
+  /**
+   * A hook for extending classes to execute operations before pending added and
+   * deleted documents are flushed to the Directory.
+   */
+  protected void doBeforeFlush() throws IOException {}
+
   /** Expert: prepare for commit.
    *
    * <p><b>NOTE</b>: if this method hits an OutOfMemoryError
@@ -3540,6 +3545,8 @@
 
     assert testPoint("startDoFlush");
 
+    doBeforeFlush();
+    
     flushCount++;
 
     // If we are flushing because too many deletes
@@ -4857,8 +4864,7 @@
    *  search, but will reduce search latency on opening a
    *  new near real-time reader after a merge completes.
    *
-   * <p><b>NOTE:</b> This API is experimental and might
-   * change in incompatible ways in the next release.</p>
+   * @lucene.experimental
    *
    * <p><b>NOTE</b>: warm is called before any deletes have
    * been carried over to the merged segment. */
@@ -4917,4 +4923,25 @@
   synchronized boolean isClosed() {
     return closed;
   }
+
+  /** Expert: remove any index files that are no longer
+   *  used.
+   * 
+   *  <p> IndexWriter normally deletes unused files itself,
+   *  during indexing.  However, on Windows, which disallows
+   *  deletion of open files, if there is a reader open on
+   *  the index then those files cannot be deleted.  This is
+   *  fine, because IndexWriter will periodically retry
+   *  the deletion.</p>
+   *
+   *  <p> However, IndexWriter doesn't try that often: only
+   *  on open, close, flushing a new segment, and finishing
+   *  a merge.  If you don't do any of these actions with your
+   *  IndexWriter, you'll see the unused files linger.  If
+   *  that's a problem, call this method to delete them
+   *  (once you've closed the open readers that were
+   *  preventing their deletion). */
+  public synchronized void deleteUnusedFiles() throws IOException {
+    deleter.deletePendingFiles();
+  }
 }
Index: src/java/org/apache/lucene/index/IndexFileDeleter.java
===================================================================
--- src/java/org/apache/lucene/index/IndexFileDeleter.java	(revision 916567)
+++ src/java/org/apache/lucene/index/IndexFileDeleter.java	(working copy)
@@ -343,7 +343,7 @@
     deletePendingFiles();
   }
 
-  private void deletePendingFiles() throws IOException {
+  public void deletePendingFiles() throws IOException {
     if (deletable != null) {
       List<String> oldDeletable = deletable;
       deletable = null;
Index: src/java/org/apache/lucene/index/codecs/pulsing/PulsingPostingsWriterImpl.java
===================================================================
--- src/java/org/apache/lucene/index/codecs/pulsing/PulsingPostingsWriterImpl.java	(revision 916567)
+++ src/java/org/apache/lucene/index/codecs/pulsing/PulsingPostingsWriterImpl.java	(working copy)
@@ -26,6 +26,7 @@
 import org.apache.lucene.store.IndexOutput;
 import org.apache.lucene.util.ArrayUtil;
 import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.RamUsageEstimator;
 
 // TODO: we now pulse entirely according to docFreq of the
 // term; it might be better to eg pulse by "net bytes used"
@@ -81,7 +82,7 @@
     }
 
     void reallocPositions(int minSize) {
-      final Position[] newArray = new Position[ArrayUtil.getNextSize(minSize)];
+      final Position[] newArray = new Position[ArrayUtil.oversize(minSize, RamUsageEstimator.NUM_BYTES_OBJECT_REF)];
       System.arraycopy(positions, 0, newArray, 0, positions.length);
       for(int i=positions.length;i<newArray.length;i++) {
         newArray[i] = new Position();
Index: src/java/org/apache/lucene/index/codecs/standard/SimpleStandardTermsIndexReader.java
===================================================================
--- src/java/org/apache/lucene/index/codecs/standard/SimpleStandardTermsIndexReader.java	(revision 916567)
+++ src/java/org/apache/lucene/index/codecs/standard/SimpleStandardTermsIndexReader.java	(working copy)
@@ -23,6 +23,7 @@
 import org.apache.lucene.index.FieldInfo;
 import org.apache.lucene.index.SegmentInfo;
 import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.RamUsageEstimator;
 import org.apache.lucene.index.codecs.Codec;
 import org.apache.lucene.index.IndexWriter;
 import org.apache.lucene.util.ArrayUtil;
@@ -339,7 +340,7 @@
               // New block
               final byte[] newBlock = new byte[BYTE_BLOCK_SIZE];
               if (blocks.length == blockUpto+1) {
-                final int newSize = ArrayUtil.getNextSize(blockUpto+2);
+                final int newSize = ArrayUtil.oversize(blockUpto+2, RamUsageEstimator.NUM_BYTES_CHAR);
                 final byte[][] newBlocks = new byte[newSize][];
                 System.arraycopy(blocks, 0, newBlocks, 0, blocks.length);
                 blocks = newBlocks;
Index: src/java/org/apache/lucene/store/ChecksumIndexOutput.java
===================================================================
--- src/java/org/apache/lucene/store/ChecksumIndexOutput.java	(revision 916567)
+++ src/java/org/apache/lucene/store/ChecksumIndexOutput.java	(working copy)
@@ -22,7 +22,10 @@
 import java.util.zip.Checksum;
 
 /** Writes bytes through to a primary IndexOutput, computing
- *  checksum.  Note that you cannot use seek().*/
+ *  checksum.  Note that you cannot use seek().
+ *
+ * @lucene.internal
+ */
 public class ChecksumIndexOutput extends IndexOutput {
   IndexOutput main;
   Checksum digest;
Index: src/java/org/apache/lucene/store/IndexInput.java
===================================================================
--- src/java/org/apache/lucene/store/IndexInput.java	(revision 916567)
+++ src/java/org/apache/lucene/store/IndexInput.java	(working copy)
@@ -22,6 +22,9 @@
 import java.util.Map;
 import java.util.HashMap;
 
+import org.apache.lucene.util.ArrayUtil;
+import org.apache.lucene.util.RamUsageEstimator;
+
 /** Abstract base class for input from a file in a {@link Directory}.  A
  * random-access input stream.  Used for all Lucene index input operations.
  * @see Directory
@@ -122,16 +125,18 @@
     if (preUTF8Strings)
       return readModifiedUTF8String();
     int length = readVInt();
-    if (bytes == null || length > bytes.length)
-      bytes = new byte[(int) (length*1.25)];
+    if (bytes == null || length > bytes.length) {
+      bytes = new byte[ArrayUtil.oversize(length, 1)];
+    }
     readBytes(bytes, 0, length);
     return new String(bytes, 0, length, "UTF-8");
   }
 
   private String readModifiedUTF8String() throws IOException {
     int length = readVInt();
-    if (chars == null || length > chars.length)
-      chars = new char[length];
+    if (chars == null || length > chars.length) {
+      chars = new char[ArrayUtil.oversize(length, RamUsageEstimator.NUM_BYTES_CHAR)];
+    }
     readChars(chars, 0, length);
     return new String(chars, 0, length);
   }
@@ -157,10 +162,11 @@
       else if ((b & 0xE0) != 0xE0) {
 	buffer[i] = (char)(((b & 0x1F) << 6)
 		 | (readByte() & 0x3F));
-      } else
+      } else {
 	buffer[i] = (char)(((b & 0x0F) << 12)
 		| ((readByte() & 0x3F) << 6)
 	        |  (readByte() & 0x3F));
+      }
     }
   }
 
@@ -181,10 +187,9 @@
       byte b = readByte();
       if ((b & 0x80) == 0){
         //do nothing, we only need one byte
-      }
-      else if ((b & 0xE0) != 0xE0) {
+      } else if ((b & 0xE0) != 0xE0) {
         readByte();//read an additional byte
-      } else{      
+      } else {      
         //read two additional bytes.
         readByte();
         readByte();
Index: src/java/org/apache/lucene/store/ChecksumIndexInput.java
===================================================================
--- src/java/org/apache/lucene/store/ChecksumIndexInput.java	(revision 916567)
+++ src/java/org/apache/lucene/store/ChecksumIndexInput.java	(working copy)
@@ -22,7 +22,10 @@
 import java.util.zip.Checksum;
 
 /** Writes bytes through to a primary IndexOutput, computing
- *  checksum as it goes. Note that you cannot use seek(). */
+ *  checksum as it goes. Note that you cannot use seek().
+ *
+ * @lucene.internal
+ */
 public class ChecksumIndexInput extends IndexInput {
   IndexInput main;
   Checksum digest;
Index: src/java/org/apache/lucene/store/NativeFSLockFactory.java
===================================================================
--- src/java/org/apache/lucene/store/NativeFSLockFactory.java	(revision 916567)
+++ src/java/org/apache/lucene/store/NativeFSLockFactory.java	(working copy)
@@ -159,12 +159,21 @@
   /*
    * The javadocs for FileChannel state that you should have
    * a single instance of a FileChannel (per JVM) for all
-   * locking against a given file.  To ensure this, we have
-   * a single (static) HashSet that contains the file paths
-   * of all currently locked locks.  This protects against
-   * possible cases where different Directory instances in
-   * one JVM (each with their own NativeFSLockFactory
-   * instance) have set the same lock dir and lock prefix.
+   * locking against a given file (locks are tracked per 
+   * FileChannel instance in Java 1.4/1.5). Even using the same 
+   * FileChannel instance is not completely thread-safe with Java 
+   * 1.4/1.5 though. To work around this, we have a single (static) 
+   * HashSet that contains the file paths of all currently 
+   * locked locks.  This protects against possible cases 
+   * where different Directory instances in one JVM (each 
+   * with their own NativeFSLockFactory instance) have set 
+   * the same lock dir and lock prefix. However, this will not 
+   * work when LockFactorys are created by different 
+   * classloaders (eg multiple webapps). 
+   * 
+   * TODO: Java 1.6 tracks system wide locks in a thread safe manner 
+   * (same FileChannel instance or not), so we may want to 
+   * change this when Lucene moves to Java 1.6.
    */
   private static HashSet<String> LOCK_HELD = new HashSet<String>();
 
Index: src/java/org/apache/lucene/store/FileSwitchDirectory.java
===================================================================
--- src/java/org/apache/lucene/store/FileSwitchDirectory.java	(revision 916567)
+++ src/java/org/apache/lucene/store/FileSwitchDirectory.java	(working copy)
@@ -31,8 +31,7 @@
  * to this class, and must allow multiple threads to call
  * contains at once.</p>
  *
- * <p><b>NOTE</b>: this API is new and experimental and is
- * subject to suddenly change in the next release.
+ * @lucene.experimental
  */
 
 public class FileSwitchDirectory extends Directory {
Index: src/java/org/apache/lucene/util/BitVector.java
===================================================================
--- src/java/org/apache/lucene/util/BitVector.java	(revision 916567)
+++ src/java/org/apache/lucene/util/BitVector.java	(working copy)
@@ -63,7 +63,7 @@
   /** Sets the value of <code>bit</code> to one. */
   public final void set(int bit) {
     if (bit >= size) {
-      throw new ArrayIndexOutOfBoundsException(bit);
+      throw new ArrayIndexOutOfBoundsException("bit=" + bit + " size=" + size);
     }
     bits[bit >> 3] |= 1 << (bit & 7);
     count = -1;
@@ -73,7 +73,7 @@
    *  returns true if bit was already set */
   public final boolean getAndSet(int bit) {
     if (bit >= size) {
-      throw new ArrayIndexOutOfBoundsException(bit);
+      throw new ArrayIndexOutOfBoundsException("bit=" + bit + " size=" + size);
     }
     final int pos = bit >> 3;
     final int v = bits[pos];
Index: src/java/org/apache/lucene/util/StringHelper.java
===================================================================
--- src/java/org/apache/lucene/util/StringHelper.java	(revision 916567)
+++ src/java/org/apache/lucene/util/StringHelper.java	(working copy)
@@ -20,6 +20,8 @@
 
 /**
  * Methods for manipulating strings.
+ *
+ * @lucene.internal
  */
 public abstract class StringHelper {
   /**
Index: src/java/org/apache/lucene/util/BitUtil.java
===================================================================
--- src/java/org/apache/lucene/util/BitUtil.java	(revision 916567)
+++ src/java/org/apache/lucene/util/BitUtil.java	(working copy)
@@ -18,8 +18,7 @@
 package org.apache.lucene.util; // from org.apache.solr.util rev 555343
 
 /**  A variety of high efficiency bit twiddling routines.
- *
- * @version $Id$
+ * @lucene.internal
  */
 public class BitUtil {
 
Index: src/java/org/apache/lucene/util/CharacterUtils.java
===================================================================
--- src/java/org/apache/lucene/util/CharacterUtils.java	(revision 916567)
+++ src/java/org/apache/lucene/util/CharacterUtils.java	(working copy)
@@ -1,5 +1,8 @@
 package org.apache.lucene.util;
 
+import java.io.IOException;
+import java.io.Reader;
+
 /**
  * Licensed to the Apache Software Foundation (ASF) under one or more
  * contributor license agreements.  See the NOTICE file distributed with
@@ -21,6 +24,8 @@
  * {@link CharacterUtils} provides a unified interface to Character-related
  * operations to implement backwards compatible character operations based on a
  * {@link Version} instance.
+ * 
+ * @lucene.internal
  */
 public abstract class CharacterUtils {
   private static final Java4CharacterUtils JAVA_4 = new Java4CharacterUtils();
@@ -104,10 +109,52 @@
    *           the char array.
    */
   public abstract int codePointAt(final char[] chars, final int offset, final int limit);
+  
+  /**
+   * Creates a new {@link CharacterBuffer} and allocates a <code>char[]</code>
+   * of the given bufferSize.
+   * 
+   * @param bufferSize
+   *          the internal char buffer size, must be <code>&gt;= 2</code>
+   * @return a new {@link CharacterBuffer} instance.
+   */
+  public static CharacterBuffer newCharacterBuffer(final int bufferSize) {
+    if(bufferSize < 2)
+      throw new IllegalArgumentException("buffersize must be >= 2");
+    return new CharacterBuffer(new char[bufferSize], 0, 0);
+  }
 
+  /**
+   * Fills the {@link CharacterBuffer} with characters read from the given
+   * reader {@link Reader}. This method tries to read as many characters into
+   * the {@link CharacterBuffer} as possible, each call to fill will start
+   * filling the buffer from offset <code>0</code> up to the length of the size
+   * of the internal character array.
+   * <p>
+   * Depending on the {@link Version} passed to
+   * {@link CharacterUtils#getInstance(Version)} this method implements
+   * supplementary character awareness when filling the given buffer. For all
+   * {@link Version} &gt; 3.0 {@link #fill(CharacterBuffer, Reader)} guarantees
+   * that the given {@link CharacterBuffer} will never contain a high surrogate
+   * character as the last element in the buffer unless it is the last available
+   * character in the reader. In other words, high and low surrogate pairs will
+   * always be preserved across buffer boarders.
+   * </p>
+   * 
+   * @param buffer
+   *          the buffer to fill.
+   * @param reader
+   *          the reader to read characters from.
+   * @return <code>true</code> if and only if no more characters are available
+   *         in the reader, otherwise <code>false</code>.
+   * @throws IOException
+   *           if the reader throws an {@link IOException}.
+   */
+  public abstract boolean fill(CharacterBuffer buffer, Reader reader) throws IOException;
+
   private static final class Java5CharacterUtils extends CharacterUtils {
     Java5CharacterUtils() {
-    };
+    }
 
     @Override
     public final int codePointAt(final char[] chars, final int offset) {
@@ -124,12 +171,32 @@
      return Character.codePointAt(chars, offset, limit);
     }
 
-    
+    @Override
+    public boolean fill(final CharacterBuffer buffer, final Reader reader) throws IOException {
+      final char[] charBuffer = buffer.buffer;
+      buffer.offset = 0;
+      charBuffer[0] = buffer.lastTrailingHighSurrogate;
+      final int offset = buffer.lastTrailingHighSurrogate == 0 ? 0 : 1;
+      buffer.lastTrailingHighSurrogate = 0;
+      final int read = reader.read(charBuffer, offset, charBuffer.length
+          - offset);
+      if (read == -1) {
+        buffer.length = offset;
+        return offset != 0;
+      }
+      buffer.length = read + offset;
+      // special case if the read returns 0 and the lastTrailingHighSurrogate was set
+      if (buffer.length > 1
+          && Character.isHighSurrogate(charBuffer[buffer.length - 1])) {
+        buffer.lastTrailingHighSurrogate = charBuffer[--buffer.length];
+      }
+      return true;
+    }
   }
 
   private static final class Java4CharacterUtils extends CharacterUtils {
     Java4CharacterUtils() {
-    };
+    }
 
     @Override
     public final int codePointAt(final char[] chars, final int offset) {
@@ -148,6 +215,72 @@
       return chars[offset];
     }
 
+    @Override
+    public boolean fill(final CharacterBuffer buffer, final Reader reader) throws IOException {
+      buffer.offset = 0;
+      final int read = reader.read(buffer.buffer);
+      if(read == -1)
+        return false;
+      buffer.length = read;
+      return true;
+    }
+
   }
+  
+  /**
+   * A simple IO buffer to use with
+   * {@link CharacterUtils#fill(CharacterBuffer, Reader)}.
+   */
+  public static final class CharacterBuffer {
+    
+    private final char[] buffer;
+    private int offset;
+    private int length;
+    private char lastTrailingHighSurrogate = 0;
+    
+    CharacterBuffer(char[] buffer, int offset, int length) {
+      this.buffer = buffer;
+      this.offset = offset;
+      this.length = length;
+    }
+    
+    /**
+     * Returns the internal buffer
+     * 
+     * @return the buffer
+     */
+    public char[] getBuffer() {
+      return buffer;
+    }
+    
+    /**
+     * Returns the data offset in the internal buffer.
+     * 
+     * @return the offset
+     */
+    public int getOffset() {
+      return offset;
+    }
+    
+    /**
+     * Return the length of the data in the internal buffer starting at
+     * {@link #getOffset()}
+     * 
+     * @return the length
+     */
+    public int getLength() {
+      return length;
+    }
+    
+    /**
+     * Resets the CharacterBuffer. All internals are reset to its default
+     * values.
+     */
+    public void reset() {
+      offset = 0;
+      length = 0;
+      lastTrailingHighSurrogate = 0;
+    }
+  }
 
 }
Index: src/java/org/apache/lucene/util/Parameter.java
===================================================================
--- src/java/org/apache/lucene/util/Parameter.java	(revision 916567)
+++ src/java/org/apache/lucene/util/Parameter.java	(working copy)
@@ -26,6 +26,7 @@
 /**
  * A serializable Enum class.
  * @deprecated Use Java 5 enum, will be removed in a later Lucene 3.x release.
+ * @lucene.internal
  */
 @Deprecated
 @SuppressWarnings("serial")
Index: src/java/org/apache/lucene/util/cache/SimpleLRUCache.java
===================================================================
--- src/java/org/apache/lucene/util/cache/SimpleLRUCache.java	(revision 916567)
+++ src/java/org/apache/lucene/util/cache/SimpleLRUCache.java	(working copy)
@@ -27,6 +27,8 @@
  *
  * @deprecated Lucene's internal use of this class has now
  * switched to {@link DoubleBarrelLRUCache}.
+ *
+ * @lucene.internal
  */
 @Deprecated
 public class SimpleLRUCache<K,V> extends SimpleMapCache<K,V> {
Index: src/java/org/apache/lucene/util/cache/Cache.java
===================================================================
--- src/java/org/apache/lucene/util/cache/Cache.java	(revision 916567)
+++ src/java/org/apache/lucene/util/cache/Cache.java	(working copy)
@@ -21,6 +21,8 @@
 
 /**
  * Base class for cache implementations.
+ *
+ * @lucene.internal
  */
 public abstract class Cache<K,V> implements Closeable {
   
Index: src/java/org/apache/lucene/util/cache/DoubleBarrelLRUCache.java
===================================================================
--- src/java/org/apache/lucene/util/cache/DoubleBarrelLRUCache.java	(revision 916567)
+++ src/java/org/apache/lucene/util/cache/DoubleBarrelLRUCache.java	(working copy)
@@ -38,11 +38,7 @@
  * not grow unbounded in size when under hideously high
  * load.</p>
  *
- * <p>NOTE: this class is meant only to be used internally
- * by Lucene; it's only public so it can be shared across
- * packages.  This means the API is freely subject to
- * change, and, the class could be removed entirely, in any
- * Lucene release.  Use directly at your own risk!
+ * @lucene.internal
  */
 
 final public class DoubleBarrelLRUCache<K,V> extends Cache<K,V> {
Index: src/java/org/apache/lucene/util/cache/SimpleMapCache.java
===================================================================
--- src/java/org/apache/lucene/util/cache/SimpleMapCache.java	(revision 916567)
+++ src/java/org/apache/lucene/util/cache/SimpleMapCache.java	(working copy)
@@ -28,6 +28,8 @@
  *
  * @deprecated Lucene's internal use of this class has now
  * switched to {@link DoubleBarrelLRUCache}.
+ *
+ * @lucene.internal
  */
 @Deprecated
 public class SimpleMapCache<K,V> extends Cache<K,V> {
Index: src/java/org/apache/lucene/util/Version.java
===================================================================
--- src/java/org/apache/lucene/util/Version.java	(revision 916567)
+++ src/java/org/apache/lucene/util/Version.java	(working copy)
@@ -50,20 +50,32 @@
   /** Match settings and bugs in Lucene's 3.0 release. */
   LUCENE_30,
 
-  /** Match settings and bugs in Lucene's 3.1 release. */
+  /** Match settings and bugs in Lucene's 3.1 release. 
+   *  <p>
+   *  Use this to get the latest &amp; greatest settings, bug
+   *  fixes, etc, for Lucene.
+   */
   LUCENE_31,
   
   /* Add new constants for later versions **here** to respect order! */
 
-  /** Use this to get the latest &amp; greatest settings, bug
-   *  fixes, etc, for Lucene.
-   *
+  /**
    * <p><b>WARNING</b>: if you use this setting, and then
    * upgrade to a newer release of Lucene, sizable changes
-   * may happen.  If precise back compatibility is important
+   * may happen.  If backwards compatibility is important
    * then you should instead explicitly specify an actual
    * version.
+   * <p>
+   * If you use this constant then you  may need to 
+   * <b>re-index all of your documents</b> when upgrading
+   * Lucene, as the way text is indexed may have changed. 
+   * Additionally, you may need to <b>re-test your entire
+   * application</b> to ensure it behaves as expected, as 
+   * some defaults may have changed and may break functionality 
+   * in your application. 
+   * @deprecated Use an actual version instead. 
    */
+  @Deprecated
   LUCENE_CURRENT;
 
   public boolean onOrAfter(Version other) {
Index: src/java/org/apache/lucene/util/SimpleStringInterner.java
===================================================================
--- src/java/org/apache/lucene/util/SimpleStringInterner.java	(revision 916567)
+++ src/java/org/apache/lucene/util/SimpleStringInterner.java	(working copy)
@@ -19,7 +19,10 @@
 
 /**
  * Simple lockless and memory barrier free String intern cache that is guaranteed
- * to return the same String instance as String.intern() does.
+ * to return the same String instance as String.intern()
+ * does.
+ *
+ * @lucene.internal
  */
 public class SimpleStringInterner extends StringInterner {
 
Index: src/java/org/apache/lucene/util/VirtualMethod.java
===================================================================
--- src/java/org/apache/lucene/util/VirtualMethod.java	(revision 916567)
+++ src/java/org/apache/lucene/util/VirtualMethod.java	(working copy)
@@ -20,7 +20,7 @@
 import java.lang.reflect.Method;
 import java.util.Collections;
 import java.util.HashSet;
-import java.util.IdentityHashMap;
+import java.util.WeakHashMap;
 import java.util.Set;
 
 /**
@@ -54,6 +54,8 @@
  * The one with the larger distance should be used preferable.
  * This way also more complicated method rename scenarios can be handled
  * (think of 2.9 {@code TokenStream} deprecations).</p>
+ *
+ * @lucene.internal
  */
 public final class VirtualMethod<C> {
 
@@ -62,8 +64,8 @@
   private final Class<C> baseClass;
   private final String method;
   private final Class<?>[] parameters;
-  private final IdentityHashMap<Class<? extends C>, Integer> cache =
-    new IdentityHashMap<Class<? extends C>, Integer>();
+  private final WeakHashMap<Class<? extends C>, Integer> cache =
+    new WeakHashMap<Class<? extends C>, Integer>();
 
   /**
    * Creates a new instance for the given {@code baseClass} and method declaration.
Index: src/java/org/apache/lucene/util/CloseableThreadLocal.java
===================================================================
--- src/java/org/apache/lucene/util/CloseableThreadLocal.java	(revision 916567)
+++ src/java/org/apache/lucene/util/CloseableThreadLocal.java	(working copy)
@@ -48,6 +48,8 @@
  *  {@link #close} takes care of all
  *  threads.  You should not call {@link #close} until all
  *  threads are done using the instance.
+ *
+ * @lucene.internal
  */
 
 public class CloseableThreadLocal<T> implements Closeable {
Index: src/java/org/apache/lucene/util/ReaderUtil.java
===================================================================
--- src/java/org/apache/lucene/util/ReaderUtil.java	(revision 916567)
+++ src/java/org/apache/lucene/util/ReaderUtil.java	(working copy)
@@ -27,6 +27,7 @@
 /**
  * Common util methods for dealing with {@link IndexReader}s.
  *
+ * @lucene.internal
  */
 public class ReaderUtil {
 
Index: src/java/org/apache/lucene/util/MapOfSets.java
===================================================================
--- src/java/org/apache/lucene/util/MapOfSets.java	(revision 916567)
+++ src/java/org/apache/lucene/util/MapOfSets.java	(working copy)
@@ -25,6 +25,7 @@
 
 /**
  * Helper class for keeping Lists of Objects associated with keys. <b>WARNING: THIS CLASS IS NOT THREAD SAFE</b>
+ * @lucene.internal
  */
 public class MapOfSets<K, V> {
 
Index: src/java/org/apache/lucene/util/DummyConcurrentLock.java
===================================================================
--- src/java/org/apache/lucene/util/DummyConcurrentLock.java	(revision 916567)
+++ src/java/org/apache/lucene/util/DummyConcurrentLock.java	(working copy)
@@ -24,6 +24,7 @@
 
 /**
  * A dummy lock as a replacement for {@link ReentrantLock} to disable locking
+ * @lucene.internal
  */
 public final class DummyConcurrentLock implements Lock {
 
Index: src/java/org/apache/lucene/util/IndexableBinaryStringTools.java
===================================================================
--- src/java/org/apache/lucene/util/IndexableBinaryStringTools.java	(revision 916567)
+++ src/java/org/apache/lucene/util/IndexableBinaryStringTools.java	(working copy)
@@ -50,6 +50,8 @@
  * WARNING: This means that the deprecated Buffer-based methods 
  * only work correctly with buffers that have an offset of 0. For example, they
  * will not correctly interpret buffers returned by {@link ByteBuffer#slice}.  
+ *
+ * @lucene.experimental
  */
 public class IndexableBinaryStringTools {
 
Index: src/java/org/apache/lucene/util/ArrayUtil.java
===================================================================
--- src/java/org/apache/lucene/util/ArrayUtil.java	(revision 916567)
+++ src/java/org/apache/lucene/util/ArrayUtil.java	(working copy)
@@ -7,9 +7,9 @@
  * The ASF licenses this file to You under the Apache License, Version 2.0
  * (the "License"); you may not use this file except in compliance with
  * the License.  You may obtain a copy of the License at
- * <p/>
- * http://www.apache.org/licenses/LICENSE-2.0
- * <p/>
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
  * Unless required by applicable law or agreed to in writing, software
  * distributed under the License is distributed on an "AS IS" BASIS,
  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -19,8 +19,20 @@
 
 /**
  * Methods for manipulating arrays.
+ *
+ * @lucene.internal
  */
 public final class ArrayUtil {
+
+  /**
+   * @deprecated This constructor was not intended to be public and should not be used.
+   *  This class contains solely a static utility methods.
+   *  It will be made private in Lucene 4.0
+   */
+  // make private in 4.0!
+  @Deprecated
+  public ArrayUtil() {} // no instance
+
   /*
      Begin Apache Harmony code
 
@@ -122,20 +134,95 @@
  END APACHE HARMONY CODE
   */
 
+  /** Returns an array size >= minTargetSize, generally
+   *  over-allocating exponentially to achieve amortized
+   *  linear-time cost as the array grows.
+   *
+   *  NOTE: this was originally borrowed from Python 2.4.2
+   *  listobject.c sources (attribution in LICENSE.txt), but
+   *  has now been substantially changed based on
+   *  discussions from java-dev thread with subject "Dynamic
+   *  array reallocation algorithms", started on Jan 12
+   *  2010.
+   *
+   * @param minTargetSize Minimum required value to be returned.
+   * @param bytesPerElement Bytes used by each element of
+   * the array.  See constants in {@link RamUsageEstimator}.
+   *
+   * @lucene.internal
+   */
 
-  public static int getNextSize(int targetSize) {
-    /* This over-allocates proportional to the list size, making room
-     * for additional growth.  The over-allocation is mild, but is
-     * enough to give linear-time amortized behavior over a long
-     * sequence of appends() in the presence of a poorly-performing
-     * system realloc().
-     * The growth pattern is:  0, 4, 8, 16, 25, 35, 46, 58, 72, 88, ...
-     */
-    return (targetSize >> 3) + (targetSize < 9 ? 3 : 6) + targetSize;
+  public static int oversize(int minTargetSize, int bytesPerElement) {
+
+    if (minTargetSize < 0) {
+      // catch usage that accidentally overflows int
+      throw new IllegalArgumentException("invalid array size " + minTargetSize);
+    }
+
+    if (minTargetSize == 0) {
+      // wait until at least one element is requested
+      return 0;
+    }
+
+    // asymptotic exponential growth by 1/8th, favors
+    // spending a bit more CPU to not tye up too much wasted
+    // RAM:
+    int extra = minTargetSize >> 3;
+
+    if (extra < 3) {
+      // for very small arrays, where constant overhead of
+      // realloc is presumably relatively high, we grow
+      // faster
+      extra = 3;
+    }
+
+    int newSize = minTargetSize + extra;
+
+    // add 7 to allow for worst case byte alignment addition below:
+    if (newSize+7 < 0) {
+      // int overflowed -- return max allowed array size
+      return Integer.MAX_VALUE;
+    }
+
+    if (Constants.JRE_IS_64BIT) {
+      // round up to 8 byte alignment in 64bit env
+      switch(bytesPerElement) {
+      case 4:
+        // round up to multiple of 2
+        return (newSize + 1) & 0x7ffffffe;
+      case 2:
+        // round up to multiple of 4
+        return (newSize + 3) & 0x7ffffffc;
+      case 1:
+        // round up to multiple of 8
+        return (newSize + 7) & 0x7ffffff8;
+      case 8:
+        // no rounding
+      default:
+        // odd (invalid?) size
+        return newSize;
+      }
+    } else {
+      // round up to 4 byte alignment in 64bit env
+      switch(bytesPerElement) {
+      case 2:
+        // round up to multiple of 2
+        return (newSize + 1) & 0x7ffffffe;
+      case 1:
+        // round up to multiple of 4
+        return (newSize + 3) & 0x7ffffffc;
+      case 4:
+      case 8:
+        // no rounding
+      default:
+        // odd (invalid?) size
+        return newSize;
+      }
+    }
   }
 
-  public static int getShrinkSize(int currentSize, int targetSize) {
-    final int newSize = getNextSize(targetSize);
+  public static int getShrinkSize(int currentSize, int targetSize, int bytesPerElement) {
+    final int newSize = oversize(targetSize, bytesPerElement);
     // Only reallocate if we are "substantially" smaller.
     // This saves us from "running hot" (constantly making a
     // bit bigger then a bit smaller, over and over):
@@ -147,7 +234,7 @@
 
   public static int[] grow(int[] array, int minSize) {
     if (array.length < minSize) {
-      int[] newArray = new int[getNextSize(minSize)];
+      int[] newArray = new int[oversize(minSize, RamUsageEstimator.NUM_BYTES_INT)];
       System.arraycopy(array, 0, newArray, 0, array.length);
       return newArray;
     } else
@@ -159,7 +246,7 @@
   }
 
   public static int[] shrink(int[] array, int targetSize) {
-    final int newSize = getShrinkSize(array.length, targetSize);
+    final int newSize = getShrinkSize(array.length, targetSize, RamUsageEstimator.NUM_BYTES_INT);
     if (newSize != array.length) {
       int[] newArray = new int[newSize];
       System.arraycopy(array, 0, newArray, 0, newSize);
@@ -170,7 +257,7 @@
 
   public static long[] grow(long[] array, int minSize) {
     if (array.length < minSize) {
-      long[] newArray = new long[getNextSize(minSize)];
+      long[] newArray = new long[oversize(minSize, RamUsageEstimator.NUM_BYTES_LONG)];
       System.arraycopy(array, 0, newArray, 0, array.length);
       return newArray;
     } else
@@ -182,7 +269,7 @@
   }
 
   public static long[] shrink(long[] array, int targetSize) {
-    final int newSize = getShrinkSize(array.length, targetSize);
+    final int newSize = getShrinkSize(array.length, targetSize, RamUsageEstimator.NUM_BYTES_LONG);
     if (newSize != array.length) {
       long[] newArray = new long[newSize];
       System.arraycopy(array, 0, newArray, 0, newSize);
@@ -193,7 +280,7 @@
 
   public static byte[] grow(byte[] array, int minSize) {
     if (array.length < minSize) {
-      byte[] newArray = new byte[getNextSize(minSize)];
+      byte[] newArray = new byte[oversize(minSize, 1)];
       System.arraycopy(array, 0, newArray, 0, array.length);
       return newArray;
     } else
@@ -204,10 +291,10 @@
     return grow(array, 1 + array.length);
   }
 
-  public static char[] shrink(char[] array, int targetSize) {
-    final int newSize = getShrinkSize(array.length, targetSize);
+  public static byte[] shrink(byte[] array, int targetSize) {
+    final int newSize = getShrinkSize(array.length, targetSize, 1);
     if (newSize != array.length) {
-      char[] newArray = new char[newSize];
+      byte[] newArray = new byte[newSize];
       System.arraycopy(array, 0, newArray, 0, newSize);
       return newArray;
     } else
@@ -216,7 +303,7 @@
 
   public static char[] grow(char[] array, int minSize) {
     if (array.length < minSize) {
-      char[] newArray = new char[getNextSize(minSize)];
+      char[] newArray = new char[oversize(minSize, RamUsageEstimator.NUM_BYTES_CHAR)];
       System.arraycopy(array, 0, newArray, 0, array.length);
       return newArray;
     } else
@@ -227,10 +314,10 @@
     return grow(array, 1 + array.length);
   }
 
-  public static byte[] shrink(byte[] array, int targetSize) {
-    final int newSize = getShrinkSize(array.length, targetSize);
+  public static char[] shrink(char[] array, int targetSize) {
+    final int newSize = getShrinkSize(array.length, targetSize, RamUsageEstimator.NUM_BYTES_CHAR);
     if (newSize != array.length) {
-      byte[] newArray = new byte[newSize];
+      char[] newArray = new char[newSize];
       System.arraycopy(array, 0, newArray, 0, newSize);
       return newArray;
     } else
Index: src/java/org/apache/lucene/util/OpenBitSet.java
===================================================================
--- src/java/org/apache/lucene/util/OpenBitSet.java	(revision 916567)
+++ src/java/org/apache/lucene/util/OpenBitSet.java	(working copy)
@@ -73,8 +73,6 @@
    <th>1% full</th> <td>2.51</td> <td>3.49</td> <td>&nbsp;</td> <td>1.00</td> <td>&nbsp;</td> <td>1.02</td>
  </tr>
 </table>
-
- * @version $Id$
  */
 
 public class OpenBitSet extends DocIdSet implements Bits, Cloneable, Serializable {
@@ -809,12 +807,16 @@
 
   @Override
   public int hashCode() {
-      long h = 0x98761234;  // something non-zero for length==0
-      for (int i = bits.length; --i>=0;) {
+    // Start with a zero hash and use a mix that results in zero if the input is zero.
+    // This effectively truncates trailing zeros without an explicit check.
+    long h = 0;
+    for (int i = bits.length; --i>=0;) {
       h ^= bits[i];
       h = (h << 1) | (h >>> 63); // rotate left
     }
-    return (int)((h>>32) ^ h);  // fold leftmost bits into right
+    // fold leftmost bits into right and add a constant to prevent
+    // empty sets from returning 0, which is too common.
+    return (int)((h>>32) ^ h) + 0x98761234;
   }
 
 }
Index: src/java/org/apache/lucene/util/NumericUtils.java
===================================================================
--- src/java/org/apache/lucene/util/NumericUtils.java	(revision 916567)
+++ src/java/org/apache/lucene/util/NumericUtils.java	(working copy)
@@ -55,8 +55,7 @@
  * {@link String#compareTo(String)}) representations of numeric data types for other
  * usages (e.g. sorting).
  *
- * <p><font color="red"><b>NOTE:</b> This API is experimental and
- * might change in incompatible ways in the next release.</font>
+ * @lucene.experimental
  *
  * @since 2.9
  */
Index: src/java/org/apache/lucene/util/AttributeSource.java
===================================================================
--- src/java/org/apache/lucene/util/AttributeSource.java	(revision 916567)
+++ src/java/org/apache/lucene/util/AttributeSource.java	(working copy)
@@ -17,11 +17,12 @@
  * limitations under the License.
  */
 
+import java.lang.ref.WeakReference;
 import java.util.Collections;
 import java.util.NoSuchElementException;
 import java.util.Iterator;
 import java.util.LinkedHashMap;
-import java.util.IdentityHashMap;
+import java.util.WeakHashMap;
 import java.util.LinkedList;
 import java.util.Map;
 import java.util.Map.Entry;
@@ -54,8 +55,8 @@
     public static final AttributeFactory DEFAULT_ATTRIBUTE_FACTORY = new DefaultAttributeFactory();
     
     private static final class DefaultAttributeFactory extends AttributeFactory {
-      private static final IdentityHashMap<Class<? extends Attribute>, Class<? extends AttributeImpl>> attClassImplMap =
-        new IdentityHashMap<Class<? extends Attribute>, Class<? extends AttributeImpl>>();
+      private static final WeakHashMap<Class<? extends Attribute>, WeakReference<Class<? extends AttributeImpl>>> attClassImplMap =
+        new WeakHashMap<Class<? extends Attribute>, WeakReference<Class<? extends AttributeImpl>>>();
       
       private DefaultAttributeFactory() {}
     
@@ -72,12 +73,15 @@
       
       private static Class<? extends AttributeImpl> getClassForInterface(Class<? extends Attribute> attClass) {
         synchronized(attClassImplMap) {
-          Class<? extends AttributeImpl> clazz = attClassImplMap.get(attClass);
+          final WeakReference<Class<? extends AttributeImpl>> ref = attClassImplMap.get(attClass);
+          Class<? extends AttributeImpl> clazz = (ref == null) ? null : ref.get();
           if (clazz == null) {
             try {
               attClassImplMap.put(attClass,
-                clazz = Class.forName(attClass.getName() + "Impl", true, attClass.getClassLoader())
-                .asSubclass(AttributeImpl.class)
+                new WeakReference<Class<? extends AttributeImpl>>(
+                  clazz = Class.forName(attClass.getName() + "Impl", true, attClass.getClassLoader())
+                  .asSubclass(AttributeImpl.class)
+                )
               );
             } catch (ClassNotFoundException e) {
               throw new IllegalArgumentException("Could not find implementing class for " + attClass.getName());
@@ -173,8 +177,8 @@
   }
   
   /** a cache that stores all interfaces for known implementation classes for performance (slow reflection) */
-  private static final IdentityHashMap<Class<? extends AttributeImpl>,LinkedList<Class<? extends Attribute>>> knownImplClasses =
-    new IdentityHashMap<Class<? extends AttributeImpl>,LinkedList<Class<? extends Attribute>>>();
+  private static final WeakHashMap<Class<? extends AttributeImpl>,LinkedList<WeakReference<Class<? extends Attribute>>>> knownImplClasses =
+    new WeakHashMap<Class<? extends AttributeImpl>,LinkedList<WeakReference<Class<? extends Attribute>>>>();
   
   /** <b>Expert:</b> Adds a custom AttributeImpl instance with one or more Attribute interfaces.
    * <p><font color="red"><b>Please note:</b> It is not guaranteed, that <code>att</code> is added to
@@ -187,18 +191,20 @@
   public void addAttributeImpl(final AttributeImpl att) {
     final Class<? extends AttributeImpl> clazz = att.getClass();
     if (attributeImpls.containsKey(clazz)) return;
-    LinkedList<Class<? extends Attribute>> foundInterfaces;
+    LinkedList<WeakReference<Class<? extends Attribute>>> foundInterfaces;
     synchronized(knownImplClasses) {
       foundInterfaces = knownImplClasses.get(clazz);
       if (foundInterfaces == null) {
-        knownImplClasses.put(clazz, foundInterfaces = new LinkedList<Class<? extends Attribute>>());
+        // we have a strong reference to the class instance holding all interfaces in the list (parameter "att"),
+        // so all WeakReferences are never evicted by GC
+        knownImplClasses.put(clazz, foundInterfaces = new LinkedList<WeakReference<Class<? extends Attribute>>>());
         // find all interfaces that this attribute instance implements
         // and that extend the Attribute interface
         Class<?> actClazz = clazz;
         do {
           for (Class<?> curInterface : actClazz.getInterfaces()) {
             if (curInterface != Attribute.class && Attribute.class.isAssignableFrom(curInterface)) {
-              foundInterfaces.add(curInterface.asSubclass(Attribute.class));
+              foundInterfaces.add(new WeakReference<Class<? extends Attribute>>(curInterface.asSubclass(Attribute.class)));
             }
           }
           actClazz = actClazz.getSuperclass();
@@ -207,7 +213,10 @@
     }
     
     // add all interfaces of this AttributeImpl to the maps
-    for (Class<? extends Attribute> curInterface : foundInterfaces) {
+    for (WeakReference<Class<? extends Attribute>> curInterfaceRef : foundInterfaces) {
+      final Class<? extends Attribute> curInterface = curInterfaceRef.get();
+      assert (curInterface != null) :
+        "We have a strong reference on the class holding the interfaces, so they should never get evicted";
       // Attribute is a superclass of this interface
       if (!attributes.containsKey(curInterface)) {
         // invalidate state to force recomputation in captureState()
Index: src/java/org/apache/lucene/util/FieldCacheSanityChecker.java
===================================================================
--- src/java/org/apache/lucene/util/FieldCacheSanityChecker.java	(revision 916567)
+++ src/java/org/apache/lucene/util/FieldCacheSanityChecker.java	(working copy)
@@ -44,11 +44,7 @@
  * Unit tests) to check at run time if the FieldCache contains "insane" 
  * usages of the FieldCache.
  * </p>
- * <p>
- * <b>EXPERIMENTAL API:</b> This API is considered extremely advanced and 
- * experimental.  It may be removed or altered w/o warning in future releases 
- * of Lucene.
- * </p>
+ * @lucene.experimental
  * @see FieldCache
  * @see FieldCacheSanityChecker.Insanity
  * @see FieldCacheSanityChecker.InsanityType
Index: src/java/org/apache/lucene/util/UnicodeUtil.java
===================================================================
--- src/java/org/apache/lucene/util/UnicodeUtil.java	(revision 916567)
+++ src/java/org/apache/lucene/util/UnicodeUtil.java	(working copy)
@@ -54,8 +54,7 @@
  * without always allocating a new byte[] as
  * String.getBytes("UTF-8") does.
  *
- * <p><b>WARNING</b>: This API is a new and experimental and
- * may suddenly change. </p>
+ * @lucene.internal
  */
 
 final public class UnicodeUtil {
@@ -72,6 +71,9 @@
   private static final long HALF_SHIFT = 10;
   private static final long HALF_MASK = 0x3FFL;
 
+  /**
+   * @lucene.internal
+   */
   public static final class UTF16Result {
     public char[] result = new char[10];
     public int[] offsets = new int[10];
@@ -113,7 +115,7 @@
     // Pre-allocate for worst case 4-for-1
     final int maxLen = length * 4;
     if (out.length < maxLen)
-      out = result.bytes = new byte[maxLen];
+      out = result.bytes = new byte[ArrayUtil.oversize(maxLen, RamUsageEstimator.NUM_BYTES_CHAR)];
     result.offset = 0;
 
     while(i < end) {
@@ -169,7 +171,7 @@
     // Pre-allocate for worst case 4-for-1
     final int maxLen = length * 4;
     if (out.length < maxLen)
-      out = result.bytes = new byte[maxLen];
+      out = result.bytes = new byte[ArrayUtil.oversize(maxLen, RamUsageEstimator.NUM_BYTES_CHAR)];
     result.offset = 0;
 
     while(i < end) {
@@ -223,7 +225,7 @@
     // Pre-allocate for worst case 4-for-1
     final int maxLen = length * 4;
     if (out.length < maxLen)
-      out = result.bytes = new byte[maxLen];
+      out = result.bytes = new byte[ArrayUtil.oversize(maxLen, RamUsageEstimator.NUM_BYTES_CHAR)];
 
     int upto = 0;
     for(int i=offset;i<end;i++) {
@@ -274,9 +276,7 @@
     final int end = offset + length;
     char[] out = result.result;
     if (result.offsets.length <= end) {
-      int[] newOffsets = new int[2*end];
-      System.arraycopy(result.offsets, 0, newOffsets, 0, result.offsets.length);
-      result.offsets  = newOffsets;
+      result.offsets = ArrayUtil.grow(result.offsets, end+1);
     }
     final int[] offsets = result.offsets;
 
@@ -290,9 +290,7 @@
 
     // Pre-allocate for worst case 1-for-1
     if (outUpto+length >= out.length) {
-      char[] newOut = new char[2*(outUpto+length)];
-      System.arraycopy(out, 0, newOut, 0, outUpto);
-      result.result = out = newOut;
+      out = result.result = ArrayUtil.grow(out, outUpto+length+1);
     }
 
     while (upto < end) {
Index: src/java/org/apache/lucene/util/PriorityQueue.java
===================================================================
--- src/java/org/apache/lucene/util/PriorityQueue.java	(revision 916567)
+++ src/java/org/apache/lucene/util/PriorityQueue.java	(working copy)
@@ -23,7 +23,8 @@
  *
  * <p><b>NOTE</b>: This class pre-allocates a full array of
  * length <code>maxSize+1</code>, in {@link #initialize}.
-  * 
+ * 
+ * @lucene.internal
 */
 public abstract class PriorityQueue<T> {
   private int size;
Index: src/java/org/apache/lucene/util/OpenBitSetIterator.java
===================================================================
--- src/java/org/apache/lucene/util/OpenBitSetIterator.java	(revision 916567)
+++ src/java/org/apache/lucene/util/OpenBitSetIterator.java	(working copy)
@@ -22,8 +22,6 @@
 /** An iterator to iterate over set bits in an OpenBitSet.
  * This is faster than nextSetBit() for iterating over the complete set of bits,
  * especially when the density of the bits set is high.
- *
- * @version $Id$
  */
 public class OpenBitSetIterator extends DocIdSetIterator {
 
Index: src/java/org/apache/lucene/util/ScorerDocQueue.java
===================================================================
--- src/java/org/apache/lucene/util/ScorerDocQueue.java	(revision 916567)
+++ src/java/org/apache/lucene/util/ScorerDocQueue.java	(working copy)
@@ -27,6 +27,8 @@
 /** A ScorerDocQueue maintains a partial ordering of its Scorers such that the
   least Scorer can always be found in constant time.  Put()'s and pop()'s
   require log(size) time. The ordering is by Scorer.doc().
+ *
+ * @lucene.internal
  */
 public class ScorerDocQueue {  // later: SpansQueue for spans with doc and term positions
   private final HeapedScorerDoc[] heap;
Index: src/java/org/apache/lucene/util/RamUsageEstimator.java
===================================================================
--- src/java/org/apache/lucene/util/RamUsageEstimator.java	(revision 916567)
+++ src/java/org/apache/lucene/util/RamUsageEstimator.java	(working copy)
@@ -30,9 +30,11 @@
  * Internally uses a Map to temporally hold a reference to every
  * object seen. 
  * 
- * If checkIntered, all Strings checked will be interned, but those
+ * If checkInterned, all Strings checked will be interned, but those
  * that were not already interned will be released for GC when the
  * estimate is complete.
+ * 
+ * @lucene.internal
  */
 public final class RamUsageEstimator {
   private MemoryModel memoryModel;
@@ -43,6 +45,14 @@
   private int arraySize;
   private int classSize;
 
+  public final static int NUM_BYTES_OBJECT_REF = Constants.JRE_IS_64BIT ? 8 : 4;
+  public final static int NUM_BYTES_CHAR = 2;
+  public final static int NUM_BYTES_SHORT = 2;
+  public final static int NUM_BYTES_INT = 4;
+  public final static int NUM_BYTES_LONG = 8;
+  public final static int NUM_BYTES_FLOAT = 4;
+  public final static int NUM_BYTES_DOUBLE = 8;
+
   private boolean checkInterned;
 
   /**
Index: src/java/org/apache/lucene/util/SortedVIntList.java
===================================================================
--- src/java/org/apache/lucene/util/SortedVIntList.java	(revision 916567)
+++ src/java/org/apache/lucene/util/SortedVIntList.java	(working copy)
@@ -128,8 +128,8 @@
       }
   
       if ((lastBytePos + MAX_BYTES_PER_INT) > bytes.length) {
-        // biggest possible int does not fit
-        resizeBytes((bytes.length * 2) + MAX_BYTES_PER_INT);
+        // Biggest possible int does not fit.
+        resizeBytes(ArrayUtil.oversize(lastBytePos + MAX_BYTES_PER_INT, 1));
       }
   
       // See org.apache.lucene.store.IndexOutput.writeVInt()
Index: src/java/org/apache/lucene/util/SmallFloat.java
===================================================================
--- src/java/org/apache/lucene/util/SmallFloat.java	(revision 916567)
+++ src/java/org/apache/lucene/util/SmallFloat.java	(working copy)
@@ -18,7 +18,7 @@
 
 /** Floating point numbers smaller than 32 bits.
  *
- * @version $Id$
+ * @lucene.internal
  */
 public class SmallFloat {
 
Index: src/java/org/apache/lucene/document/NumericField.java
===================================================================
--- src/java/org/apache/lucene/document/NumericField.java	(revision 916567)
+++ src/java/org/apache/lucene/document/NumericField.java	(working copy)
@@ -134,9 +134,7 @@
  * values are returned as {@link String}s (according to
  * <code>toString(value)</code> of the used data type).
  *
- * <p><font color="red"><b>NOTE:</b> This API is
- * experimental and might change in incompatible ways in the
- * next release.</font>
+ * @lucene.experimental
  *
  * @since 2.9
  */
Index: src/site/src/documentation/content/xdocs/site.xml
===================================================================
--- src/site/src/documentation/content/xdocs/site.xml	(revision 916567)
+++ src/site/src/documentation/content/xdocs/site.xml	(working copy)
@@ -72,7 +72,6 @@
 		    <javadoc-contrib-similarity label="Similarity" href="ext:javadocs-contrib-similarity"/>
 -->			
 		    <javadoc-contrib-remote label="Remote" href="ext:javadocs-contrib-remote"/>
-            <javadoc-contrib-snowball label="Snowball" href="ext:javadocs-contrib-snowball"/>
 		    <javadoc-contrib-spatial label="Spatial" href="ext:javadocs-contrib-spatial"/>
 		    <javadoc-contrib-spellchecker label="Spellchecker" href="ext:javadocs-contrib-spellchecker"/>
 		    <javadoc-contrib-surround label="Surround" href="ext:javadocs-contrib-surround"/>			
@@ -125,7 +124,6 @@
 	<javadocs-contrib-regex href="api/contrib-regex/index.html"/>
 	<javadocs-contrib-remote href="api/contrib-remote/index.html"/>
 	<javadocs-contrib-similarity href="api/contrib-similarity/index.html"/>
-	<javadocs-contrib-snowball href="api/contrib-snowball/index.html"/>
 	<javadocs-contrib-spatial href="api/contrib-spatial/index.html"/>
 	<javadocs-contrib-spellchecker href="api/contrib-spellchecker/index.html"/>
 	<javadocs-contrib-surround href="api/contrib-surround/index.html"/>
Index: src/site/src/documentation/content/xdocs/lucene-contrib/index.xml
===================================================================
--- src/site/src/documentation/content/xdocs/lucene-contrib/index.xml	(revision 916567)
+++ src/site/src/documentation/content/xdocs/lucene-contrib/index.xml	(working copy)
@@ -120,12 +120,7 @@
             <section id="remote"><title>remote</title>
                 <p>Classes to help use Lucene with RMI.</p>
                 <p>See <a href="../api/contrib-remote/index.html">remote javadoc</a></p>
-            </section>    
-            
-            <section id="snowball"><title>snowball</title>
-                <p>Pre-compiled versions of the Snowball stemmers for Lucene.</p>
-                <p>See <a href="../api/contrib-snowball/index.html">snowball javadoc</a></p>
-            </section>               
+            </section>                 
                         
             <section id="spatial"><title>spatial</title>
                 <p>Classes to help with efficient distance based sorting.</p>
Index: src/site/src/documentation/content/xdocs/fileformats.xml
===================================================================
--- src/site/src/documentation/content/xdocs/fileformats.xml	(revision 916567)
+++ src/site/src/documentation/content/xdocs/fileformats.xml	(working copy)
@@ -1846,11 +1846,12 @@
 
             <p>
 	      When referring to term numbers, Lucene's current
-	      implementation uses a Java <code>int</code>, which means
-	      the maximum number of unique terms in any single index
-	      segment is 2,147,483,648.  This is technically not a
-	      limitation of the index file format, just of Lucene's
-	      current implementation.
+	      implementation uses a Java <code>int</code> to hold the
+	      term index, which means the maximum number of unique
+	      terms in any single index segment is ~2.1 billion times
+	      the term index interval (default 128) = ~274 billion.
+	      This is technically not a limitation of the index file
+	      format, just of Lucene's current implementation.
 	    </p>
 	    <p>
 	      Similarly, Lucene uses a Java <code>int</code> to refer
Index: docs/fileformats.pdf
===================================================================
--- docs/fileformats.pdf	(revision 916567)
+++ docs/fileformats.pdf	(working copy)
@@ -667,10 +667,10 @@
 >>
 endobj
 111 0 obj
-<< /Length 1801 /Filter [ /ASCII85Decode /FlateDecode ]
+<< /Length 1845 /Filter [ /ASCII85Decode /FlateDecode ]
  >>
 stream
-Gatm<>E@c%'Ro4HjK13fG\lPL<L(^M)=;1H]aSa"3di^?5e;fsk/+q5qDnr@j5@/#>)aB'A"VfQS_N=$kG,NXdH;*]f';idMD;tC&-V:5k;U.'/tp@-$LTH7^Q,1p?N)YM*ArPAGr!8(0!,&.e*ZcV$F&^;F6Ba=Bj-erc@iWHmO",>o\fW2U#KJ8%,]-0<;4\qcMW)dW(W4r')L'%gC?hm*f`HWnV%dbVZG#-_jUj#'P\O_oZ'&jIQr]%54&Rn#K1_F/7O2B740fG=lc[4Ro3YmceBq7Jn539+CM0NV+[Zoel+/JaFIhGpmP4I%\.R[ZC5-ce+e*fLJ4fAT\@3Qg<OUkg%TUHnGP,C=+#_'.pZ4H",NEsLTNJM(fBH(?KeXfo^Bc8fTuro@"@"D!KU38XjWeP#<ha-pVbZZknj=14l:`^lpdt5U*+CR:Rd:o1k!7K+CnUk"W%n9HHE4gBNuJ8`C+?h=TFac<%Jbl7LF%';\'KIXPn!+dA)t3eiE2m_tS#TS7_cUE%q?XTk5a=alQPQGf:S;O8pJoArGIg8D1^=@"9Xra$]:!Qj.`rXSB_"RPF;J`A:g<&58dgZ<6Y60G+Xe$CU3W``ZHtZ,gKj2a.5`>Gc1u-7W7h"cbCre?E8TM&At<U83)97qa[m#f:qI#Ki-S6k-)'9O'n)hi>8O!?\Zsm0q0)\(>&=We,.h1o`pZE7qOk1rFOCj3(%1n;J"B0i&M[BePZMTY8(,j_P+]_P5I?\J/(K8Z6DjQ_^H`SEO/<c%UDPoC1LgWl98*ZQ1]Y35)SuSu#ZX`icHJ?a0aBM$o43HuJb;OMG;$MM6UhF!JK?Ak#AJZ[A84JW*RL^jn2O7"f!cRE`_?KbQ6?b_61\#p$X"5_iJeHa38glO9^pJ:2R^=5R/3#XE]"\+C,=$RjI@3i)9d>Y">"_S/9Z@,81;O;nec;K.Sh35ViII!^N'h`d&Fa!oTbnAF-)SW:(h*^RW!ZkFMnf4=9B6Til(FqYcoO?tniTMb>_+7;LbV\NtQAdON4g_="n/dHr.9:I`img>pmXGCoL.5dT;_%U,hKC-2."mArlK#2QtTjDcU^OZijUGX1BS&fM_IqtMc4YW2pchD7rNX.bkiX:CO+B#XE_l!(M&o8,(lC&:VFoWIf2JGO'0#O'r5)PBdF3_]8*R*2#c0Gb-m"MMVl"<Q\)h\G'())cK.N`6a-%YMn%^1Ye$lp!7*.*f0%_Tkd\nG$RNi()\E3I*dq/R_VI+tT[^l@#-T'\c[&GE1s;J9gn?>4r1ri%&PJjQZZUW)F>7c#SGN$u)DUp-W7j\O%/N"t%)+rC6*J5b&i/rg?gf)qQ>0oC<>?l$4CDGTap@Wn1#_`e#([onhm(7l/DLQ].K%&"i%@qog]SY54_K@#]&\CdSEi2a6_A-\ptml007OI2_pGssbFA@6[laqdrV2`R)'V"YX4fS*\3f>,V>4#L01afe)u\m0--fn,sQ)-]FFs,o^4H.qb\/+2kb`6l+lrN;Y4Gh8JS#)F1%g,CT;[NS:Jn>_(cEbKaGProLPjEZK-n>=?sW2b%F]e/'&0&"]e\7%IOAG78TS[D<B6J_lP+`S*fEMr*BXh*5qda[qM%Kd&rep2">N8C+WmlWBfB"=LHK?[dUc'ujV5J\KMr?G`JG;6k+/'3XN&F5`o:W;koaM4seWJhHt0u4mEMOr5Bo>0`iiqk,6HmeKLq4s5I7W&`*]&'g1*_%_OUd!0NMK,:Za1Fo%!78.6-^dpe7i'q*bg[A:V/te//^*dZeQmd3b+3">0,"]'IfX.'6n&~>
+Gatm<>BAg]'RnB3n/LcAW02O@J/Pq745JTKec65F#:5KGZ*oni-#N@,hi;M#d?,Yd>@9L+Eb+E>^2oQXAs9^1ai=_X<r-kr8MS(g'LhkJ&@T?O5+sCo9('qO5>Q0'^&@dco^idiB%[<2Vm-.F?@O`+.)LNo/%e)!DF@c9H;iPf\X6.,28n**]:&i5-/YXjesG6chX''KWD??0@/BgBgPg/i+("o=j<n96JHW2\NCtH4;(--Hcu8[Ho3me+HoMW^Nj.ai3S6_S8e,ZT<EZQ[*"p'`+#gT(P2urT+CM6TM?VH5S#$TYAAL$dI)Tj*"^5eHhXIk!>2NX(P0'ODl8@)CV8'q/0JJ'qT,2dZj@eRhl2)eG5hql,[U'&GA5g]tBkWd"ht*C]:.X,IJD:$Y8*\o"9@*4#=aI`*i2[+QM-hWkQX:B7;*(I#aIE4Y-7]I@hlm_I=Ys^3AIVYCG^U?e;aZ/$Qjg2,^d?/3#mjXsjHcBla:tk5Zc1Wp9=8F?5!XlfGL:G5q0gNL#ZR+WL!h;L)l;MY.r[`SLg6)`G/k_:L[KZnU*72gF,.XiE3`m5#7f1@JS.WIo@efS#26'!,qgh`+lfY'KqWP4WnNG$TdmPA&P1bNXJEpZ\B5_g1KufB?AQgcV/oL3562M1$jDUZ1WCYN83`gae1`4ETh\sT\c[KpZmarCa$tR*@=Y;>DfdgNV-pfL=BX>945KHrX\,OeJpGQrNP66?Z4=7n(UJ]F!FJr.#+@(,[$AS,(A(1%\_5*=<PGsSO*;ZEEi+/(;=T%W.k=A%=B:L<*HkrMe:Q"K!o(Z8M"5fTp5;nc@?lk,O^Rbo0TFml-2!Z@S",OMUdbOmPgq&m$kpp(/s\/TK^i\n,l!8D/"_jCY\W;Bet,j)"#4k9Y,V3e.jkl?8C,=oI"U('Om*36V"Gb`8+F[?+[^`ihp$Zpfdkg:i#!bSJpjj$%L?d`6_#Kp=0B4+LVMVirQss;"@\*>SV2gop=(.1,XC28DF:0,;X?n:1j]'Wb.SUFW*)2i<_uNVVVkC$3J`Vrg?RV1/(db)*gG\kW84OJT(0*+D9"Le6*KPe'UFCGc%8Uf"`,Uc=9Bb\NVp6YGICZ%"(U$^I)i[3O21F*H=B\^-/',Lfd)SVJd[/QeTj/7!KC?b2Sr8-R&2$!%f9\.Z;KjY/\pF]/`6V/E$iU0aau[6U,bNl>lkro4uNKp7S4\bE2qFEc1sV3VnHsGg=oj%ebEhM2anjEk9A5(cdng-*M"4t5uKquQQ_a69:iC8#Ds(]^mtB.WYGadC0Adc)WACId3qH"_<63NQG`$BeI4^mk/[+:Hd>#e@!<rjJ$SJH#6O1o##9u9l)W:ZfsbXV+M,&X6(="C1>eWJq1$^qlA*I$M'_V<k0bqPl/2]ggobt6/(a%(H!7o$LRAr<aGH_$n5*Nt.*49\&/q<#V:KYBinB,M$4VidP;>g+Q_O?7#,E2C?@^ATK_ZF85?IOS2(1O_eSX_NLU4h_NH#b`4qeD:hJW!qA#HkL"n)KBH@-.LpP>=ki2,O,5a/`l#J;k\mu>9-Z2"<uSKA$uSF8t(>"?/*82.L2pc7M[%J+,^%=DgZ1dXbF=T#uc3+nuNVAp&*oI>jo`AQCR[td7;TqaI0,q(ah.jdG(2!m_[[pb3OpT`#>323Wj*LEW=Faj\2j3$aQbKW?iCt:Ss^4Pb5'-*c,geVDUOr:]L=E^(LLf<IncuH^"q'WVVKi%n&-Fh)tc^hYHL#BWt#t'qHTpGo9I+%V4UGf_^XO^[]a%=lBBh/Y$0oc[M^NB,OJ?GXh<)u6NY6Y$KXhG`+:!W*djgcNqfbL0&mZ^ps9@Co["2nl!!W~>
 endstream
 endobj
 112 0 obj
@@ -1206,80 +1206,80 @@
 xref
 0 153
 0000000000 65535 f 
-0000054489 00000 n 
-0000054695 00000 n 
-0000054788 00000 n 
+0000054533 00000 n 
+0000054739 00000 n 
+0000054832 00000 n 
 0000000015 00000 n 
 0000000071 00000 n 
 0000001333 00000 n 
 0000001453 00000 n 
 0000001639 00000 n 
-0000054940 00000 n 
+0000054984 00000 n 
 0000001774 00000 n 
-0000055003 00000 n 
+0000055047 00000 n 
 0000001909 00000 n 
-0000055069 00000 n 
+0000055113 00000 n 
 0000002046 00000 n 
-0000055133 00000 n 
+0000055177 00000 n 
 0000002183 00000 n 
-0000055199 00000 n 
+0000055243 00000 n 
 0000002320 00000 n 
-0000055265 00000 n 
+0000055309 00000 n 
 0000002457 00000 n 
-0000055331 00000 n 
+0000055375 00000 n 
 0000002594 00000 n 
-0000055395 00000 n 
+0000055439 00000 n 
 0000002731 00000 n 
-0000055459 00000 n 
+0000055503 00000 n 
 0000002868 00000 n 
-0000055525 00000 n 
+0000055569 00000 n 
 0000003005 00000 n 
-0000055590 00000 n 
+0000055634 00000 n 
 0000003142 00000 n 
-0000055656 00000 n 
+0000055700 00000 n 
 0000003279 00000 n 
-0000055722 00000 n 
+0000055766 00000 n 
 0000003416 00000 n 
-0000055787 00000 n 
+0000055831 00000 n 
 0000003553 00000 n 
-0000055853 00000 n 
+0000055897 00000 n 
 0000003690 00000 n 
-0000055917 00000 n 
+0000055961 00000 n 
 0000003826 00000 n 
-0000055981 00000 n 
+0000056025 00000 n 
 0000003963 00000 n 
-0000056047 00000 n 
+0000056091 00000 n 
 0000004100 00000 n 
-0000056113 00000 n 
+0000056157 00000 n 
 0000004237 00000 n 
-0000056178 00000 n 
+0000056222 00000 n 
 0000004373 00000 n 
-0000056244 00000 n 
+0000056288 00000 n 
 0000004510 00000 n 
-0000056308 00000 n 
+0000056352 00000 n 
 0000004647 00000 n 
-0000056374 00000 n 
+0000056418 00000 n 
 0000004783 00000 n 
-0000056440 00000 n 
+0000056484 00000 n 
 0000004920 00000 n 
 0000005673 00000 n 
 0000005796 00000 n 
 0000005872 00000 n 
-0000056504 00000 n 
+0000056548 00000 n 
 0000006004 00000 n 
-0000056570 00000 n 
+0000056614 00000 n 
 0000006137 00000 n 
-0000056634 00000 n 
+0000056678 00000 n 
 0000006270 00000 n 
-0000056699 00000 n 
+0000056743 00000 n 
 0000006403 00000 n 
-0000056764 00000 n 
+0000056808 00000 n 
 0000006536 00000 n 
-0000056829 00000 n 
+0000056873 00000 n 
 0000006669 00000 n 
-0000056894 00000 n 
+0000056938 00000 n 
 0000006801 00000 n 
-0000056959 00000 n 
+0000057003 00000 n 
 0000006934 00000 n 
 0000009085 00000 n 
 0000009193 00000 n 
@@ -1317,47 +1317,47 @@
 0000043768 00000 n 
 0000045499 00000 n 
 0000045609 00000 n 
-0000047504 00000 n 
-0000057024 00000 n 
-0000047614 00000 n 
-0000047814 00000 n 
-0000048032 00000 n 
-0000048238 00000 n 
-0000048446 00000 n 
-0000048614 00000 n 
-0000048814 00000 n 
-0000048972 00000 n 
-0000049147 00000 n 
-0000049410 00000 n 
-0000049651 00000 n 
-0000049780 00000 n 
-0000049934 00000 n 
-0000050088 00000 n 
-0000050232 00000 n 
-0000050382 00000 n 
-0000050523 00000 n 
-0000050758 00000 n 
-0000050953 00000 n 
-0000051193 00000 n 
-0000051375 00000 n 
-0000051548 00000 n 
-0000051751 00000 n 
-0000051939 00000 n 
-0000052191 00000 n 
-0000052332 00000 n 
-0000052541 00000 n 
-0000052727 00000 n 
-0000052901 00000 n 
-0000053146 00000 n 
-0000053337 00000 n 
-0000053543 00000 n 
-0000053709 00000 n 
-0000053823 00000 n 
-0000053934 00000 n 
-0000054046 00000 n 
-0000054155 00000 n 
-0000054262 00000 n 
-0000054379 00000 n 
+0000047548 00000 n 
+0000057068 00000 n 
+0000047658 00000 n 
+0000047858 00000 n 
+0000048076 00000 n 
+0000048282 00000 n 
+0000048490 00000 n 
+0000048658 00000 n 
+0000048858 00000 n 
+0000049016 00000 n 
+0000049191 00000 n 
+0000049454 00000 n 
+0000049695 00000 n 
+0000049824 00000 n 
+0000049978 00000 n 
+0000050132 00000 n 
+0000050276 00000 n 
+0000050426 00000 n 
+0000050567 00000 n 
+0000050802 00000 n 
+0000050997 00000 n 
+0000051237 00000 n 
+0000051419 00000 n 
+0000051592 00000 n 
+0000051795 00000 n 
+0000051983 00000 n 
+0000052235 00000 n 
+0000052376 00000 n 
+0000052585 00000 n 
+0000052771 00000 n 
+0000052945 00000 n 
+0000053190 00000 n 
+0000053381 00000 n 
+0000053587 00000 n 
+0000053753 00000 n 
+0000053867 00000 n 
+0000053978 00000 n 
+0000054090 00000 n 
+0000054199 00000 n 
+0000054306 00000 n 
+0000054423 00000 n 
 trailer
 <<
 /Size 153
@@ -1365,5 +1365,5 @@
 /Info 4 0 R
 >>
 startxref
-57078
+57122
 %%EOF
Index: docs/demo.html
===================================================================
--- docs/demo.html	(revision 916567)
+++ docs/demo.html	(working copy)
@@ -186,9 +186,6 @@
 <a href="api/contrib-remote/index.html">Remote</a>
 </div>
 <div class="menuitem">
-<a href="api/contrib-snowball/index.html">Snowball</a>
-</div>
-<div class="menuitem">
 <a href="api/contrib-spatial/index.html">Spatial</a>
 </div>
 <div class="menuitem">
Index: docs/gettingstarted.html
===================================================================
--- docs/gettingstarted.html	(revision 916567)
+++ docs/gettingstarted.html	(working copy)
@@ -186,9 +186,6 @@
 <a href="api/contrib-remote/index.html">Remote</a>
 </div>
 <div class="menuitem">
-<a href="api/contrib-snowball/index.html">Snowball</a>
-</div>
-<div class="menuitem">
 <a href="api/contrib-spatial/index.html">Spatial</a>
 </div>
 <div class="menuitem">
Index: docs/systemrequirements.html
===================================================================
--- docs/systemrequirements.html	(revision 916567)
+++ docs/systemrequirements.html	(working copy)
@@ -184,9 +184,6 @@
 <a href="api/contrib-remote/index.html">Remote</a>
 </div>
 <div class="menuitem">
-<a href="api/contrib-snowball/index.html">Snowball</a>
-</div>
-<div class="menuitem">
 <a href="api/contrib-spatial/index.html">Spatial</a>
 </div>
 <div class="menuitem">
Index: docs/queryparsersyntax.html
===================================================================
--- docs/queryparsersyntax.html	(revision 916567)
+++ docs/queryparsersyntax.html	(working copy)
@@ -186,9 +186,6 @@
 <a href="api/contrib-remote/index.html">Remote</a>
 </div>
 <div class="menuitem">
-<a href="api/contrib-snowball/index.html">Snowball</a>
-</div>
-<div class="menuitem">
 <a href="api/contrib-spatial/index.html">Spatial</a>
 </div>
 <div class="menuitem">
Index: docs/linkmap.html
===================================================================
--- docs/linkmap.html	(revision 916567)
+++ docs/linkmap.html	(working copy)
@@ -184,9 +184,6 @@
 <a href="api/contrib-remote/index.html">Remote</a>
 </div>
 <div class="menuitem">
-<a href="api/contrib-snowball/index.html">Snowball</a>
-</div>
-<div class="menuitem">
 <a href="api/contrib-spatial/index.html">Spatial</a>
 </div>
 <div class="menuitem">
@@ -434,12 +431,6 @@
 <a href="api/contrib-remote/index.html">Remote</a>&nbsp;&nbsp;___________________&nbsp;&nbsp;<em>javadoc-contrib-remote</em>
 </li>
 </ul>
-            
-<ul>
-<li>
-<a href="api/contrib-snowball/index.html">Snowball</a>&nbsp;&nbsp;___________________&nbsp;&nbsp;<em>javadoc-contrib-snowball</em>
-</li>
-</ul>
 		    
 <ul>
 <li>
Index: docs/linkmap.pdf
===================================================================
--- docs/linkmap.pdf	(revision 916567)
+++ docs/linkmap.pdf	(working copy)
@@ -20,10 +20,10 @@
 >>
 endobj
 7 0 obj
-<< /Length 1167 /Filter [ /ASCII85Decode /FlateDecode ]
+<< /Length 1136 /Filter [ /ASCII85Decode /FlateDecode ]
  >>
 stream
-GatUtgMYb*&;KZP'Y8*6biM,Qkcuf*"Vr\oM%"KWm:*oQ[%iU33L!e#DnjdN(?NsIki?KCB`=(B5;SNfXr"jTBS8'id<`ETd;FLZBRWg38Sg"7aoI'6[,+<&^\a$GI31$QU<ri&h3VVPj<gb.$idp>GoutuXoC'flqLq_+%$)8_7%./nqZ""feTVN7d!RjU/r1pd&SDo,N0bg0Rd%!8tU_Z;$%"=[_a#)5=kPtOT=$&RFdiV$<aCo=9la`gq^q<ckWWZntOPX!RQDoE)\l"iK/j686]NJpc;%B0R3`<%F"ONj$*B>l<<Eqj'*;QVPS*<^m144FfP(,nG3FTf#M[<\m=4h0I=<8`n9\G8$u8[_s:(u`pEVE:u)NS&@(!BO/=fRFdH7bh8D5lXTo.S-$4Jp/Z[?a/;dh1h"EL8CWng)2\?R7/DR[/&$j;g3%.*6W$QEe3bnFL3HU:9oO-BUp8kQdf&NI#P^l0S1=rU+*?Qt(c<<i$>$&ab'5dheasjbVY%Z]\I&$m,9.E*djmK:OkL^Q[_X&"bN/0G$l<F7N+4MX(iS2I?P$hL)c?[""GZ[O(S0dj1d[O,lpDabX+`)ucdS[p&a^\sTrUoj,AY'@km2+fAY_`D$Ynq-jhCjlEnFY%Td1<U\GG683!QI(2IBFNs^F\tR]cM7Di)SIcET06)e[gsl<cT9>G.[]O^D4d9YKC6H_T`7;D5#2!lsk]dQ`iS8"HsEW:VN*t?5bBM?=)`I\if(SV=ubC!>EJtNX8p/CD<)h0sM7S;<M?^>r>@6E!P+(Oh+E+K7\&N3ZGC_Rq#C]gG=$rNL@+tbj1WG-eg]71)O!+Y50`lU=0s;hiE]kbnl5Y#sCG<N=8"q[aij@qcJGu>H3Is5#g"tS>WO64Itu89B,qNf93)QS/s:.KX6Wk0md8->M2eX]&=,=#nYEY3!,;"RurYKdbg5Mh<>u%1FY<$`e8X5ZBkg^g0-ZTp4$(H.r:b;l,'(EMq@Aq7+*6I?/sESXP*XU9I$[6mG_REZ2Jisk*XZV&OE-:#(e5_(u3g@Gd:!Gqi-/=FZcOb0-H'KO>%T@^4J'(;gkYUB:\L_RkC=Hp\d1NkRq;0fd0fdX%Vs@+L>dP<nla^p#O8q4WV\RgOmUbh2BAdFP?C5<bCLn:`FB,]Dhml*+R\~>
+GatUt?'!]!'Sc)P'tJ-6nG4C/aCSR-.oi_$)3DGX[]8+jR-]KZEC),SI\!Z6*7e4Ibgjgso013?O,q3Uc/Od!N=Z:Jg?]S/g+Q8*bo_fsM9SIY).5.Y2M^Lg2r?<dDkeUF'UjNbq/f7#bet4(iK*<NgZX!kq.M&+X2L)`L'/'>D1R2u0=`[`AQfkWc@.D;:8(HMYEJO:KS.pnpfSV0I0kWr#p<@&$?O9U0B&Zo!algU<i%:lAYI7F'au%&!uou]=0P09Mb9095R6Sl8a/p&6R$7fD1t"1b95c]j9ZAV0Up+C]C:a.Q-t*#f.=:#9A.76.kM:s>jCM1q?Ug$@lIZKe9<oJM/Lf[]=n0Ap?HjHrs?K.SUV-L3rpEIFrU>+oL>tN1/"sBpkGPd0B&[ZDGRHsZHcu%Nen/kZR+b$eY(m[N.p$9l_E,YdO1;D2pM^eBEAfTb?VAKFaUtE3[_lBUigr4[_]0b)jA:M`YImALl[cEB.pC5=g>`BQT2DWXtVp@+!\VIB)7rZB1KASAGC1cO<iHoR<+ri'd"qMcBe<1l><6[HNnC4KIq143bWat8OAq!p%RWFj-8b_`r?GR+smL6L.S=HG[$W%EaIJg6f*fZV0>ZWecN,rhS)ss3fIW6Wg?Xr[S9cOGKb%`=05XDD'epRa)8A9ka(4-B4V[68[u<]YUu*nR=1%HmIJf>LL;k=kK.r=`D6Tl?=%3R)P?T#e-?ljJ-k7j9\=qYUTiM_M*7e>`6Q3gQ`B@h_sM0i[FDSk*Ya88Za2AjZTcp9^--Oa;F[5%@pX1T0j3)aQGK5kl^R=O,kG&HBRW0`ba_rp485m274j/<Le[V#D\SLF[k1U.@NR)^mSd2%Skf(--#rY\Bh;8k:*5hUKO&HEN7n*>g<+k^e+IqC8:Yr6_@`/'g^H5sPO=`Zet/9a2j8hLCeU32cNea&Y&37Gh#:?bZbj_]P?mt0c/PRJ_+tt;A.UaY-^H*0!7OK>BX`4g]Pi625]]V6n;&2a&d,t6@1;R@I68ql_/<\SWK,#@o"+st'I8W,VGe'r?)eFsnY3Ag;r-RLc+8DB'TE]ZOH%ZM8WCQuUJ.TU]%khKW@(bJc08%_]02P:QKMc)C0-U$R@3qFD8E`*rrKOE35G~>
 endstream
 endobj
 8 0 obj
@@ -87,19 +87,19 @@
 xref
 0 14
 0000000000 65535 f 
-0000003247 00000 n 
-0000003311 00000 n 
-0000003361 00000 n 
+0000003216 00000 n 
+0000003280 00000 n 
+0000003330 00000 n 
 0000000015 00000 n 
 0000000071 00000 n 
 0000001219 00000 n 
 0000001325 00000 n 
-0000002584 00000 n 
-0000002690 00000 n 
-0000002802 00000 n 
-0000002912 00000 n 
-0000003023 00000 n 
-0000003131 00000 n 
+0000002553 00000 n 
+0000002659 00000 n 
+0000002771 00000 n 
+0000002881 00000 n 
+0000002992 00000 n 
+0000003100 00000 n 
 trailer
 <<
 /Size 14
@@ -107,5 +107,5 @@
 /Info 4 0 R
 >>
 startxref
-3483
+3452
 %%EOF
Index: docs/contributions.html
===================================================================
--- docs/contributions.html	(revision 916567)
+++ docs/contributions.html	(working copy)
@@ -186,9 +186,6 @@
 <a href="api/contrib-remote/index.html">Remote</a>
 </div>
 <div class="menuitem">
-<a href="api/contrib-snowball/index.html">Snowball</a>
-</div>
-<div class="menuitem">
 <a href="api/contrib-spatial/index.html">Spatial</a>
 </div>
 <div class="menuitem">
Index: docs/scoring.html
===================================================================
--- docs/scoring.html	(revision 916567)
+++ docs/scoring.html	(working copy)
@@ -186,9 +186,6 @@
 <a href="api/contrib-remote/index.html">Remote</a>
 </div>
 <div class="menuitem">
-<a href="api/contrib-snowball/index.html">Snowball</a>
-</div>
-<div class="menuitem">
 <a href="api/contrib-spatial/index.html">Spatial</a>
 </div>
 <div class="menuitem">
Index: docs/lucene-contrib/index.pdf
===================================================================
--- docs/lucene-contrib/index.pdf	(revision 916567)
+++ docs/lucene-contrib/index.pdf	(working copy)
@@ -5,10 +5,10 @@
 /Producer (FOP 0.20.5) >>
 endobj
 5 0 obj
-<< /Length 1067 /Filter [ /ASCII85Decode /FlateDecode ]
+<< /Length 1038 /Filter [ /ASCII85Decode /FlateDecode ]
  >>
 stream
-Gb!$G_/e6`&;KX9KoX7U<K\rg*1KWCkk*:P\$cNO"F+Bl,+B/?oNfRH06Cqh4]IfjJFK_+cP"!AmDF>Bi@,lnUl<N!'5$+,#p\aB-Oa=Obfk9.aCl,2AA0AImlp0fCnpF?<5]*u]DJn[</K>X^Ya@=b!tQPHh*BE5JtPsiMVekL6Jm<4G__Mk38VE>tB&&Y$1Lj1[%T!:#5Ii$"oV'E@+A7P1)+&qr9Y>>XdCh*8M:86ZrU@7K<C!rfr-@jpK5$(dp=/>Qt3DB660?0*3UVimTY7Nk^g]rSa?9\VOGQ;9W9CehIOk1Q"uO`\[!.GJ_@@LU2d`*I$H&`pjF:At*Y;T7,.W`Q`#`!!ZS_3oJ?oN8g0FJ&Ks*R:n%pgjtqV@_J@1l/kJ*Rs_mui3-#3lN8ib5pp1fQpZFX/93PApf1KVdoVelQ(E<\[hBb0nZZUqmU0N`/;hc[rQVG19*H9Nbp62k`6F;$o<N7!^%8U.9@"dtnePAe@1JG;M:g%oN8ohE<oc1pgNJp$f/S#a1#UFfW>K9#jG>$M$!Xg>o^N-E$0ZO8=$2?WS0]q#_f![2UP*JjB/_Qq5Rk`L5Xur/TplrOAM7YCfT00MPjkt[qRB1@m;onFo@su!UPRl2e&rRMG%-aS-"s\eDmqFJL,s.MUG:98XGWTqfT$haS=nj8gaMXDeq#/ioLta,/^gk3.qY2H6=!WI79UCSWDC)g*Z<0I'!YjmAs#qW]h;dhp.YB[e6jYdL^dQD98sSLSLTM5UU"N^.V;_)&AJ;abQcM"@4u5D;*qtJ]%WKWI9>Yr:WUtD$uQ[afWZ*sa=*`GjQ5+q]"Zmhh9[/.A?TCXmD<hZ+b.d]C72!6O-8G2/TNTR)15tu@)?cuE5J<sb>S-g&map(,ET`4U6:nuKAhKmD)&F^D!K,uR0aB&R,Cd3ac!9$gS-E80ob-[B^<%0iCK[X=\ZV**__PtFO,Zg7)Yce-#P=]SOJ^ci,j)cI]R]_D2JIEmJ;U"]Pe+rU:rM*p$h;DppY"g*)_d1Gr!eRqd<E%P\,<Y\REO];-UkHIoT.qU]1Nt<3S(~>
+Gb!$G9lldX&;KZQ'fnA\X-,_q*1KWCkk*<&Df$!fb;L"BNkfYi?f.-I@I6fU\Pso>EFa6h4KNf#9)$N^LW=q$O>=+Hc^=OLB8gY=S1s`s,W4IJ+eq>fOQDLMT7>X/q,Z=6KnpDP2udp><JB.bJ)>RNX&^$4GN:,SIYNtp_Z,RB0q=1Kj[%B*\'1F9gP%Jnk4O&q4,<6'eM_i*P&PTt+(rH4-ohasY!7rA`Ds9>a3auT2e5]l>P.r&k$L\n@'5U8`@/<Cjt[4@32=r"GS*bt&YE'+H2=@Zit(n.bXL#I4#(eJ=jJXrR8J?4[@V(c\J+,N*MsECHL>1()ok1B.c9YLrToa)9Q$01!uPPpl@Hie?kgBHp_$q@VU/52\V"VaL^eZ"jYSoRh\M\u$,<Uj?6=]+%XK:>?k*>QZ=o[E`NQVF?+d2MXnBO=^FaCqh-nQcgr%.\YUI6,%lZ;N0,;]^Rn9D`aYYr60<1lQoe*ONXZVMDa?*qk_AjgU'!ZPA)98)!]I[rrg3/bLju@TE([rhA.`U-iH"+il_G9K5IC]Gh!kKDg'g!"[ZUADAgl5HGPcu[,r$G#Fe6j7)F9H%g:/ceA*5mesq_:a'&mc&?TD"[jHnJL/c5gok6k%h=6,r]0'=5V*6*#uk]7iQjlEI%D>c<@E'a+-=*Ac2)6,WTT.du$@JL8mWdUm!I1i1s)=_<oFln_bH9(1,>l#rs/5DaJugl,BNPgJbs=lrqR5Pof(9ppY/*Pc&0ObA]51(0$RRY02aU%2qjH7S-<$t]TUfXMa(a=*`B(=gG)]Y7RLj>8^g8L.a)B$<T:KOaOWm+&,WI.+,OITkgN"dt"6*#)(?].faoXS&Sk^/%ET7G@h\I,lDW@bQRqKXY6tYbksiad&4p#CdK`drP.k%rUb@.VCJq@=)Mq/O=\/d>G#<?YB,q^fU0SL!`<,Tg\(W>Y:GSP,,>4pd=sUmn_[(R"p=L+%#Ol,oDPpee0&7?QU&5X4Q_ne**8k,E8o;eeLVfAN;U6fF;RGGjMVK70iJB-O,+m~>
 endstream
 endobj
 6 0 obj
@@ -44,7 +44,6 @@
 46 0 R
 48 0 R
 50 0 R
-52 0 R
 ]
 endobj
 8 0 obj
@@ -200,7 +199,7 @@
 38 0 obj
 << /Type /Annot
 /Subtype /Link
-/Rect [ 108.0 286.666 171.832 274.666 ]
+/Rect [ 108.0 286.666 159.832 274.666 ]
 /C [ 0 0 0 ]
 /Border [ 0 0 0 ]
 /A 39 0 R
@@ -210,7 +209,7 @@
 40 0 obj
 << /Type /Annot
 /Subtype /Link
-/Rect [ 108.0 268.466 159.832 256.466 ]
+/Rect [ 108.0 268.466 188.476 256.466 ]
 /C [ 0 0 0 ]
 /Border [ 0 0 0 ]
 /A 41 0 R
@@ -220,7 +219,7 @@
 42 0 obj
 << /Type /Annot
 /Subtype /Link
-/Rect [ 108.0 250.266 188.476 238.266 ]
+/Rect [ 108.0 250.266 171.16 238.266 ]
 /C [ 0 0 0 ]
 /Border [ 0 0 0 ]
 /A 43 0 R
@@ -230,7 +229,7 @@
 44 0 obj
 << /Type /Annot
 /Subtype /Link
-/Rect [ 108.0 232.066 171.16 220.066 ]
+/Rect [ 108.0 232.066 157.168 220.066 ]
 /C [ 0 0 0 ]
 /Border [ 0 0 0 ]
 /A 45 0 R
@@ -240,7 +239,7 @@
 46 0 obj
 << /Type /Annot
 /Subtype /Link
-/Rect [ 108.0 213.866 157.168 201.866 ]
+/Rect [ 108.0 213.866 175.828 201.866 ]
 /C [ 0 0 0 ]
 /Border [ 0 0 0 ]
 /A 47 0 R
@@ -250,7 +249,7 @@
 48 0 obj
 << /Type /Annot
 /Subtype /Link
-/Rect [ 108.0 195.666 175.828 183.666 ]
+/Rect [ 108.0 195.666 167.824 183.666 ]
 /C [ 0 0 0 ]
 /Border [ 0 0 0 ]
 /A 49 0 R
@@ -260,7 +259,7 @@
 50 0 obj
 << /Type /Annot
 /Subtype /Link
-/Rect [ 108.0 177.466 167.824 165.466 ]
+/Rect [ 108.0 177.466 211.804 165.466 ]
 /C [ 0 0 0 ]
 /Border [ 0 0 0 ]
 /A 51 0 R
@@ -268,287 +267,269 @@
 >>
 endobj
 52 0 obj
-<< /Type /Annot
-/Subtype /Link
-/Rect [ 108.0 159.266 211.804 147.266 ]
-/C [ 0 0 0 ]
-/Border [ 0 0 0 ]
-/A 53 0 R
-/H /I
->>
-endobj
-54 0 obj
 << /Length 1855 /Filter [ /ASCII85Decode /FlateDecode ]
  >>
 stream
 Gau0DgMZ%0&:O:S#lBhd#qbGFm8Go;[U!\cmD^_S1ju#C3p'T]?&FN'IXO`\OXrM4g*/dudN[jpB$SWa_d[eqR/Jlu8Iu*'D%rTPV8]?JL4Nd3#@mVNY9/a,BG/e`>R16iDXm[g\qJdtTt%@trA+8SR*BPtK,+tK<\r3LR)'7$7ucf4A&I>uZ.KtES5F<E>[$5HB:ndOVn?5@b>^!s4?*6r(8!(YBmJCP=pNe*J`emo3+5(N)tqd#%;JmXoMr+DEVrF]`L9T'n;0<6DREF6)io*M_3.K8WsJ?/,0eL'h:K>>Rr$+hIdmbtRuR<OPuLtjP36_$PKJ<26m-K\0Vrklhc_fY<c.5#j;gnRV0oj=^VJGTA>;BkjGkJ49<$P,Pf/#B?L-'2.ZRTbXjELem)1_%QP/m[Sq'GP_hV:)ThEV`lMXmu#?&.@&eAc;-"<,RcN'2#o*I#@qA=f9m,.J2?u<+c',ms^;FC7)('g',[Db*VhAp:[\bR8g^S@2ND-==O?Im#5C_eOh,]YQd#P@obNoT&$N>gm^`2ek!BcHc]ZhO-Fo348Ihl]V/kuNT-/^&"'5NDnRa\a'l#iR_>KZndd#_ee`@nJsq?^?_o[?,)l?I[3mJP/edbjLYg)N-pQLgD,DeMcQO_D+h2JR*ac!ST!%)iLdt%YY#q`,lj)[>@qNWVd's11c8*<1[j0jl7]#lu*6HT&ahD-.*OC7m%X[51a8,m,b9=FeN3%K3ra/8G?*HfL1ik8Y5VQ#'Wa\bfcZ@Ys`Va/s1V-VV,RX3*K`Bp_/`'5`t?rf^#m1:8[N&L9[eh0#j3Kbb_7=6+i0qlaq!t+5G>l#dsdNXAgAG;]bH["%p?dG_sr*6HLn'5*%A$#c0tS-`gC\#GqhB'N#>KNua^(VYQ=D7h#'lakD1p>F49#4]?@Oa,=Z:'T)&F>>ZoZI2q_%1-Vk%'=PHaep@(>52V9X+:OdSk!Z5)GuCYfq$j^#Iq[?A`kq2j;8gOSJ19\lWa2"UNZ?ej'E0[@h@Q4`[(BWoiT"9`Y.jr#L?L!qd7P[tJr)Ji]QZbeXnUcCLolunE^7hlC6K'?qr?tdb/TTW'0*6VL)(-.'F3'Uip^F,2D]/J54A)I[[)2jU4QrZN[kJ)"clckAK,`IV9c('4NbchckZ:]X;i@1rPR0PO#M7ri=W1p3XArg..O?Op4.E&:S?-q(D5Eng[tJNk3*S8B9F\.+b![fL'k[U@_1P\R0bG[E\&/@IG;0a<3F=jHV?0gP]tAT[]3JD?F!D,_7X[Hjoc3bs)LfA@72ub&[e!_?$eOZgEV-)D:p3,K4[Zo@kr]1kkX[bl8]iCD<_^sg?i9a&a)#MZ#'P(csa,7KBT<INCr"nDKmW8S%Y6W<[@X#Xr"^2_fcu^GA?tI+O'G/TLXX`K)*H\A*JYOenc):a"mK%Z*lb%m^#^0g4J8cHJf!CX,"*S7+lTsid.MSCtU//Sl5s-lK'24:`A?9)(?'?aE.Y:T\Fm`!X61#Y`UjE7K9!!cl`HY:Ig'670d<[B\3TM,n(g[/Q8-19W/K4$3XL)L$X:(.+IpH%os[aO?t/'U6:NF>)+c'[$8[3d^@^##4`q?0U(FeTd<ECW7<<BMd`=C^CG7]N^MUA,QLmkCmjpm"1=>MFT\.F"Q1il)$_i'I*_9_->J6W`0OY7>iY]HOIh06DRV6*H=5ga@e$%qdbFsZg_rMC5[jeHWs1jN<7;$Dm(bNr6JG\eTBg*jQ"M6rWEDXiKWO:QCtcQDV613:iYm@M#1J<^GL,CJWUdcrCqbHS\s6iq?FQqeVck3UQ0UNBdee@CX*/Al%ZhHg7*r2R(oeA-]<Q\,Tso@/0rgE:!"oZS'*~>
 endstream
 endobj
-55 0 obj
+53 0 obj
 << /Type /Page
 /Parent 1 0 R
 /MediaBox [ 0 0 612 792 ]
 /Resources 3 0 R
-/Contents 54 0 R
+/Contents 52 0 R
 >>
 endobj
-56 0 obj
+54 0 obj
 << /Length 1328 /Filter [ /ASCII85Decode /FlateDecode ]
  >>
 stream
 Gb!#[gN)%,&:N/3nBmi,&6oP2d<=eK3pfV0-r3PZ+;0jan5^*`l[F8uRgD&bD5F"qN^aW^E3X'MS6^`US(3RheXn?+mG7Nm.Y.RhE?)Stb1c/=,6DhE,,:n"lIXSRMZt"!m+qX&Sj`psL!=K=4!WrQ.-_6Z"u2q$VMK]5qP;WK1[fLtg8Znk'[hB"VZ)Z)R8rKooR!t@WCHu?8-C28qbIf=ru"Ku'UaBV]_M#&1uK\tL_2S?!&>00k7Jn$ndm[uJ0WjE7Y$'sA%)'hKh6:p39?nsM?.!R;s4W65T&dWI5'2CB<%@>lI)]GO]GQF2#3g3@2drOo/0U1e&:!F0VAh:0`_:n4U#B';-f1c(.1?a`4VZHjpWq:7NRr;_a3`%\bfQA(("E0!h@VY$"6rC+n]/[-C9^te"GDhf4ck,`oh/>R2Mg3geY81[-8a$EKLq9n%NFK;NGdlRU^6)H'0<UnVB@O3CqibfVX80CZi#u\MJp/aEHpT]p"F]T2]PUZ[NAT1erRoBZ(HD6=7VmAMKjB%Es/%aP;%k>nN[Xs&]f'm6Op*3S-Hb[Zc(h*(gOQA+5K$[)t]he#6oRnHdhgFRt+4;X$WSp$u3)rE*7'I5;[3l&U2nX=Q/AZ<>44bf33/mhhg7C6`P%kDs%d`p6lu^#s71<M%Ie*/.-qj0h7u/IM5mog_F0@RTD2+5m8Hmae*2\4<_tO,LD83!>(_HHIuIin)Zu@2uKSs+eYOqn(2'i60[JEi@BcPkh\*<u#jXWmWNMj6'=<<MmK80<@+`<97G7CGD^i67F$e<)/um5YpnAhp3K^NceVF40'uc#sdOBJ\!mG/uU$d!"n+.I_ppt*-ENS*L1Kd=tOUG>S[Hr"TsF^Es77X!XT_S$Qj;9=`7lH!Wk.Mf2@N+V;14Irr0S[oaNlsf0Q$LrnhJ>ebu6R_/"hK=K,pp0kTiA5"<t6HgJ[+hg-trAMi6_MBg48j]$gDGA,ZRW2)SU\7=KjPJkpAIS=gr-kMUg+;Xb-EublJ^`o@q]%;)-dq9C5lGGb2WWu<@59Eg(W^UnBZIJP%J%"Ia</'eVZVRX=fjsDRQYMh!'Jf_Zn,^j&XtE3uU<M^.T2.-bbV_"6&r`L(E1r/l1cgk>mVsKlM9e-=DHhc!ESo8$m8N/mM$]6Jcbe/\qP>/!WmVe+]Z%9sXZ^'i83SN6IJ%9Kgl51*=c<XjJe&DWaF392GQB#@VoU/$F*C`RhNX.<ECn@u6!^k8le,QD5FV;Md-W3l7iZ,S)IWp3Bs\a5dg`r`FlQ9Jc9=eZc.lAjGB.HK#s>OVT*CkT`Z(sJR?Pe/Pdl2CK.k9`~>
 endstream
 endobj
-57 0 obj
+55 0 obj
 << /Type /Page
 /Parent 1 0 R
 /MediaBox [ 0 0 612 792 ]
 /Resources 3 0 R
-/Contents 56 0 R
+/Contents 54 0 R
 >>
 endobj
-58 0 obj
-<< /Length 1316 /Filter [ /ASCII85Decode /FlateDecode ]
+56 0 obj
+<< /Length 1312 /Filter [ /ASCII85Decode /FlateDecode ]
  >>
 stream
-Gau1.997gc&BF8='Kd^-c1!/Za(3Rl>I6YYorZJiQ=ubi*f_<=kbl`tZ:*-G',ub4=PK(YRGn)C*0/mX(;I!f?PX>miW&j#+deoNMVqFSrqB<2Nb(m]D:!V!EUiIP'*Q4iHhu"#7$PNIeRh+%8Hf#@)CJ.3,9kL75_$QFc#.aK]r<qj?0F%$YL/LB-=8X=gu`4:<G>O%Z?8)5D+eeXMQ2,F@p<R-)8AY5dHZc;^RSafq0asZ]T%F6UB\I<,K%q9<f&"?p"tALr`bOR+]NC-q4"#5UI2=8`R0o$.PQuI86:`u+4CsZ9IQ3kN?M4/7_GC2'1<\QOU2"%n]Xp`pX=?gIo6kP,ourtFhSo>D,'>-W7loUQorusWOg)ThUpU?^\D3b]ZU;T]C`?aoudSG2l?;@e7IcfC3QQKmcCRM9AOpdrp3N]HK-j69lio,&epg/X0]c/2ncG;CTFa$j1VRp]K)r<.3sp$A"f=q0XoMoA.g8MmH&)G<._Ac<YP)SoVG'_+&JID'oDg\Xqd3m";\4n@o8NRkrcm-*]KT5S&H)go<e+rZg#c='lCN<*aMU+=(5LO_"28?#7m\3D,,pQ.#Q]b1f)A6k.5>Vq\rAC-T*R(_1N\<?*8V/fBLSh->bjEZ*n\f9rZaT+*,u?<FPnL&p.$f,(e,ipcuZH(iC^3+UDUS>VBX(@3UP?+`gH#[Fq#3ZP"%4@4(1Aa&_Dc.'/)DS?8uo-eis1dog;[SEp*BTE8*tPq^a8]s]pp1oJM+=##`4#Qe*u[XUIf*l5:/3P-7EQSF^q5%>3+VP7hM=\`3I3*,<J:7OhA-\q<aY!3/C^$fg)S]UVDe2?,D:I;^drqcRM+=OC7Ed\((>jC\^Ys`'hZ?8/NVI8E-7s]QOZ?f^?4?lJp`A-L[?;$)C*%1Z'#(<Gp19HEsF:-lmqZo&74He%AjbGk=3UYH478RNgZTMiH&g2fQ`)C1I^ucRh2a@Y&.<4Ub![4@B5rUJ`N-pG.W;cWT%#MblZ"k6*"<(th?+nFuNFXpdS(aOAcYV'b0*,kEpsNiU\qR_@Oro\B:mdsaq+YS5*BjgbL!/Q=paDY`4:sB`35>c927,%$IreQ^k+3_"PWU`'S>eJ71"`DjT\m5*SW4Wm[V6@EbV*c'V<*aZ\?!c\[d,&5+J:H0dT?1\MBVYu?g_N0/j0\C6?laTPk<?uqd48/J&5D:-"FM/fdqlAcb2bFkWG&"+sgX5LCX6bg:u/,/ifjZ<djiD@?S26,:%<)eKHrj^e<64I],0lPHMSLpJk:,;^XFu)_8cb(jn:!fKf'#BJSpppD(n8ML>~>
+Gau1.997gc&BF8='Kd^-Qf1l;Nle#b[V14di`k.>/^ZI\=`Pp)+8tgPad,K/Eaf,.$pQ.s1W0%e3?3NN/q.%X^+Lhf_#*.9,FG,L8`3OgrqDuE*6XQjB@(D`C%:/o'*Q2sGl$6K7$PNIja'4I8H`?6*@=C5'-beDLOOaEPML'Dp;X4M\'n>phP*(iR!dB8C/^S5U<^BAWEOd]Vem0-bcZ,mIVc(%]\p9_:UL&]`%S#jN./MYF5FoDqFQ=sUQK!L"KP3.Nr!Z[C&]bVV@[beFT1FcO6Dr^8oZU5M[E-'O4':;aq*`o6REL?apoRn$q>iAD%f>T_3#T8'^<P7K(jOQO[#fNVNN"u^%]74['QV=1/tI;5YP>Gb1'_W0<]5,RNFjMH.`o4eR/1&)>F!+?"7.7UooTj;nFn>\6$,Y<OrY'rS$A1$H@@jRq>Cq&epgpX0]EdBmO4Pf%*iB`d:qAAM^A6.3qYYA$MI,0XoMoA.iOHmH&)G<._Ak;A0GfoX.2o+&JID'hS:qXqd5C";\4n;c-QWq)lS=*^:VhS&?#g$afN<[J;_^'lCZB!F8N#:h!bI^rsBjUBgDER5kib755aJ(9#p4>]jmsIgo!\U_X(oGjqK&.s37eQZ$_Vo!SoJCMWt)<-hQ!+a6Xbn*E8+Jn>`K')3H!Nt`RjiH@O'JjB%Jog0E6-B_mP1(T')bf7;cQ*=X)1I3ZR[UP"[@3q`4CdUo[gGY.`/dg@r>aPHs.T0hW$$3f3.pKkY1LG%08oZUU&A^E4rVpBe$#Fs\Zcb(CR6lbk'.+<"g=<;cbDD,cVb17<M=UlAm!)jnpZFfKpOhk=2<E\U]&jsp8W<E8Da2PMK&TBmf-c(Y9>)>hQ:];K[OrrG'MP%laFI'mLVhG3p^A:2VO2FM/Kg?uSFPr\5VWAI?JF7)9jqJ"2XVYg\hV#mhUJ]^/N:k5629[]<Q&cI_[YQj8]lE-YCHiq<mV)1plYn)FC]@]jaVd?4D.&iV:`0aBa0!m3`b-.=$P@^L'TA0cj&'=0!)#J74hTiD)S7OC!'oaLE/F6P,<1TN5;'X$r_1.SeMS@T@M,1;E,/3-75<uaKorjmeSaLEP?d-8-,j.KE+Ll9i)2*.6D;?rEeZB4pJH2Q6n/aDUYGomlbVs=08g0-qPPW6e%nbo#K9ImUa/_osdtI)Q&2W\Deq&UHM*BW&LtG&M0-/6`M9V/;42Qs-]\0J7aX+9#-%>NVIq8Is(7IEi9DGl;RE^PLPX1+c.9mO3&7D6*c78Q[7IOCDU6$kRia_eMOBZrF#-N"ChNse<;>cceZ@,AW>(~>
 endstream
 endobj
-59 0 obj
+57 0 obj
 << /Type /Page
 /Parent 1 0 R
 /MediaBox [ 0 0 612 792 ]
 /Resources 3 0 R
-/Contents 58 0 R
+/Contents 56 0 R
 >>
 endobj
-60 0 obj
-<< /Length 1045 /Filter [ /ASCII85Decode /FlateDecode ]
+58 0 obj
+<< /Length 952 /Filter [ /ASCII85Decode /FlateDecode ]
  >>
 stream
-Gau1-9lo#B&;KZO$6>T<MS<XDgJ!g!:-+Rs*'2RBTR\_2^iG,8Tte=INKKHm-?RsM!3^6bfC-'"h05(1qoS'b#_U!DY=:O(#RDUO0B:=IQluZfi4s&G2u5@Wp[S@e=`jE6,mTh4$^:Al[>U.H@*gLd":E)_A-K]]\T.>Zbk1`?N(NoF2mp':aIHHNN-1t`bNV>hmnHr<e#Jp,pM>^RYA7XkkUB_707n>[RHd\f1[lDY5!0.3@`\E(DRCPVXeU]$W!>2de,?%FPn;11$rPCL1Q7fQNq*SY=+K<I5@8uTXr'ORl\ZQJS0lBNJ8(ot)E>TqA'tO@+jOIHM0!jhL_QRW=RqtKcK$Kh-9c^0+Qg3*q)8$F[*`c8UYIn2Wa;Vb%]QJ0X(1RU8F5)'Pu/$iQ!*O,D<KmQVtXc^r)#1lO)4S`jn+]51$.1Q#.(\EC6E9b><'r:h@#EWb<b_!QRSHe&-lsRcP92qIqsMSPs!?WD7d39,mqI"jm;%r5nXMHH`,=Nft:7qWbeZX3#?s.0L_sREE.f$e[f_"epjs>=XG7Q7$8*S_ts,;QnOgNB_QgXN;J<"oXG0YnVc:o9$`GWMN"eK$AM>F`[HUqn]pC[%lJWJ?!<4d(/]U5\-H-e_gEA*HFF3QJHu.#7PncWXmtY7BZ>EKU^'@+-48!i>ot@\V==.=4-^?l"Nm5C&rO0A68,BID/-C6^[sd5LQ/MgD7Nlacom0B;<E4%+&NC6WK[@4c>,%bMC5(g%9FUdOU0b&C,p2.bALSPS(TNJLdF]/W<A(XV)84Ar],u8UXL\BQ-VMNesY61\@@(/]e:@Tq08HHns(-MUdY!te&S"uI]h<WnFK9S1,%'S7AM;a8Zt[]VVP&k--W2.+nq^0j-OYnnE%ZX]-Poh][e1uNCij!&1$Ya47BH5_&.TWaQ,P')c`]ZGk?S)m3c;/&a=(7R"I-V0_b0];`)REI(6E2cC&.h6C(cLff-cfYR!Q>=HgERXV!#4g:)63Dj?7+>W0Jj5KFh1Fjj3#[(b.SXZaE(4bH&1hr#g'!r~>
+Gatn%9iKe#&;KZL'fqXJKYD:FJLSfbkR<[q3n*dF&iig//>G1Xo[d/flpq2UO:IIGijC?)qL*Op3hM?p:I^Bi%-Hu)29HIu5R?=t)/l[uPS-M@!EOdKdI$/qbmB=o(-I"kaT3`+@$6RI3siG.#.o_^5_0c>/A[BCB.%(\g,2(MY*Ii"8@qdNhlIX"p=<ta<Q<UjE*R\KY*,WjEk0]=f%VPGf8n4D<l$gP\DF0Y(`oYmi(]LnDs/t.ce^3I/Sf2qGQN>Fad3eF:^gJ[^n"B^qcR^ciOZecPFS%T[nKEq_)+ds"jK9L\G>;99*)8<E-Um,($N/9$q8JO1Pe#Ma<^i8*P".CKHYOgd0>RR:14_-7[N^#`$eK-7NlEd4&RAFaAW3ra.5ga&&#l[=]VD!AXEc[[F4YE"kqKRjAW+iQ$"5C7b8#d?*C0+<.hhC:LU\m,iTPop$o7Gbol`6]cS%<R<pdFj2td)iX`Uu1G'+^,8d/6*K:IR:q6:)V\<Qt!)IKaWnnTi&1JF%#QjY:*Pes%jMOjr=mn9.1U,6H1Lb5H'OIm81Lf-67&"<^&<%<lQ)[#=Lp7X$L6!Y!.#3KG5sf"j/6#JlCL-6ms%rmGm0d?,RW?#FrO]WFiAGrS&]O6G3r+#B1h08jHosHY1:K9k4ZXRUHX7KJVS?rf<S,G)0VjPEqe1:,6Ri@81@d[.EuOaBX(8G;^?ULnX(tdWi>*@*5(EP0lZH1h90$7SPdq:P`uU:kXk5>udn9a?-dkXV'#t9ADR2F*jE,So8.q:9K)"[?DN3P"i50=j<o6Ehk2<ZHYhS=RhJkoEn-3+RA&EpR\kGb\0#S\hO.Gh!"Yo;f9e[lM.h$oR_OSM;E7!Yel.XO=WO>jR(G4\nD)D,]3;++R89epu.FT2d;QY@[np2NtE9FMeN:[@CrNtDFBP4]d=+RIn^F(mk?F[c#SR<<~>
 endstream
 endobj
-61 0 obj
+59 0 obj
 << /Type /Page
 /Parent 1 0 R
 /MediaBox [ 0 0 612 792 ]
 /Resources 3 0 R
-/Contents 60 0 R
+/Contents 58 0 R
 >>
 endobj
-63 0 obj
+61 0 obj
 <<
  /Title (\376\377\0\61\0\40\0\114\0\165\0\143\0\145\0\156\0\145\0\40\0\103\0\157\0\156\0\164\0\162\0\151\0\142)
- /Parent 62 0 R
- /First 64 0 R
- /Last 85 0 R
- /Count -22
+ /Parent 60 0 R
+ /First 62 0 R
+ /Last 82 0 R
+ /Count -21
  /A 9 0 R
 >> endobj
-64 0 obj
+62 0 obj
 <<
  /Title (\376\377\0\61\0\56\0\61\0\40\0\141\0\156\0\141\0\154\0\171\0\172\0\145\0\162\0\163)
- /Parent 63 0 R
- /Next 65 0 R
+ /Parent 61 0 R
+ /Next 63 0 R
  /A 11 0 R
 >> endobj
-65 0 obj
+63 0 obj
 <<
  /Title (\376\377\0\61\0\56\0\62\0\40\0\141\0\156\0\164)
- /Parent 63 0 R
+ /Parent 61 0 R
+ /Prev 62 0 R
+ /Next 64 0 R
+ /A 13 0 R
+>> endobj
+64 0 obj
+<<
+ /Title (\376\377\0\61\0\56\0\63\0\40\0\142\0\145\0\156\0\143\0\150\0\155\0\141\0\162\0\153)
+ /Parent 61 0 R
+ /Prev 63 0 R
+ /Next 65 0 R
+ /A 15 0 R
+>> endobj
+65 0 obj
+<<
+ /Title (\376\377\0\61\0\56\0\64\0\40\0\144\0\142)
+ /Parent 61 0 R
  /Prev 64 0 R
  /Next 66 0 R
- /A 13 0 R
+ /A 17 0 R
 >> endobj
 66 0 obj
 <<
- /Title (\376\377\0\61\0\56\0\63\0\40\0\142\0\145\0\156\0\143\0\150\0\155\0\141\0\162\0\153)
- /Parent 63 0 R
+ /Title (\376\377\0\61\0\56\0\65\0\40\0\150\0\151\0\147\0\150\0\154\0\151\0\147\0\150\0\164\0\145\0\162)
+ /Parent 61 0 R
  /Prev 65 0 R
  /Next 67 0 R
- /A 15 0 R
+ /A 19 0 R
 >> endobj
 67 0 obj
 <<
- /Title (\376\377\0\61\0\56\0\64\0\40\0\144\0\142)
- /Parent 63 0 R
+ /Title (\376\377\0\61\0\56\0\66\0\40\0\146\0\141\0\163\0\164\0\55\0\166\0\145\0\143\0\164\0\157\0\162\0\55\0\150\0\151\0\147\0\150\0\154\0\151\0\147\0\150\0\164\0\145\0\162)
+ /Parent 61 0 R
  /Prev 66 0 R
  /Next 68 0 R
- /A 17 0 R
+ /A 21 0 R
 >> endobj
 68 0 obj
 <<
- /Title (\376\377\0\61\0\56\0\65\0\40\0\150\0\151\0\147\0\150\0\154\0\151\0\147\0\150\0\164\0\145\0\162)
- /Parent 63 0 R
+ /Title (\376\377\0\61\0\56\0\67\0\40\0\151\0\143\0\165)
+ /Parent 61 0 R
  /Prev 67 0 R
  /Next 69 0 R
- /A 19 0 R
+ /A 23 0 R
 >> endobj
 69 0 obj
 <<
- /Title (\376\377\0\61\0\56\0\66\0\40\0\146\0\141\0\163\0\164\0\55\0\166\0\145\0\143\0\164\0\157\0\162\0\55\0\150\0\151\0\147\0\150\0\154\0\151\0\147\0\150\0\164\0\145\0\162)
- /Parent 63 0 R
+ /Title (\376\377\0\61\0\56\0\70\0\40\0\151\0\156\0\163\0\164\0\141\0\156\0\164\0\151\0\141\0\164\0\145\0\144)
+ /Parent 61 0 R
  /Prev 68 0 R
  /Next 70 0 R
- /A 21 0 R
+ /A 25 0 R
 >> endobj
 70 0 obj
 <<
- /Title (\376\377\0\61\0\56\0\67\0\40\0\151\0\143\0\165)
- /Parent 63 0 R
+ /Title (\376\377\0\61\0\56\0\71\0\40\0\154\0\165\0\143\0\154\0\151)
+ /Parent 61 0 R
  /Prev 69 0 R
  /Next 71 0 R
- /A 23 0 R
+ /A 27 0 R
 >> endobj
 71 0 obj
 <<
- /Title (\376\377\0\61\0\56\0\70\0\40\0\151\0\156\0\163\0\164\0\141\0\156\0\164\0\151\0\141\0\164\0\145\0\144)
- /Parent 63 0 R
+ /Title (\376\377\0\61\0\56\0\61\0\60\0\40\0\155\0\145\0\155\0\157\0\162\0\171)
+ /Parent 61 0 R
  /Prev 70 0 R
  /Next 72 0 R
- /A 25 0 R
+ /A 29 0 R
 >> endobj
 72 0 obj
 <<
- /Title (\376\377\0\61\0\56\0\71\0\40\0\154\0\165\0\143\0\154\0\151)
- /Parent 63 0 R
+ /Title (\376\377\0\61\0\56\0\61\0\61\0\40\0\155\0\151\0\163\0\143)
+ /Parent 61 0 R
  /Prev 71 0 R
  /Next 73 0 R
- /A 27 0 R
+ /A 31 0 R
 >> endobj
 73 0 obj
 <<
- /Title (\376\377\0\61\0\56\0\61\0\60\0\40\0\155\0\145\0\155\0\157\0\162\0\171)
- /Parent 63 0 R
+ /Title (\376\377\0\61\0\56\0\61\0\62\0\40\0\161\0\165\0\145\0\162\0\171\0\160\0\141\0\162\0\163\0\145\0\162)
+ /Parent 61 0 R
  /Prev 72 0 R
  /Next 74 0 R
- /A 29 0 R
+ /A 33 0 R
 >> endobj
 74 0 obj
 <<
- /Title (\376\377\0\61\0\56\0\61\0\61\0\40\0\155\0\151\0\163\0\143)
- /Parent 63 0 R
+ /Title (\376\377\0\61\0\56\0\61\0\63\0\40\0\162\0\145\0\147\0\145\0\170)
+ /Parent 61 0 R
  /Prev 73 0 R
  /Next 75 0 R
- /A 31 0 R
+ /A 35 0 R
 >> endobj
 75 0 obj
 <<
- /Title (\376\377\0\61\0\56\0\61\0\62\0\40\0\161\0\165\0\145\0\162\0\171\0\160\0\141\0\162\0\163\0\145\0\162)
- /Parent 63 0 R
+ /Title (\376\377\0\61\0\56\0\61\0\64\0\40\0\162\0\145\0\155\0\157\0\164\0\145)
+ /Parent 61 0 R
  /Prev 74 0 R
  /Next 76 0 R
- /A 33 0 R
+ /A 37 0 R
 >> endobj
 76 0 obj
 <<
- /Title (\376\377\0\61\0\56\0\61\0\63\0\40\0\162\0\145\0\147\0\145\0\170)
- /Parent 63 0 R
+ /Title (\376\377\0\61\0\56\0\61\0\65\0\40\0\163\0\160\0\141\0\164\0\151\0\141\0\154)
+ /Parent 61 0 R
  /Prev 75 0 R
  /Next 77 0 R
- /A 35 0 R
+ /A 39 0 R
 >> endobj
 77 0 obj
 <<
- /Title (\376\377\0\61\0\56\0\61\0\64\0\40\0\162\0\145\0\155\0\157\0\164\0\145)
- /Parent 63 0 R
+ /Title (\376\377\0\61\0\56\0\61\0\66\0\40\0\163\0\160\0\145\0\154\0\154\0\143\0\150\0\145\0\143\0\153\0\145\0\162)
+ /Parent 61 0 R
  /Prev 76 0 R
  /Next 78 0 R
- /A 37 0 R
+ /A 41 0 R
 >> endobj
 78 0 obj
 <<
- /Title (\376\377\0\61\0\56\0\61\0\65\0\40\0\163\0\156\0\157\0\167\0\142\0\141\0\154\0\154)
- /Parent 63 0 R
+ /Title (\376\377\0\61\0\56\0\61\0\67\0\40\0\163\0\165\0\162\0\162\0\157\0\165\0\156\0\144)
+ /Parent 61 0 R
  /Prev 77 0 R
  /Next 79 0 R
- /A 39 0 R
+ /A 43 0 R
 >> endobj
 79 0 obj
 <<
- /Title (\376\377\0\61\0\56\0\61\0\66\0\40\0\163\0\160\0\141\0\164\0\151\0\141\0\154)
- /Parent 63 0 R
+ /Title (\376\377\0\61\0\56\0\61\0\70\0\40\0\163\0\167\0\151\0\156\0\147)
+ /Parent 61 0 R
  /Prev 78 0 R
  /Next 80 0 R
- /A 41 0 R
+ /A 45 0 R
 >> endobj
 80 0 obj
 <<
- /Title (\376\377\0\61\0\56\0\61\0\67\0\40\0\163\0\160\0\145\0\154\0\154\0\143\0\150\0\145\0\143\0\153\0\145\0\162)
- /Parent 63 0 R
+ /Title (\376\377\0\61\0\56\0\61\0\71\0\40\0\167\0\151\0\153\0\151\0\160\0\145\0\144\0\151\0\141)
+ /Parent 61 0 R
  /Prev 79 0 R
  /Next 81 0 R
- /A 43 0 R
+ /A 47 0 R
 >> endobj
 81 0 obj
 <<
- /Title (\376\377\0\61\0\56\0\61\0\70\0\40\0\163\0\165\0\162\0\162\0\157\0\165\0\156\0\144)
- /Parent 63 0 R
+ /Title (\376\377\0\61\0\56\0\62\0\60\0\40\0\167\0\157\0\162\0\144\0\156\0\145\0\164)
+ /Parent 61 0 R
  /Prev 80 0 R
  /Next 82 0 R
- /A 45 0 R
+ /A 49 0 R
 >> endobj
 82 0 obj
 <<
- /Title (\376\377\0\61\0\56\0\61\0\71\0\40\0\163\0\167\0\151\0\156\0\147)
- /Parent 63 0 R
+ /Title (\376\377\0\61\0\56\0\62\0\61\0\40\0\170\0\155\0\154\0\55\0\161\0\165\0\145\0\162\0\171\0\55\0\160\0\141\0\162\0\163\0\145\0\162)
+ /Parent 61 0 R
  /Prev 81 0 R
- /Next 83 0 R
- /A 47 0 R
->> endobj
-83 0 obj
-<<
- /Title (\376\377\0\61\0\56\0\62\0\60\0\40\0\167\0\151\0\153\0\151\0\160\0\145\0\144\0\151\0\141)
- /Parent 63 0 R
- /Prev 82 0 R
- /Next 84 0 R
- /A 49 0 R
->> endobj
-84 0 obj
-<<
- /Title (\376\377\0\61\0\56\0\62\0\61\0\40\0\167\0\157\0\162\0\144\0\156\0\145\0\164)
- /Parent 63 0 R
- /Prev 83 0 R
- /Next 85 0 R
  /A 51 0 R
 >> endobj
-85 0 obj
-<<
- /Title (\376\377\0\61\0\56\0\62\0\62\0\40\0\170\0\155\0\154\0\55\0\161\0\165\0\145\0\162\0\171\0\55\0\160\0\141\0\162\0\163\0\145\0\162)
- /Parent 63 0 R
- /Prev 84 0 R
- /A 53 0 R
->> endobj
-86 0 obj
+83 0 obj
 << /Type /Font
 /Subtype /Type1
 /Name /F3
 /BaseFont /Helvetica-Bold
 /Encoding /WinAnsiEncoding >>
 endobj
-87 0 obj
+84 0 obj
 << /Type /Font
 /Subtype /Type1
 /Name /F5
 /BaseFont /Times-Roman
 /Encoding /WinAnsiEncoding >>
 endobj
-88 0 obj
+85 0 obj
 << /Type /Font
 /Subtype /Type1
 /Name /F1
 /BaseFont /Helvetica
 /Encoding /WinAnsiEncoding >>
 endobj
-89 0 obj
+86 0 obj
 << /Type /Font
 /Subtype /Type1
 /Name /F2
 /BaseFont /Helvetica-Oblique
 /Encoding /WinAnsiEncoding >>
 endobj
-90 0 obj
+87 0 obj
 << /Type /Font
 /Subtype /Type1
 /Name /F7
@@ -558,262 +539,253 @@
 1 0 obj
 << /Type /Pages
 /Count 5
-/Kids [6 0 R 55 0 R 57 0 R 59 0 R 61 0 R ] >>
+/Kids [6 0 R 53 0 R 55 0 R 57 0 R 59 0 R ] >>
 endobj
 2 0 obj
 << /Type /Catalog
 /Pages 1 0 R
- /Outlines 62 0 R
+ /Outlines 60 0 R
  /PageMode /UseOutlines
  >>
 endobj
 3 0 obj
 << 
-/Font << /F3 86 0 R /F5 87 0 R /F1 88 0 R /F2 89 0 R /F7 90 0 R >> 
+/Font << /F3 83 0 R /F5 84 0 R /F1 85 0 R /F2 86 0 R /F7 87 0 R >> 
 /ProcSet [ /PDF /ImageC /Text ] >> 
 endobj
 9 0 obj
 <<
 /S /GoTo
-/D [55 0 R /XYZ 85.0 659.0 null]
+/D [53 0 R /XYZ 85.0 659.0 null]
 >>
 endobj
 11 0 obj
 <<
 /S /GoTo
-/D [55 0 R /XYZ 85.0 339.066 null]
+/D [53 0 R /XYZ 85.0 339.066 null]
 >>
 endobj
 13 0 obj
 <<
 /S /GoTo
-/D [55 0 R /XYZ 85.0 266.613 null]
+/D [53 0 R /XYZ 85.0 266.613 null]
 >>
 endobj
 15 0 obj
 <<
 /S /GoTo
-/D [55 0 R /XYZ 85.0 194.16 null]
+/D [53 0 R /XYZ 85.0 194.16 null]
 >>
 endobj
 17 0 obj
 <<
 /S /GoTo
-/D [57 0 R /XYZ 85.0 637.8 null]
+/D [55 0 R /XYZ 85.0 637.8 null]
 >>
 endobj
 19 0 obj
 <<
 /S /GoTo
-/D [57 0 R /XYZ 85.0 565.347 null]
+/D [55 0 R /XYZ 85.0 565.347 null]
 >>
 endobj
 21 0 obj
 <<
 /S /GoTo
-/D [57 0 R /XYZ 85.0 492.894 null]
+/D [55 0 R /XYZ 85.0 492.894 null]
 >>
 endobj
 23 0 obj
 <<
 /S /GoTo
-/D [57 0 R /XYZ 85.0 394.041 null]
+/D [55 0 R /XYZ 85.0 394.041 null]
 >>
 endobj
 25 0 obj
 <<
 /S /GoTo
-/D [57 0 R /XYZ 85.0 308.388 null]
+/D [55 0 R /XYZ 85.0 308.388 null]
 >>
 endobj
 27 0 obj
 <<
 /S /GoTo
-/D [57 0 R /XYZ 85.0 222.735 null]
+/D [55 0 R /XYZ 85.0 222.735 null]
 >>
 endobj
 29 0 obj
 <<
 /S /GoTo
-/D [57 0 R /XYZ 85.0 150.282 null]
+/D [55 0 R /XYZ 85.0 150.282 null]
 >>
 endobj
 31 0 obj
 <<
 /S /GoTo
-/D [59 0 R /XYZ 85.0 616.6 null]
+/D [57 0 R /XYZ 85.0 616.6 null]
 >>
 endobj
 33 0 obj
 <<
 /S /GoTo
-/D [59 0 R /XYZ 85.0 530.947 null]
+/D [57 0 R /XYZ 85.0 530.947 null]
 >>
 endobj
 35 0 obj
 <<
 /S /GoTo
-/D [59 0 R /XYZ 85.0 445.294 null]
+/D [57 0 R /XYZ 85.0 445.294 null]
 >>
 endobj
 37 0 obj
 <<
 /S /GoTo
-/D [59 0 R /XYZ 85.0 372.841 null]
+/D [57 0 R /XYZ 85.0 372.841 null]
 >>
 endobj
 39 0 obj
 <<
 /S /GoTo
-/D [59 0 R /XYZ 85.0 300.388 null]
+/D [57 0 R /XYZ 85.0 300.388 null]
 >>
 endobj
 41 0 obj
 <<
 /S /GoTo
-/D [59 0 R /XYZ 85.0 227.935 null]
+/D [57 0 R /XYZ 85.0 227.935 null]
 >>
 endobj
 43 0 obj
 <<
 /S /GoTo
-/D [59 0 R /XYZ 85.0 155.482 null]
+/D [57 0 R /XYZ 85.0 155.482 null]
 >>
 endobj
 45 0 obj
 <<
 /S /GoTo
-/D [61 0 R /XYZ 85.0 616.6 null]
+/D [59 0 R /XYZ 85.0 616.6 null]
 >>
 endobj
 47 0 obj
 <<
 /S /GoTo
-/D [61 0 R /XYZ 85.0 544.147 null]
+/D [59 0 R /XYZ 85.0 544.147 null]
 >>
 endobj
 49 0 obj
 <<
 /S /GoTo
-/D [61 0 R /XYZ 85.0 471.694 null]
+/D [59 0 R /XYZ 85.0 471.694 null]
 >>
 endobj
 51 0 obj
 <<
 /S /GoTo
-/D [61 0 R /XYZ 85.0 399.241 null]
+/D [59 0 R /XYZ 85.0 399.241 null]
 >>
 endobj
-53 0 obj
+60 0 obj
 <<
-/S /GoTo
-/D [61 0 R /XYZ 85.0 326.788 null]
->>
-endobj
-62 0 obj
-<<
- /First 63 0 R
- /Last 63 0 R
+ /First 61 0 R
+ /Last 61 0 R
 >> endobj
 xref
-0 91
+0 88
 0000000000 65535 f 
-0000015450 00000 n 
-0000015536 00000 n 
-0000015628 00000 n 
+0000015010 00000 n 
+0000015096 00000 n 
+0000015188 00000 n 
 0000000015 00000 n 
 0000000071 00000 n 
-0000001230 00000 n 
-0000001350 00000 n 
-0000001529 00000 n 
-0000015751 00000 n 
-0000001664 00000 n 
-0000015814 00000 n 
-0000001801 00000 n 
-0000015880 00000 n 
-0000001938 00000 n 
-0000015946 00000 n 
-0000002075 00000 n 
-0000016011 00000 n 
-0000002210 00000 n 
-0000016075 00000 n 
-0000002347 00000 n 
-0000016141 00000 n 
-0000002484 00000 n 
-0000016207 00000 n 
-0000002621 00000 n 
-0000016273 00000 n 
-0000002758 00000 n 
-0000016339 00000 n 
-0000002895 00000 n 
-0000016405 00000 n 
-0000003032 00000 n 
-0000016471 00000 n 
-0000003169 00000 n 
-0000016535 00000 n 
-0000003305 00000 n 
-0000016601 00000 n 
-0000003442 00000 n 
-0000016667 00000 n 
-0000003579 00000 n 
-0000016733 00000 n 
-0000003716 00000 n 
-0000016799 00000 n 
-0000003853 00000 n 
-0000016865 00000 n 
-0000003990 00000 n 
-0000016931 00000 n 
-0000004126 00000 n 
-0000016995 00000 n 
-0000004263 00000 n 
-0000017061 00000 n 
-0000004400 00000 n 
-0000017127 00000 n 
-0000004537 00000 n 
-0000017193 00000 n 
-0000004674 00000 n 
-0000006622 00000 n 
-0000006730 00000 n 
-0000008151 00000 n 
-0000008259 00000 n 
-0000009668 00000 n 
-0000009776 00000 n 
-0000010914 00000 n 
-0000017259 00000 n 
-0000011022 00000 n 
-0000011223 00000 n 
-0000011379 00000 n 
-0000011513 00000 n 
-0000011683 00000 n 
-0000011811 00000 n 
-0000011993 00000 n 
-0000012245 00000 n 
-0000012379 00000 n 
-0000012567 00000 n 
-0000012713 00000 n 
-0000012870 00000 n 
-0000013015 00000 n 
-0000013202 00000 n 
-0000013353 00000 n 
-0000013510 00000 n 
-0000013679 00000 n 
-0000013842 00000 n 
-0000014035 00000 n 
-0000014204 00000 n 
-0000014355 00000 n 
-0000014530 00000 n 
-0000014693 00000 n 
-0000014894 00000 n 
-0000015007 00000 n 
-0000015117 00000 n 
-0000015225 00000 n 
-0000015341 00000 n 
+0000001201 00000 n 
+0000001321 00000 n 
+0000001493 00000 n 
+0000015311 00000 n 
+0000001628 00000 n 
+0000015374 00000 n 
+0000001765 00000 n 
+0000015440 00000 n 
+0000001902 00000 n 
+0000015506 00000 n 
+0000002039 00000 n 
+0000015571 00000 n 
+0000002174 00000 n 
+0000015635 00000 n 
+0000002311 00000 n 
+0000015701 00000 n 
+0000002448 00000 n 
+0000015767 00000 n 
+0000002585 00000 n 
+0000015833 00000 n 
+0000002722 00000 n 
+0000015899 00000 n 
+0000002859 00000 n 
+0000015965 00000 n 
+0000002996 00000 n 
+0000016031 00000 n 
+0000003133 00000 n 
+0000016095 00000 n 
+0000003269 00000 n 
+0000016161 00000 n 
+0000003406 00000 n 
+0000016227 00000 n 
+0000003543 00000 n 
+0000016293 00000 n 
+0000003680 00000 n 
+0000016359 00000 n 
+0000003817 00000 n 
+0000016425 00000 n 
+0000003953 00000 n 
+0000016491 00000 n 
+0000004090 00000 n 
+0000016555 00000 n 
+0000004227 00000 n 
+0000016621 00000 n 
+0000004364 00000 n 
+0000016687 00000 n 
+0000004501 00000 n 
+0000006449 00000 n 
+0000006557 00000 n 
+0000007978 00000 n 
+0000008086 00000 n 
+0000009491 00000 n 
+0000009599 00000 n 
+0000010643 00000 n 
+0000016753 00000 n 
+0000010751 00000 n 
+0000010952 00000 n 
+0000011108 00000 n 
+0000011242 00000 n 
+0000011412 00000 n 
+0000011540 00000 n 
+0000011722 00000 n 
+0000011974 00000 n 
+0000012108 00000 n 
+0000012296 00000 n 
+0000012442 00000 n 
+0000012599 00000 n 
+0000012744 00000 n 
+0000012931 00000 n 
+0000013082 00000 n 
+0000013239 00000 n 
+0000013402 00000 n 
+0000013595 00000 n 
+0000013764 00000 n 
+0000013915 00000 n 
+0000014090 00000 n 
+0000014253 00000 n 
+0000014454 00000 n 
+0000014567 00000 n 
+0000014677 00000 n 
+0000014785 00000 n 
+0000014901 00000 n 
 trailer
 <<
-/Size 91
+/Size 88
 /Root 2 0 R
 /Info 4 0 R
 >>
 startxref
-17310
+16804
 %%EOF
Index: docs/lucene-contrib/index.html
===================================================================
--- docs/lucene-contrib/index.html	(revision 916567)
+++ docs/lucene-contrib/index.html	(working copy)
@@ -186,9 +186,6 @@
 <a href="../api/contrib-remote/index.html">Remote</a>
 </div>
 <div class="menuitem">
-<a href="../api/contrib-snowball/index.html">Snowball</a>
-</div>
-<div class="menuitem">
 <a href="../api/contrib-spatial/index.html">Spatial</a>
 </div>
 <div class="menuitem">
@@ -312,9 +309,6 @@
 <a href="#remote">remote</a>
 </li>
 <li>
-<a href="#snowball">snowball</a>
-</li>
-<li>
 <a href="#spatial">spatial</a>
 </li>
 <li>
@@ -458,42 +452,37 @@
 <p>Classes to help use Lucene with RMI.</p>
 <p>See <a href="../api/contrib-remote/index.html">remote javadoc</a>
 </p>
-<a name="N10107"></a><a name="snowball"></a>
-<h3 class="boxed">snowball</h3>
-<p>Pre-compiled versions of the Snowball stemmers for Lucene.</p>
-<p>See <a href="../api/contrib-snowball/index.html">snowball javadoc</a>
-</p>
-<a name="N10116"></a><a name="spatial"></a>
+<a name="N10107"></a><a name="spatial"></a>
 <h3 class="boxed">spatial</h3>
 <p>Classes to help with efficient distance based sorting.</p>
 <p>See <a href="../api/contrib-spatial/index.html">spatial javadoc</a>
 </p>
-<a name="N10125"></a><a name="spellchecker"></a>
+<a name="N10116"></a><a name="spellchecker"></a>
 <h3 class="boxed">spellchecker</h3>
 <p>Provides tools for spellchecking and suggestions with Lucene.</p>
 <p>See <a href="../api/contrib-spellchecker/index.html">spellchecker javadoc</a>
 </p>
-<a name="N10134"></a><a name="surround"></a>
+<a name="N10125"></a><a name="surround"></a>
 <h3 class="boxed">surround</h3>
 <p>A QueryParser that supports the Span family of queries as well as pre and infix notation.</p>
 <p>See <a href="../api/contrib-surround/index.html">surround javadoc</a>
 </p>
-<a name="N10143"></a><a name="swing"></a>
+<a name="N10134"></a><a name="swing"></a>
 <h3 class="boxed">swing</h3>
 <p>Swing components designed to integrate with Lucene.</p>
 <p>See <a href="../api/contrib-swing/index.html">swing javadoc</a>
 </p>
-<a name="N10152"></a><a name="wikipedia"></a>
+<a name="N10143"></a><a name="wikipedia"></a>
 <h3 class="boxed">wikipedia</h3>
 <p>Tools for working with wikipedia content.</p>
 <p>See <a href="../api/contrib-wikipedia/index.html">wikipedia javadoc</a>
 </p>
-<a name="N10161"></a><a name="wordnet"></a>
+<a name="N10152"></a><a name="wordnet"></a>
 <h3 class="boxed">wordnet</h3>
 <p>Tools to help utilize wordnet synonyms with Lucene</p>
 <p>See <a href="../api/contrib-wordnet/index.html">wordnet javadoc</a>
 </p>
-<a name="N10170"></a><a name="xml-query-parser"></a>
+<a name="N10161"></a><a name="xml-query-parser"></a>
 <h3 class="boxed">xml-query-parser</h3>
 <p>A QueryParser that can read queries written in an XML format.</p>
 <p>See <a href="../api/contrib-wordnet/index.html">xml-query-parser javadoc</a>
Index: docs/demo2.html
===================================================================
--- docs/demo2.html	(revision 916567)
+++ docs/demo2.html	(working copy)
@@ -186,9 +186,6 @@
 <a href="api/contrib-remote/index.html">Remote</a>
 </div>
 <div class="menuitem">
-<a href="api/contrib-snowball/index.html">Snowball</a>
-</div>
-<div class="menuitem">
 <a href="api/contrib-spatial/index.html">Spatial</a>
 </div>
 <div class="menuitem">
Index: docs/demo3.html
===================================================================
--- docs/demo3.html	(revision 916567)
+++ docs/demo3.html	(working copy)
@@ -186,9 +186,6 @@
 <a href="api/contrib-remote/index.html">Remote</a>
 </div>
 <div class="menuitem">
-<a href="api/contrib-snowball/index.html">Snowball</a>
-</div>
-<div class="menuitem">
 <a href="api/contrib-spatial/index.html">Spatial</a>
 </div>
 <div class="menuitem">
Index: docs/index.html
===================================================================
--- docs/index.html	(revision 916567)
+++ docs/index.html	(working copy)
@@ -184,9 +184,6 @@
 <a href="api/contrib-remote/index.html">Remote</a>
 </div>
 <div class="menuitem">
-<a href="api/contrib-snowball/index.html">Snowball</a>
-</div>
-<div class="menuitem">
 <a href="api/contrib-spatial/index.html">Spatial</a>
 </div>
 <div class="menuitem">
Index: docs/demo4.html
===================================================================
--- docs/demo4.html	(revision 916567)
+++ docs/demo4.html	(working copy)
@@ -186,9 +186,6 @@
 <a href="api/contrib-remote/index.html">Remote</a>
 </div>
 <div class="menuitem">
-<a href="api/contrib-snowball/index.html">Snowball</a>
-</div>
-<div class="menuitem">
 <a href="api/contrib-spatial/index.html">Spatial</a>
 </div>
 <div class="menuitem">
Index: docs/fileformats.html
===================================================================
--- docs/fileformats.html	(revision 916567)
+++ docs/fileformats.html	(working copy)
@@ -186,9 +186,6 @@
 <a href="api/contrib-remote/index.html">Remote</a>
 </div>
 <div class="menuitem">
-<a href="api/contrib-snowball/index.html">Snowball</a>
-</div>
-<div class="menuitem">
 <a href="api/contrib-spatial/index.html">Spatial</a>
 </div>
 <div class="menuitem">
@@ -2551,11 +2548,12 @@
 <div class="section">
 <p>
 	      When referring to term numbers, Lucene's current
-	      implementation uses a Java <span class="codefrag">int</span>, which means
-	      the maximum number of unique terms in any single index
-	      segment is 2,147,483,648.  This is technically not a
-	      limitation of the index file format, just of Lucene's
-	      current implementation.
+	      implementation uses a Java <span class="codefrag">int</span> to hold the
+	      term index, which means the maximum number of unique
+	      terms in any single index segment is ~2.1 billion times
+	      the term index interval (default 128) = ~274 billion.
+	      This is technically not a limitation of the index file
+	      format, just of Lucene's current implementation.
 	    </p>
 <p>
 	      Similarly, Lucene uses a Java <span class="codefrag">int</span> to refer
Index: build.xml
===================================================================
--- build.xml	(revision 916567)
+++ build.xml	(working copy)
@@ -99,10 +99,10 @@
       <echo>Initial SVN checkout for '${backwards.branch}'...</echo>
       <mkdir dir="${backwards.dir}"/>
       <exec dir="${backwards.dir}" executable="${svn.exe}" failifexecutionfails="false" failonerror="true">
-        <arg line="checkout -r ${backwards.rev} --depth empty http://svn.apache.org/repos/asf/lucene/java/branches/${backwards.branch} ${backwards.branch}"/>
+        <arg line="checkout --trust-server-cert --non-interactive -r ${backwards.rev} --depth empty https://svn.apache.org/repos/asf/lucene/java/branches/${backwards.branch} ${backwards.branch}"/>
       </exec>
       <exec dir="${backwards.dir}" executable="${svn.exe}" failifexecutionfails="false" failonerror="true">
-        <arg line="update -r ${backwards.rev} --set-depth infinity ${backwards.branch}/src"/>
+        <arg line="update --trust-server-cert --non-interactive -r ${backwards.rev} --set-depth infinity ${backwards.branch}/src"/>
       </exec>
     </sequential>
   </target>
@@ -111,7 +111,7 @@
     <sequential>
       <echo>Update backwards branch '${backwards.branch}' to revision ${backwards.rev}...</echo>
       <exec dir="${backwards.dir}" executable="${svn.exe}" failifexecutionfails="false" failonerror="true">
-        <arg line="update -r ${backwards.rev} ${backwards.branch}"/>
+        <arg line="update --trust-server-cert --non-interactive -r ${backwards.rev} ${backwards.branch}"/>
       </exec>
     </sequential>
   </target>
@@ -333,7 +333,6 @@
           <packageset dir="contrib/queries/src/java"/>
           <packageset dir="contrib/regex/src/java"/>
           <packageset dir="contrib/remote/src/java"/>
-          <packageset dir="contrib/snowball/src/java"/>
           <packageset dir="contrib/spatial/src/java"/>
           <packageset dir="contrib/spellchecker/src/java"/>
           <packageset dir="contrib/surround/src/java"/>
@@ -352,7 +351,7 @@
   
           <group title="Demo" packages="org.apache.lucene.demo*"/>
   
-          <group title="contrib: Analysis" packages="org.apache.lucene.analysis.*"/>
+          <group title="contrib: Analysis" packages="org.apache.lucene.analysis.*:org.tartarus.snowball*"/>
           <group title="contrib: Ant" packages="org.apache.lucene.ant*"/>
           <group title="contrib: Benchmark" packages="org.apache.lucene.benchmark*"/>
           <group title="contrib: ICU" packages="org.apache.lucene.collation*"/>
@@ -366,7 +365,6 @@
           <group title="contrib: Queries" packages="org.apache.lucene.search.similar*"/>
           <group title="contrib: Query Parser" packages="org.apache.lucene.queryParser.*"/>
           <group title="contrib: RegEx" packages="org.apache.lucene.search.regex*:org.apache.regexp*"/>
-          <group title="contrib: Snowball" packages="org.apache.lucene.analysis.snowball*:net.sf.snowball*"/>
           <group title="contrib: Spatial" packages="org.apache.lucene.spatial*"/>
           <group title="contrib: SpellChecker" packages="org.apache.lucene.search.spell*"/>
           <group title="contrib: Surround Parser" packages="org.apache.lucene.queryParser.surround*"/>
@@ -552,6 +550,48 @@
   </target>
 	
   <!-- ================================================================== -->
+  <!-- support for signing the artifacts using gpg                        -->
+  <!-- ================================================================== -->
+  <target name="clean-dist-signatures">
+    <delete failonerror="false">
+      <fileset dir="${dist.dir}">
+        <include name="**/*.asc"/>
+      </fileset>
+    </delete>
+  </target>
+  
+  <target name="sign-artifacts" depends="clean-dist-signatures">
+    <available property="gpg.input.handler" classname="org.apache.tools.ant.input.SecureInputHandler"
+      value="org.apache.tools.ant.input.SecureInputHandler"/>
+    <!--else:--><property name="gpg.input.handler" value="org.apache.tools.ant.input.DefaultInputHandler"/>
+    <input message="Enter GPG keystore password: >" addproperty="gpg.passphrase">
+      <handler classname="${gpg.input.handler}" />
+    </input>
+    
+    <apply executable="${gpg.exe}" inputstring="${gpg.passphrase}"
+      dest="${dist.dir}" type="file" maxparallel="1" verbose="yes">
+      <arg value="--passphrase-fd"/>
+      <arg value="0"/>
+      <arg value="--batch"/>
+      <arg value="--armor"/>
+      <arg value="--default-key"/>
+      <arg value="${gpg.key}"/>
+      <arg value="--output"/>
+      <targetfile/>
+      <arg value="--detach-sig"/>
+      <srcfile/>
+      
+      <fileset dir="${dist.dir}">
+        <include name="**/*.jar"/>
+        <include name="**/*.zip"/>
+        <include name="**/*.tar.gz"/>
+        <include name="**/*.pom"/>
+      </fileset>
+      <globmapper from="*" to="*.asc"/>
+    </apply>
+  </target>
+
+  <!-- ================================================================== -->
   <!-- Build the JavaCC files into the source tree                        -->
   <!-- ================================================================== -->
   <target name="jjdoc">
@@ -708,6 +748,7 @@
   <macrodef name="lucene-checksum">
     <attribute name="file"/>
     <sequential>
+      <echo>Building checksums for '@{file}'</echo>
       <checksum file="@{file}" algorithm="md5" format="MD5SUM" forceoverwrite="yes" readbuffersize="65536"/>
       <checksum file="@{file}" algorithm="sha1" format="MD5SUM" forceoverwrite="yes" readbuffersize="65536"/>
     </sequential>

Property changes on: build.xml
___________________________________________________________________
Added: svn:mergeinfo
   Merged /lucene/java/branches/lucene_2_9/build.xml:r909334

Index: NOTICE.txt
===================================================================
--- NOTICE.txt	(revision 916567)
+++ NOTICE.txt	(working copy)
@@ -5,8 +5,11 @@
 The Apache Software Foundation (http://www.apache.org/).
 
 The snowball stemmers in
-  contrib/snowball/src/java/net/sf/snowball
+  contrib/analyzers/common/src/java/net/sf/snowball
 were developed by Martin Porter and Richard Boulton.
+The snowball stopword lists in
+  contrib/analyzers/common/src/resources/org/apache/lucene/analysis/snowball
+were developed by Martin Porter and Richard Boulton.
 The full snowball package is available from
   http://snowball.tartarus.org/
 
@@ -20,11 +23,21 @@
 contrib/analyzers/common/src/resources/org/apache/lucene/analysis/fa/stopwords.txt.
 See http://members.unine.ch/jacques.savoy/clef/index.html.
 
+The Romanian analyzer (contrib/analyzers) comes with a default
+stopword list that is BSD-licensed created by Jacques Savoy.  The file resides in
+contrib/analyzers/common/src/resources/org/apache/lucene/analysis/ro/stopwords.txt.
+See http://members.unine.ch/jacques.savoy/clef/index.html.
+
 The Bulgarian analyzer (contrib/analyzers) comes with a default
 stopword list that is BSD-licensed created by Jacques Savoy.  The file resides in
 contrib/analyzers/common/src/resources/org/apache/lucene/analysis/bg/stopwords.txt.
 See http://members.unine.ch/jacques.savoy/clef/index.html.
 
+The Hindi analyzer (contrib/analyzers) comes with a default
+stopword list that is BSD-licensed created by Jacques Savoy.  The file resides in
+contrib/analyzers/common/src/resources/org/apache/lucene/analysis/hi/stopwords.txt.
+See http://members.unine.ch/jacques.savoy/clef/index.html.
+
 Includes lib/servlet-api-2.4.jar from  Apache Tomcat
 
 The SmartChineseAnalyzer source code (under contrib/analyzers) was

Property changes on: contrib
___________________________________________________________________
Modified: svn:mergeinfo
   Merged /lucene/java/branches/lucene_2_9/contrib:r909334
   Merged /lucene/java/trunk/contrib:r889614-889621,889623-889666,889668-889865,899002-916543

Index: contrib/snowball/SNOWBALL-LICENSE.txt
===================================================================
--- contrib/snowball/SNOWBALL-LICENSE.txt	(revision 916567)
+++ contrib/snowball/SNOWBALL-LICENSE.txt	(working copy)
@@ -1,26 +0,0 @@
-Copyright (c) 2001, Dr Martin Porter
-Copyright (c) 2002, Richard Boulton
-All rights reserved.
-
-Redistribution and use in source and binary forms, with or without
-modification, are permitted provided that the following conditions are met:
-
-    * Redistributions of source code must retain the above copyright notice,
-    * this list of conditions and the following disclaimer.
-    * Redistributions in binary form must reproduce the above copyright
-    * notice, this list of conditions and the following disclaimer in the
-    * documentation and/or other materials provided with the distribution.
-    * Neither the name of the copyright holders nor the names of its contributors
-    * may be used to endorse or promote products derived from this software
-    * without specific prior written permission.
-
-THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
-AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
-IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
-DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE
-FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
-DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
-SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
-CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
-OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
-OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
Index: contrib/snowball/xdocs/index.xml
===================================================================
--- contrib/snowball/xdocs/index.xml	(revision 916567)
+++ contrib/snowball/xdocs/index.xml	(working copy)
@@ -1,17 +0,0 @@
-<?xml version="1.0"?>
-<document>
-<properties>
-<title>Overview - Snowball Stemmers for Lucene</title>
-</properties>
-<body>
-
-<section name="Snowball Stemmers for Lucene">
-<p>
-This project provides pre-compiled version of the Snowball stemmers
-together with classes integrating them with the Lucene search engine.
-</p>
-</section>
-
-</body>
-</document>
-
Index: contrib/snowball/xdocs/stylesheets/project.xml
===================================================================
--- contrib/snowball/xdocs/stylesheets/project.xml	(revision 916567)
+++ contrib/snowball/xdocs/stylesheets/project.xml	(working copy)
@@ -1,32 +0,0 @@
-<?xml version="1.0" encoding="ISO-8859-1"?>
-<project name="Snowball Stemers for Lucene"
-         href="http://jakarta.apache.org/lucene-sandbox/snowball/">
-
-    <title>Snowball Stemmers for Lucene</title>
-
-    <body>
-    <menu name="Documentation">
-        <item name="Javadoc"           href="/api/index.html"/>
-    </menu>
-
-    <menu name="Download">
-        <item name="Releases"
-              href="http://jakarta.apache.org/builds/jakarta-lucene-sandbox/snowball/"/>
-        <item name="CVS Repository"   href="/site/cvsindex.html"/>
-    </menu>
-
-    <menu name="Links">
-        <item name="Snowball Home" href="http://snowball.tartarus.org/"/>
-        <item name="Lucene Home" href="http://jakarta.apache.org/lucene/"/>
-        <item name="Lucene Sandbox"
-              href="http://jakarta.apache.org/lucene/docs/lucene-sandbox/"/>
-    </menu>
-
-    <menu name="Jakarta">
-        <item name="Get Involved"          href="/site/getinvolved.html"/>
-        <item name="Acknowledgements"      href="/site/acknowledgements.html"/>
-        <item name="Contact"               href="/site/contact.html"/>
-        <item name="Legal"                 href="/site/legal.html"/>
-    </menu>
-    </body>
-</project>
Index: contrib/snowball/LICENSE.txt
===================================================================
--- contrib/snowball/LICENSE.txt	(revision 916567)
+++ contrib/snowball/LICENSE.txt	(working copy)
@@ -1,16 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
Index: contrib/snowball/snowball.cvspass
===================================================================
--- contrib/snowball/snowball.cvspass	(revision 916567)
+++ contrib/snowball/snowball.cvspass	(working copy)
@@ -1 +0,0 @@
-:pserver:cvsuser@cvs.tartarus.org:/home/cvs Ay=0=a%0bZ
Index: contrib/snowball/pom.xml.template
===================================================================
--- contrib/snowball/pom.xml.template	(revision 916567)
+++ contrib/snowball/pom.xml.template	(working copy)
@@ -1,43 +0,0 @@
-<project xmlns="http://maven.apache.org/POM/4.0.0"
-  xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
-  xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd">
-
-  <!--
-    Licensed to the Apache Software Foundation (ASF) under one
-    or more contributor license agreements.  See the NOTICE file
-    distributed with this work for additional information
-    regarding copyright ownership.  The ASF licenses this file
-    to you under the Apache License, Version 2.0 (the
-    "License"); you may not use this file except in compliance
-    with the License.  You may obtain a copy of the License at
-    
-    http://www.apache.org/licenses/LICENSE-2.0
-    
-    Unless required by applicable law or agreed to in writing,
-    software distributed under the License is distributed on an
-    "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
-    KIND, either express or implied.  See the License for the
-    specific language governing permissions and limitations
-    under the License.
-  -->
-
-  <modelVersion>4.0.0</modelVersion>
-  <parent>
-    <groupId>org.apache.lucene</groupId>
-    <artifactId>lucene-contrib</artifactId>
-    <version>@version@</version>
-  </parent>
-  <groupId>org.apache.lucene</groupId>
-  <artifactId>lucene-snowball</artifactId>
-  <name>Lucene Snowball</name>
-  <version>@version@</version>
-  <description>Snowball Analyzers</description>
-  <packaging>jar</packaging>
-  <dependencies>
-    <dependency>
-      <groupId>org.apache.lucene</groupId>
-      <artifactId>lucene-analyzers</artifactId>
-      <version>@version@</version>
-    </dependency>
-  </dependencies>
-</project>
Index: contrib/snowball/src/test/org/apache/lucene/analysis/snowball/TestSnowball.java
===================================================================
--- contrib/snowball/src/test/org/apache/lucene/analysis/snowball/TestSnowball.java	(revision 916567)
+++ contrib/snowball/src/test/org/apache/lucene/analysis/snowball/TestSnowball.java	(working copy)
@@ -1,144 +0,0 @@
-package org.apache.lucene.analysis.snowball;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.analysis.BaseTokenStreamTestCase;
-import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.index.Payload;
-import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.standard.StandardAnalyzer;
-import org.apache.lucene.analysis.tokenattributes.FlagsAttribute;
-import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
-import org.apache.lucene.analysis.tokenattributes.PayloadAttribute;
-import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;
-import org.apache.lucene.analysis.tokenattributes.TermAttribute;
-import org.apache.lucene.analysis.tokenattributes.TypeAttribute;
-import org.apache.lucene.util.Version;
-
-public class TestSnowball extends BaseTokenStreamTestCase {
-
-  public void testEnglish() throws Exception {
-    Analyzer a = new SnowballAnalyzer(Version.LUCENE_CURRENT, "English");
-    assertAnalyzesTo(a, "he abhorred accents",
-        new String[]{"he", "abhor", "accent"});
-  }
-  
-  public void testStopwords() throws Exception {
-    Analyzer a = new SnowballAnalyzer(Version.LUCENE_CURRENT, "English",
-        StandardAnalyzer.STOP_WORDS_SET);
-    assertAnalyzesTo(a, "the quick brown fox jumped",
-        new String[]{"quick", "brown", "fox", "jump"});
-  }
-
-  /**
-   * Test english lowercasing. Test both cases (pre-3.1 and post-3.1) to ensure
-   * we lowercase I correct for non-Turkish languages in either case.
-   */
-  public void testEnglishLowerCase() throws Exception {
-    Analyzer a = new SnowballAnalyzer(Version.LUCENE_CURRENT, "English");
-    assertAnalyzesTo(a, "cryogenic", new String[] { "cryogen" });
-    assertAnalyzesTo(a, "CRYOGENIC", new String[] { "cryogen" });
-    
-    Analyzer b = new SnowballAnalyzer(Version.LUCENE_30, "English");
-    assertAnalyzesTo(b, "cryogenic", new String[] { "cryogen" });
-    assertAnalyzesTo(b, "CRYOGENIC", new String[] { "cryogen" });
-  }
-  
-  /**
-   * Test turkish lowercasing
-   */
-  public void testTurkish() throws Exception {
-    Analyzer a = new SnowballAnalyzer(Version.LUCENE_CURRENT, "Turkish");
-
-    assertAnalyzesTo(a, "ağacı", new String[] { "ağaç" });
-    assertAnalyzesTo(a, "AĞACI", new String[] { "ağaç" });
-  }
-  
-  /**
-   * Test turkish lowercasing (old buggy behavior)
-   * @deprecated Remove this when support for 3.0 indexes is no longer required
-   */
-  @Deprecated
-  public void testTurkishBWComp() throws Exception {
-    Analyzer a = new SnowballAnalyzer(Version.LUCENE_30, "Turkish");
-    // AĞACI in turkish lowercases to ağacı, but with lowercase filter ağaci.
-    // this fails due to wrong casing, because the stemmer
-    // will only remove -ı, not -i
-    assertAnalyzesTo(a, "ağacı", new String[] { "ağaç" });
-    assertAnalyzesTo(a, "AĞACI", new String[] { "ağaci" });
-  }
-
-  
-  public void testReusableTokenStream() throws Exception {
-    Analyzer a = new SnowballAnalyzer(Version.LUCENE_CURRENT, "English");
-    assertAnalyzesToReuse(a, "he abhorred accents",
-        new String[]{"he", "abhor", "accent"});
-    assertAnalyzesToReuse(a, "she abhorred him",
-        new String[]{"she", "abhor", "him"});
-  }
-  
-  public void testFilterTokens() throws Exception {
-    SnowballFilter filter = new SnowballFilter(new TestTokenStream(), "English");
-    TermAttribute termAtt = filter.getAttribute(TermAttribute.class);
-    OffsetAttribute offsetAtt = filter.getAttribute(OffsetAttribute.class);
-    TypeAttribute typeAtt = filter.getAttribute(TypeAttribute.class);
-    PayloadAttribute payloadAtt = filter.getAttribute(PayloadAttribute.class);
-    PositionIncrementAttribute posIncAtt = filter.getAttribute(PositionIncrementAttribute.class);
-    FlagsAttribute flagsAtt = filter.getAttribute(FlagsAttribute.class);
-    
-    filter.incrementToken();
-
-    assertEquals("accent", termAtt.term());
-    assertEquals(2, offsetAtt.startOffset());
-    assertEquals(7, offsetAtt.endOffset());
-    assertEquals("wrd", typeAtt.type());
-    assertEquals(3, posIncAtt.getPositionIncrement());
-    assertEquals(77, flagsAtt.getFlags());
-    assertEquals(new Payload(new byte[]{0,1,2,3}), payloadAtt.getPayload());
-  }
-  
-  private final class TestTokenStream extends TokenStream {
-    private TermAttribute termAtt;
-    private OffsetAttribute offsetAtt;
-    private TypeAttribute typeAtt;
-    private PayloadAttribute payloadAtt;
-    private PositionIncrementAttribute posIncAtt;
-    private FlagsAttribute flagsAtt;
-    
-    TestTokenStream() {
-      super();
-      termAtt = addAttribute(TermAttribute.class);
-      offsetAtt = addAttribute(OffsetAttribute.class);
-      typeAtt = addAttribute(TypeAttribute.class);
-      payloadAtt = addAttribute(PayloadAttribute.class);
-      posIncAtt = addAttribute(PositionIncrementAttribute.class);
-      flagsAtt = addAttribute(FlagsAttribute.class);
-    }
-    
-    @Override
-    public boolean incrementToken() {
-      termAtt.setTermBuffer("accents");
-      offsetAtt.setOffset(2, 7);
-      typeAtt.setType("wrd");
-      posIncAtt.setPositionIncrement(3);
-      payloadAtt.setPayload(new Payload(new byte[]{0,1,2,3}));
-      flagsAtt.setFlags(77);
-      return true;
-    }
-  }
-}
\ No newline at end of file
Index: contrib/snowball/src/test/org/apache/lucene/analysis/snowball/TestSnowballVocab.java
===================================================================
--- contrib/snowball/src/test/org/apache/lucene/analysis/snowball/TestSnowballVocab.java	(revision 916567)
+++ contrib/snowball/src/test/org/apache/lucene/analysis/snowball/TestSnowballVocab.java	(working copy)
@@ -1,98 +0,0 @@
-package org.apache.lucene.analysis.snowball;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.BufferedReader;
-import java.io.File;
-import java.io.FileInputStream;
-import java.io.IOException;
-import java.io.InputStream;
-import java.io.InputStreamReader;
-import java.io.StringReader;
-
-import org.apache.lucene.analysis.BaseTokenStreamTestCase;
-import org.apache.lucene.analysis.KeywordTokenizer;
-import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.Tokenizer;
-
-/**
- * Test the snowball filters against the snowball data tests
- */
-public class TestSnowballVocab extends BaseTokenStreamTestCase {
-  private Tokenizer tokenizer = new KeywordTokenizer(new StringReader(""));
-  static final File dataDir = new File(System.getProperty("dataDir", "./bin"));
-  static final File dataRoot = new File(dataDir, 
-      "org/apache/lucene/analysis/snowball/data");
-  
-  /**
-   * Run all languages against their snowball vocabulary tests.
-   */
-  public void testStemmers() throws IOException {
-    assertCorrectOutput("Danish", "danish");
-    assertCorrectOutput("Dutch", "dutch");
-    assertCorrectOutput("English", "english");
-    // disabled due to snowball java code generation bug: 
-    // see http://article.gmane.org/gmane.comp.search.snowball/1139
-    // assertCorrectOutput("Finnish", "finnish");
-    assertCorrectOutput("French", "french");
-    assertCorrectOutput("German", "german");
-    assertCorrectOutput("German2", "german2");
-    assertCorrectOutput("Hungarian", "hungarian");
-    assertCorrectOutput("Italian", "italian");
-    assertCorrectOutput("Kp", "kraaij_pohlmann");
-    // disabled due to snowball java code generation bug: 
-    // see http://article.gmane.org/gmane.comp.search.snowball/1139
-    // assertCorrectOutput("Lovins", "lovins");
-    assertCorrectOutput("Norwegian", "norwegian");
-    assertCorrectOutput("Porter", "porter");
-    assertCorrectOutput("Portuguese", "portuguese");
-    assertCorrectOutput("Romanian", "romanian");
-    assertCorrectOutput("Russian", "russian");
-    assertCorrectOutput("Spanish", "spanish");
-    assertCorrectOutput("Swedish", "swedish");
-    assertCorrectOutput("Turkish", "turkish");
-  }
-    
-  /**
-   * For the supplied language, run the stemmer against all strings in voc.txt
-   * The output should be the same as the string in output.txt
-   */
-  private void assertCorrectOutput(String snowballLanguage, String dataDirectory)
-      throws IOException {
-    System.err.println("checking snowball language: " + snowballLanguage);
-    TokenStream filter = new SnowballFilter(tokenizer, snowballLanguage);
-    InputStream vocFile = new FileInputStream(new File(dataRoot, 
-        dataDirectory + "/voc.txt"));
-    InputStream outputFile = new FileInputStream(new File(dataRoot, 
-        dataDirectory + "/output.txt"));
-    BufferedReader vocReader = new BufferedReader(new InputStreamReader(
-        vocFile, "UTF-8"));
-    BufferedReader outputReader = new BufferedReader(new InputStreamReader(
-        outputFile, "UTF-8"));
-    String inputWord = null;
-    while ((inputWord = vocReader.readLine()) != null) {
-      String expectedWord = outputReader.readLine();
-      assertNotNull(expectedWord);
-      tokenizer.reset(new StringReader(inputWord));
-      filter.reset();
-      assertTokenStreamContents(filter, new String[] {expectedWord});
-    }
-    vocReader.close();
-    outputReader.close();
-  }
-}
Index: contrib/snowball/src/java/org/apache/lucene/analysis/snowball/SnowballAnalyzer.java
===================================================================
--- contrib/snowball/src/java/org/apache/lucene/analysis/snowball/SnowballAnalyzer.java	(revision 916567)
+++ contrib/snowball/src/java/org/apache/lucene/analysis/snowball/SnowballAnalyzer.java	(working copy)
@@ -1,121 +0,0 @@
-package org.apache.lucene.analysis.snowball;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.analysis.*;
-import org.apache.lucene.analysis.standard.*;
-import org.apache.lucene.analysis.tr.TurkishLowerCaseFilter;
-import org.apache.lucene.util.Version;
-
-import java.io.IOException;
-import java.io.Reader;
-import java.util.Set;
-
-/** Filters {@link StandardTokenizer} with {@link StandardFilter}, {@link
- * LowerCaseFilter}, {@link StopFilter} and {@link SnowballFilter}.
- *
- * Available stemmers are listed in org.tartarus.snowball.ext.  The name of a
- * stemmer is the part of the class name before "Stemmer", e.g., the stemmer in
- * {@link org.tartarus.snowball.ext.EnglishStemmer} is named "English".
- *
- * <p><b>NOTE</b>: This class uses the same {@link Version}
- * dependent settings as {@link StandardAnalyzer}, with the following addition:
- * <ul>
- *   <li> As of 3.1, uses {@link TurkishLowerCaseFilter} for Turkish language.
- * </ul>
- * </p>
- */
-public final class SnowballAnalyzer extends Analyzer {
-  private String name;
-  private Set<?> stopSet;
-  private final Version matchVersion;
-
-  /** Builds the named analyzer with no stop words. */
-  public SnowballAnalyzer(Version matchVersion, String name) {
-    this.name = name;
-    this.matchVersion = matchVersion;
-  }
-
-  /** 
-   * Builds the named analyzer with the given stop words.
-   * @deprecated Use {@link #SnowballAnalyzer(Version, String, Set)} instead.  
-   */
-  @Deprecated
-  public SnowballAnalyzer(Version matchVersion, String name, String[] stopWords) {
-    this(matchVersion, name);
-    stopSet = StopFilter.makeStopSet(matchVersion, stopWords);
-  }
-  
-  /** Builds the named analyzer with the given stop words. */
-  public SnowballAnalyzer(Version matchVersion, String name, Set<?> stopWords) {
-    this(matchVersion, name);
-    stopSet = CharArraySet.unmodifiableSet(CharArraySet.copy(matchVersion,
-        stopWords));
-  }
-
-  /** Constructs a {@link StandardTokenizer} filtered by a {@link
-      StandardFilter}, a {@link LowerCaseFilter}, a {@link StopFilter},
-      and a {@link SnowballFilter} */
-  @Override
-  public TokenStream tokenStream(String fieldName, Reader reader) {
-    TokenStream result = new StandardTokenizer(matchVersion, reader);
-    result = new StandardFilter(result);
-    // Use a special lowercase filter for turkish, the stemmer expects it.
-    if (matchVersion.onOrAfter(Version.LUCENE_31) && name.equals("Turkish"))
-      result = new TurkishLowerCaseFilter(result);
-    else
-      result = new LowerCaseFilter(matchVersion, result);
-    if (stopSet != null)
-      result = new StopFilter(matchVersion,
-                              result, stopSet);
-    result = new SnowballFilter(result, name);
-    return result;
-  }
-  
-  private class SavedStreams {
-    Tokenizer source;
-    TokenStream result;
-  }
-  
-  /** Returns a (possibly reused) {@link StandardTokenizer} filtered by a 
-   * {@link StandardFilter}, a {@link LowerCaseFilter}, 
-   * a {@link StopFilter}, and a {@link SnowballFilter} */
-  @Override
-  public TokenStream reusableTokenStream(String fieldName, Reader reader)
-      throws IOException {
-    SavedStreams streams = (SavedStreams) getPreviousTokenStream();
-    if (streams == null) {
-      streams = new SavedStreams();
-      streams.source = new StandardTokenizer(matchVersion, reader);
-      streams.result = new StandardFilter(streams.source);
-      // Use a special lowercase filter for turkish, the stemmer expects it.
-      if (matchVersion.onOrAfter(Version.LUCENE_31) && name.equals("Turkish"))
-        streams.result = new TurkishLowerCaseFilter(streams.result);
-      else
-        streams.result = new LowerCaseFilter(matchVersion, streams.result);
-      if (stopSet != null)
-        streams.result = new StopFilter(matchVersion,
-                                        streams.result, stopSet);
-      streams.result = new SnowballFilter(streams.result, name);
-      setPreviousTokenStream(streams);
-    } else {
-      streams.source.reset(reader);
-    }
-    return streams.result;
-  }
-}
Index: contrib/snowball/src/java/org/apache/lucene/analysis/snowball/SnowballFilter.java
===================================================================
--- contrib/snowball/src/java/org/apache/lucene/analysis/snowball/SnowballFilter.java	(revision 916567)
+++ contrib/snowball/src/java/org/apache/lucene/analysis/snowball/SnowballFilter.java	(working copy)
@@ -1,92 +0,0 @@
-package org.apache.lucene.analysis.snowball;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.analysis.TokenFilter;
-import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.tokenattributes.TermAttribute;
-import org.apache.lucene.analysis.tr.TurkishLowerCaseFilter; // javadoc @link
-import org.apache.lucene.analysis.LowerCaseFilter; // javadoc @link
-import org.tartarus.snowball.SnowballProgram;
-
-/**
- * A filter that stems words using a Snowball-generated stemmer.
- *
- * Available stemmers are listed in {@link org.tartarus.snowball.ext}.
- * <p><b>NOTE</b>: SnowballFilter expects lowercased text.
- * <ul>
- *  <li>For the Turkish language, see {@link TurkishLowerCaseFilter}.
- *  <li>For other languages, see {@link LowerCaseFilter}.
- * </ul>
- * </p>
- */
-public final class SnowballFilter extends TokenFilter {
-
-  private SnowballProgram stemmer;
-
-  private TermAttribute termAtt;
-  
-  public SnowballFilter(TokenStream input, SnowballProgram stemmer) {
-    super(input);
-    this.stemmer = stemmer;
-    termAtt = addAttribute(TermAttribute.class);
-  }
-
-  /**
-   * Construct the named stemming filter.
-   *
-   * Available stemmers are listed in {@link org.tartarus.snowball.ext}.
-   * The name of a stemmer is the part of the class name before "Stemmer",
-   * e.g., the stemmer in {@link org.tartarus.snowball.ext.EnglishStemmer} is named "English".
-   *
-   * @param in the input tokens to stem
-   * @param name the name of a stemmer
-   */
-  public SnowballFilter(TokenStream in, String name) {
-    super(in);
-    try {      
-      Class<?> stemClass = Class.forName("org.tartarus.snowball.ext." + name + "Stemmer");
-      stemmer = (SnowballProgram) stemClass.newInstance();
-    } catch (Exception e) {
-      throw new RuntimeException(e.toString());
-    }
-    termAtt = addAttribute(TermAttribute.class);
-  }
-
-  /** Returns the next input Token, after being stemmed */
-  @Override
-  public final boolean incrementToken() throws IOException {
-    if (input.incrementToken()) {
-      char termBuffer[] = termAtt.termBuffer();
-      final int length = termAtt.termLength();
-      stemmer.setCurrent(termBuffer, length);
-      stemmer.stem();
-      final char finalTerm[] = stemmer.getCurrentBuffer();
-      final int newLength = stemmer.getCurrentBufferLength();
-      if (finalTerm != termBuffer)
-        termAtt.setTermBuffer(finalTerm, 0, newLength);
-      else
-        termAtt.setTermLength(newLength); 
-      return true;
-    } else {
-      return false;
-    }
-  }
-}
Index: contrib/snowball/src/java/org/apache/lucene/analysis/snowball/package.html
===================================================================
--- contrib/snowball/src/java/org/apache/lucene/analysis/snowball/package.html	(revision 916567)
+++ contrib/snowball/src/java/org/apache/lucene/analysis/snowball/package.html	(working copy)
@@ -1,24 +0,0 @@
-<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<html>
-<body>
-{@link org.apache.lucene.analysis.TokenFilter} and {@link
-org.apache.lucene.analysis.Analyzer} implementations that use Snowball
-stemmers.
-</body>
-</html>
Index: contrib/snowball/src/java/org/tartarus/snowball/ext/PortugueseStemmer.java
===================================================================
--- contrib/snowball/src/java/org/tartarus/snowball/ext/PortugueseStemmer.java	(revision 916567)
+++ contrib/snowball/src/java/org/tartarus/snowball/ext/PortugueseStemmer.java	(working copy)
@@ -1,1116 +0,0 @@
-// This file was generated automatically by the Snowball to Java compiler
-
-package org.tartarus.snowball.ext;
-import org.tartarus.snowball.SnowballProgram;
-import org.tartarus.snowball.Among;
-
-/**
- * Generated class implementing code defined by a snowball script.
- */
-public class PortugueseStemmer extends SnowballProgram {
-
-        private Among a_0[] = {
-            new Among ( "", -1, 3, "", this),
-            new Among ( "\u00E3", 0, 1, "", this),
-            new Among ( "\u00F5", 0, 2, "", this)
-        };
-
-        private Among a_1[] = {
-            new Among ( "", -1, 3, "", this),
-            new Among ( "a~", 0, 1, "", this),
-            new Among ( "o~", 0, 2, "", this)
-        };
-
-        private Among a_2[] = {
-            new Among ( "ic", -1, -1, "", this),
-            new Among ( "ad", -1, -1, "", this),
-            new Among ( "os", -1, -1, "", this),
-            new Among ( "iv", -1, 1, "", this)
-        };
-
-        private Among a_3[] = {
-            new Among ( "ante", -1, 1, "", this),
-            new Among ( "avel", -1, 1, "", this),
-            new Among ( "\u00EDvel", -1, 1, "", this)
-        };
-
-        private Among a_4[] = {
-            new Among ( "ic", -1, 1, "", this),
-            new Among ( "abil", -1, 1, "", this),
-            new Among ( "iv", -1, 1, "", this)
-        };
-
-        private Among a_5[] = {
-            new Among ( "ica", -1, 1, "", this),
-            new Among ( "\u00E2ncia", -1, 1, "", this),
-            new Among ( "\u00EAncia", -1, 4, "", this),
-            new Among ( "ira", -1, 9, "", this),
-            new Among ( "adora", -1, 1, "", this),
-            new Among ( "osa", -1, 1, "", this),
-            new Among ( "ista", -1, 1, "", this),
-            new Among ( "iva", -1, 8, "", this),
-            new Among ( "eza", -1, 1, "", this),
-            new Among ( "log\u00EDa", -1, 2, "", this),
-            new Among ( "idade", -1, 7, "", this),
-            new Among ( "ante", -1, 1, "", this),
-            new Among ( "mente", -1, 6, "", this),
-            new Among ( "amente", 12, 5, "", this),
-            new Among ( "\u00E1vel", -1, 1, "", this),
-            new Among ( "\u00EDvel", -1, 1, "", this),
-            new Among ( "uci\u00F3n", -1, 3, "", this),
-            new Among ( "ico", -1, 1, "", this),
-            new Among ( "ismo", -1, 1, "", this),
-            new Among ( "oso", -1, 1, "", this),
-            new Among ( "amento", -1, 1, "", this),
-            new Among ( "imento", -1, 1, "", this),
-            new Among ( "ivo", -1, 8, "", this),
-            new Among ( "a\u00E7a~o", -1, 1, "", this),
-            new Among ( "ador", -1, 1, "", this),
-            new Among ( "icas", -1, 1, "", this),
-            new Among ( "\u00EAncias", -1, 4, "", this),
-            new Among ( "iras", -1, 9, "", this),
-            new Among ( "adoras", -1, 1, "", this),
-            new Among ( "osas", -1, 1, "", this),
-            new Among ( "istas", -1, 1, "", this),
-            new Among ( "ivas", -1, 8, "", this),
-            new Among ( "ezas", -1, 1, "", this),
-            new Among ( "log\u00EDas", -1, 2, "", this),
-            new Among ( "idades", -1, 7, "", this),
-            new Among ( "uciones", -1, 3, "", this),
-            new Among ( "adores", -1, 1, "", this),
-            new Among ( "antes", -1, 1, "", this),
-            new Among ( "a\u00E7o~es", -1, 1, "", this),
-            new Among ( "icos", -1, 1, "", this),
-            new Among ( "ismos", -1, 1, "", this),
-            new Among ( "osos", -1, 1, "", this),
-            new Among ( "amentos", -1, 1, "", this),
-            new Among ( "imentos", -1, 1, "", this),
-            new Among ( "ivos", -1, 8, "", this)
-        };
-
-        private Among a_6[] = {
-            new Among ( "ada", -1, 1, "", this),
-            new Among ( "ida", -1, 1, "", this),
-            new Among ( "ia", -1, 1, "", this),
-            new Among ( "aria", 2, 1, "", this),
-            new Among ( "eria", 2, 1, "", this),
-            new Among ( "iria", 2, 1, "", this),
-            new Among ( "ara", -1, 1, "", this),
-            new Among ( "era", -1, 1, "", this),
-            new Among ( "ira", -1, 1, "", this),
-            new Among ( "ava", -1, 1, "", this),
-            new Among ( "asse", -1, 1, "", this),
-            new Among ( "esse", -1, 1, "", this),
-            new Among ( "isse", -1, 1, "", this),
-            new Among ( "aste", -1, 1, "", this),
-            new Among ( "este", -1, 1, "", this),
-            new Among ( "iste", -1, 1, "", this),
-            new Among ( "ei", -1, 1, "", this),
-            new Among ( "arei", 16, 1, "", this),
-            new Among ( "erei", 16, 1, "", this),
-            new Among ( "irei", 16, 1, "", this),
-            new Among ( "am", -1, 1, "", this),
-            new Among ( "iam", 20, 1, "", this),
-            new Among ( "ariam", 21, 1, "", this),
-            new Among ( "eriam", 21, 1, "", this),
-            new Among ( "iriam", 21, 1, "", this),
-            new Among ( "aram", 20, 1, "", this),
-            new Among ( "eram", 20, 1, "", this),
-            new Among ( "iram", 20, 1, "", this),
-            new Among ( "avam", 20, 1, "", this),
-            new Among ( "em", -1, 1, "", this),
-            new Among ( "arem", 29, 1, "", this),
-            new Among ( "erem", 29, 1, "", this),
-            new Among ( "irem", 29, 1, "", this),
-            new Among ( "assem", 29, 1, "", this),
-            new Among ( "essem", 29, 1, "", this),
-            new Among ( "issem", 29, 1, "", this),
-            new Among ( "ado", -1, 1, "", this),
-            new Among ( "ido", -1, 1, "", this),
-            new Among ( "ando", -1, 1, "", this),
-            new Among ( "endo", -1, 1, "", this),
-            new Among ( "indo", -1, 1, "", this),
-            new Among ( "ara~o", -1, 1, "", this),
-            new Among ( "era~o", -1, 1, "", this),
-            new Among ( "ira~o", -1, 1, "", this),
-            new Among ( "ar", -1, 1, "", this),
-            new Among ( "er", -1, 1, "", this),
-            new Among ( "ir", -1, 1, "", this),
-            new Among ( "as", -1, 1, "", this),
-            new Among ( "adas", 47, 1, "", this),
-            new Among ( "idas", 47, 1, "", this),
-            new Among ( "ias", 47, 1, "", this),
-            new Among ( "arias", 50, 1, "", this),
-            new Among ( "erias", 50, 1, "", this),
-            new Among ( "irias", 50, 1, "", this),
-            new Among ( "aras", 47, 1, "", this),
-            new Among ( "eras", 47, 1, "", this),
-            new Among ( "iras", 47, 1, "", this),
-            new Among ( "avas", 47, 1, "", this),
-            new Among ( "es", -1, 1, "", this),
-            new Among ( "ardes", 58, 1, "", this),
-            new Among ( "erdes", 58, 1, "", this),
-            new Among ( "irdes", 58, 1, "", this),
-            new Among ( "ares", 58, 1, "", this),
-            new Among ( "eres", 58, 1, "", this),
-            new Among ( "ires", 58, 1, "", this),
-            new Among ( "asses", 58, 1, "", this),
-            new Among ( "esses", 58, 1, "", this),
-            new Among ( "isses", 58, 1, "", this),
-            new Among ( "astes", 58, 1, "", this),
-            new Among ( "estes", 58, 1, "", this),
-            new Among ( "istes", 58, 1, "", this),
-            new Among ( "is", -1, 1, "", this),
-            new Among ( "ais", 71, 1, "", this),
-            new Among ( "eis", 71, 1, "", this),
-            new Among ( "areis", 73, 1, "", this),
-            new Among ( "ereis", 73, 1, "", this),
-            new Among ( "ireis", 73, 1, "", this),
-            new Among ( "\u00E1reis", 73, 1, "", this),
-            new Among ( "\u00E9reis", 73, 1, "", this),
-            new Among ( "\u00EDreis", 73, 1, "", this),
-            new Among ( "\u00E1sseis", 73, 1, "", this),
-            new Among ( "\u00E9sseis", 73, 1, "", this),
-            new Among ( "\u00EDsseis", 73, 1, "", this),
-            new Among ( "\u00E1veis", 73, 1, "", this),
-            new Among ( "\u00EDeis", 73, 1, "", this),
-            new Among ( "ar\u00EDeis", 84, 1, "", this),
-            new Among ( "er\u00EDeis", 84, 1, "", this),
-            new Among ( "ir\u00EDeis", 84, 1, "", this),
-            new Among ( "ados", -1, 1, "", this),
-            new Among ( "idos", -1, 1, "", this),
-            new Among ( "amos", -1, 1, "", this),
-            new Among ( "\u00E1ramos", 90, 1, "", this),
-            new Among ( "\u00E9ramos", 90, 1, "", this),
-            new Among ( "\u00EDramos", 90, 1, "", this),
-            new Among ( "\u00E1vamos", 90, 1, "", this),
-            new Among ( "\u00EDamos", 90, 1, "", this),
-            new Among ( "ar\u00EDamos", 95, 1, "", this),
-            new Among ( "er\u00EDamos", 95, 1, "", this),
-            new Among ( "ir\u00EDamos", 95, 1, "", this),
-            new Among ( "emos", -1, 1, "", this),
-            new Among ( "aremos", 99, 1, "", this),
-            new Among ( "eremos", 99, 1, "", this),
-            new Among ( "iremos", 99, 1, "", this),
-            new Among ( "\u00E1ssemos", 99, 1, "", this),
-            new Among ( "\u00EAssemos", 99, 1, "", this),
-            new Among ( "\u00EDssemos", 99, 1, "", this),
-            new Among ( "imos", -1, 1, "", this),
-            new Among ( "armos", -1, 1, "", this),
-            new Among ( "ermos", -1, 1, "", this),
-            new Among ( "irmos", -1, 1, "", this),
-            new Among ( "\u00E1mos", -1, 1, "", this),
-            new Among ( "ar\u00E1s", -1, 1, "", this),
-            new Among ( "er\u00E1s", -1, 1, "", this),
-            new Among ( "ir\u00E1s", -1, 1, "", this),
-            new Among ( "eu", -1, 1, "", this),
-            new Among ( "iu", -1, 1, "", this),
-            new Among ( "ou", -1, 1, "", this),
-            new Among ( "ar\u00E1", -1, 1, "", this),
-            new Among ( "er\u00E1", -1, 1, "", this),
-            new Among ( "ir\u00E1", -1, 1, "", this)
-        };
-
-        private Among a_7[] = {
-            new Among ( "a", -1, 1, "", this),
-            new Among ( "i", -1, 1, "", this),
-            new Among ( "o", -1, 1, "", this),
-            new Among ( "os", -1, 1, "", this),
-            new Among ( "\u00E1", -1, 1, "", this),
-            new Among ( "\u00ED", -1, 1, "", this),
-            new Among ( "\u00F3", -1, 1, "", this)
-        };
-
-        private Among a_8[] = {
-            new Among ( "e", -1, 1, "", this),
-            new Among ( "\u00E7", -1, 2, "", this),
-            new Among ( "\u00E9", -1, 1, "", this),
-            new Among ( "\u00EA", -1, 1, "", this)
-        };
-
-        private static final char g_v[] = {17, 65, 16, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 19, 12, 2 };
-
-        private int I_p2;
-        private int I_p1;
-        private int I_pV;
-
-        private void copy_from(PortugueseStemmer other) {
-            I_p2 = other.I_p2;
-            I_p1 = other.I_p1;
-            I_pV = other.I_pV;
-            super.copy_from(other);
-        }
-
-        private boolean r_prelude() {
-            int among_var;
-            int v_1;
-            // repeat, line 36
-            replab0: while(true)
-            {
-                v_1 = cursor;
-                lab1: do {
-                    // (, line 36
-                    // [, line 37
-                    bra = cursor;
-                    // substring, line 37
-                    among_var = find_among(a_0, 3);
-                    if (among_var == 0)
-                    {
-                        break lab1;
-                    }
-                    // ], line 37
-                    ket = cursor;
-                    switch(among_var) {
-                        case 0:
-                            break lab1;
-                        case 1:
-                            // (, line 38
-                            // <-, line 38
-                            slice_from("a~");
-                            break;
-                        case 2:
-                            // (, line 39
-                            // <-, line 39
-                            slice_from("o~");
-                            break;
-                        case 3:
-                            // (, line 40
-                            // next, line 40
-                            if (cursor >= limit)
-                            {
-                                break lab1;
-                            }
-                            cursor++;
-                            break;
-                    }
-                    continue replab0;
-                } while (false);
-                cursor = v_1;
-                break replab0;
-            }
-            return true;
-        }
-
-        private boolean r_mark_regions() {
-            int v_1;
-            int v_2;
-            int v_3;
-            int v_6;
-            int v_8;
-            // (, line 44
-            I_pV = limit;
-            I_p1 = limit;
-            I_p2 = limit;
-            // do, line 50
-            v_1 = cursor;
-            lab0: do {
-                // (, line 50
-                // or, line 52
-                lab1: do {
-                    v_2 = cursor;
-                    lab2: do {
-                        // (, line 51
-                        if (!(in_grouping(g_v, 97, 250)))
-                        {
-                            break lab2;
-                        }
-                        // or, line 51
-                        lab3: do {
-                            v_3 = cursor;
-                            lab4: do {
-                                // (, line 51
-                                if (!(out_grouping(g_v, 97, 250)))
-                                {
-                                    break lab4;
-                                }
-                                // gopast, line 51
-                                golab5: while(true)
-                                {
-                                    lab6: do {
-                                        if (!(in_grouping(g_v, 97, 250)))
-                                        {
-                                            break lab6;
-                                        }
-                                        break golab5;
-                                    } while (false);
-                                    if (cursor >= limit)
-                                    {
-                                        break lab4;
-                                    }
-                                    cursor++;
-                                }
-                                break lab3;
-                            } while (false);
-                            cursor = v_3;
-                            // (, line 51
-                            if (!(in_grouping(g_v, 97, 250)))
-                            {
-                                break lab2;
-                            }
-                            // gopast, line 51
-                            golab7: while(true)
-                            {
-                                lab8: do {
-                                    if (!(out_grouping(g_v, 97, 250)))
-                                    {
-                                        break lab8;
-                                    }
-                                    break golab7;
-                                } while (false);
-                                if (cursor >= limit)
-                                {
-                                    break lab2;
-                                }
-                                cursor++;
-                            }
-                        } while (false);
-                        break lab1;
-                    } while (false);
-                    cursor = v_2;
-                    // (, line 53
-                    if (!(out_grouping(g_v, 97, 250)))
-                    {
-                        break lab0;
-                    }
-                    // or, line 53
-                    lab9: do {
-                        v_6 = cursor;
-                        lab10: do {
-                            // (, line 53
-                            if (!(out_grouping(g_v, 97, 250)))
-                            {
-                                break lab10;
-                            }
-                            // gopast, line 53
-                            golab11: while(true)
-                            {
-                                lab12: do {
-                                    if (!(in_grouping(g_v, 97, 250)))
-                                    {
-                                        break lab12;
-                                    }
-                                    break golab11;
-                                } while (false);
-                                if (cursor >= limit)
-                                {
-                                    break lab10;
-                                }
-                                cursor++;
-                            }
-                            break lab9;
-                        } while (false);
-                        cursor = v_6;
-                        // (, line 53
-                        if (!(in_grouping(g_v, 97, 250)))
-                        {
-                            break lab0;
-                        }
-                        // next, line 53
-                        if (cursor >= limit)
-                        {
-                            break lab0;
-                        }
-                        cursor++;
-                    } while (false);
-                } while (false);
-                // setmark pV, line 54
-                I_pV = cursor;
-            } while (false);
-            cursor = v_1;
-            // do, line 56
-            v_8 = cursor;
-            lab13: do {
-                // (, line 56
-                // gopast, line 57
-                golab14: while(true)
-                {
-                    lab15: do {
-                        if (!(in_grouping(g_v, 97, 250)))
-                        {
-                            break lab15;
-                        }
-                        break golab14;
-                    } while (false);
-                    if (cursor >= limit)
-                    {
-                        break lab13;
-                    }
-                    cursor++;
-                }
-                // gopast, line 57
-                golab16: while(true)
-                {
-                    lab17: do {
-                        if (!(out_grouping(g_v, 97, 250)))
-                        {
-                            break lab17;
-                        }
-                        break golab16;
-                    } while (false);
-                    if (cursor >= limit)
-                    {
-                        break lab13;
-                    }
-                    cursor++;
-                }
-                // setmark p1, line 57
-                I_p1 = cursor;
-                // gopast, line 58
-                golab18: while(true)
-                {
-                    lab19: do {
-                        if (!(in_grouping(g_v, 97, 250)))
-                        {
-                            break lab19;
-                        }
-                        break golab18;
-                    } while (false);
-                    if (cursor >= limit)
-                    {
-                        break lab13;
-                    }
-                    cursor++;
-                }
-                // gopast, line 58
-                golab20: while(true)
-                {
-                    lab21: do {
-                        if (!(out_grouping(g_v, 97, 250)))
-                        {
-                            break lab21;
-                        }
-                        break golab20;
-                    } while (false);
-                    if (cursor >= limit)
-                    {
-                        break lab13;
-                    }
-                    cursor++;
-                }
-                // setmark p2, line 58
-                I_p2 = cursor;
-            } while (false);
-            cursor = v_8;
-            return true;
-        }
-
-        private boolean r_postlude() {
-            int among_var;
-            int v_1;
-            // repeat, line 62
-            replab0: while(true)
-            {
-                v_1 = cursor;
-                lab1: do {
-                    // (, line 62
-                    // [, line 63
-                    bra = cursor;
-                    // substring, line 63
-                    among_var = find_among(a_1, 3);
-                    if (among_var == 0)
-                    {
-                        break lab1;
-                    }
-                    // ], line 63
-                    ket = cursor;
-                    switch(among_var) {
-                        case 0:
-                            break lab1;
-                        case 1:
-                            // (, line 64
-                            // <-, line 64
-                            slice_from("\u00E3");
-                            break;
-                        case 2:
-                            // (, line 65
-                            // <-, line 65
-                            slice_from("\u00F5");
-                            break;
-                        case 3:
-                            // (, line 66
-                            // next, line 66
-                            if (cursor >= limit)
-                            {
-                                break lab1;
-                            }
-                            cursor++;
-                            break;
-                    }
-                    continue replab0;
-                } while (false);
-                cursor = v_1;
-                break replab0;
-            }
-            return true;
-        }
-
-        private boolean r_RV() {
-            if (!(I_pV <= cursor))
-            {
-                return false;
-            }
-            return true;
-        }
-
-        private boolean r_R1() {
-            if (!(I_p1 <= cursor))
-            {
-                return false;
-            }
-            return true;
-        }
-
-        private boolean r_R2() {
-            if (!(I_p2 <= cursor))
-            {
-                return false;
-            }
-            return true;
-        }
-
-        private boolean r_standard_suffix() {
-            int among_var;
-            int v_1;
-            int v_2;
-            int v_3;
-            int v_4;
-            // (, line 76
-            // [, line 77
-            ket = cursor;
-            // substring, line 77
-            among_var = find_among_b(a_5, 45);
-            if (among_var == 0)
-            {
-                return false;
-            }
-            // ], line 77
-            bra = cursor;
-            switch(among_var) {
-                case 0:
-                    return false;
-                case 1:
-                    // (, line 92
-                    // call R2, line 93
-                    if (!r_R2())
-                    {
-                        return false;
-                    }
-                    // delete, line 93
-                    slice_del();
-                    break;
-                case 2:
-                    // (, line 97
-                    // call R2, line 98
-                    if (!r_R2())
-                    {
-                        return false;
-                    }
-                    // <-, line 98
-                    slice_from("log");
-                    break;
-                case 3:
-                    // (, line 101
-                    // call R2, line 102
-                    if (!r_R2())
-                    {
-                        return false;
-                    }
-                    // <-, line 102
-                    slice_from("u");
-                    break;
-                case 4:
-                    // (, line 105
-                    // call R2, line 106
-                    if (!r_R2())
-                    {
-                        return false;
-                    }
-                    // <-, line 106
-                    slice_from("ente");
-                    break;
-                case 5:
-                    // (, line 109
-                    // call R1, line 110
-                    if (!r_R1())
-                    {
-                        return false;
-                    }
-                    // delete, line 110
-                    slice_del();
-                    // try, line 111
-                    v_1 = limit - cursor;
-                    lab0: do {
-                        // (, line 111
-                        // [, line 112
-                        ket = cursor;
-                        // substring, line 112
-                        among_var = find_among_b(a_2, 4);
-                        if (among_var == 0)
-                        {
-                            cursor = limit - v_1;
-                            break lab0;
-                        }
-                        // ], line 112
-                        bra = cursor;
-                        // call R2, line 112
-                        if (!r_R2())
-                        {
-                            cursor = limit - v_1;
-                            break lab0;
-                        }
-                        // delete, line 112
-                        slice_del();
-                        switch(among_var) {
-                            case 0:
-                                cursor = limit - v_1;
-                                break lab0;
-                            case 1:
-                                // (, line 113
-                                // [, line 113
-                                ket = cursor;
-                                // literal, line 113
-                                if (!(eq_s_b(2, "at")))
-                                {
-                                    cursor = limit - v_1;
-                                    break lab0;
-                                }
-                                // ], line 113
-                                bra = cursor;
-                                // call R2, line 113
-                                if (!r_R2())
-                                {
-                                    cursor = limit - v_1;
-                                    break lab0;
-                                }
-                                // delete, line 113
-                                slice_del();
-                                break;
-                        }
-                    } while (false);
-                    break;
-                case 6:
-                    // (, line 121
-                    // call R2, line 122
-                    if (!r_R2())
-                    {
-                        return false;
-                    }
-                    // delete, line 122
-                    slice_del();
-                    // try, line 123
-                    v_2 = limit - cursor;
-                    lab1: do {
-                        // (, line 123
-                        // [, line 124
-                        ket = cursor;
-                        // substring, line 124
-                        among_var = find_among_b(a_3, 3);
-                        if (among_var == 0)
-                        {
-                            cursor = limit - v_2;
-                            break lab1;
-                        }
-                        // ], line 124
-                        bra = cursor;
-                        switch(among_var) {
-                            case 0:
-                                cursor = limit - v_2;
-                                break lab1;
-                            case 1:
-                                // (, line 127
-                                // call R2, line 127
-                                if (!r_R2())
-                                {
-                                    cursor = limit - v_2;
-                                    break lab1;
-                                }
-                                // delete, line 127
-                                slice_del();
-                                break;
-                        }
-                    } while (false);
-                    break;
-                case 7:
-                    // (, line 133
-                    // call R2, line 134
-                    if (!r_R2())
-                    {
-                        return false;
-                    }
-                    // delete, line 134
-                    slice_del();
-                    // try, line 135
-                    v_3 = limit - cursor;
-                    lab2: do {
-                        // (, line 135
-                        // [, line 136
-                        ket = cursor;
-                        // substring, line 136
-                        among_var = find_among_b(a_4, 3);
-                        if (among_var == 0)
-                        {
-                            cursor = limit - v_3;
-                            break lab2;
-                        }
-                        // ], line 136
-                        bra = cursor;
-                        switch(among_var) {
-                            case 0:
-                                cursor = limit - v_3;
-                                break lab2;
-                            case 1:
-                                // (, line 139
-                                // call R2, line 139
-                                if (!r_R2())
-                                {
-                                    cursor = limit - v_3;
-                                    break lab2;
-                                }
-                                // delete, line 139
-                                slice_del();
-                                break;
-                        }
-                    } while (false);
-                    break;
-                case 8:
-                    // (, line 145
-                    // call R2, line 146
-                    if (!r_R2())
-                    {
-                        return false;
-                    }
-                    // delete, line 146
-                    slice_del();
-                    // try, line 147
-                    v_4 = limit - cursor;
-                    lab3: do {
-                        // (, line 147
-                        // [, line 148
-                        ket = cursor;
-                        // literal, line 148
-                        if (!(eq_s_b(2, "at")))
-                        {
-                            cursor = limit - v_4;
-                            break lab3;
-                        }
-                        // ], line 148
-                        bra = cursor;
-                        // call R2, line 148
-                        if (!r_R2())
-                        {
-                            cursor = limit - v_4;
-                            break lab3;
-                        }
-                        // delete, line 148
-                        slice_del();
-                    } while (false);
-                    break;
-                case 9:
-                    // (, line 152
-                    // call RV, line 153
-                    if (!r_RV())
-                    {
-                        return false;
-                    }
-                    // literal, line 153
-                    if (!(eq_s_b(1, "e")))
-                    {
-                        return false;
-                    }
-                    // <-, line 154
-                    slice_from("ir");
-                    break;
-            }
-            return true;
-        }
-
-        private boolean r_verb_suffix() {
-            int among_var;
-            int v_1;
-            int v_2;
-            // setlimit, line 159
-            v_1 = limit - cursor;
-            // tomark, line 159
-            if (cursor < I_pV)
-            {
-                return false;
-            }
-            cursor = I_pV;
-            v_2 = limit_backward;
-            limit_backward = cursor;
-            cursor = limit - v_1;
-            // (, line 159
-            // [, line 160
-            ket = cursor;
-            // substring, line 160
-            among_var = find_among_b(a_6, 120);
-            if (among_var == 0)
-            {
-                limit_backward = v_2;
-                return false;
-            }
-            // ], line 160
-            bra = cursor;
-            switch(among_var) {
-                case 0:
-                    limit_backward = v_2;
-                    return false;
-                case 1:
-                    // (, line 179
-                    // delete, line 179
-                    slice_del();
-                    break;
-            }
-            limit_backward = v_2;
-            return true;
-        }
-
-        private boolean r_residual_suffix() {
-            int among_var;
-            // (, line 183
-            // [, line 184
-            ket = cursor;
-            // substring, line 184
-            among_var = find_among_b(a_7, 7);
-            if (among_var == 0)
-            {
-                return false;
-            }
-            // ], line 184
-            bra = cursor;
-            switch(among_var) {
-                case 0:
-                    return false;
-                case 1:
-                    // (, line 187
-                    // call RV, line 187
-                    if (!r_RV())
-                    {
-                        return false;
-                    }
-                    // delete, line 187
-                    slice_del();
-                    break;
-            }
-            return true;
-        }
-
-        private boolean r_residual_form() {
-            int among_var;
-            int v_1;
-            int v_2;
-            int v_3;
-            // (, line 191
-            // [, line 192
-            ket = cursor;
-            // substring, line 192
-            among_var = find_among_b(a_8, 4);
-            if (among_var == 0)
-            {
-                return false;
-            }
-            // ], line 192
-            bra = cursor;
-            switch(among_var) {
-                case 0:
-                    return false;
-                case 1:
-                    // (, line 194
-                    // call RV, line 194
-                    if (!r_RV())
-                    {
-                        return false;
-                    }
-                    // delete, line 194
-                    slice_del();
-                    // [, line 194
-                    ket = cursor;
-                    // or, line 194
-                    lab0: do {
-                        v_1 = limit - cursor;
-                        lab1: do {
-                            // (, line 194
-                            // literal, line 194
-                            if (!(eq_s_b(1, "u")))
-                            {
-                                break lab1;
-                            }
-                            // ], line 194
-                            bra = cursor;
-                            // test, line 194
-                            v_2 = limit - cursor;
-                            // literal, line 194
-                            if (!(eq_s_b(1, "g")))
-                            {
-                                break lab1;
-                            }
-                            cursor = limit - v_2;
-                            break lab0;
-                        } while (false);
-                        cursor = limit - v_1;
-                        // (, line 195
-                        // literal, line 195
-                        if (!(eq_s_b(1, "i")))
-                        {
-                            return false;
-                        }
-                        // ], line 195
-                        bra = cursor;
-                        // test, line 195
-                        v_3 = limit - cursor;
-                        // literal, line 195
-                        if (!(eq_s_b(1, "c")))
-                        {
-                            return false;
-                        }
-                        cursor = limit - v_3;
-                    } while (false);
-                    // call RV, line 195
-                    if (!r_RV())
-                    {
-                        return false;
-                    }
-                    // delete, line 195
-                    slice_del();
-                    break;
-                case 2:
-                    // (, line 196
-                    // <-, line 196
-                    slice_from("c");
-                    break;
-            }
-            return true;
-        }
-
-        public boolean stem() {
-            int v_1;
-            int v_2;
-            int v_3;
-            int v_4;
-            int v_5;
-            int v_6;
-            int v_7;
-            int v_8;
-            int v_9;
-            int v_10;
-            // (, line 201
-            // do, line 202
-            v_1 = cursor;
-            lab0: do {
-                // call prelude, line 202
-                if (!r_prelude())
-                {
-                    break lab0;
-                }
-            } while (false);
-            cursor = v_1;
-            // do, line 203
-            v_2 = cursor;
-            lab1: do {
-                // call mark_regions, line 203
-                if (!r_mark_regions())
-                {
-                    break lab1;
-                }
-            } while (false);
-            cursor = v_2;
-            // backwards, line 204
-            limit_backward = cursor; cursor = limit;
-            // (, line 204
-            // do, line 205
-            v_3 = limit - cursor;
-            lab2: do {
-                // (, line 205
-                // or, line 209
-                lab3: do {
-                    v_4 = limit - cursor;
-                    lab4: do {
-                        // (, line 206
-                        // and, line 207
-                        v_5 = limit - cursor;
-                        // (, line 206
-                        // or, line 206
-                        lab5: do {
-                            v_6 = limit - cursor;
-                            lab6: do {
-                                // call standard_suffix, line 206
-                                if (!r_standard_suffix())
-                                {
-                                    break lab6;
-                                }
-                                break lab5;
-                            } while (false);
-                            cursor = limit - v_6;
-                            // call verb_suffix, line 206
-                            if (!r_verb_suffix())
-                            {
-                                break lab4;
-                            }
-                        } while (false);
-                        cursor = limit - v_5;
-                        // do, line 207
-                        v_7 = limit - cursor;
-                        lab7: do {
-                            // (, line 207
-                            // [, line 207
-                            ket = cursor;
-                            // literal, line 207
-                            if (!(eq_s_b(1, "i")))
-                            {
-                                break lab7;
-                            }
-                            // ], line 207
-                            bra = cursor;
-                            // test, line 207
-                            v_8 = limit - cursor;
-                            // literal, line 207
-                            if (!(eq_s_b(1, "c")))
-                            {
-                                break lab7;
-                            }
-                            cursor = limit - v_8;
-                            // call RV, line 207
-                            if (!r_RV())
-                            {
-                                break lab7;
-                            }
-                            // delete, line 207
-                            slice_del();
-                        } while (false);
-                        cursor = limit - v_7;
-                        break lab3;
-                    } while (false);
-                    cursor = limit - v_4;
-                    // call residual_suffix, line 209
-                    if (!r_residual_suffix())
-                    {
-                        break lab2;
-                    }
-                } while (false);
-            } while (false);
-            cursor = limit - v_3;
-            // do, line 211
-            v_9 = limit - cursor;
-            lab8: do {
-                // call residual_form, line 211
-                if (!r_residual_form())
-                {
-                    break lab8;
-                }
-            } while (false);
-            cursor = limit - v_9;
-            cursor = limit_backward;            // do, line 213
-            v_10 = cursor;
-            lab9: do {
-                // call postlude, line 213
-                if (!r_postlude())
-                {
-                    break lab9;
-                }
-            } while (false);
-            cursor = v_10;
-            return true;
-        }
-
-}
-
Index: contrib/snowball/src/java/org/tartarus/snowball/ext/RomanianStemmer.java
===================================================================
--- contrib/snowball/src/java/org/tartarus/snowball/ext/RomanianStemmer.java	(revision 916567)
+++ contrib/snowball/src/java/org/tartarus/snowball/ext/RomanianStemmer.java	(working copy)
@@ -1,1024 +0,0 @@
-// This file was generated automatically by the Snowball to Java compiler
-
-package org.tartarus.snowball.ext;
-import org.tartarus.snowball.SnowballProgram;
-import org.tartarus.snowball.Among;
-
-/**
- * Generated class implementing code defined by a snowball script.
- */
-public class RomanianStemmer extends SnowballProgram {
-
-        private Among a_0[] = {
-            new Among ( "", -1, 3, "", this),
-            new Among ( "I", 0, 1, "", this),
-            new Among ( "U", 0, 2, "", this)
-        };
-
-        private Among a_1[] = {
-            new Among ( "ea", -1, 3, "", this),
-            new Among ( "a\u0163ia", -1, 7, "", this),
-            new Among ( "aua", -1, 2, "", this),
-            new Among ( "iua", -1, 4, "", this),
-            new Among ( "a\u0163ie", -1, 7, "", this),
-            new Among ( "ele", -1, 3, "", this),
-            new Among ( "ile", -1, 5, "", this),
-            new Among ( "iile", 6, 4, "", this),
-            new Among ( "iei", -1, 4, "", this),
-            new Among ( "atei", -1, 6, "", this),
-            new Among ( "ii", -1, 4, "", this),
-            new Among ( "ului", -1, 1, "", this),
-            new Among ( "ul", -1, 1, "", this),
-            new Among ( "elor", -1, 3, "", this),
-            new Among ( "ilor", -1, 4, "", this),
-            new Among ( "iilor", 14, 4, "", this)
-        };
-
-        private Among a_2[] = {
-            new Among ( "icala", -1, 4, "", this),
-            new Among ( "iciva", -1, 4, "", this),
-            new Among ( "ativa", -1, 5, "", this),
-            new Among ( "itiva", -1, 6, "", this),
-            new Among ( "icale", -1, 4, "", this),
-            new Among ( "a\u0163iune", -1, 5, "", this),
-            new Among ( "i\u0163iune", -1, 6, "", this),
-            new Among ( "atoare", -1, 5, "", this),
-            new Among ( "itoare", -1, 6, "", this),
-            new Among ( "\u0103toare", -1, 5, "", this),
-            new Among ( "icitate", -1, 4, "", this),
-            new Among ( "abilitate", -1, 1, "", this),
-            new Among ( "ibilitate", -1, 2, "", this),
-            new Among ( "ivitate", -1, 3, "", this),
-            new Among ( "icive", -1, 4, "", this),
-            new Among ( "ative", -1, 5, "", this),
-            new Among ( "itive", -1, 6, "", this),
-            new Among ( "icali", -1, 4, "", this),
-            new Among ( "atori", -1, 5, "", this),
-            new Among ( "icatori", 18, 4, "", this),
-            new Among ( "itori", -1, 6, "", this),
-            new Among ( "\u0103tori", -1, 5, "", this),
-            new Among ( "icitati", -1, 4, "", this),
-            new Among ( "abilitati", -1, 1, "", this),
-            new Among ( "ivitati", -1, 3, "", this),
-            new Among ( "icivi", -1, 4, "", this),
-            new Among ( "ativi", -1, 5, "", this),
-            new Among ( "itivi", -1, 6, "", this),
-            new Among ( "icit\u0103i", -1, 4, "", this),
-            new Among ( "abilit\u0103i", -1, 1, "", this),
-            new Among ( "ivit\u0103i", -1, 3, "", this),
-            new Among ( "icit\u0103\u0163i", -1, 4, "", this),
-            new Among ( "abilit\u0103\u0163i", -1, 1, "", this),
-            new Among ( "ivit\u0103\u0163i", -1, 3, "", this),
-            new Among ( "ical", -1, 4, "", this),
-            new Among ( "ator", -1, 5, "", this),
-            new Among ( "icator", 35, 4, "", this),
-            new Among ( "itor", -1, 6, "", this),
-            new Among ( "\u0103tor", -1, 5, "", this),
-            new Among ( "iciv", -1, 4, "", this),
-            new Among ( "ativ", -1, 5, "", this),
-            new Among ( "itiv", -1, 6, "", this),
-            new Among ( "ical\u0103", -1, 4, "", this),
-            new Among ( "iciv\u0103", -1, 4, "", this),
-            new Among ( "ativ\u0103", -1, 5, "", this),
-            new Among ( "itiv\u0103", -1, 6, "", this)
-        };
-
-        private Among a_3[] = {
-            new Among ( "ica", -1, 1, "", this),
-            new Among ( "abila", -1, 1, "", this),
-            new Among ( "ibila", -1, 1, "", this),
-            new Among ( "oasa", -1, 1, "", this),
-            new Among ( "ata", -1, 1, "", this),
-            new Among ( "ita", -1, 1, "", this),
-            new Among ( "anta", -1, 1, "", this),
-            new Among ( "ista", -1, 3, "", this),
-            new Among ( "uta", -1, 1, "", this),
-            new Among ( "iva", -1, 1, "", this),
-            new Among ( "ic", -1, 1, "", this),
-            new Among ( "ice", -1, 1, "", this),
-            new Among ( "abile", -1, 1, "", this),
-            new Among ( "ibile", -1, 1, "", this),
-            new Among ( "isme", -1, 3, "", this),
-            new Among ( "iune", -1, 2, "", this),
-            new Among ( "oase", -1, 1, "", this),
-            new Among ( "ate", -1, 1, "", this),
-            new Among ( "itate", 17, 1, "", this),
-            new Among ( "ite", -1, 1, "", this),
-            new Among ( "ante", -1, 1, "", this),
-            new Among ( "iste", -1, 3, "", this),
-            new Among ( "ute", -1, 1, "", this),
-            new Among ( "ive", -1, 1, "", this),
-            new Among ( "ici", -1, 1, "", this),
-            new Among ( "abili", -1, 1, "", this),
-            new Among ( "ibili", -1, 1, "", this),
-            new Among ( "iuni", -1, 2, "", this),
-            new Among ( "atori", -1, 1, "", this),
-            new Among ( "osi", -1, 1, "", this),
-            new Among ( "ati", -1, 1, "", this),
-            new Among ( "itati", 30, 1, "", this),
-            new Among ( "iti", -1, 1, "", this),
-            new Among ( "anti", -1, 1, "", this),
-            new Among ( "isti", -1, 3, "", this),
-            new Among ( "uti", -1, 1, "", this),
-            new Among ( "i\u015Fti", -1, 3, "", this),
-            new Among ( "ivi", -1, 1, "", this),
-            new Among ( "it\u0103i", -1, 1, "", this),
-            new Among ( "o\u015Fi", -1, 1, "", this),
-            new Among ( "it\u0103\u0163i", -1, 1, "", this),
-            new Among ( "abil", -1, 1, "", this),
-            new Among ( "ibil", -1, 1, "", this),
-            new Among ( "ism", -1, 3, "", this),
-            new Among ( "ator", -1, 1, "", this),
-            new Among ( "os", -1, 1, "", this),
-            new Among ( "at", -1, 1, "", this),
-            new Among ( "it", -1, 1, "", this),
-            new Among ( "ant", -1, 1, "", this),
-            new Among ( "ist", -1, 3, "", this),
-            new Among ( "ut", -1, 1, "", this),
-            new Among ( "iv", -1, 1, "", this),
-            new Among ( "ic\u0103", -1, 1, "", this),
-            new Among ( "abil\u0103", -1, 1, "", this),
-            new Among ( "ibil\u0103", -1, 1, "", this),
-            new Among ( "oas\u0103", -1, 1, "", this),
-            new Among ( "at\u0103", -1, 1, "", this),
-            new Among ( "it\u0103", -1, 1, "", this),
-            new Among ( "ant\u0103", -1, 1, "", this),
-            new Among ( "ist\u0103", -1, 3, "", this),
-            new Among ( "ut\u0103", -1, 1, "", this),
-            new Among ( "iv\u0103", -1, 1, "", this)
-        };
-
-        private Among a_4[] = {
-            new Among ( "ea", -1, 1, "", this),
-            new Among ( "ia", -1, 1, "", this),
-            new Among ( "esc", -1, 1, "", this),
-            new Among ( "\u0103sc", -1, 1, "", this),
-            new Among ( "ind", -1, 1, "", this),
-            new Among ( "\u00E2nd", -1, 1, "", this),
-            new Among ( "are", -1, 1, "", this),
-            new Among ( "ere", -1, 1, "", this),
-            new Among ( "ire", -1, 1, "", this),
-            new Among ( "\u00E2re", -1, 1, "", this),
-            new Among ( "se", -1, 2, "", this),
-            new Among ( "ase", 10, 1, "", this),
-            new Among ( "sese", 10, 2, "", this),
-            new Among ( "ise", 10, 1, "", this),
-            new Among ( "use", 10, 1, "", this),
-            new Among ( "\u00E2se", 10, 1, "", this),
-            new Among ( "e\u015Fte", -1, 1, "", this),
-            new Among ( "\u0103\u015Fte", -1, 1, "", this),
-            new Among ( "eze", -1, 1, "", this),
-            new Among ( "ai", -1, 1, "", this),
-            new Among ( "eai", 19, 1, "", this),
-            new Among ( "iai", 19, 1, "", this),
-            new Among ( "sei", -1, 2, "", this),
-            new Among ( "e\u015Fti", -1, 1, "", this),
-            new Among ( "\u0103\u015Fti", -1, 1, "", this),
-            new Among ( "ui", -1, 1, "", this),
-            new Among ( "ezi", -1, 1, "", this),
-            new Among ( "\u00E2i", -1, 1, "", this),
-            new Among ( "a\u015Fi", -1, 1, "", this),
-            new Among ( "se\u015Fi", -1, 2, "", this),
-            new Among ( "ase\u015Fi", 29, 1, "", this),
-            new Among ( "sese\u015Fi", 29, 2, "", this),
-            new Among ( "ise\u015Fi", 29, 1, "", this),
-            new Among ( "use\u015Fi", 29, 1, "", this),
-            new Among ( "\u00E2se\u015Fi", 29, 1, "", this),
-            new Among ( "i\u015Fi", -1, 1, "", this),
-            new Among ( "u\u015Fi", -1, 1, "", this),
-            new Among ( "\u00E2\u015Fi", -1, 1, "", this),
-            new Among ( "a\u0163i", -1, 2, "", this),
-            new Among ( "ea\u0163i", 38, 1, "", this),
-            new Among ( "ia\u0163i", 38, 1, "", this),
-            new Among ( "e\u0163i", -1, 2, "", this),
-            new Among ( "i\u0163i", -1, 2, "", this),
-            new Among ( "\u00E2\u0163i", -1, 2, "", this),
-            new Among ( "ar\u0103\u0163i", -1, 1, "", this),
-            new Among ( "ser\u0103\u0163i", -1, 2, "", this),
-            new Among ( "aser\u0103\u0163i", 45, 1, "", this),
-            new Among ( "seser\u0103\u0163i", 45, 2, "", this),
-            new Among ( "iser\u0103\u0163i", 45, 1, "", this),
-            new Among ( "user\u0103\u0163i", 45, 1, "", this),
-            new Among ( "\u00E2ser\u0103\u0163i", 45, 1, "", this),
-            new Among ( "ir\u0103\u0163i", -1, 1, "", this),
-            new Among ( "ur\u0103\u0163i", -1, 1, "", this),
-            new Among ( "\u00E2r\u0103\u0163i", -1, 1, "", this),
-            new Among ( "am", -1, 1, "", this),
-            new Among ( "eam", 54, 1, "", this),
-            new Among ( "iam", 54, 1, "", this),
-            new Among ( "em", -1, 2, "", this),
-            new Among ( "asem", 57, 1, "", this),
-            new Among ( "sesem", 57, 2, "", this),
-            new Among ( "isem", 57, 1, "", this),
-            new Among ( "usem", 57, 1, "", this),
-            new Among ( "\u00E2sem", 57, 1, "", this),
-            new Among ( "im", -1, 2, "", this),
-            new Among ( "\u00E2m", -1, 2, "", this),
-            new Among ( "\u0103m", -1, 2, "", this),
-            new Among ( "ar\u0103m", 65, 1, "", this),
-            new Among ( "ser\u0103m", 65, 2, "", this),
-            new Among ( "aser\u0103m", 67, 1, "", this),
-            new Among ( "seser\u0103m", 67, 2, "", this),
-            new Among ( "iser\u0103m", 67, 1, "", this),
-            new Among ( "user\u0103m", 67, 1, "", this),
-            new Among ( "\u00E2ser\u0103m", 67, 1, "", this),
-            new Among ( "ir\u0103m", 65, 1, "", this),
-            new Among ( "ur\u0103m", 65, 1, "", this),
-            new Among ( "\u00E2r\u0103m", 65, 1, "", this),
-            new Among ( "au", -1, 1, "", this),
-            new Among ( "eau", 76, 1, "", this),
-            new Among ( "iau", 76, 1, "", this),
-            new Among ( "indu", -1, 1, "", this),
-            new Among ( "\u00E2ndu", -1, 1, "", this),
-            new Among ( "ez", -1, 1, "", this),
-            new Among ( "easc\u0103", -1, 1, "", this),
-            new Among ( "ar\u0103", -1, 1, "", this),
-            new Among ( "ser\u0103", -1, 2, "", this),
-            new Among ( "aser\u0103", 84, 1, "", this),
-            new Among ( "seser\u0103", 84, 2, "", this),
-            new Among ( "iser\u0103", 84, 1, "", this),
-            new Among ( "user\u0103", 84, 1, "", this),
-            new Among ( "\u00E2ser\u0103", 84, 1, "", this),
-            new Among ( "ir\u0103", -1, 1, "", this),
-            new Among ( "ur\u0103", -1, 1, "", this),
-            new Among ( "\u00E2r\u0103", -1, 1, "", this),
-            new Among ( "eaz\u0103", -1, 1, "", this)
-        };
-
-        private Among a_5[] = {
-            new Among ( "a", -1, 1, "", this),
-            new Among ( "e", -1, 1, "", this),
-            new Among ( "ie", 1, 1, "", this),
-            new Among ( "i", -1, 1, "", this),
-            new Among ( "\u0103", -1, 1, "", this)
-        };
-
-        private static final char g_v[] = {17, 65, 16, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 32, 0, 0, 4 };
-
-        private boolean B_standard_suffix_removed;
-        private int I_p2;
-        private int I_p1;
-        private int I_pV;
-
-        private void copy_from(RomanianStemmer other) {
-            B_standard_suffix_removed = other.B_standard_suffix_removed;
-            I_p2 = other.I_p2;
-            I_p1 = other.I_p1;
-            I_pV = other.I_pV;
-            super.copy_from(other);
-        }
-
-        private boolean r_prelude() {
-            int v_1;
-            int v_2;
-            int v_3;
-            // (, line 31
-            // repeat, line 32
-            replab0: while(true)
-            {
-                v_1 = cursor;
-                lab1: do {
-                    // goto, line 32
-                    golab2: while(true)
-                    {
-                        v_2 = cursor;
-                        lab3: do {
-                            // (, line 32
-                            if (!(in_grouping(g_v, 97, 259)))
-                            {
-                                break lab3;
-                            }
-                            // [, line 33
-                            bra = cursor;
-                            // or, line 33
-                            lab4: do {
-                                v_3 = cursor;
-                                lab5: do {
-                                    // (, line 33
-                                    // literal, line 33
-                                    if (!(eq_s(1, "u")))
-                                    {
-                                        break lab5;
-                                    }
-                                    // ], line 33
-                                    ket = cursor;
-                                    if (!(in_grouping(g_v, 97, 259)))
-                                    {
-                                        break lab5;
-                                    }
-                                    // <-, line 33
-                                    slice_from("U");
-                                    break lab4;
-                                } while (false);
-                                cursor = v_3;
-                                // (, line 34
-                                // literal, line 34
-                                if (!(eq_s(1, "i")))
-                                {
-                                    break lab3;
-                                }
-                                // ], line 34
-                                ket = cursor;
-                                if (!(in_grouping(g_v, 97, 259)))
-                                {
-                                    break lab3;
-                                }
-                                // <-, line 34
-                                slice_from("I");
-                            } while (false);
-                            cursor = v_2;
-                            break golab2;
-                        } while (false);
-                        cursor = v_2;
-                        if (cursor >= limit)
-                        {
-                            break lab1;
-                        }
-                        cursor++;
-                    }
-                    continue replab0;
-                } while (false);
-                cursor = v_1;
-                break replab0;
-            }
-            return true;
-        }
-
-        private boolean r_mark_regions() {
-            int v_1;
-            int v_2;
-            int v_3;
-            int v_6;
-            int v_8;
-            // (, line 38
-            I_pV = limit;
-            I_p1 = limit;
-            I_p2 = limit;
-            // do, line 44
-            v_1 = cursor;
-            lab0: do {
-                // (, line 44
-                // or, line 46
-                lab1: do {
-                    v_2 = cursor;
-                    lab2: do {
-                        // (, line 45
-                        if (!(in_grouping(g_v, 97, 259)))
-                        {
-                            break lab2;
-                        }
-                        // or, line 45
-                        lab3: do {
-                            v_3 = cursor;
-                            lab4: do {
-                                // (, line 45
-                                if (!(out_grouping(g_v, 97, 259)))
-                                {
-                                    break lab4;
-                                }
-                                // gopast, line 45
-                                golab5: while(true)
-                                {
-                                    lab6: do {
-                                        if (!(in_grouping(g_v, 97, 259)))
-                                        {
-                                            break lab6;
-                                        }
-                                        break golab5;
-                                    } while (false);
-                                    if (cursor >= limit)
-                                    {
-                                        break lab4;
-                                    }
-                                    cursor++;
-                                }
-                                break lab3;
-                            } while (false);
-                            cursor = v_3;
-                            // (, line 45
-                            if (!(in_grouping(g_v, 97, 259)))
-                            {
-                                break lab2;
-                            }
-                            // gopast, line 45
-                            golab7: while(true)
-                            {
-                                lab8: do {
-                                    if (!(out_grouping(g_v, 97, 259)))
-                                    {
-                                        break lab8;
-                                    }
-                                    break golab7;
-                                } while (false);
-                                if (cursor >= limit)
-                                {
-                                    break lab2;
-                                }
-                                cursor++;
-                            }
-                        } while (false);
-                        break lab1;
-                    } while (false);
-                    cursor = v_2;
-                    // (, line 47
-                    if (!(out_grouping(g_v, 97, 259)))
-                    {
-                        break lab0;
-                    }
-                    // or, line 47
-                    lab9: do {
-                        v_6 = cursor;
-                        lab10: do {
-                            // (, line 47
-                            if (!(out_grouping(g_v, 97, 259)))
-                            {
-                                break lab10;
-                            }
-                            // gopast, line 47
-                            golab11: while(true)
-                            {
-                                lab12: do {
-                                    if (!(in_grouping(g_v, 97, 259)))
-                                    {
-                                        break lab12;
-                                    }
-                                    break golab11;
-                                } while (false);
-                                if (cursor >= limit)
-                                {
-                                    break lab10;
-                                }
-                                cursor++;
-                            }
-                            break lab9;
-                        } while (false);
-                        cursor = v_6;
-                        // (, line 47
-                        if (!(in_grouping(g_v, 97, 259)))
-                        {
-                            break lab0;
-                        }
-                        // next, line 47
-                        if (cursor >= limit)
-                        {
-                            break lab0;
-                        }
-                        cursor++;
-                    } while (false);
-                } while (false);
-                // setmark pV, line 48
-                I_pV = cursor;
-            } while (false);
-            cursor = v_1;
-            // do, line 50
-            v_8 = cursor;
-            lab13: do {
-                // (, line 50
-                // gopast, line 51
-                golab14: while(true)
-                {
-                    lab15: do {
-                        if (!(in_grouping(g_v, 97, 259)))
-                        {
-                            break lab15;
-                        }
-                        break golab14;
-                    } while (false);
-                    if (cursor >= limit)
-                    {
-                        break lab13;
-                    }
-                    cursor++;
-                }
-                // gopast, line 51
-                golab16: while(true)
-                {
-                    lab17: do {
-                        if (!(out_grouping(g_v, 97, 259)))
-                        {
-                            break lab17;
-                        }
-                        break golab16;
-                    } while (false);
-                    if (cursor >= limit)
-                    {
-                        break lab13;
-                    }
-                    cursor++;
-                }
-                // setmark p1, line 51
-                I_p1 = cursor;
-                // gopast, line 52
-                golab18: while(true)
-                {
-                    lab19: do {
-                        if (!(in_grouping(g_v, 97, 259)))
-                        {
-                            break lab19;
-                        }
-                        break golab18;
-                    } while (false);
-                    if (cursor >= limit)
-                    {
-                        break lab13;
-                    }
-                    cursor++;
-                }
-                // gopast, line 52
-                golab20: while(true)
-                {
-                    lab21: do {
-                        if (!(out_grouping(g_v, 97, 259)))
-                        {
-                            break lab21;
-                        }
-                        break golab20;
-                    } while (false);
-                    if (cursor >= limit)
-                    {
-                        break lab13;
-                    }
-                    cursor++;
-                }
-                // setmark p2, line 52
-                I_p2 = cursor;
-            } while (false);
-            cursor = v_8;
-            return true;
-        }
-
-        private boolean r_postlude() {
-            int among_var;
-            int v_1;
-            // repeat, line 56
-            replab0: while(true)
-            {
-                v_1 = cursor;
-                lab1: do {
-                    // (, line 56
-                    // [, line 58
-                    bra = cursor;
-                    // substring, line 58
-                    among_var = find_among(a_0, 3);
-                    if (among_var == 0)
-                    {
-                        break lab1;
-                    }
-                    // ], line 58
-                    ket = cursor;
-                    switch(among_var) {
-                        case 0:
-                            break lab1;
-                        case 1:
-                            // (, line 59
-                            // <-, line 59
-                            slice_from("i");
-                            break;
-                        case 2:
-                            // (, line 60
-                            // <-, line 60
-                            slice_from("u");
-                            break;
-                        case 3:
-                            // (, line 61
-                            // next, line 61
-                            if (cursor >= limit)
-                            {
-                                break lab1;
-                            }
-                            cursor++;
-                            break;
-                    }
-                    continue replab0;
-                } while (false);
-                cursor = v_1;
-                break replab0;
-            }
-            return true;
-        }
-
-        private boolean r_RV() {
-            if (!(I_pV <= cursor))
-            {
-                return false;
-            }
-            return true;
-        }
-
-        private boolean r_R1() {
-            if (!(I_p1 <= cursor))
-            {
-                return false;
-            }
-            return true;
-        }
-
-        private boolean r_R2() {
-            if (!(I_p2 <= cursor))
-            {
-                return false;
-            }
-            return true;
-        }
-
-        private boolean r_step_0() {
-            int among_var;
-            int v_1;
-            // (, line 72
-            // [, line 73
-            ket = cursor;
-            // substring, line 73
-            among_var = find_among_b(a_1, 16);
-            if (among_var == 0)
-            {
-                return false;
-            }
-            // ], line 73
-            bra = cursor;
-            // call R1, line 73
-            if (!r_R1())
-            {
-                return false;
-            }
-            switch(among_var) {
-                case 0:
-                    return false;
-                case 1:
-                    // (, line 75
-                    // delete, line 75
-                    slice_del();
-                    break;
-                case 2:
-                    // (, line 77
-                    // <-, line 77
-                    slice_from("a");
-                    break;
-                case 3:
-                    // (, line 79
-                    // <-, line 79
-                    slice_from("e");
-                    break;
-                case 4:
-                    // (, line 81
-                    // <-, line 81
-                    slice_from("i");
-                    break;
-                case 5:
-                    // (, line 83
-                    // not, line 83
-                    {
-                        v_1 = limit - cursor;
-                        lab0: do {
-                            // literal, line 83
-                            if (!(eq_s_b(2, "ab")))
-                            {
-                                break lab0;
-                            }
-                            return false;
-                        } while (false);
-                        cursor = limit - v_1;
-                    }
-                    // <-, line 83
-                    slice_from("i");
-                    break;
-                case 6:
-                    // (, line 85
-                    // <-, line 85
-                    slice_from("at");
-                    break;
-                case 7:
-                    // (, line 87
-                    // <-, line 87
-                    slice_from("a\u0163i");
-                    break;
-            }
-            return true;
-        }
-
-        private boolean r_combo_suffix() {
-            int among_var;
-            int v_1;
-            // test, line 91
-            v_1 = limit - cursor;
-            // (, line 91
-            // [, line 92
-            ket = cursor;
-            // substring, line 92
-            among_var = find_among_b(a_2, 46);
-            if (among_var == 0)
-            {
-                return false;
-            }
-            // ], line 92
-            bra = cursor;
-            // call R1, line 92
-            if (!r_R1())
-            {
-                return false;
-            }
-            // (, line 92
-            switch(among_var) {
-                case 0:
-                    return false;
-                case 1:
-                    // (, line 100
-                    // <-, line 101
-                    slice_from("abil");
-                    break;
-                case 2:
-                    // (, line 103
-                    // <-, line 104
-                    slice_from("ibil");
-                    break;
-                case 3:
-                    // (, line 106
-                    // <-, line 107
-                    slice_from("iv");
-                    break;
-                case 4:
-                    // (, line 112
-                    // <-, line 113
-                    slice_from("ic");
-                    break;
-                case 5:
-                    // (, line 117
-                    // <-, line 118
-                    slice_from("at");
-                    break;
-                case 6:
-                    // (, line 121
-                    // <-, line 122
-                    slice_from("it");
-                    break;
-            }
-            // set standard_suffix_removed, line 125
-            B_standard_suffix_removed = true;
-            cursor = limit - v_1;
-            return true;
-        }
-
-        private boolean r_standard_suffix() {
-            int among_var;
-            int v_1;
-            // (, line 129
-            // unset standard_suffix_removed, line 130
-            B_standard_suffix_removed = false;
-            // repeat, line 131
-            replab0: while(true)
-            {
-                v_1 = limit - cursor;
-                lab1: do {
-                    // call combo_suffix, line 131
-                    if (!r_combo_suffix())
-                    {
-                        break lab1;
-                    }
-                    continue replab0;
-                } while (false);
-                cursor = limit - v_1;
-                break replab0;
-            }
-            // [, line 132
-            ket = cursor;
-            // substring, line 132
-            among_var = find_among_b(a_3, 62);
-            if (among_var == 0)
-            {
-                return false;
-            }
-            // ], line 132
-            bra = cursor;
-            // call R2, line 132
-            if (!r_R2())
-            {
-                return false;
-            }
-            // (, line 132
-            switch(among_var) {
-                case 0:
-                    return false;
-                case 1:
-                    // (, line 148
-                    // delete, line 149
-                    slice_del();
-                    break;
-                case 2:
-                    // (, line 151
-                    // literal, line 152
-                    if (!(eq_s_b(1, "\u0163")))
-                    {
-                        return false;
-                    }
-                    // ], line 152
-                    bra = cursor;
-                    // <-, line 152
-                    slice_from("t");
-                    break;
-                case 3:
-                    // (, line 155
-                    // <-, line 156
-                    slice_from("ist");
-                    break;
-            }
-            // set standard_suffix_removed, line 160
-            B_standard_suffix_removed = true;
-            return true;
-        }
-
-        private boolean r_verb_suffix() {
-            int among_var;
-            int v_1;
-            int v_2;
-            int v_3;
-            // setlimit, line 164
-            v_1 = limit - cursor;
-            // tomark, line 164
-            if (cursor < I_pV)
-            {
-                return false;
-            }
-            cursor = I_pV;
-            v_2 = limit_backward;
-            limit_backward = cursor;
-            cursor = limit - v_1;
-            // (, line 164
-            // [, line 165
-            ket = cursor;
-            // substring, line 165
-            among_var = find_among_b(a_4, 94);
-            if (among_var == 0)
-            {
-                limit_backward = v_2;
-                return false;
-            }
-            // ], line 165
-            bra = cursor;
-            switch(among_var) {
-                case 0:
-                    limit_backward = v_2;
-                    return false;
-                case 1:
-                    // (, line 200
-                    // or, line 200
-                    lab0: do {
-                        v_3 = limit - cursor;
-                        lab1: do {
-                            if (!(out_grouping_b(g_v, 97, 259)))
-                            {
-                                break lab1;
-                            }
-                            break lab0;
-                        } while (false);
-                        cursor = limit - v_3;
-                        // literal, line 200
-                        if (!(eq_s_b(1, "u")))
-                        {
-                            limit_backward = v_2;
-                            return false;
-                        }
-                    } while (false);
-                    // delete, line 200
-                    slice_del();
-                    break;
-                case 2:
-                    // (, line 214
-                    // delete, line 214
-                    slice_del();
-                    break;
-            }
-            limit_backward = v_2;
-            return true;
-        }
-
-        private boolean r_vowel_suffix() {
-            int among_var;
-            // (, line 218
-            // [, line 219
-            ket = cursor;
-            // substring, line 219
-            among_var = find_among_b(a_5, 5);
-            if (among_var == 0)
-            {
-                return false;
-            }
-            // ], line 219
-            bra = cursor;
-            // call RV, line 219
-            if (!r_RV())
-            {
-                return false;
-            }
-            switch(among_var) {
-                case 0:
-                    return false;
-                case 1:
-                    // (, line 220
-                    // delete, line 220
-                    slice_del();
-                    break;
-            }
-            return true;
-        }
-
-        public boolean stem() {
-            int v_1;
-            int v_2;
-            int v_3;
-            int v_4;
-            int v_5;
-            int v_6;
-            int v_7;
-            int v_8;
-            // (, line 225
-            // do, line 226
-            v_1 = cursor;
-            lab0: do {
-                // call prelude, line 226
-                if (!r_prelude())
-                {
-                    break lab0;
-                }
-            } while (false);
-            cursor = v_1;
-            // do, line 227
-            v_2 = cursor;
-            lab1: do {
-                // call mark_regions, line 227
-                if (!r_mark_regions())
-                {
-                    break lab1;
-                }
-            } while (false);
-            cursor = v_2;
-            // backwards, line 228
-            limit_backward = cursor; cursor = limit;
-            // (, line 228
-            // do, line 229
-            v_3 = limit - cursor;
-            lab2: do {
-                // call step_0, line 229
-                if (!r_step_0())
-                {
-                    break lab2;
-                }
-            } while (false);
-            cursor = limit - v_3;
-            // do, line 230
-            v_4 = limit - cursor;
-            lab3: do {
-                // call standard_suffix, line 230
-                if (!r_standard_suffix())
-                {
-                    break lab3;
-                }
-            } while (false);
-            cursor = limit - v_4;
-            // do, line 231
-            v_5 = limit - cursor;
-            lab4: do {
-                // (, line 231
-                // or, line 231
-                lab5: do {
-                    v_6 = limit - cursor;
-                    lab6: do {
-                        // Boolean test standard_suffix_removed, line 231
-                        if (!(B_standard_suffix_removed))
-                        {
-                            break lab6;
-                        }
-                        break lab5;
-                    } while (false);
-                    cursor = limit - v_6;
-                    // call verb_suffix, line 231
-                    if (!r_verb_suffix())
-                    {
-                        break lab4;
-                    }
-                } while (false);
-            } while (false);
-            cursor = limit - v_5;
-            // do, line 232
-            v_7 = limit - cursor;
-            lab7: do {
-                // call vowel_suffix, line 232
-                if (!r_vowel_suffix())
-                {
-                    break lab7;
-                }
-            } while (false);
-            cursor = limit - v_7;
-            cursor = limit_backward;            // do, line 234
-            v_8 = cursor;
-            lab8: do {
-                // call postlude, line 234
-                if (!r_postlude())
-                {
-                    break lab8;
-                }
-            } while (false);
-            cursor = v_8;
-            return true;
-        }
-
-}
-
Index: contrib/snowball/src/java/org/tartarus/snowball/ext/SpanishStemmer.java
===================================================================
--- contrib/snowball/src/java/org/tartarus/snowball/ext/SpanishStemmer.java	(revision 916567)
+++ contrib/snowball/src/java/org/tartarus/snowball/ext/SpanishStemmer.java	(working copy)
@@ -1,1182 +0,0 @@
-// This file was generated automatically by the Snowball to Java compiler
-
-package org.tartarus.snowball.ext;
-import org.tartarus.snowball.SnowballProgram;
-import org.tartarus.snowball.Among;
-
-/**
- * Generated class implementing code defined by a snowball script.
- */
-public class SpanishStemmer extends SnowballProgram {
-
-        private Among a_0[] = {
-            new Among ( "", -1, 6, "", this),
-            new Among ( "\u00E1", 0, 1, "", this),
-            new Among ( "\u00E9", 0, 2, "", this),
-            new Among ( "\u00ED", 0, 3, "", this),
-            new Among ( "\u00F3", 0, 4, "", this),
-            new Among ( "\u00FA", 0, 5, "", this)
-        };
-
-        private Among a_1[] = {
-            new Among ( "la", -1, -1, "", this),
-            new Among ( "sela", 0, -1, "", this),
-            new Among ( "le", -1, -1, "", this),
-            new Among ( "me", -1, -1, "", this),
-            new Among ( "se", -1, -1, "", this),
-            new Among ( "lo", -1, -1, "", this),
-            new Among ( "selo", 5, -1, "", this),
-            new Among ( "las", -1, -1, "", this),
-            new Among ( "selas", 7, -1, "", this),
-            new Among ( "les", -1, -1, "", this),
-            new Among ( "los", -1, -1, "", this),
-            new Among ( "selos", 10, -1, "", this),
-            new Among ( "nos", -1, -1, "", this)
-        };
-
-        private Among a_2[] = {
-            new Among ( "ando", -1, 6, "", this),
-            new Among ( "iendo", -1, 6, "", this),
-            new Among ( "yendo", -1, 7, "", this),
-            new Among ( "\u00E1ndo", -1, 2, "", this),
-            new Among ( "i\u00E9ndo", -1, 1, "", this),
-            new Among ( "ar", -1, 6, "", this),
-            new Among ( "er", -1, 6, "", this),
-            new Among ( "ir", -1, 6, "", this),
-            new Among ( "\u00E1r", -1, 3, "", this),
-            new Among ( "\u00E9r", -1, 4, "", this),
-            new Among ( "\u00EDr", -1, 5, "", this)
-        };
-
-        private Among a_3[] = {
-            new Among ( "ic", -1, -1, "", this),
-            new Among ( "ad", -1, -1, "", this),
-            new Among ( "os", -1, -1, "", this),
-            new Among ( "iv", -1, 1, "", this)
-        };
-
-        private Among a_4[] = {
-            new Among ( "able", -1, 1, "", this),
-            new Among ( "ible", -1, 1, "", this),
-            new Among ( "ante", -1, 1, "", this)
-        };
-
-        private Among a_5[] = {
-            new Among ( "ic", -1, 1, "", this),
-            new Among ( "abil", -1, 1, "", this),
-            new Among ( "iv", -1, 1, "", this)
-        };
-
-        private Among a_6[] = {
-            new Among ( "ica", -1, 1, "", this),
-            new Among ( "ancia", -1, 2, "", this),
-            new Among ( "encia", -1, 5, "", this),
-            new Among ( "adora", -1, 2, "", this),
-            new Among ( "osa", -1, 1, "", this),
-            new Among ( "ista", -1, 1, "", this),
-            new Among ( "iva", -1, 9, "", this),
-            new Among ( "anza", -1, 1, "", this),
-            new Among ( "log\u00EDa", -1, 3, "", this),
-            new Among ( "idad", -1, 8, "", this),
-            new Among ( "able", -1, 1, "", this),
-            new Among ( "ible", -1, 1, "", this),
-            new Among ( "ante", -1, 2, "", this),
-            new Among ( "mente", -1, 7, "", this),
-            new Among ( "amente", 13, 6, "", this),
-            new Among ( "aci\u00F3n", -1, 2, "", this),
-            new Among ( "uci\u00F3n", -1, 4, "", this),
-            new Among ( "ico", -1, 1, "", this),
-            new Among ( "ismo", -1, 1, "", this),
-            new Among ( "oso", -1, 1, "", this),
-            new Among ( "amiento", -1, 1, "", this),
-            new Among ( "imiento", -1, 1, "", this),
-            new Among ( "ivo", -1, 9, "", this),
-            new Among ( "ador", -1, 2, "", this),
-            new Among ( "icas", -1, 1, "", this),
-            new Among ( "ancias", -1, 2, "", this),
-            new Among ( "encias", -1, 5, "", this),
-            new Among ( "adoras", -1, 2, "", this),
-            new Among ( "osas", -1, 1, "", this),
-            new Among ( "istas", -1, 1, "", this),
-            new Among ( "ivas", -1, 9, "", this),
-            new Among ( "anzas", -1, 1, "", this),
-            new Among ( "log\u00EDas", -1, 3, "", this),
-            new Among ( "idades", -1, 8, "", this),
-            new Among ( "ables", -1, 1, "", this),
-            new Among ( "ibles", -1, 1, "", this),
-            new Among ( "aciones", -1, 2, "", this),
-            new Among ( "uciones", -1, 4, "", this),
-            new Among ( "adores", -1, 2, "", this),
-            new Among ( "antes", -1, 2, "", this),
-            new Among ( "icos", -1, 1, "", this),
-            new Among ( "ismos", -1, 1, "", this),
-            new Among ( "osos", -1, 1, "", this),
-            new Among ( "amientos", -1, 1, "", this),
-            new Among ( "imientos", -1, 1, "", this),
-            new Among ( "ivos", -1, 9, "", this)
-        };
-
-        private Among a_7[] = {
-            new Among ( "ya", -1, 1, "", this),
-            new Among ( "ye", -1, 1, "", this),
-            new Among ( "yan", -1, 1, "", this),
-            new Among ( "yen", -1, 1, "", this),
-            new Among ( "yeron", -1, 1, "", this),
-            new Among ( "yendo", -1, 1, "", this),
-            new Among ( "yo", -1, 1, "", this),
-            new Among ( "yas", -1, 1, "", this),
-            new Among ( "yes", -1, 1, "", this),
-            new Among ( "yais", -1, 1, "", this),
-            new Among ( "yamos", -1, 1, "", this),
-            new Among ( "y\u00F3", -1, 1, "", this)
-        };
-
-        private Among a_8[] = {
-            new Among ( "aba", -1, 2, "", this),
-            new Among ( "ada", -1, 2, "", this),
-            new Among ( "ida", -1, 2, "", this),
-            new Among ( "ara", -1, 2, "", this),
-            new Among ( "iera", -1, 2, "", this),
-            new Among ( "\u00EDa", -1, 2, "", this),
-            new Among ( "ar\u00EDa", 5, 2, "", this),
-            new Among ( "er\u00EDa", 5, 2, "", this),
-            new Among ( "ir\u00EDa", 5, 2, "", this),
-            new Among ( "ad", -1, 2, "", this),
-            new Among ( "ed", -1, 2, "", this),
-            new Among ( "id", -1, 2, "", this),
-            new Among ( "ase", -1, 2, "", this),
-            new Among ( "iese", -1, 2, "", this),
-            new Among ( "aste", -1, 2, "", this),
-            new Among ( "iste", -1, 2, "", this),
-            new Among ( "an", -1, 2, "", this),
-            new Among ( "aban", 16, 2, "", this),
-            new Among ( "aran", 16, 2, "", this),
-            new Among ( "ieran", 16, 2, "", this),
-            new Among ( "\u00EDan", 16, 2, "", this),
-            new Among ( "ar\u00EDan", 20, 2, "", this),
-            new Among ( "er\u00EDan", 20, 2, "", this),
-            new Among ( "ir\u00EDan", 20, 2, "", this),
-            new Among ( "en", -1, 1, "", this),
-            new Among ( "asen", 24, 2, "", this),
-            new Among ( "iesen", 24, 2, "", this),
-            new Among ( "aron", -1, 2, "", this),
-            new Among ( "ieron", -1, 2, "", this),
-            new Among ( "ar\u00E1n", -1, 2, "", this),
-            new Among ( "er\u00E1n", -1, 2, "", this),
-            new Among ( "ir\u00E1n", -1, 2, "", this),
-            new Among ( "ado", -1, 2, "", this),
-            new Among ( "ido", -1, 2, "", this),
-            new Among ( "ando", -1, 2, "", this),
-            new Among ( "iendo", -1, 2, "", this),
-            new Among ( "ar", -1, 2, "", this),
-            new Among ( "er", -1, 2, "", this),
-            new Among ( "ir", -1, 2, "", this),
-            new Among ( "as", -1, 2, "", this),
-            new Among ( "abas", 39, 2, "", this),
-            new Among ( "adas", 39, 2, "", this),
-            new Among ( "idas", 39, 2, "", this),
-            new Among ( "aras", 39, 2, "", this),
-            new Among ( "ieras", 39, 2, "", this),
-            new Among ( "\u00EDas", 39, 2, "", this),
-            new Among ( "ar\u00EDas", 45, 2, "", this),
-            new Among ( "er\u00EDas", 45, 2, "", this),
-            new Among ( "ir\u00EDas", 45, 2, "", this),
-            new Among ( "es", -1, 1, "", this),
-            new Among ( "ases", 49, 2, "", this),
-            new Among ( "ieses", 49, 2, "", this),
-            new Among ( "abais", -1, 2, "", this),
-            new Among ( "arais", -1, 2, "", this),
-            new Among ( "ierais", -1, 2, "", this),
-            new Among ( "\u00EDais", -1, 2, "", this),
-            new Among ( "ar\u00EDais", 55, 2, "", this),
-            new Among ( "er\u00EDais", 55, 2, "", this),
-            new Among ( "ir\u00EDais", 55, 2, "", this),
-            new Among ( "aseis", -1, 2, "", this),
-            new Among ( "ieseis", -1, 2, "", this),
-            new Among ( "asteis", -1, 2, "", this),
-            new Among ( "isteis", -1, 2, "", this),
-            new Among ( "\u00E1is", -1, 2, "", this),
-            new Among ( "\u00E9is", -1, 1, "", this),
-            new Among ( "ar\u00E9is", 64, 2, "", this),
-            new Among ( "er\u00E9is", 64, 2, "", this),
-            new Among ( "ir\u00E9is", 64, 2, "", this),
-            new Among ( "ados", -1, 2, "", this),
-            new Among ( "idos", -1, 2, "", this),
-            new Among ( "amos", -1, 2, "", this),
-            new Among ( "\u00E1bamos", 70, 2, "", this),
-            new Among ( "\u00E1ramos", 70, 2, "", this),
-            new Among ( "i\u00E9ramos", 70, 2, "", this),
-            new Among ( "\u00EDamos", 70, 2, "", this),
-            new Among ( "ar\u00EDamos", 74, 2, "", this),
-            new Among ( "er\u00EDamos", 74, 2, "", this),
-            new Among ( "ir\u00EDamos", 74, 2, "", this),
-            new Among ( "emos", -1, 1, "", this),
-            new Among ( "aremos", 78, 2, "", this),
-            new Among ( "eremos", 78, 2, "", this),
-            new Among ( "iremos", 78, 2, "", this),
-            new Among ( "\u00E1semos", 78, 2, "", this),
-            new Among ( "i\u00E9semos", 78, 2, "", this),
-            new Among ( "imos", -1, 2, "", this),
-            new Among ( "ar\u00E1s", -1, 2, "", this),
-            new Among ( "er\u00E1s", -1, 2, "", this),
-            new Among ( "ir\u00E1s", -1, 2, "", this),
-            new Among ( "\u00EDs", -1, 2, "", this),
-            new Among ( "ar\u00E1", -1, 2, "", this),
-            new Among ( "er\u00E1", -1, 2, "", this),
-            new Among ( "ir\u00E1", -1, 2, "", this),
-            new Among ( "ar\u00E9", -1, 2, "", this),
-            new Among ( "er\u00E9", -1, 2, "", this),
-            new Among ( "ir\u00E9", -1, 2, "", this),
-            new Among ( "i\u00F3", -1, 2, "", this)
-        };
-
-        private Among a_9[] = {
-            new Among ( "a", -1, 1, "", this),
-            new Among ( "e", -1, 2, "", this),
-            new Among ( "o", -1, 1, "", this),
-            new Among ( "os", -1, 1, "", this),
-            new Among ( "\u00E1", -1, 1, "", this),
-            new Among ( "\u00E9", -1, 2, "", this),
-            new Among ( "\u00ED", -1, 1, "", this),
-            new Among ( "\u00F3", -1, 1, "", this)
-        };
-
-        private static final char g_v[] = {17, 65, 16, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 17, 4, 10 };
-
-        private int I_p2;
-        private int I_p1;
-        private int I_pV;
-
-        private void copy_from(SpanishStemmer other) {
-            I_p2 = other.I_p2;
-            I_p1 = other.I_p1;
-            I_pV = other.I_pV;
-            super.copy_from(other);
-        }
-
-        private boolean r_mark_regions() {
-            int v_1;
-            int v_2;
-            int v_3;
-            int v_6;
-            int v_8;
-            // (, line 31
-            I_pV = limit;
-            I_p1 = limit;
-            I_p2 = limit;
-            // do, line 37
-            v_1 = cursor;
-            lab0: do {
-                // (, line 37
-                // or, line 39
-                lab1: do {
-                    v_2 = cursor;
-                    lab2: do {
-                        // (, line 38
-                        if (!(in_grouping(g_v, 97, 252)))
-                        {
-                            break lab2;
-                        }
-                        // or, line 38
-                        lab3: do {
-                            v_3 = cursor;
-                            lab4: do {
-                                // (, line 38
-                                if (!(out_grouping(g_v, 97, 252)))
-                                {
-                                    break lab4;
-                                }
-                                // gopast, line 38
-                                golab5: while(true)
-                                {
-                                    lab6: do {
-                                        if (!(in_grouping(g_v, 97, 252)))
-                                        {
-                                            break lab6;
-                                        }
-                                        break golab5;
-                                    } while (false);
-                                    if (cursor >= limit)
-                                    {
-                                        break lab4;
-                                    }
-                                    cursor++;
-                                }
-                                break lab3;
-                            } while (false);
-                            cursor = v_3;
-                            // (, line 38
-                            if (!(in_grouping(g_v, 97, 252)))
-                            {
-                                break lab2;
-                            }
-                            // gopast, line 38
-                            golab7: while(true)
-                            {
-                                lab8: do {
-                                    if (!(out_grouping(g_v, 97, 252)))
-                                    {
-                                        break lab8;
-                                    }
-                                    break golab7;
-                                } while (false);
-                                if (cursor >= limit)
-                                {
-                                    break lab2;
-                                }
-                                cursor++;
-                            }
-                        } while (false);
-                        break lab1;
-                    } while (false);
-                    cursor = v_2;
-                    // (, line 40
-                    if (!(out_grouping(g_v, 97, 252)))
-                    {
-                        break lab0;
-                    }
-                    // or, line 40
-                    lab9: do {
-                        v_6 = cursor;
-                        lab10: do {
-                            // (, line 40
-                            if (!(out_grouping(g_v, 97, 252)))
-                            {
-                                break lab10;
-                            }
-                            // gopast, line 40
-                            golab11: while(true)
-                            {
-                                lab12: do {
-                                    if (!(in_grouping(g_v, 97, 252)))
-                                    {
-                                        break lab12;
-                                    }
-                                    break golab11;
-                                } while (false);
-                                if (cursor >= limit)
-                                {
-                                    break lab10;
-                                }
-                                cursor++;
-                            }
-                            break lab9;
-                        } while (false);
-                        cursor = v_6;
-                        // (, line 40
-                        if (!(in_grouping(g_v, 97, 252)))
-                        {
-                            break lab0;
-                        }
-                        // next, line 40
-                        if (cursor >= limit)
-                        {
-                            break lab0;
-                        }
-                        cursor++;
-                    } while (false);
-                } while (false);
-                // setmark pV, line 41
-                I_pV = cursor;
-            } while (false);
-            cursor = v_1;
-            // do, line 43
-            v_8 = cursor;
-            lab13: do {
-                // (, line 43
-                // gopast, line 44
-                golab14: while(true)
-                {
-                    lab15: do {
-                        if (!(in_grouping(g_v, 97, 252)))
-                        {
-                            break lab15;
-                        }
-                        break golab14;
-                    } while (false);
-                    if (cursor >= limit)
-                    {
-                        break lab13;
-                    }
-                    cursor++;
-                }
-                // gopast, line 44
-                golab16: while(true)
-                {
-                    lab17: do {
-                        if (!(out_grouping(g_v, 97, 252)))
-                        {
-                            break lab17;
-                        }
-                        break golab16;
-                    } while (false);
-                    if (cursor >= limit)
-                    {
-                        break lab13;
-                    }
-                    cursor++;
-                }
-                // setmark p1, line 44
-                I_p1 = cursor;
-                // gopast, line 45
-                golab18: while(true)
-                {
-                    lab19: do {
-                        if (!(in_grouping(g_v, 97, 252)))
-                        {
-                            break lab19;
-                        }
-                        break golab18;
-                    } while (false);
-                    if (cursor >= limit)
-                    {
-                        break lab13;
-                    }
-                    cursor++;
-                }
-                // gopast, line 45
-                golab20: while(true)
-                {
-                    lab21: do {
-                        if (!(out_grouping(g_v, 97, 252)))
-                        {
-                            break lab21;
-                        }
-                        break golab20;
-                    } while (false);
-                    if (cursor >= limit)
-                    {
-                        break lab13;
-                    }
-                    cursor++;
-                }
-                // setmark p2, line 45
-                I_p2 = cursor;
-            } while (false);
-            cursor = v_8;
-            return true;
-        }
-
-        private boolean r_postlude() {
-            int among_var;
-            int v_1;
-            // repeat, line 49
-            replab0: while(true)
-            {
-                v_1 = cursor;
-                lab1: do {
-                    // (, line 49
-                    // [, line 50
-                    bra = cursor;
-                    // substring, line 50
-                    among_var = find_among(a_0, 6);
-                    if (among_var == 0)
-                    {
-                        break lab1;
-                    }
-                    // ], line 50
-                    ket = cursor;
-                    switch(among_var) {
-                        case 0:
-                            break lab1;
-                        case 1:
-                            // (, line 51
-                            // <-, line 51
-                            slice_from("a");
-                            break;
-                        case 2:
-                            // (, line 52
-                            // <-, line 52
-                            slice_from("e");
-                            break;
-                        case 3:
-                            // (, line 53
-                            // <-, line 53
-                            slice_from("i");
-                            break;
-                        case 4:
-                            // (, line 54
-                            // <-, line 54
-                            slice_from("o");
-                            break;
-                        case 5:
-                            // (, line 55
-                            // <-, line 55
-                            slice_from("u");
-                            break;
-                        case 6:
-                            // (, line 57
-                            // next, line 57
-                            if (cursor >= limit)
-                            {
-                                break lab1;
-                            }
-                            cursor++;
-                            break;
-                    }
-                    continue replab0;
-                } while (false);
-                cursor = v_1;
-                break replab0;
-            }
-            return true;
-        }
-
-        private boolean r_RV() {
-            if (!(I_pV <= cursor))
-            {
-                return false;
-            }
-            return true;
-        }
-
-        private boolean r_R1() {
-            if (!(I_p1 <= cursor))
-            {
-                return false;
-            }
-            return true;
-        }
-
-        private boolean r_R2() {
-            if (!(I_p2 <= cursor))
-            {
-                return false;
-            }
-            return true;
-        }
-
-        private boolean r_attached_pronoun() {
-            int among_var;
-            // (, line 67
-            // [, line 68
-            ket = cursor;
-            // substring, line 68
-            if (find_among_b(a_1, 13) == 0)
-            {
-                return false;
-            }
-            // ], line 68
-            bra = cursor;
-            // substring, line 72
-            among_var = find_among_b(a_2, 11);
-            if (among_var == 0)
-            {
-                return false;
-            }
-            // call RV, line 72
-            if (!r_RV())
-            {
-                return false;
-            }
-            switch(among_var) {
-                case 0:
-                    return false;
-                case 1:
-                    // (, line 73
-                    // ], line 73
-                    bra = cursor;
-                    // <-, line 73
-                    slice_from("iendo");
-                    break;
-                case 2:
-                    // (, line 74
-                    // ], line 74
-                    bra = cursor;
-                    // <-, line 74
-                    slice_from("ando");
-                    break;
-                case 3:
-                    // (, line 75
-                    // ], line 75
-                    bra = cursor;
-                    // <-, line 75
-                    slice_from("ar");
-                    break;
-                case 4:
-                    // (, line 76
-                    // ], line 76
-                    bra = cursor;
-                    // <-, line 76
-                    slice_from("er");
-                    break;
-                case 5:
-                    // (, line 77
-                    // ], line 77
-                    bra = cursor;
-                    // <-, line 77
-                    slice_from("ir");
-                    break;
-                case 6:
-                    // (, line 81
-                    // delete, line 81
-                    slice_del();
-                    break;
-                case 7:
-                    // (, line 82
-                    // literal, line 82
-                    if (!(eq_s_b(1, "u")))
-                    {
-                        return false;
-                    }
-                    // delete, line 82
-                    slice_del();
-                    break;
-            }
-            return true;
-        }
-
-        private boolean r_standard_suffix() {
-            int among_var;
-            int v_1;
-            int v_2;
-            int v_3;
-            int v_4;
-            int v_5;
-            // (, line 86
-            // [, line 87
-            ket = cursor;
-            // substring, line 87
-            among_var = find_among_b(a_6, 46);
-            if (among_var == 0)
-            {
-                return false;
-            }
-            // ], line 87
-            bra = cursor;
-            switch(among_var) {
-                case 0:
-                    return false;
-                case 1:
-                    // (, line 98
-                    // call R2, line 99
-                    if (!r_R2())
-                    {
-                        return false;
-                    }
-                    // delete, line 99
-                    slice_del();
-                    break;
-                case 2:
-                    // (, line 104
-                    // call R2, line 105
-                    if (!r_R2())
-                    {
-                        return false;
-                    }
-                    // delete, line 105
-                    slice_del();
-                    // try, line 106
-                    v_1 = limit - cursor;
-                    lab0: do {
-                        // (, line 106
-                        // [, line 106
-                        ket = cursor;
-                        // literal, line 106
-                        if (!(eq_s_b(2, "ic")))
-                        {
-                            cursor = limit - v_1;
-                            break lab0;
-                        }
-                        // ], line 106
-                        bra = cursor;
-                        // call R2, line 106
-                        if (!r_R2())
-                        {
-                            cursor = limit - v_1;
-                            break lab0;
-                        }
-                        // delete, line 106
-                        slice_del();
-                    } while (false);
-                    break;
-                case 3:
-                    // (, line 110
-                    // call R2, line 111
-                    if (!r_R2())
-                    {
-                        return false;
-                    }
-                    // <-, line 111
-                    slice_from("log");
-                    break;
-                case 4:
-                    // (, line 114
-                    // call R2, line 115
-                    if (!r_R2())
-                    {
-                        return false;
-                    }
-                    // <-, line 115
-                    slice_from("u");
-                    break;
-                case 5:
-                    // (, line 118
-                    // call R2, line 119
-                    if (!r_R2())
-                    {
-                        return false;
-                    }
-                    // <-, line 119
-                    slice_from("ente");
-                    break;
-                case 6:
-                    // (, line 122
-                    // call R1, line 123
-                    if (!r_R1())
-                    {
-                        return false;
-                    }
-                    // delete, line 123
-                    slice_del();
-                    // try, line 124
-                    v_2 = limit - cursor;
-                    lab1: do {
-                        // (, line 124
-                        // [, line 125
-                        ket = cursor;
-                        // substring, line 125
-                        among_var = find_among_b(a_3, 4);
-                        if (among_var == 0)
-                        {
-                            cursor = limit - v_2;
-                            break lab1;
-                        }
-                        // ], line 125
-                        bra = cursor;
-                        // call R2, line 125
-                        if (!r_R2())
-                        {
-                            cursor = limit - v_2;
-                            break lab1;
-                        }
-                        // delete, line 125
-                        slice_del();
-                        switch(among_var) {
-                            case 0:
-                                cursor = limit - v_2;
-                                break lab1;
-                            case 1:
-                                // (, line 126
-                                // [, line 126
-                                ket = cursor;
-                                // literal, line 126
-                                if (!(eq_s_b(2, "at")))
-                                {
-                                    cursor = limit - v_2;
-                                    break lab1;
-                                }
-                                // ], line 126
-                                bra = cursor;
-                                // call R2, line 126
-                                if (!r_R2())
-                                {
-                                    cursor = limit - v_2;
-                                    break lab1;
-                                }
-                                // delete, line 126
-                                slice_del();
-                                break;
-                        }
-                    } while (false);
-                    break;
-                case 7:
-                    // (, line 134
-                    // call R2, line 135
-                    if (!r_R2())
-                    {
-                        return false;
-                    }
-                    // delete, line 135
-                    slice_del();
-                    // try, line 136
-                    v_3 = limit - cursor;
-                    lab2: do {
-                        // (, line 136
-                        // [, line 137
-                        ket = cursor;
-                        // substring, line 137
-                        among_var = find_among_b(a_4, 3);
-                        if (among_var == 0)
-                        {
-                            cursor = limit - v_3;
-                            break lab2;
-                        }
-                        // ], line 137
-                        bra = cursor;
-                        switch(among_var) {
-                            case 0:
-                                cursor = limit - v_3;
-                                break lab2;
-                            case 1:
-                                // (, line 140
-                                // call R2, line 140
-                                if (!r_R2())
-                                {
-                                    cursor = limit - v_3;
-                                    break lab2;
-                                }
-                                // delete, line 140
-                                slice_del();
-                                break;
-                        }
-                    } while (false);
-                    break;
-                case 8:
-                    // (, line 146
-                    // call R2, line 147
-                    if (!r_R2())
-                    {
-                        return false;
-                    }
-                    // delete, line 147
-                    slice_del();
-                    // try, line 148
-                    v_4 = limit - cursor;
-                    lab3: do {
-                        // (, line 148
-                        // [, line 149
-                        ket = cursor;
-                        // substring, line 149
-                        among_var = find_among_b(a_5, 3);
-                        if (among_var == 0)
-                        {
-                            cursor = limit - v_4;
-                            break lab3;
-                        }
-                        // ], line 149
-                        bra = cursor;
-                        switch(among_var) {
-                            case 0:
-                                cursor = limit - v_4;
-                                break lab3;
-                            case 1:
-                                // (, line 152
-                                // call R2, line 152
-                                if (!r_R2())
-                                {
-                                    cursor = limit - v_4;
-                                    break lab3;
-                                }
-                                // delete, line 152
-                                slice_del();
-                                break;
-                        }
-                    } while (false);
-                    break;
-                case 9:
-                    // (, line 158
-                    // call R2, line 159
-                    if (!r_R2())
-                    {
-                        return false;
-                    }
-                    // delete, line 159
-                    slice_del();
-                    // try, line 160
-                    v_5 = limit - cursor;
-                    lab4: do {
-                        // (, line 160
-                        // [, line 161
-                        ket = cursor;
-                        // literal, line 161
-                        if (!(eq_s_b(2, "at")))
-                        {
-                            cursor = limit - v_5;
-                            break lab4;
-                        }
-                        // ], line 161
-                        bra = cursor;
-                        // call R2, line 161
-                        if (!r_R2())
-                        {
-                            cursor = limit - v_5;
-                            break lab4;
-                        }
-                        // delete, line 161
-                        slice_del();
-                    } while (false);
-                    break;
-            }
-            return true;
-        }
-
-        private boolean r_y_verb_suffix() {
-            int among_var;
-            int v_1;
-            int v_2;
-            // (, line 167
-            // setlimit, line 168
-            v_1 = limit - cursor;
-            // tomark, line 168
-            if (cursor < I_pV)
-            {
-                return false;
-            }
-            cursor = I_pV;
-            v_2 = limit_backward;
-            limit_backward = cursor;
-            cursor = limit - v_1;
-            // (, line 168
-            // [, line 168
-            ket = cursor;
-            // substring, line 168
-            among_var = find_among_b(a_7, 12);
-            if (among_var == 0)
-            {
-                limit_backward = v_2;
-                return false;
-            }
-            // ], line 168
-            bra = cursor;
-            limit_backward = v_2;
-            switch(among_var) {
-                case 0:
-                    return false;
-                case 1:
-                    // (, line 171
-                    // literal, line 171
-                    if (!(eq_s_b(1, "u")))
-                    {
-                        return false;
-                    }
-                    // delete, line 171
-                    slice_del();
-                    break;
-            }
-            return true;
-        }
-
-        private boolean r_verb_suffix() {
-            int among_var;
-            int v_1;
-            int v_2;
-            int v_3;
-            int v_4;
-            // (, line 175
-            // setlimit, line 176
-            v_1 = limit - cursor;
-            // tomark, line 176
-            if (cursor < I_pV)
-            {
-                return false;
-            }
-            cursor = I_pV;
-            v_2 = limit_backward;
-            limit_backward = cursor;
-            cursor = limit - v_1;
-            // (, line 176
-            // [, line 176
-            ket = cursor;
-            // substring, line 176
-            among_var = find_among_b(a_8, 96);
-            if (among_var == 0)
-            {
-                limit_backward = v_2;
-                return false;
-            }
-            // ], line 176
-            bra = cursor;
-            limit_backward = v_2;
-            switch(among_var) {
-                case 0:
-                    return false;
-                case 1:
-                    // (, line 179
-                    // try, line 179
-                    v_3 = limit - cursor;
-                    lab0: do {
-                        // (, line 179
-                        // literal, line 179
-                        if (!(eq_s_b(1, "u")))
-                        {
-                            cursor = limit - v_3;
-                            break lab0;
-                        }
-                        // test, line 179
-                        v_4 = limit - cursor;
-                        // literal, line 179
-                        if (!(eq_s_b(1, "g")))
-                        {
-                            cursor = limit - v_3;
-                            break lab0;
-                        }
-                        cursor = limit - v_4;
-                    } while (false);
-                    // ], line 179
-                    bra = cursor;
-                    // delete, line 179
-                    slice_del();
-                    break;
-                case 2:
-                    // (, line 200
-                    // delete, line 200
-                    slice_del();
-                    break;
-            }
-            return true;
-        }
-
-        private boolean r_residual_suffix() {
-            int among_var;
-            int v_1;
-            int v_2;
-            // (, line 204
-            // [, line 205
-            ket = cursor;
-            // substring, line 205
-            among_var = find_among_b(a_9, 8);
-            if (among_var == 0)
-            {
-                return false;
-            }
-            // ], line 205
-            bra = cursor;
-            switch(among_var) {
-                case 0:
-                    return false;
-                case 1:
-                    // (, line 208
-                    // call RV, line 208
-                    if (!r_RV())
-                    {
-                        return false;
-                    }
-                    // delete, line 208
-                    slice_del();
-                    break;
-                case 2:
-                    // (, line 210
-                    // call RV, line 210
-                    if (!r_RV())
-                    {
-                        return false;
-                    }
-                    // delete, line 210
-                    slice_del();
-                    // try, line 210
-                    v_1 = limit - cursor;
-                    lab0: do {
-                        // (, line 210
-                        // [, line 210
-                        ket = cursor;
-                        // literal, line 210
-                        if (!(eq_s_b(1, "u")))
-                        {
-                            cursor = limit - v_1;
-                            break lab0;
-                        }
-                        // ], line 210
-                        bra = cursor;
-                        // test, line 210
-                        v_2 = limit - cursor;
-                        // literal, line 210
-                        if (!(eq_s_b(1, "g")))
-                        {
-                            cursor = limit - v_1;
-                            break lab0;
-                        }
-                        cursor = limit - v_2;
-                        // call RV, line 210
-                        if (!r_RV())
-                        {
-                            cursor = limit - v_1;
-                            break lab0;
-                        }
-                        // delete, line 210
-                        slice_del();
-                    } while (false);
-                    break;
-            }
-            return true;
-        }
-
-        public boolean stem() {
-            int v_1;
-            int v_2;
-            int v_3;
-            int v_4;
-            int v_5;
-            int v_6;
-            // (, line 215
-            // do, line 216
-            v_1 = cursor;
-            lab0: do {
-                // call mark_regions, line 216
-                if (!r_mark_regions())
-                {
-                    break lab0;
-                }
-            } while (false);
-            cursor = v_1;
-            // backwards, line 217
-            limit_backward = cursor; cursor = limit;
-            // (, line 217
-            // do, line 218
-            v_2 = limit - cursor;
-            lab1: do {
-                // call attached_pronoun, line 218
-                if (!r_attached_pronoun())
-                {
-                    break lab1;
-                }
-            } while (false);
-            cursor = limit - v_2;
-            // do, line 219
-            v_3 = limit - cursor;
-            lab2: do {
-                // (, line 219
-                // or, line 219
-                lab3: do {
-                    v_4 = limit - cursor;
-                    lab4: do {
-                        // call standard_suffix, line 219
-                        if (!r_standard_suffix())
-                        {
-                            break lab4;
-                        }
-                        break lab3;
-                    } while (false);
-                    cursor = limit - v_4;
-                    lab5: do {
-                        // call y_verb_suffix, line 220
-                        if (!r_y_verb_suffix())
-                        {
-                            break lab5;
-                        }
-                        break lab3;
-                    } while (false);
-                    cursor = limit - v_4;
-                    // call verb_suffix, line 221
-                    if (!r_verb_suffix())
-                    {
-                        break lab2;
-                    }
-                } while (false);
-            } while (false);
-            cursor = limit - v_3;
-            // do, line 223
-            v_5 = limit - cursor;
-            lab6: do {
-                // call residual_suffix, line 223
-                if (!r_residual_suffix())
-                {
-                    break lab6;
-                }
-            } while (false);
-            cursor = limit - v_5;
-            cursor = limit_backward;            // do, line 225
-            v_6 = cursor;
-            lab7: do {
-                // call postlude, line 225
-                if (!r_postlude())
-                {
-                    break lab7;
-                }
-            } while (false);
-            cursor = v_6;
-            return true;
-        }
-
-}
-
Index: contrib/snowball/src/java/org/tartarus/snowball/ext/FrenchStemmer.java
===================================================================
--- contrib/snowball/src/java/org/tartarus/snowball/ext/FrenchStemmer.java	(revision 916567)
+++ contrib/snowball/src/java/org/tartarus/snowball/ext/FrenchStemmer.java	(working copy)
@@ -1,1501 +0,0 @@
-// This file was generated automatically by the Snowball to Java compiler
-
-package org.tartarus.snowball.ext;
-import org.tartarus.snowball.SnowballProgram;
-import org.tartarus.snowball.Among;
-
-/**
- * Generated class implementing code defined by a snowball script.
- */
-public class FrenchStemmer extends SnowballProgram {
-
-        private Among a_0[] = {
-            new Among ( "col", -1, -1, "", this),
-            new Among ( "par", -1, -1, "", this),
-            new Among ( "tap", -1, -1, "", this)
-        };
-
-        private Among a_1[] = {
-            new Among ( "", -1, 4, "", this),
-            new Among ( "I", 0, 1, "", this),
-            new Among ( "U", 0, 2, "", this),
-            new Among ( "Y", 0, 3, "", this)
-        };
-
-        private Among a_2[] = {
-            new Among ( "iqU", -1, 3, "", this),
-            new Among ( "abl", -1, 3, "", this),
-            new Among ( "I\u00E8r", -1, 4, "", this),
-            new Among ( "i\u00E8r", -1, 4, "", this),
-            new Among ( "eus", -1, 2, "", this),
-            new Among ( "iv", -1, 1, "", this)
-        };
-
-        private Among a_3[] = {
-            new Among ( "ic", -1, 2, "", this),
-            new Among ( "abil", -1, 1, "", this),
-            new Among ( "iv", -1, 3, "", this)
-        };
-
-        private Among a_4[] = {
-            new Among ( "iqUe", -1, 1, "", this),
-            new Among ( "atrice", -1, 2, "", this),
-            new Among ( "ance", -1, 1, "", this),
-            new Among ( "ence", -1, 5, "", this),
-            new Among ( "logie", -1, 3, "", this),
-            new Among ( "able", -1, 1, "", this),
-            new Among ( "isme", -1, 1, "", this),
-            new Among ( "euse", -1, 11, "", this),
-            new Among ( "iste", -1, 1, "", this),
-            new Among ( "ive", -1, 8, "", this),
-            new Among ( "if", -1, 8, "", this),
-            new Among ( "usion", -1, 4, "", this),
-            new Among ( "ation", -1, 2, "", this),
-            new Among ( "ution", -1, 4, "", this),
-            new Among ( "ateur", -1, 2, "", this),
-            new Among ( "iqUes", -1, 1, "", this),
-            new Among ( "atrices", -1, 2, "", this),
-            new Among ( "ances", -1, 1, "", this),
-            new Among ( "ences", -1, 5, "", this),
-            new Among ( "logies", -1, 3, "", this),
-            new Among ( "ables", -1, 1, "", this),
-            new Among ( "ismes", -1, 1, "", this),
-            new Among ( "euses", -1, 11, "", this),
-            new Among ( "istes", -1, 1, "", this),
-            new Among ( "ives", -1, 8, "", this),
-            new Among ( "ifs", -1, 8, "", this),
-            new Among ( "usions", -1, 4, "", this),
-            new Among ( "ations", -1, 2, "", this),
-            new Among ( "utions", -1, 4, "", this),
-            new Among ( "ateurs", -1, 2, "", this),
-            new Among ( "ments", -1, 15, "", this),
-            new Among ( "ements", 30, 6, "", this),
-            new Among ( "issements", 31, 12, "", this),
-            new Among ( "it\u00E9s", -1, 7, "", this),
-            new Among ( "ment", -1, 15, "", this),
-            new Among ( "ement", 34, 6, "", this),
-            new Among ( "issement", 35, 12, "", this),
-            new Among ( "amment", 34, 13, "", this),
-            new Among ( "emment", 34, 14, "", this),
-            new Among ( "aux", -1, 10, "", this),
-            new Among ( "eaux", 39, 9, "", this),
-            new Among ( "eux", -1, 1, "", this),
-            new Among ( "it\u00E9", -1, 7, "", this)
-        };
-
-        private Among a_5[] = {
-            new Among ( "ira", -1, 1, "", this),
-            new Among ( "ie", -1, 1, "", this),
-            new Among ( "isse", -1, 1, "", this),
-            new Among ( "issante", -1, 1, "", this),
-            new Among ( "i", -1, 1, "", this),
-            new Among ( "irai", 4, 1, "", this),
-            new Among ( "ir", -1, 1, "", this),
-            new Among ( "iras", -1, 1, "", this),
-            new Among ( "ies", -1, 1, "", this),
-            new Among ( "\u00EEmes", -1, 1, "", this),
-            new Among ( "isses", -1, 1, "", this),
-            new Among ( "issantes", -1, 1, "", this),
-            new Among ( "\u00EEtes", -1, 1, "", this),
-            new Among ( "is", -1, 1, "", this),
-            new Among ( "irais", 13, 1, "", this),
-            new Among ( "issais", 13, 1, "", this),
-            new Among ( "irions", -1, 1, "", this),
-            new Among ( "issions", -1, 1, "", this),
-            new Among ( "irons", -1, 1, "", this),
-            new Among ( "issons", -1, 1, "", this),
-            new Among ( "issants", -1, 1, "", this),
-            new Among ( "it", -1, 1, "", this),
-            new Among ( "irait", 21, 1, "", this),
-            new Among ( "issait", 21, 1, "", this),
-            new Among ( "issant", -1, 1, "", this),
-            new Among ( "iraIent", -1, 1, "", this),
-            new Among ( "issaIent", -1, 1, "", this),
-            new Among ( "irent", -1, 1, "", this),
-            new Among ( "issent", -1, 1, "", this),
-            new Among ( "iront", -1, 1, "", this),
-            new Among ( "\u00EEt", -1, 1, "", this),
-            new Among ( "iriez", -1, 1, "", this),
-            new Among ( "issiez", -1, 1, "", this),
-            new Among ( "irez", -1, 1, "", this),
-            new Among ( "issez", -1, 1, "", this)
-        };
-
-        private Among a_6[] = {
-            new Among ( "a", -1, 3, "", this),
-            new Among ( "era", 0, 2, "", this),
-            new Among ( "asse", -1, 3, "", this),
-            new Among ( "ante", -1, 3, "", this),
-            new Among ( "\u00E9e", -1, 2, "", this),
-            new Among ( "ai", -1, 3, "", this),
-            new Among ( "erai", 5, 2, "", this),
-            new Among ( "er", -1, 2, "", this),
-            new Among ( "as", -1, 3, "", this),
-            new Among ( "eras", 8, 2, "", this),
-            new Among ( "\u00E2mes", -1, 3, "", this),
-            new Among ( "asses", -1, 3, "", this),
-            new Among ( "antes", -1, 3, "", this),
-            new Among ( "\u00E2tes", -1, 3, "", this),
-            new Among ( "\u00E9es", -1, 2, "", this),
-            new Among ( "ais", -1, 3, "", this),
-            new Among ( "erais", 15, 2, "", this),
-            new Among ( "ions", -1, 1, "", this),
-            new Among ( "erions", 17, 2, "", this),
-            new Among ( "assions", 17, 3, "", this),
-            new Among ( "erons", -1, 2, "", this),
-            new Among ( "ants", -1, 3, "", this),
-            new Among ( "\u00E9s", -1, 2, "", this),
-            new Among ( "ait", -1, 3, "", this),
-            new Among ( "erait", 23, 2, "", this),
-            new Among ( "ant", -1, 3, "", this),
-            new Among ( "aIent", -1, 3, "", this),
-            new Among ( "eraIent", 26, 2, "", this),
-            new Among ( "\u00E8rent", -1, 2, "", this),
-            new Among ( "assent", -1, 3, "", this),
-            new Among ( "eront", -1, 2, "", this),
-            new Among ( "\u00E2t", -1, 3, "", this),
-            new Among ( "ez", -1, 2, "", this),
-            new Among ( "iez", 32, 2, "", this),
-            new Among ( "eriez", 33, 2, "", this),
-            new Among ( "assiez", 33, 3, "", this),
-            new Among ( "erez", 32, 2, "", this),
-            new Among ( "\u00E9", -1, 2, "", this)
-        };
-
-        private Among a_7[] = {
-            new Among ( "e", -1, 3, "", this),
-            new Among ( "I\u00E8re", 0, 2, "", this),
-            new Among ( "i\u00E8re", 0, 2, "", this),
-            new Among ( "ion", -1, 1, "", this),
-            new Among ( "Ier", -1, 2, "", this),
-            new Among ( "ier", -1, 2, "", this),
-            new Among ( "\u00EB", -1, 4, "", this)
-        };
-
-        private Among a_8[] = {
-            new Among ( "ell", -1, -1, "", this),
-            new Among ( "eill", -1, -1, "", this),
-            new Among ( "enn", -1, -1, "", this),
-            new Among ( "onn", -1, -1, "", this),
-            new Among ( "ett", -1, -1, "", this)
-        };
-
-        private static final char g_v[] = {17, 65, 16, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 128, 130, 103, 8, 5 };
-
-        private static final char g_keep_with_s[] = {1, 65, 20, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 128 };
-
-        private int I_p2;
-        private int I_p1;
-        private int I_pV;
-
-        private void copy_from(FrenchStemmer other) {
-            I_p2 = other.I_p2;
-            I_p1 = other.I_p1;
-            I_pV = other.I_pV;
-            super.copy_from(other);
-        }
-
-        private boolean r_prelude() {
-            int v_1;
-            int v_2;
-            int v_3;
-            int v_4;
-            // repeat, line 38
-            replab0: while(true)
-            {
-                v_1 = cursor;
-                lab1: do {
-                    // goto, line 38
-                    golab2: while(true)
-                    {
-                        v_2 = cursor;
-                        lab3: do {
-                            // (, line 38
-                            // or, line 44
-                            lab4: do {
-                                v_3 = cursor;
-                                lab5: do {
-                                    // (, line 40
-                                    if (!(in_grouping(g_v, 97, 251)))
-                                    {
-                                        break lab5;
-                                    }
-                                    // [, line 40
-                                    bra = cursor;
-                                    // or, line 40
-                                    lab6: do {
-                                        v_4 = cursor;
-                                        lab7: do {
-                                            // (, line 40
-                                            // literal, line 40
-                                            if (!(eq_s(1, "u")))
-                                            {
-                                                break lab7;
-                                            }
-                                            // ], line 40
-                                            ket = cursor;
-                                            if (!(in_grouping(g_v, 97, 251)))
-                                            {
-                                                break lab7;
-                                            }
-                                            // <-, line 40
-                                            slice_from("U");
-                                            break lab6;
-                                        } while (false);
-                                        cursor = v_4;
-                                        lab8: do {
-                                            // (, line 41
-                                            // literal, line 41
-                                            if (!(eq_s(1, "i")))
-                                            {
-                                                break lab8;
-                                            }
-                                            // ], line 41
-                                            ket = cursor;
-                                            if (!(in_grouping(g_v, 97, 251)))
-                                            {
-                                                break lab8;
-                                            }
-                                            // <-, line 41
-                                            slice_from("I");
-                                            break lab6;
-                                        } while (false);
-                                        cursor = v_4;
-                                        // (, line 42
-                                        // literal, line 42
-                                        if (!(eq_s(1, "y")))
-                                        {
-                                            break lab5;
-                                        }
-                                        // ], line 42
-                                        ket = cursor;
-                                        // <-, line 42
-                                        slice_from("Y");
-                                    } while (false);
-                                    break lab4;
-                                } while (false);
-                                cursor = v_3;
-                                lab9: do {
-                                    // (, line 45
-                                    // [, line 45
-                                    bra = cursor;
-                                    // literal, line 45
-                                    if (!(eq_s(1, "y")))
-                                    {
-                                        break lab9;
-                                    }
-                                    // ], line 45
-                                    ket = cursor;
-                                    if (!(in_grouping(g_v, 97, 251)))
-                                    {
-                                        break lab9;
-                                    }
-                                    // <-, line 45
-                                    slice_from("Y");
-                                    break lab4;
-                                } while (false);
-                                cursor = v_3;
-                                // (, line 47
-                                // literal, line 47
-                                if (!(eq_s(1, "q")))
-                                {
-                                    break lab3;
-                                }
-                                // [, line 47
-                                bra = cursor;
-                                // literal, line 47
-                                if (!(eq_s(1, "u")))
-                                {
-                                    break lab3;
-                                }
-                                // ], line 47
-                                ket = cursor;
-                                // <-, line 47
-                                slice_from("U");
-                            } while (false);
-                            cursor = v_2;
-                            break golab2;
-                        } while (false);
-                        cursor = v_2;
-                        if (cursor >= limit)
-                        {
-                            break lab1;
-                        }
-                        cursor++;
-                    }
-                    continue replab0;
-                } while (false);
-                cursor = v_1;
-                break replab0;
-            }
-            return true;
-        }
-
-        private boolean r_mark_regions() {
-            int v_1;
-            int v_2;
-            int v_4;
-            // (, line 50
-            I_pV = limit;
-            I_p1 = limit;
-            I_p2 = limit;
-            // do, line 56
-            v_1 = cursor;
-            lab0: do {
-                // (, line 56
-                // or, line 58
-                lab1: do {
-                    v_2 = cursor;
-                    lab2: do {
-                        // (, line 57
-                        if (!(in_grouping(g_v, 97, 251)))
-                        {
-                            break lab2;
-                        }
-                        if (!(in_grouping(g_v, 97, 251)))
-                        {
-                            break lab2;
-                        }
-                        // next, line 57
-                        if (cursor >= limit)
-                        {
-                            break lab2;
-                        }
-                        cursor++;
-                        break lab1;
-                    } while (false);
-                    cursor = v_2;
-                    lab3: do {
-                        // among, line 59
-                        if (find_among(a_0, 3) == 0)
-                        {
-                            break lab3;
-                        }
-                        break lab1;
-                    } while (false);
-                    cursor = v_2;
-                    // (, line 66
-                    // next, line 66
-                    if (cursor >= limit)
-                    {
-                        break lab0;
-                    }
-                    cursor++;
-                    // gopast, line 66
-                    golab4: while(true)
-                    {
-                        lab5: do {
-                            if (!(in_grouping(g_v, 97, 251)))
-                            {
-                                break lab5;
-                            }
-                            break golab4;
-                        } while (false);
-                        if (cursor >= limit)
-                        {
-                            break lab0;
-                        }
-                        cursor++;
-                    }
-                } while (false);
-                // setmark pV, line 67
-                I_pV = cursor;
-            } while (false);
-            cursor = v_1;
-            // do, line 69
-            v_4 = cursor;
-            lab6: do {
-                // (, line 69
-                // gopast, line 70
-                golab7: while(true)
-                {
-                    lab8: do {
-                        if (!(in_grouping(g_v, 97, 251)))
-                        {
-                            break lab8;
-                        }
-                        break golab7;
-                    } while (false);
-                    if (cursor >= limit)
-                    {
-                        break lab6;
-                    }
-                    cursor++;
-                }
-                // gopast, line 70
-                golab9: while(true)
-                {
-                    lab10: do {
-                        if (!(out_grouping(g_v, 97, 251)))
-                        {
-                            break lab10;
-                        }
-                        break golab9;
-                    } while (false);
-                    if (cursor >= limit)
-                    {
-                        break lab6;
-                    }
-                    cursor++;
-                }
-                // setmark p1, line 70
-                I_p1 = cursor;
-                // gopast, line 71
-                golab11: while(true)
-                {
-                    lab12: do {
-                        if (!(in_grouping(g_v, 97, 251)))
-                        {
-                            break lab12;
-                        }
-                        break golab11;
-                    } while (false);
-                    if (cursor >= limit)
-                    {
-                        break lab6;
-                    }
-                    cursor++;
-                }
-                // gopast, line 71
-                golab13: while(true)
-                {
-                    lab14: do {
-                        if (!(out_grouping(g_v, 97, 251)))
-                        {
-                            break lab14;
-                        }
-                        break golab13;
-                    } while (false);
-                    if (cursor >= limit)
-                    {
-                        break lab6;
-                    }
-                    cursor++;
-                }
-                // setmark p2, line 71
-                I_p2 = cursor;
-            } while (false);
-            cursor = v_4;
-            return true;
-        }
-
-        private boolean r_postlude() {
-            int among_var;
-            int v_1;
-            // repeat, line 75
-            replab0: while(true)
-            {
-                v_1 = cursor;
-                lab1: do {
-                    // (, line 75
-                    // [, line 77
-                    bra = cursor;
-                    // substring, line 77
-                    among_var = find_among(a_1, 4);
-                    if (among_var == 0)
-                    {
-                        break lab1;
-                    }
-                    // ], line 77
-                    ket = cursor;
-                    switch(among_var) {
-                        case 0:
-                            break lab1;
-                        case 1:
-                            // (, line 78
-                            // <-, line 78
-                            slice_from("i");
-                            break;
-                        case 2:
-                            // (, line 79
-                            // <-, line 79
-                            slice_from("u");
-                            break;
-                        case 3:
-                            // (, line 80
-                            // <-, line 80
-                            slice_from("y");
-                            break;
-                        case 4:
-                            // (, line 81
-                            // next, line 81
-                            if (cursor >= limit)
-                            {
-                                break lab1;
-                            }
-                            cursor++;
-                            break;
-                    }
-                    continue replab0;
-                } while (false);
-                cursor = v_1;
-                break replab0;
-            }
-            return true;
-        }
-
-        private boolean r_RV() {
-            if (!(I_pV <= cursor))
-            {
-                return false;
-            }
-            return true;
-        }
-
-        private boolean r_R1() {
-            if (!(I_p1 <= cursor))
-            {
-                return false;
-            }
-            return true;
-        }
-
-        private boolean r_R2() {
-            if (!(I_p2 <= cursor))
-            {
-                return false;
-            }
-            return true;
-        }
-
-        private boolean r_standard_suffix() {
-            int among_var;
-            int v_1;
-            int v_2;
-            int v_3;
-            int v_4;
-            int v_5;
-            int v_6;
-            int v_7;
-            int v_8;
-            int v_9;
-            int v_10;
-            int v_11;
-            // (, line 91
-            // [, line 92
-            ket = cursor;
-            // substring, line 92
-            among_var = find_among_b(a_4, 43);
-            if (among_var == 0)
-            {
-                return false;
-            }
-            // ], line 92
-            bra = cursor;
-            switch(among_var) {
-                case 0:
-                    return false;
-                case 1:
-                    // (, line 96
-                    // call R2, line 96
-                    if (!r_R2())
-                    {
-                        return false;
-                    }
-                    // delete, line 96
-                    slice_del();
-                    break;
-                case 2:
-                    // (, line 99
-                    // call R2, line 99
-                    if (!r_R2())
-                    {
-                        return false;
-                    }
-                    // delete, line 99
-                    slice_del();
-                    // try, line 100
-                    v_1 = limit - cursor;
-                    lab0: do {
-                        // (, line 100
-                        // [, line 100
-                        ket = cursor;
-                        // literal, line 100
-                        if (!(eq_s_b(2, "ic")))
-                        {
-                            cursor = limit - v_1;
-                            break lab0;
-                        }
-                        // ], line 100
-                        bra = cursor;
-                        // or, line 100
-                        lab1: do {
-                            v_2 = limit - cursor;
-                            lab2: do {
-                                // (, line 100
-                                // call R2, line 100
-                                if (!r_R2())
-                                {
-                                    break lab2;
-                                }
-                                // delete, line 100
-                                slice_del();
-                                break lab1;
-                            } while (false);
-                            cursor = limit - v_2;
-                            // <-, line 100
-                            slice_from("iqU");
-                        } while (false);
-                    } while (false);
-                    break;
-                case 3:
-                    // (, line 104
-                    // call R2, line 104
-                    if (!r_R2())
-                    {
-                        return false;
-                    }
-                    // <-, line 104
-                    slice_from("log");
-                    break;
-                case 4:
-                    // (, line 107
-                    // call R2, line 107
-                    if (!r_R2())
-                    {
-                        return false;
-                    }
-                    // <-, line 107
-                    slice_from("u");
-                    break;
-                case 5:
-                    // (, line 110
-                    // call R2, line 110
-                    if (!r_R2())
-                    {
-                        return false;
-                    }
-                    // <-, line 110
-                    slice_from("ent");
-                    break;
-                case 6:
-                    // (, line 113
-                    // call RV, line 114
-                    if (!r_RV())
-                    {
-                        return false;
-                    }
-                    // delete, line 114
-                    slice_del();
-                    // try, line 115
-                    v_3 = limit - cursor;
-                    lab3: do {
-                        // (, line 115
-                        // [, line 116
-                        ket = cursor;
-                        // substring, line 116
-                        among_var = find_among_b(a_2, 6);
-                        if (among_var == 0)
-                        {
-                            cursor = limit - v_3;
-                            break lab3;
-                        }
-                        // ], line 116
-                        bra = cursor;
-                        switch(among_var) {
-                            case 0:
-                                cursor = limit - v_3;
-                                break lab3;
-                            case 1:
-                                // (, line 117
-                                // call R2, line 117
-                                if (!r_R2())
-                                {
-                                    cursor = limit - v_3;
-                                    break lab3;
-                                }
-                                // delete, line 117
-                                slice_del();
-                                // [, line 117
-                                ket = cursor;
-                                // literal, line 117
-                                if (!(eq_s_b(2, "at")))
-                                {
-                                    cursor = limit - v_3;
-                                    break lab3;
-                                }
-                                // ], line 117
-                                bra = cursor;
-                                // call R2, line 117
-                                if (!r_R2())
-                                {
-                                    cursor = limit - v_3;
-                                    break lab3;
-                                }
-                                // delete, line 117
-                                slice_del();
-                                break;
-                            case 2:
-                                // (, line 118
-                                // or, line 118
-                                lab4: do {
-                                    v_4 = limit - cursor;
-                                    lab5: do {
-                                        // (, line 118
-                                        // call R2, line 118
-                                        if (!r_R2())
-                                        {
-                                            break lab5;
-                                        }
-                                        // delete, line 118
-                                        slice_del();
-                                        break lab4;
-                                    } while (false);
-                                    cursor = limit - v_4;
-                                    // (, line 118
-                                    // call R1, line 118
-                                    if (!r_R1())
-                                    {
-                                        cursor = limit - v_3;
-                                        break lab3;
-                                    }
-                                    // <-, line 118
-                                    slice_from("eux");
-                                } while (false);
-                                break;
-                            case 3:
-                                // (, line 120
-                                // call R2, line 120
-                                if (!r_R2())
-                                {
-                                    cursor = limit - v_3;
-                                    break lab3;
-                                }
-                                // delete, line 120
-                                slice_del();
-                                break;
-                            case 4:
-                                // (, line 122
-                                // call RV, line 122
-                                if (!r_RV())
-                                {
-                                    cursor = limit - v_3;
-                                    break lab3;
-                                }
-                                // <-, line 122
-                                slice_from("i");
-                                break;
-                        }
-                    } while (false);
-                    break;
-                case 7:
-                    // (, line 128
-                    // call R2, line 129
-                    if (!r_R2())
-                    {
-                        return false;
-                    }
-                    // delete, line 129
-                    slice_del();
-                    // try, line 130
-                    v_5 = limit - cursor;
-                    lab6: do {
-                        // (, line 130
-                        // [, line 131
-                        ket = cursor;
-                        // substring, line 131
-                        among_var = find_among_b(a_3, 3);
-                        if (among_var == 0)
-                        {
-                            cursor = limit - v_5;
-                            break lab6;
-                        }
-                        // ], line 131
-                        bra = cursor;
-                        switch(among_var) {
-                            case 0:
-                                cursor = limit - v_5;
-                                break lab6;
-                            case 1:
-                                // (, line 132
-                                // or, line 132
-                                lab7: do {
-                                    v_6 = limit - cursor;
-                                    lab8: do {
-                                        // (, line 132
-                                        // call R2, line 132
-                                        if (!r_R2())
-                                        {
-                                            break lab8;
-                                        }
-                                        // delete, line 132
-                                        slice_del();
-                                        break lab7;
-                                    } while (false);
-                                    cursor = limit - v_6;
-                                    // <-, line 132
-                                    slice_from("abl");
-                                } while (false);
-                                break;
-                            case 2:
-                                // (, line 133
-                                // or, line 133
-                                lab9: do {
-                                    v_7 = limit - cursor;
-                                    lab10: do {
-                                        // (, line 133
-                                        // call R2, line 133
-                                        if (!r_R2())
-                                        {
-                                            break lab10;
-                                        }
-                                        // delete, line 133
-                                        slice_del();
-                                        break lab9;
-                                    } while (false);
-                                    cursor = limit - v_7;
-                                    // <-, line 133
-                                    slice_from("iqU");
-                                } while (false);
-                                break;
-                            case 3:
-                                // (, line 134
-                                // call R2, line 134
-                                if (!r_R2())
-                                {
-                                    cursor = limit - v_5;
-                                    break lab6;
-                                }
-                                // delete, line 134
-                                slice_del();
-                                break;
-                        }
-                    } while (false);
-                    break;
-                case 8:
-                    // (, line 140
-                    // call R2, line 141
-                    if (!r_R2())
-                    {
-                        return false;
-                    }
-                    // delete, line 141
-                    slice_del();
-                    // try, line 142
-                    v_8 = limit - cursor;
-                    lab11: do {
-                        // (, line 142
-                        // [, line 142
-                        ket = cursor;
-                        // literal, line 142
-                        if (!(eq_s_b(2, "at")))
-                        {
-                            cursor = limit - v_8;
-                            break lab11;
-                        }
-                        // ], line 142
-                        bra = cursor;
-                        // call R2, line 142
-                        if (!r_R2())
-                        {
-                            cursor = limit - v_8;
-                            break lab11;
-                        }
-                        // delete, line 142
-                        slice_del();
-                        // [, line 142
-                        ket = cursor;
-                        // literal, line 142
-                        if (!(eq_s_b(2, "ic")))
-                        {
-                            cursor = limit - v_8;
-                            break lab11;
-                        }
-                        // ], line 142
-                        bra = cursor;
-                        // or, line 142
-                        lab12: do {
-                            v_9 = limit - cursor;
-                            lab13: do {
-                                // (, line 142
-                                // call R2, line 142
-                                if (!r_R2())
-                                {
-                                    break lab13;
-                                }
-                                // delete, line 142
-                                slice_del();
-                                break lab12;
-                            } while (false);
-                            cursor = limit - v_9;
-                            // <-, line 142
-                            slice_from("iqU");
-                        } while (false);
-                    } while (false);
-                    break;
-                case 9:
-                    // (, line 144
-                    // <-, line 144
-                    slice_from("eau");
-                    break;
-                case 10:
-                    // (, line 145
-                    // call R1, line 145
-                    if (!r_R1())
-                    {
-                        return false;
-                    }
-                    // <-, line 145
-                    slice_from("al");
-                    break;
-                case 11:
-                    // (, line 147
-                    // or, line 147
-                    lab14: do {
-                        v_10 = limit - cursor;
-                        lab15: do {
-                            // (, line 147
-                            // call R2, line 147
-                            if (!r_R2())
-                            {
-                                break lab15;
-                            }
-                            // delete, line 147
-                            slice_del();
-                            break lab14;
-                        } while (false);
-                        cursor = limit - v_10;
-                        // (, line 147
-                        // call R1, line 147
-                        if (!r_R1())
-                        {
-                            return false;
-                        }
-                        // <-, line 147
-                        slice_from("eux");
-                    } while (false);
-                    break;
-                case 12:
-                    // (, line 150
-                    // call R1, line 150
-                    if (!r_R1())
-                    {
-                        return false;
-                    }
-                    if (!(out_grouping_b(g_v, 97, 251)))
-                    {
-                        return false;
-                    }
-                    // delete, line 150
-                    slice_del();
-                    break;
-                case 13:
-                    // (, line 155
-                    // call RV, line 155
-                    if (!r_RV())
-                    {
-                        return false;
-                    }
-                    // fail, line 155
-                    // (, line 155
-                    // <-, line 155
-                    slice_from("ant");
-                    return false;
-                case 14:
-                    // (, line 156
-                    // call RV, line 156
-                    if (!r_RV())
-                    {
-                        return false;
-                    }
-                    // fail, line 156
-                    // (, line 156
-                    // <-, line 156
-                    slice_from("ent");
-                    return false;
-                case 15:
-                    // (, line 158
-                    // test, line 158
-                    v_11 = limit - cursor;
-                    // (, line 158
-                    if (!(in_grouping_b(g_v, 97, 251)))
-                    {
-                        return false;
-                    }
-                    // call RV, line 158
-                    if (!r_RV())
-                    {
-                        return false;
-                    }
-                    cursor = limit - v_11;
-                    // fail, line 158
-                    // (, line 158
-                    // delete, line 158
-                    slice_del();
-                    return false;
-            }
-            return true;
-        }
-
-        private boolean r_i_verb_suffix() {
-            int among_var;
-            int v_1;
-            int v_2;
-            // setlimit, line 163
-            v_1 = limit - cursor;
-            // tomark, line 163
-            if (cursor < I_pV)
-            {
-                return false;
-            }
-            cursor = I_pV;
-            v_2 = limit_backward;
-            limit_backward = cursor;
-            cursor = limit - v_1;
-            // (, line 163
-            // [, line 164
-            ket = cursor;
-            // substring, line 164
-            among_var = find_among_b(a_5, 35);
-            if (among_var == 0)
-            {
-                limit_backward = v_2;
-                return false;
-            }
-            // ], line 164
-            bra = cursor;
-            switch(among_var) {
-                case 0:
-                    limit_backward = v_2;
-                    return false;
-                case 1:
-                    // (, line 170
-                    if (!(out_grouping_b(g_v, 97, 251)))
-                    {
-                        limit_backward = v_2;
-                        return false;
-                    }
-                    // delete, line 170
-                    slice_del();
-                    break;
-            }
-            limit_backward = v_2;
-            return true;
-        }
-
-        private boolean r_verb_suffix() {
-            int among_var;
-            int v_1;
-            int v_2;
-            int v_3;
-            // setlimit, line 174
-            v_1 = limit - cursor;
-            // tomark, line 174
-            if (cursor < I_pV)
-            {
-                return false;
-            }
-            cursor = I_pV;
-            v_2 = limit_backward;
-            limit_backward = cursor;
-            cursor = limit - v_1;
-            // (, line 174
-            // [, line 175
-            ket = cursor;
-            // substring, line 175
-            among_var = find_among_b(a_6, 38);
-            if (among_var == 0)
-            {
-                limit_backward = v_2;
-                return false;
-            }
-            // ], line 175
-            bra = cursor;
-            switch(among_var) {
-                case 0:
-                    limit_backward = v_2;
-                    return false;
-                case 1:
-                    // (, line 177
-                    // call R2, line 177
-                    if (!r_R2())
-                    {
-                        limit_backward = v_2;
-                        return false;
-                    }
-                    // delete, line 177
-                    slice_del();
-                    break;
-                case 2:
-                    // (, line 185
-                    // delete, line 185
-                    slice_del();
-                    break;
-                case 3:
-                    // (, line 190
-                    // delete, line 190
-                    slice_del();
-                    // try, line 191
-                    v_3 = limit - cursor;
-                    lab0: do {
-                        // (, line 191
-                        // [, line 191
-                        ket = cursor;
-                        // literal, line 191
-                        if (!(eq_s_b(1, "e")))
-                        {
-                            cursor = limit - v_3;
-                            break lab0;
-                        }
-                        // ], line 191
-                        bra = cursor;
-                        // delete, line 191
-                        slice_del();
-                    } while (false);
-                    break;
-            }
-            limit_backward = v_2;
-            return true;
-        }
-
-        private boolean r_residual_suffix() {
-            int among_var;
-            int v_1;
-            int v_2;
-            int v_3;
-            int v_4;
-            int v_5;
-            // (, line 198
-            // try, line 199
-            v_1 = limit - cursor;
-            lab0: do {
-                // (, line 199
-                // [, line 199
-                ket = cursor;
-                // literal, line 199
-                if (!(eq_s_b(1, "s")))
-                {
-                    cursor = limit - v_1;
-                    break lab0;
-                }
-                // ], line 199
-                bra = cursor;
-                // test, line 199
-                v_2 = limit - cursor;
-                if (!(out_grouping_b(g_keep_with_s, 97, 232)))
-                {
-                    cursor = limit - v_1;
-                    break lab0;
-                }
-                cursor = limit - v_2;
-                // delete, line 199
-                slice_del();
-            } while (false);
-            // setlimit, line 200
-            v_3 = limit - cursor;
-            // tomark, line 200
-            if (cursor < I_pV)
-            {
-                return false;
-            }
-            cursor = I_pV;
-            v_4 = limit_backward;
-            limit_backward = cursor;
-            cursor = limit - v_3;
-            // (, line 200
-            // [, line 201
-            ket = cursor;
-            // substring, line 201
-            among_var = find_among_b(a_7, 7);
-            if (among_var == 0)
-            {
-                limit_backward = v_4;
-                return false;
-            }
-            // ], line 201
-            bra = cursor;
-            switch(among_var) {
-                case 0:
-                    limit_backward = v_4;
-                    return false;
-                case 1:
-                    // (, line 202
-                    // call R2, line 202
-                    if (!r_R2())
-                    {
-                        limit_backward = v_4;
-                        return false;
-                    }
-                    // or, line 202
-                    lab1: do {
-                        v_5 = limit - cursor;
-                        lab2: do {
-                            // literal, line 202
-                            if (!(eq_s_b(1, "s")))
-                            {
-                                break lab2;
-                            }
-                            break lab1;
-                        } while (false);
-                        cursor = limit - v_5;
-                        // literal, line 202
-                        if (!(eq_s_b(1, "t")))
-                        {
-                            limit_backward = v_4;
-                            return false;
-                        }
-                    } while (false);
-                    // delete, line 202
-                    slice_del();
-                    break;
-                case 2:
-                    // (, line 204
-                    // <-, line 204
-                    slice_from("i");
-                    break;
-                case 3:
-                    // (, line 205
-                    // delete, line 205
-                    slice_del();
-                    break;
-                case 4:
-                    // (, line 206
-                    // literal, line 206
-                    if (!(eq_s_b(2, "gu")))
-                    {
-                        limit_backward = v_4;
-                        return false;
-                    }
-                    // delete, line 206
-                    slice_del();
-                    break;
-            }
-            limit_backward = v_4;
-            return true;
-        }
-
-        private boolean r_un_double() {
-            int v_1;
-            // (, line 211
-            // test, line 212
-            v_1 = limit - cursor;
-            // among, line 212
-            if (find_among_b(a_8, 5) == 0)
-            {
-                return false;
-            }
-            cursor = limit - v_1;
-            // [, line 212
-            ket = cursor;
-            // next, line 212
-            if (cursor <= limit_backward)
-            {
-                return false;
-            }
-            cursor--;
-            // ], line 212
-            bra = cursor;
-            // delete, line 212
-            slice_del();
-            return true;
-        }
-
-        private boolean r_un_accent() {
-            int v_3;
-            // (, line 215
-            // atleast, line 216
-            {
-                int v_1 = 1;
-                // atleast, line 216
-                replab0: while(true)
-                {
-                    lab1: do {
-                        if (!(out_grouping_b(g_v, 97, 251)))
-                        {
-                            break lab1;
-                        }
-                        v_1--;
-                        continue replab0;
-                    } while (false);
-                    break replab0;
-                }
-                if (v_1 > 0)
-                {
-                    return false;
-                }
-            }
-            // [, line 217
-            ket = cursor;
-            // or, line 217
-            lab2: do {
-                v_3 = limit - cursor;
-                lab3: do {
-                    // literal, line 217
-                    if (!(eq_s_b(1, "\u00E9")))
-                    {
-                        break lab3;
-                    }
-                    break lab2;
-                } while (false);
-                cursor = limit - v_3;
-                // literal, line 217
-                if (!(eq_s_b(1, "\u00E8")))
-                {
-                    return false;
-                }
-            } while (false);
-            // ], line 217
-            bra = cursor;
-            // <-, line 217
-            slice_from("e");
-            return true;
-        }
-
-        public boolean stem() {
-            int v_1;
-            int v_2;
-            int v_3;
-            int v_4;
-            int v_5;
-            int v_6;
-            int v_7;
-            int v_8;
-            int v_9;
-            int v_10;
-            int v_11;
-            // (, line 221
-            // do, line 223
-            v_1 = cursor;
-            lab0: do {
-                // call prelude, line 223
-                if (!r_prelude())
-                {
-                    break lab0;
-                }
-            } while (false);
-            cursor = v_1;
-            // do, line 224
-            v_2 = cursor;
-            lab1: do {
-                // call mark_regions, line 224
-                if (!r_mark_regions())
-                {
-                    break lab1;
-                }
-            } while (false);
-            cursor = v_2;
-            // backwards, line 225
-            limit_backward = cursor; cursor = limit;
-            // (, line 225
-            // do, line 227
-            v_3 = limit - cursor;
-            lab2: do {
-                // (, line 227
-                // or, line 237
-                lab3: do {
-                    v_4 = limit - cursor;
-                    lab4: do {
-                        // (, line 228
-                        // and, line 233
-                        v_5 = limit - cursor;
-                        // (, line 229
-                        // or, line 229
-                        lab5: do {
-                            v_6 = limit - cursor;
-                            lab6: do {
-                                // call standard_suffix, line 229
-                                if (!r_standard_suffix())
-                                {
-                                    break lab6;
-                                }
-                                break lab5;
-                            } while (false);
-                            cursor = limit - v_6;
-                            lab7: do {
-                                // call i_verb_suffix, line 230
-                                if (!r_i_verb_suffix())
-                                {
-                                    break lab7;
-                                }
-                                break lab5;
-                            } while (false);
-                            cursor = limit - v_6;
-                            // call verb_suffix, line 231
-                            if (!r_verb_suffix())
-                            {
-                                break lab4;
-                            }
-                        } while (false);
-                        cursor = limit - v_5;
-                        // try, line 234
-                        v_7 = limit - cursor;
-                        lab8: do {
-                            // (, line 234
-                            // [, line 234
-                            ket = cursor;
-                            // or, line 234
-                            lab9: do {
-                                v_8 = limit - cursor;
-                                lab10: do {
-                                    // (, line 234
-                                    // literal, line 234
-                                    if (!(eq_s_b(1, "Y")))
-                                    {
-                                        break lab10;
-                                    }
-                                    // ], line 234
-                                    bra = cursor;
-                                    // <-, line 234
-                                    slice_from("i");
-                                    break lab9;
-                                } while (false);
-                                cursor = limit - v_8;
-                                // (, line 235
-                                // literal, line 235
-                                if (!(eq_s_b(1, "\u00E7")))
-                                {
-                                    cursor = limit - v_7;
-                                    break lab8;
-                                }
-                                // ], line 235
-                                bra = cursor;
-                                // <-, line 235
-                                slice_from("c");
-                            } while (false);
-                        } while (false);
-                        break lab3;
-                    } while (false);
-                    cursor = limit - v_4;
-                    // call residual_suffix, line 238
-                    if (!r_residual_suffix())
-                    {
-                        break lab2;
-                    }
-                } while (false);
-            } while (false);
-            cursor = limit - v_3;
-            // do, line 243
-            v_9 = limit - cursor;
-            lab11: do {
-                // call un_double, line 243
-                if (!r_un_double())
-                {
-                    break lab11;
-                }
-            } while (false);
-            cursor = limit - v_9;
-            // do, line 244
-            v_10 = limit - cursor;
-            lab12: do {
-                // call un_accent, line 244
-                if (!r_un_accent())
-                {
-                    break lab12;
-                }
-            } while (false);
-            cursor = limit - v_10;
-            cursor = limit_backward;            // do, line 246
-            v_11 = cursor;
-            lab13: do {
-                // call postlude, line 246
-                if (!r_postlude())
-                {
-                    break lab13;
-                }
-            } while (false);
-            cursor = v_11;
-            return true;
-        }
-
-}
-
Index: contrib/snowball/src/java/org/tartarus/snowball/ext/SwedishStemmer.java
===================================================================
--- contrib/snowball/src/java/org/tartarus/snowball/ext/SwedishStemmer.java	(revision 916567)
+++ contrib/snowball/src/java/org/tartarus/snowball/ext/SwedishStemmer.java	(working copy)
@@ -1,349 +0,0 @@
-// This file was generated automatically by the Snowball to Java compiler
-
-package org.tartarus.snowball.ext;
-import org.tartarus.snowball.SnowballProgram;
-import org.tartarus.snowball.Among;
-
-/**
- * Generated class implementing code defined by a snowball script.
- */
-public class SwedishStemmer extends SnowballProgram {
-
-        private Among a_0[] = {
-            new Among ( "a", -1, 1, "", this),
-            new Among ( "arna", 0, 1, "", this),
-            new Among ( "erna", 0, 1, "", this),
-            new Among ( "heterna", 2, 1, "", this),
-            new Among ( "orna", 0, 1, "", this),
-            new Among ( "ad", -1, 1, "", this),
-            new Among ( "e", -1, 1, "", this),
-            new Among ( "ade", 6, 1, "", this),
-            new Among ( "ande", 6, 1, "", this),
-            new Among ( "arne", 6, 1, "", this),
-            new Among ( "are", 6, 1, "", this),
-            new Among ( "aste", 6, 1, "", this),
-            new Among ( "en", -1, 1, "", this),
-            new Among ( "anden", 12, 1, "", this),
-            new Among ( "aren", 12, 1, "", this),
-            new Among ( "heten", 12, 1, "", this),
-            new Among ( "ern", -1, 1, "", this),
-            new Among ( "ar", -1, 1, "", this),
-            new Among ( "er", -1, 1, "", this),
-            new Among ( "heter", 18, 1, "", this),
-            new Among ( "or", -1, 1, "", this),
-            new Among ( "s", -1, 2, "", this),
-            new Among ( "as", 21, 1, "", this),
-            new Among ( "arnas", 22, 1, "", this),
-            new Among ( "ernas", 22, 1, "", this),
-            new Among ( "ornas", 22, 1, "", this),
-            new Among ( "es", 21, 1, "", this),
-            new Among ( "ades", 26, 1, "", this),
-            new Among ( "andes", 26, 1, "", this),
-            new Among ( "ens", 21, 1, "", this),
-            new Among ( "arens", 29, 1, "", this),
-            new Among ( "hetens", 29, 1, "", this),
-            new Among ( "erns", 21, 1, "", this),
-            new Among ( "at", -1, 1, "", this),
-            new Among ( "andet", -1, 1, "", this),
-            new Among ( "het", -1, 1, "", this),
-            new Among ( "ast", -1, 1, "", this)
-        };
-
-        private Among a_1[] = {
-            new Among ( "dd", -1, -1, "", this),
-            new Among ( "gd", -1, -1, "", this),
-            new Among ( "nn", -1, -1, "", this),
-            new Among ( "dt", -1, -1, "", this),
-            new Among ( "gt", -1, -1, "", this),
-            new Among ( "kt", -1, -1, "", this),
-            new Among ( "tt", -1, -1, "", this)
-        };
-
-        private Among a_2[] = {
-            new Among ( "ig", -1, 1, "", this),
-            new Among ( "lig", 0, 1, "", this),
-            new Among ( "els", -1, 1, "", this),
-            new Among ( "fullt", -1, 3, "", this),
-            new Among ( "l\u00F6st", -1, 2, "", this)
-        };
-
-        private static final char g_v[] = {17, 65, 16, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 24, 0, 32 };
-
-        private static final char g_s_ending[] = {119, 127, 149 };
-
-        private int I_x;
-        private int I_p1;
-
-        private void copy_from(SwedishStemmer other) {
-            I_x = other.I_x;
-            I_p1 = other.I_p1;
-            super.copy_from(other);
-        }
-
-        private boolean r_mark_regions() {
-            int v_1;
-            int v_2;
-            // (, line 26
-            I_p1 = limit;
-            // test, line 29
-            v_1 = cursor;
-            // (, line 29
-            // hop, line 29
-            {
-                int c = cursor + 3;
-                if (0 > c || c > limit)
-                {
-                    return false;
-                }
-                cursor = c;
-            }
-            // setmark x, line 29
-            I_x = cursor;
-            cursor = v_1;
-            // goto, line 30
-            golab0: while(true)
-            {
-                v_2 = cursor;
-                lab1: do {
-                    if (!(in_grouping(g_v, 97, 246)))
-                    {
-                        break lab1;
-                    }
-                    cursor = v_2;
-                    break golab0;
-                } while (false);
-                cursor = v_2;
-                if (cursor >= limit)
-                {
-                    return false;
-                }
-                cursor++;
-            }
-            // gopast, line 30
-            golab2: while(true)
-            {
-                lab3: do {
-                    if (!(out_grouping(g_v, 97, 246)))
-                    {
-                        break lab3;
-                    }
-                    break golab2;
-                } while (false);
-                if (cursor >= limit)
-                {
-                    return false;
-                }
-                cursor++;
-            }
-            // setmark p1, line 30
-            I_p1 = cursor;
-            // try, line 31
-            lab4: do {
-                // (, line 31
-                if (!(I_p1 < I_x))
-                {
-                    break lab4;
-                }
-                I_p1 = I_x;
-            } while (false);
-            return true;
-        }
-
-        private boolean r_main_suffix() {
-            int among_var;
-            int v_1;
-            int v_2;
-            // (, line 36
-            // setlimit, line 37
-            v_1 = limit - cursor;
-            // tomark, line 37
-            if (cursor < I_p1)
-            {
-                return false;
-            }
-            cursor = I_p1;
-            v_2 = limit_backward;
-            limit_backward = cursor;
-            cursor = limit - v_1;
-            // (, line 37
-            // [, line 37
-            ket = cursor;
-            // substring, line 37
-            among_var = find_among_b(a_0, 37);
-            if (among_var == 0)
-            {
-                limit_backward = v_2;
-                return false;
-            }
-            // ], line 37
-            bra = cursor;
-            limit_backward = v_2;
-            switch(among_var) {
-                case 0:
-                    return false;
-                case 1:
-                    // (, line 44
-                    // delete, line 44
-                    slice_del();
-                    break;
-                case 2:
-                    // (, line 46
-                    if (!(in_grouping_b(g_s_ending, 98, 121)))
-                    {
-                        return false;
-                    }
-                    // delete, line 46
-                    slice_del();
-                    break;
-            }
-            return true;
-        }
-
-        private boolean r_consonant_pair() {
-            int v_1;
-            int v_2;
-            int v_3;
-            // setlimit, line 50
-            v_1 = limit - cursor;
-            // tomark, line 50
-            if (cursor < I_p1)
-            {
-                return false;
-            }
-            cursor = I_p1;
-            v_2 = limit_backward;
-            limit_backward = cursor;
-            cursor = limit - v_1;
-            // (, line 50
-            // and, line 52
-            v_3 = limit - cursor;
-            // among, line 51
-            if (find_among_b(a_1, 7) == 0)
-            {
-                limit_backward = v_2;
-                return false;
-            }
-            cursor = limit - v_3;
-            // (, line 52
-            // [, line 52
-            ket = cursor;
-            // next, line 52
-            if (cursor <= limit_backward)
-            {
-                limit_backward = v_2;
-                return false;
-            }
-            cursor--;
-            // ], line 52
-            bra = cursor;
-            // delete, line 52
-            slice_del();
-            limit_backward = v_2;
-            return true;
-        }
-
-        private boolean r_other_suffix() {
-            int among_var;
-            int v_1;
-            int v_2;
-            // setlimit, line 55
-            v_1 = limit - cursor;
-            // tomark, line 55
-            if (cursor < I_p1)
-            {
-                return false;
-            }
-            cursor = I_p1;
-            v_2 = limit_backward;
-            limit_backward = cursor;
-            cursor = limit - v_1;
-            // (, line 55
-            // [, line 56
-            ket = cursor;
-            // substring, line 56
-            among_var = find_among_b(a_2, 5);
-            if (among_var == 0)
-            {
-                limit_backward = v_2;
-                return false;
-            }
-            // ], line 56
-            bra = cursor;
-            switch(among_var) {
-                case 0:
-                    limit_backward = v_2;
-                    return false;
-                case 1:
-                    // (, line 57
-                    // delete, line 57
-                    slice_del();
-                    break;
-                case 2:
-                    // (, line 58
-                    // <-, line 58
-                    slice_from("l\u00F6s");
-                    break;
-                case 3:
-                    // (, line 59
-                    // <-, line 59
-                    slice_from("full");
-                    break;
-            }
-            limit_backward = v_2;
-            return true;
-        }
-
-        public boolean stem() {
-            int v_1;
-            int v_2;
-            int v_3;
-            int v_4;
-            // (, line 64
-            // do, line 66
-            v_1 = cursor;
-            lab0: do {
-                // call mark_regions, line 66
-                if (!r_mark_regions())
-                {
-                    break lab0;
-                }
-            } while (false);
-            cursor = v_1;
-            // backwards, line 67
-            limit_backward = cursor; cursor = limit;
-            // (, line 67
-            // do, line 68
-            v_2 = limit - cursor;
-            lab1: do {
-                // call main_suffix, line 68
-                if (!r_main_suffix())
-                {
-                    break lab1;
-                }
-            } while (false);
-            cursor = limit - v_2;
-            // do, line 69
-            v_3 = limit - cursor;
-            lab2: do {
-                // call consonant_pair, line 69
-                if (!r_consonant_pair())
-                {
-                    break lab2;
-                }
-            } while (false);
-            cursor = limit - v_3;
-            // do, line 70
-            v_4 = limit - cursor;
-            lab3: do {
-                // call other_suffix, line 70
-                if (!r_other_suffix())
-                {
-                    break lab3;
-                }
-            } while (false);
-            cursor = limit - v_4;
-            cursor = limit_backward;            return true;
-        }
-
-}
-
Index: contrib/snowball/src/java/org/tartarus/snowball/ext/DanishStemmer.java
===================================================================
--- contrib/snowball/src/java/org/tartarus/snowball/ext/DanishStemmer.java	(revision 916567)
+++ contrib/snowball/src/java/org/tartarus/snowball/ext/DanishStemmer.java	(working copy)
@@ -1,423 +0,0 @@
-// This file was generated automatically by the Snowball to Java compiler
-
-package org.tartarus.snowball.ext;
-import org.tartarus.snowball.SnowballProgram;
-import org.tartarus.snowball.Among;
-
-/**
- * Generated class implementing code defined by a snowball script.
- */
-public class DanishStemmer extends SnowballProgram {
-
-        private Among a_0[] = {
-            new Among ( "hed", -1, 1, "", this),
-            new Among ( "ethed", 0, 1, "", this),
-            new Among ( "ered", -1, 1, "", this),
-            new Among ( "e", -1, 1, "", this),
-            new Among ( "erede", 3, 1, "", this),
-            new Among ( "ende", 3, 1, "", this),
-            new Among ( "erende", 5, 1, "", this),
-            new Among ( "ene", 3, 1, "", this),
-            new Among ( "erne", 3, 1, "", this),
-            new Among ( "ere", 3, 1, "", this),
-            new Among ( "en", -1, 1, "", this),
-            new Among ( "heden", 10, 1, "", this),
-            new Among ( "eren", 10, 1, "", this),
-            new Among ( "er", -1, 1, "", this),
-            new Among ( "heder", 13, 1, "", this),
-            new Among ( "erer", 13, 1, "", this),
-            new Among ( "s", -1, 2, "", this),
-            new Among ( "heds", 16, 1, "", this),
-            new Among ( "es", 16, 1, "", this),
-            new Among ( "endes", 18, 1, "", this),
-            new Among ( "erendes", 19, 1, "", this),
-            new Among ( "enes", 18, 1, "", this),
-            new Among ( "ernes", 18, 1, "", this),
-            new Among ( "eres", 18, 1, "", this),
-            new Among ( "ens", 16, 1, "", this),
-            new Among ( "hedens", 24, 1, "", this),
-            new Among ( "erens", 24, 1, "", this),
-            new Among ( "ers", 16, 1, "", this),
-            new Among ( "ets", 16, 1, "", this),
-            new Among ( "erets", 28, 1, "", this),
-            new Among ( "et", -1, 1, "", this),
-            new Among ( "eret", 30, 1, "", this)
-        };
-
-        private Among a_1[] = {
-            new Among ( "gd", -1, -1, "", this),
-            new Among ( "dt", -1, -1, "", this),
-            new Among ( "gt", -1, -1, "", this),
-            new Among ( "kt", -1, -1, "", this)
-        };
-
-        private Among a_2[] = {
-            new Among ( "ig", -1, 1, "", this),
-            new Among ( "lig", 0, 1, "", this),
-            new Among ( "elig", 1, 1, "", this),
-            new Among ( "els", -1, 1, "", this),
-            new Among ( "l\u00F8st", -1, 2, "", this)
-        };
-
-        private static final char g_v[] = {17, 65, 16, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 48, 0, 128 };
-
-        private static final char g_s_ending[] = {239, 254, 42, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 16 };
-
-        private int I_x;
-        private int I_p1;
-        private StringBuilder S_ch = new StringBuilder();
-
-        private void copy_from(DanishStemmer other) {
-            I_x = other.I_x;
-            I_p1 = other.I_p1;
-            S_ch = other.S_ch;
-            super.copy_from(other);
-        }
-
-        private boolean r_mark_regions() {
-            int v_1;
-            int v_2;
-            // (, line 29
-            I_p1 = limit;
-            // test, line 33
-            v_1 = cursor;
-            // (, line 33
-            // hop, line 33
-            {
-                int c = cursor + 3;
-                if (0 > c || c > limit)
-                {
-                    return false;
-                }
-                cursor = c;
-            }
-            // setmark x, line 33
-            I_x = cursor;
-            cursor = v_1;
-            // goto, line 34
-            golab0: while(true)
-            {
-                v_2 = cursor;
-                lab1: do {
-                    if (!(in_grouping(g_v, 97, 248)))
-                    {
-                        break lab1;
-                    }
-                    cursor = v_2;
-                    break golab0;
-                } while (false);
-                cursor = v_2;
-                if (cursor >= limit)
-                {
-                    return false;
-                }
-                cursor++;
-            }
-            // gopast, line 34
-            golab2: while(true)
-            {
-                lab3: do {
-                    if (!(out_grouping(g_v, 97, 248)))
-                    {
-                        break lab3;
-                    }
-                    break golab2;
-                } while (false);
-                if (cursor >= limit)
-                {
-                    return false;
-                }
-                cursor++;
-            }
-            // setmark p1, line 34
-            I_p1 = cursor;
-            // try, line 35
-            lab4: do {
-                // (, line 35
-                if (!(I_p1 < I_x))
-                {
-                    break lab4;
-                }
-                I_p1 = I_x;
-            } while (false);
-            return true;
-        }
-
-        private boolean r_main_suffix() {
-            int among_var;
-            int v_1;
-            int v_2;
-            // (, line 40
-            // setlimit, line 41
-            v_1 = limit - cursor;
-            // tomark, line 41
-            if (cursor < I_p1)
-            {
-                return false;
-            }
-            cursor = I_p1;
-            v_2 = limit_backward;
-            limit_backward = cursor;
-            cursor = limit - v_1;
-            // (, line 41
-            // [, line 41
-            ket = cursor;
-            // substring, line 41
-            among_var = find_among_b(a_0, 32);
-            if (among_var == 0)
-            {
-                limit_backward = v_2;
-                return false;
-            }
-            // ], line 41
-            bra = cursor;
-            limit_backward = v_2;
-            switch(among_var) {
-                case 0:
-                    return false;
-                case 1:
-                    // (, line 48
-                    // delete, line 48
-                    slice_del();
-                    break;
-                case 2:
-                    // (, line 50
-                    if (!(in_grouping_b(g_s_ending, 97, 229)))
-                    {
-                        return false;
-                    }
-                    // delete, line 50
-                    slice_del();
-                    break;
-            }
-            return true;
-        }
-
-        private boolean r_consonant_pair() {
-            int v_1;
-            int v_2;
-            int v_3;
-            // (, line 54
-            // test, line 55
-            v_1 = limit - cursor;
-            // (, line 55
-            // setlimit, line 56
-            v_2 = limit - cursor;
-            // tomark, line 56
-            if (cursor < I_p1)
-            {
-                return false;
-            }
-            cursor = I_p1;
-            v_3 = limit_backward;
-            limit_backward = cursor;
-            cursor = limit - v_2;
-            // (, line 56
-            // [, line 56
-            ket = cursor;
-            // substring, line 56
-            if (find_among_b(a_1, 4) == 0)
-            {
-                limit_backward = v_3;
-                return false;
-            }
-            // ], line 56
-            bra = cursor;
-            limit_backward = v_3;
-            cursor = limit - v_1;
-            // next, line 62
-            if (cursor <= limit_backward)
-            {
-                return false;
-            }
-            cursor--;
-            // ], line 62
-            bra = cursor;
-            // delete, line 62
-            slice_del();
-            return true;
-        }
-
-        private boolean r_other_suffix() {
-            int among_var;
-            int v_1;
-            int v_2;
-            int v_3;
-            int v_4;
-            // (, line 65
-            // do, line 66
-            v_1 = limit - cursor;
-            lab0: do {
-                // (, line 66
-                // [, line 66
-                ket = cursor;
-                // literal, line 66
-                if (!(eq_s_b(2, "st")))
-                {
-                    break lab0;
-                }
-                // ], line 66
-                bra = cursor;
-                // literal, line 66
-                if (!(eq_s_b(2, "ig")))
-                {
-                    break lab0;
-                }
-                // delete, line 66
-                slice_del();
-            } while (false);
-            cursor = limit - v_1;
-            // setlimit, line 67
-            v_2 = limit - cursor;
-            // tomark, line 67
-            if (cursor < I_p1)
-            {
-                return false;
-            }
-            cursor = I_p1;
-            v_3 = limit_backward;
-            limit_backward = cursor;
-            cursor = limit - v_2;
-            // (, line 67
-            // [, line 67
-            ket = cursor;
-            // substring, line 67
-            among_var = find_among_b(a_2, 5);
-            if (among_var == 0)
-            {
-                limit_backward = v_3;
-                return false;
-            }
-            // ], line 67
-            bra = cursor;
-            limit_backward = v_3;
-            switch(among_var) {
-                case 0:
-                    return false;
-                case 1:
-                    // (, line 70
-                    // delete, line 70
-                    slice_del();
-                    // do, line 70
-                    v_4 = limit - cursor;
-                    lab1: do {
-                        // call consonant_pair, line 70
-                        if (!r_consonant_pair())
-                        {
-                            break lab1;
-                        }
-                    } while (false);
-                    cursor = limit - v_4;
-                    break;
-                case 2:
-                    // (, line 72
-                    // <-, line 72
-                    slice_from("l\u00F8s");
-                    break;
-            }
-            return true;
-        }
-
-        private boolean r_undouble() {
-            int v_1;
-            int v_2;
-            // (, line 75
-            // setlimit, line 76
-            v_1 = limit - cursor;
-            // tomark, line 76
-            if (cursor < I_p1)
-            {
-                return false;
-            }
-            cursor = I_p1;
-            v_2 = limit_backward;
-            limit_backward = cursor;
-            cursor = limit - v_1;
-            // (, line 76
-            // [, line 76
-            ket = cursor;
-            if (!(out_grouping_b(g_v, 97, 248)))
-            {
-                limit_backward = v_2;
-                return false;
-            }
-            // ], line 76
-            bra = cursor;
-            // -> ch, line 76
-            S_ch = slice_to(S_ch);
-            limit_backward = v_2;
-            // name ch, line 77
-            if (!(eq_v_b(S_ch)))
-            {
-                return false;
-            }
-            // delete, line 78
-            slice_del();
-            return true;
-        }
-
-        public boolean stem() {
-            int v_1;
-            int v_2;
-            int v_3;
-            int v_4;
-            int v_5;
-            // (, line 82
-            // do, line 84
-            v_1 = cursor;
-            lab0: do {
-                // call mark_regions, line 84
-                if (!r_mark_regions())
-                {
-                    break lab0;
-                }
-            } while (false);
-            cursor = v_1;
-            // backwards, line 85
-            limit_backward = cursor; cursor = limit;
-            // (, line 85
-            // do, line 86
-            v_2 = limit - cursor;
-            lab1: do {
-                // call main_suffix, line 86
-                if (!r_main_suffix())
-                {
-                    break lab1;
-                }
-            } while (false);
-            cursor = limit - v_2;
-            // do, line 87
-            v_3 = limit - cursor;
-            lab2: do {
-                // call consonant_pair, line 87
-                if (!r_consonant_pair())
-                {
-                    break lab2;
-                }
-            } while (false);
-            cursor = limit - v_3;
-            // do, line 88
-            v_4 = limit - cursor;
-            lab3: do {
-                // call other_suffix, line 88
-                if (!r_other_suffix())
-                {
-                    break lab3;
-                }
-            } while (false);
-            cursor = limit - v_4;
-            // do, line 89
-            v_5 = limit - cursor;
-            lab4: do {
-                // call undouble, line 89
-                if (!r_undouble())
-                {
-                    break lab4;
-                }
-            } while (false);
-            cursor = limit - v_5;
-            cursor = limit_backward;            return true;
-        }
-
-}
-
Index: contrib/snowball/src/java/org/tartarus/snowball/ext/DutchStemmer.java
===================================================================
--- contrib/snowball/src/java/org/tartarus/snowball/ext/DutchStemmer.java	(revision 916567)
+++ contrib/snowball/src/java/org/tartarus/snowball/ext/DutchStemmer.java	(working copy)
@@ -1,837 +0,0 @@
-// This file was generated automatically by the Snowball to Java compiler
-
-package org.tartarus.snowball.ext;
-import org.tartarus.snowball.SnowballProgram;
-import org.tartarus.snowball.Among;
-
-/**
- * Generated class implementing code defined by a snowball script.
- */
-public class DutchStemmer extends SnowballProgram {
-
-        private Among a_0[] = {
-            new Among ( "", -1, 6, "", this),
-            new Among ( "\u00E1", 0, 1, "", this),
-            new Among ( "\u00E4", 0, 1, "", this),
-            new Among ( "\u00E9", 0, 2, "", this),
-            new Among ( "\u00EB", 0, 2, "", this),
-            new Among ( "\u00ED", 0, 3, "", this),
-            new Among ( "\u00EF", 0, 3, "", this),
-            new Among ( "\u00F3", 0, 4, "", this),
-            new Among ( "\u00F6", 0, 4, "", this),
-            new Among ( "\u00FA", 0, 5, "", this),
-            new Among ( "\u00FC", 0, 5, "", this)
-        };
-
-        private Among a_1[] = {
-            new Among ( "", -1, 3, "", this),
-            new Among ( "I", 0, 2, "", this),
-            new Among ( "Y", 0, 1, "", this)
-        };
-
-        private Among a_2[] = {
-            new Among ( "dd", -1, -1, "", this),
-            new Among ( "kk", -1, -1, "", this),
-            new Among ( "tt", -1, -1, "", this)
-        };
-
-        private Among a_3[] = {
-            new Among ( "ene", -1, 2, "", this),
-            new Among ( "se", -1, 3, "", this),
-            new Among ( "en", -1, 2, "", this),
-            new Among ( "heden", 2, 1, "", this),
-            new Among ( "s", -1, 3, "", this)
-        };
-
-        private Among a_4[] = {
-            new Among ( "end", -1, 1, "", this),
-            new Among ( "ig", -1, 2, "", this),
-            new Among ( "ing", -1, 1, "", this),
-            new Among ( "lijk", -1, 3, "", this),
-            new Among ( "baar", -1, 4, "", this),
-            new Among ( "bar", -1, 5, "", this)
-        };
-
-        private Among a_5[] = {
-            new Among ( "aa", -1, -1, "", this),
-            new Among ( "ee", -1, -1, "", this),
-            new Among ( "oo", -1, -1, "", this),
-            new Among ( "uu", -1, -1, "", this)
-        };
-
-        private static final char g_v[] = {17, 65, 16, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 128 };
-
-        private static final char g_v_I[] = {1, 0, 0, 17, 65, 16, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 128 };
-
-        private static final char g_v_j[] = {17, 67, 16, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 128 };
-
-        private int I_p2;
-        private int I_p1;
-        private boolean B_e_found;
-
-        private void copy_from(DutchStemmer other) {
-            I_p2 = other.I_p2;
-            I_p1 = other.I_p1;
-            B_e_found = other.B_e_found;
-            super.copy_from(other);
-        }
-
-        private boolean r_prelude() {
-            int among_var;
-            int v_1;
-            int v_2;
-            int v_3;
-            int v_4;
-            int v_5;
-            int v_6;
-            // (, line 41
-            // test, line 42
-            v_1 = cursor;
-            // repeat, line 42
-            replab0: while(true)
-            {
-                v_2 = cursor;
-                lab1: do {
-                    // (, line 42
-                    // [, line 43
-                    bra = cursor;
-                    // substring, line 43
-                    among_var = find_among(a_0, 11);
-                    if (among_var == 0)
-                    {
-                        break lab1;
-                    }
-                    // ], line 43
-                    ket = cursor;
-                    switch(among_var) {
-                        case 0:
-                            break lab1;
-                        case 1:
-                            // (, line 45
-                            // <-, line 45
-                            slice_from("a");
-                            break;
-                        case 2:
-                            // (, line 47
-                            // <-, line 47
-                            slice_from("e");
-                            break;
-                        case 3:
-                            // (, line 49
-                            // <-, line 49
-                            slice_from("i");
-                            break;
-                        case 4:
-                            // (, line 51
-                            // <-, line 51
-                            slice_from("o");
-                            break;
-                        case 5:
-                            // (, line 53
-                            // <-, line 53
-                            slice_from("u");
-                            break;
-                        case 6:
-                            // (, line 54
-                            // next, line 54
-                            if (cursor >= limit)
-                            {
-                                break lab1;
-                            }
-                            cursor++;
-                            break;
-                    }
-                    continue replab0;
-                } while (false);
-                cursor = v_2;
-                break replab0;
-            }
-            cursor = v_1;
-            // try, line 57
-            v_3 = cursor;
-            lab2: do {
-                // (, line 57
-                // [, line 57
-                bra = cursor;
-                // literal, line 57
-                if (!(eq_s(1, "y")))
-                {
-                    cursor = v_3;
-                    break lab2;
-                }
-                // ], line 57
-                ket = cursor;
-                // <-, line 57
-                slice_from("Y");
-            } while (false);
-            // repeat, line 58
-            replab3: while(true)
-            {
-                v_4 = cursor;
-                lab4: do {
-                    // goto, line 58
-                    golab5: while(true)
-                    {
-                        v_5 = cursor;
-                        lab6: do {
-                            // (, line 58
-                            if (!(in_grouping(g_v, 97, 232)))
-                            {
-                                break lab6;
-                            }
-                            // [, line 59
-                            bra = cursor;
-                            // or, line 59
-                            lab7: do {
-                                v_6 = cursor;
-                                lab8: do {
-                                    // (, line 59
-                                    // literal, line 59
-                                    if (!(eq_s(1, "i")))
-                                    {
-                                        break lab8;
-                                    }
-                                    // ], line 59
-                                    ket = cursor;
-                                    if (!(in_grouping(g_v, 97, 232)))
-                                    {
-                                        break lab8;
-                                    }
-                                    // <-, line 59
-                                    slice_from("I");
-                                    break lab7;
-                                } while (false);
-                                cursor = v_6;
-                                // (, line 60
-                                // literal, line 60
-                                if (!(eq_s(1, "y")))
-                                {
-                                    break lab6;
-                                }
-                                // ], line 60
-                                ket = cursor;
-                                // <-, line 60
-                                slice_from("Y");
-                            } while (false);
-                            cursor = v_5;
-                            break golab5;
-                        } while (false);
-                        cursor = v_5;
-                        if (cursor >= limit)
-                        {
-                            break lab4;
-                        }
-                        cursor++;
-                    }
-                    continue replab3;
-                } while (false);
-                cursor = v_4;
-                break replab3;
-            }
-            return true;
-        }
-
-        private boolean r_mark_regions() {
-            // (, line 64
-            I_p1 = limit;
-            I_p2 = limit;
-            // gopast, line 69
-            golab0: while(true)
-            {
-                lab1: do {
-                    if (!(in_grouping(g_v, 97, 232)))
-                    {
-                        break lab1;
-                    }
-                    break golab0;
-                } while (false);
-                if (cursor >= limit)
-                {
-                    return false;
-                }
-                cursor++;
-            }
-            // gopast, line 69
-            golab2: while(true)
-            {
-                lab3: do {
-                    if (!(out_grouping(g_v, 97, 232)))
-                    {
-                        break lab3;
-                    }
-                    break golab2;
-                } while (false);
-                if (cursor >= limit)
-                {
-                    return false;
-                }
-                cursor++;
-            }
-            // setmark p1, line 69
-            I_p1 = cursor;
-            // try, line 70
-            lab4: do {
-                // (, line 70
-                if (!(I_p1 < 3))
-                {
-                    break lab4;
-                }
-                I_p1 = 3;
-            } while (false);
-            // gopast, line 71
-            golab5: while(true)
-            {
-                lab6: do {
-                    if (!(in_grouping(g_v, 97, 232)))
-                    {
-                        break lab6;
-                    }
-                    break golab5;
-                } while (false);
-                if (cursor >= limit)
-                {
-                    return false;
-                }
-                cursor++;
-            }
-            // gopast, line 71
-            golab7: while(true)
-            {
-                lab8: do {
-                    if (!(out_grouping(g_v, 97, 232)))
-                    {
-                        break lab8;
-                    }
-                    break golab7;
-                } while (false);
-                if (cursor >= limit)
-                {
-                    return false;
-                }
-                cursor++;
-            }
-            // setmark p2, line 71
-            I_p2 = cursor;
-            return true;
-        }
-
-        private boolean r_postlude() {
-            int among_var;
-            int v_1;
-            // repeat, line 75
-            replab0: while(true)
-            {
-                v_1 = cursor;
-                lab1: do {
-                    // (, line 75
-                    // [, line 77
-                    bra = cursor;
-                    // substring, line 77
-                    among_var = find_among(a_1, 3);
-                    if (among_var == 0)
-                    {
-                        break lab1;
-                    }
-                    // ], line 77
-                    ket = cursor;
-                    switch(among_var) {
-                        case 0:
-                            break lab1;
-                        case 1:
-                            // (, line 78
-                            // <-, line 78
-                            slice_from("y");
-                            break;
-                        case 2:
-                            // (, line 79
-                            // <-, line 79
-                            slice_from("i");
-                            break;
-                        case 3:
-                            // (, line 80
-                            // next, line 80
-                            if (cursor >= limit)
-                            {
-                                break lab1;
-                            }
-                            cursor++;
-                            break;
-                    }
-                    continue replab0;
-                } while (false);
-                cursor = v_1;
-                break replab0;
-            }
-            return true;
-        }
-
-        private boolean r_R1() {
-            if (!(I_p1 <= cursor))
-            {
-                return false;
-            }
-            return true;
-        }
-
-        private boolean r_R2() {
-            if (!(I_p2 <= cursor))
-            {
-                return false;
-            }
-            return true;
-        }
-
-        private boolean r_undouble() {
-            int v_1;
-            // (, line 90
-            // test, line 91
-            v_1 = limit - cursor;
-            // among, line 91
-            if (find_among_b(a_2, 3) == 0)
-            {
-                return false;
-            }
-            cursor = limit - v_1;
-            // [, line 91
-            ket = cursor;
-            // next, line 91
-            if (cursor <= limit_backward)
-            {
-                return false;
-            }
-            cursor--;
-            // ], line 91
-            bra = cursor;
-            // delete, line 91
-            slice_del();
-            return true;
-        }
-
-        private boolean r_e_ending() {
-            int v_1;
-            // (, line 94
-            // unset e_found, line 95
-            B_e_found = false;
-            // [, line 96
-            ket = cursor;
-            // literal, line 96
-            if (!(eq_s_b(1, "e")))
-            {
-                return false;
-            }
-            // ], line 96
-            bra = cursor;
-            // call R1, line 96
-            if (!r_R1())
-            {
-                return false;
-            }
-            // test, line 96
-            v_1 = limit - cursor;
-            if (!(out_grouping_b(g_v, 97, 232)))
-            {
-                return false;
-            }
-            cursor = limit - v_1;
-            // delete, line 96
-            slice_del();
-            // set e_found, line 97
-            B_e_found = true;
-            // call undouble, line 98
-            if (!r_undouble())
-            {
-                return false;
-            }
-            return true;
-        }
-
-        private boolean r_en_ending() {
-            int v_1;
-            int v_2;
-            // (, line 101
-            // call R1, line 102
-            if (!r_R1())
-            {
-                return false;
-            }
-            // and, line 102
-            v_1 = limit - cursor;
-            if (!(out_grouping_b(g_v, 97, 232)))
-            {
-                return false;
-            }
-            cursor = limit - v_1;
-            // not, line 102
-            {
-                v_2 = limit - cursor;
-                lab0: do {
-                    // literal, line 102
-                    if (!(eq_s_b(3, "gem")))
-                    {
-                        break lab0;
-                    }
-                    return false;
-                } while (false);
-                cursor = limit - v_2;
-            }
-            // delete, line 102
-            slice_del();
-            // call undouble, line 103
-            if (!r_undouble())
-            {
-                return false;
-            }
-            return true;
-        }
-
-        private boolean r_standard_suffix() {
-            int among_var;
-            int v_1;
-            int v_2;
-            int v_3;
-            int v_4;
-            int v_5;
-            int v_6;
-            int v_7;
-            int v_8;
-            int v_9;
-            int v_10;
-            // (, line 106
-            // do, line 107
-            v_1 = limit - cursor;
-            lab0: do {
-                // (, line 107
-                // [, line 108
-                ket = cursor;
-                // substring, line 108
-                among_var = find_among_b(a_3, 5);
-                if (among_var == 0)
-                {
-                    break lab0;
-                }
-                // ], line 108
-                bra = cursor;
-                switch(among_var) {
-                    case 0:
-                        break lab0;
-                    case 1:
-                        // (, line 110
-                        // call R1, line 110
-                        if (!r_R1())
-                        {
-                            break lab0;
-                        }
-                        // <-, line 110
-                        slice_from("heid");
-                        break;
-                    case 2:
-                        // (, line 113
-                        // call en_ending, line 113
-                        if (!r_en_ending())
-                        {
-                            break lab0;
-                        }
-                        break;
-                    case 3:
-                        // (, line 116
-                        // call R1, line 116
-                        if (!r_R1())
-                        {
-                            break lab0;
-                        }
-                        if (!(out_grouping_b(g_v_j, 97, 232)))
-                        {
-                            break lab0;
-                        }
-                        // delete, line 116
-                        slice_del();
-                        break;
-                }
-            } while (false);
-            cursor = limit - v_1;
-            // do, line 120
-            v_2 = limit - cursor;
-            lab1: do {
-                // call e_ending, line 120
-                if (!r_e_ending())
-                {
-                    break lab1;
-                }
-            } while (false);
-            cursor = limit - v_2;
-            // do, line 122
-            v_3 = limit - cursor;
-            lab2: do {
-                // (, line 122
-                // [, line 122
-                ket = cursor;
-                // literal, line 122
-                if (!(eq_s_b(4, "heid")))
-                {
-                    break lab2;
-                }
-                // ], line 122
-                bra = cursor;
-                // call R2, line 122
-                if (!r_R2())
-                {
-                    break lab2;
-                }
-                // not, line 122
-                {
-                    v_4 = limit - cursor;
-                    lab3: do {
-                        // literal, line 122
-                        if (!(eq_s_b(1, "c")))
-                        {
-                            break lab3;
-                        }
-                        break lab2;
-                    } while (false);
-                    cursor = limit - v_4;
-                }
-                // delete, line 122
-                slice_del();
-                // [, line 123
-                ket = cursor;
-                // literal, line 123
-                if (!(eq_s_b(2, "en")))
-                {
-                    break lab2;
-                }
-                // ], line 123
-                bra = cursor;
-                // call en_ending, line 123
-                if (!r_en_ending())
-                {
-                    break lab2;
-                }
-            } while (false);
-            cursor = limit - v_3;
-            // do, line 126
-            v_5 = limit - cursor;
-            lab4: do {
-                // (, line 126
-                // [, line 127
-                ket = cursor;
-                // substring, line 127
-                among_var = find_among_b(a_4, 6);
-                if (among_var == 0)
-                {
-                    break lab4;
-                }
-                // ], line 127
-                bra = cursor;
-                switch(among_var) {
-                    case 0:
-                        break lab4;
-                    case 1:
-                        // (, line 129
-                        // call R2, line 129
-                        if (!r_R2())
-                        {
-                            break lab4;
-                        }
-                        // delete, line 129
-                        slice_del();
-                        // or, line 130
-                        lab5: do {
-                            v_6 = limit - cursor;
-                            lab6: do {
-                                // (, line 130
-                                // [, line 130
-                                ket = cursor;
-                                // literal, line 130
-                                if (!(eq_s_b(2, "ig")))
-                                {
-                                    break lab6;
-                                }
-                                // ], line 130
-                                bra = cursor;
-                                // call R2, line 130
-                                if (!r_R2())
-                                {
-                                    break lab6;
-                                }
-                                // not, line 130
-                                {
-                                    v_7 = limit - cursor;
-                                    lab7: do {
-                                        // literal, line 130
-                                        if (!(eq_s_b(1, "e")))
-                                        {
-                                            break lab7;
-                                        }
-                                        break lab6;
-                                    } while (false);
-                                    cursor = limit - v_7;
-                                }
-                                // delete, line 130
-                                slice_del();
-                                break lab5;
-                            } while (false);
-                            cursor = limit - v_6;
-                            // call undouble, line 130
-                            if (!r_undouble())
-                            {
-                                break lab4;
-                            }
-                        } while (false);
-                        break;
-                    case 2:
-                        // (, line 133
-                        // call R2, line 133
-                        if (!r_R2())
-                        {
-                            break lab4;
-                        }
-                        // not, line 133
-                        {
-                            v_8 = limit - cursor;
-                            lab8: do {
-                                // literal, line 133
-                                if (!(eq_s_b(1, "e")))
-                                {
-                                    break lab8;
-                                }
-                                break lab4;
-                            } while (false);
-                            cursor = limit - v_8;
-                        }
-                        // delete, line 133
-                        slice_del();
-                        break;
-                    case 3:
-                        // (, line 136
-                        // call R2, line 136
-                        if (!r_R2())
-                        {
-                            break lab4;
-                        }
-                        // delete, line 136
-                        slice_del();
-                        // call e_ending, line 136
-                        if (!r_e_ending())
-                        {
-                            break lab4;
-                        }
-                        break;
-                    case 4:
-                        // (, line 139
-                        // call R2, line 139
-                        if (!r_R2())
-                        {
-                            break lab4;
-                        }
-                        // delete, line 139
-                        slice_del();
-                        break;
-                    case 5:
-                        // (, line 142
-                        // call R2, line 142
-                        if (!r_R2())
-                        {
-                            break lab4;
-                        }
-                        // Boolean test e_found, line 142
-                        if (!(B_e_found))
-                        {
-                            break lab4;
-                        }
-                        // delete, line 142
-                        slice_del();
-                        break;
-                }
-            } while (false);
-            cursor = limit - v_5;
-            // do, line 146
-            v_9 = limit - cursor;
-            lab9: do {
-                // (, line 146
-                if (!(out_grouping_b(g_v_I, 73, 232)))
-                {
-                    break lab9;
-                }
-                // test, line 148
-                v_10 = limit - cursor;
-                // (, line 148
-                // among, line 149
-                if (find_among_b(a_5, 4) == 0)
-                {
-                    break lab9;
-                }
-                if (!(out_grouping_b(g_v, 97, 232)))
-                {
-                    break lab9;
-                }
-                cursor = limit - v_10;
-                // [, line 152
-                ket = cursor;
-                // next, line 152
-                if (cursor <= limit_backward)
-                {
-                    break lab9;
-                }
-                cursor--;
-                // ], line 152
-                bra = cursor;
-                // delete, line 152
-                slice_del();
-            } while (false);
-            cursor = limit - v_9;
-            return true;
-        }
-
-        public boolean stem() {
-            int v_1;
-            int v_2;
-            int v_3;
-            int v_4;
-            // (, line 157
-            // do, line 159
-            v_1 = cursor;
-            lab0: do {
-                // call prelude, line 159
-                if (!r_prelude())
-                {
-                    break lab0;
-                }
-            } while (false);
-            cursor = v_1;
-            // do, line 160
-            v_2 = cursor;
-            lab1: do {
-                // call mark_regions, line 160
-                if (!r_mark_regions())
-                {
-                    break lab1;
-                }
-            } while (false);
-            cursor = v_2;
-            // backwards, line 161
-            limit_backward = cursor; cursor = limit;
-            // do, line 162
-            v_3 = limit - cursor;
-            lab2: do {
-                // call standard_suffix, line 162
-                if (!r_standard_suffix())
-                {
-                    break lab2;
-                }
-            } while (false);
-            cursor = limit - v_3;
-            cursor = limit_backward;            // do, line 163
-            v_4 = cursor;
-            lab3: do {
-                // call postlude, line 163
-                if (!r_postlude())
-                {
-                    break lab3;
-                }
-            } while (false);
-            cursor = v_4;
-            return true;
-        }
-
-}
-
Index: contrib/snowball/src/java/org/tartarus/snowball/ext/GermanStemmer.java
===================================================================
--- contrib/snowball/src/java/org/tartarus/snowball/ext/GermanStemmer.java	(revision 916567)
+++ contrib/snowball/src/java/org/tartarus/snowball/ext/GermanStemmer.java	(working copy)
@@ -1,688 +0,0 @@
-// This file was generated automatically by the Snowball to Java compiler
-
-package org.tartarus.snowball.ext;
-import org.tartarus.snowball.SnowballProgram;
-import org.tartarus.snowball.Among;
-
-/**
- * Generated class implementing code defined by a snowball script.
- */
-public class GermanStemmer extends SnowballProgram {
-
-        private Among a_0[] = {
-            new Among ( "", -1, 6, "", this),
-            new Among ( "U", 0, 2, "", this),
-            new Among ( "Y", 0, 1, "", this),
-            new Among ( "\u00E4", 0, 3, "", this),
-            new Among ( "\u00F6", 0, 4, "", this),
-            new Among ( "\u00FC", 0, 5, "", this)
-        };
-
-        private Among a_1[] = {
-            new Among ( "e", -1, 1, "", this),
-            new Among ( "em", -1, 1, "", this),
-            new Among ( "en", -1, 1, "", this),
-            new Among ( "ern", -1, 1, "", this),
-            new Among ( "er", -1, 1, "", this),
-            new Among ( "s", -1, 2, "", this),
-            new Among ( "es", 5, 1, "", this)
-        };
-
-        private Among a_2[] = {
-            new Among ( "en", -1, 1, "", this),
-            new Among ( "er", -1, 1, "", this),
-            new Among ( "st", -1, 2, "", this),
-            new Among ( "est", 2, 1, "", this)
-        };
-
-        private Among a_3[] = {
-            new Among ( "ig", -1, 1, "", this),
-            new Among ( "lich", -1, 1, "", this)
-        };
-
-        private Among a_4[] = {
-            new Among ( "end", -1, 1, "", this),
-            new Among ( "ig", -1, 2, "", this),
-            new Among ( "ung", -1, 1, "", this),
-            new Among ( "lich", -1, 3, "", this),
-            new Among ( "isch", -1, 2, "", this),
-            new Among ( "ik", -1, 2, "", this),
-            new Among ( "heit", -1, 3, "", this),
-            new Among ( "keit", -1, 4, "", this)
-        };
-
-        private static final char g_v[] = {17, 65, 16, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 0, 32, 8 };
-
-        private static final char g_s_ending[] = {117, 30, 5 };
-
-        private static final char g_st_ending[] = {117, 30, 4 };
-
-        private int I_x;
-        private int I_p2;
-        private int I_p1;
-
-        private void copy_from(GermanStemmer other) {
-            I_x = other.I_x;
-            I_p2 = other.I_p2;
-            I_p1 = other.I_p1;
-            super.copy_from(other);
-        }
-
-        private boolean r_prelude() {
-            int v_1;
-            int v_2;
-            int v_3;
-            int v_4;
-            int v_5;
-            int v_6;
-            // (, line 28
-            // test, line 30
-            v_1 = cursor;
-            // repeat, line 30
-            replab0: while(true)
-            {
-                v_2 = cursor;
-                lab1: do {
-                    // (, line 30
-                    // or, line 33
-                    lab2: do {
-                        v_3 = cursor;
-                        lab3: do {
-                            // (, line 31
-                            // [, line 32
-                            bra = cursor;
-                            // literal, line 32
-                            if (!(eq_s(1, "\u00DF")))
-                            {
-                                break lab3;
-                            }
-                            // ], line 32
-                            ket = cursor;
-                            // <-, line 32
-                            slice_from("ss");
-                            break lab2;
-                        } while (false);
-                        cursor = v_3;
-                        // next, line 33
-                        if (cursor >= limit)
-                        {
-                            break lab1;
-                        }
-                        cursor++;
-                    } while (false);
-                    continue replab0;
-                } while (false);
-                cursor = v_2;
-                break replab0;
-            }
-            cursor = v_1;
-            // repeat, line 36
-            replab4: while(true)
-            {
-                v_4 = cursor;
-                lab5: do {
-                    // goto, line 36
-                    golab6: while(true)
-                    {
-                        v_5 = cursor;
-                        lab7: do {
-                            // (, line 36
-                            if (!(in_grouping(g_v, 97, 252)))
-                            {
-                                break lab7;
-                            }
-                            // [, line 37
-                            bra = cursor;
-                            // or, line 37
-                            lab8: do {
-                                v_6 = cursor;
-                                lab9: do {
-                                    // (, line 37
-                                    // literal, line 37
-                                    if (!(eq_s(1, "u")))
-                                    {
-                                        break lab9;
-                                    }
-                                    // ], line 37
-                                    ket = cursor;
-                                    if (!(in_grouping(g_v, 97, 252)))
-                                    {
-                                        break lab9;
-                                    }
-                                    // <-, line 37
-                                    slice_from("U");
-                                    break lab8;
-                                } while (false);
-                                cursor = v_6;
-                                // (, line 38
-                                // literal, line 38
-                                if (!(eq_s(1, "y")))
-                                {
-                                    break lab7;
-                                }
-                                // ], line 38
-                                ket = cursor;
-                                if (!(in_grouping(g_v, 97, 252)))
-                                {
-                                    break lab7;
-                                }
-                                // <-, line 38
-                                slice_from("Y");
-                            } while (false);
-                            cursor = v_5;
-                            break golab6;
-                        } while (false);
-                        cursor = v_5;
-                        if (cursor >= limit)
-                        {
-                            break lab5;
-                        }
-                        cursor++;
-                    }
-                    continue replab4;
-                } while (false);
-                cursor = v_4;
-                break replab4;
-            }
-            return true;
-        }
-
-        private boolean r_mark_regions() {
-            int v_1;
-            // (, line 42
-            I_p1 = limit;
-            I_p2 = limit;
-            // test, line 47
-            v_1 = cursor;
-            // (, line 47
-            // hop, line 47
-            {
-                int c = cursor + 3;
-                if (0 > c || c > limit)
-                {
-                    return false;
-                }
-                cursor = c;
-            }
-            // setmark x, line 47
-            I_x = cursor;
-            cursor = v_1;
-            // gopast, line 49
-            golab0: while(true)
-            {
-                lab1: do {
-                    if (!(in_grouping(g_v, 97, 252)))
-                    {
-                        break lab1;
-                    }
-                    break golab0;
-                } while (false);
-                if (cursor >= limit)
-                {
-                    return false;
-                }
-                cursor++;
-            }
-            // gopast, line 49
-            golab2: while(true)
-            {
-                lab3: do {
-                    if (!(out_grouping(g_v, 97, 252)))
-                    {
-                        break lab3;
-                    }
-                    break golab2;
-                } while (false);
-                if (cursor >= limit)
-                {
-                    return false;
-                }
-                cursor++;
-            }
-            // setmark p1, line 49
-            I_p1 = cursor;
-            // try, line 50
-            lab4: do {
-                // (, line 50
-                if (!(I_p1 < I_x))
-                {
-                    break lab4;
-                }
-                I_p1 = I_x;
-            } while (false);
-            // gopast, line 51
-            golab5: while(true)
-            {
-                lab6: do {
-                    if (!(in_grouping(g_v, 97, 252)))
-                    {
-                        break lab6;
-                    }
-                    break golab5;
-                } while (false);
-                if (cursor >= limit)
-                {
-                    return false;
-                }
-                cursor++;
-            }
-            // gopast, line 51
-            golab7: while(true)
-            {
-                lab8: do {
-                    if (!(out_grouping(g_v, 97, 252)))
-                    {
-                        break lab8;
-                    }
-                    break golab7;
-                } while (false);
-                if (cursor >= limit)
-                {
-                    return false;
-                }
-                cursor++;
-            }
-            // setmark p2, line 51
-            I_p2 = cursor;
-            return true;
-        }
-
-        private boolean r_postlude() {
-            int among_var;
-            int v_1;
-            // repeat, line 55
-            replab0: while(true)
-            {
-                v_1 = cursor;
-                lab1: do {
-                    // (, line 55
-                    // [, line 57
-                    bra = cursor;
-                    // substring, line 57
-                    among_var = find_among(a_0, 6);
-                    if (among_var == 0)
-                    {
-                        break lab1;
-                    }
-                    // ], line 57
-                    ket = cursor;
-                    switch(among_var) {
-                        case 0:
-                            break lab1;
-                        case 1:
-                            // (, line 58
-                            // <-, line 58
-                            slice_from("y");
-                            break;
-                        case 2:
-                            // (, line 59
-                            // <-, line 59
-                            slice_from("u");
-                            break;
-                        case 3:
-                            // (, line 60
-                            // <-, line 60
-                            slice_from("a");
-                            break;
-                        case 4:
-                            // (, line 61
-                            // <-, line 61
-                            slice_from("o");
-                            break;
-                        case 5:
-                            // (, line 62
-                            // <-, line 62
-                            slice_from("u");
-                            break;
-                        case 6:
-                            // (, line 63
-                            // next, line 63
-                            if (cursor >= limit)
-                            {
-                                break lab1;
-                            }
-                            cursor++;
-                            break;
-                    }
-                    continue replab0;
-                } while (false);
-                cursor = v_1;
-                break replab0;
-            }
-            return true;
-        }
-
-        private boolean r_R1() {
-            if (!(I_p1 <= cursor))
-            {
-                return false;
-            }
-            return true;
-        }
-
-        private boolean r_R2() {
-            if (!(I_p2 <= cursor))
-            {
-                return false;
-            }
-            return true;
-        }
-
-        private boolean r_standard_suffix() {
-            int among_var;
-            int v_1;
-            int v_2;
-            int v_3;
-            int v_4;
-            int v_5;
-            int v_6;
-            int v_7;
-            int v_8;
-            int v_9;
-            // (, line 73
-            // do, line 74
-            v_1 = limit - cursor;
-            lab0: do {
-                // (, line 74
-                // [, line 75
-                ket = cursor;
-                // substring, line 75
-                among_var = find_among_b(a_1, 7);
-                if (among_var == 0)
-                {
-                    break lab0;
-                }
-                // ], line 75
-                bra = cursor;
-                // call R1, line 75
-                if (!r_R1())
-                {
-                    break lab0;
-                }
-                switch(among_var) {
-                    case 0:
-                        break lab0;
-                    case 1:
-                        // (, line 77
-                        // delete, line 77
-                        slice_del();
-                        break;
-                    case 2:
-                        // (, line 80
-                        if (!(in_grouping_b(g_s_ending, 98, 116)))
-                        {
-                            break lab0;
-                        }
-                        // delete, line 80
-                        slice_del();
-                        break;
-                }
-            } while (false);
-            cursor = limit - v_1;
-            // do, line 84
-            v_2 = limit - cursor;
-            lab1: do {
-                // (, line 84
-                // [, line 85
-                ket = cursor;
-                // substring, line 85
-                among_var = find_among_b(a_2, 4);
-                if (among_var == 0)
-                {
-                    break lab1;
-                }
-                // ], line 85
-                bra = cursor;
-                // call R1, line 85
-                if (!r_R1())
-                {
-                    break lab1;
-                }
-                switch(among_var) {
-                    case 0:
-                        break lab1;
-                    case 1:
-                        // (, line 87
-                        // delete, line 87
-                        slice_del();
-                        break;
-                    case 2:
-                        // (, line 90
-                        if (!(in_grouping_b(g_st_ending, 98, 116)))
-                        {
-                            break lab1;
-                        }
-                        // hop, line 90
-                        {
-                            int c = cursor - 3;
-                            if (limit_backward > c || c > limit)
-                            {
-                                break lab1;
-                            }
-                            cursor = c;
-                        }
-                        // delete, line 90
-                        slice_del();
-                        break;
-                }
-            } while (false);
-            cursor = limit - v_2;
-            // do, line 94
-            v_3 = limit - cursor;
-            lab2: do {
-                // (, line 94
-                // [, line 95
-                ket = cursor;
-                // substring, line 95
-                among_var = find_among_b(a_4, 8);
-                if (among_var == 0)
-                {
-                    break lab2;
-                }
-                // ], line 95
-                bra = cursor;
-                // call R2, line 95
-                if (!r_R2())
-                {
-                    break lab2;
-                }
-                switch(among_var) {
-                    case 0:
-                        break lab2;
-                    case 1:
-                        // (, line 97
-                        // delete, line 97
-                        slice_del();
-                        // try, line 98
-                        v_4 = limit - cursor;
-                        lab3: do {
-                            // (, line 98
-                            // [, line 98
-                            ket = cursor;
-                            // literal, line 98
-                            if (!(eq_s_b(2, "ig")))
-                            {
-                                cursor = limit - v_4;
-                                break lab3;
-                            }
-                            // ], line 98
-                            bra = cursor;
-                            // not, line 98
-                            {
-                                v_5 = limit - cursor;
-                                lab4: do {
-                                    // literal, line 98
-                                    if (!(eq_s_b(1, "e")))
-                                    {
-                                        break lab4;
-                                    }
-                                    cursor = limit - v_4;
-                                    break lab3;
-                                } while (false);
-                                cursor = limit - v_5;
-                            }
-                            // call R2, line 98
-                            if (!r_R2())
-                            {
-                                cursor = limit - v_4;
-                                break lab3;
-                            }
-                            // delete, line 98
-                            slice_del();
-                        } while (false);
-                        break;
-                    case 2:
-                        // (, line 101
-                        // not, line 101
-                        {
-                            v_6 = limit - cursor;
-                            lab5: do {
-                                // literal, line 101
-                                if (!(eq_s_b(1, "e")))
-                                {
-                                    break lab5;
-                                }
-                                break lab2;
-                            } while (false);
-                            cursor = limit - v_6;
-                        }
-                        // delete, line 101
-                        slice_del();
-                        break;
-                    case 3:
-                        // (, line 104
-                        // delete, line 104
-                        slice_del();
-                        // try, line 105
-                        v_7 = limit - cursor;
-                        lab6: do {
-                            // (, line 105
-                            // [, line 106
-                            ket = cursor;
-                            // or, line 106
-                            lab7: do {
-                                v_8 = limit - cursor;
-                                lab8: do {
-                                    // literal, line 106
-                                    if (!(eq_s_b(2, "er")))
-                                    {
-                                        break lab8;
-                                    }
-                                    break lab7;
-                                } while (false);
-                                cursor = limit - v_8;
-                                // literal, line 106
-                                if (!(eq_s_b(2, "en")))
-                                {
-                                    cursor = limit - v_7;
-                                    break lab6;
-                                }
-                            } while (false);
-                            // ], line 106
-                            bra = cursor;
-                            // call R1, line 106
-                            if (!r_R1())
-                            {
-                                cursor = limit - v_7;
-                                break lab6;
-                            }
-                            // delete, line 106
-                            slice_del();
-                        } while (false);
-                        break;
-                    case 4:
-                        // (, line 110
-                        // delete, line 110
-                        slice_del();
-                        // try, line 111
-                        v_9 = limit - cursor;
-                        lab9: do {
-                            // (, line 111
-                            // [, line 112
-                            ket = cursor;
-                            // substring, line 112
-                            among_var = find_among_b(a_3, 2);
-                            if (among_var == 0)
-                            {
-                                cursor = limit - v_9;
-                                break lab9;
-                            }
-                            // ], line 112
-                            bra = cursor;
-                            // call R2, line 112
-                            if (!r_R2())
-                            {
-                                cursor = limit - v_9;
-                                break lab9;
-                            }
-                            switch(among_var) {
-                                case 0:
-                                    cursor = limit - v_9;
-                                    break lab9;
-                                case 1:
-                                    // (, line 114
-                                    // delete, line 114
-                                    slice_del();
-                                    break;
-                            }
-                        } while (false);
-                        break;
-                }
-            } while (false);
-            cursor = limit - v_3;
-            return true;
-        }
-
-        public boolean stem() {
-            int v_1;
-            int v_2;
-            int v_3;
-            int v_4;
-            // (, line 124
-            // do, line 125
-            v_1 = cursor;
-            lab0: do {
-                // call prelude, line 125
-                if (!r_prelude())
-                {
-                    break lab0;
-                }
-            } while (false);
-            cursor = v_1;
-            // do, line 126
-            v_2 = cursor;
-            lab1: do {
-                // call mark_regions, line 126
-                if (!r_mark_regions())
-                {
-                    break lab1;
-                }
-            } while (false);
-            cursor = v_2;
-            // backwards, line 127
-            limit_backward = cursor; cursor = limit;
-            // do, line 128
-            v_3 = limit - cursor;
-            lab2: do {
-                // call standard_suffix, line 128
-                if (!r_standard_suffix())
-                {
-                    break lab2;
-                }
-            } while (false);
-            cursor = limit - v_3;
-            cursor = limit_backward;            // do, line 129
-            v_4 = cursor;
-            lab3: do {
-                // call postlude, line 129
-                if (!r_postlude())
-                {
-                    break lab3;
-                }
-            } while (false);
-            cursor = v_4;
-            return true;
-        }
-
-}
-
Index: contrib/snowball/src/java/org/tartarus/snowball/ext/KpStemmer.java
===================================================================
--- contrib/snowball/src/java/org/tartarus/snowball/ext/KpStemmer.java	(revision 916567)
+++ contrib/snowball/src/java/org/tartarus/snowball/ext/KpStemmer.java	(working copy)
@@ -1,2181 +0,0 @@
-// This file was generated automatically by the Snowball to Java compiler
-
-package org.tartarus.snowball.ext;
-import org.tartarus.snowball.SnowballProgram;
-import org.tartarus.snowball.Among;
-
-/**
- * Generated class implementing code defined by a snowball script.
- */
-public class KpStemmer extends SnowballProgram {
-
-        private Among a_0[] = {
-            new Among ( "nde", -1, 7, "", this),
-            new Among ( "en", -1, 6, "", this),
-            new Among ( "s", -1, 2, "", this),
-            new Among ( "'s", 2, 1, "", this),
-            new Among ( "es", 2, 4, "", this),
-            new Among ( "ies", 4, 3, "", this),
-            new Among ( "aus", 2, 5, "", this)
-        };
-
-        private Among a_1[] = {
-            new Among ( "de", -1, 5, "", this),
-            new Among ( "ge", -1, 2, "", this),
-            new Among ( "ische", -1, 4, "", this),
-            new Among ( "je", -1, 1, "", this),
-            new Among ( "lijke", -1, 3, "", this),
-            new Among ( "le", -1, 9, "", this),
-            new Among ( "ene", -1, 10, "", this),
-            new Among ( "re", -1, 8, "", this),
-            new Among ( "se", -1, 7, "", this),
-            new Among ( "te", -1, 6, "", this),
-            new Among ( "ieve", -1, 11, "", this)
-        };
-
-        private Among a_2[] = {
-            new Among ( "heid", -1, 3, "", this),
-            new Among ( "fie", -1, 7, "", this),
-            new Among ( "gie", -1, 8, "", this),
-            new Among ( "atie", -1, 1, "", this),
-            new Among ( "isme", -1, 5, "", this),
-            new Among ( "ing", -1, 5, "", this),
-            new Among ( "arij", -1, 6, "", this),
-            new Among ( "erij", -1, 5, "", this),
-            new Among ( "sel", -1, 3, "", this),
-            new Among ( "rder", -1, 4, "", this),
-            new Among ( "ster", -1, 3, "", this),
-            new Among ( "iteit", -1, 2, "", this),
-            new Among ( "dst", -1, 10, "", this),
-            new Among ( "tst", -1, 9, "", this)
-        };
-
-        private Among a_3[] = {
-            new Among ( "end", -1, 10, "", this),
-            new Among ( "atief", -1, 2, "", this),
-            new Among ( "erig", -1, 10, "", this),
-            new Among ( "achtig", -1, 9, "", this),
-            new Among ( "ioneel", -1, 1, "", this),
-            new Among ( "baar", -1, 3, "", this),
-            new Among ( "laar", -1, 5, "", this),
-            new Among ( "naar", -1, 4, "", this),
-            new Among ( "raar", -1, 6, "", this),
-            new Among ( "eriger", -1, 10, "", this),
-            new Among ( "achtiger", -1, 9, "", this),
-            new Among ( "lijker", -1, 8, "", this),
-            new Among ( "tant", -1, 7, "", this),
-            new Among ( "erigst", -1, 10, "", this),
-            new Among ( "achtigst", -1, 9, "", this),
-            new Among ( "lijkst", -1, 8, "", this)
-        };
-
-        private Among a_4[] = {
-            new Among ( "ig", -1, 1, "", this),
-            new Among ( "iger", -1, 1, "", this),
-            new Among ( "igst", -1, 1, "", this)
-        };
-
-        private Among a_5[] = {
-            new Among ( "ft", -1, 2, "", this),
-            new Among ( "kt", -1, 1, "", this),
-            new Among ( "pt", -1, 3, "", this)
-        };
-
-        private Among a_6[] = {
-            new Among ( "bb", -1, 1, "", this),
-            new Among ( "cc", -1, 2, "", this),
-            new Among ( "dd", -1, 3, "", this),
-            new Among ( "ff", -1, 4, "", this),
-            new Among ( "gg", -1, 5, "", this),
-            new Among ( "hh", -1, 6, "", this),
-            new Among ( "jj", -1, 7, "", this),
-            new Among ( "kk", -1, 8, "", this),
-            new Among ( "ll", -1, 9, "", this),
-            new Among ( "mm", -1, 10, "", this),
-            new Among ( "nn", -1, 11, "", this),
-            new Among ( "pp", -1, 12, "", this),
-            new Among ( "qq", -1, 13, "", this),
-            new Among ( "rr", -1, 14, "", this),
-            new Among ( "ss", -1, 15, "", this),
-            new Among ( "tt", -1, 16, "", this),
-            new Among ( "v", -1, 21, "", this),
-            new Among ( "vv", 16, 17, "", this),
-            new Among ( "ww", -1, 18, "", this),
-            new Among ( "xx", -1, 19, "", this),
-            new Among ( "z", -1, 22, "", this),
-            new Among ( "zz", 20, 20, "", this)
-        };
-
-        private Among a_7[] = {
-            new Among ( "d", -1, 1, "", this),
-            new Among ( "t", -1, 2, "", this)
-        };
-
-        private static final char g_v[] = {17, 65, 16, 1 };
-
-        private static final char g_v_WX[] = {17, 65, 208, 1 };
-
-        private static final char g_AOU[] = {1, 64, 16 };
-
-        private static final char g_AIOU[] = {1, 65, 16 };
-
-        private boolean B_GE_removed;
-        private boolean B_stemmed;
-        private boolean B_Y_found;
-        private int I_p2;
-        private int I_p1;
-        private int I_x;
-        private StringBuilder S_ch = new StringBuilder();
-
-        private void copy_from(KpStemmer other) {
-            B_GE_removed = other.B_GE_removed;
-            B_stemmed = other.B_stemmed;
-            B_Y_found = other.B_Y_found;
-            I_p2 = other.I_p2;
-            I_p1 = other.I_p1;
-            I_x = other.I_x;
-            S_ch = other.S_ch;
-            super.copy_from(other);
-        }
-
-        private boolean r_R1() {
-            // (, line 32
-            // setmark x, line 32
-            I_x = cursor;
-            if (!(I_x >= I_p1))
-            {
-                return false;
-            }
-            return true;
-        }
-
-        private boolean r_R2() {
-            // (, line 33
-            // setmark x, line 33
-            I_x = cursor;
-            if (!(I_x >= I_p2))
-            {
-                return false;
-            }
-            return true;
-        }
-
-        private boolean r_V() {
-            int v_1;
-            int v_2;
-            // test, line 35
-            v_1 = limit - cursor;
-            // (, line 35
-            // or, line 35
-            lab0: do {
-                v_2 = limit - cursor;
-                lab1: do {
-                    if (!(in_grouping_b(g_v, 97, 121)))
-                    {
-                        break lab1;
-                    }
-                    break lab0;
-                } while (false);
-                cursor = limit - v_2;
-                // literal, line 35
-                if (!(eq_s_b(2, "ij")))
-                {
-                    return false;
-                }
-            } while (false);
-            cursor = limit - v_1;
-            return true;
-        }
-
-        private boolean r_VX() {
-            int v_1;
-            int v_2;
-            // test, line 36
-            v_1 = limit - cursor;
-            // (, line 36
-            // next, line 36
-            if (cursor <= limit_backward)
-            {
-                return false;
-            }
-            cursor--;
-            // or, line 36
-            lab0: do {
-                v_2 = limit - cursor;
-                lab1: do {
-                    if (!(in_grouping_b(g_v, 97, 121)))
-                    {
-                        break lab1;
-                    }
-                    break lab0;
-                } while (false);
-                cursor = limit - v_2;
-                // literal, line 36
-                if (!(eq_s_b(2, "ij")))
-                {
-                    return false;
-                }
-            } while (false);
-            cursor = limit - v_1;
-            return true;
-        }
-
-        private boolean r_C() {
-            int v_1;
-            int v_2;
-            // test, line 37
-            v_1 = limit - cursor;
-            // (, line 37
-            // not, line 37
-            {
-                v_2 = limit - cursor;
-                lab0: do {
-                    // literal, line 37
-                    if (!(eq_s_b(2, "ij")))
-                    {
-                        break lab0;
-                    }
-                    return false;
-                } while (false);
-                cursor = limit - v_2;
-            }
-            if (!(out_grouping_b(g_v, 97, 121)))
-            {
-                return false;
-            }
-            cursor = limit - v_1;
-            return true;
-        }
-
-        private boolean r_lengthen_V() {
-            int v_1;
-            int v_2;
-            int v_3;
-            int v_4;
-            int v_5;
-            int v_6;
-            int v_7;
-            int v_8;
-            // do, line 39
-            v_1 = limit - cursor;
-            lab0: do {
-                // (, line 39
-                if (!(out_grouping_b(g_v_WX, 97, 121)))
-                {
-                    break lab0;
-                }
-                // [, line 40
-                ket = cursor;
-                // or, line 40
-                lab1: do {
-                    v_2 = limit - cursor;
-                    lab2: do {
-                        // (, line 40
-                        if (!(in_grouping_b(g_AOU, 97, 117)))
-                        {
-                            break lab2;
-                        }
-                        // ], line 40
-                        bra = cursor;
-                        // test, line 40
-                        v_3 = limit - cursor;
-                        // (, line 40
-                        // or, line 40
-                        lab3: do {
-                            v_4 = limit - cursor;
-                            lab4: do {
-                                if (!(out_grouping_b(g_v, 97, 121)))
-                                {
-                                    break lab4;
-                                }
-                                break lab3;
-                            } while (false);
-                            cursor = limit - v_4;
-                            // atlimit, line 40
-                            if (cursor > limit_backward)
-                            {
-                                break lab2;
-                            }
-                        } while (false);
-                        cursor = limit - v_3;
-                        break lab1;
-                    } while (false);
-                    cursor = limit - v_2;
-                    // (, line 41
-                    // literal, line 41
-                    if (!(eq_s_b(1, "e")))
-                    {
-                        break lab0;
-                    }
-                    // ], line 41
-                    bra = cursor;
-                    // test, line 41
-                    v_5 = limit - cursor;
-                    // (, line 41
-                    // or, line 41
-                    lab5: do {
-                        v_6 = limit - cursor;
-                        lab6: do {
-                            if (!(out_grouping_b(g_v, 97, 121)))
-                            {
-                                break lab6;
-                            }
-                            break lab5;
-                        } while (false);
-                        cursor = limit - v_6;
-                        // atlimit, line 41
-                        if (cursor > limit_backward)
-                        {
-                            break lab0;
-                        }
-                    } while (false);
-                    // not, line 42
-                    {
-                        v_7 = limit - cursor;
-                        lab7: do {
-                            if (!(in_grouping_b(g_AIOU, 97, 117)))
-                            {
-                                break lab7;
-                            }
-                            break lab0;
-                        } while (false);
-                        cursor = limit - v_7;
-                    }
-                    // not, line 43
-                    {
-                        v_8 = limit - cursor;
-                        lab8: do {
-                            // (, line 43
-                            // next, line 43
-                            if (cursor <= limit_backward)
-                            {
-                                break lab8;
-                            }
-                            cursor--;
-                            if (!(in_grouping_b(g_AIOU, 97, 117)))
-                            {
-                                break lab8;
-                            }
-                            if (!(out_grouping_b(g_v, 97, 121)))
-                            {
-                                break lab8;
-                            }
-                            break lab0;
-                        } while (false);
-                        cursor = limit - v_8;
-                    }
-                    cursor = limit - v_5;
-                } while (false);
-                // -> ch, line 44
-                S_ch = slice_to(S_ch);
-                // <+ ch, line 44
-                {
-                    int c = cursor;
-                    insert(cursor, cursor, S_ch);
-                    cursor = c;
-                }
-            } while (false);
-            cursor = limit - v_1;
-            return true;
-        }
-
-        private boolean r_Step_1() {
-            int among_var;
-            int v_1;
-            int v_2;
-            int v_3;
-            int v_4;
-            // (, line 48
-            // [, line 49
-            ket = cursor;
-            // among, line 49
-            among_var = find_among_b(a_0, 7);
-            if (among_var == 0)
-            {
-                return false;
-            }
-            // (, line 49
-            // ], line 49
-            bra = cursor;
-            switch(among_var) {
-                case 0:
-                    return false;
-                case 1:
-                    // (, line 51
-                    // delete, line 51
-                    slice_del();
-                    break;
-                case 2:
-                    // (, line 52
-                    // call R1, line 52
-                    if (!r_R1())
-                    {
-                        return false;
-                    }
-                    // not, line 52
-                    {
-                        v_1 = limit - cursor;
-                        lab0: do {
-                            // (, line 52
-                            // literal, line 52
-                            if (!(eq_s_b(1, "t")))
-                            {
-                                break lab0;
-                            }
-                            // call R1, line 52
-                            if (!r_R1())
-                            {
-                                break lab0;
-                            }
-                            return false;
-                        } while (false);
-                        cursor = limit - v_1;
-                    }
-                    // call C, line 52
-                    if (!r_C())
-                    {
-                        return false;
-                    }
-                    // delete, line 52
-                    slice_del();
-                    break;
-                case 3:
-                    // (, line 53
-                    // call R1, line 53
-                    if (!r_R1())
-                    {
-                        return false;
-                    }
-                    // <-, line 53
-                    slice_from("ie");
-                    break;
-                case 4:
-                    // (, line 55
-                    // or, line 55
-                    lab1: do {
-                        v_2 = limit - cursor;
-                        lab2: do {
-                            // (, line 55
-                            // literal, line 55
-                            if (!(eq_s_b(2, "ar")))
-                            {
-                                break lab2;
-                            }
-                            // call R1, line 55
-                            if (!r_R1())
-                            {
-                                break lab2;
-                            }
-                            // call C, line 55
-                            if (!r_C())
-                            {
-                                break lab2;
-                            }
-                            // ], line 55
-                            bra = cursor;
-                            // delete, line 55
-                            slice_del();
-                            // call lengthen_V, line 55
-                            if (!r_lengthen_V())
-                            {
-                                break lab2;
-                            }
-                            break lab1;
-                        } while (false);
-                        cursor = limit - v_2;
-                        lab3: do {
-                            // (, line 56
-                            // literal, line 56
-                            if (!(eq_s_b(2, "er")))
-                            {
-                                break lab3;
-                            }
-                            // call R1, line 56
-                            if (!r_R1())
-                            {
-                                break lab3;
-                            }
-                            // call C, line 56
-                            if (!r_C())
-                            {
-                                break lab3;
-                            }
-                            // ], line 56
-                            bra = cursor;
-                            // delete, line 56
-                            slice_del();
-                            break lab1;
-                        } while (false);
-                        cursor = limit - v_2;
-                        // (, line 57
-                        // call R1, line 57
-                        if (!r_R1())
-                        {
-                            return false;
-                        }
-                        // call C, line 57
-                        if (!r_C())
-                        {
-                            return false;
-                        }
-                        // <-, line 57
-                        slice_from("e");
-                    } while (false);
-                    break;
-                case 5:
-                    // (, line 59
-                    // call R1, line 59
-                    if (!r_R1())
-                    {
-                        return false;
-                    }
-                    // call V, line 59
-                    if (!r_V())
-                    {
-                        return false;
-                    }
-                    // <-, line 59
-                    slice_from("au");
-                    break;
-                case 6:
-                    // (, line 60
-                    // or, line 60
-                    lab4: do {
-                        v_3 = limit - cursor;
-                        lab5: do {
-                            // (, line 60
-                            // literal, line 60
-                            if (!(eq_s_b(3, "hed")))
-                            {
-                                break lab5;
-                            }
-                            // call R1, line 60
-                            if (!r_R1())
-                            {
-                                break lab5;
-                            }
-                            // ], line 60
-                            bra = cursor;
-                            // <-, line 60
-                            slice_from("heid");
-                            break lab4;
-                        } while (false);
-                        cursor = limit - v_3;
-                        lab6: do {
-                            // (, line 61
-                            // literal, line 61
-                            if (!(eq_s_b(2, "nd")))
-                            {
-                                break lab6;
-                            }
-                            // delete, line 61
-                            slice_del();
-                            break lab4;
-                        } while (false);
-                        cursor = limit - v_3;
-                        lab7: do {
-                            // (, line 62
-                            // literal, line 62
-                            if (!(eq_s_b(1, "d")))
-                            {
-                                break lab7;
-                            }
-                            // call R1, line 62
-                            if (!r_R1())
-                            {
-                                break lab7;
-                            }
-                            // call C, line 62
-                            if (!r_C())
-                            {
-                                break lab7;
-                            }
-                            // ], line 62
-                            bra = cursor;
-                            // delete, line 62
-                            slice_del();
-                            break lab4;
-                        } while (false);
-                        cursor = limit - v_3;
-                        lab8: do {
-                            // (, line 63
-                            // or, line 63
-                            lab9: do {
-                                v_4 = limit - cursor;
-                                lab10: do {
-                                    // literal, line 63
-                                    if (!(eq_s_b(1, "i")))
-                                    {
-                                        break lab10;
-                                    }
-                                    break lab9;
-                                } while (false);
-                                cursor = limit - v_4;
-                                // literal, line 63
-                                if (!(eq_s_b(1, "j")))
-                                {
-                                    break lab8;
-                                }
-                            } while (false);
-                            // call V, line 63
-                            if (!r_V())
-                            {
-                                break lab8;
-                            }
-                            // delete, line 63
-                            slice_del();
-                            break lab4;
-                        } while (false);
-                        cursor = limit - v_3;
-                        // (, line 64
-                        // call R1, line 64
-                        if (!r_R1())
-                        {
-                            return false;
-                        }
-                        // call C, line 64
-                        if (!r_C())
-                        {
-                            return false;
-                        }
-                        // delete, line 64
-                        slice_del();
-                        // call lengthen_V, line 64
-                        if (!r_lengthen_V())
-                        {
-                            return false;
-                        }
-                    } while (false);
-                    break;
-                case 7:
-                    // (, line 65
-                    // <-, line 65
-                    slice_from("nd");
-                    break;
-            }
-            return true;
-        }
-
-        private boolean r_Step_2() {
-            int among_var;
-            int v_1;
-            // (, line 70
-            // [, line 71
-            ket = cursor;
-            // among, line 71
-            among_var = find_among_b(a_1, 11);
-            if (among_var == 0)
-            {
-                return false;
-            }
-            // (, line 71
-            // ], line 71
-            bra = cursor;
-            switch(among_var) {
-                case 0:
-                    return false;
-                case 1:
-                    // (, line 72
-                    // or, line 72
-                    lab0: do {
-                        v_1 = limit - cursor;
-                        lab1: do {
-                            // (, line 72
-                            // literal, line 72
-                            if (!(eq_s_b(2, "'t")))
-                            {
-                                break lab1;
-                            }
-                            // ], line 72
-                            bra = cursor;
-                            // delete, line 72
-                            slice_del();
-                            break lab0;
-                        } while (false);
-                        cursor = limit - v_1;
-                        lab2: do {
-                            // (, line 73
-                            // literal, line 73
-                            if (!(eq_s_b(2, "et")))
-                            {
-                                break lab2;
-                            }
-                            // ], line 73
-                            bra = cursor;
-                            // call R1, line 73
-                            if (!r_R1())
-                            {
-                                break lab2;
-                            }
-                            // call C, line 73
-                            if (!r_C())
-                            {
-                                break lab2;
-                            }
-                            // delete, line 73
-                            slice_del();
-                            break lab0;
-                        } while (false);
-                        cursor = limit - v_1;
-                        lab3: do {
-                            // (, line 74
-                            // literal, line 74
-                            if (!(eq_s_b(3, "rnt")))
-                            {
-                                break lab3;
-                            }
-                            // ], line 74
-                            bra = cursor;
-                            // <-, line 74
-                            slice_from("rn");
-                            break lab0;
-                        } while (false);
-                        cursor = limit - v_1;
-                        lab4: do {
-                            // (, line 75
-                            // literal, line 75
-                            if (!(eq_s_b(1, "t")))
-                            {
-                                break lab4;
-                            }
-                            // ], line 75
-                            bra = cursor;
-                            // call R1, line 75
-                            if (!r_R1())
-                            {
-                                break lab4;
-                            }
-                            // call VX, line 75
-                            if (!r_VX())
-                            {
-                                break lab4;
-                            }
-                            // delete, line 75
-                            slice_del();
-                            break lab0;
-                        } while (false);
-                        cursor = limit - v_1;
-                        lab5: do {
-                            // (, line 76
-                            // literal, line 76
-                            if (!(eq_s_b(3, "ink")))
-                            {
-                                break lab5;
-                            }
-                            // ], line 76
-                            bra = cursor;
-                            // <-, line 76
-                            slice_from("ing");
-                            break lab0;
-                        } while (false);
-                        cursor = limit - v_1;
-                        lab6: do {
-                            // (, line 77
-                            // literal, line 77
-                            if (!(eq_s_b(2, "mp")))
-                            {
-                                break lab6;
-                            }
-                            // ], line 77
-                            bra = cursor;
-                            // <-, line 77
-                            slice_from("m");
-                            break lab0;
-                        } while (false);
-                        cursor = limit - v_1;
-                        lab7: do {
-                            // (, line 78
-                            // literal, line 78
-                            if (!(eq_s_b(1, "'")))
-                            {
-                                break lab7;
-                            }
-                            // ], line 78
-                            bra = cursor;
-                            // call R1, line 78
-                            if (!r_R1())
-                            {
-                                break lab7;
-                            }
-                            // delete, line 78
-                            slice_del();
-                            break lab0;
-                        } while (false);
-                        cursor = limit - v_1;
-                        // (, line 79
-                        // ], line 79
-                        bra = cursor;
-                        // call R1, line 79
-                        if (!r_R1())
-                        {
-                            return false;
-                        }
-                        // call C, line 79
-                        if (!r_C())
-                        {
-                            return false;
-                        }
-                        // delete, line 79
-                        slice_del();
-                    } while (false);
-                    break;
-                case 2:
-                    // (, line 80
-                    // call R1, line 80
-                    if (!r_R1())
-                    {
-                        return false;
-                    }
-                    // <-, line 80
-                    slice_from("g");
-                    break;
-                case 3:
-                    // (, line 81
-                    // call R1, line 81
-                    if (!r_R1())
-                    {
-                        return false;
-                    }
-                    // <-, line 81
-                    slice_from("lijk");
-                    break;
-                case 4:
-                    // (, line 82
-                    // call R1, line 82
-                    if (!r_R1())
-                    {
-                        return false;
-                    }
-                    // <-, line 82
-                    slice_from("isch");
-                    break;
-                case 5:
-                    // (, line 83
-                    // call R1, line 83
-                    if (!r_R1())
-                    {
-                        return false;
-                    }
-                    // call C, line 83
-                    if (!r_C())
-                    {
-                        return false;
-                    }
-                    // delete, line 83
-                    slice_del();
-                    break;
-                case 6:
-                    // (, line 84
-                    // call R1, line 84
-                    if (!r_R1())
-                    {
-                        return false;
-                    }
-                    // <-, line 84
-                    slice_from("t");
-                    break;
-                case 7:
-                    // (, line 85
-                    // call R1, line 85
-                    if (!r_R1())
-                    {
-                        return false;
-                    }
-                    // <-, line 85
-                    slice_from("s");
-                    break;
-                case 8:
-                    // (, line 86
-                    // call R1, line 86
-                    if (!r_R1())
-                    {
-                        return false;
-                    }
-                    // <-, line 86
-                    slice_from("r");
-                    break;
-                case 9:
-                    // (, line 87
-                    // call R1, line 87
-                    if (!r_R1())
-                    {
-                        return false;
-                    }
-                    // delete, line 87
-                    slice_del();
-                    // attach, line 87
-                    insert(cursor, cursor, "l");
-                    // call lengthen_V, line 87
-                    if (!r_lengthen_V())
-                    {
-                        return false;
-                    }
-                    break;
-                case 10:
-                    // (, line 88
-                    // call R1, line 88
-                    if (!r_R1())
-                    {
-                        return false;
-                    }
-                    // call C, line 88
-                    if (!r_C())
-                    {
-                        return false;
-                    }
-                    // delete, line 88
-                    slice_del();
-                    // attach, line 88
-                    insert(cursor, cursor, "en");
-                    // call lengthen_V, line 88
-                    if (!r_lengthen_V())
-                    {
-                        return false;
-                    }
-                    break;
-                case 11:
-                    // (, line 89
-                    // call R1, line 89
-                    if (!r_R1())
-                    {
-                        return false;
-                    }
-                    // call C, line 89
-                    if (!r_C())
-                    {
-                        return false;
-                    }
-                    // <-, line 89
-                    slice_from("ief");
-                    break;
-            }
-            return true;
-        }
-
-        private boolean r_Step_3() {
-            int among_var;
-            // (, line 94
-            // [, line 95
-            ket = cursor;
-            // among, line 95
-            among_var = find_among_b(a_2, 14);
-            if (among_var == 0)
-            {
-                return false;
-            }
-            // (, line 95
-            // ], line 95
-            bra = cursor;
-            switch(among_var) {
-                case 0:
-                    return false;
-                case 1:
-                    // (, line 96
-                    // call R1, line 96
-                    if (!r_R1())
-                    {
-                        return false;
-                    }
-                    // <-, line 96
-                    slice_from("eer");
-                    break;
-                case 2:
-                    // (, line 97
-                    // call R1, line 97
-                    if (!r_R1())
-                    {
-                        return false;
-                    }
-                    // delete, line 97
-                    slice_del();
-                    // call lengthen_V, line 97
-                    if (!r_lengthen_V())
-                    {
-                        return false;
-                    }
-                    break;
-                case 3:
-                    // (, line 100
-                    // call R1, line 100
-                    if (!r_R1())
-                    {
-                        return false;
-                    }
-                    // delete, line 100
-                    slice_del();
-                    break;
-                case 4:
-                    // (, line 101
-                    // <-, line 101
-                    slice_from("r");
-                    break;
-                case 5:
-                    // (, line 104
-                    // call R1, line 104
-                    if (!r_R1())
-                    {
-                        return false;
-                    }
-                    // delete, line 104
-                    slice_del();
-                    // call lengthen_V, line 104
-                    if (!r_lengthen_V())
-                    {
-                        return false;
-                    }
-                    break;
-                case 6:
-                    // (, line 105
-                    // call R1, line 105
-                    if (!r_R1())
-                    {
-                        return false;
-                    }
-                    // call C, line 105
-                    if (!r_C())
-                    {
-                        return false;
-                    }
-                    // <-, line 105
-                    slice_from("aar");
-                    break;
-                case 7:
-                    // (, line 106
-                    // call R2, line 106
-                    if (!r_R2())
-                    {
-                        return false;
-                    }
-                    // delete, line 106
-                    slice_del();
-                    // attach, line 106
-                    insert(cursor, cursor, "f");
-                    // call lengthen_V, line 106
-                    if (!r_lengthen_V())
-                    {
-                        return false;
-                    }
-                    break;
-                case 8:
-                    // (, line 107
-                    // call R2, line 107
-                    if (!r_R2())
-                    {
-                        return false;
-                    }
-                    // delete, line 107
-                    slice_del();
-                    // attach, line 107
-                    insert(cursor, cursor, "g");
-                    // call lengthen_V, line 107
-                    if (!r_lengthen_V())
-                    {
-                        return false;
-                    }
-                    break;
-                case 9:
-                    // (, line 108
-                    // call R1, line 108
-                    if (!r_R1())
-                    {
-                        return false;
-                    }
-                    // call C, line 108
-                    if (!r_C())
-                    {
-                        return false;
-                    }
-                    // <-, line 108
-                    slice_from("t");
-                    break;
-                case 10:
-                    // (, line 109
-                    // call R1, line 109
-                    if (!r_R1())
-                    {
-                        return false;
-                    }
-                    // call C, line 109
-                    if (!r_C())
-                    {
-                        return false;
-                    }
-                    // <-, line 109
-                    slice_from("d");
-                    break;
-            }
-            return true;
-        }
-
-        private boolean r_Step_4() {
-            int among_var;
-            int v_1;
-            // (, line 114
-            // or, line 134
-            lab0: do {
-                v_1 = limit - cursor;
-                lab1: do {
-                    // (, line 115
-                    // [, line 115
-                    ket = cursor;
-                    // among, line 115
-                    among_var = find_among_b(a_3, 16);
-                    if (among_var == 0)
-                    {
-                        break lab1;
-                    }
-                    // (, line 115
-                    // ], line 115
-                    bra = cursor;
-                    switch(among_var) {
-                        case 0:
-                            break lab1;
-                        case 1:
-                            // (, line 116
-                            // call R1, line 116
-                            if (!r_R1())
-                            {
-                                break lab1;
-                            }
-                            // <-, line 116
-                            slice_from("ie");
-                            break;
-                        case 2:
-                            // (, line 117
-                            // call R1, line 117
-                            if (!r_R1())
-                            {
-                                break lab1;
-                            }
-                            // <-, line 117
-                            slice_from("eer");
-                            break;
-                        case 3:
-                            // (, line 118
-                            // call R1, line 118
-                            if (!r_R1())
-                            {
-                                break lab1;
-                            }
-                            // delete, line 118
-                            slice_del();
-                            break;
-                        case 4:
-                            // (, line 119
-                            // call R1, line 119
-                            if (!r_R1())
-                            {
-                                break lab1;
-                            }
-                            // call V, line 119
-                            if (!r_V())
-                            {
-                                break lab1;
-                            }
-                            // <-, line 119
-                            slice_from("n");
-                            break;
-                        case 5:
-                            // (, line 120
-                            // call R1, line 120
-                            if (!r_R1())
-                            {
-                                break lab1;
-                            }
-                            // call V, line 120
-                            if (!r_V())
-                            {
-                                break lab1;
-                            }
-                            // <-, line 120
-                            slice_from("l");
-                            break;
-                        case 6:
-                            // (, line 121
-                            // call R1, line 121
-                            if (!r_R1())
-                            {
-                                break lab1;
-                            }
-                            // call V, line 121
-                            if (!r_V())
-                            {
-                                break lab1;
-                            }
-                            // <-, line 121
-                            slice_from("r");
-                            break;
-                        case 7:
-                            // (, line 122
-                            // call R1, line 122
-                            if (!r_R1())
-                            {
-                                break lab1;
-                            }
-                            // <-, line 122
-                            slice_from("teer");
-                            break;
-                        case 8:
-                            // (, line 124
-                            // call R1, line 124
-                            if (!r_R1())
-                            {
-                                break lab1;
-                            }
-                            // <-, line 124
-                            slice_from("lijk");
-                            break;
-                        case 9:
-                            // (, line 127
-                            // call R1, line 127
-                            if (!r_R1())
-                            {
-                                break lab1;
-                            }
-                            // delete, line 127
-                            slice_del();
-                            break;
-                        case 10:
-                            // (, line 131
-                            // call R1, line 131
-                            if (!r_R1())
-                            {
-                                break lab1;
-                            }
-                            // call C, line 131
-                            if (!r_C())
-                            {
-                                break lab1;
-                            }
-                            // delete, line 131
-                            slice_del();
-                            // call lengthen_V, line 131
-                            if (!r_lengthen_V())
-                            {
-                                break lab1;
-                            }
-                            break;
-                    }
-                    break lab0;
-                } while (false);
-                cursor = limit - v_1;
-                // (, line 135
-                // [, line 135
-                ket = cursor;
-                // among, line 135
-                among_var = find_among_b(a_4, 3);
-                if (among_var == 0)
-                {
-                    return false;
-                }
-                // (, line 135
-                // ], line 135
-                bra = cursor;
-                switch(among_var) {
-                    case 0:
-                        return false;
-                    case 1:
-                        // (, line 138
-                        // call R1, line 138
-                        if (!r_R1())
-                        {
-                            return false;
-                        }
-                        // call C, line 138
-                        if (!r_C())
-                        {
-                            return false;
-                        }
-                        // delete, line 138
-                        slice_del();
-                        // call lengthen_V, line 138
-                        if (!r_lengthen_V())
-                        {
-                            return false;
-                        }
-                        break;
-                }
-            } while (false);
-            return true;
-        }
-
-        private boolean r_Step_7() {
-            int among_var;
-            // (, line 144
-            // [, line 145
-            ket = cursor;
-            // among, line 145
-            among_var = find_among_b(a_5, 3);
-            if (among_var == 0)
-            {
-                return false;
-            }
-            // (, line 145
-            // ], line 145
-            bra = cursor;
-            switch(among_var) {
-                case 0:
-                    return false;
-                case 1:
-                    // (, line 146
-                    // <-, line 146
-                    slice_from("k");
-                    break;
-                case 2:
-                    // (, line 147
-                    // <-, line 147
-                    slice_from("f");
-                    break;
-                case 3:
-                    // (, line 148
-                    // <-, line 148
-                    slice_from("p");
-                    break;
-            }
-            return true;
-        }
-
-        private boolean r_Step_6() {
-            int among_var;
-            // (, line 153
-            // [, line 154
-            ket = cursor;
-            // among, line 154
-            among_var = find_among_b(a_6, 22);
-            if (among_var == 0)
-            {
-                return false;
-            }
-            // (, line 154
-            // ], line 154
-            bra = cursor;
-            switch(among_var) {
-                case 0:
-                    return false;
-                case 1:
-                    // (, line 155
-                    // <-, line 155
-                    slice_from("b");
-                    break;
-                case 2:
-                    // (, line 156
-                    // <-, line 156
-                    slice_from("c");
-                    break;
-                case 3:
-                    // (, line 157
-                    // <-, line 157
-                    slice_from("d");
-                    break;
-                case 4:
-                    // (, line 158
-                    // <-, line 158
-                    slice_from("f");
-                    break;
-                case 5:
-                    // (, line 159
-                    // <-, line 159
-                    slice_from("g");
-                    break;
-                case 6:
-                    // (, line 160
-                    // <-, line 160
-                    slice_from("h");
-                    break;
-                case 7:
-                    // (, line 161
-                    // <-, line 161
-                    slice_from("j");
-                    break;
-                case 8:
-                    // (, line 162
-                    // <-, line 162
-                    slice_from("k");
-                    break;
-                case 9:
-                    // (, line 163
-                    // <-, line 163
-                    slice_from("l");
-                    break;
-                case 10:
-                    // (, line 164
-                    // <-, line 164
-                    slice_from("m");
-                    break;
-                case 11:
-                    // (, line 165
-                    // <-, line 165
-                    slice_from("n");
-                    break;
-                case 12:
-                    // (, line 166
-                    // <-, line 166
-                    slice_from("p");
-                    break;
-                case 13:
-                    // (, line 167
-                    // <-, line 167
-                    slice_from("q");
-                    break;
-                case 14:
-                    // (, line 168
-                    // <-, line 168
-                    slice_from("r");
-                    break;
-                case 15:
-                    // (, line 169
-                    // <-, line 169
-                    slice_from("s");
-                    break;
-                case 16:
-                    // (, line 170
-                    // <-, line 170
-                    slice_from("t");
-                    break;
-                case 17:
-                    // (, line 171
-                    // <-, line 171
-                    slice_from("v");
-                    break;
-                case 18:
-                    // (, line 172
-                    // <-, line 172
-                    slice_from("w");
-                    break;
-                case 19:
-                    // (, line 173
-                    // <-, line 173
-                    slice_from("x");
-                    break;
-                case 20:
-                    // (, line 174
-                    // <-, line 174
-                    slice_from("z");
-                    break;
-                case 21:
-                    // (, line 175
-                    // <-, line 175
-                    slice_from("f");
-                    break;
-                case 22:
-                    // (, line 176
-                    // <-, line 176
-                    slice_from("s");
-                    break;
-            }
-            return true;
-        }
-
-        private boolean r_Step_1c() {
-            int among_var;
-            int v_1;
-            int v_2;
-            // (, line 181
-            // [, line 182
-            ket = cursor;
-            // among, line 182
-            among_var = find_among_b(a_7, 2);
-            if (among_var == 0)
-            {
-                return false;
-            }
-            // (, line 182
-            // ], line 182
-            bra = cursor;
-            // call R1, line 182
-            if (!r_R1())
-            {
-                return false;
-            }
-            // call C, line 182
-            if (!r_C())
-            {
-                return false;
-            }
-            switch(among_var) {
-                case 0:
-                    return false;
-                case 1:
-                    // (, line 183
-                    // not, line 183
-                    {
-                        v_1 = limit - cursor;
-                        lab0: do {
-                            // (, line 183
-                            // literal, line 183
-                            if (!(eq_s_b(1, "n")))
-                            {
-                                break lab0;
-                            }
-                            // call R1, line 183
-                            if (!r_R1())
-                            {
-                                break lab0;
-                            }
-                            return false;
-                        } while (false);
-                        cursor = limit - v_1;
-                    }
-                    // delete, line 183
-                    slice_del();
-                    break;
-                case 2:
-                    // (, line 184
-                    // not, line 184
-                    {
-                        v_2 = limit - cursor;
-                        lab1: do {
-                            // (, line 184
-                            // literal, line 184
-                            if (!(eq_s_b(1, "h")))
-                            {
-                                break lab1;
-                            }
-                            // call R1, line 184
-                            if (!r_R1())
-                            {
-                                break lab1;
-                            }
-                            return false;
-                        } while (false);
-                        cursor = limit - v_2;
-                    }
-                    // delete, line 184
-                    slice_del();
-                    break;
-            }
-            return true;
-        }
-
-        private boolean r_Lose_prefix() {
-            int v_1;
-            int v_2;
-            int v_3;
-            // (, line 189
-            // [, line 190
-            bra = cursor;
-            // literal, line 190
-            if (!(eq_s(2, "ge")))
-            {
-                return false;
-            }
-            // ], line 190
-            ket = cursor;
-            // test, line 190
-            v_1 = cursor;
-            // hop, line 190
-            {
-                int c = cursor + 3;
-                if (0 > c || c > limit)
-                {
-                    return false;
-                }
-                cursor = c;
-            }
-            cursor = v_1;
-            // (, line 190
-            // goto, line 190
-            golab0: while(true)
-            {
-                v_2 = cursor;
-                lab1: do {
-                    if (!(in_grouping(g_v, 97, 121)))
-                    {
-                        break lab1;
-                    }
-                    cursor = v_2;
-                    break golab0;
-                } while (false);
-                cursor = v_2;
-                if (cursor >= limit)
-                {
-                    return false;
-                }
-                cursor++;
-            }
-            // goto, line 190
-            golab2: while(true)
-            {
-                v_3 = cursor;
-                lab3: do {
-                    if (!(out_grouping(g_v, 97, 121)))
-                    {
-                        break lab3;
-                    }
-                    cursor = v_3;
-                    break golab2;
-                } while (false);
-                cursor = v_3;
-                if (cursor >= limit)
-                {
-                    return false;
-                }
-                cursor++;
-            }
-            // set GE_removed, line 191
-            B_GE_removed = true;
-            // delete, line 192
-            slice_del();
-            return true;
-        }
-
-        private boolean r_Lose_infix() {
-            int v_2;
-            int v_3;
-            int v_4;
-            // (, line 195
-            // next, line 196
-            if (cursor >= limit)
-            {
-                return false;
-            }
-            cursor++;
-            // gopast, line 197
-            golab0: while(true)
-            {
-                lab1: do {
-                    // (, line 197
-                    // [, line 197
-                    bra = cursor;
-                    // literal, line 197
-                    if (!(eq_s(2, "ge")))
-                    {
-                        break lab1;
-                    }
-                    // ], line 197
-                    ket = cursor;
-                    break golab0;
-                } while (false);
-                if (cursor >= limit)
-                {
-                    return false;
-                }
-                cursor++;
-            }
-            // test, line 197
-            v_2 = cursor;
-            // hop, line 197
-            {
-                int c = cursor + 3;
-                if (0 > c || c > limit)
-                {
-                    return false;
-                }
-                cursor = c;
-            }
-            cursor = v_2;
-            // (, line 197
-            // goto, line 197
-            golab2: while(true)
-            {
-                v_3 = cursor;
-                lab3: do {
-                    if (!(in_grouping(g_v, 97, 121)))
-                    {
-                        break lab3;
-                    }
-                    cursor = v_3;
-                    break golab2;
-                } while (false);
-                cursor = v_3;
-                if (cursor >= limit)
-                {
-                    return false;
-                }
-                cursor++;
-            }
-            // goto, line 197
-            golab4: while(true)
-            {
-                v_4 = cursor;
-                lab5: do {
-                    if (!(out_grouping(g_v, 97, 121)))
-                    {
-                        break lab5;
-                    }
-                    cursor = v_4;
-                    break golab4;
-                } while (false);
-                cursor = v_4;
-                if (cursor >= limit)
-                {
-                    return false;
-                }
-                cursor++;
-            }
-            // set GE_removed, line 198
-            B_GE_removed = true;
-            // delete, line 199
-            slice_del();
-            return true;
-        }
-
-        private boolean r_measure() {
-            int v_1;
-            int v_2;
-            int v_5;
-            int v_6;
-            int v_9;
-            int v_10;
-            // (, line 202
-            // do, line 203
-            v_1 = cursor;
-            lab0: do {
-                // (, line 203
-                // tolimit, line 204
-                cursor = limit;
-                // setmark p1, line 205
-                I_p1 = cursor;
-                // setmark p2, line 206
-                I_p2 = cursor;
-            } while (false);
-            cursor = v_1;
-            // do, line 208
-            v_2 = cursor;
-            lab1: do {
-                // (, line 208
-                // repeat, line 209
-                replab2: while(true)
-                {
-                    lab3: do {
-                        if (!(out_grouping(g_v, 97, 121)))
-                        {
-                            break lab3;
-                        }
-                        continue replab2;
-                    } while (false);
-                    break replab2;
-                }
-                // atleast, line 209
-                {
-                    int v_4 = 1;
-                    // atleast, line 209
-                    replab4: while(true)
-                    {
-                        v_5 = cursor;
-                        lab5: do {
-                            // (, line 209
-                            // or, line 209
-                            lab6: do {
-                                v_6 = cursor;
-                                lab7: do {
-                                    // literal, line 209
-                                    if (!(eq_s(2, "ij")))
-                                    {
-                                        break lab7;
-                                    }
-                                    break lab6;
-                                } while (false);
-                                cursor = v_6;
-                                if (!(in_grouping(g_v, 97, 121)))
-                                {
-                                    break lab5;
-                                }
-                            } while (false);
-                            v_4--;
-                            continue replab4;
-                        } while (false);
-                        cursor = v_5;
-                        break replab4;
-                    }
-                    if (v_4 > 0)
-                    {
-                        break lab1;
-                    }
-                }
-                if (!(out_grouping(g_v, 97, 121)))
-                {
-                    break lab1;
-                }
-                // setmark p1, line 209
-                I_p1 = cursor;
-                // repeat, line 210
-                replab8: while(true)
-                {
-                    lab9: do {
-                        if (!(out_grouping(g_v, 97, 121)))
-                        {
-                            break lab9;
-                        }
-                        continue replab8;
-                    } while (false);
-                    break replab8;
-                }
-                // atleast, line 210
-                {
-                    int v_8 = 1;
-                    // atleast, line 210
-                    replab10: while(true)
-                    {
-                        v_9 = cursor;
-                        lab11: do {
-                            // (, line 210
-                            // or, line 210
-                            lab12: do {
-                                v_10 = cursor;
-                                lab13: do {
-                                    // literal, line 210
-                                    if (!(eq_s(2, "ij")))
-                                    {
-                                        break lab13;
-                                    }
-                                    break lab12;
-                                } while (false);
-                                cursor = v_10;
-                                if (!(in_grouping(g_v, 97, 121)))
-                                {
-                                    break lab11;
-                                }
-                            } while (false);
-                            v_8--;
-                            continue replab10;
-                        } while (false);
-                        cursor = v_9;
-                        break replab10;
-                    }
-                    if (v_8 > 0)
-                    {
-                        break lab1;
-                    }
-                }
-                if (!(out_grouping(g_v, 97, 121)))
-                {
-                    break lab1;
-                }
-                // setmark p2, line 210
-                I_p2 = cursor;
-            } while (false);
-            cursor = v_2;
-            return true;
-        }
-
-        public boolean stem() {
-            int v_1;
-            int v_2;
-            int v_3;
-            int v_4;
-            int v_5;
-            int v_6;
-            int v_7;
-            int v_8;
-            int v_9;
-            int v_10;
-            int v_11;
-            int v_12;
-            int v_13;
-            int v_14;
-            int v_15;
-            int v_16;
-            int v_18;
-            int v_19;
-            int v_20;
-            // (, line 214
-            // unset Y_found, line 216
-            B_Y_found = false;
-            // unset stemmed, line 217
-            B_stemmed = false;
-            // do, line 218
-            v_1 = cursor;
-            lab0: do {
-                // (, line 218
-                // [, line 218
-                bra = cursor;
-                // literal, line 218
-                if (!(eq_s(1, "y")))
-                {
-                    break lab0;
-                }
-                // ], line 218
-                ket = cursor;
-                // <-, line 218
-                slice_from("Y");
-                // set Y_found, line 218
-                B_Y_found = true;
-            } while (false);
-            cursor = v_1;
-            // do, line 219
-            v_2 = cursor;
-            lab1: do {
-                // repeat, line 219
-                replab2: while(true)
-                {
-                    v_3 = cursor;
-                    lab3: do {
-                        // (, line 219
-                        // goto, line 219
-                        golab4: while(true)
-                        {
-                            v_4 = cursor;
-                            lab5: do {
-                                // (, line 219
-                                if (!(in_grouping(g_v, 97, 121)))
-                                {
-                                    break lab5;
-                                }
-                                // [, line 219
-                                bra = cursor;
-                                // literal, line 219
-                                if (!(eq_s(1, "y")))
-                                {
-                                    break lab5;
-                                }
-                                // ], line 219
-                                ket = cursor;
-                                cursor = v_4;
-                                break golab4;
-                            } while (false);
-                            cursor = v_4;
-                            if (cursor >= limit)
-                            {
-                                break lab3;
-                            }
-                            cursor++;
-                        }
-                        // <-, line 219
-                        slice_from("Y");
-                        // set Y_found, line 219
-                        B_Y_found = true;
-                        continue replab2;
-                    } while (false);
-                    cursor = v_3;
-                    break replab2;
-                }
-            } while (false);
-            cursor = v_2;
-            // call measure, line 221
-            if (!r_measure())
-            {
-                return false;
-            }
-            // backwards, line 223
-            limit_backward = cursor; cursor = limit;
-            // (, line 223
-            // do, line 224
-            v_5 = limit - cursor;
-            lab6: do {
-                // (, line 224
-                // call Step_1, line 224
-                if (!r_Step_1())
-                {
-                    break lab6;
-                }
-                // set stemmed, line 224
-                B_stemmed = true;
-            } while (false);
-            cursor = limit - v_5;
-            // do, line 225
-            v_6 = limit - cursor;
-            lab7: do {
-                // (, line 225
-                // call Step_2, line 225
-                if (!r_Step_2())
-                {
-                    break lab7;
-                }
-                // set stemmed, line 225
-                B_stemmed = true;
-            } while (false);
-            cursor = limit - v_6;
-            // do, line 226
-            v_7 = limit - cursor;
-            lab8: do {
-                // (, line 226
-                // call Step_3, line 226
-                if (!r_Step_3())
-                {
-                    break lab8;
-                }
-                // set stemmed, line 226
-                B_stemmed = true;
-            } while (false);
-            cursor = limit - v_7;
-            // do, line 227
-            v_8 = limit - cursor;
-            lab9: do {
-                // (, line 227
-                // call Step_4, line 227
-                if (!r_Step_4())
-                {
-                    break lab9;
-                }
-                // set stemmed, line 227
-                B_stemmed = true;
-            } while (false);
-            cursor = limit - v_8;
-            cursor = limit_backward;            // unset GE_removed, line 229
-            B_GE_removed = false;
-            // do, line 230
-            v_9 = cursor;
-            lab10: do {
-                // (, line 230
-                // and, line 230
-                v_10 = cursor;
-                // call Lose_prefix, line 230
-                if (!r_Lose_prefix())
-                {
-                    break lab10;
-                }
-                cursor = v_10;
-                // call measure, line 230
-                if (!r_measure())
-                {
-                    break lab10;
-                }
-            } while (false);
-            cursor = v_9;
-            // backwards, line 231
-            limit_backward = cursor; cursor = limit;
-            // (, line 231
-            // do, line 232
-            v_11 = limit - cursor;
-            lab11: do {
-                // (, line 232
-                // Boolean test GE_removed, line 232
-                if (!(B_GE_removed))
-                {
-                    break lab11;
-                }
-                // call Step_1c, line 232
-                if (!r_Step_1c())
-                {
-                    break lab11;
-                }
-            } while (false);
-            cursor = limit - v_11;
-            cursor = limit_backward;            // unset GE_removed, line 234
-            B_GE_removed = false;
-            // do, line 235
-            v_12 = cursor;
-            lab12: do {
-                // (, line 235
-                // and, line 235
-                v_13 = cursor;
-                // call Lose_infix, line 235
-                if (!r_Lose_infix())
-                {
-                    break lab12;
-                }
-                cursor = v_13;
-                // call measure, line 235
-                if (!r_measure())
-                {
-                    break lab12;
-                }
-            } while (false);
-            cursor = v_12;
-            // backwards, line 236
-            limit_backward = cursor; cursor = limit;
-            // (, line 236
-            // do, line 237
-            v_14 = limit - cursor;
-            lab13: do {
-                // (, line 237
-                // Boolean test GE_removed, line 237
-                if (!(B_GE_removed))
-                {
-                    break lab13;
-                }
-                // call Step_1c, line 237
-                if (!r_Step_1c())
-                {
-                    break lab13;
-                }
-            } while (false);
-            cursor = limit - v_14;
-            cursor = limit_backward;            // backwards, line 239
-            limit_backward = cursor; cursor = limit;
-            // (, line 239
-            // do, line 240
-            v_15 = limit - cursor;
-            lab14: do {
-                // (, line 240
-                // call Step_7, line 240
-                if (!r_Step_7())
-                {
-                    break lab14;
-                }
-                // set stemmed, line 240
-                B_stemmed = true;
-            } while (false);
-            cursor = limit - v_15;
-            // do, line 241
-            v_16 = limit - cursor;
-            lab15: do {
-                // (, line 241
-                // or, line 241
-                lab16: do {
-                    lab17: do {
-                        // Boolean test stemmed, line 241
-                        if (!(B_stemmed))
-                        {
-                            break lab17;
-                        }
-                        break lab16;
-                    } while (false);
-                    // Boolean test GE_removed, line 241
-                    if (!(B_GE_removed))
-                    {
-                        break lab15;
-                    }
-                } while (false);
-                // call Step_6, line 241
-                if (!r_Step_6())
-                {
-                    break lab15;
-                }
-            } while (false);
-            cursor = limit - v_16;
-            cursor = limit_backward;            // do, line 243
-            v_18 = cursor;
-            lab18: do {
-                // (, line 243
-                // Boolean test Y_found, line 243
-                if (!(B_Y_found))
-                {
-                    break lab18;
-                }
-                // repeat, line 243
-                replab19: while(true)
-                {
-                    v_19 = cursor;
-                    lab20: do {
-                        // (, line 243
-                        // goto, line 243
-                        golab21: while(true)
-                        {
-                            v_20 = cursor;
-                            lab22: do {
-                                // (, line 243
-                                // [, line 243
-                                bra = cursor;
-                                // literal, line 243
-                                if (!(eq_s(1, "Y")))
-                                {
-                                    break lab22;
-                                }
-                                // ], line 243
-                                ket = cursor;
-                                cursor = v_20;
-                                break golab21;
-                            } while (false);
-                            cursor = v_20;
-                            if (cursor >= limit)
-                            {
-                                break lab20;
-                            }
-                            cursor++;
-                        }
-                        // <-, line 243
-                        slice_from("y");
-                        continue replab19;
-                    } while (false);
-                    cursor = v_19;
-                    break replab19;
-                }
-            } while (false);
-            cursor = v_18;
-            return true;
-        }
-
-}
-
Index: contrib/snowball/src/java/org/tartarus/snowball/ext/LovinsStemmer.java
===================================================================
--- contrib/snowball/src/java/org/tartarus/snowball/ext/LovinsStemmer.java	(revision 916567)
+++ contrib/snowball/src/java/org/tartarus/snowball/ext/LovinsStemmer.java	(working copy)
@@ -1,1908 +0,0 @@
-// This file was generated automatically by the Snowball to Java compiler
-
-package org.tartarus.snowball.ext;
-import org.tartarus.snowball.SnowballProgram;
-import org.tartarus.snowball.Among;
-
-/**
- * Generated class implementing code defined by a snowball script.
- */
-public class LovinsStemmer extends SnowballProgram {
-
-        private Among a_0[] = {
-            new Among ( "d", -1, -1, "", this),
-            new Among ( "f", -1, -1, "", this),
-            new Among ( "ph", -1, -1, "", this),
-            new Among ( "th", -1, -1, "", this),
-            new Among ( "l", -1, -1, "", this),
-            new Among ( "er", -1, -1, "", this),
-            new Among ( "or", -1, -1, "", this),
-            new Among ( "es", -1, -1, "", this),
-            new Among ( "t", -1, -1, "", this)
-        };
-
-        private Among a_1[] = {
-            new Among ( "s'", -1, 1, "r_A", this),
-            new Among ( "a", -1, 1, "r_A", this),
-            new Among ( "ia", 1, 1, "r_A", this),
-            new Among ( "ata", 1, 1, "r_A", this),
-            new Among ( "ic", -1, 1, "r_A", this),
-            new Among ( "aic", 4, 1, "r_A", this),
-            new Among ( "allic", 4, 1, "r_BB", this),
-            new Among ( "aric", 4, 1, "r_A", this),
-            new Among ( "atic", 4, 1, "r_B", this),
-            new Among ( "itic", 4, 1, "r_H", this),
-            new Among ( "antic", 4, 1, "r_C", this),
-            new Among ( "istic", 4, 1, "r_A", this),
-            new Among ( "alistic", 11, 1, "r_B", this),
-            new Among ( "aristic", 11, 1, "r_A", this),
-            new Among ( "ivistic", 11, 1, "r_A", this),
-            new Among ( "ed", -1, 1, "r_E", this),
-            new Among ( "anced", 15, 1, "r_B", this),
-            new Among ( "enced", 15, 1, "r_A", this),
-            new Among ( "ished", 15, 1, "r_A", this),
-            new Among ( "ied", 15, 1, "r_A", this),
-            new Among ( "ened", 15, 1, "r_E", this),
-            new Among ( "ioned", 15, 1, "r_A", this),
-            new Among ( "ated", 15, 1, "r_I", this),
-            new Among ( "ented", 15, 1, "r_C", this),
-            new Among ( "ized", 15, 1, "r_F", this),
-            new Among ( "arized", 24, 1, "r_A", this),
-            new Among ( "oid", -1, 1, "r_A", this),
-            new Among ( "aroid", 26, 1, "r_A", this),
-            new Among ( "hood", -1, 1, "r_A", this),
-            new Among ( "ehood", 28, 1, "r_A", this),
-            new Among ( "ihood", 28, 1, "r_A", this),
-            new Among ( "elihood", 30, 1, "r_E", this),
-            new Among ( "ward", -1, 1, "r_A", this),
-            new Among ( "e", -1, 1, "r_A", this),
-            new Among ( "ae", 33, 1, "r_A", this),
-            new Among ( "ance", 33, 1, "r_B", this),
-            new Among ( "icance", 35, 1, "r_A", this),
-            new Among ( "ence", 33, 1, "r_A", this),
-            new Among ( "ide", 33, 1, "r_L", this),
-            new Among ( "icide", 38, 1, "r_A", this),
-            new Among ( "otide", 38, 1, "r_A", this),
-            new Among ( "age", 33, 1, "r_B", this),
-            new Among ( "able", 33, 1, "r_A", this),
-            new Among ( "atable", 42, 1, "r_A", this),
-            new Among ( "izable", 42, 1, "r_E", this),
-            new Among ( "arizable", 44, 1, "r_A", this),
-            new Among ( "ible", 33, 1, "r_A", this),
-            new Among ( "encible", 46, 1, "r_A", this),
-            new Among ( "ene", 33, 1, "r_E", this),
-            new Among ( "ine", 33, 1, "r_M", this),
-            new Among ( "idine", 49, 1, "r_I", this),
-            new Among ( "one", 33, 1, "r_R", this),
-            new Among ( "ature", 33, 1, "r_E", this),
-            new Among ( "eature", 52, 1, "r_Z", this),
-            new Among ( "ese", 33, 1, "r_A", this),
-            new Among ( "wise", 33, 1, "r_A", this),
-            new Among ( "ate", 33, 1, "r_A", this),
-            new Among ( "entiate", 56, 1, "r_A", this),
-            new Among ( "inate", 56, 1, "r_A", this),
-            new Among ( "ionate", 56, 1, "r_D", this),
-            new Among ( "ite", 33, 1, "r_AA", this),
-            new Among ( "ive", 33, 1, "r_A", this),
-            new Among ( "ative", 61, 1, "r_A", this),
-            new Among ( "ize", 33, 1, "r_F", this),
-            new Among ( "alize", 63, 1, "r_A", this),
-            new Among ( "icalize", 64, 1, "r_A", this),
-            new Among ( "ialize", 64, 1, "r_A", this),
-            new Among ( "entialize", 66, 1, "r_A", this),
-            new Among ( "ionalize", 64, 1, "r_A", this),
-            new Among ( "arize", 63, 1, "r_A", this),
-            new Among ( "ing", -1, 1, "r_N", this),
-            new Among ( "ancing", 70, 1, "r_B", this),
-            new Among ( "encing", 70, 1, "r_A", this),
-            new Among ( "aging", 70, 1, "r_B", this),
-            new Among ( "ening", 70, 1, "r_E", this),
-            new Among ( "ioning", 70, 1, "r_A", this),
-            new Among ( "ating", 70, 1, "r_I", this),
-            new Among ( "enting", 70, 1, "r_C", this),
-            new Among ( "ying", 70, 1, "r_B", this),
-            new Among ( "izing", 70, 1, "r_F", this),
-            new Among ( "arizing", 79, 1, "r_A", this),
-            new Among ( "ish", -1, 1, "r_C", this),
-            new Among ( "yish", 81, 1, "r_A", this),
-            new Among ( "i", -1, 1, "r_A", this),
-            new Among ( "al", -1, 1, "r_BB", this),
-            new Among ( "ical", 84, 1, "r_A", this),
-            new Among ( "aical", 85, 1, "r_A", this),
-            new Among ( "istical", 85, 1, "r_A", this),
-            new Among ( "oidal", 84, 1, "r_A", this),
-            new Among ( "eal", 84, 1, "r_Y", this),
-            new Among ( "ial", 84, 1, "r_A", this),
-            new Among ( "ancial", 90, 1, "r_A", this),
-            new Among ( "arial", 90, 1, "r_A", this),
-            new Among ( "ential", 90, 1, "r_A", this),
-            new Among ( "ional", 84, 1, "r_A", this),
-            new Among ( "ational", 94, 1, "r_B", this),
-            new Among ( "izational", 95, 1, "r_A", this),
-            new Among ( "ental", 84, 1, "r_A", this),
-            new Among ( "ful", -1, 1, "r_A", this),
-            new Among ( "eful", 98, 1, "r_A", this),
-            new Among ( "iful", 98, 1, "r_A", this),
-            new Among ( "yl", -1, 1, "r_R", this),
-            new Among ( "ism", -1, 1, "r_B", this),
-            new Among ( "icism", 102, 1, "r_A", this),
-            new Among ( "oidism", 102, 1, "r_A", this),
-            new Among ( "alism", 102, 1, "r_B", this),
-            new Among ( "icalism", 105, 1, "r_A", this),
-            new Among ( "ionalism", 105, 1, "r_A", this),
-            new Among ( "inism", 102, 1, "r_J", this),
-            new Among ( "ativism", 102, 1, "r_A", this),
-            new Among ( "um", -1, 1, "r_U", this),
-            new Among ( "ium", 110, 1, "r_A", this),
-            new Among ( "ian", -1, 1, "r_A", this),
-            new Among ( "ician", 112, 1, "r_A", this),
-            new Among ( "en", -1, 1, "r_F", this),
-            new Among ( "ogen", 114, 1, "r_A", this),
-            new Among ( "on", -1, 1, "r_S", this),
-            new Among ( "ion", 116, 1, "r_Q", this),
-            new Among ( "ation", 117, 1, "r_B", this),
-            new Among ( "ication", 118, 1, "r_G", this),
-            new Among ( "entiation", 118, 1, "r_A", this),
-            new Among ( "ination", 118, 1, "r_A", this),
-            new Among ( "isation", 118, 1, "r_A", this),
-            new Among ( "arisation", 122, 1, "r_A", this),
-            new Among ( "entation", 118, 1, "r_A", this),
-            new Among ( "ization", 118, 1, "r_F", this),
-            new Among ( "arization", 125, 1, "r_A", this),
-            new Among ( "action", 117, 1, "r_G", this),
-            new Among ( "o", -1, 1, "r_A", this),
-            new Among ( "ar", -1, 1, "r_X", this),
-            new Among ( "ear", 129, 1, "r_Y", this),
-            new Among ( "ier", -1, 1, "r_A", this),
-            new Among ( "ariser", -1, 1, "r_A", this),
-            new Among ( "izer", -1, 1, "r_F", this),
-            new Among ( "arizer", 133, 1, "r_A", this),
-            new Among ( "or", -1, 1, "r_T", this),
-            new Among ( "ator", 135, 1, "r_A", this),
-            new Among ( "s", -1, 1, "r_W", this),
-            new Among ( "'s", 137, 1, "r_A", this),
-            new Among ( "as", 137, 1, "r_B", this),
-            new Among ( "ics", 137, 1, "r_A", this),
-            new Among ( "istics", 140, 1, "r_A", this),
-            new Among ( "es", 137, 1, "r_E", this),
-            new Among ( "ances", 142, 1, "r_B", this),
-            new Among ( "ences", 142, 1, "r_A", this),
-            new Among ( "ides", 142, 1, "r_L", this),
-            new Among ( "oides", 145, 1, "r_A", this),
-            new Among ( "ages", 142, 1, "r_B", this),
-            new Among ( "ies", 142, 1, "r_P", this),
-            new Among ( "acies", 148, 1, "r_A", this),
-            new Among ( "ancies", 148, 1, "r_A", this),
-            new Among ( "encies", 148, 1, "r_A", this),
-            new Among ( "aries", 148, 1, "r_A", this),
-            new Among ( "ities", 148, 1, "r_A", this),
-            new Among ( "alities", 153, 1, "r_A", this),
-            new Among ( "ivities", 153, 1, "r_A", this),
-            new Among ( "ines", 142, 1, "r_M", this),
-            new Among ( "nesses", 142, 1, "r_A", this),
-            new Among ( "ates", 142, 1, "r_A", this),
-            new Among ( "atives", 142, 1, "r_A", this),
-            new Among ( "ings", 137, 1, "r_N", this),
-            new Among ( "is", 137, 1, "r_A", this),
-            new Among ( "als", 137, 1, "r_BB", this),
-            new Among ( "ials", 162, 1, "r_A", this),
-            new Among ( "entials", 163, 1, "r_A", this),
-            new Among ( "ionals", 162, 1, "r_A", this),
-            new Among ( "isms", 137, 1, "r_B", this),
-            new Among ( "ians", 137, 1, "r_A", this),
-            new Among ( "icians", 167, 1, "r_A", this),
-            new Among ( "ions", 137, 1, "r_B", this),
-            new Among ( "ations", 169, 1, "r_B", this),
-            new Among ( "arisations", 170, 1, "r_A", this),
-            new Among ( "entations", 170, 1, "r_A", this),
-            new Among ( "izations", 170, 1, "r_A", this),
-            new Among ( "arizations", 173, 1, "r_A", this),
-            new Among ( "ars", 137, 1, "r_O", this),
-            new Among ( "iers", 137, 1, "r_A", this),
-            new Among ( "izers", 137, 1, "r_F", this),
-            new Among ( "ators", 137, 1, "r_A", this),
-            new Among ( "less", 137, 1, "r_A", this),
-            new Among ( "eless", 179, 1, "r_A", this),
-            new Among ( "ness", 137, 1, "r_A", this),
-            new Among ( "eness", 181, 1, "r_E", this),
-            new Among ( "ableness", 182, 1, "r_A", this),
-            new Among ( "eableness", 183, 1, "r_E", this),
-            new Among ( "ibleness", 182, 1, "r_A", this),
-            new Among ( "ateness", 182, 1, "r_A", this),
-            new Among ( "iteness", 182, 1, "r_A", this),
-            new Among ( "iveness", 182, 1, "r_A", this),
-            new Among ( "ativeness", 188, 1, "r_A", this),
-            new Among ( "ingness", 181, 1, "r_A", this),
-            new Among ( "ishness", 181, 1, "r_A", this),
-            new Among ( "iness", 181, 1, "r_A", this),
-            new Among ( "ariness", 192, 1, "r_E", this),
-            new Among ( "alness", 181, 1, "r_A", this),
-            new Among ( "icalness", 194, 1, "r_A", this),
-            new Among ( "antialness", 194, 1, "r_A", this),
-            new Among ( "entialness", 194, 1, "r_A", this),
-            new Among ( "ionalness", 194, 1, "r_A", this),
-            new Among ( "fulness", 181, 1, "r_A", this),
-            new Among ( "lessness", 181, 1, "r_A", this),
-            new Among ( "ousness", 181, 1, "r_A", this),
-            new Among ( "eousness", 201, 1, "r_A", this),
-            new Among ( "iousness", 201, 1, "r_A", this),
-            new Among ( "itousness", 201, 1, "r_A", this),
-            new Among ( "entness", 181, 1, "r_A", this),
-            new Among ( "ants", 137, 1, "r_B", this),
-            new Among ( "ists", 137, 1, "r_A", this),
-            new Among ( "icists", 207, 1, "r_A", this),
-            new Among ( "us", 137, 1, "r_V", this),
-            new Among ( "ous", 209, 1, "r_A", this),
-            new Among ( "eous", 210, 1, "r_A", this),
-            new Among ( "aceous", 211, 1, "r_A", this),
-            new Among ( "antaneous", 211, 1, "r_A", this),
-            new Among ( "ious", 210, 1, "r_A", this),
-            new Among ( "acious", 214, 1, "r_B", this),
-            new Among ( "itous", 210, 1, "r_A", this),
-            new Among ( "ant", -1, 1, "r_B", this),
-            new Among ( "icant", 217, 1, "r_A", this),
-            new Among ( "ent", -1, 1, "r_C", this),
-            new Among ( "ement", 219, 1, "r_A", this),
-            new Among ( "izement", 220, 1, "r_A", this),
-            new Among ( "ist", -1, 1, "r_A", this),
-            new Among ( "icist", 222, 1, "r_A", this),
-            new Among ( "alist", 222, 1, "r_A", this),
-            new Among ( "icalist", 224, 1, "r_A", this),
-            new Among ( "ialist", 224, 1, "r_A", this),
-            new Among ( "ionist", 222, 1, "r_A", this),
-            new Among ( "entist", 222, 1, "r_A", this),
-            new Among ( "y", -1, 1, "r_B", this),
-            new Among ( "acy", 229, 1, "r_A", this),
-            new Among ( "ancy", 229, 1, "r_B", this),
-            new Among ( "ency", 229, 1, "r_A", this),
-            new Among ( "ly", 229, 1, "r_B", this),
-            new Among ( "ealy", 233, 1, "r_Y", this),
-            new Among ( "ably", 233, 1, "r_A", this),
-            new Among ( "ibly", 233, 1, "r_A", this),
-            new Among ( "edly", 233, 1, "r_E", this),
-            new Among ( "iedly", 237, 1, "r_A", this),
-            new Among ( "ely", 233, 1, "r_E", this),
-            new Among ( "ately", 239, 1, "r_A", this),
-            new Among ( "ively", 239, 1, "r_A", this),
-            new Among ( "atively", 241, 1, "r_A", this),
-            new Among ( "ingly", 233, 1, "r_B", this),
-            new Among ( "atingly", 243, 1, "r_A", this),
-            new Among ( "ily", 233, 1, "r_A", this),
-            new Among ( "lily", 245, 1, "r_A", this),
-            new Among ( "arily", 245, 1, "r_A", this),
-            new Among ( "ally", 233, 1, "r_B", this),
-            new Among ( "ically", 248, 1, "r_A", this),
-            new Among ( "aically", 249, 1, "r_A", this),
-            new Among ( "allically", 249, 1, "r_C", this),
-            new Among ( "istically", 249, 1, "r_A", this),
-            new Among ( "alistically", 252, 1, "r_B", this),
-            new Among ( "oidally", 248, 1, "r_A", this),
-            new Among ( "ially", 248, 1, "r_A", this),
-            new Among ( "entially", 255, 1, "r_A", this),
-            new Among ( "ionally", 248, 1, "r_A", this),
-            new Among ( "ationally", 257, 1, "r_B", this),
-            new Among ( "izationally", 258, 1, "r_B", this),
-            new Among ( "entally", 248, 1, "r_A", this),
-            new Among ( "fully", 233, 1, "r_A", this),
-            new Among ( "efully", 261, 1, "r_A", this),
-            new Among ( "ifully", 261, 1, "r_A", this),
-            new Among ( "enly", 233, 1, "r_E", this),
-            new Among ( "arly", 233, 1, "r_K", this),
-            new Among ( "early", 265, 1, "r_Y", this),
-            new Among ( "lessly", 233, 1, "r_A", this),
-            new Among ( "ously", 233, 1, "r_A", this),
-            new Among ( "eously", 268, 1, "r_A", this),
-            new Among ( "iously", 268, 1, "r_A", this),
-            new Among ( "ently", 233, 1, "r_A", this),
-            new Among ( "ary", 229, 1, "r_F", this),
-            new Among ( "ery", 229, 1, "r_E", this),
-            new Among ( "icianry", 229, 1, "r_A", this),
-            new Among ( "atory", 229, 1, "r_A", this),
-            new Among ( "ity", 229, 1, "r_A", this),
-            new Among ( "acity", 276, 1, "r_A", this),
-            new Among ( "icity", 276, 1, "r_A", this),
-            new Among ( "eity", 276, 1, "r_A", this),
-            new Among ( "ality", 276, 1, "r_A", this),
-            new Among ( "icality", 280, 1, "r_A", this),
-            new Among ( "iality", 280, 1, "r_A", this),
-            new Among ( "antiality", 282, 1, "r_A", this),
-            new Among ( "entiality", 282, 1, "r_A", this),
-            new Among ( "ionality", 280, 1, "r_A", this),
-            new Among ( "elity", 276, 1, "r_A", this),
-            new Among ( "ability", 276, 1, "r_A", this),
-            new Among ( "izability", 287, 1, "r_A", this),
-            new Among ( "arizability", 288, 1, "r_A", this),
-            new Among ( "ibility", 276, 1, "r_A", this),
-            new Among ( "inity", 276, 1, "r_CC", this),
-            new Among ( "arity", 276, 1, "r_B", this),
-            new Among ( "ivity", 276, 1, "r_A", this)
-        };
-
-        private Among a_2[] = {
-            new Among ( "bb", -1, -1, "", this),
-            new Among ( "dd", -1, -1, "", this),
-            new Among ( "gg", -1, -1, "", this),
-            new Among ( "ll", -1, -1, "", this),
-            new Among ( "mm", -1, -1, "", this),
-            new Among ( "nn", -1, -1, "", this),
-            new Among ( "pp", -1, -1, "", this),
-            new Among ( "rr", -1, -1, "", this),
-            new Among ( "ss", -1, -1, "", this),
-            new Among ( "tt", -1, -1, "", this)
-        };
-
-        private Among a_3[] = {
-            new Among ( "uad", -1, 18, "", this),
-            new Among ( "vad", -1, 19, "", this),
-            new Among ( "cid", -1, 20, "", this),
-            new Among ( "lid", -1, 21, "", this),
-            new Among ( "erid", -1, 22, "", this),
-            new Among ( "pand", -1, 23, "", this),
-            new Among ( "end", -1, 24, "", this),
-            new Among ( "ond", -1, 25, "", this),
-            new Among ( "lud", -1, 26, "", this),
-            new Among ( "rud", -1, 27, "", this),
-            new Among ( "ul", -1, 9, "", this),
-            new Among ( "her", -1, 28, "", this),
-            new Among ( "metr", -1, 7, "", this),
-            new Among ( "istr", -1, 6, "", this),
-            new Among ( "urs", -1, 5, "", this),
-            new Among ( "uct", -1, 2, "", this),
-            new Among ( "et", -1, 32, "", this),
-            new Among ( "mit", -1, 29, "", this),
-            new Among ( "ent", -1, 30, "", this),
-            new Among ( "umpt", -1, 3, "", this),
-            new Among ( "rpt", -1, 4, "", this),
-            new Among ( "ert", -1, 31, "", this),
-            new Among ( "yt", -1, 33, "", this),
-            new Among ( "iev", -1, 1, "", this),
-            new Among ( "olv", -1, 8, "", this),
-            new Among ( "ax", -1, 14, "", this),
-            new Among ( "ex", -1, 15, "", this),
-            new Among ( "bex", 26, 10, "", this),
-            new Among ( "dex", 26, 11, "", this),
-            new Among ( "pex", 26, 12, "", this),
-            new Among ( "tex", 26, 13, "", this),
-            new Among ( "ix", -1, 16, "", this),
-            new Among ( "lux", -1, 17, "", this),
-            new Among ( "yz", -1, 34, "", this)
-        };
-
-
-        private void copy_from(LovinsStemmer other) {
-            super.copy_from(other);
-        }
-
-        private boolean r_A() {
-            // (, line 21
-            // hop, line 21
-            {
-                int c = cursor - 2;
-                if (limit_backward > c || c > limit)
-                {
-                    return false;
-                }
-                cursor = c;
-            }
-            return true;
-        }
-
-        private boolean r_B() {
-            // (, line 22
-            // hop, line 22
-            {
-                int c = cursor - 3;
-                if (limit_backward > c || c > limit)
-                {
-                    return false;
-                }
-                cursor = c;
-            }
-            return true;
-        }
-
-        private boolean r_C() {
-            // (, line 23
-            // hop, line 23
-            {
-                int c = cursor - 4;
-                if (limit_backward > c || c > limit)
-                {
-                    return false;
-                }
-                cursor = c;
-            }
-            return true;
-        }
-
-        private boolean r_D() {
-            // (, line 24
-            // hop, line 24
-            {
-                int c = cursor - 5;
-                if (limit_backward > c || c > limit)
-                {
-                    return false;
-                }
-                cursor = c;
-            }
-            return true;
-        }
-
-        private boolean r_E() {
-            int v_1;
-            int v_2;
-            // (, line 25
-            // test, line 25
-            v_1 = limit - cursor;
-            // hop, line 25
-            {
-                int c = cursor - 2;
-                if (limit_backward > c || c > limit)
-                {
-                    return false;
-                }
-                cursor = c;
-            }
-            cursor = limit - v_1;
-            // not, line 25
-            {
-                v_2 = limit - cursor;
-                lab0: do {
-                    // literal, line 25
-                    if (!(eq_s_b(1, "e")))
-                    {
-                        break lab0;
-                    }
-                    return false;
-                } while (false);
-                cursor = limit - v_2;
-            }
-            return true;
-        }
-
-        private boolean r_F() {
-            int v_1;
-            int v_2;
-            // (, line 26
-            // test, line 26
-            v_1 = limit - cursor;
-            // hop, line 26
-            {
-                int c = cursor - 3;
-                if (limit_backward > c || c > limit)
-                {
-                    return false;
-                }
-                cursor = c;
-            }
-            cursor = limit - v_1;
-            // not, line 26
-            {
-                v_2 = limit - cursor;
-                lab0: do {
-                    // literal, line 26
-                    if (!(eq_s_b(1, "e")))
-                    {
-                        break lab0;
-                    }
-                    return false;
-                } while (false);
-                cursor = limit - v_2;
-            }
-            return true;
-        }
-
-        private boolean r_G() {
-            int v_1;
-            // (, line 27
-            // test, line 27
-            v_1 = limit - cursor;
-            // hop, line 27
-            {
-                int c = cursor - 3;
-                if (limit_backward > c || c > limit)
-                {
-                    return false;
-                }
-                cursor = c;
-            }
-            cursor = limit - v_1;
-            // literal, line 27
-            if (!(eq_s_b(1, "f")))
-            {
-                return false;
-            }
-            return true;
-        }
-
-        private boolean r_H() {
-            int v_1;
-            int v_2;
-            // (, line 28
-            // test, line 28
-            v_1 = limit - cursor;
-            // hop, line 28
-            {
-                int c = cursor - 2;
-                if (limit_backward > c || c > limit)
-                {
-                    return false;
-                }
-                cursor = c;
-            }
-            cursor = limit - v_1;
-            // or, line 28
-            lab0: do {
-                v_2 = limit - cursor;
-                lab1: do {
-                    // literal, line 28
-                    if (!(eq_s_b(1, "t")))
-                    {
-                        break lab1;
-                    }
-                    break lab0;
-                } while (false);
-                cursor = limit - v_2;
-                // literal, line 28
-                if (!(eq_s_b(2, "ll")))
-                {
-                    return false;
-                }
-            } while (false);
-            return true;
-        }
-
-        private boolean r_I() {
-            int v_1;
-            int v_2;
-            int v_3;
-            // (, line 29
-            // test, line 29
-            v_1 = limit - cursor;
-            // hop, line 29
-            {
-                int c = cursor - 2;
-                if (limit_backward > c || c > limit)
-                {
-                    return false;
-                }
-                cursor = c;
-            }
-            cursor = limit - v_1;
-            // not, line 29
-            {
-                v_2 = limit - cursor;
-                lab0: do {
-                    // literal, line 29
-                    if (!(eq_s_b(1, "o")))
-                    {
-                        break lab0;
-                    }
-                    return false;
-                } while (false);
-                cursor = limit - v_2;
-            }
-            // not, line 29
-            {
-                v_3 = limit - cursor;
-                lab1: do {
-                    // literal, line 29
-                    if (!(eq_s_b(1, "e")))
-                    {
-                        break lab1;
-                    }
-                    return false;
-                } while (false);
-                cursor = limit - v_3;
-            }
-            return true;
-        }
-
-        private boolean r_J() {
-            int v_1;
-            int v_2;
-            int v_3;
-            // (, line 30
-            // test, line 30
-            v_1 = limit - cursor;
-            // hop, line 30
-            {
-                int c = cursor - 2;
-                if (limit_backward > c || c > limit)
-                {
-                    return false;
-                }
-                cursor = c;
-            }
-            cursor = limit - v_1;
-            // not, line 30
-            {
-                v_2 = limit - cursor;
-                lab0: do {
-                    // literal, line 30
-                    if (!(eq_s_b(1, "a")))
-                    {
-                        break lab0;
-                    }
-                    return false;
-                } while (false);
-                cursor = limit - v_2;
-            }
-            // not, line 30
-            {
-                v_3 = limit - cursor;
-                lab1: do {
-                    // literal, line 30
-                    if (!(eq_s_b(1, "e")))
-                    {
-                        break lab1;
-                    }
-                    return false;
-                } while (false);
-                cursor = limit - v_3;
-            }
-            return true;
-        }
-
-        private boolean r_K() {
-            int v_1;
-            int v_2;
-            // (, line 31
-            // test, line 31
-            v_1 = limit - cursor;
-            // hop, line 31
-            {
-                int c = cursor - 3;
-                if (limit_backward > c || c > limit)
-                {
-                    return false;
-                }
-                cursor = c;
-            }
-            cursor = limit - v_1;
-            // or, line 31
-            lab0: do {
-                v_2 = limit - cursor;
-                lab1: do {
-                    // literal, line 31
-                    if (!(eq_s_b(1, "l")))
-                    {
-                        break lab1;
-                    }
-                    break lab0;
-                } while (false);
-                cursor = limit - v_2;
-                lab2: do {
-                    // literal, line 31
-                    if (!(eq_s_b(1, "i")))
-                    {
-                        break lab2;
-                    }
-                    break lab0;
-                } while (false);
-                cursor = limit - v_2;
-                // (, line 31
-                // literal, line 31
-                if (!(eq_s_b(1, "e")))
-                {
-                    return false;
-                }
-                // next, line 31
-                if (cursor <= limit_backward)
-                {
-                    return false;
-                }
-                cursor--;
-                // literal, line 31
-                if (!(eq_s_b(1, "u")))
-                {
-                    return false;
-                }
-            } while (false);
-            return true;
-        }
-
-        private boolean r_L() {
-            int v_1;
-            int v_2;
-            int v_3;
-            int v_4;
-            int v_5;
-            // (, line 32
-            // test, line 32
-            v_1 = limit - cursor;
-            // hop, line 32
-            {
-                int c = cursor - 2;
-                if (limit_backward > c || c > limit)
-                {
-                    return false;
-                }
-                cursor = c;
-            }
-            cursor = limit - v_1;
-            // not, line 32
-            {
-                v_2 = limit - cursor;
-                lab0: do {
-                    // literal, line 32
-                    if (!(eq_s_b(1, "u")))
-                    {
-                        break lab0;
-                    }
-                    return false;
-                } while (false);
-                cursor = limit - v_2;
-            }
-            // not, line 32
-            {
-                v_3 = limit - cursor;
-                lab1: do {
-                    // literal, line 32
-                    if (!(eq_s_b(1, "x")))
-                    {
-                        break lab1;
-                    }
-                    return false;
-                } while (false);
-                cursor = limit - v_3;
-            }
-            // not, line 32
-            {
-                v_4 = limit - cursor;
-                lab2: do {
-                    // (, line 32
-                    // literal, line 32
-                    if (!(eq_s_b(1, "s")))
-                    {
-                        break lab2;
-                    }
-                    // not, line 32
-                    {
-                        v_5 = limit - cursor;
-                        lab3: do {
-                            // literal, line 32
-                            if (!(eq_s_b(1, "o")))
-                            {
-                                break lab3;
-                            }
-                            break lab2;
-                        } while (false);
-                        cursor = limit - v_5;
-                    }
-                    return false;
-                } while (false);
-                cursor = limit - v_4;
-            }
-            return true;
-        }
-
-        private boolean r_M() {
-            int v_1;
-            int v_2;
-            int v_3;
-            int v_4;
-            int v_5;
-            // (, line 33
-            // test, line 33
-            v_1 = limit - cursor;
-            // hop, line 33
-            {
-                int c = cursor - 2;
-                if (limit_backward > c || c > limit)
-                {
-                    return false;
-                }
-                cursor = c;
-            }
-            cursor = limit - v_1;
-            // not, line 33
-            {
-                v_2 = limit - cursor;
-                lab0: do {
-                    // literal, line 33
-                    if (!(eq_s_b(1, "a")))
-                    {
-                        break lab0;
-                    }
-                    return false;
-                } while (false);
-                cursor = limit - v_2;
-            }
-            // not, line 33
-            {
-                v_3 = limit - cursor;
-                lab1: do {
-                    // literal, line 33
-                    if (!(eq_s_b(1, "c")))
-                    {
-                        break lab1;
-                    }
-                    return false;
-                } while (false);
-                cursor = limit - v_3;
-            }
-            // not, line 33
-            {
-                v_4 = limit - cursor;
-                lab2: do {
-                    // literal, line 33
-                    if (!(eq_s_b(1, "e")))
-                    {
-                        break lab2;
-                    }
-                    return false;
-                } while (false);
-                cursor = limit - v_4;
-            }
-            // not, line 33
-            {
-                v_5 = limit - cursor;
-                lab3: do {
-                    // literal, line 33
-                    if (!(eq_s_b(1, "m")))
-                    {
-                        break lab3;
-                    }
-                    return false;
-                } while (false);
-                cursor = limit - v_5;
-            }
-            return true;
-        }
-
-        private boolean r_N() {
-            int v_1;
-            int v_2;
-            int v_3;
-            // (, line 34
-            // test, line 34
-            v_1 = limit - cursor;
-            // hop, line 34
-            {
-                int c = cursor - 3;
-                if (limit_backward > c || c > limit)
-                {
-                    return false;
-                }
-                cursor = c;
-            }
-            cursor = limit - v_1;
-            // (, line 34
-            // hop, line 34
-            {
-                int c = cursor - 2;
-                if (limit_backward > c || c > limit)
-                {
-                    return false;
-                }
-                cursor = c;
-            }
-            // or, line 34
-            lab0: do {
-                v_2 = limit - cursor;
-                lab1: do {
-                    // not, line 34
-                    {
-                        v_3 = limit - cursor;
-                        lab2: do {
-                            // literal, line 34
-                            if (!(eq_s_b(1, "s")))
-                            {
-                                break lab2;
-                            }
-                            break lab1;
-                        } while (false);
-                        cursor = limit - v_3;
-                    }
-                    break lab0;
-                } while (false);
-                cursor = limit - v_2;
-                // hop, line 34
-                {
-                    int c = cursor - 2;
-                    if (limit_backward > c || c > limit)
-                    {
-                        return false;
-                    }
-                    cursor = c;
-                }
-            } while (false);
-            return true;
-        }
-
-        private boolean r_O() {
-            int v_1;
-            int v_2;
-            // (, line 35
-            // test, line 35
-            v_1 = limit - cursor;
-            // hop, line 35
-            {
-                int c = cursor - 2;
-                if (limit_backward > c || c > limit)
-                {
-                    return false;
-                }
-                cursor = c;
-            }
-            cursor = limit - v_1;
-            // or, line 35
-            lab0: do {
-                v_2 = limit - cursor;
-                lab1: do {
-                    // literal, line 35
-                    if (!(eq_s_b(1, "l")))
-                    {
-                        break lab1;
-                    }
-                    break lab0;
-                } while (false);
-                cursor = limit - v_2;
-                // literal, line 35
-                if (!(eq_s_b(1, "i")))
-                {
-                    return false;
-                }
-            } while (false);
-            return true;
-        }
-
-        private boolean r_P() {
-            int v_1;
-            int v_2;
-            // (, line 36
-            // test, line 36
-            v_1 = limit - cursor;
-            // hop, line 36
-            {
-                int c = cursor - 2;
-                if (limit_backward > c || c > limit)
-                {
-                    return false;
-                }
-                cursor = c;
-            }
-            cursor = limit - v_1;
-            // not, line 36
-            {
-                v_2 = limit - cursor;
-                lab0: do {
-                    // literal, line 36
-                    if (!(eq_s_b(1, "c")))
-                    {
-                        break lab0;
-                    }
-                    return false;
-                } while (false);
-                cursor = limit - v_2;
-            }
-            return true;
-        }
-
-        private boolean r_Q() {
-            int v_1;
-            int v_2;
-            int v_3;
-            int v_4;
-            // (, line 37
-            // test, line 37
-            v_1 = limit - cursor;
-            // hop, line 37
-            {
-                int c = cursor - 2;
-                if (limit_backward > c || c > limit)
-                {
-                    return false;
-                }
-                cursor = c;
-            }
-            cursor = limit - v_1;
-            // test, line 37
-            v_2 = limit - cursor;
-            // hop, line 37
-            {
-                int c = cursor - 3;
-                if (limit_backward > c || c > limit)
-                {
-                    return false;
-                }
-                cursor = c;
-            }
-            cursor = limit - v_2;
-            // not, line 37
-            {
-                v_3 = limit - cursor;
-                lab0: do {
-                    // literal, line 37
-                    if (!(eq_s_b(1, "l")))
-                    {
-                        break lab0;
-                    }
-                    return false;
-                } while (false);
-                cursor = limit - v_3;
-            }
-            // not, line 37
-            {
-                v_4 = limit - cursor;
-                lab1: do {
-                    // literal, line 37
-                    if (!(eq_s_b(1, "n")))
-                    {
-                        break lab1;
-                    }
-                    return false;
-                } while (false);
-                cursor = limit - v_4;
-            }
-            return true;
-        }
-
-        private boolean r_R() {
-            int v_1;
-            int v_2;
-            // (, line 38
-            // test, line 38
-            v_1 = limit - cursor;
-            // hop, line 38
-            {
-                int c = cursor - 2;
-                if (limit_backward > c || c > limit)
-                {
-                    return false;
-                }
-                cursor = c;
-            }
-            cursor = limit - v_1;
-            // or, line 38
-            lab0: do {
-                v_2 = limit - cursor;
-                lab1: do {
-                    // literal, line 38
-                    if (!(eq_s_b(1, "n")))
-                    {
-                        break lab1;
-                    }
-                    break lab0;
-                } while (false);
-                cursor = limit - v_2;
-                // literal, line 38
-                if (!(eq_s_b(1, "r")))
-                {
-                    return false;
-                }
-            } while (false);
-            return true;
-        }
-
-        private boolean r_S() {
-            int v_1;
-            int v_2;
-            int v_3;
-            // (, line 39
-            // test, line 39
-            v_1 = limit - cursor;
-            // hop, line 39
-            {
-                int c = cursor - 2;
-                if (limit_backward > c || c > limit)
-                {
-                    return false;
-                }
-                cursor = c;
-            }
-            cursor = limit - v_1;
-            // or, line 39
-            lab0: do {
-                v_2 = limit - cursor;
-                lab1: do {
-                    // literal, line 39
-                    if (!(eq_s_b(2, "dr")))
-                    {
-                        break lab1;
-                    }
-                    break lab0;
-                } while (false);
-                cursor = limit - v_2;
-                // (, line 39
-                // literal, line 39
-                if (!(eq_s_b(1, "t")))
-                {
-                    return false;
-                }
-                // not, line 39
-                {
-                    v_3 = limit - cursor;
-                    lab2: do {
-                        // literal, line 39
-                        if (!(eq_s_b(1, "t")))
-                        {
-                            break lab2;
-                        }
-                        return false;
-                    } while (false);
-                    cursor = limit - v_3;
-                }
-            } while (false);
-            return true;
-        }
-
-        private boolean r_T() {
-            int v_1;
-            int v_2;
-            int v_3;
-            // (, line 40
-            // test, line 40
-            v_1 = limit - cursor;
-            // hop, line 40
-            {
-                int c = cursor - 2;
-                if (limit_backward > c || c > limit)
-                {
-                    return false;
-                }
-                cursor = c;
-            }
-            cursor = limit - v_1;
-            // or, line 40
-            lab0: do {
-                v_2 = limit - cursor;
-                lab1: do {
-                    // literal, line 40
-                    if (!(eq_s_b(1, "s")))
-                    {
-                        break lab1;
-                    }
-                    break lab0;
-                } while (false);
-                cursor = limit - v_2;
-                // (, line 40
-                // literal, line 40
-                if (!(eq_s_b(1, "t")))
-                {
-                    return false;
-                }
-                // not, line 40
-                {
-                    v_3 = limit - cursor;
-                    lab2: do {
-                        // literal, line 40
-                        if (!(eq_s_b(1, "o")))
-                        {
-                            break lab2;
-                        }
-                        return false;
-                    } while (false);
-                    cursor = limit - v_3;
-                }
-            } while (false);
-            return true;
-        }
-
-        private boolean r_U() {
-            int v_1;
-            int v_2;
-            // (, line 41
-            // test, line 41
-            v_1 = limit - cursor;
-            // hop, line 41
-            {
-                int c = cursor - 2;
-                if (limit_backward > c || c > limit)
-                {
-                    return false;
-                }
-                cursor = c;
-            }
-            cursor = limit - v_1;
-            // or, line 41
-            lab0: do {
-                v_2 = limit - cursor;
-                lab1: do {
-                    // literal, line 41
-                    if (!(eq_s_b(1, "l")))
-                    {
-                        break lab1;
-                    }
-                    break lab0;
-                } while (false);
-                cursor = limit - v_2;
-                lab2: do {
-                    // literal, line 41
-                    if (!(eq_s_b(1, "m")))
-                    {
-                        break lab2;
-                    }
-                    break lab0;
-                } while (false);
-                cursor = limit - v_2;
-                lab3: do {
-                    // literal, line 41
-                    if (!(eq_s_b(1, "n")))
-                    {
-                        break lab3;
-                    }
-                    break lab0;
-                } while (false);
-                cursor = limit - v_2;
-                // literal, line 41
-                if (!(eq_s_b(1, "r")))
-                {
-                    return false;
-                }
-            } while (false);
-            return true;
-        }
-
-        private boolean r_V() {
-            int v_1;
-            // (, line 42
-            // test, line 42
-            v_1 = limit - cursor;
-            // hop, line 42
-            {
-                int c = cursor - 2;
-                if (limit_backward > c || c > limit)
-                {
-                    return false;
-                }
-                cursor = c;
-            }
-            cursor = limit - v_1;
-            // literal, line 42
-            if (!(eq_s_b(1, "c")))
-            {
-                return false;
-            }
-            return true;
-        }
-
-        private boolean r_W() {
-            int v_1;
-            int v_2;
-            int v_3;
-            // (, line 43
-            // test, line 43
-            v_1 = limit - cursor;
-            // hop, line 43
-            {
-                int c = cursor - 2;
-                if (limit_backward > c || c > limit)
-                {
-                    return false;
-                }
-                cursor = c;
-            }
-            cursor = limit - v_1;
-            // not, line 43
-            {
-                v_2 = limit - cursor;
-                lab0: do {
-                    // literal, line 43
-                    if (!(eq_s_b(1, "s")))
-                    {
-                        break lab0;
-                    }
-                    return false;
-                } while (false);
-                cursor = limit - v_2;
-            }
-            // not, line 43
-            {
-                v_3 = limit - cursor;
-                lab1: do {
-                    // literal, line 43
-                    if (!(eq_s_b(1, "u")))
-                    {
-                        break lab1;
-                    }
-                    return false;
-                } while (false);
-                cursor = limit - v_3;
-            }
-            return true;
-        }
-
-        private boolean r_X() {
-            int v_1;
-            int v_2;
-            // (, line 44
-            // test, line 44
-            v_1 = limit - cursor;
-            // hop, line 44
-            {
-                int c = cursor - 2;
-                if (limit_backward > c || c > limit)
-                {
-                    return false;
-                }
-                cursor = c;
-            }
-            cursor = limit - v_1;
-            // or, line 44
-            lab0: do {
-                v_2 = limit - cursor;
-                lab1: do {
-                    // literal, line 44
-                    if (!(eq_s_b(1, "l")))
-                    {
-                        break lab1;
-                    }
-                    break lab0;
-                } while (false);
-                cursor = limit - v_2;
-                lab2: do {
-                    // literal, line 44
-                    if (!(eq_s_b(1, "i")))
-                    {
-                        break lab2;
-                    }
-                    break lab0;
-                } while (false);
-                cursor = limit - v_2;
-                // (, line 44
-                // literal, line 44
-                if (!(eq_s_b(1, "e")))
-                {
-                    return false;
-                }
-                // next, line 44
-                if (cursor <= limit_backward)
-                {
-                    return false;
-                }
-                cursor--;
-                // literal, line 44
-                if (!(eq_s_b(1, "u")))
-                {
-                    return false;
-                }
-            } while (false);
-            return true;
-        }
-
-        private boolean r_Y() {
-            int v_1;
-            // (, line 45
-            // test, line 45
-            v_1 = limit - cursor;
-            // hop, line 45
-            {
-                int c = cursor - 2;
-                if (limit_backward > c || c > limit)
-                {
-                    return false;
-                }
-                cursor = c;
-            }
-            cursor = limit - v_1;
-            // literal, line 45
-            if (!(eq_s_b(2, "in")))
-            {
-                return false;
-            }
-            return true;
-        }
-
-        private boolean r_Z() {
-            int v_1;
-            int v_2;
-            // (, line 46
-            // test, line 46
-            v_1 = limit - cursor;
-            // hop, line 46
-            {
-                int c = cursor - 2;
-                if (limit_backward > c || c > limit)
-                {
-                    return false;
-                }
-                cursor = c;
-            }
-            cursor = limit - v_1;
-            // not, line 46
-            {
-                v_2 = limit - cursor;
-                lab0: do {
-                    // literal, line 46
-                    if (!(eq_s_b(1, "f")))
-                    {
-                        break lab0;
-                    }
-                    return false;
-                } while (false);
-                cursor = limit - v_2;
-            }
-            return true;
-        }
-
-        private boolean r_AA() {
-            int v_1;
-            // (, line 47
-            // test, line 47
-            v_1 = limit - cursor;
-            // hop, line 47
-            {
-                int c = cursor - 2;
-                if (limit_backward > c || c > limit)
-                {
-                    return false;
-                }
-                cursor = c;
-            }
-            cursor = limit - v_1;
-            // among, line 47
-            if (find_among_b(a_0, 9) == 0)
-            {
-                return false;
-            }
-            return true;
-        }
-
-        private boolean r_BB() {
-            int v_1;
-            int v_2;
-            int v_3;
-            // (, line 49
-            // test, line 49
-            v_1 = limit - cursor;
-            // hop, line 49
-            {
-                int c = cursor - 3;
-                if (limit_backward > c || c > limit)
-                {
-                    return false;
-                }
-                cursor = c;
-            }
-            cursor = limit - v_1;
-            // not, line 49
-            {
-                v_2 = limit - cursor;
-                lab0: do {
-                    // literal, line 49
-                    if (!(eq_s_b(3, "met")))
-                    {
-                        break lab0;
-                    }
-                    return false;
-                } while (false);
-                cursor = limit - v_2;
-            }
-            // not, line 49
-            {
-                v_3 = limit - cursor;
-                lab1: do {
-                    // literal, line 49
-                    if (!(eq_s_b(4, "ryst")))
-                    {
-                        break lab1;
-                    }
-                    return false;
-                } while (false);
-                cursor = limit - v_3;
-            }
-            return true;
-        }
-
-        private boolean r_CC() {
-            int v_1;
-            // (, line 50
-            // test, line 50
-            v_1 = limit - cursor;
-            // hop, line 50
-            {
-                int c = cursor - 2;
-                if (limit_backward > c || c > limit)
-                {
-                    return false;
-                }
-                cursor = c;
-            }
-            cursor = limit - v_1;
-            // literal, line 50
-            if (!(eq_s_b(1, "l")))
-            {
-                return false;
-            }
-            return true;
-        }
-
-        private boolean r_endings() {
-            int among_var;
-            // (, line 55
-            // [, line 56
-            ket = cursor;
-            // substring, line 56
-            among_var = find_among_b(a_1, 294);
-            if (among_var == 0)
-            {
-                return false;
-            }
-            // ], line 56
-            bra = cursor;
-            switch(among_var) {
-                case 0:
-                    return false;
-                case 1:
-                    // (, line 145
-                    // delete, line 145
-                    slice_del();
-                    break;
-            }
-            return true;
-        }
-
-        private boolean r_undouble() {
-            int v_1;
-            // (, line 151
-            // test, line 152
-            v_1 = limit - cursor;
-            // substring, line 152
-            if (find_among_b(a_2, 10) == 0)
-            {
-                return false;
-            }
-            cursor = limit - v_1;
-            // [, line 154
-            ket = cursor;
-            // next, line 154
-            if (cursor <= limit_backward)
-            {
-                return false;
-            }
-            cursor--;
-            // ], line 154
-            bra = cursor;
-            // delete, line 154
-            slice_del();
-            return true;
-        }
-
-        private boolean r_respell() {
-            int among_var;
-            int v_1;
-            int v_2;
-            int v_3;
-            int v_4;
-            int v_5;
-            int v_6;
-            int v_7;
-            int v_8;
-            // (, line 159
-            // [, line 160
-            ket = cursor;
-            // substring, line 160
-            among_var = find_among_b(a_3, 34);
-            if (among_var == 0)
-            {
-                return false;
-            }
-            // ], line 160
-            bra = cursor;
-            switch(among_var) {
-                case 0:
-                    return false;
-                case 1:
-                    // (, line 161
-                    // <-, line 161
-                    slice_from("ief");
-                    break;
-                case 2:
-                    // (, line 162
-                    // <-, line 162
-                    slice_from("uc");
-                    break;
-                case 3:
-                    // (, line 163
-                    // <-, line 163
-                    slice_from("um");
-                    break;
-                case 4:
-                    // (, line 164
-                    // <-, line 164
-                    slice_from("rb");
-                    break;
-                case 5:
-                    // (, line 165
-                    // <-, line 165
-                    slice_from("ur");
-                    break;
-                case 6:
-                    // (, line 166
-                    // <-, line 166
-                    slice_from("ister");
-                    break;
-                case 7:
-                    // (, line 167
-                    // <-, line 167
-                    slice_from("meter");
-                    break;
-                case 8:
-                    // (, line 168
-                    // <-, line 168
-                    slice_from("olut");
-                    break;
-                case 9:
-                    // (, line 169
-                    // not, line 169
-                    {
-                        v_1 = limit - cursor;
-                        lab0: do {
-                            // literal, line 169
-                            if (!(eq_s_b(1, "a")))
-                            {
-                                break lab0;
-                            }
-                            return false;
-                        } while (false);
-                        cursor = limit - v_1;
-                    }
-                    // not, line 169
-                    {
-                        v_2 = limit - cursor;
-                        lab1: do {
-                            // literal, line 169
-                            if (!(eq_s_b(1, "i")))
-                            {
-                                break lab1;
-                            }
-                            return false;
-                        } while (false);
-                        cursor = limit - v_2;
-                    }
-                    // not, line 169
-                    {
-                        v_3 = limit - cursor;
-                        lab2: do {
-                            // literal, line 169
-                            if (!(eq_s_b(1, "o")))
-                            {
-                                break lab2;
-                            }
-                            return false;
-                        } while (false);
-                        cursor = limit - v_3;
-                    }
-                    // <-, line 169
-                    slice_from("l");
-                    break;
-                case 10:
-                    // (, line 170
-                    // <-, line 170
-                    slice_from("bic");
-                    break;
-                case 11:
-                    // (, line 171
-                    // <-, line 171
-                    slice_from("dic");
-                    break;
-                case 12:
-                    // (, line 172
-                    // <-, line 172
-                    slice_from("pic");
-                    break;
-                case 13:
-                    // (, line 173
-                    // <-, line 173
-                    slice_from("tic");
-                    break;
-                case 14:
-                    // (, line 174
-                    // <-, line 174
-                    slice_from("ac");
-                    break;
-                case 15:
-                    // (, line 175
-                    // <-, line 175
-                    slice_from("ec");
-                    break;
-                case 16:
-                    // (, line 176
-                    // <-, line 176
-                    slice_from("ic");
-                    break;
-                case 17:
-                    // (, line 177
-                    // <-, line 177
-                    slice_from("luc");
-                    break;
-                case 18:
-                    // (, line 178
-                    // <-, line 178
-                    slice_from("uas");
-                    break;
-                case 19:
-                    // (, line 179
-                    // <-, line 179
-                    slice_from("vas");
-                    break;
-                case 20:
-                    // (, line 180
-                    // <-, line 180
-                    slice_from("cis");
-                    break;
-                case 21:
-                    // (, line 181
-                    // <-, line 181
-                    slice_from("lis");
-                    break;
-                case 22:
-                    // (, line 182
-                    // <-, line 182
-                    slice_from("eris");
-                    break;
-                case 23:
-                    // (, line 183
-                    // <-, line 183
-                    slice_from("pans");
-                    break;
-                case 24:
-                    // (, line 184
-                    // not, line 184
-                    {
-                        v_4 = limit - cursor;
-                        lab3: do {
-                            // literal, line 184
-                            if (!(eq_s_b(1, "s")))
-                            {
-                                break lab3;
-                            }
-                            return false;
-                        } while (false);
-                        cursor = limit - v_4;
-                    }
-                    // <-, line 184
-                    slice_from("ens");
-                    break;
-                case 25:
-                    // (, line 185
-                    // <-, line 185
-                    slice_from("ons");
-                    break;
-                case 26:
-                    // (, line 186
-                    // <-, line 186
-                    slice_from("lus");
-                    break;
-                case 27:
-                    // (, line 187
-                    // <-, line 187
-                    slice_from("rus");
-                    break;
-                case 28:
-                    // (, line 188
-                    // not, line 188
-                    {
-                        v_5 = limit - cursor;
-                        lab4: do {
-                            // literal, line 188
-                            if (!(eq_s_b(1, "p")))
-                            {
-                                break lab4;
-                            }
-                            return false;
-                        } while (false);
-                        cursor = limit - v_5;
-                    }
-                    // not, line 188
-                    {
-                        v_6 = limit - cursor;
-                        lab5: do {
-                            // literal, line 188
-                            if (!(eq_s_b(1, "t")))
-                            {
-                                break lab5;
-                            }
-                            return false;
-                        } while (false);
-                        cursor = limit - v_6;
-                    }
-                    // <-, line 188
-                    slice_from("hes");
-                    break;
-                case 29:
-                    // (, line 189
-                    // <-, line 189
-                    slice_from("mis");
-                    break;
-                case 30:
-                    // (, line 190
-                    // not, line 190
-                    {
-                        v_7 = limit - cursor;
-                        lab6: do {
-                            // literal, line 190
-                            if (!(eq_s_b(1, "m")))
-                            {
-                                break lab6;
-                            }
-                            return false;
-                        } while (false);
-                        cursor = limit - v_7;
-                    }
-                    // <-, line 190
-                    slice_from("ens");
-                    break;
-                case 31:
-                    // (, line 192
-                    // <-, line 192
-                    slice_from("ers");
-                    break;
-                case 32:
-                    // (, line 193
-                    // not, line 193
-                    {
-                        v_8 = limit - cursor;
-                        lab7: do {
-                            // literal, line 193
-                            if (!(eq_s_b(1, "n")))
-                            {
-                                break lab7;
-                            }
-                            return false;
-                        } while (false);
-                        cursor = limit - v_8;
-                    }
-                    // <-, line 193
-                    slice_from("es");
-                    break;
-                case 33:
-                    // (, line 194
-                    // <-, line 194
-                    slice_from("ys");
-                    break;
-                case 34:
-                    // (, line 195
-                    // <-, line 195
-                    slice_from("ys");
-                    break;
-            }
-            return true;
-        }
-
-        public boolean stem() {
-            int v_1;
-            int v_2;
-            int v_3;
-            // (, line 200
-            // backwards, line 202
-            limit_backward = cursor; cursor = limit;
-            // (, line 202
-            // do, line 203
-            v_1 = limit - cursor;
-            lab0: do {
-                // call endings, line 203
-                if (!r_endings())
-                {
-                    break lab0;
-                }
-            } while (false);
-            cursor = limit - v_1;
-            // do, line 204
-            v_2 = limit - cursor;
-            lab1: do {
-                // call undouble, line 204
-                if (!r_undouble())
-                {
-                    break lab1;
-                }
-            } while (false);
-            cursor = limit - v_2;
-            // do, line 205
-            v_3 = limit - cursor;
-            lab2: do {
-                // call respell, line 205
-                if (!r_respell())
-                {
-                    break lab2;
-                }
-            } while (false);
-            cursor = limit - v_3;
-            cursor = limit_backward;            return true;
-        }
-
-}
-
Index: contrib/snowball/src/java/org/tartarus/snowball/ext/PorterStemmer.java
===================================================================
--- contrib/snowball/src/java/org/tartarus/snowball/ext/PorterStemmer.java	(revision 916567)
+++ contrib/snowball/src/java/org/tartarus/snowball/ext/PorterStemmer.java	(working copy)
@@ -1,906 +0,0 @@
-// This file was generated automatically by the Snowball to Java compiler
-
-package org.tartarus.snowball.ext;
-import org.tartarus.snowball.SnowballProgram;
-import org.tartarus.snowball.Among;
-
-/**
- * Generated class implementing code defined by a snowball script.
- */
-public class PorterStemmer extends SnowballProgram {
-
-        private Among a_0[] = {
-            new Among ( "s", -1, 3, "", this),
-            new Among ( "ies", 0, 2, "", this),
-            new Among ( "sses", 0, 1, "", this),
-            new Among ( "ss", 0, -1, "", this)
-        };
-
-        private Among a_1[] = {
-            new Among ( "", -1, 3, "", this),
-            new Among ( "bb", 0, 2, "", this),
-            new Among ( "dd", 0, 2, "", this),
-            new Among ( "ff", 0, 2, "", this),
-            new Among ( "gg", 0, 2, "", this),
-            new Among ( "bl", 0, 1, "", this),
-            new Among ( "mm", 0, 2, "", this),
-            new Among ( "nn", 0, 2, "", this),
-            new Among ( "pp", 0, 2, "", this),
-            new Among ( "rr", 0, 2, "", this),
-            new Among ( "at", 0, 1, "", this),
-            new Among ( "tt", 0, 2, "", this),
-            new Among ( "iz", 0, 1, "", this)
-        };
-
-        private Among a_2[] = {
-            new Among ( "ed", -1, 2, "", this),
-            new Among ( "eed", 0, 1, "", this),
-            new Among ( "ing", -1, 2, "", this)
-        };
-
-        private Among a_3[] = {
-            new Among ( "anci", -1, 3, "", this),
-            new Among ( "enci", -1, 2, "", this),
-            new Among ( "abli", -1, 4, "", this),
-            new Among ( "eli", -1, 6, "", this),
-            new Among ( "alli", -1, 9, "", this),
-            new Among ( "ousli", -1, 12, "", this),
-            new Among ( "entli", -1, 5, "", this),
-            new Among ( "aliti", -1, 10, "", this),
-            new Among ( "biliti", -1, 14, "", this),
-            new Among ( "iviti", -1, 13, "", this),
-            new Among ( "tional", -1, 1, "", this),
-            new Among ( "ational", 10, 8, "", this),
-            new Among ( "alism", -1, 10, "", this),
-            new Among ( "ation", -1, 8, "", this),
-            new Among ( "ization", 13, 7, "", this),
-            new Among ( "izer", -1, 7, "", this),
-            new Among ( "ator", -1, 8, "", this),
-            new Among ( "iveness", -1, 13, "", this),
-            new Among ( "fulness", -1, 11, "", this),
-            new Among ( "ousness", -1, 12, "", this)
-        };
-
-        private Among a_4[] = {
-            new Among ( "icate", -1, 2, "", this),
-            new Among ( "ative", -1, 3, "", this),
-            new Among ( "alize", -1, 1, "", this),
-            new Among ( "iciti", -1, 2, "", this),
-            new Among ( "ical", -1, 2, "", this),
-            new Among ( "ful", -1, 3, "", this),
-            new Among ( "ness", -1, 3, "", this)
-        };
-
-        private Among a_5[] = {
-            new Among ( "ic", -1, 1, "", this),
-            new Among ( "ance", -1, 1, "", this),
-            new Among ( "ence", -1, 1, "", this),
-            new Among ( "able", -1, 1, "", this),
-            new Among ( "ible", -1, 1, "", this),
-            new Among ( "ate", -1, 1, "", this),
-            new Among ( "ive", -1, 1, "", this),
-            new Among ( "ize", -1, 1, "", this),
-            new Among ( "iti", -1, 1, "", this),
-            new Among ( "al", -1, 1, "", this),
-            new Among ( "ism", -1, 1, "", this),
-            new Among ( "ion", -1, 2, "", this),
-            new Among ( "er", -1, 1, "", this),
-            new Among ( "ous", -1, 1, "", this),
-            new Among ( "ant", -1, 1, "", this),
-            new Among ( "ent", -1, 1, "", this),
-            new Among ( "ment", 15, 1, "", this),
-            new Among ( "ement", 16, 1, "", this),
-            new Among ( "ou", -1, 1, "", this)
-        };
-
-        private static final char g_v[] = {17, 65, 16, 1 };
-
-        private static final char g_v_WXY[] = {1, 17, 65, 208, 1 };
-
-        private boolean B_Y_found;
-        private int I_p2;
-        private int I_p1;
-
-        private void copy_from(PorterStemmer other) {
-            B_Y_found = other.B_Y_found;
-            I_p2 = other.I_p2;
-            I_p1 = other.I_p1;
-            super.copy_from(other);
-        }
-
-        private boolean r_shortv() {
-            // (, line 19
-            if (!(out_grouping_b(g_v_WXY, 89, 121)))
-            {
-                return false;
-            }
-            if (!(in_grouping_b(g_v, 97, 121)))
-            {
-                return false;
-            }
-            if (!(out_grouping_b(g_v, 97, 121)))
-            {
-                return false;
-            }
-            return true;
-        }
-
-        private boolean r_R1() {
-            if (!(I_p1 <= cursor))
-            {
-                return false;
-            }
-            return true;
-        }
-
-        private boolean r_R2() {
-            if (!(I_p2 <= cursor))
-            {
-                return false;
-            }
-            return true;
-        }
-
-        private boolean r_Step_1a() {
-            int among_var;
-            // (, line 24
-            // [, line 25
-            ket = cursor;
-            // substring, line 25
-            among_var = find_among_b(a_0, 4);
-            if (among_var == 0)
-            {
-                return false;
-            }
-            // ], line 25
-            bra = cursor;
-            switch(among_var) {
-                case 0:
-                    return false;
-                case 1:
-                    // (, line 26
-                    // <-, line 26
-                    slice_from("ss");
-                    break;
-                case 2:
-                    // (, line 27
-                    // <-, line 27
-                    slice_from("i");
-                    break;
-                case 3:
-                    // (, line 29
-                    // delete, line 29
-                    slice_del();
-                    break;
-            }
-            return true;
-        }
-
-        private boolean r_Step_1b() {
-            int among_var;
-            int v_1;
-            int v_3;
-            int v_4;
-            // (, line 33
-            // [, line 34
-            ket = cursor;
-            // substring, line 34
-            among_var = find_among_b(a_2, 3);
-            if (among_var == 0)
-            {
-                return false;
-            }
-            // ], line 34
-            bra = cursor;
-            switch(among_var) {
-                case 0:
-                    return false;
-                case 1:
-                    // (, line 35
-                    // call R1, line 35
-                    if (!r_R1())
-                    {
-                        return false;
-                    }
-                    // <-, line 35
-                    slice_from("ee");
-                    break;
-                case 2:
-                    // (, line 37
-                    // test, line 38
-                    v_1 = limit - cursor;
-                    // gopast, line 38
-                    golab0: while(true)
-                    {
-                        lab1: do {
-                            if (!(in_grouping_b(g_v, 97, 121)))
-                            {
-                                break lab1;
-                            }
-                            break golab0;
-                        } while (false);
-                        if (cursor <= limit_backward)
-                        {
-                            return false;
-                        }
-                        cursor--;
-                    }
-                    cursor = limit - v_1;
-                    // delete, line 38
-                    slice_del();
-                    // test, line 39
-                    v_3 = limit - cursor;
-                    // substring, line 39
-                    among_var = find_among_b(a_1, 13);
-                    if (among_var == 0)
-                    {
-                        return false;
-                    }
-                    cursor = limit - v_3;
-                    switch(among_var) {
-                        case 0:
-                            return false;
-                        case 1:
-                            // (, line 41
-                            // <+, line 41
-                            {
-                                int c = cursor;
-                                insert(cursor, cursor, "e");
-                                cursor = c;
-                            }
-                            break;
-                        case 2:
-                            // (, line 44
-                            // [, line 44
-                            ket = cursor;
-                            // next, line 44
-                            if (cursor <= limit_backward)
-                            {
-                                return false;
-                            }
-                            cursor--;
-                            // ], line 44
-                            bra = cursor;
-                            // delete, line 44
-                            slice_del();
-                            break;
-                        case 3:
-                            // (, line 45
-                            // atmark, line 45
-                            if (cursor != I_p1)
-                            {
-                                return false;
-                            }
-                            // test, line 45
-                            v_4 = limit - cursor;
-                            // call shortv, line 45
-                            if (!r_shortv())
-                            {
-                                return false;
-                            }
-                            cursor = limit - v_4;
-                            // <+, line 45
-                            {
-                                int c = cursor;
-                                insert(cursor, cursor, "e");
-                                cursor = c;
-                            }
-                            break;
-                    }
-                    break;
-            }
-            return true;
-        }
-
-        private boolean r_Step_1c() {
-            int v_1;
-            // (, line 51
-            // [, line 52
-            ket = cursor;
-            // or, line 52
-            lab0: do {
-                v_1 = limit - cursor;
-                lab1: do {
-                    // literal, line 52
-                    if (!(eq_s_b(1, "y")))
-                    {
-                        break lab1;
-                    }
-                    break lab0;
-                } while (false);
-                cursor = limit - v_1;
-                // literal, line 52
-                if (!(eq_s_b(1, "Y")))
-                {
-                    return false;
-                }
-            } while (false);
-            // ], line 52
-            bra = cursor;
-            // gopast, line 53
-            golab2: while(true)
-            {
-                lab3: do {
-                    if (!(in_grouping_b(g_v, 97, 121)))
-                    {
-                        break lab3;
-                    }
-                    break golab2;
-                } while (false);
-                if (cursor <= limit_backward)
-                {
-                    return false;
-                }
-                cursor--;
-            }
-            // <-, line 54
-            slice_from("i");
-            return true;
-        }
-
-        private boolean r_Step_2() {
-            int among_var;
-            // (, line 57
-            // [, line 58
-            ket = cursor;
-            // substring, line 58
-            among_var = find_among_b(a_3, 20);
-            if (among_var == 0)
-            {
-                return false;
-            }
-            // ], line 58
-            bra = cursor;
-            // call R1, line 58
-            if (!r_R1())
-            {
-                return false;
-            }
-            switch(among_var) {
-                case 0:
-                    return false;
-                case 1:
-                    // (, line 59
-                    // <-, line 59
-                    slice_from("tion");
-                    break;
-                case 2:
-                    // (, line 60
-                    // <-, line 60
-                    slice_from("ence");
-                    break;
-                case 3:
-                    // (, line 61
-                    // <-, line 61
-                    slice_from("ance");
-                    break;
-                case 4:
-                    // (, line 62
-                    // <-, line 62
-                    slice_from("able");
-                    break;
-                case 5:
-                    // (, line 63
-                    // <-, line 63
-                    slice_from("ent");
-                    break;
-                case 6:
-                    // (, line 64
-                    // <-, line 64
-                    slice_from("e");
-                    break;
-                case 7:
-                    // (, line 66
-                    // <-, line 66
-                    slice_from("ize");
-                    break;
-                case 8:
-                    // (, line 68
-                    // <-, line 68
-                    slice_from("ate");
-                    break;
-                case 9:
-                    // (, line 69
-                    // <-, line 69
-                    slice_from("al");
-                    break;
-                case 10:
-                    // (, line 71
-                    // <-, line 71
-                    slice_from("al");
-                    break;
-                case 11:
-                    // (, line 72
-                    // <-, line 72
-                    slice_from("ful");
-                    break;
-                case 12:
-                    // (, line 74
-                    // <-, line 74
-                    slice_from("ous");
-                    break;
-                case 13:
-                    // (, line 76
-                    // <-, line 76
-                    slice_from("ive");
-                    break;
-                case 14:
-                    // (, line 77
-                    // <-, line 77
-                    slice_from("ble");
-                    break;
-            }
-            return true;
-        }
-
-        private boolean r_Step_3() {
-            int among_var;
-            // (, line 81
-            // [, line 82
-            ket = cursor;
-            // substring, line 82
-            among_var = find_among_b(a_4, 7);
-            if (among_var == 0)
-            {
-                return false;
-            }
-            // ], line 82
-            bra = cursor;
-            // call R1, line 82
-            if (!r_R1())
-            {
-                return false;
-            }
-            switch(among_var) {
-                case 0:
-                    return false;
-                case 1:
-                    // (, line 83
-                    // <-, line 83
-                    slice_from("al");
-                    break;
-                case 2:
-                    // (, line 85
-                    // <-, line 85
-                    slice_from("ic");
-                    break;
-                case 3:
-                    // (, line 87
-                    // delete, line 87
-                    slice_del();
-                    break;
-            }
-            return true;
-        }
-
-        private boolean r_Step_4() {
-            int among_var;
-            int v_1;
-            // (, line 91
-            // [, line 92
-            ket = cursor;
-            // substring, line 92
-            among_var = find_among_b(a_5, 19);
-            if (among_var == 0)
-            {
-                return false;
-            }
-            // ], line 92
-            bra = cursor;
-            // call R2, line 92
-            if (!r_R2())
-            {
-                return false;
-            }
-            switch(among_var) {
-                case 0:
-                    return false;
-                case 1:
-                    // (, line 95
-                    // delete, line 95
-                    slice_del();
-                    break;
-                case 2:
-                    // (, line 96
-                    // or, line 96
-                    lab0: do {
-                        v_1 = limit - cursor;
-                        lab1: do {
-                            // literal, line 96
-                            if (!(eq_s_b(1, "s")))
-                            {
-                                break lab1;
-                            }
-                            break lab0;
-                        } while (false);
-                        cursor = limit - v_1;
-                        // literal, line 96
-                        if (!(eq_s_b(1, "t")))
-                        {
-                            return false;
-                        }
-                    } while (false);
-                    // delete, line 96
-                    slice_del();
-                    break;
-            }
-            return true;
-        }
-
-        private boolean r_Step_5a() {
-            int v_1;
-            int v_2;
-            // (, line 100
-            // [, line 101
-            ket = cursor;
-            // literal, line 101
-            if (!(eq_s_b(1, "e")))
-            {
-                return false;
-            }
-            // ], line 101
-            bra = cursor;
-            // or, line 102
-            lab0: do {
-                v_1 = limit - cursor;
-                lab1: do {
-                    // call R2, line 102
-                    if (!r_R2())
-                    {
-                        break lab1;
-                    }
-                    break lab0;
-                } while (false);
-                cursor = limit - v_1;
-                // (, line 102
-                // call R1, line 102
-                if (!r_R1())
-                {
-                    return false;
-                }
-                // not, line 102
-                {
-                    v_2 = limit - cursor;
-                    lab2: do {
-                        // call shortv, line 102
-                        if (!r_shortv())
-                        {
-                            break lab2;
-                        }
-                        return false;
-                    } while (false);
-                    cursor = limit - v_2;
-                }
-            } while (false);
-            // delete, line 103
-            slice_del();
-            return true;
-        }
-
-        private boolean r_Step_5b() {
-            // (, line 106
-            // [, line 107
-            ket = cursor;
-            // literal, line 107
-            if (!(eq_s_b(1, "l")))
-            {
-                return false;
-            }
-            // ], line 107
-            bra = cursor;
-            // call R2, line 108
-            if (!r_R2())
-            {
-                return false;
-            }
-            // literal, line 108
-            if (!(eq_s_b(1, "l")))
-            {
-                return false;
-            }
-            // delete, line 109
-            slice_del();
-            return true;
-        }
-
-        public boolean stem() {
-            int v_1;
-            int v_2;
-            int v_3;
-            int v_4;
-            int v_5;
-            int v_10;
-            int v_11;
-            int v_12;
-            int v_13;
-            int v_14;
-            int v_15;
-            int v_16;
-            int v_17;
-            int v_18;
-            int v_19;
-            int v_20;
-            // (, line 113
-            // unset Y_found, line 115
-            B_Y_found = false;
-            // do, line 116
-            v_1 = cursor;
-            lab0: do {
-                // (, line 116
-                // [, line 116
-                bra = cursor;
-                // literal, line 116
-                if (!(eq_s(1, "y")))
-                {
-                    break lab0;
-                }
-                // ], line 116
-                ket = cursor;
-                // <-, line 116
-                slice_from("Y");
-                // set Y_found, line 116
-                B_Y_found = true;
-            } while (false);
-            cursor = v_1;
-            // do, line 117
-            v_2 = cursor;
-            lab1: do {
-                // repeat, line 117
-                replab2: while(true)
-                {
-                    v_3 = cursor;
-                    lab3: do {
-                        // (, line 117
-                        // goto, line 117
-                        golab4: while(true)
-                        {
-                            v_4 = cursor;
-                            lab5: do {
-                                // (, line 117
-                                if (!(in_grouping(g_v, 97, 121)))
-                                {
-                                    break lab5;
-                                }
-                                // [, line 117
-                                bra = cursor;
-                                // literal, line 117
-                                if (!(eq_s(1, "y")))
-                                {
-                                    break lab5;
-                                }
-                                // ], line 117
-                                ket = cursor;
-                                cursor = v_4;
-                                break golab4;
-                            } while (false);
-                            cursor = v_4;
-                            if (cursor >= limit)
-                            {
-                                break lab3;
-                            }
-                            cursor++;
-                        }
-                        // <-, line 117
-                        slice_from("Y");
-                        // set Y_found, line 117
-                        B_Y_found = true;
-                        continue replab2;
-                    } while (false);
-                    cursor = v_3;
-                    break replab2;
-                }
-            } while (false);
-            cursor = v_2;
-            I_p1 = limit;
-            I_p2 = limit;
-            // do, line 121
-            v_5 = cursor;
-            lab6: do {
-                // (, line 121
-                // gopast, line 122
-                golab7: while(true)
-                {
-                    lab8: do {
-                        if (!(in_grouping(g_v, 97, 121)))
-                        {
-                            break lab8;
-                        }
-                        break golab7;
-                    } while (false);
-                    if (cursor >= limit)
-                    {
-                        break lab6;
-                    }
-                    cursor++;
-                }
-                // gopast, line 122
-                golab9: while(true)
-                {
-                    lab10: do {
-                        if (!(out_grouping(g_v, 97, 121)))
-                        {
-                            break lab10;
-                        }
-                        break golab9;
-                    } while (false);
-                    if (cursor >= limit)
-                    {
-                        break lab6;
-                    }
-                    cursor++;
-                }
-                // setmark p1, line 122
-                I_p1 = cursor;
-                // gopast, line 123
-                golab11: while(true)
-                {
-                    lab12: do {
-                        if (!(in_grouping(g_v, 97, 121)))
-                        {
-                            break lab12;
-                        }
-                        break golab11;
-                    } while (false);
-                    if (cursor >= limit)
-                    {
-                        break lab6;
-                    }
-                    cursor++;
-                }
-                // gopast, line 123
-                golab13: while(true)
-                {
-                    lab14: do {
-                        if (!(out_grouping(g_v, 97, 121)))
-                        {
-                            break lab14;
-                        }
-                        break golab13;
-                    } while (false);
-                    if (cursor >= limit)
-                    {
-                        break lab6;
-                    }
-                    cursor++;
-                }
-                // setmark p2, line 123
-                I_p2 = cursor;
-            } while (false);
-            cursor = v_5;
-            // backwards, line 126
-            limit_backward = cursor; cursor = limit;
-            // (, line 126
-            // do, line 127
-            v_10 = limit - cursor;
-            lab15: do {
-                // call Step_1a, line 127
-                if (!r_Step_1a())
-                {
-                    break lab15;
-                }
-            } while (false);
-            cursor = limit - v_10;
-            // do, line 128
-            v_11 = limit - cursor;
-            lab16: do {
-                // call Step_1b, line 128
-                if (!r_Step_1b())
-                {
-                    break lab16;
-                }
-            } while (false);
-            cursor = limit - v_11;
-            // do, line 129
-            v_12 = limit - cursor;
-            lab17: do {
-                // call Step_1c, line 129
-                if (!r_Step_1c())
-                {
-                    break lab17;
-                }
-            } while (false);
-            cursor = limit - v_12;
-            // do, line 130
-            v_13 = limit - cursor;
-            lab18: do {
-                // call Step_2, line 130
-                if (!r_Step_2())
-                {
-                    break lab18;
-                }
-            } while (false);
-            cursor = limit - v_13;
-            // do, line 131
-            v_14 = limit - cursor;
-            lab19: do {
-                // call Step_3, line 131
-                if (!r_Step_3())
-                {
-                    break lab19;
-                }
-            } while (false);
-            cursor = limit - v_14;
-            // do, line 132
-            v_15 = limit - cursor;
-            lab20: do {
-                // call Step_4, line 132
-                if (!r_Step_4())
-                {
-                    break lab20;
-                }
-            } while (false);
-            cursor = limit - v_15;
-            // do, line 133
-            v_16 = limit - cursor;
-            lab21: do {
-                // call Step_5a, line 133
-                if (!r_Step_5a())
-                {
-                    break lab21;
-                }
-            } while (false);
-            cursor = limit - v_16;
-            // do, line 134
-            v_17 = limit - cursor;
-            lab22: do {
-                // call Step_5b, line 134
-                if (!r_Step_5b())
-                {
-                    break lab22;
-                }
-            } while (false);
-            cursor = limit - v_17;
-            cursor = limit_backward;            // do, line 137
-            v_18 = cursor;
-            lab23: do {
-                // (, line 137
-                // Boolean test Y_found, line 137
-                if (!(B_Y_found))
-                {
-                    break lab23;
-                }
-                // repeat, line 137
-                replab24: while(true)
-                {
-                    v_19 = cursor;
-                    lab25: do {
-                        // (, line 137
-                        // goto, line 137
-                        golab26: while(true)
-                        {
-                            v_20 = cursor;
-                            lab27: do {
-                                // (, line 137
-                                // [, line 137
-                                bra = cursor;
-                                // literal, line 137
-                                if (!(eq_s(1, "Y")))
-                                {
-                                    break lab27;
-                                }
-                                // ], line 137
-                                ket = cursor;
-                                cursor = v_20;
-                                break golab26;
-                            } while (false);
-                            cursor = v_20;
-                            if (cursor >= limit)
-                            {
-                                break lab25;
-                            }
-                            cursor++;
-                        }
-                        // <-, line 137
-                        slice_from("y");
-                        continue replab24;
-                    } while (false);
-                    cursor = v_19;
-                    break replab24;
-                }
-            } while (false);
-            cursor = v_18;
-            return true;
-        }
-
-}
-
Index: contrib/snowball/src/java/org/tartarus/snowball/ext/HungarianStemmer.java
===================================================================
--- contrib/snowball/src/java/org/tartarus/snowball/ext/HungarianStemmer.java	(revision 916567)
+++ contrib/snowball/src/java/org/tartarus/snowball/ext/HungarianStemmer.java	(working copy)
@@ -1,1158 +0,0 @@
-// This file was generated automatically by the Snowball to Java compiler
-
-package org.tartarus.snowball.ext;
-import org.tartarus.snowball.SnowballProgram;
-import org.tartarus.snowball.Among;
-
-/**
- * Generated class implementing code defined by a snowball script.
- */
-public class HungarianStemmer extends SnowballProgram {
-
-        private Among a_0[] = {
-            new Among ( "cs", -1, -1, "", this),
-            new Among ( "dzs", -1, -1, "", this),
-            new Among ( "gy", -1, -1, "", this),
-            new Among ( "ly", -1, -1, "", this),
-            new Among ( "ny", -1, -1, "", this),
-            new Among ( "sz", -1, -1, "", this),
-            new Among ( "ty", -1, -1, "", this),
-            new Among ( "zs", -1, -1, "", this)
-        };
-
-        private Among a_1[] = {
-            new Among ( "\u00E1", -1, 1, "", this),
-            new Among ( "\u00E9", -1, 2, "", this)
-        };
-
-        private Among a_2[] = {
-            new Among ( "bb", -1, -1, "", this),
-            new Among ( "cc", -1, -1, "", this),
-            new Among ( "dd", -1, -1, "", this),
-            new Among ( "ff", -1, -1, "", this),
-            new Among ( "gg", -1, -1, "", this),
-            new Among ( "jj", -1, -1, "", this),
-            new Among ( "kk", -1, -1, "", this),
-            new Among ( "ll", -1, -1, "", this),
-            new Among ( "mm", -1, -1, "", this),
-            new Among ( "nn", -1, -1, "", this),
-            new Among ( "pp", -1, -1, "", this),
-            new Among ( "rr", -1, -1, "", this),
-            new Among ( "ccs", -1, -1, "", this),
-            new Among ( "ss", -1, -1, "", this),
-            new Among ( "zzs", -1, -1, "", this),
-            new Among ( "tt", -1, -1, "", this),
-            new Among ( "vv", -1, -1, "", this),
-            new Among ( "ggy", -1, -1, "", this),
-            new Among ( "lly", -1, -1, "", this),
-            new Among ( "nny", -1, -1, "", this),
-            new Among ( "tty", -1, -1, "", this),
-            new Among ( "ssz", -1, -1, "", this),
-            new Among ( "zz", -1, -1, "", this)
-        };
-
-        private Among a_3[] = {
-            new Among ( "al", -1, 1, "", this),
-            new Among ( "el", -1, 2, "", this)
-        };
-
-        private Among a_4[] = {
-            new Among ( "ba", -1, -1, "", this),
-            new Among ( "ra", -1, -1, "", this),
-            new Among ( "be", -1, -1, "", this),
-            new Among ( "re", -1, -1, "", this),
-            new Among ( "ig", -1, -1, "", this),
-            new Among ( "nak", -1, -1, "", this),
-            new Among ( "nek", -1, -1, "", this),
-            new Among ( "val", -1, -1, "", this),
-            new Among ( "vel", -1, -1, "", this),
-            new Among ( "ul", -1, -1, "", this),
-            new Among ( "n\u00E1l", -1, -1, "", this),
-            new Among ( "n\u00E9l", -1, -1, "", this),
-            new Among ( "b\u00F3l", -1, -1, "", this),
-            new Among ( "r\u00F3l", -1, -1, "", this),
-            new Among ( "t\u00F3l", -1, -1, "", this),
-            new Among ( "b\u00F5l", -1, -1, "", this),
-            new Among ( "r\u00F5l", -1, -1, "", this),
-            new Among ( "t\u00F5l", -1, -1, "", this),
-            new Among ( "\u00FCl", -1, -1, "", this),
-            new Among ( "n", -1, -1, "", this),
-            new Among ( "an", 19, -1, "", this),
-            new Among ( "ban", 20, -1, "", this),
-            new Among ( "en", 19, -1, "", this),
-            new Among ( "ben", 22, -1, "", this),
-            new Among ( "k\u00E9ppen", 22, -1, "", this),
-            new Among ( "on", 19, -1, "", this),
-            new Among ( "\u00F6n", 19, -1, "", this),
-            new Among ( "k\u00E9pp", -1, -1, "", this),
-            new Among ( "kor", -1, -1, "", this),
-            new Among ( "t", -1, -1, "", this),
-            new Among ( "at", 29, -1, "", this),
-            new Among ( "et", 29, -1, "", this),
-            new Among ( "k\u00E9nt", 29, -1, "", this),
-            new Among ( "ank\u00E9nt", 32, -1, "", this),
-            new Among ( "enk\u00E9nt", 32, -1, "", this),
-            new Among ( "onk\u00E9nt", 32, -1, "", this),
-            new Among ( "ot", 29, -1, "", this),
-            new Among ( "\u00E9rt", 29, -1, "", this),
-            new Among ( "\u00F6t", 29, -1, "", this),
-            new Among ( "hez", -1, -1, "", this),
-            new Among ( "hoz", -1, -1, "", this),
-            new Among ( "h\u00F6z", -1, -1, "", this),
-            new Among ( "v\u00E1", -1, -1, "", this),
-            new Among ( "v\u00E9", -1, -1, "", this)
-        };
-
-        private Among a_5[] = {
-            new Among ( "\u00E1n", -1, 2, "", this),
-            new Among ( "\u00E9n", -1, 1, "", this),
-            new Among ( "\u00E1nk\u00E9nt", -1, 3, "", this)
-        };
-
-        private Among a_6[] = {
-            new Among ( "stul", -1, 2, "", this),
-            new Among ( "astul", 0, 1, "", this),
-            new Among ( "\u00E1stul", 0, 3, "", this),
-            new Among ( "st\u00FCl", -1, 2, "", this),
-            new Among ( "est\u00FCl", 3, 1, "", this),
-            new Among ( "\u00E9st\u00FCl", 3, 4, "", this)
-        };
-
-        private Among a_7[] = {
-            new Among ( "\u00E1", -1, 1, "", this),
-            new Among ( "\u00E9", -1, 2, "", this)
-        };
-
-        private Among a_8[] = {
-            new Among ( "k", -1, 7, "", this),
-            new Among ( "ak", 0, 4, "", this),
-            new Among ( "ek", 0, 6, "", this),
-            new Among ( "ok", 0, 5, "", this),
-            new Among ( "\u00E1k", 0, 1, "", this),
-            new Among ( "\u00E9k", 0, 2, "", this),
-            new Among ( "\u00F6k", 0, 3, "", this)
-        };
-
-        private Among a_9[] = {
-            new Among ( "\u00E9i", -1, 7, "", this),
-            new Among ( "\u00E1\u00E9i", 0, 6, "", this),
-            new Among ( "\u00E9\u00E9i", 0, 5, "", this),
-            new Among ( "\u00E9", -1, 9, "", this),
-            new Among ( "k\u00E9", 3, 4, "", this),
-            new Among ( "ak\u00E9", 4, 1, "", this),
-            new Among ( "ek\u00E9", 4, 1, "", this),
-            new Among ( "ok\u00E9", 4, 1, "", this),
-            new Among ( "\u00E1k\u00E9", 4, 3, "", this),
-            new Among ( "\u00E9k\u00E9", 4, 2, "", this),
-            new Among ( "\u00F6k\u00E9", 4, 1, "", this),
-            new Among ( "\u00E9\u00E9", 3, 8, "", this)
-        };
-
-        private Among a_10[] = {
-            new Among ( "a", -1, 18, "", this),
-            new Among ( "ja", 0, 17, "", this),
-            new Among ( "d", -1, 16, "", this),
-            new Among ( "ad", 2, 13, "", this),
-            new Among ( "ed", 2, 13, "", this),
-            new Among ( "od", 2, 13, "", this),
-            new Among ( "\u00E1d", 2, 14, "", this),
-            new Among ( "\u00E9d", 2, 15, "", this),
-            new Among ( "\u00F6d", 2, 13, "", this),
-            new Among ( "e", -1, 18, "", this),
-            new Among ( "je", 9, 17, "", this),
-            new Among ( "nk", -1, 4, "", this),
-            new Among ( "unk", 11, 1, "", this),
-            new Among ( "\u00E1nk", 11, 2, "", this),
-            new Among ( "\u00E9nk", 11, 3, "", this),
-            new Among ( "\u00FCnk", 11, 1, "", this),
-            new Among ( "uk", -1, 8, "", this),
-            new Among ( "juk", 16, 7, "", this),
-            new Among ( "\u00E1juk", 17, 5, "", this),
-            new Among ( "\u00FCk", -1, 8, "", this),
-            new Among ( "j\u00FCk", 19, 7, "", this),
-            new Among ( "\u00E9j\u00FCk", 20, 6, "", this),
-            new Among ( "m", -1, 12, "", this),
-            new Among ( "am", 22, 9, "", this),
-            new Among ( "em", 22, 9, "", this),
-            new Among ( "om", 22, 9, "", this),
-            new Among ( "\u00E1m", 22, 10, "", this),
-            new Among ( "\u00E9m", 22, 11, "", this),
-            new Among ( "o", -1, 18, "", this),
-            new Among ( "\u00E1", -1, 19, "", this),
-            new Among ( "\u00E9", -1, 20, "", this)
-        };
-
-        private Among a_11[] = {
-            new Among ( "id", -1, 10, "", this),
-            new Among ( "aid", 0, 9, "", this),
-            new Among ( "jaid", 1, 6, "", this),
-            new Among ( "eid", 0, 9, "", this),
-            new Among ( "jeid", 3, 6, "", this),
-            new Among ( "\u00E1id", 0, 7, "", this),
-            new Among ( "\u00E9id", 0, 8, "", this),
-            new Among ( "i", -1, 15, "", this),
-            new Among ( "ai", 7, 14, "", this),
-            new Among ( "jai", 8, 11, "", this),
-            new Among ( "ei", 7, 14, "", this),
-            new Among ( "jei", 10, 11, "", this),
-            new Among ( "\u00E1i", 7, 12, "", this),
-            new Among ( "\u00E9i", 7, 13, "", this),
-            new Among ( "itek", -1, 24, "", this),
-            new Among ( "eitek", 14, 21, "", this),
-            new Among ( "jeitek", 15, 20, "", this),
-            new Among ( "\u00E9itek", 14, 23, "", this),
-            new Among ( "ik", -1, 29, "", this),
-            new Among ( "aik", 18, 26, "", this),
-            new Among ( "jaik", 19, 25, "", this),
-            new Among ( "eik", 18, 26, "", this),
-            new Among ( "jeik", 21, 25, "", this),
-            new Among ( "\u00E1ik", 18, 27, "", this),
-            new Among ( "\u00E9ik", 18, 28, "", this),
-            new Among ( "ink", -1, 20, "", this),
-            new Among ( "aink", 25, 17, "", this),
-            new Among ( "jaink", 26, 16, "", this),
-            new Among ( "eink", 25, 17, "", this),
-            new Among ( "jeink", 28, 16, "", this),
-            new Among ( "\u00E1ink", 25, 18, "", this),
-            new Among ( "\u00E9ink", 25, 19, "", this),
-            new Among ( "aitok", -1, 21, "", this),
-            new Among ( "jaitok", 32, 20, "", this),
-            new Among ( "\u00E1itok", -1, 22, "", this),
-            new Among ( "im", -1, 5, "", this),
-            new Among ( "aim", 35, 4, "", this),
-            new Among ( "jaim", 36, 1, "", this),
-            new Among ( "eim", 35, 4, "", this),
-            new Among ( "jeim", 38, 1, "", this),
-            new Among ( "\u00E1im", 35, 2, "", this),
-            new Among ( "\u00E9im", 35, 3, "", this)
-        };
-
-        private static final char g_v[] = {17, 65, 16, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 17, 52, 14 };
-
-        private int I_p1;
-
-        private void copy_from(HungarianStemmer other) {
-            I_p1 = other.I_p1;
-            super.copy_from(other);
-        }
-
-        private boolean r_mark_regions() {
-            int v_1;
-            int v_2;
-            int v_3;
-            // (, line 44
-            I_p1 = limit;
-            // or, line 51
-            lab0: do {
-                v_1 = cursor;
-                lab1: do {
-                    // (, line 48
-                    if (!(in_grouping(g_v, 97, 252)))
-                    {
-                        break lab1;
-                    }
-                    // goto, line 48
-                    golab2: while(true)
-                    {
-                        v_2 = cursor;
-                        lab3: do {
-                            if (!(out_grouping(g_v, 97, 252)))
-                            {
-                                break lab3;
-                            }
-                            cursor = v_2;
-                            break golab2;
-                        } while (false);
-                        cursor = v_2;
-                        if (cursor >= limit)
-                        {
-                            break lab1;
-                        }
-                        cursor++;
-                    }
-                    // or, line 49
-                    lab4: do {
-                        v_3 = cursor;
-                        lab5: do {
-                            // among, line 49
-                            if (find_among(a_0, 8) == 0)
-                            {
-                                break lab5;
-                            }
-                            break lab4;
-                        } while (false);
-                        cursor = v_3;
-                        // next, line 49
-                        if (cursor >= limit)
-                        {
-                            break lab1;
-                        }
-                        cursor++;
-                    } while (false);
-                    // setmark p1, line 50
-                    I_p1 = cursor;
-                    break lab0;
-                } while (false);
-                cursor = v_1;
-                // (, line 53
-                if (!(out_grouping(g_v, 97, 252)))
-                {
-                    return false;
-                }
-                // gopast, line 53
-                golab6: while(true)
-                {
-                    lab7: do {
-                        if (!(in_grouping(g_v, 97, 252)))
-                        {
-                            break lab7;
-                        }
-                        break golab6;
-                    } while (false);
-                    if (cursor >= limit)
-                    {
-                        return false;
-                    }
-                    cursor++;
-                }
-                // setmark p1, line 53
-                I_p1 = cursor;
-            } while (false);
-            return true;
-        }
-
-        private boolean r_R1() {
-            if (!(I_p1 <= cursor))
-            {
-                return false;
-            }
-            return true;
-        }
-
-        private boolean r_v_ending() {
-            int among_var;
-            // (, line 60
-            // [, line 61
-            ket = cursor;
-            // substring, line 61
-            among_var = find_among_b(a_1, 2);
-            if (among_var == 0)
-            {
-                return false;
-            }
-            // ], line 61
-            bra = cursor;
-            // call R1, line 61
-            if (!r_R1())
-            {
-                return false;
-            }
-            switch(among_var) {
-                case 0:
-                    return false;
-                case 1:
-                    // (, line 62
-                    // <-, line 62
-                    slice_from("a");
-                    break;
-                case 2:
-                    // (, line 63
-                    // <-, line 63
-                    slice_from("e");
-                    break;
-            }
-            return true;
-        }
-
-        private boolean r_double() {
-            int v_1;
-            // (, line 67
-            // test, line 68
-            v_1 = limit - cursor;
-            // among, line 68
-            if (find_among_b(a_2, 23) == 0)
-            {
-                return false;
-            }
-            cursor = limit - v_1;
-            return true;
-        }
-
-        private boolean r_undouble() {
-            // (, line 72
-            // next, line 73
-            if (cursor <= limit_backward)
-            {
-                return false;
-            }
-            cursor--;
-            // [, line 73
-            ket = cursor;
-            // hop, line 73
-            {
-                int c = cursor - 1;
-                if (limit_backward > c || c > limit)
-                {
-                    return false;
-                }
-                cursor = c;
-            }
-            // ], line 73
-            bra = cursor;
-            // delete, line 73
-            slice_del();
-            return true;
-        }
-
-        private boolean r_instrum() {
-            int among_var;
-            // (, line 76
-            // [, line 77
-            ket = cursor;
-            // substring, line 77
-            among_var = find_among_b(a_3, 2);
-            if (among_var == 0)
-            {
-                return false;
-            }
-            // ], line 77
-            bra = cursor;
-            // call R1, line 77
-            if (!r_R1())
-            {
-                return false;
-            }
-            switch(among_var) {
-                case 0:
-                    return false;
-                case 1:
-                    // (, line 78
-                    // call double, line 78
-                    if (!r_double())
-                    {
-                        return false;
-                    }
-                    break;
-                case 2:
-                    // (, line 79
-                    // call double, line 79
-                    if (!r_double())
-                    {
-                        return false;
-                    }
-                    break;
-            }
-            // delete, line 81
-            slice_del();
-            // call undouble, line 82
-            if (!r_undouble())
-            {
-                return false;
-            }
-            return true;
-        }
-
-        private boolean r_case() {
-            // (, line 86
-            // [, line 87
-            ket = cursor;
-            // substring, line 87
-            if (find_among_b(a_4, 44) == 0)
-            {
-                return false;
-            }
-            // ], line 87
-            bra = cursor;
-            // call R1, line 87
-            if (!r_R1())
-            {
-                return false;
-            }
-            // delete, line 111
-            slice_del();
-            // call v_ending, line 112
-            if (!r_v_ending())
-            {
-                return false;
-            }
-            return true;
-        }
-
-        private boolean r_case_special() {
-            int among_var;
-            // (, line 115
-            // [, line 116
-            ket = cursor;
-            // substring, line 116
-            among_var = find_among_b(a_5, 3);
-            if (among_var == 0)
-            {
-                return false;
-            }
-            // ], line 116
-            bra = cursor;
-            // call R1, line 116
-            if (!r_R1())
-            {
-                return false;
-            }
-            switch(among_var) {
-                case 0:
-                    return false;
-                case 1:
-                    // (, line 117
-                    // <-, line 117
-                    slice_from("e");
-                    break;
-                case 2:
-                    // (, line 118
-                    // <-, line 118
-                    slice_from("a");
-                    break;
-                case 3:
-                    // (, line 119
-                    // <-, line 119
-                    slice_from("a");
-                    break;
-            }
-            return true;
-        }
-
-        private boolean r_case_other() {
-            int among_var;
-            // (, line 123
-            // [, line 124
-            ket = cursor;
-            // substring, line 124
-            among_var = find_among_b(a_6, 6);
-            if (among_var == 0)
-            {
-                return false;
-            }
-            // ], line 124
-            bra = cursor;
-            // call R1, line 124
-            if (!r_R1())
-            {
-                return false;
-            }
-            switch(among_var) {
-                case 0:
-                    return false;
-                case 1:
-                    // (, line 125
-                    // delete, line 125
-                    slice_del();
-                    break;
-                case 2:
-                    // (, line 126
-                    // delete, line 126
-                    slice_del();
-                    break;
-                case 3:
-                    // (, line 127
-                    // <-, line 127
-                    slice_from("a");
-                    break;
-                case 4:
-                    // (, line 128
-                    // <-, line 128
-                    slice_from("e");
-                    break;
-            }
-            return true;
-        }
-
-        private boolean r_factive() {
-            int among_var;
-            // (, line 132
-            // [, line 133
-            ket = cursor;
-            // substring, line 133
-            among_var = find_among_b(a_7, 2);
-            if (among_var == 0)
-            {
-                return false;
-            }
-            // ], line 133
-            bra = cursor;
-            // call R1, line 133
-            if (!r_R1())
-            {
-                return false;
-            }
-            switch(among_var) {
-                case 0:
-                    return false;
-                case 1:
-                    // (, line 134
-                    // call double, line 134
-                    if (!r_double())
-                    {
-                        return false;
-                    }
-                    break;
-                case 2:
-                    // (, line 135
-                    // call double, line 135
-                    if (!r_double())
-                    {
-                        return false;
-                    }
-                    break;
-            }
-            // delete, line 137
-            slice_del();
-            // call undouble, line 138
-            if (!r_undouble())
-            {
-                return false;
-            }
-            return true;
-        }
-
-        private boolean r_plural() {
-            int among_var;
-            // (, line 141
-            // [, line 142
-            ket = cursor;
-            // substring, line 142
-            among_var = find_among_b(a_8, 7);
-            if (among_var == 0)
-            {
-                return false;
-            }
-            // ], line 142
-            bra = cursor;
-            // call R1, line 142
-            if (!r_R1())
-            {
-                return false;
-            }
-            switch(among_var) {
-                case 0:
-                    return false;
-                case 1:
-                    // (, line 143
-                    // <-, line 143
-                    slice_from("a");
-                    break;
-                case 2:
-                    // (, line 144
-                    // <-, line 144
-                    slice_from("e");
-                    break;
-                case 3:
-                    // (, line 145
-                    // delete, line 145
-                    slice_del();
-                    break;
-                case 4:
-                    // (, line 146
-                    // delete, line 146
-                    slice_del();
-                    break;
-                case 5:
-                    // (, line 147
-                    // delete, line 147
-                    slice_del();
-                    break;
-                case 6:
-                    // (, line 148
-                    // delete, line 148
-                    slice_del();
-                    break;
-                case 7:
-                    // (, line 149
-                    // delete, line 149
-                    slice_del();
-                    break;
-            }
-            return true;
-        }
-
-        private boolean r_owned() {
-            int among_var;
-            // (, line 153
-            // [, line 154
-            ket = cursor;
-            // substring, line 154
-            among_var = find_among_b(a_9, 12);
-            if (among_var == 0)
-            {
-                return false;
-            }
-            // ], line 154
-            bra = cursor;
-            // call R1, line 154
-            if (!r_R1())
-            {
-                return false;
-            }
-            switch(among_var) {
-                case 0:
-                    return false;
-                case 1:
-                    // (, line 155
-                    // delete, line 155
-                    slice_del();
-                    break;
-                case 2:
-                    // (, line 156
-                    // <-, line 156
-                    slice_from("e");
-                    break;
-                case 3:
-                    // (, line 157
-                    // <-, line 157
-                    slice_from("a");
-                    break;
-                case 4:
-                    // (, line 158
-                    // delete, line 158
-                    slice_del();
-                    break;
-                case 5:
-                    // (, line 159
-                    // <-, line 159
-                    slice_from("e");
-                    break;
-                case 6:
-                    // (, line 160
-                    // <-, line 160
-                    slice_from("a");
-                    break;
-                case 7:
-                    // (, line 161
-                    // delete, line 161
-                    slice_del();
-                    break;
-                case 8:
-                    // (, line 162
-                    // <-, line 162
-                    slice_from("e");
-                    break;
-                case 9:
-                    // (, line 163
-                    // delete, line 163
-                    slice_del();
-                    break;
-            }
-            return true;
-        }
-
-        private boolean r_sing_owner() {
-            int among_var;
-            // (, line 167
-            // [, line 168
-            ket = cursor;
-            // substring, line 168
-            among_var = find_among_b(a_10, 31);
-            if (among_var == 0)
-            {
-                return false;
-            }
-            // ], line 168
-            bra = cursor;
-            // call R1, line 168
-            if (!r_R1())
-            {
-                return false;
-            }
-            switch(among_var) {
-                case 0:
-                    return false;
-                case 1:
-                    // (, line 169
-                    // delete, line 169
-                    slice_del();
-                    break;
-                case 2:
-                    // (, line 170
-                    // <-, line 170
-                    slice_from("a");
-                    break;
-                case 3:
-                    // (, line 171
-                    // <-, line 171
-                    slice_from("e");
-                    break;
-                case 4:
-                    // (, line 172
-                    // delete, line 172
-                    slice_del();
-                    break;
-                case 5:
-                    // (, line 173
-                    // <-, line 173
-                    slice_from("a");
-                    break;
-                case 6:
-                    // (, line 174
-                    // <-, line 174
-                    slice_from("e");
-                    break;
-                case 7:
-                    // (, line 175
-                    // delete, line 175
-                    slice_del();
-                    break;
-                case 8:
-                    // (, line 176
-                    // delete, line 176
-                    slice_del();
-                    break;
-                case 9:
-                    // (, line 177
-                    // delete, line 177
-                    slice_del();
-                    break;
-                case 10:
-                    // (, line 178
-                    // <-, line 178
-                    slice_from("a");
-                    break;
-                case 11:
-                    // (, line 179
-                    // <-, line 179
-                    slice_from("e");
-                    break;
-                case 12:
-                    // (, line 180
-                    // delete, line 180
-                    slice_del();
-                    break;
-                case 13:
-                    // (, line 181
-                    // delete, line 181
-                    slice_del();
-                    break;
-                case 14:
-                    // (, line 182
-                    // <-, line 182
-                    slice_from("a");
-                    break;
-                case 15:
-                    // (, line 183
-                    // <-, line 183
-                    slice_from("e");
-                    break;
-                case 16:
-                    // (, line 184
-                    // delete, line 184
-                    slice_del();
-                    break;
-                case 17:
-                    // (, line 185
-                    // delete, line 185
-                    slice_del();
-                    break;
-                case 18:
-                    // (, line 186
-                    // delete, line 186
-                    slice_del();
-                    break;
-                case 19:
-                    // (, line 187
-                    // <-, line 187
-                    slice_from("a");
-                    break;
-                case 20:
-                    // (, line 188
-                    // <-, line 188
-                    slice_from("e");
-                    break;
-            }
-            return true;
-        }
-
-        private boolean r_plur_owner() {
-            int among_var;
-            // (, line 192
-            // [, line 193
-            ket = cursor;
-            // substring, line 193
-            among_var = find_among_b(a_11, 42);
-            if (among_var == 0)
-            {
-                return false;
-            }
-            // ], line 193
-            bra = cursor;
-            // call R1, line 193
-            if (!r_R1())
-            {
-                return false;
-            }
-            switch(among_var) {
-                case 0:
-                    return false;
-                case 1:
-                    // (, line 194
-                    // delete, line 194
-                    slice_del();
-                    break;
-                case 2:
-                    // (, line 195
-                    // <-, line 195
-                    slice_from("a");
-                    break;
-                case 3:
-                    // (, line 196
-                    // <-, line 196
-                    slice_from("e");
-                    break;
-                case 4:
-                    // (, line 197
-                    // delete, line 197
-                    slice_del();
-                    break;
-                case 5:
-                    // (, line 198
-                    // delete, line 198
-                    slice_del();
-                    break;
-                case 6:
-                    // (, line 199
-                    // delete, line 199
-                    slice_del();
-                    break;
-                case 7:
-                    // (, line 200
-                    // <-, line 200
-                    slice_from("a");
-                    break;
-                case 8:
-                    // (, line 201
-                    // <-, line 201
-                    slice_from("e");
-                    break;
-                case 9:
-                    // (, line 202
-                    // delete, line 202
-                    slice_del();
-                    break;
-                case 10:
-                    // (, line 203
-                    // delete, line 203
-                    slice_del();
-                    break;
-                case 11:
-                    // (, line 204
-                    // delete, line 204
-                    slice_del();
-                    break;
-                case 12:
-                    // (, line 205
-                    // <-, line 205
-                    slice_from("a");
-                    break;
-                case 13:
-                    // (, line 206
-                    // <-, line 206
-                    slice_from("e");
-                    break;
-                case 14:
-                    // (, line 207
-                    // delete, line 207
-                    slice_del();
-                    break;
-                case 15:
-                    // (, line 208
-                    // delete, line 208
-                    slice_del();
-                    break;
-                case 16:
-                    // (, line 209
-                    // delete, line 209
-                    slice_del();
-                    break;
-                case 17:
-                    // (, line 210
-                    // delete, line 210
-                    slice_del();
-                    break;
-                case 18:
-                    // (, line 211
-                    // <-, line 211
-                    slice_from("a");
-                    break;
-                case 19:
-                    // (, line 212
-                    // <-, line 212
-                    slice_from("e");
-                    break;
-                case 20:
-                    // (, line 214
-                    // delete, line 214
-                    slice_del();
-                    break;
-                case 21:
-                    // (, line 215
-                    // delete, line 215
-                    slice_del();
-                    break;
-                case 22:
-                    // (, line 216
-                    // <-, line 216
-                    slice_from("a");
-                    break;
-                case 23:
-                    // (, line 217
-                    // <-, line 217
-                    slice_from("e");
-                    break;
-                case 24:
-                    // (, line 218
-                    // delete, line 218
-                    slice_del();
-                    break;
-                case 25:
-                    // (, line 219
-                    // delete, line 219
-                    slice_del();
-                    break;
-                case 26:
-                    // (, line 220
-                    // delete, line 220
-                    slice_del();
-                    break;
-                case 27:
-                    // (, line 221
-                    // <-, line 221
-                    slice_from("a");
-                    break;
-                case 28:
-                    // (, line 222
-                    // <-, line 222
-                    slice_from("e");
-                    break;
-                case 29:
-                    // (, line 223
-                    // delete, line 223
-                    slice_del();
-                    break;
-            }
-            return true;
-        }
-
-        public boolean stem() {
-            int v_1;
-            int v_2;
-            int v_3;
-            int v_4;
-            int v_5;
-            int v_6;
-            int v_7;
-            int v_8;
-            int v_9;
-            int v_10;
-            // (, line 228
-            // do, line 229
-            v_1 = cursor;
-            lab0: do {
-                // call mark_regions, line 229
-                if (!r_mark_regions())
-                {
-                    break lab0;
-                }
-            } while (false);
-            cursor = v_1;
-            // backwards, line 230
-            limit_backward = cursor; cursor = limit;
-            // (, line 230
-            // do, line 231
-            v_2 = limit - cursor;
-            lab1: do {
-                // call instrum, line 231
-                if (!r_instrum())
-                {
-                    break lab1;
-                }
-            } while (false);
-            cursor = limit - v_2;
-            // do, line 232
-            v_3 = limit - cursor;
-            lab2: do {
-                // call case, line 232
-                if (!r_case())
-                {
-                    break lab2;
-                }
-            } while (false);
-            cursor = limit - v_3;
-            // do, line 233
-            v_4 = limit - cursor;
-            lab3: do {
-                // call case_special, line 233
-                if (!r_case_special())
-                {
-                    break lab3;
-                }
-            } while (false);
-            cursor = limit - v_4;
-            // do, line 234
-            v_5 = limit - cursor;
-            lab4: do {
-                // call case_other, line 234
-                if (!r_case_other())
-                {
-                    break lab4;
-                }
-            } while (false);
-            cursor = limit - v_5;
-            // do, line 235
-            v_6 = limit - cursor;
-            lab5: do {
-                // call factive, line 235
-                if (!r_factive())
-                {
-                    break lab5;
-                }
-            } while (false);
-            cursor = limit - v_6;
-            // do, line 236
-            v_7 = limit - cursor;
-            lab6: do {
-                // call owned, line 236
-                if (!r_owned())
-                {
-                    break lab6;
-                }
-            } while (false);
-            cursor = limit - v_7;
-            // do, line 237
-            v_8 = limit - cursor;
-            lab7: do {
-                // call sing_owner, line 237
-                if (!r_sing_owner())
-                {
-                    break lab7;
-                }
-            } while (false);
-            cursor = limit - v_8;
-            // do, line 238
-            v_9 = limit - cursor;
-            lab8: do {
-                // call plur_owner, line 238
-                if (!r_plur_owner())
-                {
-                    break lab8;
-                }
-            } while (false);
-            cursor = limit - v_9;
-            // do, line 239
-            v_10 = limit - cursor;
-            lab9: do {
-                // call plural, line 239
-                if (!r_plural())
-                {
-                    break lab9;
-                }
-            } while (false);
-            cursor = limit - v_10;
-            cursor = limit_backward;            return true;
-        }
-
-}
-
Index: contrib/snowball/src/java/org/tartarus/snowball/ext/ItalianStemmer.java
===================================================================
--- contrib/snowball/src/java/org/tartarus/snowball/ext/ItalianStemmer.java	(revision 916567)
+++ contrib/snowball/src/java/org/tartarus/snowball/ext/ItalianStemmer.java	(working copy)
@@ -1,1180 +0,0 @@
-// This file was generated automatically by the Snowball to Java compiler
-
-package org.tartarus.snowball.ext;
-import org.tartarus.snowball.SnowballProgram;
-import org.tartarus.snowball.Among;
-
-/**
- * Generated class implementing code defined by a snowball script.
- */
-public class ItalianStemmer extends SnowballProgram {
-
-        private Among a_0[] = {
-            new Among ( "", -1, 7, "", this),
-            new Among ( "qu", 0, 6, "", this),
-            new Among ( "\u00E1", 0, 1, "", this),
-            new Among ( "\u00E9", 0, 2, "", this),
-            new Among ( "\u00ED", 0, 3, "", this),
-            new Among ( "\u00F3", 0, 4, "", this),
-            new Among ( "\u00FA", 0, 5, "", this)
-        };
-
-        private Among a_1[] = {
-            new Among ( "", -1, 3, "", this),
-            new Among ( "I", 0, 1, "", this),
-            new Among ( "U", 0, 2, "", this)
-        };
-
-        private Among a_2[] = {
-            new Among ( "la", -1, -1, "", this),
-            new Among ( "cela", 0, -1, "", this),
-            new Among ( "gliela", 0, -1, "", this),
-            new Among ( "mela", 0, -1, "", this),
-            new Among ( "tela", 0, -1, "", this),
-            new Among ( "vela", 0, -1, "", this),
-            new Among ( "le", -1, -1, "", this),
-            new Among ( "cele", 6, -1, "", this),
-            new Among ( "gliele", 6, -1, "", this),
-            new Among ( "mele", 6, -1, "", this),
-            new Among ( "tele", 6, -1, "", this),
-            new Among ( "vele", 6, -1, "", this),
-            new Among ( "ne", -1, -1, "", this),
-            new Among ( "cene", 12, -1, "", this),
-            new Among ( "gliene", 12, -1, "", this),
-            new Among ( "mene", 12, -1, "", this),
-            new Among ( "sene", 12, -1, "", this),
-            new Among ( "tene", 12, -1, "", this),
-            new Among ( "vene", 12, -1, "", this),
-            new Among ( "ci", -1, -1, "", this),
-            new Among ( "li", -1, -1, "", this),
-            new Among ( "celi", 20, -1, "", this),
-            new Among ( "glieli", 20, -1, "", this),
-            new Among ( "meli", 20, -1, "", this),
-            new Among ( "teli", 20, -1, "", this),
-            new Among ( "veli", 20, -1, "", this),
-            new Among ( "gli", 20, -1, "", this),
-            new Among ( "mi", -1, -1, "", this),
-            new Among ( "si", -1, -1, "", this),
-            new Among ( "ti", -1, -1, "", this),
-            new Among ( "vi", -1, -1, "", this),
-            new Among ( "lo", -1, -1, "", this),
-            new Among ( "celo", 31, -1, "", this),
-            new Among ( "glielo", 31, -1, "", this),
-            new Among ( "melo", 31, -1, "", this),
-            new Among ( "telo", 31, -1, "", this),
-            new Among ( "velo", 31, -1, "", this)
-        };
-
-        private Among a_3[] = {
-            new Among ( "ando", -1, 1, "", this),
-            new Among ( "endo", -1, 1, "", this),
-            new Among ( "ar", -1, 2, "", this),
-            new Among ( "er", -1, 2, "", this),
-            new Among ( "ir", -1, 2, "", this)
-        };
-
-        private Among a_4[] = {
-            new Among ( "ic", -1, -1, "", this),
-            new Among ( "abil", -1, -1, "", this),
-            new Among ( "os", -1, -1, "", this),
-            new Among ( "iv", -1, 1, "", this)
-        };
-
-        private Among a_5[] = {
-            new Among ( "ic", -1, 1, "", this),
-            new Among ( "abil", -1, 1, "", this),
-            new Among ( "iv", -1, 1, "", this)
-        };
-
-        private Among a_6[] = {
-            new Among ( "ica", -1, 1, "", this),
-            new Among ( "logia", -1, 3, "", this),
-            new Among ( "osa", -1, 1, "", this),
-            new Among ( "ista", -1, 1, "", this),
-            new Among ( "iva", -1, 9, "", this),
-            new Among ( "anza", -1, 1, "", this),
-            new Among ( "enza", -1, 5, "", this),
-            new Among ( "ice", -1, 1, "", this),
-            new Among ( "atrice", 7, 1, "", this),
-            new Among ( "iche", -1, 1, "", this),
-            new Among ( "logie", -1, 3, "", this),
-            new Among ( "abile", -1, 1, "", this),
-            new Among ( "ibile", -1, 1, "", this),
-            new Among ( "usione", -1, 4, "", this),
-            new Among ( "azione", -1, 2, "", this),
-            new Among ( "uzione", -1, 4, "", this),
-            new Among ( "atore", -1, 2, "", this),
-            new Among ( "ose", -1, 1, "", this),
-            new Among ( "ante", -1, 1, "", this),
-            new Among ( "mente", -1, 1, "", this),
-            new Among ( "amente", 19, 7, "", this),
-            new Among ( "iste", -1, 1, "", this),
-            new Among ( "ive", -1, 9, "", this),
-            new Among ( "anze", -1, 1, "", this),
-            new Among ( "enze", -1, 5, "", this),
-            new Among ( "ici", -1, 1, "", this),
-            new Among ( "atrici", 25, 1, "", this),
-            new Among ( "ichi", -1, 1, "", this),
-            new Among ( "abili", -1, 1, "", this),
-            new Among ( "ibili", -1, 1, "", this),
-            new Among ( "ismi", -1, 1, "", this),
-            new Among ( "usioni", -1, 4, "", this),
-            new Among ( "azioni", -1, 2, "", this),
-            new Among ( "uzioni", -1, 4, "", this),
-            new Among ( "atori", -1, 2, "", this),
-            new Among ( "osi", -1, 1, "", this),
-            new Among ( "anti", -1, 1, "", this),
-            new Among ( "amenti", -1, 6, "", this),
-            new Among ( "imenti", -1, 6, "", this),
-            new Among ( "isti", -1, 1, "", this),
-            new Among ( "ivi", -1, 9, "", this),
-            new Among ( "ico", -1, 1, "", this),
-            new Among ( "ismo", -1, 1, "", this),
-            new Among ( "oso", -1, 1, "", this),
-            new Among ( "amento", -1, 6, "", this),
-            new Among ( "imento", -1, 6, "", this),
-            new Among ( "ivo", -1, 9, "", this),
-            new Among ( "it\u00E0", -1, 8, "", this),
-            new Among ( "ist\u00E0", -1, 1, "", this),
-            new Among ( "ist\u00E8", -1, 1, "", this),
-            new Among ( "ist\u00EC", -1, 1, "", this)
-        };
-
-        private Among a_7[] = {
-            new Among ( "isca", -1, 1, "", this),
-            new Among ( "enda", -1, 1, "", this),
-            new Among ( "ata", -1, 1, "", this),
-            new Among ( "ita", -1, 1, "", this),
-            new Among ( "uta", -1, 1, "", this),
-            new Among ( "ava", -1, 1, "", this),
-            new Among ( "eva", -1, 1, "", this),
-            new Among ( "iva", -1, 1, "", this),
-            new Among ( "erebbe", -1, 1, "", this),
-            new Among ( "irebbe", -1, 1, "", this),
-            new Among ( "isce", -1, 1, "", this),
-            new Among ( "ende", -1, 1, "", this),
-            new Among ( "are", -1, 1, "", this),
-            new Among ( "ere", -1, 1, "", this),
-            new Among ( "ire", -1, 1, "", this),
-            new Among ( "asse", -1, 1, "", this),
-            new Among ( "ate", -1, 1, "", this),
-            new Among ( "avate", 16, 1, "", this),
-            new Among ( "evate", 16, 1, "", this),
-            new Among ( "ivate", 16, 1, "", this),
-            new Among ( "ete", -1, 1, "", this),
-            new Among ( "erete", 20, 1, "", this),
-            new Among ( "irete", 20, 1, "", this),
-            new Among ( "ite", -1, 1, "", this),
-            new Among ( "ereste", -1, 1, "", this),
-            new Among ( "ireste", -1, 1, "", this),
-            new Among ( "ute", -1, 1, "", this),
-            new Among ( "erai", -1, 1, "", this),
-            new Among ( "irai", -1, 1, "", this),
-            new Among ( "isci", -1, 1, "", this),
-            new Among ( "endi", -1, 1, "", this),
-            new Among ( "erei", -1, 1, "", this),
-            new Among ( "irei", -1, 1, "", this),
-            new Among ( "assi", -1, 1, "", this),
-            new Among ( "ati", -1, 1, "", this),
-            new Among ( "iti", -1, 1, "", this),
-            new Among ( "eresti", -1, 1, "", this),
-            new Among ( "iresti", -1, 1, "", this),
-            new Among ( "uti", -1, 1, "", this),
-            new Among ( "avi", -1, 1, "", this),
-            new Among ( "evi", -1, 1, "", this),
-            new Among ( "ivi", -1, 1, "", this),
-            new Among ( "isco", -1, 1, "", this),
-            new Among ( "ando", -1, 1, "", this),
-            new Among ( "endo", -1, 1, "", this),
-            new Among ( "Yamo", -1, 1, "", this),
-            new Among ( "iamo", -1, 1, "", this),
-            new Among ( "avamo", -1, 1, "", this),
-            new Among ( "evamo", -1, 1, "", this),
-            new Among ( "ivamo", -1, 1, "", this),
-            new Among ( "eremo", -1, 1, "", this),
-            new Among ( "iremo", -1, 1, "", this),
-            new Among ( "assimo", -1, 1, "", this),
-            new Among ( "ammo", -1, 1, "", this),
-            new Among ( "emmo", -1, 1, "", this),
-            new Among ( "eremmo", 54, 1, "", this),
-            new Among ( "iremmo", 54, 1, "", this),
-            new Among ( "immo", -1, 1, "", this),
-            new Among ( "ano", -1, 1, "", this),
-            new Among ( "iscano", 58, 1, "", this),
-            new Among ( "avano", 58, 1, "", this),
-            new Among ( "evano", 58, 1, "", this),
-            new Among ( "ivano", 58, 1, "", this),
-            new Among ( "eranno", -1, 1, "", this),
-            new Among ( "iranno", -1, 1, "", this),
-            new Among ( "ono", -1, 1, "", this),
-            new Among ( "iscono", 65, 1, "", this),
-            new Among ( "arono", 65, 1, "", this),
-            new Among ( "erono", 65, 1, "", this),
-            new Among ( "irono", 65, 1, "", this),
-            new Among ( "erebbero", -1, 1, "", this),
-            new Among ( "irebbero", -1, 1, "", this),
-            new Among ( "assero", -1, 1, "", this),
-            new Among ( "essero", -1, 1, "", this),
-            new Among ( "issero", -1, 1, "", this),
-            new Among ( "ato", -1, 1, "", this),
-            new Among ( "ito", -1, 1, "", this),
-            new Among ( "uto", -1, 1, "", this),
-            new Among ( "avo", -1, 1, "", this),
-            new Among ( "evo", -1, 1, "", this),
-            new Among ( "ivo", -1, 1, "", this),
-            new Among ( "ar", -1, 1, "", this),
-            new Among ( "ir", -1, 1, "", this),
-            new Among ( "er\u00E0", -1, 1, "", this),
-            new Among ( "ir\u00E0", -1, 1, "", this),
-            new Among ( "er\u00F2", -1, 1, "", this),
-            new Among ( "ir\u00F2", -1, 1, "", this)
-        };
-
-        private static final char g_v[] = {17, 65, 16, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 128, 128, 8, 2, 1 };
-
-        private static final char g_AEIO[] = {17, 65, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 128, 128, 8, 2 };
-
-        private static final char g_CG[] = {17 };
-
-        private int I_p2;
-        private int I_p1;
-        private int I_pV;
-
-        private void copy_from(ItalianStemmer other) {
-            I_p2 = other.I_p2;
-            I_p1 = other.I_p1;
-            I_pV = other.I_pV;
-            super.copy_from(other);
-        }
-
-        private boolean r_prelude() {
-            int among_var;
-            int v_1;
-            int v_2;
-            int v_3;
-            int v_4;
-            int v_5;
-            // (, line 34
-            // test, line 35
-            v_1 = cursor;
-            // repeat, line 35
-            replab0: while(true)
-            {
-                v_2 = cursor;
-                lab1: do {
-                    // (, line 35
-                    // [, line 36
-                    bra = cursor;
-                    // substring, line 36
-                    among_var = find_among(a_0, 7);
-                    if (among_var == 0)
-                    {
-                        break lab1;
-                    }
-                    // ], line 36
-                    ket = cursor;
-                    switch(among_var) {
-                        case 0:
-                            break lab1;
-                        case 1:
-                            // (, line 37
-                            // <-, line 37
-                            slice_from("\u00E0");
-                            break;
-                        case 2:
-                            // (, line 38
-                            // <-, line 38
-                            slice_from("\u00E8");
-                            break;
-                        case 3:
-                            // (, line 39
-                            // <-, line 39
-                            slice_from("\u00EC");
-                            break;
-                        case 4:
-                            // (, line 40
-                            // <-, line 40
-                            slice_from("\u00F2");
-                            break;
-                        case 5:
-                            // (, line 41
-                            // <-, line 41
-                            slice_from("\u00F9");
-                            break;
-                        case 6:
-                            // (, line 42
-                            // <-, line 42
-                            slice_from("qU");
-                            break;
-                        case 7:
-                            // (, line 43
-                            // next, line 43
-                            if (cursor >= limit)
-                            {
-                                break lab1;
-                            }
-                            cursor++;
-                            break;
-                    }
-                    continue replab0;
-                } while (false);
-                cursor = v_2;
-                break replab0;
-            }
-            cursor = v_1;
-            // repeat, line 46
-            replab2: while(true)
-            {
-                v_3 = cursor;
-                lab3: do {
-                    // goto, line 46
-                    golab4: while(true)
-                    {
-                        v_4 = cursor;
-                        lab5: do {
-                            // (, line 46
-                            if (!(in_grouping(g_v, 97, 249)))
-                            {
-                                break lab5;
-                            }
-                            // [, line 47
-                            bra = cursor;
-                            // or, line 47
-                            lab6: do {
-                                v_5 = cursor;
-                                lab7: do {
-                                    // (, line 47
-                                    // literal, line 47
-                                    if (!(eq_s(1, "u")))
-                                    {
-                                        break lab7;
-                                    }
-                                    // ], line 47
-                                    ket = cursor;
-                                    if (!(in_grouping(g_v, 97, 249)))
-                                    {
-                                        break lab7;
-                                    }
-                                    // <-, line 47
-                                    slice_from("U");
-                                    break lab6;
-                                } while (false);
-                                cursor = v_5;
-                                // (, line 48
-                                // literal, line 48
-                                if (!(eq_s(1, "i")))
-                                {
-                                    break lab5;
-                                }
-                                // ], line 48
-                                ket = cursor;
-                                if (!(in_grouping(g_v, 97, 249)))
-                                {
-                                    break lab5;
-                                }
-                                // <-, line 48
-                                slice_from("I");
-                            } while (false);
-                            cursor = v_4;
-                            break golab4;
-                        } while (false);
-                        cursor = v_4;
-                        if (cursor >= limit)
-                        {
-                            break lab3;
-                        }
-                        cursor++;
-                    }
-                    continue replab2;
-                } while (false);
-                cursor = v_3;
-                break replab2;
-            }
-            return true;
-        }
-
-        private boolean r_mark_regions() {
-            int v_1;
-            int v_2;
-            int v_3;
-            int v_6;
-            int v_8;
-            // (, line 52
-            I_pV = limit;
-            I_p1 = limit;
-            I_p2 = limit;
-            // do, line 58
-            v_1 = cursor;
-            lab0: do {
-                // (, line 58
-                // or, line 60
-                lab1: do {
-                    v_2 = cursor;
-                    lab2: do {
-                        // (, line 59
-                        if (!(in_grouping(g_v, 97, 249)))
-                        {
-                            break lab2;
-                        }
-                        // or, line 59
-                        lab3: do {
-                            v_3 = cursor;
-                            lab4: do {
-                                // (, line 59
-                                if (!(out_grouping(g_v, 97, 249)))
-                                {
-                                    break lab4;
-                                }
-                                // gopast, line 59
-                                golab5: while(true)
-                                {
-                                    lab6: do {
-                                        if (!(in_grouping(g_v, 97, 249)))
-                                        {
-                                            break lab6;
-                                        }
-                                        break golab5;
-                                    } while (false);
-                                    if (cursor >= limit)
-                                    {
-                                        break lab4;
-                                    }
-                                    cursor++;
-                                }
-                                break lab3;
-                            } while (false);
-                            cursor = v_3;
-                            // (, line 59
-                            if (!(in_grouping(g_v, 97, 249)))
-                            {
-                                break lab2;
-                            }
-                            // gopast, line 59
-                            golab7: while(true)
-                            {
-                                lab8: do {
-                                    if (!(out_grouping(g_v, 97, 249)))
-                                    {
-                                        break lab8;
-                                    }
-                                    break golab7;
-                                } while (false);
-                                if (cursor >= limit)
-                                {
-                                    break lab2;
-                                }
-                                cursor++;
-                            }
-                        } while (false);
-                        break lab1;
-                    } while (false);
-                    cursor = v_2;
-                    // (, line 61
-                    if (!(out_grouping(g_v, 97, 249)))
-                    {
-                        break lab0;
-                    }
-                    // or, line 61
-                    lab9: do {
-                        v_6 = cursor;
-                        lab10: do {
-                            // (, line 61
-                            if (!(out_grouping(g_v, 97, 249)))
-                            {
-                                break lab10;
-                            }
-                            // gopast, line 61
-                            golab11: while(true)
-                            {
-                                lab12: do {
-                                    if (!(in_grouping(g_v, 97, 249)))
-                                    {
-                                        break lab12;
-                                    }
-                                    break golab11;
-                                } while (false);
-                                if (cursor >= limit)
-                                {
-                                    break lab10;
-                                }
-                                cursor++;
-                            }
-                            break lab9;
-                        } while (false);
-                        cursor = v_6;
-                        // (, line 61
-                        if (!(in_grouping(g_v, 97, 249)))
-                        {
-                            break lab0;
-                        }
-                        // next, line 61
-                        if (cursor >= limit)
-                        {
-                            break lab0;
-                        }
-                        cursor++;
-                    } while (false);
-                } while (false);
-                // setmark pV, line 62
-                I_pV = cursor;
-            } while (false);
-            cursor = v_1;
-            // do, line 64
-            v_8 = cursor;
-            lab13: do {
-                // (, line 64
-                // gopast, line 65
-                golab14: while(true)
-                {
-                    lab15: do {
-                        if (!(in_grouping(g_v, 97, 249)))
-                        {
-                            break lab15;
-                        }
-                        break golab14;
-                    } while (false);
-                    if (cursor >= limit)
-                    {
-                        break lab13;
-                    }
-                    cursor++;
-                }
-                // gopast, line 65
-                golab16: while(true)
-                {
-                    lab17: do {
-                        if (!(out_grouping(g_v, 97, 249)))
-                        {
-                            break lab17;
-                        }
-                        break golab16;
-                    } while (false);
-                    if (cursor >= limit)
-                    {
-                        break lab13;
-                    }
-                    cursor++;
-                }
-                // setmark p1, line 65
-                I_p1 = cursor;
-                // gopast, line 66
-                golab18: while(true)
-                {
-                    lab19: do {
-                        if (!(in_grouping(g_v, 97, 249)))
-                        {
-                            break lab19;
-                        }
-                        break golab18;
-                    } while (false);
-                    if (cursor >= limit)
-                    {
-                        break lab13;
-                    }
-                    cursor++;
-                }
-                // gopast, line 66
-                golab20: while(true)
-                {
-                    lab21: do {
-                        if (!(out_grouping(g_v, 97, 249)))
-                        {
-                            break lab21;
-                        }
-                        break golab20;
-                    } while (false);
-                    if (cursor >= limit)
-                    {
-                        break lab13;
-                    }
-                    cursor++;
-                }
-                // setmark p2, line 66
-                I_p2 = cursor;
-            } while (false);
-            cursor = v_8;
-            return true;
-        }
-
-        private boolean r_postlude() {
-            int among_var;
-            int v_1;
-            // repeat, line 70
-            replab0: while(true)
-            {
-                v_1 = cursor;
-                lab1: do {
-                    // (, line 70
-                    // [, line 72
-                    bra = cursor;
-                    // substring, line 72
-                    among_var = find_among(a_1, 3);
-                    if (among_var == 0)
-                    {
-                        break lab1;
-                    }
-                    // ], line 72
-                    ket = cursor;
-                    switch(among_var) {
-                        case 0:
-                            break lab1;
-                        case 1:
-                            // (, line 73
-                            // <-, line 73
-                            slice_from("i");
-                            break;
-                        case 2:
-                            // (, line 74
-                            // <-, line 74
-                            slice_from("u");
-                            break;
-                        case 3:
-                            // (, line 75
-                            // next, line 75
-                            if (cursor >= limit)
-                            {
-                                break lab1;
-                            }
-                            cursor++;
-                            break;
-                    }
-                    continue replab0;
-                } while (false);
-                cursor = v_1;
-                break replab0;
-            }
-            return true;
-        }
-
-        private boolean r_RV() {
-            if (!(I_pV <= cursor))
-            {
-                return false;
-            }
-            return true;
-        }
-
-        private boolean r_R1() {
-            if (!(I_p1 <= cursor))
-            {
-                return false;
-            }
-            return true;
-        }
-
-        private boolean r_R2() {
-            if (!(I_p2 <= cursor))
-            {
-                return false;
-            }
-            return true;
-        }
-
-        private boolean r_attached_pronoun() {
-            int among_var;
-            // (, line 86
-            // [, line 87
-            ket = cursor;
-            // substring, line 87
-            if (find_among_b(a_2, 37) == 0)
-            {
-                return false;
-            }
-            // ], line 87
-            bra = cursor;
-            // among, line 97
-            among_var = find_among_b(a_3, 5);
-            if (among_var == 0)
-            {
-                return false;
-            }
-            // (, line 97
-            // call RV, line 97
-            if (!r_RV())
-            {
-                return false;
-            }
-            switch(among_var) {
-                case 0:
-                    return false;
-                case 1:
-                    // (, line 98
-                    // delete, line 98
-                    slice_del();
-                    break;
-                case 2:
-                    // (, line 99
-                    // <-, line 99
-                    slice_from("e");
-                    break;
-            }
-            return true;
-        }
-
-        private boolean r_standard_suffix() {
-            int among_var;
-            int v_1;
-            int v_2;
-            int v_3;
-            int v_4;
-            // (, line 103
-            // [, line 104
-            ket = cursor;
-            // substring, line 104
-            among_var = find_among_b(a_6, 51);
-            if (among_var == 0)
-            {
-                return false;
-            }
-            // ], line 104
-            bra = cursor;
-            switch(among_var) {
-                case 0:
-                    return false;
-                case 1:
-                    // (, line 111
-                    // call R2, line 111
-                    if (!r_R2())
-                    {
-                        return false;
-                    }
-                    // delete, line 111
-                    slice_del();
-                    break;
-                case 2:
-                    // (, line 113
-                    // call R2, line 113
-                    if (!r_R2())
-                    {
-                        return false;
-                    }
-                    // delete, line 113
-                    slice_del();
-                    // try, line 114
-                    v_1 = limit - cursor;
-                    lab0: do {
-                        // (, line 114
-                        // [, line 114
-                        ket = cursor;
-                        // literal, line 114
-                        if (!(eq_s_b(2, "ic")))
-                        {
-                            cursor = limit - v_1;
-                            break lab0;
-                        }
-                        // ], line 114
-                        bra = cursor;
-                        // call R2, line 114
-                        if (!r_R2())
-                        {
-                            cursor = limit - v_1;
-                            break lab0;
-                        }
-                        // delete, line 114
-                        slice_del();
-                    } while (false);
-                    break;
-                case 3:
-                    // (, line 117
-                    // call R2, line 117
-                    if (!r_R2())
-                    {
-                        return false;
-                    }
-                    // <-, line 117
-                    slice_from("log");
-                    break;
-                case 4:
-                    // (, line 119
-                    // call R2, line 119
-                    if (!r_R2())
-                    {
-                        return false;
-                    }
-                    // <-, line 119
-                    slice_from("u");
-                    break;
-                case 5:
-                    // (, line 121
-                    // call R2, line 121
-                    if (!r_R2())
-                    {
-                        return false;
-                    }
-                    // <-, line 121
-                    slice_from("ente");
-                    break;
-                case 6:
-                    // (, line 123
-                    // call RV, line 123
-                    if (!r_RV())
-                    {
-                        return false;
-                    }
-                    // delete, line 123
-                    slice_del();
-                    break;
-                case 7:
-                    // (, line 124
-                    // call R1, line 125
-                    if (!r_R1())
-                    {
-                        return false;
-                    }
-                    // delete, line 125
-                    slice_del();
-                    // try, line 126
-                    v_2 = limit - cursor;
-                    lab1: do {
-                        // (, line 126
-                        // [, line 127
-                        ket = cursor;
-                        // substring, line 127
-                        among_var = find_among_b(a_4, 4);
-                        if (among_var == 0)
-                        {
-                            cursor = limit - v_2;
-                            break lab1;
-                        }
-                        // ], line 127
-                        bra = cursor;
-                        // call R2, line 127
-                        if (!r_R2())
-                        {
-                            cursor = limit - v_2;
-                            break lab1;
-                        }
-                        // delete, line 127
-                        slice_del();
-                        switch(among_var) {
-                            case 0:
-                                cursor = limit - v_2;
-                                break lab1;
-                            case 1:
-                                // (, line 128
-                                // [, line 128
-                                ket = cursor;
-                                // literal, line 128
-                                if (!(eq_s_b(2, "at")))
-                                {
-                                    cursor = limit - v_2;
-                                    break lab1;
-                                }
-                                // ], line 128
-                                bra = cursor;
-                                // call R2, line 128
-                                if (!r_R2())
-                                {
-                                    cursor = limit - v_2;
-                                    break lab1;
-                                }
-                                // delete, line 128
-                                slice_del();
-                                break;
-                        }
-                    } while (false);
-                    break;
-                case 8:
-                    // (, line 133
-                    // call R2, line 134
-                    if (!r_R2())
-                    {
-                        return false;
-                    }
-                    // delete, line 134
-                    slice_del();
-                    // try, line 135
-                    v_3 = limit - cursor;
-                    lab2: do {
-                        // (, line 135
-                        // [, line 136
-                        ket = cursor;
-                        // substring, line 136
-                        among_var = find_among_b(a_5, 3);
-                        if (among_var == 0)
-                        {
-                            cursor = limit - v_3;
-                            break lab2;
-                        }
-                        // ], line 136
-                        bra = cursor;
-                        switch(among_var) {
-                            case 0:
-                                cursor = limit - v_3;
-                                break lab2;
-                            case 1:
-                                // (, line 137
-                                // call R2, line 137
-                                if (!r_R2())
-                                {
-                                    cursor = limit - v_3;
-                                    break lab2;
-                                }
-                                // delete, line 137
-                                slice_del();
-                                break;
-                        }
-                    } while (false);
-                    break;
-                case 9:
-                    // (, line 141
-                    // call R2, line 142
-                    if (!r_R2())
-                    {
-                        return false;
-                    }
-                    // delete, line 142
-                    slice_del();
-                    // try, line 143
-                    v_4 = limit - cursor;
-                    lab3: do {
-                        // (, line 143
-                        // [, line 143
-                        ket = cursor;
-                        // literal, line 143
-                        if (!(eq_s_b(2, "at")))
-                        {
-                            cursor = limit - v_4;
-                            break lab3;
-                        }
-                        // ], line 143
-                        bra = cursor;
-                        // call R2, line 143
-                        if (!r_R2())
-                        {
-                            cursor = limit - v_4;
-                            break lab3;
-                        }
-                        // delete, line 143
-                        slice_del();
-                        // [, line 143
-                        ket = cursor;
-                        // literal, line 143
-                        if (!(eq_s_b(2, "ic")))
-                        {
-                            cursor = limit - v_4;
-                            break lab3;
-                        }
-                        // ], line 143
-                        bra = cursor;
-                        // call R2, line 143
-                        if (!r_R2())
-                        {
-                            cursor = limit - v_4;
-                            break lab3;
-                        }
-                        // delete, line 143
-                        slice_del();
-                    } while (false);
-                    break;
-            }
-            return true;
-        }
-
-        private boolean r_verb_suffix() {
-            int among_var;
-            int v_1;
-            int v_2;
-            // setlimit, line 148
-            v_1 = limit - cursor;
-            // tomark, line 148
-            if (cursor < I_pV)
-            {
-                return false;
-            }
-            cursor = I_pV;
-            v_2 = limit_backward;
-            limit_backward = cursor;
-            cursor = limit - v_1;
-            // (, line 148
-            // [, line 149
-            ket = cursor;
-            // substring, line 149
-            among_var = find_among_b(a_7, 87);
-            if (among_var == 0)
-            {
-                limit_backward = v_2;
-                return false;
-            }
-            // ], line 149
-            bra = cursor;
-            switch(among_var) {
-                case 0:
-                    limit_backward = v_2;
-                    return false;
-                case 1:
-                    // (, line 163
-                    // delete, line 163
-                    slice_del();
-                    break;
-            }
-            limit_backward = v_2;
-            return true;
-        }
-
-        private boolean r_vowel_suffix() {
-            int v_1;
-            int v_2;
-            // (, line 170
-            // try, line 171
-            v_1 = limit - cursor;
-            lab0: do {
-                // (, line 171
-                // [, line 172
-                ket = cursor;
-                if (!(in_grouping_b(g_AEIO, 97, 242)))
-                {
-                    cursor = limit - v_1;
-                    break lab0;
-                }
-                // ], line 172
-                bra = cursor;
-                // call RV, line 172
-                if (!r_RV())
-                {
-                    cursor = limit - v_1;
-                    break lab0;
-                }
-                // delete, line 172
-                slice_del();
-                // [, line 173
-                ket = cursor;
-                // literal, line 173
-                if (!(eq_s_b(1, "i")))
-                {
-                    cursor = limit - v_1;
-                    break lab0;
-                }
-                // ], line 173
-                bra = cursor;
-                // call RV, line 173
-                if (!r_RV())
-                {
-                    cursor = limit - v_1;
-                    break lab0;
-                }
-                // delete, line 173
-                slice_del();
-            } while (false);
-            // try, line 175
-            v_2 = limit - cursor;
-            lab1: do {
-                // (, line 175
-                // [, line 176
-                ket = cursor;
-                // literal, line 176
-                if (!(eq_s_b(1, "h")))
-                {
-                    cursor = limit - v_2;
-                    break lab1;
-                }
-                // ], line 176
-                bra = cursor;
-                if (!(in_grouping_b(g_CG, 99, 103)))
-                {
-                    cursor = limit - v_2;
-                    break lab1;
-                }
-                // call RV, line 176
-                if (!r_RV())
-                {
-                    cursor = limit - v_2;
-                    break lab1;
-                }
-                // delete, line 176
-                slice_del();
-            } while (false);
-            return true;
-        }
-
-        public boolean stem() {
-            int v_1;
-            int v_2;
-            int v_3;
-            int v_4;
-            int v_5;
-            int v_6;
-            int v_7;
-            // (, line 181
-            // do, line 182
-            v_1 = cursor;
-            lab0: do {
-                // call prelude, line 182
-                if (!r_prelude())
-                {
-                    break lab0;
-                }
-            } while (false);
-            cursor = v_1;
-            // do, line 183
-            v_2 = cursor;
-            lab1: do {
-                // call mark_regions, line 183
-                if (!r_mark_regions())
-                {
-                    break lab1;
-                }
-            } while (false);
-            cursor = v_2;
-            // backwards, line 184
-            limit_backward = cursor; cursor = limit;
-            // (, line 184
-            // do, line 185
-            v_3 = limit - cursor;
-            lab2: do {
-                // call attached_pronoun, line 185
-                if (!r_attached_pronoun())
-                {
-                    break lab2;
-                }
-            } while (false);
-            cursor = limit - v_3;
-            // do, line 186
-            v_4 = limit - cursor;
-            lab3: do {
-                // (, line 186
-                // or, line 186
-                lab4: do {
-                    v_5 = limit - cursor;
-                    lab5: do {
-                        // call standard_suffix, line 186
-                        if (!r_standard_suffix())
-                        {
-                            break lab5;
-                        }
-                        break lab4;
-                    } while (false);
-                    cursor = limit - v_5;
-                    // call verb_suffix, line 186
-                    if (!r_verb_suffix())
-                    {
-                        break lab3;
-                    }
-                } while (false);
-            } while (false);
-            cursor = limit - v_4;
-            // do, line 187
-            v_6 = limit - cursor;
-            lab6: do {
-                // call vowel_suffix, line 187
-                if (!r_vowel_suffix())
-                {
-                    break lab6;
-                }
-            } while (false);
-            cursor = limit - v_6;
-            cursor = limit_backward;            // do, line 189
-            v_7 = cursor;
-            lab7: do {
-                // call postlude, line 189
-                if (!r_postlude())
-                {
-                    break lab7;
-                }
-            } while (false);
-            cursor = v_7;
-            return true;
-        }
-
-}
-
Index: contrib/snowball/src/java/org/tartarus/snowball/ext/RussianStemmer.java
===================================================================
--- contrib/snowball/src/java/org/tartarus/snowball/ext/RussianStemmer.java	(revision 916567)
+++ contrib/snowball/src/java/org/tartarus/snowball/ext/RussianStemmer.java	(working copy)
@@ -1,727 +0,0 @@
-// This file was generated automatically by the Snowball to Java compiler
-
-package org.tartarus.snowball.ext;
-import org.tartarus.snowball.SnowballProgram;
-import org.tartarus.snowball.Among;
-
-/**
- * Generated class implementing code defined by a snowball script.
- */
-public class RussianStemmer extends SnowballProgram {
-
-        private Among a_0[] = {
-            new Among ( "\u0432", -1, 1, "", this),
-            new Among ( "\u0438\u0432", 0, 2, "", this),
-            new Among ( "\u044B\u0432", 0, 2, "", this),
-            new Among ( "\u0432\u0448\u0438", -1, 1, "", this),
-            new Among ( "\u0438\u0432\u0448\u0438", 3, 2, "", this),
-            new Among ( "\u044B\u0432\u0448\u0438", 3, 2, "", this),
-            new Among ( "\u0432\u0448\u0438\u0441\u044C", -1, 1, "", this),
-            new Among ( "\u0438\u0432\u0448\u0438\u0441\u044C", 6, 2, "", this),
-            new Among ( "\u044B\u0432\u0448\u0438\u0441\u044C", 6, 2, "", this)
-        };
-
-        private Among a_1[] = {
-            new Among ( "\u0435\u0435", -1, 1, "", this),
-            new Among ( "\u0438\u0435", -1, 1, "", this),
-            new Among ( "\u043E\u0435", -1, 1, "", this),
-            new Among ( "\u044B\u0435", -1, 1, "", this),
-            new Among ( "\u0438\u043C\u0438", -1, 1, "", this),
-            new Among ( "\u044B\u043C\u0438", -1, 1, "", this),
-            new Among ( "\u0435\u0439", -1, 1, "", this),
-            new Among ( "\u0438\u0439", -1, 1, "", this),
-            new Among ( "\u043E\u0439", -1, 1, "", this),
-            new Among ( "\u044B\u0439", -1, 1, "", this),
-            new Among ( "\u0435\u043C", -1, 1, "", this),
-            new Among ( "\u0438\u043C", -1, 1, "", this),
-            new Among ( "\u043E\u043C", -1, 1, "", this),
-            new Among ( "\u044B\u043C", -1, 1, "", this),
-            new Among ( "\u0435\u0433\u043E", -1, 1, "", this),
-            new Among ( "\u043E\u0433\u043E", -1, 1, "", this),
-            new Among ( "\u0435\u043C\u0443", -1, 1, "", this),
-            new Among ( "\u043E\u043C\u0443", -1, 1, "", this),
-            new Among ( "\u0438\u0445", -1, 1, "", this),
-            new Among ( "\u044B\u0445", -1, 1, "", this),
-            new Among ( "\u0435\u044E", -1, 1, "", this),
-            new Among ( "\u043E\u044E", -1, 1, "", this),
-            new Among ( "\u0443\u044E", -1, 1, "", this),
-            new Among ( "\u044E\u044E", -1, 1, "", this),
-            new Among ( "\u0430\u044F", -1, 1, "", this),
-            new Among ( "\u044F\u044F", -1, 1, "", this)
-        };
-
-        private Among a_2[] = {
-            new Among ( "\u0435\u043C", -1, 1, "", this),
-            new Among ( "\u043D\u043D", -1, 1, "", this),
-            new Among ( "\u0432\u0448", -1, 1, "", this),
-            new Among ( "\u0438\u0432\u0448", 2, 2, "", this),
-            new Among ( "\u044B\u0432\u0448", 2, 2, "", this),
-            new Among ( "\u0449", -1, 1, "", this),
-            new Among ( "\u044E\u0449", 5, 1, "", this),
-            new Among ( "\u0443\u044E\u0449", 6, 2, "", this)
-        };
-
-        private Among a_3[] = {
-            new Among ( "\u0441\u044C", -1, 1, "", this),
-            new Among ( "\u0441\u044F", -1, 1, "", this)
-        };
-
-        private Among a_4[] = {
-            new Among ( "\u043B\u0430", -1, 1, "", this),
-            new Among ( "\u0438\u043B\u0430", 0, 2, "", this),
-            new Among ( "\u044B\u043B\u0430", 0, 2, "", this),
-            new Among ( "\u043D\u0430", -1, 1, "", this),
-            new Among ( "\u0435\u043D\u0430", 3, 2, "", this),
-            new Among ( "\u0435\u0442\u0435", -1, 1, "", this),
-            new Among ( "\u0438\u0442\u0435", -1, 2, "", this),
-            new Among ( "\u0439\u0442\u0435", -1, 1, "", this),
-            new Among ( "\u0435\u0439\u0442\u0435", 7, 2, "", this),
-            new Among ( "\u0443\u0439\u0442\u0435", 7, 2, "", this),
-            new Among ( "\u043B\u0438", -1, 1, "", this),
-            new Among ( "\u0438\u043B\u0438", 10, 2, "", this),
-            new Among ( "\u044B\u043B\u0438", 10, 2, "", this),
-            new Among ( "\u0439", -1, 1, "", this),
-            new Among ( "\u0435\u0439", 13, 2, "", this),
-            new Among ( "\u0443\u0439", 13, 2, "", this),
-            new Among ( "\u043B", -1, 1, "", this),
-            new Among ( "\u0438\u043B", 16, 2, "", this),
-            new Among ( "\u044B\u043B", 16, 2, "", this),
-            new Among ( "\u0435\u043C", -1, 1, "", this),
-            new Among ( "\u0438\u043C", -1, 2, "", this),
-            new Among ( "\u044B\u043C", -1, 2, "", this),
-            new Among ( "\u043D", -1, 1, "", this),
-            new Among ( "\u0435\u043D", 22, 2, "", this),
-            new Among ( "\u043B\u043E", -1, 1, "", this),
-            new Among ( "\u0438\u043B\u043E", 24, 2, "", this),
-            new Among ( "\u044B\u043B\u043E", 24, 2, "", this),
-            new Among ( "\u043D\u043E", -1, 1, "", this),
-            new Among ( "\u0435\u043D\u043E", 27, 2, "", this),
-            new Among ( "\u043D\u043D\u043E", 27, 1, "", this),
-            new Among ( "\u0435\u0442", -1, 1, "", this),
-            new Among ( "\u0443\u0435\u0442", 30, 2, "", this),
-            new Among ( "\u0438\u0442", -1, 2, "", this),
-            new Among ( "\u044B\u0442", -1, 2, "", this),
-            new Among ( "\u044E\u0442", -1, 1, "", this),
-            new Among ( "\u0443\u044E\u0442", 34, 2, "", this),
-            new Among ( "\u044F\u0442", -1, 2, "", this),
-            new Among ( "\u043D\u044B", -1, 1, "", this),
-            new Among ( "\u0435\u043D\u044B", 37, 2, "", this),
-            new Among ( "\u0442\u044C", -1, 1, "", this),
-            new Among ( "\u0438\u0442\u044C", 39, 2, "", this),
-            new Among ( "\u044B\u0442\u044C", 39, 2, "", this),
-            new Among ( "\u0435\u0448\u044C", -1, 1, "", this),
-            new Among ( "\u0438\u0448\u044C", -1, 2, "", this),
-            new Among ( "\u044E", -1, 2, "", this),
-            new Among ( "\u0443\u044E", 44, 2, "", this)
-        };
-
-        private Among a_5[] = {
-            new Among ( "\u0430", -1, 1, "", this),
-            new Among ( "\u0435\u0432", -1, 1, "", this),
-            new Among ( "\u043E\u0432", -1, 1, "", this),
-            new Among ( "\u0435", -1, 1, "", this),
-            new Among ( "\u0438\u0435", 3, 1, "", this),
-            new Among ( "\u044C\u0435", 3, 1, "", this),
-            new Among ( "\u0438", -1, 1, "", this),
-            new Among ( "\u0435\u0438", 6, 1, "", this),
-            new Among ( "\u0438\u0438", 6, 1, "", this),
-            new Among ( "\u0430\u043C\u0438", 6, 1, "", this),
-            new Among ( "\u044F\u043C\u0438", 6, 1, "", this),
-            new Among ( "\u0438\u044F\u043C\u0438", 10, 1, "", this),
-            new Among ( "\u0439", -1, 1, "", this),
-            new Among ( "\u0435\u0439", 12, 1, "", this),
-            new Among ( "\u0438\u0435\u0439", 13, 1, "", this),
-            new Among ( "\u0438\u0439", 12, 1, "", this),
-            new Among ( "\u043E\u0439", 12, 1, "", this),
-            new Among ( "\u0430\u043C", -1, 1, "", this),
-            new Among ( "\u0435\u043C", -1, 1, "", this),
-            new Among ( "\u0438\u0435\u043C", 18, 1, "", this),
-            new Among ( "\u043E\u043C", -1, 1, "", this),
-            new Among ( "\u044F\u043C", -1, 1, "", this),
-            new Among ( "\u0438\u044F\u043C", 21, 1, "", this),
-            new Among ( "\u043E", -1, 1, "", this),
-            new Among ( "\u0443", -1, 1, "", this),
-            new Among ( "\u0430\u0445", -1, 1, "", this),
-            new Among ( "\u044F\u0445", -1, 1, "", this),
-            new Among ( "\u0438\u044F\u0445", 26, 1, "", this),
-            new Among ( "\u044B", -1, 1, "", this),
-            new Among ( "\u044C", -1, 1, "", this),
-            new Among ( "\u044E", -1, 1, "", this),
-            new Among ( "\u0438\u044E", 30, 1, "", this),
-            new Among ( "\u044C\u044E", 30, 1, "", this),
-            new Among ( "\u044F", -1, 1, "", this),
-            new Among ( "\u0438\u044F", 33, 1, "", this),
-            new Among ( "\u044C\u044F", 33, 1, "", this)
-        };
-
-        private Among a_6[] = {
-            new Among ( "\u043E\u0441\u0442", -1, 1, "", this),
-            new Among ( "\u043E\u0441\u0442\u044C", -1, 1, "", this)
-        };
-
-        private Among a_7[] = {
-            new Among ( "\u0435\u0439\u0448\u0435", -1, 1, "", this),
-            new Among ( "\u043D", -1, 2, "", this),
-            new Among ( "\u0435\u0439\u0448", -1, 1, "", this),
-            new Among ( "\u044C", -1, 3, "", this)
-        };
-
-        private static final char g_v[] = {33, 65, 8, 232 };
-
-        private int I_p2;
-        private int I_pV;
-
-        private void copy_from(RussianStemmer other) {
-            I_p2 = other.I_p2;
-            I_pV = other.I_pV;
-            super.copy_from(other);
-        }
-
-        private boolean r_mark_regions() {
-            int v_1;
-            // (, line 57
-            I_pV = limit;
-            I_p2 = limit;
-            // do, line 61
-            v_1 = cursor;
-            lab0: do {
-                // (, line 61
-                // gopast, line 62
-                golab1: while(true)
-                {
-                    lab2: do {
-                        if (!(in_grouping(g_v, 1072, 1103)))
-                        {
-                            break lab2;
-                        }
-                        break golab1;
-                    } while (false);
-                    if (cursor >= limit)
-                    {
-                        break lab0;
-                    }
-                    cursor++;
-                }
-                // setmark pV, line 62
-                I_pV = cursor;
-                // gopast, line 62
-                golab3: while(true)
-                {
-                    lab4: do {
-                        if (!(out_grouping(g_v, 1072, 1103)))
-                        {
-                            break lab4;
-                        }
-                        break golab3;
-                    } while (false);
-                    if (cursor >= limit)
-                    {
-                        break lab0;
-                    }
-                    cursor++;
-                }
-                // gopast, line 63
-                golab5: while(true)
-                {
-                    lab6: do {
-                        if (!(in_grouping(g_v, 1072, 1103)))
-                        {
-                            break lab6;
-                        }
-                        break golab5;
-                    } while (false);
-                    if (cursor >= limit)
-                    {
-                        break lab0;
-                    }
-                    cursor++;
-                }
-                // gopast, line 63
-                golab7: while(true)
-                {
-                    lab8: do {
-                        if (!(out_grouping(g_v, 1072, 1103)))
-                        {
-                            break lab8;
-                        }
-                        break golab7;
-                    } while (false);
-                    if (cursor >= limit)
-                    {
-                        break lab0;
-                    }
-                    cursor++;
-                }
-                // setmark p2, line 63
-                I_p2 = cursor;
-            } while (false);
-            cursor = v_1;
-            return true;
-        }
-
-        private boolean r_R2() {
-            if (!(I_p2 <= cursor))
-            {
-                return false;
-            }
-            return true;
-        }
-
-        private boolean r_perfective_gerund() {
-            int among_var;
-            int v_1;
-            // (, line 71
-            // [, line 72
-            ket = cursor;
-            // substring, line 72
-            among_var = find_among_b(a_0, 9);
-            if (among_var == 0)
-            {
-                return false;
-            }
-            // ], line 72
-            bra = cursor;
-            switch(among_var) {
-                case 0:
-                    return false;
-                case 1:
-                    // (, line 76
-                    // or, line 76
-                    lab0: do {
-                        v_1 = limit - cursor;
-                        lab1: do {
-                            // literal, line 76
-                            if (!(eq_s_b(1, "\u0430")))
-                            {
-                                break lab1;
-                            }
-                            break lab0;
-                        } while (false);
-                        cursor = limit - v_1;
-                        // literal, line 76
-                        if (!(eq_s_b(1, "\u044F")))
-                        {
-                            return false;
-                        }
-                    } while (false);
-                    // delete, line 76
-                    slice_del();
-                    break;
-                case 2:
-                    // (, line 83
-                    // delete, line 83
-                    slice_del();
-                    break;
-            }
-            return true;
-        }
-
-        private boolean r_adjective() {
-            int among_var;
-            // (, line 87
-            // [, line 88
-            ket = cursor;
-            // substring, line 88
-            among_var = find_among_b(a_1, 26);
-            if (among_var == 0)
-            {
-                return false;
-            }
-            // ], line 88
-            bra = cursor;
-            switch(among_var) {
-                case 0:
-                    return false;
-                case 1:
-                    // (, line 97
-                    // delete, line 97
-                    slice_del();
-                    break;
-            }
-            return true;
-        }
-
-        private boolean r_adjectival() {
-            int among_var;
-            int v_1;
-            int v_2;
-            // (, line 101
-            // call adjective, line 102
-            if (!r_adjective())
-            {
-                return false;
-            }
-            // try, line 109
-            v_1 = limit - cursor;
-            lab0: do {
-                // (, line 109
-                // [, line 110
-                ket = cursor;
-                // substring, line 110
-                among_var = find_among_b(a_2, 8);
-                if (among_var == 0)
-                {
-                    cursor = limit - v_1;
-                    break lab0;
-                }
-                // ], line 110
-                bra = cursor;
-                switch(among_var) {
-                    case 0:
-                        cursor = limit - v_1;
-                        break lab0;
-                    case 1:
-                        // (, line 115
-                        // or, line 115
-                        lab1: do {
-                            v_2 = limit - cursor;
-                            lab2: do {
-                                // literal, line 115
-                                if (!(eq_s_b(1, "\u0430")))
-                                {
-                                    break lab2;
-                                }
-                                break lab1;
-                            } while (false);
-                            cursor = limit - v_2;
-                            // literal, line 115
-                            if (!(eq_s_b(1, "\u044F")))
-                            {
-                                cursor = limit - v_1;
-                                break lab0;
-                            }
-                        } while (false);
-                        // delete, line 115
-                        slice_del();
-                        break;
-                    case 2:
-                        // (, line 122
-                        // delete, line 122
-                        slice_del();
-                        break;
-                }
-            } while (false);
-            return true;
-        }
-
-        private boolean r_reflexive() {
-            int among_var;
-            // (, line 128
-            // [, line 129
-            ket = cursor;
-            // substring, line 129
-            among_var = find_among_b(a_3, 2);
-            if (among_var == 0)
-            {
-                return false;
-            }
-            // ], line 129
-            bra = cursor;
-            switch(among_var) {
-                case 0:
-                    return false;
-                case 1:
-                    // (, line 132
-                    // delete, line 132
-                    slice_del();
-                    break;
-            }
-            return true;
-        }
-
-        private boolean r_verb() {
-            int among_var;
-            int v_1;
-            // (, line 136
-            // [, line 137
-            ket = cursor;
-            // substring, line 137
-            among_var = find_among_b(a_4, 46);
-            if (among_var == 0)
-            {
-                return false;
-            }
-            // ], line 137
-            bra = cursor;
-            switch(among_var) {
-                case 0:
-                    return false;
-                case 1:
-                    // (, line 143
-                    // or, line 143
-                    lab0: do {
-                        v_1 = limit - cursor;
-                        lab1: do {
-                            // literal, line 143
-                            if (!(eq_s_b(1, "\u0430")))
-                            {
-                                break lab1;
-                            }
-                            break lab0;
-                        } while (false);
-                        cursor = limit - v_1;
-                        // literal, line 143
-                        if (!(eq_s_b(1, "\u044F")))
-                        {
-                            return false;
-                        }
-                    } while (false);
-                    // delete, line 143
-                    slice_del();
-                    break;
-                case 2:
-                    // (, line 151
-                    // delete, line 151
-                    slice_del();
-                    break;
-            }
-            return true;
-        }
-
-        private boolean r_noun() {
-            int among_var;
-            // (, line 159
-            // [, line 160
-            ket = cursor;
-            // substring, line 160
-            among_var = find_among_b(a_5, 36);
-            if (among_var == 0)
-            {
-                return false;
-            }
-            // ], line 160
-            bra = cursor;
-            switch(among_var) {
-                case 0:
-                    return false;
-                case 1:
-                    // (, line 167
-                    // delete, line 167
-                    slice_del();
-                    break;
-            }
-            return true;
-        }
-
-        private boolean r_derivational() {
-            int among_var;
-            // (, line 175
-            // [, line 176
-            ket = cursor;
-            // substring, line 176
-            among_var = find_among_b(a_6, 2);
-            if (among_var == 0)
-            {
-                return false;
-            }
-            // ], line 176
-            bra = cursor;
-            // call R2, line 176
-            if (!r_R2())
-            {
-                return false;
-            }
-            switch(among_var) {
-                case 0:
-                    return false;
-                case 1:
-                    // (, line 179
-                    // delete, line 179
-                    slice_del();
-                    break;
-            }
-            return true;
-        }
-
-        private boolean r_tidy_up() {
-            int among_var;
-            // (, line 183
-            // [, line 184
-            ket = cursor;
-            // substring, line 184
-            among_var = find_among_b(a_7, 4);
-            if (among_var == 0)
-            {
-                return false;
-            }
-            // ], line 184
-            bra = cursor;
-            switch(among_var) {
-                case 0:
-                    return false;
-                case 1:
-                    // (, line 188
-                    // delete, line 188
-                    slice_del();
-                    // [, line 189
-                    ket = cursor;
-                    // literal, line 189
-                    if (!(eq_s_b(1, "\u043D")))
-                    {
-                        return false;
-                    }
-                    // ], line 189
-                    bra = cursor;
-                    // literal, line 189
-                    if (!(eq_s_b(1, "\u043D")))
-                    {
-                        return false;
-                    }
-                    // delete, line 189
-                    slice_del();
-                    break;
-                case 2:
-                    // (, line 192
-                    // literal, line 192
-                    if (!(eq_s_b(1, "\u043D")))
-                    {
-                        return false;
-                    }
-                    // delete, line 192
-                    slice_del();
-                    break;
-                case 3:
-                    // (, line 194
-                    // delete, line 194
-                    slice_del();
-                    break;
-            }
-            return true;
-        }
-
-        public boolean stem() {
-            int v_1;
-            int v_2;
-            int v_3;
-            int v_4;
-            int v_5;
-            int v_6;
-            int v_7;
-            int v_8;
-            int v_9;
-            int v_10;
-            // (, line 199
-            // do, line 201
-            v_1 = cursor;
-            lab0: do {
-                // call mark_regions, line 201
-                if (!r_mark_regions())
-                {
-                    break lab0;
-                }
-            } while (false);
-            cursor = v_1;
-            // backwards, line 202
-            limit_backward = cursor; cursor = limit;
-            // setlimit, line 202
-            v_2 = limit - cursor;
-            // tomark, line 202
-            if (cursor < I_pV)
-            {
-                return false;
-            }
-            cursor = I_pV;
-            v_3 = limit_backward;
-            limit_backward = cursor;
-            cursor = limit - v_2;
-            // (, line 202
-            // do, line 203
-            v_4 = limit - cursor;
-            lab1: do {
-                // (, line 203
-                // or, line 204
-                lab2: do {
-                    v_5 = limit - cursor;
-                    lab3: do {
-                        // call perfective_gerund, line 204
-                        if (!r_perfective_gerund())
-                        {
-                            break lab3;
-                        }
-                        break lab2;
-                    } while (false);
-                    cursor = limit - v_5;
-                    // (, line 205
-                    // try, line 205
-                    v_6 = limit - cursor;
-                    lab4: do {
-                        // call reflexive, line 205
-                        if (!r_reflexive())
-                        {
-                            cursor = limit - v_6;
-                            break lab4;
-                        }
-                    } while (false);
-                    // or, line 206
-                    lab5: do {
-                        v_7 = limit - cursor;
-                        lab6: do {
-                            // call adjectival, line 206
-                            if (!r_adjectival())
-                            {
-                                break lab6;
-                            }
-                            break lab5;
-                        } while (false);
-                        cursor = limit - v_7;
-                        lab7: do {
-                            // call verb, line 206
-                            if (!r_verb())
-                            {
-                                break lab7;
-                            }
-                            break lab5;
-                        } while (false);
-                        cursor = limit - v_7;
-                        // call noun, line 206
-                        if (!r_noun())
-                        {
-                            break lab1;
-                        }
-                    } while (false);
-                } while (false);
-            } while (false);
-            cursor = limit - v_4;
-            // try, line 209
-            v_8 = limit - cursor;
-            lab8: do {
-                // (, line 209
-                // [, line 209
-                ket = cursor;
-                // literal, line 209
-                if (!(eq_s_b(1, "\u0438")))
-                {
-                    cursor = limit - v_8;
-                    break lab8;
-                }
-                // ], line 209
-                bra = cursor;
-                // delete, line 209
-                slice_del();
-            } while (false);
-            // do, line 212
-            v_9 = limit - cursor;
-            lab9: do {
-                // call derivational, line 212
-                if (!r_derivational())
-                {
-                    break lab9;
-                }
-            } while (false);
-            cursor = limit - v_9;
-            // do, line 213
-            v_10 = limit - cursor;
-            lab10: do {
-                // call tidy_up, line 213
-                if (!r_tidy_up())
-                {
-                    break lab10;
-                }
-            } while (false);
-            cursor = limit - v_10;
-            limit_backward = v_3;
-            cursor = limit_backward;            return true;
-        }
-
-}
-
Index: contrib/snowball/src/java/org/tartarus/snowball/ext/EnglishStemmer.java
===================================================================
--- contrib/snowball/src/java/org/tartarus/snowball/ext/EnglishStemmer.java	(revision 916567)
+++ contrib/snowball/src/java/org/tartarus/snowball/ext/EnglishStemmer.java	(working copy)
@@ -1,1314 +0,0 @@
-// This file was generated automatically by the Snowball to Java compiler
-
-package org.tartarus.snowball.ext;
-import org.tartarus.snowball.SnowballProgram;
-import org.tartarus.snowball.Among;
-
-/**
- * Generated class implementing code defined by a snowball script.
- */
-public class EnglishStemmer extends SnowballProgram {
-
-        private Among a_0[] = {
-            new Among ( "arsen", -1, -1, "", this),
-            new Among ( "commun", -1, -1, "", this),
-            new Among ( "gener", -1, -1, "", this)
-        };
-
-        private Among a_1[] = {
-            new Among ( "'", -1, 1, "", this),
-            new Among ( "'s'", 0, 1, "", this),
-            new Among ( "'s", -1, 1, "", this)
-        };
-
-        private Among a_2[] = {
-            new Among ( "ied", -1, 2, "", this),
-            new Among ( "s", -1, 3, "", this),
-            new Among ( "ies", 1, 2, "", this),
-            new Among ( "sses", 1, 1, "", this),
-            new Among ( "ss", 1, -1, "", this),
-            new Among ( "us", 1, -1, "", this)
-        };
-
-        private Among a_3[] = {
-            new Among ( "", -1, 3, "", this),
-            new Among ( "bb", 0, 2, "", this),
-            new Among ( "dd", 0, 2, "", this),
-            new Among ( "ff", 0, 2, "", this),
-            new Among ( "gg", 0, 2, "", this),
-            new Among ( "bl", 0, 1, "", this),
-            new Among ( "mm", 0, 2, "", this),
-            new Among ( "nn", 0, 2, "", this),
-            new Among ( "pp", 0, 2, "", this),
-            new Among ( "rr", 0, 2, "", this),
-            new Among ( "at", 0, 1, "", this),
-            new Among ( "tt", 0, 2, "", this),
-            new Among ( "iz", 0, 1, "", this)
-        };
-
-        private Among a_4[] = {
-            new Among ( "ed", -1, 2, "", this),
-            new Among ( "eed", 0, 1, "", this),
-            new Among ( "ing", -1, 2, "", this),
-            new Among ( "edly", -1, 2, "", this),
-            new Among ( "eedly", 3, 1, "", this),
-            new Among ( "ingly", -1, 2, "", this)
-        };
-
-        private Among a_5[] = {
-            new Among ( "anci", -1, 3, "", this),
-            new Among ( "enci", -1, 2, "", this),
-            new Among ( "ogi", -1, 13, "", this),
-            new Among ( "li", -1, 16, "", this),
-            new Among ( "bli", 3, 12, "", this),
-            new Among ( "abli", 4, 4, "", this),
-            new Among ( "alli", 3, 8, "", this),
-            new Among ( "fulli", 3, 14, "", this),
-            new Among ( "lessli", 3, 15, "", this),
-            new Among ( "ousli", 3, 10, "", this),
-            new Among ( "entli", 3, 5, "", this),
-            new Among ( "aliti", -1, 8, "", this),
-            new Among ( "biliti", -1, 12, "", this),
-            new Among ( "iviti", -1, 11, "", this),
-            new Among ( "tional", -1, 1, "", this),
-            new Among ( "ational", 14, 7, "", this),
-            new Among ( "alism", -1, 8, "", this),
-            new Among ( "ation", -1, 7, "", this),
-            new Among ( "ization", 17, 6, "", this),
-            new Among ( "izer", -1, 6, "", this),
-            new Among ( "ator", -1, 7, "", this),
-            new Among ( "iveness", -1, 11, "", this),
-            new Among ( "fulness", -1, 9, "", this),
-            new Among ( "ousness", -1, 10, "", this)
-        };
-
-        private Among a_6[] = {
-            new Among ( "icate", -1, 4, "", this),
-            new Among ( "ative", -1, 6, "", this),
-            new Among ( "alize", -1, 3, "", this),
-            new Among ( "iciti", -1, 4, "", this),
-            new Among ( "ical", -1, 4, "", this),
-            new Among ( "tional", -1, 1, "", this),
-            new Among ( "ational", 5, 2, "", this),
-            new Among ( "ful", -1, 5, "", this),
-            new Among ( "ness", -1, 5, "", this)
-        };
-
-        private Among a_7[] = {
-            new Among ( "ic", -1, 1, "", this),
-            new Among ( "ance", -1, 1, "", this),
-            new Among ( "ence", -1, 1, "", this),
-            new Among ( "able", -1, 1, "", this),
-            new Among ( "ible", -1, 1, "", this),
-            new Among ( "ate", -1, 1, "", this),
-            new Among ( "ive", -1, 1, "", this),
-            new Among ( "ize", -1, 1, "", this),
-            new Among ( "iti", -1, 1, "", this),
-            new Among ( "al", -1, 1, "", this),
-            new Among ( "ism", -1, 1, "", this),
-            new Among ( "ion", -1, 2, "", this),
-            new Among ( "er", -1, 1, "", this),
-            new Among ( "ous", -1, 1, "", this),
-            new Among ( "ant", -1, 1, "", this),
-            new Among ( "ent", -1, 1, "", this),
-            new Among ( "ment", 15, 1, "", this),
-            new Among ( "ement", 16, 1, "", this)
-        };
-
-        private Among a_8[] = {
-            new Among ( "e", -1, 1, "", this),
-            new Among ( "l", -1, 2, "", this)
-        };
-
-        private Among a_9[] = {
-            new Among ( "succeed", -1, -1, "", this),
-            new Among ( "proceed", -1, -1, "", this),
-            new Among ( "exceed", -1, -1, "", this),
-            new Among ( "canning", -1, -1, "", this),
-            new Among ( "inning", -1, -1, "", this),
-            new Among ( "earring", -1, -1, "", this),
-            new Among ( "herring", -1, -1, "", this),
-            new Among ( "outing", -1, -1, "", this)
-        };
-
-        private Among a_10[] = {
-            new Among ( "andes", -1, -1, "", this),
-            new Among ( "atlas", -1, -1, "", this),
-            new Among ( "bias", -1, -1, "", this),
-            new Among ( "cosmos", -1, -1, "", this),
-            new Among ( "dying", -1, 3, "", this),
-            new Among ( "early", -1, 9, "", this),
-            new Among ( "gently", -1, 7, "", this),
-            new Among ( "howe", -1, -1, "", this),
-            new Among ( "idly", -1, 6, "", this),
-            new Among ( "lying", -1, 4, "", this),
-            new Among ( "news", -1, -1, "", this),
-            new Among ( "only", -1, 10, "", this),
-            new Among ( "singly", -1, 11, "", this),
-            new Among ( "skies", -1, 2, "", this),
-            new Among ( "skis", -1, 1, "", this),
-            new Among ( "sky", -1, -1, "", this),
-            new Among ( "tying", -1, 5, "", this),
-            new Among ( "ugly", -1, 8, "", this)
-        };
-
-        private static final char g_v[] = {17, 65, 16, 1 };
-
-        private static final char g_v_WXY[] = {1, 17, 65, 208, 1 };
-
-        private static final char g_valid_LI[] = {55, 141, 2 };
-
-        private boolean B_Y_found;
-        private int I_p2;
-        private int I_p1;
-
-        private void copy_from(EnglishStemmer other) {
-            B_Y_found = other.B_Y_found;
-            I_p2 = other.I_p2;
-            I_p1 = other.I_p1;
-            super.copy_from(other);
-        }
-
-        private boolean r_prelude() {
-            int v_1;
-            int v_2;
-            int v_3;
-            int v_4;
-            int v_5;
-            // (, line 25
-            // unset Y_found, line 26
-            B_Y_found = false;
-            // do, line 27
-            v_1 = cursor;
-            lab0: do {
-                // (, line 27
-                // [, line 27
-                bra = cursor;
-                // literal, line 27
-                if (!(eq_s(1, "'")))
-                {
-                    break lab0;
-                }
-                // ], line 27
-                ket = cursor;
-                // delete, line 27
-                slice_del();
-            } while (false);
-            cursor = v_1;
-            // do, line 28
-            v_2 = cursor;
-            lab1: do {
-                // (, line 28
-                // [, line 28
-                bra = cursor;
-                // literal, line 28
-                if (!(eq_s(1, "y")))
-                {
-                    break lab1;
-                }
-                // ], line 28
-                ket = cursor;
-                // <-, line 28
-                slice_from("Y");
-                // set Y_found, line 28
-                B_Y_found = true;
-            } while (false);
-            cursor = v_2;
-            // do, line 29
-            v_3 = cursor;
-            lab2: do {
-                // repeat, line 29
-                replab3: while(true)
-                {
-                    v_4 = cursor;
-                    lab4: do {
-                        // (, line 29
-                        // goto, line 29
-                        golab5: while(true)
-                        {
-                            v_5 = cursor;
-                            lab6: do {
-                                // (, line 29
-                                if (!(in_grouping(g_v, 97, 121)))
-                                {
-                                    break lab6;
-                                }
-                                // [, line 29
-                                bra = cursor;
-                                // literal, line 29
-                                if (!(eq_s(1, "y")))
-                                {
-                                    break lab6;
-                                }
-                                // ], line 29
-                                ket = cursor;
-                                cursor = v_5;
-                                break golab5;
-                            } while (false);
-                            cursor = v_5;
-                            if (cursor >= limit)
-                            {
-                                break lab4;
-                            }
-                            cursor++;
-                        }
-                        // <-, line 29
-                        slice_from("Y");
-                        // set Y_found, line 29
-                        B_Y_found = true;
-                        continue replab3;
-                    } while (false);
-                    cursor = v_4;
-                    break replab3;
-                }
-            } while (false);
-            cursor = v_3;
-            return true;
-        }
-
-        private boolean r_mark_regions() {
-            int v_1;
-            int v_2;
-            // (, line 32
-            I_p1 = limit;
-            I_p2 = limit;
-            // do, line 35
-            v_1 = cursor;
-            lab0: do {
-                // (, line 35
-                // or, line 41
-                lab1: do {
-                    v_2 = cursor;
-                    lab2: do {
-                        // among, line 36
-                        if (find_among(a_0, 3) == 0)
-                        {
-                            break lab2;
-                        }
-                        break lab1;
-                    } while (false);
-                    cursor = v_2;
-                    // (, line 41
-                    // gopast, line 41
-                    golab3: while(true)
-                    {
-                        lab4: do {
-                            if (!(in_grouping(g_v, 97, 121)))
-                            {
-                                break lab4;
-                            }
-                            break golab3;
-                        } while (false);
-                        if (cursor >= limit)
-                        {
-                            break lab0;
-                        }
-                        cursor++;
-                    }
-                    // gopast, line 41
-                    golab5: while(true)
-                    {
-                        lab6: do {
-                            if (!(out_grouping(g_v, 97, 121)))
-                            {
-                                break lab6;
-                            }
-                            break golab5;
-                        } while (false);
-                        if (cursor >= limit)
-                        {
-                            break lab0;
-                        }
-                        cursor++;
-                    }
-                } while (false);
-                // setmark p1, line 42
-                I_p1 = cursor;
-                // gopast, line 43
-                golab7: while(true)
-                {
-                    lab8: do {
-                        if (!(in_grouping(g_v, 97, 121)))
-                        {
-                            break lab8;
-                        }
-                        break golab7;
-                    } while (false);
-                    if (cursor >= limit)
-                    {
-                        break lab0;
-                    }
-                    cursor++;
-                }
-                // gopast, line 43
-                golab9: while(true)
-                {
-                    lab10: do {
-                        if (!(out_grouping(g_v, 97, 121)))
-                        {
-                            break lab10;
-                        }
-                        break golab9;
-                    } while (false);
-                    if (cursor >= limit)
-                    {
-                        break lab0;
-                    }
-                    cursor++;
-                }
-                // setmark p2, line 43
-                I_p2 = cursor;
-            } while (false);
-            cursor = v_1;
-            return true;
-        }
-
-        private boolean r_shortv() {
-            int v_1;
-            // (, line 49
-            // or, line 51
-            lab0: do {
-                v_1 = limit - cursor;
-                lab1: do {
-                    // (, line 50
-                    if (!(out_grouping_b(g_v_WXY, 89, 121)))
-                    {
-                        break lab1;
-                    }
-                    if (!(in_grouping_b(g_v, 97, 121)))
-                    {
-                        break lab1;
-                    }
-                    if (!(out_grouping_b(g_v, 97, 121)))
-                    {
-                        break lab1;
-                    }
-                    break lab0;
-                } while (false);
-                cursor = limit - v_1;
-                // (, line 52
-                if (!(out_grouping_b(g_v, 97, 121)))
-                {
-                    return false;
-                }
-                if (!(in_grouping_b(g_v, 97, 121)))
-                {
-                    return false;
-                }
-                // atlimit, line 52
-                if (cursor > limit_backward)
-                {
-                    return false;
-                }
-            } while (false);
-            return true;
-        }
-
-        private boolean r_R1() {
-            if (!(I_p1 <= cursor))
-            {
-                return false;
-            }
-            return true;
-        }
-
-        private boolean r_R2() {
-            if (!(I_p2 <= cursor))
-            {
-                return false;
-            }
-            return true;
-        }
-
-        private boolean r_Step_1a() {
-            int among_var;
-            int v_1;
-            int v_2;
-            // (, line 58
-            // try, line 59
-            v_1 = limit - cursor;
-            lab0: do {
-                // (, line 59
-                // [, line 60
-                ket = cursor;
-                // substring, line 60
-                among_var = find_among_b(a_1, 3);
-                if (among_var == 0)
-                {
-                    cursor = limit - v_1;
-                    break lab0;
-                }
-                // ], line 60
-                bra = cursor;
-                switch(among_var) {
-                    case 0:
-                        cursor = limit - v_1;
-                        break lab0;
-                    case 1:
-                        // (, line 62
-                        // delete, line 62
-                        slice_del();
-                        break;
-                }
-            } while (false);
-            // [, line 65
-            ket = cursor;
-            // substring, line 65
-            among_var = find_among_b(a_2, 6);
-            if (among_var == 0)
-            {
-                return false;
-            }
-            // ], line 65
-            bra = cursor;
-            switch(among_var) {
-                case 0:
-                    return false;
-                case 1:
-                    // (, line 66
-                    // <-, line 66
-                    slice_from("ss");
-                    break;
-                case 2:
-                    // (, line 68
-                    // or, line 68
-                    lab1: do {
-                        v_2 = limit - cursor;
-                        lab2: do {
-                            // (, line 68
-                            // hop, line 68
-                            {
-                                int c = cursor - 2;
-                                if (limit_backward > c || c > limit)
-                                {
-                                    break lab2;
-                                }
-                                cursor = c;
-                            }
-                            // <-, line 68
-                            slice_from("i");
-                            break lab1;
-                        } while (false);
-                        cursor = limit - v_2;
-                        // <-, line 68
-                        slice_from("ie");
-                    } while (false);
-                    break;
-                case 3:
-                    // (, line 69
-                    // next, line 69
-                    if (cursor <= limit_backward)
-                    {
-                        return false;
-                    }
-                    cursor--;
-                    // gopast, line 69
-                    golab3: while(true)
-                    {
-                        lab4: do {
-                            if (!(in_grouping_b(g_v, 97, 121)))
-                            {
-                                break lab4;
-                            }
-                            break golab3;
-                        } while (false);
-                        if (cursor <= limit_backward)
-                        {
-                            return false;
-                        }
-                        cursor--;
-                    }
-                    // delete, line 69
-                    slice_del();
-                    break;
-            }
-            return true;
-        }
-
-        private boolean r_Step_1b() {
-            int among_var;
-            int v_1;
-            int v_3;
-            int v_4;
-            // (, line 74
-            // [, line 75
-            ket = cursor;
-            // substring, line 75
-            among_var = find_among_b(a_4, 6);
-            if (among_var == 0)
-            {
-                return false;
-            }
-            // ], line 75
-            bra = cursor;
-            switch(among_var) {
-                case 0:
-                    return false;
-                case 1:
-                    // (, line 77
-                    // call R1, line 77
-                    if (!r_R1())
-                    {
-                        return false;
-                    }
-                    // <-, line 77
-                    slice_from("ee");
-                    break;
-                case 2:
-                    // (, line 79
-                    // test, line 80
-                    v_1 = limit - cursor;
-                    // gopast, line 80
-                    golab0: while(true)
-                    {
-                        lab1: do {
-                            if (!(in_grouping_b(g_v, 97, 121)))
-                            {
-                                break lab1;
-                            }
-                            break golab0;
-                        } while (false);
-                        if (cursor <= limit_backward)
-                        {
-                            return false;
-                        }
-                        cursor--;
-                    }
-                    cursor = limit - v_1;
-                    // delete, line 80
-                    slice_del();
-                    // test, line 81
-                    v_3 = limit - cursor;
-                    // substring, line 81
-                    among_var = find_among_b(a_3, 13);
-                    if (among_var == 0)
-                    {
-                        return false;
-                    }
-                    cursor = limit - v_3;
-                    switch(among_var) {
-                        case 0:
-                            return false;
-                        case 1:
-                            // (, line 83
-                            // <+, line 83
-                            {
-                                int c = cursor;
-                                insert(cursor, cursor, "e");
-                                cursor = c;
-                            }
-                            break;
-                        case 2:
-                            // (, line 86
-                            // [, line 86
-                            ket = cursor;
-                            // next, line 86
-                            if (cursor <= limit_backward)
-                            {
-                                return false;
-                            }
-                            cursor--;
-                            // ], line 86
-                            bra = cursor;
-                            // delete, line 86
-                            slice_del();
-                            break;
-                        case 3:
-                            // (, line 87
-                            // atmark, line 87
-                            if (cursor != I_p1)
-                            {
-                                return false;
-                            }
-                            // test, line 87
-                            v_4 = limit - cursor;
-                            // call shortv, line 87
-                            if (!r_shortv())
-                            {
-                                return false;
-                            }
-                            cursor = limit - v_4;
-                            // <+, line 87
-                            {
-                                int c = cursor;
-                                insert(cursor, cursor, "e");
-                                cursor = c;
-                            }
-                            break;
-                    }
-                    break;
-            }
-            return true;
-        }
-
-        private boolean r_Step_1c() {
-            int v_1;
-            int v_2;
-            // (, line 93
-            // [, line 94
-            ket = cursor;
-            // or, line 94
-            lab0: do {
-                v_1 = limit - cursor;
-                lab1: do {
-                    // literal, line 94
-                    if (!(eq_s_b(1, "y")))
-                    {
-                        break lab1;
-                    }
-                    break lab0;
-                } while (false);
-                cursor = limit - v_1;
-                // literal, line 94
-                if (!(eq_s_b(1, "Y")))
-                {
-                    return false;
-                }
-            } while (false);
-            // ], line 94
-            bra = cursor;
-            if (!(out_grouping_b(g_v, 97, 121)))
-            {
-                return false;
-            }
-            // not, line 95
-            {
-                v_2 = limit - cursor;
-                lab2: do {
-                    // atlimit, line 95
-                    if (cursor > limit_backward)
-                    {
-                        break lab2;
-                    }
-                    return false;
-                } while (false);
-                cursor = limit - v_2;
-            }
-            // <-, line 96
-            slice_from("i");
-            return true;
-        }
-
-        private boolean r_Step_2() {
-            int among_var;
-            // (, line 99
-            // [, line 100
-            ket = cursor;
-            // substring, line 100
-            among_var = find_among_b(a_5, 24);
-            if (among_var == 0)
-            {
-                return false;
-            }
-            // ], line 100
-            bra = cursor;
-            // call R1, line 100
-            if (!r_R1())
-            {
-                return false;
-            }
-            switch(among_var) {
-                case 0:
-                    return false;
-                case 1:
-                    // (, line 101
-                    // <-, line 101
-                    slice_from("tion");
-                    break;
-                case 2:
-                    // (, line 102
-                    // <-, line 102
-                    slice_from("ence");
-                    break;
-                case 3:
-                    // (, line 103
-                    // <-, line 103
-                    slice_from("ance");
-                    break;
-                case 4:
-                    // (, line 104
-                    // <-, line 104
-                    slice_from("able");
-                    break;
-                case 5:
-                    // (, line 105
-                    // <-, line 105
-                    slice_from("ent");
-                    break;
-                case 6:
-                    // (, line 107
-                    // <-, line 107
-                    slice_from("ize");
-                    break;
-                case 7:
-                    // (, line 109
-                    // <-, line 109
-                    slice_from("ate");
-                    break;
-                case 8:
-                    // (, line 111
-                    // <-, line 111
-                    slice_from("al");
-                    break;
-                case 9:
-                    // (, line 112
-                    // <-, line 112
-                    slice_from("ful");
-                    break;
-                case 10:
-                    // (, line 114
-                    // <-, line 114
-                    slice_from("ous");
-                    break;
-                case 11:
-                    // (, line 116
-                    // <-, line 116
-                    slice_from("ive");
-                    break;
-                case 12:
-                    // (, line 118
-                    // <-, line 118
-                    slice_from("ble");
-                    break;
-                case 13:
-                    // (, line 119
-                    // literal, line 119
-                    if (!(eq_s_b(1, "l")))
-                    {
-                        return false;
-                    }
-                    // <-, line 119
-                    slice_from("og");
-                    break;
-                case 14:
-                    // (, line 120
-                    // <-, line 120
-                    slice_from("ful");
-                    break;
-                case 15:
-                    // (, line 121
-                    // <-, line 121
-                    slice_from("less");
-                    break;
-                case 16:
-                    // (, line 122
-                    if (!(in_grouping_b(g_valid_LI, 99, 116)))
-                    {
-                        return false;
-                    }
-                    // delete, line 122
-                    slice_del();
-                    break;
-            }
-            return true;
-        }
-
-        private boolean r_Step_3() {
-            int among_var;
-            // (, line 126
-            // [, line 127
-            ket = cursor;
-            // substring, line 127
-            among_var = find_among_b(a_6, 9);
-            if (among_var == 0)
-            {
-                return false;
-            }
-            // ], line 127
-            bra = cursor;
-            // call R1, line 127
-            if (!r_R1())
-            {
-                return false;
-            }
-            switch(among_var) {
-                case 0:
-                    return false;
-                case 1:
-                    // (, line 128
-                    // <-, line 128
-                    slice_from("tion");
-                    break;
-                case 2:
-                    // (, line 129
-                    // <-, line 129
-                    slice_from("ate");
-                    break;
-                case 3:
-                    // (, line 130
-                    // <-, line 130
-                    slice_from("al");
-                    break;
-                case 4:
-                    // (, line 132
-                    // <-, line 132
-                    slice_from("ic");
-                    break;
-                case 5:
-                    // (, line 134
-                    // delete, line 134
-                    slice_del();
-                    break;
-                case 6:
-                    // (, line 136
-                    // call R2, line 136
-                    if (!r_R2())
-                    {
-                        return false;
-                    }
-                    // delete, line 136
-                    slice_del();
-                    break;
-            }
-            return true;
-        }
-
-        private boolean r_Step_4() {
-            int among_var;
-            int v_1;
-            // (, line 140
-            // [, line 141
-            ket = cursor;
-            // substring, line 141
-            among_var = find_among_b(a_7, 18);
-            if (among_var == 0)
-            {
-                return false;
-            }
-            // ], line 141
-            bra = cursor;
-            // call R2, line 141
-            if (!r_R2())
-            {
-                return false;
-            }
-            switch(among_var) {
-                case 0:
-                    return false;
-                case 1:
-                    // (, line 144
-                    // delete, line 144
-                    slice_del();
-                    break;
-                case 2:
-                    // (, line 145
-                    // or, line 145
-                    lab0: do {
-                        v_1 = limit - cursor;
-                        lab1: do {
-                            // literal, line 145
-                            if (!(eq_s_b(1, "s")))
-                            {
-                                break lab1;
-                            }
-                            break lab0;
-                        } while (false);
-                        cursor = limit - v_1;
-                        // literal, line 145
-                        if (!(eq_s_b(1, "t")))
-                        {
-                            return false;
-                        }
-                    } while (false);
-                    // delete, line 145
-                    slice_del();
-                    break;
-            }
-            return true;
-        }
-
-        private boolean r_Step_5() {
-            int among_var;
-            int v_1;
-            int v_2;
-            // (, line 149
-            // [, line 150
-            ket = cursor;
-            // substring, line 150
-            among_var = find_among_b(a_8, 2);
-            if (among_var == 0)
-            {
-                return false;
-            }
-            // ], line 150
-            bra = cursor;
-            switch(among_var) {
-                case 0:
-                    return false;
-                case 1:
-                    // (, line 151
-                    // or, line 151
-                    lab0: do {
-                        v_1 = limit - cursor;
-                        lab1: do {
-                            // call R2, line 151
-                            if (!r_R2())
-                            {
-                                break lab1;
-                            }
-                            break lab0;
-                        } while (false);
-                        cursor = limit - v_1;
-                        // (, line 151
-                        // call R1, line 151
-                        if (!r_R1())
-                        {
-                            return false;
-                        }
-                        // not, line 151
-                        {
-                            v_2 = limit - cursor;
-                            lab2: do {
-                                // call shortv, line 151
-                                if (!r_shortv())
-                                {
-                                    break lab2;
-                                }
-                                return false;
-                            } while (false);
-                            cursor = limit - v_2;
-                        }
-                    } while (false);
-                    // delete, line 151
-                    slice_del();
-                    break;
-                case 2:
-                    // (, line 152
-                    // call R2, line 152
-                    if (!r_R2())
-                    {
-                        return false;
-                    }
-                    // literal, line 152
-                    if (!(eq_s_b(1, "l")))
-                    {
-                        return false;
-                    }
-                    // delete, line 152
-                    slice_del();
-                    break;
-            }
-            return true;
-        }
-
-        private boolean r_exception2() {
-            // (, line 156
-            // [, line 158
-            ket = cursor;
-            // substring, line 158
-            if (find_among_b(a_9, 8) == 0)
-            {
-                return false;
-            }
-            // ], line 158
-            bra = cursor;
-            // atlimit, line 158
-            if (cursor > limit_backward)
-            {
-                return false;
-            }
-            return true;
-        }
-
-        private boolean r_exception1() {
-            int among_var;
-            // (, line 168
-            // [, line 170
-            bra = cursor;
-            // substring, line 170
-            among_var = find_among(a_10, 18);
-            if (among_var == 0)
-            {
-                return false;
-            }
-            // ], line 170
-            ket = cursor;
-            // atlimit, line 170
-            if (cursor < limit)
-            {
-                return false;
-            }
-            switch(among_var) {
-                case 0:
-                    return false;
-                case 1:
-                    // (, line 174
-                    // <-, line 174
-                    slice_from("ski");
-                    break;
-                case 2:
-                    // (, line 175
-                    // <-, line 175
-                    slice_from("sky");
-                    break;
-                case 3:
-                    // (, line 176
-                    // <-, line 176
-                    slice_from("die");
-                    break;
-                case 4:
-                    // (, line 177
-                    // <-, line 177
-                    slice_from("lie");
-                    break;
-                case 5:
-                    // (, line 178
-                    // <-, line 178
-                    slice_from("tie");
-                    break;
-                case 6:
-                    // (, line 182
-                    // <-, line 182
-                    slice_from("idl");
-                    break;
-                case 7:
-                    // (, line 183
-                    // <-, line 183
-                    slice_from("gentl");
-                    break;
-                case 8:
-                    // (, line 184
-                    // <-, line 184
-                    slice_from("ugli");
-                    break;
-                case 9:
-                    // (, line 185
-                    // <-, line 185
-                    slice_from("earli");
-                    break;
-                case 10:
-                    // (, line 186
-                    // <-, line 186
-                    slice_from("onli");
-                    break;
-                case 11:
-                    // (, line 187
-                    // <-, line 187
-                    slice_from("singl");
-                    break;
-            }
-            return true;
-        }
-
-        private boolean r_postlude() {
-            int v_1;
-            int v_2;
-            // (, line 203
-            // Boolean test Y_found, line 203
-            if (!(B_Y_found))
-            {
-                return false;
-            }
-            // repeat, line 203
-            replab0: while(true)
-            {
-                v_1 = cursor;
-                lab1: do {
-                    // (, line 203
-                    // goto, line 203
-                    golab2: while(true)
-                    {
-                        v_2 = cursor;
-                        lab3: do {
-                            // (, line 203
-                            // [, line 203
-                            bra = cursor;
-                            // literal, line 203
-                            if (!(eq_s(1, "Y")))
-                            {
-                                break lab3;
-                            }
-                            // ], line 203
-                            ket = cursor;
-                            cursor = v_2;
-                            break golab2;
-                        } while (false);
-                        cursor = v_2;
-                        if (cursor >= limit)
-                        {
-                            break lab1;
-                        }
-                        cursor++;
-                    }
-                    // <-, line 203
-                    slice_from("y");
-                    continue replab0;
-                } while (false);
-                cursor = v_1;
-                break replab0;
-            }
-            return true;
-        }
-
-        public boolean stem() {
-            int v_1;
-            int v_2;
-            int v_3;
-            int v_4;
-            int v_5;
-            int v_6;
-            int v_7;
-            int v_8;
-            int v_9;
-            int v_10;
-            int v_11;
-            int v_12;
-            int v_13;
-            // (, line 205
-            // or, line 207
-            lab0: do {
-                v_1 = cursor;
-                lab1: do {
-                    // call exception1, line 207
-                    if (!r_exception1())
-                    {
-                        break lab1;
-                    }
-                    break lab0;
-                } while (false);
-                cursor = v_1;
-                lab2: do {
-                    // not, line 208
-                    {
-                        v_2 = cursor;
-                        lab3: do {
-                            // hop, line 208
-                            {
-                                int c = cursor + 3;
-                                if (0 > c || c > limit)
-                                {
-                                    break lab3;
-                                }
-                                cursor = c;
-                            }
-                            break lab2;
-                        } while (false);
-                        cursor = v_2;
-                    }
-                    break lab0;
-                } while (false);
-                cursor = v_1;
-                // (, line 208
-                // do, line 209
-                v_3 = cursor;
-                lab4: do {
-                    // call prelude, line 209
-                    if (!r_prelude())
-                    {
-                        break lab4;
-                    }
-                } while (false);
-                cursor = v_3;
-                // do, line 210
-                v_4 = cursor;
-                lab5: do {
-                    // call mark_regions, line 210
-                    if (!r_mark_regions())
-                    {
-                        break lab5;
-                    }
-                } while (false);
-                cursor = v_4;
-                // backwards, line 211
-                limit_backward = cursor; cursor = limit;
-                // (, line 211
-                // do, line 213
-                v_5 = limit - cursor;
-                lab6: do {
-                    // call Step_1a, line 213
-                    if (!r_Step_1a())
-                    {
-                        break lab6;
-                    }
-                } while (false);
-                cursor = limit - v_5;
-                // or, line 215
-                lab7: do {
-                    v_6 = limit - cursor;
-                    lab8: do {
-                        // call exception2, line 215
-                        if (!r_exception2())
-                        {
-                            break lab8;
-                        }
-                        break lab7;
-                    } while (false);
-                    cursor = limit - v_6;
-                    // (, line 215
-                    // do, line 217
-                    v_7 = limit - cursor;
-                    lab9: do {
-                        // call Step_1b, line 217
-                        if (!r_Step_1b())
-                        {
-                            break lab9;
-                        }
-                    } while (false);
-                    cursor = limit - v_7;
-                    // do, line 218
-                    v_8 = limit - cursor;
-                    lab10: do {
-                        // call Step_1c, line 218
-                        if (!r_Step_1c())
-                        {
-                            break lab10;
-                        }
-                    } while (false);
-                    cursor = limit - v_8;
-                    // do, line 220
-                    v_9 = limit - cursor;
-                    lab11: do {
-                        // call Step_2, line 220
-                        if (!r_Step_2())
-                        {
-                            break lab11;
-                        }
-                    } while (false);
-                    cursor = limit - v_9;
-                    // do, line 221
-                    v_10 = limit - cursor;
-                    lab12: do {
-                        // call Step_3, line 221
-                        if (!r_Step_3())
-                        {
-                            break lab12;
-                        }
-                    } while (false);
-                    cursor = limit - v_10;
-                    // do, line 222
-                    v_11 = limit - cursor;
-                    lab13: do {
-                        // call Step_4, line 222
-                        if (!r_Step_4())
-                        {
-                            break lab13;
-                        }
-                    } while (false);
-                    cursor = limit - v_11;
-                    // do, line 224
-                    v_12 = limit - cursor;
-                    lab14: do {
-                        // call Step_5, line 224
-                        if (!r_Step_5())
-                        {
-                            break lab14;
-                        }
-                    } while (false);
-                    cursor = limit - v_12;
-                } while (false);
-                cursor = limit_backward;                // do, line 227
-                v_13 = cursor;
-                lab15: do {
-                    // call postlude, line 227
-                    if (!r_postlude())
-                    {
-                        break lab15;
-                    }
-                } while (false);
-                cursor = v_13;
-            } while (false);
-            return true;
-        }
-
-}
-
Index: contrib/snowball/src/java/org/tartarus/snowball/ext/NorwegianStemmer.java
===================================================================
--- contrib/snowball/src/java/org/tartarus/snowball/ext/NorwegianStemmer.java	(revision 916567)
+++ contrib/snowball/src/java/org/tartarus/snowball/ext/NorwegianStemmer.java	(working copy)
@@ -1,358 +0,0 @@
-// This file was generated automatically by the Snowball to Java compiler
-
-package org.tartarus.snowball.ext;
-import org.tartarus.snowball.SnowballProgram;
-import org.tartarus.snowball.Among;
-
-/**
- * Generated class implementing code defined by a snowball script.
- */
-public class NorwegianStemmer extends SnowballProgram {
-
-        private Among a_0[] = {
-            new Among ( "a", -1, 1, "", this),
-            new Among ( "e", -1, 1, "", this),
-            new Among ( "ede", 1, 1, "", this),
-            new Among ( "ande", 1, 1, "", this),
-            new Among ( "ende", 1, 1, "", this),
-            new Among ( "ane", 1, 1, "", this),
-            new Among ( "ene", 1, 1, "", this),
-            new Among ( "hetene", 6, 1, "", this),
-            new Among ( "erte", 1, 3, "", this),
-            new Among ( "en", -1, 1, "", this),
-            new Among ( "heten", 9, 1, "", this),
-            new Among ( "ar", -1, 1, "", this),
-            new Among ( "er", -1, 1, "", this),
-            new Among ( "heter", 12, 1, "", this),
-            new Among ( "s", -1, 2, "", this),
-            new Among ( "as", 14, 1, "", this),
-            new Among ( "es", 14, 1, "", this),
-            new Among ( "edes", 16, 1, "", this),
-            new Among ( "endes", 16, 1, "", this),
-            new Among ( "enes", 16, 1, "", this),
-            new Among ( "hetenes", 19, 1, "", this),
-            new Among ( "ens", 14, 1, "", this),
-            new Among ( "hetens", 21, 1, "", this),
-            new Among ( "ers", 14, 1, "", this),
-            new Among ( "ets", 14, 1, "", this),
-            new Among ( "et", -1, 1, "", this),
-            new Among ( "het", 25, 1, "", this),
-            new Among ( "ert", -1, 3, "", this),
-            new Among ( "ast", -1, 1, "", this)
-        };
-
-        private Among a_1[] = {
-            new Among ( "dt", -1, -1, "", this),
-            new Among ( "vt", -1, -1, "", this)
-        };
-
-        private Among a_2[] = {
-            new Among ( "leg", -1, 1, "", this),
-            new Among ( "eleg", 0, 1, "", this),
-            new Among ( "ig", -1, 1, "", this),
-            new Among ( "eig", 2, 1, "", this),
-            new Among ( "lig", 2, 1, "", this),
-            new Among ( "elig", 4, 1, "", this),
-            new Among ( "els", -1, 1, "", this),
-            new Among ( "lov", -1, 1, "", this),
-            new Among ( "elov", 7, 1, "", this),
-            new Among ( "slov", 7, 1, "", this),
-            new Among ( "hetslov", 9, 1, "", this)
-        };
-
-        private static final char g_v[] = {17, 65, 16, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 48, 0, 128 };
-
-        private static final char g_s_ending[] = {119, 125, 149, 1 };
-
-        private int I_x;
-        private int I_p1;
-
-        private void copy_from(NorwegianStemmer other) {
-            I_x = other.I_x;
-            I_p1 = other.I_p1;
-            super.copy_from(other);
-        }
-
-        private boolean r_mark_regions() {
-            int v_1;
-            int v_2;
-            // (, line 26
-            I_p1 = limit;
-            // test, line 30
-            v_1 = cursor;
-            // (, line 30
-            // hop, line 30
-            {
-                int c = cursor + 3;
-                if (0 > c || c > limit)
-                {
-                    return false;
-                }
-                cursor = c;
-            }
-            // setmark x, line 30
-            I_x = cursor;
-            cursor = v_1;
-            // goto, line 31
-            golab0: while(true)
-            {
-                v_2 = cursor;
-                lab1: do {
-                    if (!(in_grouping(g_v, 97, 248)))
-                    {
-                        break lab1;
-                    }
-                    cursor = v_2;
-                    break golab0;
-                } while (false);
-                cursor = v_2;
-                if (cursor >= limit)
-                {
-                    return false;
-                }
-                cursor++;
-            }
-            // gopast, line 31
-            golab2: while(true)
-            {
-                lab3: do {
-                    if (!(out_grouping(g_v, 97, 248)))
-                    {
-                        break lab3;
-                    }
-                    break golab2;
-                } while (false);
-                if (cursor >= limit)
-                {
-                    return false;
-                }
-                cursor++;
-            }
-            // setmark p1, line 31
-            I_p1 = cursor;
-            // try, line 32
-            lab4: do {
-                // (, line 32
-                if (!(I_p1 < I_x))
-                {
-                    break lab4;
-                }
-                I_p1 = I_x;
-            } while (false);
-            return true;
-        }
-
-        private boolean r_main_suffix() {
-            int among_var;
-            int v_1;
-            int v_2;
-            int v_3;
-            // (, line 37
-            // setlimit, line 38
-            v_1 = limit - cursor;
-            // tomark, line 38
-            if (cursor < I_p1)
-            {
-                return false;
-            }
-            cursor = I_p1;
-            v_2 = limit_backward;
-            limit_backward = cursor;
-            cursor = limit - v_1;
-            // (, line 38
-            // [, line 38
-            ket = cursor;
-            // substring, line 38
-            among_var = find_among_b(a_0, 29);
-            if (among_var == 0)
-            {
-                limit_backward = v_2;
-                return false;
-            }
-            // ], line 38
-            bra = cursor;
-            limit_backward = v_2;
-            switch(among_var) {
-                case 0:
-                    return false;
-                case 1:
-                    // (, line 44
-                    // delete, line 44
-                    slice_del();
-                    break;
-                case 2:
-                    // (, line 46
-                    // or, line 46
-                    lab0: do {
-                        v_3 = limit - cursor;
-                        lab1: do {
-                            if (!(in_grouping_b(g_s_ending, 98, 122)))
-                            {
-                                break lab1;
-                            }
-                            break lab0;
-                        } while (false);
-                        cursor = limit - v_3;
-                        // (, line 46
-                        // literal, line 46
-                        if (!(eq_s_b(1, "k")))
-                        {
-                            return false;
-                        }
-                        if (!(out_grouping_b(g_v, 97, 248)))
-                        {
-                            return false;
-                        }
-                    } while (false);
-                    // delete, line 46
-                    slice_del();
-                    break;
-                case 3:
-                    // (, line 48
-                    // <-, line 48
-                    slice_from("er");
-                    break;
-            }
-            return true;
-        }
-
-        private boolean r_consonant_pair() {
-            int v_1;
-            int v_2;
-            int v_3;
-            // (, line 52
-            // test, line 53
-            v_1 = limit - cursor;
-            // (, line 53
-            // setlimit, line 54
-            v_2 = limit - cursor;
-            // tomark, line 54
-            if (cursor < I_p1)
-            {
-                return false;
-            }
-            cursor = I_p1;
-            v_3 = limit_backward;
-            limit_backward = cursor;
-            cursor = limit - v_2;
-            // (, line 54
-            // [, line 54
-            ket = cursor;
-            // substring, line 54
-            if (find_among_b(a_1, 2) == 0)
-            {
-                limit_backward = v_3;
-                return false;
-            }
-            // ], line 54
-            bra = cursor;
-            limit_backward = v_3;
-            cursor = limit - v_1;
-            // next, line 59
-            if (cursor <= limit_backward)
-            {
-                return false;
-            }
-            cursor--;
-            // ], line 59
-            bra = cursor;
-            // delete, line 59
-            slice_del();
-            return true;
-        }
-
-        private boolean r_other_suffix() {
-            int among_var;
-            int v_1;
-            int v_2;
-            // (, line 62
-            // setlimit, line 63
-            v_1 = limit - cursor;
-            // tomark, line 63
-            if (cursor < I_p1)
-            {
-                return false;
-            }
-            cursor = I_p1;
-            v_2 = limit_backward;
-            limit_backward = cursor;
-            cursor = limit - v_1;
-            // (, line 63
-            // [, line 63
-            ket = cursor;
-            // substring, line 63
-            among_var = find_among_b(a_2, 11);
-            if (among_var == 0)
-            {
-                limit_backward = v_2;
-                return false;
-            }
-            // ], line 63
-            bra = cursor;
-            limit_backward = v_2;
-            switch(among_var) {
-                case 0:
-                    return false;
-                case 1:
-                    // (, line 67
-                    // delete, line 67
-                    slice_del();
-                    break;
-            }
-            return true;
-        }
-
-        public boolean stem() {
-            int v_1;
-            int v_2;
-            int v_3;
-            int v_4;
-            // (, line 72
-            // do, line 74
-            v_1 = cursor;
-            lab0: do {
-                // call mark_regions, line 74
-                if (!r_mark_regions())
-                {
-                    break lab0;
-                }
-            } while (false);
-            cursor = v_1;
-            // backwards, line 75
-            limit_backward = cursor; cursor = limit;
-            // (, line 75
-            // do, line 76
-            v_2 = limit - cursor;
-            lab1: do {
-                // call main_suffix, line 76
-                if (!r_main_suffix())
-                {
-                    break lab1;
-                }
-            } while (false);
-            cursor = limit - v_2;
-            // do, line 77
-            v_3 = limit - cursor;
-            lab2: do {
-                // call consonant_pair, line 77
-                if (!r_consonant_pair())
-                {
-                    break lab2;
-                }
-            } while (false);
-            cursor = limit - v_3;
-            // do, line 78
-            v_4 = limit - cursor;
-            lab3: do {
-                // call other_suffix, line 78
-                if (!r_other_suffix())
-                {
-                    break lab3;
-                }
-            } while (false);
-            cursor = limit - v_4;
-            cursor = limit_backward;            return true;
-        }
-
-}
-
Index: contrib/snowball/src/java/org/tartarus/snowball/ext/TurkishStemmer.java
===================================================================
--- contrib/snowball/src/java/org/tartarus/snowball/ext/TurkishStemmer.java	(revision 916567)
+++ contrib/snowball/src/java/org/tartarus/snowball/ext/TurkishStemmer.java	(working copy)
@@ -1,3130 +0,0 @@
-// This file was generated automatically by the Snowball to Java compiler
-
-package org.tartarus.snowball.ext;
-import org.tartarus.snowball.SnowballProgram;
-import org.tartarus.snowball.Among;
-
-/**
- * Generated class implementing code defined by a snowball script.
- */
-public class TurkishStemmer extends SnowballProgram {
-
-        private Among a_0[] = {
-            new Among ( "m", -1, -1, "", this),
-            new Among ( "n", -1, -1, "", this),
-            new Among ( "miz", -1, -1, "", this),
-            new Among ( "niz", -1, -1, "", this),
-            new Among ( "muz", -1, -1, "", this),
-            new Among ( "nuz", -1, -1, "", this),
-            new Among ( "m\u00FCz", -1, -1, "", this),
-            new Among ( "n\u00FCz", -1, -1, "", this),
-            new Among ( "m\u0131z", -1, -1, "", this),
-            new Among ( "n\u0131z", -1, -1, "", this)
-        };
-
-        private Among a_1[] = {
-            new Among ( "leri", -1, -1, "", this),
-            new Among ( "lar\u0131", -1, -1, "", this)
-        };
-
-        private Among a_2[] = {
-            new Among ( "ni", -1, -1, "", this),
-            new Among ( "nu", -1, -1, "", this),
-            new Among ( "n\u00FC", -1, -1, "", this),
-            new Among ( "n\u0131", -1, -1, "", this)
-        };
-
-        private Among a_3[] = {
-            new Among ( "in", -1, -1, "", this),
-            new Among ( "un", -1, -1, "", this),
-            new Among ( "\u00FCn", -1, -1, "", this),
-            new Among ( "\u0131n", -1, -1, "", this)
-        };
-
-        private Among a_4[] = {
-            new Among ( "a", -1, -1, "", this),
-            new Among ( "e", -1, -1, "", this)
-        };
-
-        private Among a_5[] = {
-            new Among ( "na", -1, -1, "", this),
-            new Among ( "ne", -1, -1, "", this)
-        };
-
-        private Among a_6[] = {
-            new Among ( "da", -1, -1, "", this),
-            new Among ( "ta", -1, -1, "", this),
-            new Among ( "de", -1, -1, "", this),
-            new Among ( "te", -1, -1, "", this)
-        };
-
-        private Among a_7[] = {
-            new Among ( "nda", -1, -1, "", this),
-            new Among ( "nde", -1, -1, "", this)
-        };
-
-        private Among a_8[] = {
-            new Among ( "dan", -1, -1, "", this),
-            new Among ( "tan", -1, -1, "", this),
-            new Among ( "den", -1, -1, "", this),
-            new Among ( "ten", -1, -1, "", this)
-        };
-
-        private Among a_9[] = {
-            new Among ( "ndan", -1, -1, "", this),
-            new Among ( "nden", -1, -1, "", this)
-        };
-
-        private Among a_10[] = {
-            new Among ( "la", -1, -1, "", this),
-            new Among ( "le", -1, -1, "", this)
-        };
-
-        private Among a_11[] = {
-            new Among ( "ca", -1, -1, "", this),
-            new Among ( "ce", -1, -1, "", this)
-        };
-
-        private Among a_12[] = {
-            new Among ( "im", -1, -1, "", this),
-            new Among ( "um", -1, -1, "", this),
-            new Among ( "\u00FCm", -1, -1, "", this),
-            new Among ( "\u0131m", -1, -1, "", this)
-        };
-
-        private Among a_13[] = {
-            new Among ( "sin", -1, -1, "", this),
-            new Among ( "sun", -1, -1, "", this),
-            new Among ( "s\u00FCn", -1, -1, "", this),
-            new Among ( "s\u0131n", -1, -1, "", this)
-        };
-
-        private Among a_14[] = {
-            new Among ( "iz", -1, -1, "", this),
-            new Among ( "uz", -1, -1, "", this),
-            new Among ( "\u00FCz", -1, -1, "", this),
-            new Among ( "\u0131z", -1, -1, "", this)
-        };
-
-        private Among a_15[] = {
-            new Among ( "siniz", -1, -1, "", this),
-            new Among ( "sunuz", -1, -1, "", this),
-            new Among ( "s\u00FCn\u00FCz", -1, -1, "", this),
-            new Among ( "s\u0131n\u0131z", -1, -1, "", this)
-        };
-
-        private Among a_16[] = {
-            new Among ( "lar", -1, -1, "", this),
-            new Among ( "ler", -1, -1, "", this)
-        };
-
-        private Among a_17[] = {
-            new Among ( "niz", -1, -1, "", this),
-            new Among ( "nuz", -1, -1, "", this),
-            new Among ( "n\u00FCz", -1, -1, "", this),
-            new Among ( "n\u0131z", -1, -1, "", this)
-        };
-
-        private Among a_18[] = {
-            new Among ( "dir", -1, -1, "", this),
-            new Among ( "tir", -1, -1, "", this),
-            new Among ( "dur", -1, -1, "", this),
-            new Among ( "tur", -1, -1, "", this),
-            new Among ( "d\u00FCr", -1, -1, "", this),
-            new Among ( "t\u00FCr", -1, -1, "", this),
-            new Among ( "d\u0131r", -1, -1, "", this),
-            new Among ( "t\u0131r", -1, -1, "", this)
-        };
-
-        private Among a_19[] = {
-            new Among ( "cas\u0131na", -1, -1, "", this),
-            new Among ( "cesine", -1, -1, "", this)
-        };
-
-        private Among a_20[] = {
-            new Among ( "di", -1, -1, "", this),
-            new Among ( "ti", -1, -1, "", this),
-            new Among ( "dik", -1, -1, "", this),
-            new Among ( "tik", -1, -1, "", this),
-            new Among ( "duk", -1, -1, "", this),
-            new Among ( "tuk", -1, -1, "", this),
-            new Among ( "d\u00FCk", -1, -1, "", this),
-            new Among ( "t\u00FCk", -1, -1, "", this),
-            new Among ( "d\u0131k", -1, -1, "", this),
-            new Among ( "t\u0131k", -1, -1, "", this),
-            new Among ( "dim", -1, -1, "", this),
-            new Among ( "tim", -1, -1, "", this),
-            new Among ( "dum", -1, -1, "", this),
-            new Among ( "tum", -1, -1, "", this),
-            new Among ( "d\u00FCm", -1, -1, "", this),
-            new Among ( "t\u00FCm", -1, -1, "", this),
-            new Among ( "d\u0131m", -1, -1, "", this),
-            new Among ( "t\u0131m", -1, -1, "", this),
-            new Among ( "din", -1, -1, "", this),
-            new Among ( "tin", -1, -1, "", this),
-            new Among ( "dun", -1, -1, "", this),
-            new Among ( "tun", -1, -1, "", this),
-            new Among ( "d\u00FCn", -1, -1, "", this),
-            new Among ( "t\u00FCn", -1, -1, "", this),
-            new Among ( "d\u0131n", -1, -1, "", this),
-            new Among ( "t\u0131n", -1, -1, "", this),
-            new Among ( "du", -1, -1, "", this),
-            new Among ( "tu", -1, -1, "", this),
-            new Among ( "d\u00FC", -1, -1, "", this),
-            new Among ( "t\u00FC", -1, -1, "", this),
-            new Among ( "d\u0131", -1, -1, "", this),
-            new Among ( "t\u0131", -1, -1, "", this)
-        };
-
-        private Among a_21[] = {
-            new Among ( "sa", -1, -1, "", this),
-            new Among ( "se", -1, -1, "", this),
-            new Among ( "sak", -1, -1, "", this),
-            new Among ( "sek", -1, -1, "", this),
-            new Among ( "sam", -1, -1, "", this),
-            new Among ( "sem", -1, -1, "", this),
-            new Among ( "san", -1, -1, "", this),
-            new Among ( "sen", -1, -1, "", this)
-        };
-
-        private Among a_22[] = {
-            new Among ( "mi\u015F", -1, -1, "", this),
-            new Among ( "mu\u015F", -1, -1, "", this),
-            new Among ( "m\u00FC\u015F", -1, -1, "", this),
-            new Among ( "m\u0131\u015F", -1, -1, "", this)
-        };
-
-        private Among a_23[] = {
-            new Among ( "b", -1, 1, "", this),
-            new Among ( "c", -1, 2, "", this),
-            new Among ( "d", -1, 3, "", this),
-            new Among ( "\u011F", -1, 4, "", this)
-        };
-
-        private static final char g_vowel[] = {17, 65, 16, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 32, 8, 0, 0, 0, 0, 0, 0, 1 };
-
-        private static final char g_U[] = {1, 16, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 1 };
-
-        private static final char g_vowel1[] = {1, 64, 16, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1 };
-
-        private static final char g_vowel2[] = {17, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 130 };
-
-        private static final char g_vowel3[] = {1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1 };
-
-        private static final char g_vowel4[] = {17 };
-
-        private static final char g_vowel5[] = {65 };
-
-        private static final char g_vowel6[] = {65 };
-
-        private boolean B_continue_stemming_noun_suffixes;
-        private int I_strlen;
-
-        private void copy_from(TurkishStemmer other) {
-            B_continue_stemming_noun_suffixes = other.B_continue_stemming_noun_suffixes;
-            I_strlen = other.I_strlen;
-            super.copy_from(other);
-        }
-
-        private boolean r_check_vowel_harmony() {
-            int v_1;
-            int v_2;
-            int v_3;
-            int v_4;
-            int v_5;
-            int v_6;
-            int v_7;
-            int v_8;
-            int v_9;
-            int v_10;
-            int v_11;
-            // (, line 111
-            // test, line 112
-            v_1 = limit - cursor;
-            // (, line 113
-            // (, line 114
-            // goto, line 114
-            golab0: while(true)
-            {
-                v_2 = limit - cursor;
-                lab1: do {
-                    if (!(in_grouping_b(g_vowel, 97, 305)))
-                    {
-                        break lab1;
-                    }
-                    cursor = limit - v_2;
-                    break golab0;
-                } while (false);
-                cursor = limit - v_2;
-                if (cursor <= limit_backward)
-                {
-                    return false;
-                }
-                cursor--;
-            }
-            // (, line 115
-            // or, line 116
-            lab2: do {
-                v_3 = limit - cursor;
-                lab3: do {
-                    // (, line 116
-                    // literal, line 116
-                    if (!(eq_s_b(1, "a")))
-                    {
-                        break lab3;
-                    }
-                    // goto, line 116
-                    golab4: while(true)
-                    {
-                        v_4 = limit - cursor;
-                        lab5: do {
-                            if (!(in_grouping_b(g_vowel1, 97, 305)))
-                            {
-                                break lab5;
-                            }
-                            cursor = limit - v_4;
-                            break golab4;
-                        } while (false);
-                        cursor = limit - v_4;
-                        if (cursor <= limit_backward)
-                        {
-                            break lab3;
-                        }
-                        cursor--;
-                    }
-                    break lab2;
-                } while (false);
-                cursor = limit - v_3;
-                lab6: do {
-                    // (, line 117
-                    // literal, line 117
-                    if (!(eq_s_b(1, "e")))
-                    {
-                        break lab6;
-                    }
-                    // goto, line 117
-                    golab7: while(true)
-                    {
-                        v_5 = limit - cursor;
-                        lab8: do {
-                            if (!(in_grouping_b(g_vowel2, 101, 252)))
-                            {
-                                break lab8;
-                            }
-                            cursor = limit - v_5;
-                            break golab7;
-                        } while (false);
-                        cursor = limit - v_5;
-                        if (cursor <= limit_backward)
-                        {
-                            break lab6;
-                        }
-                        cursor--;
-                    }
-                    break lab2;
-                } while (false);
-                cursor = limit - v_3;
-                lab9: do {
-                    // (, line 118
-                    // literal, line 118
-                    if (!(eq_s_b(1, "\u0131")))
-                    {
-                        break lab9;
-                    }
-                    // goto, line 118
-                    golab10: while(true)
-                    {
-                        v_6 = limit - cursor;
-                        lab11: do {
-                            if (!(in_grouping_b(g_vowel3, 97, 305)))
-                            {
-                                break lab11;
-                            }
-                            cursor = limit - v_6;
-                            break golab10;
-                        } while (false);
-                        cursor = limit - v_6;
-                        if (cursor <= limit_backward)
-                        {
-                            break lab9;
-                        }
-                        cursor--;
-                    }
-                    break lab2;
-                } while (false);
-                cursor = limit - v_3;
-                lab12: do {
-                    // (, line 119
-                    // literal, line 119
-                    if (!(eq_s_b(1, "i")))
-                    {
-                        break lab12;
-                    }
-                    // goto, line 119
-                    golab13: while(true)
-                    {
-                        v_7 = limit - cursor;
-                        lab14: do {
-                            if (!(in_grouping_b(g_vowel4, 101, 105)))
-                            {
-                                break lab14;
-                            }
-                            cursor = limit - v_7;
-                            break golab13;
-                        } while (false);
-                        cursor = limit - v_7;
-                        if (cursor <= limit_backward)
-                        {
-                            break lab12;
-                        }
-                        cursor--;
-                    }
-                    break lab2;
-                } while (false);
-                cursor = limit - v_3;
-                lab15: do {
-                    // (, line 120
-                    // literal, line 120
-                    if (!(eq_s_b(1, "o")))
-                    {
-                        break lab15;
-                    }
-                    // goto, line 120
-                    golab16: while(true)
-                    {
-                        v_8 = limit - cursor;
-                        lab17: do {
-                            if (!(in_grouping_b(g_vowel5, 111, 117)))
-                            {
-                                break lab17;
-                            }
-                            cursor = limit - v_8;
-                            break golab16;
-                        } while (false);
-                        cursor = limit - v_8;
-                        if (cursor <= limit_backward)
-                        {
-                            break lab15;
-                        }
-                        cursor--;
-                    }
-                    break lab2;
-                } while (false);
-                cursor = limit - v_3;
-                lab18: do {
-                    // (, line 121
-                    // literal, line 121
-                    if (!(eq_s_b(1, "\u00F6")))
-                    {
-                        break lab18;
-                    }
-                    // goto, line 121
-                    golab19: while(true)
-                    {
-                        v_9 = limit - cursor;
-                        lab20: do {
-                            if (!(in_grouping_b(g_vowel6, 246, 252)))
-                            {
-                                break lab20;
-                            }
-                            cursor = limit - v_9;
-                            break golab19;
-                        } while (false);
-                        cursor = limit - v_9;
-                        if (cursor <= limit_backward)
-                        {
-                            break lab18;
-                        }
-                        cursor--;
-                    }
-                    break lab2;
-                } while (false);
-                cursor = limit - v_3;
-                lab21: do {
-                    // (, line 122
-                    // literal, line 122
-                    if (!(eq_s_b(1, "u")))
-                    {
-                        break lab21;
-                    }
-                    // goto, line 122
-                    golab22: while(true)
-                    {
-                        v_10 = limit - cursor;
-                        lab23: do {
-                            if (!(in_grouping_b(g_vowel5, 111, 117)))
-                            {
-                                break lab23;
-                            }
-                            cursor = limit - v_10;
-                            break golab22;
-                        } while (false);
-                        cursor = limit - v_10;
-                        if (cursor <= limit_backward)
-                        {
-                            break lab21;
-                        }
-                        cursor--;
-                    }
-                    break lab2;
-                } while (false);
-                cursor = limit - v_3;
-                // (, line 123
-                // literal, line 123
-                if (!(eq_s_b(1, "\u00FC")))
-                {
-                    return false;
-                }
-                // goto, line 123
-                golab24: while(true)
-                {
-                    v_11 = limit - cursor;
-                    lab25: do {
-                        if (!(in_grouping_b(g_vowel6, 246, 252)))
-                        {
-                            break lab25;
-                        }
-                        cursor = limit - v_11;
-                        break golab24;
-                    } while (false);
-                    cursor = limit - v_11;
-                    if (cursor <= limit_backward)
-                    {
-                        return false;
-                    }
-                    cursor--;
-                }
-            } while (false);
-            cursor = limit - v_1;
-            return true;
-        }
-
-        private boolean r_mark_suffix_with_optional_n_consonant() {
-            int v_1;
-            int v_2;
-            int v_3;
-            int v_4;
-            int v_5;
-            int v_6;
-            int v_7;
-            // (, line 132
-            // or, line 134
-            lab0: do {
-                v_1 = limit - cursor;
-                lab1: do {
-                    // (, line 133
-                    // (, line 133
-                    // test, line 133
-                    v_2 = limit - cursor;
-                    // literal, line 133
-                    if (!(eq_s_b(1, "n")))
-                    {
-                        break lab1;
-                    }
-                    cursor = limit - v_2;
-                    // next, line 133
-                    if (cursor <= limit_backward)
-                    {
-                        break lab1;
-                    }
-                    cursor--;
-                    // (, line 133
-                    // test, line 133
-                    v_3 = limit - cursor;
-                    if (!(in_grouping_b(g_vowel, 97, 305)))
-                    {
-                        break lab1;
-                    }
-                    cursor = limit - v_3;
-                    break lab0;
-                } while (false);
-                cursor = limit - v_1;
-                // (, line 135
-                // (, line 135
-                // not, line 135
-                {
-                    v_4 = limit - cursor;
-                    lab2: do {
-                        // (, line 135
-                        // test, line 135
-                        v_5 = limit - cursor;
-                        // literal, line 135
-                        if (!(eq_s_b(1, "n")))
-                        {
-                            break lab2;
-                        }
-                        cursor = limit - v_5;
-                        return false;
-                    } while (false);
-                    cursor = limit - v_4;
-                }
-                // test, line 135
-                v_6 = limit - cursor;
-                // (, line 135
-                // next, line 135
-                if (cursor <= limit_backward)
-                {
-                    return false;
-                }
-                cursor--;
-                // (, line 135
-                // test, line 135
-                v_7 = limit - cursor;
-                if (!(in_grouping_b(g_vowel, 97, 305)))
-                {
-                    return false;
-                }
-                cursor = limit - v_7;
-                cursor = limit - v_6;
-            } while (false);
-            return true;
-        }
-
-        private boolean r_mark_suffix_with_optional_s_consonant() {
-            int v_1;
-            int v_2;
-            int v_3;
-            int v_4;
-            int v_5;
-            int v_6;
-            int v_7;
-            // (, line 143
-            // or, line 145
-            lab0: do {
-                v_1 = limit - cursor;
-                lab1: do {
-                    // (, line 144
-                    // (, line 144
-                    // test, line 144
-                    v_2 = limit - cursor;
-                    // literal, line 144
-                    if (!(eq_s_b(1, "s")))
-                    {
-                        break lab1;
-                    }
-                    cursor = limit - v_2;
-                    // next, line 144
-                    if (cursor <= limit_backward)
-                    {
-                        break lab1;
-                    }
-                    cursor--;
-                    // (, line 144
-                    // test, line 144
-                    v_3 = limit - cursor;
-                    if (!(in_grouping_b(g_vowel, 97, 305)))
-                    {
-                        break lab1;
-                    }
-                    cursor = limit - v_3;
-                    break lab0;
-                } while (false);
-                cursor = limit - v_1;
-                // (, line 146
-                // (, line 146
-                // not, line 146
-                {
-                    v_4 = limit - cursor;
-                    lab2: do {
-                        // (, line 146
-                        // test, line 146
-                        v_5 = limit - cursor;
-                        // literal, line 146
-                        if (!(eq_s_b(1, "s")))
-                        {
-                            break lab2;
-                        }
-                        cursor = limit - v_5;
-                        return false;
-                    } while (false);
-                    cursor = limit - v_4;
-                }
-                // test, line 146
-                v_6 = limit - cursor;
-                // (, line 146
-                // next, line 146
-                if (cursor <= limit_backward)
-                {
-                    return false;
-                }
-                cursor--;
-                // (, line 146
-                // test, line 146
-                v_7 = limit - cursor;
-                if (!(in_grouping_b(g_vowel, 97, 305)))
-                {
-                    return false;
-                }
-                cursor = limit - v_7;
-                cursor = limit - v_6;
-            } while (false);
-            return true;
-        }
-
-        private boolean r_mark_suffix_with_optional_y_consonant() {
-            int v_1;
-            int v_2;
-            int v_3;
-            int v_4;
-            int v_5;
-            int v_6;
-            int v_7;
-            // (, line 153
-            // or, line 155
-            lab0: do {
-                v_1 = limit - cursor;
-                lab1: do {
-                    // (, line 154
-                    // (, line 154
-                    // test, line 154
-                    v_2 = limit - cursor;
-                    // literal, line 154
-                    if (!(eq_s_b(1, "y")))
-                    {
-                        break lab1;
-                    }
-                    cursor = limit - v_2;
-                    // next, line 154
-                    if (cursor <= limit_backward)
-                    {
-                        break lab1;
-                    }
-                    cursor--;
-                    // (, line 154
-                    // test, line 154
-                    v_3 = limit - cursor;
-                    if (!(in_grouping_b(g_vowel, 97, 305)))
-                    {
-                        break lab1;
-                    }
-                    cursor = limit - v_3;
-                    break lab0;
-                } while (false);
-                cursor = limit - v_1;
-                // (, line 156
-                // (, line 156
-                // not, line 156
-                {
-                    v_4 = limit - cursor;
-                    lab2: do {
-                        // (, line 156
-                        // test, line 156
-                        v_5 = limit - cursor;
-                        // literal, line 156
-                        if (!(eq_s_b(1, "y")))
-                        {
-                            break lab2;
-                        }
-                        cursor = limit - v_5;
-                        return false;
-                    } while (false);
-                    cursor = limit - v_4;
-                }
-                // test, line 156
-                v_6 = limit - cursor;
-                // (, line 156
-                // next, line 156
-                if (cursor <= limit_backward)
-                {
-                    return false;
-                }
-                cursor--;
-                // (, line 156
-                // test, line 156
-                v_7 = limit - cursor;
-                if (!(in_grouping_b(g_vowel, 97, 305)))
-                {
-                    return false;
-                }
-                cursor = limit - v_7;
-                cursor = limit - v_6;
-            } while (false);
-            return true;
-        }
-
-        private boolean r_mark_suffix_with_optional_U_vowel() {
-            int v_1;
-            int v_2;
-            int v_3;
-            int v_4;
-            int v_5;
-            int v_6;
-            int v_7;
-            // (, line 159
-            // or, line 161
-            lab0: do {
-                v_1 = limit - cursor;
-                lab1: do {
-                    // (, line 160
-                    // (, line 160
-                    // test, line 160
-                    v_2 = limit - cursor;
-                    if (!(in_grouping_b(g_U, 105, 305)))
-                    {
-                        break lab1;
-                    }
-                    cursor = limit - v_2;
-                    // next, line 160
-                    if (cursor <= limit_backward)
-                    {
-                        break lab1;
-                    }
-                    cursor--;
-                    // (, line 160
-                    // test, line 160
-                    v_3 = limit - cursor;
-                    if (!(out_grouping_b(g_vowel, 97, 305)))
-                    {
-                        break lab1;
-                    }
-                    cursor = limit - v_3;
-                    break lab0;
-                } while (false);
-                cursor = limit - v_1;
-                // (, line 162
-                // (, line 162
-                // not, line 162
-                {
-                    v_4 = limit - cursor;
-                    lab2: do {
-                        // (, line 162
-                        // test, line 162
-                        v_5 = limit - cursor;
-                        if (!(in_grouping_b(g_U, 105, 305)))
-                        {
-                            break lab2;
-                        }
-                        cursor = limit - v_5;
-                        return false;
-                    } while (false);
-                    cursor = limit - v_4;
-                }
-                // test, line 162
-                v_6 = limit - cursor;
-                // (, line 162
-                // next, line 162
-                if (cursor <= limit_backward)
-                {
-                    return false;
-                }
-                cursor--;
-                // (, line 162
-                // test, line 162
-                v_7 = limit - cursor;
-                if (!(out_grouping_b(g_vowel, 97, 305)))
-                {
-                    return false;
-                }
-                cursor = limit - v_7;
-                cursor = limit - v_6;
-            } while (false);
-            return true;
-        }
-
-        private boolean r_mark_possessives() {
-            // (, line 166
-            // among, line 167
-            if (find_among_b(a_0, 10) == 0)
-            {
-                return false;
-            }
-            // (, line 169
-            // call mark_suffix_with_optional_U_vowel, line 169
-            if (!r_mark_suffix_with_optional_U_vowel())
-            {
-                return false;
-            }
-            return true;
-        }
-
-        private boolean r_mark_sU() {
-            // (, line 172
-            // call check_vowel_harmony, line 173
-            if (!r_check_vowel_harmony())
-            {
-                return false;
-            }
-            if (!(in_grouping_b(g_U, 105, 305)))
-            {
-                return false;
-            }
-            // (, line 175
-            // call mark_suffix_with_optional_s_consonant, line 175
-            if (!r_mark_suffix_with_optional_s_consonant())
-            {
-                return false;
-            }
-            return true;
-        }
-
-        private boolean r_mark_lArI() {
-            // (, line 178
-            // among, line 179
-            if (find_among_b(a_1, 2) == 0)
-            {
-                return false;
-            }
-            return true;
-        }
-
-        private boolean r_mark_yU() {
-            // (, line 182
-            // call check_vowel_harmony, line 183
-            if (!r_check_vowel_harmony())
-            {
-                return false;
-            }
-            if (!(in_grouping_b(g_U, 105, 305)))
-            {
-                return false;
-            }
-            // (, line 185
-            // call mark_suffix_with_optional_y_consonant, line 185
-            if (!r_mark_suffix_with_optional_y_consonant())
-            {
-                return false;
-            }
-            return true;
-        }
-
-        private boolean r_mark_nU() {
-            // (, line 188
-            // call check_vowel_harmony, line 189
-            if (!r_check_vowel_harmony())
-            {
-                return false;
-            }
-            // among, line 190
-            if (find_among_b(a_2, 4) == 0)
-            {
-                return false;
-            }
-            return true;
-        }
-
-        private boolean r_mark_nUn() {
-            // (, line 193
-            // call check_vowel_harmony, line 194
-            if (!r_check_vowel_harmony())
-            {
-                return false;
-            }
-            // among, line 195
-            if (find_among_b(a_3, 4) == 0)
-            {
-                return false;
-            }
-            // (, line 196
-            // call mark_suffix_with_optional_n_consonant, line 196
-            if (!r_mark_suffix_with_optional_n_consonant())
-            {
-                return false;
-            }
-            return true;
-        }
-
-        private boolean r_mark_yA() {
-            // (, line 199
-            // call check_vowel_harmony, line 200
-            if (!r_check_vowel_harmony())
-            {
-                return false;
-            }
-            // among, line 201
-            if (find_among_b(a_4, 2) == 0)
-            {
-                return false;
-            }
-            // (, line 202
-            // call mark_suffix_with_optional_y_consonant, line 202
-            if (!r_mark_suffix_with_optional_y_consonant())
-            {
-                return false;
-            }
-            return true;
-        }
-
-        private boolean r_mark_nA() {
-            // (, line 205
-            // call check_vowel_harmony, line 206
-            if (!r_check_vowel_harmony())
-            {
-                return false;
-            }
-            // among, line 207
-            if (find_among_b(a_5, 2) == 0)
-            {
-                return false;
-            }
-            return true;
-        }
-
-        private boolean r_mark_DA() {
-            // (, line 210
-            // call check_vowel_harmony, line 211
-            if (!r_check_vowel_harmony())
-            {
-                return false;
-            }
-            // among, line 212
-            if (find_among_b(a_6, 4) == 0)
-            {
-                return false;
-            }
-            return true;
-        }
-
-        private boolean r_mark_ndA() {
-            // (, line 215
-            // call check_vowel_harmony, line 216
-            if (!r_check_vowel_harmony())
-            {
-                return false;
-            }
-            // among, line 217
-            if (find_among_b(a_7, 2) == 0)
-            {
-                return false;
-            }
-            return true;
-        }
-
-        private boolean r_mark_DAn() {
-            // (, line 220
-            // call check_vowel_harmony, line 221
-            if (!r_check_vowel_harmony())
-            {
-                return false;
-            }
-            // among, line 222
-            if (find_among_b(a_8, 4) == 0)
-            {
-                return false;
-            }
-            return true;
-        }
-
-        private boolean r_mark_ndAn() {
-            // (, line 225
-            // call check_vowel_harmony, line 226
-            if (!r_check_vowel_harmony())
-            {
-                return false;
-            }
-            // among, line 227
-            if (find_among_b(a_9, 2) == 0)
-            {
-                return false;
-            }
-            return true;
-        }
-
-        private boolean r_mark_ylA() {
-            // (, line 230
-            // call check_vowel_harmony, line 231
-            if (!r_check_vowel_harmony())
-            {
-                return false;
-            }
-            // among, line 232
-            if (find_among_b(a_10, 2) == 0)
-            {
-                return false;
-            }
-            // (, line 233
-            // call mark_suffix_with_optional_y_consonant, line 233
-            if (!r_mark_suffix_with_optional_y_consonant())
-            {
-                return false;
-            }
-            return true;
-        }
-
-        private boolean r_mark_ki() {
-            // (, line 236
-            // literal, line 237
-            if (!(eq_s_b(2, "ki")))
-            {
-                return false;
-            }
-            return true;
-        }
-
-        private boolean r_mark_ncA() {
-            // (, line 240
-            // call check_vowel_harmony, line 241
-            if (!r_check_vowel_harmony())
-            {
-                return false;
-            }
-            // among, line 242
-            if (find_among_b(a_11, 2) == 0)
-            {
-                return false;
-            }
-            // (, line 243
-            // call mark_suffix_with_optional_n_consonant, line 243
-            if (!r_mark_suffix_with_optional_n_consonant())
-            {
-                return false;
-            }
-            return true;
-        }
-
-        private boolean r_mark_yUm() {
-            // (, line 246
-            // call check_vowel_harmony, line 247
-            if (!r_check_vowel_harmony())
-            {
-                return false;
-            }
-            // among, line 248
-            if (find_among_b(a_12, 4) == 0)
-            {
-                return false;
-            }
-            // (, line 249
-            // call mark_suffix_with_optional_y_consonant, line 249
-            if (!r_mark_suffix_with_optional_y_consonant())
-            {
-                return false;
-            }
-            return true;
-        }
-
-        private boolean r_mark_sUn() {
-            // (, line 252
-            // call check_vowel_harmony, line 253
-            if (!r_check_vowel_harmony())
-            {
-                return false;
-            }
-            // among, line 254
-            if (find_among_b(a_13, 4) == 0)
-            {
-                return false;
-            }
-            return true;
-        }
-
-        private boolean r_mark_yUz() {
-            // (, line 257
-            // call check_vowel_harmony, line 258
-            if (!r_check_vowel_harmony())
-            {
-                return false;
-            }
-            // among, line 259
-            if (find_among_b(a_14, 4) == 0)
-            {
-                return false;
-            }
-            // (, line 260
-            // call mark_suffix_with_optional_y_consonant, line 260
-            if (!r_mark_suffix_with_optional_y_consonant())
-            {
-                return false;
-            }
-            return true;
-        }
-
-        private boolean r_mark_sUnUz() {
-            // (, line 263
-            // among, line 264
-            if (find_among_b(a_15, 4) == 0)
-            {
-                return false;
-            }
-            return true;
-        }
-
-        private boolean r_mark_lAr() {
-            // (, line 267
-            // call check_vowel_harmony, line 268
-            if (!r_check_vowel_harmony())
-            {
-                return false;
-            }
-            // among, line 269
-            if (find_among_b(a_16, 2) == 0)
-            {
-                return false;
-            }
-            return true;
-        }
-
-        private boolean r_mark_nUz() {
-            // (, line 272
-            // call check_vowel_harmony, line 273
-            if (!r_check_vowel_harmony())
-            {
-                return false;
-            }
-            // among, line 274
-            if (find_among_b(a_17, 4) == 0)
-            {
-                return false;
-            }
-            return true;
-        }
-
-        private boolean r_mark_DUr() {
-            // (, line 277
-            // call check_vowel_harmony, line 278
-            if (!r_check_vowel_harmony())
-            {
-                return false;
-            }
-            // among, line 279
-            if (find_among_b(a_18, 8) == 0)
-            {
-                return false;
-            }
-            return true;
-        }
-
-        private boolean r_mark_cAsInA() {
-            // (, line 282
-            // among, line 283
-            if (find_among_b(a_19, 2) == 0)
-            {
-                return false;
-            }
-            return true;
-        }
-
-        private boolean r_mark_yDU() {
-            // (, line 286
-            // call check_vowel_harmony, line 287
-            if (!r_check_vowel_harmony())
-            {
-                return false;
-            }
-            // among, line 288
-            if (find_among_b(a_20, 32) == 0)
-            {
-                return false;
-            }
-            // (, line 292
-            // call mark_suffix_with_optional_y_consonant, line 292
-            if (!r_mark_suffix_with_optional_y_consonant())
-            {
-                return false;
-            }
-            return true;
-        }
-
-        private boolean r_mark_ysA() {
-            // (, line 296
-            // among, line 297
-            if (find_among_b(a_21, 8) == 0)
-            {
-                return false;
-            }
-            // (, line 298
-            // call mark_suffix_with_optional_y_consonant, line 298
-            if (!r_mark_suffix_with_optional_y_consonant())
-            {
-                return false;
-            }
-            return true;
-        }
-
-        private boolean r_mark_ymUs_() {
-            // (, line 301
-            // call check_vowel_harmony, line 302
-            if (!r_check_vowel_harmony())
-            {
-                return false;
-            }
-            // among, line 303
-            if (find_among_b(a_22, 4) == 0)
-            {
-                return false;
-            }
-            // (, line 304
-            // call mark_suffix_with_optional_y_consonant, line 304
-            if (!r_mark_suffix_with_optional_y_consonant())
-            {
-                return false;
-            }
-            return true;
-        }
-
-        private boolean r_mark_yken() {
-            // (, line 307
-            // literal, line 308
-            if (!(eq_s_b(3, "ken")))
-            {
-                return false;
-            }
-            // (, line 308
-            // call mark_suffix_with_optional_y_consonant, line 308
-            if (!r_mark_suffix_with_optional_y_consonant())
-            {
-                return false;
-            }
-            return true;
-        }
-
-        private boolean r_stem_nominal_verb_suffixes() {
-            int v_1;
-            int v_2;
-            int v_3;
-            int v_4;
-            int v_5;
-            int v_6;
-            int v_7;
-            int v_8;
-            int v_9;
-            int v_10;
-            // (, line 311
-            // [, line 312
-            ket = cursor;
-            // set continue_stemming_noun_suffixes, line 313
-            B_continue_stemming_noun_suffixes = true;
-            // or, line 315
-            lab0: do {
-                v_1 = limit - cursor;
-                lab1: do {
-                    // (, line 314
-                    // or, line 314
-                    lab2: do {
-                        v_2 = limit - cursor;
-                        lab3: do {
-                            // call mark_ymUs_, line 314
-                            if (!r_mark_ymUs_())
-                            {
-                                break lab3;
-                            }
-                            break lab2;
-                        } while (false);
-                        cursor = limit - v_2;
-                        lab4: do {
-                            // call mark_yDU, line 314
-                            if (!r_mark_yDU())
-                            {
-                                break lab4;
-                            }
-                            break lab2;
-                        } while (false);
-                        cursor = limit - v_2;
-                        lab5: do {
-                            // call mark_ysA, line 314
-                            if (!r_mark_ysA())
-                            {
-                                break lab5;
-                            }
-                            break lab2;
-                        } while (false);
-                        cursor = limit - v_2;
-                        // call mark_yken, line 314
-                        if (!r_mark_yken())
-                        {
-                            break lab1;
-                        }
-                    } while (false);
-                    break lab0;
-                } while (false);
-                cursor = limit - v_1;
-                lab6: do {
-                    // (, line 316
-                    // call mark_cAsInA, line 316
-                    if (!r_mark_cAsInA())
-                    {
-                        break lab6;
-                    }
-                    // (, line 316
-                    // or, line 316
-                    lab7: do {
-                        v_3 = limit - cursor;
-                        lab8: do {
-                            // call mark_sUnUz, line 316
-                            if (!r_mark_sUnUz())
-                            {
-                                break lab8;
-                            }
-                            break lab7;
-                        } while (false);
-                        cursor = limit - v_3;
-                        lab9: do {
-                            // call mark_lAr, line 316
-                            if (!r_mark_lAr())
-                            {
-                                break lab9;
-                            }
-                            break lab7;
-                        } while (false);
-                        cursor = limit - v_3;
-                        lab10: do {
-                            // call mark_yUm, line 316
-                            if (!r_mark_yUm())
-                            {
-                                break lab10;
-                            }
-                            break lab7;
-                        } while (false);
-                        cursor = limit - v_3;
-                        lab11: do {
-                            // call mark_sUn, line 316
-                            if (!r_mark_sUn())
-                            {
-                                break lab11;
-                            }
-                            break lab7;
-                        } while (false);
-                        cursor = limit - v_3;
-                        lab12: do {
-                            // call mark_yUz, line 316
-                            if (!r_mark_yUz())
-                            {
-                                break lab12;
-                            }
-                            break lab7;
-                        } while (false);
-                        cursor = limit - v_3;
-                    } while (false);
-                    // call mark_ymUs_, line 316
-                    if (!r_mark_ymUs_())
-                    {
-                        break lab6;
-                    }
-                    break lab0;
-                } while (false);
-                cursor = limit - v_1;
-                lab13: do {
-                    // (, line 318
-                    // call mark_lAr, line 319
-                    if (!r_mark_lAr())
-                    {
-                        break lab13;
-                    }
-                    // ], line 319
-                    bra = cursor;
-                    // delete, line 319
-                    slice_del();
-                    // try, line 319
-                    v_4 = limit - cursor;
-                    lab14: do {
-                        // (, line 319
-                        // [, line 319
-                        ket = cursor;
-                        // (, line 319
-                        // or, line 319
-                        lab15: do {
-                            v_5 = limit - cursor;
-                            lab16: do {
-                                // call mark_DUr, line 319
-                                if (!r_mark_DUr())
-                                {
-                                    break lab16;
-                                }
-                                break lab15;
-                            } while (false);
-                            cursor = limit - v_5;
-                            lab17: do {
-                                // call mark_yDU, line 319
-                                if (!r_mark_yDU())
-                                {
-                                    break lab17;
-                                }
-                                break lab15;
-                            } while (false);
-                            cursor = limit - v_5;
-                            lab18: do {
-                                // call mark_ysA, line 319
-                                if (!r_mark_ysA())
-                                {
-                                    break lab18;
-                                }
-                                break lab15;
-                            } while (false);
-                            cursor = limit - v_5;
-                            // call mark_ymUs_, line 319
-                            if (!r_mark_ymUs_())
-                            {
-                                cursor = limit - v_4;
-                                break lab14;
-                            }
-                        } while (false);
-                    } while (false);
-                    // unset continue_stemming_noun_suffixes, line 320
-                    B_continue_stemming_noun_suffixes = false;
-                    break lab0;
-                } while (false);
-                cursor = limit - v_1;
-                lab19: do {
-                    // (, line 323
-                    // call mark_nUz, line 323
-                    if (!r_mark_nUz())
-                    {
-                        break lab19;
-                    }
-                    // (, line 323
-                    // or, line 323
-                    lab20: do {
-                        v_6 = limit - cursor;
-                        lab21: do {
-                            // call mark_yDU, line 323
-                            if (!r_mark_yDU())
-                            {
-                                break lab21;
-                            }
-                            break lab20;
-                        } while (false);
-                        cursor = limit - v_6;
-                        // call mark_ysA, line 323
-                        if (!r_mark_ysA())
-                        {
-                            break lab19;
-                        }
-                    } while (false);
-                    break lab0;
-                } while (false);
-                cursor = limit - v_1;
-                lab22: do {
-                    // (, line 325
-                    // (, line 325
-                    // or, line 325
-                    lab23: do {
-                        v_7 = limit - cursor;
-                        lab24: do {
-                            // call mark_sUnUz, line 325
-                            if (!r_mark_sUnUz())
-                            {
-                                break lab24;
-                            }
-                            break lab23;
-                        } while (false);
-                        cursor = limit - v_7;
-                        lab25: do {
-                            // call mark_yUz, line 325
-                            if (!r_mark_yUz())
-                            {
-                                break lab25;
-                            }
-                            break lab23;
-                        } while (false);
-                        cursor = limit - v_7;
-                        lab26: do {
-                            // call mark_sUn, line 325
-                            if (!r_mark_sUn())
-                            {
-                                break lab26;
-                            }
-                            break lab23;
-                        } while (false);
-                        cursor = limit - v_7;
-                        // call mark_yUm, line 325
-                        if (!r_mark_yUm())
-                        {
-                            break lab22;
-                        }
-                    } while (false);
-                    // ], line 325
-                    bra = cursor;
-                    // delete, line 325
-                    slice_del();
-                    // try, line 325
-                    v_8 = limit - cursor;
-                    lab27: do {
-                        // (, line 325
-                        // [, line 325
-                        ket = cursor;
-                        // call mark_ymUs_, line 325
-                        if (!r_mark_ymUs_())
-                        {
-                            cursor = limit - v_8;
-                            break lab27;
-                        }
-                    } while (false);
-                    break lab0;
-                } while (false);
-                cursor = limit - v_1;
-                // (, line 327
-                // call mark_DUr, line 327
-                if (!r_mark_DUr())
-                {
-                    return false;
-                }
-                // ], line 327
-                bra = cursor;
-                // delete, line 327
-                slice_del();
-                // try, line 327
-                v_9 = limit - cursor;
-                lab28: do {
-                    // (, line 327
-                    // [, line 327
-                    ket = cursor;
-                    // (, line 327
-                    // or, line 327
-                    lab29: do {
-                        v_10 = limit - cursor;
-                        lab30: do {
-                            // call mark_sUnUz, line 327
-                            if (!r_mark_sUnUz())
-                            {
-                                break lab30;
-                            }
-                            break lab29;
-                        } while (false);
-                        cursor = limit - v_10;
-                        lab31: do {
-                            // call mark_lAr, line 327
-                            if (!r_mark_lAr())
-                            {
-                                break lab31;
-                            }
-                            break lab29;
-                        } while (false);
-                        cursor = limit - v_10;
-                        lab32: do {
-                            // call mark_yUm, line 327
-                            if (!r_mark_yUm())
-                            {
-                                break lab32;
-                            }
-                            break lab29;
-                        } while (false);
-                        cursor = limit - v_10;
-                        lab33: do {
-                            // call mark_sUn, line 327
-                            if (!r_mark_sUn())
-                            {
-                                break lab33;
-                            }
-                            break lab29;
-                        } while (false);
-                        cursor = limit - v_10;
-                        lab34: do {
-                            // call mark_yUz, line 327
-                            if (!r_mark_yUz())
-                            {
-                                break lab34;
-                            }
-                            break lab29;
-                        } while (false);
-                        cursor = limit - v_10;
-                    } while (false);
-                    // call mark_ymUs_, line 327
-                    if (!r_mark_ymUs_())
-                    {
-                        cursor = limit - v_9;
-                        break lab28;
-                    }
-                } while (false);
-            } while (false);
-            // ], line 328
-            bra = cursor;
-            // delete, line 328
-            slice_del();
-            return true;
-        }
-
-        private boolean r_stem_suffix_chain_before_ki() {
-            int v_1;
-            int v_2;
-            int v_3;
-            int v_4;
-            int v_5;
-            int v_6;
-            int v_7;
-            int v_8;
-            int v_9;
-            int v_10;
-            int v_11;
-            // (, line 332
-            // [, line 333
-            ket = cursor;
-            // call mark_ki, line 334
-            if (!r_mark_ki())
-            {
-                return false;
-            }
-            // (, line 335
-            // or, line 342
-            lab0: do {
-                v_1 = limit - cursor;
-                lab1: do {
-                    // (, line 336
-                    // call mark_DA, line 336
-                    if (!r_mark_DA())
-                    {
-                        break lab1;
-                    }
-                    // ], line 336
-                    bra = cursor;
-                    // delete, line 336
-                    slice_del();
-                    // try, line 336
-                    v_2 = limit - cursor;
-                    lab2: do {
-                        // (, line 336
-                        // [, line 336
-                        ket = cursor;
-                        // or, line 338
-                        lab3: do {
-                            v_3 = limit - cursor;
-                            lab4: do {
-                                // (, line 337
-                                // call mark_lAr, line 337
-                                if (!r_mark_lAr())
-                                {
-                                    break lab4;
-                                }
-                                // ], line 337
-                                bra = cursor;
-                                // delete, line 337
-                                slice_del();
-                                // try, line 337
-                                v_4 = limit - cursor;
-                                lab5: do {
-                                    // (, line 337
-                                    // call stem_suffix_chain_before_ki, line 337
-                                    if (!r_stem_suffix_chain_before_ki())
-                                    {
-                                        cursor = limit - v_4;
-                                        break lab5;
-                                    }
-                                } while (false);
-                                break lab3;
-                            } while (false);
-                            cursor = limit - v_3;
-                            // (, line 339
-                            // call mark_possessives, line 339
-                            if (!r_mark_possessives())
-                            {
-                                cursor = limit - v_2;
-                                break lab2;
-                            }
-                            // ], line 339
-                            bra = cursor;
-                            // delete, line 339
-                            slice_del();
-                            // try, line 339
-                            v_5 = limit - cursor;
-                            lab6: do {
-                                // (, line 339
-                                // [, line 339
-                                ket = cursor;
-                                // call mark_lAr, line 339
-                                if (!r_mark_lAr())
-                                {
-                                    cursor = limit - v_5;
-                                    break lab6;
-                                }
-                                // ], line 339
-                                bra = cursor;
-                                // delete, line 339
-                                slice_del();
-                                // call stem_suffix_chain_before_ki, line 339
-                                if (!r_stem_suffix_chain_before_ki())
-                                {
-                                    cursor = limit - v_5;
-                                    break lab6;
-                                }
-                            } while (false);
-                        } while (false);
-                    } while (false);
-                    break lab0;
-                } while (false);
-                cursor = limit - v_1;
-                lab7: do {
-                    // (, line 343
-                    // call mark_nUn, line 343
-                    if (!r_mark_nUn())
-                    {
-                        break lab7;
-                    }
-                    // ], line 343
-                    bra = cursor;
-                    // delete, line 343
-                    slice_del();
-                    // try, line 343
-                    v_6 = limit - cursor;
-                    lab8: do {
-                        // (, line 343
-                        // [, line 343
-                        ket = cursor;
-                        // or, line 345
-                        lab9: do {
-                            v_7 = limit - cursor;
-                            lab10: do {
-                                // (, line 344
-                                // call mark_lArI, line 344
-                                if (!r_mark_lArI())
-                                {
-                                    break lab10;
-                                }
-                                // ], line 344
-                                bra = cursor;
-                                // delete, line 344
-                                slice_del();
-                                break lab9;
-                            } while (false);
-                            cursor = limit - v_7;
-                            lab11: do {
-                                // (, line 346
-                                // [, line 346
-                                ket = cursor;
-                                // or, line 346
-                                lab12: do {
-                                    v_8 = limit - cursor;
-                                    lab13: do {
-                                        // call mark_possessives, line 346
-                                        if (!r_mark_possessives())
-                                        {
-                                            break lab13;
-                                        }
-                                        break lab12;
-                                    } while (false);
-                                    cursor = limit - v_8;
-                                    // call mark_sU, line 346
-                                    if (!r_mark_sU())
-                                    {
-                                        break lab11;
-                                    }
-                                } while (false);
-                                // ], line 346
-                                bra = cursor;
-                                // delete, line 346
-                                slice_del();
-                                // try, line 346
-                                v_9 = limit - cursor;
-                                lab14: do {
-                                    // (, line 346
-                                    // [, line 346
-                                    ket = cursor;
-                                    // call mark_lAr, line 346
-                                    if (!r_mark_lAr())
-                                    {
-                                        cursor = limit - v_9;
-                                        break lab14;
-                                    }
-                                    // ], line 346
-                                    bra = cursor;
-                                    // delete, line 346
-                                    slice_del();
-                                    // call stem_suffix_chain_before_ki, line 346
-                                    if (!r_stem_suffix_chain_before_ki())
-                                    {
-                                        cursor = limit - v_9;
-                                        break lab14;
-                                    }
-                                } while (false);
-                                break lab9;
-                            } while (false);
-                            cursor = limit - v_7;
-                            // (, line 348
-                            // call stem_suffix_chain_before_ki, line 348
-                            if (!r_stem_suffix_chain_before_ki())
-                            {
-                                cursor = limit - v_6;
-                                break lab8;
-                            }
-                        } while (false);
-                    } while (false);
-                    break lab0;
-                } while (false);
-                cursor = limit - v_1;
-                // (, line 351
-                // call mark_ndA, line 351
-                if (!r_mark_ndA())
-                {
-                    return false;
-                }
-                // (, line 351
-                // or, line 353
-                lab15: do {
-                    v_10 = limit - cursor;
-                    lab16: do {
-                        // (, line 352
-                        // call mark_lArI, line 352
-                        if (!r_mark_lArI())
-                        {
-                            break lab16;
-                        }
-                        // ], line 352
-                        bra = cursor;
-                        // delete, line 352
-                        slice_del();
-                        break lab15;
-                    } while (false);
-                    cursor = limit - v_10;
-                    lab17: do {
-                        // (, line 354
-                        // (, line 354
-                        // call mark_sU, line 354
-                        if (!r_mark_sU())
-                        {
-                            break lab17;
-                        }
-                        // ], line 354
-                        bra = cursor;
-                        // delete, line 354
-                        slice_del();
-                        // try, line 354
-                        v_11 = limit - cursor;
-                        lab18: do {
-                            // (, line 354
-                            // [, line 354
-                            ket = cursor;
-                            // call mark_lAr, line 354
-                            if (!r_mark_lAr())
-                            {
-                                cursor = limit - v_11;
-                                break lab18;
-                            }
-                            // ], line 354
-                            bra = cursor;
-                            // delete, line 354
-                            slice_del();
-                            // call stem_suffix_chain_before_ki, line 354
-                            if (!r_stem_suffix_chain_before_ki())
-                            {
-                                cursor = limit - v_11;
-                                break lab18;
-                            }
-                        } while (false);
-                        break lab15;
-                    } while (false);
-                    cursor = limit - v_10;
-                    // (, line 356
-                    // call stem_suffix_chain_before_ki, line 356
-                    if (!r_stem_suffix_chain_before_ki())
-                    {
-                        return false;
-                    }
-                } while (false);
-            } while (false);
-            return true;
-        }
-
-        private boolean r_stem_noun_suffixes() {
-            int v_1;
-            int v_2;
-            int v_3;
-            int v_4;
-            int v_5;
-            int v_6;
-            int v_7;
-            int v_8;
-            int v_9;
-            int v_10;
-            int v_11;
-            int v_12;
-            int v_13;
-            int v_14;
-            int v_15;
-            int v_16;
-            int v_17;
-            int v_18;
-            int v_19;
-            int v_20;
-            int v_21;
-            int v_22;
-            int v_23;
-            int v_24;
-            int v_25;
-            int v_26;
-            int v_27;
-            // (, line 361
-            // or, line 363
-            lab0: do {
-                v_1 = limit - cursor;
-                lab1: do {
-                    // (, line 362
-                    // [, line 362
-                    ket = cursor;
-                    // call mark_lAr, line 362
-                    if (!r_mark_lAr())
-                    {
-                        break lab1;
-                    }
-                    // ], line 362
-                    bra = cursor;
-                    // delete, line 362
-                    slice_del();
-                    // try, line 362
-                    v_2 = limit - cursor;
-                    lab2: do {
-                        // (, line 362
-                        // call stem_suffix_chain_before_ki, line 362
-                        if (!r_stem_suffix_chain_before_ki())
-                        {
-                            cursor = limit - v_2;
-                            break lab2;
-                        }
-                    } while (false);
-                    break lab0;
-                } while (false);
-                cursor = limit - v_1;
-                lab3: do {
-                    // (, line 364
-                    // [, line 364
-                    ket = cursor;
-                    // call mark_ncA, line 364
-                    if (!r_mark_ncA())
-                    {
-                        break lab3;
-                    }
-                    // ], line 364
-                    bra = cursor;
-                    // delete, line 364
-                    slice_del();
-                    // try, line 365
-                    v_3 = limit - cursor;
-                    lab4: do {
-                        // (, line 365
-                        // or, line 367
-                        lab5: do {
-                            v_4 = limit - cursor;
-                            lab6: do {
-                                // (, line 366
-                                // [, line 366
-                                ket = cursor;
-                                // call mark_lArI, line 366
-                                if (!r_mark_lArI())
-                                {
-                                    break lab6;
-                                }
-                                // ], line 366
-                                bra = cursor;
-                                // delete, line 366
-                                slice_del();
-                                break lab5;
-                            } while (false);
-                            cursor = limit - v_4;
-                            lab7: do {
-                                // (, line 368
-                                // [, line 368
-                                ket = cursor;
-                                // or, line 368
-                                lab8: do {
-                                    v_5 = limit - cursor;
-                                    lab9: do {
-                                        // call mark_possessives, line 368
-                                        if (!r_mark_possessives())
-                                        {
-                                            break lab9;
-                                        }
-                                        break lab8;
-                                    } while (false);
-                                    cursor = limit - v_5;
-                                    // call mark_sU, line 368
-                                    if (!r_mark_sU())
-                                    {
-                                        break lab7;
-                                    }
-                                } while (false);
-                                // ], line 368
-                                bra = cursor;
-                                // delete, line 368
-                                slice_del();
-                                // try, line 368
-                                v_6 = limit - cursor;
-                                lab10: do {
-                                    // (, line 368
-                                    // [, line 368
-                                    ket = cursor;
-                                    // call mark_lAr, line 368
-                                    if (!r_mark_lAr())
-                                    {
-                                        cursor = limit - v_6;
-                                        break lab10;
-                                    }
-                                    // ], line 368
-                                    bra = cursor;
-                                    // delete, line 368
-                                    slice_del();
-                                    // call stem_suffix_chain_before_ki, line 368
-                                    if (!r_stem_suffix_chain_before_ki())
-                                    {
-                                        cursor = limit - v_6;
-                                        break lab10;
-                                    }
-                                } while (false);
-                                break lab5;
-                            } while (false);
-                            cursor = limit - v_4;
-                            // (, line 370
-                            // [, line 370
-                            ket = cursor;
-                            // call mark_lAr, line 370
-                            if (!r_mark_lAr())
-                            {
-                                cursor = limit - v_3;
-                                break lab4;
-                            }
-                            // ], line 370
-                            bra = cursor;
-                            // delete, line 370
-                            slice_del();
-                            // call stem_suffix_chain_before_ki, line 370
-                            if (!r_stem_suffix_chain_before_ki())
-                            {
-                                cursor = limit - v_3;
-                                break lab4;
-                            }
-                        } while (false);
-                    } while (false);
-                    break lab0;
-                } while (false);
-                cursor = limit - v_1;
-                lab11: do {
-                    // (, line 374
-                    // [, line 374
-                    ket = cursor;
-                    // (, line 374
-                    // or, line 374
-                    lab12: do {
-                        v_7 = limit - cursor;
-                        lab13: do {
-                            // call mark_ndA, line 374
-                            if (!r_mark_ndA())
-                            {
-                                break lab13;
-                            }
-                            break lab12;
-                        } while (false);
-                        cursor = limit - v_7;
-                        // call mark_nA, line 374
-                        if (!r_mark_nA())
-                        {
-                            break lab11;
-                        }
-                    } while (false);
-                    // (, line 375
-                    // or, line 377
-                    lab14: do {
-                        v_8 = limit - cursor;
-                        lab15: do {
-                            // (, line 376
-                            // call mark_lArI, line 376
-                            if (!r_mark_lArI())
-                            {
-                                break lab15;
-                            }
-                            // ], line 376
-                            bra = cursor;
-                            // delete, line 376
-                            slice_del();
-                            break lab14;
-                        } while (false);
-                        cursor = limit - v_8;
-                        lab16: do {
-                            // (, line 378
-                            // call mark_sU, line 378
-                            if (!r_mark_sU())
-                            {
-                                break lab16;
-                            }
-                            // ], line 378
-                            bra = cursor;
-                            // delete, line 378
-                            slice_del();
-                            // try, line 378
-                            v_9 = limit - cursor;
-                            lab17: do {
-                                // (, line 378
-                                // [, line 378
-                                ket = cursor;
-                                // call mark_lAr, line 378
-                                if (!r_mark_lAr())
-                                {
-                                    cursor = limit - v_9;
-                                    break lab17;
-                                }
-                                // ], line 378
-                                bra = cursor;
-                                // delete, line 378
-                                slice_del();
-                                // call stem_suffix_chain_before_ki, line 378
-                                if (!r_stem_suffix_chain_before_ki())
-                                {
-                                    cursor = limit - v_9;
-                                    break lab17;
-                                }
-                            } while (false);
-                            break lab14;
-                        } while (false);
-                        cursor = limit - v_8;
-                        // (, line 380
-                        // call stem_suffix_chain_before_ki, line 380
-                        if (!r_stem_suffix_chain_before_ki())
-                        {
-                            break lab11;
-                        }
-                    } while (false);
-                    break lab0;
-                } while (false);
-                cursor = limit - v_1;
-                lab18: do {
-                    // (, line 384
-                    // [, line 384
-                    ket = cursor;
-                    // (, line 384
-                    // or, line 384
-                    lab19: do {
-                        v_10 = limit - cursor;
-                        lab20: do {
-                            // call mark_ndAn, line 384
-                            if (!r_mark_ndAn())
-                            {
-                                break lab20;
-                            }
-                            break lab19;
-                        } while (false);
-                        cursor = limit - v_10;
-                        // call mark_nU, line 384
-                        if (!r_mark_nU())
-                        {
-                            break lab18;
-                        }
-                    } while (false);
-                    // (, line 384
-                    // or, line 384
-                    lab21: do {
-                        v_11 = limit - cursor;
-                        lab22: do {
-                            // (, line 384
-                            // call mark_sU, line 384
-                            if (!r_mark_sU())
-                            {
-                                break lab22;
-                            }
-                            // ], line 384
-                            bra = cursor;
-                            // delete, line 384
-                            slice_del();
-                            // try, line 384
-                            v_12 = limit - cursor;
-                            lab23: do {
-                                // (, line 384
-                                // [, line 384
-                                ket = cursor;
-                                // call mark_lAr, line 384
-                                if (!r_mark_lAr())
-                                {
-                                    cursor = limit - v_12;
-                                    break lab23;
-                                }
-                                // ], line 384
-                                bra = cursor;
-                                // delete, line 384
-                                slice_del();
-                                // call stem_suffix_chain_before_ki, line 384
-                                if (!r_stem_suffix_chain_before_ki())
-                                {
-                                    cursor = limit - v_12;
-                                    break lab23;
-                                }
-                            } while (false);
-                            break lab21;
-                        } while (false);
-                        cursor = limit - v_11;
-                        // (, line 384
-                        // call mark_lArI, line 384
-                        if (!r_mark_lArI())
-                        {
-                            break lab18;
-                        }
-                    } while (false);
-                    break lab0;
-                } while (false);
-                cursor = limit - v_1;
-                lab24: do {
-                    // (, line 386
-                    // [, line 386
-                    ket = cursor;
-                    // call mark_DAn, line 386
-                    if (!r_mark_DAn())
-                    {
-                        break lab24;
-                    }
-                    // ], line 386
-                    bra = cursor;
-                    // delete, line 386
-                    slice_del();
-                    // try, line 386
-                    v_13 = limit - cursor;
-                    lab25: do {
-                        // (, line 386
-                        // [, line 386
-                        ket = cursor;
-                        // (, line 387
-                        // or, line 389
-                        lab26: do {
-                            v_14 = limit - cursor;
-                            lab27: do {
-                                // (, line 388
-                                // call mark_possessives, line 388
-                                if (!r_mark_possessives())
-                                {
-                                    break lab27;
-                                }
-                                // ], line 388
-                                bra = cursor;
-                                // delete, line 388
-                                slice_del();
-                                // try, line 388
-                                v_15 = limit - cursor;
-                                lab28: do {
-                                    // (, line 388
-                                    // [, line 388
-                                    ket = cursor;
-                                    // call mark_lAr, line 388
-                                    if (!r_mark_lAr())
-                                    {
-                                        cursor = limit - v_15;
-                                        break lab28;
-                                    }
-                                    // ], line 388
-                                    bra = cursor;
-                                    // delete, line 388
-                                    slice_del();
-                                    // call stem_suffix_chain_before_ki, line 388
-                                    if (!r_stem_suffix_chain_before_ki())
-                                    {
-                                        cursor = limit - v_15;
-                                        break lab28;
-                                    }
-                                } while (false);
-                                break lab26;
-                            } while (false);
-                            cursor = limit - v_14;
-                            lab29: do {
-                                // (, line 390
-                                // call mark_lAr, line 390
-                                if (!r_mark_lAr())
-                                {
-                                    break lab29;
-                                }
-                                // ], line 390
-                                bra = cursor;
-                                // delete, line 390
-                                slice_del();
-                                // try, line 390
-                                v_16 = limit - cursor;
-                                lab30: do {
-                                    // (, line 390
-                                    // call stem_suffix_chain_before_ki, line 390
-                                    if (!r_stem_suffix_chain_before_ki())
-                                    {
-                                        cursor = limit - v_16;
-                                        break lab30;
-                                    }
-                                } while (false);
-                                break lab26;
-                            } while (false);
-                            cursor = limit - v_14;
-                            // (, line 392
-                            // call stem_suffix_chain_before_ki, line 392
-                            if (!r_stem_suffix_chain_before_ki())
-                            {
-                                cursor = limit - v_13;
-                                break lab25;
-                            }
-                        } while (false);
-                    } while (false);
-                    break lab0;
-                } while (false);
-                cursor = limit - v_1;
-                lab31: do {
-                    // (, line 396
-                    // [, line 396
-                    ket = cursor;
-                    // or, line 396
-                    lab32: do {
-                        v_17 = limit - cursor;
-                        lab33: do {
-                            // call mark_nUn, line 396
-                            if (!r_mark_nUn())
-                            {
-                                break lab33;
-                            }
-                            break lab32;
-                        } while (false);
-                        cursor = limit - v_17;
-                        // call mark_ylA, line 396
-                        if (!r_mark_ylA())
-                        {
-                            break lab31;
-                        }
-                    } while (false);
-                    // ], line 396
-                    bra = cursor;
-                    // delete, line 396
-                    slice_del();
-                    // try, line 397
-                    v_18 = limit - cursor;
-                    lab34: do {
-                        // (, line 397
-                        // or, line 399
-                        lab35: do {
-                            v_19 = limit - cursor;
-                            lab36: do {
-                                // (, line 398
-                                // [, line 398
-                                ket = cursor;
-                                // call mark_lAr, line 398
-                                if (!r_mark_lAr())
-                                {
-                                    break lab36;
-                                }
-                                // ], line 398
-                                bra = cursor;
-                                // delete, line 398
-                                slice_del();
-                                // call stem_suffix_chain_before_ki, line 398
-                                if (!r_stem_suffix_chain_before_ki())
-                                {
-                                    break lab36;
-                                }
-                                break lab35;
-                            } while (false);
-                            cursor = limit - v_19;
-                            lab37: do {
-                                // (, line 400
-                                // [, line 400
-                                ket = cursor;
-                                // or, line 400
-                                lab38: do {
-                                    v_20 = limit - cursor;
-                                    lab39: do {
-                                        // call mark_possessives, line 400
-                                        if (!r_mark_possessives())
-                                        {
-                                            break lab39;
-                                        }
-                                        break lab38;
-                                    } while (false);
-                                    cursor = limit - v_20;
-                                    // call mark_sU, line 400
-                                    if (!r_mark_sU())
-                                    {
-                                        break lab37;
-                                    }
-                                } while (false);
-                                // ], line 400
-                                bra = cursor;
-                                // delete, line 400
-                                slice_del();
-                                // try, line 400
-                                v_21 = limit - cursor;
-                                lab40: do {
-                                    // (, line 400
-                                    // [, line 400
-                                    ket = cursor;
-                                    // call mark_lAr, line 400
-                                    if (!r_mark_lAr())
-                                    {
-                                        cursor = limit - v_21;
-                                        break lab40;
-                                    }
-                                    // ], line 400
-                                    bra = cursor;
-                                    // delete, line 400
-                                    slice_del();
-                                    // call stem_suffix_chain_before_ki, line 400
-                                    if (!r_stem_suffix_chain_before_ki())
-                                    {
-                                        cursor = limit - v_21;
-                                        break lab40;
-                                    }
-                                } while (false);
-                                break lab35;
-                            } while (false);
-                            cursor = limit - v_19;
-                            // call stem_suffix_chain_before_ki, line 402
-                            if (!r_stem_suffix_chain_before_ki())
-                            {
-                                cursor = limit - v_18;
-                                break lab34;
-                            }
-                        } while (false);
-                    } while (false);
-                    break lab0;
-                } while (false);
-                cursor = limit - v_1;
-                lab41: do {
-                    // (, line 406
-                    // [, line 406
-                    ket = cursor;
-                    // call mark_lArI, line 406
-                    if (!r_mark_lArI())
-                    {
-                        break lab41;
-                    }
-                    // ], line 406
-                    bra = cursor;
-                    // delete, line 406
-                    slice_del();
-                    break lab0;
-                } while (false);
-                cursor = limit - v_1;
-                lab42: do {
-                    // (, line 408
-                    // call stem_suffix_chain_before_ki, line 408
-                    if (!r_stem_suffix_chain_before_ki())
-                    {
-                        break lab42;
-                    }
-                    break lab0;
-                } while (false);
-                cursor = limit - v_1;
-                lab43: do {
-                    // (, line 410
-                    // [, line 410
-                    ket = cursor;
-                    // or, line 410
-                    lab44: do {
-                        v_22 = limit - cursor;
-                        lab45: do {
-                            // call mark_DA, line 410
-                            if (!r_mark_DA())
-                            {
-                                break lab45;
-                            }
-                            break lab44;
-                        } while (false);
-                        cursor = limit - v_22;
-                        lab46: do {
-                            // call mark_yU, line 410
-                            if (!r_mark_yU())
-                            {
-                                break lab46;
-                            }
-                            break lab44;
-                        } while (false);
-                        cursor = limit - v_22;
-                        // call mark_yA, line 410
-                        if (!r_mark_yA())
-                        {
-                            break lab43;
-                        }
-                    } while (false);
-                    // ], line 410
-                    bra = cursor;
-                    // delete, line 410
-                    slice_del();
-                    // try, line 410
-                    v_23 = limit - cursor;
-                    lab47: do {
-                        // (, line 410
-                        // [, line 410
-                        ket = cursor;
-                        // (, line 410
-                        // or, line 410
-                        lab48: do {
-                            v_24 = limit - cursor;
-                            lab49: do {
-                                // (, line 410
-                                // call mark_possessives, line 410
-                                if (!r_mark_possessives())
-                                {
-                                    break lab49;
-                                }
-                                // ], line 410
-                                bra = cursor;
-                                // delete, line 410
-                                slice_del();
-                                // try, line 410
-                                v_25 = limit - cursor;
-                                lab50: do {
-                                    // (, line 410
-                                    // [, line 410
-                                    ket = cursor;
-                                    // call mark_lAr, line 410
-                                    if (!r_mark_lAr())
-                                    {
-                                        cursor = limit - v_25;
-                                        break lab50;
-                                    }
-                                } while (false);
-                                break lab48;
-                            } while (false);
-                            cursor = limit - v_24;
-                            // call mark_lAr, line 410
-                            if (!r_mark_lAr())
-                            {
-                                cursor = limit - v_23;
-                                break lab47;
-                            }
-                        } while (false);
-                        // ], line 410
-                        bra = cursor;
-                        // delete, line 410
-                        slice_del();
-                        // [, line 410
-                        ket = cursor;
-                        // call stem_suffix_chain_before_ki, line 410
-                        if (!r_stem_suffix_chain_before_ki())
-                        {
-                            cursor = limit - v_23;
-                            break lab47;
-                        }
-                    } while (false);
-                    break lab0;
-                } while (false);
-                cursor = limit - v_1;
-                // (, line 412
-                // [, line 412
-                ket = cursor;
-                // or, line 412
-                lab51: do {
-                    v_26 = limit - cursor;
-                    lab52: do {
-                        // call mark_possessives, line 412
-                        if (!r_mark_possessives())
-                        {
-                            break lab52;
-                        }
-                        break lab51;
-                    } while (false);
-                    cursor = limit - v_26;
-                    // call mark_sU, line 412
-                    if (!r_mark_sU())
-                    {
-                        return false;
-                    }
-                } while (false);
-                // ], line 412
-                bra = cursor;
-                // delete, line 412
-                slice_del();
-                // try, line 412
-                v_27 = limit - cursor;
-                lab53: do {
-                    // (, line 412
-                    // [, line 412
-                    ket = cursor;
-                    // call mark_lAr, line 412
-                    if (!r_mark_lAr())
-                    {
-                        cursor = limit - v_27;
-                        break lab53;
-                    }
-                    // ], line 412
-                    bra = cursor;
-                    // delete, line 412
-                    slice_del();
-                    // call stem_suffix_chain_before_ki, line 412
-                    if (!r_stem_suffix_chain_before_ki())
-                    {
-                        cursor = limit - v_27;
-                        break lab53;
-                    }
-                } while (false);
-            } while (false);
-            return true;
-        }
-
-        private boolean r_post_process_last_consonants() {
-            int among_var;
-            // (, line 415
-            // [, line 416
-            ket = cursor;
-            // substring, line 416
-            among_var = find_among_b(a_23, 4);
-            if (among_var == 0)
-            {
-                return false;
-            }
-            // ], line 416
-            bra = cursor;
-            switch(among_var) {
-                case 0:
-                    return false;
-                case 1:
-                    // (, line 417
-                    // <-, line 417
-                    slice_from("p");
-                    break;
-                case 2:
-                    // (, line 418
-                    // <-, line 418
-                    slice_from("\u00E7");
-                    break;
-                case 3:
-                    // (, line 419
-                    // <-, line 419
-                    slice_from("t");
-                    break;
-                case 4:
-                    // (, line 420
-                    // <-, line 420
-                    slice_from("k");
-                    break;
-            }
-            return true;
-        }
-
-        private boolean r_append_U_to_stems_ending_with_d_or_g() {
-            int v_1;
-            int v_2;
-            int v_3;
-            int v_4;
-            int v_5;
-            int v_6;
-            int v_7;
-            int v_8;
-            int v_9;
-            int v_10;
-            int v_11;
-            int v_12;
-            int v_13;
-            int v_14;
-            int v_15;
-            // (, line 430
-            // test, line 431
-            v_1 = limit - cursor;
-            // (, line 431
-            // or, line 431
-            lab0: do {
-                v_2 = limit - cursor;
-                lab1: do {
-                    // literal, line 431
-                    if (!(eq_s_b(1, "d")))
-                    {
-                        break lab1;
-                    }
-                    break lab0;
-                } while (false);
-                cursor = limit - v_2;
-                // literal, line 431
-                if (!(eq_s_b(1, "g")))
-                {
-                    return false;
-                }
-            } while (false);
-            cursor = limit - v_1;
-            // or, line 433
-            lab2: do {
-                v_3 = limit - cursor;
-                lab3: do {
-                    // (, line 432
-                    // test, line 432
-                    v_4 = limit - cursor;
-                    // (, line 432
-                    // (, line 432
-                    // goto, line 432
-                    golab4: while(true)
-                    {
-                        v_5 = limit - cursor;
-                        lab5: do {
-                            if (!(in_grouping_b(g_vowel, 97, 305)))
-                            {
-                                break lab5;
-                            }
-                            cursor = limit - v_5;
-                            break golab4;
-                        } while (false);
-                        cursor = limit - v_5;
-                        if (cursor <= limit_backward)
-                        {
-                            break lab3;
-                        }
-                        cursor--;
-                    }
-                    // or, line 432
-                    lab6: do {
-                        v_6 = limit - cursor;
-                        lab7: do {
-                            // literal, line 432
-                            if (!(eq_s_b(1, "a")))
-                            {
-                                break lab7;
-                            }
-                            break lab6;
-                        } while (false);
-                        cursor = limit - v_6;
-                        // literal, line 432
-                        if (!(eq_s_b(1, "\u0131")))
-                        {
-                            break lab3;
-                        }
-                    } while (false);
-                    cursor = limit - v_4;
-                    // <+, line 432
-                    {
-                        int c = cursor;
-                        insert(cursor, cursor, "\u0131");
-                        cursor = c;
-                    }
-                    break lab2;
-                } while (false);
-                cursor = limit - v_3;
-                lab8: do {
-                    // (, line 434
-                    // test, line 434
-                    v_7 = limit - cursor;
-                    // (, line 434
-                    // (, line 434
-                    // goto, line 434
-                    golab9: while(true)
-                    {
-                        v_8 = limit - cursor;
-                        lab10: do {
-                            if (!(in_grouping_b(g_vowel, 97, 305)))
-                            {
-                                break lab10;
-                            }
-                            cursor = limit - v_8;
-                            break golab9;
-                        } while (false);
-                        cursor = limit - v_8;
-                        if (cursor <= limit_backward)
-                        {
-                            break lab8;
-                        }
-                        cursor--;
-                    }
-                    // or, line 434
-                    lab11: do {
-                        v_9 = limit - cursor;
-                        lab12: do {
-                            // literal, line 434
-                            if (!(eq_s_b(1, "e")))
-                            {
-                                break lab12;
-                            }
-                            break lab11;
-                        } while (false);
-                        cursor = limit - v_9;
-                        // literal, line 434
-                        if (!(eq_s_b(1, "i")))
-                        {
-                            break lab8;
-                        }
-                    } while (false);
-                    cursor = limit - v_7;
-                    // <+, line 434
-                    {
-                        int c = cursor;
-                        insert(cursor, cursor, "i");
-                        cursor = c;
-                    }
-                    break lab2;
-                } while (false);
-                cursor = limit - v_3;
-                lab13: do {
-                    // (, line 436
-                    // test, line 436
-                    v_10 = limit - cursor;
-                    // (, line 436
-                    // (, line 436
-                    // goto, line 436
-                    golab14: while(true)
-                    {
-                        v_11 = limit - cursor;
-                        lab15: do {
-                            if (!(in_grouping_b(g_vowel, 97, 305)))
-                            {
-                                break lab15;
-                            }
-                            cursor = limit - v_11;
-                            break golab14;
-                        } while (false);
-                        cursor = limit - v_11;
-                        if (cursor <= limit_backward)
-                        {
-                            break lab13;
-                        }
-                        cursor--;
-                    }
-                    // or, line 436
-                    lab16: do {
-                        v_12 = limit - cursor;
-                        lab17: do {
-                            // literal, line 436
-                            if (!(eq_s_b(1, "o")))
-                            {
-                                break lab17;
-                            }
-                            break lab16;
-                        } while (false);
-                        cursor = limit - v_12;
-                        // literal, line 436
-                        if (!(eq_s_b(1, "u")))
-                        {
-                            break lab13;
-                        }
-                    } while (false);
-                    cursor = limit - v_10;
-                    // <+, line 436
-                    {
-                        int c = cursor;
-                        insert(cursor, cursor, "u");
-                        cursor = c;
-                    }
-                    break lab2;
-                } while (false);
-                cursor = limit - v_3;
-                // (, line 438
-                // test, line 438
-                v_13 = limit - cursor;
-                // (, line 438
-                // (, line 438
-                // goto, line 438
-                golab18: while(true)
-                {
-                    v_14 = limit - cursor;
-                    lab19: do {
-                        if (!(in_grouping_b(g_vowel, 97, 305)))
-                        {
-                            break lab19;
-                        }
-                        cursor = limit - v_14;
-                        break golab18;
-                    } while (false);
-                    cursor = limit - v_14;
-                    if (cursor <= limit_backward)
-                    {
-                        return false;
-                    }
-                    cursor--;
-                }
-                // or, line 438
-                lab20: do {
-                    v_15 = limit - cursor;
-                    lab21: do {
-                        // literal, line 438
-                        if (!(eq_s_b(1, "\u00F6")))
-                        {
-                            break lab21;
-                        }
-                        break lab20;
-                    } while (false);
-                    cursor = limit - v_15;
-                    // literal, line 438
-                    if (!(eq_s_b(1, "\u00FC")))
-                    {
-                        return false;
-                    }
-                } while (false);
-                cursor = limit - v_13;
-                // <+, line 438
-                {
-                    int c = cursor;
-                    insert(cursor, cursor, "\u00FC");
-                    cursor = c;
-                }
-            } while (false);
-            return true;
-        }
-
-        private boolean r_more_than_one_syllable_word() {
-            int v_1;
-            int v_3;
-            // (, line 445
-            // test, line 446
-            v_1 = cursor;
-            // (, line 446
-            // atleast, line 446
-            {
-                int v_2 = 2;
-                // atleast, line 446
-                replab0: while(true)
-                {
-                    v_3 = cursor;
-                    lab1: do {
-                        // (, line 446
-                        // gopast, line 446
-                        golab2: while(true)
-                        {
-                            lab3: do {
-                                if (!(in_grouping(g_vowel, 97, 305)))
-                                {
-                                    break lab3;
-                                }
-                                break golab2;
-                            } while (false);
-                            if (cursor >= limit)
-                            {
-                                break lab1;
-                            }
-                            cursor++;
-                        }
-                        v_2--;
-                        continue replab0;
-                    } while (false);
-                    cursor = v_3;
-                    break replab0;
-                }
-                if (v_2 > 0)
-                {
-                    return false;
-                }
-            }
-            cursor = v_1;
-            return true;
-        }
-
-        private boolean r_is_reserved_word() {
-            int v_1;
-            int v_2;
-            int v_4;
-            // (, line 449
-            // or, line 451
-            lab0: do {
-                v_1 = cursor;
-                lab1: do {
-                    // test, line 450
-                    v_2 = cursor;
-                    // (, line 450
-                    // gopast, line 450
-                    golab2: while(true)
-                    {
-                        lab3: do {
-                            // literal, line 450
-                            if (!(eq_s(2, "ad")))
-                            {
-                                break lab3;
-                            }
-                            break golab2;
-                        } while (false);
-                        if (cursor >= limit)
-                        {
-                            break lab1;
-                        }
-                        cursor++;
-                    }
-                    // (, line 450
-                    I_strlen = 2;
-                    // (, line 450
-                    if (!(I_strlen == limit))
-                    {
-                        break lab1;
-                    }
-                    cursor = v_2;
-                    break lab0;
-                } while (false);
-                cursor = v_1;
-                // test, line 452
-                v_4 = cursor;
-                // (, line 452
-                // gopast, line 452
-                golab4: while(true)
-                {
-                    lab5: do {
-                        // literal, line 452
-                        if (!(eq_s(5, "soyad")))
-                        {
-                            break lab5;
-                        }
-                        break golab4;
-                    } while (false);
-                    if (cursor >= limit)
-                    {
-                        return false;
-                    }
-                    cursor++;
-                }
-                // (, line 452
-                I_strlen = 5;
-                // (, line 452
-                if (!(I_strlen == limit))
-                {
-                    return false;
-                }
-                cursor = v_4;
-            } while (false);
-            return true;
-        }
-
-        private boolean r_postlude() {
-            int v_1;
-            int v_2;
-            int v_3;
-            // (, line 455
-            // not, line 456
-            {
-                v_1 = cursor;
-                lab0: do {
-                    // (, line 456
-                    // call is_reserved_word, line 456
-                    if (!r_is_reserved_word())
-                    {
-                        break lab0;
-                    }
-                    return false;
-                } while (false);
-                cursor = v_1;
-            }
-            // backwards, line 457
-            limit_backward = cursor; cursor = limit;
-            // (, line 457
-            // do, line 458
-            v_2 = limit - cursor;
-            lab1: do {
-                // call append_U_to_stems_ending_with_d_or_g, line 458
-                if (!r_append_U_to_stems_ending_with_d_or_g())
-                {
-                    break lab1;
-                }
-            } while (false);
-            cursor = limit - v_2;
-            // do, line 459
-            v_3 = limit - cursor;
-            lab2: do {
-                // call post_process_last_consonants, line 459
-                if (!r_post_process_last_consonants())
-                {
-                    break lab2;
-                }
-            } while (false);
-            cursor = limit - v_3;
-            cursor = limit_backward;            return true;
-        }
-
-        public boolean stem() {
-            int v_1;
-            int v_2;
-            // (, line 464
-            // (, line 465
-            // call more_than_one_syllable_word, line 465
-            if (!r_more_than_one_syllable_word())
-            {
-                return false;
-            }
-            // (, line 466
-            // backwards, line 467
-            limit_backward = cursor; cursor = limit;
-            // (, line 467
-            // do, line 468
-            v_1 = limit - cursor;
-            lab0: do {
-                // call stem_nominal_verb_suffixes, line 468
-                if (!r_stem_nominal_verb_suffixes())
-                {
-                    break lab0;
-                }
-            } while (false);
-            cursor = limit - v_1;
-            // Boolean test continue_stemming_noun_suffixes, line 469
-            if (!(B_continue_stemming_noun_suffixes))
-            {
-                return false;
-            }
-            // do, line 470
-            v_2 = limit - cursor;
-            lab1: do {
-                // call stem_noun_suffixes, line 470
-                if (!r_stem_noun_suffixes())
-                {
-                    break lab1;
-                }
-            } while (false);
-            cursor = limit - v_2;
-            cursor = limit_backward;            // call postlude, line 473
-            if (!r_postlude())
-            {
-                return false;
-            }
-            return true;
-        }
-
-}
-
Index: contrib/snowball/src/java/org/tartarus/snowball/ext/German2Stemmer.java
===================================================================
--- contrib/snowball/src/java/org/tartarus/snowball/ext/German2Stemmer.java	(revision 916567)
+++ contrib/snowball/src/java/org/tartarus/snowball/ext/German2Stemmer.java	(working copy)
@@ -1,726 +0,0 @@
-// This file was generated automatically by the Snowball to Java compiler
-
-package org.tartarus.snowball.ext;
-import org.tartarus.snowball.SnowballProgram;
-import org.tartarus.snowball.Among;
-
-/**
- * Generated class implementing code defined by a snowball script.
- */
-public class German2Stemmer extends SnowballProgram {
-
-        private Among a_0[] = {
-            new Among ( "", -1, 6, "", this),
-            new Among ( "ae", 0, 2, "", this),
-            new Among ( "oe", 0, 3, "", this),
-            new Among ( "qu", 0, 5, "", this),
-            new Among ( "ue", 0, 4, "", this),
-            new Among ( "\u00DF", 0, 1, "", this)
-        };
-
-        private Among a_1[] = {
-            new Among ( "", -1, 6, "", this),
-            new Among ( "U", 0, 2, "", this),
-            new Among ( "Y", 0, 1, "", this),
-            new Among ( "\u00E4", 0, 3, "", this),
-            new Among ( "\u00F6", 0, 4, "", this),
-            new Among ( "\u00FC", 0, 5, "", this)
-        };
-
-        private Among a_2[] = {
-            new Among ( "e", -1, 1, "", this),
-            new Among ( "em", -1, 1, "", this),
-            new Among ( "en", -1, 1, "", this),
-            new Among ( "ern", -1, 1, "", this),
-            new Among ( "er", -1, 1, "", this),
-            new Among ( "s", -1, 2, "", this),
-            new Among ( "es", 5, 1, "", this)
-        };
-
-        private Among a_3[] = {
-            new Among ( "en", -1, 1, "", this),
-            new Among ( "er", -1, 1, "", this),
-            new Among ( "st", -1, 2, "", this),
-            new Among ( "est", 2, 1, "", this)
-        };
-
-        private Among a_4[] = {
-            new Among ( "ig", -1, 1, "", this),
-            new Among ( "lich", -1, 1, "", this)
-        };
-
-        private Among a_5[] = {
-            new Among ( "end", -1, 1, "", this),
-            new Among ( "ig", -1, 2, "", this),
-            new Among ( "ung", -1, 1, "", this),
-            new Among ( "lich", -1, 3, "", this),
-            new Among ( "isch", -1, 2, "", this),
-            new Among ( "ik", -1, 2, "", this),
-            new Among ( "heit", -1, 3, "", this),
-            new Among ( "keit", -1, 4, "", this)
-        };
-
-        private static final char g_v[] = {17, 65, 16, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 0, 32, 8 };
-
-        private static final char g_s_ending[] = {117, 30, 5 };
-
-        private static final char g_st_ending[] = {117, 30, 4 };
-
-        private int I_x;
-        private int I_p2;
-        private int I_p1;
-
-        private void copy_from(German2Stemmer other) {
-            I_x = other.I_x;
-            I_p2 = other.I_p2;
-            I_p1 = other.I_p1;
-            super.copy_from(other);
-        }
-
-        private boolean r_prelude() {
-            int among_var;
-            int v_1;
-            int v_2;
-            int v_3;
-            int v_4;
-            int v_5;
-            // (, line 28
-            // test, line 30
-            v_1 = cursor;
-            // repeat, line 30
-            replab0: while(true)
-            {
-                v_2 = cursor;
-                lab1: do {
-                    // goto, line 30
-                    golab2: while(true)
-                    {
-                        v_3 = cursor;
-                        lab3: do {
-                            // (, line 30
-                            if (!(in_grouping(g_v, 97, 252)))
-                            {
-                                break lab3;
-                            }
-                            // [, line 31
-                            bra = cursor;
-                            // or, line 31
-                            lab4: do {
-                                v_4 = cursor;
-                                lab5: do {
-                                    // (, line 31
-                                    // literal, line 31
-                                    if (!(eq_s(1, "u")))
-                                    {
-                                        break lab5;
-                                    }
-                                    // ], line 31
-                                    ket = cursor;
-                                    if (!(in_grouping(g_v, 97, 252)))
-                                    {
-                                        break lab5;
-                                    }
-                                    // <-, line 31
-                                    slice_from("U");
-                                    break lab4;
-                                } while (false);
-                                cursor = v_4;
-                                // (, line 32
-                                // literal, line 32
-                                if (!(eq_s(1, "y")))
-                                {
-                                    break lab3;
-                                }
-                                // ], line 32
-                                ket = cursor;
-                                if (!(in_grouping(g_v, 97, 252)))
-                                {
-                                    break lab3;
-                                }
-                                // <-, line 32
-                                slice_from("Y");
-                            } while (false);
-                            cursor = v_3;
-                            break golab2;
-                        } while (false);
-                        cursor = v_3;
-                        if (cursor >= limit)
-                        {
-                            break lab1;
-                        }
-                        cursor++;
-                    }
-                    continue replab0;
-                } while (false);
-                cursor = v_2;
-                break replab0;
-            }
-            cursor = v_1;
-            // repeat, line 35
-            replab6: while(true)
-            {
-                v_5 = cursor;
-                lab7: do {
-                    // (, line 35
-                    // [, line 36
-                    bra = cursor;
-                    // substring, line 36
-                    among_var = find_among(a_0, 6);
-                    if (among_var == 0)
-                    {
-                        break lab7;
-                    }
-                    // ], line 36
-                    ket = cursor;
-                    switch(among_var) {
-                        case 0:
-                            break lab7;
-                        case 1:
-                            // (, line 37
-                            // <-, line 37
-                            slice_from("ss");
-                            break;
-                        case 2:
-                            // (, line 38
-                            // <-, line 38
-                            slice_from("\u00E4");
-                            break;
-                        case 3:
-                            // (, line 39
-                            // <-, line 39
-                            slice_from("\u00F6");
-                            break;
-                        case 4:
-                            // (, line 40
-                            // <-, line 40
-                            slice_from("\u00FC");
-                            break;
-                        case 5:
-                            // (, line 41
-                            // hop, line 41
-                            {
-                                int c = cursor + 2;
-                                if (0 > c || c > limit)
-                                {
-                                    break lab7;
-                                }
-                                cursor = c;
-                            }
-                            break;
-                        case 6:
-                            // (, line 42
-                            // next, line 42
-                            if (cursor >= limit)
-                            {
-                                break lab7;
-                            }
-                            cursor++;
-                            break;
-                    }
-                    continue replab6;
-                } while (false);
-                cursor = v_5;
-                break replab6;
-            }
-            return true;
-        }
-
-        private boolean r_mark_regions() {
-            int v_1;
-            // (, line 48
-            I_p1 = limit;
-            I_p2 = limit;
-            // test, line 53
-            v_1 = cursor;
-            // (, line 53
-            // hop, line 53
-            {
-                int c = cursor + 3;
-                if (0 > c || c > limit)
-                {
-                    return false;
-                }
-                cursor = c;
-            }
-            // setmark x, line 53
-            I_x = cursor;
-            cursor = v_1;
-            // gopast, line 55
-            golab0: while(true)
-            {
-                lab1: do {
-                    if (!(in_grouping(g_v, 97, 252)))
-                    {
-                        break lab1;
-                    }
-                    break golab0;
-                } while (false);
-                if (cursor >= limit)
-                {
-                    return false;
-                }
-                cursor++;
-            }
-            // gopast, line 55
-            golab2: while(true)
-            {
-                lab3: do {
-                    if (!(out_grouping(g_v, 97, 252)))
-                    {
-                        break lab3;
-                    }
-                    break golab2;
-                } while (false);
-                if (cursor >= limit)
-                {
-                    return false;
-                }
-                cursor++;
-            }
-            // setmark p1, line 55
-            I_p1 = cursor;
-            // try, line 56
-            lab4: do {
-                // (, line 56
-                if (!(I_p1 < I_x))
-                {
-                    break lab4;
-                }
-                I_p1 = I_x;
-            } while (false);
-            // gopast, line 57
-            golab5: while(true)
-            {
-                lab6: do {
-                    if (!(in_grouping(g_v, 97, 252)))
-                    {
-                        break lab6;
-                    }
-                    break golab5;
-                } while (false);
-                if (cursor >= limit)
-                {
-                    return false;
-                }
-                cursor++;
-            }
-            // gopast, line 57
-            golab7: while(true)
-            {
-                lab8: do {
-                    if (!(out_grouping(g_v, 97, 252)))
-                    {
-                        break lab8;
-                    }
-                    break golab7;
-                } while (false);
-                if (cursor >= limit)
-                {
-                    return false;
-                }
-                cursor++;
-            }
-            // setmark p2, line 57
-            I_p2 = cursor;
-            return true;
-        }
-
-        private boolean r_postlude() {
-            int among_var;
-            int v_1;
-            // repeat, line 61
-            replab0: while(true)
-            {
-                v_1 = cursor;
-                lab1: do {
-                    // (, line 61
-                    // [, line 63
-                    bra = cursor;
-                    // substring, line 63
-                    among_var = find_among(a_1, 6);
-                    if (among_var == 0)
-                    {
-                        break lab1;
-                    }
-                    // ], line 63
-                    ket = cursor;
-                    switch(among_var) {
-                        case 0:
-                            break lab1;
-                        case 1:
-                            // (, line 64
-                            // <-, line 64
-                            slice_from("y");
-                            break;
-                        case 2:
-                            // (, line 65
-                            // <-, line 65
-                            slice_from("u");
-                            break;
-                        case 3:
-                            // (, line 66
-                            // <-, line 66
-                            slice_from("a");
-                            break;
-                        case 4:
-                            // (, line 67
-                            // <-, line 67
-                            slice_from("o");
-                            break;
-                        case 5:
-                            // (, line 68
-                            // <-, line 68
-                            slice_from("u");
-                            break;
-                        case 6:
-                            // (, line 69
-                            // next, line 69
-                            if (cursor >= limit)
-                            {
-                                break lab1;
-                            }
-                            cursor++;
-                            break;
-                    }
-                    continue replab0;
-                } while (false);
-                cursor = v_1;
-                break replab0;
-            }
-            return true;
-        }
-
-        private boolean r_R1() {
-            if (!(I_p1 <= cursor))
-            {
-                return false;
-            }
-            return true;
-        }
-
-        private boolean r_R2() {
-            if (!(I_p2 <= cursor))
-            {
-                return false;
-            }
-            return true;
-        }
-
-        private boolean r_standard_suffix() {
-            int among_var;
-            int v_1;
-            int v_2;
-            int v_3;
-            int v_4;
-            int v_5;
-            int v_6;
-            int v_7;
-            int v_8;
-            int v_9;
-            // (, line 79
-            // do, line 80
-            v_1 = limit - cursor;
-            lab0: do {
-                // (, line 80
-                // [, line 81
-                ket = cursor;
-                // substring, line 81
-                among_var = find_among_b(a_2, 7);
-                if (among_var == 0)
-                {
-                    break lab0;
-                }
-                // ], line 81
-                bra = cursor;
-                // call R1, line 81
-                if (!r_R1())
-                {
-                    break lab0;
-                }
-                switch(among_var) {
-                    case 0:
-                        break lab0;
-                    case 1:
-                        // (, line 83
-                        // delete, line 83
-                        slice_del();
-                        break;
-                    case 2:
-                        // (, line 86
-                        if (!(in_grouping_b(g_s_ending, 98, 116)))
-                        {
-                            break lab0;
-                        }
-                        // delete, line 86
-                        slice_del();
-                        break;
-                }
-            } while (false);
-            cursor = limit - v_1;
-            // do, line 90
-            v_2 = limit - cursor;
-            lab1: do {
-                // (, line 90
-                // [, line 91
-                ket = cursor;
-                // substring, line 91
-                among_var = find_among_b(a_3, 4);
-                if (among_var == 0)
-                {
-                    break lab1;
-                }
-                // ], line 91
-                bra = cursor;
-                // call R1, line 91
-                if (!r_R1())
-                {
-                    break lab1;
-                }
-                switch(among_var) {
-                    case 0:
-                        break lab1;
-                    case 1:
-                        // (, line 93
-                        // delete, line 93
-                        slice_del();
-                        break;
-                    case 2:
-                        // (, line 96
-                        if (!(in_grouping_b(g_st_ending, 98, 116)))
-                        {
-                            break lab1;
-                        }
-                        // hop, line 96
-                        {
-                            int c = cursor - 3;
-                            if (limit_backward > c || c > limit)
-                            {
-                                break lab1;
-                            }
-                            cursor = c;
-                        }
-                        // delete, line 96
-                        slice_del();
-                        break;
-                }
-            } while (false);
-            cursor = limit - v_2;
-            // do, line 100
-            v_3 = limit - cursor;
-            lab2: do {
-                // (, line 100
-                // [, line 101
-                ket = cursor;
-                // substring, line 101
-                among_var = find_among_b(a_5, 8);
-                if (among_var == 0)
-                {
-                    break lab2;
-                }
-                // ], line 101
-                bra = cursor;
-                // call R2, line 101
-                if (!r_R2())
-                {
-                    break lab2;
-                }
-                switch(among_var) {
-                    case 0:
-                        break lab2;
-                    case 1:
-                        // (, line 103
-                        // delete, line 103
-                        slice_del();
-                        // try, line 104
-                        v_4 = limit - cursor;
-                        lab3: do {
-                            // (, line 104
-                            // [, line 104
-                            ket = cursor;
-                            // literal, line 104
-                            if (!(eq_s_b(2, "ig")))
-                            {
-                                cursor = limit - v_4;
-                                break lab3;
-                            }
-                            // ], line 104
-                            bra = cursor;
-                            // not, line 104
-                            {
-                                v_5 = limit - cursor;
-                                lab4: do {
-                                    // literal, line 104
-                                    if (!(eq_s_b(1, "e")))
-                                    {
-                                        break lab4;
-                                    }
-                                    cursor = limit - v_4;
-                                    break lab3;
-                                } while (false);
-                                cursor = limit - v_5;
-                            }
-                            // call R2, line 104
-                            if (!r_R2())
-                            {
-                                cursor = limit - v_4;
-                                break lab3;
-                            }
-                            // delete, line 104
-                            slice_del();
-                        } while (false);
-                        break;
-                    case 2:
-                        // (, line 107
-                        // not, line 107
-                        {
-                            v_6 = limit - cursor;
-                            lab5: do {
-                                // literal, line 107
-                                if (!(eq_s_b(1, "e")))
-                                {
-                                    break lab5;
-                                }
-                                break lab2;
-                            } while (false);
-                            cursor = limit - v_6;
-                        }
-                        // delete, line 107
-                        slice_del();
-                        break;
-                    case 3:
-                        // (, line 110
-                        // delete, line 110
-                        slice_del();
-                        // try, line 111
-                        v_7 = limit - cursor;
-                        lab6: do {
-                            // (, line 111
-                            // [, line 112
-                            ket = cursor;
-                            // or, line 112
-                            lab7: do {
-                                v_8 = limit - cursor;
-                                lab8: do {
-                                    // literal, line 112
-                                    if (!(eq_s_b(2, "er")))
-                                    {
-                                        break lab8;
-                                    }
-                                    break lab7;
-                                } while (false);
-                                cursor = limit - v_8;
-                                // literal, line 112
-                                if (!(eq_s_b(2, "en")))
-                                {
-                                    cursor = limit - v_7;
-                                    break lab6;
-                                }
-                            } while (false);
-                            // ], line 112
-                            bra = cursor;
-                            // call R1, line 112
-                            if (!r_R1())
-                            {
-                                cursor = limit - v_7;
-                                break lab6;
-                            }
-                            // delete, line 112
-                            slice_del();
-                        } while (false);
-                        break;
-                    case 4:
-                        // (, line 116
-                        // delete, line 116
-                        slice_del();
-                        // try, line 117
-                        v_9 = limit - cursor;
-                        lab9: do {
-                            // (, line 117
-                            // [, line 118
-                            ket = cursor;
-                            // substring, line 118
-                            among_var = find_among_b(a_4, 2);
-                            if (among_var == 0)
-                            {
-                                cursor = limit - v_9;
-                                break lab9;
-                            }
-                            // ], line 118
-                            bra = cursor;
-                            // call R2, line 118
-                            if (!r_R2())
-                            {
-                                cursor = limit - v_9;
-                                break lab9;
-                            }
-                            switch(among_var) {
-                                case 0:
-                                    cursor = limit - v_9;
-                                    break lab9;
-                                case 1:
-                                    // (, line 120
-                                    // delete, line 120
-                                    slice_del();
-                                    break;
-                            }
-                        } while (false);
-                        break;
-                }
-            } while (false);
-            cursor = limit - v_3;
-            return true;
-        }
-
-        public boolean stem() {
-            int v_1;
-            int v_2;
-            int v_3;
-            int v_4;
-            // (, line 130
-            // do, line 131
-            v_1 = cursor;
-            lab0: do {
-                // call prelude, line 131
-                if (!r_prelude())
-                {
-                    break lab0;
-                }
-            } while (false);
-            cursor = v_1;
-            // do, line 132
-            v_2 = cursor;
-            lab1: do {
-                // call mark_regions, line 132
-                if (!r_mark_regions())
-                {
-                    break lab1;
-                }
-            } while (false);
-            cursor = v_2;
-            // backwards, line 133
-            limit_backward = cursor; cursor = limit;
-            // do, line 134
-            v_3 = limit - cursor;
-            lab2: do {
-                // call standard_suffix, line 134
-                if (!r_standard_suffix())
-                {
-                    break lab2;
-                }
-            } while (false);
-            cursor = limit - v_3;
-            cursor = limit_backward;            // do, line 135
-            v_4 = cursor;
-            lab3: do {
-                // call postlude, line 135
-                if (!r_postlude())
-                {
-                    break lab3;
-                }
-            } while (false);
-            cursor = v_4;
-            return true;
-        }
-
-}
-
Index: contrib/snowball/src/java/org/tartarus/snowball/ext/FinnishStemmer.java
===================================================================
--- contrib/snowball/src/java/org/tartarus/snowball/ext/FinnishStemmer.java	(revision 916567)
+++ contrib/snowball/src/java/org/tartarus/snowball/ext/FinnishStemmer.java	(working copy)
@@ -1,1034 +0,0 @@
-// This file was generated automatically by the Snowball to Java compiler
-
-package org.tartarus.snowball.ext;
-import org.tartarus.snowball.SnowballProgram;
-import org.tartarus.snowball.Among;
-
-/**
- * Generated class implementing code defined by a snowball script.
- */
-public class FinnishStemmer extends SnowballProgram {
-
-        private Among a_0[] = {
-            new Among ( "pa", -1, 1, "", this),
-            new Among ( "sti", -1, 2, "", this),
-            new Among ( "kaan", -1, 1, "", this),
-            new Among ( "han", -1, 1, "", this),
-            new Among ( "kin", -1, 1, "", this),
-            new Among ( "h\u00E4n", -1, 1, "", this),
-            new Among ( "k\u00E4\u00E4n", -1, 1, "", this),
-            new Among ( "ko", -1, 1, "", this),
-            new Among ( "p\u00E4", -1, 1, "", this),
-            new Among ( "k\u00F6", -1, 1, "", this)
-        };
-
-        private Among a_1[] = {
-            new Among ( "lla", -1, -1, "", this),
-            new Among ( "na", -1, -1, "", this),
-            new Among ( "ssa", -1, -1, "", this),
-            new Among ( "ta", -1, -1, "", this),
-            new Among ( "lta", 3, -1, "", this),
-            new Among ( "sta", 3, -1, "", this)
-        };
-
-        private Among a_2[] = {
-            new Among ( "ll\u00E4", -1, -1, "", this),
-            new Among ( "n\u00E4", -1, -1, "", this),
-            new Among ( "ss\u00E4", -1, -1, "", this),
-            new Among ( "t\u00E4", -1, -1, "", this),
-            new Among ( "lt\u00E4", 3, -1, "", this),
-            new Among ( "st\u00E4", 3, -1, "", this)
-        };
-
-        private Among a_3[] = {
-            new Among ( "lle", -1, -1, "", this),
-            new Among ( "ine", -1, -1, "", this)
-        };
-
-        private Among a_4[] = {
-            new Among ( "nsa", -1, 3, "", this),
-            new Among ( "mme", -1, 3, "", this),
-            new Among ( "nne", -1, 3, "", this),
-            new Among ( "ni", -1, 2, "", this),
-            new Among ( "si", -1, 1, "", this),
-            new Among ( "an", -1, 4, "", this),
-            new Among ( "en", -1, 6, "", this),
-            new Among ( "\u00E4n", -1, 5, "", this),
-            new Among ( "ns\u00E4", -1, 3, "", this)
-        };
-
-        private Among a_5[] = {
-            new Among ( "aa", -1, -1, "", this),
-            new Among ( "ee", -1, -1, "", this),
-            new Among ( "ii", -1, -1, "", this),
-            new Among ( "oo", -1, -1, "", this),
-            new Among ( "uu", -1, -1, "", this),
-            new Among ( "\u00E4\u00E4", -1, -1, "", this),
-            new Among ( "\u00F6\u00F6", -1, -1, "", this)
-        };
-
-        private Among a_6[] = {
-            new Among ( "a", -1, 8, "", this),
-            new Among ( "lla", 0, -1, "", this),
-            new Among ( "na", 0, -1, "", this),
-            new Among ( "ssa", 0, -1, "", this),
-            new Among ( "ta", 0, -1, "", this),
-            new Among ( "lta", 4, -1, "", this),
-            new Among ( "sta", 4, -1, "", this),
-            new Among ( "tta", 4, 9, "", this),
-            new Among ( "lle", -1, -1, "", this),
-            new Among ( "ine", -1, -1, "", this),
-            new Among ( "ksi", -1, -1, "", this),
-            new Among ( "n", -1, 7, "", this),
-            new Among ( "han", 11, 1, "", this),
-            new Among ( "den", 11, -1, "r_VI", this),
-            new Among ( "seen", 11, -1, "r_LONG", this),
-            new Among ( "hen", 11, 2, "", this),
-            new Among ( "tten", 11, -1, "r_VI", this),
-            new Among ( "hin", 11, 3, "", this),
-            new Among ( "siin", 11, -1, "r_VI", this),
-            new Among ( "hon", 11, 4, "", this),
-            new Among ( "h\u00E4n", 11, 5, "", this),
-            new Among ( "h\u00F6n", 11, 6, "", this),
-            new Among ( "\u00E4", -1, 8, "", this),
-            new Among ( "ll\u00E4", 22, -1, "", this),
-            new Among ( "n\u00E4", 22, -1, "", this),
-            new Among ( "ss\u00E4", 22, -1, "", this),
-            new Among ( "t\u00E4", 22, -1, "", this),
-            new Among ( "lt\u00E4", 26, -1, "", this),
-            new Among ( "st\u00E4", 26, -1, "", this),
-            new Among ( "tt\u00E4", 26, 9, "", this)
-        };
-
-        private Among a_7[] = {
-            new Among ( "eja", -1, -1, "", this),
-            new Among ( "mma", -1, 1, "", this),
-            new Among ( "imma", 1, -1, "", this),
-            new Among ( "mpa", -1, 1, "", this),
-            new Among ( "impa", 3, -1, "", this),
-            new Among ( "mmi", -1, 1, "", this),
-            new Among ( "immi", 5, -1, "", this),
-            new Among ( "mpi", -1, 1, "", this),
-            new Among ( "impi", 7, -1, "", this),
-            new Among ( "ej\u00E4", -1, -1, "", this),
-            new Among ( "mm\u00E4", -1, 1, "", this),
-            new Among ( "imm\u00E4", 10, -1, "", this),
-            new Among ( "mp\u00E4", -1, 1, "", this),
-            new Among ( "imp\u00E4", 12, -1, "", this)
-        };
-
-        private Among a_8[] = {
-            new Among ( "i", -1, -1, "", this),
-            new Among ( "j", -1, -1, "", this)
-        };
-
-        private Among a_9[] = {
-            new Among ( "mma", -1, 1, "", this),
-            new Among ( "imma", 0, -1, "", this)
-        };
-
-        private static final char g_AEI[] = {17, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8 };
-
-        private static final char g_V1[] = {17, 65, 16, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 0, 32 };
-
-        private static final char g_V2[] = {17, 65, 16, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 0, 32 };
-
-        private static final char g_particle_end[] = {17, 97, 24, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 0, 32 };
-
-        private boolean B_ending_removed;
-        private StringBuilder S_x = new StringBuilder();
-        private int I_p2;
-        private int I_p1;
-
-        private void copy_from(FinnishStemmer other) {
-            B_ending_removed = other.B_ending_removed;
-            S_x = other.S_x;
-            I_p2 = other.I_p2;
-            I_p1 = other.I_p1;
-            super.copy_from(other);
-        }
-
-        private boolean r_mark_regions() {
-            int v_1;
-            int v_3;
-            // (, line 41
-            I_p1 = limit;
-            I_p2 = limit;
-            // goto, line 46
-            golab0: while(true)
-            {
-                v_1 = cursor;
-                lab1: do {
-                    if (!(in_grouping(g_V1, 97, 246)))
-                    {
-                        break lab1;
-                    }
-                    cursor = v_1;
-                    break golab0;
-                } while (false);
-                cursor = v_1;
-                if (cursor >= limit)
-                {
-                    return false;
-                }
-                cursor++;
-            }
-            // gopast, line 46
-            golab2: while(true)
-            {
-                lab3: do {
-                    if (!(out_grouping(g_V1, 97, 246)))
-                    {
-                        break lab3;
-                    }
-                    break golab2;
-                } while (false);
-                if (cursor >= limit)
-                {
-                    return false;
-                }
-                cursor++;
-            }
-            // setmark p1, line 46
-            I_p1 = cursor;
-            // goto, line 47
-            golab4: while(true)
-            {
-                v_3 = cursor;
-                lab5: do {
-                    if (!(in_grouping(g_V1, 97, 246)))
-                    {
-                        break lab5;
-                    }
-                    cursor = v_3;
-                    break golab4;
-                } while (false);
-                cursor = v_3;
-                if (cursor >= limit)
-                {
-                    return false;
-                }
-                cursor++;
-            }
-            // gopast, line 47
-            golab6: while(true)
-            {
-                lab7: do {
-                    if (!(out_grouping(g_V1, 97, 246)))
-                    {
-                        break lab7;
-                    }
-                    break golab6;
-                } while (false);
-                if (cursor >= limit)
-                {
-                    return false;
-                }
-                cursor++;
-            }
-            // setmark p2, line 47
-            I_p2 = cursor;
-            return true;
-        }
-
-        private boolean r_R2() {
-            if (!(I_p2 <= cursor))
-            {
-                return false;
-            }
-            return true;
-        }
-
-        private boolean r_particle_etc() {
-            int among_var;
-            int v_1;
-            int v_2;
-            // (, line 54
-            // setlimit, line 55
-            v_1 = limit - cursor;
-            // tomark, line 55
-            if (cursor < I_p1)
-            {
-                return false;
-            }
-            cursor = I_p1;
-            v_2 = limit_backward;
-            limit_backward = cursor;
-            cursor = limit - v_1;
-            // (, line 55
-            // [, line 55
-            ket = cursor;
-            // substring, line 55
-            among_var = find_among_b(a_0, 10);
-            if (among_var == 0)
-            {
-                limit_backward = v_2;
-                return false;
-            }
-            // ], line 55
-            bra = cursor;
-            limit_backward = v_2;
-            switch(among_var) {
-                case 0:
-                    return false;
-                case 1:
-                    // (, line 62
-                    if (!(in_grouping_b(g_particle_end, 97, 246)))
-                    {
-                        return false;
-                    }
-                    break;
-                case 2:
-                    // (, line 64
-                    // call R2, line 64
-                    if (!r_R2())
-                    {
-                        return false;
-                    }
-                    break;
-            }
-            // delete, line 66
-            slice_del();
-            return true;
-        }
-
-        private boolean r_possessive() {
-            int among_var;
-            int v_1;
-            int v_2;
-            int v_3;
-            // (, line 68
-            // setlimit, line 69
-            v_1 = limit - cursor;
-            // tomark, line 69
-            if (cursor < I_p1)
-            {
-                return false;
-            }
-            cursor = I_p1;
-            v_2 = limit_backward;
-            limit_backward = cursor;
-            cursor = limit - v_1;
-            // (, line 69
-            // [, line 69
-            ket = cursor;
-            // substring, line 69
-            among_var = find_among_b(a_4, 9);
-            if (among_var == 0)
-            {
-                limit_backward = v_2;
-                return false;
-            }
-            // ], line 69
-            bra = cursor;
-            limit_backward = v_2;
-            switch(among_var) {
-                case 0:
-                    return false;
-                case 1:
-                    // (, line 72
-                    // not, line 72
-                    {
-                        v_3 = limit - cursor;
-                        lab0: do {
-                            // literal, line 72
-                            if (!(eq_s_b(1, "k")))
-                            {
-                                break lab0;
-                            }
-                            return false;
-                        } while (false);
-                        cursor = limit - v_3;
-                    }
-                    // delete, line 72
-                    slice_del();
-                    break;
-                case 2:
-                    // (, line 74
-                    // delete, line 74
-                    slice_del();
-                    // [, line 74
-                    ket = cursor;
-                    // literal, line 74
-                    if (!(eq_s_b(3, "kse")))
-                    {
-                        return false;
-                    }
-                    // ], line 74
-                    bra = cursor;
-                    // <-, line 74
-                    slice_from("ksi");
-                    break;
-                case 3:
-                    // (, line 78
-                    // delete, line 78
-                    slice_del();
-                    break;
-                case 4:
-                    // (, line 81
-                    // among, line 81
-                    if (find_among_b(a_1, 6) == 0)
-                    {
-                        return false;
-                    }
-                    // delete, line 81
-                    slice_del();
-                    break;
-                case 5:
-                    // (, line 83
-                    // among, line 83
-                    if (find_among_b(a_2, 6) == 0)
-                    {
-                        return false;
-                    }
-                    // delete, line 84
-                    slice_del();
-                    break;
-                case 6:
-                    // (, line 86
-                    // among, line 86
-                    if (find_among_b(a_3, 2) == 0)
-                    {
-                        return false;
-                    }
-                    // delete, line 86
-                    slice_del();
-                    break;
-            }
-            return true;
-        }
-
-        private boolean r_LONG() {
-            // among, line 91
-            if (find_among_b(a_5, 7) == 0)
-            {
-                return false;
-            }
-            return true;
-        }
-
-        private boolean r_VI() {
-            // (, line 93
-            // literal, line 93
-            if (!(eq_s_b(1, "i")))
-            {
-                return false;
-            }
-            if (!(in_grouping_b(g_V2, 97, 246)))
-            {
-                return false;
-            }
-            return true;
-        }
-
-        private boolean r_case_ending() {
-            int among_var;
-            int v_1;
-            int v_2;
-            int v_3;
-            int v_4;
-            int v_5;
-            // (, line 95
-            // setlimit, line 96
-            v_1 = limit - cursor;
-            // tomark, line 96
-            if (cursor < I_p1)
-            {
-                return false;
-            }
-            cursor = I_p1;
-            v_2 = limit_backward;
-            limit_backward = cursor;
-            cursor = limit - v_1;
-            // (, line 96
-            // [, line 96
-            ket = cursor;
-            // substring, line 96
-            among_var = find_among_b(a_6, 30);
-            if (among_var == 0)
-            {
-                limit_backward = v_2;
-                return false;
-            }
-            // ], line 96
-            bra = cursor;
-            limit_backward = v_2;
-            switch(among_var) {
-                case 0:
-                    return false;
-                case 1:
-                    // (, line 98
-                    // literal, line 98
-                    if (!(eq_s_b(1, "a")))
-                    {
-                        return false;
-                    }
-                    break;
-                case 2:
-                    // (, line 99
-                    // literal, line 99
-                    if (!(eq_s_b(1, "e")))
-                    {
-                        return false;
-                    }
-                    break;
-                case 3:
-                    // (, line 100
-                    // literal, line 100
-                    if (!(eq_s_b(1, "i")))
-                    {
-                        return false;
-                    }
-                    break;
-                case 4:
-                    // (, line 101
-                    // literal, line 101
-                    if (!(eq_s_b(1, "o")))
-                    {
-                        return false;
-                    }
-                    break;
-                case 5:
-                    // (, line 102
-                    // literal, line 102
-                    if (!(eq_s_b(1, "\u00E4")))
-                    {
-                        return false;
-                    }
-                    break;
-                case 6:
-                    // (, line 103
-                    // literal, line 103
-                    if (!(eq_s_b(1, "\u00F6")))
-                    {
-                        return false;
-                    }
-                    break;
-                case 7:
-                    // (, line 111
-                    // try, line 111
-                    v_3 = limit - cursor;
-                    lab0: do {
-                        // (, line 111
-                        // and, line 113
-                        v_4 = limit - cursor;
-                        // or, line 112
-                        lab1: do {
-                            v_5 = limit - cursor;
-                            lab2: do {
-                                // call LONG, line 111
-                                if (!r_LONG())
-                                {
-                                    break lab2;
-                                }
-                                break lab1;
-                            } while (false);
-                            cursor = limit - v_5;
-                            // literal, line 112
-                            if (!(eq_s_b(2, "ie")))
-                            {
-                                cursor = limit - v_3;
-                                break lab0;
-                            }
-                        } while (false);
-                        cursor = limit - v_4;
-                        // next, line 113
-                        if (cursor <= limit_backward)
-                        {
-                            cursor = limit - v_3;
-                            break lab0;
-                        }
-                        cursor--;
-                        // ], line 113
-                        bra = cursor;
-                    } while (false);
-                    break;
-                case 8:
-                    // (, line 119
-                    if (!(in_grouping_b(g_V1, 97, 246)))
-                    {
-                        return false;
-                    }
-                    if (!(out_grouping_b(g_V1, 97, 246)))
-                    {
-                        return false;
-                    }
-                    break;
-                case 9:
-                    // (, line 121
-                    // literal, line 121
-                    if (!(eq_s_b(1, "e")))
-                    {
-                        return false;
-                    }
-                    break;
-            }
-            // delete, line 138
-            slice_del();
-            // set ending_removed, line 139
-            B_ending_removed = true;
-            return true;
-        }
-
-        private boolean r_other_endings() {
-            int among_var;
-            int v_1;
-            int v_2;
-            int v_3;
-            // (, line 141
-            // setlimit, line 142
-            v_1 = limit - cursor;
-            // tomark, line 142
-            if (cursor < I_p2)
-            {
-                return false;
-            }
-            cursor = I_p2;
-            v_2 = limit_backward;
-            limit_backward = cursor;
-            cursor = limit - v_1;
-            // (, line 142
-            // [, line 142
-            ket = cursor;
-            // substring, line 142
-            among_var = find_among_b(a_7, 14);
-            if (among_var == 0)
-            {
-                limit_backward = v_2;
-                return false;
-            }
-            // ], line 142
-            bra = cursor;
-            limit_backward = v_2;
-            switch(among_var) {
-                case 0:
-                    return false;
-                case 1:
-                    // (, line 146
-                    // not, line 146
-                    {
-                        v_3 = limit - cursor;
-                        lab0: do {
-                            // literal, line 146
-                            if (!(eq_s_b(2, "po")))
-                            {
-                                break lab0;
-                            }
-                            return false;
-                        } while (false);
-                        cursor = limit - v_3;
-                    }
-                    break;
-            }
-            // delete, line 151
-            slice_del();
-            return true;
-        }
-
-        private boolean r_i_plural() {
-            int v_1;
-            int v_2;
-            // (, line 153
-            // setlimit, line 154
-            v_1 = limit - cursor;
-            // tomark, line 154
-            if (cursor < I_p1)
-            {
-                return false;
-            }
-            cursor = I_p1;
-            v_2 = limit_backward;
-            limit_backward = cursor;
-            cursor = limit - v_1;
-            // (, line 154
-            // [, line 154
-            ket = cursor;
-            // substring, line 154
-            if (find_among_b(a_8, 2) == 0)
-            {
-                limit_backward = v_2;
-                return false;
-            }
-            // ], line 154
-            bra = cursor;
-            limit_backward = v_2;
-            // delete, line 158
-            slice_del();
-            return true;
-        }
-
-        private boolean r_t_plural() {
-            int among_var;
-            int v_1;
-            int v_2;
-            int v_3;
-            int v_4;
-            int v_5;
-            int v_6;
-            // (, line 160
-            // setlimit, line 161
-            v_1 = limit - cursor;
-            // tomark, line 161
-            if (cursor < I_p1)
-            {
-                return false;
-            }
-            cursor = I_p1;
-            v_2 = limit_backward;
-            limit_backward = cursor;
-            cursor = limit - v_1;
-            // (, line 161
-            // [, line 162
-            ket = cursor;
-            // literal, line 162
-            if (!(eq_s_b(1, "t")))
-            {
-                limit_backward = v_2;
-                return false;
-            }
-            // ], line 162
-            bra = cursor;
-            // test, line 162
-            v_3 = limit - cursor;
-            if (!(in_grouping_b(g_V1, 97, 246)))
-            {
-                limit_backward = v_2;
-                return false;
-            }
-            cursor = limit - v_3;
-            // delete, line 163
-            slice_del();
-            limit_backward = v_2;
-            // setlimit, line 165
-            v_4 = limit - cursor;
-            // tomark, line 165
-            if (cursor < I_p2)
-            {
-                return false;
-            }
-            cursor = I_p2;
-            v_5 = limit_backward;
-            limit_backward = cursor;
-            cursor = limit - v_4;
-            // (, line 165
-            // [, line 165
-            ket = cursor;
-            // substring, line 165
-            among_var = find_among_b(a_9, 2);
-            if (among_var == 0)
-            {
-                limit_backward = v_5;
-                return false;
-            }
-            // ], line 165
-            bra = cursor;
-            limit_backward = v_5;
-            switch(among_var) {
-                case 0:
-                    return false;
-                case 1:
-                    // (, line 167
-                    // not, line 167
-                    {
-                        v_6 = limit - cursor;
-                        lab0: do {
-                            // literal, line 167
-                            if (!(eq_s_b(2, "po")))
-                            {
-                                break lab0;
-                            }
-                            return false;
-                        } while (false);
-                        cursor = limit - v_6;
-                    }
-                    break;
-            }
-            // delete, line 170
-            slice_del();
-            return true;
-        }
-
-        private boolean r_tidy() {
-            int v_1;
-            int v_2;
-            int v_3;
-            int v_4;
-            int v_5;
-            int v_6;
-            int v_7;
-            int v_8;
-            int v_9;
-            // (, line 172
-            // setlimit, line 173
-            v_1 = limit - cursor;
-            // tomark, line 173
-            if (cursor < I_p1)
-            {
-                return false;
-            }
-            cursor = I_p1;
-            v_2 = limit_backward;
-            limit_backward = cursor;
-            cursor = limit - v_1;
-            // (, line 173
-            // do, line 174
-            v_3 = limit - cursor;
-            lab0: do {
-                // (, line 174
-                // and, line 174
-                v_4 = limit - cursor;
-                // call LONG, line 174
-                if (!r_LONG())
-                {
-                    break lab0;
-                }
-                cursor = limit - v_4;
-                // (, line 174
-                // [, line 174
-                ket = cursor;
-                // next, line 174
-                if (cursor <= limit_backward)
-                {
-                    break lab0;
-                }
-                cursor--;
-                // ], line 174
-                bra = cursor;
-                // delete, line 174
-                slice_del();
-            } while (false);
-            cursor = limit - v_3;
-            // do, line 175
-            v_5 = limit - cursor;
-            lab1: do {
-                // (, line 175
-                // [, line 175
-                ket = cursor;
-                if (!(in_grouping_b(g_AEI, 97, 228)))
-                {
-                    break lab1;
-                }
-                // ], line 175
-                bra = cursor;
-                if (!(out_grouping_b(g_V1, 97, 246)))
-                {
-                    break lab1;
-                }
-                // delete, line 175
-                slice_del();
-            } while (false);
-            cursor = limit - v_5;
-            // do, line 176
-            v_6 = limit - cursor;
-            lab2: do {
-                // (, line 176
-                // [, line 176
-                ket = cursor;
-                // literal, line 176
-                if (!(eq_s_b(1, "j")))
-                {
-                    break lab2;
-                }
-                // ], line 176
-                bra = cursor;
-                // or, line 176
-                lab3: do {
-                    v_7 = limit - cursor;
-                    lab4: do {
-                        // literal, line 176
-                        if (!(eq_s_b(1, "o")))
-                        {
-                            break lab4;
-                        }
-                        break lab3;
-                    } while (false);
-                    cursor = limit - v_7;
-                    // literal, line 176
-                    if (!(eq_s_b(1, "u")))
-                    {
-                        break lab2;
-                    }
-                } while (false);
-                // delete, line 176
-                slice_del();
-            } while (false);
-            cursor = limit - v_6;
-            // do, line 177
-            v_8 = limit - cursor;
-            lab5: do {
-                // (, line 177
-                // [, line 177
-                ket = cursor;
-                // literal, line 177
-                if (!(eq_s_b(1, "o")))
-                {
-                    break lab5;
-                }
-                // ], line 177
-                bra = cursor;
-                // literal, line 177
-                if (!(eq_s_b(1, "j")))
-                {
-                    break lab5;
-                }
-                // delete, line 177
-                slice_del();
-            } while (false);
-            cursor = limit - v_8;
-            limit_backward = v_2;
-            // goto, line 179
-            golab6: while(true)
-            {
-                v_9 = limit - cursor;
-                lab7: do {
-                    if (!(out_grouping_b(g_V1, 97, 246)))
-                    {
-                        break lab7;
-                    }
-                    cursor = limit - v_9;
-                    break golab6;
-                } while (false);
-                cursor = limit - v_9;
-                if (cursor <= limit_backward)
-                {
-                    return false;
-                }
-                cursor--;
-            }
-            // [, line 179
-            ket = cursor;
-            // next, line 179
-            if (cursor <= limit_backward)
-            {
-                return false;
-            }
-            cursor--;
-            // ], line 179
-            bra = cursor;
-            // -> x, line 179
-            S_x = slice_to(S_x);
-            // name x, line 179
-            if (!(eq_v_b(S_x)))
-            {
-                return false;
-            }
-            // delete, line 179
-            slice_del();
-            return true;
-        }
-
-        public boolean stem() {
-            int v_1;
-            int v_2;
-            int v_3;
-            int v_4;
-            int v_5;
-            int v_6;
-            int v_7;
-            int v_8;
-            int v_9;
-            // (, line 183
-            // do, line 185
-            v_1 = cursor;
-            lab0: do {
-                // call mark_regions, line 185
-                if (!r_mark_regions())
-                {
-                    break lab0;
-                }
-            } while (false);
-            cursor = v_1;
-            // unset ending_removed, line 186
-            B_ending_removed = false;
-            // backwards, line 187
-            limit_backward = cursor; cursor = limit;
-            // (, line 187
-            // do, line 188
-            v_2 = limit - cursor;
-            lab1: do {
-                // call particle_etc, line 188
-                if (!r_particle_etc())
-                {
-                    break lab1;
-                }
-            } while (false);
-            cursor = limit - v_2;
-            // do, line 189
-            v_3 = limit - cursor;
-            lab2: do {
-                // call possessive, line 189
-                if (!r_possessive())
-                {
-                    break lab2;
-                }
-            } while (false);
-            cursor = limit - v_3;
-            // do, line 190
-            v_4 = limit - cursor;
-            lab3: do {
-                // call case_ending, line 190
-                if (!r_case_ending())
-                {
-                    break lab3;
-                }
-            } while (false);
-            cursor = limit - v_4;
-            // do, line 191
-            v_5 = limit - cursor;
-            lab4: do {
-                // call other_endings, line 191
-                if (!r_other_endings())
-                {
-                    break lab4;
-                }
-            } while (false);
-            cursor = limit - v_5;
-            // or, line 192
-            lab5: do {
-                v_6 = limit - cursor;
-                lab6: do {
-                    // (, line 192
-                    // Boolean test ending_removed, line 192
-                    if (!(B_ending_removed))
-                    {
-                        break lab6;
-                    }
-                    // do, line 192
-                    v_7 = limit - cursor;
-                    lab7: do {
-                        // call i_plural, line 192
-                        if (!r_i_plural())
-                        {
-                            break lab7;
-                        }
-                    } while (false);
-                    cursor = limit - v_7;
-                    break lab5;
-                } while (false);
-                cursor = limit - v_6;
-                // do, line 192
-                v_8 = limit - cursor;
-                lab8: do {
-                    // call t_plural, line 192
-                    if (!r_t_plural())
-                    {
-                        break lab8;
-                    }
-                } while (false);
-                cursor = limit - v_8;
-            } while (false);
-            // do, line 193
-            v_9 = limit - cursor;
-            lab9: do {
-                // call tidy, line 193
-                if (!r_tidy())
-                {
-                    break lab9;
-                }
-            } while (false);
-            cursor = limit - v_9;
-            cursor = limit_backward;            return true;
-        }
-
-}
-
Index: contrib/snowball/src/java/org/tartarus/snowball/TestApp.java
===================================================================
--- contrib/snowball/src/java/org/tartarus/snowball/TestApp.java	(revision 916567)
+++ contrib/snowball/src/java/org/tartarus/snowball/TestApp.java	(working copy)
@@ -1,108 +0,0 @@
-/*
-
-Copyright (c) 2001, Dr Martin Porter
-Copyright (c) 2002, Richard Boulton
-All rights reserved.
-
-Redistribution and use in source and binary forms, with or without
-modification, are permitted provided that the following conditions are met:
-
-    * Redistributions of source code must retain the above copyright notice,
-    * this list of conditions and the following disclaimer.
-    * Redistributions in binary form must reproduce the above copyright
-    * notice, this list of conditions and the following disclaimer in the
-    * documentation and/or other materials provided with the distribution.
-    * Neither the name of the copyright holders nor the names of its contributors
-    * may be used to endorse or promote products derived from this software
-    * without specific prior written permission.
-
-THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
-AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
-IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
-DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE
-FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
-DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
-SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
-CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
-OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
-OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
-
- */
-
-package org.tartarus.snowball;
-
-import java.lang.reflect.Method;
-import java.io.Reader;
-import java.io.Writer;
-import java.io.BufferedReader;
-import java.io.BufferedWriter;
-import java.io.FileInputStream;
-import java.io.InputStreamReader;
-import java.io.OutputStreamWriter;
-import java.io.OutputStream;
-import java.io.FileOutputStream;
-
-public class TestApp {
-    private static void usage()
-    {
-        System.err.println("Usage: TestApp <algorithm> <input file> [-o <output file>]");
-    }
-
-    public static void main(String [] args) throws Throwable {
-	if (args.length < 2) {
-            usage();
-            return;
-        }
-
-	Class stemClass = Class.forName("org.tartarus.snowball.ext." +
-					args[0] + "Stemmer");
-        SnowballProgram stemmer = (SnowballProgram) stemClass.newInstance();
-	Method stemMethod = stemClass.getMethod("stem", new Class[0]);
-
-	Reader reader;
-	reader = new InputStreamReader(new FileInputStream(args[1]));
-	reader = new BufferedReader(reader);
-
-	StringBuffer input = new StringBuffer();
-
-        OutputStream outstream;
-
-	if (args.length > 2) {
-            if (args.length == 4 && args[2].equals("-o")) {
-                outstream = new FileOutputStream(args[3]);
-            } else {
-                usage();
-                return;
-            }
-	} else {
-	    outstream = System.out;
-	}
-	Writer output = new OutputStreamWriter(outstream);
-	output = new BufferedWriter(output);
-
-	int repeat = 1;
-	if (args.length > 4) {
-	    repeat = Integer.parseInt(args[4]);
-	}
-
-	Object [] emptyArgs = new Object[0];
-	int character;
-	while ((character = reader.read()) != -1) {
-	    char ch = (char) character;
-	    if (Character.isWhitespace((char) ch)) {
-		if (input.length() > 0) {
-		    stemmer.setCurrent(input.toString());
-		    for (int i = repeat; i != 0; i--) {
-			stemMethod.invoke(stemmer, emptyArgs);
-		    }
-		    output.write(stemmer.getCurrent());
-		    output.write('\n');
-		    input.delete(0, input.length());
-		}
-	    } else {
-		input.append(Character.toLowerCase(ch));
-	    }
-	}
-	output.flush();
-    }
-}
Index: contrib/snowball/src/java/org/tartarus/snowball/Among.java
===================================================================
--- contrib/snowball/src/java/org/tartarus/snowball/Among.java	(revision 916567)
+++ contrib/snowball/src/java/org/tartarus/snowball/Among.java	(working copy)
@@ -1,63 +0,0 @@
-/*
-
-Copyright (c) 2001, Dr Martin Porter
-Copyright (c) 2002, Richard Boulton
-All rights reserved.
-
-Redistribution and use in source and binary forms, with or without
-modification, are permitted provided that the following conditions are met:
-
-    * Redistributions of source code must retain the above copyright notice,
-    * this list of conditions and the following disclaimer.
-    * Redistributions in binary form must reproduce the above copyright
-    * notice, this list of conditions and the following disclaimer in the
-    * documentation and/or other materials provided with the distribution.
-    * Neither the name of the copyright holders nor the names of its contributors
-    * may be used to endorse or promote products derived from this software
-    * without specific prior written permission.
-
-THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
-AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
-IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
-DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE
-FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
-DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
-SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
-CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
-OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
-OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
-
- */
-
-package org.tartarus.snowball;
-
-import java.lang.reflect.Method;
-
-public class Among {
-    public Among (String s, int substring_i, int result,
-		  String methodname, SnowballProgram methodobject) {
-        this.s_size = s.length();
-        this.s = s.toCharArray();
-        this.substring_i = substring_i;
-	this.result = result;
-	this.methodobject = methodobject;
-	if (methodname.length() == 0) {
-	    this.method = null;
-	} else {
-	    try {
-		this.method = methodobject.getClass().
-		getDeclaredMethod(methodname, new Class[0]);
-	    } catch (NoSuchMethodException e) {
-		throw new RuntimeException(e);
-	    }
-	}
-    }
-
-    public final int s_size; /* search string */
-    public final char[] s; /* search string */
-    public final int substring_i; /* index to longest matching substring */
-    public final int result;      /* result of the lookup */
-    public final Method method; /* method to use if substring matches */
-    public final SnowballProgram methodobject; /* object to invoke method on */
-   
-};
Index: contrib/snowball/src/java/org/tartarus/snowball/SnowballProgram.java
===================================================================
--- contrib/snowball/src/java/org/tartarus/snowball/SnowballProgram.java	(revision 916567)
+++ contrib/snowball/src/java/org/tartarus/snowball/SnowballProgram.java	(working copy)
@@ -1,565 +0,0 @@
-/*
-
-Copyright (c) 2001, Dr Martin Porter
-Copyright (c) 2002, Richard Boulton
-All rights reserved.
-
-Redistribution and use in source and binary forms, with or without
-modification, are permitted provided that the following conditions are met:
-
-    * Redistributions of source code must retain the above copyright notice,
-    * this list of conditions and the following disclaimer.
-    * Redistributions in binary form must reproduce the above copyright
-    * notice, this list of conditions and the following disclaimer in the
-    * documentation and/or other materials provided with the distribution.
-    * Neither the name of the copyright holders nor the names of its contributors
-    * may be used to endorse or promote products derived from this software
-    * without specific prior written permission.
-
-THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
-AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
-IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
-DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE
-FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
-DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
-SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
-CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
-OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
-OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
-
- */
-
-
-package org.tartarus.snowball;
-
-import java.lang.reflect.InvocationTargetException;
-
-import org.apache.lucene.util.ArrayUtil;
-
-/**
- * This is the rev 502 of the Snowball SVN trunk,
- * but modified:
- * made abstract and introduced abstract method stem to avoid expensive reflection in filter class.
- * refactored StringBuffers to StringBuilder
- * uses char[] as buffer instead of StringBuffer/StringBuilder
- * eq_s,eq_s_b,insert,replace_s take CharSequence like eq_v and eq_v_b
- */
-public abstract class SnowballProgram {
-    protected SnowballProgram()
-    {
-	current = new char[8];
-	setCurrent("");
-    }
-
-    public abstract boolean stem();
-
-    /**
-     * Set the current string.
-     */
-    public void setCurrent(String value)
-    {
-	current = value.toCharArray();
-	cursor = 0;
-	limit = value.length();
-	limit_backward = 0;
-	bra = cursor;
-	ket = limit;
-    }
-
-    /**
-     * Get the current string.
-     */
-    public String getCurrent()
-    {
-      return new String(current, 0, limit);
-    }
-    
-    /**
-     * Set the current string.
-     * @param text character array containing input
-     * @param length valid length of text.
-     */
-    public void setCurrent(char text[], int length) {
-      current = text;
-      cursor = 0;
-      limit = length;
-      limit_backward = 0;
-      bra = cursor;
-      ket = limit;
-    }
-
-    /**
-     * Get the current buffer containing the stem.
-     * <p>
-     * NOTE: this may be a reference to a different character array than the
-     * one originally provided with setCurrent, in the exceptional case that 
-     * stemming produced a longer intermediate or result string. 
-     * </p>
-     * <p>
-     * It is necessary to use {@link #getCurrentBufferLength()} to determine
-     * the valid length of the returned buffer. For example, many words are
-     * stemmed simply by subtracting from the length to remove suffixes.
-     * </p>
-     * @see #getCurrentBufferLength()
-     */
-    public char[] getCurrentBuffer() {
-      return current;
-    }
-    
-    /**
-     * Get the valid length of the character array in 
-     * {@link #getCurrentBuffer()}. 
-     * @return valid length of the array.
-     */
-    public int getCurrentBufferLength() {
-      return limit;
-    }
-
-    // current string
-    private char current[];
-
-    protected int cursor;
-    protected int limit;
-    protected int limit_backward;
-    protected int bra;
-    protected int ket;
-
-    protected void copy_from(SnowballProgram other)
-    {
-	current          = other.current;
-	cursor           = other.cursor;
-	limit            = other.limit;
-	limit_backward   = other.limit_backward;
-	bra              = other.bra;
-	ket              = other.ket;
-    }
-
-    protected boolean in_grouping(char [] s, int min, int max)
-    {
-	if (cursor >= limit) return false;
-	char ch = current[cursor];
-	if (ch > max || ch < min) return false;
-	ch -= min;
-	if ((s[ch >> 3] & (0X1 << (ch & 0X7))) == 0) return false;
-	cursor++;
-	return true;
-    }
-
-    protected boolean in_grouping_b(char [] s, int min, int max)
-    {
-	if (cursor <= limit_backward) return false;
-	char ch = current[cursor - 1];
-	if (ch > max || ch < min) return false;
-	ch -= min;
-	if ((s[ch >> 3] & (0X1 << (ch & 0X7))) == 0) return false;
-	cursor--;
-	return true;
-    }
-
-    protected boolean out_grouping(char [] s, int min, int max)
-    {
-	if (cursor >= limit) return false;
-	char ch = current[cursor];
-	if (ch > max || ch < min) {
-	    cursor++;
-	    return true;
-	}
-	ch -= min;
-	if ((s[ch >> 3] & (0X1 << (ch & 0X7))) == 0) {
-	    cursor ++;
-	    return true;
-	}
-	return false;
-    }
-
-    protected boolean out_grouping_b(char [] s, int min, int max)
-    {
-	if (cursor <= limit_backward) return false;
-	char ch = current[cursor - 1];
-	if (ch > max || ch < min) {
-	    cursor--;
-	    return true;
-	}
-	ch -= min;
-	if ((s[ch >> 3] & (0X1 << (ch & 0X7))) == 0) {
-	    cursor--;
-	    return true;
-	}
-	return false;
-    }
-
-    protected boolean in_range(int min, int max)
-    {
-	if (cursor >= limit) return false;
-	char ch = current[cursor];
-	if (ch > max || ch < min) return false;
-	cursor++;
-	return true;
-    }
-
-    protected boolean in_range_b(int min, int max)
-    {
-	if (cursor <= limit_backward) return false;
-	char ch = current[cursor - 1];
-	if (ch > max || ch < min) return false;
-	cursor--;
-	return true;
-    }
-
-    protected boolean out_range(int min, int max)
-    {
-	if (cursor >= limit) return false;
-	char ch = current[cursor];
-	if (!(ch > max || ch < min)) return false;
-	cursor++;
-	return true;
-    }
-
-    protected boolean out_range_b(int min, int max)
-    {
-	if (cursor <= limit_backward) return false;
-	char ch = current[cursor - 1];
-	if(!(ch > max || ch < min)) return false;
-	cursor--;
-	return true;
-    }
-
-    protected boolean eq_s(int s_size, CharSequence s)
-    {
-	if (limit - cursor < s_size) return false;
-	int i;
-	for (i = 0; i != s_size; i++) {
-	    if (current[cursor + i] != s.charAt(i)) return false;
-	}
-	cursor += s_size;
-	return true;
-    }
-
-    /** @deprecated for binary back compat. Will be removed in Lucene 4.0 */
-    @Deprecated
-    protected boolean eq_s(int s_size, String s)
-    {
-	return eq_s(s_size, (CharSequence)s);
-    }
-
-    protected boolean eq_s_b(int s_size, CharSequence s)
-    {
-	if (cursor - limit_backward < s_size) return false;
-	int i;
-	for (i = 0; i != s_size; i++) {
-	    if (current[cursor - s_size + i] != s.charAt(i)) return false;
-	}
-	cursor -= s_size;
-	return true;
-    }
-
-    /** @deprecated for binary back compat. Will be removed in Lucene 4.0 */
-    @Deprecated
-    protected boolean eq_s_b(int s_size, String s)
-    {
-	return eq_s_b(s_size, (CharSequence)s);
-    }
-
-    protected boolean eq_v(CharSequence s)
-    {
-	return eq_s(s.length(), s);
-    }
-
-    /** @deprecated for binary back compat. Will be removed in Lucene 4.0 */
-    @Deprecated
-    protected boolean eq_v(StringBuilder s)
-    {
-	return eq_s(s.length(), (CharSequence)s);
-    }
-
-    protected boolean eq_v_b(CharSequence s)
-    {   return eq_s_b(s.length(), s);
-    }
-
-    /** @deprecated for binary back compat. Will be removed in Lucene 4.0 */
-    @Deprecated
-    protected boolean eq_v_b(StringBuilder s)
-    {   return eq_s_b(s.length(), (CharSequence)s);
-    }
-
-    protected int find_among(Among v[], int v_size)
-    {
-	int i = 0;
-	int j = v_size;
-
-	int c = cursor;
-	int l = limit;
-
-	int common_i = 0;
-	int common_j = 0;
-
-	boolean first_key_inspected = false;
-
-	while(true) {
-	    int k = i + ((j - i) >> 1);
-	    int diff = 0;
-	    int common = common_i < common_j ? common_i : common_j; // smaller
-	    Among w = v[k];
-	    int i2;
-	    for (i2 = common; i2 < w.s_size; i2++) {
-		if (c + common == l) {
-		    diff = -1;
-		    break;
-		}
-		diff = current[c + common] - w.s[i2];
-		if (diff != 0) break;
-		common++;
-	    }
-	    if (diff < 0) {
-		j = k;
-		common_j = common;
-	    } else {
-		i = k;
-		common_i = common;
-	    }
-	    if (j - i <= 1) {
-		if (i > 0) break; // v->s has been inspected
-		if (j == i) break; // only one item in v
-
-		// - but now we need to go round once more to get
-		// v->s inspected. This looks messy, but is actually
-		// the optimal approach.
-
-		if (first_key_inspected) break;
-		first_key_inspected = true;
-	    }
-	}
-	while(true) {
-	    Among w = v[i];
-	    if (common_i >= w.s_size) {
-		cursor = c + w.s_size;
-		if (w.method == null) return w.result;
-		boolean res;
-		try {
-		    Object resobj = w.method.invoke(w.methodobject,
-						    new Object[0]);
-		    res = resobj.toString().equals("true");
-		} catch (InvocationTargetException e) {
-		    res = false;
-		    // FIXME - debug message
-		} catch (IllegalAccessException e) {
-		    res = false;
-		    // FIXME - debug message
-		}
-		cursor = c + w.s_size;
-		if (res) return w.result;
-	    }
-	    i = w.substring_i;
-	    if (i < 0) return 0;
-	}
-    }
-
-    // find_among_b is for backwards processing. Same comments apply
-    protected int find_among_b(Among v[], int v_size)
-    {
-	int i = 0;
-	int j = v_size;
-
-	int c = cursor;
-	int lb = limit_backward;
-
-	int common_i = 0;
-	int common_j = 0;
-
-	boolean first_key_inspected = false;
-
-	while(true) {
-	    int k = i + ((j - i) >> 1);
-	    int diff = 0;
-	    int common = common_i < common_j ? common_i : common_j;
-	    Among w = v[k];
-	    int i2;
-	    for (i2 = w.s_size - 1 - common; i2 >= 0; i2--) {
-		if (c - common == lb) {
-		    diff = -1;
-		    break;
-		}
-		diff = current[c - 1 - common] - w.s[i2];
-		if (diff != 0) break;
-		common++;
-	    }
-	    if (diff < 0) {
-		j = k;
-		common_j = common;
-	    } else {
-		i = k;
-		common_i = common;
-	    }
-	    if (j - i <= 1) {
-		if (i > 0) break;
-		if (j == i) break;
-		if (first_key_inspected) break;
-		first_key_inspected = true;
-	    }
-	}
-	while(true) {
-	    Among w = v[i];
-	    if (common_i >= w.s_size) {
-		cursor = c - w.s_size;
-		if (w.method == null) return w.result;
-
-		boolean res;
-		try {
-		    Object resobj = w.method.invoke(w.methodobject,
-						    new Object[0]);
-		    res = resobj.toString().equals("true");
-		} catch (InvocationTargetException e) {
-		    res = false;
-		    // FIXME - debug message
-		} catch (IllegalAccessException e) {
-		    res = false;
-		    // FIXME - debug message
-		}
-		cursor = c - w.s_size;
-		if (res) return w.result;
-	    }
-	    i = w.substring_i;
-	    if (i < 0) return 0;
-	}
-    }
-
-    /* to replace chars between c_bra and c_ket in current by the
-     * chars in s.
-     */
-    protected int replace_s(int c_bra, int c_ket, CharSequence s)
-    {
-	final int adjustment = s.length() - (c_ket - c_bra);
-	final int newLength = limit + adjustment;
-	//resize if necessary
-	if (newLength > current.length) {
-	  char newBuffer[] = new char[ArrayUtil.getNextSize(newLength)];
-	  System.arraycopy(current, 0, newBuffer, 0, limit);
-	  current = newBuffer;
-	}
-	// if the substring being replaced is longer or shorter than the
-	// replacement, need to shift things around
-	if (adjustment != 0 && c_ket < limit) {
-	  System.arraycopy(current, c_ket, current, c_bra + s.length(), 
-	      limit - c_ket);
-	}
-	// insert the replacement text
-	// Note, faster is s.getChars(0, s.length(), current, c_bra);
-	// but would have to duplicate this method for both String and StringBuilder
-	for (int i = 0; i < s.length(); i++)
-	  current[c_bra + i] = s.charAt(i);
-	
-	limit += adjustment;
-	if (cursor >= c_ket) cursor += adjustment;
-	else if (cursor > c_bra) cursor = c_bra;
-	return adjustment;
-    }
-
-    /** @deprecated for binary back compat. Will be removed in Lucene 4.0 */
-    @Deprecated
-    protected int replace_s(int c_bra, int c_ket, String s) {
-	return replace_s(c_bra, c_ket, (CharSequence)s);
-    }
-
-    protected void slice_check()
-    {
-	if (bra < 0 ||
-	    bra > ket ||
-	    ket > limit)
-	{
-	    System.err.println("faulty slice operation");
-	// FIXME: report error somehow.
-	/*
-	    fprintf(stderr, "faulty slice operation:\n");
-	    debug(z, -1, 0);
-	    exit(1);
-	    */
-	}
-    }
-
-    protected void slice_from(CharSequence s)
-    {
-	slice_check();
-	replace_s(bra, ket, s);
-    }
- 
-    /** @deprecated for binary back compat. Will be removed in Lucene 4.0 */
-    @Deprecated
-    protected void slice_from(String s)
-    {
-	slice_from((CharSequence)s);
-    }
-
-    /** @deprecated for binary back compat. Will be removed in Lucene 4.0 */
-    @Deprecated
-    protected void slice_from(StringBuilder s)
-    {
-	slice_from((CharSequence)s);
-    }
-
-    protected void slice_del()
-    {
-	slice_from((CharSequence)"");
-    }
-
-    protected void insert(int c_bra, int c_ket, CharSequence s)
-    {
-	int adjustment = replace_s(c_bra, c_ket, s);
-	if (c_bra <= bra) bra += adjustment;
-	if (c_bra <= ket) ket += adjustment;
-    }
-
-    /** @deprecated for binary back compat. Will be removed in Lucene 4.0 */
-    @Deprecated
-    protected void insert(int c_bra, int c_ket, String s)
-    {
-	insert(c_bra, c_ket, (CharSequence)s);
-    }
-
-    /** @deprecated for binary back compat. Will be removed in Lucene 4.0 */
-    @Deprecated
-    protected void insert(int c_bra, int c_ket, StringBuilder s)
-    {
-	insert(c_bra, c_ket, (CharSequence)s);
-    }
-
-    /* Copy the slice into the supplied StringBuffer */
-    protected StringBuilder slice_to(StringBuilder s)
-    {
-	slice_check();
-	int len = ket - bra;
-	s.setLength(0);
-	s.append(current, bra, len);
-	return s;
-    }
-
-    protected StringBuilder assign_to(StringBuilder s)
-    {
-	s.setLength(0);
-	s.append(current, 0, limit);
-	return s;
-    }
-
-/*
-extern void debug(struct SN_env * z, int number, int line_count)
-{   int i;
-    int limit = SIZE(z->p);
-    //if (number >= 0) printf("%3d (line %4d): '", number, line_count);
-    if (number >= 0) printf("%3d (line %4d): [%d]'", number, line_count,limit);
-    for (i = 0; i <= limit; i++)
-    {   if (z->lb == i) printf("{");
-        if (z->bra == i) printf("[");
-        if (z->c == i) printf("|");
-        if (z->ket == i) printf("]");
-        if (z->l == i) printf("}");
-        if (i < limit)
-        {   int ch = z->p[i];
-            if (ch == 0) ch = '#';
-            printf("%c", ch);
-        }
-    }
-    printf("'\n");
-}
-*/
-
-};
-
Index: contrib/snowball/src/java/overview.html
===================================================================
--- contrib/snowball/src/java/overview.html	(revision 916567)
+++ contrib/snowball/src/java/overview.html	(working copy)
@@ -1,53 +0,0 @@
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<html>
-<body>
-<p>
-Lucene Snowball README file
-</p>
-
-<p>
-This project provides pre-compiled version of the Snowball stemmers
-based on revision 500 of the Tartarus Snowball repository,
-together with classes integrating them with the Lucene search engine.
-</p>
-<p>
-A few changes has been made to the static Snowball code and compiled stemmers:
-</p>
-<ul>
-  <li>Class SnowballProgram is made abstract and contains new abstract method stem() to avoid reflection in Lucene filter class SnowballFilter.</li>
-  <li>All use of StringBuffers has been refactored to StringBuilder for speed.</li>
-  <li>Snowball BSD license header has been added to the Java classes to avoid having RAT adding ASL headers.</li>
-</ul>
-<p>
-See the Snowball <a href ="http://snowball.tartarus.org/">home page</a> for more information about the algorithms.
-</p>
-
-<p>
-<b>IMPORTANT NOTICE ON BACKWARDS COMPATIBILITY!</b>
-</p>
-<p>
-An index created using the Snowball module in Lucene 2.3.2 and below
-might not be compatible with the Snowball module in Lucene 2.4 or greater.
-</p>
-<p>
-For more information about this issue see:
-https://issues.apache.org/jira/browse/LUCENE-1142
-</p>
-
-</body>
-</html>
Index: contrib/snowball/bin/snowball.sh
===================================================================
--- contrib/snowball/bin/snowball.sh	(revision 916567)
+++ contrib/snowball/bin/snowball.sh	(working copy)
@@ -1,7 +0,0 @@
-#!/bin/csh -f
-set infile = $1
-set outdir = $2
-
-set name = $infile:h:t:uStemmer
-
-exec $0:h/snowball $infile -o $outdir/$name -n $name -java
Index: contrib/snowball/bin/.cvsignore
===================================================================
--- contrib/snowball/bin/.cvsignore	(revision 916567)
+++ contrib/snowball/bin/.cvsignore	(working copy)
@@ -1 +0,0 @@
-snowball
Index: contrib/snowball/docs/index.html
===================================================================
--- contrib/snowball/docs/index.html	(revision 916567)
+++ contrib/snowball/docs/index.html	(working copy)
@@ -1,148 +0,0 @@
-<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
-
-<!-- Content Stylesheet for Site -->
-
-        
-<!-- start the processing -->
-    <!-- ====================================================================== -->
-    <!-- GENERATED FILE, DO NOT EDIT, EDIT THE XML FILE IN xdocs INSTEAD! -->
-    <!-- Main Page Section -->
-    <!-- ====================================================================== -->
-    <html>
-        <head>
-            <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1"/>
-
-                                                    <meta name="author" value="Doug Cutting">
-            <meta name="email" value="cutting@apache.org">
-            
-           
-                                    
-                        
-            <title>Snowball Stemmers for Lucene - Overview - Snowball Stemmers for Lucene</title>
-        </head>
-
-        <body bgcolor="#ffffff" text="#000000" link="#525D76">        
-            <table border="0" width="100%" cellspacing="0">
-                <!-- TOP IMAGE -->
-                <tr>
-                    <td colspan="2">
-<a href="http://jakarta.apache.org"><img src="http://jakarta.apache.org/images/jakarta-logo.gif" align="left" border="0"/></a>
-</td>
-                </tr>
-            </table>
-            <table border="0" width="100%" cellspacing="4">
-                <tr><td colspan="2">
-                    <hr noshade="" size="1"/>
-                </td></tr>
-                
-                <tr>
-                    <!-- LEFT SIDE NAVIGATION -->
-                    <td width="20%" valign="top" nowrap="true">
-                    
-    <!-- ============================================================ -->
-
-                <p><strong>Documentation</strong></p>
-        <ul>
-                    <li>    <a href="./api/index.html">Javadoc</a>
-</li>
-                </ul>
-            <p><strong>Download</strong></p>
-        <ul>
-                    <li>    <a href="http://jakarta.apache.org/builds/jakarta-lucene-sandbox/snowball/">Releases</a>
-</li>
-                    <li>    <a href="http://jakarta.apache.org/site/cvsindex.html">CVS Repository</a>
-</li>
-                </ul>
-            <p><strong>Links</strong></p>
-        <ul>
-                    <li>    <a href="http://snowball.tartarus.org/">Snowball Home</a>
-</li>
-                    <li>    <a href="http://jakarta.apache.org/lucene/">Lucene Home</a>
-</li>
-                    <li>    <a href="http://jakarta.apache.org/lucene/docs/lucene-sandbox/">Lucene Sandbox</a>
-</li>
-                </ul>
-            <p><strong>Jakarta</strong></p>
-        <ul>
-                    <li>    <a href="http://jakarta.apache.org/site/getinvolved.html">Get Involved</a>
-</li>
-                    <li>    <a href="http://jakarta.apache.org/site/acknowledgements.html">Acknowledgements</a>
-</li>
-                    <li>    <a href="http://jakarta.apache.org/site/contact.html">Contact</a>
-</li>
-                    <li>    <a href="http://jakarta.apache.org/site/legal.html">Legal</a>
-</li>
-                </ul>
-                        </td>
-                    <td width="80%" align="left" valign="top">
-                                                                    <table border="0" cellspacing="0" cellpadding="2" width="100%">
-      <tr><td bgcolor="#525D76">
-        <font color="#ffffff" face="arial,helvetica,sanserif">
-          <a name="Snowball Stemmers for Lucene"><strong>Snowball Stemmers for Lucene</strong></a>
-        </font>
-      </td></tr>
-      <tr><td>
-        <blockquote>
-                                    <p>
-This project provides pre-compiled version of the Snowball stemmers
-together with classes integrating them with the Lucene search engine.
-</p>
-                            </blockquote>
-        </p>
-      </td></tr>
-      <tr><td><br/></td></tr>
-    </table>
-                                                <table border="0" cellspacing="0" cellpadding="2" width="100%">
-      <tr><td bgcolor="#525D76">
-        <font color="#ffffff" face="arial,helvetica,sanserif">
-          <a name="Download"><strong>Download</strong></a>
-        </font>
-      </td></tr>
-      <tr><td>
-        <blockquote>
-                                    <p>
-Releases of the stemmers are available
-<a href="http://jakarta.apache.org/builds/jakarta-lucene-sandbox/snowball/">
-here</a>
-</p>
-                            </blockquote>
-        </p>
-      </td></tr>
-      <tr><td><br/></td></tr>
-    </table>
-                                        </td>
-                </tr>
-
-                <!-- FOOTER -->
-                <tr><td colspan="2">
-                    <hr noshade="" size="1"/>
-                </td></tr>
-                <tr><td colspan="2">
-                    <div align="center"><font color="#525D76" size="-1"><em>
-                    Copyright &#169; 1999-2004, The Apache Software Foundation
-                    </em></font></div>
-                </td></tr>
-            </table>
-        </body>
-    </html>
-<!-- end the processing -->
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
Index: contrib/snowball/README.txt
===================================================================
--- contrib/snowball/README.txt	(revision 916567)
+++ contrib/snowball/README.txt	(working copy)
@@ -1,25 +0,0 @@
-Lucene Snowball README file
-
-This project provides pre-compiled version of the Snowball stemmers
-based on revision 500 of the Tartarus Snowball repository,
-together with classes integrating them with the Lucene search engine.
-
-A few changes has been made to the static Snowball code and compiled stemmers:
-
- * Class SnowballProgram is made abstract and contains new abstract method stem() to avoid reflection in Lucene filter class SnowballFilter.
- * All use of StringBuffers has been refactored to StringBuilder for speed.
- * Snowball BSD license header has been added to the Java classes to avoid having RAT adding new ASL headers.
-
-
-IMPORTANT NOTICE ON BACKWARDS COMPATIBILITY!
-
-An index created using the Snowball module in Lucene 2.3.2 and below
-might not be compatible with the Snowball module in Lucene 2.4 or greater.
-
-For more information about this issue see:
-https://issues.apache.org/jira/browse/LUCENE-1142
-
-
-For more information on Snowball, see:
-  http://snowball.tartarus.org/
-
Index: contrib/snowball/.cvsignore
===================================================================
--- contrib/snowball/.cvsignore	(revision 916567)
+++ contrib/snowball/.cvsignore	(working copy)
@@ -1,2 +0,0 @@
-build
-snowball
Index: contrib/snowball/build.xml
===================================================================
--- contrib/snowball/build.xml	(revision 916567)
+++ contrib/snowball/build.xml	(working copy)
@@ -1,156 +0,0 @@
-<?xml version="1.0"?>
-
-<!--
-    Licensed to the Apache Software Foundation (ASF) under one or more
-    contributor license agreements.  See the NOTICE file distributed with
-    this work for additional information regarding copyright ownership.
-    The ASF licenses this file to You under the Apache License, Version 2.0
-    the "License"); you may not use this file except in compliance with
-    the License.  You may obtain a copy of the License at
- 
-        http://www.apache.org/licenses/LICENSE-2.0
- 
-    Unless required by applicable law or agreed to in writing, software
-    distributed under the License is distributed on an "AS IS" BASIS,
-    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-    See the License for the specific language governing permissions and
-    limitations under the License.
- -->
-
-<project name="snowball" default="default">
-
-  <description>
-    Snowball Analyzers
-  </description>
-
-  <import file="../contrib-build.xml"/>
-
-  <property name="snowball.cvsroot" value=":pserver:cvsuser@cvs.tartarus.org:/home/cvs"/>
-  <property name="snowball.cvs.password" value="anonymous"/>
-  <property name="snowball.root" value="snowball/website"/>
-  <property name="bin.dir" location="bin"/>
-
-  <property name="analyzers.jar" location="${common.dir}/build/contrib/analyzers/common/lucene-analyzers-${version}.jar"/>
-  <available property="analyzers.jar.present" type="file" file="${analyzers.jar}"/>
-  
-  <path id="classpath">
-	<pathelement path="${lucene.jar}"/>
-	<pathelement path="${analyzers.jar}"/>
-	<pathelement path="${project.classpath}"/>
-  </path>
-
-  <target name="jar" depends="compile" description="Create JAR">
-    <jarify>
-      <metainf-includes>
-        <metainf dir=".">
-          <include name="SNOWBALL-LICENSE.txt"/>
-        </metainf>
-      </metainf-includes>
-    </jarify>
-  </target>
-
-  <target name="jar-src" depends="init"
-    description="Packages the sources as JAR file">
-  	<jarify basedir="${src.dir}" destfile="${build.dir}/${final.name}-src.jar">
-  	  <metainf-includes>
-        <metainf dir=".">
-          <include name="SNOWBALL-LICENSE.txt"/>
-        </metainf>
-      </metainf-includes>
-  	</jarify>
-  </target>
-
-	
-  <!-- ====================================================== -->
-  <!-- Download Snowball code                                 -->
-  <!-- ====================================================== -->
-  <target name="download" depends="init">
-    <cvs cvsRoot="${snowball.cvsroot}"
-      package="${snowball.root}"
-      passfile="snowball.cvspass"/>
-  </target>
-
-  <target name="create-passfile">
-    <cvspass cvsroot="${snowball.cvsroot}"
-      password="${snowball.cvs.password}"
-      passfile="snowball.cvspass"
-      />
-  </target>
-
-  <!-- ====================================================== -->
-  <!-- Compile Snowball C code                                -->
-  <!-- ====================================================== -->
-  <target name="compile-compiler" depends="download">
-    <apply failonerror="true" executable="gcc" parallel="true">
-      <arg value="-O"/>
-      <arg value="-o"/>
-      <arg value="${bin.dir}/snowball"/>
-      <fileset dir="${snowball.root}/p" includes="*.c"/>
-    </apply>
-  </target>
-
-  <!-- ====================================================== -->
-  <!-- Generate Java code                                     -->
-  <!-- ====================================================== -->
-  <target name="generate" depends="compile-compiler">
-    <apply failonerror="true" executable="${bin.dir}/snowball.sh">
-      <srcfile/>
-      <arg value="${src.dir}/net/sf/snowball/ext"/>
-      <fileset dir="${snowball.root}" includes="**/stem.sbl"/>
-    </apply>
-
-    <copy todir="${src.dir}/net">
-      <fileset dir="${snowball.root}/net">
-        <include name="**/*.java"/>
-      </fileset>
-    </copy>
-
-  </target>
-
-
-  <target name="docs">
-    <taskdef
-      name="anakia"
-      classname="org.apache.velocity.anakia.AnakiaTask"
-      >
-      <classpath refid="anakia.classpath"/>
-    </taskdef>
-
-    <anakia
-      basedir="${docs.src}"
-      destdir="${docs.dest}/"
-      extension=".html" style="./site.vsl"
-      projectFile="stylesheets/project.xml"
-      excludes="**/stylesheets/** empty.xml"
-      includes="**/*.xml"
-      lastModifiedCheck="true"
-      templatePath="${jakarta.site2.home}/xdocs/stylesheets"
-      >
-    </anakia>
-
-  </target>
-
-  <target name="compile-core" depends="build-analyzers, common.compile-core" />
-  <target name="compile-test" depends="download-vocab-tests, common.compile-test" />
-  
-  <target name="build-analyzers" unless="analyzers.jar.present">
-    <echo>Snowball building dependency ${analyzers.jar}</echo>
-    <ant antfile="../analyzers/build.xml" target="default" inheritall="false" dir="../analyzers" />
-  </target>
-
-  <property name="snowball.vocab.rev" value="500"/>
-  <property name="snowball.vocab.url" 
-  	value="svn://svn.tartarus.org/snowball/trunk/data"/>
-  <property name="vocab.dir" value="src/test/org/apache/lucene/analysis/snowball"/>
-	
-  <target name="download-vocab-tests" depends="compile-core"
-    description="Downloads Snowball vocabulary tests">
-    <sequential>
-      <mkdir dir="${vocab.dir}"/>
-      <exec dir="${vocab.dir}" executable="${svn.exe}"
-	            failifexecutionfails="false">
-        <arg line="checkout -r ${snowball.vocab.rev} ${snowball.vocab.url}"/>
-	  </exec>
-	</sequential>
-  </target>
-</project>
Index: contrib/ant/src/java/org/apache/lucene/ant/TextDocument.java
===================================================================
--- contrib/ant/src/java/org/apache/lucene/ant/TextDocument.java	(revision 916567)
+++ contrib/ant/src/java/org/apache/lucene/ant/TextDocument.java	(working copy)
@@ -89,7 +89,7 @@
 
     /**
      *@return    The contents value
-     *@todo      finish this method
+     *TODO:      finish this method
      */
     public String getContents() {
         return contents;
Index: contrib/queryparser/src/test/org/apache/lucene/queryParser/standard/TestQPHelper.java
===================================================================
--- contrib/queryparser/src/test/org/apache/lucene/queryParser/standard/TestQPHelper.java	(revision 916567)
+++ contrib/queryparser/src/test/org/apache/lucene/queryParser/standard/TestQPHelper.java	(working copy)
@@ -119,6 +119,7 @@
     public boolean incrementToken() throws IOException {
       if (inPhrase) {
         inPhrase = false;
+        clearAttributes();
         termAtt.setTermBuffer("phrase2");
         offsetAtt.setOffset(savedStart, savedEnd);
         return true;
@@ -143,7 +144,7 @@
     /** Filters LowerCaseTokenizer with StopFilter. */
     @Override
     public final TokenStream tokenStream(String fieldName, Reader reader) {
-      return new QPTestFilter(new LowerCaseTokenizer(reader));
+      return new QPTestFilter(new LowerCaseTokenizer(Version.LUCENE_CURRENT, reader));
     }
   }
 
@@ -203,7 +204,7 @@
 
   public StandardQueryParser getParser(Analyzer a) throws Exception {
     if (a == null)
-      a = new SimpleAnalyzer();
+      a = new SimpleAnalyzer(Version.LUCENE_CURRENT);
     StandardQueryParser qp = new StandardQueryParser();
     qp.setAnalyzer(a);
 
@@ -293,7 +294,7 @@
 
   public Query getQueryDOA(String query, Analyzer a) throws Exception {
     if (a == null)
-      a = new SimpleAnalyzer();
+      a = new SimpleAnalyzer(Version.LUCENE_CURRENT);
     StandardQueryParser qp = new StandardQueryParser();
     qp.setAnalyzer(a);
     qp.setDefaultOperator(Operator.AND);
@@ -313,7 +314,7 @@
   }
 
   public void testConstantScoreAutoRewrite() throws Exception {
-    StandardQueryParser qp = new StandardQueryParser(new WhitespaceAnalyzer());
+    StandardQueryParser qp = new StandardQueryParser(new WhitespaceAnalyzer(Version.LUCENE_CURRENT));
     Query q = qp.parse("foo*bar", "field");
     assertTrue(q instanceof WildcardQuery);
     assertEquals(MultiTermQuery.CONSTANT_SCORE_AUTO_REWRITE_DEFAULT, ((MultiTermQuery) q).getRewriteMethod());
@@ -338,9 +339,9 @@
   public void testSimple() throws Exception {
     assertQueryEquals("\"term germ\"~2", null, "\"term germ\"~2");
     assertQueryEquals("term term term", null, "term term term");
-    assertQueryEquals("t�rm term term", new WhitespaceAnalyzer(),
+    assertQueryEquals("t�rm term term", new WhitespaceAnalyzer(Version.LUCENE_CURRENT),
         "t�rm term term");
-    assertQueryEquals("�mlaut", new WhitespaceAnalyzer(), "�mlaut");
+    assertQueryEquals("�mlaut", new WhitespaceAnalyzer(Version.LUCENE_CURRENT), "�mlaut");
 
     assertQueryEquals("\"\"", new KeywordAnalyzer(), "");
     assertQueryEquals("foo:\"\"", new KeywordAnalyzer(), "foo:");
@@ -397,7 +398,7 @@
   }
 
   public void testPunct() throws Exception {
-    Analyzer a = new WhitespaceAnalyzer();
+    Analyzer a = new WhitespaceAnalyzer(Version.LUCENE_CURRENT);
     assertQueryEquals("a&b", a, "a&b");
     assertQueryEquals("a&&b", a, "a&&b");
     assertQueryEquals(".NET", a, ".NET");
@@ -572,7 +573,7 @@
   public void testFarsiRangeCollating() throws Exception {
 
     RAMDirectory ramDir = new RAMDirectory();
-    IndexWriter iw = new IndexWriter(ramDir, new WhitespaceAnalyzer(), true,
+    IndexWriter iw = new IndexWriter(ramDir, new WhitespaceAnalyzer(Version.LUCENE_CURRENT), true,
         IndexWriter.MaxFieldLength.LIMITED);
     Document doc = new Document();
     doc.add(new Field("content", "\u0633\u0627\u0628", Field.Store.YES,
@@ -582,7 +583,7 @@
     IndexSearcher is = new IndexSearcher(ramDir, true);
 
     StandardQueryParser qp = new StandardQueryParser();
-    qp.setAnalyzer(new WhitespaceAnalyzer());
+    qp.setAnalyzer(new WhitespaceAnalyzer(Version.LUCENE_CURRENT));
 
     // Neither Java 1.4.2 nor 1.5.0 has Farsi Locale collation available in
     // RuleBasedCollator. However, the Arabic Locale seems to order the
@@ -736,7 +737,7 @@
   }
 
   public void testEscaped() throws Exception {
-    Analyzer a = new WhitespaceAnalyzer();
+    Analyzer a = new WhitespaceAnalyzer(Version.LUCENE_CURRENT);
 
     /*
      * assertQueryEquals("\\[brackets", a, "\\[brackets");
@@ -835,7 +836,7 @@
   }
 
   public void testQueryStringEscaping() throws Exception {
-    Analyzer a = new WhitespaceAnalyzer();
+    Analyzer a = new WhitespaceAnalyzer(Version.LUCENE_CURRENT);
 
     assertEscapedQueryEquals("a-b:c", a, "a\\-b\\:c");
     assertEscapedQueryEquals("a+b:c", a, "a\\+b\\:c");
@@ -950,7 +951,7 @@
 
   public void testCustomQueryParserWildcard() {
     try {
-      new QPTestParser(new WhitespaceAnalyzer()).parse("a?t", "contents");
+      new QPTestParser(new WhitespaceAnalyzer(Version.LUCENE_CURRENT)).parse("a?t", "contents");
       fail("Wildcard queries should not be allowed");
     } catch (QueryNodeException expected) {
       // expected exception
@@ -959,7 +960,7 @@
 
   public void testCustomQueryParserFuzzy() throws Exception {
     try {
-      new QPTestParser(new WhitespaceAnalyzer()).parse("xunit~", "contents");
+      new QPTestParser(new WhitespaceAnalyzer(Version.LUCENE_CURRENT)).parse("xunit~", "contents");
       fail("Fuzzy queries should not be allowed");
     } catch (QueryNodeException expected) {
       // expected exception
@@ -970,7 +971,7 @@
     BooleanQuery.setMaxClauseCount(2);
     try {
       StandardQueryParser qp = new StandardQueryParser();
-      qp.setAnalyzer(new WhitespaceAnalyzer());
+      qp.setAnalyzer(new WhitespaceAnalyzer(Version.LUCENE_CURRENT));
 
       qp.parse("one two three", "field");
       fail("ParseException expected due to too many boolean clauses");
@@ -984,7 +985,7 @@
    */
   public void testPrecedence() throws Exception {
     StandardQueryParser qp = new StandardQueryParser();
-    qp.setAnalyzer(new WhitespaceAnalyzer());
+    qp.setAnalyzer(new WhitespaceAnalyzer(Version.LUCENE_CURRENT));
 
     Query query1 = qp.parse("A AND B OR C AND D", "field");
     Query query2 = qp.parse("+A +B +C +D", "field");
@@ -995,7 +996,7 @@
   public void testLocalDateFormat() throws IOException, QueryNodeException {
 
     RAMDirectory ramDir = new RAMDirectory();
-    IndexWriter iw = new IndexWriter(ramDir, new WhitespaceAnalyzer(), true,
+    IndexWriter iw = new IndexWriter(ramDir, new WhitespaceAnalyzer(Version.LUCENE_CURRENT), true,
         IndexWriter.MaxFieldLength.LIMITED);
     addDateDoc("a", 2005, 12, 2, 10, 15, 33, iw);
     addDateDoc("b", 2005, 12, 4, 22, 15, 00, iw);
@@ -1120,7 +1121,7 @@
 
   public void testMatchAllDocs() throws Exception {
     StandardQueryParser qp = new StandardQueryParser();
-    qp.setAnalyzer(new WhitespaceAnalyzer());
+    qp.setAnalyzer(new WhitespaceAnalyzer(Version.LUCENE_CURRENT));
 
     assertEquals(new MatchAllDocsQuery(), qp.parse("*:*", "field"));
     assertEquals(new MatchAllDocsQuery(), qp.parse("(*:*)", "field"));
@@ -1132,7 +1133,7 @@
   private void assertHits(int expected, String query, IndexSearcher is)
       throws IOException, QueryNodeException {
     StandardQueryParser qp = new StandardQueryParser();
-    qp.setAnalyzer(new WhitespaceAnalyzer());
+    qp.setAnalyzer(new WhitespaceAnalyzer(Version.LUCENE_CURRENT));
     qp.setLocale(Locale.ENGLISH);
 
     Query q = qp.parse(query, "date");
@@ -1163,6 +1164,7 @@
     final TermAttribute term = addAttribute(TermAttribute.class);
     @Override
     public boolean incrementToken() {
+      clearAttributes();
       if (upto == 4) {
         return false;
       }
Index: contrib/queryparser/src/test/org/apache/lucene/queryParser/standard/TestQueryParserWrapper.java
===================================================================
--- contrib/queryparser/src/test/org/apache/lucene/queryParser/standard/TestQueryParserWrapper.java	(revision 916567)
+++ contrib/queryparser/src/test/org/apache/lucene/queryParser/standard/TestQueryParserWrapper.java	(working copy)
@@ -114,6 +114,7 @@
     public boolean incrementToken() throws IOException {
       if (inPhrase) {
         inPhrase = false;
+        clearAttributes();
         termAtt.setTermBuffer("phrase2");
         offsetAtt.setOffset(savedStart, savedEnd);
         return true;
@@ -138,7 +139,7 @@
     /** Filters LowerCaseTokenizer with StopFilter. */
     @Override
     public final TokenStream tokenStream(String fieldName, Reader reader) {
-      return new QPTestFilter(new LowerCaseTokenizer(reader));
+      return new QPTestFilter(new LowerCaseTokenizer(Version.LUCENE_CURRENT, reader));
     }
   }
 
@@ -216,7 +217,7 @@
 
   public QueryParserWrapper getParser(Analyzer a) throws Exception {
     if (a == null)
-      a = new SimpleAnalyzer();
+      a = new SimpleAnalyzer(Version.LUCENE_CURRENT);
     QueryParserWrapper qp = new QueryParserWrapper("field", a);
     qp.setDefaultOperator(QueryParserWrapper.OR_OPERATOR);
     return qp;
@@ -301,7 +302,7 @@
 
   public Query getQueryDOA(String query, Analyzer a) throws Exception {
     if (a == null)
-      a = new SimpleAnalyzer();
+      a = new SimpleAnalyzer(Version.LUCENE_CURRENT);
     QueryParserWrapper qp = new QueryParserWrapper("field", a);
     qp.setDefaultOperator(QueryParserWrapper.AND_OPERATOR);
     return qp.parse(query);
@@ -328,9 +329,9 @@
   public void testSimple() throws Exception {
     assertQueryEquals("\"term germ\"~2", null, "\"term germ\"~2");
     assertQueryEquals("term term term", null, "term term term");
-    assertQueryEquals("t�rm term term", new WhitespaceAnalyzer(),
+    assertQueryEquals("t�rm term term", new WhitespaceAnalyzer(Version.LUCENE_CURRENT),
         "t�rm term term");
-    assertQueryEquals("�mlaut", new WhitespaceAnalyzer(), "�mlaut");
+    assertQueryEquals("�mlaut", new WhitespaceAnalyzer(Version.LUCENE_CURRENT), "�mlaut");
 
     assertQueryEquals("\"\"", new KeywordAnalyzer(), "");
     assertQueryEquals("foo:\"\"", new KeywordAnalyzer(), "foo:");
@@ -395,7 +396,7 @@
   }
 
   public void testPunct() throws Exception {
-    Analyzer a = new WhitespaceAnalyzer();
+    Analyzer a = new WhitespaceAnalyzer(Version.LUCENE_CURRENT);
     assertQueryEquals("a&b", a, "a&b");
     assertQueryEquals("a&&b", a, "a&&b");
     assertQueryEquals(".NET", a, ".NET");
@@ -551,7 +552,7 @@
     assertEquals(MultiTermQuery.CONSTANT_SCORE_AUTO_REWRITE_DEFAULT, ((TermRangeQuery)getQuery("[ a TO z]", null)).getRewriteMethod());
 
     QueryParserWrapper qp = new QueryParserWrapper("field",
-        new SimpleAnalyzer());
+        new SimpleAnalyzer(Version.LUCENE_CURRENT));
     
     qp.setMultiTermRewriteMethod(MultiTermQuery.SCORING_BOOLEAN_QUERY_REWRITE);
     assertEquals(MultiTermQuery.SCORING_BOOLEAN_QUERY_REWRITE,((TermRangeQuery)qp.parse("[ a TO z]")).getRewriteMethod());
@@ -570,7 +571,7 @@
   public void testFarsiRangeCollating() throws Exception {
 
     RAMDirectory ramDir = new RAMDirectory();
-    IndexWriter iw = new IndexWriter(ramDir, new WhitespaceAnalyzer(), true,
+    IndexWriter iw = new IndexWriter(ramDir, new WhitespaceAnalyzer(Version.LUCENE_CURRENT), true,
         IndexWriter.MaxFieldLength.LIMITED);
     Document doc = new Document();
     doc.add(new Field("content", "\u0633\u0627\u0628", Field.Store.YES,
@@ -580,7 +581,7 @@
     IndexSearcher is = new IndexSearcher(ramDir, true);
 
     QueryParserWrapper qp = new QueryParserWrapper("content",
-        new WhitespaceAnalyzer());
+        new WhitespaceAnalyzer(Version.LUCENE_CURRENT));
 
     // Neither Java 1.4.2 nor 1.5.0 has Farsi Locale collation available in
     // RuleBasedCollator. However, the Arabic Locale seems to order the Farsi
@@ -682,7 +683,7 @@
     final String monthField = "month";
     final String hourField = "hour";
     QueryParserWrapper qp = new QueryParserWrapper("field",
-        new SimpleAnalyzer());
+        new SimpleAnalyzer(Version.LUCENE_CURRENT));
 
     // Don't set any date resolution and verify if DateField is used
     assertDateRangeQueryEquals(qp, defaultField, startDate, endDate,
@@ -726,7 +727,7 @@
   }
 
   public void testEscaped() throws Exception {
-    Analyzer a = new WhitespaceAnalyzer();
+    Analyzer a = new WhitespaceAnalyzer(Version.LUCENE_CURRENT);
 
     /*
      * assertQueryEquals("\\[brackets", a, "\\[brackets");
@@ -823,7 +824,7 @@
   }
 
   public void testQueryStringEscaping() throws Exception {
-    Analyzer a = new WhitespaceAnalyzer();
+    Analyzer a = new WhitespaceAnalyzer(Version.LUCENE_CURRENT);
 
     assertEscapedQueryEquals("a-b:c", a, "a\\-b\\:c");
     assertEscapedQueryEquals("a+b:c", a, "a\\+b\\:c");
@@ -934,7 +935,7 @@
 
   public void testCustomQueryParserWildcard() {
     try {
-      new QPTestParser("contents", new WhitespaceAnalyzer()).parse("a?t");
+      new QPTestParser("contents", new WhitespaceAnalyzer(Version.LUCENE_CURRENT)).parse("a?t");
       fail("Wildcard queries should not be allowed");
     } catch (ParseException expected) {
       // expected exception
@@ -943,7 +944,7 @@
 
   public void testCustomQueryParserFuzzy() throws Exception {
     try {
-      new QPTestParser("contents", new WhitespaceAnalyzer()).parse("xunit~");
+      new QPTestParser("contents", new WhitespaceAnalyzer(Version.LUCENE_CURRENT)).parse("xunit~");
       fail("Fuzzy queries should not be allowed");
     } catch (ParseException expected) {
       // expected exception
@@ -954,7 +955,7 @@
     BooleanQuery.setMaxClauseCount(2);
     try {
       QueryParserWrapper qp = new QueryParserWrapper("field",
-          new WhitespaceAnalyzer());
+          new WhitespaceAnalyzer(Version.LUCENE_CURRENT));
       qp.parse("one two three");
       fail("ParseException expected due to too many boolean clauses");
     } catch (ParseException expected) {
@@ -967,7 +968,7 @@
    */
   public void testPrecedence() throws Exception {
     QueryParserWrapper qp = new QueryParserWrapper("field",
-        new WhitespaceAnalyzer());
+        new WhitespaceAnalyzer(Version.LUCENE_CURRENT));
     Query query1 = qp.parse("A AND B OR C AND D");
     Query query2 = qp.parse("+A +B +C +D");
 
@@ -977,7 +978,7 @@
   public void testLocalDateFormat() throws IOException, ParseException {
 
     RAMDirectory ramDir = new RAMDirectory();
-    IndexWriter iw = new IndexWriter(ramDir, new WhitespaceAnalyzer(), true,
+    IndexWriter iw = new IndexWriter(ramDir, new WhitespaceAnalyzer(Version.LUCENE_CURRENT), true,
         IndexWriter.MaxFieldLength.LIMITED);
     addDateDoc("a", 2005, 12, 2, 10, 15, 33, iw);
     addDateDoc("b", 2005, 12, 4, 22, 15, 00, iw);
@@ -1094,7 +1095,7 @@
 
   public void testMatchAllDocs() throws Exception {
     QueryParserWrapper qp = new QueryParserWrapper("field",
-        new WhitespaceAnalyzer());
+        new WhitespaceAnalyzer(Version.LUCENE_CURRENT));
     assertEquals(new MatchAllDocsQuery(), qp.parse("*:*"));
     assertEquals(new MatchAllDocsQuery(), qp.parse("(*:*)"));
     BooleanQuery bq = (BooleanQuery) qp.parse("+*:* -*:*");
@@ -1105,7 +1106,7 @@
   private void assertHits(int expected, String query, IndexSearcher is)
       throws ParseException, IOException {
     QueryParserWrapper qp = new QueryParserWrapper("date",
-        new WhitespaceAnalyzer());
+        new WhitespaceAnalyzer(Version.LUCENE_CURRENT));
     qp.setLocale(Locale.ENGLISH);
     Query q = qp.parse(query);
     ScoreDoc[] hits = is.search(q, null, 1000).scoreDocs;
Index: contrib/queryparser/src/java/org/apache/lucene/queryParser/core/nodes/MatchAllDocsQueryNode.java
===================================================================
--- contrib/queryparser/src/java/org/apache/lucene/queryParser/core/nodes/MatchAllDocsQueryNode.java	(revision 916567)
+++ contrib/queryparser/src/java/org/apache/lucene/queryParser/core/nodes/MatchAllDocsQueryNode.java	(working copy)
@@ -33,7 +33,7 @@
 
   @Override
   public String toString() {
-    return "<matchAllDocs field='*' term='*'>";
+    return "<matchAllDocs field='*' term='*'/>";
   }
 
   public CharSequence toQueryString(EscapeQuerySyntax escapeSyntaxParser) {
Index: contrib/db/bdb-je/pom.xml.template
===================================================================
--- contrib/db/bdb-je/pom.xml.template	(revision 916567)
+++ contrib/db/bdb-je/pom.xml.template	(working copy)
@@ -36,7 +36,7 @@
     <dependency>
       <groupId>sleepycat</groupId>
       <artifactId>je</artifactId>
-      <version>1.7.0</version>
+      <version>${sleepycat-je-version}</version>
     </dependency>
   </dependencies>
 </project>
Index: contrib/db/bdb-je/build.xml
===================================================================
--- contrib/db/bdb-je/build.xml	(revision 916567)
+++ contrib/db/bdb-je/build.xml	(working copy)
@@ -21,10 +21,10 @@
     Lucene Berkeley DB Java Edition integration
   </description>
 
-  <property name="je.version" value="3.3.69" />
+  <property name="je.version" value="3.3.93" />
 
   <path id="je.jar">
-    <pathelement location="lib/je-${je.version}/lib/je-${je.version}.jar" />
+    <pathelement location="lib/je-${je.version}.jar" />
   </path>
 
   <available classname="com.sleepycat.je.Database" property="je.jar.exists">
@@ -48,13 +48,8 @@
 
   <target name="get-je-jar" unless="je.jar.exists">
     <mkdir dir="lib" />
-    <get src="http://download.oracle.com/berkeley-db/je-${je.version}.zip"
-         dest="lib/je-${je.version}.zip" />
-    <unzip src="lib/je-${je.version}.zip" dest="lib">
-      <patternset>
-        <include name="je-${je.version}/lib/je-${je.version}.jar" />
-      </patternset>
-    </unzip>
+    <get src="http://download.oracle.com/maven/com/sleepycat/je/${je.version}/je-${je.version}.jar"
+         dest="lib/je-${je.version}.jar" />
   </target>
 
   <target name="check-and-get-je-jar" depends="get-je-jar" />
Index: contrib/CHANGES.txt
===================================================================
--- contrib/CHANGES.txt	(revision 916567)
+++ contrib/CHANGES.txt	(working copy)
@@ -15,6 +15,10 @@
    preserved, but some protected/public member variables changed type. This 
    does NOT affect java code/class files produced by the snowball compiler, 
    but technically is a backwards compatibility break.  (Robert Muir)
+   
+ * LUCENE-2226: Moved contrib/snowball functionality into contrib/analyzers.
+   Be sure to remove any old obselete lucene-snowball jar files from your
+   classpath!  (Robert Muir)
     
 Changes in runtime behavior
 
@@ -23,38 +27,48 @@
    used with Version > 3.0 and the TurkishStemmer.
    (Robert Muir via Simon Willnauer)  
 
+ * LUCENE-2055: GermanAnalyzer now uses the Snowball German2 algorithm and 
+   stopwords list by default for Version > 3.0.
+   (Robert Muir, Uwe Schindler, Simon Willnauer)
+
 Bug fixes
 
- * LUCENE-2199: ShingleFilter skipped over tri-gram shingles if outputUnigram
-   was set to false. (Simon Willnauer)
-
  * LUCENE-2068: Fixed ReverseStringFilter which was not aware of supplementary
    characters. During reverse the filter created unpaired surrogates, which
    will be replaced by U+FFFD by the indexer, but not at query time. The filter
    now reverses supplementary characters correctly if used with Version > 3.0.
    (Simon Willnauer, Robert Muir)
 
- * LUCENE-2144: Fix InstantiatedIndex to handle termDocs(null)
-   correctly (enumerate all non-deleted docs).  (Karl Wettin via Mike
-   McCandless)
-   
  * LUCENE-2035: TokenSources.getTokenStream() does not assign  positionIncrement. 
    (Christopher Morris via Mark Miller)
+  
+ * LUCENE-2055: Deprecated RussianTokenizer, RussianStemmer, RussianStemFilter,
+   FrenchStemmer, FrenchStemFilter, DutchStemmer, and DutchStemFilter. For
+   these Analyzers, SnowballFilter is used instead (for Version > 3.0), as
+   the previous code did not always implement the Snowball algorithm correctly.
+   Additionally, for Version > 3.0, the Snowball stopword lists are used by
+   default.  (Robert Muir, Uwe Schindler, Simon Willnauer)
+
+ * LUCENE-2278: FastVectorHighlighter: Highlighted term is out of alignment
+   in multi-valued NOT_ANALYZED field. (Koji Sekiguchi)
+ 
+ * LUCENE-2284: MatchAllDocsQueryNode toString() created an invalid XML tag.
+   (Frank Wesemann via Robert Muir)
    
 API Changes
 
- * LUCENE-2108: Add SpellChecker.close, to close the underlying
-   reader.  (Eirik Bjørsnøs via Mike McCandless)
- 
  * LUCENE-2147: Spatial GeoHashUtils now always decode GeoHash strings
    with full precision. GeoHash#decode_exactly(String) was merged into
    GeoHash#decode(String). (Chris Male, Simon Willnauer)
    
- * LUCENE-2165: Add a constructor to SnowballAnalyzer that takes a Set of 
-   stopwords, and deprecate the String[] one.  (Nick Burch via Robert Muir)
-
  * LUCENE-2204: Change some package private classes/members to publicly accessible to implement
    custom FragmentsBuilders. (Koji Sekiguchi)
+
+ * LUCENE-2055: Integrate snowball into contrib/analyzers. SnowballAnalyzer is
+   now deprecated in favor of language-specific analyzers which contain things
+   such as stopword lists and any language-specific processing in addition to
+   stemming. Add Turkish and Romanian stopwords lists to support this.
+   (Robert Muir, Uwe Schindler, Simon Willnauer)
    
 New features
 
@@ -67,27 +81,35 @@
    customizable field naming scheme.
    (Simon Willnauer)
 
- * LUCENE-2108: Spellchecker now safely supports concurrent modifications to
-   the spell-index. Threads can safely obtain term suggestions while the spell-
-   index is rebuild, cleared or reset. Internal IndexSearcher instances remain
-   open until the last thread accessing them releases the reference.
-   (Simon Willnauer)
-
  * LUCENE-2067: Add a Czech light stemmer. CzechAnalyzer will now stem words
    when Version is set to 3.1 or higher.  (Robert Muir)
    
  * LUCENE-2062: Add a Bulgarian analyzer.  (Robert Muir, Simon Willnauer)
 
+ * LUCENE-2206: Add Snowball's stopword lists for Danish, Dutch, English,
+   Finnish, French, German, Hungarian, Italian, Norwegian, Russian, Spanish, 
+   and Swedish. These can be loaded with WordListLoader.getSnowballWordSet.
+   (Robert Muir, Simon Willnauer)
+
+ * LUCENE-2243: Add DisjunctionMaxQuery support for FastVectorHighlighter.
+   (Koji Sekiguchi)
+
+ * LUCENE-2218: ShingleFilter supports minimum shingle size, and the separator
+   character is now configurable. Its also up to 20% faster. 
+   (Steven Rowe via Robert Muir)
+
+ * LUCENE-2234: Add a Hindi analyzer.  (Robert Muir)
+
+ * LUCENE-2055: Add analyzers/misc/StemmerOverrideFilter. This filter provides
+   the ability to override any stemmer with a custom dictionary map.
+   (Robert Muir, Uwe Schindler, Simon Willnauer)
+
 Build
 
- * LUCENE-2117: SnowballAnalyzer now holds a runtime-dependency on
-   contrib-analyzers to correctly handle the unique Turkish casing behavior.
-   (Robert Muir via Simon Willnauer)  
-
  * LUCENE-2124: Moved the JDK-based collation support from contrib/collation 
    into core, and moved the ICU-based collation support into contrib/icu.  
    (Steven Rowe, Robert Muir)
-
+   
 Optimizations
 
  * LUCENE-2157: DelimitedPayloadTokenFilter no longer copies the buffer
@@ -104,6 +126,50 @@
  * LUCENE-2115: Cutover contrib tests to use Java5 generics.  (Kay Kay
    via Mike McCandless)
 
+Other
+
+ * LUCENE-1845: Updated bdb-je jar from version 3.3.69 to 3.3.93.
+   (Simon Willnauer via Mike McCandless)
+
+================== Release 2.9.2 / 3.0.1 2010-02-26 ====================
+
+New features
+
+ * LUCENE-2108: Spellchecker now safely supports concurrent modifications to
+   the spell-index. Threads can safely obtain term suggestions while the spell-
+   index is rebuild, cleared or reset. Internal IndexSearcher instances remain
+   open until the last thread accessing them releases the reference.
+   (Simon Willnauer)
+
+Bug Fixes
+
+ * LUCENE-2144: Fix InstantiatedIndex to handle termDocs(null)
+   correctly (enumerate all non-deleted docs).  (Karl Wettin via Mike
+   McCandless)
+
+ * LUCENE-2199: ShingleFilter skipped over tri-gram shingles if outputUnigram
+   was set to false. (Simon Willnauer)
+  
+ * LUCENE-2211: Fix missing clearAttributes() calls:
+   ShingleMatrix, PrefixAware, compounds, NGramTokenFilter,
+   EdgeNGramTokenFilter, Highlighter, and MemoryIndex.
+   (Uwe Schindler, Robert Muir)
+
+ * LUCENE-2207, LUCENE-2219: Fix incorrect offset calculations in end() for 
+   CJKTokenizer, ChineseTokenizer, SmartChinese SentenceTokenizer, 
+   and WikipediaTokenizer.  (Koji Sekiguchi, Robert Muir)
+   
+ * LUCENE-2266: Fixed offset calculations in NGramTokenFilter and 
+   EdgeNGramTokenFilter.  (Joe Calderon, Robert Muir via Uwe Schindler)
+   
+API Changes
+
+ * LUCENE-2108: Add SpellChecker.close, to close the underlying
+   reader.  (Eirik Bjørsnøs via Mike McCandless)
+
+ * LUCENE-2165: Add a constructor to SnowballAnalyzer that takes a Set of 
+   stopwords, and deprecate the String[] one.  (Nick Burch via Robert Muir)
+   
 ======================= Release 3.0.0 2009-11-25 =======================
 
 Changes in backwards compatibility policy
@@ -124,6 +190,10 @@
    text exactly the same as LowerCaseFilter. Please use LowerCaseFilter
    instead, which has the same functionality.  (Robert Muir)
    
+ * LUCENE-2051: Contrib Analyzer setters were deprecated and replaced
+   with ctor arguments / Version number.  Also stop word lists
+   were unified.  (Simon Willnauer)
+
 Bug fixes
 
  * LUCENE-1781: Fixed various issues with the lat/lng bounding box
@@ -163,6 +233,7 @@
    Previous versions were loading the stopword files each time a new
    instance was created. This might improve performance for applications
    creating lots of instances of these Analyzers. (Simon Willnauer) 
+
 Documentation
 
  * LUCENE-1916: Translated documentation in the smartcn hhmm package.
@@ -176,7 +247,6 @@
  * LUCENE-2031: Moved PatternAnalyzer from contrib/memory into
    contrib/analyzers/common, under miscellaneous.  (Robert Muir)
    
-Test Cases
 ======================= Release 2.9.1 2009-11-06 =======================
 
 Changes in backwards compatibility policy

Property changes on: contrib\instantiated\src\test\org\apache\lucene\store\instantiated\TestIndicesEquals.java
___________________________________________________________________
Added: svn:mergeinfo
   Merged /lucene/java/branches/lucene_2_9/contrib/instantiated/src/test/org/apache/lucene/store/instantiated/TestIndicesEquals.java:r909334

Index: contrib/instantiated/src/test/org/apache/lucene/store/instantiated/TestSerialization.java
===================================================================
--- contrib/instantiated/src/test/org/apache/lucene/store/instantiated/TestSerialization.java	(revision 916567)
+++ contrib/instantiated/src/test/org/apache/lucene/store/instantiated/TestSerialization.java	(working copy)
@@ -20,6 +20,7 @@
 import junit.framework.TestCase;
 import org.apache.lucene.store.RAMDirectory;
 import org.apache.lucene.store.Directory;
+import org.apache.lucene.util.Version;
 import org.apache.lucene.index.IndexWriter;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.analysis.WhitespaceAnalyzer;
@@ -35,7 +36,7 @@
 
     Directory dir = new RAMDirectory();
 
-    IndexWriter iw = new IndexWriter(dir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.UNLIMITED);
+    IndexWriter iw = new IndexWriter(dir, new WhitespaceAnalyzer(Version.LUCENE_CURRENT), true, IndexWriter.MaxFieldLength.UNLIMITED);
     Document doc = new Document();
     doc.add(new Field("foo", "bar rab abr bra rba", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));
     doc.add(new Field("moo", "bar rab abr bra rba", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));
Index: contrib/instantiated/src/test/org/apache/lucene/store/instantiated/TestUnoptimizedReaderOnConstructor.java
===================================================================
--- contrib/instantiated/src/test/org/apache/lucene/store/instantiated/TestUnoptimizedReaderOnConstructor.java	(revision 916567)
+++ contrib/instantiated/src/test/org/apache/lucene/store/instantiated/TestUnoptimizedReaderOnConstructor.java	(working copy)
@@ -24,6 +24,7 @@
 import org.apache.lucene.index.IndexWriter;
 import org.apache.lucene.store.RAMDirectory;
 import org.apache.lucene.store.Directory;
+import org.apache.lucene.util.Version;
 import org.apache.lucene.analysis.WhitespaceAnalyzer;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
@@ -35,17 +36,17 @@
 
   public void test() throws Exception {
     Directory dir = new RAMDirectory();
-    IndexWriter iw = new IndexWriter(dir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.UNLIMITED);
+    IndexWriter iw = new IndexWriter(dir, new WhitespaceAnalyzer(Version.LUCENE_CURRENT), true, IndexWriter.MaxFieldLength.UNLIMITED);
     addDocument(iw, "Hello, world!");
     addDocument(iw, "All work and no play makes jack a dull boy");
     iw.close();
 
-    iw = new IndexWriter(dir, new WhitespaceAnalyzer(), false, IndexWriter.MaxFieldLength.UNLIMITED);
+    iw = new IndexWriter(dir, new WhitespaceAnalyzer(Version.LUCENE_CURRENT), false, IndexWriter.MaxFieldLength.UNLIMITED);
     addDocument(iw, "Hello, tellus!");
     addDocument(iw, "All work and no play makes danny a dull boy");
     iw.close();
 
-    iw = new IndexWriter(dir, new WhitespaceAnalyzer(), false, IndexWriter.MaxFieldLength.UNLIMITED);
+    iw = new IndexWriter(dir, new WhitespaceAnalyzer(Version.LUCENE_CURRENT), false, IndexWriter.MaxFieldLength.UNLIMITED);
     addDocument(iw, "Hello, earth!");
     addDocument(iw, "All work and no play makes wendy a dull girl");
     iw.close();
Index: contrib/instantiated/src/java/org/apache/lucene/store/instantiated/package.html
===================================================================
--- contrib/instantiated/src/java/org/apache/lucene/store/instantiated/package.html	(revision 916567)
+++ contrib/instantiated/src/java/org/apache/lucene/store/instantiated/package.html	(working copy)
@@ -22,7 +22,7 @@
 <body>
 <p>InstantiatedIndex, alternative RAM store for small corpora.</p>
 
-<p>WARNING: This contrib is experimental and the APIs may change without warning.</p>
+<p>@lucene.experimental</p>
 <h2>Abstract</h2>
 
 <p>
Index: contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedIndex.java
===================================================================
--- contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedIndex.java	(revision 916567)
+++ contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedIndex.java	(working copy)
@@ -43,7 +43,7 @@
  * results up to a 100 times faster than the file-centric RAMDirectory
  * at the cost of greater RAM consumption.
  * <p>
- * WARNING: This contrib is experimental and the APIs may change without warning.
+ * @lucene.experimental
  * <p>
  * There are no read and write locks in this store.
  * {@link InstantiatedIndexReader} {@link InstantiatedIndexReader#isCurrent()} all the time
Index: contrib/fast-vector-highlighter/src/test/org/apache/lucene/search/vectorhighlight/AbstractTestCase.java
===================================================================
--- contrib/fast-vector-highlighter/src/test/org/apache/lucene/search/vectorhighlight/AbstractTestCase.java	(revision 916567)
+++ contrib/fast-vector-highlighter/src/test/org/apache/lucene/search/vectorhighlight/AbstractTestCase.java	(working copy)
@@ -24,6 +24,7 @@
 import junit.framework.TestCase;
 
 import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.analysis.KeywordAnalyzer;
 import org.apache.lucene.analysis.Token;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
@@ -40,6 +41,7 @@
 import org.apache.lucene.index.Term;
 import org.apache.lucene.index.IndexWriter.MaxFieldLength;
 import org.apache.lucene.queryParser.QueryParser;
+import org.apache.lucene.search.DisjunctionMaxQuery;
 import org.apache.lucene.search.PhraseQuery;
 import org.apache.lucene.search.Query;
 import org.apache.lucene.search.TermQuery;
@@ -55,6 +57,7 @@
   protected Directory dir;
   protected Analyzer analyzerW;
   protected Analyzer analyzerB;
+  protected Analyzer analyzerK;
   protected IndexReader reader;  
   protected QueryParser paW;
   protected QueryParser paB;
@@ -75,11 +78,18 @@
     "\nLucene/Solr does not require such additional hardware.",
     "\nWhen you talk about processing speed, the"
   };
+  
+  protected static final String[] strMVValues = {
+    "abc",
+    "defg",
+    "hijkl"
+  };
 
   @Override
   protected void setUp() throws Exception {
-    analyzerW = new WhitespaceAnalyzer();
+    analyzerW = new WhitespaceAnalyzer(Version.LUCENE_CURRENT);
     analyzerB = new BigramAnalyzer();
+    analyzerK = new KeywordAnalyzer();
     paW = new QueryParser(Version.LUCENE_CURRENT,  F, analyzerW );
     paB = new QueryParser(Version.LUCENE_CURRENT,  F, analyzerB );
     dir = new RAMDirectory();
@@ -141,6 +151,18 @@
     return query;
   }
   
+  protected Query dmq( Query... queries ){
+    return dmq( 0.0F, queries );
+  }
+  
+  protected Query dmq( float tieBreakerMultiplier, Query... queries ){
+    DisjunctionMaxQuery query = new DisjunctionMaxQuery( tieBreakerMultiplier );
+    for( Query q : queries ){
+      query.add( q );
+    }
+    return query;
+  }
+  
   protected void assertCollectionQueries( Collection<Query> actual, Query... expected ){
     assertEquals( expected.length, actual.size() );
     for( Query query : expected ){
@@ -205,7 +227,7 @@
     public boolean incrementToken() throws IOException {
       if( !getNextPartialSnippet() )
         return false;
-      
+      clearAttributes();
       termAtt.setTermBuffer(snippet, startTerm, lenTerm);
       offsetAtt.setOffset(correctOffset(startOffset), correctOffset(startOffset + lenTerm));
       return true;
@@ -301,6 +323,7 @@
     make1dmfIndex( analyzerB, values );
   }
   
+  // make 1 doc with multi valued field
   protected void make1dmfIndex( Analyzer analyzer, String... values ) throws Exception {
     IndexWriter writer = new IndexWriter( dir, analyzer, true, MaxFieldLength.LIMITED );
     Document doc = new Document();
@@ -312,6 +335,18 @@
     reader = IndexReader.open( dir, true );
   }
   
+  // make 1 doc with multi valued & not analyzed field
+  protected void make1dmfIndexNA( String... values ) throws Exception {
+    IndexWriter writer = new IndexWriter( dir, analyzerK, true, MaxFieldLength.LIMITED );
+    Document doc = new Document();
+    for( String value: values )
+      doc.add( new Field( F, value, Store.YES, Index.NOT_ANALYZED, TermVector.WITH_POSITIONS_OFFSETS ) );
+    writer.addDocument( doc );
+    writer.close();
+
+    reader = IndexReader.open( dir, true );
+  }
+  
   protected void makeIndexShortMV() throws Exception {
 
     //  012345
@@ -373,4 +408,18 @@
 
     make1dmfIndexB( biMVValues );
   }
+  
+  protected void makeIndexStrMV() throws Exception {
+
+    //  0123
+    // "abc"
+    
+    //  34567
+    // "defg"
+
+    //     111
+    //  789012
+    // "hijkl"
+    make1dmfIndexNA( strMVValues );
+  }
 }
Index: contrib/fast-vector-highlighter/src/test/org/apache/lucene/search/vectorhighlight/SimpleFragmentsBuilderTest.java
===================================================================
--- contrib/fast-vector-highlighter/src/test/org/apache/lucene/search/vectorhighlight/SimpleFragmentsBuilderTest.java	(revision 916567)
+++ contrib/fast-vector-highlighter/src/test/org/apache/lucene/search/vectorhighlight/SimpleFragmentsBuilderTest.java	(working copy)
@@ -127,4 +127,16 @@
 
     reader = IndexReader.open( dir, true );
   }
+  
+  public void test1StrMV() throws Exception {
+    makeIndexStrMV();
+
+    FieldQuery fq = new FieldQuery( tq( "defg" ), true, true );
+    FieldTermStack stack = new FieldTermStack( reader, 0, F, fq );
+    FieldPhraseList fpl = new FieldPhraseList( stack, fq );
+    SimpleFragListBuilder sflb = new SimpleFragListBuilder();
+    FieldFragList ffl = sflb.createFieldFragList( fpl, 100 );
+    SimpleFragmentsBuilder sfb = new SimpleFragmentsBuilder();
+    assertEquals( "abc<b>defg</b>hijkl", sfb.createFragment( reader, 0, F, ffl ) );
+  }
 }
Index: contrib/fast-vector-highlighter/src/test/org/apache/lucene/search/vectorhighlight/FieldQueryTest.java
===================================================================
--- contrib/fast-vector-highlighter/src/test/org/apache/lucene/search/vectorhighlight/FieldQueryTest.java	(revision 916567)
+++ contrib/fast-vector-highlighter/src/test/org/apache/lucene/search/vectorhighlight/FieldQueryTest.java	(working copy)
@@ -38,6 +38,14 @@
     assertCollectionQueries( flatQueries, tq( "A" ), tq( "B" ), tq( "C" ) );
   }
 
+  public void testFlattenDisjunctionMaxQuery() throws Exception {
+    Query query = dmq( tq( "A" ), tq( "B" ), pqF( "C", "D" ) );
+    FieldQuery fq = new FieldQuery( query, true, true );
+    Set<Query> flatQueries = new HashSet<Query>();
+    fq.flatten( query, flatQueries );
+    assertCollectionQueries( flatQueries, tq( "A" ), tq( "B" ), pqF( "C", "D" ) );
+  }
+
   public void testFlattenTermAndPhrase() throws Exception {
     Query query = paW.parse( "A AND \"B C\"" );
     FieldQuery fq = new FieldQuery( query, true, true );
Index: contrib/fast-vector-highlighter/src/java/org/apache/lucene/search/vectorhighlight/BaseFragmentsBuilder.java
===================================================================
--- contrib/fast-vector-highlighter/src/java/org/apache/lucene/search/vectorhighlight/BaseFragmentsBuilder.java	(revision 916567)
+++ contrib/fast-vector-highlighter/src/java/org/apache/lucene/search/vectorhighlight/BaseFragmentsBuilder.java	(working copy)
@@ -22,6 +22,7 @@
 import java.util.List;
 
 import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
 import org.apache.lucene.document.MapFieldSelector;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.search.vectorhighlight.FieldFragList.WeightedFragInfo;
@@ -72,7 +73,7 @@
     List<WeightedFragInfo> fragInfos = getWeightedFragInfoList( fieldFragList.fragInfos );
     
     List<String> fragments = new ArrayList<String>( maxNumFragments );
-    String[] values = getFieldValues( reader, docId, fieldName );
+    Field[] values = getFields( reader, docId, fieldName );
     if( values.length == 0 ) return null;
     StringBuilder buffer = new StringBuilder();
     int[] nextValueIndex = { 0 };
@@ -83,15 +84,31 @@
     return fragments.toArray( new String[fragments.size()] );
   }
   
+  @Deprecated
   protected String[] getFieldValues( IndexReader reader, int docId, String fieldName) throws IOException {
     Document doc = reader.document( docId, new MapFieldSelector( new String[]{ fieldName } ) );
     return doc.getValues( fieldName ); // according to Document class javadoc, this never returns null
   }
+  
+  protected Field[] getFields( IndexReader reader, int docId, String fieldName) throws IOException {
+    // according to javadoc, doc.getFields(fieldName) cannot be used with lazy loaded field???
+    Document doc = reader.document( docId, new MapFieldSelector( new String[]{ fieldName } ) );
+    return doc.getFields( fieldName ); // according to Document class javadoc, this never returns null
+  }
 
+  @Deprecated
   protected String makeFragment( StringBuilder buffer, int[] index, String[] values, WeightedFragInfo fragInfo ){
+    final int s = fragInfo.startOffset;
+    return makeFragment( fragInfo, getFragmentSource( buffer, index, values, s, fragInfo.endOffset ), s );
+  }
+
+  protected String makeFragment( StringBuilder buffer, int[] index, Field[] values, WeightedFragInfo fragInfo ){
+    final int s = fragInfo.startOffset;
+    return makeFragment( fragInfo, getFragmentSource( buffer, index, values, s, fragInfo.endOffset ), s );
+  }
+  
+  private String makeFragment( WeightedFragInfo fragInfo, String src, int s ){
     StringBuilder fragment = new StringBuilder();
-    final int s = fragInfo.startOffset;
-    String src = getFragmentSource( buffer, index, values, s, fragInfo.endOffset );
     int srcIndex = 0;
     for( SubInfo subInfo : fragInfo.subInfos ){
       for( Toffs to : subInfo.termsOffsets ){
@@ -104,6 +121,7 @@
     return fragment.toString();
   }
   
+  @Deprecated
   protected String getFragmentSource( StringBuilder buffer, int[] index, String[] values,
       int startOffset, int endOffset ){
     while( buffer.length() < endOffset && index[0] < values.length ){
@@ -114,6 +132,17 @@
     int eo = buffer.length() < endOffset ? buffer.length() : endOffset;
     return buffer.substring( startOffset, eo );
   }
+
+  protected String getFragmentSource( StringBuilder buffer, int[] index, Field[] values,
+      int startOffset, int endOffset ){
+    while( buffer.length() < endOffset && index[0] < values.length ){
+      if( index[0] > 0 && values[index[0]].isTokenized() && values[index[0]].stringValue().length() > 0 )
+        buffer.append( ' ' );
+      buffer.append( values[index[0]++].stringValue() );
+    }
+    int eo = buffer.length() < endOffset ? buffer.length() : endOffset;
+    return buffer.substring( startOffset, eo );
+  }
   
   protected String getPreTag( int num ){
     return preTags.length > num ? preTags[num] : preTags[0];
Index: contrib/fast-vector-highlighter/src/java/org/apache/lucene/search/vectorhighlight/FieldQuery.java
===================================================================
--- contrib/fast-vector-highlighter/src/java/org/apache/lucene/search/vectorhighlight/FieldQuery.java	(revision 916567)
+++ contrib/fast-vector-highlighter/src/java/org/apache/lucene/search/vectorhighlight/FieldQuery.java	(working copy)
@@ -27,6 +27,7 @@
 import org.apache.lucene.index.Term;
 import org.apache.lucene.search.BooleanClause;
 import org.apache.lucene.search.BooleanQuery;
+import org.apache.lucene.search.DisjunctionMaxQuery;
 import org.apache.lucene.search.PhraseQuery;
 import org.apache.lucene.search.Query;
 import org.apache.lucene.search.TermQuery;
@@ -78,6 +79,12 @@
           flatten( clause.getQuery(), flatQueries );
       }
     }
+    else if( sourceQuery instanceof DisjunctionMaxQuery ){
+      DisjunctionMaxQuery dmq = (DisjunctionMaxQuery)sourceQuery;
+      for( Query query : dmq ){
+        flatten( query, flatQueries );
+      }
+    }
     else if( sourceQuery instanceof TermQuery ){
       if( !flatQueries.contains( sourceQuery ) )
         flatQueries.add( sourceQuery );
Index: contrib/benchmark/conf/basicNRT.alg
===================================================================
--- contrib/benchmark/conf/basicNRT.alg	(revision 0)
+++ contrib/benchmark/conf/basicNRT.alg	(revision 0)
@@ -0,0 +1,80 @@
+#/**
+# * Licensed to the Apache Software Foundation (ASF) under one or more
+# * contributor license agreements.  See the NOTICE file distributed with
+# * this work for additional information regarding copyright ownership.
+# * The ASF licenses this file to You under the Apache License, Version 2.0
+# * (the "License"); you may not use this file except in compliance with
+# * the License.  You may obtain a copy of the License at
+# *
+# *     http://www.apache.org/licenses/LICENSE-2.0
+# *
+# * Unless required by applicable law or agreed to in writing, software
+# * distributed under the License is distributed on an "AS IS" BASIS,
+# * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# * See the License for the specific language governing permissions and
+# * limitations under the License.
+# */
+# -------------------------------------------------------------------------------------
+# multi val params are iterated by NewRound's, added to reports, start with column name.
+#
+# based on micro-standard
+#
+# modified to use wikipedia sources and index entire docs
+# currently just used to measure ingest rate
+
+analyzer=org.apache.lucene.analysis.standard.StandardAnalyzer
+directory=FSDirectory
+
+work.dir = /x/lucene/wiki.5M
+
+doc.stored=true
+doc.body.stored=false
+doc.tokenized=false
+doc.body.tokenized=true
+doc.term.vector=false
+log.step.AddDoc = 10000
+log.step.Search = 10000
+compound = false
+
+content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource
+content.source.forever = false
+file.query.maker.file = queries.txt
+
+query.maker=org.apache.lucene.benchmark.byTask.feeds.FileBasedQueryMaker
+docs.file = /x/lucene/enwiki-20090306-lines-1k-fixed.txt
+
+# task at this depth or less would print when they start
+task.max.depth.log=2
+
+log.queries=true
+
+# -------------------------------------------------------------------------------------
+
+# Open a writer
+OpenIndex
+{
+  # Get a new near-real-time reader, once per second:
+  NearRealtimeReader(1.0) &
+
+  # Warm
+  Search
+
+  # Index with 2 threads, each adding 100 docs per sec
+  [ "Indexing" { AddDoc > : * : 100/sec ] : 2 &
+
+  # Redline search (from queries.txt) with 4 threads
+  [ "Searching" { Search > : * ] : 4 &
+
+  # Wait 60 sec, then wrap up
+  Wait(5.0)
+}
+CloseReader
+
+# Don't keep any changes, so we can re-test on the same index again
+RollbackIndex
+
+RepSumByPref Indexing
+RepSumByPref Searching
+RepSumByPref NearRealtimeReader
+
+

Property changes on: contrib\benchmark\conf\basicNRT.alg
___________________________________________________________________
Added: svn:eol-style
   + native

Index: contrib/benchmark/conf/collation.alg
===================================================================
--- contrib/benchmark/conf/collation.alg	(revision 916567)
+++ contrib/benchmark/conf/collation.alg	(working copy)
@@ -1,97 +1,97 @@
-# Licensed to the Apache Software Foundation (ASF) under one or more
-# contributor license agreements.  See the NOTICE file distributed with
-# this work for additional information regarding copyright ownership.
-# The ASF licenses this file to You under the Apache License, Version 2.0
-# (the "License"); you may not use this file except in compliance with
-# the License.  You may obtain a copy of the License at
-# 
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource
-content.source.encoding=UTF-8
-doc.tokenized=false
-doc.body.tokenized=true
-docs.file=work/top100k-out/top.fr.wikipedia.words.txt
-content.source.forever=false
-log.step=100000
-
-{ "Rounds"
-    -NewAnalyzer(KeywordAnalyzer)
-    -SetProp(docs.file,work/top100k-out/top.fr.wikipedia.words.txt)
-    ResetInputs
-    { "FrenchKeyword" { ReadTokens > : * ResetInputs } : 10
-
-    -NewAnalyzer(KeywordAnalyzer)
-    -SetProp(docs.file,work/top100k-out/top.de.wikipedia.words.txt)
-    ResetInputs
-    { "GermanKeyword" { ReadTokens > : * ResetInputs } : 10
-
-    -NewAnalyzer(KeywordAnalyzer)
-    -SetProp(docs.file,work/top100k-out/top.uk.wikipedia.words.txt)
-    ResetInputs
-    { "UkrainianKeyword" { ReadTokens > : * ResetInputs } : 10
- 
-    -NewAnalyzer(KeywordAnalyzer)
-    -SetProp(docs.file,work/top100k-out/top.en.wikipedia.words.txt)
-    ResetInputs
-    { "EnglishKeyword" { ReadTokens > : * ResetInputs } : 10
- 
-    -NewLocale(fr)
-    -NewCollationAnalyzer
-    -SetProp(docs.file,work/top100k-out/top.fr.wikipedia.words.txt)
-    ResetInputs
-    { "FrenchJDK" { ReadTokens > : * ResetInputs } : 10
-
-    -NewLocale(de)
-    -NewCollationAnalyzer
-    -SetProp(docs.file,work/top100k-out/top.de.wikipedia.words.txt)
-    ResetInputs
-    { "GermanJDK" { ReadTokens > : * ResetInputs } : 10
-
-    -NewLocale(uk)
-    -NewCollationAnalyzer
-    -SetProp(docs.file,work/top100k-out/top.uk.wikipedia.words.txt)
-    ResetInputs
-    { "UkrainianJDK" { ReadTokens > : * ResetInputs } : 10
-
-    -NewLocale(en)
-    -NewCollationAnalyzer
-    -SetProp(docs.file,work/top100k-out/top.en.wikipedia.words.txt)
-    ResetInputs
-    { "EnglishJDK" { ReadTokens > : * ResetInputs } : 10
-
-    -NewLocale(fr)
-    -NewCollationAnalyzer(impl:icu)
-    -SetProp(docs.file,work/top100k-out/top.fr.wikipedia.words.txt)
-    ResetInputs
-    { "FrenchICU" { ReadTokens > : * ResetInputs } : 10
-
-    -NewLocale(de)
-    -NewCollationAnalyzer(impl:icu)
-    -SetProp(docs.file,work/top100k-out/top.de.wikipedia.words.txt)
-    ResetInputs
-    { "GermanICU" { ReadTokens > : * ResetInputs } : 10
-
-    -NewLocale(uk)
-    -NewCollationAnalyzer(impl:icu)
-    -SetProp(docs.file,work/top100k-out/top.uk.wikipedia.words.txt)
-    ResetInputs
-    { "UkrainianICU" { ReadTokens > : * ResetInputs } : 10
-
-    -NewLocale(en)
-    -NewCollationAnalyzer(impl:icu)
-    -SetProp(docs.file,work/top100k-out/top.en.wikipedia.words.txt)
-    ResetInputs
-    { "EnglishICU" { ReadTokens > : * ResetInputs } : 10
-
-    NewRound
-
-} : 5
-
-RepSumByNameRound
+# Licensed to the Apache Software Foundation (ASF) under one or more
+# contributor license agreements.  See the NOTICE file distributed with
+# this work for additional information regarding copyright ownership.
+# The ASF licenses this file to You under the Apache License, Version 2.0
+# (the "License"); you may not use this file except in compliance with
+# the License.  You may obtain a copy of the License at
+# 
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource
+content.source.encoding=UTF-8
+doc.tokenized=false
+doc.body.tokenized=true
+docs.file=work/top100k-out/top.fr.wikipedia.words.txt
+content.source.forever=false
+log.step=100000
+
+{ "Rounds"
+    -NewAnalyzer(KeywordAnalyzer)
+    -SetProp(docs.file,work/top100k-out/top.fr.wikipedia.words.txt)
+    ResetInputs
+    { "FrenchKeyword" { ReadTokens > : * ResetInputs } : 10
+
+    -NewAnalyzer(KeywordAnalyzer)
+    -SetProp(docs.file,work/top100k-out/top.de.wikipedia.words.txt)
+    ResetInputs
+    { "GermanKeyword" { ReadTokens > : * ResetInputs } : 10
+
+    -NewAnalyzer(KeywordAnalyzer)
+    -SetProp(docs.file,work/top100k-out/top.uk.wikipedia.words.txt)
+    ResetInputs
+    { "UkrainianKeyword" { ReadTokens > : * ResetInputs } : 10
+ 
+    -NewAnalyzer(KeywordAnalyzer)
+    -SetProp(docs.file,work/top100k-out/top.en.wikipedia.words.txt)
+    ResetInputs
+    { "EnglishKeyword" { ReadTokens > : * ResetInputs } : 10
+ 
+    -NewLocale(fr)
+    -NewCollationAnalyzer
+    -SetProp(docs.file,work/top100k-out/top.fr.wikipedia.words.txt)
+    ResetInputs
+    { "FrenchJDK" { ReadTokens > : * ResetInputs } : 10
+
+    -NewLocale(de)
+    -NewCollationAnalyzer
+    -SetProp(docs.file,work/top100k-out/top.de.wikipedia.words.txt)
+    ResetInputs
+    { "GermanJDK" { ReadTokens > : * ResetInputs } : 10
+
+    -NewLocale(uk)
+    -NewCollationAnalyzer
+    -SetProp(docs.file,work/top100k-out/top.uk.wikipedia.words.txt)
+    ResetInputs
+    { "UkrainianJDK" { ReadTokens > : * ResetInputs } : 10
+
+    -NewLocale(en)
+    -NewCollationAnalyzer
+    -SetProp(docs.file,work/top100k-out/top.en.wikipedia.words.txt)
+    ResetInputs
+    { "EnglishJDK" { ReadTokens > : * ResetInputs } : 10
+
+    -NewLocale(fr)
+    -NewCollationAnalyzer(impl:icu)
+    -SetProp(docs.file,work/top100k-out/top.fr.wikipedia.words.txt)
+    ResetInputs
+    { "FrenchICU" { ReadTokens > : * ResetInputs } : 10
+
+    -NewLocale(de)
+    -NewCollationAnalyzer(impl:icu)
+    -SetProp(docs.file,work/top100k-out/top.de.wikipedia.words.txt)
+    ResetInputs
+    { "GermanICU" { ReadTokens > : * ResetInputs } : 10
+
+    -NewLocale(uk)
+    -NewCollationAnalyzer(impl:icu)
+    -SetProp(docs.file,work/top100k-out/top.uk.wikipedia.words.txt)
+    ResetInputs
+    { "UkrainianICU" { ReadTokens > : * ResetInputs } : 10
+
+    -NewLocale(en)
+    -NewCollationAnalyzer(impl:icu)
+    -SetProp(docs.file,work/top100k-out/top.en.wikipedia.words.txt)
+    ResetInputs
+    { "EnglishICU" { ReadTokens > : * ResetInputs } : 10
+
+    NewRound
+
+} : 5
+
+RepSumByNameRound

Property changes on: contrib\benchmark\conf\collation.alg
___________________________________________________________________
Added: svn:eol-style
   + native


Property changes on: contrib\benchmark\conf\highlight-vs-vector-highlight.alg
___________________________________________________________________
Added: svn:eol-style
   + native


Property changes on: contrib\benchmark\conf\vector-highlight-profile.alg
___________________________________________________________________
Added: svn:eol-style
   + native

Index: contrib/benchmark/CHANGES.txt
===================================================================
--- contrib/benchmark/CHANGES.txt	(revision 916567)
+++ contrib/benchmark/CHANGES.txt	(working copy)
@@ -2,8 +2,20 @@
 
 The Benchmark contrib package contains code for benchmarking Lucene in a variety of ways.
 
-$Id:$
+2/21/2020
+  LUCENE-2254: Add support to the quality package for running
+  experiments with any combination of Title, Description, and Narrative.
+  (Robert Muir)
 
+1/28/2010
+  LUCENE-2223: Add a benchmark for ShingleFilter. You can wrap any
+  analyzer with ShingleAnalyzerWrapper and specify shingle parameters
+  with the NewShingleAnalyzer task.  (Steven Rowe via Robert Muir)
+
+1/14/2010
+  LUCENE-2210: TrecTopicsReader now properly reads descriptions and
+  narratives from trec topics files.  (Robert Muir)
+
 1/11/2010
   LUCENE-2181: Add a benchmark for collation. This adds NewLocaleTask,
   which sets a Locale in the run data for collation to use, and can be
Index: contrib/benchmark/src/test/org/apache/lucene/benchmark/quality/TestQualityRun.java
===================================================================
--- contrib/benchmark/src/test/org/apache/lucene/benchmark/quality/TestQualityRun.java	(revision 916567)
+++ contrib/benchmark/src/test/org/apache/lucene/benchmark/quality/TestQualityRun.java	(working copy)
@@ -20,10 +20,11 @@
 import java.io.BufferedReader;
 import java.io.File;
 import java.io.FileReader;
+import java.io.InputStream;
+import java.io.InputStreamReader;
 import java.io.PrintWriter;
 
 import org.apache.lucene.benchmark.byTask.TestPerfTasksLogic;
-import org.apache.lucene.benchmark.byTask.feeds.ReutersContentSource;
 import org.apache.lucene.benchmark.quality.Judge;
 import org.apache.lucene.benchmark.quality.QualityQuery;
 import org.apache.lucene.benchmark.quality.QualityQueryParser;
@@ -39,6 +40,10 @@
 
 /**
  * Test that quality run does its job.
+ * <p>
+ * NOTE: if the default scoring or StandardAnalyzer is changed, then
+ * this test will not work correctly, as it does not dynamically
+ * generate its test trec topics/qrels!
  */
 public class TestQualityRun extends TestCase {
 
@@ -52,14 +57,14 @@
   }
 
   public void testTrecQuality() throws Exception {
-    // first create the complete reuters index
+    // first create the partial reuters index
     createReutersIndex();
     
     File workDir = new File(System.getProperty("benchmark.work.dir","work"));
     assertTrue("Bad workDir: "+workDir, workDir.exists()&& workDir.isDirectory());
 
     int maxResults = 1000;
-    String docNameField = "docid"; 
+    String docNameField = "doctitle"; // orig docID is in the linedoc format title 
     
     PrintWriter logger = DEBUG ? new PrintWriter(System.out,true) : null;
 
@@ -105,13 +110,13 @@
         assertTrue("avg-p should be hurt: "+s.getAvp(), 1.0 > s.getAvp());
         assertTrue("recall should be hurt: "+s.getRecall(), 1.0 > s.getRecall());
         for (int j = 1; j <= QualityStats.MAX_POINTS; j++) {
-          assertEquals("p_at_"+j+" should be perfect: "+s.getPrecisionAt(j), 1.0, s.getPrecisionAt(j), 1E-9);
+          assertEquals("p_at_"+j+" should be perfect: "+s.getPrecisionAt(j), 1.0, s.getPrecisionAt(j), 1E-2);
         }
         break;
       
       case 1:
         assertTrue("avg-p should be hurt", 1.0 > s.getAvp());
-        assertEquals("recall should be perfect: "+s.getRecall(), 1.0, s.getRecall(), 1E-9);
+        assertEquals("recall should be perfect: "+s.getRecall(), 1.0, s.getRecall(), 1E-2);
         for (int j = 1; j <= QualityStats.MAX_POINTS; j++) {
           assertTrue("p_at_"+j+" should be hurt: "+s.getPrecisionAt(j), 1.0 > s.getPrecisionAt(j));
         }
@@ -126,10 +131,10 @@
         break;
 
       default: {
-        assertEquals("avg-p should be perfect: "+s.getAvp(), 1.0, s.getAvp(), 1E-9);
-        assertEquals("recall should be perfect: "+s.getRecall(), 1.0, s.getRecall(), 1E-9);
+        assertEquals("avg-p should be perfect: "+s.getAvp(), 1.0, s.getAvp(), 1E-2);
+        assertEquals("recall should be perfect: "+s.getRecall(), 1.0, s.getRecall(), 1E-2);
         for (int j = 1; j <= QualityStats.MAX_POINTS; j++) {
-          assertEquals("p_at_"+j+" should be perfect: "+s.getPrecisionAt(j), 1.0, s.getPrecisionAt(j), 1E-9);
+          assertEquals("p_at_"+j+" should be perfect: "+s.getPrecisionAt(j), 1.0, s.getPrecisionAt(j), 1E-2);
         }
       }
       
@@ -149,13 +154,45 @@
 
     
   }
+  
+  public void testTrecTopicsReader() throws Exception {    
+    // prepare topics
+    InputStream topicsFile = getClass().getResourceAsStream("trecTopics.txt");
+    TrecTopicsReader qReader = new TrecTopicsReader();
+    QualityQuery qqs[] = qReader.readQueries(
+        new BufferedReader(new InputStreamReader(topicsFile, "UTF-8")));
+    
+    assertEquals(20, qqs.length);
+    
+    QualityQuery qq = qqs[0];
+    assertEquals("statement months  total 1987", qq.getValue("title"));
+    assertEquals("Topic 0 Description Line 1 Topic 0 Description Line 2", 
+        qq.getValue("description"));
+    assertEquals("Topic 0 Narrative Line 1 Topic 0 Narrative Line 2", 
+        qq.getValue("narrative"));
+    
+    qq = qqs[1];
+    assertEquals("agreed 15  against five", qq.getValue("title"));
+    assertEquals("Topic 1 Description Line 1 Topic 1 Description Line 2", 
+        qq.getValue("description"));
+    assertEquals("Topic 1 Narrative Line 1 Topic 1 Narrative Line 2", 
+        qq.getValue("narrative"));
+    
+    qq = qqs[19];
+    assertEquals("20 while  common week", qq.getValue("title"));
+    assertEquals("Topic 19 Description Line 1 Topic 19 Description Line 2", 
+        qq.getValue("description"));
+    assertEquals("Topic 19 Narrative Line 1 Topic 19 Narrative Line 2", 
+        qq.getValue("narrative"));
+  }
 
-  // use benchmark logic to create the full Reuters index
+  // use benchmark logic to create the mini Reuters index
   private void createReutersIndex() throws Exception {
     // 1. alg definition
     String algLines[] = {
         "# ----- properties ",
-        "content.source="+ReutersContentSource.class.getName(),
+        "content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource",
+        "docs.file=" + getReuters578LinesFile(),
         "content.source.log.step=2500",
         "doc.term.vector=false",
         "content.source.forever=false",
@@ -172,4 +209,9 @@
     // 2. execute the algorithm  (required in every "logic" test)
     TestPerfTasksLogic.execBenchmark(algLines);
   }
+  
+  private static String getReuters578LinesFile() {
+    return System.getProperty("lucene.common.dir").replace('\\','/') +
+      "/contrib/benchmark/src/test/org/apache/lucene/benchmark/quality/reuters.578.lines.txt.bz2";
+  }  
 }
Index: contrib/benchmark/src/test/org/apache/lucene/benchmark/quality/trecTopics.txt
===================================================================
--- contrib/benchmark/src/test/org/apache/lucene/benchmark/quality/trecTopics.txt	(revision 916567)
+++ contrib/benchmark/src/test/org/apache/lucene/benchmark/quality/trecTopics.txt	(working copy)
@@ -26,11 +26,13 @@
 <title> statement months  total 1987
 
 <desc> Description:
+Topic 0 Description Line 1
+Topic 0 Description Line 2
 
-
 <narr> Narrative:
+Topic 0 Narrative Line 1
+Topic 0 Narrative Line 2
 
-
 </top>
 
 <top>
@@ -39,11 +41,13 @@
 <title> agreed 15  against five
 
 <desc> Description:
+Topic 1 Description Line 1
+Topic 1 Description Line 2
 
-
 <narr> Narrative:
+Topic 1 Narrative Line 1
+Topic 1 Narrative Line 2
 
-
 </top>
 
 <top>
@@ -273,9 +277,11 @@
 <title> 20 while  common week
 
 <desc> Description:
+Topic 19 Description Line 1
+Topic 19 Description Line 2
 
-
 <narr> Narrative:
+Topic 19 Narrative Line 1
+Topic 19 Narrative Line 2
 
-
 </top>
Index: contrib/benchmark/src/test/org/apache/lucene/benchmark/byTask/TestPerfTasksLogic.java
===================================================================
--- contrib/benchmark/src/test/org/apache/lucene/benchmark/byTask/TestPerfTasksLogic.java	(revision 916567)
+++ contrib/benchmark/src/test/org/apache/lucene/benchmark/byTask/TestPerfTasksLogic.java	(working copy)
@@ -979,6 +979,79 @@
     return algLines;
   }
   
+  /**
+   * Test that we can create ShingleAnalyzerWrappers.
+   */
+  public void testShingleAnalyzer() throws Exception {
+    String text = "one,two,three, four five six";
+    
+    // Default analyzer, maxShingleSize, and outputUnigrams
+    Benchmark benchmark = execBenchmark(getShingleConfig(""));
+    TokenStream stream = benchmark.getRunData().getAnalyzer().tokenStream
+      ("bogus", new StringReader(text));
+    assertEqualShingle(benchmark.getRunData().getAnalyzer(), text,
+                       new String[] {"one", "one two", "two", "two three",
+                                     "three", "three four", "four", "four five",
+                                     "five", "five six", "six"});
+    // Default analyzer, maxShingleSize = 3, and outputUnigrams = false
+    benchmark = execBenchmark
+      (getShingleConfig("maxShingleSize:3,outputUnigrams:false"));
+    assertEqualShingle(benchmark.getRunData().getAnalyzer(), text,
+                       new String[] { "one two", "one two three", "two three",
+                                      "two three four", "three four", 
+                                      "three four five", "four five",
+                                      "four five six", "five six" });
+    // WhitespaceAnalyzer, default maxShingleSize and outputUnigrams
+    benchmark = execBenchmark
+      (getShingleConfig("analyzer:WhitespaceAnalyzer"));
+    assertEqualShingle(benchmark.getRunData().getAnalyzer(), text,
+                       new String[] { "one,two,three,", "one,two,three, four",
+                                      "four", "four five", "five", "five six", 
+                                      "six" });
+    
+    // WhitespaceAnalyzer, maxShingleSize=3 and outputUnigrams=false
+    benchmark = execBenchmark
+      (getShingleConfig
+        ("outputUnigrams:false,maxShingleSize:3,analyzer:WhitespaceAnalyzer"));
+    assertEqualShingle(benchmark.getRunData().getAnalyzer(), text,
+                       new String[] { "one,two,three, four", 
+                                      "one,two,three, four five",
+                                      "four five", "four five six",
+                                      "five six" });
+  }
+  
+  private void assertEqualShingle
+    (Analyzer analyzer, String text, String[] expected) throws Exception {
+    TokenStream stream = analyzer.tokenStream("bogus", new StringReader(text));
+    stream.reset();
+    TermAttribute termAtt = stream.addAttribute(TermAttribute.class);
+    int termNum = 0;
+    while (stream.incrementToken()) {
+      assertTrue("Extra output term(s), starting with '"
+                 + new String(termAtt.termBuffer(), 0, termAtt.termLength()) + "'",
+                 termNum < expected.length);
+      assertEquals("Mismatch in output term # " + termNum + " - ", 
+                   expected[termNum],
+                   new String(termAtt.termBuffer(), 0, termAtt.termLength()));
+      ++termNum;
+    }
+    assertEquals("Too few output terms", expected.length, termNum);
+    stream.close();
+  }
+  
+  private static String[] getShingleConfig(String params) { 
+    String algLines[] = {
+        "content.source=org.apache.lucene.benchmark.byTask.feeds.LineDocSource",
+        "docs.file=" + getReuters20LinesFile(),
+        "content.source.forever=false",
+        "directory=RAMDirectory",
+        "NewShingleAnalyzer(" + params + ")",
+        "CreateIndex",
+        "{ \"AddDocs\"  AddDoc > : * "
+    };
+    return algLines;
+  }
+  
   private static String getReuters20LinesFile() {
     return System.getProperty("lucene.common.dir").replace('\\','/') +
       "/contrib/benchmark/src/test/org/apache/lucene/benchmark/reuters.first20.lines.txt";
Index: contrib/benchmark/src/java/org/apache/lucene/benchmark/quality/utils/SimpleQQParser.java
===================================================================
--- contrib/benchmark/src/java/org/apache/lucene/benchmark/quality/utils/SimpleQQParser.java	(revision 916567)
+++ contrib/benchmark/src/java/org/apache/lucene/benchmark/quality/utils/SimpleQQParser.java	(working copy)
@@ -21,27 +21,38 @@
 import org.apache.lucene.benchmark.quality.QualityQueryParser;
 import org.apache.lucene.queryParser.ParseException;
 import org.apache.lucene.queryParser.QueryParser;
+import org.apache.lucene.search.BooleanClause;
+import org.apache.lucene.search.BooleanQuery;
 import org.apache.lucene.search.Query;
 import org.apache.lucene.util.Version;
 
 /**
  * Simplistic quality query parser. A Lucene query is created by passing 
- * the value of the specified QualityQuery name-value pair into 
+ * the value of the specified QualityQuery name-value pair(s) into 
  * a Lucene's QueryParser using StandardAnalyzer. */
 public class SimpleQQParser implements QualityQueryParser {
 
-  private String qqName;
+  private String qqNames[];
   private String indexField;
   ThreadLocal<QueryParser> queryParser = new ThreadLocal<QueryParser>();
 
   /**
    * Constructor of a simple qq parser.
+   * @param qqNames name-value pairs of quality query to use for creating the query
+   * @param indexField corresponding index field  
+   */
+  public SimpleQQParser(String qqNames[], String indexField) {
+    this.qqNames = qqNames;
+    this.indexField = indexField;
+  }
+
+  /**
+   * Constructor of a simple qq parser.
    * @param qqName name-value pair of quality query to use for creating the query
    * @param indexField corresponding index field  
    */
   public SimpleQQParser(String qqName, String indexField) {
-    this.qqName = qqName;
-    this.indexField = indexField;
+    this(new String[] { qqName }, indexField);
   }
 
   /* (non-Javadoc)
@@ -53,7 +64,11 @@
       qp = new QueryParser(Version.LUCENE_CURRENT, indexField, new StandardAnalyzer(Version.LUCENE_CURRENT));
       queryParser.set(qp);
     }
-    return qp.parse(qq.getValue(qqName));
+    BooleanQuery bq = new BooleanQuery();
+    for (int i = 0; i < qqNames.length; i++)
+      bq.add(qp.parse(QueryParser.escape(qq.getValue(qqNames[i]))), BooleanClause.Occur.SHOULD);
+    
+    return bq;
   }
 
 }
Index: contrib/benchmark/src/java/org/apache/lucene/benchmark/quality/trec/QueryDriver.java
===================================================================
--- contrib/benchmark/src/java/org/apache/lucene/benchmark/quality/trec/QueryDriver.java	(revision 916567)
+++ contrib/benchmark/src/java/org/apache/lucene/benchmark/quality/trec/QueryDriver.java	(working copy)
@@ -30,6 +30,8 @@
 import java.io.File;
 import java.io.FileReader;
 import java.io.PrintWriter;
+import java.util.HashSet;
+import java.util.Set;
 
 
 /**
@@ -38,12 +40,14 @@
  **/
 public class QueryDriver {
   public static void main(String[] args) throws Exception {
-    if (args.length != 4) {
-      System.err.println("Usage: QueryDriver <topicsFile> <qrelsFile> <submissionFile> <indexDir>");
+    if (args.length < 4 || args.length > 5) {
+      System.err.println("Usage: QueryDriver <topicsFile> <qrelsFile> <submissionFile> <indexDir> [querySpec]");
       System.err.println("topicsFile: input file containing queries");
       System.err.println("qrelsFile: input file containing relevance judgements");
       System.err.println("submissionFile: output submission file for trec_eval");
       System.err.println("indexDir: index directory");
+      System.err.println("querySpec: string composed of fields to use in query consisting of T=title,D=description,N=narrative:");
+      System.err.println("\texample: TD (query on Title + Description). The default is T (title only)");
       System.exit(1);
     }
     
@@ -51,6 +55,7 @@
     File qrelsFile = new File(args[1]);
     SubmissionReport submitLog = new SubmissionReport(new PrintWriter(args[2]), "lucene");
     FSDirectory dir = FSDirectory.open(new File(args[3]));
+    String fieldSpec = args.length == 5 ? args[4] : "T"; // default to Title-only if not specified.
     Searcher searcher = new IndexSearcher(dir, true);
 
     int maxResults = 1000;
@@ -68,8 +73,13 @@
     // validate topics & judgments match each other
     judge.validateData(qqs, logger);
 
+    Set<String> fieldSet = new HashSet<String>();
+    if (fieldSpec.indexOf('T') >= 0) fieldSet.add("title");
+    if (fieldSpec.indexOf('D') >= 0) fieldSet.add("description");
+    if (fieldSpec.indexOf('N') >= 0) fieldSet.add("narrative");
+    
     // set the parsing of quality queries into Lucene queries.
-    QualityQueryParser qqParser = new SimpleQQParser("title", "body");
+    QualityQueryParser qqParser = new SimpleQQParser(fieldSet.toArray(new String[0]), "body");
 
     // run the benchmark
     QualityBenchmark qrun = new QualityBenchmark(qqs, qqParser, searcher, docNameField);
Index: contrib/benchmark/src/java/org/apache/lucene/benchmark/quality/trec/TrecTopicsReader.java
===================================================================
--- contrib/benchmark/src/java/org/apache/lucene/benchmark/quality/trec/TrecTopicsReader.java	(revision 916567)
+++ contrib/benchmark/src/java/org/apache/lucene/benchmark/quality/trec/TrecTopicsReader.java	(working copy)
@@ -76,16 +76,31 @@
         k = sb.indexOf(">");
         String title = sb.substring(k+1).trim();
         // description
-        sb = read(reader,"<desc>",null,false,false);
-        sb = read(reader,"<narr>",null,false,true);
-        String descripion = sb.toString().trim();
+        read(reader,"<desc>",null,false,false);
+        sb.setLength(0);
+        String line = null;
+        while ((line = reader.readLine()) != null) {
+          if (line.startsWith("<narr>"))
+            break;
+          if (sb.length() > 0) sb.append(' ');
+          sb.append(line);
+        }
+        String description = sb.toString().trim();
+        // narrative
+        sb.setLength(0);
+        while ((line = reader.readLine()) != null) {
+          if (line.startsWith("</top>"))
+            break;
+          if (sb.length() > 0) sb.append(' ');
+          sb.append(line);
+        }
+        String narrative = sb.toString().trim();
         // we got a topic!
         fields.put("title",title);
-        fields.put("description",descripion);
+        fields.put("description",description);
+        fields.put("narrative", narrative);
         QualityQuery topic = new QualityQuery(id,fields);
         res.add(topic);
-        // skip narrative, get to end of doc
-        read(reader,"</top>",null,false,false);
       }
     } finally {
       reader.close();
Index: contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/NearRealtimeReaderTask.java
===================================================================
--- contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/NearRealtimeReaderTask.java	(revision 916567)
+++ contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/NearRealtimeReaderTask.java	(working copy)
@@ -29,8 +29,7 @@
  * single query (body: 1) sorted by docdate, and prints
  * time to reopen and time to run the search.
  *
- * <b>NOTE</b>: this is very experimental at this point, and
- * subject to change.  It's also not generally usable, eg
+ * @lucene.experimental It's also not generally usable, eg
  * you cannot change which query is executed.
  */
 public class NearRealtimeReaderTask extends PerfTask {
@@ -92,6 +91,7 @@
         r = newReader;
       }
     }
+    stopNow = false;
 
     return reopenCount;
   }
Index: contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/PerfTask.java
===================================================================
--- contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/PerfTask.java	(revision 916567)
+++ contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/PerfTask.java	(working copy)
@@ -134,7 +134,6 @@
    * @return number of work items done by this task.
    */
   public final int runAndMaybeStats(boolean reportStats) throws Exception {
-    stopNow = false;
     if (!reportStats || shouldNotRecordStats()) {
       setup();
       int count = doLogic();
Index: contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/TaskSequence.java
===================================================================
--- contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/TaskSequence.java	(revision 916567)
+++ contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/TaskSequence.java	(working copy)
@@ -230,6 +230,8 @@
       getRunData().getPoints().getCurrentStats().setCountsByTime(countsByTime, logByTimeMsec);
     }
 
+    stopNow = false;
+
     return count;
   }
 
@@ -276,6 +278,7 @@
         }
       }
     }
+    stopNow = false;
     return count;
   }
 
Index: contrib/benchmark/build.xml
===================================================================
--- contrib/benchmark/build.xml	(revision 916567)
+++ contrib/benchmark/build.xml	(working copy)
@@ -128,10 +128,10 @@
     <path id="classpath">
         <pathelement path="${common.dir}/build/classes/java"/>
         <pathelement path="${common.dir}/build/classes/demo"/>
-      <pathelement path="${common.dir}/build/classes/test"/>
         <pathelement path="${common.dir}/build/contrib/highlighter/classes/java"/>
         <pathelement path="${common.dir}/build/contrib/memory/classes/java"/>
         <pathelement path="${common.dir}/build/contrib/fast-vector-highlighter/classes/java"/>
+        <pathelement path="${common.dir}/build/contrib/analyzers/common/classes/java"/>
     	<fileset dir="lib">
     		<include name="**/*.jar"/>
     	</fileset>
@@ -145,7 +145,7 @@
     <property name="task.alg" location="conf/micro-standard.alg"/>
     <property name="task.mem" value="140M"/>
 
-    <target name="run-task" depends="compile-test,check-files,get-files" 
+    <target name="run-task" depends="compile,check-files,get-files" 
      description="Run compound penalty perf test (optional: -Dtask.alg=your-algorithm-file -Dtask.mem=java-max-mem)">
         <echo>Working Directory: ${working.dir}</echo>
         <java classname="org.apache.lucene.benchmark.byTask.Benchmark" maxmemory="${task.mem}" fork="true">
@@ -193,6 +193,32 @@
 	    <echo>Benchmark output in JIRA table format is in file: ${collation.jira.output.file}</echo>
 	</target>
 	
+    <property name="shingle.alg.file" location="conf/shingle.alg"/>
+    <property name="shingle.output.file" 
+              value="${working.dir}/shingle.benchmark.output.txt"/>
+    <property name="shingle.jira.output.file" 
+              value="${working.dir}/shingle.bm2jira.output.txt"/>
+	
+    <path id="shingle.runtime.classpath">
+      <path refid="run.classpath"/>
+    </path>
+	
+    <target name="shingle" depends="compile,compile-analyzers-common,get-files">
+      <echo>Running contrib/benchmark with alg file: ${shingle.alg.file}</echo>
+      <java fork="true" classname="org.apache.lucene.benchmark.byTask.Benchmark" 
+            maxmemory="${task.mem}" output="${shingle.output.file}">
+        <classpath refid="run.classpath"/>
+        <arg file="${shingle.alg.file}"/>
+      </java>
+      <echo>Benchmark output is in file: ${shingle.output.file}</echo>
+      <echo>Converting to JIRA table format...</echo>
+      <exec executable="perl" output="${shingle.jira.output.file}" failonerror="true">
+        <arg value="scripts/shingle.bm2jira.pl"/>
+        <arg value="${shingle.output.file}"/>
+      </exec>
+      <echo>Benchmark output in JIRA table format is in file: ${shingle.jira.output.file}</echo>
+    </target>
+
     <target name="compile-demo">
       <subant target="compile-demo">
          <fileset dir="${common.dir}" includes="build.xml"/>
@@ -208,6 +234,11 @@
          <fileset dir="${common.dir}/contrib/icu" includes="build.xml"/>
       </subant>
     </target>
+    <target name="compile-analyzers-common">
+      <subant target="compile">
+        <fileset dir="${common.dir}/contrib/analyzers/common" includes="build.xml"/>
+      </subant>
+    </target>
     <target name="compile-memory">
       <subant target="compile">
          <fileset dir="${common.dir}/contrib/memory" includes="build.xml"/>
@@ -219,11 +250,6 @@
       </subant>
     </target>
 
-    <target name="init" depends="common.init,compile-demo,compile-memory,compile-highlighter,compile-vector-highlighter,check-files"/>
-
-    <!-- make sure online collections (reuters) are first downloaded -->
-    <target name="test" depends="init,get-files">
-      <antcall target="common.test" inheritRefs="true" />
-    </target>
+    <target name="init" depends="common.init,compile-demo,compile-memory,compile-highlighter,compile-vector-highlighter"/>
     
 </project>
Index: contrib/remote/src/test/org/apache/lucene/search/TestRemoteCachingWrapperFilter.java
===================================================================
--- contrib/remote/src/test/org/apache/lucene/search/TestRemoteCachingWrapperFilter.java	(revision 916567)
+++ contrib/remote/src/test/org/apache/lucene/search/TestRemoteCachingWrapperFilter.java	(working copy)
@@ -21,6 +21,7 @@
 import java.rmi.registry.LocateRegistry;
 
 import org.apache.lucene.util.LuceneTestCase;
+import org.apache.lucene.util.Version;
 import org.apache.lucene.util._TestUtil;
 
 import org.apache.lucene.analysis.SimpleAnalyzer;
@@ -33,8 +34,6 @@
 /**
  * Tests that the index is cached on the searcher side of things.
  * NOTE: This is copied from TestRemoteSearchable since it already had a remote index set up.
- * 
- * @version $Id:$
  */
 public class TestRemoteCachingWrapperFilter extends LuceneTestCase {
   public TestRemoteCachingWrapperFilter(String name) {
@@ -59,7 +58,7 @@
   private static void startServer() throws Exception {
     // construct an index
     RAMDirectory indexStore = new RAMDirectory();
-    IndexWriter writer = new IndexWriter(indexStore, new SimpleAnalyzer(), true,
+    IndexWriter writer = new IndexWriter(indexStore, new SimpleAnalyzer(Version.LUCENE_CURRENT), true,
                                          IndexWriter.MaxFieldLength.LIMITED);
     Document doc = new Document();
     doc.add(new Field("test", "test text", Field.Store.YES, Field.Index.ANALYZED));
Index: contrib/remote/src/test/org/apache/lucene/search/TestRemoteSort.java
===================================================================
--- contrib/remote/src/test/org/apache/lucene/search/TestRemoteSort.java	(revision 916567)
+++ contrib/remote/src/test/org/apache/lucene/search/TestRemoteSort.java	(working copy)
@@ -39,14 +39,13 @@
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.store.RAMDirectory;
 import org.apache.lucene.util.LuceneTestCase;
+import org.apache.lucene.util.Version;
 import org.apache.lucene.util._TestUtil;
 
 /**
  * Unit tests for remote sorting code.
  * Note: This is a modified copy of {@link TestSort} without duplicated test
  * methods and therefore unused members and methodes. 
- *
- * @version $Id:$
  */
 
 public class TestRemoteSort extends LuceneTestCase implements Serializable {
@@ -112,7 +111,7 @@
   private Searcher getIndex (boolean even, boolean odd)
   throws IOException {
     RAMDirectory indexStore = new RAMDirectory ();
-    IndexWriter writer = new IndexWriter (indexStore, new SimpleAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
+    IndexWriter writer = new IndexWriter (indexStore, new SimpleAnalyzer(Version.LUCENE_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);
     writer.setMaxBufferedDocs(2);
     writer.setMergeFactor(1000);
     for (int i=0; i<data.length; ++i) {
Index: contrib/remote/src/test/org/apache/lucene/search/TestRemoteSearchable.java
===================================================================
--- contrib/remote/src/test/org/apache/lucene/search/TestRemoteSearchable.java	(revision 916567)
+++ contrib/remote/src/test/org/apache/lucene/search/TestRemoteSearchable.java	(working copy)
@@ -18,6 +18,7 @@
  */
 
 import org.apache.lucene.util.LuceneTestCase;
+import org.apache.lucene.util.Version;
 import org.apache.lucene.util._TestUtil;
 import org.apache.lucene.analysis.SimpleAnalyzer;
 import org.apache.lucene.document.*;
@@ -58,7 +59,7 @@
   private static void startServer() throws Exception {
     // construct an index
     RAMDirectory indexStore = new RAMDirectory();
-    IndexWriter writer = new IndexWriter(indexStore,new SimpleAnalyzer(),true, IndexWriter.MaxFieldLength.LIMITED);
+    IndexWriter writer = new IndexWriter(indexStore,new SimpleAnalyzer(Version.LUCENE_CURRENT),true, IndexWriter.MaxFieldLength.LIMITED);
     Document doc = new Document();
     doc.add(new Field("test", "test text", Field.Store.YES, Field.Index.ANALYZED));
     doc.add(new Field("other", "other test text", Field.Store.YES, Field.Index.ANALYZED));
Index: contrib/remote/src/java/org/apache/lucene/search/RemoteCachingWrapperFilter.java
===================================================================
--- contrib/remote/src/java/org/apache/lucene/search/RemoteCachingWrapperFilter.java	(revision 916567)
+++ contrib/remote/src/java/org/apache/lucene/search/RemoteCachingWrapperFilter.java	(working copy)
@@ -35,8 +35,6 @@
  * To cache a result you must do something like 
  * RemoteCachingWrapperFilter f = new RemoteCachingWrapperFilter(new CachingWrapperFilter(myFilter));
  * <p/>
- * 
- * @version $Id:$
  */
 public class RemoteCachingWrapperFilter extends Filter {
   protected Filter filter;
Index: contrib/remote/src/java/org/apache/lucene/search/RMIRemoteSearchable.java
===================================================================
--- contrib/remote/src/java/org/apache/lucene/search/RMIRemoteSearchable.java	(revision 916567)
+++ contrib/remote/src/java/org/apache/lucene/search/RMIRemoteSearchable.java	(working copy)
@@ -38,8 +38,6 @@
  * 
  * </p>
  * </p>
- * 
- *  @version $Id:$
  */
 public interface RMIRemoteSearchable extends Searchable, Remote {
 
Index: contrib/misc/src/test/org/apache/lucene/queryParser/ext/TestExtendableQueryParser.java
===================================================================
--- contrib/misc/src/test/org/apache/lucene/queryParser/ext/TestExtendableQueryParser.java	(revision 916567)
+++ contrib/misc/src/test/org/apache/lucene/queryParser/ext/TestExtendableQueryParser.java	(working copy)
@@ -47,7 +47,7 @@
   public QueryParser getParser(Analyzer a, Extensions extensions)
       throws Exception {
     if (a == null)
-      a = new SimpleAnalyzer();
+      a = new SimpleAnalyzer(Version.LUCENE_CURRENT);
     QueryParser qp = extensions == null ? new ExtendableQueryParser(
         Version.LUCENE_CURRENT, "field", a) : new ExtendableQueryParser(
         Version.LUCENE_CURRENT, "field", a, extensions);
Index: contrib/misc/src/test/org/apache/lucene/queryParser/precedence/TestPrecedenceQueryParser.java
===================================================================
--- contrib/misc/src/test/org/apache/lucene/queryParser/precedence/TestPrecedenceQueryParser.java	(revision 916567)
+++ contrib/misc/src/test/org/apache/lucene/queryParser/precedence/TestPrecedenceQueryParser.java	(working copy)
@@ -36,6 +36,7 @@
 import org.apache.lucene.search.TermQuery;
 import org.apache.lucene.search.WildcardQuery;
 import org.apache.lucene.util.LocalizedTestCase;
+import org.apache.lucene.util.Version;
 
 import java.io.IOException;
 import java.io.Reader;
@@ -73,6 +74,7 @@
     
     @Override
     public boolean incrementToken() throws IOException {
+      clearAttributes();
       if (inPhrase) {
         inPhrase = false;
         termAtt.setTermBuffer("phrase2");
@@ -98,7 +100,7 @@
     /** Filters LowerCaseTokenizer with StopFilter. */
     @Override
     public final TokenStream tokenStream(String fieldName, Reader reader) {
-      return new QPTestFilter(new LowerCaseTokenizer(reader));
+      return new QPTestFilter(new LowerCaseTokenizer(Version.LUCENE_CURRENT, reader));
     }
   }
 
@@ -128,7 +130,7 @@
 
   public PrecedenceQueryParser getParser(Analyzer a) throws Exception {
     if (a == null)
-      a = new SimpleAnalyzer();
+      a = new SimpleAnalyzer(Version.LUCENE_CURRENT);
     PrecedenceQueryParser qp = new PrecedenceQueryParser("field", a);
     qp.setDefaultOperator(PrecedenceQueryParser.OR_OPERATOR);
     return qp;
@@ -173,7 +175,7 @@
   public Query getQueryDOA(String query, Analyzer a)
     throws Exception {
     if (a == null)
-      a = new SimpleAnalyzer();
+      a = new SimpleAnalyzer(Version.LUCENE_CURRENT);
     PrecedenceQueryParser qp = new PrecedenceQueryParser("field", a);
     qp.setDefaultOperator(PrecedenceQueryParser.AND_OPERATOR);
     return qp.parse(query);
@@ -253,7 +255,7 @@
   }
 
   public void testPunct() throws Exception {
-    Analyzer a = new WhitespaceAnalyzer();
+    Analyzer a = new WhitespaceAnalyzer(Version.LUCENE_CURRENT);
     assertQueryEquals("a&b", a, "a&b");
     assertQueryEquals("a&&b", a, "a&&b");
     assertQueryEquals(".NET", a, ".NET");
@@ -411,7 +413,7 @@
   }
 
   public void testEscaped() throws Exception {
-    Analyzer a = new WhitespaceAnalyzer();
+    Analyzer a = new WhitespaceAnalyzer(Version.LUCENE_CURRENT);
     
     /*assertQueryEquals("\\[brackets", a, "\\[brackets");
     assertQueryEquals("\\[brackets", null, "brackets");
@@ -543,7 +545,7 @@
 
   public void testCustomQueryParserWildcard() {
     try {
-      new QPTestParser("contents", new WhitespaceAnalyzer()).parse("a?t");
+      new QPTestParser("contents", new WhitespaceAnalyzer(Version.LUCENE_CURRENT)).parse("a?t");
     } catch (ParseException expected) {
       return;
     }
@@ -552,7 +554,7 @@
 
   public void testCustomQueryParserFuzzy() throws Exception {
     try {
-      new QPTestParser("contents", new WhitespaceAnalyzer()).parse("xunit~");
+      new QPTestParser("contents", new WhitespaceAnalyzer(Version.LUCENE_CURRENT)).parse("xunit~");
     } catch (ParseException expected) {
       return;
     }
@@ -562,7 +564,7 @@
   public void testBooleanQuery() throws Exception {
     BooleanQuery.setMaxClauseCount(2);
     try {
-      getParser(new WhitespaceAnalyzer()).parse("one two three");
+      getParser(new WhitespaceAnalyzer(Version.LUCENE_CURRENT)).parse("one two three");
       fail("ParseException expected due to too many boolean clauses");
     } catch (ParseException expected) {
       // too many boolean clauses, so ParseException is expected
@@ -576,7 +578,7 @@
   // failing tests disabled since PrecedenceQueryParser
   // is currently unmaintained
   public void _testPrecedence() throws Exception {
-    PrecedenceQueryParser parser = getParser(new WhitespaceAnalyzer());
+    PrecedenceQueryParser parser = getParser(new WhitespaceAnalyzer(Version.LUCENE_CURRENT));
     Query query1 = parser.parse("A AND B OR C AND D");
     Query query2 = parser.parse("(A AND B) OR (C AND D)");
     assertEquals(query1, query2);
Index: contrib/misc/src/test/org/apache/lucene/index/TestMultiPassIndexSplitter.java
===================================================================
--- contrib/misc/src/test/org/apache/lucene/index/TestMultiPassIndexSplitter.java	(revision 916567)
+++ contrib/misc/src/test/org/apache/lucene/index/TestMultiPassIndexSplitter.java	(working copy)
@@ -22,6 +22,7 @@
 import org.apache.lucene.index.IndexWriter.MaxFieldLength;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.RAMDirectory;
+import org.apache.lucene.util.Version;
 
 import junit.framework.TestCase;
 
@@ -32,7 +33,7 @@
   @Override
   public void setUp() throws Exception {
     RAMDirectory dir = new RAMDirectory();
-    IndexWriter w = new IndexWriter(dir, new WhitespaceAnalyzer(), true,
+    IndexWriter w = new IndexWriter(dir, new WhitespaceAnalyzer(Version.LUCENE_CURRENT), true,
             MaxFieldLength.LIMITED);
     Document doc;
     for (int i = 0; i < NUM_DOCS; i++) {
Index: contrib/misc/src/test/org/apache/lucene/index/TestFieldNormModifier.java
===================================================================
--- contrib/misc/src/test/org/apache/lucene/index/TestFieldNormModifier.java	(revision 916567)
+++ contrib/misc/src/test/org/apache/lucene/index/TestFieldNormModifier.java	(working copy)
@@ -34,11 +34,10 @@
 import org.apache.lucene.search.TermQuery;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.RAMDirectory;
+import org.apache.lucene.util.Version;
 
 /**
  * Tests changing of field norms with a custom similarity and with fake norms.
- *
- * @version $Id$
  */
 public class TestFieldNormModifier extends TestCase {
   public TestFieldNormModifier(String name) {
@@ -59,7 +58,7 @@
   
   @Override
   public void setUp() throws Exception {
-    IndexWriter writer = new IndexWriter(store, new SimpleAnalyzer(), true, MaxFieldLength.UNLIMITED);
+    IndexWriter writer = new IndexWriter(store, new SimpleAnalyzer(Version.LUCENE_CURRENT), true, MaxFieldLength.UNLIMITED);
     
     for (int i = 0; i < NUM_DOCS; i++) {
       Document d = new Document();
Index: contrib/misc/src/test/org/apache/lucene/index/TestIndexSplitter.java
===================================================================
--- contrib/misc/src/test/org/apache/lucene/index/TestIndexSplitter.java	(revision 916567)
+++ contrib/misc/src/test/org/apache/lucene/index/TestIndexSplitter.java	(working copy)
@@ -23,6 +23,7 @@
 import org.apache.lucene.index.IndexWriter.MaxFieldLength;
 import org.apache.lucene.store.FSDirectory;
 import org.apache.lucene.util.LuceneTestCase;
+import org.apache.lucene.util.Version;
 import org.apache.lucene.util._TestUtil;
 
 public class TestIndexSplitter extends LuceneTestCase {
@@ -35,7 +36,7 @@
     _TestUtil.rmDir(destDir);
     destDir.mkdirs();
     FSDirectory fsDir = FSDirectory.open(dir);
-    IndexWriter iw = new IndexWriter(fsDir, new WhitespaceAnalyzer(), true, MaxFieldLength.UNLIMITED);
+    IndexWriter iw = new IndexWriter(fsDir, new WhitespaceAnalyzer(Version.LUCENE_CURRENT), true, MaxFieldLength.UNLIMITED);
     for (int x=0; x < 100; x++) {
       Document doc = TestIndexWriterReader.createDocument(x, "index", 5);
       iw.addDocument(doc);
Index: contrib/misc/src/test/org/apache/lucene/misc/ChainedFilterTest.java
===================================================================
--- contrib/misc/src/test/org/apache/lucene/misc/ChainedFilterTest.java	(revision 916567)
+++ contrib/misc/src/test/org/apache/lucene/misc/ChainedFilterTest.java	(working copy)
@@ -43,6 +43,7 @@
 import org.apache.lucene.search.TopDocs;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.RAMDirectory;
+import org.apache.lucene.util.Version;
 
 public class ChainedFilterTest extends TestCase {
   public static final int MAX = 500;
@@ -59,7 +60,7 @@
   public void setUp() throws Exception {
     directory = new RAMDirectory();
     IndexWriter writer =
-       new IndexWriter(directory, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.UNLIMITED);
+       new IndexWriter(directory, new WhitespaceAnalyzer(Version.LUCENE_CURRENT), true, IndexWriter.MaxFieldLength.UNLIMITED);
 
     Calendar cal = new GregorianCalendar();
     cal.clear();
@@ -187,7 +188,7 @@
   
   public void testWithCachingFilter() throws Exception {
     Directory dir = new RAMDirectory();
-    Analyzer analyzer = new WhitespaceAnalyzer();
+    Analyzer analyzer = new WhitespaceAnalyzer(Version.LUCENE_CURRENT);
   
     IndexWriter writer = new IndexWriter(dir, analyzer, true, MaxFieldLength.LIMITED);
     writer.close();
Index: contrib/misc/src/test/org/apache/lucene/misc/TestLengthNormModifier.java
===================================================================
--- contrib/misc/src/test/org/apache/lucene/misc/TestLengthNormModifier.java	(revision 916567)
+++ contrib/misc/src/test/org/apache/lucene/misc/TestLengthNormModifier.java	(working copy)
@@ -37,11 +37,10 @@
 import org.apache.lucene.search.TermQuery;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.RAMDirectory;
+import org.apache.lucene.util.Version;
 
 /**
  * Tests changing the norms after changing the simularity
- *
- * @version $Id:$
  */
 public class TestLengthNormModifier extends TestCase {
     public TestLengthNormModifier(String name) {
@@ -62,7 +61,7 @@
     
     @Override
     public void setUp() throws Exception {
-	IndexWriter writer = new IndexWriter(store, new SimpleAnalyzer(), true, MaxFieldLength.UNLIMITED);
+	IndexWriter writer = new IndexWriter(store, new SimpleAnalyzer(Version.LUCENE_CURRENT), true, MaxFieldLength.UNLIMITED);
 	
 	for (int i = 0; i < NUM_DOCS; i++) {
 	    Document d = new Document();
Index: contrib/misc/src/java/org/apache/lucene/index/IndexSplitter.java
===================================================================
--- contrib/misc/src/java/org/apache/lucene/index/IndexSplitter.java	(revision 916567)
+++ contrib/misc/src/java/org/apache/lucene/index/IndexSplitter.java	(working copy)
@@ -41,8 +41,7 @@
  * File{In,Out}putStream) so it will not work with non
  * FSDirectory Directory impls.</p>
  *
- * <p><b>NOTE</b>: The tool is experimental and might change
- * in incompatible ways in the next release.  You can easily
+ * @lucene.experimental You can easily
  * accidentally remove segments from your index so be
  * careful!
  */
Index: contrib/misc/src/java/org/apache/lucene/misc/IndexMergeTool.java
===================================================================
--- contrib/misc/src/java/org/apache/lucene/misc/IndexMergeTool.java	(revision 916567)
+++ contrib/misc/src/java/org/apache/lucene/misc/IndexMergeTool.java	(working copy)
@@ -27,7 +27,6 @@
 /**
  * Merges indices specified on the command line into the index
  * specified as the first command line argument.
- * @version $Id$
  */
 public class IndexMergeTool {
   public static void main(String[] args) throws IOException {
Index: contrib/spatial/src/test/org/apache/lucene/spatial/tier/TestCartesian.java
===================================================================
--- contrib/spatial/src/test/org/apache/lucene/spatial/tier/TestCartesian.java	(revision 916567)
+++ contrib/spatial/src/test/org/apache/lucene/spatial/tier/TestCartesian.java	(working copy)
@@ -27,6 +27,7 @@
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.search.IndexSearcher;
 import org.apache.lucene.search.Query;
@@ -36,6 +37,7 @@
 import org.apache.lucene.search.TermQuery;
 import org.apache.lucene.search.TopDocs;
 import org.apache.lucene.search.function.CustomScoreQuery;
+import org.apache.lucene.search.function.CustomScoreProvider;
 import org.apache.lucene.search.function.FieldScoreQuery;
 import org.apache.lucene.search.function.FieldScoreQuery.Type;
 import org.apache.lucene.spatial.geohash.GeoHashUtils;
@@ -48,6 +50,7 @@
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.RAMDirectory;
 import org.apache.lucene.util.NumericUtils;
+import org.apache.lucene.util.Version;
 
 /**
  *
@@ -76,7 +79,7 @@
   protected void setUp() throws IOException {
     directory = new RAMDirectory();
 
-    IndexWriter writer = new IndexWriter(directory, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.UNLIMITED);
+    IndexWriter writer = new IndexWriter(directory, new WhitespaceAnalyzer(Version.LUCENE_CURRENT), true, IndexWriter.MaxFieldLength.UNLIMITED);
     
     setUpPlotter( 2, 15);
     
@@ -188,20 +191,26 @@
     CustomScoreQuery customScore = new CustomScoreQuery(dq.getQuery(tq),fsQuery){
 
       @Override
-      public float customScore(int doc, float subQueryScore, float valSrcScore){
-        System.out.println(doc);
-        if (dq.distanceFilter.getDistance(doc) == null)
-          return 0;
+      protected CustomScoreProvider getCustomScoreProvider(IndexReader reader) {
+        return new CustomScoreProvider(reader) {
+          @Override // TODO: broken, as reader is not used!
+          public float customScore(int doc, float subQueryScore, float valSrcScore){
+            System.out.println(doc);
+            if (dq.distanceFilter.getDistance(doc) == null)
+              return 0;
 
-        double distance = dq.distanceFilter.getDistance(doc);
-        // boost score shouldn't exceed 1
-        if (distance < 1.0d)
-          distance = 1.0d;
-        //boost by distance is invertly proportional to
-        // to distance from center point to location
-        float score = (float) ((miles - distance) / miles );
-        return score * subQueryScore;
+            double distance = dq.distanceFilter.getDistance(doc);
+            // boost score shouldn't exceed 1
+            if (distance < 1.0d)
+              distance = 1.0d;
+            //boost by distance is invertly proportional to
+            // to distance from center point to location
+            float score = (float) ((miles - distance) / miles );
+            return score * subQueryScore;
+          }
+        };
       }
+      
     };
     // Create a distance sort
     // As the radius filter has performed the distance calculations
@@ -275,20 +284,26 @@
     CustomScoreQuery customScore = new CustomScoreQuery(dq.getQuery(tq),fsQuery){
 
       @Override
-      public float customScore(int doc, float subQueryScore, float valSrcScore){
-        System.out.println(doc);
-        if (dq.distanceFilter.getDistance(doc) == null)
-          return 0;
+      protected CustomScoreProvider getCustomScoreProvider(IndexReader reader) {
+        return new CustomScoreProvider(reader) {
+          @Override // TODO: broken, as reader is not used!
+          public float customScore(int doc, float subQueryScore, float valSrcScore){
+            System.out.println(doc);
+            if (dq.distanceFilter.getDistance(doc) == null)
+              return 0;
 
-        double distance = dq.distanceFilter.getDistance(doc);
-        // boost score shouldn't exceed 1
-        if (distance < 1.0d)
-          distance = 1.0d;
-        //boost by distance is invertly proportional to
-        // to distance from center point to location
-        float score = (float) ((miles - distance) / miles );
-        return score * subQueryScore;
+            double distance = dq.distanceFilter.getDistance(doc);
+            // boost score shouldn't exceed 1
+            if (distance < 1.0d)
+              distance = 1.0d;
+            //boost by distance is invertly proportional to
+            // to distance from center point to location
+            float score = (float) ((miles - distance) / miles );
+            return score * subQueryScore;
+          }
+        };
       }
+      
     };
     // Create a distance sort
     // As the radius filter has performed the distance calculations
@@ -362,23 +377,27 @@
       FieldScoreQuery fsQuery = new FieldScoreQuery("geo_distance", Type.FLOAT);
     
       CustomScoreQuery customScore = new CustomScoreQuery(dq.getQuery(tq),fsQuery){
-      
-          @Override
+        @Override
+        protected CustomScoreProvider getCustomScoreProvider(IndexReader reader) {
+          return new CustomScoreProvider(reader) {
+            @Override // TODO: broken, as reader is not used!
             public float customScore(int doc, float subQueryScore, float valSrcScore){
-            //System.out.println(doc);
-            if (dq.distanceFilter.getDistance(doc) == null)
-              return 0;
-        
-            double distance = dq.distanceFilter.getDistance(doc);
-            // boost score shouldn't exceed 1
-            if (distance < 1.0d)
-              distance = 1.0d;
-            //boost by distance is invertly proportional to
-            // to distance from center point to location
-            float score = (float) ( (miles - distance) / miles );
-            return score * subQueryScore;
-          }
-        };
+              //System.out.println(doc);
+              if (dq.distanceFilter.getDistance(doc) == null)
+                return 0;
+          
+              double distance = dq.distanceFilter.getDistance(doc);
+              // boost score shouldn't exceed 1
+              if (distance < 1.0d)
+                distance = 1.0d;
+              //boost by distance is invertly proportional to
+              // to distance from center point to location
+              float score = (float) ( (miles - distance) / miles );
+              return score * subQueryScore;
+            }
+          };
+        }
+      };
       // Create a distance sort
       // As the radius filter has performed the distance calculations
       // already, pass in the filter to reuse the results.
@@ -451,23 +470,27 @@
 	    
       FieldScoreQuery fsQuery = new FieldScoreQuery("geo_distance", Type.FLOAT);
       CustomScoreQuery customScore = new CustomScoreQuery(tq,fsQuery){
-	      
-          @Override
-            public float customScore(int doc, float subQueryScore, float valSrcScore){
-            //System.out.println(doc);
-            if (dq.distanceFilter.getDistance(doc) == null)
-              return 0;
-	        
-            double distance = dq.distanceFilter.getDistance(doc);
-            // boost score shouldn't exceed 1
-            if (distance < 1.0d)
-              distance = 1.0d;
-            //boost by distance is invertly proportional to
-            // to distance from center point to location
-            float score = (float) ( (miles - distance) / miles );
-            return score * subQueryScore;
-          }
-        };
+        @Override
+        protected CustomScoreProvider getCustomScoreProvider(IndexReader reader) {
+          return new CustomScoreProvider(reader) {
+              @Override // TODO: broken, as reader is not used!
+              public float customScore(int doc, float subQueryScore, float valSrcScore){
+              //System.out.println(doc);
+              if (dq.distanceFilter.getDistance(doc) == null)
+                return 0;
+            
+              double distance = dq.distanceFilter.getDistance(doc);
+              // boost score shouldn't exceed 1
+              if (distance < 1.0d)
+                distance = 1.0d;
+              //boost by distance is invertly proportional to
+              // to distance from center point to location
+              float score = (float) ( (miles - distance) / miles );
+              return score * subQueryScore;
+            }
+          };
+        }
+      };
       // Create a distance sort
       // As the radius filter has performed the distance calculations
       // already, pass in the filter to reuse the results.
Index: contrib/spatial/src/test/org/apache/lucene/spatial/tier/TestDistance.java
===================================================================
--- contrib/spatial/src/test/org/apache/lucene/spatial/tier/TestDistance.java	(revision 916567)
+++ contrib/spatial/src/test/org/apache/lucene/spatial/tier/TestDistance.java	(working copy)
@@ -29,6 +29,7 @@
 import org.apache.lucene.search.QueryWrapperFilter;
 import org.apache.lucene.search.MatchAllDocsQuery;
 import org.apache.lucene.util.NumericUtils;
+import org.apache.lucene.util.Version;
 import org.apache.lucene.store.RAMDirectory;
 
 
@@ -49,7 +50,7 @@
   @Override
   protected void setUp() throws IOException {
     directory = new RAMDirectory();
-    writer = new IndexWriter(directory, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.UNLIMITED);
+    writer = new IndexWriter(directory, new WhitespaceAnalyzer(Version.LUCENE_CURRENT), true, IndexWriter.MaxFieldLength.UNLIMITED);
     addData(writer);
     
   }

Property changes on: contrib\highlighter\src\test
___________________________________________________________________
Modified: svn:mergeinfo
   Merged /lucene/java/trunk/contrib/highlighter/src/test:r889614-889621,889623-889666,889668-889865,899002-916543
   Merged /lucene/java/branches/lucene_2_9/contrib/highlighter/src/test:r909334

Index: contrib/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterPhraseTest.java
===================================================================
--- contrib/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterPhraseTest.java	(revision 916567)
+++ contrib/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterPhraseTest.java	(working copy)
@@ -49,6 +49,7 @@
 import org.apache.lucene.store.LockObtainFailedException;
 import org.apache.lucene.store.RAMDirectory;
 import org.apache.lucene.util.OpenBitSet;
+import org.apache.lucene.util.Version;
 
 import junit.framework.TestCase;
 
@@ -60,7 +61,7 @@
     final String TEXT = "the fox jumped";
     final Directory directory = new RAMDirectory();
     final IndexWriter indexWriter = new IndexWriter(directory,
-        new WhitespaceAnalyzer(), MaxFieldLength.UNLIMITED);
+        new WhitespaceAnalyzer(Version.LUCENE_CURRENT), MaxFieldLength.UNLIMITED);
     try {
       final Document document = new Document();
       document.add(new Field(FIELD, new TokenStreamConcurrent(),
@@ -103,7 +104,7 @@
     final String TEXT = "the fox jumped";
     final Directory directory = new RAMDirectory();
     final IndexWriter indexWriter = new IndexWriter(directory,
-        new WhitespaceAnalyzer(), MaxFieldLength.UNLIMITED);
+        new WhitespaceAnalyzer(Version.LUCENE_CURRENT), MaxFieldLength.UNLIMITED);
     try {
       final Document document = new Document();
       document.add(new Field(FIELD, new TokenStreamConcurrent(),
@@ -168,7 +169,7 @@
     final String TEXT = "the fox did not jump";
     final Directory directory = new RAMDirectory();
     final IndexWriter indexWriter = new IndexWriter(directory,
-        new WhitespaceAnalyzer(), MaxFieldLength.UNLIMITED);
+        new WhitespaceAnalyzer(Version.LUCENE_CURRENT), MaxFieldLength.UNLIMITED);
     try {
       final Document document = new Document();
       document.add(new Field(FIELD, new TokenStreamSparse(),
@@ -210,7 +211,7 @@
     final String TEXT = "the fox did not jump";
     final Directory directory = new RAMDirectory();
     final IndexWriter indexWriter = new IndexWriter(directory,
-        new WhitespaceAnalyzer(), MaxFieldLength.UNLIMITED);
+        new WhitespaceAnalyzer(Version.LUCENE_CURRENT), MaxFieldLength.UNLIMITED);
     try {
       final Document document = new Document();
       document.add(new Field(FIELD, TEXT, Store.YES, Index.ANALYZED,
@@ -250,7 +251,7 @@
     final String TEXT = "the fox did not jump";
     final Directory directory = new RAMDirectory();
     final IndexWriter indexWriter = new IndexWriter(directory,
-        new WhitespaceAnalyzer(), MaxFieldLength.UNLIMITED);
+        new WhitespaceAnalyzer(Version.LUCENE_CURRENT), MaxFieldLength.UNLIMITED);
     try {
       final Document document = new Document();
       document.add(new Field(FIELD, new TokenStreamSparse(),
@@ -311,6 +312,7 @@
       if (this.i >= this.tokens.length) {
         return false;
       }
+      clearAttributes();
       termAttribute.setTermBuffer(this.tokens[i].term(), 0, this.tokens[i]
           .term().length());
       offsetAttribute.setOffset(this.tokens[i].startOffset(), this.tokens[i]
@@ -355,6 +357,7 @@
       if (this.i >= this.tokens.length) {
         return false;
       }
+      clearAttributes();
       termAttribute.setTermBuffer(this.tokens[i].term(), 0, this.tokens[i]
           .term().length());
       offsetAttribute.setOffset(this.tokens[i].startOffset(), this.tokens[i]
Index: contrib/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterTest.java
===================================================================
--- contrib/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterTest.java	(revision 916567)
+++ contrib/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterTest.java	(working copy)
@@ -118,7 +118,7 @@
   }
 
   public void testQueryScorerHits() throws Exception {
-    Analyzer analyzer = new SimpleAnalyzer();
+    Analyzer analyzer = new SimpleAnalyzer(Version.LUCENE_CURRENT);
     QueryParser qp = new QueryParser(TEST_VERSION, FIELD_NAME, analyzer);
     query = qp.parse("\"very long\"");
     searcher = new IndexSearcher(ramDir, true);
@@ -226,7 +226,7 @@
     String f2c = f2 + ":";
     String q = "(" + f1c + ph1 + " OR " + f2c + ph1 + ") AND (" + f1c + ph2
         + " OR " + f2c + ph2 + ")";
-    Analyzer analyzer = new WhitespaceAnalyzer();
+    Analyzer analyzer = new WhitespaceAnalyzer(Version.LUCENE_CURRENT);
     QueryParser qp = new QueryParser(TEST_VERSION, f1, analyzer);
     Query query = qp.parse(q);
 
@@ -1458,6 +1458,7 @@
       public boolean incrementToken() throws IOException {
         if(iter.hasNext()) {
           Token token =  iter.next();
+          clearAttributes();
           termAtt.setTermBuffer(token.term());
           posIncrAtt.setPositionIncrement(token.getPositionIncrement());
           offsetAtt.setOffset(token.startOffset(), token.endOffset());
@@ -1506,6 +1507,7 @@
       public boolean incrementToken() throws IOException {
         if(iter.hasNext()) {
           Token token = iter.next();
+          clearAttributes();
           termAtt.setTermBuffer(token.term());
           posIncrAtt.setPositionIncrement(token.getPositionIncrement());
           offsetAtt.setOffset(token.startOffset(), token.endOffset());
@@ -1527,64 +1529,64 @@
         Highlighter highlighter;
         String result;
 
-        query = new QueryParser(TEST_VERSION, "text", new WhitespaceAnalyzer()).parse("foo");
+        query = new QueryParser(TEST_VERSION, "text", new WhitespaceAnalyzer(Version.LUCENE_CURRENT)).parse("foo");
         highlighter = getHighlighter(query, "text", getTS2(), HighlighterTest.this);
         result = highlighter.getBestFragments(getTS2(), s, 3, "...");
         assertEquals("Hi-Speed10 <B>foo</B>", result);
 
-        query = new QueryParser(TEST_VERSION, "text", new WhitespaceAnalyzer()).parse("10");
+        query = new QueryParser(TEST_VERSION, "text", new WhitespaceAnalyzer(Version.LUCENE_CURRENT)).parse("10");
         highlighter = getHighlighter(query, "text", getTS2(), HighlighterTest.this);
         result = highlighter.getBestFragments(getTS2(), s, 3, "...");
         assertEquals("Hi-Speed<B>10</B> foo", result);
 
-        query = new QueryParser(TEST_VERSION, "text", new WhitespaceAnalyzer()).parse("hi");
+        query = new QueryParser(TEST_VERSION, "text", new WhitespaceAnalyzer(Version.LUCENE_CURRENT)).parse("hi");
         highlighter = getHighlighter(query, "text", getTS2(), HighlighterTest.this);
         result = highlighter.getBestFragments(getTS2(), s, 3, "...");
         assertEquals("<B>Hi</B>-Speed10 foo", result);
 
-        query = new QueryParser(TEST_VERSION, "text", new WhitespaceAnalyzer()).parse("speed");
+        query = new QueryParser(TEST_VERSION, "text", new WhitespaceAnalyzer(Version.LUCENE_CURRENT)).parse("speed");
         highlighter = getHighlighter(query, "text", getTS2(), HighlighterTest.this);
         result = highlighter.getBestFragments(getTS2(), s, 3, "...");
         assertEquals("Hi-<B>Speed</B>10 foo", result);
 
-        query = new QueryParser(TEST_VERSION, "text", new WhitespaceAnalyzer()).parse("hispeed");
+        query = new QueryParser(TEST_VERSION, "text", new WhitespaceAnalyzer(Version.LUCENE_CURRENT)).parse("hispeed");
         highlighter = getHighlighter(query, "text", getTS2(), HighlighterTest.this);
         result = highlighter.getBestFragments(getTS2(), s, 3, "...");
         assertEquals("<B>Hi-Speed</B>10 foo", result);
 
-        query = new QueryParser(TEST_VERSION, "text", new WhitespaceAnalyzer()).parse("hi speed");
+        query = new QueryParser(TEST_VERSION, "text", new WhitespaceAnalyzer(Version.LUCENE_CURRENT)).parse("hi speed");
         highlighter = getHighlighter(query, "text", getTS2(), HighlighterTest.this);
         result = highlighter.getBestFragments(getTS2(), s, 3, "...");
         assertEquals("<B>Hi-Speed</B>10 foo", result);
 
         // ///////////////// same tests, just put the bigger overlapping token
         // first
-        query = new QueryParser(TEST_VERSION, "text", new WhitespaceAnalyzer()).parse("foo");
+        query = new QueryParser(TEST_VERSION, "text", new WhitespaceAnalyzer(Version.LUCENE_CURRENT)).parse("foo");
         highlighter = getHighlighter(query, "text", getTS2a(), HighlighterTest.this);
         result = highlighter.getBestFragments(getTS2a(), s, 3, "...");
         assertEquals("Hi-Speed10 <B>foo</B>", result);
 
-        query = new QueryParser(TEST_VERSION, "text", new WhitespaceAnalyzer()).parse("10");
+        query = new QueryParser(TEST_VERSION, "text", new WhitespaceAnalyzer(Version.LUCENE_CURRENT)).parse("10");
         highlighter = getHighlighter(query, "text", getTS2a(), HighlighterTest.this);
         result = highlighter.getBestFragments(getTS2a(), s, 3, "...");
         assertEquals("Hi-Speed<B>10</B> foo", result);
 
-        query = new QueryParser(TEST_VERSION, "text", new WhitespaceAnalyzer()).parse("hi");
+        query = new QueryParser(TEST_VERSION, "text", new WhitespaceAnalyzer(Version.LUCENE_CURRENT)).parse("hi");
         highlighter = getHighlighter(query, "text", getTS2a(), HighlighterTest.this);
         result = highlighter.getBestFragments(getTS2a(), s, 3, "...");
         assertEquals("<B>Hi</B>-Speed10 foo", result);
 
-        query = new QueryParser(TEST_VERSION, "text", new WhitespaceAnalyzer()).parse("speed");
+        query = new QueryParser(TEST_VERSION, "text", new WhitespaceAnalyzer(Version.LUCENE_CURRENT)).parse("speed");
         highlighter = getHighlighter(query, "text", getTS2a(), HighlighterTest.this);
         result = highlighter.getBestFragments(getTS2a(), s, 3, "...");
         assertEquals("Hi-<B>Speed</B>10 foo", result);
 
-        query = new QueryParser(TEST_VERSION, "text", new WhitespaceAnalyzer()).parse("hispeed");
+        query = new QueryParser(TEST_VERSION, "text", new WhitespaceAnalyzer(Version.LUCENE_CURRENT)).parse("hispeed");
         highlighter = getHighlighter(query, "text", getTS2a(), HighlighterTest.this);
         result = highlighter.getBestFragments(getTS2a(), s, 3, "...");
         assertEquals("<B>Hi-Speed</B>10 foo", result);
 
-        query = new QueryParser(TEST_VERSION, "text", new WhitespaceAnalyzer()).parse("hi speed");
+        query = new QueryParser(TEST_VERSION, "text", new WhitespaceAnalyzer(Version.LUCENE_CURRENT)).parse("hi speed");
         highlighter = getHighlighter(query, "text", getTS2a(), HighlighterTest.this);
         result = highlighter.getBestFragments(getTS2a(), s, 3, "...");
         assertEquals("<B>Hi-Speed</B>10 foo", result);
@@ -1595,7 +1597,7 @@
   }
   
   private Directory dir = new RAMDirectory();
-  private Analyzer a = new WhitespaceAnalyzer();
+  private Analyzer a = new WhitespaceAnalyzer(Version.LUCENE_CURRENT);
   
   public void testWeightedTermsWithDeletes() throws IOException, ParseException, InvalidTokenOffsetsException {
     makeIndex();
@@ -1799,7 +1801,7 @@
    */
   @Override
   public TokenStream tokenStream(String arg0, Reader arg1) {
-    LowerCaseTokenizer stream = new LowerCaseTokenizer(arg1);
+    LowerCaseTokenizer stream = new LowerCaseTokenizer(Version.LUCENE_CURRENT, arg1);
     stream.addAttribute(TermAttribute.class);
     stream.addAttribute(PositionIncrementAttribute.class);
     stream.addAttribute(OffsetAttribute.class);
@@ -1845,6 +1847,7 @@
         return false;
       }
       //Token nextRealToken = new Token(, offsetAtt.startOffset(), offsetAtt.endOffset());
+      clearAttributes();
       termAtt.setTermBuffer(realTermAtt.term());
       offsetAtt.setOffset(realOffsetAtt.startOffset(), realOffsetAtt.endOffset());
       posIncrAtt.setPositionIncrement(realPosIncrAtt.getPositionIncrement());
@@ -1862,6 +1865,7 @@
       return true;
     } else {
       String tok = st.nextToken();
+      clearAttributes();
       termAtt.setTermBuffer(tok);
       offsetAtt.setOffset(currentRealToken.startOffset(), currentRealToken.endOffset());
       posIncrAtt.setPositionIncrement(0);
Index: contrib/highlighter/src/java/org/apache/lucene/search/highlight/TokenStreamFromTermPositionVector.java
===================================================================
--- contrib/highlighter/src/java/org/apache/lucene/search/highlight/TokenStreamFromTermPositionVector.java	(revision 916567)
+++ contrib/highlighter/src/java/org/apache/lucene/search/highlight/TokenStreamFromTermPositionVector.java	(working copy)
@@ -102,6 +102,7 @@
   public boolean incrementToken() throws IOException {
     if (this.tokensAtCurrentPosition.hasNext()) {
       final Token next = this.tokensAtCurrentPosition.next();
+      clearAttributes();
       termAttribute.setTermBuffer(next.term());
       positionIncrementAttribute.setPositionIncrement(next
           .getPositionIncrement());
Index: contrib/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources.java
===================================================================
--- contrib/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources.java	(revision 916567)
+++ contrib/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources.java	(working copy)
@@ -169,6 +169,7 @@
           return false;
         }
         Token token = tokens[currentToken++];
+        clearAttributes();
         termAtt.setTermBuffer(token.term());
         offsetAtt.setOffset(token.startOffset(), token.endOffset());
         return true;
Index: contrib/spellchecker/src/test/org/apache/lucene/search/spell/TestLuceneDictionary.java
===================================================================
--- contrib/spellchecker/src/test/org/apache/lucene/search/spell/TestLuceneDictionary.java	(revision 916567)
+++ contrib/spellchecker/src/test/org/apache/lucene/search/spell/TestLuceneDictionary.java	(working copy)
@@ -29,6 +29,7 @@
 import org.apache.lucene.index.IndexWriter;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.RAMDirectory;
+import org.apache.lucene.util.Version;
 
 /**
  * Test case for LuceneDictionary.
@@ -48,7 +49,7 @@
   @Override
   public void setUp() throws Exception {
 
-    IndexWriter writer = new IndexWriter(store, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.UNLIMITED);
+    IndexWriter writer = new IndexWriter(store, new WhitespaceAnalyzer(Version.LUCENE_CURRENT), true, IndexWriter.MaxFieldLength.UNLIMITED);
 
     Document doc;
 
Index: contrib/spellchecker/src/test/org/apache/lucene/search/spell/TestSpellChecker.java
===================================================================
--- contrib/spellchecker/src/test/org/apache/lucene/search/spell/TestSpellChecker.java	(revision 916567)
+++ contrib/spellchecker/src/test/org/apache/lucene/search/spell/TestSpellChecker.java	(working copy)
@@ -38,6 +38,7 @@
 import org.apache.lucene.store.RAMDirectory;
 import org.apache.lucene.util.English;
 import org.apache.lucene.util.LuceneTestCase;
+import org.apache.lucene.util.Version;
 
 
 /**
@@ -57,7 +58,7 @@
     
     //create a user index
     userindex = new RAMDirectory();
-    IndexWriter writer = new IndexWriter(userindex, new SimpleAnalyzer(), true, IndexWriter.MaxFieldLength.UNLIMITED);
+    IndexWriter writer = new IndexWriter(userindex, new SimpleAnalyzer(Version.LUCENE_CURRENT), true, IndexWriter.MaxFieldLength.UNLIMITED);
 
     for (int i = 0; i < 1000; i++) {
       Document doc = new Document();
@@ -299,12 +300,12 @@
     
     spellChecker.close();
     executor.shutdown();
-    executor.awaitTermination(5, TimeUnit.SECONDS);
+    // wait for 60 seconds - usually this is very fast but coverage runs could take quite long
+    executor.awaitTermination(60L, TimeUnit.SECONDS);
     
-    
     for (int i = 0; i < workers.length; i++) {
-      assertFalse(workers[i].failed);
-      assertTrue(workers[i].terminated);
+      assertFalse(String.format("worker thread %d failed", i), workers[i].failed);
+      assertTrue(String.format("worker thread %d is still running but should be terminated", i), workers[i].terminated);
     }
     // 4 searchers more than iterations
     // 1. at creation
@@ -347,8 +348,8 @@
   
   private class SpellCheckWorker implements Runnable {
     private final IndexReader reader;
-    boolean terminated = false;
-    boolean failed = false;
+    volatile boolean terminated = false;
+    volatile boolean failed = false;
     
     SpellCheckWorker(IndexReader reader) {
       super();
Index: contrib/wikipedia/src/test/org/apache/lucene/wikipedia/analysis/WikipediaTokenizerTest.java
===================================================================
--- contrib/wikipedia/src/test/org/apache/lucene/wikipedia/analysis/WikipediaTokenizerTest.java	(revision 916567)
+++ contrib/wikipedia/src/test/org/apache/lucene/wikipedia/analysis/WikipediaTokenizerTest.java	(working copy)
@@ -18,8 +18,6 @@
 
 package org.apache.lucene.wikipedia.analysis;
 
-import junit.framework.TestCase;
-
 import java.io.StringReader;
 import java.io.IOException;
 import java.util.HashMap;
@@ -46,6 +44,18 @@
     super(s);
   }
 
+  public void testSimple() throws Exception {
+    String text = "This is a [[Category:foo]]";
+    WikipediaTokenizer tf = new WikipediaTokenizer(new StringReader(text));
+    assertTokenStreamContents(tf,
+        new String[] { "This", "is", "a", "foo" },
+        new int[] { 0, 5, 8, 21 },
+        new int[] { 4, 7, 9, 24 },
+        new String[] { "<ALPHANUM>", "<ALPHANUM>", "<ALPHANUM>", WikipediaTokenizer.CATEGORY },
+        new int[] { 1, 1, 1, 1, },
+        text.length());
+  }
+  
   public void testHandwritten() throws Exception {
     //make sure all tokens are in only one type
     String test = "[[link]] This is a [[Category:foo]] Category  This is a linked [[:Category:bar none withstanding]] " +
Index: contrib/wikipedia/src/java/org/apache/lucene/wikipedia/analysis/WikipediaTokenizer.java
===================================================================
--- contrib/wikipedia/src/java/org/apache/lucene/wikipedia/analysis/WikipediaTokenizer.java	(revision 916567)
+++ contrib/wikipedia/src/java/org/apache/lucene/wikipedia/analysis/WikipediaTokenizer.java	(working copy)
@@ -36,8 +36,7 @@
  * Wikipedia tutorial available at http://en.wikipedia.org/wiki/Wikipedia:Tutorial, but it may not be complete.
  * <p/>
  * <p/>
- * EXPERIMENTAL !!!!!!!!!
- * NOTE: This Tokenizer is considered experimental and the grammar is subject to change in the trunk and in follow up releases.
+ * @lucene.experimental
  */
 public final class WikipediaTokenizer extends Tokenizer {
   public static final String INTERNAL_LINK = "il";
@@ -320,4 +319,10 @@
     reset();
   }
 
+  @Override
+  public void end() throws IOException {
+    // set final offset
+    final int finalOffset = correctOffset(scanner.yychar() + scanner.yylength());
+    this.offsetAtt.setOffset(finalOffset, finalOffset);
+  }
 }
\ No newline at end of file
Index: contrib/memory/src/test/org/apache/lucene/index/memory/MemoryIndexTest.java
===================================================================
--- contrib/memory/src/test/org/apache/lucene/index/memory/MemoryIndexTest.java	(revision 916567)
+++ contrib/memory/src/test/org/apache/lucene/index/memory/MemoryIndexTest.java	(working copy)
@@ -277,10 +277,10 @@
     Set<?> stopWords = StopAnalyzer.ENGLISH_STOP_WORDS_SET;
     
     Analyzer[] analyzers = new Analyzer[] { 
-        new SimpleAnalyzer(),
+        new SimpleAnalyzer(Version.LUCENE_CURRENT),
         new StopAnalyzer(Version.LUCENE_CURRENT),
         new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT),
-//        new WhitespaceAnalyzer(),
+//        new WhitespaceAnalyzer(Version.LUCENE_CURRENT),
 //        new PatternAnalyzer(PatternAnalyzer.NON_WORD_PATTERN, false, null),
 //        new PatternAnalyzer(PatternAnalyzer.NON_WORD_PATTERN, true, stopWords),        
 //        new SnowballAnalyzer("English", StopAnalyzer.ENGLISH_STOP_WORDS),
Index: contrib/memory/src/java/org/apache/lucene/index/memory/MemoryIndex.java
===================================================================
--- contrib/memory/src/java/org/apache/lucene/index/memory/MemoryIndex.java	(revision 916567)
+++ contrib/memory/src/java/org/apache/lucene/index/memory/MemoryIndex.java	(working copy)
@@ -287,6 +287,7 @@
           throw new IllegalArgumentException("keyword must not be null");
         
         String term = obj.toString();
+        clearAttributes();
         termAtt.setTermBuffer(term);
         offsetAtt.setOffset(start, start+termAtt.termLength());
         start += term.length() + 1; // separate words by 1 (blank) character
Index: contrib/analyzers/smartcn/src/java/org/apache/lucene/analysis/cn/smart/CharType.java
===================================================================
--- contrib/analyzers/smartcn/src/java/org/apache/lucene/analysis/cn/smart/CharType.java	(revision 916567)
+++ contrib/analyzers/smartcn/src/java/org/apache/lucene/analysis/cn/smart/CharType.java	(working copy)
@@ -19,11 +19,7 @@
 
 /**
  * Internal SmartChineseAnalyzer character type constants.
- * <p><font color="#FF0000">
- * WARNING: The status of the analyzers/smartcn <b>analysis.cn.smart</b> package is experimental. 
- * The APIs and file formats introduced here might change in the future and will not be 
- * supported anymore in such a case.</font>
- * </p>
+ * @lucene.experimental
  */
 public class CharType {
 
Index: contrib/analyzers/smartcn/src/java/org/apache/lucene/analysis/cn/smart/SentenceTokenizer.java
===================================================================
--- contrib/analyzers/smartcn/src/java/org/apache/lucene/analysis/cn/smart/SentenceTokenizer.java	(revision 916567)
+++ contrib/analyzers/smartcn/src/java/org/apache/lucene/analysis/cn/smart/SentenceTokenizer.java	(working copy)
@@ -31,11 +31,7 @@
  * <p>
  * The output tokens can then be broken into words with {@link WordTokenFilter}
  * </p>
- * <p><font color="#FF0000">
- * WARNING: The status of the analyzers/smartcn <b>analysis.cn.smart</b> package is experimental. 
- * The APIs and file formats introduced here might change in the future and will not be 
- * supported anymore in such a case.</font>
- * </p>
+ * @lucene.experimental
  */
 public final class SentenceTokenizer extends Tokenizer {
 
@@ -134,4 +130,11 @@
     super.reset(input);
     reset();
   }
+
+  @Override
+  public void end() throws IOException {
+    // set final offset
+    final int finalOffset = correctOffset(tokenEnd);
+    offsetAtt.setOffset(finalOffset, finalOffset);
+  }
 }
Index: contrib/analyzers/smartcn/src/java/org/apache/lucene/analysis/cn/smart/WordTokenFilter.java
===================================================================
--- contrib/analyzers/smartcn/src/java/org/apache/lucene/analysis/cn/smart/WordTokenFilter.java	(revision 916567)
+++ contrib/analyzers/smartcn/src/java/org/apache/lucene/analysis/cn/smart/WordTokenFilter.java	(working copy)
@@ -30,11 +30,7 @@
 
 /**
  * A {@link TokenFilter} that breaks sentences into words.
- * <p><font color="#FF0000">
- * WARNING: The status of the analyzers/smartcn <b>analysis.cn.smart</b> package is experimental. 
- * The APIs and file formats introduced here might change in the future and will not be 
- * supported anymore in such a case.</font>
- * </p>
+ * @lucene.experimental
  */
 public final class WordTokenFilter extends TokenFilter {
 
Index: contrib/analyzers/smartcn/src/java/org/apache/lucene/analysis/cn/smart/Utility.java
===================================================================
--- contrib/analyzers/smartcn/src/java/org/apache/lucene/analysis/cn/smart/Utility.java	(revision 916567)
+++ contrib/analyzers/smartcn/src/java/org/apache/lucene/analysis/cn/smart/Utility.java	(working copy)
@@ -21,11 +21,7 @@
 
 /**
  * SmartChineseAnalyzer utility constants and methods
- * <p><font color="#FF0000">
- * WARNING: The status of the analyzers/smartcn <b>analysis.cn.smart</b> package is experimental. 
- * The APIs and file formats introduced here might change in the future and will not be 
- * supported anymore in such a case.</font>
- * </p>
+ * @lucene.experimental
  */
 public class Utility {
 
Index: contrib/analyzers/smartcn/src/java/org/apache/lucene/analysis/cn/smart/WordSegmenter.java
===================================================================
--- contrib/analyzers/smartcn/src/java/org/apache/lucene/analysis/cn/smart/WordSegmenter.java	(revision 916567)
+++ contrib/analyzers/smartcn/src/java/org/apache/lucene/analysis/cn/smart/WordSegmenter.java	(working copy)
@@ -26,11 +26,7 @@
 
 /**
  * Segment a sentence of Chinese text into words.
- * <p><font color="#FF0000">
- * WARNING: The status of the analyzers/smartcn <b>analysis.cn.smart</b> package is experimental. 
- * The APIs and file formats introduced here might change in the future and will not be 
- * supported anymore in such a case.</font>
- * </p>
+ * @lucene.experimental
  */
 class WordSegmenter {
 
Index: contrib/analyzers/smartcn/src/java/org/apache/lucene/analysis/cn/smart/AnalyzerProfile.java
===================================================================
--- contrib/analyzers/smartcn/src/java/org/apache/lucene/analysis/cn/smart/AnalyzerProfile.java	(revision 916567)
+++ contrib/analyzers/smartcn/src/java/org/apache/lucene/analysis/cn/smart/AnalyzerProfile.java	(working copy)
@@ -27,12 +27,7 @@
  * <p>
  * SmartChineseAnalyzer has a built-in dictionary and stopword list out-of-box.
  * </p>
- * <p><font color="#FF0000">
- * WARNING: The status of the analyzers/smartcn <b>analysis.cn.smart</b> package is experimental. 
- * The APIs and file formats introduced here might change in the future and will not be 
- * supported anymore in such a case.</font>
- * </p>
- * 
+ * @lucene.experimental
  */
 public class AnalyzerProfile {
 
Index: contrib/analyzers/smartcn/src/java/org/apache/lucene/analysis/cn/smart/hhmm/SegToken.java
===================================================================
--- contrib/analyzers/smartcn/src/java/org/apache/lucene/analysis/cn/smart/hhmm/SegToken.java	(revision 916567)
+++ contrib/analyzers/smartcn/src/java/org/apache/lucene/analysis/cn/smart/hhmm/SegToken.java	(working copy)
@@ -23,11 +23,7 @@
 
 /**
  * SmartChineseAnalyzer internal token
- * <p><font color="#FF0000">
- * WARNING: The status of the analyzers/smartcn <b>analysis.cn.smart</b> package is experimental. 
- * The APIs and file formats introduced here might change in the future and will not be 
- * supported anymore in such a case.</font>
- * </p>
+ * @lucene.experimental
  */
 public class SegToken {
   /**
Index: contrib/analyzers/smartcn/src/java/org/apache/lucene/analysis/cn/smart/hhmm/SegGraph.java
===================================================================
--- contrib/analyzers/smartcn/src/java/org/apache/lucene/analysis/cn/smart/hhmm/SegGraph.java	(revision 916567)
+++ contrib/analyzers/smartcn/src/java/org/apache/lucene/analysis/cn/smart/hhmm/SegGraph.java	(working copy)
@@ -27,11 +27,7 @@
  * <p>
  * For each start offset, a list of possible tokens is stored.
  * </p>
- * <p><font color="#FF0000">
- * WARNING: The status of the analyzers/smartcn <b>analysis.cn.smart</b> package is experimental. 
- * The APIs and file formats introduced here might change in the future and will not be 
- * supported anymore in such a case.</font>
- * </p>
+ * @lucene.experimental
  */
 class SegGraph {
 
Index: contrib/analyzers/smartcn/src/java/org/apache/lucene/analysis/cn/smart/hhmm/WordDictionary.java
===================================================================
--- contrib/analyzers/smartcn/src/java/org/apache/lucene/analysis/cn/smart/hhmm/WordDictionary.java	(revision 916567)
+++ contrib/analyzers/smartcn/src/java/org/apache/lucene/analysis/cn/smart/hhmm/WordDictionary.java	(working copy)
@@ -35,12 +35,7 @@
 
 /**
  * SmartChineseAnalyzer Word Dictionary
- *
- * <p><font color="#FF0000">
- * WARNING: The status of the analyzers/smartcn <b>analysis.cn.smart</b> package is experimental. 
- * The APIs and file formats introduced here might change in the future and will not be 
- * supported anymore in such a case.</font>
- * </p>
+ * @lucene.experimental
  */
 class WordDictionary extends AbstractDictionary {
 
Index: contrib/analyzers/smartcn/src/java/org/apache/lucene/analysis/cn/smart/hhmm/PathNode.java
===================================================================
--- contrib/analyzers/smartcn/src/java/org/apache/lucene/analysis/cn/smart/hhmm/PathNode.java	(revision 916567)
+++ contrib/analyzers/smartcn/src/java/org/apache/lucene/analysis/cn/smart/hhmm/PathNode.java	(working copy)
@@ -22,11 +22,7 @@
  * <p>
  * Used by {@link BiSegGraph} to maximize the segmentation with the Viterbi algorithm.
  * </p>
- * <p><font color="#FF0000">
- * WARNING: The status of the analyzers/smartcn <b>analysis.cn.smart</b> package is experimental. 
- * The APIs and file formats introduced here might change in the future and will not be 
- * supported anymore in such a case.</font>
- * </p>
+ * @lucene.experimental
  */
 class PathNode implements Comparable<PathNode> {
   public double weight;
Index: contrib/analyzers/smartcn/src/java/org/apache/lucene/analysis/cn/smart/hhmm/HHMMSegmenter.java
===================================================================
--- contrib/analyzers/smartcn/src/java/org/apache/lucene/analysis/cn/smart/hhmm/HHMMSegmenter.java	(revision 916567)
+++ contrib/analyzers/smartcn/src/java/org/apache/lucene/analysis/cn/smart/hhmm/HHMMSegmenter.java	(working copy)
@@ -26,11 +26,7 @@
 
 /**
  * Finds the optimal segmentation of a sentence into Chinese words
- * <p><font color="#FF0000">
- * WARNING: The status of the analyzers/smartcn <b>analysis.cn.smart</b> package is experimental. 
- * The APIs and file formats introduced here might change in the future and will not be 
- * supported anymore in such a case.</font>
- * </p>
+ * @lucene.experimental
  */
 public class HHMMSegmenter {
 
Index: contrib/analyzers/smartcn/src/java/org/apache/lucene/analysis/cn/smart/hhmm/SegTokenFilter.java
===================================================================
--- contrib/analyzers/smartcn/src/java/org/apache/lucene/analysis/cn/smart/hhmm/SegTokenFilter.java	(revision 916567)
+++ contrib/analyzers/smartcn/src/java/org/apache/lucene/analysis/cn/smart/hhmm/SegTokenFilter.java	(working copy)
@@ -25,11 +25,7 @@
  * Filters a {@link SegToken} by converting full-width latin to half-width, then lowercasing latin.
  * Additionally, all punctuation is converted into {@link Utility#COMMON_DELIMITER}
  * </p>
- * <p><font color="#FF0000">
- * WARNING: The status of the analyzers/smartcn <b>analysis.cn.smart</b> package is experimental. 
- * The APIs and file formats introduced here might change in the future and will not be 
- * supported anymore in such a case.</font>
- * </p>
+ * @lucene.experimental
  */
 public class SegTokenFilter {
 
Index: contrib/analyzers/smartcn/src/java/org/apache/lucene/analysis/cn/smart/hhmm/BigramDictionary.java
===================================================================
--- contrib/analyzers/smartcn/src/java/org/apache/lucene/analysis/cn/smart/hhmm/BigramDictionary.java	(revision 916567)
+++ contrib/analyzers/smartcn/src/java/org/apache/lucene/analysis/cn/smart/hhmm/BigramDictionary.java	(working copy)
@@ -34,11 +34,7 @@
 
 /**
  * SmartChineseAnalyzer Bigram dictionary.
- * <p><font color="#FF0000">
- * WARNING: The status of the analyzers/smartcn <b>analysis.cn.smart</b> package is experimental. 
- * The APIs and file formats introduced here might change in the future and will not be 
- * supported anymore in such a case.</font>
- * </p>
+ * @lucene.experimental
  */
 class BigramDictionary extends AbstractDictionary {
 
Index: contrib/analyzers/smartcn/src/java/org/apache/lucene/analysis/cn/smart/hhmm/AbstractDictionary.java
===================================================================
--- contrib/analyzers/smartcn/src/java/org/apache/lucene/analysis/cn/smart/hhmm/AbstractDictionary.java	(revision 916567)
+++ contrib/analyzers/smartcn/src/java/org/apache/lucene/analysis/cn/smart/hhmm/AbstractDictionary.java	(working copy)
@@ -26,11 +26,7 @@
  * <p>
  * Contains methods for dealing with GB2312 encoding.
  * </p>
- * <p><font color="#FF0000">
- * WARNING: The status of the analyzers/smartcn <b>analysis.cn.smart</b> package is experimental. 
- * The APIs and file formats introduced here might change in the future and will not be 
- * supported anymore in such a case.</font>
- * </p>
+ * @lucene.experimental
  */
 abstract class AbstractDictionary {
   /**
Index: contrib/analyzers/smartcn/src/java/org/apache/lucene/analysis/cn/smart/hhmm/BiSegGraph.java
===================================================================
--- contrib/analyzers/smartcn/src/java/org/apache/lucene/analysis/cn/smart/hhmm/BiSegGraph.java	(revision 916567)
+++ contrib/analyzers/smartcn/src/java/org/apache/lucene/analysis/cn/smart/hhmm/BiSegGraph.java	(working copy)
@@ -30,11 +30,7 @@
  * <p>
  * For each start offset, a list of possible token pairs is stored.
  * </p>
- * <p><font color="#FF0000">
- * WARNING: The status of the analyzers/smartcn <b>analysis.cn.smart</b> package is experimental. 
- * The APIs and file formats introduced here might change in the future and will not be 
- * supported anymore in such a case.</font>
- * </p>
+ * @lucene.experimental
  */
 class BiSegGraph {
 
Index: contrib/analyzers/smartcn/src/java/org/apache/lucene/analysis/cn/smart/hhmm/SegTokenPair.java
===================================================================
--- contrib/analyzers/smartcn/src/java/org/apache/lucene/analysis/cn/smart/hhmm/SegTokenPair.java	(revision 916567)
+++ contrib/analyzers/smartcn/src/java/org/apache/lucene/analysis/cn/smart/hhmm/SegTokenPair.java	(working copy)
@@ -21,11 +21,7 @@
 
 /**
  * A pair of tokens in {@link SegGraph}
- * <p><font color="#FF0000">
- * WARNING: The status of the analyzers/smartcn <b>analysis.cn.smart</b> package is experimental. 
- * The APIs and file formats introduced here might change in the future and will not be 
- * supported anymore in such a case.</font>
- * </p>
+ * @lucene.experimental
  */
 class SegTokenPair {
 
Index: contrib/analyzers/smartcn/src/java/org/apache/lucene/analysis/cn/smart/hhmm/package.html
===================================================================
--- contrib/analyzers/smartcn/src/java/org/apache/lucene/analysis/cn/smart/hhmm/package.html	(revision 916567)
+++ contrib/analyzers/smartcn/src/java/org/apache/lucene/analysis/cn/smart/hhmm/package.html	(working copy)
@@ -23,11 +23,7 @@
 SmartChineseAnalyzer Hidden Markov Model package.
 </div>
 <div>
-<font color="#FF0000">
-WARNING: The status of the analyzers/smartcn <b>analysis.cn.smart</b> package is experimental. The APIs
-and file formats introduced here might change in the future and will not be supported anymore
-in such a case.
-</font>
+@lucene.experimental
 </div>
 </body>
 </html>
Index: contrib/analyzers/smartcn/src/java/org/apache/lucene/analysis/cn/smart/SmartChineseAnalyzer.java
===================================================================
--- contrib/analyzers/smartcn/src/java/org/apache/lucene/analysis/cn/smart/SmartChineseAnalyzer.java	(revision 916567)
+++ contrib/analyzers/smartcn/src/java/org/apache/lucene/analysis/cn/smart/SmartChineseAnalyzer.java	(working copy)
@@ -52,11 +52,7 @@
  * The included dictionary data is from <a href="http://www.ictclas.org">ICTCLAS1.0</a>.
  * Thanks to ICTCLAS for their hard work, and for contributing the data under the Apache 2 License!
  * </p>
- * <p><font color="#FF0000">
- * WARNING: The status of the analyzers/smartcn <b>analysis.cn.smart</b> package is experimental. 
- * The APIs and file formats introduced here might change in the future and will not be 
- * supported anymore in such a case.</font>
- * </p>
+ * @lucene.experimental
  */
 public final class SmartChineseAnalyzer extends Analyzer {
 
Index: contrib/analyzers/smartcn/src/java/org/apache/lucene/analysis/cn/smart/WordType.java
===================================================================
--- contrib/analyzers/smartcn/src/java/org/apache/lucene/analysis/cn/smart/WordType.java	(revision 916567)
+++ contrib/analyzers/smartcn/src/java/org/apache/lucene/analysis/cn/smart/WordType.java	(working copy)
@@ -19,11 +19,7 @@
 
 /**
  * Internal SmartChineseAnalyzer token type constants
- * <p><font color="#FF0000">
- * WARNING: The status of the analyzers/smartcn <b>analysis.cn.smart</b> package is experimental. 
- * The APIs and file formats introduced here might change in the future and will not be 
- * supported anymore in such a case.</font>
- * </p>
+ * @lucene.experimental
  */
 public class WordType {
 
Index: contrib/analyzers/smartcn/src/java/org/apache/lucene/analysis/cn/smart/package.html
===================================================================
--- contrib/analyzers/smartcn/src/java/org/apache/lucene/analysis/cn/smart/package.html	(revision 916567)
+++ contrib/analyzers/smartcn/src/java/org/apache/lucene/analysis/cn/smart/package.html	(working copy)
@@ -24,23 +24,19 @@
 Analyzer for Simplified Chinese, which indexes words.
 </div>
 <div>
-<font color="#FF0000">
-WARNING: The status of the analyzers/smartcn <b>analysis.cn.smart</b> package is experimental. The APIs
-and file formats introduced here might change in the future and will not be supported anymore
-in such a case.
-</font>
+@lucene.experimental
 </div>
 <div>
 Three analyzers are provided for Chinese, each of which treats Chinese text in a different way.
 <ul>
-	<li>ChineseAnalyzer (in the analyzers/cn package): Index unigrams (individual Chinese characters) as a token.
+	<li>StandardAnalyzer: Index unigrams (individual Chinese characters) as a token.
 	<li>CJKAnalyzer (in the analyzers/cjk package): Index bigrams (overlapping groups of two adjacent Chinese characters) as tokens.
 	<li>SmartChineseAnalyzer (in this package): Index words (attempt to segment Chinese text into words) as tokens.
 </ul>
 
 Example phrase： "我是中国人"
 <ol>
-	<li>ChineseAnalyzer: 我－是－中－国－人</li>
+	<li>StandardAnalyzer: 我－是－中－国－人</li>
 	<li>CJKAnalyzer: 我是－是中－中国－国人</li>
 	<li>SmartChineseAnalyzer: 我－是－中国－人</li>
 </ol>
Index: contrib/analyzers/common/src/test/org/apache/lucene/analysis/compound/TestCompoundWordTokenFilter.java
===================================================================
--- contrib/analyzers/common/src/test/org/apache/lucene/analysis/compound/TestCompoundWordTokenFilter.java	(revision 916567)
+++ contrib/analyzers/common/src/test/org/apache/lucene/analysis/compound/TestCompoundWordTokenFilter.java	(working copy)
@@ -28,6 +28,7 @@
 import org.apache.lucene.analysis.WhitespaceTokenizer;
 import org.apache.lucene.analysis.compound.hyphenation.HyphenationTree;
 import org.apache.lucene.analysis.tokenattributes.TermAttribute;
+import org.apache.lucene.util.Version;
 
 public class TestCompoundWordTokenFilter extends BaseTokenStreamTestCase {
   static final File dataDir = new File(System.getProperty("dataDir", "./bin"));
@@ -46,8 +47,8 @@
     HyphenationTree hyphenator = HyphenationCompoundWordTokenFilter
         .getHyphenationTree(reader);
 
-    HyphenationCompoundWordTokenFilter tf = new HyphenationCompoundWordTokenFilter(
-        new WhitespaceTokenizer(new StringReader(
+    HyphenationCompoundWordTokenFilter tf = new HyphenationCompoundWordTokenFilter(Version.LUCENE_CURRENT, 
+        new WhitespaceTokenizer(Version.LUCENE_CURRENT, new StringReader(
             "min veninde som er lidt af en læsehest")), hyphenator,
         dict, CompoundWordTokenFilterBase.DEFAULT_MIN_WORD_SIZE,
         CompoundWordTokenFilterBase.DEFAULT_MIN_SUBWORD_SIZE,
@@ -66,8 +67,8 @@
         .getHyphenationTree(reader);
 
     // the word basket will not be added due to the longest match option
-    HyphenationCompoundWordTokenFilter tf = new HyphenationCompoundWordTokenFilter(
-        new WhitespaceTokenizer(new StringReader(
+    HyphenationCompoundWordTokenFilter tf = new HyphenationCompoundWordTokenFilter(Version.LUCENE_CURRENT, 
+        new WhitespaceTokenizer(Version.LUCENE_CURRENT, new StringReader(
             "basketballkurv")), hyphenator, dict,
         CompoundWordTokenFilterBase.DEFAULT_MIN_WORD_SIZE,
         CompoundWordTokenFilterBase.DEFAULT_MIN_SUBWORD_SIZE, 40, true);
@@ -83,8 +84,8 @@
         "Pelar", "Glas", "Ögon", "Fodral", "Bas", "Fiol", "Makare", "Gesäll",
         "Sko", "Vind", "Rute", "Torkare", "Blad" };
 
-    DictionaryCompoundWordTokenFilter tf = new DictionaryCompoundWordTokenFilter(
-        new WhitespaceTokenizer(
+    DictionaryCompoundWordTokenFilter tf = new DictionaryCompoundWordTokenFilter(Version.LUCENE_CURRENT, 
+        new WhitespaceTokenizer(Version.LUCENE_CURRENT, 
             new StringReader(
                 "Bildörr Bilmotor Biltak Slagborr Hammarborr Pelarborr Glasögonfodral Basfiolsfodral Basfiolsfodralmakaregesäll Skomakare Vindrutetorkare Vindrutetorkarblad abba")),
         dict);
@@ -112,8 +113,8 @@
         "Pelar", "Glas", "Ögon", "Fodral", "Bas", "Fiols", "Makare", "Gesäll",
         "Sko", "Vind", "Rute", "Torkare", "Blad", "Fiolsfodral" };
 
-    DictionaryCompoundWordTokenFilter tf = new DictionaryCompoundWordTokenFilter(
-        new WhitespaceTokenizer(new StringReader("Basfiolsfodralmakaregesäll")),
+    DictionaryCompoundWordTokenFilter tf = new DictionaryCompoundWordTokenFilter(Version.LUCENE_CURRENT, 
+        new WhitespaceTokenizer(Version.LUCENE_CURRENT, new StringReader("Basfiolsfodralmakaregesäll")),
         dict, CompoundWordTokenFilterBase.DEFAULT_MIN_WORD_SIZE,
         CompoundWordTokenFilterBase.DEFAULT_MIN_SUBWORD_SIZE,
         CompoundWordTokenFilterBase.DEFAULT_MAX_SUBWORD_SIZE, true);
@@ -128,9 +129,9 @@
     String[] dict = { "Rind", "Fleisch", "Draht", "Schere", "Gesetz",
         "Aufgabe", "Überwachung" };
 
-    Tokenizer wsTokenizer = new WhitespaceTokenizer(new StringReader(
+    Tokenizer wsTokenizer = new WhitespaceTokenizer(Version.LUCENE_CURRENT, new StringReader(
         "Rindfleischüberwachungsgesetz"));
-    DictionaryCompoundWordTokenFilter tf = new DictionaryCompoundWordTokenFilter(
+    DictionaryCompoundWordTokenFilter tf = new DictionaryCompoundWordTokenFilter(Version.LUCENE_CURRENT, 
         wsTokenizer, dict,
         CompoundWordTokenFilterBase.DEFAULT_MIN_WORD_SIZE,
         CompoundWordTokenFilterBase.DEFAULT_MIN_SUBWORD_SIZE,
Index: contrib/analyzers/common/src/test/org/apache/lucene/analysis/fa/TestPersianNormalizationFilter.java
===================================================================
--- contrib/analyzers/common/src/test/org/apache/lucene/analysis/fa/TestPersianNormalizationFilter.java	(revision 916567)
+++ contrib/analyzers/common/src/test/org/apache/lucene/analysis/fa/TestPersianNormalizationFilter.java	(working copy)
@@ -22,6 +22,7 @@
 
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.ar.ArabicLetterTokenizer;
+import org.apache.lucene.util.Version;
 
 /**
  * Test the Persian Normalization Filter
@@ -54,7 +55,7 @@
   }
 
   private void check(final String input, final String expected) throws IOException {
-    ArabicLetterTokenizer tokenStream = new ArabicLetterTokenizer(
+    ArabicLetterTokenizer tokenStream = new ArabicLetterTokenizer(Version.LUCENE_CURRENT, 
         new StringReader(input));
     PersianNormalizationFilter filter = new PersianNormalizationFilter(
         tokenStream);
Index: contrib/analyzers/common/src/test/org/apache/lucene/analysis/bg/TestBulgarianStemmer.java
===================================================================
--- contrib/analyzers/common/src/test/org/apache/lucene/analysis/bg/TestBulgarianStemmer.java	(revision 916567)
+++ contrib/analyzers/common/src/test/org/apache/lucene/analysis/bg/TestBulgarianStemmer.java	(working copy)
@@ -18,8 +18,12 @@
  */
 
 import java.io.IOException;
+import java.io.StringReader;
 
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
+import org.apache.lucene.analysis.CharArraySet;
+import org.apache.lucene.analysis.KeywordMarkerTokenFilter;
+import org.apache.lucene.analysis.WhitespaceTokenizer;
 import org.apache.lucene.util.Version;
 
 /**
@@ -207,4 +211,15 @@
     assertAnalyzesTo(a, "строя", new String[] {"стр"});
     assertAnalyzesTo(a, "строят", new String[] {"стр"});
   }
+
+  public void testWithKeywordAttribute() throws IOException {
+    CharArraySet set = new CharArraySet(Version.LUCENE_31, 1, true);
+    set.add("строеве");
+    WhitespaceTokenizer tokenStream = new WhitespaceTokenizer(Version.LUCENE_CURRENT, 
+        new StringReader("строевете строеве"));
+
+    BulgarianStemFilter filter = new BulgarianStemFilter(
+        new KeywordMarkerTokenFilter(tokenStream, set));
+    assertTokenStreamContents(filter, new String[] { "строй", "строеве" });
+  }
 }
Index: contrib/analyzers/common/src/test/org/apache/lucene/analysis/bg/TestBulgarianAnalyzer.java
===================================================================
--- contrib/analyzers/common/src/test/org/apache/lucene/analysis/bg/TestBulgarianAnalyzer.java	(revision 916567)
+++ contrib/analyzers/common/src/test/org/apache/lucene/analysis/bg/TestBulgarianAnalyzer.java	(working copy)
@@ -22,6 +22,7 @@
 
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
+import org.apache.lucene.analysis.CharArraySet;
 import org.apache.lucene.util.Version;
 
 /**
@@ -67,4 +68,11 @@
     
     assertAnalyzesTo(a, "градове", new String[] {"град"});
   }
+  
+  public void testWithStemExclusionSet() throws IOException {
+    CharArraySet set = new CharArraySet(Version.LUCENE_31, 1, true);
+    set.add("строеве");
+    Analyzer a = new BulgarianAnalyzer(Version.LUCENE_CURRENT, CharArraySet.EMPTY_SET, set);
+    assertAnalyzesTo(a, "строевете строеве", new String[] { "строй", "строеве" });
+  }
 }
Index: contrib/analyzers/common/src/test/org/apache/lucene/analysis/de/TestGermanStemFilter.java
===================================================================
--- contrib/analyzers/common/src/test/org/apache/lucene/analysis/de/TestGermanStemFilter.java	(revision 916567)
+++ contrib/analyzers/common/src/test/org/apache/lucene/analysis/de/TestGermanStemFilter.java	(working copy)
@@ -21,9 +21,13 @@
 import java.io.File;
 import java.io.FileInputStream;
 import java.io.InputStreamReader;
+import java.io.StringReader;
 
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
-import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.analysis.KeywordTokenizer;
+import org.apache.lucene.analysis.LowerCaseFilter;
+import org.apache.lucene.analysis.TokenFilter;
+import org.apache.lucene.analysis.Tokenizer;
 import org.apache.lucene.util.Version;
 
 /**
@@ -35,6 +39,8 @@
 public class TestGermanStemFilter extends BaseTokenStreamTestCase {
 
   public void testStemming() throws Exception {
+    Tokenizer tokenizer = new KeywordTokenizer(new StringReader(""));
+    TokenFilter filter = new GermanStemFilter(new LowerCaseFilter(Version.LUCENE_CURRENT, tokenizer));
     // read test cases from external file:
     File dataDir = new File(System.getProperty("dataDir", "./bin"));
     File testFile = new File(dataDir, "org/apache/lucene/analysis/de/data.txt");
@@ -50,36 +56,12 @@
         continue;    // ignore comments and empty lines
       String[] parts = line.split(";");
       //System.out.println(parts[0] + " -- " + parts[1]);
-      check(parts[0], parts[1]);
+      tokenizer.reset(new StringReader(parts[0]));
+      filter.reset();
+      assertTokenStreamContents(filter, new String[] { parts[1] });
     }
     breader.close();
     isr.close();
     fis.close();
   }
-  
-  public void testReusableTokenStream() throws Exception {
-    Analyzer a = new GermanAnalyzer(Version.LUCENE_CURRENT);
-    checkReuse(a, "Tisch", "tisch");
-    checkReuse(a, "Tische", "tisch");
-    checkReuse(a, "Tischen", "tisch");
-  }
-  
-  /* 
-   * Test that changes to the exclusion table are applied immediately
-   * when using reusable token streams.
-   */
-  public void testExclusionTableReuse() throws Exception {
-    GermanAnalyzer a = new GermanAnalyzer(Version.LUCENE_CURRENT);
-    checkReuse(a, "tischen", "tisch");
-    a.setStemExclusionTable(new String[] { "tischen" });
-    checkReuse(a, "tischen", "tischen");
-  }
-  
-  private void check(final String input, final String expected) throws Exception {
-    checkOneTerm(new GermanAnalyzer(Version.LUCENE_CURRENT), input, expected);
-  }
-  
-  private void checkReuse(Analyzer a, String input, String expected) throws Exception {
-    checkOneTermReuse(a, input, expected);
-  }
 }
Index: contrib/analyzers/common/src/test/org/apache/lucene/analysis/br/TestBrazilianStemmer.java
===================================================================
--- contrib/analyzers/common/src/test/org/apache/lucene/analysis/br/TestBrazilianStemmer.java	(revision 916567)
+++ contrib/analyzers/common/src/test/org/apache/lucene/analysis/br/TestBrazilianStemmer.java	(working copy)
@@ -17,12 +17,14 @@
  * limitations under the License.
  */
 
-import java.util.Arrays;
-import java.util.Collections;
-import java.util.HashSet;
+import java.io.IOException;
+import java.io.StringReader;
 
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.analysis.CharArraySet;
+import org.apache.lucene.analysis.KeywordMarkerTokenFilter;
+import org.apache.lucene.analysis.LowerCaseTokenizer;
 import org.apache.lucene.util.Version;
 
 /**
@@ -139,6 +141,34 @@
     checkReuse(a, "quintessência", "quintessência"); // excluded words will be completely unchanged.
   }
   
+  public void testStemExclusionTableBWCompat() throws IOException {
+    CharArraySet set = new CharArraySet(Version.LUCENE_CURRENT, 1, true);
+    set.add("Brasília");
+    BrazilianStemFilter filter = new BrazilianStemFilter(
+        new LowerCaseTokenizer(Version.LUCENE_CURRENT, new StringReader("Brasília Brasilia")), set);
+    assertTokenStreamContents(filter, new String[] { "brasília", "brasil" });
+  }
+
+  public void testWithKeywordAttribute() throws IOException {
+    CharArraySet set = new CharArraySet(Version.LUCENE_CURRENT, 1, true);
+    set.add("Brasília");
+    BrazilianStemFilter filter = new BrazilianStemFilter(
+        new KeywordMarkerTokenFilter(new LowerCaseTokenizer(Version.LUCENE_CURRENT, new StringReader(
+            "Brasília Brasilia")), set));
+    assertTokenStreamContents(filter, new String[] { "brasília", "brasil" });
+  }
+
+  public void testWithKeywordAttributeAndExclusionTable() throws IOException {
+    CharArraySet set = new CharArraySet(Version.LUCENE_CURRENT, 1, true);
+    set.add("Brasília");
+    CharArraySet set1 = new CharArraySet(Version.LUCENE_CURRENT, 1, true);
+    set1.add("Brasilia");
+    BrazilianStemFilter filter = new BrazilianStemFilter(
+        new KeywordMarkerTokenFilter(new LowerCaseTokenizer(Version.LUCENE_CURRENT, new StringReader(
+            "Brasília Brasilia")), set), set1);
+    assertTokenStreamContents(filter, new String[] { "brasília", "brasilia" });
+  }
+  
   /* 
    * Test that changes to the exclusion table are applied immediately
    * when using reusable token streams.
Index: contrib/analyzers/common/src/test/org/apache/lucene/analysis/miscellaneous/TestPrefixAwareTokenFilter.java
===================================================================
--- contrib/analyzers/common/src/test/org/apache/lucene/analysis/miscellaneous/TestPrefixAwareTokenFilter.java	(revision 916567)
+++ contrib/analyzers/common/src/test/org/apache/lucene/analysis/miscellaneous/TestPrefixAwareTokenFilter.java	(working copy)
@@ -19,10 +19,8 @@
 
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.Token;
-import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.WhitespaceTokenizer;
-import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
-import org.apache.lucene.analysis.tokenattributes.TermAttribute;
+import org.apache.lucene.util.Version;
 
 import java.io.IOException;
 import java.io.StringReader;
@@ -36,33 +34,23 @@
     ts = new PrefixAwareTokenFilter(
         new SingleTokenTokenStream(createToken("a", 0, 1)),
         new SingleTokenTokenStream(createToken("b", 0, 1)));
-    assertNext(ts, "a", 0, 1);
-    assertNext(ts, "b", 1, 2);
-    assertFalse(ts.incrementToken());
+    assertTokenStreamContents(ts, 
+        new String[] { "a", "b" },
+        new int[] { 0, 1 },
+        new int[] { 1, 2 });
 
     // prefix and suffix using 2x prefix
 
-    ts = new PrefixAwareTokenFilter(new SingleTokenTokenStream(createToken("^", 0, 0)), new WhitespaceTokenizer(new StringReader("hello world")));
+    ts = new PrefixAwareTokenFilter(new SingleTokenTokenStream(createToken("^", 0, 0)),
+        new WhitespaceTokenizer(Version.LUCENE_CURRENT, new StringReader("hello world")));
     ts = new PrefixAwareTokenFilter(ts, new SingleTokenTokenStream(createToken("$", 0, 0)));
 
-    assertNext(ts, "^", 0, 0);
-    assertNext(ts, "hello", 0, 5);
-    assertNext(ts, "world", 6, 11);
-    assertNext(ts, "$", 11, 11);
-    assertFalse(ts.incrementToken());
+    assertTokenStreamContents(ts,
+        new String[] { "^", "hello", "world", "$" },
+        new int[] { 0, 0, 6, 11 },
+        new int[] { 0, 5, 11, 11 });
   }
 
-
-  private void assertNext(TokenStream ts, String text, int startOffset, int endOffset) throws IOException {
-    TermAttribute termAtt = ts.addAttribute(TermAttribute.class);
-    OffsetAttribute offsetAtt = ts.addAttribute(OffsetAttribute.class);
-
-    assertTrue(ts.incrementToken());
-    assertEquals(text, termAtt.term());
-    assertEquals(startOffset, offsetAtt.startOffset());
-    assertEquals(endOffset, offsetAtt.endOffset());
-  }
-
   private static Token createToken(String term, int start, int offset)
   {
     Token token = new Token(start, offset);
Index: contrib/analyzers/common/src/test/org/apache/lucene/analysis/miscellaneous/TestPrefixAndSuffixAwareTokenFilter.java
===================================================================
--- contrib/analyzers/common/src/test/org/apache/lucene/analysis/miscellaneous/TestPrefixAndSuffixAwareTokenFilter.java	(revision 916567)
+++ contrib/analyzers/common/src/test/org/apache/lucene/analysis/miscellaneous/TestPrefixAndSuffixAwareTokenFilter.java	(working copy)
@@ -19,10 +19,8 @@
 
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.Token;
-import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.WhitespaceTokenizer;
-import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
-import org.apache.lucene.analysis.tokenattributes.TermAttribute;
+import org.apache.lucene.util.Version;
 
 import java.io.IOException;
 import java.io.StringReader;
@@ -33,27 +31,15 @@
 
     PrefixAndSuffixAwareTokenFilter ts = new PrefixAndSuffixAwareTokenFilter(
         new SingleTokenTokenStream(createToken("^", 0, 0)),
-        new WhitespaceTokenizer(new StringReader("hello world")),
+        new WhitespaceTokenizer(Version.LUCENE_CURRENT, new StringReader("hello world")),
         new SingleTokenTokenStream(createToken("$", 0, 0)));
 
-    assertNext(ts, "^", 0, 0);
-    assertNext(ts, "hello", 0, 5);
-    assertNext(ts, "world", 6, 11);
-    assertNext(ts, "$", 11, 11);
-    assertFalse(ts.incrementToken());
+    assertTokenStreamContents(ts,
+        new String[] { "^", "hello", "world", "$" },
+        new int[] { 0, 0, 6, 11 },
+        new int[] { 0, 5, 11, 11 });
   }
 
-
-  private void assertNext(TokenStream ts, String text, int startOffset, int endOffset) throws IOException {
-    TermAttribute termAtt = ts.addAttribute(TermAttribute.class);
-    OffsetAttribute offsetAtt = ts.addAttribute(OffsetAttribute.class);
-
-    assertTrue(ts.incrementToken());
-    assertEquals(text, termAtt.term());
-    assertEquals(startOffset, offsetAtt.startOffset());
-    assertEquals(endOffset, offsetAtt.endOffset());
-  }
-
   private static Token createToken(String term, int start, int offset)
   {
     Token token = new Token(start, offset);
Index: contrib/analyzers/common/src/test/org/apache/lucene/analysis/ngram/NGramTokenizerTest.java
===================================================================
--- contrib/analyzers/common/src/test/org/apache/lucene/analysis/ngram/NGramTokenizerTest.java	(revision 916567)
+++ contrib/analyzers/common/src/test/org/apache/lucene/analysis/ngram/NGramTokenizerTest.java	(working copy)
@@ -18,10 +18,8 @@
  */
 
 
-import java.io.IOException;
 import java.io.StringReader;
 
-import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 
 /**
@@ -58,12 +56,12 @@
 
     public void testUnigrams() throws Exception {
         NGramTokenizer tokenizer = new NGramTokenizer(input, 1, 1);
-        assertTokenStreamContents(tokenizer, new String[]{"a","b","c","d","e"}, new int[]{0,1,2,3,4}, new int[]{1,2,3,4,5});
+        assertTokenStreamContents(tokenizer, new String[]{"a","b","c","d","e"}, new int[]{0,1,2,3,4}, new int[]{1,2,3,4,5}, 5 /* abcde */);
     }
 
     public void testBigrams() throws Exception {
         NGramTokenizer tokenizer = new NGramTokenizer(input, 2, 2);
-        assertTokenStreamContents(tokenizer, new String[]{"ab","bc","cd","de"}, new int[]{0,1,2,3}, new int[]{2,3,4,5});
+        assertTokenStreamContents(tokenizer, new String[]{"ab","bc","cd","de"}, new int[]{0,1,2,3}, new int[]{2,3,4,5}, 5 /* abcde */);
     }
 
     public void testNgrams() throws Exception {
@@ -71,19 +69,20 @@
         assertTokenStreamContents(tokenizer,
           new String[]{"a","b","c","d","e", "ab","bc","cd","de", "abc","bcd","cde"}, 
           new int[]{0,1,2,3,4, 0,1,2,3, 0,1,2},
-          new int[]{1,2,3,4,5, 2,3,4,5, 3,4,5}
+          new int[]{1,2,3,4,5, 2,3,4,5, 3,4,5},
+          5 /* abcde */
         );
     }
 
     public void testOversizedNgrams() throws Exception {
         NGramTokenizer tokenizer = new NGramTokenizer(input, 6, 7);
-        assertTokenStreamContents(tokenizer, new String[0], new int[0], new int[0]);
+        assertTokenStreamContents(tokenizer, new String[0], new int[0], new int[0], 5 /* abcde */);
     }
     
     public void testReset() throws Exception {
       NGramTokenizer tokenizer = new NGramTokenizer(input, 1, 1);
-      assertTokenStreamContents(tokenizer, new String[]{"a","b","c","d","e"}, new int[]{0,1,2,3,4}, new int[]{1,2,3,4,5});
+      assertTokenStreamContents(tokenizer, new String[]{"a","b","c","d","e"}, new int[]{0,1,2,3,4}, new int[]{1,2,3,4,5}, 5 /* abcde */);
       tokenizer.reset(new StringReader("abcde"));
-      assertTokenStreamContents(tokenizer, new String[]{"a","b","c","d","e"}, new int[]{0,1,2,3,4}, new int[]{1,2,3,4,5});
+      assertTokenStreamContents(tokenizer, new String[]{"a","b","c","d","e"}, new int[]{0,1,2,3,4}, new int[]{1,2,3,4,5}, 5 /* abcde */);
     }
 }
Index: contrib/analyzers/common/src/test/org/apache/lucene/analysis/ngram/EdgeNGramTokenFilterTest.java
===================================================================
--- contrib/analyzers/common/src/test/org/apache/lucene/analysis/ngram/EdgeNGramTokenFilterTest.java	(revision 916567)
+++ contrib/analyzers/common/src/test/org/apache/lucene/analysis/ngram/EdgeNGramTokenFilterTest.java	(working copy)
@@ -20,6 +20,7 @@
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.WhitespaceTokenizer;
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
+import org.apache.lucene.util.Version;
 
 import java.io.StringReader;
 
@@ -32,7 +33,7 @@
   @Override
   public void setUp() throws Exception {
     super.setUp();
-    input = new WhitespaceTokenizer(new StringReader("abcde"));
+    input = new WhitespaceTokenizer(Version.LUCENE_CURRENT, new StringReader("abcde"));
   }
 
   public void testInvalidInput() throws Exception {
@@ -91,13 +92,13 @@
   }
   
   public void testSmallTokenInStream() throws Exception {
-    input = new WhitespaceTokenizer(new StringReader("abc de fgh"));
+    input = new WhitespaceTokenizer(Version.LUCENE_CURRENT, new StringReader("abc de fgh"));
     EdgeNGramTokenFilter tokenizer = new EdgeNGramTokenFilter(input, EdgeNGramTokenFilter.Side.FRONT, 3, 3);
-    assertTokenStreamContents(tokenizer, new String[]{"abc","fgh"}, new int[]{0,0}, new int[]{3,3});
+    assertTokenStreamContents(tokenizer, new String[]{"abc","fgh"}, new int[]{0,7}, new int[]{3,10});
   }
   
   public void testReset() throws Exception {
-    WhitespaceTokenizer tokenizer = new WhitespaceTokenizer(new StringReader("abcde"));
+    WhitespaceTokenizer tokenizer = new WhitespaceTokenizer(Version.LUCENE_CURRENT, new StringReader("abcde"));
     EdgeNGramTokenFilter filter = new EdgeNGramTokenFilter(tokenizer, EdgeNGramTokenFilter.Side.FRONT, 1, 3);
     assertTokenStreamContents(filter, new String[]{"a","ab","abc"}, new int[]{0,0,0}, new int[]{1,2,3});
     tokenizer.reset(new StringReader("abcde"));
Index: contrib/analyzers/common/src/test/org/apache/lucene/analysis/ngram/EdgeNGramTokenizerTest.java
===================================================================
--- contrib/analyzers/common/src/test/org/apache/lucene/analysis/ngram/EdgeNGramTokenizerTest.java	(revision 916567)
+++ contrib/analyzers/common/src/test/org/apache/lucene/analysis/ngram/EdgeNGramTokenizerTest.java	(working copy)
@@ -66,33 +66,33 @@
 
   public void testFrontUnigram() throws Exception {
     EdgeNGramTokenizer tokenizer = new EdgeNGramTokenizer(input, EdgeNGramTokenizer.Side.FRONT, 1, 1);
-    assertTokenStreamContents(tokenizer, new String[]{"a"}, new int[]{0}, new int[]{1});
+    assertTokenStreamContents(tokenizer, new String[]{"a"}, new int[]{0}, new int[]{1}, 5 /* abcde */);
   }
 
   public void testBackUnigram() throws Exception {
     EdgeNGramTokenizer tokenizer = new EdgeNGramTokenizer(input, EdgeNGramTokenizer.Side.BACK, 1, 1);
-    assertTokenStreamContents(tokenizer, new String[]{"e"}, new int[]{4}, new int[]{5});
+    assertTokenStreamContents(tokenizer, new String[]{"e"}, new int[]{4}, new int[]{5}, 5 /* abcde */);
   }
 
   public void testOversizedNgrams() throws Exception {
     EdgeNGramTokenizer tokenizer = new EdgeNGramTokenizer(input, EdgeNGramTokenizer.Side.FRONT, 6, 6);
-    assertTokenStreamContents(tokenizer, new String[0], new int[0], new int[0]);
+    assertTokenStreamContents(tokenizer, new String[0], new int[0], new int[0], 5 /* abcde */);
   }
 
   public void testFrontRangeOfNgrams() throws Exception {
     EdgeNGramTokenizer tokenizer = new EdgeNGramTokenizer(input, EdgeNGramTokenizer.Side.FRONT, 1, 3);
-    assertTokenStreamContents(tokenizer, new String[]{"a","ab","abc"}, new int[]{0,0,0}, new int[]{1,2,3});
+    assertTokenStreamContents(tokenizer, new String[]{"a","ab","abc"}, new int[]{0,0,0}, new int[]{1,2,3}, 5 /* abcde */);
   }
 
   public void testBackRangeOfNgrams() throws Exception {
     EdgeNGramTokenizer tokenizer = new EdgeNGramTokenizer(input, EdgeNGramTokenizer.Side.BACK, 1, 3);
-    assertTokenStreamContents(tokenizer, new String[]{"e","de","cde"}, new int[]{4,3,2}, new int[]{5,5,5});
+    assertTokenStreamContents(tokenizer, new String[]{"e","de","cde"}, new int[]{4,3,2}, new int[]{5,5,5}, 5 /* abcde */);
   }
   
   public void testReset() throws Exception {
     EdgeNGramTokenizer tokenizer = new EdgeNGramTokenizer(input, EdgeNGramTokenizer.Side.FRONT, 1, 3);
-    assertTokenStreamContents(tokenizer, new String[]{"a","ab","abc"}, new int[]{0,0,0}, new int[]{1,2,3});
+    assertTokenStreamContents(tokenizer, new String[]{"a","ab","abc"}, new int[]{0,0,0}, new int[]{1,2,3}, 5 /* abcde */);
     tokenizer.reset(new StringReader("abcde"));
-    assertTokenStreamContents(tokenizer, new String[]{"a","ab","abc"}, new int[]{0,0,0}, new int[]{1,2,3});
+    assertTokenStreamContents(tokenizer, new String[]{"a","ab","abc"}, new int[]{0,0,0}, new int[]{1,2,3}, 5 /* abcde */);
   }
 }
Index: contrib/analyzers/common/src/test/org/apache/lucene/analysis/ngram/NGramTokenFilterTest.java
===================================================================
--- contrib/analyzers/common/src/test/org/apache/lucene/analysis/ngram/NGramTokenFilterTest.java	(revision 916567)
+++ contrib/analyzers/common/src/test/org/apache/lucene/analysis/ngram/NGramTokenFilterTest.java	(working copy)
@@ -20,8 +20,8 @@
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.WhitespaceTokenizer;
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
+import org.apache.lucene.util.Version;
 
-import java.io.IOException;
 import java.io.StringReader;
 
 /**
@@ -33,7 +33,7 @@
     @Override
     public void setUp() throws Exception {
         super.setUp();
-        input = new WhitespaceTokenizer(new StringReader("abcde"));
+        input = new WhitespaceTokenizer(Version.LUCENE_CURRENT, new StringReader("abcde"));
     }
 
     public void testInvalidInput() throws Exception {
@@ -81,13 +81,13 @@
     }
     
     public void testSmallTokenInStream() throws Exception {
-      input = new WhitespaceTokenizer(new StringReader("abc de fgh"));
+      input = new WhitespaceTokenizer(Version.LUCENE_CURRENT, new StringReader("abc de fgh"));
       NGramTokenFilter filter = new NGramTokenFilter(input, 3, 3);
-      assertTokenStreamContents(filter, new String[]{"abc","fgh"}, new int[]{0,0}, new int[]{3,3});
+      assertTokenStreamContents(filter, new String[]{"abc","fgh"}, new int[]{0,7}, new int[]{3,10});
     }
     
     public void testReset() throws Exception {
-      WhitespaceTokenizer tokenizer = new WhitespaceTokenizer(new StringReader("abcde"));
+      WhitespaceTokenizer tokenizer = new WhitespaceTokenizer(Version.LUCENE_CURRENT, new StringReader("abcde"));
       NGramTokenFilter filter = new NGramTokenFilter(tokenizer, 1, 1);
       assertTokenStreamContents(filter, new String[]{"a","b","c","d","e"}, new int[]{0,1,2,3,4}, new int[]{1,2,3,4,5});
       tokenizer.reset(new StringReader("abcde"));
Index: contrib/analyzers/common/src/test/org/apache/lucene/analysis/query/QueryAutoStopWordAnalyzerTest.java
===================================================================
--- contrib/analyzers/common/src/test/org/apache/lucene/analysis/query/QueryAutoStopWordAnalyzerTest.java	(revision 916567)
+++ contrib/analyzers/common/src/test/org/apache/lucene/analysis/query/QueryAutoStopWordAnalyzerTest.java	(working copy)
@@ -51,7 +51,7 @@
   protected void setUp() throws Exception {
     super.setUp();
     dir = new RAMDirectory();
-    appAnalyzer = new WhitespaceAnalyzer();
+    appAnalyzer = new WhitespaceAnalyzer(Version.LUCENE_CURRENT);
     IndexWriter writer = new IndexWriter(dir, appAnalyzer, true, IndexWriter.MaxFieldLength.UNLIMITED);
     int numDocs = 200;
     for (int i = 0; i < numDocs; i++) {
@@ -157,9 +157,9 @@
     @Override
     public TokenStream tokenStream(String fieldName, Reader reader) {
       if (++invocationCount % 2 == 0)
-        return new WhitespaceTokenizer(reader);
+        return new WhitespaceTokenizer(Version.LUCENE_CURRENT, reader);
       else
-        return new LetterTokenizer(reader);
+        return new LetterTokenizer(Version.LUCENE_CURRENT, reader);
     }
   }
   
@@ -173,7 +173,7 @@
   }
   
   public void testTokenStream() throws Exception {
-    QueryAutoStopWordAnalyzer a = new QueryAutoStopWordAnalyzer(Version.LUCENE_CURRENT, new WhitespaceAnalyzer());
+    QueryAutoStopWordAnalyzer a = new QueryAutoStopWordAnalyzer(Version.LUCENE_CURRENT, new WhitespaceAnalyzer(Version.LUCENE_CURRENT));
     a.addStopWords(reader, 10);
     TokenStream ts = a.tokenStream("repetitiveField", new StringReader("this boring"));
     TermAttribute termAtt = ts.getAttribute(TermAttribute.class);
Index: contrib/analyzers/common/src/test/org/apache/lucene/analysis/fr/TestFrenchAnalyzer.java
===================================================================
--- contrib/analyzers/common/src/test/org/apache/lucene/analysis/fr/TestFrenchAnalyzer.java	(revision 916567)
+++ contrib/analyzers/common/src/test/org/apache/lucene/analysis/fr/TestFrenchAnalyzer.java	(working copy)
@@ -17,11 +17,10 @@
  * limitations under the License.
  */
 
-import java.io.StringReader;
+import java.io.IOException;
 
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
-import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.analysis.CharArraySet;
 import org.apache.lucene.util.Version;
 
 /**
@@ -116,6 +115,94 @@
 
 	}
 	
+	/**
+	 * @deprecated remove this test for Lucene 4.0
+	 */
+	@Deprecated
+	public void testAnalyzer30() throws Exception {
+	    FrenchAnalyzer fa = new FrenchAnalyzer(Version.LUCENE_30);
+	  
+	    assertAnalyzesTo(fa, "", new String[] {
+	    });
+
+	    assertAnalyzesTo(
+	      fa,
+	      "chien chat cheval",
+	      new String[] { "chien", "chat", "cheval" });
+
+	    assertAnalyzesTo(
+	      fa,
+	      "chien CHAT CHEVAL",
+	      new String[] { "chien", "chat", "cheval" });
+
+	    assertAnalyzesTo(
+	      fa,
+	      "  chien  ,? + = -  CHAT /: > CHEVAL",
+	      new String[] { "chien", "chat", "cheval" });
+
+	    assertAnalyzesTo(fa, "chien++", new String[] { "chien" });
+
+	    assertAnalyzesTo(
+	      fa,
+	      "mot \"entreguillemet\"",
+	      new String[] { "mot", "entreguillemet" });
+
+	    // let's do some french specific tests now  
+
+	    /* 1. couldn't resist
+	     I would expect this to stay one term as in French the minus 
+	    sign is often used for composing words */
+	    assertAnalyzesTo(
+	      fa,
+	      "Jean-François",
+	      new String[] { "jean", "françois" });
+
+	    // 2. stopwords
+	    assertAnalyzesTo(
+	      fa,
+	      "le la chien les aux chat du des à cheval",
+	      new String[] { "chien", "chat", "cheval" });
+
+	    // some nouns and adjectives
+	    assertAnalyzesTo(
+	      fa,
+	      "lances chismes habitable chiste éléments captifs",
+	      new String[] {
+	        "lanc",
+	        "chism",
+	        "habit",
+	        "chist",
+	        "élément",
+	        "captif" });
+
+	    // some verbs
+	    assertAnalyzesTo(
+	      fa,
+	      "finissions souffrirent rugissante",
+	      new String[] { "fin", "souffr", "rug" });
+
+	    // some everything else
+	    // aujourd'hui stays one term which is OK
+	    assertAnalyzesTo(
+	      fa,
+	      "C3PO aujourd'hui oeuf ïâöûàä anticonstitutionnellement Java++ ",
+	      new String[] {
+	        "c3po",
+	        "aujourd'hui",
+	        "oeuf",
+	        "ïâöûàä",
+	        "anticonstitutionnel",
+	        "jav" });
+
+	    // some more everything else
+	    // here 1940-1945 stays as one term, 1940:1945 not ?
+	    assertAnalyzesTo(
+	      fa,
+	      "33Bis 1940-1945 1940:1945 (---i+++)*",
+	      new String[] { "33bis", "1940-1945", "1940", "1945", "i" });
+
+	  }
+	
 	public void testReusableTokenStream() throws Exception {
 	  FrenchAnalyzer fa = new FrenchAnalyzer(Version.LUCENE_CURRENT);
 	  // stopwords
@@ -147,4 +234,41 @@
 	  fa.setStemExclusionTable(new String[] { "habitable" });
 	  assertAnalyzesToReuse(fa, "habitable", new String[] { "habitable" });
 	}
+	
+  public void testExclusionTableViaCtor() throws Exception {
+    CharArraySet set = new CharArraySet(Version.LUCENE_CURRENT, 1, true);
+    set.add("habitable");
+    FrenchAnalyzer fa = new FrenchAnalyzer(Version.LUCENE_CURRENT,
+        CharArraySet.EMPTY_SET, set);
+    assertAnalyzesToReuse(fa, "habitable chiste", new String[] { "habitable",
+        "chist" });
+
+    fa = new FrenchAnalyzer(Version.LUCENE_CURRENT, CharArraySet.EMPTY_SET, set);
+    assertAnalyzesTo(fa, "habitable chiste", new String[] { "habitable",
+        "chist" });
+  }
+  
+  public void testElision() throws Exception {
+    FrenchAnalyzer fa = new FrenchAnalyzer(Version.LUCENE_CURRENT);
+    assertAnalyzesTo(fa, "voir l'embrouille", new String[] { "voir", "embrouill" });
+  }
+  
+  /**
+   * Prior to 3.1, this analyzer had no lowercase filter.
+   * stopwords were case sensitive. Preserve this for back compat.
+   * @deprecated Remove this test in Lucene 4.0
+   */
+  @Deprecated
+  public void testBuggyStopwordsCasing() throws IOException {
+    FrenchAnalyzer a = new FrenchAnalyzer(Version.LUCENE_30);
+    assertAnalyzesTo(a, "Votre", new String[] { "votr" });
+  }
+  
+  /**
+   * Test that stopwords are not case sensitive
+   */
+  public void testStopwordsCasing() throws IOException {
+    FrenchAnalyzer a = new FrenchAnalyzer(Version.LUCENE_31);
+    assertAnalyzesTo(a, "Votre", new String[] { });
+  }
 }
Index: contrib/analyzers/common/src/test/org/apache/lucene/analysis/nl/TestDutchStemmer.java
===================================================================
--- contrib/analyzers/common/src/test/org/apache/lucene/analysis/nl/TestDutchStemmer.java	(revision 916567)
+++ contrib/analyzers/common/src/test/org/apache/lucene/analysis/nl/TestDutchStemmer.java	(working copy)
@@ -18,9 +18,11 @@
  */
 
 import java.io.File;
+import java.io.IOException;
 
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.analysis.CharArraySet;
 import org.apache.lucene.util.Version;
 
 /**
@@ -98,9 +100,6 @@
 	 check("ophalend", "ophal");
 	 check("ophalers", "ophaler");
 	 check("ophef", "ophef");
-	 check("opheffen", "ophef"); // versus snowball 'opheff'
-	 check("opheffende", "ophef"); // versus snowball 'opheff'
-	 check("opheffing", "ophef"); // versus snowball 'opheff'
 	 check("opheldering", "ophelder");
 	 check("ophemelde", "ophemeld");
 	 check("ophemelen", "ophemel");
@@ -116,6 +115,24 @@
 	 check("ophouden", "ophoud");
   }
   
+  /**
+   * @deprecated remove this test in Lucene 4.0
+   */
+  @Deprecated
+  public void testOldBuggyStemmer() throws Exception {
+    Analyzer a = new DutchAnalyzer(Version.LUCENE_30);
+    checkOneTermReuse(a, "opheffen", "ophef"); // versus snowball 'opheff'
+    checkOneTermReuse(a, "opheffende", "ophef"); // versus snowball 'opheff'
+    checkOneTermReuse(a, "opheffing", "ophef"); // versus snowball 'opheff'
+  }
+  
+  public void testSnowballCorrectness() throws Exception {
+    Analyzer a = new DutchAnalyzer(Version.LUCENE_CURRENT);
+    checkOneTermReuse(a, "opheffen", "opheff");
+    checkOneTermReuse(a, "opheffende", "opheff");
+    checkOneTermReuse(a, "opheffing", "opheff");
+  }
+  
   public void testReusableTokenStream() throws Exception {
     Analyzer a = new DutchAnalyzer(Version.LUCENE_CURRENT); 
     checkOneTermReuse(a, "lichaamsziek", "lichaamsziek");
@@ -133,8 +150,21 @@
     checkOneTermReuse(a, "lichamelijk", "licham");
     a.setStemExclusionTable(new String[] { "lichamelijk" });
     checkOneTermReuse(a, "lichamelijk", "lichamelijk");
+
+    
   }
   
+  public void testExclusionTableViaCtor() throws IOException {
+    CharArraySet set = new CharArraySet(Version.LUCENE_30, 1, true);
+    set.add("lichamelijk");
+    DutchAnalyzer a = new DutchAnalyzer(Version.LUCENE_CURRENT, CharArraySet.EMPTY_SET, set);
+    assertAnalyzesToReuse(a, "lichamelijk lichamelijke", new String[] { "lichamelijk", "licham" });
+    
+    a = new DutchAnalyzer(Version.LUCENE_CURRENT, CharArraySet.EMPTY_SET, set);
+    assertAnalyzesTo(a, "lichamelijk lichamelijke", new String[] { "lichamelijk", "licham" });
+
+  }
+  
   /* 
    * Test that changes to the dictionary stemming table are applied immediately
    * when using reusable token streams.
@@ -146,6 +176,25 @@
     checkOneTermReuse(a, "lichamelijk", "somethingentirelydifferent");
   }
   
+  /**
+   * Prior to 3.1, this analyzer had no lowercase filter.
+   * stopwords were case sensitive. Preserve this for back compat.
+   * @deprecated Remove this test in Lucene 4.0
+   */
+  @Deprecated
+  public void testBuggyStopwordsCasing() throws IOException {
+    DutchAnalyzer a = new DutchAnalyzer(Version.LUCENE_30);
+    assertAnalyzesTo(a, "Zelf", new String[] { "zelf" });
+  }
+  
+  /**
+   * Test that stopwords are not case sensitive
+   */
+  public void testStopwordsCasing() throws IOException {
+    DutchAnalyzer a = new DutchAnalyzer(Version.LUCENE_31);
+    assertAnalyzesTo(a, "Zelf", new String[] { });
+  }
+  
   private void check(final String input, final String expected) throws Exception {
     checkOneTerm(new DutchAnalyzer(Version.LUCENE_CURRENT), input, expected); 
   }
Index: contrib/analyzers/common/src/test/org/apache/lucene/analysis/reverse/TestReverseStringFilter.java
===================================================================
--- contrib/analyzers/common/src/test/org/apache/lucene/analysis/reverse/TestReverseStringFilter.java	(revision 916567)
+++ contrib/analyzers/common/src/test/org/apache/lucene/analysis/reverse/TestReverseStringFilter.java	(working copy)
@@ -27,9 +27,9 @@
 
 public class TestReverseStringFilter extends BaseTokenStreamTestCase {
   public void testFilter() throws Exception {
-    TokenStream stream = new WhitespaceTokenizer(
+    TokenStream stream = new WhitespaceTokenizer(Version.LUCENE_CURRENT, 
         new StringReader("Do have a nice day"));     // 1-4 length string
-    ReverseStringFilter filter = new ReverseStringFilter(stream);
+    ReverseStringFilter filter = new ReverseStringFilter(Version.LUCENE_CURRENT, stream);
     TermAttribute text = filter.getAttribute(TermAttribute.class);
     assertTrue(filter.incrementToken());
     assertEquals("oD", text.term());
@@ -45,9 +45,9 @@
   }
   
   public void testFilterWithMark() throws Exception {
-    TokenStream stream = new WhitespaceTokenizer(new StringReader(
+    TokenStream stream = new WhitespaceTokenizer(Version.LUCENE_CURRENT, new StringReader(
         "Do have a nice day")); // 1-4 length string
-    ReverseStringFilter filter = new ReverseStringFilter(stream, '\u0001');
+    ReverseStringFilter filter = new ReverseStringFilter(Version.LUCENE_CURRENT, stream, '\u0001');
     TermAttribute text = filter
         .getAttribute(TermAttribute.class);
     assertTrue(filter.incrementToken());
Index: contrib/analyzers/common/src/test/org/apache/lucene/analysis/payloads/DelimitedPayloadTokenFilterTest.java
===================================================================
--- contrib/analyzers/common/src/test/org/apache/lucene/analysis/payloads/DelimitedPayloadTokenFilterTest.java	(revision 916567)
+++ contrib/analyzers/common/src/test/org/apache/lucene/analysis/payloads/DelimitedPayloadTokenFilterTest.java	(working copy)
@@ -22,6 +22,7 @@
 import org.apache.lucene.analysis.tokenattributes.TermAttribute;
 import org.apache.lucene.index.Payload;
 import org.apache.lucene.util.LuceneTestCase;
+import org.apache.lucene.util.Version;
 
 import java.io.StringReader;
 
@@ -35,7 +36,7 @@
   public void testPayloads() throws Exception {
     String test = "The quick|JJ red|JJ fox|NN jumped|VB over the lazy|JJ brown|JJ dogs|NN";
     DelimitedPayloadTokenFilter filter = new DelimitedPayloadTokenFilter
-      (new WhitespaceTokenizer(new StringReader(test)), 
+      (new WhitespaceTokenizer(Version.LUCENE_CURRENT, new StringReader(test)), 
        DelimitedPayloadTokenFilter.DEFAULT_DELIMITER, new IdentityEncoder());
     TermAttribute termAtt = filter.getAttribute(TermAttribute.class);
     PayloadAttribute payAtt = filter.getAttribute(PayloadAttribute.class);
@@ -56,7 +57,7 @@
 
     String test = "The quick|JJ red|JJ fox|NN jumped|VB over the lazy|JJ brown|JJ dogs|NN";
     DelimitedPayloadTokenFilter filter = new DelimitedPayloadTokenFilter
-      (new WhitespaceTokenizer(new StringReader(test)), 
+      (new WhitespaceTokenizer(Version.LUCENE_CURRENT, new StringReader(test)), 
        DelimitedPayloadTokenFilter.DEFAULT_DELIMITER, new IdentityEncoder());
     assertTermEquals("The", filter, null);
     assertTermEquals("quick", filter, "JJ".getBytes("UTF-8"));
@@ -74,7 +75,7 @@
 
   public void testFloatEncoding() throws Exception {
     String test = "The quick|1.0 red|2.0 fox|3.5 jumped|0.5 over the lazy|5 brown|99.3 dogs|83.7";
-    DelimitedPayloadTokenFilter filter = new DelimitedPayloadTokenFilter(new WhitespaceTokenizer(new StringReader(test)), '|', new FloatEncoder());
+    DelimitedPayloadTokenFilter filter = new DelimitedPayloadTokenFilter(new WhitespaceTokenizer(Version.LUCENE_CURRENT, new StringReader(test)), '|', new FloatEncoder());
     TermAttribute termAtt = filter.getAttribute(TermAttribute.class);
     PayloadAttribute payAtt = filter.getAttribute(PayloadAttribute.class);
     assertTermEquals("The", filter, termAtt, payAtt, null);
@@ -92,7 +93,7 @@
 
   public void testIntEncoding() throws Exception {
     String test = "The quick|1 red|2 fox|3 jumped over the lazy|5 brown|99 dogs|83";
-    DelimitedPayloadTokenFilter filter = new DelimitedPayloadTokenFilter(new WhitespaceTokenizer(new StringReader(test)), '|', new IntegerEncoder());
+    DelimitedPayloadTokenFilter filter = new DelimitedPayloadTokenFilter(new WhitespaceTokenizer(Version.LUCENE_CURRENT, new StringReader(test)), '|', new IntegerEncoder());
     TermAttribute termAtt = filter.getAttribute(TermAttribute.class);
     PayloadAttribute payAtt = filter.getAttribute(PayloadAttribute.class);
     assertTermEquals("The", filter, termAtt, payAtt, null);
Index: contrib/analyzers/common/src/test/org/apache/lucene/analysis/payloads/NumericPayloadTokenFilterTest.java
===================================================================
--- contrib/analyzers/common/src/test/org/apache/lucene/analysis/payloads/NumericPayloadTokenFilterTest.java	(revision 916567)
+++ contrib/analyzers/common/src/test/org/apache/lucene/analysis/payloads/NumericPayloadTokenFilterTest.java	(working copy)
@@ -23,6 +23,7 @@
 import org.apache.lucene.analysis.tokenattributes.PayloadAttribute;
 import org.apache.lucene.analysis.tokenattributes.TermAttribute;
 import org.apache.lucene.analysis.tokenattributes.TypeAttribute;
+import org.apache.lucene.util.Version;
 
 import java.io.IOException;
 import java.io.StringReader;
@@ -37,7 +38,7 @@
   public void test() throws IOException {
     String test = "The quick red fox jumped over the lazy brown dogs";
 
-    NumericPayloadTokenFilter nptf = new NumericPayloadTokenFilter(new WordTokenFilter(new WhitespaceTokenizer(new StringReader(test))), 3, "D");
+    NumericPayloadTokenFilter nptf = new NumericPayloadTokenFilter(new WordTokenFilter(new WhitespaceTokenizer(Version.LUCENE_CURRENT, new StringReader(test))), 3, "D");
     boolean seenDogs = false;
     TermAttribute termAtt = nptf.getAttribute(TermAttribute.class);
     TypeAttribute typeAtt = nptf.getAttribute(TypeAttribute.class);
Index: contrib/analyzers/common/src/test/org/apache/lucene/analysis/payloads/TypeAsPayloadTokenFilterTest.java
===================================================================
--- contrib/analyzers/common/src/test/org/apache/lucene/analysis/payloads/TypeAsPayloadTokenFilterTest.java	(revision 916567)
+++ contrib/analyzers/common/src/test/org/apache/lucene/analysis/payloads/TypeAsPayloadTokenFilterTest.java	(working copy)
@@ -23,6 +23,7 @@
 import org.apache.lucene.analysis.tokenattributes.PayloadAttribute;
 import org.apache.lucene.analysis.tokenattributes.TermAttribute;
 import org.apache.lucene.analysis.tokenattributes.TypeAttribute;
+import org.apache.lucene.util.Version;
 
 import java.io.IOException;
 import java.io.StringReader;
@@ -37,7 +38,7 @@
   public void test() throws IOException {
     String test = "The quick red fox jumped over the lazy brown dogs";
 
-    TypeAsPayloadTokenFilter nptf = new TypeAsPayloadTokenFilter(new WordTokenFilter(new WhitespaceTokenizer(new StringReader(test))));
+    TypeAsPayloadTokenFilter nptf = new TypeAsPayloadTokenFilter(new WordTokenFilter(new WhitespaceTokenizer(Version.LUCENE_CURRENT, new StringReader(test))));
     int count = 0;
     TermAttribute termAtt = nptf.getAttribute(TermAttribute.class);
     TypeAttribute typeAtt = nptf.getAttribute(TypeAttribute.class);
Index: contrib/analyzers/common/src/test/org/apache/lucene/analysis/payloads/TokenOffsetPayloadTokenFilterTest.java
===================================================================
--- contrib/analyzers/common/src/test/org/apache/lucene/analysis/payloads/TokenOffsetPayloadTokenFilterTest.java	(revision 916567)
+++ contrib/analyzers/common/src/test/org/apache/lucene/analysis/payloads/TokenOffsetPayloadTokenFilterTest.java	(working copy)
@@ -21,6 +21,7 @@
 import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
 import org.apache.lucene.analysis.tokenattributes.PayloadAttribute;
 import org.apache.lucene.index.Payload;
+import org.apache.lucene.util.Version;
 
 import java.io.IOException;
 import java.io.StringReader;
@@ -35,7 +36,7 @@
   public void test() throws IOException {
     String test = "The quick red fox jumped over the lazy brown dogs";
 
-    TokenOffsetPayloadTokenFilter nptf = new TokenOffsetPayloadTokenFilter(new WhitespaceTokenizer(new StringReader(test)));
+    TokenOffsetPayloadTokenFilter nptf = new TokenOffsetPayloadTokenFilter(new WhitespaceTokenizer(Version.LUCENE_CURRENT, new StringReader(test)));
     int count = 0;
     PayloadAttribute payloadAtt = nptf.getAttribute(PayloadAttribute.class);
     OffsetAttribute offsetAtt = nptf.getAttribute(OffsetAttribute.class);

Property changes on: contrib\analyzers\common\src\test\org\apache\lucene\analysis\snowball
___________________________________________________________________
Added: svn:ignore
   + data


Index: contrib/analyzers/common/src/test/org/apache/lucene/analysis/tr/TestTurkishLowerCaseFilter.java
===================================================================
--- contrib/analyzers/common/src/test/org/apache/lucene/analysis/tr/TestTurkishLowerCaseFilter.java	(revision 916567)
+++ contrib/analyzers/common/src/test/org/apache/lucene/analysis/tr/TestTurkishLowerCaseFilter.java	(working copy)
@@ -22,6 +22,7 @@
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.util.Version;
 
 /**
  * Test the Turkish lowercase filter.
@@ -32,7 +33,7 @@
    * Test composed forms
    */
   public void testTurkishLowerCaseFilter() throws Exception {
-    TokenStream stream = new WhitespaceTokenizer(new StringReader(
+    TokenStream stream = new WhitespaceTokenizer(Version.LUCENE_CURRENT, new StringReader(
         "\u0130STANBUL \u0130ZM\u0130R ISPARTA"));
     TurkishLowerCaseFilter filter = new TurkishLowerCaseFilter(stream);
     assertTokenStreamContents(filter, new String[] {"istanbul", "izmir",
@@ -43,7 +44,7 @@
    * Test decomposed forms
    */
   public void testDecomposed() throws Exception {
-    TokenStream stream = new WhitespaceTokenizer(new StringReader(
+    TokenStream stream = new WhitespaceTokenizer(Version.LUCENE_CURRENT, new StringReader(
         "\u0049\u0307STANBUL \u0049\u0307ZM\u0049\u0307R ISPARTA"));
     TurkishLowerCaseFilter filter = new TurkishLowerCaseFilter(stream);
     assertTokenStreamContents(filter, new String[] {"istanbul", "izmir",
@@ -56,7 +57,7 @@
    * to U+0130 + U+0316, and is lowercased the same way.
    */
   public void testDecomposed2() throws Exception {
-    TokenStream stream = new WhitespaceTokenizer(new StringReader(
+    TokenStream stream = new WhitespaceTokenizer(Version.LUCENE_CURRENT, new StringReader(
         "\u0049\u0316\u0307STANBUL \u0049\u0307ZM\u0049\u0307R I\u0316SPARTA"));
     TurkishLowerCaseFilter filter = new TurkishLowerCaseFilter(stream);
     assertTokenStreamContents(filter, new String[] {"i\u0316stanbul", "izmir",
Index: contrib/analyzers/common/src/test/org/apache/lucene/analysis/ru/TestRussianAnalyzer.java
===================================================================
--- contrib/analyzers/common/src/test/org/apache/lucene/analysis/ru/TestRussianAnalyzer.java	(revision 916567)
+++ contrib/analyzers/common/src/test/org/apache/lucene/analysis/ru/TestRussianAnalyzer.java	(working copy)
@@ -26,6 +26,7 @@
 
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.analysis.CharArraySet;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.tokenattributes.TermAttribute;
 import org.apache.lucene.util.Version;
@@ -49,9 +50,14 @@
       dataDir = new File(System.getProperty("dataDir", "./bin"));
     }
 
-    public void testUnicode() throws IOException
+    /**
+     * @deprecated remove this test and its datafiles in Lucene 4.0
+     * the Snowball version has its own data tests.
+     */
+    @Deprecated
+    public void testUnicode30() throws IOException
     {
-        RussianAnalyzer ra = new RussianAnalyzer(Version.LUCENE_CURRENT);
+        RussianAnalyzer ra = new RussianAnalyzer(Version.LUCENE_30);
         inWords =
             new InputStreamReader(
                 new FileInputStream(new File(dataDir, "/org/apache/lucene/analysis/ru/testUTF8.txt")),
@@ -65,7 +71,7 @@
         TokenStream in = ra.tokenStream("all", inWords);
 
         RussianLetterTokenizer sample =
-            new RussianLetterTokenizer(
+            new RussianLetterTokenizer(Version.LUCENE_CURRENT,
                 sampleUnicode);
 
         TermAttribute text = in.getAttribute(TermAttribute.class);
@@ -109,11 +115,31 @@
         }
     }
     
+    /** @deprecated remove this test in Lucene 4.0: stopwords changed */
+    @Deprecated
+    public void testReusableTokenStream30() throws Exception {
+      Analyzer a = new RussianAnalyzer(Version.LUCENE_30);
+      assertAnalyzesToReuse(a, "Вместе с тем о силе электромагнитной энергии имели представление еще",
+          new String[] { "вмест", "сил", "электромагнитн", "энерг", "имел", "представлен" });
+      assertAnalyzesToReuse(a, "Но знание это хранилось в тайне",
+          new String[] { "знан", "хран", "тайн" });
+    }
+    
     public void testReusableTokenStream() throws Exception {
       Analyzer a = new RussianAnalyzer(Version.LUCENE_CURRENT);
       assertAnalyzesToReuse(a, "Вместе с тем о силе электромагнитной энергии имели представление еще",
           new String[] { "вмест", "сил", "электромагнитн", "энерг", "имел", "представлен" });
       assertAnalyzesToReuse(a, "Но знание это хранилось в тайне",
-          new String[] { "знан", "хран", "тайн" });
+          new String[] { "знан", "эт", "хран", "тайн" });
     }
+    
+    
+    public void testWithStemExclusionSet() throws Exception {
+      CharArraySet set = new CharArraySet(Version.LUCENE_CURRENT, 1, true);
+      set.add("представление");
+      Analyzer a = new RussianAnalyzer(Version.LUCENE_CURRENT, RussianAnalyzer.getDefaultStopSet() , set);
+      assertAnalyzesToReuse(a, "Вместе с тем о силе электромагнитной энергии имели представление еще",
+          new String[] { "вмест", "сил", "электромагнитн", "энерг", "имел", "представление" });
+     
+    }
 }
Index: contrib/analyzers/common/src/test/org/apache/lucene/analysis/ru/TestRussianStem.java
===================================================================
--- contrib/analyzers/common/src/test/org/apache/lucene/analysis/ru/TestRussianStem.java	(revision 916567)
+++ contrib/analyzers/common/src/test/org/apache/lucene/analysis/ru/TestRussianStem.java	(working copy)
@@ -24,6 +24,10 @@
 import java.io.FileInputStream;
 import java.util.ArrayList;
 
+/**
+ * @deprecated Remove this test class (and its datafiles!) in Lucene 4.0
+ */
+@Deprecated
 public class TestRussianStem extends LuceneTestCase
 {
     private ArrayList words = new ArrayList();
Index: contrib/analyzers/common/src/test/org/apache/lucene/analysis/sinks/TokenTypeSinkTokenizerTest.java
===================================================================
--- contrib/analyzers/common/src/test/org/apache/lucene/analysis/sinks/TokenTypeSinkTokenizerTest.java	(revision 916567)
+++ contrib/analyzers/common/src/test/org/apache/lucene/analysis/sinks/TokenTypeSinkTokenizerTest.java	(working copy)
@@ -27,6 +27,7 @@
 import org.apache.lucene.analysis.TeeSinkTokenFilter.SinkTokenStream;
 import org.apache.lucene.analysis.tokenattributes.TermAttribute;
 import org.apache.lucene.analysis.tokenattributes.TypeAttribute;
+import org.apache.lucene.util.Version;
 
 public class TokenTypeSinkTokenizerTest extends BaseTokenStreamTestCase {
 
@@ -39,7 +40,7 @@
     TokenTypeSinkFilter sinkFilter = new TokenTypeSinkFilter("D");
     String test = "The quick red fox jumped over the lazy brown dogs";
 
-    TeeSinkTokenFilter ttf = new TeeSinkTokenFilter(new WordTokenFilter(new WhitespaceTokenizer(new StringReader(test))));
+    TeeSinkTokenFilter ttf = new TeeSinkTokenFilter(new WordTokenFilter(new WhitespaceTokenizer(Version.LUCENE_CURRENT, new StringReader(test))));
     SinkTokenStream sink = ttf.newSinkTokenStream(sinkFilter);
     
     boolean seenDogs = false;
Index: contrib/analyzers/common/src/test/org/apache/lucene/analysis/sinks/DateRecognizerSinkTokenizerTest.java
===================================================================
--- contrib/analyzers/common/src/test/org/apache/lucene/analysis/sinks/DateRecognizerSinkTokenizerTest.java	(revision 916567)
+++ contrib/analyzers/common/src/test/org/apache/lucene/analysis/sinks/DateRecognizerSinkTokenizerTest.java	(working copy)
@@ -25,6 +25,7 @@
 import org.apache.lucene.analysis.TeeSinkTokenFilter;
 import org.apache.lucene.analysis.WhitespaceTokenizer;
 import org.apache.lucene.analysis.TeeSinkTokenFilter.SinkTokenStream;
+import org.apache.lucene.util.Version;
 
 public class DateRecognizerSinkTokenizerTest extends BaseTokenStreamTestCase {
 
@@ -36,7 +37,7 @@
   public void test() throws IOException {
     DateRecognizerSinkFilter sinkFilter = new DateRecognizerSinkFilter(new SimpleDateFormat("MM/dd/yyyy", Locale.US));
     String test = "The quick red fox jumped over the lazy brown dogs on 7/11/2006  The dogs finally reacted on 7/12/2006";
-    TeeSinkTokenFilter tee = new TeeSinkTokenFilter(new WhitespaceTokenizer(new StringReader(test)));
+    TeeSinkTokenFilter tee = new TeeSinkTokenFilter(new WhitespaceTokenizer(Version.LUCENE_CURRENT, new StringReader(test)));
     SinkTokenStream sink = tee.newSinkTokenStream(sinkFilter);
     int count = 0;
     
Index: contrib/analyzers/common/src/test/org/apache/lucene/analysis/sinks/TokenRangeSinkTokenizerTest.java
===================================================================
--- contrib/analyzers/common/src/test/org/apache/lucene/analysis/sinks/TokenRangeSinkTokenizerTest.java	(revision 916567)
+++ contrib/analyzers/common/src/test/org/apache/lucene/analysis/sinks/TokenRangeSinkTokenizerTest.java	(working copy)
@@ -23,6 +23,7 @@
 import org.apache.lucene.analysis.TeeSinkTokenFilter;
 import org.apache.lucene.analysis.WhitespaceTokenizer;
 import org.apache.lucene.analysis.TeeSinkTokenFilter.SinkTokenStream;
+import org.apache.lucene.util.Version;
 
 public class TokenRangeSinkTokenizerTest extends BaseTokenStreamTestCase {
 
@@ -34,7 +35,7 @@
   public void test() throws IOException {
     TokenRangeSinkFilter sinkFilter = new TokenRangeSinkFilter(2, 4);
     String test = "The quick red fox jumped over the lazy brown dogs";
-    TeeSinkTokenFilter tee = new TeeSinkTokenFilter(new WhitespaceTokenizer(new StringReader(test)));
+    TeeSinkTokenFilter tee = new TeeSinkTokenFilter(new WhitespaceTokenizer(Version.LUCENE_CURRENT, new StringReader(test)));
     SinkTokenStream rangeToks = tee.newSinkTokenStream(sinkFilter);
     
     int count = 0;
Index: contrib/analyzers/common/src/test/org/apache/lucene/analysis/shingle/ShingleFilterTest.java
===================================================================
--- contrib/analyzers/common/src/test/org/apache/lucene/analysis/shingle/ShingleFilterTest.java	(revision 916567)
+++ contrib/analyzers/common/src/test/org/apache/lucene/analysis/shingle/ShingleFilterTest.java	(working copy)
@@ -26,6 +26,7 @@
 import org.apache.lucene.analysis.Tokenizer;
 import org.apache.lucene.analysis.WhitespaceTokenizer;
 import org.apache.lucene.analysis.tokenattributes.*;
+import org.apache.lucene.util.Version;
 
 public class ShingleFilterTest extends BaseTokenStreamTestCase {
 
@@ -288,7 +289,360 @@
 
   };
 
+  public static final Token[] TRI_GRAM_TOKENS_MIN_TRI_GRAM = new Token[] {
+    createToken("please", 0, 6),
+    createToken("please divide this", 0, 18),
+    createToken("divide", 7, 13),
+    createToken("divide this sentence", 7, 27),
+    createToken("this", 14, 18),
+    createToken("this sentence into", 14, 32),
+    createToken("sentence", 19, 27),
+    createToken("sentence into shingles", 19, 39),
+    createToken("into", 28, 32),
+    createToken("shingles", 33, 39)
+  };
 
+  public static final int[] TRI_GRAM_POSITION_INCREMENTS_MIN_TRI_GRAM = new int[] {
+    1, 0, 1, 0, 1, 0, 1, 0, 1, 1
+  };
+
+  public static final String[] TRI_GRAM_TYPES_MIN_TRI_GRAM = new String[] {
+    "word", "shingle",
+    "word", "shingle",
+    "word", "shingle",
+    "word", "shingle",
+    "word",
+    "word"
+  };
+  
+  public static final Token[] TRI_GRAM_TOKENS_WITHOUT_UNIGRAMS_MIN_TRI_GRAM = new Token[] {
+    createToken("please divide this", 0, 18),
+    createToken("divide this sentence", 7, 27),
+    createToken("this sentence into", 14, 32),
+    createToken("sentence into shingles", 19, 39)
+  };
+
+  public static final int[] TRI_GRAM_POSITION_INCREMENTS_WITHOUT_UNIGRAMS_MIN_TRI_GRAM = new int[] {
+    1, 1, 1, 1
+  };
+  
+  public static final String[] TRI_GRAM_TYPES_WITHOUT_UNIGRAMS_MIN_TRI_GRAM = new String[] {
+    "shingle",
+    "shingle",
+    "shingle",
+    "shingle"
+  };
+  
+  public static final Token[] FOUR_GRAM_TOKENS_MIN_TRI_GRAM = new Token[] {
+    createToken("please", 0, 6),
+    createToken("please divide this", 0, 18),
+    createToken("please divide this sentence", 0, 27),
+    createToken("divide", 7, 13),
+    createToken("divide this sentence", 7, 27),
+    createToken("divide this sentence into", 7, 32),
+    createToken("this", 14, 18),
+    createToken("this sentence into", 14, 32),
+    createToken("this sentence into shingles", 14, 39),
+    createToken("sentence", 19, 27),
+    createToken("sentence into shingles", 19, 39),
+    createToken("into", 28, 32),
+    createToken("shingles", 33, 39)
+  };
+
+  public static final int[] FOUR_GRAM_POSITION_INCREMENTS_MIN_TRI_GRAM = new int[] {
+    1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1
+  };
+
+  public static final String[] FOUR_GRAM_TYPES_MIN_TRI_GRAM = new String[] {
+    "word", "shingle", "shingle",
+    "word", "shingle", "shingle",
+    "word", "shingle", "shingle",
+    "word", "shingle",
+    "word",
+    "word"
+  };
+  
+  public static final Token[] FOUR_GRAM_TOKENS_WITHOUT_UNIGRAMS_MIN_TRI_GRAM = new Token[] {
+    createToken("please divide this", 0, 18),
+    createToken("please divide this sentence", 0, 27),
+    createToken("divide this sentence", 7, 27),
+    createToken("divide this sentence into", 7, 32),
+    createToken("this sentence into", 14, 32),
+    createToken("this sentence into shingles", 14, 39),
+    createToken("sentence into shingles", 19, 39),
+  };
+
+  public static final int[] FOUR_GRAM_POSITION_INCREMENTS_WITHOUT_UNIGRAMS_MIN_TRI_GRAM = new int[] {
+    1, 0, 1, 0, 1, 0, 1
+  };
+  
+  public static final String[] FOUR_GRAM_TYPES_WITHOUT_UNIGRAMS_MIN_TRI_GRAM = new String[] {
+    "shingle", "shingle",
+    "shingle", "shingle",
+    "shingle", "shingle",
+    "shingle"
+  };
+  
+  public static final Token[] FOUR_GRAM_TOKENS_MIN_FOUR_GRAM = new Token[] {
+    createToken("please", 0, 6),
+    createToken("please divide this sentence", 0, 27),
+    createToken("divide", 7, 13),
+    createToken("divide this sentence into", 7, 32),
+    createToken("this", 14, 18),
+    createToken("this sentence into shingles", 14, 39),
+    createToken("sentence", 19, 27),
+    createToken("into", 28, 32),
+    createToken("shingles", 33, 39)
+  };
+
+  public static final int[] FOUR_GRAM_POSITION_INCREMENTS_MIN_FOUR_GRAM = new int[] {
+    1, 0, 1, 0, 1, 0, 1, 1, 1
+  };
+
+  public static final String[] FOUR_GRAM_TYPES_MIN_FOUR_GRAM = new String[] {
+    "word", "shingle",
+    "word", "shingle",
+    "word", "shingle",
+    "word",
+    "word",
+    "word"
+  };
+  
+  public static final Token[] FOUR_GRAM_TOKENS_WITHOUT_UNIGRAMS_MIN_FOUR_GRAM = new Token[] {
+    createToken("please divide this sentence", 0, 27),
+    createToken("divide this sentence into", 7, 32),
+    createToken("this sentence into shingles", 14, 39),
+  };
+
+  public static final int[] FOUR_GRAM_POSITION_INCREMENTS_WITHOUT_UNIGRAMS_MIN_FOUR_GRAM = new int[] {
+    1, 1, 1
+  };
+  
+  public static final String[] FOUR_GRAM_TYPES_WITHOUT_UNIGRAMS_MIN_FOUR_GRAM = new String[] {
+    "shingle",
+    "shingle",
+    "shingle"
+  };
+
+  public static final Token[] BI_GRAM_TOKENS_NO_SEPARATOR = new Token[] {
+    createToken("please", 0, 6),
+    createToken("pleasedivide", 0, 13),
+    createToken("divide", 7, 13),
+    createToken("dividethis", 7, 18),
+    createToken("this", 14, 18),
+    createToken("thissentence", 14, 27),
+    createToken("sentence", 19, 27),
+    createToken("sentenceinto", 19, 32),
+    createToken("into", 28, 32),
+    createToken("intoshingles", 28, 39),
+    createToken("shingles", 33, 39),
+  };
+
+  public static final int[] BI_GRAM_POSITION_INCREMENTS_NO_SEPARATOR = new int[] {
+    1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1
+  };
+
+  public static final String[] BI_GRAM_TYPES_NO_SEPARATOR = new String[] {
+    "word", "shingle", "word", "shingle", "word", "shingle", "word",
+    "shingle", "word", "shingle", "word"
+  };
+
+  public static final Token[] BI_GRAM_TOKENS_WITHOUT_UNIGRAMS_NO_SEPARATOR = new Token[] {
+    createToken("pleasedivide", 0, 13),
+    createToken("dividethis", 7, 18),
+    createToken("thissentence", 14, 27),
+    createToken("sentenceinto", 19, 32),
+    createToken("intoshingles", 28, 39),
+  };
+
+  public static final int[] BI_GRAM_POSITION_INCREMENTS_WITHOUT_UNIGRAMS_NO_SEPARATOR = new int[] {
+    1, 1, 1, 1, 1
+  };
+
+  public static final String[] BI_GRAM_TYPES_WITHOUT_UNIGRAMS_NO_SEPARATOR = new String[] {
+    "shingle", "shingle", "shingle", "shingle", "shingle"
+  };
+  
+  public static final Token[] TRI_GRAM_TOKENS_NO_SEPARATOR = new Token[] {
+    createToken("please", 0, 6),
+    createToken("pleasedivide", 0, 13),
+    createToken("pleasedividethis", 0, 18),
+    createToken("divide", 7, 13),
+    createToken("dividethis", 7, 18),
+    createToken("dividethissentence", 7, 27),
+    createToken("this", 14, 18),
+    createToken("thissentence", 14, 27),
+    createToken("thissentenceinto", 14, 32),
+    createToken("sentence", 19, 27),
+    createToken("sentenceinto", 19, 32),
+    createToken("sentenceintoshingles", 19, 39),
+    createToken("into", 28, 32),
+    createToken("intoshingles", 28, 39),
+    createToken("shingles", 33, 39)
+  };
+
+  public static final int[] TRI_GRAM_POSITION_INCREMENTS_NO_SEPARATOR = new int[] {
+    1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1
+  };
+
+  public static final String[] TRI_GRAM_TYPES_NO_SEPARATOR = new String[] {
+    "word", "shingle", "shingle",
+    "word", "shingle", "shingle",
+    "word", "shingle", "shingle",
+    "word", "shingle", "shingle",
+    "word", "shingle",
+    "word"
+  };
+  
+  public static final Token[] TRI_GRAM_TOKENS_WITHOUT_UNIGRAMS_NO_SEPARATOR = new Token[] {
+    createToken("pleasedivide", 0, 13),
+    createToken("pleasedividethis", 0, 18),
+    createToken("dividethis", 7, 18),
+    createToken("dividethissentence", 7, 27),
+    createToken("thissentence", 14, 27),
+    createToken("thissentenceinto", 14, 32),
+    createToken("sentenceinto", 19, 32),
+    createToken("sentenceintoshingles", 19, 39),
+    createToken("intoshingles", 28, 39),
+  };
+
+  public static final int[] TRI_GRAM_POSITION_INCREMENTS_WITHOUT_UNIGRAMS_NO_SEPARATOR = new int[] {
+    1, 0, 1, 0, 1, 0, 1, 0, 1
+  };
+  
+  public static final String[] TRI_GRAM_TYPES_WITHOUT_UNIGRAMS_NO_SEPARATOR = new String[] {
+    "shingle", "shingle",
+    "shingle", "shingle",
+    "shingle", "shingle",
+    "shingle", "shingle",
+    "shingle",
+  };
+
+  public static final Token[] BI_GRAM_TOKENS_ALT_SEPARATOR = new Token[] {
+    createToken("please", 0, 6),
+    createToken("please<SEP>divide", 0, 13),
+    createToken("divide", 7, 13),
+    createToken("divide<SEP>this", 7, 18),
+    createToken("this", 14, 18),
+    createToken("this<SEP>sentence", 14, 27),
+    createToken("sentence", 19, 27),
+    createToken("sentence<SEP>into", 19, 32),
+    createToken("into", 28, 32),
+    createToken("into<SEP>shingles", 28, 39),
+    createToken("shingles", 33, 39),
+  };
+
+  public static final int[] BI_GRAM_POSITION_INCREMENTS_ALT_SEPARATOR = new int[] {
+    1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1
+  };
+
+  public static final String[] BI_GRAM_TYPES_ALT_SEPARATOR = new String[] {
+    "word", "shingle", "word", "shingle", "word", "shingle", "word",
+    "shingle", "word", "shingle", "word"
+  };
+
+  public static final Token[] BI_GRAM_TOKENS_WITHOUT_UNIGRAMS_ALT_SEPARATOR = new Token[] {
+    createToken("please<SEP>divide", 0, 13),
+    createToken("divide<SEP>this", 7, 18),
+    createToken("this<SEP>sentence", 14, 27),
+    createToken("sentence<SEP>into", 19, 32),
+    createToken("into<SEP>shingles", 28, 39),
+  };
+
+  public static final int[] BI_GRAM_POSITION_INCREMENTS_WITHOUT_UNIGRAMS_ALT_SEPARATOR = new int[] {
+    1, 1, 1, 1, 1
+  };
+
+  public static final String[] BI_GRAM_TYPES_WITHOUT_UNIGRAMS_ALT_SEPARATOR = new String[] {
+    "shingle", "shingle", "shingle", "shingle", "shingle"
+  };
+  
+  public static final Token[] TRI_GRAM_TOKENS_ALT_SEPARATOR = new Token[] {
+    createToken("please", 0, 6),
+    createToken("please<SEP>divide", 0, 13),
+    createToken("please<SEP>divide<SEP>this", 0, 18),
+    createToken("divide", 7, 13),
+    createToken("divide<SEP>this", 7, 18),
+    createToken("divide<SEP>this<SEP>sentence", 7, 27),
+    createToken("this", 14, 18),
+    createToken("this<SEP>sentence", 14, 27),
+    createToken("this<SEP>sentence<SEP>into", 14, 32),
+    createToken("sentence", 19, 27),
+    createToken("sentence<SEP>into", 19, 32),
+    createToken("sentence<SEP>into<SEP>shingles", 19, 39),
+    createToken("into", 28, 32),
+    createToken("into<SEP>shingles", 28, 39),
+    createToken("shingles", 33, 39)
+  };
+
+  public static final int[] TRI_GRAM_POSITION_INCREMENTS_ALT_SEPARATOR = new int[] {
+    1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1
+  };
+
+  public static final String[] TRI_GRAM_TYPES_ALT_SEPARATOR = new String[] {
+    "word", "shingle", "shingle",
+    "word", "shingle", "shingle",
+    "word", "shingle", "shingle",
+    "word", "shingle", "shingle",
+    "word", "shingle",
+    "word"
+  };
+  
+  public static final Token[] TRI_GRAM_TOKENS_WITHOUT_UNIGRAMS_ALT_SEPARATOR = new Token[] {
+    createToken("please<SEP>divide", 0, 13),
+    createToken("please<SEP>divide<SEP>this", 0, 18),
+    createToken("divide<SEP>this", 7, 18),
+    createToken("divide<SEP>this<SEP>sentence", 7, 27),
+    createToken("this<SEP>sentence", 14, 27),
+    createToken("this<SEP>sentence<SEP>into", 14, 32),
+    createToken("sentence<SEP>into", 19, 32),
+    createToken("sentence<SEP>into<SEP>shingles", 19, 39),
+    createToken("into<SEP>shingles", 28, 39),
+  };
+
+  public static final int[] TRI_GRAM_POSITION_INCREMENTS_WITHOUT_UNIGRAMS_ALT_SEPARATOR = new int[] {
+    1, 0, 1, 0, 1, 0, 1, 0, 1
+  };
+  
+  public static final String[] TRI_GRAM_TYPES_WITHOUT_UNIGRAMS_ALT_SEPARATOR = new String[] {
+    "shingle", "shingle",
+    "shingle", "shingle",
+    "shingle", "shingle",
+    "shingle", "shingle",
+    "shingle",
+  };
+
+  public static final Token[] TRI_GRAM_TOKENS_NULL_SEPARATOR = new Token[] {
+    createToken("please", 0, 6),
+    createToken("pleasedivide", 0, 13),
+    createToken("pleasedividethis", 0, 18),
+    createToken("divide", 7, 13),
+    createToken("dividethis", 7, 18),
+    createToken("dividethissentence", 7, 27),
+    createToken("this", 14, 18),
+    createToken("thissentence", 14, 27),
+    createToken("thissentenceinto", 14, 32),
+    createToken("sentence", 19, 27),
+    createToken("sentenceinto", 19, 32),
+    createToken("sentenceintoshingles", 19, 39),
+    createToken("into", 28, 32),
+    createToken("intoshingles", 28, 39),
+    createToken("shingles", 33, 39)
+  };
+
+  public static final int[] TRI_GRAM_POSITION_INCREMENTS_NULL_SEPARATOR = new int[] {
+    1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1
+  };
+
+  public static final String[] TRI_GRAM_TYPES_NULL_SEPARATOR = new String[] {
+    "word", "shingle", "shingle",
+    "word", "shingle", "shingle",
+    "word", "shingle", "shingle",
+    "word", "shingle", "shingle",
+    "word", "shingle",
+    "word"
+  };
+  
   @Override
   protected void setUp() throws Exception {
     super.setUp();
@@ -379,8 +733,110 @@
   }
   
   
+  public void testTriGramFilterMinTriGram() throws IOException {
+    this.shingleFilterTest(3, 3, TEST_TOKEN, TRI_GRAM_TOKENS_MIN_TRI_GRAM,
+                           TRI_GRAM_POSITION_INCREMENTS_MIN_TRI_GRAM,
+                           TRI_GRAM_TYPES_MIN_TRI_GRAM,
+                           true);
+  }
+  
+  public void testTriGramFilterWithoutUnigramsMinTriGram() throws IOException {
+    this.shingleFilterTest(3, 3, TEST_TOKEN, 
+                           TRI_GRAM_TOKENS_WITHOUT_UNIGRAMS_MIN_TRI_GRAM,
+                           TRI_GRAM_POSITION_INCREMENTS_WITHOUT_UNIGRAMS_MIN_TRI_GRAM, 
+                           TRI_GRAM_TYPES_WITHOUT_UNIGRAMS_MIN_TRI_GRAM,
+                           false);
+  }
+  
+  public void testFourGramFilterMinTriGram() throws IOException {
+    this.shingleFilterTest(3, 4, TEST_TOKEN, FOUR_GRAM_TOKENS_MIN_TRI_GRAM,
+                           FOUR_GRAM_POSITION_INCREMENTS_MIN_TRI_GRAM, 
+                           FOUR_GRAM_TYPES_MIN_TRI_GRAM,
+                           true);
+  }
+  
+  public void testFourGramFilterWithoutUnigramsMinTriGram() throws IOException {
+    this.shingleFilterTest(3, 4, TEST_TOKEN, 
+                           FOUR_GRAM_TOKENS_WITHOUT_UNIGRAMS_MIN_TRI_GRAM,
+                           FOUR_GRAM_POSITION_INCREMENTS_WITHOUT_UNIGRAMS_MIN_TRI_GRAM,
+                           FOUR_GRAM_TYPES_WITHOUT_UNIGRAMS_MIN_TRI_GRAM, false);
+  }
+
+  public void testFourGramFilterMinFourGram() throws IOException {
+    this.shingleFilterTest(4, 4, TEST_TOKEN, FOUR_GRAM_TOKENS_MIN_FOUR_GRAM,
+                           FOUR_GRAM_POSITION_INCREMENTS_MIN_FOUR_GRAM, 
+                           FOUR_GRAM_TYPES_MIN_FOUR_GRAM,
+                           true);
+  }
+  
+  public void testFourGramFilterWithoutUnigramsMinFourGram() throws IOException {
+    this.shingleFilterTest(4, 4, TEST_TOKEN, 
+                           FOUR_GRAM_TOKENS_WITHOUT_UNIGRAMS_MIN_FOUR_GRAM,
+                           FOUR_GRAM_POSITION_INCREMENTS_WITHOUT_UNIGRAMS_MIN_FOUR_GRAM,
+                           FOUR_GRAM_TYPES_WITHOUT_UNIGRAMS_MIN_FOUR_GRAM, false);
+  }
+ 
+  public void testBiGramFilterNoSeparator() throws IOException {
+    this.shingleFilterTest("", 2, 2, TEST_TOKEN, BI_GRAM_TOKENS_NO_SEPARATOR,
+                           BI_GRAM_POSITION_INCREMENTS_NO_SEPARATOR, 
+                           BI_GRAM_TYPES_NO_SEPARATOR, true);
+  }
+
+  public void testBiGramFilterWithoutUnigramsNoSeparator() throws IOException {
+    this.shingleFilterTest("", 2, 2, TEST_TOKEN, 
+                           BI_GRAM_TOKENS_WITHOUT_UNIGRAMS_NO_SEPARATOR,
+                           BI_GRAM_POSITION_INCREMENTS_WITHOUT_UNIGRAMS_NO_SEPARATOR, 
+                           BI_GRAM_TYPES_WITHOUT_UNIGRAMS_NO_SEPARATOR,
+                           false);
+  }
+  public void testTriGramFilterNoSeparator() throws IOException {
+    this.shingleFilterTest("", 2, 3, TEST_TOKEN, TRI_GRAM_TOKENS_NO_SEPARATOR,
+                           TRI_GRAM_POSITION_INCREMENTS_NO_SEPARATOR, 
+                           TRI_GRAM_TYPES_NO_SEPARATOR, true);
+  }
+  
+  public void testTriGramFilterWithoutUnigramsNoSeparator() throws IOException {
+    this.shingleFilterTest("", 2, 3, TEST_TOKEN, 
+                           TRI_GRAM_TOKENS_WITHOUT_UNIGRAMS_NO_SEPARATOR,
+                           TRI_GRAM_POSITION_INCREMENTS_WITHOUT_UNIGRAMS_NO_SEPARATOR,
+                           TRI_GRAM_TYPES_WITHOUT_UNIGRAMS_NO_SEPARATOR, false);
+  }
+  
+  public void testBiGramFilterAltSeparator() throws IOException {
+    this.shingleFilterTest("<SEP>", 2, 2, TEST_TOKEN, BI_GRAM_TOKENS_ALT_SEPARATOR,
+                           BI_GRAM_POSITION_INCREMENTS_ALT_SEPARATOR, 
+                           BI_GRAM_TYPES_ALT_SEPARATOR, true);
+  }
+
+  public void testBiGramFilterWithoutUnigramsAltSeparator() throws IOException {
+    this.shingleFilterTest("<SEP>", 2, 2, TEST_TOKEN, 
+                           BI_GRAM_TOKENS_WITHOUT_UNIGRAMS_ALT_SEPARATOR,
+                           BI_GRAM_POSITION_INCREMENTS_WITHOUT_UNIGRAMS_ALT_SEPARATOR, 
+                           BI_GRAM_TYPES_WITHOUT_UNIGRAMS_ALT_SEPARATOR,
+                           false);
+  }
+  public void testTriGramFilterAltSeparator() throws IOException {
+    this.shingleFilterTest("<SEP>", 2, 3, TEST_TOKEN, TRI_GRAM_TOKENS_ALT_SEPARATOR,
+                           TRI_GRAM_POSITION_INCREMENTS_ALT_SEPARATOR, 
+                           TRI_GRAM_TYPES_ALT_SEPARATOR, true);
+  }
+  
+  public void testTriGramFilterWithoutUnigramsAltSeparator() throws IOException {
+    this.shingleFilterTest("<SEP>", 2, 3, TEST_TOKEN, 
+                           TRI_GRAM_TOKENS_WITHOUT_UNIGRAMS_ALT_SEPARATOR,
+                           TRI_GRAM_POSITION_INCREMENTS_WITHOUT_UNIGRAMS_ALT_SEPARATOR,
+                           TRI_GRAM_TYPES_WITHOUT_UNIGRAMS_ALT_SEPARATOR, false);
+  }
+
+  public void testTriGramFilterNullSeparator() throws IOException {
+    this.shingleFilterTest(null, 2, 3, TEST_TOKEN, TRI_GRAM_TOKENS_NULL_SEPARATOR,
+                           TRI_GRAM_POSITION_INCREMENTS_NULL_SEPARATOR, 
+                           TRI_GRAM_TYPES_NULL_SEPARATOR, true);
+  }
+  
+  
   public void testReset() throws Exception {
-    Tokenizer wsTokenizer = new WhitespaceTokenizer(new StringReader("please divide this sentence"));
+    Tokenizer wsTokenizer = new WhitespaceTokenizer(Version.LUCENE_CURRENT, new StringReader("please divide this sentence"));
     TokenStream filter = new ShingleFilter(wsTokenizer, 2);
     assertTokenStreamContents(filter,
       new String[]{"please","please divide","divide","divide this","this","this sentence","sentence"},
@@ -403,30 +859,50 @@
     throws IOException {
 
     ShingleFilter filter = new ShingleFilter(new TestTokenStream(tokensToShingle), maxSize);
+    shingleFilterTestCommon
+      (filter, tokensToCompare, positionIncrements, types, outputUnigrams);
+  }
+
+  protected void shingleFilterTest(int minSize, int maxSize, Token[] tokensToShingle, 
+                                   Token[] tokensToCompare, int[] positionIncrements,
+                                   String[] types, boolean outputUnigrams)
+    throws IOException {
+    ShingleFilter filter 
+      = new ShingleFilter(new TestTokenStream(tokensToShingle), minSize, maxSize);
+    shingleFilterTestCommon
+      (filter, tokensToCompare, positionIncrements, types, outputUnigrams);
+  }
+
+  protected void shingleFilterTest(String tokenSeparator, int minSize, int maxSize, Token[] tokensToShingle, 
+                                   Token[] tokensToCompare, int[] positionIncrements,
+                                   String[] types, boolean outputUnigrams)
+    throws IOException {
+    ShingleFilter filter 
+      = new ShingleFilter(new TestTokenStream(tokensToShingle), minSize, maxSize);
+    filter.setTokenSeparator(tokenSeparator);
+    shingleFilterTestCommon
+      (filter, tokensToCompare, positionIncrements, types, outputUnigrams);
+  }
+
+  protected void shingleFilterTestCommon(ShingleFilter filter,
+                                         Token[] tokensToCompare,
+                                         int[] positionIncrements,
+                                         String[] types, boolean outputUnigrams)
+    throws IOException {
+
     filter.setOutputUnigrams(outputUnigrams);
 
-    TermAttribute termAtt = filter.addAttribute(TermAttribute.class);
-    OffsetAttribute offsetAtt = filter.addAttribute(OffsetAttribute.class);
-    PositionIncrementAttribute posIncrAtt = filter.addAttribute(PositionIncrementAttribute.class);
-    TypeAttribute typeAtt = filter.addAttribute(TypeAttribute.class);
-
-    int i = 0;
-    while (filter.incrementToken()) {
-      assertTrue("ShingleFilter outputted more tokens than expected", i < tokensToCompare.length);
-      String termText = termAtt.term();
-      String goldText = tokensToCompare[i].term();
-      assertEquals("Wrong termText", goldText, termText);
-      assertEquals("Wrong startOffset for token \"" + termText + "\"",
-          tokensToCompare[i].startOffset(), offsetAtt.startOffset());
-      assertEquals("Wrong endOffset for token \"" + termText + "\"",
-          tokensToCompare[i].endOffset(), offsetAtt.endOffset());
-      assertEquals("Wrong positionIncrement for token \"" + termText + "\"",
-          positionIncrements[i], posIncrAtt.getPositionIncrement());
-      assertEquals("Wrong type for token \"" + termText + "\"", types[i], typeAtt.type());
-      i++;
+    String text[] = new String[tokensToCompare.length];
+    int startOffsets[] = new int[tokensToCompare.length];
+    int endOffsets[] = new int[tokensToCompare.length];
+    
+    for (int i = 0; i < tokensToCompare.length; i++) {
+      text[i] = tokensToCompare[i].term();
+      startOffsets[i] = tokensToCompare[i].startOffset();
+      endOffsets[i] = tokensToCompare[i].endOffset();
     }
-    assertEquals("ShingleFilter outputted wrong # of tokens. (# output = " + i + "; # expected =" + tokensToCompare.length + ")",
-                 tokensToCompare.length, i);
+    
+    assertTokenStreamContents(filter, text, startOffsets, endOffsets, types, positionIncrements);
   }
 
   private static Token createToken(String term, int start, int offset)
Index: contrib/analyzers/common/src/test/org/apache/lucene/analysis/shingle/ShingleAnalyzerWrapperTest.java
===================================================================
--- contrib/analyzers/common/src/test/org/apache/lucene/analysis/shingle/ShingleAnalyzerWrapperTest.java	(revision 916567)
+++ contrib/analyzers/common/src/test/org/apache/lucene/analysis/shingle/ShingleAnalyzerWrapperTest.java	(working copy)
@@ -106,7 +106,7 @@
    */
   public void testShingleAnalyzerWrapperQueryParsing() throws Exception {
     ScoreDoc[] hits = queryParsingTest(new ShingleAnalyzerWrapper
-                                     (new WhitespaceAnalyzer(), 2),
+                                     (new WhitespaceAnalyzer(Version.LUCENE_CURRENT), 2),
                                  "test sentence");
     int[] ranks = new int[] { 1, 2, 0 };
     compareRanks(hits, ranks);
@@ -117,7 +117,7 @@
    */
   public void testShingleAnalyzerWrapperPhraseQueryParsingFails() throws Exception {
     ScoreDoc[] hits = queryParsingTest(new ShingleAnalyzerWrapper
-                                     (new WhitespaceAnalyzer(), 2),
+                                     (new WhitespaceAnalyzer(Version.LUCENE_CURRENT), 2),
                                  "\"this sentence\"");
     int[] ranks = new int[] { 0 };
     compareRanks(hits, ranks);
@@ -128,7 +128,7 @@
    */
   public void testShingleAnalyzerWrapperPhraseQueryParsing() throws Exception {
     ScoreDoc[] hits = queryParsingTest(new ShingleAnalyzerWrapper
-                                     (new WhitespaceAnalyzer(), 2),
+                                     (new WhitespaceAnalyzer(Version.LUCENE_CURRENT), 2),
                                  "\"test sentence\"");
     int[] ranks = new int[] { 1 };
     compareRanks(hits, ranks);
@@ -139,7 +139,7 @@
    */
   public void testShingleAnalyzerWrapperRequiredQueryParsing() throws Exception {
     ScoreDoc[] hits = queryParsingTest(new ShingleAnalyzerWrapper
-                                     (new WhitespaceAnalyzer(), 2),
+                                     (new WhitespaceAnalyzer(Version.LUCENE_CURRENT), 2),
                                  "+test +sentence");
     int[] ranks = new int[] { 1, 2 };
     compareRanks(hits, ranks);
@@ -149,7 +149,7 @@
    * This shows how to construct a phrase query containing shingles.
    */
   public void testShingleAnalyzerWrapperPhraseQuery() throws Exception {
-    Analyzer analyzer = new ShingleAnalyzerWrapper(new WhitespaceAnalyzer(), 2);
+    Analyzer analyzer = new ShingleAnalyzerWrapper(new WhitespaceAnalyzer(Version.LUCENE_CURRENT), 2);
     searcher = setUpSearcher(analyzer);
 
     PhraseQuery q = new PhraseQuery();
@@ -178,7 +178,7 @@
    * in the right order and adjacent to each other.
    */
   public void testShingleAnalyzerWrapperBooleanQuery() throws Exception {
-    Analyzer analyzer = new ShingleAnalyzerWrapper(new WhitespaceAnalyzer(), 2);
+    Analyzer analyzer = new ShingleAnalyzerWrapper(new WhitespaceAnalyzer(Version.LUCENE_CURRENT), 2);
     searcher = setUpSearcher(analyzer);
 
     BooleanQuery q = new BooleanQuery();
@@ -200,7 +200,7 @@
   }
   
   public void testReusableTokenStream() throws Exception {
-    Analyzer a = new ShingleAnalyzerWrapper(new WhitespaceAnalyzer(), 2);
+    Analyzer a = new ShingleAnalyzerWrapper(new WhitespaceAnalyzer(Version.LUCENE_CURRENT), 2);
     assertAnalyzesToReuse(a, "please divide into shingles",
         new String[] { "please", "please divide", "divide", "divide into", "into", "into shingles", "shingles" },
         new int[] { 0, 0, 7, 7, 14, 14, 19 },
@@ -222,9 +222,9 @@
     @Override
     public TokenStream tokenStream(String fieldName, Reader reader) {
       if (++invocationCount % 2 == 0)
-        return new WhitespaceTokenizer(reader);
+        return new WhitespaceTokenizer(Version.LUCENE_CURRENT, reader);
       else
-        return new LetterTokenizer(reader);
+        return new LetterTokenizer(Version.LUCENE_CURRENT, reader);
     }
   }
   
@@ -246,4 +246,117 @@
         new int[] { 6, 13, 13, 18, 18, 27, 27 },
         new int[] { 1, 0, 1, 0, 1, 0, 1 });
   }
+
+  public void testNonDefaultMinShingleSize() throws Exception {
+    ShingleAnalyzerWrapper analyzer 
+      = new ShingleAnalyzerWrapper(new WhitespaceAnalyzer(), 3, 4);
+    assertAnalyzesToReuse(analyzer, "please divide this sentence into shingles",
+                          new String[] { "please",   "please divide this",   "please divide this sentence", 
+                                         "divide",   "divide this sentence", "divide this sentence into", 
+                                         "this",     "this sentence into",   "this sentence into shingles",
+                                         "sentence", "sentence into shingles",
+                                         "into",
+                                         "shingles" },
+                          new int[] { 0,  0,  0,  7,  7,  7, 14, 14, 14, 19, 19, 28, 33 },
+                          new int[] { 6, 18, 27, 13, 27, 32, 18, 32, 41, 27, 41, 32, 41 },
+                          new int[] { 1,  0,  0,  1,  0,  0,  1,  0,  0,  1,  0,  1,  1 });
+    analyzer.setOutputUnigrams(false);
+    assertAnalyzesToReuse(analyzer, "please divide this sentence into shingles",
+                          new String[] { "please divide this",   "please divide this sentence", 
+                                         "divide this sentence", "divide this sentence into", 
+                                         "this sentence into",   "this sentence into shingles",
+                                         "sentence into shingles" },
+                          new int[] {  0,  0,  7,  7, 14, 14, 19 },
+                          new int[] { 18, 27, 27, 32, 32, 41, 41 },
+                          new int[] {  1,  0,  1,  0,  1,  0,  1 });
+  }
+  
+  public void testNonDefaultMinAndSameMaxShingleSize() throws Exception {
+    ShingleAnalyzerWrapper analyzer
+      = new ShingleAnalyzerWrapper(new WhitespaceAnalyzer(), 3, 3);
+    assertAnalyzesToReuse(analyzer, "please divide this sentence into shingles",
+                          new String[] { "please",   "please divide this", 
+                                         "divide",   "divide this sentence", 
+                                         "this",     "this sentence into",
+                                         "sentence", "sentence into shingles",
+                                         "into",
+                                         "shingles" },
+                          new int[] { 0,  0,  7,  7, 14, 14, 19, 19, 28, 33 },
+                          new int[] { 6, 18, 13, 27, 18, 32, 27, 41, 32, 41 },
+                          new int[] { 1,  0,  1,  0,  1,  0,  1,  0,  1,  1 });
+    analyzer.setOutputUnigrams(false);
+    assertAnalyzesToReuse(analyzer, "please divide this sentence into shingles",
+                          new String[] { "please divide this", 
+                                         "divide this sentence", 
+                                         "this sentence into",
+                                         "sentence into shingles" },
+                          new int[] {  0,  7, 14, 19 },
+                          new int[] { 18, 27, 32, 41 },
+                          new int[] {  1,  1,  1,  1 });
+  }
+
+  public void testNoTokenSeparator() throws Exception {
+    ShingleAnalyzerWrapper analyzer 
+      = new ShingleAnalyzerWrapper(new WhitespaceAnalyzer());
+    analyzer.setTokenSeparator("");
+    assertAnalyzesToReuse(analyzer, "please divide into shingles",
+                          new String[] { "please", "pleasedivide", 
+                                         "divide", "divideinto", 
+                                         "into", "intoshingles", 
+                                         "shingles" },
+                          new int[] { 0,  0,  7,  7, 14, 14, 19 },
+                          new int[] { 6, 13, 13, 18, 18, 27, 27 },
+                          new int[] { 1,  0,  1,  0,  1,  0,  1 });
+    analyzer.setOutputUnigrams(false);
+    assertAnalyzesToReuse(analyzer, "please divide into shingles",
+                          new String[] { "pleasedivide", 
+                                         "divideinto", 
+                                         "intoshingles" },
+                          new int[] {  0,  7, 14 },
+                          new int[] { 13, 18, 27 },
+                          new int[] {  1,  1,  1 });
+  }
+
+  public void testNullTokenSeparator() throws Exception {
+    ShingleAnalyzerWrapper analyzer 
+      = new ShingleAnalyzerWrapper(new WhitespaceAnalyzer());
+    analyzer.setTokenSeparator(null);
+    assertAnalyzesToReuse(analyzer, "please divide into shingles",
+                          new String[] { "please", "pleasedivide", 
+                                         "divide", "divideinto", 
+                                         "into", "intoshingles", 
+                                         "shingles" },
+                          new int[] { 0,  0,  7,  7, 14, 14, 19 },
+                          new int[] { 6, 13, 13, 18, 18, 27, 27 },
+                          new int[] { 1,  0,  1,  0,  1,  0,  1 });
+    analyzer.setOutputUnigrams(false);
+    assertAnalyzesToReuse(analyzer, "please divide into shingles",
+                          new String[] { "pleasedivide", 
+                                         "divideinto", 
+                                         "intoshingles" },
+                          new int[] {  0,  7, 14 },
+                          new int[] { 13, 18, 27 },
+                          new int[] {  1,  1,  1 });
+  }
+  public void testAltTokenSeparator() throws Exception {
+    ShingleAnalyzerWrapper analyzer 
+      = new ShingleAnalyzerWrapper(new WhitespaceAnalyzer());
+    analyzer.setTokenSeparator("<SEP>");
+    assertAnalyzesToReuse(analyzer, "please divide into shingles",
+                          new String[] { "please", "please<SEP>divide", 
+                                         "divide", "divide<SEP>into", 
+                                         "into", "into<SEP>shingles", 
+                                         "shingles" },
+                          new int[] { 0,  0,  7,  7, 14, 14, 19 },
+                          new int[] { 6, 13, 13, 18, 18, 27, 27 },
+                          new int[] { 1,  0,  1,  0,  1,  0,  1 });
+    analyzer.setOutputUnigrams(false);
+    assertAnalyzesToReuse(analyzer, "please divide into shingles",
+                          new String[] { "please<SEP>divide", 
+                                         "divide<SEP>into", 
+                                         "into<SEP>shingles" },
+                          new int[] {  0,  7, 14 },
+                          new int[] { 13, 18, 27 },
+                          new int[] {  1,  1,  1 });
+  }
 }
Index: contrib/analyzers/common/src/test/org/apache/lucene/analysis/shingle/TestShingleMatrixFilter.java
===================================================================
--- contrib/analyzers/common/src/test/org/apache/lucene/analysis/shingle/TestShingleMatrixFilter.java	(revision 916567)
+++ contrib/analyzers/common/src/test/org/apache/lucene/analysis/shingle/TestShingleMatrixFilter.java	(working copy)
@@ -22,11 +22,8 @@
 import java.util.Collection;
 import java.util.Iterator;
 import java.util.LinkedList;
-import java.util.HashSet;
-import java.util.Arrays;
 
 import org.apache.lucene.analysis.*;
-import org.apache.lucene.analysis.standard.StandardTokenizer;
 import org.apache.lucene.analysis.miscellaneous.EmptyTokenStream;
 import org.apache.lucene.analysis.miscellaneous.PrefixAndSuffixAwareTokenFilter;
 import org.apache.lucene.analysis.miscellaneous.SingleTokenTokenStream;
@@ -34,6 +31,7 @@
 import org.apache.lucene.analysis.shingle.ShingleMatrixFilter.Matrix;
 import org.apache.lucene.analysis.shingle.ShingleMatrixFilter.Matrix.Column;
 import org.apache.lucene.analysis.tokenattributes.*;
+import org.apache.lucene.util.Version;
 
 public class TestShingleMatrixFilter extends BaseTokenStreamTestCase {
 
@@ -43,7 +41,7 @@
 
   public void testIterator() throws IOException {
 
-    WhitespaceTokenizer wst = new WhitespaceTokenizer(new StringReader("one two three four five"));
+    WhitespaceTokenizer wst = new WhitespaceTokenizer(Version.LUCENE_CURRENT, new StringReader("one two three four five"));
     ShingleMatrixFilter smf = new ShingleMatrixFilter(wst, 2, 2, '_', false, new ShingleMatrixFilter.OneDimensionalNonWeightedTokenSettingsCodec());
 
     int i;
@@ -85,22 +83,12 @@
 
     ts = new ShingleMatrixFilter(tls, 1, 2, new Character(' '), false, new ShingleMatrixFilter.OneDimensionalNonWeightedTokenSettingsCodec());
 
-
-    assertNext(ts, "please", 0, 6);
-    assertNext(ts, "please divide", 0, 13);
-    assertNext(ts, "divide", 7, 13);
-    assertNext(ts, "divide this", 7, 18);
-    assertNext(ts, "this", 14, 18);
-    assertNext(ts, "this sentence", 14, 27);
-    assertNext(ts, "sentence", 19, 27);
-    assertNext(ts, "sentence into", 19, 32);
-    assertNext(ts, "into", 28, 32);
-    assertNext(ts, "into shingles", 28, 39);
-    assertNext(ts, "shingles", 33, 39);
-
-
-    assertFalse(ts.incrementToken());
-
+    assertTokenStreamContents(ts,
+      new String[] { "please", "please divide", "divide", "divide this",
+        "this", "this sentence", "sentence", "sentence into", "into",
+        "into shingles", "shingles" },
+      new int[] { 0, 0, 7, 7, 14, 14, 19, 19, 28, 28, 33 },
+      new int[] { 6, 13, 13, 18, 18, 27, 27, 32, 32, 39, 39 });
   }
 
   /**
@@ -546,6 +534,7 @@
         return false;
       }
       Token prototype = (Token) iterator.next();
+      clearAttributes();
       termAtt.setTermBuffer(prototype.termBuffer(), 0, prototype.termLength());
       posIncrAtt.setPositionIncrement(prototype.getPositionIncrement());
       flagsAtt.setFlags(prototype.getFlags());
Index: contrib/analyzers/common/src/test/org/apache/lucene/analysis/el/GreekAnalyzerTest.java
===================================================================
--- contrib/analyzers/common/src/test/org/apache/lucene/analysis/el/GreekAnalyzerTest.java	(revision 916567)
+++ contrib/analyzers/common/src/test/org/apache/lucene/analysis/el/GreekAnalyzerTest.java	(working copy)
@@ -18,7 +18,6 @@
 
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.util.Version;
 
 /**
@@ -63,4 +62,23 @@
 	    assertAnalyzesToReuse(a, "\u03a0\u03a1\u039f\u03ab\u03a0\u039f\u0398\u0395\u03a3\u0395\u0399\u03a3  \u0386\u03c8\u03bf\u03b3\u03bf\u03c2, \u03bf \u03bc\u03b5\u03c3\u03c4\u03cc\u03c2 \u03ba\u03b1\u03b9 \u03bf\u03b9 \u03ac\u03bb\u03bb\u03bf\u03b9",
 	            new String[] { "\u03c0\u03c1\u03bf\u03c5\u03c0\u03bf\u03b8\u03b5\u03c3\u03b5\u03b9\u03c3", "\u03b1\u03c8\u03bf\u03b3\u03bf\u03c3", "\u03bc\u03b5\u03c3\u03c4\u03bf\u03c3", "\u03b1\u03bb\u03bb\u03bf\u03b9" });
 	}
+	
+	/**
+	 * Greek Analyzer didn't call standardFilter, so no normalization of acronyms.
+	 * check that this is preserved.
+	 * @deprecated remove this test in Lucene 4.0
+	 */
+	@Deprecated
+	public void testAcronymBWCompat() throws Exception {
+	  Analyzer a = new GreekAnalyzer(Version.LUCENE_30);
+	  assertAnalyzesTo(a, "Α.Π.Τ.", new String[] { "α.π.τ." });
+	}
+	
+  /**
+   * test that acronym normalization works
+   */
+  public void testAcronym() throws Exception {
+    Analyzer a = new GreekAnalyzer(Version.LUCENE_31);
+    assertAnalyzesTo(a, "Α.Π.Τ.", new String[] { "απτ" });
+  }
 }
Index: contrib/analyzers/common/src/test/org/apache/lucene/analysis/cn/TestChineseTokenizer.java
===================================================================
--- contrib/analyzers/common/src/test/org/apache/lucene/analysis/cn/TestChineseTokenizer.java	(revision 916567)
+++ contrib/analyzers/common/src/test/org/apache/lucene/analysis/cn/TestChineseTokenizer.java	(working copy)
@@ -24,11 +24,13 @@
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.Tokenizer;
 import org.apache.lucene.analysis.WhitespaceTokenizer;
 import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
+import org.apache.lucene.util.Version;
 
 
+/** @deprecated Remove this test when ChineseAnalyzer is removed. */
+@Deprecated
 public class TestChineseTokenizer extends BaseTokenStreamTestCase
 {
     public void testOtherLetterOffset() throws IOException
@@ -78,7 +80,7 @@
     private class JustChineseFilterAnalyzer extends Analyzer {
       @Override
       public TokenStream tokenStream(String fieldName, Reader reader) {
-        return new ChineseFilter(new WhitespaceTokenizer(reader));
+        return new ChineseFilter(new WhitespaceTokenizer(Version.LUCENE_CURRENT, reader));
       }
     }
     
Index: contrib/analyzers/common/src/test/org/apache/lucene/analysis/ar/TestArabicStemFilter.java
===================================================================
--- contrib/analyzers/common/src/test/org/apache/lucene/analysis/ar/TestArabicStemFilter.java	(revision 916567)
+++ contrib/analyzers/common/src/test/org/apache/lucene/analysis/ar/TestArabicStemFilter.java	(working copy)
@@ -21,7 +21,9 @@
 import java.io.StringReader;
 
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
-import org.apache.lucene.analysis.tokenattributes.TermAttribute;
+import org.apache.lucene.analysis.CharArraySet;
+import org.apache.lucene.analysis.KeywordMarkerTokenFilter;
+import org.apache.lucene.util.Version;
 
 /**
  * Test the Arabic Normalization Filter
@@ -112,11 +114,19 @@
   public void testNonArabic() throws IOException {
     check("English", "English");
   }
+  
+  public void testWithKeywordAttribute() throws IOException {
+    CharArraySet set = new CharArraySet(Version.LUCENE_CURRENT, 1, true);
+    set.add("ساهدهات");
+    ArabicLetterTokenizer tokenStream  = new ArabicLetterTokenizer(Version.LUCENE_CURRENT, new StringReader("ساهدهات"));
 
+    ArabicStemFilter filter = new ArabicStemFilter(new KeywordMarkerTokenFilter(tokenStream, set));
+    assertTokenStreamContents(filter, new String[]{"ساهدهات"});
+  }
+
   private void check(final String input, final String expected) throws IOException {
-    ArabicLetterTokenizer tokenStream  = new ArabicLetterTokenizer(new StringReader(input));
+    ArabicLetterTokenizer tokenStream  = new ArabicLetterTokenizer(Version.LUCENE_CURRENT, new StringReader(input));
     ArabicStemFilter filter = new ArabicStemFilter(tokenStream);
     assertTokenStreamContents(filter, new String[]{expected});
   }
-
 }
Index: contrib/analyzers/common/src/test/org/apache/lucene/analysis/ar/TestArabicAnalyzer.java
===================================================================
--- contrib/analyzers/common/src/test/org/apache/lucene/analysis/ar/TestArabicAnalyzer.java	(revision 916567)
+++ contrib/analyzers/common/src/test/org/apache/lucene/analysis/ar/TestArabicAnalyzer.java	(working copy)
@@ -17,11 +17,13 @@
  * limitations under the License.
  */
 
+import java.io.IOException;
 import java.util.Collections;
 import java.util.HashSet;
 import java.util.Set;
 
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
+import org.apache.lucene.analysis.CharArraySet;
 import org.apache.lucene.util.Version;
 
 /**
@@ -84,4 +86,17 @@
     assertAnalyzesTo(a, "The quick brown fox.", new String[] { "quick",
         "brown", "fox" });
   }
+  
+  public void testWithStemExclusionSet() throws IOException {
+    Set<String> set = new HashSet<String>();
+    set.add("ساهدهات");
+    ArabicAnalyzer a = new ArabicAnalyzer(Version.LUCENE_CURRENT, CharArraySet.EMPTY_SET, set);
+    assertAnalyzesTo(a, "كبيرة the quick ساهدهات", new String[] { "كبير","the", "quick", "ساهدهات" });
+    assertAnalyzesToReuse(a, "كبيرة the quick ساهدهات", new String[] { "كبير","the", "quick", "ساهدهات" });
+
+    
+    a = new ArabicAnalyzer(Version.LUCENE_CURRENT, CharArraySet.EMPTY_SET, CharArraySet.EMPTY_SET);
+    assertAnalyzesTo(a, "كبيرة the quick ساهدهات", new String[] { "كبير","the", "quick", "ساهد" });
+    assertAnalyzesToReuse(a, "كبيرة the quick ساهدهات", new String[] { "كبير","the", "quick", "ساهد" });
+  }
 }
Index: contrib/analyzers/common/src/test/org/apache/lucene/analysis/ar/TestArabicNormalizationFilter.java
===================================================================
--- contrib/analyzers/common/src/test/org/apache/lucene/analysis/ar/TestArabicNormalizationFilter.java	(revision 916567)
+++ contrib/analyzers/common/src/test/org/apache/lucene/analysis/ar/TestArabicNormalizationFilter.java	(working copy)
@@ -21,7 +21,7 @@
 import java.io.StringReader;
 
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
-import org.apache.lucene.analysis.tokenattributes.TermAttribute;
+import org.apache.lucene.util.Version;
 
 /**
  * Test the Arabic Normalization Filter
@@ -86,7 +86,7 @@
   }  
   
   private void check(final String input, final String expected) throws IOException {
-    ArabicLetterTokenizer tokenStream = new ArabicLetterTokenizer(new StringReader(input));
+    ArabicLetterTokenizer tokenStream = new ArabicLetterTokenizer(Version.LUCENE_CURRENT, new StringReader(input));
     ArabicNormalizationFilter filter = new ArabicNormalizationFilter(tokenStream);
     assertTokenStreamContents(filter, new String[]{expected});
   }
Index: contrib/analyzers/common/src/test/org/apache/lucene/analysis/cjk/TestCJKTokenizer.java
===================================================================
--- contrib/analyzers/common/src/test/org/apache/lucene/analysis/cjk/TestCJKTokenizer.java	(revision 916567)
+++ contrib/analyzers/common/src/test/org/apache/lucene/analysis/cjk/TestCJKTokenizer.java	(working copy)
@@ -18,14 +18,9 @@
  */
 
 import java.io.IOException;
-import java.io.StringReader;
 
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
-import org.apache.lucene.analysis.tokenattributes.TermAttribute;
-import org.apache.lucene.analysis.tokenattributes.TypeAttribute;
 import org.apache.lucene.util.Version;
 
 public class TestCJKTokenizer extends BaseTokenStreamTestCase {
@@ -47,33 +42,33 @@
   }
 
   public void checkCJKToken(final String str, final TestToken[] out_tokens) throws IOException {
-    CJKTokenizer tokenizer = new CJKTokenizer(new StringReader(str));
-    TermAttribute termAtt = tokenizer.getAttribute(TermAttribute.class);
-    OffsetAttribute offsetAtt = tokenizer.getAttribute(OffsetAttribute.class);
-    TypeAttribute typeAtt = tokenizer.getAttribute(TypeAttribute.class);
+    Analyzer analyzer = new CJKAnalyzer(Version.LUCENE_CURRENT);
+    String terms[] = new String[out_tokens.length];
+    int startOffsets[] = new int[out_tokens.length];
+    int endOffsets[] = new int[out_tokens.length];
+    String types[] = new String[out_tokens.length];
     for (int i = 0; i < out_tokens.length; i++) {
-      assertTrue(tokenizer.incrementToken());
-      assertEquals(termAtt.term(), out_tokens[i].termText);
-      assertEquals(offsetAtt.startOffset(), out_tokens[i].start);
-      assertEquals(offsetAtt.endOffset(), out_tokens[i].end);
-      assertEquals(typeAtt.type(), out_tokens[i].type);
+      terms[i] = out_tokens[i].termText;
+      startOffsets[i] = out_tokens[i].start;
+      endOffsets[i] = out_tokens[i].end;
+      types[i] = out_tokens[i].type;
     }
-    assertFalse(tokenizer.incrementToken());
+    assertAnalyzesTo(analyzer, str, terms, startOffsets, endOffsets, types, null);
   }
   
   public void checkCJKTokenReusable(final Analyzer a, final String str, final TestToken[] out_tokens) throws IOException {
-    TokenStream ts = a.reusableTokenStream("dummy", new StringReader(str));
-    TermAttribute termAtt = ts.getAttribute(TermAttribute.class);
-    OffsetAttribute offsetAtt = ts.getAttribute(OffsetAttribute.class);
-    TypeAttribute typeAtt = ts.getAttribute(TypeAttribute.class);
+    Analyzer analyzer = new CJKAnalyzer(Version.LUCENE_CURRENT);
+    String terms[] = new String[out_tokens.length];
+    int startOffsets[] = new int[out_tokens.length];
+    int endOffsets[] = new int[out_tokens.length];
+    String types[] = new String[out_tokens.length];
     for (int i = 0; i < out_tokens.length; i++) {
-      assertTrue(ts.incrementToken());
-      assertEquals(termAtt.term(), out_tokens[i].termText);
-      assertEquals(offsetAtt.startOffset(), out_tokens[i].start);
-      assertEquals(offsetAtt.endOffset(), out_tokens[i].end);
-      assertEquals(typeAtt.type(), out_tokens[i].type);
+      terms[i] = out_tokens[i].termText;
+      startOffsets[i] = out_tokens[i].start;
+      endOffsets[i] = out_tokens[i].end;
+      types[i] = out_tokens[i].type;
     }
-    assertFalse(ts.incrementToken());
+    assertAnalyzesToReuse(analyzer, str, terms, startOffsets, endOffsets, types, null);
   }
   
   public void testJa1() throws IOException {
@@ -219,13 +214,8 @@
   
   public void testTokenStream() throws Exception {
     Analyzer analyzer = new CJKAnalyzer(Version.LUCENE_CURRENT);
-    TokenStream ts = analyzer.tokenStream("dummy", new StringReader("\u4e00\u4e01\u4e02"));
-    TermAttribute termAtt = ts.getAttribute(TermAttribute.class);
-    assertTrue(ts.incrementToken());
-    assertEquals("\u4e00\u4e01", termAtt.term());
-    assertTrue(ts.incrementToken());
-    assertEquals("\u4e01\u4e02", termAtt.term());
-    assertFalse(ts.incrementToken());
+    assertAnalyzesTo(analyzer, "\u4e00\u4e01\u4e02", 
+        new String[] { "\u4e00\u4e01", "\u4e01\u4e02"});
   }
   
   public void testReusableTokenStream() throws Exception {
@@ -261,4 +251,24 @@
     };
     checkCJKTokenReusable(analyzer, str, out_tokens2);
   }
+  
+  /**
+   * LUCENE-2207: wrong offset calculated by end() 
+   */
+  public void testFinalOffset() throws IOException {
+    checkCJKToken("あい", new TestToken[] { 
+        newToken("あい", 0, 2, CJKTokenizer.DOUBLE_TOKEN_TYPE) });
+    checkCJKToken("あい   ", new TestToken[] { 
+        newToken("あい", 0, 2, CJKTokenizer.DOUBLE_TOKEN_TYPE) });
+    checkCJKToken("test", new TestToken[] { 
+        newToken("test", 0, 4, CJKTokenizer.SINGLE_TOKEN_TYPE) });
+    checkCJKToken("test   ", new TestToken[] { 
+        newToken("test", 0, 4, CJKTokenizer.SINGLE_TOKEN_TYPE) });
+    checkCJKToken("あいtest", new TestToken[] {
+        newToken("あい", 0, 2, CJKTokenizer.DOUBLE_TOKEN_TYPE),
+        newToken("test", 2, 6, CJKTokenizer.SINGLE_TOKEN_TYPE) });
+    checkCJKToken("testあい    ", new TestToken[] { 
+        newToken("test", 0, 4, CJKTokenizer.SINGLE_TOKEN_TYPE),
+        newToken("あい", 4, 6, CJKTokenizer.DOUBLE_TOKEN_TYPE) });
+  }
 }
Index: contrib/analyzers/common/src/test/org/apache/lucene/analysis/cz/TestCzechAnalyzer.java
===================================================================
--- contrib/analyzers/common/src/test/org/apache/lucene/analysis/cz/TestCzechAnalyzer.java	(revision 916567)
+++ contrib/analyzers/common/src/test/org/apache/lucene/analysis/cz/TestCzechAnalyzer.java	(working copy)
@@ -24,6 +24,7 @@
 
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.analysis.CharArraySet;
 import org.apache.lucene.util.Version;
 
 /**
@@ -109,5 +110,11 @@
     
     assertAnalyzesToReuse(cz, "Česká Republika", new String[] { "česká" });
   }
-
+  
+  public void testWithStemExclusionSet() throws IOException{
+    CharArraySet set = new CharArraySet(Version.LUCENE_CURRENT, 1, true);
+    set.add("hole");
+    CzechAnalyzer cz = new CzechAnalyzer(Version.LUCENE_CURRENT, CharArraySet.EMPTY_SET, set);
+    assertAnalyzesTo(cz, "hole desek", new String[] {"hole", "desk"});
+  }
 }
Index: contrib/analyzers/common/src/test/org/apache/lucene/analysis/cz/TestCzechStemmer.java
===================================================================
--- contrib/analyzers/common/src/test/org/apache/lucene/analysis/cz/TestCzechStemmer.java	(revision 916567)
+++ contrib/analyzers/common/src/test/org/apache/lucene/analysis/cz/TestCzechStemmer.java	(working copy)
@@ -18,8 +18,12 @@
  */
 
 import java.io.IOException;
+import java.io.StringReader;
 
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
+import org.apache.lucene.analysis.CharArraySet;
+import org.apache.lucene.analysis.KeywordMarkerTokenFilter;
+import org.apache.lucene.analysis.WhitespaceTokenizer;
 import org.apache.lucene.util.Version;
 
 /**
@@ -270,4 +274,13 @@
     assertAnalyzesTo(cz, "e", new String[] { "e" });
     assertAnalyzesTo(cz, "zi", new String[] { "zi" });
   }
+  
+  public void testWithKeywordAttribute() throws IOException {
+    CharArraySet set = new CharArraySet(Version.LUCENE_CURRENT, 1, true);
+    set.add("hole");
+    CzechStemFilter filter = new CzechStemFilter(new KeywordMarkerTokenFilter(
+        new WhitespaceTokenizer(Version.LUCENE_CURRENT, new StringReader("hole desek")), set));
+    assertTokenStreamContents(filter, new String[] { "hole", "desk" });
+  }
+  
 }
Index: contrib/analyzers/common/src/java/org/apache/lucene/analysis/compound/CompoundWordTokenFilterBase.java
===================================================================
--- contrib/analyzers/common/src/java/org/apache/lucene/analysis/compound/CompoundWordTokenFilterBase.java	(revision 916567)
+++ contrib/analyzers/common/src/java/org/apache/lucene/analysis/compound/CompoundWordTokenFilterBase.java	(working copy)
@@ -188,6 +188,7 @@
   }
   
   private final void setToken(final Token token) throws IOException {
+    clearAttributes();
     termAtt.setTermBuffer(token.termBuffer(), 0, token.termLength());
     flagsAtt.setFlags(token.getFlags());
     typeAtt.setType(token.type());
Index: contrib/analyzers/common/src/java/org/apache/lucene/analysis/compound/hyphenation/PatternParser.java
===================================================================
--- contrib/analyzers/common/src/java/org/apache/lucene/analysis/compound/hyphenation/PatternParser.java	(revision 916567)
+++ contrib/analyzers/common/src/java/org/apache/lucene/analysis/compound/hyphenation/PatternParser.java	(working copy)
@@ -15,8 +15,6 @@
  * limitations under the License.
  */
 
-/* $Id: PatternParser.java 426576 2006-07-28 15:44:37Z jeremias $ */
-
 package org.apache.lucene.analysis.compound.hyphenation;
 
 // SAX
Index: contrib/analyzers/common/src/java/org/apache/lucene/analysis/fa/PersianAnalyzer.java
===================================================================
--- contrib/analyzers/common/src/java/org/apache/lucene/analysis/fa/PersianAnalyzer.java	(revision 916567)
+++ contrib/analyzers/common/src/java/org/apache/lucene/analysis/fa/PersianAnalyzer.java	(working copy)
@@ -147,7 +147,7 @@
   @Override
   protected TokenStreamComponents createComponents(String fieldName,
       Reader reader) {
-    final Tokenizer source = new ArabicLetterTokenizer(reader);
+    final Tokenizer source = new ArabicLetterTokenizer(matchVersion, reader);
     TokenStream result = new LowerCaseFilter(matchVersion, source);
     result = new ArabicNormalizationFilter(result);
     /* additional persian-specific normalization */
Index: contrib/analyzers/common/src/java/org/apache/lucene/analysis/bg/BulgarianStemFilter.java
===================================================================
--- contrib/analyzers/common/src/java/org/apache/lucene/analysis/bg/BulgarianStemFilter.java	(revision 916567)
+++ contrib/analyzers/common/src/java/org/apache/lucene/analysis/bg/BulgarianStemFilter.java	(working copy)
@@ -19,29 +19,40 @@
 
 import java.io.IOException;
 
+import org.apache.lucene.analysis.KeywordMarkerTokenFilter; // for javadoc
 import org.apache.lucene.analysis.TokenFilter;
 import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.analysis.tokenattributes.KeywordAttribute;
 import org.apache.lucene.analysis.tokenattributes.TermAttribute;
 
 /**
  * A {@link TokenFilter} that applies {@link BulgarianStemmer} to stem Bulgarian
  * words.
+ * <p>
+ * To prevent terms from being stemmed use an instance of
+ * {@link KeywordMarkerTokenFilter} or a custom {@link TokenFilter} that sets
+ * the {@link KeywordAttribute} before this {@link TokenStream}.
+ * </p>
  */
 public final class BulgarianStemFilter extends TokenFilter {
   private final BulgarianStemmer stemmer;
   private final TermAttribute termAtt;
+  private final KeywordAttribute keywordAttr;
   
   public BulgarianStemFilter(final TokenStream input) {
     super(input);
     stemmer = new BulgarianStemmer();
     termAtt = addAttribute(TermAttribute.class);
+    keywordAttr = addAttribute(KeywordAttribute.class);
   }
   
   @Override
   public boolean incrementToken() throws IOException {
     if (input.incrementToken()) {
-      final int newlen = stemmer.stem(termAtt.termBuffer(), termAtt.termLength());
-      termAtt.setTermLength(newlen);
+      if(!keywordAttr.isKeyword()) {
+        final int newlen = stemmer.stem(termAtt.termBuffer(), termAtt.termLength());
+        termAtt.setTermLength(newlen);
+      }
       return true;
     } else {
       return false;
Index: contrib/analyzers/common/src/java/org/apache/lucene/analysis/bg/BulgarianAnalyzer.java
===================================================================
--- contrib/analyzers/common/src/java/org/apache/lucene/analysis/bg/BulgarianAnalyzer.java	(revision 916567)
+++ contrib/analyzers/common/src/java/org/apache/lucene/analysis/bg/BulgarianAnalyzer.java	(working copy)
@@ -25,6 +25,8 @@
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.LowerCaseFilter;
 import org.apache.lucene.analysis.ReusableAnalyzerBase.TokenStreamComponents; // javadoc @link
+import org.apache.lucene.analysis.CharArraySet;
+import org.apache.lucene.analysis.KeywordMarkerTokenFilter;
 import org.apache.lucene.analysis.StopFilter;
 import org.apache.lucene.analysis.StopwordAnalyzerBase;
 import org.apache.lucene.analysis.TokenStream;
@@ -88,6 +90,8 @@
       }
     }
   }
+  
+  private final Set<?> stemExclusionSet;
    
   /**
    * Builds an analyzer with the default stop words:
@@ -101,16 +105,27 @@
    * Builds an analyzer with the given stop words.
    */
   public BulgarianAnalyzer(Version matchVersion, Set<?> stopwords) {
-    super(matchVersion, stopwords);
+    this(matchVersion, stopwords, CharArraySet.EMPTY_SET);
   }
   
   /**
+   * Builds an analyzer with the given stop words and a stem exclusion set.
+   * If a stem exclusion set is provided this analyzer will add a {@link KeywordMarkerTokenFilter} 
+   * before {@link BulgarianStemFilter}.
+   */
+  public BulgarianAnalyzer(Version matchVersion, Set<?> stopwords, Set<?> stemExclusionSet) {
+    super(matchVersion, stopwords);
+    this.stemExclusionSet = CharArraySet.unmodifiableSet(CharArraySet.copy(
+        matchVersion, stemExclusionSet));  }
+  
+  /**
    * Creates a {@link TokenStreamComponents} which tokenizes all the text in the provided
    * {@link Reader}.
    * 
    * @return A {@link TokenStreamComponents} built from an {@link StandardTokenizer}
    *         filtered with {@link StandardFilter}, {@link LowerCaseFilter},
-   *         {@link StopFilter}, and {@link BulgarianStemFilter}.
+   *         {@link StopFilter}, {@link KeywordMarkerTokenFilter} if a stem
+   *         exclusion set is provided and {@link BulgarianStemFilter}.
    */
   @Override
   public TokenStreamComponents createComponents(String fieldName, Reader reader) {
@@ -118,6 +133,8 @@
     TokenStream result = new StandardFilter(source);
     result = new LowerCaseFilter(matchVersion, result);
     result = new StopFilter(matchVersion, result, stopwords);
+    if(!stemExclusionSet.isEmpty())
+      result = new KeywordMarkerTokenFilter(result, stemExclusionSet);
     result = new BulgarianStemFilter(result);
     return new TokenStreamComponents(source, result);
   }
Index: contrib/analyzers/common/src/java/org/apache/lucene/analysis/de/GermanAnalyzer.java
===================================================================
--- contrib/analyzers/common/src/java/org/apache/lucene/analysis/de/GermanAnalyzer.java	(revision 916567)
+++ contrib/analyzers/common/src/java/org/apache/lucene/analysis/de/GermanAnalyzer.java	(working copy)
@@ -30,15 +30,18 @@
 import org.apache.lucene.analysis.CharArraySet;
 import org.apache.lucene.analysis.LowerCaseFilter;
 import org.apache.lucene.analysis.ReusableAnalyzerBase.TokenStreamComponents; // javadoc @link
+import org.apache.lucene.analysis.KeywordMarkerTokenFilter;
 import org.apache.lucene.analysis.StopFilter;
 import org.apache.lucene.analysis.StopwordAnalyzerBase;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
 import org.apache.lucene.analysis.WordlistLoader;
+import org.apache.lucene.analysis.snowball.SnowballFilter;
 import org.apache.lucene.analysis.standard.StandardAnalyzer;
 import org.apache.lucene.analysis.standard.StandardFilter;
 import org.apache.lucene.analysis.standard.StandardTokenizer;
 import org.apache.lucene.util.Version;
+import org.tartarus.snowball.ext.German2Stemmer;
 
 /**
  * {@link Analyzer} for German language. 
@@ -50,6 +53,16 @@
  * exclusion list is empty by default.
  * </p>
  * 
+ * <a name="version"/>
+ * <p>You must specify the required {@link Version}
+ * compatibility when creating GermanAnalyzer:
+ * <ul>
+ *   <li> As of 3.1, Snowball stemming is done with SnowballFilter, and 
+ *        Snowball stopwords are used by default.
+ *   <li> As of 2.9, StopFilter preserves position
+ *        increments
+ * </ul>
+ * 
  * <p><b>NOTE</b>: This class uses the same {@link Version}
  * dependent settings as {@link StandardAnalyzer}.</p>
  */
@@ -59,7 +72,7 @@
    * List of typical german stopwords.
    * @deprecated use {@link #getDefaultStopSet()} instead
    */
-  //TODO make this private in 3.1
+  //TODO make this private in 3.1, remove in 4.0
   @Deprecated
   public final static String[] GERMAN_STOP_WORDS = {
     "einer", "eine", "eines", "einem", "einen",
@@ -76,6 +89,9 @@
     "durch", "wegen", "wird"
   };
   
+  /** File containing default German stopwords. */
+  public final static String DEFAULT_STOPWORD_FILE = "german_stop.txt";
+  
   /**
    * Returns a set of default German-stopwords 
    * @return a set of default German-stopwords 
@@ -85,8 +101,21 @@
   }
   
   private static class DefaultSetHolder {
-    private static final Set<?> DEFAULT_SET = CharArraySet.unmodifiableSet(new CharArraySet(
+    /** @deprecated remove in Lucene 4.0 */
+    @Deprecated
+    private static final Set<?> DEFAULT_SET_30 = CharArraySet.unmodifiableSet(new CharArraySet(
         Version.LUCENE_CURRENT, Arrays.asList(GERMAN_STOP_WORDS), false));
+    private static final Set<?> DEFAULT_SET;
+    static {
+      try {
+        DEFAULT_SET = 
+          WordlistLoader.getSnowballWordSet(SnowballFilter.class, DEFAULT_STOPWORD_FILE);
+      } catch (IOException ex) {
+        // default set should always be present as it is part of the
+        // distribution (JAR)
+        throw new RuntimeException("Unable to load default stopword set");
+      }
+    }
   }
 
   /**
@@ -104,7 +133,9 @@
    * {@link #getDefaultStopSet()}.
    */
   public GermanAnalyzer(Version matchVersion) {
-    this(matchVersion, DefaultSetHolder.DEFAULT_SET);
+    this(matchVersion,
+        matchVersion.onOrAfter(Version.LUCENE_31) ? DefaultSetHolder.DEFAULT_SET
+            : DefaultSetHolder.DEFAULT_SET_30);
   }
   
   /**
@@ -198,8 +229,9 @@
    * 
    * @return {@link TokenStreamComponents} built from a
    *         {@link StandardTokenizer} filtered with {@link StandardFilter},
-   *         {@link LowerCaseFilter}, {@link StopFilter}, and
-   *         {@link GermanStemFilter}
+   *         {@link LowerCaseFilter}, {@link StopFilter}, 
+   *         {@link KeywordMarkerTokenFilter} if a stem exclusion set is provided, and
+   *         {@link SnowballFilter}
    */
   @Override
   protected TokenStreamComponents createComponents(String fieldName,
@@ -208,6 +240,11 @@
     TokenStream result = new StandardFilter(source);
     result = new LowerCaseFilter(matchVersion, result);
     result = new StopFilter( matchVersion, result, stopwords);
-    return new TokenStreamComponents(source, new GermanStemFilter(result, exclusionSet));
+    result = new KeywordMarkerTokenFilter(result, exclusionSet);
+    if (matchVersion.onOrAfter(Version.LUCENE_31))
+      result = new SnowballFilter(result, new German2Stemmer());
+    else
+      result = new GermanStemFilter(result);
+    return new TokenStreamComponents(source, result);
   }
 }
Index: contrib/analyzers/common/src/java/org/apache/lucene/analysis/de/GermanStemFilter.java
===================================================================
--- contrib/analyzers/common/src/java/org/apache/lucene/analysis/de/GermanStemFilter.java	(revision 916567)
+++ contrib/analyzers/common/src/java/org/apache/lucene/analysis/de/GermanStemFilter.java	(working copy)
@@ -20,8 +20,10 @@
 import java.io.IOException;
 import java.util.Set;
 
+import org.apache.lucene.analysis.KeywordMarkerTokenFilter;// for javadoc
 import org.apache.lucene.analysis.TokenFilter;
 import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.analysis.tokenattributes.KeywordAttribute;
 import org.apache.lucene.analysis.tokenattributes.TermAttribute;
 
 /**
@@ -31,6 +33,12 @@
  * not be stemmed at all. The stemmer used can be changed at runtime after the
  * filter object is created (as long as it is a {@link GermanStemmer}).
  * </p>
+ * <p>
+ * To prevent terms from being stemmed use an instance of
+ * {@link KeywordMarkerTokenFilter} or a custom {@link TokenFilter} that sets
+ * the {@link KeywordAttribute} before this {@link TokenStream}.
+ * </p>
+ * @see KeywordMarkerTokenFilter
  */
 public final class GermanStemFilter extends TokenFilter
 {
@@ -38,21 +46,29 @@
      * The actual token in the input stream.
      */
     private GermanStemmer stemmer = null;
-    private Set exclusionSet = null;
+    private Set<?> exclusionSet = null;
 
-    private TermAttribute termAtt;
+    private final TermAttribute termAtt;
+    private final KeywordAttribute keywordAttr;
 
+    /**
+     * Creates a {@link GermanStemFilter} instance
+     * @param in the source {@link TokenStream} 
+     */
     public GermanStemFilter( TokenStream in )
     {
       super(in);
       stemmer = new GermanStemmer();
       termAtt = addAttribute(TermAttribute.class);
+      keywordAttr = addAttribute(KeywordAttribute.class);
     }
 
     /**
      * Builds a GermanStemFilter that uses an exclusion table.
+     * @deprecated use {@link KeywordAttribute} with {@link KeywordMarkerTokenFilter} instead.
      */
-    public GermanStemFilter( TokenStream in, Set exclusionSet )
+    @Deprecated
+    public GermanStemFilter( TokenStream in, Set<?> exclusionSet )
     {
       this( in );
       this.exclusionSet = exclusionSet;
@@ -66,7 +82,7 @@
       if (input.incrementToken()) {
         String term = termAtt.term();
         // Check the exclusion table.
-        if (exclusionSet == null || !exclusionSet.contains(term)) {
+        if (!keywordAttr.isKeyword() && (exclusionSet == null || !exclusionSet.contains(term))) {
           String s = stemmer.stem(term);
           // If not stemmed, don't waste the time adjusting the token.
           if ((s != null) && !s.equals(term))
@@ -91,8 +107,10 @@
 
     /**
      * Set an alternative exclusion list for this filter.
+     * @deprecated use {@link KeywordAttribute} with {@link KeywordMarkerTokenFilter} instead.
      */
-    public void setExclusionSet( Set exclusionSet )
+    @Deprecated
+    public void setExclusionSet( Set<?> exclusionSet )
     {
       this.exclusionSet = exclusionSet;
     }
Index: contrib/analyzers/common/src/java/org/apache/lucene/analysis/br/BrazilianAnalyzer.java
===================================================================
--- contrib/analyzers/common/src/java/org/apache/lucene/analysis/br/BrazilianAnalyzer.java	(revision 916567)
+++ contrib/analyzers/common/src/java/org/apache/lucene/analysis/br/BrazilianAnalyzer.java	(working copy)
@@ -30,6 +30,7 @@
 import org.apache.lucene.analysis.CharArraySet;
 import org.apache.lucene.analysis.LowerCaseFilter;
 import org.apache.lucene.analysis.ReusableAnalyzerBase.TokenStreamComponents; // javadoc @link
+import org.apache.lucene.analysis.KeywordMarkerTokenFilter;
 import org.apache.lucene.analysis.StopFilter;
 import org.apache.lucene.analysis.StopwordAnalyzerBase;
 import org.apache.lucene.analysis.TokenStream;
@@ -204,8 +205,9 @@
     TokenStream result = new LowerCaseFilter(matchVersion, source);
     result = new StandardFilter(result);
     result = new StopFilter(matchVersion, result, stopwords);
-    return new TokenStreamComponents(source, new BrazilianStemFilter(result,
-        excltable));
+    if(excltable != null && !excltable.isEmpty())
+      result = new KeywordMarkerTokenFilter(result, excltable);
+    return new TokenStreamComponents(source, new BrazilianStemFilter(result));
   }
 }
 
Index: contrib/analyzers/common/src/java/org/apache/lucene/analysis/br/BrazilianStemFilter.java
===================================================================
--- contrib/analyzers/common/src/java/org/apache/lucene/analysis/br/BrazilianStemFilter.java	(revision 916567)
+++ contrib/analyzers/common/src/java/org/apache/lucene/analysis/br/BrazilianStemFilter.java	(working copy)
@@ -20,13 +20,21 @@
 import java.io.IOException;
 import java.util.Set;
 
+import org.apache.lucene.analysis.KeywordMarkerTokenFilter; // for javadoc
 import org.apache.lucene.analysis.TokenFilter;
 import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.analysis.tokenattributes.KeywordAttribute;
 import org.apache.lucene.analysis.tokenattributes.TermAttribute;
 
 /**
  * A {@link TokenFilter} that applies {@link BrazilianStemmer}.
- *
+ * <p>
+ * To prevent terms from being stemmed use an instance of
+ * {@link KeywordMarkerTokenFilter} or a custom {@link TokenFilter} that sets
+ * the {@link KeywordAttribute} before this {@link TokenStream}.
+ * </p>
+ * @see KeywordMarkerTokenFilter
+ * 
  */
 public final class BrazilianStemFilter extends TokenFilter {
 
@@ -34,16 +42,31 @@
    * {@link BrazilianStemmer} in use by this filter.
    */
   private BrazilianStemmer stemmer = null;
-  private Set exclusions = null;
-  private TermAttribute termAtt;
-  
+  private Set<?> exclusions = null;
+  private final TermAttribute termAtt;
+  private final KeywordAttribute keywordAttr;
+
+  /**
+   * Creates a new BrazilianStemFilter 
+   * 
+   * @param in the source {@link TokenStream} 
+   */
   public BrazilianStemFilter(TokenStream in) {
     super(in);
     stemmer = new BrazilianStemmer();
     termAtt = addAttribute(TermAttribute.class);
+    keywordAttr = addAttribute(KeywordAttribute.class);
   }
-
-  public BrazilianStemFilter(TokenStream in, Set exclusiontable) {
+  
+  /**
+   * Creates a new BrazilianStemFilter 
+   * 
+   * @param in the source {@link TokenStream} 
+   * @param exclusiontable a set of terms that should be prevented from being stemmed.
+   * @deprecated use {@link KeywordAttribute} with {@link KeywordMarkerTokenFilter} instead.
+   */
+  @Deprecated
+  public BrazilianStemFilter(TokenStream in, Set<?> exclusiontable) {
     this(in);
     this.exclusions = exclusiontable;
   }
@@ -51,10 +74,10 @@
   @Override
   public boolean incrementToken() throws IOException {
     if (input.incrementToken()) {
-      String term = termAtt.term();
+      final String term = termAtt.term();
       // Check the exclusion table.
-      if (exclusions == null || !exclusions.contains(term)) {
-        String s = stemmer.stem(term);
+      if (!keywordAttr.isKeyword() && (exclusions == null || !exclusions.contains(term))) {
+        final String s = stemmer.stem(term);
         // If not stemmed, don't waste the time adjusting the token.
         if ((s != null) && !s.equals(term))
           termAtt.setTermBuffer(s);
Index: contrib/analyzers/common/src/java/org/apache/lucene/analysis/miscellaneous/PrefixAwareTokenFilter.java
===================================================================
--- contrib/analyzers/common/src/java/org/apache/lucene/analysis/miscellaneous/PrefixAwareTokenFilter.java	(revision 916567)
+++ contrib/analyzers/common/src/java/org/apache/lucene/analysis/miscellaneous/PrefixAwareTokenFilter.java	(working copy)
@@ -114,6 +114,7 @@
   
   private void setCurrentToken(Token token) {
     if (token == null) return;
+    clearAttributes();
     termAtt.setTermBuffer(token.termBuffer(), 0, token.termLength());
     posIncrAtt.setPositionIncrement(token.getPositionIncrement());
     flagsAtt.setFlags(token.getFlags());
Index: contrib/analyzers/common/src/java/org/apache/lucene/analysis/miscellaneous/EmptyTokenStream.java
===================================================================
--- contrib/analyzers/common/src/java/org/apache/lucene/analysis/miscellaneous/EmptyTokenStream.java	(revision 916567)
+++ contrib/analyzers/common/src/java/org/apache/lucene/analysis/miscellaneous/EmptyTokenStream.java	(working copy)
@@ -18,7 +18,6 @@
  */
 
 import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.Token;
 
 import java.io.IOException;
 
Index: contrib/analyzers/common/src/java/org/apache/lucene/analysis/ngram/NGramTokenizer.java
===================================================================
--- contrib/analyzers/common/src/java/org/apache/lucene/analysis/ngram/NGramTokenizer.java	(revision 916567)
+++ contrib/analyzers/common/src/java/org/apache/lucene/analysis/ngram/NGramTokenizer.java	(working copy)
@@ -17,7 +17,6 @@
  * limitations under the License.
  */
 
-import org.apache.lucene.analysis.Token;
 import org.apache.lucene.analysis.Tokenizer;
 import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
 import org.apache.lucene.analysis.tokenattributes.TermAttribute;
Index: contrib/analyzers/common/src/java/org/apache/lucene/analysis/ngram/EdgeNGramTokenFilter.java
===================================================================
--- contrib/analyzers/common/src/java/org/apache/lucene/analysis/ngram/EdgeNGramTokenFilter.java	(revision 916567)
+++ contrib/analyzers/common/src/java/org/apache/lucene/analysis/ngram/EdgeNGramTokenFilter.java	(working copy)
@@ -70,6 +70,7 @@
   private char[] curTermBuffer;
   private int curTermLength;
   private int curGramSize;
+  private int tokStart;
   
   private final TermAttribute termAtt;
   private final OffsetAttribute offsetAtt;
@@ -126,6 +127,7 @@
           curTermBuffer = (char[]) termAtt.termBuffer().clone();
           curTermLength = termAtt.termLength();
           curGramSize = minGram;
+          tokStart = offsetAtt.startOffset();
         }
       }
       if (curGramSize <= maxGram) {
@@ -134,7 +136,8 @@
           // grab gramSize chars from front or back
           int start = side == Side.FRONT ? 0 : curTermLength - curGramSize;
           int end = start + curGramSize;
-          offsetAtt.setOffset(start, end);
+          clearAttributes();
+          offsetAtt.setOffset(tokStart + start, tokStart + end);
           termAtt.setTermBuffer(curTermBuffer, start, curGramSize);
           curGramSize++;
           return true;
Index: contrib/analyzers/common/src/java/org/apache/lucene/analysis/ngram/NGramTokenFilter.java
===================================================================
--- contrib/analyzers/common/src/java/org/apache/lucene/analysis/ngram/NGramTokenFilter.java	(revision 916567)
+++ contrib/analyzers/common/src/java/org/apache/lucene/analysis/ngram/NGramTokenFilter.java	(working copy)
@@ -19,7 +19,6 @@
 
 import java.io.IOException;
 
-import org.apache.lucene.analysis.Token;
 import org.apache.lucene.analysis.TokenFilter;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
@@ -38,6 +37,7 @@
   private int curTermLength;
   private int curGramSize;
   private int curPos;
+  private int tokStart;
   
   private TermAttribute termAtt;
   private OffsetAttribute offsetAtt;
@@ -83,12 +83,14 @@
           curTermLength = termAtt.termLength();
           curGramSize = minGram;
           curPos = 0;
+          tokStart = offsetAtt.startOffset();
         }
       }
       while (curGramSize <= maxGram) {
         while (curPos+curGramSize <= curTermLength) {     // while there is input
+          clearAttributes();
           termAtt.setTermBuffer(curTermBuffer, curPos, curGramSize);
-          offsetAtt.setOffset(curPos, curPos+curGramSize);
+          offsetAtt.setOffset(tokStart + curPos, tokStart + curPos + curGramSize);
           curPos++;
           return true;
         }
Index: contrib/analyzers/common/src/java/org/apache/lucene/analysis/fr/FrenchStemmer.java
===================================================================
--- contrib/analyzers/common/src/java/org/apache/lucene/analysis/fr/FrenchStemmer.java	(revision 916567)
+++ contrib/analyzers/common/src/java/org/apache/lucene/analysis/fr/FrenchStemmer.java	(working copy)
@@ -25,8 +25,10 @@
  * refer to http://snowball.sourceforge.net/french/stemmer.html<br>
  * (French stemming algorithm) for details
  * </p>
+ * @deprecated Use {@link org.tartarus.snowball.ext.FrenchStemmer} instead, 
+ * which has the same functionality. This filter will be removed in Lucene 4.0
  */
-
+@Deprecated
 public class FrenchStemmer {
 
     /**
Index: contrib/analyzers/common/src/java/org/apache/lucene/analysis/fr/FrenchStemFilter.java
===================================================================
--- contrib/analyzers/common/src/java/org/apache/lucene/analysis/fr/FrenchStemFilter.java	(revision 916567)
+++ contrib/analyzers/common/src/java/org/apache/lucene/analysis/fr/FrenchStemFilter.java	(working copy)
@@ -17,8 +17,11 @@
  * limitations under the License.
  */
 
+import org.apache.lucene.analysis.KeywordMarkerTokenFilter;// for javadoc
 import org.apache.lucene.analysis.TokenFilter;
 import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.analysis.snowball.SnowballFilter;
+import org.apache.lucene.analysis.tokenattributes.KeywordAttribute;
 import org.apache.lucene.analysis.tokenattributes.TermAttribute;
 
 import java.io.IOException;
@@ -29,29 +32,46 @@
 /**
  * A {@link TokenFilter} that stems french words. 
  * <p>
- * It supports a table of words that should
- * not be stemmed at all. The used stemmer can be changed at runtime after the
+ * The used stemmer can be changed at runtime after the
  * filter object is created (as long as it is a {@link FrenchStemmer}).
  * </p>
+ * <p>
+ * To prevent terms from being stemmed use an instance of
+ * {@link KeywordMarkerTokenFilter} or a custom {@link TokenFilter} that sets
+ * the {@link KeywordAttribute} before this {@link TokenStream}.
+ * </p>
+ * @see KeywordMarkerTokenFilter
+ * @deprecated Use {@link SnowballFilter} with 
+ * {@link org.tartarus.snowball.ext.FrenchStemmer} instead, which has the
+ * same functionality. This filter will be removed in Lucene 4.0
  */
+@Deprecated
 public final class FrenchStemFilter extends TokenFilter {
 
 	/**
 	 * The actual token in the input stream.
 	 */
 	private FrenchStemmer stemmer = null;
-	private Set exclusions = null;
+	private Set<?> exclusions = null;
 	
-	private TermAttribute termAtt;
+	private final TermAttribute termAtt;
+  private final KeywordAttribute keywordAttr;
 
 	public FrenchStemFilter( TokenStream in ) {
           super(in);
 		stemmer = new FrenchStemmer();
 		termAtt = addAttribute(TermAttribute.class);
+    keywordAttr = addAttribute(KeywordAttribute.class);
 	}
 
-
-	public FrenchStemFilter( TokenStream in, Set exclusiontable ) {
+  /**
+   * 
+   * @param in the {@link TokenStream} to filter
+   * @param exclusiontable a set of terms not to be stemmed
+   * @deprecated use {@link KeywordAttribute} with {@link KeywordMarkerTokenFilter} instead.
+   */
+	@Deprecated // TODO remove in 3.2
+	public FrenchStemFilter( TokenStream in, Set<?> exclusiontable ) {
 		this( in );
 		exclusions = exclusiontable;
 	}
@@ -65,7 +85,7 @@
 	    String term = termAtt.term();
 
 	    // Check the exclusion table
-	    if ( exclusions == null || !exclusions.contains( term ) ) {
+	    if ( !keywordAttr.isKeyword() && (exclusions == null || !exclusions.contains( term )) ) {
 	      String s = stemmer.stem( term );
 	      // If not stemmed, don't waste the time  adjusting the token.
 	      if ((s != null) && !s.equals( term ) )
@@ -86,8 +106,10 @@
 	}
 	/**
 	 * Set an alternative exclusion list for this filter.
+   * @deprecated use {@link KeywordAttribute} with {@link KeywordMarkerTokenFilter} instead.
 	 */
-	public void setExclusionTable( Map exclusiontable ) {
+	@Deprecated // TODO remove in 3.2
+	public void setExclusionTable( Map<?,?> exclusiontable ) {
 		exclusions = new HashSet(exclusiontable.keySet());
 	}
 }
Index: contrib/analyzers/common/src/java/org/apache/lucene/analysis/fr/ElisionFilter.java
===================================================================
--- contrib/analyzers/common/src/java/org/apache/lucene/analysis/fr/ElisionFilter.java	(revision 916567)
+++ contrib/analyzers/common/src/java/org/apache/lucene/analysis/fr/ElisionFilter.java	(working copy)
@@ -68,7 +68,7 @@
   /**
    * Constructs an elision filter with standard stop words
    */
-  protected ElisionFilter(Version matchVersion, TokenStream input) {
+  public ElisionFilter(Version matchVersion, TokenStream input) {
     this(matchVersion, input, DEFAULT_ARTICLES);
   }
 
@@ -77,7 +77,7 @@
    * @deprecated use {@link #ElisionFilter(Version, TokenStream)} instead
    */
   @Deprecated
-  protected ElisionFilter(TokenStream input) {
+  public ElisionFilter(TokenStream input) {
     this(Version.LUCENE_30, input);
   }
 
Index: contrib/analyzers/common/src/java/org/apache/lucene/analysis/fr/FrenchAnalyzer.java
===================================================================
--- contrib/analyzers/common/src/java/org/apache/lucene/analysis/fr/FrenchAnalyzer.java	(revision 916567)
+++ contrib/analyzers/common/src/java/org/apache/lucene/analysis/fr/FrenchAnalyzer.java	(working copy)
@@ -21,11 +21,13 @@
 import org.apache.lucene.analysis.CharArraySet;
 import org.apache.lucene.analysis.LowerCaseFilter;
 import org.apache.lucene.analysis.ReusableAnalyzerBase.TokenStreamComponents; // javadoc @link
+import org.apache.lucene.analysis.KeywordMarkerTokenFilter;
 import org.apache.lucene.analysis.StopFilter;
 import org.apache.lucene.analysis.StopwordAnalyzerBase;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
 import org.apache.lucene.analysis.WordlistLoader;
+import org.apache.lucene.analysis.snowball.SnowballFilter;
 import org.apache.lucene.analysis.standard.StandardFilter;
 import org.apache.lucene.analysis.standard.StandardTokenizer;
 import org.apache.lucene.analysis.standard.StandardAnalyzer;  // for javadoc
@@ -54,6 +56,9 @@
  * <p>You must specify the required {@link Version}
  * compatibility when creating FrenchAnalyzer:
  * <ul>
+ *   <li> As of 3.1, Snowball stemming is done with SnowballFilter, 
+ *        LowerCaseFilter is used prior to StopFilter, and ElisionFilter and 
+ *        Snowball stopwords are used by default.
  *   <li> As of 2.9, StopFilter preserves position
  *        increments
  * </ul>
@@ -67,7 +72,7 @@
    * Extended list of typical French stopwords.
    * @deprecated use {@link #getDefaultStopSet()} instead
    */
-  // TODO make this private in 3.1
+  // TODO make this private in 3.1, remove in 4.0
   @Deprecated
   public final static String[] FRENCH_STOP_WORDS = {
     "a", "afin", "ai", "ainsi", "après", "attendu", "au", "aujourd", "auquel", "aussi",
@@ -94,6 +99,9 @@
     "été", "être", "ô"
   };
 
+  /** File containing default French stopwords. */
+  public final static String DEFAULT_STOPWORD_FILE = "french_stop.txt";
+  
   /**
    * Contains words that should be indexed but not stemmed.
    */
@@ -109,16 +117,31 @@
   }
   
   private static class DefaultSetHolder {
-    static final Set<?> DEFAULT_STOP_SET = CharArraySet
+    /** @deprecated remove this in Lucene 4.0 */
+    @Deprecated
+    static final Set<?> DEFAULT_STOP_SET_30 = CharArraySet
         .unmodifiableSet(new CharArraySet(Version.LUCENE_CURRENT, Arrays.asList(FRENCH_STOP_WORDS),
             false));
+    static final Set<?> DEFAULT_STOP_SET;
+    static {
+      try {
+        DEFAULT_STOP_SET = 
+          WordlistLoader.getSnowballWordSet(SnowballFilter.class, DEFAULT_STOPWORD_FILE);
+      } catch (IOException ex) {
+        // default set should always be present as it is part of the
+        // distribution (JAR)
+        throw new RuntimeException("Unable to load default stopword set");
+      }
+    }
   }
 
   /**
-   * Builds an analyzer with the default stop words ({@link #FRENCH_STOP_WORDS}).
+   * Builds an analyzer with the default stop words ({@link #getDefaultStopSet}).
    */
   public FrenchAnalyzer(Version matchVersion) {
-    this(matchVersion, DefaultSetHolder.DEFAULT_STOP_SET);
+    this(matchVersion,
+        matchVersion.onOrAfter(Version.LUCENE_31) ? DefaultSetHolder.DEFAULT_STOP_SET
+            : DefaultSetHolder.DEFAULT_STOP_SET_30);
   }
   
   /**
@@ -206,18 +229,34 @@
    * {@link Reader}.
    *
    * @return {@link TokenStreamComponents} built from a {@link StandardTokenizer} 
-   *         filtered with {@link StandardFilter}, {@link StopFilter}, 
-   *         {@link FrenchStemFilter} and {@link LowerCaseFilter}
+   *         filtered with {@link StandardFilter}, {@link ElisionFilter}, 
+   *         {@link LowerCaseFilter}, {@link StopFilter},
+   *         {@link KeywordMarkerTokenFilter} if a stem exclusion set is provided, 
+   *         and {@link SnowballFilter}
    */
   @Override
   protected TokenStreamComponents createComponents(String fieldName,
       Reader reader) {
-    final Tokenizer source = new StandardTokenizer(matchVersion, reader);
-    TokenStream result = new StandardFilter(source);
-    result = new StopFilter(matchVersion, result, stopwords);
-    result = new FrenchStemFilter(result, excltable);
-    // Convert to lowercase after stemming!
-    return new TokenStreamComponents(source, new LowerCaseFilter(matchVersion, result));
+    if (matchVersion.onOrAfter(Version.LUCENE_31)) {
+      final Tokenizer source = new StandardTokenizer(matchVersion, reader);
+      TokenStream result = new StandardFilter(source);
+      result = new ElisionFilter(matchVersion, result);
+      result = new LowerCaseFilter(matchVersion, result);
+      result = new StopFilter(matchVersion, result, stopwords);
+      if(!excltable.isEmpty())
+        result = new KeywordMarkerTokenFilter(result, excltable);
+      result = new SnowballFilter(result, new org.tartarus.snowball.ext.FrenchStemmer());
+      return new TokenStreamComponents(source, result);
+    } else {
+      final Tokenizer source = new StandardTokenizer(matchVersion, reader);
+      TokenStream result = new StandardFilter(source);
+      result = new StopFilter(matchVersion, result, stopwords);
+      if(!excltable.isEmpty())
+        result = new KeywordMarkerTokenFilter(result, excltable);
+      result = new FrenchStemFilter(result);
+      // Convert to lowercase after stemming!
+      return new TokenStreamComponents(source, new LowerCaseFilter(matchVersion, result));
+    }
   }
 }
 
Index: contrib/analyzers/common/src/java/org/apache/lucene/analysis/nl/DutchStemmer.java
===================================================================
--- contrib/analyzers/common/src/java/org/apache/lucene/analysis/nl/DutchStemmer.java	(revision 916567)
+++ contrib/analyzers/common/src/java/org/apache/lucene/analysis/nl/DutchStemmer.java	(working copy)
@@ -26,8 +26,10 @@
  * the <a href="http://snowball.tartarus.org/algorithms/dutch/stemmer.html">dutch stemming</a>
  * algorithm in Martin Porter's snowball project.
  * </p>
+ * @deprecated Use {@link org.tartarus.snowball.ext.DutchStemmer} instead, 
+ * which has the same functionality. This filter will be removed in Lucene 4.0
  */
-
+@Deprecated
 public class DutchStemmer {
   /**
    * Buffer for the terms while stemming them.
Index: contrib/analyzers/common/src/java/org/apache/lucene/analysis/nl/DutchStemFilter.java
===================================================================
--- contrib/analyzers/common/src/java/org/apache/lucene/analysis/nl/DutchStemFilter.java	(revision 916567)
+++ contrib/analyzers/common/src/java/org/apache/lucene/analysis/nl/DutchStemFilter.java	(working copy)
@@ -23,8 +23,11 @@
 import java.util.Map;
 import java.util.Set;
 
+import org.apache.lucene.analysis.KeywordMarkerTokenFilter;// for javadoc
 import org.apache.lucene.analysis.TokenFilter;
 import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.analysis.snowball.SnowballFilter;
+import org.apache.lucene.analysis.tokenattributes.KeywordAttribute;
 import org.apache.lucene.analysis.tokenattributes.TermAttribute;
 
 /**
@@ -34,34 +37,58 @@
  * not be stemmed at all. The stemmer used can be changed at runtime after the
  * filter object is created (as long as it is a {@link DutchStemmer}).
  * </p>
+ * <p>
+ * To prevent terms from being stemmed use an instance of
+ * {@link KeywordMarkerTokenFilter} or a custom {@link TokenFilter} that sets
+ * the {@link KeywordAttribute} before this {@link TokenStream}.
+ * </p>
+ * @see KeywordMarkerTokenFilter
+ * @deprecated Use {@link SnowballFilter} with 
+ * {@link org.tartarus.snowball.ext.DutchStemmer} instead, which has the
+ * same functionality. This filter will be removed in Lucene 4.0
  */
+@Deprecated
 public final class DutchStemFilter extends TokenFilter {
   /**
    * The actual token in the input stream.
    */
   private DutchStemmer stemmer = null;
-  private Set exclusions = null;
+  private Set<?> exclusions = null;
   
-  private TermAttribute termAtt;
+  private final TermAttribute termAtt;
+  private final KeywordAttribute keywordAttr;
 
   public DutchStemFilter(TokenStream _in) {
     super(_in);
     stemmer = new DutchStemmer();
     termAtt = addAttribute(TermAttribute.class);
+    keywordAttr = addAttribute(KeywordAttribute.class);
   }
 
   /**
    * Builds a DutchStemFilter that uses an exclusion table.
+   * @deprecated use {@link KeywordAttribute} with {@link KeywordMarkerTokenFilter} instead.
    */
-  public DutchStemFilter(TokenStream _in, Set exclusiontable) {
+  @Deprecated
+  public DutchStemFilter(TokenStream _in, Set<?> exclusiontable) {
     this(_in);
     exclusions = exclusiontable;
   }
+  
+  /**
+   * @param stemdictionary Dictionary of word stem pairs, that overrule the algorithm
+   */
+  public DutchStemFilter(TokenStream _in,  Map<?,?> stemdictionary) {
+    this(_in);
+    stemmer.setStemDictionary(stemdictionary);
+  }
 
   /**
    * @param stemdictionary Dictionary of word stem pairs, that overrule the algorithm
+   * @deprecated use {@link KeywordAttribute} with {@link KeywordMarkerTokenFilter} instead.
    */
-  public DutchStemFilter(TokenStream _in, Set exclusiontable, Map stemdictionary) {
+  @Deprecated
+  public DutchStemFilter(TokenStream _in, Set<?> exclusiontable, Map<?,?> stemdictionary) {
     this(_in, exclusiontable);
     stemmer.setStemDictionary(stemdictionary);
   }
@@ -72,11 +99,11 @@
   @Override
   public boolean incrementToken() throws IOException {
     if (input.incrementToken()) {
-      String term = termAtt.term();
+      final String term = termAtt.term();
 
       // Check the exclusion table.
-      if (exclusions == null || !exclusions.contains(term)) {
-        String s = stemmer.stem(term);
+      if (!keywordAttr.isKeyword() && (exclusions == null || !exclusions.contains(term))) {
+        final String s = stemmer.stem(term);
         // If not stemmed, don't waste the time adjusting the token.
         if ((s != null) && !s.equals(term))
           termAtt.setTermBuffer(s);
@@ -98,8 +125,10 @@
 
   /**
    * Set an alternative exclusion list for this filter.
+   * @deprecated use {@link KeywordAttribute} with {@link KeywordMarkerTokenFilter} instead.
    */
-  public void setExclusionTable(HashSet exclusiontable) {
+  @Deprecated
+  public void setExclusionTable(HashSet<?> exclusiontable) {
     exclusions = exclusiontable;
   }
 
@@ -107,7 +136,7 @@
    * Set dictionary for stemming, this dictionary overrules the algorithm,
    * so you can correct for a particular unwanted word-stem pair.
    */
-  public void setStemDictionary(HashMap dict) {
+  public void setStemDictionary(HashMap<?,?> dict) {
     if (stemmer != null)
       stemmer.setStemDictionary(dict);
   }
Index: contrib/analyzers/common/src/java/org/apache/lucene/analysis/nl/DutchAnalyzer.java
===================================================================
--- contrib/analyzers/common/src/java/org/apache/lucene/analysis/nl/DutchAnalyzer.java	(revision 916567)
+++ contrib/analyzers/common/src/java/org/apache/lucene/analysis/nl/DutchAnalyzer.java	(working copy)
@@ -19,10 +19,15 @@
 
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.CharArraySet;
+import org.apache.lucene.analysis.KeywordMarkerTokenFilter;
+import org.apache.lucene.analysis.LowerCaseFilter;
+import org.apache.lucene.analysis.ReusableAnalyzerBase;
 import org.apache.lucene.analysis.StopFilter;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
 import org.apache.lucene.analysis.WordlistLoader;
+import org.apache.lucene.analysis.miscellaneous.StemmerOverrideFilter;
+import org.apache.lucene.analysis.snowball.SnowballFilter;
 import org.apache.lucene.analysis.standard.StandardFilter;
 import org.apache.lucene.analysis.standard.StandardTokenizer;
 import org.apache.lucene.analysis.standard.StandardAnalyzer;  // for javadoc
@@ -31,7 +36,6 @@
 import java.io.File;
 import java.io.IOException;
 import java.io.Reader;
-import java.util.Arrays;
 import java.util.Collections;
 import java.util.HashMap;
 import java.util.HashSet;
@@ -49,28 +53,31 @@
  * exclusion list is empty by default.
  * </p>
  *
+ * <a name="version"/>
+ * <p>You must specify the required {@link Version}
+ * compatibility when creating DutchAnalyzer:
+ * <ul>
+ *   <li> As of 3.1, Snowball stemming is done with SnowballFilter, 
+ *        LowerCaseFilter is used prior to StopFilter, and Snowball 
+ *        stopwords are used by default.
+ *   <li> As of 2.9, StopFilter preserves position
+ *        increments
+ * </ul>
+ * 
  * <p><b>NOTE</b>: This class uses the same {@link Version}
  * dependent settings as {@link StandardAnalyzer}.</p>
  */
-public final class DutchAnalyzer extends Analyzer {
+public final class DutchAnalyzer extends ReusableAnalyzerBase {
   /**
    * List of typical Dutch stopwords.
    * @deprecated use {@link #getDefaultStopSet()} instead
    */
   @Deprecated
-  public final static String[] DUTCH_STOP_WORDS =
-      {
-        "de", "en", "van", "ik", "te", "dat", "die", "in", "een",
-        "hij", "het", "niet", "zijn", "is", "was", "op", "aan", "met", "als", "voor", "had",
-        "er", "maar", "om", "hem", "dan", "zou", "of", "wat", "mijn", "men", "dit", "zo",
-        "door", "over", "ze", "zich", "bij", "ook", "tot", "je", "mij", "uit", "der", "daar",
-        "haar", "naar", "heb", "hoe", "heeft", "hebben", "deze", "u", "want", "nog", "zal",
-        "me", "zij", "nu", "ge", "geen", "omdat", "iets", "worden", "toch", "al", "waren",
-        "veel", "meer", "doen", "toen", "moet", "ben", "zonder", "kan", "hun", "dus",
-        "alles", "onder", "ja", "eens", "hier", "wie", "werd", "altijd", "doch", "wordt",
-        "wezen", "kunnen", "ons", "zelf", "tegen", "na", "reeds", "wil", "kon", "niets",
-        "uw", "iemand", "geweest", "andere"
-      };
+  public final static String[] DUTCH_STOP_WORDS = getDefaultStopSet().toArray(new String[0]);
+  
+  /** File containing default Dutch stopwords. */
+  public final static String DEFAULT_STOPWORD_FILE = "dutch_stop.txt";
+
   /**
    * Returns an unmodifiable instance of the default stop-words set.
    * @return an unmodifiable instance of the default stop-words set.
@@ -80,9 +87,18 @@
   }
   
   private static class DefaultSetHolder {
-    static final Set<?> DEFAULT_STOP_SET = CharArraySet
-        .unmodifiableSet(new CharArraySet(Version.LUCENE_CURRENT, 
-            Arrays.asList(DUTCH_STOP_WORDS), false));
+    static final Set<?> DEFAULT_STOP_SET;
+
+    static {
+      try {
+        DEFAULT_STOP_SET = WordlistLoader.getSnowballWordSet(SnowballFilter.class, 
+            DEFAULT_STOPWORD_FILE);
+      } catch (IOException ex) {
+        // default set should always be present as it is part of the
+        // distribution (JAR)
+        throw new RuntimeException("Unable to load default stopword set");
+      }
+    }
   }
 
 
@@ -215,50 +231,38 @@
     }
   }
 
+
   /**
-   * Creates a {@link TokenStream} which tokenizes all the text in the 
-   * provided {@link Reader}.
-   *
-   * @return A {@link TokenStream} built from a {@link StandardTokenizer}
-   *   filtered with {@link StandardFilter}, {@link StopFilter}, 
-   *   and {@link DutchStemFilter}
-   */
-  @Override
-  public TokenStream tokenStream(String fieldName, Reader reader) {
-    TokenStream result = new StandardTokenizer(matchVersion, reader);
-    result = new StandardFilter(result);
-    result = new StopFilter(matchVersion, result, stoptable);
-    result = new DutchStemFilter(result, excltable, stemdict);
-    return result;
-  }
-  
-  private class SavedStreams {
-    Tokenizer source;
-    TokenStream result;
-  };
-  
-  /**
    * Returns a (possibly reused) {@link TokenStream} which tokenizes all the 
    * text in the provided {@link Reader}.
    *
    * @return A {@link TokenStream} built from a {@link StandardTokenizer}
-   *   filtered with {@link StandardFilter}, {@link StopFilter}, 
-   *   and {@link DutchStemFilter}
+   *   filtered with {@link StandardFilter}, {@link LowerCaseFilter}, 
+   *   {@link StopFilter}, {@link KeywordMarkerTokenFilter} if a stem exclusion set is provided,
+   *   {@link StemmerOverrideFilter}, and {@link SnowballFilter}
    */
   @Override
-  public TokenStream reusableTokenStream(String fieldName, Reader reader)
-      throws IOException {
-    SavedStreams streams = (SavedStreams) getPreviousTokenStream();
-    if (streams == null) {
-      streams = new SavedStreams();
-      streams.source = new StandardTokenizer(matchVersion, reader);
-      streams.result = new StandardFilter(streams.source);
-      streams.result = new StopFilter(matchVersion, streams.result, stoptable);
-      streams.result = new DutchStemFilter(streams.result, excltable, stemdict);
-      setPreviousTokenStream(streams);
+  protected TokenStreamComponents createComponents(String fieldName,
+      Reader aReader) {
+    if (matchVersion.onOrAfter(Version.LUCENE_31)) {
+      final Tokenizer source = new StandardTokenizer(matchVersion, aReader);
+      TokenStream result = new StandardFilter(source);
+      result = new LowerCaseFilter(matchVersion, result);
+      result = new StopFilter(matchVersion, result, stoptable);
+      if (!excltable.isEmpty())
+        result = new KeywordMarkerTokenFilter(result, excltable);
+      if (!stemdict.isEmpty())
+        result = new StemmerOverrideFilter(matchVersion, result, stemdict);
+      result = new SnowballFilter(result, new org.tartarus.snowball.ext.DutchStemmer());
+      return new TokenStreamComponents(source, result);
     } else {
-      streams.source.reset(reader);
+      final Tokenizer source = new StandardTokenizer(matchVersion, aReader);
+      TokenStream result = new StandardFilter(source);
+      result = new StopFilter(matchVersion, result, stoptable);
+      if (!excltable.isEmpty())
+        result = new KeywordMarkerTokenFilter(result, excltable);
+      result = new DutchStemFilter(result, stemdict);
+      return new TokenStreamComponents(source, result);
     }
-    return streams.result;
   }
 }
Index: contrib/analyzers/common/src/java/org/apache/lucene/analysis/th/ThaiWordFilter.java
===================================================================
--- contrib/analyzers/common/src/java/org/apache/lucene/analysis/th/ThaiWordFilter.java	(revision 916567)
+++ contrib/analyzers/common/src/java/org/apache/lucene/analysis/th/ThaiWordFilter.java	(working copy)
@@ -19,7 +19,6 @@
 import java.io.IOException;
 import java.util.Locale;
 import java.lang.Character.UnicodeBlock;
-import org.apache.lucene.analysis.Token;
 import org.apache.lucene.analysis.TokenFilter;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
Index: contrib/analyzers/common/src/java/org/apache/lucene/analysis/payloads/NumericPayloadTokenFilter.java
===================================================================
--- contrib/analyzers/common/src/java/org/apache/lucene/analysis/payloads/NumericPayloadTokenFilter.java	(revision 916567)
+++ contrib/analyzers/common/src/java/org/apache/lucene/analysis/payloads/NumericPayloadTokenFilter.java	(working copy)
@@ -17,7 +17,6 @@
  */
 
 
-import org.apache.lucene.analysis.Token;
 import org.apache.lucene.analysis.TokenFilter;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.tokenattributes.PayloadAttribute;
Index: contrib/analyzers/common/src/java/org/apache/lucene/analysis/payloads/TypeAsPayloadTokenFilter.java
===================================================================
--- contrib/analyzers/common/src/java/org/apache/lucene/analysis/payloads/TypeAsPayloadTokenFilter.java	(revision 916567)
+++ contrib/analyzers/common/src/java/org/apache/lucene/analysis/payloads/TypeAsPayloadTokenFilter.java	(working copy)
@@ -19,7 +19,6 @@
 
 import org.apache.lucene.analysis.TokenFilter;
 import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.Token;
 import org.apache.lucene.analysis.tokenattributes.PayloadAttribute;
 import org.apache.lucene.analysis.tokenattributes.TypeAttribute;
 import org.apache.lucene.index.Payload;
Index: contrib/analyzers/common/src/java/org/apache/lucene/analysis/payloads/TokenOffsetPayloadTokenFilter.java
===================================================================
--- contrib/analyzers/common/src/java/org/apache/lucene/analysis/payloads/TokenOffsetPayloadTokenFilter.java	(revision 916567)
+++ contrib/analyzers/common/src/java/org/apache/lucene/analysis/payloads/TokenOffsetPayloadTokenFilter.java	(working copy)
@@ -19,7 +19,6 @@
 
 import java.io.IOException;
 
-import org.apache.lucene.analysis.Token;
 import org.apache.lucene.analysis.TokenFilter;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
Index: contrib/analyzers/common/src/java/org/apache/lucene/analysis/tr/package.html
===================================================================
--- contrib/analyzers/common/src/java/org/apache/lucene/analysis/tr/package.html	(revision 916567)
+++ contrib/analyzers/common/src/java/org/apache/lucene/analysis/tr/package.html	(working copy)
@@ -17,15 +17,6 @@
 -->
 <html><head></head>
 <body>
-Support for Turkish.
-<p>
-This package contains just the TokenStream for handling turkish casing,
-for a stemmer please see the snowball package. 
-</p>
-<p>
-WARNING: SnowballAnalyzer uses LowerCaseFilter by default, even when the
-language is set to Turkish, so you will need to construct your own
-analyzer that combines TurkishLowerCaseFilter and SnowballFilter.
-</p>
+Analyzer for Turkish.
 </body>
-</html>
\ No newline at end of file
+</html>
Index: contrib/analyzers/common/src/java/org/apache/lucene/analysis/ru/RussianLetterTokenizer.java
===================================================================
--- contrib/analyzers/common/src/java/org/apache/lucene/analysis/ru/RussianLetterTokenizer.java	(revision 916567)
+++ contrib/analyzers/common/src/java/org/apache/lucene/analysis/ru/RussianLetterTokenizer.java	(working copy)
@@ -21,40 +21,112 @@
 import org.apache.lucene.analysis.CharTokenizer;
 import org.apache.lucene.analysis.Tokenizer; // for javadocs
 import org.apache.lucene.analysis.LetterTokenizer; // for javadocs
+import org.apache.lucene.analysis.standard.StandardTokenizer; // for javadocs
 import org.apache.lucene.util.AttributeSource;
+import org.apache.lucene.util.Version;
 
 /**
  * A RussianLetterTokenizer is a {@link Tokenizer} that extends {@link LetterTokenizer}
- * by also allowing the basic latin digits 0-9. 
+ * by also allowing the basic Latin digits 0-9.
+ * <p>
+ * <a name="version"/>
+ * You must specify the required {@link Version} compatibility when creating
+ * {@link RussianLetterTokenizer}:
+ * <ul>
+ * <li>As of 3.1, {@link CharTokenizer} uses an int based API to normalize and
+ * detect token characters. See {@link CharTokenizer#isTokenChar(int)} and
+ * {@link CharTokenizer#normalize(int)} for details.</li>
+ * </ul>
+ * @deprecated Use {@link StandardTokenizer} instead, which has the same functionality.
+ * This filter will be removed in Lucene 4.0 
  */
-
+@Deprecated
 public class RussianLetterTokenizer extends CharTokenizer
 {    
-    public RussianLetterTokenizer(Reader in)
-    {
-    	super(in);
+    private static final int DIGIT_0 = '0';
+    private static final int DIGIT_9 = '9';
+    
+    /**
+     * Construct a new RussianLetterTokenizer. * @param matchVersion Lucene version
+     * to match See {@link <a href="#version">above</a>}
+     * 
+     * @param in
+     *          the input to split up into tokens
+     */
+    public RussianLetterTokenizer(Version matchVersion, Reader in) {
+      super(matchVersion, in);
     }
 
-    public RussianLetterTokenizer(AttributeSource source, Reader in)
-    {
-        super(source, in);
+    /**
+     * Construct a new RussianLetterTokenizer using a given {@link AttributeSource}.
+     * 
+     * @param matchVersion
+     *          Lucene version to match See {@link <a href="#version">above</a>}
+     * @param source
+     *          the attribute source to use for this {@link Tokenizer}
+     * @param in
+     *          the input to split up into tokens
+     */
+    public RussianLetterTokenizer(Version matchVersion, AttributeSource source, Reader in) {
+      super(matchVersion, source, in);
     }
 
-    public RussianLetterTokenizer(AttributeFactory factory, Reader in)
-    {
-        super(factory, in);
+    /**
+     * Construct a new RussianLetterTokenizer using a given
+     * {@link org.apache.lucene.util.AttributeSource.AttributeFactory}. * @param
+     * matchVersion Lucene version to match See
+     * {@link <a href="#version">above</a>}
+     * 
+     * @param factory
+     *          the attribute factory to use for this {@link Tokenizer}
+     * @param in
+     *          the input to split up into tokens
+     */
+    public RussianLetterTokenizer(Version matchVersion, AttributeFactory factory, Reader in) {
+      super(matchVersion, factory, in);
     }
     
     /**
+     * Construct a new RussianLetterTokenizer.
+     * 
+     * @deprecated use {@link #RussianLetterTokenizer(Version, Reader)} instead. This will
+     *             be removed in Lucene 4.0.
+     */
+    @Deprecated
+    public RussianLetterTokenizer(Reader in) {
+      super(in);
+    }
+
+    /**
+     * Construct a new RussianLetterTokenizer using a given {@link AttributeSource}.
+     * 
+     * @deprecated use {@link #RussianLetterTokenizer(Version, AttributeSource, Reader)}
+     *             instead. This will be removed in Lucene 4.0.
+     */
+    @Deprecated
+    public RussianLetterTokenizer(AttributeSource source, Reader in) {
+      super(source, in);
+    }
+
+    /**
+     * Construct a new RussianLetterTokenizer using a given
+     * {@link org.apache.lucene.util.AttributeSource.AttributeFactory}.
+     * 
+     * @deprecated use {@link #RussianLetterTokenizer(Version, AttributeSource.AttributeFactory, Reader)}
+     *             instead. This will be removed in Lucene 4.0.
+     */
+    @Deprecated
+    public RussianLetterTokenizer(AttributeFactory factory, Reader in) {
+      super(factory, in);
+    }
+    
+    
+    /**
      * Collects only characters which satisfy
-     * {@link Character#isLetter(char)}.
+     * {@link Character#isLetter(int)}.
      */
     @Override
-    protected boolean isTokenChar(char c)
-    {
-        if (Character.isLetter(c) || (c >= '0' && c <= '9'))
-            return true;
-        else
-            return false;
+    protected boolean isTokenChar(int c) {
+        return Character.isLetter(c) || (c >= DIGIT_0 && c <= DIGIT_9);
     }
 }
Index: contrib/analyzers/common/src/java/org/apache/lucene/analysis/ru/RussianStemmer.java
===================================================================
--- contrib/analyzers/common/src/java/org/apache/lucene/analysis/ru/RussianStemmer.java	(revision 916567)
+++ contrib/analyzers/common/src/java/org/apache/lucene/analysis/ru/RussianStemmer.java	(working copy)
@@ -19,7 +19,10 @@
 
 /**
  * Russian stemming algorithm implementation (see http://snowball.sourceforge.net for detailed description).
+ * @deprecated Use {@link org.tartarus.snowball.ext.RussianStemmer} instead, 
+ * which has the same functionality. This filter will be removed in Lucene 4.0
  */
+@Deprecated
 class RussianStemmer
 {
     // positions of RV, R1 and R2 respectively
Index: contrib/analyzers/common/src/java/org/apache/lucene/analysis/ru/RussianStemFilter.java
===================================================================
--- contrib/analyzers/common/src/java/org/apache/lucene/analysis/ru/RussianStemFilter.java	(revision 916567)
+++ contrib/analyzers/common/src/java/org/apache/lucene/analysis/ru/RussianStemFilter.java	(working copy)
@@ -17,11 +17,14 @@
  * limitations under the License.
  */
 
+import org.apache.lucene.analysis.KeywordMarkerTokenFilter;// for javadoc
 import org.apache.lucene.analysis.LowerCaseFilter; // for javadoc
 import org.apache.lucene.analysis.TokenFilter;
 import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.analysis.tokenattributes.KeywordAttribute;
 import org.apache.lucene.analysis.tokenattributes.TermAttribute;
 import org.apache.lucene.analysis.ru.RussianStemmer;//javadoc @link
+import org.apache.lucene.analysis.snowball.SnowballFilter; // javadoc @link
 
 import java.io.IOException;
 
@@ -32,7 +35,17 @@
  * The input should be filtered by {@link LowerCaseFilter} before passing it to RussianStemFilter ,
  * because RussianStemFilter only works with lowercase characters.
  * </p>
+ * <p>
+ * To prevent terms from being stemmed use an instance of
+ * {@link KeywordMarkerTokenFilter} or a custom {@link TokenFilter} that sets
+ * the {@link KeywordAttribute} before this {@link TokenStream}.
+ * </p>
+ * @see KeywordMarkerTokenFilter
+ * @deprecated Use {@link SnowballFilter} with 
+ * {@link org.tartarus.snowball.ext.RussianStemmer} instead, which has the
+ * same functionality. This filter will be removed in Lucene 4.0
  */
+@Deprecated
 public final class RussianStemFilter extends TokenFilter
 {
     /**
@@ -40,13 +53,15 @@
      */
     private RussianStemmer stemmer = null;
 
-    private TermAttribute termAtt;
+    private final TermAttribute termAtt;
+    private final KeywordAttribute keywordAttr;
 
     public RussianStemFilter(TokenStream in)
     {
         super(in);
         stemmer = new RussianStemmer();
         termAtt = addAttribute(TermAttribute.class);
+        keywordAttr = addAttribute(KeywordAttribute.class);
     }
     /**
      * Returns the next token in the stream, or null at EOS
@@ -55,10 +70,12 @@
     public final boolean incrementToken() throws IOException
     {
       if (input.incrementToken()) {
-        String term = termAtt.term();
-        String s = stemmer.stem(term);
-        if (s != null && !s.equals(term))
-          termAtt.setTermBuffer(s);
+        if(!keywordAttr.isKeyword()) {
+          final String term = termAtt.term();
+          final String s = stemmer.stem(term);
+          if (s != null && !s.equals(term))
+            termAtt.setTermBuffer(s);
+        }
         return true;
       } else {
         return false;
Index: contrib/analyzers/common/src/java/org/apache/lucene/analysis/ru/RussianAnalyzer.java
===================================================================
--- contrib/analyzers/common/src/java/org/apache/lucene/analysis/ru/RussianAnalyzer.java	(revision 916567)
+++ contrib/analyzers/common/src/java/org/apache/lucene/analysis/ru/RussianAnalyzer.java	(working copy)
@@ -17,6 +17,7 @@
  * limitations under the License.
  */
 
+import java.io.IOException;
 import java.io.Reader;
 import java.util.Arrays;
 import java.util.Map;
@@ -26,10 +27,15 @@
 import org.apache.lucene.analysis.CharArraySet;
 import org.apache.lucene.analysis.LowerCaseFilter;
 import org.apache.lucene.analysis.ReusableAnalyzerBase.TokenStreamComponents; // javadoc @link
+import org.apache.lucene.analysis.snowball.SnowballFilter;
+import org.apache.lucene.analysis.standard.StandardFilter;
+import org.apache.lucene.analysis.standard.StandardTokenizer;
+import org.apache.lucene.analysis.KeywordMarkerTokenFilter;
 import org.apache.lucene.analysis.StopFilter;
 import org.apache.lucene.analysis.StopwordAnalyzerBase;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
+import org.apache.lucene.analysis.WordlistLoader;
 import org.apache.lucene.util.Version;
 
 /**
@@ -39,13 +45,22 @@
  * will not be indexed at all).
  * A default set of stopwords is used unless an alternative list is specified.
  * </p>
+ * <a name="version"/>
+ * <p>You must specify the required {@link Version}
+ * compatibility when creating RussianAnalyzer:
+ * <ul>
+ *   <li> As of 3.1, StandardTokenizer is used, Snowball stemming is done with
+ *        SnowballFilter, and Snowball stopwords are used by default.
+ * </ul>
  */
 public final class RussianAnalyzer extends StopwordAnalyzerBase
 {
     /**
-     * List of typical Russian stopwords.
+     * List of typical Russian stopwords. (for backwards compatibility)
+     * @deprecated Remove this for LUCENE 4.0
      */
-    private static final String[] RUSSIAN_STOP_WORDS = {
+    @Deprecated
+    private static final String[] RUSSIAN_STOP_WORDS_30 = {
       "а", "без", "более", "бы", "был", "была", "были", "было", "быть", "в",
       "вам", "вас", "весь", "во", "вот", "все", "всего", "всех", "вы", "где", 
       "да", "даже", "для", "до", "его", "ее", "ей", "ею", "если", "есть", 
@@ -58,14 +73,44 @@
       "чем", "что", "чтобы", "чье", "чья", "эта", "эти", "это", "я"
     };
     
+    /** File containing default Russian stopwords. */
+    public final static String DEFAULT_STOPWORD_FILE = "russian_stop.txt";
+    
     private static class DefaultSetHolder {
-      static final Set<?> DEFAULT_STOP_SET = CharArraySet
+      /** @deprecated remove this for Lucene 4.0 */
+      @Deprecated
+      static final Set<?> DEFAULT_STOP_SET_30 = CharArraySet
           .unmodifiableSet(new CharArraySet(Version.LUCENE_CURRENT, 
-              Arrays.asList(RUSSIAN_STOP_WORDS), false));
+              Arrays.asList(RUSSIAN_STOP_WORDS_30), false));
+      static final Set<?> DEFAULT_STOP_SET;
+      
+      static {
+        try {
+          DEFAULT_STOP_SET = 
+            WordlistLoader.getSnowballWordSet(SnowballFilter.class, DEFAULT_STOPWORD_FILE);
+        } catch (IOException ex) {
+          // default set should always be present as it is part of the
+          // distribution (JAR)
+          throw new RuntimeException("Unable to load default stopword set");
+        }
+      }
     }
+    
+    private final Set<?> stemExclusionSet;
+    
+    /**
+     * Returns an unmodifiable instance of the default stop-words set.
+     * 
+     * @return an unmodifiable instance of the default stop-words set.
+     */
+    public static Set<?> getDefaultStopSet() {
+      return DefaultSetHolder.DEFAULT_STOP_SET;
+    }
 
     public RussianAnalyzer(Version matchVersion) {
-      this(matchVersion, DefaultSetHolder.DEFAULT_STOP_SET);
+      this(matchVersion,
+        matchVersion.onOrAfter(Version.LUCENE_31) ? DefaultSetHolder.DEFAULT_STOP_SET
+            : DefaultSetHolder.DEFAULT_STOP_SET_30);
     }
   
     /**
@@ -86,9 +131,24 @@
      *          a stopword set
      */
     public RussianAnalyzer(Version matchVersion, Set<?> stopwords){
+      this(matchVersion, stopwords, CharArraySet.EMPTY_SET);
+    }
+    
+    /**
+     * Builds an analyzer with the given stop words
+     * 
+     * @param matchVersion
+     *          lucene compatibility version
+     * @param stopwords
+     *          a stopword set
+     * @param stemExclusionSet a set of words not to be stemmed
+     */
+    public RussianAnalyzer(Version matchVersion, Set<?> stopwords, Set<?> stemExclusionSet){
       super(matchVersion, stopwords);
+      this.stemExclusionSet = CharArraySet.unmodifiableSet(CharArraySet.copy(matchVersion, stemExclusionSet));
     }
    
+   
     /**
      * Builds an analyzer with the given stop words.
      * TODO: create a Set version of this ctor
@@ -105,17 +165,30 @@
      * provided {@link Reader}.
      *
      * @return {@link TokenStreamComponents} built from a 
-     *   {@link RussianLetterTokenizer} filtered with 
+     *   {@link StandardTokenizer} filtered with {@link StandardFilter},
      *   {@link LowerCaseFilter}, {@link StopFilter}, 
-     *   and {@link RussianStemFilter}
+     *   {@link KeywordMarkerTokenFilter} if a stem exclusion set is provided,
+     *   and {@link SnowballFilter}
      */
     @Override
     protected TokenStreamComponents createComponents(String fieldName,
         Reader reader) {
-      final Tokenizer source = new RussianLetterTokenizer(reader);
-      TokenStream result = new LowerCaseFilter(matchVersion, source);
-      result = new StopFilter(matchVersion, result, stopwords);
-      return new TokenStreamComponents(source, new RussianStemFilter(result));
-      
+      if (matchVersion.onOrAfter(Version.LUCENE_31)) {
+        final Tokenizer source = new StandardTokenizer(matchVersion, reader);
+        TokenStream result = new StandardFilter(source);
+        result = new LowerCaseFilter(matchVersion, result);
+        result = new StopFilter(matchVersion, result, stopwords);
+        if (!stemExclusionSet.isEmpty()) result = new KeywordMarkerTokenFilter(
+            result, stemExclusionSet);
+        result = new SnowballFilter(result, new org.tartarus.snowball.ext.RussianStemmer());
+        return new TokenStreamComponents(source, result);
+      } else {
+        final Tokenizer source = new RussianLetterTokenizer(matchVersion, reader);
+        TokenStream result = new LowerCaseFilter(matchVersion, source);
+        result = new StopFilter(matchVersion, result, stopwords);
+        if (!stemExclusionSet.isEmpty()) result = new KeywordMarkerTokenFilter(
+          result, stemExclusionSet);
+        return new TokenStreamComponents(source, new RussianStemFilter(result));
+      }
     }
 }
Index: contrib/analyzers/common/src/java/org/apache/lucene/analysis/shingle/ShingleFilter.java
===================================================================
--- contrib/analyzers/common/src/java/org/apache/lucene/analysis/shingle/ShingleFilter.java	(revision 916567)
+++ contrib/analyzers/common/src/java/org/apache/lucene/analysis/shingle/ShingleFilter.java	(working copy)
@@ -18,19 +18,16 @@
  */
 
 import java.io.IOException;
-import java.util.Iterator;
 import java.util.LinkedList;
 
-import org.apache.lucene.analysis.Token;
 import org.apache.lucene.analysis.TokenFilter;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
 import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;
 import org.apache.lucene.analysis.tokenattributes.TermAttribute;
 import org.apache.lucene.analysis.tokenattributes.TypeAttribute;
-import org.apache.lucene.util.AttributeSource;
-import org.apache.lucene.util.AttributeSource.State;
 
+
 /**
  * <p>A ShingleFilter constructs shingles (token n-grams) from a token stream.
  * In other words, it creates combinations of tokens as a single token.
@@ -44,27 +41,60 @@
  */
 public final class ShingleFilter extends TokenFilter {
 
-  private LinkedList<State> shingleBuf = new LinkedList<State>();
-  private StringBuilder[] shingles;
-  private String tokenType = "shingle";
-
   /**
    * filler token for when positionIncrement is more than 1
    */
   public static final char[] FILLER_TOKEN = { '_' };
 
-
   /**
    * default maximum shingle size is 2.
    */
   public static final int DEFAULT_MAX_SHINGLE_SIZE = 2;
 
   /**
-   * The string to use when joining adjacent tokens to form a shingle
+   * default minimum shingle size is 2.
    */
+  public static final int DEFAULT_MIN_SHINGLE_SIZE = 2;
+
+  /**
+   * default token type attribute value is "shingle" 
+   */
+  public static final String DEFAULT_TOKEN_TYPE = "shingle";
+  
+  /**
+   * The default string to use when joining adjacent tokens to form a shingle
+   */
   public static final String TOKEN_SEPARATOR = " ";
 
+
   /**
+   * The sequence of input stream tokens (or filler tokens, if necessary)
+   * that will be composed to form output shingles.
+   */
+  private LinkedList<State> inputWindow = new LinkedList<State>();
+  
+  /**
+   * The number of input tokens in the next output token.  This is the "n" in
+   * "token n-grams".
+   */
+  private CircularSequence gramSize;
+
+  /**
+   * Shingle text is composed here.
+   */
+  private StringBuilder shingleBuilder = new StringBuilder();
+
+  /**
+   * The token type attribute value to use - default is "shingle"
+   */
+  private String tokenType = DEFAULT_TOKEN_TYPE;
+
+  /**
+   * The string to use when joining adjacent tokens to form a shingle
+   */
+  private String tokenSeparator = TOKEN_SEPARATOR;
+
+  /**
    * By default, we output unigrams (individual tokens) as well as shingles
    * (token n-grams).
    */
@@ -76,15 +106,40 @@
   private int maxShingleSize;
 
   /**
-   * Constructs a ShingleFilter with the specified single size from the
+   * minimum shingle size (number of tokens)
+   */
+  private int minShingleSize;
+
+  /**
+   * The remaining number of filler tokens inserted into the input stream
+   * from which shingles are composed, to handle position increments greater
+   * than one.
+   */
+  private int numFillerTokensToInsert;
+
+  /**
+   * The next input stream token.
+   */
+  private State nextInputStreamToken;
+  
+  private final TermAttribute termAtt;
+  private final OffsetAttribute offsetAtt;
+  private final PositionIncrementAttribute posIncrAtt;
+  private final TypeAttribute typeAtt;
+
+
+  /**
+   * Constructs a ShingleFilter with the specified shingle size from the
    * {@link TokenStream} <code>input</code>
    *
    * @param input input stream
+   * @param minShingleSize minimum shingle size produced by the filter.
    * @param maxShingleSize maximum shingle size produced by the filter.
    */
-  public ShingleFilter(TokenStream input, int maxShingleSize) {
+  public ShingleFilter(TokenStream input, int minShingleSize, int maxShingleSize) {
     super(input);
     setMaxShingleSize(maxShingleSize);
+    setMinShingleSize(minShingleSize);
     this.termAtt = addAttribute(TermAttribute.class);
     this.offsetAtt = addAttribute(OffsetAttribute.class);
     this.posIncrAtt = addAttribute(PositionIncrementAttribute.class);
@@ -92,22 +147,34 @@
   }
 
   /**
-   * Construct a ShingleFilter with default shingle size.
+   * Constructs a ShingleFilter with the specified shingle size from the
+   * {@link TokenStream} <code>input</code>
    *
    * @param input input stream
+   * @param maxShingleSize maximum shingle size produced by the filter.
    */
+  public ShingleFilter(TokenStream input, int maxShingleSize) {
+    this(input, DEFAULT_MIN_SHINGLE_SIZE, maxShingleSize);
+  }
+  
+  /**
+   * Construct a ShingleFilter with default shingle size: 2.
+   *
+   * @param input input stream
+   */
   public ShingleFilter(TokenStream input) {
-    this(input, DEFAULT_MAX_SHINGLE_SIZE);
+    this(input, DEFAULT_MIN_SHINGLE_SIZE, DEFAULT_MAX_SHINGLE_SIZE);
   }
 
   /**
-   * Construct a ShingleFilter with the specified token type for shingle tokens.
+   * Construct a ShingleFilter with the specified token type for shingle tokens
+   * and the default shingle size: 2
    *
    * @param input input stream
    * @param tokenType token type for shingle tokens
    */
   public ShingleFilter(TokenStream input, String tokenType) {
-    this(input, DEFAULT_MAX_SHINGLE_SIZE);
+    this(input, DEFAULT_MIN_SHINGLE_SIZE, DEFAULT_MAX_SHINGLE_SIZE);
     setTokenType(tokenType);
   }
 
@@ -130,6 +197,7 @@
    */
   public void setOutputUnigrams(boolean outputUnigrams) {
     this.outputUnigrams = outputUnigrams;
+    gramSize = new CircularSequence();
   }
 
   /**
@@ -141,203 +209,239 @@
     if (maxShingleSize < 2) {
       throw new IllegalArgumentException("Max shingle size must be >= 2");
     }
-    shingles = new StringBuilder[maxShingleSize];
-    for (int i = 0; i < shingles.length; i++) {
-      shingles[i] = new StringBuilder();
-    }
     this.maxShingleSize = maxShingleSize;
   }
 
   /**
-   * Clear the StringBuilders that are used for storing the output shingles.
+   * <p>Set the min shingle size (default: 2).
+   * <p>This method requires that the passed in minShingleSize is not greater
+   * than maxShingleSize, so make sure that maxShingleSize is set before
+   * calling this method.
+   * <p>The unigram output option is independent of the min shingle size.
+   *
+   * @param minShingleSize min size of output shingles
    */
-  private void clearShingles() {
-    for (int i = 0; i < shingles.length; i++) {
-      shingles[i].setLength(0);
+  public void setMinShingleSize(int minShingleSize) {
+    if (minShingleSize < 2) {
+      throw new IllegalArgumentException("Min shingle size must be >= 2");
     }
+    if (minShingleSize > maxShingleSize) {
+      throw new IllegalArgumentException
+        ("Min shingle size must be <= max shingle size");
+    }
+    this.minShingleSize = minShingleSize;
+    gramSize = new CircularSequence();
   }
-  
-  private AttributeSource.State nextToken;
-  private int shingleBufferPosition;
-  private int[] endOffsets;
 
+  /**
+   * Sets the string to use when joining adjacent tokens to form a shingle
+   * @param tokenSeparator used to separate input stream tokens in output shingles
+   */
+  public void setTokenSeparator(String tokenSeparator) {
+    this.tokenSeparator = null == tokenSeparator ? "" : tokenSeparator;
+  }
+
   /* (non-Javadoc)
    * @see org.apache.lucene.analysis.TokenStream#next()
    */
   @Override
   public final boolean incrementToken() throws IOException {
-    while (true) {
-      if (nextToken == null) {
-        if (!fillShingleBuffer()) {
-          return false;
-        }
+    boolean tokenAvailable = false; 
+    if (gramSize.atMinValue() || inputWindow.size() < gramSize.getValue()) {
+      shiftInputWindow();
+    }
+    if ( ! inputWindow.isEmpty()) {
+      restoreState(inputWindow.getFirst());
+      if (1 == gramSize.getValue()) {
+        posIncrAtt.setPositionIncrement(1);
+        gramSize.advance();
+        tokenAvailable = true;
+      } else if (inputWindow.size() >= gramSize.getValue()) {
+        getNextShingle();
+        gramSize.advance();
+        tokenAvailable = true;
       }
-      
-      nextToken = shingleBuf.getFirst();
-      
-      if (outputUnigrams) {
-        if (shingleBufferPosition == 0) {
-          restoreState(nextToken);
-          posIncrAtt.setPositionIncrement(1);
-          shingleBufferPosition++;
-          return true;
-        }
-      } else if (shingleBufferPosition % this.maxShingleSize == 0){
-        shingleBufferPosition++;
+    }
+    return tokenAvailable;
+  }
+
+  /**
+   * <p>Makes the next token a shingle of length {@link #gramSize}, 
+   * composed of tokens taken from {@link #inputWindow}.
+   * <p>Callers of this method must first insure that there are at least 
+   * <code>gramSize</code> tokens available in <code>inputWindow</code>.
+   */
+  private void getNextShingle() {
+    int startOffset = offsetAtt.startOffset();
+
+    int minTokNum = gramSize.getValue() - 1; // zero-based inputWindow position
+    if (gramSize.getValue() == minShingleSize) {
+      // Clear the shingle text buffer if this is the first shingle
+      // at the current position in the input stream.
+      shingleBuilder.setLength(0);
+      minTokNum = 0;
+    }
+    for (int tokNum = minTokNum ; tokNum < gramSize.getValue() ; ++tokNum) {
+      if (tokNum > 0) {
+        shingleBuilder.append(tokenSeparator);
       }
-  
-      if (shingleBufferPosition < shingleBuf.size()) {
-        restoreState(nextToken);
-        typeAtt.setType(tokenType);
-        offsetAtt.setOffset(offsetAtt.startOffset(), endOffsets[shingleBufferPosition]);
-        StringBuilder buf = shingles[shingleBufferPosition];
-        int termLength = buf.length();
-        char[] termBuffer = termAtt.termBuffer();
-        if (termBuffer.length < termLength)
-          termBuffer = termAtt.resizeTermBuffer(termLength);
-        buf.getChars(0, termLength, termBuffer, 0);
-        termAtt.setTermLength(termLength);
-        if ((! outputUnigrams) && shingleBufferPosition % this.maxShingleSize == 1) {
-          posIncrAtt.setPositionIncrement(1);
-        } else {
-          posIncrAtt.setPositionIncrement(0);
-        }
-        shingleBufferPosition++;
-        if (shingleBufferPosition == shingleBuf.size()) {
-          nextToken = null;
-          shingleBufferPosition = 0;
-        }
-        return true;
-      } else {
-        nextToken = null;
-        shingleBufferPosition = 0;
-      }
+      restoreState(inputWindow.get(tokNum));
+      shingleBuilder.append(termAtt.termBuffer(), 0, termAtt.termLength());
     }
+    char[] termBuffer = termAtt.termBuffer();
+    int termLength = shingleBuilder.length();
+    if (termBuffer.length < termLength) {
+      termBuffer = termAtt.resizeTermBuffer(termLength);
+    }
+    shingleBuilder.getChars(0, termLength, termBuffer, 0);
+    termAtt.setTermLength(termLength);
+    posIncrAtt.setPositionIncrement(gramSize.atMinValue() ? 1 : 0);
+    typeAtt.setType(tokenType);
+    offsetAtt.setOffset(startOffset, offsetAtt.endOffset());
   }
   
-  private int numFillerTokensToInsert;
-  private AttributeSource.State currentToken;
-  private boolean hasCurrentToken;
-   
-  private TermAttribute termAtt;
-  private OffsetAttribute offsetAtt;
-  private PositionIncrementAttribute posIncrAtt;
-  private TypeAttribute typeAtt;
-  
   /**
-   * Get the next token from the input stream and push it on the token buffer.
-   * If we encounter a token with position increment > 1, we put filler tokens
-   * on the token buffer.
-   * <p/>
-   * Returns null when the end of the input stream is reached.
-   * @return the next token, or null if at end of input stream
+   * <p>Get the next token from the input stream.
+   * <p>If the next token has <code>positionIncrement > 1</code>,
+   * <code>positionIncrement - 1</code> {@link #FILLER_TOKEN}s are
+   * inserted first.
+   * @return false for end of stream; true otherwise
    * @throws IOException if the input stream has a problem
    */
   private boolean getNextToken() throws IOException {
-    
-    while (true) {
-  	  if (numFillerTokensToInsert > 0) {
-  	    if (currentToken == null) {
-  	      currentToken = captureState();
-  	    } else {
-  	      restoreState(currentToken);
-  	    }
-  	    numFillerTokensToInsert--;
-        // A filler token occupies no space
-  	    offsetAtt.setOffset(offsetAtt.startOffset(), offsetAtt.startOffset());
-  	    termAtt.setTermBuffer(FILLER_TOKEN, 0, FILLER_TOKEN.length);
-        return true;
-  	  } 
-  	  
-  	  if (hasCurrentToken) {
-  	    if (currentToken != null) {
-  	      restoreState(currentToken);
-  	      currentToken = null;
-  	    }
-  	    hasCurrentToken = false;
-  	    return true;
-  	  }
-  	  
-  	  if (!input.incrementToken()) return false;
-  	  hasCurrentToken = true;
-  	  
-  	  if (posIncrAtt.getPositionIncrement() > 1) {
-  	    numFillerTokensToInsert = posIncrAtt.getPositionIncrement() - 1;
-  	  }
+    boolean success = false;
+    if (numFillerTokensToInsert > 0) {
+      insertFillerToken();
+      success = true;
+    } else if (null != nextInputStreamToken) {
+      restoreState(nextInputStreamToken);
+      nextInputStreamToken = null;
+      success = true;
+    } else if (input.incrementToken()) {
+      if (posIncrAtt.getPositionIncrement() > 1) {
+        numFillerTokensToInsert = posIncrAtt.getPositionIncrement() - 1;
+        insertFillerToken();
+      }
+      success = true;
     }
+    return success;
 	}
 
   /**
-   * Fill the output buffer with new shingles.
+   * Inserts a {@link #FILLER_TOKEN} and decrements
+   * {@link #numFillerTokensToInsert}.
+   */
+  private void insertFillerToken() {
+    if (null == nextInputStreamToken) {
+      nextInputStreamToken = captureState();
+    } else {
+      restoreState(nextInputStreamToken);
+    }
+    --numFillerTokensToInsert;
+    // A filler token occupies no space
+    offsetAtt.setOffset(offsetAtt.startOffset(), offsetAtt.startOffset());
+    termAtt.setTermBuffer(FILLER_TOKEN, 0, FILLER_TOKEN.length);
+  }
+
+  /**
+   * <p>Fills {@link #inputWindow} with input stream tokens, if available, 
+   * shifting to the right if the window was previously full.
+   * <p>Resets {@link #gramSize} to its minimum value.
    *
    * @throws IOException if there's a problem getting the next token
    */
-  private boolean fillShingleBuffer() throws IOException {
-    boolean addedToken = false;
-    /*
-     * Try to fill the shingle buffer.
-     */
-    do {
-      if (getNextToken()) {
-        shingleBuf.add(captureState());
-        if (shingleBuf.size() > maxShingleSize)
-        {
-          shingleBuf.removeFirst();
-        }
-        addedToken = true;
-      } else {
+  private void shiftInputWindow() throws IOException {
+    if (inputWindow.size() > 0) {
+      inputWindow.removeFirst();
+    }
+    while (getNextToken()) {
+      inputWindow.add(captureState());
+      if (inputWindow.size() == maxShingleSize) {
         break;
       }
-    } while (shingleBuf.size() < maxShingleSize);
+    }
+    gramSize.reset();
+  }
 
-    if (shingleBuf.isEmpty()) {
-      return false;
+  @Override
+  public void reset() throws IOException {
+    super.reset();
+    gramSize.reset();
+    inputWindow.clear();
+    numFillerTokensToInsert = 0;
+  }
+
+
+  /**
+   * <p>An instance of this class is used to maintain the number of input
+   * stream tokens that will be used to compose the next unigram or shingle:
+   * {@link #gramSize}.
+   * <p><code>gramSize</code> will take on values from the circular sequence
+   * <b>{ [ 1, ] {@link #minShingleSize} [ , ... , {@link #maxShingleSize} ] }</b>.
+   * <p>1 is included in the circular sequence only if 
+   * {@link #outputUnigrams} = true.
+   */
+  private class CircularSequence {
+    private int value;
+    private int minValue;
+    
+    public CircularSequence() {
+      minValue = outputUnigrams ? 1 : minShingleSize;
+      reset();
     }
-    
-    /*
-     * If no new token could be added to the shingle buffer, we have reached
-     * the end of the input stream and have to discard the least recent token.
+
+    /**
+     * {@see #advance()}
+     * @return the current value.  
      */
-    if (! addedToken) {
-      shingleBuf.removeFirst();
+    public int getValue() {
+      return value;
     }
     
-    if (shingleBuf.isEmpty()) {
-      return false;
+    /**
+     * <p>Increments this circular number's value to the next member in the
+     * circular sequence
+     * <code>gramSize</code> will take on values from the circular sequence
+     * <b>{ [ 1, ] {@link #minShingleSize} [ , ... , {@link #maxShingleSize} ] }</b>.
+     * <p>1 is included in the circular sequence only if 
+     * {@link #outputUnigrams} = true.
+     * 
+     * @return the next member in the circular sequence
+     */
+    public int advance() {
+      if (value == 1) {
+        value = minShingleSize;
+      } else if (value == maxShingleSize) {
+        reset();
+      } else {
+        ++value;
+      }
+      return value;
     }
 
-    clearShingles();
-
-    endOffsets = new int[shingleBuf.size()];
-    for (int i = 0; i < endOffsets.length; i++) {
-      endOffsets[i] = 0;
+    /**
+     * <p>Sets this circular number's value to the first member of the 
+     * circular sequence
+     * <p><code>gramSize</code> will take on values from the circular sequence
+     * <b>{ [ 1, ] {@link #minShingleSize} [ , ... , {@link #maxShingleSize} ] }</b>.
+     * <p>1 is included in the circular sequence only if 
+     * {@link #outputUnigrams} = true.
+     */
+    public void reset() {
+      value = minValue;
     }
 
-    int i = 0;
-    for (Iterator<State> it = shingleBuf.iterator(); it.hasNext(); ) {
-      restoreState(it.next());
-      for (int j = i; j < shingles.length; j++) {
-        if (shingles[j].length() != 0) {
-          shingles[j].append(TOKEN_SEPARATOR);
-        }
-        shingles[j].append(termAtt.termBuffer(), 0, termAtt.termLength());
-      }
-
-      endOffsets[i] = offsetAtt.endOffset();
-      i++;
+    /**
+     * <p>Returns true if the current value is the first member of the circular
+     * sequence.
+     * <p>If {@link #outputUnigrams} = true, the first member of the circular
+     * sequence will be 1; otherwise, it will be {@link #minShingleSize}.
+     * 
+     * @return true if the current value is the first member of the circular
+     *  sequence; false otherwise
+     */
+    public boolean atMinValue() {
+      return value == minValue;
     }
-    
-    return true;
   }
-
-  @Override
-  public void reset() throws IOException {
-    super.reset();
-    nextToken = null;
-    shingleBufferPosition = 0;
-    shingleBuf.clear();
-    numFillerTokensToInsert = 0;
-    currentToken = null;
-    hasCurrentToken = false;
-  }
 }
Index: contrib/analyzers/common/src/java/org/apache/lucene/analysis/shingle/ShingleAnalyzerWrapper.java
===================================================================
--- contrib/analyzers/common/src/java/org/apache/lucene/analysis/shingle/ShingleAnalyzerWrapper.java	(revision 916567)
+++ contrib/analyzers/common/src/java/org/apache/lucene/analysis/shingle/ShingleAnalyzerWrapper.java	(working copy)
@@ -34,7 +34,9 @@
 public final class ShingleAnalyzerWrapper extends Analyzer {
 
   private final Analyzer defaultAnalyzer;
-  private int maxShingleSize = 2;
+  private int maxShingleSize = ShingleFilter.DEFAULT_MAX_SHINGLE_SIZE;
+  private int minShingleSize = ShingleFilter.DEFAULT_MIN_SHINGLE_SIZE;
+  private String tokenSeparator = ShingleFilter.TOKEN_SEPARATOR;
   private boolean outputUnigrams = true;
 
   public ShingleAnalyzerWrapper(Analyzer defaultAnalyzer) {
@@ -44,9 +46,15 @@
 
   public ShingleAnalyzerWrapper(Analyzer defaultAnalyzer, int maxShingleSize) {
     this(defaultAnalyzer);
-    this.maxShingleSize = maxShingleSize;
+    setMaxShingleSize(maxShingleSize);
   }
 
+  public ShingleAnalyzerWrapper(Analyzer defaultAnalyzer, int minShingleSize, int maxShingleSize) {
+    this(defaultAnalyzer);
+    setMaxShingleSize(maxShingleSize);
+    setMinShingleSize(minShingleSize);
+  }
+
   /**
    * Wraps {@link StandardAnalyzer}. 
    */
@@ -58,29 +66,73 @@
   /**
    * Wraps {@link StandardAnalyzer}. 
    */
-  public ShingleAnalyzerWrapper(Version matchVersion, int nGramSize) {
+  public ShingleAnalyzerWrapper(Version matchVersion, int minShingleSize, int maxShingleSize) {
     this(matchVersion);
-    this.maxShingleSize = nGramSize;
+    setMaxShingleSize(maxShingleSize);
+    setMinShingleSize(minShingleSize);
   }
 
   /**
-   * The max shingle (ngram) size
+   * The max shingle (token ngram) size
    * 
-   * @return The max shingle (ngram) size
+   * @return The max shingle (token ngram) size
    */
   public int getMaxShingleSize() {
     return maxShingleSize;
   }
 
   /**
-   * Set the maximum size of output shingles
-   * 
+   * Set the maximum size of output shingles (default: 2)
+   *
    * @param maxShingleSize max shingle size
    */
   public void setMaxShingleSize(int maxShingleSize) {
+    if (maxShingleSize < 2) {
+      throw new IllegalArgumentException("Max shingle size must be >= 2");
+    }
     this.maxShingleSize = maxShingleSize;
   }
 
+  /**
+   * The min shingle (token ngram) size
+   * 
+   * @return The min shingle (token ngram) size
+   */
+  public int getMinShingleSize() {
+    return minShingleSize;
+  }
+
+  /**
+   * <p>Set the min shingle size (default: 2).
+   * <p>This method requires that the passed in minShingleSize is not greater
+   * than maxShingleSize, so make sure that maxShingleSize is set before
+   * calling this method.
+   *
+   * @param minShingleSize min size of output shingles
+   */
+  public void setMinShingleSize(int minShingleSize) {
+    if (minShingleSize < 2) {
+      throw new IllegalArgumentException("Min shingle size must be >= 2");
+    }
+    if (minShingleSize > maxShingleSize) {
+      throw new IllegalArgumentException
+        ("Min shingle size must be <= max shingle size");
+    }
+    this.minShingleSize = minShingleSize;
+  }
+
+  public String getTokenSeparator() {
+    return tokenSeparator;
+  }
+
+  /**
+   * Sets the string to use when joining adjacent tokens to form a shingle
+   * @param tokenSeparator used to separate input stream tokens in output shingles
+   */
+  public void setTokenSeparator(String tokenSeparator) {
+    this.tokenSeparator = (tokenSeparator == null ? "" : tokenSeparator);
+  }
+  
   public boolean isOutputUnigrams() {
     return outputUnigrams;
   }
@@ -104,8 +156,10 @@
     } catch (IOException e) {
       wrapped = defaultAnalyzer.tokenStream(fieldName, reader);
     }
-    ShingleFilter filter = new ShingleFilter(wrapped);
+    ShingleFilter filter = new ShingleFilter(wrapped, minShingleSize, maxShingleSize);
+    filter.setMinShingleSize(minShingleSize);
     filter.setMaxShingleSize(maxShingleSize);
+    filter.setTokenSeparator(tokenSeparator);
     filter.setOutputUnigrams(outputUnigrams);
     return filter;
   }
@@ -113,7 +167,7 @@
   private class SavedStreams {
     TokenStream wrapped;
     ShingleFilter shingle;
-  };
+  }
   
   @Override
   public TokenStream reusableTokenStream(String fieldName, Reader reader) throws IOException {
@@ -135,6 +189,8 @@
       }
     }
     streams.shingle.setMaxShingleSize(maxShingleSize);
+    streams.shingle.setMinShingleSize(minShingleSize);
+    streams.shingle.setTokenSeparator(tokenSeparator);
     streams.shingle.setOutputUnigrams(outputUnigrams);
     return streams.shingle;
   }
Index: contrib/analyzers/common/src/java/org/apache/lucene/analysis/shingle/ShingleMatrixFilter.java
===================================================================
--- contrib/analyzers/common/src/java/org/apache/lucene/analysis/shingle/ShingleMatrixFilter.java	(revision 916567)
+++ contrib/analyzers/common/src/java/org/apache/lucene/analysis/shingle/ShingleMatrixFilter.java	(working copy)
@@ -377,6 +377,7 @@
     } while (token == request_next_token);
     if (token == null) return false;
 
+    clearAttributes();
     termAtt.setTermBuffer(token.termBuffer(), 0, token.termLength());
     posIncrAtt.setPositionIncrement(token.getPositionIncrement());
     flagsAtt.setFlags(token.getFlags());
Index: contrib/analyzers/common/src/java/org/apache/lucene/analysis/el/GreekAnalyzer.java
===================================================================
--- contrib/analyzers/common/src/java/org/apache/lucene/analysis/el/GreekAnalyzer.java	(revision 916567)
+++ contrib/analyzers/common/src/java/org/apache/lucene/analysis/el/GreekAnalyzer.java	(working copy)
@@ -24,6 +24,7 @@
 import org.apache.lucene.analysis.StopwordAnalyzerBase;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
+import org.apache.lucene.analysis.standard.StandardFilter;
 import org.apache.lucene.analysis.standard.StandardTokenizer;
 import org.apache.lucene.analysis.standard.StandardAnalyzer;  // for javadoc
 import org.apache.lucene.util.Version;
@@ -41,6 +42,15 @@
  * A default set of stopwords is used unless an alternative list is specified.
  * </p>
  *
+ * <a name="version"/>
+ * <p>You must specify the required {@link Version}
+ * compatibility when creating GreekAnalyzer:
+ * <ul>
+ *   <li> As of 3.1, StandardFilter is used by default.
+ *   <li> As of 2.9, StopFilter preserves position
+ *        increments
+ * </ul>
+ * 
  * <p><b>NOTE</b>: This class uses the same {@link Version}
  * dependent settings as {@link StandardAnalyzer}.</p>
  */
@@ -117,13 +127,15 @@
     * 
     * @return {@link TokenStreamComponents} built from a
     *         {@link StandardTokenizer} filtered with
-    *         {@link GreekLowerCaseFilter} and {@link StopFilter}
+    *         {@link GreekLowerCaseFilter}, {@link StandardFilter} and {@link StopFilter}
     */
     @Override
     protected TokenStreamComponents createComponents(String fieldName,
         Reader reader) {
       final Tokenizer source = new StandardTokenizer(matchVersion, reader);
-      final TokenStream result = new GreekLowerCaseFilter(source);
+      TokenStream result = new GreekLowerCaseFilter(source);
+      if (matchVersion.onOrAfter(Version.LUCENE_31))
+        result = new StandardFilter(result);
       return new TokenStreamComponents(source, new StopFilter(matchVersion, result, stopwords));
     }
 }
Index: contrib/analyzers/common/src/java/org/apache/lucene/analysis/cn/ChineseAnalyzer.java
===================================================================
--- contrib/analyzers/common/src/java/org/apache/lucene/analysis/cn/ChineseAnalyzer.java	(revision 916567)
+++ contrib/analyzers/common/src/java/org/apache/lucene/analysis/cn/ChineseAnalyzer.java	(working copy)
@@ -21,15 +21,17 @@
 
 import org.apache.lucene.analysis.ReusableAnalyzerBase;
 import org.apache.lucene.analysis.ReusableAnalyzerBase.TokenStreamComponents; // javadoc @link
+import org.apache.lucene.analysis.standard.StandardAnalyzer; // javadoc @link
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.Tokenizer;
 
 /**
  * An {@link Analyzer} that tokenizes text with {@link ChineseTokenizer} and
  * filters with {@link ChineseFilter}
- *
+ * @deprecated Use {@link StandardAnalyzer} instead, which has the same functionality.
+ * This analyzer will be removed in Lucene 4.0
  */
-
+@Deprecated
 public final class ChineseAnalyzer extends ReusableAnalyzerBase {
 
   /**
Index: contrib/analyzers/common/src/java/org/apache/lucene/analysis/cn/ChineseFilter.java
===================================================================
--- contrib/analyzers/common/src/java/org/apache/lucene/analysis/cn/ChineseFilter.java	(revision 916567)
+++ contrib/analyzers/common/src/java/org/apache/lucene/analysis/cn/ChineseFilter.java	(working copy)
@@ -23,6 +23,7 @@
 import org.apache.lucene.analysis.CharArraySet;
 import org.apache.lucene.analysis.TokenFilter;
 import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.analysis.StopFilter;
 import org.apache.lucene.analysis.tokenattributes.TermAttribute;
 import org.apache.lucene.util.Version;
 
@@ -41,9 +42,10 @@
  * </ol>
  * 
  * @version 1.0
- *
+ * @deprecated Use {@link StopFilter} instead, which has the same functionality.
+ * This filter will be removed in Lucene 4.0
  */
-
+@Deprecated
 public final class ChineseFilter extends TokenFilter {
 
 
Index: contrib/analyzers/common/src/java/org/apache/lucene/analysis/cn/ChineseTokenizer.java
===================================================================
--- contrib/analyzers/common/src/java/org/apache/lucene/analysis/cn/ChineseTokenizer.java	(revision 916567)
+++ contrib/analyzers/common/src/java/org/apache/lucene/analysis/cn/ChineseTokenizer.java	(working copy)
@@ -21,6 +21,7 @@
 import java.io.IOException;
 import java.io.Reader;
 
+import org.apache.lucene.analysis.standard.StandardTokenizer;
 import org.apache.lucene.analysis.Tokenizer;
 import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
 import org.apache.lucene.analysis.tokenattributes.TermAttribute;
@@ -52,9 +53,10 @@
  * CJKTokenizer will not work.
  * </p>
  * @version 1.0
- *
+ * @deprecated Use {@link StandardTokenizer} instead, which has the same functionality.
+ * This filter will be removed in Lucene 4.0
  */
-
+@Deprecated
 public final class ChineseTokenizer extends Tokenizer {
 
 
@@ -129,8 +131,10 @@
                 bufferIndex = 0;
             }
 
-            if (dataLen == -1) return flush();
-            else
+            if (dataLen == -1) {
+              offset--;
+              return flush();
+            } else
                 c = ioBuffer[bufferIndex++];
 
 
@@ -162,7 +166,7 @@
     @Override
     public final void end() {
       // set final offset
-      final int finalOffset = offset;
+      final int finalOffset = correctOffset(offset);
       this.offsetAtt.setOffset(finalOffset, finalOffset);
     }
 
Index: contrib/analyzers/common/src/java/org/apache/lucene/analysis/cn/package.html
===================================================================
--- contrib/analyzers/common/src/java/org/apache/lucene/analysis/cn/package.html	(revision 916567)
+++ contrib/analyzers/common/src/java/org/apache/lucene/analysis/cn/package.html	(working copy)
@@ -24,14 +24,14 @@
 <p>
 Three analyzers are provided for Chinese, each of which treats Chinese text in a different way.
 <ul>
-	<li>ChineseAnalyzer (in this package): Index unigrams (individual Chinese characters) as a token.
+	<li>StandardAnalyzer: Index unigrams (individual Chinese characters) as a token.
 	<li>CJKAnalyzer (in the analyzers/cjk package): Index bigrams (overlapping groups of two adjacent Chinese characters) as tokens.
 	<li>SmartChineseAnalyzer (in the analyzers/smartcn package): Index words (attempt to segment Chinese text into words) as tokens.
 </ul>
 
 Example phrase： "我是中国人"
 <ol>
-	<li>ChineseAnalyzer: 我－是－中－国－人</li>
+	<li>StandardAnalyzer: 我－是－中－国－人</li>
 	<li>CJKAnalyzer: 我是－是中－中国－国人</li>
 	<li>SmartChineseAnalyzer: 我－是－中国－人</li>
 </ol>
Index: contrib/analyzers/common/src/java/org/apache/lucene/analysis/ar/ArabicStemFilter.java
===================================================================
--- contrib/analyzers/common/src/java/org/apache/lucene/analysis/ar/ArabicStemFilter.java	(revision 916567)
+++ contrib/analyzers/common/src/java/org/apache/lucene/analysis/ar/ArabicStemFilter.java	(working copy)
@@ -19,31 +19,41 @@
 
 import java.io.IOException;
 
+import org.apache.lucene.analysis.KeywordMarkerTokenFilter;
 import org.apache.lucene.analysis.TokenFilter;
 import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.analysis.tokenattributes.KeywordAttribute;
 import org.apache.lucene.analysis.tokenattributes.TermAttribute;
 
 /**
  * A {@link TokenFilter} that applies {@link ArabicStemmer} to stem Arabic words..
- * 
- */
+ * <p>
+ * To prevent terms from being stemmed use an instance of
+ * {@link KeywordMarkerTokenFilter} or a custom {@link TokenFilter} that sets
+ * the {@link KeywordAttribute} before this {@link TokenStream}.
+ * </p>
+ * @see KeywordMarkerTokenFilter */
 
 public final class ArabicStemFilter extends TokenFilter {
 
   private final ArabicStemmer stemmer;
   private final TermAttribute termAtt;
+  private final KeywordAttribute keywordAttr;
   
   public ArabicStemFilter(TokenStream input) {
     super(input);
     stemmer = new ArabicStemmer();
     termAtt = addAttribute(TermAttribute.class);
+    keywordAttr = addAttribute(KeywordAttribute.class);
   }
 
   @Override
   public boolean incrementToken() throws IOException {
     if (input.incrementToken()) {
-      int newlen = stemmer.stem(termAtt.termBuffer(), termAtt.termLength());
-      termAtt.setTermLength(newlen);
+      if(!keywordAttr.isKeyword()) {
+        final int newlen = stemmer.stem(termAtt.termBuffer(), termAtt.termLength());
+        termAtt.setTermLength(newlen);
+      }
       return true;
     } else {
       return false;
Index: contrib/analyzers/common/src/java/org/apache/lucene/analysis/ar/ArabicAnalyzer.java
===================================================================
--- contrib/analyzers/common/src/java/org/apache/lucene/analysis/ar/ArabicAnalyzer.java	(revision 916567)
+++ contrib/analyzers/common/src/java/org/apache/lucene/analysis/ar/ArabicAnalyzer.java	(working copy)
@@ -26,6 +26,8 @@
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.LowerCaseFilter;
 import org.apache.lucene.analysis.ReusableAnalyzerBase.TokenStreamComponents; // javadoc @link
+import org.apache.lucene.analysis.CharArraySet;
+import org.apache.lucene.analysis.KeywordMarkerTokenFilter;
 import org.apache.lucene.analysis.StopFilter;
 import org.apache.lucene.analysis.StopwordAnalyzerBase;
 import org.apache.lucene.analysis.TokenStream;
@@ -93,6 +95,8 @@
       }
     }
   }
+  
+  private final Set<?> stemExclusionSet;
 
   /**
    * Builds an analyzer with the default stop words: {@link #DEFAULT_STOPWORD_FILE}.
@@ -110,7 +114,25 @@
    *          a stopword set
    */
   public ArabicAnalyzer(Version matchVersion, Set<?> stopwords){
+    this(matchVersion, stopwords, CharArraySet.EMPTY_SET);
+  }
+
+  /**
+   * Builds an analyzer with the given stop word. If a none-empty stem exclusion set is
+   * provided this analyzer will add a {@link KeywordMarkerTokenFilter} before
+   * {@link ArabicStemFilter}.
+   * 
+   * @param matchVersion
+   *          lucene compatibility version
+   * @param stopwords
+   *          a stopword set
+   * @param stemExclusionSet
+   *          a set of terms not to be stemmed
+   */
+  public ArabicAnalyzer(Version matchVersion, Set<?> stopwords, Set<?> stemExclusionSet){
     super(matchVersion, stopwords);
+    this.stemExclusionSet = CharArraySet.unmodifiableSet(CharArraySet.copy(
+        matchVersion, stemExclusionSet));
   }
 
   /**
@@ -145,17 +167,22 @@
    * Creates {@link TokenStreamComponents} used to tokenize all the text in the provided {@link Reader}.
    *
    * @return {@link TokenStreamComponents} built from an {@link ArabicLetterTokenizer} filtered with
-   * 			{@link LowerCaseFilter}, {@link StopFilter}, {@link ArabicNormalizationFilter}
+   * 			{@link LowerCaseFilter}, {@link StopFilter}, {@link ArabicNormalizationFilter},
+   *      {@link KeywordMarkerTokenFilter} if a stem exclusion set is provided
    *            and {@link ArabicStemFilter}.
    */
   @Override
   protected TokenStreamComponents createComponents(String fieldName,
       Reader reader) {
-    final Tokenizer source = new ArabicLetterTokenizer(reader);
+    final Tokenizer source = new ArabicLetterTokenizer(matchVersion, reader);
     TokenStream result = new LowerCaseFilter(matchVersion, source);
     // the order here is important: the stopword list is not normalized!
     result = new StopFilter( matchVersion, result, stopwords);
+    // TODO maybe we should make ArabicNormalization filter also KeywordAttribute aware?!
     result = new ArabicNormalizationFilter(result);
+    if(!stemExclusionSet.isEmpty()) {
+      result = new KeywordMarkerTokenFilter(result, stemExclusionSet);
+    }
     return new TokenStreamComponents(source, new ArabicStemFilter(result));
   }
 }
Index: contrib/analyzers/common/src/java/org/apache/lucene/analysis/ar/ArabicLetterTokenizer.java
===================================================================
--- contrib/analyzers/common/src/java/org/apache/lucene/analysis/ar/ArabicLetterTokenizer.java	(revision 916567)
+++ contrib/analyzers/common/src/java/org/apache/lucene/analysis/ar/ArabicLetterTokenizer.java	(working copy)
@@ -18,8 +18,10 @@
 
 import java.io.Reader;
 
+import org.apache.lucene.analysis.CharTokenizer;
 import org.apache.lucene.analysis.LetterTokenizer;
 import org.apache.lucene.util.AttributeSource;
+import org.apache.lucene.util.Version;
 
 /**
  * Tokenizer that breaks text into runs of letters and diacritics.
@@ -27,28 +29,101 @@
  * The problem with the standard Letter tokenizer is that it fails on diacritics.
  * Handling similar to this is necessary for Indic Scripts, Hebrew, Thaana, etc.
  * </p>
- *
+ * <p>
+ * <a name="version"/>
+ * You must specify the required {@link Version} compatibility when creating
+ * {@link ArabicLetterTokenizer}:
+ * <ul>
+ * <li>As of 3.1, {@link CharTokenizer} uses an int based API to normalize and
+ * detect token characters. See {@link #isTokenChar(int)} and
+ * {@link #normalize(int)} for details.</li>
+ * </ul>
  */
 public class ArabicLetterTokenizer extends LetterTokenizer {
 
+  
+  /**
+   * Construct a new ArabicLetterTokenizer.
+   * @param matchVersion Lucene version
+   * to match See {@link <a href="#version">above</a>}
+   * 
+   * @param in
+   *          the input to split up into tokens
+   */
+  public ArabicLetterTokenizer(Version matchVersion, Reader in) {
+    super(matchVersion, in);
+  }
+
+  /**
+   * Construct a new ArabicLetterTokenizer using a given {@link AttributeSource}.
+   * 
+   * @param matchVersion
+   *          Lucene version to match See {@link <a href="#version">above</a>}
+   * @param source
+   *          the attribute source to use for this Tokenizer
+   * @param in
+   *          the input to split up into tokens
+   */
+  public ArabicLetterTokenizer(Version matchVersion, AttributeSource source, Reader in) {
+    super(matchVersion, source, in);
+  }
+
+  /**
+   * Construct a new ArabicLetterTokenizer using a given
+   * {@link org.apache.lucene.util.AttributeSource.AttributeFactory}. * @param
+   * matchVersion Lucene version to match See
+   * {@link <a href="#version">above</a>}
+   * 
+   * @param factory
+   *          the attribute factory to use for this Tokenizer
+   * @param in
+   *          the input to split up into tokens
+   */
+  public ArabicLetterTokenizer(Version matchVersion, AttributeFactory factory, Reader in) {
+    super(matchVersion, factory, in);
+  }
+  
+  /**
+   * Construct a new ArabicLetterTokenizer.
+   * 
+   * @deprecated use {@link #ArabicLetterTokenizer(Version, Reader)} instead. This will
+   *             be removed in Lucene 4.0.
+   */
+  @Deprecated
   public ArabicLetterTokenizer(Reader in) {
     super(in);
   }
 
+  /**
+   * Construct a new ArabicLetterTokenizer using a given {@link AttributeSource}.
+   * 
+   * @deprecated use {@link #ArabicLetterTokenizer(Version, AttributeSource, Reader)}
+   *             instead. This will be removed in Lucene 4.0.
+   */
+  @Deprecated
   public ArabicLetterTokenizer(AttributeSource source, Reader in) {
     super(source, in);
   }
 
+  /**
+   * Construct a new ArabicLetterTokenizer using a given
+   * {@link org.apache.lucene.util.AttributeSource.AttributeFactory}.
+   * 
+   * @deprecated use {@link #ArabicLetterTokenizer(Version, AttributeSource.AttributeFactory, Reader)}
+   *             instead. This will be removed in Lucene 4.0.
+   */
+  @Deprecated
   public ArabicLetterTokenizer(AttributeFactory factory, Reader in) {
     super(factory, in);
   }
   
+  
   /** 
    * Allows for Letter category or NonspacingMark category
-   * @see org.apache.lucene.analysis.LetterTokenizer#isTokenChar(char)
+   * @see org.apache.lucene.analysis.LetterTokenizer#isTokenChar(int)
    */
   @Override
-  protected boolean isTokenChar(char c) {
+  protected boolean isTokenChar(int c) {
     return super.isTokenChar(c) || Character.getType(c) == Character.NON_SPACING_MARK;
   }
 
Index: contrib/analyzers/common/src/java/org/apache/lucene/analysis/position/PositionFilter.java
===================================================================
--- contrib/analyzers/common/src/java/org/apache/lucene/analysis/position/PositionFilter.java	(revision 916567)
+++ contrib/analyzers/common/src/java/org/apache/lucene/analysis/position/PositionFilter.java	(working copy)
@@ -21,7 +21,6 @@
 
 import org.apache.lucene.analysis.TokenFilter;
 import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.Token;
 import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;
 
 /** Set the positionIncrement of all tokens to the "positionIncrement",
Index: contrib/analyzers/common/src/java/org/apache/lucene/analysis/cjk/CJKTokenizer.java
===================================================================
--- contrib/analyzers/common/src/java/org/apache/lucene/analysis/cjk/CJKTokenizer.java	(revision 916567)
+++ contrib/analyzers/common/src/java/org/apache/lucene/analysis/cjk/CJKTokenizer.java	(working copy)
@@ -175,9 +175,13 @@
                         length = 0;
                         preIsTokened = false;
                     }
+                    else{
+                      offset--;
+                    }
 
                     break;
                 } else {
+                    offset--;
                     return false;
                 }
             } else {
@@ -288,6 +292,7 @@
           typeAtt.setType(TOKEN_TYPE_NAMES[tokenType]);
           return true;
         } else if (dataLen == -1) {
+          offset--;
           return false;
         }
 
@@ -299,7 +304,7 @@
     @Override
     public final void end() {
       // set final offset
-      final int finalOffset = offset;
+      final int finalOffset = correctOffset(offset);
       this.offsetAtt.setOffset(finalOffset, finalOffset);
     }
     
Index: contrib/analyzers/common/src/java/org/apache/lucene/analysis/cz/CzechAnalyzer.java
===================================================================
--- contrib/analyzers/common/src/java/org/apache/lucene/analysis/cz/CzechAnalyzer.java	(revision 916567)
+++ contrib/analyzers/common/src/java/org/apache/lucene/analysis/cz/CzechAnalyzer.java	(working copy)
@@ -21,6 +21,7 @@
 import org.apache.lucene.analysis.ReusableAnalyzerBase.TokenStreamComponents; // javadoc @link
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.CharArraySet;
+import org.apache.lucene.analysis.KeywordMarkerTokenFilter;
 import org.apache.lucene.analysis.LowerCaseFilter;
 import org.apache.lucene.analysis.StopFilter;
 import org.apache.lucene.analysis.TokenStream;
@@ -105,6 +106,7 @@
 	// TODO once loadStopWords is gone those member should be removed too in favor of StopwordAnalyzerBase
 	private Set<?> stoptable;
   private final Version matchVersion;
+  private final Set<?> stemExclusionTable;
 
   /**
    * Builds an analyzer with the default stop words ({@link #CZECH_STOP_WORDS}).
@@ -124,8 +126,22 @@
    * @param stopwords a stopword set
    */
   public CzechAnalyzer(Version matchVersion, Set<?> stopwords) {
+    this(matchVersion, stopwords, CharArraySet.EMPTY_SET);
+  }
+  
+  /**
+   * Builds an analyzer with the given stop words and a set of work to be
+   * excluded from the {@link CzechStemFilter}.
+   * 
+   * @param matchVersion Lucene version to match See
+   *          {@link <a href="#version">above</a>}
+   * @param stopwords a stopword set
+   * @param stemExclusionTable a stemming exclusion set
+   */
+  public CzechAnalyzer(Version matchVersion, Set<?> stopwords, Set<?> stemExclusionTable) {
     this.matchVersion = matchVersion;
     this.stoptable = CharArraySet.unmodifiableSet(CharArraySet.copy(matchVersion, stopwords));
+    this.stemExclusionTable = CharArraySet.unmodifiableSet(CharArraySet.copy(matchVersion, stemExclusionTable));
   }
 
 
@@ -207,7 +223,9 @@
    * @return {@link TokenStreamComponents} built from a {@link StandardTokenizer}
    *         filtered with {@link StandardFilter}, {@link LowerCaseFilter},
    *         {@link StopFilter}, and {@link CzechStemFilter} (only if version is
-   *         >= LUCENE_31)
+   *         >= LUCENE_31). If a version is >= LUCENE_31 and a stem exclusion set
+   *         is provided via {@link #CzechAnalyzer(Version, Set, Set)} a 
+   *         {@link KeywordMarkerTokenFilter} is added before {@link CzechStemFilter}.
    */
   @Override
   protected TokenStreamComponents createComponents(String fieldName,
@@ -216,8 +234,11 @@
     TokenStream result = new StandardFilter(source);
     result = new LowerCaseFilter(matchVersion, result);
     result = new StopFilter( matchVersion, result, stoptable);
-    if (matchVersion.onOrAfter(Version.LUCENE_31))
+    if (matchVersion.onOrAfter(Version.LUCENE_31)) {
+      if(!this.stemExclusionTable.isEmpty())
+        result = new KeywordMarkerTokenFilter(result, stemExclusionTable);
       result = new CzechStemFilter(result);
+    }
     return new TokenStreamComponents(source, result);
   }
 }
Index: contrib/analyzers/common/src/java/org/apache/lucene/analysis/cz/CzechStemFilter.java
===================================================================
--- contrib/analyzers/common/src/java/org/apache/lucene/analysis/cz/CzechStemFilter.java	(revision 916567)
+++ contrib/analyzers/common/src/java/org/apache/lucene/analysis/cz/CzechStemFilter.java	(working copy)
@@ -2,8 +2,10 @@
 
 import java.io.IOException;
 
+import org.apache.lucene.analysis.KeywordMarkerTokenFilter;// for javadoc
 import org.apache.lucene.analysis.TokenFilter;
 import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.analysis.tokenattributes.KeywordAttribute;
 import org.apache.lucene.analysis.tokenattributes.TermAttribute;
 
 /**
@@ -25,25 +27,34 @@
 
 /**
  * A {@link TokenFilter} that applies {@link CzechStemmer} to stem Czech words.
- * 
+ * <p>
+ * To prevent terms from being stemmed use an instance of
+ * {@link KeywordMarkerTokenFilter} or a custom {@link TokenFilter} that sets
+ * the {@link KeywordAttribute} before this {@link TokenStream}.
+ * </p>
  * <p><b>NOTE</b>: Input is expected to be in lowercase, 
  * but with diacritical marks</p>
+ * @see KeywordMarkerTokenFilter
  */
 public final class CzechStemFilter extends TokenFilter {
   private final CzechStemmer stemmer;
   private final TermAttribute termAtt;
+  private final KeywordAttribute keywordAttr;
   
   public CzechStemFilter(TokenStream input) {
     super(input);
     stemmer = new CzechStemmer();
     termAtt = addAttribute(TermAttribute.class);
+    keywordAttr = addAttribute(KeywordAttribute.class);
   }
 
   @Override
   public boolean incrementToken() throws IOException {
     if (input.incrementToken()) {
-      int newlen = stemmer.stem(termAtt.termBuffer(), termAtt.termLength());
-      termAtt.setTermLength(newlen);
+      if(!keywordAttr.isKeyword()) {
+        final int newlen = stemmer.stem(termAtt.termBuffer(), termAtt.termLength());
+        termAtt.setTermLength(newlen);
+      }
       return true;
     } else {
       return false;
Index: contrib/analyzers/common/build.xml
===================================================================
--- contrib/analyzers/common/build.xml	(revision 916567)
+++ contrib/analyzers/common/build.xml	(working copy)
@@ -35,4 +35,20 @@
     <path refid="junit-path"/>
     <pathelement location="${build.dir}/classes/java"/>
   </path>	
+
+  <target name="compile-test" depends="download-snowball-vocab-tests, common.compile-test" />
+  <property name="snowball.vocab.rev" value="500"/>
+  <property name="snowball.vocab.url" 
+            value="svn://svn.tartarus.org/snowball/trunk/data"/>
+  <property name="snowball.vocab.dir" value="src/test/org/apache/lucene/analysis/snowball"/>
+		
+  <target name="download-snowball-vocab-tests" depends="compile-core"
+	      description="Downloads Snowball vocabulary tests">
+	<sequential>
+	  <mkdir dir="${snowball.vocab.dir}"/>
+	    <exec dir="${snowball.vocab.dir}" executable="${svn.exe}" failifexecutionfails="false" failonerror="true">
+	      <arg line="checkout --trust-server-cert --non-interactive -r ${snowball.vocab.rev} ${snowball.vocab.url}"/>
+	    </exec>
+	</sequential>
+  </target>
 </project>
Index: contrib/regex/src/test/org/apache/lucene/search/regex/TestRegexQuery.java
===================================================================
--- contrib/regex/src/test/org/apache/lucene/search/regex/TestRegexQuery.java	(revision 916567)
+++ contrib/regex/src/test/org/apache/lucene/search/regex/TestRegexQuery.java	(working copy)
@@ -29,6 +29,7 @@
 
 import org.apache.lucene.search.spans.SpanNearQuery;
 import org.apache.lucene.search.spans.SpanQuery;
+import org.apache.lucene.util.Version;
 
 public class TestRegexQuery extends TestCase {
   private IndexSearcher searcher;
@@ -39,7 +40,7 @@
   public void setUp() {
     RAMDirectory directory = new RAMDirectory();
     try {
-      IndexWriter writer = new IndexWriter(directory, new SimpleAnalyzer(), true, 
+      IndexWriter writer = new IndexWriter(directory, new SimpleAnalyzer(Version.LUCENE_CURRENT), true, 
                                            IndexWriter.MaxFieldLength.LIMITED);
       Document doc = new Document();
       doc.add(new Field(FN, "the quick brown fox jumps over the lazy dog", Field.Store.NO, Field.Index.ANALYZED));
Index: contrib/regex/src/test/org/apache/lucene/search/regex/TestSpanRegexQuery.java
===================================================================
--- contrib/regex/src/test/org/apache/lucene/search/regex/TestSpanRegexQuery.java	(revision 916567)
+++ contrib/regex/src/test/org/apache/lucene/search/regex/TestSpanRegexQuery.java	(working copy)
@@ -36,6 +36,7 @@
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.LockObtainFailedException;
 import org.apache.lucene.store.RAMDirectory;
+import org.apache.lucene.util.Version;
 
 public class TestSpanRegexQuery extends TestCase {
   Directory indexStoreA = new RAMDirectory();
@@ -44,7 +45,7 @@
 
   public void testSpanRegex() throws Exception {
     RAMDirectory directory = new RAMDirectory();
-    IndexWriter writer = new IndexWriter(directory, new SimpleAnalyzer(), true, IndexWriter.MaxFieldLength.UNLIMITED);
+    IndexWriter writer = new IndexWriter(directory, new SimpleAnalyzer(Version.LUCENE_CURRENT), true, IndexWriter.MaxFieldLength.UNLIMITED);
     Document doc = new Document();
     // doc.add(new Field("field", "the quick brown fox jumps over the lazy dog",
     // Field.Store.NO, Field.Index.ANALYZED));
Index: contrib/queries/src/test/org/apache/lucene/search/TermsFilterTest.java
===================================================================
--- contrib/queries/src/test/org/apache/lucene/search/TermsFilterTest.java	(revision 916567)
+++ contrib/queries/src/test/org/apache/lucene/search/TermsFilterTest.java	(working copy)
@@ -30,6 +30,7 @@
 import org.apache.lucene.index.IndexWriter.MaxFieldLength;
 import org.apache.lucene.store.RAMDirectory;
 import org.apache.lucene.util.OpenBitSet;
+import org.apache.lucene.util.Version;
 
 public class TermsFilterTest extends TestCase
 {
@@ -55,7 +56,7 @@
 	{
 		String fieldName="field1";
 		RAMDirectory rd=new RAMDirectory();
-		IndexWriter w=new IndexWriter(rd,new WhitespaceAnalyzer(),MaxFieldLength.UNLIMITED);
+		IndexWriter w=new IndexWriter(rd,new WhitespaceAnalyzer(Version.LUCENE_CURRENT),MaxFieldLength.UNLIMITED);
 		for (int i = 0; i < 100; i++)
 		{
 			Document doc=new Document();
Index: contrib/queries/src/test/org/apache/lucene/search/BooleanFilterTest.java
===================================================================
--- contrib/queries/src/test/org/apache/lucene/search/BooleanFilterTest.java	(revision 916567)
+++ contrib/queries/src/test/org/apache/lucene/search/BooleanFilterTest.java	(working copy)
@@ -28,6 +28,7 @@
 import org.apache.lucene.index.IndexWriter;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.store.RAMDirectory;
+import org.apache.lucene.util.Version;
 
 public class BooleanFilterTest extends TestCase
 {
@@ -38,7 +39,7 @@
 	protected void setUp() throws Exception
 	{
 		directory = new RAMDirectory();
-		IndexWriter writer = new IndexWriter(directory, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.UNLIMITED);
+		IndexWriter writer = new IndexWriter(directory, new WhitespaceAnalyzer(Version.LUCENE_CURRENT), true, IndexWriter.MaxFieldLength.UNLIMITED);
 		
 		//Add series of docs with filterable fields : acces rights, prices, dates and "in-stock" flags
 		addDoc(writer, "admin guest", "010", "20040101","Y");
Index: contrib/queries/src/test/org/apache/lucene/search/FuzzyLikeThisQueryTest.java
===================================================================
--- contrib/queries/src/test/org/apache/lucene/search/FuzzyLikeThisQueryTest.java	(revision 916567)
+++ contrib/queries/src/test/org/apache/lucene/search/FuzzyLikeThisQueryTest.java	(working copy)
@@ -30,12 +30,13 @@
 import org.apache.lucene.index.Term;
 import org.apache.lucene.index.IndexWriter.MaxFieldLength;
 import org.apache.lucene.store.RAMDirectory;
+import org.apache.lucene.util.Version;
 
 public class FuzzyLikeThisQueryTest extends TestCase
 {
 	private RAMDirectory directory;
 	private IndexSearcher searcher;
-	private Analyzer analyzer=new WhitespaceAnalyzer();
+	private Analyzer analyzer=new WhitespaceAnalyzer(Version.LUCENE_CURRENT);
 
 	@Override
 	protected void setUp() throws Exception
@@ -114,7 +115,7 @@
 	}
 	
 	public void testFuzzyLikeThisQueryEquals() {
-	  Analyzer analyzer = new WhitespaceAnalyzer();
+	  Analyzer analyzer = new WhitespaceAnalyzer(Version.LUCENE_CURRENT);
     FuzzyLikeThisQuery fltq1 = new FuzzyLikeThisQuery(10, analyzer);
     fltq1.addTerms("javi", "subject", 0.5f, 2);
     FuzzyLikeThisQuery fltq2 = new FuzzyLikeThisQuery(10, analyzer);
Index: README.txt
===================================================================
--- README.txt	(revision 916567)
+++ README.txt	(working copy)
@@ -1,7 +1,5 @@
 Lucene README file
 
-$Id$
-
 INTRODUCTION
 
 Lucene is a Java full-text search engine.  Lucene is not a complete
@@ -27,7 +25,7 @@
 
 contrib/*
   Contributed code which extends and enhances Lucene, but is not
-  part of the core library.  Of special note are the JAR files in the analyzers and snowball directory which
+  part of the core library.  Of special note are the JAR files in the analyzers directory which
   contain various analyzers that people may find useful in place of the StandardAnalyzer.
 
 
Index: lucene-contrib-pom.xml.template
===================================================================
--- lucene-contrib-pom.xml.template	(revision 916567)
+++ lucene-contrib-pom.xml.template	(working copy)
@@ -44,6 +44,7 @@
     <commons-collections-version>3.1</commons-collections-version>
     <commons-beanutils-version>1.7.0</commons-beanutils-version>
     <jakarta-regexp-version>1.4</jakarta-regexp-version>
+    <sleepycat-je-version>3.3.93</sleepycat-je-version>
     <icu-version>4.0</icu-version>
   </properties>
 </project>
Index: BUILD.txt
===================================================================
--- BUILD.txt	(revision 916567)
+++ BUILD.txt	(working copy)
@@ -1,7 +1,5 @@
 Lucene Build Instructions
 
-$Id$
-
 Basic steps:
   0) Install JDK 1.5 (or greater), Ant 1.6.3 (or greater)
   1) Download Lucene from Apache and unpack it
