diff --git a/lucene/analysis/icu/src/test/org/apache/lucene/analysis/icu/TestICUNormalizer2CharFilter.java b/lucene/analysis/icu/src/test/org/apache/lucene/analysis/icu/TestICUNormalizer2CharFilter.java
index 79b1dfe68f6..e1207d49363 100644
--- a/lucene/analysis/icu/src/test/org/apache/lucene/analysis/icu/TestICUNormalizer2CharFilter.java
+++ b/lucene/analysis/icu/src/test/org/apache/lucene/analysis/icu/TestICUNormalizer2CharFilter.java
@@ -25,9 +25,11 @@ import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.CharFilter;
 import org.apache.lucene.analysis.MockTokenizer;
+import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
 import org.apache.lucene.analysis.core.KeywordTokenizer;
 import org.apache.lucene.analysis.ngram.NGramTokenizer;
+import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
 import org.apache.lucene.util.TestUtil;
 
 public class TestICUNormalizer2CharFilter extends BaseTokenStreamTestCase {
@@ -74,6 +76,69 @@ public class TestICUNormalizer2CharFilter extends BaseTokenStreamTestCase {
         input.length());
   }
 
+  public void testVerySlow() throws IOException {
+    final boolean insertWhitespace = false;
+    StringBuilder builder = new StringBuilder();
+    final int charsPerOutputUnit = insertWhitespace ? 5 : 4;
+    final int repetitions = 100000;
+    builder.append("℃aa");
+    for (int i = 0; i < repetitions - 1; i++) {
+      if (insertWhitespace) {
+        builder.append(' ');
+      }
+      builder.append("℃aa");
+    }
+    final int expectedResultLength = repetitions * charsPerOutputUnit - (insertWhitespace ? 1 : 0);
+    StringBuilder result = new StringBuilder(expectedResultLength);
+    CharFilter reader = new ICUNormalizer2CharFilter(new StringReader(builder.toString()),
+        Normalizer2.getInstance(null, "nfkc_cf", Normalizer2.Mode.COMPOSE));
+    char[] tempBuff = new char[1024];
+    int total = 0;
+    long start = System.currentTimeMillis();
+    while (true) {
+      int length = reader.read(tempBuff);
+      if (length == -1) {
+        break;
+      }
+      result.append(tempBuff, 0, length);
+      total += length;
+    }
+    long elapsed = System.currentTimeMillis() - start;
+    assertEquals(expectedResultLength, result.length());
+    String first = result.toString();
+    System.out.println("normalized " + total + " chars in " + elapsed + "ms");
+
+    result.setLength(0);
+    Analyzer a = new Analyzer() {
+      @Override
+      public TokenStreamComponents createComponents(String fieldName) {
+        Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);
+        return new TokenStreamComponents(tokenizer, new ICUNormalizer2Filter(
+            tokenizer,
+            Normalizer2.getInstance(null, "nfkc_cf", Normalizer2.Mode.COMPOSE)));
+      }
+    };
+    start = System.currentTimeMillis();
+    int numTokens = 0;
+    try (TokenStream tokenStream = a.tokenStream("", builder.toString())) {
+      tokenStream.reset();
+      final CharTermAttribute termAtt = tokenStream.getAttribute(CharTermAttribute.class);
+      while (tokenStream.incrementToken()) {
+        if (numTokens > 0) {
+          result.append(' ');
+        }
+        numTokens ++;
+        result.append(termAtt.toString());
+      }
+      tokenStream.end();
+    }
+    a.close();
+    assertEquals(first, result.toString());
+    assertEquals(insertWhitespace ? repetitions : 1, numTokens);
+    elapsed = System.currentTimeMillis() - start;
+    System.out.println("normalized " + total + " chars in " + elapsed + "ms");
+  }
+
   public void testTokenStream2() throws IOException {
     // '㌰', '<<'゙, '5', '℃', '№', '㈱', '㌘', 'ｻ', '<<', 'ｿ', '<<'
     String input = "㌰゙5℃№㈱㌘ｻﾞｿﾞ";
