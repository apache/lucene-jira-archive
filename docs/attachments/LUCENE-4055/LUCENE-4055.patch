

diff -ruN -x .svn -x build lucene-trunk/lucene/CHANGES.txt lucene4055/lucene/CHANGES.txt
--- lucene-trunk/lucene/CHANGES.txt	2012-05-24 16:55:47.364233416 -0400
+++ lucene4055/lucene/CHANGES.txt	2012-05-22 11:39:04.504893276 -0400
@@ -1,3 +1,4 @@
+
 Lucene Change Log
 
 For more information on past and future Lucene versions, please see:
@@ -271,6 +272,8 @@
   that take two booleans indicating whether hit scores and max
   score should be computed.  (Mike McCandless)
 
+* LUCENE-4055: You can't put foreign files into the index dir anymore.
+
 Changes in Runtime Behavior
 
 * LUCENE-2846: omitNorms now behaves like omitTermFrequencyAndPositions, if you


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/appending/AppendingCodec.java lucene4055/lucene/core/src/java/org/apache/lucene/codecs/appending/AppendingCodec.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/appending/AppendingCodec.java	2012-05-24 16:55:47.956233428 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/codecs/appending/AppendingCodec.java	2012-05-22 17:13:19.717242527 -0400
@@ -23,7 +23,7 @@
 import org.apache.lucene.codecs.LiveDocsFormat;
 import org.apache.lucene.codecs.NormsFormat;
 import org.apache.lucene.codecs.PostingsFormat;
-import org.apache.lucene.codecs.SegmentInfosFormat;
+import org.apache.lucene.codecs.SegmentInfoFormat;
 import org.apache.lucene.codecs.StoredFieldsFormat;
 import org.apache.lucene.codecs.TermVectorsFormat;
 import org.apache.lucene.codecs.lucene40.Lucene40Codec;
@@ -31,6 +31,7 @@
 import org.apache.lucene.codecs.lucene40.Lucene40FieldInfosFormat;
 import org.apache.lucene.codecs.lucene40.Lucene40LiveDocsFormat;
 import org.apache.lucene.codecs.lucene40.Lucene40NormsFormat;
+import org.apache.lucene.codecs.lucene40.Lucene40SegmentInfoFormat;
 import org.apache.lucene.codecs.lucene40.Lucene40StoredFieldsFormat;
 import org.apache.lucene.codecs.lucene40.Lucene40TermVectorsFormat;
 
@@ -46,7 +47,7 @@
   }
 
   private final PostingsFormat postings = new AppendingPostingsFormat();
-  private final SegmentInfosFormat infos = new AppendingSegmentInfosFormat();
+  private final SegmentInfoFormat infos = new Lucene40SegmentInfoFormat();
   private final StoredFieldsFormat fields = new Lucene40StoredFieldsFormat();
   private final FieldInfosFormat fieldInfos = new Lucene40FieldInfosFormat();
   private final TermVectorsFormat vectors = new Lucene40TermVectorsFormat();
@@ -75,7 +76,7 @@
   }
 
   @Override
-  public SegmentInfosFormat segmentInfosFormat() {
+  public SegmentInfoFormat segmentInfoFormat() {
     return infos;
   }
   


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/appending/AppendingPostingsFormat.java lucene4055/lucene/core/src/java/org/apache/lucene/codecs/appending/AppendingPostingsFormat.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/appending/AppendingPostingsFormat.java	2012-05-24 16:55:47.956233428 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/codecs/appending/AppendingPostingsFormat.java	2012-05-24 11:55:39.259919817 -0400
@@ -18,9 +18,7 @@
  */
 
 import java.io.IOException;
-import java.util.Set;
 
-import org.apache.lucene.codecs.BlockTreeTermsReader;
 import org.apache.lucene.codecs.BlockTreeTermsWriter;
 import org.apache.lucene.codecs.FieldsConsumer;
 import org.apache.lucene.codecs.FieldsProducer;
@@ -29,7 +27,6 @@
 import org.apache.lucene.codecs.PostingsWriterBase;
 import org.apache.lucene.codecs.lucene40.Lucene40PostingsReader;
 import org.apache.lucene.codecs.lucene40.Lucene40PostingsWriter;
-import org.apache.lucene.index.SegmentInfo;
 import org.apache.lucene.index.SegmentReadState;
 import org.apache.lucene.index.SegmentWriteState;
 
@@ -60,7 +57,7 @@
 
   @Override
   public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {
-    PostingsReaderBase postings = new Lucene40PostingsReader(state.dir, state.segmentInfo, state.context, state.segmentSuffix);
+    PostingsReaderBase postings = new Lucene40PostingsReader(state.dir, state.fieldInfos, state.segmentInfo, state.context, state.segmentSuffix);
     
     boolean success = false;
     try {
@@ -80,10 +77,4 @@
       }
     }
   }
-
-  @Override
-  public void files(SegmentInfo segmentInfo, String segmentSuffix, Set<String> files) throws IOException {
-    Lucene40PostingsReader.files(segmentInfo, segmentSuffix, files);
-    BlockTreeTermsReader.files(segmentInfo, segmentSuffix, files);
-  }
 }


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/appending/AppendingSegmentInfosFormat.java lucene4055/lucene/core/src/java/org/apache/lucene/codecs/appending/AppendingSegmentInfosFormat.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/appending/AppendingSegmentInfosFormat.java	2012-05-24 16:55:47.952233428 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/codecs/appending/AppendingSegmentInfosFormat.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,38 +0,0 @@
-package org.apache.lucene.codecs.appending;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.codecs.SegmentInfosWriter;
-import org.apache.lucene.codecs.lucene40.Lucene40SegmentInfosFormat;
-
-/**
- * Append-only SegmentInfos format.
- * <p>
- * Only a writer is supplied, as the format is written 
- * the same as {@link Lucene40SegmentInfosFormat}.
- * 
- * @see AppendingSegmentInfosWriter
- */
-public class AppendingSegmentInfosFormat extends Lucene40SegmentInfosFormat {
-  private final SegmentInfosWriter writer = new AppendingSegmentInfosWriter();
-
-  @Override
-  public SegmentInfosWriter getSegmentInfosWriter() {
-    return writer;
-  }
-}


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/appending/AppendingSegmentInfosWriter.java lucene4055/lucene/core/src/java/org/apache/lucene/codecs/appending/AppendingSegmentInfosWriter.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/appending/AppendingSegmentInfosWriter.java	2012-05-24 16:55:47.956233428 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/codecs/appending/AppendingSegmentInfosWriter.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,39 +0,0 @@
-package org.apache.lucene.codecs.appending;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.codecs.lucene40.Lucene40SegmentInfosWriter;
-import org.apache.lucene.store.IndexOutput;
-
-/**
- * Append-only SegmentInfos writer.
- * <p>
- * Extends {@link Lucene40SegmentInfosWriter}, writing the same
- * format, but the first phase of a two-phase commit 
- * ({@link #prepareCommit(IndexOutput)}) is not implemented.
- */
-public class AppendingSegmentInfosWriter extends Lucene40SegmentInfosWriter {
-
-  @Override
-  public void prepareCommit(IndexOutput segmentOutput) throws IOException {
-    // noop
-  }
-
-}


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/BlockTermsReader.java lucene4055/lucene/core/src/java/org/apache/lucene/codecs/BlockTermsReader.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/BlockTermsReader.java	2012-05-24 16:55:48.036233429 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/codecs/BlockTermsReader.java	2012-05-21 13:58:03.147533880 -0400
@@ -135,7 +135,7 @@
         assert numTerms >= 0;
         final long termsStartPointer = in.readVLong();
         final FieldInfo fieldInfo = fieldInfos.fieldInfo(field);
-        final long sumTotalTermFreq = fieldInfo.indexOptions == IndexOptions.DOCS_ONLY ? -1 : in.readVLong();
+        final long sumTotalTermFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS_ONLY ? -1 : in.readVLong();
         final long sumDocFreq = in.readVLong();
         final int docCount = in.readVInt();
         assert !fields.containsKey(fieldInfo.name);
@@ -186,10 +186,6 @@
     }
   }
 
-  public static void files(SegmentInfo segmentInfo, String segmentSuffix, Collection<String> files) {
-    files.add(IndexFileNames.segmentFileName(segmentInfo.name, segmentSuffix, BlockTermsWriter.TERMS_EXTENSION));
-  }
-
   @Override
   public FieldsEnum iterator() {
     return new TermFieldsEnum();
@@ -699,13 +695,13 @@
 
       @Override
       public DocsAndPositionsEnum docsAndPositions(Bits liveDocs, DocsAndPositionsEnum reuse, boolean needsOffsets) throws IOException {
-        if (fieldInfo.indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) < 0) {
+        if (fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) < 0) {
           // Positions were not indexed:
           return null;
         }
 
         if (needsOffsets &&
-            fieldInfo.indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) < 0) {
+            fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) < 0) {
           // Offsets were not indexed:
           return null;
         }
@@ -860,7 +856,7 @@
             // just skipN here:
             state.docFreq = freqReader.readVInt();
             //System.out.println("    dF=" + state.docFreq);
-            if (fieldInfo.indexOptions != IndexOptions.DOCS_ONLY) {
+            if (fieldInfo.getIndexOptions() != IndexOptions.DOCS_ONLY) {
               state.totalTermFreq = state.docFreq + freqReader.readVLong();
               //System.out.println("    totTF=" + state.totalTermFreq);
             }


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/BlockTermsWriter.java lucene4055/lucene/core/src/java/org/apache/lucene/codecs/BlockTermsWriter.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/BlockTermsWriter.java	2012-05-24 16:55:47.932233427 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/codecs/BlockTermsWriter.java	2012-05-22 18:37:03.925330020 -0400
@@ -71,7 +71,7 @@
   public BlockTermsWriter(TermsIndexWriterBase termsIndexWriter,
       SegmentWriteState state, PostingsWriterBase postingsWriter)
       throws IOException {
-    final String termsFileName = IndexFileNames.segmentFileName(state.segmentName, state.segmentSuffix, TERMS_EXTENSION);
+    final String termsFileName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, TERMS_EXTENSION);
     this.termsIndexWriter = termsIndexWriter;
     out = state.directory.createOutput(termsFileName, state.context);
     boolean success = false;
@@ -130,7 +130,7 @@
           out.writeVInt(field.fieldInfo.number);
           out.writeVLong(field.numTerms);
           out.writeVLong(field.termsStartPointer);
-          if (field.fieldInfo.indexOptions != IndexOptions.DOCS_ONLY) {
+          if (field.fieldInfo.getIndexOptions() != IndexOptions.DOCS_ONLY) {
             out.writeVLong(field.sumTotalTermFreq);
           }
           out.writeVLong(field.sumDocFreq);
@@ -302,7 +302,7 @@
         final TermStats stats = pendingTerms[termCount].stats;
         assert stats != null;
         bytesWriter.writeVInt(stats.docFreq);
-        if (fieldInfo.indexOptions != IndexOptions.DOCS_ONLY) {
+        if (fieldInfo.getIndexOptions() != IndexOptions.DOCS_ONLY) {
           bytesWriter.writeVLong(stats.totalTermFreq-stats.docFreq);
         }
       }


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/BlockTreeTermsReader.java lucene4055/lucene/core/src/java/org/apache/lucene/codecs/BlockTreeTermsReader.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/BlockTreeTermsReader.java	2012-05-24 16:55:48.024233429 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/codecs/BlockTreeTermsReader.java	2012-05-24 11:53:47.559917871 -0400
@@ -20,7 +20,6 @@
 import java.io.ByteArrayOutputStream;
 import java.io.IOException;
 import java.io.PrintStream;
-import java.util.Collection;
 import java.util.Comparator;
 import java.util.Iterator;
 import java.util.TreeMap;
@@ -32,7 +31,6 @@
 import org.apache.lucene.index.FieldInfos;
 import org.apache.lucene.index.FieldsEnum;
 import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.SegmentInfo;
 import org.apache.lucene.index.TermState;
 import org.apache.lucene.index.Terms;
 import org.apache.lucene.index.TermsEnum;
@@ -147,7 +145,7 @@
         rootCode.length = numBytes;
         final FieldInfo fieldInfo = fieldInfos.fieldInfo(field);
         assert fieldInfo != null: "field=" + field;
-        final long sumTotalTermFreq = fieldInfo.indexOptions == IndexOptions.DOCS_ONLY ? -1 : in.readVLong();
+        final long sumTotalTermFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS_ONLY ? -1 : in.readVLong();
         final long sumDocFreq = in.readVLong();
         final int docCount = in.readVInt();
         final long indexStartFP = indexDivisor != -1 ? indexIn.readVLong() : 0;
@@ -199,11 +197,6 @@
     }
   }
 
-  public static void files(SegmentInfo segmentInfo, String segmentSuffix, Collection<String> files) {
-    files.add(IndexFileNames.segmentFileName(segmentInfo.name, segmentSuffix, BlockTreeTermsWriter.TERMS_EXTENSION));
-    files.add(IndexFileNames.segmentFileName(segmentInfo.name, segmentSuffix, BlockTreeTermsWriter.TERMS_INDEX_EXTENSION));
-  }
-
   @Override
   public FieldsEnum iterator() {
     return new TermFieldsEnum();
@@ -732,7 +725,7 @@
             // just skipN here:
             termState.docFreq = statsReader.readVInt();
             //if (DEBUG) System.out.println("    dF=" + state.docFreq);
-            if (fieldInfo.indexOptions != IndexOptions.DOCS_ONLY) {
+            if (fieldInfo.getIndexOptions() != IndexOptions.DOCS_ONLY) {
               termState.totalTermFreq = termState.docFreq + statsReader.readVLong();
               //if (DEBUG) System.out.println("    totTF=" + state.totalTermFreq);
             }
@@ -897,13 +890,13 @@
 
       @Override
       public DocsAndPositionsEnum docsAndPositions(Bits skipDocs, DocsAndPositionsEnum reuse, boolean needsOffsets) throws IOException {
-        if (fieldInfo.indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) < 0) {
+        if (fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) < 0) {
           // Positions were not indexed:
           return null;
         }
 
         if (needsOffsets &&
-            fieldInfo.indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) < 0) {
+            fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) < 0) {
           // Offsets were not indexed:
           return null;
         }
@@ -2129,13 +2122,13 @@
 
       @Override
       public DocsAndPositionsEnum docsAndPositions(Bits skipDocs, DocsAndPositionsEnum reuse, boolean needsOffsets) throws IOException {
-        if (fieldInfo.indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) < 0) {
+        if (fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) < 0) {
           // Positions were not indexed:
           return null;
         }
 
         if (needsOffsets &&
-            fieldInfo.indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) < 0) {
+            fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) < 0) {
           // Offsets were not indexed:
           return null;
         }
@@ -2546,7 +2539,7 @@
             // just skipN here:
             state.docFreq = statsReader.readVInt();
             //if (DEBUG) System.out.println("    dF=" + state.docFreq);
-            if (fieldInfo.indexOptions != IndexOptions.DOCS_ONLY) {
+            if (fieldInfo.getIndexOptions() != IndexOptions.DOCS_ONLY) {
               state.totalTermFreq = state.docFreq + statsReader.readVLong();
               //if (DEBUG) System.out.println("    totTF=" + state.totalTermFreq);
             }


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/BlockTreeTermsWriter.java lucene4055/lucene/core/src/java/org/apache/lucene/codecs/BlockTreeTermsWriter.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/BlockTreeTermsWriter.java	2012-05-24 16:55:48.036233429 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/codecs/BlockTreeTermsWriter.java	2012-05-22 18:37:03.933330020 -0400
@@ -144,7 +144,7 @@
       throw new IllegalArgumentException("maxItemsInBlock must be at least 2*(minItemsInBlock-1); got maxItemsInBlock=" + maxItemsInBlock + " minItemsInBlock=" + minItemsInBlock);
     }
 
-    final String termsFileName = IndexFileNames.segmentFileName(state.segmentName, state.segmentSuffix, TERMS_EXTENSION);
+    final String termsFileName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, TERMS_EXTENSION);
     out = state.directory.createOutput(termsFileName, state.context);
     boolean success = false;
     IndexOutput indexOut = null;
@@ -156,7 +156,7 @@
 
       //DEBUG = state.segmentName.equals("_4a");
 
-      final String termsIndexFileName = IndexFileNames.segmentFileName(state.segmentName, state.segmentSuffix, TERMS_INDEX_EXTENSION);
+      final String termsIndexFileName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, TERMS_INDEX_EXTENSION);
       indexOut = state.directory.createOutput(termsIndexFileName, state.context);
       writeIndexHeader(indexOut);
 
@@ -724,7 +724,7 @@
 
           // Write term stats, to separate byte[] blob:
           bytesWriter2.writeVInt(term.stats.docFreq);
-          if (fieldInfo.indexOptions != IndexOptions.DOCS_ONLY) {
+          if (fieldInfo.getIndexOptions() != IndexOptions.DOCS_ONLY) {
             assert term.stats.totalTermFreq >= term.stats.docFreq;
             bytesWriter2.writeVLong(term.stats.totalTermFreq - term.stats.docFreq);
           }
@@ -750,7 +750,7 @@
 
             // Write term stats, to separate byte[] blob:
             bytesWriter2.writeVInt(term.stats.docFreq);
-            if (fieldInfo.indexOptions != IndexOptions.DOCS_ONLY) {
+            if (fieldInfo.getIndexOptions() != IndexOptions.DOCS_ONLY) {
               assert term.stats.totalTermFreq >= term.stats.docFreq;
               bytesWriter2.writeVLong(term.stats.totalTermFreq - term.stats.docFreq);
             }
@@ -930,7 +930,7 @@
           assert rootCode != null: "field=" + field.fieldInfo.name + " numTerms=" + field.numTerms;
           out.writeVInt(rootCode.length);
           out.writeBytes(rootCode.bytes, rootCode.offset, rootCode.length);
-          if (field.fieldInfo.indexOptions != IndexOptions.DOCS_ONLY) {
+          if (field.fieldInfo.getIndexOptions() != IndexOptions.DOCS_ONLY) {
             out.writeVLong(field.sumTotalTermFreq);
           }
           out.writeVLong(field.sumDocFreq);


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/Codec.java lucene4055/lucene/core/src/java/org/apache/lucene/codecs/Codec.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/Codec.java	2012-05-24 16:55:48.032233429 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/codecs/Codec.java	2012-05-24 11:54:28.239918575 -0400
@@ -17,13 +17,10 @@
  * limitations under the License.
  */
 
-import java.io.IOException;
 import java.util.Set;
 import java.util.ServiceLoader; // javadocs
 
-import org.apache.lucene.index.IndexFileNames;
 import org.apache.lucene.index.IndexWriterConfig; // javadocs
-import org.apache.lucene.index.SegmentInfo;
 import org.apache.lucene.util.NamedSPILoader;
 
 /**
@@ -45,36 +42,16 @@
   private final String name;
 
   public Codec(String name) {
+    NamedSPILoader.checkServiceName(name);
     this.name = name;
   }
   
   /** Returns this codec's name */
   @Override
-  public String getName() {
+  public final String getName() {
     return name;
   }
   
-  /** Populates <code>files</code> with all filenames needed for 
-   * the <code>info</code> segment.
-   */
-  public void files(SegmentInfo info, Set<String> files) throws IOException {
-    if (info.getUseCompoundFile()) {
-      files.add(IndexFileNames.segmentFileName(info.name, "", IndexFileNames.COMPOUND_FILE_EXTENSION));
-      files.add(IndexFileNames.segmentFileName(info.name, "", IndexFileNames.COMPOUND_FILE_ENTRIES_EXTENSION));
-    } else {
-      postingsFormat().files(info, "", files);
-      storedFieldsFormat().files(info, files);
-      termVectorsFormat().files(info, files);
-      fieldInfosFormat().files(info, files);
-      // TODO: segmentInfosFormat should be allowed to declare additional files
-      // if it wants, in addition to segments_N
-      docValuesFormat().files(info, files);
-      normsFormat().files(info, files);
-    }
-    // never inside CFS
-    liveDocsFormat().files(info, files);
-  }
-  
   /** Encodes/decodes postings */
   public abstract PostingsFormat postingsFormat();
   
@@ -90,8 +67,8 @@
   /** Encodes/decodes field infos file */
   public abstract FieldInfosFormat fieldInfosFormat();
   
-  /** Encodes/decodes segments file */
-  public abstract SegmentInfosFormat segmentInfosFormat();
+  /** Encodes/decodes segment info file */
+  public abstract SegmentInfoFormat segmentInfoFormat();
   
   /** Encodes/decodes document normalization values */
   public abstract NormsFormat normsFormat();


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/DocValuesConsumer.java lucene4055/lucene/core/src/java/org/apache/lucene/codecs/DocValuesConsumer.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/DocValuesConsumer.java	2012-05-24 16:55:47.900233426 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/codecs/DocValuesConsumer.java	2012-05-23 15:57:15.026667644 -0400
@@ -103,7 +103,7 @@
     }
     // only finish if no exception is thrown!
     if (hasMerged) {
-      finish(mergeState.mergedDocCount);
+      finish(mergeState.segmentInfo.getDocCount());
     }
   }
 


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/DocValuesFormat.java lucene4055/lucene/core/src/java/org/apache/lucene/codecs/DocValuesFormat.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/DocValuesFormat.java	2012-05-24 16:55:48.036233429 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/codecs/DocValuesFormat.java	2012-05-24 11:54:36.715918727 -0400
@@ -18,11 +18,9 @@
  */
 
 import java.io.IOException;
-import java.util.Set;
 
 import org.apache.lucene.index.DocValues; // javadocs
 import org.apache.lucene.index.PerDocWriteState;
-import org.apache.lucene.index.SegmentInfo;
 import org.apache.lucene.index.SegmentReadState;
 
 /**
@@ -30,7 +28,10 @@
  * @lucene.experimental
  */
 public abstract class DocValuesFormat {
+
+  /** Consumes (writes) doc values during indexing. */
   public abstract PerDocConsumer docsConsumer(PerDocWriteState state) throws IOException;
+
+  /** Produces (reads) doc values during reading/searching. */
   public abstract PerDocProducer docsProducer(SegmentReadState state) throws IOException;
-  public abstract void files(SegmentInfo info, Set<String> files) throws IOException;
 }


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/FieldInfosFormat.java lucene4055/lucene/core/src/java/org/apache/lucene/codecs/FieldInfosFormat.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/FieldInfosFormat.java	2012-05-24 16:55:48.036233429 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/codecs/FieldInfosFormat.java	2012-05-24 11:54:44.527918863 -0400
@@ -18,17 +18,19 @@
  */
 
 import java.io.IOException;
-import java.util.Set;
 
 import org.apache.lucene.index.FieldInfos; // javadocs
-import org.apache.lucene.index.SegmentInfo;
 
 /**
  * Encodes/decodes {@link FieldInfos}
  * @lucene.experimental
  */
 public abstract class FieldInfosFormat {
+  /** Returns a {@link FieldInfosReader} to read field infos
+   *  from the index */
   public abstract FieldInfosReader getFieldInfosReader() throws IOException;
+
+  /** Returns a {@link FieldInfosWriter} to write field infos
+   *  to the index */
   public abstract FieldInfosWriter getFieldInfosWriter() throws IOException;
-  public abstract void files(SegmentInfo info, Set<String> files) throws IOException;
 }


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/FixedGapTermsIndexReader.java lucene4055/lucene/core/src/java/org/apache/lucene/codecs/FixedGapTermsIndexReader.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/FixedGapTermsIndexReader.java	2012-05-24 16:55:47.936233427 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/codecs/FixedGapTermsIndexReader.java	2012-05-24 11:54:51.403918983 -0400
@@ -22,7 +22,6 @@
 import org.apache.lucene.store.IndexInput;
 import org.apache.lucene.index.FieldInfos;
 import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.SegmentInfo;
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.CodecUtil;
 import org.apache.lucene.util.IOUtils;
@@ -30,7 +29,6 @@
 import org.apache.lucene.util.packed.PackedInts;
 
 import java.util.HashMap;
-import java.util.Collection;
 import java.util.Comparator;
 import java.io.IOException;
 
@@ -389,10 +387,6 @@
     }
   }
 
-  public static void files(SegmentInfo info, String segmentSuffix, Collection<String> files) {
-    files.add(IndexFileNames.segmentFileName(info.name, segmentSuffix, FixedGapTermsIndexWriter.TERMS_INDEX_EXTENSION));
-  }
-
   @Override
   public void close() throws IOException {
     if (in != null && !indexLoaded) {


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/FixedGapTermsIndexWriter.java lucene4055/lucene/core/src/java/org/apache/lucene/codecs/FixedGapTermsIndexWriter.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/FixedGapTermsIndexWriter.java	2012-05-24 16:55:47.960233428 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/codecs/FixedGapTermsIndexWriter.java	2012-05-22 18:37:03.925330020 -0400
@@ -57,7 +57,7 @@
   @SuppressWarnings("unused") private final FieldInfos fieldInfos; // unread
 
   public FixedGapTermsIndexWriter(SegmentWriteState state) throws IOException {
-    final String indexFileName = IndexFileNames.segmentFileName(state.segmentName, state.segmentSuffix, TERMS_INDEX_EXTENSION);
+    final String indexFileName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, TERMS_INDEX_EXTENSION);
     termIndexInterval = state.termIndexInterval;
     out = state.directory.createOutput(indexFileName, state.context);
     boolean success = false;


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/LiveDocsFormat.java lucene4055/lucene/core/src/java/org/apache/lucene/codecs/LiveDocsFormat.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/LiveDocsFormat.java	2012-05-24 16:55:47.948233428 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/codecs/LiveDocsFormat.java	2012-05-24 11:53:30.319917568 -0400
@@ -18,9 +18,9 @@
  */
 
 import java.io.IOException;
-import java.util.Set;
+import java.util.Collection;
 
-import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.index.SegmentInfoPerCommit;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.IOContext;
 import org.apache.lucene.util.Bits;
@@ -29,13 +29,20 @@
 /** Format for live/deleted documents
  * @lucene.experimental */
 public abstract class LiveDocsFormat {
-  /** creates a new mutablebits, with all bits set, for the specified size */
+  /** Creates a new MutableBits, with all bits set, for the specified size. */
   public abstract MutableBits newLiveDocs(int size) throws IOException;
-  /** creates a new mutablebits of the same bits set and size of existing */
+
+  /** Creates a new mutablebits of the same bits set and size of existing. */
   public abstract MutableBits newLiveDocs(Bits existing) throws IOException;
-  /** reads bits from a file */
-  public abstract Bits readLiveDocs(Directory dir, SegmentInfo info, IOContext context) throws IOException;
-  /** writes bits to a file */
-  public abstract void writeLiveDocs(MutableBits bits, Directory dir, SegmentInfo info, IOContext context) throws IOException;
-  public abstract void files(SegmentInfo info, Set<String> files) throws IOException;
+
+  /** Read live docs bits. */
+  public abstract Bits readLiveDocs(Directory dir, SegmentInfoPerCommit info, IOContext context) throws IOException;
+
+  /** Persist live docs bits.  Use {@link
+   *  SegmentInfoPerCommit#getNextDelGen} to determine the
+   *  generation of the deletes file you should write to. */
+  public abstract void writeLiveDocs(MutableBits bits, Directory dir, SegmentInfoPerCommit info, int newDelCount, IOContext context) throws IOException;
+
+  /** Records all files in use by this {@link SegmentInfoPerCommit} into the files argument. */
+  public abstract void files(SegmentInfoPerCommit info, Collection<String> files) throws IOException;
 }


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/lucene3x/Lucene3xCodec.java lucene4055/lucene/core/src/java/org/apache/lucene/codecs/lucene3x/Lucene3xCodec.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/lucene3x/Lucene3xCodec.java	2012-05-24 16:55:48.004233429 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/codecs/lucene3x/Lucene3xCodec.java	2012-05-24 14:54:12.008106374 -0400
@@ -18,6 +18,7 @@
  */
 
 import java.io.IOException;
+import java.util.HashSet;
 import java.util.Set;
 
 import org.apache.lucene.codecs.Codec;
@@ -28,13 +29,14 @@
 import org.apache.lucene.codecs.PerDocConsumer;
 import org.apache.lucene.codecs.PerDocProducer;
 import org.apache.lucene.codecs.PostingsFormat;
-import org.apache.lucene.codecs.SegmentInfosFormat;
+import org.apache.lucene.codecs.SegmentInfoFormat;
 import org.apache.lucene.codecs.StoredFieldsFormat;
 import org.apache.lucene.codecs.TermVectorsFormat;
 import org.apache.lucene.codecs.lucene40.Lucene40LiveDocsFormat;
 import org.apache.lucene.index.IndexFileNames;
 import org.apache.lucene.index.PerDocWriteState;
 import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.index.SegmentInfoPerCommit;
 import org.apache.lucene.index.SegmentReadState;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.IOContext;
@@ -58,7 +60,7 @@
   
   private final FieldInfosFormat fieldInfosFormat = new Lucene3xFieldInfosFormat();
 
-  private final SegmentInfosFormat infosFormat = new Lucene3xSegmentInfosFormat();
+  private final SegmentInfoFormat infosFormat = new Lucene3xSegmentInfoFormat();
   
   private final Lucene3xNormsFormat normsFormat = new Lucene3xNormsFormat();
   
@@ -68,7 +70,7 @@
   // TODO: this should really be a different impl
   private final LiveDocsFormat liveDocsFormat = new Lucene40LiveDocsFormat() {
     @Override
-    public void writeLiveDocs(MutableBits bits, Directory dir, SegmentInfo info, IOContext context) throws IOException {
+    public void writeLiveDocs(MutableBits bits, Directory dir, SegmentInfoPerCommit info, int newDelCount, IOContext context) throws IOException {
       throw new UnsupportedOperationException("this codec can only be used for reading");
     }
   };
@@ -84,9 +86,6 @@
     public PerDocProducer docsProducer(SegmentReadState state) throws IOException {
       return null;
     }
-
-    @Override
-    public void files(SegmentInfo info, Set<String> files) throws IOException {}
   };
   
   @Override
@@ -115,7 +114,7 @@
   }
 
   @Override
-  public SegmentInfosFormat segmentInfosFormat() {
+  public SegmentInfoFormat segmentInfoFormat() {
     return infosFormat;
   }
 
@@ -128,31 +127,25 @@
   public LiveDocsFormat liveDocsFormat() {
     return liveDocsFormat;
   }
-  
-  // overrides the default implementation in codec.java to handle CFS without CFE, 
-  // shared doc stores, compound doc stores, separate norms, etc
-  @Override
-  public void files(SegmentInfo info, Set<String> files) throws IOException {
-    if (info.getUseCompoundFile()) {
-      files.add(IndexFileNames.segmentFileName(info.name, "", IndexFileNames.COMPOUND_FILE_EXTENSION));
+
+  /** Returns file names for shared doc stores, if any, else
+   * null. */
+  public static Set<String> getDocStoreFiles(SegmentInfo info) {
+    if (Lucene3xSegmentInfoFormat.getDocStoreOffset(info) != -1) {
+      final String dsName = Lucene3xSegmentInfoFormat.getDocStoreSegment(info);
+      Set<String> files = new HashSet<String>();
+      if (Lucene3xSegmentInfoFormat.getDocStoreIsCompoundFile(info)) {
+        files.add(IndexFileNames.segmentFileName(dsName, "", COMPOUND_FILE_STORE_EXTENSION));
+      } else {
+        files.add(IndexFileNames.segmentFileName(dsName, "", Lucene3xStoredFieldsReader.FIELDS_INDEX_EXTENSION));
+        files.add(IndexFileNames.segmentFileName(dsName, "", Lucene3xStoredFieldsReader.FIELDS_EXTENSION));
+        files.add(IndexFileNames.segmentFileName(dsName, "", Lucene3xTermVectorsReader.VECTORS_INDEX_EXTENSION));
+        files.add(IndexFileNames.segmentFileName(dsName, "", Lucene3xTermVectorsReader.VECTORS_FIELDS_EXTENSION));
+        files.add(IndexFileNames.segmentFileName(dsName, "", Lucene3xTermVectorsReader.VECTORS_DOCUMENTS_EXTENSION));
+      }
+      return files;
     } else {
-      postingsFormat().files(info, "", files);
-      storedFieldsFormat().files(info, files);
-      termVectorsFormat().files(info, files);
-      fieldInfosFormat().files(info, files);
-      // TODO: segmentInfosFormat should be allowed to declare additional files
-      // if it wants, in addition to segments_N
-      docValuesFormat().files(info, files);
-      normsFormat().files(info, files);
-    }
-    // never inside CFS
-    liveDocsFormat().files(info, files);
-    ((Lucene3xNormsFormat)normsFormat()).separateFiles(info, files);
-    
-    // shared docstores: these guys check the hair
-    if (info.getDocStoreOffset() != -1) {
-      storedFieldsFormat().files(info, files);
-      termVectorsFormat().files(info, files);
+      return null;
     }
-  }  
+  }
 }


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/lucene3x/Lucene3xFieldInfosFormat.java lucene4055/lucene/core/src/java/org/apache/lucene/codecs/lucene3x/Lucene3xFieldInfosFormat.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/lucene3x/Lucene3xFieldInfosFormat.java	2012-05-24 16:55:47.996233427 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/codecs/lucene3x/Lucene3xFieldInfosFormat.java	2012-05-24 11:56:15.771920452 -0400
@@ -18,12 +18,10 @@
  */
 
 import java.io.IOException;
-import java.util.Set;
 
 import org.apache.lucene.codecs.FieldInfosFormat;
 import org.apache.lucene.codecs.FieldInfosReader;
 import org.apache.lucene.codecs.FieldInfosWriter;
-import org.apache.lucene.index.SegmentInfo;
 
 /**
  * Lucene3x ReadOnly FieldInfosFromat implementation
@@ -44,9 +42,4 @@
   public FieldInfosWriter getFieldInfosWriter() throws IOException {
     throw new UnsupportedOperationException("this codec can only be used for reading");
   }
-
-  @Override
-  public void files(SegmentInfo info, Set<String> files) throws IOException {
-    Lucene3xFieldInfosReader.files(info, files);
-  }
 }


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/lucene3x/Lucene3xFieldInfosReader.java lucene4055/lucene/core/src/java/org/apache/lucene/codecs/lucene3x/Lucene3xFieldInfosReader.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/lucene3x/Lucene3xFieldInfosReader.java	2012-05-24 16:55:48.000233428 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/codecs/lucene3x/Lucene3xFieldInfosReader.java	2012-05-24 11:56:31.219920721 -0400
@@ -1,4 +1,5 @@
 package org.apache.lucene.codecs.lucene3x;
+
 /**
  * Licensed to the Apache Software Foundation (ASF) under one or more
  * contributor license agreements.  See the NOTICE file distributed with
@@ -15,8 +16,9 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
+
 import java.io.IOException;
-import java.util.Set;
+import java.util.Collections;
 
 import org.apache.lucene.codecs.FieldInfosReader;
 import org.apache.lucene.index.CorruptIndexException;
@@ -26,7 +28,6 @@
 import org.apache.lucene.index.IndexFileNames;
 import org.apache.lucene.index.IndexFormatTooNewException;
 import org.apache.lucene.index.IndexFormatTooOldException;
-import org.apache.lucene.index.SegmentInfo;
 import org.apache.lucene.index.FieldInfo.IndexOptions;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.IOContext;
@@ -58,10 +59,6 @@
   public FieldInfos read(Directory directory, String segmentName, IOContext iocontext) throws IOException {
     final String fileName = IndexFileNames.segmentFileName(segmentName, "", FIELD_INFOS_EXTENSION);
     IndexInput input = directory.openInput(fileName, iocontext);
-
-    boolean hasVectors = false;
-    boolean hasFreq = false;
-    boolean hasProx = false;
     
     try {
       final int format = input.readVInt();
@@ -103,23 +100,16 @@
         if (indexOptions != IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) {
           storePayloads = false;
         }
-        hasVectors |= storeTermVector;
-        hasProx |= isIndexed && indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS;
-        hasFreq |= isIndexed && indexOptions != IndexOptions.DOCS_ONLY;
         infos[i] = new FieldInfo(name, isIndexed, fieldNumber, storeTermVector, 
-          omitNorms, storePayloads, indexOptions, null, isIndexed && !omitNorms? Type.FIXED_INTS_8 : null);
+          omitNorms, storePayloads, indexOptions, null, isIndexed && !omitNorms? Type.FIXED_INTS_8 : null, Collections.<String,String>emptyMap());
       }
 
       if (input.getFilePointer() != input.length()) {
         throw new CorruptIndexException("did not read all bytes from file \"" + fileName + "\": read " + input.getFilePointer() + " vs size " + input.length() + " (resource: " + input + ")");
       }
-      return new FieldInfos(infos, hasFreq, hasProx, hasVectors);
+      return new FieldInfos(infos);
     } finally {
       input.close();
     }
   }
-  
-  public static void files(SegmentInfo info, Set<String> files) throws IOException {
-    files.add(IndexFileNames.segmentFileName(info.name, "", FIELD_INFOS_EXTENSION));
-  }
 }


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/lucene3x/Lucene3xFields.java lucene4055/lucene/core/src/java/org/apache/lucene/codecs/lucene3x/Lucene3xFields.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/lucene3x/Lucene3xFields.java	2012-05-24 16:55:48.000233428 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/codecs/lucene3x/Lucene3xFields.java	2012-05-24 11:56:38.851920855 -0400
@@ -18,7 +18,6 @@
  */
 
 import java.io.IOException;
-import java.util.Collection;
 import java.util.Comparator;
 import java.util.HashMap;
 import java.util.Iterator;
@@ -96,10 +95,10 @@
       freqStream = dir.openInput(IndexFileNames.segmentFileName(info.name, "", Lucene3xPostingsFormat.FREQ_EXTENSION), context);
       boolean anyProx = false;
       for (FieldInfo fi : fieldInfos) {
-        if (fi.isIndexed) {
+        if (fi.isIndexed()) {
           fields.put(fi.name, fi);
           preTerms.put(fi.name, new PreTerms(fi));
-          if (fi.indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) {
+          if (fi.getIndexOptions() == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) {
             anyProx = true;
           }
         }
@@ -133,23 +132,6 @@
     return true;
   }
 
-  static void files(SegmentInfo info, Collection<String> files) throws IOException {
-    files.add(IndexFileNames.segmentFileName(info.name, "", Lucene3xPostingsFormat.TERMS_EXTENSION));
-    files.add(IndexFileNames.segmentFileName(info.name, "", Lucene3xPostingsFormat.TERMS_INDEX_EXTENSION));
-    files.add(IndexFileNames.segmentFileName(info.name, "", Lucene3xPostingsFormat.FREQ_EXTENSION));
-    if (info.getHasProx()) {
-      // LUCENE-1739: for certain versions of 2.9-dev,
-      // hasProx would be incorrectly computed during
-      // indexing as true, and then stored into the segments
-      // file, when it should have been false.  So we do the
-      // extra check, here:
-      final String prx = IndexFileNames.segmentFileName(info.name, "", Lucene3xPostingsFormat.PROX_EXTENSION);
-      if (info.dir.fileExists(prx)) {
-        files.add(prx);
-      }
-    }
-  }
-
   @Override
   public FieldsEnum iterator() throws IOException {
     return new PreFlexFieldsEnum();
@@ -952,7 +934,7 @@
     @Override
     public DocsEnum docs(Bits liveDocs, DocsEnum reuse, boolean needsFreqs) throws IOException {
       PreDocsEnum docsEnum;
-      if (needsFreqs && fieldInfo.indexOptions == IndexOptions.DOCS_ONLY) {
+      if (needsFreqs && fieldInfo.getIndexOptions() == IndexOptions.DOCS_ONLY) {
         return null;
       } else if (reuse == null || !(reuse instanceof PreDocsEnum)) {
         docsEnum = new PreDocsEnum();
@@ -973,7 +955,7 @@
       }
 
       PreDocsAndPositionsEnum docsPosEnum;
-      if (fieldInfo.indexOptions != IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) {
+      if (fieldInfo.getIndexOptions() != IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) {
         return null;
       } else if (reuse == null || !(reuse instanceof PreDocsAndPositionsEnum)) {
         docsPosEnum = new PreDocsAndPositionsEnum();


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/lucene3x/Lucene3xNormsFormat.java lucene4055/lucene/core/src/java/org/apache/lucene/codecs/lucene3x/Lucene3xNormsFormat.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/lucene3x/Lucene3xNormsFormat.java	2012-05-24 16:55:48.000233428 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/codecs/lucene3x/Lucene3xNormsFormat.java	2012-05-24 11:56:47.935921009 -0400
@@ -18,13 +18,11 @@
  */
 
 import java.io.IOException;
-import java.util.Set;
 
 import org.apache.lucene.codecs.NormsFormat;
 import org.apache.lucene.codecs.PerDocConsumer;
 import org.apache.lucene.codecs.PerDocProducer;
 import org.apache.lucene.index.PerDocWriteState;
-import org.apache.lucene.index.SegmentInfo;
 import org.apache.lucene.index.SegmentReadState;
 
 /**
@@ -36,17 +34,6 @@
 @Deprecated
 class Lucene3xNormsFormat extends NormsFormat {
 
-
-  @Override
-  public void files(SegmentInfo info, Set<String> files) throws IOException {
-    Lucene3xNormsProducer.files(info, files);
-  }
-
-  public void separateFiles(SegmentInfo info, Set<String> files) throws IOException {
-    Lucene3xNormsProducer.separateFiles(info, files);
-  }
-
-
   @Override
   public PerDocConsumer docsConsumer(PerDocWriteState state) throws IOException {
     throw new UnsupportedOperationException("this codec can only be used for reading");


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/lucene3x/Lucene3xNormsProducer.java lucene4055/lucene/core/src/java/org/apache/lucene/codecs/lucene3x/Lucene3xNormsProducer.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/lucene3x/Lucene3xNormsProducer.java	2012-05-24 16:55:48.000233428 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/codecs/lucene3x/Lucene3xNormsProducer.java	2012-05-24 10:05:46.851805016 -0400
@@ -23,7 +23,6 @@
 import java.util.IdentityHashMap;
 import java.util.Map;
 import java.util.Set;
-import java.util.Map.Entry;
 
 import org.apache.lucene.codecs.PerDocProducer;
 import org.apache.lucene.index.DocValues;
@@ -69,16 +68,15 @@
   // but we just don't do any seeks or reading yet.
   public Lucene3xNormsProducer(Directory dir, SegmentInfo info, FieldInfos fields, IOContext context) throws IOException {
     Directory separateNormsDir = info.dir; // separate norms are never inside CFS
-    maxdoc = info.docCount;
+    maxdoc = info.getDocCount();
     String segmentName = info.name;
-    Map<Integer,Long> normGen = info.getNormGen();
     boolean success = false;
     try {
       long nextNormSeek = NORMS_HEADER.length; //skip header (header unused for now)
       for (FieldInfo fi : fields) {
         if (fi.hasNorms()) {
-          String fileName = getNormFilename(segmentName, normGen, fi.number);
-          Directory d = hasSeparateNorms(normGen, fi.number) ? separateNormsDir : dir;
+          String fileName = getNormFilename(info, fi.number);
+          Directory d = hasSeparateNorms(info, fi.number) ? separateNormsDir : dir;
         
           // singleNormFile means multiple norms share this file
           boolean singleNormFile = IndexFileNames.matchesExtension(fileName, NORMS_EXTENSION);
@@ -142,22 +140,24 @@
     }
   }
   
-  private static String getNormFilename(String segmentName, Map<Integer,Long> normGen, int number) {
-    if (hasSeparateNorms(normGen, number)) {
-      return IndexFileNames.fileNameFromGeneration(segmentName, SEPARATE_NORMS_EXTENSION + number, normGen.get(number));
+  private static String getNormFilename(SegmentInfo info, int number) {
+    if (hasSeparateNorms(info, number)) {
+      long gen = Long.parseLong(info.getAttribute(Lucene3xSegmentInfoFormat.NORMGEN_PREFIX + number));
+      return IndexFileNames.fileNameFromGeneration(info.name, SEPARATE_NORMS_EXTENSION + number, gen);
     } else {
       // single file for all norms
-      return IndexFileNames.fileNameFromGeneration(segmentName, NORMS_EXTENSION, SegmentInfo.WITHOUT_GEN);
+      return IndexFileNames.segmentFileName(info.name, "", NORMS_EXTENSION);
     }
   }
   
-  private static boolean hasSeparateNorms(Map<Integer,Long> normGen, int number) {
-    if (normGen == null) {
+  private static boolean hasSeparateNorms(SegmentInfo info, int number) {
+    String v = info.getAttribute(Lucene3xSegmentInfoFormat.NORMGEN_PREFIX + number);
+    if (v == null) {
       return false;
+    } else {
+      assert Long.parseLong(v) != SegmentInfo.NO;
+      return true;
     }
-
-    Long gen = normGen.get(number);
-    return gen != null && gen.longValue() != SegmentInfo.NO;
   }
   
   static final class NormSource extends Source {
@@ -192,29 +192,6 @@
     }
     
   }
-  
-  static void files(SegmentInfo info, Set<String> files) throws IOException {
-    // TODO: This is what SI always did... but we can do this cleaner?
-    // like first FI that has norms but doesn't have separate norms?
-    final String normsFileName = IndexFileNames.segmentFileName(info.name, "", NORMS_EXTENSION);
-    if (info.dir.fileExists(normsFileName)) {
-      // only needed to do this in 3x - 4x can decide if the norms are present
-      files.add(normsFileName);
-    }
-  }
-  
-  static void separateFiles(SegmentInfo info, Set<String> files) throws IOException {
-    Map<Integer,Long> normGen = info.getNormGen();
-    if (normGen != null) {
-      for (Entry<Integer,Long> entry : normGen.entrySet()) {
-        long gen = entry.getValue();
-        if (gen >= SegmentInfo.YES) {
-          // Definitely a separate norm file, with generation:
-          files.add(IndexFileNames.fileNameFromGeneration(info.name, SEPARATE_NORMS_EXTENSION + entry.getKey(), gen));
-        }
-      }
-    }
-  }
 
   private class NormsDocValues extends DocValues {
     private final IndexInput file;


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/lucene3x/Lucene3xPostingsFormat.java lucene4055/lucene/core/src/java/org/apache/lucene/codecs/lucene3x/Lucene3xPostingsFormat.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/lucene3x/Lucene3xPostingsFormat.java	2012-05-24 16:55:48.004233429 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/codecs/lucene3x/Lucene3xPostingsFormat.java	2012-05-24 11:57:30.675921755 -0400
@@ -17,19 +17,17 @@
  * limitations under the License.
  */
 
-import java.util.Set;
 import java.io.IOException;
 
 import org.apache.lucene.codecs.FieldsConsumer;
 import org.apache.lucene.codecs.FieldsProducer;
 import org.apache.lucene.codecs.PostingsFormat;
-import org.apache.lucene.index.SegmentInfo;
 import org.apache.lucene.index.SegmentWriteState;
 import org.apache.lucene.index.SegmentReadState;
 
 /** Codec that reads the pre-flex-indexing postings
  *  format.  It does not provide a writer because newly
- *  written segments should use StandardCodec.
+ *  written segments should use the Codec configured on IndexWriter.
  *
  * @deprecated (4.0) This is only used to read indexes created
  * before 4.0.
@@ -63,10 +61,4 @@
   public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {
     return new Lucene3xFields(state.dir, state.fieldInfos, state.segmentInfo, state.context, state.termsIndexDivisor);
   }
-
-  @Override
-  public void files(SegmentInfo info, String segmentSuffix, Set<String> files) throws IOException {
-    // preflex fields have no segmentSuffix - we ignore it here
-    Lucene3xFields.files(info, files);
-  }
 }


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/lucene3x/Lucene3xSegmentInfoFormat.java lucene4055/lucene/core/src/java/org/apache/lucene/codecs/lucene3x/Lucene3xSegmentInfoFormat.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/lucene3x/Lucene3xSegmentInfoFormat.java	1969-12-31 19:00:00.000000000 -0500
+++ lucene4055/lucene/core/src/java/org/apache/lucene/codecs/lucene3x/Lucene3xSegmentInfoFormat.java	2012-05-24 09:53:16.451791948 -0400
@@ -0,0 +1,89 @@
+package org.apache.lucene.codecs.lucene3x;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.codecs.SegmentInfoFormat;
+import org.apache.lucene.codecs.SegmentInfoReader;
+import org.apache.lucene.codecs.SegmentInfoWriter;
+import org.apache.lucene.index.SegmentInfo;
+
+/**
+ * Lucene3x ReadOnly SegmentInfoFormat implementation
+ * @deprecated (4.0) This is only used to read indexes created
+ * before 4.0.
+ * @lucene.experimental
+ */
+@Deprecated
+public class Lucene3xSegmentInfoFormat extends SegmentInfoFormat {
+  private final SegmentInfoReader reader = new Lucene3xSegmentInfoReader();
+
+  /** This format adds optional per-segment String
+   *  diagnostics storage, and switches userData to Map */
+  public static final int FORMAT_DIAGNOSTICS = -9;
+
+  /** Each segment records whether it has term vectors */
+  public static final int FORMAT_HAS_VECTORS = -10;
+
+  /** Each segment records the Lucene version that created it. */
+  public static final int FORMAT_3_1 = -11;
+
+  /** Extension used for saving each SegmentInfo, once a 3.x
+   *  index is first committed to with 4.0. */
+  public static final String UPGRADED_SI_EXTENSION = "si";
+  public static final String UPGRADED_SI_CODEC_NAME = "Lucene3xSegmentInfo";
+  public static final int UPGRADED_SI_VERSION_START = 0;
+  public static final int UPGRADED_SI_VERSION_CURRENT = UPGRADED_SI_VERSION_START;
+  
+  @Override
+  public SegmentInfoReader getSegmentInfosReader() {
+    return reader;
+  }
+
+  @Override
+  public SegmentInfoWriter getSegmentInfosWriter() {
+    throw new UnsupportedOperationException("this codec can only be used for reading");
+  }
+  
+  // only for backwards compat
+  public static final String DS_OFFSET_KEY = Lucene3xSegmentInfoFormat.class.getSimpleName() + ".dsoffset";
+  public static final String DS_NAME_KEY = Lucene3xSegmentInfoFormat.class.getSimpleName() + ".dsname";
+  public static final String DS_COMPOUND_KEY = Lucene3xSegmentInfoFormat.class.getSimpleName() + ".dscompound";
+  public static final String NORMGEN_KEY = Lucene3xSegmentInfoFormat.class.getSimpleName() + ".normgen";
+  public static final String NORMGEN_PREFIX = Lucene3xSegmentInfoFormat.class.getSimpleName() + ".normfield";
+
+  /** 
+   * @return if this segment shares stored fields & vectors, this
+   *         offset is where in that file this segment's docs begin 
+   */
+  public static int getDocStoreOffset(SegmentInfo si) {
+    String v = si.getAttribute(DS_OFFSET_KEY);
+    return v == null ? -1 : Integer.parseInt(v);
+  }
+  
+  /** @return name used to derive fields/vectors file we share with other segments */
+  public static String getDocStoreSegment(SegmentInfo si) {
+    String v = si.getAttribute(DS_NAME_KEY);
+    return v == null ? si.name : v;
+  }
+  
+  /** @return whether doc store files are stored in compound file (*.cfx) */
+  public static boolean getDocStoreIsCompoundFile(SegmentInfo si) {
+    String v = si.getAttribute(DS_COMPOUND_KEY);
+    return v == null ? false : Boolean.parseBoolean(v);
+  }
+}


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/lucene3x/Lucene3xSegmentInfoReader.java lucene4055/lucene/core/src/java/org/apache/lucene/codecs/lucene3x/Lucene3xSegmentInfoReader.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/lucene3x/Lucene3xSegmentInfoReader.java	1969-12-31 19:00:00.000000000 -0500
+++ lucene4055/lucene/core/src/java/org/apache/lucene/codecs/lucene3x/Lucene3xSegmentInfoReader.java	2012-05-24 10:01:02.275800057 -0400
@@ -0,0 +1,272 @@
+package org.apache.lucene.codecs.lucene3x;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.Map;
+import java.util.Set;
+
+import org.apache.lucene.codecs.SegmentInfoReader;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.IndexFormatTooNewException;
+import org.apache.lucene.index.IndexFormatTooOldException;
+import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.index.SegmentInfoPerCommit;
+import org.apache.lucene.index.SegmentInfos;
+import org.apache.lucene.store.CompoundFileDirectory;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.util.CodecUtil;
+import org.apache.lucene.util.IOUtils;
+
+/**
+ * Lucene 3x implementation of {@link SegmentInfoReader}.
+ * @lucene.experimental
+ * @deprecated
+ */
+@Deprecated
+public class Lucene3xSegmentInfoReader extends SegmentInfoReader {
+
+  public static void readLegacyInfos(SegmentInfos infos, Directory directory, IndexInput input, int format) throws IOException {
+    infos.version = input.readLong(); // read version
+    infos.counter = input.readInt(); // read counter
+    Lucene3xSegmentInfoReader reader = new Lucene3xSegmentInfoReader();
+    for (int i = input.readInt(); i > 0; i--) { // read segmentInfos
+      SegmentInfoPerCommit siPerCommit = reader.readLegacySegmentInfo(directory, format, input);
+      SegmentInfo si = siPerCommit.info;
+
+      if (si.getVersion() == null) {
+        // Could be a 3.0 - try to open the doc stores - if it fails, it's a
+        // 2.x segment, and an IndexFormatTooOldException will be thrown,
+        // which is what we want.
+        Directory dir = directory;
+        if (Lucene3xSegmentInfoFormat.getDocStoreOffset(si) != -1) {
+          if (Lucene3xSegmentInfoFormat.getDocStoreIsCompoundFile(si)) {
+            dir = new CompoundFileDirectory(dir, IndexFileNames.segmentFileName(
+                Lucene3xSegmentInfoFormat.getDocStoreSegment(si), "",
+                Lucene3xCodec.COMPOUND_FILE_STORE_EXTENSION), IOContext.READONCE, false);
+          }
+        } else if (si.getUseCompoundFile()) {
+          dir = new CompoundFileDirectory(dir, IndexFileNames.segmentFileName(
+              si.name, "", IndexFileNames.COMPOUND_FILE_EXTENSION), IOContext.READONCE, false);
+        }
+
+        try {
+          Lucene3xStoredFieldsReader.checkCodeVersion(dir, Lucene3xSegmentInfoFormat.getDocStoreSegment(si));
+        } finally {
+          // If we opened the directory, close it
+          if (dir != directory) dir.close();
+        }
+          
+        // Above call succeeded, so it's a 3.0 segment. Upgrade it so the next
+        // time the segment is read, its version won't be null and we won't
+        // need to open FieldsReader every time for each such segment.
+        si.setVersion("3.0");
+      } else if (si.getVersion().equals("2.x")) {
+        // If it's a 3x index touched by 3.1+ code, then segments record their
+        // version, whether they are 2.x ones or not. We detect that and throw
+        // appropriate exception.
+        throw new IndexFormatTooOldException("segment " + si.name + " in resource " + input, si.getVersion());
+      }
+      infos.add(siPerCommit);
+    }
+      
+    infos.userData = input.readStringStringMap();
+  }
+
+  @Override
+  public SegmentInfo read(Directory directory, String segmentName, IOContext context) throws IOException { 
+    // NOTE: this is NOT how 3.x is really written...
+    String fileName = IndexFileNames.segmentFileName(segmentName, "", Lucene3xSegmentInfoFormat.UPGRADED_SI_EXTENSION);
+
+    boolean success = false;
+
+    IndexInput input = directory.openInput(fileName, context);
+
+    try {
+      SegmentInfo si = readUpgradedSegmentInfo(segmentName, directory, input);
+      success = true;
+      return si;
+    } finally {
+      if (!success) {
+        IOUtils.closeWhileHandlingException(input);
+      } else {
+        input.close();
+      }
+    }
+  }
+
+  private static void addIfExists(Directory dir, Set<String> files, String fileName) throws IOException {
+    if (dir.fileExists(fileName)) {
+      files.add(fileName);
+    }
+  }
+  
+  /** reads from legacy 3.x segments_N */
+  private SegmentInfoPerCommit readLegacySegmentInfo(Directory dir, int format, IndexInput input) throws IOException {
+    // check that it is a format we can understand
+    if (format > Lucene3xSegmentInfoFormat.FORMAT_DIAGNOSTICS) {
+      throw new IndexFormatTooOldException(input, format,
+                                           Lucene3xSegmentInfoFormat.FORMAT_DIAGNOSTICS, Lucene3xSegmentInfoFormat.FORMAT_3_1);
+    }
+    if (format < Lucene3xSegmentInfoFormat.FORMAT_3_1) {
+      throw new IndexFormatTooNewException(input, format,
+                                           Lucene3xSegmentInfoFormat.FORMAT_DIAGNOSTICS, Lucene3xSegmentInfoFormat.FORMAT_3_1);
+    }
+    final String version;
+    if (format <= Lucene3xSegmentInfoFormat.FORMAT_3_1) {
+      version = input.readString();
+    } else {
+      version = null;
+    }
+
+    final String name = input.readString();
+
+    final int docCount = input.readInt();
+    final long delGen = input.readLong();
+    
+    final int docStoreOffset = input.readInt();
+    final Map<String,String> attributes = new HashMap<String,String>();
+    
+    // parse the docstore stuff and shove it into attributes
+    final String docStoreSegment;
+    final boolean docStoreIsCompoundFile;
+    if (docStoreOffset != -1) {
+      docStoreSegment = input.readString();
+      docStoreIsCompoundFile = input.readByte() == SegmentInfo.YES;
+      attributes.put(Lucene3xSegmentInfoFormat.DS_OFFSET_KEY, Integer.toString(docStoreOffset));
+      attributes.put(Lucene3xSegmentInfoFormat.DS_NAME_KEY, docStoreSegment);
+      attributes.put(Lucene3xSegmentInfoFormat.DS_COMPOUND_KEY, Boolean.toString(docStoreIsCompoundFile));
+    } else {
+      docStoreSegment = name;
+      docStoreIsCompoundFile = false;
+    }
+
+    // pre-4.0 indexes write a byte if there is a single norms file
+    byte b = input.readByte();
+
+    //System.out.println("version=" + version + " name=" + name + " docCount=" + docCount + " delGen=" + delGen + " dso=" + docStoreOffset + " dss=" + docStoreSegment + " dssCFs=" + docStoreIsCompoundFile + " b=" + b + " format=" + format);
+
+    assert 1 == b : "expected 1 but was: "+ b + " format: " + format;
+    final int numNormGen = input.readInt();
+    final Map<Integer,Long> normGen;
+    if (numNormGen == SegmentInfo.NO) {
+      normGen = null;
+    } else {
+      normGen = new HashMap<Integer, Long>();
+      for(int j=0;j<numNormGen;j++) {
+        normGen.put(j, input.readLong());
+      }
+    }
+    final boolean isCompoundFile = input.readByte() == SegmentInfo.YES;
+
+    final int delCount = input.readInt();
+    assert delCount <= docCount;
+
+    final boolean hasProx = input.readByte() == 1;
+
+    final Map<String,String> diagnostics = input.readStringStringMap();
+
+    if (format <= Lucene3xSegmentInfoFormat.FORMAT_HAS_VECTORS) {
+      // NOTE: unused
+      final int hasVectors = input.readByte();
+    }
+
+    // Replicate logic from 3.x's SegmentInfo.files():
+    final Set<String> files = new HashSet<String>();
+    if (isCompoundFile) {
+      files.add(IndexFileNames.segmentFileName(name, "", IndexFileNames.COMPOUND_FILE_EXTENSION));
+    } else {
+      addIfExists(dir, files, IndexFileNames.segmentFileName(name, "", Lucene3xFieldInfosReader.FIELD_INFOS_EXTENSION));
+      addIfExists(dir, files, IndexFileNames.segmentFileName(name, "", Lucene3xPostingsFormat.FREQ_EXTENSION));
+      addIfExists(dir, files, IndexFileNames.segmentFileName(name, "", Lucene3xPostingsFormat.PROX_EXTENSION));
+      addIfExists(dir, files, IndexFileNames.segmentFileName(name, "", Lucene3xPostingsFormat.TERMS_EXTENSION));
+      addIfExists(dir, files, IndexFileNames.segmentFileName(name, "", Lucene3xPostingsFormat.TERMS_INDEX_EXTENSION));
+      addIfExists(dir, files, IndexFileNames.segmentFileName(name, "", Lucene3xNormsProducer.NORMS_EXTENSION));
+    }
+    
+    if (docStoreOffset != -1) {
+      if (docStoreIsCompoundFile) {
+        files.add(IndexFileNames.segmentFileName(docStoreSegment, "", Lucene3xCodec.COMPOUND_FILE_STORE_EXTENSION));
+      } else {
+        files.add(IndexFileNames.segmentFileName(docStoreSegment, "", Lucene3xStoredFieldsReader.FIELDS_INDEX_EXTENSION));
+        files.add(IndexFileNames.segmentFileName(docStoreSegment, "", Lucene3xStoredFieldsReader.FIELDS_EXTENSION));
+        addIfExists(dir, files, IndexFileNames.segmentFileName(docStoreSegment, "", Lucene3xTermVectorsReader.VECTORS_INDEX_EXTENSION));
+        addIfExists(dir, files, IndexFileNames.segmentFileName(docStoreSegment, "", Lucene3xTermVectorsReader.VECTORS_FIELDS_EXTENSION));
+        addIfExists(dir, files, IndexFileNames.segmentFileName(docStoreSegment, "", Lucene3xTermVectorsReader.VECTORS_DOCUMENTS_EXTENSION));
+      }
+    } else if (!isCompoundFile) {
+      files.add(IndexFileNames.segmentFileName(name, "", Lucene3xStoredFieldsReader.FIELDS_INDEX_EXTENSION));
+      files.add(IndexFileNames.segmentFileName(name, "", Lucene3xStoredFieldsReader.FIELDS_EXTENSION));
+      addIfExists(dir, files, IndexFileNames.segmentFileName(name, "", Lucene3xTermVectorsReader.VECTORS_INDEX_EXTENSION));
+      addIfExists(dir, files, IndexFileNames.segmentFileName(name, "", Lucene3xTermVectorsReader.VECTORS_FIELDS_EXTENSION));
+      addIfExists(dir, files, IndexFileNames.segmentFileName(name, "", Lucene3xTermVectorsReader.VECTORS_DOCUMENTS_EXTENSION));
+    }
+    
+    // parse the normgen stuff and shove it into attributes
+    if (normGen != null) {
+      attributes.put(Lucene3xSegmentInfoFormat.NORMGEN_KEY, Integer.toString(numNormGen));
+      for(Map.Entry<Integer,Long> ent : normGen.entrySet()) {
+        long gen = ent.getValue();
+        if (gen >= SegmentInfo.YES) {
+          // Definitely a separate norm file, with generation:
+          files.add(IndexFileNames.fileNameFromGeneration(name, "s" + ent.getKey(), gen));
+          attributes.put(Lucene3xSegmentInfoFormat.NORMGEN_PREFIX + ent.getKey(), Long.toString(gen));
+        } else if (gen == SegmentInfo.NO) {
+          // No separate norm
+        } else {
+          // We should have already hit indexformat too old exception
+          assert false;
+        }
+      }
+    }
+
+    SegmentInfo info = new SegmentInfo(dir, version, name, docCount, isCompoundFile,
+                                       null, diagnostics, Collections.unmodifiableMap(attributes));
+    info.setFiles(files);
+
+    SegmentInfoPerCommit infoPerCommit = new SegmentInfoPerCommit(info, delCount, delGen);
+    return infoPerCommit;
+  }
+
+  private SegmentInfo readUpgradedSegmentInfo(String name, Directory dir, IndexInput input) throws IOException {
+    CodecUtil.checkHeader(input, Lucene3xSegmentInfoFormat.UPGRADED_SI_CODEC_NAME,
+                                 Lucene3xSegmentInfoFormat.UPGRADED_SI_VERSION_START,
+                                 Lucene3xSegmentInfoFormat.UPGRADED_SI_VERSION_CURRENT);
+    final String version = input.readString();
+
+    final int docCount = input.readInt();
+    
+    final Map<String,String> attributes = input.readStringStringMap();
+
+    final boolean isCompoundFile = input.readByte() == SegmentInfo.YES;
+
+    final Map<String,String> diagnostics = input.readStringStringMap();
+
+    final Set<String> files = input.readStringSet();
+
+    SegmentInfo info = new SegmentInfo(dir, version, name, docCount, isCompoundFile,
+                                       null, diagnostics, Collections.unmodifiableMap(attributes));
+    info.setFiles(files);
+    return info;
+  }
+}


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/lucene3x/Lucene3xSegmentInfosFormat.java lucene4055/lucene/core/src/java/org/apache/lucene/codecs/lucene3x/Lucene3xSegmentInfosFormat.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/lucene3x/Lucene3xSegmentInfosFormat.java	2012-05-24 16:55:48.000233428 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/codecs/lucene3x/Lucene3xSegmentInfosFormat.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,43 +0,0 @@
-package org.apache.lucene.codecs.lucene3x;
-
-import org.apache.lucene.codecs.SegmentInfosFormat;
-import org.apache.lucene.codecs.SegmentInfosReader;
-import org.apache.lucene.codecs.SegmentInfosWriter;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * Lucene3x ReadOnly SegmentInfosFormat implementation
- * @deprecated (4.0) This is only used to read indexes created
- * before 4.0.
- * @lucene.experimental
- */
-@Deprecated
-class Lucene3xSegmentInfosFormat extends SegmentInfosFormat {
-  private final SegmentInfosReader reader = new Lucene3xSegmentInfosReader();
-  
-  @Override
-  public SegmentInfosReader getSegmentInfosReader() {
-    return reader;
-  }
-
-  @Override
-  public SegmentInfosWriter getSegmentInfosWriter() {
-    throw new UnsupportedOperationException("this codec can only be used for reading");
-  }
-}


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/lucene3x/Lucene3xSegmentInfosReader.java lucene4055/lucene/core/src/java/org/apache/lucene/codecs/lucene3x/Lucene3xSegmentInfosReader.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/lucene3x/Lucene3xSegmentInfosReader.java	2012-05-24 16:55:48.004233429 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/codecs/lucene3x/Lucene3xSegmentInfosReader.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,170 +0,0 @@
-package org.apache.lucene.codecs.lucene3x;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.HashMap;
-import java.util.Map;
-
-import org.apache.lucene.codecs.Codec;
-import org.apache.lucene.codecs.SegmentInfosReader;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.IndexFormatTooOldException;
-import org.apache.lucene.index.SegmentInfo;
-import org.apache.lucene.index.SegmentInfos;
-import org.apache.lucene.store.ChecksumIndexInput;
-import org.apache.lucene.store.CompoundFileDirectory;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-
-/**
- * Lucene 3x implementation of {@link SegmentInfosReader}.
- * @lucene.experimental
- * @deprecated
- */
-@Deprecated
-class Lucene3xSegmentInfosReader extends SegmentInfosReader {
-
-  @Override
-  public void read(Directory directory, String segmentsFileName, ChecksumIndexInput input, SegmentInfos infos, IOContext context) throws IOException { 
-    infos.version = input.readLong(); // read version
-    infos.counter = input.readInt(); // read counter
-    final int format = infos.getFormat();
-    for (int i = input.readInt(); i > 0; i--) { // read segmentInfos
-      SegmentInfo si = readSegmentInfo(directory, format, input);
-      if (si.getVersion() == null) {
-        // Could be a 3.0 - try to open the doc stores - if it fails, it's a
-        // 2.x segment, and an IndexFormatTooOldException will be thrown,
-        // which is what we want.
-        Directory dir = directory;
-        if (si.getDocStoreOffset() != -1) {
-          if (si.getDocStoreIsCompoundFile()) {
-            dir = new CompoundFileDirectory(dir, IndexFileNames.segmentFileName(
-                si.getDocStoreSegment(), "",
-                Lucene3xCodec.COMPOUND_FILE_STORE_EXTENSION), context, false);
-          }
-        } else if (si.getUseCompoundFile()) {
-          dir = new CompoundFileDirectory(dir, IndexFileNames.segmentFileName(
-              si.name, "", IndexFileNames.COMPOUND_FILE_EXTENSION), context, false);
-        }
-
-        try {
-          Lucene3xStoredFieldsReader.checkCodeVersion(dir, si.getDocStoreSegment());
-        } finally {
-          // If we opened the directory, close it
-          if (dir != directory) dir.close();
-        }
-          
-        // Above call succeeded, so it's a 3.0 segment. Upgrade it so the next
-        // time the segment is read, its version won't be null and we won't
-        // need to open FieldsReader every time for each such segment.
-        si.setVersion("3.0");
-      } else if (si.getVersion().equals("2.x")) {
-        // If it's a 3x index touched by 3.1+ code, then segments record their
-        // version, whether they are 2.x ones or not. We detect that and throw
-        // appropriate exception.
-        throw new IndexFormatTooOldException("segment " + si.name + " in resource " + input, si.getVersion());
-      }
-      infos.add(si);
-    }
-      
-    infos.userData = input.readStringStringMap();
-  }
-  
-  // if we make a preflex impl we can remove a lot of this hair...
-  public SegmentInfo readSegmentInfo(Directory dir, int format, ChecksumIndexInput input) throws IOException {
-    final String version;
-    if (format <= SegmentInfos.FORMAT_3_1) {
-      version = input.readString();
-    } else {
-      version = null;
-    }
-    final String name = input.readString();
-    final int docCount = input.readInt();
-    final long delGen = input.readLong();
-    final int docStoreOffset = input.readInt();
-    final String docStoreSegment;
-    final boolean docStoreIsCompoundFile;
-    if (docStoreOffset != -1) {
-      docStoreSegment = input.readString();
-      docStoreIsCompoundFile = input.readByte() == SegmentInfo.YES;
-    } else {
-      docStoreSegment = name;
-      docStoreIsCompoundFile = false;
-    }
-
-    // pre-4.0 indexes write a byte if there is a single norms file
-    byte b = input.readByte();
-    assert 1 == b : "expected 1 but was: "+ b + " format: " + format;
-
-    final int numNormGen = input.readInt();
-    final Map<Integer,Long> normGen;
-    if (numNormGen == SegmentInfo.NO) {
-      normGen = null;
-    } else {
-      normGen = new HashMap<Integer, Long>();
-      for(int j=0;j<numNormGen;j++) {
-        normGen.put(j, input.readLong());
-      }
-    }
-    final boolean isCompoundFile = input.readByte() == SegmentInfo.YES;
-
-    final int delCount = input.readInt();
-    assert delCount <= docCount;
-
-    final int hasProx = input.readByte();
-
-    final Codec codec = Codec.forName("Lucene3x");
-    final Map<String,String> diagnostics = input.readStringStringMap();
-
-    final int hasVectors;
-    if (format <= SegmentInfos.FORMAT_HAS_VECTORS) {
-      hasVectors = input.readByte();
-    } else {
-      final String storesSegment;
-      final String ext;
-      final boolean storeIsCompoundFile;
-      if (docStoreOffset != -1) {
-        storesSegment = docStoreSegment;
-        storeIsCompoundFile = docStoreIsCompoundFile;
-        ext = Lucene3xCodec.COMPOUND_FILE_STORE_EXTENSION;
-      } else {
-        storesSegment = name;
-        storeIsCompoundFile = isCompoundFile;
-        ext = IndexFileNames.COMPOUND_FILE_EXTENSION;
-      }
-      final Directory dirToTest;
-      if (storeIsCompoundFile) {
-        dirToTest = new CompoundFileDirectory(dir, IndexFileNames.segmentFileName(storesSegment, "", ext), IOContext.READONCE, false);
-      } else {
-        dirToTest = dir;
-      }
-      try {
-        hasVectors = dirToTest.fileExists(IndexFileNames.segmentFileName(storesSegment, "", Lucene3xTermVectorsReader.VECTORS_INDEX_EXTENSION)) ? SegmentInfo.YES : SegmentInfo.NO;
-      } finally {
-        if (isCompoundFile) {
-          dirToTest.close();
-        }
-      }
-    }
-    
-    return new SegmentInfo(dir, version, name, docCount, delGen, docStoreOffset,
-      docStoreSegment, docStoreIsCompoundFile, normGen, isCompoundFile,
-      delCount, hasProx, codec, diagnostics, hasVectors);
-  }
-}


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/lucene3x/Lucene3xStoredFieldsFormat.java lucene4055/lucene/core/src/java/org/apache/lucene/codecs/lucene3x/Lucene3xStoredFieldsFormat.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/lucene3x/Lucene3xStoredFieldsFormat.java	2012-05-24 16:55:47.996233427 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/codecs/lucene3x/Lucene3xStoredFieldsFormat.java	2012-05-24 11:57:52.623922142 -0400
@@ -18,7 +18,6 @@
  */
 
 import java.io.IOException;
-import java.util.Set;
 
 import org.apache.lucene.codecs.StoredFieldsFormat;
 import org.apache.lucene.codecs.StoredFieldsReader;
@@ -39,13 +38,8 @@
   }
 
   @Override
-  public StoredFieldsWriter fieldsWriter(Directory directory, String segment,
+  public StoredFieldsWriter fieldsWriter(Directory directory, SegmentInfo si,
       IOContext context) throws IOException {
     throw new UnsupportedOperationException("this codec can only be used for reading");
   }
-
-  @Override
-  public void files(SegmentInfo info, Set<String> files) throws IOException {
-    Lucene3xStoredFieldsReader.files(info, files);
-  }
 }


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/lucene3x/Lucene3xStoredFieldsReader.java lucene4055/lucene/core/src/java/org/apache/lucene/codecs/lucene3x/Lucene3xStoredFieldsReader.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/lucene3x/Lucene3xStoredFieldsReader.java	2012-05-24 16:55:48.000233428 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/codecs/lucene3x/Lucene3xStoredFieldsReader.java	2012-05-23 22:10:32.479057683 -0400
@@ -36,7 +36,6 @@
 import org.apache.lucene.util.IOUtils;
 
 import java.io.Closeable;
-import java.util.Set;
 
 /**
  * Class responsible for access to stored document fields.
@@ -139,13 +138,13 @@
   }
 
   public Lucene3xStoredFieldsReader(Directory d, SegmentInfo si, FieldInfos fn, IOContext context) throws IOException {
-    final String segment = si.getDocStoreSegment();
-    final int docStoreOffset = si.getDocStoreOffset();
-    final int size = si.docCount;
+    final String segment = Lucene3xSegmentInfoFormat.getDocStoreSegment(si);
+    final int docStoreOffset = Lucene3xSegmentInfoFormat.getDocStoreOffset(si);
+    final int size = si.getDocCount();
     boolean success = false;
     fieldInfos = fn;
     try {
-      if (docStoreOffset != -1 && si.getDocStoreIsCompoundFile()) {
+      if (docStoreOffset != -1 && Lucene3xSegmentInfoFormat.getDocStoreIsCompoundFile(si)) {
         d = storeCFSReader = new CompoundFileDirectory(si.dir, 
             IndexFileNames.segmentFileName(segment, "", Lucene3xCodec.COMPOUND_FILE_STORE_EXTENSION), context, false);
       } else {
@@ -176,8 +175,8 @@
         this.docStoreOffset = 0;
         this.size = (int) (indexSize >> 3);
         // Verify two sources of "maxDoc" agree:
-        if (this.size != si.docCount) {
-          throw new CorruptIndexException("doc counts differ for segment " + segment + ": fieldsReader shows " + this.size + " but segmentInfo shows " + si.docCount);
+        if (this.size != si.getDocCount()) {
+          throw new CorruptIndexException("doc counts differ for segment " + segment + ": fieldsReader shows " + this.size + " but segmentInfo shows " + si.getDocCount());
         }
       }
       numTotalDocs = (int) (indexSize >> 3);
@@ -296,21 +295,4 @@
       fieldsStream.seek(fieldsStream.getFilePointer() + length);
     }
   }
-
-  // note: if there are shared docstores, we are also called by Lucene3xCodec even in 
-  // the CFS case. so logic here must handle this.
-  public static void files(SegmentInfo info, Set<String> files) throws IOException {
-    if (info.getDocStoreOffset() != -1) {
-      assert info.getDocStoreSegment() != null;
-      if (info.getDocStoreIsCompoundFile()) {
-        files.add(IndexFileNames.segmentFileName(info.getDocStoreSegment(), "", Lucene3xCodec.COMPOUND_FILE_STORE_EXTENSION));
-      } else {
-        files.add(IndexFileNames.segmentFileName(info.getDocStoreSegment(), "", FIELDS_INDEX_EXTENSION));
-        files.add(IndexFileNames.segmentFileName(info.getDocStoreSegment(), "", FIELDS_EXTENSION));
-      }
-    } else if (!info.getUseCompoundFile()) {
-      files.add(IndexFileNames.segmentFileName(info.name, "", FIELDS_INDEX_EXTENSION));
-      files.add(IndexFileNames.segmentFileName(info.name, "", FIELDS_EXTENSION));
-    }
-  }
 }


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/lucene3x/Lucene3xTermVectorsFormat.java lucene4055/lucene/core/src/java/org/apache/lucene/codecs/lucene3x/Lucene3xTermVectorsFormat.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/lucene3x/Lucene3xTermVectorsFormat.java	2012-05-24 16:55:48.004233429 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/codecs/lucene3x/Lucene3xTermVectorsFormat.java	2012-05-23 22:09:25.871056523 -0400
@@ -18,13 +18,14 @@
  */
 
 import java.io.IOException;
-import java.util.Set;
 
 import org.apache.lucene.codecs.TermVectorsFormat;
 import org.apache.lucene.codecs.TermVectorsReader;
 import org.apache.lucene.codecs.TermVectorsWriter;
 import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.IndexFileNames;
 import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.store.CompoundFileDirectory;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.IOContext;
 
@@ -38,18 +39,42 @@
 class Lucene3xTermVectorsFormat extends TermVectorsFormat {
 
   @Override
-  public TermVectorsReader vectorsReader(Directory directory,SegmentInfo segmentInfo, FieldInfos fieldInfos, IOContext context) throws IOException {
-    return new Lucene3xTermVectorsReader(directory, segmentInfo, fieldInfos, context);
-  }
+  public TermVectorsReader vectorsReader(Directory directory, SegmentInfo segmentInfo, FieldInfos fieldInfos, IOContext context) throws IOException {
+    final String fileName = IndexFileNames.segmentFileName(Lucene3xSegmentInfoFormat.getDocStoreSegment(segmentInfo), "", Lucene3xTermVectorsReader.VECTORS_FIELDS_EXTENSION);
 
-  @Override
-  public TermVectorsWriter vectorsWriter(Directory directory, String segment, IOContext context) throws IOException {
-    throw new UnsupportedOperationException("this codec can only be used for reading");
+    // Unfortunately, for 3.x indices, each segment's
+    // FieldInfos can lie about hasVectors (claim it's true
+    // when really it's false).... so we have to carefully
+    // check if the files really exist before trying to open
+    // them (4.x has fixed this):
+    final boolean exists;
+    if (Lucene3xSegmentInfoFormat.getDocStoreOffset(segmentInfo) != -1 && Lucene3xSegmentInfoFormat.getDocStoreIsCompoundFile(segmentInfo)) {
+      String cfxFileName = IndexFileNames.segmentFileName(Lucene3xSegmentInfoFormat.getDocStoreSegment(segmentInfo), "", Lucene3xCodec.COMPOUND_FILE_STORE_EXTENSION);
+      if (segmentInfo.dir.fileExists(cfxFileName)) {
+        Directory cfsDir = new CompoundFileDirectory(segmentInfo.dir, cfxFileName, context, false);
+        try {
+          exists = cfsDir.fileExists(fileName);
+        } finally {
+          cfsDir.close();
+        }
+      } else {
+        exists = false;
+      }
+    } else {
+      exists = directory.fileExists(fileName);
+    }
+
+    if (!exists) {
+      // 3x's FieldInfos sometimes lies and claims a segment
+      // has vectors when it doesn't:
+      return null;
+    } else {
+      return new Lucene3xTermVectorsReader(directory, segmentInfo, fieldInfos, context);
+    }
   }
 
   @Override
-  public void files(SegmentInfo info, Set<String> files) throws IOException {
-    Lucene3xTermVectorsReader.files(info, files);
+  public TermVectorsWriter vectorsWriter(Directory directory, SegmentInfo segmentInfo, IOContext context) throws IOException {
+    throw new UnsupportedOperationException("this codec can only be used for reading");
   }
-  
 }


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/lucene3x/Lucene3xTermVectorsReader.java lucene4055/lucene/core/src/java/org/apache/lucene/codecs/lucene3x/Lucene3xTermVectorsReader.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/lucene3x/Lucene3xTermVectorsReader.java	2012-05-24 16:55:47.996233427 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/codecs/lucene3x/Lucene3xTermVectorsReader.java	2012-05-23 22:07:41.875054712 -0400
@@ -22,7 +22,6 @@
 import java.util.Comparator;
 import java.util.HashMap;
 import java.util.Map;
-import java.util.Set;
 
 import org.apache.lucene.codecs.TermVectorsReader;
 import org.apache.lucene.index.CorruptIndexException;
@@ -114,14 +113,14 @@
     
   public Lucene3xTermVectorsReader(Directory d, SegmentInfo si, FieldInfos fieldInfos, IOContext context)
     throws CorruptIndexException, IOException {
-    final String segment = si.getDocStoreSegment();
-    final int docStoreOffset = si.getDocStoreOffset();
-    final int size = si.docCount;
+    final String segment = Lucene3xSegmentInfoFormat.getDocStoreSegment(si);
+    final int docStoreOffset = Lucene3xSegmentInfoFormat.getDocStoreOffset(si);
+    final int size = si.getDocCount();
     
     boolean success = false;
 
     try {
-      if (docStoreOffset != -1 && si.getDocStoreIsCompoundFile()) {
+      if (docStoreOffset != -1 && Lucene3xSegmentInfoFormat.getDocStoreIsCompoundFile(si)) {
         d = storeCFSReader = new CompoundFileDirectory(si.dir, 
             IndexFileNames.segmentFileName(segment, "", Lucene3xCodec.COMPOUND_FILE_STORE_EXTENSION), context, false);
       } else {
@@ -239,7 +238,7 @@
         @Override
         public String next() throws IOException {
           if (fieldNumbers != null && fieldUpto < fieldNumbers.length) {
-            return fieldInfos.fieldName(fieldNumbers[fieldUpto++]);
+            return fieldInfos.fieldInfo(fieldNumbers[fieldUpto++]).name;
           } else {
             return null;
           }
@@ -247,7 +246,7 @@
 
         @Override
         public Terms terms() throws IOException {
-          return TVFields.this.terms(fieldInfos.fieldName(fieldNumbers[fieldUpto-1]));
+          return TVFields.this.terms(fieldInfos.fieldInfo(fieldNumbers[fieldUpto-1]).name);
         }
       };
     }
@@ -690,27 +689,6 @@
     return new Lucene3xTermVectorsReader(fieldInfos, cloneTvx, cloneTvd, cloneTvf, size, numTotalDocs, docStoreOffset, format);
   }
   
-  // note: if there are shared docstores, we are also called by Lucene3xCodec even in 
-  // the CFS case. so logic here must handle this.
-  public static void files(SegmentInfo info, Set<String> files) throws IOException {
-    if (info.getHasVectors()) {
-      if (info.getDocStoreOffset() != -1) {
-        assert info.getDocStoreSegment() != null;
-        if (info.getDocStoreIsCompoundFile()) {
-          files.add(IndexFileNames.segmentFileName(info.getDocStoreSegment(), "", Lucene3xCodec.COMPOUND_FILE_STORE_EXTENSION));
-        } else {
-          files.add(IndexFileNames.segmentFileName(info.getDocStoreSegment(), "", VECTORS_INDEX_EXTENSION));
-          files.add(IndexFileNames.segmentFileName(info.getDocStoreSegment(), "", VECTORS_FIELDS_EXTENSION));
-          files.add(IndexFileNames.segmentFileName(info.getDocStoreSegment(), "", VECTORS_DOCUMENTS_EXTENSION));
-        }
-      } else if (!info.getUseCompoundFile()) {
-        files.add(IndexFileNames.segmentFileName(info.name, "", VECTORS_INDEX_EXTENSION));
-        files.add(IndexFileNames.segmentFileName(info.name, "", VECTORS_FIELDS_EXTENSION));
-        files.add(IndexFileNames.segmentFileName(info.name, "", VECTORS_DOCUMENTS_EXTENSION));
-      }
-    }
-  }
-  
   // If this returns, we do the surrogates shuffle so that the
   // terms are sorted by unicode sort order.  This should be
   // true when segments are used for "normal" searching;


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/lucene3x/SegmentTermDocs.java lucene4055/lucene/core/src/java/org/apache/lucene/codecs/lucene3x/SegmentTermDocs.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/lucene3x/SegmentTermDocs.java	2012-05-24 16:55:47.996233427 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/codecs/lucene3x/SegmentTermDocs.java	2012-05-21 13:58:03.111533880 -0400
@@ -89,8 +89,8 @@
   void seek(TermInfo ti, Term term) throws IOException {
     count = 0;
     FieldInfo fi = fieldInfos.fieldInfo(term.field());
-    this.indexOptions = (fi != null) ? fi.indexOptions : IndexOptions.DOCS_AND_FREQS_AND_POSITIONS;
-    currentFieldStoresPayloads = (fi != null) ? fi.storePayloads : false;
+    this.indexOptions = (fi != null) ? fi.getIndexOptions() : IndexOptions.DOCS_AND_FREQS_AND_POSITIONS;
+    currentFieldStoresPayloads = (fi != null) ? fi.hasPayloads() : false;
     if (ti == null) {
       df = 0;
     } else {


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/lucene3x/TermBuffer.java lucene4055/lucene/core/src/java/org/apache/lucene/codecs/lucene3x/TermBuffer.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/lucene3x/TermBuffer.java	2012-05-24 16:55:47.996233427 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/codecs/lucene3x/TermBuffer.java	2012-05-22 17:03:51.989232640 -0400
@@ -67,9 +67,15 @@
     final int fieldNumber = input.readVInt();
     if (fieldNumber != currentFieldNumber) {
       currentFieldNumber = fieldNumber;
-      field = fieldInfos.fieldName(currentFieldNumber).intern();
+      // NOTE: too much sneakiness here, seriously this is a negative vint?!
+      if (currentFieldNumber == -1) {
+        field = "";
+      } else {
+        assert fieldInfos.fieldInfo(currentFieldNumber) != null : currentFieldNumber;
+        field = fieldInfos.fieldInfo(currentFieldNumber).name.intern();
+      }
     } else {
-      assert field.equals(fieldInfos.fieldName(fieldNumber)): "currentFieldNumber=" + currentFieldNumber + " field=" + field + " vs " + fieldInfos.fieldName(fieldNumber);
+      assert field.equals(fieldInfos.fieldInfo(fieldNumber).name) : "currentFieldNumber=" + currentFieldNumber + " field=" + field + " vs " + fieldInfos.fieldInfo(fieldNumber) == null ? "null" : fieldInfos.fieldInfo(fieldNumber).name;
     }
   }
 


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40Codec.java lucene4055/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40Codec.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40Codec.java	2012-05-24 16:55:47.880233427 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40Codec.java	2012-05-22 17:13:19.757242527 -0400
@@ -23,7 +23,7 @@
 import org.apache.lucene.codecs.LiveDocsFormat;
 import org.apache.lucene.codecs.NormsFormat;
 import org.apache.lucene.codecs.PostingsFormat;
-import org.apache.lucene.codecs.SegmentInfosFormat;
+import org.apache.lucene.codecs.SegmentInfoFormat;
 import org.apache.lucene.codecs.StoredFieldsFormat;
 import org.apache.lucene.codecs.TermVectorsFormat;
 import org.apache.lucene.codecs.perfield.PerFieldPostingsFormat;
@@ -42,7 +42,7 @@
   private final TermVectorsFormat vectorsFormat = new Lucene40TermVectorsFormat();
   private final FieldInfosFormat fieldInfosFormat = new Lucene40FieldInfosFormat();
   private final DocValuesFormat docValuesFormat = new Lucene40DocValuesFormat();
-  private final SegmentInfosFormat infosFormat = new Lucene40SegmentInfosFormat();
+  private final SegmentInfoFormat infosFormat = new Lucene40SegmentInfoFormat();
   private final NormsFormat normsFormat = new Lucene40NormsFormat();
   private final LiveDocsFormat liveDocsFormat = new Lucene40LiveDocsFormat();
   
@@ -83,7 +83,7 @@
   }
   
   @Override
-  public SegmentInfosFormat segmentInfosFormat() {
+  public SegmentInfoFormat segmentInfoFormat() {
     return infosFormat;
   }
 


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40DocValuesConsumer.java lucene4055/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40DocValuesConsumer.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40DocValuesConsumer.java	2012-05-24 16:55:47.880233427 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40DocValuesConsumer.java	2012-05-24 11:58:12.911922495 -0400
@@ -18,14 +18,10 @@
  */
 
 import java.io.IOException;
-import java.util.Set;
 
 import org.apache.lucene.codecs.lucene40.values.DocValuesWriterBase;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.FieldInfos;
 import org.apache.lucene.index.IndexFileNames;
 import org.apache.lucene.index.PerDocWriteState;
-import org.apache.lucene.index.SegmentInfo;
 import org.apache.lucene.store.CompoundFileDirectory;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.util.IOUtils;
@@ -68,24 +64,12 @@
     }
   }
 
-  public static void files(SegmentInfo segmentInfo, Set<String> files) throws IOException {
-    FieldInfos fieldInfos = segmentInfo.getFieldInfos();
-    for (FieldInfo fieldInfo : fieldInfos) {
-      if (fieldInfo.hasDocValues()) {
-        files.add(IndexFileNames.segmentFileName(segmentInfo.name, DOC_VALUES_SEGMENT_SUFFIX, IndexFileNames.COMPOUND_FILE_EXTENSION));
-        files.add(IndexFileNames.segmentFileName(segmentInfo.name, DOC_VALUES_SEGMENT_SUFFIX, IndexFileNames.COMPOUND_FILE_ENTRIES_EXTENSION));
-        assert segmentInfo.dir.fileExists(IndexFileNames.segmentFileName(segmentInfo.name, DOC_VALUES_SEGMENT_SUFFIX, IndexFileNames.COMPOUND_FILE_ENTRIES_EXTENSION)); 
-        assert segmentInfo.dir.fileExists(IndexFileNames.segmentFileName(segmentInfo.name, DOC_VALUES_SEGMENT_SUFFIX, IndexFileNames.COMPOUND_FILE_EXTENSION)); 
-        break;
-      }
-    }
-  }
-
   @Override
   public void abort() {
     try {
       close();
-    } catch (IOException ignored) {}
+    } catch (IOException ignored) {
+    }
     IOUtils.deleteFilesIgnoringExceptions(mainDirectory, IndexFileNames.segmentFileName(
         segmentName, segmentSuffix, IndexFileNames.COMPOUND_FILE_EXTENSION),
         IndexFileNames.segmentFileName(segmentName, segmentSuffix,


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40DocValuesFormat.java lucene4055/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40DocValuesFormat.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40DocValuesFormat.java	2012-05-24 16:55:47.892233424 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40DocValuesFormat.java	2012-05-24 11:58:23.295922673 -0400
@@ -18,7 +18,6 @@
  */
 
 import java.io.IOException;
-import java.util.Set;
 
 import org.apache.lucene.codecs.DocValuesFormat;
 import org.apache.lucene.codecs.PerDocConsumer;
@@ -26,7 +25,6 @@
 import org.apache.lucene.index.DocValues; // javadocs
 import org.apache.lucene.index.DocValues.Type; // javadocs
 import org.apache.lucene.index.PerDocWriteState;
-import org.apache.lucene.index.SegmentInfo;
 import org.apache.lucene.index.SegmentReadState;
 import org.apache.lucene.store.CompoundFileDirectory; // javadocs
 import org.apache.lucene.store.DataOutput; // javadocs
@@ -141,9 +139,4 @@
   public PerDocProducer docsProducer(SegmentReadState state) throws IOException {
     return new Lucene40DocValuesProducer(state, Lucene40DocValuesConsumer.DOC_VALUES_SEGMENT_SUFFIX);
   }
-
-  @Override
-  public void files(SegmentInfo info, Set<String> files) throws IOException {
-    Lucene40DocValuesConsumer.files(info, files);
-  }
 }


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40DocValuesProducer.java lucene4055/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40DocValuesProducer.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40DocValuesProducer.java	2012-05-24 16:55:47.892233424 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40DocValuesProducer.java	2012-05-23 14:42:53.850589955 -0400
@@ -56,7 +56,7 @@
                                       IndexFileNames.segmentFileName(state.segmentInfo.name,
                                                                      segmentSuffix, IndexFileNames.COMPOUND_FILE_EXTENSION), 
                                       state.context, false);
-      docValues = load(state.fieldInfos, state.segmentInfo.name, state.segmentInfo.docCount, cfs, state.context);
+      docValues = load(state.fieldInfos, state.segmentInfo.name, state.segmentInfo.getDocCount(), cfs, state.context);
     } else {
       cfs = null;
       docValues = new TreeMap<String,DocValues>();


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40FieldInfosFormat.java lucene4055/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40FieldInfosFormat.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40FieldInfosFormat.java	2012-05-24 16:55:47.892233424 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40FieldInfosFormat.java	2012-05-24 08:17:03.215691408 -0400
@@ -18,33 +18,33 @@
  */
 
 import java.io.IOException;
-import java.util.Set;
 
 import org.apache.lucene.codecs.FieldInfosFormat;
 import org.apache.lucene.codecs.FieldInfosReader;
 import org.apache.lucene.codecs.FieldInfosWriter;
 import org.apache.lucene.index.DocValues; // javadoc
 import org.apache.lucene.index.DocValues.Type; // javadoc
-import org.apache.lucene.index.SegmentInfo;
 import org.apache.lucene.store.DataOutput; // javadoc
+import org.apache.lucene.util.CodecUtil; // javadoc
 
 /**
  * Lucene 4.0 Field Infos format.
  * <p>
  * <p>Field names are stored in the field info file, with suffix <tt>.fnm</tt>.</p>
- * <p>FieldInfos (.fnm) --&gt; FNMVersion,FieldsCount, &lt;FieldName,FieldNumber,
- * FieldBits,DocValuesBits&gt; <sup>FieldsCount</sup></p>
+ * <p>FieldInfos (.fnm) --&gt; Header,FieldsCount, &lt;FieldName,FieldNumber,
+ * FieldBits,DocValuesBits,Attributes&gt; <sup>FieldsCount</sup></p>
  * <p>Data types:
  * <ul>
- *   <li>FNMVersion, FieldsCount --&gt; {@link DataOutput#writeVInt VInt}</li>
+ *   <li>Header --&gt; {@link CodecUtil#checkHeader CodecHeader}</li>
+ *   <li>FieldsCount --&gt; {@link DataOutput#writeVInt VInt}</li>
  *   <li>FieldName --&gt; {@link DataOutput#writeString String}</li>
  *   <li>FieldBits, DocValuesBits --&gt; {@link DataOutput#writeByte Byte}</li>
- *   <li>FieldNumber --&gt; {@link DataOutput#writeInt Uint32}</li>
+ *   <li>FieldNumber --&gt; {@link DataOutput#writeInt VInt}</li>
+ *   <li>Attributes --&gt; {@link DataOutput#writeStringStringMap Map&lt;String,String&gt;}</li>
  * </ul>
  * </p>
  * Field Descriptions:
  * <ul>
- *   <li>FNMVersion is <code>Lucene40FieldInfosWriter.FORMAT_CURRENT</code>.</li>
  *   <li>FieldsCount: the number of fields in this file.</li>
  *   <li>FieldName: name of the field as a UTF-8 String.</li>
  *   <li>FieldNumber: the field's number. Note that unlike previous versions of
@@ -90,6 +90,7 @@
  *          <li>13: variable-length sorted byte array values. ({@link Type#BYTES_VAR_SORTED BYTES_VAR_SORTED})</li>
  *        </ul>
  *    </li>
+ *    <li>Attributes: a key-value map of codec-private attributes.</li>
  * </ul>
  *
  * @lucene.experimental
@@ -107,9 +108,4 @@
   public FieldInfosWriter getFieldInfosWriter() throws IOException {
     return writer;
   }
-
-  @Override
-  public void files(SegmentInfo info, Set<String> files) throws IOException {
-    Lucene40FieldInfosReader.files(info, files);
-  }
 }


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40FieldInfosReader.java lucene4055/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40FieldInfosReader.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40FieldInfosReader.java	2012-05-24 16:55:47.880233427 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40FieldInfosReader.java	2012-05-24 11:58:35.127922875 -0400
@@ -1,21 +1,20 @@
 package org.apache.lucene.codecs.lucene40;
 
 import java.io.IOException;
-import java.util.Set;
+import java.util.Collections;
+import java.util.Map;
 
 import org.apache.lucene.codecs.FieldInfosReader;
 import org.apache.lucene.index.CorruptIndexException;
 import org.apache.lucene.index.FieldInfo;
 import org.apache.lucene.index.FieldInfos;
 import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.IndexFormatTooNewException;
-import org.apache.lucene.index.IndexFormatTooOldException;
-import org.apache.lucene.index.SegmentInfo;
 import org.apache.lucene.index.FieldInfo.IndexOptions;
 import org.apache.lucene.index.DocValues;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.IOContext;
 import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.util.CodecUtil;
 
 /**
  * Licensed to the Apache Software Foundation (ASF) under one or more
@@ -42,33 +41,22 @@
  */
 public class Lucene40FieldInfosReader extends FieldInfosReader {
 
-  static final int FORMAT_MINIMUM = Lucene40FieldInfosWriter.FORMAT_START;
-
   @Override
   public FieldInfos read(Directory directory, String segmentName, IOContext iocontext) throws IOException {
     final String fileName = IndexFileNames.segmentFileName(segmentName, "", Lucene40FieldInfosWriter.FIELD_INFOS_EXTENSION);
     IndexInput input = directory.openInput(fileName, iocontext);
-
-    boolean hasVectors = false;
-    boolean hasFreq = false;
-    boolean hasProx = false;
     
     try {
-      final int format = input.readVInt();
-
-      if (format > FORMAT_MINIMUM) {
-        throw new IndexFormatTooOldException(input, format, FORMAT_MINIMUM, Lucene40FieldInfosWriter.FORMAT_CURRENT);
-      }
-      if (format < Lucene40FieldInfosWriter.FORMAT_CURRENT) {
-        throw new IndexFormatTooNewException(input, format, FORMAT_MINIMUM, Lucene40FieldInfosWriter.FORMAT_CURRENT);
-      }
+      CodecUtil.checkHeader(input, Lucene40FieldInfosWriter.CODEC_NAME, 
+                                   Lucene40FieldInfosWriter.FORMAT_START, 
+                                   Lucene40FieldInfosWriter.FORMAT_CURRENT);
 
       final int size = input.readVInt(); //read in the size
       FieldInfo infos[] = new FieldInfo[size];
 
       for (int i = 0; i < size; i++) {
         String name = input.readString();
-        final int fieldNumber = input.readInt();
+        final int fieldNumber = input.readVInt();
         byte bits = input.readByte();
         boolean isIndexed = (bits & Lucene40FieldInfosWriter.IS_INDEXED) != 0;
         boolean storeTermVector = (bits & Lucene40FieldInfosWriter.STORE_TERMVECTOR) != 0;
@@ -91,22 +79,20 @@
         if (indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) < 0) {
           storePayloads = false;
         }
-        hasVectors |= storeTermVector;
-        hasProx |= isIndexed && indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;
-        hasFreq |= isIndexed && indexOptions != IndexOptions.DOCS_ONLY;
         // DV Types are packed in one byte
         byte val = input.readByte();
         final DocValues.Type docValuesType = getDocValuesType((byte) (val & 0x0F));
         final DocValues.Type normsType = getDocValuesType((byte) ((val >>> 4) & 0x0F));
+        final Map<String,String> attributes = input.readStringStringMap();
         infos[i] = new FieldInfo(name, isIndexed, fieldNumber, storeTermVector, 
-          omitNorms, storePayloads, indexOptions, docValuesType, normsType);
+          omitNorms, storePayloads, indexOptions, docValuesType, normsType, Collections.unmodifiableMap(attributes));
       }
 
       if (input.getFilePointer() != input.length()) {
         throw new CorruptIndexException("did not read all bytes from file \"" + fileName + "\": read " + input.getFilePointer() + " vs size " + input.length() + " (resource: " + input + ")");
       }
       
-      return new FieldInfos(infos, hasFreq, hasProx, hasVectors);
+      return new FieldInfos(infos);
     } finally {
       input.close();
     }
@@ -147,8 +133,4 @@
         throw new IllegalStateException("unhandled indexValues type " + b);
     }
   }
-  
-  public static void files(SegmentInfo info, Set<String> files) throws IOException {
-    files.add(IndexFileNames.segmentFileName(info.name, "", Lucene40FieldInfosWriter.FIELD_INFOS_EXTENSION));
-  }
 }


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40FieldInfosWriter.java lucene4055/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40FieldInfosWriter.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40FieldInfosWriter.java	2012-05-24 16:55:47.880233427 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40FieldInfosWriter.java	2012-05-24 08:14:25.423688659 -0400
@@ -27,6 +27,7 @@
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.IOContext;
 import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.util.CodecUtil;
 
 /**
  * Lucene 4.0 FieldInfos writer.
@@ -39,10 +40,8 @@
   /** Extension of field infos */
   static final String FIELD_INFOS_EXTENSION = "fnm";
   
-  // per-field codec support, records index values for fields
-  static final int FORMAT_START = -4;
-
-  // whenever you add a new format, make it 1 smaller (negative version logic)!
+  static final String CODEC_NAME = "Lucene40FieldInfos";
+  static final int FORMAT_START = 0;
   static final int FORMAT_CURRENT = FORMAT_START;
   
   static final byte IS_INDEXED = 0x1;
@@ -58,24 +57,25 @@
     final String fileName = IndexFileNames.segmentFileName(segmentName, "", FIELD_INFOS_EXTENSION);
     IndexOutput output = directory.createOutput(fileName, context);
     try {
-      output.writeVInt(FORMAT_CURRENT);
+      CodecUtil.writeHeader(output, CODEC_NAME, FORMAT_CURRENT);
       output.writeVInt(infos.size());
       for (FieldInfo fi : infos) {
-        assert fi.indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 || !fi.storePayloads;
+        IndexOptions indexOptions = fi.getIndexOptions();
+        assert indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 || !fi.hasPayloads();
         byte bits = 0x0;
-        if (fi.isIndexed) bits |= IS_INDEXED;
-        if (fi.storeTermVector) bits |= STORE_TERMVECTOR;
-        if (fi.omitNorms) bits |= OMIT_NORMS;
-        if (fi.storePayloads) bits |= STORE_PAYLOADS;
-        if (fi.indexOptions == IndexOptions.DOCS_ONLY) {
+        if (fi.isIndexed()) bits |= IS_INDEXED;
+        if (fi.hasVectors()) bits |= STORE_TERMVECTOR;
+        if (fi.omitsNorms()) bits |= OMIT_NORMS;
+        if (fi.hasPayloads()) bits |= STORE_PAYLOADS;
+        if (indexOptions == IndexOptions.DOCS_ONLY) {
           bits |= OMIT_TERM_FREQ_AND_POSITIONS;
-        } else if (fi.indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) {
+        } else if (indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) {
           bits |= STORE_OFFSETS_IN_POSTINGS;
-        } else if (fi.indexOptions == IndexOptions.DOCS_AND_FREQS) {
+        } else if (indexOptions == IndexOptions.DOCS_AND_FREQS) {
           bits |= OMIT_POSITIONS;
         }
         output.writeString(fi.name);
-        output.writeInt(fi.number);
+        output.writeVInt(fi.number);
         output.writeByte(bits);
 
         // pack the DV types in one byte
@@ -84,6 +84,7 @@
         assert (dv & (~0xF)) == 0 && (nrm & (~0x0F)) == 0;
         byte val = (byte) (0xff & ((nrm << 4) | dv));
         output.writeByte(val);
+        output.writeStringStringMap(fi.attributes());
       }
     } finally {
       output.close();


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40LiveDocsFormat.java lucene4055/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40LiveDocsFormat.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40LiveDocsFormat.java	2012-05-24 16:55:47.892233424 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40LiveDocsFormat.java	2012-05-24 11:58:55.015923223 -0400
@@ -18,11 +18,11 @@
  */
 
 import java.io.IOException;
-import java.util.Set;
+import java.util.Collection;
 
 import org.apache.lucene.codecs.LiveDocsFormat;
 import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.index.SegmentInfoPerCommit;
 import org.apache.lucene.store.DataOutput; // javadocs
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.IOContext;
@@ -81,27 +81,28 @@
   }
 
   @Override
-  public Bits readLiveDocs(Directory dir, SegmentInfo info, IOContext context) throws IOException {
-    String filename = IndexFileNames.fileNameFromGeneration(info.name, DELETES_EXTENSION, info.getDelGen());
+  public Bits readLiveDocs(Directory dir, SegmentInfoPerCommit info, IOContext context) throws IOException {
+    String filename = IndexFileNames.fileNameFromGeneration(info.info.name, DELETES_EXTENSION, info.getDelGen());
     final BitVector liveDocs = new BitVector(dir, filename, context);
-    assert liveDocs.count() == info.docCount - info.getDelCount();
-    assert liveDocs.length() == info.docCount;
+    assert liveDocs.count() == info.info.getDocCount() - info.getDelCount():
+      "liveDocs.count()=" + liveDocs.count() + " info.docCount=" + info.info.getDocCount() + " info.getDelCount()=" + info.getDelCount();
+    assert liveDocs.length() == info.info.getDocCount();
     return liveDocs;
   }
 
   @Override
-  public void writeLiveDocs(MutableBits bits, Directory dir, SegmentInfo info, IOContext context) throws IOException {
-    String filename = IndexFileNames.fileNameFromGeneration(info.name, DELETES_EXTENSION, info.getDelGen());
+  public void writeLiveDocs(MutableBits bits, Directory dir, SegmentInfoPerCommit info, int newDelCount, IOContext context) throws IOException {
+    String filename = IndexFileNames.fileNameFromGeneration(info.info.name, DELETES_EXTENSION, info.getNextDelGen());
     final BitVector liveDocs = (BitVector) bits;
-    assert liveDocs.count() == info.docCount - info.getDelCount();
-    assert liveDocs.length() == info.docCount;
+    assert liveDocs.count() == info.info.getDocCount() - info.getDelCount() - newDelCount;
+    assert liveDocs.length() == info.info.getDocCount();
     liveDocs.write(dir, filename, context);
   }
 
   @Override
-  public void files(SegmentInfo info, Set<String> files) throws IOException {
+  public void files(SegmentInfoPerCommit info, Collection<String> files) throws IOException {
     if (info.hasDeletions()) {
-      files.add(IndexFileNames.fileNameFromGeneration(info.name, DELETES_EXTENSION, info.getDelGen()));
+      files.add(IndexFileNames.fileNameFromGeneration(info.info.name, DELETES_EXTENSION, info.getDelGen()));
     }
   }
 }


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40NormsFormat.java lucene4055/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40NormsFormat.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40NormsFormat.java	2012-05-24 16:55:47.880233427 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40NormsFormat.java	2012-05-24 11:59:09.047923469 -0400
@@ -1,4 +1,5 @@
 package org.apache.lucene.codecs.lucene40;
+
 /**
  * Licensed to the Apache Software Foundation (ASF) under one or more
  * contributor license agreements.  See the NOTICE file distributed with
@@ -15,8 +16,8 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
+
 import java.io.IOException;
-import java.util.Set;
 
 import org.apache.lucene.codecs.NormsFormat;
 import org.apache.lucene.codecs.PerDocConsumer;
@@ -25,10 +26,8 @@
 import org.apache.lucene.index.DocValues.Type;
 import org.apache.lucene.index.FieldInfo;
 import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.IndexFileNames;
 import org.apache.lucene.index.AtomicReader;
 import org.apache.lucene.index.PerDocWriteState;
-import org.apache.lucene.index.SegmentInfo;
 import org.apache.lucene.index.SegmentReadState;
 import org.apache.lucene.store.CompoundFileDirectory; // javadocs
 
@@ -59,11 +58,6 @@
     return new Lucene40NormsDocValuesProducer(state, NORMS_SEGMENT_SUFFIX);
   }
 
-  @Override
-  public void files(SegmentInfo info, Set<String> files) throws IOException {
-    Lucene40NormsDocValuesConsumer.files(info, files);
-  }
- 
   /**
    * Lucene 4.0 PerDocProducer implementation that uses compound file.
    * 
@@ -121,19 +115,5 @@
     protected Type getDocValuesType(FieldInfo info) {
       return info.getNormType();
     }
-    
-    public static void files(SegmentInfo segmentInfo, Set<String> files) throws IOException {
-      final String normsFileName = IndexFileNames.segmentFileName(segmentInfo.name, NORMS_SEGMENT_SUFFIX, IndexFileNames.COMPOUND_FILE_EXTENSION);
-      FieldInfos fieldInfos = segmentInfo.getFieldInfos();
-      for (FieldInfo fieldInfo : fieldInfos) {
-        if (fieldInfo.hasNorms()) {
-          final String normsEntriesFileName = IndexFileNames.segmentFileName(segmentInfo.name, NORMS_SEGMENT_SUFFIX, IndexFileNames.COMPOUND_FILE_ENTRIES_EXTENSION);
-          files.add(normsFileName);
-          files.add(normsEntriesFileName);
-          return;
-        }
-      }
-    }
   }
-
 }


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40PostingsBaseFormat.java lucene4055/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40PostingsBaseFormat.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40PostingsBaseFormat.java	2012-05-24 16:55:47.880233427 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40PostingsBaseFormat.java	2012-05-24 11:59:26.927923778 -0400
@@ -18,12 +18,10 @@
  */
 
 import java.io.IOException;
-import java.util.Set;
 
 import org.apache.lucene.codecs.PostingsBaseFormat;
 import org.apache.lucene.codecs.PostingsReaderBase;
 import org.apache.lucene.codecs.PostingsWriterBase;
-import org.apache.lucene.index.SegmentInfo;
 import org.apache.lucene.index.SegmentReadState;
 import org.apache.lucene.index.SegmentWriteState;
 
@@ -42,16 +40,11 @@
 
   @Override
   public PostingsReaderBase postingsReaderBase(SegmentReadState state) throws IOException {
-    return new Lucene40PostingsReader(state.dir, state.segmentInfo, state.context, state.segmentSuffix);
+    return new Lucene40PostingsReader(state.dir, state.fieldInfos, state.segmentInfo, state.context, state.segmentSuffix);
   }
 
   @Override
   public PostingsWriterBase postingsWriterBase(SegmentWriteState state) throws IOException {
     return new Lucene40PostingsWriter(state);
   }
-  
-  @Override
-  public void files(SegmentInfo segmentInfo, String segmentSuffix, Set<String> files) throws IOException {
-    Lucene40PostingsReader.files(segmentInfo, segmentSuffix, files);
-  }
 }


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40PostingsFormat.java lucene4055/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40PostingsFormat.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40PostingsFormat.java	2012-05-24 16:55:47.880233427 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40PostingsFormat.java	2012-05-24 11:59:38.879923992 -0400
@@ -18,7 +18,6 @@
  */
 
 import java.io.IOException;
-import java.util.Set;
 
 import org.apache.lucene.codecs.BlockTreeTermsReader;
 import org.apache.lucene.codecs.BlockTreeTermsWriter;
@@ -30,7 +29,6 @@
 import org.apache.lucene.index.DocsEnum; // javadocs
 import org.apache.lucene.index.FieldInfo.IndexOptions; // javadocs
 import org.apache.lucene.index.FieldInfos; // javadocs
-import org.apache.lucene.index.SegmentInfo;
 import org.apache.lucene.index.SegmentReadState;
 import org.apache.lucene.index.SegmentWriteState;
 import org.apache.lucene.store.DataOutput; // javadocs
@@ -307,7 +305,7 @@
 
   @Override
   public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {
-    PostingsReaderBase postings = new Lucene40PostingsReader(state.dir, state.segmentInfo, state.context, state.segmentSuffix);
+    PostingsReaderBase postings = new Lucene40PostingsReader(state.dir, state.fieldInfos, state.segmentInfo, state.context, state.segmentSuffix);
 
     boolean success = false;
     try {
@@ -335,12 +333,6 @@
   static final String PROX_EXTENSION = "prx";
 
   @Override
-  public void files(SegmentInfo segmentInfo, String segmentSuffix, Set<String> files) throws IOException {
-    Lucene40PostingsReader.files(segmentInfo, segmentSuffix, files);
-    BlockTreeTermsReader.files(segmentInfo, segmentSuffix, files);
-  }
-
-  @Override
   public String toString() {
     return getName() + "(minBlockSize=" + minBlockSize + " maxBlockSize=" + maxBlockSize + ")";
   }


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40PostingsReader.java lucene4055/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40PostingsReader.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40PostingsReader.java	2012-05-24 16:55:47.892233424 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40PostingsReader.java	2012-05-24 11:59:44.667924087 -0400
@@ -18,14 +18,14 @@
  */
 
 import java.io.IOException;
-import java.util.Collection;
 
 import org.apache.lucene.codecs.BlockTermState;
 import org.apache.lucene.codecs.PostingsReaderBase;
 import org.apache.lucene.index.DocsAndPositionsEnum;
 import org.apache.lucene.index.DocsEnum;
-import org.apache.lucene.index.FieldInfo;
 import org.apache.lucene.index.FieldInfo.IndexOptions;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.FieldInfos;
 import org.apache.lucene.index.IndexFileNames;
 import org.apache.lucene.index.SegmentInfo;
 import org.apache.lucene.index.TermState;
@@ -57,11 +57,18 @@
 
   // private String segment;
 
-  public Lucene40PostingsReader(Directory dir, SegmentInfo segmentInfo, IOContext ioContext, String segmentSuffix) throws IOException {
+  public Lucene40PostingsReader(Directory dir, FieldInfos fieldInfos, SegmentInfo segmentInfo, IOContext ioContext, String segmentSuffix) throws IOException {
     freqIn = dir.openInput(IndexFileNames.segmentFileName(segmentInfo.name, segmentSuffix, Lucene40PostingsFormat.FREQ_EXTENSION),
                            ioContext);
-    // this.segment = segmentInfo.name;
-    if (segmentInfo.getHasProx()) {
+    // TODO: hasProx should (somehow!) become codec private,
+    // but it's tricky because 1) FIS.hasProx is global (it
+    // could be all fields that have prox are written by a
+    // different codec), 2) the field may have had prox in
+    // the past but all docs w/ that field were deleted.
+    // Really we'd need to init prxOut lazily on write, and
+    // then somewhere record that we actually wrote it so we
+    // know whether to open on read:
+    if (fieldInfos.hasProx()) {
       boolean success = false;
       try {
         proxIn = dir.openInput(IndexFileNames.segmentFileName(segmentInfo.name, segmentSuffix, Lucene40PostingsFormat.PROX_EXTENSION),
@@ -77,13 +84,6 @@
     }
   }
 
-  public static void files(SegmentInfo segmentInfo, String segmentSuffix, Collection<String> files) throws IOException {
-    files.add(IndexFileNames.segmentFileName(segmentInfo.name, segmentSuffix, Lucene40PostingsFormat.FREQ_EXTENSION));
-    if (segmentInfo.getHasProx()) {
-      files.add(IndexFileNames.segmentFileName(segmentInfo.name, segmentSuffix, Lucene40PostingsFormat.PROX_EXTENSION));
-    }
-  }
-
   @Override
   public void init(IndexInput termsIn) throws IOException {
 
@@ -200,7 +200,7 @@
       // undefined
     }
 
-    if (fieldInfo.indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0) {
+    if (fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0) {
       if (isFirstTerm) {
         termState.proxOffset = termState.bytesReader.readVLong();
       } else {
@@ -212,7 +212,7 @@
     
   @Override
   public DocsEnum docs(FieldInfo fieldInfo, BlockTermState termState, Bits liveDocs, DocsEnum reuse, boolean needsFreqs) throws IOException {
-    if (needsFreqs && fieldInfo.indexOptions == IndexOptions.DOCS_ONLY) {
+    if (needsFreqs && fieldInfo.getIndexOptions() == IndexOptions.DOCS_ONLY) {
       return null;
     } else if (canReuse(reuse, liveDocs)) {
       // if (DEBUG) System.out.println("SPR.docs ts=" + termState);
@@ -248,13 +248,13 @@
                                                DocsAndPositionsEnum reuse, boolean needsOffsets)
     throws IOException {
 
-    boolean hasOffsets = fieldInfo.indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;
+    boolean hasOffsets = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;
     if (needsOffsets && !hasOffsets) {
       return null; // not available
     }
 
     // TODO: refactor
-    if (fieldInfo.storePayloads || hasOffsets) {
+    if (fieldInfo.hasPayloads() || hasOffsets) {
       SegmentFullPositionsEnum docsEnum;
       if (reuse == null || !(reuse instanceof SegmentFullPositionsEnum)) {
         docsEnum = new SegmentFullPositionsEnum(freqIn, proxIn);
@@ -326,9 +326,9 @@
     
     
     DocsEnum reset(FieldInfo fieldInfo, StandardTermState termState) throws IOException {
-      indexOmitsTF = fieldInfo.indexOptions == IndexOptions.DOCS_ONLY;
-      storePayloads = fieldInfo.storePayloads;
-      storeOffsets = fieldInfo.indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;
+      indexOmitsTF = fieldInfo.getIndexOptions() == IndexOptions.DOCS_ONLY;
+      storePayloads = fieldInfo.hasPayloads();
+      storeOffsets = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;
       freqOffset = termState.freqOffset;
       skipOffset = termState.skipOffset;
 
@@ -701,8 +701,8 @@
     }
 
     public SegmentDocsAndPositionsEnum reset(FieldInfo fieldInfo, StandardTermState termState, Bits liveDocs) throws IOException {
-      assert fieldInfo.indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS;
-      assert !fieldInfo.storePayloads;
+      assert fieldInfo.getIndexOptions() == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS;
+      assert !fieldInfo.hasPayloads();
 
       this.liveDocs = liveDocs;
 
@@ -914,9 +914,9 @@
     }
 
     public SegmentFullPositionsEnum reset(FieldInfo fieldInfo, StandardTermState termState, Bits liveDocs) throws IOException {
-      storeOffsets = fieldInfo.indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;
-      storePayloads = fieldInfo.storePayloads;
-      assert fieldInfo.indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;
+      storeOffsets = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;
+      storePayloads = fieldInfo.hasPayloads();
+      assert fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;
       assert storePayloads || storeOffsets;
       if (payload == null) {
         payload = new BytesRef();


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40PostingsWriter.java lucene4055/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40PostingsWriter.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40PostingsWriter.java	2012-05-24 16:55:47.880233427 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40PostingsWriter.java	2012-05-23 15:14:02.054622488 -0400
@@ -99,14 +99,18 @@
     this.skipInterval = skipInterval;
     this.skipMinimum = skipInterval; /* set to the same for now */
     // this.segment = state.segmentName;
-    String fileName = IndexFileNames.segmentFileName(state.segmentName, state.segmentSuffix, Lucene40PostingsFormat.FREQ_EXTENSION);
+    String fileName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, Lucene40PostingsFormat.FREQ_EXTENSION);
     freqOut = state.directory.createOutput(fileName, state.context);
     boolean success = false;
     try {
+      // TODO: this is a best effort, if one of these fields has no postings
+      // then we make an empty prx file, same as if we are wrapped in 
+      // per-field postingsformat. maybe... we shouldn't
+      // bother w/ this opto?  just create empty prx file...?
       if (state.fieldInfos.hasProx()) {
         // At least one field does not omit TF, so create the
         // prox file
-        fileName = IndexFileNames.segmentFileName(state.segmentName, state.segmentSuffix, Lucene40PostingsFormat.PROX_EXTENSION);
+        fileName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, Lucene40PostingsFormat.PROX_EXTENSION);
         proxOut = state.directory.createOutput(fileName, state.context);
       } else {
         // Every field omits TF so we will write no prox file
@@ -119,11 +123,11 @@
       }
     }
 
-    totalNumDocs = state.numDocs;
+    totalNumDocs = state.segmentInfo.getDocCount();
 
     skipListWriter = new Lucene40SkipListWriter(skipInterval,
                                                maxSkipLevels,
-                                               state.numDocs,
+                                               totalNumDocs,
                                                freqOut,
                                                proxOut);
   }
@@ -164,10 +168,10 @@
     }
     */
     this.fieldInfo = fieldInfo;
-    indexOptions = fieldInfo.indexOptions;
+    indexOptions = fieldInfo.getIndexOptions();
     
     storeOffsets = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;        
-    storePayloads = fieldInfo.storePayloads;
+    storePayloads = fieldInfo.hasPayloads();
     //System.out.println("  set init blockFreqStart=" + freqStart);
     //System.out.println("  set init blockProxStart=" + proxStart);
   }


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40SegmentInfoFormat.java lucene4055/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40SegmentInfoFormat.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40SegmentInfoFormat.java	1969-12-31 19:00:00.000000000 -0500
+++ lucene4055/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40SegmentInfoFormat.java	2012-05-24 12:12:44.707937674 -0400
@@ -0,0 +1,88 @@
+package org.apache.lucene.codecs.lucene40;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.codecs.SegmentInfoFormat;
+import org.apache.lucene.codecs.SegmentInfoReader;
+import org.apache.lucene.codecs.SegmentInfoWriter;
+import org.apache.lucene.index.IndexWriter; // javadocs
+import org.apache.lucene.index.SegmentInfos; // javadocs
+import org.apache.lucene.store.DataOutput; // javadocs
+import org.apache.lucene.util.CodecUtil; // javadocs
+
+/**
+ * Lucene 4.0 Segment info format.
+ * <p>
+ * Files:
+ * <ul>
+ *   <li><tt>.si</tt>: Header, SegVersion, SegSize, IsCompoundFile, Diagnostics, Attributes, Files
+ * </ul>
+ * </p>
+ * Data types:
+ * <p>
+ * <ul>
+ *   <li>Header --&gt; {@link CodecUtil#writeHeader CodecHeader}</li>
+ *   <li>SegSize --&gt; {@link DataOutput#writeInt Int32}</li>
+ *   <li>SegVersion --&gt; {@link DataOutput#writeString String}</li>
+ *   <li>Files --&gt; {@link DataOutput#writeStringSet Set&lt;String&gt;}</li>
+ *   <li>Diagnostics, Attributes --&gt; {@link DataOutput#writeStringStringMap Map&lt;String,String&gt;}</li>
+ *   <li>IsCompoundFile --&gt; {@link DataOutput#writeByte Int8}</li>
+ * </ul>
+ * </p>
+ * Field Descriptions:
+ * <p>
+ * <ul>
+ *   <li>SegVersion is the code version that created the segment.</li>
+ *   <li>SegSize is the number of documents contained in the segment index.</li>
+ *   <li>IsCompoundFile records whether the segment is written as a compound file or
+ *       not. If this is -1, the segment is not a compound file. If it is 1, the segment
+ *       is a compound file.</li>
+ *   <li>Checksum contains the CRC32 checksum of all bytes in the segments_N file up
+ *       until the checksum. This is used to verify integrity of the file on opening the
+ *       index.</li>
+ *   <li>The Diagnostics Map is privately written by {@link IndexWriter}, as a debugging aid,
+ *       for each segment it creates. It includes metadata like the current Lucene
+ *       version, OS, Java version, why the segment was created (merge, flush,
+ *       addIndexes), etc.</li>
+ *   <li>Attributes: a key-value map of codec-private attributes.</li>
+ *   <li>Files is a list of files referred to by this segment.</li>
+ * </ul>
+ * </p>
+ * 
+ * @see SegmentInfos
+ * @lucene.experimental
+ */
+public class Lucene40SegmentInfoFormat extends SegmentInfoFormat {
+  private final SegmentInfoReader reader = new Lucene40SegmentInfoReader();
+  private final SegmentInfoWriter writer = new Lucene40SegmentInfoWriter();
+  
+  @Override
+  public SegmentInfoReader getSegmentInfosReader() {
+    return reader;
+  }
+
+  @Override
+  public SegmentInfoWriter getSegmentInfosWriter() {
+    return writer;
+  }
+
+  public final static String SI_EXTENSION = "si";
+  static final String CODEC_NAME = "Lucene40SegmentInfo";
+  static final int VERSION_START = 0;
+  static final int VERSION_CURRENT = VERSION_START;
+}


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40SegmentInfoReader.java lucene4055/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40SegmentInfoReader.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40SegmentInfoReader.java	1969-12-31 19:00:00.000000000 -0500
+++ lucene4055/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40SegmentInfoReader.java	2012-05-24 09:58:24.791797315 -0400
@@ -0,0 +1,74 @@
+package org.apache.lucene.codecs.lucene40;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Collections;
+import java.util.Map;
+import java.util.Set;
+
+import org.apache.lucene.codecs.SegmentInfoReader;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.util.CodecUtil;
+import org.apache.lucene.util.IOUtils;
+
+/**
+ * Lucene 4.0 implementation of {@link SegmentInfoReader}.
+ * 
+ * @see Lucene40SegmentInfoFormat
+ * @lucene.experimental
+ */
+public class Lucene40SegmentInfoReader extends SegmentInfoReader {
+
+  @Override
+  public SegmentInfo read(Directory dir, String segment, IOContext context) throws IOException {
+    final String fileName = IndexFileNames.segmentFileName(segment, "", Lucene40SegmentInfoFormat.SI_EXTENSION);
+    final IndexInput input = dir.openInput(fileName, context);
+    boolean success = false;
+    try {
+      CodecUtil.checkHeader(input, Lucene40SegmentInfoFormat.CODEC_NAME,
+                                   Lucene40SegmentInfoFormat.VERSION_START,
+                                   Lucene40SegmentInfoFormat.VERSION_CURRENT);
+      final String version = input.readString();
+      final int docCount = input.readInt();
+      final boolean isCompoundFile = input.readByte() == SegmentInfo.YES;
+      final Map<String,String> diagnostics = input.readStringStringMap();
+      final Map<String,String> attributes = input.readStringStringMap();
+      final Set<String> files = input.readStringSet();
+
+      final SegmentInfo si = new SegmentInfo(dir, version, segment, docCount, isCompoundFile,
+                                             null, diagnostics, Collections.unmodifiableMap(attributes));
+      si.setFiles(files);
+
+      success = true;
+
+      return si;
+
+    } finally {
+      if (!success) {
+        IOUtils.closeWhileHandlingException(input);
+      } else {
+        input.close();
+      }
+    }
+  }
+}


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40SegmentInfosFormat.java lucene4055/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40SegmentInfosFormat.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40SegmentInfosFormat.java	2012-05-24 16:55:47.880233427 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40SegmentInfosFormat.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,132 +0,0 @@
-package org.apache.lucene.codecs.lucene40;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.codecs.Codec; // javadocs
-import org.apache.lucene.codecs.LiveDocsFormat; // javadocs
-import org.apache.lucene.codecs.SegmentInfosFormat;
-import org.apache.lucene.codecs.SegmentInfosReader;
-import org.apache.lucene.codecs.SegmentInfosWriter;
-import org.apache.lucene.codecs.StoredFieldsFormat; // javadocs
-import org.apache.lucene.codecs.TermVectorsFormat; // javadocs
-import org.apache.lucene.index.FieldInfo.IndexOptions; // javadocs
-import org.apache.lucene.index.IndexWriter; // javadocs
-import org.apache.lucene.index.SegmentInfos; // javadocs
-import org.apache.lucene.store.DataOutput; // javadocs
-
-/**
- * Lucene 4.0 Segments format.
- * <p>
- * Files:
- * <ul>
- *   <li><tt>segments.gen</tt>: described in {@link SegmentInfos}
- *   <li><tt>segments_N</tt>: Format, Codec, Version, NameCounter, SegCount,
- *    &lt;SegVersion, SegName, SegSize, DelGen, DocStoreOffset, [DocStoreSegment,
- *    DocStoreIsCompoundFile], NumField, NormGen<sup>NumField</sup>, 
- *    IsCompoundFile, DeletionCount, HasProx, SegCodec Diagnostics, 
- *    HasVectors&gt;<sup>SegCount</sup>, CommitUserData, Checksum
- * </ul>
- * </p>
- * Data types:
- * <p>
- * <ul>
- *   <li>Format, NameCounter, SegCount, SegSize, NumField, DocStoreOffset,
- *       DeletionCount --&gt; {@link DataOutput#writeInt Int32}</li>
- *   <li>Version, DelGen, NormGen, Checksum --&gt; 
- *       {@link DataOutput#writeLong Int64}</li>
- *   <li>SegVersion, SegName, DocStoreSegment, Codec, SegCodec --&gt; 
- *       {@link DataOutput#writeString String}</li>
- *   <li>Diagnostics, CommitUserData --&gt; 
- *       {@link DataOutput#writeStringStringMap Map&lt;String,String&gt;}</li>
- *   <li>IsCompoundFile, DocStoreIsCompoundFile, HasProx,
- *       HasVectors --&gt; {@link DataOutput#writeByte Int8}</li>
- * </ul>
- * </p>
- * Field Descriptions:
- * <p>
- * <ul>
- *   <li>Format is {@link SegmentInfos#FORMAT_4_0}.</li>
- *   <li>Codec is "Lucene40", its the {@link Codec} that wrote this particular segments file.</li>
- *   <li>Version counts how often the index has been changed by adding or deleting
- *       documents.</li>
- *   <li>NameCounter is used to generate names for new segment files.</li>
- *   <li>SegVersion is the code version that created the segment.</li>
- *   <li>SegName is the name of the segment, and is used as the file name prefix for
- *       all of the files that compose the segment's index.</li>
- *   <li>SegSize is the number of documents contained in the segment index.</li>
- *   <li>DelGen is the generation count of the deletes file. If this is -1,
- *       there are no deletes. Anything above zero means there are deletes 
- *       stored by {@link LiveDocsFormat}.</li>
- *   <li>NumField is the size of the array for NormGen, or -1 if there are no
- *       NormGens stored.</li>
- *   <li>NormGen records the generation of the separate norms files. If NumField is
- *       -1, there are no normGens stored and all assumed to be -1. The generation 
- *       then has the same meaning as delGen (above).</li>
- *   <li>IsCompoundFile records whether the segment is written as a compound file or
- *       not. If this is -1, the segment is not a compound file. If it is 1, the segment
- *       is a compound file. Else it is 0, which means we check filesystem to see if
- *       _X.cfs exists.</li>
- *   <li>DocStoreOffset, DocStoreSegment, DocStoreIsCompoundFile: If DocStoreOffset
- *       is -1, this segment has its own doc store (stored fields values and term
- *       vectors) files and DocStoreSegment and DocStoreIsCompoundFile are not stored.
- *       In this case all files for  {@link StoredFieldsFormat stored field values} and
- *       {@link TermVectorsFormat term vectors} will be stored with this segment. 
- *       Otherwise, DocStoreSegment is the name of the segment that has the shared doc 
- *       store files; DocStoreIsCompoundFile is 1 if that segment is stored in compound 
- *       file format (as a <tt>.cfx</tt> file); and DocStoreOffset is the starting document 
- *       in the shared doc store files where this segment's documents begin. In this case, 
- *       this segment does not store its own doc store files but instead shares a single 
- *       set of these files with other segments.</li>
- *   <li>Checksum contains the CRC32 checksum of all bytes in the segments_N file up
- *       until the checksum. This is used to verify integrity of the file on opening the
- *       index.</li>
- *   <li>DeletionCount records the number of deleted documents in this segment.</li>
- *   <li>HasProx is 1 if any fields in this segment have position data
- *       ({@link IndexOptions#DOCS_AND_FREQS_AND_POSITIONS DOCS_AND_FREQS_AND_POSITIONS} or 
- *       {@link IndexOptions#DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS}); 
- *       else, it's 0.</li>
- *   <li>SegCodec is the {@link Codec#getName() name} of the Codec that encoded
- *       this segment.</li>
- *   <li>CommitUserData stores an optional user-supplied opaque
- *       Map&lt;String,String&gt; that was passed to {@link IndexWriter#commit(java.util.Map)} 
- *       or {@link IndexWriter#prepareCommit(java.util.Map)}.</li>
- *   <li>The Diagnostics Map is privately written by IndexWriter, as a debugging aid,
- *       for each segment it creates. It includes metadata like the current Lucene
- *       version, OS, Java version, why the segment was created (merge, flush,
- *       addIndexes), etc.</li>
- *   <li>HasVectors is 1 if this segment stores term vectors, else it's 0.</li>
- * </ul>
- * </p>
- * 
- * @see SegmentInfos
- * @lucene.experimental
- */
-public class Lucene40SegmentInfosFormat extends SegmentInfosFormat {
-  private final SegmentInfosReader reader = new Lucene40SegmentInfosReader();
-  private final SegmentInfosWriter writer = new Lucene40SegmentInfosWriter();
-  
-  @Override
-  public SegmentInfosReader getSegmentInfosReader() {
-    return reader;
-  }
-
-  @Override
-  public SegmentInfosWriter getSegmentInfosWriter() {
-    return writer;
-  }
-}


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40SegmentInfosReader.java lucene4055/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40SegmentInfosReader.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40SegmentInfosReader.java	2012-05-24 16:55:47.892233424 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40SegmentInfosReader.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,94 +0,0 @@
-package org.apache.lucene.codecs.lucene40;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.HashMap;
-import java.util.Map;
-
-import org.apache.lucene.codecs.Codec;
-import org.apache.lucene.codecs.SegmentInfosReader;
-import org.apache.lucene.index.SegmentInfo;
-import org.apache.lucene.index.SegmentInfos;
-import org.apache.lucene.store.ChecksumIndexInput;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-
-/**
- * Lucene 4.0 implementation of {@link SegmentInfosReader}.
- * 
- * @see Lucene40SegmentInfosFormat
- * @lucene.experimental
- */
-public class Lucene40SegmentInfosReader extends SegmentInfosReader {
-
-  @Override
-  public void read(Directory directory, String segmentsFileName, ChecksumIndexInput input, SegmentInfos infos, IOContext context) throws IOException { 
-    infos.version = input.readLong(); // read version
-    infos.counter = input.readInt(); // read counter
-    final int format = infos.getFormat();
-    assert format <= SegmentInfos.FORMAT_4_0;
-    for (int i = input.readInt(); i > 0; i--) { // read segmentInfos
-      SegmentInfo si = readSegmentInfo(directory, format, input);
-      assert si.getVersion() != null;
-      infos.add(si);
-    }
-      
-    infos.userData = input.readStringStringMap();
-  }
-  
-  public SegmentInfo readSegmentInfo(Directory dir, int format, ChecksumIndexInput input) throws IOException {
-    final String version = input.readString();
-    final String name = input.readString();
-    final int docCount = input.readInt();
-    final long delGen = input.readLong();
-    // this is still written in 4.0 if we open a 3.x and upgrade the SI
-    final int docStoreOffset = input.readInt();
-    final String docStoreSegment;
-    final boolean docStoreIsCompoundFile;
-    if (docStoreOffset != -1) { 
-      docStoreSegment = input.readString();
-      docStoreIsCompoundFile = input.readByte() == SegmentInfo.YES;
-    } else {
-      docStoreSegment = name;
-      docStoreIsCompoundFile = false;
-    }
-    final int numNormGen = input.readInt();
-    final Map<Integer,Long> normGen;
-    if (numNormGen == SegmentInfo.NO) {
-      normGen = null;
-    } else {
-      normGen = new HashMap<Integer, Long>();
-      for(int j=0;j<numNormGen;j++) {
-        normGen.put(input.readInt(), input.readLong());
-      }
-    }
-    final boolean isCompoundFile = input.readByte() == SegmentInfo.YES;
-
-    final int delCount = input.readInt();
-    assert delCount <= docCount;
-    final int hasProx = input.readByte();
-    final Codec codec = Codec.forName(input.readString());
-    final Map<String,String> diagnostics = input.readStringStringMap();
-    final int hasVectors = input.readByte();
-    
-    return new SegmentInfo(dir, version, name, docCount, delGen, docStoreOffset,
-      docStoreSegment, docStoreIsCompoundFile, normGen, isCompoundFile,
-      delCount, hasProx, codec, diagnostics, hasVectors);
-  }
-}


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40SegmentInfosWriter.java lucene4055/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40SegmentInfosWriter.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40SegmentInfosWriter.java	2012-05-24 16:55:47.892233424 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40SegmentInfosWriter.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,121 +0,0 @@
-package org.apache.lucene.codecs.lucene40;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Map;
-import java.util.Map.Entry;
-
-import org.apache.lucene.codecs.SegmentInfosWriter;
-import org.apache.lucene.index.SegmentInfo;
-import org.apache.lucene.index.SegmentInfos;
-import org.apache.lucene.store.ChecksumIndexOutput;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.FlushInfo;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.util.IOUtils;
-
-/**
- * Lucene 4.0 implementation of {@link SegmentInfosWriter}.
- * 
- * @see Lucene40SegmentInfosFormat
- * @lucene.experimental
- */
-public class Lucene40SegmentInfosWriter extends SegmentInfosWriter {
-
-  @Override
-  public IndexOutput writeInfos(Directory dir, String segmentFileName, String codecID, SegmentInfos infos, IOContext context)
-          throws IOException {
-    IndexOutput out = createOutput(dir, segmentFileName, new IOContext(new FlushInfo(infos.size(), infos.totalDocCount())));
-    boolean success = false;
-    try {
-      /*
-       * TODO its not ideal that we write the format and the codecID inside the
-       * codec private classes but we read it in SegmentInfos.
-       */
-      out.writeInt(SegmentInfos.FORMAT_CURRENT); // write FORMAT
-      out.writeString(codecID); // write codecID
-      out.writeLong(infos.version);
-      out.writeInt(infos.counter); // write counter
-      out.writeInt(infos.size()); // write infos
-      for (SegmentInfo si : infos) {
-        writeInfo(out, si);
-      }
-      out.writeStringStringMap(infos.getUserData());
-      success = true;
-      return out;
-    } finally {
-      if (!success) {
-        IOUtils.closeWhileHandlingException(out);
-      }
-    }
-  }
-  
-  /** Save a single segment's info. */
-  private void writeInfo(IndexOutput output, SegmentInfo si) throws IOException {
-    assert si.getDelCount() <= si.docCount: "delCount=" + si.getDelCount() + " docCount=" + si.docCount + " segment=" + si.name;
-    // Write the Lucene version that created this segment, since 3.1
-    output.writeString(si.getVersion());
-    output.writeString(si.name);
-    output.writeInt(si.docCount);
-    output.writeLong(si.getDelGen());
-    // we still need to write this in 4.0 since we can open a 3.x with shared docStores
-    output.writeInt(si.getDocStoreOffset());
-    if (si.getDocStoreOffset() != -1) {
-      output.writeString(si.getDocStoreSegment());
-      output.writeByte((byte) (si.getDocStoreIsCompoundFile() ? 1:0));
-    }
-
-    Map<Integer,Long> normGen = si.getNormGen();
-    if (normGen == null) {
-      output.writeInt(SegmentInfo.NO);
-    } else {
-      output.writeInt(normGen.size());
-      for (Entry<Integer,Long> entry : normGen.entrySet()) {
-        output.writeInt(entry.getKey());
-        output.writeLong(entry.getValue());
-      }
-    }
-
-    output.writeByte((byte) (si.getUseCompoundFile() ? SegmentInfo.YES : SegmentInfo.NO));
-    output.writeInt(si.getDelCount());
-    output.writeByte((byte) (si.getHasProxInternal()));
-    output.writeString(si.getCodec().getName());
-    output.writeStringStringMap(si.getDiagnostics());
-    output.writeByte((byte) (si.getHasVectorsInternal()));
-  }
-  
-  protected IndexOutput createOutput(Directory dir, String segmentFileName, IOContext context)
-      throws IOException {
-    IndexOutput plainOut = dir.createOutput(segmentFileName, context);
-    ChecksumIndexOutput out = new ChecksumIndexOutput(plainOut);
-    return out;
-  }
-
-  @Override
-  public void prepareCommit(IndexOutput segmentOutput) throws IOException {
-    ((ChecksumIndexOutput)segmentOutput).prepareCommit();
-  }
-
-  @Override
-  public void finishCommit(IndexOutput out) throws IOException {
-    ((ChecksumIndexOutput)out).finishCommit();
-    out.close();
-  }
-}


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40SegmentInfoWriter.java lucene4055/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40SegmentInfoWriter.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40SegmentInfoWriter.java	1969-12-31 19:00:00.000000000 -0500
+++ lucene4055/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40SegmentInfoWriter.java	2012-05-24 09:58:32.579797451 -0400
@@ -0,0 +1,70 @@
+package org.apache.lucene.codecs.lucene40;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.codecs.SegmentInfoWriter;
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.util.CodecUtil;
+import org.apache.lucene.util.IOUtils;
+
+/**
+ * Lucene 4.0 implementation of {@link SegmentInfoWriter}.
+ * 
+ * @see Lucene40SegmentInfoFormat
+ * @lucene.experimental
+ */
+public class Lucene40SegmentInfoWriter extends SegmentInfoWriter {
+
+  /** Save a single segment's info. */
+  @Override
+  public void write(Directory dir, SegmentInfo si, FieldInfos fis, IOContext ioContext) throws IOException {
+    final String fileName = IndexFileNames.segmentFileName(si.name, "", Lucene40SegmentInfoFormat.SI_EXTENSION);
+    si.addFile(fileName);
+
+    final IndexOutput output = dir.createOutput(fileName, ioContext);
+
+    boolean success = false;
+    try {
+      CodecUtil.writeHeader(output, Lucene40SegmentInfoFormat.CODEC_NAME, Lucene40SegmentInfoFormat.VERSION_CURRENT);
+      // Write the Lucene version that created this segment, since 3.1
+      output.writeString(si.getVersion());
+      output.writeInt(si.getDocCount());
+
+      output.writeByte((byte) (si.getUseCompoundFile() ? SegmentInfo.YES : SegmentInfo.NO));
+      output.writeStringStringMap(si.getDiagnostics());
+      output.writeStringStringMap(si.attributes());
+      output.writeStringSet(si.files());
+
+      success = true;
+    } finally {
+      if (!success) {
+        IOUtils.closeWhileHandlingException(output);
+        si.dir.deleteFile(fileName);
+      } else {
+        output.close();
+      }
+    }
+  }
+}


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40StoredFieldsFormat.java lucene4055/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40StoredFieldsFormat.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40StoredFieldsFormat.java	2012-05-24 16:55:47.880233426 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40StoredFieldsFormat.java	2012-05-24 12:00:16.583924646 -0400
@@ -18,7 +18,6 @@
  */
 
 import java.io.IOException;
-import java.util.Set;
 
 import org.apache.lucene.codecs.StoredFieldsFormat;
 import org.apache.lucene.codecs.StoredFieldsReader;
@@ -89,13 +88,8 @@
   }
 
   @Override
-  public StoredFieldsWriter fieldsWriter(Directory directory, String segment,
+  public StoredFieldsWriter fieldsWriter(Directory directory, SegmentInfo si,
       IOContext context) throws IOException {
-    return new Lucene40StoredFieldsWriter(directory, segment, context);
-  }
-
-  @Override
-  public void files(SegmentInfo info, Set<String> files) throws IOException {
-    Lucene40StoredFieldsReader.files(info, files);
+    return new Lucene40StoredFieldsWriter(directory, si.name, context);
   }
 }


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40StoredFieldsReader.java lucene4055/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40StoredFieldsReader.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40StoredFieldsReader.java	2012-05-24 16:55:47.880233427 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40StoredFieldsReader.java	2012-05-24 12:00:23.095924759 -0400
@@ -34,7 +34,6 @@
 import org.apache.lucene.util.IOUtils;
 
 import java.io.Closeable;
-import java.util.Set;
 
 import static org.apache.lucene.codecs.lucene40.Lucene40StoredFieldsWriter.*;
 
@@ -90,8 +89,8 @@
       final long indexSize = indexStream.length() - HEADER_LENGTH_IDX;
       this.size = (int) (indexSize >> 3);
       // Verify two sources of "maxDoc" agree:
-      if (this.size != si.docCount) {
-        throw new CorruptIndexException("doc counts differ for segment " + segment + ": fieldsReader shows " + this.size + " but segmentInfo shows " + si.docCount);
+      if (this.size != si.getDocCount()) {
+        throw new CorruptIndexException("doc counts differ for segment " + segment + ": fieldsReader shows " + this.size + " but segmentInfo shows " + si.getDocCount());
       }
       numTotalDocs = (int) (indexSize >> 3);
       success = true;
@@ -239,9 +238,4 @@
 
     return fieldsStream;
   }
-
-  public static void files(SegmentInfo info, Set<String> files) throws IOException {
-    files.add(IndexFileNames.segmentFileName(info.name, "", FIELDS_INDEX_EXTENSION));
-    files.add(IndexFileNames.segmentFileName(info.name, "", FIELDS_EXTENSION));
-  }
 }


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40StoredFieldsWriter.java lucene4055/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40StoredFieldsWriter.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40StoredFieldsWriter.java	2012-05-24 16:55:47.892233424 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40StoredFieldsWriter.java	2012-05-23 12:12:59.746433327 -0400
@@ -23,6 +23,7 @@
 import org.apache.lucene.document.Document;
 import org.apache.lucene.index.CorruptIndexException;
 import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.FieldInfos;
 import org.apache.lucene.index.IndexFileNames;
 import org.apache.lucene.index.IndexableField;
 import org.apache.lucene.index.MergePolicy.MergeAbortedException;
@@ -208,7 +209,7 @@
   }
 
   @Override
-  public void finish(int numDocs) throws IOException {
+  public void finish(FieldInfos fis, int numDocs) throws IOException {
     if (HEADER_LENGTH_IDX+((long) numDocs)*8 != indexStream.getFilePointer())
       // This is most likely a bug in Sun JRE 1.6.0_04/_05;
       // we detect that the bug has struck, here, and
@@ -244,7 +245,7 @@
                                           reader, matchingFieldsReader, rawDocLengths);
       }
     }
-    finish(docCount);
+    finish(mergeState.fieldInfos, docCount);
     return docCount;
   }
 


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40TermVectorsFormat.java lucene4055/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40TermVectorsFormat.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40TermVectorsFormat.java	2012-05-24 16:55:47.892233424 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40TermVectorsFormat.java	2012-05-24 12:00:32.671924926 -0400
@@ -18,7 +18,6 @@
  */
 
 import java.io.IOException;
-import java.util.Set;
 
 import org.apache.lucene.codecs.TermVectorsFormat;
 import org.apache.lucene.codecs.TermVectorsReader;
@@ -109,12 +108,7 @@
   }
 
   @Override
-  public TermVectorsWriter vectorsWriter(Directory directory, String segment, IOContext context) throws IOException {
-    return new Lucene40TermVectorsWriter(directory, segment, context);
-  }
-
-  @Override
-  public void files(SegmentInfo info, Set<String> files) throws IOException {
-    Lucene40TermVectorsReader.files(info, files);
+  public TermVectorsWriter vectorsWriter(Directory directory, SegmentInfo segmentInfo, IOContext context) throws IOException {
+    return new Lucene40TermVectorsWriter(directory, segmentInfo.name, context);
   }
 }


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40TermVectorsReader.java lucene4055/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40TermVectorsReader.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40TermVectorsReader.java	2012-05-24 16:55:47.880233426 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40TermVectorsReader.java	2012-05-24 12:00:41.371925077 -0400
@@ -22,7 +22,6 @@
 import java.util.Comparator;
 import java.util.HashMap;
 import java.util.Map;
-import java.util.Set;
 
 import org.apache.lucene.codecs.TermVectorsReader;
 import org.apache.lucene.index.CorruptIndexException;
@@ -44,7 +43,6 @@
 import org.apache.lucene.util.CodecUtil;
 import org.apache.lucene.util.IOUtils;
 
-
 /**
  * Lucene 4.0 Term Vectors reader.
  * <p>
@@ -100,7 +98,7 @@
   public Lucene40TermVectorsReader(Directory d, SegmentInfo si, FieldInfos fieldInfos, IOContext context)
     throws CorruptIndexException, IOException {
     final String segment = si.name;
-    final int size = si.docCount;
+    final int size = si.getDocCount();
     
     boolean success = false;
 
@@ -256,7 +254,7 @@
         @Override
         public String next() throws IOException {
           if (fieldNumbers != null && fieldUpto < fieldNumbers.length) {
-            return fieldInfos.fieldName(fieldNumbers[fieldUpto++]);
+            return fieldInfos.fieldInfo(fieldNumbers[fieldUpto++]).name;
           } else {
             return null;
           }
@@ -264,7 +262,7 @@
 
         @Override
         public Terms terms() throws IOException {
-          return TVFields.this.terms(fieldInfos.fieldName(fieldNumbers[fieldUpto-1]));
+          return TVFields.this.terms(fieldInfos.fieldInfo(fieldNumbers[fieldUpto-1]).name);
         }
       };
     }
@@ -690,13 +688,5 @@
     
     return new Lucene40TermVectorsReader(fieldInfos, cloneTvx, cloneTvd, cloneTvf, size, numTotalDocs);
   }
-  
-  public static void files(SegmentInfo info, Set<String> files) throws IOException {
-    if (info.getHasVectors()) {
-      files.add(IndexFileNames.segmentFileName(info.name, "", VECTORS_INDEX_EXTENSION));
-      files.add(IndexFileNames.segmentFileName(info.name, "", VECTORS_FIELDS_EXTENSION));
-      files.add(IndexFileNames.segmentFileName(info.name, "", VECTORS_DOCUMENTS_EXTENSION));
-    }
-  }
 }
 


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40TermVectorsWriter.java lucene4055/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40TermVectorsWriter.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40TermVectorsWriter.java	2012-05-24 16:55:47.880233427 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40TermVectorsWriter.java	2012-05-23 12:12:20.990432652 -0400
@@ -23,6 +23,7 @@
 import org.apache.lucene.codecs.TermVectorsReader;
 import org.apache.lucene.codecs.TermVectorsWriter;
 import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.FieldInfos;
 import org.apache.lucene.index.Fields;
 import org.apache.lucene.index.IndexFileNames;
 import org.apache.lucene.index.MergePolicy.MergeAbortedException;
@@ -270,7 +271,7 @@
         numDocs += copyVectorsNoDeletions(mergeState, matchingVectorsReader, reader, rawDocLengths, rawDocLengths2);
       }
     }
-    finish(numDocs);
+    finish(mergeState.fieldInfos, numDocs);
     return numDocs;
   }
 
@@ -361,7 +362,7 @@
   }
   
   @Override
-  public void finish(int numDocs) throws IOException {
+  public void finish(FieldInfos fis, int numDocs) throws IOException {
     if (HEADER_LENGTH_INDEX+((long) numDocs)*16 != tvx.getFilePointer())
       // This is most likely a bug in Sun JRE 1.6.0_04/_05;
       // we detect that the bug has struck, here, and


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/lucene40/values/DocValuesWriterBase.java lucene4055/lucene/core/src/java/org/apache/lucene/codecs/lucene40/values/DocValuesWriterBase.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/lucene40/values/DocValuesWriterBase.java	2012-05-24 16:55:47.884233427 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/codecs/lucene40/values/DocValuesWriterBase.java	2012-05-23 13:47:15.338531816 -0400
@@ -67,7 +67,7 @@
    *                         docvalues of type {@link Type#BYTES_FIXED_SORTED} and {@link Type#BYTES_VAR_SORTED}.
    */
   protected DocValuesWriterBase(PerDocWriteState state, boolean fasterButMoreRam) {
-    this.segmentName = state.segmentName;
+    this.segmentName = state.segmentInfo.name;
     this.bytesUsed = state.bytesUsed;
     this.context = state.context;
     this.fasterButMoreRam = fasterButMoreRam;


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/lucene40/values/FixedSortedBytesImpl.java lucene4055/lucene/core/src/java/org/apache/lucene/codecs/lucene40/values/FixedSortedBytesImpl.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/lucene40/values/FixedSortedBytesImpl.java	2012-05-24 16:55:47.884233427 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/codecs/lucene40/values/FixedSortedBytesImpl.java	2012-05-23 15:56:42.170667071 -0400
@@ -68,7 +68,7 @@
         throws IOException {
       boolean success = false;
       try {
-        final MergeContext ctx = SortedBytesMergeUtils.init(Type.BYTES_FIXED_SORTED, docValues, comp, mergeState.mergedDocCount);
+        final MergeContext ctx = SortedBytesMergeUtils.init(Type.BYTES_FIXED_SORTED, docValues, comp, mergeState.segmentInfo.getDocCount());
         List<SortedSourceSlice> slices = SortedBytesMergeUtils.buildSlices(mergeState.docBase, mergeState.docMaps, docValues, ctx);
         final IndexOutput datOut = getOrCreateDataOut();
         datOut.writeInt(ctx.sizePerValues);


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/lucene40/values/VarSortedBytesImpl.java lucene4055/lucene/core/src/java/org/apache/lucene/codecs/lucene40/values/VarSortedBytesImpl.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/lucene40/values/VarSortedBytesImpl.java	2012-05-24 16:55:47.880233427 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/codecs/lucene40/values/VarSortedBytesImpl.java	2012-05-23 15:56:57.210667333 -0400
@@ -71,7 +71,7 @@
         throws IOException {
       boolean success = false;
       try {
-        MergeContext ctx = SortedBytesMergeUtils.init(Type.BYTES_VAR_SORTED, docValues, comp, mergeState.mergedDocCount);
+        MergeContext ctx = SortedBytesMergeUtils.init(Type.BYTES_VAR_SORTED, docValues, comp, mergeState.segmentInfo.getDocCount());
         final List<SortedSourceSlice> slices = SortedBytesMergeUtils.buildSlices(mergeState.docBase, mergeState.docMaps, docValues, ctx);
         IndexOutput datOut = getOrCreateDataOut();
         


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/memory/MemoryPostingsFormat.java lucene4055/lucene/core/src/java/org/apache/lucene/codecs/memory/MemoryPostingsFormat.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/memory/MemoryPostingsFormat.java	2012-05-24 16:55:47.960233428 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/codecs/memory/MemoryPostingsFormat.java	2012-05-24 12:00:56.495925341 -0400
@@ -20,7 +20,6 @@
 import java.io.IOException;
 import java.util.Comparator;
 import java.util.Iterator;
-import java.util.Set;
 import java.util.SortedMap;
 import java.util.TreeMap;
 
@@ -37,7 +36,6 @@
 import org.apache.lucene.index.FieldInfos;
 import org.apache.lucene.index.FieldsEnum;
 import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.SegmentInfo;
 import org.apache.lucene.index.SegmentReadState;
 import org.apache.lucene.index.SegmentWriteState;
 import org.apache.lucene.index.Terms;
@@ -133,7 +131,7 @@
         lastDocID = docID;
         docCount++;
 
-        if (field.indexOptions == IndexOptions.DOCS_ONLY) {
+        if (field.getIndexOptions() == IndexOptions.DOCS_ONLY) {
           buffer.writeVInt(delta);
         } else if (termDocFreq == 1) {
           buffer.writeVInt((delta<<1) | 1);
@@ -149,7 +147,7 @@
 
       @Override
       public void addPosition(int pos, BytesRef payload, int startOffset, int endOffset) throws IOException {
-        assert payload == null || field.storePayloads;
+        assert payload == null || field.hasPayloads();
 
         //System.out.println("      addPos pos=" + pos + " payload=" + payload);
 
@@ -159,7 +157,7 @@
         
         int payloadLen = 0;
         
-        if (field.storePayloads) {
+        if (field.hasPayloads()) {
           payloadLen = payload == null ? 0 : payload.length;
           if (payloadLen != lastPayloadLen) {
             lastPayloadLen = payloadLen;
@@ -172,7 +170,7 @@
           buffer.writeVInt(delta);
         }
         
-        if (field.indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0) {
+        if (field.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0) {
           // don't use startOffset - lastEndOffset, because this creates lots of negative vints for synonyms,
           // and the numbers aren't that much smaller anyways.
           int offsetDelta = startOffset - lastOffset;
@@ -229,7 +227,7 @@
       assert buffer2.getFilePointer() == 0;
 
       buffer2.writeVInt(stats.docFreq);
-      if (field.indexOptions != IndexOptions.DOCS_ONLY) {
+      if (field.getIndexOptions() != IndexOptions.DOCS_ONLY) {
         buffer2.writeVLong(stats.totalTermFreq-stats.docFreq);
       }
       int pos = (int) buffer2.getFilePointer();
@@ -260,7 +258,7 @@
       if (termCount > 0) {
         out.writeVInt(termCount);
         out.writeVInt(field.number);
-        if (field.indexOptions != IndexOptions.DOCS_ONLY) {
+        if (field.getIndexOptions() != IndexOptions.DOCS_ONLY) {
           out.writeVLong(sumTotalTermFreq);
         }
         out.writeVLong(sumDocFreq);
@@ -285,7 +283,7 @@
   @Override
   public FieldsConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
 
-    final String fileName = IndexFileNames.segmentFileName(state.segmentName, state.segmentSuffix, EXTENSION);
+    final String fileName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, EXTENSION);
     final IndexOutput out = state.directory.createOutput(fileName, state.context);
     
     return new FieldsConsumer() {
@@ -648,7 +646,7 @@
       if (!didDecode) {
         buffer.reset(current.output.bytes, 0, current.output.length);
         docFreq = buffer.readVInt();
-        if (field.indexOptions != IndexOptions.DOCS_ONLY) {
+        if (field.getIndexOptions() != IndexOptions.DOCS_ONLY) {
           totalTermFreq = docFreq + buffer.readVLong();
         } else {
           totalTermFreq = -1;
@@ -697,14 +695,14 @@
       decodeMetaData();
       FSTDocsEnum docsEnum;
 
-      if (needsFreqs && field.indexOptions == IndexOptions.DOCS_ONLY) {
+      if (needsFreqs && field.getIndexOptions() == IndexOptions.DOCS_ONLY) {
         return null;
       } else if (reuse == null || !(reuse instanceof FSTDocsEnum)) {
-        docsEnum = new FSTDocsEnum(field.indexOptions, field.storePayloads);
+        docsEnum = new FSTDocsEnum(field.getIndexOptions(), field.hasPayloads());
       } else {
         docsEnum = (FSTDocsEnum) reuse;        
-        if (!docsEnum.canReuse(field.indexOptions, field.storePayloads)) {
-          docsEnum = new FSTDocsEnum(field.indexOptions, field.storePayloads);
+        if (!docsEnum.canReuse(field.getIndexOptions(), field.hasPayloads())) {
+          docsEnum = new FSTDocsEnum(field.getIndexOptions(), field.hasPayloads());
         }
       }
       return docsEnum.reset(current.output, liveDocs, docFreq);
@@ -713,22 +711,22 @@
     @Override
     public DocsAndPositionsEnum docsAndPositions(Bits liveDocs, DocsAndPositionsEnum reuse, boolean needsOffsets) throws IOException {
 
-      boolean hasOffsets = field.indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;
+      boolean hasOffsets = field.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;
       if (needsOffsets && !hasOffsets) {
         return null; // not available
       }
       
-      if (field.indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) < 0) {
+      if (field.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) < 0) {
         return null;
       }
       decodeMetaData();
       FSTDocsAndPositionsEnum docsAndPositionsEnum;
       if (reuse == null || !(reuse instanceof FSTDocsAndPositionsEnum)) {
-        docsAndPositionsEnum = new FSTDocsAndPositionsEnum(field.storePayloads, hasOffsets);
+        docsAndPositionsEnum = new FSTDocsAndPositionsEnum(field.hasPayloads(), hasOffsets);
       } else {
         docsAndPositionsEnum = (FSTDocsAndPositionsEnum) reuse;        
-        if (!docsAndPositionsEnum.canReuse(field.storePayloads, hasOffsets)) {
-          docsAndPositionsEnum = new FSTDocsAndPositionsEnum(field.storePayloads, hasOffsets);
+        if (!docsAndPositionsEnum.canReuse(field.hasPayloads(), hasOffsets)) {
+          docsAndPositionsEnum = new FSTDocsAndPositionsEnum(field.hasPayloads(), hasOffsets);
         }
       }
       //System.out.println("D&P reset this=" + this);
@@ -797,7 +795,7 @@
       this.termCount = termCount;
       final int fieldNumber = in.readVInt();
       field = fieldInfos.fieldInfo(fieldNumber);
-      if (field.indexOptions != IndexOptions.DOCS_ONLY) {
+      if (field.getIndexOptions() != IndexOptions.DOCS_ONLY) {
         sumTotalTermFreq = in.readVLong();
       } else {
         sumTotalTermFreq = -1;
@@ -901,9 +899,4 @@
       }
     };
   }
-
-  @Override
-  public void files(SegmentInfo segmentInfo, String segmentSuffix, Set<String> files) throws IOException {
-    files.add(IndexFileNames.segmentFileName(segmentInfo.name, segmentSuffix, EXTENSION));
-  }
 }


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/NormsFormat.java lucene4055/lucene/core/src/java/org/apache/lucene/codecs/NormsFormat.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/NormsFormat.java	2012-05-24 16:55:48.012233429 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/codecs/NormsFormat.java	2012-05-24 11:54:58.923919113 -0400
@@ -18,17 +18,20 @@
  */
 
 import java.io.IOException;
-import java.util.Set;
 
 import org.apache.lucene.index.PerDocWriteState;
-import org.apache.lucene.index.SegmentInfo;
 import org.apache.lucene.index.SegmentReadState;
 
 /**
  * format for normalization factors
  */
 public abstract class NormsFormat {
+
+  /** Returns a {@link PerDocConsumer} to write norms to the
+   *  index. */
   public abstract PerDocConsumer docsConsumer(PerDocWriteState state) throws IOException;
+
+  /** Returns a {@link PerDocProducer} to read norms from the
+   *  index. */
   public abstract PerDocProducer docsProducer(SegmentReadState state) throws IOException;
-  public abstract void files(SegmentInfo info, Set<String> files) throws IOException;
 }


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/perfield/PerFieldPostingsFormat.java lucene4055/lucene/core/src/java/org/apache/lucene/codecs/perfield/PerFieldPostingsFormat.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/perfield/PerFieldPostingsFormat.java	2012-05-24 16:55:48.028233429 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/codecs/perfield/PerFieldPostingsFormat.java	2012-05-22 11:39:04.604893278 -0400
@@ -17,15 +17,11 @@
  * limitations under the License.
  */
 
-import java.io.Closeable;
-import java.io.FileNotFoundException;
 import java.io.IOException;
-import java.util.HashMap;
 import java.util.IdentityHashMap;
 import java.util.Iterator;
 import java.util.Map;
 import java.util.ServiceLoader; // javadocs
-import java.util.Set;
 import java.util.TreeMap;
 
 import org.apache.lucene.codecs.FieldsConsumer;
@@ -34,17 +30,9 @@
 import org.apache.lucene.codecs.TermsConsumer;
 import org.apache.lucene.index.FieldInfo;
 import org.apache.lucene.index.FieldsEnum;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.SegmentInfo;
 import org.apache.lucene.index.SegmentReadState;
 import org.apache.lucene.index.SegmentWriteState;
 import org.apache.lucene.index.Terms;
-import org.apache.lucene.store.DataOutput; // javadocs
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.util.CodecUtil;
 import org.apache.lucene.util.IOUtils;
 
 /**
@@ -56,32 +44,17 @@
  * This method uses Java's 
  * {@link ServiceLoader Service Provider Interface} to resolve format names.
  * <p>
- * PerFieldFile format:
- * <ul>
- *   <li>PerFieldFile (.per) --&gt; Header, IdToFormat, FieldToFormat</li>
- *   <li>Header --&gt; {@link CodecUtil#checkHeader CodecHeader}</li>
- *   <li>IdToFormat,FieldToFormat --&gt; {@link DataOutput#writeStringStringMap(Map) Map&lt;String,String&gt;}</li> 
- * </ul>
- * <p>Notes:</p>
- * <ul>
- *   <li>each format is assigned an id, and files written by that posting format
- *       have an additional suffix containing the id. For example, in a per-field
- *       configuration instead of <tt>_1.prx</tt> filenames would look like 
- *       <tt>_1_0.prx</tt>.</li>
- *   <li>IdToFormat is a mapping between these ids and the available formats.</li>
- *   <li>FieldToFormat is a mapping between field names and format names.</li>
- * </ul>
+ * Files written by each posting format have an additional suffix containing the 
+ * format name. For example, in a per-field configuration instead of <tt>_1.prx</tt> 
+ * filenames would look like <tt>_1_Lucene40.prx</tt>.
  * @see ServiceLoader
  * @lucene.experimental
  */
 
 public abstract class PerFieldPostingsFormat extends PostingsFormat {
-
-  public static final String PER_FIELD_EXTENSION = "per";
   public static final String PER_FIELD_NAME = "PerField40";
-
-  public static final int VERSION_START = 0;
-  public static final int VERSION_LATEST = VERSION_START;
+  
+  public static final String PER_FIELD_FORMAT_KEY = PerFieldPostingsFormat.class.getSimpleName() + ".format";
 
   public PerFieldPostingsFormat() {
     super(PER_FIELD_NAME);
@@ -92,29 +65,10 @@
       throws IOException {
     return new FieldsWriter(state);
   }
-
-  // NOTE: not private to avoid $accessN at runtime!!
-  static class FieldsConsumerAndID implements Closeable {
-    final FieldsConsumer fieldsConsumer;
-    final String segmentSuffix;
-
-    public FieldsConsumerAndID(FieldsConsumer fieldsConsumer, String segmentSuffix) {
-      this.fieldsConsumer = fieldsConsumer;
-      this.segmentSuffix = segmentSuffix;
-    }
-
-    @Override
-    public void close() throws IOException {
-      fieldsConsumer.close();
-    }
-  };
     
   private class FieldsWriter extends FieldsConsumer {
 
-    private final Map<PostingsFormat,FieldsConsumerAndID> formats = new IdentityHashMap<PostingsFormat,FieldsConsumerAndID>();
-
-    /** Records all fields we wrote. */
-    private final Map<String,PostingsFormat> fieldToFormat = new HashMap<String,PostingsFormat>();
+    private final Map<PostingsFormat,FieldsConsumer> formats = new IdentityHashMap<PostingsFormat,FieldsConsumer>();
 
     private final SegmentWriteState segmentWriteState;
 
@@ -128,61 +82,32 @@
       if (format == null) {
         throw new IllegalStateException("invalid null PostingsFormat for field=\"" + field.name + "\"");
       }
-
-      assert !fieldToFormat.containsKey(field.name);
-      fieldToFormat.put(field.name, format);
-
-      FieldsConsumerAndID consumerAndId = formats.get(format);
-      if (consumerAndId == null) {
-        // First time we are seeing this format; assign
-        // next id and init it:
+      
+      String previousValue = field.putAttribute(PER_FIELD_FORMAT_KEY, format.getName());
+      assert previousValue == null;
+
+      FieldsConsumer consumer = formats.get(format);
+      if (consumer == null) {
+        // First time we are seeing this format; create a new instance
         final String segmentSuffix = getFullSegmentSuffix(field.name,
                                                           segmentWriteState.segmentSuffix,
-                                                          ""+formats.size());
-        consumerAndId = new FieldsConsumerAndID(format.fieldsConsumer(new SegmentWriteState(segmentWriteState, segmentSuffix)),
-                                                segmentSuffix);
-        formats.put(format, consumerAndId);
+                                                          format.getName());
+        consumer = format.fieldsConsumer(new SegmentWriteState(segmentWriteState, segmentSuffix));
+        formats.put(format, consumer);
       }
 
-      return consumerAndId.fieldsConsumer.addField(field);
+      // TODO: we should only provide the "slice" of FIS
+      // that this PF actually sees ... then stuff like
+      // .hasProx could work correctly?
+      // NOTE: .hasProx is already broken in the same way for the non-perfield case,
+      // if there is a fieldinfo with prox that has no postings, you get a 0 byte file.
+      return consumer.addField(field);
     }
 
     @Override
     public void close() throws IOException {
-
       // Close all subs
       IOUtils.close(formats.values());
-
-      // Write _X.per: maps field name -> format name and
-      // format name -> format id
-      final String mapFileName = IndexFileNames.segmentFileName(segmentWriteState.segmentName, segmentWriteState.segmentSuffix, PER_FIELD_EXTENSION);
-      final IndexOutput out = segmentWriteState.directory.createOutput(mapFileName, segmentWriteState.context);
-      boolean success = false;
-      try {
-        CodecUtil.writeHeader(out, PER_FIELD_NAME, VERSION_LATEST);
-
-        // format name -> int id
-        out.writeVInt(formats.size());
-        for(Map.Entry<PostingsFormat,FieldsConsumerAndID> ent : formats.entrySet()) {
-          out.writeString(ent.getValue().segmentSuffix);
-          out.writeString(ent.getKey().getName());
-        }
-
-        // field name -> format name
-        out.writeVInt(fieldToFormat.size());
-        for(Map.Entry<String,PostingsFormat> ent : fieldToFormat.entrySet()) {
-          out.writeString(ent.getKey());
-          out.writeString(ent.getValue().getName());
-        }
-
-        success = true;
-      } finally {
-        if (!success) {
-          IOUtils.closeWhileHandlingException(out);
-        } else {
-          IOUtils.close(out);
-        }
-      }
     }
   }
 
@@ -207,18 +132,21 @@
       // Read _X.per and init each format:
       boolean success = false;
       try {
-        new VisitPerFieldFile(readState.dir, readState.segmentInfo.name, readState.segmentSuffix) {
-          @Override
-          protected void visitOneFormat(String segmentSuffix, PostingsFormat postingsFormat) throws IOException {
-            formats.put(postingsFormat, postingsFormat.fieldsProducer(new SegmentReadState(readState, segmentSuffix)));
-          }
-
-          @Override
-          protected void visitOneField(String fieldName, PostingsFormat postingsFormat) throws IOException {
-            assert formats.containsKey(postingsFormat);
-            fields.put(fieldName, formats.get(postingsFormat));
+        // Read field name -> format name
+        for (FieldInfo fi : readState.fieldInfos) {
+          if (fi.isIndexed()) {
+            final String fieldName = fi.name;
+            final String formatName = fi.getAttribute(PER_FIELD_FORMAT_KEY);
+            if (formatName != null) {
+              // null formatName means the field is in fieldInfos, but has no postings!
+              PostingsFormat format = PostingsFormat.forName(formatName);
+              if (!formats.containsKey(format)) {
+                formats.put(format, format.fieldsProducer(new SegmentReadState(readState, formatName)));
+              }
+              fields.put(fieldName, formats.get(format));
+            }
           }
-        };
+        }
         success = true;
       } finally {
         if (!success) {
@@ -280,83 +208,6 @@
     return new FieldsReader(state);
   }
 
-  private abstract class VisitPerFieldFile {
-    public VisitPerFieldFile(Directory dir, String segmentName, String outerSegmentSuffix) throws IOException {
-      final String mapFileName = IndexFileNames.segmentFileName(segmentName, outerSegmentSuffix, PER_FIELD_EXTENSION);
-      final IndexInput in = dir.openInput(mapFileName, IOContext.READONCE);
-      boolean success = false;
-      try {
-        CodecUtil.checkHeader(in, PER_FIELD_NAME, VERSION_START, VERSION_LATEST);
-
-        // Read format name -> format id
-        final int formatCount = in.readVInt();
-        for(int formatIDX=0;formatIDX<formatCount;formatIDX++) {
-          final String segmentSuffix = in.readString();
-          final String formatName = in.readString();
-          PostingsFormat postingsFormat = PostingsFormat.forName(formatName);
-          //System.out.println("do lookup " + formatName + " -> " + postingsFormat);
-          if (postingsFormat == null) {
-            throw new IllegalStateException("unable to lookup PostingsFormat for name=\"" + formatName + "\": got null");
-          }
-
-          // Better be defined, because it was defined
-          // during indexing:
-          visitOneFormat(segmentSuffix, postingsFormat);
-        }
-
-        // Read field name -> format name
-        final int fieldCount = in.readVInt();
-        for(int fieldIDX=0;fieldIDX<fieldCount;fieldIDX++) {
-          final String fieldName = in.readString();
-          final String formatName = in.readString();
-          visitOneField(fieldName, PostingsFormat.forName(formatName));
-        }
-
-        success = true;
-      } finally {
-        if (!success) {
-          IOUtils.closeWhileHandlingException(in);
-        } else {
-          IOUtils.close(in);
-        }
-      }
-    }
-
-    // This is called first, for all formats:
-    protected abstract void visitOneFormat(String segmentSuffix, PostingsFormat format) throws IOException;
-
-    // ... then this is called, for all fields:
-    protected abstract void visitOneField(String fieldName, PostingsFormat format) throws IOException;
-  }
-
-  @Override
-  public void files(final SegmentInfo info, String segmentSuffix, final Set<String> files) throws IOException {
-    final Directory dir = info.dir;
-
-    final String mapFileName = IndexFileNames.segmentFileName(info.name, segmentSuffix, PER_FIELD_EXTENSION);
-    files.add(mapFileName);
-
-    try {
-      new VisitPerFieldFile(dir, info.name, segmentSuffix) {
-        @Override
-        protected void visitOneFormat(String segmentSuffix, PostingsFormat format) throws IOException {
-          format.files(info, segmentSuffix, files);
-        }
-
-        @Override
-          protected void visitOneField(String field, PostingsFormat format) {
-        }
-      };
-    } catch (FileNotFoundException fnfe) {
-      // TODO: this is somewhat shady... if we can't open
-      // the .per file then most likely someone is calling
-      // .files() after this segment was deleted, so, they
-      // wouldn't be able to do anything with the files even
-      // if we could return them, so we don't add any files
-      // in this case.
-    }
-  }
-
   // NOTE: only called during writing; for reading we read
   // all we need from the index (ie we save the field ->
   // format mapping)


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/PostingsBaseFormat.java lucene4055/lucene/core/src/java/org/apache/lucene/codecs/PostingsBaseFormat.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/PostingsBaseFormat.java	2012-05-24 16:55:47.932233427 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/codecs/PostingsBaseFormat.java	2012-05-24 11:55:08.023919272 -0400
@@ -18,9 +18,7 @@
  */
 
 import java.io.IOException;
-import java.util.Set;
 
-import org.apache.lucene.index.SegmentInfo;
 import org.apache.lucene.index.SegmentWriteState;
 import org.apache.lucene.index.SegmentReadState;
 
@@ -49,6 +47,4 @@
   public abstract PostingsReaderBase postingsReaderBase(SegmentReadState state) throws IOException;
 
   public abstract PostingsWriterBase postingsWriterBase(SegmentWriteState state) throws IOException;
-
-  public abstract void files(SegmentInfo segmentInfo, String segmentSuffix, Set<String> files) throws IOException;
 }


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/PostingsConsumer.java lucene4055/lucene/core/src/java/org/apache/lucene/codecs/PostingsConsumer.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/PostingsConsumer.java	2012-05-24 16:55:47.948233428 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/codecs/PostingsConsumer.java	2012-05-21 13:58:03.063533879 -0400
@@ -70,7 +70,8 @@
     int df = 0;
     long totTF = 0;
 
-    if (mergeState.fieldInfo.indexOptions == IndexOptions.DOCS_ONLY) {
+    IndexOptions indexOptions = mergeState.fieldInfo.getIndexOptions();
+    if (indexOptions == IndexOptions.DOCS_ONLY) {
       while(true) {
         final int doc = postings.nextDoc();
         if (doc == DocIdSetIterator.NO_MORE_DOCS) {
@@ -82,7 +83,7 @@
         df++;
       }
       totTF = -1;
-    } else if (mergeState.fieldInfo.indexOptions == IndexOptions.DOCS_AND_FREQS) {
+    } else if (indexOptions == IndexOptions.DOCS_AND_FREQS) {
       while(true) {
         final int doc = postings.nextDoc();
         if (doc == DocIdSetIterator.NO_MORE_DOCS) {
@@ -95,7 +96,7 @@
         df++;
         totTF += freq;
       }
-    } else if (mergeState.fieldInfo.indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) {
+    } else if (indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) {
       final DocsAndPositionsEnum postingsEnum = (DocsAndPositionsEnum) postings;
       while(true) {
         final int doc = postingsEnum.nextDoc();
@@ -120,7 +121,7 @@
         df++;
       }
     } else {
-      assert mergeState.fieldInfo.indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS;
+      assert indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS;
       final DocsAndPositionsEnum postingsEnum = (DocsAndPositionsEnum) postings;
       while(true) {
         final int doc = postingsEnum.nextDoc();


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/PostingsFormat.java lucene4055/lucene/core/src/java/org/apache/lucene/codecs/PostingsFormat.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/PostingsFormat.java	2012-05-24 16:55:47.948233428 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/codecs/PostingsFormat.java	2012-05-22 11:39:04.532893276 -0400
@@ -20,7 +20,6 @@
 import java.io.IOException;
 import java.util.Set;
 
-import org.apache.lucene.index.SegmentInfo;
 import org.apache.lucene.index.SegmentWriteState;
 import org.apache.lucene.index.SegmentReadState;
 import org.apache.lucene.util.NamedSPILoader;
@@ -40,11 +39,12 @@
   private final String name;
   
   protected PostingsFormat(String name) {
+    NamedSPILoader.checkServiceName(name);
     this.name = name;
   }
 
   @Override
-  public String getName() {
+  public final String getName() {
     return name;
   }
   
@@ -56,15 +56,6 @@
    *  use; else, those files may be deleted. */
   public abstract FieldsProducer fieldsProducer(SegmentReadState state) throws IOException;
 
-  /**
-   * Gathers files associated with this segment
-   * 
-   * @param segmentInfo the {@link SegmentInfo} for this segment 
-   * @param segmentSuffix the format's suffix within this segment
-   * @param files the of files to add the codec files to.
-   */
-  public abstract void files(SegmentInfo segmentInfo, String segmentSuffix, Set<String> files) throws IOException;
-
   @Override
   public String toString() {
     return "PostingsFormat(name=" + name + ")";
@@ -79,5 +70,4 @@
   public static Set<String> availablePostingsFormats() {
     return loader.availableServices();
   }
-  
 }


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/pulsing/PulsingPostingsFormat.java lucene4055/lucene/core/src/java/org/apache/lucene/codecs/pulsing/PulsingPostingsFormat.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/pulsing/PulsingPostingsFormat.java	2012-05-24 16:55:47.900233426 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/codecs/pulsing/PulsingPostingsFormat.java	2012-05-24 12:01:08.427925549 -0400
@@ -18,7 +18,6 @@
  */
 
 import java.io.IOException;
-import java.util.Set;
 
 import org.apache.lucene.codecs.BlockTreeTermsReader;
 import org.apache.lucene.codecs.BlockTreeTermsWriter;
@@ -28,7 +27,6 @@
 import org.apache.lucene.codecs.PostingsFormat;
 import org.apache.lucene.codecs.PostingsReaderBase;
 import org.apache.lucene.codecs.PostingsWriterBase;
-import org.apache.lucene.index.SegmentInfo;
 import org.apache.lucene.index.SegmentReadState;
 import org.apache.lucene.index.SegmentWriteState;
 
@@ -112,10 +110,4 @@
   public int getFreqCutoff() {
     return freqCutoff;
   }
-
-  @Override
-  public void files(SegmentInfo segmentInfo, String segmentSuffix, Set<String> files) throws IOException {
-    wrappedPostingsBaseFormat.files(segmentInfo, segmentSuffix, files);
-    BlockTreeTermsReader.files(segmentInfo, segmentSuffix, files);
-  }
 }


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/pulsing/PulsingPostingsReader.java lucene4055/lucene/core/src/java/org/apache/lucene/codecs/pulsing/PulsingPostingsReader.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/pulsing/PulsingPostingsReader.java	2012-05-24 16:55:47.900233426 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/codecs/pulsing/PulsingPostingsReader.java	2012-05-21 13:58:02.991533877 -0400
@@ -148,7 +148,7 @@
     PulsingTermState termState = (PulsingTermState) _termState;
 
     // if we have positions, its total TF, otherwise its computed based on docFreq.
-    long count = fieldInfo.indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 ? termState.totalTermFreq : termState.docFreq;
+    long count = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 ? termState.totalTermFreq : termState.docFreq;
     //System.out.println("  count=" + count + " threshold=" + maxPositions);
 
     if (count <= maxPositions) {
@@ -179,7 +179,7 @@
 
   @Override
   public DocsEnum docs(FieldInfo field, BlockTermState _termState, Bits liveDocs, DocsEnum reuse, boolean needsFreqs) throws IOException {
-    if (needsFreqs && field.indexOptions == IndexOptions.DOCS_ONLY) {
+    if (needsFreqs && field.getIndexOptions() == IndexOptions.DOCS_ONLY) {
       return null;
     }
     PulsingTermState termState = (PulsingTermState) _termState;
@@ -217,9 +217,9 @@
   @Override
   public DocsAndPositionsEnum docsAndPositions(FieldInfo field, BlockTermState _termState, Bits liveDocs, DocsAndPositionsEnum reuse,
                                                boolean needsOffsets) throws IOException {
-    if (field.indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) < 0) {
+    if (field.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) < 0) {
       return null;
-    } else if (needsOffsets && field.indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) < 0) {
+    } else if (needsOffsets && field.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) < 0) {
       return null;
     }
 
@@ -270,9 +270,9 @@
     private int payloadLength;
 
     public PulsingDocsEnum(FieldInfo fieldInfo) {
-      indexOptions = fieldInfo.indexOptions;
-      storePayloads = fieldInfo.storePayloads;
-      storeOffsets = fieldInfo.indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;
+      indexOptions = fieldInfo.getIndexOptions();
+      storePayloads = fieldInfo.hasPayloads();
+      storeOffsets = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;
     }
 
     public PulsingDocsEnum reset(Bits liveDocs, PulsingTermState termState) {
@@ -296,7 +296,7 @@
     }
 
     boolean canReuse(FieldInfo fieldInfo) {
-      return indexOptions == fieldInfo.indexOptions && storePayloads == fieldInfo.storePayloads;
+      return indexOptions == fieldInfo.getIndexOptions() && storePayloads == fieldInfo.hasPayloads();
     }
 
     @Override
@@ -400,13 +400,13 @@
     private boolean payloadRetrieved;
 
     public PulsingDocsAndPositionsEnum(FieldInfo fieldInfo) {
-      indexOptions = fieldInfo.indexOptions;
-      storePayloads = fieldInfo.storePayloads;
-      storeOffsets = fieldInfo.indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;
+      indexOptions = fieldInfo.getIndexOptions();
+      storePayloads = fieldInfo.hasPayloads();
+      storeOffsets = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;
     }
 
     boolean canReuse(FieldInfo fieldInfo) {
-      return indexOptions == fieldInfo.indexOptions && storePayloads == fieldInfo.storePayloads;
+      return indexOptions == fieldInfo.getIndexOptions() && storePayloads == fieldInfo.hasPayloads();
     }
 
     public PulsingDocsAndPositionsEnum reset(Bits liveDocs, PulsingTermState termState) {


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/pulsing/PulsingPostingsWriter.java lucene4055/lucene/core/src/java/org/apache/lucene/codecs/pulsing/PulsingPostingsWriter.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/pulsing/PulsingPostingsWriter.java	2012-05-24 16:55:47.900233426 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/codecs/pulsing/PulsingPostingsWriter.java	2012-05-21 13:58:02.991533877 -0400
@@ -124,9 +124,9 @@
   // our parent calls setField whenever the field changes
   @Override
   public void setField(FieldInfo fieldInfo) {
-    this.indexOptions = fieldInfo.indexOptions;
+    this.indexOptions = fieldInfo.getIndexOptions();
     if (DEBUG) System.out.println("PW field=" + fieldInfo.name + " indexOptions=" + indexOptions);
-    storePayloads = fieldInfo.storePayloads;
+    storePayloads = fieldInfo.hasPayloads();
     wrappedPostingsWriter.setField(fieldInfo);
     //DEBUG = BlockTreeTermsWriter.DEBUG;
   }


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/SegmentInfoFormat.java lucene4055/lucene/core/src/java/org/apache/lucene/codecs/SegmentInfoFormat.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/SegmentInfoFormat.java	1969-12-31 19:00:00.000000000 -0500
+++ lucene4055/lucene/core/src/java/org/apache/lucene/codecs/SegmentInfoFormat.java	2012-05-22 11:39:04.580893277 -0400
@@ -0,0 +1,33 @@
+package org.apache.lucene.codecs;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.index.SegmentInfo;
+
+/**
+ * Expert: Controls the format of the 
+ * {@link SegmentInfo} (segment metadata file).
+ * <p>
+ * 
+ * @see SegmentInfo
+ * @lucene.experimental
+ */
+public abstract class SegmentInfoFormat {
+  public abstract SegmentInfoReader getSegmentInfosReader();
+  public abstract SegmentInfoWriter getSegmentInfosWriter();
+}


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/SegmentInfoReader.java lucene4055/lucene/core/src/java/org/apache/lucene/codecs/SegmentInfoReader.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/SegmentInfoReader.java	1969-12-31 19:00:00.000000000 -0500
+++ lucene4055/lucene/core/src/java/org/apache/lucene/codecs/SegmentInfoReader.java	2012-05-22 11:39:04.584893276 -0400
@@ -0,0 +1,41 @@
+package org.apache.lucene.codecs;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+
+/**
+ * Specifies an API for classes that can read {@link SegmentInfo} information.
+ * @lucene.experimental
+ */
+
+public abstract class SegmentInfoReader {
+
+  /**
+   * Read {@link SegmentInfo} data from a directory.
+   * @param directory directory to read from
+   * @param segmentName name of the segment to read
+   * @return infos instance to be populated with data
+   * @throws IOException
+   */
+  public abstract SegmentInfo read(Directory directory, String segmentName, IOContext context) throws IOException;
+}


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/SegmentInfosFormat.java lucene4055/lucene/core/src/java/org/apache/lucene/codecs/SegmentInfosFormat.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/SegmentInfosFormat.java	2012-05-24 16:55:47.932233427 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/codecs/SegmentInfosFormat.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,42 +0,0 @@
-package org.apache.lucene.codecs;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.index.SegmentInfos; // javadocs
-
-/**
- * Expert: Controls the format of the 
- * {@link SegmentInfos} (segments file).
- * <p>
- * NOTE: This isn't a per-segment file. If you change the format, other versions
- * of lucene won't be able to read it.
- * 
- * @see SegmentInfos
- * @lucene.experimental
- */
-// TODO: would be great to handle this situation better.
-// ideally a custom implementation could implement two-phase commit differently,
-// (e.g. atomic rename), and ideally all versions of lucene could still read it.
-// but this is just reflecting reality as it is today...
-//
-// also, perhaps the name should change (to cover all global files like .fnx?)
-// then again, maybe we can just remove that file...
-public abstract class SegmentInfosFormat {
-  public abstract SegmentInfosReader getSegmentInfosReader();
-  public abstract SegmentInfosWriter getSegmentInfosWriter();
-}


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/SegmentInfosReader.java lucene4055/lucene/core/src/java/org/apache/lucene/codecs/SegmentInfosReader.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/SegmentInfosReader.java	2012-05-24 16:55:47.948233428 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/codecs/SegmentInfosReader.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,42 +0,0 @@
-package org.apache.lucene.codecs;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.index.SegmentInfos;
-import org.apache.lucene.store.ChecksumIndexInput;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-
-/**
- * Specifies an API for classes that can read {@link SegmentInfos} information.
- * @lucene.experimental
- */
-public abstract class SegmentInfosReader {
-
-  /**
-   * Read {@link SegmentInfos} data from a directory.
-   * @param directory directory to read from
-   * @param segmentsFileName name of the "segments_N" file
-   * @param header input of "segments_N" file after reading preamble
-   * @param infos empty instance to be populated with data
-   * @throws IOException
-   */
-  public abstract void read(Directory directory, String segmentsFileName, ChecksumIndexInput header, SegmentInfos infos, IOContext context) throws IOException;
-}


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/SegmentInfosWriter.java lucene4055/lucene/core/src/java/org/apache/lucene/codecs/SegmentInfosWriter.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/SegmentInfosWriter.java	2012-05-24 16:55:47.992233428 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/codecs/SegmentInfosWriter.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,64 +0,0 @@
-package org.apache.lucene.codecs;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.index.SegmentInfos;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexOutput;
-
-/**
- * Specifies an API for classes that can write out {@link SegmentInfos} data.
- * @lucene.experimental
- */
-public abstract class SegmentInfosWriter {
-
-  /**
-   * Write {@link SegmentInfos} data without closing the output. The returned
-   * output will become finished only after a successful completion of
-   * "two phase commit" that first calls {@link #prepareCommit(IndexOutput)} and
-   * then {@link #finishCommit(IndexOutput)}.
-   * @param dir directory to write data to
-   * @param segmentsFileName name of the "segments_N" file to create
-   * @param infos data to write
-   * @return an instance of {@link IndexOutput} to be used in subsequent "two
-   * phase commit" operations as described above.
-   * @throws IOException
-   */
-  public abstract IndexOutput writeInfos(Directory dir, String segmentsFileName, String codecID, SegmentInfos infos, IOContext context) throws IOException;
-  
-  /**
-   * First phase of the two-phase commit - ensure that all output can be
-   * successfully written out.
-   * @param out an instance of {@link IndexOutput} returned from a previous
-   * call to {@link #writeInfos(Directory, String, String, SegmentInfos, IOContext)}.
-   * @throws IOException
-   */
-  public abstract void prepareCommit(IndexOutput out) throws IOException;
-  
-  /**
-   * Second phase of the two-phase commit. In this step the output should be
-   * finalized and closed.
-   * @param out an instance of {@link IndexOutput} returned from a previous
-   * call to {@link #writeInfos(Directory, String, String, SegmentInfos, IOContext)}.
-   * @throws IOException
-   */
-  public abstract void finishCommit(IndexOutput out) throws IOException;
-}


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/SegmentInfoWriter.java lucene4055/lucene/core/src/java/org/apache/lucene/codecs/SegmentInfoWriter.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/SegmentInfoWriter.java	1969-12-31 19:00:00.000000000 -0500
+++ lucene4055/lucene/core/src/java/org/apache/lucene/codecs/SegmentInfoWriter.java	2012-05-22 11:39:04.604893278 -0400
@@ -0,0 +1,39 @@
+package org.apache.lucene.codecs;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+
+/**
+ * Specifies an API for classes that can write out {@link SegmentInfo} data.
+ * @lucene.experimental
+ */
+
+public abstract class SegmentInfoWriter {
+
+  /**
+   * Write {@link SegmentInfo} data. 
+   * @throws IOException
+   */
+  public abstract void write(Directory dir, SegmentInfo info, FieldInfos fis, IOContext ioContext) throws IOException;
+}


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/sep/SepDocValuesConsumer.java lucene4055/lucene/core/src/java/org/apache/lucene/codecs/sep/SepDocValuesConsumer.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/sep/SepDocValuesConsumer.java	2012-05-24 16:55:48.012233429 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/codecs/sep/SepDocValuesConsumer.java	2012-05-24 12:01:28.007925891 -0400
@@ -18,31 +18,22 @@
  */
 
 import java.io.IOException;
-import java.util.HashSet;
-import java.util.Set;
 
-import org.apache.lucene.codecs.PerDocProducerBase;
 import org.apache.lucene.codecs.lucene40.values.DocValuesWriterBase;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.IndexFileNames;
 import org.apache.lucene.index.PerDocWriteState;
-import org.apache.lucene.index.SegmentInfo;
 import org.apache.lucene.store.Directory;
-import org.apache.lucene.util.IOUtils;
 
 /**
  * Implementation of PerDocConsumer that uses separate files.
  * @lucene.experimental
  */
+
 public class SepDocValuesConsumer extends DocValuesWriterBase {
   private final Directory directory;
-  private final FieldInfos fieldInfos;
 
   public SepDocValuesConsumer(PerDocWriteState state) throws IOException {
     super(state);
     this.directory = state.directory;
-    fieldInfos = state.fieldInfos;
   }
   
   @Override
@@ -50,61 +41,9 @@
     return directory;
   }
 
-  public static void files(SegmentInfo segmentInfo,
-      Set<String> files) throws IOException {
-    files(segmentInfo.dir, segmentInfo.getFieldInfos(), segmentInfo.name, files);
-  }
-  
-  @SuppressWarnings("fallthrough")
-  private static void files(Directory dir,FieldInfos fieldInfos, String segmentName, Set<String> files)  {
-    for (FieldInfo fieldInfo : fieldInfos) {
-      if (fieldInfo.hasDocValues()) {
-        String filename = PerDocProducerBase.docValuesId(segmentName, fieldInfo.number);
-        switch (fieldInfo.getDocValuesType()) {
-          case BYTES_FIXED_DEREF:
-          case BYTES_VAR_DEREF:
-          case BYTES_VAR_STRAIGHT:
-          case BYTES_FIXED_SORTED:
-          case BYTES_VAR_SORTED:
-            files.add(IndexFileNames.segmentFileName(filename, "",
-                INDEX_EXTENSION));
-            try {
-            assert dir.fileExists(IndexFileNames.segmentFileName(filename, "",
-                INDEX_EXTENSION));
-            } catch (IOException e) {
-              // don't throw checked exception - dir is only used in assert 
-              throw new RuntimeException(e);
-            }
-            // until here all types use an index
-          case BYTES_FIXED_STRAIGHT:
-          case FLOAT_32:
-          case FLOAT_64:
-          case VAR_INTS:
-          case FIXED_INTS_16:
-          case FIXED_INTS_32:
-          case FIXED_INTS_64:
-          case FIXED_INTS_8:
-            files.add(IndexFileNames.segmentFileName(filename, "",
-                DATA_EXTENSION));
-          try {
-            assert dir.fileExists(IndexFileNames.segmentFileName(filename, "",
-                DATA_EXTENSION));
-          } catch (IOException e) {
-            // don't throw checked exception - dir is only used in assert
-            throw new RuntimeException(e);
-          }
-            break;
-          default:
-            assert false;
-        }
-      }
-    }
-  }
-
   @Override
   public void abort() {
-    Set<String> files = new HashSet<String>();
-    files(directory, fieldInfos, segmentName, files);
-    IOUtils.deleteFilesIgnoringExceptions(directory, files.toArray(new String[0]));
+    // We don't have to remove files here: IndexFileDeleter
+    // will do so
   }
 }


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/sep/SepDocValuesProducer.java lucene4055/lucene/core/src/java/org/apache/lucene/codecs/sep/SepDocValuesProducer.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/sep/SepDocValuesProducer.java	2012-05-24 16:55:48.012233429 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/codecs/sep/SepDocValuesProducer.java	2012-05-23 14:42:53.858589954 -0400
@@ -45,7 +45,7 @@
    * {@link DocValues} instances for this segment and codec.
    */
   public SepDocValuesProducer(SegmentReadState state) throws IOException {
-    docValues = load(state.fieldInfos, state.segmentInfo.name, state.segmentInfo.docCount, state.dir, state.context);
+    docValues = load(state.fieldInfos, state.segmentInfo.name, state.segmentInfo.getDocCount(), state.dir, state.context);
   }
   
   @Override


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/sep/SepPostingsReader.java lucene4055/lucene/core/src/java/org/apache/lucene/codecs/sep/SepPostingsReader.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/sep/SepPostingsReader.java	2012-05-24 16:55:48.012233429 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/codecs/sep/SepPostingsReader.java	2012-05-24 12:01:34.931926010 -0400
@@ -18,14 +18,14 @@
  */
 
 import java.io.IOException;
-import java.util.Collection;
 
 import org.apache.lucene.codecs.BlockTermState;
 import org.apache.lucene.codecs.PostingsReaderBase;
 import org.apache.lucene.index.DocsAndPositionsEnum;
 import org.apache.lucene.index.DocsEnum;
-import org.apache.lucene.index.FieldInfo;
 import org.apache.lucene.index.FieldInfo.IndexOptions;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.FieldInfos;
 import org.apache.lucene.index.IndexFileNames;
 import org.apache.lucene.index.SegmentInfo;
 import org.apache.lucene.index.TermState;
@@ -60,7 +60,7 @@
   int maxSkipLevels;
   int skipMinimum;
 
-  public SepPostingsReader(Directory dir, SegmentInfo segmentInfo, IOContext context, IntStreamFactory intFactory, String segmentSuffix) throws IOException {
+  public SepPostingsReader(Directory dir, FieldInfos fieldInfos, SegmentInfo segmentInfo, IOContext context, IntStreamFactory intFactory, String segmentSuffix) throws IOException {
     boolean success = false;
     try {
 
@@ -69,12 +69,12 @@
 
       skipIn = dir.openInput(IndexFileNames.segmentFileName(segmentInfo.name, segmentSuffix, SepPostingsWriter.SKIP_EXTENSION), context);
 
-      if (segmentInfo.getFieldInfos().hasFreq()) {
+      if (fieldInfos.hasFreq()) {
         freqIn = intFactory.openInput(dir, IndexFileNames.segmentFileName(segmentInfo.name, segmentSuffix, SepPostingsWriter.FREQ_EXTENSION), context);        
       } else {
         freqIn = null;
       }
-      if (segmentInfo.getHasProx()) {
+      if (fieldInfos.hasProx()) {
         posIn = intFactory.openInput(dir, IndexFileNames.segmentFileName(segmentInfo.name, segmentSuffix, SepPostingsWriter.POS_EXTENSION), context);
         payloadIn = dir.openInput(IndexFileNames.segmentFileName(segmentInfo.name, segmentSuffix, SepPostingsWriter.PAYLOAD_EXTENSION), context);
       } else {
@@ -89,20 +89,6 @@
     }
   }
 
-  public static void files(SegmentInfo segmentInfo, String segmentSuffix, Collection<String> files) throws IOException {
-    files.add(IndexFileNames.segmentFileName(segmentInfo.name, segmentSuffix, SepPostingsWriter.DOC_EXTENSION));
-    files.add(IndexFileNames.segmentFileName(segmentInfo.name, segmentSuffix, SepPostingsWriter.SKIP_EXTENSION));
-
-    if (segmentInfo.getFieldInfos().hasFreq()) {
-      files.add(IndexFileNames.segmentFileName(segmentInfo.name, segmentSuffix, SepPostingsWriter.FREQ_EXTENSION));
-    }
-
-    if (segmentInfo.getHasProx()) {
-      files.add(IndexFileNames.segmentFileName(segmentInfo.name, segmentSuffix, SepPostingsWriter.POS_EXTENSION));
-      files.add(IndexFileNames.segmentFileName(segmentInfo.name, segmentSuffix, SepPostingsWriter.PAYLOAD_EXTENSION));
-    }
-  }
-
   @Override
   public void init(IndexInput termsIn) throws IOException {
     // Make sure we are talking to the matching past writer
@@ -241,13 +227,13 @@
     //System.out.println("  docFreq=" + termState.docFreq);
     termState.docIndex.read(termState.bytesReader, isFirstTerm);
     //System.out.println("  docIndex=" + termState.docIndex);
-    if (fieldInfo.indexOptions != IndexOptions.DOCS_ONLY) {
+    if (fieldInfo.getIndexOptions() != IndexOptions.DOCS_ONLY) {
       termState.freqIndex.read(termState.bytesReader, isFirstTerm);
-      if (fieldInfo.indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) {
+      if (fieldInfo.getIndexOptions() == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) {
         //System.out.println("  freqIndex=" + termState.freqIndex);
         termState.posIndex.read(termState.bytesReader, isFirstTerm);
         //System.out.println("  posIndex=" + termState.posIndex);
-        if (fieldInfo.storePayloads) {
+        if (fieldInfo.hasPayloads()) {
           if (isFirstTerm) {
             termState.payloadFP = termState.bytesReader.readVLong();
           } else {
@@ -273,7 +259,7 @@
 
   @Override
   public DocsEnum docs(FieldInfo fieldInfo, BlockTermState _termState, Bits liveDocs, DocsEnum reuse, boolean needsFreqs) throws IOException {
-    if (needsFreqs && fieldInfo.indexOptions == IndexOptions.DOCS_ONLY) {
+    if (needsFreqs && fieldInfo.getIndexOptions() == IndexOptions.DOCS_ONLY) {
       return null;
     }
     final SepTermState termState = (SepTermState) _termState;
@@ -298,7 +284,7 @@
                                                DocsAndPositionsEnum reuse, boolean needsOffsets)
     throws IOException {
 
-    if (fieldInfo.indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) < 0) {
+    if (fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) < 0) {
       return null;
     }
 
@@ -306,7 +292,7 @@
       return null;
     }
 
-    assert fieldInfo.indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS;
+    assert fieldInfo.getIndexOptions() == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS;
     final SepTermState termState = (SepTermState) _termState;
     SepDocsAndPositionsEnum postingsEnum;
     if (reuse == null || !(reuse instanceof SepDocsAndPositionsEnum)) {
@@ -371,9 +357,9 @@
 
     SepDocsEnum init(FieldInfo fieldInfo, SepTermState termState, Bits liveDocs) throws IOException {
       this.liveDocs = liveDocs;
-      this.indexOptions = fieldInfo.indexOptions;
+      this.indexOptions = fieldInfo.getIndexOptions();
       omitTF = indexOptions == IndexOptions.DOCS_ONLY;
-      storePayloads = fieldInfo.storePayloads;
+      storePayloads = fieldInfo.hasPayloads();
 
       // TODO: can't we only do this if consumer
       // skipped consuming the previous docs?
@@ -536,7 +522,7 @@
 
     SepDocsAndPositionsEnum init(FieldInfo fieldInfo, SepTermState termState, Bits liveDocs) throws IOException {
       this.liveDocs = liveDocs;
-      storePayloads = fieldInfo.storePayloads;
+      storePayloads = fieldInfo.hasPayloads();
       //System.out.println("Sep D&P init");
 
       // TODO: can't we only do this if consumer


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/sep/SepPostingsWriter.java lucene4055/lucene/core/src/java/org/apache/lucene/codecs/sep/SepPostingsWriter.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/sep/SepPostingsWriter.java	2012-05-24 16:55:48.012233429 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/codecs/sep/SepPostingsWriter.java	2012-05-23 15:14:22.962622852 -0400
@@ -115,34 +115,34 @@
     try {
       this.skipInterval = skipInterval;
       this.skipMinimum = skipInterval; /* set to the same for now */
-      final String docFileName = IndexFileNames.segmentFileName(state.segmentName, state.segmentSuffix, DOC_EXTENSION);
+      final String docFileName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, DOC_EXTENSION);
       docOut = factory.createOutput(state.directory, docFileName, state.context);
       docIndex = docOut.index();
       
       if (state.fieldInfos.hasFreq()) {
-        final String frqFileName = IndexFileNames.segmentFileName(state.segmentName, state.segmentSuffix, FREQ_EXTENSION);
+        final String frqFileName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, FREQ_EXTENSION);
         freqOut = factory.createOutput(state.directory, frqFileName, state.context);
         freqIndex = freqOut.index();
       }
 
       if (state.fieldInfos.hasProx()) {      
-        final String posFileName = IndexFileNames.segmentFileName(state.segmentName, state.segmentSuffix, POS_EXTENSION);
+        final String posFileName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, POS_EXTENSION);
         posOut = factory.createOutput(state.directory, posFileName, state.context);
         posIndex = posOut.index();
         
         // TODO: -- only if at least one field stores payloads?
-        final String payloadFileName = IndexFileNames.segmentFileName(state.segmentName, state.segmentSuffix, PAYLOAD_EXTENSION);
+        final String payloadFileName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, PAYLOAD_EXTENSION);
         payloadOut = state.directory.createOutput(payloadFileName, state.context);
       }
       
-      final String skipFileName = IndexFileNames.segmentFileName(state.segmentName, state.segmentSuffix, SKIP_EXTENSION);
+      final String skipFileName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, SKIP_EXTENSION);
       skipOut = state.directory.createOutput(skipFileName, state.context);
       
-      totalNumDocs = state.numDocs;
+      totalNumDocs = state.segmentInfo.getDocCount();
       
       skipListWriter = new SepSkipListWriter(skipInterval,
           maxSkipLevels,
-          state.numDocs,
+          totalNumDocs,
           freqOut, docOut,
           posOut, payloadOut);
       
@@ -187,12 +187,12 @@
   @Override
   public void setField(FieldInfo fieldInfo) {
     this.fieldInfo = fieldInfo;
-    this.indexOptions = fieldInfo.indexOptions;
+    this.indexOptions = fieldInfo.getIndexOptions();
     if (indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0) {
       throw new UnsupportedOperationException("this codec cannot index offsets");
     }
     skipListWriter.setIndexOptions(indexOptions);
-    storePayloads = indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS && fieldInfo.storePayloads;
+    storePayloads = indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS && fieldInfo.hasPayloads();
   }
 
   /** Adds a new doc in this term.  If this returns null


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextCodec.java lucene4055/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextCodec.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextCodec.java	2012-05-24 16:55:47.940233426 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextCodec.java	2012-05-22 17:13:19.773242527 -0400
@@ -23,7 +23,7 @@
 import org.apache.lucene.codecs.LiveDocsFormat;
 import org.apache.lucene.codecs.NormsFormat;
 import org.apache.lucene.codecs.PostingsFormat;
-import org.apache.lucene.codecs.SegmentInfosFormat;
+import org.apache.lucene.codecs.SegmentInfoFormat;
 import org.apache.lucene.codecs.StoredFieldsFormat;
 import org.apache.lucene.codecs.TermVectorsFormat;
 
@@ -36,7 +36,7 @@
 public final class SimpleTextCodec extends Codec {
   private final PostingsFormat postings = new SimpleTextPostingsFormat();
   private final StoredFieldsFormat storedFields = new SimpleTextStoredFieldsFormat();
-  private final SegmentInfosFormat segmentInfos = new SimpleTextSegmentInfosFormat();
+  private final SegmentInfoFormat segmentInfos = new SimpleTextSegmentInfoFormat();
   private final FieldInfosFormat fieldInfosFormat = new SimpleTextFieldInfosFormat();
   private final TermVectorsFormat vectorsFormat = new SimpleTextTermVectorsFormat();
   // TODO: need a plain-text impl
@@ -75,7 +75,7 @@
   }
 
   @Override
-  public SegmentInfosFormat segmentInfosFormat() {
+  public SegmentInfoFormat segmentInfoFormat() {
     return segmentInfos;
   }
 


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextDocValuesConsumer.java lucene4055/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextDocValuesConsumer.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextDocValuesConsumer.java	2012-05-24 16:55:47.940233426 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextDocValuesConsumer.java	2012-05-21 13:58:03.047533879 -0400
@@ -286,7 +286,4 @@
   protected Type getType() {
     return type;
   }
-  
-  
-
-}
\ No newline at end of file
+}


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextDocValuesFormat.java lucene4055/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextDocValuesFormat.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextDocValuesFormat.java	2012-05-24 16:55:47.936233427 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextDocValuesFormat.java	2012-05-24 12:01:48.307926242 -0400
@@ -16,14 +16,13 @@
  * License for the specific language governing permissions and limitations under
  * the License.
  */
+
 import java.io.IOException;
-import java.util.Set;
 
 import org.apache.lucene.codecs.DocValuesFormat;
 import org.apache.lucene.codecs.PerDocConsumer;
 import org.apache.lucene.codecs.PerDocProducer;
 import org.apache.lucene.index.PerDocWriteState;
-import org.apache.lucene.index.SegmentInfo;
 import org.apache.lucene.index.SegmentReadState;
 import org.apache.lucene.util.BytesRef;
 
@@ -49,10 +48,4 @@
   static String docValuesId(String segmentsName, int fieldId) {
     return segmentsName + "_" + fieldId;
   }
-
-  @Override
-  public void files(SegmentInfo info, Set<String> files)
-      throws IOException {
-    SimpleTextPerDocConsumer.files(info, files, DOC_VALUES_SEG_SUFFIX);
-  }
 }


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextFieldInfosFormat.java lucene4055/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextFieldInfosFormat.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextFieldInfosFormat.java	2012-05-24 16:55:47.936233427 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextFieldInfosFormat.java	2012-05-24 12:01:55.831926369 -0400
@@ -18,12 +18,10 @@
  */
 
 import java.io.IOException;
-import java.util.Set;
 
 import org.apache.lucene.codecs.FieldInfosFormat;
 import org.apache.lucene.codecs.FieldInfosReader;
 import org.apache.lucene.codecs.FieldInfosWriter;
-import org.apache.lucene.index.SegmentInfo;
 
 /**
  * plaintext field infos format
@@ -44,9 +42,4 @@
   public FieldInfosWriter getFieldInfosWriter() throws IOException {
     return writer;
   }
-
-  @Override
-  public void files(SegmentInfo info, Set<String> files) throws IOException {
-    SimpleTextFieldInfosReader.files(info, files);
-  }
 }


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextFieldInfosReader.java lucene4055/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextFieldInfosReader.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextFieldInfosReader.java	2012-05-24 16:55:47.940233426 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextFieldInfosReader.java	2012-05-24 07:53:52.963667197 -0400
@@ -18,14 +18,15 @@
  */
 
 import java.io.IOException;
-import java.util.Set;
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.Map;
 
 import org.apache.lucene.codecs.FieldInfosReader;
 import org.apache.lucene.index.CorruptIndexException;
 import org.apache.lucene.index.FieldInfo;
 import org.apache.lucene.index.FieldInfos;
 import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.SegmentInfo;
 import org.apache.lucene.index.FieldInfo.IndexOptions;
 import org.apache.lucene.index.DocValues;
 import org.apache.lucene.store.Directory;
@@ -51,10 +52,6 @@
     IndexInput input = directory.openInput(fileName, iocontext);
     BytesRef scratch = new BytesRef();
     
-    boolean hasVectors = false;
-    boolean hasFreq = false;
-    boolean hasProx = false;
-    
     try {
       
       SimpleTextUtil.readLine(input, scratch);
@@ -97,25 +94,35 @@
         String dvType = readString(DOCVALUES.length, scratch);
         final DocValues.Type docValuesType = docValuesType(dvType);
         
-        
-        
         SimpleTextUtil.readLine(input, scratch);
         assert StringHelper.startsWith(scratch, INDEXOPTIONS);
         IndexOptions indexOptions = IndexOptions.valueOf(readString(INDEXOPTIONS.length, scratch));
+      
+        SimpleTextUtil.readLine(input, scratch);
+        assert StringHelper.startsWith(scratch, NUM_ATTS);
+        int numAtts = Integer.parseInt(readString(NUM_ATTS.length, scratch));
+        Map<String,String> atts = new HashMap<String,String>();
+
+        for (int j = 0; j < numAtts; j++) {
+          SimpleTextUtil.readLine(input, scratch);
+          assert StringHelper.startsWith(scratch, ATT_KEY);
+          String key = readString(ATT_KEY.length, scratch);
+        
+          SimpleTextUtil.readLine(input, scratch);
+          assert StringHelper.startsWith(scratch, ATT_VALUE);
+          String value = readString(ATT_VALUE.length, scratch);
+          atts.put(key, value);
+        }
 
-        hasVectors |= storeTermVector;
-        hasProx |= isIndexed && indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;
-        hasFreq |= isIndexed && indexOptions != IndexOptions.DOCS_ONLY;
-        
         infos[i] = new FieldInfo(name, isIndexed, fieldNumber, storeTermVector, 
-          omitNorms, storePayloads, indexOptions, docValuesType, normsType);
+          omitNorms, storePayloads, indexOptions, docValuesType, normsType, Collections.unmodifiableMap(atts));
       }
 
       if (input.getFilePointer() != input.length()) {
         throw new CorruptIndexException("did not read all bytes from file \"" + fileName + "\": read " + input.getFilePointer() + " vs size " + input.length() + " (resource: " + input + ")");
       }
       
-      return new FieldInfos(infos, hasFreq, hasProx, hasVectors);
+      return new FieldInfos(infos);
     } finally {
       input.close();
     }
@@ -132,8 +139,4 @@
   private String readString(int offset, BytesRef scratch) {
     return new String(scratch.bytes, scratch.offset+offset, scratch.length-offset, IOUtils.CHARSET_UTF_8);
   }
-  
-  public static void files(SegmentInfo info, Set<String> files) throws IOException {
-    files.add(IndexFileNames.segmentFileName(info.name, "", FIELD_INFOS_EXTENSION));
-  }
 }


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextFieldInfosWriter.java lucene4055/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextFieldInfosWriter.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextFieldInfosWriter.java	2012-05-24 16:55:47.940233426 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextFieldInfosWriter.java	2012-05-21 14:57:21.131595841 -0400
@@ -17,6 +17,7 @@
  * limitations under the License.
  */
 import java.io.IOException;
+import java.util.Map;
 
 import org.apache.lucene.codecs.FieldInfosWriter;
 import org.apache.lucene.index.DocValues;
@@ -52,6 +53,9 @@
   static final BytesRef NORMS_TYPE      =  new BytesRef("  norms type ");
   static final BytesRef DOCVALUES       =  new BytesRef("  doc values ");
   static final BytesRef INDEXOPTIONS    =  new BytesRef("  index options ");
+  static final BytesRef NUM_ATTS        =  new BytesRef("  attributes ");
+  final static BytesRef ATT_KEY         =  new BytesRef("    key ");
+  final static BytesRef ATT_VALUE       =  new BytesRef("    value ");
   
   @Override
   public void write(Directory directory, String segmentName, FieldInfos infos, IOContext context) throws IOException {
@@ -64,7 +68,7 @@
       SimpleTextUtil.writeNewline(out);
       
       for (FieldInfo fi : infos) {
-        assert fi.indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 || !fi.storePayloads;
+        assert fi.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 || !fi.hasPayloads();
 
         SimpleTextUtil.write(out, NAME);
         SimpleTextUtil.write(out, fi.name, scratch);
@@ -75,19 +79,19 @@
         SimpleTextUtil.writeNewline(out);
         
         SimpleTextUtil.write(out, ISINDEXED);
-        SimpleTextUtil.write(out, Boolean.toString(fi.isIndexed), scratch);
+        SimpleTextUtil.write(out, Boolean.toString(fi.isIndexed()), scratch);
         SimpleTextUtil.writeNewline(out);
         
         SimpleTextUtil.write(out, STORETV);
-        SimpleTextUtil.write(out, Boolean.toString(fi.storeTermVector), scratch);
+        SimpleTextUtil.write(out, Boolean.toString(fi.hasVectors()), scratch);
         SimpleTextUtil.writeNewline(out);
         
         SimpleTextUtil.write(out, PAYLOADS);
-        SimpleTextUtil.write(out, Boolean.toString(fi.storePayloads), scratch);
+        SimpleTextUtil.write(out, Boolean.toString(fi.hasPayloads()), scratch);
         SimpleTextUtil.writeNewline(out);
                
         SimpleTextUtil.write(out, NORMS);
-        SimpleTextUtil.write(out, Boolean.toString(!fi.omitNorms), scratch);
+        SimpleTextUtil.write(out, Boolean.toString(!fi.omitsNorms()), scratch);
         SimpleTextUtil.writeNewline(out);
         
         SimpleTextUtil.write(out, NORMS_TYPE);
@@ -99,8 +103,26 @@
         SimpleTextUtil.writeNewline(out);
         
         SimpleTextUtil.write(out, INDEXOPTIONS);
-        SimpleTextUtil.write(out, fi.indexOptions.toString(), scratch);
+        SimpleTextUtil.write(out, fi.getIndexOptions().toString(), scratch);
         SimpleTextUtil.writeNewline(out);
+        
+        Map<String,String> atts = fi.attributes();
+        int numAtts = atts == null ? 0 : atts.size();
+        SimpleTextUtil.write(out, NUM_ATTS);
+        SimpleTextUtil.write(out, Integer.toString(numAtts), scratch);
+        SimpleTextUtil.writeNewline(out);
+      
+        if (numAtts > 0) {
+          for (Map.Entry<String,String> entry : atts.entrySet()) {
+            SimpleTextUtil.write(out, ATT_KEY);
+            SimpleTextUtil.write(out, entry.getKey(), scratch);
+            SimpleTextUtil.writeNewline(out);
+          
+            SimpleTextUtil.write(out, ATT_VALUE);
+            SimpleTextUtil.write(out, entry.getValue(), scratch);
+            SimpleTextUtil.writeNewline(out);
+          }
+        }
       }
     } finally {
       out.close();


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextFieldsReader.java lucene4055/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextFieldsReader.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextFieldsReader.java	2012-05-24 16:55:47.936233427 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextFieldsReader.java	2012-05-21 15:34:12.259634346 -0400
@@ -514,7 +514,7 @@
 
     public SimpleTextTerms(String field, long termsStart) throws IOException {
       this.termsStart = termsStart;
-      indexOptions = fieldInfos.fieldInfo(field).indexOptions;
+      indexOptions = fieldInfos.fieldInfo(field).getIndexOptions();
       loadTerms();
     }
 


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextFieldsWriter.java lucene4055/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextFieldsWriter.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextFieldsWriter.java	2012-05-24 16:55:47.940233426 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextFieldsWriter.java	2012-05-22 18:37:03.917330020 -0400
@@ -46,7 +46,7 @@
   final static BytesRef PAYLOAD      = new BytesRef("        payload ");
 
   public SimpleTextFieldsWriter(SegmentWriteState state) throws IOException {
-    final String fileName = SimpleTextPostingsFormat.getPostingsFileName(state.segmentName, state.segmentSuffix);
+    final String fileName = SimpleTextPostingsFormat.getPostingsFileName(state.segmentInfo.name, state.segmentSuffix);
     out = state.directory.createOutput(fileName, state.context);
   }
 
@@ -107,7 +107,7 @@
     private int lastEndOffset = -1;
 
     public SimpleTextPostingsWriter(FieldInfo field) {
-      this.indexOptions = field.indexOptions;
+      this.indexOptions = field.getIndexOptions();
       writePositions = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;
       writeOffsets = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;
       //System.out.println("writeOffsets=" + writeOffsets);


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextLiveDocsFormat.java lucene4055/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextLiveDocsFormat.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextLiveDocsFormat.java	2012-05-24 16:55:47.936233427 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextLiveDocsFormat.java	2012-05-24 12:02:02.507926490 -0400
@@ -19,11 +19,11 @@
 
 import java.io.IOException;
 import java.util.BitSet;
-import java.util.Set;
+import java.util.Collection;
 
 import org.apache.lucene.codecs.LiveDocsFormat;
 import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.index.SegmentInfoPerCommit;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.IOContext;
 import org.apache.lucene.store.IndexInput;
@@ -63,12 +63,12 @@
   }
 
   @Override
-  public Bits readLiveDocs(Directory dir, SegmentInfo info, IOContext context) throws IOException {
+  public Bits readLiveDocs(Directory dir, SegmentInfoPerCommit info, IOContext context) throws IOException {
     assert info.hasDeletions();
     BytesRef scratch = new BytesRef();
     CharsRef scratchUTF16 = new CharsRef();
     
-    String fileName = IndexFileNames.fileNameFromGeneration(info.name, LIVEDOCS_EXTENSION, info.getDelGen());
+    String fileName = IndexFileNames.fileNameFromGeneration(info.info.name, LIVEDOCS_EXTENSION, info.getDelGen());
     IndexInput in = null;
     boolean success = false;
     try {
@@ -105,12 +105,12 @@
   }
 
   @Override
-  public void writeLiveDocs(MutableBits bits, Directory dir, SegmentInfo info, IOContext context) throws IOException {
+  public void writeLiveDocs(MutableBits bits, Directory dir, SegmentInfoPerCommit info, int newDelCount, IOContext context) throws IOException {
     BitSet set = ((SimpleTextBits) bits).bits;
     int size = bits.length();
     BytesRef scratch = new BytesRef();
     
-    String fileName = IndexFileNames.fileNameFromGeneration(info.name, LIVEDOCS_EXTENSION, info.getDelGen());
+    String fileName = IndexFileNames.fileNameFromGeneration(info.info.name, LIVEDOCS_EXTENSION, info.getNextDelGen());
     IndexOutput out = null;
     boolean success = false;
     try {
@@ -138,9 +138,9 @@
   }
 
   @Override
-  public void files(SegmentInfo info, Set<String> files) throws IOException {
+  public void files(SegmentInfoPerCommit info, Collection<String> files) throws IOException {
     if (info.hasDeletions()) {
-      files.add(IndexFileNames.fileNameFromGeneration(info.name, LIVEDOCS_EXTENSION, info.getDelGen()));
+      files.add(IndexFileNames.fileNameFromGeneration(info.info.name, LIVEDOCS_EXTENSION, info.getDelGen()));
     }
   }
   


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextNormsFormat.java lucene4055/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextNormsFormat.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextNormsFormat.java	2012-05-24 16:55:47.940233426 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextNormsFormat.java	2012-05-24 12:02:15.735926718 -0400
@@ -19,8 +19,6 @@
 
 import java.io.IOException;
 import java.util.Comparator;
-import java.util.HashSet;
-import java.util.Set;
 
 import org.apache.lucene.codecs.NormsFormat;
 import org.apache.lucene.codecs.PerDocConsumer;
@@ -30,12 +28,9 @@
 import org.apache.lucene.index.DocValues.Type;
 import org.apache.lucene.index.FieldInfo;
 import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.IndexFileNames;
 import org.apache.lucene.index.PerDocWriteState;
-import org.apache.lucene.index.SegmentInfo;
 import org.apache.lucene.index.SegmentReadState;
 import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IOUtils;
 
 /**
  * plain-text norms format.
@@ -49,18 +44,13 @@
   
   @Override
   public PerDocConsumer docsConsumer(PerDocWriteState state) throws IOException {
-    return new SimpleTextNormsPerDocConsumer(state, NORMS_SEG_SUFFIX);
+    return new SimpleTextNormsPerDocConsumer(state);
   }
   
   @Override
   public PerDocProducer docsProducer(SegmentReadState state) throws IOException {
     return new SimpleTextNormsPerDocProducer(state,
-        BytesRef.getUTF8SortedAsUnicodeComparator(), NORMS_SEG_SUFFIX);
-  }
-  
-  @Override
-  public void files(SegmentInfo info, Set<String> files) throws IOException {
-    SimpleTextNormsPerDocConsumer.files(info, files);
+        BytesRef.getUTF8SortedAsUnicodeComparator());
   }
   
   /**
@@ -74,8 +64,8 @@
       SimpleTextPerDocProducer {
     
     public SimpleTextNormsPerDocProducer(SegmentReadState state,
-        Comparator<BytesRef> comp, String segmentSuffix) throws IOException {
-      super(state, comp, segmentSuffix);
+        Comparator<BytesRef> comp) throws IOException {
+      super(state, comp, NORMS_SEG_SUFFIX);
     }
     
     @Override
@@ -105,9 +95,9 @@
   public static class SimpleTextNormsPerDocConsumer extends
       SimpleTextPerDocConsumer {
     
-    public SimpleTextNormsPerDocConsumer(PerDocWriteState state,
-        String segmentSuffix) throws IOException {
-      super(state, segmentSuffix);
+    public SimpleTextNormsPerDocConsumer(PerDocWriteState state)
+      throws IOException {
+      super(state, NORMS_SEG_SUFFIX);
     }
     
     @Override
@@ -128,27 +118,8 @@
     
     @Override
     public void abort() {
-      Set<String> files = new HashSet<String>();
-      filesInternal(state.fieldInfos, state.segmentName, files, segmentSuffix);
-      IOUtils.deleteFilesIgnoringExceptions(state.directory,
-          files.toArray(new String[0]));
-    }
-    
-    public static void files(SegmentInfo segmentInfo, Set<String> files)
-        throws IOException {
-      filesInternal(segmentInfo.getFieldInfos(), segmentInfo.name, files,
-          NORMS_SEG_SUFFIX);
-    }
-    
-    public static void filesInternal(FieldInfos fieldInfos, String segmentName,
-        Set<String> files, String segmentSuffix) {
-      for (FieldInfo fieldInfo : fieldInfos) {
-        if (fieldInfo.hasNorms()) {
-          String id = docValuesId(segmentName, fieldInfo.number);
-          files.add(IndexFileNames.segmentFileName(id, "",
-              segmentSuffix));
-        }
-      }
+      // We don't have to remove files here: IndexFileDeleter
+      // will do so
     }
   }
 }


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextPerDocConsumer.java lucene4055/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextPerDocConsumer.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextPerDocConsumer.java	2012-05-24 16:55:47.936233427 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextPerDocConsumer.java	2012-05-24 17:37:47.716277308 -0400
@@ -1,4 +1,5 @@
 package org.apache.lucene.codecs.simpletext;
+
 /**
  * Licensed to the Apache Software Foundation (ASF) under one or more
  * contributor license agreements. See the NOTICE file distributed with this
@@ -15,20 +16,14 @@
  * License for the specific language governing permissions and limitations under
  * the License.
  */
+
 import java.io.IOException;
-import java.util.HashSet;
-import java.util.Set;
 
 import org.apache.lucene.codecs.DocValuesConsumer;
 import org.apache.lucene.codecs.PerDocConsumer;
 import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.IndexFileNames;
 import org.apache.lucene.index.PerDocWriteState;
-import org.apache.lucene.index.SegmentInfo;
 import org.apache.lucene.index.DocValues.Type;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.util.IOUtils;
 
 /**
  * @lucene.experimental
@@ -51,44 +46,17 @@
   @Override
   public DocValuesConsumer addValuesField(Type type, FieldInfo field)
       throws IOException {
-    return new SimpleTextDocValuesConsumer(SimpleTextDocValuesFormat.docValuesId(state.segmentName,
+    return new SimpleTextDocValuesConsumer(SimpleTextDocValuesFormat.docValuesId(state.segmentInfo.name,
         field.number), state.directory, state.context, type, segmentSuffix);
   }
 
   @Override
   public void abort() {
-    Set<String> files = new HashSet<String>();
-    files(state.directory, state.fieldInfos, state.segmentName, files, segmentSuffix);
-    IOUtils.deleteFilesIgnoringExceptions(state.directory,
-        files.toArray(new String[0]));
-  }
-  
-  
-  static void files(SegmentInfo info, Set<String> files, String segmentSuffix) throws IOException {
-    files(info.dir, info.getFieldInfos(), info.name, files, segmentSuffix);
+    // We don't have to remove files here: IndexFileDeleter
+    // will do so
   }
   
   static String docValuesId(String segmentsName, int fieldId) {
     return segmentsName + "_" + fieldId;
   }
-
-  @SuppressWarnings("fallthrough")
-  private static void files(Directory dir, FieldInfos fieldInfos,
-      String segmentName, Set<String> files, String segmentSuffix) {
-    for (FieldInfo fieldInfo : fieldInfos) {
-      if (fieldInfo.hasDocValues()) {
-        String filename = docValuesId(segmentName, fieldInfo.number);
-        files.add(IndexFileNames.segmentFileName(filename, "",
-            segmentSuffix));
-        try {
-          assert dir.fileExists(IndexFileNames.segmentFileName(filename, "",
-              segmentSuffix));
-        } catch (IOException e) {
-          // don't throw checked exception - dir is only used in assert
-          throw new RuntimeException(e);
-        }
-      }
-    }
-  }
-
-}
\ No newline at end of file
+}


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextPerDocProducer.java lucene4055/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextPerDocProducer.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextPerDocProducer.java	2012-05-24 16:55:47.936233427 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextPerDocProducer.java	2012-05-23 14:42:53.854589954 -0400
@@ -68,7 +68,7 @@
     this.segmentSuffix = segmentSuffix;
     if (anyDocValuesFields(state.fieldInfos)) {
       docValues = load(state.fieldInfos, state.segmentInfo.name,
-          state.segmentInfo.docCount, state.dir, state.context);
+                       state.segmentInfo.getDocCount(), state.dir, state.context);
     } else {
       docValues = new TreeMap<String, DocValues>();
     }


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextPostingsFormat.java lucene4055/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextPostingsFormat.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextPostingsFormat.java	2012-05-24 16:55:47.940233426 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextPostingsFormat.java	2012-05-24 12:02:51.151927340 -0400
@@ -18,12 +18,10 @@
  */
 
 import java.io.IOException;
-import java.util.Set;
 
 import org.apache.lucene.codecs.FieldsConsumer;
 import org.apache.lucene.codecs.FieldsProducer;
 import org.apache.lucene.codecs.PostingsFormat;
-import org.apache.lucene.index.SegmentInfo;
 import org.apache.lucene.index.SegmentWriteState;
 import org.apache.lucene.index.SegmentReadState;
 import org.apache.lucene.index.IndexFileNames;
@@ -58,9 +56,4 @@
   static String getPostingsFileName(String segment, String segmentSuffix) {
     return IndexFileNames.segmentFileName(segment, segmentSuffix, POSTINGS_EXTENSION);
   }
-
-  @Override
-  public void files(SegmentInfo segmentInfo, String segmentSuffix, Set<String> files) throws IOException {
-    files.add(getPostingsFileName(segmentInfo.name, segmentSuffix));
-  }
 }


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextSegmentInfoFormat.java lucene4055/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextSegmentInfoFormat.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextSegmentInfoFormat.java	1969-12-31 19:00:00.000000000 -0500
+++ lucene4055/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextSegmentInfoFormat.java	2012-05-24 12:03:00.115927493 -0400
@@ -0,0 +1,45 @@
+package org.apache.lucene.codecs.simpletext;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.codecs.SegmentInfoFormat;
+import org.apache.lucene.codecs.SegmentInfoReader;
+import org.apache.lucene.codecs.SegmentInfoWriter;
+
+/**
+ * plain text segments file format.
+ * <p>
+ * <b><font color="red">FOR RECREATIONAL USE ONLY</font></B>
+ * @lucene.experimental
+ */
+public class SimpleTextSegmentInfoFormat extends SegmentInfoFormat {
+  private final SegmentInfoReader reader = new SimpleTextSegmentInfoReader();
+  private final SegmentInfoWriter writer = new SimpleTextSegmentInfoWriter();
+
+  public static final String SI_EXTENSION = "si";
+  
+  @Override
+  public SegmentInfoReader getSegmentInfosReader() {
+    return reader;
+  }
+
+  @Override
+  public SegmentInfoWriter getSegmentInfosWriter() {
+    return writer;
+  }
+}


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextSegmentInfoReader.java lucene4055/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextSegmentInfoReader.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextSegmentInfoReader.java	1969-12-31 19:00:00.000000000 -0500
+++ lucene4055/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextSegmentInfoReader.java	2012-05-24 09:59:07.067798054 -0400
@@ -0,0 +1,127 @@
+package org.apache.lucene.codecs.simpletext;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.Map;
+import java.util.Set;
+
+import org.apache.lucene.codecs.SegmentInfoReader;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util.StringHelper;
+
+import static org.apache.lucene.codecs.simpletext.SimpleTextSegmentInfoWriter.*;
+
+/**
+ * reads plaintext segments files
+ * <p>
+ * <b><font color="red">FOR RECREATIONAL USE ONLY</font></B>
+ * @lucene.experimental
+ */
+public class SimpleTextSegmentInfoReader extends SegmentInfoReader {
+
+  @Override
+  public SegmentInfo read(Directory directory, String segmentName, IOContext context) throws IOException {
+    BytesRef scratch = new BytesRef();
+    String segFileName = IndexFileNames.segmentFileName(segmentName, "", SimpleTextSegmentInfoFormat.SI_EXTENSION);
+    IndexInput input = directory.openInput(segFileName, context);
+    boolean success = false;
+    try {
+      SimpleTextUtil.readLine(input, scratch);
+      assert StringHelper.startsWith(scratch, SI_VERSION);
+      final String version = readString(SI_VERSION.length, scratch);
+    
+      SimpleTextUtil.readLine(input, scratch);
+      assert StringHelper.startsWith(scratch, SI_DOCCOUNT);
+      final int docCount = Integer.parseInt(readString(SI_DOCCOUNT.length, scratch));
+    
+      SimpleTextUtil.readLine(input, scratch);
+      assert StringHelper.startsWith(scratch, SI_USECOMPOUND);
+      final boolean isCompoundFile = Boolean.parseBoolean(readString(SI_USECOMPOUND.length, scratch));
+    
+      SimpleTextUtil.readLine(input, scratch);
+      assert StringHelper.startsWith(scratch, SI_NUM_DIAG);
+      int numDiag = Integer.parseInt(readString(SI_NUM_DIAG.length, scratch));
+      Map<String,String> diagnostics = new HashMap<String,String>();
+
+      for (int i = 0; i < numDiag; i++) {
+        SimpleTextUtil.readLine(input, scratch);
+        assert StringHelper.startsWith(scratch, SI_DIAG_KEY);
+        String key = readString(SI_DIAG_KEY.length, scratch);
+      
+        SimpleTextUtil.readLine(input, scratch);
+        assert StringHelper.startsWith(scratch, SI_DIAG_VALUE);
+        String value = readString(SI_DIAG_VALUE.length, scratch);
+        diagnostics.put(key, value);
+      }
+      
+      SimpleTextUtil.readLine(input, scratch);
+      assert StringHelper.startsWith(scratch, SI_NUM_ATTS);
+      int numAtts = Integer.parseInt(readString(SI_NUM_ATTS.length, scratch));
+      Map<String,String> attributes = new HashMap<String,String>();
+
+      for (int i = 0; i < numAtts; i++) {
+        SimpleTextUtil.readLine(input, scratch);
+        assert StringHelper.startsWith(scratch, SI_ATT_KEY);
+        String key = readString(SI_ATT_KEY.length, scratch);
+      
+        SimpleTextUtil.readLine(input, scratch);
+        assert StringHelper.startsWith(scratch, SI_ATT_VALUE);
+        String value = readString(SI_ATT_VALUE.length, scratch);
+        attributes.put(key, value);
+      }
+
+      SimpleTextUtil.readLine(input, scratch);
+      assert StringHelper.startsWith(scratch, SI_NUM_FILES);
+      int numFiles = Integer.parseInt(readString(SI_NUM_FILES.length, scratch));
+      Set<String> files = new HashSet<String>();
+
+      for (int i = 0; i < numFiles; i++) {
+        SimpleTextUtil.readLine(input, scratch);
+        assert StringHelper.startsWith(scratch, SI_FILE);
+        String fileName = readString(SI_FILE.length, scratch);
+        files.add(fileName);
+      }
+
+      SegmentInfo info = new SegmentInfo(directory, version, segmentName, docCount, 
+                                         isCompoundFile, null, diagnostics, Collections.unmodifiableMap(attributes));
+      info.setFiles(files);
+      success = true;
+      return info;
+    } finally {
+      if (!success) {
+        IOUtils.closeWhileHandlingException(input);
+      } else {
+        input.close();
+      }
+    }
+  }
+
+  private String readString(int offset, BytesRef scratch) {
+    return new String(scratch.bytes, scratch.offset+offset, scratch.length-offset, IOUtils.CHARSET_UTF_8);
+  }
+}


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextSegmentInfosFormat.java lucene4055/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextSegmentInfosFormat.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextSegmentInfosFormat.java	2012-05-24 16:55:47.940233426 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextSegmentInfosFormat.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,43 +0,0 @@
-package org.apache.lucene.codecs.simpletext;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.codecs.SegmentInfosFormat;
-import org.apache.lucene.codecs.SegmentInfosReader;
-import org.apache.lucene.codecs.SegmentInfosWriter;
-
-/**
- * plain text segments file format.
- * <p>
- * <b><font color="red">FOR RECREATIONAL USE ONLY</font></B>
- * @lucene.experimental
- */
-public class SimpleTextSegmentInfosFormat extends SegmentInfosFormat {
-  private final SegmentInfosReader reader = new SimpleTextSegmentInfosReader();
-  private final SegmentInfosWriter writer = new SimpleTextSegmentInfosWriter();
-  
-  @Override
-  public SegmentInfosReader getSegmentInfosReader() {
-    return reader;
-  }
-
-  @Override
-  public SegmentInfosWriter getSegmentInfosWriter() {
-    return writer;
-  }
-}


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextSegmentInfosReader.java lucene4055/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextSegmentInfosReader.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextSegmentInfosReader.java	2012-05-24 16:55:47.936233427 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextSegmentInfosReader.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,190 +0,0 @@
-package org.apache.lucene.codecs.simpletext;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.HashMap;
-import java.util.Map;
-
-import org.apache.lucene.codecs.Codec;
-import org.apache.lucene.codecs.SegmentInfosReader;
-import org.apache.lucene.index.CorruptIndexException;
-import org.apache.lucene.index.SegmentInfo;
-import org.apache.lucene.index.SegmentInfos;
-import org.apache.lucene.store.ChecksumIndexInput;
-import org.apache.lucene.store.DataInput;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.StringHelper;
-
-import static org.apache.lucene.codecs.simpletext.SimpleTextSegmentInfosWriter.*;
-
-/**
- * reads plaintext segments files
- * <p>
- * <b><font color="red">FOR RECREATIONAL USE ONLY</font></B>
- * @lucene.experimental
- */
-public class SimpleTextSegmentInfosReader extends SegmentInfosReader {
-
-  @Override
-  public void read(Directory directory, String segmentsFileName, ChecksumIndexInput input, SegmentInfos infos, IOContext context) throws IOException {
-    final BytesRef scratch = new BytesRef();
-    
-    SimpleTextUtil.readLine(input, scratch);
-    assert StringHelper.startsWith(scratch, VERSION);
-    infos.version = Long.parseLong(readString(VERSION.length, scratch));
-    
-    SimpleTextUtil.readLine(input, scratch);
-    assert StringHelper.startsWith(scratch, COUNTER);
-    infos.counter = Integer.parseInt(readString(COUNTER.length, scratch));
-    
-    SimpleTextUtil.readLine(input, scratch);
-    assert StringHelper.startsWith(scratch, NUM_USERDATA);
-    int numUserData = Integer.parseInt(readString(NUM_USERDATA.length, scratch));
-    infos.userData = new HashMap<String,String>();
-
-    for (int i = 0; i < numUserData; i++) {
-      SimpleTextUtil.readLine(input, scratch);
-      assert StringHelper.startsWith(scratch, USERDATA_KEY);
-      String key = readString(USERDATA_KEY.length, scratch);
-      
-      SimpleTextUtil.readLine(input, scratch);
-      assert StringHelper.startsWith(scratch, USERDATA_VALUE);
-      String value = readString(USERDATA_VALUE.length, scratch);
-      infos.userData.put(key, value);
-    }
-    
-    SimpleTextUtil.readLine(input, scratch);
-    assert StringHelper.startsWith(scratch, NUM_SEGMENTS);
-    int numSegments = Integer.parseInt(readString(NUM_SEGMENTS.length, scratch));
-    
-    for (int i = 0; i < numSegments; i++) {
-      infos.add(readSegmentInfo(directory, input, scratch));
-    }
-  }
-  
-  public SegmentInfo readSegmentInfo(Directory directory, DataInput input, BytesRef scratch) throws IOException {
-    SimpleTextUtil.readLine(input, scratch);
-    assert StringHelper.startsWith(scratch, SI_NAME);
-    final String name = readString(SI_NAME.length, scratch);
-    
-    SimpleTextUtil.readLine(input, scratch);
-    assert StringHelper.startsWith(scratch, SI_CODEC);
-    final Codec codec = Codec.forName(readString(SI_CODEC.length, scratch));
-    
-    SimpleTextUtil.readLine(input, scratch);
-    assert StringHelper.startsWith(scratch, SI_VERSION);
-    final String version = readString(SI_VERSION.length, scratch);
-    
-    SimpleTextUtil.readLine(input, scratch);
-    assert StringHelper.startsWith(scratch, SI_DOCCOUNT);
-    final int docCount = Integer.parseInt(readString(SI_DOCCOUNT.length, scratch));
-    
-    SimpleTextUtil.readLine(input, scratch);
-    assert StringHelper.startsWith(scratch, SI_DELCOUNT);
-    final int delCount = Integer.parseInt(readString(SI_DELCOUNT.length, scratch));
-    
-    SimpleTextUtil.readLine(input, scratch);
-    assert StringHelper.startsWith(scratch, SI_HASPROX);
-    final int hasProx = readTernary(SI_HASPROX.length, scratch);
-    
-    SimpleTextUtil.readLine(input, scratch);
-    assert StringHelper.startsWith(scratch, SI_HASVECTORS);
-    final int hasVectors = readTernary(SI_HASVECTORS.length, scratch);
-    
-    SimpleTextUtil.readLine(input, scratch);
-    assert StringHelper.startsWith(scratch, SI_USECOMPOUND);
-    final boolean isCompoundFile = Boolean.parseBoolean(readString(SI_USECOMPOUND.length, scratch));
-    
-    SimpleTextUtil.readLine(input, scratch);
-    assert StringHelper.startsWith(scratch, SI_DSOFFSET);
-    final int dsOffset = Integer.parseInt(readString(SI_DSOFFSET.length, scratch));
-    
-    SimpleTextUtil.readLine(input, scratch);
-    assert StringHelper.startsWith(scratch, SI_DSSEGMENT);
-    final String dsSegment = readString(SI_DSSEGMENT.length, scratch);
-    
-    SimpleTextUtil.readLine(input, scratch);
-    assert StringHelper.startsWith(scratch, SI_DSCOMPOUND);
-    final boolean dsCompoundFile = Boolean.parseBoolean(readString(SI_DSCOMPOUND.length, scratch));
-    
-    SimpleTextUtil.readLine(input, scratch);
-    assert StringHelper.startsWith(scratch, SI_DELGEN);
-    final long delGen = Long.parseLong(readString(SI_DELGEN.length, scratch));
-    
-    SimpleTextUtil.readLine(input, scratch);
-    assert StringHelper.startsWith(scratch, SI_NUM_NORMGEN);
-    final int numNormGen = Integer.parseInt(readString(SI_NUM_NORMGEN.length, scratch));
-    final Map<Integer,Long> normGen;
-    if (numNormGen == 0) {
-      normGen = null;
-    } else {
-      normGen = new HashMap<Integer,Long>();
-      for (int i = 0; i < numNormGen; i++) {
-        SimpleTextUtil.readLine(input, scratch);
-        assert StringHelper.startsWith(scratch, SI_NORMGEN_KEY);
-        int key = Integer.parseInt(readString(SI_NORMGEN_KEY.length, scratch));
-        
-        SimpleTextUtil.readLine(input, scratch);
-        assert StringHelper.startsWith(scratch, SI_NORMGEN_VALUE);
-        long value = Long.parseLong(readString(SI_NORMGEN_VALUE.length, scratch));
-        normGen.put(key, value);
-      }
-    }
-    
-    SimpleTextUtil.readLine(input, scratch);
-    assert StringHelper.startsWith(scratch, SI_NUM_DIAG);
-    int numDiag = Integer.parseInt(readString(SI_NUM_DIAG.length, scratch));
-    Map<String,String> diagnostics = new HashMap<String,String>();
-
-    for (int i = 0; i < numDiag; i++) {
-      SimpleTextUtil.readLine(input, scratch);
-      assert StringHelper.startsWith(scratch, SI_DIAG_KEY);
-      String key = readString(SI_DIAG_KEY.length, scratch);
-      
-      SimpleTextUtil.readLine(input, scratch);
-      assert StringHelper.startsWith(scratch, SI_DIAG_VALUE);
-      String value = readString(SI_DIAG_VALUE.length, scratch);
-      diagnostics.put(key, value);
-    }
-    
-    return new SegmentInfo(directory, version, name, docCount, delGen, dsOffset,
-        dsSegment, dsCompoundFile, normGen, isCompoundFile,
-        delCount, hasProx, codec, diagnostics, hasVectors);
-  }
-  
-  private String readString(int offset, BytesRef scratch) {
-    return new String(scratch.bytes, scratch.offset+offset, scratch.length-offset, IOUtils.CHARSET_UTF_8);
-  }
-  
-  private int readTernary(int offset, BytesRef scratch) throws IOException {
-    String s = readString(offset, scratch);
-    if ("check fieldinfo".equals(s)) {
-      return SegmentInfo.CHECK_FIELDINFO;
-    } else if ("true".equals(s)) {
-      return SegmentInfo.YES;
-    } else if ("false".equals(s)) {
-      return 0;
-    } else {
-      throw new CorruptIndexException("invalid ternary value: " + s);
-    }
-  }
-}


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextSegmentInfosWriter.java lucene4055/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextSegmentInfosWriter.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextSegmentInfosWriter.java	2012-05-24 16:55:47.936233427 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextSegmentInfosWriter.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,234 +0,0 @@
-package org.apache.lucene.codecs.simpletext;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Map;
-import java.util.Map.Entry;
-
-import org.apache.lucene.codecs.SegmentInfosWriter;
-import org.apache.lucene.index.SegmentInfo;
-import org.apache.lucene.index.SegmentInfos;
-import org.apache.lucene.store.ChecksumIndexOutput;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.FlushInfo;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IOUtils;
-
-/**
- * writes plaintext segments files
- * <p>
- * <b><font color="red">FOR RECREATIONAL USE ONLY</font></B>
- * @lucene.experimental
- */
-public class SimpleTextSegmentInfosWriter extends SegmentInfosWriter {
-
-  final static BytesRef VERSION             = new BytesRef("version ");
-  final static BytesRef COUNTER             = new BytesRef("counter ");
-  final static BytesRef NUM_USERDATA        = new BytesRef("user data entries ");
-  final static BytesRef USERDATA_KEY        = new BytesRef("  key ");
-  final static BytesRef USERDATA_VALUE      = new BytesRef("  value ");
-  final static BytesRef NUM_SEGMENTS        = new BytesRef("number of segments ");
-  final static BytesRef SI_NAME             = new BytesRef("  name ");
-  final static BytesRef SI_CODEC            = new BytesRef("    codec ");
-  final static BytesRef SI_VERSION          = new BytesRef("    version ");
-  final static BytesRef SI_DOCCOUNT         = new BytesRef("    number of documents ");
-  final static BytesRef SI_DELCOUNT         = new BytesRef("    number of deletions ");
-  final static BytesRef SI_HASPROX          = new BytesRef("    has prox ");
-  final static BytesRef SI_HASVECTORS       = new BytesRef("    has vectors ");
-  final static BytesRef SI_USECOMPOUND      = new BytesRef("    uses compound file ");
-  final static BytesRef SI_DSOFFSET         = new BytesRef("    docstore offset ");
-  final static BytesRef SI_DSSEGMENT        = new BytesRef("    docstore segment ");
-  final static BytesRef SI_DSCOMPOUND       = new BytesRef("    docstore is compound file ");
-  final static BytesRef SI_DELGEN           = new BytesRef("    deletion generation ");
-  final static BytesRef SI_NUM_NORMGEN      = new BytesRef("    norms generations ");
-  final static BytesRef SI_NORMGEN_KEY      = new BytesRef("      key ");
-  final static BytesRef SI_NORMGEN_VALUE    = new BytesRef("      value ");
-  final static BytesRef SI_NUM_DIAG         = new BytesRef("    diagnostics ");
-  final static BytesRef SI_DIAG_KEY         = new BytesRef("      key ");
-  final static BytesRef SI_DIAG_VALUE       = new BytesRef("      value ");
-  
-  @Override
-  public IndexOutput writeInfos(Directory dir, String segmentsFileName, String codecID, SegmentInfos infos, IOContext context) throws IOException {
-    BytesRef scratch = new BytesRef();
-    IndexOutput out = new ChecksumIndexOutput(dir.createOutput(segmentsFileName, new IOContext(new FlushInfo(infos.size(), infos.totalDocCount()))));
-    boolean success = false;
-    try {
-      // required preamble:
-      out.writeInt(SegmentInfos.FORMAT_CURRENT); // write FORMAT
-      out.writeString(codecID); // write codecID
-      // end preamble
-      
-      // version
-      SimpleTextUtil.write(out, VERSION);
-      SimpleTextUtil.write(out, Long.toString(infos.version), scratch);
-      SimpleTextUtil.writeNewline(out);
-
-      // counter
-      SimpleTextUtil.write(out, COUNTER);
-      SimpleTextUtil.write(out, Integer.toString(infos.counter), scratch);
-      SimpleTextUtil.writeNewline(out);
-
-      // user data
-      int numUserDataEntries = infos.getUserData() == null ? 0 : infos.getUserData().size();
-      SimpleTextUtil.write(out, NUM_USERDATA);
-      SimpleTextUtil.write(out, Integer.toString(numUserDataEntries), scratch);
-      SimpleTextUtil.writeNewline(out);
-      
-      if (numUserDataEntries > 0) {
-        for (Map.Entry<String,String> userEntry : infos.getUserData().entrySet()) {
-          SimpleTextUtil.write(out, USERDATA_KEY);
-          SimpleTextUtil.write(out, userEntry.getKey(), scratch);
-          SimpleTextUtil.writeNewline(out);
-          
-          SimpleTextUtil.write(out, USERDATA_VALUE);
-          SimpleTextUtil.write(out, userEntry.getValue(), scratch);
-          SimpleTextUtil.writeNewline(out);
-        }
-      }
-      
-      // infos size
-      SimpleTextUtil.write(out, NUM_SEGMENTS);
-      SimpleTextUtil.write(out, Integer.toString(infos.size()), scratch);
-      SimpleTextUtil.writeNewline(out);
-
-      for (SegmentInfo si : infos) {
-        writeInfo(out, si);
-      } 
-
-      success = true;
-      return out;
-    } finally {
-      if (!success) {
-        IOUtils.closeWhileHandlingException(out);
-      }
-    }
-  }
-  
-  private void writeInfo(IndexOutput output, SegmentInfo si) throws IOException {
-    assert si.getDelCount() <= si.docCount: "delCount=" + si.getDelCount() + " docCount=" + si.docCount + " segment=" + si.name;
-    BytesRef scratch = new BytesRef();
-    
-    SimpleTextUtil.write(output, SI_NAME);
-    SimpleTextUtil.write(output, si.name, scratch);
-    SimpleTextUtil.writeNewline(output);
-    
-    SimpleTextUtil.write(output, SI_CODEC);
-    SimpleTextUtil.write(output, si.getCodec().getName(), scratch);
-    SimpleTextUtil.writeNewline(output);
-    
-    SimpleTextUtil.write(output, SI_VERSION);
-    SimpleTextUtil.write(output, si.getVersion(), scratch);
-    SimpleTextUtil.writeNewline(output);
-    
-    SimpleTextUtil.write(output, SI_DOCCOUNT);
-    SimpleTextUtil.write(output, Integer.toString(si.docCount), scratch);
-    SimpleTextUtil.writeNewline(output);
-    
-    SimpleTextUtil.write(output, SI_DELCOUNT);
-    SimpleTextUtil.write(output, Integer.toString(si.getDelCount()), scratch);
-    SimpleTextUtil.writeNewline(output);
-    
-    SimpleTextUtil.write(output, SI_HASPROX);
-    switch(si.getHasProxInternal()) {
-      case SegmentInfo.YES: SimpleTextUtil.write(output, "true", scratch); break;
-      case SegmentInfo.CHECK_FIELDINFO: SimpleTextUtil.write(output, "check fieldinfo", scratch); break;
-      // its "NO" if its 'anything but YES'... such as 0
-      default: SimpleTextUtil.write(output, "false", scratch); break;
-    }
-    SimpleTextUtil.writeNewline(output);
-    
-    SimpleTextUtil.write(output, SI_HASVECTORS);
-    switch(si.getHasVectorsInternal()) {
-      case SegmentInfo.YES: SimpleTextUtil.write(output, "true", scratch); break;
-      case SegmentInfo.CHECK_FIELDINFO: SimpleTextUtil.write(output, "check fieldinfo", scratch); break;
-      // its "NO" if its 'anything but YES'... such as 0
-      default: SimpleTextUtil.write(output, "false", scratch); break;
-    }
-    SimpleTextUtil.writeNewline(output);
-    
-    SimpleTextUtil.write(output, SI_USECOMPOUND);
-    SimpleTextUtil.write(output, Boolean.toString(si.getUseCompoundFile()), scratch);
-    SimpleTextUtil.writeNewline(output);
-    
-    SimpleTextUtil.write(output, SI_DSOFFSET);
-    SimpleTextUtil.write(output, Integer.toString(si.getDocStoreOffset()), scratch);
-    SimpleTextUtil.writeNewline(output);
-    
-    SimpleTextUtil.write(output, SI_DSSEGMENT);
-    SimpleTextUtil.write(output, si.getDocStoreSegment(), scratch);
-    SimpleTextUtil.writeNewline(output);
-    
-    SimpleTextUtil.write(output, SI_DSCOMPOUND);
-    SimpleTextUtil.write(output, Boolean.toString(si.getDocStoreIsCompoundFile()), scratch);
-    SimpleTextUtil.writeNewline(output);
-    
-    SimpleTextUtil.write(output, SI_DELGEN);
-    SimpleTextUtil.write(output, Long.toString(si.getDelGen()), scratch);
-    SimpleTextUtil.writeNewline(output);
-    
-    Map<Integer,Long> normGen = si.getNormGen();
-    int numNormGen = normGen == null ? 0 : normGen.size();
-    SimpleTextUtil.write(output, SI_NUM_NORMGEN);
-    SimpleTextUtil.write(output, Integer.toString(numNormGen), scratch);
-    SimpleTextUtil.writeNewline(output);
-    
-    if (numNormGen > 0) {
-      for (Entry<Integer,Long> entry : normGen.entrySet()) {
-        SimpleTextUtil.write(output, SI_NORMGEN_KEY);
-        SimpleTextUtil.write(output, Integer.toString(entry.getKey()), scratch);
-        SimpleTextUtil.writeNewline(output);
-        
-        SimpleTextUtil.write(output, SI_NORMGEN_VALUE);
-        SimpleTextUtil.write(output, Long.toString(entry.getValue()), scratch);
-        SimpleTextUtil.writeNewline(output);
-      }
-    }
-    
-    Map<String,String> diagnostics = si.getDiagnostics();
-    int numDiagnostics = diagnostics == null ? 0 : diagnostics.size();
-    SimpleTextUtil.write(output, SI_NUM_DIAG);
-    SimpleTextUtil.write(output, Integer.toString(numDiagnostics), scratch);
-    SimpleTextUtil.writeNewline(output);
-    
-    if (numDiagnostics > 0) {
-      for (Map.Entry<String,String> diagEntry : diagnostics.entrySet()) {
-        SimpleTextUtil.write(output, SI_DIAG_KEY);
-        SimpleTextUtil.write(output, diagEntry.getKey(), scratch);
-        SimpleTextUtil.writeNewline(output);
-        
-        SimpleTextUtil.write(output, SI_DIAG_VALUE);
-        SimpleTextUtil.write(output, diagEntry.getValue(), scratch);
-        SimpleTextUtil.writeNewline(output);
-      }
-    }
-  }
-
-  @Override
-  public void prepareCommit(IndexOutput out) throws IOException {
-    ((ChecksumIndexOutput)out).prepareCommit();
-  }
-
-  @Override
-  public void finishCommit(IndexOutput out) throws IOException {
-    ((ChecksumIndexOutput)out).finishCommit();
-    out.close();
-  }
-}


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextSegmentInfoWriter.java lucene4055/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextSegmentInfoWriter.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextSegmentInfoWriter.java	1969-12-31 19:00:00.000000000 -0500
+++ lucene4055/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextSegmentInfoWriter.java	2012-05-24 12:03:11.147927683 -0400
@@ -0,0 +1,136 @@
+package org.apache.lucene.codecs.simpletext;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Map;
+import java.util.Set;
+
+import org.apache.lucene.codecs.SegmentInfoWriter;
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IOUtils;
+
+/**
+ * writes plaintext segments files
+ * <p>
+ * <b><font color="red">FOR RECREATIONAL USE ONLY</font></B>
+ * @lucene.experimental
+ */
+public class SimpleTextSegmentInfoWriter extends SegmentInfoWriter {
+
+  final static BytesRef SI_VERSION          = new BytesRef("    version ");
+  final static BytesRef SI_DOCCOUNT         = new BytesRef("    number of documents ");
+  final static BytesRef SI_USECOMPOUND      = new BytesRef("    uses compound file ");
+  final static BytesRef SI_NUM_DIAG         = new BytesRef("    diagnostics ");
+  final static BytesRef SI_DIAG_KEY         = new BytesRef("      key ");
+  final static BytesRef SI_DIAG_VALUE       = new BytesRef("      value ");
+  final static BytesRef SI_NUM_ATTS         = new BytesRef("    attributes ");
+  final static BytesRef SI_ATT_KEY          = new BytesRef("      key ");
+  final static BytesRef SI_ATT_VALUE        = new BytesRef("      value ");
+  final static BytesRef SI_NUM_FILES        = new BytesRef("    files ");
+  final static BytesRef SI_FILE             = new BytesRef("      file ");
+  
+  @Override
+  public void write(Directory dir, SegmentInfo si, FieldInfos fis, IOContext ioContext) throws IOException {
+
+    String segFileName = IndexFileNames.segmentFileName(si.name, "", SimpleTextSegmentInfoFormat.SI_EXTENSION);
+    si.addFile(segFileName);
+
+    boolean success = false;
+    IndexOutput output = dir.createOutput(segFileName,  ioContext);
+
+    try {
+      BytesRef scratch = new BytesRef();
+    
+      SimpleTextUtil.write(output, SI_VERSION);
+      SimpleTextUtil.write(output, si.getVersion(), scratch);
+      SimpleTextUtil.writeNewline(output);
+    
+      SimpleTextUtil.write(output, SI_DOCCOUNT);
+      SimpleTextUtil.write(output, Integer.toString(si.getDocCount()), scratch);
+      SimpleTextUtil.writeNewline(output);
+    
+      SimpleTextUtil.write(output, SI_USECOMPOUND);
+      SimpleTextUtil.write(output, Boolean.toString(si.getUseCompoundFile()), scratch);
+      SimpleTextUtil.writeNewline(output);
+    
+      Map<String,String> diagnostics = si.getDiagnostics();
+      int numDiagnostics = diagnostics == null ? 0 : diagnostics.size();
+      SimpleTextUtil.write(output, SI_NUM_DIAG);
+      SimpleTextUtil.write(output, Integer.toString(numDiagnostics), scratch);
+      SimpleTextUtil.writeNewline(output);
+    
+      if (numDiagnostics > 0) {
+        for (Map.Entry<String,String> diagEntry : diagnostics.entrySet()) {
+          SimpleTextUtil.write(output, SI_DIAG_KEY);
+          SimpleTextUtil.write(output, diagEntry.getKey(), scratch);
+          SimpleTextUtil.writeNewline(output);
+        
+          SimpleTextUtil.write(output, SI_DIAG_VALUE);
+          SimpleTextUtil.write(output, diagEntry.getValue(), scratch);
+          SimpleTextUtil.writeNewline(output);
+        }
+      }
+      
+      Map<String,String> atts = si.attributes();
+      int numAtts = atts == null ? 0 : atts.size();
+      SimpleTextUtil.write(output, SI_NUM_ATTS);
+      SimpleTextUtil.write(output, Integer.toString(numAtts), scratch);
+      SimpleTextUtil.writeNewline(output);
+    
+      if (numAtts > 0) {
+        for (Map.Entry<String,String> entry : atts.entrySet()) {
+          SimpleTextUtil.write(output, SI_ATT_KEY);
+          SimpleTextUtil.write(output, entry.getKey(), scratch);
+          SimpleTextUtil.writeNewline(output);
+        
+          SimpleTextUtil.write(output, SI_ATT_VALUE);
+          SimpleTextUtil.write(output, entry.getValue(), scratch);
+          SimpleTextUtil.writeNewline(output);
+        }
+      }
+
+      Set<String> files = si.files();
+      int numFiles = files == null ? 0 : files.size();
+      SimpleTextUtil.write(output, SI_NUM_FILES);
+      SimpleTextUtil.write(output, Integer.toString(numFiles), scratch);
+      SimpleTextUtil.writeNewline(output);
+
+      if (numFiles > 0) {
+        for(String fileName : files) {
+          SimpleTextUtil.write(output, SI_FILE);
+          SimpleTextUtil.write(output, fileName, scratch);
+          SimpleTextUtil.writeNewline(output);
+        }
+      }
+      success = true;
+    } finally {
+      if (!success) {
+        IOUtils.closeWhileHandlingException(output);
+      } else {
+        output.close();
+      }
+    }
+  }
+}


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextStoredFieldsFormat.java lucene4055/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextStoredFieldsFormat.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextStoredFieldsFormat.java	2012-05-24 16:55:47.940233426 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextStoredFieldsFormat.java	2012-05-22 19:13:08.333367712 -0400
@@ -18,7 +18,6 @@
  */
 
 import java.io.IOException;
-import java.util.Set;
 
 import org.apache.lucene.codecs.StoredFieldsFormat;
 import org.apache.lucene.codecs.StoredFieldsReader;
@@ -37,17 +36,12 @@
 public class SimpleTextStoredFieldsFormat extends StoredFieldsFormat {
 
   @Override
-  public StoredFieldsReader fieldsReader(Directory directory, SegmentInfo si, FieldInfos fn, IOContext context) throws IOException {
+  public StoredFieldsReader fieldsReader(Directory directory, SegmentInfo si, FieldInfos fn, IOContext context) throws IOException {;
     return new SimpleTextStoredFieldsReader(directory, si, fn, context);
   }
 
   @Override
-  public StoredFieldsWriter fieldsWriter(Directory directory, String segment, IOContext context) throws IOException {
-    return new SimpleTextStoredFieldsWriter(directory, segment, context);
-  }
-
-  @Override
-  public void files(SegmentInfo info, Set<String> files) throws IOException {
-    SimpleTextStoredFieldsReader.files(info, files);
+  public StoredFieldsWriter fieldsWriter(Directory directory, SegmentInfo si, IOContext context) throws IOException {
+    return new SimpleTextStoredFieldsWriter(directory, si.name, context);
   }
 }


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextStoredFieldsReader.java lucene4055/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextStoredFieldsReader.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextStoredFieldsReader.java	2012-05-24 16:55:47.940233426 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextStoredFieldsReader.java	2012-05-24 12:03:17.579927798 -0400
@@ -19,7 +19,6 @@
 
 import java.io.IOException;
 import java.util.ArrayList;
-import java.util.Set;
 
 import org.apache.lucene.codecs.StoredFieldsReader;
 import org.apache.lucene.index.CorruptIndexException;
@@ -178,10 +177,6 @@
     }
   }
   
-  public static void files(SegmentInfo info, Set<String> files) throws IOException {
-    files.add(IndexFileNames.segmentFileName(info.name, "", SimpleTextStoredFieldsWriter.FIELDS_EXTENSION));
-  }
-  
   private void readLine() throws IOException {
     SimpleTextUtil.readLine(in, scratch);
   }


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextStoredFieldsWriter.java lucene4055/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextStoredFieldsWriter.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextStoredFieldsWriter.java	2012-05-24 16:55:47.936233427 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextStoredFieldsWriter.java	2012-05-22 18:37:03.917330020 -0400
@@ -21,6 +21,7 @@
 
 import org.apache.lucene.codecs.StoredFieldsWriter;
 import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.FieldInfos;
 import org.apache.lucene.index.IndexFileNames;
 import org.apache.lucene.index.IndexableField;
 import org.apache.lucene.store.Directory;
@@ -163,7 +164,7 @@
   }
 
   @Override
-  public void finish(int numDocs) throws IOException {
+  public void finish(FieldInfos fis, int numDocs) throws IOException {
     if (numDocsWritten != numDocs) {
       throw new RuntimeException("mergeFields produced an invalid result: docCount is " + numDocs 
           + " but only saw " + numDocsWritten + " file=" + out.toString() + "; now aborting this merge to prevent index corruption");


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextTermVectorsFormat.java lucene4055/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextTermVectorsFormat.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextTermVectorsFormat.java	2012-05-24 16:55:47.936233427 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextTermVectorsFormat.java	2012-05-24 12:03:23.239927897 -0400
@@ -18,7 +18,6 @@
  */
 
 import java.io.IOException;
-import java.util.Set;
 
 import org.apache.lucene.codecs.TermVectorsFormat;
 import org.apache.lucene.codecs.TermVectorsReader;
@@ -42,12 +41,7 @@
   }
 
   @Override
-  public TermVectorsWriter vectorsWriter(Directory directory, String segment, IOContext context) throws IOException {
-    return new SimpleTextTermVectorsWriter(directory, segment, context);
-  }
-
-  @Override
-  public void files(SegmentInfo info, Set<String> files) throws IOException {
-    SimpleTextTermVectorsReader.files(info, files);
+  public TermVectorsWriter vectorsWriter(Directory directory, SegmentInfo segmentInfo, IOContext context) throws IOException {
+    return new SimpleTextTermVectorsWriter(directory, segmentInfo.name, context);
   }
 }


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextTermVectorsReader.java lucene4055/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextTermVectorsReader.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextTermVectorsReader.java	2012-05-24 16:55:47.936233427 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextTermVectorsReader.java	2012-05-24 12:03:30.359928020 -0400
@@ -22,7 +22,6 @@
 import java.util.Comparator;
 import java.util.Iterator;
 import java.util.Map;
-import java.util.Set;
 import java.util.SortedMap;
 import java.util.TreeMap;
 
@@ -200,13 +199,7 @@
       offsets = null;
     }
   }
-  
-  public static void files(SegmentInfo info, Set<String> files) throws IOException {
-    if (info.getHasVectors()) {
-      files.add(IndexFileNames.segmentFileName(info.name, "", VECTORS_EXTENSION));
-    }
-  }
-  
+
   private void readLine() throws IOException {
     SimpleTextUtil.readLine(in, scratch);
   }


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextTermVectorsWriter.java lucene4055/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextTermVectorsWriter.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextTermVectorsWriter.java	2012-05-24 16:55:47.940233426 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextTermVectorsWriter.java	2012-05-22 19:20:18.853375209 -0400
@@ -22,6 +22,7 @@
 
 import org.apache.lucene.codecs.TermVectorsWriter;
 import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.FieldInfos;
 import org.apache.lucene.index.IndexFileNames;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.IOContext;
@@ -154,7 +155,7 @@
   }
 
   @Override
-  public void finish(int numDocs) throws IOException {
+  public void finish(FieldInfos fis, int numDocs) throws IOException {
     if (numDocsWritten != numDocs) {
       throw new RuntimeException("mergeVectors produced an invalid result: mergedDocs is " + numDocs + " but vec numDocs is " + numDocsWritten + " file=" + out.toString() + "; now aborting this merge to prevent index corruption");
     }


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/StoredFieldsFormat.java lucene4055/lucene/core/src/java/org/apache/lucene/codecs/StoredFieldsFormat.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/StoredFieldsFormat.java	2012-05-24 16:55:47.900233426 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/codecs/StoredFieldsFormat.java	2012-05-23 12:11:16.090431522 -0400
@@ -1,13 +1,5 @@
 package org.apache.lucene.codecs;
 
-import java.io.IOException;
-import java.util.Set;
-
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.SegmentInfo;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-
 /**
  * Licensed to the Apache Software Foundation (ASF) under one or more
  * contributor license agreements.  See the NOTICE file distributed with
@@ -25,11 +17,22 @@
  * limitations under the License.
  */
 
+import java.io.IOException;
+
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+
 /**
  * Controls the format of stored fields
  */
 public abstract class StoredFieldsFormat {
+  /** Returns a {@link StoredFieldsReader} to load stored
+   *  fields. */
   public abstract StoredFieldsReader fieldsReader(Directory directory, SegmentInfo si, FieldInfos fn, IOContext context) throws IOException;
-  public abstract StoredFieldsWriter fieldsWriter(Directory directory, String segment, IOContext context) throws IOException;
-  public abstract void files(SegmentInfo info, Set<String> files) throws IOException;
+
+  /** Returns a {@link StoredFieldsWriter} to write stored
+   *  fields. */
+  public abstract StoredFieldsWriter fieldsWriter(Directory directory, SegmentInfo si, IOContext context) throws IOException;
 }


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/StoredFieldsWriter.java lucene4055/lucene/core/src/java/org/apache/lucene/codecs/StoredFieldsWriter.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/StoredFieldsWriter.java	2012-05-24 16:55:47.948233428 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/codecs/StoredFieldsWriter.java	2012-05-24 12:11:50.823936736 -0400
@@ -1,15 +1,5 @@
 package org.apache.lucene.codecs;
 
-import java.io.Closeable;
-import java.io.IOException;
-
-import org.apache.lucene.document.Document;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.IndexableField;
-import org.apache.lucene.index.MergeState;
-import org.apache.lucene.util.Bits;
-
 /**
  * Copyright 2004 The Apache Software Foundation
  *
@@ -26,6 +16,16 @@
  * the License.
  */
 
+import java.io.Closeable;
+import java.io.IOException;
+
+import org.apache.lucene.document.Document;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.IndexableField;
+import org.apache.lucene.index.MergeState;
+import org.apache.lucene.util.Bits;
+
 /**
  * Codec API for writing stored fields:
  * <p>
@@ -34,7 +34,7 @@
  *       informing the Codec how many fields will be written.
  *   <li>{@link #writeField(FieldInfo, IndexableField)} is called for 
  *       each field in the document.
- *   <li>After all documents have been written, {@link #finish(int)} 
+ *   <li>After all documents have been written, {@link #finish(FieldInfos, int)} 
  *       is called for verification/sanity-checks.
  *   <li>Finally the writer is closed ({@link #close()})
  * </ol>
@@ -63,12 +63,12 @@
    *  calls to {@link #startDocument(int)}, but a Codec should
    *  check that this is the case to detect the JRE bug described 
    *  in LUCENE-1282. */
-  public abstract void finish(int numDocs) throws IOException;
+  public abstract void finish(FieldInfos fis, int numDocs) throws IOException;
   
   /** Merges in the stored fields from the readers in 
    *  <code>mergeState</code>. The default implementation skips
    *  over deleted documents, and uses {@link #startDocument(int)},
-   *  {@link #writeField(FieldInfo, IndexableField)}, and {@link #finish(int)},
+   *  {@link #writeField(FieldInfo, IndexableField)}, and {@link #finish(FieldInfos, int)},
    *  returning the number of documents that were written.
    *  Implementations can override this method for more sophisticated
    *  merging (bulk-byte copying, etc). */
@@ -94,7 +94,7 @@
         mergeState.checkAbort.work(300);
       }
     }
-    finish(docCount);
+    finish(mergeState.fieldInfos, docCount);
     return docCount;
   }
   


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/TermsConsumer.java lucene4055/lucene/core/src/java/org/apache/lucene/codecs/TermsConsumer.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/TermsConsumer.java	2012-05-24 16:55:47.996233427 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/codecs/TermsConsumer.java	2012-05-23 15:57:28.590667879 -0400
@@ -79,9 +79,10 @@
     long sumTotalTermFreq = 0;
     long sumDocFreq = 0;
     long sumDFsinceLastAbortCheck = 0;
-    FixedBitSet visitedDocs = new FixedBitSet(mergeState.mergedDocCount);
+    FixedBitSet visitedDocs = new FixedBitSet(mergeState.segmentInfo.getDocCount());
 
-    if (mergeState.fieldInfo.indexOptions == IndexOptions.DOCS_ONLY) {
+    IndexOptions indexOptions = mergeState.fieldInfo.getIndexOptions();
+    if (indexOptions == IndexOptions.DOCS_ONLY) {
       if (docsEnum == null) {
         docsEnum = new MappingMultiDocsEnum();
       }
@@ -109,7 +110,7 @@
           }
         }
       }
-    } else if (mergeState.fieldInfo.indexOptions == IndexOptions.DOCS_AND_FREQS) {
+    } else if (indexOptions == IndexOptions.DOCS_AND_FREQS) {
       if (docsAndFreqsEnum == null) {
         docsAndFreqsEnum = new MappingMultiDocsEnum();
       }
@@ -136,7 +137,7 @@
           }
         }
       }
-    } else if (mergeState.fieldInfo.indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) {
+    } else if (indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) {
       if (postingsEnum == null) {
         postingsEnum = new MappingMultiDocsAndPositionsEnum();
       }
@@ -170,7 +171,7 @@
         }
       }
     } else {
-      assert mergeState.fieldInfo.indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS;
+      assert indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS;
       if (postingsEnum == null) {
         postingsEnum = new MappingMultiDocsAndPositionsEnum();
       }


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/TermVectorsFormat.java lucene4055/lucene/core/src/java/org/apache/lucene/codecs/TermVectorsFormat.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/TermVectorsFormat.java	2012-05-24 16:55:47.960233428 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/codecs/TermVectorsFormat.java	2012-05-24 11:55:13.987919371 -0400
@@ -18,7 +18,6 @@
  */
 
 import java.io.IOException;
-import java.util.Set;
 
 import org.apache.lucene.index.FieldInfos;
 import org.apache.lucene.index.SegmentInfo;
@@ -29,7 +28,11 @@
  * Controls the format of term vectors
  */
 public abstract class TermVectorsFormat {
+  /** Returns a {@link TermVectorsReader} to read term
+   *  vectors. */
   public abstract TermVectorsReader vectorsReader(Directory directory, SegmentInfo segmentInfo, FieldInfos fieldInfos, IOContext context) throws IOException;
-  public abstract TermVectorsWriter vectorsWriter(Directory directory, String segment, IOContext context) throws IOException;
-  public abstract void files(SegmentInfo info, Set<String> files) throws IOException;
+
+  /** Returns a {@link TermVectorsWriter} to write term
+   *  vectors. */
+  public abstract TermVectorsWriter vectorsWriter(Directory directory, SegmentInfo segmentInfo, IOContext context) throws IOException;
 }


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/TermVectorsWriter.java lucene4055/lucene/core/src/java/org/apache/lucene/codecs/TermVectorsWriter.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/TermVectorsWriter.java	2012-05-24 16:55:48.028233429 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/codecs/TermVectorsWriter.java	2012-05-24 12:12:20.839937258 -0400
@@ -49,7 +49,7 @@
  *   <li>If offsets and/or positions are enabled, then 
  *       {@link #addPosition(int, int, int)} will be called for each term
  *       occurrence.
- *   <li>After all documents have been written, {@link #finish(int)} 
+ *   <li>After all documents have been written, {@link #finish(FieldInfos, int)} 
  *       is called for verification/sanity-checks.
  *   <li>Finally the writer is closed ({@link #close()})
  * </ol>
@@ -90,7 +90,7 @@
    *  calls to {@link #startDocument(int)}, but a Codec should
    *  check that this is the case to detect the JRE bug described 
    *  in LUCENE-1282. */
-  public abstract void finish(int numDocs) throws IOException;
+  public abstract void finish(FieldInfos fis, int numDocs) throws IOException;
   
   /** 
    * Called by IndexWriter when writing new segments.
@@ -137,7 +137,7 @@
    *  over deleted documents, and uses {@link #startDocument(int)},
    *  {@link #startField(FieldInfo, int, boolean, boolean)}, 
    *  {@link #startTerm(BytesRef, int)}, {@link #addPosition(int, int, int)},
-   *  and {@link #finish(int)},
+   *  and {@link #finish(FieldInfos, int)},
    *  returning the number of documents that were written.
    *  Implementations can override this method for more sophisticated
    *  merging (bulk-byte copying, etc). */
@@ -159,7 +159,7 @@
         mergeState.checkAbort.work(300);
       }
     }
-    finish(docCount);
+    finish(mergeState.fieldInfos, docCount);
     return docCount;
   }
   


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/VariableGapTermsIndexReader.java lucene4055/lucene/core/src/java/org/apache/lucene/codecs/VariableGapTermsIndexReader.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/VariableGapTermsIndexReader.java	2012-05-24 16:55:47.996233427 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/codecs/VariableGapTermsIndexReader.java	2012-05-24 11:55:25.751919581 -0400
@@ -21,13 +21,11 @@
 import java.io.FileOutputStream;   // for toDot
 import java.io.OutputStreamWriter; // for toDot
 import java.io.Writer;             // for toDot
-import java.util.Collection;
 import java.util.HashMap;
 
 import org.apache.lucene.index.FieldInfo;
 import org.apache.lucene.index.FieldInfos;
 import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.SegmentInfo;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.IOContext;
 import org.apache.lucene.store.IndexInput;
@@ -217,10 +215,6 @@
     }
   }
 
-  public static void files(SegmentInfo info, String segmentSuffix, Collection<String> files) {
-    files.add(IndexFileNames.segmentFileName(info.name, segmentSuffix, VariableGapTermsIndexWriter.TERMS_INDEX_EXTENSION));
-  }
-
   @Override
   public void close() throws IOException {
     if (in != null && !indexLoaded) {


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/VariableGapTermsIndexWriter.java lucene4055/lucene/core/src/java/org/apache/lucene/codecs/VariableGapTermsIndexWriter.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/codecs/VariableGapTermsIndexWriter.java	2012-05-24 16:55:48.032233429 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/codecs/VariableGapTermsIndexWriter.java	2012-05-22 18:37:03.933330020 -0400
@@ -173,7 +173,7 @@
   // in the extremes.
 
   public VariableGapTermsIndexWriter(SegmentWriteState state, IndexTermSelector policy) throws IOException {
-    final String indexFileName = IndexFileNames.segmentFileName(state.segmentName, state.segmentSuffix, TERMS_INDEX_EXTENSION);
+    final String indexFileName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, TERMS_INDEX_EXTENSION);
     out = state.directory.createOutput(indexFileName, state.context);
     boolean success = false;
     try {


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/document/DocumentStoredFieldVisitor.java lucene4055/lucene/core/src/java/org/apache/lucene/document/DocumentStoredFieldVisitor.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/document/DocumentStoredFieldVisitor.java	2012-05-24 16:55:48.688233441 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/document/DocumentStoredFieldVisitor.java	2012-05-21 13:58:03.747533891 -0400
@@ -63,11 +63,10 @@
   @Override
   public void stringField(FieldInfo fieldInfo, String value) throws IOException {
     final FieldType ft = new FieldType(TextField.TYPE_STORED);
-    ft.setStoreTermVectors(fieldInfo.storeTermVector);
-    ft.setStoreTermVectors(fieldInfo.storeTermVector);
-    ft.setIndexed(fieldInfo.isIndexed);
-    ft.setOmitNorms(fieldInfo.omitNorms);
-    ft.setIndexOptions(fieldInfo.indexOptions);
+    ft.setStoreTermVectors(fieldInfo.hasVectors());
+    ft.setIndexed(fieldInfo.isIndexed());
+    ft.setOmitNorms(fieldInfo.omitsNorms());
+    ft.setIndexOptions(fieldInfo.getIndexOptions());
     doc.add(new Field(fieldInfo.name, value, ft));
   }
 


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/index/AtomicReader.java lucene4055/lucene/core/src/java/org/apache/lucene/index/AtomicReader.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/index/AtomicReader.java	2012-05-24 16:55:48.484233437 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/index/AtomicReader.java	2012-05-21 13:58:03.511533887 -0400
@@ -194,9 +194,7 @@
 
   /**
    * Get the {@link FieldInfos} describing all fields in
-   * this reader.  NOTE: do not make any changes to the
-   * returned FieldInfos!
-   *
+   * this reader.
    * @lucene.experimental
    */
   public abstract FieldInfos getFieldInfos();


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/index/BufferedDeletesStream.java lucene4055/lucene/core/src/java/org/apache/lucene/index/BufferedDeletesStream.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/index/BufferedDeletesStream.java	2012-05-24 16:55:48.476233437 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/index/BufferedDeletesStream.java	2012-05-23 14:42:53.846589955 -0400
@@ -121,9 +121,9 @@
     public final long gen;
 
     // If non-null, contains segments that are 100% deleted
-    public final List<SegmentInfo> allDeleted;
+    public final List<SegmentInfoPerCommit> allDeleted;
 
-    ApplyDeletesResult(boolean anyDeletes, long gen, List<SegmentInfo> allDeleted) {
+    ApplyDeletesResult(boolean anyDeletes, long gen, List<SegmentInfoPerCommit> allDeleted) {
       this.anyDeletes = anyDeletes;
       this.gen = gen;
       this.allDeleted = allDeleted;
@@ -131,9 +131,9 @@
   }
 
   // Sorts SegmentInfos from smallest to biggest bufferedDelGen:
-  private static final Comparator<SegmentInfo> sortSegInfoByDelGen = new Comparator<SegmentInfo>() {
+  private static final Comparator<SegmentInfoPerCommit> sortSegInfoByDelGen = new Comparator<SegmentInfoPerCommit>() {
     @Override
-    public int compare(SegmentInfo si1, SegmentInfo si2) {
+    public int compare(SegmentInfoPerCommit si1, SegmentInfoPerCommit si2) {
       final long cmp = si1.getBufferedDeletesGen() - si2.getBufferedDeletesGen();
       if (cmp > 0) {
         return 1;
@@ -148,7 +148,7 @@
   /** Resolves the buffered deleted Term/Query/docIDs, into
    *  actual deleted docIDs in the liveDocs MutableBits for
    *  each SegmentReader. */
-  public synchronized ApplyDeletesResult applyDeletes(IndexWriter.ReaderPool readerPool, List<SegmentInfo> infos) throws IOException {
+  public synchronized ApplyDeletesResult applyDeletes(IndexWriter.ReaderPool readerPool, List<SegmentInfoPerCommit> infos) throws IOException {
     final long t0 = System.currentTimeMillis();
 
     if (infos.size() == 0) {
@@ -168,7 +168,7 @@
       infoStream.message("BD", "applyDeletes: infos=" + infos + " packetCount=" + deletes.size());
     }
 
-    List<SegmentInfo> infos2 = new ArrayList<SegmentInfo>();
+    List<SegmentInfoPerCommit> infos2 = new ArrayList<SegmentInfoPerCommit>();
     infos2.addAll(infos);
     Collections.sort(infos2, sortSegInfoByDelGen);
 
@@ -178,13 +178,13 @@
     int infosIDX = infos2.size()-1;
     int delIDX = deletes.size()-1;
 
-    List<SegmentInfo> allDeleted = null;
+    List<SegmentInfoPerCommit> allDeleted = null;
 
     while (infosIDX >= 0) {
       //System.out.println("BD: cycle delIDX=" + delIDX + " infoIDX=" + infosIDX);
 
       final FrozenBufferedDeletes packet = delIDX >= 0 ? deletes.get(delIDX) : null;
-      final SegmentInfo info = infos2.get(infosIDX);
+      final SegmentInfoPerCommit info = infos2.get(infosIDX);
       final long segGen = info.getBufferedDeletesGen();
 
       if (packet != null && segGen < packet.delGen()) {
@@ -225,8 +225,8 @@
           // already did that on flush:
           delCount += applyQueryDeletes(packet.queriesIterable(), rld, reader);
           final int fullDelCount = rld.info.getDelCount() + rld.getPendingDeleteCount();
-          assert fullDelCount <= rld.info.docCount;
-          segAllDeletes = fullDelCount == rld.info.docCount;
+          assert fullDelCount <= rld.info.info.getDocCount();
+          segAllDeletes = fullDelCount == rld.info.info.getDocCount();
         } finally {
           rld.release(reader);
           readerPool.release(rld);
@@ -235,7 +235,7 @@
 
         if (segAllDeletes) {
           if (allDeleted == null) {
-            allDeleted = new ArrayList<SegmentInfo>();
+            allDeleted = new ArrayList<SegmentInfoPerCommit>();
           }
           allDeleted.add(info);
         }
@@ -271,8 +271,8 @@
             delCount += applyTermDeletes(coalescedDeletes.termsIterable(), rld, reader);
             delCount += applyQueryDeletes(coalescedDeletes.queriesIterable(), rld, reader);
             final int fullDelCount = rld.info.getDelCount() + rld.getPendingDeleteCount();
-            assert fullDelCount <= rld.info.docCount;
-            segAllDeletes = fullDelCount == rld.info.docCount;
+            assert fullDelCount <= rld.info.info.getDocCount();
+            segAllDeletes = fullDelCount == rld.info.info.getDocCount();
           } finally {   
             rld.release(reader);
             readerPool.release(rld);
@@ -281,7 +281,7 @@
 
           if (segAllDeletes) {
             if (allDeleted == null) {
-              allDeleted = new ArrayList<SegmentInfo>();
+              allDeleted = new ArrayList<SegmentInfoPerCommit>();
             }
             allDeleted.add(info);
           }
@@ -316,7 +316,7 @@
   public synchronized void prune(SegmentInfos segmentInfos) {
     assert checkDeleteStats();
     long minGen = Long.MAX_VALUE;
-    for(SegmentInfo info : segmentInfos) {
+    for(SegmentInfoPerCommit info : segmentInfos) {
       minGen = Math.min(info.getBufferedDeletesGen(), minGen);
     }
 


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/index/CheckIndex.java lucene4055/lucene/core/src/java/org/apache/lucene/index/CheckIndex.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/index/CheckIndex.java	2012-05-24 16:55:48.488233437 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/index/CheckIndex.java	2012-05-23 21:57:45.775044331 -0400
@@ -88,9 +88,6 @@
     /** Number of segments in the index. */
     public int numSegments;
 
-    /** String description of the version of the index. */
-    public String segmentFormat;
-
     /** Empty unless you passed specific segments list to check as optional 3rd argument.
      *  @see CheckIndex#checkIndex(List) */
     public List<String> segmentsChecked = new ArrayList<String>();
@@ -185,11 +182,6 @@
       /** Number of fields in this segment. */
       int numFields;
 
-      /** True if at least one of the fields in this segment
-       *  has position data
-       *  @see FieldType#setIndexOptions(org.apache.lucene.index.FieldInfo.IndexOptions) */
-      public boolean hasProx;
-
       /** Map that includes certain
        *  debugging details that IndexWriter records into
        *  each segment it creates */
@@ -369,8 +361,8 @@
     String oldSegs = null;
     boolean foundNonNullVersion = false;
     Comparator<String> versionComparator = StringHelper.getVersionComparator();
-    for (SegmentInfo si : sis) {
-      String version = si.getVersion();
+    for (SegmentInfoPerCommit si : sis) {
+      String version = si.info.getVersion();
       if (version == null) {
         // pre-3.1 segment
         oldSegs = "pre-3.1";
@@ -415,27 +407,8 @@
     String sFormat = "";
     boolean skip = false;
 
-    if (format == SegmentInfos.FORMAT_DIAGNOSTICS) {
-      sFormat = "FORMAT_DIAGNOSTICS [Lucene 2.9]";
-    } else if (format == SegmentInfos.FORMAT_HAS_VECTORS) {
-      sFormat = "FORMAT_HAS_VECTORS [Lucene 3.1]";
-    } else if (format == SegmentInfos.FORMAT_3_1) {
-      sFormat = "FORMAT_3_1 [Lucene 3.1+]";
-    } else if (format == SegmentInfos.FORMAT_4_0) {
-      sFormat = "FORMAT_4_0 [Lucene 4.0]";
-    } else if (format == SegmentInfos.FORMAT_CURRENT) {
-      throw new RuntimeException("BUG: You should update this tool!");
-    } else if (format < SegmentInfos.FORMAT_CURRENT) {
-      sFormat = "int=" + format + " [newer version of Lucene than this tool supports]";
-      skip = true;
-    } else if (format > SegmentInfos.FORMAT_MINIMUM) {
-      sFormat = "int=" + format + " [older version of Lucene than this tool supports]";
-      skip = true;
-    }
-
     result.segmentsFileName = segmentsFileName;
     result.numSegments = numSegments;
-    result.segmentFormat = sFormat;
     result.userData = sis.getUserData();
     String userDataString;
     if (sis.getUserData().size() > 0) {
@@ -482,50 +455,41 @@
     result.maxSegmentName = -1;
 
     for(int i=0;i<numSegments;i++) {
-      final SegmentInfo info = sis.info(i);
-      int segmentName = Integer.parseInt(info.name.substring(1), Character.MAX_RADIX);
+      final SegmentInfoPerCommit info = sis.info(i);
+      int segmentName = Integer.parseInt(info.info.name.substring(1), Character.MAX_RADIX);
       if (segmentName > result.maxSegmentName) {
         result.maxSegmentName = segmentName;
       }
-      if (onlySegments != null && !onlySegments.contains(info.name))
+      if (onlySegments != null && !onlySegments.contains(info.info.name)) {
         continue;
+      }
       Status.SegmentInfoStatus segInfoStat = new Status.SegmentInfoStatus();
       result.segmentInfos.add(segInfoStat);
-      msg("  " + (1+i) + " of " + numSegments + ": name=" + info.name + " docCount=" + info.docCount);
-      segInfoStat.name = info.name;
-      segInfoStat.docCount = info.docCount;
+      msg("  " + (1+i) + " of " + numSegments + ": name=" + info.info.name + " docCount=" + info.info.getDocCount());
+      segInfoStat.name = info.info.name;
+      segInfoStat.docCount = info.info.getDocCount();
 
-      int toLoseDocCount = info.docCount;
+      int toLoseDocCount = info.info.getDocCount();
 
       SegmentReader reader = null;
 
       try {
-        final Codec codec = info.getCodec();
+        final Codec codec = info.info.getCodec();
         msg("    codec=" + codec);
         segInfoStat.codec = codec;
-        msg("    compound=" + info.getUseCompoundFile());
-        segInfoStat.compound = info.getUseCompoundFile();
-        msg("    hasProx=" + info.getHasProx());
-        segInfoStat.hasProx = info.getHasProx();
+        msg("    compound=" + info.info.getUseCompoundFile());
+        segInfoStat.compound = info.info.getUseCompoundFile();
         msg("    numFiles=" + info.files().size());
         segInfoStat.numFiles = info.files().size();
         segInfoStat.sizeMB = info.sizeInBytes()/(1024.*1024.);
         msg("    size (MB)=" + nf.format(segInfoStat.sizeMB));
-        Map<String,String> diagnostics = info.getDiagnostics();
+        Map<String,String> diagnostics = info.info.getDiagnostics();
         segInfoStat.diagnostics = diagnostics;
         if (diagnostics.size() > 0) {
           msg("    diagnostics = " + diagnostics);
         }
 
-        final int docStoreOffset = info.getDocStoreOffset();
-        if (docStoreOffset != -1) {
-          msg("    docStoreOffset=" + docStoreOffset);
-          segInfoStat.docStoreOffset = docStoreOffset;
-          msg("    docStoreSegment=" + info.getDocStoreSegment());
-          segInfoStat.docStoreSegment = info.getDocStoreSegment();
-          msg("    docStoreIsCompoundFile=" + info.getDocStoreIsCompoundFile());
-          segInfoStat.docStoreCompoundFile = info.getDocStoreIsCompoundFile();
-        }
+        // TODO: we could append the info attributes() to the msg?
 
         if (info.hasDeletions()) {
           msg("    no deletions");
@@ -545,14 +509,14 @@
         final int numDocs = reader.numDocs();
         toLoseDocCount = numDocs;
         if (reader.hasDeletions()) {
-          if (reader.numDocs() != info.docCount - info.getDelCount()) {
-            throw new RuntimeException("delete count mismatch: info=" + (info.docCount - info.getDelCount()) + " vs reader=" + reader.numDocs());
+          if (reader.numDocs() != info.info.getDocCount() - info.getDelCount()) {
+            throw new RuntimeException("delete count mismatch: info=" + (info.info.getDocCount() - info.getDelCount()) + " vs reader=" + reader.numDocs());
           }
-          if ((info.docCount-reader.numDocs()) > reader.maxDoc()) {
-            throw new RuntimeException("too many deleted docs: maxDoc()=" + reader.maxDoc() + " vs del count=" + (info.docCount-reader.numDocs()));
+          if ((info.info.getDocCount()-reader.numDocs()) > reader.maxDoc()) {
+            throw new RuntimeException("too many deleted docs: maxDoc()=" + reader.maxDoc() + " vs del count=" + (info.info.getDocCount()-reader.numDocs()));
           }
-          if (info.docCount - numDocs != info.getDelCount()) {
-            throw new RuntimeException("delete count mismatch: info=" + info.getDelCount() + " vs reader=" + (info.docCount - numDocs));
+          if (info.info.getDocCount() - numDocs != info.getDelCount()) {
+            throw new RuntimeException("delete count mismatch: info=" + info.getDelCount() + " vs reader=" + (info.info.getDocCount() - numDocs));
           }
           Bits liveDocs = reader.getLiveDocs();
           if (liveDocs == null) {
@@ -569,11 +533,11 @@
             }
           }
           
-          segInfoStat.numDeleted = info.docCount - numDocs;
+          segInfoStat.numDeleted = info.info.getDocCount() - numDocs;
           msg("OK [" + (segInfoStat.numDeleted) + " deleted docs]");
         } else {
           if (info.getDelCount() != 0) {
-            throw new RuntimeException("delete count mismatch: info=" + info.getDelCount() + " vs reader=" + (info.docCount - numDocs));
+            throw new RuntimeException("delete count mismatch: info=" + info.getDelCount() + " vs reader=" + (info.info.getDocCount() - numDocs));
           }
           Bits liveDocs = reader.getLiveDocs();
           if (liveDocs != null) {
@@ -586,8 +550,9 @@
           }
           msg("OK");
         }
-        if (reader.maxDoc() != info.docCount)
-          throw new RuntimeException("SegmentReader.maxDoc() " + reader.maxDoc() + " != SegmentInfos.docCount " + info.docCount);
+        if (reader.maxDoc() != info.info.getDocCount()) {
+          throw new RuntimeException("SegmentReader.maxDoc() " + reader.maxDoc() + " != SegmentInfos.docCount " + info.info.getDocCount());
+        }
 
         // Test getFieldInfos()
         if (infoStream != null) {
@@ -609,7 +574,7 @@
         // Test Term Vectors
         segInfoStat.termVectorStatus = testTermVectors(fieldInfos, info, reader, nf);
         
-        segInfoStat.docValuesStatus = testDocValues(info, reader);
+        segInfoStat.docValuesStatus = testDocValues(info, fieldInfos, reader);
 
         // Rethrow the first exception we encountered
         //  This will cause stats for failed segments to be incremented properly
@@ -706,8 +671,7 @@
    * checks Fields api is consistent with itself.
    * searcher is optional, to verify with queries. Can be null.
    */
-  // TODO: cutover term vectors to this!
-  private Status.TermIndexStatus checkFields(Fields fields, Bits liveDocs, int maxDoc, FieldInfos fieldInfos, IndexSearcher searcher) throws IOException {
+  private Status.TermIndexStatus checkFields(Fields fields, Bits liveDocs, int maxDoc, FieldInfos fieldInfos, IndexSearcher searcher, boolean doPrint) throws IOException {
     // TODO: we should probably return our own stats thing...?!
     
     final Status.TermIndexStatus status = new Status.TermIndexStatus();
@@ -741,7 +705,7 @@
       if (fi == null) {
         throw new RuntimeException("fieldsEnum inconsistent with fieldInfos, no fieldInfos for: " + field);
       }
-      if (!fi.isIndexed) {
+      if (!fi.isIndexed()) {
         throw new RuntimeException("fieldsEnum inconsistent with fieldInfos, isIndexed == false for: " + field);
       }
       
@@ -1131,8 +1095,10 @@
     if (status.termCount != uniqueTermCountAllFields) {
       throw new RuntimeException("termCount mismatch " + uniqueTermCountAllFields + " vs " + (status.termCount));
     }
-    
-    msg("OK [" + status.termCount + " terms; " + status.totFreq + " terms/docs pairs; " + status.totPos + " tokens]");
+
+    if (doPrint) {
+      msg("OK [" + status.termCount + " terms; " + status.totFreq + " terms/docs pairs; " + status.totPos + " tokens]");
+    }
     
     if (verbose && status.blockTreeStats != null && infoStream != null && status.termCount > 0) {
       for(Map.Entry<String,BlockTreeTermsReader.Stats> ent : status.blockTreeStats.entrySet()) {
@@ -1163,13 +1129,13 @@
       }
 
       final Fields fields = reader.fields();
-      status = checkFields(fields, liveDocs, maxDoc, fieldInfos, is);
+      status = checkFields(fields, liveDocs, maxDoc, fieldInfos, is, true);
       if (liveDocs != null) {
         if (infoStream != null) {
           infoStream.print("    test (ignoring deletes): terms, freq, prox...");
         }
         // TODO: can we make a IS that ignores all deletes?
-        checkFields(fields, null, maxDoc, fieldInfos, null);
+        checkFields(fields, null, maxDoc, fieldInfos, null, true);
       }
     } catch (Throwable e) {
       msg("ERROR: " + e);
@@ -1186,7 +1152,7 @@
   /**
    * Test stored fields for a segment.
    */
-  private Status.StoredFieldStatus testStoredFields(SegmentInfo info, SegmentReader reader, NumberFormat format) {
+  private Status.StoredFieldStatus testStoredFields(SegmentInfoPerCommit info, SegmentReader reader, NumberFormat format) {
     final Status.StoredFieldStatus status = new Status.StoredFieldStatus();
 
     try {
@@ -1196,7 +1162,7 @@
 
       // Scan stored fields for all documents
       final Bits liveDocs = reader.getLiveDocs();
-      for (int j = 0; j < info.docCount; ++j) {
+      for (int j = 0; j < info.info.getDocCount(); ++j) {
         // Intentionally pull even deleted documents to
         // make sure they too are not corrupt:
         Document doc = reader.document(j);
@@ -1310,14 +1276,14 @@
     }
   }
   
-  private Status.DocValuesStatus testDocValues(SegmentInfo info,
-      SegmentReader reader) {
+  private Status.DocValuesStatus testDocValues(SegmentInfoPerCommit info,
+                                               FieldInfos fieldInfos,
+                                               SegmentReader reader) {
     final Status.DocValuesStatus status = new Status.DocValuesStatus();
     try {
       if (infoStream != null) {
         infoStream.print("    test: DocValues........");
       }
-      final FieldInfos fieldInfos = info.getFieldInfos();
       for (FieldInfo fieldInfo : fieldInfos) {
         if (fieldInfo.hasDocValues()) {
           status.totalValueFields++;
@@ -1345,7 +1311,7 @@
   /**
    * Test term vectors for a segment.
    */
-  private Status.TermVectorStatus testTermVectors(FieldInfos fieldInfos, SegmentInfo info, SegmentReader reader, NumberFormat format) {
+  private Status.TermVectorStatus testTermVectors(FieldInfos fieldInfos, SegmentInfoPerCommit info, SegmentReader reader, NumberFormat format) {
     final Status.TermVectorStatus status = new Status.TermVectorStatus();
 
     final Bits onlyDocIsDeleted = new FixedBitSet(1);
@@ -1375,7 +1341,7 @@
       TermsEnum termsEnum = null;
       TermsEnum postingsTermsEnum = null;
 
-      for (int j = 0; j < info.docCount; ++j) {
+      for (int j = 0; j < info.info.getDocCount(); ++j) {
         // Intentionally pull/visit (but don't count in
         // stats) deleted documents to make sure they too
         // are not corrupt:
@@ -1386,10 +1352,10 @@
 
         if (tfv != null) {
           // First run with no deletions:
-          checkFields(tfv, null, 1, fieldInfos, null);
+          checkFields(tfv, null, 1, fieldInfos, null, false);
 
           // Again, with the one doc deleted:
-          checkFields(tfv, onlyDocIsDeleted, 1, fieldInfos, null);
+          checkFields(tfv, onlyDocIsDeleted, 1, fieldInfos, null, false);
 
           // Only agg stats if the doc is live:
           final boolean doStats = liveDocs == null || liveDocs.get(j);
@@ -1406,7 +1372,7 @@
 
             // Make sure FieldInfo thinks this field is vector'd:
             final FieldInfo fieldInfo = fieldInfos.fieldInfo(field);
-            if (!fieldInfo.storeTermVector) {
+            if (!fieldInfo.hasVectors()) {
               throw new RuntimeException("docID=" + j + " has term vectors for field=" + field + " but FieldInfo has storeTermVector=false");
             }
 
@@ -1594,7 +1560,7 @@
     if (result.partial)
       throw new IllegalArgumentException("can only fix an index that was fully checked (this status checked a subset of segments)");
     result.newSegments.changed();
-    result.newSegments.commit(result.dir, codec);
+    result.newSegments.commit(result.dir);
   }
 
   private static boolean assertsOn;


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/index/DocConsumer.java lucene4055/lucene/core/src/java/org/apache/lucene/index/DocConsumer.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/index/DocConsumer.java	2012-05-24 16:55:48.488233437 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/index/DocConsumer.java	2012-05-21 13:58:03.519533887 -0400
@@ -20,7 +20,7 @@
 import java.io.IOException;
 
 abstract class DocConsumer {
-  abstract void processDocument(FieldInfos fieldInfos) throws IOException;
+  abstract void processDocument(FieldInfos.Builder fieldInfos) throws IOException;
   abstract void finishDocument() throws IOException;
   abstract void flush(final SegmentWriteState state) throws IOException;
   abstract void abort();


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/index/DocFieldConsumer.java lucene4055/lucene/core/src/java/org/apache/lucene/index/DocFieldConsumer.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/index/DocFieldConsumer.java	2012-05-24 16:55:48.476233437 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/index/DocFieldConsumer.java	2012-05-21 13:58:03.503533886 -0400
@@ -23,7 +23,7 @@
 abstract class DocFieldConsumer {
   /** Called when DocumentsWriterPerThread decides to create a new
    *  segment */
-  abstract void flush(Map<FieldInfo, DocFieldConsumerPerField> fieldsToFlush, SegmentWriteState state) throws IOException;
+  abstract void flush(Map<String, DocFieldConsumerPerField> fieldsToFlush, SegmentWriteState state) throws IOException;
 
   /** Called when an aborting exception is hit */
   abstract void abort();


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/index/DocFieldProcessor.java lucene4055/lucene/core/src/java/org/apache/lucene/index/DocFieldProcessor.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/index/DocFieldProcessor.java	2012-05-24 16:55:48.472233437 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/index/DocFieldProcessor.java	2012-05-23 15:14:40.826623164 -0400
@@ -71,28 +71,28 @@
   @Override
   public void flush(SegmentWriteState state) throws IOException {
 
-    Map<FieldInfo, DocFieldConsumerPerField> childFields = new HashMap<FieldInfo, DocFieldConsumerPerField>();
+    Map<String,DocFieldConsumerPerField> childFields = new HashMap<String,DocFieldConsumerPerField>();
     Collection<DocFieldConsumerPerField> fields = fields();
     for (DocFieldConsumerPerField f : fields) {
-      childFields.put(f.getFieldInfo(), f);
+      childFields.put(f.getFieldInfo().name, f);
     }
 
     fieldsWriter.flush(state);
     consumer.flush(childFields, state);
 
     for (DocValuesConsumerAndDocID consumer : docValues.values()) {
-      consumer.docValuesConsumer.finish(state.numDocs);
+      consumer.docValuesConsumer.finish(state.segmentInfo.getDocCount());
     }
+    
+    // close perDocConsumer during flush to ensure all files are flushed due to PerCodec CFS
+    IOUtils.close(perDocConsumer);
 
     // Important to save after asking consumer to flush so
     // consumer can alter the FieldInfo* if necessary.  EG,
     // FreqProxTermsWriter does this with
     // FieldInfo.storePayload.
     FieldInfosWriter infosWriter = codec.fieldInfosFormat().getFieldInfosWriter();
-    infosWriter.write(state.directory, state.segmentName, state.fieldInfos, IOContext.DEFAULT);
-
-    // close perDocConsumer during flush to ensure all files are flushed due to PerCodec CFS
-    IOUtils.close(perDocConsumer);
+    infosWriter.write(state.directory, state.segmentInfo.name, state.fieldInfos, IOContext.DEFAULT);
   }
 
   @Override
@@ -203,7 +203,7 @@
   }
 
   @Override
-  public void processDocument(FieldInfos fieldInfos) throws IOException {
+  public void processDocument(FieldInfos.Builder fieldInfos) throws IOException {
 
     consumer.startDocument();
     fieldsWriter.startDocument();
@@ -342,7 +342,8 @@
       }
     }
     DocValuesConsumer docValuesConsumer = perDocConsumer.addValuesField(valueType, fieldInfo);
-    fieldInfo.setDocValuesType(valueType, false);
+    assert fieldInfo.getDocValuesType() == null || fieldInfo.getDocValuesType() == valueType;
+    fieldInfo.setDocValuesType(valueType);
 
     docValuesConsumerAndDocID = new DocValuesConsumerAndDocID(docValuesConsumer);
     docValuesConsumerAndDocID.docID = docState.docID;


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/index/DocInverter.java lucene4055/lucene/core/src/java/org/apache/lucene/index/DocInverter.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/index/DocInverter.java	2012-05-24 16:55:48.488233437 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/index/DocInverter.java	2012-05-21 13:58:03.519533887 -0400
@@ -39,12 +39,12 @@
   }
 
   @Override
-  void flush(Map<FieldInfo, DocFieldConsumerPerField> fieldsToFlush, SegmentWriteState state) throws IOException {
+  void flush(Map<String, DocFieldConsumerPerField> fieldsToFlush, SegmentWriteState state) throws IOException {
 
-    Map<FieldInfo, InvertedDocConsumerPerField> childFieldsToFlush = new HashMap<FieldInfo, InvertedDocConsumerPerField>();
-    Map<FieldInfo, InvertedDocEndConsumerPerField> endChildFieldsToFlush = new HashMap<FieldInfo, InvertedDocEndConsumerPerField>();
+    Map<String, InvertedDocConsumerPerField> childFieldsToFlush = new HashMap<String, InvertedDocConsumerPerField>();
+    Map<String, InvertedDocEndConsumerPerField> endChildFieldsToFlush = new HashMap<String, InvertedDocEndConsumerPerField>();
 
-    for (Map.Entry<FieldInfo, DocFieldConsumerPerField> fieldToFlush : fieldsToFlush.entrySet()) {
+    for (Map.Entry<String, DocFieldConsumerPerField> fieldToFlush : fieldsToFlush.entrySet()) {
       DocInverterPerField perField = (DocInverterPerField) fieldToFlush.getValue();
       childFieldsToFlush.put(fieldToFlush.getKey(), perField.consumer);
       endChildFieldsToFlush.put(fieldToFlush.getKey(), perField.endConsumer);


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/index/DocumentsWriterFlushQueue.java lucene4055/lucene/core/src/java/org/apache/lucene/index/DocumentsWriterFlushQueue.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/index/DocumentsWriterFlushQueue.java	2012-05-24 16:55:48.468233437 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/index/DocumentsWriterFlushQueue.java	2012-05-21 13:58:03.499533887 -0400
@@ -170,7 +170,7 @@
     protected abstract boolean canPublish();
   }
   
-  static final class GlobalDeletesTicket extends FlushTicket{
+  static final class GlobalDeletesTicket extends FlushTicket {
 
     protected GlobalDeletesTicket(FrozenBufferedDeletes frozenDeletes) {
       super(frozenDeletes);


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/index/DocumentsWriter.java lucene4055/lucene/core/src/java/org/apache/lucene/index/DocumentsWriter.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/index/DocumentsWriter.java	2012-05-24 16:55:48.484233437 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/index/DocumentsWriter.java	2012-05-22 19:29:26.657384749 -0400
@@ -28,7 +28,7 @@
 import org.apache.lucene.index.DocumentsWriterPerThread.FlushedSegment;
 import org.apache.lucene.index.DocumentsWriterPerThread.IndexingChain;
 import org.apache.lucene.index.DocumentsWriterPerThreadPool.ThreadState;
-import org.apache.lucene.index.FieldInfos.FieldNumberBiMap;
+import org.apache.lucene.index.FieldInfos.FieldNumbers;
 import org.apache.lucene.search.Query;
 import org.apache.lucene.search.similarities.Similarity;
 import org.apache.lucene.store.AlreadyClosedException;
@@ -133,7 +133,7 @@
   final DocumentsWriterFlushControl flushControl;
   
   final Codec codec;
-  DocumentsWriter(Codec codec, IndexWriterConfig config, Directory directory, IndexWriter writer, FieldNumberBiMap globalFieldNumbers,
+  DocumentsWriter(Codec codec, IndexWriterConfig config, Directory directory, IndexWriter writer, FieldNumbers globalFieldNumbers,
       BufferedDeletesStream bufferedDeletesStream) throws IOException {
     this.codec = codec;
     this.directory = directory;
@@ -494,7 +494,8 @@
   private void publishFlushedSegment(FlushedSegment newSegment, FrozenBufferedDeletes globalPacket)
       throws IOException {
     assert newSegment != null;
-    final SegmentInfo segInfo = indexWriter.prepareFlushedSegment(newSegment);
+    assert newSegment.segmentInfo != null;
+    final SegmentInfoPerCommit segInfo = indexWriter.prepareFlushedSegment(newSegment);
     final BufferedDeletes deletes = newSegment.segmentDeletes;
     if (infoStream.isEnabled("DW")) {
       infoStream.message("DW", Thread.currentThread().getName() + ": publishFlushedSegment seg-private deletes=" + deletes);  


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThread.java lucene4055/lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThread.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThread.java	2012-05-24 16:55:48.468233437 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThread.java	2012-05-24 12:04:20.547928895 -0400
@@ -17,11 +17,9 @@
  * limitations under the License.
  */
 
-import static org.apache.lucene.util.ByteBlockPool.BYTE_BLOCK_MASK;
-import static org.apache.lucene.util.ByteBlockPool.BYTE_BLOCK_SIZE;
-
 import java.io.IOException;
 import java.text.NumberFormat;
+import java.util.HashSet;
 
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.codecs.Codec;
@@ -30,12 +28,17 @@
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.FlushInfo;
 import org.apache.lucene.store.IOContext;
-import org.apache.lucene.util.Counter;
+import org.apache.lucene.store.TrackingDirectoryWrapper;
 import org.apache.lucene.util.ByteBlockPool.Allocator;
 import org.apache.lucene.util.ByteBlockPool.DirectTrackingAllocator;
+import org.apache.lucene.util.Constants;
+import org.apache.lucene.util.Counter;
 import org.apache.lucene.util.InfoStream;
-import org.apache.lucene.util.RamUsageEstimator;
 import org.apache.lucene.util.MutableBits;
+import org.apache.lucene.util.RamUsageEstimator;
+
+import static org.apache.lucene.util.ByteBlockPool.BYTE_BLOCK_MASK;
+import static org.apache.lucene.util.ByteBlockPool.BYTE_BLOCK_SIZE;
 
 class DocumentsWriterPerThread {
 
@@ -112,14 +115,16 @@
   }
 
   static class FlushedSegment {
-    final SegmentInfo segmentInfo;
+    final SegmentInfoPerCommit segmentInfo;
+    final FieldInfos fieldInfos;
     final BufferedDeletes segmentDeletes;
     final MutableBits liveDocs;
     final int delCount;
 
-    private FlushedSegment(SegmentInfo segmentInfo,
+    private FlushedSegment(SegmentInfoPerCommit segmentInfo, FieldInfos fieldInfos,
                            BufferedDeletes segmentDeletes, MutableBits liveDocs, int delCount) {
       this.segmentInfo = segmentInfo;
+      this.fieldInfos = fieldInfos;
       this.segmentDeletes = segmentDeletes;
       this.liveDocs = liveDocs;
       this.delCount = delCount;
@@ -157,7 +162,8 @@
   final DocumentsWriter parent;
   final Codec codec;
   final IndexWriter writer;
-  final Directory directory;
+  final TrackingDirectoryWrapper directory;
+  final Directory directoryOrig;
   final DocState docState;
   final DocConsumer consumer;
   final Counter bytesUsed;
@@ -165,11 +171,11 @@
   SegmentWriteState flushState;
   //Deletes for our still-in-RAM (to be flushed next) segment
   BufferedDeletes pendingDeletes;  
-  String segment;     // Current segment we are working on
+  SegmentInfo segmentInfo;     // Current segment we are working on
   boolean aborting = false;   // True if an abort is pending
   boolean hasAborted = false; // True if the last exception throws by #updateDocument was aborting
 
-  private FieldInfos fieldInfos;
+  private FieldInfos.Builder fieldInfos;
   private final InfoStream infoStream;
   private int numDocsInRAM;
   private int flushedDocCount;
@@ -180,8 +186,9 @@
 
   
   public DocumentsWriterPerThread(Directory directory, DocumentsWriter parent,
-      FieldInfos fieldInfos, IndexingChain indexingChain) {
-    this.directory = directory;
+      FieldInfos.Builder fieldInfos, IndexingChain indexingChain) {
+    this.directoryOrig = directory;
+    this.directory = new TrackingDirectoryWrapper(directory);
     this.parent = parent;
     this.fieldInfos = fieldInfos;
     this.writer = parent.indexWriter;
@@ -196,8 +203,8 @@
     initialize();
   }
   
-  public DocumentsWriterPerThread(DocumentsWriterPerThread other, FieldInfos fieldInfos) {
-    this(other.directory, other.parent, fieldInfos, other.parent.chain);
+  public DocumentsWriterPerThread(DocumentsWriterPerThread other, FieldInfos.Builder fieldInfos) {
+    this(other.directoryOrig, other.parent, fieldInfos, other.parent.chain);
   }
   
   void initialize() {
@@ -223,17 +230,11 @@
     docState.doc = doc;
     docState.analyzer = analyzer;
     docState.docID = numDocsInRAM;
-    if (segment == null) {
-      // this call is synchronized on IndexWriter.segmentInfos
-      segment = writer.newSegmentName();
-      assert numDocsInRAM == 0;
-      if (INFO_VERBOSE && infoStream.isEnabled("DWPT")) {
-        infoStream.message("DWPT", Thread.currentThread().getName() + " init seg=" + segment + " delQueue=" + deleteQueue);  
-      }
-      
+    if (segmentInfo == null) {
+      initSegmentInfo();
     }
     if (INFO_VERBOSE && infoStream.isEnabled("DWPT")) {
-      infoStream.message("DWPT", Thread.currentThread().getName() + " update delTerm=" + delTerm + " docID=" + docState.docID + " seg=" + segment);
+      infoStream.message("DWPT", Thread.currentThread().getName() + " update delTerm=" + delTerm + " docID=" + docState.docID + " seg=" + segmentInfo.name);
     }
     boolean success = false;
     try {
@@ -265,21 +266,26 @@
     }
     finishDocument(delTerm);
   }
+
+  private void initSegmentInfo() {
+    String segment = writer.newSegmentName();
+    segmentInfo = new SegmentInfo(directoryOrig, Constants.LUCENE_MAIN_VERSION, segment, -1,
+                                  false, codec, null, null);
+    assert numDocsInRAM == 0;
+    if (INFO_VERBOSE && infoStream.isEnabled("DWPT")) {
+      infoStream.message("DWPT", Thread.currentThread().getName() + " init seg=" + segment + " delQueue=" + deleteQueue);  
+    }
+  }
   
   public int updateDocuments(Iterable<? extends Iterable<? extends IndexableField>> docs, Analyzer analyzer, Term delTerm) throws IOException {
     assert writer.testPoint("DocumentsWriterPerThread addDocuments start");
     assert deleteQueue != null;
     docState.analyzer = analyzer;
-    if (segment == null) {
-      // this call is synchronized on IndexWriter.segmentInfos
-      segment = writer.newSegmentName();
-      assert numDocsInRAM == 0;
-      if (INFO_VERBOSE && infoStream.isEnabled("DWPT")) {
-        infoStream.message("DWPT", Thread.currentThread().getName() + " init seg=" + segment + " delQueue=" + deleteQueue);  
-      }
+    if (segmentInfo == null) {
+      initSegmentInfo();
     }
     if (INFO_VERBOSE && infoStream.isEnabled("DWPT")) {
-      infoStream.message("DWPT", Thread.currentThread().getName() + " update delTerm=" + delTerm + " docID=" + docState.docID + " seg=" + segment);
+      infoStream.message("DWPT", Thread.currentThread().getName() + " update delTerm=" + delTerm + " docID=" + docState.docID + " seg=" + segmentInfo.name);
     }
     int docCount = 0;
     try {
@@ -405,15 +411,12 @@
     return numDocsInRAM;
   }
 
-  Codec getCodec() {
-    return flushState.codec;
-  }
-
   /** Reset after a flush */
   private void doAfterFlush() throws IOException {
-    segment = null;
+    segmentInfo = null;
     consumer.doAfterFlush();
-    fieldInfos = FieldInfos.from(fieldInfos);
+    directory.getCreatedFiles().clear();
+    fieldInfos = new FieldInfos.Builder(fieldInfos.globalFieldNumbers);
     parent.subtractFlushedNumDocs(numDocsInRAM);
     numDocsInRAM = 0;
   }
@@ -441,10 +444,12 @@
   FlushedSegment flush() throws IOException {
     assert numDocsInRAM > 0;
     assert deleteSlice == null : "all deletes must be applied in prepareFlush";
-    flushState = new SegmentWriteState(infoStream, directory, segment, fieldInfos,
-        numDocsInRAM, writer.getConfig().getTermIndexInterval(),
-        codec, pendingDeletes, new IOContext(new FlushInfo(numDocsInRAM, bytesUsed())));
+    segmentInfo.setDocCount(numDocsInRAM);
+    flushState = new SegmentWriteState(infoStream, directory, segmentInfo, fieldInfos.finish(),
+        writer.getConfig().getTermIndexInterval(),
+        pendingDeletes, new IOContext(new FlushInfo(numDocsInRAM, bytesUsed())));
     final double startMBUsed = parent.flushControl.netBytes() / 1024. / 1024.;
+
     // Apply delete-by-docID now (delete-byDocID only
     // happens when an exception is hit processing that
     // doc, eg if analyzer has some problem w/ the text):
@@ -459,7 +464,7 @@
     }
 
     if (infoStream.isEnabled("DWPT")) {
-      infoStream.message("DWPT", "flush postings as segment " + flushState.segmentName + " numDocs=" + numDocsInRAM);
+      infoStream.message("DWPT", "flush postings as segment " + flushState.segmentInfo.name + " numDocs=" + numDocsInRAM);
     }
 
     if (aborting) {
@@ -474,14 +479,22 @@
     try {
       consumer.flush(flushState);
       pendingDeletes.terms.clear();
-      final SegmentInfo newSegment = new SegmentInfo(segment, flushState.numDocs, directory, false, flushState.codec, fieldInfos.asReadOnly());
+      segmentInfo.setFiles(new HashSet<String>(directory.getCreatedFiles()));
+
+      final SegmentInfoPerCommit segmentInfoPerCommit = new SegmentInfoPerCommit(segmentInfo, 0, -1L);
       if (infoStream.isEnabled("DWPT")) {
-        infoStream.message("DWPT", "new segment has " + (flushState.liveDocs == null ? 0 : (flushState.numDocs - flushState.delCountOnFlush)) + " deleted docs");
-        infoStream.message("DWPT", "new segment has " + (newSegment.getHasVectors() ? "vectors" : "no vectors"));
-        infoStream.message("DWPT", "flushedFiles=" + newSegment.files());
-        infoStream.message("DWPT", "flushed codec=" + newSegment.getCodec());
+        infoStream.message("DWPT", "new segment has " + (flushState.liveDocs == null ? 0 : (flushState.segmentInfo.getDocCount() - flushState.delCountOnFlush)) + " deleted docs");
+        infoStream.message("DWPT", "new segment has " +
+                           (flushState.fieldInfos.hasVectors() ? "vectors" : "no vectors") + "; " +
+                           (flushState.fieldInfos.hasNorms() ? "norms" : "no norms") + "; " + 
+                           (flushState.fieldInfos.hasDocValues() ? "docValues" : "no docValues") + "; " + 
+                           (flushState.fieldInfos.hasProx() ? "prox" : "no prox") + "; " + 
+                           (flushState.fieldInfos.hasFreq() ? "freqs" : "no freqs"));
+        infoStream.message("DWPT", "flushedFiles=" + segmentInfoPerCommit.files());
+        infoStream.message("DWPT", "flushed codec=" + codec);
       }
-      flushedDocCount += flushState.numDocs;
+
+      flushedDocCount += flushState.segmentInfo.getDocCount();
 
       final BufferedDeletes segmentDeletes;
       if (pendingDeletes.queries.isEmpty()) {
@@ -493,21 +506,26 @@
       }
 
       if (infoStream.isEnabled("DWPT")) {
-        final double newSegmentSize = newSegment.sizeInBytes()/1024./1024.;
-        infoStream.message("DWPT", "flushed: segment=" + newSegment + 
+        final double newSegmentSize = segmentInfo.sizeInBytes()/1024./1024.;
+        infoStream.message("DWPT", "flushed: segment=" + segmentInfo.name + 
                 " ramUsed=" + nf.format(startMBUsed) + " MB" +
                 " newFlushedSize(includes docstores)=" + nf.format(newSegmentSize) + " MB" +
                 " docs/MB=" + nf.format(flushedDocCount / newSegmentSize));
       }
+
+      assert segmentInfo != null;
+
+      FlushedSegment fs = new FlushedSegment(segmentInfoPerCommit, flushState.fieldInfos,
+                                             segmentDeletes, flushState.liveDocs, flushState.delCountOnFlush);
       doAfterFlush();
       success = true;
 
-      return new FlushedSegment(newSegment, segmentDeletes, flushState.liveDocs, flushState.delCountOnFlush);
+      return fs;
     } finally {
       if (!success) {
-        if (segment != null) {
+        if (segmentInfo != null) {
           synchronized(parent.indexWriter) {
-            parent.indexWriter.deleter.refresh(segment);
+            parent.indexWriter.deleter.refresh(segmentInfo.name);
           }
         }
         abort();
@@ -515,9 +533,9 @@
     }
   }
 
-  /** Get current segment name we are writing. */
-  String getSegment() {
-    return segment;
+  /** Get current segment info we are writing. */
+  SegmentInfo getSegmentInfo() {
+    return segmentInfo;
   }
 
   long bytesUsed() {
@@ -550,14 +568,14 @@
   }
 
   PerDocWriteState newPerDocWriteState(String segmentSuffix) {
-    assert segment != null;
-    return new PerDocWriteState(infoStream, directory, segment, fieldInfos, bytesUsed, segmentSuffix, IOContext.DEFAULT);
+    assert segmentInfo != null;
+    return new PerDocWriteState(infoStream, directory, segmentInfo, bytesUsed, segmentSuffix, IOContext.DEFAULT);
   }
   
   @Override
   public String toString() {
     return "DocumentsWriterPerThread [pendingDeletes=" + pendingDeletes
-        + ", segment=" + segment + ", aborting=" + aborting + ", numDocsInRAM="
+      + ", segment=" + (segmentInfo != null ? segmentInfo.name : "null") + ", aborting=" + aborting + ", numDocsInRAM="
         + numDocsInRAM + ", deleteQueue=" + deleteQueue + "]";
   }
 }


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThreadPool.java lucene4055/lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThreadPool.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThreadPool.java	2012-05-24 16:55:48.464233437 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThreadPool.java	2012-05-22 19:29:26.657384749 -0400
@@ -18,7 +18,7 @@
 
 import java.util.concurrent.locks.ReentrantLock;
 
-import org.apache.lucene.index.FieldInfos.FieldNumberBiMap;
+import org.apache.lucene.index.FieldInfos.FieldNumbers;
 import org.apache.lucene.util.SetOnce;
 
 /**
@@ -121,7 +121,7 @@
 
   private final ThreadState[] threadStates;
   private volatile int numThreadStatesActive;
-  private final SetOnce<FieldNumberBiMap> globalFieldMap = new SetOnce<FieldNumberBiMap>();
+  private final SetOnce<FieldNumbers> globalFieldMap = new SetOnce<FieldNumbers>();
   private final SetOnce<DocumentsWriter> documentsWriter = new SetOnce<DocumentsWriter>();
   
   /**
@@ -135,11 +135,11 @@
     numThreadStatesActive = 0;
   }
 
-  void initialize(DocumentsWriter documentsWriter, FieldNumberBiMap globalFieldMap, IndexWriterConfig config) {
+  void initialize(DocumentsWriter documentsWriter, FieldNumbers globalFieldMap, IndexWriterConfig config) {
     this.documentsWriter.set(documentsWriter); // thread pool is bound to DW
     this.globalFieldMap.set(globalFieldMap);
     for (int i = 0; i < threadStates.length; i++) {
-      final FieldInfos infos = new FieldInfos(globalFieldMap);
+      final FieldInfos.Builder infos = new FieldInfos.Builder(globalFieldMap);
       threadStates[i] = new ThreadState(new DocumentsWriterPerThread(documentsWriter.directory, documentsWriter, infos, documentsWriter.chain));
     }
   }
@@ -228,7 +228,7 @@
     assert globalFieldMap.get() != null;
     final DocumentsWriterPerThread dwpt = threadState.dwpt;
     if (!closed) {
-      final FieldInfos infos = new FieldInfos(globalFieldMap.get());
+      final FieldInfos.Builder infos = new FieldInfos.Builder(globalFieldMap.get());
       final DocumentsWriterPerThread newDwpt = new DocumentsWriterPerThread(dwpt, infos);
       newDwpt.initialize();
       threadState.resetWriter(newDwpt);


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/index/FieldInfo.java lucene4055/lucene/core/src/java/org/apache/lucene/index/FieldInfo.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/index/FieldInfo.java	2012-05-24 16:55:48.476233437 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/index/FieldInfo.java	2012-05-22 11:39:04.616893278 -0400
@@ -1,8 +1,5 @@
 package org.apache.lucene.index;
 
-import org.apache.lucene.index.DocValues.Type;
-
-
 /**
  * Licensed to the Apache Software Foundation (ASF) under one or more
  * contributor license agreements.  See the NOTICE file distributed with
@@ -20,6 +17,11 @@
  * limitations under the License.
  */
 
+import java.util.HashMap;
+import java.util.Map;
+
+import org.apache.lucene.index.DocValues.Type;
+
 /**
  *  Access to the Fieldable Info file that describes document fields and whether or
  *  not they are indexed. Each segment has a separate Fieldable Info file. Objects
@@ -27,20 +29,23 @@
  *  be adding documents at a time, with no other reader or writer threads
  *  accessing this object.
  **/
+
 public final class FieldInfo {
   public final String name;
   public final int number;
 
-  public boolean isIndexed;
+  private boolean indexed;
   private DocValues.Type docValueType;
 
   // True if any document indexed term vectors
-  public boolean storeTermVector;
+  private boolean storeTermVector;
 
   private DocValues.Type normType;
-  public boolean omitNorms; // omit norms associated with indexed fields  
-  public IndexOptions indexOptions;
-  public boolean storePayloads; // whether this field stores payloads together with term positions
+  private boolean omitNorms; // omit norms associated with indexed fields  
+  private IndexOptions indexOptions;
+  private boolean storePayloads; // whether this field stores payloads together with term positions
+
+  private Map<String,String> attributes;
 
   /**
    * Controls how much information is stored in the postings lists.
@@ -64,13 +69,13 @@
   /**
    * @lucene.experimental
    */
-  public FieldInfo(String name, boolean isIndexed, int number, boolean storeTermVector, 
-            boolean omitNorms, boolean storePayloads, IndexOptions indexOptions, DocValues.Type docValues, DocValues.Type normsType) {
+  public FieldInfo(String name, boolean indexed, int number, boolean storeTermVector, 
+            boolean omitNorms, boolean storePayloads, IndexOptions indexOptions, DocValues.Type docValues, DocValues.Type normsType, Map<String,String> attributes) {
     this.name = name;
-    this.isIndexed = isIndexed;
+    this.indexed = indexed;
     this.number = number;
     this.docValueType = docValues;
-    if (isIndexed) {
+    if (indexed) {
       this.storeTermVector = storeTermVector;
       this.storePayloads = storePayloads;
       this.omitNorms = omitNorms;
@@ -83,22 +88,38 @@
       this.indexOptions = IndexOptions.DOCS_AND_FREQS_AND_POSITIONS;
       this.normType = null;
     }
+    this.attributes = attributes;
+    assert checkConsistency();
     assert indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 || !storePayloads;
   }
-  
-  @Override
-  public FieldInfo clone() {
-    return new FieldInfo(name, isIndexed, number, storeTermVector,
-                         omitNorms, storePayloads, indexOptions, docValueType, normType);
+
+  private boolean checkConsistency() {
+    if (!indexed) {
+      assert !storeTermVector;
+      assert !storePayloads;
+      assert !omitNorms;
+      assert normType == null;
+      assert indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS;
+    } else {
+      assert indexOptions != null;
+      if (omitNorms) {
+        assert normType == null;
+      }
+    }
+
+    // Cannot store payloads unless positions are indexed:
+    assert indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 || !this.storePayloads;
+
+    return true;
   }
 
   // should only be called by FieldInfos#addOrUpdate
-  void update(boolean isIndexed, boolean storeTermVector, boolean omitNorms, boolean storePayloads, IndexOptions indexOptions) {
+  void update(boolean indexed, boolean storeTermVector, boolean omitNorms, boolean storePayloads, IndexOptions indexOptions) {
 
-    if (this.isIndexed != isIndexed) {
-      this.isIndexed = true;                      // once indexed, always index
+    if (this.indexed != indexed) {
+      this.indexed = true;                      // once indexed, always index
     }
-    if (isIndexed) { // if updated field data is not for indexing, leave the updates out
+    if (indexed) { // if updated field data is not for indexing, leave the updates out
       if (this.storeTermVector != storeTermVector) {
         this.storeTermVector = true;                // once vector, always vector
       }
@@ -107,6 +128,7 @@
       }
       if (this.omitNorms != omitNorms) {
         this.omitNorms = true;                // if one require omitNorms at least once, it remains off for life
+        this.normType = null;
       }
       if (this.indexOptions != indexOptions) {
         // downgrade
@@ -118,14 +140,17 @@
       }
     }
     assert this.indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 || !this.storePayloads;
+    assert checkConsistency();
   }
 
-  void setDocValuesType(DocValues.Type type, boolean force) {
-    if (docValueType == null || force) {
-      docValueType = type;
-    } else if (type != docValueType) {
-      throw new IllegalArgumentException("DocValues type already set to " + docValueType + " but was: " + type);
-    }
+  void setDocValuesType(DocValues.Type type) {
+    docValueType = type;
+    assert checkConsistency();
+  }
+  
+  /** @return IndexOptions for the field */
+  public IndexOptions getIndexOptions() {
+    return indexOptions;
   }
   
   /**
@@ -149,22 +174,27 @@
     return normType;
   }
 
-  public void setStoreTermVectors() {
+  void setStoreTermVectors() {
     storeTermVector = true;
+    assert checkConsistency();
   }
-
-  public void setNormValueType(Type type, boolean force) {
-    if (normType == null || force) {
-      normType = type;
-    } else if (type != normType) {
-      throw new IllegalArgumentException("Norm type already set to " + normType);
+  
+  void setStorePayloads() {
+    if (indexed && indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0) {
+      storePayloads = true;
     }
+    assert checkConsistency();
+  }
+
+  void setNormValueType(Type type) {
+    normType = type;
+    assert checkConsistency();
   }
   
   /**
    * @return true if norms are explicitly omitted for this field
    */
-  public boolean omitNorms() {
+  public boolean omitsNorms() {
     return omitNorms;
   }
   
@@ -172,7 +202,62 @@
    * @return true if this field actually has any norms.
    */
   public boolean hasNorms() {
-    return isIndexed && !omitNorms && normType != null;
+    return normType != null;
+  }
+  
+  /**
+   * @return true if this field is indexed.
+   */
+  public boolean isIndexed() {
+    return indexed;
+  }
+  
+  /**
+   * @return true if any payloads exist for this field.
+   */
+  public boolean hasPayloads() {
+    return storePayloads;
   }
   
+  /**
+   * @return true if any term vectors exist for this field.
+   */
+  public boolean hasVectors() {
+    return storeTermVector;
+  }
+  
+  /**
+   * Get a codec attribute value, or null if it does not exist
+   */
+  public String getAttribute(String key) {
+    if (attributes == null) {
+      return null;
+    } else {
+      return attributes.get(key);
+    }
+  }
+  
+  /**
+   * Puts a codec attribute value.
+   * <p>
+   * This is a key-value mapping for the field that the codec can use
+   * to store additional metadata, and will be available to the codec
+   * when reading the segment via {@link #getAttribute(String)}
+   * <p>
+   * If a value already exists for the field, it will be replaced with 
+   * the new value.
+   */
+  public String putAttribute(String key, String value) {
+    if (attributes == null) {
+      attributes = new HashMap<String,String>();
+    }
+    return attributes.put(key, value);
+  }
+  
+  /**
+   * @return internal codec attributes map. May be null if no mappings exist.
+   */
+  public Map<String,String> attributes() {
+    return attributes;
+  }
 }


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/index/FieldInfos.java lucene4055/lucene/core/src/java/org/apache/lucene/index/FieldInfos.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/index/FieldInfos.java	2012-05-24 16:55:48.464233437 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/index/FieldInfos.java	2012-05-22 19:29:26.689384750 -0400
@@ -18,6 +18,7 @@
  */
 
 import java.util.Collection;
+import java.util.Collections;
 import java.util.HashMap;
 import java.util.Iterator;
 import java.util.Map;
@@ -30,14 +31,119 @@
  * Collection of {@link FieldInfo}s (accessible by number or by name).
  *  @lucene.experimental
  */
-public final class FieldInfos implements Iterable<FieldInfo> {
-  static final class FieldNumberBiMap {
+public class FieldInfos implements Iterable<FieldInfo> {
+  private final boolean hasFreq;
+  private final boolean hasProx;
+  private final boolean hasVectors;
+  private final boolean hasNorms;
+  private final boolean hasDocValues;
+  
+  private final SortedMap<Integer,FieldInfo> byNumber = new TreeMap<Integer,FieldInfo>();
+  private final HashMap<String,FieldInfo> byName = new HashMap<String,FieldInfo>();
+  private final Collection<FieldInfo> values; // for an unmodifiable iterator
+  
+  public FieldInfos(FieldInfo[] infos) {
+    boolean hasVectors = false;
+    boolean hasProx = false;
+    boolean hasFreq = false;
+    boolean hasNorms = false;
+    boolean hasDocValues = false;
+    
+    for (FieldInfo info : infos) {
+      assert !byNumber.containsKey(info.number);
+      byNumber.put(info.number, info);
+      assert !byName.containsKey(info.name);
+      byName.put(info.name, info);
+      
+      hasVectors |= info.hasVectors();
+      hasProx |= info.isIndexed() && info.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;
+      hasFreq |= info.isIndexed() && info.getIndexOptions() != IndexOptions.DOCS_ONLY;
+      hasNorms |= info.hasNorms();
+      hasDocValues |= info.hasDocValues();
+    }
+    
+    this.hasVectors = hasVectors;
+    this.hasProx = hasProx;
+    this.hasFreq = hasFreq;
+    this.hasNorms = hasNorms;
+    this.hasDocValues = hasDocValues;
+    this.values = Collections.unmodifiableCollection(byNumber.values());
+  }
+  
+  /** Returns true if any fields have freqs */
+  public boolean hasFreq() {
+    return hasFreq;
+  }
+  
+  /** Returns true if any fields have positions */
+  public boolean hasProx() {
+    return hasProx;
+  }
+  
+  /**
+   * @return true if at least one field has any vectors
+   */
+  public boolean hasVectors() {
+    return hasVectors;
+  }
+  
+  /**
+   * @return true if at least one field has any norms
+   */
+  public boolean hasNorms() {
+    return hasNorms;
+  }
+  
+  /**
+   * @return true if at least one field has doc values
+   */
+  public boolean hasDocValues() {
+    return hasDocValues;
+  }
+  
+  /**
+   * @return number of fields
+   */
+  public int size() {
+    assert byNumber.size() == byName.size();
+    return byNumber.size();
+  }
+  
+  /**
+   * Returns an iterator over all the fieldinfo objects present,
+   * ordered by ascending field number
+   */
+  // TODO: what happens if in fact a different order is used?
+  public Iterator<FieldInfo> iterator() {
+    return values.iterator();
+  }
+
+  /**
+   * Return the fieldinfo object referenced by the field name
+   * @return the FieldInfo object or null when the given fieldName
+   * doesn't exist.
+   */  
+  public FieldInfo fieldInfo(String fieldName) {
+    return byName.get(fieldName);
+  }
+
+  /**
+   * Return the fieldinfo object referenced by the fieldNumber.
+   * @param fieldNumber
+   * @return the FieldInfo object or null when the given fieldNumber
+   * doesn't exist.
+   */  
+  public FieldInfo fieldInfo(int fieldNumber) {
+    return (fieldNumber >= 0) ? byNumber.get(fieldNumber) : null;
+  }
+  
+  static final class FieldNumbers {
     
     private final Map<Integer,String> numberToName;
     private final Map<String,Integer> nameToNumber;
     private int lowestUnassignedFieldNumber = -1;
     
-    FieldNumberBiMap() {
+    FieldNumbers() {
       this.nameToNumber = new HashMap<String, Integer>();
       this.numberToName = new HashMap<Integer, String>();
     }
@@ -54,8 +160,8 @@
         final Integer preferredBoxed = Integer.valueOf(preferredFieldNumber);
 
         if (preferredFieldNumber != -1 && !numberToName.containsKey(preferredBoxed)) {
-            // cool - we can use this number globally
-            fieldNumber = preferredBoxed;
+          // cool - we can use this number globally
+          fieldNumber = preferredBoxed;
         } else {
           // find a new FieldNumber
           while (numberToName.containsKey(++lowestUnassignedFieldNumber)) {
@@ -66,7 +172,6 @@
         
         numberToName.put(fieldNumber, fieldName);
         nameToNumber.put(fieldName, fieldNumber);
-        
       }
 
       return fieldNumber.intValue();
@@ -93,386 +198,115 @@
     }
   }
   
-  private final SortedMap<Integer,FieldInfo> byNumber = new TreeMap<Integer,FieldInfo>();
-  private final HashMap<String,FieldInfo> byName = new HashMap<String,FieldInfo>();
-  private final FieldNumberBiMap globalFieldNumbers;
-  
-  private boolean hasFreq; // only set if readonly
-  private boolean hasProx; // only set if readonly
-  private boolean hasVectors; // only set if readonly
-  private long version; // internal use to track changes
-
-  /**
-   * Creates a new read-only FieldInfos: only public to be accessible
-   * from the codecs package
-   * 
-   * @lucene.internal
-   */
-  public FieldInfos(FieldInfo[] infos, boolean hasFreq, boolean hasProx, boolean hasVectors) {
-    this(null);
-    this.hasFreq = hasFreq;
-    this.hasProx = hasProx;
-    this.hasVectors = hasVectors;
-    for (FieldInfo info : infos) {
-      putInternal(info);
-    }
-  }
-
-  public FieldInfos() {
-    this(new FieldNumberBiMap());
-  }
+  static final class Builder {
+    private final HashMap<String,FieldInfo> byName = new HashMap<String,FieldInfo>();
+    final FieldNumbers globalFieldNumbers;
 
-  public void add(FieldInfos other) {
-    for(FieldInfo fieldInfo : other){ 
-      add(fieldInfo);
+    Builder() {
+      this(new FieldNumbers());
     }
-  }
-
-  /**
-   * Creates a new FieldInfos instance with the given {@link FieldNumberBiMap}. 
-   * If the {@link FieldNumberBiMap} is <code>null</code> this instance will be read-only.
-   * @see #isReadOnly()
-   */
-  FieldInfos(FieldNumberBiMap globalFieldNumbers) {
-    this.globalFieldNumbers = globalFieldNumbers;
-  }
-  
-  /**
-   * adds the given field to this FieldInfos name / number mapping. The given FI
-   * must be present in the global field number mapping before this method it
-   * called
-   */
-  private void putInternal(FieldInfo fi) {
-    assert !byNumber.containsKey(fi.number);
-    assert !byName.containsKey(fi.name);
-    assert globalFieldNumbers == null || globalFieldNumbers.containsConsistent(Integer.valueOf(fi.number), fi.name);
-    byNumber.put(fi.number, fi);
-    byName.put(fi.name, fi);
-  }
-  
-  private int nextFieldNumber(String name, int preferredFieldNumber) {
-    // get a global number for this field
-    final int fieldNumber = globalFieldNumbers.addOrGet(name,
-        preferredFieldNumber);
-    assert byNumber.get(fieldNumber) == null : "field number " + fieldNumber
-        + " already taken";
-    return fieldNumber;
-  }
-
-  /**
-   * Returns a deep clone of this FieldInfos instance.
-   */
-  @Override
-  synchronized public FieldInfos clone() {
-    FieldInfos fis = new FieldInfos(globalFieldNumbers);
-    fis.hasFreq = hasFreq;
-    fis.hasProx = hasProx;
-    fis.hasVectors = hasVectors;
-    for (FieldInfo fi : this) {
-      FieldInfo clone = fi.clone();
-      fis.putInternal(clone);
+    
+    /**
+     * Creates a new instance with the given {@link FieldNumbers}. 
+     */
+    Builder(FieldNumbers globalFieldNumbers) {
+      assert globalFieldNumbers != null;
+      this.globalFieldNumbers = globalFieldNumbers;
     }
-    return fis;
-  }
 
-  /** Returns true if any fields have positions */
-  public boolean hasProx() {
-    if (isReadOnly()) {
-      return hasProx;
-    }
-    // mutable FIs must check!
-    for (FieldInfo fi : this) {
-      if (fi.isIndexed && fi.indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0) {
-        return true;
+    public void add(FieldInfos other) {
+      for(FieldInfo fieldInfo : other){ 
+        add(fieldInfo);
       }
     }
-    return false;
-  }
-  
-  /** Returns true if any fields have freqs */
-  public boolean hasFreq() {
-    if (isReadOnly()) {
-      return hasFreq;
-    }
-    // mutable FIs must check!
-    for (FieldInfo fi : this) {
-      if (fi.isIndexed && fi.indexOptions != IndexOptions.DOCS_ONLY) {
-        return true;
-      }
-    }
-    return false;
-  }
-  
-  /**
-   * Adds or updates fields that are indexed. Whether they have termvectors has to be specified.
-   * 
-   * @param names The names of the fields
-   * @param storeTermVectors Whether the fields store term vectors or not
-   */
-  synchronized public void addOrUpdateIndexed(Collection<String> names, boolean storeTermVectors) {
-    for (String name : names) {
-      addOrUpdate(name, true, storeTermVectors);
-    }
-  }
-
-  /**
-   * Assumes the fields are not storing term vectors.
-   * 
-   * @param names The names of the fields
-   * @param isIndexed Whether the fields are indexed or not
-   * 
-   * @see #addOrUpdate(String, boolean)
-   */
-  synchronized public void addOrUpdate(Collection<String> names, boolean isIndexed) {
-    for (String name : names) {
-      addOrUpdate(name, isIndexed);
+   
+    /**
+     * adds the given field to this FieldInfos name / number mapping. The given FI
+     * must be present in the global field number mapping before this method it
+     * called
+     */
+    private void putInternal(FieldInfo fi) {
+      assert !byName.containsKey(fi.name);
+      assert globalFieldNumbers.containsConsistent(Integer.valueOf(fi.number), fi.name);
+      byName.put(fi.name, fi);
     }
-  }
-
-  /**
-   * Calls 5 parameter add with false for all TermVector parameters.
-   * 
-   * @param name The name of the IndexableField
-   * @param isIndexed true if the field is indexed
-   * @see #addOrUpdate(String, boolean, boolean)
-   */
-  synchronized public void addOrUpdate(String name, boolean isIndexed) {
-    addOrUpdate(name, isIndexed, false, false);
-  }
-
-  /** If the field is not yet known, adds it. If it is known, checks to make
-   *  sure that the isIndexed flag is the same as was given previously for this
-   *  field. If not - marks it as being indexed.  Same goes for the TermVector
-   * parameters.
-   * 
-   * @param name The name of the field
-   * @param isIndexed true if the field is indexed
-   * @param storeTermVector true if the term vector should be stored
-   */
-  synchronized public void addOrUpdate(String name, boolean isIndexed, boolean storeTermVector) {
-    addOrUpdate(name, isIndexed, storeTermVector, false);
-  }
-
+    
     /** If the field is not yet known, adds it. If it is known, checks to make
-   *  sure that the isIndexed flag is the same as was given previously for this
-   *  field. If not - marks it as being indexed.  Same goes for the TermVector
-   * parameters.
-   *
-   * @param name The name of the field
-   * @param isIndexed true if the field is indexed
-   * @param storeTermVector true if the term vector should be stored
-   * @param omitNorms true if the norms for the indexed field should be omitted
-   */
-  synchronized public void addOrUpdate(String name, boolean isIndexed, boolean storeTermVector,
-                  boolean omitNorms) {
-    addOrUpdate(name, isIndexed, storeTermVector, omitNorms, false, IndexOptions.DOCS_AND_FREQS_AND_POSITIONS, null, null);
-  }
-  
-  /** If the field is not yet known, adds it. If it is known, checks to make
-   *  sure that the isIndexed flag is the same as was given previously for this
-   *  field. If not - marks it as being indexed.  Same goes for the TermVector
-   * parameters.
-   *
-   * @param name The name of the field
-   * @param isIndexed true if the field is indexed
-   * @param storeTermVector true if the term vector should be stored
-   * @param omitNorms true if the norms for the indexed field should be omitted
-   * @param storePayloads true if payloads should be stored for this field
-   * @param indexOptions if term freqs should be omitted for this field
-   */
-  synchronized public FieldInfo addOrUpdate(String name, boolean isIndexed, boolean storeTermVector,
-                       boolean omitNorms, boolean storePayloads, IndexOptions indexOptions, DocValues.Type docValues, DocValues.Type normType) {
-    return addOrUpdateInternal(name, -1, isIndexed, storeTermVector, omitNorms, storePayloads, indexOptions, docValues, normType);
-  }
-
-  // NOTE: this method does not carry over termVector
-  // booleans nor docValuesType; the indexer chain
-  // (TermVectorsConsumerPerField, DocFieldProcessor) must
-  // set these fields when they succeed in consuming
-  // the document:
-  public FieldInfo addOrUpdate(String name, IndexableFieldType fieldType) {
-    // TODO: really, indexer shouldn't even call this
-    // method (it's only called from DocFieldProcessor);
-    // rather, each component in the chain should update
-    // what it "owns".  EG fieldType.indexOptions() should
-    // be updated by maybe FreqProxTermsWriterPerField:
-    return addOrUpdateInternal(name, -1, fieldType.indexed(), false,
-                               fieldType.omitNorms(), false,
-                               fieldType.indexOptions(), null, null);
-  }
-
-  synchronized private FieldInfo addOrUpdateInternal(String name, int preferredFieldNumber, boolean isIndexed,
-      boolean storeTermVector,
-      boolean omitNorms, boolean storePayloads, IndexOptions indexOptions, DocValues.Type docValues, DocValues.Type normType) {
-    if (globalFieldNumbers == null) {
-      throw new IllegalStateException("FieldInfos are read-only, create a new instance with a global field map to make modifications to FieldInfos");
-    }
-    FieldInfo fi = fieldInfo(name);
-    if (fi == null) {
-      final int fieldNumber = nextFieldNumber(name, preferredFieldNumber);
-      fi = addInternal(name, fieldNumber, isIndexed, storeTermVector, omitNorms, storePayloads, indexOptions, docValues, normType);
-    } else {
-      fi.update(isIndexed, storeTermVector, omitNorms, storePayloads, indexOptions);
-      if (docValues != null) {
-        fi.setDocValuesType(docValues, true);
-      }
-      if (normType != null) {
-        fi.setNormValueType(normType, true);
+     *  sure that the isIndexed flag is the same as was given previously for this
+     *  field. If not - marks it as being indexed.  Same goes for the TermVector
+     * parameters.
+     *
+     * @param name The name of the field
+     * @param isIndexed true if the field is indexed
+     * @param storeTermVector true if the term vector should be stored
+     * @param omitNorms true if the norms for the indexed field should be omitted
+     * @param storePayloads true if payloads should be stored for this field
+     * @param indexOptions if term freqs should be omitted for this field
+     */
+    // TODO: fix testCodecs to do this another way, its the only user of this
+    FieldInfo addOrUpdate(String name, boolean isIndexed, boolean storeTermVector,
+                         boolean omitNorms, boolean storePayloads, IndexOptions indexOptions, DocValues.Type docValues, DocValues.Type normType) {
+      return addOrUpdateInternal(name, -1, isIndexed, storeTermVector, omitNorms, storePayloads, indexOptions, docValues, normType);
+    }
+
+    // NOTE: this method does not carry over termVector
+    // booleans nor docValuesType; the indexer chain
+    // (TermVectorsConsumerPerField, DocFieldProcessor) must
+    // set these fields when they succeed in consuming
+    // the document:
+    public FieldInfo addOrUpdate(String name, IndexableFieldType fieldType) {
+      // TODO: really, indexer shouldn't even call this
+      // method (it's only called from DocFieldProcessor);
+      // rather, each component in the chain should update
+      // what it "owns".  EG fieldType.indexOptions() should
+      // be updated by maybe FreqProxTermsWriterPerField:
+      return addOrUpdateInternal(name, -1, fieldType.indexed(), false,
+                                 fieldType.omitNorms(), false,
+                                 fieldType.indexOptions(), null, null);
+    }
+
+    private FieldInfo addOrUpdateInternal(String name, int preferredFieldNumber, boolean isIndexed,
+        boolean storeTermVector,
+        boolean omitNorms, boolean storePayloads, IndexOptions indexOptions, DocValues.Type docValues, DocValues.Type normType) {
+      FieldInfo fi = fieldInfo(name);
+      if (fi == null) {
+        // get a global number for this field
+        final int fieldNumber = globalFieldNumbers.addOrGet(name, preferredFieldNumber);
+        fi = addInternal(name, fieldNumber, isIndexed, storeTermVector, omitNorms, storePayloads, indexOptions, docValues, normType);
+      } else {
+        fi.update(isIndexed, storeTermVector, omitNorms, storePayloads, indexOptions);
+        if (docValues != null) {
+          fi.setDocValuesType(docValues);
+        }
+        if (!fi.omitsNorms() && normType != null) {
+          fi.setNormValueType(normType);
+        }
       }
+      return fi;
     }
-    version++;
-    return fi;
-  }
-  
-  synchronized public FieldInfo add(FieldInfo fi) {
-    // IMPORTANT - reuse the field number if possible for consistent field numbers across segments
-    return addOrUpdateInternal(fi.name, fi.number, fi.isIndexed, fi.storeTermVector,
-               fi.omitNorms, fi.storePayloads,
-               fi.indexOptions, fi.getDocValuesType(), fi.getNormType());
-  }
-  
-  /*
-   * NOTE: if you call this method from a public method make sure you check if we are modifiable and throw an exception otherwise
-   */
-  private FieldInfo addInternal(String name, int fieldNumber, boolean isIndexed,
-                                boolean storeTermVector, boolean omitNorms, boolean storePayloads,
-                                IndexOptions indexOptions, DocValues.Type docValuesType, DocValues.Type normType) {
-    // don't check modifiable here since we use that to initially build up FIs
-    if (globalFieldNumbers != null) {
-      globalFieldNumbers.setIfNotSet(fieldNumber, name);
-    } 
-    final FieldInfo fi = new FieldInfo(name, isIndexed, fieldNumber, storeTermVector, omitNorms, storePayloads, indexOptions, docValuesType, normType);
-    putInternal(fi);
-    return fi;
-  }
-
-  /**
-   * lookup the number of a field by name.
-   * 
-   * @param fieldName field's name
-   * @return number of field, or -1 if it does not exist.
-   */
-  public int fieldNumber(String fieldName) {
-    FieldInfo fi = fieldInfo(fieldName);
-    return (fi != null) ? fi.number : -1;
-  }
-
-  public FieldInfo fieldInfo(String fieldName) {
-    return byName.get(fieldName);
-  }
-
-  /**
-   * Return the fieldName identified by its number.
-   * 
-   * @param fieldNumber
-   * @return the fieldName or an empty string when the field
-   * with the given number doesn't exist.
-   */  
-  public String fieldName(int fieldNumber) {
-  	FieldInfo fi = fieldInfo(fieldNumber);
-  	return (fi != null) ? fi.name : "";
-  }
-
-  /**
-   * Return the fieldinfo object referenced by the fieldNumber.
-   * @param fieldNumber
-   * @return the FieldInfo object or null when the given fieldNumber
-   * doesn't exist.
-   */  
-  public FieldInfo fieldInfo(int fieldNumber) {
-    return (fieldNumber >= 0) ? byNumber.get(fieldNumber) : null;
-  }
-
-  public Iterator<FieldInfo> iterator() {
-    return byNumber.values().iterator();
-  }
-
-  /**
-   * @return number of fields
-   */
-  public int size() {
-    assert byNumber.size() == byName.size();
-    return byNumber.size();
-  }
-
-  /**
-   * @return true if at least one field has any vectors
-   */
-  public boolean hasVectors() {
-    if (isReadOnly()) {
-      return hasVectors;
-    }
-    // mutable FIs must check
-    for (FieldInfo fi : this) {
-      if (fi.storeTermVector) {
-        return true;
-      }
+    
+    public FieldInfo add(FieldInfo fi) {
+      // IMPORTANT - reuse the field number if possible for consistent field numbers across segments
+      return addOrUpdateInternal(fi.name, fi.number, fi.isIndexed(), fi.hasVectors(),
+                 fi.omitsNorms(), fi.hasPayloads(),
+                 fi.getIndexOptions(), fi.getDocValuesType(), fi.getNormType());
     }
-    return false;
-  }
-
-  /**
-   * @return true if at least one field has any norms
-   */
-  public boolean hasNorms() {
-    for (FieldInfo fi : this) {
-      if (fi.hasNorms()) {
-        return true;
-      }
+    
+    private FieldInfo addInternal(String name, int fieldNumber, boolean isIndexed,
+                                  boolean storeTermVector, boolean omitNorms, boolean storePayloads,
+                                  IndexOptions indexOptions, DocValues.Type docValuesType, DocValues.Type normType) {
+      globalFieldNumbers.setIfNotSet(fieldNumber, name);
+      final FieldInfo fi = new FieldInfo(name, isIndexed, fieldNumber, storeTermVector, omitNorms, storePayloads, indexOptions, docValuesType, normType, null);
+      putInternal(fi);
+      return fi;
     }
-    return false;
-  }
 
-  /**
-   * Returns <code>true</code> iff this instance is not backed by a
-   * {@link org.apache.lucene.index.FieldInfos.FieldNumberBiMap}. Instances read from a directory via
-   * {@link FieldInfos#FieldInfos(FieldInfo[], boolean, boolean, boolean)} will always be read-only
-   * since no {@link org.apache.lucene.index.FieldInfos.FieldNumberBiMap} is supplied, otherwise 
-   * <code>false</code>.
-   */
-  public final boolean isReadOnly() {
-    return globalFieldNumbers == null;
-  }
-  
-  synchronized final long getVersion() {
-    return version;
-  }
-  
-  final FieldInfos asReadOnly() {
-    if (isReadOnly()) {
-      return this;
+    public FieldInfo fieldInfo(String fieldName) {
+      return byName.get(fieldName);
     }
-    final FieldInfos roFis = new FieldInfos((FieldNumberBiMap)null);
-    for (FieldInfo fieldInfo : this) {
-      FieldInfo clone = fieldInfo.clone();
-      roFis.putInternal(clone);
-      roFis.hasVectors |= clone.storeTermVector;
-      roFis.hasProx |= clone.isIndexed && clone.indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;
-      roFis.hasFreq |= clone.isIndexed && clone.indexOptions != IndexOptions.DOCS_ONLY;
-    }
-    return roFis;
-  }
-
-  /**
-   * @return true if at least one field has docValues
-   */
-  public boolean hasDocValues() {
-    for (FieldInfo fi : this) {
-      if (fi.hasDocValues()) { 
-        return true;
-      }
+    
+    final FieldInfos finish() {
+      return new FieldInfos(byName.values().toArray(new FieldInfo[byName.size()]));
     }
-
-    return false;
-  }
-  
-  /**
-   * Creates a new {@link FieldInfo} instance from the given instance. If the given instance is
-   * read-only this instance will be read-only too.
-   * 
-   * @see #isReadOnly()
-   */
-  static FieldInfos from(FieldInfos other) {
-    return new FieldInfos(other.globalFieldNumbers);
   }
 }


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriter.java lucene4055/lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriter.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriter.java	2012-05-24 16:55:48.472233437 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriter.java	2012-05-24 12:04:35.827929160 -0400
@@ -23,7 +23,6 @@
 import java.util.Map;
 
 import org.apache.lucene.codecs.FieldsConsumer;
-import org.apache.lucene.index.FieldInfo.IndexOptions;
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.CollectionUtil;
 import org.apache.lucene.util.IOUtils;
@@ -39,7 +38,7 @@
   // Other writers would presumably share alot of this...
 
   @Override
-  public void flush(Map<FieldInfo, TermsHashConsumerPerField> fieldsToFlush, final SegmentWriteState state) throws IOException {
+  public void flush(Map<String,TermsHashConsumerPerField> fieldsToFlush, final SegmentWriteState state) throws IOException {
 
     // Gather all FieldData's that have postings, across all
     // ThreadStates
@@ -57,7 +56,7 @@
     // Sort by field name
     CollectionUtil.quickSort(allFields);
 
-    final FieldsConsumer consumer = state.codec.postingsFormat().fieldsConsumer(state);
+    final FieldsConsumer consumer = state.segmentInfo.getCodec().postingsFormat().fieldsConsumer(state);
 
     boolean success = false;
 
@@ -80,13 +79,7 @@
         final FieldInfo fieldInfo = allFields.get(fieldNumber).fieldInfo;
         
         final FreqProxTermsWriterPerField fieldWriter = allFields.get(fieldNumber);
-        
-        // Aggregate the storePayload as seen by the same
-        // field across multiple threads
-        if (fieldInfo.indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0) {
-          fieldInfo.storePayloads |= fieldWriter.hasPayloads;
-        }
-        
+
         // If this field has postings then add them to the
         // segment
         fieldWriter.flush(fieldInfo.name, consumer, state);


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriterPerField.java lucene4055/lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriterPerField.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriterPerField.java	2012-05-24 16:55:48.476233437 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriterPerField.java	2012-05-23 15:17:14.966625848 -0400
@@ -55,7 +55,7 @@
     this.fieldInfo = fieldInfo;
     docState = termsHashPerField.docState;
     fieldState = termsHashPerField.fieldState;
-    setIndexOptions(fieldInfo.indexOptions);
+    setIndexOptions(fieldInfo.getIndexOptions());
   }
 
   @Override
@@ -68,7 +68,11 @@
   }
 
   @Override
-  void finish() {}
+  void finish() {
+    if (hasPayloads) {
+      fieldInfo.setStorePayloads();
+    }
+  }
 
   boolean hasPayloads;
 
@@ -83,7 +87,7 @@
   void reset() {
     // Record, up front, whether our in-RAM format will be
     // with or without term freqs:
-    setIndexOptions(fieldInfo.indexOptions);
+    setIndexOptions(fieldInfo.getIndexOptions());
     payloadAttribute = null;
   }
 
@@ -330,7 +334,7 @@
     // according to this.indexOptions, but then write the
     // new segment to the directory according to
     // currentFieldIndexOptions:
-    final IndexOptions currentFieldIndexOptions = fieldInfo.indexOptions;
+    final IndexOptions currentFieldIndexOptions = fieldInfo.getIndexOptions();
 
     final boolean writeTermFreq = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;
     final boolean writePositions = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;
@@ -363,7 +367,7 @@
     final ByteSliceReader freq = new ByteSliceReader();
     final ByteSliceReader prox = new ByteSliceReader();
 
-    FixedBitSet visitedDocs = new FixedBitSet(state.numDocs);
+    FixedBitSet visitedDocs = new FixedBitSet(state.segmentInfo.getDocCount());
     long sumTotalTermFreq = 0;
     long sumDocFreq = 0;
 
@@ -441,7 +445,7 @@
         }
 
         numDocs++;
-        assert docID < state.numDocs: "doc=" + docID + " maxDoc=" + state.numDocs;
+        assert docID < state.segmentInfo.getDocCount(): "doc=" + docID + " maxDoc=" + state.segmentInfo.getDocCount();
 
         // NOTE: we could check here if the docID was
         // deleted, and skip it.  However, this is somewhat
@@ -463,7 +467,7 @@
           
           // TODO: can we do this reach-around in a cleaner way????
           if (state.liveDocs == null) {
-            state.liveDocs = docState.docWriter.codec.liveDocsFormat().newLiveDocs(state.numDocs);
+            state.liveDocs = docState.docWriter.codec.liveDocsFormat().newLiveDocs(state.segmentInfo.getDocCount());
           }
           if (state.liveDocs.get(docID)) {
             state.delCountOnFlush++;


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/index/IndexFileDeleter.java lucene4055/lucene/core/src/java/org/apache/lucene/index/IndexFileDeleter.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/index/IndexFileDeleter.java	2012-05-24 16:55:48.488233437 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/index/IndexFileDeleter.java	2012-05-23 16:36:44.910708914 -0400
@@ -150,7 +150,7 @@
 
     for (String fileName : files) {
 
-      if ((IndexFileNameFilter.INSTANCE.accept(null, fileName)) && !fileName.endsWith("write.lock") && !fileName.equals(IndexFileNames.SEGMENTS_GEN)) {
+      if (!fileName.endsWith("write.lock") && !fileName.equals(IndexFileNames.SEGMENTS_GEN)) {
 
         // Add this file to refCounts with initial count 0:
         getRefCount(fileName);
@@ -189,30 +189,6 @@
             }
           }
           if (sis != null) {
-            final SegmentInfos infos = sis;
-            for (SegmentInfo segmentInfo : infos) {
-              try {
-                /*
-                 * Force FI to load for each segment since we could see a
-                 * segments file and load successfully above if the files are
-                 * still referenced when they are deleted and the os doesn't let
-                 * you delete them. Yet its likely that fnm files are removed
-                 * while seg file is still around Since LUCENE-2984 we need FI
-                 * to find out if a seg has vectors and prox so we need those
-                 * files to be opened for a commit point.
-                 */
-                segmentInfo.getFieldInfos();
-              } catch (FileNotFoundException e) {
-                refresh(segmentInfo.name);
-                sis = null;
-                if (infoStream.isEnabled("IFD")) {
-                  infoStream.message("IFD", "init: hit FileNotFoundException when loading commit \"" + fileName + "\"; skipping this commit point");
-                }
-              }
-            }
-           
-          }
-          if (sis != null) {
             final CommitPoint commitPoint = new CommitPoint(commitsToDelete, directory, sis);
             if (sis.getGeneration() == segmentInfos.getGeneration()) {
               currentCommitPoint = commitPoint;
@@ -356,7 +332,7 @@
     for(int i=0;i<files.length;i++) {
       String fileName = files[i];
       if ((segmentName == null || fileName.startsWith(segmentPrefix1) || fileName.startsWith(segmentPrefix2)) &&
-          IndexFileNameFilter.INSTANCE.accept(null, fileName) &&
+          !fileName.endsWith("write.lock") &&
           !refCounts.containsKey(fileName) &&
           !fileName.equals(IndexFileNames.SEGMENTS_GEN)) {
         // Unreferenced file, so remove it
@@ -488,7 +464,7 @@
     assert locked();
     // If this is a commit point, also incRef the
     // segments_N file:
-    for( final String fileName: segmentInfos.files(directory, isCommit) ) {
+    for(final String fileName: segmentInfos.files(directory, isCommit)) {
       incRef(fileName);
     }
   }


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/index/IndexFileNameFilter.java lucene4055/lucene/core/src/java/org/apache/lucene/index/IndexFileNameFilter.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/index/IndexFileNameFilter.java	2012-05-24 16:55:48.488233437 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/index/IndexFileNameFilter.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,61 +0,0 @@
-package org.apache.lucene.index;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.File;
-import java.io.FilenameFilter;
-import java.util.regex.Pattern;
-
-/**
- * Filename filter that attempts to accept only filenames
- * created by Lucene.  Note that this is a "best effort"
- * process.  If a file is used in a Lucene index, it will
- * always match the file; but if a file is not used in a
- * Lucene index but is named in a similar way to Lucene's
- * files then this filter may accept the file.
- *
- * <p>This does not accept <code>*-write.lock</code> files.
- *
- * @lucene.internal
- */
-
-public class IndexFileNameFilter implements FilenameFilter {
-
-  public static final FilenameFilter INSTANCE = new IndexFileNameFilter();
-  
-  private IndexFileNameFilter() {
-  }
-
-  // Approximate match for files that seem to be Lucene
-  // index files.  This can easily over-match, ie if some
-  // app names a file _foo_bar.go:
-  private final Pattern luceneFilePattern = Pattern.compile("^_[a-z0-9]+(_[a-z0-9]+)?\\.[a-z0-9]+$");
-
-  /* (non-Javadoc)
-   * @see java.io.FilenameFilter#accept(java.io.File, java.lang.String)
-   */
-  public boolean accept(File dir, String name) {
-    if (name.lastIndexOf('.') != -1) {
-      // Has an extension
-      return luceneFilePattern.matcher(name).matches();
-    } else {
-      // No extension -- only segments_N file;
-      return name.startsWith(IndexFileNames.SEGMENTS);
-    }
-  }
-}


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/index/IndexFileNames.java lucene4055/lucene/core/src/java/org/apache/lucene/index/IndexFileNames.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/index/IndexFileNames.java	2012-05-24 16:55:48.464233437 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/index/IndexFileNames.java	2012-05-22 12:04:43.420920074 -0400
@@ -80,11 +80,12 @@
    * @param gen generation
    */
   public static String fileNameFromGeneration(String base, String ext, long gen) {
-    if (gen == SegmentInfo.NO) {
+    if (gen == -1) {
       return null;
-    } else if (gen == SegmentInfo.WITHOUT_GEN) {
+    } else if (gen == 0) {
       return segmentFileName(base, "", ext);
     } else {
+      assert gen > 0;
       // The '6' part in the length is: 1 for '.', 1 for '_' and 4 as estimate
       // to the gen length as string (hopefully an upper limit so SB won't
       // expand in the middle.


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/index/IndexWriter.java lucene4055/lucene/core/src/java/org/apache/lucene/index/IndexWriter.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/index/IndexWriter.java	2012-05-24 16:55:48.484233437 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/index/IndexWriter.java	2012-05-24 15:09:44.084122604 -0400
@@ -33,8 +33,10 @@
 
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.codecs.Codec;
+import org.apache.lucene.codecs.lucene3x.Lucene3xCodec;
+import org.apache.lucene.codecs.lucene3x.Lucene3xSegmentInfoFormat;
 import org.apache.lucene.index.DocumentsWriterPerThread.FlushedSegment;
-import org.apache.lucene.index.FieldInfos.FieldNumberBiMap;
+import org.apache.lucene.index.FieldInfos.FieldNumbers;
 import org.apache.lucene.index.IndexWriterConfig.OpenMode;
 import org.apache.lucene.index.MergeState.CheckAbort;
 import org.apache.lucene.search.Query;
@@ -46,6 +48,7 @@
 import org.apache.lucene.store.Lock;
 import org.apache.lucene.store.LockObtainFailedException;
 import org.apache.lucene.store.MergeInfo;
+import org.apache.lucene.store.TrackingDirectoryWrapper;
 import org.apache.lucene.util.Bits;
 import org.apache.lucene.util.Constants;
 import org.apache.lucene.util.IOUtils;
@@ -204,7 +207,7 @@
   private volatile long changeCount; // increments every time a change is completed
   private long lastCommitChangeCount; // last changeCount that was committed
 
-  private List<SegmentInfo> rollbackSegments;      // list of segmentInfo we will fallback to if the commit fails
+  private List<SegmentInfoPerCommit> rollbackSegments;      // list of segmentInfo we will fallback to if the commit fails
 
   volatile SegmentInfos pendingCommit;            // set when a commit is pending (after prepareCommit() & before commit())
   volatile long pendingCommitChangeCount;
@@ -212,13 +215,13 @@
   private Collection<String> filesToCommit;
 
   final SegmentInfos segmentInfos;       // the segments
-  final FieldNumberBiMap globalFieldNumberMap;
+  final FieldNumbers globalFieldNumberMap;
 
   private DocumentsWriter docWriter;
   final IndexFileDeleter deleter;
 
   // used by forceMerge to note those needing merging
-  private Map<SegmentInfo,Boolean> segmentsToMerge = new HashMap<SegmentInfo,Boolean>();
+  private Map<SegmentInfoPerCommit,Boolean> segmentsToMerge = new HashMap<SegmentInfoPerCommit,Boolean>();
   private int mergeMaxNumSegments;
 
   private Lock writeLock;
@@ -228,7 +231,7 @@
 
   // Holds all SegmentInfo instances currently involved in
   // merges
-  private HashSet<SegmentInfo> mergingSegments = new HashSet<SegmentInfo>();
+  private HashSet<SegmentInfoPerCommit> mergingSegments = new HashSet<SegmentInfoPerCommit>();
 
   private MergePolicy mergePolicy;
   private final MergeScheduler mergeScheduler;
@@ -400,17 +403,17 @@
 
   class ReaderPool {
     
-    private final Map<SegmentInfo,ReadersAndLiveDocs> readerMap = new HashMap<SegmentInfo,ReadersAndLiveDocs>();
+    private final Map<SegmentInfoPerCommit,ReadersAndLiveDocs> readerMap = new HashMap<SegmentInfoPerCommit,ReadersAndLiveDocs>();
 
     // used only by asserts
-    public synchronized boolean infoIsLive(SegmentInfo info) {
+    public synchronized boolean infoIsLive(SegmentInfoPerCommit info) {
       int idx = segmentInfos.indexOf(info);
       assert idx != -1: "info=" + info + " isn't live";
       assert segmentInfos.info(idx) == info: "info=" + info + " doesn't match live info in segmentInfos";
       return true;
     }
 
-    public synchronized void drop(SegmentInfo info) throws IOException {
+    public synchronized void drop(SegmentInfoPerCommit info) throws IOException {
       final ReadersAndLiveDocs rld = readerMap.get(info);
       if (rld != null) {
         assert info == rld.info;
@@ -446,7 +449,7 @@
     /** Remove all our references to readers, and commits
      *  any pending changes. */
     synchronized void dropAll(boolean doSave) throws IOException {
-      final Iterator<Map.Entry<SegmentInfo,ReadersAndLiveDocs>> it = readerMap.entrySet().iterator();
+      final Iterator<Map.Entry<SegmentInfoPerCommit,ReadersAndLiveDocs>> it = readerMap.entrySet().iterator();
       while(it.hasNext()) {
         final ReadersAndLiveDocs rld = it.next().getValue();
         if (doSave && rld.writeLiveDocs(directory)) {
@@ -479,7 +482,7 @@
      * @throws IOException
      */
     public synchronized void commit(SegmentInfos infos) throws IOException {
-      for (SegmentInfo info : infos) {
+      for (SegmentInfoPerCommit info : infos) {
         final ReadersAndLiveDocs rld = readerMap.get(info);
         if (rld != null) {
           assert rld.info == info;
@@ -500,9 +503,9 @@
      * {@link #release(ReadersAndLiveDocs)}.
      * @throws IOException
      */
-    public synchronized ReadersAndLiveDocs get(SegmentInfo info, boolean create) {
+    public synchronized ReadersAndLiveDocs get(SegmentInfoPerCommit info, boolean create) {
 
-      assert info.dir == directory;
+      assert info.info.dir == directory: "info.dir=" + info.info.dir + " vs " + directory;
 
       ReadersAndLiveDocs rld = readerMap.get(info);
       if (rld == null) {
@@ -530,7 +533,7 @@
    * If the reader isn't being pooled, the segmentInfo's 
    * delCount is returned.
    */
-  public int numDeletedDocs(SegmentInfo info) throws IOException {
+  public int numDeletedDocs(SegmentInfoPerCommit info) throws IOException {
     ensureOpen(false);
     int delCount = info.getDelCount();
 
@@ -658,10 +661,10 @@
         }
       }
 
-      rollbackSegments = segmentInfos.createBackupSegmentInfos(true);
+      rollbackSegments = segmentInfos.createBackupSegmentInfos();
 
       // start with previous field numbers, but new FieldInfos
-      globalFieldNumberMap = segmentInfos.getOrLoadGlobalFieldNumberMap();
+      globalFieldNumberMap = getFieldNumberMap();
       docWriter = new DocumentsWriter(codec, config, directory, this, globalFieldNumberMap, bufferedDeletesStream);
 
       // Default deleter (for backwards compatibility) is
@@ -702,6 +705,54 @@
       }
     }
   }
+
+  private FieldInfos getFieldInfos(SegmentInfo info) throws IOException {
+    Directory cfsDir = null;
+    try {
+      if (info.getUseCompoundFile()) {
+        cfsDir = new CompoundFileDirectory(info.dir,
+                                           IndexFileNames.segmentFileName(info.name, "", IndexFileNames.COMPOUND_FILE_EXTENSION),
+                                           IOContext.READONCE,
+                                           false);
+      } else {
+        cfsDir = info.dir;
+      }
+      return info.getCodec().fieldInfosFormat().getFieldInfosReader().read(cfsDir,
+                                                                                info.name,
+                                                                                IOContext.READONCE);
+    } finally {
+      if (info.getUseCompoundFile() && cfsDir != null) {
+        cfsDir.close();
+      }
+    }
+  }
+
+  /**
+   * Loads or returns the already loaded the global field number map for this {@link SegmentInfos}.
+   * If this {@link SegmentInfos} has no global field number map the returned instance is empty
+   */
+  private FieldNumbers getFieldNumberMap() throws IOException {
+    final FieldNumbers map  = new FieldNumbers();
+
+    SegmentInfoPerCommit biggest = null;
+    for(SegmentInfoPerCommit info : segmentInfos) {
+      if (biggest == null || (info.info.getDocCount()-info.getDelCount()) > (biggest.info.getDocCount()-biggest.getDelCount())) {
+        biggest = info;
+      }
+    }
+
+    if (biggest != null) {
+      for(FieldInfo fi : getFieldInfos(biggest.info)) {
+        map.addOrGet(fi.name, fi.number);
+      }
+    }
+
+    // TODO: we could also pull DV type of each field here,
+    // and use that to make sure new segment(s) don't change
+    // the type...
+
+    return map;
+  }
   
   /**
    * Returns the private {@link IndexWriterConfig}, cloned
@@ -947,8 +998,8 @@
     else
       count = 0;
 
-    for (final SegmentInfo info : segmentInfos) {
-      count += info.docCount - numDeletedDocs(info);
+    for (final SegmentInfoPerCommit info : segmentInfos) {
+      count += info.info.getDocCount() - numDeletedDocs(info);
     }
     return count;
   }
@@ -961,7 +1012,7 @@
     if (docWriter.anyDeletions()) {
       return true;
     }
-    for (final SegmentInfo info : segmentInfos) {
+    for (final SegmentInfoPerCommit info : segmentInfos) {
       if (info.hasDeletions()) {
         return true;
       }
@@ -1316,7 +1367,7 @@
   // for test purpose
   final synchronized int getDocCount(int i) {
     if (i >= 0 && i < segmentInfos.size()) {
-      return segmentInfos.info(i).docCount;
+      return segmentInfos.info(i).info.getDocCount();
     } else {
       return -1;
     }
@@ -1445,18 +1496,19 @@
     synchronized(this) {
       resetMergeExceptions();
       segmentsToMerge.clear();
-      for(SegmentInfo info : segmentInfos) {
+      for(SegmentInfoPerCommit info : segmentInfos) {
         segmentsToMerge.put(info, Boolean.TRUE);
       }
       mergeMaxNumSegments = maxNumSegments;
 
-      // Now mark all pending & running merges as isMaxNumSegments:
+      // Now mark all pending & running merges for forced
+      // merge:
       for(final MergePolicy.OneMerge merge  : pendingMerges) {
         merge.maxNumSegments = maxNumSegments;
         segmentsToMerge.put(merge.info, Boolean.TRUE);
       }
 
-      for ( final MergePolicy.OneMerge merge: runningMerges ) {
+      for (final MergePolicy.OneMerge merge: runningMerges) {
         merge.maxNumSegments = maxNumSegments;
         segmentsToMerge.put(merge.info, Boolean.TRUE);
       }
@@ -1577,8 +1629,9 @@
           running = false;
           for(int i=0;i<numMerges;i++) {
             final MergePolicy.OneMerge merge = spec.merges.get(i);
-            if (pendingMerges.contains(merge) || runningMerges.contains(merge))
+            if (pendingMerges.contains(merge) || runningMerges.contains(merge)) {
               running = true;
+            }
             Throwable t = merge.getException();
             if (t != null) {
               IOException ioe = new IOException("background merge hit exception: " + merge.segString(directory));
@@ -1695,7 +1748,7 @@
    *  MergePolicy).
    *
    *  <p>Do not alter the returned collection! */
-  public synchronized Collection<SegmentInfo> getMergingSegments() {
+  public synchronized Collection<SegmentInfoPerCommit> getMergingSegments() {
     return mergingSegments;
   }
 
@@ -1706,9 +1759,9 @@
    * @lucene.experimental
    */
   public synchronized MergePolicy.OneMerge getNextMerge() {
-    if (pendingMerges.size() == 0)
+    if (pendingMerges.size() == 0) {
       return null;
-    else {
+    } else {
       // Advance the merge from pending to running
       MergePolicy.OneMerge merge = pendingMerges.removeFirst();
       runningMerges.add(merge);
@@ -1961,50 +2014,43 @@
    * 
    * @see #publishFlushedSegment(SegmentInfo, FrozenBufferedDeletes, FrozenBufferedDeletes)
    */
-  SegmentInfo prepareFlushedSegment(FlushedSegment flushedSegment) throws IOException {
+  SegmentInfoPerCommit prepareFlushedSegment(FlushedSegment flushedSegment) throws IOException {
     assert flushedSegment != null;
 
-    SegmentInfo newSegment = flushedSegment.segmentInfo;
+    SegmentInfoPerCommit newSegment = flushedSegment.segmentInfo;
 
-    setDiagnostics(newSegment, "flush");
+    setDiagnostics(newSegment.info, "flush");
     
-    IOContext context = new IOContext(new FlushInfo(newSegment.docCount, newSegment.sizeInBytes()));
+    IOContext context = new IOContext(new FlushInfo(newSegment.info.getDocCount(), newSegment.info.sizeInBytes()));
 
     boolean success = false;
     try {
       if (useCompoundFile(newSegment)) {
-        String compoundFileName = IndexFileNames.segmentFileName(newSegment.name, "", IndexFileNames.COMPOUND_FILE_EXTENSION);
-        if (infoStream.isEnabled("IW")) {
-          infoStream.message("IW", "creating compound file " + compoundFileName);
-        }
+
         // Now build compound file
-        final Directory cfsDir = new CompoundFileDirectory(directory, compoundFileName, context, true);
-        IOException prior = null;
-        try {
-          for(String fileName : newSegment.files()) {
-            directory.copy(cfsDir, fileName, fileName, context);
-          }
-        } catch(IOException ex) {
-          prior = ex;
-        } finally {
-          IOUtils.closeWhileHandlingException(prior, cfsDir);
-        }
-        // Perform the merge
+        Collection<String> oldFiles = createCompoundFile(infoStream, directory, MergeState.CheckAbort.NONE, newSegment.info, context);
+        newSegment.info.setUseCompoundFile(true);
         
         synchronized(this) {
-          deleter.deleteNewFiles(newSegment.files());
+          deleter.deleteNewFiles(oldFiles);
         }
-
-        newSegment.setUseCompoundFile(true);
       }
 
+      // Have codec write SegmentInfo.  Must do this after
+      // creating CFS so that 1) .si isn't slurped into CFS,
+      // and 2) .si reflects useCompoundFile=true change
+      // above:
+      codec.segmentInfoFormat().getSegmentInfosWriter().write(directory, newSegment.info, flushedSegment.fieldInfos, context);
+
+      // TODO: ideally we would freeze newSegment here!!
+      // because any changes after writing the .si will be
+      // lost... 
+
       // Must write deleted docs after the CFS so we don't
       // slurp the del file into CFS:
       if (flushedSegment.liveDocs != null) {
         final int delCount = flushedSegment.delCount;
         assert delCount > 0;
-        newSegment.setDelCount(delCount);
-        newSegment.advanceDelGen();
         if (infoStream.isEnabled("IW")) {
           infoStream.message("IW", "flush: write " + delCount + " deletes gen=" + flushedSegment.segmentInfo.getDelGen());
         }
@@ -2015,9 +2061,11 @@
         // carry the changes; there's no reason to use
         // filesystem as intermediary here.
           
-        SegmentInfo info = flushedSegment.segmentInfo;
-        Codec codec = info.getCodec();
-        codec.liveDocsFormat().writeLiveDocs(flushedSegment.liveDocs, directory, info, context);
+        SegmentInfoPerCommit info = flushedSegment.segmentInfo;
+        Codec codec = info.info.getCodec();
+        codec.liveDocsFormat().writeLiveDocs(flushedSegment.liveDocs, directory, info, delCount, context);
+        newSegment.setDelCount(delCount);
+        newSegment.advanceDelGen();
       }
 
       success = true;
@@ -2025,11 +2073,11 @@
       if (!success) {
         if (infoStream.isEnabled("IW")) {
           infoStream.message("IW", "hit exception " +
-              "reating compound file for newly flushed segment " + newSegment.name);
+              "reating compound file for newly flushed segment " + newSegment.info.name);
         }
 
         synchronized(this) {
-          deleter.refresh(newSegment.name);
+          deleter.refresh(newSegment.info.name);
         }
       }
     }
@@ -2051,7 +2099,7 @@
    * 
    * @see #prepareFlushedSegment(FlushedSegment)
    */
-  synchronized void publishFlushedSegment(SegmentInfo newSegment,
+  synchronized void publishFlushedSegment(SegmentInfoPerCommit newSegment,
       FrozenBufferedDeletes packet, FrozenBufferedDeletes globalPacket) throws IOException {
     // Lock order IW -> BDS
     synchronized (bufferedDeletesStream) {
@@ -2081,7 +2129,7 @@
     }
   }
 
-  synchronized boolean useCompoundFile(SegmentInfo segmentInfo) throws IOException {
+  synchronized boolean useCompoundFile(SegmentInfoPerCommit segmentInfo) throws IOException {
     return mergePolicy.useCompoundFile(segmentInfos, segmentInfo);
   }
 
@@ -2157,7 +2205,7 @@
 
       flush(false, true);
 
-      List<SegmentInfo> infos = new ArrayList<SegmentInfo>();
+      List<SegmentInfoPerCommit> infos = new ArrayList<SegmentInfoPerCommit>();
       for (Directory dir : dirs) {
         if (infoStream.isEnabled("IW")) {
           infoStream.message("IW", "addIndexes: process directory " + dir);
@@ -2167,22 +2215,19 @@
         final Set<String> dsFilesCopied = new HashSet<String>();
         final Map<String, String> dsNames = new HashMap<String, String>();
         final Set<String> copiedFiles = new HashSet<String>();
-
-        for (SegmentInfo info : sis) {
-          assert !infos.contains(info): "dup info dir=" + info.dir + " name=" + info.name;
+        for (SegmentInfoPerCommit info : sis) {
+          assert !infos.contains(info): "dup info dir=" + info.info.dir + " name=" + info.info.name;
 
           String newSegName = newSegmentName();
-          String dsName = info.getDocStoreSegment();
+          String dsName = Lucene3xSegmentInfoFormat.getDocStoreSegment(info.info);
 
           if (infoStream.isEnabled("IW")) {
-            infoStream.message("IW", "addIndexes: process segment origName=" + info.name + " newName=" + newSegName + " dsName=" + dsName + " info=" + info);
+            infoStream.message("IW", "addIndexes: process segment origName=" + info.info.name + " newName=" + newSegName + " dsName=" + dsName + " info=" + info);
           }
 
-          IOContext context = new IOContext(new MergeInfo(info.docCount, info.sizeInBytes(), true, -1));
+          IOContext context = new IOContext(new MergeInfo(info.info.getDocCount(), info.info.sizeInBytes(), true, -1));
           
-          copySegmentAsIs(info, newSegName, dsNames, dsFilesCopied, context, copiedFiles);
-
-          infos.add(info);
+          infos.add(copySegmentAsIs(info, newSegName, dsNames, dsFilesCopied, context, copiedFiles));
         }
       }
 
@@ -2233,47 +2278,63 @@
       String mergedName = newSegmentName();
       for (IndexReader indexReader : readers) {
         numDocs += indexReader.numDocs();
-       }
-       final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));
+      }
+      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));
 
       // TODO: somehow we should fix this merge so it's
       // abortable so that IW.close(false) is able to stop it
-      SegmentMerger merger = new SegmentMerger(infoStream, directory, config.getTermIndexInterval(),
-                                               mergedName, MergeState.CheckAbort.NONE, payloadProcessorProvider,
-                                               new FieldInfos(globalFieldNumberMap), codec, context);
+      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);
+
+      SegmentInfo info = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergedName, -1,
+                                         false, codec, null, null);
 
-      for (IndexReader reader : readers)      // add new indexes
+      SegmentMerger merger = new SegmentMerger(info, infoStream, trackingDir, config.getTermIndexInterval(),
+                                               MergeState.CheckAbort.NONE, payloadProcessorProvider,
+                                               globalFieldNumberMap, context);
+
+      for (IndexReader reader : readers) {    // add new indexes
         merger.add(reader);
+      }
+
       MergeState mergeState = merger.merge();                // merge 'em
-      int docCount = mergeState.mergedDocCount;
-      final FieldInfos fieldInfos = mergeState.fieldInfos;
-      SegmentInfo info = new SegmentInfo(mergedName, docCount, directory,
-                                         false, codec,
-                                         fieldInfos);
+
+      SegmentInfoPerCommit infoPerCommit = new SegmentInfoPerCommit(info, 0, -1L);
+
+      info.setFiles(new HashSet<String>(trackingDir.getCreatedFiles()));
+      trackingDir.getCreatedFiles().clear();
+                                         
       setDiagnostics(info, "addIndexes(IndexReader...)");
 
       boolean useCompoundFile;
       synchronized(this) { // Guard segmentInfos
         if (stopMerges) {
-          deleter.deleteNewFiles(info.files());
+          deleter.deleteNewFiles(infoPerCommit.files());
           return;
         }
         ensureOpen();
-        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, info);
+        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit);
       }
 
       // Now create the compound file if needed
       if (useCompoundFile) {
-        createCompoundFile(directory, IndexFileNames.segmentFileName(mergedName, "", IndexFileNames.COMPOUND_FILE_EXTENSION), MergeState.CheckAbort.NONE, info, context);
+        Collection<String> filesToDelete = infoPerCommit.files();
+        createCompoundFile(infoStream, directory, MergeState.CheckAbort.NONE, info, context);
 
         // delete new non cfs files directly: they were never
         // registered with IFD
         synchronized(this) {
-          deleter.deleteNewFiles(info.files());
+          deleter.deleteNewFiles(filesToDelete);
         }
         info.setUseCompoundFile(true);
       }
 
+      // Have codec write SegmentInfo.  Must do this after
+      // creating CFS so that 1) .si isn't slurped into CFS,
+      // and 2) .si reflects useCompoundFile=true change
+      // above:
+      codec.segmentInfoFormat().getSegmentInfosWriter().write(trackingDir, info, mergeState.fieldInfos, context);
+      info.addFiles(trackingDir.getCreatedFiles());
+
       // Register the new segment
       synchronized(this) {
         if (stopMerges) {
@@ -2281,7 +2342,7 @@
           return;
         }
         ensureOpen();
-        segmentInfos.add(info);
+        segmentInfos.add(infoPerCommit);
         checkpoint();
       }
     } catch (OutOfMemoryError oom) {
@@ -2290,15 +2351,15 @@
   }
 
   /** Copies the segment files as-is into the IndexWriter's directory. */
-  private void copySegmentAsIs(SegmentInfo info, String segName,
-                               Map<String, String> dsNames, Set<String> dsFilesCopied, IOContext context,
-                               Set<String> copiedFiles)
+  private SegmentInfoPerCommit copySegmentAsIs(SegmentInfoPerCommit info, String segName,
+                                               Map<String, String> dsNames, Set<String> dsFilesCopied, IOContext context,
+                                               Set<String> copiedFiles)
       throws IOException {
     // Determine if the doc store of this segment needs to be copied. It's
     // only relevant for segments that share doc store with others,
     // because the DS might have been copied already, in which case we
     // just want to update the DS name of this SegmentInfo.
-    String dsName = info.getDocStoreSegment();
+    final String dsName = Lucene3xSegmentInfoFormat.getDocStoreSegment(info.info);
     assert dsName != null;
     final String newDsName;
     if (dsNames.containsKey(dsName)) {
@@ -2307,18 +2368,68 @@
       dsNames.put(dsName, segName);
       newDsName = segName;
     }
+
+    // note: we don't really need this fis (its copied), but we load it up
+    // so we don't pass a null value to the si writer
+    FieldInfos fis = getFieldInfos(info.info);
     
-    Set<String> codecDocStoreFiles = new HashSet<String>();
-    if (info.getDocStoreOffset() != -1) {
-      // only violate the codec this way if its preflex
-      info.getCodec().storedFieldsFormat().files(info, codecDocStoreFiles);
-      info.getCodec().termVectorsFormat().files(info, codecDocStoreFiles);
+    Set<String> docStoreFiles3xOnly = Lucene3xCodec.getDocStoreFiles(info.info);
+
+    final Map<String,String> attributes;
+    // copy the attributes map, we might modify it below.
+    // also we need to ensure its read-write, since we will invoke the SIwriter (which might want to set something).
+    if (info.info.attributes() == null) {
+      attributes = new HashMap<String,String>();
+    } else {
+      attributes = new HashMap<String,String>(info.info.attributes());
     }
+    if (docStoreFiles3xOnly != null) {
+      // only violate the codec this way if it's preflex &
+      // shares doc stores
+      // change docStoreSegment to newDsName
+      attributes.put(Lucene3xSegmentInfoFormat.DS_NAME_KEY, newDsName);
+    }
+
+    //System.out.println("copy seg=" + info.info.name + " version=" + info.info.getVersion());
+    // Same SI as before but we change directory, name and docStoreSegment:
+    SegmentInfo newInfo = new SegmentInfo(directory, info.info.getVersion(), segName, info.info.getDocCount(),
+                                          info.info.getUseCompoundFile(),
+                                          info.info.getCodec(), info.info.getDiagnostics(), attributes);
+    SegmentInfoPerCommit newInfoPerCommit = new SegmentInfoPerCommit(newInfo, info.getDelCount(), info.getDelGen());
+
+    Set<String> segFiles = new HashSet<String>();
 
-    // Copy the segment files
+    // Build up new segment's file names.  Must do this
+    // before writing SegmentInfo:
     for (String file: info.files()) {
       final String newFileName;
-      if (codecDocStoreFiles.contains(file)) {
+      if (docStoreFiles3xOnly != null && docStoreFiles3xOnly.contains(file)) {
+        newFileName = newDsName + IndexFileNames.stripSegmentName(file);
+      } else {
+        newFileName = segName + IndexFileNames.stripSegmentName(file);
+      }
+      segFiles.add(newFileName);
+    }
+    newInfo.setFiles(segFiles);
+
+    // We must rewrite the SI file because it references
+    // segment name (its own name, if its 3.x, and doc
+    // store segment name):
+    TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);
+    try {
+      newInfo.getCodec().segmentInfoFormat().getSegmentInfosWriter().write(trackingDir, newInfo, fis, context);
+    } catch (UnsupportedOperationException uoe) {
+      // OK: 3x codec cannot write a new SI file;
+      // SegmentInfos will write this on commit
+    }
+
+    final Collection<String> siFiles = trackingDir.getCreatedFiles();
+
+    // Copy the segment's files
+    for (String file: info.files()) {
+
+      final String newFileName;
+      if (docStoreFiles3xOnly != null && docStoreFiles3xOnly.contains(file)) {
         newFileName = newDsName + IndexFileNames.stripSegmentName(file);
         if (dsFilesCopied.contains(newFileName)) {
           continue;
@@ -2327,16 +2438,19 @@
       } else {
         newFileName = segName + IndexFileNames.stripSegmentName(file);
       }
-      
-      assert !directory.fileExists(newFileName): "file \"" + newFileName + "\" already exists";
+
+      if (siFiles.contains(newFileName)) {
+        // We already rewrote this above
+        continue;
+      }
+
+      assert !directory.fileExists(newFileName): "file \"" + newFileName + "\" already exists; siFiles=" + siFiles;
       assert !copiedFiles.contains(file): "file \"" + file + "\" is being copied more than once";
       copiedFiles.add(file);
-      info.dir.copy(directory, file, newFileName, context);
+      info.info.dir.copy(directory, file, newFileName, context);
     }
     
-    info.setDocStore(info.getDocStoreOffset(), newDsName, info.getDocStoreIsCompoundFile());
-    info.dir = directory;
-    info.name = segName;
+    return newInfoPerCommit;
   }
   
   /**
@@ -2575,14 +2689,14 @@
         if (infoStream.isEnabled("IW")) {
           infoStream.message("IW", "commit: pendingCommit != null");
         }
-        pendingCommit.finishCommit(directory, codec);
+        pendingCommit.finishCommit(directory);
         if (infoStream.isEnabled("IW")) {
           infoStream.message("IW", "commit: wrote segments file \"" + pendingCommit.getSegmentsFileName() + "\"");
         }
         lastCommitChangeCount = pendingCommitChangeCount;
         segmentInfos.updateGeneration(pendingCommit);
         segmentInfos.setUserData(pendingCommit.getUserData());
-        rollbackSegments = pendingCommit.createBackupSegmentInfos(true);
+        rollbackSegments = pendingCommit.createBackupSegmentInfos();
         deleter.checkpoint(pendingCommit, true);
       } finally {
         // Matches the incRef done in prepareCommit:
@@ -2700,7 +2814,7 @@
       if (infoStream.isEnabled("IW")) {
         infoStream.message("IW", "drop 100% deleted segments: " + segString(result.allDeleted));
       }
-      for (SegmentInfo info : result.allDeleted) {
+      for (SegmentInfoPerCommit info : result.allDeleted) {
         // If a merge has already registered for this
         // segment, we leave it in the readerPool; the
         // merge will skip merging it and will then drop
@@ -2738,9 +2852,9 @@
   }
 
   private synchronized void ensureValidMerge(MergePolicy.OneMerge merge) throws IOException {
-    for(SegmentInfo info : merge.segments) {
+    for(SegmentInfoPerCommit info : merge.segments) {
       if (!segmentInfos.contains(info)) {
-        throw new MergePolicy.MergeException("MergePolicy selected a segment (" + info.name + ") that is not in the current index " + segString(), directory);
+        throw new MergePolicy.MergeException("MergePolicy selected a segment (" + info.info.name + ") that is not in the current index " + segString(), directory);
       }
     }
   }
@@ -2758,7 +2872,7 @@
 
     assert testPoint("startCommitMergeDeletes");
 
-    final List<SegmentInfo> sourceSegments = merge.segments;
+    final List<SegmentInfoPerCommit> sourceSegments = merge.segments;
 
     if (infoStream.isEnabled("IW")) {
       infoStream.message("IW", "commitMergeDeletes " + segString(merge.segments));
@@ -2773,14 +2887,14 @@
     ReadersAndLiveDocs mergedDeletes = null;
 
     for(int i=0; i < sourceSegments.size(); i++) {
-      SegmentInfo info = sourceSegments.get(i);
+      SegmentInfoPerCommit info = sourceSegments.get(i);
       minGen = Math.min(info.getBufferedDeletesGen(), minGen);
-      final int docCount = info.docCount;
+      final int docCount = info.info.getDocCount();
       final Bits prevLiveDocs = merge.readerLiveDocs.get(i);
       final Bits currentLiveDocs;
       final ReadersAndLiveDocs rld = readerPool.get(info, false);
       // We hold a ref so it should still be in the pool:
-      assert rld != null: "seg=" + info.name;
+      assert rld != null: "seg=" + info.info.name;
       currentLiveDocs = rld.getLiveDocs();
 
       if (prevLiveDocs != null) {
@@ -2823,7 +2937,7 @@
             }
           }
         } else {
-          docUpto += info.docCount - info.getDelCount() - rld.getPendingDeleteCount();
+          docUpto += info.info.getDocCount() - info.getDelCount() - rld.getPendingDeleteCount();
         }
       } else if (currentLiveDocs != null) {
         assert currentLiveDocs.length() == docCount;
@@ -2841,11 +2955,11 @@
         }
       } else {
         // No deletes before or after
-        docUpto += info.docCount;
+        docUpto += info.info.getDocCount();
       }
     }
 
-    assert docUpto == merge.info.docCount;
+    assert docUpto == merge.info.info.getDocCount();
 
     if (infoStream.isEnabled("IW")) {
       if (mergedDeletes == null) {
@@ -2894,7 +3008,7 @@
       return false;
     }
 
-    final ReadersAndLiveDocs mergedDeletes =  merge.info.docCount == 0 ? null : commitMergedDeletes(merge);
+    final ReadersAndLiveDocs mergedDeletes =  merge.info.info.getDocCount() == 0 ? null : commitMergedDeletes(merge);
 
     assert mergedDeletes == null || mergedDeletes.getPendingDeleteCount() != 0;
 
@@ -2906,9 +3020,9 @@
     assert !segmentInfos.contains(merge.info);
 
     final boolean allDeleted = merge.segments.size() == 0 ||
-      merge.info.docCount == 0 ||
+      merge.info.info.getDocCount() == 0 ||
       (mergedDeletes != null &&
-       mergedDeletes.getPendingDeleteCount() == merge.info.docCount);
+       mergedDeletes.getPendingDeleteCount() == merge.info.info.getDocCount());
 
     if (infoStream.isEnabled("IW")) {
       if (allDeleted) {
@@ -2922,7 +3036,7 @@
     // the new segment:
     assert merge.segments.size() > 0 || dropSegment;
 
-    assert merge.info.docCount != 0 || keepFullyDeletedSegments || dropSegment;
+    assert merge.info.info.getDocCount() != 0 || keepFullyDeletedSegments || dropSegment;
 
     segmentInfos.applyMergeChanges(merge, dropSegment);
 
@@ -3028,7 +3142,7 @@
               infoStream.message("IW", "hit exception during merge");
             }
             if (merge.info != null && !segmentInfos.contains(merge.info)) {
-              deleter.refresh(merge.info.name);
+              deleter.refresh(merge.info.info.name);
             }
           }
 
@@ -3043,9 +3157,9 @@
     } catch (OutOfMemoryError oom) {
       handleOOM(oom, "merge");
     }
-    if (merge.info != null) {
+    if (merge.info != null && !merge.isAborted()) {
       if (infoStream.isEnabled("IW")) {
-        infoStream.message("IW", "merge time " + (System.currentTimeMillis()-t0) + " msec for " + merge.info.docCount + " docs");
+        infoStream.message("IW", "merge time " + (System.currentTimeMillis()-t0) + " msec for " + merge.info.info.getDocCount() + " docs");
       }
     }
   }
@@ -3073,14 +3187,14 @@
     }
 
     boolean isExternal = false;
-    for(SegmentInfo info : merge.segments) {
+    for(SegmentInfoPerCommit info : merge.segments) {
       if (mergingSegments.contains(info)) {
         return false;
       }
       if (!segmentInfos.contains(info)) {
         return false;
       }
-      if (info.dir != directory) {
+      if (info.info.dir != directory) {
         isExternal = true;
       }
       if (segmentsToMerge.containsKey(info)) {
@@ -3105,8 +3219,8 @@
     // threads, start
     if (infoStream.isEnabled("IW")) {
       StringBuilder builder = new StringBuilder("registerMerge merging= [");
-      for (SegmentInfo info : mergingSegments) {
-        builder.append(info.name).append(", ");  
+      for (SegmentInfoPerCommit info : mergingSegments) {
+        builder.append(info.info.name).append(", ");  
       }
       builder.append("]");
       // don't call mergingSegments.toString() could lead to ConcurrentModException
@@ -3115,7 +3229,7 @@
         infoStream.message("IW", builder.toString());  
       }
     }
-    for(SegmentInfo info : merge.segments) {
+    for(SegmentInfoPerCommit info : merge.segments) {
       if (infoStream.isEnabled("IW")) {
         infoStream.message("IW", "registerMerge info=" + info);
       }
@@ -3156,20 +3270,14 @@
       throw new IllegalStateException("this writer hit an OutOfMemoryError; cannot merge");
     }
 
-    // TODO: is there any perf benefit to sorting
-    // merged segments?  eg biggest to smallest?
-
-    if (merge.info != null)
+    if (merge.info != null) {
       // mergeInit already done
       return;
+    }
 
-    if (merge.isAborted())
+    if (merge.isAborted()) {
       return;
-
-    // Bind a new segment name here so even with
-    // ConcurrentMergePolicy we keep deterministic segment
-    // names.
-    merge.info = new SegmentInfo(newSegmentName(), 0, directory, false, null, new FieldInfos(globalFieldNumberMap));
+    }
 
     // TODO: in the non-pool'd case this is somewhat
     // wasteful, because we open these readers, close them,
@@ -3187,7 +3295,7 @@
       if (infoStream.isEnabled("IW")) {
         infoStream.message("IW", "drop 100% deleted segments: " + result.allDeleted);
       }
-      for(SegmentInfo info : result.allDeleted) {
+      for(SegmentInfoPerCommit info : result.allDeleted) {
         segmentInfos.remove(info);
         if (merge.segments.contains(info)) {
           mergingSegments.remove(info);
@@ -3198,38 +3306,33 @@
       checkpoint();
     }
 
-    merge.info.setBufferedDeletesGen(result.gen);
-
-    // Lock order: IW -> BD
-    bufferedDeletesStream.prune(segmentInfos);
     Map<String,String> details = new HashMap<String,String>();
     details.put("mergeMaxNumSegments", ""+merge.maxNumSegments);
     details.put("mergeFactor", Integer.toString(merge.segments.size()));
-    setDiagnostics(merge.info, "merge", details);
+
+    // Bind a new segment name here so even with
+    // ConcurrentMergePolicy we keep deterministic segment
+    // names.
+    final String mergeSegmentName = newSegmentName();
+    SegmentInfo si = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergeSegmentName, -1, false, codec, details, null);
+    merge.info = new SegmentInfoPerCommit(si, 0, -1L);
+
+    // Lock order: IW -> BD
+    bufferedDeletesStream.prune(segmentInfos);
 
     if (infoStream.isEnabled("IW")) {
-      infoStream.message("IW", "merge seg=" + merge.info.name);
+      infoStream.message("IW", "merge seg=" + merge.info.info.name);
     }
 
     assert merge.estimatedMergeBytes == 0;
-    for(SegmentInfo info : merge.segments) {
-      if (info.docCount > 0) {
+    for(SegmentInfoPerCommit info : merge.segments) {
+      if (info.info.getDocCount() > 0) {
         final int delCount = numDeletedDocs(info);
-        assert delCount <= info.docCount;
-        final double delRatio = ((double) delCount)/info.docCount;
-        merge.estimatedMergeBytes += info.sizeInBytes() * (1.0 - delRatio);
+        assert delCount <= info.info.getDocCount();
+        final double delRatio = ((double) delCount)/info.info.getDocCount();
+        merge.estimatedMergeBytes += info.info.sizeInBytes() * (1.0 - delRatio);
       }
     }
-
-    // TODO: I think this should no longer be needed (we
-    // now build CFS before adding segment to the infos);
-    // however, on removing it, tests fail for some reason!
-
-    // Also enroll the merged segment into mergingSegments;
-    // this prevents it from getting selected for a merge
-    // after our merge is done but while we are building the
-    // CFS:
-    mergingSegments.add(merge.info);
   }
 
   static void setDiagnostics(SegmentInfo info, String source) {
@@ -3262,13 +3365,10 @@
     // It's possible we are called twice, eg if there was an
     // exception inside mergeInit
     if (merge.registerDone) {
-      final List<SegmentInfo> sourceSegments = merge.segments;
-      for(SegmentInfo info : sourceSegments) {
+      final List<SegmentInfoPerCommit> sourceSegments = merge.segments;
+      for(SegmentInfoPerCommit info : sourceSegments) {
         mergingSegments.remove(info);
       }
-      // TODO: if we remove the add in _mergeInit, we should
-      // also remove this:
-      mergingSegments.remove(merge.info);
       merge.registerDone = false;
     }
 
@@ -3322,20 +3422,20 @@
 
     merge.checkAborted(directory);
 
-    final String mergedName = merge.info.name;
+    final String mergedName = merge.info.info.name;
 
-    int mergedDocCount = 0;
-
-    List<SegmentInfo> sourceSegments = merge.segments;
+    List<SegmentInfoPerCommit> sourceSegments = merge.segments;
     
     IOContext context = new IOContext(merge.getMergeInfo());
 
     final MergeState.CheckAbort checkAbort = new MergeState.CheckAbort(merge, directory);
-    SegmentMerger merger = new SegmentMerger(infoStream, directory, config.getTermIndexInterval(), mergedName, checkAbort,
-                                             payloadProcessorProvider, merge.info.getFieldInfos(), codec, context);
+    final TrackingDirectoryWrapper dirWrapper = new TrackingDirectoryWrapper(directory);
+
+    SegmentMerger merger = new SegmentMerger(merge.info.info, infoStream, dirWrapper, config.getTermIndexInterval(), checkAbort,
+                                             payloadProcessorProvider, globalFieldNumberMap, context);
 
     if (infoStream.isEnabled("IW")) {
-      infoStream.message("IW", "merging " + segString(merge.segments) + " mergeVectors=" + merge.info.getFieldInfos().hasVectors());
+      infoStream.message("IW", "merging " + segString(merge.segments));
     }
 
     merge.readers = new ArrayList<SegmentReader>();
@@ -3348,7 +3448,7 @@
       int segUpto = 0;
       while(segUpto < sourceSegments.size()) {
 
-        final SegmentInfo info = sourceSegments.get(segUpto);
+        final SegmentInfoPerCommit info = sourceSegments.get(segUpto);
 
         // Hold onto the "live" reader; we will use this to
         // commit merged deletes
@@ -3381,8 +3481,8 @@
         }
         merge.readerLiveDocs.add(liveDocs);
         merge.readers.add(reader);
-        assert delCount <= info.docCount: "delCount=" + delCount + " info.docCount=" + info.docCount + " rld.pendingDeleteCount=" + rld.getPendingDeleteCount() + " info.getDelCount()=" + info.getDelCount();
-        if (delCount < info.docCount) {
+        assert delCount <= info.info.getDocCount(): "delCount=" + delCount + " info.docCount=" + info.info.getDocCount() + " rld.pendingDeleteCount=" + rld.getPendingDeleteCount() + " info.getDelCount()=" + info.getDelCount();
+        if (delCount < info.info.getDocCount()) {
           merger.add(reader, liveDocs);
         }
         segUpto++;
@@ -3392,13 +3492,18 @@
 
       // This is where all the work happens:
       MergeState mergeState = merger.merge();
-      mergedDocCount = merge.info.docCount = mergeState.mergedDocCount;
+      assert mergeState.segmentInfo == merge.info.info;
+      merge.info.info.setFiles(new HashSet<String>(dirWrapper.getCreatedFiles()));
 
       // Record which codec was used to write the segment
-      merge.info.setCodec(codec);
 
       if (infoStream.isEnabled("IW")) {
-        infoStream.message("IW", "merge codec=" + codec + " docCount=" + mergedDocCount);
+        infoStream.message("IW", "merge codec=" + codec + " docCount=" + merge.info.info.getDocCount() + "; merged segment has " +
+                           (mergeState.fieldInfos.hasVectors() ? "vectors" : "no vectors") + "; " +
+                           (mergeState.fieldInfos.hasNorms() ? "norms" : "no norms") + "; " + 
+                           (mergeState.fieldInfos.hasDocValues() ? "docValues" : "no docValues") + "; " + 
+                           (mergeState.fieldInfos.hasProx() ? "prox" : "no prox") + "; " + 
+                           (mergeState.fieldInfos.hasProx() ? "freqs" : "no freqs"));
       }
 
       // Very important to do this before opening the reader
@@ -3409,16 +3514,14 @@
       synchronized (this) { // Guard segmentInfos
         useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info);
       }
-      
+
       if (useCompoundFile) {
         success = false;
-        final String compoundFileName = IndexFileNames.segmentFileName(mergedName, "", IndexFileNames.COMPOUND_FILE_EXTENSION);
+
+        Collection<String> filesToRemove = merge.info.files();
 
         try {
-          if (infoStream.isEnabled("IW")) {
-            infoStream.message("IW", "create compound file " + compoundFileName);
-          }
-          createCompoundFile(directory, compoundFileName, checkAbort, merge.info, new IOContext(merge.getMergeInfo()));
+          filesToRemove = createCompoundFile(infoStream, directory, checkAbort, merge.info.info, context);
           success = true;
         } catch (IOException ioe) {
           synchronized(this) {
@@ -3439,35 +3542,64 @@
             }
 
             synchronized(this) {
-              deleter.deleteFile(compoundFileName);
+              deleter.deleteFile(IndexFileNames.segmentFileName(mergedName, "", IndexFileNames.COMPOUND_FILE_EXTENSION));
               deleter.deleteFile(IndexFileNames.segmentFileName(mergedName, "", IndexFileNames.COMPOUND_FILE_ENTRIES_EXTENSION));
               deleter.deleteNewFiles(merge.info.files());
             }
           }
         }
 
+        // So that, if we hit exc in deleteNewFiles (next)
+        // or in commitMerge (later), we close the
+        // per-segment readers in the finally clause below:
         success = false;
 
         synchronized(this) {
 
           // delete new non cfs files directly: they were never
           // registered with IFD
-          deleter.deleteNewFiles(merge.info.files());
+          deleter.deleteNewFiles(filesToRemove);
 
           if (merge.isAborted()) {
             if (infoStream.isEnabled("IW")) {
               infoStream.message("IW", "abort merge after building CFS");
             }
-            deleter.deleteFile(compoundFileName);
+            deleter.deleteFile(IndexFileNames.segmentFileName(mergedName, "", IndexFileNames.COMPOUND_FILE_EXTENSION));
+            deleter.deleteFile(IndexFileNames.segmentFileName(mergedName, "", IndexFileNames.COMPOUND_FILE_ENTRIES_EXTENSION));
             return 0;
           }
         }
 
-        merge.info.setUseCompoundFile(true);
+        merge.info.info.setUseCompoundFile(true);
+      } else {
+        // So that, if we hit exc in commitMerge (later),
+        // we close the per-segment readers in the finally
+        // clause below:
+        success = false;
+      }
+
+      // Have codec write SegmentInfo.  Must do this after
+      // creating CFS so that 1) .si isn't slurped into CFS,
+      // and 2) .si reflects useCompoundFile=true change
+      // above:
+      boolean success2 = false;
+      try {
+        codec.segmentInfoFormat().getSegmentInfosWriter().write(directory, merge.info.info, mergeState.fieldInfos, context);
+        success2 = true;
+      } finally {
+        if (!success2) {
+          synchronized(this) {
+            deleter.deleteNewFiles(merge.info.files());
+          }
+        }
       }
 
+      // TODO: ideally we would freeze merge.info here!!
+      // because any changes after writing the .si will be
+      // lost... 
+
       if (infoStream.isEnabled("IW")) {
-        infoStream.message("IW", String.format("merged segment size=%.3f MB vs estimate=%.3f MB", merge.info.sizeInBytes()/1024./1024., merge.estimatedMergeBytes/1024/1024.));
+        infoStream.message("IW", String.format("merged segment size=%.3f MB vs estimate=%.3f MB", merge.info.info.sizeInBytes()/1024./1024., merge.estimatedMergeBytes/1024/1024.));
       }
 
       final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();
@@ -3502,13 +3634,14 @@
       }
     }
 
-    return mergedDocCount;
+    return merge.info.info.getDocCount();
   }
 
   synchronized void addMergeException(MergePolicy.OneMerge merge) {
     assert merge.getException() != null;
-    if (!mergeExceptions.contains(merge) && mergeGen == merge.mergeGen)
+    if (!mergeExceptions.contains(merge) && mergeGen == merge.mergeGen) {
       mergeExceptions.add(merge);
+    }
   }
 
   // For test purposes.
@@ -3522,7 +3655,7 @@
   }
 
   // utility routines for tests
-  synchronized SegmentInfo newestSegment() {
+  synchronized SegmentInfoPerCommit newestSegment() {
     return segmentInfos.size() > 0 ? segmentInfos.info(segmentInfos.size()-1) : null;
   }
 
@@ -3532,9 +3665,9 @@
   }
 
   /** @lucene.internal */
-  public synchronized String segString(Iterable<SegmentInfo> infos) throws IOException {
+  public synchronized String segString(Iterable<SegmentInfoPerCommit> infos) throws IOException {
     final StringBuilder buffer = new StringBuilder();
-    for(final SegmentInfo info : infos) {
+    for(final SegmentInfoPerCommit info : infos) {
       if (buffer.length() > 0) {
         buffer.append(' ');
       }
@@ -3544,8 +3677,8 @@
   }
 
   /** @lucene.internal */
-  public synchronized String segString(SegmentInfo info) throws IOException {
-    return info.toString(info.dir, numDeletedDocs(info) - info.getDelCount());
+  public synchronized String segString(SegmentInfoPerCommit info) throws IOException {
+    return info.toString(info.info.dir, numDeletedDocs(info) - info.getDelCount());
   }
 
   private synchronized void doWait() {
@@ -3594,12 +3727,12 @@
   // For infoStream output
   synchronized SegmentInfos toLiveInfos(SegmentInfos sis) {
     final SegmentInfos newSIS = new SegmentInfos();
-    final Map<SegmentInfo,SegmentInfo> liveSIS = new HashMap<SegmentInfo,SegmentInfo>();        
-    for(SegmentInfo info : segmentInfos) {
+    final Map<SegmentInfoPerCommit,SegmentInfoPerCommit> liveSIS = new HashMap<SegmentInfoPerCommit,SegmentInfoPerCommit>();        
+    for(SegmentInfoPerCommit info : segmentInfos) {
       liveSIS.put(info, info);
     }
-    for(SegmentInfo info : sis) {
-      SegmentInfo liveInfo = liveSIS.get(info);
+    for(SegmentInfoPerCommit info : sis) {
+      SegmentInfoPerCommit liveInfo = liveSIS.get(info);
       if (liveInfo != null) {
         info = liveInfo;
       }
@@ -3658,9 +3791,6 @@
       boolean pendingCommitSet = false;
 
       try {
-        // This call can take a long time -- 10s of seconds
-        // or more.  We do it without sync:
-        directory.sync(toSync.files(directory, false));
 
         assert testPoint("midStartCommit2");
 
@@ -3673,14 +3803,30 @@
           // Exception here means nothing is prepared
           // (this method unwinds everything it did on
           // an exception)
-          toSync.prepareCommit(directory, codec);
+          toSync.prepareCommit(directory);
+          //System.out.println("DONE prepareCommit");
 
           pendingCommitSet = true;
           pendingCommit = toSync;
         }
 
+        // This call can take a long time -- 10s of seconds
+        // or more.  We do it without sync:
+        boolean success = false;
+        final Collection<String> filesToSync = toSync.files(directory, false);
+        try {
+          directory.sync(filesToSync);
+          success = true;
+        } finally {
+          if (!success) {
+            pendingCommitSet = false;
+            pendingCommit = null;
+            toSync.rollbackCommit(directory);
+          }
+        }
+
         if (infoStream.isEnabled("IW")) {
-          infoStream.message("IW", "done all syncs");
+          infoStream.message("IW", "done all syncs: " + filesToSync);
         }
 
         assert testPoint("midStartCommitSuccess");
@@ -3859,21 +4005,35 @@
    * deletion files, this SegmentInfo must not reference such files when this
    * method is called, because they are not allowed within a compound file.
    */
-  static final Collection<String> createCompoundFile(Directory directory, String fileName, CheckAbort checkAbort, final SegmentInfo info, IOContext context)
+  static final Collection<String> createCompoundFile(InfoStream infoStream, Directory directory, CheckAbort checkAbort, final SegmentInfo info, IOContext context)
           throws IOException {
-    assert info.getDocStoreOffset() == -1;
+
+    final String fileName = IndexFileNames.segmentFileName(info.name, "", IndexFileNames.COMPOUND_FILE_EXTENSION);
+    if (infoStream.isEnabled("IW")) {
+      infoStream.message("IW", "create compound file " + fileName);
+    }
+    assert Lucene3xSegmentInfoFormat.getDocStoreOffset(info) == -1;
     // Now merge all added files
     Collection<String> files = info.files();
     CompoundFileDirectory cfsDir = new CompoundFileDirectory(directory, fileName, context, true);
+    IOException prior = null;
     try {
       for (String file : files) {
         directory.copy(cfsDir, file, file, context);
         checkAbort.work(directory.fileLength(file));
       }
+    } catch(IOException ex) {
+      prior = ex;
     } finally {
-      cfsDir.close();
+      IOUtils.closeWhileHandlingException(prior, cfsDir);
     }
 
+    // Replace all previous files with the CFS/CFE files:
+    Set<String> siFiles = new HashSet<String>();
+    siFiles.add(fileName);
+    siFiles.add(IndexFileNames.segmentFileName(info.name, "", IndexFileNames.COMPOUND_FILE_ENTRIES_EXTENSION));
+    info.setFiles(siFiles);
+
     return files;
   }
 }


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/index/InvertedDocConsumer.java lucene4055/lucene/core/src/java/org/apache/lucene/index/InvertedDocConsumer.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/index/InvertedDocConsumer.java	2012-05-24 16:55:48.472233437 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/index/InvertedDocConsumer.java	2012-05-21 13:58:03.503533886 -0400
@@ -26,7 +26,7 @@
   abstract void abort();
 
   /** Flush a new segment */
-  abstract void flush(Map<FieldInfo, InvertedDocConsumerPerField> fieldsToFlush, SegmentWriteState state) throws IOException;
+  abstract void flush(Map<String, InvertedDocConsumerPerField> fieldsToFlush, SegmentWriteState state) throws IOException;
 
   abstract InvertedDocConsumerPerField addField(DocInverterPerField docInverterPerField, FieldInfo fieldInfo);
 


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/index/InvertedDocEndConsumer.java lucene4055/lucene/core/src/java/org/apache/lucene/index/InvertedDocEndConsumer.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/index/InvertedDocEndConsumer.java	2012-05-24 16:55:48.476233437 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/index/InvertedDocEndConsumer.java	2012-05-21 13:58:03.507533886 -0400
@@ -21,7 +21,7 @@
 import java.util.Map;
 
 abstract class InvertedDocEndConsumer {
-  abstract void flush(Map<FieldInfo, InvertedDocEndConsumerPerField> fieldsToFlush, SegmentWriteState state) throws IOException;
+  abstract void flush(Map<String, InvertedDocEndConsumerPerField> fieldsToFlush, SegmentWriteState state) throws IOException;
   abstract void abort();
   abstract InvertedDocEndConsumerPerField addField(DocInverterPerField docInverterPerField, FieldInfo fieldInfo);
   abstract void startDocument() throws IOException;


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/index/LogByteSizeMergePolicy.java lucene4055/lucene/core/src/java/org/apache/lucene/index/LogByteSizeMergePolicy.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/index/LogByteSizeMergePolicy.java	2012-05-24 16:55:48.468233437 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/index/LogByteSizeMergePolicy.java	2012-05-22 12:04:43.420920074 -0400
@@ -41,7 +41,7 @@
   }
   
   @Override
-  protected long size(SegmentInfo info) throws IOException {
+  protected long size(SegmentInfoPerCommit info) throws IOException {
     return sizeBytes(info);
   }
 


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/index/LogDocMergePolicy.java lucene4055/lucene/core/src/java/org/apache/lucene/index/LogDocMergePolicy.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/index/LogDocMergePolicy.java	2012-05-24 16:55:48.460233436 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/index/LogDocMergePolicy.java	2012-05-22 12:04:43.424920075 -0400
@@ -38,7 +38,7 @@
   }
 
   @Override
-  protected long size(SegmentInfo info) throws IOException {
+  protected long size(SegmentInfoPerCommit info) throws IOException {
     return sizeDocs(info);
   }
 


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/index/LogMergePolicy.java lucene4055/lucene/core/src/java/org/apache/lucene/index/LogMergePolicy.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/index/LogMergePolicy.java	2012-05-24 16:55:48.480233437 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/index/LogMergePolicy.java	2012-05-23 14:42:53.846589955 -0400
@@ -134,7 +134,7 @@
 
   // Javadoc inherited
   @Override
-  public boolean useCompoundFile(SegmentInfos infos, SegmentInfo mergedInfo) throws IOException {
+  public boolean useCompoundFile(SegmentInfos infos, SegmentInfoPerCommit mergedInfo) throws IOException {
     final boolean doCFS;
 
     if (!useCompoundFile) {
@@ -143,8 +143,9 @@
       doCFS = true;
     } else {
       long totalSize = 0;
-      for (SegmentInfo info : infos)
+      for (SegmentInfoPerCommit info : infos) {
         totalSize += size(info);
+      }
 
       doCFS = size(mergedInfo) <= noCFSRatio * totalSize;
     }
@@ -179,37 +180,37 @@
   @Override
   public void close() {}
 
-  abstract protected long size(SegmentInfo info) throws IOException;
+  abstract protected long size(SegmentInfoPerCommit info) throws IOException;
 
-  protected long sizeDocs(SegmentInfo info) throws IOException {
+  protected long sizeDocs(SegmentInfoPerCommit info) throws IOException {
     if (calibrateSizeByDeletes) {
       int delCount = writer.get().numDeletedDocs(info);
-      assert delCount <= info.docCount;
-      return (info.docCount - (long)delCount);
+      assert delCount <= info.info.getDocCount();
+      return (info.info.getDocCount() - (long)delCount);
     } else {
-      return info.docCount;
+      return info.info.getDocCount();
     }
   }
   
-  protected long sizeBytes(SegmentInfo info) throws IOException {
+  protected long sizeBytes(SegmentInfoPerCommit info) throws IOException {
     long byteSize = info.sizeInBytes();
     if (calibrateSizeByDeletes) {
       int delCount = writer.get().numDeletedDocs(info);
-      double delRatio = (info.docCount <= 0 ? 0.0f : ((float)delCount / (float)info.docCount));
+      double delRatio = (info.info.getDocCount() <= 0 ? 0.0f : ((float)delCount / (float)info.info.getDocCount()));
       assert delRatio <= 1.0;
-      return (info.docCount <= 0 ?  byteSize : (long)(byteSize * (1.0 - delRatio)));
+      return (info.info.getDocCount() <= 0 ?  byteSize : (long)(byteSize * (1.0 - delRatio)));
     } else {
       return byteSize;
     }
   }
   
-  protected boolean isMerged(SegmentInfos infos, int maxNumSegments, Map<SegmentInfo,Boolean> segmentsToMerge) throws IOException {
+  protected boolean isMerged(SegmentInfos infos, int maxNumSegments, Map<SegmentInfoPerCommit,Boolean> segmentsToMerge) throws IOException {
     final int numSegments = infos.size();
     int numToMerge = 0;
-    SegmentInfo mergeInfo = null;
+    SegmentInfoPerCommit mergeInfo = null;
     boolean segmentIsOriginal = false;
     for(int i=0;i<numSegments && numToMerge <= maxNumSegments;i++) {
-      final SegmentInfo info = infos.info(i);
+      final SegmentInfoPerCommit info = infos.info(i);
       final Boolean isOriginal = segmentsToMerge.get(info);
       if (isOriginal != null) {
         segmentIsOriginal = isOriginal;
@@ -225,15 +226,15 @@
   /** Returns true if this single info is already fully merged (has no
    *  pending norms or deletes, is in the same dir as the
    *  writer, and matches the current compound file setting */
-  protected boolean isMerged(SegmentInfo info)
+  protected boolean isMerged(SegmentInfoPerCommit info)
     throws IOException {
     IndexWriter w = writer.get();
     assert w != null;
     boolean hasDeletions = w.numDeletedDocs(info) > 0;
     return !hasDeletions &&
-      !info.hasSeparateNorms() &&
-      info.dir == w.getDirectory() &&
-      (info.getUseCompoundFile() == useCompoundFile || noCFSRatio < 1.0);
+      !info.info.hasSeparateNorms() &&
+      info.info.dir == w.getDirectory() &&
+      (info.info.getUseCompoundFile() == useCompoundFile || noCFSRatio < 1.0);
   }
 
   /**
@@ -247,11 +248,11 @@
   private MergeSpecification findForcedMergesSizeLimit(
       SegmentInfos infos, int maxNumSegments, int last) throws IOException {
     MergeSpecification spec = new MergeSpecification();
-    final List<SegmentInfo> segments = infos.asList();
+    final List<SegmentInfoPerCommit> segments = infos.asList();
 
     int start = last - 1;
     while (start >= 0) {
-      SegmentInfo info = infos.info(start);
+      SegmentInfoPerCommit info = infos.info(start);
       if (size(info) > maxMergeSizeForForcedMerge || sizeDocs(info) > maxMergeDocs) {
         if (verbose()) {
           message("findForcedMergesSizeLimit: skip segment=" + info + ": size is > maxMergeSize (" + maxMergeSizeForForcedMerge + ") or sizeDocs is > maxMergeDocs (" + maxMergeDocs + ")");
@@ -288,7 +289,7 @@
    */
   private MergeSpecification findForcedMergesMaxNumSegments(SegmentInfos infos, int maxNumSegments, int last) throws IOException {
     MergeSpecification spec = new MergeSpecification();
-    final List<SegmentInfo> segments = infos.asList();
+    final List<SegmentInfoPerCommit> segments = infos.asList();
 
     // First, enroll all "full" merges (size
     // mergeFactor) to potentially be run concurrently:
@@ -326,8 +327,9 @@
 
         for(int i=0;i<last-finalMergeSize+1;i++) {
           long sumSize = 0;
-          for(int j=0;j<finalMergeSize;j++)
+          for(int j=0;j<finalMergeSize;j++) {
             sumSize += size(infos.info(j+i));
+          }
           if (i == 0 || (sumSize < 2*size(infos.info(i-1)) && sumSize < bestSize)) {
             bestStart = i;
             bestSize = sumSize;
@@ -352,7 +354,7 @@
    *  in use may make use of concurrency. */
   @Override
   public MergeSpecification findForcedMerges(SegmentInfos infos,
-            int maxNumSegments, Map<SegmentInfo,Boolean> segmentsToMerge) throws IOException {
+            int maxNumSegments, Map<SegmentInfoPerCommit,Boolean> segmentsToMerge) throws IOException {
 
     assert maxNumSegments > 0;
     if (verbose()) {
@@ -373,7 +375,7 @@
     // since merging started):
     int last = infos.size();
     while (last > 0) {
-      final SegmentInfo info = infos.info(--last);
+      final SegmentInfoPerCommit info = infos.info(--last);
       if (segmentsToMerge.get(info) != null) {
         last++;
         break;
@@ -398,7 +400,7 @@
     // Check if there are any segments above the threshold
     boolean anyTooLarge = false;
     for (int i = 0; i < last; i++) {
-      SegmentInfo info = infos.info(i);
+      SegmentInfoPerCommit info = infos.info(i);
       if (size(info) > maxMergeSizeForForcedMerge || sizeDocs(info) > maxMergeDocs) {
         anyTooLarge = true;
         break;
@@ -420,7 +422,7 @@
   @Override
   public MergeSpecification findForcedDeletesMerges(SegmentInfos segmentInfos)
       throws CorruptIndexException, IOException {
-    final List<SegmentInfo> segments = segmentInfos.asList();
+    final List<SegmentInfoPerCommit> segments = segmentInfos.asList();
     final int numSegments = segments.size();
 
     if (verbose()) {
@@ -432,11 +434,11 @@
     IndexWriter w = writer.get();
     assert w != null;
     for(int i=0;i<numSegments;i++) {
-      final SegmentInfo info = segmentInfos.info(i);
+      final SegmentInfoPerCommit info = segmentInfos.info(i);
       int delCount = w.numDeletedDocs(info);
       if (delCount > 0) {
         if (verbose()) {
-          message("  segment " + info.name + " has deletions");
+          message("  segment " + info.info.name + " has deletions");
         }
         if (firstSegmentWithDeletions == -1)
           firstSegmentWithDeletions = i;
@@ -472,11 +474,11 @@
   }
 
   private static class SegmentInfoAndLevel implements Comparable<SegmentInfoAndLevel> {
-    SegmentInfo info;
+    SegmentInfoPerCommit info;
     float level;
     int index;
     
-    public SegmentInfoAndLevel(SegmentInfo info, float level, int index) {
+    public SegmentInfoAndLevel(SegmentInfoPerCommit info, float level, int index) {
       this.info = info;
       this.level = level;
       this.index = index;
@@ -484,12 +486,13 @@
 
     // Sorts largest to smallest
     public int compareTo(SegmentInfoAndLevel other) {
-      if (level < other.level)
+      if (level < other.level) {
         return 1;
-      else if (level > other.level)
+      } else if (level > other.level) {
         return -1;
-      else
+      } else {
         return 0;
+      }
     }
   }
 
@@ -513,10 +516,10 @@
     final List<SegmentInfoAndLevel> levels = new ArrayList<SegmentInfoAndLevel>();
     final float norm = (float) Math.log(mergeFactor);
 
-    final Collection<SegmentInfo> mergingSegments = writer.get().getMergingSegments();
+    final Collection<SegmentInfoPerCommit> mergingSegments = writer.get().getMergingSegments();
 
     for(int i=0;i<numSegments;i++) {
-      final SegmentInfo info = infos.info(i);
+      final SegmentInfoPerCommit info = infos.info(i);
       long size = size(info);
 
       // Floor tiny segments
@@ -562,22 +565,24 @@
       float maxLevel = levels.get(start).level;
       for(int i=1+start;i<numMergeableSegments;i++) {
         final float level = levels.get(i).level;
-        if (level > maxLevel)
+        if (level > maxLevel) {
           maxLevel = level;
+        }
       }
 
       // Now search backwards for the rightmost segment that
       // falls into this level:
       float levelBottom;
-      if (maxLevel <= levelFloor)
+      if (maxLevel <= levelFloor) {
         // All remaining segments fall into the min level
         levelBottom = -1.0F;
-      else {
+      } else {
         levelBottom = (float) (maxLevel - LEVEL_LOG_SPAN);
 
         // Force a boundary at the level floor
-        if (levelBottom < levelFloor && maxLevel >= levelFloor)
+        if (levelBottom < levelFloor && maxLevel >= levelFloor) {
           levelBottom = levelFloor;
+        }
       }
 
       int upto = numMergeableSegments-1;
@@ -597,7 +602,7 @@
         boolean anyTooLarge = false;
         boolean anyMerging = false;
         for(int i=start;i<end;i++) {
-          final SegmentInfo info = levels.get(i).info;
+          final SegmentInfoPerCommit info = levels.get(i).info;
           anyTooLarge |= (size(info) >= maxMergeSize || sizeDocs(info) >= maxMergeDocs);
           if (mergingSegments.contains(info)) {
             anyMerging = true;
@@ -610,7 +615,7 @@
         } else if (!anyTooLarge) {
           if (spec == null)
             spec = new MergeSpecification();
-          final List<SegmentInfo> mergeInfos = new ArrayList<SegmentInfo>();
+          final List<SegmentInfoPerCommit> mergeInfos = new ArrayList<SegmentInfoPerCommit>();
           for(int i=start;i<end;i++) {
             mergeInfos.add(levels.get(i).info);
             assert infos.contains(levels.get(i).info);


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/index/MergePolicy.java lucene4055/lucene/core/src/java/org/apache/lucene/index/MergePolicy.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/index/MergePolicy.java	2012-05-24 16:55:48.480233437 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/index/MergePolicy.java	2012-05-23 14:42:53.846589955 -0400
@@ -67,7 +67,7 @@
 
   public static class OneMerge {
 
-    SegmentInfo info;               // used by IndexWriter
+    SegmentInfoPerCommit info;      // used by IndexWriter
     boolean registerDone;           // used by IndexWriter
     long mergeGen;                  // used by IndexWriter
     boolean isExternal;             // used by IndexWriter
@@ -75,20 +75,20 @@
     public long estimatedMergeBytes;       // used by IndexWriter
     List<SegmentReader> readers;        // used by IndexWriter
     List<Bits> readerLiveDocs;      // used by IndexWriter
-    public final List<SegmentInfo> segments;
+    public final List<SegmentInfoPerCommit> segments;
     public final int totalDocCount;
     boolean aborted;
     Throwable error;
     boolean paused;
 
-    public OneMerge(List<SegmentInfo> segments) {
+    public OneMerge(List<SegmentInfoPerCommit> segments) {
       if (0 == segments.size())
         throw new RuntimeException("segments must include at least one segment");
       // clone the list, as the in list may be based off original SegmentInfos and may be modified
-      this.segments = new ArrayList<SegmentInfo>(segments);
+      this.segments = new ArrayList<SegmentInfoPerCommit>(segments);
       int count = 0;
-      for(SegmentInfo info : segments) {
-        count += info.docCount;
+      for(SegmentInfoPerCommit info : segments) {
+        count += info.info.getDocCount();
       }
       totalDocCount = count;
     }
@@ -156,8 +156,9 @@
         if (i > 0) b.append(' ');
         b.append(segments.get(i).toString(dir, 0));
       }
-      if (info != null)
-        b.append(" into ").append(info.name);
+      if (info != null) {
+        b.append(" into ").append(info.info.name);
+      }
       if (maxNumSegments != -1)
         b.append(" [maxNumSegments=" + maxNumSegments + "]");
       if (aborted) {
@@ -172,8 +173,8 @@
      * */
     public long totalBytesSize() throws IOException {
       long total = 0;
-      for (SegmentInfo info : segments) {
-        total += info.sizeInBytes();
+      for (SegmentInfoPerCommit info : segments) {
+        total += info.info.sizeInBytes();
       }
       return total;
     }
@@ -184,8 +185,8 @@
      * */
     public int totalNumDocs() throws IOException {
       int total = 0;
-      for (SegmentInfo info : segments) {
-        total += info.docCount;
+      for (SegmentInfoPerCommit info : segments) {
+        total += info.info.getDocCount();
       }
       return total;
     }
@@ -309,7 +310,7 @@
    *          produced by a cascaded merge.
    */
   public abstract MergeSpecification findForcedMerges(
-          SegmentInfos segmentInfos, int maxSegmentCount, Map<SegmentInfo,Boolean> segmentsToMerge)
+          SegmentInfos segmentInfos, int maxSegmentCount, Map<SegmentInfoPerCommit,Boolean> segmentsToMerge)
       throws CorruptIndexException, IOException;
 
   /**
@@ -330,5 +331,5 @@
   /**
    * Returns true if a new segment (regardless of its origin) should use the compound file format.
    */
-  public abstract boolean useCompoundFile(SegmentInfos segments, SegmentInfo newSegment) throws IOException;
+  public abstract boolean useCompoundFile(SegmentInfos segments, SegmentInfoPerCommit newSegment) throws IOException;
 }


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/index/MergeState.java lucene4055/lucene/core/src/java/org/apache/lucene/index/MergeState.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/index/MergeState.java	2012-05-24 16:55:48.468233437 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/index/MergeState.java	2012-05-23 15:56:05.858666439 -0400
@@ -40,11 +40,11 @@
     }
   }
 
+  public SegmentInfo segmentInfo;
   public FieldInfos fieldInfos;
   public List<IndexReaderAndLiveDocs> readers;        // Readers & liveDocs being merged
   public int[][] docMaps;                             // Maps docIDs around deletions
   public int[] docBase;                               // New docID base per reader
-  public int mergedDocCount;                          // Total # merged docs
   public CheckAbort checkAbort;
   public InfoStream infoStream;
 


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/index/MultiDocValues.java lucene4055/lucene/core/src/java/org/apache/lucene/index/MultiDocValues.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/index/MultiDocValues.java	2012-05-24 16:55:48.456233435 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/index/MultiDocValues.java	2012-05-21 13:58:03.487533886 -0400
@@ -51,7 +51,7 @@
       // for norms we drop all norms if one leaf reader has no norms and the field is present
       FieldInfos fieldInfos = reader.getFieldInfos();
       FieldInfo fieldInfo = fieldInfos.fieldInfo(field);
-      return fieldInfo != null && fieldInfo.omitNorms;
+      return fieldInfo != null && fieldInfo.omitsNorms();
     }
   };
 


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/index/MultiFields.java lucene4055/lucene/core/src/java/org/apache/lucene/index/MultiFields.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/index/MultiFields.java	2012-05-24 16:55:48.488233437 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/index/MultiFields.java	2012-05-21 15:41:31.783642000 -0400
@@ -244,21 +244,27 @@
   }
 
   /** Call this to get the (merged) FieldInfos for a
-   *  composite reader */
+   *  composite reader. 
+   *  <p>
+   *  NOTE: the returned field numbers will likely not
+   *  correspond to the actual field numbers in the underlying
+   *  readers, and codec metadata ({@link FieldInfo#getAttribute(String)}
+   *  will be unavailable.
+   */
   public static FieldInfos getMergedFieldInfos(IndexReader reader) {
     final List<AtomicReader> subReaders = new ArrayList<AtomicReader>();
     ReaderUtil.gatherSubReaders(subReaders, reader);
-    final FieldInfos fieldInfos = new FieldInfos();
+    final FieldInfos.Builder builder = new FieldInfos.Builder();
     for(AtomicReader subReader : subReaders) {
-      fieldInfos.add(subReader.getFieldInfos());
+      builder.add(subReader.getFieldInfos());
     }
-    return fieldInfos;
+    return builder.finish();
   }
 
   public static Collection<String> getIndexedFields(IndexReader reader) {
     final Collection<String> fields = new HashSet<String>();
     for(FieldInfo fieldInfo : getMergedFieldInfos(reader)) {
-      if (fieldInfo.isIndexed) {
+      if (fieldInfo.isIndexed()) {
         fields.add(fieldInfo.name);
       }
     }


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/index/NoMergePolicy.java lucene4055/lucene/core/src/java/org/apache/lucene/index/NoMergePolicy.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/index/NoMergePolicy.java	2012-05-24 16:55:48.480233437 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/index/NoMergePolicy.java	2012-05-22 12:04:43.424920075 -0400
@@ -59,7 +59,7 @@
 
   @Override
   public MergeSpecification findForcedMerges(SegmentInfos segmentInfos,
-             int maxSegmentCount, Map<SegmentInfo,Boolean> segmentsToMerge)
+             int maxSegmentCount, Map<SegmentInfoPerCommit,Boolean> segmentsToMerge)
       throws CorruptIndexException, IOException { return null; }
 
   @Override
@@ -67,7 +67,7 @@
       throws CorruptIndexException, IOException { return null; }
 
   @Override
-  public boolean useCompoundFile(SegmentInfos segments, SegmentInfo newSegment) { return useCompoundFile; }
+  public boolean useCompoundFile(SegmentInfos segments, SegmentInfoPerCommit newSegment) { return useCompoundFile; }
 
   @Override
   public void setIndexWriter(IndexWriter writer) {}


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/index/NormsConsumer.java lucene4055/lucene/core/src/java/org/apache/lucene/index/NormsConsumer.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/index/NormsConsumer.java	2012-05-24 16:55:48.488233437 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/index/NormsConsumer.java	2012-05-24 12:04:59.299929569 -0400
@@ -18,7 +18,6 @@
  */
 
 import java.io.IOException;
-import java.util.Collection;
 import java.util.Map;
 
 import org.apache.lucene.codecs.DocValuesConsumer;
@@ -49,30 +48,26 @@
     }
   }
 
-  // We only write the _X.nrm file at flush
-  void files(Collection<String> files) {}
-
   /** Produce _X.nrm if any document had a field with norms
    *  not disabled */
   @Override
-  public void flush(Map<FieldInfo,InvertedDocEndConsumerPerField> fieldsToFlush, SegmentWriteState state) throws IOException {
+  public void flush(Map<String,InvertedDocEndConsumerPerField> fieldsToFlush, SegmentWriteState state) throws IOException {
     boolean success = false;
     boolean anythingFlushed = false;
     try {
       if (state.fieldInfos.hasNorms()) {
         for (FieldInfo fi : state.fieldInfos) {
-          final NormsConsumerPerField toWrite = (NormsConsumerPerField) fieldsToFlush.get(fi);
+          final NormsConsumerPerField toWrite = (NormsConsumerPerField) fieldsToFlush.get(fi.name);
           // we must check the final value of omitNorms for the fieldinfo, it could have 
           // changed for this field since the first time we added it.
-          if (!fi.omitNorms) {
+          if (!fi.omitsNorms()) {
             if (toWrite != null && toWrite.initialized()) {
               anythingFlushed = true;
-              final Type type = toWrite.flush(state.numDocs);
+              final Type type = toWrite.flush(state.segmentInfo.getDocCount());
               assert fi.getNormType() == type;
-            } else if (fi.isIndexed) {
+            } else if (fi.isIndexed()) {
               anythingFlushed = true;
-              assert fi.getNormType() == null;
-              fi.setNormValueType(null, false);
+              assert fi.getNormType() == null: "got " + fi.getNormType() + "; field=" + fi.name;
             }
           }
         }


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/index/NormsConsumerPerField.java lucene4055/lucene/core/src/java/org/apache/lucene/index/NormsConsumerPerField.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/index/NormsConsumerPerField.java	2012-05-24 16:55:48.492233437 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/index/NormsConsumerPerField.java	2012-05-21 13:58:03.519533887 -0400
@@ -47,7 +47,7 @@
 
   @Override
   void finish() throws IOException {
-    if (fieldInfo.isIndexed && !fieldInfo.omitNorms) {
+    if (fieldInfo.isIndexed() && !fieldInfo.omitsNorms()) {
       similarity.computeNorm(fieldState, norm);
       
       if (norm.type() != null) {
@@ -69,7 +69,8 @@
   
   private DocValuesConsumer getConsumer(Type type) throws IOException {
     if (consumer == null) {
-      fieldInfo.setNormValueType(type, false);
+      assert fieldInfo.getNormType() == null || fieldInfo.getNormType() == type;
+      fieldInfo.setNormValueType(type);
       consumer = parent.newConsumer(docState.docWriter.newPerDocWriteState(""), fieldInfo, type);
       this.initType = type;
     }


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/index/ParallelAtomicReader.java lucene4055/lucene/core/src/java/org/apache/lucene/index/ParallelAtomicReader.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/index/ParallelAtomicReader.java	2012-05-24 16:55:48.480233437 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/index/ParallelAtomicReader.java	2012-05-21 15:41:37.895642107 -0400
@@ -48,7 +48,7 @@
  * undefined behavior</em>.
  */
 public final class ParallelAtomicReader extends AtomicReader {
-  private final FieldInfos fieldInfos = new FieldInfos();
+  private final FieldInfos fieldInfos;
   private final ParallelFields fields = new ParallelFields();
   private final AtomicReader[] parallelReaders, storedFieldsReaders;
   private final Set<AtomicReader> completeReaderSet =
@@ -99,20 +99,23 @@
       }
     }
     
+    // TODO: make this read-only in a cleaner way?
+    FieldInfos.Builder builder = new FieldInfos.Builder();
     // build FieldInfos and fieldToReader map:
     for (final AtomicReader reader : this.parallelReaders) {
       final FieldInfos readerFieldInfos = reader.getFieldInfos();
       for (FieldInfo fieldInfo : readerFieldInfos) {
         // NOTE: first reader having a given field "wins":
         if (!fieldToReader.containsKey(fieldInfo.name)) {
-          fieldInfos.add(fieldInfo);
+          builder.add(fieldInfo);
           fieldToReader.put(fieldInfo.name, reader);
-          if (fieldInfo.storeTermVector) {
+          if (fieldInfo.hasVectors()) {
             tvFieldToReader.put(fieldInfo.name, reader);
           }
         }
       }
     }
+    fieldInfos = builder.finish();
     
     // build Fields instance
     for (final AtomicReader reader : this.parallelReaders) {
@@ -202,6 +205,14 @@
     }
   }
   
+  /**
+   * {@inheritDoc}
+   * <p>
+   * NOTE: the returned field numbers will likely not
+   * correspond to the actual field numbers in the underlying
+   * readers, and codec metadata ({@link FieldInfo#getAttribute(String)}
+   * will be unavailable.
+   */
   @Override
   public FieldInfos getFieldInfos() {
     return fieldInfos;


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/index/PerDocWriteState.java lucene4055/lucene/core/src/java/org/apache/lucene/index/PerDocWriteState.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/index/PerDocWriteState.java	2012-05-24 16:55:48.492233437 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/index/PerDocWriteState.java	2012-05-22 18:37:03.901330020 -0400
@@ -32,19 +32,17 @@
 public class PerDocWriteState {
   public final InfoStream infoStream;
   public final Directory directory;
-  public final String segmentName;
-  public final FieldInfos fieldInfos;
+  public final SegmentInfo segmentInfo;
   public final Counter bytesUsed;
   public final String segmentSuffix;
   public final IOContext context;
 
   public PerDocWriteState(InfoStream infoStream, Directory directory,
-      String segmentName, FieldInfos fieldInfos, Counter bytesUsed,
+      SegmentInfo segmentInfo, Counter bytesUsed,
       String segmentSuffix, IOContext context) {
     this.infoStream = infoStream;
     this.directory = directory;
-    this.segmentName = segmentName;
-    this.fieldInfos = fieldInfos;
+    this.segmentInfo = segmentInfo;
     this.segmentSuffix = segmentSuffix;
     this.bytesUsed = bytesUsed;
     this.context = context;
@@ -53,8 +51,7 @@
   public PerDocWriteState(SegmentWriteState state) {
     infoStream = state.infoStream;
     directory = state.directory;
-    segmentName = state.segmentName;
-    fieldInfos = state.fieldInfos;
+    segmentInfo = state.segmentInfo;
     segmentSuffix = state.segmentSuffix;
     bytesUsed = Counter.newCounter();
     context = state.context;
@@ -63,8 +60,7 @@
   public PerDocWriteState(PerDocWriteState state, String segmentSuffix) {
     this.infoStream = state.infoStream;
     this.directory = state.directory;
-    this.segmentName = state.segmentName;
-    this.fieldInfos = state.fieldInfos;
+    this.segmentInfo = state.segmentInfo;
     this.segmentSuffix = segmentSuffix;
     this.bytesUsed = state.bytesUsed;
     this.context = state.context;


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/index/ReadersAndLiveDocs.java lucene4055/lucene/core/src/java/org/apache/lucene/index/ReadersAndLiveDocs.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/index/ReadersAndLiveDocs.java	2012-05-24 16:55:48.476233437 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/index/ReadersAndLiveDocs.java	2012-05-23 14:42:53.846589955 -0400
@@ -32,7 +32,7 @@
 class ReadersAndLiveDocs {
   // Not final because we replace (clone) when we need to
   // change it and it's been shared:
-  public final SegmentInfo info;
+  public final SegmentInfoPerCommit info;
 
   // Tracks how many consumers are using this instance:
   private final AtomicInteger refCount = new AtomicInteger(1);
@@ -67,7 +67,7 @@
   // external NRT reader:
   private boolean shared;
 
-  public ReadersAndLiveDocs(IndexWriter writer, SegmentInfo info) {
+  public ReadersAndLiveDocs(IndexWriter writer, SegmentInfoPerCommit info) {
     this.info = info;
     this.writer = writer;
     shared = true;
@@ -98,16 +98,16 @@
     int count;
     if (liveDocs != null) {
       count = 0;
-      for(int docID=0;docID<info.docCount;docID++) {
+      for(int docID=0;docID<info.info.getDocCount();docID++) {
         if (liveDocs.get(docID)) {
           count++;
         }
       }
     } else {
-      count = info.docCount;
+      count = info.info.getDocCount();
     }
 
-    assert info.docCount - info.getDelCount() - pendingDeleteCount == count: "info.docCount=" + info.docCount + " info.getDelCount()=" + info.getDelCount() + " pendingDeleteCount=" + pendingDeleteCount + " count=" + count;;
+    assert info.info.getDocCount() - info.getDelCount() - pendingDeleteCount == count: "info.docCount=" + info.info.getDocCount() + " info.getDelCount()=" + info.getDelCount() + " pendingDeleteCount=" + pendingDeleteCount + " count=" + count;
     return true;
   }
 
@@ -169,7 +169,7 @@
   public synchronized boolean delete(int docID) {
     assert liveDocs != null;
     assert Thread.holdsLock(writer);
-    assert docID >= 0 && docID < liveDocs.length() : "out of bounds: docid=" + docID + " liveDocsLength=" + liveDocs.length() + " seg=" + info.name + " docCount=" + info.docCount;
+    assert docID >= 0 && docID < liveDocs.length() : "out of bounds: docid=" + docID + " liveDocsLength=" + liveDocs.length() + " seg=" + info.info.name + " docCount=" + info.info.getDocCount();
     assert !shared;
     final boolean didDelete = liveDocs.get(docID);
     if (didDelete) {
@@ -207,7 +207,7 @@
     }
     shared = true;
     if (liveDocs != null) {
-      return new SegmentReader(reader.getSegmentInfo(), reader.core, liveDocs, info.docCount - info.getDelCount() - pendingDeleteCount);
+      return new SegmentReader(reader.getSegmentInfo(), reader.core, liveDocs, info.info.getDocCount() - info.getDelCount() - pendingDeleteCount);
     } else {
       assert reader.getLiveDocs() == liveDocs;
       reader.incRef();
@@ -217,17 +217,17 @@
 
   public synchronized void initWritableLiveDocs() throws IOException {
     assert Thread.holdsLock(writer);
-    assert info.docCount > 0;
+    assert info.info.getDocCount() > 0;
     //System.out.println("initWritableLivedocs seg=" + info + " liveDocs=" + liveDocs + " shared=" + shared);
     if (shared) {
       // Copy on write: this means we've cloned a
       // SegmentReader sharing the current liveDocs
       // instance; must now make a private clone so we can
       // change it:
-      LiveDocsFormat liveDocsFormat = info.getCodec().liveDocsFormat();
+      LiveDocsFormat liveDocsFormat = info.info.getCodec().liveDocsFormat();
       if (liveDocs == null) {
         //System.out.println("create BV seg=" + info);
-        liveDocs = liveDocsFormat.newLiveDocs(info.docCount);
+        liveDocs = liveDocsFormat.newLiveDocs(info.info.getDocCount());
       } else {
         liveDocs = liveDocsFormat.newLiveDocs(liveDocs);
       }
@@ -270,25 +270,19 @@
     //System.out.println("rld.writeLiveDocs seg=" + info + " pendingDelCount=" + pendingDeleteCount);
     if (pendingDeleteCount != 0) {
       // We have new deletes
-      assert liveDocs.length() == info.docCount;
-
-      // Save in case we need to rollback on failure:
-      final SegmentInfo sav = info.clone();
-      info.advanceDelGen();
-      info.setDelCount(info.getDelCount() + pendingDeleteCount);
+      assert liveDocs.length() == info.info.getDocCount();
 
       // We can write directly to the actual name (vs to a
       // .tmp & renaming it) because the file is not live
       // until segments file is written:
-      boolean success = false;
-      try {
-        info.getCodec().liveDocsFormat().writeLiveDocs((MutableBits)liveDocs, dir, info, IOContext.DEFAULT);
-        success = true;
-      } finally {
-        if (!success) {
-          info.reset(sav);
-        }
-      }
+      info.info.getCodec().liveDocsFormat().writeLiveDocs((MutableBits)liveDocs, dir, info, pendingDeleteCount, IOContext.DEFAULT);
+
+      // If we hit an exc in the line above (eg disk full)
+      // then info remains pointing to the previous
+      // (successfully written) del docs:
+      info.advanceDelGen();
+      info.setDelCount(info.getDelCount() + pendingDeleteCount);
+
       pendingDeleteCount = 0;
       return true;
     } else {


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/index/SegmentCoreReaders.java lucene4055/lucene/core/src/java/org/apache/lucene/index/SegmentCoreReaders.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/index/SegmentCoreReaders.java	2012-05-24 16:55:48.488233437 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/index/SegmentCoreReaders.java	2012-05-24 12:05:10.595929768 -0400
@@ -80,30 +80,29 @@
   private final Set<CoreClosedListener> coreClosedListeners = 
       Collections.synchronizedSet(new LinkedHashSet<CoreClosedListener>());
   
-  SegmentCoreReaders(SegmentReader owner, Directory dir, SegmentInfo si, IOContext context, int termsIndexDivisor) throws IOException {
+  SegmentCoreReaders(SegmentReader owner, Directory dir, SegmentInfoPerCommit si, IOContext context, int termsIndexDivisor) throws IOException {
     
     if (termsIndexDivisor == 0) {
       throw new IllegalArgumentException("indexDivisor must be < 0 (don't load terms index) or greater than 0 (got 0)");
     }
     
-    final Codec codec = si.getCodec();
+    final Codec codec = si.info.getCodec();
     final Directory cfsDir; // confusing name: if (cfs) its the cfsdir, otherwise its the segment's directory.
 
     boolean success = false;
     
     try {
-      if (si.getUseCompoundFile()) {
-        cfsDir = cfsReader = new CompoundFileDirectory(dir, IndexFileNames.segmentFileName(si.name, "", IndexFileNames.COMPOUND_FILE_EXTENSION), context, false);
+      if (si.info.getUseCompoundFile()) {
+        cfsDir = cfsReader = new CompoundFileDirectory(dir, IndexFileNames.segmentFileName(si.info.name, "", IndexFileNames.COMPOUND_FILE_EXTENSION), context, false);
       } else {
         cfsReader = null;
         cfsDir = dir;
       }
-      si.loadFieldInfos(cfsDir, false); // prevent opening the CFS to load fieldInfos
-      fieldInfos = si.getFieldInfos();
-      
+      fieldInfos = codec.fieldInfosFormat().getFieldInfosReader().read(cfsDir, si.info.name, IOContext.READONCE);
+
       this.termsIndexDivisor = termsIndexDivisor;
       final PostingsFormat format = codec.postingsFormat();
-      final SegmentReadState segmentReadState = new SegmentReadState(cfsDir, si, fieldInfos, context, termsIndexDivisor);
+      final SegmentReadState segmentReadState = new SegmentReadState(cfsDir, si.info, fieldInfos, context, termsIndexDivisor);
       // Ask codec for its Fields
       fields = format.fieldsProducer(segmentReadState);
       assert fields != null;
@@ -113,10 +112,10 @@
       norms = codec.normsFormat().docsProducer(segmentReadState);
       perDocProducer = codec.docValuesFormat().docsProducer(segmentReadState);
   
-      fieldsReaderOrig = si.getCodec().storedFieldsFormat().fieldsReader(cfsDir, si, fieldInfos, context);
- 
-      if (si.getHasVectors()) { // open term vector files only as needed
-        termVectorsReaderOrig = si.getCodec().termVectorsFormat().vectorsReader(cfsDir, si, fieldInfos, context);
+      fieldsReaderOrig = si.info.getCodec().storedFieldsFormat().fieldsReader(cfsDir, si.info, fieldInfos, context);
+
+      if (fieldInfos.hasVectors()) { // open term vector files only as needed
+        termVectorsReaderOrig = si.info.getCodec().termVectorsFormat().vectorsReader(cfsDir, si.info, fieldInfos, context);
       } else {
         termVectorsReaderOrig = null;
       }


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/index/SegmentInfo.java lucene4055/lucene/core/src/java/org/apache/lucene/index/SegmentInfo.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/index/SegmentInfo.java	2012-05-24 16:55:48.472233437 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/index/SegmentInfo.java	2012-05-24 12:16:59.263942107 -0400
@@ -19,20 +19,16 @@
 
 
 import java.io.IOException;
-import java.util.ArrayList;
+import java.util.Collection;
+import java.util.Collections;
 import java.util.HashMap;
-import java.util.HashSet;
-import java.util.List;
 import java.util.Map;
-import java.util.Map.Entry;
 import java.util.Set;
 
 import org.apache.lucene.codecs.Codec;
-import org.apache.lucene.codecs.FieldInfosReader;
-import org.apache.lucene.store.CompoundFileDirectory;
+import org.apache.lucene.codecs.lucene3x.Lucene3xSegmentInfoFormat;
 import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.util.Constants;
+import org.apache.lucene.store.TrackingDirectoryWrapper;
 
 /**
  * Information about a segment such as it's name, directory, and files related
@@ -40,63 +36,25 @@
  *
  * @lucene.experimental
  */
-public final class SegmentInfo implements Cloneable {
-  // TODO: remove with hasVector and hasProx
-  public static final int CHECK_FIELDINFO = -2;
+public final class SegmentInfo {
   
   // TODO: remove these from this class, for now this is the representation
   public static final int NO = -1;          // e.g. no norms; no deletes;
   public static final int YES = 1;          // e.g. have norms; have deletes;
-  public static final int WITHOUT_GEN = 0;  // a file name that has no GEN in it.
 
-  public String name;				  // unique name in dir
-  public int docCount;				  // number of docs in seg
-  public Directory dir;				  // where segment resides
-
-  /*
-   * Current generation of del file:
-   * - NO if there are no deletes
-   * - YES or higher if there are deletes at generation N
-   */
-  private long delGen;
-
-  /*
-   * Current generation of each field's norm file. If this array is null,
-   * means no separate norms. If this array is not null, its values mean:
-   * - NO says this field has no separate norms
-   * >= YES says this field has separate norms with the specified generation
-   */
-  private Map<Integer,Long> normGen;
+  public final String name;				  // unique name in dir
+  private int docCount;				  // number of docs in seg
+  public final Directory dir;				  // where segment resides
 
   private boolean isCompoundFile;
 
-  private volatile List<String> files;                     // cached list of files that this segment uses
-                                                  // in the Directory
-
-  private volatile long sizeInBytes = -1;           // total byte size of all files (computed on demand)
-
-  //TODO: LUCENE-2555: remove once we don't need to support shared doc stores (pre 4.0)
-  private int docStoreOffset;                     // if this segment shares stored fields & vectors, this
-                                                  // offset is where in that file this segment's docs begin
-  //TODO: LUCENE-2555: remove once we don't need to support shared doc stores (pre 4.0)
-  private String docStoreSegment;                 // name used to derive fields/vectors file we share with
-                                                  // other segments
-  //TODO: LUCENE-2555: remove once we don't need to support shared doc stores (pre 4.0)
-  private boolean docStoreIsCompoundFile;         // whether doc store files are stored in compound file (*.cfx)
-
-  private int delCount;                           // How many deleted docs in this segment
-  
-  //TODO: remove when we don't have to support old indexes anymore that had this field
-  private int hasVectors = CHECK_FIELDINFO;
-  //TODO: remove when we don't have to support old indexes anymore that had this field
-  private int hasProx = CHECK_FIELDINFO;     // True if this segment has any fields with positional information
-
-  
-  private FieldInfos fieldInfos;
+  private volatile long sizeInBytes = -1;         // total byte size of all files (computed on demand)
 
   private Codec codec;
 
   private Map<String,String> diagnostics;
+  
+  private Map<String,String> attributes;
 
   // Tracks the Lucene version this segment was created with, since 3.1. Null
   // indicates an older than 3.0 index, and it's used to detect a too old index.
@@ -105,57 +63,6 @@
   // see Constants.LUCENE_MAIN_VERSION.
   private String version;
 
-  // NOTE: only used in-RAM by IW to track buffered deletes;
-  // this is never written to/read from the Directory
-  private long bufferedDeletesGen;
-  
-  // holds the fieldInfos Version to refresh files() cache if FI has changed
-  private long fieldInfosVersion;
-  
-  public SegmentInfo(String name, int docCount, Directory dir, boolean isCompoundFile,
-                     Codec codec, FieldInfos fieldInfos) {
-    this.name = name;
-    this.docCount = docCount;
-    this.dir = dir;
-    delGen = NO;
-    this.isCompoundFile = isCompoundFile;
-    this.docStoreOffset = -1;
-    this.docStoreSegment = name;
-    this.codec = codec;
-    delCount = 0;
-    version = Constants.LUCENE_MAIN_VERSION;
-    this.fieldInfos = fieldInfos;
-  }
-
-  /**
-   * Copy everything from src SegmentInfo into our instance.
-   */
-  void reset(SegmentInfo src) {
-    clearFilesCache();
-    version = src.version;
-    name = src.name;
-    docCount = src.docCount;
-    dir = src.dir;
-    delGen = src.delGen;
-    docStoreOffset = src.docStoreOffset;
-    docStoreSegment = src.docStoreSegment;
-    docStoreIsCompoundFile = src.docStoreIsCompoundFile;
-    hasVectors = src.hasVectors;
-    hasProx = src.hasProx;
-    fieldInfos = src.fieldInfos == null ? null : src.fieldInfos.clone();
-    if (src.normGen == null) {
-      normGen = null;
-    } else {
-      normGen = new HashMap<Integer, Long>(src.normGen.size());
-      for (Entry<Integer,Long> entry : src.normGen.entrySet()) {
-        normGen.put(entry.getKey(), entry.getValue());
-      }
-    }
-    isCompoundFile = src.isCompoundFile;
-    delCount = src.delCount;
-    codec = src.codec;
-  }
-
   void setDiagnostics(Map<String, String> diagnostics) {
     this.diagnostics = diagnostics;
   }
@@ -169,108 +76,34 @@
    * <p>Note: this is public only to allow access from
    * the codecs package.</p>
    */
-  public SegmentInfo(Directory dir, String version, String name, int docCount, long delGen, int docStoreOffset,
-      String docStoreSegment, boolean docStoreIsCompoundFile, Map<Integer,Long> normGen, boolean isCompoundFile,
-      int delCount, int hasProx, Codec codec, Map<String,String> diagnostics, int hasVectors) {
+  public SegmentInfo(Directory dir, String version, String name, int docCount, 
+                     boolean isCompoundFile, Codec codec, Map<String,String> diagnostics, Map<String,String> attributes) {
+    assert !(dir instanceof TrackingDirectoryWrapper);
     this.dir = dir;
     this.version = version;
     this.name = name;
     this.docCount = docCount;
-    this.delGen = delGen;
-    this.docStoreOffset = docStoreOffset;
-    this.docStoreSegment = docStoreSegment;
-    this.docStoreIsCompoundFile = docStoreIsCompoundFile;
-    this.normGen = normGen;
     this.isCompoundFile = isCompoundFile;
-    this.delCount = delCount;
-    this.hasProx = hasProx;
     this.codec = codec;
     this.diagnostics = diagnostics;
-    this.hasVectors = hasVectors;
-  }
-
-  synchronized void loadFieldInfos(Directory dir, boolean checkCompoundFile) throws IOException {
-    if (fieldInfos == null) {
-      Directory dir0 = dir;
-      if (isCompoundFile && checkCompoundFile) {
-        dir0 = new CompoundFileDirectory(dir, IndexFileNames.segmentFileName(name,
-            "", IndexFileNames.COMPOUND_FILE_EXTENSION), IOContext.READONCE, false);
-      }
-      try {
-        FieldInfosReader reader = codec.fieldInfosFormat().getFieldInfosReader();
-        fieldInfos = reader.read(dir0, name, IOContext.READONCE);
-      } finally {
-        if (dir != dir0) {
-          dir0.close();
-        }
-      }
-    }
+    this.attributes = attributes;
   }
 
   /**
-   * Returns total size in bytes of all of files used by this segment
+   * Returns total size in bytes of all of files used by
+   * this segment.  Note that this will not include any live
+   * docs for the segment; to include that use {@link
+   * SegmentInfoPerCommit#sizeInBytes()} instead.
    */
   public long sizeInBytes() throws IOException {
+    if (sizeInBytes == -1) {
       long sum = 0;
       for (final String fileName : files()) {
         sum += dir.fileLength(fileName);
       }
       sizeInBytes = sum;
-      return sizeInBytes;
-  }
-
-  public boolean getHasVectors() throws IOException {
-    return hasVectors == CHECK_FIELDINFO ? getFieldInfos().hasVectors() : hasVectors == YES;
-  }
-  
-  public FieldInfos getFieldInfos() throws IOException {
-    loadFieldInfos(dir, true);
-    return fieldInfos;
-  }
-
-  public boolean hasDeletions() {
-    // Cases:
-    //
-    //   delGen == NO: this means this segment does not have deletions yet
-    //   delGen >= YES: this means this segment has deletions
-    //
-    return delGen != NO;
-  }
-
-  void advanceDelGen() {
-    if (delGen == NO) {
-      delGen = YES;
-    } else {
-      delGen++;
-    }
-    clearFilesCache();
-  }
-
-  void clearDelGen() {
-    delGen = NO;
-    clearFilesCache();
-  }
-
-  @Override
-  public SegmentInfo clone() {
-    final SegmentInfo si = new SegmentInfo(name, docCount, dir, isCompoundFile, codec,
-        fieldInfos == null ? null : fieldInfos.clone());
-    si.docStoreOffset = docStoreOffset;
-    si.docStoreSegment = docStoreSegment;
-    si.docStoreIsCompoundFile = docStoreIsCompoundFile;
-    si.delGen = delGen;
-    si.delCount = delCount;
-    si.diagnostics = new HashMap<String, String>(diagnostics);
-    if (normGen != null) {
-      si.normGen = new HashMap<Integer, Long>();
-      for (Entry<Integer,Long> entry : normGen.entrySet()) {
-        si.normGen.put(entry.getKey(), entry.getValue());
-      }
     }
-    si.version = version;
-    si.hasProx = hasProx;
-    si.hasVectors = hasVectors;
-    return si;
+    return sizeInBytes;
   }
 
   /**
@@ -278,17 +111,7 @@
    */
   @Deprecated
   boolean hasSeparateNorms() {
-    if (normGen == null) {
-      return false;
-    } else {
-      for (long fieldNormGen : normGen.values()) {
-        if (fieldNormGen >= YES) {
-          return true;
-        }
-      }
-    }
-
-    return false;
+    return getAttribute(Lucene3xSegmentInfoFormat.NORMGEN_KEY) != null;
   }
 
   /**
@@ -299,7 +122,6 @@
    */
   void setUseCompoundFile(boolean isCompoundFile) {
     this.isCompoundFile = isCompoundFile;
-    clearFilesCache();
   }
   
   /**
@@ -310,87 +132,6 @@
     return isCompoundFile;
   }
 
-  public int getDelCount() {
-    return delCount;
-  }
-
-  void setDelCount(int delCount) {
-    this.delCount = delCount;
-    assert delCount <= docCount;
-  }
-
-  /**
-   * @deprecated shared doc stores are not supported in >= 4.0
-   */
-  @Deprecated
-  public int getDocStoreOffset() {
-    // TODO: LUCENE-2555: remove once we don't need to support shared doc stores (pre 4.0)
-    return docStoreOffset;
-  }
-
-  /**
-   * @deprecated shared doc stores are not supported in >= 4.0
-   */
-  @Deprecated
-  public boolean getDocStoreIsCompoundFile() {
-    // TODO: LUCENE-2555: remove once we don't need to support shared doc stores (pre 4.0)
-    return docStoreIsCompoundFile;
-  }
-
-  /**
-   * @deprecated shared doc stores are not supported in >= 4.0
-   */
-  @Deprecated
-  public void setDocStoreIsCompoundFile(boolean docStoreIsCompoundFile) {
-    // TODO: LUCENE-2555: remove once we don't need to support shared doc stores (pre 4.0)
-    this.docStoreIsCompoundFile = docStoreIsCompoundFile;
-    clearFilesCache();
-  }
-
-  /**
-   * @deprecated shared doc stores are not supported in >= 4.0
-   */
-  @Deprecated
-  void setDocStore(int offset, String segment, boolean isCompoundFile) {
-    // TODO: LUCENE-2555: remove once we don't need to support shared doc stores (pre 4.0)
-    docStoreOffset = offset;
-    docStoreSegment = segment;
-    docStoreIsCompoundFile = isCompoundFile;
-    clearFilesCache();
-  }
-
-  /**
-   * @deprecated shared doc stores are not supported in >= 4.0
-   */
-  @Deprecated
-  public String getDocStoreSegment() {
-    // TODO: LUCENE-2555: remove once we don't need to support shared doc stores (pre 4.0)
-    return docStoreSegment;
-  }
-
-  /**
-   * @deprecated shared doc stores are not supported in >= 4.0
-   */
-  @Deprecated
-  void setDocStoreOffset(int offset) {
-    // TODO: LUCENE-2555: remove once we don't need to support shared doc stores (pre 4.0)
-    docStoreOffset = offset;
-    clearFilesCache();
-  }
-
-  /**
-   * @deprecated shared doc stores are not supported in 4.0
-   */
-  @Deprecated
-  public void setDocStoreSegment(String docStoreSegment) {
-    // TODO: LUCENE-2555: remove once we don't need to support shared doc stores (pre 4.0)
-    this.docStoreSegment = docStoreSegment;
-  }
-
-  public boolean getHasProx() throws IOException {
-    return hasProx == CHECK_FIELDINFO ? getFieldInfos().hasProx() : hasProx == YES;
-  }
-
   /** Can only be called once. */
   public void setCodec(Codec codec) {
     assert this.codec == null;
@@ -404,34 +145,32 @@
     return codec;
   }
 
+  public int getDocCount() {
+    if (this.docCount == -1) {
+      throw new IllegalStateException("docCount isn't set yet");
+    }
+    return docCount;
+  }
+
+  // NOTE: leave package private
+  void setDocCount(int docCount) {
+    if (this.docCount != -1) {
+      throw new IllegalStateException("docCount was already set");
+    }
+    this.docCount = docCount;
+  }
+
   /*
    * Return all files referenced by this SegmentInfo.  The
    * returns List is a locally cached List so you should not
    * modify it.
    */
 
-  public List<String> files() throws IOException {
-    final long fisVersion = fieldInfosVersion;
-    if (fisVersion != (fieldInfosVersion = getFieldInfos().getVersion())) {
-      clearFilesCache(); // FIS has modifications - need to recompute
-    } else if (files != null) {
-      // Already cached:
-      return files;
+  public Set<String> files() throws IOException {
+    if (setFiles == null) {
+      throw new IllegalStateException("files were not computed yet");
     }
-    final Set<String> fileSet = new HashSet<String>();
-
-    codec.files(this, fileSet);
-
-    files = new ArrayList<String>(fileSet);
-
-    return files;
-  }
-
-  /* Called whenever any change is made that affects which
-   * files this segment has. */
-  private void clearFilesCache() {
-    files = null;
-    sizeInBytes = -1;
+    return Collections.unmodifiableSet(setFiles);
   }
 
   /** {@inheritDoc} */
@@ -443,16 +182,14 @@
   /** Used for debugging.  Format may suddenly change.
    *
    *  <p>Current format looks like
-   *  <code>_a(3.1):c45/4->_1</code>, which means the segment's
+   *  <code>_a(3.1):c45/4</code>, which means the segment's
    *  name is <code>_a</code>; it was created with Lucene 3.1 (or
    *  '?' if it's unknown); it's using compound file
    *  format (would be <code>C</code> if not compound); it
    *  has 45 documents; it has 4 deletions (this part is
-   *  left off when there are no deletions); it's using the
-   *  shared doc stores named <code>_1</code> (this part is
-   *  left off if doc stores are private).</p>
+   *  left off when there are no deletions).</p>
    */
-  public String toString(Directory dir, int pendingDelCount) {
+  public String toString(Directory dir, int delCount) {
 
     StringBuilder s = new StringBuilder();
     s.append(name).append('(').append(version == null ? "?" : version).append(')').append(':');
@@ -462,35 +199,13 @@
     if (this.dir != dir) {
       s.append('x');
     }
-    try {
-      if (getHasVectors()) {
-        s.append('v');
-      }
-    } catch (Throwable e) {
-      // Messy: because getHasVectors may be used in an
-      // thread-unsafe way, and may attempt to open an fnm
-      // file that has since (legitimately) been deleted by
-      // IndexWriter, instead of throwing these exceptions
-      // up, just add v? to indicate we don't know if this
-      // segment has vectors:
-      s.append("v?");
-    }
     s.append(docCount);
 
-    int delCount = getDelCount() + pendingDelCount;
     if (delCount != 0) {
       s.append('/').append(delCount);
     }
 
-    if (docStoreOffset != -1) {
-      s.append("->").append(docStoreSegment);
-      if (docStoreIsCompoundFile) {
-        s.append('c');
-      } else {
-        s.append('C');
-      }
-      s.append('+').append(docStoreOffset);
-    }
+    // TODO: we could append toString of attributes() here?
 
     return s.toString();
   }
@@ -533,34 +248,53 @@
     return version;
   }
 
-  long getBufferedDeletesGen() {
-    return bufferedDeletesGen;
+  private Set<String> setFiles;
+
+  public void setFiles(Set<String> files) {
+    setFiles = files;
+    sizeInBytes = -1;
   }
 
-  void setBufferedDeletesGen(long v) {
-    bufferedDeletesGen = v;
+  public void addFiles(Collection<String> files) {
+    setFiles.addAll(files);
   }
-  
-  /** @lucene.internal */
-  public long getDelGen() {
-    return delGen;
+
+  public void addFile(String file) {
+    setFiles.add(file);
   }
-  
-  /** @lucene.internal */
-  public Map<Integer,Long> getNormGen() {
-    return normGen;
+    
+  /**
+   * Get a codec attribute value, or null if it does not exist
+   */
+  public String getAttribute(String key) {
+    if (attributes == null) {
+      return null;
+    } else {
+      return attributes.get(key);
+    }
   }
   
-  // TODO: clean up this SI/FI stuff here
-  /** returns the 'real' value for hasProx (doesn't consult fieldinfos) 
-   * @lucene.internal */
-  public int getHasProxInternal() {
-    return hasProx;
+  /**
+   * Puts a codec attribute value.
+   * <p>
+   * This is a key-value mapping for the field that the codec can use
+   * to store additional metadata, and will be available to the codec
+   * when reading the segment via {@link #getAttribute(String)}
+   * <p>
+   * If a value already exists for the field, it will be replaced with 
+   * the new value.
+   */
+  public String putAttribute(String key, String value) {
+    if (attributes == null) {
+      attributes = new HashMap<String,String>();
+    }
+    return attributes.put(key, value);
   }
   
-  /** returns the 'real' value for hasVectors (doesn't consult fieldinfos) 
-   * @lucene.internal */
-  public int getHasVectorsInternal() {
-    return hasVectors;
+  /**
+   * @return internal codec attributes map. May be null if no mappings exist.
+   */
+  public Map<String,String> attributes() {
+    return attributes;
   }
 }


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/index/SegmentInfoPerCommit.java lucene4055/lucene/core/src/java/org/apache/lucene/index/SegmentInfoPerCommit.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/index/SegmentInfoPerCommit.java	1969-12-31 19:00:00.000000000 -0500
+++ lucene4055/lucene/core/src/java/org/apache/lucene/index/SegmentInfoPerCommit.java	2012-05-23 16:36:44.910708914 -0400
@@ -0,0 +1,138 @@
+package org.apache.lucene.index;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Collection;
+import java.util.HashSet;
+
+import org.apache.lucene.store.Directory;
+
+/** Embeds a [read-only] SegmentInfo and adds per-commit
+ *  fields.
+ *
+ *  @lucene.experimental */
+
+public class SegmentInfoPerCommit {
+
+  public final SegmentInfo info;
+
+  // How many deleted docs in the segment:
+  private int delCount;
+
+  // Generation number of the live docs file (-1 if there
+  // are no deletes yet):
+  private long delGen;
+
+  private volatile long sizeInBytes = -1;
+
+  public SegmentInfoPerCommit(SegmentInfo info, int delCount, long delGen) {
+    this.info = info;
+    this.delCount = delCount;
+    this.delGen = delGen;
+  }
+
+  void advanceDelGen() {
+    if (delGen == -1) {
+      delGen = 1;
+    } else {
+      delGen++;
+    }
+    sizeInBytes = -1;
+  }
+
+  public long sizeInBytes() throws IOException {
+    if (sizeInBytes == -1) {
+      final Collection<String> files = new HashSet<String>();
+      info.getCodec().liveDocsFormat().files(this, files);
+      long sum = info.sizeInBytes();
+      for (final String fileName : files()) {
+        sum += info.dir.fileLength(fileName);
+      }
+      sizeInBytes = sum;
+    }
+
+    return sizeInBytes;
+  }
+
+  public Collection<String> files() throws IOException {
+    Collection<String> files = new HashSet<String>(info.files());
+
+    // Must separately add any live docs files:
+    info.getCodec().liveDocsFormat().files(this, files);
+
+    return files;
+  }
+
+  // NOTE: only used in-RAM by IW to track buffered deletes;
+  // this is never written to/read from the Directory
+  private long bufferedDeletesGen;
+  
+  long getBufferedDeletesGen() {
+    return bufferedDeletesGen;
+  }
+
+  void setBufferedDeletesGen(long v) {
+    bufferedDeletesGen = v;
+    sizeInBytes =  -1;
+  }
+  
+  void clearDelGen() {
+    delGen = -1;
+    sizeInBytes =  -1;
+  }
+
+  public void setDelGen(long delGen) {
+    this.delGen = delGen;
+    sizeInBytes =  -1;
+  }
+
+  public boolean hasDeletions() {
+    return delGen != -1;
+  }
+
+  public long getNextDelGen() {
+    if (delGen == -1) {
+      return 1;
+    } else {
+      return delGen + 1;
+    }
+  }
+
+  public long getDelGen() {
+    return delGen;
+  }
+  
+  public int getDelCount() {
+    return delCount;
+  }
+
+  void setDelCount(int delCount) {
+    this.delCount = delCount;
+    assert delCount <= info.getDocCount();
+  }
+
+  public String toString(Directory dir, int pendingDelCount) {
+    return info.toString(dir, delCount + pendingDelCount);
+  }
+
+  @Override
+  public SegmentInfoPerCommit clone() {
+    return new SegmentInfoPerCommit(info, delCount, delGen);
+  }
+}


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/index/SegmentInfos.java lucene4055/lucene/core/src/java/org/apache/lucene/index/SegmentInfos.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/index/SegmentInfos.java	2012-05-24 16:55:48.484233437 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/index/SegmentInfos.java	2012-05-24 09:58:02.947796934 -0400
@@ -32,17 +32,21 @@
 import java.util.Set;
 
 import org.apache.lucene.codecs.Codec;
-import org.apache.lucene.codecs.SegmentInfosReader;
-import org.apache.lucene.codecs.SegmentInfosWriter;
-import org.apache.lucene.index.FieldInfos.FieldNumberBiMap;
+import org.apache.lucene.codecs.LiveDocsFormat;
+import org.apache.lucene.codecs.lucene3x.Lucene3xCodec;
+import org.apache.lucene.codecs.lucene3x.Lucene3xSegmentInfoFormat;
+import org.apache.lucene.codecs.lucene3x.Lucene3xSegmentInfoReader;
 import org.apache.lucene.store.ChecksumIndexInput;
+import org.apache.lucene.store.ChecksumIndexOutput;
+import org.apache.lucene.store.DataOutput; // javadocs
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.DataOutput; // javadocs
 import org.apache.lucene.store.IndexInput;
 import org.apache.lucene.store.IndexOutput;
 import org.apache.lucene.store.NoSuchDirectoryException;
+import org.apache.lucene.util.CodecUtil;
 import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util.StringHelper;
 import org.apache.lucene.util.ThreadInterruptedException;
 
 /**
@@ -55,9 +59,8 @@
  * older segments_N files are present it's because they temporarily cannot be
  * deleted, or, a writer is in the process of committing, or a custom 
  * {@link org.apache.lucene.index.IndexDeletionPolicy IndexDeletionPolicy}
- * is in use). This file lists each segment by name, has details about the
- * separate norms and deletion files, and also contains the size of each
- * segment.
+ * is in use). This file lists each segment by name and has details about the
+ * codec and generation of deletes.
  * </p>
  * <p>There is also a file <tt>segments.gen</tt>. This file contains
  * the current generation (the <tt>_N</tt> in <tt>segments_N</tt>) of the index.
@@ -67,93 +70,84 @@
  * an {@link DataOutput#writeInt Int32} version header 
  * ({@link #FORMAT_SEGMENTS_GEN_CURRENT}), followed by the
  * generation recorded as {@link DataOutput#writeLong Int64}, written twice.</p>
+ * <p>
+ * Files:
+ * <ul>
+ *   <li><tt>segments.gen</tt>: GenHeader, Generation, Generation
+ *   <li><tt>segments_N</tt>: Header, Version, NameCounter, SegCount,
+ *    &lt;SegName, SegCodec, DelGen, DeletionCount&gt;<sup>SegCount</sup>, 
+ *    CommitUserData, Checksum
+ * </ul>
+ * </p>
+ * Data types:
+ * <p>
+ * <ul>
+ *   <li>Header --&gt; {@link CodecUtil#writeHeader CodecHeader}</li>
+ *   <li>GenHeader, NameCounter, SegCount, DeletionCount --&gt; {@link DataOutput#writeInt Int32}</li>
+ *   <li>Generation, Version, DelGen, Checksum --&gt; {@link DataOutput#writeLong Int64}</li>
+ *   <li>SegName, SegCodec --&gt; {@link DataOutput#writeString String}</li>
+ *   <li>CommitUserData --&gt; {@link DataOutput#writeStringStringMap Map&lt;String,String&gt;}</li>
+ * </ul>
+ * </p>
+ * Field Descriptions:
+ * <p>
+ * <ul>
+ *   <li>Version counts how often the index has been changed by adding or deleting
+ *       documents.</li>
+ *   <li>NameCounter is used to generate names for new segment files.</li>
+ *   <li>SegName is the name of the segment, and is used as the file name prefix for
+ *       all of the files that compose the segment's index.</li>
+ *   <li>DelGen is the generation count of the deletes file. If this is -1,
+ *       there are no deletes. Anything above zero means there are deletes 
+ *       stored by {@link LiveDocsFormat}.</li>
+ *   <li>DeletionCount records the number of deleted documents in this segment.</li>
+ *   <li>Checksum contains the CRC32 checksum of all bytes in the segments_N file up
+ *       until the checksum. This is used to verify integrity of the file on opening the
+ *       index.</li>
+ *   <li>SegCodec is the {@link Codec#getName() name} of the Codec that encoded
+ *       this segment.</li>
+ *   <li>CommitUserData stores an optional user-supplied opaque
+ *       Map&lt;String,String&gt; that was passed to {@link IndexWriter#commit(java.util.Map)} 
+ *       or {@link IndexWriter#prepareCommit(java.util.Map)}.</li>
+ * </ul>
+ * </p>
  * 
  * @lucene.experimental
  */
-public final class SegmentInfos implements Cloneable, Iterable<SegmentInfo> {
+public final class SegmentInfos implements Cloneable, Iterable<SegmentInfoPerCommit> {
 
-  /* 
-   * The file format version, a negative number.
-   *  
-   * NOTE: future format numbers must always be one smaller 
-   * than the latest. With time, support for old formats will
-   * be removed, however the numbers should continue to decrease. 
+  /**
+   * The file format version for the segments_N codec header
    */
+  public static final int VERSION_40 = 0;
 
-  // TODO: i don't think we need *all* these version numbers here?
-  // most codecs only need FORMAT_CURRENT? and we should rename it 
-  // to FORMAT_FLEX? because the 'preamble' is just FORMAT_CURRENT + codecname
-  // after that the codec takes over. 
-  
-  // also i think this class should write this, somehow we let 
-  // preflexrw hackishly override this (like seek backwards and overwrite it)
-
-  /** This format adds optional per-segment String
-   *  diagnostics storage, and switches userData to Map */
-  public static final int FORMAT_DIAGNOSTICS = -9;
-
-  /** Each segment records whether it has term vectors */
-  public static final int FORMAT_HAS_VECTORS = -10;
-
-  /** Each segment records the Lucene version that created it. */
-  public static final int FORMAT_3_1 = -11;
-
-  /** Each segment records whether its postings are written
-   *  in the new flex format */
-  public static final int FORMAT_4_0 = -12;
-
-  /** This must always point to the most recent file format.
-   * whenever you add a new format, make it 1 smaller (negative version logic)! */
-  // TODO: move this, as its currently part of required preamble
-  public static final int FORMAT_CURRENT = FORMAT_4_0;
-  
-  /** This must always point to the first supported file format. */
-  public static final int FORMAT_MINIMUM = FORMAT_DIAGNOSTICS;
-  
   /** Used for the segments.gen file only!
    * Whenever you add a new format, make it 1 smaller (negative version logic)! */
   public static final int FORMAT_SEGMENTS_GEN_CURRENT = -2;
-    
+
   public int counter;    // used to name new segments
   
   /**
    * counts how often the index has been changed
    */
   public long version;
-  
+
   private long generation;     // generation of the "segments_N" for the next commit
   private long lastGeneration; // generation of the "segments_N" file we last successfully read
-                                   // or wrote; this is normally the same as generation except if
-                                   // there was an IOException that had interrupted a commit
+                               // or wrote; this is normally the same as generation except if
+                               // there was an IOException that had interrupted a commit
 
   public Map<String,String> userData = Collections.<String,String>emptyMap();       // Opaque Map<String, String> that user can specify during IndexWriter.commit
-
-  private int format;
-  
-  private FieldNumberBiMap globalFieldNumberMap; // this segments global field number map - lazy loaded on demand
-  
-  private List<SegmentInfo> segments = new ArrayList<SegmentInfo>();
-  private Set<SegmentInfo> segmentSet = new HashSet<SegmentInfo>();
-  private transient List<SegmentInfo> cachedUnmodifiableList;
-  private transient Set<SegmentInfo> cachedUnmodifiableSet;  
   
-  private Codec codecFormat;
+  private List<SegmentInfoPerCommit> segments = new ArrayList<SegmentInfoPerCommit>();
   
   /**
    * If non-null, information about loading segments_N files
    * will be printed here.  @see #setInfoStream.
    */
   private static PrintStream infoStream = null;
-  
-  public void setFormat(int format) {
-    this.format = format;
-  }
-
-  public int getFormat() {
-    return format;
-  }
 
-  public SegmentInfo info(int i) {
+  public SegmentInfoPerCommit info(int i) {
     return segments.get(i);
   }
 
@@ -278,44 +272,50 @@
 
     lastGeneration = generation;
 
-    // TODO: scary to have default impl reopen the file... but to make it a bit more flexible,
-    // maybe we could use a plain indexinput here... could default impl rewind/wrap with checksumII,
-    // and any checksumming is then up to implementation?
-    ChecksumIndexInput input = null;
+    ChecksumIndexInput input = new ChecksumIndexInput(directory.openInput(segmentFileName, IOContext.READ));
     try {
-      input = new ChecksumIndexInput(directory.openInput(segmentFileName, IOContext.READ));
       final int format = input.readInt();
-      setFormat(format);
-    
-      // check that it is a format we can understand
-      if (format > FORMAT_MINIMUM)
-        throw new IndexFormatTooOldException(input, format,
-          FORMAT_MINIMUM, FORMAT_CURRENT);
-      if (format < FORMAT_CURRENT)
-        throw new IndexFormatTooNewException(input, format,
-          FORMAT_MINIMUM, FORMAT_CURRENT);
-
-      if (format <= FORMAT_4_0) {
-        codecFormat = Codec.forName(input.readString());
+      if (format == CodecUtil.CODEC_MAGIC) {
+        // 4.0+
+        CodecUtil.checkHeaderNoMagic(input, "segments", VERSION_40, VERSION_40);
+        version = input.readLong();
+        counter = input.readInt();
+        int numSegments = input.readInt();
+        for(int seg=0;seg<numSegments;seg++) {
+          String segName = input.readString();
+          Codec codec = Codec.forName(input.readString());
+          //System.out.println("SIS.read seg=" + seg + " codec=" + codec);
+          SegmentInfo info = codec.segmentInfoFormat().getSegmentInfosReader().read(directory, segName, IOContext.READ);
+          info.setCodec(codec);
+          long delGen = input.readLong();
+          int delCount = input.readInt();
+          assert delCount <= info.getDocCount();
+          add(new SegmentInfoPerCommit(info, delCount, delGen));
+        }
+        userData = input.readStringStringMap();
       } else {
-        codecFormat = Codec.forName("Lucene3x");
+        Lucene3xSegmentInfoReader.readLegacyInfos(this, directory, input, format);
+        Codec codec = Codec.forName("Lucene3x");
+        for (SegmentInfoPerCommit info : this) {
+          info.info.setCodec(codec);
+        }
       }
-      SegmentInfosReader infosReader = codecFormat.segmentInfosFormat().getSegmentInfosReader();
-      infosReader.read(directory, segmentFileName, input, this, IOContext.READ);
+
       final long checksumNow = input.getChecksum();
       final long checksumThen = input.readLong();
-      if (checksumNow != checksumThen)
+      if (checksumNow != checksumThen) {
         throw new CorruptIndexException("checksum mismatch in segments file (resource: " + input + ")");
-      success = true;
-    }
-    finally {
-      if (input != null) {
-        input.close();
       }
+
+      success = true;
+    } finally {
       if (!success) {
         // Clear any segment infos we had loaded so we
         // have a clean slate on retry:
         this.clear();
+        IOUtils.closeWhileHandlingException(input);
+      } else {
+        input.close();
       }
     }
   }
@@ -335,9 +335,9 @@
 
   // Only non-null after prepareCommit has been called and
   // before finishCommit is called
-  IndexOutput pendingSegnOutput;
+  ChecksumIndexOutput pendingSegnOutput;
 
-  private void write(Directory directory, Codec codec) throws IOException {
+  private void write(Directory directory) throws IOException {
 
     String segmentFileName = getNextSegmentFileName();
     
@@ -348,15 +348,38 @@
       generation++;
     }
     
-    IndexOutput segnOutput = null;
-    
-
+    ChecksumIndexOutput segnOutput = null;
     boolean success = false;
 
+    final Set<String> upgradedSIFiles = new HashSet<String>();
+
     try {
-      SegmentInfosWriter infosWriter = codec.segmentInfosFormat().getSegmentInfosWriter();
-      segnOutput = infosWriter.writeInfos(directory, segmentFileName, codec.getName(), this, IOContext.DEFAULT);
-      infosWriter.prepareCommit(segnOutput);
+      segnOutput = new ChecksumIndexOutput(directory.createOutput(segmentFileName, IOContext.DEFAULT));
+      CodecUtil.writeHeader(segnOutput, "segments", VERSION_40);
+      segnOutput.writeLong(version); 
+      segnOutput.writeInt(counter); // write counter
+      segnOutput.writeInt(size()); // write infos
+      for (SegmentInfoPerCommit siPerCommit : this) {
+        SegmentInfo si = siPerCommit.info;
+        segnOutput.writeString(si.name);
+        segnOutput.writeString(si.getCodec().getName());
+        segnOutput.writeLong(siPerCommit.getDelGen());
+        segnOutput.writeInt(siPerCommit.getDelCount());
+        assert si.dir == directory;
+
+        assert siPerCommit.getDelCount() <= si.getDocCount();
+
+        // If this segment is pre-4.x, perform a one-time
+        // "ugprade" to write the .si file for it:
+        String version = si.getVersion();
+        if (version == null || StringHelper.getVersionComparator().compare(version, "4.0") < 0) {
+          String fileName = IndexFileNames.segmentFileName(si.name, "", Lucene3xSegmentInfoFormat.UPGRADED_SI_EXTENSION);
+          if (!directory.fileExists(fileName)) {
+            upgradedSIFiles.add(write3xInfo(directory, si, IOContext.DEFAULT));
+          }
+        }
+      }
+      segnOutput.writeStringStringMap(userData);
       pendingSegnOutput = segnOutput;
       success = true;
     } finally {
@@ -364,6 +387,15 @@
         // We hit an exception above; try to close the file
         // but suppress any exception:
         IOUtils.closeWhileHandlingException(segnOutput);
+
+        for(String fileName : upgradedSIFiles) {
+          try {
+            directory.deleteFile(fileName);
+          } catch (Throwable t) {
+            // Suppress so we keep throwing the original exception
+          }
+        }
+
         try {
           // Try not to leave a truncated segments_N file in
           // the index:
@@ -375,17 +407,47 @@
     }
   }
 
-  /** Prunes any segment whose docs are all deleted. */
-  public void pruneDeletedSegments() {
-    for(final Iterator<SegmentInfo> it = segments.iterator(); it.hasNext();) {
-      final SegmentInfo info = it.next();
-      if (info.getDelCount() == info.docCount) {
-        it.remove();
-        final boolean didRemove = segmentSet.remove(info);
-        assert didRemove;
+  @Deprecated
+  public static String write3xInfo(Directory dir, SegmentInfo si, IOContext context) throws IOException {
+
+    // NOTE: this is NOT how 3.x is really written...
+    String fileName = IndexFileNames.segmentFileName(si.name, "", Lucene3xSegmentInfoFormat.UPGRADED_SI_EXTENSION);
+    si.addFile(fileName);
+
+    //System.out.println("UPGRADE write " + fileName);
+    boolean success = false;
+    IndexOutput output = dir.createOutput(fileName, context);
+    try {
+      // we are about to write this SI in 3.x format, dropping all codec information, etc.
+      // so it had better be a 3.x segment or you will get very confusing errors later.
+      assert si.getCodec() instanceof Lucene3xCodec : "broken test, trying to mix preflex with other codecs";
+      CodecUtil.writeHeader(output, Lucene3xSegmentInfoFormat.UPGRADED_SI_CODEC_NAME, 
+                                    Lucene3xSegmentInfoFormat.UPGRADED_SI_VERSION_CURRENT);
+      // Write the Lucene version that created this segment, since 3.1
+      output.writeString(si.getVersion());
+      output.writeInt(si.getDocCount());
+
+      output.writeStringStringMap(si.attributes());
+
+      output.writeByte((byte) (si.getUseCompoundFile() ? SegmentInfo.YES : SegmentInfo.NO));
+      output.writeStringStringMap(si.getDiagnostics());
+      output.writeStringSet(si.files());
+
+      success = true;
+    } finally {
+      if (!success) {
+        IOUtils.closeWhileHandlingException(output);
+        try {
+          si.dir.deleteFile(fileName);
+        } catch (Throwable t) {
+          // Suppress so we keep throwing the original exception
+        }
+      } else {
+        output.close();
       }
     }
-    assert segmentSet.size() == segments.size();
+
+    return fileName;
   }
 
   /**
@@ -398,12 +460,9 @@
     try {
       final SegmentInfos sis = (SegmentInfos) super.clone();
       // deep clone, first recreate all collections:
-      sis.segments = new ArrayList<SegmentInfo>(size());
-      sis.segmentSet = new HashSet<SegmentInfo>(size());
-      sis.cachedUnmodifiableList = null;
-      sis.cachedUnmodifiableSet = null;
-      for(final SegmentInfo info : this) {
-        assert info.getCodec() != null;
+      sis.segments = new ArrayList<SegmentInfoPerCommit>(size());
+      for(final SegmentInfoPerCommit info : this) {
+        assert info.info.getCodec() != null;
         // dont directly access segments, use add method!!!
         sis.add(info.clone());
       }
@@ -746,10 +805,11 @@
    *  method if changes have been made to this {@link SegmentInfos} instance
    *  </p>  
    **/
-  final void prepareCommit(Directory dir, Codec codec) throws IOException {
-    if (pendingSegnOutput != null)
+  final void prepareCommit(Directory dir) throws IOException {
+    if (pendingSegnOutput != null) {
       throw new IllegalStateException("prepareCommit was already called");
-    write(dir, codec);
+    }
+    write(dir);
   }
 
   /** Returns all file names referenced by SegmentInfo
@@ -771,26 +831,31 @@
     }
     final int size = size();
     for(int i=0;i<size;i++) {
-      final SegmentInfo info = info(i);
-      if (info.dir == dir) {
-        files.addAll(info(i).files());
+      final SegmentInfoPerCommit info = info(i);
+      assert info.info.dir == dir;
+      if (info.info.dir == dir) {
+        files.addAll(info.files());
       }
     }
     return files;
   }
 
-  final void finishCommit(Directory dir, Codec codec) throws IOException {
-    if (pendingSegnOutput == null)
+  final void finishCommit(Directory dir) throws IOException {
+    if (pendingSegnOutput == null) {
       throw new IllegalStateException("prepareCommit was not called");
+    }
     boolean success = false;
     try {
-      SegmentInfosWriter infosWriter = codec.segmentInfosFormat().getSegmentInfosWriter();
-      infosWriter.finishCommit(pendingSegnOutput);
-      pendingSegnOutput = null;
+      pendingSegnOutput.finishCommit();
       success = true;
     } finally {
-      if (!success)
+      if (!success) {
+        IOUtils.closeWhileHandlingException(pendingSegnOutput);
         rollbackCommit(dir);
+      } else {
+        pendingSegnOutput.close();
+        pendingSegnOutput = null;
+      }
     }
 
     // NOTE: if we crash here, we have left a segments_N
@@ -852,9 +917,9 @@
    *  method if changes have been made to this {@link SegmentInfos} instance
    *  </p>  
    **/
-  final void commit(Directory dir, Codec codec) throws IOException {
-    prepareCommit(dir, codec);
-    finishCommit(dir, codec);
+  final void commit(Directory dir) throws IOException {
+    prepareCommit(dir);
+    finishCommit(dir);
   }
 
   public String toString(Directory directory) {
@@ -865,7 +930,7 @@
       if (i > 0) {
         buffer.append(' ');
       }
-      final SegmentInfo info = info(i);
+      final SegmentInfoPerCommit info = info(i);
       buffer.append(info.toString(directory, 0));
     }
     return buffer.toString();
@@ -890,15 +955,14 @@
   void replace(SegmentInfos other) {
     rollbackSegmentInfos(other.asList());
     lastGeneration = other.lastGeneration;
-    format = other.format;
   }
 
   /** Returns sum of all segment's docCounts.  Note that
    *  this does not include deletions */
   public int totalDocCount() {
     int count = 0;
-    for(SegmentInfo info : this) {
-      count += info.docCount;
+    for(SegmentInfoPerCommit info : this) {
+      count += info.info.getDocCount();
     }
     return count;
   }
@@ -909,36 +973,14 @@
     version++;
   }
   
-  /**
-   * Loads or returns the already loaded the global field number map for this {@link SegmentInfos}.
-   * If this {@link SegmentInfos} has no global field number map the returned instance is empty
-   */
-  FieldNumberBiMap getOrLoadGlobalFieldNumberMap() throws IOException {
-    if (globalFieldNumberMap != null) {
-      return globalFieldNumberMap;
-    }
-    final FieldNumberBiMap map  = new FieldNumberBiMap();
-    
-    if (size() > 0) {
-      // build the map up
-      for (SegmentInfo info : this) {
-        final FieldInfos segFieldInfos = info.getFieldInfos();
-        for (FieldInfo fi : segFieldInfos) {
-          map.addOrGet(fi.name, fi.number);
-        }
-      }
-    }
-    return globalFieldNumberMap = map;
-  }
-
   /** applies all changes caused by committing a merge to this SegmentInfos */
   void applyMergeChanges(MergePolicy.OneMerge merge, boolean dropSegment) {
-    final Set<SegmentInfo> mergedAway = new HashSet<SegmentInfo>(merge.segments);
+    final Set<SegmentInfoPerCommit> mergedAway = new HashSet<SegmentInfoPerCommit>(merge.segments);
     boolean inserted = false;
     int newSegIdx = 0;
     for (int segIdx = 0, cnt = segments.size(); segIdx < cnt; segIdx++) {
       assert segIdx >= newSegIdx;
-      final SegmentInfo info = segments.get(segIdx);
+      final SegmentInfoPerCommit info = segments.get(segIdx);
       if (mergedAway.contains(info)) {
         if (!inserted && !dropSegment) {
           segments.set(segIdx, merge.info);
@@ -962,110 +1004,68 @@
     if (!inserted && !dropSegment) {
       segments.add(0, merge.info);
     }
-
-    // update the Set
-    if (!dropSegment) {
-      segmentSet.add(merge.info);
-    }
-    segmentSet.removeAll(mergedAway);
-    
-    assert segmentSet.size() == segments.size();
   }
 
-  List<SegmentInfo> createBackupSegmentInfos(boolean cloneChildren) {
-    if (cloneChildren) {
-      final List<SegmentInfo> list = new ArrayList<SegmentInfo>(size());
-      for(final SegmentInfo info : this) {
-        assert info.getCodec() != null;
-        list.add(info.clone());
-      }
-      return list;
-    } else {
-      return new ArrayList<SegmentInfo>(segments);
+  List<SegmentInfoPerCommit> createBackupSegmentInfos() {
+    final List<SegmentInfoPerCommit> list = new ArrayList<SegmentInfoPerCommit>(size());
+    for(final SegmentInfoPerCommit info : this) {
+      assert info.info.getCodec() != null;
+      list.add(info.clone());
     }
+    return list;
   }
   
-  void rollbackSegmentInfos(List<SegmentInfo> infos) {
+  void rollbackSegmentInfos(List<SegmentInfoPerCommit> infos) {
     this.clear();
     this.addAll(infos);
   }
   
-  /**
-   * Returns the codec used to decode this SegmentInfos from disk 
-   * @lucene.internal
-   */
-  Codec codecFormat() {
-    return codecFormat;
-  }
-  
   /** Returns an <b>unmodifiable</b> {@link Iterator} of contained segments in order. */
   // @Override (comment out until Java 6)
-  public Iterator<SegmentInfo> iterator() {
+  public Iterator<SegmentInfoPerCommit> iterator() {
     return asList().iterator();
   }
   
   /** Returns all contained segments as an <b>unmodifiable</b> {@link List} view. */
-  public List<SegmentInfo> asList() {
-    if (cachedUnmodifiableList == null) {
-      cachedUnmodifiableList = Collections.unmodifiableList(segments);
-    }
-    return cachedUnmodifiableList;
-  }
-  
-  /** Returns all contained segments as an <b>unmodifiable</b> {@link Set} view.
-   * The iterator is not sorted, use {@link List} view or {@link #iterator} to get all segments in order. */
-  public Set<SegmentInfo> asSet() {
-    if (cachedUnmodifiableSet == null) {
-      cachedUnmodifiableSet = Collections.unmodifiableSet(segmentSet);
-    }
-    return cachedUnmodifiableSet;
+  public List<SegmentInfoPerCommit> asList() {
+    return Collections.unmodifiableList(segments);
   }
   
   public int size() {
     return segments.size();
   }
 
-  public void add(SegmentInfo si) {
-    if (segmentSet.contains(si)) {
-      throw new IllegalStateException("Cannot add the same segment two times to this SegmentInfos instance");
-    }
+  public void add(SegmentInfoPerCommit si) {
     segments.add(si);
-    segmentSet.add(si);
-    assert segmentSet.size() == segments.size();
   }
   
-  public void addAll(Iterable<SegmentInfo> sis) {
-    for (final SegmentInfo si : sis) {
+  public void addAll(Iterable<SegmentInfoPerCommit> sis) {
+    for (final SegmentInfoPerCommit si : sis) {
       this.add(si);
     }
   }
   
   public void clear() {
     segments.clear();
-    segmentSet.clear();
   }
-  
-  public void remove(SegmentInfo si) {
-    final int index = this.indexOf(si);
-    if (index >= 0) {
-      this.remove(index);
-    }
+
+  /** WARNING: O(N) cost */
+  public void remove(SegmentInfoPerCommit si) {
+    segments.remove(si);
   }
   
-  public void remove(int index) {
-    segmentSet.remove(segments.remove(index));
-    assert segmentSet.size() == segments.size();
+  /** WARNING: O(N) cost */
+  void remove(int index) {
+    segments.remove(index);
   }
-  
-  public boolean contains(SegmentInfo si) {
-    return segmentSet.contains(si);
+
+  /** WARNING: O(N) cost */
+  boolean contains(SegmentInfoPerCommit si) {
+    return segments.contains(si);
   }
 
-  public int indexOf(SegmentInfo si) {
-    if (segmentSet.contains(si)) {
-      return segments.indexOf(si);
-    } else {
-      return -1;
-    }
+  /** WARNING: O(N) cost */
+  int indexOf(SegmentInfoPerCommit si) {
+    return segments.indexOf(si);
   }
 }


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/index/SegmentMerger.java lucene4055/lucene/core/src/java/org/apache/lucene/index/SegmentMerger.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/index/SegmentMerger.java	2012-05-24 16:55:48.468233437 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/index/SegmentMerger.java	2012-05-23 15:58:26.406668887 -0400
@@ -46,7 +46,6 @@
  */
 final class SegmentMerger {
   private final Directory directory;
-  private final String segment;
   private final int termIndexInterval;
 
   private final Codec codec;
@@ -54,18 +53,22 @@
   private final IOContext context;
   
   private final MergeState mergeState = new MergeState();
+  private final FieldInfos.Builder fieldInfosBuilder;
 
-  SegmentMerger(InfoStream infoStream, Directory dir, int termIndexInterval, String name, MergeState.CheckAbort checkAbort, PayloadProcessorProvider payloadProcessorProvider, FieldInfos fieldInfos, Codec codec, IOContext context) {
+  // note, just like in codec apis Directory 'dir' is NOT the same as segmentInfo.dir!!
+  SegmentMerger(SegmentInfo segmentInfo, InfoStream infoStream, Directory dir, int termIndexInterval,
+                MergeState.CheckAbort checkAbort, PayloadProcessorProvider payloadProcessorProvider,
+                FieldInfos.FieldNumbers fieldNumbers, IOContext context) {
+    mergeState.segmentInfo = segmentInfo;
     mergeState.infoStream = infoStream;
     mergeState.readers = new ArrayList<MergeState.IndexReaderAndLiveDocs>();
-    mergeState.fieldInfos = fieldInfos;
     mergeState.checkAbort = checkAbort;
     mergeState.payloadProcessorProvider = payloadProcessorProvider;
     directory = dir;
-    segment = name;
     this.termIndexInterval = termIndexInterval;
-    this.codec = codec;
+    this.codec = segmentInfo.getCodec();
     this.context = context;
+    this.fieldInfosBuilder = new FieldInfos.Builder(fieldNumbers);
   }
 
   /**
@@ -104,14 +107,14 @@
     // IndexWriter.close(false) takes to actually stop the
     // threads.
     
-    mergeState.mergedDocCount = setDocMaps();
-
-    mergeFieldInfos();
+    mergeState.segmentInfo.setDocCount(setDocMaps());
+    mergeDocValuesAndNormsFieldInfos();
     setMatchingSegmentReaders();
     int numMerged = mergeFields();
-    assert numMerged == mergeState.mergedDocCount;
+    assert numMerged == mergeState.segmentInfo.getDocCount();
 
-    final SegmentWriteState segmentWriteState = new SegmentWriteState(mergeState.infoStream, directory, segment, mergeState.fieldInfos, mergeState.mergedDocCount, termIndexInterval, codec, null, context);
+    final SegmentWriteState segmentWriteState = new SegmentWriteState(mergeState.infoStream, directory, mergeState.segmentInfo,
+                                                                      mergeState.fieldInfos, termIndexInterval, null, context);
     mergeTerms(segmentWriteState);
     mergePerDoc(segmentWriteState);
     
@@ -121,8 +124,12 @@
 
     if (mergeState.fieldInfos.hasVectors()) {
       numMerged = mergeVectors();
-      assert numMerged == mergeState.mergedDocCount;
+      assert numMerged == mergeState.segmentInfo.getDocCount();
     }
+    
+    // write the merged infos
+    FieldInfosWriter fieldInfosWriter = codec.fieldInfosFormat().getFieldInfosWriter();
+    fieldInfosWriter.write(directory, mergeState.segmentInfo.name, mergeState.fieldInfos, context);
 
     return mergeState;
   }
@@ -150,7 +157,8 @@
         boolean same = true;
         FieldInfos segmentFieldInfos = segmentReader.getFieldInfos();
         for (FieldInfo fi : segmentFieldInfos) {
-          if (!mergeState.fieldInfos.fieldName(fi.number).equals(fi.name)) {
+          FieldInfo other = mergeState.fieldInfos.fieldInfo(fi.number);
+          if (other == null || !other.name.equals(fi.name)) {
             same = false;
             break;
           }
@@ -186,17 +194,11 @@
     }
   }
 
-  private void mergeFieldInfos() throws IOException {
-    mergeDocValuesAndNormsFieldInfos();
-    // write the merged infos
-    FieldInfosWriter fieldInfosWriter = codec.fieldInfosFormat()
-        .getFieldInfosWriter();
-    fieldInfosWriter.write(directory, segment, mergeState.fieldInfos, context);
-  }
-
+  // NOTE: this is actually merging all the fieldinfos
   public void mergeDocValuesAndNormsFieldInfos() throws IOException {
     // mapping from all docvalues fields found to their promoted types
-    // this is because FieldInfos does not store the valueSize
+    // this is because FieldInfos does not store the
+    // valueSize
     Map<FieldInfo,TypePromoter> docValuesTypes = new HashMap<FieldInfo,TypePromoter>();
     Map<FieldInfo,TypePromoter> normValuesTypes = new HashMap<FieldInfo,TypePromoter>();
 
@@ -204,7 +206,7 @@
       final AtomicReader reader = readerAndLiveDocs.reader;
       FieldInfos readerFieldInfos = reader.getFieldInfos();
       for (FieldInfo fi : readerFieldInfos) {
-        FieldInfo merged = mergeState.fieldInfos.add(fi);
+        FieldInfo merged = fieldInfosBuilder.add(fi);
         // update the type promotion mapping for this reader
         if (fi.hasDocValues()) {
           TypePromoter previous = docValuesTypes.get(merged);
@@ -218,6 +220,7 @@
     }
     updatePromoted(normValuesTypes, true);
     updatePromoted(docValuesTypes, false);
+    mergeState.fieldInfos = fieldInfosBuilder.finish();
   }
   
   protected void updatePromoted(Map<FieldInfo,TypePromoter> infoAndPromoter, boolean norms) {
@@ -227,21 +230,21 @@
       TypePromoter promoter = e.getValue();
       if (promoter == null) {
         if (norms) {
-          fi.setNormValueType(null, true);
+          fi.setNormValueType(null);
         } else {
-          fi.setDocValuesType(null, true);
+          fi.setDocValuesType(null);
         }
       } else {
         assert promoter != TypePromoter.getIdentityPromoter();
         if (norms) {
-          if (fi.getNormType() != promoter.type()) {
+          if (fi.getNormType() != promoter.type() && !fi.omitsNorms()) {
             // reset the type if we got promoted
-            fi.setNormValueType(promoter.type(), true);
+            fi.setNormValueType(promoter.type());
           }  
         } else {
           if (fi.getDocValuesType() != promoter.type()) {
             // reset the type if we got promoted
-            fi.setDocValuesType(promoter.type(), true);
+            fi.setDocValuesType(promoter.type());
           }
         }
       }
@@ -256,7 +259,7 @@
    * @throws IOException if there is a low-level IO error
    */
   private int mergeFields() throws CorruptIndexException, IOException {
-    final StoredFieldsWriter fieldsWriter = codec.storedFieldsFormat().fieldsWriter(directory, segment, context);
+    final StoredFieldsWriter fieldsWriter = codec.storedFieldsFormat().fieldsWriter(directory, mergeState.segmentInfo, context);
     
     try {
       return fieldsWriter.merge(mergeState);
@@ -270,7 +273,7 @@
    * @throws IOException
    */
   private final int mergeVectors() throws IOException {
-    final TermVectorsWriter termVectorsWriter = codec.termVectorsFormat().vectorsWriter(directory, segment, context);
+    final TermVectorsWriter termVectorsWriter = codec.termVectorsFormat().vectorsWriter(directory, mergeState.segmentInfo, context);
     
     try {
       return termVectorsWriter.merge(mergeState);


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/index/SegmentReader.java lucene4055/lucene/core/src/java/org/apache/lucene/index/SegmentReader.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/index/SegmentReader.java	2012-05-24 16:55:48.476233437 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/index/SegmentReader.java	2012-05-24 11:38:56.831902358 -0400
@@ -36,7 +36,7 @@
  */
 public final class SegmentReader extends AtomicReader {
 
-  private final SegmentInfo si;
+  private final SegmentInfoPerCommit si;
   private final Bits liveDocs;
 
   // Normally set to si.docCount - si.delDocCount, unless we
@@ -50,19 +50,19 @@
    * @throws CorruptIndexException if the index is corrupt
    * @throws IOException if there is a low-level IO error
    */
-  public SegmentReader(SegmentInfo si, int termInfosIndexDivisor, IOContext context) throws IOException {
+  public SegmentReader(SegmentInfoPerCommit si, int termInfosIndexDivisor, IOContext context) throws IOException {
     this.si = si;
-    core = new SegmentCoreReaders(this, si.dir, si, context, termInfosIndexDivisor);
+    core = new SegmentCoreReaders(this, si.info.dir, si, context, termInfosIndexDivisor);
     boolean success = false;
     try {
       if (si.hasDeletions()) {
         // NOTE: the bitvector is stored using the regular directory, not cfs
-        liveDocs = si.getCodec().liveDocsFormat().readLiveDocs(directory(), si, new IOContext(IOContext.READ, true));
+        liveDocs = si.info.getCodec().liveDocsFormat().readLiveDocs(directory(), si, new IOContext(IOContext.READ, true));
       } else {
         assert si.getDelCount() == 0;
         liveDocs = null;
       }
-      numDocs = si.docCount - si.getDelCount();
+      numDocs = si.info.getDocCount() - si.getDelCount();
       success = true;
     } finally {
       // With lock-less commits, it's entirely possible (and
@@ -79,15 +79,17 @@
   // Create new SegmentReader sharing core from a previous
   // SegmentReader and loading new live docs from a new
   // deletes file.  Used by openIfChanged.
-  SegmentReader(SegmentInfo si, SegmentCoreReaders core, IOContext context) throws IOException {
-    this(si, core, si.getCodec().liveDocsFormat().readLiveDocs(si.dir, si, context), si.docCount - si.getDelCount());
+  SegmentReader(SegmentInfoPerCommit si, SegmentCoreReaders core, IOContext context) throws IOException {
+    this(si, core,
+         si.info.getCodec().liveDocsFormat().readLiveDocs(si.info.dir, si, context),
+         si.info.getDocCount() - si.getDelCount());
   }
 
   // Create new SegmentReader sharing core from a previous
   // SegmentReader and using the provided in-memory
   // liveDocs.  Used by IndexWriter to provide a new NRT
   // reader:
-  SegmentReader(SegmentInfo si, SegmentCoreReaders core, Bits liveDocs, int numDocs) throws IOException {
+  SegmentReader(SegmentInfoPerCommit si, SegmentCoreReaders core, Bits liveDocs, int numDocs) throws IOException {
     this.si = si;
     this.core = core;
     core.incRef();
@@ -151,7 +153,7 @@
   @Override
   public int maxDoc() {
     // Don't call ensureOpen() here (it could affect performance)
-    return si.docCount;
+    return si.info.getDocCount();
   }
 
   /** @lucene.internal */
@@ -179,20 +181,20 @@
   public String toString() {
     // SegmentInfo.toString takes dir and number of
     // *pending* deletions; so we reverse compute that here:
-    return si.toString(si.dir, si.docCount - numDocs - si.getDelCount());
+    return si.toString(si.info.dir, si.info.getDocCount() - numDocs - si.getDelCount());
   }
   
   /**
    * Return the name of the segment this reader is reading.
    */
   public String getSegmentName() {
-    return si.name;
+    return si.info.name;
   }
   
   /**
-   * Return the SegmentInfo of the segment this reader is reading.
+   * Return the SegmentInfoPerCommit of the segment this reader is reading.
    */
-  SegmentInfo getSegmentInfo() {
+  SegmentInfoPerCommit getSegmentInfo() {
     return si;
   }
 
@@ -201,7 +203,7 @@
     // Don't ensureOpen here -- in certain cases, when a
     // cloned/reopened reader needs to commit, it may call
     // this method on the closed original reader
-    return si.dir;
+    return si.info.dir;
   }
 
   // This is necessary so that cloned SegmentReaders (which


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/index/SegmentWriteState.java lucene4055/lucene/core/src/java/org/apache/lucene/index/SegmentWriteState.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/index/SegmentWriteState.java	2012-05-24 16:55:48.488233437 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/index/SegmentWriteState.java	2012-05-23 15:13:39.902622103 -0400
@@ -17,7 +17,6 @@
  * limitations under the License.
  */
 
-import org.apache.lucene.codecs.Codec;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.IOContext;
 import org.apache.lucene.util.InfoStream;
@@ -30,9 +29,8 @@
 public class SegmentWriteState {
   public final InfoStream infoStream;
   public final Directory directory;
-  public final String segmentName;
+  public final SegmentInfo segmentInfo;
   public final FieldInfos fieldInfos;
-  public final int numDocs;
   public int delCountOnFlush;
 
   // Deletes to apply while we are flushing the segment.  A
@@ -45,7 +43,6 @@
   // Lazily created:
   public MutableBits liveDocs;
 
-  public final Codec codec;
   public final String segmentSuffix;
 
   /** Expert: The fraction of terms in the "dictionary" which should be stored
@@ -57,16 +54,14 @@
   
   public final IOContext context;
 
-  public SegmentWriteState(InfoStream infoStream, Directory directory, String segmentName, FieldInfos fieldInfos,
-      int numDocs, int termIndexInterval, Codec codec, BufferedDeletes segDeletes, IOContext context) {
+  public SegmentWriteState(InfoStream infoStream, Directory directory, SegmentInfo segmentInfo, FieldInfos fieldInfos,
+      int termIndexInterval, BufferedDeletes segDeletes, IOContext context) {
     this.infoStream = infoStream;
     this.segDeletes = segDeletes;
     this.directory = directory;
-    this.segmentName = segmentName;
+    this.segmentInfo = segmentInfo;
     this.fieldInfos = fieldInfos;
-    this.numDocs = numDocs;
     this.termIndexInterval = termIndexInterval;
-    this.codec = codec;
     segmentSuffix = "";
     this.context = context;
   }
@@ -77,12 +72,10 @@
   public SegmentWriteState(SegmentWriteState state, String segmentSuffix) {
     infoStream = state.infoStream;
     directory = state.directory;
-    segmentName = state.segmentName;
+    segmentInfo = state.segmentInfo;
     fieldInfos = state.fieldInfos;
-    numDocs = state.numDocs;
     termIndexInterval = state.termIndexInterval;
     context = state.context;
-    codec = state.codec;
     this.segmentSuffix = segmentSuffix;
     segDeletes = state.segDeletes;
     delCountOnFlush = state.delCountOnFlush;


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/index/StandardDirectoryReader.java lucene4055/lucene/core/src/java/org/apache/lucene/index/StandardDirectoryReader.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/index/StandardDirectoryReader.java	2012-05-24 16:55:48.460233436 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/index/StandardDirectoryReader.java	2012-05-22 12:04:43.420920074 -0400
@@ -89,8 +89,8 @@
       IOException prior = null;
       boolean success = false;
       try {
-        final SegmentInfo info = infos.info(i);
-        assert info.dir == dir;
+        final SegmentInfoPerCommit info = infos.info(i);
+        assert info.info.dir == dir;
         final ReadersAndLiveDocs rld = writer.readerPool.get(info, true);
         try {
           final SegmentReader reader = rld.getReadOnlyClone(IOContext.READ);
@@ -140,7 +140,7 @@
     
     for (int i = infos.size() - 1; i>=0; i--) {
       // find SegmentReader for this segment
-      Integer oldReaderIndex = segmentReaders.get(infos.info(i).name);
+      Integer oldReaderIndex = segmentReaders.get(infos.info(i).info.name);
       if (oldReaderIndex == null) {
         // this is a new segment, no old SegmentReader can be reused
         newReaders[i] = null;
@@ -153,7 +153,7 @@
       IOException prior = null;
       try {
         SegmentReader newReader;
-        if (newReaders[i] == null || infos.info(i).getUseCompoundFile() != newReaders[i].getSegmentInfo().getUseCompoundFile()) {
+        if (newReaders[i] == null || infos.info(i).info.getUseCompoundFile() != newReaders[i].getSegmentInfo().info.getUseCompoundFile()) {
 
           // this is a new reader; in case we hit an exception we can close it safely
           newReader = new SegmentReader(infos.info(i), termInfosIndexDivisor, IOContext.READ);
@@ -169,7 +169,7 @@
           } else {
             readerShared[i] = false;
             // Steal the ref returned by SegmentReader ctor:
-            assert infos.info(i).dir == newReaders[i].getSegmentInfo().dir;
+            assert infos.info(i).info.dir == newReaders[i].getSegmentInfo().info.dir;
             assert infos.info(i).hasDeletions();
             newReaders[i] = new SegmentReader(infos.info(i), newReaders[i].core, IOContext.READ);
           }


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/index/StoredFieldsConsumer.java lucene4055/lucene/core/src/java/org/apache/lucene/index/StoredFieldsConsumer.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/index/StoredFieldsConsumer.java	2012-05-24 16:55:48.460233436 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/index/StoredFieldsConsumer.java	2012-05-23 15:20:02.070628758 -0400
@@ -58,18 +58,19 @@
   }
 
   public void flush(SegmentWriteState state) throws IOException {
+    int numDocs = state.segmentInfo.getDocCount();
 
-    if (state.numDocs > 0) {
+    if (numDocs > 0) {
       // It's possible that all documents seen in this segment
       // hit non-aborting exceptions, in which case we will
       // not have yet init'd the FieldsWriter:
       initFieldsWriter(state.context);
-      fill(state.numDocs);
+      fill(numDocs);
     }
 
     if (fieldsWriter != null) {
       try {
-        fieldsWriter.finish(state.numDocs);
+        fieldsWriter.finish(state.fieldInfos, numDocs);
       } finally {
         fieldsWriter.close();
         fieldsWriter = null;
@@ -80,7 +81,7 @@
 
   private synchronized void initFieldsWriter(IOContext context) throws IOException {
     if (fieldsWriter == null) {
-      fieldsWriter = codec.storedFieldsFormat().fieldsWriter(docWriter.directory, docWriter.getSegment(), context);
+      fieldsWriter = codec.storedFieldsFormat().fieldsWriter(docWriter.directory, docWriter.getSegmentInfo(), context);
       lastDocID = 0;
     }
   }


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/index/TermsHashConsumer.java lucene4055/lucene/core/src/java/org/apache/lucene/index/TermsHashConsumer.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/index/TermsHashConsumer.java	2012-05-24 16:55:48.480233437 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/index/TermsHashConsumer.java	2012-05-21 13:58:03.507533886 -0400
@@ -21,7 +21,7 @@
 import java.util.Map;
 
 abstract class TermsHashConsumer {
-  abstract void flush(Map<FieldInfo, TermsHashConsumerPerField> fieldsToFlush, final SegmentWriteState state) throws IOException;
+  abstract void flush(Map<String, TermsHashConsumerPerField> fieldsToFlush, final SegmentWriteState state) throws IOException;
   abstract void abort();
   abstract void startDocument() throws IOException;
   abstract void finishDocument(TermsHash termsHash) throws IOException;


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/index/TermsHash.java lucene4055/lucene/core/src/java/org/apache/lucene/index/TermsHash.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/index/TermsHash.java	2012-05-24 16:55:48.468233437 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/index/TermsHash.java	2012-05-21 13:58:03.495533887 -0400
@@ -96,17 +96,17 @@
   }
 
   @Override
-  void flush(Map<FieldInfo,InvertedDocConsumerPerField> fieldsToFlush, final SegmentWriteState state) throws IOException {
-    Map<FieldInfo,TermsHashConsumerPerField> childFields = new HashMap<FieldInfo,TermsHashConsumerPerField>();
-    Map<FieldInfo,InvertedDocConsumerPerField> nextChildFields;
+  void flush(Map<String,InvertedDocConsumerPerField> fieldsToFlush, final SegmentWriteState state) throws IOException {
+    Map<String,TermsHashConsumerPerField> childFields = new HashMap<String,TermsHashConsumerPerField>();
+    Map<String,InvertedDocConsumerPerField> nextChildFields;
 
     if (nextTermsHash != null) {
-      nextChildFields = new HashMap<FieldInfo,InvertedDocConsumerPerField>();
+      nextChildFields = new HashMap<String,InvertedDocConsumerPerField>();
     } else {
       nextChildFields = null;
     }
 
-    for (final Map.Entry<FieldInfo,InvertedDocConsumerPerField> entry : fieldsToFlush.entrySet()) {
+    for (final Map.Entry<String,InvertedDocConsumerPerField> entry : fieldsToFlush.entrySet()) {
       TermsHashPerField perField = (TermsHashPerField) entry.getValue();
       childFields.put(entry.getKey(), perField.consumer);
       if (nextTermsHash != null) {


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/index/TermVectorsConsumer.java lucene4055/lucene/core/src/java/org/apache/lucene/index/TermVectorsConsumer.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/index/TermVectorsConsumer.java	2012-05-24 16:55:48.464233437 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/index/TermVectorsConsumer.java	2012-05-23 15:20:30.510629253 -0400
@@ -49,13 +49,14 @@
   }
 
   @Override
-  void flush(Map<FieldInfo, TermsHashConsumerPerField> fieldsToFlush, final SegmentWriteState state) throws IOException {
+  void flush(Map<String, TermsHashConsumerPerField> fieldsToFlush, final SegmentWriteState state) throws IOException {
     if (writer != null) {
+      int numDocs = state.segmentInfo.getDocCount();
       // At least one doc in this run had term vectors enabled
       try {
-        fill(state.numDocs);
-        assert state.segmentName != null;
-        writer.finish(state.numDocs);
+        fill(numDocs);
+        assert state.segmentInfo != null;
+        writer.finish(state.fieldInfos, numDocs);
       } finally {
         IOUtils.close(writer);
         writer = null;
@@ -84,7 +85,7 @@
   private final void initTermVectorsWriter() throws IOException {
     if (writer == null) {
       IOContext context = new IOContext(new FlushInfo(docWriter.getNumDocsInRAM(), docWriter.bytesUsed()));
-      writer = docWriter.codec.termVectorsFormat().vectorsWriter(docWriter.directory, docWriter.getSegment(), context);
+      writer = docWriter.codec.termVectorsFormat().vectorsWriter(docWriter.directory, docWriter.getSegmentInfo(), context);
       lastDocID = 0;
     }
   }


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/index/TermVectorsConsumerPerField.java lucene4055/lucene/core/src/java/org/apache/lucene/index/TermVectorsConsumerPerField.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/index/TermVectorsConsumerPerField.java	2012-05-24 16:55:48.468233437 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/index/TermVectorsConsumerPerField.java	2012-05-21 13:58:03.499533887 -0400
@@ -91,8 +91,9 @@
    *  RAMOutputStream, which is then quickly flushed to
    *  the real term vectors files in the Directory. */  @Override
   void finish() throws IOException {
-    if (!doVectors || termsHashPerField.bytesHash.size() == 0)
+    if (!doVectors || termsHashPerField.bytesHash.size() == 0) {
       return;
+    }
 
     termsWriter.addFieldToFlush(this);
   }
@@ -148,7 +149,7 @@
 
     termsHashPerField.reset();
 
-    // commit the termVectors once successful success - FI will otherwise reset them
+    // commit the termVectors once successful - FI will otherwise reset them
     fieldInfo.setStoreTermVectors();
   }
 


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/index/TieredMergePolicy.java lucene4055/lucene/core/src/java/org/apache/lucene/index/TieredMergePolicy.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/index/TieredMergePolicy.java	2012-05-24 16:55:48.472233437 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/index/TieredMergePolicy.java	2012-05-23 14:42:53.846589955 -0400
@@ -238,8 +238,8 @@
     return noCFSRatio;
   }
 
-  private class SegmentByteSizeDescending implements Comparator<SegmentInfo> {
-    public int compare(SegmentInfo o1, SegmentInfo o2) {
+  private class SegmentByteSizeDescending implements Comparator<SegmentInfoPerCommit> {
+    public int compare(SegmentInfoPerCommit o1, SegmentInfoPerCommit o2) {
       try {
         final long sz1 = size(o1);
         final long sz2 = size(o2);
@@ -248,7 +248,7 @@
         } else if (sz2 > sz1) {
           return 1;
         } else {
-          return o1.name.compareTo(o2.name);
+          return o1.info.name.compareTo(o2.info.name);
         }
       } catch (IOException ioe) {
         throw new RuntimeException(ioe);
@@ -256,7 +256,7 @@
     }
   }
 
-  private final Comparator<SegmentInfo> segmentByteSizeDescending = new SegmentByteSizeDescending();
+  private final Comparator<SegmentInfoPerCommit> segmentByteSizeDescending = new SegmentByteSizeDescending();
 
   /** Holds score and explanation for a single candidate
    *  merge. */
@@ -273,16 +273,16 @@
     if (infos.size() == 0) {
       return null;
     }
-    final Collection<SegmentInfo> merging = writer.get().getMergingSegments();
-    final Collection<SegmentInfo> toBeMerged = new HashSet<SegmentInfo>();
+    final Collection<SegmentInfoPerCommit> merging = writer.get().getMergingSegments();
+    final Collection<SegmentInfoPerCommit> toBeMerged = new HashSet<SegmentInfoPerCommit>();
 
-    final List<SegmentInfo> infosSorted = new ArrayList<SegmentInfo>(infos.asList());
+    final List<SegmentInfoPerCommit> infosSorted = new ArrayList<SegmentInfoPerCommit>(infos.asList());
     Collections.sort(infosSorted, segmentByteSizeDescending);
 
     // Compute total index bytes & print details about the index
     long totIndexBytes = 0;
     long minSegmentBytes = Long.MAX_VALUE;
-    for(SegmentInfo info : infosSorted) {
+    for(SegmentInfoPerCommit info : infosSorted) {
       final long segBytes = size(info);
       if (verbose()) {
         String extra = merging.contains(info) ? " [merging]" : "";
@@ -335,11 +335,11 @@
       // Gather eligible segments for merging, ie segments
       // not already being merged and not already picked (by
       // prior iteration of this loop) for merging:
-      final List<SegmentInfo> eligible = new ArrayList<SegmentInfo>();
+      final List<SegmentInfoPerCommit> eligible = new ArrayList<SegmentInfoPerCommit>();
       for(int idx = tooBigCount; idx<infosSorted.size(); idx++) {
-        final SegmentInfo info = infosSorted.get(idx);
+        final SegmentInfoPerCommit info = infosSorted.get(idx);
         if (merging.contains(info)) {
-          mergingBytes += info.sizeInBytes();
+          mergingBytes += info.info.sizeInBytes();
         } else if (!toBeMerged.contains(info)) {
           eligible.add(info);
         }
@@ -359,7 +359,7 @@
 
         // OK we are over budget -- find best merge!
         MergeScore bestScore = null;
-        List<SegmentInfo> best = null;
+        List<SegmentInfoPerCommit> best = null;
         boolean bestTooLarge = false;
         long bestMergeBytes = 0;
 
@@ -368,10 +368,10 @@
 
           long totAfterMergeBytes = 0;
 
-          final List<SegmentInfo> candidate = new ArrayList<SegmentInfo>();
+          final List<SegmentInfoPerCommit> candidate = new ArrayList<SegmentInfoPerCommit>();
           boolean hitTooLarge = false;
           for(int idx = startIdx;idx<eligible.size() && candidate.size() < maxMergeAtOnce;idx++) {
-            final SegmentInfo info = eligible.get(idx);
+            final SegmentInfoPerCommit info = eligible.get(idx);
             final long segBytes = size(info);
 
             if (totAfterMergeBytes + segBytes > maxMergedSegmentBytes) {
@@ -410,7 +410,7 @@
           }
           final OneMerge merge = new OneMerge(best);
           spec.add(merge);
-          for(SegmentInfo info : merge.segments) {
+          for(SegmentInfoPerCommit info : merge.segments) {
             toBeMerged.add(info);
           }
 
@@ -427,15 +427,15 @@
   }
 
   /** Expert: scores one merge; subclasses can override. */
-  protected MergeScore score(List<SegmentInfo> candidate, boolean hitTooLarge, long mergingBytes) throws IOException {
+  protected MergeScore score(List<SegmentInfoPerCommit> candidate, boolean hitTooLarge, long mergingBytes) throws IOException {
     long totBeforeMergeBytes = 0;
     long totAfterMergeBytes = 0;
     long totAfterMergeBytesFloored = 0;
-    for(SegmentInfo info : candidate) {
+    for(SegmentInfoPerCommit info : candidate) {
       final long segBytes = size(info);
       totAfterMergeBytes += segBytes;
       totAfterMergeBytesFloored += floorSize(segBytes);
-      totBeforeMergeBytes += info.sizeInBytes();
+      totBeforeMergeBytes += info.info.sizeInBytes();
     }
 
     // Measure "skew" of the merge, which can range
@@ -483,16 +483,16 @@
   }
 
   @Override
-  public MergeSpecification findForcedMerges(SegmentInfos infos, int maxSegmentCount, Map<SegmentInfo,Boolean> segmentsToMerge) throws IOException {
+  public MergeSpecification findForcedMerges(SegmentInfos infos, int maxSegmentCount, Map<SegmentInfoPerCommit,Boolean> segmentsToMerge) throws IOException {
     if (verbose()) {
       message("findForcedMerges maxSegmentCount=" + maxSegmentCount + " infos=" + writer.get().segString(infos) + " segmentsToMerge=" + segmentsToMerge);
     }
 
-    List<SegmentInfo> eligible = new ArrayList<SegmentInfo>();
+    List<SegmentInfoPerCommit> eligible = new ArrayList<SegmentInfoPerCommit>();
     boolean forceMergeRunning = false;
-    final Collection<SegmentInfo> merging = writer.get().getMergingSegments();
+    final Collection<SegmentInfoPerCommit> merging = writer.get().getMergingSegments();
     boolean segmentIsOriginal = false;
-    for(SegmentInfo info : infos) {
+    for(SegmentInfoPerCommit info : infos) {
       final Boolean isOriginal = segmentsToMerge.get(info);
       if (isOriginal != null) {
         segmentIsOriginal = isOriginal;
@@ -560,10 +560,10 @@
     if (verbose()) {
       message("findForcedDeletesMerges infos=" + writer.get().segString(infos) + " forceMergeDeletesPctAllowed=" + forceMergeDeletesPctAllowed);
     }
-    final List<SegmentInfo> eligible = new ArrayList<SegmentInfo>();
-    final Collection<SegmentInfo> merging = writer.get().getMergingSegments();
-    for(SegmentInfo info : infos) {
-      double pctDeletes = 100.*((double) writer.get().numDeletedDocs(info))/info.docCount;
+    final List<SegmentInfoPerCommit> eligible = new ArrayList<SegmentInfoPerCommit>();
+    final Collection<SegmentInfoPerCommit> merging = writer.get().getMergingSegments();
+    for(SegmentInfoPerCommit info : infos) {
+      double pctDeletes = 100.*((double) writer.get().numDeletedDocs(info))/info.info.getDocCount();
       if (pctDeletes > forceMergeDeletesPctAllowed && !merging.contains(info)) {
         eligible.add(info);
       }
@@ -603,7 +603,7 @@
   }
 
   @Override
-  public boolean useCompoundFile(SegmentInfos infos, SegmentInfo mergedInfo) throws IOException {
+  public boolean useCompoundFile(SegmentInfos infos, SegmentInfoPerCommit mergedInfo) throws IOException {
     final boolean doCFS;
 
     if (!useCompoundFile) {
@@ -612,8 +612,9 @@
       doCFS = true;
     } else {
       long totalSize = 0;
-      for (SegmentInfo info : infos)
+      for (SegmentInfoPerCommit info : infos) {
         totalSize += size(info);
+      }
 
       doCFS = size(mergedInfo) <= noCFSRatio * totalSize;
     }
@@ -624,22 +625,22 @@
   public void close() {
   }
 
-  private boolean isMerged(SegmentInfo info)
+  private boolean isMerged(SegmentInfoPerCommit info)
     throws IOException {
     IndexWriter w = writer.get();
     assert w != null;
     boolean hasDeletions = w.numDeletedDocs(info) > 0;
     return !hasDeletions &&
-      !info.hasSeparateNorms() &&
-      info.dir == w.getDirectory() &&
-      (info.getUseCompoundFile() == useCompoundFile || noCFSRatio < 1.0);
+      !info.info.hasSeparateNorms() &&
+      info.info.dir == w.getDirectory() &&
+      (info.info.getUseCompoundFile() == useCompoundFile || noCFSRatio < 1.0);
   }
 
   // Segment size in bytes, pro-rated by % deleted
-  private long size(SegmentInfo info) throws IOException {
-    final long byteSize = info.sizeInBytes();    
+  private long size(SegmentInfoPerCommit info) throws IOException {
+    final long byteSize = info.info.sizeInBytes();    
     final int delCount = writer.get().numDeletedDocs(info);
-    final double delRatio = (info.docCount <= 0 ? 0.0f : ((double)delCount / (double)info.docCount));    
+    final double delRatio = (info.info.getDocCount() <= 0 ? 0.0f : ((double)delCount / (double)info.info.getDocCount()));    
     assert delRatio <= 1.0;
     return (long) (byteSize * (1.0-delRatio));
   }


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/index/TypePromoter.java lucene4055/lucene/core/src/java/org/apache/lucene/index/TypePromoter.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/index/TypePromoter.java	2012-05-24 16:55:48.472233437 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/index/TypePromoter.java	2012-05-23 16:36:44.910708914 -0400
@@ -21,6 +21,10 @@
 
 import org.apache.lucene.index.DocValues.Type;
 
+// TODO: maybe we should not automagically promote
+// types... and instead require a given field always has the
+// same type?
+
 /**
  * Type promoter that promotes {@link DocValues} during merge based on
  * their {@link Type} and {@link #getValueSize()}


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/index/UpgradeIndexMergePolicy.java lucene4055/lucene/core/src/java/org/apache/lucene/index/UpgradeIndexMergePolicy.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/index/UpgradeIndexMergePolicy.java	2012-05-24 16:55:48.472233437 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/index/UpgradeIndexMergePolicy.java	2012-05-22 12:04:43.424920075 -0400
@@ -63,8 +63,8 @@
    * so all segments created with a different version number than this Lucene version will
    * get upgraded.
    */
-  protected boolean shouldUpgradeSegment(SegmentInfo si) {
-    return !Constants.LUCENE_MAIN_VERSION.equals(si.getVersion());
+  protected boolean shouldUpgradeSegment(SegmentInfoPerCommit si) {
+    return !Constants.LUCENE_MAIN_VERSION.equals(si.info.getVersion());
   }
 
   @Override
@@ -79,10 +79,10 @@
   }
   
   @Override
-  public MergeSpecification findForcedMerges(SegmentInfos segmentInfos, int maxSegmentCount, Map<SegmentInfo,Boolean> segmentsToMerge) throws CorruptIndexException, IOException {
+  public MergeSpecification findForcedMerges(SegmentInfos segmentInfos, int maxSegmentCount, Map<SegmentInfoPerCommit,Boolean> segmentsToMerge) throws CorruptIndexException, IOException {
     // first find all old segments
-    final Map<SegmentInfo,Boolean> oldSegments = new HashMap<SegmentInfo,Boolean>();
-    for (final SegmentInfo si : segmentInfos) {
+    final Map<SegmentInfoPerCommit,Boolean> oldSegments = new HashMap<SegmentInfoPerCommit,Boolean>();
+    for (final SegmentInfoPerCommit si : segmentInfos) {
       final Boolean v = segmentsToMerge.get(si);
       if (v != null && shouldUpgradeSegment(si)) {
         oldSegments.put(si, v);
@@ -112,8 +112,8 @@
         message("findForcedMerges: " +  base.getClass().getSimpleName() +
         " does not want to merge all old segments, merge remaining ones into new segment: " + oldSegments);
       }
-      final List<SegmentInfo> newInfos = new ArrayList<SegmentInfo>();
-      for (final SegmentInfo si : segmentInfos) {
+      final List<SegmentInfoPerCommit> newInfos = new ArrayList<SegmentInfoPerCommit>();
+      for (final SegmentInfoPerCommit si : segmentInfos) {
         if (oldSegments.containsKey(si)) {
           newInfos.add(si);
         }
@@ -134,7 +134,7 @@
   }
   
   @Override
-  public boolean useCompoundFile(SegmentInfos segments, SegmentInfo newSegment) throws IOException {
+  public boolean useCompoundFile(SegmentInfos segments, SegmentInfoPerCommit newSegment) throws IOException {
     return base.useCompoundFile(segments, newSegment);
   }
   


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/store/ChecksumIndexOutput.java lucene4055/lucene/core/src/java/org/apache/lucene/store/ChecksumIndexOutput.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/store/ChecksumIndexOutput.java	2012-05-24 16:55:48.512233436 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/store/ChecksumIndexOutput.java	2012-05-22 11:39:04.620893278 -0400
@@ -71,26 +71,7 @@
     throw new UnsupportedOperationException();    
   }
 
-  /**
-   * Starts but does not complete the commit of this file (=
-   * writing of the final checksum at the end).  After this
-   * is called must call {@link #finishCommit} and the
-   * {@link #close} to complete the commit.
-   */
-  public void prepareCommit() throws IOException {
-    final long checksum = getChecksum();
-    // Intentionally write a mismatched checksum.  This is
-    // because we want to 1) test, as best we can, that we
-    // are able to write a long to the file, but 2) not
-    // actually "commit" the file yet.  This (prepare
-    // commit) is phase 1 of a two-phase commit.
-    final long pos = main.getFilePointer();
-    main.writeLong(checksum-1);
-    main.flush();
-    main.seek(pos);
-  }
-
-  /** See {@link #prepareCommit} */
+  /** writes the checksum */
   public void finishCommit() throws IOException {
     main.writeLong(getChecksum());
   }


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/store/DataInput.java lucene4055/lucene/core/src/java/org/apache/lucene/store/DataInput.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/store/DataInput.java	2012-05-24 16:55:48.520233438 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/store/DataInput.java	2012-05-21 13:58:03.571533888 -0400
@@ -19,7 +19,9 @@
 
 import java.io.IOException;
 import java.util.HashMap;
+import java.util.HashSet;
 import java.util.Map;
+import java.util.Set;
 
 import org.apache.lucene.util.IOUtils;
 
@@ -201,6 +203,8 @@
     return clone;
   }
 
+  /** Reads a Map&lt;String,String&gt; previously written
+   *  with {@link DataOutput#writeStringStringMap}. */
   public Map<String,String> readStringStringMap() throws IOException {
     final Map<String,String> map = new HashMap<String,String>();
     final int count = readInt();
@@ -212,4 +216,16 @@
 
     return map;
   }
+
+  /** Reads a Set&lt;String&gt; previously written
+   *  with {@link DataOutput#writeStringSet}. */
+  public Set<String> readStringSet() throws IOException {
+    final Set<String> set = new HashSet<String>();
+    final int count = readInt();
+    for(int i=0;i<count;i++) {
+      set.add(readString());
+    }
+
+    return set;
+  }
 }


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/store/DataOutput.java lucene4055/lucene/core/src/java/org/apache/lucene/store/DataOutput.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/store/DataOutput.java	2012-05-24 16:55:48.520233438 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/store/DataOutput.java	2012-05-21 13:58:03.571533888 -0400
@@ -19,6 +19,7 @@
 
 import java.io.IOException;
 import java.util.Map;
+import java.util.Set;
 
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.UnicodeUtil;
@@ -273,4 +274,24 @@
       }
     }
   }
+
+  /**
+   * Writes a String set.
+   * <p>
+   * First the size is written as an {@link #writeInt(int) Int32},
+   * followed by each value written as a
+   * {@link #writeString(String) String}.
+   * 
+   * @param set Input set. May be null (equivalent to an empty set)
+   */
+  public void writeStringSet(Set<String> set) throws IOException {
+    if (set == null) {
+      writeInt(0);
+    } else {
+      writeInt(set.size());
+      for(String value : set) {
+        writeString(value);
+      }
+    }
+  }
 }


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/store/Directory.java lucene4055/lucene/core/src/java/org/apache/lucene/store/Directory.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/store/Directory.java	2012-05-24 16:55:48.516233437 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/store/Directory.java	2012-05-21 13:58:03.567533887 -0400
@@ -153,7 +153,7 @@
    * their own locking implementation.
    */
   public LockFactory getLockFactory() {
-      return this.lockFactory;
+    return this.lockFactory;
   }
 
   /**


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/store/RateLimiter.java lucene4055/lucene/core/src/java/org/apache/lucene/store/RateLimiter.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/store/RateLimiter.java	2012-05-24 16:55:48.512233436 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/store/RateLimiter.java	2012-05-21 13:58:03.563533887 -0400
@@ -62,6 +62,9 @@
    *  might exceed the target).  It's best to call this
    *  with a biggish count, not one byte at a time. */
   public void pause(long bytes) {
+    if (bytes == 1) {
+      return;
+    }
 
     // TODO: this is purely instantaneous rate; maybe we
     // should also offer decayed recent history one?


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/store/TrackingDirectoryWrapper.java lucene4055/lucene/core/src/java/org/apache/lucene/store/TrackingDirectoryWrapper.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/store/TrackingDirectoryWrapper.java	1969-12-31 19:00:00.000000000 -0500
+++ lucene4055/lucene/core/src/java/org/apache/lucene/store/TrackingDirectoryWrapper.java	2012-05-21 13:58:03.571533888 -0400
@@ -0,0 +1,126 @@
+package org.apache.lucene.store;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.Closeable;
+import java.io.IOException;
+import java.util.Collections;
+import java.util.Collection;
+import java.util.HashSet;
+import java.util.Set;
+
+/** A delegating Directory that records which files were
+ *  written to and deleted. */
+public final class TrackingDirectoryWrapper extends Directory implements Closeable {
+
+  private final Directory other;
+  private final Set<String> createdFileNames = Collections.synchronizedSet(new HashSet<String>());
+
+  public TrackingDirectoryWrapper(Directory other) {
+    this.other = other;
+  }
+
+  @Override
+  public String[] listAll() throws IOException {
+    return other.listAll();
+  }
+
+  @Override
+  public boolean fileExists(String name) throws IOException {
+    return other.fileExists(name);
+  }
+
+  @Override
+  public void deleteFile(String name) throws IOException {
+    createdFileNames.remove(name);
+    other.deleteFile(name);
+  }
+
+  @Override
+  public long fileLength(String name) throws IOException {
+    return other.fileLength(name);
+  }
+
+  @Override
+  public IndexOutput createOutput(String name, IOContext context) throws IOException {
+    createdFileNames.add(name);
+    return other.createOutput(name, context);
+  }
+
+  @Override
+  public void sync(Collection<String> names) throws IOException {
+    other.sync(names);
+  }
+
+  @Override
+  public IndexInput openInput(String name, IOContext context) throws IOException {
+    return other.openInput(name, context);
+  }
+
+  @Override
+  public Lock makeLock(String name) {
+    return other.makeLock(name);
+  }
+
+  @Override
+  public void clearLock(String name) throws IOException {
+    other.clearLock(name);
+  }
+
+  @Override
+  public void close() throws IOException {
+    other.close();
+  }
+
+  @Override
+  public void setLockFactory(LockFactory lockFactory) throws IOException {
+    other.setLockFactory(lockFactory);
+  }
+
+  @Override
+  public LockFactory getLockFactory() {
+    return other.getLockFactory();
+  }
+
+  @Override
+  public String getLockID() {
+    return other.getLockID();
+  }
+
+  @Override
+  public String toString() {
+    return "TrackingDirectoryWrapper(" + other.toString() + ")";
+  }
+
+  @Override
+  public void copy(Directory to, String src, String dest, IOContext context) throws IOException {
+    createdFileNames.add(dest);
+    other.copy(to, src, dest, context);
+  }
+
+  @Override
+  public Directory.IndexInputSlicer createSlicer(final String name, final IOContext context) throws IOException {
+    return other.createSlicer(name, context);
+  }
+
+  // maybe clone before returning.... all callers are
+  // cloning anyway....
+  public Set<String> getCreatedFiles() {
+    return createdFileNames;
+  }
+}


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/util/CodecUtil.java lucene4055/lucene/core/src/java/org/apache/lucene/util/CodecUtil.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/util/CodecUtil.java	2012-05-24 16:55:48.632233439 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/util/CodecUtil.java	2012-05-22 11:39:04.620893278 -0400
@@ -126,7 +126,14 @@
     if (actualHeader != CODEC_MAGIC) {
       throw new CorruptIndexException("codec header mismatch: actual header=" + actualHeader + " vs expected header=" + CODEC_MAGIC + " (resource: " + in + ")");
     }
+    return checkHeaderNoMagic(in, codec, minVersion, maxVersion);
+  }
 
+  /** Like {@link
+   *  #checkHeader(DataInput,String,int,int)} except this
+   *  version assumes the first int has already been read
+   *  and validated from the input. */
+  public static int checkHeaderNoMagic(DataInput in, String codec, int minVersion, int maxVersion) throws IOException {
     final String actualCodec = in.readString();
     if (!actualCodec.equals(codec)) {
       throw new CorruptIndexException("codec mismatch: actual codec=" + actualCodec + " vs expected codec=" + codec + " (resource: " + in + ")");


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/util/NamedSPILoader.java lucene4055/lucene/core/src/java/org/apache/lucene/util/NamedSPILoader.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/util/NamedSPILoader.java	2012-05-24 16:55:48.632233439 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/util/NamedSPILoader.java	2012-05-22 11:39:04.620893278 -0400
@@ -28,6 +28,7 @@
  * Helper class for loading named SPIs from classpath (e.g. Codec, PostingsFormat).
  * @lucene.internal
  */
+// TODO: would be nice to have case insensitive lookups.
 public final class NamedSPILoader<S extends NamedSPILoader.NamedSPI> implements Iterable<S> {
 
   private final Map<String,S> services;
@@ -51,6 +52,7 @@
       // this allows to place services before others in classpath to make 
       // them used instead of others
       if (!services.containsKey(name)) {
+        assert checkServiceName(name);
         services.put(name, service);
       }
     }
@@ -58,6 +60,37 @@
     this.services = Collections.unmodifiableMap(services);
   }
   
+  /**
+   * Validates that a service name meets the requirements of {@link NamedSPI}
+   */
+  public static boolean checkServiceName(String name) {
+    // based on harmony charset.java
+    if (name.length() >= 128) {
+      throw new IllegalArgumentException("Illegal service name: '" + name + "' is too long (must be < 128 chars).");
+    }
+    for (int i = 0; i < name.length(); i++) {
+      char c = name.charAt(i);
+      if (!isLetter(c) && !isDigit(c)) {
+        throw new IllegalArgumentException("Illegal service name: '" + name + "' must be simple ascii alphanumeric.");
+      }
+    }
+    return true;
+  }
+  
+  /*
+   * Checks whether a character is a letter (ascii) which are defined in the spec.
+   */
+  private static boolean isLetter(char c) {
+      return ('a' <= c && c <= 'z') || ('A' <= c && c <= 'Z');
+  }
+
+  /*
+   * Checks whether a character is a digit (ascii) which are defined in the spec.
+   */
+  private static boolean isDigit(char c) {
+      return ('0' <= c && c <= '9');
+  }
+  
   public S lookup(String name) {
     final S service = services.get(name);
     if (service != null) return service;
@@ -76,6 +109,8 @@
   
   /**
    * Interface to support {@link NamedSPILoader#lookup(String)} by name.
+   * <p>
+   * Names must be all ascii alphanumeric, and less than 128 characters in length.
    */
   public static interface NamedSPI {
     String getName();


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/java/org/apache/lucene/util/PrintStreamInfoStream.java lucene4055/lucene/core/src/java/org/apache/lucene/util/PrintStreamInfoStream.java
--- lucene-trunk/lucene/core/src/java/org/apache/lucene/util/PrintStreamInfoStream.java	2012-05-24 16:55:48.532233438 -0400
+++ lucene4055/lucene/core/src/java/org/apache/lucene/util/PrintStreamInfoStream.java	2012-05-21 13:58:03.595533888 -0400
@@ -33,7 +33,7 @@
   private static final AtomicInteger MESSAGE_ID = new AtomicInteger();
   protected final int messageID;
   
-  private final PrintStream stream;
+  protected final PrintStream stream;
   
   public PrintStreamInfoStream(PrintStream stream) {
     this(stream, MESSAGE_ID.getAndIncrement());


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/test/org/apache/lucene/index/TestAddIndexes.java lucene4055/lucene/core/src/test/org/apache/lucene/index/TestAddIndexes.java
--- lucene-trunk/lucene/core/src/test/org/apache/lucene/index/TestAddIndexes.java	2012-05-24 16:55:47.780233425 -0400
+++ lucene4055/lucene/core/src/test/org/apache/lucene/index/TestAddIndexes.java	2012-05-22 17:13:19.777242528 -0400
@@ -30,7 +30,7 @@
 import org.apache.lucene.codecs.LiveDocsFormat;
 import org.apache.lucene.codecs.NormsFormat;
 import org.apache.lucene.codecs.PostingsFormat;
-import org.apache.lucene.codecs.SegmentInfosFormat;
+import org.apache.lucene.codecs.SegmentInfoFormat;
 import org.apache.lucene.codecs.StoredFieldsFormat;
 import org.apache.lucene.codecs.TermVectorsFormat;
 import org.apache.lucene.codecs.lucene40.Lucene40Codec;
@@ -38,7 +38,7 @@
 import org.apache.lucene.codecs.lucene40.Lucene40FieldInfosFormat;
 import org.apache.lucene.codecs.lucene40.Lucene40LiveDocsFormat;
 import org.apache.lucene.codecs.lucene40.Lucene40NormsFormat;
-import org.apache.lucene.codecs.lucene40.Lucene40SegmentInfosFormat;
+import org.apache.lucene.codecs.lucene40.Lucene40SegmentInfoFormat;
 import org.apache.lucene.codecs.lucene40.Lucene40StoredFieldsFormat;
 import org.apache.lucene.codecs.lucene40.Lucene40TermVectorsFormat;
 import org.apache.lucene.codecs.pulsing.Pulsing40PostingsFormat;
@@ -1114,8 +1114,8 @@
     w3.addIndexes(readers);
     w3.close();
     // we should now see segments_X,
-    // segments.gen,_Y.cfs,_Y.cfe, _Z.fnx
-    assertEquals("Only one compound segment should exist, but got: " + Arrays.toString(dir.listAll()), 4, dir.listAll().length);
+    // segments.gen,_Y.cfs,_Y.cfe, _Z.si
+    assertEquals("Only one compound segment should exist, but got: " + Arrays.toString(dir.listAll()), 5, dir.listAll().length);
     dir.close();
   }
   
@@ -1150,8 +1150,8 @@
     }
 
     @Override
-    public SegmentInfosFormat segmentInfosFormat() {
-      return new Lucene40SegmentInfosFormat();
+    public SegmentInfoFormat segmentInfoFormat() {
+      return new Lucene40SegmentInfoFormat();
     }
 
     @Override


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/test/org/apache/lucene/index/TestBackwardsCompatibility.java lucene4055/lucene/core/src/test/org/apache/lucene/index/TestBackwardsCompatibility.java
--- lucene-trunk/lucene/core/src/test/org/apache/lucene/index/TestBackwardsCompatibility.java	2012-05-24 16:55:47.784233425 -0400
+++ lucene4055/lucene/core/src/test/org/apache/lucene/index/TestBackwardsCompatibility.java	2012-05-23 12:12:19.134432620 -0400
@@ -230,10 +230,16 @@
 
   public void testAddOldIndexes() throws IOException {
     for (String name : oldNames) {
+      if (VERBOSE) {
+        System.out.println("\nTEST: old index " + name);
+      }
       Directory targetDir = newDirectory();
       IndexWriter w = new IndexWriter(targetDir, newIndexWriterConfig(
           TEST_VERSION_CURRENT, new MockAnalyzer(random())));
       w.addIndexes(oldIndexDirs.get(name));
+      if (VERBOSE) {
+        System.out.println("\nTEST: done adding indices; now close");
+      }
       w.close();
       
       targetDir.close();
@@ -323,11 +329,14 @@
           assertEquals("field with non-ascii name", f.stringValue());
         }
 
-        Terms tfv = reader.getTermVectors(i).terms("utf8");
+        Fields tfvFields = reader.getTermVectors(i);
+        assertNotNull("i=" + i, tfvFields);
+        Terms tfv = tfvFields.terms("utf8");
         assertNotNull("docID=" + i + " index=" + oldName, tfv);
-      } else
+      } else {
         // Only ID 7 is deleted
         assertEquals(7, i);
+      }
     }
     
     ScoreDoc[] hits = searcher.search(new TermQuery(new Term(new String("content"), "aaa")), null, 1000).scoreDocs;
@@ -507,14 +516,16 @@
       // Now verify file names... TODO: fix this test better, we could populate from 
       // separateFiles() or something.
       String[] expected = new String[] {"_0.cfs", "_0.cfe",
-                               "_0_1.del",
-                               "segments_2",
-                               "segments.gen"};
+                                        "_0_1.del",
+                                        "_0.si",
+                                        "segments_2",
+                                        "segments.gen"};
       
       String[] expectedSimpleText = new String[] {"_0.cfs", "_0.cfe",
-          "_0_1.liv",
-          "segments_2",
-          "segments.gen"};
+                                                  "_0_1.liv",
+                                                  "_0.si",
+                                                  "segments_2",
+                                                  "segments.gen"};
 
       String[] actual = dir.listAll();
       Arrays.sort(expected);
@@ -671,8 +682,8 @@
     if (VERBOSE) {
       System.out.println("checkAllSegmentsUpgraded: " + infos);
     }
-    for (SegmentInfo si : infos) {
-      assertEquals(Constants.LUCENE_MAIN_VERSION, si.getVersion());
+    for (SegmentInfoPerCommit si : infos) {
+      assertEquals(Constants.LUCENE_MAIN_VERSION, si.info.getVersion());
     }
     return infos.size();
   }


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/test/org/apache/lucene/index/TestCodecs.java lucene4055/lucene/core/src/test/org/apache/lucene/index/TestCodecs.java
--- lucene-trunk/lucene/core/src/test/org/apache/lucene/index/TestCodecs.java	2012-05-24 16:55:47.792233425 -0400
+++ lucene4055/lucene/core/src/test/org/apache/lucene/index/TestCodecs.java	2012-05-24 10:06:24.743805673 -0400
@@ -31,9 +31,9 @@
 import org.apache.lucene.codecs.lucene3x.Lucene3xCodec;
 import org.apache.lucene.codecs.mocksep.MockSepPostingsFormat;
 import org.apache.lucene.document.Document;
-import org.apache.lucene.index.FieldInfo.IndexOptions;
 import org.apache.lucene.document.FieldType;
 import org.apache.lucene.document.TextField;
+import org.apache.lucene.index.FieldInfo.IndexOptions;
 import org.apache.lucene.search.DocIdSetIterator;
 import org.apache.lucene.search.IndexSearcher;
 import org.apache.lucene.search.PhraseQuery;
@@ -41,6 +41,7 @@
 import org.apache.lucene.search.ScoreDoc;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.Constants;
 import org.apache.lucene.util.InfoStream;
 import org.apache.lucene.util.LuceneTestCase;
 import org.apache.lucene.util.OpenBitSet;
@@ -87,14 +88,12 @@
     final boolean omitTF;
     final boolean storePayloads;
 
-    public FieldData(final String name, final FieldInfos fieldInfos, final TermData[] terms, final boolean omitTF, final boolean storePayloads) {
+    public FieldData(final String name, final FieldInfos.Builder fieldInfos, final TermData[] terms, final boolean omitTF, final boolean storePayloads) {
       this.omitTF = omitTF;
       this.storePayloads = storePayloads;
-      fieldInfos.addOrUpdate(name, true);
-      fieldInfo = fieldInfos.fieldInfo(name);
       // TODO: change this test to use all three
-      fieldInfo.indexOptions = omitTF ? IndexOptions.DOCS_ONLY : IndexOptions.DOCS_AND_FREQS_AND_POSITIONS;
-      fieldInfo.storePayloads = storePayloads;
+      fieldInfos.addOrUpdate(name, true, false, false, storePayloads, omitTF ? IndexOptions.DOCS_ONLY : IndexOptions.DOCS_AND_FREQS_AND_POSITIONS, null, null);
+      fieldInfo = fieldInfos.fieldInfo(name);
       this.terms = terms;
       for(int i=0;i<terms.length;i++)
         terms[i].field = this;
@@ -249,16 +248,15 @@
       terms[i] = new TermData(text, docs, null);
     }
 
-    final FieldInfos fieldInfos = new FieldInfos(new FieldInfos.FieldNumberBiMap());
+    final FieldInfos.Builder builder = new FieldInfos.Builder();
 
-    final FieldData field = new FieldData("field", fieldInfos, terms, true, false);
+    final FieldData field = new FieldData("field", builder, terms, true, false);
     final FieldData[] fields = new FieldData[] {field};
-
+    final FieldInfos fieldInfos = builder.finish();
     final Directory dir = newDirectory();
-    FieldInfos clonedFieldInfos = fieldInfos.clone();
     this.write(fieldInfos, dir, fields, true);
     Codec codec = Codec.getDefault();
-    final SegmentInfo si = new SegmentInfo(SEGMENT, 10000, dir, false, codec, clonedFieldInfos);
+    final SegmentInfo si = new SegmentInfo(dir, Constants.LUCENE_MAIN_VERSION, SEGMENT, 10000, false, codec, null, null);
 
     final FieldsProducer reader = codec.postingsFormat().fieldsProducer(new SegmentReadState(dir, si, fieldInfos, newIOContext(random()), DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR));
 
@@ -296,25 +294,26 @@
   }
 
   public void testRandomPostings() throws Throwable {
-    final FieldInfos fieldInfos = new FieldInfos(new FieldInfos.FieldNumberBiMap());
+    final FieldInfos.Builder builder = new FieldInfos.Builder();
 
     final FieldData[] fields = new FieldData[NUM_FIELDS];
     for(int i=0;i<NUM_FIELDS;i++) {
       final boolean omitTF = 0==(i%3);
       final boolean storePayloads = 1==(i%3);
-      fields[i] = new FieldData(fieldNames[i], fieldInfos, this.makeRandomTerms(omitTF, storePayloads), omitTF, storePayloads);
+      fields[i] = new FieldData(fieldNames[i], builder, this.makeRandomTerms(omitTF, storePayloads), omitTF, storePayloads);
     }
 
     final Directory dir = newDirectory();
+    final FieldInfos fieldInfos = builder.finish();
 
     if (VERBOSE) {
       System.out.println("TEST: now write postings");
     }
 
-    FieldInfos clonedFieldInfos = fieldInfos.clone();
     this.write(fieldInfos, dir, fields, false);
     Codec codec = Codec.getDefault();
-    final SegmentInfo si = new SegmentInfo(SEGMENT, 10000, dir, false, codec, clonedFieldInfos);
+    final SegmentInfo si = new SegmentInfo(dir, Constants.LUCENE_MAIN_VERSION, SEGMENT, 10000,
+                                           false, codec, null, null);
 
     if (VERBOSE) {
       System.out.println("TEST: now read postings");
@@ -618,7 +617,8 @@
 
     final int termIndexInterval = _TestUtil.nextInt(random(), 13, 27);
     final Codec codec = Codec.getDefault();
-    final SegmentWriteState state = new SegmentWriteState(InfoStream.getDefault(), dir, SEGMENT, fieldInfos, 10000, termIndexInterval, codec, null, newIOContext(random()));
+    final SegmentInfo si = new SegmentInfo(dir, Constants.LUCENE_MAIN_VERSION, SEGMENT, 10000, false, codec, null, null);
+    final SegmentWriteState state = new SegmentWriteState(InfoStream.getDefault(), dir, si, fieldInfos, termIndexInterval, null, newIOContext(random()));
 
     final FieldsConsumer consumer = codec.postingsFormat().fieldsConsumer(state);
     Arrays.sort(fields);


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/test/org/apache/lucene/index/TestConsistentFieldNumbers.java lucene4055/lucene/core/src/test/org/apache/lucene/index/TestConsistentFieldNumbers.java
--- lucene-trunk/lucene/core/src/test/org/apache/lucene/index/TestConsistentFieldNumbers.java	2012-05-24 16:55:47.800233425 -0400
+++ lucene4055/lucene/core/src/test/org/apache/lucene/index/TestConsistentFieldNumbers.java	2012-05-22 12:04:43.448920076 -0400
@@ -26,9 +26,12 @@
 import org.apache.lucene.document.StoredField;
 import org.apache.lucene.document.StringField;
 import org.apache.lucene.document.TextField;
+import org.apache.lucene.store.CompoundFileDirectory;
 import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
 import org.apache.lucene.util.FailOnNonBulkMergesInfoStream;
 import org.apache.lucene.util.LuceneTestCase;
+import org.apache.lucene.util._TestUtil;
 import org.junit.Test;
 
 public class TestConsistentFieldNumbers extends LuceneTestCase {
@@ -66,8 +69,8 @@
       sis.read(dir);
       assertEquals(2, sis.size());
 
-      FieldInfos fis1 = sis.info(0).getFieldInfos();
-      FieldInfos fis2 = sis.info(1).getFieldInfos();
+      FieldInfos fis1 = _TestUtil.getFieldInfos(sis.info(0).info);
+      FieldInfos fis2 = _TestUtil.getFieldInfos(sis.info(1).info);
 
       assertEquals("f1", fis1.fieldInfo(0).name);
       assertEquals("f2", fis1.fieldInfo(1).name);
@@ -84,7 +87,7 @@
       sis.read(dir);
       assertEquals(1, sis.size());
 
-      FieldInfos fis3 = sis.info(0).getFieldInfos();
+      FieldInfos fis3 = _TestUtil.getFieldInfos(sis.info(0).info);
 
       assertEquals("f1", fis3.fieldInfo(0).name);
       assertEquals("f2", fis3.fieldInfo(1).name);
@@ -129,8 +132,8 @@
     sis.read(dir1);
     assertEquals(2, sis.size());
 
-    FieldInfos fis1 = sis.info(0).getFieldInfos();
-    FieldInfos fis2 = sis.info(1).getFieldInfos();
+    FieldInfos fis1 = _TestUtil.getFieldInfos(sis.info(0).info);
+    FieldInfos fis2 = _TestUtil.getFieldInfos(sis.info(1).info);
 
     assertEquals("f1", fis1.fieldInfo(0).name);
     assertEquals("f2", fis1.fieldInfo(1).name);
@@ -140,22 +143,6 @@
     assertEquals("f3", fis2.fieldInfo(2).name);
     assertEquals("f4", fis2.fieldInfo(3).name);
 
-    writer = new IndexWriter(dir1, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random())));
-    writer.forceMerge(1);
-    writer.close();
-
-    sis = new SegmentInfos();
-    sis.read(dir1);
-    assertEquals(1, sis.size());
-
-    FieldInfos fis3 = sis.info(0).getFieldInfos();
-
-    // after merging the ordering should be identical to the first segment
-    assertEquals("f1", fis3.fieldInfo(0).name);
-    assertEquals("f2", fis3.fieldInfo(1).name);
-    assertEquals("f3", fis3.fieldInfo(2).name);
-    assertEquals("f4", fis3.fieldInfo(3).name);
-
     dir1.close();
     dir2.close();
   }
@@ -176,7 +163,7 @@
         SegmentInfos sis = new SegmentInfos();
         sis.read(dir);
         assertEquals(1, sis.size());
-        FieldInfos fis1 = sis.info(0).getFieldInfos();
+        FieldInfos fis1 = _TestUtil.getFieldInfos(sis.info(0).info);
         assertEquals("f1", fis1.fieldInfo(0).name);
         assertEquals("f2", fis1.fieldInfo(1).name);
       }
@@ -195,8 +182,8 @@
         SegmentInfos sis = new SegmentInfos();
         sis.read(dir);
         assertEquals(2, sis.size());
-        FieldInfos fis1 = sis.info(0).getFieldInfos();
-        FieldInfos fis2 = sis.info(1).getFieldInfos();
+        FieldInfos fis1 = _TestUtil.getFieldInfos(sis.info(0).info);
+        FieldInfos fis2 = _TestUtil.getFieldInfos(sis.info(1).info);
         assertEquals("f1", fis1.fieldInfo(0).name);
         assertEquals("f2", fis1.fieldInfo(1).name);
         assertEquals("f1", fis2.fieldInfo(0).name);
@@ -218,9 +205,9 @@
         SegmentInfos sis = new SegmentInfos();
         sis.read(dir);
         assertEquals(3, sis.size());
-        FieldInfos fis1 = sis.info(0).getFieldInfos();
-        FieldInfos fis2 = sis.info(1).getFieldInfos();
-        FieldInfos fis3 = sis.info(2).getFieldInfos();
+        FieldInfos fis1 = _TestUtil.getFieldInfos(sis.info(0).info);
+        FieldInfos fis2 = _TestUtil.getFieldInfos(sis.info(1).info);
+        FieldInfos fis3 = _TestUtil.getFieldInfos(sis.info(2).info);
         assertEquals("f1", fis1.fieldInfo(0).name);
         assertEquals("f2", fis1.fieldInfo(1).name);
         assertEquals("f1", fis2.fieldInfo(0).name);
@@ -252,7 +239,7 @@
       SegmentInfos sis = new SegmentInfos();
       sis.read(dir);
       assertEquals(1, sis.size());
-      FieldInfos fis1 = sis.info(0).getFieldInfos();
+      FieldInfos fis1 = _TestUtil.getFieldInfos(sis.info(0).info);
       assertEquals("f1", fis1.fieldInfo(0).name);
       assertEquals("f2", fis1.fieldInfo(1).name);
       assertEquals("f3", fis1.fieldInfo(2).name);
@@ -289,13 +276,13 @@
 
     SegmentInfos sis = new SegmentInfos();
     sis.read(dir);
-    for (SegmentInfo si : sis) {
-      FieldInfos fis = si.getFieldInfos();
+    for (SegmentInfoPerCommit si : sis) {
+      FieldInfos fis = _TestUtil.getFieldInfos(si.info);
 
       for (FieldInfo fi : fis) {
         Field expected = getField(Integer.parseInt(fi.name));
-        assertEquals(expected.fieldType().indexed(), fi.isIndexed);
-        assertEquals(expected.fieldType().storeTermVectors(), fi.storeTermVector);
+        assertEquals(expected.fieldType().indexed(), fi.isIndexed());
+        assertEquals(expected.fieldType().storeTermVectors(), fi.hasVectors());
       }
     }
 


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/test/org/apache/lucene/index/TestDirectoryReader.java lucene4055/lucene/core/src/test/org/apache/lucene/index/TestDirectoryReader.java
--- lucene-trunk/lucene/core/src/test/org/apache/lucene/index/TestDirectoryReader.java	2012-05-24 16:55:47.776233425 -0400
+++ lucene4055/lucene/core/src/test/org/apache/lucene/index/TestDirectoryReader.java	2012-05-21 13:58:02.803533874 -0400
@@ -258,12 +258,12 @@
       for(FieldInfo fieldInfo : fieldInfos) {
         final String name = fieldInfo.name;
         allFieldNames.add(name);
-        if (fieldInfo.isIndexed) {
+        if (fieldInfo.isIndexed()) {
           indexedFieldNames.add(name);
         } else {
           notIndexedFieldNames.add(name);
         }
-        if (fieldInfo.storeTermVector) {
+        if (fieldInfo.hasVectors()) {
           tvFieldNames.add(name);
         }
       }


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/test/org/apache/lucene/index/TestDoc.java lucene4055/lucene/core/src/test/org/apache/lucene/index/TestDoc.java
--- lucene-trunk/lucene/core/src/test/org/apache/lucene/index/TestDoc.java	2012-05-24 16:55:47.760233424 -0400
+++ lucene4055/lucene/core/src/test/org/apache/lucene/index/TestDoc.java	2012-05-24 10:06:39.299805926 -0400
@@ -22,10 +22,10 @@
 import java.io.IOException;
 import java.io.PrintWriter;
 import java.io.StringWriter;
-
-import java.util.LinkedList;
 import java.util.Collection;
-
+import java.util.HashSet;
+import java.util.LinkedList;
+import java.util.Set;
 
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.codecs.Codec;
@@ -35,6 +35,8 @@
 import org.apache.lucene.search.DocIdSetIterator;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.TrackingDirectoryWrapper;
+import org.apache.lucene.util.Constants;
 import org.apache.lucene.util.InfoStream;
 import org.apache.lucene.util.LuceneTestCase;
 import org.apache.lucene.util._TestUtil;
@@ -116,20 +118,20 @@
               setMergePolicy(newLogMergePolicy(10))
       );
 
-      SegmentInfo si1 = indexDoc(writer, "test.txt");
+      SegmentInfoPerCommit si1 = indexDoc(writer, "test.txt");
       printSegment(out, si1);
 
-      SegmentInfo si2 = indexDoc(writer, "test2.txt");
+      SegmentInfoPerCommit si2 = indexDoc(writer, "test2.txt");
       printSegment(out, si2);
       writer.close();
 
-      SegmentInfo siMerge = merge(directory, si1, si2, "merge", false);
+      SegmentInfoPerCommit siMerge = merge(directory, si1, si2, "merge", false);
       printSegment(out, siMerge);
 
-      SegmentInfo siMerge2 = merge(directory, si1, si2, "merge2", false);
+      SegmentInfoPerCommit siMerge2 = merge(directory, si1, si2, "merge2", false);
       printSegment(out, siMerge2);
 
-      SegmentInfo siMerge3 = merge(directory, siMerge, siMerge2, "merge3", false);
+      SegmentInfoPerCommit siMerge3 = merge(directory, siMerge, siMerge2, "merge3", false);
       printSegment(out, siMerge3);
       
       directory.close();
@@ -175,7 +177,7 @@
       assertEquals(multiFileOutput, singleFileOutput);
    }
 
-   private SegmentInfo indexDoc(IndexWriter writer, String fileName)
+   private SegmentInfoPerCommit indexDoc(IndexWriter writer, String fileName)
    throws Exception
    {
       File file = new File(workDir, fileName);
@@ -187,36 +189,42 @@
    }
 
 
-   private SegmentInfo merge(Directory dir, SegmentInfo si1, SegmentInfo si2, String merged, boolean useCompoundFile)
+   private SegmentInfoPerCommit merge(Directory dir, SegmentInfoPerCommit si1, SegmentInfoPerCommit si2, String merged, boolean useCompoundFile)
    throws Exception {
       IOContext context = newIOContext(random());
       SegmentReader r1 = new SegmentReader(si1, DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, context);
       SegmentReader r2 = new SegmentReader(si2, DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, context);
 
       final Codec codec = Codec.getDefault();
-      SegmentMerger merger = new SegmentMerger(InfoStream.getDefault(), si1.dir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL, merged, MergeState.CheckAbort.NONE, null, new FieldInfos(new FieldInfos.FieldNumberBiMap()), codec, context);
+      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(si1.info.dir);
+      final SegmentInfo si = new SegmentInfo(si1.info.dir, Constants.LUCENE_MAIN_VERSION, merged, -1, false, codec, null, null);
+
+      SegmentMerger merger = new SegmentMerger(si, InfoStream.getDefault(), trackingDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL,
+                                               MergeState.CheckAbort.NONE, null, new FieldInfos.FieldNumbers(), context);
 
       merger.add(r1);
       merger.add(r2);
       MergeState mergeState = merger.merge();
       r1.close();
       r2.close();
-      final FieldInfos fieldInfos =  mergeState.fieldInfos;
-      final SegmentInfo info = new SegmentInfo(merged, si1.docCount + si2.docCount, si1.dir,
-                                               false, codec, fieldInfos);
+      final SegmentInfo info = new SegmentInfo(si1.info.dir, Constants.LUCENE_MAIN_VERSION, merged,
+                                               si1.info.getDocCount() + si2.info.getDocCount(),
+                                               false, codec, null, null);
+      info.setFiles(new HashSet<String>(trackingDir.getCreatedFiles()));
       
       if (useCompoundFile) {
-        Collection<String> filesToDelete = IndexWriter.createCompoundFile(dir, merged + ".cfs", MergeState.CheckAbort.NONE, info, newIOContext(random()));
+        Collection<String> filesToDelete = IndexWriter.createCompoundFile(InfoStream.getDefault(), dir, MergeState.CheckAbort.NONE, info, newIOContext(random()));
         info.setUseCompoundFile(true);
-        for (final String fileToDelete : filesToDelete) 
-          si1.dir.deleteFile(fileToDelete);
+        for (final String fileToDelete : filesToDelete) {
+          si1.info.dir.deleteFile(fileToDelete);
+        }
       }
 
-      return info;
+      return new SegmentInfoPerCommit(info, 0, -1L);
    }
 
 
-   private void printSegment(PrintWriter out, SegmentInfo si)
+   private void printSegment(PrintWriter out, SegmentInfoPerCommit si)
    throws Exception {
       SegmentReader reader = new SegmentReader(si, DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random()));
 


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/test/org/apache/lucene/index/TestDocumentWriter.java lucene4055/lucene/core/src/test/org/apache/lucene/index/TestDocumentWriter.java
--- lucene-trunk/lucene/core/src/test/org/apache/lucene/index/TestDocumentWriter.java	2012-05-24 16:55:47.768233423 -0400
+++ lucene4055/lucene/core/src/test/org/apache/lucene/index/TestDocumentWriter.java	2012-05-22 12:04:43.448920076 -0400
@@ -62,7 +62,7 @@
     IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));
     writer.addDocument(testDoc);
     writer.commit();
-    SegmentInfo info = writer.newestSegment();
+    SegmentInfoPerCommit info = writer.newestSegment();
     writer.close();
     //After adding the document, we should be able to read it back in
     SegmentReader reader = new SegmentReader(info, DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random()));
@@ -96,8 +96,8 @@
     // test that the norms are not present in the segment if
     // omitNorms is true
     for (FieldInfo fi : reader.getFieldInfos()) {
-      if (fi.isIndexed) {
-        assertTrue(fi.omitNorms == (reader.normValues(fi.name) == null));
+      if (fi.isIndexed()) {
+        assertTrue(fi.omitsNorms() == (reader.normValues(fi.name) == null));
       }
     }
     reader.close();
@@ -124,7 +124,7 @@
 
     writer.addDocument(doc);
     writer.commit();
-    SegmentInfo info = writer.newestSegment();
+    SegmentInfoPerCommit info = writer.newestSegment();
     writer.close();
     SegmentReader reader = new SegmentReader(info, DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random()));
 
@@ -196,7 +196,7 @@
 
     writer.addDocument(doc);
     writer.commit();
-    SegmentInfo info = writer.newestSegment();
+    SegmentInfoPerCommit info = writer.newestSegment();
     writer.close();
     SegmentReader reader = new SegmentReader(info, DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random()));
 
@@ -240,7 +240,7 @@
     
     writer.addDocument(doc);
     writer.commit();
-    SegmentInfo info = writer.newestSegment();
+    SegmentInfoPerCommit info = writer.newestSegment();
     writer.close();
     SegmentReader reader = new SegmentReader(info, DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random()));
 
@@ -331,10 +331,10 @@
     FieldInfos fi = reader.getFieldInfos();
     // f1
     assertFalse("f1 should have no norms", fi.fieldInfo("f1").hasNorms());
-    assertEquals("omitTermFreqAndPositions field bit should not be set for f1", IndexOptions.DOCS_AND_FREQS_AND_POSITIONS, fi.fieldInfo("f1").indexOptions);
+    assertEquals("omitTermFreqAndPositions field bit should not be set for f1", IndexOptions.DOCS_AND_FREQS_AND_POSITIONS, fi.fieldInfo("f1").getIndexOptions());
     // f2
     assertTrue("f2 should have norms", fi.fieldInfo("f2").hasNorms());
-    assertEquals("omitTermFreqAndPositions field bit should be set for f2", IndexOptions.DOCS_ONLY, fi.fieldInfo("f2").indexOptions);
+    assertEquals("omitTermFreqAndPositions field bit should be set for f2", IndexOptions.DOCS_ONLY, fi.fieldInfo("f2").getIndexOptions());
     reader.close();
   }
 }


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/test/org/apache/lucene/index/TestFieldInfos.java lucene4055/lucene/core/src/test/org/apache/lucene/index/TestFieldInfos.java
--- lucene-trunk/lucene/core/src/test/org/apache/lucene/index/TestFieldInfos.java	2012-05-24 16:55:47.796233425 -0400
+++ lucene4055/lucene/core/src/test/org/apache/lucene/index/TestFieldInfos.java	2012-05-21 13:58:02.819533874 -0400
@@ -46,8 +46,11 @@
   public FieldInfos createAndWriteFieldInfos(Directory dir, String filename) throws IOException{
   //Positive test of FieldInfos
     assertTrue(testDoc != null);
-    FieldInfos fieldInfos = new FieldInfos(new FieldInfos.FieldNumberBiMap());
-    _TestUtil.add(testDoc, fieldInfos);
+    FieldInfos.Builder builder = new FieldInfos.Builder();
+    for (IndexableField field : testDoc) {
+      builder.addOrUpdate(field.name(), field.fieldType());
+    }
+    FieldInfos fieldInfos = builder.finish();
     //Since the complement is stored as well in the fields map
     assertTrue(fieldInfos.size() == DocHelper.all.size()); //this is all b/c we are using the no-arg constructor
     
@@ -76,22 +79,22 @@
     assertTrue(fieldInfos.size() == readIn.size());
     FieldInfo info = readIn.fieldInfo("textField1");
     assertTrue(info != null);
-    assertTrue(info.storeTermVector == false);
-    assertTrue(info.omitNorms == false);
+    assertTrue(info.hasVectors() == false);
+    assertTrue(info.omitsNorms() == false);
 
     info = readIn.fieldInfo("textField2");
     assertTrue(info != null);
-    assertTrue(info.omitNorms == false);
+    assertTrue(info.omitsNorms() == false);
 
     info = readIn.fieldInfo("textField3");
     assertTrue(info != null);
-    assertTrue(info.storeTermVector == false);
-    assertTrue(info.omitNorms == true);
+    assertTrue(info.hasVectors() == false);
+    assertTrue(info.omitsNorms() == true);
 
     info = readIn.fieldInfo("omitNorms");
     assertTrue(info != null);
-    assertTrue(info.storeTermVector == false);
-    assertTrue(info.omitNorms == true);
+    assertTrue(info.hasVectors() == false);
+    assertTrue(info.omitsNorms() == true);
 
     dir.close();
   }
@@ -102,64 +105,14 @@
     FieldInfos fieldInfos = createAndWriteFieldInfos(dir, name);
     FieldInfos readOnly = readFieldInfos(dir, name);
     assertReadOnly(readOnly, fieldInfos);
-    FieldInfos readOnlyClone = readOnly.clone();
-    assertNotSame(readOnly, readOnlyClone);
-    // clone is also read only - no global field map
-    assertReadOnly(readOnlyClone, fieldInfos);
     dir.close();
   }
   
-  private void assertReadOnly(FieldInfos readOnly, FieldInfos modifiable) {
-    assertTrue(readOnly.isReadOnly());
-    assertFalse(modifiable.isReadOnly());
-    try {
-      readOnly.add(modifiable.fieldInfo(0));
-      fail("instance should be read only");
-    } catch (IllegalStateException e) {
-      // expected
-    }
-    
-    try {
-      readOnly.addOrUpdate("bogus", random().nextBoolean());
-      fail("instance should be read only");
-    } catch (IllegalStateException e) {
-      // expected
-    }
-    try {
-      readOnly.addOrUpdate("bogus", random().nextBoolean(), random().nextBoolean());
-      fail("instance should be read only");
-    } catch (IllegalStateException e) {
-      // expected
-    }
-    try {
-      readOnly.addOrUpdate("bogus", random().nextBoolean(), random().nextBoolean(),
-          random().nextBoolean());
-      fail("instance should be read only");
-    } catch (IllegalStateException e) {
-      // expected
-    }
-    try {
-      readOnly.addOrUpdate("bogus", random().nextBoolean(), random().nextBoolean(),
-          random().nextBoolean(),
-          random().nextBoolean(), random().nextBoolean() ? IndexOptions.DOCS_ONLY : IndexOptions.DOCS_AND_FREQS_AND_POSITIONS, null, null);
-      fail("instance should be read only");
-    } catch (IllegalStateException e) {
-      // expected
-    }
-    try {
-      readOnly.addOrUpdate(Arrays.asList("a", "b", "c"), random().nextBoolean());
-      fail("instance should be read only");
-    } catch (IllegalStateException e) {
-      // expected
-    }
-    
+  private void assertReadOnly(FieldInfos readOnly, FieldInfos modifiable) {    
     assertEquals(modifiable.size(), readOnly.size());
     // assert we can iterate
     for (FieldInfo fi : readOnly) {
-      assertEquals(fi.name, modifiable.fieldName(fi.number));
+      assertEquals(fi.name, modifiable.fieldInfo(fi.number).name);
     }
-    
   }
-  
-  
 }


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/test/org/apache/lucene/index/TestFieldsReader.java lucene4055/lucene/core/src/test/org/apache/lucene/index/TestFieldsReader.java
--- lucene-trunk/lucene/core/src/test/org/apache/lucene/index/TestFieldsReader.java	2012-05-24 16:55:47.796233425 -0400
+++ lucene4055/lucene/core/src/test/org/apache/lucene/index/TestFieldsReader.java	2012-05-21 13:58:02.823533875 -0400
@@ -49,13 +49,15 @@
 public class TestFieldsReader extends LuceneTestCase {
   private static Directory dir;
   private static Document testDoc = new Document();
-  private static FieldInfos fieldInfos = null;
+  private static FieldInfos.Builder fieldInfos = null;
 
   @BeforeClass
   public static void beforeClass() throws Exception {
-    fieldInfos = new FieldInfos(new FieldInfos.FieldNumberBiMap());
+    fieldInfos = new FieldInfos.Builder();
     DocHelper.setupDoc(testDoc);
-    _TestUtil.add(testDoc, fieldInfos);
+    for (IndexableField field : testDoc) {
+      fieldInfos.addOrUpdate(field.name(), field.fieldType());
+    }
     dir = newDirectory();
     IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(newLogMergePolicy());
     ((LogMergePolicy) conf.getMergePolicy()).setUseCompoundFile(false);


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions.java lucene4055/lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions.java
--- lucene-trunk/lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions.java	2012-05-24 16:55:47.764233424 -0400
+++ lucene4055/lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions.java	2012-05-21 13:58:02.795533874 -0400
@@ -800,6 +800,10 @@
         for (int i = 0; i < trace.length; i++) {
           if (doFail && MockDirectoryWrapper.class.getName().equals(trace[i].getClassName()) && "sync".equals(trace[i].getMethodName())) {
             didFail = true;
+            if (VERBOSE) {
+              System.out.println("TEST: now throw exc:");
+              new Throwable().printStackTrace(System.out);
+            }
             throw new IOException("now failing on purpose during sync");
           }
         }
@@ -1120,49 +1124,49 @@
   // files and make sure we get an IOException trying to
   // open the index:
   public void testSimulatedCorruptIndex2() throws IOException {
-      MockDirectoryWrapper dir = newDirectory();
-      dir.setCheckIndexOnClose(false); // we are corrupting it!
-      IndexWriter writer = null;
+    MockDirectoryWrapper dir = newDirectory();
+    dir.setCheckIndexOnClose(false); // we are corrupting it!
+    IndexWriter writer = null;
 
-      writer  = new IndexWriter(
-          dir,
-          newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).
-              setMergePolicy(newLogMergePolicy(true))
-      );
-      ((LogMergePolicy) writer.getConfig().getMergePolicy()).setNoCFSRatio(1.0);
+    writer  = new IndexWriter(
+                              dir,
+                              newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).
+                              setMergePolicy(newLogMergePolicy(true))
+                              );
+    ((LogMergePolicy) writer.getConfig().getMergePolicy()).setNoCFSRatio(1.0);
 
-      // add 100 documents
-      for (int i = 0; i < 100; i++) {
-          addDoc(writer);
-      }
+    // add 100 documents
+    for (int i = 0; i < 100; i++) {
+      addDoc(writer);
+    }
 
-      // close
-      writer.close();
+    // close
+    writer.close();
 
-      long gen = SegmentInfos.getLastCommitGeneration(dir);
-      assertTrue("segment generation should be > 0 but got " + gen, gen > 0);
+    long gen = SegmentInfos.getLastCommitGeneration(dir);
+    assertTrue("segment generation should be > 0 but got " + gen, gen > 0);
 
-      String[] files = dir.listAll();
-      boolean corrupted = false;
-      for(int i=0;i<files.length;i++) {
-        if (files[i].endsWith(".cfs")) {
-          dir.deleteFile(files[i]);
-          corrupted = true;
-          break;
-        }
+    String[] files = dir.listAll();
+    boolean corrupted = false;
+    for(int i=0;i<files.length;i++) {
+      if (files[i].endsWith(".cfs")) {
+        dir.deleteFile(files[i]);
+        corrupted = true;
+        break;
       }
-      assertTrue("failed to find cfs file to remove", corrupted);
+    }
+    assertTrue("failed to find cfs file to remove", corrupted);
 
-      IndexReader reader = null;
-      try {
-        reader = IndexReader.open(dir);
-        fail("reader did not hit IOException on opening a corrupt index");
-      } catch (Exception e) {
-      }
-      if (reader != null) {
-        reader.close();
-      }
-      dir.close();
+    IndexReader reader = null;
+    try {
+      reader = IndexReader.open(dir);
+      fail("reader did not hit IOException on opening a corrupt index");
+    } catch (Exception e) {
+    }
+    if (reader != null) {
+      reader.close();
+    }
+    dir.close();
   }
 
   // Simulate a writer that crashed while writing segments
@@ -1284,14 +1288,13 @@
         w.close();
         IndexReader reader = IndexReader.open(dir);
         assertTrue(reader.numDocs() > 0);
-        reader.close();
         SegmentInfos sis = new SegmentInfos();
         sis.read(dir);
-        for (SegmentInfo segmentInfo : sis) {
-          assertFalse(segmentInfo.getHasVectors());
+        for(AtomicReaderContext context : reader.getTopReaderContext().leaves()) {
+          assertFalse(context.reader().getFieldInfos().hasVectors());
         }
+        reader.close();
         dir.close();
-        
       }
     }
   }


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/test/org/apache/lucene/index/TestIndexWriter.java lucene4055/lucene/core/src/test/org/apache/lucene/index/TestIndexWriter.java
--- lucene-trunk/lucene/core/src/test/org/apache/lucene/index/TestIndexWriter.java	2012-05-24 16:55:47.788233425 -0400
+++ lucene4055/lucene/core/src/test/org/apache/lucene/index/TestIndexWriter.java	2012-05-23 16:36:44.898708913 -0400
@@ -985,26 +985,6 @@
     dir.close();
   }
 
-
-  // LUCENE-1468 -- make sure opening an IndexWriter with
-  // create=true does not remove non-index files
-
-  public void testOtherFiles() throws Throwable {
-    Directory dir = newDirectory();
-    try {
-      // Create my own random file:
-      IndexOutput out = dir.createOutput("myrandomfile", newIOContext(random()));
-      out.writeByte((byte) 42);
-      out.close();
-
-      new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random()))).close();
-
-      assertTrue(dir.fileExists("myrandomfile"));
-    } finally {
-      dir.close();
-    }
-  }
-
   public void testDeadlock() throws Exception {
     Directory dir = newDirectory();
     IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMaxBufferedDocs(2));
@@ -1546,11 +1526,8 @@
     assertNoUnreferencedFiles(dir, "no tv files");
     DirectoryReader r0 = IndexReader.open(dir);
     for (IndexReader r : r0.getSequentialSubReaders()) {
-      SegmentInfo s = ((SegmentReader) r).getSegmentInfo();
-      assertFalse(s.getHasVectors());
-      Set<String> files = new HashSet<String>();
-      s.getCodec().termVectorsFormat().files(s, files);
-      assertTrue(files.isEmpty());
+      SegmentInfoPerCommit s = ((SegmentReader) r).getSegmentInfo();
+      assertFalse(((SegmentReader) r).getFieldInfos().hasVectors());
     }
     
     r0.close();


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/test/org/apache/lucene/index/TestIndexWriterMerging.java lucene4055/lucene/core/src/test/org/apache/lucene/index/TestIndexWriterMerging.java
--- lucene-trunk/lucene/core/src/test/org/apache/lucene/index/TestIndexWriterMerging.java	2012-05-24 16:55:47.796233425 -0400
+++ lucene4055/lucene/core/src/test/org/apache/lucene/index/TestIndexWriterMerging.java	2012-05-23 14:42:53.858589954 -0400
@@ -321,7 +321,7 @@
           break;
         }
         for(int i=0;i<merge.segments.size();i++) {
-          assert merge.segments.get(i).docCount < 20;
+          assert merge.segments.get(i).info.getDocCount() < 20;
         }
         writer.merge(merge);
       }


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull.java lucene4055/lucene/core/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull.java
--- lucene-trunk/lucene/core/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull.java	2012-05-24 16:55:47.764233424 -0400
+++ lucene4055/lucene/core/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull.java	2012-05-21 13:58:02.791533874 -0400
@@ -220,8 +220,9 @@
 
     for(int iter=0;iter<3;iter++) {
       
-      if (VERBOSE)
+      if (VERBOSE) {
         System.out.println("TEST: iter=" + iter);
+      }
       
       // Start with 100 bytes more than we are currently using:
       long diskFree = diskUsage+_TestUtil.nextInt(random(), 50, 200);


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/test/org/apache/lucene/index/TestNorms.java lucene4055/lucene/core/src/test/org/apache/lucene/index/TestNorms.java
--- lucene-trunk/lucene/core/src/test/org/apache/lucene/index/TestNorms.java	2012-05-24 16:55:47.784233425 -0400
+++ lucene4055/lucene/core/src/test/org/apache/lucene/index/TestNorms.java	2012-05-21 13:58:02.807533875 -0400
@@ -129,8 +129,8 @@
     AtomicReader reader = SlowCompositeReaderWrapper.wrap(IndexReader.open(otherDir));
     FieldInfos fieldInfos = reader.getFieldInfos();
     FieldInfo fieldInfo = fieldInfos.fieldInfo(byteTestField);
-    assertFalse(fieldInfo.omitNorms);
-    assertTrue(fieldInfo.isIndexed);
+    assertFalse(fieldInfo.omitsNorms());
+    assertTrue(fieldInfo.isIndexed());
     if (secondWriteNorm) {
       assertTrue(fieldInfo.hasNorms());
     } else {
@@ -146,13 +146,13 @@
       DocValues normValues = mergedReader.normValues(byteTestField);
       assertNull(normValues);
       FieldInfo fi = mergedReader.getFieldInfos().fieldInfo(byteTestField);
-      assertFalse(fi.omitNorms);
-      assertTrue(fi.isIndexed);
+      assertFalse(fi.omitsNorms());
+      assertTrue(fi.isIndexed());
       assertFalse(fi.hasNorms());
     } else {
       FieldInfo fi = mergedReader.getFieldInfos().fieldInfo(byteTestField);
-      assertFalse(fi.omitNorms);
-      assertTrue(fi.isIndexed);
+      assertFalse(fi.omitsNorms());
+      assertTrue(fi.isIndexed());
       assertTrue(fi.hasNorms());
       
       DocValues normValues = mergedReader.normValues(byteTestField);


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/test/org/apache/lucene/index/TestOmitNorms.java lucene4055/lucene/core/src/test/org/apache/lucene/index/TestOmitNorms.java
--- lucene-trunk/lucene/core/src/test/org/apache/lucene/index/TestOmitNorms.java	2012-05-24 16:55:47.776233425 -0400
+++ lucene4055/lucene/core/src/test/org/apache/lucene/index/TestOmitNorms.java	2012-05-21 13:58:02.803533874 -0400
@@ -68,8 +68,8 @@
 
     SegmentReader reader = getOnlySegmentReader(IndexReader.open(ram));
     FieldInfos fi = reader.getFieldInfos();
-    assertTrue("OmitNorms field bit should be set.", fi.fieldInfo("f1").omitNorms);
-    assertTrue("OmitNorms field bit should be set.", fi.fieldInfo("f2").omitNorms);
+    assertTrue("OmitNorms field bit should be set.", fi.fieldInfo("f1").omitsNorms());
+    assertTrue("OmitNorms field bit should be set.", fi.fieldInfo("f2").omitsNorms());
         
     reader.close();
     ram.close();
@@ -122,8 +122,8 @@
 
     SegmentReader reader = getOnlySegmentReader(IndexReader.open(ram));
     FieldInfos fi = reader.getFieldInfos();
-    assertTrue("OmitNorms field bit should be set.", fi.fieldInfo("f1").omitNorms);
-    assertTrue("OmitNorms field bit should be set.", fi.fieldInfo("f2").omitNorms);
+    assertTrue("OmitNorms field bit should be set.", fi.fieldInfo("f1").omitsNorms());
+    assertTrue("OmitNorms field bit should be set.", fi.fieldInfo("f2").omitsNorms());
         
     reader.close();
     ram.close();
@@ -170,8 +170,8 @@
 
     SegmentReader reader = getOnlySegmentReader(IndexReader.open(ram));
     FieldInfos fi = reader.getFieldInfos();
-    assertTrue("OmitNorms field bit should not be set.", !fi.fieldInfo("f1").omitNorms);
-    assertTrue("OmitNorms field bit should be set.", fi.fieldInfo("f2").omitNorms);
+    assertTrue("OmitNorms field bit should not be set.", !fi.fieldInfo("f1").omitsNorms());
+    assertTrue("OmitNorms field bit should be set.", fi.fieldInfo("f2").omitsNorms());
         
     reader.close();
     ram.close();


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/test/org/apache/lucene/index/TestOmitPositions.java lucene4055/lucene/core/src/test/org/apache/lucene/index/TestOmitPositions.java
--- lucene-trunk/lucene/core/src/test/org/apache/lucene/index/TestOmitPositions.java	2012-05-24 16:55:47.776233425 -0400
+++ lucene4055/lucene/core/src/test/org/apache/lucene/index/TestOmitPositions.java	2012-05-21 13:58:02.799533874 -0400
@@ -156,23 +156,23 @@
     SegmentReader reader = getOnlySegmentReader(IndexReader.open(ram));
     FieldInfos fi = reader.getFieldInfos();
     // docs + docs = docs
-    assertEquals(IndexOptions.DOCS_ONLY, fi.fieldInfo("f1").indexOptions);
+    assertEquals(IndexOptions.DOCS_ONLY, fi.fieldInfo("f1").getIndexOptions());
     // docs + docs/freqs = docs
-    assertEquals(IndexOptions.DOCS_ONLY, fi.fieldInfo("f2").indexOptions);
+    assertEquals(IndexOptions.DOCS_ONLY, fi.fieldInfo("f2").getIndexOptions());
     // docs + docs/freqs/pos = docs
-    assertEquals(IndexOptions.DOCS_ONLY, fi.fieldInfo("f3").indexOptions);
+    assertEquals(IndexOptions.DOCS_ONLY, fi.fieldInfo("f3").getIndexOptions());
     // docs/freqs + docs = docs
-    assertEquals(IndexOptions.DOCS_ONLY, fi.fieldInfo("f4").indexOptions);
+    assertEquals(IndexOptions.DOCS_ONLY, fi.fieldInfo("f4").getIndexOptions());
     // docs/freqs + docs/freqs = docs/freqs
-    assertEquals(IndexOptions.DOCS_AND_FREQS, fi.fieldInfo("f5").indexOptions);
+    assertEquals(IndexOptions.DOCS_AND_FREQS, fi.fieldInfo("f5").getIndexOptions());
     // docs/freqs + docs/freqs/pos = docs/freqs
-    assertEquals(IndexOptions.DOCS_AND_FREQS, fi.fieldInfo("f6").indexOptions);
+    assertEquals(IndexOptions.DOCS_AND_FREQS, fi.fieldInfo("f6").getIndexOptions());
     // docs/freqs/pos + docs = docs
-    assertEquals(IndexOptions.DOCS_ONLY, fi.fieldInfo("f7").indexOptions);
+    assertEquals(IndexOptions.DOCS_ONLY, fi.fieldInfo("f7").getIndexOptions());
     // docs/freqs/pos + docs/freqs = docs/freqs
-    assertEquals(IndexOptions.DOCS_AND_FREQS, fi.fieldInfo("f8").indexOptions);
+    assertEquals(IndexOptions.DOCS_AND_FREQS, fi.fieldInfo("f8").getIndexOptions());
     // docs/freqs/pos + docs/freqs/pos = docs/freqs/pos
-    assertEquals(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS, fi.fieldInfo("f9").indexOptions);
+    assertEquals(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS, fi.fieldInfo("f9").getIndexOptions());
     
     reader.close();
     ram.close();


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/test/org/apache/lucene/index/TestOmitTf.java lucene4055/lucene/core/src/test/org/apache/lucene/index/TestOmitTf.java
--- lucene-trunk/lucene/core/src/test/org/apache/lucene/index/TestOmitTf.java	2012-05-24 16:55:47.772233424 -0400
+++ lucene4055/lucene/core/src/test/org/apache/lucene/index/TestOmitTf.java	2012-05-21 13:58:02.799533874 -0400
@@ -95,8 +95,8 @@
 
     SegmentReader reader = getOnlySegmentReader(IndexReader.open(ram));
     FieldInfos fi = reader.getFieldInfos();
-    assertEquals("OmitTermFreqAndPositions field bit should be set.", IndexOptions.DOCS_ONLY, fi.fieldInfo("f1").indexOptions);
-    assertEquals("OmitTermFreqAndPositions field bit should be set.", IndexOptions.DOCS_ONLY, fi.fieldInfo("f2").indexOptions);
+    assertEquals("OmitTermFreqAndPositions field bit should be set.", IndexOptions.DOCS_ONLY, fi.fieldInfo("f1").getIndexOptions());
+    assertEquals("OmitTermFreqAndPositions field bit should be set.", IndexOptions.DOCS_ONLY, fi.fieldInfo("f2").getIndexOptions());
         
     reader.close();
     ram.close();
@@ -147,8 +147,8 @@
 
     SegmentReader reader = getOnlySegmentReader(IndexReader.open(ram));
     FieldInfos fi = reader.getFieldInfos();
-    assertEquals("OmitTermFreqAndPositions field bit should be set.", IndexOptions.DOCS_ONLY, fi.fieldInfo("f1").indexOptions);
-    assertEquals("OmitTermFreqAndPositions field bit should be set.", IndexOptions.DOCS_ONLY, fi.fieldInfo("f2").indexOptions);
+    assertEquals("OmitTermFreqAndPositions field bit should be set.", IndexOptions.DOCS_ONLY, fi.fieldInfo("f1").getIndexOptions());
+    assertEquals("OmitTermFreqAndPositions field bit should be set.", IndexOptions.DOCS_ONLY, fi.fieldInfo("f2").getIndexOptions());
         
     reader.close();
     ram.close();
@@ -190,8 +190,8 @@
 
     SegmentReader reader = getOnlySegmentReader(IndexReader.open(ram));
     FieldInfos fi = reader.getFieldInfos();
-    assertEquals("OmitTermFreqAndPositions field bit should not be set.", IndexOptions.DOCS_AND_FREQS_AND_POSITIONS, fi.fieldInfo("f1").indexOptions);
-    assertEquals("OmitTermFreqAndPositions field bit should be set.", IndexOptions.DOCS_ONLY, fi.fieldInfo("f2").indexOptions);
+    assertEquals("OmitTermFreqAndPositions field bit should not be set.", IndexOptions.DOCS_AND_FREQS_AND_POSITIONS, fi.fieldInfo("f1").getIndexOptions());
+    assertEquals("OmitTermFreqAndPositions field bit should be set.", IndexOptions.DOCS_ONLY, fi.fieldInfo("f2").getIndexOptions());
         
     reader.close();
     ram.close();


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/test/org/apache/lucene/index/TestParallelReaderEmptyIndex.java lucene4055/lucene/core/src/test/org/apache/lucene/index/TestParallelReaderEmptyIndex.java
--- lucene-trunk/lucene/core/src/test/org/apache/lucene/index/TestParallelReaderEmptyIndex.java	2012-05-24 16:55:47.800233425 -0400
+++ lucene4055/lucene/core/src/test/org/apache/lucene/index/TestParallelReaderEmptyIndex.java	2012-05-21 13:58:02.827533875 -0400
@@ -19,15 +19,15 @@
 
 import java.io.IOException;
 
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.util.LuceneTestCase;
-
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.FieldType;
 import org.apache.lucene.document.TextField;
 import org.apache.lucene.index.IndexWriterConfig.OpenMode;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.util.LuceneTestCase;
+import org.apache.lucene.util._TestUtil;
 
 /**
  * Some tests for {@link ParallelAtomicReader}s with empty indexes
@@ -89,6 +89,9 @@
   public void testEmptyIndexWithVectors() throws IOException {
     Directory rd1 = newDirectory();
     {
+      if (VERBOSE) {
+        System.out.println("\nTEST: make 1st writer");
+      }
       IndexWriter iw = new IndexWriter(rd1, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random())));
       Document doc = new Document();
       Field idField = newField("id", "", TextField.TYPE_UNSTORED);
@@ -105,6 +108,9 @@
 
       IndexWriterConfig dontMergeConfig = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()))
         .setMergePolicy(NoMergePolicy.COMPOUND_FILES);
+      if (VERBOSE) {
+        System.out.println("\nTEST: make 2nd writer");
+      }
       IndexWriter writer = new IndexWriter(rd1, dontMergeConfig);
       
       writer.deleteDocuments(new Term("id", "1"));


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/test/org/apache/lucene/index/TestPayloads.java lucene4055/lucene/core/src/test/org/apache/lucene/index/TestPayloads.java
--- lucene-trunk/lucene/core/src/test/org/apache/lucene/index/TestPayloads.java	2012-05-24 16:55:47.780233425 -0400
+++ lucene4055/lucene/core/src/test/org/apache/lucene/index/TestPayloads.java	2012-05-21 13:58:02.807533875 -0400
@@ -113,9 +113,9 @@
 
       SegmentReader reader = getOnlySegmentReader(IndexReader.open(ram));
         FieldInfos fi = reader.getFieldInfos();
-        assertFalse("Payload field bit should not be set.", fi.fieldInfo("f1").storePayloads);
-        assertTrue("Payload field bit should be set.", fi.fieldInfo("f2").storePayloads);
-        assertFalse("Payload field bit should not be set.", fi.fieldInfo("f3").storePayloads);
+        assertFalse("Payload field bit should not be set.", fi.fieldInfo("f1").hasPayloads());
+        assertTrue("Payload field bit should be set.", fi.fieldInfo("f2").hasPayloads());
+        assertFalse("Payload field bit should not be set.", fi.fieldInfo("f3").hasPayloads());
         reader.close();
         
         // now we add another document which has payloads for field f3 and verify if the SegmentMerger
@@ -140,9 +140,9 @@
 
       reader = getOnlySegmentReader(IndexReader.open(ram));
         fi = reader.getFieldInfos();
-        assertFalse("Payload field bit should not be set.", fi.fieldInfo("f1").storePayloads);
-        assertTrue("Payload field bit should be set.", fi.fieldInfo("f2").storePayloads);
-        assertTrue("Payload field bit should be set.", fi.fieldInfo("f3").storePayloads);
+        assertFalse("Payload field bit should not be set.", fi.fieldInfo("f1").hasPayloads());
+        assertTrue("Payload field bit should be set.", fi.fieldInfo("f2").hasPayloads());
+        assertTrue("Payload field bit should be set.", fi.fieldInfo("f3").hasPayloads());
         reader.close();
         ram.close();
     }


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/test/org/apache/lucene/index/TestPerSegmentDeletes.java lucene4055/lucene/core/src/test/org/apache/lucene/index/TestPerSegmentDeletes.java
--- lucene-trunk/lucene/core/src/test/org/apache/lucene/index/TestPerSegmentDeletes.java	2012-05-24 16:55:47.796233425 -0400
+++ lucene4055/lucene/core/src/test/org/apache/lucene/index/TestPerSegmentDeletes.java	2012-05-22 12:04:43.448920076 -0400
@@ -206,9 +206,9 @@
     //System.out.println("segdels4:" + writer.docWriter.deletesToString());
   }
 
-  boolean segThere(SegmentInfo info, SegmentInfos infos) {
-    for (SegmentInfo si : infos) {
-      if (si.name.equals(info.name)) return true;
+  boolean segThere(SegmentInfoPerCommit info, SegmentInfos infos) {
+    for (SegmentInfoPerCommit si : infos) {
+      if (si.info.name.equals(info.info.name)) return true;
     }
     return false;
   }
@@ -270,7 +270,7 @@
 
     @Override
     public MergeSpecification findForcedMerges(SegmentInfos segmentInfos,
-        int maxSegmentCount, Map<SegmentInfo,Boolean> segmentsToMerge)
+        int maxSegmentCount, Map<SegmentInfoPerCommit,Boolean> segmentsToMerge)
         throws CorruptIndexException, IOException {
       return null;
     }
@@ -282,7 +282,7 @@
     }
 
     @Override
-    public boolean useCompoundFile(SegmentInfos segments, SegmentInfo newSegment) {
+    public boolean useCompoundFile(SegmentInfos segments, SegmentInfoPerCommit newSegment) {
       return useCompoundFile;
     }
   }


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger.java lucene4055/lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger.java
--- lucene-trunk/lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger.java	2012-05-24 16:55:47.780233425 -0400
+++ lucene4055/lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger.java	2012-05-24 10:06:53.127806162 -0400
@@ -24,6 +24,7 @@
 import org.apache.lucene.search.DocIdSetIterator;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.Constants;
 import org.apache.lucene.util.InfoStream;
 import org.apache.lucene.util.LuceneTestCase;
 import org.apache.lucene.util._TestUtil;
@@ -49,9 +50,9 @@
     merge1Dir = newDirectory();
     merge2Dir = newDirectory();
     DocHelper.setupDoc(doc1);
-    SegmentInfo info1 = DocHelper.writeDoc(random(), merge1Dir, doc1);
+    SegmentInfoPerCommit info1 = DocHelper.writeDoc(random(), merge1Dir, doc1);
     DocHelper.setupDoc(doc2);
-    SegmentInfo info2 = DocHelper.writeDoc(random(), merge2Dir, doc2);
+    SegmentInfoPerCommit info2 = DocHelper.writeDoc(random(), merge2Dir, doc2);
     reader1 = new SegmentReader(info1, DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random()));
     reader2 = new SegmentReader(info2, DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random()));
   }
@@ -76,16 +77,20 @@
 
   public void testMerge() throws IOException {
     final Codec codec = Codec.getDefault();
-    SegmentMerger merger = new SegmentMerger(InfoStream.getDefault(), mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, MergeState.CheckAbort.NONE, null, new FieldInfos(new FieldInfos.FieldNumberBiMap()), codec, newIOContext(random()));
+    final SegmentInfo si = new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, -1, false, codec, null, null);
+
+    SegmentMerger merger = new SegmentMerger(si, InfoStream.getDefault(), mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL,
+                                             MergeState.CheckAbort.NONE, null, new FieldInfos.FieldNumbers(), newIOContext(random()));
     merger.add(reader1);
     merger.add(reader2);
     MergeState mergeState = merger.merge();
-    int docsMerged = mergeState.mergedDocCount;
+    int docsMerged = mergeState.segmentInfo.getDocCount();
     assertTrue(docsMerged == 2);
-    final FieldInfos fieldInfos = mergeState.fieldInfos;
     //Should be able to open a new SegmentReader against the new directory
-    SegmentReader mergedReader = new SegmentReader(new SegmentInfo(mergedSegment, docsMerged, mergedDir, false,
-                                                                                     codec, fieldInfos),
+    SegmentReader mergedReader = new SegmentReader(new SegmentInfoPerCommit(
+                                                         new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, docsMerged,
+                                                                         false, codec, null, null),
+                                                         0, -1L),
                                                    DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random()));
     assertTrue(mergedReader != null);
     assertTrue(mergedReader.numDocs() == 2);
@@ -108,7 +113,7 @@
 
     int tvCount = 0;
     for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {
-      if (fieldInfo.storeTermVector) {
+      if (fieldInfo.hasVectors()) {
         tvCount++;
       }
     }


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/test/org/apache/lucene/index/TestSegmentReader.java lucene4055/lucene/core/src/test/org/apache/lucene/index/TestSegmentReader.java
--- lucene-trunk/lucene/core/src/test/org/apache/lucene/index/TestSegmentReader.java	2012-05-24 16:55:47.800233425 -0400
+++ lucene4055/lucene/core/src/test/org/apache/lucene/index/TestSegmentReader.java	2012-05-22 12:04:43.448920076 -0400
@@ -41,7 +41,7 @@
     super.setUp();
     dir = newDirectory();
     DocHelper.setupDoc(testDoc);
-    SegmentInfo info = DocHelper.writeDoc(random(), dir, testDoc);
+    SegmentInfoPerCommit info = DocHelper.writeDoc(random(), dir, testDoc);
     reader = new SegmentReader(info, DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, IOContext.READ);
   }
   
@@ -84,14 +84,14 @@
     for(FieldInfo fieldInfo : reader.getFieldInfos()) {
       final String name = fieldInfo.name;
       allFieldNames.add(name);
-      if (fieldInfo.isIndexed) {
+      if (fieldInfo.isIndexed()) {
         indexedFieldNames.add(name);
       } else {
         notIndexedFieldNames.add(name);
       }
-      if (fieldInfo.storeTermVector) {
+      if (fieldInfo.hasVectors()) {
         tvFieldNames.add(name);
-      } else if (fieldInfo.isIndexed) {
+      } else if (fieldInfo.isIndexed()) {
         noTVFieldNames.add(name);
       }
     }


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/test/org/apache/lucene/index/TestSegmentTermDocs.java lucene4055/lucene/core/src/test/org/apache/lucene/index/TestSegmentTermDocs.java
--- lucene-trunk/lucene/core/src/test/org/apache/lucene/index/TestSegmentTermDocs.java	2012-05-24 16:55:47.764233424 -0400
+++ lucene4055/lucene/core/src/test/org/apache/lucene/index/TestSegmentTermDocs.java	2012-05-22 12:04:43.448920076 -0400
@@ -31,7 +31,7 @@
 public class TestSegmentTermDocs extends LuceneTestCase {
   private Document testDoc = new Document();
   private Directory dir;
-  private SegmentInfo info;
+  private SegmentInfoPerCommit info;
 
   @Override
   public void setUp() throws Exception {


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/test/org/apache/lucene/index/TestStressAdvance.java lucene4055/lucene/core/src/test/org/apache/lucene/index/TestStressAdvance.java
--- lucene-trunk/lucene/core/src/test/org/apache/lucene/index/TestStressAdvance.java	2012-05-24 16:55:47.784233425 -0400
+++ lucene4055/lucene/core/src/test/org/apache/lucene/index/TestStressAdvance.java	2012-05-21 13:58:02.811533875 -0400
@@ -42,6 +42,9 @@
       final Field idField = newField("id", "", StringField.TYPE_STORED);
       doc.add(idField);
       int num = atLeast(4097);
+      if (VERBOSE) {
+        System.out.println("\nTEST: numDocs=" + num);
+      }
       for(int id=0;id<num;id++) {
         if (random().nextInt(4) == 3) {
           f.setStringValue("a");
@@ -51,6 +54,9 @@
         }
         idField.setStringValue(""+id);
         w.addDocument(doc);
+        if (VERBOSE) {
+          System.out.println("\nTEST: doc upto " + id);
+        }
       }
 
       w.forceMerge(1);


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/test/org/apache/lucene/index/TestTermVectorsReader.java lucene4055/lucene/core/src/test/org/apache/lucene/index/TestTermVectorsReader.java
--- lucene-trunk/lucene/core/src/test/org/apache/lucene/index/TestTermVectorsReader.java	2012-05-24 16:55:47.788233425 -0400
+++ lucene4055/lucene/core/src/test/org/apache/lucene/index/TestTermVectorsReader.java	2012-05-23 13:47:15.318531816 -0400
@@ -47,8 +47,8 @@
   private String[] testTerms = {"this", "is", "a", "test"};
   private int[][] positions = new int[testTerms.length][];
   private Directory dir;
-  private SegmentInfo seg;
-  private FieldInfos fieldInfos = new FieldInfos(new FieldInfos.FieldNumberBiMap());
+  private SegmentInfoPerCommit seg;
+  private FieldInfos fieldInfos = new FieldInfos(new FieldInfo[0]);
   private static int TERM_FREQ = 3;
 
   private class TestToken implements Comparable<TestToken> {
@@ -128,7 +128,7 @@
     seg = writer.newestSegment();
     writer.close();
 
-    fieldInfos = seg.getFieldInfos(); //new FieldInfos(dir, IndexFileNames.segmentFileName(seg.name, "", IndexFileNames.FIELD_INFOS_EXTENSION));
+    fieldInfos = _TestUtil.getFieldInfos(seg.info);
   }
   
   @Override
@@ -187,20 +187,14 @@
     //Check to see the files were created properly in setup
     DirectoryReader reader = IndexReader.open(dir);
     for (IndexReader r : reader.getSequentialSubReaders()) {
-      SegmentInfo s = ((SegmentReader) r).getSegmentInfo();
-      assertTrue(s.getHasVectors());
-      Set<String> files = new HashSet<String>();
-      s.getCodec().termVectorsFormat().files(s, files);
-      assertFalse(files.isEmpty());
-      for (String file : files) {
-        assertTrue(dir.fileExists(file));
-      }
+      SegmentInfoPerCommit s = ((SegmentReader) r).getSegmentInfo();
+      assertTrue(((SegmentReader) r).getFieldInfos().hasVectors());
     }
     reader.close();
   }
 
   public void testReader() throws IOException {
-    TermVectorsReader reader = Codec.getDefault().termVectorsFormat().vectorsReader(dir, seg, fieldInfos, newIOContext(random()));
+    TermVectorsReader reader = Codec.getDefault().termVectorsFormat().vectorsReader(dir, seg.info, fieldInfos, newIOContext(random()));
     for (int j = 0; j < 5; j++) {
       Terms vector = reader.get(j).terms(testFields[0]);
       assertNotNull(vector);
@@ -219,7 +213,7 @@
   }
   
   public void testDocsEnum() throws IOException {
-    TermVectorsReader reader = Codec.getDefault().termVectorsFormat().vectorsReader(dir, seg, fieldInfos, newIOContext(random()));
+    TermVectorsReader reader = Codec.getDefault().termVectorsFormat().vectorsReader(dir, seg.info, fieldInfos, newIOContext(random()));
     for (int j = 0; j < 5; j++) {
       Terms vector = reader.get(j).terms(testFields[0]);
       assertNotNull(vector);
@@ -246,7 +240,7 @@
   }
 
   public void testPositionReader() throws IOException {
-    TermVectorsReader reader = Codec.getDefault().termVectorsFormat().vectorsReader(dir, seg, fieldInfos, newIOContext(random()));
+    TermVectorsReader reader = Codec.getDefault().termVectorsFormat().vectorsReader(dir, seg.info, fieldInfos, newIOContext(random()));
     BytesRef[] terms;
     Terms vector = reader.get(0).terms(testFields[0]);
     assertNotNull(vector);
@@ -301,7 +295,7 @@
   }
 
   public void testOffsetReader() throws IOException {
-    TermVectorsReader reader = Codec.getDefault().termVectorsFormat().vectorsReader(dir, seg, fieldInfos, newIOContext(random()));
+    TermVectorsReader reader = Codec.getDefault().termVectorsFormat().vectorsReader(dir, seg.info, fieldInfos, newIOContext(random()));
     Terms vector = reader.get(0).terms(testFields[0]);
     assertNotNull(vector);
     TermsEnum termsEnum = vector.iterator(null);
@@ -343,7 +337,7 @@
   public void testBadParams() throws IOException {
     TermVectorsReader reader = null;
     try {
-      reader = Codec.getDefault().termVectorsFormat().vectorsReader(dir, seg, fieldInfos, newIOContext(random()));
+      reader = Codec.getDefault().termVectorsFormat().vectorsReader(dir, seg.info, fieldInfos, newIOContext(random()));
       //Bad document number, good field number
       reader.get(50);
       fail();
@@ -352,7 +346,7 @@
     } finally {
       reader.close();
     }
-    reader = Codec.getDefault().termVectorsFormat().vectorsReader(dir, seg, fieldInfos, newIOContext(random()));
+    reader = Codec.getDefault().termVectorsFormat().vectorsReader(dir, seg.info, fieldInfos, newIOContext(random()));
     //good document number, bad field
     Terms vector = reader.get(0).terms("f50");
     assertNull(vector);


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/test/org/apache/lucene/search/TestBoolean2.java lucene4055/lucene/core/src/test/org/apache/lucene/search/TestBoolean2.java
--- lucene-trunk/lucene/core/src/test/org/apache/lucene/search/TestBoolean2.java	2012-05-24 16:55:47.512233420 -0400
+++ lucene4055/lucene/core/src/test/org/apache/lucene/search/TestBoolean2.java	2012-05-21 13:58:02.547533870 -0400
@@ -72,7 +72,13 @@
     // First multiply small test index:
     mulFactor = 1;
     int docCount = 0;
+    if (VERBOSE) {
+      System.out.println("\nTEST: now copy index...");
+    }
     do {
+      if (VERBOSE) {
+        System.out.println("\nTEST: cycle...");
+      }
       final Directory copy = new MockDirectoryWrapper(random(), new RAMDirectory(dir2, IOContext.DEFAULT));
       RandomIndexWriter w = new RandomIndexWriter(random(), dir2);
       w.addIndexes(copy);


diff -ruN -x .svn -x build lucene-trunk/lucene/core/src/test/org/apache/lucene/TestDemo.java lucene4055/lucene/core/src/test/org/apache/lucene/TestDemo.java
--- lucene-trunk/lucene/core/src/test/org/apache/lucene/TestDemo.java	2012-05-24 16:55:47.416233418 -0400
+++ lucene4055/lucene/core/src/test/org/apache/lucene/TestDemo.java	2012-05-21 15:17:41.247617088 -0400
@@ -44,7 +44,7 @@
     // Store the index in memory:
     Directory directory = newDirectory();
     // To store an index on disk, use this instead:
-    //Directory directory = FSDirectory.open("/tmp/testindex");
+    // Directory directory = FSDirectory.open(new File("/tmp/testindex"));
     RandomIndexWriter iwriter = new RandomIndexWriter(random(), directory, analyzer);
     Document doc = new Document();
     String longTerm = "longtermlongtermlongtermlongtermlongtermlongtermlongtermlongtermlongtermlongtermlongtermlongtermlongtermlongtermlongtermlongtermlongtermlongterm";


diff -ruN -x .svn -x build lucene-trunk/lucene/highlighter/src/java/org/apache/lucene/search/vectorhighlight/BaseFragmentsBuilder.java lucene4055/lucene/highlighter/src/java/org/apache/lucene/search/vectorhighlight/BaseFragmentsBuilder.java
--- lucene-trunk/lucene/highlighter/src/java/org/apache/lucene/search/vectorhighlight/BaseFragmentsBuilder.java	2012-05-24 16:55:50.332233469 -0400
+++ lucene4055/lucene/highlighter/src/java/org/apache/lucene/search/vectorhighlight/BaseFragmentsBuilder.java	2012-05-21 13:58:05.371533919 -0400
@@ -127,7 +127,7 @@
         @Override
         public void stringField(FieldInfo fieldInfo, String value) throws IOException {
           FieldType ft = new FieldType(TextField.TYPE_STORED);
-          ft.setStoreTermVectors(fieldInfo.storeTermVector);
+          ft.setStoreTermVectors(fieldInfo.hasVectors());
           fields.add(new Field(fieldInfo.name, value, ft));
         }
 


diff -ruN -x .svn -x build lucene-trunk/lucene/memory/src/java/org/apache/lucene/index/memory/MemoryIndex.java lucene4055/lucene/memory/src/java/org/apache/lucene/index/memory/MemoryIndex.java
--- lucene-trunk/lucene/memory/src/java/org/apache/lucene/index/memory/MemoryIndex.java	2012-05-24 16:55:50.424233471 -0400
+++ lucene4055/lucene/memory/src/java/org/apache/lucene/index/memory/MemoryIndex.java	2012-05-21 14:20:31.595557363 -0400
@@ -35,6 +35,7 @@
 import org.apache.lucene.document.Document;
 import org.apache.lucene.index.AtomicReader;
 import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.FieldInfo;
 import org.apache.lucene.index.Norm;
 import org.apache.lucene.index.DocValues;
 import org.apache.lucene.index.DocsAndPositionsEnum;
@@ -48,6 +49,7 @@
 import org.apache.lucene.index.TermState;
 import org.apache.lucene.index.Terms;
 import org.apache.lucene.index.TermsEnum;
+import org.apache.lucene.index.FieldInfo.IndexOptions;
 import org.apache.lucene.index.memory.MemoryIndexNormDocValues.SingleValueSource;
 import org.apache.lucene.search.Collector;
 import org.apache.lucene.search.IndexSearcher;
@@ -199,7 +201,7 @@
 
   private static final boolean DEBUG = false;
 
-  private final FieldInfos fieldInfos;
+  private HashMap<String,FieldInfo> fieldInfos = new HashMap<String,FieldInfo>();
   
   /**
    * Sorts term entries into ascending order; also works for
@@ -235,7 +237,6 @@
    */
   protected MemoryIndex(boolean storeOffsets) {
     this.stride = storeOffsets ? 3 : 1;
-    fieldInfos = new FieldInfos();
   }
   
   /**
@@ -352,7 +353,10 @@
       int numOverlapTokens = 0;
       int pos = -1;
 
-      fieldInfos.addOrUpdate(fieldName, true);
+      if (!fieldInfos.containsKey(fieldName)) {
+        fieldInfos.put(fieldName, 
+            new FieldInfo(fieldName, true, fieldInfos.size(), false, false, false, IndexOptions.DOCS_AND_FREQS_AND_POSITIONS, null, null, null));
+      }
       
       TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);
       PositionIncrementAttribute posIncrAttribute = stream.addAttribute(PositionIncrementAttribute.class);
@@ -551,10 +555,6 @@
     return result.toString();
   }
   
-  
-  ///////////////////////////////////////////////////////////////////////////////
-  // Nested classes:
-  ///////////////////////////////////////////////////////////////////////////////
   /**
    * Index data structure for a field; Contains the tokenized term texts and
    * their positions.
@@ -714,7 +714,7 @@
     
     @Override
     public FieldInfos getFieldInfos() {
-      return fieldInfos;
+      return new FieldInfos(fieldInfos.values().toArray(new FieldInfo[fieldInfos.size()]));
     }
 
     private class MemoryFields extends Fields {


diff -ruN -x .svn -x build lucene-trunk/lucene/misc/src/java/org/apache/lucene/index/BalancedSegmentMergePolicy.java lucene4055/lucene/misc/src/java/org/apache/lucene/index/BalancedSegmentMergePolicy.java
--- lucene-trunk/lucene/misc/src/java/org/apache/lucene/index/BalancedSegmentMergePolicy.java	2012-05-24 16:55:49.056233447 -0400
+++ lucene4055/lucene/misc/src/java/org/apache/lucene/index/BalancedSegmentMergePolicy.java	2012-05-23 14:42:53.862589955 -0400
@@ -57,10 +57,10 @@
   }
   
   @Override
-  protected long size(SegmentInfo info) throws IOException {
+  protected long size(SegmentInfoPerCommit info) throws IOException {
     long byteSize = info.sizeInBytes();
-    float delRatio = (info.docCount <= 0 ? 0.0f : ((float)info.getDelCount() / (float)info.docCount));
-    return (info.docCount <= 0 ?  byteSize : (long)((1.0f - delRatio) * byteSize));
+    float delRatio = (info.info.getDocCount() <= 0 ? 0.0f : ((float)info.getDelCount() / (float)info.info.getDocCount()));
+    return (info.info.getDocCount() <= 0 ?  byteSize : (long)((1.0f - delRatio) * byteSize));
   }
   
   public void setPartialExpunge(boolean doPartialExpunge) {
@@ -106,7 +106,7 @@
   }
   
   @Override
-  public MergeSpecification findForcedMerges(SegmentInfos infos, int maxNumSegments, Map<SegmentInfo,Boolean> segmentsToMerge) throws IOException {
+  public MergeSpecification findForcedMerges(SegmentInfos infos, int maxNumSegments, Map<SegmentInfoPerCommit,Boolean> segmentsToMerge) throws IOException {
     
     assert maxNumSegments > 0;
 
@@ -120,7 +120,7 @@
       int last = infos.size();
       while(last > 0) {
 
-        final SegmentInfo info = infos.info(--last);
+        final SegmentInfoPerCommit info = infos.info(--last);
         if (segmentsToMerge.containsKey(info)) {
           last++;
           break;
@@ -196,7 +196,7 @@
         spec.add(new OneMerge(infos.asList().subList(mergeStart, mergeEnd)));
       } else {
         if(partialExpunge) {
-          SegmentInfo info = infos.info(mergeStart);
+          SegmentInfoPerCommit info = infos.info(mergeStart);
           int delCount = info.getDelCount();
           if(delCount > maxDelCount) {
             expungeCandidate = mergeStart;
@@ -260,8 +260,8 @@
     
     if(spec == null) spec = new MergeSpecification();
     for(int i = 0; i < numLargeSegs; i++) {
-      SegmentInfo info = infos.info(i);
-      if(info.hasDeletions()) {
+      SegmentInfoPerCommit info = infos.info(i);
+      if (info.hasDeletions()) {
         spec.add(new OneMerge(Collections.singletonList(infos.info(i))));
       }
     }
@@ -279,7 +279,7 @@
     
     long totalLargeSegSize = 0;
     long totalSmallSegSize = 0;
-    SegmentInfo info;
+    SegmentInfoPerCommit info;
     
     // compute the total size of large segments
     for(int i = 0; i < numLargeSegs; i++) {
@@ -340,7 +340,7 @@
     int maxDelCount = 0;
     
     for(int i = maxNumSegments - 1; i >= 0; i--) {
-      SegmentInfo info = infos.info(i);
+      SegmentInfoPerCommit info = infos.info(i);
       int delCount = info.getDelCount();
       if (delCount > maxDelCount) {
         expungeCandidate = i;


diff -ruN -x .svn -x build lucene-trunk/lucene/misc/src/java/org/apache/lucene/index/IndexSplitter.java lucene4055/lucene/misc/src/java/org/apache/lucene/index/IndexSplitter.java
--- lucene-trunk/lucene/misc/src/java/org/apache/lucene/index/IndexSplitter.java	2012-05-24 16:55:49.056233447 -0400
+++ lucene4055/lucene/misc/src/java/org/apache/lucene/index/IndexSplitter.java	2012-05-24 10:07:08.411806438 -0400
@@ -24,6 +24,7 @@
 import java.io.OutputStream;
 import java.text.DecimalFormat;
 import java.util.ArrayList;
+import java.util.Collection;
 import java.util.List;
 
 import org.apache.lucene.index.IndexWriter;  // Required for javadocs
@@ -104,23 +105,23 @@
   public void listSegments() throws IOException {
     DecimalFormat formatter = new DecimalFormat("###,###.###");
     for (int x = 0; x < infos.size(); x++) {
-      SegmentInfo info = infos.info(x);
+      SegmentInfoPerCommit info = infos.info(x);
       String sizeStr = formatter.format(info.sizeInBytes());
-      System.out.println(info.name + " " + sizeStr);
+      System.out.println(info.info.name + " " + sizeStr);
     }
   }
 
   private int getIdx(String name) {
     for (int x = 0; x < infos.size(); x++) {
-      if (name.equals(infos.info(x).name))
+      if (name.equals(infos.info(x).info.name))
         return x;
     }
     return -1;
   }
 
-  private SegmentInfo getInfo(String name) {
+  private SegmentInfoPerCommit getInfo(String name) {
     for (int x = 0; x < infos.size(); x++) {
-      if (name.equals(infos.info(x).name))
+      if (name.equals(infos.info(x).info.name))
         return infos.info(x);
     }
     return null;
@@ -132,7 +133,7 @@
       infos.remove(idx);
     }
     infos.changed();
-    infos.commit(fsDir, infos.codecFormat());
+    infos.commit(fsDir);
   }
 
   public void split(File destDir, String[] segs) throws IOException {
@@ -141,10 +142,15 @@
     SegmentInfos destInfos = new SegmentInfos();
     destInfos.counter = infos.counter;
     for (String n : segs) {
-      SegmentInfo info = getInfo(n);
-      destInfos.add(info);
+      SegmentInfoPerCommit infoPerCommit = getInfo(n);
+      SegmentInfo info = infoPerCommit.info;
+      // Same info just changing the dir:
+      SegmentInfo newInfo = new SegmentInfo(destFSDir, info.getVersion(), info.name, info.getDocCount(), 
+                                            info.getUseCompoundFile(),
+                                            info.getCodec(), info.getDiagnostics(), info.attributes());
+      destInfos.add(new SegmentInfoPerCommit(newInfo, infoPerCommit.getDelCount(), infoPerCommit.getDelGen()));
       // now copy files over
-      List<String> files = info.files();
+      Collection<String> files = infoPerCommit.files();
       for (final String srcName : files) {
         File srcFile = new File(dir, srcName);
         File destFile = new File(destDir, srcName);
@@ -152,7 +158,7 @@
       }
     }
     destInfos.changed();
-    destInfos.commit(destFSDir, infos.codecFormat());
+    destInfos.commit(destFSDir);
     // System.out.println("destDir:"+destDir.getAbsolutePath());
   }
 


diff -ruN -x .svn -x build lucene-trunk/lucene/misc/src/test/org/apache/lucene/index/TestIndexSplitter.java lucene4055/lucene/misc/src/test/org/apache/lucene/index/TestIndexSplitter.java
--- lucene-trunk/lucene/misc/src/test/org/apache/lucene/index/TestIndexSplitter.java	2012-05-24 16:55:49.048233447 -0400
+++ lucene4055/lucene/misc/src/test/org/apache/lucene/index/TestIndexSplitter.java	2012-05-22 12:04:43.324920074 -0400
@@ -64,7 +64,7 @@
     iw.close();
     // we should have 2 segments now
     IndexSplitter is = new IndexSplitter(dir);
-    String splitSegName = is.infos.info(1).name;
+    String splitSegName = is.infos.info(1).info.name;
     is.split(destDir, new String[] {splitSegName});
     Directory fsDirDest = newFSDirectory(destDir);
     DirectoryReader r = DirectoryReader.open(fsDirDest);
@@ -77,7 +77,7 @@
     _TestUtil.rmDir(destDir2);
     destDir2.mkdirs();
     IndexSplitter.main(new String[] {dir.getAbsolutePath(), destDir2.getAbsolutePath(), splitSegName});
-    assertEquals(4, destDir2.listFiles().length);
+    assertEquals(5, destDir2.listFiles().length);
     Directory fsDirDest2 = newFSDirectory(destDir2);
     r = DirectoryReader.open(fsDirDest2);
     assertEquals(50, r.maxDoc());


diff -ruN -x .svn -x build lucene-trunk/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene3x/PreFlexRWCodec.java lucene4055/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene3x/PreFlexRWCodec.java
--- lucene-trunk/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene3x/PreFlexRWCodec.java	2012-05-24 16:55:50.472233471 -0400
+++ lucene4055/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene3x/PreFlexRWCodec.java	2012-05-24 12:05:48.127930414 -0400
@@ -17,21 +17,15 @@
  * limitations under the License.
  */
 
-import java.io.IOException;
-import java.util.Set;
-
 import org.apache.lucene.codecs.FieldInfosFormat;
 import org.apache.lucene.codecs.LiveDocsFormat;
 import org.apache.lucene.codecs.NormsFormat;
 import org.apache.lucene.codecs.PostingsFormat;
-import org.apache.lucene.codecs.SegmentInfosFormat;
+import org.apache.lucene.codecs.SegmentInfoFormat;
 import org.apache.lucene.codecs.StoredFieldsFormat;
 import org.apache.lucene.codecs.TermVectorsFormat;
 import org.apache.lucene.codecs.lucene40.Lucene40LiveDocsFormat;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.SegmentInfo;
 import org.apache.lucene.util.LuceneTestCase;
-import org.apache.lucene.util.StringHelper;
 
 /**
  * Writes 3.x-like indexes (not perfect emulation yet) for testing only!
@@ -42,7 +36,7 @@
   private final Lucene3xNormsFormat norms = new PreFlexRWNormsFormat();
   private final FieldInfosFormat fieldInfos = new PreFlexRWFieldInfosFormat();
   private final TermVectorsFormat termVectors = new PreFlexRWTermVectorsFormat();
-  private final SegmentInfosFormat segmentInfos = new PreFlexRWSegmentInfosFormat();
+  private final SegmentInfoFormat segmentInfos = new PreFlexRWSegmentInfoFormat();
   private final StoredFieldsFormat storedFields = new PreFlexRWStoredFieldsFormat();
   // TODO: this should really be a different impl
   private final LiveDocsFormat liveDocs = new Lucene40LiveDocsFormat();
@@ -66,11 +60,11 @@
   }
 
   @Override
-  public SegmentInfosFormat segmentInfosFormat() {
+  public SegmentInfoFormat segmentInfoFormat() {
     if (LuceneTestCase.PREFLEX_IMPERSONATION_IS_ACTIVE) {
       return segmentInfos ;
     } else {
-      return super.segmentInfosFormat();
+      return super.segmentInfoFormat();
     }
   }
 
@@ -109,19 +103,4 @@
       return super.storedFieldsFormat();
     }
   }
-
-  @Override
-  public void files(SegmentInfo info, Set<String> files) throws IOException {
-    if (info.getUseCompoundFile() && LuceneTestCase.PREFLEX_IMPERSONATION_IS_ACTIVE) {
-      // because we don't fully emulate 3.x codec, PreFlexRW actually writes 4.x format CFS files.
-      // so we must check segment version here to see if its a "real" 3.x segment or a "fake"
-      // one that we wrote with a 4.x-format CFS+CFE, in this case we must add the .CFE
-      String version = info.getVersion();
-      if (version != null && StringHelper.getVersionComparator().compare("4.0", version) <= 0) {
-        files.add(IndexFileNames.segmentFileName(info.name, "", IndexFileNames.COMPOUND_FILE_ENTRIES_EXTENSION));
-      }
-    }
-    
-    super.files(info, files);
-  }
 }


diff -ruN -x .svn -x build lucene-trunk/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene3x/PreFlexRWFieldInfosFormat.java lucene4055/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene3x/PreFlexRWFieldInfosFormat.java
--- lucene-trunk/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene3x/PreFlexRWFieldInfosFormat.java	2012-05-24 16:55:50.472233471 -0400
+++ lucene4055/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene3x/PreFlexRWFieldInfosFormat.java	2012-05-24 12:05:55.727930552 -0400
@@ -16,6 +16,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
+
 import java.io.IOException;
 
 import org.apache.lucene.codecs.FieldInfosReader;


diff -ruN -x .svn -x build lucene-trunk/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene3x/PreFlexRWFieldInfosReader.java lucene4055/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene3x/PreFlexRWFieldInfosReader.java
--- lucene-trunk/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene3x/PreFlexRWFieldInfosReader.java	2012-05-24 16:55:50.472233471 -0400
+++ lucene4055/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene3x/PreFlexRWFieldInfosReader.java	2012-05-21 14:20:41.867557542 -0400
@@ -43,10 +43,6 @@
   public FieldInfos read(Directory directory, String segmentName, IOContext iocontext) throws IOException {
     final String fileName = IndexFileNames.segmentFileName(segmentName, "", PreFlexRWFieldInfosWriter.FIELD_INFOS_EXTENSION);
     IndexInput input = directory.openInput(fileName, iocontext);
-
-    boolean hasVectors = false;
-    boolean hasFreq = false;
-    boolean hasProx = false;
     
     try {
       final int format = input.readVInt();
@@ -88,9 +84,6 @@
         if (indexOptions != IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) {
           storePayloads = false;
         }
-        hasVectors |= storeTermVector;
-        hasProx |= isIndexed && indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS;
-        hasFreq |= isIndexed && indexOptions != IndexOptions.DOCS_ONLY;
         
         Type normType = isIndexed && !omitNorms ? Type.FIXED_INTS_8 : null;
         if (format == PreFlexRWFieldInfosWriter.FORMAT_PREFLEX_RW && normType != null) {
@@ -99,13 +92,13 @@
         }
         
         infos[i] = new FieldInfo(name, isIndexed, fieldNumber, storeTermVector, 
-          omitNorms, storePayloads, indexOptions, null, normType);
+          omitNorms, storePayloads, indexOptions, null, normType, null);
       }
 
       if (input.getFilePointer() != input.length()) {
         throw new CorruptIndexException("did not read all bytes from file \"" + fileName + "\": read " + input.getFilePointer() + " vs size " + input.length() + " (resource: " + input + ")");
       }
-      return new FieldInfos(infos, hasFreq, hasProx, hasVectors);
+      return new FieldInfos(infos);
     } finally {
       input.close();
     }


diff -ruN -x .svn -x build lucene-trunk/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene3x/PreFlexRWFieldInfosWriter.java lucene4055/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene3x/PreFlexRWFieldInfosWriter.java
--- lucene-trunk/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene3x/PreFlexRWFieldInfosWriter.java	2012-05-24 16:55:50.472233471 -0400
+++ lucene4055/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene3x/PreFlexRWFieldInfosWriter.java	2012-05-21 14:53:37.311591943 -0400
@@ -62,15 +62,15 @@
       output.writeVInt(FORMAT_PREFLEX_RW);
       output.writeVInt(infos.size());
       for (FieldInfo fi : infos) {
-        assert fi.indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS || !fi.storePayloads;
+        assert fi.getIndexOptions() == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS || !fi.hasPayloads();
         byte bits = 0x0;
-        if (fi.isIndexed) bits |= IS_INDEXED;
-        if (fi.storeTermVector) bits |= STORE_TERMVECTOR;
-        if (fi.omitNorms) bits |= OMIT_NORMS;
-        if (fi.storePayloads) bits |= STORE_PAYLOADS;
-        if (fi.indexOptions == IndexOptions.DOCS_ONLY) {
+        if (fi.isIndexed()) bits |= IS_INDEXED;
+        if (fi.hasVectors()) bits |= STORE_TERMVECTOR;
+        if (fi.omitsNorms()) bits |= OMIT_NORMS;
+        if (fi.hasPayloads()) bits |= STORE_PAYLOADS;
+        if (fi.getIndexOptions() == IndexOptions.DOCS_ONLY) {
           bits |= OMIT_TERM_FREQ_AND_POSITIONS;
-        } else if (fi.indexOptions == IndexOptions.DOCS_AND_FREQS) {
+        } else if (fi.getIndexOptions() == IndexOptions.DOCS_AND_FREQS) {
           bits |= OMIT_POSITIONS;
         }
         output.writeString(fi.name);
@@ -81,11 +81,12 @@
          */
         output.writeInt(fi.number);
         output.writeByte(bits);
-        if (fi.isIndexed && !fi.omitNorms) {
+        if (fi.isIndexed() && !fi.omitsNorms()) {
           // to allow null norm types we need to indicate if norms are written 
           // only in RW case
           output.writeByte((byte) (fi.getNormType() == null ? 0 : 1));
         }
+        assert fi.attributes() == null; // not used or supported
       }
     } finally {
       output.close();


diff -ruN -x .svn -x build lucene-trunk/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene3x/PreFlexRWFieldsWriter.java lucene4055/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene3x/PreFlexRWFieldsWriter.java
--- lucene-trunk/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene3x/PreFlexRWFieldsWriter.java	2012-05-24 16:55:50.472233471 -0400
+++ lucene4055/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene3x/PreFlexRWFieldsWriter.java	2012-05-23 15:21:01.234629788 -0400
@@ -43,15 +43,15 @@
 
   public PreFlexRWFieldsWriter(SegmentWriteState state) throws IOException {
     termsOut = new TermInfosWriter(state.directory,
-                                   state.segmentName,
+                                   state.segmentInfo.name,
                                    state.fieldInfos,
                                    state.termIndexInterval);
 
     boolean success = false;
     try {
-      final String freqFile = IndexFileNames.segmentFileName(state.segmentName, "", Lucene3xPostingsFormat.FREQ_EXTENSION);
+      final String freqFile = IndexFileNames.segmentFileName(state.segmentInfo.name, "", Lucene3xPostingsFormat.FREQ_EXTENSION);
       freqOut = state.directory.createOutput(freqFile, state.context);
-      totalNumDocs = state.numDocs;
+      totalNumDocs = state.segmentInfo.getDocCount();
       success = true;
     } finally {
       if (!success) {
@@ -62,7 +62,7 @@
     success = false;
     try {
       if (state.fieldInfos.hasProx()) {
-        final String proxFile = IndexFileNames.segmentFileName(state.segmentName, "", Lucene3xPostingsFormat.PROX_EXTENSION);
+        final String proxFile = IndexFileNames.segmentFileName(state.segmentInfo.name, "", Lucene3xPostingsFormat.PROX_EXTENSION);
         proxOut = state.directory.createOutput(proxFile, state.context);
       } else {
         proxOut = null;
@@ -85,7 +85,7 @@
   @Override
   public TermsConsumer addField(FieldInfo field) throws IOException {
     assert field.number != -1;
-    if (field.indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0) {
+    if (field.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0) {
       throw new UnsupportedOperationException("this codec cannot index offsets");
     }
     //System.out.println("w field=" + field.name + " storePayload=" + field.storePayloads + " number=" + field.number);
@@ -107,8 +107,8 @@
 
     public PreFlexTermsWriter(FieldInfo fieldInfo) {
       this.fieldInfo = fieldInfo;
-      omitTF = fieldInfo.indexOptions == IndexOptions.DOCS_ONLY;
-      storePayloads = fieldInfo.storePayloads;
+      omitTF = fieldInfo.getIndexOptions() == IndexOptions.DOCS_ONLY;
+      storePayloads = fieldInfo.hasPayloads();
     }
 
     private class PostingsWriter extends PostingsConsumer {


diff -ruN -x .svn -x build lucene-trunk/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene3x/PreFlexRWNormsConsumer.java lucene4055/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene3x/PreFlexRWNormsConsumer.java
--- lucene-trunk/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene3x/PreFlexRWNormsConsumer.java	2012-05-24 16:55:50.472233471 -0400
+++ lucene4055/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene3x/PreFlexRWNormsConsumer.java	2012-05-21 13:58:05.563533923 -0400
@@ -209,7 +209,7 @@
     }
     
     public void startField(FieldInfo info) throws IOException {
-      assert info.omitNorms == false;
+      assert info.omitsNorms() == false;
       normCount++;
     }
     


diff -ruN -x .svn -x build lucene-trunk/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene3x/PreFlexRWNormsFormat.java lucene4055/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene3x/PreFlexRWNormsFormat.java
--- lucene-trunk/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene3x/PreFlexRWNormsFormat.java	2012-05-24 16:55:50.472233471 -0400
+++ lucene4055/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene3x/PreFlexRWNormsFormat.java	2012-05-24 12:06:09.471930793 -0400
@@ -1,4 +1,5 @@
 package org.apache.lucene.codecs.lucene3x;
+
 /**
  * Licensed to the Apache Software Foundation (ASF) under one or more
  * contributor license agreements.  See the NOTICE file distributed with
@@ -15,6 +16,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
+
 import java.io.IOException;
 
 import org.apache.lucene.codecs.PerDocConsumer;
@@ -28,7 +30,6 @@
 
   @Override
   public PerDocConsumer docsConsumer(PerDocWriteState state) throws IOException {
-    return new PreFlexRWNormsConsumer(state.directory, state.segmentName, state.context);
+    return new PreFlexRWNormsConsumer(state.directory, state.segmentInfo.name, state.context);
   }
-
 }


diff -ruN -x .svn -x build lucene-trunk/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene3x/PreFlexRWSegmentInfoFormat.java lucene4055/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene3x/PreFlexRWSegmentInfoFormat.java
--- lucene-trunk/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene3x/PreFlexRWSegmentInfoFormat.java	1969-12-31 19:00:00.000000000 -0500
+++ lucene4055/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene3x/PreFlexRWSegmentInfoFormat.java	2012-05-22 16:52:25.721220689 -0400
@@ -0,0 +1,32 @@
+package org.apache.lucene.codecs.lucene3x;
+
+import org.apache.lucene.codecs.SegmentInfoWriter;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * @lucene.experimental
+ */
+class PreFlexRWSegmentInfoFormat extends Lucene3xSegmentInfoFormat {
+  private final SegmentInfoWriter writer = new PreFlexRWSegmentInfoWriter();
+
+  @Override
+  public SegmentInfoWriter getSegmentInfosWriter() {
+    return writer;
+  }
+}


diff -ruN -x .svn -x build lucene-trunk/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene3x/PreFlexRWSegmentInfosFormat.java lucene4055/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene3x/PreFlexRWSegmentInfosFormat.java
--- lucene-trunk/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene3x/PreFlexRWSegmentInfosFormat.java	2012-05-24 16:55:50.472233471 -0400
+++ lucene4055/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene3x/PreFlexRWSegmentInfosFormat.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,32 +0,0 @@
-package org.apache.lucene.codecs.lucene3x;
-
-import org.apache.lucene.codecs.SegmentInfosWriter;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * @lucene.experimental
- */
-class PreFlexRWSegmentInfosFormat extends Lucene3xSegmentInfosFormat {
-  private final SegmentInfosWriter writer = new PreFlexRWSegmentInfosWriter();
-  
-  @Override
-  public SegmentInfosWriter getSegmentInfosWriter() {
-    return writer;
-  }
-}


diff -ruN -x .svn -x build lucene-trunk/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene3x/PreFlexRWSegmentInfosWriter.java lucene4055/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene3x/PreFlexRWSegmentInfosWriter.java
--- lucene-trunk/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene3x/PreFlexRWSegmentInfosWriter.java	2012-05-24 16:55:50.472233471 -0400
+++ lucene4055/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene3x/PreFlexRWSegmentInfosWriter.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,118 +0,0 @@
-package org.apache.lucene.codecs.lucene3x;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Map;
-import java.util.Map.Entry;
-
-import org.apache.lucene.codecs.SegmentInfosWriter;
-import org.apache.lucene.index.SegmentInfo;
-import org.apache.lucene.index.SegmentInfos;
-import org.apache.lucene.store.ChecksumIndexOutput;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.FlushInfo;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.util.IOUtils;
-
-/**
- * PreFlex implementation of {@link SegmentInfosWriter}.
- * @lucene.experimental
- */
-class PreFlexRWSegmentInfosWriter extends SegmentInfosWriter {
-
-  @Override
-  public IndexOutput writeInfos(Directory dir, String segmentFileName, String codecID, SegmentInfos infos, IOContext context)
-          throws IOException {
-    IndexOutput out = createOutput(dir, segmentFileName, new IOContext(new FlushInfo(infos.size(), infos.totalDocCount())));
-    boolean success = false;
-    try {
-      out.writeInt(SegmentInfos.FORMAT_3_1); // write FORMAT
-      // we don't write a codec - this is 3.x
-      out.writeLong(infos.version);
-      out.writeInt(infos.counter); // write counter
-      out.writeInt(infos.size()); // write infos
-      for (SegmentInfo si : infos) {
-        writeInfo(out, si);
-      }
-      out.writeStringStringMap(infos.getUserData());
-      success = true;
-      return out;
-    } finally {
-      if (!success) {
-        IOUtils.closeWhileHandlingException(out);
-      }
-    }
-  }
-  
-  /** Save a single segment's info. */
-  private void writeInfo(IndexOutput output, SegmentInfo si) throws IOException {
-    // we are about to write this SI in 3.x format, dropping all codec information, etc.
-    // so it had better be a 3.x segment or you will get very confusing errors later.
-    assert si.getCodec() instanceof Lucene3xCodec : "broken test, trying to mix preflex with other codecs";
-    assert si.getDelCount() <= si.docCount: "delCount=" + si.getDelCount() + " docCount=" + si.docCount + " segment=" + si.name;
-    // Write the Lucene version that created this segment, since 3.1
-    output.writeString(si.getVersion());
-    output.writeString(si.name);
-    output.writeInt(si.docCount);
-    output.writeLong(si.getDelGen());
-
-    output.writeInt(si.getDocStoreOffset());
-    if (si.getDocStoreOffset() != -1) {
-      output.writeString(si.getDocStoreSegment());
-      output.writeByte((byte) (si.getDocStoreIsCompoundFile() ? 1:0));
-    }
-    // pre-4.0 indexes write a byte if there is a single norms file
-    output.writeByte((byte) 1);
-
-    Map<Integer,Long> normGen = si.getNormGen();
-    if (normGen == null) {
-      output.writeInt(SegmentInfo.NO);
-    } else {
-      output.writeInt(normGen.size());
-      for (Entry<Integer,Long> entry : normGen.entrySet()) {
-        output.writeLong(entry.getValue());
-      }
-    }
-
-    output.writeByte((byte) (si.getUseCompoundFile() ? SegmentInfo.YES : SegmentInfo.NO));
-    output.writeInt(si.getDelCount());
-    output.writeByte((byte) (si.getHasProxInternal()));
-    output.writeStringStringMap(si.getDiagnostics());
-    output.writeByte((byte) (si.getHasVectorsInternal()));
-  }
-  
-  protected IndexOutput createOutput(Directory dir, String segmentFileName, IOContext context)
-      throws IOException {
-    IndexOutput plainOut = dir.createOutput(segmentFileName, context);
-    ChecksumIndexOutput out = new ChecksumIndexOutput(plainOut);
-    return out;
-  }
-
-  @Override
-  public void prepareCommit(IndexOutput segmentOutput) throws IOException {
-    ((ChecksumIndexOutput)segmentOutput).prepareCommit();
-  }
-
-  @Override
-  public void finishCommit(IndexOutput out) throws IOException {
-    ((ChecksumIndexOutput)out).finishCommit();
-    out.close();
-  }
-}


diff -ruN -x .svn -x build lucene-trunk/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene3x/PreFlexRWSegmentInfoWriter.java lucene4055/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene3x/PreFlexRWSegmentInfoWriter.java
--- lucene-trunk/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene3x/PreFlexRWSegmentInfoWriter.java	1969-12-31 19:00:00.000000000 -0500
+++ lucene4055/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene3x/PreFlexRWSegmentInfoWriter.java	2012-05-24 12:06:29.307931139 -0400
@@ -0,0 +1,45 @@
+package org.apache.lucene.codecs.lucene3x;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.codecs.SegmentInfoWriter;
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.index.SegmentInfos;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+
+/**
+ * PreFlex implementation of {@link SegmentInfoWriter}.
+ * @lucene.experimental
+ */
+class PreFlexRWSegmentInfoWriter extends SegmentInfoWriter {
+
+  // NOTE: this is not "really" 3.x format, because we are
+  // writing each SI to its own file, vs 3.x where the list
+  // of segments and SI for each segment is written into a
+  // single segments_N file
+
+  /** Save a single segment's info. */
+  @Override
+  public void write(Directory dir, SegmentInfo si, FieldInfos fis, IOContext ioContext) throws IOException {
+    SegmentInfos.write3xInfo(dir, si, ioContext);
+  }
+}


diff -ruN -x .svn -x build lucene-trunk/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene3x/PreFlexRWStoredFieldsFormat.java lucene4055/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene3x/PreFlexRWStoredFieldsFormat.java
--- lucene-trunk/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene3x/PreFlexRWStoredFieldsFormat.java	2012-05-24 16:55:50.472233471 -0400
+++ lucene4055/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene3x/PreFlexRWStoredFieldsFormat.java	2012-05-24 12:06:36.383931259 -0400
@@ -20,14 +20,14 @@
 import java.io.IOException;
 
 import org.apache.lucene.codecs.StoredFieldsWriter;
+import org.apache.lucene.index.SegmentInfo;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.IOContext;
 
 class PreFlexRWStoredFieldsFormat extends Lucene3xStoredFieldsFormat {
 
   @Override
-  public StoredFieldsWriter fieldsWriter(Directory directory, String segment, IOContext context) throws IOException {
-    return new PreFlexRWStoredFieldsWriter(directory, segment, context);
+  public StoredFieldsWriter fieldsWriter(Directory directory, SegmentInfo segmentInfo, IOContext context) throws IOException {
+    return new PreFlexRWStoredFieldsWriter(directory, segmentInfo.name, context);
   }
-  
 }


diff -ruN -x .svn -x build lucene-trunk/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene3x/PreFlexRWStoredFieldsWriter.java lucene4055/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene3x/PreFlexRWStoredFieldsWriter.java
--- lucene-trunk/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene3x/PreFlexRWStoredFieldsWriter.java	2012-05-24 16:55:50.472233471 -0400
+++ lucene4055/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene3x/PreFlexRWStoredFieldsWriter.java	2012-05-22 18:37:03.945330021 -0400
@@ -20,6 +20,7 @@
 
 import org.apache.lucene.codecs.StoredFieldsWriter;
 import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.FieldInfos;
 import org.apache.lucene.index.IndexFileNames;
 import org.apache.lucene.index.IndexableField;
 import org.apache.lucene.store.Directory;
@@ -143,7 +144,7 @@
   }
 
   @Override
-  public void finish(int numDocs) throws IOException {
+  public void finish(FieldInfos fis, int numDocs) throws IOException {
     if (4+((long) numDocs)*8 != indexStream.getFilePointer())
       // This is most likely a bug in Sun JRE 1.6.0_04/_05;
       // we detect that the bug has struck, here, and


diff -ruN -x .svn -x build lucene-trunk/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene3x/PreFlexRWTermVectorsFormat.java lucene4055/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene3x/PreFlexRWTermVectorsFormat.java
--- lucene-trunk/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene3x/PreFlexRWTermVectorsFormat.java	2012-05-24 16:55:50.472233471 -0400
+++ lucene4055/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene3x/PreFlexRWTermVectorsFormat.java	2012-05-22 19:20:48.073375716 -0400
@@ -30,8 +30,8 @@
 class PreFlexRWTermVectorsFormat extends Lucene3xTermVectorsFormat {
 
   @Override
-  public TermVectorsWriter vectorsWriter(Directory directory, String segment, IOContext context) throws IOException {
-    return new PreFlexRWTermVectorsWriter(directory, segment, context);
+  public TermVectorsWriter vectorsWriter(Directory directory, SegmentInfo segmentInfo, IOContext context) throws IOException {
+    return new PreFlexRWTermVectorsWriter(directory, segmentInfo.name, context);
   }
 
   @Override


diff -ruN -x .svn -x build lucene-trunk/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene3x/PreFlexRWTermVectorsWriter.java lucene4055/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene3x/PreFlexRWTermVectorsWriter.java
--- lucene-trunk/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene3x/PreFlexRWTermVectorsWriter.java	2012-05-24 16:55:50.472233471 -0400
+++ lucene4055/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene3x/PreFlexRWTermVectorsWriter.java	2012-05-22 19:21:07.405376056 -0400
@@ -22,6 +22,7 @@
 
 import org.apache.lucene.codecs.TermVectorsWriter;
 import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.FieldInfos;
 import org.apache.lucene.index.IndexFileNames;
 import org.apache.lucene.store.DataInput;
 import org.apache.lucene.store.Directory;
@@ -194,7 +195,7 @@
   }
   
   @Override
-  public void finish(int numDocs) throws IOException {
+  public void finish(FieldInfos fis, int numDocs) throws IOException {
     if (4+((long) numDocs)*16 != tvx.getFilePointer())
       // This is most likely a bug in Sun JRE 1.6.0_04/_05;
       // we detect that the bug has struck, here, and


diff -ruN -x .svn -x build lucene-trunk/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene3x/TermInfosWriter.java lucene4055/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene3x/TermInfosWriter.java
--- lucene-trunk/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene3x/TermInfosWriter.java	2012-05-24 16:55:50.472233471 -0400
+++ lucene4055/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene3x/TermInfosWriter.java	2012-05-22 17:10:19.129239381 -0400
@@ -21,6 +21,7 @@
 import java.io.Closeable;
 import java.io.IOException;
 
+import org.apache.lucene.index.FieldInfo;
 import org.apache.lucene.index.FieldInfos;
 import org.apache.lucene.index.IndexFileNames;
 import org.apache.lucene.store.Directory;
@@ -154,12 +155,18 @@
     utf16Result2 = new CharsRef(10);
     return true;
   }
+  
+  /** note: -1 is the empty field: "" !!!! */
+  static String fieldName(FieldInfos infos, int fieldNumber) {
+    FieldInfo fi = infos.fieldInfo(fieldNumber);
+    return (fi != null) ? fi.name : "";
+  }
 
   // Currently used only by assert statement
   private int compareToLastTerm(int fieldNumber, BytesRef term) {
 
     if (lastFieldNumber != fieldNumber) {
-      final int cmp = fieldInfos.fieldName(lastFieldNumber).compareTo(fieldInfos.fieldName(fieldNumber));
+      final int cmp = fieldName(fieldInfos, lastFieldNumber).compareTo(fieldName(fieldInfos, fieldNumber));
       // If there is a field named "" (empty string) then we
       // will get 0 on this comparison, yet, it's "OK".  But
       // it's not OK if two different field numbers map to
@@ -203,8 +210,8 @@
 
     assert compareToLastTerm(fieldNumber, term) < 0 ||
       (isIndex && term.length == 0 && lastTerm.length == 0) :
-      "Terms are out of order: field=" + fieldInfos.fieldName(fieldNumber) + " (number " + fieldNumber + ")" +
-        " lastField=" + fieldInfos.fieldName(lastFieldNumber) + " (number " + lastFieldNumber + ")" +
+      "Terms are out of order: field=" + fieldName(fieldInfos, fieldNumber) + " (number " + fieldNumber + ")" +
+        " lastField=" + fieldName(fieldInfos, lastFieldNumber) + " (number " + lastFieldNumber + ")" +
         " text=" + term.utf8ToString() + " lastText=" + lastTerm.utf8ToString();
 
     assert ti.freqPointer >= lastTi.freqPointer: "freqPointer out of order (" + ti.freqPointer + " < " + lastTi.freqPointer + ")";


diff -ruN -x .svn -x build lucene-trunk/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene40ords/Lucene40WithOrds.java lucene4055/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene40ords/Lucene40WithOrds.java
--- lucene-trunk/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene40ords/Lucene40WithOrds.java	2012-05-24 16:55:50.476233472 -0400
+++ lucene4055/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene40ords/Lucene40WithOrds.java	2012-05-24 12:06:55.763931598 -0400
@@ -18,7 +18,6 @@
  */
 
 import java.io.IOException;
-import java.util.Set;
 
 import org.apache.lucene.codecs.BlockTermsReader;
 import org.apache.lucene.codecs.BlockTermsWriter;
@@ -33,7 +32,6 @@
 import org.apache.lucene.codecs.TermsIndexWriterBase;
 import org.apache.lucene.codecs.lucene40.Lucene40PostingsReader;
 import org.apache.lucene.codecs.lucene40.Lucene40PostingsWriter;
-import org.apache.lucene.index.SegmentInfo;
 import org.apache.lucene.index.SegmentReadState;
 import org.apache.lucene.index.SegmentWriteState;
 import org.apache.lucene.util.BytesRef;
@@ -88,7 +86,7 @@
 
   @Override
   public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {
-    PostingsReaderBase postings = new Lucene40PostingsReader(state.dir, state.segmentInfo, state.context, state.segmentSuffix);
+    PostingsReaderBase postings = new Lucene40PostingsReader(state.dir, state.fieldInfos, state.segmentInfo, state.context, state.segmentSuffix);
     TermsIndexReaderBase indexReader;
 
     boolean success = false;
@@ -134,11 +132,4 @@
 
   /** Extension of prox postings file */
   static final String PROX_EXTENSION = "prx";
-
-  @Override
-  public void files(SegmentInfo segmentInfo, String segmentSuffix, Set<String> files) throws IOException {
-    Lucene40PostingsReader.files(segmentInfo, segmentSuffix, files);
-    BlockTermsReader.files(segmentInfo, segmentSuffix, files);
-    FixedGapTermsIndexReader.files(segmentInfo, segmentSuffix, files);
-  }
 }


diff -ruN -x .svn -x build lucene-trunk/lucene/test-framework/src/java/org/apache/lucene/codecs/mockintblock/MockFixedIntBlockPostingsFormat.java lucene4055/lucene/test-framework/src/java/org/apache/lucene/codecs/mockintblock/MockFixedIntBlockPostingsFormat.java
--- lucene-trunk/lucene/test-framework/src/java/org/apache/lucene/codecs/mockintblock/MockFixedIntBlockPostingsFormat.java	2012-05-24 16:55:50.456233471 -0400
+++ lucene4055/lucene/test-framework/src/java/org/apache/lucene/codecs/mockintblock/MockFixedIntBlockPostingsFormat.java	2012-05-24 12:07:05.679931771 -0400
@@ -18,7 +18,6 @@
  */
 
 import java.io.IOException;
-import java.util.Set;
 
 import org.apache.lucene.codecs.BlockTermsReader;
 import org.apache.lucene.codecs.BlockTermsWriter;
@@ -38,7 +37,6 @@
 import org.apache.lucene.codecs.sep.IntStreamFactory;
 import org.apache.lucene.codecs.sep.SepPostingsReader;
 import org.apache.lucene.codecs.sep.SepPostingsWriter;
-import org.apache.lucene.index.SegmentInfo;
 import org.apache.lucene.index.SegmentWriteState;
 import org.apache.lucene.index.SegmentReadState;
 import org.apache.lucene.store.*;
@@ -156,6 +154,7 @@
   @Override
   public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {
     PostingsReaderBase postingsReader = new SepPostingsReader(state.dir,
+                                                              state.fieldInfos,
                                                               state.segmentInfo,
                                                               state.context,
                                                               new MockIntFactory(blockSize), state.segmentSuffix);
@@ -198,11 +197,4 @@
       }
     }
   }
-
-  @Override
-  public void files(SegmentInfo segmentInfo, String segmentSuffix, Set<String> files) throws IOException {
-    SepPostingsReader.files(segmentInfo, segmentSuffix, files);
-    BlockTermsReader.files(segmentInfo, segmentSuffix, files);
-    FixedGapTermsIndexReader.files(segmentInfo, segmentSuffix, files);
-  }
 }


diff -ruN -x .svn -x build lucene-trunk/lucene/test-framework/src/java/org/apache/lucene/codecs/mockintblock/MockVariableIntBlockPostingsFormat.java lucene4055/lucene/test-framework/src/java/org/apache/lucene/codecs/mockintblock/MockVariableIntBlockPostingsFormat.java
--- lucene-trunk/lucene/test-framework/src/java/org/apache/lucene/codecs/mockintblock/MockVariableIntBlockPostingsFormat.java	2012-05-24 16:55:50.456233471 -0400
+++ lucene4055/lucene/test-framework/src/java/org/apache/lucene/codecs/mockintblock/MockVariableIntBlockPostingsFormat.java	2012-05-24 12:07:12.739931890 -0400
@@ -18,7 +18,6 @@
  */
 
 import java.io.IOException;
-import java.util.Set;
 
 import org.apache.lucene.codecs.BlockTermsReader;
 import org.apache.lucene.codecs.BlockTermsWriter;
@@ -38,7 +37,6 @@
 import org.apache.lucene.codecs.sep.IntStreamFactory;
 import org.apache.lucene.codecs.sep.SepPostingsReader;
 import org.apache.lucene.codecs.sep.SepPostingsWriter;
-import org.apache.lucene.index.SegmentInfo;
 import org.apache.lucene.index.SegmentWriteState;
 import org.apache.lucene.index.SegmentReadState;
 import org.apache.lucene.store.Directory;
@@ -179,6 +177,7 @@
   @Override
   public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {
     PostingsReaderBase postingsReader = new SepPostingsReader(state.dir,
+                                                              state.fieldInfos,
                                                               state.segmentInfo,
                                                               state.context,
                                                               new MockIntFactory(baseBlockSize), state.segmentSuffix);
@@ -221,11 +220,4 @@
       }
     }
   }
-
-  @Override
-  public void files(SegmentInfo segmentInfo, String segmentSuffix, Set<String> files) throws IOException {
-    SepPostingsReader.files(segmentInfo, segmentSuffix, files);
-    BlockTermsReader.files(segmentInfo, segmentSuffix, files);
-    FixedGapTermsIndexReader.files(segmentInfo, segmentSuffix, files);
-  }
 }


diff -ruN -x .svn -x build lucene-trunk/lucene/test-framework/src/java/org/apache/lucene/codecs/mockrandom/MockRandomPostingsFormat.java lucene4055/lucene/test-framework/src/java/org/apache/lucene/codecs/mockrandom/MockRandomPostingsFormat.java
--- lucene-trunk/lucene/test-framework/src/java/org/apache/lucene/codecs/mockrandom/MockRandomPostingsFormat.java	2012-05-24 16:55:50.476233472 -0400
+++ lucene4055/lucene/test-framework/src/java/org/apache/lucene/codecs/mockrandom/MockRandomPostingsFormat.java	2012-05-24 12:07:25.463932117 -0400
@@ -19,10 +19,8 @@
 
 import java.io.IOException;
 import java.util.ArrayList;
-import java.util.Iterator;
 import java.util.List;
 import java.util.Random;
-import java.util.Set;
 
 import org.apache.lucene.codecs.BlockTermsReader;
 import org.apache.lucene.codecs.BlockTermsWriter;
@@ -54,7 +52,6 @@
 import org.apache.lucene.codecs.sep.SepPostingsWriter;
 import org.apache.lucene.index.FieldInfo;
 import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.SegmentInfo;
 import org.apache.lucene.index.SegmentReadState;
 import org.apache.lucene.index.SegmentWriteState;
 import org.apache.lucene.store.Directory;
@@ -138,10 +135,10 @@
     final long seed = seedRandom.nextLong();
 
     if (LuceneTestCase.VERBOSE) {
-      System.out.println("MockRandomCodec: writing to seg=" + state.segmentName + " formatID=" + state.segmentSuffix + " seed=" + seed);
+      System.out.println("MockRandomCodec: writing to seg=" + state.segmentInfo.name + " formatID=" + state.segmentSuffix + " seed=" + seed);
     }
 
-    final String seedFileName = IndexFileNames.segmentFileName(state.segmentName, state.segmentSuffix, SEED_EXT);
+    final String seedFileName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, SEED_EXT);
     final IndexOutput out = state.directory.createOutput(seedFileName, state.context);
     try {
       out.writeLong(seed);
@@ -293,13 +290,13 @@
       if (LuceneTestCase.VERBOSE) {
         System.out.println("MockRandomCodec: reading Sep postings");
       }
-      postingsReader = new SepPostingsReader(state.dir, state.segmentInfo,
+      postingsReader = new SepPostingsReader(state.dir, state.fieldInfos, state.segmentInfo,
                                              state.context, new MockIntStreamFactory(random), state.segmentSuffix);
     } else {
       if (LuceneTestCase.VERBOSE) {
         System.out.println("MockRandomCodec: reading Standard postings");
       }
-      postingsReader = new Lucene40PostingsReader(state.dir, state.segmentInfo, state.context, state.segmentSuffix);
+      postingsReader = new Lucene40PostingsReader(state.dir, state.fieldInfos, state.segmentInfo, state.context, state.segmentSuffix);
     }
 
     if (random.nextBoolean()) {
@@ -411,25 +408,4 @@
 
     return fields;
   }
-
-  @Override
-  public void files(SegmentInfo segmentInfo, String segmentSuffix, Set<String> files) throws IOException {
-    final String seedFileName = IndexFileNames.segmentFileName(segmentInfo.name, segmentSuffix, SEED_EXT);    
-    files.add(seedFileName);
-    SepPostingsReader.files(segmentInfo, segmentSuffix, files);
-    Lucene40PostingsReader.files(segmentInfo, segmentSuffix, files);
-    BlockTermsReader.files(segmentInfo, segmentSuffix, files);
-    BlockTreeTermsReader.files(segmentInfo, segmentSuffix, files);
-    FixedGapTermsIndexReader.files(segmentInfo, segmentSuffix, files);
-    VariableGapTermsIndexReader.files(segmentInfo, segmentSuffix, files);
-    // hackish!
-    Iterator<String> it = files.iterator();
-    while(it.hasNext()) {
-      final String file = it.next();
-      if (!segmentInfo.dir.fileExists(file)) {
-        it.remove();
-      }
-    }
-    //System.out.println("MockRandom.files return " + files);
-  }
 }


diff -ruN -x .svn -x build lucene-trunk/lucene/test-framework/src/java/org/apache/lucene/codecs/mocksep/MockSepDocValuesFormat.java lucene4055/lucene/test-framework/src/java/org/apache/lucene/codecs/mocksep/MockSepDocValuesFormat.java
--- lucene-trunk/lucene/test-framework/src/java/org/apache/lucene/codecs/mocksep/MockSepDocValuesFormat.java	2012-05-24 16:55:50.460233471 -0400
+++ lucene4055/lucene/test-framework/src/java/org/apache/lucene/codecs/mocksep/MockSepDocValuesFormat.java	2012-05-24 12:07:34.063932264 -0400
@@ -18,7 +18,6 @@
  */
 
 import java.io.IOException;
-import java.util.Set;
 
 import org.apache.lucene.codecs.DocValuesFormat;
 import org.apache.lucene.codecs.PerDocConsumer;
@@ -26,7 +25,6 @@
 import org.apache.lucene.codecs.sep.SepDocValuesConsumer;
 import org.apache.lucene.codecs.sep.SepDocValuesProducer;
 import org.apache.lucene.index.PerDocWriteState;
-import org.apache.lucene.index.SegmentInfo;
 import org.apache.lucene.index.SegmentReadState;
 
 /**
@@ -45,9 +43,4 @@
   public PerDocProducer docsProducer(SegmentReadState state) throws IOException {
     return new SepDocValuesProducer(state);
   }
-
-  @Override
-  public void files(SegmentInfo info, Set<String> files) throws IOException {
-    SepDocValuesConsumer.files(info, files);
-  }
 }


diff -ruN -x .svn -x build lucene-trunk/lucene/test-framework/src/java/org/apache/lucene/codecs/mocksep/MockSepPostingsFormat.java lucene4055/lucene/test-framework/src/java/org/apache/lucene/codecs/mocksep/MockSepPostingsFormat.java
--- lucene-trunk/lucene/test-framework/src/java/org/apache/lucene/codecs/mocksep/MockSepPostingsFormat.java	2012-05-24 16:55:50.460233471 -0400
+++ lucene4055/lucene/test-framework/src/java/org/apache/lucene/codecs/mocksep/MockSepPostingsFormat.java	2012-05-24 12:07:41.975932402 -0400
@@ -18,7 +18,6 @@
  */
 
 import java.io.IOException;
-import java.util.Set;
 
 import org.apache.lucene.codecs.BlockTermsReader;
 import org.apache.lucene.codecs.BlockTermsWriter;
@@ -33,7 +32,6 @@
 import org.apache.lucene.codecs.TermsIndexWriterBase;
 import org.apache.lucene.codecs.sep.SepPostingsReader;
 import org.apache.lucene.codecs.sep.SepPostingsWriter;
-import org.apache.lucene.index.SegmentInfo;
 import org.apache.lucene.index.SegmentWriteState;
 import org.apache.lucene.index.SegmentReadState;
 import org.apache.lucene.util.BytesRef;
@@ -85,7 +83,7 @@
   @Override
   public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {
 
-    PostingsReaderBase postingsReader = new SepPostingsReader(state.dir, state.segmentInfo,
+    PostingsReaderBase postingsReader = new SepPostingsReader(state.dir, state.fieldInfos, state.segmentInfo,
         state.context, new MockSingleIntFactory(), state.segmentSuffix);
 
     TermsIndexReaderBase indexReader;
@@ -126,11 +124,4 @@
       }
     }
   }
-
-  @Override
-  public void files(SegmentInfo segmentInfo, String segmentSuffix, Set<String> files) throws IOException {
-    SepPostingsReader.files(segmentInfo, segmentSuffix, files);
-    BlockTermsReader.files(segmentInfo, segmentSuffix, files);
-    FixedGapTermsIndexReader.files(segmentInfo, segmentSuffix, files);
-  }
 }


diff -ruN -x .svn -x build lucene-trunk/lucene/test-framework/src/java/org/apache/lucene/codecs/nestedpulsing/NestedPulsingPostingsFormat.java lucene4055/lucene/test-framework/src/java/org/apache/lucene/codecs/nestedpulsing/NestedPulsingPostingsFormat.java
--- lucene-trunk/lucene/test-framework/src/java/org/apache/lucene/codecs/nestedpulsing/NestedPulsingPostingsFormat.java	2012-05-24 16:55:50.464233471 -0400
+++ lucene4055/lucene/test-framework/src/java/org/apache/lucene/codecs/nestedpulsing/NestedPulsingPostingsFormat.java	2012-05-24 12:07:52.175932580 -0400
@@ -18,7 +18,6 @@
  */
 
 import java.io.IOException;
-import java.util.Set;
 
 import org.apache.lucene.codecs.BlockTreeTermsReader;
 import org.apache.lucene.codecs.BlockTreeTermsWriter;
@@ -31,7 +30,6 @@
 import org.apache.lucene.codecs.lucene40.Lucene40PostingsWriter;
 import org.apache.lucene.codecs.pulsing.PulsingPostingsReader;
 import org.apache.lucene.codecs.pulsing.PulsingPostingsWriter;
-import org.apache.lucene.index.SegmentInfo;
 import org.apache.lucene.index.SegmentReadState;
 import org.apache.lucene.index.SegmentWriteState;
 
@@ -70,7 +68,7 @@
 
   @Override
   public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {
-    PostingsReaderBase docsReader = new Lucene40PostingsReader(state.dir, state.segmentInfo, state.context, state.segmentSuffix);
+    PostingsReaderBase docsReader = new Lucene40PostingsReader(state.dir, state.fieldInfos, state.segmentInfo, state.context, state.segmentSuffix);
     PostingsReaderBase pulsingReaderInner = new PulsingPostingsReader(docsReader);
     PostingsReaderBase pulsingReader = new PulsingPostingsReader(pulsingReaderInner);
     boolean success = false;
@@ -89,10 +87,4 @@
       }
     }
   }
-
-  @Override
-  public void files(SegmentInfo segmentInfo, String segmentSuffix, Set<String> files) throws IOException {
-    Lucene40PostingsReader.files(segmentInfo, segmentSuffix, files);
-    BlockTreeTermsReader.files(segmentInfo, segmentSuffix, files);
-  }
 }


diff -ruN -x .svn -x build lucene-trunk/lucene/test-framework/src/java/org/apache/lucene/codecs/ramonly/RAMOnlyPostingsFormat.java lucene4055/lucene/test-framework/src/java/org/apache/lucene/codecs/ramonly/RAMOnlyPostingsFormat.java
--- lucene-trunk/lucene/test-framework/src/java/org/apache/lucene/codecs/ramonly/RAMOnlyPostingsFormat.java	2012-05-24 16:55:50.460233471 -0400
+++ lucene4055/lucene/test-framework/src/java/org/apache/lucene/codecs/ramonly/RAMOnlyPostingsFormat.java	2012-05-24 12:08:02.015932751 -0400
@@ -24,7 +24,6 @@
 import java.util.Iterator;
 import java.util.List;
 import java.util.Map;
-import java.util.Set;
 import java.util.SortedMap;
 import java.util.TreeMap;
 import java.util.concurrent.atomic.AtomicInteger;
@@ -41,7 +40,6 @@
 import org.apache.lucene.index.FieldInfo;
 import org.apache.lucene.index.FieldsEnum;
 import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.SegmentInfo;
 import org.apache.lucene.index.SegmentReadState;
 import org.apache.lucene.index.SegmentWriteState;
 import org.apache.lucene.index.Terms;
@@ -197,7 +195,7 @@
 
     @Override
     public TermsConsumer addField(FieldInfo field) {
-      if (field.indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0) {
+      if (field.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0) {
         throw new UnsupportedOperationException("this codec cannot index offsets");
       }
       RAMField ramField = new RAMField(field.name);
@@ -540,7 +538,7 @@
     // TODO -- ok to do this up front instead of
     // on close....?  should be ok?
     // Write our ID:
-    final String idFileName = IndexFileNames.segmentFileName(writeState.segmentName, writeState.segmentSuffix, ID_EXTENSION);
+    final String idFileName = IndexFileNames.segmentFileName(writeState.segmentInfo.name, writeState.segmentSuffix, ID_EXTENSION);
     IndexOutput out = writeState.directory.createOutput(idFileName, writeState.context);
     boolean success = false;
     try {
@@ -589,10 +587,4 @@
       return state.get(id);
     }
   }
-
-  @Override
-  public void files(SegmentInfo segmentInfo, String segmentSuffix, Set<String> files) {
-    final String idFileName = IndexFileNames.segmentFileName(segmentInfo.name, segmentSuffix, ID_EXTENSION);
-    files.add(idFileName);
-  }
 }


diff -ruN -x .svn -x build lucene-trunk/lucene/test-framework/src/java/org/apache/lucene/index/AlcoholicMergePolicy.java lucene4055/lucene/test-framework/src/java/org/apache/lucene/index/AlcoholicMergePolicy.java
--- lucene-trunk/lucene/test-framework/src/java/org/apache/lucene/index/AlcoholicMergePolicy.java	2012-05-24 16:55:50.536233473 -0400
+++ lucene4055/lucene/test-framework/src/java/org/apache/lucene/index/AlcoholicMergePolicy.java	2012-05-22 12:04:43.352920074 -0400
@@ -52,7 +52,7 @@
   
   @Override
   //@BlackMagic(level=Voodoo);
-  protected long size(SegmentInfo info) throws IOException {
+  protected long size(SegmentInfoPerCommit info) throws IOException {
     int hourOfDay = calendar.get(Calendar.HOUR_OF_DAY);
     if (hourOfDay < 6 || 
         hourOfDay > 20 || 


diff -ruN -x .svn -x build lucene-trunk/lucene/test-framework/src/java/org/apache/lucene/index/DocHelper.java lucene4055/lucene/test-framework/src/java/org/apache/lucene/index/DocHelper.java
--- lucene-trunk/lucene/test-framework/src/java/org/apache/lucene/index/DocHelper.java	2012-05-24 16:55:50.536233473 -0400
+++ lucene4055/lucene/test-framework/src/java/org/apache/lucene/index/DocHelper.java	2012-05-22 12:04:43.352920074 -0400
@@ -261,7 +261,7 @@
    * @param doc
    * @throws IOException
    */ 
-  public static SegmentInfo writeDoc(Random random, Directory dir, Document doc) throws IOException
+  public static SegmentInfoPerCommit writeDoc(Random random, Directory dir, Document doc) throws IOException
   {
     return writeDoc(random, dir, new MockAnalyzer(random, MockTokenizer.WHITESPACE, false), null, doc);
   }
@@ -276,13 +276,13 @@
    * @param doc
    * @throws IOException
    */ 
-  public static SegmentInfo writeDoc(Random random, Directory dir, Analyzer analyzer, Similarity similarity, Document doc) throws IOException {
+  public static SegmentInfoPerCommit writeDoc(Random random, Directory dir, Analyzer analyzer, Similarity similarity, Document doc) throws IOException {
     IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig( /* LuceneTestCase.newIndexWriterConfig(random, */ 
         TEST_VERSION_CURRENT, analyzer).setSimilarity(similarity));
     //writer.setUseCompoundFile(false);
     writer.addDocument(doc);
     writer.commit();
-    SegmentInfo info = writer.newestSegment();
+    SegmentInfoPerCommit info = writer.newestSegment();
     writer.close();
     return info;
   }


diff -ruN -x .svn -x build lucene-trunk/lucene/test-framework/src/java/org/apache/lucene/index/FieldFilterAtomicReader.java lucene4055/lucene/test-framework/src/java/org/apache/lucene/index/FieldFilterAtomicReader.java
--- lucene-trunk/lucene/test-framework/src/java/org/apache/lucene/index/FieldFilterAtomicReader.java	2012-05-24 16:55:50.536233473 -0400
+++ lucene4055/lucene/test-framework/src/java/org/apache/lucene/index/FieldFilterAtomicReader.java	2012-05-21 13:58:05.615533923 -0400
@@ -18,6 +18,7 @@
  */
 
 import java.io.IOException;
+import java.util.ArrayList;
 import java.util.Set;
 
 import org.apache.lucene.index.FilterAtomicReader;
@@ -32,12 +33,13 @@
     super(in);
     this.fields = fields;
     this.negate = negate;
-    this.fieldInfos = new FieldInfos();
+    ArrayList<FieldInfo> filteredInfos = new ArrayList<FieldInfo>();
     for (FieldInfo fi : in.getFieldInfos()) {
       if (hasField(fi.name)) {
-        fieldInfos.add(fi);
+        filteredInfos.add(fi);
       }
     }
+    fieldInfos = new FieldInfos(filteredInfos.toArray(new FieldInfo[filteredInfos.size()]));
   }
   
   boolean hasField(String field) {


diff -ruN -x .svn -x build lucene-trunk/lucene/test-framework/src/java/org/apache/lucene/index/MockRandomMergePolicy.java lucene4055/lucene/test-framework/src/java/org/apache/lucene/index/MockRandomMergePolicy.java
--- lucene-trunk/lucene/test-framework/src/java/org/apache/lucene/index/MockRandomMergePolicy.java	2012-05-24 16:55:50.536233473 -0400
+++ lucene4055/lucene/test-framework/src/java/org/apache/lucene/index/MockRandomMergePolicy.java	2012-05-22 12:04:43.352920074 -0400
@@ -45,7 +45,7 @@
 
     if (segmentInfos.size() > 1 && random.nextInt(5) == 3) {
       
-      List<SegmentInfo> segments = new ArrayList<SegmentInfo>(segmentInfos.asList());
+      List<SegmentInfoPerCommit> segments = new ArrayList<SegmentInfoPerCommit>(segmentInfos.asList());
       Collections.shuffle(segments, random);
 
       // TODO: sometimes make more than 1 merge?
@@ -59,11 +59,11 @@
 
   @Override
   public MergeSpecification findForcedMerges(
-       SegmentInfos segmentInfos, int maxSegmentCount, Map<SegmentInfo,Boolean> segmentsToMerge)
+       SegmentInfos segmentInfos, int maxSegmentCount, Map<SegmentInfoPerCommit,Boolean> segmentsToMerge)
     throws CorruptIndexException, IOException {
 
-    final List<SegmentInfo> eligibleSegments = new ArrayList<SegmentInfo>();
-    for(SegmentInfo info : segmentInfos) {
+    final List<SegmentInfoPerCommit> eligibleSegments = new ArrayList<SegmentInfoPerCommit>();
+    for(SegmentInfoPerCommit info : segmentInfos) {
       if (segmentsToMerge.containsKey(info)) {
         eligibleSegments.add(info);
       }
@@ -87,7 +87,7 @@
 
     if (mergeSpec != null) {
       for(OneMerge merge : mergeSpec.merges) {
-        for(SegmentInfo info : merge.segments) {
+        for(SegmentInfoPerCommit info : merge.segments) {
           assert segmentsToMerge.containsKey(info);
         }
       }
@@ -107,7 +107,7 @@
   }
 
   @Override
-  public boolean useCompoundFile(SegmentInfos infos, SegmentInfo mergedInfo) throws IOException {
+  public boolean useCompoundFile(SegmentInfos infos, SegmentInfoPerCommit mergedInfo) throws IOException {
     // 80% of the time we create CFS:
     return random.nextInt(5) != 1;
   }


diff -ruN -x .svn -x build lucene-trunk/lucene/test-framework/src/java/org/apache/lucene/store/MockDirectoryWrapper.java lucene4055/lucene/test-framework/src/java/org/apache/lucene/store/MockDirectoryWrapper.java
--- lucene-trunk/lucene/test-framework/src/java/org/apache/lucene/store/MockDirectoryWrapper.java	2012-05-24 16:55:50.548233473 -0400
+++ lucene4055/lucene/test-framework/src/java/org/apache/lucene/store/MockDirectoryWrapper.java	2012-05-21 13:58:05.623533924 -0400
@@ -169,8 +169,9 @@
   public synchronized void sync(Collection<String> names) throws IOException {
     maybeYield();
     maybeThrowDeterministicException();
-    if (crashed)
+    if (crashed) {
       throw new IOException("cannot sync after crash");
+    }
     unSyncedFiles.removeAll(names);
     if (LuceneTestCase.rarely(randomState) || delegate instanceof NRTCachingDirectory) {
       // don't wear out our hardware so much in tests.
@@ -506,8 +507,9 @@
     if (failOnOpenInput) {
       maybeThrowDeterministicException();
     }
-    if (!delegate.fileExists(name))
-      throw new FileNotFoundException(name);
+    if (!delegate.fileExists(name)) {
+      throw new FileNotFoundException(name + " in dir=" + delegate);
+    }
 
     // cannot open a file for input if it's still open for
     // output, except for segments.gen and segments_N
@@ -784,8 +786,9 @@
   public IndexInputSlicer createSlicer(final String name, IOContext context)
       throws IOException {
     maybeYield();
-    if (!delegate.fileExists(name))
+    if (!delegate.fileExists(name)) {
       throw new FileNotFoundException(name);
+    }
     // cannot open a file for input if it's still open for
     // output, except for segments.gen and segments_N
     if (openFilesForWrite.contains(name) && !name.startsWith("segments")) {


diff -ruN -x .svn -x build lucene-trunk/lucene/test-framework/src/java/org/apache/lucene/util/TestRuleSetupAndRestoreClassEnv.java lucene4055/lucene/test-framework/src/java/org/apache/lucene/util/TestRuleSetupAndRestoreClassEnv.java
--- lucene-trunk/lucene/test-framework/src/java/org/apache/lucene/util/TestRuleSetupAndRestoreClassEnv.java	2012-05-24 16:55:50.608233474 -0400
+++ lucene4055/lucene/test-framework/src/java/org/apache/lucene/util/TestRuleSetupAndRestoreClassEnv.java	2012-05-21 13:58:05.651533924 -0400
@@ -1,6 +1,7 @@
 package org.apache.lucene.util;
 
 import java.util.Arrays;
+import java.util.Date;
 import java.util.HashMap;
 import java.util.HashSet;
 import java.util.Locale;
@@ -20,14 +21,12 @@
 import org.apache.lucene.search.similarities.DefaultSimilarity;
 import org.apache.lucene.search.similarities.Similarity;
 import org.apache.lucene.util.LuceneTestCase.SuppressCodecs;
-
 import com.carrotsearch.randomizedtesting.RandomizedContext;
 
-import static org.apache.lucene.util.LuceneTestCase.VERBOSE;
+import static org.apache.lucene.util.LuceneTestCase.*;
 import static org.apache.lucene.util.LuceneTestCase.INFOSTREAM;
 import static org.apache.lucene.util.LuceneTestCase.TEST_CODEC;
-
-import static org.apache.lucene.util.LuceneTestCase.*;
+import static org.apache.lucene.util.LuceneTestCase.VERBOSE;
 
 
 
@@ -126,7 +125,20 @@
     final Random random = RandomizedContext.current().getRandom();
     final boolean v = random.nextBoolean();
     if (INFOSTREAM) {
-      InfoStream.setDefault(new PrintStreamInfoStream(System.out));
+      InfoStream.setDefault(new PrintStreamInfoStream(System.out) {
+          @Override
+          public void message(String component, String message) {
+            final String name;
+            if (Thread.currentThread().getName().startsWith("TEST-")) {
+              // The name of the main thread is way too
+              // long when looking at IW verbose output...:
+              name = "main";
+            } else {
+              name = Thread.currentThread().getName();
+            }
+            stream.println(component + " " + messageID + " [" + new Date() + "; " + name + "]: " + message);    
+          }
+        });
     } else if (v) {
       InfoStream.setDefault(new NullInfoStream());
     }


diff -ruN -x .svn -x build lucene-trunk/lucene/test-framework/src/java/org/apache/lucene/util/_TestUtil.java lucene4055/lucene/test-framework/src/java/org/apache/lucene/util/_TestUtil.java
--- lucene-trunk/lucene/test-framework/src/java/org/apache/lucene/util/_TestUtil.java	2012-05-24 16:55:50.608233474 -0400
+++ lucene4055/lucene/test-framework/src/java/org/apache/lucene/util/_TestUtil.java	2012-05-22 16:30:03.993197321 -0400
@@ -55,6 +55,7 @@
 import org.apache.lucene.index.DocsAndPositionsEnum;
 import org.apache.lucene.index.DocsEnum;
 import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.IndexFileNames;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexWriter;
 import org.apache.lucene.index.IndexableField;
@@ -62,13 +63,16 @@
 import org.apache.lucene.index.MergePolicy;
 import org.apache.lucene.index.MergeScheduler;
 import org.apache.lucene.index.MultiFields;
+import org.apache.lucene.index.SegmentInfo;
 import org.apache.lucene.index.Terms;
 import org.apache.lucene.index.TermsEnum;
 import org.apache.lucene.index.TieredMergePolicy;
 import org.apache.lucene.search.FieldDoc;
 import org.apache.lucene.search.ScoreDoc;
 import org.apache.lucene.search.TopDocs;
+import org.apache.lucene.store.CompoundFileDirectory;
 import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
 import org.junit.Assert;
 
 /**
@@ -678,13 +682,6 @@
     }
   }
   
-  /** Adds field info for a Document. */
-  public static void add(Document doc, FieldInfos fieldInfos) {
-    for (IndexableField field : doc) {
-      fieldInfos.addOrUpdate(field.name(), field.fieldType());
-    }
-  }
-  
   /** 
    * insecure, fast version of File.createTempFile
    * uses Random instead of SecureRandom.
@@ -900,4 +897,25 @@
       }
     }
   }
+
+  public static FieldInfos getFieldInfos(SegmentInfo info) throws IOException {
+    Directory cfsDir = null;
+    try {
+      if (info.getUseCompoundFile()) {
+        cfsDir = new CompoundFileDirectory(info.dir,
+                                           IndexFileNames.segmentFileName(info.name, "", IndexFileNames.COMPOUND_FILE_EXTENSION),
+                                           IOContext.READONCE,
+                                           false);
+      } else {
+        cfsDir = info.dir;
+      }
+      return info.getCodec().fieldInfosFormat().getFieldInfosReader().read(cfsDir,
+                                                                           info.name,
+                                                                           IOContext.READONCE);
+    } finally {
+      if (info.getUseCompoundFile() && cfsDir != null) {
+        cfsDir.close();
+      }
+    }
+  }
 }


diff -ruN -x .svn -x build lucene-trunk/solr/CHANGES.txt lucene4055/solr/CHANGES.txt
--- lucene-trunk/solr/CHANGES.txt	2012-05-24 16:55:34.792233198 -0400
+++ lucene4055/solr/CHANGES.txt	2012-05-23 12:24:32.290445387 -0400
@@ -18,7 +18,7 @@
 See the tutorial at http://lucene.apache.org/solr/tutorial.html
 
 
-$Id: CHANGES.txt 1341894 2012-05-23 15:27:29Z jdyer $
+$Id: CHANGES.txt 1341936 2012-05-23 16:24:21Z rmuir $
 
 ==================  4.0.0-dev ==================
 More information about this release, including any errata related to the 


diff -ruN -x .svn -x build lucene-trunk/solr/contrib/clustering/src/java/org/apache/solr/handler/clustering/ClusteringComponent.java lucene4055/solr/contrib/clustering/src/java/org/apache/solr/handler/clustering/ClusteringComponent.java
--- lucene-trunk/solr/contrib/clustering/src/java/org/apache/solr/handler/clustering/ClusteringComponent.java	2012-05-24 16:55:36.336233224 -0400
+++ lucene4055/solr/contrib/clustering/src/java/org/apache/solr/handler/clustering/ClusteringComponent.java	2012-05-21 13:57:51.271533674 -0400
@@ -256,6 +256,6 @@
 
   @Override
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/contrib/clustering/src/java/org/apache/solr/handler/clustering/ClusteringComponent.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene4055/solr/contrib/clustering/src/java/org/apache/solr/handler/clustering/ClusteringComponent.java $";
   }
 }


diff -ruN -x .svn -x build lucene-trunk/solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/DataImportHandler.java lucene4055/solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/DataImportHandler.java
--- lucene-trunk/solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/DataImportHandler.java	2012-05-24 16:55:36.644233232 -0400
+++ lucene4055/solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/DataImportHandler.java	2012-05-21 13:57:51.559533678 -0400
@@ -340,7 +340,7 @@
 
   @Override
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/DataImportHandler.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene4055/solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/DataImportHandler.java $";
   }
 
   public static final String ENABLE_DEBUG = "enableDebug";


diff -ruN -x .svn -x build lucene-trunk/solr/contrib/extraction/src/java/org/apache/solr/handler/extraction/ExtractingRequestHandler.java lucene4055/solr/contrib/extraction/src/java/org/apache/solr/handler/extraction/ExtractingRequestHandler.java
--- lucene-trunk/solr/contrib/extraction/src/java/org/apache/solr/handler/extraction/ExtractingRequestHandler.java	2012-05-24 16:55:36.476233228 -0400
+++ lucene4055/solr/contrib/extraction/src/java/org/apache/solr/handler/extraction/ExtractingRequestHandler.java	2012-05-21 13:57:51.407533676 -0400
@@ -124,7 +124,7 @@
 
   @Override
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/contrib/extraction/src/java/org/apache/solr/handler/extraction/ExtractingRequestHandler.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene4055/solr/contrib/extraction/src/java/org/apache/solr/handler/extraction/ExtractingRequestHandler.java $";
   }
 }
 


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/core/RequestHandlers.java lucene4055/solr/core/src/java/org/apache/solr/core/RequestHandlers.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/core/RequestHandlers.java	2012-05-24 16:55:35.336233208 -0400
+++ lucene4055/solr/core/src/java/org/apache/solr/core/RequestHandlers.java	2012-05-21 13:57:50.051533652 -0400
@@ -286,7 +286,7 @@
     }
 
     public String getSource() {
-      String rev = "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/core/RequestHandlers.java $";
+      String rev = "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene4055/solr/core/src/java/org/apache/solr/core/RequestHandlers.java $";
       if( _handler != null ) {
         rev += "\n" + _handler.getSource();
       }


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/core/SolrCore.java lucene4055/solr/core/src/java/org/apache/solr/core/SolrCore.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/core/SolrCore.java	2012-05-24 16:55:35.344233208 -0400
+++ lucene4055/solr/core/src/java/org/apache/solr/core/SolrCore.java	2012-05-21 13:57:50.055533652 -0400
@@ -1922,7 +1922,7 @@
   }
 
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/core/SolrCore.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene4055/solr/core/src/java/org/apache/solr/core/SolrCore.java $";
   }
 
   public URL[] getDocs() {


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/handler/admin/AdminHandlers.java lucene4055/solr/core/src/java/org/apache/solr/handler/admin/AdminHandlers.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/handler/admin/AdminHandlers.java	2012-05-24 16:55:35.624233213 -0400
+++ lucene4055/solr/core/src/java/org/apache/solr/handler/admin/AdminHandlers.java	2012-05-21 13:57:50.467533660 -0400
@@ -117,7 +117,7 @@
   }
 
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/handler/admin/AdminHandlers.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene4055/solr/core/src/java/org/apache/solr/handler/admin/AdminHandlers.java $";
   }
 
   public Category getCategory() {


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/handler/admin/CoreAdminHandler.java lucene4055/solr/core/src/java/org/apache/solr/handler/admin/CoreAdminHandler.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/handler/admin/CoreAdminHandler.java	2012-05-24 16:55:35.624233213 -0400
+++ lucene4055/solr/core/src/java/org/apache/solr/handler/admin/CoreAdminHandler.java	2012-05-21 13:57:50.467533660 -0400
@@ -861,6 +861,6 @@
 
   @Override
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/handler/admin/CoreAdminHandler.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene4055/solr/core/src/java/org/apache/solr/handler/admin/CoreAdminHandler.java $";
   }
 }


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/handler/admin/LoggingHandler.java lucene4055/solr/core/src/java/org/apache/solr/handler/admin/LoggingHandler.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/handler/admin/LoggingHandler.java	2012-05-24 16:55:35.624233213 -0400
+++ lucene4055/solr/core/src/java/org/apache/solr/handler/admin/LoggingHandler.java	2012-05-21 13:57:50.467533660 -0400
@@ -147,6 +147,6 @@
 
   @Override
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/handler/admin/LoggingHandler.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene4055/solr/core/src/java/org/apache/solr/handler/admin/LoggingHandler.java $";
   }
 }
\ No newline at end of file


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/handler/admin/LukeRequestHandler.java lucene4055/solr/core/src/java/org/apache/solr/handler/admin/LukeRequestHandler.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/handler/admin/LukeRequestHandler.java	2012-05-24 16:55:35.624233213 -0400
+++ lucene4055/solr/core/src/java/org/apache/solr/handler/admin/LukeRequestHandler.java	2012-05-21 13:57:50.467533660 -0400
@@ -616,7 +616,7 @@
 
   @Override
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/handler/admin/LukeRequestHandler.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene4055/solr/core/src/java/org/apache/solr/handler/admin/LukeRequestHandler.java $";
   }
 
   @Override


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/handler/admin/PluginInfoHandler.java lucene4055/solr/core/src/java/org/apache/solr/handler/admin/PluginInfoHandler.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/handler/admin/PluginInfoHandler.java	2012-05-24 16:55:35.624233213 -0400
+++ lucene4055/solr/core/src/java/org/apache/solr/handler/admin/PluginInfoHandler.java	2012-05-21 13:57:50.467533660 -0400
@@ -92,6 +92,6 @@
 
   @Override
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/handler/admin/PluginInfoHandler.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene4055/solr/core/src/java/org/apache/solr/handler/admin/PluginInfoHandler.java $";
   }
 }


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/handler/admin/PropertiesRequestHandler.java lucene4055/solr/core/src/java/org/apache/solr/handler/admin/PropertiesRequestHandler.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/handler/admin/PropertiesRequestHandler.java	2012-05-24 16:55:35.624233213 -0400
+++ lucene4055/solr/core/src/java/org/apache/solr/handler/admin/PropertiesRequestHandler.java	2012-05-21 13:57:50.467533660 -0400
@@ -56,6 +56,6 @@
 
   @Override
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/handler/admin/PropertiesRequestHandler.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene4055/solr/core/src/java/org/apache/solr/handler/admin/PropertiesRequestHandler.java $";
   }
 }


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/handler/admin/ShowFileRequestHandler.java lucene4055/solr/core/src/java/org/apache/solr/handler/admin/ShowFileRequestHandler.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/handler/admin/ShowFileRequestHandler.java	2012-05-24 16:55:35.624233213 -0400
+++ lucene4055/solr/core/src/java/org/apache/solr/handler/admin/ShowFileRequestHandler.java	2012-05-21 13:57:50.467533660 -0400
@@ -287,6 +287,6 @@
 
   @Override
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/handler/admin/ShowFileRequestHandler.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene4055/solr/core/src/java/org/apache/solr/handler/admin/ShowFileRequestHandler.java $";
   }
 }


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/handler/admin/SolrInfoMBeanHandler.java lucene4055/solr/core/src/java/org/apache/solr/handler/admin/SolrInfoMBeanHandler.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/handler/admin/SolrInfoMBeanHandler.java	2012-05-24 16:55:35.624233213 -0400
+++ lucene4055/solr/core/src/java/org/apache/solr/handler/admin/SolrInfoMBeanHandler.java	2012-05-21 13:57:50.471533660 -0400
@@ -299,6 +299,6 @@
 
   @Override
   public String getSource() {    
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/handler/admin/SolrInfoMBeanHandler.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene4055/solr/core/src/java/org/apache/solr/handler/admin/SolrInfoMBeanHandler.java $";
   }
 }


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/handler/admin/SystemInfoHandler.java lucene4055/solr/core/src/java/org/apache/solr/handler/admin/SystemInfoHandler.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/handler/admin/SystemInfoHandler.java	2012-05-24 16:55:35.624233213 -0400
+++ lucene4055/solr/core/src/java/org/apache/solr/handler/admin/SystemInfoHandler.java	2012-05-21 13:57:50.471533660 -0400
@@ -281,7 +281,7 @@
 
   @Override
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/handler/admin/SystemInfoHandler.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene4055/solr/core/src/java/org/apache/solr/handler/admin/SystemInfoHandler.java $";
   }
   
   private static final long ONE_KB = 1024;


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/handler/admin/ThreadDumpHandler.java lucene4055/solr/core/src/java/org/apache/solr/handler/admin/ThreadDumpHandler.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/handler/admin/ThreadDumpHandler.java	2012-05-24 16:55:35.624233213 -0400
+++ lucene4055/solr/core/src/java/org/apache/solr/handler/admin/ThreadDumpHandler.java	2012-05-21 13:57:50.471533660 -0400
@@ -132,6 +132,6 @@
 
   @Override
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/handler/admin/ThreadDumpHandler.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene4055/solr/core/src/java/org/apache/solr/handler/admin/ThreadDumpHandler.java $";
   }
 }


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/handler/BinaryUpdateRequestHandler.java lucene4055/solr/core/src/java/org/apache/solr/handler/BinaryUpdateRequestHandler.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/handler/BinaryUpdateRequestHandler.java	2012-05-24 16:55:35.740233212 -0400
+++ lucene4055/solr/core/src/java/org/apache/solr/handler/BinaryUpdateRequestHandler.java	2012-05-21 13:57:50.555533661 -0400
@@ -44,6 +44,6 @@
 
   @Override
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/handler/BinaryUpdateRequestHandler.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene4055/solr/core/src/java/org/apache/solr/handler/BinaryUpdateRequestHandler.java $";
   }
 }


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/handler/component/DebugComponent.java lucene4055/solr/core/src/java/org/apache/solr/handler/component/DebugComponent.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/handler/component/DebugComponent.java	2012-05-24 16:55:35.652233212 -0400
+++ lucene4055/solr/core/src/java/org/apache/solr/handler/component/DebugComponent.java	2012-05-21 13:57:50.519533661 -0400
@@ -250,7 +250,7 @@
 
   @Override
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/handler/component/DebugComponent.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene4055/solr/core/src/java/org/apache/solr/handler/component/DebugComponent.java $";
   }
 
   @Override


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/handler/component/FacetComponent.java lucene4055/solr/core/src/java/org/apache/solr/handler/component/FacetComponent.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/handler/component/FacetComponent.java	2012-05-24 16:55:35.648233213 -0400
+++ lucene4055/solr/core/src/java/org/apache/solr/handler/component/FacetComponent.java	2012-05-21 13:57:50.519533661 -0400
@@ -606,7 +606,7 @@
 
   @Override
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/handler/component/FacetComponent.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene4055/solr/core/src/java/org/apache/solr/handler/component/FacetComponent.java $";
   }
 
   @Override


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/handler/component/HighlightComponent.java lucene4055/solr/core/src/java/org/apache/solr/handler/component/HighlightComponent.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/handler/component/HighlightComponent.java	2012-05-24 16:55:35.656233213 -0400
+++ lucene4055/solr/core/src/java/org/apache/solr/handler/component/HighlightComponent.java	2012-05-21 13:57:50.523533661 -0400
@@ -204,7 +204,7 @@
   
   @Override
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/handler/component/HighlightComponent.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene4055/solr/core/src/java/org/apache/solr/handler/component/HighlightComponent.java $";
   }
   
   @Override


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/handler/component/MoreLikeThisComponent.java lucene4055/solr/core/src/java/org/apache/solr/handler/component/MoreLikeThisComponent.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/handler/component/MoreLikeThisComponent.java	2012-05-24 16:55:35.652233212 -0400
+++ lucene4055/solr/core/src/java/org/apache/solr/handler/component/MoreLikeThisComponent.java	2012-05-21 13:57:50.523533661 -0400
@@ -118,7 +118,7 @@
 
   @Override
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/handler/component/MoreLikeThisComponent.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene4055/solr/core/src/java/org/apache/solr/handler/component/MoreLikeThisComponent.java $";
   }
 
   @Override


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/handler/component/PivotFacetHelper.java lucene4055/solr/core/src/java/org/apache/solr/handler/component/PivotFacetHelper.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/handler/component/PivotFacetHelper.java	2012-05-24 16:55:35.648233213 -0400
+++ lucene4055/solr/core/src/java/org/apache/solr/handler/component/PivotFacetHelper.java	2012-05-21 13:57:50.519533661 -0400
@@ -245,6 +245,6 @@
 //  }
 //
 //  public String getSource() {
-//    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/handler/component/PivotFacetHelper.java $";
+//    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene4055/solr/core/src/java/org/apache/solr/handler/component/PivotFacetHelper.java $";
 //  }
 }


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/handler/component/QueryComponent.java lucene4055/solr/core/src/java/org/apache/solr/handler/component/QueryComponent.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/handler/component/QueryComponent.java	2012-05-24 16:55:35.656233213 -0400
+++ lucene4055/solr/core/src/java/org/apache/solr/handler/component/QueryComponent.java	2012-05-21 13:57:50.523533661 -0400
@@ -994,7 +994,7 @@
 
   @Override
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/handler/component/QueryComponent.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene4055/solr/core/src/java/org/apache/solr/handler/component/QueryComponent.java $";
   }
 
   @Override


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/handler/component/QueryElevationComponent.java lucene4055/solr/core/src/java/org/apache/solr/handler/component/QueryElevationComponent.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/handler/component/QueryElevationComponent.java	2012-05-24 16:55:35.648233213 -0400
+++ lucene4055/solr/core/src/java/org/apache/solr/handler/component/QueryElevationComponent.java	2012-05-21 13:57:50.515533660 -0400
@@ -462,7 +462,7 @@
 
   @Override
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/handler/component/QueryElevationComponent.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene4055/solr/core/src/java/org/apache/solr/handler/component/QueryElevationComponent.java $";
   }
 
   @Override


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/handler/component/RealTimeGetComponent.java lucene4055/solr/core/src/java/org/apache/solr/handler/component/RealTimeGetComponent.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/handler/component/RealTimeGetComponent.java	2012-05-24 16:55:35.652233212 -0400
+++ lucene4055/solr/core/src/java/org/apache/solr/handler/component/RealTimeGetComponent.java	2012-05-23 12:12:09.426432451 -0400
@@ -472,7 +472,7 @@
 
   @Override
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/handler/component/RealTimeGetComponent.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene4055/solr/core/src/java/org/apache/solr/handler/component/RealTimeGetComponent.java $";
   }
 
   @Override


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/handler/component/SearchHandler.java lucene4055/solr/core/src/java/org/apache/solr/handler/component/SearchHandler.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/handler/component/SearchHandler.java	2012-05-24 16:55:35.648233213 -0400
+++ lucene4055/solr/core/src/java/org/apache/solr/handler/component/SearchHandler.java	2012-05-21 13:57:50.519533661 -0400
@@ -335,7 +335,7 @@
 
   @Override
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/handler/component/SearchHandler.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene4055/solr/core/src/java/org/apache/solr/handler/component/SearchHandler.java $";
   }
 }
 


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/handler/component/SpellCheckComponent.java lucene4055/solr/core/src/java/org/apache/solr/handler/component/SpellCheckComponent.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/handler/component/SpellCheckComponent.java	2012-05-24 16:55:35.648233213 -0400
+++ lucene4055/solr/core/src/java/org/apache/solr/handler/component/SpellCheckComponent.java	2012-05-23 12:12:08.806432440 -0400
@@ -693,7 +693,7 @@
 
   @Override
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/handler/component/SpellCheckComponent.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene4055/solr/core/src/java/org/apache/solr/handler/component/SpellCheckComponent.java $";
   }
 
 }


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/handler/component/StatsComponent.java lucene4055/solr/core/src/java/org/apache/solr/handler/component/StatsComponent.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/handler/component/StatsComponent.java	2012-05-24 16:55:35.652233212 -0400
+++ lucene4055/solr/core/src/java/org/apache/solr/handler/component/StatsComponent.java	2012-05-21 13:57:50.523533661 -0400
@@ -155,7 +155,7 @@
 
   @Override
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/handler/component/StatsComponent.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene4055/solr/core/src/java/org/apache/solr/handler/component/StatsComponent.java $";
   }
 
 }


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/handler/component/TermsComponent.java lucene4055/solr/core/src/java/org/apache/solr/handler/component/TermsComponent.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/handler/component/TermsComponent.java	2012-05-24 16:55:35.652233212 -0400
+++ lucene4055/solr/core/src/java/org/apache/solr/handler/component/TermsComponent.java	2012-05-21 13:57:50.519533661 -0400
@@ -474,7 +474,7 @@
 
   @Override
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/handler/component/TermsComponent.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene4055/solr/core/src/java/org/apache/solr/handler/component/TermsComponent.java $";
   }
 
   @Override


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/handler/component/TermVectorComponent.java lucene4055/solr/core/src/java/org/apache/solr/handler/component/TermVectorComponent.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/handler/component/TermVectorComponent.java	2012-05-24 16:55:35.652233212 -0400
+++ lucene4055/solr/core/src/java/org/apache/solr/handler/component/TermVectorComponent.java	2012-05-21 13:57:50.519533661 -0400
@@ -414,7 +414,7 @@
 
   @Override
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/handler/component/TermVectorComponent.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene4055/solr/core/src/java/org/apache/solr/handler/component/TermVectorComponent.java $";
   }
 
   @Override


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/handler/CSVRequestHandler.java lucene4055/solr/core/src/java/org/apache/solr/handler/CSVRequestHandler.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/handler/CSVRequestHandler.java	2012-05-24 16:55:35.620233213 -0400
+++ lucene4055/solr/core/src/java/org/apache/solr/handler/CSVRequestHandler.java	2012-05-21 13:57:50.467533660 -0400
@@ -40,7 +40,7 @@
 
   @Override
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/handler/CSVRequestHandler.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene4055/solr/core/src/java/org/apache/solr/handler/CSVRequestHandler.java $";
   }
 }
 


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/handler/DocumentAnalysisRequestHandler.java lucene4055/solr/core/src/java/org/apache/solr/handler/DocumentAnalysisRequestHandler.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/handler/DocumentAnalysisRequestHandler.java	2012-05-24 16:55:35.740233212 -0400
+++ lucene4055/solr/core/src/java/org/apache/solr/handler/DocumentAnalysisRequestHandler.java	2012-05-21 13:57:50.551533661 -0400
@@ -122,7 +122,7 @@
 
   @Override
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/handler/DocumentAnalysisRequestHandler.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene4055/solr/core/src/java/org/apache/solr/handler/DocumentAnalysisRequestHandler.java $";
   }
 
 


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/handler/DumpRequestHandler.java lucene4055/solr/core/src/java/org/apache/solr/handler/DumpRequestHandler.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/handler/DumpRequestHandler.java	2012-05-24 16:55:35.648233213 -0400
+++ lucene4055/solr/core/src/java/org/apache/solr/handler/DumpRequestHandler.java	2012-05-21 13:57:50.515533660 -0400
@@ -69,6 +69,6 @@
 
   @Override
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/handler/DumpRequestHandler.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene4055/solr/core/src/java/org/apache/solr/handler/DumpRequestHandler.java $";
   }
 }


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/handler/FieldAnalysisRequestHandler.java lucene4055/solr/core/src/java/org/apache/solr/handler/FieldAnalysisRequestHandler.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/handler/FieldAnalysisRequestHandler.java	2012-05-24 16:55:35.740233212 -0400
+++ lucene4055/solr/core/src/java/org/apache/solr/handler/FieldAnalysisRequestHandler.java	2012-05-21 13:57:50.555533661 -0400
@@ -107,7 +107,7 @@
 
   @Override
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/handler/FieldAnalysisRequestHandler.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene4055/solr/core/src/java/org/apache/solr/handler/FieldAnalysisRequestHandler.java $";
   }
 
   // ================================================= Helper methods ================================================


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/handler/JsonUpdateRequestHandler.java lucene4055/solr/core/src/java/org/apache/solr/handler/JsonUpdateRequestHandler.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/handler/JsonUpdateRequestHandler.java	2012-05-24 16:55:35.728233217 -0400
+++ lucene4055/solr/core/src/java/org/apache/solr/handler/JsonUpdateRequestHandler.java	2012-05-21 13:57:50.543533661 -0400
@@ -41,7 +41,7 @@
 
   @Override
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/handler/JsonUpdateRequestHandler.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene4055/solr/core/src/java/org/apache/solr/handler/JsonUpdateRequestHandler.java $";
   }
 }
 


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/handler/MoreLikeThisHandler.java lucene4055/solr/core/src/java/org/apache/solr/handler/MoreLikeThisHandler.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/handler/MoreLikeThisHandler.java	2012-05-24 16:55:35.740233212 -0400
+++ lucene4055/solr/core/src/java/org/apache/solr/handler/MoreLikeThisHandler.java	2012-05-21 13:57:50.555533661 -0400
@@ -429,7 +429,7 @@
 
   @Override
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/handler/MoreLikeThisHandler.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene4055/solr/core/src/java/org/apache/solr/handler/MoreLikeThisHandler.java $";
   }
 
   @Override


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/handler/PingRequestHandler.java lucene4055/solr/core/src/java/org/apache/solr/handler/PingRequestHandler.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/handler/PingRequestHandler.java	2012-05-24 16:55:35.620233213 -0400
+++ lucene4055/solr/core/src/java/org/apache/solr/handler/PingRequestHandler.java	2012-05-21 13:57:50.467533660 -0400
@@ -293,6 +293,6 @@
 
   @Override
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/handler/PingRequestHandler.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene4055/solr/core/src/java/org/apache/solr/handler/PingRequestHandler.java $";
   }
 }


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/handler/RealTimeGetHandler.java lucene4055/solr/core/src/java/org/apache/solr/handler/RealTimeGetHandler.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/handler/RealTimeGetHandler.java	2012-05-24 16:55:35.620233213 -0400
+++ lucene4055/solr/core/src/java/org/apache/solr/handler/RealTimeGetHandler.java	2012-05-21 13:57:50.463533660 -0400
@@ -42,7 +42,7 @@
 
   @Override
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/handler/RealTimeGetHandler.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene4055/solr/core/src/java/org/apache/solr/handler/RealTimeGetHandler.java $";
   }
 
   @Override


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/handler/ReplicationHandler.java lucene4055/solr/core/src/java/org/apache/solr/handler/ReplicationHandler.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/handler/ReplicationHandler.java	2012-05-24 16:55:35.620233213 -0400
+++ lucene4055/solr/core/src/java/org/apache/solr/handler/ReplicationHandler.java	2012-05-21 13:57:50.467533660 -0400
@@ -496,7 +496,7 @@
 
   @Override
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/handler/ReplicationHandler.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene4055/solr/core/src/java/org/apache/solr/handler/ReplicationHandler.java $";
   }
 
   private long[] getIndexVersion() {


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/handler/StandardRequestHandler.java lucene4055/solr/core/src/java/org/apache/solr/handler/StandardRequestHandler.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/handler/StandardRequestHandler.java	2012-05-24 16:55:35.620233213 -0400
+++ lucene4055/solr/core/src/java/org/apache/solr/handler/StandardRequestHandler.java	2012-05-21 13:57:50.467533660 -0400
@@ -52,7 +52,7 @@
 
   @Override
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/handler/StandardRequestHandler.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene4055/solr/core/src/java/org/apache/solr/handler/StandardRequestHandler.java $";
   }
 
   @Override


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/handler/UpdateRequestHandler.java lucene4055/solr/core/src/java/org/apache/solr/handler/UpdateRequestHandler.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/handler/UpdateRequestHandler.java	2012-05-24 16:55:35.740233212 -0400
+++ lucene4055/solr/core/src/java/org/apache/solr/handler/UpdateRequestHandler.java	2012-05-21 13:57:50.555533661 -0400
@@ -159,7 +159,7 @@
 
   @Override
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/handler/UpdateRequestHandler.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene4055/solr/core/src/java/org/apache/solr/handler/UpdateRequestHandler.java $";
   }
 }
 


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/handler/XmlUpdateRequestHandler.java lucene4055/solr/core/src/java/org/apache/solr/handler/XmlUpdateRequestHandler.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/handler/XmlUpdateRequestHandler.java	2012-05-24 16:55:35.648233213 -0400
+++ lucene4055/solr/core/src/java/org/apache/solr/handler/XmlUpdateRequestHandler.java	2012-05-21 13:57:50.515533660 -0400
@@ -43,7 +43,7 @@
 
   @Override
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/handler/XmlUpdateRequestHandler.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene4055/solr/core/src/java/org/apache/solr/handler/XmlUpdateRequestHandler.java $";
   }
 }
 


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/handler/XsltUpdateRequestHandler.java lucene4055/solr/core/src/java/org/apache/solr/handler/XsltUpdateRequestHandler.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/handler/XsltUpdateRequestHandler.java	2012-05-24 16:55:35.620233213 -0400
+++ lucene4055/solr/core/src/java/org/apache/solr/handler/XsltUpdateRequestHandler.java	2012-05-21 13:57:50.467533660 -0400
@@ -43,6 +43,6 @@
 
   @Override
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/handler/XsltUpdateRequestHandler.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene4055/solr/core/src/java/org/apache/solr/handler/XsltUpdateRequestHandler.java $";
   }
 }


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/highlight/BreakIteratorBoundaryScanner.java lucene4055/solr/core/src/java/org/apache/solr/highlight/BreakIteratorBoundaryScanner.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/highlight/BreakIteratorBoundaryScanner.java	2012-05-24 16:55:35.548233212 -0400
+++ lucene4055/solr/core/src/java/org/apache/solr/highlight/BreakIteratorBoundaryScanner.java	2012-05-21 13:57:50.423533659 -0400
@@ -75,6 +75,6 @@
 
   @Override
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/highlight/BreakIteratorBoundaryScanner.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene4055/solr/core/src/java/org/apache/solr/highlight/BreakIteratorBoundaryScanner.java $";
   }
 }


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/highlight/DefaultEncoder.java lucene4055/solr/core/src/java/org/apache/solr/highlight/DefaultEncoder.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/highlight/DefaultEncoder.java	2012-05-24 16:55:35.552233212 -0400
+++ lucene4055/solr/core/src/java/org/apache/solr/highlight/DefaultEncoder.java	2012-05-21 13:57:50.423533659 -0400
@@ -42,6 +42,6 @@
 
   @Override
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/highlight/DefaultEncoder.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene4055/solr/core/src/java/org/apache/solr/highlight/DefaultEncoder.java $";
   }
 }


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/highlight/GapFragmenter.java lucene4055/solr/core/src/java/org/apache/solr/highlight/GapFragmenter.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/highlight/GapFragmenter.java	2012-05-24 16:55:35.552233212 -0400
+++ lucene4055/solr/core/src/java/org/apache/solr/highlight/GapFragmenter.java	2012-05-21 13:57:50.423533659 -0400
@@ -48,7 +48,7 @@
 
   @Override
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/highlight/GapFragmenter.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene4055/solr/core/src/java/org/apache/solr/highlight/GapFragmenter.java $";
   }
 }
 


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/highlight/HtmlEncoder.java lucene4055/solr/core/src/java/org/apache/solr/highlight/HtmlEncoder.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/highlight/HtmlEncoder.java	2012-05-24 16:55:35.548233212 -0400
+++ lucene4055/solr/core/src/java/org/apache/solr/highlight/HtmlEncoder.java	2012-05-21 13:57:50.419533659 -0400
@@ -42,6 +42,6 @@
 
   @Override
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/highlight/HtmlEncoder.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene4055/solr/core/src/java/org/apache/solr/highlight/HtmlEncoder.java $";
   }
 }


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/highlight/HtmlFormatter.java lucene4055/solr/core/src/java/org/apache/solr/highlight/HtmlFormatter.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/highlight/HtmlFormatter.java	2012-05-24 16:55:35.552233212 -0400
+++ lucene4055/solr/core/src/java/org/apache/solr/highlight/HtmlFormatter.java	2012-05-21 13:57:50.423533659 -0400
@@ -47,6 +47,6 @@
 
   @Override
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/highlight/HtmlFormatter.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene4055/solr/core/src/java/org/apache/solr/highlight/HtmlFormatter.java $";
   }
 }


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/highlight/RegexFragmenter.java lucene4055/solr/core/src/java/org/apache/solr/highlight/RegexFragmenter.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/highlight/RegexFragmenter.java	2012-05-24 16:55:35.552233212 -0400
+++ lucene4055/solr/core/src/java/org/apache/solr/highlight/RegexFragmenter.java	2012-05-21 13:57:50.423533659 -0400
@@ -95,7 +95,7 @@
 
   @Override
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/highlight/RegexFragmenter.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene4055/solr/core/src/java/org/apache/solr/highlight/RegexFragmenter.java $";
   }
 }
 


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/highlight/ScoreOrderFragmentsBuilder.java lucene4055/solr/core/src/java/org/apache/solr/highlight/ScoreOrderFragmentsBuilder.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/highlight/ScoreOrderFragmentsBuilder.java	2012-05-24 16:55:35.552233212 -0400
+++ lucene4055/solr/core/src/java/org/apache/solr/highlight/ScoreOrderFragmentsBuilder.java	2012-05-21 13:57:50.423533659 -0400
@@ -43,6 +43,6 @@
 
   @Override
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/highlight/ScoreOrderFragmentsBuilder.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene4055/solr/core/src/java/org/apache/solr/highlight/ScoreOrderFragmentsBuilder.java $";
   }
 }


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/highlight/SimpleBoundaryScanner.java lucene4055/solr/core/src/java/org/apache/solr/highlight/SimpleBoundaryScanner.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/highlight/SimpleBoundaryScanner.java	2012-05-24 16:55:35.548233212 -0400
+++ lucene4055/solr/core/src/java/org/apache/solr/highlight/SimpleBoundaryScanner.java	2012-05-21 13:57:50.419533659 -0400
@@ -46,6 +46,6 @@
 
   @Override
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/highlight/SimpleBoundaryScanner.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene4055/solr/core/src/java/org/apache/solr/highlight/SimpleBoundaryScanner.java $";
   }
 }


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/highlight/SimpleFragListBuilder.java lucene4055/solr/core/src/java/org/apache/solr/highlight/SimpleFragListBuilder.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/highlight/SimpleFragListBuilder.java	2012-05-24 16:55:35.552233212 -0400
+++ lucene4055/solr/core/src/java/org/apache/solr/highlight/SimpleFragListBuilder.java	2012-05-21 13:57:50.423533659 -0400
@@ -44,6 +44,6 @@
 
   @Override
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/highlight/SimpleFragListBuilder.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene4055/solr/core/src/java/org/apache/solr/highlight/SimpleFragListBuilder.java $";
   }
 }


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/highlight/SimpleFragmentsBuilder.java lucene4055/solr/core/src/java/org/apache/solr/highlight/SimpleFragmentsBuilder.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/highlight/SimpleFragmentsBuilder.java	2012-05-24 16:55:35.548233212 -0400
+++ lucene4055/solr/core/src/java/org/apache/solr/highlight/SimpleFragmentsBuilder.java	2012-05-21 13:57:50.419533659 -0400
@@ -43,6 +43,6 @@
 
   @Override
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/highlight/SimpleFragmentsBuilder.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene4055/solr/core/src/java/org/apache/solr/highlight/SimpleFragmentsBuilder.java $";
   }
 }


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/highlight/SingleFragListBuilder.java lucene4055/solr/core/src/java/org/apache/solr/highlight/SingleFragListBuilder.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/highlight/SingleFragListBuilder.java	2012-05-24 16:55:35.552233212 -0400
+++ lucene4055/solr/core/src/java/org/apache/solr/highlight/SingleFragListBuilder.java	2012-05-21 13:57:50.423533659 -0400
@@ -44,6 +44,6 @@
 
   @Override
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/highlight/SingleFragListBuilder.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene4055/solr/core/src/java/org/apache/solr/highlight/SingleFragListBuilder.java $";
   }
 }


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/search/FastLRUCache.java lucene4055/solr/core/src/java/org/apache/solr/search/FastLRUCache.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/search/FastLRUCache.java	2012-05-24 16:55:35.464233210 -0400
+++ lucene4055/solr/core/src/java/org/apache/solr/search/FastLRUCache.java	2012-05-21 13:57:50.207533655 -0400
@@ -181,7 +181,7 @@
   }
 
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/search/FastLRUCache.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene4055/solr/core/src/java/org/apache/solr/search/FastLRUCache.java $";
   }
 
 


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/search/function/FileFloatSource.java lucene4055/solr/core/src/java/org/apache/solr/search/function/FileFloatSource.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/search/function/FileFloatSource.java	2012-05-24 16:55:35.468233210 -0400
+++ lucene4055/solr/core/src/java/org/apache/solr/search/function/FileFloatSource.java	2012-05-21 13:57:50.211533655 -0400
@@ -327,7 +327,7 @@
 
     @Override
     public String getSource() {
-      return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/search/function/FileFloatSource.java $";
+      return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene4055/solr/core/src/java/org/apache/solr/search/function/FileFloatSource.java $";
     }
   }
 }


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/search/LFUCache.java lucene4055/solr/core/src/java/org/apache/solr/search/LFUCache.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/search/LFUCache.java	2012-05-24 16:55:35.484233210 -0400
+++ lucene4055/solr/core/src/java/org/apache/solr/search/LFUCache.java	2012-05-21 13:57:50.235533656 -0400
@@ -208,7 +208,7 @@
   }
 
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/search/LFUCache.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene4055/solr/core/src/java/org/apache/solr/search/LFUCache.java $";
   }
 
   public URL[] getDocs() {


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/search/LRUCache.java lucene4055/solr/core/src/java/org/apache/solr/search/LRUCache.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/search/LRUCache.java	2012-05-24 16:55:35.520233211 -0400
+++ lucene4055/solr/core/src/java/org/apache/solr/search/LRUCache.java	2012-05-21 13:57:50.367533658 -0400
@@ -209,7 +209,7 @@
   }
 
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/search/LRUCache.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene4055/solr/core/src/java/org/apache/solr/search/LRUCache.java $";
   }
 
   public NamedList getStatistics() {


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/search/SolrFieldCacheMBean.java lucene4055/solr/core/src/java/org/apache/solr/search/SolrFieldCacheMBean.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/search/SolrFieldCacheMBean.java	2012-05-24 16:55:35.480233209 -0400
+++ lucene4055/solr/core/src/java/org/apache/solr/search/SolrFieldCacheMBean.java	2012-05-21 13:57:50.231533656 -0400
@@ -47,7 +47,7 @@
   }
   public Category getCategory() { return Category.CACHE; } 
   public String getSource() { 
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/search/SolrFieldCacheMBean.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene4055/solr/core/src/java/org/apache/solr/search/SolrFieldCacheMBean.java $";
   }
   public URL[] getDocs() {
     return null;


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/search/SolrIndexSearcher.java lucene4055/solr/core/src/java/org/apache/solr/search/SolrIndexSearcher.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/search/SolrIndexSearcher.java	2012-05-24 16:55:35.520233211 -0400
+++ lucene4055/solr/core/src/java/org/apache/solr/search/SolrIndexSearcher.java	2012-05-21 13:57:50.367533658 -0400
@@ -452,11 +452,10 @@
     @Override
     public void stringField(FieldInfo fieldInfo, String value) throws IOException {
       final FieldType ft = new FieldType(TextField.TYPE_STORED);
-      ft.setStoreTermVectors(fieldInfo.storeTermVector);
-      ft.setStoreTermVectors(fieldInfo.storeTermVector);
-      ft.setIndexed(fieldInfo.isIndexed);
-      ft.setOmitNorms(fieldInfo.omitNorms);
-      ft.setIndexOptions(fieldInfo.indexOptions);
+      ft.setStoreTermVectors(fieldInfo.hasVectors());
+      ft.setIndexed(fieldInfo.isIndexed());
+      ft.setOmitNorms(fieldInfo.omitsNorms());
+      ft.setIndexOptions(fieldInfo.getIndexOptions());
       doc.add(new Field(fieldInfo.name, value, ft));
     }
 
@@ -464,7 +463,7 @@
     public void intField(FieldInfo fieldInfo, int value) {
       FieldType ft = new FieldType(IntField.TYPE);
       ft.setStored(true);
-      ft.setIndexed(fieldInfo.isIndexed);
+      ft.setIndexed(fieldInfo.isIndexed());
       doc.add(new IntField(fieldInfo.name, value, ft));
     }
 
@@ -472,7 +471,7 @@
     public void longField(FieldInfo fieldInfo, long value) {
       FieldType ft = new FieldType(LongField.TYPE);
       ft.setStored(true);
-      ft.setIndexed(fieldInfo.isIndexed);
+      ft.setIndexed(fieldInfo.isIndexed());
       doc.add(new LongField(fieldInfo.name, value, ft));
     }
 
@@ -480,7 +479,7 @@
     public void floatField(FieldInfo fieldInfo, float value) {
       FieldType ft = new FieldType(FloatField.TYPE);
       ft.setStored(true);
-      ft.setIndexed(fieldInfo.isIndexed);
+      ft.setIndexed(fieldInfo.isIndexed());
       doc.add(new FloatField(fieldInfo.name, value, ft));
     }
 
@@ -488,7 +487,7 @@
     public void doubleField(FieldInfo fieldInfo, double value) {
       FieldType ft = new FieldType(DoubleField.TYPE);
       ft.setStored(true);
-      ft.setIndexed(fieldInfo.isIndexed);
+      ft.setIndexed(fieldInfo.isIndexed());
       doc.add(new DoubleField(fieldInfo.name, value, ft));
     }
   }
@@ -1988,7 +1987,7 @@
   }
 
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/search/SolrIndexSearcher.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene4055/solr/core/src/java/org/apache/solr/search/SolrIndexSearcher.java $";
   }
 
   public URL[] getDocs() {


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/java/org/apache/solr/update/DirectUpdateHandler2.java lucene4055/solr/core/src/java/org/apache/solr/update/DirectUpdateHandler2.java
--- lucene-trunk/solr/core/src/java/org/apache/solr/update/DirectUpdateHandler2.java	2012-05-24 16:55:35.204233206 -0400
+++ lucene4055/solr/core/src/java/org/apache/solr/update/DirectUpdateHandler2.java	2012-05-21 13:57:49.875533649 -0400
@@ -639,7 +639,7 @@
   }
 
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/update/DirectUpdateHandler2.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene4055/solr/core/src/java/org/apache/solr/update/DirectUpdateHandler2.java $";
   }
 
   public URL[] getDocs() {


diff -ruN -x .svn -x build lucene-trunk/solr/core/src/test/org/apache/solr/search/TestDocSet.java lucene4055/solr/core/src/test/org/apache/solr/search/TestDocSet.java
--- lucene-trunk/solr/core/src/test/org/apache/solr/search/TestDocSet.java	2012-05-24 16:55:34.984233202 -0400
+++ lucene4055/solr/core/src/test/org/apache/solr/search/TestDocSet.java	2012-05-21 13:57:49.527533643 -0400
@@ -21,6 +21,7 @@
 import java.util.Arrays;
 import java.util.Random;
 
+import org.apache.lucene.index.FieldInfo;
 import org.apache.lucene.index.FieldInfos;
 import org.apache.lucene.index.DocValues;
 import org.apache.lucene.index.StoredFieldVisitor;
@@ -365,7 +366,7 @@
 
       @Override
       public FieldInfos getFieldInfos() {
-        return new FieldInfos();
+        return new FieldInfos(new FieldInfo[0]);
       }
 
       @Override


diff -ruN -x .svn -x build lucene-trunk/solr/solrj/src/test-files/solrj/solr/conf/schema-replication1.xml lucene4055/solr/solrj/src/test-files/solrj/solr/conf/schema-replication1.xml
--- lucene-trunk/solr/solrj/src/test-files/solrj/solr/conf/schema-replication1.xml	2011-09-11 11:01:09.406123944 -0400
+++ lucene4055/solr/solrj/src/test-files/solrj/solr/conf/schema-replication1.xml	2012-05-21 13:57:50.743533664 -0400
@@ -23,7 +23,7 @@
      kitchen sink thrown in. See example/solr/conf/schema.xml for a 
      more concise example.
 
-     $Id: schema-replication1.xml 1160700 2011-08-23 14:06:58Z rmuir $
+     $Id: schema-replication1.xml 1144761 2011-07-09 23:01:53Z sarowe $
      $Source$
      $Name$
   -->
