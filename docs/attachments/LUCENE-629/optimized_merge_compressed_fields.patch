Index: src/java/org/apache/lucene/index/FieldsReader.java
===================================================================
--- src/java/org/apache/lucene/index/FieldsReader.java	(revision 419199)
+++ src/java/org/apache/lucene/index/FieldsReader.java	(working copy)
@@ -16,16 +16,21 @@
  * limitations under the License.
  */
 
-import org.apache.lucene.document.*;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IndexInput;
-
 import java.io.ByteArrayOutputStream;
 import java.io.IOException;
 import java.io.Reader;
 import java.util.zip.DataFormatException;
 import java.util.zip.Inflater;
 
+import org.apache.lucene.document.AbstractField;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.document.FieldSelector;
+import org.apache.lucene.document.FieldSelectorResult;
+import org.apache.lucene.document.Fieldable;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IndexInput;
+
 /**
  * Class responsible for access to stored document fields.
  * <p/>
@@ -89,6 +94,9 @@
       if (acceptField.equals(FieldSelectorResult.LOAD) == true) {
         addField(doc, fi, binary, compressed, tokenize);
       }
+      else if (acceptField.equals(FieldSelectorResult.LOAD_FOR_MERGE) == true) {
+        addFieldForMerge(doc, fi, binary, compressed, tokenize);
+      }
       else if (acceptField.equals(FieldSelectorResult.LOAD_AND_BREAK) == true){
         addField(doc, fi, binary, compressed, tokenize);
         break;//Get out of this loop
@@ -161,6 +169,22 @@
 
   }
 
+  // in merge mode we don't uncompress the data of a compressed field
+  private void addFieldForMerge(Document doc, FieldInfo fi, boolean binary, boolean compressed, boolean tokenize) throws IOException {
+    Object data;
+      
+    if (binary || compressed) {
+      int toRead = fieldsStream.readVInt();
+      final byte[] b = new byte[toRead];
+      fieldsStream.readBytes(b, 0, b.length);
+      data = b;
+    } else {
+      data = fieldsStream.readString();
+    }
+      
+    doc.add(new FieldForMerge(data, fi, binary, compressed, tokenize));
+  }
+  
   private void addField(Document doc, FieldInfo fi, boolean binary, boolean compressed, boolean tokenize) throws IOException {
 
     //we have a binary stored field, and it may be compressed
@@ -370,4 +394,37 @@
     // Get the decompressed data
     return bos.toByteArray();
   }
+  
+  // Instances of this class hold field properties and data
+  // for merge
+  final static class FieldForMerge extends AbstractField {
+    public String stringValue() {
+      return (String) this.fieldsData;
+    }
+
+    public Reader readerValue() {
+      // not needed for merge
+      return null;
+    }
+
+    public byte[] binaryValue() {
+      return (byte[]) this.fieldsData;
+    }
+    
+    public FieldForMerge(Object value, FieldInfo fi, boolean binary, boolean compressed, boolean tokenize) {
+      this.isStored = true;  
+      this.fieldsData = value;
+      this.isCompressed = compressed;
+      this.isBinary = binary;
+      this.isTokenized = tokenize;
+
+      this.name = fi.name.intern();
+      this.isIndexed = fi.isIndexed;
+      this.omitNorms = fi.omitNorms;          
+      this.storeOffsetWithTermVector = fi.storeOffsetWithTermVector;
+      this.storePositionWithTermVector = fi.storePositionWithTermVector;
+      this.storeTermVector = fi.storeTermVector;            
+    }
+     
+  }
 }
Index: src/java/org/apache/lucene/index/FieldsWriter.java
===================================================================
--- src/java/org/apache/lucene/index/FieldsWriter.java	(revision 419199)
+++ src/java/org/apache/lucene/index/FieldsWriter.java	(working copy)
@@ -23,6 +23,7 @@
 
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
+import org.apache.lucene.document.Fieldable;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.IndexOutput;
 
@@ -55,7 +56,7 @@
         int storedCount = 0;
         Enumeration fields = doc.fields();
         while (fields.hasMoreElements()) {
-            Field field = (Field) fields.nextElement();
+            Fieldable field = (Fieldable) fields.nextElement();
             if (field.isStored())
                 storedCount++;
         }
@@ -63,7 +64,11 @@
 
         fields = doc.fields();
         while (fields.hasMoreElements()) {
-            Field field = (Field) fields.nextElement();
+            Fieldable field = (Fieldable) fields.nextElement();
+            // if the field as an instanceof FieldsReader.FieldForMerge, we're in merge mode
+            // and field.binaryValue() already returns the compressed value for a field
+            // with isCompressed()==true, so we disable compression in that case
+            boolean disableCompression = (field instanceof FieldsReader.FieldForMerge);
             if (field.isStored()) {
                 fieldsStream.writeVInt(fieldInfos.fieldNumber(field.name()));
 
@@ -80,13 +85,20 @@
                 if (field.isCompressed()) {
                   // compression is enabled for the current field
                   byte[] data = null;
-                  // check if it is a binary field
-                  if (field.isBinary()) {
-                    data = compress(field.binaryValue());
+                  
+                  if (disableCompression) {
+                      // optimized case for merging, the data
+                      // is already compressed
+                      data = field.binaryValue();
+                  } else {
+                      // check if it is a binary field
+                      if (field.isBinary()) {
+                        data = compress(field.binaryValue());
+                      }
+                      else {
+                        data = compress(field.stringValue().getBytes("UTF-8"));
+                      }
                   }
-                  else {
-                    data = compress(field.stringValue().getBytes("UTF-8"));
-                  }
                   final int len = data.length;
                   fieldsStream.writeVInt(len);
                   fieldsStream.writeBytes(data, len);
Index: src/java/org/apache/lucene/index/SegmentMerger.java
===================================================================
--- src/java/org/apache/lucene/index/SegmentMerger.java	(revision 419199)
+++ src/java/org/apache/lucene/index/SegmentMerger.java	(working copy)
@@ -21,6 +21,8 @@
 import java.util.Collection;
 import java.io.IOException;
 
+import org.apache.lucene.document.FieldSelector;
+import org.apache.lucene.document.FieldSelectorResult;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.IndexOutput;
 import org.apache.lucene.store.RAMOutputStream;
@@ -177,13 +179,22 @@
 
     FieldsWriter fieldsWriter = // merge field values
             new FieldsWriter(directory, segment, fieldInfos);
+    
+    // for merging we don't want to compress/uncompress the data, so to tell the FieldsReader that we're
+    // in  merge mode, we use this FieldSelector
+    FieldSelector fieldSelectorMerge = new FieldSelector() {
+      public FieldSelectorResult accept(String fieldName) {
+        return FieldSelectorResult.LOAD_FOR_MERGE;
+      }        
+    };
+    
     try {
       for (int i = 0; i < readers.size(); i++) {
         IndexReader reader = (IndexReader) readers.elementAt(i);
         int maxDoc = reader.maxDoc();
         for (int j = 0; j < maxDoc; j++)
           if (!reader.isDeleted(j)) {               // skip deleted docs
-            fieldsWriter.addDocument(reader.document(j));
+            fieldsWriter.addDocument(reader.document(j, fieldSelectorMerge));
             docCount++;
           }
       }
Index: src/java/org/apache/lucene/document/FieldSelectorResult.java
===================================================================
--- src/java/org/apache/lucene/document/FieldSelectorResult.java	(revision 419199)
+++ src/java/org/apache/lucene/document/FieldSelectorResult.java	(working copy)
@@ -26,6 +26,7 @@
   public static final FieldSelectorResult LAZY_LOAD = new FieldSelectorResult(1);
   public static final FieldSelectorResult NO_LOAD = new FieldSelectorResult(2);
   public static final FieldSelectorResult LOAD_AND_BREAK = new FieldSelectorResult(3);
+  public static final FieldSelectorResult LOAD_FOR_MERGE = new FieldSelectorResult(4);
   
   private int id;
 
