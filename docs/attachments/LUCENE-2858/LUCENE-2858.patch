Index: .
===================================================================
--- .	(revision 1238034)
+++ .	(working copy)

Property changes on: .
___________________________________________________________________
Modified: svn:mergeinfo
   Merged /lucene/dev/branches/lucene2858:r1234440-1238051
Index: dev-tools/idea/lucene/contrib
===================================================================
--- dev-tools/idea/lucene/contrib	(revision 1238034)
+++ dev-tools/idea/lucene/contrib	(working copy)

Property changes on: dev-tools/idea/lucene/contrib
___________________________________________________________________
Modified: svn:mergeinfo
   Merged /lucene/dev/branches/lucene2858/dev-tools/idea/lucene/contrib:r1234440-1238051
Index: lucene
===================================================================
--- lucene	(revision 1238034)
+++ lucene	(working copy)

Property changes on: lucene
___________________________________________________________________
Modified: svn:mergeinfo
   Merged /lucene/dev/branches/lucene2858/lucene:r1234440-1238051
Index: lucene/contrib/CHANGES.txt
===================================================================
--- lucene/contrib/CHANGES.txt	(revision 1238034)
+++ lucene/contrib/CHANGES.txt	(working copy)

Property changes on: lucene/contrib/CHANGES.txt
___________________________________________________________________
Modified: svn:mergeinfo
   Merged /lucene/dev/branches/lucene2858/lucene/contrib/CHANGES.txt:r1234440-1238051
Index: lucene/contrib/highlighter/src/java/org/apache/lucene/search/highlight/WeightedSpanTermExtractor.java
===================================================================
--- lucene/contrib/highlighter/src/java/org/apache/lucene/search/highlight/WeightedSpanTermExtractor.java	(revision 1238034)
+++ lucene/contrib/highlighter/src/java/org/apache/lucene/search/highlight/WeightedSpanTermExtractor.java	(working copy)
@@ -29,9 +29,9 @@
 
 import org.apache.lucene.analysis.CachingTokenFilter;
 import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.Term;
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
 import org.apache.lucene.index.memory.MemoryIndex;
 import org.apache.lucene.search.*;
 import org.apache.lucene.search.spans.FieldMaskingSpanQuery;
@@ -74,7 +74,7 @@
 
     for (final AtomicReaderContext ctx : ctxSet) {
       try {
-        ctx.reader.close();
+        ctx.reader().close();
       } catch (IOException e) {
         // alert?
       }
@@ -153,7 +153,7 @@
         query = mtq;
       }
       if (mtq.getField() != null) {
-        IndexReader ir = getLeafContextForField(mtq.getField()).reader;
+        IndexReader ir = getLeafContextForField(mtq.getField()).reader();
         extract(query.rewrite(ir), terms);
       }
     } else if (query instanceof MultiPhraseQuery) {
@@ -244,7 +244,7 @@
     final boolean mustRewriteQuery = mustRewriteQuery(spanQuery);
     if (mustRewriteQuery) {
       for (final String field : fieldNames) {
-        final SpanQuery rewrittenQuery = (SpanQuery) spanQuery.rewrite(getLeafContextForField(field).reader);
+        final SpanQuery rewrittenQuery = (SpanQuery) spanQuery.rewrite(getLeafContextForField(field).reader());
         queries.put(field, rewrittenQuery);
         rewrittenQuery.extractTerms(nonWeightedTerms);
       }
@@ -268,7 +268,7 @@
       for (Term term : extractedTerms) {
         termContexts.put(term, TermContext.build(context, term, true));
       }
-      Bits acceptDocs = context.reader.getLiveDocs();
+      Bits acceptDocs = context.reader().getLiveDocs();
       final Spans spans = q.getSpans(context, acceptDocs, termContexts);
 
       // collect span positions
Index: lucene/contrib/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterPhraseTest.java
===================================================================
--- lucene/contrib/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterPhraseTest.java	(revision 1238034)
+++ lucene/contrib/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterPhraseTest.java	(working copy)
@@ -30,11 +30,11 @@
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.TextField;
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.CorruptIndexException;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexWriter;
 import org.apache.lucene.index.Term;
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
 import org.apache.lucene.search.Collector;
 import org.apache.lucene.search.IndexSearcher;
 import org.apache.lucene.search.PhraseQuery;
Index: lucene/contrib/memory/src/java/org/apache/lucene/index/memory/MemoryIndex.java
===================================================================
--- lucene/contrib/memory/src/java/org/apache/lucene/index/memory/MemoryIndex.java	(revision 1238034)
+++ lucene/contrib/memory/src/java/org/apache/lucene/index/memory/MemoryIndex.java	(working copy)
@@ -33,6 +33,8 @@
 import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;
 import org.apache.lucene.analysis.tokenattributes.TermToBytesRefAttribute;
 import org.apache.lucene.document.Document;
+import org.apache.lucene.index.AtomicReader;
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.Norm;
 import org.apache.lucene.index.DocValues;
 import org.apache.lucene.index.DocsAndPositionsEnum;
@@ -41,7 +43,6 @@
 import org.apache.lucene.index.FieldInvertState;
 import org.apache.lucene.index.Fields;
 import org.apache.lucene.index.FieldsEnum;
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.OrdTermState;
 import org.apache.lucene.index.StoredFieldVisitor;
@@ -749,10 +750,9 @@
    * Search support for Lucene framework integration; implements all methods
    * required by the Lucene IndexReader contracts.
    */
-  private final class MemoryIndexReader extends IndexReader {
+  private final class MemoryIndexReader extends AtomicReader {
     
     private IndexSearcher searcher; // needed to find searcher.getSimilarity() 
-    private final ReaderContext readerInfos = new AtomicReaderContext(this);
     
     private MemoryIndexReader() {
       super(); // avoid as much superclass baggage as possible
@@ -776,20 +776,6 @@
       return fieldInfos;
     }
 
-    @Override
-    public int docFreq(String field, BytesRef term) {
-      Info info = getInfo(field);
-      int freq = 0;
-      if (info != null) freq = info.getPositions(term) != null ? 1 : 0;
-      if (DEBUG) System.err.println("MemoryIndexReader.docFreq: " + field + ":" + term + ", freq:" + freq);
-      return freq;
-    }
-    
-    @Override
-    public ReaderContext getTopReaderContext() {
-      return readerInfos;
-    }
-
     private class MemoryFields extends Fields {
       @Override
       public FieldsEnum iterator() {
Index: lucene/contrib/memory/src/test/org/apache/lucene/index/memory/MemoryIndexTest.java
===================================================================
--- lucene/contrib/memory/src/test/org/apache/lucene/index/memory/MemoryIndexTest.java	(revision 1238034)
+++ lucene/contrib/memory/src/test/org/apache/lucene/index/memory/MemoryIndexTest.java	(working copy)
@@ -33,6 +33,7 @@
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.TextField;
+import org.apache.lucene.index.AtomicReader;
 import org.apache.lucene.index.DocsAndPositionsEnum;
 import org.apache.lucene.index.DocsEnum;
 import org.apache.lucene.index.IndexReader;
@@ -185,7 +186,7 @@
     Analyzer analyzer = new MockAnalyzer(random);
     MemoryIndex memory = new MemoryIndex();
     memory.addField("foo", "bar", analyzer);
-    IndexReader reader = memory.createSearcher().getIndexReader();
+    AtomicReader reader = (AtomicReader) memory.createSearcher().getIndexReader();
     DocsEnum disi = _TestUtil.docs(random, reader, "foo", new BytesRef("bar"), null, null, false);
     int docid = disi.docID();
     assertTrue(docid == -1 || docid == DocIdSetIterator.NO_MORE_DOCS);
@@ -205,7 +206,7 @@
     Analyzer analyzer = new MockAnalyzer(random);
     MemoryIndex memory = new MemoryIndex();
     memory.addField("foo", "bar", analyzer);
-    IndexReader reader = memory.createSearcher().getIndexReader();
+    AtomicReader reader = (AtomicReader) memory.createSearcher().getIndexReader();
     DocsAndPositionsEnum disi = reader.termPositionsEnum(null, "foo", new BytesRef("bar"), false);
     int docid = disi.docID();
     assertTrue(docid == -1 || docid == DocIdSetIterator.NO_MORE_DOCS);
Index: lucene/contrib/misc/src/java/org/apache/lucene/index/MultiPassIndexSplitter.java
===================================================================
--- lucene/contrib/misc/src/java/org/apache/lucene/index/MultiPassIndexSplitter.java	(revision 1238034)
+++ lucene/contrib/misc/src/java/org/apache/lucene/index/MultiPassIndexSplitter.java	(working copy)
@@ -135,7 +135,7 @@
         }
         Directory dir = FSDirectory.open(new File(args[i]));
         try {
-          if (!IndexReader.indexExists(dir)) {
+          if (!DirectoryReader.indexExists(dir)) {
             System.err.println("Invalid input index - skipping: " + file);
             continue;
           }
@@ -143,7 +143,7 @@
           System.err.println("Invalid input index - skipping: " + file);
           continue;
         }
-        indexes.add(IndexReader.open(dir));
+        indexes.add(DirectoryReader.open(dir));
       }
     }
     if (outDir == null) {
@@ -182,15 +182,15 @@
       super(initSubReaders(reader), false /* dont close */);
     }
     
-    private static IndexReader[] initSubReaders(IndexReader reader) throws IOException {
-      final ArrayList<IndexReader> subs = new ArrayList<IndexReader>();
+    private static AtomicReader[] initSubReaders(IndexReader reader) throws IOException {
+      final ArrayList<AtomicReader> subs = new ArrayList<AtomicReader>();
       new ReaderUtil.Gather(reader) {
         @Override
-        protected void add(int base, IndexReader r) {
+        protected void add(int base, AtomicReader r) {
           subs.add(new FakeDeleteAtomicIndexReader(r));
         }
       }.run();
-      return subs.toArray(new IndexReader[subs.size()]);
+      return subs.toArray(new AtomicReader[subs.size()]);
     }
         
     public void deleteDocument(int docID) {
@@ -226,7 +226,7 @@
   private static final class FakeDeleteAtomicIndexReader extends FilterIndexReader {
     FixedBitSet liveDocs;
 
-    public FakeDeleteAtomicIndexReader(IndexReader reader) {
+    public FakeDeleteAtomicIndexReader(AtomicReader reader) {
       super(reader);
       undeleteAll(); // initialize main bitset
     }
Index: lucene/contrib/misc/src/java/org/apache/lucene/index/PKIndexSplitter.java
===================================================================
--- lucene/contrib/misc/src/java/org/apache/lucene/index/PKIndexSplitter.java	(revision 1238034)
+++ lucene/contrib/misc/src/java/org/apache/lucene/index/PKIndexSplitter.java	(working copy)
@@ -20,7 +20,6 @@
 import java.io.IOException;
 
 import org.apache.lucene.index.IndexWriterConfig.OpenMode;
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
 import org.apache.lucene.search.DocIdSet;
 import org.apache.lucene.search.DocIdSetIterator;
 import org.apache.lucene.search.Filter;
@@ -84,7 +83,7 @@
   
   public void split() throws IOException {
     boolean success = false;
-    IndexReader reader = IndexReader.open(input);
+    DirectoryReader reader = DirectoryReader.open(input);
     try {
       // pass an individual config in here since one config can not be reused!
       createIndex(config1, dir1, reader, docsInFirstIndex, false);
@@ -124,7 +123,7 @@
     final int numDocs;
     
     public DocumentFilteredAtomicIndexReader(AtomicReaderContext context, Filter preserveFilter, boolean negateFilter) throws IOException {
-      super(context.reader);
+      super(context.reader());
       final int maxDoc = in.maxDoc();
       final FixedBitSet bits = new FixedBitSet(maxDoc);
       // ignore livedocs here, as we filter them later:
Index: lucene/contrib/misc/src/java/org/apache/lucene/misc/HighFreqTerms.java
===================================================================
--- lucene/contrib/misc/src/java/org/apache/lucene/misc/HighFreqTerms.java	(revision 1238034)
+++ lucene/contrib/misc/src/java/org/apache/lucene/misc/HighFreqTerms.java	(working copy)
@@ -17,6 +17,7 @@
  * limitations under the License.
  */
 
+import org.apache.lucene.index.AtomicReader;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.MultiFields;
 import org.apache.lucene.index.Fields;
@@ -187,7 +188,7 @@
     new ReaderUtil.Gather(reader) {
 
       @Override
-      protected void add(int base, IndexReader r) throws IOException {
+      protected void add(int base, AtomicReader r) throws IOException {
         Bits liveDocs = r.getLiveDocs();
         if (liveDocs == null) {
           // TODO: we could do this up front, during the scan
Index: lucene/contrib/misc/src/test/org/apache/lucene/index/TestIndexSplitter.java
===================================================================
--- lucene/contrib/misc/src/test/org/apache/lucene/index/TestIndexSplitter.java	(revision 1238034)
+++ lucene/contrib/misc/src/test/org/apache/lucene/index/TestIndexSplitter.java	(working copy)
@@ -60,7 +60,7 @@
       iw.addDocument(doc);
     }
     iw.commit();
-    IndexReader iwReader = iw.getReader();
+    DirectoryReader iwReader = iw.getReader();
     assertEquals(3, iwReader.getSequentialSubReaders().length);
     iwReader.close();
     iw.close();
@@ -69,7 +69,7 @@
     String splitSegName = is.infos.info(1).name;
     is.split(destDir, new String[] {splitSegName});
     Directory fsDirDest = newFSDirectory(destDir);
-    IndexReader r = IndexReader.open(fsDirDest);
+    DirectoryReader r = DirectoryReader.open(fsDirDest);
     assertEquals(50, r.maxDoc());
     r.close();
     fsDirDest.close();
@@ -81,14 +81,14 @@
     IndexSplitter.main(new String[] {dir.getAbsolutePath(), destDir2.getAbsolutePath(), splitSegName});
     assertEquals(4, destDir2.listFiles().length);
     Directory fsDirDest2 = newFSDirectory(destDir2);
-    r = IndexReader.open(fsDirDest2);
+    r = DirectoryReader.open(fsDirDest2);
     assertEquals(50, r.maxDoc());
     r.close();
     fsDirDest2.close();
     
     // now remove the copied segment from src
     IndexSplitter.main(new String[] {dir.getAbsolutePath(), "-d", splitSegName});
-    r = IndexReader.open(fsDir);
+    r = DirectoryReader.open(fsDir);
     assertEquals(2, r.getSequentialSubReaders().length);
     r.close();
     fsDir.close();
Index: lucene/contrib/sandbox/src/java/org/apache/lucene/sandbox/queries/DuplicateFilter.java
===================================================================
--- lucene/contrib/sandbox/src/java/org/apache/lucene/sandbox/queries/DuplicateFilter.java	(revision 1238034)
+++ lucene/contrib/sandbox/src/java/org/apache/lucene/sandbox/queries/DuplicateFilter.java	(working copy)
@@ -17,7 +17,6 @@
  */
 
 import org.apache.lucene.index.*;
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
 import org.apache.lucene.search.DocIdSet;
 import org.apache.lucene.search.Filter;
 import org.apache.lucene.util.Bits;
@@ -72,13 +71,13 @@
   @Override
   public DocIdSet getDocIdSet(AtomicReaderContext context, Bits acceptDocs) throws IOException {
     if (processingMode == ProcessingMode.PM_FAST_INVALIDATION) {
-      return fastBits(context.reader, acceptDocs);
+      return fastBits(context.reader(), acceptDocs);
     } else {
-      return correctBits(context.reader, acceptDocs);
+      return correctBits(context.reader(), acceptDocs);
     }
   }
 
-  private FixedBitSet correctBits(IndexReader reader, Bits acceptDocs) throws IOException {
+  private FixedBitSet correctBits(AtomicReader reader, Bits acceptDocs) throws IOException {
     FixedBitSet bits = new FixedBitSet(reader.maxDoc()); //assume all are INvalid
     Terms terms = reader.fields().terms(fieldName);
 
@@ -115,7 +114,7 @@
     return bits;
   }
 
-  private FixedBitSet fastBits(IndexReader reader, Bits acceptDocs) throws IOException {
+  private FixedBitSet fastBits(AtomicReader reader, Bits acceptDocs) throws IOException {
     FixedBitSet bits = new FixedBitSet(reader.maxDoc());
     bits.set(0, reader.maxDoc()); //assume all are valid
     Terms terms = reader.fields().terms(fieldName);
Index: lucene/contrib/sandbox/src/java/org/apache/lucene/sandbox/queries/SlowCollatedStringComparator.java
===================================================================
--- lucene/contrib/sandbox/src/java/org/apache/lucene/sandbox/queries/SlowCollatedStringComparator.java	(revision 1238034)
+++ lucene/contrib/sandbox/src/java/org/apache/lucene/sandbox/queries/SlowCollatedStringComparator.java	(working copy)
@@ -20,7 +20,7 @@
 import java.io.IOException;
 import java.text.Collator;
 
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.search.FieldCache;
 import org.apache.lucene.search.FieldCache.DocTerms;
 import org.apache.lucene.search.FieldComparator;
@@ -91,7 +91,7 @@
 
   @Override
   public FieldComparator setNextReader(AtomicReaderContext context) throws IOException {
-    currentDocTerms = FieldCache.DEFAULT.getTerms(context.reader, field);
+    currentDocTerms = FieldCache.DEFAULT.getTerms(context.reader(), field);
     return this;
   }
   
Index: lucene/contrib/sandbox/src/test/org/apache/lucene/sandbox/queries/regex/TestSpanRegexQuery.java
===================================================================
--- lucene/contrib/sandbox/src/test/org/apache/lucene/sandbox/queries/regex/TestSpanRegexQuery.java	(revision 1238034)
+++ lucene/contrib/sandbox/src/test/org/apache/lucene/sandbox/queries/regex/TestSpanRegexQuery.java	(working copy)

Property changes on: lucene/contrib/sandbox/src/test/org/apache/lucene/sandbox/queries/regex/TestSpanRegexQuery.java
___________________________________________________________________
Modified: svn:mergeinfo
   Merged /lucene/dev/branches/lucene2858/lucene/contrib/sandbox/src/test/org/apache/lucene/sandbox/queries/regex/TestSpanRegexQuery.java:r1234440-1238051
Index: lucene/contrib/spatial/src/java/org/apache/lucene/spatial/geohash/GeoHashDistanceFilter.java
===================================================================
--- lucene/contrib/spatial/src/java/org/apache/lucene/spatial/geohash/GeoHashDistanceFilter.java	(revision 1238034)
+++ lucene/contrib/spatial/src/java/org/apache/lucene/spatial/geohash/GeoHashDistanceFilter.java	(working copy)
@@ -19,7 +19,7 @@
 
 import java.io.IOException;
 
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.search.FieldCache;
 import org.apache.lucene.search.FieldCache.DocTerms;
 import org.apache.lucene.search.Filter;
@@ -60,11 +60,11 @@
   @Override
   public DocIdSet getDocIdSet(AtomicReaderContext context, Bits acceptDocs) throws IOException {
 
-    final DocTerms geoHashValues = FieldCache.DEFAULT.getTerms(context.reader, geoHashField);
+    final DocTerms geoHashValues = FieldCache.DEFAULT.getTerms(context.reader(), geoHashField);
     final BytesRef br = new BytesRef();
 
     final int docBase = nextDocBase;
-    nextDocBase += context.reader.maxDoc();
+    nextDocBase += context.reader().maxDoc();
 
     return new FilteredDocIdSet(startingFilter.getDocIdSet(context, acceptDocs)) {
       @Override
Index: lucene/contrib/spatial/src/java/org/apache/lucene/spatial/tier/CartesianShapeFilter.java
===================================================================
--- lucene/contrib/spatial/src/java/org/apache/lucene/spatial/tier/CartesianShapeFilter.java	(revision 1238034)
+++ lucene/contrib/spatial/src/java/org/apache/lucene/spatial/tier/CartesianShapeFilter.java	(working copy)
@@ -19,8 +19,8 @@
 import java.io.IOException;
 import java.util.List;
 
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.DocsEnum;
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
 import org.apache.lucene.search.Filter;
 import org.apache.lucene.search.DocIdSet;
 import org.apache.lucene.search.DocIdSetIterator;
@@ -57,7 +57,7 @@
       return new DocIdSet() {
         @Override
         public DocIdSetIterator iterator() throws IOException {
-          return context.reader.termDocsEnum(acceptDocs, fieldName, bytesRef, false);
+          return context.reader().termDocsEnum(acceptDocs, fieldName, bytesRef, false);
         }
         
         @Override
@@ -66,11 +66,11 @@
         }
       };
     } else {
-      final FixedBitSet bits = new FixedBitSet(context.reader.maxDoc());
+      final FixedBitSet bits = new FixedBitSet(context.reader().maxDoc());
       for (int i =0; i< sz; i++) {
         double boxId = area.get(i).doubleValue();
         NumericUtils.longToPrefixCoded(NumericUtils.doubleToSortableLong(boxId), 0, bytesRef);
-        final DocsEnum docsEnum = context.reader.termDocsEnum(acceptDocs, fieldName, bytesRef, false);
+        final DocsEnum docsEnum = context.reader().termDocsEnum(acceptDocs, fieldName, bytesRef, false);
         if (docsEnum == null) continue;
         // iterate through all documents
         // which have this boxId
Index: lucene/contrib/spatial/src/java/org/apache/lucene/spatial/tier/DistanceFieldComparatorSource.java
===================================================================
--- lucene/contrib/spatial/src/java/org/apache/lucene/spatial/tier/DistanceFieldComparatorSource.java	(revision 1238034)
+++ lucene/contrib/spatial/src/java/org/apache/lucene/spatial/tier/DistanceFieldComparatorSource.java	(working copy)
@@ -19,7 +19,7 @@
 
 import java.io.IOException;
 
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.search.Filter;
 import org.apache.lucene.search.FieldComparator;
 import org.apache.lucene.search.FieldComparatorSource;
Index: lucene/contrib/spatial/src/java/org/apache/lucene/spatial/tier/LatLongDistanceFilter.java
===================================================================
--- lucene/contrib/spatial/src/java/org/apache/lucene/spatial/tier/LatLongDistanceFilter.java	(revision 1238034)
+++ lucene/contrib/spatial/src/java/org/apache/lucene/spatial/tier/LatLongDistanceFilter.java	(working copy)
@@ -19,7 +19,7 @@
 
 import java.io.IOException;
 
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.search.FilteredDocIdSet;
 import org.apache.lucene.search.FieldCache;
 import org.apache.lucene.search.Filter;
@@ -63,11 +63,11 @@
   @Override
   public DocIdSet getDocIdSet(AtomicReaderContext context, Bits acceptDocs) throws IOException {
 
-    final double[] latIndex = FieldCache.DEFAULT.getDoubles(context.reader, latField, false);
-    final double[] lngIndex = FieldCache.DEFAULT.getDoubles(context.reader, lngField, false);
+    final double[] latIndex = FieldCache.DEFAULT.getDoubles(context.reader(), latField, false);
+    final double[] lngIndex = FieldCache.DEFAULT.getDoubles(context.reader(), lngField, false);
 
     final int docBase = nextDocBase;
-    nextDocBase += context.reader.maxDoc();
+    nextDocBase += context.reader().maxDoc();
 
     return new FilteredDocIdSet(startingFilter.getDocIdSet(context, acceptDocs)) {
       @Override
Index: lucene/contrib/spatial/src/test/org/apache/lucene/spatial/tier/TestDistance.java
===================================================================
--- lucene/contrib/spatial/src/test/org/apache/lucene/spatial/tier/TestDistance.java	(revision 1238034)
+++ lucene/contrib/spatial/src/test/org/apache/lucene/spatial/tier/TestDistance.java	(working copy)
@@ -23,8 +23,8 @@
 import org.apache.lucene.document.FieldType;
 import org.apache.lucene.document.NumericField;
 import org.apache.lucene.document.TextField;
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.FieldInfo.IndexOptions;
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexWriter;
 import org.apache.lucene.index.Term;
@@ -119,7 +119,7 @@
 
     AtomicReaderContext[] leaves = ReaderUtil.leaves(r.getTopReaderContext());
     for (int i = 0; i < leaves.length; i++) {
-      f.getDocIdSet(leaves[i], leaves[i].reader.getLiveDocs());
+      f.getDocIdSet(leaves[i], leaves[i].reader().getLiveDocs());
     }
     r.close();
   }
Index: lucene/src/java/org/apache/lucene/codecs/lucene3x/TermInfosReaderIndex.java
===================================================================
--- lucene/src/java/org/apache/lucene/codecs/lucene3x/TermInfosReaderIndex.java	(revision 1238034)
+++ lucene/src/java/org/apache/lucene/codecs/lucene3x/TermInfosReaderIndex.java	(working copy)

Property changes on: lucene/src/java/org/apache/lucene/codecs/lucene3x/TermInfosReaderIndex.java
___________________________________________________________________
Modified: svn:mergeinfo
   Merged /lucene/dev/branches/lucene2858/lucene/src/java/org/apache/lucene/codecs/lucene3x/TermInfosReaderIndex.java:r1234440-1238051
Index: lucene/src/java/org/apache/lucene/codecs/lucene40/Lucene40NormsFormat.java
===================================================================
--- lucene/src/java/org/apache/lucene/codecs/lucene40/Lucene40NormsFormat.java	(revision 1238034)
+++ lucene/src/java/org/apache/lucene/codecs/lucene40/Lucene40NormsFormat.java	(working copy)
@@ -26,7 +26,7 @@
 import org.apache.lucene.index.FieldInfo;
 import org.apache.lucene.index.FieldInfos;
 import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.AtomicReader;
 import org.apache.lucene.index.PerDocWriteState;
 import org.apache.lucene.index.SegmentInfo;
 import org.apache.lucene.index.SegmentReadState;
@@ -85,7 +85,7 @@
     }
 
     @Override
-    protected DocValues getDocValuesForMerge(IndexReader reader, FieldInfo info)
+    protected DocValues getDocValuesForMerge(AtomicReader reader, FieldInfo info)
         throws IOException {
       return reader.normValues(info.name);
     }
Index: lucene/src/java/org/apache/lucene/codecs/PerDocConsumer.java
===================================================================
--- lucene/src/java/org/apache/lucene/codecs/PerDocConsumer.java	(revision 1238034)
+++ lucene/src/java/org/apache/lucene/codecs/PerDocConsumer.java	(working copy)
@@ -20,7 +20,7 @@
 
 import org.apache.lucene.index.FieldInfo;
 import org.apache.lucene.index.DocValues;
-import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.AtomicReader;
 import org.apache.lucene.index.MergeState;
 import org.apache.lucene.index.DocValues.Type;
 
@@ -65,13 +65,13 @@
   /**
    * Returns a {@link DocValues} instance for merging from the given reader for the given
    * {@link FieldInfo}. This method is used for merging and uses
-   * {@link IndexReader#docValues(String)} by default.
+   * {@link AtomicReader#docValues(String)} by default.
    * <p>
    * To enable {@link DocValues} merging for different {@link DocValues} than
    * the default override this method accordingly.
    * <p>
    */
-  protected DocValues getDocValuesForMerge(IndexReader reader, FieldInfo info) throws IOException {
+  protected DocValues getDocValuesForMerge(AtomicReader reader, FieldInfo info) throws IOException {
     return reader.docValues(info.name);
   }
   
Index: lucene/src/java/org/apache/lucene/codecs/simpletext/SimpleTextNormsConsumer.java
===================================================================
--- lucene/src/java/org/apache/lucene/codecs/simpletext/SimpleTextNormsConsumer.java	(revision 1238034)
+++ lucene/src/java/org/apache/lucene/codecs/simpletext/SimpleTextNormsConsumer.java	(working copy)
@@ -28,7 +28,7 @@
 import org.apache.lucene.index.FieldInfo;
 import org.apache.lucene.index.FieldInfos;
 import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.AtomicReader;
 import org.apache.lucene.index.IndexableField;
 import org.apache.lucene.index.SegmentInfo;
 import org.apache.lucene.store.Directory;
@@ -87,7 +87,7 @@
   }
   
   @Override
-  protected DocValues getDocValuesForMerge(IndexReader reader, FieldInfo info)
+  protected DocValues getDocValuesForMerge(AtomicReader reader, FieldInfo info)
       throws IOException {
     return reader.normValues(info.name);
   }
Index: lucene/src/java/org/apache/lucene/index/AtomicReader.java
===================================================================
--- lucene/src/java/org/apache/lucene/index/AtomicReader.java	(revision 0)
+++ lucene/src/java/org/apache/lucene/index/AtomicReader.java	(working copy)
@@ -0,0 +1,248 @@
+package org.apache.lucene.index;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.search.SearcherManager; // javadocs
+import org.apache.lucene.store.*;
+import org.apache.lucene.util.Bits;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.ReaderUtil;         // for javadocs
+
+/** {@code AtomicReader} is an abstract class, providing an interface for accessing an
+ index.  Search of an index is done entirely through this abstract interface,
+ so that any subclass which implements it is searchable. IndexReaders implemented
+ by this subclass do not consist of several sub-readers,
+ they are atomic. They support retrieval of stored fields, doc values, terms,
+ and postings.
+
+ <p>For efficiency, in this API documents are often referred to via
+ <i>document numbers</i>, non-negative integers which each name a unique
+ document in the index.  These document numbers are ephemeral -- they may change
+ as documents are added to and deleted from an index.  Clients should thus not
+ rely on a given document having the same number between sessions.
+
+ <p>
+ <a name="thread-safety"></a><p><b>NOTE</b>: {@link
+ IndexReader} instances are completely thread
+ safe, meaning multiple threads can call any of its methods,
+ concurrently.  If your application requires external
+ synchronization, you should <b>not</b> synchronize on the
+ <code>IndexReader</code> instance; use your own
+ (non-Lucene) objects instead.
+*/
+public abstract class AtomicReader extends IndexReader {
+
+  private final AtomicReaderContext readerContext = new AtomicReaderContext(this);
+  
+  protected AtomicReader() {
+    super();
+  }
+
+  @Override
+  public final AtomicReaderContext getTopReaderContext() {
+    ensureOpen();
+    return readerContext;
+  }
+
+  /** Returns true if there are norms stored for this field. */
+  public boolean hasNorms(String field) throws IOException {
+    // backward compatible implementation.
+    // SegmentReader has an efficient implementation.
+    ensureOpen();
+    return normValues(field) != null;
+  }
+
+  /**
+   * Returns {@link Fields} for this reader.
+   * This method may return null if the reader has no
+   * postings.
+   */
+  public abstract Fields fields() throws IOException;
+  
+  @Override
+  public final int docFreq(String field, BytesRef term) throws IOException {
+    final Fields fields = fields();
+    if (fields == null) {
+      return 0;
+    }
+    final Terms terms = fields.terms(field);
+    if (terms == null) {
+      return 0;
+    }
+    final TermsEnum termsEnum = terms.iterator(null);
+    if (termsEnum.seekExact(term, true)) {
+      return termsEnum.docFreq();
+    } else {
+      return 0;
+    }
+  }
+
+  /** Returns the number of documents containing the term
+   * <code>t</code>.  This method returns 0 if the term or
+   * field does not exists.  This method does not take into
+   * account deleted documents that have not yet been merged
+   * away. */
+  public final long totalTermFreq(String field, BytesRef term) throws IOException {
+    final Fields fields = fields();
+    if (fields == null) {
+      return 0;
+    }
+    final Terms terms = fields.terms(field);
+    if (terms == null) {
+      return 0;
+    }
+    final TermsEnum termsEnum = terms.iterator(null);
+    if (termsEnum.seekExact(term, true)) {
+      return termsEnum.totalTermFreq();
+    } else {
+      return 0;
+    }
+  }
+
+  /** This may return null if the field does not exist.*/
+  public final Terms terms(String field) throws IOException {
+    final Fields fields = fields();
+    if (fields == null) {
+      return null;
+    }
+    return fields.terms(field);
+  }
+
+  /** Returns {@link DocsEnum} for the specified field &
+   *  term.  This may return null, if either the field or
+   *  term does not exist. */
+  public final DocsEnum termDocsEnum(Bits liveDocs, String field, BytesRef term, boolean needsFreqs) throws IOException {
+    assert field != null;
+    assert term != null;
+    final Fields fields = fields();
+    if (fields != null) {
+      final Terms terms = fields.terms(field);
+      if (terms != null) {
+        final TermsEnum termsEnum = terms.iterator(null);
+        if (termsEnum.seekExact(term, true)) {
+          return termsEnum.docs(liveDocs, null, needsFreqs);
+        }
+      }
+    }
+    return null;
+  }
+
+  /** Returns {@link DocsAndPositionsEnum} for the specified
+   *  field & term.  This may return null, if either the
+   *  field or term does not exist, or needsOffsets is
+   *  true but offsets were not indexed for this field. */
+  public final DocsAndPositionsEnum termPositionsEnum(Bits liveDocs, String field, BytesRef term, boolean needsOffsets) throws IOException {
+    assert field != null;
+    assert term != null;
+    final Fields fields = fields();
+    if (fields != null) {
+      final Terms terms = fields.terms(field);
+      if (terms != null) {
+        final TermsEnum termsEnum = terms.iterator(null);
+        if (termsEnum.seekExact(term, true)) {
+          return termsEnum.docsAndPositions(liveDocs, null, needsOffsets);
+        }
+      }
+    }
+    return null;
+  }
+  
+  /**
+   * Returns {@link DocsEnum} for the specified field and
+   * {@link TermState}. This may return null, if either the field or the term
+   * does not exists or the {@link TermState} is invalid for the underlying
+   * implementation.*/
+  public final DocsEnum termDocsEnum(Bits liveDocs, String field, BytesRef term, TermState state, boolean needsFreqs) throws IOException {
+    assert state != null;
+    assert field != null;
+    final Fields fields = fields();
+    if (fields != null) {
+      final Terms terms = fields.terms(field);
+      if (terms != null) {
+        final TermsEnum termsEnum = terms.iterator(null);
+        termsEnum.seekExact(term, state);
+        return termsEnum.docs(liveDocs, null, needsFreqs);
+      }
+    }
+    return null;
+  }
+  
+  /**
+   * Returns {@link DocsAndPositionsEnum} for the specified field and
+   * {@link TermState}. This may return null, if either the field or the term
+   * does not exists, the {@link TermState} is invalid for the underlying
+   * implementation, or needsOffsets is true but offsets
+   * were not indexed for this field. */
+  public final DocsAndPositionsEnum termPositionsEnum(Bits liveDocs, String field, BytesRef term, TermState state, boolean needsOffsets) throws IOException {
+    assert state != null;
+    assert field != null;
+    final Fields fields = fields();
+    if (fields != null) {
+      final Terms terms = fields.terms(field);
+      if (terms != null) {
+        final TermsEnum termsEnum = terms.iterator(null);
+        termsEnum.seekExact(term, state);
+        return termsEnum.docsAndPositions(liveDocs, null, needsOffsets);
+      }
+    }
+    return null;
+  }
+
+  /** Returns the number of unique terms (across all fields)
+   *  in this reader.
+   */
+  public final long getUniqueTermCount() throws IOException {
+    final Fields fields = fields();
+    if (fields == null) {
+      return 0;
+    }
+    return fields.getUniqueTermCount();
+  }
+  
+  /**
+   * Returns {@link DocValues} for this field.
+   * This method may return null if the reader has no per-document
+   * values stored.
+   */
+  public abstract DocValues docValues(String field) throws IOException;
+  
+  public abstract DocValues normValues(String field) throws IOException;
+
+  /**
+   * Get the {@link FieldInfos} describing all fields in
+   * this reader.  NOTE: do not make any changes to the
+   * returned FieldInfos!
+   *
+   * @lucene.experimental
+   */
+  public abstract FieldInfos getFieldInfos();
+  
+  /** Returns the {@link Bits} representing live (not
+   *  deleted) docs.  A set bit indicates the doc ID has not
+   *  been deleted.  If this method returns null it means
+   *  there are no deleted documents (all documents are
+   *  live).
+   *
+   *  The returned instance has been safely published for
+   *  use by multiple threads without additional
+   *  synchronization.
+   */
+  public abstract Bits getLiveDocs();
+}
Index: lucene/src/java/org/apache/lucene/index/AtomicReader.java
===================================================================
--- lucene/src/java/org/apache/lucene/index/AtomicReader.java	(revision 1238051)
+++ lucene/src/java/org/apache/lucene/index/AtomicReader.java	(working copy)

Property changes on: lucene/src/java/org/apache/lucene/index/AtomicReader.java
___________________________________________________________________
Added: cvs2svn:cvs-rev
## -0,0 +1 ##
+1.43
Added: svn:keywords
## -0,0 +1 ##
+Author Date Id Revision
Added: svn:eol-style
## -0,0 +1 ##
+native
Index: lucene/src/java/org/apache/lucene/index/AtomicReaderContext.java
===================================================================
--- lucene/src/java/org/apache/lucene/index/AtomicReaderContext.java	(revision 0)
+++ lucene/src/java/org/apache/lucene/index/AtomicReaderContext.java	(working copy)
@@ -0,0 +1,61 @@
+package org.apache.lucene.index;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * {@link IndexReaderContext} for {@link AtomicReader} instances
+ * @lucene.experimental
+ */
+public final class AtomicReaderContext extends IndexReaderContext {
+  /** The readers ord in the top-level's leaves array */
+  public final int ord;
+  /** The readers absolute doc base */
+  public final int docBase;
+  
+  private final AtomicReader reader;
+
+  /**
+   * Creates a new {@link AtomicReaderContext} 
+   */    
+  AtomicReaderContext(CompositeReaderContext parent, AtomicReader reader,
+      int ord, int docBase, int leafOrd, int leafDocBase) {
+    super(parent, ord, docBase);
+    this.ord = leafOrd;
+    this.docBase = leafDocBase;
+    this.reader = reader;
+  }
+  
+  AtomicReaderContext(AtomicReader atomicReader) {
+    this(null, atomicReader, 0, 0, 0, 0);
+  }
+  
+  @Override
+  public AtomicReaderContext[] leaves() {
+    return null;
+  }
+  
+  @Override
+  public IndexReaderContext[] children() {
+    return null;
+  }
+  
+  @Override
+  public AtomicReader reader() {
+    return reader;
+  }
+}
\ No newline at end of file
Index: lucene/src/java/org/apache/lucene/index/AtomicReaderContext.java
===================================================================
--- lucene/src/java/org/apache/lucene/index/AtomicReaderContext.java	(revision 1238051)
+++ lucene/src/java/org/apache/lucene/index/AtomicReaderContext.java	(working copy)

Property changes on: lucene/src/java/org/apache/lucene/index/AtomicReaderContext.java
___________________________________________________________________
Added: svn:keywords
## -0,0 +1 ##
+Date Author Id Revision HeadURL
Added: svn:eol-style
## -0,0 +1 ##
+native
Index: lucene/src/java/org/apache/lucene/index/BaseMultiReader.java
===================================================================
--- lucene/src/java/org/apache/lucene/index/BaseMultiReader.java	(revision 1238034)
+++ lucene/src/java/org/apache/lucene/index/BaseMultiReader.java	(working copy)
@@ -23,10 +23,9 @@
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.ReaderUtil;
 
-abstract class BaseMultiReader<R extends IndexReader> extends IndexReader {
+abstract class BaseMultiReader<R extends IndexReader> extends CompositeReader {
   protected final R[] subReaders;
   protected final int[] starts;       // 1st docno for each segment
-  private final ReaderContext topLevelContext;
   private final int maxDoc;
   private final int numDocs;
   private final boolean hasDeletions;
@@ -40,7 +39,6 @@
       starts[i] = maxDoc;
       maxDoc += subReaders[i].maxDoc();      // compute maxDocs
       numDocs += subReaders[i].numDocs();    // compute numDocs
-
       if (subReaders[i].hasDeletions()) {
         hasDeletions = true;
       }
@@ -49,28 +47,9 @@
     this.maxDoc = maxDoc;
     this.numDocs = numDocs;
     this.hasDeletions = hasDeletions;
-    topLevelContext = ReaderUtil.buildReaderContext(this);
   }
-  
-  @Override
-  public FieldInfos getFieldInfos() {
-    throw new UnsupportedOperationException("call getFieldInfos() on each sub reader, or use ReaderUtil.getMergedFieldInfos, instead");
-  }
 
   @Override
-  public Fields fields() throws IOException {
-    throw new UnsupportedOperationException("please use MultiFields.getFields, or wrap your IndexReader with SlowMultiReaderWrapper, if you really need a top level Fields");
-  }
-
-  @Override
-  protected abstract IndexReader doOpenIfChanged() throws CorruptIndexException, IOException;
-  
-  @Override
-  public Bits getLiveDocs() {
-    throw new UnsupportedOperationException("please use MultiFields.getLiveDocs, or wrap your IndexReader with SlowMultiReaderWrapper, if you really need a top level Bits liveDocs");
-  }
-
-  @Override
   public Fields getTermVectors(int docID) throws IOException {
     ensureOpen();
     final int i = readerIndex(docID);        // find segment num
@@ -102,24 +81,7 @@
     return hasDeletions;
   }
 
-  /** Helper method for subclasses to get the corresponding reader for a doc ID */
-  protected final int readerIndex(int docID) {
-    if (docID < 0 || docID >= maxDoc) {
-      throw new IllegalArgumentException("docID must be >= 0 and < maxDoc=" + maxDoc + " (got docID=" + docID + ")");
-    }
-    return ReaderUtil.subIndex(docID, this.starts);
-  }
-
   @Override
-  public boolean hasNorms(String field) throws IOException {
-    ensureOpen();
-    for (int i = 0; i < subReaders.length; i++) {
-      if (subReaders[i].hasNorms(field)) return true;
-    }
-    return false;
-  }
-  
-  @Override
   public int docFreq(String field, BytesRef t) throws IOException {
     ensureOpen();
     int total = 0;          // sum freqs in segments
@@ -129,23 +91,16 @@
     return total;
   }
 
+  /** Helper method for subclasses to get the corresponding reader for a doc ID */
+  protected final int readerIndex(int docID) {
+    if (docID < 0 || docID >= maxDoc) {
+      throw new IllegalArgumentException("docID must be >= 0 and < maxDoc=" + maxDoc + " (got docID=" + docID + ")");
+    }
+    return ReaderUtil.subIndex(docID, this.starts);
+  }
+  
   @Override
   public IndexReader[] getSequentialSubReaders() {
     return subReaders;
   }
-  
-  @Override
-  public ReaderContext getTopReaderContext() {
-    return topLevelContext;
-  }
-  
-  @Override
-  public DocValues docValues(String field) throws IOException {
-    throw new UnsupportedOperationException("please use MultiDocValues#getDocValues, or wrap your IndexReader with SlowMultiReaderWrapper, if you really need a top level DocValues");
-  }
-  
-  @Override
-  public DocValues normValues(String field) throws IOException {
-    throw new UnsupportedOperationException("please use MultiDocValues#getNormValues, or wrap your IndexReader with SlowMultiReaderWrapper, if you really need a top level Norm DocValues ");
-  }
 }
Index: lucene/src/java/org/apache/lucene/index/BufferedDeletesStream.java
===================================================================
--- lucene/src/java/org/apache/lucene/index/BufferedDeletesStream.java	(revision 1238034)
+++ lucene/src/java/org/apache/lucene/index/BufferedDeletesStream.java	(working copy)
@@ -25,7 +25,6 @@
 import java.util.concurrent.atomic.AtomicInteger;
 import java.util.concurrent.atomic.AtomicLong;
 
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
 import org.apache.lucene.search.DocIdSet;
 import org.apache.lucene.search.DocIdSetIterator;
 import org.apache.lucene.search.Query;
@@ -435,14 +434,14 @@
   }
 
   // Delete by query
-  private static long applyQueryDeletes(Iterable<QueryAndLimit> queriesIter, IndexWriter.ReadersAndLiveDocs rld, SegmentReader reader) throws IOException {
+  private static long applyQueryDeletes(Iterable<QueryAndLimit> queriesIter, IndexWriter.ReadersAndLiveDocs rld, final SegmentReader reader) throws IOException {
     long delCount = 0;
-    final AtomicReaderContext readerContext = (AtomicReaderContext) reader.getTopReaderContext();
+    final AtomicReaderContext readerContext = reader.getTopReaderContext();
     boolean any = false;
     for (QueryAndLimit ent : queriesIter) {
       Query query = ent.query;
       int limit = ent.limit;
-      final DocIdSet docs = new QueryWrapperFilter(query).getDocIdSet(readerContext, readerContext.reader.getLiveDocs());
+      final DocIdSet docs = new QueryWrapperFilter(query).getDocIdSet(readerContext, reader.getLiveDocs());
       if (docs != null) {
         final DocIdSetIterator it = docs.iterator();
         if (it != null) {
Index: lucene/src/java/org/apache/lucene/index/CheckIndex.java
===================================================================
--- lucene/src/java/org/apache/lucene/index/CheckIndex.java	(revision 1238034)
+++ lucene/src/java/org/apache/lucene/index/CheckIndex.java	(working copy)
@@ -537,7 +537,7 @@
         }
         if (infoStream != null)
           infoStream.print("    test: open reader.........");
-        reader = new SegmentReader(info, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR, IOContext.DEFAULT);
+        reader = new SegmentReader(info, DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, IOContext.DEFAULT);
 
         segInfoStat.openReaderPassed = true;
 
Index: lucene/src/java/org/apache/lucene/index/CompositeReader.java
===================================================================
--- lucene/src/java/org/apache/lucene/index/CompositeReader.java	(revision 0)
+++ lucene/src/java/org/apache/lucene/index/CompositeReader.java	(working copy)
@@ -0,0 +1,97 @@
+package org.apache.lucene.index;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.search.SearcherManager; // javadocs
+import org.apache.lucene.store.*;
+
+/** Instances of this reader type can only
+  be used to get stored fields from the underlying AtomicReaders,
+  but it is not possible to directly retrieve postings. To do that, get
+  the sub-readers via {@link #getSequentialSubReaders}.
+  Alternatively, you can mimic an {@link AtomicReader} (with a serious slowdown),
+  by wrapping composite readers with {@link SlowCompositeReaderWrapper}.
+ 
+ <p>IndexReader instances for indexes on disk are usually constructed
+ with a call to one of the static <code>DirectoryReader,open()</code> methods,
+ e.g. {@link DirectoryReader#open(Directory)}. {@link DirectoryReader} implements
+ the {@code CompositeReader} interface, it is not possible to directly get postings.
+ <p> Concrete subclasses of IndexReader are usually constructed with a call to
+ one of the static <code>open()</code> methods, e.g. {@link
+ #open(Directory)}.
+
+ <p> For efficiency, in this API documents are often referred to via
+ <i>document numbers</i>, non-negative integers which each name a unique
+ document in the index.  These document numbers are ephemeral -- they may change
+ as documents are added to and deleted from an index.  Clients should thus not
+ rely on a given document having the same number between sessions.
+
+ <p>
+ <a name="thread-safety"></a><p><b>NOTE</b>: {@link
+ IndexReader} instances are completely thread
+ safe, meaning multiple threads can call any of its methods,
+ concurrently.  If your application requires external
+ synchronization, you should <b>not</b> synchronize on the
+ <code>IndexReader</code> instance; use your own
+ (non-Lucene) objects instead.
+*/
+public abstract class CompositeReader extends IndexReader {
+
+  private volatile CompositeReaderContext readerContext = null; // lazy init
+
+  protected CompositeReader() { 
+    super();
+  }
+  
+  @Override
+  public String toString() {
+    final StringBuilder buffer = new StringBuilder();
+    buffer.append(getClass().getSimpleName());
+    buffer.append('(');
+    final IndexReader[] subReaders = getSequentialSubReaders();
+    assert subReaders != null;
+    if (subReaders.length > 0) {
+      buffer.append(subReaders[0]);
+      for (int i = 1; i < subReaders.length; ++i) {
+        buffer.append(" ").append(subReaders[i]);
+      }
+    }
+    buffer.append(')');
+    return buffer.toString();
+  }
+  
+  /** Expert: returns the sequential sub readers that this
+   *  reader is logically composed of. It contrast to previous
+   *  Lucene versions may not return null.
+   *  If this method returns an empty array, that means this
+   *  reader is a null reader (for example a MultiReader
+   *  that has no sub readers).
+   */
+  public abstract IndexReader[] getSequentialSubReaders();
+
+  @Override
+  public final CompositeReaderContext getTopReaderContext() {
+    ensureOpen();
+    // lazy init without thread safety for perf reasons: Building the readerContext twice does not hurt!
+    if (readerContext == null) {
+      assert getSequentialSubReaders() != null;
+      readerContext = CompositeReaderContext.create(this);
+    }
+    return readerContext;
+  }
+}
Index: lucene/src/java/org/apache/lucene/index/CompositeReader.java
===================================================================
--- lucene/src/java/org/apache/lucene/index/CompositeReader.java	(revision 1238051)
+++ lucene/src/java/org/apache/lucene/index/CompositeReader.java	(working copy)

Property changes on: lucene/src/java/org/apache/lucene/index/CompositeReader.java
___________________________________________________________________
Added: cvs2svn:cvs-rev
## -0,0 +1 ##
+1.43
Added: svn:keywords
## -0,0 +1 ##
+Author Date Id Revision
Added: svn:eol-style
## -0,0 +1 ##
+native
Index: lucene/src/java/org/apache/lucene/index/CompositeReaderContext.java
===================================================================
--- lucene/src/java/org/apache/lucene/index/CompositeReaderContext.java	(revision 0)
+++ lucene/src/java/org/apache/lucene/index/CompositeReaderContext.java	(working copy)
@@ -0,0 +1,136 @@
+package org.apache.lucene.index;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import org.apache.lucene.util.ReaderUtil;
+
+/**
+ * {@link IndexReaderContext} for {@link CompositeReader} instance.
+ * @lucene.experimental
+ */
+public final class CompositeReaderContext extends IndexReaderContext {
+  private final IndexReaderContext[] children;
+  private final AtomicReaderContext[] leaves;
+  private final CompositeReader reader;
+  
+  static CompositeReaderContext create(CompositeReader reader) {
+    return new Builder(reader).build();
+  }
+
+  /**
+   * Creates a {@link CompositeReaderContext} for intermediate readers that aren't
+   * not top-level readers in the current context
+   */
+  CompositeReaderContext(CompositeReaderContext parent, CompositeReader reader,
+      int ordInParent, int docbaseInParent, IndexReaderContext[] children) {
+    this(parent, reader, ordInParent, docbaseInParent, children, null);
+  }
+  
+  /**
+   * Creates a {@link CompositeReaderContext} for top-level readers with parent set to <code>null</code>
+   */
+  CompositeReaderContext(CompositeReader reader, IndexReaderContext[] children, AtomicReaderContext[] leaves) {
+    this(null, reader, 0, 0, children, leaves);
+  }
+  
+  private CompositeReaderContext(CompositeReaderContext parent, CompositeReader reader,
+      int ordInParent, int docbaseInParent, IndexReaderContext[] children,
+      AtomicReaderContext[] leaves) {
+    super(parent, ordInParent, docbaseInParent);
+    this.children = children;
+    this.leaves = leaves;
+    this.reader = reader;
+  }
+
+  @Override
+  public AtomicReaderContext[] leaves() {
+    return leaves;
+  }
+  
+  
+  @Override
+  public IndexReaderContext[] children() {
+    return children;
+  }
+  
+  @Override
+  public CompositeReader reader() {
+    return reader;
+  }
+  
+  private static final class Builder {
+    private final CompositeReader reader;
+    private final AtomicReaderContext[] leaves;
+    private int leafOrd = 0;
+    private int leafDocBase = 0;
+    
+    public Builder(CompositeReader reader) {
+      this.reader = reader;
+      leaves = new AtomicReaderContext[numLeaves(reader)];
+    }
+    
+    public CompositeReaderContext build() {
+      return (CompositeReaderContext) build(null, reader, 0, 0);
+    }
+    
+    private IndexReaderContext build(CompositeReaderContext parent, IndexReader reader, int ord, int docBase) {
+      if (reader instanceof AtomicReader) {
+        final AtomicReader ar = (AtomicReader) reader;
+        final AtomicReaderContext atomic = new AtomicReaderContext(parent, ar, ord, docBase, leafOrd, leafDocBase);
+        leaves[leafOrd++] = atomic;
+        leafDocBase += reader.maxDoc();
+        return atomic;
+      } else {
+        final CompositeReader cr = (CompositeReader) reader;
+        final IndexReader[] sequentialSubReaders = cr.getSequentialSubReaders();
+        final IndexReaderContext[] children = new IndexReaderContext[sequentialSubReaders.length];
+        final CompositeReaderContext newParent;
+        if (parent == null) {
+          newParent = new CompositeReaderContext(cr, children, leaves);
+        } else {
+          newParent = new CompositeReaderContext(parent, cr, ord, docBase, children);
+        }
+        int newDocBase = 0;
+        for (int i = 0; i < sequentialSubReaders.length; i++) {
+          children[i] = build(newParent, sequentialSubReaders[i], i, newDocBase);
+          newDocBase += sequentialSubReaders[i].maxDoc();
+        }
+        return newParent;
+      }
+    }
+    
+    private int numLeaves(IndexReader reader) {
+      final int[] numLeaves = new int[1];
+      try {
+        new ReaderUtil.Gather(reader) {
+          @Override
+          protected void add(int base, AtomicReader r) {
+            numLeaves[0]++;
+          }
+        }.run();
+      } catch (IOException ioe) {
+        // won't happen
+        throw new RuntimeException(ioe);
+      }
+      return numLeaves[0];
+    }
+    
+  }
+
+}
\ No newline at end of file
Index: lucene/src/java/org/apache/lucene/index/CompositeReaderContext.java
===================================================================
--- lucene/src/java/org/apache/lucene/index/CompositeReaderContext.java	(revision 1238051)
+++ lucene/src/java/org/apache/lucene/index/CompositeReaderContext.java	(working copy)

Property changes on: lucene/src/java/org/apache/lucene/index/CompositeReaderContext.java
___________________________________________________________________
Added: svn:keywords
## -0,0 +1 ##
+Date Author Id Revision HeadURL
Added: svn:eol-style
## -0,0 +1 ##
+native
Index: lucene/src/java/org/apache/lucene/index/DirectoryReader.java
===================================================================
--- lucene/src/java/org/apache/lucene/index/DirectoryReader.java	(revision 1238034)
+++ lucene/src/java/org/apache/lucene/index/DirectoryReader.java	(working copy)
@@ -26,20 +26,126 @@
 import java.util.List;
 import java.util.Map;
 
+import org.apache.lucene.search.SearcherManager; // javadocs
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.IOContext;
 import org.apache.lucene.util.IOUtils;
 
-/** 
- * An IndexReader which reads indexes with multiple segments.
- */
-final class DirectoryReader extends BaseMultiReader<SegmentReader> {
+/** DirectoryReader is an implementation of {@link CompositeReader}
+ that can read indexes in a {@link Directory}. 
+
+ <p>DirectoryReader instances are usually constructed with a call to
+ one of the static <code>open()</code> methods, e.g. {@link
+ #open(Directory)}.
+
+ <p> For efficiency, in this API documents are often referred to via
+ <i>document numbers</i>, non-negative integers which each name a unique
+ document in the index.  These document numbers are ephemeral -- they may change
+ as documents are added to and deleted from an index.  Clients should thus not
+ rely on a given document having the same number between sessions.
+
+ <p>
+ <a name="thread-safety"></a><p><b>NOTE</b>: {@link
+ IndexReader} instances are completely thread
+ safe, meaning multiple threads can call any of its methods,
+ concurrently.  If your application requires external
+ synchronization, you should <b>not</b> synchronize on the
+ <code>IndexReader</code> instance; use your own
+ (non-Lucene) objects instead.
+*/
+public final class DirectoryReader extends BaseMultiReader<SegmentReader> {
+  static int DEFAULT_TERMS_INDEX_DIVISOR = 1;
+
   protected final Directory directory;
   private final IndexWriter writer;
   private final SegmentInfos segmentInfos;
   private final int termInfosIndexDivisor;
   private final boolean applyAllDeletes;
   
+  /** Returns a IndexReader reading the index in the given
+   *  Directory
+   * @param directory the index directory
+   * @throws CorruptIndexException if the index is corrupt
+   * @throws IOException if there is a low-level IO error
+   */
+  public static DirectoryReader open(final Directory directory) throws CorruptIndexException, IOException {
+    return open(directory, null, DEFAULT_TERMS_INDEX_DIVISOR);
+  }
+  
+  /** Expert: Returns a IndexReader reading the index in the given
+   *  Directory with the given termInfosIndexDivisor.
+   * @param directory the index directory
+   * @param termInfosIndexDivisor Subsamples which indexed
+   *  terms are loaded into RAM. This has the same effect as {@link
+   *  IndexWriterConfig#setTermIndexInterval} except that setting
+   *  must be done at indexing time while this setting can be
+   *  set per reader.  When set to N, then one in every
+   *  N*termIndexInterval terms in the index is loaded into
+   *  memory.  By setting this to a value > 1 you can reduce
+   *  memory usage, at the expense of higher latency when
+   *  loading a TermInfo.  The default value is 1.  Set this
+   *  to -1 to skip loading the terms index entirely.
+   * @throws CorruptIndexException if the index is corrupt
+   * @throws IOException if there is a low-level IO error
+   */
+  public static DirectoryReader open(final Directory directory, int termInfosIndexDivisor) throws CorruptIndexException, IOException {
+    return open(directory, null, termInfosIndexDivisor);
+  }
+  
+  /**
+   * Open a near real time IndexReader from the {@link org.apache.lucene.index.IndexWriter}.
+   *
+   * @param writer The IndexWriter to open from
+   * @param applyAllDeletes If true, all buffered deletes will
+   * be applied (made visible) in the returned reader.  If
+   * false, the deletes are not applied but remain buffered
+   * (in IndexWriter) so that they will be applied in the
+   * future.  Applying deletes can be costly, so if your app
+   * can tolerate deleted documents being returned you might
+   * gain some performance by passing false.
+   * @return The new IndexReader
+   * @throws CorruptIndexException
+   * @throws IOException if there is a low-level IO error
+   *
+   * @see #openIfChanged(DirectoryReader,IndexWriter,boolean)
+   *
+   * @lucene.experimental
+   */
+  public static DirectoryReader open(final IndexWriter writer, boolean applyAllDeletes) throws CorruptIndexException, IOException {
+    return writer.getReader(applyAllDeletes);
+  }
+
+  /** Expert: returns an IndexReader reading the index in the given
+   *  {@link IndexCommit}.
+   * @param commit the commit point to open
+   * @throws CorruptIndexException if the index is corrupt
+   * @throws IOException if there is a low-level IO error
+   */
+  public static DirectoryReader open(final IndexCommit commit) throws CorruptIndexException, IOException {
+    return open(commit.getDirectory(), commit, DEFAULT_TERMS_INDEX_DIVISOR);
+  }
+
+
+  /** Expert: returns an IndexReader reading the index in the given
+   *  {@link IndexCommit} and termInfosIndexDivisor.
+   * @param commit the commit point to open
+   * @param termInfosIndexDivisor Subsamples which indexed
+   *  terms are loaded into RAM. This has the same effect as {@link
+   *  IndexWriterConfig#setTermIndexInterval} except that setting
+   *  must be done at indexing time while this setting can be
+   *  set per reader.  When set to N, then one in every
+   *  N*termIndexInterval terms in the index is loaded into
+   *  memory.  By setting this to a value > 1 you can reduce
+   *  memory usage, at the expense of higher latency when
+   *  loading a TermInfo.  The default value is 1.  Set this
+   *  to -1 to skip loading the terms index entirely.
+   * @throws CorruptIndexException if the index is corrupt
+   * @throws IOException if there is a low-level IO error
+   */
+  public static DirectoryReader open(final IndexCommit commit, int termInfosIndexDivisor) throws CorruptIndexException, IOException {
+    return open(commit.getDirectory(), commit, termInfosIndexDivisor);
+  }
+
   DirectoryReader(SegmentReader[] readers, Directory directory, IndexWriter writer,
     SegmentInfos sis, int termInfosIndexDivisor, boolean applyAllDeletes) throws IOException {
     super(readers);
@@ -50,9 +156,9 @@
     this.applyAllDeletes = applyAllDeletes;
   }
 
-  static IndexReader open(final Directory directory, final IndexCommit commit,
+  private static DirectoryReader open(final Directory directory, final IndexCommit commit,
                           final int termInfosIndexDivisor) throws CorruptIndexException, IOException {
-    return (IndexReader) new SegmentInfos.FindSegmentsFile(directory) {
+    return (DirectoryReader) new SegmentInfos.FindSegmentsFile(directory) {
       @Override
       protected Object doBody(String segmentFileName) throws CorruptIndexException, IOException {
         SegmentInfos sis = new SegmentInfos();
@@ -116,7 +222,7 @@
   }
 
   /** This constructor is only used for {@link #doOpenIfChanged()} */
-  static DirectoryReader open(Directory directory, IndexWriter writer, SegmentInfos infos, SegmentReader[] oldReaders,
+  private static DirectoryReader open(Directory directory, IndexWriter writer, SegmentInfos infos, SegmentReader[] oldReaders,
     int termInfosIndexDivisor) throws IOException {
     // we put the old SegmentReaders in a map, that allows us
     // to lookup a reader using its segment name
@@ -202,6 +308,116 @@
         infos, termInfosIndexDivisor, false);
   }
 
+  /**
+   * If the index has changed since the provided reader was
+   * opened, open and return a new reader; else, return
+   * null.  The new reader, if not null, will be the same
+   * type of reader as the previous one, ie an NRT reader
+   * will open a new NRT reader, a MultiReader will open a
+   * new MultiReader,  etc.
+   *
+   * <p>This method is typically far less costly than opening a
+   * fully new <code>DirectoryReader</code> as it shares
+   * resources (for example sub-readers) with the provided
+   * <code>DirectoryReader</code>, when possible.
+   *
+   * <p>The provided reader is not closed (you are responsible
+   * for doing so); if a new reader is returned you also
+   * must eventually close it.  Be sure to never close a
+   * reader while other threads are still using it; see
+   * {@link SearcherManager} to simplify managing this.
+   *
+   * @throws CorruptIndexException if the index is corrupt
+   * @throws IOException if there is a low-level IO error
+   * @return null if there are no changes; else, a new
+   * DirectoryReader instance which you must eventually close
+   */  
+  public static DirectoryReader openIfChanged(DirectoryReader oldReader) throws IOException {
+    final DirectoryReader newReader = oldReader.doOpenIfChanged();
+    assert newReader != oldReader;
+    return newReader;
+  }
+
+  /**
+   * If the IndexCommit differs from what the
+   * provided reader is searching, open and return a new
+   * reader; else, return null.
+   *
+   * @see #openIfChanged(DirectoryReader)
+   */
+  public static DirectoryReader openIfChanged(DirectoryReader oldReader, IndexCommit commit) throws IOException {
+    final DirectoryReader newReader = oldReader.doOpenIfChanged(commit);
+    assert newReader != oldReader;
+    return newReader;
+  }
+
+  /**
+   * Expert: If there changes (committed or not) in the
+   * {@link IndexWriter} versus what the provided reader is
+   * searching, then open and return a new
+   * IndexReader searching both committed and uncommitted
+   * changes from the writer; else, return null (though, the
+   * current implementation never returns null).
+   *
+   * <p>This provides "near real-time" searching, in that
+   * changes made during an {@link IndexWriter} session can be
+   * quickly made available for searching without closing
+   * the writer nor calling {@link IndexWriter#commit}.
+   *
+   * <p>It's <i>near</i> real-time because there is no hard
+   * guarantee on how quickly you can get a new reader after
+   * making changes with IndexWriter.  You'll have to
+   * experiment in your situation to determine if it's
+   * fast enough.  As this is a new and experimental
+   * feature, please report back on your findings so we can
+   * learn, improve and iterate.</p>
+   *
+   * <p>The very first time this method is called, this
+   * writer instance will make every effort to pool the
+   * readers that it opens for doing merges, applying
+   * deletes, etc.  This means additional resources (RAM,
+   * file descriptors, CPU time) will be consumed.</p>
+   *
+   * <p>For lower latency on reopening a reader, you should
+   * call {@link IndexWriterConfig#setMergedSegmentWarmer} to
+   * pre-warm a newly merged segment before it's committed
+   * to the index.  This is important for minimizing
+   * index-to-search delay after a large merge.  </p>
+   *
+   * <p>If an addIndexes* call is running in another thread,
+   * then this reader will only search those segments from
+   * the foreign index that have been successfully copied
+   * over, so far.</p>
+   *
+   * <p><b>NOTE</b>: Once the writer is closed, any
+   * outstanding readers may continue to be used.  However,
+   * if you attempt to reopen any of those readers, you'll
+   * hit an {@link org.apache.lucene.store.AlreadyClosedException}.</p>
+   *
+   * @return DirectoryReader that covers entire index plus all
+   * changes made so far by this IndexWriter instance, or
+   * null if there are no new changes
+   *
+   * @param writer The IndexWriter to open from
+   *
+   * @param applyAllDeletes If true, all buffered deletes will
+   * be applied (made visible) in the returned reader.  If
+   * false, the deletes are not applied but remain buffered
+   * (in IndexWriter) so that they will be applied in the
+   * future.  Applying deletes can be costly, so if your app
+   * can tolerate deleted documents being returned you might
+   * gain some performance by passing false.
+   *
+   * @throws IOException
+   *
+   * @lucene.experimental
+   */
+  public static DirectoryReader openIfChanged(DirectoryReader oldReader, IndexWriter writer, boolean applyAllDeletes) throws IOException {
+    final DirectoryReader newReader = oldReader.doOpenIfChanged(writer, applyAllDeletes);
+    assert newReader != oldReader;
+    return newReader;
+  }
+
   /** {@inheritDoc} */
   @Override
   public String toString() {
@@ -223,13 +439,11 @@
     return buffer.toString();
   }
 
-  @Override
-  protected final IndexReader doOpenIfChanged() throws CorruptIndexException, IOException {
+  protected final DirectoryReader doOpenIfChanged() throws CorruptIndexException, IOException {
     return doOpenIfChanged(null);
   }
 
-  @Override
-  protected final IndexReader doOpenIfChanged(final IndexCommit commit) throws CorruptIndexException, IOException {
+  protected final DirectoryReader doOpenIfChanged(final IndexCommit commit) throws CorruptIndexException, IOException {
     ensureOpen();
 
     // If we were obtained by writer.getReader(), re-ask the
@@ -241,18 +455,16 @@
     }
   }
 
-  @Override
-  protected final IndexReader doOpenIfChanged(IndexWriter writer, boolean applyAllDeletes) throws CorruptIndexException, IOException {
+  protected final DirectoryReader doOpenIfChanged(IndexWriter writer, boolean applyAllDeletes) throws CorruptIndexException, IOException {
     ensureOpen();
     if (writer == this.writer && applyAllDeletes == this.applyAllDeletes) {
       return doOpenFromWriter(null);
     } else {
-      // fail by calling supers impl throwing UOE
-      return super.doOpenIfChanged(writer, applyAllDeletes);
+      return writer.getReader(applyAllDeletes);
     }
   }
 
-  private final IndexReader doOpenFromWriter(IndexCommit commit) throws CorruptIndexException, IOException {
+  private final DirectoryReader doOpenFromWriter(IndexCommit commit) throws CorruptIndexException, IOException {
     if (commit != null) {
       throw new IllegalArgumentException("a reader obtained from IndexWriter.getReader() cannot currently accept a commit");
     }
@@ -261,7 +473,7 @@
       return null;
     }
 
-    IndexReader reader = writer.getReader(applyAllDeletes);
+    DirectoryReader reader = writer.getReader(applyAllDeletes);
 
     // If in fact no changes took place, return null:
     if (reader.getVersion() == segmentInfos.getVersion()) {
@@ -272,7 +484,7 @@
     return reader;
   }
 
-  private synchronized IndexReader doOpenNoWriter(IndexCommit commit) throws CorruptIndexException, IOException {
+  private synchronized DirectoryReader doOpenNoWriter(IndexCommit commit) throws CorruptIndexException, IOException {
 
     if (commit == null) {
       if (isCurrent()) {
@@ -287,7 +499,7 @@
       }
     }
 
-    return (IndexReader) new SegmentInfos.FindSegmentsFile(directory) {
+    return (DirectoryReader) new SegmentInfos.FindSegmentsFile(directory) {
       @Override
       protected Object doBody(String segmentFileName) throws CorruptIndexException, IOException {
         final SegmentInfos infos = new SegmentInfos();
@@ -301,20 +513,25 @@
     return DirectoryReader.open(directory, writer, infos, subReaders, termInfosIndexDivisor);
   }
 
-  /** Version number when this IndexReader was opened. */
-  @Override
+  /**
+   * Version number when this IndexReader was opened. Not
+   * implemented in the IndexReader base class.
+   *
+   * <p>This method
+   * returns the version recorded in the commit that the
+   * reader opened.  This version is advanced every time
+   * a change is made with {@link IndexWriter}.</p>
+   */
   public long getVersion() {
     ensureOpen();
     return segmentInfos.getVersion();
   }
 
-  @Override
   public Map<String,String> getCommitUserData() {
     ensureOpen();
     return segmentInfos.getUserData();
   }
 
-  @Override
   public boolean isCurrent() throws CorruptIndexException, IOException {
     ensureOpen();
     if (writer == null || writer.isClosed()) {
@@ -348,7 +565,6 @@
   }
 
   /** Returns the directory this index resides in. */
-  @Override
   public Directory directory() {
     // Don't ensureOpen here -- in certain cases, when a
     // cloned/reopened reader needs to commit, it may call
@@ -356,7 +572,6 @@
     return directory;
   }
 
-  @Override
   public int getTermInfosIndexDivisor() {
     ensureOpen();
     return termInfosIndexDivisor;
@@ -367,13 +582,26 @@
    * <p/>
    * @lucene.experimental
    */
-  @Override
   public IndexCommit getIndexCommit() throws IOException {
     ensureOpen();
     return new ReaderCommit(segmentInfos, directory);
   }
 
-  /** @see org.apache.lucene.index.IndexReader#listCommits */
+  /** Returns all commit points that exist in the Directory.
+   *  Normally, because the default is {@link
+   *  KeepOnlyLastCommitDeletionPolicy}, there would be only
+   *  one commit point.  But if you're using a custom {@link
+   *  IndexDeletionPolicy} then there could be many commits.
+   *  Once you have a given commit, you can open a reader on
+   *  it by calling {@link IndexReader#open(IndexCommit)}
+   *  There must be at least one commit in
+   *  the Directory, else this method throws {@link
+   *  IndexNotFoundException}.  Note that if a commit is in
+   *  progress while this method is running, that commit
+   *  may or may not be returned.
+   *  
+   *  @return a sorted list of {@link IndexCommit}s, from oldest 
+   *  to latest. */
   public static List<IndexCommit> listCommits(Directory dir) throws IOException {
     final String[] files = dir.listAll();
 
@@ -420,6 +648,53 @@
     return commits;
   }  
   
+  /**
+   * Reads version number from segments files. The version number is
+   * initialized with a timestamp and then increased by one for each change of
+   * the index.
+   * 
+   * @param directory where the index resides.
+   * @return version number.
+   * @throws CorruptIndexException if the index is corrupt
+   * @throws IOException if there is a low-level IO error
+   */
+  public static long getCurrentVersion(Directory directory) throws CorruptIndexException, IOException {
+    return SegmentInfos.readCurrentVersion(directory);
+  }
+    
+  /**
+   * Reads commitUserData, previously passed to {@link
+   * IndexWriter#commit(Map)}, from current index
+   * segments file.  This will return null if {@link
+   * IndexWriter#commit(Map)} has never been called for
+   * this index.
+   * 
+   * @param directory where the index resides.
+   * @return commit userData.
+   * @throws CorruptIndexException if the index is corrupt
+   * @throws IOException if there is a low-level IO error
+   *
+   * @see #getCommitUserData()
+   */
+  public static Map<String, String> getCommitUserData(Directory directory) throws CorruptIndexException, IOException {
+    return SegmentInfos.readCurrentUserData(directory);
+  }
+
+  /**
+   * Returns <code>true</code> if an index exists at the specified directory.
+   * @param  directory the directory to check for an index
+   * @return <code>true</code> if an index exists; <code>false</code> otherwise
+   * @throws IOException if there is a problem with accessing the index
+   */
+  public static boolean indexExists(Directory directory) throws IOException {
+    try {
+      new SegmentInfos().read(directory);
+      return true;
+    } catch (IOException ioe) {
+      return false;
+    }
+  }
+
   private static final class ReaderCommit extends IndexCommit {
     private String segmentsFileName;
     Collection<String> files;
Index: lucene/src/java/org/apache/lucene/index/DocTermOrds.java
===================================================================
--- lucene/src/java/org/apache/lucene/index/DocTermOrds.java	(revision 1238034)
+++ lucene/src/java/org/apache/lucene/index/DocTermOrds.java	(working copy)
@@ -68,8 +68,6 @@
  *
  * The RAM consumption of this class can be high!
  *
- * <p>NOTE: the provided reader must be an atomic reader
- *
  * @lucene.experimental
  */
 
@@ -149,19 +147,19 @@
   }
 
   /** Inverts all terms */
-  public DocTermOrds(IndexReader reader, String field) throws IOException {
+  public DocTermOrds(AtomicReader reader, String field) throws IOException {
     this(reader, field, null, Integer.MAX_VALUE);
   }
 
   /** Inverts only terms starting w/ prefix */
-  public DocTermOrds(IndexReader reader, String field, BytesRef termPrefix) throws IOException {
+  public DocTermOrds(AtomicReader reader, String field, BytesRef termPrefix) throws IOException {
     this(reader, field, termPrefix, Integer.MAX_VALUE);
   }
 
   /** Inverts only terms starting w/ prefix, and only terms
    *  whose docFreq (not taking deletions into account) is
    *  <=  maxTermDocFreq */
-  public DocTermOrds(IndexReader reader, String field, BytesRef termPrefix, int maxTermDocFreq) throws IOException {
+  public DocTermOrds(AtomicReader reader, String field, BytesRef termPrefix, int maxTermDocFreq) throws IOException {
     this(reader, field, termPrefix, maxTermDocFreq, DEFAULT_INDEX_INTERVAL_BITS);
     uninvert(reader, termPrefix);
   }
@@ -170,7 +168,7 @@
    *  whose docFreq (not taking deletions into account) is
    *  <=  maxTermDocFreq, with a custom indexing interval
    *  (default is every 128nd term). */
-  public DocTermOrds(IndexReader reader, String field, BytesRef termPrefix, int maxTermDocFreq, int indexIntervalBits) throws IOException {
+  public DocTermOrds(AtomicReader reader, String field, BytesRef termPrefix, int maxTermDocFreq, int indexIntervalBits) throws IOException {
     this(field, maxTermDocFreq, indexIntervalBits);
     uninvert(reader, termPrefix);
   }
@@ -196,7 +194,7 @@
    *
    *  <p><b>NOTE</b>: you must pass the same reader that was
    *  used when creating this class */
-  public TermsEnum getOrdTermsEnum(IndexReader reader) throws IOException {
+  public TermsEnum getOrdTermsEnum(AtomicReader reader) throws IOException {
     if (termInstances == 0) {
       return null;
     }
@@ -226,7 +224,7 @@
   }
 
   // Call this only once (if you subclass!)
-  protected void uninvert(final IndexReader reader, final BytesRef termPrefix) throws IOException {
+  protected void uninvert(final AtomicReader reader, final BytesRef termPrefix) throws IOException {
     //System.out.println("DTO uninvert field=" + field + " prefix=" + termPrefix);
     final long startTime = System.currentTimeMillis();
     prefix = termPrefix == null ? null : BytesRef.deepCopyOf(termPrefix);
@@ -644,12 +642,12 @@
    * ord; in this case we "wrap" our own terms index
    * around it. */
   private final class OrdWrappedTermsEnum extends TermsEnum {
-    private final IndexReader reader;
+    private final AtomicReader reader;
     private final TermsEnum termsEnum;
     private BytesRef term;
     private long ord = -indexInterval-1;          // force "real" seek
     
-    public OrdWrappedTermsEnum(IndexReader reader) throws IOException {
+    public OrdWrappedTermsEnum(AtomicReader reader) throws IOException {
       this.reader = reader;
       assert indexedTermsArray != null;
       termsEnum = reader.fields().terms(field).iterator(null);
Index: lucene/src/java/org/apache/lucene/index/DocValues.java
===================================================================
--- lucene/src/java/org/apache/lucene/index/DocValues.java	(revision 1238034)
+++ lucene/src/java/org/apache/lucene/index/DocValues.java	(working copy)
@@ -33,7 +33,7 @@
  * <li>via {@link #getSource()} providing RAM resident random access</li>
  * <li>via {@link #getDirectSource()} providing on disk random access</li>
  * </ul> {@link DocValues} are exposed via
- * {@link IndexReader#docValues(String)} on a per-segment basis. For best
+ * {@link AtomicReader#docValues(String)} on a per-segment basis. For best
  * performance {@link DocValues} should be consumed per-segment just like
  * IndexReader.
  * <p>
Index: lucene/src/java/org/apache/lucene/index/FilterIndexReader.java
===================================================================
--- lucene/src/java/org/apache/lucene/index/FilterIndexReader.java	(revision 1238034)
+++ lucene/src/java/org/apache/lucene/index/FilterIndexReader.java	(working copy)
@@ -17,12 +17,10 @@
  * limitations under the License.
  */
 
-import org.apache.lucene.store.Directory;
 import org.apache.lucene.util.Bits;
 import org.apache.lucene.util.BytesRef;
 
 import java.io.IOException;
-import java.util.Map;
 import java.util.Comparator;
 
 /**  A <code>FilterIndexReader</code> contains another IndexReader, which it
@@ -33,13 +31,8 @@
  * contained index reader. Subclasses of <code>FilterIndexReader</code> may
  * further override some of these methods and may also provide additional
  * methods and fields.
- * <p><b>Note:</b> The default implementation of {@link FilterIndexReader#doOpenIfChanged}
- * throws {@link UnsupportedOperationException} (like the base class),
- * so it's not possible to reopen a <code>FilterIndexReader</code>.
- * To reopen, you have to first reopen the underlying reader
- * and wrap it again with the custom filter.
  */
-public class FilterIndexReader extends IndexReader {
+public class FilterIndexReader extends AtomicReader {
 
   /** Base class for filtering {@link Fields}
    *  implementations. */
@@ -279,25 +272,19 @@
     }
   }
 
-  protected IndexReader in;
+  protected AtomicReader in;
 
   /**
    * <p>Construct a FilterIndexReader based on the specified base reader.
    * <p>Note that base reader is closed if this FilterIndexReader is closed.</p>
    * @param in specified base reader.
    */
-  public FilterIndexReader(IndexReader in) {
+  public FilterIndexReader(AtomicReader in) {
     super();
     this.in = in;
   }
 
   @Override
-  public Directory directory() {
-    ensureOpen();
-    return in.directory();
-  }
-  
-  @Override
   public Bits getLiveDocs() {
     ensureOpen();
     return in.getLiveDocs();
@@ -346,45 +333,11 @@
   }
 
   @Override
-  public int docFreq(String field, BytesRef t) throws IOException {
-    ensureOpen();
-    return in.docFreq(field, t);
-  }
-  
-  @Override
   protected void doClose() throws IOException {
     in.close();
   }
-
-  @Override
-  public long getVersion() {
-    ensureOpen();
-    return in.getVersion();
-  }
-
-  @Override
-  public boolean isCurrent() throws CorruptIndexException, IOException {
-    ensureOpen();
-    return in.isCurrent();
-  }
   
   @Override
-  public IndexReader[] getSequentialSubReaders() {
-    return in.getSequentialSubReaders();
-  }
-  
-  @Override
-  public ReaderContext getTopReaderContext() {
-    ensureOpen();
-    return in.getTopReaderContext();
-  }
-
-  @Override
-  public Map<String, String> getCommitUserData() { 
-    return in.getCommitUserData();
-  }
-  
-  @Override
   public Fields fields() throws IOException {
     ensureOpen();
     return in.fields();
@@ -410,7 +363,7 @@
 
   @Override
   public String toString() {
-    final StringBuilder buffer = new StringBuilder("FilterReader(");
+    final StringBuilder buffer = new StringBuilder("FilterIndexReader(");
     buffer.append(in);
     buffer.append(')');
     return buffer.toString();
@@ -427,14 +380,4 @@
     ensureOpen();
     return in.normValues(field);
   }
-
-  @Override
-  public IndexCommit getIndexCommit() throws IOException {
-    return in.getIndexCommit();
-  }
-
-  @Override
-  public int getTermInfosIndexDivisor() {
-    return in.getTermInfosIndexDivisor();
-  }  
 }
Index: lucene/src/java/org/apache/lucene/index/IndexReader.java
===================================================================
--- lucene/src/java/org/apache/lucene/index/IndexReader.java	(revision 1238034)
+++ lucene/src/java/org/apache/lucene/index/IndexReader.java	(working copy)
@@ -21,8 +21,6 @@
 import java.io.IOException;
 import java.util.Collections;
 import java.util.LinkedHashSet;
-import java.util.List;
-import java.util.Map;
 import java.util.Set;
 import java.util.concurrent.atomic.AtomicInteger;
 
@@ -38,26 +36,32 @@
  index.  Search of an index is done entirely through this abstract interface,
  so that any subclass which implements it is searchable.
 
- <p> Concrete subclasses of IndexReader are usually constructed with a call to
- one of the static <code>open()</code> methods, e.g. {@link
- #open(Directory)}.
+ <p>There are two different types of IndexReaders:
+ <ul>
+  <li>{@link AtomicReader}: These indexes do not consist of several sub-readers,
+  they are atomic. They support retrieval of stored fields, doc values, terms,
+  and postings.
+  <li>{@link CompositeReader}: Instances (like {@link DirectoryReader})
+  of this reader can only
+  be used to get stored fields from the underlying AtomicReaders,
+  but it is not possible to directly retrieve postings. To do that, get
+  the sub-readers via {@link CompositeReader#getSequentialSubReaders}.
+  Alternatively, you can mimic an {@link AtomicReader} (with a serious slowdown),
+  by wrapping composite readers with {@link SlowCompositeReaderWrapper}.
+ </ul>
+ 
+ <p>IndexReader instances for indexes on disk are usually constructed
+ with a call to one of the static <code>DirectoryReader,open()</code> methods,
+ e.g. {@link DirectoryReader#open(Directory)}. {@link DirectoryReader} implements
+ the {@link CompositeReader} interface, it is not possible to directly get postings.
 
  <p> For efficiency, in this API documents are often referred to via
  <i>document numbers</i>, non-negative integers which each name a unique
- document in the index.  These document numbers are ephemeral--they may change
+ document in the index.  These document numbers are ephemeral -- they may change
  as documents are added to and deleted from an index.  Clients should thus not
  rely on a given document having the same number between sessions.
 
  <p>
- <b>NOTE</b>: for backwards API compatibility, several methods are not listed 
- as abstract, but have no useful implementations in this base class and 
- instead always throw UnsupportedOperationException.  Subclasses are 
- strongly encouraged to override these methods, but in many cases may not 
- need to.
- </p>
-
- <p>
-
  <a name="thread-safety"></a><p><b>NOTE</b>: {@link
  IndexReader} instances are completely thread
  safe, meaning multiple threads can call any of its methods,
@@ -67,7 +71,13 @@
  (non-Lucene) objects instead.
 */
 public abstract class IndexReader implements Closeable {
-
+  
+  IndexReader() {
+    if (!(this instanceof CompositeReader || this instanceof AtomicReader))
+      throw new Error("This class should never be directly extended, subclass AtomicReader or CompositeReader instead!");
+    refCount.set(1);
+  }
+  
   /**
    * A custom listener that's invoked when the IndexReader
    * is closed.
@@ -110,8 +120,6 @@
   
   private final AtomicInteger refCount = new AtomicInteger();
 
-  static int DEFAULT_TERMS_INDEX_DIVISOR = 1;
-
   /** Expert: returns the current refCount for this reader */
   public final int getRefCount() {
     // NOTE: don't ensureOpen, so that callers can see
@@ -172,23 +180,6 @@
     return false;
   }
 
-  /** {@inheritDoc} */
-  @Override
-  public String toString() {
-    final StringBuilder buffer = new StringBuilder();
-    buffer.append(getClass().getSimpleName());
-    buffer.append('(');
-    final IndexReader[] subReaders = getSequentialSubReaders();
-    if ((subReaders != null) && (subReaders.length > 0)) {
-      buffer.append(subReaders[0]);
-      for (int i = 1; i < subReaders.length; ++i) {
-        buffer.append(" ").append(subReaders[i]);
-      }
-    }
-    buffer.append(')');
-    return buffer.toString();
-  }
-
   /**
    * Expert: decreases the refCount of this IndexReader
    * instance.  If the refCount drops to 0, then this
@@ -219,10 +210,6 @@
     }
   }
   
-  protected IndexReader() { 
-    refCount.set(1);
-  }
-  
   /**
    * @throws AlreadyClosedException if this IndexReader is closed
    */
@@ -237,9 +224,11 @@
    * @param directory the index directory
    * @throws CorruptIndexException if the index is corrupt
    * @throws IOException if there is a low-level IO error
+   * @deprecated Use {@link DirectoryReader#open(Directory)}
    */
-  public static IndexReader open(final Directory directory) throws CorruptIndexException, IOException {
-    return DirectoryReader.open(directory, null, DEFAULT_TERMS_INDEX_DIVISOR);
+  @Deprecated
+  public static DirectoryReader open(final Directory directory) throws CorruptIndexException, IOException {
+    return DirectoryReader.open(directory);
   }
   
   /** Expert: Returns a IndexReader reading the index in the given
@@ -257,9 +246,11 @@
    *  to -1 to skip loading the terms index entirely.
    * @throws CorruptIndexException if the index is corrupt
    * @throws IOException if there is a low-level IO error
+   * @deprecated Use {@link DirectoryReader#open(Directory,int)}
    */
-  public static IndexReader open(final Directory directory, int termInfosIndexDivisor) throws CorruptIndexException, IOException {
-    return DirectoryReader.open(directory, null, termInfosIndexDivisor);
+  @Deprecated
+  public static DirectoryReader open(final Directory directory, int termInfosIndexDivisor) throws CorruptIndexException, IOException {
+    return DirectoryReader.open(directory, termInfosIndexDivisor);
   }
   
   /**
@@ -277,12 +268,14 @@
    * @throws CorruptIndexException
    * @throws IOException if there is a low-level IO error
    *
-   * @see #openIfChanged(IndexReader,IndexWriter,boolean)
+   * @see DirectoryReader#openIfChanged(DirectoryReader,IndexWriter,boolean)
    *
    * @lucene.experimental
+   * @deprecated Use {@link DirectoryReader#open(IndexWriter,boolean)}
    */
-  public static IndexReader open(final IndexWriter writer, boolean applyAllDeletes) throws CorruptIndexException, IOException {
-    return writer.getReader(applyAllDeletes);
+  @Deprecated
+  public static DirectoryReader open(final IndexWriter writer, boolean applyAllDeletes) throws CorruptIndexException, IOException {
+    return DirectoryReader.open(writer, applyAllDeletes);
   }
 
   /** Expert: returns an IndexReader reading the index in the given
@@ -290,9 +283,11 @@
    * @param commit the commit point to open
    * @throws CorruptIndexException if the index is corrupt
    * @throws IOException if there is a low-level IO error
+   * @deprecated Use {@link DirectoryReader#open(IndexCommit)}
    */
-  public static IndexReader open(final IndexCommit commit) throws CorruptIndexException, IOException {
-    return DirectoryReader.open(commit.getDirectory(), commit, DEFAULT_TERMS_INDEX_DIVISOR);
+  @Deprecated
+  public static DirectoryReader open(final IndexCommit commit) throws CorruptIndexException, IOException {
+    return DirectoryReader.open(commit);
   }
 
 
@@ -311,242 +306,13 @@
    *  to -1 to skip loading the terms index entirely.
    * @throws CorruptIndexException if the index is corrupt
    * @throws IOException if there is a low-level IO error
+   * @deprecated Use {@link DirectoryReader#open(IndexCommit,int)}
    */
-  public static IndexReader open(final IndexCommit commit, int termInfosIndexDivisor) throws CorruptIndexException, IOException {
-    return DirectoryReader.open(commit.getDirectory(), commit, termInfosIndexDivisor);
+  @Deprecated
+  public static DirectoryReader open(final IndexCommit commit, int termInfosIndexDivisor) throws CorruptIndexException, IOException {
+    return DirectoryReader.open(commit, termInfosIndexDivisor);
   }
 
-  /**
-   * If the index has changed since the provided reader was
-   * opened, open and return a new reader; else, return
-   * null.  The new reader, if not null, will be the same
-   * type of reader as the previous one, ie an NRT reader
-   * will open a new NRT reader, a MultiReader will open a
-   * new MultiReader,  etc.
-   *
-   * <p>This method is typically far less costly than opening a
-   * fully new <code>IndexReader</code> as it shares
-   * resources (for example sub-readers) with the provided
-   * <code>IndexReader</code>, when possible.
-   *
-   * <p>The provided reader is not closed (you are responsible
-   * for doing so); if a new reader is returned you also
-   * must eventually close it.  Be sure to never close a
-   * reader while other threads are still using it; see
-   * {@link SearcherManager} to simplify managing this.
-   *
-   * @throws CorruptIndexException if the index is corrupt
-   * @throws IOException if there is a low-level IO error
-   * @return null if there are no changes; else, a new
-   * IndexReader instance which you must eventually close
-   */  
-  public static IndexReader openIfChanged(IndexReader oldReader) throws IOException {
-    final IndexReader newReader = oldReader.doOpenIfChanged();
-    assert newReader != oldReader;
-    return newReader;
-  }
-
-  /**
-   * If the IndexCommit differs from what the
-   * provided reader is searching, open and return a new
-   * reader; else, return null.
-   *
-   * @see #openIfChanged(IndexReader)
-   */
-  public static IndexReader openIfChanged(IndexReader oldReader, IndexCommit commit) throws IOException {
-    final IndexReader newReader = oldReader.doOpenIfChanged(commit);
-    assert newReader != oldReader;
-    return newReader;
-  }
-
-  /**
-   * Expert: If there changes (committed or not) in the
-   * {@link IndexWriter} versus what the provided reader is
-   * searching, then open and return a new
-   * IndexReader searching both committed and uncommitted
-   * changes from the writer; else, return null (though, the
-   * current implementation never returns null).
-   *
-   * <p>This provides "near real-time" searching, in that
-   * changes made during an {@link IndexWriter} session can be
-   * quickly made available for searching without closing
-   * the writer nor calling {@link IndexWriter#commit}.
-   *
-   * <p>It's <i>near</i> real-time because there is no hard
-   * guarantee on how quickly you can get a new reader after
-   * making changes with IndexWriter.  You'll have to
-   * experiment in your situation to determine if it's
-   * fast enough.  As this is a new and experimental
-   * feature, please report back on your findings so we can
-   * learn, improve and iterate.</p>
-   *
-   * <p>The very first time this method is called, this
-   * writer instance will make every effort to pool the
-   * readers that it opens for doing merges, applying
-   * deletes, etc.  This means additional resources (RAM,
-   * file descriptors, CPU time) will be consumed.</p>
-   *
-   * <p>For lower latency on reopening a reader, you should
-   * call {@link IndexWriterConfig#setMergedSegmentWarmer} to
-   * pre-warm a newly merged segment before it's committed
-   * to the index.  This is important for minimizing
-   * index-to-search delay after a large merge.  </p>
-   *
-   * <p>If an addIndexes* call is running in another thread,
-   * then this reader will only search those segments from
-   * the foreign index that have been successfully copied
-   * over, so far.</p>
-   *
-   * <p><b>NOTE</b>: Once the writer is closed, any
-   * outstanding readers may continue to be used.  However,
-   * if you attempt to reopen any of those readers, you'll
-   * hit an {@link AlreadyClosedException}.</p>
-   *
-   * @return IndexReader that covers entire index plus all
-   * changes made so far by this IndexWriter instance, or
-   * null if there are no new changes
-   *
-   * @param writer The IndexWriter to open from
-   *
-   * @param applyAllDeletes If true, all buffered deletes will
-   * be applied (made visible) in the returned reader.  If
-   * false, the deletes are not applied but remain buffered
-   * (in IndexWriter) so that they will be applied in the
-   * future.  Applying deletes can be costly, so if your app
-   * can tolerate deleted documents being returned you might
-   * gain some performance by passing false.
-   *
-   * @throws IOException
-   *
-   * @lucene.experimental
-   */
-  public static IndexReader openIfChanged(IndexReader oldReader, IndexWriter writer, boolean applyAllDeletes) throws IOException {
-    final IndexReader newReader = oldReader.doOpenIfChanged(writer, applyAllDeletes);
-    assert newReader != oldReader;
-    return newReader;
-  }
-
-  /**
-   * If the index has changed since it was opened, open and return a new reader;
-   * else, return {@code null}.
-   * 
-   * @see #openIfChanged(IndexReader)
-   */
-  protected IndexReader doOpenIfChanged() throws CorruptIndexException, IOException {
-    throw new UnsupportedOperationException("This reader does not support reopen().");
-  }
-  
-  /**
-   * If the index has changed since it was opened, open and return a new reader;
-   * else, return {@code null}.
-   * 
-   * @see #openIfChanged(IndexReader, IndexCommit)
-   */
-  protected IndexReader doOpenIfChanged(final IndexCommit commit) throws CorruptIndexException, IOException {
-    throw new UnsupportedOperationException("This reader does not support reopen(IndexCommit).");
-  }
-
-  /**
-   * If the index has changed since it was opened, open and return a new reader;
-   * else, return {@code null}.
-   * 
-   * @see #openIfChanged(IndexReader, IndexWriter, boolean)
-   */
-  protected IndexReader doOpenIfChanged(IndexWriter writer, boolean applyAllDeletes) throws CorruptIndexException, IOException {
-    return writer.getReader(applyAllDeletes);
-  }
-
-  /** 
-   * Returns the directory associated with this index.  The Default 
-   * implementation returns the directory specified by subclasses when 
-   * delegating to the IndexReader(Directory) constructor, or throws an 
-   * UnsupportedOperationException if one was not specified.
-   * @throws UnsupportedOperationException if no directory
-   */
-  public Directory directory() {
-    ensureOpen();
-    throw new UnsupportedOperationException("This reader does not support this method.");  
-  }
-
-  /**
-   * Reads commitUserData, previously passed to {@link
-   * IndexWriter#commit(Map)}, from current index
-   * segments file.  This will return null if {@link
-   * IndexWriter#commit(Map)} has never been called for
-   * this index.
-   * 
-   * @param directory where the index resides.
-   * @return commit userData.
-   * @throws CorruptIndexException if the index is corrupt
-   * @throws IOException if there is a low-level IO error
-   *
-   * @see #getCommitUserData()
-   */
-  public static Map<String, String> getCommitUserData(Directory directory) throws CorruptIndexException, IOException {
-    return SegmentInfos.readCurrentUserData(directory);
-  }
-
-  /**
-   * Version number when this IndexReader was opened. Not
-   * implemented in the IndexReader base class.
-   *
-   * <p>If this reader is based on a Directory (ie, was
-   * created by calling {@link #open}, or {@link #openIfChanged} on
-   * a reader based on a Directory), then this method
-   * returns the version recorded in the commit that the
-   * reader opened.  This version is advanced every time
-   * a change is made with {@link IndexWriter}.</p>
-   *
-   * @throws UnsupportedOperationException unless overridden in subclass
-   */
-  public long getVersion() {
-    throw new UnsupportedOperationException("This reader does not support this method.");
-  }
-
-  /**
-   * Retrieve the String userData optionally passed to
-   * IndexWriter#commit.  This will return null if {@link
-   * IndexWriter#commit(Map)} has never been called for
-   * this index.
-   *
-   * @see #getCommitUserData(Directory)
-   */
-  public Map<String,String> getCommitUserData() {
-    throw new UnsupportedOperationException("This reader does not support this method.");
-  }
-
-
-  /**
-   * Check whether any new changes have occurred to the
-   * index since this reader was opened.
-   *
-   * <p>If this reader is based on a Directory (ie, was
-   * created by calling {@link #open}, or {@link #openIfChanged} on
-   * a reader based on a Directory), then this method checks
-   * if any further commits (see {@link IndexWriter#commit}
-   * have occurred in that directory).</p>
-   *
-   * <p>If instead this reader is a near real-time reader
-   * (ie, obtained by a call to {@link
-   * IndexWriter#getReader}, or by calling {@link #openIfChanged}
-   * on a near real-time reader), then this method checks if
-   * either a new commit has occurred, or any new
-   * uncommitted changes have taken place via the writer.
-   * Note that even if the writer has only performed
-   * merging, this method will still return false.</p>
-   *
-   * <p>In any event, if this returns false, you should call
-   * {@link #openIfChanged} to get a new reader that sees the
-   * changes.</p>
-   *
-   * @throws CorruptIndexException if the index is corrupt
-   * @throws IOException           if there is a low-level IO error
-   * @throws UnsupportedOperationException unless overridden in subclass
-   */
-  public boolean isCurrent() throws CorruptIndexException, IOException {
-    throw new UnsupportedOperationException("This reader does not support this method.");
-  }
-
   /** Retrieve term vectors for this document, or null if
    *  term vectors were not indexed.  The returned Fields
    *  instance acts like a single-document inverted index
@@ -567,21 +333,6 @@
     return vectors.terms(field);
   }
 
-  /**
-   * Returns <code>true</code> if an index exists at the specified directory.
-   * @param  directory the directory to check for an index
-   * @return <code>true</code> if an index exists; <code>false</code> otherwise
-   * @throws IOException if there is a problem with accessing the index
-   */
-  public static boolean indexExists(Directory directory) throws IOException {
-    try {
-      new SegmentInfos().read(directory);
-      return true;
-    } catch (IOException ioe) {
-      return false;
-    }
-  }
-
   /** Returns the number of documents in this index. */
   public abstract int numDocs();
 
@@ -646,167 +397,7 @@
   /** Returns true if any documents have been deleted */
   public abstract boolean hasDeletions();
 
-  /** Returns true if there are norms stored for this field. */
-  public boolean hasNorms(String field) throws IOException {
-    // backward compatible implementation.
-    // SegmentReader has an efficient implementation.
-    ensureOpen();
-    return normValues(field) != null;
-  }
-
   /**
-   * Returns {@link Fields} for this reader.
-   * This method may return null if the reader has no
-   * postings.
-   *
-   * <p><b>NOTE</b>: if this is a multi reader ({@link
-   * #getSequentialSubReaders} is not null) then this
-   * method will throw UnsupportedOperationException.  If
-   * you really need a {@link Fields} for such a reader,
-   * use {@link MultiFields#getFields}.  However, for
-   * performance reasons, it's best to get all sub-readers
-   * using {@link ReaderUtil#gatherSubReaders} and iterate
-   * through them yourself. */
-  public abstract Fields fields() throws IOException;
-  
-  public final int docFreq(Term term) throws IOException {
-    return docFreq(term.field(), term.bytes());
-  }
-
-  /** Returns the number of documents containing the term
-   * <code>t</code>.  This method returns 0 if the term or
-   * field does not exists.  This method does not take into
-   * account deleted documents that have not yet been merged
-   * away. */
-  public int docFreq(String field, BytesRef term) throws IOException {
-    final Fields fields = fields();
-    if (fields == null) {
-      return 0;
-    }
-    final Terms terms = fields.terms(field);
-    if (terms == null) {
-      return 0;
-    }
-    final TermsEnum termsEnum = terms.iterator(null);
-    if (termsEnum.seekExact(term, true)) {
-      return termsEnum.docFreq();
-    } else {
-      return 0;
-    }
-  }
-
-  /** Returns the number of documents containing the term
-   * <code>t</code>.  This method returns 0 if the term or
-   * field does not exists.  This method does not take into
-   * account deleted documents that have not yet been merged
-   * away. */
-  public final long totalTermFreq(String field, BytesRef term) throws IOException {
-    final Fields fields = fields();
-    if (fields == null) {
-      return 0;
-    }
-    final Terms terms = fields.terms(field);
-    if (terms == null) {
-      return 0;
-    }
-    final TermsEnum termsEnum = terms.iterator(null);
-    if (termsEnum.seekExact(term, true)) {
-      return termsEnum.totalTermFreq();
-    } else {
-      return 0;
-    }
-  }
-
-  /** This may return null if the field does not exist.*/
-  public final Terms terms(String field) throws IOException {
-    final Fields fields = fields();
-    if (fields == null) {
-      return null;
-    }
-    return fields.terms(field);
-  }
-
-  /** Returns {@link DocsEnum} for the specified field &
-   *  term.  This may return null, if either the field or
-   *  term does not exist. */
-  public final DocsEnum termDocsEnum(Bits liveDocs, String field, BytesRef term, boolean needsFreqs) throws IOException {
-    assert field != null;
-    assert term != null;
-    final Fields fields = fields();
-    if (fields != null) {
-      final Terms terms = fields.terms(field);
-      if (terms != null) {
-        final TermsEnum termsEnum = terms.iterator(null);
-        if (termsEnum.seekExact(term, true)) {
-          return termsEnum.docs(liveDocs, null, needsFreqs);
-        }
-      }
-    }
-    return null;
-  }
-
-  /** Returns {@link DocsAndPositionsEnum} for the specified
-   *  field & term.  This may return null, if either the
-   *  field or term does not exist, or needsOffsets is
-   *  true but offsets were not indexed for this field. */
-  public final DocsAndPositionsEnum termPositionsEnum(Bits liveDocs, String field, BytesRef term, boolean needsOffsets) throws IOException {
-    assert field != null;
-    assert term != null;
-    final Fields fields = fields();
-    if (fields != null) {
-      final Terms terms = fields.terms(field);
-      if (terms != null) {
-        final TermsEnum termsEnum = terms.iterator(null);
-        if (termsEnum.seekExact(term, true)) {
-          return termsEnum.docsAndPositions(liveDocs, null, needsOffsets);
-        }
-      }
-    }
-    return null;
-  }
-  
-  /**
-   * Returns {@link DocsEnum} for the specified field and
-   * {@link TermState}. This may return null, if either the field or the term
-   * does not exists or the {@link TermState} is invalid for the underlying
-   * implementation.*/
-  public final DocsEnum termDocsEnum(Bits liveDocs, String field, BytesRef term, TermState state, boolean needsFreqs) throws IOException {
-    assert state != null;
-    assert field != null;
-    final Fields fields = fields();
-    if (fields != null) {
-      final Terms terms = fields.terms(field);
-      if (terms != null) {
-        final TermsEnum termsEnum = terms.iterator(null);
-        termsEnum.seekExact(term, state);
-        return termsEnum.docs(liveDocs, null, needsFreqs);
-      }
-    }
-    return null;
-  }
-  
-  /**
-   * Returns {@link DocsAndPositionsEnum} for the specified field and
-   * {@link TermState}. This may return null, if either the field or the term
-   * does not exists, the {@link TermState} is invalid for the underlying
-   * implementation, or needsOffsets is true but offsets
-   * were not indexed for this field. */
-  public final DocsAndPositionsEnum termPositionsEnum(Bits liveDocs, String field, BytesRef term, TermState state, boolean needsOffsets) throws IOException {
-    assert state != null;
-    assert field != null;
-    final Fields fields = fields();
-    if (fields != null) {
-      final Terms terms = fields.terms(field);
-      if (terms != null) {
-        final TermsEnum termsEnum = terms.iterator(null);
-        termsEnum.seekExact(term, state);
-        return termsEnum.docsAndPositions(liveDocs, null, needsOffsets);
-      }
-    }
-    return null;
-  }
-
-  /**
    * Closes files associated with this index.
    * Also saves any new deletions to disk.
    * No other methods should be called after this has been called.
@@ -823,76 +414,12 @@
   protected abstract void doClose() throws IOException;
 
   /**
-   * Get the {@link FieldInfos} describing all fields in
-   * this reader.  NOTE: do not make any changes to the
-   * returned FieldInfos!
-   *
-   * @lucene.experimental
-   */
-  public abstract FieldInfos getFieldInfos();
-
-  /** Returns the {@link Bits} representing live (not
-   *  deleted) docs.  A set bit indicates the doc ID has not
-   *  been deleted.  If this method returns null it means
-   *  there are no deleted documents (all documents are
-   *  live).
-   *
-   *  The returned instance has been safely published for
-   *  use by multiple threads without additional
-   *  synchronization.
-   * @lucene.experimental */
-  public abstract Bits getLiveDocs();
-
-  /**
-   * Expert: return the IndexCommit that this reader has
-   * opened.  This method is only implemented by those
-   * readers that correspond to a Directory with its own
-   * segments_N file.
-   *
-   * @lucene.experimental
-   */
-  public IndexCommit getIndexCommit() throws IOException {
-    throw new UnsupportedOperationException("This reader does not support this method.");
-  }
-  
-  /** Returns all commit points that exist in the Directory.
-   *  Normally, because the default is {@link
-   *  KeepOnlyLastCommitDeletionPolicy}, there would be only
-   *  one commit point.  But if you're using a custom {@link
-   *  IndexDeletionPolicy} then there could be many commits.
-   *  Once you have a given commit, you can open a reader on
-   *  it by calling {@link IndexReader#open(IndexCommit)}
-   *  There must be at least one commit in
-   *  the Directory, else this method throws {@link
-   *  IndexNotFoundException}.  Note that if a commit is in
-   *  progress while this method is running, that commit
-   *  may or may not be returned.
-   *  
-   *  @return a sorted list of {@link IndexCommit}s, from oldest 
-   *  to latest. */
-  public static List<IndexCommit> listCommits(Directory dir) throws IOException {
-    return DirectoryReader.listCommits(dir);
-  }
-
-  /** Expert: returns the sequential sub readers that this
-   *  reader is logically composed of. If this reader is not composed
-   *  of sequential child readers, it should return null.
-   *  If this method returns an empty array, that means this
-   *  reader is a null reader (for example a MultiReader
-   *  that has no sub readers).
-   */
-  public IndexReader[] getSequentialSubReaders() {
-    ensureOpen();
-    return null;
-  }
-  
-  /**
-   * Expert: Returns a the root {@link ReaderContext} for this
+   * Expert: Returns a the root {@link IndexReaderContext} for this
    * {@link IndexReader}'s sub-reader tree. Iff this reader is composed of sub
    * readers ,ie. this reader being a composite reader, this method returns a
    * {@link CompositeReaderContext} holding the reader's direct children as well as a
    * view of the reader tree's atomic leaf contexts. All sub-
-   * {@link ReaderContext} instances referenced from this readers top-level
+   * {@link IndexReaderContext} instances referenced from this readers top-level
    * context are private to this reader and are not shared with another context
    * tree. For example, IndexSearcher uses this API to drive searching by one
    * atomic leaf reader at a time. If this reader is not composed of child
@@ -905,7 +432,7 @@
    * 
    * @lucene.experimental
    */
-  public abstract ReaderContext getTopReaderContext();
+  public abstract IndexReaderContext getTopReaderContext();
 
   /** Expert: Returns a key for this IndexReader, so FieldCache/CachingWrapperFilter can find
    * it again.
@@ -924,191 +451,15 @@
     // on close
     return this;
   }
-
-  /** Returns the number of unique terms (across all fields)
-   *  in this reader.
-   *
-   *  @return number of unique terms or -1 if this count
-   *  cannot be easily determined (eg Multi*Readers).
-   *  Instead, you should call {@link
-   *  #getSequentialSubReaders} and ask each sub reader for
-   *  its unique term count. */
-  public final long getUniqueTermCount() throws IOException {
-    if (!getTopReaderContext().isAtomic) {
-      return -1;
-    }
-    final Fields fields = fields();
-    if (fields == null) {
-      return 0;
-    }
-    return fields.getUniqueTermCount();
-  }
-
-  /** For IndexReader implementations that use
-   *  TermInfosReader to read terms, this returns the
-   *  current indexDivisor as specified when the reader was
-   *  opened.
-   */
-  public int getTermInfosIndexDivisor() {
-    throw new UnsupportedOperationException("This reader does not support this method.");
-  }
   
-  /**
-   * Returns {@link DocValues} for this field.
-   * This method may return null if the reader has no per-document
-   * values stored.
-   *
-   * <p><b>NOTE</b>: if this is a multi reader ({@link
-   * #getSequentialSubReaders} is not null) then this
-   * method will throw UnsupportedOperationException.  If
-   * you really need {@link DocValues} for such a reader,
-   * use {@link MultiDocValues#getDocValues(IndexReader,String)}.  However, for
-   * performance reasons, it's best to get all sub-readers
-   * using {@link ReaderUtil#gatherSubReaders} and iterate
-   * through them yourself. */
-  public abstract DocValues docValues(String field) throws IOException;
-  
-  public abstract DocValues normValues(String field) throws IOException;
-
-  private volatile Fields fields;
-
-  /** @lucene.internal */
-  void storeFields(Fields fields) {
-    ensureOpen();
-    this.fields = fields;
+  public final int docFreq(Term term) throws IOException {
+    return docFreq(term.field(), term.bytes());
   }
 
-  /** @lucene.internal */
-  Fields retrieveFields() {
-    ensureOpen();
-    return fields;
-  }
-  
-  /**
-   * A struct like class that represents a hierarchical relationship between
-   * {@link IndexReader} instances. 
-   * @lucene.experimental
-   */
-  public static abstract class ReaderContext {
-    /** The reader context for this reader's immediate parent, or null if none */
-    public final ReaderContext parent;
-    /** The actual reader */
-    public final IndexReader reader;
-    /** <code>true</code> iff the reader is an atomic reader */
-    public final boolean isAtomic;
-    /** <code>true</code> if this context struct represents the top level reader within the hierarchical context */
-    public final boolean isTopLevel;
-    /** the doc base for this reader in the parent, <tt>0</tt> if parent is null */
-    public final int docBaseInParent;
-    /** the ord for this reader in the parent, <tt>0</tt> if parent is null */
-    public final int ordInParent;
-    
-    ReaderContext(ReaderContext parent, IndexReader reader,
-        boolean isAtomic, int ordInParent, int docBaseInParent) {
-      this.parent = parent;
-      this.reader = reader;
-      this.isAtomic = isAtomic;
-      this.docBaseInParent = docBaseInParent;
-      this.ordInParent = ordInParent;
-      this.isTopLevel = parent==null;
-    }
-    
-    /**
-     * Returns the context's leaves if this context is a top-level context
-     * otherwise <code>null</code>.
-     * <p>
-     * Note: this is convenience method since leaves can always be obtained by
-     * walking the context tree.
-     */
-    public AtomicReaderContext[] leaves() {
-      return null;
-    }
-    
-    /**
-     * Returns the context's children iff this context is a composite context
-     * otherwise <code>null</code>.
-     * <p>
-     * Note: this method is a convenience method to prevent
-     * <code>instanceof</code> checks and type-casts to
-     * {@link CompositeReaderContext}.
-     */
-    public ReaderContext[] children() {
-      return null;
-    }
-  }
-  
-  /**
-   * {@link ReaderContext} for composite {@link IndexReader} instance.
-   * @lucene.experimental
-   */
-  public static final class CompositeReaderContext extends ReaderContext {
-    /** the composite readers immediate children */
-    public final ReaderContext[] children;
-    /** the composite readers leaf reader contexts if this is the top level reader in this context */
-    public final AtomicReaderContext[] leaves;
-
-    /**
-     * Creates a {@link CompositeReaderContext} for intermediate readers that aren't
-     * not top-level readers in the current context
-     */
-    public CompositeReaderContext(ReaderContext parent, IndexReader reader,
-        int ordInParent, int docbaseInParent, ReaderContext[] children) {
-      this(parent, reader, ordInParent, docbaseInParent, children, null);
-    }
-    
-    /**
-     * Creates a {@link CompositeReaderContext} for top-level readers with parent set to <code>null</code>
-     */
-    public CompositeReaderContext(IndexReader reader, ReaderContext[] children, AtomicReaderContext[] leaves) {
-      this(null, reader, 0, 0, children, leaves);
-    }
-    
-    private CompositeReaderContext(ReaderContext parent, IndexReader reader,
-        int ordInParent, int docbaseInParent, ReaderContext[] children,
-        AtomicReaderContext[] leaves) {
-      super(parent, reader, false, ordInParent, docbaseInParent);
-      this.children = children;
-      this.leaves = leaves;
-    }
-
-    @Override
-    public AtomicReaderContext[] leaves() {
-      return leaves;
-    }
-    
-    
-    @Override
-    public ReaderContext[] children() {
-      return children;
-    }
-  }
-  
-  /**
-   * {@link ReaderContext} for atomic {@link IndexReader} instances
-   * @lucene.experimental
-   */
-  public static final class AtomicReaderContext extends ReaderContext {
-    /** The readers ord in the top-level's leaves array */
-    public final int ord;
-    /** The readers absolute doc base */
-    public final int docBase;
-    /**
-     * Creates a new {@link AtomicReaderContext} 
-     */    
-    public AtomicReaderContext(ReaderContext parent, IndexReader reader,
-        int ord, int docBase, int leafOrd, int leafDocBase) {
-      super(parent, reader, true, ord, docBase);
-      assert reader.getSequentialSubReaders() == null : "Atomic readers must not have subreaders";
-      this.ord = leafOrd;
-      this.docBase = leafDocBase;
-    }
-    
-    /**
-     * Creates a new {@link AtomicReaderContext} for a atomic reader without an immediate
-     * parent.
-     */
-    public AtomicReaderContext(IndexReader atomicReader) {
-      this(null, atomicReader, 0, 0, 0, 0);
-    }
-  }
+  /** Returns the number of documents containing the term
+   * <code>t</code>.  This method returns 0 if the term or
+   * field does not exists.  This method does not take into
+   * account deleted documents that have not yet been merged
+   * away. */
+  public abstract int docFreq(String field, BytesRef term) throws IOException;
 }
Index: lucene/src/java/org/apache/lucene/index/IndexReaderContext.java
===================================================================
--- lucene/src/java/org/apache/lucene/index/IndexReaderContext.java	(revision 0)
+++ lucene/src/java/org/apache/lucene/index/IndexReaderContext.java	(working copy)
@@ -0,0 +1,64 @@
+package org.apache.lucene.index;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * A struct like class that represents a hierarchical relationship between
+ * {@link IndexReader} instances. 
+ * @lucene.experimental
+ */
+public abstract class IndexReaderContext {
+  /** The reader context for this reader's immediate parent, or null if none */
+  public final CompositeReaderContext parent;
+  /** <code>true</code> if this context struct represents the top level reader within the hierarchical context */
+  public final boolean isTopLevel;
+  /** the doc base for this reader in the parent, <tt>0</tt> if parent is null */
+  public final int docBaseInParent;
+  /** the ord for this reader in the parent, <tt>0</tt> if parent is null */
+  public final int ordInParent;
+  
+  IndexReaderContext(CompositeReaderContext parent, int ordInParent, int docBaseInParent) {
+    if (!(this instanceof CompositeReaderContext || this instanceof AtomicReaderContext))
+      throw new Error("This class should never be extended by custom code!");
+    this.parent = parent;
+    this.docBaseInParent = docBaseInParent;
+    this.ordInParent = ordInParent;
+    this.isTopLevel = parent==null;
+  }
+  
+  public abstract IndexReader reader();
+  
+  /**
+   * Returns the context's leaves if this context is a top-level context
+   * otherwise <code>null</code>.
+   * <p>
+   * Note: this is convenience method since leaves can always be obtained by
+   * walking the context tree.
+   */
+  public abstract AtomicReaderContext[] leaves();
+  
+  /**
+   * Returns the context's children iff this context is a composite context
+   * otherwise <code>null</code>.
+   * <p>
+   * Note: this method is a convenience method to prevent
+   * <code>instanceof</code> checks and type-casts to
+   * {@link CompositeReaderContext}.
+   */
+  public abstract IndexReaderContext[] children();
+}
\ No newline at end of file
Index: lucene/src/java/org/apache/lucene/index/IndexReaderContext.java
===================================================================
--- lucene/src/java/org/apache/lucene/index/IndexReaderContext.java	(revision 1238051)
+++ lucene/src/java/org/apache/lucene/index/IndexReaderContext.java	(working copy)

Property changes on: lucene/src/java/org/apache/lucene/index/IndexReaderContext.java
___________________________________________________________________
Added: svn:keywords
## -0,0 +1 ##
+Date Author Id Revision HeadURL
Added: svn:eol-style
## -0,0 +1 ##
+native
Index: lucene/src/java/org/apache/lucene/index/IndexUpgrader.java
===================================================================
--- lucene/src/java/org/apache/lucene/index/IndexUpgrader.java	(revision 1238034)
+++ lucene/src/java/org/apache/lucene/index/IndexUpgrader.java	(working copy)
@@ -134,7 +134,7 @@
   }
   
   public void upgrade() throws IOException {
-    if (!IndexReader.indexExists(dir)) {
+    if (!DirectoryReader.indexExists(dir)) {
       throw new IndexNotFoundException(dir.toString());
     }
   
Index: lucene/src/java/org/apache/lucene/index/IndexWriter.java
===================================================================
--- lucene/src/java/org/apache/lucene/index/IndexWriter.java	(revision 1238034)
+++ lucene/src/java/org/apache/lucene/index/IndexWriter.java	(working copy)
@@ -264,7 +264,7 @@
   // The PayloadProcessorProvider to use when segments are merged
   private PayloadProcessorProvider payloadProcessorProvider;
 
-  IndexReader getReader() throws IOException {
+  DirectoryReader getReader() throws IOException {
     return getReader(true);
   }
 
@@ -327,7 +327,7 @@
    *
    * @throws IOException
    */
-  IndexReader getReader(boolean applyAllDeletes) throws IOException {
+  DirectoryReader getReader(boolean applyAllDeletes) throws IOException {
     ensureOpen();
 
     final long tStart = System.currentTimeMillis();
@@ -339,7 +339,7 @@
     // obtained during this flush are pooled, the first time
     // this method is called:
     poolReaders = true;
-    final IndexReader r;
+    final DirectoryReader r;
     doBeforeFlush();
     boolean anySegmentFlushed = false;
     /*
@@ -871,7 +871,7 @@
         create = false;
       } else {
         // CREATE_OR_APPEND - create only if an index does not exist
-        create = !IndexReader.indexExists(directory);
+        create = !DirectoryReader.indexExists(directory);
       }
 
       // If index is too old, reading the segments will throw
@@ -2631,7 +2631,7 @@
    *  @param commitUserData Opaque Map (String->String)
    *  that's recorded into the segments file in the index,
    *  and retrievable by {@link
-   *  IndexReader#getCommitUserData}.  Note that when
+   *  DirectoryReader#getCommitUserData}.  Note that when
    *  IndexWriter commits itself during {@link #close}, the
    *  commitUserData is unchanged (just carried over from
    *  the prior commit).  If this is null then the previous
@@ -3954,7 +3954,7 @@
    * <p><b>NOTE</b>: warm is called before any deletes have
    * been carried over to the merged segment. */
   public static abstract class IndexReaderWarmer {
-    public abstract void warm(IndexReader reader) throws IOException;
+    public abstract void warm(AtomicReader reader) throws IOException;
   }
 
   private void handleOOM(OutOfMemoryError oom, String location) {
Index: lucene/src/java/org/apache/lucene/index/IndexWriterConfig.java
===================================================================
--- lucene/src/java/org/apache/lucene/index/IndexWriterConfig.java	(revision 1238034)
+++ lucene/src/java/org/apache/lucene/index/IndexWriterConfig.java	(working copy)
@@ -90,7 +90,7 @@
   public final static boolean DEFAULT_READER_POOLING = false;
 
   /** Default value is 1. Change using {@link #setReaderTermsIndexDivisor(int)}. */
-  public static final int DEFAULT_READER_TERMS_INDEX_DIVISOR = IndexReader.DEFAULT_TERMS_INDEX_DIVISOR;
+  public static final int DEFAULT_READER_TERMS_INDEX_DIVISOR = DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR;
 
   /** Default value is 1945. Change using {@link #setRAMPerThreadHardLimitMB(int)} */
   public static final int DEFAULT_RAM_PER_THREAD_HARD_LIMIT_MB = 1945;
Index: lucene/src/java/org/apache/lucene/index/MergeState.java
===================================================================
--- lucene/src/java/org/apache/lucene/index/MergeState.java	(revision 1238034)
+++ lucene/src/java/org/apache/lucene/index/MergeState.java	(working copy)
@@ -31,10 +31,10 @@
 public class MergeState {
 
   public static class IndexReaderAndLiveDocs {
-    public final IndexReader reader;
+    public final AtomicReader reader;
     public final Bits liveDocs;
 
-    public IndexReaderAndLiveDocs(IndexReader reader, Bits liveDocs) {
+    public IndexReaderAndLiveDocs(AtomicReader reader, Bits liveDocs) {
       this.reader = reader;
       this.liveDocs = liveDocs;
     }
Index: lucene/src/java/org/apache/lucene/index/MultiDocValues.java
===================================================================
--- lucene/src/java/org/apache/lucene/index/MultiDocValues.java	(revision 1238034)
+++ lucene/src/java/org/apache/lucene/index/MultiDocValues.java	(working copy)
@@ -33,7 +33,7 @@
 import org.apache.lucene.util.packed.PackedInts.Reader;
 
 /**
- * A wrapper for compound IndexReader providing access to per segment
+ * A wrapper for CompositeIndexReader providing access to per segment
  * {@link DocValues}
  * 
  * @lucene.experimental
@@ -43,11 +43,11 @@
   
   private static DocValuesPuller DEFAULT_PULLER = new DocValuesPuller();
   private static final DocValuesPuller NORMS_PULLER = new DocValuesPuller() {
-    public DocValues pull(IndexReader reader, String field) throws IOException {
+    public DocValues pull(AtomicReader reader, String field) throws IOException {
       return reader.normValues(field);
     }
     
-    public boolean stopLoadingOnNull(IndexReader reader, String field) throws IOException {
+    public boolean stopLoadingOnNull(AtomicReader reader, String field) throws IOException {
       // for norms we drop all norms if one leaf reader has no norms and the field is present
       FieldInfos fieldInfos = reader.getFieldInfos();
       FieldInfo fieldInfo = fieldInfos.fieldInfo(field);
@@ -69,11 +69,11 @@
   }
   
   private static class DocValuesPuller {
-    public DocValues pull(IndexReader reader, String field) throws IOException {
+    public DocValues pull(AtomicReader reader, String field) throws IOException {
       return reader.docValues(field);
     }
     
-    public boolean stopLoadingOnNull(IndexReader reader, String field) throws IOException {
+    public boolean stopLoadingOnNull(AtomicReader reader, String field) throws IOException {
       return false;
     }
   }
@@ -115,11 +115,13 @@
   
  
   private static DocValues getDocValues(IndexReader r, final String field, final DocValuesPuller puller) throws IOException {
-    final IndexReader[] subs = r.getSequentialSubReaders();
-    if (subs == null) {
+    if (r instanceof AtomicReader) {
       // already an atomic reader
-      return puller.pull(r, field);
-    } else if (subs.length == 0) {
+      return puller.pull((AtomicReader) r, field);
+    }
+    assert r instanceof CompositeReader;
+    final IndexReader[] subs = ((CompositeReader) r).getSequentialSubReaders();
+    if (subs.length == 0) {
       // no fields
       return null;
     } else if (subs.length == 1) {
@@ -136,7 +138,7 @@
       new ReaderUtil.Gather(r) {
         boolean stop = false;
         @Override
-        protected void add(int base, IndexReader r) throws IOException {
+        protected void add(int base, AtomicReader r) throws IOException {
           if (stop) {
             return;
           }
Index: lucene/src/java/org/apache/lucene/index/MultiFields.java
===================================================================
--- lucene/src/java/org/apache/lucene/index/MultiFields.java	(revision 1238034)
+++ lucene/src/java/org/apache/lucene/index/MultiFields.java	(working copy)
@@ -21,6 +21,8 @@
 import java.util.ArrayList;
 import java.util.List;
 import java.util.Map;
+import java.util.Collection;
+import java.util.HashSet;
 import java.util.concurrent.ConcurrentHashMap;
 
 import org.apache.lucene.util.Bits;
@@ -59,59 +61,50 @@
    *  Gather}) and iterate through them
    *  yourself. */
   public static Fields getFields(IndexReader r) throws IOException {
-    final IndexReader[] subs = r.getSequentialSubReaders();
-    if (subs == null) {
+    if (r instanceof AtomicReader) {
       // already an atomic reader
-      return r.fields();
-    } else if (subs.length == 0) {
+      return ((AtomicReader) r).fields();
+    }
+    assert r instanceof CompositeReader;
+    final IndexReader[] subs = ((CompositeReader) r).getSequentialSubReaders();
+    if (subs.length == 0) {
       // no fields
       return null;
-    } else if (subs.length == 1) {
-      return getFields(subs[0]);
     } else {
+      final List<Fields> fields = new ArrayList<Fields>();
+      final List<ReaderUtil.Slice> slices = new ArrayList<ReaderUtil.Slice>();
 
-      Fields currentFields = r.retrieveFields();
-      if (currentFields == null) {
-      
-        final List<Fields> fields = new ArrayList<Fields>();
-        final List<ReaderUtil.Slice> slices = new ArrayList<ReaderUtil.Slice>();
-
-        new ReaderUtil.Gather(r) {
-          @Override
-          protected void add(int base, IndexReader r) throws IOException {
-            final Fields f = r.fields();
-            if (f != null) {
-              fields.add(f);
-              slices.add(new ReaderUtil.Slice(base, r.maxDoc(), fields.size()-1));
-            }
+      new ReaderUtil.Gather(r) {
+        @Override
+        protected void add(int base, AtomicReader r) throws IOException {
+          final Fields f = r.fields();
+          if (f != null) {
+            fields.add(f);
+            slices.add(new ReaderUtil.Slice(base, r.maxDoc(), fields.size()-1));
           }
-        }.run();
+        }
+      }.run();
 
-        if (fields.size() == 0) {
-          return null;
-        } else if (fields.size() == 1) {
-          currentFields = fields.get(0);
-        } else {
-          currentFields = new MultiFields(fields.toArray(Fields.EMPTY_ARRAY),
-                                         slices.toArray(ReaderUtil.Slice.EMPTY_ARRAY));
-        }
-        r.storeFields(currentFields);
+      if (fields.isEmpty()) {
+        return null;
+      } else if (fields.size() == 1) {
+        return fields.get(0);
+      } else {
+        return new MultiFields(fields.toArray(Fields.EMPTY_ARRAY),
+                                       slices.toArray(ReaderUtil.Slice.EMPTY_ARRAY));
       }
-      return currentFields;
     }
   }
 
   public static Bits getLiveDocs(IndexReader r) {
-    Bits result;
     if (r.hasDeletions()) {
-
       final List<Bits> liveDocs = new ArrayList<Bits>();
       final List<Integer> starts = new ArrayList<Integer>();
 
       try {
         final int maxDoc = new ReaderUtil.Gather(r) {
             @Override
-            protected void add(int base, IndexReader r) throws IOException {
+            protected void add(int base, AtomicReader r) throws IOException {
               // record all liveDocs, even if they are null
               liveDocs.add(r.getLiveDocs());
               starts.add(base);
@@ -126,16 +119,13 @@
       assert liveDocs.size() > 0;
       if (liveDocs.size() == 1) {
         // Only one actual sub reader -- optimize this case
-        result = liveDocs.get(0);
+        return liveDocs.get(0);
       } else {
-        result = new MultiBits(liveDocs, starts, true);
+        return new MultiBits(liveDocs, starts, true);
       }
-
     } else {
-      result = null;
+      return null;
     }
-
-    return result;
   }
 
   /**  This method may return null if the field does not exist.*/
@@ -237,6 +227,11 @@
     return result;
   }
 
+  @Override
+  public int getUniqueFieldCount() {
+    return -1;
+  }
+
   public static long totalTermFreq(IndexReader r, String field, BytesRef text) throws IOException {
     final Terms terms = getTerms(r, field);
     if (terms != null) {
@@ -248,9 +243,26 @@
     return 0;
   }
 
-  @Override
-  public int getUniqueFieldCount() {
-    return -1;
+  /** Call this to get the (merged) FieldInfos for a
+   *  composite reader */
+  public static FieldInfos getMergedFieldInfos(IndexReader reader) {
+    final List<AtomicReader> subReaders = new ArrayList<AtomicReader>();
+    ReaderUtil.gatherSubReaders(subReaders, reader);
+    final FieldInfos fieldInfos = new FieldInfos();
+    for(AtomicReader subReader : subReaders) {
+      fieldInfos.add(subReader.getFieldInfos());
+    }
+    return fieldInfos;
   }
+
+  public static Collection<String> getIndexedFields(IndexReader reader) {
+    final Collection<String> fields = new HashSet<String>();
+    for(FieldInfo fieldInfo : getMergedFieldInfos(reader)) {
+      if (fieldInfo.isIndexed) {
+        fields.add(fieldInfo.name);
+      }
+    }
+    return fields;
+  }
 }
 
Index: lucene/src/java/org/apache/lucene/index/MultiReader.java
===================================================================
--- lucene/src/java/org/apache/lucene/index/MultiReader.java	(revision 1238034)
+++ lucene/src/java/org/apache/lucene/index/MultiReader.java	(working copy)
@@ -22,7 +22,7 @@
 /** An IndexReader which reads multiple indexes, appending
  *  their content. */
 public class MultiReader extends BaseMultiReader<IndexReader> {
-  private final boolean[] decrefOnClose; // remember which subreaders to decRef on close
+  private final boolean closeSubReaders;
   
  /**
   * <p>Construct a MultiReader aggregating the named set of (sub)readers.
@@ -41,80 +41,23 @@
    */
   public MultiReader(IndexReader[] subReaders, boolean closeSubReaders) throws IOException {
     super(subReaders.clone());
-    decrefOnClose = new boolean[subReaders.length];
-    for (int i = 0; i < subReaders.length; i++) {
-      if (!closeSubReaders) {
+    this.closeSubReaders = closeSubReaders;
+    if (!closeSubReaders) {
+      for (int i = 0; i < subReaders.length; i++) {
         subReaders[i].incRef();
-        decrefOnClose[i] = true;
-      } else {
-        decrefOnClose[i] = false;
       }
     }
   }
-  
-  // used only by openIfChaged
-  private MultiReader(IndexReader[] subReaders, boolean[] decrefOnClose)
-                      throws IOException {
-    super(subReaders);
-    this.decrefOnClose = decrefOnClose;
-  }
 
   @Override
-  protected synchronized IndexReader doOpenIfChanged() throws CorruptIndexException, IOException {
-    ensureOpen();
-    
-    boolean changed = false;
-    IndexReader[] newSubReaders = new IndexReader[subReaders.length];
-    
-    boolean success = false;
-    try {
-      for (int i = 0; i < subReaders.length; i++) {
-        final IndexReader newSubReader = IndexReader.openIfChanged(subReaders[i]);
-        if (newSubReader != null) {
-          newSubReaders[i] = newSubReader;
-          changed = true;
-        } else {
-          newSubReaders[i] = subReaders[i];
-        }
-      }
-      success = true;
-    } finally {
-      if (!success && changed) {
-        for (int i = 0; i < newSubReaders.length; i++) {
-          if (newSubReaders[i] != subReaders[i]) {
-            try {
-              newSubReaders[i].close();
-            } catch (IOException ignore) {
-              // keep going - we want to clean up as much as possible
-            }
-          }
-        }
-      }
-    }
-
-    if (changed) {
-      boolean[] newDecrefOnClose = new boolean[subReaders.length];
-      for (int i = 0; i < subReaders.length; i++) {
-        if (newSubReaders[i] == subReaders[i]) {
-          newSubReaders[i].incRef();
-          newDecrefOnClose[i] = true;
-        }
-      }
-      return new MultiReader(newSubReaders, newDecrefOnClose);
-    } else {
-      return null;
-    }
-  }
-
-  @Override
   protected synchronized void doClose() throws IOException {
     IOException ioe = null;
     for (int i = 0; i < subReaders.length; i++) {
       try {
-        if (decrefOnClose[i]) {
+        if (closeSubReaders) {
+          subReaders[i].close();
+        } else {
           subReaders[i].decRef();
-        } else {
-          subReaders[i].close();
         }
       } catch (IOException e) {
         if (ioe == null) ioe = e;
@@ -123,25 +66,4 @@
     // throw the first exception
     if (ioe != null) throw ioe;
   }
-  
-  @Override
-  public boolean isCurrent() throws CorruptIndexException, IOException {
-    ensureOpen();
-    for (int i = 0; i < subReaders.length; i++) {
-      if (!subReaders[i].isCurrent()) {
-        return false;
-      }
-    }
-    
-    // all subreaders are up to date
-    return true;
-  }
-  
-  /** Not implemented.
-   * @throws UnsupportedOperationException
-   */
-  @Override
-  public long getVersion() {
-    throw new UnsupportedOperationException("MultiReader does not support this method.");
-  }
 }
Index: lucene/src/java/org/apache/lucene/index/ParallelReader.java
===================================================================
--- lucene/src/java/org/apache/lucene/index/ParallelReader.java	(revision 1238034)
+++ lucene/src/java/org/apache/lucene/index/ParallelReader.java	(working copy)
@@ -22,10 +22,9 @@
 
 import org.apache.lucene.util.Bits;
 import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.ReaderUtil;
 
 
-/** An IndexReader which reads multiple, parallel indexes.  Each index added
+/** An AtomicIndexReader which reads multiple, parallel indexes.  Each index added
  * must have the same number of documents, but typically each contains
  * different fields.  Each document contains the union of the fields of all
  * documents with the same document number.  When searching, matches for a
@@ -42,15 +41,14 @@
  * same order to the other indexes. <em>Failure to do so will result in
  * undefined behavior</em>.
  */
-public class ParallelReader extends IndexReader {
-  private List<IndexReader> readers = new ArrayList<IndexReader>();
+public class ParallelReader extends AtomicReader {
+  private List<AtomicReader> readers = new ArrayList<AtomicReader>();
   private List<Boolean> decrefOnClose = new ArrayList<Boolean>(); // remember which subreaders to decRef on close
   boolean incRefReaders = false;
-  private SortedMap<String,IndexReader> fieldToReader = new TreeMap<String,IndexReader>();
-  private Map<IndexReader,Collection<String>> readerToFields = new HashMap<IndexReader,Collection<String>>();
-  private List<IndexReader> storedFieldReaders = new ArrayList<IndexReader>();
+  private SortedMap<String,AtomicReader> fieldToReader = new TreeMap<String,AtomicReader>();
+  private Map<AtomicReader,Collection<String>> readerToFields = new HashMap<AtomicReader,Collection<String>>();
+  private List<AtomicReader> storedFieldReaders = new ArrayList<AtomicReader>();
   private Map<String, DocValues> normsCache = new HashMap<String,DocValues>();
-  private final ReaderContext topLevelReaderContext = new AtomicReaderContext(this);
   private int maxDoc;
   private int numDocs;
   private boolean hasDeletions;
@@ -77,7 +75,7 @@
   @Override
   public String toString() {
     final StringBuilder buffer = new StringBuilder("ParallelReader(");
-    final Iterator<IndexReader> iter = readers.iterator();
+    final Iterator<AtomicReader> iter = readers.iterator();
     if (iter.hasNext()) {
       buffer.append(iter.next());
     }
@@ -88,25 +86,25 @@
     return buffer.toString();
   }
   
- /** Add an IndexReader.
+ /** Add an AtomicIndexReader.
   * @throws IOException if there is a low-level IO error
   */
-  public void add(IndexReader reader) throws IOException {
+  public void add(AtomicReader reader) throws IOException {
     ensureOpen();
     add(reader, false);
   }
 
- /** Add an IndexReader whose stored fields will not be returned.  This can
+ /** Add an AtomicIndexReader whose stored fields will not be returned.  This can
   * accelerate search when stored fields are only needed from a subset of
   * the IndexReaders.
   *
   * @throws IllegalArgumentException if not all indexes contain the same number
   *     of documents
   * @throws IllegalArgumentException if not all indexes have the same value
-  *     of {@link IndexReader#maxDoc()}
+  *     of {@link AtomicReader#maxDoc()}
   * @throws IOException if there is a low-level IO error
   */
-  public void add(IndexReader reader, boolean ignoreStoredFields)
+  public void add(AtomicReader reader, boolean ignoreStoredFields)
     throws IOException {
 
     ensureOpen();
@@ -123,13 +121,13 @@
       throw new IllegalArgumentException
         ("All readers must have same numDocs: "+numDocs+"!="+reader.numDocs());
 
-    final FieldInfos readerFieldInfos = ReaderUtil.getMergedFieldInfos(reader);
+    final FieldInfos readerFieldInfos = MultiFields.getMergedFieldInfos(reader);
     for(FieldInfo fieldInfo : readerFieldInfos) {   // update fieldToReader map
       // NOTE: first reader having a given field "wins":
       if (fieldToReader.get(fieldInfo.name) == null) {
         fieldInfos.add(fieldInfo);
         fieldToReader.put(fieldInfo.name, reader);
-        this.fields.addField(fieldInfo.name, MultiFields.getFields(reader).terms(fieldInfo.name));
+        this.fields.addField(fieldInfo.name, reader.terms(fieldInfo.name));
       }
     }
 
@@ -205,7 +203,7 @@
   @Override
   public Bits getLiveDocs() {
     ensureOpen();
-    return MultiFields.getLiveDocs(readers.get(0));
+    return readers.get(0).getLiveDocs();
   }
 
   @Override
@@ -214,89 +212,7 @@
     return fields;
   }
   
-  /**
-   * Tries to reopen the subreaders.
-   * <br>
-   * If one or more subreaders could be re-opened (i. e. subReader.reopen() 
-   * returned a new instance != subReader), then a new ParallelReader instance 
-   * is returned, otherwise null is returned.
-   * <p>
-   * A re-opened instance might share one or more subreaders with the old 
-   * instance. Index modification operations result in undefined behavior
-   * when performed before the old instance is closed.
-   * (see {@link IndexReader#openIfChanged}).
-   * <p>
-   * If subreaders are shared, then the reference count of those
-   * readers is increased to ensure that the subreaders remain open
-   * until the last referring reader is closed.
-   * 
-   * @throws CorruptIndexException if the index is corrupt
-   * @throws IOException if there is a low-level IO error 
-   */
   @Override
-  protected synchronized IndexReader doOpenIfChanged() throws CorruptIndexException, IOException {
-    ensureOpen();
-    
-    boolean reopened = false;
-    List<IndexReader> newReaders = new ArrayList<IndexReader>();
-    
-    boolean success = false;
-    
-    try {
-      for (final IndexReader oldReader : readers) {
-        IndexReader newReader = null;
-        newReader = IndexReader.openIfChanged(oldReader);
-        if (newReader != null) {
-          reopened = true;
-        } else {
-          newReader = oldReader;
-        }
-        newReaders.add(newReader);
-      }
-      success = true;
-    } finally {
-      if (!success && reopened) {
-        for (int i = 0; i < newReaders.size(); i++) {
-          IndexReader r = newReaders.get(i);
-          if (r != readers.get(i)) {
-            try {
-              r.close();
-            } catch (IOException ignore) {
-              // keep going - we want to clean up as much as possible
-            }
-          }
-        }
-      }
-    }
-
-    if (reopened) {
-      List<Boolean> newDecrefOnClose = new ArrayList<Boolean>();
-      // TODO: maybe add a special reopen-ctor for norm-copying?
-      ParallelReader pr = new ParallelReader();
-      for (int i = 0; i < readers.size(); i++) {
-        IndexReader oldReader = readers.get(i);
-        IndexReader newReader = newReaders.get(i);
-        if (newReader == oldReader) {
-          newDecrefOnClose.add(Boolean.TRUE);
-          newReader.incRef();
-        } else {
-          // this is a new subreader instance, so on close() we don't
-          // decRef but close it 
-          newDecrefOnClose.add(Boolean.FALSE);
-        }
-        pr.add(newReader, !storedFieldReaders.contains(oldReader));
-      }
-      pr.decrefOnClose = newDecrefOnClose;
-      pr.incRefReaders = incRefReaders;
-      return pr;
-    } else {
-      // No subreader was refreshed
-      return null;
-    }
-  }
-
-
-  @Override
   public int numDocs() {
     // Don't call ensureOpen() here (it could affect performance)
     return numDocs;
@@ -317,7 +233,7 @@
   @Override
   public void document(int docID, StoredFieldVisitor visitor) throws CorruptIndexException, IOException {
     ensureOpen();
-    for (final IndexReader reader: storedFieldReaders) {
+    for (final AtomicReader reader: storedFieldReaders) {
       reader.document(docID, visitor);
     }
   }
@@ -327,7 +243,7 @@
   public Fields getTermVectors(int docID) throws IOException {
     ensureOpen();
     ParallelFields fields = new ParallelFields();
-    for (Map.Entry<String,IndexReader> ent : fieldToReader.entrySet()) {
+    for (Map.Entry<String,AtomicReader> ent : fieldToReader.entrySet()) {
       String fieldName = ent.getKey();
       Terms vector = ent.getValue().getTermVector(docID, fieldName);
       if (vector != null) {
@@ -341,44 +257,13 @@
   @Override
   public boolean hasNorms(String field) throws IOException {
     ensureOpen();
-    IndexReader reader = fieldToReader.get(field);
+    AtomicReader reader = fieldToReader.get(field);
     return reader==null ? false : reader.hasNorms(field);
   }
 
-  @Override
-  public int docFreq(String field, BytesRef term) throws IOException {
-    ensureOpen();
-    IndexReader reader = fieldToReader.get(field);
-    return reader == null? 0 : reader.docFreq(field, term);
-  }
-
-  /**
-   * Checks recursively if all subreaders are up to date. 
-   */
-  @Override
-  public boolean isCurrent() throws CorruptIndexException, IOException {
-    ensureOpen();
-    for (final IndexReader reader : readers) {
-      if (!reader.isCurrent()) {
-        return false;
-      }
-    }
-    
-    // all subreaders are up to date
-    return true;
-  }
-
-  /** Not implemented.
-   * @throws UnsupportedOperationException
-   */
-  @Override
-  public long getVersion() {
-    throw new UnsupportedOperationException("ParallelReader does not support this method.");
-  }
-
   // for testing
-  IndexReader[] getSubReaders() {
-    return readers.toArray(new IndexReader[readers.size()]);
+  AtomicReader[] getSubReaders() {
+    return readers.toArray(new AtomicReader[readers.size()]);
   }
 
   @Override
@@ -392,17 +277,11 @@
     }
   }
 
-  @Override
-  public ReaderContext getTopReaderContext() {
-    ensureOpen();
-    return topLevelReaderContext;
-  }
-
   // TODO: I suspect this is completely untested!!!!!
   @Override
   public DocValues docValues(String field) throws IOException {
-    IndexReader reader = fieldToReader.get(field);
-    return reader == null ? null : MultiDocValues.getDocValues(reader, field);
+    AtomicReader reader = fieldToReader.get(field);
+    return reader == null ? null : reader.docValues(field);
   }
   
   // TODO: I suspect this is completely untested!!!!!
@@ -410,8 +289,8 @@
   public synchronized DocValues normValues(String field) throws IOException {
     DocValues values = normsCache.get(field);
     if (values == null) {
-      IndexReader reader = fieldToReader.get(field);
-      values = reader == null ? null : MultiDocValues.getNormDocValues(reader, field);
+      AtomicReader reader = fieldToReader.get(field);
+      values = reader == null ? null : reader.normValues(field);
       normsCache.put(field, values);
     } 
     return values;
Index: lucene/src/java/org/apache/lucene/index/PersistentSnapshotDeletionPolicy.java
===================================================================
--- lucene/src/java/org/apache/lucene/index/PersistentSnapshotDeletionPolicy.java	(revision 1238034)
+++ lucene/src/java/org/apache/lucene/index/PersistentSnapshotDeletionPolicy.java	(working copy)
@@ -62,7 +62,7 @@
    * keeps a lock on the snapshots directory).
    */
   public static Map<String, String> readSnapshotsInfo(Directory dir) throws IOException {
-    IndexReader r = IndexReader.open(dir);
+    IndexReader r = DirectoryReader.open(dir);
     Map<String, String> snapshots = new HashMap<String, String>();
     try {
       int numDocs = r.numDocs();
Index: lucene/src/java/org/apache/lucene/index/SegmentMerger.java
===================================================================
--- lucene/src/java/org/apache/lucene/index/SegmentMerger.java	(revision 1238034)
+++ lucene/src/java/org/apache/lucene/index/SegmentMerger.java	(working copy)
@@ -76,7 +76,7 @@
     try {
       new ReaderUtil.Gather(reader) {
         @Override
-        protected void add(int base, IndexReader r) {
+        protected void add(int base, AtomicReader r) {
           mergeState.readers.add(new MergeState.IndexReaderAndLiveDocs(r, r.getLiveDocs()));
         }
       }.run();
@@ -201,7 +201,7 @@
     Map<FieldInfo,TypePromoter> normValuesTypes = new HashMap<FieldInfo,TypePromoter>();
 
     for (MergeState.IndexReaderAndLiveDocs readerAndLiveDocs : mergeState.readers) {
-      final IndexReader reader = readerAndLiveDocs.reader;
+      final AtomicReader reader = readerAndLiveDocs.reader;
       FieldInfos readerFieldInfos = reader.getFieldInfos();
       for (FieldInfo fi : readerFieldInfos) {
         FieldInfo merged = mergeState.fieldInfos.add(fi);
@@ -323,7 +323,12 @@
       docBase += docCount;
 
       if (mergeState.payloadProcessorProvider != null) {
-        mergeState.dirPayloadProcessor[i] = mergeState.payloadProcessorProvider.getDirProcessor(reader.reader.directory());
+        // TODO: the PayloadProcessorProvider should take AtomicReader as parameter
+        // and find out by itself if it can provide a processor:
+        if (!(reader.reader instanceof SegmentReader))
+          throw new UnsupportedOperationException("Payload processing currently requires exclusively SegmentReaders to be merged.");
+        final Directory dir = ((SegmentReader) reader.reader).directory();
+        mergeState.dirPayloadProcessor[i] = mergeState.payloadProcessorProvider.getDirProcessor(dir);
       }
 
       i++;
Index: lucene/src/java/org/apache/lucene/index/SegmentReader.java
===================================================================
--- lucene/src/java/org/apache/lucene/index/SegmentReader.java	(revision 1238034)
+++ lucene/src/java/org/apache/lucene/index/SegmentReader.java	(working copy)
@@ -30,11 +30,9 @@
 /**
  * @lucene.experimental
  */
-public final class SegmentReader extends IndexReader {
+public final class SegmentReader extends AtomicReader {
 
   private final SegmentInfo si;
-  private final ReaderContext readerContext = new AtomicReaderContext(this);
-  
   private final Bits liveDocs;
 
   // Normally set to si.docCount - si.delDocCount, unless we
@@ -186,12 +184,6 @@
     return si.toString(si.dir, si.docCount - numDocs - si.getDelCount());
   }
   
-  @Override
-  public ReaderContext getTopReaderContext() {
-    ensureOpen();
-    return readerContext;
-  }
-
   /**
    * Return the name of the segment this reader is reading.
    */
@@ -207,7 +199,6 @@
   }
 
   /** Returns the directory this index resides in. */
-  @Override
   public Directory directory() {
     // Don't ensureOpen here -- in certain cases, when a
     // cloned/reopened reader needs to commit, it may call
@@ -228,7 +219,6 @@
     return this;
   }
   
-  @Override
   public int getTermInfosIndexDivisor() {
     return core.termsIndexDivisor;
   }
Index: lucene/src/java/org/apache/lucene/index/SlowCompositeReaderWrapper.java
===================================================================
--- lucene/src/java/org/apache/lucene/index/SlowCompositeReaderWrapper.java	(revision 0)
+++ lucene/src/java/org/apache/lucene/index/SlowCompositeReaderWrapper.java	(working copy)
@@ -0,0 +1,166 @@
+package org.apache.lucene.index;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.HashMap;
+import java.util.Map;
+
+import org.apache.lucene.util.Bits;
+import org.apache.lucene.util.ReaderUtil; // javadoc
+
+import org.apache.lucene.index.DirectoryReader; // javadoc
+import org.apache.lucene.index.MultiReader; // javadoc
+
+/**
+ * This class forces a composite reader (eg a {@link
+ * MultiReader} or {@link DirectoryReader} or any other
+ * IndexReader subclass that returns non-null from {@link
+ * CompositeReader#getSequentialSubReaders}) to emulate an
+ * atomic reader.  This requires implementing the postings
+ * APIs on-the-fly, using the static methods in {@link
+ * MultiFields}, {@link MultiDocValues}, 
+ * by stepping through the sub-readers to merge fields/terms, 
+ * appending docs, etc.
+ *
+ * <p>If you ever hit an UnsupportedOperationException saying
+ * "please use MultiXXX.YYY instead", the simple
+ * but non-performant workaround is to wrap your reader
+ * using this class.</p>
+ *
+ * <p><b>NOTE</b>: this class almost always results in a
+ * performance hit.  If this is important to your use case,
+ * it's better to get the sequential sub readers (see {@link
+ * ReaderUtil#gatherSubReaders}, instead, and iterate through them
+ * yourself.</p>
+ */
+
+public final class SlowCompositeReaderWrapper extends AtomicReader {
+
+  private final CompositeReader in;
+  private final Map<String, DocValues> normsCache = new HashMap<String, DocValues>();
+  private final Fields fields;
+  private final Bits liveDocs;
+  
+  /** This method is sugar for getting an {@link AtomicReader} from
+   * an {@link IndexReader} of any kind. If the reader is already atomic,
+   * it is returned unchanged, otherwise wrapped by this class.
+   */
+  public static AtomicReader wrap(IndexReader reader) throws IOException {
+    if (reader instanceof CompositeReader) {
+      return new SlowCompositeReaderWrapper((CompositeReader) reader);
+    } else {
+      assert reader instanceof AtomicReader;
+      return (AtomicReader) reader;
+    }
+  }
+  
+  public SlowCompositeReaderWrapper(CompositeReader reader) throws IOException {
+    super();
+    in = reader;
+    fields = MultiFields.getFields(in);
+    liveDocs = MultiFields.getLiveDocs(in);
+  }
+
+  @Override
+  public String toString() {
+    return "SlowCompositeReaderWrapper(" + in + ")";
+  }
+
+  @Override
+  public Fields fields() throws IOException {
+    ensureOpen();
+    return fields;
+  }
+
+  @Override
+  public DocValues docValues(String field) throws IOException {
+    ensureOpen();
+    return MultiDocValues.getDocValues(in, field);
+  }
+  
+  @Override
+  public synchronized DocValues normValues(String field) throws IOException {
+    ensureOpen();
+    DocValues values = normsCache.get(field);
+    if (values == null) {
+      values = MultiDocValues.getNormDocValues(in, field);
+      normsCache.put(field, values);
+    }
+    return values;
+  }
+  
+  @Override
+  public Fields getTermVectors(int docID)
+          throws IOException {
+    ensureOpen();
+    return in.getTermVectors(docID);
+  }
+
+  @Override
+  public int numDocs() {
+    // Don't call ensureOpen() here (it could affect performance)
+    return in.numDocs();
+  }
+
+  @Override
+  public int maxDoc() {
+    // Don't call ensureOpen() here (it could affect performance)
+    return in.maxDoc();
+  }
+
+  @Override
+  public void document(int docID, StoredFieldVisitor visitor) throws CorruptIndexException, IOException {
+    ensureOpen();
+    in.document(docID, visitor);
+  }
+
+  @Override
+  public Bits getLiveDocs() {
+    ensureOpen();
+    return liveDocs;
+  }
+
+  @Override
+  public FieldInfos getFieldInfos() {
+    ensureOpen();
+    return MultiFields.getMergedFieldInfos(in);
+  }
+  
+  @Override
+  public boolean hasDeletions() {
+    ensureOpen();
+    return liveDocs != null;
+  }
+
+  @Override
+  public Object getCoreCacheKey() {
+    return in.getCoreCacheKey();
+  }
+
+  @Override
+  public Object getCombinedCoreAndDeletesKey() {
+    return in.getCombinedCoreAndDeletesKey();
+  }
+
+  @Override
+  protected void doClose() throws IOException {
+    // TODO: as this is a wrapper, should we really close the delegate?
+    in.close();
+  }
+}
Index: lucene/src/java/org/apache/lucene/index/SlowCompositeReaderWrapper.java
===================================================================
--- lucene/src/java/org/apache/lucene/index/SlowCompositeReaderWrapper.java	(revision 1238051)
+++ lucene/src/java/org/apache/lucene/index/SlowCompositeReaderWrapper.java	(working copy)

Property changes on: lucene/src/java/org/apache/lucene/index/SlowCompositeReaderWrapper.java
___________________________________________________________________
Added: svn:eol-style
## -0,0 +1 ##
+native
Index: lucene/src/java/org/apache/lucene/index/SlowMultiReaderWrapper.java
===================================================================
--- lucene/src/java/org/apache/lucene/index/SlowMultiReaderWrapper.java	(revision 1238034)
+++ lucene/src/java/org/apache/lucene/index/SlowMultiReaderWrapper.java	(working copy)
@@ -1,111 +0,0 @@
-package org.apache.lucene.index;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.HashMap;
-import java.util.Map;
-
-import org.apache.lucene.util.Bits;
-import org.apache.lucene.util.ReaderUtil; // javadoc
-
-import org.apache.lucene.index.DirectoryReader; // javadoc
-import org.apache.lucene.index.MultiReader; // javadoc
-
-/**
- * This class forces a composite reader (eg a {@link
- * MultiReader} or {@link DirectoryReader} or any other
- * IndexReader subclass that returns non-null from {@link
- * IndexReader#getSequentialSubReaders}) to emulate an
- * atomic reader.  This requires implementing the postings
- * APIs on-the-fly, using the static methods in {@link
- * MultiFields}, {@link MultiDocValues}, 
- * by stepping through the sub-readers to merge fields/terms, 
- * appending docs, etc.
- *
- * <p>If you ever hit an UnsupportedOperationException saying
- * "please use MultiXXX.YYY instead", the simple
- * but non-performant workaround is to wrap your reader
- * using this class.</p>
- *
- * <p><b>NOTE</b>: this class almost always results in a
- * performance hit.  If this is important to your use case,
- * it's better to get the sequential sub readers (see {@link
- * ReaderUtil#gatherSubReaders}, instead, and iterate through them
- * yourself.</p>
- */
-
-public final class SlowMultiReaderWrapper extends FilterIndexReader {
-
-  private final ReaderContext readerContext;
-  private final Map<String, DocValues> normsCache = new HashMap<String, DocValues>();
-  
-  public SlowMultiReaderWrapper(IndexReader other) {
-    super(other);
-    readerContext = new AtomicReaderContext(this); // emulate atomic reader!
-  }
-
-  @Override
-  public String toString() {
-    return "SlowMultiReaderWrapper(" + in + ")";
-  }
-
-  @Override
-  public Fields fields() throws IOException {
-    ensureOpen();
-    return MultiFields.getFields(in);
-  }
-
-  @Override
-  public DocValues docValues(String field) throws IOException {
-    ensureOpen();
-    return MultiDocValues.getDocValues(in, field);
-  }
-  
-  @Override
-  public synchronized DocValues normValues(String field) throws IOException {
-    ensureOpen();
-    DocValues values = normsCache.get(field);
-    if (values == null) {
-      values = MultiDocValues.getNormDocValues(in, field);
-      normsCache.put(field, values);
-    }
-    return values;
-  }
-  @Override
-  public Bits getLiveDocs() {
-    ensureOpen();
-    return MultiFields.getLiveDocs(in);
-  }
-  
-  @Override
-  public IndexReader[] getSequentialSubReaders() {
-    return null;
-  }
-  
-  @Override
-  public ReaderContext getTopReaderContext() {
-    ensureOpen();
-    return readerContext;
-  }
-
-  @Override
-  public FieldInfos getFieldInfos() {
-    return ReaderUtil.getMergedFieldInfos(in);
-  }
-}
Index: lucene/src/java/org/apache/lucene/search/BooleanQuery.java
===================================================================
--- lucene/src/java/org/apache/lucene/search/BooleanQuery.java	(revision 1238034)
+++ lucene/src/java/org/apache/lucene/search/BooleanQuery.java	(working copy)
@@ -20,8 +20,8 @@
 import java.io.IOException;
 import java.util.*;
 
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.DocsEnum;
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.index.TermsEnum;
@@ -240,7 +240,7 @@
       for (Iterator<Weight> wIter = weights.iterator(); wIter.hasNext();) {
         Weight w = wIter.next();
         BooleanClause c = cIter.next();
-        if (w.scorer(context, true, true, context.reader.getLiveDocs()) == null) {
+        if (w.scorer(context, true, true, context.reader().getLiveDocs()) == null) {
           if (c.isRequired()) {
             fail = true;
             Explanation r = new Explanation(0.0f, "no match on required clause (" + c.getQuery().toString() + ")");
Index: lucene/src/java/org/apache/lucene/search/BooleanScorer.java
===================================================================
--- lucene/src/java/org/apache/lucene/search/BooleanScorer.java	(revision 1238034)
+++ lucene/src/java/org/apache/lucene/search/BooleanScorer.java	(working copy)
@@ -22,7 +22,7 @@
 import java.util.Collection;
 import java.util.List;
 
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.search.BooleanClause.Occur;
 import org.apache.lucene.search.BooleanQuery.BooleanWeight;
 
Index: lucene/src/java/org/apache/lucene/search/CachingCollector.java
===================================================================
--- lucene/src/java/org/apache/lucene/search/CachingCollector.java	(revision 1238034)
+++ lucene/src/java/org/apache/lucene/search/CachingCollector.java	(working copy)
@@ -17,7 +17,7 @@
  * limitations under the License.
  */
 
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.util.RamUsageEstimator;
 
 import java.io.IOException;
Index: lucene/src/java/org/apache/lucene/search/CachingWrapperFilter.java
===================================================================
--- lucene/src/java/org/apache/lucene/search/CachingWrapperFilter.java	(revision 1238034)
+++ lucene/src/java/org/apache/lucene/search/CachingWrapperFilter.java	(working copy)
@@ -22,8 +22,9 @@
 import java.util.Map;
 import java.util.WeakHashMap;
 
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
+import org.apache.lucene.index.AtomicReader;
+import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.DirectoryReader; // javadocs
 import org.apache.lucene.util.FixedBitSet;
 import org.apache.lucene.util.Bits;
 
@@ -53,13 +54,13 @@
 
   /** Wraps another filter's result and caches it. If
    * {@code recacheDeletes} is {@code true}, then new deletes (for example
-   * after {@link IndexReader#openIfChanged}) will cause the filter
+   * after {@link DirectoryReader#openIfChanged}) will cause the filter
    * {@link DocIdSet} to be recached.
    *
    * <p>If your index changes seldom, it is recommended to use {@code recacheDeletes=true},
    * as recaching will only occur when the index is reopened.
    * For near-real-time indexes or indexes that are often
-   * reopened with (e.g., {@link IndexReader#openIfChanged} is used), you should
+   * reopened with (e.g., {@link DirectoryReader#openIfChanged} is used), you should
    * pass {@code recacheDeletes=false}. This will cache the filter results omitting
    * deletions and will AND them in while scoring.
    * @param filter Filter to cache results of
@@ -76,7 +77,7 @@
    *  returns <code>true</code>, else it copies the {@link DocIdSetIterator} into
    *  a {@link FixedBitSet}.
    */
-  protected DocIdSet docIdSetToCache(DocIdSet docIdSet, IndexReader reader) throws IOException {
+  protected DocIdSet docIdSetToCache(DocIdSet docIdSet, AtomicReader reader) throws IOException {
     if (docIdSet == null) {
       // this is better than returning null, as the nonnull result can be cached
       return DocIdSet.EMPTY_DOCIDSET;
@@ -102,7 +103,7 @@
 
   @Override
   public DocIdSet getDocIdSet(AtomicReaderContext context, final Bits acceptDocs) throws IOException {
-    final IndexReader reader = context.reader;
+    final AtomicReader reader = context.reader();
 
     // Only cache if incoming acceptDocs is == live docs;
     // if Lucene passes in more interesting acceptDocs in
Index: lucene/src/java/org/apache/lucene/search/Collector.java
===================================================================
--- lucene/src/java/org/apache/lucene/search/Collector.java	(revision 1238034)
+++ lucene/src/java/org/apache/lucene/search/Collector.java	(working copy)
@@ -19,8 +19,8 @@
 
 import java.io.IOException;
 
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
-import org.apache.lucene.index.IndexReader.ReaderContext;
+import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.IndexReaderContext;
 
 /**
  * <p>Expert: Collectors are primarily meant to be used to
@@ -145,9 +145,9 @@
 
   /**
    * Called before collecting from each {@link AtomicReaderContext}. All doc ids in
-   * {@link #collect(int)} will correspond to {@link ReaderContext#reader}.
+   * {@link #collect(int)} will correspond to {@link IndexReaderContext#reader}.
    * 
-   * Add {@link AtomicReaderContext#docBase} to the current  {@link ReaderContext#reader}'s
+   * Add {@link AtomicReaderContext#docBase} to the current  {@link IndexReaderContext#reader}'s
    * internal document id to re-base ids in {@link #collect(int)}.
    * 
    * @param context
Index: lucene/src/java/org/apache/lucene/search/ConstantScoreQuery.java
===================================================================
--- lucene/src/java/org/apache/lucene/search/ConstantScoreQuery.java	(revision 1238034)
+++ lucene/src/java/org/apache/lucene/search/ConstantScoreQuery.java	(working copy)
@@ -17,8 +17,8 @@
  * limitations under the License.
  */
 
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.util.Bits;
 import org.apache.lucene.util.ToStringUtils;
@@ -149,7 +149,7 @@
 
     @Override
     public Explanation explain(AtomicReaderContext context, int doc) throws IOException {
-      final Scorer cs = scorer(context, true, false, context.reader.getLiveDocs());
+      final Scorer cs = scorer(context, true, false, context.reader().getLiveDocs());
       final boolean exists = (cs != null && cs.advance(doc) == doc);
 
       final ComplexExplanation result = new ComplexExplanation();
Index: lucene/src/java/org/apache/lucene/search/DisjunctionMaxQuery.java
===================================================================
--- lucene/src/java/org/apache/lucene/search/DisjunctionMaxQuery.java	(revision 1238034)
+++ lucene/src/java/org/apache/lucene/search/DisjunctionMaxQuery.java	(working copy)
@@ -22,8 +22,8 @@
 import java.util.Iterator;
 import java.util.Set;
 
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.util.Bits;
 
Index: lucene/src/java/org/apache/lucene/search/FieldCache.java
===================================================================
--- lucene/src/java/org/apache/lucene/search/FieldCache.java	(revision 1238034)
+++ lucene/src/java/org/apache/lucene/search/FieldCache.java	(working copy)
@@ -24,7 +24,7 @@
 import org.apache.lucene.analysis.NumericTokenStream; // for javadocs
 import org.apache.lucene.document.NumericField; // for javadocs
 import org.apache.lucene.index.DocTermOrds;
-import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.AtomicReader;
 import org.apache.lucene.index.TermsEnum;
 import org.apache.lucene.util.Bits;
 import org.apache.lucene.util.BytesRef;
@@ -63,7 +63,7 @@
   }
 
   /** Interface to parse bytes from document fields.
-   * @see FieldCache#getBytes(IndexReader, String, FieldCache.ByteParser, boolean)
+   * @see FieldCache#getBytes(AtomicReader, String, FieldCache.ByteParser, boolean)
    */
   public interface ByteParser extends Parser {
     /** Return a single Byte representation of this field's value. */
@@ -71,7 +71,7 @@
   }
 
   /** Interface to parse shorts from document fields.
-   * @see FieldCache#getShorts(IndexReader, String, FieldCache.ShortParser, boolean)
+   * @see FieldCache#getShorts(AtomicReader, String, FieldCache.ShortParser, boolean)
    */
   public interface ShortParser extends Parser {
     /** Return a short representation of this field's value. */
@@ -79,7 +79,7 @@
   }
 
   /** Interface to parse ints from document fields.
-   * @see FieldCache#getInts(IndexReader, String, FieldCache.IntParser, boolean)
+   * @see FieldCache#getInts(AtomicReader, String, FieldCache.IntParser, boolean)
    */
   public interface IntParser extends Parser {
     /** Return an integer representation of this field's value. */
@@ -87,7 +87,7 @@
   }
 
   /** Interface to parse floats from document fields.
-   * @see FieldCache#getFloats(IndexReader, String, FieldCache.FloatParser, boolean)
+   * @see FieldCache#getFloats(AtomicReader, String, FieldCache.FloatParser, boolean)
    */
   public interface FloatParser extends Parser {
     /** Return an float representation of this field's value. */
@@ -95,7 +95,7 @@
   }
 
   /** Interface to parse long from document fields.
-   * @see FieldCache#getLongs(IndexReader, String, FieldCache.LongParser, boolean)
+   * @see FieldCache#getLongs(AtomicReader, String, FieldCache.LongParser, boolean)
    */
   public interface LongParser extends Parser {
     /** Return an long representation of this field's value. */
@@ -103,7 +103,7 @@
   }
 
   /** Interface to parse doubles from document fields.
-   * @see FieldCache#getDoubles(IndexReader, String, FieldCache.DoubleParser, boolean)
+   * @see FieldCache#getDoubles(AtomicReader, String, FieldCache.DoubleParser, boolean)
    */
   public interface DoubleParser extends Parser {
     /** Return an long representation of this field's value. */
@@ -303,7 +303,7 @@
    * <code>reader.maxDoc()</code>, with turned on bits for each docid that 
    * does have a value for this field.
    */
-  public Bits getDocsWithField(IndexReader reader, String field) 
+  public Bits getDocsWithField(AtomicReader reader, String field) 
   throws IOException;
 
   /** Checks the internal cache for an appropriate entry, and if none is
@@ -317,7 +317,7 @@
    * @return The values in the given field for each document.
    * @throws IOException  If any error occurs.
    */
-  public byte[] getBytes (IndexReader reader, String field, boolean setDocsWithField)
+  public byte[] getBytes (AtomicReader reader, String field, boolean setDocsWithField)
   throws IOException;
 
   /** Checks the internal cache for an appropriate entry, and if none is found,
@@ -332,7 +332,7 @@
    * @return The values in the given field for each document.
    * @throws IOException  If any error occurs.
    */
-  public byte[] getBytes (IndexReader reader, String field, ByteParser parser, boolean setDocsWithField)
+  public byte[] getBytes (AtomicReader reader, String field, ByteParser parser, boolean setDocsWithField)
   throws IOException;
 
   /** Checks the internal cache for an appropriate entry, and if none is
@@ -346,7 +346,7 @@
    * @return The values in the given field for each document.
    * @throws IOException  If any error occurs.
    */
-  public short[] getShorts (IndexReader reader, String field, boolean setDocsWithField)
+  public short[] getShorts (AtomicReader reader, String field, boolean setDocsWithField)
   throws IOException;
 
   /** Checks the internal cache for an appropriate entry, and if none is found,
@@ -361,7 +361,7 @@
    * @return The values in the given field for each document.
    * @throws IOException  If any error occurs.
    */
-  public short[] getShorts (IndexReader reader, String field, ShortParser parser, boolean setDocsWithField)
+  public short[] getShorts (AtomicReader reader, String field, ShortParser parser, boolean setDocsWithField)
   throws IOException;
   
   /** Checks the internal cache for an appropriate entry, and if none is
@@ -375,7 +375,7 @@
    * @return The values in the given field for each document.
    * @throws IOException  If any error occurs.
    */
-  public int[] getInts (IndexReader reader, String field, boolean setDocsWithField)
+  public int[] getInts (AtomicReader reader, String field, boolean setDocsWithField)
   throws IOException;
 
   /** Checks the internal cache for an appropriate entry, and if none is found,
@@ -390,7 +390,7 @@
    * @return The values in the given field for each document.
    * @throws IOException  If any error occurs.
    */
-  public int[] getInts (IndexReader reader, String field, IntParser parser, boolean setDocsWithField)
+  public int[] getInts (AtomicReader reader, String field, IntParser parser, boolean setDocsWithField)
   throws IOException;
 
   /** Checks the internal cache for an appropriate entry, and if
@@ -404,7 +404,7 @@
    * @return The values in the given field for each document.
    * @throws IOException  If any error occurs.
    */
-  public float[] getFloats (IndexReader reader, String field, boolean setDocsWithField)
+  public float[] getFloats (AtomicReader reader, String field, boolean setDocsWithField)
   throws IOException;
 
   /** Checks the internal cache for an appropriate entry, and if
@@ -419,7 +419,7 @@
    * @return The values in the given field for each document.
    * @throws IOException  If any error occurs.
    */
-  public float[] getFloats (IndexReader reader, String field,
+  public float[] getFloats (AtomicReader reader, String field,
                             FloatParser parser, boolean setDocsWithField) throws IOException;
 
   /**
@@ -435,7 +435,7 @@
    * @return The values in the given field for each document.
    * @throws java.io.IOException If any error occurs.
    */
-  public long[] getLongs(IndexReader reader, String field, boolean setDocsWithField)
+  public long[] getLongs(AtomicReader reader, String field, boolean setDocsWithField)
           throws IOException;
 
   /**
@@ -452,7 +452,7 @@
    * @return The values in the given field for each document.
    * @throws IOException If any error occurs.
    */
-  public long[] getLongs(IndexReader reader, String field, LongParser parser, boolean setDocsWithField)
+  public long[] getLongs(AtomicReader reader, String field, LongParser parser, boolean setDocsWithField)
           throws IOException;
 
   /**
@@ -468,7 +468,7 @@
    * @return The values in the given field for each document.
    * @throws IOException If any error occurs.
    */
-  public double[] getDoubles(IndexReader reader, String field, boolean setDocsWithField)
+  public double[] getDoubles(AtomicReader reader, String field, boolean setDocsWithField)
           throws IOException;
 
   /**
@@ -485,7 +485,7 @@
    * @return The values in the given field for each document.
    * @throws IOException If any error occurs.
    */
-  public double[] getDoubles(IndexReader reader, String field, DoubleParser parser, boolean setDocsWithField)
+  public double[] getDoubles(AtomicReader reader, String field, DoubleParser parser, boolean setDocsWithField)
           throws IOException;
 
   /** Returned by {@link #getTerms} */
@@ -513,15 +513,15 @@
    * @return The values in the given field for each document.
    * @throws IOException  If any error occurs.
    */
-  public DocTerms getTerms (IndexReader reader, String field)
+  public DocTerms getTerms (AtomicReader reader, String field)
   throws IOException;
 
-  /** Expert: just like {@link #getTerms(IndexReader,String)},
+  /** Expert: just like {@link #getTerms(AtomicReader,String)},
    *  but you can specify whether more RAM should be consumed in exchange for
    *  faster lookups (default is "true").  Note that the
    *  first call for a given reader and field "wins",
    *  subsequent calls will share the same cache entry. */
-  public DocTerms getTerms (IndexReader reader, String field, boolean fasterButMoreRAM)
+  public DocTerms getTerms (AtomicReader reader, String field, boolean fasterButMoreRAM)
   throws IOException;
 
   /** Returned by {@link #getTermsIndex} */
@@ -589,16 +589,16 @@
    * @return The values in the given field for each document.
    * @throws IOException  If any error occurs.
    */
-  public DocTermsIndex getTermsIndex (IndexReader reader, String field)
+  public DocTermsIndex getTermsIndex (AtomicReader reader, String field)
   throws IOException;
 
   /** Expert: just like {@link
-   *  #getTermsIndex(IndexReader,String)}, but you can specify
+   *  #getTermsIndex(AtomicReader,String)}, but you can specify
    *  whether more RAM should be consumed in exchange for
    *  faster lookups (default is "true").  Note that the
    *  first call for a given reader and field "wins",
    *  subsequent calls will share the same cache entry. */
-  public DocTermsIndex getTermsIndex (IndexReader reader, String field, boolean fasterButMoreRAM)
+  public DocTermsIndex getTermsIndex (AtomicReader reader, String field, boolean fasterButMoreRAM)
   throws IOException;
 
   /**
@@ -611,7 +611,7 @@
    * @return a {@link DocTermOrds} instance
    * @throws IOException  If any error occurs.
    */
-  public DocTermOrds getDocTermOrds(IndexReader reader, String field) throws IOException;
+  public DocTermOrds getDocTermOrds(AtomicReader reader, String field) throws IOException;
 
   /**
    * EXPERT: A unique Identifier/Description for each item in the FieldCache. 
@@ -677,7 +677,7 @@
    * currently in the FieldCache.
    * <p>
    * NOTE: These CacheEntry objects maintain a strong reference to the 
-   * Cached Values.  Maintaining references to a CacheEntry the IndexReader 
+   * Cached Values.  Maintaining references to a CacheEntry the AtomicIndexReader 
    * associated with it has garbage collected will prevent the Value itself
    * from being garbage collected when the Cache drops the WeakReference.
    * </p>
@@ -705,7 +705,7 @@
    * top-level reader, it usually will have no effect as
    * Lucene now caches at the segment reader level.
    */
-  public abstract void purge(IndexReader r);
+  public abstract void purge(AtomicReader r);
 
   /**
    * If non-null, FieldCacheImpl will warn whenever
Index: lucene/src/java/org/apache/lucene/search/FieldCacheImpl.java
===================================================================
--- lucene/src/java/org/apache/lucene/search/FieldCacheImpl.java	(revision 1238034)
+++ lucene/src/java/org/apache/lucene/search/FieldCacheImpl.java	(working copy)
@@ -29,6 +29,7 @@
 import org.apache.lucene.index.DocTermOrds;
 import org.apache.lucene.index.DocsAndPositionsEnum;
 import org.apache.lucene.index.DocsEnum;
+import org.apache.lucene.index.AtomicReader;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.OrdTermState;
 import org.apache.lucene.index.SegmentReader;
@@ -48,8 +49,6 @@
  * Expert: The default cache implementation, storing all values in memory.
  * A WeakHashMap is used for storage.
  *
- * <p>Created: May 19, 2004 4:40:36 PM
- *
  * @since   lucene 1.4
  */
 class FieldCacheImpl implements FieldCache {
@@ -76,7 +75,7 @@
     init();
   }
 
-  public synchronized void purge(IndexReader r) {
+  public synchronized void purge(AtomicReader r) {
     for(Cache c : caches.values()) {
       c.purge(r);
     }
@@ -158,21 +157,20 @@
   final IndexReader.ReaderClosedListener purgeReader = new IndexReader.ReaderClosedListener() {
     @Override
     public void onClose(IndexReader owner) {
-      FieldCacheImpl.this.purge(owner);
+      assert owner instanceof AtomicReader;
+      FieldCacheImpl.this.purge((AtomicReader) owner);
     }
   };
   
-  private void initReader(IndexReader reader) {
+  private void initReader(AtomicReader reader) {
     if (reader instanceof SegmentReader) {
       ((SegmentReader) reader).addCoreClosedListener(purgeCore);
-    } else if (reader.getSequentialSubReaders() != null) {
-      throw new UnsupportedOperationException("Please use SlowMultiReaderWrapper, if you really need a top level FieldCache");
     } else {
       // we have a slow reader of some sort, try to register a purge event
       // rather than relying on gc:
       Object key = reader.getCoreCacheKey();
-      if (key instanceof IndexReader) {
-        ((IndexReader)key).addReaderClosedListener(purgeReader); 
+      if (key instanceof AtomicReader) {
+        ((AtomicReader)key).addReaderClosedListener(purgeReader); 
       } else {
         // last chance
         reader.addReaderClosedListener(purgeReader); 				
@@ -191,11 +189,11 @@
 
     final Map<Object,Map<Entry,Object>> readerCache = new WeakHashMap<Object,Map<Entry,Object>>();
     
-    protected abstract Object createValue(IndexReader reader, Entry key, boolean setDocsWithField)
+    protected abstract Object createValue(AtomicReader reader, Entry key, boolean setDocsWithField)
         throws IOException;
 
     /** Remove this reader from the cache, if present. */
-    public void purge(IndexReader r) {
+    public void purge(AtomicReader r) {
       Object readerKey = r.getCoreCacheKey();
       synchronized(readerCache) {
         readerCache.remove(readerKey);
@@ -204,7 +202,7 @@
 
     /** Sets the key to the value for the provided reader;
      *  if the key is already set then this doesn't change it. */
-    public void put(IndexReader reader, Entry key, Object value) {
+    public void put(AtomicReader reader, Entry key, Object value) {
       final Object readerKey = reader.getCoreCacheKey();
       synchronized (readerCache) {
         Map<Entry,Object> innerCache = readerCache.get(readerKey);
@@ -223,7 +221,7 @@
       }
     }
 
-    public Object get(IndexReader reader, Entry key, boolean setDocsWithField) throws IOException {
+    public Object get(AtomicReader reader, Entry key, boolean setDocsWithField) throws IOException {
       Map<Entry,Object> innerCache;
       Object value;
       final Object readerKey = reader.getCoreCacheKey();
@@ -321,12 +319,12 @@
   }
 
   // inherit javadocs
-  public byte[] getBytes (IndexReader reader, String field, boolean setDocsWithField) throws IOException {
+  public byte[] getBytes (AtomicReader reader, String field, boolean setDocsWithField) throws IOException {
     return getBytes(reader, field, null, setDocsWithField);
   }
 
   // inherit javadocs
-  public byte[] getBytes(IndexReader reader, String field, ByteParser parser, boolean setDocsWithField)
+  public byte[] getBytes(AtomicReader reader, String field, ByteParser parser, boolean setDocsWithField)
       throws IOException {
     return (byte[]) caches.get(Byte.TYPE).get(reader, new Entry(field, parser), setDocsWithField);
   }
@@ -336,7 +334,7 @@
       super(wrapper);
     }
     @Override
-    protected Object createValue(IndexReader reader, Entry entryKey, boolean setDocsWithField)
+    protected Object createValue(AtomicReader reader, Entry entryKey, boolean setDocsWithField)
         throws IOException {
       String field = entryKey.field;
       ByteParser parser = (ByteParser) entryKey.custom;
@@ -393,12 +391,12 @@
   }
   
   // inherit javadocs
-  public short[] getShorts (IndexReader reader, String field, boolean setDocsWithField) throws IOException {
+  public short[] getShorts (AtomicReader reader, String field, boolean setDocsWithField) throws IOException {
     return getShorts(reader, field, null, setDocsWithField);
   }
 
   // inherit javadocs
-  public short[] getShorts(IndexReader reader, String field, ShortParser parser, boolean setDocsWithField)
+  public short[] getShorts(AtomicReader reader, String field, ShortParser parser, boolean setDocsWithField)
       throws IOException {
     return (short[]) caches.get(Short.TYPE).get(reader, new Entry(field, parser), setDocsWithField);
   }
@@ -409,7 +407,7 @@
     }
 
     @Override
-    protected Object createValue(IndexReader reader, Entry entryKey, boolean setDocsWithField)
+    protected Object createValue(AtomicReader reader, Entry entryKey, boolean setDocsWithField)
         throws IOException {
       String field = entryKey.field;
       ShortParser parser = (ShortParser) entryKey.custom;
@@ -466,7 +464,7 @@
   }
 
   // null Bits means no docs matched
-  void setDocsWithField(IndexReader reader, String field, Bits docsWithField) {
+  void setDocsWithField(AtomicReader reader, String field, Bits docsWithField) {
     final int maxDoc = reader.maxDoc();
     final Bits bits;
     if (docsWithField == null) {
@@ -487,12 +485,12 @@
   }
   
   // inherit javadocs
-  public int[] getInts (IndexReader reader, String field, boolean setDocsWithField) throws IOException {
+  public int[] getInts (AtomicReader reader, String field, boolean setDocsWithField) throws IOException {
     return getInts(reader, field, null, setDocsWithField);
   }
 
   // inherit javadocs
-  public int[] getInts(IndexReader reader, String field, IntParser parser, boolean setDocsWithField)
+  public int[] getInts(AtomicReader reader, String field, IntParser parser, boolean setDocsWithField)
       throws IOException {
     return (int[]) caches.get(Integer.TYPE).get(reader, new Entry(field, parser), setDocsWithField);
   }
@@ -503,7 +501,7 @@
     }
 
     @Override
-    protected Object createValue(IndexReader reader, Entry entryKey, boolean setDocsWithField)
+    protected Object createValue(AtomicReader reader, Entry entryKey, boolean setDocsWithField)
         throws IOException {
       String field = entryKey.field;
       IntParser parser = (IntParser) entryKey.custom;
@@ -574,7 +572,7 @@
     }
   }
   
-  public Bits getDocsWithField(IndexReader reader, String field)
+  public Bits getDocsWithField(AtomicReader reader, String field)
       throws IOException {
     return (Bits) caches.get(DocsWithFieldCache.class).get(reader, new Entry(field, null), false);
   }
@@ -585,7 +583,7 @@
     }
     
     @Override
-      protected Object createValue(IndexReader reader, Entry entryKey, boolean setDocsWithField /* ignored */)
+      protected Object createValue(AtomicReader reader, Entry entryKey, boolean setDocsWithField /* ignored */)
     throws IOException {
       final String field = entryKey.field;      
       FixedBitSet res = null;
@@ -635,13 +633,13 @@
   }
 
   // inherit javadocs
-  public float[] getFloats (IndexReader reader, String field, boolean setDocsWithField)
+  public float[] getFloats (AtomicReader reader, String field, boolean setDocsWithField)
     throws IOException {
     return getFloats(reader, field, null, setDocsWithField);
   }
 
   // inherit javadocs
-  public float[] getFloats(IndexReader reader, String field, FloatParser parser, boolean setDocsWithField)
+  public float[] getFloats(AtomicReader reader, String field, FloatParser parser, boolean setDocsWithField)
     throws IOException {
 
     return (float[]) caches.get(Float.TYPE).get(reader, new Entry(field, parser), setDocsWithField);
@@ -653,7 +651,7 @@
     }
 
     @Override
-    protected Object createValue(IndexReader reader, Entry entryKey, boolean setDocsWithField)
+    protected Object createValue(AtomicReader reader, Entry entryKey, boolean setDocsWithField)
         throws IOException {
       String field = entryKey.field;
       FloatParser parser = (FloatParser) entryKey.custom;
@@ -725,12 +723,12 @@
   }
 
 
-  public long[] getLongs(IndexReader reader, String field, boolean setDocsWithField) throws IOException {
+  public long[] getLongs(AtomicReader reader, String field, boolean setDocsWithField) throws IOException {
     return getLongs(reader, field, null, setDocsWithField);
   }
   
   // inherit javadocs
-  public long[] getLongs(IndexReader reader, String field, FieldCache.LongParser parser, boolean setDocsWithField)
+  public long[] getLongs(AtomicReader reader, String field, FieldCache.LongParser parser, boolean setDocsWithField)
       throws IOException {
     return (long[]) caches.get(Long.TYPE).get(reader, new Entry(field, parser), setDocsWithField);
   }
@@ -741,7 +739,7 @@
     }
 
     @Override
-    protected Object createValue(IndexReader reader, Entry entryKey, boolean setDocsWithField)
+    protected Object createValue(AtomicReader reader, Entry entryKey, boolean setDocsWithField)
         throws IOException {
       String field = entryKey.field;
       FieldCache.LongParser parser = (FieldCache.LongParser) entryKey.custom;
@@ -813,13 +811,13 @@
   }
 
   // inherit javadocs
-  public double[] getDoubles(IndexReader reader, String field, boolean setDocsWithField)
+  public double[] getDoubles(AtomicReader reader, String field, boolean setDocsWithField)
     throws IOException {
     return getDoubles(reader, field, null, setDocsWithField);
   }
 
   // inherit javadocs
-  public double[] getDoubles(IndexReader reader, String field, FieldCache.DoubleParser parser, boolean setDocsWithField)
+  public double[] getDoubles(AtomicReader reader, String field, FieldCache.DoubleParser parser, boolean setDocsWithField)
       throws IOException {
     return (double[]) caches.get(Double.TYPE).get(reader, new Entry(field, parser), setDocsWithField);
   }
@@ -830,7 +828,7 @@
     }
 
     @Override
-    protected Object createValue(IndexReader reader, Entry entryKey, boolean setDocsWithField)
+    protected Object createValue(AtomicReader reader, Entry entryKey, boolean setDocsWithField)
         throws IOException {
       String field = entryKey.field;
       FieldCache.DoubleParser parser = (FieldCache.DoubleParser) entryKey.custom;
@@ -1075,11 +1073,11 @@
 
   private static boolean DEFAULT_FASTER_BUT_MORE_RAM = true;
 
-  public DocTermsIndex getTermsIndex(IndexReader reader, String field) throws IOException {
+  public DocTermsIndex getTermsIndex(AtomicReader reader, String field) throws IOException {
     return getTermsIndex(reader, field, DEFAULT_FASTER_BUT_MORE_RAM);
   }
 
-  public DocTermsIndex getTermsIndex(IndexReader reader, String field, boolean fasterButMoreRAM) throws IOException {
+  public DocTermsIndex getTermsIndex(AtomicReader reader, String field, boolean fasterButMoreRAM) throws IOException {
     return (DocTermsIndex) caches.get(DocTermsIndex.class).get(reader, new Entry(field, Boolean.valueOf(fasterButMoreRAM)), false);
   }
 
@@ -1089,7 +1087,7 @@
     }
 
     @Override
-    protected Object createValue(IndexReader reader, Entry entryKey, boolean setDocsWithField /* ignored */)
+    protected Object createValue(AtomicReader reader, Entry entryKey, boolean setDocsWithField /* ignored */)
         throws IOException {
 
       Terms terms = reader.terms(entryKey.field);
@@ -1220,11 +1218,11 @@
 
   // TODO: this if DocTermsIndex was already created, we
   // should share it...
-  public DocTerms getTerms(IndexReader reader, String field) throws IOException {
+  public DocTerms getTerms(AtomicReader reader, String field) throws IOException {
     return getTerms(reader, field, DEFAULT_FASTER_BUT_MORE_RAM);
   }
 
-  public DocTerms getTerms(IndexReader reader, String field, boolean fasterButMoreRAM) throws IOException {
+  public DocTerms getTerms(AtomicReader reader, String field, boolean fasterButMoreRAM) throws IOException {
     return (DocTerms) caches.get(DocTerms.class).get(reader, new Entry(field, Boolean.valueOf(fasterButMoreRAM)), false);
   }
 
@@ -1234,7 +1232,7 @@
     }
 
     @Override
-    protected Object createValue(IndexReader reader, Entry entryKey, boolean setDocsWithField /* ignored */)
+    protected Object createValue(AtomicReader reader, Entry entryKey, boolean setDocsWithField /* ignored */)
         throws IOException {
 
       Terms terms = reader.terms(entryKey.field);
@@ -1308,7 +1306,7 @@
     }
   }
 
-  public DocTermOrds getDocTermOrds(IndexReader reader, String field) throws IOException {
+  public DocTermOrds getDocTermOrds(AtomicReader reader, String field) throws IOException {
     return (DocTermOrds) caches.get(DocTermOrds.class).get(reader, new Entry(field, null), false);
   }
 
@@ -1318,7 +1316,7 @@
     }
 
     @Override
-    protected Object createValue(IndexReader reader, Entry entryKey, boolean setDocsWithField /* ignored */)
+    protected Object createValue(AtomicReader reader, Entry entryKey, boolean setDocsWithField /* ignored */)
         throws IOException {
       return new DocTermOrds(reader, entryKey.field);
     }
Index: lucene/src/java/org/apache/lucene/search/FieldCacheRangeFilter.java
===================================================================
--- lucene/src/java/org/apache/lucene/search/FieldCacheRangeFilter.java	(revision 1238034)
+++ lucene/src/java/org/apache/lucene/search/FieldCacheRangeFilter.java	(working copy)
@@ -18,8 +18,8 @@
 
 import java.io.IOException;
 
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
+import org.apache.lucene.index.AtomicReader; // for javadocs
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.util.NumericUtils;
 import org.apache.lucene.util.Bits;
 import org.apache.lucene.util.BytesRef;
@@ -84,7 +84,7 @@
     return new FieldCacheRangeFilter<String>(field, null, lowerVal, upperVal, includeLower, includeUpper) {
       @Override
       public DocIdSet getDocIdSet(AtomicReaderContext context, Bits acceptDocs) throws IOException {
-        final FieldCache.DocTermsIndex fcsi = FieldCache.DEFAULT.getTermsIndex(context.reader, field);
+        final FieldCache.DocTermsIndex fcsi = FieldCache.DEFAULT.getTermsIndex(context.reader(), field);
         final BytesRef spare = new BytesRef();
         final int lowerPoint = fcsi.binarySearchLookup(lowerVal == null ? null : new BytesRef(lowerVal), spare);
         final int upperPoint = fcsi.binarySearchLookup(upperVal == null ? null : new BytesRef(upperVal), spare);
@@ -122,7 +122,7 @@
         
         assert inclusiveLowerPoint > 0 && inclusiveUpperPoint > 0;
         
-        return new FieldCacheDocIdSet(context.reader.maxDoc(), acceptDocs) {
+        return new FieldCacheDocIdSet(context.reader().maxDoc(), acceptDocs) {
           @Override
           protected final boolean matchDoc(int doc) {
             final int docOrd = fcsi.getOrd(doc);
@@ -134,7 +134,7 @@
   }
   
   /**
-   * Creates a numeric range filter using {@link FieldCache#getBytes(IndexReader,String,boolean)}. This works with all
+   * Creates a numeric range filter using {@link FieldCache#getBytes(AtomicReader,String,boolean)}. This works with all
    * byte fields containing exactly one numeric term in the field. The range can be half-open by setting one
    * of the values to <code>null</code>.
    */
@@ -143,7 +143,7 @@
   }
   
   /**
-   * Creates a numeric range filter using {@link FieldCache#getBytes(IndexReader,String,FieldCache.ByteParser,boolean)}. This works with all
+   * Creates a numeric range filter using {@link FieldCache#getBytes(AtomicReader,String,FieldCache.ByteParser,boolean)}. This works with all
    * byte fields containing exactly one numeric term in the field. The range can be half-open by setting one
    * of the values to <code>null</code>.
    */
@@ -172,8 +172,8 @@
         if (inclusiveLowerPoint > inclusiveUpperPoint)
           return DocIdSet.EMPTY_DOCIDSET;
         
-        final byte[] values = FieldCache.DEFAULT.getBytes(context.reader, field, (FieldCache.ByteParser) parser, false);
-        return new FieldCacheDocIdSet(context.reader.maxDoc(), acceptDocs) {
+        final byte[] values = FieldCache.DEFAULT.getBytes(context.reader(), field, (FieldCache.ByteParser) parser, false);
+        return new FieldCacheDocIdSet(context.reader().maxDoc(), acceptDocs) {
           @Override
           protected boolean matchDoc(int doc) {
             return values[doc] >= inclusiveLowerPoint && values[doc] <= inclusiveUpperPoint;
@@ -184,7 +184,7 @@
   }
   
   /**
-   * Creates a numeric range filter using {@link FieldCache#getShorts(IndexReader,String,boolean)}. This works with all
+   * Creates a numeric range filter using {@link FieldCache#getShorts(AtomicReader,String,boolean)}. This works with all
    * short fields containing exactly one numeric term in the field. The range can be half-open by setting one
    * of the values to <code>null</code>.
    */
@@ -193,7 +193,7 @@
   }
   
   /**
-   * Creates a numeric range filter using {@link FieldCache#getShorts(IndexReader,String,FieldCache.ShortParser,boolean)}. This works with all
+   * Creates a numeric range filter using {@link FieldCache#getShorts(AtomicReader,String,FieldCache.ShortParser,boolean)}. This works with all
    * short fields containing exactly one numeric term in the field. The range can be half-open by setting one
    * of the values to <code>null</code>.
    */
@@ -222,8 +222,8 @@
         if (inclusiveLowerPoint > inclusiveUpperPoint)
           return DocIdSet.EMPTY_DOCIDSET;
         
-        final short[] values = FieldCache.DEFAULT.getShorts(context.reader, field, (FieldCache.ShortParser) parser, false);
-        return new FieldCacheDocIdSet(context.reader.maxDoc(), acceptDocs) {
+        final short[] values = FieldCache.DEFAULT.getShorts(context.reader(), field, (FieldCache.ShortParser) parser, false);
+        return new FieldCacheDocIdSet(context.reader().maxDoc(), acceptDocs) {
           @Override
           protected boolean matchDoc(int doc) {
             return values[doc] >= inclusiveLowerPoint && values[doc] <= inclusiveUpperPoint;
@@ -234,7 +234,7 @@
   }
   
   /**
-   * Creates a numeric range filter using {@link FieldCache#getInts(IndexReader,String,boolean)}. This works with all
+   * Creates a numeric range filter using {@link FieldCache#getInts(AtomicReader,String,boolean)}. This works with all
    * int fields containing exactly one numeric term in the field. The range can be half-open by setting one
    * of the values to <code>null</code>.
    */
@@ -243,7 +243,7 @@
   }
   
   /**
-   * Creates a numeric range filter using {@link FieldCache#getInts(IndexReader,String,FieldCache.IntParser,boolean)}. This works with all
+   * Creates a numeric range filter using {@link FieldCache#getInts(AtomicReader,String,FieldCache.IntParser,boolean)}. This works with all
    * int fields containing exactly one numeric term in the field. The range can be half-open by setting one
    * of the values to <code>null</code>.
    */
@@ -272,8 +272,8 @@
         if (inclusiveLowerPoint > inclusiveUpperPoint)
           return DocIdSet.EMPTY_DOCIDSET;
         
-        final int[] values = FieldCache.DEFAULT.getInts(context.reader, field, (FieldCache.IntParser) parser, false);
-        return new FieldCacheDocIdSet(context.reader.maxDoc(), acceptDocs) {
+        final int[] values = FieldCache.DEFAULT.getInts(context.reader(), field, (FieldCache.IntParser) parser, false);
+        return new FieldCacheDocIdSet(context.reader().maxDoc(), acceptDocs) {
           @Override
           protected boolean matchDoc(int doc) {
             return values[doc] >= inclusiveLowerPoint && values[doc] <= inclusiveUpperPoint;
@@ -284,7 +284,7 @@
   }
   
   /**
-   * Creates a numeric range filter using {@link FieldCache#getLongs(IndexReader,String,boolean)}. This works with all
+   * Creates a numeric range filter using {@link FieldCache#getLongs(AtomicReader,String,boolean)}. This works with all
    * long fields containing exactly one numeric term in the field. The range can be half-open by setting one
    * of the values to <code>null</code>.
    */
@@ -293,7 +293,7 @@
   }
   
   /**
-   * Creates a numeric range filter using {@link FieldCache#getLongs(IndexReader,String,FieldCache.LongParser,boolean)}. This works with all
+   * Creates a numeric range filter using {@link FieldCache#getLongs(AtomicReader,String,FieldCache.LongParser,boolean)}. This works with all
    * long fields containing exactly one numeric term in the field. The range can be half-open by setting one
    * of the values to <code>null</code>.
    */
@@ -322,8 +322,8 @@
         if (inclusiveLowerPoint > inclusiveUpperPoint)
           return DocIdSet.EMPTY_DOCIDSET;
         
-        final long[] values = FieldCache.DEFAULT.getLongs(context.reader, field, (FieldCache.LongParser) parser, false);
-        return new FieldCacheDocIdSet(context.reader.maxDoc(), acceptDocs) {
+        final long[] values = FieldCache.DEFAULT.getLongs(context.reader(), field, (FieldCache.LongParser) parser, false);
+        return new FieldCacheDocIdSet(context.reader().maxDoc(), acceptDocs) {
           @Override
           protected boolean matchDoc(int doc) {
             return values[doc] >= inclusiveLowerPoint && values[doc] <= inclusiveUpperPoint;
@@ -334,7 +334,7 @@
   }
   
   /**
-   * Creates a numeric range filter using {@link FieldCache#getFloats(IndexReader,String,boolean)}. This works with all
+   * Creates a numeric range filter using {@link FieldCache#getFloats(AtomicReader,String,boolean)}. This works with all
    * float fields containing exactly one numeric term in the field. The range can be half-open by setting one
    * of the values to <code>null</code>.
    */
@@ -343,7 +343,7 @@
   }
   
   /**
-   * Creates a numeric range filter using {@link FieldCache#getFloats(IndexReader,String,FieldCache.FloatParser,boolean)}. This works with all
+   * Creates a numeric range filter using {@link FieldCache#getFloats(AtomicReader,String,FieldCache.FloatParser,boolean)}. This works with all
    * float fields containing exactly one numeric term in the field. The range can be half-open by setting one
    * of the values to <code>null</code>.
    */
@@ -376,8 +376,8 @@
         if (inclusiveLowerPoint > inclusiveUpperPoint)
           return DocIdSet.EMPTY_DOCIDSET;
         
-        final float[] values = FieldCache.DEFAULT.getFloats(context.reader, field, (FieldCache.FloatParser) parser, false);
-        return new FieldCacheDocIdSet(context.reader.maxDoc(), acceptDocs) {
+        final float[] values = FieldCache.DEFAULT.getFloats(context.reader(), field, (FieldCache.FloatParser) parser, false);
+        return new FieldCacheDocIdSet(context.reader().maxDoc(), acceptDocs) {
           @Override
           protected boolean matchDoc(int doc) {
             return values[doc] >= inclusiveLowerPoint && values[doc] <= inclusiveUpperPoint;
@@ -388,7 +388,7 @@
   }
   
   /**
-   * Creates a numeric range filter using {@link FieldCache#getDoubles(IndexReader,String,boolean)}. This works with all
+   * Creates a numeric range filter using {@link FieldCache#getDoubles(AtomicReader,String,boolean)}. This works with all
    * double fields containing exactly one numeric term in the field. The range can be half-open by setting one
    * of the values to <code>null</code>.
    */
@@ -397,7 +397,7 @@
   }
   
   /**
-   * Creates a numeric range filter using {@link FieldCache#getDoubles(IndexReader,String,FieldCache.DoubleParser,boolean)}. This works with all
+   * Creates a numeric range filter using {@link FieldCache#getDoubles(AtomicReader,String,FieldCache.DoubleParser,boolean)}. This works with all
    * double fields containing exactly one numeric term in the field. The range can be half-open by setting one
    * of the values to <code>null</code>.
    */
@@ -430,9 +430,9 @@
         if (inclusiveLowerPoint > inclusiveUpperPoint)
           return DocIdSet.EMPTY_DOCIDSET;
         
-        final double[] values = FieldCache.DEFAULT.getDoubles(context.reader, field, (FieldCache.DoubleParser) parser, false);
+        final double[] values = FieldCache.DEFAULT.getDoubles(context.reader(), field, (FieldCache.DoubleParser) parser, false);
         // ignore deleted docs if range doesn't contain 0
-        return new FieldCacheDocIdSet(context.reader.maxDoc(), acceptDocs) {
+        return new FieldCacheDocIdSet(context.reader().maxDoc(), acceptDocs) {
           @Override
           protected boolean matchDoc(int doc) {
             return values[doc] >= inclusiveLowerPoint && values[doc] <= inclusiveUpperPoint;
Index: lucene/src/java/org/apache/lucene/search/FieldCacheTermsFilter.java
===================================================================
--- lucene/src/java/org/apache/lucene/search/FieldCacheTermsFilter.java	(revision 1238034)
+++ lucene/src/java/org/apache/lucene/search/FieldCacheTermsFilter.java	(working copy)
@@ -19,9 +19,9 @@
 
 import java.io.IOException;
 
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.DocsEnum; // javadoc @link
 import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
 import org.apache.lucene.util.FixedBitSet;
 import org.apache.lucene.util.Bits;
 import org.apache.lucene.util.BytesRef;
@@ -118,7 +118,7 @@
 
   @Override
   public DocIdSet getDocIdSet(AtomicReaderContext context, Bits acceptDocs) throws IOException {
-    final FieldCache.DocTermsIndex fcsi = getFieldCache().getTermsIndex(context.reader, field);
+    final FieldCache.DocTermsIndex fcsi = getFieldCache().getTermsIndex(context.reader(), field);
     final FixedBitSet bits = new FixedBitSet(fcsi.numOrd());
     final BytesRef spare = new BytesRef();
     for (int i=0;i<terms.length;i++) {
@@ -127,7 +127,7 @@
         bits.set(termNumber);
       }
     }
-    return new FieldCacheDocIdSet(context.reader.maxDoc(), acceptDocs) {
+    return new FieldCacheDocIdSet(context.reader().maxDoc(), acceptDocs) {
       @Override
       protected final boolean matchDoc(int doc) {
         return bits.get(fcsi.getOrd(doc));
Index: lucene/src/java/org/apache/lucene/search/FieldComparator.java
===================================================================
--- lucene/src/java/org/apache/lucene/search/FieldComparator.java	(revision 1238034)
+++ lucene/src/java/org/apache/lucene/search/FieldComparator.java	(working copy)
@@ -20,9 +20,9 @@
 import java.io.IOException;
 import java.util.Comparator;
 
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
+import org.apache.lucene.index.AtomicReader; // javadocs
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.DocValues;
-import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.search.FieldCache.ByteParser;
 import org.apache.lucene.search.FieldCache.DocTerms;
 import org.apache.lucene.search.FieldCache.DocTermsIndex;
@@ -72,7 +72,7 @@
  *       priority queue.  The {@link FieldValueHitQueue}
  *       calls this method when a new hit is competitive.
  *
- *  <li> {@link #setNextReader(IndexReader.AtomicReaderContext)} Invoked
+ *  <li> {@link #setNextReader(AtomicReaderContext)} Invoked
  *       when the search is switching to the next segment.
  *       You may need to update internal state of the
  *       comparator, for example retrieving new values from
@@ -203,7 +203,7 @@
     @Override
     public FieldComparator setNextReader(AtomicReaderContext context) throws IOException {
       if (missingValue != null) {
-        docsWithField = FieldCache.DEFAULT.getDocsWithField(context.reader, field);
+        docsWithField = FieldCache.DEFAULT.getDocsWithField(context.reader(), field);
         // optimization to remove unneeded checks on the bit interface:
         if (docsWithField instanceof Bits.MatchAllBits) {
           docsWithField = null;
@@ -261,7 +261,7 @@
     public FieldComparator setNextReader(AtomicReaderContext context) throws IOException {
       // NOTE: must do this before calling super otherwise
       // we compute the docsWithField Bits twice!
-      currentReaderValues = FieldCache.DEFAULT.getBytes(context.reader, field, parser, missingValue != null);
+      currentReaderValues = FieldCache.DEFAULT.getBytes(context.reader(), field, parser, missingValue != null);
       return super.setNextReader(context);
     }
     
@@ -338,7 +338,7 @@
     public FieldComparator setNextReader(AtomicReaderContext context) throws IOException {
       // NOTE: must do this before calling super otherwise
       // we compute the docsWithField Bits twice!
-      currentReaderValues = FieldCache.DEFAULT.getDoubles(context.reader, field, parser, missingValue != null);
+      currentReaderValues = FieldCache.DEFAULT.getDoubles(context.reader(), field, parser, missingValue != null);
       return super.setNextReader(context);
     }
     
@@ -397,7 +397,7 @@
 
     @Override
     public FieldComparator setNextReader(AtomicReaderContext context) throws IOException {
-      final DocValues docValues = context.reader.docValues(field);
+      final DocValues docValues = context.reader().docValues(field);
       if (docValues != null) {
         currentReaderValues = docValues.getSource(); 
       } else {
@@ -481,7 +481,7 @@
     public FieldComparator setNextReader(AtomicReaderContext context) throws IOException {
       // NOTE: must do this before calling super otherwise
       // we compute the docsWithField Bits twice!
-      currentReaderValues = FieldCache.DEFAULT.getFloats(context.reader, field, parser, missingValue != null);
+      currentReaderValues = FieldCache.DEFAULT.getFloats(context.reader(), field, parser, missingValue != null);
       return super.setNextReader(context);
     }
     
@@ -543,7 +543,7 @@
     public FieldComparator setNextReader(AtomicReaderContext context) throws IOException {
       // NOTE: must do this before calling super otherwise
       // we compute the docsWithField Bits twice!
-      currentReaderValues = FieldCache.DEFAULT.getShorts(context.reader, field, parser, missingValue != null);
+      currentReaderValues = FieldCache.DEFAULT.getShorts(context.reader(), field, parser, missingValue != null);
       return super.setNextReader(context);
     }
 
@@ -627,7 +627,7 @@
     public FieldComparator setNextReader(AtomicReaderContext context) throws IOException {
       // NOTE: must do this before calling super otherwise
       // we compute the docsWithField Bits twice!
-      currentReaderValues = FieldCache.DEFAULT.getInts(context.reader, field, parser, missingValue != null);
+      currentReaderValues = FieldCache.DEFAULT.getInts(context.reader(), field, parser, missingValue != null);
       return super.setNextReader(context);
     }
     
@@ -690,7 +690,7 @@
 
     @Override
     public FieldComparator setNextReader(AtomicReaderContext context) throws IOException {
-      DocValues docValues = context.reader.docValues(field);
+      DocValues docValues = context.reader().docValues(field);
       if (docValues != null) {
         currentReaderValues = docValues.getSource();
       } else {
@@ -775,7 +775,7 @@
     public FieldComparator setNextReader(AtomicReaderContext context) throws IOException {
       // NOTE: must do this before calling super otherwise
       // we compute the docsWithField Bits twice!
-      currentReaderValues = FieldCache.DEFAULT.getLongs(context.reader, field, parser, missingValue != null);
+      currentReaderValues = FieldCache.DEFAULT.getLongs(context.reader(), field, parser, missingValue != null);
       return super.setNextReader(context);
     }
     
@@ -1288,7 +1288,7 @@
     @Override
     public FieldComparator setNextReader(AtomicReaderContext context) throws IOException {
       final int docBase = context.docBase;
-      termsIndex = FieldCache.DEFAULT.getTermsIndex(context.reader, field);
+      termsIndex = FieldCache.DEFAULT.getTermsIndex(context.reader(), field);
       final PackedInts.Reader docToOrd = termsIndex.getDocToOrd();
       FieldComparator perSegComp = null;
       if (docToOrd.hasArray()) {
@@ -1706,19 +1706,19 @@
     public FieldComparator setNextReader(AtomicReaderContext context) throws IOException {
       final int docBase = context.docBase;
 
-      final DocValues dv = context.reader.docValues(field);
+      final DocValues dv = context.reader().docValues(field);
       if (dv == null) {
         // This may mean entire segment had no docs with
         // this DV field; use default field value (empty
         // byte[]) in this case:
-        termsIndex = DocValues.getDefaultSortedSource(DocValues.Type.BYTES_VAR_SORTED, context.reader.maxDoc());
+        termsIndex = DocValues.getDefaultSortedSource(DocValues.Type.BYTES_VAR_SORTED, context.reader().maxDoc());
       } else {
         termsIndex = dv.getSource().asSortedSource();
         if (termsIndex == null) {
           // This means segment has doc values, but they are
           // not able to provide a sorted source; consider
           // this a hard error:
-          throw new IllegalStateException("DocValues exist for field \"" + field + "\", but not as a sorted source: type=" + dv.getSource().type() + " reader=" + context.reader);
+          throw new IllegalStateException("DocValues exist for field \"" + field + "\", but not as a sorted source: type=" + dv.getSource().type() + " reader=" + context.reader());
         }
       }
 
@@ -1853,7 +1853,7 @@
 
     @Override
     public FieldComparator setNextReader(AtomicReaderContext context) throws IOException {
-      docTerms = FieldCache.DEFAULT.getTerms(context.reader, field);
+      docTerms = FieldCache.DEFAULT.getTerms(context.reader(), field);
       return this;
     }
     
@@ -1885,7 +1885,7 @@
    *  comparisons are done using BytesRef.compareTo, which is
    *  slow for medium to large result sets but possibly
    *  very fast for very small results sets.  The BytesRef
-   *  values are obtained using {@link IndexReader#docValues}. */
+   *  values are obtained using {@link AtomicReader#docValues}. */
   public static final class TermValDocValuesComparator extends FieldComparator<BytesRef> {
 
     private BytesRef[] values;
@@ -1922,7 +1922,7 @@
 
     @Override
     public FieldComparator setNextReader(AtomicReaderContext context) throws IOException {
-      final DocValues dv = context.reader.docValues(field);
+      final DocValues dv = context.reader().docValues(field);
       if (dv != null) {
         docTerms = dv.getSource();
       } else {
Index: lucene/src/java/org/apache/lucene/search/FieldValueFilter.java
===================================================================
--- lucene/src/java/org/apache/lucene/search/FieldValueFilter.java	(revision 1238034)
+++ lucene/src/java/org/apache/lucene/search/FieldValueFilter.java	(working copy)
@@ -18,7 +18,7 @@
  */
 import java.io.IOException;
 
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.util.Bits;
 import org.apache.lucene.util.Bits.MatchAllBits;
 import org.apache.lucene.util.Bits.MatchNoBits;
@@ -77,12 +77,12 @@
   public DocIdSet getDocIdSet(AtomicReaderContext context, Bits acceptDocs)
       throws IOException {
     final Bits docsWithField = FieldCache.DEFAULT.getDocsWithField(
-        context.reader, field);
+        context.reader(), field);
     if (negate) {
       if (docsWithField instanceof MatchAllBits) {
         return null;
       }
-      return new FieldCacheDocIdSet(context.reader.maxDoc(), acceptDocs) {
+      return new FieldCacheDocIdSet(context.reader().maxDoc(), acceptDocs) {
         @Override
         protected final boolean matchDoc(int doc) {
           return !docsWithField.get(doc);
@@ -97,7 +97,7 @@
         // :-)
         return BitsFilteredDocIdSet.wrap((DocIdSet) docsWithField, acceptDocs);
       }
-      return new FieldCacheDocIdSet(context.reader.maxDoc(), acceptDocs) {
+      return new FieldCacheDocIdSet(context.reader().maxDoc(), acceptDocs) {
         @Override
         protected final boolean matchDoc(int doc) {
           return docsWithField.get(doc);
Index: lucene/src/java/org/apache/lucene/search/Filter.java
===================================================================
--- lucene/src/java/org/apache/lucene/search/Filter.java	(revision 1238034)
+++ lucene/src/java/org/apache/lucene/search/Filter.java	(working copy)
@@ -19,8 +19,9 @@
 
 import java.io.IOException;
 
+import org.apache.lucene.index.AtomicReader; // javadocs
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.IndexReader; // javadocs
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
 import org.apache.lucene.util.Bits;
 
 /** 
@@ -44,7 +45,7 @@
    *         represent the whole underlying index i.e. if the index has more than
    *         one segment the given reader only represents a single segment.
    *         The provided context is always an atomic context, so you can call 
-   *         {@link IndexReader#fields()}
+   *         {@link AtomicReader#fields()}
    *         on the context's reader, for example.
    *
    * @param acceptDocs
Index: lucene/src/java/org/apache/lucene/search/FilteredQuery.java
===================================================================
--- lucene/src/java/org/apache/lucene/search/FilteredQuery.java	(revision 1238034)
+++ lucene/src/java/org/apache/lucene/search/FilteredQuery.java	(working copy)
@@ -17,8 +17,8 @@
  * limitations under the License.
  */
 
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.util.Bits;
 import org.apache.lucene.util.ToStringUtils;
@@ -100,7 +100,7 @@
       public Explanation explain (AtomicReaderContext ir, int i) throws IOException {
         Explanation inner = weight.explain (ir, i);
         Filter f = FilteredQuery.this.filter;
-        DocIdSet docIdSet = f.getDocIdSet(ir, ir.reader.getLiveDocs());
+        DocIdSet docIdSet = f.getDocIdSet(ir, ir.reader().getLiveDocs());
         DocIdSetIterator docIdSetIterator = docIdSet == null ? DocIdSet.EMPTY_DOCIDSET.iterator() : docIdSet.iterator();
         if (docIdSetIterator == null) {
           docIdSetIterator = DocIdSet.EMPTY_DOCIDSET.iterator();
Index: lucene/src/java/org/apache/lucene/search/IndexSearcher.java
===================================================================
--- lucene/src/java/org/apache/lucene/search/IndexSearcher.java	(revision 1238034)
+++ lucene/src/java/org/apache/lucene/search/IndexSearcher.java	(working copy)
@@ -31,11 +31,12 @@
 import java.util.concurrent.locks.ReentrantLock;
 
 import org.apache.lucene.document.Document;
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.CorruptIndexException;
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
-import org.apache.lucene.index.IndexReader.ReaderContext;
+import org.apache.lucene.index.DirectoryReader; // javadocs
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.MultiFields;
+import org.apache.lucene.index.IndexReaderContext;
 import org.apache.lucene.index.StoredFieldVisitor;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.index.Terms;
@@ -56,10 +57,11 @@
  * multiple searches instead of creating a new one
  * per-search.  If your index has changed and you wish to
  * see the changes reflected in searching, you should
- * use {@link IndexReader#openIfChanged} to obtain a new reader and
+ * use {@link DirectoryReader#openIfChanged(DirectoryReader)}
+ * to obtain a new reader and
  * then create a new IndexSearcher from that.  Also, for
  * low-latency turnaround it's best to use a near-real-time
- * reader ({@link IndexReader#open(IndexWriter,boolean)}).
+ * reader ({@link DirectoryReader#open(IndexWriter,boolean)}).
  * Once you have a new {@link IndexReader}, it's relatively
  * cheap to create a new IndexSearcher from it.
  * 
@@ -76,7 +78,7 @@
   
   // NOTE: these members might change in incompatible ways
   // in the next release
-  protected final ReaderContext readerContext;
+  protected final IndexReaderContext readerContext;
   protected final AtomicReaderContext[] leafContexts;
   // used with executor - each slice holds a set of leafs executed within one thread
   protected final LeafSlice[] leafSlices;
@@ -122,7 +124,7 @@
   }
 
   /**
-   * Creates a searcher searching the provided top-level {@link ReaderContext}.
+   * Creates a searcher searching the provided top-level {@link IndexReaderContext}.
    * <p>
    * Given a non-<code>null</code> {@link ExecutorService} this method runs
    * searches for each segment separately, using the provided ExecutorService.
@@ -133,13 +135,13 @@
    * silently close file descriptors (see <a
    * href="https://issues.apache.org/jira/browse/LUCENE-2239">LUCENE-2239</a>).
    * 
-   * @see ReaderContext
+   * @see IndexReaderContext
    * @see IndexReader#getTopReaderContext()
    * @lucene.experimental
    */
-  public IndexSearcher(ReaderContext context, ExecutorService executor) {
-    assert context.isTopLevel: "IndexSearcher's ReaderContext must be topLevel for reader" + context.reader;
-    reader = context.reader;
+  public IndexSearcher(IndexReaderContext context, ExecutorService executor) {
+    assert context.isTopLevel: "IndexSearcher's ReaderContext must be topLevel for reader" + context.reader();
+    reader = context.reader();
     this.executor = executor;
     this.readerContext = context;
     leafContexts = ReaderUtil.leaves(context);
@@ -147,13 +149,13 @@
   }
 
   /**
-   * Creates a searcher searching the provided top-level {@link ReaderContext}.
+   * Creates a searcher searching the provided top-level {@link IndexReaderContext}.
    *
-   * @see ReaderContext
+   * @see IndexReaderContext
    * @see IndexReader#getTopReaderContext()
    * @lucene.experimental
    */
-  public IndexSearcher(ReaderContext context) {
+  public IndexSearcher(IndexReaderContext context) {
     this(context, null);
   }
   
@@ -402,7 +404,7 @@
    * <p>NOTE: this does not compute scores by default.  If you
    * need scores, create a {@link TopFieldCollector}
    * instance by calling {@link TopFieldCollector#create} and
-   * then pass that to {@link #search(IndexReader.AtomicReaderContext[], Weight,
+   * then pass that to {@link #search(AtomicReaderContext[], Weight,
    * Collector)}.</p>
    */
   protected TopFieldDocs search(Weight weight, int nDocs,
@@ -451,7 +453,7 @@
    * <p>NOTE: this does not compute scores by default.  If you
    * need scores, create a {@link TopFieldCollector}
    * instance by calling {@link TopFieldCollector#create} and
-   * then pass that to {@link #search(IndexReader.AtomicReaderContext[], Weight, 
+   * then pass that to {@link #search(AtomicReaderContext[], Weight, 
    * Collector)}.</p>
    */
   protected TopFieldDocs search(AtomicReaderContext[] leaves, Weight weight, int nDocs,
@@ -501,7 +503,7 @@
     // always use single thread:
     for (int i = 0; i < leaves.length; i++) { // search each subreader
       collector.setNextReader(leaves[i]);
-      Scorer scorer = weight.scorer(leaves[i], !collector.acceptsDocsOutOfOrder(), true, leaves[i].reader.getLiveDocs());
+      Scorer scorer = weight.scorer(leaves[i], !collector.acceptsDocsOutOfOrder(), true, leaves[i].reader().getLiveDocs());
       if (scorer != null) {
         scorer.score(collector);
       }
@@ -589,11 +591,11 @@
   }
   
   /**
-   * Returns this searchers the top-level {@link ReaderContext}.
+   * Returns this searchers the top-level {@link IndexReaderContext}.
    * @see IndexReader#getTopReaderContext()
    */
   /* sugar for #getReader().getTopReaderContext() */
-  public ReaderContext getTopReaderContext() {
+  public IndexReaderContext getTopReaderContext() {
     return readerContext;
   }
 
Index: lucene/src/java/org/apache/lucene/search/MatchAllDocsQuery.java
===================================================================
--- lucene/src/java/org/apache/lucene/search/MatchAllDocsQuery.java	(revision 1238034)
+++ lucene/src/java/org/apache/lucene/search/MatchAllDocsQuery.java	(working copy)
@@ -17,8 +17,8 @@
  * limitations under the License.
  */
 
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.util.ToStringUtils;
 import org.apache.lucene.util.Bits;
@@ -106,7 +106,7 @@
     @Override
     public Scorer scorer(AtomicReaderContext context, boolean scoreDocsInOrder,
         boolean topScorer, Bits acceptDocs) throws IOException {
-      return new MatchAllScorer(context.reader, acceptDocs, this, queryWeight);
+      return new MatchAllScorer(context.reader(), acceptDocs, this, queryWeight);
     }
 
     @Override
Index: lucene/src/java/org/apache/lucene/search/MultiCollector.java
===================================================================
--- lucene/src/java/org/apache/lucene/search/MultiCollector.java	(revision 1238034)
+++ lucene/src/java/org/apache/lucene/search/MultiCollector.java	(working copy)
@@ -19,7 +19,7 @@
 
 import java.io.IOException;
 
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.search.Collector;
 import org.apache.lucene.search.Scorer;
 
Index: lucene/src/java/org/apache/lucene/search/MultiPhraseQuery.java
===================================================================
--- lucene/src/java/org/apache/lucene/search/MultiPhraseQuery.java	(revision 1238034)
+++ lucene/src/java/org/apache/lucene/search/MultiPhraseQuery.java	(working copy)
@@ -20,11 +20,12 @@
 import java.io.IOException;
 import java.util.*;
 
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.DocsAndPositionsEnum;
 import org.apache.lucene.index.DocsEnum;
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
-import org.apache.lucene.index.IndexReader.ReaderContext;
+import org.apache.lucene.index.AtomicReader;
 import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.IndexReaderContext;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.index.TermState;
 import org.apache.lucene.index.Terms;
@@ -142,7 +143,7 @@
     public MultiPhraseWeight(IndexSearcher searcher)
       throws IOException {
       this.similarity = searcher.getSimilarityProvider().get(field);
-      final ReaderContext context = searcher.getTopReaderContext();
+      final IndexReaderContext context = searcher.getTopReaderContext();
       
       // compute idf
       ArrayList<TermStatistics> allTermStats = new ArrayList<TermStatistics>();
@@ -177,7 +178,7 @@
     public Scorer scorer(AtomicReaderContext context, boolean scoreDocsInOrder,
         boolean topScorer, Bits acceptDocs) throws IOException {
       assert !termArrays.isEmpty();
-      final IndexReader reader = context.reader;
+      final AtomicReader reader = context.reader();
       final Bits liveDocs = acceptDocs;
       
       PhraseQuery.PostingsAndFreq[] postingsFreqs = new PhraseQuery.PostingsAndFreq[termArrays.size()];
@@ -258,7 +259,7 @@
 
     @Override
     public Explanation explain(AtomicReaderContext context, int doc) throws IOException {
-      Scorer scorer = scorer(context, true, false, context.reader.getLiveDocs());
+      Scorer scorer = scorer(context, true, false, context.reader().getLiveDocs());
       if (scorer != null) {
         int newDoc = scorer.advance(doc);
         if (newDoc == doc) {
Index: lucene/src/java/org/apache/lucene/search/MultiTermQueryWrapperFilter.java
===================================================================
--- lucene/src/java/org/apache/lucene/search/MultiTermQueryWrapperFilter.java	(revision 1238034)
+++ lucene/src/java/org/apache/lucene/search/MultiTermQueryWrapperFilter.java	(working copy)
@@ -19,10 +19,10 @@
 
 import java.io.IOException;
 
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.DocsEnum;
 import org.apache.lucene.index.Fields;
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
-import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.AtomicReader;
 import org.apache.lucene.index.Terms;
 import org.apache.lucene.index.TermsEnum;
 import org.apache.lucene.util.FixedBitSet;
@@ -83,7 +83,7 @@
    */
   @Override
   public DocIdSet getDocIdSet(AtomicReaderContext context, Bits acceptDocs) throws IOException {
-    final IndexReader reader = context.reader;
+    final AtomicReader reader = context.reader();
     final Fields fields = reader.fields();
     if (fields == null) {
       // reader has no fields
@@ -100,7 +100,7 @@
     assert termsEnum != null;
     if (termsEnum.next() != null) {
       // fill into a FixedBitSet
-      final FixedBitSet bitSet = new FixedBitSet(context.reader.maxDoc());
+      final FixedBitSet bitSet = new FixedBitSet(context.reader().maxDoc());
       DocsEnum docsEnum = null;
       do {
         // System.out.println("  iter termCount=" + termCount + " term=" +
Index: lucene/src/java/org/apache/lucene/search/payloads/PayloadNearQuery.java
===================================================================
--- lucene/src/java/org/apache/lucene/search/payloads/PayloadNearQuery.java	(revision 1238034)
+++ lucene/src/java/org/apache/lucene/search/payloads/PayloadNearQuery.java	(working copy)
@@ -17,7 +17,7 @@
  * limitations under the License.
  */
 
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.search.ComplexExplanation;
 import org.apache.lucene.search.Explanation;
 import org.apache.lucene.search.Scorer;
@@ -156,7 +156,7 @@
     
     @Override
     public Explanation explain(AtomicReaderContext context, int doc) throws IOException {
-      PayloadNearSpanScorer scorer = (PayloadNearSpanScorer) scorer(context, true, false, context.reader.getLiveDocs());
+      PayloadNearSpanScorer scorer = (PayloadNearSpanScorer) scorer(context, true, false, context.reader().getLiveDocs());
       if (scorer != null) {
         int newDoc = scorer.advance(doc);
         if (newDoc == doc) {
Index: lucene/src/java/org/apache/lucene/search/payloads/PayloadSpanUtil.java
===================================================================
--- lucene/src/java/org/apache/lucene/search/payloads/PayloadSpanUtil.java	(revision 1238034)
+++ lucene/src/java/org/apache/lucene/search/payloads/PayloadSpanUtil.java	(working copy)
@@ -26,9 +26,9 @@
 import java.util.Map;
 import java.util.TreeSet;
 
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
-import org.apache.lucene.index.IndexReader.ReaderContext;
+import org.apache.lucene.index.IndexReaderContext;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.search.BooleanClause;
 import org.apache.lucene.search.BooleanQuery;
@@ -55,7 +55,7 @@
  * 
  */
 public class PayloadSpanUtil {
-  private ReaderContext context;
+  private IndexReaderContext context;
 
   /**
    * @param context
@@ -63,7 +63,7 @@
    *          
    * @see IndexReader#getTopReaderContext()
    */
-  public PayloadSpanUtil(ReaderContext context) {
+  public PayloadSpanUtil(IndexReaderContext context) {
     this.context = context;
   }
 
@@ -186,7 +186,7 @@
     }
     final AtomicReaderContext[] leaves = ReaderUtil.leaves(context);
     for (AtomicReaderContext atomicReaderContext : leaves) {
-      final Spans spans = query.getSpans(atomicReaderContext, atomicReaderContext.reader.getLiveDocs(), termContexts);
+      final Spans spans = query.getSpans(atomicReaderContext, atomicReaderContext.reader().getLiveDocs(), termContexts);
       while (spans.next() == true) {
         if (spans.isPayloadAvailable()) {
           Collection<byte[]> payload = spans.getPayload();
Index: lucene/src/java/org/apache/lucene/search/payloads/PayloadTermQuery.java
===================================================================
--- lucene/src/java/org/apache/lucene/search/payloads/PayloadTermQuery.java	(revision 1238034)
+++ lucene/src/java/org/apache/lucene/search/payloads/PayloadTermQuery.java	(working copy)
@@ -17,7 +17,7 @@
  * limitations under the License.
  */
 
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.index.DocsAndPositionsEnum;
 import org.apache.lucene.search.IndexSearcher;
@@ -175,7 +175,7 @@
     
     @Override
     public Explanation explain(AtomicReaderContext context, int doc) throws IOException {
-      PayloadTermSpanScorer scorer = (PayloadTermSpanScorer) scorer(context, true, false, context.reader.getLiveDocs());
+      PayloadTermSpanScorer scorer = (PayloadTermSpanScorer) scorer(context, true, false, context.reader().getLiveDocs());
       if (scorer != null) {
         int newDoc = scorer.advance(doc);
         if (newDoc == doc) {
Index: lucene/src/java/org/apache/lucene/search/PhraseQuery.java
===================================================================
--- lucene/src/java/org/apache/lucene/search/PhraseQuery.java	(revision 1238034)
+++ lucene/src/java/org/apache/lucene/search/PhraseQuery.java	(working copy)
@@ -21,10 +21,11 @@
 import java.util.ArrayList;
 import java.util.Set;
 
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.DocsAndPositionsEnum;
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
-import org.apache.lucene.index.IndexReader.ReaderContext;
 import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.AtomicReader;
+import org.apache.lucene.index.IndexReaderContext;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.index.TermState;
 import org.apache.lucene.index.Terms;
@@ -188,7 +189,7 @@
     public PhraseWeight(IndexSearcher searcher)
       throws IOException {
       this.similarity = searcher.getSimilarityProvider().get(field);
-      final ReaderContext context = searcher.getTopReaderContext();
+      final IndexReaderContext context = searcher.getTopReaderContext();
       states = new TermContext[terms.size()];
       TermStatistics termStats[] = new TermStatistics[terms.size()];
       for (int i = 0; i < terms.size(); i++) {
@@ -219,7 +220,7 @@
     public Scorer scorer(AtomicReaderContext context, boolean scoreDocsInOrder,
         boolean topScorer, Bits acceptDocs) throws IOException {
       assert !terms.isEmpty();
-      final IndexReader reader = context.reader;
+      final AtomicReader reader = context.reader();
       final Bits liveDocs = acceptDocs;
       PostingsAndFreq[] postingsFreqs = new PostingsAndFreq[terms.size()];
 
@@ -270,13 +271,13 @@
     }
     
     // only called from assert
-    private boolean termNotInReader(IndexReader reader, String field, BytesRef bytes) throws IOException {
+    private boolean termNotInReader(AtomicReader reader, String field, BytesRef bytes) throws IOException {
       return reader.docFreq(field, bytes) == 0;
     }
 
     @Override
     public Explanation explain(AtomicReaderContext context, int doc) throws IOException {
-      Scorer scorer = scorer(context, true, false, context.reader.getLiveDocs());
+      Scorer scorer = scorer(context, true, false, context.reader().getLiveDocs());
       if (scorer != null) {
         int newDoc = scorer.advance(doc);
         if (newDoc == doc) {
Index: lucene/src/java/org/apache/lucene/search/PositiveScoresOnlyCollector.java
===================================================================
--- lucene/src/java/org/apache/lucene/search/PositiveScoresOnlyCollector.java	(revision 1238034)
+++ lucene/src/java/org/apache/lucene/search/PositiveScoresOnlyCollector.java	(working copy)
@@ -19,7 +19,7 @@
 
 import java.io.IOException;
 
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
+import org.apache.lucene.index.AtomicReaderContext;
 
 /**
  * A {@link Collector} implementation which wraps another
Index: lucene/src/java/org/apache/lucene/search/QueryWrapperFilter.java
===================================================================
--- lucene/src/java/org/apache/lucene/search/QueryWrapperFilter.java	(revision 1238034)
+++ lucene/src/java/org/apache/lucene/search/QueryWrapperFilter.java	(working copy)
@@ -19,7 +19,7 @@
 
 import java.io.IOException;
 
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.util.Bits;
 
 /** 
@@ -50,8 +50,7 @@
   @Override
   public DocIdSet getDocIdSet(final AtomicReaderContext context, final Bits acceptDocs) throws IOException {
     // get a private context that is used to rewrite, createWeight and score eventually
-    assert context.reader.getTopReaderContext().isAtomic;
-    final AtomicReaderContext privateContext = (AtomicReaderContext) context.reader.getTopReaderContext();
+    final AtomicReaderContext privateContext = context.reader().getTopReaderContext();
     final Weight weight = new IndexSearcher(privateContext).createNormalizedWeight(query);
     return new DocIdSet() {
       @Override
Index: lucene/src/java/org/apache/lucene/search/SearcherLifetimeManager.java
===================================================================
--- lucene/src/java/org/apache/lucene/search/SearcherLifetimeManager.java	(revision 1238034)
+++ lucene/src/java/org/apache/lucene/search/SearcherLifetimeManager.java	(working copy)
@@ -25,7 +25,7 @@
 import java.util.concurrent.ConcurrentHashMap;
 
 import org.apache.lucene.search.NRTManager;        // javadocs
-import org.apache.lucene.index.IndexReader;        // javadocs
+import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.store.AlreadyClosedException;
 import org.apache.lucene.util.IOUtils;
 
@@ -85,9 +85,9 @@
  * <p><b>NOTE</b>: keeping many searchers around means
  * you'll use more resources (open files, RAM) than a single
  * searcher.  However, as long as you are using {@link
- * IndexReader#openIfChanged}, the searchers will usually
- * share almost all segments and the added resource usage is
- * contained.  When a large merge has completed, and
+ * DirectoryReader#openIfChanged(DirectoryReader)}, the searchers
+ * will usually share almost all segments and the added resource usage
+ * is contained.  When a large merge has completed, and
  * you reopen, because that is a large change, the new
  * searcher will use higher additional RAM than other
  * searchers; but large merges don't complete very often and
@@ -109,7 +109,7 @@
 
     public SearcherTracker(IndexSearcher searcher) {
       this.searcher = searcher;
-      version = searcher.getIndexReader().getVersion();
+      version = ((DirectoryReader) searcher.getIndexReader()).getVersion();
       searcher.getIndexReader().incRef();
       // Use nanoTime not currentTimeMillis since it [in
       // theory] reduces risk from clock shift
@@ -168,7 +168,7 @@
     // TODO: we don't have to use IR.getVersion to track;
     // could be risky (if it's buggy); we could get better
     // bug isolation if we assign our own private ID:
-    final long version = searcher.getIndexReader().getVersion();
+    final long version = ((DirectoryReader) searcher.getIndexReader()).getVersion();
     SearcherTracker tracker = searchers.get(version);
     if (tracker == null) {
       //System.out.println("RECORD version=" + version + " ms=" + System.currentTimeMillis());
Index: lucene/src/java/org/apache/lucene/search/SearcherManager.java
===================================================================
--- lucene/src/java/org/apache/lucene/search/SearcherManager.java	(revision 1238034)
+++ lucene/src/java/org/apache/lucene/search/SearcherManager.java	(working copy)
@@ -23,6 +23,7 @@
 
 import org.apache.lucene.index.CorruptIndexException;
 import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexWriter;
 import org.apache.lucene.search.NRTManager; // javadocs
 import org.apache.lucene.search.IndexSearcher; // javadocs
@@ -76,12 +77,12 @@
    * Creates and returns a new SearcherManager from the given {@link IndexWriter}. 
    * @param writer the IndexWriter to open the IndexReader from.
    * @param applyAllDeletes If <code>true</code>, all buffered deletes will
-   *        be applied (made visible) in the {@link IndexSearcher} / {@link IndexReader}.
+   *        be applied (made visible) in the {@link IndexSearcher} / {@link DirectoryReader}.
    *        If <code>false</code>, the deletes may or may not be applied, but remain buffered 
    *        (in IndexWriter) so that they will be applied in the future.
    *        Applying deletes can be costly, so if your app can tolerate deleted documents
    *        being returned you might gain some performance by passing <code>false</code>.
-   *        See {@link IndexReader#openIfChanged(IndexReader, IndexWriter, boolean)}.
+   *        See {@link DirectoryReader#openIfChanged(DirectoryReader, IndexWriter, boolean)}.
    * @param searcherFactory An optional {@link SearcherFactory}. Pass
    *        <code>null</code> if you don't require the searcher to be warmed
    *        before going live or other custom behavior.
@@ -93,12 +94,12 @@
       searcherFactory = new SearcherFactory();
     }
     this.searcherFactory = searcherFactory;
-    currentSearcher = searcherFactory.newSearcher(IndexReader.open(writer, applyAllDeletes));
+    currentSearcher = searcherFactory.newSearcher(DirectoryReader.open(writer, applyAllDeletes));
   }
 
   /**
    * Creates and returns a new SearcherManager from the given {@link Directory}. 
-   * @param dir the directory to open the IndexReader on.
+   * @param dir the directory to open the DirectoryReader on.
    * @param searcherFactory An optional {@link SearcherFactory}. Pass
    *        <code>null</code> if you don't require the searcher to be warmed
    *        before going live or other custom behavior.
@@ -110,12 +111,12 @@
       searcherFactory = new SearcherFactory();
     }
     this.searcherFactory = searcherFactory;
-    currentSearcher = searcherFactory.newSearcher(IndexReader.open(dir));
+    currentSearcher = searcherFactory.newSearcher(DirectoryReader.open(dir));
   }
 
   /**
    * You must call this, periodically, to perform a reopen. This calls
-   * {@link IndexReader#openIfChanged(IndexReader)} with the underlying reader, and if that returns a
+   * {@link DirectoryReader#openIfChanged(DirectoryReader)} with the underlying reader, and if that returns a
    * new reader, it's warmed (if you provided a {@link SearcherFactory} and then
    * swapped into production.
    * 
@@ -144,7 +145,10 @@
         final IndexReader newReader;
         final IndexSearcher searcherToReopen = acquire();
         try {
-          newReader = IndexReader.openIfChanged(searcherToReopen.getIndexReader());
+          final IndexReader r = searcherToReopen.getIndexReader();
+          newReader = (r instanceof DirectoryReader) ?
+            DirectoryReader.openIfChanged((DirectoryReader) r) :
+            null;
         } finally {
           release(searcherToReopen);
         }
@@ -172,13 +176,16 @@
   /**
    * Returns <code>true</code> if no changes have occured since this searcher
    * ie. reader was opened, otherwise <code>false</code>.
-   * @see IndexReader#isCurrent() 
+   * @see DirectoryReader#isCurrent() 
    */
   public boolean isSearcherCurrent() throws CorruptIndexException,
       IOException {
     final IndexSearcher searcher = acquire();
     try {
-      return searcher.getIndexReader().isCurrent();
+      final IndexReader r = searcher.getIndexReader();
+      return r instanceof DirectoryReader ?
+        ((DirectoryReader ) r).isCurrent() :
+        true;
     } finally {
       release(searcher);
     }
Index: lucene/src/java/org/apache/lucene/search/similarities/BM25Similarity.java
===================================================================
--- lucene/src/java/org/apache/lucene/search/similarities/BM25Similarity.java	(revision 1238034)
+++ lucene/src/java/org/apache/lucene/search/similarities/BM25Similarity.java	(working copy)
@@ -19,9 +19,9 @@
 
 import java.io.IOException;
 
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.DocValues;
 import org.apache.lucene.index.FieldInvertState;
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
 import org.apache.lucene.index.Norm;
 import org.apache.lucene.search.CollectionStatistics;
 import org.apache.lucene.search.Explanation;
@@ -168,7 +168,7 @@
 
   @Override
   public final ExactDocScorer exactDocScorer(Stats stats, String fieldName, AtomicReaderContext context) throws IOException {
-    final DocValues norms = context.reader.normValues(fieldName);
+    final DocValues norms = context.reader().normValues(fieldName);
     return norms == null 
       ? new ExactBM25DocScorerNoNorms((BM25Stats)stats)
       : new ExactBM25DocScorer((BM25Stats)stats, norms);
@@ -176,7 +176,7 @@
 
   @Override
   public final SloppyDocScorer sloppyDocScorer(Stats stats, String fieldName, AtomicReaderContext context) throws IOException {
-    return new SloppyBM25DocScorer((BM25Stats) stats, context.reader.normValues(fieldName));
+    return new SloppyBM25DocScorer((BM25Stats) stats, context.reader().normValues(fieldName));
   }
   
   private class ExactBM25DocScorer extends ExactDocScorer {
Index: lucene/src/java/org/apache/lucene/search/similarities/MultiSimilarity.java
===================================================================
--- lucene/src/java/org/apache/lucene/search/similarities/MultiSimilarity.java	(revision 1238034)
+++ lucene/src/java/org/apache/lucene/search/similarities/MultiSimilarity.java	(working copy)
@@ -19,8 +19,8 @@
 
 import java.io.IOException;
 
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.FieldInvertState;
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
 import org.apache.lucene.index.Norm;
 import org.apache.lucene.search.CollectionStatistics;
 import org.apache.lucene.search.Explanation;
Index: lucene/src/java/org/apache/lucene/search/similarities/Similarity.java
===================================================================
--- lucene/src/java/org/apache/lucene/search/similarities/Similarity.java	(revision 1238034)
+++ lucene/src/java/org/apache/lucene/search/similarities/Similarity.java	(working copy)
@@ -21,9 +21,10 @@
 import java.io.IOException;
 
 import org.apache.lucene.document.DocValuesField; // javadoc
+import org.apache.lucene.index.AtomicReader; // javadoc
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.FieldInvertState;
 import org.apache.lucene.index.IndexReader; // javadoc
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
 import org.apache.lucene.index.Norm;
 import org.apache.lucene.index.Terms; // javadoc
 import org.apache.lucene.search.BooleanQuery;
@@ -57,7 +58,7 @@
  * <a name="indextime"/>
  * At indexing time, the indexer calls {@link #computeNorm(FieldInvertState, Norm)}, allowing
  * the Similarity implementation to set a per-document value for the field that will 
- * be later accessible via {@link IndexReader#normValues(String)}.  Lucene makes no assumption
+ * be later accessible via {@link AtomicReader#normValues(String)}.  Lucene makes no assumption
  * about what is in this byte, but it is most useful for encoding length normalization 
  * information.
  * <p>
@@ -72,7 +73,7 @@
  * Because index-time boost is handled entirely at the application level anyway,
  * an application can alternatively store the index-time boost separately using an 
  * {@link DocValuesField}, and access this at query-time with 
- * {@link IndexReader#docValues(String)}.
+ * {@link AtomicReader#docValues(String)}.
  * <p>
  * Finally, using index-time boosts (either via folding into the normalization byte or
  * via DocValues), is an inefficient way to boost the scores of different fields if the
@@ -93,9 +94,9 @@
  *       is called for each query leaf node, {@link SimilarityProvider#queryNorm(float)} is called for the top-level
  *       query, and finally {@link Similarity.Stats#normalize(float, float)} passes down the normalization value
  *       and any top-level boosts (e.g. from enclosing {@link BooleanQuery}s).
- *   <li>For each segment in the index, the Query creates a {@link #exactDocScorer(Stats, String, IndexReader.AtomicReaderContext)}
+ *   <li>For each segment in the index, the Query creates a {@link #exactDocScorer(Stats, String, AtomicReaderContext)}
  *       (for queries with exact frequencies such as TermQuerys and exact PhraseQueries) or a 
- *       {@link #sloppyDocScorer(Stats, String, IndexReader.AtomicReaderContext)} (for queries with sloppy frequencies such as
+ *       {@link #sloppyDocScorer(Stats, String, AtomicReaderContext)} (for queries with sloppy frequencies such as
  *       SpanQuerys and sloppy PhraseQueries). The score() method is called for each matching document.
  * </ol>
  * <p>
Index: lucene/src/java/org/apache/lucene/search/similarities/SimilarityBase.java
===================================================================
--- lucene/src/java/org/apache/lucene/search/similarities/SimilarityBase.java	(revision 1238034)
+++ lucene/src/java/org/apache/lucene/search/similarities/SimilarityBase.java	(working copy)
@@ -19,9 +19,9 @@
 
 import java.io.IOException;
 
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.DocValues;
 import org.apache.lucene.index.FieldInvertState;
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
 import org.apache.lucene.index.Norm;
 import org.apache.lucene.search.CollectionStatistics;
 import org.apache.lucene.search.Explanation;
@@ -181,7 +181,7 @@
   @Override
   public ExactDocScorer exactDocScorer(Stats stats, String fieldName,
       AtomicReaderContext context) throws IOException {
-    DocValues norms = context.reader.normValues(fieldName);
+    DocValues norms = context.reader().normValues(fieldName);
     
     if (stats instanceof MultiSimilarity.MultiStats) {
       // a multi term query (e.g. phrase). return the summation, 
@@ -200,7 +200,7 @@
   @Override
   public SloppyDocScorer sloppyDocScorer(Stats stats, String fieldName,
       AtomicReaderContext context) throws IOException {
-    DocValues norms = context.reader.normValues(fieldName);
+    DocValues norms = context.reader().normValues(fieldName);
     
     if (stats instanceof MultiSimilarity.MultiStats) {
       // a multi term query (e.g. phrase). return the summation, 
Index: lucene/src/java/org/apache/lucene/search/similarities/TFIDFSimilarity.java
===================================================================
--- lucene/src/java/org/apache/lucene/search/similarities/TFIDFSimilarity.java	(revision 1238034)
+++ lucene/src/java/org/apache/lucene/search/similarities/TFIDFSimilarity.java	(working copy)
@@ -20,8 +20,8 @@
 
 import java.io.IOException;
 
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.DocValues;
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
 import org.apache.lucene.search.CollectionStatistics;
 import org.apache.lucene.search.Explanation;
 import org.apache.lucene.search.IndexSearcher;
@@ -703,12 +703,12 @@
 
   @Override
   public final ExactDocScorer exactDocScorer(Stats stats, String fieldName, AtomicReaderContext context) throws IOException {
-    return new ExactTFIDFDocScorer((IDFStats)stats, context.reader.normValues(fieldName));
+    return new ExactTFIDFDocScorer((IDFStats)stats, context.reader().normValues(fieldName));
   }
 
   @Override
   public final SloppyDocScorer sloppyDocScorer(Stats stats, String fieldName, AtomicReaderContext context) throws IOException {
-    return new SloppyTFIDFDocScorer((IDFStats)stats, context.reader.normValues(fieldName));
+    return new SloppyTFIDFDocScorer((IDFStats)stats, context.reader().normValues(fieldName));
   }
   
   // TODO: we can specialize these for omitNorms up front, but we should test that it doesn't confuse stupid hotspot.
Index: lucene/src/java/org/apache/lucene/search/spans/FieldMaskingSpanQuery.java
===================================================================
--- lucene/src/java/org/apache/lucene/search/spans/FieldMaskingSpanQuery.java	(revision 1238034)
+++ lucene/src/java/org/apache/lucene/search/spans/FieldMaskingSpanQuery.java	(working copy)
@@ -21,8 +21,8 @@
 import java.util.Map;
 import java.util.Set;
 
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.search.Query;
 import org.apache.lucene.search.Weight;
Index: lucene/src/java/org/apache/lucene/search/spans/NearSpansOrdered.java
===================================================================
--- lucene/src/java/org/apache/lucene/search/spans/NearSpansOrdered.java	(revision 1238034)
+++ lucene/src/java/org/apache/lucene/search/spans/NearSpansOrdered.java	(working copy)
@@ -17,8 +17,8 @@
  * limitations under the License.
  */
 
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.Term;
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
 import org.apache.lucene.util.ArrayUtil;
 import org.apache.lucene.util.Bits;
 import org.apache.lucene.util.TermContext;
Index: lucene/src/java/org/apache/lucene/search/spans/NearSpansUnordered.java
===================================================================
--- lucene/src/java/org/apache/lucene/search/spans/NearSpansUnordered.java	(revision 1238034)
+++ lucene/src/java/org/apache/lucene/search/spans/NearSpansUnordered.java	(working copy)
@@ -17,8 +17,8 @@
  * limitations under the License.
  */
 
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.Term;
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
 import org.apache.lucene.util.Bits;
 import org.apache.lucene.util.PriorityQueue;
 import org.apache.lucene.util.TermContext;
Index: lucene/src/java/org/apache/lucene/search/spans/SpanMultiTermQueryWrapper.java
===================================================================
--- lucene/src/java/org/apache/lucene/search/spans/SpanMultiTermQueryWrapper.java	(revision 1238034)
+++ lucene/src/java/org/apache/lucene/search/spans/SpanMultiTermQueryWrapper.java	(working copy)
@@ -20,8 +20,8 @@
 import java.io.IOException;
 import java.util.Map;
 
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.search.MultiTermQuery;
 import org.apache.lucene.search.Query;
Index: lucene/src/java/org/apache/lucene/search/spans/SpanNearQuery.java
===================================================================
--- lucene/src/java/org/apache/lucene/search/spans/SpanNearQuery.java	(revision 1238034)
+++ lucene/src/java/org/apache/lucene/search/spans/SpanNearQuery.java	(working copy)
@@ -27,8 +27,8 @@
 import java.util.Set;
 
 
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.search.Query;
 import org.apache.lucene.util.Bits;
Index: lucene/src/java/org/apache/lucene/search/spans/SpanNotQuery.java
===================================================================
--- lucene/src/java/org/apache/lucene/search/spans/SpanNotQuery.java	(revision 1238034)
+++ lucene/src/java/org/apache/lucene/search/spans/SpanNotQuery.java	(working copy)
@@ -17,8 +17,8 @@
  * limitations under the License.
  */
 
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.search.Query;
 import org.apache.lucene.util.Bits;
Index: lucene/src/java/org/apache/lucene/search/spans/SpanOrQuery.java
===================================================================
--- lucene/src/java/org/apache/lucene/search/spans/SpanOrQuery.java	(revision 1238034)
+++ lucene/src/java/org/apache/lucene/search/spans/SpanOrQuery.java	(working copy)
@@ -26,8 +26,8 @@
 import java.util.Map;
 import java.util.Set;
 
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.util.Bits;
 import org.apache.lucene.util.PriorityQueue;
Index: lucene/src/java/org/apache/lucene/search/spans/SpanPositionCheckQuery.java
===================================================================
--- lucene/src/java/org/apache/lucene/search/spans/SpanPositionCheckQuery.java	(revision 1238034)
+++ lucene/src/java/org/apache/lucene/search/spans/SpanPositionCheckQuery.java	(working copy)
@@ -17,8 +17,8 @@
  */
 
 
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.search.Query;
 import org.apache.lucene.util.Bits;
Index: lucene/src/java/org/apache/lucene/search/spans/SpanQuery.java
===================================================================
--- lucene/src/java/org/apache/lucene/search/spans/SpanQuery.java	(revision 1238034)
+++ lucene/src/java/org/apache/lucene/search/spans/SpanQuery.java	(working copy)
@@ -20,7 +20,7 @@
 import java.io.IOException;
 import java.util.Map;
 
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.search.Query;
 import org.apache.lucene.search.IndexSearcher;
Index: lucene/src/java/org/apache/lucene/search/spans/SpanTermQuery.java
===================================================================
--- lucene/src/java/org/apache/lucene/search/spans/SpanTermQuery.java	(revision 1238034)
+++ lucene/src/java/org/apache/lucene/search/spans/SpanTermQuery.java	(working copy)
@@ -17,7 +17,7 @@
  * limitations under the License.
  */
 
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.Fields;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.index.DocsAndPositionsEnum;
@@ -93,7 +93,7 @@
     if (termContext == null) {
       // this happens with span-not query, as it doesn't include the NOT side in extractTerms()
       // so we seek to the term now in this segment..., this sucks because its ugly mostly!
-      final Fields fields = context.reader.fields();
+      final Fields fields = context.reader().fields();
       if (fields != null) {
         final Terms terms = fields.terms(term.field());
         if (terms != null) {
@@ -117,7 +117,7 @@
       return TermSpans.EMPTY_TERM_SPANS;
     }
     
-    final TermsEnum termsEnum = context.reader.terms(term.field()).iterator(null);
+    final TermsEnum termsEnum = context.reader().terms(term.field()).iterator(null);
     termsEnum.seekExact(term.bytes(), state);
     
     final DocsAndPositionsEnum postings = termsEnum.docsAndPositions(acceptDocs, null, false);
Index: lucene/src/java/org/apache/lucene/search/spans/SpanWeight.java
===================================================================
--- lucene/src/java/org/apache/lucene/search/spans/SpanWeight.java	(revision 1238034)
+++ lucene/src/java/org/apache/lucene/search/spans/SpanWeight.java	(working copy)
@@ -17,8 +17,8 @@
  * limitations under the License.
  */
 
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
-import org.apache.lucene.index.IndexReader.ReaderContext;
+import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.IndexReaderContext;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.search.*;
 import org.apache.lucene.search.similarities.Similarity;
@@ -48,7 +48,7 @@
     termContexts = new HashMap<Term,TermContext>();
     TreeSet<Term> terms = new TreeSet<Term>();
     query.extractTerms(terms);
-    final ReaderContext context = searcher.getTopReaderContext();
+    final IndexReaderContext context = searcher.getTopReaderContext();
     final TermStatistics termStats[] = new TermStatistics[terms.size()];
     int i = 0;
     for (Term term : terms) {
@@ -84,7 +84,7 @@
 
   @Override
   public Explanation explain(AtomicReaderContext context, int doc) throws IOException {
-    Scorer scorer = scorer(context, true, false, context.reader.getLiveDocs());
+    Scorer scorer = scorer(context, true, false, context.reader().getLiveDocs());
     if (scorer != null) {
       int newDoc = scorer.advance(doc);
       if (newDoc == doc) {
Index: lucene/src/java/org/apache/lucene/search/TermCollectingRewrite.java
===================================================================
--- lucene/src/java/org/apache/lucene/search/TermCollectingRewrite.java	(revision 1238034)
+++ lucene/src/java/org/apache/lucene/search/TermCollectingRewrite.java	(working copy)
@@ -20,13 +20,13 @@
 import java.io.IOException;
 import java.util.Comparator;
 
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.Fields;
 import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.IndexReaderContext;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.index.Terms;
 import org.apache.lucene.index.TermsEnum;
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
-import org.apache.lucene.index.IndexReader.ReaderContext;
 import org.apache.lucene.util.AttributeSource;
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.TermContext;
@@ -47,11 +47,11 @@
 
   
   protected final void collectTerms(IndexReader reader, MultiTermQuery query, TermCollector collector) throws IOException {
-    ReaderContext topReaderContext = reader.getTopReaderContext();
+    IndexReaderContext topReaderContext = reader.getTopReaderContext();
     Comparator<BytesRef> lastTermComp = null;
     final AtomicReaderContext[] leaves = ReaderUtil.leaves(topReaderContext);
     for (AtomicReaderContext context : leaves) {
-      final Fields fields = context.reader.fields();
+      final Fields fields = context.reader().fields();
       if (fields == null) {
         // reader has no fields
         continue;
@@ -87,9 +87,9 @@
   protected static abstract class TermCollector {
     
     protected AtomicReaderContext readerContext;
-    protected ReaderContext topReaderContext;
+    protected IndexReaderContext topReaderContext;
 
-    public void setReaderContext(ReaderContext topReaderContext, AtomicReaderContext readerContext) {
+    public void setReaderContext(IndexReaderContext topReaderContext, AtomicReaderContext readerContext) {
       this.readerContext = readerContext;
       this.topReaderContext = topReaderContext;
     }
Index: lucene/src/java/org/apache/lucene/search/TermQuery.java
===================================================================
--- lucene/src/java/org/apache/lucene/search/TermQuery.java	(revision 1238034)
+++ lucene/src/java/org/apache/lucene/search/TermQuery.java	(working copy)
@@ -20,10 +20,10 @@
 import java.io.IOException;
 import java.util.Set;
 
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.DocsEnum;
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
-import org.apache.lucene.index.IndexReader.ReaderContext;
-import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.AtomicReader;
+import org.apache.lucene.index.IndexReaderContext;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.index.TermState;
 import org.apache.lucene.index.TermsEnum;
@@ -108,16 +108,16 @@
     TermsEnum getTermsEnum(AtomicReaderContext context) throws IOException {
       final TermState state = termStates.get(context.ord);
       if (state == null) { // term is not present in that reader
-        assert termNotInReader(context.reader, term.field(), term.bytes()) : "no termstate found but term exists in reader term=" + term;
+        assert termNotInReader(context.reader(), term.field(), term.bytes()) : "no termstate found but term exists in reader term=" + term;
         return null;
       }
       //System.out.println("LD=" + reader.getLiveDocs() + " set?=" + (reader.getLiveDocs() != null ? reader.getLiveDocs().get(0) : "null"));
-      final TermsEnum termsEnum = context.reader.terms(term.field()).iterator(null);
+      final TermsEnum termsEnum = context.reader().terms(term.field()).iterator(null);
       termsEnum.seekExact(term.bytes(), state);
       return termsEnum;
     }
     
-    private boolean termNotInReader(IndexReader reader, String field, BytesRef bytes) throws IOException {
+    private boolean termNotInReader(AtomicReader reader, String field, BytesRef bytes) throws IOException {
       // only called from assert
       //System.out.println("TQ.termNotInReader reader=" + reader + " term=" + field + ":" + bytes.utf8ToString());
       return reader.docFreq(field, bytes) == 0;
@@ -125,7 +125,7 @@
     
     @Override
     public Explanation explain(AtomicReaderContext context, int doc) throws IOException {
-      Scorer scorer = scorer(context, true, false, context.reader.getLiveDocs());
+      Scorer scorer = scorer(context, true, false, context.reader().getLiveDocs());
       if (scorer != null) {
         int newDoc = scorer.advance(doc);
         if (newDoc == doc) {
@@ -173,7 +173,7 @@
 
   @Override
   public Weight createWeight(IndexSearcher searcher) throws IOException {
-    final ReaderContext context = searcher.getTopReaderContext();
+    final IndexReaderContext context = searcher.getTopReaderContext();
     final TermContext termState;
     if (perReaderTermState == null || perReaderTermState.topReaderContext != context) {
       // make TermQuery single-pass if we don't have a PRTS or if the context differs!
Index: lucene/src/java/org/apache/lucene/search/TermStatistics.java
===================================================================
--- lucene/src/java/org/apache/lucene/search/TermStatistics.java	(revision 1238034)
+++ lucene/src/java/org/apache/lucene/search/TermStatistics.java	(working copy)
@@ -17,7 +17,7 @@
  * limitations under the License.
  */
 
-import org.apache.lucene.index.IndexReader; // javadocs
+import org.apache.lucene.index.AtomicReader; // javadocs
 import org.apache.lucene.util.BytesRef;
 /**
  * Contains statistics for a specific term
@@ -42,13 +42,13 @@
   }
   
   /** returns the number of documents this term occurs in 
-   * @see IndexReader#docFreq(String, BytesRef) */
+   * @see AtomicReader#docFreq(String, BytesRef) */
   public final long docFreq() {
     return docFreq;
   }
   
   /** returns the total number of occurrences of this term
-   * @see IndexReader#totalTermFreq(String, BytesRef) */
+   * @see AtomicReader#totalTermFreq(String, BytesRef) */
   public final long totalTermFreq() {
     return totalTermFreq;
   }
Index: lucene/src/java/org/apache/lucene/search/TimeLimitingCollector.java
===================================================================
--- lucene/src/java/org/apache/lucene/search/TimeLimitingCollector.java	(revision 1238034)
+++ lucene/src/java/org/apache/lucene/search/TimeLimitingCollector.java	(working copy)
@@ -19,7 +19,7 @@
 
 import java.io.IOException;
 
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.util.Counter;
 import org.apache.lucene.util.ThreadInterruptedException;
 
Index: lucene/src/java/org/apache/lucene/search/TopFieldCollector.java
===================================================================
--- lucene/src/java/org/apache/lucene/search/TopFieldCollector.java	(revision 1238034)
+++ lucene/src/java/org/apache/lucene/search/TopFieldCollector.java	(working copy)
@@ -19,7 +19,7 @@
 
 import java.io.IOException;
 
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.search.FieldValueHitQueue.Entry;
 import org.apache.lucene.util.PriorityQueue;
 
Index: lucene/src/java/org/apache/lucene/search/TopScoreDocCollector.java
===================================================================
--- lucene/src/java/org/apache/lucene/search/TopScoreDocCollector.java	(revision 1238034)
+++ lucene/src/java/org/apache/lucene/search/TopScoreDocCollector.java	(working copy)
@@ -19,7 +19,7 @@
 
 import java.io.IOException;
 
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
+import org.apache.lucene.index.AtomicReaderContext;
 
 /**
  * A {@link Collector} implementation that collects the top-scoring hits,
Index: lucene/src/java/org/apache/lucene/search/TotalHitCountCollector.java
===================================================================
--- lucene/src/java/org/apache/lucene/search/TotalHitCountCollector.java	(revision 1238034)
+++ lucene/src/java/org/apache/lucene/search/TotalHitCountCollector.java	(working copy)
@@ -17,7 +17,7 @@
  * limitations under the License.
  */
 
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
+import org.apache.lucene.index.AtomicReaderContext;
 
 /**
  * Just counts the total number of hits.
Index: lucene/src/java/org/apache/lucene/search/Weight.java
===================================================================
--- lucene/src/java/org/apache/lucene/search/Weight.java	(revision 1238034)
+++ lucene/src/java/org/apache/lucene/search/Weight.java	(working copy)
@@ -19,9 +19,9 @@
 
 import java.io.IOException;
 
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
-import org.apache.lucene.index.IndexReader.ReaderContext;
+import org.apache.lucene.index.AtomicReader; // javadocs
+import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.IndexReaderContext; // javadocs
 import org.apache.lucene.search.similarities.SimilarityProvider;
 import org.apache.lucene.util.Bits;
 
@@ -32,13 +32,13 @@
  * {@link Query}, so that a {@link Query} instance can be reused. <br>
  * {@link IndexSearcher} dependent state of the query should reside in the
  * {@link Weight}. <br>
- * {@link IndexReader} dependent state should reside in the {@link Scorer}.
+ * {@link AtomicReader} dependent state should reside in the {@link Scorer}.
  * <p>
  * Since {@link Weight} creates {@link Scorer} instances for a given
- * {@link AtomicReaderContext} ({@link #scorer(IndexReader.AtomicReaderContext, 
+ * {@link AtomicReaderContext} ({@link #scorer(AtomicReaderContext, 
  * boolean, boolean, Bits)})
  * callers must maintain the relationship between the searcher's top-level
- * {@link ReaderContext} and the context used to create a {@link Scorer}. 
+ * {@link IndexReaderContext} and the context used to create a {@link Scorer}. 
  * <p>
  * A <code>Weight</code> is used in the following way:
  * <ol>
@@ -51,7 +51,7 @@
  * <li>The query normalization factor is passed to {@link #normalize(float, float)}. At
  * this point the weighting is complete.
  * <li>A <code>Scorer</code> is constructed by
- * {@link #scorer(IndexReader.AtomicReaderContext, boolean, boolean, Bits)}.
+ * {@link #scorer(AtomicReaderContext, boolean, boolean, Bits)}.
  * </ol>
  * 
  * @since 2.9
@@ -117,7 +117,7 @@
    * Returns true iff this implementation scores docs only out of order. This
    * method is used in conjunction with {@link Collector}'s
    * {@link Collector#acceptsDocsOutOfOrder() acceptsDocsOutOfOrder} and
-   * {@link #scorer(IndexReader.AtomicReaderContext, boolean, boolean, Bits)} to
+   * {@link #scorer(AtomicReaderContext, boolean, boolean, Bits)} to
    * create a matching {@link Scorer} instance for a given {@link Collector}, or
    * vice versa.
    * <p>
Index: lucene/src/java/org/apache/lucene/util/FieldCacheSanityChecker.java
===================================================================
--- lucene/src/java/org/apache/lucene/util/FieldCacheSanityChecker.java	(revision 1238034)
+++ lucene/src/java/org/apache/lucene/util/FieldCacheSanityChecker.java	(working copy)
@@ -23,7 +23,7 @@
 import java.util.Map;
 import java.util.Set;
 
-import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.AtomicReader;
 import org.apache.lucene.search.FieldCache;
 import org.apache.lucene.search.FieldCache.CacheEntry;
 
@@ -146,9 +146,6 @@
     insanity.addAll(checkValueMismatch(valIdToItems, 
                                        readerFieldToValIds, 
                                        valMismatchKeys));
-    insanity.addAll(checkSubreaders(valIdToItems, 
-                                    readerFieldToValIds));
-                    
     return insanity.toArray(new Insanity[insanity.size()]);
   }
 
@@ -189,108 +186,7 @@
     return insanity;
   }
 
-  /** 
-   * Internal helper method used by check that iterates over 
-   * the keys of readerFieldToValIds and generates a Collection 
-   * of Insanity instances whenever two (or more) ReaderField instances are 
-   * found that have an ancestry relationships.  
-   *
-   * @see InsanityType#SUBREADER
-   */
-  private Collection<Insanity> checkSubreaders( MapOfSets<Integer, CacheEntry>  valIdToItems,
-                                      MapOfSets<ReaderField, Integer> readerFieldToValIds) {
-
-    final List<Insanity> insanity = new ArrayList<Insanity>(23);
-
-    Map<ReaderField, Set<ReaderField>> badChildren = new HashMap<ReaderField, Set<ReaderField>>(17);
-    MapOfSets<ReaderField, ReaderField> badKids = new MapOfSets<ReaderField, ReaderField>(badChildren); // wrapper
-
-    Map<Integer, Set<CacheEntry>> viToItemSets = valIdToItems.getMap();
-    Map<ReaderField, Set<Integer>> rfToValIdSets = readerFieldToValIds.getMap();
-
-    Set<ReaderField> seen = new HashSet<ReaderField>(17);
-
-    Set<ReaderField> readerFields = rfToValIdSets.keySet();
-    for (final ReaderField rf : readerFields) {
-      
-      if (seen.contains(rf)) continue;
-
-      List<Object> kids = getAllDescendantReaderKeys(rf.readerKey);
-      for (Object kidKey : kids) {
-        ReaderField kid = new ReaderField(kidKey, rf.fieldName);
-        
-        if (badChildren.containsKey(kid)) {
-          // we've already process this kid as RF and found other problems
-          // track those problems as our own
-          badKids.put(rf, kid);
-          badKids.putAll(rf, badChildren.get(kid));
-          badChildren.remove(kid);
-          
-        } else if (rfToValIdSets.containsKey(kid)) {
-          // we have cache entries for the kid
-          badKids.put(rf, kid);
-        }
-        seen.add(kid);
-      }
-      seen.add(rf);
-    }
-
-    // every mapping in badKids represents an Insanity
-    for (final ReaderField parent : badChildren.keySet()) {
-      Set<ReaderField> kids = badChildren.get(parent);
-
-      List<CacheEntry> badEntries = new ArrayList<CacheEntry>(kids.size() * 2);
-
-      // put parent entr(ies) in first
-      {
-        for (final Integer value  : rfToValIdSets.get(parent)) {
-          badEntries.addAll(viToItemSets.get(value));
-        }
-      }
-
-      // now the entries for the descendants
-      for (final ReaderField kid : kids) {
-        for (final Integer value : rfToValIdSets.get(kid)) {
-          badEntries.addAll(viToItemSets.get(value));
-        }
-      }
-
-      CacheEntry[] badness = new CacheEntry[badEntries.size()];
-      badness = badEntries.toArray(badness);
-
-      insanity.add(new Insanity(InsanityType.SUBREADER,
-                                "Found caches for descendants of " + 
-                                parent.toString(),
-                                badness));
-    }
-
-    return insanity;
-
-  }
-
   /**
-   * Checks if the seed is an IndexReader, and if so will walk
-   * the hierarchy of subReaders building up a list of the objects 
-   * returned by obj.getFieldCacheKey()
-   */
-  private List<Object> getAllDescendantReaderKeys(Object seed) {
-    List<Object> all = new ArrayList<Object>(17); // will grow as we iter
-    all.add(seed);
-    for (int i = 0; i < all.size(); i++) {
-      Object obj = all.get(i);
-      if (obj instanceof IndexReader) {
-        IndexReader[] subs = ((IndexReader)obj).getSequentialSubReaders();
-        for (int j = 0; (null != subs) && (j < subs.length); j++) {
-          all.add(subs[j].getCoreCacheKey());
-        }
-      }
-      
-    }
-    // need to skip the first, because it was the seed
-    return all.subList(1, all.size());
-  }
-
-  /**
    * Simple pair object for using "readerKey + fieldName" a Map key
    */
   private final static class ReaderField {
Index: lucene/src/java/org/apache/lucene/util/ReaderUtil.java
===================================================================
--- lucene/src/java/org/apache/lucene/util/ReaderUtil.java	(revision 1238034)
+++ lucene/src/java/org/apache/lucene/util/ReaderUtil.java	(working copy)
@@ -18,17 +18,16 @@
  */
 
 import java.util.ArrayList;
-import java.util.Collection;
-import java.util.HashSet;
 import java.util.List;
 import java.io.IOException;
 
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.FieldInfo;
 import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.CompositeReader;
+import org.apache.lucene.index.AtomicReader;
 import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
-import org.apache.lucene.index.IndexReader.CompositeReaderContext;
-import org.apache.lucene.index.IndexReader.ReaderContext;
+import org.apache.lucene.index.IndexReaderContext;
 
 /**
  * Common util methods for dealing with {@link IndexReader}s.
@@ -68,11 +67,11 @@
    * @param reader
    */
 
-  public static void gatherSubReaders(final List<IndexReader> allSubReaders, IndexReader reader) {
+  public static void gatherSubReaders(final List<AtomicReader> allSubReaders, IndexReader reader) {
     try {
       new Gather(reader) {
         @Override
-        protected void add(int base, IndexReader r) {
+        protected void add(int base, AtomicReader r) {
           allSubReaders.add(r);
         }
       }.run();
@@ -103,13 +102,13 @@
     }
 
     private int run(int base, IndexReader reader) throws IOException {
-      IndexReader[] subReaders = reader.getSequentialSubReaders();
-      if (subReaders == null) {
+      if (reader instanceof AtomicReader) {
         // atomic reader
-        add(base, reader);
+        add(base, (AtomicReader) reader);
         base += reader.maxDoc();
       } else {
-        // composite reader
+        assert reader instanceof CompositeReader : "must be a composite reader";
+        IndexReader[] subReaders = ((CompositeReader) reader).getSequentialSubReaders();
         for (int i = 0; i < subReaders.length; i++) {
           base = run(base, subReaders[i]);
         }
@@ -118,81 +117,19 @@
       return base;
     }
 
-    protected abstract void add(int base, IndexReader r) throws IOException;
+    protected abstract void add(int base, AtomicReader r) throws IOException;
   }
   
-  public static ReaderContext buildReaderContext(IndexReader reader) {
-    return new ReaderContextBuilder(reader).build();
-  }
-  
-  public static class ReaderContextBuilder {
-    private final IndexReader reader;
-    private final AtomicReaderContext[] leaves;
-    private int leafOrd = 0;
-    private int leafDocBase = 0;
-    public ReaderContextBuilder(IndexReader reader) {
-      this.reader = reader;
-      leaves = new AtomicReaderContext[numLeaves(reader)];
-    }
-    
-    public ReaderContext build() {
-      return build(null, reader, 0, 0);
-    }
-    
-    private ReaderContext build(CompositeReaderContext parent, IndexReader reader, int ord, int docBase) {
-      IndexReader[] sequentialSubReaders = reader.getSequentialSubReaders();
-      if (sequentialSubReaders == null) {
-        AtomicReaderContext atomic = new AtomicReaderContext(parent, reader, ord, docBase, leafOrd, leafDocBase);
-        leaves[leafOrd++] = atomic;
-        leafDocBase += reader.maxDoc();
-        return atomic;
-      } else {
-        ReaderContext[] children = new ReaderContext[sequentialSubReaders.length];
-        final CompositeReaderContext newParent;
-        if (parent == null) {
-          newParent = new CompositeReaderContext(reader, children, leaves);
-        } else {
-          newParent = new CompositeReaderContext(parent, reader, ord, docBase, children);
-        }
-        
-        int newDocBase = 0;
-        for (int i = 0; i < sequentialSubReaders.length; i++) {
-          children[i] = build(newParent, sequentialSubReaders[i], i, newDocBase);
-          newDocBase += sequentialSubReaders[i].maxDoc();
-        }
-        return newParent;
-      }
-    }
-    
-    private int numLeaves(IndexReader reader) {
-      final int[] numLeaves = new int[1];
-      try {
-        new Gather(reader) {
-          @Override
-          protected void add(int base, IndexReader r) {
-            numLeaves[0]++;
-          }
-        }.run();
-      } catch (IOException ioe) {
-        // won't happen
-        throw new RuntimeException(ioe);
-      }
-      return numLeaves[0];
-    }
-    
-  }
-
   /**
    * Returns the context's leaves or the context itself as the only element of
    * the returned array. If the context's #leaves() method returns
    * <code>null</code> the given context must be an instance of
    * {@link AtomicReaderContext}
    */
-  public static AtomicReaderContext[] leaves(ReaderContext context) {
+  public static AtomicReaderContext[] leaves(IndexReaderContext context) {
     assert context != null && context.isTopLevel : "context must be non-null & top-level";
     final AtomicReaderContext[] leaves = context.leaves();
     if (leaves == null) {
-      assert context.isAtomic : "top-level context without leaves must be atomic";
       return new AtomicReaderContext[] { (AtomicReaderContext) context };
     }
     return leaves;
@@ -202,7 +139,7 @@
    * Walks up the reader tree and return the given context's top level reader
    * context, or in other words the reader tree's root context.
    */
-  public static ReaderContext getTopLevelContext(ReaderContext context) {
+  public static IndexReaderContext getTopLevelContext(IndexReaderContext context) {
     while (context.parent != null) {
       context = context.parent;
     }
@@ -260,26 +197,4 @@
     }
     return hi;
   }
-
-  public static Collection<String> getIndexedFields(IndexReader reader) {
-    final Collection<String> fields = new HashSet<String>();
-    for(FieldInfo fieldInfo : getMergedFieldInfos(reader)) {
-      if (fieldInfo.isIndexed) {
-        fields.add(fieldInfo.name);
-      }
-    }
-    return fields;
-  }
-
-  /** Call this to get the (merged) FieldInfos for a
-   *  composite reader */
-  public static FieldInfos getMergedFieldInfos(IndexReader reader) {
-    final List<IndexReader> subReaders = new ArrayList<IndexReader>();
-    ReaderUtil.gatherSubReaders(subReaders, reader);
-    final FieldInfos fieldInfos = new FieldInfos();
-    for(IndexReader subReader : subReaders) {
-      fieldInfos.add(subReader.getFieldInfos());
-    }
-    return fieldInfos;
-  }
 }
Index: lucene/src/java/org/apache/lucene/util/TermContext.java
===================================================================
--- lucene/src/java/org/apache/lucene/util/TermContext.java	(revision 1238034)
+++ lucene/src/java/org/apache/lucene/util/TermContext.java	(working copy)
@@ -20,10 +20,10 @@
 import java.io.IOException;
 import java.util.Arrays;
 
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.Fields;
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
-import org.apache.lucene.index.IndexReader.ReaderContext;
 import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.IndexReaderContext;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.index.TermState;
 import org.apache.lucene.index.Terms;
@@ -39,7 +39,7 @@
  * @lucene.experimental
  */
 public final class TermContext {
-  public final ReaderContext topReaderContext; // for asserting!
+  public final IndexReaderContext topReaderContext; // for asserting!
   private final TermState[] states;
   private int docFreq;
   private long totalTermFreq;
@@ -47,9 +47,9 @@
   //public static boolean DEBUG = BlockTreeTermsWriter.DEBUG;
 
   /**
-   * Creates an empty {@link TermContext} from a {@link ReaderContext}
+   * Creates an empty {@link TermContext} from a {@link IndexReaderContext}
    */
-  public TermContext(ReaderContext context) {
+  public TermContext(IndexReaderContext context) {
     assert context != null && context.isTopLevel;
     topReaderContext = context;
     docFreq = 0;
@@ -66,20 +66,20 @@
    * Creates a {@link TermContext} with an initial {@link TermState},
    * {@link IndexReader} pair.
    */
-  public TermContext(ReaderContext context, TermState state, int ord, int docFreq, long totalTermFreq) {
+  public TermContext(IndexReaderContext context, TermState state, int ord, int docFreq, long totalTermFreq) {
     this(context);
     register(state, ord, docFreq, totalTermFreq);
   }
 
   /**
-   * Creates a {@link TermContext} from a top-level {@link ReaderContext} and the
+   * Creates a {@link TermContext} from a top-level {@link IndexReaderContext} and the
    * given {@link Term}. This method will lookup the given term in all context's leaf readers 
    * and register each of the readers containing the term in the returned {@link TermContext}
    * using the leaf reader's ordinal.
    * <p>
    * Note: the given context must be a top-level context.
    */
-  public static TermContext build(ReaderContext context, Term term, boolean cache)
+  public static TermContext build(IndexReaderContext context, Term term, boolean cache)
       throws IOException {
     assert context != null && context.isTopLevel;
     final String field = term.field();
@@ -89,7 +89,7 @@
     //if (DEBUG) System.out.println("prts.build term=" + term);
     for (int i = 0; i < leaves.length; i++) {
       //if (DEBUG) System.out.println("  r=" + leaves[i].reader);
-      final Fields fields = leaves[i].reader.fields();
+      final Fields fields = leaves[i].reader().fields();
       if (fields != null) {
         final Terms terms = fields.terms(field);
         if (terms != null) {
@@ -116,7 +116,7 @@
 
   /**
    * Registers and associates a {@link TermState} with an leaf ordinal. The leaf ordinal
-   * should be derived from a {@link ReaderContext}'s leaf ord.
+   * should be derived from a {@link IndexReaderContext}'s leaf ord.
    */
   public void register(TermState state, final int ord, final int docFreq, final long totalTermFreq) {
     assert state != null : "state must not be null";
Index: lucene/src/test-framework/java/org/apache/lucene/index/RandomIndexWriter.java
===================================================================
--- lucene/src/test-framework/java/org/apache/lucene/index/RandomIndexWriter.java	(revision 1238034)
+++ lucene/src/test-framework/java/org/apache/lucene/index/RandomIndexWriter.java	(working copy)
@@ -328,7 +328,7 @@
     w.deleteAll();
   }
 
-  public IndexReader getReader() throws IOException {
+  public DirectoryReader getReader() throws IOException {
     return getReader(true);
   }
 
@@ -367,7 +367,7 @@
     switchDoDocValues();
   }
 
-  public IndexReader getReader(boolean applyDeletions) throws IOException {
+  public DirectoryReader getReader(boolean applyDeletions) throws IOException {
     getReaderCalled = true;
     if (r.nextInt(4) == 2) {
       doRandomForceMerge();
Index: lucene/src/test-framework/java/org/apache/lucene/index/ThreadedIndexingAndSearchingTestCase.java
===================================================================
--- lucene/src/test-framework/java/org/apache/lucene/index/ThreadedIndexingAndSearchingTestCase.java	(revision 1238034)
+++ lucene/src/test-framework/java/org/apache/lucene/index/ThreadedIndexingAndSearchingTestCase.java	(working copy)
@@ -455,7 +455,7 @@
 
     conf.setMergedSegmentWarmer(new IndexWriter.IndexReaderWarmer() {
       @Override
-      public void warm(IndexReader reader) throws IOException {
+      public void warm(AtomicReader reader) throws IOException {
         if (VERBOSE) {
           System.out.println("TEST: now warm merged reader=" + reader);
         }
Index: lucene/src/test-framework/java/org/apache/lucene/search/AssertingIndexSearcher.java
===================================================================
--- lucene/src/test-framework/java/org/apache/lucene/search/AssertingIndexSearcher.java	(revision 1238034)
+++ lucene/src/test-framework/java/org/apache/lucene/search/AssertingIndexSearcher.java	(working copy)
@@ -21,9 +21,9 @@
 import java.util.concurrent.ExecutorService;
 import java.io.IOException;
 
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
-import org.apache.lucene.index.IndexReader.ReaderContext;
+import org.apache.lucene.index.IndexReaderContext;
 import org.apache.lucene.util.Bits;
 
 /** 
@@ -38,7 +38,7 @@
     this.random = new Random(random.nextLong());
   }
   
-  public  AssertingIndexSearcher(Random random, ReaderContext context) {
+  public  AssertingIndexSearcher(Random random, IndexReaderContext context) {
     super(context);
     this.random = new Random(random.nextLong());
   }
@@ -48,7 +48,7 @@
     this.random = new Random(random.nextLong());
   }
   
-  public  AssertingIndexSearcher(Random random, ReaderContext context, ExecutorService ex) {
+  public  AssertingIndexSearcher(Random random, IndexReaderContext context, ExecutorService ex) {
     super(context, ex);
     this.random = new Random(random.nextLong());
   }
Index: lucene/src/test-framework/java/org/apache/lucene/search/CheckHits.java
===================================================================
--- lucene/src/test-framework/java/org/apache/lucene/search/CheckHits.java	(revision 1238034)
+++ lucene/src/test-framework/java/org/apache/lucene/search/CheckHits.java	(working copy)
@@ -25,8 +25,8 @@
 
 import junit.framework.Assert;
 
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.util.LuceneTestCase;
 
@@ -116,7 +116,7 @@
       Assert.assertEquals("Wrap Reader " + i + ": " +
                           query.toString(defaultFieldName),
                           correct, actual);
-      FieldCache.DEFAULT.purge(s.getIndexReader()); // our wrapping can create insanity otherwise
+      // TODO: FieldCache.DEFAULT.purge(s.getIndexReader()); // our wrapping can create insanity otherwise
     }
   }
 
Index: lucene/src/test-framework/java/org/apache/lucene/search/QueryUtils.java
===================================================================
--- lucene/src/test-framework/java/org/apache/lucene/search/QueryUtils.java	(revision 1238034)
+++ lucene/src/test-framework/java/org/apache/lucene/search/QueryUtils.java	(working copy)
@@ -24,8 +24,10 @@
 
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.document.Document;
+import org.apache.lucene.index.AtomicReader;
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
+import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexWriter;
 import org.apache.lucene.index.IndexWriterConfig;
 import org.apache.lucene.index.MultiReader;
@@ -114,11 +116,14 @@
         if (wrap) {
           IndexSearcher wrapped;
           check(random, q1, wrapped = wrapUnderlyingReader(random, s, -1), false);
-          FieldCache.DEFAULT.purge(wrapped.getIndexReader()); // // our wrapping can create insanity otherwise
+          // TODO: I removed that as we can never get insanity by composite readers anymore... Is this ok?
+          //FieldCache.DEFAULT.purge(wrapped.getIndexReader()); // our wrapping can create insanity otherwise
           check(random, q1, wrapped = wrapUnderlyingReader(random, s,  0), false);
-          FieldCache.DEFAULT.purge(wrapped.getIndexReader()); // // our wrapping can create insanity otherwise
+          // TODO: I removed that as we can never get insanity by composite readers anymore... Is this ok?
+          //FieldCache.DEFAULT.purge(wrapped.getIndexReader()); // our wrapping can create insanity otherwise
           check(random, q1, wrapped = wrapUnderlyingReader(random, s, +1), false);
-          FieldCache.DEFAULT.purge(wrapped.getIndexReader()); // // our wrapping can create insanity otherwise
+          // TODO: I removed that as we can never get insanity by composite readers anymore... Is this ok?
+          //FieldCache.DEFAULT.purge(wrapped.getIndexReader()); // our wrapping can create insanity otherwise
         }
         checkExplanations(q1,s);
         
@@ -176,7 +181,7 @@
     }
   }
 
-  private static IndexReader makeEmptyIndex(Random random, final int numDeletedDocs) 
+  private static DirectoryReader makeEmptyIndex(Random random, final int numDeletedDocs) 
     throws IOException {
     Directory d = new MockDirectoryWrapper(random, new RAMDirectory());
       IndexWriter w = new IndexWriter(d, new IndexWriterConfig(
@@ -197,7 +202,7 @@
       Assert.assertEquals("writer has non-deleted docs", 
                           0, w.numDocs());
       w.close();
-      IndexReader r = IndexReader.open(d);
+      DirectoryReader r = DirectoryReader.open(d);
       Assert.assertEquals("reader has wrong number of deleted docs", 
                           numDeletedDocs, r.numDeletedDocs());
       return r;
@@ -234,7 +239,7 @@
         // FUTURE: ensure scorer.doc()==-1
 
         final float maxDiff = 1e-5f;
-        final IndexReader lastReader[] = {null};
+        final AtomicReader lastReader[] = {null};
 
         s.search(q, new Collector() {
           private Scorer sc;
@@ -254,7 +259,7 @@
               if (scorer == null) {
                 Weight w = s.createNormalizedWeight(q);
                 AtomicReaderContext context = readerContextArray[leafPtr];
-                scorer = w.scorer(context, true, false, context.reader.getLiveDocs());
+                scorer = w.scorer(context, true, false, context.reader().getLiveDocs());
               }
               
               int op = order[(opidx[0]++) % order.length];
@@ -296,19 +301,19 @@
             // confirm that skipping beyond the last doc, on the
             // previous reader, hits NO_MORE_DOCS
             if (lastReader[0] != null) {
-              final IndexReader previousReader = lastReader[0];
+              final AtomicReader previousReader = lastReader[0];
               IndexSearcher indexSearcher = LuceneTestCase.newSearcher(previousReader);
               Weight w = indexSearcher.createNormalizedWeight(q);
               AtomicReaderContext ctx = (AtomicReaderContext)indexSearcher.getTopReaderContext();
-              Scorer scorer = w.scorer(ctx, true, false, ctx.reader.getLiveDocs());
+              Scorer scorer = w.scorer(ctx, true, false, ctx.reader().getLiveDocs());
               if (scorer != null) {
                 boolean more = scorer.advance(lastDoc[0] + 1) != DocIdSetIterator.NO_MORE_DOCS;
                 Assert.assertFalse("query's last doc was "+ lastDoc[0] +" but skipTo("+(lastDoc[0]+1)+") got to "+scorer.docID(),more);
               }
               leafPtr++;
             }
-            lastReader[0] = context.reader;
-            assert readerContextArray[leafPtr].reader == context.reader;
+            lastReader[0] = context.reader();
+            assert readerContextArray[leafPtr].reader() == context.reader();
             this.scorer = null;
             lastDoc[0] = -1;
           }
@@ -322,11 +327,11 @@
         if (lastReader[0] != null) {
           // confirm that skipping beyond the last doc, on the
           // previous reader, hits NO_MORE_DOCS
-          final IndexReader previousReader = lastReader[0];
+          final AtomicReader previousReader = lastReader[0];
           IndexSearcher indexSearcher = LuceneTestCase.newSearcher(previousReader, false);
           Weight w = indexSearcher.createNormalizedWeight(q);
-          AtomicReaderContext ctx = (AtomicReaderContext)previousReader.getTopReaderContext();
-          Scorer scorer = w.scorer(ctx, true, false, ctx.reader.getLiveDocs());
+          AtomicReaderContext ctx = previousReader.getTopReaderContext();
+          Scorer scorer = w.scorer(ctx, true, false, ctx.reader().getLiveDocs());
           if (scorer != null) {
             boolean more = scorer.advance(lastDoc[0] + 1) != DocIdSetIterator.NO_MORE_DOCS;
             Assert.assertFalse("query's last doc was "+ lastDoc[0] +" but skipTo("+(lastDoc[0]+1)+") got to "+scorer.docID(),more);
@@ -340,7 +345,7 @@
     //System.out.println("checkFirstSkipTo: "+q);
     final float maxDiff = 1e-3f;
     final int lastDoc[] = {-1};
-    final IndexReader lastReader[] = {null};
+    final AtomicReader lastReader[] = {null};
     final AtomicReaderContext[] context = ReaderUtil.leaves(s.getTopReaderContext());
     s.search(q,new Collector() {
       private Scorer scorer;
@@ -381,7 +386,7 @@
         // confirm that skipping beyond the last doc, on the
         // previous reader, hits NO_MORE_DOCS
         if (lastReader[0] != null) {
-          final IndexReader previousReader = lastReader[0];
+          final AtomicReader previousReader = lastReader[0];
           IndexSearcher indexSearcher = LuceneTestCase.newSearcher(previousReader);
           Weight w = indexSearcher.createNormalizedWeight(q);
           Scorer scorer = w.scorer((AtomicReaderContext)indexSearcher.getTopReaderContext(), true, false, previousReader.getLiveDocs());
@@ -392,9 +397,9 @@
           leafPtr++;
         }
 
-        lastReader[0] = context.reader;
+        lastReader[0] = context.reader();
         lastDoc[0] = -1;
-        liveDocs = context.reader.getLiveDocs();
+        liveDocs = context.reader().getLiveDocs();
       }
       @Override
       public boolean acceptsDocsOutOfOrder() {
@@ -405,7 +410,7 @@
     if (lastReader[0] != null) {
       // confirm that skipping beyond the last doc, on the
       // previous reader, hits NO_MORE_DOCS
-      final IndexReader previousReader = lastReader[0];
+      final AtomicReader previousReader = lastReader[0];
       IndexSearcher indexSearcher = LuceneTestCase.newSearcher(previousReader);
       Weight w = indexSearcher.createNormalizedWeight(q);
       Scorer scorer = w.scorer((AtomicReaderContext)indexSearcher.getTopReaderContext(), true, false, previousReader.getLiveDocs());
Index: lucene/src/test-framework/java/org/apache/lucene/store/MockDirectoryWrapper.java
===================================================================
--- lucene/src/test-framework/java/org/apache/lucene/store/MockDirectoryWrapper.java	(revision 1238034)
+++ lucene/src/test-framework/java/org/apache/lucene/store/MockDirectoryWrapper.java	(working copy)
@@ -33,7 +33,7 @@
 import java.util.Set;
 import java.util.concurrent.atomic.AtomicInteger;
 
-import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexWriter;
 import org.apache.lucene.index.IndexWriterConfig;
 import org.apache.lucene.util.LuceneTestCase;
@@ -559,7 +559,7 @@
     }
     open = false;
     if (checkIndexOnClose) {
-      if (IndexReader.indexExists(this)) {
+      if (DirectoryReader.indexExists(this)) {
         if (LuceneTestCase.VERBOSE) {
           System.out.println("\nNOTE: MockDirectoryWrapper: now crash");
         }
@@ -582,11 +582,11 @@
             assert false : "unreferenced files: before delete:\n    " + Arrays.toString(startFiles) + "\n  after delete:\n    " + Arrays.toString(endFiles);
           }
 
-          IndexReader ir1 = IndexReader.open(this);
+          DirectoryReader ir1 = DirectoryReader.open(this);
           int numDocs1 = ir1.numDocs();
           ir1.close();
           new IndexWriter(this, new IndexWriterConfig(LuceneTestCase.TEST_VERSION_CURRENT, null)).close();
-          IndexReader ir2 = IndexReader.open(this);
+          DirectoryReader ir2 = DirectoryReader.open(this);
           int numDocs2 = ir2.numDocs();
           ir2.close();
           assert numDocs1 == numDocs2 : "numDocs changed after opening/closing IW: before=" + numDocs1 + " after=" + numDocs2;
Index: lucene/src/test-framework/java/org/apache/lucene/util/LuceneTestCase.java
===================================================================
--- lucene/src/test-framework/java/org/apache/lucene/util/LuceneTestCase.java	(revision 1238034)
+++ lucene/src/test-framework/java/org/apache/lucene/util/LuceneTestCase.java	(working copy)
@@ -189,14 +189,11 @@
    * Some tests expect the directory to contain a single segment, and want to do tests on that segment's reader.
    * This is an utility method to help them.
    */
-  public static SegmentReader getOnlySegmentReader(IndexReader reader) {
-    if (reader instanceof SegmentReader)
-      return (SegmentReader) reader;
-
+  public static SegmentReader getOnlySegmentReader(DirectoryReader reader) {
     IndexReader[] subReaders = reader.getSequentialSubReaders();
     if (subReaders.length != 1)
       throw new IllegalArgumentException(reader + " has " + subReaders.length + " segments instead of exactly one");
-
+    assertTrue(subReaders[0] instanceof SegmentReader);
     return (SegmentReader) subReaders[0];
   }
 
@@ -1239,7 +1236,7 @@
   public static IndexSearcher newSearcher(IndexReader r, boolean maybeWrap) throws IOException {
     if (usually()) {
       if (maybeWrap && rarely()) {
-        r = new SlowMultiReaderWrapper(r);
+        r = SlowCompositeReaderWrapper.wrap(r);
       }
       IndexSearcher ret = random.nextBoolean() ? new AssertingIndexSearcher(random, r) : new AssertingIndexSearcher(random, r.getTopReaderContext());
       ret.setSimilarityProvider(similarityProvider);
Index: lucene/src/test/org/apache/lucene/codecs/lucene3x/TestTermInfosReaderIndex.java
===================================================================
--- lucene/src/test/org/apache/lucene/codecs/lucene3x/TestTermInfosReaderIndex.java	(revision 1238034)
+++ lucene/src/test/org/apache/lucene/codecs/lucene3x/TestTermInfosReaderIndex.java	(working copy)
@@ -34,6 +34,7 @@
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.StringField;
 import org.apache.lucene.index.CorruptIndexException;
+import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.FieldInfos;
 import org.apache.lucene.index.FieldsEnum;
 import org.apache.lucene.index.IndexFileNames;
@@ -93,8 +94,8 @@
     
     populate(directory, config);
 
-    IndexReader r0 = IndexReader.open(directory);
-    SegmentReader r = (SegmentReader) r0.getSequentialSubReaders()[0];
+    DirectoryReader r0 = IndexReader.open(directory);
+    SegmentReader r = LuceneTestCase.getOnlySegmentReader(r0);
     String segment = r.getSegmentName();
     r.close();
 
Index: lucene/src/test/org/apache/lucene/codecs/lucene40/TestReuseDocsEnum.java
===================================================================
--- lucene/src/test/org/apache/lucene/codecs/lucene40/TestReuseDocsEnum.java	(revision 1238034)
+++ lucene/src/test/org/apache/lucene/codecs/lucene40/TestReuseDocsEnum.java	(working copy)
@@ -22,6 +22,8 @@
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.codecs.Codec;
 import org.apache.lucene.codecs.lucene40.Lucene40PostingsFormat;
+import org.apache.lucene.index.AtomicReader;
+import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.DocsEnum;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.RandomIndexWriter;
@@ -34,6 +36,7 @@
 import org.apache.lucene.util.IOUtils;
 import org.apache.lucene.util.LineFileDocs;
 import org.apache.lucene.util.LuceneTestCase;
+import org.apache.lucene.util.ReaderUtil;
 import org.apache.lucene.util._TestUtil;
 
 public class TestReuseDocsEnum extends LuceneTestCase {
@@ -47,20 +50,22 @@
     createRandomIndex(numdocs, writer, random);
     writer.commit();
 
-    IndexReader open = IndexReader.open(dir);
-    IndexReader[] sequentialSubReaders = open.getSequentialSubReaders();
-    for (IndexReader indexReader : sequentialSubReaders) {
-      Terms terms = indexReader.terms("body");
-      TermsEnum iterator = terms.iterator(null);
-      IdentityHashMap<DocsEnum, Boolean> enums = new IdentityHashMap<DocsEnum, Boolean>();
-      MatchNoBits bits = new Bits.MatchNoBits(open.maxDoc());
-      while ((iterator.next()) != null) {
-        DocsEnum docs = iterator.docs(random.nextBoolean() ? bits : new Bits.MatchNoBits(open.maxDoc()), null, random.nextBoolean());
-        enums.put(docs, true);
+    DirectoryReader open = DirectoryReader.open(dir);
+    new ReaderUtil.Gather(open) {
+      @Override
+      protected void add(int base, AtomicReader r) throws IOException {
+        Terms terms = r.terms("body");
+        TermsEnum iterator = terms.iterator(null);
+        IdentityHashMap<DocsEnum, Boolean> enums = new IdentityHashMap<DocsEnum, Boolean>();
+        MatchNoBits bits = new Bits.MatchNoBits(r.maxDoc());
+        while ((iterator.next()) != null) {
+          DocsEnum docs = iterator.docs(random.nextBoolean() ? bits : new Bits.MatchNoBits(r.maxDoc()), null, random.nextBoolean());
+          enums.put(docs, true);
+        }
+        
+        assertEquals(terms.getUniqueTermCount(), enums.size());  
       }
-      
-      assertEquals(terms.getUniqueTermCount(), enums.size());  
-    }
+    }.run();
     IOUtils.close(writer, open, dir);
   }
   
@@ -74,10 +79,10 @@
     createRandomIndex(numdocs, writer, random);
     writer.commit();
 
-    IndexReader open = IndexReader.open(dir);
+    DirectoryReader open = DirectoryReader.open(dir);
     IndexReader[] sequentialSubReaders = open.getSequentialSubReaders();
     for (IndexReader indexReader : sequentialSubReaders) {
-      Terms terms = indexReader.terms("body");
+      Terms terms = ((AtomicReader) indexReader).terms("body");
       TermsEnum iterator = terms.iterator(null);
       IdentityHashMap<DocsEnum, Boolean> enums = new IdentityHashMap<DocsEnum, Boolean>();
       MatchNoBits bits = new Bits.MatchNoBits(open.maxDoc());
@@ -119,13 +124,13 @@
     createRandomIndex(numdocs, writer, random);
     writer.commit();
 
-    IndexReader firstReader = IndexReader.open(dir);
-    IndexReader secondReader = IndexReader.open(dir);
+    DirectoryReader firstReader = DirectoryReader.open(dir);
+    DirectoryReader secondReader = DirectoryReader.open(dir);
     IndexReader[] sequentialSubReaders = firstReader.getSequentialSubReaders();
     IndexReader[] sequentialSubReaders2 = secondReader.getSequentialSubReaders();
     
     for (IndexReader indexReader : sequentialSubReaders) {
-      Terms terms = indexReader.terms("body");
+      Terms terms = ((AtomicReader) indexReader).terms("body");
       TermsEnum iterator = terms.iterator(null);
       IdentityHashMap<DocsEnum, Boolean> enums = new IdentityHashMap<DocsEnum, Boolean>();
       MatchNoBits bits = new Bits.MatchNoBits(firstReader.maxDoc());
@@ -154,7 +159,7 @@
     if (random.nextInt(10) == 0) {
       return null;
     }
-    IndexReader indexReader = readers[random.nextInt(readers.length)];
+    AtomicReader indexReader = (AtomicReader) readers[random.nextInt(readers.length)];
     return indexReader.termDocsEnum(bits, field, term, random.nextBoolean());
   }
 
Index: lucene/src/test/org/apache/lucene/codecs/pulsing/TestPulsingReuse.java
===================================================================
--- lucene/src/test/org/apache/lucene/codecs/pulsing/TestPulsingReuse.java	(revision 1238034)
+++ lucene/src/test/org/apache/lucene/codecs/pulsing/TestPulsingReuse.java	(working copy)
@@ -26,7 +26,9 @@
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.TextField;
+import org.apache.lucene.index.AtomicReader;
 import org.apache.lucene.index.CheckIndex;
+import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.DocsAndPositionsEnum;
 import org.apache.lucene.index.DocsEnum;
 import org.apache.lucene.index.IndexReader;
@@ -51,10 +53,10 @@
     Document doc = new Document();
     doc.add(new Field("foo", "a b b c c c d e f g g h i i j j k", TextField.TYPE_UNSTORED));
     iw.addDocument(doc);
-    IndexReader ir = iw.getReader();
+    DirectoryReader ir = iw.getReader();
     iw.close();
     
-    IndexReader segment = ir.getSequentialSubReaders()[0];
+    AtomicReader segment = getOnlySegmentReader(ir);
     DocsEnum reuse = null;
     Map<DocsEnum,Boolean> allEnums = new IdentityHashMap<DocsEnum,Boolean>();
     TermsEnum te = segment.terms("foo").iterator(null);
@@ -93,10 +95,10 @@
     // this is because we only track the 'last' enum we reused (not all).
     // but this seems 'good enough' for now.
     iw.addDocument(doc);
-    IndexReader ir = iw.getReader();
+    DirectoryReader ir = iw.getReader();
     iw.close();
     
-    IndexReader segment = ir.getSequentialSubReaders()[0];
+    AtomicReader segment = getOnlySegmentReader(ir);
     DocsEnum reuse = null;
     Map<DocsEnum,Boolean> allEnums = new IdentityHashMap<DocsEnum,Boolean>();
     TermsEnum te = segment.terms("foo").iterator(null);
Index: lucene/src/test/org/apache/lucene/index/TestAddIndexes.java
===================================================================
--- lucene/src/test/org/apache/lucene/index/TestAddIndexes.java	(revision 1238034)
+++ lucene/src/test/org/apache/lucene/index/TestAddIndexes.java	(working copy)
@@ -1279,16 +1279,16 @@
 
     Directory d3 = newDirectory();
     w = new RandomIndexWriter(random, d3);
-    w.addIndexes(new SlowMultiReaderWrapper(r1), new SlowMultiReaderWrapper(r2));
+    w.addIndexes(SlowCompositeReaderWrapper.wrap(r1), SlowCompositeReaderWrapper.wrap(r2));
     r1.close();
     d1.close();
     r2.close();
     d2.close();
 
     w.forceMerge(1);
-    IndexReader r3 = w.getReader();
+    DirectoryReader r3 = w.getReader();
     w.close();
-    IndexReader sr = getOnlySegmentReader(r3);
+    AtomicReader sr = getOnlySegmentReader(r3);
     assertEquals(2, sr.numDocs());
     DocValues docValues = sr.docValues("dv");
     assertNotNull(docValues);
Index: lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility.java
===================================================================
--- lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility.java	(revision 1238034)
+++ lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility.java	(working copy)
@@ -644,12 +644,12 @@
       assertEquals("wrong number of hits", 34, hits.length);
       
       // check decoding into field cache
-      int[] fci = FieldCache.DEFAULT.getInts(new SlowMultiReaderWrapper(searcher.getIndexReader()), "trieInt", false);
+      int[] fci = FieldCache.DEFAULT.getInts(SlowCompositeReaderWrapper.wrap(searcher.getIndexReader()), "trieInt", false);
       for (int val : fci) {
         assertTrue("value in id bounds", val >= 0 && val < 35);
       }
       
-      long[] fcl = FieldCache.DEFAULT.getLongs(new SlowMultiReaderWrapper(searcher.getIndexReader()), "trieLong", false);
+      long[] fcl = FieldCache.DEFAULT.getLongs(SlowCompositeReaderWrapper.wrap(searcher.getIndexReader()), "trieLong", false);
       for (long val : fcl) {
         assertTrue("value in id bounds", val >= 0L && val < 35L);
       }
Index: lucene/src/test/org/apache/lucene/index/TestCodecs.java
===================================================================
--- lucene/src/test/org/apache/lucene/index/TestCodecs.java	(revision 1238034)
+++ lucene/src/test/org/apache/lucene/index/TestCodecs.java	(working copy)
@@ -260,7 +260,7 @@
     Codec codec = Codec.getDefault();
     final SegmentInfo si = new SegmentInfo(SEGMENT, 10000, dir, false, codec, clonedFieldInfos);
 
-    final FieldsProducer reader = codec.postingsFormat().fieldsProducer(new SegmentReadState(dir, si, fieldInfos, newIOContext(random), IndexReader.DEFAULT_TERMS_INDEX_DIVISOR));
+    final FieldsProducer reader = codec.postingsFormat().fieldsProducer(new SegmentReadState(dir, si, fieldInfos, newIOContext(random), DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR));
 
     final FieldsEnum fieldsEnum = reader.iterator();
     assertNotNull(fieldsEnum.next());
@@ -319,7 +319,7 @@
     if (VERBOSE) {
       System.out.println("TEST: now read postings");
     }
-    final FieldsProducer terms = codec.postingsFormat().fieldsProducer(new SegmentReadState(dir, si, fieldInfos, newIOContext(random), IndexReader.DEFAULT_TERMS_INDEX_DIVISOR));
+    final FieldsProducer terms = codec.postingsFormat().fieldsProducer(new SegmentReadState(dir, si, fieldInfos, newIOContext(random), DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR));
 
     final Verify[] threads = new Verify[NUM_TEST_THREADS-1];
     for(int i=0;i<NUM_TEST_THREADS-1;i++) {
Index: lucene/src/test/org/apache/lucene/index/TestCustomNorms.java
===================================================================
--- lucene/src/test/org/apache/lucene/index/TestCustomNorms.java	(revision 1238034)
+++ lucene/src/test/org/apache/lucene/index/TestCustomNorms.java	(working copy)
@@ -79,7 +79,7 @@
     }
     writer.commit();
     writer.close();
-    IndexReader open = new SlowMultiReaderWrapper(IndexReader.open(dir));
+    AtomicReader open = SlowCompositeReaderWrapper.wrap(DirectoryReader.open(dir));
     DocValues normValues = open.normValues(floatTestField);
     assertNotNull(normValues);
     Source source = normValues.getSource();
Index: lucene/src/test/org/apache/lucene/index/TestDeletionPolicy.java
===================================================================
--- lucene/src/test/org/apache/lucene/index/TestDeletionPolicy.java	(revision 1238034)
+++ lucene/src/test/org/apache/lucene/index/TestDeletionPolicy.java	(working copy)
@@ -67,7 +67,7 @@
     }
     public void onCommit(List<? extends IndexCommit> commits) throws IOException {
       IndexCommit lastCommit =  commits.get(commits.size()-1);
-      IndexReader r = IndexReader.open(dir);
+      DirectoryReader r = DirectoryReader.open(dir);
       assertEquals("lastCommit.segmentCount()=" + lastCommit.getSegmentCount() + " vs IndexReader.segmentCount=" + r.getSequentialSubReaders().length, r.getSequentialSubReaders().length, lastCommit.getSegmentCount());
       r.close();
       verifyCommitOrder(commits);
@@ -325,7 +325,7 @@
 
       final boolean needsMerging;
       {
-        IndexReader r = IndexReader.open(dir);
+        DirectoryReader r = DirectoryReader.open(dir);
         needsMerging = r.getSequentialSubReaders().length != 1;
         r.close();
       }
@@ -351,7 +351,7 @@
       assertEquals(1 + (needsMerging ? 1:0), policy.numOnCommit);
 
       // Test listCommits
-      Collection<IndexCommit> commits = IndexReader.listCommits(dir);
+      Collection<IndexCommit> commits = DirectoryReader.listCommits(dir);
       // 2 from closing writer
       assertEquals(1 + (needsMerging ? 1:0), commits.size());
 
@@ -415,7 +415,7 @@
     }
     writer.close();
 
-    Collection<IndexCommit> commits = IndexReader.listCommits(dir);
+    Collection<IndexCommit> commits = DirectoryReader.listCommits(dir);
     assertEquals(5, commits.size());
     IndexCommit lastCommit = null;
     for (final IndexCommit commit : commits) {
@@ -431,7 +431,7 @@
     writer.forceMerge(1);
     writer.close();
 
-    assertEquals(6, IndexReader.listCommits(dir).size());
+    assertEquals(6, DirectoryReader.listCommits(dir).size());
 
     // Now open writer on the commit just before merge:
     writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random))
@@ -441,7 +441,7 @@
     // Should undo our rollback:
     writer.rollback();
 
-    IndexReader r = IndexReader.open(dir);
+    DirectoryReader r = DirectoryReader.open(dir);
     // Still merged, still 11 docs
     assertEquals(1, r.getSequentialSubReaders().length);
     assertEquals(11, r.numDocs());
@@ -454,9 +454,9 @@
     writer.close();
 
     // Now 8 because we made another commit
-    assertEquals(7, IndexReader.listCommits(dir).size());
+    assertEquals(7, DirectoryReader.listCommits(dir).size());
     
-    r = IndexReader.open(dir);
+    r = DirectoryReader.open(dir);
     // Not fully merged because we rolled it back, and now only
     // 10 docs
     assertTrue(r.getSequentialSubReaders().length > 1);
Index: lucene/src/test/org/apache/lucene/index/TestDirectoryReader.java
===================================================================
--- lucene/src/test/org/apache/lucene/index/TestDirectoryReader.java	(revision 1238034)
+++ lucene/src/test/org/apache/lucene/index/TestDirectoryReader.java	(working copy)
@@ -86,31 +86,19 @@
     assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());
     Terms vector = reader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);
     assertNotNull(vector);
-    TestSegmentReader.checkNorms(reader);
+    // TODO: pretty sure this check makes zero sense TestSegmentReader.checkNorms(reader);
     reader.close();
   }
         
   public void testIsCurrent() throws IOException {
-    Directory ramDir1=newDirectory();
-    addDoc(random, ramDir1, "test foo", true);
-    Directory ramDir2=newDirectory();
-    addDoc(random, ramDir2, "test blah", true);
-    IndexReader[] readers = new IndexReader[]{IndexReader.open(ramDir1), IndexReader.open(ramDir2)};
-    MultiReader mr = new MultiReader(readers);
-    assertTrue(mr.isCurrent());   // just opened, must be current
-    addDoc(random, ramDir1, "more text", false);
-    assertFalse(mr.isCurrent());   // has been modified, not current anymore
-    addDoc(random, ramDir2, "even more text", false);
-    assertFalse(mr.isCurrent());   // has been modified even more, not current anymore
-    try {
-      mr.getVersion();
-      fail();
-    } catch (UnsupportedOperationException e) {
-      // expected exception
-    }
-    mr.close();
-    ramDir1.close();
-    ramDir2.close();
+    Directory ramDir=newDirectory();
+    addDoc(random, ramDir, "test foo", true);
+    DirectoryReader reader = DirectoryReader.open(ramDir);
+    assertTrue(reader.isCurrent());   // just opened, must be current
+    addDoc(random, ramDir, "more text", false);
+    assertFalse(reader.isCurrent());   // has been modified, not current anymore
+    reader.close();
+    ramDir.close();
   }
 
   public void testMultiTermDocs() throws IOException {
Index: lucene/src/test/org/apache/lucene/index/TestDoc.java
===================================================================
--- lucene/src/test/org/apache/lucene/index/TestDoc.java	(revision 1238034)
+++ lucene/src/test/org/apache/lucene/index/TestDoc.java	(working copy)
@@ -190,8 +190,8 @@
    private SegmentInfo merge(Directory dir, SegmentInfo si1, SegmentInfo si2, String merged, boolean useCompoundFile)
    throws Exception {
       IOContext context = newIOContext(random);
-      SegmentReader r1 = new SegmentReader(si1, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR, context);
-      SegmentReader r2 = new SegmentReader(si2, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR, context);
+      SegmentReader r1 = new SegmentReader(si1, DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, context);
+      SegmentReader r2 = new SegmentReader(si2, DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, context);
 
       final Codec codec = Codec.getDefault();
       SegmentMerger merger = new SegmentMerger(InfoStream.getDefault(), si1.dir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL, merged, MergeState.CheckAbort.NONE, null, new FieldInfos(new FieldInfos.FieldNumberBiMap()), codec, context);
@@ -218,7 +218,7 @@
 
    private void printSegment(PrintWriter out, SegmentInfo si)
    throws Exception {
-      SegmentReader reader = new SegmentReader(si, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random));
+      SegmentReader reader = new SegmentReader(si, DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random));
 
       for (int i = 0; i < reader.numDocs(); i++)
         out.println(reader.document(i));
Index: lucene/src/test/org/apache/lucene/index/TestDocsAndPositions.java
===================================================================
--- lucene/src/test/org/apache/lucene/index/TestDocsAndPositions.java	(revision 1238034)
+++ lucene/src/test/org/apache/lucene/index/TestDocsAndPositions.java	(working copy)
@@ -25,8 +25,6 @@
 import org.apache.lucene.document.FieldType;
 import org.apache.lucene.document.StringField;
 import org.apache.lucene.document.TextField;
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
-import org.apache.lucene.index.IndexReader.ReaderContext;
 import org.apache.lucene.search.DocIdSetIterator;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.util.Bits;
@@ -66,16 +64,16 @@
     int num = atLeast(13);
     for (int i = 0; i < num; i++) {
       BytesRef bytes = new BytesRef("1");
-      ReaderContext topReaderContext = reader.getTopReaderContext();
+      IndexReaderContext topReaderContext = reader.getTopReaderContext();
       AtomicReaderContext[] leaves = ReaderUtil.leaves(topReaderContext);
       for (AtomicReaderContext atomicReaderContext : leaves) {
         DocsAndPositionsEnum docsAndPosEnum = getDocsAndPositions(
-            atomicReaderContext.reader, bytes, null);
+            atomicReaderContext.reader(), bytes, null);
         assertNotNull(docsAndPosEnum);
-        if (atomicReaderContext.reader.maxDoc() == 0) {
+        if (atomicReaderContext.reader().maxDoc() == 0) {
           continue;
         }
-        final int advance = docsAndPosEnum.advance(random.nextInt(atomicReaderContext.reader.maxDoc()));
+        final int advance = docsAndPosEnum.advance(random.nextInt(atomicReaderContext.reader().maxDoc()));
         do {
           String msg = "Advanced to: " + advance + " current doc: "
               + docsAndPosEnum.docID(); // TODO: + " usePayloads: " + usePayload;
@@ -94,7 +92,7 @@
     directory.close();
   }
 
-  public DocsAndPositionsEnum getDocsAndPositions(IndexReader reader,
+  public DocsAndPositionsEnum getDocsAndPositions(AtomicReader reader,
       BytesRef bytes, Bits liveDocs) throws IOException {
     return reader.termPositionsEnum(null, fieldName, bytes, false);
   }
@@ -142,14 +140,14 @@
     int num = atLeast(13);
     for (int i = 0; i < num; i++) {
       BytesRef bytes = new BytesRef("" + term);
-      ReaderContext topReaderContext = reader.getTopReaderContext();
+      IndexReaderContext topReaderContext = reader.getTopReaderContext();
       AtomicReaderContext[] leaves = ReaderUtil.leaves(topReaderContext);
       for (AtomicReaderContext atomicReaderContext : leaves) {
         DocsAndPositionsEnum docsAndPosEnum = getDocsAndPositions(
-            atomicReaderContext.reader, bytes, null);
+            atomicReaderContext.reader(), bytes, null);
         assertNotNull(docsAndPosEnum);
         int initDoc = 0;
-        int maxDoc = atomicReaderContext.reader.maxDoc();
+        int maxDoc = atomicReaderContext.reader().maxDoc();
         // initially advance or do next doc
         if (random.nextBoolean()) {
           initDoc = docsAndPosEnum.nextDoc();
@@ -218,11 +216,11 @@
     int num = atLeast(13);
     for (int i = 0; i < num; i++) {
       BytesRef bytes = new BytesRef("" + term);
-      ReaderContext topReaderContext = reader.getTopReaderContext();
+      IndexReaderContext topReaderContext = reader.getTopReaderContext();
       AtomicReaderContext[] leaves = ReaderUtil.leaves(topReaderContext);
       for (AtomicReaderContext context : leaves) {
-        int maxDoc = context.reader.maxDoc();
-        DocsEnum docsEnum = _TestUtil.docs(random, context.reader, fieldName, bytes, null, null, true);
+        int maxDoc = context.reader().maxDoc();
+        DocsEnum docsEnum = _TestUtil.docs(random, context.reader(), fieldName, bytes, null, null, true);
         if (findNext(freqInDoc, context.docBase, context.docBase + maxDoc) == Integer.MAX_VALUE) {
           assertNull(docsEnum);
           continue;
@@ -297,15 +295,15 @@
     for (int i = 0; i < num; i++) {
       BytesRef bytes = new BytesRef("even");
 
-      ReaderContext topReaderContext = reader.getTopReaderContext();
+      IndexReaderContext topReaderContext = reader.getTopReaderContext();
       AtomicReaderContext[] leaves = ReaderUtil.leaves(topReaderContext);
       for (AtomicReaderContext atomicReaderContext : leaves) {
         DocsAndPositionsEnum docsAndPosEnum = getDocsAndPositions(
-            atomicReaderContext.reader, bytes, null);
+            atomicReaderContext.reader(), bytes, null);
         assertNotNull(docsAndPosEnum);
 
         int initDoc = 0;
-        int maxDoc = atomicReaderContext.reader.maxDoc();
+        int maxDoc = atomicReaderContext.reader().maxDoc();
         // initially advance or do next doc
         if (random.nextBoolean()) {
           initDoc = docsAndPosEnum.nextDoc();
@@ -331,8 +329,8 @@
     Document doc = new Document();
     doc.add(newField("foo", "bar", StringField.TYPE_UNSTORED));
     writer.addDocument(doc);
-    IndexReader reader = writer.getReader();
-    IndexReader r = getOnlySegmentReader(reader);
+    DirectoryReader reader = writer.getReader();
+    AtomicReader r = getOnlySegmentReader(reader);
     DocsEnum disi = _TestUtil.docs(random, r, "foo", new BytesRef("bar"), null, null, false);
     int docid = disi.docID();
     assertTrue(docid == -1 || docid == DocIdSetIterator.NO_MORE_DOCS);
@@ -356,8 +354,8 @@
     Document doc = new Document();
     doc.add(newField("foo", "bar", TextField.TYPE_UNSTORED));
     writer.addDocument(doc);
-    IndexReader reader = writer.getReader();
-    IndexReader r = getOnlySegmentReader(reader);
+    DirectoryReader reader = writer.getReader();
+    AtomicReader r = getOnlySegmentReader(reader);
     DocsAndPositionsEnum disi = r.termPositionsEnum(null, "foo", new BytesRef("bar"), false);
     int docid = disi.docID();
     assertTrue(docid == -1 || docid == DocIdSetIterator.NO_MORE_DOCS);
Index: lucene/src/test/org/apache/lucene/index/TestDocTermOrds.java
===================================================================
--- lucene/src/test/org/apache/lucene/index/TestDocTermOrds.java	(revision 1238034)
+++ lucene/src/test/org/apache/lucene/index/TestDocTermOrds.java	(working copy)
@@ -66,7 +66,7 @@
     final IndexReader r = w.getReader();
     w.close();
 
-    final DocTermOrds dto = new DocTermOrds(new SlowMultiReaderWrapper(r), "field");
+    final DocTermOrds dto = new DocTermOrds(SlowCompositeReaderWrapper.wrap(r), "field");
 
     TermOrdsIterator iter = dto.lookup(0, null);
     final int[] buffer = new int[5];
@@ -149,7 +149,7 @@
       w.addDocument(doc);
     }
     
-    final IndexReader r = w.getReader();
+    final DirectoryReader r = w.getReader();
     w.close();
 
     if (VERBOSE) {
@@ -160,7 +160,7 @@
       if (VERBOSE) {
         System.out.println("\nTEST: sub=" + subR);
       }
-      verify(subR, idToOrds, termsArray, null);
+      verify((AtomicReader) subR, idToOrds, termsArray, null);
     }
 
     // Also test top-level reader: its enum does not support
@@ -168,9 +168,10 @@
     if (VERBOSE) {
       System.out.println("TEST: top reader");
     }
-    verify(new SlowMultiReaderWrapper(r), idToOrds, termsArray, null);
+    AtomicReader slowR = SlowCompositeReaderWrapper.wrap(r);
+    verify(slowR, idToOrds, termsArray, null);
 
-    FieldCache.DEFAULT.purge(r);
+    FieldCache.DEFAULT.purge(slowR);
 
     r.close();
     dir.close();
@@ -245,13 +246,14 @@
       w.addDocument(doc);
     }
     
-    final IndexReader r = w.getReader();
+    final DirectoryReader r = w.getReader();
     w.close();
 
     if (VERBOSE) {
       System.out.println("TEST: reader=" + r);
     }
     
+    AtomicReader slowR = SlowCompositeReaderWrapper.wrap(r);
     for(String prefix : prefixesArray) {
 
       final BytesRef prefixRef = prefix == null ? null : new BytesRef(prefix);
@@ -277,7 +279,7 @@
         if (VERBOSE) {
           System.out.println("\nTEST: sub=" + subR);
         }
-        verify(subR, idToOrdsPrefix, termsArray, prefixRef);
+        verify((AtomicReader) subR, idToOrdsPrefix, termsArray, prefixRef);
       }
 
       // Also test top-level reader: its enum does not support
@@ -285,16 +287,16 @@
       if (VERBOSE) {
         System.out.println("TEST: top reader");
       }
-      verify(new SlowMultiReaderWrapper(r), idToOrdsPrefix, termsArray, prefixRef);
+      verify(slowR, idToOrdsPrefix, termsArray, prefixRef);
     }
 
-    FieldCache.DEFAULT.purge(r);
+    FieldCache.DEFAULT.purge(slowR);
 
     r.close();
     dir.close();
   }
 
-  private void verify(IndexReader r, int[][] idToOrds, BytesRef[] termsArray, BytesRef prefixRef) throws Exception {
+  private void verify(AtomicReader r, int[][] idToOrds, BytesRef[] termsArray, BytesRef prefixRef) throws Exception {
 
     final DocTermOrds dto = new DocTermOrds(r,
                                             "field",
Index: lucene/src/test/org/apache/lucene/index/TestDocumentWriter.java
===================================================================
--- lucene/src/test/org/apache/lucene/index/TestDocumentWriter.java	(revision 1238034)
+++ lucene/src/test/org/apache/lucene/index/TestDocumentWriter.java	(working copy)
@@ -64,7 +64,7 @@
     SegmentInfo info = writer.newestSegment();
     writer.close();
     //After adding the document, we should be able to read it back in
-    SegmentReader reader = new SegmentReader(info, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random));
+    SegmentReader reader = new SegmentReader(info, DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random));
     assertTrue(reader != null);
     Document doc = reader.document(0);
     assertTrue(doc != null);
@@ -125,7 +125,7 @@
     writer.commit();
     SegmentInfo info = writer.newestSegment();
     writer.close();
-    SegmentReader reader = new SegmentReader(info, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random));
+    SegmentReader reader = new SegmentReader(info, DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random));
 
     DocsAndPositionsEnum termPositions = MultiFields.getTermPositionsEnum(reader, MultiFields.getLiveDocs(reader),
                                                                           "repeated", new BytesRef("repeated"), false);
@@ -197,7 +197,7 @@
     writer.commit();
     SegmentInfo info = writer.newestSegment();
     writer.close();
-    SegmentReader reader = new SegmentReader(info, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random));
+    SegmentReader reader = new SegmentReader(info, DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random));
 
     DocsAndPositionsEnum termPositions = MultiFields.getTermPositionsEnum(reader, reader.getLiveDocs(), "f1", new BytesRef("a"), false);
     assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);
@@ -241,7 +241,7 @@
     writer.commit();
     SegmentInfo info = writer.newestSegment();
     writer.close();
-    SegmentReader reader = new SegmentReader(info, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random));
+    SegmentReader reader = new SegmentReader(info, DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random));
 
     DocsAndPositionsEnum termPositions = reader.termPositionsEnum(reader.getLiveDocs(), "preanalyzed", new BytesRef("term1"), false);
     assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);
Index: lucene/src/test/org/apache/lucene/index/TestDocValuesIndexing.java
===================================================================
--- lucene/src/test/org/apache/lucene/index/TestDocValuesIndexing.java	(revision 1238034)
+++ lucene/src/test/org/apache/lucene/index/TestDocValuesIndexing.java	(working copy)
@@ -82,7 +82,7 @@
 
     writer.close(true);
 
-    IndexReader reader = IndexReader.open(dir, 1);
+    DirectoryReader reader = DirectoryReader.open(dir, 1);
     assertEquals(1, reader.getSequentialSubReaders().length);
 
     IndexSearcher searcher = new IndexSearcher(reader);
@@ -694,9 +694,9 @@
     doc.add(f);
     w.addDocument(doc);
     w.forceMerge(1);
-    IndexReader r = w.getReader();
+    DirectoryReader r = w.getReader();
     w.close();
-    assertEquals(17, r.getSequentialSubReaders()[0].docValues("field").load().getInt(0));
+    assertEquals(17, ((AtomicReader) r.getSequentialSubReaders()[0]).docValues("field").load().getInt(0));
     r.close();
     d.close();
   }
@@ -721,9 +721,9 @@
     doc.add(f);
     w.addDocument(doc);
     w.forceMerge(1);
-    IndexReader r = w.getReader();
+    DirectoryReader r = w.getReader();
     w.close();
-    assertEquals(17, r.getSequentialSubReaders()[0].docValues("field").load().getInt(0));
+    assertEquals(17, getOnlySegmentReader(r).docValues("field").load().getInt(0));
     r.close();
     d.close();
   }
@@ -795,11 +795,11 @@
         int ord = asSortedSource.getByValue(expected, actual);
         assertEquals(i, ord);
       }
-      reader = new SlowMultiReaderWrapper(reader);
+      AtomicReader slowR = SlowCompositeReaderWrapper.wrap(reader);
       Set<Entry<String, String>> entrySet = docToString.entrySet();
 
       for (Entry<String, String> entry : entrySet) {
-        int docId = docId(reader, new Term("id", entry.getKey()));
+        int docId = docId(slowR, new Term("id", entry.getKey()));
         expected.copyChars(entry.getValue());
         assertEquals(expected, asSortedSource.getBytes(docId, actual));
       }
@@ -810,7 +810,7 @@
     }
   }
   
-  public int docId(IndexReader reader, Term term) throws IOException {
+  public int docId(AtomicReader reader, Term term) throws IOException {
     int docFreq = reader.docFreq(term);
     assertEquals(1, docFreq);
     DocsEnum termDocsEnum = reader.termDocsEnum(null, term.field, term.bytes, false);
Index: lucene/src/test/org/apache/lucene/index/TestDuelingCodecs.java
===================================================================
--- lucene/src/test/org/apache/lucene/index/TestDuelingCodecs.java	(revision 1238034)
+++ lucene/src/test/org/apache/lucene/index/TestDuelingCodecs.java	(working copy)
@@ -151,10 +151,6 @@
     assertEquals(info, leftReader.numDocs(), rightReader.numDocs());
     assertEquals(info, leftReader.numDeletedDocs(), rightReader.numDeletedDocs());
     assertEquals(info, leftReader.hasDeletions(), rightReader.hasDeletions());
-    
-    if (leftReader.getUniqueTermCount() != -1 && rightReader.getUniqueTermCount() != -1) {
-      assertEquals(info, leftReader.getUniqueTermCount(), rightReader.getUniqueTermCount());
-    }
   }
   
   /** 
@@ -462,11 +458,13 @@
     FieldsEnum fieldsEnum = leftFields.iterator();
     String field;
     while ((field = fieldsEnum.next()) != null) {
-      assertEquals(info, leftReader.hasNorms(field), rightReader.hasNorms(field));
-      if (leftReader.hasNorms(field)) {
-        DocValues leftNorms = MultiDocValues.getNormDocValues(leftReader, field);
-        DocValues rightNorms = MultiDocValues.getNormDocValues(rightReader, field);
+      DocValues leftNorms = MultiDocValues.getNormDocValues(leftReader, field);
+      DocValues rightNorms = MultiDocValues.getNormDocValues(rightReader, field);
+      if (leftNorms != null && rightNorms != null) {
         assertDocValues(leftNorms, rightNorms);
+      } else {
+        assertNull(leftNorms);
+        assertNull(rightNorms);
       }
     }
   }
@@ -519,7 +517,7 @@
 
   private static Set<String> getDVFields(IndexReader reader) {
     Set<String> fields = new HashSet<String>();
-    for(FieldInfo fi : ReaderUtil.getMergedFieldInfos(reader)) {
+    for(FieldInfo fi : MultiFields.getMergedFieldInfos(reader)) {
       if (fi.hasDocValues()) {
         fields.add(fi.name);
       }
@@ -539,7 +537,12 @@
     for (String field : leftValues) {
       DocValues leftDocValues = MultiDocValues.getDocValues(leftReader, field);
       DocValues rightDocValues = MultiDocValues.getDocValues(rightReader, field);
-      assertDocValues(leftDocValues, rightDocValues);
+      if (leftDocValues != null && rightDocValues != null) {
+        assertDocValues(leftDocValues, rightDocValues);
+      } else {
+        assertNull(leftDocValues);
+        assertNull(rightDocValues);
+      }
     }
   }
   
Index: lucene/src/test/org/apache/lucene/index/TestFieldsReader.java
===================================================================
--- lucene/src/test/org/apache/lucene/index/TestFieldsReader.java	(revision 1238034)
+++ lucene/src/test/org/apache/lucene/index/TestFieldsReader.java	(working copy)
@@ -272,13 +272,13 @@
       doc.add(new NumericField("id", id, ft));
       w.addDocument(doc);
     }
-    final IndexReader r = w.getReader();
+    final DirectoryReader r = w.getReader();
     w.close();
     
     assertEquals(numDocs, r.numDocs());
 
     for(IndexReader sub : r.getSequentialSubReaders()) {
-      final int[] ids = FieldCache.DEFAULT.getInts(sub, "id", false);
+      final int[] ids = FieldCache.DEFAULT.getInts((AtomicReader) sub, "id", false);
       for(int docID=0;docID<sub.numDocs();docID++) {
         final Document doc = sub.document(docID);
         final Field f = (Field) doc.getField("nf");
Index: lucene/src/test/org/apache/lucene/index/TestFilterIndexReader.java
===================================================================
--- lucene/src/test/org/apache/lucene/index/TestFilterIndexReader.java	(revision 1238034)
+++ lucene/src/test/org/apache/lucene/index/TestFilterIndexReader.java	(working copy)
@@ -114,19 +114,14 @@
       }
     }
     
-    public TestReader(IndexReader reader) {
-      super(new SlowMultiReaderWrapper(reader));
+    public TestReader(IndexReader reader) throws IOException {
+      super(SlowCompositeReaderWrapper.wrap(reader));
     }
 
     @Override
     public Fields fields() throws IOException {
       return new TestFields(super.fields());
     }
-
-    @Override
-    public FieldInfos getFieldInfos() {
-      return ReaderUtil.getMergedFieldInfos(in);
-    }
   }
     
   /**
@@ -183,23 +178,17 @@
   }
 
   public void testOverrideMethods() throws Exception {
-    HashSet<String> methodsThatShouldNotBeOverridden = new HashSet<String>();
-    methodsThatShouldNotBeOverridden.add("doOpenIfChanged");
-    methodsThatShouldNotBeOverridden.add("clone");
     boolean fail = false;
     for (Method m : FilterIndexReader.class.getMethods()) {
       int mods = m.getModifiers();
-      if (Modifier.isStatic(mods) || Modifier.isFinal(mods)) {
+      if (Modifier.isStatic(mods) || Modifier.isFinal(mods) || m.isSynthetic()) {
         continue;
       }
-      Class< ? > declaringClass = m.getDeclaringClass();
+      Class<?> declaringClass = m.getDeclaringClass();
       String name = m.getName();
-      if (declaringClass != FilterIndexReader.class && declaringClass != Object.class && !methodsThatShouldNotBeOverridden.contains(name)) {
+      if (declaringClass != FilterIndexReader.class && declaringClass != Object.class) {
         System.err.println("method is not overridden by FilterIndexReader: " + name);
         fail = true;
-      } else if (declaringClass == FilterIndexReader.class && methodsThatShouldNotBeOverridden.contains(name)) {
-        System.err.println("method should not be overridden by FilterIndexReader: " + name);
-        fail = true;
       }
     }
     assertFalse("FilterIndexReader overrides (or not) some problematic methods; see log above", fail);
Index: lucene/src/test/org/apache/lucene/index/TestFlex.java
===================================================================
--- lucene/src/test/org/apache/lucene/index/TestFlex.java	(revision 1238034)
+++ lucene/src/test/org/apache/lucene/index/TestFlex.java	(working copy)
@@ -69,8 +69,9 @@
     Document doc = new Document();
     doc.add(newField("f", "a b c", TextField.TYPE_UNSTORED));
     w.addDocument(doc);
-    IndexReader r = w.getReader();
-    TermsEnum terms = r.getSequentialSubReaders()[0].fields().terms("f").iterator(null);
+    w.forceMerge(1);
+    DirectoryReader r = w.getReader();
+    TermsEnum terms = getOnlySegmentReader(r).fields().terms("f").iterator(null);
     assertTrue(terms.next() != null);
     try {
       assertEquals(0, terms.ord());
Index: lucene/src/test/org/apache/lucene/index/TestIndexReader.java
===================================================================
--- lucene/src/test/org/apache/lucene/index/TestIndexReader.java	(revision 1238034)
+++ lucene/src/test/org/apache/lucene/index/TestIndexReader.java	(working copy)
@@ -58,7 +58,7 @@
       addDocumentWithFields(writer);
       writer.close();
       // set up reader:
-      IndexReader reader = IndexReader.open(d);
+      DirectoryReader reader = DirectoryReader.open(d);
       assertTrue(reader.isCurrent());
       // modify index by adding another document:
       writer = new IndexWriter(d, newIndexWriterConfig(TEST_VERSION_CURRENT,
@@ -101,8 +101,8 @@
 
         writer.close();
         // set up reader
-        IndexReader reader = IndexReader.open(d);
-        FieldInfos fieldInfos = ReaderUtil.getMergedFieldInfos(reader);
+        DirectoryReader reader = DirectoryReader.open(d);
+        FieldInfos fieldInfos = MultiFields.getMergedFieldInfos(reader);
         assertNotNull(fieldInfos.fieldInfo("keyword"));
         assertNotNull(fieldInfos.fieldInfo("text"));
         assertNotNull(fieldInfos.fieldInfo("unindexed"));
@@ -162,8 +162,8 @@
         writer.close();
 
         // verify fields again
-        reader = IndexReader.open(d);
-        fieldInfos = ReaderUtil.getMergedFieldInfos(reader);
+        reader = DirectoryReader.open(d);
+        fieldInfos = MultiFields.getMergedFieldInfos(reader);
 
         Collection<String> allFieldNames = new HashSet<String>();
         Collection<String> indexedFieldNames = new HashSet<String>();
@@ -301,7 +301,7 @@
         doc.add(new TextField("junk", "junk text"));
         writer.addDocument(doc);
         writer.close();
-        IndexReader reader = IndexReader.open(dir);
+        DirectoryReader reader = DirectoryReader.open(dir);
         Document doc2 = reader.document(reader.maxDoc() - 1);
         IndexableField[] fields = doc2.getFields("bin1");
         assertNotNull(fields);
@@ -320,7 +320,7 @@
         writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setOpenMode(OpenMode.APPEND).setMergePolicy(newLogMergePolicy()));
         writer.forceMerge(1);
         writer.close();
-        reader = IndexReader.open(dir);
+        reader = DirectoryReader.open(dir);
         doc2 = reader.document(reader.maxDoc() - 1);
         fields = doc2.getFields("bin1");
         assertNotNull(fields);
@@ -343,8 +343,8 @@
         fileDirName.mkdir();
       }
       try {
-        IndexReader.open(fileDirName);
-        fail("opening IndexReader on empty directory failed to produce FileNotFoundException");
+        DirectoryReader.open(fileDirName);
+        fail("opening DirectoryReader on empty directory failed to produce FileNotFoundException");
       } catch (FileNotFoundException e) {
         // GOOD
       }
@@ -372,7 +372,7 @@
 
         // Now open existing directory and test that reader closes all files
         dir = newFSDirectory(dirFile);
-        IndexReader reader1 = IndexReader.open(dir);
+        DirectoryReader reader1 = DirectoryReader.open(dir);
         reader1.close();
         dir.close();
 
@@ -385,7 +385,7 @@
       File dirFile = _TestUtil.getTempDir("deletetest");
       Directory dir = newFSDirectory(dirFile);
       try {
-        IndexReader.open(dir);
+        DirectoryReader.open(dir);
         fail("expected FileNotFoundException");
       } catch (FileNotFoundException e) {
         // expected
@@ -395,7 +395,7 @@
 
       // Make sure we still get a CorruptIndexException (not NPE):
       try {
-        IndexReader.open(dir);
+        DirectoryReader.open(dir);
         fail("expected FileNotFoundException");
       } catch (FileNotFoundException e) {
         // expected
@@ -461,17 +461,15 @@
     }
 
     // TODO: maybe this can reuse the logic of test dueling codecs?
-    public static void assertIndexEquals(IndexReader index1, IndexReader index2) throws IOException {
+    public static void assertIndexEquals(DirectoryReader index1, DirectoryReader index2) throws IOException {
       assertEquals("IndexReaders have different values for numDocs.", index1.numDocs(), index2.numDocs());
       assertEquals("IndexReaders have different values for maxDoc.", index1.maxDoc(), index2.maxDoc());
       assertEquals("Only one IndexReader has deletions.", index1.hasDeletions(), index2.hasDeletions());
-      if (!(index1 instanceof ParallelReader)) {
-        assertEquals("Single segment test differs.", index1.getSequentialSubReaders().length == 1, index2.getSequentialSubReaders().length == 1);
-      }
+      assertEquals("Single segment test differs.", index1.getSequentialSubReaders().length == 1, index2.getSequentialSubReaders().length == 1);
       
       // check field names
-      FieldInfos fieldInfos1 = ReaderUtil.getMergedFieldInfos(index1);
-      FieldInfos fieldInfos2 = ReaderUtil.getMergedFieldInfos(index2);
+      FieldInfos fieldInfos1 = MultiFields.getMergedFieldInfos(index1);
+      FieldInfos fieldInfos2 = MultiFields.getMergedFieldInfos(index2);
       assertEquals("IndexReaders have different numbers of fields.", fieldInfos1.size(), fieldInfos2.size());
       final int numFields = fieldInfos1.size();
       for(int fieldID=0;fieldID<numFields;fieldID++) {
@@ -581,7 +579,7 @@
 
       SegmentInfos sis = new SegmentInfos();
       sis.read(d);
-      IndexReader r = IndexReader.open(d);
+      DirectoryReader r = DirectoryReader.open(d);
       IndexCommit c = r.getIndexCommit();
 
       assertEquals(sis.getCurrentSegmentFileName(), c.getSegmentsFileName());
@@ -600,7 +598,7 @@
         addDocumentWithFields(writer);
       writer.close();
 
-      IndexReader r2 = IndexReader.openIfChanged(r);
+      DirectoryReader r2 = DirectoryReader.openIfChanged(r);
       assertNotNull(r2);
       assertFalse(c.equals(r2.getIndexCommit()));
       assertFalse(r2.getIndexCommit().getSegmentCount() == 1);
@@ -612,9 +610,9 @@
       writer.forceMerge(1);
       writer.close();
 
-      r2 = IndexReader.openIfChanged(r);
+      r2 = DirectoryReader.openIfChanged(r);
       assertNotNull(r2);
-      assertNull(IndexReader.openIfChanged(r2));
+      assertNull(DirectoryReader.openIfChanged(r2));
       assertEquals(1, r2.getIndexCommit().getSegmentCount());
 
       r.close();
@@ -633,12 +631,12 @@
   }
 
   // LUCENE-1468 -- make sure on attempting to open an
-  // IndexReader on a non-existent directory, you get a
+  // DirectoryReader on a non-existent directory, you get a
   // good exception
   public void testNoDir() throws Throwable {
     Directory dir = newFSDirectory(_TestUtil.getTempDir("doesnotexist"));
     try {
-      IndexReader.open(dir);
+      DirectoryReader.open(dir);
       fail("did not hit expected exception");
     } catch (NoSuchDirectoryException nsde) {
       // expected
@@ -659,7 +657,7 @@
     writer.addDocument(createDocument("a"));
     writer.close();
     
-    Collection<IndexCommit> commits = IndexReader.listCommits(dir);
+    Collection<IndexCommit> commits = DirectoryReader.listCommits(dir);
     for (final IndexCommit commit : commits) {
       Collection<String> files = commit.getFileNames();
       HashSet<String> seen = new HashSet<String>();
@@ -688,8 +686,8 @@
     writer.commit();
 
     // Open reader1
-    IndexReader r = IndexReader.open(dir);
-    IndexReader r1 = getOnlySegmentReader(r);
+    DirectoryReader r = DirectoryReader.open(dir);
+    AtomicReader r1 = getOnlySegmentReader(r);
     final int[] ints = FieldCache.DEFAULT.getInts(r1, "number", false);
     assertEquals(1, ints.length);
     assertEquals(17, ints[0]);
@@ -699,10 +697,10 @@
     writer.commit();
 
     // Reopen reader1 --> reader2
-    IndexReader r2 = IndexReader.openIfChanged(r);
+    DirectoryReader r2 = DirectoryReader.openIfChanged(r);
     assertNotNull(r2);
     r.close();
-    IndexReader sub0 = r2.getSequentialSubReaders()[0];
+    AtomicReader sub0 = (AtomicReader) r2.getSequentialSubReaders()[0];
     final int[] ints2 = FieldCache.DEFAULT.getInts(sub0, "number", false);
     r2.close();
     assertTrue(ints == ints2);
@@ -722,19 +720,18 @@
     writer.addDocument(doc);
     writer.commit();
 
-    IndexReader r = IndexReader.open(dir);
-    IndexReader r1 = getOnlySegmentReader(r);
+    DirectoryReader r = DirectoryReader.open(dir);
+    AtomicReader r1 = getOnlySegmentReader(r);
     assertEquals(36, r1.getUniqueTermCount());
     writer.addDocument(doc);
     writer.commit();
-    IndexReader r2 = IndexReader.openIfChanged(r);
+    DirectoryReader r2 = DirectoryReader.openIfChanged(r);
     assertNotNull(r2);
     r.close();
-    assertEquals(-1, r2.getUniqueTermCount());
 
     IndexReader[] subs = r2.getSequentialSubReaders();
     for(int i=0;i<subs.length;i++) {
-      assertEquals(36, subs[i].getUniqueTermCount());
+      assertEquals(36, ((AtomicReader) subs[i]).getUniqueTermCount());
     }
     r2.close();
     writer.close();
@@ -752,7 +749,7 @@
     writer.addDocument(doc);
     writer.close();
 
-    IndexReader r = IndexReader.open(dir, -1);
+    DirectoryReader r = DirectoryReader.open(dir, -1);
     try {
       r.docFreq(new Term("field", "f"));
       fail("did not hit expected exception");
@@ -771,9 +768,9 @@
     writer.close();
 
     // LUCENE-1718: ensure re-open carries over no terms index:
-    IndexReader r2 = IndexReader.openIfChanged(r);
+    DirectoryReader r2 = DirectoryReader.openIfChanged(r);
     assertNotNull(r2);
-    assertNull(IndexReader.openIfChanged(r2));
+    assertNull(DirectoryReader.openIfChanged(r2));
     r.close();
     IndexReader[] subReaders = r2.getSequentialSubReaders();
     assertEquals(2, subReaders.length);
@@ -797,12 +794,12 @@
     writer.commit();
     Document doc = new Document();
     writer.addDocument(doc);
-    IndexReader r = IndexReader.open(dir);
+    DirectoryReader r = DirectoryReader.open(dir);
     assertTrue(r.isCurrent());
     writer.addDocument(doc);
     writer.prepareCommit();
     assertTrue(r.isCurrent());
-    IndexReader r2 = IndexReader.openIfChanged(r);
+    DirectoryReader r2 = DirectoryReader.openIfChanged(r);
     assertNull(r2);
     writer.commit();
     assertFalse(r.isCurrent());
@@ -828,7 +825,7 @@
     sdp.snapshot("c3");
     writer.close();
     long currentGen = 0;
-    for (IndexCommit ic : IndexReader.listCommits(dir)) {
+    for (IndexCommit ic : DirectoryReader.listCommits(dir)) {
       assertTrue("currentGen=" + currentGen + " commitGen=" + ic.getGeneration(), currentGen < ic.getGeneration());
       currentGen = ic.getGeneration();
     }
@@ -841,9 +838,9 @@
     IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)));
     writer.addDocument(new Document());
     writer.prepareCommit();
-    assertFalse(IndexReader.indexExists(dir));
+    assertFalse(DirectoryReader.indexExists(dir));
     writer.close();
-    assertTrue(IndexReader.indexExists(dir));
+    assertTrue(DirectoryReader.indexExists(dir));
     dir.close();
   }
 
@@ -855,7 +852,7 @@
     Document d = new Document();
     d.add(newField("f", "a a b", TextField.TYPE_UNSTORED));
     writer.addDocument(d);
-    IndexReader r = writer.getReader();
+    DirectoryReader r = writer.getReader();
     writer.close();
     try {
       // Make sure codec impls totalTermFreq (eg PreFlex doesn't)
@@ -878,7 +875,7 @@
     writer.commit();
     writer.addDocument(new Document());
     writer.commit();
-    final IndexReader reader = writer.getReader();
+    final DirectoryReader reader = writer.getReader();
     final int[] closeCount = new int[1];
     final IndexReader.ReaderClosedListener listener = new IndexReader.ReaderClosedListener() {
       public void onClose(IndexReader reader) {
@@ -894,7 +891,7 @@
     assertEquals(1, closeCount[0]);
     writer.close();
 
-    IndexReader reader2 = IndexReader.open(dir);
+    DirectoryReader reader2 = DirectoryReader.open(dir);
     reader2.addReaderClosedListener(listener);
 
     closeCount[0] = 0;
@@ -907,7 +904,7 @@
     Directory dir = newDirectory();
     IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)));
     writer.addDocument(new Document());
-    IndexReader r = writer.getReader();
+    DirectoryReader r = writer.getReader();
     writer.close();
     r.document(0);
     try {
@@ -925,7 +922,7 @@
     IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)));
     writer.addDocument(new Document());
     writer.commit();
-    IndexReader r = IndexReader.open(dir);
+    DirectoryReader r = DirectoryReader.open(dir);
     assertTrue(r.tryIncRef());
     r.decRef();
     r.close();
@@ -939,7 +936,7 @@
     IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)));
     writer.addDocument(new Document());
     writer.commit();
-    IndexReader r = IndexReader.open(dir);
+    DirectoryReader r = DirectoryReader.open(dir);
     int numThreads = atLeast(2);
     
     IncThread[] threads = new IncThread[numThreads];
@@ -993,7 +990,7 @@
     doc.add(newField("field1", "foobar", StringField.TYPE_STORED));
     doc.add(newField("field2", "foobaz", StringField.TYPE_STORED));
     writer.addDocument(doc);
-    IndexReader r = writer.getReader();
+    DirectoryReader r = writer.getReader();
     writer.close();
     Set<String> fieldsToLoad = new HashSet<String>();
     assertEquals(0, r.document(0, fieldsToLoad).getFields().size());
Index: lucene/src/test/org/apache/lucene/index/TestIndexReaderReopen.java
===================================================================
--- lucene/src/test/org/apache/lucene/index/TestIndexReaderReopen.java	(revision 1238034)
+++ lucene/src/test/org/apache/lucene/index/TestIndexReaderReopen.java	(working copy)
@@ -60,8 +60,8 @@
       }
 
       @Override
-      protected IndexReader openReader() throws IOException {
-        return IndexReader.open(dir1);
+      protected DirectoryReader openReader() throws IOException {
+        return DirectoryReader.open(dir1);
       }
       
     });
@@ -78,69 +78,14 @@
       }
 
       @Override
-      protected IndexReader openReader() throws IOException {
-        return IndexReader.open(dir2);
+      protected DirectoryReader openReader() throws IOException {
+        return DirectoryReader.open(dir2);
       }
       
     });
     dir2.close();
   }
   
-  public void testParallelReaderReopen() throws Exception {
-    final Directory dir1 = newDirectory();
-    createIndex(random, dir1, true);
-    final Directory dir2 = newDirectory();
-    createIndex(random, dir2, true);
-    
-    performDefaultTests(new TestReopen() {
-
-      @Override
-      protected void modifyIndex(int i) throws IOException {
-        TestIndexReaderReopen.modifyIndex(i, dir1);
-        TestIndexReaderReopen.modifyIndex(i, dir2);
-      }
-
-      @Override
-      protected IndexReader openReader() throws IOException {
-        ParallelReader pr = new ParallelReader();
-        pr.add(IndexReader.open(dir1));
-        pr.add(IndexReader.open(dir2));
-        return pr;
-      }
-      
-    });
-    dir1.close();
-    dir2.close();
-    
-    final Directory dir3 = newDirectory();
-    createIndex(random, dir3, true);
-    final Directory dir4 = newDirectory();
-    createIndex(random, dir4, true);
-
-    performTestsWithExceptionInReopen(new TestReopen() {
-
-      @Override
-      protected void modifyIndex(int i) throws IOException {
-        TestIndexReaderReopen.modifyIndex(i, dir3);
-        TestIndexReaderReopen.modifyIndex(i, dir4);
-      }
-
-      @Override
-      protected IndexReader openReader() throws IOException {
-        ParallelReader pr = new ParallelReader();
-        pr.add(IndexReader.open(dir3));
-        pr.add(IndexReader.open(dir4));
-        // Does not implement reopen, so
-        // hits exception:
-        pr.add(new FilterIndexReader(IndexReader.open(dir3)));
-        return pr;
-      }
-      
-    });
-    dir3.close();
-    dir4.close();
-  }
-
   // LUCENE-1228: IndexWriter.commit() does not update the index version
   // populate an index in iterations.
   // at the end of every iteration, commit the index and reopen/recreate the reader.
@@ -162,7 +107,7 @@
         TEST_VERSION_CURRENT, new MockAnalyzer(random)).setOpenMode(
                                                               OpenMode.CREATE).setMergeScheduler(new SerialMergeScheduler()).setMergePolicy(newLogMergePolicy()));
     iwriter.commit();
-    IndexReader reader = IndexReader.open(dir);
+    DirectoryReader reader = DirectoryReader.open(dir);
     try {
       int M = 3;
       FieldType customType = new FieldType(TextField.TYPE_STORED);
@@ -191,7 +136,7 @@
         iwriter.commit();
         if (withReopen) {
           // reopen
-          IndexReader r2 = IndexReader.openIfChanged(reader);
+          DirectoryReader r2 = DirectoryReader.openIfChanged(reader);
           if (r2 != null) {
             reader.close();
             reader = r2;
@@ -199,7 +144,7 @@
         } else {
           // recreate
           reader.close();
-          reader = IndexReader.open(dir);
+          reader = DirectoryReader.open(dir);
         }
       }
     } finally {
@@ -207,99 +152,11 @@
       reader.close();
     }
   }
-  
-  public void testMultiReaderReopen() throws Exception {
-    final Directory dir1 = newDirectory();
-    createIndex(random, dir1, true);
-
-    final Directory dir2 = newDirectory();
-    createIndex(random, dir2, true);
-
-    performDefaultTests(new TestReopen() {
-
-      @Override
-      protected void modifyIndex(int i) throws IOException {
-        TestIndexReaderReopen.modifyIndex(i, dir1);
-        TestIndexReaderReopen.modifyIndex(i, dir2);
-      }
-
-      @Override
-      protected IndexReader openReader() throws IOException {
-        return new MultiReader(IndexReader.open(dir1),
-            IndexReader.open(dir2));
-      }
-      
-    });
-
-    dir1.close();
-    dir2.close();
     
-    final Directory dir3 = newDirectory();
-    createIndex(random, dir3, true);
-
-    final Directory dir4 = newDirectory();
-    createIndex(random, dir4, true);
-
-    performTestsWithExceptionInReopen(new TestReopen() {
-
-      @Override
-      protected void modifyIndex(int i) throws IOException {
-        TestIndexReaderReopen.modifyIndex(i, dir3);
-        TestIndexReaderReopen.modifyIndex(i, dir4);
-      }
-
-      @Override
-      protected IndexReader openReader() throws IOException {
-        return new MultiReader(IndexReader.open(dir3),
-            IndexReader.open(dir4),
-            // Does not implement reopen, so
-            // hits exception:
-            new FilterIndexReader(IndexReader.open(dir3)));
-      }
-      
-    });
-    dir3.close();
-    dir4.close();
-  }
-
-  public void testMixedReaders() throws Exception {
-    final Directory dir1 = newDirectory();
-    createIndex(random, dir1, true);
-    final Directory dir2 = newDirectory();
-    createIndex(random, dir2, true);
-    final Directory dir3 = newDirectory();
-    createIndex(random, dir3, false);
-    final Directory dir4 = newDirectory();
-    createIndex(random, dir4, true);
-    final Directory dir5 = newDirectory();
-    createIndex(random, dir5, false);
-    
-    performDefaultTests(new TestReopen() {
-
-      @Override
-      protected void modifyIndex(int i) throws IOException {
-        TestIndexReaderReopen.modifyIndex(i, dir4);
-        TestIndexReaderReopen.modifyIndex(i, dir5);
-      }
-
-      @Override
-      protected IndexReader openReader() throws IOException {
-        MultiReader mr1 = new MultiReader(IndexReader.open(dir1), IndexReader.open(dir2));
-        MultiReader mr2 = new MultiReader(IndexReader.open(dir3), IndexReader.open(dir4));
-        return new MultiReader(mr1, mr2, IndexReader.open(dir5));
-      }
-    });
-    dir1.close();
-    dir2.close();
-    dir3.close();
-    dir4.close();
-    dir5.close();
-  }  
-  
   private void performDefaultTests(TestReopen test) throws Exception {
 
-    IndexReader index1 = test.openReader();
-    IndexReader index2 = test.openReader();
+    DirectoryReader index1 = test.openReader();
+    DirectoryReader index2 = test.openReader();
         
     TestIndexReader.assertIndexEquals(index1, index2);
 
@@ -312,7 +169,7 @@
     index1.close();
     index1 = couple.newReader;
 
-    IndexReader index2_refreshed = couple.refreshedReader;
+    DirectoryReader index2_refreshed = couple.refreshedReader;
     index2.close();
     
     // test if refreshed reader and newly opened reader return equal results
@@ -328,7 +185,7 @@
       
       index1.close();
       couple = refreshReader(index2, test, i, true);
-      // refresh IndexReader
+      // refresh DirectoryReader
       index2.close();
       
       index2 = couple.refreshedReader;
@@ -341,85 +198,10 @@
     assertReaderClosed(index1, true, true);
     assertReaderClosed(index2, true, true);
   }
-
-  public void testReferenceCountingMultiReader() throws IOException {
-    for (int mode = 0; mode <=1; mode++) {
-      Directory dir1 = newDirectory();
-      createIndex(random, dir1, false);
-      Directory dir2 = newDirectory();
-      createIndex(random, dir2, true);
-      
-      IndexReader reader1 = IndexReader.open(dir1);
-      assertRefCountEquals(1, reader1);
-
-      IndexReader initReader2 = IndexReader.open(dir2);
-      IndexReader multiReader1 = new MultiReader(new IndexReader[] {reader1, initReader2}, (mode == 0));
-      modifyIndex(0, dir2);
-      assertRefCountEquals(1 + mode, reader1);
-      
-      IndexReader multiReader2 = IndexReader.openIfChanged(multiReader1);
-      assertNotNull(multiReader2);
-      // index1 hasn't changed, so multiReader2 should share reader1 now with multiReader1
-      assertRefCountEquals(2 + mode, reader1);
-      
-      modifyIndex(0, dir1);
-      IndexReader reader2 = IndexReader.openIfChanged(reader1);
-      assertNotNull(reader2);
-      assertNull(IndexReader.openIfChanged(reader2));
-      assertRefCountEquals(2 + mode, reader1);
-
-      if (mode == 1) {
-        initReader2.close();
-      }
-      
-      modifyIndex(1, dir1);
-      IndexReader reader3 = IndexReader.openIfChanged(reader2);
-      assertNotNull(reader3);
-      assertRefCountEquals(2 + mode, reader1);
-      assertRefCountEquals(1, reader2);
-      
-      multiReader1.close();
-      assertRefCountEquals(1 + mode, reader1);
-      
-      multiReader1.close();
-      assertRefCountEquals(1 + mode, reader1);
-
-      if (mode == 1) {
-        initReader2.close();
-      }
-      
-      reader1.close();
-      assertRefCountEquals(1, reader1);
-      
-      multiReader2.close();
-      assertRefCountEquals(0, reader1);
-      
-      multiReader2.close();
-      assertRefCountEquals(0, reader1);
-      
-      reader3.close();
-      assertRefCountEquals(0, reader1);
-      assertReaderClosed(reader1, true, false);
-      
-      reader2.close();
-      assertRefCountEquals(0, reader1);
-      assertReaderClosed(reader1, true, false);
-      
-      reader2.close();
-      assertRefCountEquals(0, reader1);
-      
-      reader3.close();
-      assertRefCountEquals(0, reader1);
-      assertReaderClosed(reader1, true, true);
-      dir1.close();
-      dir2.close();
-    }
-
-  }
   
   private void performTestsWithExceptionInReopen(TestReopen test) throws Exception {
-    IndexReader index1 = test.openReader();
-    IndexReader index2 = test.openReader();
+    DirectoryReader index1 = test.openReader();
+    DirectoryReader index2 = test.openReader();
 
     TestIndexReader.assertIndexEquals(index1, index2);
     
@@ -459,28 +241,28 @@
       }
 
       @Override
-      protected IndexReader openReader() throws IOException {
-        return IndexReader.open(dir);
+      protected DirectoryReader openReader() throws IOException {
+        return DirectoryReader.open(dir);
       }      
     };
     
     final List<ReaderCouple> readers = Collections.synchronizedList(new ArrayList<ReaderCouple>());
-    IndexReader firstReader = IndexReader.open(dir);
-    IndexReader reader = firstReader;
+    DirectoryReader firstReader = DirectoryReader.open(dir);
+    DirectoryReader reader = firstReader;
     final Random rnd = random;
     
     ReaderThread[] threads = new ReaderThread[n];
-    final Set<IndexReader> readersToClose = Collections.synchronizedSet(new HashSet<IndexReader>());
+    final Set<DirectoryReader> readersToClose = Collections.synchronizedSet(new HashSet<DirectoryReader>());
     
     for (int i = 0; i < n; i++) {
       if (i % 2 == 0) {
-        IndexReader refreshed = IndexReader.openIfChanged(reader);
+        DirectoryReader refreshed = DirectoryReader.openIfChanged(reader);
         if (refreshed != null) {
           readersToClose.add(reader);
           reader = refreshed;
         }
       }
-      final IndexReader r = reader;
+      final DirectoryReader r = reader;
       
       final int index = i;    
       
@@ -502,7 +284,7 @@
                 break;
               } else {
                 // not synchronized
-                IndexReader refreshed = IndexReader.openIfChanged(r);
+                DirectoryReader refreshed = DirectoryReader.openIfChanged(r);
                 if (refreshed == null) {
                   refreshed = r;
                 }
@@ -569,14 +351,14 @@
       
     }
     
-    for (final IndexReader readerToClose : readersToClose) {
+    for (final DirectoryReader readerToClose : readersToClose) {
       readerToClose.close();
     }
     
     firstReader.close();
     reader.close();
     
-    for (final IndexReader readerToClose : readersToClose) {
+    for (final DirectoryReader readerToClose : readersToClose) {
       assertReaderClosed(readerToClose, true, true);
     }
 
@@ -587,13 +369,13 @@
   }
   
   private static class ReaderCouple {
-    ReaderCouple(IndexReader r1, IndexReader r2) {
+    ReaderCouple(DirectoryReader r1, DirectoryReader r2) {
       newReader = r1;
       refreshedReader = r2;
     }
     
-    IndexReader newReader;
-    IndexReader refreshedReader;
+    DirectoryReader newReader;
+    DirectoryReader refreshedReader;
   }
   
   private abstract static class ReaderThreadTask {
@@ -631,21 +413,21 @@
   
   private Object createReaderMutex = new Object();
   
-  private ReaderCouple refreshReader(IndexReader reader, boolean hasChanges) throws IOException {
+  private ReaderCouple refreshReader(DirectoryReader reader, boolean hasChanges) throws IOException {
     return refreshReader(reader, null, -1, hasChanges);
   }
   
-  ReaderCouple refreshReader(IndexReader reader, TestReopen test, int modify, boolean hasChanges) throws IOException {
+  ReaderCouple refreshReader(DirectoryReader reader, TestReopen test, int modify, boolean hasChanges) throws IOException {
     synchronized (createReaderMutex) {
-      IndexReader r = null;
+      DirectoryReader r = null;
       if (test != null) {
         test.modifyIndex(modify);
         r = test.openReader();
       }
       
-      IndexReader refreshed = null;
+      DirectoryReader refreshed = null;
       try {
-        refreshed = IndexReader.openIfChanged(reader);
+        refreshed = DirectoryReader.openIfChanged(reader);
         if (refreshed == null) {
           refreshed = reader;
         }
@@ -658,11 +440,11 @@
       
       if (hasChanges) {
         if (refreshed == reader) {
-          fail("No new IndexReader instance created during refresh.");
+          fail("No new DirectoryReader instance created during refresh.");
         }
       } else {
         if (refreshed != reader) {
-          fail("New IndexReader instance created during refresh even though index had no changes.");
+          fail("New DirectoryReader instance created during refresh even though index had no changes.");
         }
       }
       
@@ -689,7 +471,7 @@
     
     w.close();
 
-    IndexReader r = IndexReader.open(dir);
+    DirectoryReader r = DirectoryReader.open(dir);
     if (multiSegment) {
       assertTrue(r.getSequentialSubReaders().length > 1);
     } else {
@@ -758,41 +540,25 @@
   static void assertReaderClosed(IndexReader reader, boolean checkSubReaders, boolean checkNormsClosed) {
     assertEquals(0, reader.getRefCount());
     
-    if (checkNormsClosed && reader instanceof SegmentReader) {
+    if (checkNormsClosed && reader instanceof AtomicReader) {
       // TODO: should we really assert something here? we check for open files and this is obselete...
       // assertTrue(((SegmentReader) reader).normsClosed());
     }
     
-    if (checkSubReaders) {
-      if (reader instanceof DirectoryReader) {
-        IndexReader[] subReaders = reader.getSequentialSubReaders();
-        for (int i = 0; i < subReaders.length; i++) {
-          assertReaderClosed(subReaders[i], checkSubReaders, checkNormsClosed);
-        }
+    if (checkSubReaders && reader instanceof CompositeReader) {
+      IndexReader[] subReaders = ((CompositeReader) reader).getSequentialSubReaders();
+      for (int i = 0; i < subReaders.length; i++) {
+        assertReaderClosed(subReaders[i], checkSubReaders, checkNormsClosed);
       }
-      
-      if (reader instanceof MultiReader) {
-        IndexReader[] subReaders = reader.getSequentialSubReaders();
-        for (int i = 0; i < subReaders.length; i++) {
-          assertReaderClosed(subReaders[i], checkSubReaders, checkNormsClosed);
-        }
-      }
-      
-      if (reader instanceof ParallelReader) {
-        IndexReader[] subReaders = ((ParallelReader) reader).getSubReaders();
-        for (int i = 0; i < subReaders.length; i++) {
-          assertReaderClosed(subReaders[i], checkSubReaders, checkNormsClosed);
-        }
-      }
     }
   }
 
   /*
-  private void assertReaderOpen(IndexReader reader) {
+  private void assertReaderOpen(DirectoryReader reader) {
     reader.ensureOpen();
     
     if (reader instanceof DirectoryReader) {
-      IndexReader[] subReaders = reader.getSequentialSubReaders();
+      DirectoryReader[] subReaders = reader.getSequentialSubReaders();
       for (int i = 0; i < subReaders.length; i++) {
         assertReaderOpen(subReaders[i]);
       }
@@ -800,13 +566,13 @@
   }
   */
 
-  private void assertRefCountEquals(int refCount, IndexReader reader) {
+  private void assertRefCountEquals(int refCount, DirectoryReader reader) {
     assertEquals("Reader has wrong refCount value.", refCount, reader.getRefCount());
   }
 
 
   private abstract static class TestReopen {
-    protected abstract IndexReader openReader() throws IOException;
+    protected abstract DirectoryReader openReader() throws IOException;
     protected abstract void modifyIndex(int i) throws IOException;
   }
   
@@ -842,12 +608,12 @@
     }
     writer.close();
 
-    IndexReader r = IndexReader.open(dir);
+    DirectoryReader r = DirectoryReader.open(dir);
     assertEquals(0, r.numDocs());
 
-    Collection<IndexCommit> commits = IndexReader.listCommits(dir);
+    Collection<IndexCommit> commits = DirectoryReader.listCommits(dir);
     for (final IndexCommit commit : commits) {
-      IndexReader r2 = IndexReader.openIfChanged(r, commit);
+      DirectoryReader r2 = DirectoryReader.openIfChanged(r, commit);
       assertNotNull(r2);
       assertTrue(r2 != r);
 
Index: lucene/src/test/org/apache/lucene/index/TestIndexWriter.java
===================================================================
--- lucene/src/test/org/apache/lucene/index/TestIndexWriter.java	(revision 1238034)
+++ lucene/src/test/org/apache/lucene/index/TestIndexWriter.java	(working copy)
@@ -775,8 +775,8 @@
     doc.add(newField("", "a b c", TextField.TYPE_UNSTORED));
     writer.addDocument(doc);  
     writer.close();
-    IndexReader reader = IndexReader.open(dir);
-    IndexReader subreader = getOnlySegmentReader(reader);
+    DirectoryReader reader = IndexReader.open(dir);
+    AtomicReader subreader = getOnlySegmentReader(reader);
     TermsEnum te = subreader.fields().terms("").iterator(null);
     assertEquals(new BytesRef("a"), te.next());
     assertEquals(new BytesRef("b"), te.next());
@@ -796,8 +796,8 @@
     doc.add(newField("", "c", StringField.TYPE_UNSTORED));
     writer.addDocument(doc);  
     writer.close();
-    IndexReader reader = IndexReader.open(dir);
-    IndexReader subreader = getOnlySegmentReader(reader);
+    DirectoryReader reader = IndexReader.open(dir);
+    AtomicReader subreader = getOnlySegmentReader(reader);
     TermsEnum te = subreader.fields().terms("").iterator(null);
     assertEquals(new BytesRef(""), te.next());
     assertEquals(new BytesRef("a"), te.next());
@@ -1301,7 +1301,7 @@
     d.add(f);
     w.addDocument(d);
 
-    IndexReader r = w.getReader().getSequentialSubReaders()[0];
+    AtomicReader r = getOnlySegmentReader(w.getReader());
     TermsEnum t = r.fields().terms("field").iterator(null);
     int count = 0;
     while(t.next() != null) {
@@ -1331,7 +1331,7 @@
       Document doc = new Document();
       doc.add(newField("field", "go", TextField.TYPE_UNSTORED));
       w.addDocument(doc);
-      IndexReader r;
+      DirectoryReader r;
       if (iter == 0) {
         // use NRT
         r = w.getReader();
@@ -1348,7 +1348,7 @@
       if (iter == 1) {
         w.commit();
       }
-      IndexReader r2 = IndexReader.openIfChanged(r);
+      IndexReader r2 = DirectoryReader.openIfChanged(r);
       assertNotNull(r2);
       assertTrue(r != r2);
       files = Arrays.asList(dir.listAll());
@@ -1407,7 +1407,7 @@
     doc.add(newField("c", "val", customType));
     writer.addDocument(doc);
     writer.commit();
-    assertEquals(1, IndexReader.listCommits(dir).size());
+    assertEquals(1, DirectoryReader.listCommits(dir).size());
 
     // Keep that commit
     sdp.snapshot("id");
@@ -1417,12 +1417,12 @@
     doc.add(newField("c", "val", customType));
     writer.addDocument(doc);
     writer.commit();
-    assertEquals(2, IndexReader.listCommits(dir).size());
+    assertEquals(2, DirectoryReader.listCommits(dir).size());
 
     // Should delete the unreferenced commit
     sdp.release("id");
     writer.deleteUnusedFiles();
-    assertEquals(1, IndexReader.listCommits(dir).size());
+    assertEquals(1, DirectoryReader.listCommits(dir).size());
 
     writer.close();
     dir.close();
@@ -1543,7 +1543,7 @@
     _TestUtil.checkIndex(dir);
 
     assertNoUnreferencedFiles(dir, "no tv files");
-    IndexReader r0 = IndexReader.open(dir);
+    DirectoryReader r0 = IndexReader.open(dir);
     for (IndexReader r : r0.getSequentialSubReaders()) {
       SegmentInfo s = ((SegmentReader) r).getSegmentInfo();
       assertFalse(s.getHasVectors());
@@ -1675,7 +1675,7 @@
     w.close();
     assertEquals(1, reader.docFreq(new Term("content", bigTerm)));
 
-    FieldCache.DocTermsIndex dti = FieldCache.DEFAULT.getTermsIndex(new SlowMultiReaderWrapper(reader), "content", random.nextBoolean());
+    FieldCache.DocTermsIndex dti = FieldCache.DEFAULT.getTermsIndex(SlowCompositeReaderWrapper.wrap(reader), "content", random.nextBoolean());
     assertEquals(5, dti.numOrd());                // +1 for null ord
     assertEquals(4, dti.size());
     assertEquals(bigTermBytesRef, dti.lookup(3, new BytesRef()));
@@ -1727,7 +1727,7 @@
     Document doc = new Document();
     doc.add(newField("id", "0", StringField.TYPE_STORED));
     w.addDocument(doc);
-    IndexReader r = w.getReader();
+    DirectoryReader r = w.getReader();
     long version = r.getVersion();
     r.close();
 
Index: lucene/src/test/org/apache/lucene/index/TestIndexWriterCommit.java
===================================================================
--- lucene/src/test/org/apache/lucene/index/TestIndexWriterCommit.java	(revision 1238034)
+++ lucene/src/test/org/apache/lucene/index/TestIndexWriterCommit.java	(working copy)
@@ -51,7 +51,7 @@
       writer.close();
 
       Term searchTerm = new Term("content", "aaa");
-      IndexReader reader = IndexReader.open(dir);
+      DirectoryReader reader = IndexReader.open(dir);
       IndexSearcher searcher = new IndexSearcher(reader);
       ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;
       assertEquals("first number of hits", 14, hits.length);
@@ -279,7 +279,7 @@
     writer.forceMerge(1);
 
     // Open a reader before closing (commiting) the writer:
-    IndexReader reader = IndexReader.open(dir);
+    DirectoryReader reader = IndexReader.open(dir);
 
     // Reader should see index as multi-seg at this
     // point:
@@ -339,7 +339,7 @@
           public void run() {
             try {
               final Document doc = new Document();
-              IndexReader r = IndexReader.open(dir);
+              DirectoryReader r = IndexReader.open(dir);
               Field f = newField("f", "", StringField.TYPE_UNSTORED);
               doc.add(f);
               int count = 0;
@@ -350,7 +350,7 @@
                   f.setValue(s);
                   w.addDocument(doc);
                   w.commit();
-                  IndexReader r2 = IndexReader.openIfChanged(r);
+                  DirectoryReader r2 = DirectoryReader.openIfChanged(r);
                   assertNotNull(r2);
                   assertTrue(r2 != r);
                   r.close();
@@ -390,10 +390,10 @@
     for (int i = 0; i < 23; i++)
       TestIndexWriter.addDoc(writer);
 
-    IndexReader reader = IndexReader.open(dir);
+    DirectoryReader reader = IndexReader.open(dir);
     assertEquals(0, reader.numDocs());
     writer.commit();
-    IndexReader reader2 = IndexReader.openIfChanged(reader);
+    DirectoryReader reader2 = DirectoryReader.openIfChanged(reader);
     assertNotNull(reader2);
     assertEquals(0, reader.numDocs());
     assertEquals(23, reader2.numDocs());
@@ -435,7 +435,7 @@
 
     // open "first" with IndexWriter
     IndexCommit commit = null;
-    for(IndexCommit c : IndexReader.listCommits(dir)) {
+    for(IndexCommit c : DirectoryReader.listCommits(dir)) {
       if (c.getUserData().get("tag").equals("first")) {
         commit = c;
         break;
@@ -456,7 +456,7 @@
 
     // make sure "second" commit is still there
     commit = null;
-    for(IndexCommit c : IndexReader.listCommits(dir)) {
+    for(IndexCommit c : DirectoryReader.listCommits(dir)) {
       if (c.getUserData().get("tag").equals("second")) {
         commit = c;
         break;
@@ -475,14 +475,14 @@
     Directory dir = newDirectory();
     IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)));
     try {
-      IndexReader.listCommits(dir);
+      DirectoryReader.listCommits(dir);
       fail("listCommits should have thrown an exception over empty index");
     } catch (IndexNotFoundException e) {
       // that's expected !
     }
     // No changes still should generate a commit, because it's a new index.
     writer.close();
-    assertEquals("expected 1 commits!", 1, IndexReader.listCommits(dir).size());
+    assertEquals("expected 1 commits!", 1, DirectoryReader.listCommits(dir).size());
     dir.close();
   }
   
@@ -501,7 +501,7 @@
     for (int i = 0; i < 23; i++)
       TestIndexWriter.addDoc(writer);
 
-    IndexReader reader = IndexReader.open(dir);
+    DirectoryReader reader = IndexReader.open(dir);
     assertEquals(0, reader.numDocs());
 
     writer.prepareCommit();
@@ -511,7 +511,7 @@
 
     writer.commit();
 
-    IndexReader reader3 = IndexReader.openIfChanged(reader);
+    IndexReader reader3 = DirectoryReader.openIfChanged(reader);
     assertNotNull(reader3);
     assertEquals(0, reader.numDocs());
     assertEquals(0, reader2.numDocs());
@@ -558,7 +558,7 @@
     for (int i = 0; i < 23; i++)
       TestIndexWriter.addDoc(writer);
 
-    IndexReader reader = IndexReader.open(dir);
+    DirectoryReader reader = IndexReader.open(dir);
     assertEquals(0, reader.numDocs());
 
     writer.prepareCommit();
@@ -568,7 +568,7 @@
 
     writer.rollback();
 
-    IndexReader reader3 = IndexReader.openIfChanged(reader);
+    IndexReader reader3 = DirectoryReader.openIfChanged(reader);
     assertNull(reader3);
     assertEquals(0, reader.numDocs());
     assertEquals(0, reader2.numDocs());
@@ -620,9 +620,9 @@
       TestIndexWriter.addDoc(w);
     w.close();
 
-    assertEquals(0, IndexReader.getCommitUserData(dir).size());
+    assertEquals(0, DirectoryReader.getCommitUserData(dir).size());
 
-    IndexReader r = IndexReader.open(dir);
+    DirectoryReader r = IndexReader.open(dir);
     // commit(Map) never called for this index
     assertEquals(0, r.getCommitUserData().size());
     r.close();
@@ -635,7 +635,7 @@
     w.commit(data);
     w.close();
 
-    assertEquals("test1", IndexReader.getCommitUserData(dir).get("label"));
+    assertEquals("test1", DirectoryReader.getCommitUserData(dir).get("label"));
 
     r = IndexReader.open(dir);
     assertEquals("test1", r.getCommitUserData().get("label"));
@@ -645,7 +645,7 @@
     w.forceMerge(1);
     w.close();
 
-    assertEquals("test1", IndexReader.getCommitUserData(dir).get("label"));
+    assertEquals("test1", DirectoryReader.getCommitUserData(dir).get("label"));
 
     dir.close();
   }
Index: lucene/src/test/org/apache/lucene/index/TestIndexWriterConfig.java
===================================================================
--- lucene/src/test/org/apache/lucene/index/TestIndexWriterConfig.java	(revision 1238034)
+++ lucene/src/test/org/apache/lucene/index/TestIndexWriterConfig.java	(working copy)
@@ -133,7 +133,7 @@
     assertEquals(IndexWriterConfig.DISABLE_AUTO_FLUSH, IndexWriterConfig.DEFAULT_MAX_BUFFERED_DOCS);
     assertEquals(16.0, IndexWriterConfig.DEFAULT_RAM_BUFFER_SIZE_MB, 0.0);
     assertEquals(false, IndexWriterConfig.DEFAULT_READER_POOLING);
-    assertEquals(IndexReader.DEFAULT_TERMS_INDEX_DIVISOR, IndexWriterConfig.DEFAULT_READER_TERMS_INDEX_DIVISOR);
+    assertEquals(DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, IndexWriterConfig.DEFAULT_READER_TERMS_INDEX_DIVISOR);
   }
 
   @Test
Index: lucene/src/test/org/apache/lucene/index/TestIndexWriterForceMerge.java
===================================================================
--- lucene/src/test/org/apache/lucene/index/TestIndexWriterForceMerge.java	(revision 1238034)
+++ lucene/src/test/org/apache/lucene/index/TestIndexWriterForceMerge.java	(working copy)
@@ -186,7 +186,7 @@
 
       if (0 == pass) {
         writer.close();
-        IndexReader reader = IndexReader.open(dir);
+        DirectoryReader reader = IndexReader.open(dir);
         assertEquals(1, reader.getSequentialSubReaders().length);
         reader.close();
       } else {
@@ -196,7 +196,7 @@
         writer.addDocument(doc);
         writer.close();
 
-        IndexReader reader = IndexReader.open(dir);
+        DirectoryReader reader = IndexReader.open(dir);
         assertTrue(reader.getSequentialSubReaders().length > 1);
         reader.close();
 
Index: lucene/src/test/org/apache/lucene/index/TestIndexWriterNRTIsCurrent.java
===================================================================
--- lucene/src/test/org/apache/lucene/index/TestIndexWriterNRTIsCurrent.java	(revision 1238034)
+++ lucene/src/test/org/apache/lucene/index/TestIndexWriterNRTIsCurrent.java	(working copy)
@@ -32,7 +32,7 @@
 public class TestIndexWriterNRTIsCurrent extends LuceneTestCase {
 
   public static class ReaderHolder {
-    volatile IndexReader reader;
+    volatile DirectoryReader reader;
     volatile boolean stop = false;
   }
 
@@ -90,7 +90,7 @@
     }
 
     public void run() {
-      IndexReader currentReader = null;
+      DirectoryReader currentReader = null;
       try {
         Document doc = new Document();
         doc.add(new Field("id", "1", TextField.TYPE_UNSTORED));
@@ -117,7 +117,7 @@
           }
           if (random.nextBoolean()) {
             writer.commit();
-            final IndexReader newReader = IndexReader
+            final DirectoryReader newReader = DirectoryReader
                 .openIfChanged(currentReader);
             if (newReader != null) { 
               currentReader.decRef();
@@ -167,7 +167,7 @@
         failed = e;
         return;
       }
-      IndexReader reader;
+      DirectoryReader reader;
       while ((reader = holder.reader) != null) {
         if (reader.tryIncRef()) {
           try {
Index: lucene/src/test/org/apache/lucene/index/TestIndexWriterOnJRECrash.java
===================================================================
--- lucene/src/test/org/apache/lucene/index/TestIndexWriterOnJRECrash.java	(revision 1238034)
+++ lucene/src/test/org/apache/lucene/index/TestIndexWriterOnJRECrash.java	(working copy)
@@ -128,7 +128,7 @@
     if (file.isDirectory()) {
       MockDirectoryWrapper dir = newFSDirectory(file);
       dir.setCheckIndexOnClose(false); // don't double-checkindex
-      if (IndexReader.indexExists(dir)) {
+      if (DirectoryReader.indexExists(dir)) {
         if (VERBOSE) {
           System.err.println("Checking index: " + file);
         }
Index: lucene/src/test/org/apache/lucene/index/TestIndexWriterReader.java
===================================================================
--- lucene/src/test/org/apache/lucene/index/TestIndexWriterReader.java	(revision 1238034)
+++ lucene/src/test/org/apache/lucene/index/TestIndexWriterReader.java	(working copy)
@@ -71,7 +71,7 @@
     
     IndexWriter writer = new IndexWriter(dir1, iwc);
     for (int i = 0; i < 97 ; i++) {
-      IndexReader reader = writer.getReader();
+      DirectoryReader reader = writer.getReader();
       if (i == 0) {
         writer.addDocument(DocHelper.createDocument(i, "x", 1 + random.nextInt(5)));
       } else {
@@ -96,7 +96,7 @@
       reader.close();
     }
     writer.forceMerge(1); // make sure all merging is done etc.
-    IndexReader reader = writer.getReader();
+    DirectoryReader reader = writer.getReader();
     writer.commit(); // no changes that are not visible to the reader
     assertTrue(reader.isCurrent());
     writer.close();
@@ -137,7 +137,7 @@
     // writer.flush(false, true, true);
 
     // get a reader
-    IndexReader r1 = writer.getReader();
+    DirectoryReader r1 = writer.getReader();
     assertTrue(r1.isCurrent());
 
     String id10 = r1.document(10).getField("id").stringValue();
@@ -148,7 +148,7 @@
     writer.updateDocument(new Term("id", id10), newDoc);
     assertFalse(r1.isCurrent());
 
-    IndexReader r2 = writer.getReader();
+    DirectoryReader r2 = writer.getReader();
     assertTrue(r2.isCurrent());
     assertEquals(0, count(new Term("id", id10), r2));
     if (VERBOSE) {
@@ -160,7 +160,7 @@
     writer.close();
     assertTrue(r2.isCurrent());
     
-    IndexReader r3 = IndexReader.open(dir1);
+    DirectoryReader r3 = IndexReader.open(dir1);
     assertTrue(r3.isCurrent());
     assertTrue(r2.isCurrent());
     assertEquals(0, count(new Term("id", id10), r3));
@@ -198,7 +198,7 @@
     writer = new IndexWriter(dir, iwc);
     doc = new Document();
     doc.add(newField("field", "a b c", TextField.TYPE_UNSTORED));
-    IndexReader nrtReader = writer.getReader();
+    DirectoryReader nrtReader = writer.getReader();
     assertTrue(nrtReader.isCurrent());
     writer.addDocument(doc);
     assertFalse(nrtReader.isCurrent()); // should see the changes
@@ -206,7 +206,7 @@
     assertFalse(nrtReader.isCurrent());
     nrtReader.close();
     
-    IndexReader dirReader = IndexReader.open(dir);
+    DirectoryReader dirReader = IndexReader.open(dir);
     nrtReader = writer.getReader();
     
     assertTrue(dirReader.isCurrent());
@@ -253,13 +253,13 @@
     createIndexNoClose(!doFullMerge, "index2", writer2);
     writer2.close();
 
-    IndexReader r0 = writer.getReader();
+    DirectoryReader r0 = writer.getReader();
     assertTrue(r0.isCurrent());
     writer.addIndexes(dir2);
     assertFalse(r0.isCurrent());
     r0.close();
 
-    IndexReader r1 = writer.getReader();
+    DirectoryReader r1 = writer.getReader();
     assertTrue(r1.isCurrent());
 
     writer.commit();
@@ -602,7 +602,7 @@
   private static class MyWarmer extends IndexWriter.IndexReaderWarmer {
     int warmCount;
     @Override
-    public void warm(IndexReader reader) throws IOException {
+    public void warm(AtomicReader reader) throws IOException {
       warmCount++;
     }
   }
@@ -656,7 +656,7 @@
     createIndexNoClose(false, "test", writer);
 
     // get a reader to put writer into near real-time mode
-    IndexReader r1 = writer.getReader();
+    DirectoryReader r1 = writer.getReader();
     _TestUtil.checkIndex(dir1);
     writer.commit();
     _TestUtil.checkIndex(dir1);
@@ -667,7 +667,7 @@
     }
     ((ConcurrentMergeScheduler) writer.getConfig().getMergeScheduler()).sync();
 
-    IndexReader r2 = IndexReader.openIfChanged(r1);
+    DirectoryReader r2 = DirectoryReader.openIfChanged(r1);
     if (r2 != null) {
       r1.close();
       r1 = r2;
@@ -686,7 +686,7 @@
     // create the index
     createIndexNoClose(false, "test", writer);
 
-    IndexReader r = writer.getReader();
+    DirectoryReader r = writer.getReader();
     writer.close();
 
     _TestUtil.checkIndex(dir1);
@@ -697,7 +697,7 @@
     IndexSearcher searcher = newSearcher(r);
     assertEquals(100, searcher.search(q, 10).totalHits);
     try {
-      IndexReader.openIfChanged(r);
+      DirectoryReader.openIfChanged(r);
       fail("failed to hit AlreadyClosedException");
     } catch (AlreadyClosedException ace) {
       // expected
@@ -724,7 +724,7 @@
       dirs[i] = new MockDirectoryWrapper(random, new RAMDirectory(dir1, newIOContext(random)));
     }
 
-    IndexReader r = writer.getReader();
+    DirectoryReader r = writer.getReader();
 
     final float SECONDS = 0.5f;
 
@@ -753,7 +753,7 @@
 
     int lastCount = 0;
     while(System.currentTimeMillis() < endTime) {
-      IndexReader r2 = IndexReader.openIfChanged(r);
+      DirectoryReader r2 = DirectoryReader.openIfChanged(r);
       if (r2 != null) {
         r.close();
         r = r2;
@@ -769,7 +769,7 @@
       threads[i].join();
     }
     // final check
-    IndexReader r2 = IndexReader.openIfChanged(r);
+    DirectoryReader r2 = DirectoryReader.openIfChanged(r);
     if (r2 != null) {
       r.close();
       r = r2;
@@ -802,7 +802,7 @@
     createIndexNoClose(false, "test", writer);
     writer.commit();
 
-    IndexReader r = writer.getReader();
+    DirectoryReader r = writer.getReader();
 
     final float SECONDS = 0.5f;
 
@@ -841,7 +841,7 @@
 
     int sum = 0;
     while(System.currentTimeMillis() < endTime) {
-      IndexReader r2 = IndexReader.openIfChanged(r);
+      DirectoryReader r2 = DirectoryReader.openIfChanged(r);
       if (r2 != null) {
         r.close();
         r = r2;
@@ -855,7 +855,7 @@
       threads[i].join();
     }
     // at least search once
-    IndexReader r2 = IndexReader.openIfChanged(r);
+    DirectoryReader r2 = DirectoryReader.openIfChanged(r);
     if (r2 != null) {
       r.close();
       r = r2;
@@ -946,7 +946,7 @@
             setReaderPooling(true).
             setMergedSegmentWarmer(new IndexWriter.IndexReaderWarmer() {
               @Override
-              public void warm(IndexReader r) throws IOException {
+              public void warm(AtomicReader r) throws IOException {
                 IndexSearcher s = newSearcher(r);
                 TopDocs hits = s.search(new TermQuery(new Term("foo", "bar")), 10);
                 assertEquals(20, hits.totalHits);
@@ -1005,13 +1005,13 @@
         d,
         newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)));
 
-    IndexReader r = w.getReader(); // start pooling readers
+    DirectoryReader r = w.getReader(); // start pooling readers
 
-    IndexReader r2 = IndexReader.openIfChanged(r);
+    DirectoryReader r2 = DirectoryReader.openIfChanged(r);
     assertNull(r2);
     
     w.addDocument(new Document());
-    IndexReader r3 = IndexReader.openIfChanged(r);
+    DirectoryReader r3 = DirectoryReader.openIfChanged(r);
     assertNotNull(r3);
     assertTrue(r3.getVersion() != r.getVersion());
     assertTrue(r3.isCurrent());
@@ -1021,12 +1021,12 @@
 
     // ... but IW marks this as not current:
     assertFalse(r3.isCurrent());
-    IndexReader r4 = IndexReader.openIfChanged(r3);
+    DirectoryReader r4 = DirectoryReader.openIfChanged(r3);
     assertNull(r4);
 
     // Deletes nothing in reality...:
     w.deleteDocuments(new Term("foo", "bar"));
-    IndexReader r5 = IndexReader.openIfChanged(r3, w, true);
+    DirectoryReader r5 = DirectoryReader.openIfChanged(r3, w, true);
     assertNull(r5);
 
     r3.close();
Index: lucene/src/test/org/apache/lucene/index/TestIndexWriterUnicode.java
===================================================================
--- lucene/src/test/org/apache/lucene/index/TestIndexWriterUnicode.java	(revision 1238034)
+++ lucene/src/test/org/apache/lucene/index/TestIndexWriterUnicode.java	(working copy)
@@ -32,6 +32,7 @@
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.CharsRef;
 import org.apache.lucene.util.LuceneTestCase;
+import org.apache.lucene.util.ReaderUtil;
 import org.apache.lucene.util.UnicodeUtil;
 
 public class TestIndexWriterUnicode extends LuceneTestCase {
@@ -317,10 +318,12 @@
     IndexReader r = writer.getReader();
 
     // Test each sub-segment
-    final IndexReader[] subs = r.getSequentialSubReaders();
-    for(int i=0;i<subs.length;i++) {
-      checkTermsOrder(subs[i], allTerms, false);
-    }
+    new ReaderUtil.Gather(r) {
+      @Override
+      protected void add(int base, AtomicReader r) throws IOException {
+        checkTermsOrder(r, allTerms, false);
+      }
+    }.run();
     checkTermsOrder(r, allTerms, true);
 
     // Test multi segment
Index: lucene/src/test/org/apache/lucene/index/TestIsCurrent.java
===================================================================
--- lucene/src/test/org/apache/lucene/index/TestIsCurrent.java	(revision 1238034)
+++ lucene/src/test/org/apache/lucene/index/TestIsCurrent.java	(working copy)
@@ -63,7 +63,7 @@
   public void testDeleteByTermIsCurrent() throws IOException {
 
     // get reader
-    IndexReader reader = writer.getReader();
+    DirectoryReader reader = writer.getReader();
 
     // assert index has a document and reader is up2date 
     assertEquals("One document should be in the index", 1, writer.numDocs());
@@ -90,7 +90,7 @@
   public void testDeleteAllIsCurrent() throws IOException {
 
     // get reader
-    IndexReader reader = writer.getReader();
+    DirectoryReader reader = writer.getReader();
 
     // assert index has a document and reader is up2date 
     assertEquals("One document should be in the index", 1, writer.numDocs());
Index: lucene/src/test/org/apache/lucene/index/TestMultiLevelSkipList.java
===================================================================
--- lucene/src/test/org/apache/lucene/index/TestMultiLevelSkipList.java	(revision 1238034)
+++ lucene/src/test/org/apache/lucene/index/TestMultiLevelSkipList.java	(working copy)
@@ -80,7 +80,7 @@
     writer.forceMerge(1);
     writer.close();
 
-    IndexReader reader = getOnlySegmentReader(IndexReader.open(dir));
+    AtomicReader reader = getOnlySegmentReader(IndexReader.open(dir));
     
     for (int i = 0; i < 2; i++) {
       counter = 0;
Index: lucene/src/test/org/apache/lucene/index/TestMultiReader.java
===================================================================
--- lucene/src/test/org/apache/lucene/index/TestMultiReader.java	(revision 1238034)
+++ lucene/src/test/org/apache/lucene/index/TestMultiReader.java	(working copy)
@@ -38,8 +38,8 @@
     IndexReader reader;
 
     sis.read(dir);
-    SegmentReader reader1 = new SegmentReader(sis.info(0), IndexReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random));
-    SegmentReader reader2 = new SegmentReader(sis.info(1), IndexReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random));
+    SegmentReader reader1 = new SegmentReader(sis.info(0), DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random));
+    SegmentReader reader2 = new SegmentReader(sis.info(1), DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random));
     readers[0] = reader1;
     readers[1] = reader2;
     assertTrue(reader1 != null);
Index: lucene/src/test/org/apache/lucene/index/TestNeverDelete.java
===================================================================
--- lucene/src/test/org/apache/lucene/index/TestNeverDelete.java	(revision 1238034)
+++ lucene/src/test/org/apache/lucene/index/TestNeverDelete.java	(working copy)
@@ -80,7 +80,7 @@
 
     final Set<String> allFiles = new HashSet<String>();
 
-    IndexReader r = IndexReader.open(d);
+    DirectoryReader r = IndexReader.open(d);
     while(System.currentTimeMillis() < stopTime) {
       final IndexCommit ic = r.getIndexCommit();
       if (VERBOSE) {
@@ -91,7 +91,7 @@
       for(String fileName : allFiles) {
         assertTrue("file " + fileName + " does not exist", d.fileExists(fileName));
       }
-      IndexReader r2 = IndexReader.openIfChanged(r);
+      DirectoryReader r2 = DirectoryReader.openIfChanged(r);
       if (r2 != null) {
         r.close();
         r = r2;
Index: lucene/src/test/org/apache/lucene/index/TestNoDeletionPolicy.java
===================================================================
--- lucene/src/test/org/apache/lucene/index/TestNoDeletionPolicy.java	(revision 1238034)
+++ lucene/src/test/org/apache/lucene/index/TestNoDeletionPolicy.java	(working copy)
@@ -76,7 +76,7 @@
       doc.add(newField("c", "a" + i, TextField.TYPE_STORED));
       writer.addDocument(doc);
       writer.commit();
-      assertEquals("wrong number of commits !", i + 1, IndexReader.listCommits(dir).size());
+      assertEquals("wrong number of commits !", i + 1, DirectoryReader.listCommits(dir).size());
     }
     writer.close();
     dir.close();
Index: lucene/src/test/org/apache/lucene/index/TestNorms.java
===================================================================
--- lucene/src/test/org/apache/lucene/index/TestNorms.java	(revision 1238034)
+++ lucene/src/test/org/apache/lucene/index/TestNorms.java	(working copy)
@@ -98,7 +98,7 @@
   public void testMaxByteNorms() throws IOException {
     Directory dir = newDirectory();
     buildIndex(dir, true);
-    IndexReader open = new SlowMultiReaderWrapper(IndexReader.open(dir));
+    AtomicReader open = SlowCompositeReaderWrapper.wrap(IndexReader.open(dir));
     DocValues normValues = open.normValues(byteTestField);
     assertNotNull(normValues);
     Source source = normValues.getSource();
@@ -129,7 +129,7 @@
     boolean secondWriteNorm = random.nextBoolean();
     buildIndex(otherDir, secondWriteNorm);
 
-    IndexReader reader = new SlowMultiReaderWrapper(IndexReader.open(otherDir));
+    AtomicReader reader = SlowCompositeReaderWrapper.wrap(IndexReader.open(otherDir));
     FieldInfos fieldInfos = reader.getFieldInfos();
     FieldInfo fieldInfo = fieldInfos.fieldInfo(byteTestField);
     assertFalse(fieldInfo.omitNorms);
@@ -144,7 +144,7 @@
         new MockAnalyzer(random));
     RandomIndexWriter writer = new RandomIndexWriter(random, dir, config);
     writer.addIndexes(reader);
-    IndexReader mergedReader = new SlowMultiReaderWrapper(writer.getReader());
+    AtomicReader mergedReader = SlowCompositeReaderWrapper.wrap(writer.getReader());
     if (!firstWriteNorm && !secondWriteNorm) {
       DocValues normValues = mergedReader.normValues(byteTestField);
       assertNull(normValues);
Index: lucene/src/test/org/apache/lucene/index/TestNRTThreads.java
===================================================================
--- lucene/src/test/org/apache/lucene/index/TestNRTThreads.java	(revision 1238034)
+++ lucene/src/test/org/apache/lucene/index/TestNRTThreads.java	(working copy)
@@ -36,14 +36,14 @@
 
     boolean anyOpenDelFiles = false;
 
-    IndexReader r = IndexReader.open(writer, true);
+    DirectoryReader r = IndexReader.open(writer, true);
 
     while (System.currentTimeMillis() < stopTime && !failed.get()) {
       if (random.nextBoolean()) {
         if (VERBOSE) {
           System.out.println("TEST: now reopen r=" + r);
         }
-        final IndexReader r2 = IndexReader.openIfChanged(r);
+        final DirectoryReader r2 = DirectoryReader.openIfChanged(r);
         if (r2 != null) {
           r.close();
           r = r2;
Index: lucene/src/test/org/apache/lucene/index/TestOmitNorms.java
===================================================================
--- lucene/src/test/org/apache/lucene/index/TestOmitNorms.java	(revision 1238034)
+++ lucene/src/test/org/apache/lucene/index/TestOmitNorms.java	(working copy)
@@ -295,8 +295,8 @@
     
     // fully merge and validate MultiNorms against single segment.
     riw.forceMerge(1);
-    IndexReader ir2 = riw.getReader();
-    DocValues dv2 = ir2.getSequentialSubReaders()[0].normValues(field);
+    DirectoryReader ir2 = riw.getReader();
+    DocValues dv2 = getOnlySegmentReader(ir2).normValues(field);
     byte[] norms2 = dv2 == null ? null : (byte[]) dv2.getSource().getArray();
     
     assertArrayEquals(norms1, norms2);
Index: lucene/src/test/org/apache/lucene/index/TestOmitTf.java
===================================================================
--- lucene/src/test/org/apache/lucene/index/TestOmitTf.java	(revision 1238034)
+++ lucene/src/test/org/apache/lucene/index/TestOmitTf.java	(working copy)
@@ -28,7 +28,6 @@
 import org.apache.lucene.index.FieldInfo.IndexOptions;
 import org.apache.lucene.document.FieldType;
 import org.apache.lucene.document.TextField;
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
 import org.apache.lucene.search.*;
 import org.apache.lucene.search.BooleanClause.Occur;
 import org.apache.lucene.search.similarities.Similarity;
Index: lucene/src/test/org/apache/lucene/index/TestParallelReader.java
===================================================================
--- lucene/src/test/org/apache/lucene/index/TestParallelReader.java	(revision 1238034)
+++ lucene/src/test/org/apache/lucene/index/TestParallelReader.java	(working copy)
@@ -71,8 +71,8 @@
     Directory dir1 = getDir1(random);
     Directory dir2 = getDir2(random);
     ParallelReader pr = new ParallelReader();
-    pr.add(IndexReader.open(dir1));
-    pr.add(IndexReader.open(dir2));
+    pr.add(SlowCompositeReaderWrapper.wrap(DirectoryReader.open(dir1)));
+    pr.add(SlowCompositeReaderWrapper.wrap(DirectoryReader.open(dir2)));
     FieldInfos fieldInfos = pr.getFieldInfos();
     assertEquals(4, fieldInfos.size());
     assertNotNull(fieldInfos.fieldInfo("f1"));
@@ -98,10 +98,10 @@
     w2.close();
     
     ParallelReader pr = new ParallelReader();
-    pr.add(IndexReader.open(dir1));
-    IndexReader ir = IndexReader.open(dir2);
+    pr.add(SlowCompositeReaderWrapper.wrap(DirectoryReader.open(dir1)));
+    DirectoryReader ir = DirectoryReader.open(dir2);
     try {
-      pr.add(ir);
+      pr.add(SlowCompositeReaderWrapper.wrap(ir));
       fail("didn't get exptected exception: indexes don't have same number of documents");
     } catch (IllegalArgumentException e) {
       // expected exception
@@ -145,7 +145,7 @@
     w.addDocument(d2);
     w.close();
 
-    IndexReader ir = IndexReader.open(dir);
+    DirectoryReader ir = DirectoryReader.open(dir);
     return newSearcher(ir);
   }
 
@@ -154,8 +154,8 @@
     dir1 = getDir1(random);
     dir2 = getDir2(random);
     ParallelReader pr = new ParallelReader();
-    pr.add(IndexReader.open(dir1));
-    pr.add(IndexReader.open(dir2));
+    pr.add(SlowCompositeReaderWrapper.wrap(DirectoryReader.open(dir1)));
+    pr.add(SlowCompositeReaderWrapper.wrap(DirectoryReader.open(dir2)));
     return newSearcher(pr);
   }
 
Index: lucene/src/test/org/apache/lucene/index/TestParallelReaderEmptyIndex.java
===================================================================
--- lucene/src/test/org/apache/lucene/index/TestParallelReaderEmptyIndex.java	(revision 1238034)
+++ lucene/src/test/org/apache/lucene/index/TestParallelReaderEmptyIndex.java	(working copy)
@@ -53,8 +53,8 @@
 
     IndexWriter iwOut = new IndexWriter(rdOut, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)));
     ParallelReader pr = new ParallelReader();
-    pr.add(IndexReader.open(rd1));
-    pr.add(IndexReader.open(rd2));
+    pr.add(SlowCompositeReaderWrapper.wrap(DirectoryReader.open(rd1)));
+    pr.add(SlowCompositeReaderWrapper.wrap(DirectoryReader.open(rd2)));
 		
     // When unpatched, Lucene crashes here with a NoSuchElementException (caused by ParallelTermEnum)
     iwOut.addIndexes(pr);
@@ -116,8 +116,8 @@
 
     IndexWriter iwOut = new IndexWriter(rdOut, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)));
     ParallelReader pr = new ParallelReader();
-    pr.add(IndexReader.open(rd1));
-    pr.add(IndexReader.open(rd2));
+    pr.add(SlowCompositeReaderWrapper.wrap(DirectoryReader.open(rd1)));
+    pr.add(SlowCompositeReaderWrapper.wrap(DirectoryReader.open(rd2)));
 
     // When unpatched, Lucene crashes here with an ArrayIndexOutOfBoundsException (caused by TermVectorsWriter)
     iwOut.addIndexes(pr);
Index: lucene/src/test/org/apache/lucene/index/TestParallelTermEnum.java
===================================================================
--- lucene/src/test/org/apache/lucene/index/TestParallelTermEnum.java	(revision 1238034)
+++ lucene/src/test/org/apache/lucene/index/TestParallelTermEnum.java	(working copy)
@@ -28,8 +28,8 @@
 import org.apache.lucene.util._TestUtil;
 
 public class TestParallelTermEnum extends LuceneTestCase {
-    private IndexReader ir1;
-    private IndexReader ir2;
+    private AtomicReader ir1;
+    private AtomicReader ir2;
     private Directory rd1;
     private Directory rd2;
     
@@ -58,8 +58,8 @@
 
         iw2.close();
 
-        this.ir1 = IndexReader.open(rd1);
-        this.ir2 = IndexReader.open(rd2);
+        this.ir1 = SlowCompositeReaderWrapper.wrap(DirectoryReader.open(rd1));
+        this.ir2 = SlowCompositeReaderWrapper.wrap(DirectoryReader.open(rd2));
     }
 
     @Override
Index: lucene/src/test/org/apache/lucene/index/TestPostingsOffsets.java
===================================================================
--- lucene/src/test/org/apache/lucene/index/TestPostingsOffsets.java	(revision 1238034)
+++ lucene/src/test/org/apache/lucene/index/TestPostingsOffsets.java	(working copy)
@@ -284,11 +284,13 @@
       doc.add(new Field("content", new CannedAnalyzer.CannedTokenizer(tokens.toArray(new Token[tokens.size()])), ft));
       w.addDocument(doc);
     }
-    final IndexReader r = w.getReader();
+    final DirectoryReader r = w.getReader();
     w.close();
 
     final String[] terms = new String[] {"a", "b", "c", "d"};
-    for(IndexReader sub : r.getSequentialSubReaders()) {
+    for(IndexReader reader : r.getSequentialSubReaders()) {
+      // TODO: improve this
+      AtomicReader sub = (AtomicReader) reader;
       //System.out.println("\nsub=" + sub);
       final TermsEnum termsEnum = sub.fields().terms("content").iterator(null);
       DocsEnum docs = null;
Index: lucene/src/test/org/apache/lucene/index/TestRollingUpdates.java
===================================================================
--- lucene/src/test/org/apache/lucene/index/TestRollingUpdates.java	(revision 1238034)
+++ lucene/src/test/org/apache/lucene/index/TestRollingUpdates.java	(working copy)
@@ -128,7 +128,7 @@
 
     public void run() {
       try {
-        IndexReader open = null;
+        DirectoryReader open = null;
         for (int i = 0; i < num; i++) {
           Document doc = new Document();// docs.nextDoc();
           doc.add(newField("id", "test", StringField.TYPE_UNSTORED));
@@ -137,7 +137,7 @@
             if (open == null) {
               open = IndexReader.open(writer, true);
             }
-            IndexReader reader = IndexReader.openIfChanged(open);
+            DirectoryReader reader = DirectoryReader.openIfChanged(open);
             if (reader != null) {
               open.close();
               open = reader;
Index: lucene/src/test/org/apache/lucene/index/TestSegmentMerger.java
===================================================================
--- lucene/src/test/org/apache/lucene/index/TestSegmentMerger.java	(revision 1238034)
+++ lucene/src/test/org/apache/lucene/index/TestSegmentMerger.java	(working copy)
@@ -53,8 +53,8 @@
     SegmentInfo info1 = DocHelper.writeDoc(random, merge1Dir, doc1);
     DocHelper.setupDoc(doc2);
     SegmentInfo info2 = DocHelper.writeDoc(random, merge2Dir, doc2);
-    reader1 = new SegmentReader(info1, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random));
-    reader2 = new SegmentReader(info2, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random));
+    reader1 = new SegmentReader(info1, DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random));
+    reader2 = new SegmentReader(info2, DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random));
   }
 
   @Override
@@ -87,7 +87,7 @@
     //Should be able to open a new SegmentReader against the new directory
     SegmentReader mergedReader = new SegmentReader(new SegmentInfo(mergedSegment, docsMerged, mergedDir, false,
                                                                                      codec, fieldInfos),
-                                                   IndexReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random));
+                                                   DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random));
     assertTrue(mergedReader != null);
     assertTrue(mergedReader.numDocs() == 2);
     Document newDoc1 = mergedReader.document(0);
Index: lucene/src/test/org/apache/lucene/index/TestSegmentReader.java
===================================================================
--- lucene/src/test/org/apache/lucene/index/TestSegmentReader.java	(revision 1238034)
+++ lucene/src/test/org/apache/lucene/index/TestSegmentReader.java	(working copy)
@@ -41,7 +41,7 @@
     dir = newDirectory();
     DocHelper.setupDoc(testDoc);
     SegmentInfo info = DocHelper.writeDoc(random, dir, testDoc);
-    reader = new SegmentReader(info, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR, IOContext.READ);
+    reader = new SegmentReader(info, DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, IOContext.READ);
   }
   
   @Override
@@ -173,7 +173,7 @@
     checkNorms(reader);
   }
 
-  public static void checkNorms(IndexReader reader) throws IOException {
+  public static void checkNorms(AtomicReader reader) throws IOException {
         // test omit norms
     for (int i=0; i<DocHelper.fields.length; i++) {
       IndexableField f = DocHelper.fields[i];
Index: lucene/src/test/org/apache/lucene/index/TestSnapshotDeletionPolicy.java
===================================================================
--- lucene/src/test/org/apache/lucene/index/TestSnapshotDeletionPolicy.java	(revision 1238034)
+++ lucene/src/test/org/apache/lucene/index/TestSnapshotDeletionPolicy.java	(working copy)
@@ -252,7 +252,7 @@
     writer = new IndexWriter(dir, getConfig(random, sdp));
     writer.deleteUnusedFiles();
     writer.close();
-    assertEquals("no snapshots should exist", 1, IndexReader.listCommits(dir).size());
+    assertEquals("no snapshots should exist", 1, DirectoryReader.listCommits(dir).size());
     
     for (int i = 0; i < numSnapshots; i++) {
       try {
@@ -304,7 +304,7 @@
       sdp.release(t.getName());
       writer.deleteUnusedFiles();
     }
-    assertEquals(1, IndexReader.listCommits(dir).size());
+    assertEquals(1, DirectoryReader.listCommits(dir).size());
     writer.close();
     dir.close();
   }
Index: lucene/src/test/org/apache/lucene/index/TestStressAdvance.java
===================================================================
--- lucene/src/test/org/apache/lucene/index/TestStressAdvance.java	(revision 1238034)
+++ lucene/src/test/org/apache/lucene/index/TestStressAdvance.java	(working copy)
@@ -57,7 +57,7 @@
       final List<Integer> aDocIDs = new ArrayList<Integer>();
       final List<Integer> bDocIDs = new ArrayList<Integer>();
 
-      final IndexReader r = w.getReader();
+      final DirectoryReader r = w.getReader();
       final int[] idToDocID = new int[r.maxDoc()];
       for(int docID=0;docID<idToDocID.length;docID++) {
         int id = Integer.parseInt(r.document(docID).get("id"));
@@ -67,7 +67,7 @@
           bDocIDs.add(docID);
         }
       }
-      final TermsEnum te = r.getSequentialSubReaders()[0].fields().terms("field").iterator(null);
+      final TermsEnum te = getOnlySegmentReader(r).fields().terms("field").iterator(null);
       
       DocsEnum de = null;
       for(int iter2=0;iter2<10;iter2++) {
Index: lucene/src/test/org/apache/lucene/index/TestStressIndexing2.java
===================================================================
--- lucene/src/test/org/apache/lucene/index/TestStressIndexing2.java	(revision 1238034)
+++ lucene/src/test/org/apache/lucene/index/TestStressIndexing2.java	(working copy)
@@ -66,7 +66,7 @@
     
     // TODO: verify equals using IW.getReader
     DocsAndWriter dw = indexRandomIWReader(5, 3, 100, dir);
-    IndexReader reader = dw.writer.getReader();
+    DirectoryReader reader = dw.writer.getReader();
     dw.writer.commit();
     verifyEquals(random, reader, dir, "id");
     reader.close();
@@ -265,24 +265,25 @@
     w.close();
   }
   
-  public static void verifyEquals(Random r, IndexReader r1, Directory dir2, String idField) throws Throwable {
-    IndexReader r2 = IndexReader.open(dir2);
+  public static void verifyEquals(Random r, DirectoryReader r1, Directory dir2, String idField) throws Throwable {
+    DirectoryReader r2 = IndexReader.open(dir2);
     verifyEquals(r1, r2, idField);
     r2.close();
   }
 
   public static void verifyEquals(Directory dir1, Directory dir2, String idField) throws Throwable {
-    IndexReader r1 = IndexReader.open(dir1);
-    IndexReader r2 = IndexReader.open(dir2);
+    DirectoryReader r1 = IndexReader.open(dir1);
+    DirectoryReader r2 = IndexReader.open(dir2);
     verifyEquals(r1, r2, idField);
     r1.close();
     r2.close();
   }
 
-  private static void printDocs(IndexReader r) throws Throwable {
+  private static void printDocs(DirectoryReader r) throws Throwable {
     IndexReader[] subs = r.getSequentialSubReaders();
     for(IndexReader sub : subs) {
-      Bits liveDocs = sub.getLiveDocs();
+      // TODO: improve this
+      Bits liveDocs = ((AtomicReader)sub).getLiveDocs();
       System.out.println("  " + ((SegmentReader) sub).getSegmentInfo());
       for(int docID=0;docID<sub.maxDoc();docID++) {
         Document doc = sub.document(docID);
@@ -296,7 +297,7 @@
   }
 
 
-  public static void verifyEquals(IndexReader r1, IndexReader r2, String idField) throws Throwable {
+  public static void verifyEquals(DirectoryReader r1, DirectoryReader r2, String idField) throws Throwable {
     if (VERBOSE) {
       System.out.println("\nr1 docs:");
       printDocs(r1);
Index: lucene/src/test/org/apache/lucene/index/TestStressNRT.java
===================================================================
--- lucene/src/test/org/apache/lucene/index/TestStressNRT.java	(revision 1238034)
+++ lucene/src/test/org/apache/lucene/index/TestStressNRT.java	(working copy)
@@ -40,7 +40,7 @@
 import org.apache.lucene.util._TestUtil;
 
 public class TestStressNRT extends LuceneTestCase {
-  volatile IndexReader reader;
+  volatile DirectoryReader reader;
 
   final ConcurrentHashMap<Integer,Long> model = new ConcurrentHashMap<Integer,Long>();
   Map<Integer,Long> committedModel = new HashMap<Integer,Long>();
@@ -125,7 +125,7 @@
                 if (numCommitting.incrementAndGet() <= maxConcurrentCommits) {
                   Map<Integer,Long> newCommittedModel;
                   long version;
-                  IndexReader oldReader;
+                  DirectoryReader oldReader;
 
                   synchronized(TestStressNRT.this) {
                     newCommittedModel = new HashMap<Integer,Long>(model);  // take a snapshot
@@ -134,7 +134,7 @@
                     oldReader.incRef();  // increment the reference since we will use this for reopening
                   }
 
-                  IndexReader newReader;
+                  DirectoryReader newReader;
                   if (rand.nextInt(100) < softCommitPercent) {
                     // assertU(h.commit("softCommit","true"));
                     if (random.nextBoolean()) {
@@ -146,7 +146,7 @@
                       if (VERBOSE) {
                         System.out.println("TEST: " + Thread.currentThread().getName() + ": reopen reader=" + oldReader + " version=" + version);
                       }
-                      newReader = IndexReader.openIfChanged(oldReader, writer.w, true);
+                      newReader = DirectoryReader.openIfChanged(oldReader, writer.w, true);
                     }
                   } else {
                     // assertU(commit());
@@ -157,7 +157,7 @@
                     if (VERBOSE) {
                       System.out.println("TEST: " + Thread.currentThread().getName() + ": now reopen after commit");
                     }
-                    newReader = IndexReader.openIfChanged(oldReader);
+                    newReader = DirectoryReader.openIfChanged(oldReader);
                   }
 
                   // Code below assumes newReader comes w/
@@ -306,7 +306,7 @@
               // so when querying, we should first check the model, and then the index
 
               long val;
-              IndexReader r;
+              DirectoryReader r;
               synchronized(TestStressNRT.this) {
                 val = committedModel.get(id);
                 r = reader;
Index: lucene/src/test/org/apache/lucene/index/TestTermsEnum.java
===================================================================
--- lucene/src/test/org/apache/lucene/index/TestTermsEnum.java	(revision 1238034)
+++ lucene/src/test/org/apache/lucene/index/TestTermsEnum.java	(working copy)
@@ -232,7 +232,7 @@
     w.close();
 
     // NOTE: intentional insanity!!
-    final int[] docIDToID = FieldCache.DEFAULT.getInts(new SlowMultiReaderWrapper(r), "id", false);
+    final int[] docIDToID = FieldCache.DEFAULT.getInts(SlowCompositeReaderWrapper.wrap(r), "id", false);
 
     for(int iter=0;iter<10*RANDOM_MULTIPLIER;iter++) {
 
@@ -377,7 +377,6 @@
   }
 
   private void close() throws Exception {
-    final Directory d = ((SegmentReader) r.getSequentialSubReaders()[0]).directory();
     r.close();
     d.close();
   }
Index: lucene/src/test/org/apache/lucene/index/TestTermVectorsReader.java
===================================================================
--- lucene/src/test/org/apache/lucene/index/TestTermVectorsReader.java	(revision 1238034)
+++ lucene/src/test/org/apache/lucene/index/TestTermVectorsReader.java	(working copy)
@@ -184,7 +184,7 @@
 
   public void test() throws IOException {
     //Check to see the files were created properly in setup
-    IndexReader reader = IndexReader.open(dir);
+    DirectoryReader reader = IndexReader.open(dir);
     for (IndexReader r : reader.getSequentialSubReaders()) {
       SegmentInfo s = ((SegmentReader) r).getSegmentInfo();
       assertTrue(s.getHasVectors());
Index: lucene/src/test/org/apache/lucene/index/TestThreadedForceMerge.java
===================================================================
--- lucene/src/test/org/apache/lucene/index/TestThreadedForceMerge.java	(revision 1238034)
+++ lucene/src/test/org/apache/lucene/index/TestThreadedForceMerge.java	(working copy)
@@ -123,7 +123,7 @@
           TEST_VERSION_CURRENT, ANALYZER).setOpenMode(
           OpenMode.APPEND).setMaxBufferedDocs(2));
       
-      IndexReader reader = IndexReader.open(directory);
+      DirectoryReader reader = IndexReader.open(directory);
       assertEquals("reader=" + reader, 1, reader.getSequentialSubReaders().length);
       assertEquals(expectedDocCount, reader.numDocs());
       reader.close();
Index: lucene/src/test/org/apache/lucene/index/TestTransactionRollback.java
===================================================================
--- lucene/src/test/org/apache/lucene/index/TestTransactionRollback.java	(revision 1238034)
+++ lucene/src/test/org/apache/lucene/index/TestTransactionRollback.java	(working copy)
@@ -51,7 +51,7 @@
     // System.out.println("Attempting to rollback to "+id);
     String ids="-"+id;
     IndexCommit last=null;
-    Collection<IndexCommit> commits = IndexReader.listCommits(dir);
+    Collection<IndexCommit> commits = DirectoryReader.listCommits(dir);
     for (Iterator<IndexCommit> iterator = commits.iterator(); iterator.hasNext();) {
       IndexCommit commit =  iterator.next();
       Map<String,String> ud=commit.getUserData();
@@ -107,7 +107,7 @@
 
   /*
   private void showAvailableCommitPoints() throws Exception {
-    Collection commits = IndexReader.listCommits(dir);
+    Collection commits = DirectoryReader.listCommits(dir);
     for (Iterator iterator = commits.iterator(); iterator.hasNext();) {
       IndexCommit comm = (IndexCommit) iterator.next();
       System.out.print("\t Available commit point:["+comm.getUserData()+"] files=");
Index: lucene/src/test/org/apache/lucene/index/TestTypePromotion.java
===================================================================
--- lucene/src/test/org/apache/lucene/index/TestTypePromotion.java	(revision 1238034)
+++ lucene/src/test/org/apache/lucene/index/TestTypePromotion.java	(working copy)
@@ -28,7 +28,6 @@
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.DocValuesField;
 import org.apache.lucene.document.TextField;
-import org.apache.lucene.index.IndexReader.ReaderContext;
 import org.apache.lucene.index.DocValues.Source;
 import org.apache.lucene.index.DocValues.Type;
 import org.apache.lucene.store.Directory;
@@ -99,7 +98,7 @@
       } else {
         // do a real merge here
         IndexReader open = IndexReader.open(dir_2);
-        writer.addIndexes(new SlowMultiReaderWrapper(open));
+        writer.addIndexes(SlowCompositeReaderWrapper.wrap(open));
         open.close();
       }
       dir_2.close();
@@ -117,12 +116,12 @@
   
   private void assertValues(TestType type, Directory dir, long[] values)
       throws CorruptIndexException, IOException {
-    IndexReader reader = IndexReader.open(dir);
+    DirectoryReader reader = DirectoryReader.open(dir);
     assertEquals(1, reader.getSequentialSubReaders().length);
-    ReaderContext topReaderContext = reader.getTopReaderContext();
-    ReaderContext[] children = topReaderContext.children();
-    DocValues docValues = children[0].reader.docValues("promote");
+    IndexReaderContext topReaderContext = reader.getTopReaderContext();
+    AtomicReaderContext[] children = topReaderContext.leaves();
     assertEquals(1, children.length);
+    DocValues docValues = children[0].reader().docValues("promote");
     Source directSource = docValues.getDirectSource();
     for (int i = 0; i < values.length; i++) {
       int id = Integer.parseInt(reader.document(i).get("id"));
@@ -332,11 +331,11 @@
     // now merge
     writer.forceMerge(1);
     writer.close();
-    IndexReader reader = IndexReader.open(dir);
+    DirectoryReader reader = DirectoryReader.open(dir);
     assertEquals(1, reader.getSequentialSubReaders().length);
-    ReaderContext topReaderContext = reader.getTopReaderContext();
-    ReaderContext[] children = topReaderContext.children();
-    DocValues docValues = children[0].reader.docValues("promote");
+    IndexReaderContext topReaderContext = reader.getTopReaderContext();
+    AtomicReaderContext[] children = topReaderContext.leaves();
+    DocValues docValues = children[0].reader().docValues("promote");
     assertNotNull(docValues);
     assertValues(TestType.Byte, dir, values);
     assertEquals(Type.BYTES_VAR_STRAIGHT, docValues.type());
Index: lucene/src/test/org/apache/lucene/search/FieldCacheRewriteMethod.java
===================================================================
--- lucene/src/test/org/apache/lucene/search/FieldCacheRewriteMethod.java	(revision 1238034)
+++ lucene/src/test/org/apache/lucene/search/FieldCacheRewriteMethod.java	(working copy)
@@ -20,8 +20,8 @@
 import java.io.IOException;
 import java.util.Comparator;
 
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
 import org.apache.lucene.index.Terms;
 import org.apache.lucene.index.TermsEnum;
 import org.apache.lucene.util.Bits;
@@ -88,7 +88,7 @@
      */
     @Override
     public DocIdSet getDocIdSet(AtomicReaderContext context, final Bits acceptDocs) throws IOException {
-      final FieldCache.DocTermsIndex fcsi = FieldCache.DEFAULT.getTermsIndex(context.reader, query.field);
+      final FieldCache.DocTermsIndex fcsi = FieldCache.DEFAULT.getTermsIndex(context.reader(), query.field);
       // Cannot use FixedBitSet because we require long index (ord):
       final OpenBitSet termSet = new OpenBitSet(fcsi.numOrd());
       TermsEnum termsEnum = query.getTermsEnum(new Terms() {
@@ -137,7 +137,7 @@
         return DocIdSet.EMPTY_DOCIDSET;
       }
       
-      return new FieldCacheDocIdSet(context.reader.maxDoc(), acceptDocs) {
+      return new FieldCacheDocIdSet(context.reader().maxDoc(), acceptDocs) {
         @Override
         protected final boolean matchDoc(int doc) throws ArrayIndexOutOfBoundsException {
           return termSet.get(fcsi.getOrd(doc));
Index: lucene/src/test/org/apache/lucene/search/JustCompileSearch.java
===================================================================
--- lucene/src/test/org/apache/lucene/search/JustCompileSearch.java	(revision 1238034)
+++ lucene/src/test/org/apache/lucene/search/JustCompileSearch.java	(working copy)
@@ -19,7 +19,7 @@
 
 import java.io.IOException;
 
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.Norm;
 import org.apache.lucene.search.similarities.Similarity;
 import org.apache.lucene.search.similarities.SimilarityProvider;
Index: lucene/src/test/org/apache/lucene/search/MockFilter.java
===================================================================
--- lucene/src/test/org/apache/lucene/search/MockFilter.java	(revision 1238034)
+++ lucene/src/test/org/apache/lucene/search/MockFilter.java	(working copy)
@@ -17,7 +17,7 @@
  * limitations under the License.
  */
 
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.util.DocIdBitSet;
 import org.apache.lucene.util.FixedBitSet;
 import org.apache.lucene.util.Bits;
@@ -28,7 +28,7 @@
   @Override
   public DocIdSet getDocIdSet(AtomicReaderContext context, Bits acceptDocs) {
     wasCalled = true;
-    return new FixedBitSet(context.reader.maxDoc());
+    return new FixedBitSet(context.reader().maxDoc());
   }
 
   public void clear() {
Index: lucene/src/test/org/apache/lucene/search/MultiCollectorTest.java
===================================================================
--- lucene/src/test/org/apache/lucene/search/MultiCollectorTest.java	(revision 1238034)
+++ lucene/src/test/org/apache/lucene/search/MultiCollectorTest.java	(working copy)
@@ -19,7 +19,7 @@
 
 import java.io.IOException;
 
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.search.Collector;
 import org.apache.lucene.search.Scorer;
 import org.apache.lucene.util.LuceneTestCase;
Index: lucene/src/test/org/apache/lucene/search/similarities/TestSimilarity2.java
===================================================================
--- lucene/src/test/org/apache/lucene/search/similarities/TestSimilarity2.java	(revision 1238034)
+++ lucene/src/test/org/apache/lucene/search/similarities/TestSimilarity2.java	(working copy)
@@ -27,7 +27,6 @@
 import org.apache.lucene.document.FieldType;
 import org.apache.lucene.document.TextField;
 import org.apache.lucene.index.FieldInfo.IndexOptions;
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexWriterConfig;
 import org.apache.lucene.index.RandomIndexWriter;
Index: lucene/src/test/org/apache/lucene/search/SingleDocTestFilter.java
===================================================================
--- lucene/src/test/org/apache/lucene/search/SingleDocTestFilter.java	(revision 1238034)
+++ lucene/src/test/org/apache/lucene/search/SingleDocTestFilter.java	(working copy)
@@ -17,7 +17,7 @@
  * limitations under the License.
  */
 
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.util.Bits;
 import org.apache.lucene.util.FixedBitSet;
 
@@ -32,7 +32,7 @@
 
   @Override
   public DocIdSet getDocIdSet(AtomicReaderContext context, Bits acceptDocs) throws IOException {
-    FixedBitSet bits = new FixedBitSet(context.reader.maxDoc());
+    FixedBitSet bits = new FixedBitSet(context.reader().maxDoc());
     bits.set(doc);
     if (acceptDocs != null && !acceptDocs.get(doc)) bits.clear(doc);
     return bits;
Index: lucene/src/test/org/apache/lucene/search/spans/JustCompileSearchSpans.java
===================================================================
--- lucene/src/test/org/apache/lucene/search/spans/JustCompileSearchSpans.java	(revision 1238034)
+++ lucene/src/test/org/apache/lucene/search/spans/JustCompileSearchSpans.java	(working copy)
@@ -21,8 +21,8 @@
 import java.util.Collection;
 import java.util.Map;
 
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.Term;
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
 import org.apache.lucene.search.Weight;
 import org.apache.lucene.search.similarities.Similarity;
 import org.apache.lucene.util.Bits;
Index: lucene/src/test/org/apache/lucene/search/spans/MultiSpansWrapper.java
===================================================================
--- lucene/src/test/org/apache/lucene/search/spans/MultiSpansWrapper.java	(revision 1238034)
+++ lucene/src/test/org/apache/lucene/search/spans/MultiSpansWrapper.java	(working copy)
@@ -24,10 +24,10 @@
 import java.util.Map;
 import java.util.TreeSet;
 
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.DocsEnum;
+import org.apache.lucene.index.IndexReaderContext;
 import org.apache.lucene.index.Term;
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
-import org.apache.lucene.index.IndexReader.ReaderContext;
 import org.apache.lucene.util.ReaderUtil;
 import org.apache.lucene.util.TermContext;
 
@@ -53,7 +53,7 @@
 
   }
   
-  public static Spans wrap(ReaderContext topLevelReaderContext, SpanQuery query) throws IOException {
+  public static Spans wrap(IndexReaderContext topLevelReaderContext, SpanQuery query) throws IOException {
     Map<Term,TermContext> termContexts = new HashMap<Term,TermContext>();
     TreeSet<Term> terms = new TreeSet<Term>();
     query.extractTerms(terms);
@@ -62,7 +62,7 @@
     }
     AtomicReaderContext[] leaves = ReaderUtil.leaves(topLevelReaderContext);
     if(leaves.length == 1) {
-      return query.getSpans(leaves[0], leaves[0].reader.getLiveDocs(), termContexts);
+      return query.getSpans(leaves[0], leaves[0].reader().getLiveDocs(), termContexts);
     }
     return new MultiSpansWrapper(leaves, query, termContexts);
   }
@@ -73,14 +73,14 @@
       return false;
     }
     if (current == null) {
-      current = query.getSpans(leaves[leafOrd], leaves[leafOrd].reader.getLiveDocs(), termContexts);
+      current = query.getSpans(leaves[leafOrd], leaves[leafOrd].reader().getLiveDocs(), termContexts);
     }
     while(true) {
       if (current.next()) {
         return true;
       }
       if (++leafOrd < leaves.length) {
-        current = query.getSpans(leaves[leafOrd], leaves[leafOrd].reader.getLiveDocs(), termContexts);
+        current = query.getSpans(leaves[leafOrd], leaves[leafOrd].reader().getLiveDocs(), termContexts);
       } else {
         current = null;
         break;
@@ -98,17 +98,17 @@
     int subIndex = ReaderUtil.subIndex(target, leaves);
     assert subIndex >= leafOrd;
     if (subIndex != leafOrd) {
-      current = query.getSpans(leaves[subIndex], leaves[subIndex].reader.getLiveDocs(), termContexts);
+      current = query.getSpans(leaves[subIndex], leaves[subIndex].reader().getLiveDocs(), termContexts);
       leafOrd = subIndex;
     } else if (current == null) {
-      current = query.getSpans(leaves[leafOrd], leaves[leafOrd].reader.getLiveDocs(), termContexts);
+      current = query.getSpans(leaves[leafOrd], leaves[leafOrd].reader().getLiveDocs(), termContexts);
     }
     while (true) {
       if (current.skipTo(target - leaves[leafOrd].docBase)) {
         return true;
       }
       if (++leafOrd < leaves.length) {
-        current = query.getSpans(leaves[leafOrd], leaves[leafOrd].reader.getLiveDocs(), termContexts);
+        current = query.getSpans(leaves[leafOrd], leaves[leafOrd].reader().getLiveDocs(), termContexts);
       } else {
           current = null;
           break;
Index: lucene/src/test/org/apache/lucene/search/spans/TestNearSpansOrdered.java
===================================================================
--- lucene/src/test/org/apache/lucene/search/spans/TestNearSpansOrdered.java	(revision 1238034)
+++ lucene/src/test/org/apache/lucene/search/spans/TestNearSpansOrdered.java	(working copy)
@@ -20,10 +20,10 @@
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.TextField;
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
-import org.apache.lucene.index.IndexReader.ReaderContext;
 import org.apache.lucene.index.RandomIndexWriter;
+import org.apache.lucene.index.IndexReaderContext;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.search.CheckHits;
 import org.apache.lucene.search.Explanation;
@@ -166,9 +166,9 @@
   public void testSpanNearScorerSkipTo1() throws Exception {
     SpanNearQuery q = makeQuery();
     Weight w = searcher.createNormalizedWeight(q);
-    ReaderContext topReaderContext = searcher.getTopReaderContext();
+    IndexReaderContext topReaderContext = searcher.getTopReaderContext();
     AtomicReaderContext[] leaves = ReaderUtil.leaves(topReaderContext);
-    Scorer s = w.scorer(leaves[0], true, false, leaves[0].reader.getLiveDocs());
+    Scorer s = w.scorer(leaves[0], true, false, leaves[0].reader().getLiveDocs());
     assertEquals(1, s.advance(1));
   }
   
Index: lucene/src/test/org/apache/lucene/search/spans/TestSpans.java
===================================================================
--- lucene/src/test/org/apache/lucene/search/spans/TestSpans.java	(revision 1238034)
+++ lucene/src/test/org/apache/lucene/search/spans/TestSpans.java	(working copy)
@@ -29,12 +29,12 @@
 import org.apache.lucene.search.similarities.SimilarityProvider;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.analysis.MockAnalyzer;
-import org.apache.lucene.index.IndexReader.ReaderContext;
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.IndexWriter;
 import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
 import org.apache.lucene.index.IndexWriterConfig;
 import org.apache.lucene.index.RandomIndexWriter;
+import org.apache.lucene.index.IndexReaderContext;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.StringField;
@@ -405,7 +405,7 @@
   public void testSpanScorerZeroSloppyFreq() throws Exception {
     boolean ordered = true;
     int slop = 1;
-    ReaderContext topReaderContext = searcher.getTopReaderContext();
+    IndexReaderContext topReaderContext = searcher.getTopReaderContext();
     AtomicReaderContext[] leaves = ReaderUtil.leaves(topReaderContext);
     int subIndex = ReaderUtil.subIndex(11, leaves);
     for (int i = 0; i < leaves.length; i++) {
@@ -433,7 +433,7 @@
                                 slop,
                                 ordered);
   
-        spanScorer = searcher.createNormalizedWeight(snq).scorer(leaves[i], true, false, leaves[i].reader.getLiveDocs());
+        spanScorer = searcher.createNormalizedWeight(snq).scorer(leaves[i], true, false, leaves[i].reader().getLiveDocs());
       } finally {
         searcher.setSimilarityProvider(oldSim);
       }
Index: lucene/src/test/org/apache/lucene/search/TestBooleanScorer.java
===================================================================
--- lucene/src/test/org/apache/lucene/search/TestBooleanScorer.java	(revision 1238034)
+++ lucene/src/test/org/apache/lucene/search/TestBooleanScorer.java	(working copy)
@@ -25,7 +25,7 @@
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.StringField;
 import org.apache.lucene.document.TextField;
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.RandomIndexWriter;
 import org.apache.lucene.index.Term;
Index: lucene/src/test/org/apache/lucene/search/TestCachingCollector.java
===================================================================
--- lucene/src/test/org/apache/lucene/search/TestCachingCollector.java	(revision 1238034)
+++ lucene/src/test/org/apache/lucene/search/TestCachingCollector.java	(working copy)
@@ -17,7 +17,7 @@
  * limitations under the License.
  */
 
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.util.LuceneTestCase;
 
 import java.io.IOException;
Index: lucene/src/test/org/apache/lucene/search/TestCachingWrapperFilter.java
===================================================================
--- lucene/src/test/org/apache/lucene/search/TestCachingWrapperFilter.java	(revision 1238034)
+++ lucene/src/test/org/apache/lucene/search/TestCachingWrapperFilter.java	(working copy)
@@ -22,11 +22,12 @@
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.StringField;
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
+import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.RandomIndexWriter;
 import org.apache.lucene.index.SerialMergeScheduler;
-import org.apache.lucene.index.SlowMultiReaderWrapper;
+import org.apache.lucene.index.SlowCompositeReaderWrapper;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.util.Bits;
@@ -41,21 +42,21 @@
     RandomIndexWriter writer = new RandomIndexWriter(random, dir);
     writer.close();
 
-    IndexReader reader = new SlowMultiReaderWrapper(IndexReader.open(dir));
+    IndexReader reader = SlowCompositeReaderWrapper.wrap(IndexReader.open(dir));
     AtomicReaderContext context = (AtomicReaderContext) reader.getTopReaderContext();
     MockFilter filter = new MockFilter();
     CachingWrapperFilter cacher = new CachingWrapperFilter(filter);
 
     // first time, nested filter is called
-    DocIdSet strongRef = cacher.getDocIdSet(context, context.reader.getLiveDocs());
+    DocIdSet strongRef = cacher.getDocIdSet(context, context.reader().getLiveDocs());
     assertTrue("first time", filter.wasCalled());
 
     // make sure no exception if cache is holding the wrong docIdSet
-    cacher.getDocIdSet(context, context.reader.getLiveDocs());
+    cacher.getDocIdSet(context, context.reader().getLiveDocs());
 
     // second time, nested filter should not be called
     filter.clear();
-    cacher.getDocIdSet(context, context.reader.getLiveDocs());
+    cacher.getDocIdSet(context, context.reader().getLiveDocs());
     assertFalse("second time", filter.wasCalled());
 
     reader.close();
@@ -67,7 +68,7 @@
     RandomIndexWriter writer = new RandomIndexWriter(random, dir);
     writer.close();
 
-    IndexReader reader = new SlowMultiReaderWrapper(IndexReader.open(dir));
+    IndexReader reader = SlowCompositeReaderWrapper.wrap(IndexReader.open(dir));
     AtomicReaderContext context = (AtomicReaderContext) reader.getTopReaderContext();
 
     final Filter filter = new Filter() {
@@ -79,7 +80,7 @@
     CachingWrapperFilter cacher = new CachingWrapperFilter(filter);
 
     // the caching filter should return the empty set constant
-    assertSame(DocIdSet.EMPTY_DOCIDSET, cacher.getDocIdSet(context, context.reader.getLiveDocs()));
+    assertSame(DocIdSet.EMPTY_DOCIDSET, cacher.getDocIdSet(context, context.reader().getLiveDocs()));
     
     reader.close();
     dir.close();
@@ -90,7 +91,7 @@
     RandomIndexWriter writer = new RandomIndexWriter(random, dir);
     writer.close();
 
-    IndexReader reader = new SlowMultiReaderWrapper(IndexReader.open(dir));
+    IndexReader reader = SlowCompositeReaderWrapper.wrap(IndexReader.open(dir));
     AtomicReaderContext context = (AtomicReaderContext) reader.getTopReaderContext();
 
     final Filter filter = new Filter() {
@@ -107,18 +108,18 @@
     CachingWrapperFilter cacher = new CachingWrapperFilter(filter);
 
     // the caching filter should return the empty set constant
-    assertSame(DocIdSet.EMPTY_DOCIDSET, cacher.getDocIdSet(context, context.reader.getLiveDocs()));
+    assertSame(DocIdSet.EMPTY_DOCIDSET, cacher.getDocIdSet(context, context.reader().getLiveDocs()));
     
     reader.close();
     dir.close();
   }
   
   private static void assertDocIdSetCacheable(IndexReader reader, Filter filter, boolean shouldCacheable) throws IOException {
-    assertTrue(reader.getTopReaderContext().isAtomic);
+    assertTrue(reader.getTopReaderContext() instanceof AtomicReaderContext);
     AtomicReaderContext context = (AtomicReaderContext) reader.getTopReaderContext();
     final CachingWrapperFilter cacher = new CachingWrapperFilter(filter);
-    final DocIdSet originalSet = filter.getDocIdSet(context, context.reader.getLiveDocs());
-    final DocIdSet cachedSet = cacher.getDocIdSet(context, context.reader.getLiveDocs());
+    final DocIdSet originalSet = filter.getDocIdSet(context, context.reader().getLiveDocs());
+    final DocIdSet cachedSet = cacher.getDocIdSet(context, context.reader().getLiveDocs());
     assertTrue(cachedSet.isCacheable());
     assertEquals(shouldCacheable, originalSet.isCacheable());
     //System.out.println("Original: "+originalSet.getClass().getName()+" -- cached: "+cachedSet.getClass().getName());
@@ -135,7 +136,7 @@
     writer.addDocument(new Document());
     writer.close();
 
-    IndexReader reader = new SlowMultiReaderWrapper(IndexReader.open(dir));
+    IndexReader reader = SlowCompositeReaderWrapper.wrap(IndexReader.open(dir));
 
     // not cacheable:
     assertDocIdSetCacheable(reader, new QueryWrapperFilter(new TermQuery(new Term("test","value"))), false);
@@ -147,7 +148,7 @@
     assertDocIdSetCacheable(reader, new Filter() {
       @Override
       public DocIdSet getDocIdSet(AtomicReaderContext context, Bits acceptDocs) {
-        return new FixedBitSet(context.reader.maxDoc());
+        return new FixedBitSet(context.reader().maxDoc());
       }
     }, true);
 
@@ -171,7 +172,7 @@
     // flipping a coin) may give us a newly opened reader,
     // but we use .reopen on this reader below and expect to
     // (must) get an NRT reader:
-    IndexReader reader = IndexReader.open(writer.w, true);
+    DirectoryReader reader = IndexReader.open(writer.w, true);
     // same reason we don't wrap?
     IndexSearcher searcher = newSearcher(reader, false);
 
@@ -298,9 +299,9 @@
     dir.close();
   }
 
-  private static IndexReader refreshReader(IndexReader reader) throws IOException {
-    IndexReader oldReader = reader;
-    reader = IndexReader.openIfChanged(reader);
+  private static DirectoryReader refreshReader(DirectoryReader reader) throws IOException {
+    DirectoryReader oldReader = reader;
+    reader = DirectoryReader.openIfChanged(reader);
     if (reader != null) {
       oldReader.close();
       return reader;
Index: lucene/src/test/org/apache/lucene/search/TestConstantScoreQuery.java
===================================================================
--- lucene/src/test/org/apache/lucene/search/TestConstantScoreQuery.java	(revision 1238034)
+++ lucene/src/test/org/apache/lucene/search/TestConstantScoreQuery.java	(working copy)
@@ -19,8 +19,8 @@
 
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.StringField;
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
 import org.apache.lucene.index.RandomIndexWriter;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.search.similarities.DefaultSimilarityProvider;
Index: lucene/src/test/org/apache/lucene/search/TestDisjunctionMaxQuery.java
===================================================================
--- lucene/src/test/org/apache/lucene/search/TestDisjunctionMaxQuery.java	(revision 1238034)
+++ lucene/src/test/org/apache/lucene/search/TestDisjunctionMaxQuery.java	(working copy)
@@ -22,10 +22,10 @@
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.FieldType;
 import org.apache.lucene.document.TextField;
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
 import org.apache.lucene.index.Norm;
-import org.apache.lucene.index.SlowMultiReaderWrapper;
+import org.apache.lucene.index.SlowCompositeReaderWrapper;
 import org.apache.lucene.index.FieldInvertState;
 import org.apache.lucene.index.RandomIndexWriter;
 import org.apache.lucene.index.Term;
@@ -156,7 +156,7 @@
       writer.addDocument(d4);
     }
     
-    r = new SlowMultiReaderWrapper(writer.getReader());
+    r = SlowCompositeReaderWrapper.wrap(writer.getReader());
     writer.close();
     s = newSearcher(r);
     s.setSimilarityProvider(sim);
@@ -175,10 +175,10 @@
     dq.add(tq("dek", "DOES_NOT_EXIST"));
     
     QueryUtils.check(random, dq, s);
-    assertTrue(s.getTopReaderContext().isAtomic);
+    assertTrue(s.getTopReaderContext() instanceof AtomicReaderContext);
     final Weight dw = s.createNormalizedWeight(dq);
     AtomicReaderContext context = (AtomicReaderContext)s.getTopReaderContext();
-    final Scorer ds = dw.scorer(context, true, false, context.reader.getLiveDocs());
+    final Scorer ds = dw.scorer(context, true, false, context.reader().getLiveDocs());
     final boolean skipOk = ds.advance(3) != DocIdSetIterator.NO_MORE_DOCS;
     if (skipOk) {
       fail("firsttime skipTo found a match? ... "
@@ -190,11 +190,11 @@
     final DisjunctionMaxQuery dq = new DisjunctionMaxQuery(0.0f);
     dq.add(tq("dek", "albino"));
     dq.add(tq("dek", "DOES_NOT_EXIST"));
-    assertTrue(s.getTopReaderContext().isAtomic);
+    assertTrue(s.getTopReaderContext() instanceof AtomicReaderContext);
     QueryUtils.check(random, dq, s);
     final Weight dw = s.createNormalizedWeight(dq);
     AtomicReaderContext context = (AtomicReaderContext)s.getTopReaderContext();
-    final Scorer ds = dw.scorer(context, true, false, context.reader.getLiveDocs());
+    final Scorer ds = dw.scorer(context, true, false, context.reader().getLiveDocs());
     assertTrue("firsttime skipTo found no match",
         ds.advance(3) != DocIdSetIterator.NO_MORE_DOCS);
     assertEquals("found wrong docid", "d4", r.document(ds.docID()).get("id"));
Index: lucene/src/test/org/apache/lucene/search/TestDocBoost.java
===================================================================
--- lucene/src/test/org/apache/lucene/search/TestDocBoost.java	(revision 1238034)
+++ lucene/src/test/org/apache/lucene/search/TestDocBoost.java	(working copy)
@@ -21,7 +21,7 @@
 
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.document.*;
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.RandomIndexWriter;
 import org.apache.lucene.index.Term;
Index: lucene/src/test/org/apache/lucene/search/TestDocIdSet.java
===================================================================
--- lucene/src/test/org/apache/lucene/search/TestDocIdSet.java	(revision 1238034)
+++ lucene/src/test/org/apache/lucene/search/TestDocIdSet.java	(working copy)
@@ -26,8 +26,8 @@
 
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.StringField;
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
 import org.apache.lucene.index.RandomIndexWriter;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.util.Bits;
Index: lucene/src/test/org/apache/lucene/search/TestDocValuesScoring.java
===================================================================
--- lucene/src/test/org/apache/lucene/search/TestDocValuesScoring.java	(revision 1238034)
+++ lucene/src/test/org/apache/lucene/search/TestDocValuesScoring.java	(working copy)
@@ -24,10 +24,10 @@
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.TextField;
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.DocValues.Source;
 import org.apache.lucene.index.DocValues;
 import org.apache.lucene.index.FieldInvertState;
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.Norm;
 import org.apache.lucene.index.RandomIndexWriter;
@@ -165,7 +165,7 @@
     @Override
     public ExactDocScorer exactDocScorer(Stats stats, String fieldName, AtomicReaderContext context) throws IOException {
       final ExactDocScorer sub = sim.exactDocScorer(stats, fieldName, context);
-      final Source values = context.reader.docValues(boostField).getSource();
+      final Source values = context.reader().docValues(boostField).getSource();
 
       return new ExactDocScorer() {
         @Override
@@ -188,7 +188,7 @@
     @Override
     public SloppyDocScorer sloppyDocScorer(Stats stats, String fieldName, AtomicReaderContext context) throws IOException {
       final SloppyDocScorer sub = sim.sloppyDocScorer(stats, fieldName, context);
-      final Source values = context.reader.docValues(boostField).getSource();
+      final Source values = context.reader().docValues(boostField).getSource();
       
       return new SloppyDocScorer() {
         @Override
Index: lucene/src/test/org/apache/lucene/search/TestElevationComparator.java
===================================================================
--- lucene/src/test/org/apache/lucene/search/TestElevationComparator.java	(revision 1238034)
+++ lucene/src/test/org/apache/lucene/search/TestElevationComparator.java	(working copy)
@@ -21,7 +21,6 @@
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.TextField;
 import org.apache.lucene.index.*;
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
 import org.apache.lucene.search.FieldValueHitQueue.Entry;
 import org.apache.lucene.search.similarities.DefaultSimilarityProvider;
 import org.apache.lucene.store.*;
@@ -181,7 +180,7 @@
 
      @Override
      public FieldComparator setNextReader(AtomicReaderContext context) throws IOException {
-       idIndex = FieldCache.DEFAULT.getTermsIndex(context.reader, fieldname);
+       idIndex = FieldCache.DEFAULT.getTermsIndex(context.reader(), fieldname);
        return this;
      }
 
Index: lucene/src/test/org/apache/lucene/search/TestFieldCache.java
===================================================================
--- lucene/src/test/org/apache/lucene/search/TestFieldCache.java	(revision 1238034)
+++ lucene/src/test/org/apache/lucene/search/TestFieldCache.java	(working copy)
@@ -41,7 +41,7 @@
 import org.junit.BeforeClass;
 
 public class TestFieldCache extends LuceneTestCase {
-  private static IndexReader reader;
+  private static AtomicReader reader;
   private static int NUM_DOCS;
   private static int NUM_ORDS;
   private static String[] unicodeStrings;
@@ -99,7 +99,7 @@
       writer.addDocument(doc);
     }
     IndexReader r = writer.getReader();
-    reader = new SlowMultiReaderWrapper(r);
+    reader = SlowCompositeReaderWrapper.wrap(r);
     writer.close();
   }
 
@@ -293,11 +293,12 @@
   public void testEmptyIndex() throws Exception {
     Directory dir = newDirectory();
     IndexWriter writer= new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMaxBufferedDocs(500));
-    IndexReader r = IndexReader.open(writer, true);
-    SlowMultiReaderWrapper reader = new SlowMultiReaderWrapper(r);
+    writer.close();
+    IndexReader r = DirectoryReader.open(dir);
+    AtomicReader reader = SlowCompositeReaderWrapper.wrap(r);
     FieldCache.DEFAULT.getTerms(reader, "foobar");
     FieldCache.DEFAULT.getTermsIndex(reader, "foobar");
-    writer.close();
+    FieldCache.DEFAULT.purge(reader);
     r.close();
     dir.close();
   }
Index: lucene/src/test/org/apache/lucene/search/TestFilteredQuery.java
===================================================================
--- lucene/src/test/org/apache/lucene/search/TestFilteredQuery.java	(revision 1238034)
+++ lucene/src/test/org/apache/lucene/search/TestFilteredQuery.java	(working copy)
@@ -22,7 +22,7 @@
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.TextField;
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.RandomIndexWriter;
 import org.apache.lucene.index.Term;
Index: lucene/src/test/org/apache/lucene/search/TestFilteredSearch.java
===================================================================
--- lucene/src/test/org/apache/lucene/search/TestFilteredSearch.java	(revision 1238034)
+++ lucene/src/test/org/apache/lucene/search/TestFilteredSearch.java	(working copy)
@@ -23,8 +23,8 @@
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.StringField;
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.CorruptIndexException;
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexWriter;
 import org.apache.lucene.index.Term;
@@ -101,10 +101,9 @@
     @Override
     public DocIdSet getDocIdSet(AtomicReaderContext context, Bits acceptDocs) {
       assertNull("acceptDocs should be null, as we have an index without deletions", acceptDocs);
-      assert context.isAtomic;
-      final FixedBitSet set = new FixedBitSet(context.reader.maxDoc());
+      final FixedBitSet set = new FixedBitSet(context.reader().maxDoc());
       int docBase = context.docBase;
-      final int limit = docBase+context.reader.maxDoc();
+      final int limit = docBase+context.reader().maxDoc();
       for (;index < docs.length; index++) {
         final int docId = docs[index];
         if(docId > limit)
Index: lucene/src/test/org/apache/lucene/search/TestMultiTermConstantScore.java
===================================================================
--- lucene/src/test/org/apache/lucene/search/TestMultiTermConstantScore.java	(revision 1238034)
+++ lucene/src/test/org/apache/lucene/search/TestMultiTermConstantScore.java	(working copy)
@@ -22,8 +22,8 @@
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.FieldType;
 import org.apache.lucene.document.TextField;
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
 import org.apache.lucene.index.RandomIndexWriter;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.search.similarities.DefaultSimilarityProvider;
Index: lucene/src/test/org/apache/lucene/search/TestNumericRangeQuery32.java
===================================================================
--- lucene/src/test/org/apache/lucene/search/TestNumericRangeQuery32.java	(revision 1238034)
+++ lucene/src/test/org/apache/lucene/search/TestNumericRangeQuery32.java	(working copy)
@@ -21,11 +21,11 @@
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.FieldType;
 import org.apache.lucene.document.NumericField;
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.MultiFields;
 import org.apache.lucene.index.RandomIndexWriter;
-import org.apache.lucene.index.SlowMultiReaderWrapper;
+import org.apache.lucene.index.SlowCompositeReaderWrapper;
 import org.apache.lucene.index.Terms;
 import org.apache.lucene.index.TermsEnum;
 import org.apache.lucene.store.Directory;
@@ -192,15 +192,15 @@
   
   @Test
   public void testInverseRange() throws Exception {
-    AtomicReaderContext context = (AtomicReaderContext) new SlowMultiReaderWrapper(reader).getTopReaderContext();
+    AtomicReaderContext context = SlowCompositeReaderWrapper.wrap(reader).getTopReaderContext();
     NumericRangeFilter<Integer> f = NumericRangeFilter.newIntRange("field8", 8, 1000, -1000, true, true);
-    assertSame("A inverse range should return the EMPTY_DOCIDSET instance", DocIdSet.EMPTY_DOCIDSET, f.getDocIdSet(context, context.reader.getLiveDocs()));
+    assertSame("A inverse range should return the EMPTY_DOCIDSET instance", DocIdSet.EMPTY_DOCIDSET, f.getDocIdSet(context, context.reader().getLiveDocs()));
     f = NumericRangeFilter.newIntRange("field8", 8, Integer.MAX_VALUE, null, false, false);
     assertSame("A exclusive range starting with Integer.MAX_VALUE should return the EMPTY_DOCIDSET instance",
-               DocIdSet.EMPTY_DOCIDSET, f.getDocIdSet(context, context.reader.getLiveDocs()));
+               DocIdSet.EMPTY_DOCIDSET, f.getDocIdSet(context, context.reader().getLiveDocs()));
     f = NumericRangeFilter.newIntRange("field8", 8, null, Integer.MIN_VALUE, false, false);
     assertSame("A exclusive range ending with Integer.MIN_VALUE should return the EMPTY_DOCIDSET instance",
-               DocIdSet.EMPTY_DOCIDSET, f.getDocIdSet(context, context.reader.getLiveDocs()));
+               DocIdSet.EMPTY_DOCIDSET, f.getDocIdSet(context, context.reader().getLiveDocs()));
   }
   
   @Test
Index: lucene/src/test/org/apache/lucene/search/TestNumericRangeQuery64.java
===================================================================
--- lucene/src/test/org/apache/lucene/search/TestNumericRangeQuery64.java	(revision 1238034)
+++ lucene/src/test/org/apache/lucene/search/TestNumericRangeQuery64.java	(working copy)
@@ -21,11 +21,11 @@
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.FieldType;
 import org.apache.lucene.document.NumericField;
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.MultiFields;
 import org.apache.lucene.index.RandomIndexWriter;
-import org.apache.lucene.index.SlowMultiReaderWrapper;
+import org.apache.lucene.index.SlowCompositeReaderWrapper;
 import org.apache.lucene.index.Terms;
 import org.apache.lucene.index.TermsEnum;
 import org.apache.lucene.store.Directory;
@@ -206,16 +206,16 @@
   
   @Test
   public void testInverseRange() throws Exception {
-    AtomicReaderContext context = (AtomicReaderContext) new SlowMultiReaderWrapper(searcher.getIndexReader()).getTopReaderContext();
+    AtomicReaderContext context = SlowCompositeReaderWrapper.wrap(searcher.getIndexReader()).getTopReaderContext();
     NumericRangeFilter<Long> f = NumericRangeFilter.newLongRange("field8", 8, 1000L, -1000L, true, true);
     assertSame("A inverse range should return the EMPTY_DOCIDSET instance", DocIdSet.EMPTY_DOCIDSET,
-        f.getDocIdSet(context, context.reader.getLiveDocs()));
+        f.getDocIdSet(context, context.reader().getLiveDocs()));
     f = NumericRangeFilter.newLongRange("field8", 8, Long.MAX_VALUE, null, false, false);
     assertSame("A exclusive range starting with Long.MAX_VALUE should return the EMPTY_DOCIDSET instance",
-               DocIdSet.EMPTY_DOCIDSET, f.getDocIdSet(context, context.reader.getLiveDocs()));
+               DocIdSet.EMPTY_DOCIDSET, f.getDocIdSet(context, context.reader().getLiveDocs()));
     f = NumericRangeFilter.newLongRange("field8", 8, null, Long.MIN_VALUE, false, false);
     assertSame("A exclusive range ending with Long.MIN_VALUE should return the EMPTY_DOCIDSET instance",
-               DocIdSet.EMPTY_DOCIDSET, f.getDocIdSet(context, context.reader.getLiveDocs()));
+               DocIdSet.EMPTY_DOCIDSET, f.getDocIdSet(context, context.reader().getLiveDocs()));
   }
   
   @Test
Index: lucene/src/test/org/apache/lucene/search/TestPositionIncrement.java
===================================================================
--- lucene/src/test/org/apache/lucene/search/TestPositionIncrement.java	(revision 1238034)
+++ lucene/src/test/org/apache/lucene/search/TestPositionIncrement.java	(working copy)
@@ -28,11 +28,12 @@
 import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.TextField;
+import org.apache.lucene.index.AtomicReader;
 import org.apache.lucene.index.MultiFields;
 import org.apache.lucene.index.DocsAndPositionsEnum;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.RandomIndexWriter;
-import org.apache.lucene.index.SlowMultiReaderWrapper;
+import org.apache.lucene.index.SlowCompositeReaderWrapper;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.search.payloads.PayloadSpanUtil;
@@ -209,7 +210,7 @@
     writer.addDocument(doc);
 
     final IndexReader readerFromWriter = writer.getReader();
-    SlowMultiReaderWrapper r = new SlowMultiReaderWrapper(readerFromWriter);
+    AtomicReader r = SlowCompositeReaderWrapper.wrap(readerFromWriter);
 
     DocsAndPositionsEnum tp = r.termPositionsEnum(r.getLiveDocs(),
                                                   "content",
Index: lucene/src/test/org/apache/lucene/search/TestScoreCachingWrappingScorer.java
===================================================================
--- lucene/src/test/org/apache/lucene/search/TestScoreCachingWrappingScorer.java	(revision 1238034)
+++ lucene/src/test/org/apache/lucene/search/TestScoreCachingWrappingScorer.java	(working copy)
@@ -19,9 +19,9 @@
 
 import java.io.IOException;
 
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.RandomIndexWriter;
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.util.LuceneTestCase;
Index: lucene/src/test/org/apache/lucene/search/TestScorerPerf.java
===================================================================
--- lucene/src/test/org/apache/lucene/search/TestScorerPerf.java	(revision 1238034)
+++ lucene/src/test/org/apache/lucene/search/TestScorerPerf.java	(working copy)
@@ -7,8 +7,8 @@
 import java.util.BitSet;
 import java.io.IOException;
 
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
 import org.apache.lucene.index.IndexWriter;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.index.IndexWriterConfig.OpenMode;
Index: lucene/src/test/org/apache/lucene/search/TestShardSearching.java
===================================================================
--- lucene/src/test/org/apache/lucene/search/TestShardSearching.java	(revision 1238034)
+++ lucene/src/test/org/apache/lucene/search/TestShardSearching.java	(working copy)
@@ -22,6 +22,7 @@
 import java.util.Collections;
 import java.util.List;
 
+import org.apache.lucene.index.CompositeReader;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.MultiFields;
 import org.apache.lucene.index.MultiReader;
@@ -310,7 +311,7 @@
 
     final int numNodes = shardSearcher.nodeVersions.length;
     int[] base = new int[numNodes];
-    final IndexReader[] subs = mockSearcher.getIndexReader().getSequentialSubReaders();
+    final IndexReader[] subs = ((CompositeReader) mockSearcher.getIndexReader()).getSequentialSubReaders();
     assertEquals(numNodes, subs.length);
 
     int docCount = 0;
Index: lucene/src/test/org/apache/lucene/search/TestSimilarity.java
===================================================================
--- lucene/src/test/org/apache/lucene/search/TestSimilarity.java	(revision 1238034)
+++ lucene/src/test/org/apache/lucene/search/TestSimilarity.java	(working copy)
@@ -21,9 +21,9 @@
 
 import java.io.IOException;
 
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.FieldInvertState;
 import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
 import org.apache.lucene.index.Norm;
 import org.apache.lucene.index.RandomIndexWriter;
 import org.apache.lucene.index.Term;
Index: lucene/src/test/org/apache/lucene/search/TestSloppyPhraseQuery.java
===================================================================
--- lucene/src/test/org/apache/lucene/search/TestSloppyPhraseQuery.java	(revision 1238034)
+++ lucene/src/test/org/apache/lucene/search/TestSloppyPhraseQuery.java	(working copy)
@@ -26,8 +26,8 @@
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.FieldType;
 import org.apache.lucene.document.TextField;
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
 import org.apache.lucene.index.RandomIndexWriter;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.store.Directory;
Index: lucene/src/test/org/apache/lucene/search/TestSort.java
===================================================================
--- lucene/src/test/org/apache/lucene/search/TestSort.java	(revision 1238034)
+++ lucene/src/test/org/apache/lucene/search/TestSort.java	(working copy)
@@ -32,8 +32,8 @@
 import org.apache.lucene.document.DocValuesField;
 import org.apache.lucene.document.StringField;
 import org.apache.lucene.document.TextField;
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.CorruptIndexException;
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexWriter;
 import org.apache.lucene.index.IndexableField;
@@ -688,7 +688,7 @@
 
     @Override
     public FieldComparator setNextReader(AtomicReaderContext context) throws IOException {
-      docValues = FieldCache.DEFAULT.getInts(context.reader, "parser", testIntParser, false);
+      docValues = FieldCache.DEFAULT.getInts(context.reader(), "parser", testIntParser, false);
       return this;
     }
 
@@ -858,8 +858,8 @@
       @Override
       public DocIdSet getDocIdSet (AtomicReaderContext context, Bits acceptDocs) {
         assertNull("acceptDocs should be null, as we have no deletions", acceptDocs);
-        BitSet bs = new BitSet(context.reader.maxDoc());
-        bs.set(0, context.reader.maxDoc());
+        BitSet bs = new BitSet(context.reader().maxDoc());
+        bs.set(0, context.reader().maxDoc());
         bs.set(docs1.scoreDocs[0].doc);
         return new DocIdBitSet(bs);
       }
Index: lucene/src/test/org/apache/lucene/search/TestSubScorerFreqs.java
===================================================================
--- lucene/src/test/org/apache/lucene/search/TestSubScorerFreqs.java	(revision 1238034)
+++ lucene/src/test/org/apache/lucene/search/TestSubScorerFreqs.java	(working copy)
@@ -23,7 +23,6 @@
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.document.*;
 import org.apache.lucene.index.*;
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
 import org.apache.lucene.search.BooleanClause.Occur;
 import org.apache.lucene.search.Scorer.ChildScorer;
 import org.apache.lucene.store.*;
Index: lucene/src/test/org/apache/lucene/search/TestTermScorer.java
===================================================================
--- lucene/src/test/org/apache/lucene/search/TestTermScorer.java	(revision 1238034)
+++ lucene/src/test/org/apache/lucene/search/TestTermScorer.java	(working copy)
@@ -24,10 +24,10 @@
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.TextField;
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.RandomIndexWriter;
-import org.apache.lucene.index.SlowMultiReaderWrapper;
+import org.apache.lucene.index.SlowCompositeReaderWrapper;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.search.similarities.DefaultSimilarityProvider;
 import org.apache.lucene.store.Directory;
@@ -57,7 +57,7 @@
           .add(newField(FIELD, values[i], TextField.TYPE_STORED));
       writer.addDocument(doc);
     }
-    indexReader = new SlowMultiReaderWrapper(writer.getReader());
+    indexReader = SlowCompositeReaderWrapper.wrap(writer.getReader());
     writer.close();
     indexSearcher = newSearcher(indexReader);
     indexSearcher.setSimilarityProvider(new DefaultSimilarityProvider());
@@ -76,9 +76,9 @@
     TermQuery termQuery = new TermQuery(allTerm);
     
     Weight weight = indexSearcher.createNormalizedWeight(termQuery);
-    assertTrue(indexSearcher.getTopReaderContext().isAtomic);
+    assertTrue(indexSearcher.getTopReaderContext() instanceof AtomicReaderContext);
     AtomicReaderContext context = (AtomicReaderContext)indexSearcher.getTopReaderContext();
-    Scorer ts = weight.scorer(context, true, true, context.reader.getLiveDocs());
+    Scorer ts = weight.scorer(context, true, true, context.reader().getLiveDocs());
     // we have 2 documents with the term all in them, one document for all the
     // other values
     final List<TestHit> docs = new ArrayList<TestHit>();
@@ -138,9 +138,9 @@
     TermQuery termQuery = new TermQuery(allTerm);
     
     Weight weight = indexSearcher.createNormalizedWeight(termQuery);
-    assertTrue(indexSearcher.getTopReaderContext().isAtomic);
+    assertTrue(indexSearcher.getTopReaderContext() instanceof AtomicReaderContext);
     AtomicReaderContext context = (AtomicReaderContext) indexSearcher.getTopReaderContext();
-    Scorer ts = weight.scorer(context, true, true, context.reader.getLiveDocs());
+    Scorer ts = weight.scorer(context, true, true, context.reader().getLiveDocs());
     assertTrue("next did not return a doc",
         ts.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);
     assertTrue("score is not correct", ts.score() == 1.6931472f);
@@ -157,9 +157,9 @@
     TermQuery termQuery = new TermQuery(allTerm);
     
     Weight weight = indexSearcher.createNormalizedWeight(termQuery);
-    assertTrue(indexSearcher.getTopReaderContext().isAtomic);
+    assertTrue(indexSearcher.getTopReaderContext() instanceof AtomicReaderContext);
     AtomicReaderContext context = (AtomicReaderContext) indexSearcher.getTopReaderContext();
-    Scorer ts = weight.scorer(context, true, true, context.reader.getLiveDocs());
+    Scorer ts = weight.scorer(context, true, true, context.reader().getLiveDocs());
     assertTrue("Didn't skip", ts.advance(3) != DocIdSetIterator.NO_MORE_DOCS);
     // The next doc should be doc 5
     assertTrue("doc should be number 5", ts.docID() == 5);
Index: lucene/src/test/org/apache/lucene/search/TestTimeLimitingCollector.java
===================================================================
--- lucene/src/test/org/apache/lucene/search/TestTimeLimitingCollector.java	(revision 1238034)
+++ lucene/src/test/org/apache/lucene/search/TestTimeLimitingCollector.java	(working copy)
@@ -23,8 +23,8 @@
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.TextField;
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
 import org.apache.lucene.index.RandomIndexWriter;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.search.TimeLimitingCollector.TimeExceededException;
Index: lucene/src/test/org/apache/lucene/search/TestTopDocsCollector.java
===================================================================
--- lucene/src/test/org/apache/lucene/search/TestTopDocsCollector.java	(revision 1238034)
+++ lucene/src/test/org/apache/lucene/search/TestTopDocsCollector.java	(working copy)
@@ -20,8 +20,8 @@
 import java.io.IOException;
 
 import org.apache.lucene.document.Document;
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
 import org.apache.lucene.index.RandomIndexWriter;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.util.LuceneTestCase;
Index: lucene/src/test/org/apache/lucene/search/TestTopDocsMerge.java
===================================================================
--- lucene/src/test/org/apache/lucene/search/TestTopDocsMerge.java	(revision 1238034)
+++ lucene/src/test/org/apache/lucene/search/TestTopDocsMerge.java	(working copy)
@@ -25,8 +25,11 @@
 import org.apache.lucene.document.NumericField;
 import org.apache.lucene.document.StringField;
 import org.apache.lucene.document.TextField;
+import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.CompositeReaderContext;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.RandomIndexWriter;
+import org.apache.lucene.index.IndexReaderContext;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.util.LuceneTestCase;
@@ -36,11 +39,11 @@
 public class TestTopDocsMerge extends LuceneTestCase {
 
   private static class ShardSearcher extends IndexSearcher {
-    private final IndexReader.AtomicReaderContext[] ctx;
+    private final AtomicReaderContext[] ctx;
 
-    public ShardSearcher(IndexReader.AtomicReaderContext ctx, IndexReader.ReaderContext parent) {
+    public ShardSearcher(AtomicReaderContext ctx, CompositeReaderContext parent) {
       super(parent);
-      this.ctx = new IndexReader.AtomicReaderContext[] {ctx};
+      this.ctx = new AtomicReaderContext[] {ctx};
     }
 
     public void search(Weight weight, Collector collector) throws IOException {
@@ -116,20 +119,25 @@
     // NOTE: sometimes reader has just one segment, which is
     // important to test
     final IndexSearcher searcher = newSearcher(reader);
-    IndexReader[] subReaders = searcher.getIndexReader().getSequentialSubReaders();
-    if (subReaders == null) {
-      subReaders = new IndexReader[] {searcher.getIndexReader()};
-    }
-    final ShardSearcher[] subSearchers = new ShardSearcher[subReaders.length];
-    final IndexReader.ReaderContext ctx = searcher.getTopReaderContext();
+    final IndexReaderContext ctx = searcher.getTopReaderContext();
 
-    if (ctx instanceof IndexReader.AtomicReaderContext) {
-      assert subSearchers.length == 1;
-      subSearchers[0] = new ShardSearcher((IndexReader.AtomicReaderContext) ctx, ctx);
+    final ShardSearcher[] subSearchers;
+    final int[] docStarts;
+    
+    if (ctx instanceof AtomicReaderContext) {
+      subSearchers = new ShardSearcher[1];
+      docStarts = new int[1];
+      subSearchers[0] = new ShardSearcher((AtomicReaderContext) ctx, null);
+      docStarts[0] = 0;
     } else {
-      final IndexReader.CompositeReaderContext compCTX = (IndexReader.CompositeReaderContext) ctx;
+      final CompositeReaderContext compCTX = (CompositeReaderContext) ctx;
+      subSearchers = new ShardSearcher[compCTX.leaves().length];
+      docStarts = new int[compCTX.leaves().length];
+      int docBase = 0;
       for(int searcherIDX=0;searcherIDX<subSearchers.length;searcherIDX++) { 
-        subSearchers[searcherIDX] = new ShardSearcher(compCTX.leaves[searcherIDX], compCTX);
+        subSearchers[searcherIDX] = new ShardSearcher(compCTX.leaves()[searcherIDX], compCTX);
+        docStarts[searcherIDX] = docBase;
+        docBase += compCTX.leaves()[searcherIDX].reader().maxDoc();
       }
     }
 
@@ -145,14 +153,6 @@
     sortFields.add(new SortField(null, SortField.Type.DOC, true));
     sortFields.add(new SortField(null, SortField.Type.DOC, false));
 
-    final int[] docStarts = new int[subSearchers.length];
-    int docBase = 0;
-    for(int subIDX=0;subIDX<docStarts.length;subIDX++) {
-      docStarts[subIDX] = docBase;
-      docBase += subReaders[subIDX].maxDoc();
-      //System.out.println("docStarts[" + subIDX + "]=" + docStarts[subIDX]);
-    }
-
     for(int iter=0;iter<1000*RANDOM_MULTIPLIER;iter++) {
 
       // TODO: custom FieldComp...
Index: lucene/src/test/org/apache/lucene/store/TestNRTCachingDirectory.java
===================================================================
--- lucene/src/test/org/apache/lucene/store/TestNRTCachingDirectory.java	(revision 1238034)
+++ lucene/src/test/org/apache/lucene/store/TestNRTCachingDirectory.java	(working copy)
@@ -26,6 +26,7 @@
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.document.Document;
+import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexWriter;
 import org.apache.lucene.index.IndexWriterConfig;
@@ -55,7 +56,7 @@
     }
 
     final List<BytesRef> ids = new ArrayList<BytesRef>();
-    IndexReader r = null;
+    DirectoryReader r = null;
     for(int docCount=0;docCount<numDocs;docCount++) {
       final Document doc = docs.nextDoc();
       ids.add(new BytesRef(doc.get("docid")));
@@ -64,7 +65,7 @@
         if (r == null) {
           r = IndexReader.open(w.w, false);
         } else {
-          final IndexReader r2 = IndexReader.openIfChanged(r);
+          final DirectoryReader r2 = DirectoryReader.openIfChanged(r);
           if (r2 != null) {
             r.close();
             r = r2;
Index: lucene/src/test/org/apache/lucene/TestExternalCodecs.java
===================================================================
--- lucene/src/test/org/apache/lucene/TestExternalCodecs.java	(revision 1238034)
+++ lucene/src/test/org/apache/lucene/TestExternalCodecs.java	(working copy)
@@ -93,7 +93,6 @@
     w.deleteDocuments(new Term("id", "77"));
 
     IndexReader r = IndexReader.open(w, true);
-    IndexReader[] subs = r.getSequentialSubReaders();
     
     assertEquals(NUM_DOCS-1, r.numDocs());
     IndexSearcher s = newSearcher(r);
Index: lucene/src/test/org/apache/lucene/util/TestFieldCacheSanityChecker.java
===================================================================
--- lucene/src/test/org/apache/lucene/util/TestFieldCacheSanityChecker.java	(revision 1238034)
+++ lucene/src/test/org/apache/lucene/util/TestFieldCacheSanityChecker.java	(working copy)
@@ -20,10 +20,11 @@
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.StringField;
 import org.apache.lucene.search.FieldCache;
-import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.AtomicReader;
+import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.MultiReader;
 import org.apache.lucene.index.IndexWriter;
-import org.apache.lucene.index.SlowMultiReaderWrapper;
+import org.apache.lucene.index.SlowCompositeReaderWrapper;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.util.FieldCacheSanityChecker.Insanity;
 import org.apache.lucene.util.FieldCacheSanityChecker.InsanityType;
@@ -32,9 +33,10 @@
 
 public class TestFieldCacheSanityChecker extends LuceneTestCase {
 
-  protected IndexReader readerA;
-  protected IndexReader readerB;
-  protected IndexReader readerX;
+  protected AtomicReader readerA;
+  protected AtomicReader readerB;
+  protected AtomicReader readerX;
+  protected AtomicReader readerAclone;
   protected Directory dirA, dirB;
   private static final int NUM_DOCS = 1000;
 
@@ -69,14 +71,18 @@
     }
     wA.close();
     wB.close();
-    readerA = IndexReader.open(dirA);
-    readerB = IndexReader.open(dirB);
-    readerX = new MultiReader(readerA, readerB);
+    DirectoryReader rA = DirectoryReader.open(dirA);
+    readerA = SlowCompositeReaderWrapper.wrap(rA);
+    readerAclone = SlowCompositeReaderWrapper.wrap(rA);
+    readerA = SlowCompositeReaderWrapper.wrap(DirectoryReader.open(dirA));
+    readerB = SlowCompositeReaderWrapper.wrap(DirectoryReader.open(dirB));
+    readerX = SlowCompositeReaderWrapper.wrap(new MultiReader(readerA, readerB));
   }
 
   @Override
   public void tearDown() throws Exception {
     readerA.close();
+    readerAclone.close();
     readerB.close();
     readerX.close();
     dirA.close();
@@ -88,12 +94,13 @@
     FieldCache cache = FieldCache.DEFAULT;
     cache.purgeAllCaches();
 
-    cache.getDoubles(new SlowMultiReaderWrapper(readerA), "theDouble", false);
-    cache.getDoubles(new SlowMultiReaderWrapper(readerA), "theDouble", FieldCache.DEFAULT_DOUBLE_PARSER, false);
-    cache.getDoubles(new SlowMultiReaderWrapper(readerB), "theDouble", FieldCache.DEFAULT_DOUBLE_PARSER, false);
+    cache.getDoubles(readerA, "theDouble", false);
+    cache.getDoubles(readerA, "theDouble", FieldCache.DEFAULT_DOUBLE_PARSER, false);
+    cache.getDoubles(readerAclone, "theDouble", FieldCache.DEFAULT_DOUBLE_PARSER, false);
+    cache.getDoubles(readerB, "theDouble", FieldCache.DEFAULT_DOUBLE_PARSER, false);
 
-    cache.getInts(new SlowMultiReaderWrapper(readerX), "theInt", false);
-    cache.getInts(new SlowMultiReaderWrapper(readerX), "theInt", FieldCache.DEFAULT_INT_PARSER, false);
+    cache.getInts(readerX, "theInt", false);
+    cache.getInts(readerX, "theInt", FieldCache.DEFAULT_INT_PARSER, false);
 
     // // // 
 
@@ -111,9 +118,9 @@
     FieldCache cache = FieldCache.DEFAULT;
     cache.purgeAllCaches();
 
-    cache.getInts(new SlowMultiReaderWrapper(readerX), "theInt", FieldCache.DEFAULT_INT_PARSER, false);
-    cache.getTerms(new SlowMultiReaderWrapper(readerX), "theInt");
-    cache.getBytes(new SlowMultiReaderWrapper(readerX), "theByte", false);
+    cache.getInts(readerX, "theInt", FieldCache.DEFAULT_INT_PARSER, false);
+    cache.getTerms(readerX, "theInt");
+    cache.getBytes(readerX, "theByte", false);
 
     // // // 
 
@@ -131,36 +138,4 @@
     cache.purgeAllCaches();
   }
 
-  public void testInsanity2() throws IOException {
-    FieldCache cache = FieldCache.DEFAULT;
-    cache.purgeAllCaches();
-
-    cache.getTerms(new SlowMultiReaderWrapper(readerA), "theString");
-    cache.getTerms(new SlowMultiReaderWrapper(readerB), "theString");
-    cache.getTerms(new SlowMultiReaderWrapper(readerX), "theString");
-
-    cache.getBytes(new SlowMultiReaderWrapper(readerX), "theByte", false);
-
-
-    // // // 
-
-    Insanity[] insanity = 
-      FieldCacheSanityChecker.checkSanity(cache.getCacheEntries());
-    
-    assertEquals("wrong number of cache errors", 1, insanity.length);
-    assertEquals("wrong type of cache error", 
-                 InsanityType.SUBREADER,
-                 insanity[0].getType());
-    assertEquals("wrong number of entries in cache error", 3,
-                 insanity[0].getCacheEntries().length);
-
-    // we expect bad things, don't let tearDown complain about them
-    cache.purgeAllCaches();
-  }
-  
-  public void testInsanity3() throws IOException {
-
-    // :TODO: subreader tree walking is really hairy ... add more crazy tests.
-  }
-
 }
Index: modules/analysis/common/src/java/org/apache/lucene/analysis/query/QueryAutoStopWordAnalyzer.java
===================================================================
--- modules/analysis/common/src/java/org/apache/lucene/analysis/query/QueryAutoStopWordAnalyzer.java	(revision 1238034)
+++ modules/analysis/common/src/java/org/apache/lucene/analysis/query/QueryAutoStopWordAnalyzer.java	(working copy)
@@ -29,7 +29,6 @@
 import org.apache.lucene.index.TermsEnum;
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.CharsRef;
-import org.apache.lucene.util.ReaderUtil;
 import org.apache.lucene.util.UnicodeUtil;
 import org.apache.lucene.util.Version;
 
@@ -85,7 +84,7 @@
       Analyzer delegate,
       IndexReader indexReader,
       int maxDocFreq) throws IOException {
-    this(matchVersion, delegate, indexReader, ReaderUtil.getIndexedFields(indexReader), maxDocFreq);
+    this(matchVersion, delegate, indexReader, MultiFields.getIndexedFields(indexReader), maxDocFreq);
   }
 
   /**
@@ -105,7 +104,7 @@
       Analyzer delegate,
       IndexReader indexReader,
       float maxPercentDocs) throws IOException {
-    this(matchVersion, delegate, indexReader, ReaderUtil.getIndexedFields(indexReader), maxPercentDocs);
+    this(matchVersion, delegate, indexReader, MultiFields.getIndexedFields(indexReader), maxPercentDocs);
   }
 
   /**
Index: modules/analysis/kuromoji
===================================================================
--- modules/analysis/kuromoji	(revision 1238034)
+++ modules/analysis/kuromoji	(working copy)

Property changes on: modules/analysis/kuromoji
___________________________________________________________________
Modified: svn:mergeinfo
   Merged /lucene/dev/branches/lucene2858/modules/analysis/kuromoji:r1234440-1238051
Index: modules/benchmark
===================================================================
--- modules/benchmark	(revision 1238034)
+++ modules/benchmark	(working copy)

Property changes on: modules/benchmark
___________________________________________________________________
Modified: svn:mergeinfo
   Merged /lucene/dev/branches/lucene2858/modules/benchmark:r1234440-1238051
Index: modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/PerfRunData.java
===================================================================
--- modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/PerfRunData.java	(revision 1238034)
+++ modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/PerfRunData.java	(working copy)
@@ -37,6 +37,7 @@
 import org.apache.lucene.benchmark.byTask.tasks.NewAnalyzerTask;
 import org.apache.lucene.facet.taxonomy.TaxonomyReader;
 import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
+import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexWriter;
 import org.apache.lucene.search.IndexSearcher;
@@ -90,7 +91,7 @@
   private HashMap<Class<? extends ReadTask>,QueryMaker> readTaskQueryMaker;
   private Class<? extends QueryMaker> qmkrClass;
 
-  private IndexReader indexReader;
+  private DirectoryReader indexReader;
   private IndexSearcher indexSearcher;
   private IndexWriter indexWriter;
   private Config config;
@@ -288,7 +289,7 @@
    * reference.  You must call IndexReader.decRef() when
    * you're done.
    */
-  public synchronized IndexReader getIndexReader() {
+  public synchronized DirectoryReader getIndexReader() {
     if (indexReader != null) {
       indexReader.incRef();
     }
@@ -314,7 +315,7 @@
    * the reader will remain open). 
    * @param indexReader The indexReader to set.
    */
-  public synchronized void setIndexReader(IndexReader indexReader) throws IOException {
+  public synchronized void setIndexReader(DirectoryReader indexReader) throws IOException {
     if (indexReader == this.indexReader) {
       return;
     }
Index: modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/NearRealtimeReaderTask.java
===================================================================
--- modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/NearRealtimeReaderTask.java	(revision 1238034)
+++ modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/NearRealtimeReaderTask.java	(working copy)
@@ -18,6 +18,7 @@
  */
 
 import org.apache.lucene.benchmark.byTask.PerfRunData;
+import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexWriter;
 import org.apache.lucene.util.ArrayUtil;
@@ -59,7 +60,7 @@
     }
     
     long t = System.currentTimeMillis();
-    IndexReader r = IndexReader.open(w, true);
+    DirectoryReader r = IndexReader.open(w, true);
     runData.setIndexReader(r);
     // Transfer our reference to runData
     r.decRef();
@@ -77,7 +78,7 @@
       }
 
       t = System.currentTimeMillis();
-      final IndexReader newReader = IndexReader.openIfChanged(r);
+      final DirectoryReader newReader = DirectoryReader.openIfChanged(r);
       if (newReader != null) {
         final int delay = (int) (System.currentTimeMillis()-t);
         if (reopenTimes.length == reopenCount) {
Index: modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/OpenReaderTask.java
===================================================================
--- modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/OpenReaderTask.java	(revision 1238034)
+++ modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/OpenReaderTask.java	(working copy)
@@ -22,6 +22,7 @@
 import java.util.Map;
 
 import org.apache.lucene.benchmark.byTask.PerfRunData;
+import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexCommit;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.store.Directory;
@@ -42,7 +43,7 @@
   @Override
   public int doLogic() throws IOException {
     Directory dir = getRunData().getDirectory();
-    IndexReader r = null;
+    DirectoryReader r = null;
     if (commitUserData != null) {
       r = IndexReader.open(OpenReaderTask.findIndexCommit(dir, commitUserData)); 
     } else {
@@ -71,7 +72,7 @@
   }
 
   public static IndexCommit findIndexCommit(Directory dir, String userData) throws IOException {
-    Collection<IndexCommit> commits = IndexReader.listCommits(dir);
+    Collection<IndexCommit> commits = DirectoryReader.listCommits(dir);
     for (final IndexCommit ic : commits) {
       Map<String,String> map = ic.getUserData();
       String ud = null;
Index: modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ReopenReaderTask.java
===================================================================
--- modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ReopenReaderTask.java	(revision 1238034)
+++ modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ReopenReaderTask.java	(working copy)
@@ -20,6 +20,7 @@
 import java.io.IOException;
 
 import org.apache.lucene.benchmark.byTask.PerfRunData;
+import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexReader;
 
 /**
@@ -33,8 +34,8 @@
 
   @Override
   public int doLogic() throws IOException {
-    IndexReader r = getRunData().getIndexReader();
-    IndexReader nr = IndexReader.openIfChanged(r);
+    DirectoryReader r = getRunData().getIndexReader();
+    DirectoryReader nr = DirectoryReader.openIfChanged(r);
     if (nr != null) {
       getRunData().setIndexReader(nr);
       nr.decRef();
Index: modules/benchmark/src/test/org/apache/lucene/benchmark/byTask/TestPerfTasksLogic.java
===================================================================
--- modules/benchmark/src/test/org/apache/lucene/benchmark/byTask/TestPerfTasksLogic.java	(revision 1238034)
+++ modules/benchmark/src/test/org/apache/lucene/benchmark/byTask/TestPerfTasksLogic.java	(working copy)
@@ -39,6 +39,7 @@
 import org.apache.lucene.benchmark.byTask.tasks.WriteLineDocTask;
 import org.apache.lucene.collation.CollationKeyAnalyzer;
 import org.apache.lucene.facet.taxonomy.TaxonomyReader;
+import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.DocsEnum;
 import org.apache.lucene.index.Fields;
 import org.apache.lucene.index.FieldsEnum;
@@ -51,7 +52,7 @@
 import org.apache.lucene.index.MultiFields;
 import org.apache.lucene.index.SegmentInfos;
 import org.apache.lucene.index.SerialMergeScheduler;
-import org.apache.lucene.index.SlowMultiReaderWrapper;
+import org.apache.lucene.index.SlowCompositeReaderWrapper;
 import org.apache.lucene.index.Terms;
 import org.apache.lucene.index.TermsEnum;
 import org.apache.lucene.search.FieldCache.DocTermsIndex;
@@ -97,7 +98,7 @@
 
     // 4. test specific checks after the benchmark run completed.
     assertEquals("TestSearchTask was supposed to be called!",279,CountingSearchTestTask.numSearches);
-    assertTrue("Index does not exist?...!", IndexReader.indexExists(benchmark.getRunData().getDirectory()));
+    assertTrue("Index does not exist?...!", DirectoryReader.indexExists(benchmark.getRunData().getDirectory()));
     // now we should be able to open the index for write. 
     IndexWriter iw = new IndexWriter(benchmark.getRunData().getDirectory(),
         new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random))
@@ -185,7 +186,7 @@
     //we probably should use a different doc/query maker, but...
     assertTrue("TestSearchTask was supposed to be called!", CountingHighlighterTestTask.numDocsRetrieved >= CountingHighlighterTestTask.numHighlightedResults && CountingHighlighterTestTask.numHighlightedResults > 0);
 
-    assertTrue("Index does not exist?...!", IndexReader.indexExists(benchmark.getRunData().getDirectory()));
+    assertTrue("Index does not exist?...!", DirectoryReader.indexExists(benchmark.getRunData().getDirectory()));
     // now we should be able to open the index for write.
     IndexWriter iw = new IndexWriter(benchmark.getRunData().getDirectory(), new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setOpenMode(OpenMode.APPEND));
     iw.close();
@@ -224,7 +225,7 @@
     //we probably should use a different doc/query maker, but...
     assertTrue("TestSearchTask was supposed to be called!", CountingHighlighterTestTask.numDocsRetrieved >= CountingHighlighterTestTask.numHighlightedResults && CountingHighlighterTestTask.numHighlightedResults > 0);
 
-    assertTrue("Index does not exist?...!", IndexReader.indexExists(benchmark.getRunData().getDirectory()));
+    assertTrue("Index does not exist?...!", DirectoryReader.indexExists(benchmark.getRunData().getDirectory()));
     // now we should be able to open the index for write.
     IndexWriter iw = new IndexWriter(benchmark.getRunData().getDirectory(), new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setOpenMode(OpenMode.APPEND));
     iw.close();
@@ -297,7 +298,7 @@
 
     // 4. test specific checks after the benchmark run completed.
     assertEquals("TestSearchTask was supposed to be called!",139,CountingSearchTestTask.numSearches);
-    assertTrue("Index does not exist?...!", IndexReader.indexExists(benchmark.getRunData().getDirectory()));
+    assertTrue("Index does not exist?...!", DirectoryReader.indexExists(benchmark.getRunData().getDirectory()));
     // now we should be able to open the index for write. 
     IndexWriter iw = new IndexWriter(benchmark.getRunData().getDirectory(), new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setOpenMode(OpenMode.APPEND));
     iw.close();
@@ -332,8 +333,8 @@
     // 3. execute the algorithm  (required in every "logic" test)
     Benchmark benchmark = execBenchmark(algLines);
 
-    IndexReader r = IndexReader.open(benchmark.getRunData().getDirectory());
-    DocTermsIndex idx = FieldCache.DEFAULT.getTermsIndex(new SlowMultiReaderWrapper(r), "country");
+    DirectoryReader r = IndexReader.open(benchmark.getRunData().getDirectory());
+    DocTermsIndex idx = FieldCache.DEFAULT.getTermsIndex(new SlowCompositeReaderWrapper(r), "country");
     final int maxDoc = r.maxDoc();
     assertEquals(1000, maxDoc);
     BytesRef br = new BytesRef();
Index: modules/facet
===================================================================
--- modules/facet	(revision 1238034)
+++ modules/facet	(working copy)

Property changes on: modules/facet
___________________________________________________________________
Modified: svn:mergeinfo
   Merged /lucene/dev/branches/lucene2858/modules/facet:r1234440-1238051
Index: modules/facet/src/java/org/apache/lucene/facet/search/FacetsAccumulator.java
===================================================================
--- modules/facet/src/java/org/apache/lucene/facet/search/FacetsAccumulator.java	(revision 1238034)
+++ modules/facet/src/java/org/apache/lucene/facet/search/FacetsAccumulator.java	(working copy)
@@ -98,12 +98,6 @@
    * is considered, and used to decrement from the overall counts, thereby 
    * walking through less documents, which is faster.
    * <p>
-   * Note that this optimization is only available when searching an index
-   * whose {@link IndexReader} implements both 
-   * {@link IndexReader#directory()} and {@link IndexReader#getVersion()} 
-   * otherwise the optimization is silently disabled regardless of
-   * the complement threshold settings.
-   * <p>
    * For the default settings see {@link #DEFAULT_COMPLEMENT_THRESHOLD}.
    * <p>
    * To forcing complements in all cases pass {@link #FORCE_COMPLEMENT}.
Index: modules/facet/src/java/org/apache/lucene/facet/search/FacetsCollector.java
===================================================================
--- modules/facet/src/java/org/apache/lucene/facet/search/FacetsCollector.java	(revision 1238034)
+++ modules/facet/src/java/org/apache/lucene/facet/search/FacetsCollector.java	(working copy)
@@ -3,8 +3,8 @@
 import java.io.IOException;
 import java.util.List;
 
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
 import org.apache.lucene.search.Collector;
 import org.apache.lucene.search.Scorer;
 
Index: modules/facet/src/java/org/apache/lucene/facet/search/ScoredDocIdCollector.java
===================================================================
--- modules/facet/src/java/org/apache/lucene/facet/search/ScoredDocIdCollector.java	(revision 1238034)
+++ modules/facet/src/java/org/apache/lucene/facet/search/ScoredDocIdCollector.java	(working copy)
@@ -2,8 +2,8 @@
 
 import java.io.IOException;
 
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
 import org.apache.lucene.search.Collector;
 import org.apache.lucene.search.DocIdSet;
 import org.apache.lucene.search.DocIdSetIterator;
Index: modules/facet/src/java/org/apache/lucene/facet/taxonomy/directory/DirectoryTaxonomyReader.java
===================================================================
--- modules/facet/src/java/org/apache/lucene/facet/taxonomy/directory/DirectoryTaxonomyReader.java	(revision 1238034)
+++ modules/facet/src/java/org/apache/lucene/facet/taxonomy/directory/DirectoryTaxonomyReader.java	(working copy)
@@ -15,6 +15,7 @@
 import org.apache.lucene.facet.taxonomy.TaxonomyReader;
 import org.apache.lucene.facet.taxonomy.directory.Consts.LoadFullPathOnly;
 import org.apache.lucene.index.CorruptIndexException;
+import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.DocsEnum;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.MultiFields;
@@ -58,7 +59,7 @@
 
   private static final Logger logger = Logger.getLogger(DirectoryTaxonomyReader.class.getName());
   
-  private IndexReader indexReader;
+  private DirectoryReader indexReader;
 
   // The following lock is used to allow multiple threads to read from the
   // index concurrently, while having them block during the very short
@@ -126,8 +127,8 @@
     parentArray.refresh(indexReader);
   }
 
-  protected IndexReader openIndexReader(Directory directory) throws CorruptIndexException, IOException {
-    return IndexReader.open(directory);
+  protected DirectoryReader openIndexReader(Directory directory) throws CorruptIndexException, IOException {
+    return DirectoryReader.open(directory);
   }
 
   /**
@@ -353,7 +354,7 @@
     // safely read indexReader without holding the write lock, because
     // no other thread can be writing at this time (this method is the
     // only possible writer, and it is "synchronized" to avoid this case).
-    IndexReader r2 = IndexReader.openIfChanged(indexReader);
+    DirectoryReader r2 = DirectoryReader.openIfChanged(indexReader);
     if (r2 == null) {
     	return false; // no changes, nothing to do
     } 
@@ -557,7 +558,7 @@
    * 
    * @return lucene indexReader
    */
-  IndexReader getInternalIndexReader() {
+  DirectoryReader getInternalIndexReader() {
     ensureOpen();
     return this.indexReader;
   }
Index: modules/facet/src/java/org/apache/lucene/facet/taxonomy/directory/DirectoryTaxonomyWriter.java
===================================================================
--- modules/facet/src/java/org/apache/lucene/facet/taxonomy/directory/DirectoryTaxonomyWriter.java	(revision 1238034)
+++ modules/facet/src/java/org/apache/lucene/facet/taxonomy/directory/DirectoryTaxonomyWriter.java	(working copy)
@@ -22,6 +22,7 @@
 import org.apache.lucene.document.StringField;
 import org.apache.lucene.document.TextField;
 import org.apache.lucene.index.CorruptIndexException;
+import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.DocsEnum;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexWriter;
@@ -112,7 +113,7 @@
    * that some of the cached data was cleared).
    */
   private boolean cacheIsComplete;
-  private IndexReader reader;
+  private DirectoryReader reader;
   private int cacheMisses;
 
   /**
@@ -188,7 +189,7 @@
   throws CorruptIndexException, LockObtainFailedException,
   IOException {
 
-    if (!IndexReader.indexExists(directory) || openMode==OpenMode.CREATE) {
+    if (!DirectoryReader.indexExists(directory) || openMode==OpenMode.CREATE) {
       taxoIndexCreateTime = Long.toString(System.nanoTime());
     }
     
@@ -281,8 +282,8 @@
    * calling {@link IndexReader#open(IndexWriter, boolean)}. Extending classes can override
    * this method to return their own {@link IndexReader}.
    */
-  protected IndexReader openReader() throws IOException {
-    return IndexReader.open(indexWriter, true); 
+  protected DirectoryReader openReader() throws IOException {
+    return DirectoryReader.open(indexWriter, true); 
   }
 
   /**
@@ -616,9 +617,9 @@
     }
   }
 
-  private synchronized void refreshReader() throws IOException {
+  protected synchronized void refreshReader() throws IOException {
     if (reader != null) {
-      IndexReader r2 = IndexReader.openIfChanged(reader);
+      DirectoryReader r2 = DirectoryReader.openIfChanged(reader);
       if (r2 != null) {
         reader.close();
         reader = r2;
@@ -985,6 +986,18 @@
   }
 
   /**
+   * Expert:  This method is only for expert use.
+   * Note also that any call to refresh() will invalidate the returned reader,
+   * so the caller needs to take care of appropriate locking.
+   * 
+   * @return lucene indexReader
+   */
+  DirectoryReader getInternalIndexReader() {
+    ensureOpen();
+    return this.reader;
+  }
+
+  /**
    * Mapping from old ordinal to new ordinals, used when merging indexes 
    * wit separate taxonomies.
    * <p> 
Index: modules/facet/src/test/org/apache/lucene/facet/FacetTestUtils.java
===================================================================
--- modules/facet/src/test/org/apache/lucene/facet/FacetTestUtils.java	(revision 1238034)
+++ modules/facet/src/test/org/apache/lucene/facet/FacetTestUtils.java	(working copy)
@@ -9,6 +9,7 @@
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.TextField;
 import org.apache.lucene.index.CorruptIndexException;
+import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexWriter;
 import org.apache.lucene.index.IndexWriterConfig;
@@ -133,7 +134,7 @@
   }
 
   public static class IndexTaxonomyReaderPair {
-    public IndexReader indexReader;
+    public DirectoryReader indexReader;
     public TaxonomyReader taxReader;
     public IndexSearcher indexSearcher;
 
Index: modules/facet/src/test/org/apache/lucene/facet/index/FacetsPayloadProcessorProviderTest.java
===================================================================
--- modules/facet/src/test/org/apache/lucene/facet/index/FacetsPayloadProcessorProviderTest.java	(revision 1238034)
+++ modules/facet/src/test/org/apache/lucene/facet/index/FacetsPayloadProcessorProviderTest.java	(working copy)
@@ -7,7 +7,7 @@
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.analysis.MockTokenizer;
 import org.apache.lucene.document.Document;
-import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexWriterConfig;
 import org.apache.lucene.index.RandomIndexWriter;
 import org.apache.lucene.search.IndexSearcher;
@@ -67,7 +67,7 @@
   }
 
   private void verifyResults(Directory dir, Directory taxDir) throws IOException {
-    IndexReader reader1 = IndexReader.open(dir);
+    DirectoryReader reader1 = DirectoryReader.open(dir);
     DirectoryTaxonomyReader taxReader = new DirectoryTaxonomyReader(taxDir);
     IndexSearcher searcher = newSearcher(reader1);
     FacetSearchParams fsp = new FacetSearchParams();
Index: modules/facet/src/test/org/apache/lucene/facet/search/TestFacetsAccumulatorWithComplement.java
===================================================================
--- modules/facet/src/test/org/apache/lucene/facet/search/TestFacetsAccumulatorWithComplement.java	(revision 1238034)
+++ modules/facet/src/test/org/apache/lucene/facet/search/TestFacetsAccumulatorWithComplement.java	(working copy)
@@ -6,6 +6,7 @@
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.MultiReader;
 import org.apache.lucene.index.ParallelReader;
+import org.apache.lucene.index.SlowCompositeReaderWrapper;
 import org.apache.lucene.search.MatchAllDocsQuery;
 import org.apache.lucene.search.Query;
 import org.junit.After;
@@ -68,7 +69,7 @@
   public void testComplementsWithParallerReader() throws Exception {
     IndexReader origReader = indexReader; 
     ParallelReader pr = new ParallelReader(true);
-    pr.add(origReader);
+    pr.add(SlowCompositeReaderWrapper.wrap(origReader));
     indexReader = pr;
     try {
       doTestComplements();
Index: modules/facet/src/test/org/apache/lucene/facet/search/TestTotalFacetCountsCache.java
===================================================================
--- modules/facet/src/test/org/apache/lucene/facet/search/TestTotalFacetCountsCache.java	(revision 1238034)
+++ modules/facet/src/test/org/apache/lucene/facet/search/TestTotalFacetCountsCache.java	(working copy)
@@ -9,6 +9,7 @@
 import org.apache.lucene.analysis.MockTokenizer;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.index.CorruptIndexException;
+import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexWriter;
 import org.apache.lucene.index.IndexWriterConfig;
@@ -299,7 +300,7 @@
     writers[0].taxWriter.close();
 
     readers[0].taxReader.refresh();
-    IndexReader r2 = IndexReader.openIfChanged(readers[0].indexReader);
+    DirectoryReader r2 = DirectoryReader.openIfChanged(readers[0].indexReader);
     assertNotNull(r2);
     // Hold on to the 'original' reader so we can do some checks with it
     IndexReader origReader = null;
Index: modules/facet/src/test/org/apache/lucene/facet/taxonomy/directory/TestDirectoryTaxonomyWriter.java
===================================================================
--- modules/facet/src/test/org/apache/lucene/facet/taxonomy/directory/TestDirectoryTaxonomyWriter.java	(revision 1238034)
+++ modules/facet/src/test/org/apache/lucene/facet/taxonomy/directory/TestDirectoryTaxonomyWriter.java	(working copy)
@@ -3,6 +3,7 @@
 import java.util.HashMap;
 import java.util.Map;
 
+import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexWriterConfig.OpenMode;
 import org.apache.lucene.store.AlreadyClosedException;
@@ -54,7 +55,7 @@
     // commit() wasn't called.
     Directory dir = newDirectory();
     DirectoryTaxonomyWriter ltw = new DirectoryTaxonomyWriter(dir, OpenMode.CREATE_OR_APPEND, new NoOpCache());
-    assertFalse(IndexReader.indexExists(dir));
+    assertFalse(DirectoryReader.indexExists(dir));
     ltw.commit(); // first commit, so that an index will be created
     ltw.addCategory(new CategoryPath("a"));
     
@@ -70,7 +71,7 @@
     // Verifies that committed data is retrievable
     Directory dir = newDirectory();
     DirectoryTaxonomyWriter ltw = new DirectoryTaxonomyWriter(dir, OpenMode.CREATE_OR_APPEND, new NoOpCache());
-    assertFalse(IndexReader.indexExists(dir));
+    assertFalse(DirectoryReader.indexExists(dir));
     ltw.commit(); // first commit, so that an index will be created
     ltw.addCategory(new CategoryPath("a"));
     ltw.addCategory(new CategoryPath("b"));
@@ -78,7 +79,7 @@
     userCommitData.put("testing", "1 2 3");
     ltw.commit(userCommitData);
     ltw.close();
-    IndexReader r = IndexReader.open(dir);
+    DirectoryReader r = IndexReader.open(dir);
     assertEquals("2 categories plus root should have been committed to the underlying directory", 3, r.numDocs());
     Map <String, String> readUserCommitData = r.getCommitUserData();
     assertTrue("wrong value extracted from commit data", 
Index: modules/facet/src/test/org/apache/lucene/facet/taxonomy/directory/TestIndexClose.java
===================================================================
--- modules/facet/src/test/org/apache/lucene/facet/taxonomy/directory/TestIndexClose.java	(revision 1238034)
+++ modules/facet/src/test/org/apache/lucene/facet/taxonomy/directory/TestIndexClose.java	(working copy)
@@ -2,22 +2,25 @@
 
 import java.io.IOException;
 import java.util.HashSet;
+import java.util.IdentityHashMap;
 import java.util.Set;
 
 import org.apache.lucene.index.CorruptIndexException;
-import org.apache.lucene.index.FilterIndexReader;
-import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexWriter;
 import org.apache.lucene.index.IndexWriterConfig;
 import org.apache.lucene.index.IndexWriterConfig.OpenMode;
+import org.apache.lucene.store.AlreadyClosedException;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.LockObtainFailedException;
 import org.junit.Test;
 
 import org.apache.lucene.util.LuceneTestCase;
+import org.apache.lucene.util.MapBackedSet;
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.analysis.MockTokenizer;
 import org.apache.lucene.facet.taxonomy.CategoryPath;
+import org.apache.lucene.facet.taxonomy.InconsistentTaxonomyException;
 import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
 import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
 
@@ -92,8 +95,7 @@
   }
 
   private static class LeakChecker {
-    int ireader=0;
-    Set<Integer> openReaders = new HashSet<Integer>();
+    Set<DirectoryReader> readers = new MapBackedSet<DirectoryReader>(new IdentityHashMap<DirectoryReader,Boolean>());
 
     int iwriter=0;
     Set<Integer> openWriters = new HashSet<Integer>();
@@ -110,9 +112,15 @@
 
     public int nopen() {
       int ret=0;
-      for (int i: openReaders) {
-        System.err.println("reader "+i+" still open");
-        ret++;
+      for (DirectoryReader r: readers) {
+        try {
+          // this should throw ex, if already closed!
+          r.getTopReaderContext();
+          System.err.println("reader "+r+" still open");
+          ret++;
+        } catch (AlreadyClosedException e) {
+          // fine
+        }
       }
       for (int i: openWriters) {
         System.err.println("writer "+i+" still open");
@@ -126,10 +134,18 @@
         super(dir);
       }    
       @Override
-      protected IndexReader openReader() throws IOException {
-        return new InstrumentedIndexReader(super.openReader()); 
+      protected DirectoryReader openReader() throws IOException {
+        DirectoryReader r = super.openReader();
+        readers.add(r);
+        return r; 
       }
       @Override
+      protected synchronized void refreshReader() throws IOException {
+        super.refreshReader();
+        final DirectoryReader r = getInternalIndexReader();
+        if (r != null) readers.add(r);
+      }
+      @Override
       protected IndexWriter openIndexWriter (Directory directory, IndexWriterConfig config) throws IOException {
         return new InstrumentedIndexWriter(directory, config);
       }
@@ -146,44 +162,19 @@
         super(dir);
       }  
       @Override
-      protected IndexReader openIndexReader(Directory dir) throws CorruptIndexException, IOException {
-        return new InstrumentedIndexReader(IndexReader.open(dir)); 
+      protected DirectoryReader openIndexReader(Directory dir) throws CorruptIndexException, IOException {
+        DirectoryReader r = super.openIndexReader(dir);
+        readers.add(r);
+        return r; 
       }
-
-    }
-
-    private class InstrumentedIndexReader extends FilterIndexReader {
-      int mynum;
-      public InstrumentedIndexReader(IndexReader in) {
-        super(in);
-        this.in = in;
-        mynum = ireader++;
-        openReaders.add(mynum);
-        //        System.err.println("opened "+mynum);
-      }
       @Override
-      protected synchronized IndexReader doOpenIfChanged() throws CorruptIndexException, IOException {
-        IndexReader n = IndexReader.openIfChanged(in);
-        if (n == null) {
-          return null;
-        }
-        return new InstrumentedIndexReader(n);
+      public synchronized boolean refresh() throws IOException, InconsistentTaxonomyException {
+        final boolean ret = super.refresh();
+        readers.add(getInternalIndexReader());
+        return ret;
       }
+    }
 
-      // Unfortunately, IndexReader.close() is marked final so we can't
-      // change it! Fortunately, close() calls (if the object wasn't
-      // already closed) doClose() so we can override it to do our thing -
-      // just like FilterIndexReader does.
-      @Override
-      public void doClose() throws IOException {
-        in.close();
-        if (!openReaders.contains(mynum)) { // probably can't happen...
-          fail("Reader #"+mynum+" was closed twice!");
-        }
-        openReaders.remove(mynum);
-        //        System.err.println("closed "+mynum);
-      }
-    }
     private class InstrumentedIndexWriter extends IndexWriter {
       int mynum;
       public InstrumentedIndexWriter(Directory d, IndexWriterConfig conf) throws CorruptIndexException, LockObtainFailedException, IOException {
Index: modules/grouping/src/java/org/apache/lucene/search/grouping/AbstractFirstPassGroupingCollector.java
===================================================================
--- modules/grouping/src/java/org/apache/lucene/search/grouping/AbstractFirstPassGroupingCollector.java	(revision 1238034)
+++ modules/grouping/src/java/org/apache/lucene/search/grouping/AbstractFirstPassGroupingCollector.java	(working copy)
@@ -17,7 +17,7 @@
  * limitations under the License.
  */
 
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.search.*;
 
 import java.io.IOException;
Index: modules/grouping/src/java/org/apache/lucene/search/grouping/AbstractSecondPassGroupingCollector.java
===================================================================
--- modules/grouping/src/java/org/apache/lucene/search/grouping/AbstractSecondPassGroupingCollector.java	(revision 1238034)
+++ modules/grouping/src/java/org/apache/lucene/search/grouping/AbstractSecondPassGroupingCollector.java	(working copy)
@@ -17,7 +17,7 @@
  * limitations under the License.
  */
 
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.search.*;
 
 import java.io.IOException;
Index: modules/grouping/src/java/org/apache/lucene/search/grouping/BlockGroupingCollector.java
===================================================================
--- modules/grouping/src/java/org/apache/lucene/search/grouping/BlockGroupingCollector.java	(revision 1238034)
+++ modules/grouping/src/java/org/apache/lucene/search/grouping/BlockGroupingCollector.java	(working copy)
@@ -20,7 +20,7 @@
 
 import java.io.IOException;
 
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.IndexWriter;       // javadocs
 import org.apache.lucene.search.Collector;
 import org.apache.lucene.search.DocIdSetIterator;
@@ -505,7 +505,7 @@
     subDocUpto = 0;
     docBase = readerContext.docBase;
     //System.out.println("setNextReader base=" + docBase + " r=" + readerContext.reader);
-    lastDocPerGroupBits = lastDocPerGroup.getDocIdSet(readerContext, readerContext.reader.getLiveDocs()).iterator();
+    lastDocPerGroupBits = lastDocPerGroup.getDocIdSet(readerContext, readerContext.reader().getLiveDocs()).iterator();
     groupEndDocID = -1;
 
     currentReaderContext = readerContext;
Index: modules/grouping/src/java/org/apache/lucene/search/grouping/dv/DVAllGroupHeadsCollector.java
===================================================================
--- modules/grouping/src/java/org/apache/lucene/search/grouping/dv/DVAllGroupHeadsCollector.java	(revision 1238034)
+++ modules/grouping/src/java/org/apache/lucene/search/grouping/dv/DVAllGroupHeadsCollector.java	(working copy)
@@ -17,6 +17,7 @@
  * limitations under the License.
  */
 
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.DocValues;
 import org.apache.lucene.index.DocValues.Type; // javadocs
 import org.apache.lucene.index.IndexReader;
@@ -43,7 +44,7 @@
   final DocValues.Type valueType;
   final BytesRef scratchBytesRef = new BytesRef();
 
-  IndexReader.AtomicReaderContext readerContext;
+  AtomicReaderContext readerContext;
   Scorer scorer;
 
   DVAllGroupHeadsCollector(String groupField, DocValues.Type valueType, int numberOfSorts, boolean diskResident) {
@@ -91,10 +92,10 @@
   static class GroupHead extends AbstractAllGroupHeadsCollector.GroupHead<Comparable> {
 
     final FieldComparator[] comparators;
-    IndexReader.AtomicReaderContext readerContext;
+    AtomicReaderContext readerContext;
     Scorer scorer;
 
-    GroupHead(Comparable groupValue, Sort sort, int doc, IndexReader.AtomicReaderContext readerContext, Scorer scorer) throws IOException {
+    GroupHead(Comparable groupValue, Sort sort, int doc, AtomicReaderContext readerContext, Scorer scorer) throws IOException {
       super(groupValue, doc + readerContext.docBase);
       final SortField[] sortFields = sort.getSort();
       comparators = new FieldComparator[sortFields.length];
@@ -123,10 +124,10 @@
   }
 
   @Override
-  public void setNextReader(IndexReader.AtomicReaderContext readerContext) throws IOException {
+  public void setNextReader(AtomicReaderContext readerContext) throws IOException {
     this.readerContext = readerContext;
 
-    final DocValues dv = readerContext.reader.docValues(groupField);
+    final DocValues dv = readerContext.reader().docValues(groupField);
     final DocValues.Source dvSource;
     if (dv != null) {
       dvSource = diskResident ? dv.getDirectSource() : dv.getSource();
@@ -147,7 +148,7 @@
    * @return The default source when no doc values are available.
    * @param readerContext The current reader context
    */
-  protected DocValues.Source getDefaultSource(IndexReader.AtomicReaderContext readerContext) {
+  protected DocValues.Source getDefaultSource(AtomicReaderContext readerContext) {
     return DocValues.getDefaultSource(valueType);
   }
 
@@ -189,7 +190,7 @@
       return groups.values();
     }
 
-    public void setNextReader(IndexReader.AtomicReaderContext context) throws IOException {
+    public void setNextReader(AtomicReaderContext context) throws IOException {
       super.setNextReader(context);
       for (GroupHead groupHead : groups.values()) {
         for (int i = 0; i < groupHead.comparators.length; i++) {
@@ -230,8 +231,8 @@
       }
 
       @Override
-      protected DocValues.Source getDefaultSource(IndexReader.AtomicReaderContext readerContext) {
-        return DocValues.getDefaultSortedSource(valueType, readerContext.reader.maxDoc());
+      protected DocValues.Source getDefaultSource(AtomicReaderContext readerContext) {
+        return DocValues.getDefaultSortedSource(valueType, readerContext.reader().maxDoc());
       }
     }
 
Index: modules/grouping/src/java/org/apache/lucene/search/grouping/dv/DVAllGroupsCollector.java
===================================================================
--- modules/grouping/src/java/org/apache/lucene/search/grouping/dv/DVAllGroupsCollector.java	(revision 1238034)
+++ modules/grouping/src/java/org/apache/lucene/search/grouping/dv/DVAllGroupsCollector.java	(working copy)
@@ -17,6 +17,7 @@
  * limitations under the License.
  */
 
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.DocValues;
 import org.apache.lucene.index.DocValues.Type; // javadocs
 import org.apache.lucene.index.IndexReader;
@@ -104,8 +105,8 @@
   }
 
   @Override
-  public void setNextReader(IndexReader.AtomicReaderContext readerContext) throws IOException {
-    final DocValues dv = readerContext.reader.docValues(groupField);
+  public void setNextReader(AtomicReaderContext readerContext) throws IOException {
+    final DocValues dv = readerContext.reader().docValues(groupField);
     final DocValues.Source dvSource;
     if (dv != null) {
       dvSource = diskResident ? dv.getDirectSource() : dv.getSource();
@@ -121,13 +122,13 @@
    * @param source The idv source to be used by concrete implementations
    * @param readerContext The current reader context
    */
-  protected abstract void setDocValuesSources(DocValues.Source source, IndexReader.AtomicReaderContext readerContext);
+  protected abstract void setDocValuesSources(DocValues.Source source, AtomicReaderContext readerContext);
 
   /**
    * @return The default source when no doc values are available.
    * @param readerContext The current reader context
    */
-  protected DocValues.Source getDefaultSource(IndexReader.AtomicReaderContext readerContext) {
+  protected DocValues.Source getDefaultSource(AtomicReaderContext readerContext) {
     return DocValues.getDefaultSource(valueType);
   }
 
@@ -150,7 +151,7 @@
       return groups;
     }
 
-    protected void setDocValuesSources(DocValues.Source source, IndexReader.AtomicReaderContext readerContext) {
+    protected void setDocValuesSources(DocValues.Source source, AtomicReaderContext readerContext) {
       this.source = source;
     }
 
@@ -175,7 +176,7 @@
       return groups;
     }
 
-    protected void setDocValuesSources(DocValues.Source source, IndexReader.AtomicReaderContext readerContext) {
+    protected void setDocValuesSources(DocValues.Source source, AtomicReaderContext readerContext) {
       this.source = source;
     }
 
@@ -202,7 +203,7 @@
       return groups;
     }
 
-    protected void setDocValuesSources(DocValues.Source source, IndexReader.AtomicReaderContext readerContext) {
+    protected void setDocValuesSources(DocValues.Source source, AtomicReaderContext readerContext) {
       this.source = source;
     }
 
@@ -233,7 +234,7 @@
       return groups;
     }
 
-    protected void setDocValuesSources(DocValues.Source source, IndexReader.AtomicReaderContext readerContext) {
+    protected void setDocValuesSources(DocValues.Source source, AtomicReaderContext readerContext) {
       this.source = source.asSortedSource();
 
       ordSet.clear();
@@ -246,8 +247,8 @@
     }
 
     @Override
-    protected DocValues.Source getDefaultSource(IndexReader.AtomicReaderContext readerContext) {
-      return DocValues.getDefaultSortedSource(valueType, readerContext.reader.maxDoc());
+    protected DocValues.Source getDefaultSource(AtomicReaderContext readerContext) {
+      return DocValues.getDefaultSortedSource(valueType, readerContext.reader().maxDoc());
     }
 
   }
Index: modules/grouping/src/java/org/apache/lucene/search/grouping/dv/DVFirstPassGroupingCollector.java
===================================================================
--- modules/grouping/src/java/org/apache/lucene/search/grouping/dv/DVFirstPassGroupingCollector.java	(revision 1238034)
+++ modules/grouping/src/java/org/apache/lucene/search/grouping/dv/DVFirstPassGroupingCollector.java	(working copy)
@@ -17,6 +17,7 @@
  * limitations under the License.
  */
 
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.DocValues;
 import org.apache.lucene.index.DocValues.Type; // javadocs
 import org.apache.lucene.index.IndexReader;
@@ -69,10 +70,10 @@
   }
 
   @Override
-  public void setNextReader(IndexReader.AtomicReaderContext readerContext) throws IOException {
+  public void setNextReader(AtomicReaderContext readerContext) throws IOException {
     super.setNextReader(readerContext);
 
-    final DocValues dv = readerContext.reader.docValues(groupField);
+    final DocValues dv = readerContext.reader().docValues(groupField);
     final DocValues.Source dvSource;
     if (dv != null) {
       dvSource = diskResident ? dv.getDirectSource() : dv.getSource();
@@ -93,7 +94,7 @@
    * @return The default source when no doc values are available.
    * @param readerContext The current reader context
    */
-  protected DocValues.Source getDefaultSource(IndexReader.AtomicReaderContext readerContext) {
+  protected DocValues.Source getDefaultSource(AtomicReaderContext readerContext) {
     return DocValues.getDefaultSource(valueType);
   }
 
@@ -197,8 +198,8 @@
     }
 
     @Override
-    protected DocValues.Source getDefaultSource(IndexReader.AtomicReaderContext readerContext) {
-      return DocValues.getDefaultSortedSource(valueType, readerContext.reader.maxDoc());
+    protected DocValues.Source getDefaultSource(AtomicReaderContext readerContext) {
+      return DocValues.getDefaultSortedSource(valueType, readerContext.reader().maxDoc());
     }
   }
 
Index: modules/grouping/src/java/org/apache/lucene/search/grouping/dv/DVSecondPassGroupingCollector.java
===================================================================
--- modules/grouping/src/java/org/apache/lucene/search/grouping/dv/DVSecondPassGroupingCollector.java	(revision 1238034)
+++ modules/grouping/src/java/org/apache/lucene/search/grouping/dv/DVSecondPassGroupingCollector.java	(working copy)
@@ -17,6 +17,7 @@
  * limitations under the License.
  */
 
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.DocValues;
 import org.apache.lucene.index.DocValues.Type; // javadocs
 import org.apache.lucene.index.IndexReader;
@@ -103,10 +104,10 @@
   }
 
   @Override
-  public void setNextReader(IndexReader.AtomicReaderContext readerContext) throws IOException {
+  public void setNextReader(AtomicReaderContext readerContext) throws IOException {
     super.setNextReader(readerContext);
 
-    final DocValues dv = readerContext.reader.docValues(groupField);
+    final DocValues dv = readerContext.reader().docValues(groupField);
     final DocValues.Source dvSource;
     if (dv != null) {
       dvSource = diskResident ? dv.getDirectSource() : dv.getSource();
@@ -122,13 +123,13 @@
    * @param source The idv source to be used by concrete implementations
    * @param readerContext The current reader context
    */
-  protected abstract void setDocValuesSources(DocValues.Source source, IndexReader.AtomicReaderContext readerContext);
+  protected abstract void setDocValuesSources(DocValues.Source source, AtomicReaderContext readerContext);
 
   /**
    * @return The default source when no doc values are available.
    * @param readerContext The current reader context
    */
-  protected DocValues.Source getDefaultSource(IndexReader.AtomicReaderContext readerContext) {
+  protected DocValues.Source getDefaultSource(AtomicReaderContext readerContext) {
     return DocValues.getDefaultSource(valueType);
   }
 
@@ -144,7 +145,7 @@
       return groupMap.get(source.getInt(doc));
     }
 
-    protected void setDocValuesSources(DocValues.Source source, IndexReader.AtomicReaderContext readerContext) {
+    protected void setDocValuesSources(DocValues.Source source, AtomicReaderContext readerContext) {
       this.source = source;
     }
   }
@@ -161,7 +162,7 @@
       return groupMap.get(source.getFloat(doc));
     }
 
-    protected void setDocValuesSources(DocValues.Source source, IndexReader.AtomicReaderContext readerContext) {
+    protected void setDocValuesSources(DocValues.Source source, AtomicReaderContext readerContext) {
       this.source = source;
     }
   }
@@ -180,7 +181,7 @@
     }
 
     @Override
-    protected void setDocValuesSources(DocValues.Source source, IndexReader.AtomicReaderContext readerContext) {
+    protected void setDocValuesSources(DocValues.Source source, AtomicReaderContext readerContext) {
       this.source = source;
     }
 
@@ -209,7 +210,7 @@
     }
 
     @Override
-    protected void setDocValuesSources(DocValues.Source source, IndexReader.AtomicReaderContext readerContext) {
+    protected void setDocValuesSources(DocValues.Source source, AtomicReaderContext readerContext) {
       this.source = source.asSortedSource();
 
       ordSet.clear();
@@ -222,8 +223,8 @@
     }
 
     @Override
-    protected DocValues.Source getDefaultSource(IndexReader.AtomicReaderContext readerContext) {
-      return DocValues.getDefaultSortedSource(valueType, readerContext.reader.maxDoc());
+    protected DocValues.Source getDefaultSource(AtomicReaderContext readerContext) {
+      return DocValues.getDefaultSortedSource(valueType, readerContext.reader().maxDoc());
     }
   }
 
Index: modules/grouping/src/java/org/apache/lucene/search/grouping/function/FunctionAllGroupHeadsCollector.java
===================================================================
--- modules/grouping/src/java/org/apache/lucene/search/grouping/function/FunctionAllGroupHeadsCollector.java	(revision 1238034)
+++ modules/grouping/src/java/org/apache/lucene/search/grouping/function/FunctionAllGroupHeadsCollector.java	(working copy)
@@ -17,6 +17,7 @@
  * limitations under the License.
  */
 
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
@@ -47,7 +48,7 @@
 
   private FunctionValues.ValueFiller filler;
   private MutableValue mval;
-  private IndexReader.AtomicReaderContext readerContext;
+  private AtomicReaderContext readerContext;
   private Scorer scorer;
 
   /**
@@ -103,7 +104,7 @@
     }
   }
 
-  public void setNextReader(IndexReader.AtomicReaderContext context) throws IOException {
+  public void setNextReader(AtomicReaderContext context) throws IOException {
     this.readerContext = context;
     FunctionValues docValues = groupBy.getValues(vsContext, context);
     filler = docValues.getValueFiller();
Index: modules/grouping/src/java/org/apache/lucene/search/grouping/function/FunctionAllGroupsCollector.java
===================================================================
--- modules/grouping/src/java/org/apache/lucene/search/grouping/function/FunctionAllGroupsCollector.java	(revision 1238034)
+++ modules/grouping/src/java/org/apache/lucene/search/grouping/function/FunctionAllGroupsCollector.java	(working copy)
@@ -17,6 +17,7 @@
  * limitations under the License.
  */
 
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
@@ -78,7 +79,7 @@
   /**
    * {@inheritDoc}
    */
-  public void setNextReader(IndexReader.AtomicReaderContext context) throws IOException {
+  public void setNextReader(AtomicReaderContext context) throws IOException {
     FunctionValues docValues = groupBy.getValues(vsContext, context);
     filler = docValues.getValueFiller();
     mval = filler.getValue();
Index: modules/grouping/src/java/org/apache/lucene/search/grouping/function/FunctionFirstPassGroupingCollector.java
===================================================================
--- modules/grouping/src/java/org/apache/lucene/search/grouping/function/FunctionFirstPassGroupingCollector.java	(revision 1238034)
+++ modules/grouping/src/java/org/apache/lucene/search/grouping/function/FunctionFirstPassGroupingCollector.java	(working copy)
@@ -17,6 +17,7 @@
  * limitations under the License.
  */
 
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
@@ -78,7 +79,7 @@
   }
 
   @Override
-  public void setNextReader(IndexReader.AtomicReaderContext readerContext) throws IOException {
+  public void setNextReader(AtomicReaderContext readerContext) throws IOException {
     super.setNextReader(readerContext);
     docValues = groupByVS.getValues(vsContext, readerContext);
     filler = docValues.getValueFiller();
Index: modules/grouping/src/java/org/apache/lucene/search/grouping/function/FunctionSecondPassGroupingCollector.java
===================================================================
--- modules/grouping/src/java/org/apache/lucene/search/grouping/function/FunctionSecondPassGroupingCollector.java	(revision 1238034)
+++ modules/grouping/src/java/org/apache/lucene/search/grouping/function/FunctionSecondPassGroupingCollector.java	(working copy)
@@ -17,6 +17,7 @@
  * limitations under the License.
  */
 
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
@@ -75,7 +76,7 @@
   /**
    * {@inheritDoc}
    */
-  public void setNextReader(IndexReader.AtomicReaderContext readerContext) throws IOException {
+  public void setNextReader(AtomicReaderContext readerContext) throws IOException {
     super.setNextReader(readerContext);
     FunctionValues docValues = groupByVS.getValues(vsContext, readerContext);
     filler = docValues.getValueFiller();
Index: modules/grouping/src/java/org/apache/lucene/search/grouping/term/TermAllGroupHeadsCollector.java
===================================================================
--- modules/grouping/src/java/org/apache/lucene/search/grouping/term/TermAllGroupHeadsCollector.java	(revision 1238034)
+++ modules/grouping/src/java/org/apache/lucene/search/grouping/term/TermAllGroupHeadsCollector.java	(working copy)
@@ -17,6 +17,7 @@
  * limitations under the License.
  */
 
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.search.*;
 import org.apache.lucene.search.grouping.AbstractAllGroupHeadsCollector;
@@ -41,7 +42,7 @@
   final BytesRef scratchBytesRef = new BytesRef();
 
   FieldCache.DocTermsIndex groupIndex;
-  IndexReader.AtomicReaderContext readerContext;
+  AtomicReaderContext readerContext;
 
   protected TermAllGroupHeadsCollector(String groupField, int numberOfSorts) {
     super(numberOfSorts);
@@ -142,9 +143,9 @@
       return groups.values();
     }
 
-    public void setNextReader(IndexReader.AtomicReaderContext context) throws IOException {
+    public void setNextReader(AtomicReaderContext context) throws IOException {
       this.readerContext = context;
-      groupIndex = FieldCache.DEFAULT.getTermsIndex(context.reader, groupField);
+      groupIndex = FieldCache.DEFAULT.getTermsIndex(context.reader(), groupField);
 
       for (GroupHead groupHead : groups.values()) {
         for (int i = 0; i < groupHead.comparators.length; i++) {
@@ -243,15 +244,15 @@
       temporalResult.groupHead = groupHead;
     }
 
-    public void setNextReader(IndexReader.AtomicReaderContext context) throws IOException {
+    public void setNextReader(AtomicReaderContext context) throws IOException {
       this.readerContext = context;
-      groupIndex = FieldCache.DEFAULT.getTermsIndex(context.reader, groupField);
+      groupIndex = FieldCache.DEFAULT.getTermsIndex(context.reader(), groupField);
       for (int i = 0; i < fields.length; i++) {
         if (fields[i].getType() == SortField.Type.SCORE) {
           continue;
         }
 
-        sortsIndex[i] = FieldCache.DEFAULT.getTermsIndex(context.reader, fields[i].getField());
+        sortsIndex[i] = FieldCache.DEFAULT.getTermsIndex(context.reader(), fields[i].getField());
       }
 
       // Clear ordSet and fill it with previous encountered groups that can occur in the current segment.
@@ -380,11 +381,11 @@
       temporalResult.groupHead = groupHead;
     }
 
-    public void setNextReader(IndexReader.AtomicReaderContext context) throws IOException {
+    public void setNextReader(AtomicReaderContext context) throws IOException {
       this.readerContext = context;
-      groupIndex = FieldCache.DEFAULT.getTermsIndex(context.reader, groupField);
+      groupIndex = FieldCache.DEFAULT.getTermsIndex(context.reader(), groupField);
       for (int i = 0; i < fields.length; i++) {
-        sortsIndex[i] = FieldCache.DEFAULT.getTermsIndex(context.reader, fields[i].getField());
+        sortsIndex[i] = FieldCache.DEFAULT.getTermsIndex(context.reader(), fields[i].getField());
       }
 
       // Clear ordSet and fill it with previous encountered groups that can occur in the current segment.
@@ -488,9 +489,9 @@
       temporalResult.groupHead = groupHead;
     }
 
-    public void setNextReader(IndexReader.AtomicReaderContext context) throws IOException {
+    public void setNextReader(AtomicReaderContext context) throws IOException {
       this.readerContext = context;
-      groupIndex = FieldCache.DEFAULT.getTermsIndex(context.reader, groupField);
+      groupIndex = FieldCache.DEFAULT.getTermsIndex(context.reader(), groupField);
 
       // Clear ordSet and fill it with previous encountered groups that can occur in the current segment.
       ordSet.clear();
Index: modules/grouping/src/java/org/apache/lucene/search/grouping/term/TermAllGroupsCollector.java
===================================================================
--- modules/grouping/src/java/org/apache/lucene/search/grouping/term/TermAllGroupsCollector.java	(revision 1238034)
+++ modules/grouping/src/java/org/apache/lucene/search/grouping/term/TermAllGroupsCollector.java	(working copy)
@@ -17,6 +17,7 @@
  * limitations under the License.
  */
 
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.search.FieldCache;
 import org.apache.lucene.search.grouping.AbstractAllGroupsCollector;
@@ -97,8 +98,8 @@
     return groups;
   }
 
-  public void setNextReader(IndexReader.AtomicReaderContext context) throws IOException {
-    index = FieldCache.DEFAULT.getTermsIndex(context.reader, groupField);
+  public void setNextReader(AtomicReaderContext context) throws IOException {
+    index = FieldCache.DEFAULT.getTermsIndex(context.reader(), groupField);
 
     // Clear ordSet and fill it with previous encountered groups that can occur in the current segment.
     ordSet.clear();
Index: modules/grouping/src/java/org/apache/lucene/search/grouping/term/TermFirstPassGroupingCollector.java
===================================================================
--- modules/grouping/src/java/org/apache/lucene/search/grouping/term/TermFirstPassGroupingCollector.java	(revision 1238034)
+++ modules/grouping/src/java/org/apache/lucene/search/grouping/term/TermFirstPassGroupingCollector.java	(working copy)
@@ -17,7 +17,7 @@
  * limitations under the License.
  */
 
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.search.FieldCache;
 import org.apache.lucene.search.Sort;
 import org.apache.lucene.search.grouping.AbstractFirstPassGroupingCollector;
@@ -81,6 +81,6 @@
   @Override
   public void setNextReader(AtomicReaderContext readerContext) throws IOException {
     super.setNextReader(readerContext);
-    index = FieldCache.DEFAULT.getTermsIndex(readerContext.reader, groupField);
+    index = FieldCache.DEFAULT.getTermsIndex(readerContext.reader(), groupField);
   }
 }
Index: modules/grouping/src/java/org/apache/lucene/search/grouping/term/TermSecondPassGroupingCollector.java
===================================================================
--- modules/grouping/src/java/org/apache/lucene/search/grouping/term/TermSecondPassGroupingCollector.java	(revision 1238034)
+++ modules/grouping/src/java/org/apache/lucene/search/grouping/term/TermSecondPassGroupingCollector.java	(working copy)
@@ -17,7 +17,7 @@
  * limitations under the License.
  */
 
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.search.FieldCache;
 import org.apache.lucene.search.Sort;
 import org.apache.lucene.search.grouping.AbstractSecondPassGroupingCollector;
@@ -55,7 +55,7 @@
   @Override
   public void setNextReader(AtomicReaderContext readerContext) throws IOException {
     super.setNextReader(readerContext);
-    index = FieldCache.DEFAULT.getTermsIndex(readerContext.reader, groupField);
+    index = FieldCache.DEFAULT.getTermsIndex(readerContext.reader(), groupField);
 
     // Rebuild ordSet
     ordSet.clear();
Index: modules/grouping/src/test/org/apache/lucene/search/grouping/AllGroupHeadsCollectorTest.java
===================================================================
--- modules/grouping/src/test/org/apache/lucene/search/grouping/AllGroupHeadsCollectorTest.java	(revision 1238034)
+++ modules/grouping/src/test/org/apache/lucene/search/grouping/AllGroupHeadsCollectorTest.java	(working copy)
@@ -19,9 +19,10 @@
 
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.document.*;
+import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.RandomIndexWriter;
-import org.apache.lucene.index.SlowMultiReaderWrapper;
+import org.apache.lucene.index.SlowCompositeReaderWrapper;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.index.DocValues.Type;
 import org.apache.lucene.queries.function.ValueSource;
@@ -113,7 +114,7 @@
 
     IndexReader reader = w.getReader();
     IndexSearcher indexSearcher = new IndexSearcher(reader);
-    if (SlowMultiReaderWrapper.class.isAssignableFrom(reader.getClass())) {
+    if (SlowCompositeReaderWrapper.class.isAssignableFrom(reader.getClass())) {
       canUseIDV = false;
     }
 
@@ -272,11 +273,11 @@
         }
       }
 
-      final IndexReader r = w.getReader();
+      final DirectoryReader r = w.getReader();
       w.close();
 
       // NOTE: intentional but temporary field cache insanity!
-      final int[] docIdToFieldId = FieldCache.DEFAULT.getInts(new SlowMultiReaderWrapper(r), "id", false);
+      final int[] docIdToFieldId = FieldCache.DEFAULT.getInts(new SlowCompositeReaderWrapper(r), "id", false);
       final int[] fieldIdToDocID = new int[numDocs];
       for (int i = 0; i < docIdToFieldId.length; i++) {
         int fieldId = docIdToFieldId[i];
@@ -285,7 +286,7 @@
 
       try {
         final IndexSearcher s = newSearcher(r);
-        if (SlowMultiReaderWrapper.class.isAssignableFrom(s.getIndexReader().getClass())) {
+        if (SlowCompositeReaderWrapper.class.isAssignableFrom(s.getIndexReader().getClass())) {
           canUseIDV = false;
         } else {
           canUseIDV = !preFlex;
@@ -365,7 +366,7 @@
           }
         }
       } finally {
-        FieldCache.DEFAULT.purge(r);
+        // TODO: FieldCache.DEFAULT.purge(r);
       }
 
       r.close();
Index: modules/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping.java
===================================================================
--- modules/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping.java	(revision 1238034)
+++ modules/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping.java	(working copy)
@@ -19,10 +19,14 @@
 
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.document.*;
+import org.apache.lucene.index.AtomicReader;
+import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.CompositeReaderContext;
+import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.FieldInfo.IndexOptions;
-import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.RandomIndexWriter;
-import org.apache.lucene.index.SlowMultiReaderWrapper;
+import org.apache.lucene.index.IndexReaderContext;
+import org.apache.lucene.index.SlowCompositeReaderWrapper;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.index.DocValues.Type;
 import org.apache.lucene.queries.function.ValueSource;
@@ -553,7 +557,7 @@
     }
   }
 
-  private IndexReader getDocBlockReader(Directory dir, GroupDoc[] groupDocs) throws IOException {
+  private DirectoryReader getDocBlockReader(Directory dir, GroupDoc[] groupDocs) throws IOException {
     // Coalesce by group, but in random order:
     Collections.shuffle(Arrays.asList(groupDocs), random);
     final Map<BytesRef,List<GroupDoc>> groupMap = new HashMap<BytesRef,List<GroupDoc>>();
@@ -610,7 +614,7 @@
       w.updateDocuments(new Term("group", docs.get(0).get("group")), docs);
     }
 
-    final IndexReader r = w.getReader();
+    final DirectoryReader r = w.getReader();
     w.close();
 
     return r;
@@ -622,19 +626,17 @@
     public final int[] docStarts;
 
     public ShardState(IndexSearcher s) {
-      IndexReader[] subReaders = s.getIndexReader().getSequentialSubReaders();
-      if (subReaders == null) {
-        subReaders = new IndexReader[] {s.getIndexReader()};
-      }
-      subSearchers = new ShardSearcher[subReaders.length];
-      final IndexReader.ReaderContext ctx = s.getTopReaderContext();
-      if (ctx instanceof IndexReader.AtomicReaderContext) {
+      List<AtomicReader> subReaders = new ArrayList<AtomicReader>();
+      ReaderUtil.gatherSubReaders(subReaders, s.getIndexReader());
+      subSearchers = new ShardSearcher[subReaders.size()];
+      final IndexReaderContext ctx = s.getTopReaderContext();
+      if (ctx instanceof AtomicReaderContext) {
         assert subSearchers.length == 1;
-        subSearchers[0] = new ShardSearcher((IndexReader.AtomicReaderContext) ctx, ctx);
+        subSearchers[0] = new ShardSearcher((AtomicReaderContext) ctx, ctx);
       } else {
-        final IndexReader.CompositeReaderContext compCTX = (IndexReader.CompositeReaderContext) ctx;
+        final CompositeReaderContext compCTX = (CompositeReaderContext) ctx;
         for(int searcherIDX=0;searcherIDX<subSearchers.length;searcherIDX++) {
-          subSearchers[searcherIDX] = new ShardSearcher(compCTX.leaves[searcherIDX], compCTX);
+          subSearchers[searcherIDX] = new ShardSearcher(compCTX.leaves()[searcherIDX], compCTX);
         }
       }
 
@@ -642,7 +644,7 @@
       int docBase = 0;
       for(int subIDX=0;subIDX<docStarts.length;subIDX++) {
         docStarts[subIDX] = docBase;
-        docBase += subReaders[subIDX].maxDoc();
+        docBase += subReaders.get(subIDX).maxDoc();
         //System.out.println("docStarts[" + subIDX + "]=" + docStarts[subIDX]);
       }
     }
@@ -762,17 +764,17 @@
       final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];
       System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);
 
-      final IndexReader r = w.getReader();
+      final DirectoryReader r = w.getReader();
       w.close();
 
       // NOTE: intentional but temporary field cache insanity!
-      final int[] docIDToID = FieldCache.DEFAULT.getInts(new SlowMultiReaderWrapper(r), "id", false);
-      IndexReader rBlocks = null;
+      final int[] docIDToID = FieldCache.DEFAULT.getInts(new SlowCompositeReaderWrapper(r), "id", false);
+      DirectoryReader rBlocks = null;
       Directory dirBlocks = null;
 
       try {
         final IndexSearcher s = newSearcher(r);
-        if (SlowMultiReaderWrapper.class.isAssignableFrom(s.getIndexReader().getClass())) {
+        if (SlowCompositeReaderWrapper.class.isAssignableFrom(s.getIndexReader().getClass())) {
           canUseIDV = false;
         } else {
           canUseIDV = !preFlex;
@@ -799,7 +801,7 @@
         dirBlocks = newDirectory();
         rBlocks = getDocBlockReader(dirBlocks, groupDocs);
         final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term("groupend", "x"))));
-        final int[] docIDToIDBlocks = FieldCache.DEFAULT.getInts(new SlowMultiReaderWrapper(rBlocks), "id", false);
+        final int[] docIDToIDBlocks = FieldCache.DEFAULT.getInts(new SlowCompositeReaderWrapper(rBlocks), "id", false);
 
         final IndexSearcher sBlocks = newSearcher(rBlocks);
         final ShardState shardsBlocks = new ShardState(sBlocks);
@@ -1138,9 +1140,9 @@
           assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);
         }
       } finally {
-        FieldCache.DEFAULT.purge(r);
+        // TODO: FieldCache.DEFAULT.purge(r);
         if (rBlocks != null) {
-          FieldCache.DEFAULT.purge(rBlocks);
+          // TODO: FieldCache.DEFAULT.purge(rBlocks);
         }
       }
 
@@ -1177,7 +1179,7 @@
     List<AbstractFirstPassGroupingCollector> firstPassGroupingCollectors = new ArrayList<AbstractFirstPassGroupingCollector>();
     AbstractFirstPassGroupingCollector firstPassCollector = null;
     for(int shardIDX=0;shardIDX<subSearchers.length;shardIDX++) {
-      if (SlowMultiReaderWrapper.class.isAssignableFrom(subSearchers[shardIDX].getIndexReader().getClass())) {
+      if (SlowCompositeReaderWrapper.class.isAssignableFrom(subSearchers[shardIDX].getIndexReader().getClass())) {
         canUseIDV = false;
       } else {
         canUseIDV = !preFlex;
@@ -1311,11 +1313,11 @@
   }
 
   private static class ShardSearcher extends IndexSearcher {
-    private final IndexReader.AtomicReaderContext[] ctx;
+    private final AtomicReaderContext[] ctx;
 
-    public ShardSearcher(IndexReader.AtomicReaderContext ctx, IndexReader.ReaderContext parent) {
+    public ShardSearcher(AtomicReaderContext ctx, IndexReaderContext parent) {
       super(parent);
-      this.ctx = new IndexReader.AtomicReaderContext[] {ctx};
+      this.ctx = new AtomicReaderContext[] {ctx};
     }
 
     public void search(Weight weight, Collector collector) throws IOException {
@@ -1328,7 +1330,7 @@
 
     @Override
     public String toString() {
-      return "ShardSearcher(" + ctx[0].reader + ")";
+      return "ShardSearcher(" + ctx[0].reader() + ")";
     }
   }
 
Index: modules/join/src/java/org/apache/lucene/search/join/TermsCollector.java
===================================================================
--- modules/join/src/java/org/apache/lucene/search/join/TermsCollector.java	(revision 1238034)
+++ modules/join/src/java/org/apache/lucene/search/join/TermsCollector.java	(working copy)
@@ -17,8 +17,8 @@
  * limitations under the License.
  */
 
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.DocTermOrds;
-import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.TermsEnum;
 import org.apache.lucene.search.Collector;
 import org.apache.lucene.search.FieldCache;
@@ -94,9 +94,9 @@
       } while (chunk >= buffer.length);
     }
 
-    public void setNextReader(IndexReader.AtomicReaderContext context) throws IOException {
-      docTermOrds = FieldCache.DEFAULT.getDocTermOrds(context.reader, field);
-      docTermsEnum = docTermOrds.getOrdTermsEnum(context.reader);
+    public void setNextReader(AtomicReaderContext context) throws IOException {
+      docTermOrds = FieldCache.DEFAULT.getDocTermOrds(context.reader(), field);
+      docTermsEnum = docTermOrds.getOrdTermsEnum(context.reader());
       reuse = null; // LUCENE-3377 needs to be fixed first then this statement can be removed...
     }
   }
@@ -115,8 +115,8 @@
       collectorTerms.add(fromDocTerms.getTerm(doc, spare));
     }
 
-    public void setNextReader(IndexReader.AtomicReaderContext context) throws IOException {
-      fromDocTerms = FieldCache.DEFAULT.getTerms(context.reader, field);
+    public void setNextReader(AtomicReaderContext context) throws IOException {
+      fromDocTerms = FieldCache.DEFAULT.getTerms(context.reader(), field);
     }
   }
 
Index: modules/join/src/java/org/apache/lucene/search/join/ToChildBlockJoinQuery.java
===================================================================
--- modules/join/src/java/org/apache/lucene/search/join/ToChildBlockJoinQuery.java	(revision 1238034)
+++ modules/join/src/java/org/apache/lucene/search/join/ToChildBlockJoinQuery.java	(working copy)
@@ -22,7 +22,7 @@
 import java.util.Collections;
 import java.util.Set;
 
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexWriter;       // javadocs
 import org.apache.lucene.index.Term;
@@ -120,7 +120,7 @@
         return null;
       }
 
-      final DocIdSet parents = parentsFilter.getDocIdSet(readerContext, readerContext.reader.getLiveDocs());
+      final DocIdSet parents = parentsFilter.getDocIdSet(readerContext, readerContext.reader().getLiveDocs());
       // TODO: once we do random-access filters we can
       // generalize this:
       if (parents == null) {
Index: modules/join/src/java/org/apache/lucene/search/join/ToParentBlockJoinCollector.java
===================================================================
--- modules/join/src/java/org/apache/lucene/search/join/ToParentBlockJoinCollector.java	(revision 1238034)
+++ modules/join/src/java/org/apache/lucene/search/join/ToParentBlockJoinCollector.java	(working copy)
@@ -24,7 +24,7 @@
 import java.util.Map;
 import java.util.Queue;
 
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexWriter;       // javadocs
 import org.apache.lucene.search.Collector;
@@ -107,7 +107,7 @@
 
   private int docBase;
   private ToParentBlockJoinQuery.BlockJoinScorer[] joinScorers = new ToParentBlockJoinQuery.BlockJoinScorer[0];
-  private IndexReader.AtomicReaderContext currentReaderContext;
+  private AtomicReaderContext currentReaderContext;
   private Scorer scorer;
   private boolean queueFull;
 
Index: modules/join/src/java/org/apache/lucene/search/join/ToParentBlockJoinQuery.java
===================================================================
--- modules/join/src/java/org/apache/lucene/search/join/ToParentBlockJoinQuery.java	(revision 1238034)
+++ modules/join/src/java/org/apache/lucene/search/join/ToParentBlockJoinQuery.java	(working copy)
@@ -22,7 +22,7 @@
 import java.util.Collections;
 import java.util.Set;
 
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexWriter;       // javadocs
 import org.apache.lucene.index.Term;
@@ -186,7 +186,7 @@
         return null;
       }
 
-      final DocIdSet parents = parentsFilter.getDocIdSet(readerContext, readerContext.reader.getLiveDocs());
+      final DocIdSet parents = parentsFilter.getDocIdSet(readerContext, readerContext.reader().getLiveDocs());
       // TODO: once we do random-access filters we can
       // generalize this:
       if (parents == null) {
Index: modules/join/src/test/org/apache/lucene/search/join/TestBlockJoin.java
===================================================================
--- modules/join/src/test/org/apache/lucene/search/join/TestBlockJoin.java	(revision 1238034)
+++ modules/join/src/test/org/apache/lucene/search/join/TestBlockJoin.java	(working copy)
@@ -28,7 +28,7 @@
 import org.apache.lucene.document.NumericField;
 import org.apache.lucene.document.StoredField;
 import org.apache.lucene.document.StringField;
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.LogDocMergePolicy;
 import org.apache.lucene.index.RandomIndexWriter;
@@ -160,7 +160,7 @@
     final int subIndex = ReaderUtil.subIndex(childDocID, leaves);
     final AtomicReaderContext leaf = leaves[subIndex];
     final FixedBitSet bits = (FixedBitSet) parents.getDocIdSet(leaf, null);
-    return leaf.reader.document(bits.nextSetBit(childDocID - leaf.docBase));
+    return leaf.reader().document(bits.nextSetBit(childDocID - leaf.docBase));
   }
   
   public void testBoostBug() throws Exception {
Index: modules/join/src/test/org/apache/lucene/search/join/TestJoinUtil.java
===================================================================
--- modules/join/src/test/org/apache/lucene/search/join/TestJoinUtil.java	(revision 1238034)
+++ modules/join/src/test/org/apache/lucene/search/join/TestJoinUtil.java	(working copy)
@@ -184,7 +184,7 @@
             actualResult.set(doc + docBase);
           }
 
-          public void setNextReader(IndexReader.AtomicReaderContext context) throws IOException {
+          public void setNextReader(AtomicReaderContext context) throws IOException {
             docBase = context.docBase;
           }
 
Index: modules/queries/src/java/org/apache/lucene/queries/BooleanFilter.java
===================================================================
--- modules/queries/src/java/org/apache/lucene/queries/BooleanFilter.java	(revision 1238034)
+++ modules/queries/src/java/org/apache/lucene/queries/BooleanFilter.java	(working copy)
@@ -22,8 +22,8 @@
 import java.util.List;
 import java.util.Iterator;
 
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
+import org.apache.lucene.index.AtomicReader;
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.search.BitsFilteredDocIdSet;
 import org.apache.lucene.search.BooleanClause.Occur;
 import org.apache.lucene.search.DocIdSet;
@@ -52,7 +52,7 @@
   @Override
   public DocIdSet getDocIdSet(AtomicReaderContext context, Bits acceptDocs) throws IOException {
     FixedBitSet res = null;
-    final IndexReader reader = context.reader;
+    final AtomicReader reader = context.reader();
     
     boolean hasShouldClauses = false;
     for (final FilterClause fc : clauses) {
Index: modules/queries/src/java/org/apache/lucene/queries/ChainedFilter.java
===================================================================
--- modules/queries/src/java/org/apache/lucene/queries/ChainedFilter.java	(revision 1238034)
+++ modules/queries/src/java/org/apache/lucene/queries/ChainedFilter.java	(working copy)
@@ -17,8 +17,8 @@
  * limitations under the License.
  */
 
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
+import org.apache.lucene.index.AtomicReader;
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.search.BitsFilteredDocIdSet;
 import org.apache.lucene.search.DocIdSet;
 import org.apache.lucene.search.DocIdSetIterator;
@@ -129,7 +129,7 @@
 
   private OpenBitSetDISI initialResult(AtomicReaderContext context, int logic, int[] index)
       throws IOException {
-    IndexReader reader = context.reader;
+    AtomicReader reader = context.reader();
     OpenBitSetDISI result;
     /**
      * First AND operation takes place against a completely false
Index: modules/queries/src/java/org/apache/lucene/queries/CustomScoreProvider.java
===================================================================
--- modules/queries/src/java/org/apache/lucene/queries/CustomScoreProvider.java	(revision 1238034)
+++ modules/queries/src/java/org/apache/lucene/queries/CustomScoreProvider.java	(working copy)
@@ -19,7 +19,7 @@
 
 import java.io.IOException;
 
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.IndexReader; // for javadocs
 import org.apache.lucene.search.Explanation;
 import org.apache.lucene.search.FieldCache; // for javadocs
Index: modules/queries/src/java/org/apache/lucene/queries/CustomScoreQuery.java
===================================================================
--- modules/queries/src/java/org/apache/lucene/queries/CustomScoreQuery.java	(revision 1238034)
+++ modules/queries/src/java/org/apache/lucene/queries/CustomScoreQuery.java	(working copy)
@@ -21,8 +21,8 @@
 import java.util.Set;
 import java.util.Arrays;
 
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.search.ComplexExplanation;
 import org.apache.lucene.search.Explanation;
Index: modules/queries/src/java/org/apache/lucene/queries/function/BoostedQuery.java
===================================================================
--- modules/queries/src/java/org/apache/lucene/queries/function/BoostedQuery.java	(revision 1238034)
+++ modules/queries/src/java/org/apache/lucene/queries/function/BoostedQuery.java	(working copy)
@@ -18,8 +18,8 @@
  */
 
 import org.apache.lucene.search.*;
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
 import org.apache.lucene.util.Bits;
 import org.apache.lucene.util.ToStringUtils;
 
Index: modules/queries/src/java/org/apache/lucene/queries/function/docvalues/StringIndexDocValues.java
===================================================================
--- modules/queries/src/java/org/apache/lucene/queries/function/docvalues/StringIndexDocValues.java	(revision 1238034)
+++ modules/queries/src/java/org/apache/lucene/queries/function/docvalues/StringIndexDocValues.java	(working copy)
@@ -21,8 +21,8 @@
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.queries.function.ValueSourceScorer;
 import org.apache.lucene.search.FieldCache;
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.CharsRef;
 import org.apache.lucene.util.UnicodeUtil;
@@ -43,7 +43,7 @@
 
   public StringIndexDocValues(ValueSource vs, AtomicReaderContext context, String field) throws IOException {
     try {
-      termsIndex = FieldCache.DEFAULT.getTermsIndex(context.reader, field);
+      termsIndex = FieldCache.DEFAULT.getTermsIndex(context.reader(), field);
     } catch (RuntimeException e) {
       throw new StringIndexException(field, e);
     }
Index: modules/queries/src/java/org/apache/lucene/queries/function/FunctionQuery.java
===================================================================
--- modules/queries/src/java/org/apache/lucene/queries/function/FunctionQuery.java	(revision 1238034)
+++ modules/queries/src/java/org/apache/lucene/queries/function/FunctionQuery.java	(working copy)
@@ -17,8 +17,8 @@
  * limitations under the License.
  */
 
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
 import org.apache.lucene.search.*;
 import org.apache.lucene.index.MultiFields;
 import org.apache.lucene.util.Bits;
@@ -96,7 +96,7 @@
 
     @Override
     public Explanation explain(AtomicReaderContext context, int doc) throws IOException {
-      return ((AllScorer)scorer(context, true, true, context.reader.getLiveDocs())).explain(doc);
+      return ((AllScorer)scorer(context, true, true, context.reader().getLiveDocs())).explain(doc);
     }
   }
 
@@ -113,7 +113,7 @@
       super(w);
       this.weight = w;
       this.qWeight = qWeight;
-      this.reader = context.reader;
+      this.reader = context.reader();
       this.maxDoc = reader.maxDoc();
       this.liveDocs = acceptDocs;
       vals = func.getValues(weight.context, context);
Index: modules/queries/src/java/org/apache/lucene/queries/function/ValueSource.java
===================================================================
--- modules/queries/src/java/org/apache/lucene/queries/function/ValueSource.java	(revision 1238034)
+++ modules/queries/src/java/org/apache/lucene/queries/function/ValueSource.java	(working copy)
@@ -17,15 +17,14 @@
  * limitations under the License.
  */
 
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
 import org.apache.lucene.search.FieldComparator;
 import org.apache.lucene.search.FieldComparatorSource;
 import org.apache.lucene.search.Scorer;
 import org.apache.lucene.search.IndexSearcher;
 import org.apache.lucene.search.SortField;
 import org.apache.lucene.util.Bits;
-import org.apache.lucene.index.MultiFields;
 
 import java.io.IOException;
 import java.io.Serializable;
@@ -89,7 +88,7 @@
   /**
    * EXPERIMENTAL: This method is subject to change.
    * <p>
-   * Get the SortField for this ValueSource.  Uses the {@link #getValues(java.util.Map, IndexReader.AtomicReaderContext)}
+   * Get the SortField for this ValueSource.  Uses the {@link #getValues(java.util.Map, AtomicReaderContext)}
    * to populate the SortField.
    *
    * @param reverse true if this is a reverse sort.
Index: modules/queries/src/java/org/apache/lucene/queries/function/valuesource/ByteFieldSource.java
===================================================================
--- modules/queries/src/java/org/apache/lucene/queries/function/valuesource/ByteFieldSource.java	(revision 1238034)
+++ modules/queries/src/java/org/apache/lucene/queries/function/valuesource/ByteFieldSource.java	(working copy)
@@ -19,7 +19,7 @@
 import java.io.IOException;
 import java.util.Map;
 
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.search.FieldCache;
 
@@ -51,7 +51,7 @@
 
   @Override
   public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
-    final byte[] arr = cache.getBytes(readerContext.reader, field, parser, false);
+    final byte[] arr = cache.getBytes(readerContext.reader(), field, parser, false);
     
     return new FunctionValues() {
       @Override
Index: modules/queries/src/java/org/apache/lucene/queries/function/valuesource/BytesRefFieldSource.java
===================================================================
--- modules/queries/src/java/org/apache/lucene/queries/function/valuesource/BytesRefFieldSource.java	(revision 1238034)
+++ modules/queries/src/java/org/apache/lucene/queries/function/valuesource/BytesRefFieldSource.java	(working copy)
@@ -17,7 +17,7 @@
  * limitations under the License.
  */
 
-import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.docvalues.StringIndexDocValues;
 import org.apache.lucene.queries.function.ValueSource; //javadoc
@@ -35,7 +35,7 @@
   }
 
   @Override
-  public FunctionValues getValues(Map context, IndexReader.AtomicReaderContext readerContext) throws IOException {
+  public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
     return new StringIndexDocValues(this, readerContext, field) {
 
       @Override
Index: modules/queries/src/java/org/apache/lucene/queries/function/valuesource/ConstValueSource.java
===================================================================
--- modules/queries/src/java/org/apache/lucene/queries/function/valuesource/ConstValueSource.java	(revision 1238034)
+++ modules/queries/src/java/org/apache/lucene/queries/function/valuesource/ConstValueSource.java	(working copy)
@@ -17,7 +17,7 @@
 
 package org.apache.lucene.queries.function.valuesource;
 
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.docvalues.FloatDocValues;
 
Index: modules/queries/src/java/org/apache/lucene/queries/function/valuesource/DefFunction.java
===================================================================
--- modules/queries/src/java/org/apache/lucene/queries/function/valuesource/DefFunction.java	(revision 1238034)
+++ modules/queries/src/java/org/apache/lucene/queries/function/valuesource/DefFunction.java	(working copy)
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.search.IndexSearcher;
Index: modules/queries/src/java/org/apache/lucene/queries/function/valuesource/DocFreqValueSource.java
===================================================================
--- modules/queries/src/java/org/apache/lucene/queries/function/valuesource/DocFreqValueSource.java	(revision 1238034)
+++ modules/queries/src/java/org/apache/lucene/queries/function/valuesource/DocFreqValueSource.java	(working copy)
@@ -17,7 +17,7 @@
 
 package org.apache.lucene.queries.function.valuesource;
 
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
Index: modules/queries/src/java/org/apache/lucene/queries/function/valuesource/DoubleConstValueSource.java
===================================================================
--- modules/queries/src/java/org/apache/lucene/queries/function/valuesource/DoubleConstValueSource.java	(revision 1238034)
+++ modules/queries/src/java/org/apache/lucene/queries/function/valuesource/DoubleConstValueSource.java	(working copy)
@@ -17,7 +17,7 @@
 
 package org.apache.lucene.queries.function.valuesource;
 
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.docvalues.DoubleDocValues;
 
Index: modules/queries/src/java/org/apache/lucene/queries/function/valuesource/DoubleFieldSource.java
===================================================================
--- modules/queries/src/java/org/apache/lucene/queries/function/valuesource/DoubleFieldSource.java	(revision 1238034)
+++ modules/queries/src/java/org/apache/lucene/queries/function/valuesource/DoubleFieldSource.java	(working copy)
@@ -20,7 +20,7 @@
 import java.io.IOException;
 import java.util.Map;
 
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSourceScorer;
@@ -57,8 +57,8 @@
 
   @Override
   public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
-    final double[] arr = cache.getDoubles(readerContext.reader, field, parser, true);
-    final Bits valid = cache.getDocsWithField(readerContext.reader, field);
+    final double[] arr = cache.getDoubles(readerContext.reader(), field, parser, true);
+    final Bits valid = cache.getDocsWithField(readerContext.reader(), field);
     return new DoubleDocValues(this) {
       @Override
       public double doubleVal(int doc) {
Index: modules/queries/src/java/org/apache/lucene/queries/function/valuesource/DualFloatFunction.java
===================================================================
--- modules/queries/src/java/org/apache/lucene/queries/function/valuesource/DualFloatFunction.java	(revision 1238034)
+++ modules/queries/src/java/org/apache/lucene/queries/function/valuesource/DualFloatFunction.java	(working copy)
@@ -17,7 +17,7 @@
 
 package org.apache.lucene.queries.function.valuesource;
 
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.queries.function.docvalues.FloatDocValues;
Index: modules/queries/src/java/org/apache/lucene/queries/function/valuesource/FloatFieldSource.java
===================================================================
--- modules/queries/src/java/org/apache/lucene/queries/function/valuesource/FloatFieldSource.java	(revision 1238034)
+++ modules/queries/src/java/org/apache/lucene/queries/function/valuesource/FloatFieldSource.java	(working copy)
@@ -20,7 +20,7 @@
 import java.io.IOException;
 import java.util.Map;
 
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.docvalues.FloatDocValues;
 import org.apache.lucene.search.FieldCache;
@@ -55,8 +55,8 @@
 
   @Override
   public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
-    final float[] arr = cache.getFloats(readerContext.reader, field, parser, true);
-    final Bits valid = cache.getDocsWithField(readerContext.reader, field);
+    final float[] arr = cache.getFloats(readerContext.reader(), field, parser, true);
+    final Bits valid = cache.getDocsWithField(readerContext.reader(), field);
 
     return new FloatDocValues(this) {
       @Override
Index: modules/queries/src/java/org/apache/lucene/queries/function/valuesource/IDFValueSource.java
===================================================================
--- modules/queries/src/java/org/apache/lucene/queries/function/valuesource/IDFValueSource.java	(revision 1238034)
+++ modules/queries/src/java/org/apache/lucene/queries/function/valuesource/IDFValueSource.java	(working copy)
@@ -18,7 +18,6 @@
 package org.apache.lucene.queries.function.valuesource;
 
 import org.apache.lucene.index.*;
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
 import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.search.IndexSearcher;
 import org.apache.lucene.search.similarities.Similarity;
Index: modules/queries/src/java/org/apache/lucene/queries/function/valuesource/IfFunction.java
===================================================================
--- modules/queries/src/java/org/apache/lucene/queries/function/valuesource/IfFunction.java	(revision 1238034)
+++ modules/queries/src/java/org/apache/lucene/queries/function/valuesource/IfFunction.java	(working copy)
@@ -17,8 +17,8 @@
 
 package org.apache.lucene.queries.function.valuesource;
 
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
 import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.search.Explanation;
Index: modules/queries/src/java/org/apache/lucene/queries/function/valuesource/IntFieldSource.java
===================================================================
--- modules/queries/src/java/org/apache/lucene/queries/function/valuesource/IntFieldSource.java	(revision 1238034)
+++ modules/queries/src/java/org/apache/lucene/queries/function/valuesource/IntFieldSource.java	(working copy)
@@ -20,7 +20,7 @@
 import java.io.IOException;
 import java.util.Map;
 
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSourceScorer;
@@ -57,8 +57,8 @@
 
   @Override
   public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
-    final int[] arr = cache.getInts(readerContext.reader, field, parser, true);
-    final Bits valid = cache.getDocsWithField(readerContext.reader, field);
+    final int[] arr = cache.getInts(readerContext.reader(), field, parser, true);
+    final Bits valid = cache.getDocsWithField(readerContext.reader(), field);
     
     return new IntDocValues(this) {
       final MutableValueInt val = new MutableValueInt();
Index: modules/queries/src/java/org/apache/lucene/queries/function/valuesource/JoinDocFreqValueSource.java
===================================================================
--- modules/queries/src/java/org/apache/lucene/queries/function/valuesource/JoinDocFreqValueSource.java	(revision 1238034)
+++ modules/queries/src/java/org/apache/lucene/queries/function/valuesource/JoinDocFreqValueSource.java	(working copy)
@@ -20,8 +20,8 @@
 import java.io.IOException;
 import java.util.Map;
 
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
 import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.docvalues.IntDocValues;
 import org.apache.lucene.search.FieldCache.DocTerms;
@@ -52,8 +52,8 @@
   @Override
   public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException
   {
-    final DocTerms terms = cache.getTerms(readerContext.reader, field, true );
-    final IndexReader top = ReaderUtil.getTopLevelContext(readerContext).reader;
+    final DocTerms terms = cache.getTerms(readerContext.reader(), field, true );
+    final IndexReader top = ReaderUtil.getTopLevelContext(readerContext).reader();
     
     return new IntDocValues(this) {
       BytesRef ref = new BytesRef();
Index: modules/queries/src/java/org/apache/lucene/queries/function/valuesource/LinearFloatFunction.java
===================================================================
--- modules/queries/src/java/org/apache/lucene/queries/function/valuesource/LinearFloatFunction.java	(revision 1238034)
+++ modules/queries/src/java/org/apache/lucene/queries/function/valuesource/LinearFloatFunction.java	(working copy)
@@ -17,7 +17,7 @@
 
 package org.apache.lucene.queries.function.valuesource;
 
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.queries.function.docvalues.FloatDocValues;
Index: modules/queries/src/java/org/apache/lucene/queries/function/valuesource/LiteralValueSource.java
===================================================================
--- modules/queries/src/java/org/apache/lucene/queries/function/valuesource/LiteralValueSource.java	(revision 1238034)
+++ modules/queries/src/java/org/apache/lucene/queries/function/valuesource/LiteralValueSource.java	(working copy)
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.queries.function.docvalues.StrDocValues;
Index: modules/queries/src/java/org/apache/lucene/queries/function/valuesource/LongFieldSource.java
===================================================================
--- modules/queries/src/java/org/apache/lucene/queries/function/valuesource/LongFieldSource.java	(revision 1238034)
+++ modules/queries/src/java/org/apache/lucene/queries/function/valuesource/LongFieldSource.java	(working copy)
@@ -20,7 +20,7 @@
 import java.io.IOException;
 import java.util.Map;
 
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSourceScorer;
@@ -66,8 +66,8 @@
 
   @Override
   public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
-    final long[] arr = cache.getLongs(readerContext.reader, field, parser, true);
-    final Bits valid = cache.getDocsWithField(readerContext.reader, field);
+    final long[] arr = cache.getLongs(readerContext.reader(), field, parser, true);
+    final Bits valid = cache.getDocsWithField(readerContext.reader(), field);
     
     return new LongDocValues(this) {
       @Override
Index: modules/queries/src/java/org/apache/lucene/queries/function/valuesource/MaxDocValueSource.java
===================================================================
--- modules/queries/src/java/org/apache/lucene/queries/function/valuesource/MaxDocValueSource.java	(revision 1238034)
+++ modules/queries/src/java/org/apache/lucene/queries/function/valuesource/MaxDocValueSource.java	(working copy)
@@ -16,7 +16,7 @@
  */
 package org.apache.lucene.queries.function.valuesource;
 
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.search.IndexSearcher;
Index: modules/queries/src/java/org/apache/lucene/queries/function/valuesource/MultiBoolFunction.java
===================================================================
--- modules/queries/src/java/org/apache/lucene/queries/function/valuesource/MultiBoolFunction.java	(revision 1238034)
+++ modules/queries/src/java/org/apache/lucene/queries/function/valuesource/MultiBoolFunction.java	(working copy)
@@ -17,7 +17,7 @@
 
 package org.apache.lucene.queries.function.valuesource;
 
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.queries.function.docvalues.BoolDocValues;
Index: modules/queries/src/java/org/apache/lucene/queries/function/valuesource/MultiFloatFunction.java
===================================================================
--- modules/queries/src/java/org/apache/lucene/queries/function/valuesource/MultiFloatFunction.java	(revision 1238034)
+++ modules/queries/src/java/org/apache/lucene/queries/function/valuesource/MultiFloatFunction.java	(working copy)
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.queries.function.docvalues.FloatDocValues;
Index: modules/queries/src/java/org/apache/lucene/queries/function/valuesource/MultiFunction.java
===================================================================
--- modules/queries/src/java/org/apache/lucene/queries/function/valuesource/MultiFunction.java	(revision 1238034)
+++ modules/queries/src/java/org/apache/lucene/queries/function/valuesource/MultiFunction.java	(working copy)
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.search.IndexSearcher;
Index: modules/queries/src/java/org/apache/lucene/queries/function/valuesource/NormValueSource.java
===================================================================
--- modules/queries/src/java/org/apache/lucene/queries/function/valuesource/NormValueSource.java	(revision 1238034)
+++ modules/queries/src/java/org/apache/lucene/queries/function/valuesource/NormValueSource.java	(working copy)
@@ -17,8 +17,8 @@
 
 package org.apache.lucene.queries.function.valuesource;
 
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.DocValues;
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
 import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.queries.function.docvalues.FloatDocValues;
@@ -57,7 +57,7 @@
       throw new UnsupportedOperationException("requires a TFIDFSimilarity (such as DefaultSimilarity)");
     }
     final TFIDFSimilarity similarity = (TFIDFSimilarity) sim;
-    DocValues dv = readerContext.reader.normValues(field);
+    DocValues dv = readerContext.reader().normValues(field);
 
     if (dv == null) {
       return new ConstDoubleDocValues(0.0, this);
Index: modules/queries/src/java/org/apache/lucene/queries/function/valuesource/NumDocsValueSource.java
===================================================================
--- modules/queries/src/java/org/apache/lucene/queries/function/valuesource/NumDocsValueSource.java	(revision 1238034)
+++ modules/queries/src/java/org/apache/lucene/queries/function/valuesource/NumDocsValueSource.java	(working copy)
@@ -16,7 +16,7 @@
  */
 package org.apache.lucene.queries.function.valuesource;
 
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.util.ReaderUtil;
@@ -37,7 +37,7 @@
   @Override
   public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
     // Searcher has no numdocs so we must use the reader instead
-    return new ConstIntDocValues(ReaderUtil.getTopLevelContext(readerContext).reader.numDocs(), this);
+    return new ConstIntDocValues(ReaderUtil.getTopLevelContext(readerContext).reader().numDocs(), this);
   }
 
   @Override
Index: modules/queries/src/java/org/apache/lucene/queries/function/valuesource/NumericIndexDocValueSource.java
===================================================================
--- modules/queries/src/java/org/apache/lucene/queries/function/valuesource/NumericIndexDocValueSource.java	(revision 1238034)
+++ modules/queries/src/java/org/apache/lucene/queries/function/valuesource/NumericIndexDocValueSource.java	(working copy)
@@ -19,9 +19,9 @@
 import java.io.IOException;
 import java.util.Map;
 
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.DocValues.Source;
 import org.apache.lucene.index.DocValues.Type;
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
 import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 
@@ -43,7 +43,7 @@
 
   @Override
   public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
-    final Source source = readerContext.reader.docValues(field)
+    final Source source = readerContext.reader().docValues(field)
         .getSource();
     Type type = source.type();
     switch (type) {
Index: modules/queries/src/java/org/apache/lucene/queries/function/valuesource/OrdFieldSource.java
===================================================================
--- modules/queries/src/java/org/apache/lucene/queries/function/valuesource/OrdFieldSource.java	(revision 1238034)
+++ modules/queries/src/java/org/apache/lucene/queries/function/valuesource/OrdFieldSource.java	(working copy)
@@ -17,9 +17,11 @@
 
 package org.apache.lucene.queries.function.valuesource;
 
+import org.apache.lucene.index.AtomicReader;
+import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.CompositeReader;
 import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
-import org.apache.lucene.index.SlowMultiReaderWrapper;
+import org.apache.lucene.index.SlowCompositeReaderWrapper;
 import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.queries.function.docvalues.IntDocValues;
@@ -66,8 +68,11 @@
   @Override
   public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
     final int off = readerContext.docBase;
-    final IndexReader topReader = ReaderUtil.getTopLevelContext(readerContext).reader;
-    final FieldCache.DocTermsIndex sindex = FieldCache.DEFAULT.getTermsIndex(new SlowMultiReaderWrapper(topReader), field);
+    final IndexReader topReader = ReaderUtil.getTopLevelContext(readerContext).reader();
+    final AtomicReader r = topReader instanceof CompositeReader 
+        ? new SlowCompositeReaderWrapper((CompositeReader)topReader) 
+        : (AtomicReader) topReader;
+    final FieldCache.DocTermsIndex sindex = FieldCache.DEFAULT.getTermsIndex(r, field);
     return new IntDocValues(this) {
       protected String toTerm(String readableValue) {
         return readableValue;
Index: modules/queries/src/java/org/apache/lucene/queries/function/valuesource/QueryValueSource.java
===================================================================
--- modules/queries/src/java/org/apache/lucene/queries/function/valuesource/QueryValueSource.java	(revision 1238034)
+++ modules/queries/src/java/org/apache/lucene/queries/function/valuesource/QueryValueSource.java	(working copy)
@@ -17,7 +17,7 @@
 
 package org.apache.lucene.queries.function.valuesource;
 
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.queries.function.docvalues.FloatDocValues;
@@ -96,7 +96,7 @@
     super(vs);
 
     this.readerContext = readerContext;
-    this.acceptDocs = readerContext.reader.getLiveDocs();
+    this.acceptDocs = readerContext.reader().getLiveDocs();
     this.defVal = vs.defVal;
     this.q = vs.q;
     this.fcontext = fcontext;
Index: modules/queries/src/java/org/apache/lucene/queries/function/valuesource/RangeMapFloatFunction.java
===================================================================
--- modules/queries/src/java/org/apache/lucene/queries/function/valuesource/RangeMapFloatFunction.java	(revision 1238034)
+++ modules/queries/src/java/org/apache/lucene/queries/function/valuesource/RangeMapFloatFunction.java	(working copy)
@@ -17,7 +17,7 @@
 
 package org.apache.lucene.queries.function.valuesource;
 
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.queries.function.docvalues.FloatDocValues;
Index: modules/queries/src/java/org/apache/lucene/queries/function/valuesource/ReciprocalFloatFunction.java
===================================================================
--- modules/queries/src/java/org/apache/lucene/queries/function/valuesource/ReciprocalFloatFunction.java	(revision 1238034)
+++ modules/queries/src/java/org/apache/lucene/queries/function/valuesource/ReciprocalFloatFunction.java	(working copy)
@@ -17,7 +17,7 @@
 
 package org.apache.lucene.queries.function.valuesource;
 
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.queries.function.docvalues.FloatDocValues;
Index: modules/queries/src/java/org/apache/lucene/queries/function/valuesource/ReverseOrdFieldSource.java
===================================================================
--- modules/queries/src/java/org/apache/lucene/queries/function/valuesource/ReverseOrdFieldSource.java	(revision 1238034)
+++ modules/queries/src/java/org/apache/lucene/queries/function/valuesource/ReverseOrdFieldSource.java	(working copy)
@@ -17,9 +17,11 @@
 
 package org.apache.lucene.queries.function.valuesource;
 
+import org.apache.lucene.index.AtomicReader;
+import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.CompositeReader;
 import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
-import org.apache.lucene.index.SlowMultiReaderWrapper;
+import org.apache.lucene.index.SlowCompositeReaderWrapper;
 import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.queries.function.docvalues.IntDocValues;
@@ -65,10 +67,13 @@
   // TODO: this is trappy? perhaps this query instead should make you pass a slow reader yourself?
   @Override
   public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
-    final IndexReader topReader = ReaderUtil.getTopLevelContext(readerContext).reader;
+    final IndexReader topReader = ReaderUtil.getTopLevelContext(readerContext).reader();
+    final AtomicReader r = topReader instanceof CompositeReader 
+        ? new SlowCompositeReaderWrapper((CompositeReader)topReader) 
+        : (AtomicReader) topReader;
     final int off = readerContext.docBase;
 
-    final FieldCache.DocTermsIndex sindex = FieldCache.DEFAULT.getTermsIndex(new SlowMultiReaderWrapper(topReader), field);
+    final FieldCache.DocTermsIndex sindex = FieldCache.DEFAULT.getTermsIndex(r, field);
     final int end = sindex.numOrd();
 
     return new IntDocValues(this) {
Index: modules/queries/src/java/org/apache/lucene/queries/function/valuesource/ScaleFloatFunction.java
===================================================================
--- modules/queries/src/java/org/apache/lucene/queries/function/valuesource/ScaleFloatFunction.java	(revision 1238034)
+++ modules/queries/src/java/org/apache/lucene/queries/function/valuesource/ScaleFloatFunction.java	(working copy)
@@ -17,7 +17,7 @@
 
 package org.apache.lucene.queries.function.valuesource;
 
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.queries.function.docvalues.FloatDocValues;
@@ -66,7 +66,7 @@
     float maxVal = Float.NEGATIVE_INFINITY;
 
     for (AtomicReaderContext leaf : leaves) {
-      int maxDoc = leaf.reader.maxDoc();
+      int maxDoc = leaf.reader().maxDoc();
       FunctionValues vals =  source.getValues(context, leaf);
       for (int i=0; i<maxDoc; i++) {
 
Index: modules/queries/src/java/org/apache/lucene/queries/function/valuesource/ShortFieldSource.java
===================================================================
--- modules/queries/src/java/org/apache/lucene/queries/function/valuesource/ShortFieldSource.java	(revision 1238034)
+++ modules/queries/src/java/org/apache/lucene/queries/function/valuesource/ShortFieldSource.java	(working copy)
@@ -19,7 +19,7 @@
 import java.io.IOException;
 import java.util.Map;
 
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.search.FieldCache;
 
@@ -48,7 +48,7 @@
 
   @Override
   public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
-    final short[] arr = cache.getShorts(readerContext.reader, field, parser, false);
+    final short[] arr = cache.getShorts(readerContext.reader(), field, parser, false);
     
     return new FunctionValues() {
       @Override
Index: modules/queries/src/java/org/apache/lucene/queries/function/valuesource/SimpleBoolFunction.java
===================================================================
--- modules/queries/src/java/org/apache/lucene/queries/function/valuesource/SimpleBoolFunction.java	(revision 1238034)
+++ modules/queries/src/java/org/apache/lucene/queries/function/valuesource/SimpleBoolFunction.java	(working copy)
@@ -17,7 +17,7 @@
 
 package org.apache.lucene.queries.function.valuesource;
 
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.queries.function.docvalues.BoolDocValues;
Index: modules/queries/src/java/org/apache/lucene/queries/function/valuesource/SimpleFloatFunction.java
===================================================================
--- modules/queries/src/java/org/apache/lucene/queries/function/valuesource/SimpleFloatFunction.java	(revision 1238034)
+++ modules/queries/src/java/org/apache/lucene/queries/function/valuesource/SimpleFloatFunction.java	(working copy)
@@ -17,7 +17,7 @@
 
 package org.apache.lucene.queries.function.valuesource;
 
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.queries.function.docvalues.FloatDocValues;
Index: modules/queries/src/java/org/apache/lucene/queries/function/valuesource/SumTotalTermFreqValueSource.java
===================================================================
--- modules/queries/src/java/org/apache/lucene/queries/function/valuesource/SumTotalTermFreqValueSource.java	(revision 1238034)
+++ modules/queries/src/java/org/apache/lucene/queries/function/valuesource/SumTotalTermFreqValueSource.java	(working copy)
@@ -17,8 +17,8 @@
 
 package org.apache.lucene.queries.function.valuesource;
 
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.Fields;
-import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.Terms;
 import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
@@ -50,15 +50,15 @@
   }
 
   @Override
-  public FunctionValues getValues(Map context, IndexReader.AtomicReaderContext readerContext) throws IOException {
+  public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
     return (FunctionValues)context.get(this);
   }
 
   @Override
   public void createWeight(Map context, IndexSearcher searcher) throws IOException {
     long sumTotalTermFreq = 0;
-    for (IndexReader.AtomicReaderContext readerContext : searcher.getTopReaderContext().leaves()) {
-      Fields fields = readerContext.reader.fields();
+    for (AtomicReaderContext readerContext : searcher.getTopReaderContext().leaves()) {
+      Fields fields = readerContext.reader().fields();
       if (fields == null) continue;
       Terms terms = fields.terms(indexedField);
       if (terms == null) continue;
Index: modules/queries/src/java/org/apache/lucene/queries/function/valuesource/TermFreqValueSource.java
===================================================================
--- modules/queries/src/java/org/apache/lucene/queries/function/valuesource/TermFreqValueSource.java	(revision 1238034)
+++ modules/queries/src/java/org/apache/lucene/queries/function/valuesource/TermFreqValueSource.java	(working copy)
@@ -18,7 +18,6 @@
 package org.apache.lucene.queries.function.valuesource;
 
 import org.apache.lucene.index.*;
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
 import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.docvalues.IntDocValues;
 import org.apache.lucene.search.DocIdSetIterator;
@@ -39,7 +38,7 @@
 
   @Override
   public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
-    Fields fields = readerContext.reader.fields();
+    Fields fields = readerContext.reader().fields();
     final Terms terms = fields.terms(field);
 
     return new IntDocValues(this) {
Index: modules/queries/src/java/org/apache/lucene/queries/function/valuesource/TFValueSource.java
===================================================================
--- modules/queries/src/java/org/apache/lucene/queries/function/valuesource/TFValueSource.java	(revision 1238034)
+++ modules/queries/src/java/org/apache/lucene/queries/function/valuesource/TFValueSource.java	(working copy)
@@ -18,7 +18,6 @@
  */
 
 import org.apache.lucene.index.*;
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
 import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.docvalues.FloatDocValues;
 import org.apache.lucene.search.DocIdSetIterator;
@@ -42,7 +41,7 @@
 
   @Override
   public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
-    Fields fields = readerContext.reader.fields();
+    Fields fields = readerContext.reader().fields();
     final Terms terms = fields.terms(field);
     final Similarity sim = ((IndexSearcher)context.get("searcher")).getSimilarityProvider().get(field);
     if (!(sim instanceof TFIDFSimilarity)) {
Index: modules/queries/src/java/org/apache/lucene/queries/function/valuesource/TotalTermFreqValueSource.java
===================================================================
--- modules/queries/src/java/org/apache/lucene/queries/function/valuesource/TotalTermFreqValueSource.java	(revision 1238034)
+++ modules/queries/src/java/org/apache/lucene/queries/function/valuesource/TotalTermFreqValueSource.java	(working copy)
@@ -17,7 +17,7 @@
 
 package org.apache.lucene.queries.function.valuesource;
 
-import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.queries.function.docvalues.LongDocValues;
@@ -54,15 +54,15 @@
   }
 
   @Override
-  public FunctionValues getValues(Map context, IndexReader.AtomicReaderContext readerContext) throws IOException {
+  public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
     return (FunctionValues)context.get(this);
   }
 
   @Override
   public void createWeight(Map context, IndexSearcher searcher) throws IOException {
     long totalTermFreq = 0;
-    for (IndexReader.AtomicReaderContext readerContext : searcher.getTopReaderContext().leaves()) {
-      totalTermFreq += readerContext.reader.totalTermFreq(indexedField, indexedBytes);
+    for (AtomicReaderContext readerContext : searcher.getTopReaderContext().leaves()) {
+      totalTermFreq += readerContext.reader().totalTermFreq(indexedField, indexedBytes);
     }
     final long ttf = Math.max(-1, totalTermFreq);  // we may have added up -1s if not supported
     context.put(this, new LongDocValues(this) {
Index: modules/queries/src/java/org/apache/lucene/queries/function/valuesource/VectorValueSource.java
===================================================================
--- modules/queries/src/java/org/apache/lucene/queries/function/valuesource/VectorValueSource.java	(revision 1238034)
+++ modules/queries/src/java/org/apache/lucene/queries/function/valuesource/VectorValueSource.java	(working copy)
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.search.IndexSearcher;
Index: modules/queries/src/java/org/apache/lucene/queries/mlt/MoreLikeThis.java
===================================================================
--- modules/queries/src/java/org/apache/lucene/queries/mlt/MoreLikeThis.java	(revision 1238034)
+++ modules/queries/src/java/org/apache/lucene/queries/mlt/MoreLikeThis.java	(working copy)
@@ -25,6 +25,7 @@
 import org.apache.lucene.index.Fields;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexableField;
+import org.apache.lucene.index.MultiFields;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.index.Terms;
 import org.apache.lucene.index.TermsEnum;
@@ -34,7 +35,6 @@
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.CharsRef;
 import org.apache.lucene.util.PriorityQueue;
-import org.apache.lucene.util.ReaderUtil;
 import org.apache.lucene.util.UnicodeUtil;
 
 
@@ -570,7 +570,7 @@
   public Query like(int docNum) throws IOException {
     if (fieldNames == null) {
       // gather list of valid fields from lucene
-      Collection<String> fields = ReaderUtil.getIndexedFields(ir);
+      Collection<String> fields = MultiFields.getIndexedFields(ir);
       fieldNames = fields.toArray(new String[fields.size()]);
     }
 
Index: modules/queries/src/java/org/apache/lucene/queries/TermsFilter.java
===================================================================
--- modules/queries/src/java/org/apache/lucene/queries/TermsFilter.java	(revision 1238034)
+++ modules/queries/src/java/org/apache/lucene/queries/TermsFilter.java	(working copy)
@@ -18,7 +18,6 @@
  */
 
 import org.apache.lucene.index.*;
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
 import org.apache.lucene.search.DocIdSet;
 import org.apache.lucene.search.Filter;
 import org.apache.lucene.util.Bits;
@@ -55,7 +54,7 @@
 
   @Override
   public DocIdSet getDocIdSet(AtomicReaderContext context, Bits acceptDocs) throws IOException {
-    IndexReader reader = context.reader;
+    AtomicReader reader = context.reader();
     FixedBitSet result = new FixedBitSet(reader.maxDoc());
     Fields fields = reader.fields();
 
Index: modules/queries/src/test/org/apache/lucene/queries/BooleanFilterTest.java
===================================================================
--- modules/queries/src/test/org/apache/lucene/queries/BooleanFilterTest.java	(revision 1238034)
+++ modules/queries/src/test/org/apache/lucene/queries/BooleanFilterTest.java	(working copy)
@@ -21,17 +21,16 @@
 import org.apache.lucene.analysis.MockTokenizer;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.TextField;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
+import org.apache.lucene.index.AtomicReader;
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.RandomIndexWriter;
-import org.apache.lucene.index.SlowMultiReaderWrapper;
+import org.apache.lucene.index.SlowCompositeReaderWrapper;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.search.BooleanClause.Occur;
 import org.apache.lucene.search.DocIdSetIterator;
 import org.apache.lucene.search.Filter;
 import org.apache.lucene.search.TermRangeFilter;
 import org.apache.lucene.search.DocIdSet;
-import org.apache.lucene.search.DocIdSetIterator;
 import org.apache.lucene.search.TermQuery;
 import org.apache.lucene.search.QueryWrapperFilter;
 import org.apache.lucene.store.Directory;
@@ -43,7 +42,7 @@
 
 public class BooleanFilterTest extends LuceneTestCase {
   private Directory directory;
-  private IndexReader reader;
+  private AtomicReader reader;
 
   @Override
   public void setUp() throws Exception {
@@ -57,7 +56,7 @@
     addDoc(writer, "guest", "020", "20050101", "Y");
     addDoc(writer, "admin", "020", "20050101", "Maybe");
     addDoc(writer, "admin guest", "030", "20050101", "N");
-    reader = new SlowMultiReaderWrapper(writer.getReader());
+    reader = new SlowCompositeReaderWrapper(writer.getReader());
     writer.close();
   }
 
@@ -97,7 +96,7 @@
     return new Filter() {
       @Override
       public DocIdSet getDocIdSet(AtomicReaderContext context, Bits acceptDocs) {
-        return new FixedBitSet(context.reader.maxDoc());
+        return new FixedBitSet(context.reader().maxDoc());
       }
     };
   }
@@ -133,7 +132,7 @@
   private void tstFilterCard(String mes, int expected, Filter filt)
       throws Exception {
     // BooleanFilter never returns null DIS or null DISI!
-    DocIdSetIterator disi = filt.getDocIdSet(new AtomicReaderContext(reader), reader.getLiveDocs()).iterator();
+    DocIdSetIterator disi = filt.getDocIdSet(reader.getTopReaderContext(), reader.getLiveDocs()).iterator();
     int actual = 0;
     while (disi.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {
       actual++;
Index: modules/queries/src/test/org/apache/lucene/queries/function/TestOrdValues.java
===================================================================
--- modules/queries/src/test/org/apache/lucene/queries/function/TestOrdValues.java	(revision 1238034)
+++ modules/queries/src/test/org/apache/lucene/queries/function/TestOrdValues.java	(working copy)
@@ -19,11 +19,9 @@
 
 import org.apache.lucene.index.CorruptIndexException;
 import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
 import org.apache.lucene.queries.function.valuesource.OrdFieldSource;
 import org.apache.lucene.queries.function.valuesource.ReverseOrdFieldSource;
 import org.apache.lucene.search.*;
-import org.apache.lucene.util.ReaderUtil;
 import org.junit.BeforeClass;
 import org.junit.Test;
 
Index: modules/queries/src/test/org/apache/lucene/queries/TermsFilterTest.java
===================================================================
--- modules/queries/src/test/org/apache/lucene/queries/TermsFilterTest.java	(revision 1238034)
+++ modules/queries/src/test/org/apache/lucene/queries/TermsFilterTest.java	(working copy)
@@ -21,11 +21,11 @@
 
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.StringField;
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
 import org.apache.lucene.index.MultiReader;
 import org.apache.lucene.index.RandomIndexWriter;
-import org.apache.lucene.index.SlowMultiReaderWrapper;
+import org.apache.lucene.index.SlowCompositeReaderWrapper;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.search.Filter;
 import org.apache.lucene.store.Directory;
@@ -62,27 +62,26 @@
       doc.add(newField(fieldName, "" + term, StringField.TYPE_STORED));
       w.addDocument(doc);
     }
-    IndexReader reader = new SlowMultiReaderWrapper(w.getReader());
-    assertTrue(reader.getTopReaderContext().isAtomic);
+    IndexReader reader = new SlowCompositeReaderWrapper(w.getReader());
+    assertTrue(reader.getTopReaderContext() instanceof AtomicReaderContext);
     AtomicReaderContext context = (AtomicReaderContext) reader.getTopReaderContext();
-    assertTrue(context.isAtomic);
     w.close();
 
     TermsFilter tf = new TermsFilter();
     tf.addTerm(new Term(fieldName, "19"));
-    FixedBitSet bits = (FixedBitSet) tf.getDocIdSet(context, context.reader.getLiveDocs());
+    FixedBitSet bits = (FixedBitSet) tf.getDocIdSet(context, context.reader().getLiveDocs());
     assertEquals("Must match nothing", 0, bits.cardinality());
 
     tf.addTerm(new Term(fieldName, "20"));
-    bits = (FixedBitSet) tf.getDocIdSet(context, context.reader.getLiveDocs());
+    bits = (FixedBitSet) tf.getDocIdSet(context, context.reader().getLiveDocs());
     assertEquals("Must match 1", 1, bits.cardinality());
 
     tf.addTerm(new Term(fieldName, "10"));
-    bits = (FixedBitSet) tf.getDocIdSet(context, context.reader.getLiveDocs());
+    bits = (FixedBitSet) tf.getDocIdSet(context, context.reader().getLiveDocs());
     assertEquals("Must match 2", 2, bits.cardinality());
 
     tf.addTerm(new Term(fieldName, "00"));
-    bits = (FixedBitSet) tf.getDocIdSet(context, context.reader.getLiveDocs());
+    bits = (FixedBitSet) tf.getDocIdSet(context, context.reader().getLiveDocs());
     assertEquals("Must match 2", 2, bits.cardinality());
 
     reader.close();
@@ -112,8 +111,8 @@
     tf.addTerm(new Term(fieldName, "content1"));
     
     MultiReader multi = new MultiReader(reader1, reader2);
-    for (IndexReader.AtomicReaderContext context : ReaderUtil.leaves(multi.getTopReaderContext())) {
-      FixedBitSet bits = (FixedBitSet) tf.getDocIdSet(context, context.reader.getLiveDocs());
+    for (AtomicReaderContext context : ReaderUtil.leaves(multi.getTopReaderContext())) {
+      FixedBitSet bits = (FixedBitSet) tf.getDocIdSet(context, context.reader().getLiveDocs());
       assertTrue("Must be >= 0", bits.cardinality() >= 0);      
     }
     multi.close();
Index: modules/queries/src/test/org/apache/lucene/queries/TestCustomScoreQuery.java
===================================================================
--- modules/queries/src/test/org/apache/lucene/queries/TestCustomScoreQuery.java	(revision 1238034)
+++ modules/queries/src/test/org/apache/lucene/queries/TestCustomScoreQuery.java	(working copy)
@@ -28,7 +28,7 @@
 import java.util.HashMap;
 import java.util.Map;
 
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.Term;
 
@@ -174,11 +174,11 @@
 
     @Override
     protected CustomScoreProvider getCustomScoreProvider(AtomicReaderContext context) throws IOException {
-      final int[] values = FieldCache.DEFAULT.getInts(context.reader, INT_FIELD, false);
+      final int[] values = FieldCache.DEFAULT.getInts(context.reader(), INT_FIELD, false);
       return new CustomScoreProvider(context) {
         @Override
         public float customScore(int doc, float subScore, float valSrcScore) throws IOException {
-          assertTrue(doc <= context.reader.maxDoc());
+          assertTrue(doc <= context.reader().maxDoc());
           return values[doc];
         }
       };
Index: modules/queryparser/src/java/org/apache/lucene/queryparser/xml/builders/NumericRangeFilterBuilder.java
===================================================================
--- modules/queryparser/src/java/org/apache/lucene/queryparser/xml/builders/NumericRangeFilterBuilder.java	(revision 1238034)
+++ modules/queryparser/src/java/org/apache/lucene/queryparser/xml/builders/NumericRangeFilterBuilder.java	(working copy)
@@ -17,7 +17,7 @@
  * limitations under the License.
  */
 
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.search.DocIdSet;
 import org.apache.lucene.search.Filter;
 import org.apache.lucene.search.NumericRangeFilter;
Index: modules/queryparser/src/test/org/apache/lucene/queryparser/surround/query/BooleanQueryTst.java
===================================================================
--- modules/queryparser/src/test/org/apache/lucene/queryparser/surround/query/BooleanQueryTst.java	(revision 1238034)
+++ modules/queryparser/src/test/org/apache/lucene/queryparser/surround/query/BooleanQueryTst.java	(working copy)
@@ -19,8 +19,8 @@
 
 import java.io.IOException;
 
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
 import org.apache.lucene.search.IndexSearcher;
 import org.apache.lucene.search.Collector;
 import org.apache.lucene.search.Scorer;
Index: modules/queryparser/src/test/org/apache/lucene/queryparser/xml/builders/TestNumericRangeFilterBuilder.java
===================================================================
--- modules/queryparser/src/test/org/apache/lucene/queryparser/xml/builders/TestNumericRangeFilterBuilder.java	(revision 1238034)
+++ modules/queryparser/src/test/org/apache/lucene/queryparser/xml/builders/TestNumericRangeFilterBuilder.java	(working copy)
@@ -17,10 +17,11 @@
  * limitations under the License.
  */
 
+import org.apache.lucene.index.AtomicReader;
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
 import org.apache.lucene.index.IndexWriter;
-import org.apache.lucene.index.SlowMultiReaderWrapper;
+import org.apache.lucene.index.SlowCompositeReaderWrapper;
 import org.apache.lucene.search.Filter;
 import org.apache.lucene.search.NumericRangeFilter;
 import org.apache.lucene.store.Directory;
@@ -63,7 +64,7 @@
     IndexWriter writer = new IndexWriter(ramDir, newIndexWriterConfig(TEST_VERSION_CURRENT, null));
     writer.commit();
     try {
-      IndexReader reader = new SlowMultiReaderWrapper(IndexReader.open(ramDir));
+      AtomicReader reader = new SlowCompositeReaderWrapper(IndexReader.open(ramDir));
       try {
         assertNull(filter.getDocIdSet((AtomicReaderContext) reader.getTopReaderContext(), reader.getLiveDocs()));
       }
Index: modules/queryparser/src/test/org/apache/lucene/queryparser/xml/builders/TestNumericRangeFilterBuilder.java
===================================================================
--- modules/queryparser/src/test/org/apache/lucene/queryparser/xml/builders/TestNumericRangeFilterBuilder.java	(revision 1238034)
+++ modules/queryparser/src/test/org/apache/lucene/queryparser/xml/builders/TestNumericRangeFilterBuilder.java	(working copy)

Property changes on: modules/queryparser/src/test/org/apache/lucene/queryparser/xml/builders/TestNumericRangeFilterBuilder.java
___________________________________________________________________
Modified: svn:mergeinfo
   Merged /lucene/dev/branches/lucene2858/modules/queryparser/src/test/org/apache/lucene/queryparser/xml/builders/TestNumericRangeFilterBuilder.java:r1234440-1238051
Index: modules/queryparser/src/test/org/apache/lucene/queryparser/xml/TestParser.java
===================================================================
--- modules/queryparser/src/test/org/apache/lucene/queryparser/xml/TestParser.java	(revision 1238034)
+++ modules/queryparser/src/test/org/apache/lucene/queryparser/xml/TestParser.java	(working copy)
@@ -24,6 +24,7 @@
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.NumericField;
 import org.apache.lucene.document.TextField;
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexWriter;
 import org.apache.lucene.search.IndexSearcher;
@@ -180,8 +181,8 @@
   }
 
   public void testDuplicateFilterQueryXML() throws ParserException, IOException {
-    Assume.assumeTrue(searcher.getIndexReader().getSequentialSubReaders() == null ||
-        searcher.getIndexReader().getSequentialSubReaders().length == 1);
+    AtomicReaderContext leaves[] = searcher.getTopReaderContext().leaves();
+    Assume.assumeTrue(leaves == null || leaves.length == 1);
     Query q = parse("DuplicateFilterQuery.xml");
     int h = searcher.search(q, null, 1000).totalHits;
     assertEquals("DuplicateFilterQuery should produce 1 result ", 1, h);
Index: modules/suggest/src/java/org/apache/lucene/search/spell/SpellChecker.java
===================================================================
--- modules/suggest/src/java/org/apache/lucene/search/spell/SpellChecker.java	(revision 1238034)
+++ modules/suggest/src/java/org/apache/lucene/search/spell/SpellChecker.java	(working copy)
@@ -28,6 +28,8 @@
 import org.apache.lucene.document.FieldType;
 import org.apache.lucene.index.FieldInfo.IndexOptions;
 import org.apache.lucene.document.StringField;
+import org.apache.lucene.index.AtomicReader;
+import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexWriter;
 import org.apache.lucene.index.IndexWriterConfig;
@@ -166,7 +168,7 @@
     // modifications to the directory should be synchronized 
     synchronized (modifyCurrentIndexLock) {
       ensureOpen();
-      if (!IndexReader.indexExists(spellIndexDir)) {
+      if (!DirectoryReader.indexExists(spellIndexDir)) {
           IndexWriter writer = new IndexWriter(spellIndexDir,
             new IndexWriterConfig(Version.LUCENE_CURRENT,
                 null));
@@ -497,7 +499,7 @@
       if (reader.maxDoc() > 0) {
         new ReaderUtil.Gather(reader) {
           @Override
-          protected void add(int base, IndexReader r) throws IOException {
+          protected void add(int base, AtomicReader r) throws IOException {
             Terms terms = r.terms(F_WORD);
             if (terms != null)
               termsEnums.add(terms.iterator(null));
Index: solr
===================================================================
--- solr	(revision 1238034)
+++ solr	(working copy)

Property changes on: solr
___________________________________________________________________
Modified: svn:mergeinfo
   Merged /lucene/dev/branches/lucene2858/solr:r1234440-1238051
Index: solr/build.xml
===================================================================
--- solr/build.xml	(revision 1238034)
+++ solr/build.xml	(working copy)

Property changes on: solr/build.xml
___________________________________________________________________
Modified: svn:mergeinfo
   Merged /lucene/dev/branches/lucene2858/solr/build.xml:r1234440-1238051
Index: solr/CHANGES.txt
===================================================================
--- solr/CHANGES.txt	(revision 1238034)
+++ solr/CHANGES.txt	(working copy)

Property changes on: solr/CHANGES.txt
___________________________________________________________________
Modified: svn:mergeinfo
   Merged /lucene/dev/branches/lucene2858/solr/CHANGES.txt:r1234440-1238051
Index: solr/client
===================================================================
--- solr/client	(revision 1238034)
+++ solr/client	(working copy)

Property changes on: solr/client
___________________________________________________________________
Modified: svn:mergeinfo
   Merged /lucene/dev/branches/lucene2858/solr/client:r1234440-1238051
Index: solr/common-build.xml
===================================================================
--- solr/common-build.xml	(revision 1238034)
+++ solr/common-build.xml	(working copy)

Property changes on: solr/common-build.xml
___________________________________________________________________
Modified: svn:mergeinfo
   Merged /lucene/dev/branches/lucene2858/solr/common-build.xml:r1234440-1238051
Index: solr/contrib
===================================================================
--- solr/contrib	(revision 1238034)
+++ solr/contrib	(working copy)

Property changes on: solr/contrib
___________________________________________________________________
Modified: svn:mergeinfo
   Merged /lucene/dev/branches/lucene2858/solr/contrib:r1234440-1238051
Index: solr/contrib/clustering/src/test-files
===================================================================
--- solr/contrib/clustering/src/test-files	(revision 1238034)
+++ solr/contrib/clustering/src/test-files	(working copy)

Property changes on: solr/contrib/clustering/src/test-files
___________________________________________________________________
Modified: svn:mergeinfo
   Merged /lucene/dev/branches/lucene2858/solr/contrib/clustering/src/test-files:r1234440-1238051
Index: solr/contrib/dataimporthandler-extras/src/java
===================================================================
--- solr/contrib/dataimporthandler-extras/src/java	(revision 1238034)
+++ solr/contrib/dataimporthandler-extras/src/java	(working copy)

Property changes on: solr/contrib/dataimporthandler-extras/src/java
___________________________________________________________________
Modified: svn:mergeinfo
   Merged /lucene/dev/branches/lucene2858/solr/contrib/dataimporthandler-extras/src/java:r1234440-1238051
Index: solr/contrib/dataimporthandler/src/java
===================================================================
--- solr/contrib/dataimporthandler/src/java	(revision 1238034)
+++ solr/contrib/dataimporthandler/src/java	(working copy)

Property changes on: solr/contrib/dataimporthandler/src/java
___________________________________________________________________
Modified: svn:mergeinfo
   Merged /lucene/dev/branches/lucene2858/solr/contrib/dataimporthandler/src/java:r1234440-1238051
Index: solr/contrib/dataimporthandler/src/test-files
===================================================================
--- solr/contrib/dataimporthandler/src/test-files	(revision 1238034)
+++ solr/contrib/dataimporthandler/src/test-files	(working copy)

Property changes on: solr/contrib/dataimporthandler/src/test-files
___________________________________________________________________
Modified: svn:mergeinfo
   Merged /lucene/dev/branches/lucene2858/solr/contrib/dataimporthandler/src/test-files:r1234440-1238051
Index: solr/contrib/dataimporthandler/src/test/org
===================================================================
--- solr/contrib/dataimporthandler/src/test/org	(revision 1238034)
+++ solr/contrib/dataimporthandler/src/test/org	(working copy)

Property changes on: solr/contrib/dataimporthandler/src/test/org
___________________________________________________________________
Modified: svn:mergeinfo
   Merged /lucene/dev/branches/lucene2858/solr/contrib/dataimporthandler/src/test/org:r1234440-1238051
Index: solr/contrib/uima/src/java
===================================================================
--- solr/contrib/uima/src/java	(revision 1238034)
+++ solr/contrib/uima/src/java	(working copy)

Property changes on: solr/contrib/uima/src/java
___________________________________________________________________
Modified: svn:mergeinfo
   Merged /lucene/dev/branches/lucene2858/solr/contrib/uima/src/java:r1234440-1238051
Index: solr/contrib/uima/src/test-files
===================================================================
--- solr/contrib/uima/src/test-files	(revision 1238034)
+++ solr/contrib/uima/src/test-files	(working copy)

Property changes on: solr/contrib/uima/src/test-files
___________________________________________________________________
Modified: svn:mergeinfo
   Merged /lucene/dev/branches/lucene2858/solr/contrib/uima/src/test-files:r1234440-1238051
Index: solr/core
===================================================================
--- solr/core	(revision 1238034)
+++ solr/core	(working copy)

Property changes on: solr/core
___________________________________________________________________
Modified: svn:mergeinfo
   Merged /lucene/dev/branches/lucene2858/solr/core:r1234440-1238051
Index: solr/core/src/java
===================================================================
--- solr/core/src/java	(revision 1238034)
+++ solr/core/src/java	(working copy)

Property changes on: solr/core/src/java
___________________________________________________________________
Modified: svn:mergeinfo
   Merged /lucene/dev/branches/lucene2858/solr/core/src/java:r1234440-1238051
Index: solr/core/src/java/org/apache/solr/core/IndexReaderFactory.java
===================================================================
--- solr/core/src/java/org/apache/solr/core/IndexReaderFactory.java	(revision 1238034)
+++ solr/core/src/java/org/apache/solr/core/IndexReaderFactory.java	(working copy)
@@ -18,6 +18,7 @@
 
 import java.io.IOException;
 
+import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.store.Directory;
 import org.apache.solr.common.util.NamedList;
@@ -60,6 +61,6 @@
    * @return An IndexReader instance
    * @throws IOException
    */
-  public abstract IndexReader newReader(Directory indexDir)
+  public abstract DirectoryReader newReader(Directory indexDir)
       throws IOException;
 }
Index: solr/core/src/java/org/apache/solr/core/SolrCore.java
===================================================================
--- solr/core/src/java/org/apache/solr/core/SolrCore.java	(revision 1238034)
+++ solr/core/src/java/org/apache/solr/core/SolrCore.java	(working copy)
@@ -19,7 +19,7 @@
 
 import org.apache.lucene.codecs.Codec;
 import org.apache.lucene.index.IndexDeletionPolicy;
-import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexWriter;
 import org.apache.lucene.search.BooleanQuery;
 import org.apache.lucene.store.Directory;
@@ -758,7 +758,7 @@
     }
 
     try {
-      updateHandler.close();
+      if (updateHandler != null) updateHandler.close();
     } catch (Throwable e) {
       SolrException.log(log,e);
     }
@@ -1082,17 +1082,17 @@
       if (newestSearcher != null && solrConfig.reopenReaders
           && (nrt || indexDirFile.equals(newIndexDirFile))) {
 
-        IndexReader newReader;
-        IndexReader currentReader = newestSearcher.get().getIndexReader();
+        DirectoryReader newReader;
+        DirectoryReader currentReader = newestSearcher.get().getIndexReader();
 
         if (updateHandlerReopens) {
           // SolrCore.verbose("start reopen from",previousSearcher,"writer=",writer);
           IndexWriter writer = getUpdateHandler().getSolrCoreState().getIndexWriter(this);
-          newReader = IndexReader.openIfChanged(currentReader, writer, true);
+          newReader = DirectoryReader.openIfChanged(currentReader, writer, true);
 
         } else {
           // verbose("start reopen without writer, reader=", currentReader);
-          newReader = IndexReader.openIfChanged(currentReader);
+          newReader = DirectoryReader.openIfChanged(currentReader);
           // verbose("reopen result", newReader);
         }
 
Index: solr/core/src/java/org/apache/solr/core/StandardIndexReaderFactory.java
===================================================================
--- solr/core/src/java/org/apache/solr/core/StandardIndexReaderFactory.java	(revision 1238034)
+++ solr/core/src/java/org/apache/solr/core/StandardIndexReaderFactory.java	(working copy)
@@ -18,19 +18,19 @@
 
 import java.io.IOException;
 
-import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.store.Directory;
 
 /**
  * Default IndexReaderFactory implementation. Returns a standard Lucene
- * IndexReader.
+ * {@link DirectoryReader}.
  * 
- * @see IndexReader#open(Directory)
+ * @see DirectoryReader#open(Directory)
  */
 public class StandardIndexReaderFactory extends IndexReaderFactory {
   
   @Override
-  public IndexReader newReader(Directory indexDir) throws IOException {
-    return IndexReader.open(indexDir, termInfosIndexDivisor);
+  public DirectoryReader newReader(Directory indexDir) throws IOException {
+    return DirectoryReader.open(indexDir, termInfosIndexDivisor);
   }
 }
Index: solr/core/src/java/org/apache/solr/handler/admin/CoreAdminHandler.java
===================================================================
--- solr/core/src/java/org/apache/solr/handler/admin/CoreAdminHandler.java	(revision 1238034)
+++ solr/core/src/java/org/apache/solr/handler/admin/CoreAdminHandler.java	(working copy)
@@ -25,7 +25,7 @@
 import java.util.Properties;
 
 import org.apache.commons.io.FileUtils;
-import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.search.MatchAllDocsQuery;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.util.IOUtils;
@@ -218,7 +218,7 @@
     SolrCore[] sourceCores = null;
     RefCounted<SolrIndexSearcher>[] searchers = null;
     // stores readers created from indexDir param values
-    IndexReader[] readersToBeClosed = null;
+    DirectoryReader[] readersToBeClosed = null;
     Directory[] dirsToBeReleased = null;
     if (core != null) {
       try {
@@ -239,22 +239,22 @@
             sourceCores[i] = srcCore;
           }
         } else  {
-          readersToBeClosed = new IndexReader[dirNames.length];
+          readersToBeClosed = new DirectoryReader[dirNames.length];
           dirsToBeReleased = new Directory[dirNames.length];
           DirectoryFactory dirFactory = core.getDirectoryFactory();
           for (int i = 0; i < dirNames.length; i++) {
             Directory dir = dirFactory.get(dirNames[i], core.getSolrConfig().mainIndexConfig.lockType);
             dirsToBeReleased[i] = dir;
             // TODO: why doesn't this use the IR factory? what is going on here?
-            readersToBeClosed[i] = IndexReader.open(dir);
+            readersToBeClosed[i] = DirectoryReader.open(dir);
           }
         }
 
-        IndexReader[] readers = null;
+        DirectoryReader[] readers = null;
         if (readersToBeClosed != null)  {
           readers = readersToBeClosed;
         } else {
-          readers = new IndexReader[sourceCores.length];
+          readers = new DirectoryReader[sourceCores.length];
           searchers = new RefCounted[sourceCores.length];
           for (int i = 0; i < sourceCores.length; i++) {
             SolrCore solrCore = sourceCores[i];
Index: solr/core/src/java/org/apache/solr/handler/admin/LukeRequestHandler.java
===================================================================
--- solr/core/src/java/org/apache/solr/handler/admin/LukeRequestHandler.java	(revision 1238034)
+++ solr/core/src/java/org/apache/solr/handler/admin/LukeRequestHandler.java	(working copy)
@@ -90,7 +90,7 @@
   {
     IndexSchema schema = req.getSchema();
     SolrIndexSearcher searcher = req.getSearcher();
-    IndexReader reader = searcher.getIndexReader();
+    DirectoryReader reader = searcher.getIndexReader();
     SolrParams params = req.getParams();
     int numTerms = params.getInt( NUMTERMS, DEFAULT_COUNT );
 
@@ -287,17 +287,17 @@
       final SolrIndexSearcher searcher, final Set<String> fields, final int numTerms, Map<String,TopTermQueue> ttinfo)
       throws Exception {
 
-    IndexReader reader = searcher.getIndexReader();
+    AtomicReader reader = searcher.getAtomicReader();
     IndexSchema schema = searcher.getSchema();
 
     Set<String> fieldNames = new TreeSet<String>();
-    for(FieldInfo fieldInfo : ReaderUtil.getMergedFieldInfos(reader)) {
+    for(FieldInfo fieldInfo : reader.getFieldInfos()) {
       fieldNames.add(fieldInfo.name);
     }
 
     // Walk the term enum and keep a priority queue for each map in our set
     SimpleOrderedMap<Object> finfo = new SimpleOrderedMap<Object>();
-    Fields theFields = MultiFields.getFields(reader);
+    Fields theFields = reader.fields();
 
     for (String fieldName : fieldNames) {
       if (fields != null && ! fields.contains(fieldName)) {
@@ -328,8 +328,7 @@
           Document doc = null;
           if (topTerms != null && topTerms.getTopTermInfo() != null) {
             Term term = topTerms.getTopTermInfo().term;
-            DocsEnum docsEnum = MultiFields.getTermDocsEnum(reader,
-                MultiFields.getLiveDocs(reader),
+            DocsEnum docsEnum = reader.termDocsEnum(reader.getLiveDocs(),
                 term.field(),
                 new BytesRef(term.text()),
                 false);
@@ -498,10 +497,10 @@
     v.add( f.getName() );
     typeusemap.put( ft.getTypeName(), v );
   }
-  public static SimpleOrderedMap<Object> getIndexInfo(IndexReader reader, boolean countTerms) throws IOException {
+  public static SimpleOrderedMap<Object> getIndexInfo(DirectoryReader reader, boolean countTerms) throws IOException {
     return getIndexInfo(reader, countTerms ? 1 : 0, null, null);
   }
-  public static SimpleOrderedMap<Object> getIndexInfo( IndexReader reader, int numTerms,
+  public static SimpleOrderedMap<Object> getIndexInfo( DirectoryReader reader, int numTerms,
                                                        Map<String, TopTermQueue> topTerms,
                                                        Set<String> fieldList) throws IOException {
     Directory dir = reader.directory();
Index: solr/core/src/java/org/apache/solr/handler/component/QueryComponent.java
===================================================================
--- solr/core/src/java/org/apache/solr/handler/component/QueryComponent.java	(revision 1238034)
+++ solr/core/src/java/org/apache/solr/handler/component/QueryComponent.java	(working copy)
@@ -19,8 +19,8 @@
 
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.StringField;
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
-import org.apache.lucene.index.IndexReader.ReaderContext;
+import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.IndexReaderContext;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.queryparser.classic.ParseException;
 import org.apache.lucene.search.*;
@@ -582,7 +582,7 @@
       SortField[] sortFields = sort==null ? new SortField[]{SortField.FIELD_SCORE} : sort.getSort();
       NamedList<List> sortVals = new NamedList<List>(); // order is important for the sort fields
       Field field = new StringField("dummy", ""); // a dummy Field
-      ReaderContext topReaderContext = searcher.getTopReaderContext();
+      IndexReaderContext topReaderContext = searcher.getTopReaderContext();
       AtomicReaderContext[] leaves = ReaderUtil.leaves(topReaderContext);
       AtomicReaderContext currentLeaf = null;
       if (leaves.length==1) {
Index: solr/core/src/java/org/apache/solr/handler/component/QueryElevationComponent.java
===================================================================
--- solr/core/src/java/org/apache/solr/handler/component/QueryElevationComponent.java	(revision 1238034)
+++ solr/core/src/java/org/apache/solr/handler/component/QueryElevationComponent.java	(working copy)
@@ -21,7 +21,7 @@
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
 import org.apache.lucene.index.*;
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.search.*;
 import org.apache.lucene.util.Bits;
 import org.apache.lucene.util.BytesRef;
@@ -535,13 +535,13 @@
       public FieldComparator setNextReader(AtomicReaderContext context) throws IOException {
         //convert the ids to Lucene doc ids, the ordSet and termValues needs to be the same size as the number of elevation docs we have
         ordSet.clear();
-        Fields fields = context.reader.fields();
+        Fields fields = context.reader().fields();
         if (fields == null) return this;
         Terms terms = fields.terms(fieldname);
         if (terms == null) return this;
         termsEnum = terms.iterator(termsEnum);
         BytesRef term = new BytesRef();
-        Bits liveDocs = context.reader.getLiveDocs();
+        Bits liveDocs = context.reader().getLiveDocs();
 
         for (String id : elevations.ids) {
           term.copyChars(id);
Index: solr/core/src/java/org/apache/solr/handler/component/StatsComponent.java
===================================================================
--- solr/core/src/java/org/apache/solr/handler/component/StatsComponent.java	(revision 1238034)
+++ solr/core/src/java/org/apache/solr/handler/component/StatsComponent.java	(working copy)
@@ -23,7 +23,6 @@
 import java.util.List;
 import java.util.Map;
 
-import org.apache.lucene.index.SlowMultiReaderWrapper;
 import org.apache.lucene.search.FieldCache;
 import org.apache.lucene.util.BytesRef;
 import org.apache.solr.common.SolrException;
@@ -253,7 +252,7 @@
     
     FieldCache.DocTermsIndex si;
     try {
-      si = FieldCache.DEFAULT.getTermsIndex(new SlowMultiReaderWrapper(searcher.getIndexReader()), fieldName);
+      si = FieldCache.DEFAULT.getTermsIndex(searcher.getAtomicReader(), fieldName);
     } 
     catch (IOException e) {
       throw new RuntimeException( "failed to open field cache for: "+fieldName, e );
@@ -275,7 +274,7 @@
           + "[" + facetFieldType + "]");
         }
       try {
-        facetTermsIndex = FieldCache.DEFAULT.getTermsIndex(new SlowMultiReaderWrapper(searcher.getIndexReader()), facetField);
+        facetTermsIndex = FieldCache.DEFAULT.getTermsIndex(searcher.getAtomicReader(), facetField);
       }
       catch (IOException e) {
         throw new RuntimeException( "failed to open field cache for: "
Index: solr/core/src/java/org/apache/solr/handler/component/TermsComponent.java
===================================================================
--- solr/core/src/java/org/apache/solr/handler/component/TermsComponent.java	(revision 1238034)
+++ solr/core/src/java/org/apache/solr/handler/component/TermsComponent.java	(working copy)
@@ -117,8 +117,8 @@
     boolean raw = params.getBool(TermsParams.TERMS_RAW, false);
 
 
-    final IndexReader indexReader = rb.req.getSearcher().getTopReaderContext().reader;
-    Fields lfields = MultiFields.getFields(indexReader);
+    final AtomicReader indexReader = rb.req.getSearcher().getAtomicReader();
+    Fields lfields = indexReader.fields();
 
     for (String field : fields) {
       NamedList<Integer> fieldTerms = new NamedList<Integer>();
Index: solr/core/src/java/org/apache/solr/handler/component/TermVectorComponent.java
===================================================================
--- solr/core/src/java/org/apache/solr/handler/component/TermVectorComponent.java	(revision 1238034)
+++ solr/core/src/java/org/apache/solr/handler/component/TermVectorComponent.java	(working copy)
@@ -13,8 +13,6 @@
 import org.apache.lucene.index.Fields;
 import org.apache.lucene.index.FieldsEnum;
 import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.MultiFields;
-import org.apache.lucene.index.StoredFieldVisitor.Status;
 import org.apache.lucene.index.StoredFieldVisitor;
 import org.apache.lucene.index.Terms;
 import org.apache.lucene.index.TermsEnum;
@@ -390,13 +388,7 @@
   private static int getDocFreq(IndexReader reader, String field, BytesRef term) {
     int result = 1;
     try {
-      Terms terms = MultiFields.getTerms(reader, field);
-      if (terms != null) {
-        TermsEnum termsEnum = terms.iterator(null);
-        if (termsEnum.seekExact(term, true)) {
-          result = termsEnum.docFreq();
-        }
-      }
+      result = reader.docFreq(field, term);
     } catch (IOException e) {
       throw new RuntimeException(e);
     }
Index: solr/core/src/java/org/apache/solr/handler/ReplicationHandler.java
===================================================================
--- solr/core/src/java/org/apache/solr/handler/ReplicationHandler.java	(revision 1238034)
+++ solr/core/src/java/org/apache/solr/handler/ReplicationHandler.java	(working copy)
@@ -43,6 +43,7 @@
 import org.apache.lucene.index.IndexCommit;
 import org.apache.lucene.index.IndexDeletionPolicy;
 import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.DirectoryReader;
 import org.apache.solr.common.SolrException;
 import org.apache.solr.common.params.CommonParams;
 import org.apache.solr.common.params.ModifiableSolrParams;
@@ -844,11 +845,11 @@
         replicateOnStart = true;
         RefCounted<SolrIndexSearcher> s = core.getNewestSearcher(false);
         try {
-          IndexReader reader = s==null ? null : s.get().getIndexReader();
+          DirectoryReader reader = s==null ? null : s.get().getIndexReader();
           if (reader!=null && reader.getIndexCommit() != null && reader.getIndexCommit().getGeneration() != 1L) {
             try {
               if(replicateOnOptimize){
-                Collection<IndexCommit> commits = IndexReader.listCommits(reader.directory());
+                Collection<IndexCommit> commits = DirectoryReader.listCommits(reader.directory());
                 for (IndexCommit ic : commits) {
                   if(ic.getSegmentCount() == 1){
                     if(indexCommitPoint == null || indexCommitPoint.getGeneration() < ic.getGeneration()) indexCommitPoint = ic;
Index: solr/core/src/java/org/apache/solr/request/PerSegmentSingleValuedFaceting.java
===================================================================
--- solr/core/src/java/org/apache/solr/request/PerSegmentSingleValuedFaceting.java	(revision 1238034)
+++ solr/core/src/java/org/apache/solr/request/PerSegmentSingleValuedFaceting.java	(working copy)
@@ -17,7 +17,7 @@
 
 package org.apache.solr.request;
 
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.TermsEnum;
 import org.apache.lucene.search.DocIdSet;
 import org.apache.lucene.search.DocIdSetIterator;
@@ -235,7 +235,7 @@
     BytesRef tempBR = new BytesRef();
 
     void countTerms() throws IOException {
-      si = FieldCache.DEFAULT.getTermsIndex(context.reader, fieldName);
+      si = FieldCache.DEFAULT.getTermsIndex(context.reader(), fieldName);
       // SolrCore.log.info("reader= " + reader + "  FC=" + System.identityHashCode(si));
 
       if (prefix!=null) {
Index: solr/core/src/java/org/apache/solr/request/SimpleFacets.java
===================================================================
--- solr/core/src/java/org/apache/solr/request/SimpleFacets.java	(revision 1238034)
+++ solr/core/src/java/org/apache/solr/request/SimpleFacets.java	(working copy)
@@ -408,7 +408,7 @@
     FieldType ft = searcher.getSchema().getFieldType(fieldName);
     NamedList<Integer> res = new NamedList<Integer>();
 
-    FieldCache.DocTermsIndex si = FieldCache.DEFAULT.getTermsIndex(new SlowMultiReaderWrapper(searcher.getIndexReader()), fieldName);
+    FieldCache.DocTermsIndex si = FieldCache.DEFAULT.getTermsIndex(searcher.getAtomicReader(), fieldName);
 
     final BytesRef prefixRef;
     if (prefix == null) {
@@ -609,7 +609,7 @@
 
 
     IndexSchema schema = searcher.getSchema();
-    IndexReader r = searcher.getIndexReader();
+    AtomicReader r = searcher.getAtomicReader();
     FieldType ft = schema.getFieldType(field);
 
     boolean sortByCount = sort.equals("count") || sort.equals("true");
@@ -627,7 +627,7 @@
       startTermBytes = new BytesRef(indexedPrefix);
     }
 
-    Fields fields = MultiFields.getFields(r);
+    Fields fields = r.fields();
     Terms terms = fields==null ? null : fields.terms(field);
     TermsEnum termsEnum = null;
     SolrIndexSearcher.DocsEnumState deState = null;
@@ -673,7 +673,7 @@
             if (deState==null) {
               deState = new SolrIndexSearcher.DocsEnumState();
               deState.fieldName = field;
-              deState.liveDocs = MultiFields.getLiveDocs(r);
+              deState.liveDocs = r.getLiveDocs();
               deState.termsEnum = termsEnum;
               deState.docsEnum = docsEnum;
             }
Index: solr/core/src/java/org/apache/solr/request/UnInvertedField.java
===================================================================
--- solr/core/src/java/org/apache/solr/request/UnInvertedField.java	(revision 1238034)
+++ solr/core/src/java/org/apache/solr/request/UnInvertedField.java	(working copy)
@@ -19,7 +19,6 @@
 
 import org.apache.lucene.search.FieldCache;
 import org.apache.lucene.index.DocTermOrds;
-import org.apache.lucene.index.SlowMultiReaderWrapper;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.index.TermsEnum;
 import org.apache.lucene.search.TermQuery;
@@ -175,7 +174,7 @@
     final String prefix = TrieField.getMainValuePrefix(searcher.getSchema().getFieldType(field));
     this.searcher = searcher;
     try {
-      uninvert(new SlowMultiReaderWrapper(searcher.getIndexReader()), prefix == null ? null : new BytesRef(prefix));
+      uninvert(searcher.getAtomicReader(), prefix == null ? null : new BytesRef(prefix));
     } catch (IllegalStateException ise) {
       throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, ise.getMessage());
     }
@@ -227,7 +226,7 @@
       int startTerm = 0;
       int endTerm = numTermsInField;  // one past the end
 
-      TermsEnum te = getOrdTermsEnum(new SlowMultiReaderWrapper(searcher.getIndexReader()));
+      TermsEnum te = getOrdTermsEnum(searcher.getAtomicReader());
       if (prefix != null && prefix.length() > 0) {
         final BytesRef prefixBr = new BytesRef(prefix);
         if (te.seekCeil(prefixBr, true) == TermsEnum.SeekStatus.END) {
@@ -485,7 +484,7 @@
     for (String f : facet) {
       SchemaField facet_sf = searcher.getSchema().getField(f);
       try {
-        si = FieldCache.DEFAULT.getTermsIndex(new SlowMultiReaderWrapper(searcher.getIndexReader()), f);
+        si = FieldCache.DEFAULT.getTermsIndex(searcher.getAtomicReader(), f);
       }
       catch (IOException e) {
         throw new RuntimeException("failed to open field cache for: " + f, e);
@@ -497,7 +496,7 @@
     final int[] index = this.index;
     final int[] counts = new int[numTermsInField];//keep track of the number of times we see each word in the field for all the documents in the docset
 
-    TermsEnum te = getOrdTermsEnum(new SlowMultiReaderWrapper(searcher.getIndexReader()));
+    TermsEnum te = getOrdTermsEnum(searcher.getAtomicReader());
 
     boolean doNegative = false;
     if (finfo.length == 0) {
Index: solr/core/src/java/org/apache/solr/response/transform/ValueSourceAugmenter.java
===================================================================
--- solr/core/src/java/org/apache/solr/response/transform/ValueSourceAugmenter.java	(revision 1238034)
+++ solr/core/src/java/org/apache/solr/response/transform/ValueSourceAugmenter.java	(working copy)
@@ -19,6 +19,7 @@
 import java.io.IOException;
 import java.util.Map;
 
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
@@ -76,7 +77,7 @@
 
   Map fcontext;
   SolrIndexSearcher searcher;
-  IndexReader.AtomicReaderContext[] readerContexts;
+  AtomicReaderContext[] readerContexts;
   FunctionValues docValuesArr[];
 
 
@@ -88,7 +89,7 @@
 
       // TODO: calculate this stuff just once across diff functions
       int idx = ReaderUtil.subIndex(docid, readerContexts);
-      IndexReader.AtomicReaderContext rcontext = readerContexts[idx];
+      AtomicReaderContext rcontext = readerContexts[idx];
       FunctionValues values = docValuesArr[idx];
       if (values == null) {
         docValuesArr[idx] = values = valueSource.getValues(fcontext, rcontext);
Index: solr/core/src/java/org/apache/solr/schema/BoolField.java
===================================================================
--- solr/core/src/java/org/apache/solr/schema/BoolField.java	(revision 1238034)
+++ solr/core/src/java/org/apache/solr/schema/BoolField.java	(working copy)
@@ -18,6 +18,7 @@
 package org.apache.solr.schema;
 
 import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.IndexableField;
 import org.apache.lucene.search.FieldCache;
 import org.apache.lucene.search.SortField;
@@ -170,8 +171,8 @@
 
 
   @Override
-  public FunctionValues getValues(Map context, IndexReader.AtomicReaderContext readerContext) throws IOException {
-    final FieldCache.DocTermsIndex sindex = FieldCache.DEFAULT.getTermsIndex(readerContext.reader, field);
+  public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
+    final FieldCache.DocTermsIndex sindex = FieldCache.DEFAULT.getTermsIndex(readerContext.reader(), field);
 
     // figure out what ord maps to true
     int nord = sindex.numOrd();
Index: solr/core/src/java/org/apache/solr/schema/DateField.java
===================================================================
--- solr/core/src/java/org/apache/solr/schema/DateField.java	(revision 1238034)
+++ solr/core/src/java/org/apache/solr/schema/DateField.java	(working copy)
@@ -17,7 +17,7 @@
 
 package org.apache.solr.schema;
 
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.IndexableField;
 import org.apache.lucene.search.Query;
 import org.apache.lucene.search.SortField;
Index: solr/core/src/java/org/apache/solr/schema/LatLonType.java
===================================================================
--- solr/core/src/java/org/apache/solr/schema/LatLonType.java	(revision 1238034)
+++ solr/core/src/java/org/apache/solr/schema/LatLonType.java	(working copy)
@@ -19,7 +19,7 @@
 import org.apache.lucene.document.FieldType;
 import org.apache.lucene.index.IndexableField;
 import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.queries.function.valuesource.VectorValueSource;
@@ -373,7 +373,7 @@
 
     @Override
     public Explanation explain(AtomicReaderContext context, int doc) throws IOException {
-      return ((SpatialScorer)scorer(context, true, true, context.reader.getLiveDocs())).explain(doc);
+      return ((SpatialScorer)scorer(context, true, true, context.reader().getLiveDocs())).explain(doc);
     }
   }
 
@@ -405,7 +405,7 @@
       super(w);
       this.weight = w;
       this.qWeight = qWeight;
-      this.reader = readerContext.reader;
+      this.reader = readerContext.reader();
       this.maxDoc = reader.maxDoc();
       this.liveDocs = acceptDocs;
       latVals = latSource.getValues(weight.latContext, readerContext);
Index: solr/core/src/java/org/apache/solr/schema/RandomSortField.java
===================================================================
--- solr/core/src/java/org/apache/solr/schema/RandomSortField.java	(revision 1238034)
+++ solr/core/src/java/org/apache/solr/schema/RandomSortField.java	(working copy)
@@ -21,8 +21,8 @@
 import java.util.Map;
 
 import org.apache.lucene.index.IndexableField;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
+import org.apache.lucene.index.DirectoryReader;
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.queries.function.docvalues.IntDocValues;
@@ -80,7 +80,7 @@
    * Using dynamic fields, you can force the random order to change 
    */
   private static int getSeed(String fieldName, AtomicReaderContext context) {
-    final IndexReader top = ReaderUtil.getTopLevelContext(context).reader;
+    final DirectoryReader top = (DirectoryReader) ReaderUtil.getTopLevelContext(context).reader();
     // calling getVersion() on a segment will currently give you a null pointer exception, so
     // we use the top-level reader.
     return fieldName.hashCode() + context.docBase + (int)top.getVersion();
Index: solr/core/src/java/org/apache/solr/schema/SortableDoubleField.java
===================================================================
--- solr/core/src/java/org/apache/solr/schema/SortableDoubleField.java	(revision 1238034)
+++ solr/core/src/java/org/apache/solr/schema/SortableDoubleField.java	(working copy)
@@ -28,7 +28,7 @@
 import org.apache.lucene.util.mutable.MutableValue;
 import org.apache.lucene.util.mutable.MutableValueDouble;
 import org.apache.solr.search.QParser;
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.IndexableField;
 import org.apache.solr.util.NumberUtils;
 import org.apache.solr.response.TextResponseWriter;
Index: solr/core/src/java/org/apache/solr/schema/SortableFloatField.java
===================================================================
--- solr/core/src/java/org/apache/solr/schema/SortableFloatField.java	(revision 1238034)
+++ solr/core/src/java/org/apache/solr/schema/SortableFloatField.java	(working copy)
@@ -28,7 +28,7 @@
 import org.apache.lucene.util.mutable.MutableValue;
 import org.apache.lucene.util.mutable.MutableValueFloat;
 import org.apache.solr.search.QParser;
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.IndexableField;
 import org.apache.solr.util.NumberUtils;
 import org.apache.solr.response.TextResponseWriter;
Index: solr/core/src/java/org/apache/solr/schema/SortableIntField.java
===================================================================
--- solr/core/src/java/org/apache/solr/schema/SortableIntField.java	(revision 1238034)
+++ solr/core/src/java/org/apache/solr/schema/SortableIntField.java	(working copy)
@@ -28,7 +28,7 @@
 import org.apache.lucene.util.mutable.MutableValue;
 import org.apache.lucene.util.mutable.MutableValueInt;
 import org.apache.solr.search.QParser;
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.IndexableField;
 import org.apache.solr.util.NumberUtils;
 import org.apache.solr.response.TextResponseWriter;
Index: solr/core/src/java/org/apache/solr/schema/SortableLongField.java
===================================================================
--- solr/core/src/java/org/apache/solr/schema/SortableLongField.java	(revision 1238034)
+++ solr/core/src/java/org/apache/solr/schema/SortableLongField.java	(working copy)
@@ -28,7 +28,7 @@
 import org.apache.lucene.util.mutable.MutableValue;
 import org.apache.lucene.util.mutable.MutableValueLong;
 import org.apache.solr.search.QParser;
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.IndexableField;
 import org.apache.solr.util.NumberUtils;
 import org.apache.solr.response.TextResponseWriter;
Index: solr/core/src/java/org/apache/solr/schema/StrFieldSource.java
===================================================================
--- solr/core/src/java/org/apache/solr/schema/StrFieldSource.java	(revision 1238034)
+++ solr/core/src/java/org/apache/solr/schema/StrFieldSource.java	(working copy)
@@ -17,7 +17,7 @@
 
 package org.apache.solr.schema;
 
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.docvalues.StringIndexDocValues;
 import org.apache.lucene.queries.function.valuesource.FieldCacheSource;
Index: solr/core/src/java/org/apache/solr/search/BitDocSet.java
===================================================================
--- solr/core/src/java/org/apache/solr/search/BitDocSet.java	(revision 1238034)
+++ solr/core/src/java/org/apache/solr/search/BitDocSet.java	(working copy)
@@ -18,6 +18,7 @@
 package org.apache.solr.search;
 
 import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.search.BitsFilteredDocIdSet;
 import org.apache.lucene.search.DocIdSet;
 import org.apache.lucene.search.Filter;
@@ -247,8 +248,8 @@
 
     return new Filter() {
       @Override
-      public DocIdSet getDocIdSet(final IndexReader.AtomicReaderContext context, final Bits acceptDocs) throws IOException {
-        IndexReader reader = context.reader;
+      public DocIdSet getDocIdSet(final AtomicReaderContext context, final Bits acceptDocs) throws IOException {
+        IndexReader reader = context.reader();
 
         if (context.isTopLevel) {
           return BitsFilteredDocIdSet.wrap(bs, acceptDocs);
Index: solr/core/src/java/org/apache/solr/search/DelegatingCollector.java
===================================================================
--- solr/core/src/java/org/apache/solr/search/DelegatingCollector.java	(revision 1238034)
+++ solr/core/src/java/org/apache/solr/search/DelegatingCollector.java	(working copy)
@@ -19,6 +19,7 @@
 
 
 import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.search.Collector;
 import org.apache.lucene.search.Scorer;
 
@@ -31,7 +32,7 @@
 
   protected Collector delegate;
   protected Scorer scorer;
-  protected IndexReader.AtomicReaderContext context;
+  protected AtomicReaderContext context;
   protected int docBase;
 
   public Collector getDelegate() {
@@ -62,7 +63,7 @@
   }
 
   @Override
-  public void setNextReader(IndexReader.AtomicReaderContext context) throws IOException {
+  public void setNextReader(AtomicReaderContext context) throws IOException {
     this.context = context;
     this.docBase = context.docBase;
     delegate.setNextReader(context);
Index: solr/core/src/java/org/apache/solr/search/DocSet.java
===================================================================
--- solr/core/src/java/org/apache/solr/search/DocSet.java	(revision 1238034)
+++ solr/core/src/java/org/apache/solr/search/DocSet.java	(working copy)
@@ -25,7 +25,7 @@
 import org.apache.lucene.search.DocIdSetIterator;
 import org.apache.lucene.search.BitsFilteredDocIdSet;
 import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
+import org.apache.lucene.index.AtomicReaderContext;
 
 import java.io.IOException;
 
@@ -273,7 +273,7 @@
     return new Filter() {
       @Override
       public DocIdSet getDocIdSet(final AtomicReaderContext context, final Bits acceptDocs) throws IOException {
-        IndexReader reader = context.reader;
+        IndexReader reader = context.reader();
 
         if (context.isTopLevel) {
           return BitsFilteredDocIdSet.wrap(bs, acceptDocs);
Index: solr/core/src/java/org/apache/solr/search/DocSetCollector.java
===================================================================
--- solr/core/src/java/org/apache/solr/search/DocSetCollector.java	(revision 1238034)
+++ solr/core/src/java/org/apache/solr/search/DocSetCollector.java	(working copy)
@@ -1,95 +1,96 @@
-package org.apache.solr.search;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.search.Collector;
-import org.apache.lucene.search.Scorer;
-import org.apache.lucene.util.OpenBitSet;
-
-import java.io.IOException;
-
-/**
- *
- */
-
-public class DocSetCollector extends Collector {
-  int pos=0;
-  OpenBitSet bits;
-  final int maxDoc;
-  final int smallSetSize;
-  int base;
-
-  // in case there aren't that many hits, we may not want a very sparse
-  // bit array.  Optimistically collect the first few docs in an array
-  // in case there are only a few.
-  final int[] scratch;
-
-  public DocSetCollector(int smallSetSize, int maxDoc) {
-    this.smallSetSize = smallSetSize;
-    this.maxDoc = maxDoc;
-    this.scratch = new int[smallSetSize];
-  }
-
-  @Override
-  public void collect(int doc) throws IOException {
-    doc += base;
-    // optimistically collect the first docs in an array
-    // in case the total number will be small enough to represent
-    // as a small set like SortedIntDocSet instead...
-    // Storing in this array will be quicker to convert
-    // than scanning through a potentially huge bit vector.
-    // FUTURE: when search methods all start returning docs in order, maybe
-    // we could have a ListDocSet() and use the collected array directly.
-    if (pos < scratch.length) {
-      scratch[pos]=doc;
-    } else {
-      // this conditional could be removed if BitSet was preallocated, but that
-      // would take up more memory, and add more GC time...
-      if (bits==null) bits = new OpenBitSet(maxDoc);
-      bits.fastSet(doc);
-    }
-
-    pos++;
-  }
-
-  public DocSet getDocSet() {
-    if (pos<=scratch.length) {
-      // assumes docs were collected in sorted order!
-      return new SortedIntDocSet(scratch, pos);
-    } else {
-      // set the bits for ids that were collected in the array
-      for (int i=0; i<scratch.length; i++) bits.fastSet(scratch[i]);
-      return new BitDocSet(bits,pos);
-    }
-  }
-
-  @Override
-  public void setScorer(Scorer scorer) throws IOException {
-  }
-
-  @Override
-  public void setNextReader(IndexReader.AtomicReaderContext context) throws IOException {
-    this.base = context.docBase;
-  }
-
-  @Override
-  public boolean acceptsDocsOutOfOrder() {
-    return false;
-  }
-}
+package org.apache.solr.search;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.search.Collector;
+import org.apache.lucene.search.Scorer;
+import org.apache.lucene.util.OpenBitSet;
+
+import java.io.IOException;
+
+/**
+ *
+ */
+
+public class DocSetCollector extends Collector {
+  int pos=0;
+  OpenBitSet bits;
+  final int maxDoc;
+  final int smallSetSize;
+  int base;
+
+  // in case there aren't that many hits, we may not want a very sparse
+  // bit array.  Optimistically collect the first few docs in an array
+  // in case there are only a few.
+  final int[] scratch;
+
+  public DocSetCollector(int smallSetSize, int maxDoc) {
+    this.smallSetSize = smallSetSize;
+    this.maxDoc = maxDoc;
+    this.scratch = new int[smallSetSize];
+  }
+
+  @Override
+  public void collect(int doc) throws IOException {
+    doc += base;
+    // optimistically collect the first docs in an array
+    // in case the total number will be small enough to represent
+    // as a small set like SortedIntDocSet instead...
+    // Storing in this array will be quicker to convert
+    // than scanning through a potentially huge bit vector.
+    // FUTURE: when search methods all start returning docs in order, maybe
+    // we could have a ListDocSet() and use the collected array directly.
+    if (pos < scratch.length) {
+      scratch[pos]=doc;
+    } else {
+      // this conditional could be removed if BitSet was preallocated, but that
+      // would take up more memory, and add more GC time...
+      if (bits==null) bits = new OpenBitSet(maxDoc);
+      bits.fastSet(doc);
+    }
+
+    pos++;
+  }
+
+  public DocSet getDocSet() {
+    if (pos<=scratch.length) {
+      // assumes docs were collected in sorted order!
+      return new SortedIntDocSet(scratch, pos);
+    } else {
+      // set the bits for ids that were collected in the array
+      for (int i=0; i<scratch.length; i++) bits.fastSet(scratch[i]);
+      return new BitDocSet(bits,pos);
+    }
+  }
+
+  @Override
+  public void setScorer(Scorer scorer) throws IOException {
+  }
+
+  @Override
+  public void setNextReader(AtomicReaderContext context) throws IOException {
+    this.base = context.docBase;
+  }
+
+  @Override
+  public boolean acceptsDocsOutOfOrder() {
+    return false;
+  }
+}
Index: solr/core/src/java/org/apache/solr/search/DocSetDelegateCollector.java
===================================================================
--- solr/core/src/java/org/apache/solr/search/DocSetDelegateCollector.java	(revision 1238034)
+++ solr/core/src/java/org/apache/solr/search/DocSetDelegateCollector.java	(working copy)
@@ -1,67 +1,68 @@
-package org.apache.solr.search;
-
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.search.Collector;
-import org.apache.lucene.search.Scorer;
-import org.apache.lucene.util.OpenBitSet;
-
-import java.io.IOException;
-
-/**
- *
- */
-public class DocSetDelegateCollector extends DocSetCollector {
-  final Collector collector;
-
-  public DocSetDelegateCollector(int smallSetSize, int maxDoc, Collector collector) {
-    super(smallSetSize, maxDoc);
-    this.collector = collector;
-  }
-
-  @Override
-  public void collect(int doc) throws IOException {
-    collector.collect(doc);
-
-    doc += base;
-    // optimistically collect the first docs in an array
-    // in case the total number will be small enough to represent
-    // as a small set like SortedIntDocSet instead...
-    // Storing in this array will be quicker to convert
-    // than scanning through a potentially huge bit vector.
-    // FUTURE: when search methods all start returning docs in order, maybe
-    // we could have a ListDocSet() and use the collected array directly.
-    if (pos < scratch.length) {
-      scratch[pos]=doc;
-    } else {
-      // this conditional could be removed if BitSet was preallocated, but that
-      // would take up more memory, and add more GC time...
-      if (bits==null) bits = new OpenBitSet(maxDoc);
-      bits.fastSet(doc);
-    }
-
-    pos++;
-  }
-
-  @Override
-  public DocSet getDocSet() {
-    if (pos<=scratch.length) {
-      // assumes docs were collected in sorted order!
-      return new SortedIntDocSet(scratch, pos);
-    } else {
-      // set the bits for ids that were collected in the array
-      for (int i=0; i<scratch.length; i++) bits.fastSet(scratch[i]);
-      return new BitDocSet(bits,pos);
-    }
-  }
-
-  @Override
-  public void setScorer(Scorer scorer) throws IOException {
-    collector.setScorer(scorer);
-  }
-
-  @Override
-  public void setNextReader(IndexReader.AtomicReaderContext context) throws IOException {
-    collector.setNextReader(context);
-    this.base = context.docBase;
-  }
-}
+package org.apache.solr.search;
+
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.search.Collector;
+import org.apache.lucene.search.Scorer;
+import org.apache.lucene.util.OpenBitSet;
+
+import java.io.IOException;
+
+/**
+ *
+ */
+public class DocSetDelegateCollector extends DocSetCollector {
+  final Collector collector;
+
+  public DocSetDelegateCollector(int smallSetSize, int maxDoc, Collector collector) {
+    super(smallSetSize, maxDoc);
+    this.collector = collector;
+  }
+
+  @Override
+  public void collect(int doc) throws IOException {
+    collector.collect(doc);
+
+    doc += base;
+    // optimistically collect the first docs in an array
+    // in case the total number will be small enough to represent
+    // as a small set like SortedIntDocSet instead...
+    // Storing in this array will be quicker to convert
+    // than scanning through a potentially huge bit vector.
+    // FUTURE: when search methods all start returning docs in order, maybe
+    // we could have a ListDocSet() and use the collected array directly.
+    if (pos < scratch.length) {
+      scratch[pos]=doc;
+    } else {
+      // this conditional could be removed if BitSet was preallocated, but that
+      // would take up more memory, and add more GC time...
+      if (bits==null) bits = new OpenBitSet(maxDoc);
+      bits.fastSet(doc);
+    }
+
+    pos++;
+  }
+
+  @Override
+  public DocSet getDocSet() {
+    if (pos<=scratch.length) {
+      // assumes docs were collected in sorted order!
+      return new SortedIntDocSet(scratch, pos);
+    } else {
+      // set the bits for ids that were collected in the array
+      for (int i=0; i<scratch.length; i++) bits.fastSet(scratch[i]);
+      return new BitDocSet(bits,pos);
+    }
+  }
+
+  @Override
+  public void setScorer(Scorer scorer) throws IOException {
+    collector.setScorer(scorer);
+  }
+
+  @Override
+  public void setNextReader(AtomicReaderContext context) throws IOException {
+    collector.setNextReader(context);
+    this.base = context.docBase;
+  }
+}
Index: solr/core/src/java/org/apache/solr/search/function/distance/GeohashFunction.java
===================================================================
--- solr/core/src/java/org/apache/solr/search/function/distance/GeohashFunction.java	(revision 1238034)
+++ solr/core/src/java/org/apache/solr/search/function/distance/GeohashFunction.java	(working copy)
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.spatial.geohash.GeoHashUtils;
Index: solr/core/src/java/org/apache/solr/search/function/distance/GeohashHaversineFunction.java
===================================================================
--- solr/core/src/java/org/apache/solr/search/function/distance/GeohashHaversineFunction.java	(revision 1238034)
+++ solr/core/src/java/org/apache/solr/search/function/distance/GeohashHaversineFunction.java	(working copy)
@@ -21,7 +21,7 @@
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.queries.function.docvalues.DoubleDocValues;
 import org.apache.lucene.spatial.DistanceUtils;
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.search.IndexSearcher;
 import org.apache.lucene.spatial.geohash.GeoHashUtils;
 
Index: solr/core/src/java/org/apache/solr/search/function/distance/HaversineConstFunction.java
===================================================================
--- solr/core/src/java/org/apache/solr/search/function/distance/HaversineConstFunction.java	(revision 1238034)
+++ solr/core/src/java/org/apache/solr/search/function/distance/HaversineConstFunction.java	(working copy)
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.queries.function.docvalues.DoubleDocValues;
Index: solr/core/src/java/org/apache/solr/search/function/distance/HaversineFunction.java
===================================================================
--- solr/core/src/java/org/apache/solr/search/function/distance/HaversineFunction.java	(revision 1238034)
+++ solr/core/src/java/org/apache/solr/search/function/distance/HaversineFunction.java	(working copy)
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.queries.function.docvalues.DoubleDocValues;
Index: solr/core/src/java/org/apache/solr/search/function/distance/StringDistanceFunction.java
===================================================================
--- solr/core/src/java/org/apache/solr/search/function/distance/StringDistanceFunction.java	(revision 1238034)
+++ solr/core/src/java/org/apache/solr/search/function/distance/StringDistanceFunction.java	(working copy)
@@ -17,7 +17,7 @@
  * limitations under the License.
  */
 
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.queries.function.docvalues.FloatDocValues;
Index: solr/core/src/java/org/apache/solr/search/function/distance/VectorDistanceFunction.java
===================================================================
--- solr/core/src/java/org/apache/solr/search/function/distance/VectorDistanceFunction.java	(revision 1238034)
+++ solr/core/src/java/org/apache/solr/search/function/distance/VectorDistanceFunction.java	(working copy)
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.queries.function.docvalues.DoubleDocValues;
Index: solr/core/src/java/org/apache/solr/search/function/FileFloatSource.java
===================================================================
--- solr/core/src/java/org/apache/solr/search/function/FileFloatSource.java	(revision 1238034)
+++ solr/core/src/java/org/apache/solr/search/function/FileFloatSource.java	(working copy)
@@ -30,9 +30,9 @@
 import org.apache.lucene.index.DocsEnum;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.MultiFields;
+import org.apache.lucene.index.IndexReaderContext;
 import org.apache.lucene.index.TermsEnum;
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
-import org.apache.lucene.index.IndexReader.ReaderContext;
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.queries.function.docvalues.FloatDocValues;
@@ -78,9 +78,9 @@
   @Override
   public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
     final int off = readerContext.docBase;
-    ReaderContext topLevelContext = ReaderUtil.getTopLevelContext(readerContext);
+    IndexReaderContext topLevelContext = ReaderUtil.getTopLevelContext(readerContext);
 
-    final float[] arr = getCachedFloats(topLevelContext.reader);
+    final float[] arr = getCachedFloats(topLevelContext.reader());
     return new FloatDocValues(this) {
       @Override
       public float floatVal(int doc) {
Index: solr/core/src/java/org/apache/solr/search/function/ValueSourceRangeFilter.java
===================================================================
--- solr/core/src/java/org/apache/solr/search/function/ValueSourceRangeFilter.java	(revision 1238034)
+++ solr/core/src/java/org/apache/solr/search/function/ValueSourceRangeFilter.java	(working copy)
@@ -22,7 +22,7 @@
 import org.apache.lucene.search.DocIdSetIterator;
 import org.apache.lucene.search.IndexSearcher;
 import org.apache.lucene.search.BitsFilteredDocIdSet;
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.util.Bits;
 import org.apache.solr.search.SolrFilter;
 
@@ -78,7 +78,7 @@
      return BitsFilteredDocIdSet.wrap(new DocIdSet() {
        @Override
        public DocIdSetIterator iterator() throws IOException {
-         return valueSource.getValues(context, readerContext).getRangeScorer(readerContext.reader, lowerVal, upperVal, includeLower, includeUpper);
+         return valueSource.getValues(context, readerContext).getRangeScorer(readerContext.reader(), lowerVal, upperVal, includeLower, includeUpper);
        }
        @Override
        public Bits bits() throws IOException {
Index: solr/core/src/java/org/apache/solr/search/FunctionRangeQuery.java
===================================================================
--- solr/core/src/java/org/apache/solr/search/FunctionRangeQuery.java	(revision 1238034)
+++ solr/core/src/java/org/apache/solr/search/FunctionRangeQuery.java	(working copy)
@@ -1,69 +1,70 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.solr.search;
-
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.queries.function.FunctionValues;
-import org.apache.lucene.queries.function.ValueSource;
-import org.apache.lucene.queries.function.ValueSourceScorer;
-import org.apache.lucene.search.IndexSearcher;
-import org.apache.solr.search.function.ValueSourceRangeFilter;
-
-import java.io.IOException;
-import java.util.Map;
-
-// This class works as either a normal constant score query, or as a PostFilter using a collector
-public class FunctionRangeQuery extends SolrConstantScoreQuery implements PostFilter {
-  final ValueSourceRangeFilter rangeFilt;
-
-  public FunctionRangeQuery(ValueSourceRangeFilter filter) {
-    super(filter);
-    this.rangeFilt = filter;
-  }
-
-  @Override
-  public DelegatingCollector getFilterCollector(IndexSearcher searcher) {
-    Map fcontext = ValueSource.newContext(searcher);
-    return new FunctionRangeCollector(fcontext);
-  }
-
-  class FunctionRangeCollector extends DelegatingCollector {
-    final Map fcontext;
-    ValueSourceScorer scorer;
-    int maxdoc;
-
-    public FunctionRangeCollector(Map fcontext) {
-      this.fcontext = fcontext;
-    }
-
-    @Override
-    public void collect(int doc) throws IOException {
-      if (doc<maxdoc && scorer.matches(doc)) {
-        delegate.collect(doc);
-      }
-    }
-
-    @Override
-    public void setNextReader(IndexReader.AtomicReaderContext context) throws IOException {
-      maxdoc = context.reader.maxDoc();
-      FunctionValues dv = rangeFilt.getValueSource().getValues(fcontext, context);
-      scorer = dv.getRangeScorer(context.reader, rangeFilt.getLowerVal(), rangeFilt.getUpperVal(), rangeFilt.isIncludeLower(), rangeFilt.isIncludeUpper());
-      super.setNextReader(context);
-    }
-  }
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.solr.search;
+
+import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.queries.function.FunctionValues;
+import org.apache.lucene.queries.function.ValueSource;
+import org.apache.lucene.queries.function.ValueSourceScorer;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.solr.search.function.ValueSourceRangeFilter;
+
+import java.io.IOException;
+import java.util.Map;
+
+// This class works as either a normal constant score query, or as a PostFilter using a collector
+public class FunctionRangeQuery extends SolrConstantScoreQuery implements PostFilter {
+  final ValueSourceRangeFilter rangeFilt;
+
+  public FunctionRangeQuery(ValueSourceRangeFilter filter) {
+    super(filter);
+    this.rangeFilt = filter;
+  }
+
+  @Override
+  public DelegatingCollector getFilterCollector(IndexSearcher searcher) {
+    Map fcontext = ValueSource.newContext(searcher);
+    return new FunctionRangeCollector(fcontext);
+  }
+
+  class FunctionRangeCollector extends DelegatingCollector {
+    final Map fcontext;
+    ValueSourceScorer scorer;
+    int maxdoc;
+
+    public FunctionRangeCollector(Map fcontext) {
+      this.fcontext = fcontext;
+    }
+
+    @Override
+    public void collect(int doc) throws IOException {
+      if (doc<maxdoc && scorer.matches(doc)) {
+        delegate.collect(doc);
+      }
+    }
+
+    @Override
+    public void setNextReader(AtomicReaderContext context) throws IOException {
+      maxdoc = context.reader().maxDoc();
+      FunctionValues dv = rangeFilt.getValueSource().getValues(fcontext, context);
+      scorer = dv.getRangeScorer(context.reader(), rangeFilt.getLowerVal(), rangeFilt.getUpperVal(), rangeFilt.isIncludeLower(), rangeFilt.isIncludeUpper());
+      super.setNextReader(context);
+    }
+  }
+}
Index: solr/core/src/java/org/apache/solr/search/grouping/collector/FilterCollector.java
===================================================================
--- solr/core/src/java/org/apache/solr/search/grouping/collector/FilterCollector.java	(revision 1238034)
+++ solr/core/src/java/org/apache/solr/search/grouping/collector/FilterCollector.java	(working copy)
@@ -1,76 +1,76 @@
-package org.apache.solr.search.grouping.collector;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.search.Collector;
-import org.apache.lucene.search.Scorer;
-import org.apache.solr.search.DocSet;
-
-import java.io.IOException;
-
-/**
- * A collector that filters incoming doc ids that are not in the filter.
- *
- * @lucene.experimental
- */
-public class FilterCollector extends Collector {
-
-  private final DocSet filter;
-  private final Collector delegate;
-  private int docBase;
-  private int matches;
-
-  public FilterCollector(DocSet filter, Collector delegate) throws IOException {
-    this.filter = filter;
-    this.delegate = delegate;
-  }
-
-  public void setScorer(Scorer scorer) throws IOException {
-    delegate.setScorer(scorer);
-  }
-
-  public void collect(int doc) throws IOException {
-    matches++;
-    if (filter.exists(doc + docBase)) {
-      delegate.collect(doc);
-    }
-  }
-
-  public void setNextReader(IndexReader.AtomicReaderContext context) throws IOException {
-    this.docBase = context.docBase;
-    delegate.setNextReader(context);
-  }
-
-  public boolean acceptsDocsOutOfOrder() {
-    return delegate.acceptsDocsOutOfOrder();
-  }
-
-  public int getMatches() {
-    return matches;
-  }
-
-  /**
-   * Returns the delegate collector
-   *
-   * @return the delegate collector
-   */
-  public Collector getDelegate() {
-    return delegate;
-  }
-}
+package org.apache.solr.search.grouping.collector;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.search.Collector;
+import org.apache.lucene.search.Scorer;
+import org.apache.solr.search.DocSet;
+
+import java.io.IOException;
+
+/**
+ * A collector that filters incoming doc ids that are not in the filter.
+ *
+ * @lucene.experimental
+ */
+public class FilterCollector extends Collector {
+
+  private final DocSet filter;
+  private final Collector delegate;
+  private int docBase;
+  private int matches;
+
+  public FilterCollector(DocSet filter, Collector delegate) throws IOException {
+    this.filter = filter;
+    this.delegate = delegate;
+  }
+
+  public void setScorer(Scorer scorer) throws IOException {
+    delegate.setScorer(scorer);
+  }
+
+  public void collect(int doc) throws IOException {
+    matches++;
+    if (filter.exists(doc + docBase)) {
+      delegate.collect(doc);
+    }
+  }
+
+  public void setNextReader(AtomicReaderContext context) throws IOException {
+    this.docBase = context.docBase;
+    delegate.setNextReader(context);
+  }
+
+  public boolean acceptsDocsOutOfOrder() {
+    return delegate.acceptsDocsOutOfOrder();
+  }
+
+  public int getMatches() {
+    return matches;
+  }
+
+  /**
+   * Returns the delegate collector
+   *
+   * @return the delegate collector
+   */
+  public Collector getDelegate() {
+    return delegate;
+  }
+}
Index: solr/core/src/java/org/apache/solr/search/JoinQParserPlugin.java
===================================================================
--- solr/core/src/java/org/apache/solr/search/JoinQParserPlugin.java	(revision 1238034)
+++ solr/core/src/java/org/apache/solr/search/JoinQParserPlugin.java	(working copy)
@@ -188,7 +188,7 @@
 
 
     @Override
-    public Scorer scorer(IndexReader.AtomicReaderContext context, boolean scoreDocsInOrder,
+    public Scorer scorer(AtomicReaderContext context, boolean scoreDocsInOrder,
         boolean topScorer, Bits acceptDocs) throws IOException {
       if (filter == null) {
         boolean debug = rb != null && rb.isDebug();
@@ -261,8 +261,8 @@
         fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());
       }
 
-      Fields fromFields = MultiFields.getFields(fromSearcher.getIndexReader());
-      Fields toFields = fromSearcher==toSearcher ? fromFields : MultiFields.getFields(toSearcher.getIndexReader());
+      Fields fromFields = fromSearcher.getAtomicReader().fields();
+      Fields toFields = fromSearcher==toSearcher ? fromFields : toSearcher.getAtomicReader().fields();
       if (fromFields == null) return DocSet.EMPTY;
       Terms terms = fromFields.terms(fromField);
       Terms toTerms = toFields.terms(toField);
@@ -284,8 +284,8 @@
         }
       }
 
-      Bits fromLiveDocs = MultiFields.getLiveDocs(fromSearcher.getIndexReader());
-      Bits toLiveDocs = fromSearcher == toSearcher ? fromLiveDocs : MultiFields.getLiveDocs(toSearcher.getIndexReader());
+      Bits fromLiveDocs = fromSearcher.getAtomicReader().getLiveDocs();
+      Bits toLiveDocs = fromSearcher == toSearcher ? fromLiveDocs : toSearcher.getAtomicReader().getLiveDocs();
 
       fromDeState = new SolrIndexSearcher.DocsEnumState();
       fromDeState.fieldName = fromField;
@@ -456,8 +456,8 @@
     }
 
     @Override
-    public Explanation explain(IndexReader.AtomicReaderContext context, int doc) throws IOException {
-      Scorer scorer = scorer(context, true, false, context.reader.getLiveDocs());
+    public Explanation explain(AtomicReaderContext context, int doc) throws IOException {
+      Scorer scorer = scorer(context, true, false, context.reader().getLiveDocs());
       boolean exists = scorer.advance(doc) == doc;
 
       ComplexExplanation result = new ComplexExplanation();
Index: solr/core/src/java/org/apache/solr/search/MissingStringLastComparatorSource.java
===================================================================
--- solr/core/src/java/org/apache/solr/search/MissingStringLastComparatorSource.java	(revision 1238034)
+++ solr/core/src/java/org/apache/solr/search/MissingStringLastComparatorSource.java	(working copy)
@@ -17,8 +17,8 @@
 
 package org.apache.solr.search;
 
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
+import org.apache.lucene.index.AtomicReader;
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.search.FieldCache;
 import org.apache.lucene.search.FieldComparator;
 import org.apache.lucene.search.FieldComparatorSource;
@@ -118,7 +118,7 @@
 
   @Override
   public FieldComparator setNextReader(AtomicReaderContext context) throws IOException {
-    return TermOrdValComparator_SML.createComparator(context.reader, this);
+    return TermOrdValComparator_SML.createComparator(context.reader(), this);
   }
 
   // Base class for specialized (per bit width of the
@@ -159,7 +159,7 @@
 
     @Override
     public FieldComparator setNextReader(AtomicReaderContext context) throws IOException {
-      return TermOrdValComparator_SML.createComparator(context.reader, parent);
+      return TermOrdValComparator_SML.createComparator(context.reader(), parent);
     }
 
     @Override
@@ -432,7 +432,7 @@
     }
   }
 
-  public static FieldComparator createComparator(IndexReader reader, TermOrdValComparator_SML parent) throws IOException {
+  public static FieldComparator createComparator(AtomicReader reader, TermOrdValComparator_SML parent) throws IOException {
     parent.termsIndex = FieldCache.DEFAULT.getTermsIndex(reader, parent.field);
     final PackedInts.Reader docToOrd = parent.termsIndex.getDocToOrd();
     PerSegmentComparator perSegComp = null;
Index: solr/core/src/java/org/apache/solr/search/SolrConstantScoreQuery.java
===================================================================
--- solr/core/src/java/org/apache/solr/search/SolrConstantScoreQuery.java	(revision 1238034)
+++ solr/core/src/java/org/apache/solr/search/SolrConstantScoreQuery.java	(working copy)
@@ -4,7 +4,7 @@
 import org.apache.lucene.search.*;
 import org.apache.lucene.util.Bits;
 import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.solr.common.SolrException;
 
 import java.io.IOException;
@@ -127,7 +127,7 @@
     @Override
     public Explanation explain(AtomicReaderContext context, int doc) throws IOException {
 
-      ConstantScorer cs = new ConstantScorer(context, this, queryWeight, context.reader.getLiveDocs());
+      ConstantScorer cs = new ConstantScorer(context, this, queryWeight, context.reader().getLiveDocs());
       boolean exists = cs.docIdSetIterator.advance(doc) == doc;
 
       ComplexExplanation result = new ComplexExplanation();
Index: solr/core/src/java/org/apache/solr/search/SolrFilter.java
===================================================================
--- solr/core/src/java/org/apache/solr/search/SolrFilter.java	(revision 1238034)
+++ solr/core/src/java/org/apache/solr/search/SolrFilter.java	(working copy)
@@ -22,8 +22,8 @@
 import org.apache.lucene.search.DocIdSet;
 import org.apache.lucene.util.Bits;
 import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
+import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.AtomicReaderContext;
 
 import java.util.Map;
 import java.io.IOException;
Index: solr/core/src/java/org/apache/solr/search/SolrIndexSearcher.java
===================================================================
--- solr/core/src/java/org/apache/solr/search/SolrIndexSearcher.java	(revision 1238034)
+++ solr/core/src/java/org/apache/solr/search/SolrIndexSearcher.java	(working copy)
@@ -31,7 +31,7 @@
 import org.apache.lucene.document.NumericField;
 import org.apache.lucene.document.TextField;
 import org.apache.lucene.index.*;
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.search.*;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.FSDirectory;
@@ -81,7 +81,7 @@
   private long openTime = System.currentTimeMillis();
   private long registerTime = 0;
   private long warmupTime = 0;
-  private final IndexReader reader;
+  private final DirectoryReader reader;
   private final boolean closeReader;
 
   private final int queryResultWindowSize;
@@ -108,16 +108,19 @@
   private final Collection<String> fieldNames;
   private Collection<String> storedHighlightFieldNames;
   private DirectoryFactory directoryFactory;
+  
+  private final AtomicReader atomicReader; 
 
   public SolrIndexSearcher(SolrCore core, String path, IndexSchema schema, SolrIndexConfig config, String name, boolean enableCache, DirectoryFactory directoryFactory) throws IOException {
     // we don't need to reserve the directory because we get it from the factory
     this(core, schema,name, core.getIndexReaderFactory().newReader(directoryFactory.get(path, config.lockType)), true, enableCache, false, directoryFactory);
   }
 
-  public SolrIndexSearcher(SolrCore core, IndexSchema schema, String name, IndexReader r, boolean closeReader, boolean enableCache, boolean reserveDirectory, DirectoryFactory directoryFactory) {
+  public SolrIndexSearcher(SolrCore core, IndexSchema schema, String name, DirectoryReader r, boolean closeReader, boolean enableCache, boolean reserveDirectory, DirectoryFactory directoryFactory) throws IOException {
     super(r);
     this.directoryFactory = directoryFactory;
-    this.reader = getIndexReader();
+    this.reader = r;
+    this.atomicReader = SlowCompositeReaderWrapper.wrap(r);
     this.core = core;
     this.schema = schema;
     this.name = "Searcher@" + Integer.toHexString(hashCode()) + (name!=null ? " "+name : "");
@@ -184,7 +187,7 @@
     optimizer = solrConfig.filtOptEnabled ? new LuceneQueryOptimizer(solrConfig.filtOptCacheSize,solrConfig.filtOptThreshold) : null;
 
     fieldNames = new HashSet<String>();
-    for(FieldInfo fieldInfo : ReaderUtil.getMergedFieldInfos(r)) {
+    for(FieldInfo fieldInfo : atomicReader.getFieldInfos()) {
       fieldNames.add(fieldInfo.name);
     }
 
@@ -208,6 +211,16 @@
   public final int docFreq(Term term) throws IOException {
     return reader.docFreq(term);
   }
+  
+  public final AtomicReader getAtomicReader() {
+    return atomicReader;
+  }
+  
+  @Override
+  public final DirectoryReader getIndexReader() {
+    assert reader == super.getIndexReader();
+    return reader; 
+  }
 
   /** Register sub-objects such as caches
    */
@@ -556,7 +569,7 @@
    * @return the first document number containing the term
    */
   public int getFirstMatch(Term t) throws IOException {
-    Fields fields = MultiFields.getFields(reader);
+    Fields fields = atomicReader.fields();
     if (fields == null) return -1;
     Terms terms = fields.terms(t.field());
     if (terms == null) return -1;
@@ -565,7 +578,7 @@
     if (!termsEnum.seekExact(termBytes, false)) {
       return -1;
     }
-    DocsEnum docs = termsEnum.docs(MultiFields.getLiveDocs(reader), null, false);
+    DocsEnum docs = termsEnum.docs(atomicReader.getLiveDocs(), null, false);
     if (docs == null) return -1;
     int id = docs.nextDoc();
     return id == DocIdSetIterator.NO_MORE_DOCS ? -1 : id;
@@ -582,7 +595,7 @@
 
     for (int i=0; i<leaves.length; i++) {
       final AtomicReaderContext leaf = leaves[i];
-      final IndexReader reader = leaf.reader;
+      final AtomicReader reader = leaf.reader();
 
       final Fields fields = reader.fields();
       if (fields == null) continue;
@@ -736,7 +749,7 @@
 
     for (int i=0; i<leaves.length; i++) {
       final AtomicReaderContext leaf = leaves[i];
-      final IndexReader reader = leaf.reader;
+      final AtomicReader reader = leaf.reader();
       final Bits liveDocs = reader.getLiveDocs();   // TODO: the filter may already only have liveDocs...
       DocIdSet idSet = null;
       if (pf.filter != null) {
@@ -968,7 +981,7 @@
 
         for (int i=0; i<leaves.length; i++) {
           final AtomicReaderContext leaf = leaves[i];
-          final IndexReader reader = leaf.reader;
+          final AtomicReader reader = leaf.reader();
           collector.setNextReader(leaf);
           Fields fields = reader.fields();
           Terms terms = fields.terms(t.field());
@@ -979,7 +992,7 @@
           if (terms != null) {
             final TermsEnum termsEnum = terms.iterator(null);
             if (termsEnum.seekExact(termBytes, false)) {
-              docsEnum = termsEnum.docs(MultiFields.getLiveDocs(reader), null, false);
+              docsEnum = termsEnum.docs(liveDocs, null, false);
             }
           }
 
@@ -1768,7 +1781,7 @@
       while (doc>=end) {
         AtomicReaderContext leaf = leafContexts[readerIndex++];
         base = leaf.docBase;
-        end = base + leaf.reader.maxDoc();
+        end = base + leaf.reader().maxDoc();
         topCollector.setNextReader(leaf);
         // we should never need to set the scorer given the settings for the collector
       }
@@ -2173,7 +2186,7 @@
         iterators.add(iter);
       }
       for (Weight w : weights) {
-        Scorer scorer = w.scorer(context, true, false, context.reader.getLiveDocs());
+        Scorer scorer = w.scorer(context, true, false, context.reader().getLiveDocs());
         if (scorer == null) return null;
         iterators.add(scorer);
       }
Index: solr/core/src/java/org/apache/solr/search/SortedIntDocSet.java
===================================================================
--- solr/core/src/java/org/apache/solr/search/SortedIntDocSet.java	(revision 1238034)
+++ solr/core/src/java/org/apache/solr/search/SortedIntDocSet.java	(working copy)
@@ -24,7 +24,7 @@
 import org.apache.lucene.search.DocIdSetIterator;
 import org.apache.lucene.search.Filter;
 import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
+import org.apache.lucene.index.AtomicReaderContext;
 
 import java.io.IOException;
 
@@ -658,7 +658,7 @@
 
       @Override
       public DocIdSet getDocIdSet(final AtomicReaderContext context, final Bits acceptDocs) throws IOException {
-        IndexReader reader = context.reader;
+        IndexReader reader = context.reader();
 
         final int base = context.docBase;
         final int maxDoc = reader.maxDoc();
Index: solr/core/src/java/org/apache/solr/search/ValueSourceParser.java
===================================================================
--- solr/core/src/java/org/apache/solr/search/ValueSourceParser.java	(revision 1238034)
+++ solr/core/src/java/org/apache/solr/search/ValueSourceParser.java	(working copy)
@@ -16,7 +16,7 @@
  */
 package org.apache.solr.search;
 
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.queries.function.BoostedQuery;
 import org.apache.lucene.queries.function.FunctionValues;
Index: solr/core/src/java/org/apache/solr/update/MergeIndexesCommand.java
===================================================================
--- solr/core/src/java/org/apache/solr/update/MergeIndexesCommand.java	(revision 1238034)
+++ solr/core/src/java/org/apache/solr/update/MergeIndexesCommand.java	(working copy)
@@ -17,8 +17,7 @@
 
 package org.apache.solr.update;
 
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.store.Directory;
+import org.apache.lucene.index.DirectoryReader;
 import org.apache.solr.request.SolrQueryRequest;
 
 /**
@@ -28,9 +27,9 @@
  *
  */
 public class MergeIndexesCommand extends UpdateCommand {
-  public IndexReader[] readers;
+  public DirectoryReader[] readers;
 
-  public MergeIndexesCommand(IndexReader[] readers, SolrQueryRequest req) {
+  public MergeIndexesCommand(DirectoryReader[] readers, SolrQueryRequest req) {
     super(req);
     this.readers = readers;
   }
Index: solr/core/src/test
===================================================================
--- solr/core/src/test	(revision 1238034)
+++ solr/core/src/test	(working copy)

Property changes on: solr/core/src/test
___________________________________________________________________
Modified: svn:mergeinfo
   Merged /lucene/dev/branches/lucene2858/solr/core/src/test:r1234440-1238051
Index: solr/core/src/test/org/apache/solr/core/AlternateDirectoryTest.java
===================================================================
--- solr/core/src/test/org/apache/solr/core/AlternateDirectoryTest.java	(revision 1238034)
+++ solr/core/src/test/org/apache/solr/core/AlternateDirectoryTest.java	(working copy)
@@ -19,7 +19,7 @@
 import java.io.File;
 import java.io.IOException;
 
-import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.store.Directory;
 import org.apache.solr.SolrTestCaseJ4;
 import org.junit.BeforeClass;
@@ -61,9 +61,9 @@
     static volatile boolean newReaderCalled = false;
 
     @Override
-    public IndexReader newReader(Directory indexDir) throws IOException {
+    public DirectoryReader newReader(Directory indexDir) throws IOException {
       TestIndexReaderFactory.newReaderCalled = true;
-      return IndexReader.open(indexDir);
+      return DirectoryReader.open(indexDir);
     }
   }
 
Index: solr/core/src/test/org/apache/solr/request/TestFaceting.java
===================================================================
--- solr/core/src/test/org/apache/solr/request/TestFaceting.java	(revision 1238034)
+++ solr/core/src/test/org/apache/solr/request/TestFaceting.java	(working copy)
@@ -21,7 +21,6 @@
 import java.util.Random;
 
 import org.apache.lucene.index.DocTermOrds;
-import org.apache.lucene.index.SlowMultiReaderWrapper;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.index.TermsEnum;
 import org.apache.lucene.util.BytesRef;
@@ -81,7 +80,7 @@
 
     assertEquals(size, uif.getNumTerms());
 
-    TermsEnum te = uif.getOrdTermsEnum(new SlowMultiReaderWrapper(req.getSearcher().getIndexReader()));
+    TermsEnum te = uif.getOrdTermsEnum(req.getSearcher().getAtomicReader());
     assertEquals(size == 0, te == null);
 
     Random r = new Random(size);
Index: solr/core/src/test/org/apache/solr/search/TestDocSet.java
===================================================================
--- solr/core/src/test/org/apache/solr/search/TestDocSet.java	(revision 1238034)
+++ solr/core/src/test/org/apache/solr/search/TestDocSet.java	(working copy)
@@ -25,9 +25,9 @@
 import org.apache.lucene.index.FilterIndexReader;
 import org.apache.lucene.util.ReaderUtil;
 import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
-import org.apache.lucene.index.IndexReader.ReaderContext;
+import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.MultiReader;
+import org.apache.lucene.index.IndexReaderContext;
 import org.apache.lucene.search.DocIdSet;
 import org.apache.lucene.search.DocIdSetIterator;
 import org.apache.lucene.search.Filter;
@@ -356,11 +356,6 @@
       }
 
       @Override
-      public IndexReader[] getSequentialSubReaders() {
-        return null;
-      }
-
-      @Override
       public FieldInfos getFieldInfos() {
         return new FieldInfos();
       }
@@ -418,7 +413,7 @@
   }
 
   public void doFilterTest(IndexReader reader) throws IOException {
-    ReaderContext topLevelContext = reader.getTopReaderContext();
+    IndexReaderContext topLevelContext = reader.getTopReaderContext();
     OpenBitSet bs = getRandomSet(reader.maxDoc(), rand.nextInt(reader.maxDoc()+1));
     DocSet a = new BitDocSet(bs);
     DocSet b = getIntDocSet(bs);
Index: solr/core/src/test/org/apache/solr/search/TestIndexSearcher.java
===================================================================
--- solr/core/src/test/org/apache/solr/search/TestIndexSearcher.java	(revision 1238034)
+++ solr/core/src/test/org/apache/solr/search/TestIndexSearcher.java	(working copy)
@@ -16,8 +16,8 @@
  */
 package org.apache.solr.search;
 
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
-import org.apache.lucene.index.IndexReader.ReaderContext;
+import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.IndexReaderContext;
 import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.util.ReaderUtil;
@@ -49,7 +49,7 @@
     ValueSource vs = sf.getType().getValueSource(sf, null);
     Map context = ValueSource.newContext(sqr.getSearcher());
     vs.createWeight(context, sqr.getSearcher());
-    ReaderContext topReaderContext = sqr.getSearcher().getTopReaderContext();
+    IndexReaderContext topReaderContext = sqr.getSearcher().getTopReaderContext();
     AtomicReaderContext[] leaves = ReaderUtil.leaves(topReaderContext);
     int idx = ReaderUtil.subIndex(doc, leaves);
     AtomicReaderContext leaf = leaves[idx];
@@ -64,7 +64,7 @@
     assertU(commit());
 
     SolrQueryRequest sr1 = req("q","foo");
-    ReaderContext rCtx1 = sr1.getSearcher().getTopReaderContext();
+    IndexReaderContext rCtx1 = sr1.getSearcher().getTopReaderContext();
 
     String sval1 = getStringVal(sr1, "v_s1",0);
     assertEquals("string1", sval1);
@@ -74,33 +74,33 @@
     assertU(commit());
 
     SolrQueryRequest sr2 = req("q","foo");
-    ReaderContext rCtx2 = sr2.getSearcher().getTopReaderContext();
+    IndexReaderContext rCtx2 = sr2.getSearcher().getTopReaderContext();
 
     // make sure the readers share the first segment
     // Didn't work w/ older versions of lucene2.9 going from segment -> multi
-    assertEquals(ReaderUtil.leaves(rCtx1)[0].reader, ReaderUtil.leaves(rCtx2)[0].reader);
+    assertEquals(ReaderUtil.leaves(rCtx1)[0].reader(), ReaderUtil.leaves(rCtx2)[0].reader());
 
     assertU(adoc("id","5", "v_f","3.14159"));
     assertU(adoc("id","6", "v_f","8983", "v_s1","string6"));
     assertU(commit());
 
     SolrQueryRequest sr3 = req("q","foo");
-    ReaderContext rCtx3 = sr3.getSearcher().getTopReaderContext();
+    IndexReaderContext rCtx3 = sr3.getSearcher().getTopReaderContext();
     // make sure the readers share segments
     // assertEquals(r1.getLeafReaders()[0], r3.getLeafReaders()[0]);
-    assertEquals(ReaderUtil.leaves(rCtx2)[0].reader, ReaderUtil.leaves(rCtx3)[0].reader);
-    assertEquals(ReaderUtil.leaves(rCtx2)[1].reader, ReaderUtil.leaves(rCtx3)[1].reader);
+    assertEquals(ReaderUtil.leaves(rCtx2)[0].reader(), ReaderUtil.leaves(rCtx3)[0].reader());
+    assertEquals(ReaderUtil.leaves(rCtx2)[1].reader(), ReaderUtil.leaves(rCtx3)[1].reader());
 
     sr1.close();
     sr2.close();            
 
     // should currently be 1, but this could change depending on future index management
-    int baseRefCount = rCtx3.reader.getRefCount();
+    int baseRefCount = rCtx3.reader().getRefCount();
     assertEquals(1, baseRefCount);
 
     assertU(commit());
     SolrQueryRequest sr4 = req("q","foo");
-    ReaderContext rCtx4 = sr4.getSearcher().getTopReaderContext();
+    IndexReaderContext rCtx4 = sr4.getSearcher().getTopReaderContext();
 
     // force an index change so the registered searcher won't be the one we are testing (and
     // then we should be able to test the refCount going all the way to 0
@@ -108,23 +108,23 @@
     assertU(commit()); 
 
     // test that reader didn't change (according to equals at least... which uses the wrapped reader)
-    assertEquals(rCtx3.reader, rCtx4.reader);
-    assertEquals(baseRefCount+1, rCtx4.reader.getRefCount());
+    assertEquals(rCtx3.reader(), rCtx4.reader());
+    assertEquals(baseRefCount+1, rCtx4.reader().getRefCount());
     sr3.close();
-    assertEquals(baseRefCount, rCtx4.reader.getRefCount());
+    assertEquals(baseRefCount, rCtx4.reader().getRefCount());
     sr4.close();
-    assertEquals(baseRefCount-1, rCtx4.reader.getRefCount());
+    assertEquals(baseRefCount-1, rCtx4.reader().getRefCount());
 
 
     SolrQueryRequest sr5 = req("q","foo");
-    ReaderContext rCtx5 = sr5.getSearcher().getTopReaderContext();
+    IndexReaderContext rCtx5 = sr5.getSearcher().getTopReaderContext();
 
     assertU(delI("1"));
     assertU(commit());
     SolrQueryRequest sr6 = req("q","foo");
-    ReaderContext rCtx6 = sr6.getSearcher().getTopReaderContext();
-    assertEquals(1, ReaderUtil.leaves(rCtx6)[0].reader.numDocs()); // only a single doc left in the first segment
-    assertTrue( !ReaderUtil.leaves(rCtx5)[0].reader.equals(ReaderUtil.leaves(rCtx6)[0].reader) );  // readers now different
+    IndexReaderContext rCtx6 = sr6.getSearcher().getTopReaderContext();
+    assertEquals(1, ReaderUtil.leaves(rCtx6)[0].reader().numDocs()); // only a single doc left in the first segment
+    assertTrue( !ReaderUtil.leaves(rCtx5)[0].reader().equals(ReaderUtil.leaves(rCtx6)[0].reader()) );  // readers now different
 
     sr5.close();
     sr6.close();
Index: solr/core/src/test/org/apache/solr/search/TestRealTimeGet.java
===================================================================
--- solr/core/src/test/org/apache/solr/search/TestRealTimeGet.java	(revision 1238034)
+++ solr/core/src/test/org/apache/solr/search/TestRealTimeGet.java	(working copy)
@@ -1342,7 +1342,7 @@
 
 
   // The purpose of this test is to roughly model how solr uses lucene
-  IndexReader reader;
+  DirectoryReader reader;
   @Test
   public void testStressLuceneNRT() throws Exception {
     final int commitPercent = 5 + random.nextInt(20);
@@ -1408,7 +1408,7 @@
     // reader = IndexReader.open(dir);
     // make this reader an NRT reader from the start to avoid the first non-writer openIfChanged
     // to only opening at the last commit point.
-    reader = IndexReader.open(writer.w, true);
+    reader = DirectoryReader.open(writer.w, true);
 
     for (int i=0; i<nWriteThreads; i++) {
       Thread thread = new Thread("WRITER"+i) {
@@ -1424,7 +1424,7 @@
                 if (numCommitting.incrementAndGet() <= maxConcurrentCommits) {
                   Map<Integer,DocInfo> newCommittedModel;
                   long version;
-                  IndexReader oldReader;
+                  DirectoryReader oldReader;
 
                   boolean softCommit = rand.nextInt(100) < softCommitPercent;
 
@@ -1452,12 +1452,12 @@
 
                   verbose("reopen start using", oldReader);
 
-                  IndexReader newReader;
+                  DirectoryReader newReader;
                   if (softCommit) {
-                    newReader = IndexReader.openIfChanged(oldReader, writer.w, true);
+                    newReader = DirectoryReader.openIfChanged(oldReader, writer.w, true);
                   } else {
                     // will only open to last commit
-                   newReader = IndexReader.openIfChanged(oldReader);
+                   newReader = DirectoryReader.openIfChanged(oldReader);
                   }
 
 
Index: solr/core/src/test/org/apache/solr/search/TestSort.java
===================================================================
--- solr/core/src/test/org/apache/solr/search/TestSort.java	(revision 1238034)
+++ solr/core/src/test/org/apache/solr/search/TestSort.java	(working copy)
@@ -24,8 +24,8 @@
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.StringField;
-import org.apache.lucene.index.IndexReader.AtomicReaderContext;
-import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexWriter;
 import org.apache.lucene.index.IndexWriterConfig;
 import org.apache.lucene.search.*;
@@ -194,7 +194,7 @@
       iw.close();
 
 
-      IndexReader reader = IndexReader.open(dir);
+      DirectoryReader reader = DirectoryReader.open(dir);
       IndexSearcher searcher = new IndexSearcher(reader);
       // System.out.println("segments="+searcher.getIndexReader().getSequentialSubReaders().length);
       assertTrue(reader.getSequentialSubReaders().length > 1);
@@ -203,7 +203,7 @@
         Filter filt = new Filter() {
           @Override
           public DocIdSet getDocIdSet(AtomicReaderContext context, Bits acceptDocs) throws IOException {
-            return BitsFilteredDocIdSet.wrap(randSet(context.reader.maxDoc()), acceptDocs);
+            return BitsFilteredDocIdSet.wrap(randSet(context.reader().maxDoc()), acceptDocs);
           }
         };
 
Index: solr/core/src/test/org/apache/solr/update/DirectUpdateHandlerTest.java
===================================================================
--- solr/core/src/test/org/apache/solr/update/DirectUpdateHandlerTest.java	(revision 1238034)
+++ solr/core/src/test/org/apache/solr/update/DirectUpdateHandlerTest.java	(working copy)
@@ -22,7 +22,7 @@
 import java.util.HashMap;
 import java.util.Map;
 
-import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.store.Directory;
 import org.apache.solr.SolrTestCaseJ4;
 import org.apache.solr.common.params.CommonParams;
@@ -256,18 +256,16 @@
     assertU(commit());
 
     SolrQueryRequest sr = req("q","foo");
-    IndexReader r = sr.getSearcher().getTopReaderContext().reader;
+    DirectoryReader r = sr.getSearcher().getIndexReader();
     assertTrue(r.maxDoc() > r.numDocs());   // should have deletions
-    assertFalse(r.getTopReaderContext().isAtomic);  // more than 1 segment
     sr.close();
 
     assertU(commit("expungeDeletes","true"));
 
     sr = req("q","foo");
-    r = sr.getSearcher().getTopReaderContext().reader;
+    r = r = sr.getSearcher().getIndexReader();
     assertEquals(r.maxDoc(), r.numDocs());  // no deletions
     assertEquals(4,r.maxDoc());             // no dups
-    assertFalse(r.getTopReaderContext().isAtomic);  //still more than 1 segment
     sr.close();
   }
   
@@ -278,7 +276,7 @@
     assertU(commit());       // commit a second time to make sure index files aren't still referenced by the old searcher
 
     SolrQueryRequest sr = req();
-    IndexReader r = sr.getSearcher().getTopReaderContext().reader;
+    DirectoryReader r = sr.getSearcher().getIndexReader();
     Directory d = r.directory();
 
     log.info("FILES before addDoc="+ Arrays.asList(d.listAll()));
Index: solr/dev-tools
===================================================================
--- solr/dev-tools	(revision 1238034)
+++ solr/dev-tools	(working copy)

Property changes on: solr/dev-tools
___________________________________________________________________
Modified: svn:mergeinfo
   Merged /lucene/dev/branches/lucene2858/solr/dev-tools:r1234440-1238051
Index: solr/example
===================================================================
--- solr/example	(revision 1238034)
+++ solr/example	(working copy)

Property changes on: solr/example
___________________________________________________________________
Modified: svn:mergeinfo
   Merged /lucene/dev/branches/lucene2858/solr/example:r1234440-1238051
Index: solr/lib
===================================================================
--- solr/lib	(revision 1238034)
+++ solr/lib	(working copy)

Property changes on: solr/lib
___________________________________________________________________
Modified: svn:mergeinfo
   Merged /lucene/dev/branches/lucene2858/solr/lib:r1234440-1238051
Index: solr/LICENSE.txt
===================================================================
--- solr/LICENSE.txt	(revision 1238034)
+++ solr/LICENSE.txt	(working copy)

Property changes on: solr/LICENSE.txt
___________________________________________________________________
Modified: svn:mergeinfo
   Merged /lucene/dev/branches/lucene2858/solr/LICENSE.txt:r1234440-1238051
Index: solr/NOTICE.txt
===================================================================
--- solr/NOTICE.txt	(revision 1238034)
+++ solr/NOTICE.txt	(working copy)

Property changes on: solr/NOTICE.txt
___________________________________________________________________
Modified: svn:mergeinfo
   Merged /lucene/dev/branches/lucene2858/solr/NOTICE.txt:r1234440-1238051
Index: solr/README.txt
===================================================================
--- solr/README.txt	(revision 1238034)
+++ solr/README.txt	(working copy)

Property changes on: solr/README.txt
___________________________________________________________________
Modified: svn:mergeinfo
   Merged /lucene/dev/branches/lucene2858/solr/README.txt:r1234440-1238051
Index: solr/scripts
===================================================================
--- solr/scripts	(revision 1238034)
+++ solr/scripts	(working copy)

Property changes on: solr/scripts
___________________________________________________________________
Modified: svn:mergeinfo
   Merged /lucene/dev/branches/lucene2858/solr/scripts:r1234440-1238051
Index: solr/site
===================================================================
--- solr/site	(revision 1238034)
+++ solr/site	(working copy)

Property changes on: solr/site
___________________________________________________________________
Modified: svn:mergeinfo
   Merged /lucene/dev/branches/lucene2858/solr/site:r1234440-1238051
Index: solr/site-src
===================================================================
--- solr/site-src	(revision 1238034)
+++ solr/site-src	(working copy)

Property changes on: solr/site-src
___________________________________________________________________
Modified: svn:mergeinfo
   Merged /lucene/dev/branches/lucene2858/solr/site-src:r1234440-1238051
Index: solr/solrj
===================================================================
--- solr/solrj	(revision 1238034)
+++ solr/solrj	(working copy)

Property changes on: solr/solrj
___________________________________________________________________
Modified: svn:mergeinfo
   Merged /lucene/dev/branches/lucene2858/solr/solrj:r1234440-1238051
Index: solr/solrj/src/java
===================================================================
--- solr/solrj/src/java	(revision 1238034)
+++ solr/solrj/src/java	(working copy)

Property changes on: solr/solrj/src/java
___________________________________________________________________
Modified: svn:mergeinfo
   Merged /lucene/dev/branches/lucene2858/solr/solrj/src/java:r1234440-1238051
Index: solr/solrj/src/test/org/apache/solr/client
===================================================================
--- solr/solrj/src/test/org/apache/solr/client	(revision 1238034)
+++ solr/solrj/src/test/org/apache/solr/client	(working copy)

Property changes on: solr/solrj/src/test/org/apache/solr/client
___________________________________________________________________
Modified: svn:mergeinfo
   Merged /lucene/dev/branches/lucene2858/solr/solrj/src/test/org/apache/solr/client:r1234440-1238051
Index: solr/solrj/src/test/org/apache/solr/client/solrj
===================================================================
--- solr/solrj/src/test/org/apache/solr/client/solrj	(revision 1238034)
+++ solr/solrj/src/test/org/apache/solr/client/solrj	(working copy)

Property changes on: solr/solrj/src/test/org/apache/solr/client/solrj
___________________________________________________________________
Modified: svn:mergeinfo
   Merged /lucene/dev/branches/lucene2858/solr/solrj/src/test/org/apache/solr/client/solrj:r1234440-1238051
Index: solr/solrj/src/test/org/apache/solr/common
===================================================================
--- solr/solrj/src/test/org/apache/solr/common	(revision 1238034)
+++ solr/solrj/src/test/org/apache/solr/common	(working copy)

Property changes on: solr/solrj/src/test/org/apache/solr/common
___________________________________________________________________
Modified: svn:mergeinfo
   Merged /lucene/dev/branches/lucene2858/solr/solrj/src/test/org/apache/solr/common:r1234440-1238051
Index: solr/test-framework
===================================================================
--- solr/test-framework	(revision 1238034)
+++ solr/test-framework	(working copy)

Property changes on: solr/test-framework
___________________________________________________________________
Modified: svn:mergeinfo
   Merged /lucene/dev/branches/lucene2858/solr/test-framework:r1234440-1238051
Index: solr/testlogging.properties
===================================================================
--- solr/testlogging.properties	(revision 1238034)
+++ solr/testlogging.properties	(working copy)

Property changes on: solr/testlogging.properties
___________________________________________________________________
Modified: svn:mergeinfo
   Merged /lucene/dev/branches/lucene2858/solr/testlogging.properties:r1234440-1238051
Index: solr/webapp
===================================================================
--- solr/webapp	(revision 1238034)
+++ solr/webapp	(working copy)

Property changes on: solr/webapp
___________________________________________________________________
Modified: svn:mergeinfo
   Merged /lucene/dev/branches/lucene2858/solr/webapp:r1234440-1238051
