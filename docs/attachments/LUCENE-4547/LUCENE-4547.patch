Index: lucene/grouping/src/java/org/apache/lucene/search/grouping/term/TermGroupFacetCollector.java
===================================================================
--- lucene/grouping/src/java/org/apache/lucene/search/grouping/term/TermGroupFacetCollector.java	(revision 1442822)
+++ lucene/grouping/src/java/org/apache/lucene/search/grouping/term/TermGroupFacetCollector.java	(working copy)
@@ -17,17 +17,19 @@
  * limitations under the License.
  */
 
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.List;
+
 import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.DocTermOrds;
+import org.apache.lucene.index.SortedDocValues;
+import org.apache.lucene.index.SortedDocValuesTermsEnum;
 import org.apache.lucene.index.TermsEnum;
 import org.apache.lucene.search.FieldCache;
 import org.apache.lucene.search.grouping.AbstractGroupFacetCollector;
 import org.apache.lucene.util.*;
 
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.List;
-
 /**
  * An implementation of {@link AbstractGroupFacetCollector} that computes grouped facets based on the indexed terms
  * from the {@link FieldCache}.
@@ -38,9 +40,8 @@
 
   final List<GroupedFacetHit> groupedFacetHits;
   final SentinelIntSet segmentGroupedFacetHits;
-  final BytesRef spare = new BytesRef();
 
-  FieldCache.DocTermsIndex groupFieldTermsIndex;
+  SortedDocValues groupFieldTermsIndex;
 
   /**
    * Factory method for creating the right implementation based on the fact whether the facet field contains
@@ -70,13 +71,13 @@
   TermGroupFacetCollector(String groupField, String facetField, BytesRef facetPrefix, int initialSize) {
     super(groupField, facetField, facetPrefix);
     groupedFacetHits = new ArrayList<GroupedFacetHit>(initialSize);
-    segmentGroupedFacetHits = new SentinelIntSet(initialSize, -1);
+    segmentGroupedFacetHits = new SentinelIntSet(initialSize, Integer.MIN_VALUE);
   }
 
   // Implementation for single valued facet fields.
   static class SV extends TermGroupFacetCollector {
 
-    private FieldCache.DocTermsIndex facetFieldTermsIndex;
+    private SortedDocValues facetFieldTermsIndex;
 
     SV(String groupField, String facetField, BytesRef facetPrefix, int initialSize) {
       super(groupField, facetField, facetPrefix, initialSize);
@@ -90,21 +91,33 @@
       }
 
       int groupOrd = groupFieldTermsIndex.getOrd(doc);
-      int segmentGroupedFacetsIndex = (groupOrd * facetFieldTermsIndex.numOrd()) + facetOrd;
+      int segmentGroupedFacetsIndex = groupOrd * (facetFieldTermsIndex.getValueCount()+1) + facetOrd;
       if (segmentGroupedFacetHits.exists(segmentGroupedFacetsIndex)) {
         return;
       }
 
       segmentTotalCount++;
-      segmentFacetCounts[facetOrd]++;
+      segmentFacetCounts[facetOrd+1]++;
 
       segmentGroupedFacetHits.put(segmentGroupedFacetsIndex);
-      groupedFacetHits.add(
-          new GroupedFacetHit(
-              groupOrd == 0 ? null : groupFieldTermsIndex.lookup(groupOrd, new BytesRef()),
-              facetOrd == 0 ? null : facetFieldTermsIndex.lookup(facetOrd, new BytesRef())
-          )
-      );
+
+      BytesRef groupKey;
+      if (groupOrd == -1) {
+        groupKey = null;
+      } else {
+        groupKey = new BytesRef();
+        groupFieldTermsIndex.lookupOrd(groupOrd, groupKey);
+      }
+
+      BytesRef facetKey;
+      if (facetOrd == -1) {
+        facetKey = null;
+      } else {
+        facetKey = new BytesRef();
+        facetFieldTermsIndex.lookupOrd(facetOrd, facetKey);
+      }
+
+      groupedFacetHits.add(new GroupedFacetHit(groupKey, facetKey));
     }
 
     @Override
@@ -115,44 +128,47 @@
 
       groupFieldTermsIndex = FieldCache.DEFAULT.getTermsIndex(context.reader(), groupField);
       facetFieldTermsIndex = FieldCache.DEFAULT.getTermsIndex(context.reader(), facetField);
-      segmentFacetCounts = new int[facetFieldTermsIndex.numOrd()];
+
+      // 1+ to allow for the -1 "not set":
+      segmentFacetCounts = new int[facetFieldTermsIndex.getValueCount()+1];
       segmentTotalCount = 0;
 
       segmentGroupedFacetHits.clear();
       for (GroupedFacetHit groupedFacetHit : groupedFacetHits) {
-        int facetOrd = facetFieldTermsIndex.binarySearchLookup(groupedFacetHit.facetValue, spare);
-        if (facetOrd < 0) {
+        int facetOrd = groupedFacetHit.facetValue == null ? -1 : facetFieldTermsIndex.lookupTerm(groupedFacetHit.facetValue);
+        if (groupedFacetHit.facetValue != null && facetOrd < 0) {
           continue;
         }
 
-        int groupOrd = groupFieldTermsIndex.binarySearchLookup(groupedFacetHit.groupValue, spare);
-        if (groupOrd < 0) {
+        int groupOrd = groupedFacetHit.groupValue == null ? -1 : groupFieldTermsIndex.lookupTerm(groupedFacetHit.groupValue);
+        if (groupedFacetHit.groupValue != null && groupOrd < 0) {
           continue;
         }
 
-        int segmentGroupedFacetsIndex = (groupOrd * facetFieldTermsIndex.numOrd()) + facetOrd;
+        int segmentGroupedFacetsIndex = groupOrd * (facetFieldTermsIndex.getValueCount()+1) + facetOrd;
         segmentGroupedFacetHits.put(segmentGroupedFacetsIndex);
       }
 
       if (facetPrefix != null) {
-        startFacetOrd = facetFieldTermsIndex.binarySearchLookup(facetPrefix, spare);
+        startFacetOrd = facetFieldTermsIndex.lookupTerm(facetPrefix);
         if (startFacetOrd < 0) {
           // Points to the ord one higher than facetPrefix
           startFacetOrd = -startFacetOrd - 1;
         }
         BytesRef facetEndPrefix = BytesRef.deepCopyOf(facetPrefix);
         facetEndPrefix.append(UnicodeUtil.BIG_TERM);
-        endFacetOrd = facetFieldTermsIndex.binarySearchLookup(facetEndPrefix, spare);
+        endFacetOrd = facetFieldTermsIndex.lookupTerm(facetEndPrefix);
+        assert endFacetOrd < 0;
         endFacetOrd = -endFacetOrd - 1; // Points to the ord one higher than facetEndPrefix
       } else {
-        startFacetOrd = 0;
-        endFacetOrd = facetFieldTermsIndex.numOrd();
+        startFacetOrd = -1;
+        endFacetOrd = facetFieldTermsIndex.getValueCount();
       }
     }
 
     @Override
     protected SegmentResult createSegmentResult() throws IOException {
-      return new SegmentResult(segmentFacetCounts, segmentTotalCount, facetFieldTermsIndex.getTermsEnum(), startFacetOrd, endFacetOrd);
+      return new SegmentResult(segmentFacetCounts, segmentTotalCount, new SortedDocValuesTermsEnum(facetFieldTermsIndex), startFacetOrd, endFacetOrd);
     }
 
     private static class SegmentResult extends AbstractGroupFacetCollector.SegmentResult {
@@ -160,11 +176,12 @@
       final TermsEnum tenum;
 
       SegmentResult(int[] counts, int total, TermsEnum tenum, int startFacetOrd, int endFacetOrd) throws IOException {
-        super(counts, total - counts[0], counts[0], endFacetOrd);
+        super(counts, total - counts[0], counts[0], endFacetOrd+1);
         this.tenum = tenum;
-        this.mergePos = startFacetOrd == 0 ? 1 : startFacetOrd;
+        this.mergePos = startFacetOrd == -1 ? 1 : startFacetOrd+1;
         if (mergePos < maxTermPos) {
-          tenum.seekExact(mergePos);
+          assert tenum != null;
+          tenum.seekExact(startFacetOrd == -1 ? 0 : startFacetOrd);
           mergeTerm = tenum.term();
         }
       }
@@ -173,9 +190,7 @@
       protected void nextTerm() throws IOException {
         mergeTerm = tenum.next();
       }
-
     }
-
   }
 
   // Implementation for multi valued facet fields.
@@ -202,9 +217,14 @@
         segmentFacetCounts[facetFieldDocTermOrds.numTerms()]++;
 
         segmentGroupedFacetHits.put(segmentGroupedFacetsIndex);
-        groupedFacetHits.add(
-            new GroupedFacetHit(groupOrd == 0 ? null : groupFieldTermsIndex.lookup(groupOrd, new BytesRef()), null)
-        );
+        BytesRef groupKey;
+        if (groupOrd == -1) {
+          groupKey = null;
+        } else {
+          groupKey = new BytesRef();
+          groupFieldTermsIndex.lookupOrd(groupOrd, groupKey);
+        }
+        groupedFacetHits.add(new GroupedFacetHit(groupKey, null));
         return;
       }
 
@@ -228,7 +248,7 @@
             continue;
           }
 
-          int segmentGroupedFacetsIndex = (groupOrd * (facetFieldDocTermOrds.numTerms() + 1)) + facetOrd;
+          int segmentGroupedFacetsIndex = groupOrd * (facetFieldDocTermOrds.numTerms() + 1) + facetOrd;
           if (segmentGroupedFacetHits.exists(segmentGroupedFacetsIndex)) {
             continue;
           }
@@ -237,9 +257,17 @@
           segmentFacetCounts[facetOrd]++;
 
           segmentGroupedFacetHits.put(segmentGroupedFacetsIndex);
+
+          BytesRef groupKey;
+          if (groupOrd == -1) {
+            groupKey = null;
+          } else {
+            groupKey = new BytesRef();
+            groupFieldTermsIndex.lookupOrd(groupOrd, groupKey);
+          }
+
           groupedFacetHits.add(
-              new GroupedFacetHit(
-                  groupOrd == 0 ? null : groupFieldTermsIndex.lookup(groupOrd, new BytesRef()),
+              new GroupedFacetHit(groupKey,
                   facetOrd == facetFieldDocTermOrds.numTerms() ? null : BytesRef.deepCopyOf(facetFieldDocTermOrds.lookupTerm(facetOrdTermsEnum, facetOrd))
               )
           );
@@ -263,8 +291,8 @@
 
       segmentGroupedFacetHits.clear();
       for (GroupedFacetHit groupedFacetHit : groupedFacetHits) {
-        int groupOrd = groupFieldTermsIndex.binarySearchLookup(groupedFacetHit.groupValue, spare);
-        if (groupOrd < 0) {
+        int groupOrd = groupedFacetHit.groupValue == null ? -1 : groupFieldTermsIndex.lookupTerm(groupedFacetHit.groupValue);
+        if (groupedFacetHit.groupValue != null && groupOrd < 0) {
           continue;
         }
 
@@ -279,7 +307,7 @@
         }
 
         // (facetFieldDocTermOrds.numTerms() + 1) for all possible facet values and docs not containing facet field
-        int segmentGroupedFacetsIndex = (groupOrd * (facetFieldDocTermOrds.numTerms() + 1)) + facetOrd;
+        int segmentGroupedFacetsIndex = groupOrd * (facetFieldDocTermOrds.numTerms() + 1) + facetOrd;
         segmentGroupedFacetHits.put(segmentGroupedFacetsIndex);
       }
 
@@ -337,10 +365,8 @@
       protected void nextTerm() throws IOException {
         mergeTerm = tenum.next();
       }
-
     }
   }
-
 }
 
 class GroupedFacetHit {
Index: lucene/grouping/src/java/org/apache/lucene/search/grouping/term/TermDistinctValuesCollector.java
===================================================================
--- lucene/grouping/src/java/org/apache/lucene/search/grouping/term/TermDistinctValuesCollector.java	(revision 1442822)
+++ lucene/grouping/src/java/org/apache/lucene/search/grouping/term/TermDistinctValuesCollector.java	(working copy)
@@ -17,20 +17,20 @@
  * limitations under the License.
  */
 
+import java.io.IOException;
+import java.util.*;
+
 import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.SortedDocValues;
 import org.apache.lucene.search.FieldCache;
-import org.apache.lucene.search.FieldCache.DocTermsIndex; // javadocs
 import org.apache.lucene.search.grouping.AbstractDistinctValuesCollector;
 import org.apache.lucene.search.grouping.SearchGroup;
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.SentinelIntSet;
 
-import java.io.IOException;
-import java.util.*;
-
 /**
  * A term based implementation of {@link org.apache.lucene.search.grouping.AbstractDistinctValuesCollector} that relies
- * on {@link DocTermsIndex} to count the distinct values per group.
+ * on {@link SortedDocValues} to count the distinct values per group.
  *
  * @lucene.experimental
  */
@@ -41,10 +41,9 @@
   private final List<GroupCount> groups;
   private final SentinelIntSet ordSet;
   private final GroupCount groupCounts[];
-  private final BytesRef spare = new BytesRef();
 
-  private FieldCache.DocTermsIndex groupFieldTermIndex;
-  private FieldCache.DocTermsIndex countFieldTermIndex;
+  private SortedDocValues groupFieldTermIndex;
+  private SortedDocValues countFieldTermIndex;
 
   /**
    * Constructs {@link TermDistinctValuesCollector} instance.
@@ -60,7 +59,7 @@
     for (SearchGroup<BytesRef> group : groups) {
       this.groups.add(new GroupCount(group.groupValue));
     }
-    ordSet = new SentinelIntSet(groups.size(), -1);
+    ordSet = new SentinelIntSet(groups.size(), -2);
     groupCounts = new GroupCount[ordSet.keys.length];
   }
 
@@ -73,11 +72,13 @@
 
     GroupCount gc = groupCounts[slot];
     int countOrd = countFieldTermIndex.getOrd(doc);
-    if (doesNotContainsOrd(countOrd, gc.ords)) {
-      if (countOrd == 0) {
+    if (doesNotContainOrd(countOrd, gc.ords)) {
+      if (countOrd == -1) {
         gc.uniqueValues.add(null);
       } else {
-        gc.uniqueValues.add(countFieldTermIndex.lookup(countOrd, new BytesRef()));
+        BytesRef br = new BytesRef();
+        countFieldTermIndex.lookupOrd(countOrd, br);
+        gc.uniqueValues.add(br);
       }
 
       gc.ords = Arrays.copyOf(gc.ords, gc.ords.length + 1);
@@ -88,7 +89,7 @@
     }
   }
 
-  private boolean doesNotContainsOrd(int ord, int[] ords) {
+  private boolean doesNotContainOrd(int ord, int[] ords) {
     if (ords.length == 0) {
       return true;
     } else if (ords.length == 1) {
@@ -106,21 +107,20 @@
   public void setNextReader(AtomicReaderContext context) throws IOException {
     groupFieldTermIndex = FieldCache.DEFAULT.getTermsIndex(context.reader(), groupField);
     countFieldTermIndex = FieldCache.DEFAULT.getTermsIndex(context.reader(), countField);
-
     ordSet.clear();
     for (GroupCount group : groups) {
-      int groupOrd = group.groupValue == null ? 0 : groupFieldTermIndex.binarySearchLookup(group.groupValue, spare);
-      if (groupOrd < 0) {
+      int groupOrd = group.groupValue == null ? -1 : groupFieldTermIndex.lookupTerm(group.groupValue);
+      if (group.groupValue != null && groupOrd < 0) {
         continue;
       }
 
       groupCounts[ordSet.put(groupOrd)] = group;
       group.ords = new int[group.uniqueValues.size()];
-      Arrays.fill(group.ords, -1);
+      Arrays.fill(group.ords, -2);
       int i = 0;
       for (BytesRef value : group.uniqueValues) {
-        int countOrd = value == null ? 0 : countFieldTermIndex.binarySearchLookup(value, new BytesRef());
-        if (countOrd >= 0) {
+        int countOrd = value == null ? -1 : countFieldTermIndex.lookupTerm(value);
+        if (value == null || countOrd >= 0) {
           group.ords[i++] = countOrd;
         }
       }
Index: lucene/grouping/src/java/org/apache/lucene/search/grouping/term/TermAllGroupsCollector.java
===================================================================
--- lucene/grouping/src/java/org/apache/lucene/search/grouping/term/TermAllGroupsCollector.java	(revision 1442822)
+++ lucene/grouping/src/java/org/apache/lucene/search/grouping/term/TermAllGroupsCollector.java	(working copy)
@@ -17,18 +17,18 @@
  * limitations under the License.
  */
 
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Collection;
+import java.util.List;
+
 import org.apache.lucene.index.AtomicReaderContext;
-import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.SortedDocValues;
 import org.apache.lucene.search.FieldCache;
 import org.apache.lucene.search.grouping.AbstractAllGroupsCollector;
+import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.SentinelIntSet;
-import org.apache.lucene.util.BytesRef;
 
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Collection;
-import java.util.List;
-
 /**
  * A collector that collects all groups that match the
  * query. Only the group value is collected, and the order
@@ -52,8 +52,7 @@
   private final SentinelIntSet ordSet;
   private final List<BytesRef> groups;
 
-  private FieldCache.DocTermsIndex index;
-  private final BytesRef spareBytesRef = new BytesRef();
+  private SortedDocValues index;
 
   /**
    * Expert: Constructs a {@link AbstractAllGroupsCollector}
@@ -66,7 +65,7 @@
    *                    heap usage is 4 bytes * initialSize.
    */
   public TermAllGroupsCollector(String groupField, int initialSize) {
-    ordSet = new SentinelIntSet(initialSize, -1);
+    ordSet = new SentinelIntSet(initialSize, -2);
     groups = new ArrayList<BytesRef>(initialSize);
     this.groupField = groupField;
   }
@@ -87,7 +86,13 @@
     int key = index.getOrd(doc);
     if (!ordSet.exists(key)) {
       ordSet.put(key);
-      BytesRef term = key == 0 ? null : index.lookup(key, new BytesRef());
+      BytesRef term;
+      if (key == -1) {
+        term = null;
+      } else {
+        term =  new BytesRef();
+        index.lookupOrd(key, term);
+      }
       groups.add(term);
     }
   }
@@ -104,11 +109,14 @@
     // Clear ordSet and fill it with previous encountered groups that can occur in the current segment.
     ordSet.clear();
     for (BytesRef countedGroup : groups) {
-      int ord = index.binarySearchLookup(countedGroup, spareBytesRef);
-      if (ord >= 0) {
-        ordSet.put(ord);
+      if (countedGroup == null) {
+        ordSet.put(-1);
+      } else {
+        int ord = index.lookupTerm(countedGroup);
+        if (ord >= 0) {
+          ordSet.put(ord);
+        }
       }
     }
   }
-
 }
Index: lucene/grouping/src/java/org/apache/lucene/search/grouping/term/TermFirstPassGroupingCollector.java
===================================================================
--- lucene/grouping/src/java/org/apache/lucene/search/grouping/term/TermFirstPassGroupingCollector.java	(revision 1442822)
+++ lucene/grouping/src/java/org/apache/lucene/search/grouping/term/TermFirstPassGroupingCollector.java	(working copy)
@@ -17,17 +17,18 @@
  * limitations under the License.
  */
 
+import java.io.IOException;
+
 import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.SortedDocValues;
 import org.apache.lucene.search.FieldCache;
 import org.apache.lucene.search.Sort;
 import org.apache.lucene.search.grouping.AbstractFirstPassGroupingCollector;
 import org.apache.lucene.util.BytesRef;
 
-import java.io.IOException;
-
 /**
  * Concrete implementation of {@link org.apache.lucene.search.grouping.AbstractFirstPassGroupingCollector} that groups based on
- * field values and more specifically uses {@link org.apache.lucene.search.FieldCache.DocTermsIndex}
+ * field values and more specifically uses {@link org.apache.lucene.index.SortedDocValues}
  * to collect groups.
  *
  * @lucene.experimental
@@ -35,7 +36,7 @@
 public class TermFirstPassGroupingCollector extends AbstractFirstPassGroupingCollector<BytesRef> {
 
   private final BytesRef scratchBytesRef = new BytesRef();
-  private FieldCache.DocTermsIndex index;
+  private SortedDocValues index;
 
   private String groupField;
 
@@ -63,7 +64,12 @@
   @Override
   protected BytesRef getDocGroupValue(int doc) {
     final int ord = index.getOrd(doc);
-    return ord == 0 ? null : index.lookup(ord, scratchBytesRef);
+    if (ord == -1) {
+      return null;
+    } else {
+      index.lookupOrd(ord, scratchBytesRef);
+      return scratchBytesRef;
+    }
   }
 
   @Override
Index: lucene/grouping/src/java/org/apache/lucene/search/grouping/term/TermAllGroupHeadsCollector.java
===================================================================
--- lucene/grouping/src/java/org/apache/lucene/search/grouping/term/TermAllGroupHeadsCollector.java	(revision 1442822)
+++ lucene/grouping/src/java/org/apache/lucene/search/grouping/term/TermAllGroupHeadsCollector.java	(working copy)
@@ -17,19 +17,20 @@
  * limitations under the License.
  */
 
+import java.io.IOException;
+import java.util.*;
+
 import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.SortedDocValues;
 import org.apache.lucene.search.*;
 import org.apache.lucene.search.grouping.AbstractAllGroupHeadsCollector;
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.SentinelIntSet;
 
-import java.io.IOException;
-import java.util.*;
-
 /**
  * A base implementation of {@link org.apache.lucene.search.grouping.AbstractAllGroupHeadsCollector} for retrieving the most relevant groups when grouping
  * on a string based group field. More specifically this all concrete implementations of this base implementation
- * use {@link org.apache.lucene.search.FieldCache.DocTermsIndex}.
+ * use {@link org.apache.lucene.index.SortedDocValues}.
  *
  * @lucene.experimental
  */
@@ -40,7 +41,7 @@
   final String groupField;
   final BytesRef scratchBytesRef = new BytesRef();
 
-  FieldCache.DocTermsIndex groupIndex;
+  SortedDocValues groupIndex;
   AtomicReaderContext readerContext;
 
   protected TermAllGroupHeadsCollector(String groupField, int numberOfSorts) {
@@ -125,7 +126,13 @@
     @Override
     protected void retrieveGroupHeadAndAddIfNotExist(int doc) throws IOException {
       final int ord = groupIndex.getOrd(doc);
-      final BytesRef groupValue = ord == 0 ? null : groupIndex.lookup(ord, scratchBytesRef);
+      final BytesRef groupValue;
+      if (ord == -1) {
+        groupValue = null;
+      } else {
+        groupIndex.lookupOrd(ord, scratchBytesRef);
+        groupValue = scratchBytesRef;
+      }
       GroupHead groupHead = groups.get(groupValue);
       if (groupHead == null) {
         groupHead = new GroupHead(groupValue, sortWithinGroup, doc);
@@ -205,18 +212,18 @@
     private final List<GroupHead> collectedGroups;
     private final SortField[] fields;
 
-    private FieldCache.DocTermsIndex[] sortsIndex;
+    private SortedDocValues[] sortsIndex;
     private Scorer scorer;
     private GroupHead[] segmentGroupHeads;
 
     OrdScoreAllGroupHeadsCollector(String groupField, Sort sortWithinGroup, int initialSize) {
       super(groupField, sortWithinGroup.getSort().length);
-      ordSet = new SentinelIntSet(initialSize, -1);
+      ordSet = new SentinelIntSet(initialSize, -2);
       collectedGroups = new ArrayList<GroupHead>(initialSize);
 
       final SortField[] sortFields = sortWithinGroup.getSort();
       fields = new SortField[sortFields.length];
-      sortsIndex = new FieldCache.DocTermsIndex[sortFields.length];
+      sortsIndex = new SortedDocValues[sortFields.length];
       for (int i = 0; i < sortFields.length; i++) {
         reversed[i] = sortFields[i].getReverse() ? -1 : 1;
         fields[i] = sortFields[i];
@@ -239,14 +246,20 @@
       GroupHead groupHead;
       if (!ordSet.exists(key)) {
         ordSet.put(key);
-        BytesRef term = key == 0 ? null : groupIndex.getTerm(doc, new BytesRef());
+        BytesRef term;
+        if (key == -1) {
+          term = null;
+        } else {
+          term = new BytesRef();
+          groupIndex.lookupOrd(key, term);
+        }
         groupHead = new GroupHead(doc, term);
         collectedGroups.add(groupHead);
-        segmentGroupHeads[key] = groupHead;
+        segmentGroupHeads[key+1] = groupHead;
         temporalResult.stop = true;
       } else {
         temporalResult.stop = false;
-        groupHead = segmentGroupHeads[key];
+        groupHead = segmentGroupHeads[key+1];
       }
       temporalResult.groupHead = groupHead;
     }
@@ -265,19 +278,29 @@
 
       // Clear ordSet and fill it with previous encountered groups that can occur in the current segment.
       ordSet.clear();
-      segmentGroupHeads = new GroupHead[groupIndex.numOrd()];
+      segmentGroupHeads = new GroupHead[groupIndex.getValueCount()+1];
       for (GroupHead collectedGroup : collectedGroups) {
-        int ord = groupIndex.binarySearchLookup(collectedGroup.groupValue, scratchBytesRef);
-        if (ord >= 0) {
+        int ord;
+        if (collectedGroup.groupValue == null) {
+          ord = -1;
+        } else {
+          ord = groupIndex.lookupTerm(collectedGroup.groupValue);
+        }
+        if (collectedGroup.groupValue == null || ord >= 0) {
           ordSet.put(ord);
-          segmentGroupHeads[ord] = collectedGroup;
+          segmentGroupHeads[ord+1] = collectedGroup;
 
           for (int i = 0; i < sortsIndex.length; i++) {
             if (fields[i].getType() == SortField.Type.SCORE) {
               continue;
             }
-
-            collectedGroup.sortOrds[i] = sortsIndex[i].binarySearchLookup(collectedGroup.sortValues[i], scratchBytesRef);
+            int sortOrd;
+            if (collectedGroup.sortValues[i] == null) {
+              sortOrd = -1;
+            } else {
+              sortOrd = sortsIndex[i].lookupTerm(collectedGroup.sortValues[i]);
+            }
+            collectedGroup.sortOrds[i] = sortOrd;
           }
         }
       }
@@ -298,11 +321,13 @@
           if (fields[i].getType() == SortField.Type.SCORE) {
             scores[i] = scorer.score();
           } else {
-            sortValues[i] = sortsIndex[i].getTerm(doc, new BytesRef());
             sortOrds[i] = sortsIndex[i].getOrd(doc);
+            sortValues[i] = new BytesRef();
+            if (sortOrds[i] != -1) {
+              sortsIndex[i].get(doc, sortValues[i]);
+            }
           }
         }
-
       }
 
       @Override
@@ -318,7 +343,12 @@
         } else {
           if (sortOrds[compIDX] < 0) {
             // The current segment doesn't contain the sort value we encountered before. Therefore the ord is negative.
-            return sortValues[compIDX].compareTo(sortsIndex[compIDX].getTerm(doc, scratchBytesRef));
+            if (sortsIndex[compIDX].getOrd(doc) == -1) {
+              scratchBytesRef.length = 0;
+            } else {
+              sortsIndex[compIDX].get(doc, scratchBytesRef);
+            }
+            return sortValues[compIDX].compareTo(scratchBytesRef);
           } else {
             return sortOrds[compIDX] - sortsIndex[compIDX].getOrd(doc);
           }
@@ -331,15 +361,17 @@
           if (fields[i].getType() == SortField.Type.SCORE) {
             scores[i] = scorer.score();
           } else {
-            sortValues[i] = sortsIndex[i].getTerm(doc, sortValues[i]);
             sortOrds[i] = sortsIndex[i].getOrd(doc);
+            if (sortOrds[i] == -1) {
+              sortValues[i].length = 0;
+            } else {
+              sortsIndex[i].get(doc, sortValues[i]);
+            }
           }
         }
         this.doc = doc + readerContext.docBase;
       }
-
     }
-
   }
 
 
@@ -350,17 +382,17 @@
     private final List<GroupHead> collectedGroups;
     private final SortField[] fields;
 
-    private FieldCache.DocTermsIndex[] sortsIndex;
+    private SortedDocValues[] sortsIndex;
     private GroupHead[] segmentGroupHeads;
 
     OrdAllGroupHeadsCollector(String groupField, Sort sortWithinGroup, int initialSize) {
       super(groupField, sortWithinGroup.getSort().length);
-      ordSet = new SentinelIntSet(initialSize, -1);
+      ordSet = new SentinelIntSet(initialSize, -2);
       collectedGroups = new ArrayList<GroupHead>(initialSize);
 
       final SortField[] sortFields = sortWithinGroup.getSort();
       fields = new SortField[sortFields.length];
-      sortsIndex = new FieldCache.DocTermsIndex[sortFields.length];
+      sortsIndex = new SortedDocValues[sortFields.length];
       for (int i = 0; i < sortFields.length; i++) {
         reversed[i] = sortFields[i].getReverse() ? -1 : 1;
         fields[i] = sortFields[i];
@@ -382,14 +414,20 @@
       GroupHead groupHead;
       if (!ordSet.exists(key)) {
         ordSet.put(key);
-        BytesRef term = key == 0 ? null : groupIndex.getTerm(doc, new BytesRef());
+        BytesRef term;
+        if (key == -1) {
+          term = null;
+        } else {
+          term = new BytesRef();
+          groupIndex.lookupOrd(key, term);
+        }
         groupHead = new GroupHead(doc, term);
         collectedGroups.add(groupHead);
-        segmentGroupHeads[key] = groupHead;
+        segmentGroupHeads[key+1] = groupHead;
         temporalResult.stop = true;
       } else {
         temporalResult.stop = false;
-        groupHead = segmentGroupHeads[key];
+        groupHead = segmentGroupHeads[key+1];
       }
       temporalResult.groupHead = groupHead;
     }
@@ -404,15 +442,26 @@
 
       // Clear ordSet and fill it with previous encountered groups that can occur in the current segment.
       ordSet.clear();
-      segmentGroupHeads = new GroupHead[groupIndex.numOrd()];
+      segmentGroupHeads = new GroupHead[groupIndex.getValueCount()+1];
       for (GroupHead collectedGroup : collectedGroups) {
-        int groupOrd = groupIndex.binarySearchLookup(collectedGroup.groupValue, scratchBytesRef);
-        if (groupOrd >= 0) {
+        int groupOrd;
+        if (collectedGroup.groupValue == null) {
+          groupOrd = -1;
+        } else {
+          groupOrd = groupIndex.lookupTerm(collectedGroup.groupValue);
+        }
+        if (collectedGroup.groupValue == null || groupOrd >= 0) {
           ordSet.put(groupOrd);
-          segmentGroupHeads[groupOrd] = collectedGroup;
+          segmentGroupHeads[groupOrd+1] = collectedGroup;
 
           for (int i = 0; i < sortsIndex.length; i++) {
-            collectedGroup.sortOrds[i] = sortsIndex[i].binarySearchLookup(collectedGroup.sortValues[i], scratchBytesRef);
+            int sortOrd;
+            if (collectedGroup.sortOrds[i] == -1) {
+              sortOrd = -1;
+            } else {
+              sortOrd = sortsIndex[i].lookupTerm(collectedGroup.sortValues[i]);
+            }
+            collectedGroup.sortOrds[i] = sortOrd;
           }
         }
       }
@@ -428,8 +477,11 @@
         sortValues = new BytesRef[sortsIndex.length];
         sortOrds = new int[sortsIndex.length];
         for (int i = 0; i < sortsIndex.length; i++) {
-          sortValues[i] = sortsIndex[i].getTerm(doc, new BytesRef());
           sortOrds[i] = sortsIndex[i].getOrd(doc);
+          sortValues[i] = new BytesRef();
+          if (sortOrds[i] != -1) {
+            sortsIndex[i].get(doc, sortValues[i]);
+          }
         }
       }
 
@@ -437,7 +489,12 @@
       public int compare(int compIDX, int doc) throws IOException {
         if (sortOrds[compIDX] < 0) {
           // The current segment doesn't contain the sort value we encountered before. Therefore the ord is negative.
-          return sortValues[compIDX].compareTo(sortsIndex[compIDX].getTerm(doc, scratchBytesRef));
+          if (sortsIndex[compIDX].getOrd(doc) == -1) {
+            scratchBytesRef.length = 0;
+          } else {
+            sortsIndex[compIDX].get(doc, scratchBytesRef);
+          }
+          return sortValues[compIDX].compareTo(scratchBytesRef);
         } else {
           return sortOrds[compIDX] - sortsIndex[compIDX].getOrd(doc);
         }
@@ -446,8 +503,12 @@
       @Override
       public void updateDocHead(int doc) throws IOException {
         for (int i = 0; i < sortsIndex.length; i++) {
-          sortValues[i] = sortsIndex[i].getTerm(doc, sortValues[i]);
           sortOrds[i] = sortsIndex[i].getOrd(doc);
+          if (sortOrds[i] == -1) {
+            sortValues[i].length = 0;
+          } else {
+            sortsIndex[i].lookupOrd(sortOrds[i], sortValues[i]);
+          }
         }
         this.doc = doc + readerContext.docBase;
       }
@@ -469,7 +530,7 @@
 
     ScoreAllGroupHeadsCollector(String groupField, Sort sortWithinGroup, int initialSize) {
       super(groupField, sortWithinGroup.getSort().length);
-      ordSet = new SentinelIntSet(initialSize, -1);
+      ordSet = new SentinelIntSet(initialSize, -2);
       collectedGroups = new ArrayList<GroupHead>(initialSize);
 
       final SortField[] sortFields = sortWithinGroup.getSort();
@@ -496,14 +557,20 @@
       GroupHead groupHead;
       if (!ordSet.exists(key)) {
         ordSet.put(key);
-        BytesRef term = key == 0 ? null : groupIndex.getTerm(doc, new BytesRef());
+        BytesRef term;
+        if (key == -1) {
+          term = null;
+        } else {
+          term = new BytesRef();
+          groupIndex.lookupOrd(key, term);
+        }
         groupHead = new GroupHead(doc, term);
         collectedGroups.add(groupHead);
-        segmentGroupHeads[key] = groupHead;
+        segmentGroupHeads[key+1] = groupHead;
         temporalResult.stop = true;
       } else {
         temporalResult.stop = false;
-        groupHead = segmentGroupHeads[key];
+        groupHead = segmentGroupHeads[key+1];
       }
       temporalResult.groupHead = groupHead;
     }
@@ -515,12 +582,17 @@
 
       // Clear ordSet and fill it with previous encountered groups that can occur in the current segment.
       ordSet.clear();
-      segmentGroupHeads = new GroupHead[groupIndex.numOrd()];
+      segmentGroupHeads = new GroupHead[groupIndex.getValueCount()+1];
       for (GroupHead collectedGroup : collectedGroups) {
-        int ord = groupIndex.binarySearchLookup(collectedGroup.groupValue, scratchBytesRef);
-        if (ord >= 0) {
+        int ord;
+        if (collectedGroup.groupValue == null) {
+          ord = -1;
+        } else {
+          ord = groupIndex.lookupTerm(collectedGroup.groupValue);
+        }
+        if (collectedGroup.groupValue == null || ord >= 0) {
           ordSet.put(ord);
-          segmentGroupHeads[ord] = collectedGroup;
+          segmentGroupHeads[ord+1] = collectedGroup;
         }
       }
     }
@@ -561,5 +633,4 @@
     }
 
   }
-
 }
Index: lucene/grouping/src/java/org/apache/lucene/search/grouping/term/TermSecondPassGroupingCollector.java
===================================================================
--- lucene/grouping/src/java/org/apache/lucene/search/grouping/term/TermSecondPassGroupingCollector.java	(revision 1442822)
+++ lucene/grouping/src/java/org/apache/lucene/search/grouping/term/TermSecondPassGroupingCollector.java	(working copy)
@@ -17,20 +17,21 @@
  * limitations under the License.
  */
 
+import java.io.IOException;
+import java.util.Collection;
+
 import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.SortedDocValues;
 import org.apache.lucene.search.FieldCache;
 import org.apache.lucene.search.Sort;
 import org.apache.lucene.search.grouping.AbstractSecondPassGroupingCollector;
 import org.apache.lucene.search.grouping.SearchGroup;
+import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.SentinelIntSet;
-import org.apache.lucene.util.BytesRef;
 
-import java.io.IOException;
-import java.util.Collection;
-
 /**
  * Concrete implementation of {@link org.apache.lucene.search.grouping.AbstractSecondPassGroupingCollector} that groups based on
- * field values and more specifically uses {@link org.apache.lucene.search.FieldCache.DocTermsIndex}
+ * field values and more specifically uses {@link org.apache.lucene.index.SortedDocValues}
  * to collect grouped docs.
  *
  * @lucene.experimental
@@ -38,8 +39,7 @@
 public class TermSecondPassGroupingCollector extends AbstractSecondPassGroupingCollector<BytesRef> {
 
   private final SentinelIntSet ordSet;
-  private FieldCache.DocTermsIndex index;
-  private final BytesRef spareBytesRef = new BytesRef();
+  private SortedDocValues index;
   private final String groupField;
 
   @SuppressWarnings({"unchecked", "rawtypes"})
@@ -47,7 +47,7 @@
                                          int maxDocsPerGroup, boolean getScores, boolean getMaxScores, boolean fillSortFields)
       throws IOException {
     super(groups, groupSort, withinGroupSort, maxDocsPerGroup, getScores, getMaxScores, fillSortFields);
-    ordSet = new SentinelIntSet(groupMap.size(), -1);
+    ordSet = new SentinelIntSet(groupMap.size(), -2);
     this.groupField = groupField;
     groupDocs = (SearchGroupDocs<BytesRef>[]) new SearchGroupDocs[ordSet.keys.length];
   }
@@ -61,8 +61,8 @@
     ordSet.clear();
     for (SearchGroupDocs<BytesRef> group : groupMap.values()) {
 //      System.out.println("  group=" + (group.groupValue == null ? "null" : group.groupValue.utf8ToString()));
-      int ord = group.groupValue == null ? 0 : index.binarySearchLookup(group.groupValue, spareBytesRef);
-      if (ord >= 0) {
+      int ord = group.groupValue == null ? -1 : index.lookupTerm(group.groupValue);
+      if (group.groupValue == null || ord >= 0) {
         groupDocs[ordSet.put(ord)] = group;
       }
     }
Index: lucene/grouping/src/java/org/apache/lucene/search/grouping/dv/DVAllGroupsCollector.java
===================================================================
--- lucene/grouping/src/java/org/apache/lucene/search/grouping/dv/DVAllGroupsCollector.java	(revision 1442822)
+++ lucene/grouping/src/java/org/apache/lucene/search/grouping/dv/DVAllGroupsCollector.java	(working copy)
@@ -1,272 +0,0 @@
-package org.apache.lucene.search.grouping.dv;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.index.AtomicReaderContext;
-import org.apache.lucene.index.DocValues;
-import org.apache.lucene.index.DocValues.Type; // javadocs
-import org.apache.lucene.search.grouping.AbstractAllGroupsCollector;
-import org.apache.lucene.util.SentinelIntSet;
-import org.apache.lucene.util.BytesRef;
-
-import java.io.IOException;
-import java.util.*;
-
-/**
- * Implementation of {@link AbstractAllGroupsCollector} that groups documents based on
- * {@link DocValues} fields.
- *
- * @lucene.experimental
- */
-public abstract class DVAllGroupsCollector<GROUP_VALUE_TYPE> extends AbstractAllGroupsCollector<GROUP_VALUE_TYPE> {
-
-  private static final int DEFAULT_INITIAL_SIZE = 128;
-
-  /**
-   * Expert: Constructs a {@link DVAllGroupsCollector}.
-   * Selects and constructs the most optimal all groups collector implementation for grouping by {@link DocValues}.
-   * 
-   *
-   * @param groupField  The field to group by
-   * @param type The {@link Type} which is used to select a concrete implementation.
-   * @param diskResident Whether the values to group by should be disk resident
-   * @param initialSize The initial allocation size of the
-   *                    internal int set and group list
-   *                    which should roughly match the total
-   *                    number of expected unique groups. Be aware that the
-   *                    heap usage is 4 bytes * initialSize. Not all concrete implementions use this!
-   * @return the most optimal all groups collector implementation for grouping by {@link DocValues}
-   */
-  @SuppressWarnings("unchecked")
-  public static <T> DVAllGroupsCollector<T> create(String groupField, DocValues.Type type, boolean diskResident, int initialSize) {
-    switch (type) {
-      case VAR_INTS:
-      case FIXED_INTS_8:
-      case FIXED_INTS_16:
-      case FIXED_INTS_32:
-      case FIXED_INTS_64:
-        // Type erasure b/c otherwise we have inconvertible types...
-        return (DVAllGroupsCollector) new Lng(groupField, type, diskResident);
-      case FLOAT_32:
-      case FLOAT_64:
-        // Type erasure b/c otherwise we have inconvertible types...
-        return (DVAllGroupsCollector) new Dbl(groupField, type, diskResident);
-      case BYTES_FIXED_STRAIGHT:
-      case BYTES_FIXED_DEREF:
-      case BYTES_VAR_STRAIGHT:
-      case BYTES_VAR_DEREF:
-        // Type erasure b/c otherwise we have inconvertible types...
-        return (DVAllGroupsCollector) new BR(groupField, type, diskResident);
-      case BYTES_VAR_SORTED:
-      case BYTES_FIXED_SORTED:
-        // Type erasure b/c otherwise we have inconvertible types...
-        return (DVAllGroupsCollector) new SortedBR(groupField, type, diskResident, initialSize);
-      default:
-        throw new IllegalArgumentException(String.format(Locale.ROOT, "ValueType %s not supported", type));
-    }
-  }
-
-  /**
-   * Constructs a {@link DVAllGroupsCollector}.
-   * Selects and constructs the most optimal all groups collector implementation for grouping by {@link DocValues}.
-   * If implementations require an initial allocation size then this will be set to 128.
-   *
-   *
-   * @param groupField  The field to group by
-   * @param type The {@link Type} which is used to select a concrete implementation.
-   * @param diskResident Wether the values to group by should be disk resident
-   * @return the most optimal all groups collector implementation for grouping by {@link DocValues}
-   */
-  public static <T> DVAllGroupsCollector<T> create(String groupField, DocValues.Type type, boolean diskResident) {
-    return create(groupField, type, diskResident, DEFAULT_INITIAL_SIZE);
-  }
-
-  final String groupField;
-  final DocValues.Type valueType;
-  final boolean diskResident;
-  final Collection<GROUP_VALUE_TYPE> groups;
-
-  DVAllGroupsCollector(String groupField, DocValues.Type valueType, boolean diskResident, Collection<GROUP_VALUE_TYPE> groups) {
-    this.groupField = groupField;
-    this.valueType = valueType;
-    this.diskResident = diskResident;
-    this.groups = groups;
-  }
-
-  @Override
-  public void setNextReader(AtomicReaderContext readerContext) throws IOException {
-    final DocValues dv = readerContext.reader().docValues(groupField);
-    final DocValues.Source dvSource;
-    if (dv != null) {
-      dvSource = diskResident ? dv.getDirectSource() : dv.getSource();
-    } else {
-      dvSource = getDefaultSource(readerContext);
-    }
-    setDocValuesSources(dvSource, readerContext);
-  }
-
-  /**
-   * Sets the idv source for concrete implementations to use.
-   *
-   * @param source The idv source to be used by concrete implementations
-   * @param readerContext The current reader context
-   */
-  protected abstract void setDocValuesSources(DocValues.Source source, AtomicReaderContext readerContext);
-
-  /**
-   * @return The default source when no doc values are available.
-   * @param readerContext The current reader context
-   */
-  protected DocValues.Source getDefaultSource(AtomicReaderContext readerContext) {
-    return DocValues.getDefaultSource(valueType);
-  }
-
-  static class Lng extends DVAllGroupsCollector<Long> {
-
-    private DocValues.Source source;
-
-    Lng(String groupField, DocValues.Type valueType, boolean diskResident) {
-      super(groupField, valueType, diskResident, new TreeSet<Long>());
-    }
-
-    @Override
-    public void collect(int doc) throws IOException {
-      long value = source.getInt(doc);
-      if (!groups.contains(value)) {
-        groups.add(value);
-      }
-    }
-
-    @Override
-    public Collection<Long> getGroups() {
-      return groups;
-    }
-
-    @Override
-    protected void setDocValuesSources(DocValues.Source source, AtomicReaderContext readerContext) {
-      this.source = source;
-    }
-
-  }
-
-  static class Dbl extends DVAllGroupsCollector<Double> {
-
-    private DocValues.Source source;
-
-    Dbl(String groupField, DocValues.Type valueType, boolean diskResident) {
-      super(groupField, valueType, diskResident, new TreeSet<Double>());
-    }
-
-    @Override
-    public void collect(int doc) throws IOException {
-      double value = source.getFloat(doc);
-      if (!groups.contains(value)) {
-        groups.add(value);
-      }
-    }
-
-    @Override
-    public Collection<Double> getGroups() {
-      return groups;
-    }
-
-    @Override
-    protected void setDocValuesSources(DocValues.Source source, AtomicReaderContext readerContext) {
-      this.source = source;
-    }
-
-  }
-
-  static class BR extends DVAllGroupsCollector<BytesRef> {
-
-    private final BytesRef spare = new BytesRef();
-
-    private DocValues.Source source;
-
-    BR(String groupField, DocValues.Type valueType, boolean diskResident) {
-      super(groupField, valueType, diskResident, new TreeSet<BytesRef>());
-    }
-
-    @Override
-    public void collect(int doc) throws IOException {
-      BytesRef value = source.getBytes(doc, spare);
-      if (!groups.contains(value)) {
-        groups.add(BytesRef.deepCopyOf(value));
-      }
-    }
-
-    @Override
-    public Collection<BytesRef> getGroups() {
-      return groups;
-    }
-
-    @Override
-    protected void setDocValuesSources(DocValues.Source source, AtomicReaderContext readerContext) {
-      this.source = source;
-    }
-
-  }
-
-  static class SortedBR extends DVAllGroupsCollector<BytesRef> {
-
-    private final SentinelIntSet ordSet;
-    private final BytesRef spare = new BytesRef();
-
-    private DocValues.SortedSource source;
-
-    SortedBR(String groupField, DocValues.Type valueType, boolean diskResident, int initialSize) {
-      super(groupField, valueType, diskResident, new ArrayList<BytesRef>(initialSize));
-      ordSet = new SentinelIntSet(initialSize, -1);
-    }
-
-    @Override
-    public void collect(int doc) throws IOException {
-      int ord = source.ord(doc);
-      if (!ordSet.exists(ord)) {
-        ordSet.put(ord);
-        BytesRef value = source.getBytes(doc, new BytesRef());
-        groups.add(value);
-      }
-    }
-
-    @Override
-    public Collection<BytesRef> getGroups() {
-      return groups;
-    }
-
-    @Override
-    protected void setDocValuesSources(DocValues.Source source, AtomicReaderContext readerContext) {
-      this.source = source.asSortedSource();
-
-      ordSet.clear();
-      for (BytesRef countedGroup : groups) {
-        int ord = this.source.getOrdByValue(countedGroup, spare);
-        if (ord >= 0) {
-          ordSet.put(ord);
-        }
-      }
-    }
-
-    @Override
-    protected DocValues.Source getDefaultSource(AtomicReaderContext readerContext) {
-      return DocValues.getDefaultSortedSource(valueType, readerContext.reader().maxDoc());
-    }
-
-  }
-
-}
Index: lucene/grouping/src/java/org/apache/lucene/search/grouping/dv/DVFirstPassGroupingCollector.java
===================================================================
--- lucene/grouping/src/java/org/apache/lucene/search/grouping/dv/DVFirstPassGroupingCollector.java	(revision 1442822)
+++ lucene/grouping/src/java/org/apache/lucene/search/grouping/dv/DVFirstPassGroupingCollector.java	(working copy)
@@ -1,231 +0,0 @@
-package org.apache.lucene.search.grouping.dv;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.index.AtomicReaderContext;
-import org.apache.lucene.index.DocValues;
-import org.apache.lucene.index.DocValues.Type; // javadocs
-import org.apache.lucene.search.Sort;
-import org.apache.lucene.search.grouping.AbstractFirstPassGroupingCollector;
-import org.apache.lucene.util.BytesRef;
-
-import java.io.IOException;
-import java.util.Locale;
-
-/**
- * IDV based Implementations of {@link AbstractFirstPassGroupingCollector}.
- *
- * @lucene.experimental 
- */
-public abstract class DVFirstPassGroupingCollector<GROUP_VALUE_TYPE> extends AbstractFirstPassGroupingCollector<GROUP_VALUE_TYPE> {
-
-  final String groupField;
-  final boolean diskResident;
-  final DocValues.Type valueType;
-
-  /**
-   * Constructs a {@link DVFirstPassGroupingCollector}.
-   * Selects and constructs the most optimal first pass collector implementation for grouping by {@link DocValues}.
-   *
-   * @param groupField      The field to group by
-   * @param topNGroups      The maximum top number of groups to return. Typically this equals to offset + rows.
-   * @param diskResident    Whether the values to group by should be disk resident
-   * @param type            The {@link Type} which is used to select a concrete implementation.
-   * @param groupSort       The sort used for the groups
-   * @return the most optimal first pass collector implementation for grouping by {@link DocValues}
-   * @throws IOException    If I/O related errors occur
-   */
-  @SuppressWarnings("unchecked")
-  public static <T> DVFirstPassGroupingCollector<T> create(Sort groupSort, int topNGroups, String groupField, DocValues.Type type, boolean diskResident) throws IOException {
-    switch (type) {
-      case VAR_INTS:
-      case FIXED_INTS_8:
-      case FIXED_INTS_16:
-      case FIXED_INTS_32:
-      case FIXED_INTS_64:
-        // Type erasure b/c otherwise we have inconvertible types...
-        return (DVFirstPassGroupingCollector) new Lng(groupSort, topNGroups, groupField, diskResident, type);
-      case FLOAT_32:
-      case FLOAT_64:
-        // Type erasure b/c otherwise we have inconvertible types...
-        return (DVFirstPassGroupingCollector) new Dbl(groupSort, topNGroups, groupField, diskResident, type);
-      case BYTES_FIXED_STRAIGHT:
-      case BYTES_FIXED_DEREF:
-      case BYTES_VAR_STRAIGHT:
-      case BYTES_VAR_DEREF:
-        // Type erasure b/c otherwise we have inconvertible types...
-        return (DVFirstPassGroupingCollector) new BR(groupSort, topNGroups, groupField, diskResident, type);
-      case BYTES_VAR_SORTED:
-      case BYTES_FIXED_SORTED:
-        // Type erasure b/c otherwise we have inconvertible types...
-        return (DVFirstPassGroupingCollector) new SortedBR(groupSort, topNGroups, groupField, diskResident, type);
-      default:
-        throw new IllegalArgumentException(String.format(Locale.ROOT, "ValueType %s not supported", type));
-    }
-  }
-
-  DVFirstPassGroupingCollector(Sort groupSort, int topNGroups, String groupField, boolean diskResident, DocValues.Type valueType) throws IOException {
-    super(groupSort, topNGroups);
-    this.groupField = groupField;
-    this.diskResident = diskResident;
-    this.valueType = valueType;
-  }
-
-  @Override
-  public void setNextReader(AtomicReaderContext readerContext) throws IOException {
-    super.setNextReader(readerContext);
-
-    final DocValues dv = readerContext.reader().docValues(groupField);
-    final DocValues.Source dvSource;
-    if (dv != null) {
-      dvSource = diskResident ? dv.getDirectSource() : dv.getSource();
-    } else {
-      dvSource = getDefaultSource(readerContext);
-    }
-    setDocValuesSources(dvSource);
-  }
-
-  /**
-   * Sets the idv source for concrete implementations to use.
-   *
-   * @param source The idv source to be used by concrete implementations
-   */
-  protected abstract void setDocValuesSources(DocValues.Source source);
-
-  /**
-   * @return The default source when no doc values are available.
-   * @param readerContext The current reader context
-   */
-  protected DocValues.Source getDefaultSource(AtomicReaderContext readerContext) {
-    return DocValues.getDefaultSource(valueType);
-  }
-
-  static class Lng extends DVFirstPassGroupingCollector<Long> {
-
-    private DocValues.Source source;
-
-    Lng(Sort groupSort, int topNGroups, String groupField, boolean diskResident, DocValues.Type type) throws IOException {
-      super(groupSort, topNGroups, groupField, diskResident, type);
-    }
-
-    @Override
-    protected Long getDocGroupValue(int doc) {
-      return source.getInt(doc);
-    }
-
-    @Override
-    protected Long copyDocGroupValue(Long groupValue, Long reuse) {
-      return groupValue;
-    }
-
-    @Override
-    protected void setDocValuesSources(DocValues.Source source) {
-      this.source = source;
-    }
-  }
-
-  static class Dbl extends DVFirstPassGroupingCollector<Double> {
-
-    private DocValues.Source source;
-
-    Dbl(Sort groupSort, int topNGroups, String groupField, boolean diskResident, DocValues.Type type) throws IOException {
-      super(groupSort, topNGroups, groupField, diskResident, type);
-    }
-
-    @Override
-    protected Double getDocGroupValue(int doc) {
-      return source.getFloat(doc);
-    }
-
-    @Override
-    protected Double copyDocGroupValue(Double groupValue, Double reuse) {
-      return groupValue;
-    }
-
-    @Override
-    protected void setDocValuesSources(DocValues.Source source) {
-      this.source = source;
-    }
-  }
-
-  static class BR extends DVFirstPassGroupingCollector<BytesRef> {
-
-    private DocValues.Source source;
-    private final BytesRef spare = new BytesRef();
-
-    BR(Sort groupSort, int topNGroups, String groupField, boolean diskResident, DocValues.Type type) throws IOException {
-      super(groupSort, topNGroups, groupField, diskResident, type);
-    }
-
-    @Override
-    protected BytesRef getDocGroupValue(int doc) {
-      return source.getBytes(doc, spare);
-    }
-
-    @Override
-    protected BytesRef copyDocGroupValue(BytesRef groupValue, BytesRef reuse) {
-      if (reuse != null) {
-        reuse.copyBytes(groupValue);
-        return reuse;
-      } else {
-        return BytesRef.deepCopyOf(groupValue);
-      }
-    }
-
-    @Override
-    protected void setDocValuesSources(DocValues.Source source) {
-      this.source = source;
-    }
-  }
-
-  static class SortedBR extends DVFirstPassGroupingCollector<BytesRef> {
-
-    private DocValues.SortedSource sortedSource;
-    private final BytesRef spare = new BytesRef();
-
-    SortedBR(Sort groupSort, int topNGroups, String groupField, boolean diskResident, DocValues.Type type) throws IOException {
-      super(groupSort, topNGroups, groupField, diskResident, type);
-    }
-
-    @Override
-    protected BytesRef getDocGroupValue(int doc) {
-      return sortedSource.getBytes(doc, spare);
-    }
-
-    @Override
-    protected BytesRef copyDocGroupValue(BytesRef groupValue, BytesRef reuse) {
-      if (reuse != null) {
-        reuse.copyBytes(groupValue);
-        return reuse;
-      } else {
-        return BytesRef.deepCopyOf(groupValue);
-      }
-    }
-
-    @Override
-    protected void setDocValuesSources(DocValues.Source source) {
-      this.sortedSource = source.asSortedSource();
-    }
-
-    @Override
-    protected DocValues.Source getDefaultSource(AtomicReaderContext readerContext) {
-      return DocValues.getDefaultSortedSource(valueType, readerContext.reader().maxDoc());
-    }
-  }
-
-}
Index: lucene/grouping/src/java/org/apache/lucene/search/grouping/dv/DVAllGroupHeadsCollector.java
===================================================================
--- lucene/grouping/src/java/org/apache/lucene/search/grouping/dv/DVAllGroupHeadsCollector.java	(revision 1442822)
+++ lucene/grouping/src/java/org/apache/lucene/search/grouping/dv/DVAllGroupHeadsCollector.java	(working copy)
@@ -1,328 +0,0 @@
-package org.apache.lucene.search.grouping.dv;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.index.AtomicReaderContext;
-import org.apache.lucene.index.DocValues;
-import org.apache.lucene.index.DocValues.Type;
-import org.apache.lucene.search.FieldComparator;
-import org.apache.lucene.search.Scorer;
-import org.apache.lucene.search.Sort;
-import org.apache.lucene.search.SortField;
-import org.apache.lucene.search.grouping.AbstractAllGroupHeadsCollector;
-import org.apache.lucene.util.BytesRef;
-
-import java.io.IOException;
-import java.util.Collection;
-import java.util.HashMap;
-import java.util.Locale;
-import java.util.Map;
-
-/**
- * A base implementation of {@link org.apache.lucene.search.grouping.AbstractAllGroupHeadsCollector} for retrieving
- * the most relevant groups when grouping on a indexed doc values field.
- *
- * @lucene.experimental
- */
-//TODO - (MvG): Add more optimized implementations
-public abstract class DVAllGroupHeadsCollector<GH extends AbstractAllGroupHeadsCollector.GroupHead<?>> extends AbstractAllGroupHeadsCollector<GH> {
-
-  final String groupField;
-  final boolean diskResident;
-  final DocValues.Type valueType;
-  final BytesRef scratchBytesRef = new BytesRef();
-
-  AtomicReaderContext readerContext;
-  Scorer scorer;
-
-  DVAllGroupHeadsCollector(String groupField, DocValues.Type valueType, int numberOfSorts, boolean diskResident) {
-    super(numberOfSorts);
-    this.groupField = groupField;
-    this.valueType = valueType;
-    this.diskResident = diskResident;
-  }
-
-  /**
-   * Creates an <code>AbstractAllGroupHeadsCollector</code> instance based on the supplied arguments.
-   * This factory method decides with implementation is best suited.
-   *
-   * @param groupField      The field to group by
-   * @param sortWithinGroup The sort within each group
-   * @param type The {@link Type} which is used to select a concrete implementation.
-   * @param diskResident Whether the values to group by should be disk resident
-   * @return an <code>AbstractAllGroupHeadsCollector</code> instance based on the supplied arguments
-   */
-  @SuppressWarnings("unchecked")
-  public static <T extends AbstractAllGroupHeadsCollector.GroupHead<?>> DVAllGroupHeadsCollector<T> create(String groupField, Sort sortWithinGroup, DocValues.Type type, boolean diskResident) {
-    switch (type) {
-      case VAR_INTS:
-      case FIXED_INTS_8:
-      case FIXED_INTS_16:
-      case FIXED_INTS_32:
-      case FIXED_INTS_64:
-        // Type erasure b/c otherwise we have inconvertible types...
-        return (DVAllGroupHeadsCollector) new GeneralAllGroupHeadsCollector.Lng(groupField, type, sortWithinGroup, diskResident);
-      case FLOAT_32:
-      case FLOAT_64:
-        return (DVAllGroupHeadsCollector) new GeneralAllGroupHeadsCollector.Dbl(groupField, type, sortWithinGroup, diskResident);
-      case BYTES_FIXED_STRAIGHT:
-      case BYTES_FIXED_DEREF:
-      case BYTES_VAR_STRAIGHT:
-      case BYTES_VAR_DEREF:
-        return (DVAllGroupHeadsCollector) new GeneralAllGroupHeadsCollector.BR(groupField, type, sortWithinGroup, diskResident);
-      case BYTES_VAR_SORTED:
-      case BYTES_FIXED_SORTED:
-        return (DVAllGroupHeadsCollector) new GeneralAllGroupHeadsCollector.SortedBR(groupField, type, sortWithinGroup, diskResident);
-      default:
-        throw new IllegalArgumentException(String.format(Locale.ROOT, "ValueType %s not supported", type));
-    }
-  }
-
-  static class GroupHead extends AbstractAllGroupHeadsCollector.GroupHead<Comparable<?>> {
-
-    final FieldComparator<?>[] comparators;
-    AtomicReaderContext readerContext;
-    Scorer scorer;
-
-    GroupHead(Comparable<?> groupValue, Sort sort, int doc, AtomicReaderContext readerContext, Scorer scorer) throws IOException {
-      super(groupValue, doc + readerContext.docBase);
-      final SortField[] sortFields = sort.getSort();
-      comparators = new FieldComparator<?>[sortFields.length];
-      for (int i = 0; i < sortFields.length; i++) {
-        comparators[i] = sortFields[i].getComparator(1, i).setNextReader(readerContext);
-        comparators[i].setScorer(scorer);
-        comparators[i].copy(0, doc);
-        comparators[i].setBottom(0);
-      }
-
-      this.readerContext = readerContext;
-      this.scorer = scorer;
-    }
-
-    @Override
-    public int compare(int compIDX, int doc) throws IOException {
-      return comparators[compIDX].compareBottom(doc);
-    }
-
-    @Override
-    public void updateDocHead(int doc) throws IOException {
-      for (FieldComparator<?> comparator : comparators) {
-        comparator.copy(0, doc);
-        comparator.setBottom(0);
-      }
-      this.doc = doc + readerContext.docBase;
-    }
-  }
-
-  @Override
-  public void setNextReader(AtomicReaderContext readerContext) throws IOException {
-    this.readerContext = readerContext;
-
-    final DocValues dv = readerContext.reader().docValues(groupField);
-    final DocValues.Source dvSource;
-    if (dv != null) {
-      dvSource = diskResident ? dv.getDirectSource() : dv.getSource();
-    } else {
-      dvSource = getDefaultSource(readerContext);
-    }
-    setDocValuesSources(dvSource);
-  }
-
-  /**
-   * Sets the idv source for concrete implementations to use.
-   *
-   * @param source The idv source to be used by concrete implementations
-   */
-  protected abstract void setDocValuesSources(DocValues.Source source);
-
-  /**
-   * @return The default source when no doc values are available.
-   * @param readerContext The current reader context
-   */
-  protected DocValues.Source getDefaultSource(AtomicReaderContext readerContext) {
-    return DocValues.getDefaultSource(valueType);
-  }
-
-  // A general impl that works for any group sort.
-  static abstract class GeneralAllGroupHeadsCollector extends DVAllGroupHeadsCollector<DVAllGroupHeadsCollector.GroupHead> {
-
-    private final Sort sortWithinGroup;
-    private final Map<Comparable<?>, GroupHead> groups;
-
-    GeneralAllGroupHeadsCollector(String groupField, DocValues.Type valueType, Sort sortWithinGroup, boolean diskResident) {
-      super(groupField, valueType, sortWithinGroup.getSort().length, diskResident);
-      this.sortWithinGroup = sortWithinGroup;
-      groups = new HashMap<Comparable<?>, GroupHead>();
-
-      final SortField[] sortFields = sortWithinGroup.getSort();
-      for (int i = 0; i < sortFields.length; i++) {
-        reversed[i] = sortFields[i].getReverse() ? -1 : 1;
-      }
-    }
-
-    @Override
-    protected void retrieveGroupHeadAndAddIfNotExist(int doc) throws IOException {
-      final Comparable<?> groupValue = getGroupValue(doc);
-      GroupHead groupHead = groups.get(groupValue);
-      if (groupHead == null) {
-        groupHead = new GroupHead(groupValue, sortWithinGroup, doc, readerContext, scorer);
-        groups.put(groupValue == null ? null : duplicate(groupValue), groupHead);
-        temporalResult.stop = true;
-      } else {
-        temporalResult.stop = false;
-      }
-      temporalResult.groupHead = groupHead;
-    }
-
-    protected abstract Comparable<?> getGroupValue(int doc);
-
-    protected abstract Comparable<?> duplicate(Comparable<?> value);
-
-    @Override
-    protected Collection<GroupHead> getCollectedGroupHeads() {
-      return groups.values();
-    }
-
-    @Override
-    public void setNextReader(AtomicReaderContext context) throws IOException {
-      super.setNextReader(context);
-      for (GroupHead groupHead : groups.values()) {
-        for (int i = 0; i < groupHead.comparators.length; i++) {
-          groupHead.comparators[i] = groupHead.comparators[i].setNextReader(context);
-          groupHead.readerContext = context;
-        }
-      }
-    }
-
-    @Override
-    public void setScorer(Scorer scorer) throws IOException {
-      this.scorer = scorer;
-      for (GroupHead groupHead : groups.values()) {
-        groupHead.scorer = scorer;
-        for (FieldComparator<?> comparator : groupHead.comparators) {
-          comparator.setScorer(scorer);
-        }
-      }
-    }
-
-    static class SortedBR extends GeneralAllGroupHeadsCollector {
-
-      private DocValues.SortedSource source;
-
-      SortedBR(String groupField, DocValues.Type valueType, Sort sortWithinGroup, boolean diskResident) {
-        super(groupField, valueType, sortWithinGroup, diskResident);
-      }
-
-      @Override
-      protected Comparable<?> getGroupValue(int doc) {
-        return source.getBytes(doc, scratchBytesRef);
-      }
-
-      @Override
-      protected Comparable<?> duplicate(Comparable<?> value) {
-        return BytesRef.deepCopyOf((BytesRef) value);
-      }
-
-      @Override
-      protected void setDocValuesSources(DocValues.Source source) {
-        this.source = source.asSortedSource();
-      }
-
-      @Override
-      protected DocValues.Source getDefaultSource(AtomicReaderContext readerContext) {
-        return DocValues.getDefaultSortedSource(valueType, readerContext.reader().maxDoc());
-      }
-    }
-
-    static class BR extends GeneralAllGroupHeadsCollector {
-
-      private DocValues.Source source;
-
-      BR(String groupField, DocValues.Type valueType, Sort sortWithinGroup, boolean diskResident) {
-        super(groupField, valueType, sortWithinGroup, diskResident);
-      }
-
-      @Override
-      protected Comparable<?> getGroupValue(int doc) {
-        return source.getBytes(doc, scratchBytesRef);
-      }
-
-      @Override
-      protected Comparable<?> duplicate(Comparable<?> value) {
-        return BytesRef.deepCopyOf((BytesRef) value);
-      }
-
-      @Override
-      protected void setDocValuesSources(DocValues.Source source) {
-        this.source = source;
-      }
-
-    }
-
-    static class Lng extends GeneralAllGroupHeadsCollector {
-
-      private DocValues.Source source;
-
-      Lng(String groupField, DocValues.Type valueType, Sort sortWithinGroup, boolean diskResident) {
-        super(groupField, valueType, sortWithinGroup, diskResident);
-      }
-
-      @Override
-      protected Comparable<?> getGroupValue(int doc) {
-        return source.getInt(doc);
-      }
-
-      @Override
-      protected Comparable<?> duplicate(Comparable<?> value) {
-        return value;
-      }
-
-      @Override
-      protected void setDocValuesSources(DocValues.Source source) {
-        this.source = source;
-      }
-    }
-
-    static class Dbl extends GeneralAllGroupHeadsCollector {
-
-      private DocValues.Source source;
-
-      Dbl(String groupField, DocValues.Type valueType, Sort sortWithinGroup, boolean diskResident) {
-        super(groupField, valueType, sortWithinGroup, diskResident);
-      }
-
-      @Override
-      protected Comparable<?> getGroupValue(int doc) {
-        return source.getFloat(doc);
-      }
-
-      @Override
-      protected Comparable<?> duplicate(Comparable<?> value) {
-        return value;
-      }
-
-      @Override
-      protected void setDocValuesSources(DocValues.Source source) {
-        this.source = source;
-      }
-
-    }
-
-  }
-
-}
Index: lucene/grouping/src/java/org/apache/lucene/search/grouping/dv/DVSecondPassGroupingCollector.java
===================================================================
--- lucene/grouping/src/java/org/apache/lucene/search/grouping/dv/DVSecondPassGroupingCollector.java	(revision 1442822)
+++ lucene/grouping/src/java/org/apache/lucene/search/grouping/dv/DVSecondPassGroupingCollector.java	(working copy)
@@ -1,237 +0,0 @@
-package org.apache.lucene.search.grouping.dv;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.index.AtomicReaderContext;
-import org.apache.lucene.index.DocValues;
-import org.apache.lucene.index.DocValues.Type;
-import org.apache.lucene.search.Sort;
-import org.apache.lucene.search.grouping.AbstractSecondPassGroupingCollector;
-import org.apache.lucene.search.grouping.SearchGroup;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.SentinelIntSet;
-
-import java.io.IOException;
-import java.util.Collection;
-import java.util.Locale;
-
-/**
- * IDV based implementation of {@link AbstractSecondPassGroupingCollector}.
- *
- * @lucene.experimental
- */
-public abstract class DVSecondPassGroupingCollector<GROUP_VALUE> extends AbstractSecondPassGroupingCollector<GROUP_VALUE> {
-
-  /**
-   * Constructs a {@link DVSecondPassGroupingCollector}.
-   * Selects and constructs the most optimal second pass collector implementation for grouping by {@link DocValues}.
-   *
-   * @param groupField      The field to group by
-   * @param diskResident    Whether the values to group by should be disk resident
-   * @param type            The {@link Type} which is used to select a concrete implementation.
-   * @param searchGroups    The groups from the first phase search
-   * @param groupSort       The sort used for the groups
-   * @param withinGroupSort The sort used for documents inside a group
-   * @param maxDocsPerGroup The maximum number of documents to collect per group
-   * @param getScores       Whether to include scores for the documents inside a group
-   * @param getMaxScores    Whether to keep track of the higest score per group
-   * @param fillSortFields  Whether to include the sort values
-   * @return the most optimal second pass collector implementation for grouping by {@link DocValues}
-   * @throws IOException    If I/O related errors occur
-   */
-  @SuppressWarnings("unchecked")
-  public static <T> DVSecondPassGroupingCollector<T> create(String groupField,
-                                                     boolean diskResident,
-                                                     DocValues.Type type,
-                                                     Collection<SearchGroup<T>> searchGroups,
-                                                     Sort groupSort,
-                                                     Sort withinGroupSort,
-                                                     int maxDocsPerGroup,
-                                                     boolean getScores,
-                                                     boolean getMaxScores,
-                                                     boolean fillSortFields) throws IOException {
-    switch (type) {
-      case VAR_INTS:
-      case FIXED_INTS_8:
-      case FIXED_INTS_16:
-      case FIXED_INTS_32:
-      case FIXED_INTS_64:
-        // Type erasure b/c otherwise we have inconvertible types...
-        return (DVSecondPassGroupingCollector) new Lng(groupField, type, diskResident, (Collection) searchGroups, groupSort, withinGroupSort, maxDocsPerGroup, getScores, getMaxScores, fillSortFields);
-      case FLOAT_32:
-      case FLOAT_64:
-        // Type erasure b/c otherwise we have inconvertible types...
-        return (DVSecondPassGroupingCollector) new Dbl(groupField, type, diskResident, (Collection) searchGroups, groupSort, withinGroupSort, maxDocsPerGroup, getScores, getMaxScores, fillSortFields);
-      case BYTES_FIXED_STRAIGHT:
-      case BYTES_FIXED_DEREF:
-      case BYTES_VAR_STRAIGHT:
-      case BYTES_VAR_DEREF:
-        // Type erasure b/c otherwise we have inconvertible types...
-        return (DVSecondPassGroupingCollector) new BR(groupField, type, diskResident, (Collection) searchGroups, groupSort, withinGroupSort, maxDocsPerGroup, getScores, getMaxScores, fillSortFields);
-      case BYTES_VAR_SORTED:
-      case BYTES_FIXED_SORTED:
-        // Type erasure b/c otherwise we have inconvertible types...
-        return (DVSecondPassGroupingCollector) new SortedBR(groupField, type, diskResident, (Collection) searchGroups, groupSort, withinGroupSort, maxDocsPerGroup, getScores, getMaxScores, fillSortFields);
-      default:
-        throw new IllegalArgumentException(String.format(Locale.ROOT, "ValueType %s not supported", type));
-    }
-  }
-
-  final String groupField;
-  final DocValues.Type valueType;
-  final boolean diskResident;
-
-  DVSecondPassGroupingCollector(String groupField, DocValues.Type valueType, boolean diskResident, Collection<SearchGroup<GROUP_VALUE>> searchGroups, Sort groupSort, Sort withinGroupSort, int maxDocsPerGroup, boolean getScores, boolean getMaxScores, boolean fillSortFields) throws IOException {
-    super(searchGroups, groupSort, withinGroupSort, maxDocsPerGroup, getScores, getMaxScores, fillSortFields);
-    this.groupField = groupField;
-    this.valueType = valueType;
-    this.diskResident = diskResident;
-  }
-
-  @Override
-  public void setNextReader(AtomicReaderContext readerContext) throws IOException {
-    super.setNextReader(readerContext);
-
-    final DocValues dv = readerContext.reader().docValues(groupField);
-    final DocValues.Source dvSource;
-    if (dv != null) {
-      dvSource = diskResident ? dv.getDirectSource() : dv.getSource();
-    } else {
-      dvSource = getDefaultSource(readerContext);
-    }
-    setDocValuesSources(dvSource, readerContext);
-  }
-
-  /**
-   * Sets the idv source for concrete implementations to use.
-   *
-   * @param source The idv source to be used by concrete implementations
-   * @param readerContext The current reader context
-   */
-  protected abstract void setDocValuesSources(DocValues.Source source, AtomicReaderContext readerContext);
-
-  /**
-   * @return The default source when no doc values are available.
-   * @param readerContext The current reader context
-   */
-  protected DocValues.Source getDefaultSource(AtomicReaderContext readerContext) {
-    return DocValues.getDefaultSource(valueType);
-  }
-
-  static class Lng extends DVSecondPassGroupingCollector<Long> {
-
-    private DocValues.Source source;
-
-    Lng(String groupField, DocValues.Type valueType, boolean diskResident, Collection<SearchGroup<Long>> searchGroups, Sort groupSort, Sort withinGroupSort, int maxDocsPerGroup, boolean getScores, boolean getMaxScores, boolean fillSortFields) throws IOException {
-      super(groupField, valueType, diskResident, searchGroups, groupSort, withinGroupSort, maxDocsPerGroup, getScores, getMaxScores, fillSortFields);
-    }
-
-    @Override
-    protected SearchGroupDocs<Long> retrieveGroup(int doc) throws IOException {
-      return groupMap.get(source.getInt(doc));
-    }
-
-    @Override
-    protected void setDocValuesSources(DocValues.Source source, AtomicReaderContext readerContext) {
-      this.source = source;
-    }
-  }
-
-  static class Dbl extends DVSecondPassGroupingCollector<Double> {
-
-    private DocValues.Source source;
-
-    Dbl(String groupField, DocValues.Type valueType, boolean diskResident, Collection<SearchGroup<Double>> searchGroups, Sort groupSort, Sort withinGroupSort, int maxDocsPerGroup, boolean getScores, boolean getMaxScores, boolean fillSortFields) throws IOException {
-      super(groupField, valueType, diskResident, searchGroups, groupSort, withinGroupSort, maxDocsPerGroup, getScores, getMaxScores, fillSortFields);
-    }
-
-    @Override
-    protected SearchGroupDocs<Double> retrieveGroup(int doc) throws IOException {
-      return groupMap.get(source.getFloat(doc));
-    }
-
-    @Override
-    protected void setDocValuesSources(DocValues.Source source, AtomicReaderContext readerContext) {
-      this.source = source;
-    }
-  }
-
-  static class BR extends DVSecondPassGroupingCollector<BytesRef> {
-
-    private DocValues.Source source;
-    private final BytesRef spare = new BytesRef();
-
-    BR(String groupField, DocValues.Type valueType, boolean diskResident, Collection<SearchGroup<BytesRef>> searchGroups, Sort groupSort, Sort withinGroupSort, int maxDocsPerGroup, boolean getScores, boolean getMaxScores, boolean fillSortFields) throws IOException {
-      super(groupField, valueType, diskResident, searchGroups, groupSort, withinGroupSort, maxDocsPerGroup, getScores, getMaxScores, fillSortFields);
-    }
-
-    @Override
-    protected SearchGroupDocs<BytesRef> retrieveGroup(int doc) throws IOException {
-      return groupMap.get(source.getBytes(doc, spare));
-    }
-
-    @Override
-    protected void setDocValuesSources(DocValues.Source source, AtomicReaderContext readerContext) {
-      this.source = source;
-    }
-
-  }
-
-  static class SortedBR extends DVSecondPassGroupingCollector<BytesRef> {
-
-    private DocValues.SortedSource source;
-    private final BytesRef spare = new BytesRef();
-    private final SentinelIntSet ordSet;
-
-    @SuppressWarnings({"unchecked","rawtypes"})
-    SortedBR(String groupField,  DocValues.Type valueType, boolean diskResident, Collection<SearchGroup<BytesRef>> searchGroups, Sort groupSort, Sort withinGroupSort, int maxDocsPerGroup, boolean getScores, boolean getMaxScores, boolean fillSortFields) throws IOException {
-      super(groupField, valueType, diskResident, searchGroups, groupSort, withinGroupSort, maxDocsPerGroup, getScores, getMaxScores, fillSortFields);
-      ordSet = new SentinelIntSet(groupMap.size(), -1);
-      groupDocs = (SearchGroupDocs<BytesRef>[]) new SearchGroupDocs[ordSet.keys.length];
-    }
-
-    @Override
-    protected SearchGroupDocs<BytesRef> retrieveGroup(int doc) throws IOException {
-      int slot = ordSet.find(source.ord(doc));
-      if (slot >= 0) {
-        return groupDocs[slot];
-      }
-
-      return null;
-    }
-
-    @Override
-    protected void setDocValuesSources(DocValues.Source source, AtomicReaderContext readerContext) {
-      this.source = source.asSortedSource();
-
-      ordSet.clear();
-      for (SearchGroupDocs<BytesRef> group : groupMap.values()) {
-        int ord = this.source.getOrdByValue(group.groupValue, spare);
-        if (ord >= 0) {
-          groupDocs[ordSet.put(ord)] = group;
-        }
-      }
-    }
-
-    @Override
-    protected DocValues.Source getDefaultSource(AtomicReaderContext readerContext) {
-      return DocValues.getDefaultSortedSource(valueType, readerContext.reader().maxDoc());
-    }
-  }
-
-}
Index: lucene/grouping/src/java/org/apache/lucene/search/grouping/dv/DVGroupFacetCollector.java
===================================================================
--- lucene/grouping/src/java/org/apache/lucene/search/grouping/dv/DVGroupFacetCollector.java	(revision 1442822)
+++ lucene/grouping/src/java/org/apache/lucene/search/grouping/dv/DVGroupFacetCollector.java	(working copy)
@@ -1,290 +0,0 @@
-package org.apache.lucene.search.grouping.dv;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.index.AtomicReader;
-import org.apache.lucene.index.AtomicReaderContext;
-import org.apache.lucene.index.DocValues;
-import org.apache.lucene.index.DocValues.Type;
-import org.apache.lucene.search.grouping.AbstractGroupFacetCollector;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.SentinelIntSet;
-import org.apache.lucene.util.UnicodeUtil;
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.List;
-import java.util.Locale;
-
-/**
- * An implementation of {@link AbstractGroupFacetCollector} that computes grouped facets based on docvalues.
- *
- * @lucene.experimental
- */
-public abstract class DVGroupFacetCollector extends AbstractGroupFacetCollector {
-
-  final Type groupDvType;
-  final boolean groupDiskResident;
-  final Type facetFieldDvType;
-  final boolean facetDiskResident;
-
-  final List<GroupedFacetHit> groupedFacetHits;
-  final SentinelIntSet segmentGroupedFacetHits;
-
-  /**
-   * Factory method for creating the right implementation based on the group docvalues type and the facet docvalues
-   * type.
-   *
-   * Currently only the {@link Type#BYTES_VAR_SORTED} and the {@link Type#BYTES_FIXED_SORTED} are
-   * the only docvalues type supported for both the group and facet field.
-   *
-   * @param groupField        The group field
-   * @param groupDvType       The docvalues type for the group field
-   * @param groupDiskResident Whether the group docvalues should be disk resident
-   * @param facetField        The facet field
-   * @param facetDvType       The docvalues type for the facet field
-   * @param facetDiskResident Whether the facet docvalues should be disk resident
-   * @param facetPrefix       The facet prefix a facet entry should start with to be included.
-   * @param initialSize       The initial allocation size of the internal int set and group facet list which should roughly
-   *                          match the total number of expected unique groups. Be aware that the heap usage is
-   *                          4 bytes * initialSize.
-   * @return a <code>DVGroupFacetCollector</code> implementation
-   */
-  public static DVGroupFacetCollector createDvGroupFacetCollector(String groupField,
-                                                                  Type groupDvType,
-                                                                  boolean groupDiskResident,
-                                                                  String facetField,
-                                                                  Type facetDvType,
-                                                                  boolean facetDiskResident,
-                                                                  BytesRef facetPrefix,
-                                                                  int initialSize) {
-    switch (groupDvType) {
-      case VAR_INTS:
-      case FIXED_INTS_8:
-      case FIXED_INTS_16:
-      case FIXED_INTS_32:
-      case FIXED_INTS_64:
-      case FLOAT_32:
-      case FLOAT_64:
-      case BYTES_FIXED_STRAIGHT:
-      case BYTES_FIXED_DEREF:
-      case BYTES_VAR_STRAIGHT:
-      case BYTES_VAR_DEREF:
-        throw new IllegalArgumentException(String.format(Locale.ROOT, "Group valueType %s not supported", groupDvType));
-      case BYTES_VAR_SORTED:
-      case BYTES_FIXED_SORTED:
-        return GroupSortedBR.createGroupSortedFacetCollector(groupField, groupDvType, groupDiskResident, facetField, facetDvType, facetDiskResident, facetPrefix, initialSize);
-      default:
-        throw new IllegalArgumentException(String.format(Locale.ROOT, "Group valueType %s not supported", groupDvType));
-    }
-  }
-
-  DVGroupFacetCollector(String groupField, Type groupDvType, boolean groupDiskResident, String facetField, Type facetFieldDvType, boolean facetDiskResident, BytesRef facetPrefix, int initialSize) {
-    super(groupField, facetField, facetPrefix);
-    this.groupDvType = groupDvType;
-    this.groupDiskResident = groupDiskResident;
-    this.facetFieldDvType = facetFieldDvType;
-    this.facetDiskResident = facetDiskResident;
-    groupedFacetHits = new ArrayList<GroupedFacetHit>(initialSize);
-    segmentGroupedFacetHits = new SentinelIntSet(initialSize, -1);
-  }
-
-  static abstract class GroupSortedBR extends DVGroupFacetCollector {
-
-    final BytesRef facetSpare = new BytesRef();
-    final BytesRef groupSpare = new BytesRef();
-    DocValues.SortedSource groupFieldSource;
-
-    GroupSortedBR(String groupField, Type groupDvType, boolean groupDiskResident, String facetField, Type facetFieldDvType, boolean facetDiskResident, BytesRef facetPrefix, int initialSize) {
-      super(groupField, groupDvType, groupDiskResident, facetField, facetFieldDvType, facetDiskResident, facetPrefix, initialSize);
-    }
-
-    static DVGroupFacetCollector createGroupSortedFacetCollector(String groupField,
-                                                                 Type groupDvType,
-                                                                 boolean groupDiskResident,
-                                                                 String facetField,
-                                                                 Type facetDvType,
-                                                                 boolean facetDiskResident,
-                                                                 BytesRef facetPrefix,
-                                                                 int initialSize) {
-      switch (facetDvType) {
-        case VAR_INTS:
-        case FIXED_INTS_8:
-        case FIXED_INTS_16:
-        case FIXED_INTS_32:
-        case FIXED_INTS_64:
-        case FLOAT_32:
-        case FLOAT_64:
-        case BYTES_FIXED_STRAIGHT:
-        case BYTES_FIXED_DEREF:
-        case BYTES_VAR_STRAIGHT:
-        case BYTES_VAR_DEREF:
-          throw new IllegalArgumentException(String.format(Locale.ROOT, "Facet valueType %s not supported", facetDvType));
-        case BYTES_VAR_SORTED:
-        case BYTES_FIXED_SORTED:
-          return new FacetSortedBR(groupField, groupDvType, groupDiskResident, facetField, facetDvType, facetDiskResident, facetPrefix, initialSize);
-        default:
-          throw new IllegalArgumentException(String.format(Locale.ROOT, "Facet valueType %s not supported", facetDvType));
-      }
-    }
-
-
-    static class FacetSortedBR extends GroupSortedBR {
-
-      private DocValues.SortedSource facetFieldSource;
-
-      FacetSortedBR(String groupField, Type groupDvType, boolean groupDiskResident, String facetField, Type facetDvType, boolean diskResident, BytesRef facetPrefix, int initialSize) {
-        super(groupField, groupDvType, groupDiskResident, facetField, facetDvType, diskResident, facetPrefix, initialSize);
-      }
-
-      @Override
-      public void collect(int doc) throws IOException {
-        int facetOrd = facetFieldSource.ord(doc);
-        if (facetOrd < startFacetOrd || facetOrd >= endFacetOrd) {
-          return;
-        }
-
-        int groupOrd = groupFieldSource.ord(doc);
-        int segmentGroupedFacetsIndex = (groupOrd * facetFieldSource.getValueCount()) + facetOrd;
-        if (segmentGroupedFacetHits.exists(segmentGroupedFacetsIndex)) {
-          return;
-        }
-
-        segmentTotalCount++;
-        segmentFacetCounts[facetOrd]++;
-
-        segmentGroupedFacetHits.put(segmentGroupedFacetsIndex);
-        groupedFacetHits.add(
-            new GroupedFacetHit(
-                groupFieldSource.getByOrd(groupOrd, new BytesRef()),
-                facetFieldSource.getByOrd(facetOrd, new BytesRef())
-            )
-        );
-      }
-
-      @Override
-      public void setNextReader(AtomicReaderContext context) throws IOException {
-        if (segmentFacetCounts != null) {
-          segmentResults.add(createSegmentResult());
-        }
-
-        groupFieldSource = getDocValuesSortedSource(groupField, groupDvType, groupDiskResident, context.reader());
-        facetFieldSource = getDocValuesSortedSource(facetField, facetFieldDvType, facetDiskResident, context.reader());
-        segmentFacetCounts = new int[facetFieldSource.getValueCount()];
-        segmentTotalCount = 0;
-
-        segmentGroupedFacetHits.clear();
-        for (GroupedFacetHit groupedFacetHit : groupedFacetHits) {
-          int facetOrd = facetFieldSource.getOrdByValue(groupedFacetHit.facetValue, facetSpare);
-          if (facetOrd < 0) {
-            continue;
-          }
-
-          int groupOrd = groupFieldSource.getOrdByValue(groupedFacetHit.groupValue, groupSpare);
-          if (groupOrd < 0) {
-            continue;
-          }
-
-          int segmentGroupedFacetsIndex = (groupOrd * facetFieldSource.getValueCount()) + facetOrd;
-          segmentGroupedFacetHits.put(segmentGroupedFacetsIndex);
-        }
-
-        if (facetPrefix != null) {
-          startFacetOrd = facetFieldSource.getOrdByValue(facetPrefix, facetSpare);
-          if (startFacetOrd < 0) {
-            // Points to the ord one higher than facetPrefix
-            startFacetOrd = -startFacetOrd - 1;
-          }
-          BytesRef facetEndPrefix = BytesRef.deepCopyOf(facetPrefix);
-          facetEndPrefix.append(UnicodeUtil.BIG_TERM);
-          endFacetOrd = facetFieldSource.getOrdByValue(facetEndPrefix, facetSpare);
-          endFacetOrd = -endFacetOrd - 1; // Points to the ord one higher than facetEndPrefix
-        } else {
-          startFacetOrd = 0;
-          endFacetOrd = facetFieldSource.getValueCount();
-        }
-      }
-
-      @Override
-      protected SegmentResult createSegmentResult() throws IOException {
-        if (startFacetOrd == 0 && facetFieldSource.getByOrd(startFacetOrd, facetSpare).length == 0) {
-          int missing = segmentFacetCounts[0];
-          int total = segmentTotalCount - segmentFacetCounts[0];
-          return new SegmentResult(segmentFacetCounts, total, missing, facetFieldSource, endFacetOrd);
-        } else {
-          return new SegmentResult(segmentFacetCounts, segmentTotalCount, facetFieldSource, startFacetOrd, endFacetOrd);
-        }
-      }
-
-      private DocValues.SortedSource getDocValuesSortedSource(String field, Type dvType, boolean diskResident, AtomicReader reader) throws IOException {
-        DocValues dv = reader.docValues(field);
-        DocValues.Source dvSource;
-        if (dv != null) {
-          dvSource = diskResident ? dv.getDirectSource() : dv.getSource();
-        } else {
-          dvSource = DocValues.getDefaultSortedSource(dvType, reader.maxDoc());
-        }
-        return dvSource.asSortedSource();
-      }
-
-      private static class SegmentResult extends AbstractGroupFacetCollector.SegmentResult {
-
-        final DocValues.SortedSource facetFieldSource;
-        final BytesRef spare = new BytesRef();
-
-        SegmentResult(int[] counts, int total, int missing, DocValues.SortedSource facetFieldSource, int endFacetOrd) {
-          super(counts, total, missing, endFacetOrd);
-          this.facetFieldSource = facetFieldSource;
-          this.mergePos = 1;
-          if (mergePos < maxTermPos) {
-            mergeTerm = facetFieldSource.getByOrd(mergePos, spare);
-          }
-        }
-
-        SegmentResult(int[] counts, int total, DocValues.SortedSource facetFieldSource, int startFacetOrd, int endFacetOrd) {
-          super(counts, total, 0, endFacetOrd);
-          this.facetFieldSource = facetFieldSource;
-          this.mergePos = startFacetOrd;
-          if (mergePos < maxTermPos) {
-            mergeTerm = facetFieldSource.getByOrd(mergePos, spare);
-          }
-        }
-
-        @Override
-        protected void nextTerm() throws IOException {
-          mergeTerm = facetFieldSource.getByOrd(mergePos, spare);
-        }
-
-      }
-
-    }
-
-  }
-
-}
-
-class GroupedFacetHit {
-
-  final BytesRef groupValue;
-  final BytesRef facetValue;
-
-  GroupedFacetHit(BytesRef groupValue, BytesRef facetValue) {
-    this.groupValue = groupValue;
-    this.facetValue = facetValue;
-  }
-}
Index: lucene/grouping/src/java/org/apache/lucene/search/grouping/dv/DVDistinctValuesCollector.java
===================================================================
--- lucene/grouping/src/java/org/apache/lucene/search/grouping/dv/DVDistinctValuesCollector.java	(revision 1442822)
+++ lucene/grouping/src/java/org/apache/lucene/search/grouping/dv/DVDistinctValuesCollector.java	(working copy)
@@ -1,305 +0,0 @@
-package org.apache.lucene.search.grouping.dv;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.index.AtomicReaderContext;
-import org.apache.lucene.index.DocValues;
-import org.apache.lucene.search.grouping.AbstractDistinctValuesCollector;
-import org.apache.lucene.search.grouping.SearchGroup;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.SentinelIntSet;
-import org.apache.lucene.index.DocValues.Type; // javadocs
-
-import java.io.IOException;
-import java.util.*;
-
-/**
- * Docvalues implementation of {@link org.apache.lucene.search.grouping.AbstractDistinctValuesCollector}.
- *
- * @lucene.experimental
- */
-public abstract class DVDistinctValuesCollector<GC extends AbstractDistinctValuesCollector.GroupCount<?>> extends AbstractDistinctValuesCollector<GC> {
-
-  final String groupField;
-  final String countField;
-  final boolean diskResident;
-  final Type valueType;
-
-  DVDistinctValuesCollector(String groupField, String countField, boolean diskResident, Type valueType) {
-    this.groupField = groupField;
-    this.countField = countField;
-    this.diskResident = diskResident;
-    this.valueType = valueType;
-  }
-
-  /**
-   * Constructs a docvalues based implementation of {@link org.apache.lucene.search.grouping.AbstractDistinctValuesCollector} based on the specified
-   * type.
-   *
-   * @param groupField    The field to group by
-   * @param countField    The field to count distinct values for
-   * @param groups        The top N groups, collected during the first phase search
-   * @param diskResident  Whether the values to group and count by should be disk resident
-   * @param type          The {@link Type} which is used to select a concrete implementation
-   * @return a docvalues based distinct count collector
-   */
-  @SuppressWarnings("unchecked")
-  public static <T> DVDistinctValuesCollector<GroupCount<T>> create(String groupField, String countField, Collection<SearchGroup<T>> groups, boolean diskResident, Type type) {
-    switch (type) {
-      case VAR_INTS:
-      case FIXED_INTS_8:
-      case FIXED_INTS_16:
-      case FIXED_INTS_32:
-      case FIXED_INTS_64:
-        // Type erasure b/c otherwise we have inconvertible types...
-        return (DVDistinctValuesCollector) new NonSorted.Lng(groupField, countField, (Collection) groups, diskResident, type);
-      case FLOAT_32:
-      case FLOAT_64:
-        // Type erasure b/c otherwise we have inconvertible types...
-        return (DVDistinctValuesCollector) new NonSorted.Dbl(groupField, countField, (Collection) groups, diskResident, type);
-      case BYTES_FIXED_STRAIGHT:
-      case BYTES_FIXED_DEREF:
-      case BYTES_VAR_STRAIGHT:
-      case BYTES_VAR_DEREF:
-        // Type erasure b/c otherwise we have inconvertible types...
-        return (DVDistinctValuesCollector) new NonSorted.BR(groupField, countField, (Collection) groups, diskResident, type);
-      case BYTES_VAR_SORTED:
-      case BYTES_FIXED_SORTED:
-        // Type erasure b/c otherwise we have inconvertible types...
-        return (DVDistinctValuesCollector) new Sorted.BR(groupField, countField, (Collection) groups, diskResident, type);
-      default:
-        throw new IllegalArgumentException(String.format(Locale.ROOT, "ValueType %s not supported", type));
-    }
-  }
-
-
-  static abstract class NonSorted<K> extends DVDistinctValuesCollector<NonSorted.GroupCount> {
-
-    final Map<K, GroupCount> groupMap = new LinkedHashMap<K, GroupCount>();
-
-    DocValues.Source groupFieldSource;
-    DocValues.Source countFieldSource;
-
-    NonSorted(String groupField, String countField, boolean diskResident, Type valueType) {
-      super(groupField, countField, diskResident, valueType);
-    }
-
-    @Override
-    public List<GroupCount> getGroups() {
-      return new ArrayList<GroupCount>(groupMap.values());
-    }
-
-    @Override
-    public void setNextReader(AtomicReaderContext context) throws IOException {
-      groupFieldSource = retrieveSource(groupField, context);
-      countFieldSource = retrieveSource(countField, context);
-    }
-
-    private DocValues.Source retrieveSource(String fieldName, AtomicReaderContext context) throws IOException {
-      DocValues groupFieldDv = context.reader().docValues(fieldName);
-      if (groupFieldDv != null) {
-        return diskResident ? groupFieldDv.getDirectSource() : groupFieldDv.getSource();
-      } else {
-        return DocValues.getDefaultSource(valueType);
-      }
-    }
-
-    static class Dbl extends NonSorted<Double> {
-
-      Dbl(String groupField, String countField, Collection<SearchGroup<Double>> groups, boolean diskResident, Type valueType) {
-        super(groupField, countField, diskResident, valueType);
-        for (SearchGroup<Double> group : groups) {
-          groupMap.put(group.groupValue, new GroupCount(group.groupValue));
-        }
-      }
-
-      @Override
-      public void collect(int doc) throws IOException {
-        GroupCount groupCount = groupMap.get(groupFieldSource.getFloat(doc));
-        if (groupCount != null) {
-          groupCount.uniqueValues.add(countFieldSource.getFloat(doc));
-        }
-      }
-
-    }
-
-    static class Lng extends NonSorted<Long> {
-
-      Lng(String groupField, String countField, Collection<SearchGroup<Long>> groups, boolean diskResident, Type valueType) {
-        super(groupField, countField, diskResident, valueType);
-        for (SearchGroup<Long> group : groups) {
-          groupMap.put(group.groupValue, new GroupCount(group.groupValue));
-        }
-      }
-
-      @Override
-      public void collect(int doc) throws IOException {
-        GroupCount groupCount = groupMap.get(groupFieldSource.getInt(doc));
-        if (groupCount != null) {
-          groupCount.uniqueValues.add(countFieldSource.getInt(doc));
-        }
-      }
-
-    }
-
-    static class BR extends NonSorted<BytesRef> {
-
-      private final BytesRef spare = new BytesRef();
-
-      BR(String groupField, String countField, Collection<SearchGroup<BytesRef>> groups, boolean diskResident, Type valueType) {
-        super(groupField, countField, diskResident, valueType);
-        for (SearchGroup<BytesRef> group : groups) {
-          groupMap.put(group.groupValue, new GroupCount(group.groupValue));
-        }
-      }
-
-      @Override
-      public void collect(int doc) throws IOException {
-        GroupCount groupCount = groupMap.get(groupFieldSource.getBytes(doc, spare));
-        if (groupCount != null) {
-          BytesRef countValue = countFieldSource.getBytes(doc, spare);
-          if (!groupCount.uniqueValues.contains(countValue)) {
-            groupCount.uniqueValues.add(BytesRef.deepCopyOf(countValue));
-          }
-        }
-      }
-
-    }
-
-    static class GroupCount extends AbstractDistinctValuesCollector.GroupCount<Comparable<?>> {
-
-      GroupCount(Comparable<?> groupValue) {
-        super(groupValue);
-      }
-
-    }
-
-  }
-
-
-  static abstract class Sorted extends DVDistinctValuesCollector<Sorted.GroupCount> {
-
-    final SentinelIntSet ordSet;
-    final GroupCount groupCounts[];
-    final List<GroupCount> groups = new ArrayList<GroupCount>();
-
-    DocValues.SortedSource groupFieldSource;
-    DocValues.SortedSource countFieldSource;
-
-    Sorted(String groupField, String countField, int groupSize, boolean diskResident, Type valueType) {
-      super(groupField, countField, diskResident, valueType);
-      ordSet = new SentinelIntSet(groupSize, -1);
-      groupCounts = new GroupCount[ordSet.keys.length];
-    }
-
-    @Override
-    public List<GroupCount> getGroups() {
-      return groups;
-    }
-
-    @Override
-    public void setNextReader(AtomicReaderContext context) throws IOException {
-      groupFieldSource = retrieveSortedSource(groupField, context);
-      countFieldSource = retrieveSortedSource(countField, context);
-      ordSet.clear();
-    }
-
-    private DocValues.SortedSource retrieveSortedSource(String field, AtomicReaderContext context) throws IOException {
-      DocValues countFieldDv = context.reader().docValues(field);
-      if (countFieldDv != null) {
-        return diskResident ? countFieldDv.getDirectSource().asSortedSource() : countFieldDv.getSource().asSortedSource();
-      } else {
-        return DocValues.getDefaultSortedSource(valueType, context.reader().maxDoc());
-      }
-    }
-
-    static class BR extends Sorted {
-
-      final BytesRef spare = new BytesRef();
-
-      BR(String groupField, String countField, Collection<SearchGroup<BytesRef>> searchGroups, boolean diskResident, Type valueType) {
-        super(groupField, countField, searchGroups.size(), diskResident, valueType);
-        for (SearchGroup<BytesRef> group : searchGroups) {
-          this.groups.add(new GroupCount(group.groupValue));
-        }
-      }
-
-      @Override
-      public void collect(int doc) throws IOException {
-        int slot = ordSet.find(groupFieldSource.ord(doc));
-        if (slot < 0) {
-          return;
-        }
-
-        GroupCount gc = groupCounts[slot];
-        int countOrd = countFieldSource.ord(doc);
-        if (doesNotContainsOrd(countOrd, gc.ords)) {
-          gc.uniqueValues.add(countFieldSource.getByOrd(countOrd, new BytesRef()));
-          gc.ords = Arrays.copyOf(gc.ords, gc.ords.length + 1);
-          gc.ords[gc.ords.length - 1] = countOrd;
-          if (gc.ords.length > 1) {
-            Arrays.sort(gc.ords);
-          }
-        }
-      }
-
-      private boolean doesNotContainsOrd(int ord, int[] ords) {
-        if (ords.length == 0) {
-          return true;
-        } else if (ords.length == 1) {
-          return ord != ords[0];
-        }
-        return Arrays.binarySearch(ords, ord) < 0;
-      }
-
-      @Override
-      public void setNextReader(AtomicReaderContext context) throws IOException {
-        super.setNextReader(context);
-        for (GroupCount group : groups) {
-          int groupOrd = groupFieldSource.getOrdByValue((BytesRef) group.groupValue, spare);
-          if (groupOrd < 0) {
-            continue;
-          }
-
-          groupCounts[ordSet.put(groupOrd)] = group;
-          group.ords = new int[group.uniqueValues.size()];
-          Arrays.fill(group.ords, -1);
-          int i = 0;
-          for (Comparable<?> value : group.uniqueValues) {
-            int countOrd = countFieldSource.getOrdByValue((BytesRef) value, spare);
-            if (countOrd >= 0) {
-              group.ords[i++] = countOrd;
-            }
-          }
-        }
-      }
-    }
-
-    static class GroupCount extends AbstractDistinctValuesCollector.GroupCount<Comparable<?>> {
-
-      int[] ords;
-
-      GroupCount(Comparable<?> groupValue) {
-        super(groupValue);
-      }
-
-    }
-
-  }
-
-}
Index: lucene/grouping/src/java/org/apache/lucene/search/grouping/GroupingSearch.java
===================================================================
--- lucene/grouping/src/java/org/apache/lucene/search/grouping/GroupingSearch.java	(revision 1442822)
+++ lucene/grouping/src/java/org/apache/lucene/search/grouping/GroupingSearch.java	(working copy)
@@ -20,14 +20,8 @@
 import java.io.IOException;
 import java.util.*;
 
-import org.apache.lucene.document.DerefBytesDocValuesField;
-import org.apache.lucene.index.DocValues;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.search.*;
-import org.apache.lucene.search.grouping.dv.DVAllGroupHeadsCollector;
-import org.apache.lucene.search.grouping.dv.DVAllGroupsCollector;
-import org.apache.lucene.search.grouping.dv.DVFirstPassGroupingCollector;
-import org.apache.lucene.search.grouping.dv.DVSecondPassGroupingCollector;
 import org.apache.lucene.search.grouping.function.FunctionAllGroupHeadsCollector;
 import org.apache.lucene.search.grouping.function.FunctionAllGroupsCollector;
 import org.apache.lucene.search.grouping.function.FunctionFirstPassGroupingCollector;
@@ -51,8 +45,6 @@
   private final ValueSource groupFunction;
   private final Map<?, ?> valueSourceContext;
   private final Filter groupEndDocs;
-  private final DocValues.Type docValuesType;
-  private final boolean diskResidentDocValues;
 
   private Sort groupSort = Sort.RELEVANCE;
   private Sort sortWithinGroup;
@@ -80,23 +72,10 @@
    * @param groupField The name of the field to group by.
    */
   public GroupingSearch(String groupField) {
-    this(groupField, null, null, null, null, false);
+    this(groupField, null, null, null);
   }
 
   /**
-   * Constructs a <code>GroupingSearch</code> instance that groups documents by doc values.
-   * This constructor can only be used when the groupField
-   * is a <code>*DocValuesField</code> (eg, {@link DerefBytesDocValuesField}.
-   *
-   * @param groupField            The name of the field to group by that contains doc values
-   * @param docValuesType         The doc values type of the specified groupField
-   * @param diskResidentDocValues Whether the values to group by should be disk resident
-   */
-  public GroupingSearch(String groupField, DocValues.Type docValuesType, boolean diskResidentDocValues) {
-    this(groupField, null, null, null, docValuesType, diskResidentDocValues);
-  }
-
-  /**
    * Constructs a <code>GroupingSearch</code> instance that groups documents by function using a {@link ValueSource}
    * instance.
    *
@@ -104,7 +83,7 @@
    * @param valueSourceContext The context of the specified groupFunction
    */
   public GroupingSearch(ValueSource groupFunction, Map<?, ?> valueSourceContext) {
-    this(null, groupFunction, valueSourceContext, null, null, false);
+    this(null, groupFunction, valueSourceContext, null);
   }
 
   /**
@@ -114,16 +93,14 @@
    * @param groupEndDocs The filter that marks the last document in all doc blocks
    */
   public GroupingSearch(Filter groupEndDocs) {
-    this(null, null, null, groupEndDocs, null, false);
+    this(null, null, null, groupEndDocs);
   }
 
-  private GroupingSearch(String groupField, ValueSource groupFunction, Map<?, ?> valueSourceContext, Filter groupEndDocs, DocValues.Type docValuesType, boolean diskResidentDocValues) {
+  private GroupingSearch(String groupField, ValueSource groupFunction, Map<?, ?> valueSourceContext, Filter groupEndDocs) {
     this.groupField = groupField;
     this.groupFunction = groupFunction;
     this.valueSourceContext = valueSourceContext;
     this.groupEndDocs = groupEndDocs;
-    this.docValuesType = docValuesType;
-    this.diskResidentDocValues = diskResidentDocValues;
   }
 
   /**
@@ -180,18 +157,6 @@
       } else {
         allGroupHeadsCollector = null;
       }
-    } else if (docValuesType != null) {
-      firstPassCollector = DVFirstPassGroupingCollector.create(groupSort, topN, groupField, docValuesType, diskResidentDocValues);
-      if (allGroups) {
-        allGroupsCollector = DVAllGroupsCollector.create(groupField, docValuesType, diskResidentDocValues, initialSize);
-      } else {
-        allGroupsCollector = null;
-      }
-      if (allGroupHeads) {
-        allGroupHeadsCollector = DVAllGroupHeadsCollector.create(groupField, sortWithinGroup, docValuesType, diskResidentDocValues);
-      } else {
-        allGroupHeadsCollector = null;
-      }
     } else {
       firstPassCollector = new TermFirstPassGroupingCollector(groupField, groupSort, topN);
       if (allGroups) {
@@ -253,8 +218,6 @@
     AbstractSecondPassGroupingCollector secondPassCollector;
     if (groupFunction != null) {
       secondPassCollector = new FunctionSecondPassGroupingCollector((Collection) topSearchGroups, groupSort, sortWithinGroup, topNInsideGroup, includeScores, includeMaxScore, fillSortFields, groupFunction, valueSourceContext);
-    } else if (docValuesType != null) {
-      secondPassCollector = DVSecondPassGroupingCollector.create(groupField, diskResidentDocValues, docValuesType, (Collection) topSearchGroups, groupSort, sortWithinGroup, topNInsideGroup, includeScores, includeMaxScore, fillSortFields);
     } else {
       secondPassCollector = new TermSecondPassGroupingCollector(groupField, (Collection) topSearchGroups, groupSort, sortWithinGroup, topNInsideGroup, includeScores, includeMaxScore, fillSortFields);
     }
Index: lucene/grouping/src/test/org/apache/lucene/search/grouping/AllGroupsCollectorTest.java
===================================================================
--- lucene/grouping/src/test/org/apache/lucene/search/grouping/AllGroupsCollectorTest.java	(revision 1442822)
+++ lucene/grouping/src/test/org/apache/lucene/search/grouping/AllGroupsCollectorTest.java	(working copy)
@@ -21,13 +21,12 @@
 import org.apache.lucene.document.*;
 import org.apache.lucene.index.RandomIndexWriter;
 import org.apache.lucene.index.Term;
-import org.apache.lucene.index.DocValues.Type;
+import org.apache.lucene.index.FieldInfo.DocValuesType;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.queries.function.valuesource.BytesRefFieldSource;
 import org.apache.lucene.search.IndexSearcher;
 import org.apache.lucene.search.TermQuery;
 import org.apache.lucene.search.grouping.function.FunctionAllGroupsCollector;
-import org.apache.lucene.search.grouping.dv.DVAllGroupsCollector;
 import org.apache.lucene.search.grouping.term.TermAllGroupsCollector;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.util.BytesRef;
@@ -122,16 +121,13 @@
   private void addGroupField(Document doc, String groupField, String value, boolean canUseIDV) {
     doc.add(new TextField(groupField, value, Field.Store.YES));
     if (canUseIDV) {
-      doc.add(new SortedBytesDocValuesField(groupField, new BytesRef(value)));
+      doc.add(new SortedDocValuesField(groupField, new BytesRef(value)));
     }
   }
 
   private AbstractAllGroupsCollector<?> createRandomCollector(String groupField, boolean canUseIDV) {
     AbstractAllGroupsCollector<?> selected;
-    if (random().nextBoolean() && canUseIDV) {
-      boolean diskResident = random().nextBoolean();
-      selected = DVAllGroupsCollector.create(groupField, Type.BYTES_VAR_SORTED, diskResident);
-    } else if (random().nextBoolean()) {
+    if (random().nextBoolean()) {
       selected = new TermAllGroupsCollector(groupField);
     } else {
       ValueSource vs = new BytesRefFieldSource(groupField);
Index: lucene/grouping/src/test/org/apache/lucene/search/grouping/GroupingSearchTest.java
===================================================================
--- lucene/grouping/src/test/org/apache/lucene/search/grouping/GroupingSearchTest.java	(revision 1442822)
+++ lucene/grouping/src/test/org/apache/lucene/search/grouping/GroupingSearchTest.java	(working copy)
@@ -21,10 +21,9 @@
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.FieldType;
-import org.apache.lucene.document.SortedBytesDocValuesField;
+import org.apache.lucene.document.SortedDocValuesField;
 import org.apache.lucene.document.StringField;
 import org.apache.lucene.document.TextField;
-import org.apache.lucene.index.DocValues;
 import org.apache.lucene.index.RandomIndexWriter;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.queries.function.ValueSource;
@@ -177,7 +176,7 @@
   private void addGroupField(Document doc, String groupField, String value, boolean canUseIDV) {
     doc.add(new TextField(groupField, value, Field.Store.YES));
     if (canUseIDV) {
-      doc.add(new SortedBytesDocValuesField(groupField, new BytesRef(value)));
+      doc.add(new SortedDocValuesField(groupField, new BytesRef(value)));
     }
   }
 
@@ -210,12 +209,7 @@
       ValueSource vs = new BytesRefFieldSource(groupField);
       groupingSearch = new GroupingSearch(vs, new HashMap<Object, Object>());
     } else {
-      if (canUseIDV && random().nextBoolean()) {
-        boolean diskResident = random().nextBoolean();
-        groupingSearch = new GroupingSearch(groupField, DocValues.Type.BYTES_VAR_SORTED, diskResident);
-      } else {
-        groupingSearch = new GroupingSearch(groupField);  
-      }
+      groupingSearch = new GroupingSearch(groupField);
     }
 
     groupingSearch.setGroupSort(groupSort);
Index: lucene/grouping/src/test/org/apache/lucene/search/grouping/AllGroupHeadsCollectorTest.java
===================================================================
--- lucene/grouping/src/test/org/apache/lucene/search/grouping/AllGroupHeadsCollectorTest.java	(revision 1442822)
+++ lucene/grouping/src/test/org/apache/lucene/search/grouping/AllGroupHeadsCollectorTest.java	(working copy)
@@ -23,7 +23,7 @@
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.document.*;
 import org.apache.lucene.index.DirectoryReader;
-import org.apache.lucene.index.DocValues.Type;
+import org.apache.lucene.index.FieldInfo.DocValuesType;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.RandomIndexWriter;
 import org.apache.lucene.index.SlowCompositeReaderWrapper;
@@ -31,7 +31,6 @@
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.queries.function.valuesource.BytesRefFieldSource;
 import org.apache.lucene.search.*;
-import org.apache.lucene.search.grouping.dv.DVAllGroupHeadsCollector;
 import org.apache.lucene.search.grouping.function.FunctionAllGroupHeadsCollector;
 import org.apache.lucene.search.grouping.term.TermAllGroupHeadsCollector;
 import org.apache.lucene.store.Directory;
@@ -42,8 +41,8 @@
 
 public class AllGroupHeadsCollectorTest extends LuceneTestCase {
 
-  private static final Type[] vts = new Type[]{
-      Type.BYTES_VAR_DEREF, Type.BYTES_VAR_STRAIGHT, Type.BYTES_VAR_SORTED
+  private static final DocValuesType[] vts = new DocValuesType[]{
+      DocValuesType.BINARY, DocValuesType.SORTED
   };
 
   public void testBasic() throws Exception {
@@ -55,7 +54,7 @@
         newIndexWriterConfig(TEST_VERSION_CURRENT,
             new MockAnalyzer(random())).setMergePolicy(newLogMergePolicy()));
     boolean canUseIDV = true;
-    Type valueType = vts[random().nextInt(vts.length)];
+    DocValuesType valueType = vts[random().nextInt(vts.length)];
 
     // 0
     Document doc = new Document();
@@ -203,7 +202,7 @@
           newIndexWriterConfig(TEST_VERSION_CURRENT,
               new MockAnalyzer(random())));
       boolean canUseIDV = true;
-      Type valueType = vts[random().nextInt(vts.length)];
+      DocValuesType valueType = vts[random().nextInt(vts.length)];
 
       Document doc = new Document();
       Document docNoGroup = new Document();
@@ -212,15 +211,12 @@
       Field valuesField = null;
       if (canUseIDV) {
         switch(valueType) {
-        case BYTES_VAR_DEREF:
-          valuesField = new DerefBytesDocValuesField("group", new BytesRef());
+        case BINARY:
+          valuesField = new BinaryDocValuesField("group_dv", new BytesRef());
           break;
-        case BYTES_VAR_STRAIGHT:
-          valuesField = new StraightBytesDocValuesField("group", new BytesRef());
+        case SORTED:
+          valuesField = new SortedDocValuesField("group_dv", new BytesRef());
           break;
-        case BYTES_VAR_SORTED:
-          valuesField = new SortedBytesDocValuesField("group", new BytesRef());
-          break;
         default:
           fail("unhandled type");
         }
@@ -288,10 +284,10 @@
       w.close();
 
       // NOTE: intentional but temporary field cache insanity!
-      final int[] docIdToFieldId = FieldCache.DEFAULT.getInts(new SlowCompositeReaderWrapper(r), "id", false);
+      final FieldCache.Ints docIdToFieldId = FieldCache.DEFAULT.getInts(new SlowCompositeReaderWrapper(r), "id", false);
       final int[] fieldIdToDocID = new int[numDocs];
-      for (int i = 0; i < docIdToFieldId.length; i++) {
-        int fieldId = docIdToFieldId[i];
+      for (int i = 0; i < numDocs; i++) {
+        int fieldId = docIdToFieldId.get(i);
         fieldIdToDocID[fieldId] = i;
       }
 
@@ -306,11 +302,11 @@
         for (int contentID = 0; contentID < 3; contentID++) {
           final ScoreDoc[] hits = s.search(new TermQuery(new Term("content", "real" + contentID)), numDocs).scoreDocs;
           for (ScoreDoc hit : hits) {
-            final GroupDoc gd = groupDocs[docIdToFieldId[hit.doc]];
+            final GroupDoc gd = groupDocs[docIdToFieldId.get(hit.doc)];
             assertTrue(gd.score == 0.0);
             gd.score = hit.score;
             int docId = gd.id;
-            assertEquals(docId, docIdToFieldId[hit.doc]);
+            assertEquals(docId, docIdToFieldId.get(hit.doc));
           }
         }
 
@@ -333,7 +329,7 @@
           int[] actualGroupHeads = allGroupHeadsCollector.retrieveGroupHeads();
           // The actual group heads contains Lucene ids. Need to change them into our id value.
           for (int i = 0; i < actualGroupHeads.length; i++) {
-            actualGroupHeads[i] = docIdToFieldId[actualGroupHeads[i]];
+            actualGroupHeads[i] = docIdToFieldId.get(actualGroupHeads[i]);
           }
           // Allows us the easily iterate and assert the actual and expected results.
           Arrays.sort(expectedGroupHeads);
@@ -518,14 +514,11 @@
   }
 
   @SuppressWarnings({"unchecked","rawtypes"})
-  private AbstractAllGroupHeadsCollector<?> createRandomCollector(String groupField, Sort sortWithinGroup, boolean canUseIDV, Type valueType) {
+  private AbstractAllGroupHeadsCollector<?> createRandomCollector(String groupField, Sort sortWithinGroup, boolean canUseIDV, DocValuesType valueType) {
     AbstractAllGroupHeadsCollector<? extends AbstractAllGroupHeadsCollector.GroupHead> collector;
     if (random().nextBoolean()) {
       ValueSource vs = new BytesRefFieldSource(groupField);
       collector =  new FunctionAllGroupHeadsCollector(vs, new HashMap<Object, Object>(), sortWithinGroup);
-    } else if (canUseIDV && random().nextBoolean()) {
-      boolean diskResident = random().nextBoolean();
-      collector =  DVAllGroupHeadsCollector.create(groupField, sortWithinGroup, valueType, diskResident);
     } else {
       collector =  TermAllGroupHeadsCollector.create(groupField, sortWithinGroup);
     }
@@ -537,20 +530,17 @@
     return collector;
   }
 
-  private void addGroupField(Document doc, String groupField, String value, boolean canUseIDV, Type valueType) {
+  private void addGroupField(Document doc, String groupField, String value, boolean canUseIDV, DocValuesType valueType) {
     doc.add(new TextField(groupField, value, Field.Store.YES));
     if (canUseIDV) {
       Field valuesField = null;
       switch(valueType) {
-      case BYTES_VAR_DEREF:
-        valuesField = new DerefBytesDocValuesField(groupField, new BytesRef(value));
+      case BINARY:
+        valuesField = new BinaryDocValuesField(groupField + "_dv", new BytesRef(value));
         break;
-      case BYTES_VAR_STRAIGHT:
-        valuesField = new StraightBytesDocValuesField(groupField, new BytesRef(value));
+      case SORTED:
+        valuesField = new SortedDocValuesField(groupField + "_dv", new BytesRef(value));
         break;
-      case BYTES_VAR_SORTED:
-        valuesField = new SortedBytesDocValuesField(groupField, new BytesRef(value));
-        break;
       default:
         fail("unhandled type");
       }
Index: lucene/grouping/src/test/org/apache/lucene/search/grouping/GroupFacetCollectorTest.java
===================================================================
--- lucene/grouping/src/test/org/apache/lucene/search/grouping/GroupFacetCollectorTest.java	(revision 1442822)
+++ lucene/grouping/src/test/org/apache/lucene/search/grouping/GroupFacetCollectorTest.java	(working copy)
@@ -20,14 +20,12 @@
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.document.*;
 import org.apache.lucene.index.DirectoryReader;
-import org.apache.lucene.index.DocValues;
 import org.apache.lucene.index.NoMergePolicy;
 import org.apache.lucene.index.RandomIndexWriter;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.search.IndexSearcher;
 import org.apache.lucene.search.MatchAllDocsQuery;
 import org.apache.lucene.search.TermQuery;
-import org.apache.lucene.search.grouping.dv.DVGroupFacetCollector;
 import org.apache.lucene.search.grouping.term.TermGroupFacetCollector;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.FSDirectory;
@@ -55,42 +53,42 @@
 
     // 0
     Document doc = new Document();
-    addField(doc, groupField, "a", canUseDV);
-    addField(doc, "airport", "ams", canUseDV);
-    addField(doc, "duration", "5", canUseDV);
+    addField(doc, groupField, "a", useDv);
+    addField(doc, "airport", "ams", useDv);
+    addField(doc, "duration", "5", useDv);
     w.addDocument(doc);
 
     // 1
     doc = new Document();
-    addField(doc, groupField, "a", canUseDV);
-    addField(doc, "airport", "dus", canUseDV);
-    addField(doc, "duration", "10", canUseDV);
+    addField(doc, groupField, "a", useDv);
+    addField(doc, "airport", "dus", useDv);
+    addField(doc, "duration", "10", useDv);
     w.addDocument(doc);
 
     // 2
     doc = new Document();
-    addField(doc, groupField, "b", canUseDV);
-    addField(doc, "airport", "ams", canUseDV);
-    addField(doc, "duration", "10", canUseDV);
+    addField(doc, groupField, "b", useDv);
+    addField(doc, "airport", "ams", useDv);
+    addField(doc, "duration", "10", useDv);
     w.addDocument(doc);
     w.commit(); // To ensure a second segment
 
     // 3
     doc = new Document();
-    addField(doc, groupField, "b", canUseDV);
-    addField(doc, "airport", "ams", canUseDV);
-    addField(doc, "duration", "5", canUseDV);
+    addField(doc, groupField, "b", useDv);
+    addField(doc, "airport", "ams", useDv);
+    addField(doc, "duration", "5", useDv);
     w.addDocument(doc);
 
     // 4
     doc = new Document();
-    addField(doc, groupField, "b", canUseDV);
-    addField(doc, "airport", "ams", canUseDV);
-    addField(doc, "duration", "5", canUseDV);
+    addField(doc, groupField, "b", useDv);
+    addField(doc, "airport", "ams", useDv);
+    addField(doc, "duration", "5", useDv);
     w.addDocument(doc);
 
     IndexSearcher indexSearcher = new IndexSearcher(w.getReader());
-    AbstractGroupFacetCollector groupedAirportFacetCollector = createRandomCollector(groupField, "airport", null, false, useDv);
+    AbstractGroupFacetCollector groupedAirportFacetCollector = createRandomCollector(useDv ? "hotel_dv" : "hotel", useDv ? "airport_dv" : "airport", null, false);
     indexSearcher.search(new MatchAllDocsQuery(), groupedAirportFacetCollector);
     TermGroupFacetCollector.GroupedFacetResult airportResult = groupedAirportFacetCollector.mergeSegmentResults(10, 0, false);
     assertEquals(3, airportResult.getTotalCount());
@@ -104,7 +102,7 @@
     assertEquals(1, entries.get(1).getCount());
 
 
-    AbstractGroupFacetCollector groupedDurationFacetCollector = createRandomCollector(groupField, "duration", null, false, useDv);
+    AbstractGroupFacetCollector groupedDurationFacetCollector = createRandomCollector(useDv ? "hotel_dv" : "hotel", useDv ? "duration_dv" : "duration", null, false);
     indexSearcher.search(new MatchAllDocsQuery(), groupedDurationFacetCollector);
     TermGroupFacetCollector.GroupedFacetResult durationResult = groupedDurationFacetCollector.mergeSegmentResults(10, 0, false);
     assertEquals(4, durationResult.getTotalCount());
@@ -119,47 +117,59 @@
 
     // 5
     doc = new Document();
-    addField(doc, groupField, "b", canUseDV);
-    addField(doc, "duration", "5", canUseDV);
+    addField(doc, groupField, "b", useDv);
+    // missing airport
+    if (useDv) {
+      addField(doc, "airport", "", useDv);
+    }
+    addField(doc, "duration", "5", useDv);
     w.addDocument(doc);
 
     // 6
     doc = new Document();
-    addField(doc, groupField, "b", canUseDV);
-    addField(doc, "airport", "bru", canUseDV);
-    addField(doc, "duration", "10", canUseDV);
+    addField(doc, groupField, "b", useDv);
+    addField(doc, "airport", "bru", useDv);
+    addField(doc, "duration", "10", useDv);
     w.addDocument(doc);
 
     // 7
     doc = new Document();
-    addField(doc, groupField, "b", canUseDV);
-    addField(doc, "airport", "bru", canUseDV);
-    addField(doc, "duration", "15", canUseDV);
+    addField(doc, groupField, "b", useDv);
+    addField(doc, "airport", "bru", useDv);
+    addField(doc, "duration", "15", useDv);
     w.addDocument(doc);
 
     // 8
     doc = new Document();
-    addField(doc, groupField, "a", canUseDV);
-    addField(doc, "airport", "bru", canUseDV);
-    addField(doc, "duration", "10", canUseDV);
+    addField(doc, groupField, "a", useDv);
+    addField(doc, "airport", "bru", useDv);
+    addField(doc, "duration", "10", useDv);
     w.addDocument(doc);
 
     indexSearcher.getIndexReader().close();
     indexSearcher = new IndexSearcher(w.getReader());
-    groupedAirportFacetCollector = createRandomCollector(groupField, "airport", null, true, useDv);
+    groupedAirportFacetCollector = createRandomCollector(useDv ? "hotel_dv" : "hotel", useDv ? "airport_dv" : "airport", null, !useDv);
     indexSearcher.search(new MatchAllDocsQuery(), groupedAirportFacetCollector);
     airportResult = groupedAirportFacetCollector.mergeSegmentResults(3, 0, true);
-    assertEquals(5, airportResult.getTotalCount());
-    assertEquals(1, airportResult.getTotalMissingCount());
-
     entries = airportResult.getFacetEntries(1, 2);
     assertEquals(2, entries.size());
-    assertEquals("bru", entries.get(0).getValue().utf8ToString());
-    assertEquals(2, entries.get(0).getCount());
-    assertEquals("dus", entries.get(1).getValue().utf8ToString());
-    assertEquals(1, entries.get(1).getCount());
+    if (useDv) {
+      assertEquals(6, airportResult.getTotalCount());
+      assertEquals(0, airportResult.getTotalMissingCount());
+      assertEquals("bru", entries.get(0).getValue().utf8ToString());
+      assertEquals(2, entries.get(0).getCount());
+      assertEquals("", entries.get(1).getValue().utf8ToString());
+      assertEquals(1, entries.get(1).getCount());
+    } else {
+      assertEquals(5, airportResult.getTotalCount());
+      assertEquals(1, airportResult.getTotalMissingCount());
+      assertEquals("bru", entries.get(0).getValue().utf8ToString());
+      assertEquals(2, entries.get(0).getCount());
+      assertEquals("dus", entries.get(1).getValue().utf8ToString());
+      assertEquals(1, entries.get(1).getCount());
+    }
 
-    groupedDurationFacetCollector = createRandomCollector(groupField, "duration", null, false, useDv);
+    groupedDurationFacetCollector = createRandomCollector(useDv ? "hotel_dv" : "hotel", useDv ? "duration_dv" : "duration", null, false);
     indexSearcher.search(new MatchAllDocsQuery(), groupedDurationFacetCollector);
     durationResult = groupedDurationFacetCollector.mergeSegmentResults(10, 2, true);
     assertEquals(5, durationResult.getTotalCount());
@@ -172,36 +182,49 @@
 
     // 9
     doc = new Document();
-    addField(doc, groupField, "c", canUseDV);
-    addField(doc, "airport", "bru", canUseDV);
-    addField(doc, "duration", "15", canUseDV);
+    addField(doc, groupField, "c", useDv);
+    addField(doc, "airport", "bru", useDv);
+    addField(doc, "duration", "15", useDv);
     w.addDocument(doc);
 
     // 10
     doc = new Document();
-    addField(doc, groupField, "c", canUseDV);
-    addField(doc, "airport", "dus", canUseDV);
-    addField(doc, "duration", "10", canUseDV);
+    addField(doc, groupField, "c", useDv);
+    addField(doc, "airport", "dus", useDv);
+    addField(doc, "duration", "10", useDv);
     w.addDocument(doc);
 
     indexSearcher.getIndexReader().close();
     indexSearcher = new IndexSearcher(w.getReader());
-    groupedAirportFacetCollector = createRandomCollector(groupField, "airport", null, false, useDv);
+    groupedAirportFacetCollector = createRandomCollector(useDv ? "hotel_dv" : "hotel", useDv ? "airport_dv" : "airport", null, false);
     indexSearcher.search(new MatchAllDocsQuery(), groupedAirportFacetCollector);
     airportResult = groupedAirportFacetCollector.mergeSegmentResults(10, 0, false);
-    assertEquals(7, airportResult.getTotalCount());
-    assertEquals(1, airportResult.getTotalMissingCount());
-
     entries = airportResult.getFacetEntries(0, 10);
-    assertEquals(3, entries.size());
-    assertEquals("ams", entries.get(0).getValue().utf8ToString());
-    assertEquals(2, entries.get(0).getCount());
-    assertEquals("bru", entries.get(1).getValue().utf8ToString());
-    assertEquals(3, entries.get(1).getCount());
-    assertEquals("dus", entries.get(2).getValue().utf8ToString());
-    assertEquals(2, entries.get(2).getCount());
+    if (useDv) {
+      assertEquals(8, airportResult.getTotalCount());
+      assertEquals(0, airportResult.getTotalMissingCount());
+      assertEquals(4, entries.size());
+      assertEquals("", entries.get(0).getValue().utf8ToString());
+      assertEquals(1, entries.get(0).getCount());
+      assertEquals("ams", entries.get(1).getValue().utf8ToString());
+      assertEquals(2, entries.get(1).getCount());
+      assertEquals("bru", entries.get(2).getValue().utf8ToString());
+      assertEquals(3, entries.get(2).getCount());
+      assertEquals("dus", entries.get(3).getValue().utf8ToString());
+      assertEquals(2, entries.get(3).getCount());
+    } else {
+      assertEquals(7, airportResult.getTotalCount());
+      assertEquals(1, airportResult.getTotalMissingCount());
+      assertEquals(3, entries.size());
+      assertEquals("ams", entries.get(0).getValue().utf8ToString());
+      assertEquals(2, entries.get(0).getCount());
+      assertEquals("bru", entries.get(1).getValue().utf8ToString());
+      assertEquals(3, entries.get(1).getCount());
+      assertEquals("dus", entries.get(2).getValue().utf8ToString());
+      assertEquals(2, entries.get(2).getCount());
+    }
 
-    groupedDurationFacetCollector = createRandomCollector(groupField, "duration", "1", false, useDv);
+    groupedDurationFacetCollector = createRandomCollector(useDv ? "hotel_dv" : "hotel", useDv ? "duration_dv" : "duration", "1", false);
     indexSearcher.search(new MatchAllDocsQuery(), groupedDurationFacetCollector);
     durationResult = groupedDurationFacetCollector.mergeSegmentResults(10, 0, true);
     assertEquals(5, durationResult.getTotalCount());
@@ -237,13 +260,13 @@
 
     // 0
     Document doc = new Document();
-    addField(doc, "x", "x", useDv);
+    doc.add(new StringField("x", "x", Field.Store.NO));
     w.addDocument(doc);
 
     // 1
     doc = new Document();
     addField(doc, groupField, "a", useDv);
-    addField(doc, "airport", "ams", useDv);
+    doc.add(new StringField("airport", "ams", Field.Store.NO));
     w.addDocument(doc);
 
     w.commit();
@@ -252,43 +275,44 @@
     // 2
     doc = new Document();
     addField(doc, groupField, "a", useDv);
-    addField(doc, "airport", "ams", useDv);
+    doc.add(new StringField("airport", "ams", Field.Store.NO));
     w.addDocument(doc);
 
     // 3
     doc = new Document();
     addField(doc, groupField, "a", useDv);
-    addField(doc, "airport", "dus", useDv);
+    doc.add(new StringField("airport", "dus", Field.Store.NO));
+
     w.addDocument(doc);
 
     // 4
     doc = new Document();
     addField(doc, groupField, "b", useDv);
-    addField(doc, "airport", "ams", useDv);
+    doc.add(new StringField("airport", "ams", Field.Store.NO));
     w.addDocument(doc);
 
     // 5
     doc = new Document();
     addField(doc, groupField, "b", useDv);
-    addField(doc, "airport", "ams", useDv);
+    doc.add(new StringField("airport", "ams", Field.Store.NO));
     w.addDocument(doc);
 
     // 6
     doc = new Document();
     addField(doc, groupField, "b", useDv);
-    addField(doc, "airport", "ams", useDv);
+    doc.add(new StringField("airport", "ams", Field.Store.NO));
     w.addDocument(doc);
     w.commit();
 
     // 7
     doc = new Document();
-    addField(doc, "x", "x", useDv);
+    doc.add(new StringField("x", "x", Field.Store.NO));
     w.addDocument(doc);
     w.commit();
 
     w.close();
     IndexSearcher indexSearcher = new IndexSearcher(DirectoryReader.open(dir));
-    AbstractGroupFacetCollector groupedAirportFacetCollector = createRandomCollector(groupField, "airport", null, true, useDv);
+    AbstractGroupFacetCollector groupedAirportFacetCollector = createRandomCollector(groupField, "airport", null, true);
     indexSearcher.search(new MatchAllDocsQuery(), groupedAirportFacetCollector);
     TermGroupFacetCollector.GroupedFacetResult airportResult = groupedAirportFacetCollector.mergeSegmentResults(10, 0, false);
     assertEquals(3, airportResult.getTotalCount());
@@ -308,7 +332,7 @@
   private void addField(Document doc, String field, String value, boolean canUseIDV) {
     doc.add(new StringField(field, value, Field.Store.NO));
     if (canUseIDV) {
-      doc.add(new SortedBytesDocValuesField(field, new BytesRef(value)));
+      doc.add(new SortedDocValuesField(field + "_dv", new BytesRef(value)));
     }
   }
 
@@ -320,8 +344,15 @@
       IndexContext context = createIndexContext(multipleFacetsPerDocument);
       final IndexSearcher searcher = newSearcher(context.indexReader);
 
+      if (VERBOSE) {
+        System.out.println("TEST: searcher=" + searcher);
+      }
+
       for (int searchIter = 0; searchIter < 100; searchIter++) {
-        boolean useDv = context.useDV && random.nextBoolean();
+        if (VERBOSE) {
+          System.out.println("TEST: searchIter=" + searchIter);
+        }
+        boolean useDv = !multipleFacetsPerDocument && context.useDV && random.nextBoolean();
         String searchTerm = context.contentStrings[random.nextInt(context.contentStrings.length)];
         int limit = random.nextInt(context.facetValues.size());
         int offset = random.nextInt(context.facetValues.size() - limit);
@@ -344,7 +375,7 @@
         }
 
         GroupedFacetResult expectedFacetResult = createExpectedFacetResult(searchTerm, context, offset, limit, minCount, orderByCount, facetPrefix);
-        AbstractGroupFacetCollector groupFacetCollector = createRandomCollector("group", "facet", facetPrefix, multipleFacetsPerDocument, useDv);
+        AbstractGroupFacetCollector groupFacetCollector = createRandomCollector(useDv ? "group_dv" : "group", useDv ? "facet_dv" : "facet", facetPrefix, multipleFacetsPerDocument);
         searcher.search(new TermQuery(new Term("content", searchTerm)), groupFacetCollector);
         TermGroupFacetCollector.GroupedFacetResult actualFacetResult = groupFacetCollector.mergeSegmentResults(size, minCount, orderByCount);
 
@@ -352,6 +383,7 @@
         List<TermGroupFacetCollector.FacetEntry> actualFacetEntries = actualFacetResult.getFacetEntries(offset, limit);
 
         if (VERBOSE) {
+          System.out.println("Use DV: " + useDv);
           System.out.println("Collector: " + groupFacetCollector.getClass().getSimpleName());
           System.out.println("Num group: " + context.numGroups);
           System.out.println("Num doc: " + context.numDocs);
@@ -369,7 +401,7 @@
           System.out.println("\n=== Expected: \n");
           System.out.println("Total count " + expectedFacetResult.getTotalCount());
           System.out.println("Total missing count " + expectedFacetResult.getTotalMissingCount());
-          int counter = 1;
+          int counter = 0;
           for (TermGroupFacetCollector.FacetEntry expectedFacetEntry : expectedFacetEntries) {
             System.out.println(
                 String.format(Locale.ROOT,
@@ -382,7 +414,7 @@
           System.out.println("\n=== Actual: \n");
           System.out.println("Total count " + actualFacetResult.getTotalCount());
           System.out.println("Total missing count " + actualFacetResult.getTotalMissingCount());
-          counter = 1;
+          counter = 0;
           for (TermGroupFacetCollector.FacetEntry actualFacetEntry : actualFacetEntries) {
             System.out.println(
                 String.format(Locale.ROOT,
@@ -393,15 +425,15 @@
           }
           System.out.println("\n===================================================================================");
         }
-
+        
         assertEquals(expectedFacetResult.getTotalCount(), actualFacetResult.getTotalCount());
         assertEquals(expectedFacetResult.getTotalMissingCount(), actualFacetResult.getTotalMissingCount());
         assertEquals(expectedFacetEntries.size(), actualFacetEntries.size());
         for (int i = 0; i < expectedFacetEntries.size(); i++) {
           TermGroupFacetCollector.FacetEntry expectedFacetEntry = expectedFacetEntries.get(i);
           TermGroupFacetCollector.FacetEntry actualFacetEntry = actualFacetEntries.get(i);
-          assertEquals(expectedFacetEntry.getValue().utf8ToString() + " != " + actualFacetEntry.getValue().utf8ToString(), expectedFacetEntry.getValue(), actualFacetEntry.getValue());
-          assertEquals(expectedFacetEntry.getCount() + " != " + actualFacetEntry.getCount(), expectedFacetEntry.getCount(), actualFacetEntry.getCount());
+          assertEquals("i=" + i + ": " + expectedFacetEntry.getValue().utf8ToString() + " != " + actualFacetEntry.getValue().utf8ToString(), expectedFacetEntry.getValue(), actualFacetEntry.getValue());
+          assertEquals("i=" + i + ": " + expectedFacetEntry.getCount() + " != " + actualFacetEntry.getCount(), expectedFacetEntry.getCount(), actualFacetEntry.getCount());
         }
       }
 
@@ -449,14 +481,14 @@
         )
     );
     boolean canUseDV = true;
-    boolean useDv = canUseDV && random.nextBoolean();
+    boolean useDv = canUseDV && !multipleFacetValuesPerDocument && random.nextBoolean();
 
     Document doc = new Document();
     Document docNoGroup = new Document();
     Document docNoFacet = new Document();
     Document docNoGroupNoFacet = new Document();
     Field group = newStringField("group", "", Field.Store.NO);
-    Field groupDc = new SortedBytesDocValuesField("group", new BytesRef());
+    Field groupDc = new SortedDocValuesField("group_dv", new BytesRef());
     if (useDv) {
       doc.add(groupDc);
       docNoFacet.add(groupDc);
@@ -465,11 +497,12 @@
     docNoFacet.add(group);
     Field[] facetFields;
     if (useDv) {
+      assert !multipleFacetValuesPerDocument;
       facetFields = new Field[2];
       facetFields[0] = newStringField("facet", "", Field.Store.NO);
       doc.add(facetFields[0]);
       docNoGroup.add(facetFields[0]);
-      facetFields[1] = new SortedBytesDocValuesField("facet", new BytesRef());
+      facetFields[1] = new SortedDocValuesField("facet_dv", new BytesRef());
       doc.add(facetFields[1]);
       docNoGroup.add(facetFields[1]);
     } else {
@@ -509,7 +542,11 @@
       if (random.nextInt(24) == 17) {
         // So we test the "doc doesn't have the group'd
         // field" case:
-        groupValue = null;
+        if (useDv) {
+          groupValue = "";
+        } else {
+          groupValue = null;
+        }
       } else {
         groupValue = groups.get(random.nextInt(groups.size()));
       }
@@ -521,7 +558,7 @@
       Map<String, Set<String>> facetToGroups = searchTermToFacetToGroups.get(contentStr);
 
       List<String> facetVals = new ArrayList<String>();
-      if (random.nextInt(24) != 18) {
+      if (useDv || random.nextInt(24) != 18) {
         if (useDv) {
           String facetValue = facetValues.get(random.nextInt(facetValues.size()));
           uniqueFacetValues.add(facetValue);
@@ -573,6 +610,9 @@
           groupDc.setBytesValue(new BytesRef(groupValue));
         }
         group.setStringValue(groupValue);
+      } else if (useDv) {
+        // DV cannot have missing values:
+        groupDc.setBytesValue(new BytesRef());
       }
       content.setStringValue(contentStr);
       if (groupValue == null && facetVals.isEmpty()) {
@@ -662,14 +702,11 @@
     return new GroupedFacetResult(totalCount, totalMissCount, entriesResult);
   }
 
-  private AbstractGroupFacetCollector createRandomCollector(String groupField, String facetField, String facetPrefix, boolean multipleFacetsPerDocument, boolean useDv) {
+  private AbstractGroupFacetCollector createRandomCollector(String groupField, String facetField, String facetPrefix, boolean multipleFacetsPerDocument) {
     BytesRef facetPrefixBR = facetPrefix == null ? null : new BytesRef(facetPrefix);
-    if (useDv) {
-      return DVGroupFacetCollector.createDvGroupFacetCollector(groupField, DocValues.Type.BYTES_VAR_SORTED,
-          random().nextBoolean(), facetField, DocValues.Type.BYTES_VAR_SORTED, random().nextBoolean(), facetPrefixBR, random().nextInt(1024));
-    } else {
-      return TermGroupFacetCollector.createTermGroupFacetCollector(groupField, facetField, multipleFacetsPerDocument, facetPrefixBR, random().nextInt(1024));
-    }
+    // DocValues cannot be multi-valued:
+    assert !multipleFacetsPerDocument || !groupField.endsWith("_dv");
+    return TermGroupFacetCollector.createTermGroupFacetCollector(groupField, facetField, multipleFacetsPerDocument, facetPrefixBR, random().nextInt(1024));
   }
 
   private String getFromSet(Set<String> set, int index) {
Index: lucene/grouping/src/test/org/apache/lucene/search/grouping/DistinctValuesCollectorTest.java
===================================================================
--- lucene/grouping/src/test/org/apache/lucene/search/grouping/DistinctValuesCollectorTest.java	(revision 1442822)
+++ lucene/grouping/src/test/org/apache/lucene/search/grouping/DistinctValuesCollectorTest.java	(working copy)
@@ -23,13 +23,12 @@
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.document.*;
 import org.apache.lucene.index.*;
+import org.apache.lucene.index.FieldInfo.DocValuesType;
 import org.apache.lucene.queries.function.valuesource.BytesRefFieldSource;
 import org.apache.lucene.search.IndexSearcher;
 import org.apache.lucene.search.Sort;
 import org.apache.lucene.search.SortField;
 import org.apache.lucene.search.TermQuery;
-import org.apache.lucene.search.grouping.dv.DVDistinctValuesCollector;
-import org.apache.lucene.search.grouping.dv.DVFirstPassGroupingCollector;
 import org.apache.lucene.search.grouping.function.FunctionDistinctValuesCollector;
 import org.apache.lucene.search.grouping.function.FunctionFirstPassGroupingCollector;
 import org.apache.lucene.search.grouping.term.TermDistinctValuesCollector;
@@ -45,15 +44,16 @@
   private final static NullComparator nullComparator = new NullComparator();
   
   private final String groupField = "author";
+  private final String dvGroupField = "author_dv";
   private final String countField = "publisher";
+  private final String dvCountField = "publisher_dv";
 
   public void testSimple() throws Exception {
     Random random = random();
-    DocValues.Type[] dvTypes = new DocValues.Type[]{
-        DocValues.Type.VAR_INTS,
-        DocValues.Type.FLOAT_64,
-        DocValues.Type.BYTES_VAR_STRAIGHT,
-        DocValues.Type.BYTES_VAR_SORTED
+    DocValuesType[] dvTypes = new DocValuesType[]{
+        DocValuesType.NUMERIC,
+        DocValuesType.BINARY,
+        DocValuesType.SORTED,
     };
     Directory dir = newDirectory();
     RandomIndexWriter w = new RandomIndexWriter(
@@ -62,7 +62,7 @@
         newIndexWriterConfig(TEST_VERSION_CURRENT,
             new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));
     boolean canUseDV = true;
-    DocValues.Type dvType = canUseDV ? dvTypes[random.nextInt(dvTypes.length)] : null;
+    DocValuesType dvType = canUseDV ? dvTypes[random.nextInt(dvTypes.length)] : null;
 
     Document doc = new Document();
     addField(doc, groupField, "1", dvType);
@@ -232,7 +232,7 @@
       for (int searchIter = 0; searchIter < 100; searchIter++) {
         final IndexSearcher searcher = newSearcher(context.indexReader);
         boolean useDv = context.dvType != null && random.nextBoolean();
-        DocValues.Type dvType = useDv ? context.dvType : null;
+        DocValuesType dvType = useDv ? context.dvType : null;
         String term = context.contentStrings[random.nextInt(context.contentStrings.length)];
         Sort groupSort = new Sort(new SortField("id", SortField.Type.STRING));
         int topN = 1 + random.nextInt(10);
@@ -250,7 +250,15 @@
         if (VERBOSE) {
           System.out.println("Index iter=" + indexIter);
           System.out.println("Search iter=" + searchIter);
-          System.out.println("Collector class name=" + distinctValuesCollector.getClass().getName());
+          System.out.println("1st pass collector class name=" + firstCollector.getClass().getName());
+          System.out.println("2nd pass collector class name=" + distinctValuesCollector.getClass().getName());
+          System.out.println("Search term=" + term);
+          System.out.println("DVType=" + dvType);
+          System.out.println("1st pass groups=" + firstCollector.getTopGroups(0, false));
+          System.out.println("Expected:");      
+          printGroups(expectedResult);
+          System.out.println("Actual:");      
+          printGroups(actualResult);
         }
 
         assertEquals(expectedResult.size(), actualResult.size());
@@ -263,7 +271,7 @@
           Collections.sort(expectedUniqueValues, nullComparator);
           List<Comparable<?>> actualUniqueValues = new ArrayList<Comparable<?>>(actual.uniqueValues);
           Collections.sort(actualUniqueValues, nullComparator);
-          for (int j = 0; j < expected.uniqueValues.size(); j++) {
+          for (int j = 0; j < expectedUniqueValues.size(); j++) {
             assertValues(expectedUniqueValues.get(j), actualUniqueValues.get(j));
           }
         }
@@ -273,6 +281,25 @@
     }
   }
 
+  private void printGroups(List<AbstractDistinctValuesCollector.GroupCount<Comparable<?>>> results) {
+    for(int i=0;i<results.size();i++) {
+      AbstractDistinctValuesCollector.GroupCount<Comparable<?>> group = results.get(i);
+      Object gv = group.groupValue;
+      if (gv instanceof BytesRef) {
+        System.out.println(i + ": groupValue=" + ((BytesRef) gv).utf8ToString());
+      } else {
+        System.out.println(i + ": groupValue=" + gv);
+      }
+      for(Object o : group.uniqueValues) {
+        if (o instanceof BytesRef) {
+          System.out.println("  " + ((BytesRef) o).utf8ToString());
+        } else {
+          System.out.println("  " + o);
+        }
+      }
+    }
+  }
+
   private void assertValues(Object expected, Object actual) {
     if (expected == null) {
       compareNull(actual);
@@ -316,26 +343,24 @@
     }
   }
 
-  private void addField(Document doc, String field, String value, DocValues.Type type) {
-    doc.add(new StringField(field, value, Field.Store.NO));
+  private void addField(Document doc, String field, String value, DocValuesType type) {
+    doc.add(new StringField(field, value, Field.Store.YES));
     if (type == null) {
       return;
     }
+    String dvField = field + "_dv";
 
     Field valuesField = null;
     switch (type) {
-      case VAR_INTS:
-        valuesField = new PackedLongDocValuesField(field, Integer.parseInt(value));
+      case NUMERIC:
+        valuesField = new NumericDocValuesField(dvField, Integer.parseInt(value));
         break;
-      case FLOAT_64:
-        valuesField = new DoubleDocValuesField(field, Double.parseDouble(value));
+      case BINARY:
+        valuesField = new BinaryDocValuesField(dvField, new BytesRef(value));
         break;
-      case BYTES_VAR_STRAIGHT:
-        valuesField = new StraightBytesDocValuesField(field, new BytesRef(value));
+      case SORTED:
+        valuesField = new SortedDocValuesField(dvField, new BytesRef(value));
         break;
-      case BYTES_VAR_SORTED:
-        valuesField = new SortedBytesDocValuesField(field, new BytesRef(value));
-        break;
     }
     doc.add(valuesField);
   }
@@ -344,13 +369,10 @@
   private <T extends Comparable> AbstractDistinctValuesCollector<AbstractDistinctValuesCollector.GroupCount<T>> createDistinctCountCollector(AbstractFirstPassGroupingCollector<T> firstPassGroupingCollector,
                                                                       String groupField,
                                                                       String countField,
-                                                                      DocValues.Type dvType) {
+                                                                      DocValuesType dvType) {
     Random random = random();
     Collection<SearchGroup<T>> searchGroups = firstPassGroupingCollector.getTopGroups(0, false);
-    if (DVFirstPassGroupingCollector.class.isAssignableFrom(firstPassGroupingCollector.getClass())) {
-      boolean diskResident = random.nextBoolean();
-      return DVDistinctValuesCollector.create(groupField, countField, searchGroups, diskResident, dvType);
-    } else if (FunctionFirstPassGroupingCollector.class.isAssignableFrom(firstPassGroupingCollector.getClass())) {
+    if (FunctionFirstPassGroupingCollector.class.isAssignableFrom(firstPassGroupingCollector.getClass())) {
       return (AbstractDistinctValuesCollector) new FunctionDistinctValuesCollector(new HashMap<Object, Object>(), new BytesRefFieldSource(groupField), new BytesRefFieldSource(countField), (Collection) searchGroups);
     } else {
       return (AbstractDistinctValuesCollector) new TermDistinctValuesCollector(groupField, countField, (Collection) searchGroups);
@@ -358,13 +380,10 @@
   }
 
   @SuppressWarnings({"unchecked","rawtypes"})
-  private <T> AbstractFirstPassGroupingCollector<T> createRandomFirstPassCollector(DocValues.Type dvType, Sort groupSort, String groupField, int topNGroups) throws IOException {
+  private <T> AbstractFirstPassGroupingCollector<T> createRandomFirstPassCollector(DocValuesType dvType, Sort groupSort, String groupField, int topNGroups) throws IOException {
     Random random = random();
     if (dvType != null) {
       if (random.nextBoolean()) {
-        boolean diskResident = random.nextBoolean();
-        return DVFirstPassGroupingCollector.create(groupSort, topNGroups, groupField, dvType, diskResident);
-      } else if (random.nextBoolean()) {
         return (AbstractFirstPassGroupingCollector<T>) new FunctionFirstPassGroupingCollector(new BytesRefFieldSource(groupField), new HashMap<Object, Object>(), groupSort, topNGroups);
       } else {
         return (AbstractFirstPassGroupingCollector<T>) new TermFirstPassGroupingCollector(groupField, groupSort, topNGroups);
@@ -405,9 +424,9 @@
 
   private IndexContext createIndexContext() throws Exception {
     Random random = random();
-    DocValues.Type[] dvTypes = new DocValues.Type[]{
-        DocValues.Type.BYTES_VAR_STRAIGHT,
-        DocValues.Type.BYTES_VAR_SORTED
+    DocValuesType[] dvTypes = new DocValuesType[]{
+        DocValuesType.BINARY,
+        DocValuesType.SORTED
     };
 
     Directory dir = newDirectory();
@@ -419,7 +438,7 @@
       );
 
     boolean canUseDV = true;
-    DocValues.Type dvType = canUseDV ? dvTypes[random.nextInt(dvTypes.length)] : null;
+    DocValuesType dvType = canUseDV ? dvTypes[random.nextInt(dvTypes.length)] : null;
 
     int numDocs = 86 + random.nextInt(1087) * RANDOM_MULTIPLIER;
     String[] groupValues = new String[numDocs / 5];
@@ -451,18 +470,25 @@
       countsVals.add(countValue);
 
       Document doc = new Document();
-      doc.add(new StringField("id", String.format(Locale.ROOT, "%09d", i), Field.Store.NO));
+      doc.add(new StringField("id", String.format(Locale.ROOT, "%09d", i), Field.Store.YES));
       if (groupValue != null) {
         addField(doc, groupField, groupValue, dvType);
       }
       if (countValue != null) {
         addField(doc, countField, countValue, dvType);
       }
-      doc.add(new TextField("content", content, Field.Store.NO));
+      doc.add(new TextField("content", content, Field.Store.YES));
       w.addDocument(doc);
     }
 
     DirectoryReader reader = w.getReader();
+    if (VERBOSE) {
+      for(int docID=0;docID<reader.maxDoc();docID++) {
+        StoredDocument doc = reader.document(docID);
+        System.out.println("docID=" + docID + " id=" + doc.get("id") + " content=" + doc.get("content") + " author=" + doc.get("author") + " publisher=" + doc.get("publisher"));
+      }
+    }
+
     w.close();
     return new IndexContext(dir, reader, dvType, searchTermToGroupCounts, contentStrings.toArray(new String[contentStrings.size()]));
   }
@@ -471,11 +497,11 @@
 
     final Directory directory;
     final DirectoryReader indexReader;
-    final DocValues.Type dvType;
+    final DocValuesType dvType;
     final Map<String, Map<String, Set<String>>> searchTermToGroupCounts;
     final String[] contentStrings;
 
-    IndexContext(Directory directory, DirectoryReader indexReader, DocValues.Type dvType,
+    IndexContext(Directory directory, DirectoryReader indexReader, DocValuesType dvType,
                  Map<String, Map<String, Set<String>>> searchTermToGroupCounts, String[] contentStrings) {
       this.directory = directory;
       this.indexReader = indexReader;
Index: lucene/grouping/src/test/org/apache/lucene/search/grouping/AbstractGroupingTestCase.java
===================================================================
--- lucene/grouping/src/test/org/apache/lucene/search/grouping/AbstractGroupingTestCase.java	(revision 1442822)
+++ lucene/grouping/src/test/org/apache/lucene/search/grouping/AbstractGroupingTestCase.java	(working copy)
@@ -30,10 +30,11 @@
     String randomValue;
     do {
       // B/c of DV based impl we can't see the difference between an empty string and a null value.
-      // For that reason we don't generate empty string groups.
+      // For that reason we don't generate empty string
+      // groups.
       randomValue = _TestUtil.randomRealisticUnicodeString(random());
+      //randomValue = _TestUtil.randomSimpleString(random());
     } while ("".equals(randomValue));
     return randomValue;
   }
-
 }
Index: lucene/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping.java
===================================================================
--- lucene/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping.java	(revision 1442822)
+++ lucene/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping.java	(working copy)
@@ -19,9 +19,7 @@
 
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.document.*;
-import org.apache.lucene.index.AtomicReader;
 import org.apache.lucene.index.AtomicReaderContext;
-import org.apache.lucene.index.CompositeReaderContext;
 import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.ReaderUtil;
 import org.apache.lucene.index.FieldInfo.IndexOptions;
@@ -29,13 +27,9 @@
 import org.apache.lucene.index.IndexReaderContext;
 import org.apache.lucene.index.SlowCompositeReaderWrapper;
 import org.apache.lucene.index.Term;
-import org.apache.lucene.index.DocValues.Type;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.queries.function.valuesource.BytesRefFieldSource;
 import org.apache.lucene.search.*;
-import org.apache.lucene.search.grouping.dv.DVAllGroupsCollector;
-import org.apache.lucene.search.grouping.dv.DVFirstPassGroupingCollector;
-import org.apache.lucene.search.grouping.dv.DVSecondPassGroupingCollector;
 import org.apache.lucene.search.grouping.function.FunctionAllGroupsCollector;
 import org.apache.lucene.search.grouping.function.FunctionFirstPassGroupingCollector;
 import org.apache.lucene.search.grouping.function.FunctionSecondPassGroupingCollector;
@@ -62,7 +56,7 @@
 
   public void testBasic() throws Exception {
 
-    final String groupField = "author";
+    String groupField = "author";
 
     FieldType customType = new FieldType();
     customType.setStored(true);
@@ -126,7 +120,12 @@
     w.close();
 
     final Sort groupSort = Sort.RELEVANCE;
-    final AbstractFirstPassGroupingCollector<?> c1 = createRandomFirstPassCollector(groupField, groupSort, 10, canUseIDV);
+
+    if (canUseIDV && random().nextBoolean()) {
+      groupField += "_dv";
+    }
+
+    final AbstractFirstPassGroupingCollector<?> c1 = createRandomFirstPassCollector(groupField, groupSort, 10);
     indexSearcher.search(new TermQuery(new Term("content", "random")), c1);
 
     final AbstractSecondPassGroupingCollector<?> c2 = createSecondPassCollector(c1, groupField, groupSort, null, 0, 5, true, true, true);
@@ -176,16 +175,13 @@
   private void addGroupField(Document doc, String groupField, String value, boolean canUseIDV) {
     doc.add(new TextField(groupField, value, Field.Store.YES));
     if (canUseIDV) {
-      doc.add(new SortedBytesDocValuesField(groupField, new BytesRef(value)));
+      doc.add(new SortedDocValuesField(groupField + "_dv", new BytesRef(value)));
     }
   }
 
-  private AbstractFirstPassGroupingCollector<?> createRandomFirstPassCollector(String groupField, Sort groupSort, int topDocs, boolean canUseIDV) throws IOException {
+  private AbstractFirstPassGroupingCollector<?> createRandomFirstPassCollector(String groupField, Sort groupSort, int topDocs) throws IOException {
     AbstractFirstPassGroupingCollector<?> selected;
-    if (canUseIDV && random().nextBoolean()) {
-      boolean diskResident = random().nextBoolean();
-      selected = DVFirstPassGroupingCollector.create(groupSort, topDocs, groupField, Type.BYTES_VAR_SORTED, diskResident);
-    } else if (random().nextBoolean()) {
+    if (random().nextBoolean()) {
       ValueSource vs = new BytesRefFieldSource(groupField);
       selected = new FunctionFirstPassGroupingCollector(vs, new HashMap<Object, Object>(), groupSort, topDocs);
     } else {
@@ -198,10 +194,7 @@
   }
 
   private AbstractFirstPassGroupingCollector<?> createFirstPassCollector(String groupField, Sort groupSort, int topDocs, AbstractFirstPassGroupingCollector<?> firstPassGroupingCollector) throws IOException {
-    if (DVFirstPassGroupingCollector.class.isAssignableFrom(firstPassGroupingCollector.getClass())) {
-      boolean diskResident = random().nextBoolean();
-      return DVFirstPassGroupingCollector.create(groupSort, topDocs, groupField, Type.BYTES_VAR_SORTED, diskResident);
-    } else if (TermFirstPassGroupingCollector.class.isAssignableFrom(firstPassGroupingCollector.getClass())) {
+    if (TermFirstPassGroupingCollector.class.isAssignableFrom(firstPassGroupingCollector.getClass())) {
       ValueSource vs = new BytesRefFieldSource(groupField);
       return new FunctionFirstPassGroupingCollector(vs, new HashMap<Object, Object>(), groupSort, topDocs);
     } else {
@@ -220,11 +213,7 @@
                                                                         boolean getMaxScores,
                                                                         boolean fillSortFields) throws IOException {
 
-    if (DVFirstPassGroupingCollector.class.isAssignableFrom(firstPassGroupingCollector.getClass())) {
-      boolean diskResident = random().nextBoolean();
-      Collection<SearchGroup<T>> searchGroups = firstPassGroupingCollector.getTopGroups(groupOffset, fillSortFields);
-      return DVSecondPassGroupingCollector.create(groupField, diskResident, Type.BYTES_VAR_SORTED, searchGroups, groupSort, sortWithinGroup, maxDocsPerGroup, getScores, getMaxScores, fillSortFields);
-    } else if (TermFirstPassGroupingCollector.class.isAssignableFrom(firstPassGroupingCollector.getClass())) {
+    if (TermFirstPassGroupingCollector.class.isAssignableFrom(firstPassGroupingCollector.getClass())) {
       Collection<SearchGroup<BytesRef>> searchGroups = firstPassGroupingCollector.getTopGroups(groupOffset, fillSortFields);
       return (AbstractSecondPassGroupingCollector) new TermSecondPassGroupingCollector(groupField, searchGroups, groupSort, sortWithinGroup, maxDocsPerGroup , getScores, getMaxScores, fillSortFields);
     } else {
@@ -245,10 +234,7 @@
                                                                         boolean getScores,
                                                                         boolean getMaxScores,
                                                                         boolean fillSortFields) throws IOException {
-    if (DVFirstPassGroupingCollector.class.isAssignableFrom(firstPassGroupingCollector.getClass())) {
-      boolean diskResident = random().nextBoolean();
-      return DVSecondPassGroupingCollector.create(groupField, diskResident, Type.BYTES_VAR_SORTED, (Collection) searchGroups, groupSort, sortWithinGroup, maxDocsPerGroup, getScores, getMaxScores, fillSortFields);
-    } else if (firstPassGroupingCollector.getClass().isAssignableFrom(TermFirstPassGroupingCollector.class)) {
+    if (firstPassGroupingCollector.getClass().isAssignableFrom(TermFirstPassGroupingCollector.class)) {
       return new TermSecondPassGroupingCollector(groupField, searchGroups, groupSort, sortWithinGroup, maxDocsPerGroup , getScores, getMaxScores, fillSortFields);
     } else {
       ValueSource vs = new BytesRefFieldSource(groupField);
@@ -275,9 +261,6 @@
                                                               String groupField) {
     if (firstPassGroupingCollector.getClass().isAssignableFrom(TermFirstPassGroupingCollector.class)) {
       return new TermAllGroupsCollector(groupField);
-    } else if (firstPassGroupingCollector.getClass().isAssignableFrom(DVFirstPassGroupingCollector.class)) {
-      boolean diskResident = random().nextBoolean();
-      return DVAllGroupsCollector.create(groupField, Type.BYTES_VAR_SORTED, diskResident);
     } else {
       ValueSource vs = new BytesRefFieldSource(groupField);
       return new FunctionAllGroupsCollector(vs, new HashMap<Object, Object>());
@@ -324,10 +307,6 @@
         groups.add(sg);
       }
       return groups;
-    } else if (DVFirstPassGroupingCollector.class.isAssignableFrom(c.getClass())) {
-      @SuppressWarnings("unchecked")
-      Collection<SearchGroup<BytesRef>> topGroups = ((DVFirstPassGroupingCollector<BytesRef>) c).getTopGroups(groupOffset, fillFields);
-      return topGroups;
     }
     fail();
     return null;
@@ -345,8 +324,6 @@
         groups.add(new GroupDocs<BytesRef>(Float.NaN, mvalGd.maxScore, mvalGd.totalHits, mvalGd.scoreDocs, groupValue, mvalGd.groupSortValues));
       }
       return new TopGroups<BytesRef>(mvalTopGroups.groupSort, mvalTopGroups.withinGroupSort, mvalTopGroups.totalHitCount, mvalTopGroups.totalGroupedHitCount, groups.toArray(new GroupDocs[groups.size()]), Float.NaN);
-    } else if (DVSecondPassGroupingCollector.class.isAssignableFrom(c.getClass())) {
-      return ((DVSecondPassGroupingCollector<BytesRef>) c).getTopGroups(withinGroupOffset);
     }
     fail();
     return null;
@@ -665,8 +642,10 @@
         String randomValue;
         do {
           // B/c of DV based impl we can't see the difference between an empty string and a null value.
-          // For that reason we don't generate empty string groups.
+          // For that reason we don't generate empty string
+          // groups.
           randomValue = _TestUtil.randomRealisticUnicodeString(random());
+          //randomValue = _TestUtil.randomSimpleString(random());
         } while ("".equals(randomValue));
 
         groups.add(new BytesRef(randomValue));
@@ -698,9 +677,10 @@
 
       Document doc = new Document();
       Document docNoGroup = new Document();
-      Field idvGroupField = new SortedBytesDocValuesField("group", new BytesRef());
+      Field idvGroupField = new SortedDocValuesField("group_dv", new BytesRef());
       if (canUseIDV) {
         doc.add(idvGroupField);
+        docNoGroup.add(idvGroupField);
       }
 
       Field group = newStringField("group", "", Field.Store.NO);
@@ -742,6 +722,11 @@
           if (canUseIDV) {
             idvGroupField.setBytesValue(BytesRef.deepCopyOf(groupDoc.group));
           }
+        } else if (canUseIDV) {
+          // Must explicitly set empty string, else eg if
+          // the segment has all docs missing the field then
+          // we get null back instead of empty BytesRef:
+          idvGroupField.setBytesValue(new BytesRef());
         }
         sort1.setStringValue(groupDoc.sort1.utf8ToString());
         sort2.setStringValue(groupDoc.sort2.utf8ToString());
@@ -761,12 +746,16 @@
       w.close();
 
       // NOTE: intentional but temporary field cache insanity!
-      final int[] docIDToID = FieldCache.DEFAULT.getInts(new SlowCompositeReaderWrapper(r), "id", false);
+      final FieldCache.Ints docIDToID = FieldCache.DEFAULT.getInts(new SlowCompositeReaderWrapper(r), "id", false);
       DirectoryReader rBlocks = null;
       Directory dirBlocks = null;
 
       try {
         final IndexSearcher s = newSearcher(r);
+        if (VERBOSE) {
+          System.out.println("\nTEST: searcher=" + s);
+        }
+
         if (SlowCompositeReaderWrapper.class.isAssignableFrom(s.getIndexReader().getClass())) {
           canUseIDV = false;
         } else {
@@ -777,11 +766,10 @@
         for(int contentID=0;contentID<3;contentID++) {
           final ScoreDoc[] hits = s.search(new TermQuery(new Term("content", "real"+contentID)), numDocs).scoreDocs;
           for(ScoreDoc hit : hits) {
-            final GroupDoc gd = groupDocs[docIDToID[hit.doc]];
+            final GroupDoc gd = groupDocs[docIDToID.get(hit.doc)];
             assertTrue(gd.score == 0.0);
             gd.score = hit.score;
-            assertEquals(gd.id, docIDToID[hit.doc]);
-            //System.out.println("  score=" + hit.score + " id=" + docIDToID[hit.doc]);
+            assertEquals(gd.id, docIDToID.get(hit.doc));
           }
         }
 
@@ -794,7 +782,7 @@
         dirBlocks = newDirectory();
         rBlocks = getDocBlockReader(dirBlocks, groupDocs);
         final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term("groupend", "x"))));
-        final int[] docIDToIDBlocks = FieldCache.DEFAULT.getInts(new SlowCompositeReaderWrapper(rBlocks), "id", false);
+        final FieldCache.Ints docIDToIDBlocks = FieldCache.DEFAULT.getInts(new SlowCompositeReaderWrapper(rBlocks), "id", false);
 
         final IndexSearcher sBlocks = newSearcher(rBlocks);
         final ShardState shardsBlocks = new ShardState(sBlocks);
@@ -815,11 +803,11 @@
           //" dfnew=" + sBlocks.docFreq(new Term("content", "real"+contentID)));
           final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term("content", "real"+contentID)), numDocs).scoreDocs;
           for(ScoreDoc hit : hits) {
-            final GroupDoc gd = groupDocsByID[docIDToIDBlocks[hit.doc]];
+            final GroupDoc gd = groupDocsByID[docIDToIDBlocks.get(hit.doc)];
             assertTrue(gd.score2 == 0.0);
             gd.score2 = hit.score;
-            assertEquals(gd.id, docIDToIDBlocks[hit.doc]);
-            //System.out.println("    score=" + gd.score + " score2=" + hit.score + " id=" + docIDToIDBlocks[hit.doc]);
+            assertEquals(gd.id, docIDToIDBlocks.get(hit.doc));
+            //System.out.println("    score=" + gd.score + " score2=" + hit.score + " id=" + docIDToIDBlocks.get(hit.doc));
             termScoreMap.put(gd.score, gd.score2);
           }
         }
@@ -867,13 +855,20 @@
             System.out.println("TEST: groupSort=" + groupSort + " docSort=" + docSort + " searchTerm=" + searchTerm + " dF=" + r.docFreq(new Term("content", searchTerm))  +" dFBlock=" + rBlocks.docFreq(new Term("content", searchTerm)) + " topNGroups=" + topNGroups + " groupOffset=" + groupOffset + " docOffset=" + docOffset + " doCache=" + doCache + " docsPerGroup=" + docsPerGroup + " doAllGroups=" + doAllGroups + " getScores=" + getScores + " getMaxScores=" + getMaxScores);
           }
 
-          final AbstractFirstPassGroupingCollector<?> c1 = createRandomFirstPassCollector("group", groupSort, groupOffset+topNGroups, canUseIDV);
+          String groupField = "group";
+          if (canUseIDV && random().nextBoolean()) {
+            groupField += "_dv";
+          }
+          if (VERBOSE) {
+            System.out.println("  groupField=" + groupField);
+          }
+          final AbstractFirstPassGroupingCollector<?> c1 = createRandomFirstPassCollector(groupField, groupSort, groupOffset+topNGroups);
           final CachingCollector cCache;
           final Collector c;
 
           final AbstractAllGroupsCollector<?> allGroupsCollector;
           if (doAllGroups) {
-            allGroupsCollector = createAllGroupsCollector(c1, "group");
+            allGroupsCollector = createAllGroupsCollector(c1, groupField);
           } else {
             allGroupsCollector = null;
           }
@@ -908,6 +903,7 @@
 
           // Search top reader:
           final Query query = new TermQuery(new Term("content", searchTerm));
+
           s.search(query, c);
 
           if (doCache && !useWrappingCollector) {
@@ -956,7 +952,7 @@
               }
             }
 
-            c2 = createSecondPassCollector(c1, "group", groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);
+            c2 = createSecondPassCollector(c1, groupField, groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);
             if (doCache) {
               if (cCache.isCached()) {
                 if (VERBOSE) {
@@ -995,7 +991,7 @@
             } else {
               System.out.println("TEST: expected groups totalGroupedHitCount=" + expectedGroups.totalGroupedHitCount);
               for(GroupDocs<BytesRef> gd : expectedGroups.groups) {
-                System.out.println("  group=" + (gd.groupValue == null ? "null" : gd.groupValue) + " totalHits=" + gd.totalHits);
+                System.out.println("  group=" + (gd.groupValue == null ? "null" : gd.groupValue) + " totalHits=" + gd.totalHits + " scoreDocs.len=" + gd.scoreDocs.length);
                 for(ScoreDoc sd : gd.scoreDocs) {
                   System.out.println("    id=" + sd.doc + " score=" + sd.score);
                 }
@@ -1009,13 +1005,13 @@
               for(GroupDocs<BytesRef> gd : groupsResult.groups) {
                 System.out.println("  group=" + (gd.groupValue == null ? "null" : gd.groupValue) + " totalHits=" + gd.totalHits);
                 for(ScoreDoc sd : gd.scoreDocs) {
-                  System.out.println("    id=" + docIDToID[sd.doc] + " score=" + sd.score);
+                  System.out.println("    id=" + docIDToID.get(sd.doc) + " score=" + sd.score);
                 }
               }
 
               if (searchIter == 14) {
                 for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {
-                  System.out.println("ID=" + docIDToID[docIDX] + " explain=" + s.explain(query, docIDX));
+                  System.out.println("ID=" + docIDToID.get(docIDX) + " explain=" + s.explain(query, docIDX));
                 }
               }
             }
@@ -1027,14 +1023,13 @@
               for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {
                 System.out.println("  group=" + (gd.groupValue == null ? "null" : gd.groupValue) + " totalHits=" + gd.totalHits);
                 for(ScoreDoc sd : gd.scoreDocs) {
-                  System.out.println("    id=" + docIDToID[sd.doc] + " score=" + sd.score);
+                  System.out.println("    id=" + docIDToID.get(sd.doc) + " score=" + sd.score);
                 }
               }
             }
           }
 
-          boolean idvBasedImplsUsed = DVFirstPassGroupingCollector.class.isAssignableFrom(c1.getClass());
-          assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, idvBasedImplsUsed);
+          assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, groupField.endsWith("_dv"));
 
           // Confirm merged shards match:
           assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, idvBasedImplsUsedSharded.value);
@@ -1047,6 +1042,9 @@
           final TermAllGroupsCollector allGroupsCollector2;
           final Collector c4;
           if (doAllGroups) {
+            // NOTE: must be "group" and not "group_dv"
+            // (groupField) because we didn't index doc
+            // values in the block index:
             allGroupsCollector2 = new TermAllGroupsCollector("group");
             c4 = MultiCollector.wrap(c3, allGroupsCollector2);
           } else {
@@ -1074,7 +1072,7 @@
               for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {
                 System.out.println("  group=" + (gd.groupValue == null ? "null" : gd.groupValue.utf8ToString()) + " totalHits=" + gd.totalHits);
                 for(ScoreDoc sd : gd.scoreDocs) {
-                  System.out.println("    id=" + docIDToIDBlocks[sd.doc] + " score=" + sd.score);
+                  System.out.println("    id=" + docIDToIDBlocks.get(sd.doc) + " score=" + sd.score);
                   if (first) {
                     System.out.println("explain: " + sBlocks.explain(query, sd.doc));
                     first = false;
@@ -1085,8 +1083,10 @@
           }
 
           // Get shard'd block grouping result:
+          // Block index does not index DocValues so we pass
+          // false for canUseIDV:
           final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,
-              groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, true, new ValueHolder<Boolean>(false));
+              groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, false, new ValueHolder<Boolean>(false));
 
           if (expectedGroups != null) {
             // Fixup scores for reader2
@@ -1164,34 +1164,49 @@
     // TODO: swap in caching, all groups collector hereassertEquals(expected.totalHitCount, actual.totalHitCount);
     // too...
     if (VERBOSE) {
-      System.out.println("TEST: " + subSearchers.length + " shards: " + Arrays.toString(subSearchers));
+      System.out.println("TEST: " + subSearchers.length + " shards: " + Arrays.toString(subSearchers) + " canUseIDV=" + canUseIDV);
     }
     // Run 1st pass collector to get top groups per shard
     final Weight w = topSearcher.createNormalizedWeight(query);
     final List<Collection<SearchGroup<BytesRef>>> shardGroups = new ArrayList<Collection<SearchGroup<BytesRef>>>();
     List<AbstractFirstPassGroupingCollector<?>> firstPassGroupingCollectors = new ArrayList<AbstractFirstPassGroupingCollector<?>>();
     AbstractFirstPassGroupingCollector<?> firstPassCollector = null;
-    for(int shardIDX=0;shardIDX<subSearchers.length;shardIDX++) {
-      if (SlowCompositeReaderWrapper.class.isAssignableFrom(subSearchers[shardIDX].getIndexReader().getClass())) {
-        canUseIDV = false;
+    boolean shardsCanUseIDV;
+    if (canUseIDV) {
+      if (SlowCompositeReaderWrapper.class.isAssignableFrom(subSearchers[0].getIndexReader().getClass())) {
+        shardsCanUseIDV = false;
       } else {
-        canUseIDV = !preFlex;
+        shardsCanUseIDV = !preFlex;
       }
+    } else {
+      shardsCanUseIDV = false;
+    }
 
+    String groupField = "group";
+    if (shardsCanUseIDV && random().nextBoolean()) {
+      groupField += "_dv";
+      usedIdvBasedImpl.value = true;
+    }
+
+    for(int shardIDX=0;shardIDX<subSearchers.length;shardIDX++) {
+
+      // First shard determines whether we use IDV or not;
+      // all other shards match that:
       if (firstPassCollector == null) {
-        firstPassCollector = createRandomFirstPassCollector("group", groupSort, groupOffset + topNGroups, canUseIDV);
-        if (DVFirstPassGroupingCollector.class.isAssignableFrom(firstPassCollector.getClass())) {
-          usedIdvBasedImpl.value = true;
-        }
+        firstPassCollector = createRandomFirstPassCollector(groupField, groupSort, groupOffset + topNGroups);
       } else {
-        firstPassCollector = createFirstPassCollector("group", groupSort, groupOffset + topNGroups, firstPassCollector);
+        firstPassCollector = createFirstPassCollector(groupField, groupSort, groupOffset + topNGroups, firstPassCollector);
       }
+      if (VERBOSE) {
+        System.out.println("  shard=" + shardIDX + " groupField=" + groupField);
+        System.out.println("    1st pass collector=" + firstPassCollector);
+      }
       firstPassGroupingCollectors.add(firstPassCollector);
       subSearchers[shardIDX].search(w, firstPassCollector);
       final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(firstPassCollector, 0, true);
       if (topGroups != null) {
         if (VERBOSE) {
-          System.out.println("  shard " + shardIDX + " s=" + subSearchers[shardIDX] + " " + topGroups.size() + " groups:");
+          System.out.println("  shard " + shardIDX + " s=" + subSearchers[shardIDX] + " totalGroupedHitCount=?" + " " + topGroups.size() + " groups:");
           for(SearchGroup<BytesRef> group : topGroups) {
             System.out.println("    " + groupToString(group.groupValue) + " groupSort=" + Arrays.toString(group.sortValues));
           }
@@ -1219,7 +1234,7 @@
       final TopGroups<BytesRef>[] shardTopGroups = new TopGroups[subSearchers.length];
       for(int shardIDX=0;shardIDX<subSearchers.length;shardIDX++) {
         final AbstractSecondPassGroupingCollector<?> secondPassCollector = createSecondPassCollector(firstPassGroupingCollectors.get(shardIDX),
-            "group", mergedTopGroups, groupSort, docSort, docOffset + topNDocs, getScores, getMaxScores, true);
+            groupField, mergedTopGroups, groupSort, docSort, docOffset + topNDocs, getScores, getMaxScores, true);
         subSearchers[shardIDX].search(w, secondPassCollector);
         shardTopGroups[shardIDX] = getTopGroups(secondPassCollector, 0);
         if (VERBOSE) {
@@ -1243,7 +1258,7 @@
     }
   }
 
-  private void assertEquals(int[] docIDtoID, TopGroups<BytesRef> expected, TopGroups<BytesRef> actual, boolean verifyGroupValues, boolean verifyTotalGroupCount, boolean verifySortValues, boolean testScores, boolean idvBasedImplsUsed) {
+  private void assertEquals(FieldCache.Ints docIDtoID, TopGroups<BytesRef> expected, TopGroups<BytesRef> actual, boolean verifyGroupValues, boolean verifyTotalGroupCount, boolean verifySortValues, boolean testScores, boolean idvBasedImplsUsed) {
     if (expected == null) {
       assertNull(actual);
       return;
@@ -1290,8 +1305,8 @@
       for(int docIDX=0;docIDX<expectedFDs.length;docIDX++) {
         final FieldDoc expectedFD = (FieldDoc) expectedFDs[docIDX];
         final FieldDoc actualFD = (FieldDoc) actualFDs[docIDX];
-        //System.out.println("  actual doc=" + docIDtoID[actualFD.doc] + " score=" + actualFD.score);
-        assertEquals(expectedFD.doc, docIDtoID[actualFD.doc]);
+        //System.out.println("  actual doc=" + docIDtoID.get(actualFD.doc) + " score=" + actualFD.score);
+        assertEquals(expectedFD.doc, docIDtoID.get(actualFD.doc));
         if (testScores) {
           assertEquals(expectedFD.score, actualFD.score, 0.1);
         } else {
Index: lucene/highlighter/src/java/org/apache/lucene/search/highlight/WeightedSpanTermExtractor.java
===================================================================
--- lucene/highlighter/src/java/org/apache/lucene/search/highlight/WeightedSpanTermExtractor.java	(revision 1442822)
+++ lucene/highlighter/src/java/org/apache/lucene/search/highlight/WeightedSpanTermExtractor.java	(working copy)
@@ -31,11 +31,13 @@
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.index.AtomicReader;
 import org.apache.lucene.index.AtomicReaderContext;
-import org.apache.lucene.index.DocValues;
+import org.apache.lucene.index.BinaryDocValues;
 import org.apache.lucene.index.FieldInfos;
 import org.apache.lucene.index.Fields;
 import org.apache.lucene.index.FilterAtomicReader;
 import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.NumericDocValues;
+import org.apache.lucene.index.SortedDocValues;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.index.TermContext;
 import org.apache.lucene.index.Terms;
@@ -402,14 +404,24 @@
     }
 
     @Override
-    public DocValues docValues(String field) throws IOException {
-      return super.docValues(FIELD_NAME);
+    public NumericDocValues getNumericDocValues(String field) throws IOException {
+      return super.getNumericDocValues(FIELD_NAME);
     }
-
+    
     @Override
-    public DocValues normValues(String field) throws IOException {
-      return super.normValues(FIELD_NAME);
+    public BinaryDocValues getBinaryDocValues(String field) throws IOException {
+      return super.getBinaryDocValues(FIELD_NAME);
     }
+    
+    @Override
+    public SortedDocValues getSortedDocValues(String field) throws IOException {
+      return super.getSortedDocValues(FIELD_NAME);
+    }
+    
+    @Override
+    public NumericDocValues getNormValues(String field) throws IOException {
+      return super.getNormValues(FIELD_NAME);
+    }
   }
 
   /**
Index: lucene/codecs/src/java/org/apache/lucene/codecs/sep/SepDocValuesProducer.java
===================================================================
--- lucene/codecs/src/java/org/apache/lucene/codecs/sep/SepDocValuesProducer.java	(revision 1442822)
+++ lucene/codecs/src/java/org/apache/lucene/codecs/sep/SepDocValuesProducer.java	(working copy)
@@ -1,91 +0,0 @@
-package org.apache.lucene.codecs.sep;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-import java.io.Closeable;
-import java.io.IOException;
-import java.util.Collection;
-import java.util.Map;
-import java.util.TreeMap;
-
-import org.apache.lucene.codecs.PerDocProducerBase;
-import org.apache.lucene.codecs.lucene40.values.Bytes;
-import org.apache.lucene.codecs.lucene40.values.Floats;
-import org.apache.lucene.codecs.lucene40.values.Ints;
-import org.apache.lucene.index.DocValues;
-import org.apache.lucene.index.DocValues.Type;
-import org.apache.lucene.index.SegmentReadState;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.util.IOUtils;
-
-/**
- * Implementation of PerDocProducer that uses separate files.
- * @lucene.experimental
- */
-public class SepDocValuesProducer extends PerDocProducerBase {
-  private final TreeMap<String, DocValues> docValues;
-
-  /**
-   * Creates a new {@link SepDocValuesProducer} instance and loads all
-   * {@link DocValues} instances for this segment and codec.
-   */
-  public SepDocValuesProducer(SegmentReadState state) throws IOException {
-    docValues = load(state.fieldInfos, state.segmentInfo.name, state.segmentInfo.getDocCount(), state.dir, state.context);
-  }
-  
-  @Override
-  protected Map<String,DocValues> docValues() {
-    return docValues;
-  }
-  
-  @Override
-  protected void closeInternal(Collection<? extends Closeable> closeables) throws IOException {
-    IOUtils.close(closeables);
-  }
-
-  @Override
-  protected DocValues loadDocValues(int docCount, Directory dir, String id,
-      Type type, IOContext context) throws IOException {
-      switch (type) {
-      case FIXED_INTS_16:
-      case FIXED_INTS_32:
-      case FIXED_INTS_64:
-      case FIXED_INTS_8:
-      case VAR_INTS:
-        return Ints.getValues(dir, id, docCount, type, context);
-      case FLOAT_32:
-        return Floats.getValues(dir, id, docCount, context, type);
-      case FLOAT_64:
-        return Floats.getValues(dir, id, docCount, context, type);
-      case BYTES_FIXED_STRAIGHT:
-        return Bytes.getValues(dir, id, Bytes.Mode.STRAIGHT, true, docCount, getComparator(), context);
-      case BYTES_FIXED_DEREF:
-        return Bytes.getValues(dir, id, Bytes.Mode.DEREF, true, docCount, getComparator(), context);
-      case BYTES_FIXED_SORTED:
-        return Bytes.getValues(dir, id, Bytes.Mode.SORTED, true, docCount, getComparator(), context);
-      case BYTES_VAR_STRAIGHT:
-        return Bytes.getValues(dir, id, Bytes.Mode.STRAIGHT, false, docCount, getComparator(), context);
-      case BYTES_VAR_DEREF:
-        return Bytes.getValues(dir, id, Bytes.Mode.DEREF, false, docCount, getComparator(), context);
-      case BYTES_VAR_SORTED:
-        return Bytes.getValues(dir, id, Bytes.Mode.SORTED, false, docCount, getComparator(), context);
-      default:
-        throw new IllegalStateException("unrecognized index values mode " + type);
-      }
-    }
-}
Index: lucene/codecs/src/java/org/apache/lucene/codecs/sep/SepDocValuesConsumer.java
===================================================================
--- lucene/codecs/src/java/org/apache/lucene/codecs/sep/SepDocValuesConsumer.java	(revision 1442822)
+++ lucene/codecs/src/java/org/apache/lucene/codecs/sep/SepDocValuesConsumer.java	(working copy)
@@ -1,47 +0,0 @@
-package org.apache.lucene.codecs.sep;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.codecs.lucene40.values.DocValuesWriterBase;
-import org.apache.lucene.index.PerDocWriteState;
-import org.apache.lucene.store.Directory;
-
-/**
- * Implementation of PerDocConsumer that uses separate files.
- * @lucene.experimental
- */
-
-public class SepDocValuesConsumer extends DocValuesWriterBase {
-  private final Directory directory;
-
-  public SepDocValuesConsumer(PerDocWriteState state) {
-    super(state);
-    this.directory = state.directory;
-  }
-  
-  @Override
-  protected Directory getDirectory() {
-    return directory;
-  }
-
-  @Override
-  public void abort() {
-    // We don't have to remove files here: IndexFileDeleter
-    // will do so
-  }
-}
Index: lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextFieldsReader.java
===================================================================
--- lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextFieldsReader.java	(revision 1442822)
+++ lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextFieldsReader.java	(working copy)
@@ -68,7 +68,7 @@
 
   public SimpleTextFieldsReader(SegmentReadState state) throws IOException {
     fieldInfos = state.fieldInfos;
-    in = state.dir.openInput(SimpleTextPostingsFormat.getPostingsFileName(state.segmentInfo.name, state.segmentSuffix), state.context);
+    in = state.directory.openInput(SimpleTextPostingsFormat.getPostingsFileName(state.segmentInfo.name, state.segmentSuffix), state.context);
     boolean success = false;
     try {
       fields = readFields(in.clone());
Index: lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextDocValuesFormat.java
===================================================================
--- lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextDocValuesFormat.java	(revision 1442822)
+++ lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextDocValuesFormat.java	(working copy)
@@ -2,50 +2,104 @@
 
 /*
  * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to You under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
  * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
  */
 
 import java.io.IOException;
 
+import org.apache.lucene.codecs.DocValuesConsumer;
+import org.apache.lucene.codecs.DocValuesProducer;
 import org.apache.lucene.codecs.DocValuesFormat;
-import org.apache.lucene.codecs.PerDocConsumer;
-import org.apache.lucene.codecs.PerDocProducer;
-import org.apache.lucene.index.PerDocWriteState;
 import org.apache.lucene.index.SegmentReadState;
-import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.index.SegmentWriteState;
 
 /**
- * Plain-text DocValues format.
+ * plain text doc values format.
  * <p>
  * <b><font color="red">FOR RECREATIONAL USE ONLY</font></B>
- * 
- * @lucene.experimental
+ * <p>
+ * the .dat file contains the data.
+ *  for numbers this is a "fixed-width" file, for example a single byte range:
+ *  <pre>
+ *  field myField
+ *    type NUMERIC
+ *    minvalue 0
+ *    pattern 000
+ *  005
+ *  234
+ *  123
+ *  ...
+ *  </pre>
+ *  so a document's value (delta encoded from minvalue) can be retrieved by 
+ *  seeking to startOffset + (1+pattern.length())*docid. The extra 1 is the newline.
+ *  
+ *  for bytes this is also a "fixed-width" file, for example:
+ *  <pre>
+ *  field myField
+ *    type BINARY
+ *    maxlength 6
+ *    pattern 0
+ *  length 6
+ *  foobar[space][space]
+ *  length 3
+ *  baz[space][space][space][space][space]
+ *  ...
+ *  </pre>
+ *  so a doc's value can be retrieved by seeking to startOffset + (9+pattern.length+maxlength)*doc
+ *  the extra 9 is 2 newlines, plus "length " itself.
+ *  
+ *  for sorted bytes this is a fixed-width file, for example:
+ *  <pre>
+ *  field myField
+ *    type SORTED
+ *    numvalues 10
+ *    maxLength 8
+ *    pattern 0
+ *    ordpattern 00
+ *  length 6
+ *  foobar[space][space]
+ *  length 3
+ *  baz[space][space][space][space][space]
+ *  ...
+ *  03
+ *  06
+ *  01
+ *  10
+ *  ...
+ *  </pre>
+ *  so the "ord section" begins at startOffset + (9+pattern.length+maxlength)*numValues.
+ *  a document's ord can be retrieved by seeking to "ord section" + (1+ordpattern.length())*docid
+ *  an ord's value can be retrieved by seeking to startOffset + (9+pattern.length+maxlength)*ord
+ *   
+ *  the reader can just scan this file when it opens, skipping over the data blocks
+ *  and saving the offset/etc for each field. 
+ *  @lucene.experimental
  */
 public class SimpleTextDocValuesFormat extends DocValuesFormat {
-  private static final String DOC_VALUES_SEG_SUFFIX = "dv";
-  @Override
-  public PerDocConsumer docsConsumer(PerDocWriteState state) throws IOException {
-    return new SimpleTextPerDocConsumer(state, DOC_VALUES_SEG_SUFFIX);
+  
+  public SimpleTextDocValuesFormat() {
+    super("SimpleText");
   }
 
   @Override
-  public PerDocProducer docsProducer(SegmentReadState state) throws IOException {
-    return new SimpleTextPerDocProducer(state, BytesRef.getUTF8SortedAsUnicodeComparator(), DOC_VALUES_SEG_SUFFIX);
+  public DocValuesConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
+    return new SimpleTextDocValuesWriter(state, "dat");
   }
 
-  static String docValuesId(String segmentsName, int fieldId) {
-    return segmentsName + "_" + fieldId;
+  @Override
+  public DocValuesProducer fieldsProducer(SegmentReadState state) throws IOException {
+    return new SimpleTextDocValuesReader(state, "dat");
   }
 }
Index: lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextDocValuesConsumer.java
===================================================================
--- lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextDocValuesConsumer.java	(revision 1442822)
+++ lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextDocValuesConsumer.java	(working copy)
@@ -1,295 +0,0 @@
-package org.apache.lucene.codecs.simpletext;
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to You under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-import java.io.IOException;
-
-import org.apache.lucene.codecs.DocValuesArraySource;
-import org.apache.lucene.codecs.DocValuesConsumer;
-import org.apache.lucene.index.DocValues.Type;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.StorableField;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.util.ArrayUtil;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.BytesRefHash;
-import org.apache.lucene.util.IOUtils;
-
-/**
- * Writes plain-text DocValues.
- * <p>
- * <b><font color="red">FOR RECREATIONAL USE ONLY</font></B>
- * 
- * @lucene.experimental
- */
-public class SimpleTextDocValuesConsumer extends DocValuesConsumer {
-
-  static final BytesRef ZERO_DOUBLE = new BytesRef(Double.toString(0d));
-  static final BytesRef ZERO_INT = new BytesRef(Integer.toString(0));
-  static final BytesRef HEADER = new BytesRef("SimpleTextDocValues"); 
-
-  static final BytesRef END = new BytesRef("END");
-  static final BytesRef VALUE_SIZE = new BytesRef("valuesize ");
-  static final BytesRef DOC = new BytesRef("  doc ");
-  static final BytesRef VALUE = new BytesRef("    value ");
-  protected BytesRef scratch = new BytesRef();
-  protected int maxDocId = -1;
-  protected final String segment;
-  protected final Directory dir;
-  protected final IOContext ctx;
-  protected final Type type;
-  protected final BytesRefHash hash;
-  private int[] ords;
-  private int valueSize = Integer.MIN_VALUE;
-  private BytesRef zeroBytes;
-  private final String segmentSuffix;
-  
-
-  public SimpleTextDocValuesConsumer(String segment, Directory dir,
-      IOContext ctx, Type type, String segmentSuffix) {
-    this.ctx = ctx;
-    this.dir = dir;
-    this.segment = segment;
-    this.type = type;
-    hash = new BytesRefHash();
-    ords = new int[0];
-    this.segmentSuffix = segmentSuffix;
-  }
-
-  @Override
-  public void add(int docID, StorableField value) throws IOException {
-    assert docID >= 0;
-    final int ord, vSize;
-    switch (type) {
-    case BYTES_FIXED_DEREF:
-    case BYTES_FIXED_SORTED:
-    case BYTES_FIXED_STRAIGHT:
-      vSize = value.binaryValue().length;
-      ord = hash.add(value.binaryValue());
-      break;
-    case BYTES_VAR_DEREF:
-    case BYTES_VAR_SORTED:
-    case BYTES_VAR_STRAIGHT:
-      vSize = -1;
-      ord = hash.add(value.binaryValue());
-      break;
-    case FIXED_INTS_16:
-      vSize = 2;
-      scratch.grow(2);
-      DocValuesArraySource.copyShort(scratch, value.numericValue().shortValue());
-      ord = hash.add(scratch);
-      break;
-    case FIXED_INTS_32:
-      vSize = 4;
-      scratch.grow(4);
-      DocValuesArraySource.copyInt(scratch, value.numericValue().intValue());
-      ord = hash.add(scratch);
-      break;
-    case FIXED_INTS_8:
-      vSize = 1;
-      scratch.grow(1); 
-      scratch.bytes[scratch.offset] = value.numericValue().byteValue();
-      scratch.length = 1;
-      ord = hash.add(scratch);
-      break;
-    case FIXED_INTS_64:
-      vSize = 8;
-      scratch.grow(8);
-      DocValuesArraySource.copyLong(scratch, value.numericValue().longValue());
-      ord = hash.add(scratch);
-      break;
-    case VAR_INTS:
-      vSize = -1;
-      scratch.grow(8);
-      DocValuesArraySource.copyLong(scratch, value.numericValue().longValue());
-      ord = hash.add(scratch);
-      break;
-    case FLOAT_32:
-      vSize = 4;
-      scratch.grow(4);
-      DocValuesArraySource.copyInt(scratch,
-          Float.floatToRawIntBits(value.numericValue().floatValue()));
-      ord = hash.add(scratch);
-      break;
-    case FLOAT_64:
-      vSize = 8;
-      scratch.grow(8);
-      DocValuesArraySource.copyLong(scratch,
-          Double.doubleToRawLongBits(value.numericValue().doubleValue()));
-      ord = hash.add(scratch);
-      break;
-    default:
-      throw new RuntimeException("should not reach this line");
-    }
-    
-    if (valueSize == Integer.MIN_VALUE) {
-      assert maxDocId == -1;
-      valueSize = vSize;
-    } else {
-      if (valueSize != vSize) {
-        throw new IllegalArgumentException("value size must be " + valueSize + " but was: " + vSize);
-      }
-    }
-    maxDocId = Math.max(docID, maxDocId);
-    ords = grow(ords, docID);
-    
-    ords[docID] = (ord < 0 ? (-ord)-1 : ord) + 1;
-  }
-  
-  protected BytesRef getHeader() {
-    return HEADER;
-  }
-
-  private int[] grow(int[] array, int upto) {
-    if (array.length <= upto) {
-      return ArrayUtil.grow(array, 1 + upto);
-    }
-    return array;
-  }
-
-  private void prepareFlush(int docCount) {
-    assert ords != null;
-    ords = grow(ords, docCount);
-  }
-
-  @Override
-  public void finish(int docCount) throws IOException {
-    final String fileName = IndexFileNames.segmentFileName(segment, "",
-        segmentSuffix);
-    IndexOutput output = dir.createOutput(fileName, ctx);
-    boolean success = false;
-    BytesRef spare = new BytesRef();
-    try {
-      SimpleTextUtil.write(output, getHeader());
-      SimpleTextUtil.writeNewline(output);
-      SimpleTextUtil.write(output, VALUE_SIZE);
-      SimpleTextUtil.write(output, Integer.toString(this.valueSize), scratch);
-      SimpleTextUtil.writeNewline(output);
-      prepareFlush(docCount);
-      for (int i = 0; i < docCount; i++) {
-        SimpleTextUtil.write(output, DOC);
-        SimpleTextUtil.write(output, Integer.toString(i), scratch);
-        SimpleTextUtil.writeNewline(output);
-        SimpleTextUtil.write(output, VALUE);
-        writeDoc(output, i, spare);
-        SimpleTextUtil.writeNewline(output);
-      }
-      SimpleTextUtil.write(output, END);
-      SimpleTextUtil.writeNewline(output);
-      success = true;
-    } finally {
-      hash.close();
-      if (success) {
-        IOUtils.close(output);
-      } else {
-        IOUtils.closeWhileHandlingException(output);
-        dir.deleteFile(fileName);
-      }
-    }
-  }
-
-  protected void writeDoc(IndexOutput output, int docId, BytesRef spare) throws IOException {
-    int ord = ords[docId] - 1;
-    if (ord != -1) {
-      assert ord >= 0;
-      hash.get(ord, spare);
-
-      switch (type) {
-      case BYTES_FIXED_DEREF:
-      case BYTES_FIXED_SORTED:
-      case BYTES_FIXED_STRAIGHT:
-      case BYTES_VAR_DEREF:
-      case BYTES_VAR_SORTED:
-      case BYTES_VAR_STRAIGHT:
-        SimpleTextUtil.write(output, spare);
-        break;
-      case FIXED_INTS_16:
-        SimpleTextUtil.write(output,
-            Short.toString(DocValuesArraySource.asShort(spare)), scratch);
-        break;
-      case FIXED_INTS_32:
-        SimpleTextUtil.write(output,
-            Integer.toString(DocValuesArraySource.asInt(spare)), scratch);
-        break;
-      case VAR_INTS:
-      case FIXED_INTS_64:
-        SimpleTextUtil.write(output,
-            Long.toString(DocValuesArraySource.asLong(spare)), scratch);
-        break;
-      case FIXED_INTS_8:
-        assert spare.length == 1 : spare.length;
-        SimpleTextUtil.write(output,
-            Integer.toString(spare.bytes[spare.offset]), scratch);
-        break;
-      case FLOAT_32:
-        float valueFloat = Float.intBitsToFloat(DocValuesArraySource.asInt(spare));
-        SimpleTextUtil.write(output, Float.toString(valueFloat), scratch);
-        break;
-      case FLOAT_64:
-        double valueDouble = Double.longBitsToDouble(DocValuesArraySource
-            .asLong(spare));
-        SimpleTextUtil.write(output, Double.toString(valueDouble), scratch);
-        break;
-      default:
-        throw new IllegalArgumentException("unsupported type: " + type);
-      }
-    } else {
-      switch (type) {
-      case BYTES_FIXED_DEREF:
-      case BYTES_FIXED_SORTED:
-      case BYTES_FIXED_STRAIGHT:
-        if(zeroBytes == null) {
-          assert valueSize > 0;
-          zeroBytes = new BytesRef(new byte[valueSize]);
-        }
-        SimpleTextUtil.write(output, zeroBytes);
-        break;
-      case BYTES_VAR_DEREF:
-      case BYTES_VAR_SORTED:
-      case BYTES_VAR_STRAIGHT:
-        scratch.length = 0;
-        SimpleTextUtil.write(output, scratch);
-        break;
-      case FIXED_INTS_16:
-      case FIXED_INTS_32:
-      case FIXED_INTS_64:
-      case FIXED_INTS_8:
-      case VAR_INTS:
-        SimpleTextUtil.write(output, ZERO_INT);
-        break;
-      case FLOAT_32:
-      case FLOAT_64:
-        SimpleTextUtil.write(output, ZERO_DOUBLE);
-        break;
-      default:
-        throw new IllegalArgumentException("unsupported type: " + type);
-      }
-    }
-
-  }
-
-  @Override
-  protected Type getType() {
-    return type;
-  }
-
-  @Override
-  public int getValueSize() {
-    return valueSize;
-  }
-}
Index: lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextDocValuesReader.java
===================================================================
--- lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextDocValuesReader.java	(revision 0)
+++ lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextDocValuesReader.java	(working copy)
@@ -0,0 +1,307 @@
+package org.apache.lucene.codecs.simpletext;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.math.BigDecimal;
+import java.math.BigInteger;
+import java.text.DecimalFormat;
+import java.text.DecimalFormatSymbols;
+import java.text.ParseException;
+import java.util.HashMap;
+import java.util.Locale;
+import java.util.Map;
+
+import org.apache.lucene.codecs.DocValuesProducer;
+import org.apache.lucene.index.BinaryDocValues;
+import org.apache.lucene.index.CorruptIndexException;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.NumericDocValues;
+import org.apache.lucene.index.SegmentReadState;
+import org.apache.lucene.index.SortedDocValues;
+import org.apache.lucene.index.FieldInfo.DocValuesType;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.StringHelper;
+
+import static org.apache.lucene.codecs.simpletext.SimpleTextDocValuesWriter.END;
+import static org.apache.lucene.codecs.simpletext.SimpleTextDocValuesWriter.FIELD;
+import static org.apache.lucene.codecs.simpletext.SimpleTextDocValuesWriter.LENGTH;
+import static org.apache.lucene.codecs.simpletext.SimpleTextDocValuesWriter.MAXLENGTH;
+import static org.apache.lucene.codecs.simpletext.SimpleTextDocValuesWriter.MINVALUE;
+import static org.apache.lucene.codecs.simpletext.SimpleTextDocValuesWriter.NUMVALUES;
+import static org.apache.lucene.codecs.simpletext.SimpleTextDocValuesWriter.ORDPATTERN;
+import static org.apache.lucene.codecs.simpletext.SimpleTextDocValuesWriter.PATTERN;
+import static org.apache.lucene.codecs.simpletext.SimpleTextDocValuesWriter.TYPE;
+
+class SimpleTextDocValuesReader extends DocValuesProducer {
+
+  static class OneField {
+    long dataStartFilePointer;
+    String pattern;
+    String ordPattern;
+    int maxLength;
+    boolean fixedLength;
+    long minValue;
+    int numValues;
+  };
+
+  final int maxDoc;
+  final IndexInput data;
+  final BytesRef scratch = new BytesRef();
+  final Map<String,OneField> fields = new HashMap<String,OneField>();
+  
+  public SimpleTextDocValuesReader(SegmentReadState state, String ext) throws IOException {
+    //System.out.println("dir=" + state.directory + " seg=" + state.segmentInfo.name + " ext=" + ext);
+    data = state.directory.openInput(IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, ext), state.context);
+    maxDoc = state.segmentInfo.getDocCount();
+    while(true) {
+      readLine();
+      //System.out.println("READ field=" + scratch.utf8ToString());
+      if (scratch.equals(END)) {
+        break;
+      }
+      assert startsWith(FIELD) : scratch.utf8ToString();
+      String fieldName = stripPrefix(FIELD);
+      //System.out.println("  field=" + fieldName);
+      FieldInfo fieldInfo = state.fieldInfos.fieldInfo(fieldName);
+      assert fieldInfo != null;
+
+      OneField field = new OneField();
+      fields.put(fieldName, field);
+
+      readLine();
+      assert startsWith(TYPE) : scratch.utf8ToString();
+
+      DocValuesType dvType = DocValuesType.valueOf(stripPrefix(TYPE));
+      assert dvType != null;
+      if (dvType == DocValuesType.NUMERIC) {
+        readLine();
+        assert startsWith(MINVALUE): "got " + scratch.utf8ToString() + " field=" + fieldName + " ext=" + ext;
+        field.minValue = Long.parseLong(stripPrefix(MINVALUE));
+        readLine();
+        assert startsWith(PATTERN);
+        field.pattern = stripPrefix(PATTERN);
+        field.dataStartFilePointer = data.getFilePointer();
+        data.seek(data.getFilePointer() + (1+field.pattern.length()) * maxDoc);
+      } else if (dvType == DocValuesType.BINARY) {
+        readLine();
+        assert startsWith(MAXLENGTH);
+        field.maxLength = Integer.parseInt(stripPrefix(MAXLENGTH));
+        readLine();
+        assert startsWith(PATTERN);
+        field.pattern = stripPrefix(PATTERN);
+        field.dataStartFilePointer = data.getFilePointer();
+        data.seek(data.getFilePointer() + (9+field.pattern.length()+field.maxLength) * maxDoc);
+      } else if (dvType == DocValuesType.SORTED) {
+        readLine();
+        assert startsWith(NUMVALUES);
+        field.numValues = Integer.parseInt(stripPrefix(NUMVALUES));
+        readLine();
+        assert startsWith(MAXLENGTH);
+        field.maxLength = Integer.parseInt(stripPrefix(MAXLENGTH));
+        readLine();
+        assert startsWith(PATTERN);
+        field.pattern = stripPrefix(PATTERN);
+        readLine();
+        assert startsWith(ORDPATTERN);
+        field.ordPattern = stripPrefix(ORDPATTERN);
+        field.dataStartFilePointer = data.getFilePointer();
+        data.seek(data.getFilePointer() + (9+field.pattern.length()+field.maxLength) * field.numValues + (1+field.ordPattern.length())*maxDoc);
+      } else {
+        throw new AssertionError();
+      }
+    }
+
+    // We should only be called from above if at least one
+    // field has DVs:
+    assert !fields.isEmpty();
+  }
+
+  @Override
+  public NumericDocValues getNumeric(FieldInfo fieldInfo) throws IOException {
+    final OneField field = fields.get(fieldInfo.name);
+    assert field != null;
+
+    // SegmentCoreReaders already verifies this field is
+    // valid:
+    assert field != null: "field=" + fieldInfo.name + " fields=" + fields;
+
+    final IndexInput in = data.clone();
+    final BytesRef scratch = new BytesRef();
+    final DecimalFormat decoder = new DecimalFormat(field.pattern, new DecimalFormatSymbols(Locale.ROOT));
+
+    decoder.setParseBigDecimal(true);
+
+    return new NumericDocValues() {
+      @Override
+      public long get(int docID) {
+        try {
+          //System.out.println(Thread.currentThread().getName() + ": get docID=" + docID + " in=" + in);
+          if (docID < 0 || docID >= maxDoc) {
+            throw new IndexOutOfBoundsException("docID must be 0 .. " + (maxDoc-1) + "; got " + docID);
+          }
+          in.seek(field.dataStartFilePointer + (1+field.pattern.length())*docID);
+          SimpleTextUtil.readLine(in, scratch);
+          //System.out.println("parsing delta: " + scratch.utf8ToString());
+          BigDecimal bd;
+          try {
+            bd = (BigDecimal) decoder.parse(scratch.utf8ToString());
+          } catch (ParseException pe) {
+            CorruptIndexException e = new CorruptIndexException("failed to parse BigDecimal value");
+            e.initCause(pe);
+            throw e;
+          }
+          return BigInteger.valueOf(field.minValue).add(bd.toBigIntegerExact()).longValue();
+        } catch (IOException ioe) {
+          throw new RuntimeException(ioe);
+        }
+      }
+    };
+  }
+
+  @Override
+  public BinaryDocValues getBinary(FieldInfo fieldInfo) throws IOException {
+    final OneField field = fields.get(fieldInfo.name);
+
+    // SegmentCoreReaders already verifies this field is
+    // valid:
+    assert field != null;
+
+    final IndexInput in = data.clone();
+    final BytesRef scratch = new BytesRef();
+    final DecimalFormat decoder = new DecimalFormat(field.pattern, new DecimalFormatSymbols(Locale.ROOT));
+
+    return new BinaryDocValues() {
+      @Override
+      public void get(int docID, BytesRef result) {
+        try {
+          if (docID < 0 || docID >= maxDoc) {
+            throw new IndexOutOfBoundsException("docID must be 0 .. " + (maxDoc-1) + "; got " + docID);
+          }
+          in.seek(field.dataStartFilePointer + (9+field.pattern.length() + field.maxLength)*docID);
+          SimpleTextUtil.readLine(in, scratch);
+          assert StringHelper.startsWith(scratch, LENGTH);
+          int len;
+          try {
+            len = decoder.parse(new String(scratch.bytes, scratch.offset + LENGTH.length, scratch.length - LENGTH.length, "UTF-8")).intValue();
+          } catch (ParseException pe) {
+            CorruptIndexException e = new CorruptIndexException("failed to parse int length");
+            e.initCause(pe);
+            throw e;
+          }
+          result.bytes = new byte[len];
+          result.offset = 0;
+          result.length = len;
+          in.readBytes(result.bytes, 0, len);
+        } catch (IOException ioe) {
+          throw new RuntimeException(ioe);
+        }
+      }
+    };
+  }
+
+  @Override
+  public SortedDocValues getSorted(FieldInfo fieldInfo) throws IOException {
+    final OneField field = fields.get(fieldInfo.name);
+
+    // SegmentCoreReaders already verifies this field is
+    // valid:
+    assert field != null;
+
+    final IndexInput in = data.clone();
+    final BytesRef scratch = new BytesRef();
+    final DecimalFormat decoder = new DecimalFormat(field.pattern, new DecimalFormatSymbols(Locale.ROOT));
+    final DecimalFormat ordDecoder = new DecimalFormat(field.ordPattern, new DecimalFormatSymbols(Locale.ROOT));
+
+    return new SortedDocValues() {
+      @Override
+      public int getOrd(int docID) {
+        if (docID < 0 || docID >= maxDoc) {
+          throw new IndexOutOfBoundsException("docID must be 0 .. " + (maxDoc-1) + "; got " + docID);
+        }
+        try {
+          in.seek(field.dataStartFilePointer + field.numValues * (9 + field.pattern.length() + field.maxLength) + docID * (1 + field.ordPattern.length()));
+          SimpleTextUtil.readLine(in, scratch);
+          try {
+            return ordDecoder.parse(scratch.utf8ToString()).intValue();
+          } catch (ParseException pe) {
+            CorruptIndexException e = new CorruptIndexException("failed to parse ord");
+            e.initCause(pe);
+            throw e;
+          }
+        } catch (IOException ioe) {
+          throw new RuntimeException(ioe);
+        }
+      }
+
+      @Override
+      public void lookupOrd(int ord, BytesRef result) {
+        try {
+          if (ord < 0 || ord >= field.numValues) {
+            throw new IndexOutOfBoundsException("ord must be 0 .. " + (field.numValues-1) + "; got " + ord);
+          }
+          in.seek(field.dataStartFilePointer + ord * (9 + field.pattern.length() + field.maxLength));
+          SimpleTextUtil.readLine(in, scratch);
+          assert StringHelper.startsWith(scratch, LENGTH): "got " + scratch.utf8ToString() + " in=" + in;
+          int len;
+          try {
+            len = decoder.parse(new String(scratch.bytes, scratch.offset + LENGTH.length, scratch.length - LENGTH.length, "UTF-8")).intValue();
+          } catch (ParseException pe) {
+            CorruptIndexException e = new CorruptIndexException("failed to parse int length");
+            e.initCause(pe);
+            throw e;
+          }
+          result.bytes = new byte[len];
+          result.offset = 0;
+          result.length = len;
+          in.readBytes(result.bytes, 0, len);
+        } catch (IOException ioe) {
+          throw new RuntimeException(ioe);
+        }
+      }
+
+      @Override
+      public int getValueCount() {
+        return field.numValues;
+      }
+    };
+  }
+
+  @Override
+  public void close() throws IOException {
+    data.close();
+  }
+
+  /** Used only in ctor: */
+  private void readLine() throws IOException {
+    SimpleTextUtil.readLine(data, scratch);
+    //System.out.println("line: " + scratch.utf8ToString());
+  }
+
+  /** Used only in ctor: */
+  private boolean startsWith(BytesRef prefix) {
+    return StringHelper.startsWith(scratch, prefix);
+  }
+
+  /** Used only in ctor: */
+  private String stripPrefix(BytesRef prefix) throws IOException {
+    return new String(scratch.bytes, scratch.offset + prefix.length, scratch.length - prefix.length, "UTF-8");
+  }
+}

Property changes on: lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextDocValuesReader.java
___________________________________________________________________
Added: svn:eol-style
## -0,0 +1 ##
+native
Index: lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextPerDocProducer.java
===================================================================
--- lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextPerDocProducer.java	(revision 1442822)
+++ lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextPerDocProducer.java	(working copy)
@@ -1,447 +0,0 @@
-package org.apache.lucene.codecs.simpletext;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to You under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-import static org.apache.lucene.codecs.simpletext.SimpleTextDocValuesConsumer.DOC;
-import static org.apache.lucene.codecs.simpletext.SimpleTextDocValuesConsumer.END;
-import static org.apache.lucene.codecs.simpletext.SimpleTextDocValuesConsumer.HEADER;
-import static org.apache.lucene.codecs.simpletext.SimpleTextDocValuesConsumer.VALUE;
-import static org.apache.lucene.codecs.simpletext.SimpleTextDocValuesConsumer.VALUE_SIZE;
-
-import java.io.Closeable;
-import java.io.IOException;
-import java.util.Collection;
-import java.util.Comparator;
-import java.util.Map;
-import java.util.TreeMap;
-
-import org.apache.lucene.codecs.DocValuesArraySource;
-import org.apache.lucene.codecs.PerDocProducerBase;
-import org.apache.lucene.index.DocValues;
-import org.apache.lucene.index.DocValues.SortedSource;
-import org.apache.lucene.index.DocValues.Source;
-import org.apache.lucene.index.DocValues.Type;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.SegmentReadState;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.BytesRefHash;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.StringHelper;
-import org.apache.lucene.util.packed.PackedInts.Reader;
-
-/**
- * Reads plain-text DocValues.
- * <p>
- * <b><font color="red">FOR RECREATIONAL USE ONLY</font></B>
- * 
- * @lucene.experimental
- */
-public class SimpleTextPerDocProducer extends PerDocProducerBase {
-  protected final TreeMap<String, DocValues> docValues;
-  private Comparator<BytesRef> comp;
-  private final String segmentSuffix;
-
-  /**
-   * Creates a new {@link SimpleTextPerDocProducer} instance and loads all
-   * {@link DocValues} instances for this segment and codec.
-   */
-  public SimpleTextPerDocProducer(SegmentReadState state,
-      Comparator<BytesRef> comp, String segmentSuffix) throws IOException {
-    this.comp = comp;
-    this.segmentSuffix = segmentSuffix;
-    if (anyDocValuesFields(state.fieldInfos)) {
-      docValues = load(state.fieldInfos, state.segmentInfo.name,
-                       state.segmentInfo.getDocCount(), state.dir, state.context);
-    } else {
-      docValues = new TreeMap<String, DocValues>();
-    }
-  }
-
-  @Override
-  protected Map<String, DocValues> docValues() {
-    return docValues;
-  }
-
-  @Override
-  protected DocValues loadDocValues(int docCount, Directory dir, String id,
-      DocValues.Type type, IOContext context) throws IOException {
-    return new SimpleTextDocValues(dir, context, type, id, docCount, comp, segmentSuffix);
-  }
-
-  @Override
-  protected void closeInternal(Collection<? extends Closeable> closeables)
-      throws IOException {
-    IOUtils.close(closeables);
-  }
-
-  private static class SimpleTextDocValues extends DocValues {
-
-    private int docCount;
-
-    @Override
-    public void close() throws IOException {
-      boolean success = false;
-      try {
-        super.close();
-        success = true;
-      } finally {
-        if (success) {
-          IOUtils.close(input);
-        } else {
-          IOUtils.closeWhileHandlingException(input);
-        }
-      }
-    }
-
-    private Type type;
-    private Comparator<BytesRef> comp;
-    private int valueSize;
-    private final IndexInput input;
-
-    public SimpleTextDocValues(Directory dir, IOContext ctx, Type type,
-        String id, int docCount, Comparator<BytesRef> comp, String segmentSuffix) throws IOException {
-      this.type = type;
-      this.docCount = docCount;
-      this.comp = comp;
-      final String fileName = IndexFileNames.segmentFileName(id, "", segmentSuffix);
-      boolean success = false;
-      IndexInput in = null;
-      try {
-        in = dir.openInput(fileName, ctx);
-        valueSize = readHeader(in);
-        success = true;
-      } finally {
-        if (!success) {
-          IOUtils.closeWhileHandlingException(in);
-        }
-      }
-      input = in;
-
-    }
-
-    @Override
-    protected Source loadSource() throws IOException {
-      boolean success = false;
-      IndexInput in = input.clone();
-      try {
-        Source source = null;
-        switch (type) {
-        case BYTES_FIXED_DEREF:
-        case BYTES_FIXED_SORTED:
-        case BYTES_FIXED_STRAIGHT:
-        case BYTES_VAR_DEREF:
-        case BYTES_VAR_SORTED:
-        case BYTES_VAR_STRAIGHT:
-          source = read(in, new ValueReader(type, docCount, comp));
-          break;
-        case FIXED_INTS_16:
-        case FIXED_INTS_32:
-        case VAR_INTS:
-        case FIXED_INTS_64:
-        case FIXED_INTS_8:
-        case FLOAT_32:
-        case FLOAT_64:
-          source = read(in, new ValueReader(type, docCount, null));
-          break;
-        default:
-          throw new IllegalArgumentException("unknown type: " + type);
-        }
-        assert source != null;
-        success = true;
-        return source;
-      } finally {
-        if (!success) {
-          IOUtils.closeWhileHandlingException(in);
-        } else {
-          IOUtils.close(in);
-        }
-      }
-    }
-
-    private int readHeader(IndexInput in) throws IOException {
-      BytesRef scratch = new BytesRef();
-      SimpleTextUtil.readLine(in, scratch);
-      assert StringHelper.startsWith(scratch, HEADER);
-      SimpleTextUtil.readLine(in, scratch);
-      assert StringHelper.startsWith(scratch, VALUE_SIZE);
-      return Integer.parseInt(readString(scratch.offset + VALUE_SIZE.length,
-          scratch));
-    }
-
-    private Source read(IndexInput in, ValueReader reader) throws IOException {
-      BytesRef scratch = new BytesRef();
-      for (int i = 0; i < docCount; i++) {
-        SimpleTextUtil.readLine(in, scratch);
-
-        assert StringHelper.startsWith(scratch, DOC) : scratch.utf8ToString();
-        SimpleTextUtil.readLine(in, scratch);
-        assert StringHelper.startsWith(scratch, VALUE);
-        reader.fromString(i, scratch, scratch.offset + VALUE.length);
-      }
-      SimpleTextUtil.readLine(in, scratch);
-      assert scratch.equals(END);
-      return reader.getSource();
-    }
-    
-    @Override
-    public Source getDirectSource() throws IOException {
-      return this.getSource(); // don't cache twice
-    }
-
-    @Override
-    protected Source loadDirectSource() throws IOException {
-      return this.getSource();
-    }
-
-    @Override
-    public int getValueSize() {
-      return valueSize;
-    }
-
-    @Override
-    public Type getType() {
-      return type;
-    }
-
-  }
-
-  public static String readString(int offset, BytesRef scratch) {
-    return new String(scratch.bytes, scratch.offset + offset, scratch.length
-        - offset, IOUtils.CHARSET_UTF_8);
-  }
-
-  private static final class ValueReader {
-    private final Type type;
-    private byte[] bytes;
-    private short[] shorts;
-    private int[] ints;
-    private long[] longs;
-    private float[] floats;
-    private double[] doubles;
-    private Source source;
-    private BytesRefHash hash;
-    private BytesRef scratch;
-
-    public ValueReader(Type type, int maxDocs, Comparator<BytesRef> comp) {
-      super();
-      this.type = type;
-      Source docValuesArray = null;
-      switch (type) {
-      case FIXED_INTS_16:
-        shorts = new short[maxDocs];
-        docValuesArray = DocValuesArraySource.forType(type)
-            .newFromArray(shorts);
-        break;
-      case FIXED_INTS_32:
-        ints = new int[maxDocs];
-        docValuesArray = DocValuesArraySource.forType(type).newFromArray(ints);
-        break;
-      case FIXED_INTS_64:
-        longs = new long[maxDocs];
-        docValuesArray = DocValuesArraySource.forType(type)
-            .newFromArray(longs);
-        break;
-      case VAR_INTS:
-        longs = new long[maxDocs];
-        docValuesArray = new VarIntsArraySource(type, longs);
-        break;
-      case FIXED_INTS_8:
-        bytes = new byte[maxDocs];
-        docValuesArray = DocValuesArraySource.forType(type).newFromArray(bytes);
-        break;
-      case FLOAT_32:
-        floats = new float[maxDocs];
-        docValuesArray = DocValuesArraySource.forType(type)
-            .newFromArray(floats);
-        break;
-      case FLOAT_64:
-        doubles = new double[maxDocs];
-        docValuesArray = DocValuesArraySource.forType(type).newFromArray(
-            doubles);
-        break;
-      case BYTES_FIXED_DEREF:
-      case BYTES_FIXED_SORTED:
-      case BYTES_FIXED_STRAIGHT:
-      case BYTES_VAR_DEREF:
-      case BYTES_VAR_SORTED:
-      case BYTES_VAR_STRAIGHT:
-        assert comp != null;
-        hash = new BytesRefHash();
-        BytesSource bytesSource = new BytesSource(type, comp, maxDocs, hash);
-        ints = bytesSource.docIdToEntry;
-        source = bytesSource;
-        scratch = new BytesRef();
-        break;
-
-      }
-      if (docValuesArray != null) {
-        assert source == null;
-        this.source = docValuesArray;
-      }
-    }
-
-    public void fromString(int ord, BytesRef ref, int offset) {
-      switch (type) {
-      case FIXED_INTS_16:
-        assert shorts != null;
-        shorts[ord] = Short.parseShort(readString(offset, ref));
-        break;
-      case FIXED_INTS_32:
-        assert ints != null;
-        ints[ord] = Integer.parseInt(readString(offset, ref));
-        break;
-      case FIXED_INTS_64:
-      case VAR_INTS:
-        assert longs != null;
-        longs[ord] = Long.parseLong(readString(offset, ref));
-        break;
-      case FIXED_INTS_8:
-        assert bytes != null;
-        bytes[ord] = (byte) Integer.parseInt(readString(offset, ref));
-        break;
-      case FLOAT_32:
-        assert floats != null;
-        floats[ord] = Float.parseFloat(readString(offset, ref));
-        break;
-      case FLOAT_64:
-        assert doubles != null;
-        doubles[ord] = Double.parseDouble(readString(offset, ref));
-        break;
-      case BYTES_FIXED_DEREF:
-      case BYTES_FIXED_SORTED:
-      case BYTES_FIXED_STRAIGHT:
-      case BYTES_VAR_DEREF:
-      case BYTES_VAR_SORTED:
-      case BYTES_VAR_STRAIGHT:
-        scratch.bytes = ref.bytes;
-        scratch.length = ref.length - offset;
-        scratch.offset = ref.offset + offset;
-        int key = hash.add(scratch);
-        ints[ord] = key < 0 ? (-key) - 1 : key;
-        break;
-      }
-    }
-
-    public Source getSource() {
-      if (source instanceof BytesSource) {
-        ((BytesSource) source).maybeSort();
-      }
-      return source;
-    }
-  }
-
-  private static final class BytesSource extends SortedSource {
-
-    private final BytesRefHash hash;
-    int[] docIdToEntry;
-    int[] sortedEntries;
-    int[] adresses;
-    private final boolean isSorted;
-
-    protected BytesSource(Type type, Comparator<BytesRef> comp, int maxDoc,
-        BytesRefHash hash) {
-      super(type, comp);
-      docIdToEntry = new int[maxDoc];
-      this.hash = hash;
-      isSorted = type == Type.BYTES_FIXED_SORTED
-          || type == Type.BYTES_VAR_SORTED;
-    }
-
-    void maybeSort() {
-      if (isSorted) {
-        adresses = new int[hash.size()];
-        sortedEntries = hash.sort(getComparator());
-        for (int i = 0; i < adresses.length; i++) {
-          int entry = sortedEntries[i];
-          adresses[entry] = i;
-        }
-      }
-
-    }
-
-    @Override
-    public BytesRef getBytes(int docID, BytesRef ref) {
-      if (isSorted) {
-        return hash.get(sortedEntries[ord(docID)], ref);
-      } else {
-        return hash.get(docIdToEntry[docID], ref);
-      }
-    }
-
-    @Override
-    public SortedSource asSortedSource() {
-      if (isSorted) {
-        return this;
-      }
-      return null;
-    }
-
-    @Override
-    public int ord(int docID) {
-      assert isSorted;
-      try {
-        return adresses[docIdToEntry[docID]];
-      } catch (Exception e) {
-
-        return 0;
-      }
-    }
-
-    @Override
-    public BytesRef getByOrd(int ord, BytesRef bytesRef) {
-      assert isSorted;
-      return hash.get(sortedEntries[ord], bytesRef);
-    }
-
-    @Override
-    public Reader getDocToOrd() {
-      return null;
-    }
-
-    @Override
-    public int getValueCount() {
-      return hash.size();
-    }
-
-  }
-  
-  private static class VarIntsArraySource extends Source {
-
-    private final long[] array;
-
-    protected VarIntsArraySource(Type type, long[] array) {
-      super(type);
-      this.array = array;
-    }
-
-    @Override
-    public long getInt(int docID) {
-      return array[docID];
-    }
-
-    @Override
-    public BytesRef getBytes(int docID, BytesRef ref) {
-      DocValuesArraySource.copyLong(ref, getInt(docID));
-      return ref;
-    }
-    
-  }
-
-}
Index: lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextDocValuesWriter.java
===================================================================
--- lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextDocValuesWriter.java	(revision 0)
+++ lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextDocValuesWriter.java	(working copy)
@@ -0,0 +1,281 @@
+package org.apache.lucene.codecs.simpletext;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.math.BigInteger;
+import java.text.DecimalFormat;
+import java.text.DecimalFormatSymbols;
+import java.util.HashSet;
+import java.util.Locale;
+import java.util.Set;
+
+import org.apache.lucene.codecs.DocValuesConsumer;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.SegmentWriteState;
+import org.apache.lucene.index.FieldInfo.DocValuesType;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IOUtils;
+
+class SimpleTextDocValuesWriter extends DocValuesConsumer {
+  final static BytesRef END     = new BytesRef("END");
+  final static BytesRef FIELD   = new BytesRef("field ");
+  final static BytesRef TYPE    = new BytesRef("  type ");
+  // used for numerics
+  final static BytesRef MINVALUE = new BytesRef("  minvalue ");
+  final static BytesRef PATTERN  = new BytesRef("  pattern ");
+  // used for bytes
+  final static BytesRef LENGTH = new BytesRef("length ");
+  final static BytesRef MAXLENGTH = new BytesRef("  maxlength ");
+  // used for sorted bytes
+  final static BytesRef NUMVALUES = new BytesRef("  numvalues ");
+  final static BytesRef ORDPATTERN = new BytesRef("  ordpattern ");
+  
+  final IndexOutput data;
+  final BytesRef scratch = new BytesRef();
+  final int numDocs;
+  private final Set<String> fieldsSeen = new HashSet<String>(); // for asserting
+  
+  public SimpleTextDocValuesWriter(SegmentWriteState state, String ext) throws IOException {
+    //System.out.println("WRITE: " + IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, ext) + " " + state.segmentInfo.getDocCount() + " docs");
+    data = state.directory.createOutput(IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, ext), state.context);
+    numDocs = state.segmentInfo.getDocCount();
+  }
+
+  // for asserting
+  private boolean fieldSeen(String field) {
+    assert !fieldsSeen.contains(field): "field \"" + field + "\" was added more than once during flush";
+    fieldsSeen.add(field);
+    return true;
+  }
+
+  @Override
+  public void addNumericField(FieldInfo field, Iterable<Number> values) throws IOException {
+    assert fieldSeen(field.name);
+    assert (field.getDocValuesType() == FieldInfo.DocValuesType.NUMERIC ||
+            field.getNormType() == FieldInfo.DocValuesType.NUMERIC);
+    writeFieldEntry(field, FieldInfo.DocValuesType.NUMERIC);
+
+    // first pass to find min/max
+    long minValue = Long.MAX_VALUE;
+    long maxValue = Long.MIN_VALUE;
+    for(Number n : values) {
+      long v = n.longValue();
+      minValue = Math.min(minValue, v);
+      maxValue = Math.max(maxValue, v);
+    }
+    
+    // write our minimum value to the .dat, all entries are deltas from that
+    SimpleTextUtil.write(data, MINVALUE);
+    SimpleTextUtil.write(data, Long.toString(minValue), scratch);
+    SimpleTextUtil.writeNewline(data);
+    
+    // build up our fixed-width "simple text packed ints"
+    // format
+    BigInteger maxBig = BigInteger.valueOf(maxValue);
+    BigInteger minBig = BigInteger.valueOf(minValue);
+    BigInteger diffBig = maxBig.subtract(minBig);
+    int maxBytesPerValue = diffBig.toString().length();
+    StringBuilder sb = new StringBuilder();
+    for (int i = 0; i < maxBytesPerValue; i++) {
+      sb.append('0');
+    }
+    
+    // write our pattern to the .dat
+    SimpleTextUtil.write(data, PATTERN);
+    SimpleTextUtil.write(data, sb.toString(), scratch);
+    SimpleTextUtil.writeNewline(data);
+
+    final String patternString = sb.toString();
+    
+    final DecimalFormat encoder = new DecimalFormat(patternString, new DecimalFormatSymbols(Locale.ROOT));
+    
+    int numDocsWritten = 0;
+
+    // second pass to write the values
+    for(Number n : values) {
+      long value = n.longValue();
+      assert value >= minValue;
+      Number delta = BigInteger.valueOf(value).subtract(BigInteger.valueOf(minValue));
+      String s = encoder.format(delta);
+      assert s.length() == patternString.length();
+      SimpleTextUtil.write(data, s, scratch);
+      SimpleTextUtil.writeNewline(data);
+      numDocsWritten++;
+      assert numDocsWritten <= numDocs;
+    }
+
+    assert numDocs == numDocsWritten: "numDocs=" + numDocs + " numDocsWritten=" + numDocsWritten;
+  }
+
+  @Override
+  public void addBinaryField(FieldInfo field, Iterable<BytesRef> values) throws IOException {
+    assert fieldSeen(field.name);
+    assert field.getDocValuesType() == DocValuesType.BINARY;
+    int maxLength = 0;
+    for(BytesRef value : values) {
+      maxLength = Math.max(maxLength, value.length);
+    }
+    writeFieldEntry(field, FieldInfo.DocValuesType.BINARY);
+
+    // write maxLength
+    SimpleTextUtil.write(data, MAXLENGTH);
+    SimpleTextUtil.write(data, Integer.toString(maxLength), scratch);
+    SimpleTextUtil.writeNewline(data);
+    
+    int maxBytesLength = Long.toString(maxLength).length();
+    StringBuilder sb = new StringBuilder();
+    for (int i = 0; i < maxBytesLength; i++) {
+      sb.append('0');
+    }
+    // write our pattern for encoding lengths
+    SimpleTextUtil.write(data, PATTERN);
+    SimpleTextUtil.write(data, sb.toString(), scratch);
+    SimpleTextUtil.writeNewline(data);
+    final DecimalFormat encoder = new DecimalFormat(sb.toString(), new DecimalFormatSymbols(Locale.ROOT));
+
+    int numDocsWritten = 0;
+    for(BytesRef value : values) {
+      // write length
+      SimpleTextUtil.write(data, LENGTH);
+      SimpleTextUtil.write(data, encoder.format(value.length), scratch);
+      SimpleTextUtil.writeNewline(data);
+        
+      // write bytes -- don't use SimpleText.write
+      // because it escapes:
+      data.writeBytes(value.bytes, value.offset, value.length);
+
+      // pad to fit
+      for (int i = value.length; i < maxLength; i++) {
+        data.writeByte((byte)' ');
+      }
+      SimpleTextUtil.writeNewline(data);
+      numDocsWritten++;
+    }
+
+    assert numDocs == numDocsWritten;
+  }
+  
+  @Override
+  public void addSortedField(FieldInfo field, Iterable<BytesRef> values, Iterable<Number> docToOrd) throws IOException {
+    assert fieldSeen(field.name);
+    assert field.getDocValuesType() == DocValuesType.SORTED;
+    writeFieldEntry(field, FieldInfo.DocValuesType.SORTED);
+
+    int valueCount = 0;
+    int maxLength = -1;
+    for(BytesRef value : values) {
+      maxLength = Math.max(maxLength, value.length);
+      valueCount++;
+    }
+
+    // write numValues
+    SimpleTextUtil.write(data, NUMVALUES);
+    SimpleTextUtil.write(data, Integer.toString(valueCount), scratch);
+    SimpleTextUtil.writeNewline(data);
+    
+    // write maxLength
+    SimpleTextUtil.write(data, MAXLENGTH);
+    SimpleTextUtil.write(data, Integer.toString(maxLength), scratch);
+    SimpleTextUtil.writeNewline(data);
+    
+    int maxBytesLength = Integer.toString(maxLength).length();
+    StringBuilder sb = new StringBuilder();
+    for (int i = 0; i < maxBytesLength; i++) {
+      sb.append('0');
+    }
+    
+    // write our pattern for encoding lengths
+    SimpleTextUtil.write(data, PATTERN);
+    SimpleTextUtil.write(data, sb.toString(), scratch);
+    SimpleTextUtil.writeNewline(data);
+    final DecimalFormat encoder = new DecimalFormat(sb.toString(), new DecimalFormatSymbols(Locale.ROOT));
+    
+    int maxOrdBytes = Integer.toString(valueCount).length();
+    sb.setLength(0);
+    for (int i = 0; i < maxOrdBytes; i++) {
+      sb.append('0');
+    }
+    
+    // write our pattern for ords
+    SimpleTextUtil.write(data, ORDPATTERN);
+    SimpleTextUtil.write(data, sb.toString(), scratch);
+    SimpleTextUtil.writeNewline(data);
+    final DecimalFormat ordEncoder = new DecimalFormat(sb.toString(), new DecimalFormatSymbols(Locale.ROOT));
+
+    // for asserts:
+    int valuesSeen = 0;
+
+    for(BytesRef value : values) {
+      // write length
+      SimpleTextUtil.write(data, LENGTH);
+      SimpleTextUtil.write(data, encoder.format(value.length), scratch);
+      SimpleTextUtil.writeNewline(data);
+        
+      // write bytes -- don't use SimpleText.write
+      // because it escapes:
+      data.writeBytes(value.bytes, value.offset, value.length);
+
+      // pad to fit
+      for (int i = value.length; i < maxLength; i++) {
+        data.writeByte((byte)' ');
+      }
+      SimpleTextUtil.writeNewline(data);
+      valuesSeen++;
+      assert valuesSeen <= valueCount;
+    }
+
+    assert valuesSeen == valueCount;
+
+    for(Number ord : docToOrd) {
+      SimpleTextUtil.write(data, ordEncoder.format(ord.intValue()), scratch);
+      SimpleTextUtil.writeNewline(data);
+    }
+  }
+
+  /** write the header for this field */
+  private void writeFieldEntry(FieldInfo field, FieldInfo.DocValuesType type) throws IOException {
+    SimpleTextUtil.write(data, FIELD);
+    SimpleTextUtil.write(data, field.name, scratch);
+    SimpleTextUtil.writeNewline(data);
+    
+    SimpleTextUtil.write(data, TYPE);
+    SimpleTextUtil.write(data, type.toString(), scratch);
+    SimpleTextUtil.writeNewline(data);
+  }
+  
+  @Override
+  public void close() throws IOException {
+    boolean success = false;
+    try {
+      assert !fieldsSeen.isEmpty();
+      // TODO: sheisty to do this here?
+      SimpleTextUtil.write(data, END);
+      SimpleTextUtil.writeNewline(data);
+      success = true;
+    } finally {
+      if (success) {
+        IOUtils.close(data);
+      } else {
+        IOUtils.closeWhileHandlingException(data);
+      }
+    }
+  }
+}

Property changes on: lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextDocValuesWriter.java
___________________________________________________________________
Added: svn:eol-style
## -0,0 +1 ##
+native
Index: lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextPerDocConsumer.java
===================================================================
--- lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextPerDocConsumer.java	(revision 1442822)
+++ lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextPerDocConsumer.java	(working copy)
@@ -1,61 +0,0 @@
-package org.apache.lucene.codecs.simpletext;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to You under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.codecs.DocValuesConsumer;
-import org.apache.lucene.codecs.PerDocConsumer;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.PerDocWriteState;
-import org.apache.lucene.index.DocValues.Type;
-
-/**
- * @lucene.experimental
- */
-class SimpleTextPerDocConsumer extends PerDocConsumer {
-
-  protected final PerDocWriteState state;
-  protected final String segmentSuffix;
-  public SimpleTextPerDocConsumer(PerDocWriteState state, String segmentSuffix) {
-    this.state = state;
-    this.segmentSuffix = segmentSuffix;
-  }
-
-  @Override
-  public void close() throws IOException {
-
-  }
-
-  @Override
-  public DocValuesConsumer addValuesField(Type type, FieldInfo field)
-      throws IOException {
-    return new SimpleTextDocValuesConsumer(SimpleTextDocValuesFormat.docValuesId(state.segmentInfo.name,
-        field.number), state.directory, state.context, type, segmentSuffix);
-  }
-
-  @Override
-  public void abort() {
-    // We don't have to remove files here: IndexFileDeleter
-    // will do so
-  }
-  
-  static String docValuesId(String segmentsName, int fieldId) {
-    return segmentsName + "_" + fieldId;
-  }
-}
Index: lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextFieldInfosReader.java
===================================================================
--- lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextFieldInfosReader.java	(revision 1442822)
+++ lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextFieldInfosReader.java	(working copy)
@@ -25,10 +25,10 @@
 import org.apache.lucene.codecs.FieldInfosReader;
 import org.apache.lucene.index.CorruptIndexException;
 import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.FieldInfo.DocValuesType;
 import org.apache.lucene.index.FieldInfos;
 import org.apache.lucene.index.IndexFileNames;
 import org.apache.lucene.index.FieldInfo.IndexOptions;
-import org.apache.lucene.index.DocValues;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.IOContext;
 import org.apache.lucene.store.IndexInput;
@@ -97,12 +97,12 @@
         SimpleTextUtil.readLine(input, scratch);
         assert StringHelper.startsWith(scratch, NORMS_TYPE);
         String nrmType = readString(NORMS_TYPE.length, scratch);
-        final DocValues.Type normsType = docValuesType(nrmType);
+        final DocValuesType normsType = docValuesType(nrmType);
         
         SimpleTextUtil.readLine(input, scratch);
         assert StringHelper.startsWith(scratch, DOCVALUES);
         String dvType = readString(DOCVALUES.length, scratch);
-        final DocValues.Type docValuesType = docValuesType(dvType);
+        final DocValuesType docValuesType = docValuesType(dvType);
         
         SimpleTextUtil.readLine(input, scratch);
         assert StringHelper.startsWith(scratch, NUM_ATTS);
@@ -140,11 +140,11 @@
     }
   }
 
-  public DocValues.Type docValuesType(String dvType) {
+  public DocValuesType docValuesType(String dvType) {
     if ("false".equals(dvType)) {
       return null;
     } else {
-      return DocValues.Type.valueOf(dvType);
+      return DocValuesType.valueOf(dvType);
     }
   }
   
Index: lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextNormsFormat.java
===================================================================
--- lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextNormsFormat.java	(revision 1442822)
+++ lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextNormsFormat.java	(working copy)
@@ -18,19 +18,12 @@
  */
 
 import java.io.IOException;
-import java.util.Comparator;
 
+import org.apache.lucene.codecs.DocValuesConsumer;
+import org.apache.lucene.codecs.DocValuesProducer;
 import org.apache.lucene.codecs.NormsFormat;
-import org.apache.lucene.codecs.PerDocConsumer;
-import org.apache.lucene.codecs.PerDocProducer;
-import org.apache.lucene.index.AtomicReader;
-import org.apache.lucene.index.DocValues;
-import org.apache.lucene.index.DocValues.Type;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.PerDocWriteState;
 import org.apache.lucene.index.SegmentReadState;
-import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.index.SegmentWriteState;
 
 /**
  * plain-text norms format.
@@ -40,17 +33,16 @@
  * @lucene.experimental
  */
 public class SimpleTextNormsFormat extends NormsFormat {
-  private static final String NORMS_SEG_SUFFIX = "len";
+  private static final String NORMS_SEG_EXTENSION = "len";
   
   @Override
-  public PerDocConsumer docsConsumer(PerDocWriteState state) throws IOException {
-    return new SimpleTextNormsPerDocConsumer(state);
+  public DocValuesConsumer normsConsumer(SegmentWriteState state) throws IOException {
+    return new SimpleTextNormsConsumer(state);
   }
   
   @Override
-  public PerDocProducer docsProducer(SegmentReadState state) throws IOException {
-    return new SimpleTextNormsPerDocProducer(state,
-        BytesRef.getUTF8SortedAsUnicodeComparator());
+  public DocValuesProducer normsProducer(SegmentReadState state) throws IOException {
+    return new SimpleTextNormsProducer(state);
   }
   
   /**
@@ -60,29 +52,12 @@
    * 
    * @lucene.experimental
    */
-  public static class SimpleTextNormsPerDocProducer extends
-      SimpleTextPerDocProducer {
-    
-    public SimpleTextNormsPerDocProducer(SegmentReadState state,
-        Comparator<BytesRef> comp) throws IOException {
-      super(state, comp, NORMS_SEG_SUFFIX);
+  public static class SimpleTextNormsProducer extends SimpleTextDocValuesReader {
+    public SimpleTextNormsProducer(SegmentReadState state) throws IOException {
+      // All we do is change the extension from .dat -> .len;
+      // otherwise this is a normal simple doc values file:
+      super(state, NORMS_SEG_EXTENSION);
     }
-    
-    @Override
-    protected boolean canLoad(FieldInfo info) {
-      return info.hasNorms();
-    }
-    
-    @Override
-    protected Type getDocValuesType(FieldInfo info) {
-      return info.getNormType();
-    }
-    
-    @Override
-    protected boolean anyDocValuesFields(FieldInfos infos) {
-      return infos.hasNorms();
-    }
-    
   }
   
   /**
@@ -92,33 +67,11 @@
    * 
    * @lucene.experimental
    */
-  public static class SimpleTextNormsPerDocConsumer extends
-      SimpleTextPerDocConsumer {
-    
-    public SimpleTextNormsPerDocConsumer(PerDocWriteState state) {
-      super(state, NORMS_SEG_SUFFIX);
+  public static class SimpleTextNormsConsumer extends SimpleTextDocValuesWriter {
+    public SimpleTextNormsConsumer(SegmentWriteState state) throws IOException {
+      // All we do is change the extension from .dat -> .len;
+      // otherwise this is a normal simple doc values file:
+      super(state, NORMS_SEG_EXTENSION);
     }
-    
-    @Override
-    protected DocValues getDocValuesForMerge(AtomicReader reader, FieldInfo info)
-        throws IOException {
-      return reader.normValues(info.name);
-    }
-    
-    @Override
-    protected boolean canMerge(FieldInfo info) {
-      return info.hasNorms();
-    }
-    
-    @Override
-    protected Type getDocValuesType(FieldInfo info) {
-      return info.getNormType();
-    }
-    
-    @Override
-    public void abort() {
-      // We don't have to remove files here: IndexFileDeleter
-      // will do so
-    }
   }
 }
Index: lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextCodec.java
===================================================================
--- lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextCodec.java	(revision 1442822)
+++ lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextCodec.java	(working copy)
@@ -18,12 +18,12 @@
  */
 
 import org.apache.lucene.codecs.Codec;
-import org.apache.lucene.codecs.DocValuesFormat;
 import org.apache.lucene.codecs.FieldInfosFormat;
 import org.apache.lucene.codecs.LiveDocsFormat;
-import org.apache.lucene.codecs.NormsFormat;
 import org.apache.lucene.codecs.PostingsFormat;
 import org.apache.lucene.codecs.SegmentInfoFormat;
+import org.apache.lucene.codecs.DocValuesFormat;
+import org.apache.lucene.codecs.NormsFormat;
 import org.apache.lucene.codecs.StoredFieldsFormat;
 import org.apache.lucene.codecs.TermVectorsFormat;
 
@@ -39,11 +39,9 @@
   private final SegmentInfoFormat segmentInfos = new SimpleTextSegmentInfoFormat();
   private final FieldInfosFormat fieldInfosFormat = new SimpleTextFieldInfosFormat();
   private final TermVectorsFormat vectorsFormat = new SimpleTextTermVectorsFormat();
-  // TODO: need a plain-text impl
-  private final DocValuesFormat docValues = new SimpleTextDocValuesFormat();
-  // TODO: need a plain-text impl (using the above)
   private final NormsFormat normsFormat = new SimpleTextNormsFormat();
   private final LiveDocsFormat liveDocs = new SimpleTextLiveDocsFormat();
+  private final DocValuesFormat dvFormat = new SimpleTextDocValuesFormat();
   
   public SimpleTextCodec() {
     super("SimpleText");
@@ -55,11 +53,6 @@
   }
 
   @Override
-  public DocValuesFormat docValuesFormat() {
-    return docValues;
-  }
-
-  @Override
   public StoredFieldsFormat storedFieldsFormat() {
     return storedFields;
   }
@@ -88,4 +81,9 @@
   public LiveDocsFormat liveDocsFormat() {
     return liveDocs;
   }
+
+  @Override
+  public DocValuesFormat docValuesFormat() {
+    return dvFormat;
+  }
 }
Index: lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextFieldInfosWriter.java
===================================================================
--- lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextFieldInfosWriter.java	(revision 1442822)
+++ lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextFieldInfosWriter.java	(working copy)
@@ -20,8 +20,8 @@
 import java.util.Map;
 
 import org.apache.lucene.codecs.FieldInfosWriter;
-import org.apache.lucene.index.DocValues;
 import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.FieldInfo.DocValuesType;
 import org.apache.lucene.index.FieldInfos;
 import org.apache.lucene.index.IndexFileNames;
 import org.apache.lucene.index.FieldInfo.IndexOptions;
@@ -137,7 +137,7 @@
     }
   }
   
-  private static String getDocValuesType(DocValues.Type type) {
+  private static String getDocValuesType(DocValuesType type) {
     return type == null ? "false" : type.toString();
   }
 }
Index: lucene/codecs/src/java/org/apache/lucene/codecs/bloom/BloomFilteringPostingsFormat.java
===================================================================
--- lucene/codecs/src/java/org/apache/lucene/codecs/bloom/BloomFilteringPostingsFormat.java	(revision 1442822)
+++ lucene/codecs/src/java/org/apache/lucene/codecs/bloom/BloomFilteringPostingsFormat.java	(working copy)
@@ -163,7 +163,7 @@
       IndexInput bloomIn = null;
       boolean success = false;
       try {
-        bloomIn = state.dir.openInput(bloomFileName, state.context);
+        bloomIn = state.directory.openInput(bloomFileName, state.context);
         CodecUtil.checkHeader(bloomIn, BLOOM_CODEC_NAME, BLOOM_CODEC_VERSION,
             BLOOM_CODEC_VERSION);
         // // Load the hash function used in the BloomFilter
Index: lucene/codecs/src/java/org/apache/lucene/codecs/memory/MemoryPostingsFormat.java
===================================================================
--- lucene/codecs/src/java/org/apache/lucene/codecs/memory/MemoryPostingsFormat.java	(revision 1442822)
+++ lucene/codecs/src/java/org/apache/lucene/codecs/memory/MemoryPostingsFormat.java	(working copy)
@@ -842,7 +842,7 @@
   @Override
   public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {
     final String fileName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, EXTENSION);
-    final IndexInput in = state.dir.openInput(fileName, IOContext.READONCE);
+    final IndexInput in = state.directory.openInput(fileName, IOContext.READONCE);
 
     final SortedMap<String,TermsReader> fields = new TreeMap<String,TermsReader>();
 
Index: lucene/codecs/src/java/org/apache/lucene/codecs/pulsing/PulsingPostingsFormat.java
===================================================================
--- lucene/codecs/src/java/org/apache/lucene/codecs/pulsing/PulsingPostingsFormat.java	(revision 1442822)
+++ lucene/codecs/src/java/org/apache/lucene/codecs/pulsing/PulsingPostingsFormat.java	(working copy)
@@ -100,7 +100,7 @@
       docsReader = wrappedPostingsBaseFormat.postingsReaderBase(state);
       pulsingReader = new PulsingPostingsReader(docsReader);
       FieldsProducer ret = new BlockTreeTermsReader(
-                                                    state.dir, state.fieldInfos, state.segmentInfo,
+                                                    state.directory, state.fieldInfos, state.segmentInfo,
                                                     pulsingReader,
                                                     state.context,
                                                     state.segmentSuffix,
Index: lucene/codecs/src/java/org/apache/lucene/codecs/diskdv/DiskDocValuesProducer.java
===================================================================
--- lucene/codecs/src/java/org/apache/lucene/codecs/diskdv/DiskDocValuesProducer.java	(revision 0)
+++ lucene/codecs/src/java/org/apache/lucene/codecs/diskdv/DiskDocValuesProducer.java	(working copy)
@@ -0,0 +1,277 @@
+package org.apache.lucene.codecs.diskdv;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.HashMap;
+import java.util.Map;
+
+import org.apache.lucene.codecs.CodecUtil;
+import org.apache.lucene.codecs.DocValuesProducer;
+import org.apache.lucene.index.BinaryDocValues;
+import org.apache.lucene.index.CorruptIndexException;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.NumericDocValues;
+import org.apache.lucene.index.SegmentReadState;
+import org.apache.lucene.index.SortedDocValues;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util.packed.BlockPackedReader;
+import org.apache.lucene.util.packed.MonotonicBlockPackedReader;
+
+class DiskDocValuesProducer extends DocValuesProducer {
+  private final Map<Integer,NumericEntry> numerics;
+  private final Map<Integer,BinaryEntry> binaries;
+  private final Map<Integer,NumericEntry> ords;
+  private final IndexInput data;
+
+  // memory-resident structures
+  private final Map<Integer,BlockPackedReader> ordinalInstances = new HashMap<Integer,BlockPackedReader>();
+  private final Map<Integer,MonotonicBlockPackedReader> addressInstances = new HashMap<Integer,MonotonicBlockPackedReader>();
+  
+  DiskDocValuesProducer(SegmentReadState state, String dataCodec, String dataExtension, String metaCodec, String metaExtension) throws IOException {
+    String metaName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, metaExtension);
+    // read in the entries from the metadata file.
+    IndexInput in = state.directory.openInput(metaName, state.context);
+    boolean success = false;
+    try {
+      CodecUtil.checkHeader(in, metaCodec, 
+                                DiskDocValuesFormat.VERSION_START,
+                                DiskDocValuesFormat.VERSION_START);
+      numerics = new HashMap<Integer,NumericEntry>();
+      ords = new HashMap<Integer,NumericEntry>();
+      binaries = new HashMap<Integer,BinaryEntry>();
+      readFields(in, state.fieldInfos);
+      success = true;
+    } finally {
+      if (success) {
+        IOUtils.close(in);
+      } else {
+        IOUtils.closeWhileHandlingException(in);
+      }
+    }
+    
+    String dataName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, dataExtension);
+    data = state.directory.openInput(dataName, state.context);
+    CodecUtil.checkHeader(data, dataCodec, 
+                                DiskDocValuesFormat.VERSION_START,
+                                DiskDocValuesFormat.VERSION_START);
+  }
+  
+  private void readFields(IndexInput meta, FieldInfos infos) throws IOException {
+    int fieldNumber = meta.readVInt();
+    while (fieldNumber != -1) {
+      byte type = meta.readByte();
+      if (type == DiskDocValuesFormat.NUMERIC) {
+        numerics.put(fieldNumber, readNumericEntry(meta));
+      } else if (type == DiskDocValuesFormat.BINARY) {
+        BinaryEntry b = readBinaryEntry(meta);
+        binaries.put(fieldNumber, b);
+      } else if (type == DiskDocValuesFormat.SORTED) {
+        // sorted = binary + numeric
+        if (meta.readVInt() != fieldNumber) {
+          throw new CorruptIndexException("sorted entry for field: " + fieldNumber + " is corrupt");
+        }
+        if (meta.readByte() != DiskDocValuesFormat.BINARY) {
+          throw new CorruptIndexException("sorted entry for field: " + fieldNumber + " is corrupt");
+        }
+        BinaryEntry b = readBinaryEntry(meta);
+        binaries.put(fieldNumber, b);
+        
+        if (meta.readVInt() != fieldNumber) {
+          throw new CorruptIndexException("sorted entry for field: " + fieldNumber + " is corrupt");
+        }
+        if (meta.readByte() != DiskDocValuesFormat.NUMERIC) {
+          throw new CorruptIndexException("sorted entry for field: " + fieldNumber + " is corrupt");
+        }
+        NumericEntry n = readNumericEntry(meta);
+        ords.put(fieldNumber, n);
+      }
+      fieldNumber = meta.readVInt();
+    }
+  }
+  
+  static NumericEntry readNumericEntry(IndexInput meta) throws IOException {
+    NumericEntry entry = new NumericEntry();
+    entry.packedIntsVersion = meta.readVInt();
+    entry.offset = meta.readLong();
+    entry.count = meta.readVInt();
+    entry.blockSize = meta.readVInt();
+    return entry;
+  }
+  
+  static BinaryEntry readBinaryEntry(IndexInput meta) throws IOException {
+    BinaryEntry entry = new BinaryEntry();
+    entry.minLength = meta.readVInt();
+    entry.maxLength = meta.readVInt();
+    entry.count = meta.readVInt();
+    entry.offset = meta.readLong();
+    if (entry.minLength != entry.maxLength) {
+      entry.addressesOffset = meta.readLong();
+      entry.packedIntsVersion = meta.readVInt();
+      entry.blockSize = meta.readVInt();
+    }
+    return entry;
+  }
+
+  @Override
+  public NumericDocValues getNumeric(FieldInfo field) throws IOException {
+    NumericEntry entry = numerics.get(field.number);
+    final IndexInput data = this.data.clone();
+    data.seek(entry.offset);
+
+    final BlockPackedReader reader = new BlockPackedReader(data, entry.packedIntsVersion, entry.blockSize, entry.count, true);
+    return new NumericDocValues() {
+      @Override
+      public long get(int docID) {
+        return reader.get(docID);
+      }
+    };
+  }
+
+  @Override
+  public BinaryDocValues getBinary(FieldInfo field) throws IOException {
+    BinaryEntry bytes = binaries.get(field.number);
+    if (bytes.minLength == bytes.maxLength) {
+      return getFixedBinary(field, bytes);
+    } else {
+      return getVariableBinary(field, bytes);
+    }
+  }
+  
+  private BinaryDocValues getFixedBinary(FieldInfo field, final BinaryEntry bytes) {
+    final IndexInput data = this.data.clone();
+
+    return new BinaryDocValues() {
+      @Override
+      public void get(int docID, BytesRef result) {
+        long address = bytes.offset + docID * (long)bytes.maxLength;
+        try {
+          data.seek(address);
+          // NOTE: we could have one buffer, but various consumers (e.g. FieldComparatorSource) 
+          // assume "they" own the bytes after calling this!
+          final byte[] buffer = new byte[bytes.maxLength];
+          data.readBytes(buffer, 0, buffer.length);
+          result.bytes = buffer;
+          result.offset = 0;
+          result.length = buffer.length;
+        } catch (IOException e) {
+          throw new RuntimeException(e);
+        }
+      }
+    };
+  }
+  
+  private BinaryDocValues getVariableBinary(FieldInfo field, final BinaryEntry bytes) throws IOException {
+    final IndexInput data = this.data.clone();
+    
+    final MonotonicBlockPackedReader addresses;
+    synchronized (addressInstances) {
+      MonotonicBlockPackedReader addrInstance = addressInstances.get(field.number);
+      if (addrInstance == null) {
+        data.seek(bytes.addressesOffset);
+        addrInstance = new MonotonicBlockPackedReader(data, bytes.packedIntsVersion, bytes.blockSize, bytes.count, false);
+        addressInstances.put(field.number, addrInstance);
+      }
+      addresses = addrInstance;
+    }
+
+    return new BinaryDocValues() {
+      @Override
+      public void get(int docID, BytesRef result) {
+        long startAddress = bytes.offset + (docID == 0 ? 0 : + addresses.get(docID-1));
+        long endAddress = bytes.offset + addresses.get(docID);
+        int length = (int) (endAddress - startAddress);
+        try {
+          data.seek(startAddress);
+          // NOTE: we could have one buffer, but various consumers (e.g. FieldComparatorSource) 
+          // assume "they" own the bytes after calling this!
+          final byte[] buffer = new byte[length];
+          data.readBytes(buffer, 0, buffer.length);
+          result.bytes = buffer;
+          result.offset = 0;
+          result.length = length;
+        } catch (IOException e) {
+          throw new RuntimeException(e);
+        }
+      }
+    };
+  }
+
+  @Override
+  public SortedDocValues getSorted(FieldInfo field) throws IOException {
+    final int valueCount = binaries.get(field.number).count;
+    final BinaryDocValues binary = getBinary(field);
+    final BlockPackedReader ordinals;
+    synchronized (ordinalInstances) {
+      BlockPackedReader ordsInstance = ordinalInstances.get(field.number);
+      if (ordsInstance == null) {
+        NumericEntry entry = ords.get(field.number);
+        IndexInput data = this.data.clone();
+        data.seek(entry.offset);
+        ordsInstance = new BlockPackedReader(data, entry.packedIntsVersion, entry.blockSize, entry.count, false);
+        ordinalInstances.put(field.number, ordsInstance);
+      }
+      ordinals = ordsInstance;
+    }
+    return new SortedDocValues() {
+
+      @Override
+      public int getOrd(int docID) {
+        return (int) ordinals.get(docID);
+      }
+
+      @Override
+      public void lookupOrd(int ord, BytesRef result) {
+        binary.get(ord, result);
+      }
+
+      @Override
+      public int getValueCount() {
+        return valueCount;
+      }
+    };
+  }
+
+  @Override
+  public void close() throws IOException {
+    data.close();
+  }
+  
+  static class NumericEntry {
+    long offset;
+
+    int packedIntsVersion;
+    int count;
+    int blockSize;
+  }
+  
+  static class BinaryEntry {
+    long offset;
+
+    int count;
+    int minLength;
+    int maxLength;
+    long addressesOffset;
+    int packedIntsVersion;
+    int blockSize;
+  }
+}

Property changes on: lucene/codecs/src/java/org/apache/lucene/codecs/diskdv/DiskDocValuesProducer.java
___________________________________________________________________
Added: svn:eol-style
## -0,0 +1 ##
+native
Index: lucene/codecs/src/java/org/apache/lucene/codecs/diskdv/DiskDocValuesFormat.java
===================================================================
--- lucene/codecs/src/java/org/apache/lucene/codecs/diskdv/DiskDocValuesFormat.java	(revision 0)
+++ lucene/codecs/src/java/org/apache/lucene/codecs/diskdv/DiskDocValuesFormat.java	(working copy)
@@ -0,0 +1,61 @@
+package org.apache.lucene.codecs.diskdv;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.codecs.DocValuesConsumer;
+import org.apache.lucene.codecs.DocValuesProducer;
+import org.apache.lucene.codecs.DocValuesFormat;
+import org.apache.lucene.index.SegmentReadState;
+import org.apache.lucene.index.SegmentWriteState;
+
+/**
+ * DocValues format that keeps most things on disk.
+ * <p>
+ * Things like ordinals and disk offsets are loaded into ram,
+ * for single-seek access to all the types.
+ * <p>
+ * @lucene.experimental
+ */
+public final class DiskDocValuesFormat extends DocValuesFormat {
+
+  public DiskDocValuesFormat() {
+    super("Disk");
+  }
+
+  @Override
+  public DocValuesConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
+    return new DiskDocValuesConsumer(state, DATA_CODEC, DATA_EXTENSION, META_CODEC, META_EXTENSION);
+  }
+
+  @Override
+  public DocValuesProducer fieldsProducer(SegmentReadState state) throws IOException {
+    return new DiskDocValuesProducer(state, DATA_CODEC, DATA_EXTENSION, META_CODEC, META_EXTENSION);
+  }
+  
+  public static final String DATA_CODEC = "DiskDocValuesData";
+  public static final String DATA_EXTENSION = "dvdd";
+  public static final String META_CODEC = "DiskDocValuesMetadata";
+  public static final String META_EXTENSION = "dvdm";
+  public static final int VERSION_START = 0;
+  public static final int VERSION_CURRENT = VERSION_START;
+  public static final byte NUMERIC = 0;
+  public static final byte BINARY = 1;
+  public static final byte SORTED = 2;
+}

Property changes on: lucene/codecs/src/java/org/apache/lucene/codecs/diskdv/DiskDocValuesFormat.java
___________________________________________________________________
Added: svn:eol-style
## -0,0 +1 ##
+native
Index: lucene/codecs/src/java/org/apache/lucene/codecs/diskdv/DiskDocValuesConsumer.java
===================================================================
--- lucene/codecs/src/java/org/apache/lucene/codecs/diskdv/DiskDocValuesConsumer.java	(revision 0)
+++ lucene/codecs/src/java/org/apache/lucene/codecs/diskdv/DiskDocValuesConsumer.java	(working copy)
@@ -0,0 +1,142 @@
+package org.apache.lucene.codecs.diskdv;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.codecs.CodecUtil;
+import org.apache.lucene.codecs.DocValuesConsumer;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.SegmentWriteState;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util.packed.BlockPackedWriter;
+import org.apache.lucene.util.packed.MonotonicBlockPackedWriter;
+import org.apache.lucene.util.packed.PackedInts;
+
+/** writer for {@link DiskDocValuesFormat} */
+public class DiskDocValuesConsumer extends DocValuesConsumer {
+
+  static final int BLOCK_SIZE = 16384;
+
+  final IndexOutput data, meta;
+  final int maxDoc;
+  
+  public DiskDocValuesConsumer(SegmentWriteState state, String dataCodec, String dataExtension, String metaCodec, String metaExtension) throws IOException {
+    boolean success = false;
+    try {
+      String dataName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, dataExtension);
+      data = state.directory.createOutput(dataName, state.context);
+      CodecUtil.writeHeader(data, dataCodec, DiskDocValuesFormat.VERSION_CURRENT);
+      String metaName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, metaExtension);
+      meta = state.directory.createOutput(metaName, state.context);
+      CodecUtil.writeHeader(meta, metaCodec, DiskDocValuesFormat.VERSION_CURRENT);
+      maxDoc = state.segmentInfo.getDocCount();
+      success = true;
+    } finally {
+      if (!success) {
+        IOUtils.closeWhileHandlingException(this);
+      }
+    }
+  }
+  
+  @Override
+  public void addNumericField(FieldInfo field, Iterable<Number> values) throws IOException {
+    int count = 0;
+    for (@SuppressWarnings("unused") Number nv : values) {
+      ++count;
+    }
+
+    meta.writeVInt(field.number);
+    meta.writeByte(DiskDocValuesFormat.NUMERIC);
+    meta.writeVInt(PackedInts.VERSION_CURRENT);
+    meta.writeLong(data.getFilePointer());
+    meta.writeVInt(count);
+    meta.writeVInt(BLOCK_SIZE);
+
+    final BlockPackedWriter writer = new BlockPackedWriter(data, BLOCK_SIZE);
+    for (Number nv : values) {
+      writer.add(nv.longValue());
+    }
+    writer.finish();
+  }
+
+  @Override
+  public void addBinaryField(FieldInfo field, final Iterable<BytesRef> values) throws IOException {
+    // write the byte[] data
+    meta.writeVInt(field.number);
+    meta.writeByte(DiskDocValuesFormat.BINARY);
+    int minLength = Integer.MAX_VALUE;
+    int maxLength = Integer.MIN_VALUE;
+    final long startFP = data.getFilePointer();
+    int count = 0;
+    for(BytesRef v : values) {
+      minLength = Math.min(minLength, v.length);
+      maxLength = Math.max(maxLength, v.length);
+      data.writeBytes(v.bytes, v.offset, v.length);
+      count++;
+    }
+    meta.writeVInt(minLength);
+    meta.writeVInt(maxLength);
+    meta.writeVInt(count);
+    meta.writeLong(startFP);
+    
+    // if minLength == maxLength, its a fixed-length byte[], we are done (the addresses are implicit)
+    // otherwise, we need to record the length fields...
+    if (minLength != maxLength) {
+      meta.writeLong(data.getFilePointer());
+      meta.writeVInt(PackedInts.VERSION_CURRENT);
+      meta.writeVInt(BLOCK_SIZE);
+
+      final MonotonicBlockPackedWriter writer = new MonotonicBlockPackedWriter(data, BLOCK_SIZE);
+      long addr = 0;
+      for (BytesRef v : values) {
+        addr += v.length;
+        writer.add(addr);
+      }
+      writer.finish();
+    }
+  }
+
+  @Override
+  public void addSortedField(FieldInfo field, Iterable<BytesRef> values, Iterable<Number> docToOrd) throws IOException {
+    meta.writeVInt(field.number);
+    meta.writeByte(DiskDocValuesFormat.SORTED);
+    addBinaryField(field, values);
+    addNumericField(field, docToOrd);
+  }
+  
+  @Override
+  public void close() throws IOException {
+    boolean success = false;
+    try {
+      if (meta != null) {
+        meta.writeVInt(-1); // write EOF marker
+      }
+      success = true;
+    } finally {
+      if (success) {
+        IOUtils.close(data, meta);
+      } else {
+        IOUtils.closeWhileHandlingException(data, meta);
+      }
+    }
+  }
+}

Property changes on: lucene/codecs/src/java/org/apache/lucene/codecs/diskdv/DiskDocValuesConsumer.java
___________________________________________________________________
Added: svn:eol-style
## -0,0 +1 ##
+native
Index: lucene/codecs/src/resources/META-INF/services/org.apache.lucene.codecs.DocValuesFormat
===================================================================
--- lucene/codecs/src/resources/META-INF/services/org.apache.lucene.codecs.DocValuesFormat	(revision 0)
+++ lucene/codecs/src/resources/META-INF/services/org.apache.lucene.codecs.DocValuesFormat	(working copy)
@@ -0,0 +1,17 @@
+#  Licensed to the Apache Software Foundation (ASF) under one or more
+#  contributor license agreements.  See the NOTICE file distributed with
+#  this work for additional information regarding copyright ownership.
+#  The ASF licenses this file to You under the Apache License, Version 2.0
+#  (the "License"); you may not use this file except in compliance with
+#  the License.  You may obtain a copy of the License at
+#
+#       http://www.apache.org/licenses/LICENSE-2.0
+#
+#  Unless required by applicable law or agreed to in writing, software
+#  distributed under the License is distributed on an "AS IS" BASIS,
+#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+#  See the License for the specific language governing permissions and
+#  limitations under the License.
+
+org.apache.lucene.codecs.diskdv.DiskDocValuesFormat
+org.apache.lucene.codecs.simpletext.SimpleTextDocValuesFormat
Index: lucene/codecs/src/test/org/apache/lucene/codecs/diskdv/TestDiskDocValuesFormat.java
===================================================================
--- lucene/codecs/src/test/org/apache/lucene/codecs/diskdv/TestDiskDocValuesFormat.java	(revision 0)
+++ lucene/codecs/src/test/org/apache/lucene/codecs/diskdv/TestDiskDocValuesFormat.java	(working copy)
@@ -0,0 +1,34 @@
+package org.apache.lucene.codecs.diskdv;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.codecs.Codec;
+import org.apache.lucene.index.BaseDocValuesFormatTestCase;
+import org.apache.lucene.util._TestUtil;
+
+/**
+ * Tests DiskDocValuesFormat
+ */
+public class TestDiskDocValuesFormat extends BaseDocValuesFormatTestCase {
+  private final Codec codec = _TestUtil.alwaysDocValuesFormat(new DiskDocValuesFormat());
+
+  @Override
+  protected Codec getCodec() {
+    return codec;
+  }
+}

Property changes on: lucene/codecs/src/test/org/apache/lucene/codecs/diskdv/TestDiskDocValuesFormat.java
___________________________________________________________________
Added: svn:eol-style
## -0,0 +1 ##
+native
Index: lucene/codecs/src/test/org/apache/lucene/codecs/simpletext/TestSimpleTextDocValuesFormat.java
===================================================================
--- lucene/codecs/src/test/org/apache/lucene/codecs/simpletext/TestSimpleTextDocValuesFormat.java	(revision 0)
+++ lucene/codecs/src/test/org/apache/lucene/codecs/simpletext/TestSimpleTextDocValuesFormat.java	(working copy)
@@ -0,0 +1,33 @@
+package org.apache.lucene.codecs.simpletext;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.codecs.Codec;
+import org.apache.lucene.index.BaseDocValuesFormatTestCase;
+
+/**
+ * Tests SimpleTextDocValuesFormat
+ */
+public class TestSimpleTextDocValuesFormat extends BaseDocValuesFormatTestCase {
+  private final Codec codec = new SimpleTextCodec();
+
+  @Override
+  protected Codec getCodec() {
+    return codec;
+  }
+}

Property changes on: lucene/codecs/src/test/org/apache/lucene/codecs/simpletext/TestSimpleTextDocValuesFormat.java
___________________________________________________________________
Added: svn:eol-style
## -0,0 +1 ##
+native
Index: lucene/queries/src/test/org/apache/lucene/queries/TestCustomScoreQuery.java
===================================================================
--- lucene/queries/src/test/org/apache/lucene/queries/TestCustomScoreQuery.java	(revision 1442822)
+++ lucene/queries/src/test/org/apache/lucene/queries/TestCustomScoreQuery.java	(working copy)
@@ -183,12 +183,12 @@
 
     @Override
     protected CustomScoreProvider getCustomScoreProvider(AtomicReaderContext context) throws IOException {
-      final int[] values = FieldCache.DEFAULT.getInts(context.reader(), INT_FIELD, false);
+      final FieldCache.Ints values = FieldCache.DEFAULT.getInts(context.reader(), INT_FIELD, false);
       return new CustomScoreProvider(context) {
         @Override
         public float customScore(int doc, float subScore, float valSrcScore) {
           assertTrue(doc <= context.reader().maxDoc());
-          return values[doc];
+          return values.get(doc);
         }
       };
     }
Index: lucene/queries/src/test/org/apache/lucene/queries/function/TestDocValuesFieldSources.java
===================================================================
--- lucene/queries/src/test/org/apache/lucene/queries/function/TestDocValuesFieldSources.java	(revision 1442822)
+++ lucene/queries/src/test/org/apache/lucene/queries/function/TestDocValuesFieldSources.java	(working copy)
@@ -18,29 +18,20 @@
  */
 
 import java.io.IOException;
-import java.util.Date;
 
 import org.apache.lucene.analysis.MockAnalyzer;
-import org.apache.lucene.document.ByteDocValuesField;
-import org.apache.lucene.document.DerefBytesDocValuesField;
+import org.apache.lucene.document.BinaryDocValuesField;
 import org.apache.lucene.document.Document;
-import org.apache.lucene.document.DoubleDocValuesField;
 import org.apache.lucene.document.Field;
-import org.apache.lucene.document.FloatDocValuesField;
-import org.apache.lucene.document.IntDocValuesField;
-import org.apache.lucene.document.LongDocValuesField;
-import org.apache.lucene.document.PackedLongDocValuesField;
-import org.apache.lucene.document.ShortDocValuesField;
-import org.apache.lucene.document.SortedBytesDocValuesField;
-import org.apache.lucene.document.StraightBytesDocValuesField;
+import org.apache.lucene.document.NumericDocValuesField;
+import org.apache.lucene.document.SortedDocValuesField;
 import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.DirectoryReader;
-import org.apache.lucene.index.DocValues;
+import org.apache.lucene.index.FieldInfo.DocValuesType;
 import org.apache.lucene.index.IndexWriterConfig;
 import org.apache.lucene.index.RandomIndexWriter;
-import org.apache.lucene.queries.function.valuesource.DateDocValuesFieldSource;
-import org.apache.lucene.queries.function.valuesource.NumericDocValuesFieldSource;
-import org.apache.lucene.queries.function.valuesource.StrDocValuesFieldSource;
+import org.apache.lucene.queries.function.valuesource.BytesRefFieldSource;
+import org.apache.lucene.queries.function.valuesource.LongFieldSource;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.LuceneTestCase;
@@ -49,54 +40,25 @@
 
 import com.carrotsearch.randomizedtesting.generators.RandomInts;
 
+
 public class TestDocValuesFieldSources extends LuceneTestCase {
 
-  public void test(DocValues.Type type) throws IOException {
+  public void test(DocValuesType type) throws IOException {
     Directory d = newDirectory();
     IndexWriterConfig iwConfig = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
     final int nDocs = atLeast(50);
-    final Field id = new IntDocValuesField("id", 0);
+    final Field id = new NumericDocValuesField("id", 0);
     final Field f;
     switch (type) {
-      case BYTES_FIXED_DEREF:
-        f = new DerefBytesDocValuesField("dv", new BytesRef(), true);
+      case BINARY:
+        f = new BinaryDocValuesField("dv", new BytesRef());
         break;
-      case BYTES_FIXED_SORTED:
-        f = new SortedBytesDocValuesField("dv", new BytesRef(), true);
+      case SORTED:
+        f = new SortedDocValuesField("dv", new BytesRef());
         break;
-      case BYTES_FIXED_STRAIGHT:
-        f = new StraightBytesDocValuesField("dv", new BytesRef(), true);
+      case NUMERIC:
+        f = new NumericDocValuesField("dv", 0);
         break;
-      case BYTES_VAR_DEREF:
-        f = new DerefBytesDocValuesField("dv", new BytesRef(), false);
-        break;
-      case BYTES_VAR_SORTED:
-        f = new SortedBytesDocValuesField("dv", new BytesRef(), false);
-        break;
-      case BYTES_VAR_STRAIGHT:
-        f = new StraightBytesDocValuesField("dv", new BytesRef(), false);
-        break;
-      case FIXED_INTS_8:
-        f = new ByteDocValuesField("dv", (byte) 0);
-        break;
-      case FIXED_INTS_16:
-        f = new ShortDocValuesField("dv", (short) 0);
-        break;
-      case FIXED_INTS_32:
-        f = new IntDocValuesField("dv", 0);
-        break;
-      case FIXED_INTS_64:
-        f = new LongDocValuesField("dv", 0L);
-        break;
-      case VAR_INTS:
-        f = new PackedLongDocValuesField("dv", 0L);
-        break;
-      case FLOAT_32:
-        f = new FloatDocValuesField("dv", 0f);
-        break;
-      case FLOAT_64:
-        f = new DoubleDocValuesField("dv", 0d);
-        break;
       default:
         throw new AssertionError();
     }
@@ -108,46 +70,20 @@
 
     RandomIndexWriter iw = new RandomIndexWriter(random(), d, iwConfig);
     for (int i = 0; i < nDocs; ++i) {
-      id.setIntValue(i);
+      id.setLongValue(i);
       switch (type) {
-        case BYTES_FIXED_DEREF:
-        case BYTES_FIXED_SORTED:
-        case BYTES_FIXED_STRAIGHT:
-          vals[i] = _TestUtil.randomFixedByteLengthUnicodeString(random(), 10);
+        case SORTED:
+        case BINARY:
+          do {
+            vals[i] = _TestUtil.randomSimpleString(random(), 20);
+          } while (((String) vals[i]).isEmpty());
           f.setBytesValue(new BytesRef((String) vals[i]));
           break;
-        case BYTES_VAR_DEREF:
-        case BYTES_VAR_SORTED:
-        case BYTES_VAR_STRAIGHT:
-          vals[i] = _TestUtil.randomSimpleString(random(), 20);
-          f.setBytesValue(new BytesRef((String) vals[i]));
-          break;
-        case FIXED_INTS_8:
-          vals[i] = (byte) random().nextInt(256);
-          f.setByteValue((Byte) vals[i]);
-          break;
-        case FIXED_INTS_16:
-          vals[i] = (short) random().nextInt(1 << 16);
-          f.setShortValue((Short) vals[i]);
-          break;
-        case FIXED_INTS_32:
-          vals[i] = random().nextInt();
-          f.setIntValue((Integer) vals[i]);
-          break;
-        case FIXED_INTS_64:
-        case VAR_INTS:
+        case NUMERIC:
           final int bitsPerValue = RandomInts.randomIntBetween(random(), 1, 31); // keep it an int
           vals[i] = (long) random().nextInt((int) PackedInts.maxValue(bitsPerValue));
           f.setLongValue((Long) vals[i]);
           break;
-        case FLOAT_32:
-          vals[i] = random().nextFloat();
-          f.setFloatValue((Float) vals[i]);
-          break;
-        case FLOAT_64:
-          vals[i] = random().nextDouble();
-          f.setDoubleValue((Double) vals[i]);
-          break;
       }
       iw.addDocument(document);
       if (random().nextBoolean() && i % 10 == 9) {
@@ -158,33 +94,16 @@
 
     DirectoryReader rd = DirectoryReader.open(d);
     for (AtomicReaderContext leave : rd.leaves()) {
-      final FunctionValues ids = new NumericDocValuesFieldSource("id", false).getValues(null, leave);
+      final FunctionValues ids = new LongFieldSource("id").getValues(null, leave);
       final ValueSource vs;
-      final boolean direct = random().nextBoolean();
       switch (type) {
-        case BYTES_FIXED_DEREF:
-        case BYTES_FIXED_SORTED:
-        case BYTES_FIXED_STRAIGHT:
-        case BYTES_VAR_DEREF:
-        case BYTES_VAR_SORTED:
-        case BYTES_VAR_STRAIGHT:
-          vs = new StrDocValuesFieldSource("dv", direct);
+        case BINARY:
+        case SORTED:
+          vs = new BytesRefFieldSource("dv");
           break;
-        case FLOAT_32:
-        case FLOAT_64:
-        case FIXED_INTS_8:
-        case FIXED_INTS_16:
-        case FIXED_INTS_32:
-          vs = new NumericDocValuesFieldSource("dv", direct);
+        case NUMERIC:
+          vs = new LongFieldSource("dv");
           break;
-        case FIXED_INTS_64:
-        case VAR_INTS:
-          if (random().nextBoolean()) {
-            vs = new NumericDocValuesFieldSource("dv", direct);
-          } else {
-            vs = new DateDocValuesFieldSource("dv", direct);
-          }
-          break;
         default:
           throw new AssertionError();
       }
@@ -192,61 +111,21 @@
       BytesRef bytes = new BytesRef();
       for (int i = 0; i < leave.reader().maxDoc(); ++i) {
         assertTrue(values.exists(i));
-        if (vs instanceof StrDocValuesFieldSource) {
+        if (vs instanceof BytesRefFieldSource) {
           assertTrue(values.objectVal(i) instanceof String);
-        } else if (vs instanceof NumericDocValuesFieldSource) {
-          assertTrue(values.objectVal(i) instanceof Number);
-          switch (type) {
-            case FIXED_INTS_8:
-              assertTrue(values.objectVal(i) instanceof Byte);
-              assertTrue(values.bytesVal(i, bytes));
-              assertEquals(1, bytes.length);
-              break;
-            case FIXED_INTS_16:
-              assertTrue(values.objectVal(i) instanceof Short);
-              assertTrue(values.bytesVal(i, bytes));
-              assertEquals(2, bytes.length);
-              break;
-            case FIXED_INTS_32:
-              assertTrue(values.objectVal(i) instanceof Integer);
-              assertTrue(values.bytesVal(i, bytes));
-              assertEquals(4, bytes.length);
-              break;
-            case FIXED_INTS_64:
-            case VAR_INTS:
-              assertTrue(values.objectVal(i) instanceof Long);
-              assertTrue(values.bytesVal(i, bytes));
-              assertEquals(8, bytes.length);
-              break;
-            case FLOAT_32:
-              assertTrue(values.objectVal(i) instanceof Float);
-              assertTrue(values.bytesVal(i, bytes));
-              assertEquals(4, bytes.length);
-              break;
-            case FLOAT_64:
-              assertTrue(values.objectVal(i) instanceof Double);
-              assertTrue(values.bytesVal(i, bytes));
-              assertEquals(8, bytes.length);
-              break;
-            default:
-              throw new AssertionError();
-          }
-        } else if (vs instanceof DateDocValuesFieldSource) {
-          assertTrue(values.objectVal(i) instanceof Date);
+        } else if (vs instanceof LongFieldSource) {
+          assertTrue(values.objectVal(i) instanceof Long);
+          assertTrue(values.bytesVal(i, bytes));
         } else {
           throw new AssertionError();
         }
         
         Object expected = vals[ids.intVal(i)];
         switch (type) {
-          case BYTES_VAR_SORTED:
-          case BYTES_FIXED_SORTED:
+          case SORTED:
             values.ordVal(i); // no exception
             assertTrue(values.numOrd() >= 1);
-          case BYTES_FIXED_DEREF:
-          case BYTES_FIXED_STRAIGHT:
-          case BYTES_VAR_DEREF:
-          case BYTES_VAR_STRAIGHT:
+          case BINARY:
             assertEquals(expected, values.objectVal(i));
             assertEquals(expected, values.strVal(i));
             assertEquals(expected, values.objectVal(i));
@@ -254,17 +133,7 @@
             assertTrue(values.bytesVal(i, bytes));
             assertEquals(new BytesRef((String) expected), bytes);
             break;
-          case FLOAT_32:
-            assertEquals(((Number) expected).floatValue(), values.floatVal(i), 0.001);
-            break;
-          case FLOAT_64:
-            assertEquals(((Number) expected).doubleValue(), values.doubleVal(i), 0.001d);
-            break;
-          case FIXED_INTS_8:
-          case FIXED_INTS_16:
-          case FIXED_INTS_32:
-          case FIXED_INTS_64:
-          case VAR_INTS:
+          case NUMERIC:
             assertEquals(((Number) expected).longValue(), values.longVal(i));
             break;
         }
@@ -275,7 +144,7 @@
   }
 
   public void test() throws IOException {
-    for (DocValues.Type type : DocValues.Type.values()) {
+    for (DocValuesType type : DocValuesType.values()) {
       test(type);
     }
   }
Index: lucene/queries/src/test/org/apache/lucene/queries/function/TestOrdValues.java
===================================================================
--- lucene/queries/src/test/org/apache/lucene/queries/function/TestOrdValues.java	(revision 1442822)
+++ lucene/queries/src/test/org/apache/lucene/queries/function/TestOrdValues.java	(working copy)
@@ -133,7 +133,7 @@
       String id = s.getIndexReader().document(sd[i].doc).get(ID_FIELD);
       log("-------- " + i + ". Explain doc " + id);
       log(s.explain(q, sd[i].doc));
-      float expectedScore = N_DOCS - i;
+      float expectedScore = N_DOCS - i - 1;
       assertEquals("score of result " + i + " shuould be " + expectedScore + " != " + score, expectedScore, score, TEST_SCORE_TOLERANCE_DELTA);
       String expectedId = inOrder
               ? id2String(N_DOCS - i) // in-order ==> larger  values first
Index: lucene/queries/src/java/org/apache/lucene/queries/function/docvalues/DocTermsIndexDocValues.java
===================================================================
--- lucene/queries/src/java/org/apache/lucene/queries/function/docvalues/DocTermsIndexDocValues.java	(revision 1442822)
+++ lucene/queries/src/java/org/apache/lucene/queries/function/docvalues/DocTermsIndexDocValues.java	(working copy)
@@ -17,26 +17,29 @@
 
 package org.apache.lucene.queries.function.docvalues;
 
+import java.io.IOException;
+
+import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.SortedDocValues;
 import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.queries.function.ValueSourceScorer;
 import org.apache.lucene.search.FieldCache;
-import org.apache.lucene.index.AtomicReaderContext;
-import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.util.Bits;
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.CharsRef;
 import org.apache.lucene.util.UnicodeUtil;
 import org.apache.lucene.util.mutable.MutableValue;
 import org.apache.lucene.util.mutable.MutableValueStr;
 
-import java.io.IOException;
-
 /**
- * Internal class, subject to change.
  * Serves as base class for FunctionValues based on DocTermsIndex.
+ * @lucene.internal
  */
 public abstract class DocTermsIndexDocValues extends FunctionValues {
-  protected final FieldCache.DocTermsIndex termsIndex;
+  protected final SortedDocValues termsIndex;
+  protected final Bits valid;
   protected final ValueSource vs;
   protected final MutableValueStr val = new MutableValueStr();
   protected final BytesRef spare = new BytesRef();
@@ -45,40 +48,42 @@
   public DocTermsIndexDocValues(ValueSource vs, AtomicReaderContext context, String field) throws IOException {
     try {
       termsIndex = FieldCache.DEFAULT.getTermsIndex(context.reader(), field);
+      valid = FieldCache.DEFAULT.getDocsWithField(context.reader(), field);
     } catch (RuntimeException e) {
       throw new DocTermsIndexException(field, e);
     }
     this.vs = vs;
   }
 
-  public FieldCache.DocTermsIndex getDocTermsIndex() {
-    return termsIndex;
-  }
-
   protected abstract String toTerm(String readableValue);
 
   @Override
   public boolean exists(int doc) {
-    return termsIndex.getOrd(doc) != 0;
+    return valid.get(doc);
   }
 
+  @Override
+  public int ordVal(int doc) {
+    return termsIndex.getOrd(doc);
+  }
 
   @Override
+  public int numOrd() {
+    return termsIndex.getValueCount();
+  }
+
+  @Override
   public boolean bytesVal(int doc, BytesRef target) {
-    int ord=termsIndex.getOrd(doc);
-    if (ord==0) {
-      target.length = 0;
-      return false;
-    }
-    termsIndex.lookup(ord, target);
-    return true;
+    termsIndex.get(doc, target);
+    return target.length > 0;
   }
 
   @Override
   public String strVal(int doc) {
-    int ord=termsIndex.getOrd(doc);
-    if (ord==0) return null;
-    termsIndex.lookup(ord, spare);
+    termsIndex.get(doc, spare);
+    if (spare.length == 0) {
+      return null;
+    }
     UnicodeUtil.UTF8toUTF16(spare, spareChars);
     return spareChars.toString();
   }
@@ -97,11 +102,9 @@
     lowerVal = lowerVal == null ? null : toTerm(lowerVal);
     upperVal = upperVal == null ? null : toTerm(upperVal);
 
-    final BytesRef spare = new BytesRef();
-
     int lower = Integer.MIN_VALUE;
     if (lowerVal != null) {
-      lower = termsIndex.binarySearchLookup(new BytesRef(lowerVal), spare);
+      lower = termsIndex.lookupTerm(new BytesRef(lowerVal));
       if (lower < 0) {
         lower = -lower-1;
       } else if (!includeLower) {
@@ -111,7 +114,7 @@
 
     int upper = Integer.MAX_VALUE;
     if (upperVal != null) {
-      upper = termsIndex.binarySearchLookup(new BytesRef(upperVal), spare);
+      upper = termsIndex.lookupTerm(new BytesRef(upperVal));
       if (upper < 0) {
         upper = -upper-2;
       } else if (!includeUpper) {
@@ -148,9 +151,8 @@
 
       @Override
       public void fillValue(int doc) {
-        int ord = termsIndex.getOrd(doc);
-        mval.exists = ord != 0;
-        mval.value = termsIndex.lookup(ord, mval.value);
+        termsIndex.get(doc, mval.value);
+        mval.exists = mval.value.bytes != SortedDocValues.MISSING;
       }
     };
   }
Index: lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/IntFieldSource.java
===================================================================
--- lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/IntFieldSource.java	(revision 1442822)
+++ lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/IntFieldSource.java	(working copy)
@@ -57,7 +57,7 @@
 
   @Override
   public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
-    final int[] arr = cache.getInts(readerContext.reader(), field, parser, true);
+    final FieldCache.Ints arr = cache.getInts(readerContext.reader(), field, parser, true);
     final Bits valid = cache.getDocsWithField(readerContext.reader(), field);
     
     return new IntDocValues(this) {
@@ -65,32 +65,32 @@
       
       @Override
       public float floatVal(int doc) {
-        return (float)arr[doc];
+        return (float)arr.get(doc);
       }
 
       @Override
       public int intVal(int doc) {
-        return arr[doc];
+        return arr.get(doc);
       }
 
       @Override
       public long longVal(int doc) {
-        return (long)arr[doc];
+        return (long)arr.get(doc);
       }
 
       @Override
       public double doubleVal(int doc) {
-        return (double)arr[doc];
+        return (double)arr.get(doc);
       }
 
       @Override
       public String strVal(int doc) {
-        return Float.toString(arr[doc]);
+        return Float.toString(arr.get(doc));
       }
 
       @Override
       public Object objectVal(int doc) {
-        return valid.get(doc) ? arr[doc] : null;
+        return valid.get(doc) ? arr.get(doc) : null;
       }
 
       @Override
@@ -129,7 +129,7 @@
         return new ValueSourceScorer(reader, this) {
           @Override
           public boolean matchesValue(int doc) {
-            int val = arr[doc];
+            int val = arr.get(doc);
             // only check for deleted if it's the default value
             // if (val==0 && reader.isDeleted(doc)) return false;
             return val >= ll && val <= uu;
@@ -140,7 +140,6 @@
       @Override
       public ValueFiller getValueFiller() {
         return new ValueFiller() {
-          private final int[] intArr = arr;
           private final MutableValueInt mval = new MutableValueInt();
 
           @Override
@@ -150,7 +149,7 @@
 
           @Override
           public void fillValue(int doc) {
-            mval.value = intArr[doc];
+            mval.value = arr.get(doc);
             mval.exists = valid.get(doc);
           }
         };
Index: lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/LongFieldSource.java
===================================================================
--- lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/LongFieldSource.java	(revision 1442822)
+++ lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/LongFieldSource.java	(working copy)
@@ -66,13 +66,13 @@
 
   @Override
   public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
-    final long[] arr = cache.getLongs(readerContext.reader(), field, parser, true);
+    final FieldCache.Longs arr = cache.getLongs(readerContext.reader(), field, parser, true);
     final Bits valid = cache.getDocsWithField(readerContext.reader(), field);
     
     return new LongDocValues(this) {
       @Override
       public long longVal(int doc) {
-        return arr[doc];
+        return arr.get(doc);
       }
 
       @Override
@@ -82,7 +82,7 @@
 
       @Override
       public Object objectVal(int doc) {
-        return valid.get(doc) ? longToObject(arr[doc]) : null;
+        return valid.get(doc) ? longToObject(arr.get(doc)) : null;
       }
 
       @Override
@@ -111,7 +111,7 @@
         return new ValueSourceScorer(reader, this) {
           @Override
           public boolean matchesValue(int doc) {
-            long val = arr[doc];
+            long val = arr.get(doc);
             // only check for deleted if it's the default value
             // if (val==0 && reader.isDeleted(doc)) return false;
             return val >= ll && val <= uu;
@@ -122,7 +122,6 @@
       @Override
       public ValueFiller getValueFiller() {
         return new ValueFiller() {
-          private final long[] longArr = arr;
           private final MutableValueLong mval = newMutableValueLong();
 
           @Override
@@ -132,7 +131,7 @@
 
           @Override
           public void fillValue(int doc) {
-            mval.value = longArr[doc];
+            mval.value = arr.get(doc);
             mval.exists = valid.get(doc);
           }
         };
Index: lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/ByteFieldSource.java
===================================================================
--- lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/ByteFieldSource.java	(revision 1442822)
+++ lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/ByteFieldSource.java	(working copy)
@@ -51,42 +51,42 @@
 
   @Override
   public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
-    final byte[] arr = cache.getBytes(readerContext.reader(), field, parser, false);
+    final FieldCache.Bytes arr = cache.getBytes(readerContext.reader(), field, parser, false);
     
     return new FunctionValues() {
       @Override
       public byte byteVal(int doc) {
-        return arr[doc];
+        return arr.get(doc);
       }
 
       @Override
       public short shortVal(int doc) {
-        return (short) arr[doc];
+        return (short) arr.get(doc);
       }
 
       @Override
       public float floatVal(int doc) {
-        return (float) arr[doc];
+        return (float) arr.get(doc);
       }
 
       @Override
       public int intVal(int doc) {
-        return (int) arr[doc];
+        return (int) arr.get(doc);
       }
 
       @Override
       public long longVal(int doc) {
-        return (long) arr[doc];
+        return (long) arr.get(doc);
       }
 
       @Override
       public double doubleVal(int doc) {
-        return (double) arr[doc];
+        return (double) arr.get(doc);
       }
 
       @Override
       public String strVal(int doc) {
-        return Byte.toString(arr[doc]);
+        return Byte.toString(arr.get(doc));
       }
 
       @Override
@@ -96,7 +96,7 @@
 
       @Override
       public Object objectVal(int doc) {
-        return arr[doc];  // TODO: valid?
+        return arr.get(doc);  // TODO: valid?
       }
 
     };
Index: lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/FloatFieldSource.java
===================================================================
--- lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/FloatFieldSource.java	(revision 1442822)
+++ lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/FloatFieldSource.java	(working copy)
@@ -56,18 +56,18 @@
 
   @Override
   public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
-    final float[] arr = cache.getFloats(readerContext.reader(), field, parser, true);
+    final FieldCache.Floats arr = cache.getFloats(readerContext.reader(), field, parser, true);
     final Bits valid = cache.getDocsWithField(readerContext.reader(), field);
 
     return new FloatDocValues(this) {
       @Override
       public float floatVal(int doc) {
-        return arr[doc];
+        return arr.get(doc);
       }
 
       @Override
       public Object objectVal(int doc) {
-        return valid.get(doc) ? arr[doc] : null;
+        return valid.get(doc) ? arr.get(doc) : null;
       }
 
       @Override
@@ -78,7 +78,6 @@
       @Override
       public ValueFiller getValueFiller() {
         return new ValueFiller() {
-          private final float[] floatArr = arr;
           private final MutableValueFloat mval = new MutableValueFloat();
 
           @Override
@@ -88,7 +87,7 @@
 
           @Override
           public void fillValue(int doc) {
-            mval.value = floatArr[doc];
+            mval.value = arr.get(doc);
             mval.exists = valid.get(doc);
           }
         };
Index: lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/StrDocValuesFieldSource.java
===================================================================
--- lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/StrDocValuesFieldSource.java	(revision 1442822)
+++ lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/StrDocValuesFieldSource.java	(working copy)
@@ -1,135 +0,0 @@
-package org.apache.lucene.queries.function.valuesource;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Map;
-
-import org.apache.lucene.index.AtomicReaderContext;
-import org.apache.lucene.index.DocValues;
-import org.apache.lucene.queries.function.FunctionValues;
-import org.apache.lucene.queries.function.ValueSource;
-import org.apache.lucene.queries.function.docvalues.StrDocValues;
-import org.apache.lucene.util.Bits;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.packed.PackedInts;
-
-/**
- * A {@link ValueSource} for binary {@link DocValues} that represent an UTF-8
- * encoded String using:<ul>
- * <li>{@link org.apache.lucene.index.DocValues.Type#BYTES_FIXED_DEREF},</li>
- * <li>{@link org.apache.lucene.index.DocValues.Type#BYTES_FIXED_STRAIGHT},</li>
- * <li>{@link org.apache.lucene.index.DocValues.Type#BYTES_VAR_DEREF},</li>
- * <li>{@link org.apache.lucene.index.DocValues.Type#BYTES_VAR_STRAIGHT},</li>
- * <li>{@link org.apache.lucene.index.DocValues.Type#BYTES_FIXED_SORTED},</li>
- * <li>{@link org.apache.lucene.index.DocValues.Type#BYTES_VAR_SORTED}.</li></ul>
- * <p>
- * If the segment has no {@link DocValues}, the default
- * {@link org.apache.lucene.index.DocValues.Source} of type
- * {@link org.apache.lucene.index.DocValues.Type#BYTES_VAR_SORTED} will be used.
- *
- * @lucene.experimental
- */
-public class StrDocValuesFieldSource extends DocValuesFieldSource {
-
-  private static class DVStrValues extends StrDocValues {
-
-    private final Bits liveDocs;
-    private final DocValues.Source source;
-
-    public DVStrValues(ValueSource vs, DocValues.Source source, Bits liveDocs) {
-      super(vs);
-      this.liveDocs = liveDocs;
-      this.source = source;
-    }
-
-    @Override
-    public boolean exists(int doc) {
-      return liveDocs == null || liveDocs.get(doc);
-    }
-
-    @Override
-    public boolean bytesVal(int doc, BytesRef target) {
-      source.getBytes(doc, target);
-      return true;
-    }
-
-    @Override
-    public String strVal(int doc) {
-      BytesRef utf8Bytes = new BytesRef();
-      source.getBytes(doc, utf8Bytes);
-      return utf8Bytes.utf8ToString();
-    }
-  }
-
-  /**
-   * @param fieldName the name of the {@link DocValues} field
-   * @param direct    whether or not to use a direct {@link org.apache.lucene.index.DocValues.Source}
-   */
-  public StrDocValuesFieldSource(String fieldName, boolean direct) {
-    super(fieldName, direct);
-  }
-
-  @Override
-  public FunctionValues getValues(@SuppressWarnings("rawtypes") Map context, AtomicReaderContext readerContext) throws IOException {
-    final DocValues.Source source = getSource(readerContext.reader(), DocValues.Type.BYTES_VAR_SORTED);
-    final Bits liveDocs = readerContext.reader().getLiveDocs();
-    switch (source.getType()) {
-      case BYTES_FIXED_DEREF:
-      case BYTES_FIXED_STRAIGHT:
-      case BYTES_VAR_DEREF:
-      case BYTES_VAR_STRAIGHT:
-        return new DVStrValues(this, source, liveDocs);
-      case BYTES_FIXED_SORTED:
-      case BYTES_VAR_SORTED:
-        final DocValues.SortedSource sortedSource = source.asSortedSource();
-        if (sortedSource.hasPackedDocToOrd()) {
-          final PackedInts.Reader docToOrd = sortedSource.getDocToOrd();
-          return new DVStrValues(this, source, liveDocs) {
-
-            @Override
-            public int ordVal(int doc) {
-              return (int) docToOrd.get(doc);
-            }
-
-            @Override
-            public int numOrd() {
-              return sortedSource.getValueCount();
-            }
-
-          };
-        }
-        return new DVStrValues(this, source, liveDocs) {
-
-          @Override
-          public int ordVal(int doc) {
-            return sortedSource.ord(doc);
-          }
-
-          @Override
-          public int numOrd() {
-            return sortedSource.getValueCount();
-          }
-
-        };
-      default:
-        throw new IllegalStateException(getClass().getSimpleName() + " only works with binary types, not " + source.getType());
-    }
-  }
-
-}
Index: lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/JoinDocFreqValueSource.java
===================================================================
--- lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/JoinDocFreqValueSource.java	(revision 1442822)
+++ lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/JoinDocFreqValueSource.java	(working copy)
@@ -21,6 +21,7 @@
 import java.util.Map;
 
 import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.BinaryDocValues;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.MultiFields;
 import org.apache.lucene.index.ReaderUtil;
@@ -28,7 +29,6 @@
 import org.apache.lucene.index.TermsEnum;
 import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.docvalues.IntDocValues;
-import org.apache.lucene.search.FieldCache.DocTerms;
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.packed.PackedInts;
 
@@ -56,7 +56,7 @@
   @Override
   public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException
   {
-    final DocTerms terms = cache.getTerms(readerContext.reader(), field, PackedInts.FAST);
+    final BinaryDocValues terms = cache.getTerms(readerContext.reader(), field, PackedInts.FAST);
     final IndexReader top = ReaderUtil.getTopLevelContext(readerContext).reader();
     Terms t = MultiFields.getTerms(top, qfield);
     final TermsEnum termsEnum = t == null ? TermsEnum.EMPTY : t.iterator(null);
@@ -68,7 +68,7 @@
       public int intVal(int doc) 
       {
         try {
-          terms.getTerm(doc, ref);
+          terms.get(doc, ref);
           if (termsEnum.seekExact(ref, true)) {
             return termsEnum.docFreq();
           } else {
Index: lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/NormValueSource.java
===================================================================
--- lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/NormValueSource.java	(revision 1442822)
+++ lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/NormValueSource.java	(working copy)
@@ -18,7 +18,7 @@
 package org.apache.lucene.queries.function.valuesource;
 
 import org.apache.lucene.index.AtomicReaderContext;
-import org.apache.lucene.index.DocValues;
+import org.apache.lucene.index.NumericDocValues;
 import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.queries.function.docvalues.FloatDocValues;
@@ -62,18 +62,16 @@
     if (similarity == null) {
       throw new UnsupportedOperationException("requires a TFIDFSimilarity (such as DefaultSimilarity)");
     }
-    DocValues dv = readerContext.reader().normValues(field);
+    final NumericDocValues norms = readerContext.reader().getNormValues(field);
 
-    if (dv == null) {
+    if (norms == null) {
       return new ConstDoubleDocValues(0.0, this);
     }
     
-    final byte[] norms = (byte[]) dv.getSource().getArray();
-
     return new FloatDocValues(this) {
       @Override
       public float floatVal(int doc) {
-        return similarity.decodeNormValue(norms[doc]);
+        return similarity.decodeNormValue((byte)norms.get(doc));
       }
     };
   }
Index: lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/DoubleFieldSource.java
===================================================================
--- lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/DoubleFieldSource.java	(revision 1442822)
+++ lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/DoubleFieldSource.java	(working copy)
@@ -58,12 +58,12 @@
 
   @Override
   public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
-    final double[] arr = cache.getDoubles(readerContext.reader(), field, parser, true);
+    final FieldCache.Doubles arr = cache.getDoubles(readerContext.reader(), field, parser, true);
     final Bits valid = cache.getDocsWithField(readerContext.reader(), field);
     return new DoubleDocValues(this) {
       @Override
       public double doubleVal(int doc) {
-        return arr[doc];
+        return arr.get(doc);
       }
 
       @Override
@@ -132,7 +132,6 @@
       @Override
       public ValueFiller getValueFiller() {
         return new ValueFiller() {
-          private final double[] doubleArr = arr;
           private final MutableValueDouble mval = new MutableValueDouble();
 
           @Override
@@ -142,7 +141,7 @@
 
           @Override
           public void fillValue(int doc) {
-            mval.value = doubleArr[doc];
+            mval.value = arr.get(doc);
             mval.exists = valid.get(doc);
           }
         };
Index: lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/DateDocValuesFieldSource.java
===================================================================
--- lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/DateDocValuesFieldSource.java	(revision 1442822)
+++ lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/DateDocValuesFieldSource.java	(working copy)
@@ -1,122 +0,0 @@
-package org.apache.lucene.queries.function.valuesource;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Date;
-import java.util.Map;
-
-import org.apache.lucene.index.AtomicReaderContext;
-import org.apache.lucene.index.DocValues;
-import org.apache.lucene.queries.function.FunctionValues;
-import org.apache.lucene.queries.function.ValueSource;
-import org.apache.lucene.queries.function.docvalues.LongDocValues;
-import org.apache.lucene.util.Bits;
-import org.apache.lucene.util.BytesRef;
-
-/**
- * A {@link ValueSource} for {@link DocValues} dates, backed by
- * {@link org.apache.lucene.index.DocValues.Type#FIXED_INTS_64}
- * or {@link org.apache.lucene.index.DocValues.Type#VAR_INTS}.
- * <p>
- * If the segment has no {@link DocValues}, the default
- * {@link org.apache.lucene.index.DocValues.Source} of type
- * {@link org.apache.lucene.index.DocValues.Type#FIXED_INTS_64} will be used.
- *
- * @lucene.experimental
- */
-public class DateDocValuesFieldSource extends DocValuesFieldSource {
-
-  private class DVDateValues extends LongDocValues {
-
-    private final Bits liveDocs;
-    private final DocValues.Source source;
-
-    public DVDateValues(ValueSource vs, DocValues.Source source, Bits liveDocs) {
-      super(vs);
-      this.liveDocs = liveDocs;
-      this.source = source;
-    }
-
-    @Override
-    public boolean exists(int doc) {
-      return liveDocs == null || liveDocs.get(doc);
-    }
-
-    @Override
-    public long longVal(int doc) {
-      return source.getInt(doc);
-    }
-
-    @Override
-    public boolean bytesVal(int doc, BytesRef target) {
-      source.getBytes(doc, target);
-      return true;
-    }
-
-    @Override
-    public Date objectVal(int doc) {
-      return new Date(longVal(doc));
-    }
-
-    @Override
-    public String strVal(int doc) {
-      return dateToString(objectVal(doc));
-    }
-
-  }
-
-  /**
-   * @param fieldName the name of the {@link DocValues} field
-   * @param direct    whether or not to use a direct {@link org.apache.lucene.index.DocValues.Source}
-   */
-  public DateDocValuesFieldSource(String fieldName, boolean direct) {
-    super(fieldName, direct);
-  }
-
-  @Override
-  public FunctionValues getValues(@SuppressWarnings("rawtypes") Map context, AtomicReaderContext readerContext) throws IOException {
-    final DocValues.Source source = getSource(readerContext.reader(), DocValues.Type.FIXED_INTS_64);
-    final Bits liveDocs = readerContext.reader().getLiveDocs();
-    switch (source.getType()) {
-      case FIXED_INTS_64:
-      case VAR_INTS:
-        if (source.hasArray() && source.getArray() instanceof long[]) {
-          final long[] values = (long[]) source.getArray();
-          return new DVDateValues(this, source, liveDocs) {
-
-            @Override
-            public long longVal(int doc) {
-              return values[doc];
-            }
-
-          };
-        }
-        return new DVDateValues(this, source, liveDocs);
-      default:
-        throw new IllegalStateException(getClass().getSimpleName() + " only works with 64-bits integer types, not " + source.getType());
-    }
-  }
-
-  /** Return the string representation of the provided {@link Date}.
-   */
-  protected String dateToString(Date date) {
-    return date.toString();
-  }
-
-}
Index: lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/DocValuesFieldSource.java
===================================================================
--- lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/DocValuesFieldSource.java	(revision 1442822)
+++ lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/DocValuesFieldSource.java	(working copy)
@@ -1,90 +0,0 @@
-package org.apache.lucene.queries.function.valuesource;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.index.AtomicReader;
-import org.apache.lucene.index.DocValues;
-import org.apache.lucene.queries.function.ValueSource;
-
-/**
- * A {@link ValueSource} that is based on a field's {@link DocValues}.
- * @lucene.experimental
- */
-public abstract class DocValuesFieldSource extends ValueSource {
-
-  protected final String fieldName;
-  protected final boolean direct;
-
-  protected DocValuesFieldSource(String fieldName, boolean direct) {
-    this.fieldName = fieldName;
-    this.direct = direct;
-  }
-
-  protected final DocValues.Source getSource(AtomicReader reader, DocValues.Type defaultType) throws IOException {
-    final DocValues vals = reader.docValues(fieldName);
-    if (vals == null) {
-      switch (defaultType) {
-        case BYTES_FIXED_SORTED:
-        case BYTES_VAR_SORTED:
-          return DocValues.getDefaultSortedSource(defaultType, reader.maxDoc());
-        default:
-          return DocValues.getDefaultSource(defaultType);
-      }
-    }
-    return direct ? vals.getDirectSource() : vals.getSource();
-  }
-
-  /**
-   * @return whether or not a direct
-   * {@link org.apache.lucene.index.DocValues.Source} is used.
-   */
-  public boolean isDirect() {
-    return direct;
-  }
-
-  /**
-   * @return the field name
-   */
-  public String getFieldName() {
-    return fieldName;
-  }
-
-  @Override
-  public boolean equals(Object o) {
-    if (o == null || !getClass().isInstance(o)) {
-      return false;
-    }
-    final DocValuesFieldSource other = (DocValuesFieldSource) o;
-    return fieldName.equals(other.fieldName) && direct == other.direct;
-  }
-
-  @Override
-  public int hashCode() {
-    int h = getClass().hashCode();
-    h = 31 * h + fieldName.hashCode();
-    h = 31 * h + (direct ? 1 : 0);
-    return h;
-  }
-
-  @Override
-  public String description() {
-    return fieldName;
-  }
-}
Index: lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/ShortFieldSource.java
===================================================================
--- lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/ShortFieldSource.java	(revision 1442822)
+++ lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/ShortFieldSource.java	(working copy)
@@ -49,42 +49,42 @@
 
   @Override
   public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
-    final short[] arr = cache.getShorts(readerContext.reader(), field, parser, false);
+    final FieldCache.Shorts arr = cache.getShorts(readerContext.reader(), field, parser, false);
     
     return new FunctionValues() {
       @Override
       public byte byteVal(int doc) {
-        return (byte) arr[doc];
+        return (byte) arr.get(doc);
       }
 
       @Override
       public short shortVal(int doc) {
-        return arr[doc];
+        return arr.get(doc);
       }
 
       @Override
       public float floatVal(int doc) {
-        return (float) arr[doc];
+        return (float) arr.get(doc);
       }
 
       @Override
       public int intVal(int doc) {
-        return (int) arr[doc];
+        return (int) arr.get(doc);
       }
 
       @Override
       public long longVal(int doc) {
-        return (long) arr[doc];
+        return (long) arr.get(doc);
       }
 
       @Override
       public double doubleVal(int doc) {
-        return (double) arr[doc];
+        return (double) arr.get(doc);
       }
 
       @Override
       public String strVal(int doc) {
-        return Short.toString(arr[doc]);
+        return Short.toString(arr.get(doc));
       }
 
       @Override
Index: lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/NumericDocValuesFieldSource.java
===================================================================
--- lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/NumericDocValuesFieldSource.java	(revision 1442822)
+++ lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/NumericDocValuesFieldSource.java	(working copy)
@@ -1,285 +0,0 @@
-package org.apache.lucene.queries.function.valuesource;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Map;
-
-import org.apache.lucene.index.AtomicReaderContext;
-import org.apache.lucene.index.DocValues;
-import org.apache.lucene.queries.function.FunctionValues;
-import org.apache.lucene.queries.function.ValueSource;
-import org.apache.lucene.queries.function.docvalues.DoubleDocValues;
-import org.apache.lucene.queries.function.docvalues.FloatDocValues;
-import org.apache.lucene.queries.function.docvalues.IntDocValues;
-import org.apache.lucene.queries.function.docvalues.LongDocValues;
-import org.apache.lucene.util.Bits;
-import org.apache.lucene.util.BytesRef;
-
-/**
- * A {@link ValueSource} for numeric {@link DocValues} types:<ul>
- * <li>{@link org.apache.lucene.index.DocValues.Type#FLOAT_32},</li>
- * <li>{@link org.apache.lucene.index.DocValues.Type#FLOAT_64},</li>
- * <li>{@link org.apache.lucene.index.DocValues.Type#FIXED_INTS_8},</li>
- * <li>{@link org.apache.lucene.index.DocValues.Type#FIXED_INTS_16},</li>
- * <li>{@link org.apache.lucene.index.DocValues.Type#FIXED_INTS_32},</li>
- * <li>{@link org.apache.lucene.index.DocValues.Type#FIXED_INTS_64},</li>
- * <li>{@link org.apache.lucene.index.DocValues.Type#VAR_INTS}.</li></ul>
- * <p>
- * If the segment has no {@link DocValues}, the default
- * {@link org.apache.lucene.index.DocValues.Source} of type
- * {@link org.apache.lucene.index.DocValues.Type#FLOAT_64} will be used.
- *
- * @lucene.experimental
- */
-public final class NumericDocValuesFieldSource extends DocValuesFieldSource {
-
-  private static abstract class DVIntValues extends IntDocValues {
-
-    private final Bits liveDocs;
-    private final DocValues.Source source;
-
-    public DVIntValues(ValueSource vs, DocValues.Source source, Bits liveDocs) {
-      super(vs);
-      this.liveDocs = liveDocs;
-      this.source = source;
-    }
-
-    @Override
-    public boolean exists(int doc) {
-      return liveDocs == null || liveDocs.get(doc);
-    }
-
-    @Override
-    public boolean bytesVal(int doc, BytesRef target) {
-      source.getBytes(doc, target);
-      return true;
-    }
-
-    @Override
-    public int intVal(int doc) {
-      return (int) source.getInt(doc);
-    }
-  }
-
-  private static class DVLongValues extends LongDocValues {
-
-    private final Bits liveDocs;
-    private final DocValues.Source source;
-
-    public DVLongValues(ValueSource vs, DocValues.Source source, Bits liveDocs) {
-      super(vs);
-      this.liveDocs = liveDocs;
-      this.source = source;
-    }
-
-    @Override
-    public boolean exists(int doc) {
-      return liveDocs == null || liveDocs.get(doc);
-    }
-
-    @Override
-    public boolean bytesVal(int doc, BytesRef target) {
-      source.getBytes(doc, target);
-      return true;
-    }
-
-    @Override
-    public long longVal(int doc) {
-      return source.getInt(doc);
-    }
-  }
-
-  private static abstract class DVDoubleValues extends DoubleDocValues {
-
-    private final Bits liveDocs;
-    private final DocValues.Source source;
-
-    public DVDoubleValues(ValueSource vs, DocValues.Source source, Bits liveDocs) {
-      super(vs);
-      this.liveDocs = liveDocs;
-      this.source = source;
-    }
-
-    @Override
-    public boolean exists(int doc) {
-      return liveDocs == null || liveDocs.get(doc);
-    }
-
-    @Override
-    public boolean bytesVal(int doc, BytesRef target) {
-      source.getBytes(doc, target);
-      return true;
-    }
-
-    @Override
-    public double doubleVal(int doc) {
-      return source.getFloat(doc);
-    }
-  }
-
-  /**
-   * @param fieldName the name of the {@link DocValues} field
-   * @param direct    whether or not to use a direct {@link org.apache.lucene.index.DocValues.Source}
-   */
-  public NumericDocValuesFieldSource(String fieldName, boolean direct) {
-    super(fieldName, direct);
-  }
-
-  @Override
-  public FunctionValues getValues(@SuppressWarnings("rawtypes") Map context, AtomicReaderContext readerContext) throws IOException {
-    final DocValues.Source source = getSource(readerContext.reader(), DocValues.Type.FLOAT_64);
-    final Bits liveDocs = readerContext.reader().getLiveDocs();
-    switch (source.getType()) {
-      case FIXED_INTS_8:
-      case FIXED_INTS_16:
-      case FIXED_INTS_32:
-      case FIXED_INTS_64:
-      case VAR_INTS:
-        if (source.hasArray()) {
-          final Object valuesArr = source.getArray();
-          if (valuesArr instanceof long[]) {
-            final long[] values = (long[]) source.getArray();
-            return new DVLongValues(this, source, liveDocs) {
-
-              @Override
-              public long longVal(int doc) {
-                return values[doc];
-              }
-
-            };
-          } else if (valuesArr instanceof int[]) {
-            final int[] values = (int[]) source.getArray();
-            return new DVIntValues(this, source, liveDocs) {
-
-              @Override
-              public int intVal(int doc) {
-                return values[doc];
-              }
-
-            };
-          } else if (valuesArr instanceof short[]) {
-            final short[] values = (short[]) source.getArray();
-            return new DVIntValues(this, source, liveDocs) {
-
-              @Override
-              public int intVal(int doc) {
-                return values[doc];
-              }
-
-              @Override
-              public Object objectVal(int doc) {
-                return shortVal(doc);
-              }
-
-            };
-          } else if (valuesArr instanceof byte[]) {
-            final byte[] values = (byte[]) source.getArray();
-            return new DVIntValues(this, source, liveDocs) {
-
-              @Override
-              public int intVal(int doc) {
-                return values[doc];
-              }
-
-              @Override
-              public Object objectVal(int doc) {
-                return byteVal(doc);
-              }
-
-            };
-          }
-        }
-        return new DVLongValues(this, source, liveDocs) {
-
-          @Override
-          public Object objectVal(int doc) {
-            switch (source.getType()) {
-              case FIXED_INTS_8:
-                return byteVal(doc);
-              case FIXED_INTS_16:
-                return shortVal(doc);
-              case FIXED_INTS_32:
-                return intVal(doc);
-              case FIXED_INTS_64:
-              case VAR_INTS:
-                return longVal(doc);
-              default:
-                throw new AssertionError();
-            }
-          }
-
-        };
-      case FLOAT_32:
-      case FLOAT_64:
-        if (source.hasArray()) {
-          final Object valuesArr = source.getArray();
-          if (valuesArr instanceof float[]) {
-            final float[] values = (float[]) valuesArr;
-            return new FloatDocValues(this) {
-
-              @Override
-              public boolean exists(int doc) {
-                return liveDocs == null || liveDocs.get(doc);
-              }
-
-              @Override
-              public boolean bytesVal(int doc, BytesRef target) {
-                source.getBytes(doc, target);
-                return true;
-              }
-
-              @Override
-              public float floatVal(int doc) {
-                return values[doc];
-              }
-
-            };
-          } else if (valuesArr instanceof double[]) {
-            final double[] values = (double[]) valuesArr;
-            return new DVDoubleValues(this, source, liveDocs) {
-
-              @Override
-              public double doubleVal(int doc) {
-                return values[doc];
-              }
-
-            };
-          }
-        }
-        return new DVDoubleValues(this, source, liveDocs) {
-
-          @Override
-          public Object objectVal(int doc) {
-            switch (source.getType()) {
-              case FLOAT_32:
-                return floatVal(doc);
-              case FLOAT_64:
-                return doubleVal(doc);
-              default:
-                throw new AssertionError();
-            }
-          }
-
-        };
-      default:
-        throw new IllegalStateException(getClass().getSimpleName() + " only works with numeric types, not " + source.getType());
-    }
-  }
-
-}
Index: lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/NumericIndexDocValueSource.java
===================================================================
--- lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/NumericIndexDocValueSource.java	(revision 1442822)
+++ lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/NumericIndexDocValueSource.java	(working copy)
@@ -1,121 +0,0 @@
-package org.apache.lucene.queries.function.valuesource;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-import java.io.IOException;
-import java.util.Map;
-
-import org.apache.lucene.index.AtomicReaderContext;
-import org.apache.lucene.index.DocValues.Source;
-import org.apache.lucene.index.DocValues.Type;
-import org.apache.lucene.queries.function.FunctionValues;
-import org.apache.lucene.queries.function.ValueSource;
-
-/**
- * Expert: obtains numeric field values from a {@link FunctionValues} field.
- * This {@link ValueSource} is compatible with all numerical
- * {@link FunctionValues}
- * 
- * @deprecated Use {@link NumericDocValuesFieldSource} instead.
- */
-public class NumericIndexDocValueSource extends ValueSource {
-
-  private final String field;
-
-  public NumericIndexDocValueSource(String field) {
-    this.field = field;
-  }
-
-  @Override
-  public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
-    final Source source = readerContext.reader().docValues(field)
-        .getSource();
-    Type type = source.getType();
-    switch (type) {
-    case FLOAT_32:
-    case FLOAT_64:
-      // TODO (chrism) Change to use FloatDocValues and IntDocValues
-      return new FunctionValues() {
-
-        @Override
-        public String toString(int doc) {
-          return "float: [" + floatVal(doc) + "]";
-        }
-
-        @Override
-        public float floatVal(int doc) {
-          return (float) source.getFloat(doc);
-        }
-      };
-
-    case FIXED_INTS_8:
-    case FIXED_INTS_16:
-    case FIXED_INTS_32:
-    case FIXED_INTS_64:
-    case VAR_INTS:
-      return new FunctionValues() {
-        @Override
-        public String toString(int doc) {
-          return "float: [" + floatVal(doc) + "]";
-        }
-
-        @Override
-        public float floatVal(int doc) {
-          return (float) source.getInt(doc);
-        }
-      };
-    default:
-      throw new IOException("Type: " + type + "is not numeric");
-    }
-
-  }
-
-  @Override
-  public String description() {
-    return toString();
-  }
-
-  @Override
-  public int hashCode() {
-    final int prime = 31;
-    int result = 1;
-    result = prime * result + ((field == null) ? 0 : field.hashCode());
-    return result;
-  }
-
-  @Override
-  public boolean equals(Object obj) {
-    if (this == obj)
-      return true;
-    if (obj == null)
-      return false;
-    if (getClass() != obj.getClass())
-      return false;
-    NumericIndexDocValueSource other = (NumericIndexDocValueSource) obj;
-    if (field == null) {
-      if (other.field != null)
-        return false;
-    } else if (!field.equals(other.field))
-      return false;
-    return true;
-  }
-
-  @Override
-  public String toString() {
-    return "FunctionValues float(" + field + ')';
-  }
-}
Index: lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/ReverseOrdFieldSource.java
===================================================================
--- lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/ReverseOrdFieldSource.java	(revision 1442822)
+++ lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/ReverseOrdFieldSource.java	(working copy)
@@ -17,20 +17,21 @@
 
 package org.apache.lucene.queries.function.valuesource;
 
+import java.io.IOException;
+import java.util.Map;
+
 import org.apache.lucene.index.AtomicReader;
 import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.CompositeReader;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.ReaderUtil;
 import org.apache.lucene.index.SlowCompositeReaderWrapper;
+import org.apache.lucene.index.SortedDocValues;
 import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.queries.function.docvalues.IntDocValues;
 import org.apache.lucene.search.FieldCache;
 
-import java.io.IOException;
-import java.util.Map;
-
 /**
  * Obtains the ordinal of the field value from the default Lucene {@link org.apache.lucene.search.FieldCache} using getTermsIndex()
  * and reverses the order.
@@ -73,13 +74,13 @@
         : (AtomicReader) topReader;
     final int off = readerContext.docBase;
 
-    final FieldCache.DocTermsIndex sindex = FieldCache.DEFAULT.getTermsIndex(r, field);
-    final int end = sindex.numOrd();
+    final SortedDocValues sindex = FieldCache.DEFAULT.getTermsIndex(r, field);
+    final int end = sindex.getValueCount();
 
     return new IntDocValues(this) {
      @Override
       public int intVal(int doc) {
-        return (end - sindex.getOrd(doc+off));
+        return (end - sindex.getOrd(doc+off) - 1);
       }
     };
   }
Index: lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/BytesRefFieldSource.java
===================================================================
--- lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/BytesRefFieldSource.java	(revision 1442822)
+++ lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/BytesRefFieldSource.java	(working copy)
@@ -17,13 +17,19 @@
  * limitations under the License.
  */
 
+import java.io.IOException;
+import java.util.Map;
+
 import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.BinaryDocValues;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.FieldInfo.DocValuesType;
 import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.docvalues.DocTermsIndexDocValues;
+import org.apache.lucene.search.FieldCache;
+import org.apache.lucene.util.Bits;
+import org.apache.lucene.util.BytesRef;
 
-import java.io.IOException;
-import java.util.Map;
-
 /**
  * An implementation for retrieving {@link FunctionValues} instances for string based fields.
  */
@@ -35,23 +41,59 @@
 
   @Override
   public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
-    return new DocTermsIndexDocValues(this, readerContext, field) {
+    final FieldInfo fieldInfo = readerContext.reader().getFieldInfos().fieldInfo(field);
+    // To be sorted or not to be sorted, that is the question
+    // TODO: do it cleaner?
+    if (fieldInfo != null && fieldInfo.getDocValuesType() == DocValuesType.BINARY) {
+      final BinaryDocValues binaryValues = FieldCache.DEFAULT.getTerms(readerContext.reader(), field);
+      return new FunctionValues() {
 
-      @Override
-      protected String toTerm(String readableValue) {
-        return readableValue;
-      }
+        @Override
+        public boolean exists(int doc) {
+          return true; // doc values are dense
+        }
 
-      @Override
-      public Object objectVal(int doc) {
-        return strVal(doc);
-      }
+        @Override
+        public boolean bytesVal(int doc, BytesRef target) {
+          binaryValues.get(doc, target);
+          return target.length > 0;
+        }
 
-      @Override
-      public String toString(int doc) {
-        return description() + '=' + strVal(doc);
-      }
+        public String strVal(int doc) {
+          final BytesRef bytes = new BytesRef();
+          return bytesVal(doc, bytes)
+              ? bytes.utf8ToString()
+              : null;
+        }
 
-    };
+        @Override
+        public Object objectVal(int doc) {
+          return strVal(doc);
+        }
+
+        @Override
+        public String toString(int doc) {
+          return description() + '=' + strVal(doc);
+        }
+      };
+    } else {
+      return new DocTermsIndexDocValues(this, readerContext, field) {
+
+        @Override
+        protected String toTerm(String readableValue) {
+          return readableValue;
+        }
+
+        @Override
+        public Object objectVal(int doc) {
+          return strVal(doc);
+        }
+
+        @Override
+        public String toString(int doc) {
+          return description() + '=' + strVal(doc);
+        }
+      };
+    }
   }
 }
Index: lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/OrdFieldSource.java
===================================================================
--- lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/OrdFieldSource.java	(revision 1442822)
+++ lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/OrdFieldSource.java	(working copy)
@@ -17,12 +17,16 @@
 
 package org.apache.lucene.queries.function.valuesource;
 
+import java.io.IOException;
+import java.util.Map;
+
 import org.apache.lucene.index.AtomicReader;
 import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.CompositeReader;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.ReaderUtil;
 import org.apache.lucene.index.SlowCompositeReaderWrapper;
+import org.apache.lucene.index.SortedDocValues;
 import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.queries.function.docvalues.IntDocValues;
@@ -30,9 +34,6 @@
 import org.apache.lucene.util.mutable.MutableValue;
 import org.apache.lucene.util.mutable.MutableValueInt;
 
-import java.io.IOException;
-import java.util.Map;
-
 /**
  * Obtains the ordinal of the field value from the default Lucene {@link org.apache.lucene.search.FieldCache} using getStringIndex().
  * <br>
@@ -72,7 +73,7 @@
     final AtomicReader r = topReader instanceof CompositeReader 
         ? new SlowCompositeReaderWrapper((CompositeReader)topReader) 
         : (AtomicReader) topReader;
-    final FieldCache.DocTermsIndex sindex = FieldCache.DEFAULT.getTermsIndex(r, field);
+    final SortedDocValues sindex = FieldCache.DEFAULT.getTermsIndex(r, field);
     return new IntDocValues(this) {
       protected String toTerm(String readableValue) {
         return readableValue;
@@ -87,7 +88,7 @@
       }
       @Override
       public int numOrd() {
-        return sindex.numOrd();
+        return sindex.getValueCount();
       }
 
       @Override
Index: lucene/common-build.xml
===================================================================
--- lucene/common-build.xml	(revision 1442822)
+++ lucene/common-build.xml	(working copy)
@@ -88,6 +88,7 @@
   <property name="tests.multiplier" value="1" />
   <property name="tests.codec" value="random" />
   <property name="tests.postingsformat" value="random" />
+  <property name="tests.docvaluesformat" value="random" />
   <property name="tests.locale" value="random" />
   <property name="tests.timezone" value="random" />
   <property name="tests.directory" value="random" />
@@ -864,6 +865,8 @@
             <sysproperty key="tests.codec" value="${tests.codec}"/>
             <!-- set the postingsformat tests should run with -->
             <sysproperty key="tests.postingsformat" value="${tests.postingsformat}"/>
+            <!-- set the docvaluesformat tests should run with -->
+            <sysproperty key="tests.docvaluesformat" value="${tests.docvaluesformat}"/>
             <!-- set the locale tests should run with -->
             <sysproperty key="tests.locale" value="${tests.locale}"/>
             <!-- set the timezone tests should run with -->
Index: lucene/spatial/src/java/org/apache/lucene/spatial/SpatialStrategy.java
===================================================================
--- lucene/spatial/src/java/org/apache/lucene/spatial/SpatialStrategy.java	(revision 1442822)
+++ lucene/spatial/src/java/org/apache/lucene/spatial/SpatialStrategy.java	(working copy)
@@ -41,8 +41,8 @@
  *   <li>What types of query shapes can be used?</li>
  *   <li>What types of query operations are supported?
  *   This might vary per shape.</li>
- *   <li>Does it use the {@link org.apache.lucene.search.FieldCache}, {@link
- *   org.apache.lucene.index.DocValues} or some other type of cache?  When?
+ *   <li>Does it use the {@link org.apache.lucene.search.FieldCache},
+ *   or some other type of cache?  When?
  * </ul>
  * If a strategy only supports certain shapes at index or query time, then in
  * general it will throw an exception if given an incompatible one.  It will not
Index: lucene/spatial/src/java/org/apache/lucene/spatial/vector/DistanceValueSource.java
===================================================================
--- lucene/spatial/src/java/org/apache/lucene/spatial/vector/DistanceValueSource.java	(revision 1442822)
+++ lucene/spatial/src/java/org/apache/lucene/spatial/vector/DistanceValueSource.java	(working copy)
@@ -63,8 +63,8 @@
   public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
     AtomicReader reader = readerContext.reader();
 
-    final double[] ptX = FieldCache.DEFAULT.getDoubles(reader, strategy.getFieldNameX(), true);
-    final double[] ptY = FieldCache.DEFAULT.getDoubles(reader, strategy.getFieldNameY(), true);
+    final FieldCache.Doubles ptX = FieldCache.DEFAULT.getDoubles(reader, strategy.getFieldNameX(), true);
+    final FieldCache.Doubles ptY = FieldCache.DEFAULT.getDoubles(reader, strategy.getFieldNameY(), true);
     final Bits validX =  FieldCache.DEFAULT.getDocsWithField(reader, strategy.getFieldNameX());
     final Bits validY =  FieldCache.DEFAULT.getDocsWithField(reader, strategy.getFieldNameY());
 
@@ -84,7 +84,7 @@
         // make sure it has minX and area
         if (validX.get(doc)) {
           assert validY.get(doc);
-          return calculator.distance(from, ptX[doc], ptY[doc]);
+          return calculator.distance(from, ptX.get(doc), ptY.get(doc));
         }
         return nullValue;
       }
Index: lucene/spatial/src/java/org/apache/lucene/spatial/bbox/BBoxSimilarityValueSource.java
===================================================================
--- lucene/spatial/src/java/org/apache/lucene/spatial/bbox/BBoxSimilarityValueSource.java	(revision 1442822)
+++ lucene/spatial/src/java/org/apache/lucene/spatial/bbox/BBoxSimilarityValueSource.java	(working copy)
@@ -64,10 +64,10 @@
   @Override
   public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
     AtomicReader reader = readerContext.reader();
-    final double[] minX = FieldCache.DEFAULT.getDoubles(reader, strategy.field_minX, true);
-    final double[] minY = FieldCache.DEFAULT.getDoubles(reader, strategy.field_minY, true);
-    final double[] maxX = FieldCache.DEFAULT.getDoubles(reader, strategy.field_maxX, true);
-    final double[] maxY = FieldCache.DEFAULT.getDoubles(reader, strategy.field_maxY, true);
+    final FieldCache.Doubles minX = FieldCache.DEFAULT.getDoubles(reader, strategy.field_minX, true);
+    final FieldCache.Doubles minY = FieldCache.DEFAULT.getDoubles(reader, strategy.field_minY, true);
+    final FieldCache.Doubles maxX = FieldCache.DEFAULT.getDoubles(reader, strategy.field_maxX, true);
+    final FieldCache.Doubles maxY = FieldCache.DEFAULT.getDoubles(reader, strategy.field_maxY, true);
 
     final Bits validMinX = FieldCache.DEFAULT.getDocsWithField(reader, strategy.field_minX);
     final Bits validMaxX = FieldCache.DEFAULT.getDocsWithField(reader, strategy.field_maxX);
@@ -81,8 +81,8 @@
         // make sure it has minX and area
         if (validMinX.get(doc) && validMaxX.get(doc)) {
           rect.reset(
-              minX[doc], maxX[doc],
-              minY[doc], maxY[doc]);
+              minX.get(doc), maxX.get(doc),
+              minY.get(doc), maxY.get(doc));
           return (float) similarity.score(rect, null);
         } else {
           return (float) similarity.score(null, null);
@@ -94,8 +94,8 @@
         // make sure it has minX and area
         if (validMinX.get(doc) && validMaxX.get(doc)) {
           rect.reset(
-              minX[doc], maxX[doc],
-              minY[doc], maxY[doc]);
+              minX.get(doc), maxX.get(doc),
+              minY.get(doc), maxY.get(doc));
           Explanation exp = new Explanation();
           similarity.score(rect, exp);
           return exp;
Index: lucene/memory/src/test/org/apache/lucene/index/memory/MemoryIndexTest.java
===================================================================
--- lucene/memory/src/test/org/apache/lucene/index/memory/MemoryIndexTest.java	(revision 1442822)
+++ lucene/memory/src/test/org/apache/lucene/index/memory/MemoryIndexTest.java	(working copy)
@@ -36,8 +36,6 @@
 import org.apache.lucene.index.AtomicReader;
 import org.apache.lucene.index.CompositeReader;
 import org.apache.lucene.index.DirectoryReader;
-import org.apache.lucene.index.DocValues;
-import org.apache.lucene.index.DocValues.Source;
 import org.apache.lucene.index.DocsAndPositionsEnum;
 import org.apache.lucene.index.DocsEnum;
 import org.apache.lucene.index.Fields;
@@ -46,6 +44,7 @@
 import org.apache.lucene.index.IndexWriterConfig;
 import org.apache.lucene.index.IndexableField;
 import org.apache.lucene.index.MultiFields;
+import org.apache.lucene.index.NumericDocValues;
 import org.apache.lucene.index.SlowCompositeReaderWrapper;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.index.Terms;
@@ -178,12 +177,12 @@
       if (iwTerms == null) {
         assertNull(memTerms);
       } else {
-        DocValues normValues = competitor.normValues(field);
-        DocValues memNormValues = memIndexReader.normValues(field);
+        NumericDocValues normValues = competitor.getNormValues(field);
+        NumericDocValues memNormValues = memIndexReader.getNormValues(field);
         if (normValues != null) {
           // mem idx always computes norms on the fly
           assertNotNull(memNormValues);
-          assertEquals(normValues.getDirectSource().getInt(0), memNormValues.getDirectSource().getInt(0), 0.01);
+          assertEquals(normValues.get(0), memNormValues.get(0));
         }
           
         assertNotNull(memTerms);
@@ -386,8 +385,8 @@
     MockAnalyzer mockAnalyzer = new MockAnalyzer(random());
     mindex.addField("field", "the quick brown fox", mockAnalyzer);
     AtomicReader reader = (AtomicReader) mindex.createSearcher().getIndexReader();
-    assertNull(reader.docValues("not-in-index"));
-    assertNull(reader.normValues("not-in-index"));
+    assertNull(reader.getNumericDocValues("not-in-index"));
+    assertNull(reader.getNormValues("not-in-index"));
     assertNull(reader.termDocsEnum(new Term("not-in-index", "foo")));
     assertNull(reader.termPositionsEnum(new Term("not-in-index", "foo")));
     assertNull(reader.terms("not-in-index"));
Index: lucene/memory/src/java/org/apache/lucene/index/memory/MemoryIndexNormDocValues.java
===================================================================
--- lucene/memory/src/java/org/apache/lucene/index/memory/MemoryIndexNormDocValues.java	(revision 1442822)
+++ lucene/memory/src/java/org/apache/lucene/index/memory/MemoryIndexNormDocValues.java	(working copy)
@@ -15,129 +15,25 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
-import java.io.IOException;
 
-import org.apache.lucene.index.Norm;
-import org.apache.lucene.index.DocValues;
-import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.index.NumericDocValues;
 
 /**
  * 
  * @lucene.internal
  */
-class MemoryIndexNormDocValues extends DocValues {
-
-  private final Source source;
-
-  MemoryIndexNormDocValues(Source source) {
-    this.source = source;
+class MemoryIndexNormDocValues extends NumericDocValues {
+  private final long value;
+  public MemoryIndexNormDocValues(long value) {
+    this.value = value;
   }
-  @Override
-  protected Source loadSource() throws IOException {
-    return source;
-  }
 
   @Override
-  protected Source loadDirectSource() throws IOException {
-    return source;
+  public long get(int docID) {
+    if (docID != 0)
+      throw new IndexOutOfBoundsException();
+    else
+      return value;
   }
 
-  @Override
-  public Type getType() {
-    return source.getType();
-  }
-  
-  @Override
-  public int getValueSize() {
-    return 1;
-  }
-
-  public static class SingleValueSource extends Source {
-
-    private final Number numericValue;
-    private final BytesRef binaryValue;
-
-    protected SingleValueSource(Norm norm) {
-      super(norm.type());
-      this.numericValue = norm.field().numericValue();
-      this.binaryValue = norm.field().binaryValue();
-    }
-
-    @Override
-    public long getInt(int docID) {
-      switch (type) {
-      case FIXED_INTS_16:
-      case FIXED_INTS_32:
-      case FIXED_INTS_64:
-      case FIXED_INTS_8:
-      case VAR_INTS:
-        assert numericValue != null;
-        return numericValue.longValue();
-      }
-      return super.getInt(docID);
-    }
-
-    @Override
-    public double getFloat(int docID) {
-      switch (type) {
-      case FLOAT_32:
-      case FLOAT_64:
-        assert numericValue != null;
-        return numericValue.floatValue();
-      }
-      return super.getFloat(docID);
-    }
-
-    @Override
-    public BytesRef getBytes(int docID, BytesRef ref) {
-      switch (type) {
-      case BYTES_FIXED_DEREF:
-      case BYTES_FIXED_SORTED:
-      case BYTES_FIXED_STRAIGHT:
-      case BYTES_VAR_DEREF:
-      case BYTES_VAR_SORTED:
-      case BYTES_VAR_STRAIGHT:
-        assert binaryValue != null;
-        ref.copyBytes(binaryValue);
-        return ref;
-      }
-      return super.getBytes(docID, ref);
-    }
-
-    @Override
-    public boolean hasArray() {
-      return true;
-    }
-
-    @Override
-    public Object getArray() {
-      switch (type) {
-      case BYTES_FIXED_DEREF:
-      case BYTES_FIXED_SORTED:
-      case BYTES_FIXED_STRAIGHT:
-      case BYTES_VAR_DEREF:
-      case BYTES_VAR_SORTED:
-      case BYTES_VAR_STRAIGHT:
-        return binaryValue.bytes;
-      case FIXED_INTS_16:
-        return new short[] { numericValue.shortValue() };
-      case FIXED_INTS_32:
-        return new int[] { numericValue.intValue() };
-      case FIXED_INTS_64:
-        return new long[] { numericValue.longValue() };
-      case FIXED_INTS_8:
-        return new byte[] { numericValue.byteValue() };
-      case VAR_INTS:
-        return new long[] { numericValue.longValue() };
-      case FLOAT_32:
-        return new float[] { numericValue.floatValue() };
-      case FLOAT_64:
-        return new double[] { numericValue.doubleValue() };
-      default:
-        throw new IllegalArgumentException("unknown type " + type);
-      }
-
-    }
-  }
-
 }
Index: lucene/memory/src/java/org/apache/lucene/index/memory/MemoryIndex.java
===================================================================
--- lucene/memory/src/java/org/apache/lucene/index/memory/MemoryIndex.java	(revision 1442822)
+++ lucene/memory/src/java/org/apache/lucene/index/memory/MemoryIndex.java	(working copy)
@@ -35,21 +35,21 @@
 import org.apache.lucene.analysis.tokenattributes.TermToBytesRefAttribute;
 import org.apache.lucene.index.AtomicReader;
 import org.apache.lucene.index.AtomicReaderContext;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.Norm;
-import org.apache.lucene.index.DocValues;
+import org.apache.lucene.index.BinaryDocValues;
 import org.apache.lucene.index.DocsAndPositionsEnum;
 import org.apache.lucene.index.DocsEnum;
+import org.apache.lucene.index.FieldInfo.IndexOptions;
+import org.apache.lucene.index.FieldInfo;
 import org.apache.lucene.index.FieldInfos;
 import org.apache.lucene.index.FieldInvertState;
 import org.apache.lucene.index.Fields;
+import org.apache.lucene.index.NumericDocValues;
 import org.apache.lucene.index.OrdTermState;
+import org.apache.lucene.index.SortedDocValues;
 import org.apache.lucene.index.StoredFieldVisitor;
 import org.apache.lucene.index.TermState;
 import org.apache.lucene.index.Terms;
 import org.apache.lucene.index.TermsEnum;
-import org.apache.lucene.index.FieldInfo.IndexOptions;
-import org.apache.lucene.index.memory.MemoryIndexNormDocValues.SingleValueSource;
 import org.apache.lucene.search.Collector;
 import org.apache.lucene.search.IndexSearcher;
 import org.apache.lucene.search.Query;
@@ -60,13 +60,13 @@
 import org.apache.lucene.util.Bits;
 import org.apache.lucene.util.ByteBlockPool;
 import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.BytesRefHash.DirectBytesStartArray;
 import org.apache.lucene.util.BytesRefHash;
+import org.apache.lucene.util.Constants; // for javadocs
 import org.apache.lucene.util.Counter;
-import org.apache.lucene.util.IntBlockPool;
-import org.apache.lucene.util.BytesRefHash.DirectBytesStartArray;
 import org.apache.lucene.util.IntBlockPool.SliceReader;
 import org.apache.lucene.util.IntBlockPool.SliceWriter;
-import org.apache.lucene.util.Constants; // for javadocs
+import org.apache.lucene.util.IntBlockPool;
 import org.apache.lucene.util.RamUsageEstimator;
 import org.apache.lucene.util.RecyclingByteBlockAllocator;
 import org.apache.lucene.util.RecyclingIntBlockAllocator;
@@ -738,6 +738,21 @@
       return new FieldInfos(fieldInfos.values().toArray(new FieldInfo[fieldInfos.size()]));
     }
 
+    @Override
+    public NumericDocValues getNumericDocValues(String field) {
+      return null;
+    }
+
+    @Override
+    public BinaryDocValues getBinaryDocValues(String field) {
+      return null;
+    }
+
+    @Override
+    public SortedDocValues getSortedDocValues(String field) {
+      return null;
+    }
+
     private class MemoryFields extends Fields {
       @Override
       public Iterator<String> iterator() {
@@ -1127,23 +1142,18 @@
     protected void doClose() {
       if (DEBUG) System.err.println("MemoryIndexReader.doClose");
     }
-
-    @Override
-    public DocValues docValues(String field) {
-      return null;
-    }
     
     /** performance hack: cache norms to avoid repeated expensive calculations */
-    private DocValues cachedNormValues;
+    private NumericDocValues cachedNormValues;
     private String cachedFieldName;
     private Similarity cachedSimilarity;
     
     @Override
-    public DocValues normValues(String field) {
+    public NumericDocValues getNormValues(String field) {
       FieldInfo fieldInfo = fieldInfos.get(field);
       if (fieldInfo == null || fieldInfo.omitsNorms())
         return null;
-      DocValues norms = cachedNormValues;
+      NumericDocValues norms = cachedNormValues;
       Similarity sim = getSimilarity();
       if (!field.equals(cachedFieldName) || sim != cachedSimilarity) { // not cached?
         Info info = getInfo(field);
@@ -1151,15 +1161,13 @@
         int numOverlapTokens = info != null ? info.numOverlapTokens : 0;
         float boost = info != null ? info.getBoost() : 1.0f; 
         FieldInvertState invertState = new FieldInvertState(field, 0, numTokens, numOverlapTokens, 0, boost);
-        Norm norm = new Norm();
-        sim.computeNorm(invertState, norm);
-        SingleValueSource singleByteSource = new SingleValueSource(norm);
-        norms = new MemoryIndexNormDocValues(singleByteSource);
+        long value = sim.computeNorm(invertState);
+        norms = new MemoryIndexNormDocValues(value);
         // cache it for future reuse
         cachedNormValues = norms;
         cachedFieldName = field;
         cachedSimilarity = sim;
-        if (DEBUG) System.err.println("MemoryIndexReader.norms: " + field + ":" + norm + ":" + numTokens);
+        if (DEBUG) System.err.println("MemoryIndexReader.norms: " + field + ":" + value + ":" + numTokens);
       }
       return norms;
     }
Index: lucene/suggest/src/test/org/apache/lucene/search/suggest/fst/BytesRefSortersTest.java
===================================================================
--- lucene/suggest/src/test/org/apache/lucene/search/suggest/fst/BytesRefSortersTest.java	(revision 1442822)
+++ lucene/suggest/src/test/org/apache/lucene/search/suggest/fst/BytesRefSortersTest.java	(working copy)
@@ -17,6 +17,8 @@
  * limitations under the License.
  */
 
+import org.apache.lucene.search.suggest.InMemorySorter;
+import org.apache.lucene.search.suggest.Sort;
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.BytesRefIterator;
 import org.apache.lucene.util.LuceneTestCase;
Index: lucene/suggest/src/test/org/apache/lucene/search/suggest/fst/LargeInputFST.java
===================================================================
--- lucene/suggest/src/test/org/apache/lucene/search/suggest/fst/LargeInputFST.java	(revision 1442822)
+++ lucene/suggest/src/test/org/apache/lucene/search/suggest/fst/LargeInputFST.java	(working copy)
@@ -19,6 +19,7 @@
 
 import java.io.*;
 
+import org.apache.lucene.search.suggest.Sort;
 import org.apache.lucene.util.BytesRef;
 
 /**
Index: lucene/suggest/src/test/org/apache/lucene/search/suggest/fst/TestSort.java
===================================================================
--- lucene/suggest/src/test/org/apache/lucene/search/suggest/fst/TestSort.java	(revision 1442822)
+++ lucene/suggest/src/test/org/apache/lucene/search/suggest/fst/TestSort.java	(working copy)
@@ -22,9 +22,10 @@
 import java.util.Arrays;
 import java.util.Comparator;
 
-import org.apache.lucene.search.suggest.fst.Sort.BufferSize;
-import org.apache.lucene.search.suggest.fst.Sort.ByteSequencesWriter;
-import org.apache.lucene.search.suggest.fst.Sort.SortInfo;
+import org.apache.lucene.search.suggest.Sort;
+import org.apache.lucene.search.suggest.Sort.BufferSize;
+import org.apache.lucene.search.suggest.Sort.ByteSequencesWriter;
+import org.apache.lucene.search.suggest.Sort.SortInfo;
 import org.apache.lucene.util.*;
 import org.junit.*;
 
Index: lucene/suggest/src/test/org/apache/lucene/search/suggest/TestBytesRefList.java
===================================================================
--- lucene/suggest/src/test/org/apache/lucene/search/suggest/TestBytesRefList.java	(revision 1442822)
+++ lucene/suggest/src/test/org/apache/lucene/search/suggest/TestBytesRefList.java	(working copy)
@@ -1,107 +0,0 @@
-package org.apache.lucene.search.suggest;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to You under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-
-import java.io.IOException;
-import java.util.*;
-
-import org.apache.lucene.search.suggest.BytesRefList;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.BytesRefIterator;
-import org.apache.lucene.util.LuceneTestCase;
-import org.apache.lucene.util._TestUtil;
-
-public class TestBytesRefList extends LuceneTestCase {
-
-  public void testAppend() throws IOException {
-    Random random = random();
-    BytesRefList list = new BytesRefList();
-    List<String> stringList = new ArrayList<String>();
-    for (int j = 0; j < 2; j++) {
-      if (j > 0 && random.nextBoolean()) {
-        list.clear();
-        stringList.clear();
-      }
-      int entries = atLeast(500);
-      BytesRef spare = new BytesRef();
-      for (int i = 0; i < entries; i++) {
-        String randomRealisticUnicodeString = _TestUtil
-            .randomRealisticUnicodeString(random);
-        spare.copyChars(randomRealisticUnicodeString);
-        list.append(spare);
-        stringList.add(randomRealisticUnicodeString);
-      }
-      for (int i = 0; i < entries; i++) {
-        assertNotNull(list.get(spare, i));
-        assertEquals("entry " + i + " doesn't match", stringList.get(i),
-            spare.utf8ToString());
-      }
-      
-      // check random
-      for (int i = 0; i < entries; i++) {
-        int e = random.nextInt(entries);
-        assertNotNull(list.get(spare, e));
-        assertEquals("entry " + i + " doesn't match", stringList.get(e),
-            spare.utf8ToString());
-      }
-      for (int i = 0; i < 2; i++) {
-        
-        BytesRefIterator iterator = list.iterator();
-        for (String string : stringList) {
-          assertEquals(string, iterator.next().utf8ToString());
-        }
-      }
-    }
-  }
-
-  public void testSort() throws IOException {
-    Random random = random();
-    BytesRefList list = new BytesRefList();
-    List<String> stringList = new ArrayList<String>();
-
-    for (int j = 0; j < 2; j++) {
-      if (j > 0 && random.nextBoolean()) {
-        list.clear();
-        stringList.clear();
-      }
-      int entries = atLeast(500);
-      BytesRef spare = new BytesRef();
-      for (int i = 0; i < entries; i++) {
-        String randomRealisticUnicodeString = _TestUtil
-            .randomRealisticUnicodeString(random);
-        spare.copyChars(randomRealisticUnicodeString);
-        list.append(spare);
-        stringList.add(randomRealisticUnicodeString);
-      }
-      
-      Collections.sort(stringList);
-      BytesRefIterator iter = list.iterator(BytesRef
-          .getUTF8SortedAsUTF16Comparator());
-      int i = 0;
-      while ((spare = iter.next()) != null) {
-        assertEquals("entry " + i + " doesn't match", stringList.get(i),
-            spare.utf8ToString());
-        i++;
-      }
-      assertNull(iter.next());
-      assertEquals(i, stringList.size());
-    }
-    
-  }
-  
-}
Index: lucene/suggest/src/test/org/apache/lucene/search/suggest/TestBytesRefArray.java
===================================================================
--- lucene/suggest/src/test/org/apache/lucene/search/suggest/TestBytesRefArray.java	(revision 0)
+++ lucene/suggest/src/test/org/apache/lucene/search/suggest/TestBytesRefArray.java	(working copy)
@@ -0,0 +1,107 @@
+package org.apache.lucene.search.suggest;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements. See the NOTICE file distributed with this
+ * work for additional information regarding copyright ownership. The ASF
+ * licenses this file to You under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ * 
+ * http://www.apache.org/licenses/LICENSE-2.0
+ * 
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
+ * License for the specific language governing permissions and limitations under
+ * the License.
+ */
+
+import java.io.IOException;
+import java.util.*;
+
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.BytesRefIterator;
+import org.apache.lucene.util.Counter;
+import org.apache.lucene.util.LuceneTestCase;
+import org.apache.lucene.util._TestUtil;
+
+public class TestBytesRefArray extends LuceneTestCase {
+
+  public void testAppend() throws IOException {
+    Random random = random();
+    BytesRefArray list = new BytesRefArray(Counter.newCounter());
+    List<String> stringList = new ArrayList<String>();
+    for (int j = 0; j < 2; j++) {
+      if (j > 0 && random.nextBoolean()) {
+        list.clear();
+        stringList.clear();
+      }
+      int entries = atLeast(500);
+      BytesRef spare = new BytesRef();
+      for (int i = 0; i < entries; i++) {
+        String randomRealisticUnicodeString = _TestUtil
+            .randomRealisticUnicodeString(random);
+        spare.copyChars(randomRealisticUnicodeString);
+        list.append(spare);
+        stringList.add(randomRealisticUnicodeString);
+      }
+      for (int i = 0; i < entries; i++) {
+        assertNotNull(list.get(spare, i));
+        assertEquals("entry " + i + " doesn't match", stringList.get(i),
+            spare.utf8ToString());
+      }
+      
+      // check random
+      for (int i = 0; i < entries; i++) {
+        int e = random.nextInt(entries);
+        assertNotNull(list.get(spare, e));
+        assertEquals("entry " + i + " doesn't match", stringList.get(e),
+            spare.utf8ToString());
+      }
+      for (int i = 0; i < 2; i++) {
+        
+        BytesRefIterator iterator = list.iterator();
+        for (String string : stringList) {
+          assertEquals(string, iterator.next().utf8ToString());
+        }
+      }
+    }
+  }
+
+  public void testSort() throws IOException {
+    Random random = random();
+    BytesRefArray list = new BytesRefArray(Counter.newCounter());
+    List<String> stringList = new ArrayList<String>();
+
+    for (int j = 0; j < 2; j++) {
+      if (j > 0 && random.nextBoolean()) {
+        list.clear();
+        stringList.clear();
+      }
+      int entries = atLeast(500);
+      BytesRef spare = new BytesRef();
+      for (int i = 0; i < entries; i++) {
+        String randomRealisticUnicodeString = _TestUtil
+            .randomRealisticUnicodeString(random);
+        spare.copyChars(randomRealisticUnicodeString);
+        list.append(spare);
+        stringList.add(randomRealisticUnicodeString);
+      }
+      
+      Collections.sort(stringList);
+      BytesRefIterator iter = list.iterator(BytesRef
+          .getUTF8SortedAsUTF16Comparator());
+      int i = 0;
+      while ((spare = iter.next()) != null) {
+        assertEquals("entry " + i + " doesn't match", stringList.get(i),
+            spare.utf8ToString());
+        i++;
+      }
+      assertNull(iter.next());
+      assertEquals(i, stringList.size());
+    }
+    
+  }
+  
+}

Property changes on: lucene/suggest/src/test/org/apache/lucene/search/suggest/TestBytesRefArray.java
___________________________________________________________________
Added: svn:eol-style
## -0,0 +1 ##
+native
Index: lucene/suggest/src/java/org/apache/lucene/search/suggest/SortedTermFreqIteratorWrapper.java
===================================================================
--- lucene/suggest/src/java/org/apache/lucene/search/suggest/SortedTermFreqIteratorWrapper.java	(revision 1442822)
+++ lucene/suggest/src/java/org/apache/lucene/search/suggest/SortedTermFreqIteratorWrapper.java	(working copy)
@@ -22,9 +22,8 @@
 import java.util.Comparator;
 
 import org.apache.lucene.search.spell.TermFreqIterator;
-import org.apache.lucene.search.suggest.fst.Sort;
-import org.apache.lucene.search.suggest.fst.Sort.ByteSequencesReader;
-import org.apache.lucene.search.suggest.fst.Sort.ByteSequencesWriter;
+import org.apache.lucene.search.suggest.Sort.ByteSequencesReader;
+import org.apache.lucene.search.suggest.Sort.ByteSequencesWriter;
 import org.apache.lucene.store.ByteArrayDataInput;
 import org.apache.lucene.store.ByteArrayDataOutput;
 import org.apache.lucene.util.ArrayUtil;
Index: lucene/suggest/src/java/org/apache/lucene/search/suggest/BytesRefArray.java
===================================================================
--- lucene/suggest/src/java/org/apache/lucene/search/suggest/BytesRefArray.java	(revision 0)
+++ lucene/suggest/src/java/org/apache/lucene/search/suggest/BytesRefArray.java	(working copy)
@@ -0,0 +1,197 @@
+package org.apache.lucene.search.suggest;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements. See the NOTICE file distributed with this
+ * work for additional information regarding copyright ownership. The ASF
+ * licenses this file to You under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ * 
+ * http://www.apache.org/licenses/LICENSE-2.0
+ * 
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
+ * License for the specific language governing permissions and limitations under
+ * the License.
+ */
+
+import java.util.Arrays;
+import java.util.Comparator;
+
+import org.apache.lucene.util.ArrayUtil;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.ByteBlockPool;
+import org.apache.lucene.util.BytesRefIterator;
+import org.apache.lucene.util.Counter;
+import org.apache.lucene.util.RamUsageEstimator;
+import org.apache.lucene.util.SorterTemplate;
+
+/**
+ * A simple append only random-access {@link BytesRef} array that stores full
+ * copies of the appended bytes in a {@link ByteBlockPool}.
+ * 
+ * 
+ * <b>Note: This class is not Thread-Safe!</b>
+ * 
+ * @lucene.internal
+ * @lucene.experimental
+ */
+public final class BytesRefArray {
+  private final ByteBlockPool pool;
+  private int[] offsets = new int[1];
+  private int lastElement = 0;
+  private int currentOffset = 0;
+  private final Counter bytesUsed;
+  
+  /**
+   * Creates a new {@link BytesRefArray} with a counter to track allocated bytes
+   */
+  public BytesRefArray(Counter bytesUsed) {
+    this.pool = new ByteBlockPool(new ByteBlockPool.DirectTrackingAllocator(
+        bytesUsed));
+    pool.nextBuffer();
+    bytesUsed.addAndGet(RamUsageEstimator.NUM_BYTES_ARRAY_HEADER
+        + RamUsageEstimator.NUM_BYTES_INT);
+    this.bytesUsed = bytesUsed;
+  }
+ 
+  /**
+   * Clears this {@link BytesRefArray}
+   */
+  public void clear() {
+    lastElement = 0;
+    currentOffset = 0;
+    Arrays.fill(offsets, 0);
+    pool.reset(false, true); // no need to 0 fill the buffers we control the allocator
+  }
+  
+  /**
+   * Appends a copy of the given {@link BytesRef} to this {@link BytesRefArray}.
+   * @param bytes the bytes to append
+   * @return the ordinal of the appended bytes
+   */
+  public int append(BytesRef bytes) {
+    if (lastElement >= offsets.length) {
+      int oldLen = offsets.length;
+      offsets = ArrayUtil.grow(offsets, offsets.length + 1);
+      bytesUsed.addAndGet((offsets.length - oldLen)
+          * RamUsageEstimator.NUM_BYTES_INT);
+    }
+    pool.append(bytes);
+    offsets[lastElement++] = currentOffset;
+    currentOffset += bytes.length;
+    return lastElement;
+  }
+  
+  /**
+   * Returns the current size of this {@link BytesRefArray}
+   * @return the current size of this {@link BytesRefArray}
+   */
+  public int size() {
+    return lastElement;
+  }
+  
+  /**
+   * Returns the <i>n'th</i> element of this {@link BytesRefArray}
+   * @param spare a spare {@link BytesRef} instance
+   * @param ord the elements ordinal to retrieve 
+   * @return the <i>n'th</i> element of this {@link BytesRefArray}
+   */
+  public BytesRef get(BytesRef spare, int ord) {
+    if (lastElement > ord) {
+      int offset = offsets[ord];
+      int length = ord == lastElement - 1 ? currentOffset - offset
+          : offsets[ord + 1] - offset;
+      assert spare.offset == 0;
+      spare.grow(length);
+      spare.length = length;
+      pool.readBytes(offset, spare.bytes, spare.offset, spare.length);
+      return spare;
+    }
+    throw new IndexOutOfBoundsException("index " + ord
+        + " must be less than the size: " + lastElement);
+    
+  }
+  
+  private int[] sort(final Comparator<BytesRef> comp) {
+    final int[] orderedEntries = new int[size()];
+    for (int i = 0; i < orderedEntries.length; i++) {
+      orderedEntries[i] = i;
+    }
+    new SorterTemplate() {
+      @Override
+      protected void swap(int i, int j) {
+        final int o = orderedEntries[i];
+        orderedEntries[i] = orderedEntries[j];
+        orderedEntries[j] = o;
+      }
+      
+      @Override
+      protected int compare(int i, int j) {
+        final int ord1 = orderedEntries[i], ord2 = orderedEntries[j];
+        return comp.compare(get(scratch1, ord1), get(scratch2, ord2));
+      }
+      
+      @Override
+      protected void setPivot(int i) {
+        final int ord = orderedEntries[i];
+        get(pivot, ord);
+      }
+      
+      @Override
+      protected int comparePivot(int j) {
+        final int ord = orderedEntries[j];
+        return comp.compare(pivot, get(scratch2, ord));
+      }
+      
+      private final BytesRef pivot = new BytesRef(), scratch1 = new BytesRef(),
+          scratch2 = new BytesRef();
+    }.quickSort(0, size() - 1);
+    return orderedEntries;
+  }
+  
+  /**
+   * sugar for {@link #iterator(Comparator)} with a <code>null</code> comparator
+   */
+  public BytesRefIterator iterator() {
+    return iterator(null);
+  }
+  
+  /**
+   * <p>
+   * Returns a {@link BytesRefIterator} with point in time semantics. The
+   * iterator provides access to all so far appended {@link BytesRef} instances.
+   * </p>
+   * <p>
+   * If a non <code>null</code> {@link Comparator} is provided the iterator will
+   * iterate the byte values in the order specified by the comparator. Otherwise
+   * the order is the same as the values were appended.
+   * </p>
+   * <p>
+   * This is a non-destructive operation.
+   * </p>
+   */
+  public BytesRefIterator iterator(final Comparator<BytesRef> comp) {
+    final BytesRef spare = new BytesRef();
+    final int size = size();
+    final int[] ords = comp == null ? null : sort(comp);
+    return new BytesRefIterator() {
+      int pos = 0;
+      
+      @Override
+      public BytesRef next() {
+        if (pos < size) {
+          return get(spare, ords == null ? pos++ : ords[pos++]);
+        }
+        return null;
+      }
+      
+      @Override
+      public Comparator<BytesRef> getComparator() {
+        return comp;
+      }
+    };
+  }
+}

Property changes on: lucene/suggest/src/java/org/apache/lucene/search/suggest/BytesRefArray.java
___________________________________________________________________
Added: svn:eol-style
## -0,0 +1 ##
+native
Index: lucene/suggest/src/java/org/apache/lucene/search/suggest/Sort.java
===================================================================
--- lucene/suggest/src/java/org/apache/lucene/search/suggest/Sort.java	(revision 0)
+++ lucene/suggest/src/java/org/apache/lucene/search/suggest/Sort.java	(working copy)
@@ -0,0 +1,559 @@
+package org.apache.lucene.search.suggest;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.*;
+import java.util.*;
+
+import org.apache.lucene.util.*;
+import org.apache.lucene.util.PriorityQueue;
+
+/**
+ * On-disk sorting of byte arrays. Each byte array (entry) is a composed of the following
+ * fields:
+ * <ul>
+ *   <li>(two bytes) length of the following byte array,
+ *   <li>exactly the above count of bytes for the sequence to be sorted.
+ * </ul>
+ * 
+ * @see #sort(File, File)
+ * @lucene.experimental
+ * @lucene.internal
+ */
+public final class Sort {
+  /** Convenience constant for megabytes */
+  public final static long MB = 1024 * 1024;
+  /** Convenience constant for gigabytes */
+  public final static long GB = MB * 1024;
+  
+  /**
+   * Minimum recommended buffer size for sorting.
+   */
+  public final static long MIN_BUFFER_SIZE_MB = 32;
+
+  /**
+   * Absolute minimum required buffer size for sorting.
+   */
+  public static final long ABSOLUTE_MIN_SORT_BUFFER_SIZE = MB / 2;
+  private static final String MIN_BUFFER_SIZE_MSG = "At least 0.5MB RAM buffer is needed";
+
+  /**
+   * Maximum number of temporary files before doing an intermediate merge.
+   */
+  public final static int MAX_TEMPFILES = 128;
+
+  /** 
+   * A bit more descriptive unit for constructors.
+   * 
+   * @see #automatic()
+   * @see #megabytes(long)
+   */
+  public static final class BufferSize {
+    final int bytes;
+  
+    private BufferSize(long bytes) {
+      if (bytes > Integer.MAX_VALUE) {
+        throw new IllegalArgumentException("Buffer too large for Java ("
+            + (Integer.MAX_VALUE / MB) + "mb max): " + bytes);
+      }
+      
+      if (bytes < ABSOLUTE_MIN_SORT_BUFFER_SIZE) {
+        throw new IllegalArgumentException(MIN_BUFFER_SIZE_MSG + ": " + bytes);
+      }
+  
+      this.bytes = (int) bytes;
+    }
+    
+    /**
+     * Creates a {@link BufferSize} in MB. The given 
+     * values must be $gt; 0 and &lt; 2048.
+     */
+    public static BufferSize megabytes(long mb) {
+      return new BufferSize(mb * MB);
+    }
+  
+    /** 
+     * Approximately half of the currently available free heap, but no less
+     * than {@link #ABSOLUTE_MIN_SORT_BUFFER_SIZE}. However if current heap allocation 
+     * is insufficient or if there is a large portion of unallocated heap-space available 
+     * for sorting consult with max allowed heap size. 
+     */
+    public static BufferSize automatic() {
+      Runtime rt = Runtime.getRuntime();
+      
+      // take sizes in "conservative" order
+      final long max = rt.maxMemory(); // max allocated
+      final long total = rt.totalMemory(); // currently allocated
+      final long free = rt.freeMemory(); // unused portion of currently allocated
+      final long totalAvailableBytes = max - total + free;
+      
+      // by free mem (attempting to not grow the heap for this)
+      long sortBufferByteSize = free/2;
+      final long minBufferSizeBytes = MIN_BUFFER_SIZE_MB*MB;
+      if (sortBufferByteSize <  minBufferSizeBytes
+          || totalAvailableBytes > 10 * minBufferSizeBytes) { // lets see if we need/should to grow the heap 
+        if (totalAvailableBytes/2 > minBufferSizeBytes){ // there is enough mem for a reasonable buffer
+          sortBufferByteSize = totalAvailableBytes/2; // grow the heap
+        } else {
+          //heap seems smallish lets be conservative fall back to the free/2 
+          sortBufferByteSize = Math.max(ABSOLUTE_MIN_SORT_BUFFER_SIZE, sortBufferByteSize);
+        }
+      }
+      return new BufferSize(Math.min((long)Integer.MAX_VALUE, sortBufferByteSize));
+    }
+  }
+  
+  /**
+   * Sort info (debugging mostly).
+   */
+  public class SortInfo {
+    /** number of temporary files created when merging partitions */
+    public int tempMergeFiles;
+    /** number of partition merges */
+    public int mergeRounds;
+    /** number of lines of data read */
+    public int lines;
+    /** time spent merging sorted partitions (in milliseconds) */
+    public long mergeTime;
+    /** time spent sorting data (in milliseconds) */
+    public long sortTime;
+    /** total time spent (in milliseconds) */
+    public long totalTime;
+    /** time spent in i/o read (in milliseconds) */
+    public long readTime;
+    /** read buffer size (in bytes) */
+    public final long bufferSize = ramBufferSize.bytes;
+    
+    /** create a new SortInfo (with empty statistics) for debugging */
+    public SortInfo() {}
+    
+    @Override
+    public String toString() {
+      return String.format(Locale.ROOT,
+          "time=%.2f sec. total (%.2f reading, %.2f sorting, %.2f merging), lines=%d, temp files=%d, merges=%d, soft ram limit=%.2f MB",
+          totalTime / 1000.0d, readTime / 1000.0d, sortTime / 1000.0d, mergeTime / 1000.0d,
+          lines, tempMergeFiles, mergeRounds,
+          (double) bufferSize / MB);
+    }
+  }
+
+  private final BufferSize ramBufferSize;
+  private final File tempDirectory;
+  
+  private final Counter bufferBytesUsed = Counter.newCounter();
+  private final BytesRefArray buffer = new BytesRefArray(bufferBytesUsed);
+  private SortInfo sortInfo;
+  private int maxTempFiles;
+  private final Comparator<BytesRef> comparator;
+  
+  /** Default comparator: sorts in binary (codepoint) order */
+  public static final Comparator<BytesRef> DEFAULT_COMPARATOR = BytesRef.getUTF8SortedAsUnicodeComparator();
+
+  /**
+   * Defaults constructor.
+   * 
+   * @see #defaultTempDir()
+   * @see BufferSize#automatic()
+   */
+  public Sort() throws IOException {
+    this(DEFAULT_COMPARATOR, BufferSize.automatic(), defaultTempDir(), MAX_TEMPFILES);
+  }
+  
+  /**
+   * Defaults constructor with a custom comparator.
+   * 
+   * @see #defaultTempDir()
+   * @see BufferSize#automatic()
+   */
+  public Sort(Comparator<BytesRef> comparator) throws IOException {
+    this(comparator, BufferSize.automatic(), defaultTempDir(), MAX_TEMPFILES);
+  }
+
+  /**
+   * All-details constructor.
+   */
+  public Sort(Comparator<BytesRef> comparator, BufferSize ramBufferSize, File tempDirectory, int maxTempfiles) {
+    if (ramBufferSize.bytes < ABSOLUTE_MIN_SORT_BUFFER_SIZE) {
+      throw new IllegalArgumentException(MIN_BUFFER_SIZE_MSG + ": " + ramBufferSize.bytes);
+    }
+    
+    if (maxTempfiles < 2) {
+      throw new IllegalArgumentException("maxTempFiles must be >= 2");
+    }
+
+    this.ramBufferSize = ramBufferSize;
+    this.tempDirectory = tempDirectory;
+    this.maxTempFiles = maxTempfiles;
+    this.comparator = comparator;
+  }
+
+  /** 
+   * Sort input to output, explicit hint for the buffer size. The amount of allocated
+   * memory may deviate from the hint (may be smaller or larger).  
+   */
+  public SortInfo sort(File input, File output) throws IOException {
+    sortInfo = new SortInfo();
+    sortInfo.totalTime = System.currentTimeMillis();
+
+    output.delete();
+
+    ArrayList<File> merges = new ArrayList<File>();
+    boolean success2 = false;
+    try {
+      ByteSequencesReader is = new ByteSequencesReader(input);
+      boolean success = false;
+      try {
+        int lines = 0;
+        while ((lines = readPartition(is)) > 0) {
+          merges.add(sortPartition(lines));
+          sortInfo.tempMergeFiles++;
+          sortInfo.lines += lines;
+
+          // Handle intermediate merges.
+          if (merges.size() == maxTempFiles) {
+            File intermediate = File.createTempFile("sort", "intermediate", tempDirectory);
+            try {
+              mergePartitions(merges, intermediate);
+            } finally {
+              for (File file : merges) {
+                file.delete();
+              }
+              merges.clear();
+              merges.add(intermediate);
+            }
+            sortInfo.tempMergeFiles++;
+          }
+        }
+        success = true;
+      } finally {
+        if (success)
+          IOUtils.close(is);
+        else
+          IOUtils.closeWhileHandlingException(is);
+      }
+
+      // One partition, try to rename or copy if unsuccessful.
+      if (merges.size() == 1) {     
+        File single = merges.get(0);
+        // If simple rename doesn't work this means the output is
+        // on a different volume or something. Copy the input then.
+        if (!single.renameTo(output)) {
+          copy(single, output);
+        }
+      } else { 
+        // otherwise merge the partitions with a priority queue.
+        mergePartitions(merges, output);
+      }
+      success2 = true;
+    } finally {
+      for (File file : merges) {
+        file.delete();
+      }
+      if (!success2) {
+        output.delete();
+      }
+    }
+
+    sortInfo.totalTime = (System.currentTimeMillis() - sortInfo.totalTime); 
+    return sortInfo;
+  }
+
+  /**
+   * Returns the default temporary directory. By default, java.io.tmpdir. If not accessible
+   * or not available, an IOException is thrown
+   */
+  public static File defaultTempDir() throws IOException {
+    String tempDirPath = System.getProperty("java.io.tmpdir");
+    if (tempDirPath == null) 
+      throw new IOException("Java has no temporary folder property (java.io.tmpdir)?");
+
+    File tempDirectory = new File(tempDirPath);
+    if (!tempDirectory.exists() || !tempDirectory.canWrite()) {
+      throw new IOException("Java's temporary folder not present or writeable?: " 
+          + tempDirectory.getAbsolutePath());
+    }
+    return tempDirectory;
+  }
+
+  /**
+   * Copies one file to another.
+   */
+  private static void copy(File file, File output) throws IOException {
+    // 64kb copy buffer (empirical pick).
+    byte [] buffer = new byte [16 * 1024];
+    InputStream is = null;
+    OutputStream os = null;
+    try {
+      is = new FileInputStream(file);
+      os = new FileOutputStream(output);
+      int length;
+      while ((length = is.read(buffer)) > 0) {
+        os.write(buffer, 0, length);
+      }
+    } finally {
+      IOUtils.close(is, os);
+    }
+  }
+
+  /** Sort a single partition in-memory. */
+  protected File sortPartition(int len) throws IOException {
+    BytesRefArray data = this.buffer;
+    File tempFile = File.createTempFile("sort", "partition", tempDirectory);
+
+    long start = System.currentTimeMillis();
+    sortInfo.sortTime += (System.currentTimeMillis() - start);
+    
+    final ByteSequencesWriter out = new ByteSequencesWriter(tempFile);
+    BytesRef spare;
+    try {
+      BytesRefIterator iter = buffer.iterator(comparator);
+      while((spare = iter.next()) != null) {
+        assert spare.length <= Short.MAX_VALUE;
+        out.write(spare);
+      }
+      
+      out.close();
+
+      // Clean up the buffer for the next partition.
+      data.clear();
+      return tempFile;
+    } finally {
+      IOUtils.close(out);
+    }
+  }
+
+  /** Merge a list of sorted temporary files (partitions) into an output file */
+  void mergePartitions(List<File> merges, File outputFile) throws IOException {
+    long start = System.currentTimeMillis();
+
+    ByteSequencesWriter out = new ByteSequencesWriter(outputFile);
+
+    PriorityQueue<FileAndTop> queue = new PriorityQueue<FileAndTop>(merges.size()) {
+      @Override
+      protected boolean lessThan(FileAndTop a, FileAndTop b) {
+        return comparator.compare(a.current, b.current) < 0;
+      }
+    };
+
+    ByteSequencesReader [] streams = new ByteSequencesReader [merges.size()];
+    try {
+      // Open streams and read the top for each file
+      for (int i = 0; i < merges.size(); i++) {
+        streams[i] = new ByteSequencesReader(merges.get(i));
+        byte line[] = streams[i].read();
+        if (line != null) {
+          queue.insertWithOverflow(new FileAndTop(i, line));
+        }
+      }
+  
+      // Unix utility sort() uses ordered array of files to pick the next line from, updating
+      // it as it reads new lines. The PQ used here is a more elegant solution and has 
+      // a nicer theoretical complexity bound :) The entire sorting process is I/O bound anyway
+      // so it shouldn't make much of a difference (didn't check).
+      FileAndTop top;
+      while ((top = queue.top()) != null) {
+        out.write(top.current);
+        if (!streams[top.fd].read(top.current)) {
+          queue.pop();
+        } else {
+          queue.updateTop();
+        }
+      }
+  
+      sortInfo.mergeTime += System.currentTimeMillis() - start;
+      sortInfo.mergeRounds++;
+    } finally {
+      // The logic below is: if an exception occurs in closing out, it has a priority over exceptions
+      // happening in closing streams.
+      try {
+        IOUtils.close(streams);
+      } finally {
+        IOUtils.close(out);
+      }
+    }
+  }
+
+  /** Read in a single partition of data */
+  int readPartition(ByteSequencesReader reader) throws IOException {
+    long start = System.currentTimeMillis();
+    final BytesRef scratch = new BytesRef();
+    while ((scratch.bytes = reader.read()) != null) {
+      scratch.length = scratch.bytes.length; 
+      buffer.append(scratch);
+      // Account for the created objects.
+      // (buffer slots do not account to buffer size.) 
+      if (ramBufferSize.bytes < bufferBytesUsed.get()) {
+        break;
+      }
+    }
+    sortInfo.readTime += (System.currentTimeMillis() - start);
+    return buffer.size();
+  }
+
+  static class FileAndTop {
+    final int fd;
+    final BytesRef current;
+
+    FileAndTop(int fd, byte [] firstLine) {
+      this.fd = fd;
+      this.current = new BytesRef(firstLine);
+    }
+  }
+
+  /**
+   * Utility class to emit length-prefixed byte[] entries to an output stream for sorting.
+   * Complementary to {@link ByteSequencesReader}.
+   */
+  public static class ByteSequencesWriter implements Closeable {
+    private final DataOutput os;
+
+    /** Constructs a ByteSequencesWriter to the provided File */
+    public ByteSequencesWriter(File file) throws IOException {
+      this(new DataOutputStream(
+          new BufferedOutputStream(
+              new FileOutputStream(file))));
+    }
+
+    /** Constructs a ByteSequencesWriter to the provided DataOutput */
+    public ByteSequencesWriter(DataOutput os) {
+      this.os = os;
+    }
+
+    /**
+     * Writes a BytesRef.
+     * @see #write(byte[], int, int)
+     */
+    public void write(BytesRef ref) throws IOException {
+      assert ref != null;
+      write(ref.bytes, ref.offset, ref.length);
+    }
+
+    /**
+     * Writes a byte array.
+     * @see #write(byte[], int, int)
+     */
+    public void write(byte [] bytes) throws IOException {
+      write(bytes, 0, bytes.length);
+    }
+
+    /**
+     * Writes a byte array.
+     * <p>
+     * The length is written as a <code>short</code>, followed
+     * by the bytes.
+     */
+    public void write(byte [] bytes, int off, int len) throws IOException {
+      assert bytes != null;
+      assert off >= 0 && off + len <= bytes.length;
+      assert len >= 0;
+      os.writeShort(len);
+      os.write(bytes, off, len);
+    }        
+    
+    /**
+     * Closes the provided {@link DataOutput} if it is {@link Closeable}.
+     */
+    @Override
+    public void close() throws IOException {
+      if (os instanceof Closeable) {
+        ((Closeable) os).close();
+      }
+    }    
+  }
+
+  /**
+   * Utility class to read length-prefixed byte[] entries from an input.
+   * Complementary to {@link ByteSequencesWriter}.
+   */
+  public static class ByteSequencesReader implements Closeable {
+    private final DataInput is;
+
+    /** Constructs a ByteSequencesReader from the provided File */
+    public ByteSequencesReader(File file) throws IOException {
+      this(new DataInputStream(
+          new BufferedInputStream(
+              new FileInputStream(file))));
+    }
+
+    /** Constructs a ByteSequencesReader from the provided DataInput */
+    public ByteSequencesReader(DataInput is) {
+      this.is = is;
+    }
+
+    /**
+     * Reads the next entry into the provided {@link BytesRef}. The internal
+     * storage is resized if needed.
+     * 
+     * @return Returns <code>false</code> if EOF occurred when trying to read
+     * the header of the next sequence. Returns <code>true</code> otherwise.
+     * @throws EOFException if the file ends before the full sequence is read.
+     */
+    public boolean read(BytesRef ref) throws IOException {
+      short length;
+      try {
+        length = is.readShort();
+      } catch (EOFException e) {
+        return false;
+      }
+
+      ref.grow(length);
+      ref.offset = 0;
+      ref.length = length;
+      is.readFully(ref.bytes, 0, length);
+      return true;
+    }
+
+    /**
+     * Reads the next entry and returns it if successful.
+     * 
+     * @see #read(BytesRef)
+     * 
+     * @return Returns <code>null</code> if EOF occurred before the next entry
+     * could be read.
+     * @throws EOFException if the file ends before the full sequence is read.
+     */
+    public byte[] read() throws IOException {
+      short length;
+      try {
+        length = is.readShort();
+      } catch (EOFException e) {
+        return null;
+      }
+
+      assert length >= 0 : "Sanity: sequence length < 0: " + length;
+      byte [] result = new byte [length];
+      is.readFully(result);
+      return result;
+    }
+
+    /**
+     * Closes the provided {@link DataInput} if it is {@link Closeable}.
+     */
+    @Override
+    public void close() throws IOException {
+      if (is instanceof Closeable) {
+        ((Closeable) is).close();
+      }
+    }
+  }
+
+  /** Returns the comparator in use to sort entries */
+  public Comparator<BytesRef> getComparator() {
+    return comparator;
+  }  
+}

Property changes on: lucene/suggest/src/java/org/apache/lucene/search/suggest/Sort.java
___________________________________________________________________
Added: svn:eol-style
## -0,0 +1 ##
+native
Index: lucene/suggest/src/java/org/apache/lucene/search/suggest/BufferingTermFreqIteratorWrapper.java
===================================================================
--- lucene/suggest/src/java/org/apache/lucene/search/suggest/BufferingTermFreqIteratorWrapper.java	(revision 1442822)
+++ lucene/suggest/src/java/org/apache/lucene/search/suggest/BufferingTermFreqIteratorWrapper.java	(working copy)
@@ -22,6 +22,7 @@
 import org.apache.lucene.search.spell.TermFreqIterator;
 import org.apache.lucene.util.ArrayUtil;
 import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.Counter;
 
 /**
  * This wrapper buffers incoming elements.
@@ -30,7 +31,7 @@
 public class BufferingTermFreqIteratorWrapper implements TermFreqIterator {
   // TODO keep this for now
   /** buffered term entries */
-  protected BytesRefList entries = new BytesRefList();
+  protected BytesRefArray entries = new BytesRefArray(Counter.newCounter());
   /** current buffer position */
   protected int curPos = -1;
   /** buffered weights, parallel with {@link #entries} */
Index: lucene/suggest/src/java/org/apache/lucene/search/suggest/InMemorySorter.java
===================================================================
--- lucene/suggest/src/java/org/apache/lucene/search/suggest/InMemorySorter.java	(revision 0)
+++ lucene/suggest/src/java/org/apache/lucene/search/suggest/InMemorySorter.java	(working copy)
@@ -0,0 +1,61 @@
+package org.apache.lucene.search.suggest;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.util.Comparator;
+
+import org.apache.lucene.search.suggest.fst.BytesRefSorter;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.BytesRefIterator;
+import org.apache.lucene.util.Counter;
+
+/**
+ * An {@link BytesRefSorter} that keeps all the entries in memory.
+ * @lucene.experimental
+ * @lucene.internal
+ */
+public final class InMemorySorter implements BytesRefSorter {
+  private final BytesRefArray buffer = new BytesRefArray(Counter.newCounter());
+  private boolean closed = false;
+  private final Comparator<BytesRef> comparator;
+
+  /**
+   * Creates an InMemorySorter, sorting entries by the
+   * provided comparator.
+   */
+  public InMemorySorter(Comparator<BytesRef> comparator) {
+    this.comparator = comparator;
+  }
+  
+  @Override
+  public void add(BytesRef utf8) {
+    if (closed) throw new IllegalStateException();
+    buffer.append(utf8);
+  }
+
+  @Override
+  public BytesRefIterator iterator() {
+    closed = true;
+    return buffer.iterator(comparator);
+  }
+
+  @Override
+  public Comparator<BytesRef> getComparator() {
+    return comparator;
+  }
+}

Property changes on: lucene/suggest/src/java/org/apache/lucene/search/suggest/InMemorySorter.java
___________________________________________________________________
Added: svn:eol-style
## -0,0 +1 ##
+native
Index: lucene/suggest/src/java/org/apache/lucene/search/suggest/fst/FSTCompletionBuilder.java
===================================================================
--- lucene/suggest/src/java/org/apache/lucene/search/suggest/fst/FSTCompletionBuilder.java	(revision 1442822)
+++ lucene/suggest/src/java/org/apache/lucene/search/suggest/fst/FSTCompletionBuilder.java	(working copy)
@@ -20,6 +20,7 @@
 import java.io.Closeable;
 import java.io.IOException;
 
+import org.apache.lucene.search.suggest.InMemorySorter;
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.BytesRefIterator;
 import org.apache.lucene.util.IntsRef;
Index: lucene/suggest/src/java/org/apache/lucene/search/suggest/fst/InMemorySorter.java
===================================================================
--- lucene/suggest/src/java/org/apache/lucene/search/suggest/fst/InMemorySorter.java	(revision 1442822)
+++ lucene/suggest/src/java/org/apache/lucene/search/suggest/fst/InMemorySorter.java	(working copy)
@@ -1,60 +0,0 @@
-package org.apache.lucene.search.suggest.fst;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.util.Comparator;
-
-import org.apache.lucene.search.suggest.BytesRefList;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.BytesRefIterator;
-
-/**
- * An {@link BytesRefSorter} that keeps all the entries in memory.
- * @lucene.experimental
- * @lucene.internal
- */
-public final class InMemorySorter implements BytesRefSorter {
-  private final BytesRefList buffer = new BytesRefList();
-  private boolean closed = false;
-  private final Comparator<BytesRef> comparator;
-
-  /**
-   * Creates an InMemorySorter, sorting entries by the
-   * provided comparator.
-   */
-  public InMemorySorter(Comparator<BytesRef> comparator) {
-    this.comparator = comparator;
-  }
-  
-  @Override
-  public void add(BytesRef utf8) {
-    if (closed) throw new IllegalStateException();
-    buffer.append(utf8);
-  }
-
-  @Override
-  public BytesRefIterator iterator() {
-    closed = true;
-    return buffer.iterator(comparator);
-  }
-
-  @Override
-  public Comparator<BytesRef> getComparator() {
-    return comparator;
-  }
-}
Index: lucene/suggest/src/java/org/apache/lucene/search/suggest/fst/WFSTCompletionLookup.java
===================================================================
--- lucene/suggest/src/java/org/apache/lucene/search/suggest/fst/WFSTCompletionLookup.java	(revision 1442822)
+++ lucene/suggest/src/java/org/apache/lucene/search/suggest/fst/WFSTCompletionLookup.java	(working copy)
@@ -28,7 +28,7 @@
 import org.apache.lucene.search.spell.TermFreqIterator;
 import org.apache.lucene.search.suggest.Lookup;
 import org.apache.lucene.search.suggest.SortedTermFreqIteratorWrapper;
-import org.apache.lucene.search.suggest.fst.Sort.ByteSequencesWriter;
+import org.apache.lucene.search.suggest.Sort.ByteSequencesWriter;
 import org.apache.lucene.store.ByteArrayDataInput;
 import org.apache.lucene.store.ByteArrayDataOutput;
 import org.apache.lucene.store.InputStreamDataInput;
Index: lucene/suggest/src/java/org/apache/lucene/search/suggest/fst/ExternalRefSorter.java
===================================================================
--- lucene/suggest/src/java/org/apache/lucene/search/suggest/fst/ExternalRefSorter.java	(revision 1442822)
+++ lucene/suggest/src/java/org/apache/lucene/search/suggest/fst/ExternalRefSorter.java	(working copy)
@@ -20,7 +20,8 @@
 import java.io.*;
 import java.util.Comparator;
 
-import org.apache.lucene.search.suggest.fst.Sort.ByteSequencesReader;
+import org.apache.lucene.search.suggest.Sort;
+import org.apache.lucene.search.suggest.Sort.ByteSequencesReader;
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.BytesRefIterator;
 import org.apache.lucene.util.IOUtils;
Index: lucene/suggest/src/java/org/apache/lucene/search/suggest/fst/FSTCompletionLookup.java
===================================================================
--- lucene/suggest/src/java/org/apache/lucene/search/suggest/fst/FSTCompletionLookup.java	(revision 1442822)
+++ lucene/suggest/src/java/org/apache/lucene/search/suggest/fst/FSTCompletionLookup.java	(working copy)
@@ -26,8 +26,9 @@
 
 import org.apache.lucene.search.spell.TermFreqIterator;
 import org.apache.lucene.search.suggest.Lookup;
+import org.apache.lucene.search.suggest.Sort;
+import org.apache.lucene.search.suggest.Sort.SortInfo;
 import org.apache.lucene.search.suggest.fst.FSTCompletion.Completion;
-import org.apache.lucene.search.suggest.fst.Sort.SortInfo;
 import org.apache.lucene.search.suggest.tst.TSTLookup;
 import org.apache.lucene.store.ByteArrayDataInput;
 import org.apache.lucene.store.ByteArrayDataOutput;
Index: lucene/suggest/src/java/org/apache/lucene/search/suggest/fst/Sort.java
===================================================================
--- lucene/suggest/src/java/org/apache/lucene/search/suggest/fst/Sort.java	(revision 1442822)
+++ lucene/suggest/src/java/org/apache/lucene/search/suggest/fst/Sort.java	(working copy)
@@ -1,559 +0,0 @@
-package org.apache.lucene.search.suggest.fst;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.*;
-import java.util.*;
-
-import org.apache.lucene.search.suggest.BytesRefList;
-import org.apache.lucene.util.*;
-import org.apache.lucene.util.PriorityQueue;
-
-/**
- * On-disk sorting of byte arrays. Each byte array (entry) is a composed of the following
- * fields:
- * <ul>
- *   <li>(two bytes) length of the following byte array,
- *   <li>exactly the above count of bytes for the sequence to be sorted.
- * </ul>
- * 
- * @see #sort(File, File)
- * @lucene.experimental
- * @lucene.internal
- */
-public final class Sort {
-  /** Convenience constant for megabytes */
-  public final static long MB = 1024 * 1024;
-  /** Convenience constant for gigabytes */
-  public final static long GB = MB * 1024;
-  
-  /**
-   * Minimum recommended buffer size for sorting.
-   */
-  public final static long MIN_BUFFER_SIZE_MB = 32;
-
-  /**
-   * Absolute minimum required buffer size for sorting.
-   */
-  public static final long ABSOLUTE_MIN_SORT_BUFFER_SIZE = MB / 2;
-  private static final String MIN_BUFFER_SIZE_MSG = "At least 0.5MB RAM buffer is needed";
-
-  /**
-   * Maximum number of temporary files before doing an intermediate merge.
-   */
-  public final static int MAX_TEMPFILES = 128;
-
-  /** 
-   * A bit more descriptive unit for constructors.
-   * 
-   * @see #automatic()
-   * @see #megabytes(long)
-   */
-  public static final class BufferSize {
-    final int bytes;
-  
-    private BufferSize(long bytes) {
-      if (bytes > Integer.MAX_VALUE) {
-        throw new IllegalArgumentException("Buffer too large for Java ("
-            + (Integer.MAX_VALUE / MB) + "mb max): " + bytes);
-      }
-      
-      if (bytes < ABSOLUTE_MIN_SORT_BUFFER_SIZE) {
-        throw new IllegalArgumentException(MIN_BUFFER_SIZE_MSG + ": " + bytes);
-      }
-  
-      this.bytes = (int) bytes;
-    }
-    
-    /**
-     * Creates a {@link BufferSize} in MB. The given 
-     * values must be $gt; 0 and &lt; 2048.
-     */
-    public static BufferSize megabytes(long mb) {
-      return new BufferSize(mb * MB);
-    }
-  
-    /** 
-     * Approximately half of the currently available free heap, but no less
-     * than {@link #ABSOLUTE_MIN_SORT_BUFFER_SIZE}. However if current heap allocation 
-     * is insufficient or if there is a large portion of unallocated heap-space available 
-     * for sorting consult with max allowed heap size. 
-     */
-    public static BufferSize automatic() {
-      Runtime rt = Runtime.getRuntime();
-      
-      // take sizes in "conservative" order
-      final long max = rt.maxMemory(); // max allocated
-      final long total = rt.totalMemory(); // currently allocated
-      final long free = rt.freeMemory(); // unused portion of currently allocated
-      final long totalAvailableBytes = max - total + free;
-      
-      // by free mem (attempting to not grow the heap for this)
-      long sortBufferByteSize = free/2;
-      final long minBufferSizeBytes = MIN_BUFFER_SIZE_MB*MB;
-      if (sortBufferByteSize <  minBufferSizeBytes
-          || totalAvailableBytes > 10 * minBufferSizeBytes) { // lets see if we need/should to grow the heap 
-        if (totalAvailableBytes/2 > minBufferSizeBytes){ // there is enough mem for a reasonable buffer
-          sortBufferByteSize = totalAvailableBytes/2; // grow the heap
-        } else {
-          //heap seems smallish lets be conservative fall back to the free/2 
-          sortBufferByteSize = Math.max(ABSOLUTE_MIN_SORT_BUFFER_SIZE, sortBufferByteSize);
-        }
-      }
-      return new BufferSize(Math.min((long)Integer.MAX_VALUE, sortBufferByteSize));
-    }
-  }
-  
-  /**
-   * Sort info (debugging mostly).
-   */
-  public class SortInfo {
-    /** number of temporary files created when merging partitions */
-    public int tempMergeFiles;
-    /** number of partition merges */
-    public int mergeRounds;
-    /** number of lines of data read */
-    public int lines;
-    /** time spent merging sorted partitions (in milliseconds) */
-    public long mergeTime;
-    /** time spent sorting data (in milliseconds) */
-    public long sortTime;
-    /** total time spent (in milliseconds) */
-    public long totalTime;
-    /** time spent in i/o read (in milliseconds) */
-    public long readTime;
-    /** read buffer size (in bytes) */
-    public final long bufferSize = ramBufferSize.bytes;
-    
-    /** create a new SortInfo (with empty statistics) for debugging */
-    public SortInfo() {}
-    
-    @Override
-    public String toString() {
-      return String.format(Locale.ROOT,
-          "time=%.2f sec. total (%.2f reading, %.2f sorting, %.2f merging), lines=%d, temp files=%d, merges=%d, soft ram limit=%.2f MB",
-          totalTime / 1000.0d, readTime / 1000.0d, sortTime / 1000.0d, mergeTime / 1000.0d,
-          lines, tempMergeFiles, mergeRounds,
-          (double) bufferSize / MB);
-    }
-  }
-
-  private final BufferSize ramBufferSize;
-  private final File tempDirectory;
-  
-  private final BytesRefList buffer = new BytesRefList();
-  private SortInfo sortInfo;
-  private int maxTempFiles;
-  private final Comparator<BytesRef> comparator;
-  
-  /** Default comparator: sorts in binary (codepoint) order */
-  public static final Comparator<BytesRef> DEFAULT_COMPARATOR = BytesRef.getUTF8SortedAsUnicodeComparator();
-
-  /**
-   * Defaults constructor.
-   * 
-   * @see #defaultTempDir()
-   * @see BufferSize#automatic()
-   */
-  public Sort() throws IOException {
-    this(DEFAULT_COMPARATOR, BufferSize.automatic(), defaultTempDir(), MAX_TEMPFILES);
-  }
-  
-  /**
-   * Defaults constructor with a custom comparator.
-   * 
-   * @see #defaultTempDir()
-   * @see BufferSize#automatic()
-   */
-  public Sort(Comparator<BytesRef> comparator) throws IOException {
-    this(comparator, BufferSize.automatic(), defaultTempDir(), MAX_TEMPFILES);
-  }
-
-  /**
-   * All-details constructor.
-   */
-  public Sort(Comparator<BytesRef> comparator, BufferSize ramBufferSize, File tempDirectory, int maxTempfiles) {
-    if (ramBufferSize.bytes < ABSOLUTE_MIN_SORT_BUFFER_SIZE) {
-      throw new IllegalArgumentException(MIN_BUFFER_SIZE_MSG + ": " + ramBufferSize.bytes);
-    }
-    
-    if (maxTempfiles < 2) {
-      throw new IllegalArgumentException("maxTempFiles must be >= 2");
-    }
-
-    this.ramBufferSize = ramBufferSize;
-    this.tempDirectory = tempDirectory;
-    this.maxTempFiles = maxTempfiles;
-    this.comparator = comparator;
-  }
-
-  /** 
-   * Sort input to output, explicit hint for the buffer size. The amount of allocated
-   * memory may deviate from the hint (may be smaller or larger).  
-   */
-  public SortInfo sort(File input, File output) throws IOException {
-    sortInfo = new SortInfo();
-    sortInfo.totalTime = System.currentTimeMillis();
-
-    output.delete();
-
-    ArrayList<File> merges = new ArrayList<File>();
-    boolean success2 = false;
-    try {
-      ByteSequencesReader is = new ByteSequencesReader(input);
-      boolean success = false;
-      try {
-        int lines = 0;
-        while ((lines = readPartition(is)) > 0) {
-          merges.add(sortPartition(lines));
-          sortInfo.tempMergeFiles++;
-          sortInfo.lines += lines;
-
-          // Handle intermediate merges.
-          if (merges.size() == maxTempFiles) {
-            File intermediate = File.createTempFile("sort", "intermediate", tempDirectory);
-            try {
-              mergePartitions(merges, intermediate);
-            } finally {
-              for (File file : merges) {
-                file.delete();
-              }
-              merges.clear();
-              merges.add(intermediate);
-            }
-            sortInfo.tempMergeFiles++;
-          }
-        }
-        success = true;
-      } finally {
-        if (success)
-          IOUtils.close(is);
-        else
-          IOUtils.closeWhileHandlingException(is);
-      }
-
-      // One partition, try to rename or copy if unsuccessful.
-      if (merges.size() == 1) {     
-        File single = merges.get(0);
-        // If simple rename doesn't work this means the output is
-        // on a different volume or something. Copy the input then.
-        if (!single.renameTo(output)) {
-          copy(single, output);
-        }
-      } else { 
-        // otherwise merge the partitions with a priority queue.
-        mergePartitions(merges, output);
-      }
-      success2 = true;
-    } finally {
-      for (File file : merges) {
-        file.delete();
-      }
-      if (!success2) {
-        output.delete();
-      }
-    }
-
-    sortInfo.totalTime = (System.currentTimeMillis() - sortInfo.totalTime); 
-    return sortInfo;
-  }
-
-  /**
-   * Returns the default temporary directory. By default, java.io.tmpdir. If not accessible
-   * or not available, an IOException is thrown
-   */
-  public static File defaultTempDir() throws IOException {
-    String tempDirPath = System.getProperty("java.io.tmpdir");
-    if (tempDirPath == null) 
-      throw new IOException("Java has no temporary folder property (java.io.tmpdir)?");
-
-    File tempDirectory = new File(tempDirPath);
-    if (!tempDirectory.exists() || !tempDirectory.canWrite()) {
-      throw new IOException("Java's temporary folder not present or writeable?: " 
-          + tempDirectory.getAbsolutePath());
-    }
-    return tempDirectory;
-  }
-
-  /**
-   * Copies one file to another.
-   */
-  private static void copy(File file, File output) throws IOException {
-    // 64kb copy buffer (empirical pick).
-    byte [] buffer = new byte [16 * 1024];
-    InputStream is = null;
-    OutputStream os = null;
-    try {
-      is = new FileInputStream(file);
-      os = new FileOutputStream(output);
-      int length;
-      while ((length = is.read(buffer)) > 0) {
-        os.write(buffer, 0, length);
-      }
-    } finally {
-      IOUtils.close(is, os);
-    }
-  }
-
-  /** Sort a single partition in-memory. */
-  protected File sortPartition(int len) throws IOException {
-    BytesRefList data = this.buffer;
-    File tempFile = File.createTempFile("sort", "partition", tempDirectory);
-
-    long start = System.currentTimeMillis();
-    sortInfo.sortTime += (System.currentTimeMillis() - start);
-    
-    final ByteSequencesWriter out = new ByteSequencesWriter(tempFile);
-    BytesRef spare;
-    try {
-      BytesRefIterator iter = buffer.iterator(comparator);
-      while((spare = iter.next()) != null) {
-        assert spare.length <= Short.MAX_VALUE;
-        out.write(spare);
-      }
-      
-      out.close();
-
-      // Clean up the buffer for the next partition.
-      data.clear();
-      return tempFile;
-    } finally {
-      IOUtils.close(out);
-    }
-  }
-
-  /** Merge a list of sorted temporary files (partitions) into an output file */
-  void mergePartitions(List<File> merges, File outputFile) throws IOException {
-    long start = System.currentTimeMillis();
-
-    ByteSequencesWriter out = new ByteSequencesWriter(outputFile);
-
-    PriorityQueue<FileAndTop> queue = new PriorityQueue<FileAndTop>(merges.size()) {
-      @Override
-      protected boolean lessThan(FileAndTop a, FileAndTop b) {
-        return comparator.compare(a.current, b.current) < 0;
-      }
-    };
-
-    ByteSequencesReader [] streams = new ByteSequencesReader [merges.size()];
-    try {
-      // Open streams and read the top for each file
-      for (int i = 0; i < merges.size(); i++) {
-        streams[i] = new ByteSequencesReader(merges.get(i));
-        byte line[] = streams[i].read();
-        if (line != null) {
-          queue.insertWithOverflow(new FileAndTop(i, line));
-        }
-      }
-  
-      // Unix utility sort() uses ordered array of files to pick the next line from, updating
-      // it as it reads new lines. The PQ used here is a more elegant solution and has 
-      // a nicer theoretical complexity bound :) The entire sorting process is I/O bound anyway
-      // so it shouldn't make much of a difference (didn't check).
-      FileAndTop top;
-      while ((top = queue.top()) != null) {
-        out.write(top.current);
-        if (!streams[top.fd].read(top.current)) {
-          queue.pop();
-        } else {
-          queue.updateTop();
-        }
-      }
-  
-      sortInfo.mergeTime += System.currentTimeMillis() - start;
-      sortInfo.mergeRounds++;
-    } finally {
-      // The logic below is: if an exception occurs in closing out, it has a priority over exceptions
-      // happening in closing streams.
-      try {
-        IOUtils.close(streams);
-      } finally {
-        IOUtils.close(out);
-      }
-    }
-  }
-
-  /** Read in a single partition of data */
-  int readPartition(ByteSequencesReader reader) throws IOException {
-    long start = System.currentTimeMillis();
-    final BytesRef scratch = new BytesRef();
-    while ((scratch.bytes = reader.read()) != null) {
-      scratch.length = scratch.bytes.length; 
-      buffer.append(scratch);
-      // Account for the created objects.
-      // (buffer slots do not account to buffer size.) 
-      if (ramBufferSize.bytes < buffer.bytesUsed()) {
-        break;
-      }
-    }
-    sortInfo.readTime += (System.currentTimeMillis() - start);
-    return buffer.size();
-  }
-
-  static class FileAndTop {
-    final int fd;
-    final BytesRef current;
-
-    FileAndTop(int fd, byte [] firstLine) {
-      this.fd = fd;
-      this.current = new BytesRef(firstLine);
-    }
-  }
-
-  /**
-   * Utility class to emit length-prefixed byte[] entries to an output stream for sorting.
-   * Complementary to {@link ByteSequencesReader}.
-   */
-  public static class ByteSequencesWriter implements Closeable {
-    private final DataOutput os;
-
-    /** Constructs a ByteSequencesWriter to the provided File */
-    public ByteSequencesWriter(File file) throws IOException {
-      this(new DataOutputStream(
-          new BufferedOutputStream(
-              new FileOutputStream(file))));
-    }
-
-    /** Constructs a ByteSequencesWriter to the provided DataOutput */
-    public ByteSequencesWriter(DataOutput os) {
-      this.os = os;
-    }
-
-    /**
-     * Writes a BytesRef.
-     * @see #write(byte[], int, int)
-     */
-    public void write(BytesRef ref) throws IOException {
-      assert ref != null;
-      write(ref.bytes, ref.offset, ref.length);
-    }
-
-    /**
-     * Writes a byte array.
-     * @see #write(byte[], int, int)
-     */
-    public void write(byte [] bytes) throws IOException {
-      write(bytes, 0, bytes.length);
-    }
-
-    /**
-     * Writes a byte array.
-     * <p>
-     * The length is written as a <code>short</code>, followed
-     * by the bytes.
-     */
-    public void write(byte [] bytes, int off, int len) throws IOException {
-      assert bytes != null;
-      assert off >= 0 && off + len <= bytes.length;
-      assert len >= 0;
-      os.writeShort(len);
-      os.write(bytes, off, len);
-    }        
-    
-    /**
-     * Closes the provided {@link DataOutput} if it is {@link Closeable}.
-     */
-    @Override
-    public void close() throws IOException {
-      if (os instanceof Closeable) {
-        ((Closeable) os).close();
-      }
-    }    
-  }
-
-  /**
-   * Utility class to read length-prefixed byte[] entries from an input.
-   * Complementary to {@link ByteSequencesWriter}.
-   */
-  public static class ByteSequencesReader implements Closeable {
-    private final DataInput is;
-
-    /** Constructs a ByteSequencesReader from the provided File */
-    public ByteSequencesReader(File file) throws IOException {
-      this(new DataInputStream(
-          new BufferedInputStream(
-              new FileInputStream(file))));
-    }
-
-    /** Constructs a ByteSequencesReader from the provided DataInput */
-    public ByteSequencesReader(DataInput is) {
-      this.is = is;
-    }
-
-    /**
-     * Reads the next entry into the provided {@link BytesRef}. The internal
-     * storage is resized if needed.
-     * 
-     * @return Returns <code>false</code> if EOF occurred when trying to read
-     * the header of the next sequence. Returns <code>true</code> otherwise.
-     * @throws EOFException if the file ends before the full sequence is read.
-     */
-    public boolean read(BytesRef ref) throws IOException {
-      short length;
-      try {
-        length = is.readShort();
-      } catch (EOFException e) {
-        return false;
-      }
-
-      ref.grow(length);
-      ref.offset = 0;
-      ref.length = length;
-      is.readFully(ref.bytes, 0, length);
-      return true;
-    }
-
-    /**
-     * Reads the next entry and returns it if successful.
-     * 
-     * @see #read(BytesRef)
-     * 
-     * @return Returns <code>null</code> if EOF occurred before the next entry
-     * could be read.
-     * @throws EOFException if the file ends before the full sequence is read.
-     */
-    public byte[] read() throws IOException {
-      short length;
-      try {
-        length = is.readShort();
-      } catch (EOFException e) {
-        return null;
-      }
-
-      assert length >= 0 : "Sanity: sequence length < 0: " + length;
-      byte [] result = new byte [length];
-      is.readFully(result);
-      return result;
-    }
-
-    /**
-     * Closes the provided {@link DataInput} if it is {@link Closeable}.
-     */
-    @Override
-    public void close() throws IOException {
-      if (is instanceof Closeable) {
-        ((Closeable) is).close();
-      }
-    }
-  }
-
-  /** Returns the comparator in use to sort entries */
-  public Comparator<BytesRef> getComparator() {
-    return comparator;
-  }  
-}
Index: lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingSuggester.java
===================================================================
--- lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingSuggester.java	(revision 1442822)
+++ lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingSuggester.java	(working copy)
@@ -34,7 +34,7 @@
 import org.apache.lucene.analysis.TokenStreamToAutomaton;
 import org.apache.lucene.search.spell.TermFreqIterator;
 import org.apache.lucene.search.suggest.Lookup;
-import org.apache.lucene.search.suggest.fst.Sort;
+import org.apache.lucene.search.suggest.Sort;
 import org.apache.lucene.store.ByteArrayDataInput;
 import org.apache.lucene.store.ByteArrayDataOutput;
 import org.apache.lucene.store.DataInput;
Index: lucene/suggest/src/java/org/apache/lucene/search/suggest/BytesRefList.java
===================================================================
--- lucene/suggest/src/java/org/apache/lucene/search/suggest/BytesRefList.java	(revision 1442822)
+++ lucene/suggest/src/java/org/apache/lucene/search/suggest/BytesRefList.java	(working copy)
@@ -1,205 +0,0 @@
-package org.apache.lucene.search.suggest;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to You under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-
-import java.util.Arrays;
-import java.util.Comparator;
-
-import org.apache.lucene.util.ArrayUtil;
-import org.apache.lucene.util.ByteBlockPool;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.BytesRefIterator;
-import org.apache.lucene.util.Counter;
-import org.apache.lucene.util.RamUsageEstimator;
-import org.apache.lucene.util.SorterTemplate;
-
-/**
- * A simple append only random-access {@link BytesRef} array that stores full
- * copies of the appended bytes in a {@link ByteBlockPool}.
- * 
- * 
- * <b>Note: This class is not Thread-Safe!</b>
- * 
- * @lucene.internal
- * @lucene.experimental
- */
-public final class BytesRefList {
-  // TODO rename to BytesRefArray
-  private final ByteBlockPool pool;
-  private int[] offsets = new int[1];
-  private int lastElement = 0;
-  private int currentOffset = 0;
-  private final Counter bytesUsed = Counter.newCounter(false);
-  
-  /**
-   * Creates a new {@link BytesRefList}
-   */
-  public BytesRefList() {
-    this.pool = new ByteBlockPool(new ByteBlockPool.DirectTrackingAllocator(
-        bytesUsed));
-    pool.nextBuffer();
-    bytesUsed.addAndGet(RamUsageEstimator.NUM_BYTES_ARRAY_HEADER
-        + RamUsageEstimator.NUM_BYTES_INT);
-  }
- 
-  /**
-   * Clears this {@link BytesRefList}
-   */
-  public void clear() {
-    lastElement = 0;
-    currentOffset = 0;
-    Arrays.fill(offsets, 0);
-    pool.reset(false, true); // no need to 0 fill the buffers we control the allocator
-  }
-  
-  /**
-   * Appends a copy of the given {@link BytesRef} to this {@link BytesRefList}.
-   * @param bytes the bytes to append
-   * @return the ordinal of the appended bytes
-   */
-  public int append(BytesRef bytes) {
-    if (lastElement >= offsets.length) {
-      int oldLen = offsets.length;
-      offsets = ArrayUtil.grow(offsets, offsets.length + 1);
-      bytesUsed.addAndGet((offsets.length - oldLen)
-          * RamUsageEstimator.NUM_BYTES_INT);
-    }
-    pool.copy(bytes);
-    offsets[lastElement++] = currentOffset;
-    currentOffset += bytes.length;
-    return lastElement;
-  }
-  
-  /**
-   * Returns the current size of this {@link BytesRefList}
-   * @return the current size of this {@link BytesRefList}
-   */
-  public int size() {
-    return lastElement;
-  }
-  
-  /**
-   * Returns the <i>n'th</i> element of this {@link BytesRefList}
-   * @param spare a spare {@link BytesRef} instance
-   * @param ord the elements ordinal to retrieve 
-   * @return the <i>n'th</i> element of this {@link BytesRefList}
-   */
-  public BytesRef get(BytesRef spare, int ord) {
-    if (lastElement > ord) {
-      int offset = offsets[ord];
-      int length = ord == lastElement - 1 ? currentOffset - offset
-          : offsets[ord + 1] - offset;
-      pool.copyFrom(spare, offset, length);
-      return spare;
-    }
-    throw new IndexOutOfBoundsException("index " + ord
-        + " must be less than the size: " + lastElement);
-    
-  }
-  
-  /**
-   * Returns the number internally used bytes to hold the appended bytes in
-   * memory
-   * 
-   * @return the number internally used bytes to hold the appended bytes in
-   *         memory
-   */
-  public long bytesUsed() {
-    return bytesUsed.get();
-  }
-  
-  private int[] sort(final Comparator<BytesRef> comp) {
-    final int[] orderedEntries = new int[size()];
-    for (int i = 0; i < orderedEntries.length; i++) {
-      orderedEntries[i] = i;
-    }
-    new SorterTemplate() {
-      @Override
-      protected void swap(int i, int j) {
-        final int o = orderedEntries[i];
-        orderedEntries[i] = orderedEntries[j];
-        orderedEntries[j] = o;
-      }
-      
-      @Override
-      protected int compare(int i, int j) {
-        final int ord1 = orderedEntries[i], ord2 = orderedEntries[j];
-        return comp.compare(get(scratch1, ord1), get(scratch2, ord2));
-      }
-      
-      @Override
-      protected void setPivot(int i) {
-        final int ord = orderedEntries[i];
-        get(pivot, ord);
-      }
-      
-      @Override
-      protected int comparePivot(int j) {
-        final int ord = orderedEntries[j];
-        return comp.compare(pivot, get(scratch2, ord));
-      }
-      
-      private final BytesRef pivot = new BytesRef(), scratch1 = new BytesRef(),
-          scratch2 = new BytesRef();
-    }.quickSort(0, size() - 1);
-    return orderedEntries;
-  }
-  
-  /**
-   * sugar for {@link #iterator(Comparator)} with a <code>null</code> comparator
-   */
-  public BytesRefIterator iterator() {
-    return iterator(null);
-  }
-  
-  /**
-   * <p>
-   * Returns a {@link BytesRefIterator} with point in time semantics. The
-   * iterator provides access to all so far appended {@link BytesRef} instances.
-   * </p>
-   * <p>
-   * If a non <code>null</code> {@link Comparator} is provided the iterator will
-   * iterate the byte values in the order specified by the comparator. Otherwise
-   * the order is the same as the values were appended.
-   * </p>
-   * <p>
-   * This is a non-destructive operation.
-   * </p>
-   */
-  public BytesRefIterator iterator(final Comparator<BytesRef> comp) {
-    final BytesRef spare = new BytesRef();
-    final int size = size();
-    final int[] ords = comp == null ? null : sort(comp);
-    return new BytesRefIterator() {
-      int pos = 0;
-      
-      @Override
-      public BytesRef next() {
-        if (pos < size) {
-          return get(spare, ords == null ? pos++ : ords[pos++]);
-        }
-        return null;
-      }
-      
-      @Override
-      public Comparator<BytesRef> getComparator() {
-        return comp;
-      }
-    };
-  }
-}
Index: lucene/analysis/icu/src/test/org/apache/lucene/collation/TestICUCollationDocValuesField.java
===================================================================
--- lucene/analysis/icu/src/test/org/apache/lucene/collation/TestICUCollationDocValuesField.java	(revision 0)
+++ lucene/analysis/icu/src/test/org/apache/lucene/collation/TestICUCollationDocValuesField.java	(working copy)
@@ -0,0 +1,143 @@
+package org.apache.lucene.collation;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.document.StringField;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.RandomIndexWriter;
+import org.apache.lucene.search.BooleanQuery;
+import org.apache.lucene.search.ConstantScoreQuery;
+import org.apache.lucene.search.FieldCacheRangeFilter;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.MatchAllDocsQuery;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.QueryUtils;
+import org.apache.lucene.search.ScoreDoc;
+import org.apache.lucene.search.Sort;
+import org.apache.lucene.search.SortField;
+import org.apache.lucene.search.TopDocs;
+import org.apache.lucene.search.BooleanClause.Occur;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.LuceneTestCase;
+import org.apache.lucene.util._TestUtil;
+import org.apache.lucene.util.LuceneTestCase.SuppressCodecs;
+
+import com.ibm.icu.text.Collator;
+import com.ibm.icu.util.ULocale;
+
+/**
+ * trivial test of ICUCollationDocValuesField
+ */
+@SuppressCodecs("Lucene3x")
+public class TestICUCollationDocValuesField extends LuceneTestCase {
+  
+  public void testBasic() throws Exception {
+    Directory dir = newDirectory();
+    RandomIndexWriter iw = new RandomIndexWriter(random(), dir);
+    Document doc = new Document();
+    Field field = newField("field", "", StringField.TYPE_STORED);
+    ICUCollationDocValuesField collationField = new ICUCollationDocValuesField("collated", Collator.getInstance(ULocale.ENGLISH));
+    doc.add(field);
+    doc.add(collationField);
+
+    field.setStringValue("ABC");
+    collationField.setStringValue("ABC");
+    iw.addDocument(doc);
+    
+    field.setStringValue("abc");
+    collationField.setStringValue("abc");
+    iw.addDocument(doc);
+    
+    IndexReader ir = iw.getReader();
+    iw.close();
+    
+    IndexSearcher is = newSearcher(ir);
+    
+    SortField sortField = new SortField("collated", SortField.Type.STRING);
+    
+    TopDocs td = is.search(new MatchAllDocsQuery(), 5, new Sort(sortField));
+    assertEquals("abc", ir.document(td.scoreDocs[0].doc).get("field"));
+    assertEquals("ABC", ir.document(td.scoreDocs[1].doc).get("field"));
+    ir.close();
+    dir.close();
+  }
+  
+  public void testRanges() throws Exception {
+    Directory dir = newDirectory();
+    RandomIndexWriter iw = new RandomIndexWriter(random(), dir);
+    Document doc = new Document();
+    Field field = newField("field", "", StringField.TYPE_STORED);
+    Collator collator = Collator.getInstance(); // uses -Dtests.locale
+    if (random().nextBoolean()) {
+      collator.setStrength(Collator.PRIMARY);
+    }
+    ICUCollationDocValuesField collationField = new ICUCollationDocValuesField("collated", collator);
+    doc.add(field);
+    doc.add(collationField);
+    
+    int numDocs = atLeast(500);
+    for (int i = 0; i < numDocs; i++) {
+      String value = _TestUtil.randomSimpleString(random());
+      field.setStringValue(value);
+      collationField.setStringValue(value);
+      iw.addDocument(doc);
+    }
+    
+    IndexReader ir = iw.getReader();
+    iw.close();
+    IndexSearcher is = newSearcher(ir);
+    
+    int numChecks = atLeast(100);
+    for (int i = 0; i < numChecks; i++) {
+      String start = _TestUtil.randomSimpleString(random());
+      String end = _TestUtil.randomSimpleString(random());
+      BytesRef lowerVal = new BytesRef(collator.getCollationKey(start).toByteArray());
+      BytesRef upperVal = new BytesRef(collator.getCollationKey(end).toByteArray());
+      Query query = new ConstantScoreQuery(FieldCacheRangeFilter.newBytesRefRange("collated", lowerVal, upperVal, true, true));
+      doTestRanges(is, start, end, query, collator);
+    }
+    
+    ir.close();
+    dir.close();
+  }
+  
+  private void doTestRanges(IndexSearcher is, String startPoint, String endPoint, Query query, Collator collator) throws Exception { 
+    QueryUtils.check(query);
+    
+    // positive test
+    TopDocs docs = is.search(query, is.getIndexReader().maxDoc());
+    for (ScoreDoc doc : docs.scoreDocs) {
+      String value = is.doc(doc.doc).get("field");
+      assertTrue(collator.compare(value, startPoint) >= 0);
+      assertTrue(collator.compare(value, endPoint) <= 0);
+    }
+    
+    // negative test
+    BooleanQuery bq = new BooleanQuery();
+    bq.add(new MatchAllDocsQuery(), Occur.SHOULD);
+    bq.add(query, Occur.MUST_NOT);
+    docs = is.search(bq, is.getIndexReader().maxDoc());
+    for (ScoreDoc doc : docs.scoreDocs) {
+      String value = is.doc(doc.doc).get("field");
+      assertTrue(collator.compare(value, startPoint) < 0 || collator.compare(value, endPoint) > 0);
+    }
+  }
+}

Property changes on: lucene/analysis/icu/src/test/org/apache/lucene/collation/TestICUCollationDocValuesField.java
___________________________________________________________________
Added: svn:eol-style
## -0,0 +1 ##
+native
Index: lucene/analysis/icu/src/java/org/apache/lucene/collation/ICUCollationDocValuesField.java
===================================================================
--- lucene/analysis/icu/src/java/org/apache/lucene/collation/ICUCollationDocValuesField.java	(revision 0)
+++ lucene/analysis/icu/src/java/org/apache/lucene/collation/ICUCollationDocValuesField.java	(working copy)
@@ -0,0 +1,77 @@
+package org.apache.lucene.collation;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.document.Field;
+import org.apache.lucene.document.SortedDocValuesField;
+import org.apache.lucene.search.FieldCacheRangeFilter;
+import org.apache.lucene.util.BytesRef;
+
+import com.ibm.icu.text.Collator;
+import com.ibm.icu.text.RawCollationKey;
+
+/**
+ * Indexes collation keys as a single-valued {@link SortedDocValuesField}.
+ * <p>
+ * This is more efficient that {@link ICUCollationKeyAnalyzer} if the field 
+ * only has one value: no uninversion is necessary to sort on the field, 
+ * locale-sensitive range queries can still work via {@link FieldCacheRangeFilter}, 
+ * and the underlying data structures built at index-time are likely more efficient 
+ * and use less memory than FieldCache.
+ */
+public final class ICUCollationDocValuesField extends Field {
+  private final String name;
+  private final Collator collator;
+  private final BytesRef bytes = new BytesRef();
+  private final RawCollationKey key = new RawCollationKey();
+  
+  /**
+   * Create a new ICUCollationDocValuesField.
+   * <p>
+   * NOTE: you should not create a new one for each document, instead
+   * just make one and reuse it during your indexing process, setting
+   * the value via {@link #setStringValue(String)}.
+   * @param name field name
+   * @param collator Collator for generating collation keys.
+   */
+  // TODO: can we make this trap-free? maybe just synchronize on the collator
+  // instead? 
+  public ICUCollationDocValuesField(String name, Collator collator) {
+    super(name, SortedDocValuesField.TYPE);
+    this.name = name;
+    try {
+      this.collator = (Collator) collator.clone();
+    } catch (CloneNotSupportedException e) {
+      throw new RuntimeException(e);
+    }
+    fieldsData = bytes; // so wrong setters cannot be called
+  }
+
+  @Override
+  public String name() {
+    return name;
+  }
+  
+  @Override
+  public void setStringValue(String value) {
+    collator.getRawCollationKey(value, key);
+    bytes.bytes = key.bytes;
+    bytes.offset = 0;
+    bytes.length = key.size;
+  }
+}

Property changes on: lucene/analysis/icu/src/java/org/apache/lucene/collation/ICUCollationDocValuesField.java
___________________________________________________________________
Added: svn:eol-style
## -0,0 +1 ##
+native
Index: lucene/misc/src/test/org/apache/lucene/misc/SweetSpotSimilarityTest.java
===================================================================
--- lucene/misc/src/test/org/apache/lucene/misc/SweetSpotSimilarityTest.java	(revision 1442822)
+++ lucene/misc/src/test/org/apache/lucene/misc/SweetSpotSimilarityTest.java	(working copy)
@@ -23,7 +23,6 @@
 import org.apache.lucene.search.similarities.Similarity;
 import org.apache.lucene.search.similarities.TFIDFSimilarity;
 import org.apache.lucene.util.LuceneTestCase;
-import org.apache.lucene.index.Norm;
 import org.apache.lucene.index.FieldInvertState;
 
 
@@ -37,9 +36,7 @@
   }
   
   public static byte computeAndGetNorm(Similarity s, FieldInvertState state) {
-    Norm norm = new Norm();
-    s.computeNorm(state, norm);
-    return norm.field().numericValue().byteValue();
+    return (byte) s.computeNorm(state);
   }
 
   public void testSweetSpotComputeNorm() {
@@ -56,10 +53,6 @@
     invertState.setBoost(1.0f);
     for (int i = 1; i < 1000; i++) {
       invertState.setLength(i);
-      Norm lNorm = new Norm();
-      Norm rNorm = new Norm();
-      d.computeNorm(invertState, lNorm);
-      s.computeNorm(invertState, rNorm);
       assertEquals("base case: i="+i,
                    computeAndGetNorm(d, invertState),
                    computeAndGetNorm(s, invertState),
Index: lucene/misc/src/java/org/apache/lucene/misc/SweetSpotSimilarity.java
===================================================================
--- lucene/misc/src/java/org/apache/lucene/misc/SweetSpotSimilarity.java	(revision 1442822)
+++ lucene/misc/src/java/org/apache/lucene/misc/SweetSpotSimilarity.java	(working copy)
@@ -19,7 +19,6 @@
 
 import org.apache.lucene.search.similarities.DefaultSimilarity;
 import org.apache.lucene.index.FieldInvertState;
-import org.apache.lucene.index.Norm;
 
 /**
  * A similarity with a lengthNorm that provides for a "plateau" of
Index: lucene/sandbox/src/java/org/apache/lucene/sandbox/queries/SlowCollatedStringComparator.java
===================================================================
--- lucene/sandbox/src/java/org/apache/lucene/sandbox/queries/SlowCollatedStringComparator.java	(revision 1442822)
+++ lucene/sandbox/src/java/org/apache/lucene/sandbox/queries/SlowCollatedStringComparator.java	(working copy)
@@ -21,8 +21,8 @@
 import java.text.Collator;
 
 import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.BinaryDocValues;
 import org.apache.lucene.search.FieldCache;
-import org.apache.lucene.search.FieldCache.DocTerms;
 import org.apache.lucene.search.FieldComparator;
 import org.apache.lucene.util.BytesRef;
 
@@ -38,7 +38,7 @@
 public final class SlowCollatedStringComparator extends FieldComparator<String> {
 
   private final String[] values;
-  private DocTerms currentDocTerms;
+  private BinaryDocValues currentDocTerms;
   private final String field;
   final Collator collator;
   private String bottom;
@@ -67,7 +67,8 @@
 
   @Override
   public int compareBottom(int doc) {
-    final String val2 = currentDocTerms.getTerm(doc, tempBR).utf8ToString();
+    currentDocTerms.get(doc, tempBR);
+    final String val2 = tempBR.bytes == BinaryDocValues.MISSING ? null : tempBR.utf8ToString();
     if (bottom == null) {
       if (val2 == null) {
         return 0;
@@ -81,11 +82,11 @@
 
   @Override
   public void copy(int slot, int doc) {
-    final BytesRef br = currentDocTerms.getTerm(doc, tempBR);
-    if (br == null) {
+    currentDocTerms.get(doc, tempBR);
+    if (tempBR.bytes == BinaryDocValues.MISSING) {
       values[slot] = null;
     } else {
-      values[slot] = br.utf8ToString();
+      values[slot] = tempBR.utf8ToString();
     }
   }
 
@@ -121,12 +122,12 @@
 
   @Override
   public int compareDocToValue(int doc, String value) {
-    final BytesRef br = currentDocTerms.getTerm(doc, tempBR);
+    currentDocTerms.get(doc, tempBR);
     final String docValue;
-    if (br == null) {
+    if (tempBR.bytes == BinaryDocValues.MISSING) {
       docValue = null;
     } else {
-      docValue = br.utf8ToString();
+      docValue = tempBR.utf8ToString();
     }
     return compareValues(docValue, value);
   }
Index: lucene/test-framework/src/java/org/apache/lucene/index/RandomIndexWriter.java
===================================================================
--- lucene/test-framework/src/java/org/apache/lucene/index/RandomIndexWriter.java	(revision 1442822)
+++ lucene/test-framework/src/java/org/apache/lucene/index/RandomIndexWriter.java	(working copy)
@@ -25,18 +25,12 @@
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.codecs.Codec;
-import org.apache.lucene.document.ByteDocValuesField; 
-import org.apache.lucene.document.DerefBytesDocValuesField; 
+import org.apache.lucene.document.BinaryDocValuesField;
 import org.apache.lucene.document.Document;
-import org.apache.lucene.document.DoubleDocValuesField; 
 import org.apache.lucene.document.Field;
-import org.apache.lucene.document.FloatDocValuesField; 
-import org.apache.lucene.document.IntDocValuesField; 
-import org.apache.lucene.document.LongDocValuesField; 
-import org.apache.lucene.document.PackedLongDocValuesField; 
-import org.apache.lucene.document.ShortDocValuesField; 
-import org.apache.lucene.document.SortedBytesDocValuesField; 
-import org.apache.lucene.document.StraightBytesDocValuesField; 
+import org.apache.lucene.document.NumericDocValuesField; 
+import org.apache.lucene.document.SortedDocValuesField; 
+import org.apache.lucene.index.FieldInfo.DocValuesType;
 import org.apache.lucene.index.IndexWriter; // javadoc
 import org.apache.lucene.search.Query;
 import org.apache.lucene.store.Directory;
@@ -59,9 +53,6 @@
   int flushAt;
   private double flushAtFactor = 1.0;
   private boolean getReaderCalled;
-  private final int fixedBytesLength;
-  private final long docValuesFieldPrefix;
-  private volatile boolean doDocValues;
   private final Codec codec; // sugar
 
   // Randomly calls Thread.yield so we mixup thread scheduling
@@ -109,50 +100,13 @@
       System.out.println("RIW dir=" + dir + " config=" + w.getConfig());
       System.out.println("codec default=" + codec.getName());
     }
-    /* TODO: find some way to make this random...
-     * This must be fixed across all fixed bytes 
-     * fields in one index. so if you open another writer
-     * this might change if I use r.nextInt(x)
-     * maybe we can peek at the existing files here? 
-     */
-    fixedBytesLength = 17; 
 
-    // NOTE: this means up to 13 * 5 unique fields (we have
-    // 13 different DV types):
-    docValuesFieldPrefix = r.nextInt(5);
-    switchDoDocValues();
-
     // Make sure we sometimes test indices that don't get
     // any forced merges:
     doRandomForceMerge = r.nextBoolean();
   } 
   
-  private boolean addDocValuesFields = true;
-  
   /**
-   * set to false if you don't want RandomIndexWriter
-   * adding docvalues fields.
-   */
-  public void setAddDocValuesFields(boolean v) {
-    addDocValuesFields = v;
-    switchDoDocValues();
-  }
-
-  private void switchDoDocValues() {
-    if (addDocValuesFields == false) {
-      doDocValues = false;
-      return;
-    }
-    // randomly enable / disable docValues 
-    doDocValues = LuceneTestCase.rarely(r);
-    if (LuceneTestCase.VERBOSE) {
-      if (doDocValues) {
-        System.out.println("NOTE: RIW: turning on random DocValues fields");
-      }
-    }
-  }
-  
-  /**
    * Adds a Document.
    * @see IndexWriter#addDocument(org.apache.lucene.index.IndexDocument)
    */
@@ -161,9 +115,6 @@
   }
 
   public <T extends IndexableField> void addDocument(final IndexDocument doc, Analyzer a) throws IOException {
-    if (doDocValues && doc instanceof Document) {
-      randomPerDocFieldValues((Document) doc);
-    }
     if (r.nextInt(5) == 3) {
       // TODO: maybe, we should simply buffer up added docs
       // (but we need to clone them), and only when
@@ -204,75 +155,6 @@
     maybeCommit();
   }
 
-  private BytesRef getFixedRandomBytes() {
-    final String randomUnicodeString = _TestUtil.randomFixedByteLengthUnicodeString(r, fixedBytesLength);
-    BytesRef fixedRef = new BytesRef(randomUnicodeString);
-    if (fixedRef.length > fixedBytesLength) {
-      fixedRef = new BytesRef(fixedRef.bytes, 0, fixedBytesLength);
-    } else {
-      fixedRef.grow(fixedBytesLength);
-      fixedRef.length = fixedBytesLength;
-    }
-    return fixedRef;
-  }
-  
-  private void randomPerDocFieldValues(Document doc) {
-    
-    DocValues.Type[] values = DocValues.Type.values();
-    DocValues.Type type = values[r.nextInt(values.length)];
-    String name = "random_" + type.name() + "" + docValuesFieldPrefix;
-    if (doc.getField(name) != null) {
-      return;
-    }
-    final Field f;
-    switch (type) {
-    case BYTES_FIXED_DEREF:
-      f = new DerefBytesDocValuesField(name, getFixedRandomBytes(), true);
-      break;
-    case BYTES_VAR_DEREF:
-      f = new DerefBytesDocValuesField(name, new BytesRef(_TestUtil.randomUnicodeString(r, 20)), false);
-      break;
-    case BYTES_FIXED_STRAIGHT:
-      f = new StraightBytesDocValuesField(name, getFixedRandomBytes(), true);
-      break;
-    case BYTES_VAR_STRAIGHT:
-      f = new StraightBytesDocValuesField(name, new BytesRef(_TestUtil.randomUnicodeString(r, 20)), false);
-      break;
-    case BYTES_FIXED_SORTED:
-      f = new SortedBytesDocValuesField(name, getFixedRandomBytes(), true);
-      break;
-    case BYTES_VAR_SORTED:
-      f = new SortedBytesDocValuesField(name, new BytesRef(_TestUtil.randomUnicodeString(r, 20)), false);
-      break;
-    case FLOAT_32:
-      f = new FloatDocValuesField(name, r.nextFloat());
-      break;
-    case FLOAT_64:
-      f = new DoubleDocValuesField(name, r.nextDouble());
-      break;
-    case VAR_INTS:
-      f = new PackedLongDocValuesField(name, r.nextLong());
-      break;
-    case FIXED_INTS_16:
-      // TODO: we should test negatives too?
-      f = new ShortDocValuesField(name, (short) r.nextInt(Short.MAX_VALUE));
-      break;
-    case FIXED_INTS_32:
-      f = new IntDocValuesField(name, r.nextInt());
-      break;
-    case FIXED_INTS_64:
-      f = new LongDocValuesField(name, r.nextLong());
-      break;
-    case FIXED_INTS_8:  
-      // TODO: we should test negatives too?
-      f = new ByteDocValuesField(name, (byte) r.nextInt(128));
-      break;
-    default:
-      throw new IllegalArgumentException("no such type: " + type);
-    }
-    doc.add(f);
-  }
-
   private void maybeCommit() throws IOException {
     if (docCount++ == flushAt) {
       if (LuceneTestCase.VERBOSE) {
@@ -284,7 +166,6 @@
         // gradually but exponentially increase time b/w flushes
         flushAtFactor *= 1.05;
       }
-      switchDoDocValues();
     }
   }
   
@@ -303,9 +184,6 @@
    * @see IndexWriter#updateDocument(Term, org.apache.lucene.index.IndexDocument)
    */
   public <T extends IndexableField> void updateDocument(Term t, final IndexDocument doc) throws IOException {
-    if (doDocValues) {
-      randomPerDocFieldValues((Document) doc);
-    }
     if (r.nextInt(5) == 3) {
       w.updateDocuments(t, new Iterable<IndexDocument>() {
 
@@ -359,7 +237,6 @@
   
   public void commit() throws IOException {
     w.commit();
-    switchDoDocValues();
   }
   
   public int numDocs() {
@@ -416,7 +293,6 @@
         assert !doRandomForceMergeAssert || w.getSegmentCount() <= limit: "limit=" + limit + " actual=" + w.getSegmentCount();
       }
     }
-    switchDoDocValues();
   }
 
   public DirectoryReader getReader(boolean applyDeletions) throws IOException {
@@ -437,7 +313,6 @@
         System.out.println("RIW.getReader: open new reader");
       }
       w.commit();
-      switchDoDocValues();
       if (r.nextBoolean()) {
         return DirectoryReader.open(w.getDirectory(), _TestUtil.nextInt(r, 1, 10));
       } else {
Index: lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase.java
===================================================================
--- lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase.java	(revision 1442822)
+++ lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase.java	(working copy)
@@ -38,6 +38,7 @@
 import org.apache.lucene.codecs.PostingsConsumer;
 import org.apache.lucene.codecs.TermStats;
 import org.apache.lucene.codecs.TermsConsumer;
+import org.apache.lucene.index.FieldInfo.DocValuesType;
 import org.apache.lucene.index.FieldInfo.IndexOptions;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.FlushInfo;
@@ -345,7 +346,7 @@
 
       fieldInfoArray[fieldUpto] = new FieldInfo(field, true, fieldUpto, false, false, true,
                                                 IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS,
-                                                null, DocValues.Type.FIXED_INTS_8, null);
+                                                null, DocValuesType.NUMERIC, null);
       fieldUpto++;
 
       Map<BytesRef,Long> postings = new TreeMap<BytesRef,Long>();
@@ -470,7 +471,7 @@
                                                    doPayloads,
                                                    indexOptions,
                                                    null,
-                                                   DocValues.Type.FIXED_INTS_8,
+                                                   DocValuesType.NUMERIC,
                                                    null);
     }
 
Index: lucene/test-framework/src/java/org/apache/lucene/index/ThreadedIndexingAndSearchingTestCase.java
===================================================================
--- lucene/test-framework/src/java/org/apache/lucene/index/ThreadedIndexingAndSearchingTestCase.java	(revision 1442822)
+++ lucene/test-framework/src/java/org/apache/lucene/index/ThreadedIndexingAndSearchingTestCase.java	(working copy)
@@ -637,7 +637,6 @@
     s.search(q, 10);
     int hitCount = s.search(q, null, 10, new Sort(new SortField("title", SortField.Type.STRING))).totalHits;
     final Sort dvSort = new Sort(new SortField("title", SortField.Type.STRING));
-    dvSort.getSort()[0].setUseIndexValues(true);
     int hitCount2 = s.search(q, null, 10, dvSort).totalHits;
     assertEquals(hitCount, hitCount2);
     return hitCount;
Index: lucene/test-framework/src/java/org/apache/lucene/index/FieldFilterAtomicReader.java
===================================================================
--- lucene/test-framework/src/java/org/apache/lucene/index/FieldFilterAtomicReader.java	(revision 1442822)
+++ lucene/test-framework/src/java/org/apache/lucene/index/FieldFilterAtomicReader.java	(working copy)
@@ -113,18 +113,30 @@
     final Fields f = super.fields();
     return (f == null) ? null : new FieldFilterFields(f);
   }
+  
+  
 
   @Override
-  public DocValues docValues(String field) throws IOException {
-    return hasField(field) ? super.docValues(field) : null;
+  public NumericDocValues getNumericDocValues(String field) throws IOException {
+    return hasField(field) ? super.getNumericDocValues(field) : null;
   }
 
   @Override
-  public DocValues normValues(String field) throws IOException {
-    return hasField(field) ? super.normValues(field) : null;
+  public BinaryDocValues getBinaryDocValues(String field) throws IOException {
+    return hasField(field) ? super.getBinaryDocValues(field) : null;
   }
 
   @Override
+  public SortedDocValues getSortedDocValues(String field) throws IOException {
+    return hasField(field) ? super.getSortedDocValues(field) : null;
+  }
+
+  @Override
+  public NumericDocValues getNormValues(String field) throws IOException {
+    return hasField(field) ? super.getNormValues(field) : null;
+  }
+
+  @Override
   public String toString() {
     final StringBuilder sb = new StringBuilder("FieldFilterAtomicReader(reader=");
     sb.append(in).append(", fields=");
Index: lucene/test-framework/src/java/org/apache/lucene/index/RandomCodec.java
===================================================================
--- lucene/test-framework/src/java/org/apache/lucene/index/RandomCodec.java	(revision 1442822)
+++ lucene/test-framework/src/java/org/apache/lucene/index/RandomCodec.java	(working copy)
@@ -28,11 +28,16 @@
 import java.util.Set;
 
 import org.apache.lucene.codecs.PostingsFormat;
+import org.apache.lucene.codecs.DocValuesFormat;
+import org.apache.lucene.codecs.asserting.AssertingDocValuesFormat;
 import org.apache.lucene.codecs.asserting.AssertingPostingsFormat;
 import org.apache.lucene.codecs.lucene41.Lucene41PostingsFormat;
 import org.apache.lucene.codecs.lucene41ords.Lucene41WithOrds;
 import org.apache.lucene.codecs.lucene42.Lucene42Codec;
+import org.apache.lucene.codecs.lucene42.Lucene42DocValuesFormat;
 import org.apache.lucene.codecs.bloom.TestBloomFilteredLucene41Postings;
+import org.apache.lucene.codecs.cheapbastard.CheapBastardDocValuesFormat;
+import org.apache.lucene.codecs.diskdv.DiskDocValuesFormat;
 import org.apache.lucene.codecs.memory.DirectPostingsFormat;
 import org.apache.lucene.codecs.memory.MemoryPostingsFormat;
 import org.apache.lucene.codecs.mockintblock.MockFixedIntBlockPostingsFormat;
@@ -42,6 +47,7 @@
 import org.apache.lucene.codecs.nestedpulsing.NestedPulsingPostingsFormat;
 import org.apache.lucene.codecs.pulsing.Pulsing41PostingsFormat;
 import org.apache.lucene.codecs.simpletext.SimpleTextPostingsFormat;
+import org.apache.lucene.codecs.simpletext.SimpleTextDocValuesFormat;
 import org.apache.lucene.util.LuceneTestCase;
 import org.apache.lucene.util._TestUtil;
 
@@ -58,14 +64,21 @@
   /** Shuffled list of postings formats to use for new mappings */
   private List<PostingsFormat> formats = new ArrayList<PostingsFormat>();
   
+  /** Shuffled list of docvalues formats to use for new mappings */
+  private List<DocValuesFormat> dvFormats = new ArrayList<DocValuesFormat>();
+  
   /** unique set of format names this codec knows about */
   public Set<String> formatNames = new HashSet<String>();
+  
+  /** unique set of docvalues format names this codec knows about */
+  public Set<String> dvFormatNames = new HashSet<String>();
 
   /** memorized field->postingsformat mappings */
   // note: we have to sync this map even though its just for debugging/toString, 
   // otherwise DWPT's .toString() calls that iterate over the map can 
   // cause concurrentmodificationexception if indexwriter's infostream is on
   private Map<String,PostingsFormat> previousMappings = Collections.synchronizedMap(new HashMap<String,PostingsFormat>());
+  private Map<String,DocValuesFormat> previousDVMappings = Collections.synchronizedMap(new HashMap<String,DocValuesFormat>());
   private final int perFieldSeed;
 
   @Override
@@ -84,6 +97,22 @@
     return codec;
   }
 
+  @Override
+  public DocValuesFormat getDocValuesFormatForField(String name) {
+    DocValuesFormat codec = previousDVMappings.get(name);
+    if (codec == null) {
+      codec = dvFormats.get(Math.abs(perFieldSeed ^ name.hashCode()) % dvFormats.size());
+      if (codec instanceof SimpleTextDocValuesFormat && perFieldSeed % 5 != 0) {
+        // make simpletext rarer, choose again
+        codec = dvFormats.get(Math.abs(perFieldSeed ^ name.toUpperCase(Locale.ROOT).hashCode()) % dvFormats.size());
+      }
+      previousDVMappings.put(name, codec);
+      // Safety:
+      assert previousDVMappings.size() < 10000: "test went insane";
+    }
+    return codec;
+  }
+
   public RandomCodec(Random random, Set<String> avoidCodecs) {
     this.perFieldSeed = random.nextInt();
     // TODO: make it possible to specify min/max iterms per
@@ -113,11 +142,20 @@
         new AssertingPostingsFormat(),
         new MemoryPostingsFormat(true, random.nextFloat()),
         new MemoryPostingsFormat(false, random.nextFloat()));
+    
+    addDocValues(avoidCodecs,
+        new Lucene42DocValuesFormat(),
+        new DiskDocValuesFormat(),
+        new SimpleTextDocValuesFormat(),
+        new AssertingDocValuesFormat(),
+        new CheapBastardDocValuesFormat());
 
     Collections.shuffle(formats, random);
+    Collections.shuffle(dvFormats, random);
 
     // Avoid too many open files:
     formats = formats.subList(0, 4);
+    dvFormats = dvFormats.subList(0, 4);
   }
 
   public RandomCodec(Random random) {
@@ -132,9 +170,18 @@
       }
     }
   }
+  
+  private final void addDocValues(Set<String> avoidCodecs, DocValuesFormat... docvalues) {
+    for (DocValuesFormat d : docvalues) {
+      if (!avoidCodecs.contains(d.getName())) {
+        dvFormats.add(d);
+        dvFormatNames.add(d.getName());
+      }
+    }
+  }
 
   @Override
   public String toString() {
-    return super.toString() + ": " + previousMappings.toString();
+    return super.toString() + ": " + previousMappings.toString() + ", docValues:" + previousDVMappings.toString();
   }
 }
Index: lucene/test-framework/src/java/org/apache/lucene/index/AssertingAtomicReader.java
===================================================================
--- lucene/test-framework/src/java/org/apache/lucene/index/AssertingAtomicReader.java	(revision 1442822)
+++ lucene/test-framework/src/java/org/apache/lucene/index/AssertingAtomicReader.java	(working copy)
@@ -86,6 +86,7 @@
     public TermsEnum intersect(CompiledAutomaton automaton, BytesRef bytes) throws IOException {
       TermsEnum termsEnum = super.intersect(automaton, bytes);
       assert termsEnum != null;
+      assert bytes == null || bytes.isValid();
       return new AssertingTermsEnum(termsEnum);
     }
 
@@ -145,6 +146,7 @@
       if (result == null) {
         state = State.UNPOSITIONED;
       } else {
+        assert result.isValid();
         state = State.POSITIONED;
       }
       return result;
@@ -171,7 +173,9 @@
     @Override
     public BytesRef term() throws IOException {
       assert state == State.POSITIONED : "term() called on unpositioned TermsEnum";
-      return super.term();
+      BytesRef ret = super.term();
+      assert ret == null || ret.isValid();
+      return ret;
     }
 
     @Override
@@ -182,6 +186,7 @@
 
     @Override
     public SeekStatus seekCeil(BytesRef term, boolean useCache) throws IOException {
+      assert term.isValid();
       SeekStatus result = super.seekCeil(term, useCache);
       if (result == SeekStatus.END) {
         state = State.UNPOSITIONED;
@@ -193,6 +198,7 @@
 
     @Override
     public boolean seekExact(BytesRef text, boolean useCache) throws IOException {
+      assert text.isValid();
       if (super.seekExact(text, useCache)) {
         state = State.POSITIONED;
         return true;
@@ -210,6 +216,7 @@
 
     @Override
     public void seekExact(BytesRef term, TermState state) throws IOException {
+      assert term.isValid();
       super.seekExact(term, state);
       this.state = State.POSITIONED;
     }
@@ -352,11 +359,157 @@
       assert state != DocsEnumState.FINISHED : "getPayload() called after NO_MORE_DOCS";
       assert positionCount > 0 : "getPayload() called before nextPosition()!";
       BytesRef payload = super.getPayload();
-      assert payload == null || payload.length > 0 : "getPayload() returned payload with invalid length!";
+      assert payload == null || payload.isValid() && payload.length > 0 : "getPayload() returned payload with invalid length!";
       return payload;
     }
   }
+  
+  /** Wraps a NumericDocValues but with additional asserts */
+  public static class AssertingNumericDocValues extends NumericDocValues {
+    private final NumericDocValues in;
+    private final int maxDoc;
+    
+    public AssertingNumericDocValues(NumericDocValues in, int maxDoc) {
+      this.in = in;
+      this.maxDoc = maxDoc;
+    }
 
+    @Override
+    public long get(int docID) {
+      assert docID >= 0 && docID < maxDoc;
+      return in.get(docID);
+    }    
+  }
+  
+  /** Wraps a BinaryDocValues but with additional asserts */
+  public static class AssertingBinaryDocValues extends BinaryDocValues {
+    private final BinaryDocValues in;
+    private final int maxDoc;
+    
+    public AssertingBinaryDocValues(BinaryDocValues in, int maxDoc) {
+      this.in = in;
+      this.maxDoc = maxDoc;
+    }
+
+    @Override
+    public void get(int docID, BytesRef result) {
+      assert docID >= 0 && docID < maxDoc;
+      assert result.isValid();
+      in.get(docID, result);
+      assert result.isValid();
+    }
+  }
+  
+  /** Wraps a SortedDocValues but with additional asserts */
+  public static class AssertingSortedDocValues extends SortedDocValues {
+    private final SortedDocValues in;
+    private final int maxDoc;
+    private final int valueCount;
+    
+    public AssertingSortedDocValues(SortedDocValues in, int maxDoc) {
+      this.in = in;
+      this.maxDoc = maxDoc;
+      this.valueCount = in.getValueCount();
+      assert valueCount >= 1 && valueCount <= maxDoc;
+    }
+
+    @Override
+    public int getOrd(int docID) {
+      assert docID >= 0 && docID < maxDoc;
+      int ord = in.getOrd(docID);
+      assert ord >= 0 && ord < valueCount;
+      return ord;
+    }
+
+    @Override
+    public void lookupOrd(int ord, BytesRef result) {
+      assert ord >= 0 && ord < valueCount;
+      assert result.isValid();
+      in.lookupOrd(ord, result);
+      assert result.isValid();
+    }
+
+    @Override
+    public int getValueCount() {
+      int valueCount = in.getValueCount();
+      assert valueCount == this.valueCount; // should not change
+      return valueCount;
+    }
+
+    @Override
+    public void get(int docID, BytesRef result) {
+      assert docID >= 0 && docID < maxDoc;
+      assert result.isValid();
+      in.get(docID, result);
+      assert result.isValid();
+    }
+
+    @Override
+    public int lookupTerm(BytesRef key) {
+      assert key.isValid();
+      int result = in.lookupTerm(key);
+      assert result < valueCount;
+      assert key.isValid();
+      return result;
+    }
+  }
+
+  @Override
+  public NumericDocValues getNumericDocValues(String field) throws IOException {
+    NumericDocValues dv = super.getNumericDocValues(field);
+    FieldInfo fi = getFieldInfos().fieldInfo(field);
+    if (dv != null) {
+      assert fi != null;
+      assert fi.getDocValuesType() == FieldInfo.DocValuesType.NUMERIC;
+      return new AssertingNumericDocValues(dv, maxDoc());
+    } else {
+      assert fi == null || fi.getDocValuesType() != FieldInfo.DocValuesType.NUMERIC;
+      return null;
+    }
+  }
+
+  @Override
+  public BinaryDocValues getBinaryDocValues(String field) throws IOException {
+    BinaryDocValues dv = super.getBinaryDocValues(field);
+    FieldInfo fi = getFieldInfos().fieldInfo(field);
+    if (dv != null) {
+      assert fi != null;
+      assert fi.getDocValuesType() == FieldInfo.DocValuesType.BINARY;
+      return new AssertingBinaryDocValues(dv, maxDoc());
+    } else {
+      assert fi == null || fi.getDocValuesType() != FieldInfo.DocValuesType.BINARY;
+      return null;
+    }
+  }
+
+  @Override
+  public SortedDocValues getSortedDocValues(String field) throws IOException {
+    SortedDocValues dv = super.getSortedDocValues(field);
+    FieldInfo fi = getFieldInfos().fieldInfo(field);
+    if (dv != null) {
+      assert fi != null;
+      assert fi.getDocValuesType() == FieldInfo.DocValuesType.SORTED;
+      return new AssertingSortedDocValues(dv, maxDoc());
+    } else {
+      assert fi == null || fi.getDocValuesType() != FieldInfo.DocValuesType.SORTED;
+      return null;
+    }
+  }
+
+  @Override
+  public NumericDocValues getNormValues(String field) throws IOException {
+    NumericDocValues dv = super.getNormValues(field);
+    FieldInfo fi = getFieldInfos().fieldInfo(field);
+    if (dv != null) {
+      assert fi != null;
+      assert fi.hasNorms();
+      return new AssertingNumericDocValues(dv, maxDoc());
+    } else {
+      assert fi == null || fi.hasNorms() == false;
+      return null;
+    }
+  }
+
   // this is the same hack as FCInvisible
   @Override
   public Object getCoreCacheKey() {
Index: lucene/test-framework/src/java/org/apache/lucene/index/BaseDocValuesFormatTestCase.java
===================================================================
--- lucene/test-framework/src/java/org/apache/lucene/index/BaseDocValuesFormatTestCase.java	(revision 0)
+++ lucene/test-framework/src/java/org/apache/lucene/index/BaseDocValuesFormatTestCase.java	(working copy)
@@ -0,0 +1,1269 @@
+package org.apache.lucene.index;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.HashMap;
+import java.util.Map;
+import java.util.Set;
+import java.util.Map.Entry;
+
+import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.analysis.MockAnalyzer;
+import org.apache.lucene.codecs.Codec;
+import org.apache.lucene.codecs.DocValuesFormat;
+import org.apache.lucene.codecs.lucene42.Lucene42Codec;
+import org.apache.lucene.document.BinaryDocValuesField;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.document.FloatDocValuesField;
+import org.apache.lucene.document.NumericDocValuesField;
+import org.apache.lucene.document.SortedDocValuesField;
+import org.apache.lucene.document.StoredField;
+import org.apache.lucene.document.StringField;
+import org.apache.lucene.document.TextField;
+import org.apache.lucene.index.FieldInfo.DocValuesType;
+import org.apache.lucene.search.BooleanClause;
+import org.apache.lucene.search.BooleanQuery;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.ScoreDoc;
+import org.apache.lucene.search.TermQuery;
+import org.apache.lucene.search.TopDocs;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.BytesRefHash;
+import org.apache.lucene.util.LuceneTestCase;
+import org.apache.lucene.util._TestUtil;
+
+/**
+ * Abstract class to do basic tests for a docvalues format.
+ * NOTE: This test focuses on the docvalues impl, nothing else.
+ * The [stretch] goal is for this test to be
+ * so thorough in testing a new DocValuesFormat that if this
+ * test passes, then all Lucene/Solr tests should also pass.  Ie,
+ * if there is some bug in a given DocValuesFormat that this
+ * test fails to catch then this test needs to be improved! */
+public abstract class BaseDocValuesFormatTestCase extends LuceneTestCase {
+  
+  /** Returns the codec to run tests against */
+  protected abstract Codec getCodec();
+  
+  private Codec savedCodec;
+  
+  public void setUp() throws Exception {
+    super.setUp();
+    // set the default codec, so adding test cases to this isn't fragile
+    savedCodec = Codec.getDefault();
+    Codec.setDefault(getCodec());
+  }
+  
+  public void tearDown() throws Exception {
+    Codec.setDefault(savedCodec); // restore
+    super.tearDown();
+  }
+
+  public void testOneNumber() throws IOException {
+    Directory directory = newDirectory();
+    RandomIndexWriter iwriter = new RandomIndexWriter(random(), directory);
+    Document doc = new Document();
+    String longTerm = "longtermlongtermlongtermlongtermlongtermlongtermlongtermlongtermlongtermlongtermlongtermlongtermlongtermlongtermlongtermlongtermlongtermlongterm";
+    String text = "This is the text to be indexed. " + longTerm;
+    doc.add(newTextField("fieldname", text, Field.Store.YES));
+    doc.add(new NumericDocValuesField("dv", 5));
+    iwriter.addDocument(doc);
+    iwriter.close();
+    
+    // Now search the index:
+    IndexReader ireader = DirectoryReader.open(directory); // read-only=true
+    IndexSearcher isearcher = new IndexSearcher(ireader);
+
+    assertEquals(1, isearcher.search(new TermQuery(new Term("fieldname", longTerm)), 1).totalHits);
+    Query query = new TermQuery(new Term("fieldname", "text"));
+    TopDocs hits = isearcher.search(query, null, 1);
+    assertEquals(1, hits.totalHits);
+    // Iterate through the results:
+    for (int i = 0; i < hits.scoreDocs.length; i++) {
+      StoredDocument hitDoc = isearcher.doc(hits.scoreDocs[i].doc);
+      assertEquals(text, hitDoc.get("fieldname"));
+      assert ireader.leaves().size() == 1;
+      NumericDocValues dv = ireader.leaves().get(0).reader().getNumericDocValues("dv");
+      assertEquals(5, dv.get(hits.scoreDocs[i].doc));
+    }
+
+    ireader.close();
+    directory.close();
+  }
+
+  public void testOneFloat() throws IOException {
+    Directory directory = newDirectory();
+    RandomIndexWriter iwriter = new RandomIndexWriter(random(), directory);
+    Document doc = new Document();
+    String longTerm = "longtermlongtermlongtermlongtermlongtermlongtermlongtermlongtermlongtermlongtermlongtermlongtermlongtermlongtermlongtermlongtermlongtermlongterm";
+    String text = "This is the text to be indexed. " + longTerm;
+    doc.add(newTextField("fieldname", text, Field.Store.YES));
+    doc.add(new FloatDocValuesField("dv", 5.7f));
+    iwriter.addDocument(doc);
+    iwriter.close();
+    
+    // Now search the index:
+    IndexReader ireader = DirectoryReader.open(directory); // read-only=true
+    IndexSearcher isearcher = new IndexSearcher(ireader);
+
+    assertEquals(1, isearcher.search(new TermQuery(new Term("fieldname", longTerm)), 1).totalHits);
+    Query query = new TermQuery(new Term("fieldname", "text"));
+    TopDocs hits = isearcher.search(query, null, 1);
+    assertEquals(1, hits.totalHits);
+    // Iterate through the results:
+    for (int i = 0; i < hits.scoreDocs.length; i++) {
+      StoredDocument hitDoc = isearcher.doc(hits.scoreDocs[i].doc);
+      assertEquals(text, hitDoc.get("fieldname"));
+      assert ireader.leaves().size() == 1;
+      NumericDocValues dv = ireader.leaves().get(0).reader().getNumericDocValues("dv");
+      assertEquals(Float.floatToRawIntBits(5.7f), dv.get(hits.scoreDocs[i].doc));
+    }
+
+    ireader.close();
+    directory.close();
+  }
+  
+  public void testTwoNumbers() throws IOException {
+    Directory directory = newDirectory();
+    RandomIndexWriter iwriter = new RandomIndexWriter(random(), directory);
+    Document doc = new Document();
+    String longTerm = "longtermlongtermlongtermlongtermlongtermlongtermlongtermlongtermlongtermlongtermlongtermlongtermlongtermlongtermlongtermlongtermlongtermlongterm";
+    String text = "This is the text to be indexed. " + longTerm;
+    doc.add(newTextField("fieldname", text, Field.Store.YES));
+    doc.add(new NumericDocValuesField("dv1", 5));
+    doc.add(new NumericDocValuesField("dv2", 17));
+    iwriter.addDocument(doc);
+    iwriter.close();
+    
+    // Now search the index:
+    IndexReader ireader = DirectoryReader.open(directory); // read-only=true
+    IndexSearcher isearcher = new IndexSearcher(ireader);
+
+    assertEquals(1, isearcher.search(new TermQuery(new Term("fieldname", longTerm)), 1).totalHits);
+    Query query = new TermQuery(new Term("fieldname", "text"));
+    TopDocs hits = isearcher.search(query, null, 1);
+    assertEquals(1, hits.totalHits);
+    // Iterate through the results:
+    for (int i = 0; i < hits.scoreDocs.length; i++) {
+      StoredDocument hitDoc = isearcher.doc(hits.scoreDocs[i].doc);
+      assertEquals(text, hitDoc.get("fieldname"));
+      assert ireader.leaves().size() == 1;
+      NumericDocValues dv = ireader.leaves().get(0).reader().getNumericDocValues("dv1");
+      assertEquals(5, dv.get(hits.scoreDocs[i].doc));
+      dv = ireader.leaves().get(0).reader().getNumericDocValues("dv2");
+      assertEquals(17, dv.get(hits.scoreDocs[i].doc));
+    }
+
+    ireader.close();
+    directory.close();
+  }
+
+  public void testTwoFieldsMixed() throws IOException {
+    Directory directory = newDirectory();
+    RandomIndexWriter iwriter = new RandomIndexWriter(random(), directory);
+    Document doc = new Document();
+    String longTerm = "longtermlongtermlongtermlongtermlongtermlongtermlongtermlongtermlongtermlongtermlongtermlongtermlongtermlongtermlongtermlongtermlongtermlongterm";
+    String text = "This is the text to be indexed. " + longTerm;
+    doc.add(newTextField("fieldname", text, Field.Store.YES));
+    doc.add(new NumericDocValuesField("dv1", 5));
+    doc.add(new BinaryDocValuesField("dv2", new BytesRef("hello world")));
+    iwriter.addDocument(doc);
+    iwriter.close();
+    
+    // Now search the index:
+    IndexReader ireader = DirectoryReader.open(directory); // read-only=true
+    IndexSearcher isearcher = new IndexSearcher(ireader);
+
+    assertEquals(1, isearcher.search(new TermQuery(new Term("fieldname", longTerm)), 1).totalHits);
+    Query query = new TermQuery(new Term("fieldname", "text"));
+    TopDocs hits = isearcher.search(query, null, 1);
+    assertEquals(1, hits.totalHits);
+    BytesRef scratch = new BytesRef();
+    // Iterate through the results:
+    for (int i = 0; i < hits.scoreDocs.length; i++) {
+      StoredDocument hitDoc = isearcher.doc(hits.scoreDocs[i].doc);
+      assertEquals(text, hitDoc.get("fieldname"));
+      assert ireader.leaves().size() == 1;
+      NumericDocValues dv = ireader.leaves().get(0).reader().getNumericDocValues("dv1");
+      assertEquals(5, dv.get(hits.scoreDocs[i].doc));
+      BinaryDocValues dv2 = ireader.leaves().get(0).reader().getBinaryDocValues("dv2");
+      dv2.get(hits.scoreDocs[i].doc, scratch);
+      assertEquals(new BytesRef("hello world"), scratch);
+    }
+
+    ireader.close();
+    directory.close();
+  }
+  
+  public void testThreeFieldsMixed() throws IOException {
+    Directory directory = newDirectory();
+    RandomIndexWriter iwriter = new RandomIndexWriter(random(), directory);
+    Document doc = new Document();
+    String longTerm = "longtermlongtermlongtermlongtermlongtermlongtermlongtermlongtermlongtermlongtermlongtermlongtermlongtermlongtermlongtermlongtermlongtermlongterm";
+    String text = "This is the text to be indexed. " + longTerm;
+    doc.add(newTextField("fieldname", text, Field.Store.YES));
+    doc.add(new SortedDocValuesField("dv1", new BytesRef("hello hello")));
+    doc.add(new NumericDocValuesField("dv2", 5));
+    doc.add(new BinaryDocValuesField("dv3", new BytesRef("hello world")));
+    iwriter.addDocument(doc);
+    iwriter.close();
+    
+    // Now search the index:
+    IndexReader ireader = DirectoryReader.open(directory); // read-only=true
+    IndexSearcher isearcher = new IndexSearcher(ireader);
+
+    assertEquals(1, isearcher.search(new TermQuery(new Term("fieldname", longTerm)), 1).totalHits);
+    Query query = new TermQuery(new Term("fieldname", "text"));
+    TopDocs hits = isearcher.search(query, null, 1);
+    assertEquals(1, hits.totalHits);
+    BytesRef scratch = new BytesRef();
+    // Iterate through the results:
+    for (int i = 0; i < hits.scoreDocs.length; i++) {
+      StoredDocument hitDoc = isearcher.doc(hits.scoreDocs[i].doc);
+      assertEquals(text, hitDoc.get("fieldname"));
+      assert ireader.leaves().size() == 1;
+      SortedDocValues dv = ireader.leaves().get(0).reader().getSortedDocValues("dv1");
+      int ord = dv.getOrd(0);
+      dv.lookupOrd(ord, scratch);
+      assertEquals(new BytesRef("hello hello"), scratch);
+      NumericDocValues dv2 = ireader.leaves().get(0).reader().getNumericDocValues("dv2");
+      assertEquals(5, dv2.get(hits.scoreDocs[i].doc));
+      BinaryDocValues dv3 = ireader.leaves().get(0).reader().getBinaryDocValues("dv3");
+      dv3.get(hits.scoreDocs[i].doc, scratch);
+      assertEquals(new BytesRef("hello world"), scratch);
+    }
+
+    ireader.close();
+    directory.close();
+  }
+  
+  public void testThreeFieldsMixed2() throws IOException {
+    Directory directory = newDirectory();
+    RandomIndexWriter iwriter = new RandomIndexWriter(random(), directory);
+    Document doc = new Document();
+    String longTerm = "longtermlongtermlongtermlongtermlongtermlongtermlongtermlongtermlongtermlongtermlongtermlongtermlongtermlongtermlongtermlongtermlongtermlongterm";
+    String text = "This is the text to be indexed. " + longTerm;
+    doc.add(newTextField("fieldname", text, Field.Store.YES));
+    doc.add(new BinaryDocValuesField("dv1", new BytesRef("hello world")));
+    doc.add(new SortedDocValuesField("dv2", new BytesRef("hello hello")));
+    doc.add(new NumericDocValuesField("dv3", 5));
+    iwriter.addDocument(doc);
+    iwriter.close();
+    
+    // Now search the index:
+    IndexReader ireader = DirectoryReader.open(directory); // read-only=true
+    IndexSearcher isearcher = new IndexSearcher(ireader);
+
+    assertEquals(1, isearcher.search(new TermQuery(new Term("fieldname", longTerm)), 1).totalHits);
+    Query query = new TermQuery(new Term("fieldname", "text"));
+    TopDocs hits = isearcher.search(query, null, 1);
+    assertEquals(1, hits.totalHits);
+    BytesRef scratch = new BytesRef();
+    // Iterate through the results:
+    for (int i = 0; i < hits.scoreDocs.length; i++) {
+      StoredDocument hitDoc = isearcher.doc(hits.scoreDocs[i].doc);
+      assertEquals(text, hitDoc.get("fieldname"));
+      assert ireader.leaves().size() == 1;
+      SortedDocValues dv = ireader.leaves().get(0).reader().getSortedDocValues("dv2");
+      int ord = dv.getOrd(0);
+      dv.lookupOrd(ord, scratch);
+      assertEquals(new BytesRef("hello hello"), scratch);
+      NumericDocValues dv2 = ireader.leaves().get(0).reader().getNumericDocValues("dv3");
+      assertEquals(5, dv2.get(hits.scoreDocs[i].doc));
+      BinaryDocValues dv3 = ireader.leaves().get(0).reader().getBinaryDocValues("dv1");
+      dv3.get(hits.scoreDocs[i].doc, scratch);
+      assertEquals(new BytesRef("hello world"), scratch);
+    }
+
+    ireader.close();
+    directory.close();
+  }
+  
+  public void testTwoDocumentsNumeric() throws IOException {
+    Analyzer analyzer = new MockAnalyzer(random());
+
+    Directory directory = newDirectory();
+    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);
+    conf.setMergePolicy(newLogMergePolicy());
+    RandomIndexWriter iwriter = new RandomIndexWriter(random(), directory, conf);
+    Document doc = new Document();
+    doc.add(new NumericDocValuesField("dv", 1));
+    iwriter.addDocument(doc);
+    doc = new Document();
+    doc.add(new NumericDocValuesField("dv", 2));
+    iwriter.addDocument(doc);
+    iwriter.forceMerge(1);
+    iwriter.close();
+    
+    // Now search the index:
+    IndexReader ireader = DirectoryReader.open(directory); // read-only=true
+    assert ireader.leaves().size() == 1;
+    NumericDocValues dv = ireader.leaves().get(0).reader().getNumericDocValues("dv");
+    assertEquals(1, dv.get(0));
+    assertEquals(2, dv.get(1));
+
+    ireader.close();
+    directory.close();
+  }
+  
+  public void testTwoDocumentsMerged() throws IOException {
+    Analyzer analyzer = new MockAnalyzer(random());
+
+    Directory directory = newDirectory();
+    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);
+    conf.setMergePolicy(newLogMergePolicy());
+    RandomIndexWriter iwriter = new RandomIndexWriter(random(), directory, conf);
+    Document doc = new Document();
+    doc.add(newField("id", "0", StringField.TYPE_STORED));
+    doc.add(new NumericDocValuesField("dv", -10));
+    iwriter.addDocument(doc);
+    iwriter.commit();
+    doc = new Document();
+    doc.add(newField("id", "1", StringField.TYPE_STORED));
+    doc.add(new NumericDocValuesField("dv", 99));
+    iwriter.addDocument(doc);
+    iwriter.forceMerge(1);
+    iwriter.close();
+    
+    // Now search the index:
+    IndexReader ireader = DirectoryReader.open(directory); // read-only=true
+    assert ireader.leaves().size() == 1;
+    NumericDocValues dv = ireader.leaves().get(0).reader().getNumericDocValues("dv");
+    for(int i=0;i<2;i++) {
+      StoredDocument doc2 = ireader.leaves().get(0).reader().document(i);
+      long expected;
+      if (doc2.get("id").equals("0")) {
+        expected = -10;
+      } else {
+        expected = 99;
+      }
+      assertEquals(expected, dv.get(i));
+    }
+
+    ireader.close();
+    directory.close();
+  }
+
+  public void testBigNumericRange() throws IOException {
+    Analyzer analyzer = new MockAnalyzer(random());
+
+    Directory directory = newDirectory();
+    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);
+    conf.setMergePolicy(newLogMergePolicy());
+    RandomIndexWriter iwriter = new RandomIndexWriter(random(), directory, conf);
+    Document doc = new Document();
+    doc.add(new NumericDocValuesField("dv", Long.MIN_VALUE));
+    iwriter.addDocument(doc);
+    doc = new Document();
+    doc.add(new NumericDocValuesField("dv", Long.MAX_VALUE));
+    iwriter.addDocument(doc);
+    iwriter.forceMerge(1);
+    iwriter.close();
+    
+    // Now search the index:
+    IndexReader ireader = DirectoryReader.open(directory); // read-only=true
+    assert ireader.leaves().size() == 1;
+    NumericDocValues dv = ireader.leaves().get(0).reader().getNumericDocValues("dv");
+    assertEquals(Long.MIN_VALUE, dv.get(0));
+    assertEquals(Long.MAX_VALUE, dv.get(1));
+
+    ireader.close();
+    directory.close();
+  }
+  
+  public void testBigNumericRange2() throws IOException {
+    Analyzer analyzer = new MockAnalyzer(random());
+
+    Directory directory = newDirectory();
+    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);
+    conf.setMergePolicy(newLogMergePolicy());
+    RandomIndexWriter iwriter = new RandomIndexWriter(random(), directory, conf);
+    Document doc = new Document();
+    doc.add(new NumericDocValuesField("dv", -8841491950446638677L));
+    iwriter.addDocument(doc);
+    doc = new Document();
+    doc.add(new NumericDocValuesField("dv", 9062230939892376225L));
+    iwriter.addDocument(doc);
+    iwriter.forceMerge(1);
+    iwriter.close();
+    
+    // Now search the index:
+    IndexReader ireader = DirectoryReader.open(directory); // read-only=true
+    assert ireader.leaves().size() == 1;
+    NumericDocValues dv = ireader.leaves().get(0).reader().getNumericDocValues("dv");
+    assertEquals(-8841491950446638677L, dv.get(0));
+    assertEquals(9062230939892376225L, dv.get(1));
+
+    ireader.close();
+    directory.close();
+  }
+  
+  public void testBytes() throws IOException {
+    Analyzer analyzer = new MockAnalyzer(random());
+
+    Directory directory = newDirectory();
+    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);
+    RandomIndexWriter iwriter = new RandomIndexWriter(random(), directory, conf);
+    Document doc = new Document();
+    String longTerm = "longtermlongtermlongtermlongtermlongtermlongtermlongtermlongtermlongtermlongtermlongtermlongtermlongtermlongtermlongtermlongtermlongtermlongterm";
+    String text = "This is the text to be indexed. " + longTerm;
+    doc.add(newTextField("fieldname", text, Field.Store.YES));
+    doc.add(new BinaryDocValuesField("dv", new BytesRef("hello world")));
+    iwriter.addDocument(doc);
+    iwriter.close();
+    
+    // Now search the index:
+    IndexReader ireader = DirectoryReader.open(directory); // read-only=true
+    IndexSearcher isearcher = new IndexSearcher(ireader);
+
+    assertEquals(1, isearcher.search(new TermQuery(new Term("fieldname", longTerm)), 1).totalHits);
+    Query query = new TermQuery(new Term("fieldname", "text"));
+    TopDocs hits = isearcher.search(query, null, 1);
+    assertEquals(1, hits.totalHits);
+    BytesRef scratch = new BytesRef();
+    // Iterate through the results:
+    for (int i = 0; i < hits.scoreDocs.length; i++) {
+      StoredDocument hitDoc = isearcher.doc(hits.scoreDocs[i].doc);
+      assertEquals(text, hitDoc.get("fieldname"));
+      assert ireader.leaves().size() == 1;
+      BinaryDocValues dv = ireader.leaves().get(0).reader().getBinaryDocValues("dv");
+      dv.get(hits.scoreDocs[i].doc, scratch);
+      assertEquals(new BytesRef("hello world"), scratch);
+    }
+
+    ireader.close();
+    directory.close();
+  }
+  
+  public void testBytesTwoDocumentsMerged() throws IOException {
+    Analyzer analyzer = new MockAnalyzer(random());
+
+    Directory directory = newDirectory();
+    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);
+    conf.setMergePolicy(newLogMergePolicy());
+    RandomIndexWriter iwriter = new RandomIndexWriter(random(), directory, conf);
+    Document doc = new Document();
+    doc.add(newField("id", "0", StringField.TYPE_STORED));
+    doc.add(new BinaryDocValuesField("dv", new BytesRef("hello world 1")));
+    iwriter.addDocument(doc);
+    iwriter.commit();
+    doc = new Document();
+    doc.add(newField("id", "1", StringField.TYPE_STORED));
+    doc.add(new BinaryDocValuesField("dv", new BytesRef("hello 2")));
+    iwriter.addDocument(doc);
+    iwriter.forceMerge(1);
+    iwriter.close();
+    
+    // Now search the index:
+    IndexReader ireader = DirectoryReader.open(directory); // read-only=true
+    assert ireader.leaves().size() == 1;
+    BinaryDocValues dv = ireader.leaves().get(0).reader().getBinaryDocValues("dv");
+    BytesRef scratch = new BytesRef();
+    for(int i=0;i<2;i++) {
+      StoredDocument doc2 = ireader.leaves().get(0).reader().document(i);
+      String expected;
+      if (doc2.get("id").equals("0")) {
+        expected = "hello world 1";
+      } else {
+        expected = "hello 2";
+      }
+      dv.get(i, scratch);
+      assertEquals(expected, scratch.utf8ToString());
+    }
+
+    ireader.close();
+    directory.close();
+  }
+
+  public void testSortedBytes() throws IOException {
+    Analyzer analyzer = new MockAnalyzer(random());
+
+    Directory directory = newDirectory();
+    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);
+    RandomIndexWriter iwriter = new RandomIndexWriter(random(), directory, conf);
+    Document doc = new Document();
+    String longTerm = "longtermlongtermlongtermlongtermlongtermlongtermlongtermlongtermlongtermlongtermlongtermlongtermlongtermlongtermlongtermlongtermlongtermlongterm";
+    String text = "This is the text to be indexed. " + longTerm;
+    doc.add(newTextField("fieldname", text, Field.Store.YES));
+    doc.add(new SortedDocValuesField("dv", new BytesRef("hello world")));
+    iwriter.addDocument(doc);
+    iwriter.close();
+    
+    // Now search the index:
+    IndexReader ireader = DirectoryReader.open(directory); // read-only=true
+    IndexSearcher isearcher = new IndexSearcher(ireader);
+
+    assertEquals(1, isearcher.search(new TermQuery(new Term("fieldname", longTerm)), 1).totalHits);
+    Query query = new TermQuery(new Term("fieldname", "text"));
+    TopDocs hits = isearcher.search(query, null, 1);
+    assertEquals(1, hits.totalHits);
+    BytesRef scratch = new BytesRef();
+    // Iterate through the results:
+    for (int i = 0; i < hits.scoreDocs.length; i++) {
+      StoredDocument hitDoc = isearcher.doc(hits.scoreDocs[i].doc);
+      assertEquals(text, hitDoc.get("fieldname"));
+      assert ireader.leaves().size() == 1;
+      SortedDocValues dv = ireader.leaves().get(0).reader().getSortedDocValues("dv");
+      dv.lookupOrd(dv.getOrd(hits.scoreDocs[i].doc), scratch);
+      assertEquals(new BytesRef("hello world"), scratch);
+    }
+
+    ireader.close();
+    directory.close();
+  }
+
+  public void testSortedBytesTwoDocuments() throws IOException {
+    Analyzer analyzer = new MockAnalyzer(random());
+
+    Directory directory = newDirectory();
+    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);
+    conf.setMergePolicy(newLogMergePolicy());
+    RandomIndexWriter iwriter = new RandomIndexWriter(random(), directory, conf);
+    Document doc = new Document();
+    doc.add(new SortedDocValuesField("dv", new BytesRef("hello world 1")));
+    iwriter.addDocument(doc);
+    doc = new Document();
+    doc.add(new SortedDocValuesField("dv", new BytesRef("hello world 2")));
+    iwriter.addDocument(doc);
+    iwriter.forceMerge(1);
+    iwriter.close();
+    
+    // Now search the index:
+    IndexReader ireader = DirectoryReader.open(directory); // read-only=true
+    assert ireader.leaves().size() == 1;
+    SortedDocValues dv = ireader.leaves().get(0).reader().getSortedDocValues("dv");
+    BytesRef scratch = new BytesRef();
+    dv.lookupOrd(dv.getOrd(0), scratch);
+    assertEquals("hello world 1", scratch.utf8ToString());
+    dv.lookupOrd(dv.getOrd(1), scratch);
+    assertEquals("hello world 2", scratch.utf8ToString());
+
+    ireader.close();
+    directory.close();
+  }
+  
+  public void testSortedBytesThreeDocuments() throws IOException {
+    Analyzer analyzer = new MockAnalyzer(random());
+
+    Directory directory = newDirectory();
+    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);
+    conf.setMergePolicy(newLogMergePolicy());
+    RandomIndexWriter iwriter = new RandomIndexWriter(random(), directory, conf);
+    Document doc = new Document();
+    doc.add(new SortedDocValuesField("dv", new BytesRef("hello world 1")));
+    iwriter.addDocument(doc);
+    doc = new Document();
+    doc.add(new SortedDocValuesField("dv", new BytesRef("hello world 2")));
+    iwriter.addDocument(doc);
+    doc = new Document();
+    doc.add(new SortedDocValuesField("dv", new BytesRef("hello world 1")));
+    iwriter.addDocument(doc);
+    iwriter.forceMerge(1);
+    iwriter.close();
+    
+    // Now search the index:
+    IndexReader ireader = DirectoryReader.open(directory); // read-only=true
+    assert ireader.leaves().size() == 1;
+    SortedDocValues dv = ireader.leaves().get(0).reader().getSortedDocValues("dv");
+    assertEquals(2, dv.getValueCount());
+    BytesRef scratch = new BytesRef();
+    assertEquals(0, dv.getOrd(0));
+    dv.lookupOrd(0, scratch);
+    assertEquals("hello world 1", scratch.utf8ToString());
+    assertEquals(1, dv.getOrd(1));
+    dv.lookupOrd(1, scratch);
+    assertEquals("hello world 2", scratch.utf8ToString());
+    assertEquals(0, dv.getOrd(2));
+
+    ireader.close();
+    directory.close();
+  }
+
+  public void testSortedBytesTwoDocumentsMerged() throws IOException {
+    Analyzer analyzer = new MockAnalyzer(random());
+
+    Directory directory = newDirectory();
+    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);
+    conf.setMergePolicy(newLogMergePolicy());
+    RandomIndexWriter iwriter = new RandomIndexWriter(random(), directory, conf);
+    Document doc = new Document();
+    doc.add(newField("id", "0", StringField.TYPE_STORED));
+    doc.add(new SortedDocValuesField("dv", new BytesRef("hello world 1")));
+    iwriter.addDocument(doc);
+    iwriter.commit();
+    doc = new Document();
+    doc.add(newField("id", "1", StringField.TYPE_STORED));
+    doc.add(new SortedDocValuesField("dv", new BytesRef("hello world 2")));
+    iwriter.addDocument(doc);
+    iwriter.forceMerge(1);
+    iwriter.close();
+    
+    // Now search the index:
+    IndexReader ireader = DirectoryReader.open(directory); // read-only=true
+    assert ireader.leaves().size() == 1;
+    SortedDocValues dv = ireader.leaves().get(0).reader().getSortedDocValues("dv");
+    assertEquals(2, dv.getValueCount()); // 2 ords
+    BytesRef scratch = new BytesRef();
+    dv.lookupOrd(0, scratch);
+    assertEquals(new BytesRef("hello world 1"), scratch);
+    dv.lookupOrd(1, scratch);
+    assertEquals(new BytesRef("hello world 2"), scratch);
+    for(int i=0;i<2;i++) {
+      StoredDocument doc2 = ireader.leaves().get(0).reader().document(i);
+      String expected;
+      if (doc2.get("id").equals("0")) {
+        expected = "hello world 1";
+      } else {
+        expected = "hello world 2";
+      }
+      dv.lookupOrd(dv.getOrd(i), scratch);
+      assertEquals(expected, scratch.utf8ToString());
+    }
+
+    ireader.close();
+    directory.close();
+  }
+
+  public void testBytesWithNewline() throws IOException {
+    Analyzer analyzer = new MockAnalyzer(random());
+
+    Directory directory = newDirectory();
+    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);
+    conf.setMergePolicy(newLogMergePolicy());
+    RandomIndexWriter iwriter = new RandomIndexWriter(random(), directory, conf);
+    Document doc = new Document();
+    doc.add(new BinaryDocValuesField("dv", new BytesRef("hello\nworld\r1")));
+    iwriter.addDocument(doc);
+    iwriter.close();
+    
+    // Now search the index:
+    IndexReader ireader = DirectoryReader.open(directory); // read-only=true
+    assert ireader.leaves().size() == 1;
+    BinaryDocValues dv = ireader.leaves().get(0).reader().getBinaryDocValues("dv");
+    BytesRef scratch = new BytesRef();
+    dv.get(0, scratch);
+    assertEquals(new BytesRef("hello\nworld\r1"), scratch);
+
+    ireader.close();
+    directory.close();
+  }
+
+  public void testMissingSortedBytes() throws IOException {
+    Analyzer analyzer = new MockAnalyzer(random());
+
+    Directory directory = newDirectory();
+    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);
+    conf.setMergePolicy(newLogMergePolicy());
+    RandomIndexWriter iwriter = new RandomIndexWriter(random(), directory, conf);
+    Document doc = new Document();
+    doc.add(new SortedDocValuesField("dv", new BytesRef("hello world 2")));
+    iwriter.addDocument(doc);
+    // 2nd doc missing the DV field
+    iwriter.addDocument(new Document());
+    iwriter.close();
+    
+    // Now search the index:
+    IndexReader ireader = DirectoryReader.open(directory); // read-only=true
+    assert ireader.leaves().size() == 1;
+    SortedDocValues dv = ireader.leaves().get(0).reader().getSortedDocValues("dv");
+    BytesRef scratch = new BytesRef();
+    dv.lookupOrd(dv.getOrd(0), scratch);
+    assertEquals(new BytesRef("hello world 2"), scratch);
+    dv.lookupOrd(dv.getOrd(1), scratch);
+    assertEquals(new BytesRef(""), scratch);
+    ireader.close();
+    directory.close();
+  }
+  
+  public void testEmptySortedBytes() throws IOException {
+    Analyzer analyzer = new MockAnalyzer(random());
+
+    Directory directory = newDirectory();
+    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);
+    conf.setMergePolicy(newLogMergePolicy());
+    RandomIndexWriter iwriter = new RandomIndexWriter(random(), directory, conf);
+    Document doc = new Document();
+    doc.add(new SortedDocValuesField("dv", new BytesRef("")));
+    iwriter.addDocument(doc);
+    doc = new Document();
+    doc.add(new SortedDocValuesField("dv", new BytesRef("")));
+    iwriter.addDocument(doc);
+    iwriter.forceMerge(1);
+    iwriter.close();
+    
+    // Now search the index:
+    IndexReader ireader = DirectoryReader.open(directory); // read-only=true
+    assert ireader.leaves().size() == 1;
+    SortedDocValues dv = ireader.leaves().get(0).reader().getSortedDocValues("dv");
+    BytesRef scratch = new BytesRef();
+    assertEquals(0, dv.getOrd(0));
+    assertEquals(0, dv.getOrd(1));
+    dv.lookupOrd(dv.getOrd(0), scratch);
+    assertEquals("", scratch.utf8ToString());
+
+    ireader.close();
+    directory.close();
+  }
+  
+  public void testEmptyBytes() throws IOException {
+    Analyzer analyzer = new MockAnalyzer(random());
+
+    Directory directory = newDirectory();
+    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);
+    conf.setMergePolicy(newLogMergePolicy());
+    RandomIndexWriter iwriter = new RandomIndexWriter(random(), directory, conf);
+    Document doc = new Document();
+    doc.add(new BinaryDocValuesField("dv", new BytesRef("")));
+    iwriter.addDocument(doc);
+    doc = new Document();
+    doc.add(new BinaryDocValuesField("dv", new BytesRef("")));
+    iwriter.addDocument(doc);
+    iwriter.forceMerge(1);
+    iwriter.close();
+    
+    // Now search the index:
+    IndexReader ireader = DirectoryReader.open(directory); // read-only=true
+    assert ireader.leaves().size() == 1;
+    BinaryDocValues dv = ireader.leaves().get(0).reader().getBinaryDocValues("dv");
+    BytesRef scratch = new BytesRef();
+    dv.get(0, scratch);
+    assertEquals("", scratch.utf8ToString());
+    dv.get(1, scratch);
+    assertEquals("", scratch.utf8ToString());
+
+    ireader.close();
+    directory.close();
+  }
+  
+  public void testVeryLargeButLegalBytes() throws IOException {
+    Analyzer analyzer = new MockAnalyzer(random());
+
+    Directory directory = newDirectory();
+    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);
+    conf.setMergePolicy(newLogMergePolicy());
+    RandomIndexWriter iwriter = new RandomIndexWriter(random(), directory, conf);
+    Document doc = new Document();
+    byte bytes[] = new byte[32766];
+    BytesRef b = new BytesRef(bytes);
+    random().nextBytes(bytes);
+    doc.add(new BinaryDocValuesField("dv", b));
+    iwriter.addDocument(doc);
+    iwriter.close();
+    
+    // Now search the index:
+    IndexReader ireader = DirectoryReader.open(directory); // read-only=true
+    assert ireader.leaves().size() == 1;
+    BinaryDocValues dv = ireader.leaves().get(0).reader().getBinaryDocValues("dv");
+    BytesRef scratch = new BytesRef();
+    dv.get(0, scratch);
+    assertEquals(new BytesRef(bytes), scratch);
+
+    ireader.close();
+    directory.close();
+  }
+  
+  public void testVeryLargeButLegalSortedBytes() throws IOException {
+    Analyzer analyzer = new MockAnalyzer(random());
+
+    Directory directory = newDirectory();
+    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);
+    conf.setMergePolicy(newLogMergePolicy());
+    RandomIndexWriter iwriter = new RandomIndexWriter(random(), directory, conf);
+    Document doc = new Document();
+    byte bytes[] = new byte[32766];
+    BytesRef b = new BytesRef(bytes);
+    random().nextBytes(bytes);
+    doc.add(new SortedDocValuesField("dv", b));
+    iwriter.addDocument(doc);
+    iwriter.close();
+    
+    // Now search the index:
+    IndexReader ireader = DirectoryReader.open(directory); // read-only=true
+    assert ireader.leaves().size() == 1;
+    BinaryDocValues dv = ireader.leaves().get(0).reader().getSortedDocValues("dv");
+    BytesRef scratch = new BytesRef();
+    dv.get(0, scratch);
+    assertEquals(new BytesRef(bytes), scratch);
+    ireader.close();
+    directory.close();
+  }
+  
+  public void testCodecUsesOwnBytes() throws IOException {
+    Analyzer analyzer = new MockAnalyzer(random());
+
+    Directory directory = newDirectory();
+    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);
+    conf.setMergePolicy(newLogMergePolicy());
+    RandomIndexWriter iwriter = new RandomIndexWriter(random(), directory, conf);
+    Document doc = new Document();
+    doc.add(new BinaryDocValuesField("dv", new BytesRef("boo!")));
+    iwriter.addDocument(doc);
+    iwriter.close();
+    
+    // Now search the index:
+    IndexReader ireader = DirectoryReader.open(directory); // read-only=true
+    assert ireader.leaves().size() == 1;
+    BinaryDocValues dv = ireader.leaves().get(0).reader().getBinaryDocValues("dv");
+    byte mybytes[] = new byte[20];
+    BytesRef scratch = new BytesRef(mybytes);
+    dv.get(0, scratch);
+    assertEquals("boo!", scratch.utf8ToString());
+    assertFalse(scratch.bytes == mybytes);
+
+    ireader.close();
+    directory.close();
+  }
+  
+  public void testCodecUsesOwnSortedBytes() throws IOException {
+    Analyzer analyzer = new MockAnalyzer(random());
+
+    Directory directory = newDirectory();
+    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);
+    conf.setMergePolicy(newLogMergePolicy());
+    RandomIndexWriter iwriter = new RandomIndexWriter(random(), directory, conf);
+    Document doc = new Document();
+    doc.add(new SortedDocValuesField("dv", new BytesRef("boo!")));
+    iwriter.addDocument(doc);
+    iwriter.close();
+    
+    // Now search the index:
+    IndexReader ireader = DirectoryReader.open(directory); // read-only=true
+    assert ireader.leaves().size() == 1;
+    BinaryDocValues dv = ireader.leaves().get(0).reader().getSortedDocValues("dv");
+    byte mybytes[] = new byte[20];
+    BytesRef scratch = new BytesRef(mybytes);
+    dv.get(0, scratch);
+    assertEquals("boo!", scratch.utf8ToString());
+    assertFalse(scratch.bytes == mybytes);
+
+    ireader.close();
+    directory.close();
+  }
+  
+  public void testCodecUsesOwnBytesEachTime() throws IOException {
+    Analyzer analyzer = new MockAnalyzer(random());
+
+    Directory directory = newDirectory();
+    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);
+    conf.setMergePolicy(newLogMergePolicy());
+    RandomIndexWriter iwriter = new RandomIndexWriter(random(), directory, conf);
+    Document doc = new Document();
+    doc.add(new BinaryDocValuesField("dv", new BytesRef("foo!")));
+    iwriter.addDocument(doc);
+    doc = new Document();
+    doc.add(new BinaryDocValuesField("dv", new BytesRef("bar!")));
+    iwriter.addDocument(doc);
+    iwriter.close();
+    
+    // Now search the index:
+    IndexReader ireader = DirectoryReader.open(directory); // read-only=true
+    assert ireader.leaves().size() == 1;
+    BinaryDocValues dv = ireader.leaves().get(0).reader().getBinaryDocValues("dv");
+    BytesRef scratch = new BytesRef();
+    dv.get(0, scratch);
+    assertEquals("foo!", scratch.utf8ToString());
+    
+    BytesRef scratch2 = new BytesRef();
+    dv.get(1, scratch2);
+    assertEquals("bar!", scratch2.utf8ToString());
+    // check scratch is still valid
+    assertEquals("foo!", scratch.utf8ToString());
+
+    ireader.close();
+    directory.close();
+  }
+  
+  public void testCodecUsesOwnSortedBytesEachTime() throws IOException {
+    Analyzer analyzer = new MockAnalyzer(random());
+
+    Directory directory = newDirectory();
+    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);
+    conf.setMergePolicy(newLogMergePolicy());
+    RandomIndexWriter iwriter = new RandomIndexWriter(random(), directory, conf);
+    Document doc = new Document();
+    doc.add(new SortedDocValuesField("dv", new BytesRef("foo!")));
+    iwriter.addDocument(doc);
+    doc = new Document();
+    doc.add(new SortedDocValuesField("dv", new BytesRef("bar!")));
+    iwriter.addDocument(doc);
+    iwriter.close();
+    
+    // Now search the index:
+    IndexReader ireader = DirectoryReader.open(directory); // read-only=true
+    assert ireader.leaves().size() == 1;
+    BinaryDocValues dv = ireader.leaves().get(0).reader().getSortedDocValues("dv");
+    BytesRef scratch = new BytesRef();
+    dv.get(0, scratch);
+    assertEquals("foo!", scratch.utf8ToString());
+    
+    BytesRef scratch2 = new BytesRef();
+    dv.get(1, scratch2);
+    assertEquals("bar!", scratch2.utf8ToString());
+    // check scratch is still valid
+    assertEquals("foo!", scratch.utf8ToString());
+
+    ireader.close();
+    directory.close();
+  }
+  
+  /*
+   * Simple test case to show how to use the API
+   */
+  public void testDocValuesSimple() throws IOException {
+    Directory dir = newDirectory();
+    Analyzer analyzer = new MockAnalyzer(random());
+    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);
+    conf.setMergePolicy(newLogMergePolicy());
+    IndexWriter writer = new IndexWriter(dir, conf);
+    for (int i = 0; i < 5; i++) {
+      Document doc = new Document();
+      doc.add(new NumericDocValuesField("docId", i));
+      doc.add(new TextField("docId", "" + i, Field.Store.NO));
+      writer.addDocument(doc);
+    }
+    writer.commit();
+    writer.forceMerge(1, true);
+
+    writer.close(true);
+
+    DirectoryReader reader = DirectoryReader.open(dir, 1);
+    assertEquals(1, reader.leaves().size());
+  
+    IndexSearcher searcher = new IndexSearcher(reader);
+
+    BooleanQuery query = new BooleanQuery();
+    query.add(new TermQuery(new Term("docId", "0")), BooleanClause.Occur.SHOULD);
+    query.add(new TermQuery(new Term("docId", "1")), BooleanClause.Occur.SHOULD);
+    query.add(new TermQuery(new Term("docId", "2")), BooleanClause.Occur.SHOULD);
+    query.add(new TermQuery(new Term("docId", "3")), BooleanClause.Occur.SHOULD);
+    query.add(new TermQuery(new Term("docId", "4")), BooleanClause.Occur.SHOULD);
+
+    TopDocs search = searcher.search(query, 10);
+    assertEquals(5, search.totalHits);
+    ScoreDoc[] scoreDocs = search.scoreDocs;
+    NumericDocValues docValues = getOnlySegmentReader(reader).getNumericDocValues("docId");
+    for (int i = 0; i < scoreDocs.length; i++) {
+      assertEquals(i, scoreDocs[i].doc);
+      assertEquals(i, docValues.get(scoreDocs[i].doc));
+    }
+    reader.close();
+    dir.close();
+  }
+  
+  public void testRandomSortedBytes() throws IOException {
+    Directory dir = newDirectory();
+    IndexWriterConfig cfg = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
+    RandomIndexWriter w = new RandomIndexWriter(random(), dir, cfg);
+    int numDocs = atLeast(100);
+    BytesRefHash hash = new BytesRefHash();
+    Map<String, String> docToString = new HashMap<String, String>();
+    int maxLength = _TestUtil.nextInt(random(), 1, 50);
+    for (int i = 0; i < numDocs; i++) {
+      Document doc = new Document();
+      doc.add(newTextField("id", "" + i, Field.Store.YES));
+      String string = _TestUtil.randomRealisticUnicodeString(random(), 1, maxLength);
+      BytesRef br = new BytesRef(string);
+      doc.add(new SortedDocValuesField("field", br));
+      hash.add(br);
+      docToString.put("" + i, string);
+      w.addDocument(doc);
+    }
+    if (rarely()) {
+      w.commit();
+    }
+    int numDocsNoValue = atLeast(10);
+    for (int i = 0; i < numDocsNoValue; i++) {
+      Document doc = new Document();
+      doc.add(newTextField("id", "noValue", Field.Store.YES));
+      w.addDocument(doc);
+    }
+    BytesRef bytesRef = new BytesRef();
+    hash.add(bytesRef); // add empty value for the gaps
+    if (rarely()) {
+      w.commit();
+    }
+    for (int i = 0; i < numDocs; i++) {
+      Document doc = new Document();
+      String id = "" + i + numDocs;
+      doc.add(newTextField("id", id, Field.Store.YES));
+      String string = _TestUtil.randomRealisticUnicodeString(random(), 1, maxLength);
+      BytesRef br = new BytesRef(string);
+      hash.add(br);
+      docToString.put(id, string);
+      doc.add(new SortedDocValuesField("field", br));
+      w.addDocument(doc);
+    }
+    w.commit();
+    IndexReader reader = w.getReader();
+    SortedDocValues docValues = MultiDocValues.getSortedValues(reader, "field");
+    int[] sort = hash.sort(BytesRef.getUTF8SortedAsUnicodeComparator());
+    BytesRef expected = new BytesRef();
+    BytesRef actual = new BytesRef();
+    assertEquals(hash.size(), docValues.getValueCount());
+    for (int i = 0; i < hash.size(); i++) {
+      hash.get(sort[i], expected);
+      docValues.lookupOrd(i, actual);
+      assertEquals(expected.utf8ToString(), actual.utf8ToString());
+      int ord = docValues.lookupTerm(expected);
+      assertEquals(i, ord);
+    }
+    AtomicReader slowR = SlowCompositeReaderWrapper.wrap(reader);
+    Set<Entry<String, String>> entrySet = docToString.entrySet();
+
+    for (Entry<String, String> entry : entrySet) {
+      // pk lookup
+      DocsEnum termDocsEnum = slowR.termDocsEnum(new Term("id", entry.getKey()));
+      int docId = termDocsEnum.nextDoc();
+      expected = new BytesRef(entry.getValue());
+      docValues.get(docId, actual);
+      assertEquals(expected, actual);
+    }
+
+    reader.close();
+    w.close();
+    dir.close();
+  }
+  
+  private void doTestNumericsVsStoredFields(long minValue, long maxValue) throws Exception {
+    Directory dir = newDirectory();
+    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, conf);
+    Document doc = new Document();
+    Field idField = new StringField("id", "", Field.Store.NO);
+    Field storedField = newStringField("stored", "", Field.Store.YES);
+    Field dvField = new NumericDocValuesField("dv", 0);
+    doc.add(idField);
+    doc.add(storedField);
+    doc.add(dvField);
+    
+    // index some docs
+    int numDocs = atLeast(1000);
+    for (int i = 0; i < numDocs; i++) {
+      idField.setStringValue(Integer.toString(i));
+      long value = _TestUtil.nextLong(random(), minValue, maxValue);
+      storedField.setStringValue(Long.toString(value));
+      dvField.setLongValue(value);
+      writer.addDocument(doc);
+      if (random().nextInt(31) == 0) {
+        writer.commit();
+      }
+    }
+    
+    // delete some docs
+    int numDeletions = random().nextInt(numDocs/10);
+    for (int i = 0; i < numDeletions; i++) {
+      int id = random().nextInt(numDocs);
+      writer.deleteDocuments(new Term("id", Integer.toString(id)));
+    }
+    writer.close();
+    
+    // compare
+    DirectoryReader ir = DirectoryReader.open(dir);
+    for (AtomicReaderContext context : ir.leaves()) {
+      AtomicReader r = context.reader();
+      NumericDocValues docValues = r.getNumericDocValues("dv");
+      for (int i = 0; i < r.maxDoc(); i++) {
+        long storedValue = Long.parseLong(r.document(i).get("stored"));
+        assertEquals(storedValue, docValues.get(i));
+      }
+    }
+    ir.close();
+    dir.close();
+  }
+  
+  public void testBooleanNumericsVsStoredFields() throws Exception {
+    int numIterations = atLeast(1);
+    for (int i = 0; i < numIterations; i++) {
+      doTestNumericsVsStoredFields(0, 1);
+    }
+  }
+  
+  public void testByteNumericsVsStoredFields() throws Exception {
+    int numIterations = atLeast(1);
+    for (int i = 0; i < numIterations; i++) {
+      doTestNumericsVsStoredFields(Byte.MIN_VALUE, Byte.MAX_VALUE);
+    }
+  }
+  
+  public void testShortNumericsVsStoredFields() throws Exception {
+    int numIterations = atLeast(1);
+    for (int i = 0; i < numIterations; i++) {
+      doTestNumericsVsStoredFields(Short.MIN_VALUE, Short.MAX_VALUE);
+    }
+  }
+  
+  public void testIntNumericsVsStoredFields() throws Exception {
+    int numIterations = atLeast(1);
+    for (int i = 0; i < numIterations; i++) {
+      doTestNumericsVsStoredFields(Integer.MIN_VALUE, Integer.MAX_VALUE);
+    }
+  }
+  
+  public void testLongNumericsVsStoredFields() throws Exception {
+    int numIterations = atLeast(1);
+    for (int i = 0; i < numIterations; i++) {
+      doTestNumericsVsStoredFields(Long.MIN_VALUE, Long.MAX_VALUE);
+    }
+  }
+  
+  private void doTestBinaryVsStoredFields(int minLength, int maxLength) throws Exception {
+    Directory dir = newDirectory();
+    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, conf);
+    Document doc = new Document();
+    Field idField = new StringField("id", "", Field.Store.NO);
+    Field storedField = new StoredField("stored", new byte[0]);
+    Field dvField = new BinaryDocValuesField("dv", new BytesRef());
+    doc.add(idField);
+    doc.add(storedField);
+    doc.add(dvField);
+    
+    // index some docs
+    int numDocs = atLeast(1000);
+    for (int i = 0; i < numDocs; i++) {
+      idField.setStringValue(Integer.toString(i));
+      final int length;
+      if (minLength == maxLength) {
+        length = minLength; // fixed length
+      } else {
+        length = _TestUtil.nextInt(random(), minLength, maxLength);
+      }
+      byte buffer[] = new byte[length];
+      random().nextBytes(buffer);
+      storedField.setBytesValue(buffer);
+      dvField.setBytesValue(buffer);
+      writer.addDocument(doc);
+      if (random().nextInt(31) == 0) {
+        writer.commit();
+      }
+    }
+    
+    // delete some docs
+    int numDeletions = random().nextInt(numDocs/10);
+    for (int i = 0; i < numDeletions; i++) {
+      int id = random().nextInt(numDocs);
+      writer.deleteDocuments(new Term("id", Integer.toString(id)));
+    }
+    writer.close();
+    
+    // compare
+    DirectoryReader ir = DirectoryReader.open(dir);
+    for (AtomicReaderContext context : ir.leaves()) {
+      AtomicReader r = context.reader();
+      BinaryDocValues docValues = r.getBinaryDocValues("dv");
+      for (int i = 0; i < r.maxDoc(); i++) {
+        BytesRef binaryValue = r.document(i).getBinaryValue("stored");
+        BytesRef scratch = new BytesRef();
+        docValues.get(i, scratch);
+        assertEquals(binaryValue, scratch);
+      }
+    }
+    ir.close();
+    dir.close();
+  }
+  
+  public void testBinaryFixedLengthVsStoredFields() throws Exception {
+    int numIterations = atLeast(1);
+    for (int i = 0; i < numIterations; i++) {
+      int fixedLength = _TestUtil.nextInt(random(), 1, 10);
+      doTestBinaryVsStoredFields(fixedLength, fixedLength);
+    }
+  }
+  
+  public void testBinaryVariableLengthVsStoredFields() throws Exception {
+    int numIterations = atLeast(1);
+    for (int i = 0; i < numIterations; i++) {
+      doTestBinaryVsStoredFields(1, 10);
+    }
+  }
+  
+  private void doTestSortedVsStoredFields(int minLength, int maxLength) throws Exception {
+    Directory dir = newDirectory();
+    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, conf);
+    Document doc = new Document();
+    Field idField = new StringField("id", "", Field.Store.NO);
+    Field storedField = new StoredField("stored", new byte[0]);
+    Field dvField = new SortedDocValuesField("dv", new BytesRef());
+    doc.add(idField);
+    doc.add(storedField);
+    doc.add(dvField);
+    
+    // index some docs
+    int numDocs = atLeast(1000);
+    for (int i = 0; i < numDocs; i++) {
+      idField.setStringValue(Integer.toString(i));
+      final int length;
+      if (minLength == maxLength) {
+        length = minLength; // fixed length
+      } else {
+        length = _TestUtil.nextInt(random(), minLength, maxLength);
+      }
+      byte buffer[] = new byte[length];
+      random().nextBytes(buffer);
+      storedField.setBytesValue(buffer);
+      dvField.setBytesValue(buffer);
+      writer.addDocument(doc);
+      if (random().nextInt(31) == 0) {
+        writer.commit();
+      }
+    }
+    
+    // delete some docs
+    int numDeletions = random().nextInt(numDocs/10);
+    for (int i = 0; i < numDeletions; i++) {
+      int id = random().nextInt(numDocs);
+      writer.deleteDocuments(new Term("id", Integer.toString(id)));
+    }
+    writer.close();
+    
+    // compare
+    DirectoryReader ir = DirectoryReader.open(dir);
+    for (AtomicReaderContext context : ir.leaves()) {
+      AtomicReader r = context.reader();
+      BinaryDocValues docValues = r.getSortedDocValues("dv");
+      for (int i = 0; i < r.maxDoc(); i++) {
+        BytesRef binaryValue = r.document(i).getBinaryValue("stored");
+        BytesRef scratch = new BytesRef();
+        docValues.get(i, scratch);
+        assertEquals(binaryValue, scratch);
+      }
+    }
+    ir.close();
+    dir.close();
+  }
+  
+  public void testSortedFixedLengthVsStoredFields() throws Exception {
+    int numIterations = atLeast(1);
+    for (int i = 0; i < numIterations; i++) {
+      int fixedLength = _TestUtil.nextInt(random(), 1, 10);
+      doTestSortedVsStoredFields(fixedLength, fixedLength);
+    }
+  }
+  
+  public void testSortedVariableLengthVsStoredFields() throws Exception {
+    int numIterations = atLeast(1);
+    for (int i = 0; i < numIterations; i++) {
+      doTestSortedVsStoredFields(1, 10);
+    }
+  }
+}

Property changes on: lucene/test-framework/src/java/org/apache/lucene/index/BaseDocValuesFormatTestCase.java
___________________________________________________________________
Added: svn:eol-style
## -0,0 +1 ##
+native
Index: lucene/test-framework/src/java/org/apache/lucene/util/TestRuleSetupAndRestoreClassEnv.java
===================================================================
--- lucene/test-framework/src/java/org/apache/lucene/util/TestRuleSetupAndRestoreClassEnv.java	(revision 1442822)
+++ lucene/test-framework/src/java/org/apache/lucene/util/TestRuleSetupAndRestoreClassEnv.java	(working copy)
@@ -28,10 +28,13 @@
 import java.util.TimeZone;
 
 import org.apache.lucene.codecs.Codec;
+import org.apache.lucene.codecs.DocValuesFormat;
 import org.apache.lucene.codecs.PostingsFormat;
 import org.apache.lucene.codecs.asserting.AssertingCodec;
+import org.apache.lucene.codecs.cheapbastard.CheapBastardCodec;
 import org.apache.lucene.codecs.compressing.CompressingCodec;
 import org.apache.lucene.codecs.lucene40.Lucene40Codec;
+import org.apache.lucene.codecs.lucene40.Lucene40RWCodec;
 import org.apache.lucene.codecs.lucene40.Lucene40RWPostingsFormat;
 import org.apache.lucene.codecs.lucene41.Lucene41Codec;
 import org.apache.lucene.codecs.lucene41.Lucene41RWCodec;
@@ -137,23 +140,39 @@
     int randomVal = random.nextInt(10);
     if ("Lucene40".equals(TEST_CODEC) || ("random".equals(TEST_CODEC) &&
                                           "random".equals(TEST_POSTINGSFORMAT) &&
+                                          "random".equals(TEST_DOCVALUESFORMAT) &&
                                           randomVal == 0 &&
                                           !shouldAvoidCodec("Lucene40"))) {
       codec = Codec.forName("Lucene40");
+      assert codec instanceof Lucene40RWCodec : "fix your classpath to have tests-framework.jar before lucene-core.jar";
       assert (PostingsFormat.forName("Lucene40") instanceof Lucene40RWPostingsFormat) : "fix your classpath to have tests-framework.jar before lucene-core.jar";
     } else if ("Lucene41".equals(TEST_CODEC) || ("random".equals(TEST_CODEC) &&
                                                  "random".equals(TEST_POSTINGSFORMAT) &&
+                                                 "random".equals(TEST_DOCVALUESFORMAT) &&
                                                  randomVal == 1 &&
                                                  !shouldAvoidCodec("Lucene41"))) { 
       codec = Codec.forName("Lucene41");
       assert codec instanceof Lucene41RWCodec : "fix your classpath to have tests-framework.jar before lucene-core.jar";
-    } else if (!"random".equals(TEST_POSTINGSFORMAT)) {
+    } else if (("random".equals(TEST_POSTINGSFORMAT) == false) || ("random".equals(TEST_DOCVALUESFORMAT) == false)) {
+      // the user wired postings or DV: this is messy
+      // refactor into RandomCodec....
+      
       final PostingsFormat format;
-      if ("MockRandom".equals(TEST_POSTINGSFORMAT)) {
-        format = new MockRandomPostingsFormat(random);
+      if ("random".equals(TEST_POSTINGSFORMAT)) {
+        format = PostingsFormat.forName("Lucene41");
       } else {
         format = PostingsFormat.forName(TEST_POSTINGSFORMAT);
       }
+      
+      final DocValuesFormat dvFormat;
+      if ("random".equals(TEST_DOCVALUESFORMAT)) {
+        // pick one from SPI
+        String formats[] = DocValuesFormat.availableDocValuesFormats().toArray(new String[0]);
+        dvFormat = DocValuesFormat.forName(formats[random.nextInt(formats.length)]);
+      } else {
+        dvFormat = DocValuesFormat.forName(TEST_DOCVALUESFORMAT);
+      }
+      
       codec = new Lucene42Codec() {       
         @Override
         public PostingsFormat getPostingsFormatForField(String field) {
@@ -161,12 +180,19 @@
         }
 
         @Override
+        public DocValuesFormat getDocValuesFormatForField(String field) {
+          return dvFormat;
+        }
+
+        @Override
         public String toString() {
-          return super.toString() + ": " + format.toString();
+          return super.toString() + ": " + format.toString() + ", " + dvFormat.toString();
         }
       };
     } else if ("SimpleText".equals(TEST_CODEC) || ("random".equals(TEST_CODEC) && randomVal == 9 && !shouldAvoidCodec("SimpleText"))) {
       codec = new SimpleTextCodec();
+    } else if ("CheapBastard".equals(TEST_CODEC) || ("random".equals(TEST_CODEC) && randomVal == 8 && !shouldAvoidCodec("CheapBastard"))) {
+      codec = new CheapBastardCodec();
     } else if ("Asserting".equals(TEST_CODEC) || ("random".equals(TEST_CODEC) && randomVal == 7 && !shouldAvoidCodec("Asserting"))) {
       codec = new AssertingCodec();
     } else if ("Compressing".equals(TEST_CODEC) || ("random".equals(TEST_CODEC) && randomVal == 6 && !shouldAvoidCodec("Compressing"))) {
Index: lucene/test-framework/src/java/org/apache/lucene/util/LuceneTestCase.java
===================================================================
--- lucene/test-framework/src/java/org/apache/lucene/util/LuceneTestCase.java	(revision 1442822)
+++ lucene/test-framework/src/java/org/apache/lucene/util/LuceneTestCase.java	(working copy)
@@ -253,6 +253,9 @@
 
   /** Gets the postingsFormat to run tests with. */
   public static final String TEST_POSTINGSFORMAT = System.getProperty("tests.postingsformat", "random");
+  
+  /** Gets the docValuesFormat to run tests with */
+  public static final String TEST_DOCVALUESFORMAT = System.getProperty("tests.docvaluesformat", "random");
 
   /** Gets the directory to run tests with */
   public static final String TEST_DIRECTORY = System.getProperty("tests.directory", "random");
@@ -1243,6 +1246,9 @@
       if (maybeWrap) {
         r = maybeWrapReader(r);
       }
+      if (r instanceof AtomicReader) {
+        _TestUtil.checkReader((AtomicReader)r);
+      }
       IndexSearcher ret = random.nextBoolean() ? new AssertingIndexSearcher(random, r) : new AssertingIndexSearcher(random, r.getContext());
       ret.setSimilarity(classEnvRule.similarity);
       return ret;
Index: lucene/test-framework/src/java/org/apache/lucene/util/_TestUtil.java
===================================================================
--- lucene/test-framework/src/java/org/apache/lucene/util/_TestUtil.java	(revision 1442822)
+++ lucene/test-framework/src/java/org/apache/lucene/util/_TestUtil.java	(working copy)
@@ -43,26 +43,26 @@
 import java.util.zip.ZipFile;
 
 import org.apache.lucene.codecs.Codec;
+import org.apache.lucene.codecs.DocValuesFormat;
 import org.apache.lucene.codecs.PostingsFormat;
 import org.apache.lucene.codecs.lucene42.Lucene42Codec;
 import org.apache.lucene.codecs.perfield.PerFieldPostingsFormat;
-import org.apache.lucene.document.ByteDocValuesField;
-import org.apache.lucene.document.DerefBytesDocValuesField;
+import org.apache.lucene.document.BinaryDocValuesField;
 import org.apache.lucene.document.Document;
-import org.apache.lucene.document.DoubleDocValuesField;
 import org.apache.lucene.document.Field;
-import org.apache.lucene.document.FloatDocValuesField;
-import org.apache.lucene.document.IntDocValuesField;
-import org.apache.lucene.document.LongDocValuesField;
-import org.apache.lucene.document.PackedLongDocValuesField;
-import org.apache.lucene.document.ShortDocValuesField;
-import org.apache.lucene.document.SortedBytesDocValuesField;
-import org.apache.lucene.document.StraightBytesDocValuesField;
+import org.apache.lucene.document.NumericDocValuesField;
+import org.apache.lucene.document.SortedDocValuesField;
+import org.apache.lucene.index.AtomicReader;
 import org.apache.lucene.index.CheckIndex;
+import org.apache.lucene.index.CheckIndex.Status.DocValuesStatus;
+import org.apache.lucene.index.CheckIndex.Status.FieldNormStatus;
+import org.apache.lucene.index.CheckIndex.Status.StoredFieldStatus;
+import org.apache.lucene.index.CheckIndex.Status.TermIndexStatus;
+import org.apache.lucene.index.CheckIndex.Status.TermVectorStatus;
 import org.apache.lucene.index.ConcurrentMergeScheduler;
-import org.apache.lucene.index.DocValues;
 import org.apache.lucene.index.DocsAndPositionsEnum;
 import org.apache.lucene.index.DocsEnum;
+import org.apache.lucene.index.FieldInfo.DocValuesType;
 import org.apache.lucene.index.FieldInfos;
 import org.apache.lucene.index.IndexFileNames;
 import org.apache.lucene.index.IndexReader;
@@ -77,15 +77,14 @@
 import org.apache.lucene.index.TermsEnum;
 import org.apache.lucene.index.TieredMergePolicy;
 import org.apache.lucene.search.FieldDoc;
+import org.apache.lucene.search.FilteredQuery.FilterStrategy;
 import org.apache.lucene.search.FilteredQuery;
-import org.apache.lucene.search.FilteredQuery.FilterStrategy;
 import org.apache.lucene.search.ScoreDoc;
 import org.apache.lucene.search.TopDocs;
 import org.apache.lucene.store.CompoundFileDirectory;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.IOContext;
 import org.junit.Assert;
-
 import com.carrotsearch.randomizedtesting.RandomizedContext;
 import com.carrotsearch.randomizedtesting.generators.RandomInts;
 import com.carrotsearch.randomizedtesting.generators.RandomPicks;
@@ -226,13 +225,46 @@
       return indexStatus;
     }
   }
+  
+  /** This runs the CheckIndex tool on the Reader.  If any
+   *  issues are hit, a RuntimeException is thrown */
+  public static void checkReader(AtomicReader reader) throws IOException {
+    checkReader(reader, true);
+  }
+  
+  public static void checkReader(AtomicReader reader, boolean crossCheckTermVectors) throws IOException {
+    ByteArrayOutputStream bos = new ByteArrayOutputStream(1024);
+    PrintStream infoStream = new PrintStream(bos, false, "UTF-8");
 
+    FieldNormStatus fieldNormStatus = CheckIndex.testFieldNorms(reader, infoStream);
+    TermIndexStatus termIndexStatus = CheckIndex.testPostings(reader, infoStream);
+    StoredFieldStatus storedFieldStatus = CheckIndex.testStoredFields(reader, infoStream);
+    TermVectorStatus termVectorStatus = CheckIndex.testTermVectors(reader, infoStream, false, crossCheckTermVectors);
+    DocValuesStatus docValuesStatus = CheckIndex.testDocValues(reader, infoStream);
+    
+    if (fieldNormStatus.error != null || 
+      termIndexStatus.error != null ||
+      storedFieldStatus.error != null ||
+      termVectorStatus.error != null ||
+      docValuesStatus.error != null) {
+      System.out.println("CheckReader failed");
+      System.out.println(bos.toString("UTF-8"));
+      throw new RuntimeException("CheckReader failed");
+    } else {
+      if (LuceneTestCase.INFOSTREAM) {
+        System.out.println(bos.toString("UTF-8"));
+      }
+    }
+  }
+
   // NOTE: only works for TMP and LMP!!
   public static void setUseCompoundFile(MergePolicy mp, boolean v) {
     if (mp instanceof TieredMergePolicy) {
       ((TieredMergePolicy) mp).setUseCompoundFile(v);
     } else if (mp instanceof LogMergePolicy) {
       ((LogMergePolicy) mp).setUseCompoundFile(v);
+    } else {
+      throw new IllegalArgumentException("cannot set compound file for MergePolicy " + mp);
     }
   }
 
@@ -676,6 +708,24 @@
       }
     };
   }
+  
+  /** Return a Codec that can read any of the
+   *  default codecs and formats, but always writes in the specified
+   *  format. */
+  public static Codec alwaysDocValuesFormat(final DocValuesFormat format) {
+    // TODO: we really need for docvalues impls etc to announce themselves
+    // (and maybe their params, too) to infostream on flush and merge.
+    // otherwise in a real debugging situation we won't know whats going on!
+    if (LuceneTestCase.VERBOSE) {
+      System.out.println("forcing docvalues format to:" + format);
+    }
+    return new Lucene42Codec() {
+      @Override
+      public DocValuesFormat getDocValuesFormatForField(String field) {
+        return format;
+      }
+    };
+  }
 
   // TODO: generalize all 'test-checks-for-crazy-codecs' to
   // annotations (LUCENE-3489)
@@ -825,50 +875,20 @@
     for(IndexableField f : doc1.getFields()) {
       final Field field1 = (Field) f;
       final Field field2;
-      final DocValues.Type dvType = field1.fieldType().docValueType();
+      final DocValuesType dvType = field1.fieldType().docValueType();
       if (dvType != null) {
         switch(dvType) {
-        case VAR_INTS:
-          field2 = new PackedLongDocValuesField(field1.name(), field1.numericValue().longValue());
+          case NUMERIC:
+            field2 = new NumericDocValuesField(field1.name(), field1.numericValue().longValue());
+            break;
+          case BINARY:
+            field2 = new BinaryDocValuesField(field1.name(), field1.binaryValue());
           break;
-        case FIXED_INTS_8:
-          field2 = new ByteDocValuesField(field1.name(), field1.numericValue().byteValue());
-          break;
-        case FIXED_INTS_16:
-          field2 = new ShortDocValuesField(field1.name(), field1.numericValue().shortValue());
-          break;
-        case FIXED_INTS_32:
-          field2 = new IntDocValuesField(field1.name(), field1.numericValue().intValue());
-          break;
-        case FIXED_INTS_64:
-          field2 = new LongDocValuesField(field1.name(), field1.numericValue().longValue());
-          break;
-        case FLOAT_32:
-          field2 = new FloatDocValuesField(field1.name(), field1.numericValue().floatValue());
-          break;
-        case FLOAT_64:
-          field2 = new DoubleDocValuesField(field1.name(), field1.numericValue().doubleValue());
-          break;
-        case BYTES_FIXED_STRAIGHT:
-          field2 = new StraightBytesDocValuesField(field1.name(), field1.binaryValue(), true);
-          break;
-        case BYTES_VAR_STRAIGHT:
-          field2 = new StraightBytesDocValuesField(field1.name(), field1.binaryValue(), false);
-          break;
-        case BYTES_FIXED_DEREF:
-          field2 = new DerefBytesDocValuesField(field1.name(), field1.binaryValue(), true);
-          break;
-        case BYTES_VAR_DEREF:
-          field2 = new DerefBytesDocValuesField(field1.name(), field1.binaryValue(), false);
-          break;
-        case BYTES_FIXED_SORTED:
-          field2 = new SortedBytesDocValuesField(field1.name(), field1.binaryValue(), true);
-          break;
-        case BYTES_VAR_SORTED:
-          field2 = new SortedBytesDocValuesField(field1.name(), field1.binaryValue(), false);
-          break;
-        default:
-          throw new IllegalStateException("unknown Type: " + dvType);
+          case SORTED:
+            field2 = new SortedDocValuesField(field1.name(), field1.binaryValue());
+            break;
+          default:
+            throw new IllegalStateException("unknown Type: " + dvType);
         }
       } else {
         field2 = new Field(field1.name(), field1.stringValue(), field1.fieldType());
Index: lucene/test-framework/src/java/org/apache/lucene/util/RunListenerPrintReproduceInfo.java
===================================================================
--- lucene/test-framework/src/java/org/apache/lucene/util/RunListenerPrintReproduceInfo.java	(revision 1442822)
+++ lucene/test-framework/src/java/org/apache/lucene/util/RunListenerPrintReproduceInfo.java	(working copy)
@@ -150,6 +150,7 @@
     // Codec, postings, directories.
     if (!TEST_CODEC.equals("random")) addVmOpt(b, "tests.codec", TEST_CODEC);
     if (!TEST_POSTINGSFORMAT.equals("random")) addVmOpt(b, "tests.postingsformat", TEST_POSTINGSFORMAT);
+    if (!TEST_DOCVALUESFORMAT.equals("random")) addVmOpt(b, "tests.docvaluesformat", TEST_DOCVALUESFORMAT);
     if (!TEST_DIRECTORY.equals("random")) addVmOpt(b, "tests.directory", TEST_DIRECTORY);
 
     // Environment.
Index: lucene/test-framework/src/java/org/apache/lucene/util/LineFileDocs.java
===================================================================
--- lucene/test-framework/src/java/org/apache/lucene/util/LineFileDocs.java	(revision 1442822)
+++ lucene/test-framework/src/java/org/apache/lucene/util/LineFileDocs.java	(working copy)
@@ -36,7 +36,7 @@
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.FieldType;
-import org.apache.lucene.document.SortedBytesDocValuesField;
+import org.apache.lucene.document.SortedDocValuesField;
 import org.apache.lucene.document.StringField;
 import org.apache.lucene.document.TextField;
 
@@ -184,7 +184,7 @@
       doc.add(date);
 
       if (useDocValues) {
-        titleDV = new SortedBytesDocValuesField("titleDV", new BytesRef());
+        titleDV = new SortedDocValuesField("titleDV", new BytesRef());
         doc.add(titleDV);
       } else {
         titleDV = null;
Index: lucene/test-framework/src/java/org/apache/lucene/codecs/mocksep/MockSepDocValuesFormat.java
===================================================================
--- lucene/test-framework/src/java/org/apache/lucene/codecs/mocksep/MockSepDocValuesFormat.java	(revision 1442822)
+++ lucene/test-framework/src/java/org/apache/lucene/codecs/mocksep/MockSepDocValuesFormat.java	(working copy)
@@ -1,46 +0,0 @@
-package org.apache.lucene.codecs.mocksep;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.codecs.DocValuesFormat;
-import org.apache.lucene.codecs.PerDocConsumer;
-import org.apache.lucene.codecs.PerDocProducer;
-import org.apache.lucene.codecs.sep.SepDocValuesConsumer;
-import org.apache.lucene.codecs.sep.SepDocValuesProducer;
-import org.apache.lucene.index.PerDocWriteState;
-import org.apache.lucene.index.SegmentReadState;
-
-/**
- * Separate-file docvalues implementation
- * @lucene.experimental
- */
-// TODO: we could move this out of src/test ?
-public class MockSepDocValuesFormat extends DocValuesFormat {
-
-  @Override
-  public PerDocConsumer docsConsumer(PerDocWriteState state) throws IOException {
-    return new SepDocValuesConsumer(state);
-  }
-
-  @Override
-  public PerDocProducer docsProducer(SegmentReadState state) throws IOException {
-    return new SepDocValuesProducer(state);
-  }
-}
Index: lucene/test-framework/src/java/org/apache/lucene/codecs/mocksep/MockSepPostingsFormat.java
===================================================================
--- lucene/test-framework/src/java/org/apache/lucene/codecs/mocksep/MockSepPostingsFormat.java	(revision 1442822)
+++ lucene/test-framework/src/java/org/apache/lucene/codecs/mocksep/MockSepPostingsFormat.java	(working copy)
@@ -83,13 +83,13 @@
   @Override
   public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {
 
-    PostingsReaderBase postingsReader = new SepPostingsReader(state.dir, state.fieldInfos, state.segmentInfo,
+    PostingsReaderBase postingsReader = new SepPostingsReader(state.directory, state.fieldInfos, state.segmentInfo,
         state.context, new MockSingleIntFactory(), state.segmentSuffix);
 
     TermsIndexReaderBase indexReader;
     boolean success = false;
     try {
-      indexReader = new FixedGapTermsIndexReader(state.dir,
+      indexReader = new FixedGapTermsIndexReader(state.directory,
                                                        state.fieldInfos,
                                                        state.segmentInfo.name,
                                                        state.termsIndexDivisor,
@@ -105,7 +105,7 @@
     success = false;
     try {
       FieldsProducer ret = new BlockTermsReader(indexReader,
-                                                state.dir,
+                                                state.directory,
                                                 state.fieldInfos,
                                                 state.segmentInfo,
                                                 postingsReader,
Index: lucene/test-framework/src/java/org/apache/lucene/codecs/lucene41ords/Lucene41WithOrds.java
===================================================================
--- lucene/test-framework/src/java/org/apache/lucene/codecs/lucene41ords/Lucene41WithOrds.java	(revision 1442822)
+++ lucene/test-framework/src/java/org/apache/lucene/codecs/lucene41ords/Lucene41WithOrds.java	(working copy)
@@ -91,12 +91,12 @@
 
   @Override
   public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {
-    PostingsReaderBase postings = new Lucene41PostingsReader(state.dir, state.fieldInfos, state.segmentInfo, state.context, state.segmentSuffix);
+    PostingsReaderBase postings = new Lucene41PostingsReader(state.directory, state.fieldInfos, state.segmentInfo, state.context, state.segmentSuffix);
     TermsIndexReaderBase indexReader;
 
     boolean success = false;
     try {
-      indexReader = new FixedGapTermsIndexReader(state.dir,
+      indexReader = new FixedGapTermsIndexReader(state.directory,
                                                  state.fieldInfos,
                                                  state.segmentInfo.name,
                                                  state.termsIndexDivisor,
@@ -112,7 +112,7 @@
     success = false;
     try {
       FieldsProducer ret = new BlockTermsReader(indexReader,
-                                                state.dir,
+                                                state.directory,
                                                 state.fieldInfos,
                                                 state.segmentInfo,
                                                 postings,
Index: lucene/test-framework/src/java/org/apache/lucene/codecs/mockintblock/MockFixedIntBlockPostingsFormat.java
===================================================================
--- lucene/test-framework/src/java/org/apache/lucene/codecs/mockintblock/MockFixedIntBlockPostingsFormat.java	(revision 1442822)
+++ lucene/test-framework/src/java/org/apache/lucene/codecs/mockintblock/MockFixedIntBlockPostingsFormat.java	(working copy)
@@ -157,7 +157,7 @@
 
   @Override
   public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {
-    PostingsReaderBase postingsReader = new SepPostingsReader(state.dir,
+    PostingsReaderBase postingsReader = new SepPostingsReader(state.directory,
                                                               state.fieldInfos,
                                                               state.segmentInfo,
                                                               state.context,
@@ -166,7 +166,7 @@
     TermsIndexReaderBase indexReader;
     boolean success = false;
     try {
-      indexReader = new FixedGapTermsIndexReader(state.dir,
+      indexReader = new FixedGapTermsIndexReader(state.directory,
                                                        state.fieldInfos,
                                                        state.segmentInfo.name,
                                                        state.termsIndexDivisor,
@@ -182,7 +182,7 @@
     success = false;
     try {
       FieldsProducer ret = new BlockTermsReader(indexReader,
-                                                state.dir,
+                                                state.directory,
                                                 state.fieldInfos,
                                                 state.segmentInfo,
                                                 postingsReader,
Index: lucene/test-framework/src/java/org/apache/lucene/codecs/mockintblock/MockVariableIntBlockPostingsFormat.java
===================================================================
--- lucene/test-framework/src/java/org/apache/lucene/codecs/mockintblock/MockVariableIntBlockPostingsFormat.java	(revision 1442822)
+++ lucene/test-framework/src/java/org/apache/lucene/codecs/mockintblock/MockVariableIntBlockPostingsFormat.java	(working copy)
@@ -182,7 +182,7 @@
 
   @Override
   public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {
-    PostingsReaderBase postingsReader = new SepPostingsReader(state.dir,
+    PostingsReaderBase postingsReader = new SepPostingsReader(state.directory,
                                                               state.fieldInfos,
                                                               state.segmentInfo,
                                                               state.context,
@@ -191,7 +191,7 @@
     TermsIndexReaderBase indexReader;
     boolean success = false;
     try {
-      indexReader = new FixedGapTermsIndexReader(state.dir,
+      indexReader = new FixedGapTermsIndexReader(state.directory,
                                                        state.fieldInfos,
                                                        state.segmentInfo.name,
                                                        state.termsIndexDivisor,
@@ -207,7 +207,7 @@
     success = false;
     try {
       FieldsProducer ret = new BlockTermsReader(indexReader,
-                                                state.dir,
+                                                state.directory,
                                                 state.fieldInfos,
                                                 state.segmentInfo,
                                                 postingsReader,
Index: lucene/test-framework/src/java/org/apache/lucene/codecs/lucene40/Lucene40DocValuesWriter.java
===================================================================
--- lucene/test-framework/src/java/org/apache/lucene/codecs/lucene40/Lucene40DocValuesWriter.java	(revision 0)
+++ lucene/test-framework/src/java/org/apache/lucene/codecs/lucene40/Lucene40DocValuesWriter.java	(working copy)
@@ -0,0 +1,500 @@
+package org.apache.lucene.codecs.lucene40;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.TreeSet;
+
+import org.apache.lucene.codecs.CodecUtil;
+import org.apache.lucene.codecs.DocValuesConsumer;
+import org.apache.lucene.codecs.lucene40.Lucene40FieldInfosReader.LegacyDocValuesType;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.SegmentWriteState;
+import org.apache.lucene.store.CompoundFileDirectory;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util.packed.PackedInts;
+
+class Lucene40DocValuesWriter extends DocValuesConsumer {
+  private final Directory dir;
+  private final SegmentWriteState state;
+  private final String legacyKey;
+  private final static String segmentSuffix = "dv";
+
+  // note: intentionally ignores seg suffix
+  Lucene40DocValuesWriter(SegmentWriteState state, String filename, String legacyKey) throws IOException {
+    this.state = state;
+    this.legacyKey = legacyKey;
+    this.dir = new CompoundFileDirectory(state.directory, filename, state.context, true);
+  }
+  
+  @Override
+  public void addNumericField(FieldInfo field, Iterable<Number> values) throws IOException {
+    // examine the values to determine best type to use
+    long minValue = Long.MAX_VALUE;
+    long maxValue = Long.MIN_VALUE;
+    for (Number n : values) {
+      long v = n.longValue();
+      minValue = Math.min(minValue, v);
+      maxValue = Math.max(maxValue, v);
+    }
+    
+    String fileName = IndexFileNames.segmentFileName(state.segmentInfo.name + "_" + Integer.toString(field.number), segmentSuffix, "dat");
+    IndexOutput data = dir.createOutput(fileName, state.context);
+    boolean success = false;
+    try {
+      if (minValue >= Byte.MIN_VALUE && maxValue <= Byte.MAX_VALUE && PackedInts.bitsRequired(maxValue-minValue) > 4) {
+        // fits in a byte[], would be more than 4bpv, just write byte[]
+        addBytesField(field, data, values);
+      } else if (minValue >= Short.MIN_VALUE && maxValue <= Short.MAX_VALUE && PackedInts.bitsRequired(maxValue-minValue) > 8) {
+        // fits in a short[], would be more than 8bpv, just write short[]
+        addShortsField(field, data, values);
+      } else if (minValue >= Integer.MIN_VALUE && maxValue <= Integer.MAX_VALUE && PackedInts.bitsRequired(maxValue-minValue) > 16) {
+        // fits in a int[], would be more than 16bpv, just write int[]
+        addIntsField(field, data, values);
+      } else {
+        addVarIntsField(field, data, values, minValue, maxValue);
+      }
+      success = true;
+    } finally {
+      if (success) {
+        IOUtils.close(data);
+      } else {
+        IOUtils.closeWhileHandlingException(data);
+      }
+    }
+  }
+
+  private void addBytesField(FieldInfo field, IndexOutput output, Iterable<Number> values) throws IOException {
+    field.putAttribute(legacyKey, LegacyDocValuesType.FIXED_INTS_8.name());
+    CodecUtil.writeHeader(output, 
+                          Lucene40DocValuesFormat.INTS_CODEC_NAME, 
+                          Lucene40DocValuesFormat.INTS_VERSION_CURRENT);
+    output.writeInt(1); // size
+    for (Number n : values) {
+      output.writeByte(n.byteValue());
+    }
+  }
+  
+  private void addShortsField(FieldInfo field, IndexOutput output, Iterable<Number> values) throws IOException {
+    field.putAttribute(legacyKey, LegacyDocValuesType.FIXED_INTS_16.name());
+    CodecUtil.writeHeader(output, 
+                          Lucene40DocValuesFormat.INTS_CODEC_NAME, 
+                          Lucene40DocValuesFormat.INTS_VERSION_CURRENT);
+    output.writeInt(2); // size
+    for (Number n : values) {
+      output.writeShort(n.shortValue());
+    }
+  }
+  
+  private void addIntsField(FieldInfo field, IndexOutput output, Iterable<Number> values) throws IOException {
+    field.putAttribute(legacyKey, LegacyDocValuesType.FIXED_INTS_32.name());
+    CodecUtil.writeHeader(output, 
+                          Lucene40DocValuesFormat.INTS_CODEC_NAME, 
+                          Lucene40DocValuesFormat.INTS_VERSION_CURRENT);
+    output.writeInt(4); // size
+    for (Number n : values) {
+      output.writeInt(n.intValue());
+    }
+  }
+  
+  private void addVarIntsField(FieldInfo field, IndexOutput output, Iterable<Number> values, long minValue, long maxValue) throws IOException {
+    field.putAttribute(legacyKey, LegacyDocValuesType.VAR_INTS.name());
+    
+    CodecUtil.writeHeader(output, 
+                          Lucene40DocValuesFormat.VAR_INTS_CODEC_NAME, 
+                          Lucene40DocValuesFormat.VAR_INTS_VERSION_CURRENT);
+    
+    final long delta = maxValue - minValue;
+    
+    if (delta < 0) {
+      // writes longs
+      output.writeByte(Lucene40DocValuesFormat.VAR_INTS_FIXED_64);
+      for (Number n : values) {
+        output.writeLong(n.longValue());
+      }
+    } else {
+      // writes packed ints
+      output.writeByte(Lucene40DocValuesFormat.VAR_INTS_PACKED);
+      output.writeLong(minValue);
+      output.writeLong(0 - minValue); // default value (representation of 0)
+      PackedInts.Writer writer = PackedInts.getWriter(output, 
+                                                      state.segmentInfo.getDocCount(),
+                                                      PackedInts.bitsRequired(delta), 
+                                                      PackedInts.DEFAULT);
+      for (Number n : values) {
+        writer.add(n.longValue() - minValue);
+      }
+      writer.finish();
+    }
+  }
+
+  @Override
+  public void addBinaryField(FieldInfo field, Iterable<BytesRef> values) throws IOException {
+    // examine the values to determine best type to use
+    HashSet<BytesRef> uniqueValues = new HashSet<BytesRef>();
+    int minLength = Integer.MAX_VALUE;
+    int maxLength = Integer.MIN_VALUE;
+    for (BytesRef b : values) {
+      minLength = Math.min(minLength, b.length);
+      maxLength = Math.max(maxLength, b.length);
+      if (uniqueValues != null) {
+        if (uniqueValues.add(BytesRef.deepCopyOf(b))) {
+          if (uniqueValues.size() > 256) {
+            uniqueValues = null;
+          }
+        }
+      }
+    }
+    
+    int maxDoc = state.segmentInfo.getDocCount();
+    final boolean fixed = minLength == maxLength;
+    final boolean dedup = uniqueValues != null && uniqueValues.size() * 2 < maxDoc;
+    
+    if (dedup) {
+      // we will deduplicate and deref values
+      boolean success = false;
+      IndexOutput data = null;
+      IndexOutput index = null;
+      String dataName = IndexFileNames.segmentFileName(state.segmentInfo.name + "_" + Integer.toString(field.number), segmentSuffix, "dat");
+      String indexName = IndexFileNames.segmentFileName(state.segmentInfo.name + "_" + Integer.toString(field.number), segmentSuffix, "idx");
+      try {
+        data = dir.createOutput(dataName, state.context);
+        index = dir.createOutput(indexName, state.context);
+        if (fixed) {
+          addFixedDerefBytesField(field, data, index, values, minLength);
+        } else {
+          addVarDerefBytesField(field, data, index, values);
+        }
+        success = true;
+      } finally {
+        if (success) {
+          IOUtils.close(data, index);
+        } else {
+          IOUtils.closeWhileHandlingException(data, index);
+        }
+      }
+    } else {
+      // we dont deduplicate, just write values straight
+      if (fixed) {
+        // fixed byte[]
+        String fileName = IndexFileNames.segmentFileName(state.segmentInfo.name + "_" + Integer.toString(field.number), segmentSuffix, "dat");
+        IndexOutput data = dir.createOutput(fileName, state.context);
+        boolean success = false;
+        try {
+          addFixedStraightBytesField(field, data, values, minLength);
+          success = true;
+        } finally {
+          if (success) {
+            IOUtils.close(data);
+          } else {
+            IOUtils.closeWhileHandlingException(data);
+          }
+        }
+      } else {
+        // variable byte[]
+        boolean success = false;
+        IndexOutput data = null;
+        IndexOutput index = null;
+        String dataName = IndexFileNames.segmentFileName(state.segmentInfo.name + "_" + Integer.toString(field.number), segmentSuffix, "dat");
+        String indexName = IndexFileNames.segmentFileName(state.segmentInfo.name + "_" + Integer.toString(field.number), segmentSuffix, "idx");
+        try {
+          data = dir.createOutput(dataName, state.context);
+          index = dir.createOutput(indexName, state.context);
+          addVarStraightBytesField(field, data, index, values);
+          success = true;
+        } finally {
+          if (success) {
+            IOUtils.close(data, index);
+          } else {
+            IOUtils.closeWhileHandlingException(data, index);
+          }
+        }
+      }
+    }
+  }
+  
+  private void addFixedStraightBytesField(FieldInfo field, IndexOutput output, Iterable<BytesRef> values, int length) throws IOException {
+    field.putAttribute(legacyKey, LegacyDocValuesType.BYTES_FIXED_STRAIGHT.name());
+
+    CodecUtil.writeHeader(output, 
+                          Lucene40DocValuesFormat.BYTES_FIXED_STRAIGHT_CODEC_NAME,
+                          Lucene40DocValuesFormat.BYTES_FIXED_STRAIGHT_VERSION_CURRENT);
+    
+    output.writeInt(length);
+    for (BytesRef v : values) {
+      output.writeBytes(v.bytes, v.offset, v.length);
+    }
+  }
+  
+  // NOTE: 4.0 file format docs are crazy/wrong here...
+  private void addVarStraightBytesField(FieldInfo field, IndexOutput data, IndexOutput index, Iterable<BytesRef> values) throws IOException {
+    field.putAttribute(legacyKey, LegacyDocValuesType.BYTES_VAR_STRAIGHT.name());
+    
+    CodecUtil.writeHeader(data, 
+                          Lucene40DocValuesFormat.BYTES_VAR_STRAIGHT_CODEC_NAME_DAT,
+                          Lucene40DocValuesFormat.BYTES_VAR_STRAIGHT_VERSION_CURRENT);
+    
+    CodecUtil.writeHeader(index, 
+                          Lucene40DocValuesFormat.BYTES_VAR_STRAIGHT_CODEC_NAME_IDX,
+                          Lucene40DocValuesFormat.BYTES_VAR_STRAIGHT_VERSION_CURRENT);
+    
+    /* values */
+    
+    final long startPos = data.getFilePointer();
+    
+    for (BytesRef v : values) {
+      data.writeBytes(v.bytes, v.offset, v.length);
+    }
+    
+    /* addresses */
+    
+    final long maxAddress = data.getFilePointer() - startPos;
+    index.writeVLong(maxAddress);
+    
+    final int maxDoc = state.segmentInfo.getDocCount();
+    assert maxDoc != Integer.MAX_VALUE; // unsupported by the 4.0 impl
+    
+    final PackedInts.Writer w = PackedInts.getWriter(index, maxDoc+1, PackedInts.bitsRequired(maxAddress), PackedInts.DEFAULT);
+    long currentPosition = 0;
+    for (BytesRef v : values) {
+      w.add(currentPosition);
+      currentPosition += v.length;
+    }
+    // write sentinel
+    assert currentPosition == maxAddress;
+    w.add(currentPosition);
+    w.finish();
+  }
+  
+  private void addFixedDerefBytesField(FieldInfo field, IndexOutput data, IndexOutput index, Iterable<BytesRef> values, int length) throws IOException {
+    field.putAttribute(legacyKey, LegacyDocValuesType.BYTES_FIXED_DEREF.name());
+
+    CodecUtil.writeHeader(data, 
+                          Lucene40DocValuesFormat.BYTES_FIXED_DEREF_CODEC_NAME_DAT,
+                          Lucene40DocValuesFormat.BYTES_FIXED_DEREF_VERSION_CURRENT);
+    
+    CodecUtil.writeHeader(index, 
+                          Lucene40DocValuesFormat.BYTES_FIXED_DEREF_CODEC_NAME_IDX,
+                          Lucene40DocValuesFormat.BYTES_FIXED_DEREF_VERSION_CURRENT);
+    
+    // deduplicate
+    TreeSet<BytesRef> dictionary = new TreeSet<BytesRef>();
+    for (BytesRef v : values) {
+      dictionary.add(BytesRef.deepCopyOf(v));
+    }
+    
+    /* values */
+    data.writeInt(length);
+    for (BytesRef v : dictionary) {
+      data.writeBytes(v.bytes, v.offset, v.length);
+    }
+    
+    /* ordinals */
+    int valueCount = dictionary.size();
+    assert valueCount > 0;
+    index.writeInt(valueCount);
+    final int maxDoc = state.segmentInfo.getDocCount();
+    final PackedInts.Writer w = PackedInts.getWriter(index, maxDoc, PackedInts.bitsRequired(valueCount-1), PackedInts.DEFAULT);
+
+    for (BytesRef v : values) {
+      int ord = dictionary.headSet(v).size();
+      w.add(ord);
+    }
+    w.finish();
+  }
+  
+  private void addVarDerefBytesField(FieldInfo field, IndexOutput data, IndexOutput index, Iterable<BytesRef> values) throws IOException {
+    field.putAttribute(legacyKey, LegacyDocValuesType.BYTES_VAR_DEREF.name());
+
+    CodecUtil.writeHeader(data, 
+                          Lucene40DocValuesFormat.BYTES_VAR_DEREF_CODEC_NAME_DAT,
+                          Lucene40DocValuesFormat.BYTES_VAR_DEREF_VERSION_CURRENT);
+    
+    CodecUtil.writeHeader(index, 
+                          Lucene40DocValuesFormat.BYTES_VAR_DEREF_CODEC_NAME_IDX,
+                          Lucene40DocValuesFormat.BYTES_VAR_DEREF_VERSION_CURRENT);
+    
+    // deduplicate
+    TreeSet<BytesRef> dictionary = new TreeSet<BytesRef>();
+    for (BytesRef v : values) {
+      dictionary.add(BytesRef.deepCopyOf(v));
+    }
+    
+    /* values */
+    long startPosition = data.getFilePointer();
+    long currentAddress = 0;
+    HashMap<BytesRef,Long> valueToAddress = new HashMap<BytesRef,Long>();
+    for (BytesRef v : dictionary) {
+      currentAddress = data.getFilePointer() - startPosition;
+      valueToAddress.put(v, currentAddress);
+      writeVShort(data, v.length);
+      data.writeBytes(v.bytes, v.offset, v.length);
+    }
+    
+    /* ordinals */
+    long totalBytes = data.getFilePointer() - startPosition;
+    index.writeLong(totalBytes);
+    final int maxDoc = state.segmentInfo.getDocCount();
+    final PackedInts.Writer w = PackedInts.getWriter(index, maxDoc, PackedInts.bitsRequired(currentAddress), PackedInts.DEFAULT);
+
+    for (BytesRef v : values) {
+      w.add(valueToAddress.get(v));
+    }
+    w.finish();
+  }
+  
+  // the little vint encoding used for var-deref
+  private static void writeVShort(IndexOutput o, int i) throws IOException {
+    assert i >= 0 && i <= Short.MAX_VALUE;
+    if (i < 128) {
+      o.writeByte((byte)i);
+    } else {
+      o.writeByte((byte) (0x80 | (i >> 8)));
+      o.writeByte((byte) (i & 0xff));
+    }
+  }
+
+  @Override
+  public void addSortedField(FieldInfo field, Iterable<BytesRef> values, Iterable<Number> docToOrd) throws IOException {
+    // examine the values to determine best type to use
+    int minLength = Integer.MAX_VALUE;
+    int maxLength = Integer.MIN_VALUE;
+    for (BytesRef b : values) {
+      minLength = Math.min(minLength, b.length);
+      maxLength = Math.max(maxLength, b.length);
+    }
+    
+    boolean success = false;
+    IndexOutput data = null;
+    IndexOutput index = null;
+    String dataName = IndexFileNames.segmentFileName(state.segmentInfo.name + "_" + Integer.toString(field.number), segmentSuffix, "dat");
+    String indexName = IndexFileNames.segmentFileName(state.segmentInfo.name + "_" + Integer.toString(field.number), segmentSuffix, "idx");
+    
+    try {
+      data = dir.createOutput(dataName, state.context);
+      index = dir.createOutput(indexName, state.context);
+      if (minLength == maxLength) {
+        // fixed byte[]
+        addFixedSortedBytesField(field, data, index, values, docToOrd, minLength);
+      } else {
+        // var byte[]
+        addVarSortedBytesField(field, data, index, values, docToOrd);
+      }
+      success = true;
+    } finally {
+      if (success) {
+        IOUtils.close(data, index);
+      } else {
+        IOUtils.closeWhileHandlingException(data, index);
+      }
+    }
+  }
+  
+  private void addFixedSortedBytesField(FieldInfo field, IndexOutput data, IndexOutput index, Iterable<BytesRef> values, Iterable<Number> docToOrd, int length) throws IOException {
+    field.putAttribute(legacyKey, LegacyDocValuesType.BYTES_FIXED_SORTED.name());
+
+    CodecUtil.writeHeader(data, 
+                          Lucene40DocValuesFormat.BYTES_FIXED_SORTED_CODEC_NAME_DAT,
+                          Lucene40DocValuesFormat.BYTES_FIXED_SORTED_VERSION_CURRENT);
+    
+    CodecUtil.writeHeader(index, 
+                          Lucene40DocValuesFormat.BYTES_FIXED_SORTED_CODEC_NAME_IDX,
+                          Lucene40DocValuesFormat.BYTES_FIXED_SORTED_VERSION_CURRENT);
+    
+    /* values */
+    
+    data.writeInt(length);
+    int valueCount = 0;
+    for (BytesRef v : values) {
+      data.writeBytes(v.bytes, v.offset, v.length);
+      valueCount++;
+    }
+    
+    /* ordinals */
+    
+    index.writeInt(valueCount);
+    int maxDoc = state.segmentInfo.getDocCount();
+    assert valueCount > 0;
+    final PackedInts.Writer w = PackedInts.getWriter(index, maxDoc, PackedInts.bitsRequired(valueCount-1), PackedInts.DEFAULT);
+    for (Number n : docToOrd) {
+      w.add(n.longValue());
+    }
+    w.finish();
+  }
+  
+  private void addVarSortedBytesField(FieldInfo field, IndexOutput data, IndexOutput index, Iterable<BytesRef> values, Iterable<Number> docToOrd) throws IOException {
+    field.putAttribute(legacyKey, LegacyDocValuesType.BYTES_VAR_SORTED.name());
+    
+    CodecUtil.writeHeader(data, 
+                          Lucene40DocValuesFormat.BYTES_VAR_SORTED_CODEC_NAME_DAT,
+                          Lucene40DocValuesFormat.BYTES_VAR_SORTED_VERSION_CURRENT);
+
+    CodecUtil.writeHeader(index, 
+                          Lucene40DocValuesFormat.BYTES_VAR_SORTED_CODEC_NAME_IDX,
+                          Lucene40DocValuesFormat.BYTES_VAR_SORTED_VERSION_CURRENT);
+
+    /* values */
+    
+    final long startPos = data.getFilePointer();
+    
+    int valueCount = 0;
+    for (BytesRef v : values) {
+      data.writeBytes(v.bytes, v.offset, v.length);
+      valueCount++;
+    }
+    
+    /* addresses */
+    
+    final long maxAddress = data.getFilePointer() - startPos;
+    index.writeLong(maxAddress);
+    
+    assert valueCount != Integer.MAX_VALUE; // unsupported by the 4.0 impl
+    
+    final PackedInts.Writer w = PackedInts.getWriter(index, valueCount+1, PackedInts.bitsRequired(maxAddress), PackedInts.DEFAULT);
+    long currentPosition = 0;
+    for (BytesRef v : values) {
+      w.add(currentPosition);
+      currentPosition += v.length;
+    }
+    // write sentinel
+    assert currentPosition == maxAddress;
+    w.add(currentPosition);
+    w.finish();
+    
+    /* ordinals */
+    
+    final int maxDoc = state.segmentInfo.getDocCount();
+    assert valueCount > 0;
+    final PackedInts.Writer ords = PackedInts.getWriter(index, maxDoc, PackedInts.bitsRequired(valueCount-1), PackedInts.DEFAULT);
+    for (Number n : docToOrd) {
+      ords.add(n.longValue());
+    }
+    ords.finish();
+  }
+  
+  @Override
+  public void close() throws IOException {
+    dir.close();
+  }
+}

Property changes on: lucene/test-framework/src/java/org/apache/lucene/codecs/lucene40/Lucene40DocValuesWriter.java
___________________________________________________________________
Added: svn:eol-style
## -0,0 +1 ##
+native
Index: lucene/test-framework/src/java/org/apache/lucene/codecs/lucene40/Lucene40RWCodec.java
===================================================================
--- lucene/test-framework/src/java/org/apache/lucene/codecs/lucene40/Lucene40RWCodec.java	(revision 0)
+++ lucene/test-framework/src/java/org/apache/lucene/codecs/lucene40/Lucene40RWCodec.java	(working copy)
@@ -0,0 +1,53 @@
+package org.apache.lucene.codecs.lucene40;
+
+import java.io.IOException;
+
+import org.apache.lucene.codecs.DocValuesFormat;
+import org.apache.lucene.codecs.FieldInfosFormat;
+import org.apache.lucene.codecs.FieldInfosWriter;
+import org.apache.lucene.codecs.NormsFormat;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/** Read-write version of Lucene40Codec for testing */
+public final class Lucene40RWCodec extends Lucene40Codec {
+  private final FieldInfosFormat fieldInfos = new Lucene40FieldInfosFormat() {
+    @Override
+    public FieldInfosWriter getFieldInfosWriter() throws IOException {
+      return new Lucene40FieldInfosWriter();
+    }
+  };
+  
+  private final DocValuesFormat docValues = new Lucene40RWDocValuesFormat();
+  private final NormsFormat norms = new Lucene40RWNormsFormat();
+  
+  @Override
+  public FieldInfosFormat fieldInfosFormat() {
+    return fieldInfos;
+  }
+
+  @Override
+  public DocValuesFormat docValuesFormat() {
+    return docValues;
+  }
+
+  @Override
+  public NormsFormat normsFormat() {
+    return norms;
+  }
+}

Property changes on: lucene/test-framework/src/java/org/apache/lucene/codecs/lucene40/Lucene40RWCodec.java
___________________________________________________________________
Added: svn:eol-style
## -0,0 +1 ##
+native
Index: lucene/test-framework/src/java/org/apache/lucene/codecs/lucene40/Lucene40RWDocValuesFormat.java
===================================================================
--- lucene/test-framework/src/java/org/apache/lucene/codecs/lucene40/Lucene40RWDocValuesFormat.java	(revision 0)
+++ lucene/test-framework/src/java/org/apache/lucene/codecs/lucene40/Lucene40RWDocValuesFormat.java	(working copy)
@@ -0,0 +1,36 @@
+package org.apache.lucene.codecs.lucene40;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.codecs.DocValuesConsumer;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.SegmentWriteState;
+
+/** Read-write version of {@link Lucene40DocValuesFormat} for testing */
+public class Lucene40RWDocValuesFormat extends Lucene40DocValuesFormat {
+
+  @Override
+  public DocValuesConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
+    String filename = IndexFileNames.segmentFileName(state.segmentInfo.name, 
+                                                     "dv", 
+                                                     IndexFileNames.COMPOUND_FILE_EXTENSION);
+    return new Lucene40DocValuesWriter(state, filename, Lucene40FieldInfosReader.LEGACY_DV_TYPE_KEY);
+  }
+}

Property changes on: lucene/test-framework/src/java/org/apache/lucene/codecs/lucene40/Lucene40RWDocValuesFormat.java
___________________________________________________________________
Added: svn:eol-style
## -0,0 +1 ##
+native
Index: lucene/test-framework/src/java/org/apache/lucene/codecs/lucene40/Lucene40FieldInfosWriter.java
===================================================================
--- lucene/test-framework/src/java/org/apache/lucene/codecs/lucene40/Lucene40FieldInfosWriter.java	(revision 0)
+++ lucene/test-framework/src/java/org/apache/lucene/codecs/lucene40/Lucene40FieldInfosWriter.java	(working copy)
@@ -0,0 +1,103 @@
+package org.apache.lucene.codecs.lucene40;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+import java.io.IOException;
+
+import org.apache.lucene.codecs.CodecUtil;
+import org.apache.lucene.codecs.FieldInfosWriter;
+import org.apache.lucene.codecs.lucene40.Lucene40FieldInfosReader.LegacyDocValuesType;
+import org.apache.lucene.index.FieldInfo.DocValuesType;
+import org.apache.lucene.index.FieldInfo.IndexOptions;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.util.IOUtils;
+
+/**
+ * Lucene 4.0 FieldInfos writer.
+ * 
+ * @see Lucene40FieldInfosFormat
+ * @lucene.experimental
+ */
+public class Lucene40FieldInfosWriter extends FieldInfosWriter {
+
+  /** Sole constructor. */
+  public Lucene40FieldInfosWriter() {
+  }
+  
+  @Override
+  public void write(Directory directory, String segmentName, FieldInfos infos, IOContext context) throws IOException {
+    final String fileName = IndexFileNames.segmentFileName(segmentName, "", Lucene40FieldInfosFormat.FIELD_INFOS_EXTENSION);
+    IndexOutput output = directory.createOutput(fileName, context);
+    boolean success = false;
+    try {
+      CodecUtil.writeHeader(output, Lucene40FieldInfosFormat.CODEC_NAME, Lucene40FieldInfosFormat.FORMAT_CURRENT);
+      output.writeVInt(infos.size());
+      for (FieldInfo fi : infos) {
+        IndexOptions indexOptions = fi.getIndexOptions();
+        byte bits = 0x0;
+        if (fi.hasVectors()) bits |= Lucene40FieldInfosFormat.STORE_TERMVECTOR;
+        if (fi.omitsNorms()) bits |= Lucene40FieldInfosFormat.OMIT_NORMS;
+        if (fi.hasPayloads()) bits |= Lucene40FieldInfosFormat.STORE_PAYLOADS;
+        if (fi.isIndexed()) {
+          bits |= Lucene40FieldInfosFormat.IS_INDEXED;
+          assert indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 || !fi.hasPayloads();
+          if (indexOptions == IndexOptions.DOCS_ONLY) {
+            bits |= Lucene40FieldInfosFormat.OMIT_TERM_FREQ_AND_POSITIONS;
+          } else if (indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) {
+            bits |= Lucene40FieldInfosFormat.STORE_OFFSETS_IN_POSTINGS;
+          } else if (indexOptions == IndexOptions.DOCS_AND_FREQS) {
+            bits |= Lucene40FieldInfosFormat.OMIT_POSITIONS;
+          }
+        }
+        output.writeString(fi.name);
+        output.writeVInt(fi.number);
+        output.writeByte(bits);
+
+        // pack the DV types in one byte
+        final byte dv = docValuesByte(fi.getDocValuesType(), fi.getAttribute(Lucene40FieldInfosReader.LEGACY_DV_TYPE_KEY));
+        final byte nrm = docValuesByte(fi.getNormType(), fi.getAttribute(Lucene40FieldInfosReader.LEGACY_NORM_TYPE_KEY));
+        assert (dv & (~0xF)) == 0 && (nrm & (~0x0F)) == 0;
+        byte val = (byte) (0xff & ((nrm << 4) | dv));
+        output.writeByte(val);
+        output.writeStringStringMap(fi.attributes());
+      }
+      success = true;
+    } finally {
+      if (success) {
+        output.close();
+      } else {
+        IOUtils.closeWhileHandlingException(output);
+      }
+    }
+  }
+  
+  /** 4.0-style docvalues byte */
+  public byte docValuesByte(DocValuesType type, String legacyTypeAtt) {
+    if (type == null) {
+      assert legacyTypeAtt == null;
+      return 0;
+    } else {
+      assert legacyTypeAtt != null;
+      return (byte) LegacyDocValuesType.valueOf(legacyTypeAtt).ordinal();
+    }
+  }  
+}

Property changes on: lucene/test-framework/src/java/org/apache/lucene/codecs/lucene40/Lucene40FieldInfosWriter.java
___________________________________________________________________
Added: svn:eol-style
## -0,0 +1 ##
+native
Index: lucene/test-framework/src/java/org/apache/lucene/codecs/lucene40/Lucene40RWNormsFormat.java
===================================================================
--- lucene/test-framework/src/java/org/apache/lucene/codecs/lucene40/Lucene40RWNormsFormat.java	(revision 0)
+++ lucene/test-framework/src/java/org/apache/lucene/codecs/lucene40/Lucene40RWNormsFormat.java	(working copy)
@@ -0,0 +1,36 @@
+package org.apache.lucene.codecs.lucene40;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.codecs.DocValuesConsumer;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.SegmentWriteState;
+
+/** Read-write version of {@link Lucene40NormsFormat} for testing */
+public class Lucene40RWNormsFormat extends Lucene40NormsFormat {
+
+  @Override
+  public DocValuesConsumer normsConsumer(SegmentWriteState state) throws IOException {
+    String filename = IndexFileNames.segmentFileName(state.segmentInfo.name, 
+                                                     "nrm", 
+                                                     IndexFileNames.COMPOUND_FILE_EXTENSION);
+    return new Lucene40DocValuesWriter(state, filename, Lucene40FieldInfosReader.LEGACY_NORM_TYPE_KEY);
+  }
+}

Property changes on: lucene/test-framework/src/java/org/apache/lucene/codecs/lucene40/Lucene40RWNormsFormat.java
___________________________________________________________________
Added: svn:eol-style
## -0,0 +1 ##
+native
Index: lucene/test-framework/src/java/org/apache/lucene/codecs/lucene41/Lucene41RWCodec.java
===================================================================
--- lucene/test-framework/src/java/org/apache/lucene/codecs/lucene41/Lucene41RWCodec.java	(revision 1442822)
+++ lucene/test-framework/src/java/org/apache/lucene/codecs/lucene41/Lucene41RWCodec.java	(working copy)
@@ -1,6 +1,16 @@
 package org.apache.lucene.codecs.lucene41;
 
+import java.io.IOException;
+
+import org.apache.lucene.codecs.DocValuesFormat;
+import org.apache.lucene.codecs.FieldInfosFormat;
+import org.apache.lucene.codecs.FieldInfosWriter;
+import org.apache.lucene.codecs.NormsFormat;
 import org.apache.lucene.codecs.StoredFieldsFormat;
+import org.apache.lucene.codecs.lucene40.Lucene40FieldInfosFormat;
+import org.apache.lucene.codecs.lucene40.Lucene40FieldInfosWriter;
+import org.apache.lucene.codecs.lucene40.Lucene40RWDocValuesFormat;
+import org.apache.lucene.codecs.lucene40.Lucene40RWNormsFormat;
 
 /*
  * Licensed to the Apache Software Foundation (ASF) under one or more
@@ -24,9 +34,33 @@
  */
 public class Lucene41RWCodec extends Lucene41Codec {
   private final StoredFieldsFormat fieldsFormat = new Lucene41StoredFieldsFormat();
+  private final FieldInfosFormat fieldInfos = new Lucene40FieldInfosFormat() {
+    @Override
+    public FieldInfosWriter getFieldInfosWriter() throws IOException {
+      return new Lucene40FieldInfosWriter();
+    }
+  };
+  
+  private final DocValuesFormat docValues = new Lucene40RWDocValuesFormat();
+  private final NormsFormat norms = new Lucene40RWNormsFormat();
+  
+  @Override
+  public FieldInfosFormat fieldInfosFormat() {
+    return fieldInfos;
+  }
 
   @Override
   public StoredFieldsFormat storedFieldsFormat() {
     return fieldsFormat;
   }
+
+  @Override
+  public DocValuesFormat docValuesFormat() {
+    return docValues;
+  }
+
+  @Override
+  public NormsFormat normsFormat() {
+    return norms;
+  }
 }
Index: lucene/test-framework/src/java/org/apache/lucene/codecs/cheapbastard/CheapBastardNormsFormat.java
===================================================================
--- lucene/test-framework/src/java/org/apache/lucene/codecs/cheapbastard/CheapBastardNormsFormat.java	(revision 0)
+++ lucene/test-framework/src/java/org/apache/lucene/codecs/cheapbastard/CheapBastardNormsFormat.java	(working copy)
@@ -0,0 +1,46 @@
+package org.apache.lucene.codecs.cheapbastard;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.codecs.DocValuesConsumer;
+import org.apache.lucene.codecs.DocValuesProducer;
+import org.apache.lucene.codecs.NormsFormat;
+import org.apache.lucene.codecs.diskdv.DiskDocValuesConsumer;
+import org.apache.lucene.index.SegmentReadState;
+import org.apache.lucene.index.SegmentWriteState;
+
+/** Norms format that keeps all norms on disk */
+public final class CheapBastardNormsFormat extends NormsFormat {
+
+  @Override
+  public DocValuesConsumer normsConsumer(SegmentWriteState state) throws IOException {
+    return new DiskDocValuesConsumer(state, DATA_CODEC, DATA_EXTENSION, META_CODEC, META_EXTENSION);
+  }
+
+  @Override
+  public DocValuesProducer normsProducer(SegmentReadState state) throws IOException {
+    return new CheapBastardDocValuesProducer(state, DATA_CODEC, DATA_EXTENSION, META_CODEC, META_EXTENSION);
+  }
+  
+  static final String DATA_CODEC = "CheapBastardNormsData";
+  static final String DATA_EXTENSION = "cbnd";
+  static final String META_CODEC = "CheapBastardNormsMetadata";
+  static final String META_EXTENSION = "cbnm";
+}

Property changes on: lucene/test-framework/src/java/org/apache/lucene/codecs/cheapbastard/CheapBastardNormsFormat.java
___________________________________________________________________
Added: svn:eol-style
## -0,0 +1 ##
+native
Index: lucene/test-framework/src/java/org/apache/lucene/codecs/cheapbastard/CheapBastardDocValuesProducer.java
===================================================================
--- lucene/test-framework/src/java/org/apache/lucene/codecs/cheapbastard/CheapBastardDocValuesProducer.java	(revision 0)
+++ lucene/test-framework/src/java/org/apache/lucene/codecs/cheapbastard/CheapBastardDocValuesProducer.java	(working copy)
@@ -0,0 +1,257 @@
+package org.apache.lucene.codecs.cheapbastard;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.HashMap;
+import java.util.Map;
+
+import org.apache.lucene.codecs.CodecUtil;
+import org.apache.lucene.codecs.DocValuesProducer;
+import org.apache.lucene.codecs.diskdv.DiskDocValuesFormat;
+import org.apache.lucene.index.BinaryDocValues;
+import org.apache.lucene.index.CorruptIndexException;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.NumericDocValues;
+import org.apache.lucene.index.SegmentReadState;
+import org.apache.lucene.index.SortedDocValues;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util.packed.BlockPackedReader;
+import org.apache.lucene.util.packed.MonotonicBlockPackedReader;
+
+class CheapBastardDocValuesProducer extends DocValuesProducer {
+  private final Map<Integer,NumericEntry> numerics;
+  private final Map<Integer,NumericEntry> ords;
+  private final Map<Integer,BinaryEntry> binaries;
+  private final IndexInput data;
+  
+  CheapBastardDocValuesProducer(SegmentReadState state, String dataCodec, String dataExtension, String metaCodec, String metaExtension) throws IOException {
+    String metaName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, metaExtension);
+    // read in the entries from the metadata file.
+    IndexInput in = state.directory.openInput(metaName, state.context);
+    boolean success = false;
+    try {
+      CodecUtil.checkHeader(in, metaCodec, 
+                                DiskDocValuesFormat.VERSION_START,
+                                DiskDocValuesFormat.VERSION_START);
+      numerics = new HashMap<Integer,NumericEntry>();
+      ords = new HashMap<Integer,NumericEntry>();
+      binaries = new HashMap<Integer,BinaryEntry>();
+      readFields(in);
+      success = true;
+    } finally {
+      if (success) {
+        IOUtils.close(in);
+      } else {
+        IOUtils.closeWhileHandlingException(in);
+      }
+    }
+    
+    String dataName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, dataExtension);
+    data = state.directory.openInput(dataName, state.context);
+    CodecUtil.checkHeader(data, dataCodec, 
+                                DiskDocValuesFormat.VERSION_START,
+                                DiskDocValuesFormat.VERSION_START);
+  }
+  
+  private void readFields(IndexInput meta) throws IOException {
+    int fieldNumber = meta.readVInt();
+    while (fieldNumber != -1) {
+      byte type = meta.readByte();
+      if (type == DiskDocValuesFormat.NUMERIC) {
+        numerics.put(fieldNumber, readNumericEntry(meta));
+      } else if (type == DiskDocValuesFormat.BINARY) {
+        BinaryEntry b = readBinaryEntry(meta);
+        binaries.put(fieldNumber, b);
+      } else if (type == DiskDocValuesFormat.SORTED) {
+        // sorted = binary + numeric
+        if (meta.readVInt() != fieldNumber) {
+          throw new CorruptIndexException("sorted entry for field: " + fieldNumber + " is corrupt");
+        }
+        if (meta.readByte() != DiskDocValuesFormat.BINARY) {
+          throw new CorruptIndexException("sorted entry for field: " + fieldNumber + " is corrupt");
+        }
+        BinaryEntry b = readBinaryEntry(meta);
+        binaries.put(fieldNumber, b);
+        
+        if (meta.readVInt() != fieldNumber) {
+          throw new CorruptIndexException("sorted entry for field: " + fieldNumber + " is corrupt");
+        }
+        if (meta.readByte() != DiskDocValuesFormat.NUMERIC) {
+          throw new CorruptIndexException("sorted entry for field: " + fieldNumber + " is corrupt");
+        }
+        NumericEntry n = readNumericEntry(meta);
+        ords.put(fieldNumber, n);
+      }
+      fieldNumber = meta.readVInt();
+    }
+  }
+  
+  static NumericEntry readNumericEntry(IndexInput meta) throws IOException {
+    NumericEntry entry = new NumericEntry();
+    entry.packedIntsVersion = meta.readVInt();
+    entry.offset = meta.readLong();
+    entry.count = meta.readVInt();
+    entry.blockSize = meta.readVInt();
+    return entry;
+  }
+  
+  static BinaryEntry readBinaryEntry(IndexInput meta) throws IOException {
+    BinaryEntry entry = new BinaryEntry();
+    entry.minLength = meta.readVInt();
+    entry.maxLength = meta.readVInt();
+    entry.count = meta.readVInt();
+    entry.offset = meta.readLong();
+    if (entry.minLength != entry.maxLength) {
+      entry.addressesOffset = meta.readLong();
+      entry.packedIntsVersion = meta.readVInt();
+      entry.blockSize = meta.readVInt();
+    }
+    return entry;
+  }
+
+  @Override
+  public NumericDocValues getNumeric(FieldInfo field) throws IOException {
+    NumericEntry entry = numerics.get(field.number);
+    return getNumeric(field, entry);
+  }
+  
+  private NumericDocValues getNumeric(FieldInfo field, final NumericEntry entry) throws IOException {
+    final IndexInput data = this.data.clone();
+    data.seek(entry.offset);
+
+    final BlockPackedReader reader = new BlockPackedReader(data, entry.packedIntsVersion, entry.blockSize, entry.count, true);
+    return new NumericDocValues() {
+      @Override
+      public long get(int docID) {
+        return reader.get(docID);
+      }
+    };
+  }
+
+  @Override
+  public BinaryDocValues getBinary(FieldInfo field) throws IOException {
+    BinaryEntry bytes = binaries.get(field.number);
+    if (bytes.minLength == bytes.maxLength) {
+      return getFixedBinary(field, bytes);
+    } else {
+      return getVariableBinary(field, bytes);
+    }
+  }
+  
+  private BinaryDocValues getFixedBinary(FieldInfo field, final BinaryEntry bytes) {
+    final IndexInput data = this.data.clone();
+
+    return new BinaryDocValues() {
+      @Override
+      public void get(int docID, BytesRef result) {
+        long address = bytes.offset + docID * (long)bytes.maxLength;
+        try {
+          data.seek(address);
+          // NOTE: we could have one buffer, but various consumers (e.g. FieldComparatorSource) 
+          // assume "they" own the bytes after calling this!
+          final byte[] buffer = new byte[bytes.maxLength];
+          data.readBytes(buffer, 0, buffer.length);
+          result.bytes = buffer;
+          result.offset = 0;
+          result.length = buffer.length;
+        } catch (IOException e) {
+          throw new RuntimeException(e);
+        }
+      }
+    };
+  }
+  
+  private BinaryDocValues getVariableBinary(FieldInfo field, final BinaryEntry bytes) throws IOException {
+    final IndexInput data = this.data.clone();
+    data.seek(bytes.addressesOffset);
+
+    final MonotonicBlockPackedReader addresses = new MonotonicBlockPackedReader(data, bytes.packedIntsVersion, bytes.blockSize, bytes.count, true);
+    return new BinaryDocValues() {
+      @Override
+      public void get(int docID, BytesRef result) {
+        long startAddress = bytes.offset + (docID == 0 ? 0 : + addresses.get(docID-1));
+        long endAddress = bytes.offset + addresses.get(docID);
+        int length = (int) (endAddress - startAddress);
+        try {
+          data.seek(startAddress);
+          // NOTE: we could have one buffer, but various consumers (e.g. FieldComparatorSource) 
+          // assume "they" own the bytes after calling this!
+          final byte[] buffer = new byte[length];
+          data.readBytes(buffer, 0, buffer.length);
+          result.bytes = buffer;
+          result.offset = 0;
+          result.length = length;
+        } catch (IOException e) {
+          throw new RuntimeException(e);
+        }
+      }
+    };
+  }
+
+  @Override
+  public SortedDocValues getSorted(FieldInfo field) throws IOException {
+    final int valueCount = binaries.get(field.number).count;
+    final BinaryDocValues binary = getBinary(field);
+    final NumericDocValues ordinals = getNumeric(field, ords.get(field.number));
+    return new SortedDocValues() {
+
+      @Override
+      public int getOrd(int docID) {
+        return (int) ordinals.get(docID);
+      }
+
+      @Override
+      public void lookupOrd(int ord, BytesRef result) {
+        binary.get(ord, result);
+      }
+
+      @Override
+      public int getValueCount() {
+        return valueCount;
+      }
+    };
+  }
+
+  @Override
+  public void close() throws IOException {
+    data.close();
+  }
+  
+  static class NumericEntry {
+    long offset;
+
+    int packedIntsVersion;
+    int count;
+    int blockSize;
+  }
+  
+  static class BinaryEntry {
+    long offset;
+
+    int count;
+    int minLength;
+    int maxLength;
+    long addressesOffset;
+    int packedIntsVersion;
+    int blockSize;
+  }
+}

Property changes on: lucene/test-framework/src/java/org/apache/lucene/codecs/cheapbastard/CheapBastardDocValuesProducer.java
___________________________________________________________________
Added: svn:eol-style
## -0,0 +1 ##
+native
Index: lucene/test-framework/src/java/org/apache/lucene/codecs/cheapbastard/CheapBastardCodec.java
===================================================================
--- lucene/test-framework/src/java/org/apache/lucene/codecs/cheapbastard/CheapBastardCodec.java	(revision 0)
+++ lucene/test-framework/src/java/org/apache/lucene/codecs/cheapbastard/CheapBastardCodec.java	(working copy)
@@ -0,0 +1,72 @@
+package org.apache.lucene.codecs.cheapbastard;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.codecs.DocValuesFormat;
+import org.apache.lucene.codecs.FilterCodec;
+import org.apache.lucene.codecs.NormsFormat;
+import org.apache.lucene.codecs.PostingsFormat;
+import org.apache.lucene.codecs.StoredFieldsFormat;
+import org.apache.lucene.codecs.TermVectorsFormat;
+import org.apache.lucene.codecs.lucene40.Lucene40StoredFieldsFormat;
+import org.apache.lucene.codecs.lucene40.Lucene40TermVectorsFormat;
+import org.apache.lucene.codecs.lucene41.Lucene41PostingsFormat;
+import org.apache.lucene.codecs.lucene42.Lucene42Codec;
+
+/** Codec that tries to use as little ram as possible because he spent all his money on beer */
+// TODO: better name :) 
+// but if we named it "LowMemory" in codecs/ package, it would be irresistible like optimize()!
+public class CheapBastardCodec extends FilterCodec {
+  
+  // TODO: would be better to have no terms index at all and bsearch a terms dict
+  private final PostingsFormat postings = new Lucene41PostingsFormat(100, 200);
+  // uncompressing versions, waste lots of disk but no ram
+  private final StoredFieldsFormat storedFields = new Lucene40StoredFieldsFormat();
+  private final TermVectorsFormat termVectors = new Lucene40TermVectorsFormat();
+  // these go to disk for all docvalues/norms datastructures
+  private final DocValuesFormat docValues = new CheapBastardDocValuesFormat();
+  private final NormsFormat norms = new CheapBastardNormsFormat();
+
+  public CheapBastardCodec() {
+    super("CheapBastard", new Lucene42Codec());
+  }
+  
+  public PostingsFormat postingsFormat() {
+    return postings;
+  }
+
+  @Override
+  public DocValuesFormat docValuesFormat() {
+    return docValues;
+  }
+  
+  @Override
+  public NormsFormat normsFormat() {
+    return norms;
+  }
+  
+  @Override
+  public StoredFieldsFormat storedFieldsFormat() {
+    return storedFields;
+  }
+
+  @Override
+  public TermVectorsFormat termVectorsFormat() {
+    return termVectors;
+  }
+}

Property changes on: lucene/test-framework/src/java/org/apache/lucene/codecs/cheapbastard/CheapBastardCodec.java
___________________________________________________________________
Added: svn:eol-style
## -0,0 +1 ##
+native
Index: lucene/test-framework/src/java/org/apache/lucene/codecs/cheapbastard/CheapBastardDocValuesFormat.java
===================================================================
--- lucene/test-framework/src/java/org/apache/lucene/codecs/cheapbastard/CheapBastardDocValuesFormat.java	(revision 0)
+++ lucene/test-framework/src/java/org/apache/lucene/codecs/cheapbastard/CheapBastardDocValuesFormat.java	(working copy)
@@ -0,0 +1,66 @@
+package org.apache.lucene.codecs.cheapbastard;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.codecs.DocValuesConsumer;
+import org.apache.lucene.codecs.DocValuesProducer;
+import org.apache.lucene.codecs.DocValuesFormat;
+import org.apache.lucene.codecs.diskdv.DiskDocValuesConsumer;
+import org.apache.lucene.codecs.diskdv.DiskDocValuesFormat;
+import org.apache.lucene.index.SegmentReadState;
+import org.apache.lucene.index.SegmentWriteState;
+
+/**
+ * DocValues format that keeps everything on disk.
+ * <p>
+ * Internally there are only 2 field types:
+ * <ul>
+ *   <li>BINARY: a big byte[].
+ *   <li>NUMERIC: packed ints
+ * </ul>
+ * SORTED is encoded as BINARY + NUMERIC
+ * <p>
+ * NOTE: Don't use this format in production (its not very efficient).
+ * Most likely you would want some parts in RAM, other parts on disk. 
+ * <p>
+ * @lucene.experimental
+ */
+public final class CheapBastardDocValuesFormat extends DocValuesFormat {
+
+  public CheapBastardDocValuesFormat() {
+    super("CheapBastard");
+  }
+
+  @Override
+  public DocValuesConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
+    return new DiskDocValuesConsumer(state, DiskDocValuesFormat.DATA_CODEC, 
+                                            DiskDocValuesFormat.DATA_EXTENSION, 
+                                            DiskDocValuesFormat.META_CODEC, 
+                                            DiskDocValuesFormat.META_EXTENSION);
+  }
+
+  @Override
+  public DocValuesProducer fieldsProducer(SegmentReadState state) throws IOException {
+    return new CheapBastardDocValuesProducer(state, DiskDocValuesFormat.DATA_CODEC, 
+                                                    DiskDocValuesFormat.DATA_EXTENSION, 
+                                                    DiskDocValuesFormat.META_CODEC, 
+                                                    DiskDocValuesFormat.META_EXTENSION);
+  }
+}

Property changes on: lucene/test-framework/src/java/org/apache/lucene/codecs/cheapbastard/CheapBastardDocValuesFormat.java
___________________________________________________________________
Added: svn:eol-style
## -0,0 +1 ##
+native
Index: lucene/test-framework/src/java/org/apache/lucene/codecs/ramonly/RAMOnlyPostingsFormat.java
===================================================================
--- lucene/test-framework/src/java/org/apache/lucene/codecs/ramonly/RAMOnlyPostingsFormat.java	(revision 1442822)
+++ lucene/test-framework/src/java/org/apache/lucene/codecs/ramonly/RAMOnlyPostingsFormat.java	(working copy)
@@ -555,7 +555,7 @@
 
     // Load our ID:
     final String idFileName = IndexFileNames.segmentFileName(readState.segmentInfo.name, readState.segmentSuffix, ID_EXTENSION);
-    IndexInput in = readState.dir.openInput(idFileName, readState.context);
+    IndexInput in = readState.directory.openInput(idFileName, readState.context);
     boolean success = false;
     final int id;
     try {
Index: lucene/test-framework/src/java/org/apache/lucene/codecs/nestedpulsing/NestedPulsingPostingsFormat.java
===================================================================
--- lucene/test-framework/src/java/org/apache/lucene/codecs/nestedpulsing/NestedPulsingPostingsFormat.java	(revision 1442822)
+++ lucene/test-framework/src/java/org/apache/lucene/codecs/nestedpulsing/NestedPulsingPostingsFormat.java	(working copy)
@@ -77,11 +77,11 @@
     PostingsReaderBase pulsingReader = null;
     boolean success = false;
     try {
-      docsReader = new Lucene41PostingsReader(state.dir, state.fieldInfos, state.segmentInfo, state.context, state.segmentSuffix);
+      docsReader = new Lucene41PostingsReader(state.directory, state.fieldInfos, state.segmentInfo, state.context, state.segmentSuffix);
       pulsingReaderInner = new PulsingPostingsReader(docsReader);
       pulsingReader = new PulsingPostingsReader(pulsingReaderInner);
       FieldsProducer ret = new BlockTreeTermsReader(
-                                                    state.dir, state.fieldInfos, state.segmentInfo,
+                                                    state.directory, state.fieldInfos, state.segmentInfo,
                                                     pulsingReader,
                                                     state.context,
                                                     state.segmentSuffix,
Index: lucene/test-framework/src/java/org/apache/lucene/codecs/mockrandom/MockRandomPostingsFormat.java
===================================================================
--- lucene/test-framework/src/java/org/apache/lucene/codecs/mockrandom/MockRandomPostingsFormat.java	(revision 1442822)
+++ lucene/test-framework/src/java/org/apache/lucene/codecs/mockrandom/MockRandomPostingsFormat.java	(working copy)
@@ -288,7 +288,7 @@
   public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {
 
     final String seedFileName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, SEED_EXT);
-    final IndexInput in = state.dir.openInput(seedFileName, state.context);
+    final IndexInput in = state.directory.openInput(seedFileName, state.context);
     final long seed = in.readLong();
     if (LuceneTestCase.VERBOSE) {
       System.out.println("MockRandomCodec: reading from seg=" + state.segmentInfo.name + " formatID=" + state.segmentSuffix + " seed=" + seed);
@@ -308,13 +308,13 @@
       if (LuceneTestCase.VERBOSE) {
         System.out.println("MockRandomCodec: reading Sep postings");
       }
-      postingsReader = new SepPostingsReader(state.dir, state.fieldInfos, state.segmentInfo,
+      postingsReader = new SepPostingsReader(state.directory, state.fieldInfos, state.segmentInfo,
                                              state.context, new MockIntStreamFactory(random), state.segmentSuffix);
     } else {
       if (LuceneTestCase.VERBOSE) {
         System.out.println("MockRandomCodec: reading Standard postings");
       }
-      postingsReader = new Lucene41PostingsReader(state.dir, state.fieldInfos, state.segmentInfo, state.context, state.segmentSuffix);
+      postingsReader = new Lucene41PostingsReader(state.directory, state.fieldInfos, state.segmentInfo, state.context, state.segmentSuffix);
     }
 
     if (random.nextBoolean()) {
@@ -335,7 +335,7 @@
 
       boolean success = false;
       try {
-        fields = new BlockTreeTermsReader(state.dir,
+        fields = new BlockTreeTermsReader(state.directory,
                                           state.fieldInfos,
                                           state.segmentInfo,
                                           postingsReader,
@@ -369,7 +369,7 @@
           if (LuceneTestCase.VERBOSE) {
             System.out.println("MockRandomCodec: fixed-gap terms index (divisor=" + state.termsIndexDivisor + ")");
           }
-          indexReader = new FixedGapTermsIndexReader(state.dir,
+          indexReader = new FixedGapTermsIndexReader(state.directory,
                                                      state.fieldInfos,
                                                      state.segmentInfo.name,
                                                      state.termsIndexDivisor,
@@ -385,7 +385,7 @@
           if (LuceneTestCase.VERBOSE) {
             System.out.println("MockRandomCodec: variable-gap terms index (divisor=" + state.termsIndexDivisor + ")");
           }
-          indexReader = new VariableGapTermsIndexReader(state.dir,
+          indexReader = new VariableGapTermsIndexReader(state.directory,
                                                         state.fieldInfos,
                                                         state.segmentInfo.name,
                                                         state.termsIndexDivisor,
@@ -405,7 +405,7 @@
       success = false;
       try {
         fields = new BlockTermsReader(indexReader,
-                                      state.dir,
+                                      state.directory,
                                       state.fieldInfos,
                                       state.segmentInfo,
                                       postingsReader,
Index: lucene/test-framework/src/java/org/apache/lucene/codecs/asserting/AssertingNormsFormat.java
===================================================================
--- lucene/test-framework/src/java/org/apache/lucene/codecs/asserting/AssertingNormsFormat.java	(revision 0)
+++ lucene/test-framework/src/java/org/apache/lucene/codecs/asserting/AssertingNormsFormat.java	(working copy)
@@ -0,0 +1,51 @@
+package org.apache.lucene.codecs.asserting;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.codecs.DocValuesConsumer;
+import org.apache.lucene.codecs.DocValuesProducer;
+import org.apache.lucene.codecs.NormsFormat;
+import org.apache.lucene.codecs.asserting.AssertingDocValuesFormat.AssertingDocValuesConsumer;
+import org.apache.lucene.codecs.asserting.AssertingDocValuesFormat.AssertingDocValuesProducer;
+import org.apache.lucene.codecs.lucene42.Lucene42NormsFormat;
+import org.apache.lucene.index.SegmentReadState;
+import org.apache.lucene.index.SegmentWriteState;
+
+/**
+ * Just like {@link Lucene42NormsFormat} but with additional asserts.
+ */
+public class AssertingNormsFormat extends NormsFormat {
+  private final NormsFormat in = new Lucene42NormsFormat();
+  
+  @Override
+  public DocValuesConsumer normsConsumer(SegmentWriteState state) throws IOException {
+    DocValuesConsumer consumer = in.normsConsumer(state);
+    assert consumer != null;
+    return new AssertingDocValuesConsumer(consumer, state.segmentInfo.getDocCount());
+  }
+
+  @Override
+  public DocValuesProducer normsProducer(SegmentReadState state) throws IOException {
+    assert state.fieldInfos.hasNorms();
+    DocValuesProducer producer = in.normsProducer(state);
+    assert producer != null;
+    return new AssertingDocValuesProducer(producer, state.segmentInfo.getDocCount());
+  }
+}

Property changes on: lucene/test-framework/src/java/org/apache/lucene/codecs/asserting/AssertingNormsFormat.java
___________________________________________________________________
Added: svn:eol-style
## -0,0 +1 ##
+native
Index: lucene/test-framework/src/java/org/apache/lucene/codecs/asserting/AssertingCodec.java
===================================================================
--- lucene/test-framework/src/java/org/apache/lucene/codecs/asserting/AssertingCodec.java	(revision 1442822)
+++ lucene/test-framework/src/java/org/apache/lucene/codecs/asserting/AssertingCodec.java	(working copy)
@@ -17,7 +17,9 @@
  * limitations under the License.
  */
 
+import org.apache.lucene.codecs.DocValuesFormat;
 import org.apache.lucene.codecs.FilterCodec;
+import org.apache.lucene.codecs.NormsFormat;
 import org.apache.lucene.codecs.PostingsFormat;
 import org.apache.lucene.codecs.StoredFieldsFormat;
 import org.apache.lucene.codecs.TermVectorsFormat;
@@ -31,6 +33,8 @@
   private final PostingsFormat postings = new AssertingPostingsFormat();
   private final TermVectorsFormat vectors = new AssertingTermVectorsFormat();
   private final StoredFieldsFormat storedFields = new AssertingStoredFieldsFormat();
+  private final DocValuesFormat docValues = new AssertingDocValuesFormat();
+  private final NormsFormat norms = new AssertingNormsFormat();
 
   public AssertingCodec() {
     super("Asserting", new Lucene42Codec());
@@ -50,4 +54,14 @@
   public StoredFieldsFormat storedFieldsFormat() {
     return storedFields;
   }
+
+  @Override
+  public DocValuesFormat docValuesFormat() {
+    return docValues;
+  }
+
+  @Override
+  public NormsFormat normsFormat() {
+    return norms;
+  }
 }
Index: lucene/test-framework/src/java/org/apache/lucene/codecs/asserting/AssertingDocValuesFormat.java
===================================================================
--- lucene/test-framework/src/java/org/apache/lucene/codecs/asserting/AssertingDocValuesFormat.java	(revision 0)
+++ lucene/test-framework/src/java/org/apache/lucene/codecs/asserting/AssertingDocValuesFormat.java	(working copy)
@@ -0,0 +1,197 @@
+package org.apache.lucene.codecs.asserting;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Iterator;
+import java.util.NoSuchElementException;
+
+import org.apache.lucene.codecs.DocValuesConsumer;
+import org.apache.lucene.codecs.DocValuesFormat;
+import org.apache.lucene.codecs.DocValuesProducer;
+import org.apache.lucene.codecs.lucene42.Lucene42DocValuesFormat;
+import org.apache.lucene.index.AssertingAtomicReader;
+import org.apache.lucene.index.BinaryDocValues;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.NumericDocValues;
+import org.apache.lucene.index.SegmentReadState;
+import org.apache.lucene.index.SegmentWriteState;
+import org.apache.lucene.index.SortedDocValues;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.FixedBitSet;
+
+/**
+ * Just like {@link Lucene42DocValuesFormat} but with additional asserts.
+ */
+public class AssertingDocValuesFormat extends DocValuesFormat {
+  private final DocValuesFormat in = new Lucene42DocValuesFormat();
+  
+  public AssertingDocValuesFormat() {
+    super("Asserting");
+  }
+
+  @Override
+  public DocValuesConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
+    DocValuesConsumer consumer = in.fieldsConsumer(state);
+    assert consumer != null;
+    return new AssertingDocValuesConsumer(consumer, state.segmentInfo.getDocCount());
+  }
+
+  @Override
+  public DocValuesProducer fieldsProducer(SegmentReadState state) throws IOException {
+    assert state.fieldInfos.hasDocValues();
+    DocValuesProducer producer = in.fieldsProducer(state);
+    assert producer != null;
+    return new AssertingDocValuesProducer(producer, state.segmentInfo.getDocCount());
+  }
+  
+  static class AssertingDocValuesConsumer extends DocValuesConsumer {
+    private final DocValuesConsumer in;
+    private final int maxDoc;
+    
+    AssertingDocValuesConsumer(DocValuesConsumer in, int maxDoc) {
+      this.in = in;
+      this.maxDoc = maxDoc;
+    }
+
+    @Override
+    public void addNumericField(FieldInfo field, Iterable<Number> values) throws IOException {
+      int count = 0;
+      for (Number v : values) {
+        assert v != null;
+        count++;
+      }
+      assert count == maxDoc;
+      checkIterator(values.iterator(), maxDoc);
+      in.addNumericField(field, values);
+    }
+    
+    @Override
+    public void addBinaryField(FieldInfo field, Iterable<BytesRef> values) throws IOException {
+      int count = 0;
+      for (BytesRef b : values) {
+        assert b != null;
+        assert b.isValid();
+        count++;
+      }
+      assert count == maxDoc;
+      checkIterator(values.iterator(), maxDoc);
+      in.addBinaryField(field, values);
+    }
+    
+    @Override
+    public void addSortedField(FieldInfo field, Iterable<BytesRef> values, Iterable<Number> docToOrd) throws IOException {
+      int valueCount = 0;
+      BytesRef lastValue = null;
+      for (BytesRef b : values) {
+        assert b != null;
+        assert b.isValid();
+        if (valueCount > 0) {
+          assert b.compareTo(lastValue) > 0;
+        }
+        lastValue = BytesRef.deepCopyOf(b);
+        valueCount++;
+      }
+      assert valueCount <= maxDoc;
+      
+      FixedBitSet seenOrds = new FixedBitSet(valueCount);
+      
+      int count = 0;
+      for (Number v : docToOrd) {
+        assert v != null;
+        int ord = v.intValue();
+        assert ord >= 0 && ord < valueCount;
+        seenOrds.set(ord);
+        count++;
+      }
+      
+      assert count == maxDoc;
+      assert seenOrds.cardinality() == valueCount;
+      checkIterator(values.iterator(), valueCount);
+      checkIterator(docToOrd.iterator(), maxDoc);
+      in.addSortedField(field, values, docToOrd);
+    }
+    
+    private <T> void checkIterator(Iterator<T> iterator, int expectedSize) {
+      for (int i = 0; i < expectedSize; i++) {
+        boolean hasNext = iterator.hasNext();
+        assert hasNext;
+        T v = iterator.next();
+        assert v != null;
+        try {
+          iterator.remove();
+          throw new AssertionError("broken iterator (supports remove): " + iterator);
+        } catch (UnsupportedOperationException expected) {
+          // ok
+        }
+      }
+      assert !iterator.hasNext();
+      try {
+        iterator.next();
+        throw new AssertionError("broken iterator (allows next() when hasNext==false) " + iterator);
+      } catch (NoSuchElementException expected) {
+        // ok
+      }
+    }
+    
+    @Override
+    public void close() throws IOException {
+      in.close();
+    }
+  }
+  
+  static class AssertingDocValuesProducer extends DocValuesProducer {
+    private final DocValuesProducer in;
+    private final int maxDoc;
+    
+    AssertingDocValuesProducer(DocValuesProducer in, int maxDoc) {
+      this.in = in;
+      this.maxDoc = maxDoc;
+    }
+
+    @Override
+    public NumericDocValues getNumeric(FieldInfo field) throws IOException {
+      assert field.getDocValuesType() == FieldInfo.DocValuesType.NUMERIC || 
+             field.getNormType() == FieldInfo.DocValuesType.NUMERIC;
+      NumericDocValues values = in.getNumeric(field);
+      assert values != null;
+      return new AssertingAtomicReader.AssertingNumericDocValues(values, maxDoc);
+    }
+
+    @Override
+    public BinaryDocValues getBinary(FieldInfo field) throws IOException {
+      assert field.getDocValuesType() == FieldInfo.DocValuesType.BINARY;
+      BinaryDocValues values = in.getBinary(field);
+      assert values != null;
+      return new AssertingAtomicReader.AssertingBinaryDocValues(values, maxDoc);
+    }
+
+    @Override
+    public SortedDocValues getSorted(FieldInfo field) throws IOException {
+      assert field.getDocValuesType() == FieldInfo.DocValuesType.SORTED;
+      SortedDocValues values = in.getSorted(field);
+      assert values != null;
+      return new AssertingAtomicReader.AssertingSortedDocValues(values, maxDoc);
+    }
+    
+    @Override
+    public void close() throws IOException {
+      in.close();
+    }
+  }
+}

Property changes on: lucene/test-framework/src/java/org/apache/lucene/codecs/asserting/AssertingDocValuesFormat.java
___________________________________________________________________
Added: svn:eol-style
## -0,0 +1 ##
+native
Index: lucene/test-framework/src/resources/META-INF/services/org.apache.lucene.codecs.Codec
===================================================================
--- lucene/test-framework/src/resources/META-INF/services/org.apache.lucene.codecs.Codec	(revision 1442822)
+++ lucene/test-framework/src/resources/META-INF/services/org.apache.lucene.codecs.Codec	(working copy)
@@ -14,8 +14,10 @@
 #  limitations under the License.
 
 org.apache.lucene.codecs.asserting.AssertingCodec
+org.apache.lucene.codecs.cheapbastard.CheapBastardCodec
 org.apache.lucene.codecs.compressing.FastCompressingCodec
 org.apache.lucene.codecs.compressing.FastDecompressionCompressingCodec
 org.apache.lucene.codecs.compressing.HighCompressionCompressingCodec
 org.apache.lucene.codecs.compressing.dummy.DummyCompressingCodec
+org.apache.lucene.codecs.lucene40.Lucene40RWCodec
 org.apache.lucene.codecs.lucene41.Lucene41RWCodec
Index: lucene/test-framework/src/resources/META-INF/services/org.apache.lucene.codecs.DocValuesFormat
===================================================================
--- lucene/test-framework/src/resources/META-INF/services/org.apache.lucene.codecs.DocValuesFormat	(revision 0)
+++ lucene/test-framework/src/resources/META-INF/services/org.apache.lucene.codecs.DocValuesFormat	(working copy)
@@ -0,0 +1,17 @@
+#  Licensed to the Apache Software Foundation (ASF) under one or more
+#  contributor license agreements.  See the NOTICE file distributed with
+#  this work for additional information regarding copyright ownership.
+#  The ASF licenses this file to You under the Apache License, Version 2.0
+#  (the "License"); you may not use this file except in compliance with
+#  the License.  You may obtain a copy of the License at
+#
+#       http://www.apache.org/licenses/LICENSE-2.0
+#
+#  Unless required by applicable law or agreed to in writing, software
+#  distributed under the License is distributed on an "AS IS" BASIS,
+#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+#  See the License for the specific language governing permissions and
+#  limitations under the License.
+
+org.apache.lucene.codecs.asserting.AssertingDocValuesFormat
+org.apache.lucene.codecs.cheapbastard.CheapBastardDocValuesFormat
\ No newline at end of file
Index: lucene/facet/src/test/org/apache/lucene/util/SlowRAMDirectory.java
===================================================================
--- lucene/facet/src/test/org/apache/lucene/util/SlowRAMDirectory.java	(revision 1442822)
+++ lucene/facet/src/test/org/apache/lucene/util/SlowRAMDirectory.java	(working copy)
@@ -33,7 +33,7 @@
 
   private static final int IO_SLEEP_THRESHOLD = 50;
   
-  private Random random;
+  Random random;
   private int sleepMillis;
 
   public void setSleepMillis(int sleepMillis) {
@@ -62,7 +62,7 @@
     return super.openInput(name, context);
   }
 
-  void doSleep(int length) {
+  void doSleep(Random random, int length) {
     int sTime = length<10 ? sleepMillis : (int) (sleepMillis * Math.log(length));
     if (random!=null) {
       sTime = random.nextInt(sTime);
@@ -74,6 +74,14 @@
     }
   }
 
+  /** Make a private random. */
+  Random forkRandom() {
+    if (random == null) {
+      return null;
+    }
+    return new Random(random.nextLong());
+  }
+
   /**
    * Delegate class to wrap an IndexInput and delay reading bytes by some
    * specified time.
@@ -81,16 +89,18 @@
   private class SlowIndexInput extends IndexInput {
     private IndexInput ii;
     private int numRead = 0;
+    private Random random;
     
     public SlowIndexInput(IndexInput ii) {
       super("SlowIndexInput(" + ii + ")");
+      this.random = forkRandom();
       this.ii = ii;
     }
     
     @Override
     public byte readByte() throws IOException {
       if (numRead >= IO_SLEEP_THRESHOLD) {
-        doSleep(0);
+        doSleep(random, 0);
         numRead = 0;
       }
       ++numRead;
@@ -100,7 +110,7 @@
     @Override
     public void readBytes(byte[] b, int offset, int len) throws IOException {
       if (numRead >= IO_SLEEP_THRESHOLD) {
-        doSleep(len);
+        doSleep(random, len);
         numRead = 0;
       }
       numRead += len;
@@ -125,15 +135,17 @@
     
     private IndexOutput io;
     private int numWrote;
+    private final Random random;
     
     public SlowIndexOutput(IndexOutput io) {
       this.io = io;
+      this.random = forkRandom();
     }
     
     @Override
     public void writeByte(byte b) throws IOException {
       if (numWrote >= IO_SLEEP_THRESHOLD) {
-        doSleep(0);
+        doSleep(random, 0);
         numWrote = 0;
       }
       ++numWrote;
@@ -143,7 +155,7 @@
     @Override
     public void writeBytes(byte[] b, int offset, int length) throws IOException {
       if (numWrote >= IO_SLEEP_THRESHOLD) {
-        doSleep(length);
+        doSleep(random, length);
         numWrote = 0;
       }
       numWrote += length;
Index: lucene/facet/src/test/org/apache/lucene/facet/search/TestMultipleCategoryLists.java
===================================================================
--- lucene/facet/src/test/org/apache/lucene/facet/search/TestMultipleCategoryLists.java	(revision 1442822)
+++ lucene/facet/src/test/org/apache/lucene/facet/search/TestMultipleCategoryLists.java	(working copy)
@@ -187,7 +187,7 @@
   private void assertOrdinalsExist(String field, IndexReader ir) throws IOException {
     for (AtomicReaderContext context : ir.leaves()) {
       AtomicReader r = context.reader();
-      if (r.docValues(field) != null) {
+      if (r.getBinaryDocValues(field) != null) {
         return; // not all segments must have this DocValues
       }
     }
Index: lucene/facet/src/test/org/apache/lucene/facet/search/CountingFacetsCollectorTest.java
===================================================================
--- lucene/facet/src/test/org/apache/lucene/facet/search/CountingFacetsCollectorTest.java	(revision 1442822)
+++ lucene/facet/src/test/org/apache/lucene/facet/search/CountingFacetsCollectorTest.java	(working copy)
@@ -433,7 +433,7 @@
     
     FacetSearchParams fsp = new FacetSearchParams(new CountFacetRequest(CP_A, NUM_CHILDREN_CP_A), 
         new CountFacetRequest(CP_B, NUM_CHILDREN_CP_B));
-    FacetsCollector fc = new CountingFacetsCollector(fsp , taxoReader, new FacetArrays(taxoReader.getSize()), true);
+    FacetsCollector fc = new CountingFacetsCollector(fsp , taxoReader, new FacetArrays(taxoReader.getSize()));
     searcher.search(new MatchAllDocsQuery(), fc);
     
     List<FacetResult> facetResults = fc.getFacetResults();
Index: lucene/facet/src/test/org/apache/lucene/facet/search/CategoryListIteratorTest.java
===================================================================
--- lucene/facet/src/test/org/apache/lucene/facet/search/CategoryListIteratorTest.java	(revision 1442822)
+++ lucene/facet/src/test/org/apache/lucene/facet/search/CategoryListIteratorTest.java	(working copy)
@@ -5,8 +5,8 @@
 
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.analysis.MockTokenizer;
+import org.apache.lucene.document.BinaryDocValuesField;
 import org.apache.lucene.document.Document;
-import org.apache.lucene.document.StraightBytesDocValuesField;
 import org.apache.lucene.facet.FacetTestCase;
 import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.IndexReader;
@@ -57,7 +57,7 @@
     for (int i = 0; i < data.length; i++) {
       Document doc = new Document();
       encoder.encode(IntsRef.deepCopyOf(data[i]), buf);
-      doc.add(new StraightBytesDocValuesField("f", buf));
+      doc.add(new BinaryDocValuesField("f", buf));
       writer.addDocument(doc);
     }
     IndexReader reader = writer.getReader();
@@ -100,9 +100,9 @@
       if (i == 0) {
         BytesRef buf = new BytesRef();
         encoder.encode(IntsRef.deepCopyOf(data[i]), buf );
-        doc.add(new StraightBytesDocValuesField("f", buf));
+        doc.add(new BinaryDocValuesField("f", buf));
       } else {
-        doc.add(new StraightBytesDocValuesField("f", new BytesRef()));
+        doc.add(new BinaryDocValuesField("f", new BytesRef()));
       }
       writer.addDocument(doc);
       writer.commit();
Index: lucene/facet/src/java/org/apache/lucene/facet/associations/FloatAssociationsIterator.java
===================================================================
--- lucene/facet/src/java/org/apache/lucene/facet/associations/FloatAssociationsIterator.java	(revision 1442822)
+++ lucene/facet/src/java/org/apache/lucene/facet/associations/FloatAssociationsIterator.java	(working copy)
@@ -2,8 +2,6 @@
 
 import java.io.IOException;
 
-import org.apache.lucene.index.DocValues;
-import org.apache.lucene.index.DocValues.Source;
 import org.apache.lucene.util.collections.IntToFloatMap;
 
 /*
@@ -31,23 +29,13 @@
 public class FloatAssociationsIterator extends AssociationsIterator<CategoryFloatAssociation> {
 
   private final IntToFloatMap ordinalAssociations = new IntToFloatMap();
-
+  
   /**
-   * Constructs a new {@link FloatAssociationsIterator} which uses an
-   * in-memory {@link DocValues#getSource() DocValues source}.
+   * Constructs a new {@link FloatAssociationsIterator}.
    */
   public FloatAssociationsIterator(String field, CategoryFloatAssociation association) throws IOException {
-    this(field, association, false);
+    super(field, association);
   }
-  
-  /**
-   * Constructs a new {@link FloatAssociationsIterator} which uses a
-   * {@link DocValues} {@link Source} per {@code useDirectSource}.
-   */
-  public FloatAssociationsIterator(String field, CategoryFloatAssociation association, boolean useDirectSource) 
-      throws IOException {
-    super(field, association, useDirectSource);
-  }
 
   @Override
   protected void handleAssociation(int ordinal, CategoryFloatAssociation association) {
Index: lucene/facet/src/java/org/apache/lucene/facet/associations/AssociationsIterator.java
===================================================================
--- lucene/facet/src/java/org/apache/lucene/facet/associations/AssociationsIterator.java	(revision 1442822)
+++ lucene/facet/src/java/org/apache/lucene/facet/associations/AssociationsIterator.java	(working copy)
@@ -3,8 +3,7 @@
 import java.io.IOException;
 
 import org.apache.lucene.index.AtomicReaderContext;
-import org.apache.lucene.index.DocValues;
-import org.apache.lucene.index.DocValues.Source;
+import org.apache.lucene.index.BinaryDocValues;
 import org.apache.lucene.store.ByteArrayDataInput;
 import org.apache.lucene.util.BytesRef;
 
@@ -34,25 +33,19 @@
 
   private final T association;
   private final String dvField;
-  private final boolean useDirectSource;
   private final BytesRef bytes = new BytesRef(32);
   
-  private DocValues.Source current;
+  private BinaryDocValues current;
   
   /**
    * Construct a new associations iterator. The given
    * {@link CategoryAssociation} is used to deserialize the association values.
    * It is assumed that all association values can be deserialized with the
    * given {@link CategoryAssociation}.
-   * 
-   * <p>
-   * <b>NOTE:</b> if {@code useDirectSource} is {@code false}, then a
-   * {@link DocValues#getSource()} is used, which is an in-memory {@link Source}.
    */
-  public AssociationsIterator(String field, T association, boolean useDirectSource) throws IOException {
+  public AssociationsIterator(String field, T association) throws IOException {
     this.association = association;
     this.dvField = field + association.getCategoryListID();
-    this.useDirectSource = useDirectSource;
   }
 
   /**
@@ -61,14 +54,8 @@
    * of the documents belonging to the association given to the constructor.
    */
   public final boolean setNextReader(AtomicReaderContext context) throws IOException {
-    DocValues dv = context.reader().docValues(dvField);
-    if (dv == null) {
-      current = null;
-      return false;
-    }
-    
-    current = useDirectSource ? dv.getDirectSource() : dv.getSource();
-    return true;
+    current = context.reader().getBinaryDocValues(dvField);
+    return current != null;
   }
   
   /**
@@ -78,7 +65,7 @@
    * extending classes.
    */
   protected final boolean setNextDoc(int docID) throws IOException {
-    current.getBytes(docID, bytes);
+    current.get(docID, bytes);
     if (bytes.length == 0) {
       return false; // no associations for the requested document
     }
Index: lucene/facet/src/java/org/apache/lucene/facet/associations/IntAssociationsIterator.java
===================================================================
--- lucene/facet/src/java/org/apache/lucene/facet/associations/IntAssociationsIterator.java	(revision 1442822)
+++ lucene/facet/src/java/org/apache/lucene/facet/associations/IntAssociationsIterator.java	(working copy)
@@ -2,8 +2,6 @@
 
 import java.io.IOException;
 
-import org.apache.lucene.index.DocValues;
-import org.apache.lucene.index.DocValues.Source;
 import org.apache.lucene.util.collections.IntToIntMap;
 
 /*
@@ -33,22 +31,12 @@
   private final IntToIntMap ordinalAssociations = new IntToIntMap();
 
   /**
-   * Constructs a new {@link IntAssociationsIterator} which uses an
-   * in-memory {@link DocValues#getSource() DocValues source}.
+   * Constructs a new {@link IntAssociationsIterator}.
    */
   public IntAssociationsIterator(String field, CategoryIntAssociation association) throws IOException {
-    this(field, association, false);
+    super(field, association);
   }
 
-  /**
-   * Constructs a new {@link IntAssociationsIterator} which uses a
-   * {@link DocValues} {@link Source} per {@code useDirectSource}.
-   */
-  public IntAssociationsIterator(String field, CategoryIntAssociation association, boolean useDirectSource)
-      throws IOException {
-    super(field, association, useDirectSource);
-  }
-
   @Override
   protected void handleAssociation(int ordinal, CategoryIntAssociation association) {
     ordinalAssociations.put(ordinal, association.getValue());
Index: lucene/facet/src/java/org/apache/lucene/facet/search/DocValuesCategoryListIterator.java
===================================================================
--- lucene/facet/src/java/org/apache/lucene/facet/search/DocValuesCategoryListIterator.java	(revision 1442822)
+++ lucene/facet/src/java/org/apache/lucene/facet/search/DocValuesCategoryListIterator.java	(working copy)
@@ -3,8 +3,7 @@
 import java.io.IOException;
 
 import org.apache.lucene.index.AtomicReaderContext;
-import org.apache.lucene.index.DocValues;
-import org.apache.lucene.index.DocValues.Source;
+import org.apache.lucene.index.BinaryDocValues;
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.IntsRef;
 import org.apache.lucene.util.encoding.IntDecoder;
@@ -26,35 +25,23 @@
  * limitations under the License.
  */
 
-/** A {@link CategoryListIterator} which reads the ordinals from a {@link DocValues}. */
+/** A {@link CategoryListIterator} which reads the ordinals from a {@link BinaryDocValues}. */
 public class DocValuesCategoryListIterator implements CategoryListIterator {
   
   private final IntDecoder decoder;
   private final String field;
   private final int hashCode;
-  private final boolean useDirectSource;
   private final BytesRef bytes = new BytesRef(32);
   
-  private DocValues.Source current;
+  private BinaryDocValues current;
   
   /**
-   * Constructs a new {@link DocValuesCategoryListIterator} which uses an
-   * in-memory {@link Source}.
+   * Constructs a new {@link DocValuesCategoryListIterator}.
    */
   public DocValuesCategoryListIterator(String field, IntDecoder decoder) {
-    this(field, decoder, false);
-  }
-  
-  /**
-   * Constructs a new {@link DocValuesCategoryListIterator} which uses either a
-   * {@link DocValues#getDirectSource() direct source} or
-   * {@link DocValues#getSource() in-memory} one.
-   */
-  public DocValuesCategoryListIterator(String field, IntDecoder decoder, boolean useDirectSource) {
     this.field = field;
     this.decoder = decoder;
     this.hashCode = field.hashCode();
-    this.useDirectSource = useDirectSource;
   }
   
   @Override
@@ -78,19 +65,13 @@
   
   @Override
   public boolean setNextReader(AtomicReaderContext context) throws IOException {
-    DocValues dv = context.reader().docValues(field);
-    if (dv == null) {
-      current = null;
-      return false;
-    }
-    
-    current = useDirectSource ? dv.getDirectSource() : dv.getSource();
-    return true;
+    current = context.reader().getBinaryDocValues(field);
+    return current != null;
   }
   
   @Override
   public void getOrdinals(int docID, IntsRef ints) throws IOException {
-    current.getBytes(docID, bytes);
+    current.get(docID, bytes);
     ints.length = 0;
     if (bytes.length > 0) {
       decoder.decode(bytes, ints);
Index: lucene/facet/src/java/org/apache/lucene/facet/search/CountingFacetsCollector.java
===================================================================
--- lucene/facet/src/java/org/apache/lucene/facet/search/CountingFacetsCollector.java	(revision 1442822)
+++ lucene/facet/src/java/org/apache/lucene/facet/search/CountingFacetsCollector.java	(working copy)
@@ -13,17 +13,16 @@
 import org.apache.lucene.facet.index.params.CategoryListParams.OrdinalPolicy;
 import org.apache.lucene.facet.index.params.FacetIndexingParams;
 import org.apache.lucene.facet.search.params.CountFacetRequest;
-import org.apache.lucene.facet.search.params.FacetRequest;
 import org.apache.lucene.facet.search.params.FacetRequest.SortBy;
 import org.apache.lucene.facet.search.params.FacetRequest.SortOrder;
+import org.apache.lucene.facet.search.params.FacetRequest;
 import org.apache.lucene.facet.search.params.FacetSearchParams;
 import org.apache.lucene.facet.search.results.FacetResult;
 import org.apache.lucene.facet.search.results.FacetResultNode;
 import org.apache.lucene.facet.taxonomy.TaxonomyReader;
 import org.apache.lucene.facet.taxonomy.directory.ParallelTaxonomyArrays;
 import org.apache.lucene.index.AtomicReaderContext;
-import org.apache.lucene.index.DocValues;
-import org.apache.lucene.index.DocValues.Source;
+import org.apache.lucene.index.BinaryDocValues;
 import org.apache.lucene.search.Collector;
 import org.apache.lucene.search.Scorer;
 import org.apache.lucene.util.BytesRef;
@@ -66,12 +65,9 @@
  * </ul>
  * 
  * <p>
- * <b>NOTE:</b> this colletro uses {@link DocValues#getSource()} by default,
+ * <b>NOTE:</b> this collector uses {@link BinaryDocValues} by default,
  * which pre-loads the values into memory. If your application cannot afford the
- * RAM, you should use
- * {@link #CountingFacetsCollector(FacetSearchParams, TaxonomyReader, FacetArrays, boolean)}
- * and specify to use a direct source (corresponds to
- * {@link DocValues#getDirectSource()}).
+ * RAM, you should pick a codec which keeps the values (or parts of them) on disk.
  * 
  * <p>
  * <b>NOTE:</b> this collector supports category lists that were indexed with
@@ -91,18 +87,16 @@
   private final FacetArrays facetArrays;
   private final int[] counts;
   private final String facetsField;
-  private final boolean useDirectSource;
-  private final HashMap<Source,FixedBitSet> matchingDocs = new HashMap<Source,FixedBitSet>();
+  private final HashMap<BinaryDocValues,FixedBitSet> matchingDocs = new HashMap<BinaryDocValues,FixedBitSet>();
   
-  private DocValues facetsValues;
+  private BinaryDocValues facetsValues;
   private FixedBitSet bits;
   
   public CountingFacetsCollector(FacetSearchParams fsp, TaxonomyReader taxoReader) {
-    this(fsp, taxoReader, new FacetArrays(taxoReader.getSize()), false);
+    this(fsp, taxoReader, new FacetArrays(taxoReader.getSize()));
   }
   
-  public CountingFacetsCollector(FacetSearchParams fsp, TaxonomyReader taxoReader, FacetArrays facetArrays, 
-      boolean useDirectSource) {
+  public CountingFacetsCollector(FacetSearchParams fsp, TaxonomyReader taxoReader, FacetArrays facetArrays) {
     assert facetArrays.arrayLength >= taxoReader.getSize() : "too small facet array";
     assert assertParams(fsp) == null : assertParams(fsp);
     
@@ -112,7 +106,6 @@
     this.taxoReader = taxoReader;
     this.facetArrays = facetArrays;
     this.counts = facetArrays.getIntArray();
-    this.useDirectSource = useDirectSource;
   }
   
   /**
@@ -169,11 +162,10 @@
   
   @Override
   public void setNextReader(AtomicReaderContext context) throws IOException {
-    facetsValues = context.reader().docValues(facetsField);
+    facetsValues = context.reader().getBinaryDocValues(facetsField);
     if (facetsValues != null) {
-      Source facetSource = useDirectSource ? facetsValues.getDirectSource() : facetsValues.getSource();
       bits = new FixedBitSet(context.reader().maxDoc());
-      matchingDocs.put(facetSource, bits);
+      matchingDocs.put(facetsValues, bits);
     }
   }
   
@@ -187,13 +179,13 @@
   }
   
   private void countFacets() {
-    for (Entry<Source,FixedBitSet> entry : matchingDocs.entrySet()) {
-      Source facetsSource = entry.getKey();
+    for (Entry<BinaryDocValues,FixedBitSet> entry : matchingDocs.entrySet()) {
+      BinaryDocValues facetsSource = entry.getKey();
       FixedBitSet bits = entry.getValue();
       int doc = 0;
       int length = bits.length();
       while (doc < length && (doc = bits.nextSetBit(doc)) != -1) {
-        facetsSource .getBytes(doc, buf);
+        facetsSource.get(doc, buf);
         if (buf.length > 0) {
           // this document has facets
           int upto = buf.offset + buf.length;
Index: lucene/facet/src/java/org/apache/lucene/facet/index/FacetFields.java
===================================================================
--- lucene/facet/src/java/org/apache/lucene/facet/index/FacetFields.java	(revision 1442822)
+++ lucene/facet/src/java/org/apache/lucene/facet/index/FacetFields.java	(working copy)
@@ -8,10 +8,10 @@
 import java.util.Map;
 import java.util.Map.Entry;
 
+import org.apache.lucene.document.BinaryDocValuesField;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.FieldType;
-import org.apache.lucene.document.StraightBytesDocValuesField;
 import org.apache.lucene.document.TextField;
 import org.apache.lucene.facet.index.params.CategoryListParams;
 import org.apache.lucene.facet.index.params.FacetIndexingParams;
@@ -154,7 +154,7 @@
    */
   protected void addCountingListData(Document doc, Map<String,BytesRef> categoriesData, String field) {
     for (Entry<String,BytesRef> entry : categoriesData.entrySet()) {
-      doc.add(new StraightBytesDocValuesField(field + entry.getKey(), entry.getValue()));
+      doc.add(new BinaryDocValuesField(field + entry.getKey(), entry.getValue()));
     }
   }
   
Index: lucene/facet/src/java/org/apache/lucene/facet/index/FacetsPayloadMigrationReader.java
===================================================================
--- lucene/facet/src/java/org/apache/lucene/facet/index/FacetsPayloadMigrationReader.java	(revision 1442822)
+++ lucene/facet/src/java/org/apache/lucene/facet/index/FacetsPayloadMigrationReader.java	(working copy)
@@ -27,11 +27,10 @@
 import org.apache.lucene.facet.index.params.FacetIndexingParams;
 import org.apache.lucene.index.AtomicReader;
 import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.BinaryDocValues;
 import org.apache.lucene.index.DirectoryReader;
-import org.apache.lucene.index.DocValues;
-import org.apache.lucene.index.DocValues.Source;
-import org.apache.lucene.index.DocValues.Type;
 import org.apache.lucene.index.DocsAndPositionsEnum;
+import org.apache.lucene.index.FieldInfo.DocValuesType;
 import org.apache.lucene.index.FieldInfo;
 import org.apache.lucene.index.FieldInfos;
 import org.apache.lucene.index.Fields;
@@ -45,9 +44,9 @@
 
 /**
  * A {@link FilterAtomicReader} for migrating a facets index which encodes
- * category ordinals in a payload to {@link DocValues}. To migrate the index,
+ * category ordinals in a payload to {@link BinaryDocValues}. To migrate the index,
  * you should build a mapping from a field (String) to term ({@link Term}),
- * which denotes under which DocValues field to put the data encoded in the
+ * which denotes under which BinaryDocValues field to put the data encoded in the
  * matching term's payload. You can follow the code example below to migrate an
  * existing index:
  * 
@@ -76,39 +75,37 @@
  */
 public class FacetsPayloadMigrationReader extends FilterAtomicReader {  
 
-  private class PayloadMigratingDocValues extends DocValues {
+  private class PayloadMigratingBinaryDocValues extends BinaryDocValues {
 
-    private final DocsAndPositionsEnum dpe;
-    
-    public PayloadMigratingDocValues(DocsAndPositionsEnum dpe) {
-      this.dpe = dpe;
-    }
+    private Fields fields;
+    private Term term;
+    private DocsAndPositionsEnum dpe;
+    private int curDocID = -1;
+    private int lastRequestedDocID;
 
-    @Override
-    protected Source loadDirectSource() throws IOException {
-      return new PayloadMigratingSource(getType(), dpe);
+    private DocsAndPositionsEnum getDPE() {
+      try {
+        DocsAndPositionsEnum dpe = null;
+        if (fields != null) {
+          Terms terms = fields.terms(term.field());
+          if (terms != null) {
+            TermsEnum te = terms.iterator(null); // no use for reusing
+            if (te.seekExact(term.bytes(), true)) {
+              // we're not expected to be called for deleted documents
+              dpe = te.docsAndPositions(null, null, DocsAndPositionsEnum.FLAG_PAYLOADS);
+            }
+          }
+        }
+        return dpe;
+      } catch (IOException ioe) {
+        throw new RuntimeException(ioe);
+      }
     }
-
-    @Override
-    protected Source loadSource() throws IOException {
-      throw new UnsupportedOperationException("in-memory Source is not supported by this reader");
-    }
-
-    @Override
-    public Type getType() {
-      return Type.BYTES_VAR_STRAIGHT;
-    }
     
-  }
-  
-  private class PayloadMigratingSource extends Source {
-
-    private final DocsAndPositionsEnum dpe;
-    private int curDocID;
-    
-    protected PayloadMigratingSource(Type type, DocsAndPositionsEnum dpe) {
-      super(type);
-      this.dpe = dpe;
+    protected PayloadMigratingBinaryDocValues(Fields fields, Term term) {
+      this.fields = fields;
+      this.term = term;
+      this.dpe = getDPE();
       if (dpe == null) {
         curDocID = DocIdSetIterator.NO_MORE_DOCS;
       } else {
@@ -121,31 +118,41 @@
     }
     
     @Override
-    public BytesRef getBytes(int docID, BytesRef ref) {
-      if (curDocID > docID) {
-        // document does not exist
-        ref.length = 0;
-        return ref;
-      }
+    public void get(int docID, BytesRef result) {
+      try {
+        // If caller is moving backwards (eg, during merge,
+        // the consuming DV format is free to iterate over
+        // our values as many times as it wants), we must
+        // re-init the dpe:
+        if (docID <= lastRequestedDocID) {
+          dpe = getDPE();
+          if (dpe == null) {
+            curDocID = DocIdSetIterator.NO_MORE_DOCS;
+          } else{
+            curDocID = dpe.nextDoc();
+          }
+        }
+        lastRequestedDocID = docID;
+        if (curDocID > docID) {
+          // document does not exist
+          result.length = 0;
+          return;
+        }
       
-      try {
         if (curDocID < docID) {
           curDocID = dpe.advance(docID);
           if (curDocID != docID) { // requested document does not have a payload
-            ref.length = 0;
-            return ref;
+            result.length = 0;
+            return;
           }
         }
         
-        // we're on the document
         dpe.nextPosition();
-        ref.copyBytes(dpe.getPayload());
-        return ref;
+        result.copyBytes(dpe.getPayload());
       } catch (IOException e) {
         throw new RuntimeException(e);
       }
     }
-    
   }
   
   /** The {@link Term} text of the ordinals payload. */
@@ -195,7 +202,7 @@
   private final Map<String,Term> fieldTerms;
   
   /**
-   * Wraps an {@link AtomicReader} and migrates the payload to {@link DocValues}
+   * Wraps an {@link AtomicReader} and migrates the payload to {@link BinaryDocValues}
    * fields by using the given mapping.
    */
   public FacetsPayloadMigrationReader(AtomicReader in, Map<String,Term> fieldTerms) {
@@ -204,26 +211,14 @@
   }
   
   @Override
-  public DocValues docValues(String field) throws IOException {
+  public BinaryDocValues getBinaryDocValues(String field) throws IOException {
     Term term = fieldTerms.get(field);
     if (term == null) {
-      return super.docValues(field);
+      return super.getBinaryDocValues(field);
     } else {
-      DocsAndPositionsEnum dpe = null;
-      Fields fields = fields();
-      if (fields != null) {
-        Terms terms = fields.terms(term.field());
-        if (terms != null) {
-          TermsEnum te = terms.iterator(null); // no use for reusing
-          if (te.seekExact(term.bytes(), true)) {
-            // we're not expected to be called for deleted documents
-            dpe = te.docsAndPositions(null, null, DocsAndPositionsEnum.FLAG_PAYLOADS);
-          }
-        }
-      }
       // we shouldn't return null, even if the term does not exist or has no
       // payloads, since we already marked the field as having DocValues.
-      return new PayloadMigratingDocValues(dpe);
+      return new PayloadMigratingBinaryDocValues(fields(), term);
     }
   }
 
@@ -240,7 +235,7 @@
         // mark this field as having a DocValues
         infos.add(new FieldInfo(info.name, true, info.number,
             info.hasVectors(), info.omitsNorms(), info.hasPayloads(),
-            info.getIndexOptions(), Type.BYTES_VAR_STRAIGHT,
+            info.getIndexOptions(), DocValuesType.BINARY,
             info.getNormType(), info.attributes()));
         leftoverFields.remove(info.name);
       } else {
@@ -250,9 +245,8 @@
     }
     for (String field : leftoverFields) {
       infos.add(new FieldInfo(field, false, ++number, false, false, false,
-          null, Type.BYTES_VAR_STRAIGHT, null, null));
+          null, DocValuesType.BINARY, null, null));
     }
     return new FieldInfos(infos.toArray(new FieldInfo[infos.size()]));
   }
-  
 }
Index: lucene/facet/src/java/org/apache/lucene/facet/index/OrdinalMappingAtomicReader.java
===================================================================
--- lucene/facet/src/java/org/apache/lucene/facet/index/OrdinalMappingAtomicReader.java	(revision 1442822)
+++ lucene/facet/src/java/org/apache/lucene/facet/index/OrdinalMappingAtomicReader.java	(working copy)
@@ -25,9 +25,7 @@
 import org.apache.lucene.facet.index.params.FacetIndexingParams;
 import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter.OrdinalMap;
 import org.apache.lucene.index.AtomicReader;
-import org.apache.lucene.index.DocValues;
-import org.apache.lucene.index.DocValues.Source;
-import org.apache.lucene.index.DocValues.Type;
+import org.apache.lucene.index.BinaryDocValues;
 import org.apache.lucene.index.FilterAtomicReader;
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.IntsRef;
@@ -92,8 +90,8 @@
   }
 
   @Override
-  public DocValues docValues(String field) throws IOException {
-    DocValues inner = super.docValues(field);
+  public BinaryDocValues getBinaryDocValues(String field) throws IOException {
+    BinaryDocValues inner = super.getBinaryDocValues(field);
     if (inner == null) {
       return inner;
     }
@@ -102,46 +100,19 @@
     if (clp == null) {
       return inner;
     } else {
-      return new OrdinalMappingDocValues(inner, clp);
+      return new OrdinalMappingBinaryDocValues(clp, inner);
     }
   }
   
-  private class OrdinalMappingDocValues extends DocValues {
+  private class OrdinalMappingBinaryDocValues extends BinaryDocValues {
 
-    private final CategoryListParams clp;
-    private final DocValues delegate;
-    
-    public OrdinalMappingDocValues(DocValues delegate, CategoryListParams clp) {
-      this.delegate = delegate;
-      this.clp = clp;
-    }
-
-    @Override
-    protected Source loadSource() throws IOException {
-      return new OrdinalMappingSource(getType(), clp, delegate.getSource());
-    }
-
-    @Override
-    protected Source loadDirectSource() throws IOException {
-      return new OrdinalMappingSource(getType(), clp, delegate.getDirectSource());
-    }
-
-    @Override
-    public Type getType() {
-      return Type.BYTES_VAR_STRAIGHT;
-    }
-    
-  }
-  
-  private class OrdinalMappingSource extends Source {
-
     private final IntEncoder encoder;
     private final IntDecoder decoder;
     private final IntsRef ordinals = new IntsRef(32);
-    private final Source delegate;
+    private final BinaryDocValues delegate;
+    private final BytesRef scratch = new BytesRef();
     
-    protected OrdinalMappingSource(Type type, CategoryListParams clp, Source delegate) {
-      super(type);
+    protected OrdinalMappingBinaryDocValues(CategoryListParams clp, BinaryDocValues delegate) {
       this.delegate = delegate;
       encoder = clp.createEncoder();
       decoder = encoder.createMatchingDecoder();
@@ -149,23 +120,26 @@
     
     @SuppressWarnings("synthetic-access")
     @Override
-    public BytesRef getBytes(int docID, BytesRef ref) {
-      ref = delegate.getBytes(docID, ref);
-      if (ref == null || ref.length == 0) {
-        return ref;
-      } else {
-        decoder.decode(ref, ordinals);
+    public void get(int docID, BytesRef result) {
+      // NOTE: this isn't quite koscher, because in general
+      // multiple threads can call BinaryDV.get which would
+      // then conflict on the single scratch instance, but
+      // because this impl is only used for merging, we know
+      // only 1 thread calls us:
+      delegate.get(docID, scratch);
+      if (scratch.length > 0) {
+        // We must use scratch (and not re-use result) here,
+        // else encoder may overwrite the DV provider's
+        // private byte[]:
+        decoder.decode(scratch, ordinals);
         
         // map the ordinals
         for (int i = 0; i < ordinals.length; i++) {
           ordinals.ints[i] = ordinalMap[ordinals.ints[i]];
         }
         
-        encoder.encode(ordinals, ref);
-        return ref;
+        encoder.encode(ordinals, result);
       }
     }
-    
   }
-  
 }
Index: lucene/core/src/test/org/apache/lucene/codecs/lucene40/TestLucene40TermVectorsFormat.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/codecs/lucene40/TestLucene40TermVectorsFormat.java	(revision 1442822)
+++ lucene/core/src/test/org/apache/lucene/codecs/lucene40/TestLucene40TermVectorsFormat.java	(working copy)
@@ -24,7 +24,7 @@
 
   @Override
   protected Codec getCodec() {
-    return new Lucene40Codec();
+    return new Lucene40RWCodec();
   }
   
 }
Index: lucene/core/src/test/org/apache/lucene/codecs/lucene40/TestLucene40DocValuesFormat.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/codecs/lucene40/TestLucene40DocValuesFormat.java	(revision 0)
+++ lucene/core/src/test/org/apache/lucene/codecs/lucene40/TestLucene40DocValuesFormat.java	(working copy)
@@ -0,0 +1,34 @@
+package org.apache.lucene.codecs.lucene40;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.codecs.Codec;
+import org.apache.lucene.index.BaseDocValuesFormatTestCase;
+
+/**
+ * Tests Lucene40DocValuesFormat
+ */
+public class TestLucene40DocValuesFormat extends BaseDocValuesFormatTestCase {
+  private final Codec codec = new Lucene40RWCodec();
+
+  @Override
+  protected Codec getCodec() {
+    return codec;
+  }
+  
+}

Property changes on: lucene/core/src/test/org/apache/lucene/codecs/lucene40/TestLucene40DocValuesFormat.java
___________________________________________________________________
Added: svn:eol-style
## -0,0 +1 ##
+native
Index: lucene/core/src/test/org/apache/lucene/codecs/lucene40/values/TestDocValues.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/codecs/lucene40/values/TestDocValues.java	(revision 1442822)
+++ lucene/core/src/test/org/apache/lucene/codecs/lucene40/values/TestDocValues.java	(working copy)
@@ -1,476 +0,0 @@
-package org.apache.lucene.codecs.lucene40.values;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.io.Reader;
-import java.util.Comparator;
-import java.util.Random;
-
-import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.codecs.DocValuesConsumer;
-import org.apache.lucene.codecs.lucene40.values.Bytes;
-import org.apache.lucene.codecs.lucene40.values.Floats;
-import org.apache.lucene.codecs.lucene40.values.Ints;
-import org.apache.lucene.document.FieldType;
-import org.apache.lucene.index.DocValues.SortedSource;
-import org.apache.lucene.index.DocValues.Source;
-import org.apache.lucene.index.DocValues.Type;
-import org.apache.lucene.index.DocValues;
-import org.apache.lucene.index.IndexableField;
-import org.apache.lucene.index.IndexableFieldType;
-import org.apache.lucene.index.StorableField;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.Counter;
-import org.apache.lucene.util.LuceneTestCase;
-import org.apache.lucene.util.UnicodeUtil;
-import org.apache.lucene.util._TestUtil;
-import org.apache.lucene.util.packed.PackedInts;
-
-// TODO: some of this should be under lucene40 codec tests? is talking to codec directly?f
-public class TestDocValues extends LuceneTestCase {
-  private static final Comparator<BytesRef> COMP = BytesRef.getUTF8SortedAsUnicodeComparator();
-  // TODO -- for sorted test, do our own Sort of the
-  // values and verify it's identical
-
-  public void testBytesStraight() throws IOException {
-    runTestBytes(Bytes.Mode.STRAIGHT, true);
-    runTestBytes(Bytes.Mode.STRAIGHT, false);
-  }
-
-  public void testBytesDeref() throws IOException {
-    runTestBytes(Bytes.Mode.DEREF, true);
-    runTestBytes(Bytes.Mode.DEREF, false);
-  }
-  
-  public void testBytesSorted() throws IOException {
-    runTestBytes(Bytes.Mode.SORTED, true);
-    runTestBytes(Bytes.Mode.SORTED, false);
-  }
-
-  public void runTestBytes(final Bytes.Mode mode, final boolean fixedSize)
-      throws IOException {
-    DocValueHolder valueHolder = new DocValueHolder();
-    valueHolder.comp = COMP;
-    final BytesRef bytesRef = new BytesRef();
-
-    Directory dir = newDirectory();
-    final Counter trackBytes = Counter.newCounter();
-    DocValuesConsumer w = Bytes.getWriter(dir, "test", mode, fixedSize, COMP, trackBytes, newIOContext(random()),
-        random().nextFloat() * PackedInts.FAST);
-    int maxDoc = 220;
-    final String[] values = new String[maxDoc];
-    final int fixedLength = 1 + atLeast(50);
-    for (int i = 0; i < 100; i++) {
-      final String s;
-      if (i > 0 && random().nextInt(5) <= 2) {
-        // use prior value
-        s = values[2 * random().nextInt(i)];
-      } else {
-        s = _TestUtil.randomFixedByteLengthUnicodeString(random(), fixedSize? fixedLength : 1 + random().nextInt(39));
-      }
-      values[2 * i] = s;
-
-      UnicodeUtil.UTF16toUTF8(s, 0, s.length(), bytesRef);
-      valueHolder.bytes = bytesRef;
-      w.add(2 * i, valueHolder);
-    }
-    w.finish(maxDoc);
-    assertEquals(0, trackBytes.get());
-
-    DocValues r = Bytes.getValues(dir, "test", mode, fixedSize, maxDoc, COMP, newIOContext(random()));
-
-    // Verify we can load source twice:
-    for (int iter = 0; iter < 2; iter++) {
-      Source s;
-      DocValues.SortedSource ss;
-      if (mode == Bytes.Mode.SORTED) {
-        // default is unicode so we can simply pass null here
-        s = ss = getSortedSource(r);  
-      } else {
-        s = getSource(r);
-        ss = null;
-      }
-      for (int i = 0; i < 100; i++) {
-        final int idx = 2 * i;
-        assertNotNull("doc " + idx + "; value=" + values[idx], s.getBytes(idx,
-            bytesRef));
-        assertEquals("doc " + idx, values[idx], s.getBytes(idx, bytesRef)
-            .utf8ToString());
-        if (ss != null) {
-          assertEquals("doc " + idx, values[idx], ss.getByOrd(ss.ord(idx),
-              bytesRef).utf8ToString());
-         int ord = ss
-              .getOrdByValue(new BytesRef(values[idx]), new BytesRef());
-          assertTrue(ord >= 0);
-          assertEquals(ss.ord(idx), ord);
-        }
-      }
-
-      // Lookup random strings:
-      if (mode == Bytes.Mode.SORTED) {
-        final int valueCount = ss.getValueCount();
-        Random random = random();
-        for (int i = 0; i < 1000; i++) {
-          BytesRef bytesValue = new BytesRef(_TestUtil.randomFixedByteLengthUnicodeString(random, fixedSize? fixedLength : 1 + random.nextInt(39)));
-          int ord = ss.getOrdByValue(bytesValue, new BytesRef());
-          if (ord >= 0) {
-            assertTrue(bytesValue
-                .bytesEquals(ss.getByOrd(ord, bytesRef)));
-            int count = 0;
-            for (int k = 0; k < 100; k++) {
-              if (bytesValue.utf8ToString().equals(values[2 * k])) {
-                assertEquals(ss.ord(2 * k), ord);
-                count++;
-              }
-            }
-            assertTrue(count > 0);
-          } else {
-            assert ord < 0;
-            int insertIndex = (-ord)-1;
-            if (insertIndex == 0) {
-              final BytesRef firstRef = ss.getByOrd(1, bytesRef);
-              // random string was before our first
-              assertTrue(firstRef.compareTo(bytesValue) > 0);
-            } else if (insertIndex == valueCount) {
-              final BytesRef lastRef = ss.getByOrd(valueCount-1, bytesRef);
-              // random string was after our last
-              assertTrue(lastRef.compareTo(bytesValue) < 0);
-            } else {
-              // TODO: I don't think this actually needs a deep copy?
-              final BytesRef before = BytesRef.deepCopyOf(ss.getByOrd(insertIndex-1, bytesRef));
-              BytesRef after = ss.getByOrd(insertIndex, bytesRef);
-              assertTrue(COMP.compare(before, bytesValue) < 0);
-              assertTrue(COMP.compare(bytesValue, after) < 0);
-            }
-          }
-        }
-      }
-    }
-
-
-    r.close();
-    dir.close();
-  }
-
-  public void testVariableIntsLimits() throws IOException {
-    long[][] minMax = new long[][] { { Long.MIN_VALUE, Long.MAX_VALUE },
-        { Long.MIN_VALUE + 1, 1 }, { -1, Long.MAX_VALUE },
-        { Long.MIN_VALUE, -1 }, { 1, Long.MAX_VALUE },
-        { -1, Long.MAX_VALUE - 1 }, { Long.MIN_VALUE + 2, 1 }, };
-    Type[] expectedTypes = new Type[] { Type.FIXED_INTS_64,
-        Type.FIXED_INTS_64, Type.FIXED_INTS_64,
-        Type.FIXED_INTS_64, Type.VAR_INTS, Type.VAR_INTS,
-        Type.VAR_INTS, };
-    DocValueHolder valueHolder = new DocValueHolder();
-    for (int i = 0; i < minMax.length; i++) {
-      Directory dir = newDirectory();
-      final Counter trackBytes = Counter.newCounter();
-      DocValuesConsumer w = Ints.getWriter(dir, "test", trackBytes, Type.VAR_INTS, newIOContext(random()));
-      valueHolder.numberValue = minMax[i][0];
-      w.add(0, valueHolder);
-      valueHolder.numberValue = minMax[i][1];
-      w.add(1, valueHolder);
-      w.finish(2);
-      assertEquals(0, trackBytes.get());
-      DocValues r = Ints.getValues(dir, "test", 2,  Type.VAR_INTS, newIOContext(random()));
-      Source source = getSource(r);
-      assertEquals(i + " with min: " + minMax[i][0] + " max: " + minMax[i][1],
-          expectedTypes[i], source.getType());
-      assertEquals(minMax[i][0], source.getInt(0));
-      assertEquals(minMax[i][1], source.getInt(1));
-
-      r.close();
-      dir.close();
-    }
-  }
-  
-  public void testVInts() throws IOException {
-    testInts(Type.VAR_INTS, 63);
-  }
-  
-  public void testFixedInts() throws IOException {
-    testInts(Type.FIXED_INTS_64, 63);
-    testInts(Type.FIXED_INTS_32, 31);
-    testInts(Type.FIXED_INTS_16, 15);
-    testInts(Type.FIXED_INTS_8, 7);
-
-  }
-  
-  public void testGetInt8Array() throws IOException {
-    DocValueHolder valueHolder = new DocValueHolder();
-    byte[] sourceArray = new byte[] {1,2,3};
-    Directory dir = newDirectory();
-    final Counter trackBytes = Counter.newCounter();
-    DocValuesConsumer w = Ints.getWriter(dir, "test", trackBytes, Type.FIXED_INTS_8, newIOContext(random()));
-    for (int i = 0; i < sourceArray.length; i++) {
-      valueHolder.numberValue = (long) sourceArray[i];
-      w.add(i, valueHolder);
-    }
-    w.finish(sourceArray.length);
-    DocValues r = Ints.getValues(dir, "test", sourceArray.length, Type.FIXED_INTS_8, newIOContext(random()));
-    Source source = r.getSource();
-    assertTrue(source.hasArray());
-    byte[] loaded = ((byte[])source.getArray());
-    assertEquals(loaded.length, sourceArray.length);
-    for (int i = 0; i < loaded.length; i++) {
-      assertEquals("value didn't match at index " + i, sourceArray[i], loaded[i]);
-    }
-    r.close();
-    dir.close();
-  }
-  
-  public void testGetInt16Array() throws IOException {
-    DocValueHolder valueHolder = new DocValueHolder();
-    short[] sourceArray = new short[] {1,2,3};
-    Directory dir = newDirectory();
-    final Counter trackBytes = Counter.newCounter();
-    DocValuesConsumer w = Ints.getWriter(dir, "test", trackBytes, Type.FIXED_INTS_16, newIOContext(random()));
-    for (int i = 0; i < sourceArray.length; i++) {
-      valueHolder.numberValue = (long) sourceArray[i];
-      w.add(i, valueHolder);
-    }
-    w.finish(sourceArray.length);
-    DocValues r = Ints.getValues(dir, "test", sourceArray.length, Type.FIXED_INTS_16, newIOContext(random()));
-    Source source = r.getSource();
-    assertTrue(source.hasArray());
-    short[] loaded = ((short[])source.getArray());
-    assertEquals(loaded.length, sourceArray.length);
-    for (int i = 0; i < loaded.length; i++) {
-      assertEquals("value didn't match at index " + i, sourceArray[i], loaded[i]);
-    }
-    r.close();
-    dir.close();
-  }
-  
-  public void testGetInt64Array() throws IOException {
-    DocValueHolder valueHolder = new DocValueHolder();
-    long[] sourceArray = new long[] {1,2,3};
-    Directory dir = newDirectory();
-    final Counter trackBytes = Counter.newCounter();
-    DocValuesConsumer w = Ints.getWriter(dir, "test", trackBytes, Type.FIXED_INTS_64, newIOContext(random()));
-    for (int i = 0; i < sourceArray.length; i++) {
-      valueHolder.numberValue = sourceArray[i];
-      w.add(i, valueHolder);
-    }
-    w.finish(sourceArray.length);
-    DocValues r = Ints.getValues(dir, "test", sourceArray.length, Type.FIXED_INTS_64, newIOContext(random()));
-    Source source = r.getSource();
-    assertTrue(source.hasArray());
-    long[] loaded = ((long[])source.getArray());
-    assertEquals(loaded.length, sourceArray.length);
-    for (int i = 0; i < loaded.length; i++) {
-      assertEquals("value didn't match at index " + i, sourceArray[i], loaded[i]);
-    }
-    r.close();
-    dir.close();
-  }
-  
-  public void testGetInt32Array() throws IOException {
-    DocValueHolder valueHolder = new DocValueHolder();
-    int[] sourceArray = new int[] {1,2,3};
-    Directory dir = newDirectory();
-    final Counter trackBytes = Counter.newCounter();
-    DocValuesConsumer w = Ints.getWriter(dir, "test", trackBytes, Type.FIXED_INTS_32, newIOContext(random()));
-    for (int i = 0; i < sourceArray.length; i++) {
-      valueHolder.numberValue = (long) sourceArray[i];
-      w.add(i, valueHolder);
-    }
-    w.finish(sourceArray.length);
-    DocValues r = Ints.getValues(dir, "test", sourceArray.length, Type.FIXED_INTS_32, newIOContext(random()));
-    Source source = r.getSource();
-    assertTrue(source.hasArray());
-    int[] loaded = ((int[])source.getArray());
-    assertEquals(loaded.length, sourceArray.length);
-    for (int i = 0; i < loaded.length; i++) {
-      assertEquals("value didn't match at index " + i, sourceArray[i], loaded[i]);
-    }
-    r.close();
-    dir.close();
-  }
-  
-  public void testGetFloat32Array() throws IOException {
-    DocValueHolder valueHolder = new DocValueHolder();
-    float[] sourceArray = new float[] {1,2,3};
-    Directory dir = newDirectory();
-    final Counter trackBytes = Counter.newCounter();
-    DocValuesConsumer w = Floats.getWriter(dir, "test", trackBytes, newIOContext(random()), Type.FLOAT_32);
-    for (int i = 0; i < sourceArray.length; i++) {
-      valueHolder.numberValue = sourceArray[i];
-      w.add(i, valueHolder);
-    }
-    w.finish(sourceArray.length);
-    DocValues r = Floats.getValues(dir, "test", 3, newIOContext(random()), Type.FLOAT_32);
-    Source source = r.getSource();
-    assertTrue(source.hasArray());
-    float[] loaded = ((float[])source.getArray());
-    assertEquals(loaded.length, sourceArray.length);
-    for (int i = 0; i < loaded.length; i++) {
-      assertEquals("value didn't match at index " + i, sourceArray[i], loaded[i], 0.0f);
-    }
-    r.close();
-    dir.close();
-  }
-  
-  public void testGetFloat64Array() throws IOException {
-    DocValueHolder valueHolder = new DocValueHolder();
-    double[] sourceArray = new double[] {1,2,3};
-    Directory dir = newDirectory();
-    final Counter trackBytes = Counter.newCounter();
-    DocValuesConsumer w = Floats.getWriter(dir, "test", trackBytes, newIOContext(random()), Type.FLOAT_64);
-    for (int i = 0; i < sourceArray.length; i++) {
-      valueHolder.numberValue = sourceArray[i];
-      w.add(i, valueHolder);
-    }
-    w.finish(sourceArray.length);
-    DocValues r = Floats.getValues(dir, "test", 3, newIOContext(random()), Type.FLOAT_64);
-    Source source = r.getSource();
-    assertTrue(source.hasArray());
-    double[] loaded = ((double[])source.getArray());
-    assertEquals(loaded.length, sourceArray.length);
-    for (int i = 0; i < loaded.length; i++) {
-      assertEquals("value didn't match at index " + i, sourceArray[i], loaded[i], 0.0d);
-    }
-    r.close();
-    dir.close();
-  }
-
-  private void testInts(Type type, int maxBit) throws IOException {
-    DocValueHolder valueHolder = new DocValueHolder();
-    long maxV = 1;
-    final int NUM_VALUES = 333 + random().nextInt(333);
-    final long[] values = new long[NUM_VALUES];
-    for (int rx = 1; rx < maxBit; rx++, maxV *= 2) {
-      Directory dir = newDirectory();
-      final Counter trackBytes = Counter.newCounter();
-      DocValuesConsumer w = Ints.getWriter(dir, "test", trackBytes, type, newIOContext(random()));
-      for (int i = 0; i < NUM_VALUES; i++) {
-        final long v = _TestUtil.nextLong(random(), -maxV, maxV);
-        valueHolder.numberValue = values[i] = v;
-        w.add(i, valueHolder);
-      }
-      final int additionalDocs = 1 + random().nextInt(9);
-      w.finish(NUM_VALUES + additionalDocs);
-      assertEquals(0, trackBytes.get());
-
-      DocValues r = Ints.getValues(dir, "test", NUM_VALUES + additionalDocs, type, newIOContext(random()));
-      for (int iter = 0; iter < 2; iter++) {
-        Source s = getSource(r);
-        assertEquals(type, s.getType());
-        for (int i = 0; i < NUM_VALUES; i++) {
-          final long v = s.getInt(i);
-          assertEquals("index " + i, values[i], v);
-        }
-      }
-
-      r.close();
-      dir.close();
-    }
-  }
-
-  public void testFloats4() throws IOException {
-    runTestFloats(Type.FLOAT_32);
-  }
-
-  private void runTestFloats(Type type) throws IOException {
-    DocValueHolder valueHolder = new DocValueHolder();
-    Directory dir = newDirectory();
-    final Counter trackBytes = Counter.newCounter();
-    DocValuesConsumer w = Floats.getWriter(dir, "test", trackBytes, newIOContext(random()), type);
-    final int NUM_VALUES = 777 + random().nextInt(777);
-    final double[] values = new double[NUM_VALUES];
-    for (int i = 0; i < NUM_VALUES; i++) {
-      final double v = type == Type.FLOAT_32 ? random().nextFloat() : random()
-          .nextDouble();
-      valueHolder.numberValue = values[i] = v;
-      w.add(i, valueHolder);
-    }
-    final int additionalValues = 1 + random().nextInt(10);
-    w.finish(NUM_VALUES + additionalValues);
-    assertEquals(0, trackBytes.get());
-
-    DocValues r = Floats.getValues(dir, "test", NUM_VALUES + additionalValues, newIOContext(random()), type);
-    for (int iter = 0; iter < 2; iter++) {
-      Source s = getSource(r);
-      for (int i = 0; i < NUM_VALUES; i++) {
-        assertEquals("" + i, values[i], s.getFloat(i), 0.0f);
-      }
-    }
-    r.close();
-    dir.close();
-  }
-
-  public void testFloats8() throws IOException {
-    runTestFloats(Type.FLOAT_64);
-  }
-  
-
-  private Source getSource(DocValues values) throws IOException {
-    // getSource uses cache internally
-    switch(random().nextInt(5)) {
-    case 2:
-      return values.getDirectSource();
-    case 1:
-      return values.getSource();
-    default:
-      return values.getSource();
-    }
-  }
-  
-  private SortedSource getSortedSource(DocValues values) throws IOException {
-    return getSource(values).asSortedSource();
-  }
-  
-  public static class DocValueHolder implements StorableField {
-    BytesRef bytes;
-    Number numberValue;
-    Comparator<BytesRef> comp;
-
-    @Override
-    public String name() {
-      return "test";
-    }
-
-    @Override
-    public BytesRef binaryValue() {
-      return bytes;
-    }
-
-    @Override
-    public Number numericValue() {
-      return numberValue;
-    }
-
-    @Override
-    public String stringValue() {
-      return null;
-    }
-
-    @Override
-    public Reader readerValue() {
-      return null;
-    }
-
-    @Override
-    public FieldType fieldType() {
-      return null;
-    }
-  }
-}
Index: lucene/core/src/test/org/apache/lucene/codecs/lucene41/TestBlockPostingsFormat2.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/codecs/lucene41/TestBlockPostingsFormat2.java	(revision 1442822)
+++ lucene/core/src/test/org/apache/lucene/codecs/lucene41/TestBlockPostingsFormat2.java	(working copy)
@@ -47,7 +47,6 @@
     iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
     iwc.setCodec(_TestUtil.alwaysPostingsFormat(new Lucene41PostingsFormat()));
     iw = new RandomIndexWriter(random(), dir, iwc);
-    iw.setAddDocValuesFields(false);
     iw.setDoRandomForceMerge(false); // we will ourselves
   }
   
Index: lucene/core/src/test/org/apache/lucene/codecs/lucene42/TestLucene42DocValuesFormat.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/codecs/lucene42/TestLucene42DocValuesFormat.java	(revision 0)
+++ lucene/core/src/test/org/apache/lucene/codecs/lucene42/TestLucene42DocValuesFormat.java	(working copy)
@@ -0,0 +1,33 @@
+package org.apache.lucene.codecs.lucene42;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.codecs.Codec;
+import org.apache.lucene.index.BaseDocValuesFormatTestCase;
+
+/**
+ * Tests Lucene42DocValuesFormat
+ */
+public class TestLucene42DocValuesFormat extends BaseDocValuesFormatTestCase {
+  private final Codec codec = new Lucene42Codec();
+
+  @Override
+  protected Codec getCodec() {
+    return codec;
+  }
+}

Property changes on: lucene/core/src/test/org/apache/lucene/codecs/lucene42/TestLucene42DocValuesFormat.java
___________________________________________________________________
Added: svn:eol-style
## -0,0 +1 ##
+native
Index: lucene/core/src/test/org/apache/lucene/codecs/perfield/TestPerFieldDocValuesFormat.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/codecs/perfield/TestPerFieldDocValuesFormat.java	(revision 0)
+++ lucene/core/src/test/org/apache/lucene/codecs/perfield/TestPerFieldDocValuesFormat.java	(working copy)
@@ -0,0 +1,122 @@
+package org.apache.lucene.codecs.perfield;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Collections;
+import java.util.Random;
+
+import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.analysis.MockAnalyzer;
+import org.apache.lucene.codecs.Codec;
+import org.apache.lucene.codecs.DocValuesFormat;
+import org.apache.lucene.codecs.lucene42.Lucene42Codec;
+import org.apache.lucene.document.BinaryDocValuesField;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.document.NumericDocValuesField;
+import org.apache.lucene.index.BaseDocValuesFormatTestCase;
+import org.apache.lucene.index.BinaryDocValues;
+import org.apache.lucene.index.DirectoryReader;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.index.IndexWriterConfig;
+import org.apache.lucene.index.NumericDocValues;
+import org.apache.lucene.index.RandomCodec;
+import org.apache.lucene.index.StoredDocument;
+import org.apache.lucene.index.Term;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.TermQuery;
+import org.apache.lucene.search.TopDocs;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.util.BytesRef;
+
+/**
+ * Basic tests of PerFieldDocValuesFormat
+ */
+public class TestPerFieldDocValuesFormat extends BaseDocValuesFormatTestCase {
+  private Codec codec;
+  
+  @Override
+  public void setUp() throws Exception {
+    codec = new RandomCodec(new Random(random().nextLong()), Collections.<String>emptySet());
+    super.setUp();
+  }
+  
+  @Override
+  protected Codec getCodec() {
+    return codec;
+  }
+  
+  // just a simple trivial test
+  // TODO: we should come up with a test that somehow checks that segment suffix
+  // is respected by all codec apis (not just docvalues and postings)
+  public void testTwoFieldsTwoFormats() throws IOException {
+    Analyzer analyzer = new MockAnalyzer(random());
+
+    Directory directory = newDirectory();
+    // we don't use RandomIndexWriter because it might add more docvalues than we expect !!!!1
+    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);
+    final DocValuesFormat fast = DocValuesFormat.forName("Lucene42");
+    final DocValuesFormat slow = DocValuesFormat.forName("SimpleText");
+    iwc.setCodec(new Lucene42Codec() {
+      @Override
+      public DocValuesFormat getDocValuesFormatForField(String field) {
+        if ("dv1".equals(field)) {
+          return fast;
+        } else {
+          return slow;
+        }
+      }
+    });
+    IndexWriter iwriter = new IndexWriter(directory, iwc);
+    Document doc = new Document();
+    String longTerm = "longtermlongtermlongtermlongtermlongtermlongtermlongtermlongtermlongtermlongtermlongtermlongtermlongtermlongtermlongtermlongtermlongtermlongterm";
+    String text = "This is the text to be indexed. " + longTerm;
+    doc.add(newTextField("fieldname", text, Field.Store.YES));
+    doc.add(new NumericDocValuesField("dv1", 5));
+    doc.add(new BinaryDocValuesField("dv2", new BytesRef("hello world")));
+    iwriter.addDocument(doc);
+    iwriter.close();
+    
+    // Now search the index:
+    IndexReader ireader = DirectoryReader.open(directory); // read-only=true
+    IndexSearcher isearcher = new IndexSearcher(ireader);
+
+    assertEquals(1, isearcher.search(new TermQuery(new Term("fieldname", longTerm)), 1).totalHits);
+    Query query = new TermQuery(new Term("fieldname", "text"));
+    TopDocs hits = isearcher.search(query, null, 1);
+    assertEquals(1, hits.totalHits);
+    BytesRef scratch = new BytesRef();
+    // Iterate through the results:
+    for (int i = 0; i < hits.scoreDocs.length; i++) {
+      StoredDocument hitDoc = isearcher.doc(hits.scoreDocs[i].doc);
+      assertEquals(text, hitDoc.get("fieldname"));
+      assert ireader.leaves().size() == 1;
+      NumericDocValues dv = ireader.leaves().get(0).reader().getNumericDocValues("dv1");
+      assertEquals(5, dv.get(hits.scoreDocs[i].doc));
+      BinaryDocValues dv2 = ireader.leaves().get(0).reader().getBinaryDocValues("dv2");
+      dv2.get(hits.scoreDocs[i].doc, scratch);
+      assertEquals(new BytesRef("hello world"), scratch);
+    }
+
+    ireader.close();
+    directory.close();
+  }
+}

Property changes on: lucene/core/src/test/org/apache/lucene/codecs/perfield/TestPerFieldDocValuesFormat.java
___________________________________________________________________
Added: svn:eol-style
## -0,0 +1 ##
+native
Index: lucene/core/src/test/org/apache/lucene/search/TestFieldCache.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/search/TestFieldCache.java	(revision 1442822)
+++ lucene/core/src/test/org/apache/lucene/search/TestFieldCache.java	(working copy)
@@ -32,6 +32,7 @@
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.IntField;
+import org.apache.lucene.document.StringField;
 import org.apache.lucene.index.*;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.util.Bits;
@@ -129,58 +130,46 @@
 
   public void test() throws IOException {
     FieldCache cache = FieldCache.DEFAULT;
-    double [] doubles = cache.getDoubles(reader, "theDouble", random().nextBoolean());
+    FieldCache.Doubles doubles = cache.getDoubles(reader, "theDouble", random().nextBoolean());
     assertSame("Second request to cache return same array", doubles, cache.getDoubles(reader, "theDouble", random().nextBoolean()));
     assertSame("Second request with explicit parser return same array", doubles, cache.getDoubles(reader, "theDouble", FieldCache.DEFAULT_DOUBLE_PARSER, random().nextBoolean()));
-    assertTrue("doubles Size: " + doubles.length + " is not: " + NUM_DOCS, doubles.length == NUM_DOCS);
-    for (int i = 0; i < doubles.length; i++) {
-      assertTrue(doubles[i] + " does not equal: " + (Double.MAX_VALUE - i), doubles[i] == (Double.MAX_VALUE - i));
-
+    for (int i = 0; i < NUM_DOCS; i++) {
+      assertTrue(doubles.get(i) + " does not equal: " + (Double.MAX_VALUE - i), doubles.get(i) == (Double.MAX_VALUE - i));
     }
     
-    long [] longs = cache.getLongs(reader, "theLong", random().nextBoolean());
+    FieldCache.Longs longs = cache.getLongs(reader, "theLong", random().nextBoolean());
     assertSame("Second request to cache return same array", longs, cache.getLongs(reader, "theLong", random().nextBoolean()));
     assertSame("Second request with explicit parser return same array", longs, cache.getLongs(reader, "theLong", FieldCache.DEFAULT_LONG_PARSER, random().nextBoolean()));
-    assertTrue("longs Size: " + longs.length + " is not: " + NUM_DOCS, longs.length == NUM_DOCS);
-    for (int i = 0; i < longs.length; i++) {
-      assertTrue(longs[i] + " does not equal: " + (Long.MAX_VALUE - i) + " i=" + i, longs[i] == (Long.MAX_VALUE - i));
-
+    for (int i = 0; i < NUM_DOCS; i++) {
+      assertTrue(longs.get(i) + " does not equal: " + (Long.MAX_VALUE - i) + " i=" + i, longs.get(i) == (Long.MAX_VALUE - i));
     }
     
-    byte [] bytes = cache.getBytes(reader, "theByte", random().nextBoolean());
+    FieldCache.Bytes bytes = cache.getBytes(reader, "theByte", random().nextBoolean());
     assertSame("Second request to cache return same array", bytes, cache.getBytes(reader, "theByte", random().nextBoolean()));
     assertSame("Second request with explicit parser return same array", bytes, cache.getBytes(reader, "theByte", FieldCache.DEFAULT_BYTE_PARSER, random().nextBoolean()));
-    assertTrue("bytes Size: " + bytes.length + " is not: " + NUM_DOCS, bytes.length == NUM_DOCS);
-    for (int i = 0; i < bytes.length; i++) {
-      assertTrue(bytes[i] + " does not equal: " + (Byte.MAX_VALUE - i), bytes[i] == (byte) (Byte.MAX_VALUE - i));
-
+    for (int i = 0; i < NUM_DOCS; i++) {
+      assertTrue(bytes.get(i) + " does not equal: " + (Byte.MAX_VALUE - i), bytes.get(i) == (byte) (Byte.MAX_VALUE - i));
     }
     
-    short [] shorts = cache.getShorts(reader, "theShort", random().nextBoolean());
+    FieldCache.Shorts shorts = cache.getShorts(reader, "theShort", random().nextBoolean());
     assertSame("Second request to cache return same array", shorts, cache.getShorts(reader, "theShort", random().nextBoolean()));
     assertSame("Second request with explicit parser return same array", shorts, cache.getShorts(reader, "theShort", FieldCache.DEFAULT_SHORT_PARSER, random().nextBoolean()));
-    assertTrue("shorts Size: " + shorts.length + " is not: " + NUM_DOCS, shorts.length == NUM_DOCS);
-    for (int i = 0; i < shorts.length; i++) {
-      assertTrue(shorts[i] + " does not equal: " + (Short.MAX_VALUE - i), shorts[i] == (short) (Short.MAX_VALUE - i));
-
+    for (int i = 0; i < NUM_DOCS; i++) {
+      assertTrue(shorts.get(i) + " does not equal: " + (Short.MAX_VALUE - i), shorts.get(i) == (short) (Short.MAX_VALUE - i));
     }
     
-    int [] ints = cache.getInts(reader, "theInt", random().nextBoolean());
+    FieldCache.Ints ints = cache.getInts(reader, "theInt", random().nextBoolean());
     assertSame("Second request to cache return same array", ints, cache.getInts(reader, "theInt", random().nextBoolean()));
     assertSame("Second request with explicit parser return same array", ints, cache.getInts(reader, "theInt", FieldCache.DEFAULT_INT_PARSER, random().nextBoolean()));
-    assertTrue("ints Size: " + ints.length + " is not: " + NUM_DOCS, ints.length == NUM_DOCS);
-    for (int i = 0; i < ints.length; i++) {
-      assertTrue(ints[i] + " does not equal: " + (Integer.MAX_VALUE - i), ints[i] == (Integer.MAX_VALUE - i));
-
+    for (int i = 0; i < NUM_DOCS; i++) {
+      assertTrue(ints.get(i) + " does not equal: " + (Integer.MAX_VALUE - i), ints.get(i) == (Integer.MAX_VALUE - i));
     }
     
-    float [] floats = cache.getFloats(reader, "theFloat", random().nextBoolean());
+    FieldCache.Floats floats = cache.getFloats(reader, "theFloat", random().nextBoolean());
     assertSame("Second request to cache return same array", floats, cache.getFloats(reader, "theFloat", random().nextBoolean()));
     assertSame("Second request with explicit parser return same array", floats, cache.getFloats(reader, "theFloat", FieldCache.DEFAULT_FLOAT_PARSER, random().nextBoolean()));
-    assertTrue("floats Size: " + floats.length + " is not: " + NUM_DOCS, floats.length == NUM_DOCS);
-    for (int i = 0; i < floats.length; i++) {
-      assertTrue(floats[i] + " does not equal: " + (Float.MAX_VALUE - i), floats[i] == (Float.MAX_VALUE - i));
-
+    for (int i = 0; i < NUM_DOCS; i++) {
+      assertTrue(floats.get(i) + " does not equal: " + (Float.MAX_VALUE - i), floats.get(i) == (Float.MAX_VALUE - i));
     }
 
     Bits docsWithField = cache.getDocsWithField(reader, "theLong");
@@ -200,46 +189,62 @@
     }
 
     // getTermsIndex
-    FieldCache.DocTermsIndex termsIndex = cache.getTermsIndex(reader, "theRandomUnicodeString");
+    SortedDocValues termsIndex = cache.getTermsIndex(reader, "theRandomUnicodeString");
     assertSame("Second request to cache return same array", termsIndex, cache.getTermsIndex(reader, "theRandomUnicodeString"));
-    assertTrue("doubles Size: " + termsIndex.size() + " is not: " + NUM_DOCS, termsIndex.size() == NUM_DOCS);
     final BytesRef br = new BytesRef();
     for (int i = 0; i < NUM_DOCS; i++) {
-      final BytesRef term = termsIndex.getTerm(i, br);
+      final BytesRef term;
+      final int ord = termsIndex.getOrd(i);
+      if (ord == -1) {
+        term = null;
+      } else {
+        termsIndex.lookupOrd(ord, br);
+        term = br;
+      }
       final String s = term == null ? null : term.utf8ToString();
       assertTrue("for doc " + i + ": " + s + " does not equal: " + unicodeStrings[i], unicodeStrings[i] == null || unicodeStrings[i].equals(s));
     }
 
-    int nTerms = termsIndex.numOrd();
-    // System.out.println("nTerms="+nTerms);
+    int nTerms = termsIndex.getValueCount();
 
-    TermsEnum tenum = termsIndex.getTermsEnum();
+    TermsEnum tenum = new SortedDocValuesTermsEnum(termsIndex);
     BytesRef val = new BytesRef();
-    for (int i=1; i<nTerms; i++) {
+    for (int i=0; i<nTerms; i++) {
       BytesRef val1 = tenum.next();
-      BytesRef val2 = termsIndex.lookup(i,val);
+      termsIndex.lookupOrd(i, val);
       // System.out.println("i="+i);
-      assertEquals(val2, val1);
+      assertEquals(val, val1);
     }
 
     // seek the enum around (note this isn't a great test here)
     int num = atLeast(100);
     for (int i = 0; i < num; i++) {
-      int k = _TestUtil.nextInt(random(), 1, nTerms-1);
-      BytesRef val1 = termsIndex.lookup(k, val);
-      assertEquals(TermsEnum.SeekStatus.FOUND, tenum.seekCeil(val1));
-      assertEquals(val1, tenum.term());
+      int k = random().nextInt(nTerms);
+      termsIndex.lookupOrd(k, val);
+      assertEquals(TermsEnum.SeekStatus.FOUND, tenum.seekCeil(val));
+      assertEquals(val, tenum.term());
     }
-    
+
+    for(int i=0;i<nTerms;i++) {
+      termsIndex.lookupOrd(i, val);
+      assertEquals(TermsEnum.SeekStatus.FOUND, tenum.seekCeil(val));
+      assertEquals(val, tenum.term());
+    }
+
     // test bad field
     termsIndex = cache.getTermsIndex(reader, "bogusfield");
 
     // getTerms
-    FieldCache.DocTerms terms = cache.getTerms(reader, "theRandomUnicodeString");
+    BinaryDocValues terms = cache.getTerms(reader, "theRandomUnicodeString");
     assertSame("Second request to cache return same array", terms, cache.getTerms(reader, "theRandomUnicodeString"));
-    assertTrue("doubles Size: " + terms.size() + " is not: " + NUM_DOCS, terms.size() == NUM_DOCS);
     for (int i = 0; i < NUM_DOCS; i++) {
-      final BytesRef term = terms.getTerm(i, br);
+      terms.get(i, br);
+      final BytesRef term;
+      if (br.bytes == BinaryDocValues.MISSING) {
+        term = null;
+      } else {
+        term = br;
+      }
       final String s = term == null ? null : term.utf8ToString();
       assertTrue("for doc " + i + ": " + s + " does not equal: " + unicodeStrings[i], unicodeStrings[i] == null || unicodeStrings[i].equals(s));
     }
@@ -324,7 +329,7 @@
     FieldCache cache = FieldCache.DEFAULT;
     cache.purgeAllCaches();
     assertEquals(0, cache.getCacheEntries().length);
-    double[] doubles = cache.getDoubles(reader, "theDouble", true);
+    cache.getDoubles(reader, "theDouble", true);
 
     // The double[] takes two slots (one w/ null parser, one
     // w/ real parser), and docsWithField should also
@@ -336,25 +341,25 @@
     assertEquals(3, cache.getCacheEntries().length);
     assertTrue(bits instanceof Bits.MatchAllBits);
 
-    int[] ints = cache.getInts(reader, "sparse", true);
+    FieldCache.Ints ints = cache.getInts(reader, "sparse", true);
     assertEquals(6, cache.getCacheEntries().length);
     Bits docsWithField = cache.getDocsWithField(reader, "sparse");
     assertEquals(6, cache.getCacheEntries().length);
     for (int i = 0; i < docsWithField.length(); i++) {
       if (i%2 == 0) {
         assertTrue(docsWithField.get(i));
-        assertEquals(i, ints[i]);
+        assertEquals(i, ints.get(i));
       } else {
         assertFalse(docsWithField.get(i));
       }
     }
 
-    int[] numInts = cache.getInts(reader, "numInt", random().nextBoolean());
+    FieldCache.Ints numInts = cache.getInts(reader, "numInt", random().nextBoolean());
     docsWithField = cache.getDocsWithField(reader, "numInt");
     for (int i = 0; i < docsWithField.length(); i++) {
       if (i%2 == 0) {
         assertTrue(docsWithField.get(i));
-        assertEquals(i, numInts[i]);
+        assertEquals(i, numInts.get(i));
       } else {
         assertFalse(docsWithField.get(i));
       }
@@ -399,12 +404,12 @@
                     assertEquals(i%2 == 0, docsWithField.get(i));
                   }
                 } else {
-                  int[] ints = cache.getInts(reader, "sparse", true);
+                  FieldCache.Ints ints = cache.getInts(reader, "sparse", true);
                   Bits docsWithField = cache.getDocsWithField(reader, "sparse");
                   for (int i = 0; i < docsWithField.length(); i++) {
                     if (i%2 == 0) {
                       assertTrue(docsWithField.get(i));
-                      assertEquals(i, ints[i]);
+                      assertEquals(i, ints.get(i));
                     } else {
                       assertFalse(docsWithField.get(i));
                     }
Index: lucene/core/src/test/org/apache/lucene/search/TestDisjunctionMaxQuery.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/search/TestDisjunctionMaxQuery.java	(revision 1442822)
+++ lucene/core/src/test/org/apache/lucene/search/TestDisjunctionMaxQuery.java	(working copy)
@@ -25,7 +25,6 @@
 import org.apache.lucene.document.TextField;
 import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.Norm;
 import org.apache.lucene.index.SlowCompositeReaderWrapper;
 import org.apache.lucene.index.FieldInvertState;
 import org.apache.lucene.index.RandomIndexWriter;
Index: lucene/core/src/test/org/apache/lucene/search/TestSearchAfter.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/search/TestSearchAfter.java	(revision 1442822)
+++ lucene/core/src/test/org/apache/lucene/search/TestSearchAfter.java	(working copy)
@@ -19,16 +19,16 @@
 
 import java.util.Arrays;
 
+import org.apache.lucene.document.BinaryDocValuesField;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.DoubleField;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.FloatDocValuesField;
 import org.apache.lucene.document.FloatField;
-import org.apache.lucene.document.IntDocValuesField;
 import org.apache.lucene.document.IntField;
 import org.apache.lucene.document.LongField;
-import org.apache.lucene.document.SortedBytesDocValuesField;
-import org.apache.lucene.document.StraightBytesDocValuesField;
+import org.apache.lucene.document.NumericDocValuesField;
+import org.apache.lucene.document.SortedDocValuesField;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.RandomIndexWriter;
 import org.apache.lucene.index.Term;
@@ -41,17 +41,11 @@
 /**
  * Tests IndexSearcher's searchAfter() method
  */
-
 public class TestSearchAfter extends LuceneTestCase {
   private Directory dir;
   private IndexReader reader;
   private IndexSearcher searcher;
 
-  private static SortField useDocValues(SortField field) {
-    field.setUseIndexValues(true);
-    return field;
-  }
-
   @Override
   public void setUp() throws Exception {
     super.setUp();
@@ -73,11 +67,11 @@
       document.add(newStringField("bytesval", _TestUtil.randomRealisticUnicodeString(random()), Field.Store.NO));
       document.add(new DoubleField("double", random().nextDouble(), Field.Store.NO));
 
-      document.add(new IntDocValuesField("intdocvalues", random().nextInt()));
+      document.add(new NumericDocValuesField("intdocvalues", random().nextInt()));
       document.add(new FloatDocValuesField("floatdocvalues", random().nextFloat()));
-      document.add(new SortedBytesDocValuesField("sortedbytesdocvalues", new BytesRef(_TestUtil.randomRealisticUnicodeString(random()))));
-      document.add(new SortedBytesDocValuesField("sortedbytesdocvaluesval", new BytesRef(_TestUtil.randomRealisticUnicodeString(random()))));
-      document.add(new StraightBytesDocValuesField("straightbytesdocvalues", new BytesRef(_TestUtil.randomRealisticUnicodeString(random()))));
+      document.add(new SortedDocValuesField("sortedbytesdocvalues", new BytesRef(_TestUtil.randomRealisticUnicodeString(random()))));
+      document.add(new SortedDocValuesField("sortedbytesdocvaluesval", new BytesRef(_TestUtil.randomRealisticUnicodeString(random()))));
+      document.add(new BinaryDocValuesField("straightbytesdocvalues", new BytesRef(_TestUtil.randomRealisticUnicodeString(random()))));
 
       iw.addDocument(document);
     }
@@ -125,11 +119,11 @@
       assertQuery(query, filter, new Sort(new SortField[] {new SortField("double", SortField.Type.DOUBLE, reversed)}));
       assertQuery(query, filter, new Sort(new SortField[] {new SortField("bytes", SortField.Type.STRING, reversed)}));
       assertQuery(query, filter, new Sort(new SortField[] {new SortField("bytesval", SortField.Type.STRING_VAL, reversed)}));
-      assertQuery(query, filter, new Sort(new SortField[] {useDocValues(new SortField("intdocvalues", SortField.Type.INT, reversed))}));
-      assertQuery(query, filter, new Sort(new SortField[] {useDocValues(new SortField("floatdocvalues", SortField.Type.FLOAT, reversed))}));
-      assertQuery(query, filter, new Sort(new SortField[] {useDocValues(new SortField("sortedbytesdocvalues", SortField.Type.STRING, reversed))}));
-      assertQuery(query, filter, new Sort(new SortField[] {useDocValues(new SortField("sortedbytesdocvaluesval", SortField.Type.STRING_VAL, reversed))}));
-      assertQuery(query, filter, new Sort(new SortField[] {useDocValues(new SortField("straightbytesdocvalues", SortField.Type.STRING_VAL, reversed))}));
+      assertQuery(query, filter, new Sort(new SortField[] {new SortField("intdocvalues", SortField.Type.INT, reversed)}));
+      assertQuery(query, filter, new Sort(new SortField[] {new SortField("floatdocvalues", SortField.Type.FLOAT, reversed)}));
+      assertQuery(query, filter, new Sort(new SortField[] {new SortField("sortedbytesdocvalues", SortField.Type.STRING, reversed)}));
+      assertQuery(query, filter, new Sort(new SortField[] {new SortField("sortedbytesdocvaluesval", SortField.Type.STRING_VAL, reversed)}));
+      assertQuery(query, filter, new Sort(new SortField[] {new SortField("straightbytesdocvalues", SortField.Type.STRING_VAL, reversed)}));
     }
   }
 
Index: lucene/core/src/test/org/apache/lucene/search/TestBooleanOr.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/search/TestBooleanOr.java	(revision 1442822)
+++ lucene/core/src/test/org/apache/lucene/search/TestBooleanOr.java	(working copy)
@@ -18,11 +18,13 @@
 import java.io.IOException;
 import java.util.concurrent.atomic.AtomicInteger;
 
+import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.TextField;
 import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.RandomIndexWriter;
+import org.apache.lucene.index.SerialMergeScheduler;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.util.FixedBitSet;
@@ -160,7 +162,7 @@
 
   public void testBooleanScorerMax() throws IOException {
     Directory dir = newDirectory();
-    RandomIndexWriter riw = new RandomIndexWriter(random(), dir);
+    RandomIndexWriter riw = new RandomIndexWriter(random(), dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));
 
     int docCount = atLeast(10000);
 
Index: lucene/core/src/test/org/apache/lucene/search/spans/TestBasics.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/search/spans/TestBasics.java	(revision 1442822)
+++ lucene/core/src/test/org/apache/lucene/search/spans/TestBasics.java	(working copy)
@@ -96,18 +96,18 @@
     }
   }
   
-  static Analyzer simplePayloadAnalyzer = new Analyzer() {
+  static Analyzer simplePayloadAnalyzer;
 
-    @Override
-    public TokenStreamComponents createComponents(String fieldName, Reader reader) {
-      Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.SIMPLE, true);
-      return new TokenStreamComponents(tokenizer, new SimplePayloadFilter(tokenizer));
-    }
-    
-  };
-  
   @BeforeClass
   public static void beforeClass() throws Exception {
+    simplePayloadAnalyzer = new Analyzer() {
+        @Override
+        public TokenStreamComponents createComponents(String fieldName, Reader reader) {
+          Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.SIMPLE, true);
+          return new TokenStreamComponents(tokenizer, new SimplePayloadFilter(tokenizer));
+        }
+    };
+  
     directory = newDirectory();
     RandomIndexWriter writer = new RandomIndexWriter(random(), directory,
         newIndexWriterConfig(TEST_VERSION_CURRENT, simplePayloadAnalyzer)
Index: lucene/core/src/test/org/apache/lucene/search/TestConjunctions.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/search/TestConjunctions.java	(revision 1442822)
+++ lucene/core/src/test/org/apache/lucene/search/TestConjunctions.java	(working copy)
@@ -29,7 +29,6 @@
 import org.apache.lucene.index.FieldInvertState;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexWriterConfig;
-import org.apache.lucene.index.Norm;
 import org.apache.lucene.index.RandomIndexWriter;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.search.similarities.Similarity;
@@ -90,8 +89,8 @@
   private static class TFSimilarity extends Similarity {
 
     @Override
-    public void computeNorm(FieldInvertState state, Norm norm) {
-      norm.setByte((byte)1); // we dont care
+    public long computeNorm(FieldInvertState state) {
+      return 1; // we dont care
     }
 
     @Override
Index: lucene/core/src/test/org/apache/lucene/search/FieldCacheRewriteMethod.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/search/FieldCacheRewriteMethod.java	(revision 1442822)
+++ lucene/core/src/test/org/apache/lucene/search/FieldCacheRewriteMethod.java	(working copy)
@@ -22,6 +22,8 @@
 
 import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.SortedDocValues;
+import org.apache.lucene.index.SortedDocValuesTermsEnum;
 import org.apache.lucene.index.Terms;
 import org.apache.lucene.index.TermsEnum;
 import org.apache.lucene.util.Bits;
@@ -88,9 +90,9 @@
      */
     @Override
     public DocIdSet getDocIdSet(AtomicReaderContext context, final Bits acceptDocs) throws IOException {
-      final FieldCache.DocTermsIndex fcsi = FieldCache.DEFAULT.getTermsIndex(context.reader(), query.field);
+      final SortedDocValues fcsi = FieldCache.DEFAULT.getTermsIndex(context.reader(), query.field);
       // Cannot use FixedBitSet because we require long index (ord):
-      final OpenBitSet termSet = new OpenBitSet(fcsi.numOrd());
+      final OpenBitSet termSet = new OpenBitSet(fcsi.getValueCount());
       TermsEnum termsEnum = query.getTermsEnum(new Terms() {
         
         @Override
@@ -100,7 +102,7 @@
         
         @Override
         public TermsEnum iterator(TermsEnum reuse) {
-          return fcsi.getTermsEnum();
+          return new SortedDocValuesTermsEnum(fcsi);
         }
 
         @Override
@@ -144,7 +146,7 @@
         // fill into a OpenBitSet
         do {
           long ord = termsEnum.ord();
-          if (ord > 0) {
+          if (ord >= 0) {
             termSet.set(ord);
           }
         } while (termsEnum.next() != null);
@@ -155,7 +157,11 @@
       return new FieldCacheDocIdSet(context.reader().maxDoc(), acceptDocs) {
         @Override
         protected final boolean matchDoc(int doc) throws ArrayIndexOutOfBoundsException {
-          return termSet.get(fcsi.getOrd(doc));
+          int ord = fcsi.getOrd(doc);
+          if (ord == -1) {
+            return false;
+          }
+          return termSet.get(ord);
         }
       };
     }
Index: lucene/core/src/test/org/apache/lucene/search/JustCompileSearch.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/search/JustCompileSearch.java	(revision 1442822)
+++ lucene/core/src/test/org/apache/lucene/search/JustCompileSearch.java	(working copy)
@@ -20,7 +20,6 @@
 import java.io.IOException;
 
 import org.apache.lucene.index.AtomicReaderContext;
-import org.apache.lucene.index.Norm;
 import org.apache.lucene.index.Terms;
 import org.apache.lucene.index.TermsEnum;
 import org.apache.lucene.search.similarities.Similarity;
@@ -267,7 +266,7 @@
     }
 
     @Override
-    public void computeNorm(FieldInvertState state, Norm norm) {
+    public long computeNorm(FieldInvertState state) {
       throw new UnsupportedOperationException(UNSUPPORTED_MSG);
     }
   }
Index: lucene/core/src/test/org/apache/lucene/search/TestSimilarity.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/search/TestSimilarity.java	(revision 1442822)
+++ lucene/core/src/test/org/apache/lucene/search/TestSimilarity.java	(working copy)
@@ -25,7 +25,6 @@
 import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.FieldInvertState;
 import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.Norm;
 import org.apache.lucene.index.RandomIndexWriter;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.search.similarities.DefaultSimilarity;
Index: lucene/core/src/test/org/apache/lucene/search/TestSimilarityProvider.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/search/TestSimilarityProvider.java	(revision 1442822)
+++ lucene/core/src/test/org/apache/lucene/search/TestSimilarityProvider.java	(working copy)
@@ -24,7 +24,7 @@
 import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.FieldInvertState;
 import org.apache.lucene.index.IndexWriterConfig;
-import org.apache.lucene.index.Norm;
+import org.apache.lucene.index.NumericDocValues;
 import org.apache.lucene.index.RandomIndexWriter;
 import org.apache.lucene.index.SlowCompositeReaderWrapper;
 import org.apache.lucene.index.Term;
@@ -77,10 +77,10 @@
     // sanity check of norms writer
     // TODO: generalize
     AtomicReader slow = new SlowCompositeReaderWrapper(reader);
-    byte fooNorms[] = (byte[]) slow.normValues("foo").getSource().getArray();
-    byte barNorms[] = (byte[]) slow.normValues("bar").getSource().getArray();
-    for (int i = 0; i < fooNorms.length; i++) {
-      assertFalse(fooNorms[i] == barNorms[i]);
+    NumericDocValues fooNorms = slow.getNormValues("foo");
+    NumericDocValues barNorms = slow.getNormValues("bar");
+    for (int i = 0; i < slow.maxDoc(); i++) {
+      assertFalse(fooNorms.get(i) == barNorms.get(i));
     }
     
     // sanity check of searching
Index: lucene/core/src/test/org/apache/lucene/search/TestElevationComparator.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/search/TestElevationComparator.java	(revision 1442822)
+++ lucene/core/src/test/org/apache/lucene/search/TestElevationComparator.java	(working copy)
@@ -142,7 +142,7 @@
   public FieldComparator<Integer> newComparator(final String fieldname, final int numHits, int sortPos, boolean reversed) throws IOException {
    return new FieldComparator<Integer>() {
 
-     FieldCache.DocTermsIndex idIndex;
+     SortedDocValues idIndex;
      private final int[] values = new int[numHits];
      private final BytesRef tempBR = new BytesRef();
      int bottomVal;
@@ -159,11 +159,11 @@
 
      private int docVal(int doc) {
        int ord = idIndex.getOrd(doc);
-       if (ord == 0) {
+       if (ord == -1) {
          return 0;
        } else {
-         BytesRef id = idIndex.lookup(ord, tempBR);
-         Integer prio = priority.get(id);
+         idIndex.lookupOrd(ord, tempBR);
+         Integer prio = priority.get(tempBR);
          return prio == null ? 0 : prio.intValue();
        }
      }
Index: lucene/core/src/test/org/apache/lucene/search/TestSort.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/search/TestSort.java	(revision 1442822)
+++ lucene/core/src/test/org/apache/lucene/search/TestSort.java	(working copy)
@@ -30,20 +30,18 @@
 import java.util.concurrent.TimeUnit;
 
 import org.apache.lucene.analysis.MockAnalyzer;
-import org.apache.lucene.document.DerefBytesDocValuesField;
+import org.apache.lucene.document.BinaryDocValuesField;
 import org.apache.lucene.document.Document;
-import org.apache.lucene.document.DoubleDocValuesField;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.FieldType;
 import org.apache.lucene.document.FloatDocValuesField;
-import org.apache.lucene.document.PackedLongDocValuesField;
-import org.apache.lucene.document.SortedBytesDocValuesField;
-import org.apache.lucene.document.StraightBytesDocValuesField;
+import org.apache.lucene.document.NumericDocValuesField;
+import org.apache.lucene.document.SortedDocValuesField;
 import org.apache.lucene.document.StringField;
 import org.apache.lucene.document.TextField;
 import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.DirectoryReader;
-import org.apache.lucene.index.DocValues;
+import org.apache.lucene.index.FieldInfo.DocValuesType;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexWriter;
 import org.apache.lucene.index.IndexWriterConfig;
@@ -66,7 +64,6 @@
 import org.apache.lucene.util.NamedThreadFactory;
 import org.apache.lucene.util._TestUtil;
 import org.junit.BeforeClass;
-
 /**
  * Unit tests for sorting code.
  *
@@ -126,24 +123,23 @@
   }; 
 
   // create an index of all the documents, or just the x, or just the y documents
-  private IndexSearcher getIndex (boolean even, boolean odd)
+  private IndexSearcher getIndex(boolean even, boolean odd)
   throws IOException {
     Directory indexStore = newDirectory();
     dirs.add(indexStore);
     RandomIndexWriter writer = new RandomIndexWriter(random(), indexStore, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(newLogMergePolicy()));
 
-    final DocValues.Type stringDVType;
+    final DocValuesType stringDVType;
     if (dvStringSorted) {
       // Index sorted
-      stringDVType = random().nextBoolean() ? DocValues.Type.BYTES_VAR_SORTED : DocValues.Type.BYTES_FIXED_SORTED;
+      stringDVType = DocValuesType.SORTED;
     } else {
-      // Index non-sorted
       if (random().nextBoolean()) {
-        // Fixed
-        stringDVType = random().nextBoolean() ? DocValues.Type.BYTES_FIXED_STRAIGHT : DocValues.Type.BYTES_FIXED_DEREF;
+        // Index non-sorted
+        stringDVType = DocValuesType.BINARY;
       } else {
-        // Var
-        stringDVType = random().nextBoolean() ? DocValues.Type.BYTES_VAR_STRAIGHT : DocValues.Type.BYTES_VAR_DEREF;
+        // sorted anyway
+        stringDVType = DocValuesType.SORTED;
       }
     }
 
@@ -151,54 +147,53 @@
     ft1.setStored(true);
     FieldType ft2 = new FieldType();
     ft2.setIndexed(true);
-    for (int i=0; i<data.length; ++i) {
+    for(int i=0; i<data.length; ++i) {
       if (((i%2)==0 && even) || ((i%2)==1 && odd)) {
         Document doc = new Document();
-        doc.add (new Field ("tracer", data[i][0], ft1));
-        doc.add (new TextField ("contents", data[i][1], Field.Store.NO));
+        doc.add(new Field("tracer", data[i][0], ft1));
+        doc.add(new TextField("contents", data[i][1], Field.Store.NO));
         if (data[i][2] != null) {
-          doc.add(new StringField ("int", data[i][2], Field.Store.NO));
-          doc.add(new PackedLongDocValuesField("int", Integer.parseInt(data[i][2])));
+          doc.add(new StringField("int", data[i][2], Field.Store.NO));
+          doc.add(new NumericDocValuesField("int_dv", Integer.parseInt(data[i][2])));
         }
         if (data[i][3] != null) {
-          doc.add(new StringField ("float", data[i][3], Field.Store.NO));
-          doc.add(new FloatDocValuesField("float", Float.parseFloat(data[i][3])));
+          doc.add(new StringField("float", data[i][3], Field.Store.NO));
+          doc.add(new FloatDocValuesField("float_dv", Float.parseFloat(data[i][3])));
         }
         if (data[i][4] != null) {
-          doc.add(new StringField ("string", data[i][4], Field.Store.NO));
+          doc.add(new StringField("string", data[i][4], Field.Store.NO));
           switch(stringDVType) {
-            case BYTES_FIXED_SORTED:
-              doc.add(new SortedBytesDocValuesField("string", new BytesRef(data[i][4]), true));
+            case SORTED:
+              doc.add(new SortedDocValuesField("string_dv", new BytesRef(data[i][4])));
               break;
-            case BYTES_VAR_SORTED:
-              doc.add(new SortedBytesDocValuesField("string", new BytesRef(data[i][4]), false));
+            case BINARY:
+              doc.add(new BinaryDocValuesField("string_dv", new BytesRef(data[i][4])));
               break;
-            case BYTES_FIXED_STRAIGHT:
-              doc.add(new StraightBytesDocValuesField("string", new BytesRef(data[i][4]), true));
+            default:
+              throw new IllegalStateException("unknown type " + stringDVType);
+          }
+        } else {
+          switch(stringDVType) {
+            case SORTED:
+              doc.add(new SortedDocValuesField("string_dv", new BytesRef()));
               break;
-            case BYTES_VAR_STRAIGHT:
-              doc.add(new StraightBytesDocValuesField("string", new BytesRef(data[i][4]), false));
+            case BINARY:
+              doc.add(new BinaryDocValuesField("string_dv", new BytesRef()));
               break;
-            case BYTES_FIXED_DEREF:
-              doc.add(new DerefBytesDocValuesField("string", new BytesRef(data[i][4]), true));
-              break;
-            case BYTES_VAR_DEREF:
-              doc.add(new DerefBytesDocValuesField("string", new BytesRef(data[i][4]), false));
-              break;
             default:
               throw new IllegalStateException("unknown type " + stringDVType);
           }
         }
-        if (data[i][5] != null) doc.add (new StringField ("custom",   data[i][5], Field.Store.NO));
-        if (data[i][6] != null) doc.add (new StringField ("i18n",     data[i][6], Field.Store.NO));
-        if (data[i][7] != null) doc.add (new StringField ("long",     data[i][7], Field.Store.NO));
+        if (data[i][5] != null) doc.add(new StringField("custom",   data[i][5], Field.Store.NO));
+        if (data[i][6] != null) doc.add(new StringField("i18n",     data[i][6], Field.Store.NO));
+        if (data[i][7] != null) doc.add(new StringField("long",     data[i][7], Field.Store.NO));
         if (data[i][8] != null) {
-          doc.add(new StringField ("double", data[i][8], Field.Store.NO));
-          doc.add(new DoubleDocValuesField("double", Double.parseDouble(data[i][8])));
+          doc.add(new StringField("double", data[i][8], Field.Store.NO));
+          doc.add(new NumericDocValuesField("double_dv", Double.doubleToRawLongBits(Double.parseDouble(data[i][8]))));
         }
-        if (data[i][9] != null) doc.add (new StringField ("short",     data[i][9], Field.Store.NO));
-        if (data[i][10] != null) doc.add (new StringField ("byte",     data[i][10], Field.Store.NO));
-        if (data[i][11] != null) doc.add (new StringField ("parser",     data[i][11], Field.Store.NO));
+        if (data[i][9] != null) doc.add(new StringField("short",     data[i][9], Field.Store.NO));
+        if (data[i][10] != null) doc.add(new StringField("byte",     data[i][10], Field.Store.NO));
+        if (data[i][11] != null) doc.add(new StringField("parser",     data[i][11], Field.Store.NO));
 
         for(IndexableField f : doc.getFields()) {
           if (f.fieldType().indexed() && !f.fieldType().omitNorms()) {
@@ -206,9 +201,10 @@
           }
         }
 
-        writer.addDocument (doc);
+        writer.addDocument(doc);
       }
     }
+
     IndexReader reader = writer.getReader();
     writer.close ();
     IndexSearcher s = newSearcher(reader);
@@ -217,32 +213,39 @@
 
   private IndexSearcher getFullIndex()
   throws IOException {
-    return getIndex (true, true);
+    return getIndex(true, true);
   }
   
   private IndexSearcher getFullStrings() throws IOException {
     Directory indexStore = newDirectory();
     dirs.add(indexStore);
     IndexWriter writer = new IndexWriter(
-        indexStore,
-        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).
-            setMergePolicy(newLogMergePolicy(97))
-    );
+                                         indexStore,
+                                         newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).
+                                         setMergePolicy(newLogMergePolicy(97)));
     FieldType onlyStored = new FieldType();
     onlyStored.setStored(true);
     final int fixedLen = getRandomNumber(2, 8);
     final int fixedLen2 = getRandomNumber(1, 4);
-    for (int i=0; i<NUM_STRINGS; i++) {
+    for(int i=0; i<NUM_STRINGS; i++) {
       Document doc = new Document();
       String num = getRandomCharString(getRandomNumber(2, 8), 48, 52);
-      doc.add (new Field ("tracer", num, onlyStored));
-      //doc.add (new Field ("contents", Integer.toString(i), Field.Store.NO, Field.Index.ANALYZED));
+      doc.add(new Field("tracer", num, onlyStored));
+      //doc.add(new Field("contents", Integer.toString(i), Field.Store.NO, Field.Index.ANALYZED));
       doc.add(new StringField("string", num, Field.Store.NO));
-      doc.add(new SortedBytesDocValuesField("string", new BytesRef(num)));
+      if (dvStringSorted) {
+        doc.add(new SortedDocValuesField("string_dv", new BytesRef(num)));
+      } else {
+        doc.add(new BinaryDocValuesField("string_dv", new BytesRef(num)));
+      }
       String num2 = getRandomCharString(getRandomNumber(1, 4), 48, 50);
-      doc.add(new StringField ("string2", num2, Field.Store.NO));
-      doc.add(new SortedBytesDocValuesField("string2", new BytesRef(num2)));
-      doc.add (new Field ("tracer2", num2, onlyStored));
+      doc.add(new StringField("string2", num2, Field.Store.NO));
+      if (dvStringSorted) {
+        doc.add(new SortedDocValuesField("string2_dv", new BytesRef(num2)));
+      } else {
+        doc.add(new BinaryDocValuesField("string2_dv", new BytesRef(num2)));
+      }
+      doc.add(new Field("tracer2", num2, onlyStored));
       for(IndexableField f2 : doc.getFields()) {
         if (f2.fieldType().indexed() && !f2.fieldType().omitNorms()) {
           ((Field) f2).setBoost(2.0f);
@@ -250,14 +253,22 @@
       }
 
       String numFixed = getRandomCharString(fixedLen, 48, 52);
-      doc.add (new Field ("fixed_tracer", numFixed, onlyStored));
-      //doc.add (new Field ("contents", Integer.toString(i), Field.Store.NO, Field.Index.ANALYZED));
+      doc.add(new Field("tracer_fixed", numFixed, onlyStored));
+      //doc.add(new Field("contents", Integer.toString(i), Field.Store.NO, Field.Index.ANALYZED));
       doc.add(new StringField("string_fixed", numFixed, Field.Store.NO));
-      doc.add(new SortedBytesDocValuesField("string_fixed", new BytesRef(numFixed), true));
+      if (dvStringSorted) {
+        doc.add(new SortedDocValuesField("string_fixed_dv", new BytesRef(numFixed)));
+      } else {
+        doc.add(new BinaryDocValuesField("string_fixed_dv", new BytesRef(numFixed)));
+      }
       String num2Fixed = getRandomCharString(fixedLen2, 48, 52);
-      doc.add(new StringField ("string2_fixed", num2Fixed, Field.Store.NO));
-      doc.add(new SortedBytesDocValuesField("string2_fixed", new BytesRef(num2Fixed), true));
-      doc.add (new Field ("tracer2_fixed", num2Fixed, onlyStored));
+      doc.add(new StringField("string2_fixed", num2Fixed, Field.Store.NO));
+      if (dvStringSorted) {
+        doc.add(new SortedDocValuesField("string2_fixed_dv", new BytesRef(num2Fixed)));
+      } else {
+        doc.add(new BinaryDocValuesField("string2_fixed_dv", new BytesRef(num2Fixed)));
+      }
+      doc.add(new Field("tracer2_fixed", num2Fixed, onlyStored));
 
       for(IndexableField f2 : doc.getFields()) {
         if (f2.fieldType().indexed() && !f2.fieldType().omitNorms()) {
@@ -265,18 +276,47 @@
         }
       }
 
-      writer.addDocument (doc);
+      writer.addDocument(doc);
     }
-    //writer.forceMerge(1);
     //System.out.println(writer.getSegmentCount());
     writer.close();
     IndexReader reader = DirectoryReader.open(indexStore);
-    return newSearcher(reader);
+    IndexSearcher searcher = newSearcher(reader);
+
+    /*
+    for(int docID=0;docID<reader.maxDoc();docID++) {
+      StoredDocument doc = reader.document(docID);
+      String s = doc.get("tracer");
+      TopDocs hits = searcher.search(new TermQuery(new Term("string", s)), NUM_STRINGS);
+      System.out.println("string=" + s + " has " + hits.totalHits + " docs");
+      boolean found = false;
+      for(int hit=0;!found && hit<hits.totalHits;hit++) {
+        if (hits.scoreDocs[hit].doc == docID) {
+          found = true;
+          break;
+        }
+      }
+      assertTrue(found);
+      s = doc.get("tracer2");
+      hits = searcher.search(new TermQuery(new Term("string2", s)), NUM_STRINGS);
+      System.out.println("string2=" + s + " has " + hits.totalHits + " docs");
+      found = false;
+      for(int hit=0;!found && hit<hits.totalHits;hit++) {
+        if (hits.scoreDocs[hit].doc == docID) {
+          found = true;
+          break;
+        }
+      }
+      assertTrue(found);
+    }
+    */
+
+    return searcher;
   }
   
   public String getRandomNumberString(int num, int low, int high) {
     StringBuilder sb = new StringBuilder();
-    for (int i = 0; i < num; i++) {
+    for(int i = 0; i < num; i++) {
       sb.append(getRandomNumber(low, high));
     }
     return sb.toString();
@@ -288,7 +328,7 @@
   
   public String getRandomCharString(int num, int start, int end) {
     StringBuilder sb = new StringBuilder();
-    for (int i = 0; i < num; i++) {
+    for(int i = 0; i < num; i++) {
       sb.append(new Character((char) getRandomNumber(start, end)));
     }
     return sb.toString();
@@ -303,17 +343,17 @@
 
   private IndexSearcher getXIndex()
   throws IOException {
-    return getIndex (true, false);
+    return getIndex(true, false);
   }
 
   private IndexSearcher getYIndex()
   throws IOException {
-    return getIndex (false, true);
+    return getIndex(false, true);
   }
 
   private IndexSearcher getEmptyIndex()
   throws IOException {
-    return getIndex (false, false);
+    return getIndex(false, false);
   }
 
   // Set to true if the DV "string" field is indexed as a
@@ -328,13 +368,13 @@
     full = getFullIndex();
     searchX = getXIndex();
     searchY = getYIndex();
-    queryX = new TermQuery (new Term ("contents", "x"));
-    queryY = new TermQuery (new Term ("contents", "y"));
-    queryA = new TermQuery (new Term ("contents", "a"));
-    queryE = new TermQuery (new Term ("contents", "e"));
-    queryF = new TermQuery (new Term ("contents", "f"));
-    queryG = new TermQuery (new Term ("contents", "g"));
-    queryM = new TermQuery (new Term ("contents", "m"));
+    queryX = new TermQuery(new Term("contents", "x"));
+    queryY = new TermQuery(new Term("contents", "y"));
+    queryA = new TermQuery(new Term("contents", "a"));
+    queryE = new TermQuery(new Term("contents", "e"));
+    queryF = new TermQuery(new Term("contents", "f"));
+    queryG = new TermQuery(new Term("contents", "g"));
+    queryM = new TermQuery(new Term("contents", "m"));
     sort = new Sort();
     
   }
@@ -346,75 +386,76 @@
     full.reader.close();
     searchX.reader.close();
     searchY.reader.close();
-    for (Directory dir : dirs)
+    for(Directory dir : dirs) {
       dir.close();
+    }
     super.tearDown();
   }
 
   // test the sorts by score and document number
   public void testBuiltInSorts() throws Exception {
     sort = new Sort();
-    assertMatches (full, queryX, sort, "ACEGI");
-    assertMatches (full, queryY, sort, "BDFHJ");
+    assertMatches(full, queryX, sort, "ACEGI");
+    assertMatches(full, queryY, sort, "BDFHJ");
 
     sort.setSort(SortField.FIELD_DOC);
-    assertMatches (full, queryX, sort, "ACEGI");
-    assertMatches (full, queryY, sort, "BDFHJ");
+    assertMatches(full, queryX, sort, "ACEGI");
+    assertMatches(full, queryY, sort, "BDFHJ");
   }
 
-  private static SortField useDocValues(SortField field) {
-    field.setUseIndexValues(true);
-    return field;
-  }
   // test sorts where the type of field is specified
   public void testTypedSort() throws Exception {
-    sort.setSort (new SortField ("int", SortField.Type.INT), SortField.FIELD_DOC );
-    assertMatches (full, queryX, sort, "IGAEC");
-    assertMatches (full, queryY, sort, "DHFJB");
+    sort.setSort(new SortField("int", SortField.Type.INT), SortField.FIELD_DOC);
+    assertMatches(full, queryX, sort, "IGAEC");
+    assertMatches(full, queryY, sort, "DHFJB");
     
-    sort.setSort (new SortField ("float", SortField.Type.FLOAT), SortField.FIELD_DOC );
-    assertMatches (full, queryX, sort, "GCIEA");
-    assertMatches (full, queryY, sort, "DHJFB");
+    sort.setSort(new SortField("float", SortField.Type.FLOAT), SortField.FIELD_DOC);
+    assertMatches(full, queryX, sort, "GCIEA");
+    assertMatches(full, queryY, sort, "DHJFB");
 
-    sort.setSort (new SortField ("long", SortField.Type.LONG), SortField.FIELD_DOC );
-    assertMatches (full, queryX, sort, "EACGI");
-    assertMatches (full, queryY, sort, "FBJHD");
+    sort.setSort(new SortField("long", SortField.Type.LONG), SortField.FIELD_DOC);
+    assertMatches(full, queryX, sort, "EACGI");
+    assertMatches(full, queryY, sort, "FBJHD");
 
-    sort.setSort (new SortField ("double", SortField.Type.DOUBLE), SortField.FIELD_DOC );
-    assertMatches (full, queryX, sort, "AGICE");
-    assertMatches (full, queryY, sort, "DJHBF");
+    sort.setSort(new SortField("double", SortField.Type.DOUBLE), SortField.FIELD_DOC);
+    assertMatches(full, queryX, sort, "AGICE");
+    assertMatches(full, queryY, sort, "DJHBF");
     
-    sort.setSort (new SortField ("byte", SortField.Type.BYTE), SortField.FIELD_DOC );
-    assertMatches (full, queryX, sort, "CIGAE");
-    assertMatches (full, queryY, sort, "DHFBJ");
+    sort.setSort(new SortField("byte", SortField.Type.BYTE), SortField.FIELD_DOC);
+    assertMatches(full, queryX, sort, "CIGAE");
+    assertMatches(full, queryY, sort, "DHFBJ");
 
-    sort.setSort (new SortField ("short", SortField.Type.SHORT), SortField.FIELD_DOC );
-    assertMatches (full, queryX, sort, "IAGCE");
-    assertMatches (full, queryY, sort, "DFHBJ");
+    sort.setSort(new SortField("short", SortField.Type.SHORT), SortField.FIELD_DOC);
+    assertMatches(full, queryX, sort, "IAGCE");
+    assertMatches(full, queryY, sort, "DFHBJ");
 
-    sort.setSort (new SortField ("string", SortField.Type.STRING), SortField.FIELD_DOC );
-    assertMatches (full, queryX, sort, "AIGEC");
-    assertMatches (full, queryY, sort, "DJHFB");
+    sort.setSort(new SortField("string", SortField.Type.STRING), SortField.FIELD_DOC);
+    assertMatches(full, queryX, sort, "AIGEC");
+    assertMatches(full, queryY, sort, "DJHFB");
     
-    sort.setSort (useDocValues(new SortField ("int", SortField.Type.INT)), SortField.FIELD_DOC );
-    assertMatches (full, queryX, sort, "IGAEC");
-    assertMatches (full, queryY, sort, "DHFJB");
+    sort.setSort(new SortField("int_dv", SortField.Type.INT), SortField.FIELD_DOC);
+    assertMatches(full, queryX, sort, "IGAEC");
+    assertMatches(full, queryY, sort, "DHFJB");
 
-    sort.setSort (useDocValues(new SortField ("float", SortField.Type.FLOAT)), SortField.FIELD_DOC );
-    assertMatches (full, queryX, sort, "GCIEA");
-    assertMatches (full, queryY, sort, "DHJFB");
-      
-    sort.setSort (useDocValues(new SortField ("double", SortField.Type.DOUBLE)), SortField.FIELD_DOC );
-    assertMatches (full, queryX, sort, "AGICE");
-    assertMatches (full, queryY, sort, "DJHBF");
+    sort.setSort(new SortField("float_dv", SortField.Type.FLOAT), SortField.FIELD_DOC);
+    assertMatches(full, queryX, sort, "GCIEA");
+    assertMatches(full, queryY, sort, "DHJFB");
 
-    sort.setSort (useDocValues(new SortField ("string", getDVStringSortType())), SortField.FIELD_DOC );
-    assertMatches (full, queryX, sort, "AIGEC");
-    assertMatches (full, queryY, sort, "DJHFB");
+    sort.setSort(new SortField("double_dv", SortField.Type.DOUBLE), SortField.FIELD_DOC);
+    assertMatches(full, queryX, sort, "AGICE");
+    assertMatches(full, queryY, sort, "DJHBF");
+
+    sort.setSort(new SortField("string_dv", getDVStringSortType()), SortField.FIELD_DOC);
+    assertMatches(full, queryX, sort, "AIGEC");
+    assertMatches(full, queryY, sort, "DJHFB");
   }
 
   private SortField.Type getDVStringSortType() {
-    if (dvStringSorted) {
+    return getDVStringSortType(true);
+  }
+
+  private SortField.Type getDVStringSortType(boolean allowSorted) {
+    if (dvStringSorted && allowSorted) {
       // If you index as sorted source you can still sort by
       // value instead:
       return random().nextBoolean() ? SortField.Type.STRING : SortField.Type.STRING_VAL;
@@ -503,20 +544,23 @@
 
     // Doc values field, var length
     sort.setSort(
-                 useDocValues(new SortField("string", getDVStringSortType())),
-                 useDocValues(new SortField("string2", getDVStringSortType(), true)),
+                 new SortField("string_dv", getDVStringSortType()),
+                 new SortField("string2_dv", getDVStringSortType(), true),
                  SortField.FIELD_DOC);
     verifyStringSort(sort);
 
     // Doc values field, fixed length
     sort.setSort(
-                 useDocValues(new SortField("string_fixed", getDVStringSortType())),
-                 useDocValues(new SortField("string2_fixed", getDVStringSortType(), true)),
+                 new SortField("string_fixed_dv", getDVStringSortType()),
+                 new SortField("string2_fixed_dv", getDVStringSortType(), true),
                  SortField.FIELD_DOC);
     verifyStringSort(sort);
   }
 
   private void verifyStringSort(Sort sort) throws Exception {
+    if (VERBOSE) {
+      System.out.println("verifySort sort=" + sort);
+    }
     final IndexSearcher searcher = getFullStrings();
     final ScoreDoc[] result = searcher.search(new MatchAllDocsQuery(), null, _TestUtil.nextInt(random(), 500, searcher.getIndexReader().maxDoc()), sort).scoreDocs;
     StringBuilder buff = new StringBuilder();
@@ -525,13 +569,13 @@
     String lastSub = null;
     int lastDocId = 0;
     boolean fail = false;
-    final String fieldSuffix = sort.getSort()[0].getField().endsWith("_fixed") ? "_fixed" : "";
-    for (int x = 0; x < n; ++x) {
+    final String fieldSuffix = (sort.getSort()[0].getField().indexOf("_fixed") != -1) ? "_fixed" : "";
+    for(int x = 0; x < n; ++x) {
       StoredDocument doc2 = searcher.doc(result[x].doc);
       StorableField[] v = doc2.getFields("tracer" + fieldSuffix);
       StorableField[] v2 = doc2.getFields("tracer2" + fieldSuffix);
-      for (int j = 0; j < v.length; ++j) {
-        buff.append(v[j] + "(" + v2[j] + ")(" + result[x].doc+")\n");
+      for(int j = 0; j < v.length; ++j) {
+        buff.append(v[j].stringValue() + "(" + v2[j].stringValue() + ")(" + result[x].doc+")\n");
         if (last != null) {
           int cmp = v[j].stringValue().compareTo(last);
           if (!(cmp >= 0)) { // ensure first field is in order
@@ -578,7 +622,7 @@
     FieldCache fc = FieldCache.DEFAULT;
 
 
-    sort.setSort (new SortField ("parser", new FieldCache.IntParser(){
+    sort.setSort(new SortField("parser", new FieldCache.IntParser(){
       @Override
       public final int parseInt(final BytesRef term) {
         return (term.bytes[term.offset]-'A') * 123456;
@@ -593,10 +637,10 @@
     assertSaneFieldCaches(getTestName() + " IntParser");
     fc.purgeAllCaches();
 
-    sort.setSort (new SortField ("parser", new FieldCache.FloatParser(){
+    sort.setSort(new SortField("parser", new FieldCache.FloatParser(){
       @Override
       public final float parseFloat(final BytesRef term) {
-        return (float) Math.sqrt( term.bytes[term.offset] );
+        return (float) Math.sqrt( term.bytes[term.offset]);
       }
       @Override
       public TermsEnum termsEnum(Terms terms) throws IOException {
@@ -607,7 +651,7 @@
     assertSaneFieldCaches(getTestName() + " FloatParser");
     fc.purgeAllCaches();
 
-    sort.setSort (new SortField ("parser", new FieldCache.LongParser(){
+    sort.setSort(new SortField("parser", new FieldCache.LongParser(){
       @Override
       public final long parseLong(final BytesRef term) {
         return (term.bytes[term.offset]-'A') * 1234567890L;
@@ -622,10 +666,10 @@
     assertSaneFieldCaches(getTestName() + " LongParser");
     fc.purgeAllCaches();
 
-    sort.setSort (new SortField ("parser", new FieldCache.DoubleParser(){
+    sort.setSort(new SortField("parser", new FieldCache.DoubleParser(){
       @Override
       public final double parseDouble(final BytesRef term) {
-        return Math.pow( term.bytes[term.offset], (term.bytes[term.offset]-'A') );
+        return Math.pow( term.bytes[term.offset], (term.bytes[term.offset]-'A'));
       }
       @Override
       public TermsEnum termsEnum(Terms terms) throws IOException {
@@ -636,7 +680,7 @@
     assertSaneFieldCaches(getTestName() + " DoubleParser");
     fc.purgeAllCaches();
 
-    sort.setSort (new SortField ("parser", new FieldCache.ByteParser(){
+    sort.setSort(new SortField("parser", new FieldCache.ByteParser(){
       @Override
       public final byte parseByte(final BytesRef term) {
         return (byte) (term.bytes[term.offset]-'A');
@@ -651,7 +695,7 @@
     assertSaneFieldCaches(getTestName() + " ByteParser");
     fc.purgeAllCaches();
 
-    sort.setSort (new SortField ("parser", new FieldCache.ShortParser(){
+    sort.setSort(new SortField("parser", new FieldCache.ShortParser(){
       @Override
       public final short parseShort(final BytesRef term) {
         return (short) (term.bytes[term.offset]-'A');
@@ -671,39 +715,39 @@
     IndexSearcher empty = getEmptyIndex();
 
     sort = new Sort();
-    assertMatches (empty, queryX, sort, "");
+    assertMatches(empty, queryX, sort, "");
 
     sort.setSort(SortField.FIELD_DOC);
-    assertMatches (empty, queryX, sort, "");
+    assertMatches(empty, queryX, sort, "");
 
-    sort.setSort (new SortField ("int", SortField.Type.INT), SortField.FIELD_DOC );
-    assertMatches (empty, queryX, sort, "");
+    sort.setSort(new SortField("int", SortField.Type.INT), SortField.FIELD_DOC);
+    assertMatches(empty, queryX, sort, "");
     
-    sort.setSort (useDocValues(new SortField ("int", SortField.Type.INT)), SortField.FIELD_DOC );
-    assertMatches (empty, queryX, sort, "");
+    sort.setSort(new SortField("int_dv", SortField.Type.INT), SortField.FIELD_DOC);
+    assertMatches(empty, queryX, sort, "");
 
-    sort.setSort (new SortField ("string", SortField.Type.STRING, true), SortField.FIELD_DOC );
-    assertMatches (empty, queryX, sort, "");
+    sort.setSort(new SortField("string", SortField.Type.STRING, true), SortField.FIELD_DOC);
+    assertMatches(empty, queryX, sort, "");
 
-    sort.setSort (new SortField ("float", SortField.Type.FLOAT), new SortField ("string", SortField.Type.STRING) );
-    assertMatches (empty, queryX, sort, "");
+    sort.setSort(new SortField("float", SortField.Type.FLOAT), new SortField("string", SortField.Type.STRING));
+    assertMatches(empty, queryX, sort, "");
     
-    sort.setSort (useDocValues(new SortField ("float", SortField.Type.FLOAT)), new SortField ("string", SortField.Type.STRING) );
-    assertMatches (empty, queryX, sort, "");
+    sort.setSort(new SortField("float_dv", SortField.Type.FLOAT), new SortField("string", SortField.Type.STRING));
+    assertMatches(empty, queryX, sort, "");
 
-    sort.setSort (useDocValues(new SortField ("string", getDVStringSortType(), true)), SortField.FIELD_DOC );
-    assertMatches (empty, queryX, sort, "");
+    sort.setSort(new SortField("string_dv", getDVStringSortType(false), true), SortField.FIELD_DOC);
+    assertMatches(empty, queryX, sort, "");
 
-    sort.setSort (useDocValues(new SortField ("float", SortField.Type.FLOAT)),
-                  useDocValues(new SortField ("string", getDVStringSortType())) );
-    assertMatches (empty, queryX, sort, "");
+    sort.setSort(new SortField("float_dv", SortField.Type.FLOAT),
+                  new SortField("string_dv", getDVStringSortType(false)));
+    assertMatches(empty, queryX, sort, "");
     
-    sort.setSort (useDocValues(new SortField ("float", SortField.Type.FLOAT)), useDocValues(new SortField ("string", getDVStringSortType())) );
-    assertMatches (empty, queryX, sort, "");
+    sort.setSort(new SortField("float_dv", SortField.Type.FLOAT), new SortField("string_dv", getDVStringSortType(false)));
+    assertMatches(empty, queryX, sort, "");
   }
 
   static class MyFieldComparator extends FieldComparator<Integer> {
-    int[] docValues;
+    FieldCache.Ints docValues;
     int[] slotValues;
     int bottomValue;
 
@@ -713,7 +757,7 @@
 
     @Override
     public void copy(int slot, int doc) {
-      slotValues[slot] = docValues[doc];
+      slotValues[slot] = docValues.get(doc);
     }
 
     @Override
@@ -724,7 +768,7 @@
 
     @Override
     public int compareBottom(int doc) {
-      return bottomValue - docValues[doc];
+      return bottomValue - docValues.get(doc);
     }
 
     @Override
@@ -758,7 +802,7 @@
     @Override
     public int compareDocToValue(int doc, Integer valueObj) {
       final int value = valueObj.intValue();
-      final int docValue = docValues[doc];
+      final int docValue = docValues.get(doc);
 
       // values are small enough that overflow won't happen
       return docValue - value;
@@ -774,43 +818,43 @@
 
   // Test sorting w/ custom FieldComparator
   public void testNewCustomFieldParserSort() throws Exception {
-    sort.setSort (new SortField ("parser", new MyFieldComparatorSource()));
-    assertMatches (full, queryA, sort, "JIHGFEDCBA");
+    sort.setSort(new SortField("parser", new MyFieldComparatorSource()));
+    assertMatches(full, queryA, sort, "JIHGFEDCBA");
   }
 
   // test sorts in reverse
   public void testReverseSort() throws Exception {
-    sort.setSort (new SortField (null, SortField.Type.SCORE, true), SortField.FIELD_DOC );
-    assertMatches (full, queryX, sort, "IEGCA");
-    assertMatches (full, queryY, sort, "JFHDB");
+    sort.setSort(new SortField(null, SortField.Type.SCORE, true), SortField.FIELD_DOC);
+    assertMatches(full, queryX, sort, "IEGCA");
+    assertMatches(full, queryY, sort, "JFHDB");
 
-    sort.setSort (new SortField (null, SortField.Type.DOC, true));
-    assertMatches (full, queryX, sort, "IGECA");
-    assertMatches (full, queryY, sort, "JHFDB");
+    sort.setSort(new SortField(null, SortField.Type.DOC, true));
+    assertMatches(full, queryX, sort, "IGECA");
+    assertMatches(full, queryY, sort, "JHFDB");
 
-    sort.setSort (new SortField ("int", SortField.Type.INT, true) );
-    assertMatches (full, queryX, sort, "CAEGI");
-    assertMatches (full, queryY, sort, "BJFHD");
+    sort.setSort(new SortField("int", SortField.Type.INT, true));
+    assertMatches(full, queryX, sort, "CAEGI");
+    assertMatches(full, queryY, sort, "BJFHD");
 
-    sort.setSort (new SortField ("float", SortField.Type.FLOAT, true) );
-    assertMatches (full, queryX, sort, "AECIG");
-    assertMatches (full, queryY, sort, "BFJHD");
+    sort.setSort(new SortField("float", SortField.Type.FLOAT, true));
+    assertMatches(full, queryX, sort, "AECIG");
+    assertMatches(full, queryY, sort, "BFJHD");
     
-    sort.setSort (new SortField ("string", SortField.Type.STRING, true) );
-    assertMatches (full, queryX, sort, "CEGIA");
-    assertMatches (full, queryY, sort, "BFHJD");
+    sort.setSort(new SortField("string", SortField.Type.STRING, true));
+    assertMatches(full, queryX, sort, "CEGIA");
+    assertMatches(full, queryY, sort, "BFHJD");
     
-    sort.setSort (useDocValues(new SortField ("int", SortField.Type.INT, true)) );
-    assertMatches (full, queryX, sort, "CAEGI");
-    assertMatches (full, queryY, sort, "BJFHD");
+    sort.setSort(new SortField("int_dv", SortField.Type.INT, true));
+    assertMatches(full, queryX, sort, "CAEGI");
+    assertMatches(full, queryY, sort, "BJFHD");
     
-    sort.setSort (useDocValues(new SortField ("float", SortField.Type.FLOAT, true)) );
-    assertMatches (full, queryX, sort, "AECIG");
-    assertMatches (full, queryY, sort, "BFJHD");
+    sort.setSort(new SortField("float_dv", SortField.Type.FLOAT, true));
+    assertMatches(full, queryX, sort, "AECIG");
+    assertMatches(full, queryY, sort, "BFJHD");
 
-    sort.setSort (useDocValues(new SortField ("string", getDVStringSortType(), true)) );
-    assertMatches (full, queryX, sort, "CEGIA");
-    assertMatches (full, queryY, sort, "BFHJD");
+    sort.setSort(new SortField("string_dv", getDVStringSortType(), true));
+    assertMatches(full, queryX, sort, "CEGIA");
+    assertMatches(full, queryY, sort, "BFHJD");
   }
 
   // test sorting when the sort field is empty (undefined) for some of the documents
@@ -818,80 +862,80 @@
 
     // NOTE: do not test DocValues fields here, since you
     // can't sort when some documents don't have the field
-    sort.setSort (new SortField ("string", SortField.Type.STRING) );
-    assertMatches (full, queryF, sort, "ZJI");
+    sort.setSort(new SortField("string", SortField.Type.STRING));
+    assertMatches(full, queryF, sort, "ZJI");
 
-    sort.setSort (new SortField ("string", SortField.Type.STRING, true) );
-    assertMatches (full, queryF, sort, "IJZ");
+    sort.setSort(new SortField("string", SortField.Type.STRING, true));
+    assertMatches(full, queryF, sort, "IJZ");
     
-    sort.setSort (new SortField ("int", SortField.Type.INT) );
-    assertMatches (full, queryF, sort, "IZJ");
+    sort.setSort(new SortField("int", SortField.Type.INT));
+    assertMatches(full, queryF, sort, "IZJ");
 
-    sort.setSort (new SortField ("int", SortField.Type.INT, true) );
-    assertMatches (full, queryF, sort, "JZI");
+    sort.setSort(new SortField("int", SortField.Type.INT, true));
+    assertMatches(full, queryF, sort, "JZI");
 
-    sort.setSort (new SortField ("float", SortField.Type.FLOAT) );
-    assertMatches (full, queryF, sort, "ZJI");
+    sort.setSort(new SortField("float", SortField.Type.FLOAT));
+    assertMatches(full, queryF, sort, "ZJI");
 
     // using a nonexisting field as first sort key shouldn't make a difference:
-    sort.setSort (new SortField ("nosuchfield", SortField.Type.STRING),
-        new SortField ("float", SortField.Type.FLOAT) );
-    assertMatches (full, queryF, sort, "ZJI");
+    sort.setSort(new SortField("nosuchfield", SortField.Type.STRING),
+        new SortField("float", SortField.Type.FLOAT));
+    assertMatches(full, queryF, sort, "ZJI");
 
-    sort.setSort (new SortField ("float", SortField.Type.FLOAT, true) );
-    assertMatches (full, queryF, sort, "IJZ");
+    sort.setSort(new SortField("float", SortField.Type.FLOAT, true));
+    assertMatches(full, queryF, sort, "IJZ");
 
     // When a field is null for both documents, the next SortField should be used.
-    sort.setSort (new SortField ("int", SortField.Type.INT),
-                                new SortField ("string", SortField.Type.STRING),
-        new SortField ("float", SortField.Type.FLOAT) );
-    assertMatches (full, queryG, sort, "ZWXY");
+    sort.setSort(new SortField("int", SortField.Type.INT),
+                 new SortField("string", SortField.Type.STRING),
+        new SortField("float", SortField.Type.FLOAT));
+    assertMatches(full, queryG, sort, "ZWXY");
 
     // Reverse the last criterium to make sure the test didn't pass by chance
-    sort.setSort (new SortField ("int", SortField.Type.INT),
-                                new SortField ("string", SortField.Type.STRING),
-                  new SortField ("float", SortField.Type.FLOAT, true) );
-    assertMatches (full, queryG, sort, "ZYXW");
+    sort.setSort(new SortField("int", SortField.Type.INT),
+                 new SortField("string", SortField.Type.STRING),
+                 new SortField("float", SortField.Type.FLOAT, true));
+    assertMatches(full, queryG, sort, "ZYXW");
 
     // Do the same for a ParallelMultiSearcher
     ExecutorService exec = Executors.newFixedThreadPool(_TestUtil.nextInt(random(), 2, 8), new NamedThreadFactory("testEmptyFieldSort"));
-    IndexSearcher parallelSearcher=new IndexSearcher (full.getIndexReader(), exec);
+    IndexSearcher parallelSearcher = new IndexSearcher(full.getIndexReader(), exec);
 
-    sort.setSort (new SortField ("int", SortField.Type.INT),
-                  new SortField ("string", SortField.Type.STRING),
-                  new SortField ("float", SortField.Type.FLOAT) );
-    assertMatches (parallelSearcher, queryG, sort, "ZWXY");
+    sort.setSort(new SortField("int", SortField.Type.INT),
+                 new SortField("string", SortField.Type.STRING),
+                 new SortField("float", SortField.Type.FLOAT));
+    assertMatches(parallelSearcher, queryG, sort, "ZWXY");
 
-    sort.setSort (new SortField ("int", SortField.Type.INT),
-                  new SortField ("string", SortField.Type.STRING),
-                  new SortField ("float", SortField.Type.FLOAT, true) );
-    assertMatches (parallelSearcher, queryG, sort, "ZYXW");
+    sort.setSort(new SortField("int", SortField.Type.INT),
+                 new SortField("string", SortField.Type.STRING),
+                 new SortField("float", SortField.Type.FLOAT, true));
+    assertMatches(parallelSearcher, queryG, sort, "ZYXW");
     exec.shutdown();
     exec.awaitTermination(1000, TimeUnit.MILLISECONDS);
   }
 
   // test sorts using a series of fields
   public void testSortCombos() throws Exception {
-    sort.setSort (new SortField ("int", SortField.Type.INT), new SortField ("float", SortField.Type.FLOAT) );
-    assertMatches (full, queryX, sort, "IGEAC");
+    sort.setSort(new SortField("int", SortField.Type.INT), new SortField("float", SortField.Type.FLOAT));
+    assertMatches(full, queryX, sort, "IGEAC");
 
-    sort.setSort (new SortField ("int", SortField.Type.INT, true), new SortField (null, SortField.Type.DOC, true) );
-    assertMatches (full, queryX, sort, "CEAGI");
+    sort.setSort(new SortField("int", SortField.Type.INT, true), new SortField(null, SortField.Type.DOC, true));
+    assertMatches(full, queryX, sort, "CEAGI");
 
-    sort.setSort (new SortField ("float", SortField.Type.FLOAT), new SortField ("string", SortField.Type.STRING) );
-    assertMatches (full, queryX, sort, "GICEA");
+    sort.setSort(new SortField("float", SortField.Type.FLOAT), new SortField("string", SortField.Type.STRING));
+    assertMatches(full, queryX, sort, "GICEA");
 
-    sort.setSort (useDocValues(new SortField ("int", SortField.Type.INT)),
-                  useDocValues(new SortField ("float", SortField.Type.FLOAT)));
-    assertMatches (full, queryX, sort, "IGEAC");
+    sort.setSort(new SortField("int_dv", SortField.Type.INT),
+                 new SortField("float_dv", SortField.Type.FLOAT));
+    assertMatches(full, queryX, sort, "IGEAC");
 
-    sort.setSort (useDocValues(new SortField ("int", SortField.Type.INT, true)),
-                  useDocValues(new SortField (null, SortField.Type.DOC, true)));
-    assertMatches (full, queryX, sort, "CEAGI");
+    sort.setSort(new SortField("int_dv", SortField.Type.INT, true),
+                 new SortField(null, SortField.Type.DOC, true));
+    assertMatches(full, queryX, sort, "CEAGI");
 
-    sort.setSort (useDocValues(new SortField ("float", SortField.Type.FLOAT)),
-                  useDocValues(new SortField ("string", getDVStringSortType())));
-    assertMatches (full, queryX, sort, "GICEA");
+    sort.setSort(new SortField("float_dv", SortField.Type.FLOAT),
+                 new SortField("string_dv", getDVStringSortType()));
+    assertMatches(full, queryX, sort, "GICEA");
   }
 
   // test a variety of sorts using a parallel multisearcher
@@ -900,9 +944,12 @@
     IndexSearcher searcher = new IndexSearcher(
                                   new MultiReader(searchX.getIndexReader(),
                                                   searchY.getIndexReader()), exec);
-    runMultiSorts(searcher, false);
-    exec.shutdown();
-    exec.awaitTermination(1000, TimeUnit.MILLISECONDS);
+    try {
+      runMultiSorts(searcher, false);
+    } finally {
+      exec.shutdown();
+      exec.awaitTermination(1000, TimeUnit.MILLISECONDS);
+    }
   }
 
   public void testTopDocsScores() throws Exception {
@@ -919,7 +966,7 @@
     // a filter that only allows through the first hit
     Filter filt = new Filter() {
       @Override
-      public DocIdSet getDocIdSet (AtomicReaderContext context, Bits acceptDocs) {
+      public DocIdSet getDocIdSet(AtomicReaderContext context, Bits acceptDocs) {
         assertNull("acceptDocs should be null, as we have no deletions", acceptDocs);
         BitSet bs = new BitSet(context.reader().maxDoc());
         bs.set(0, context.reader().maxDoc());
@@ -941,7 +988,7 @@
     // does not use Searcher's default search methods (with Sort) since all set
     // fillFields to true.
     Sort[] sort = new Sort[] { new Sort(SortField.FIELD_DOC), new Sort() };
-    for (int i = 0; i < sort.length; i++) {
+    for(int i = 0; i < sort.length; i++) {
       Query q = new MatchAllDocsQuery();
       TopDocsCollector<Entry> tdc = TopFieldCollector.create(sort[i], 10, false,
           false, false, true);
@@ -949,7 +996,7 @@
       full.search(q, tdc);
       
       ScoreDoc[] sd = tdc.topDocs().scoreDocs;
-      for (int j = 1; j < sd.length; j++) {
+      for(int j = 1; j < sd.length; j++) {
         assertTrue(sd[j].doc != sd[j - 1].doc);
       }
       
@@ -960,7 +1007,7 @@
 
     // Two Sort criteria to instantiate the multi/single comparators.
     Sort[] sort = new Sort[] {new Sort(SortField.FIELD_DOC), new Sort() };
-    for (int i = 0; i < sort.length; i++) {
+    for(int i = 0; i < sort.length; i++) {
       Query q = new MatchAllDocsQuery();
       TopDocsCollector<Entry> tdc = TopFieldCollector.create(sort[i], 10, true, false,
           false, true);
@@ -969,7 +1016,7 @@
       
       TopDocs td = tdc.topDocs();
       ScoreDoc[] sd = td.scoreDocs;
-      for (int j = 0; j < sd.length; j++) {
+      for(int j = 0; j < sd.length; j++) {
         assertTrue(Float.isNaN(sd[j].score));
       }
       assertTrue(Float.isNaN(td.getMaxScore()));
@@ -980,7 +1027,7 @@
     
     // Two Sort criteria to instantiate the multi/single comparators.
     Sort[] sort = new Sort[] {new Sort(SortField.FIELD_DOC), new Sort() };
-    for (int i = 0; i < sort.length; i++) {
+    for(int i = 0; i < sort.length; i++) {
       Query q = new MatchAllDocsQuery();
       TopDocsCollector<Entry> tdc = TopFieldCollector.create(sort[i], 10, true, true,
           false, true);
@@ -989,7 +1036,7 @@
       
       TopDocs td = tdc.topDocs();
       ScoreDoc[] sd = td.scoreDocs;
-      for (int j = 0; j < sd.length; j++) {
+      for(int j = 0; j < sd.length; j++) {
         assertTrue(!Float.isNaN(sd[j].score));
       }
       assertTrue(Float.isNaN(td.getMaxScore()));
@@ -1001,7 +1048,7 @@
     
     // Two Sort criteria to instantiate the multi/single comparators.
     Sort[] sort = new Sort[] {new Sort(SortField.FIELD_DOC, SortField.FIELD_SCORE) };
-    for (int i = 0; i < sort.length; i++) {
+    for(int i = 0; i < sort.length; i++) {
       Query q = new MatchAllDocsQuery();
       TopDocsCollector<Entry> tdc = TopFieldCollector.create(sort[i], 10, true, true,
           false, true);
@@ -1010,7 +1057,7 @@
       
       TopDocs td = tdc.topDocs();
       ScoreDoc[] sd = td.scoreDocs;
-      for (int j = 0; j < sd.length; j++) {
+      for(int j = 0; j < sd.length; j++) {
         assertTrue(!Float.isNaN(sd[j].score));
       }
       assertTrue(Float.isNaN(td.getMaxScore()));
@@ -1021,7 +1068,7 @@
     
     // Two Sort criteria to instantiate the multi/single comparators.
     Sort[] sort = new Sort[] {new Sort(SortField.FIELD_DOC), new Sort() };
-    for (int i = 0; i < sort.length; i++) {
+    for(int i = 0; i < sort.length; i++) {
       Query q = new MatchAllDocsQuery();
       TopDocsCollector<Entry> tdc = TopFieldCollector.create(sort[i], 10, true, true,
           true, true);
@@ -1030,7 +1077,7 @@
       
       TopDocs td = tdc.topDocs();
       ScoreDoc[] sd = td.scoreDocs;
-      for (int j = 0; j < sd.length; j++) {
+      for(int j = 0; j < sd.length; j++) {
         assertTrue(!Float.isNaN(sd[j].score));
       }
       assertTrue(!Float.isNaN(td.getMaxScore()));
@@ -1069,8 +1116,8 @@
     // Set minNrShouldMatch to 1 so that BQ will not optimize rewrite to return
     // the clause instead of BQ.
     bq.setMinimumNumberShouldMatch(1);
-    for (int i = 0; i < sort.length; i++) {
-      for (int j = 0; j < tfcOptions.length; j++) {
+    for(int i = 0; i < sort.length; i++) {
+      for(int j = 0; j < tfcOptions.length; j++) {
         TopDocsCollector<Entry> tdc = TopFieldCollector.create(sort[i], 10,
             tfcOptions[j][0], tfcOptions[j][1], tfcOptions[j][2], false);
 
@@ -1118,8 +1165,8 @@
     // Set minNrShouldMatch to 1 so that BQ will not optimize rewrite to return
     // the clause instead of BQ.
     bq.setMinimumNumberShouldMatch(1);
-    for (int i = 0; i < sort.length; i++) {
-      for (int j = 0; j < tfcOptions.length; j++) {
+    for(int i = 0; i < sort.length; i++) {
+      for(int j = 0; j < tfcOptions.length; j++) {
         TopDocsCollector<Entry> tdc = TopFieldCollector.create(sort[i], 10,
             tfcOptions[j][0], tfcOptions[j][1], tfcOptions[j][2], false);
 
@@ -1138,7 +1185,7 @@
     
     // Two Sort criteria to instantiate the multi/single comparators.
     Sort[] sort = new Sort[] {new Sort(SortField.FIELD_DOC), new Sort() };
-    for (int i = 0; i < sort.length; i++) {
+    for(int i = 0; i < sort.length; i++) {
       TopDocsCollector<Entry> tdc = TopFieldCollector.create(sort[i], 10, true, true, true, true);
       TopDocs td = tdc.topDocs();
       assertEquals(0, td.totalHits);
@@ -1152,11 +1199,11 @@
     String expected = isFull ? "ABCDEFGHIJ" : "ACEGIBDFHJ";
     assertMatches(multi, queryA, sort, expected);
 
-    sort.setSort(new SortField ("int", SortField.Type.INT));
+    sort.setSort(new SortField("int", SortField.Type.INT));
     expected = isFull ? "IDHFGJABEC" : "IDHFGJAEBC";
     assertMatches(multi, queryA, sort, expected);
 
-    sort.setSort(new SortField ("int", SortField.Type.INT), SortField.FIELD_DOC);
+    sort.setSort(new SortField("int", SortField.Type.INT), SortField.FIELD_DOC);
     expected = isFull ? "IDHFGJABEC" : "IDHFGJAEBC";
     assertMatches(multi, queryA, sort, expected);
 
@@ -1164,7 +1211,7 @@
     expected = isFull ? "IDHFGJABEC" : "IDHFGJAEBC";
     assertMatches(multi, queryA, sort, expected);
     
-    sort.setSort(new SortField ("float", SortField.Type.FLOAT), SortField.FIELD_DOC);
+    sort.setSort(new SortField("float", SortField.Type.FLOAT), SortField.FIELD_DOC);
     assertMatches(multi, queryA, sort, "GDHJCIEFAB");
 
     sort.setSort(new SortField("float", SortField.Type.FLOAT));
@@ -1189,65 +1236,65 @@
     sort.setSort(new SortField("float", SortField.Type.FLOAT),new SortField("string", SortField.Type.STRING));
     assertMatches(multi, queryA, sort, "GDHJICEFAB");
 
-    sort.setSort(new SortField ("int", SortField.Type.INT));
+    sort.setSort(new SortField("int", SortField.Type.INT));
     assertMatches(multi, queryF, sort, "IZJ");
 
-    sort.setSort(new SortField ("int", SortField.Type.INT, true));
+    sort.setSort(new SortField("int", SortField.Type.INT, true));
     assertMatches(multi, queryF, sort, "JZI");
 
-    sort.setSort(new SortField ("float", SortField.Type.FLOAT));
+    sort.setSort(new SortField("float", SortField.Type.FLOAT));
     assertMatches(multi, queryF, sort, "ZJI");
 
-    sort.setSort(new SortField ("string", SortField.Type.STRING));
+    sort.setSort(new SortField("string", SortField.Type.STRING));
     assertMatches(multi, queryF, sort, "ZJI");
 
-    sort.setSort(new SortField ("string", SortField.Type.STRING, true));
+    sort.setSort(new SortField("string", SortField.Type.STRING, true));
     assertMatches(multi, queryF, sort, "IJZ");
 
-    sort.setSort(useDocValues(new SortField ("int", SortField.Type.INT)));
+    sort.setSort(new SortField("int_dv", SortField.Type.INT));
     expected = isFull ? "IDHFGJABEC" : "IDHFGJAEBC";
     assertMatches(multi, queryA, sort, expected);
 
-    sort.setSort(useDocValues(new SortField ("int", SortField.Type.INT)), SortField.FIELD_DOC);
+    sort.setSort(new SortField("int_dv", SortField.Type.INT), SortField.FIELD_DOC);
     expected = isFull ? "IDHFGJABEC" : "IDHFGJAEBC";
     assertMatches(multi, queryA, sort, expected);
 
-    sort.setSort(useDocValues(new SortField("int", SortField.Type.INT)));
+    sort.setSort(new SortField("int_dv", SortField.Type.INT));
     expected = isFull ? "IDHFGJABEC" : "IDHFGJAEBC";
     assertMatches(multi, queryA, sort, expected);
     
-    sort.setSort(useDocValues(new SortField ("float", SortField.Type.FLOAT)), SortField.FIELD_DOC);
+    sort.setSort(new SortField("float_dv", SortField.Type.FLOAT), SortField.FIELD_DOC);
     assertMatches(multi, queryA, sort, "GDHJCIEFAB");
 
-    sort.setSort(useDocValues(new SortField("float", SortField.Type.FLOAT)));
+    sort.setSort(new SortField("float_dv", SortField.Type.FLOAT));
     assertMatches(multi, queryA, sort, "GDHJCIEFAB");
     
-    sort.setSort(useDocValues(new SortField("int", SortField.Type.INT, true)));
+    sort.setSort(new SortField("int_dv", SortField.Type.INT, true));
     expected = isFull ? "CABEJGFHDI" : "CAEBJGFHDI";
     assertMatches(multi, queryA, sort, expected);
     
-    sort.setSort(useDocValues(new SortField("int", SortField.Type.INT)), useDocValues(new SortField("float", SortField.Type.FLOAT)));
+    sort.setSort(new SortField("int_dv", SortField.Type.INT), new SortField("float_dv", SortField.Type.FLOAT));
     assertMatches(multi, queryA, sort, "IDHFGJEABC");
     
-    sort.setSort(useDocValues(new SortField ("int", SortField.Type.INT)));
+    sort.setSort(new SortField("int_dv", SortField.Type.INT));
     assertMatches(multi, queryF, sort, "IZJ");
 
-    sort.setSort(useDocValues(new SortField ("int", SortField.Type.INT, true)));
+    sort.setSort(new SortField("int_dv", SortField.Type.INT, true));
     assertMatches(multi, queryF, sort, "JZI");
 
-    sort.setSort(useDocValues(new SortField("string", getDVStringSortType())));
+    sort.setSort(new SortField("string_dv", getDVStringSortType()));
     assertMatches(multi, queryA, sort, "DJAIHGFEBC");
       
-    sort.setSort(useDocValues(new SortField("string", getDVStringSortType(), true)));
+    sort.setSort(new SortField("string_dv", getDVStringSortType(), true));
     assertMatches(multi, queryA, sort, "CBEFGHIAJD");
       
-    sort.setSort(useDocValues(new SortField("float", SortField.Type.FLOAT)),useDocValues(new SortField("string", getDVStringSortType())));
+    sort.setSort(new SortField("float_dv", SortField.Type.FLOAT), new SortField("string_dv", getDVStringSortType()));
     assertMatches(multi, queryA, sort, "GDHJICEFAB");
 
-    sort.setSort(useDocValues(new SortField ("string", getDVStringSortType())));
+    sort.setSort(new SortField("string_dv", getDVStringSortType()));
     assertMatches(multi, queryF, sort, "ZJI");
 
-    sort.setSort(useDocValues(new SortField ("string", getDVStringSortType(), true)));
+    sort.setSort(new SortField("string_dv", getDVStringSortType(), true));
     assertMatches(multi, queryF, sort, "IJZ");
     
     // up to this point, all of the searches should have "sane" 
@@ -1258,13 +1305,16 @@
   }
 
   private void assertMatches(IndexSearcher searcher, Query query, Sort sort, String expectedResult) throws IOException {
-    assertMatches( null, searcher, query, sort, expectedResult );
+    assertMatches( null, searcher, query, sort, expectedResult);
   }
 
 
   // make sure the documents returned by the search match the expected list
   private void assertMatches(String msg, IndexSearcher searcher, Query query, Sort sort,
       String expectedResult) throws IOException {
+    if (VERBOSE) {
+      System.out.println("assertMatches searcher=" + searcher + " sort=" + sort);
+    }
 
     //ScoreDoc[] result = searcher.search (query, null, 1000, sort).scoreDocs;
     TopDocs hits = searcher.search(query, null, Math.max(1, expectedResult.length()), sort, true, true);
@@ -1272,10 +1322,10 @@
     assertEquals(expectedResult.length(),hits.totalHits);
     StringBuilder buff = new StringBuilder(10);
     int n = result.length;
-    for (int i=0; i<n; ++i) {
+    for(int i=0; i<n; ++i) {
       StoredDocument doc = searcher.doc(result[i].doc);
       StorableField[] v = doc.getFields("tracer");
-      for (int j=0; j<v.length; ++j) {
+      for(int j=0; j<v.length; ++j) {
         buff.append (v[j].stringValue());
       }
     }
@@ -1311,17 +1361,17 @@
     Directory indexStore = newDirectory();
     IndexWriter writer = new IndexWriter(indexStore, newIndexWriterConfig(
         TEST_VERSION_CURRENT, new MockAnalyzer(random())));
-    for (int i=0; i<5; i++) {
+    for(int i=0; i<5; i++) {
         Document doc = new Document();
-        doc.add (new StringField ("string", "a"+i, Field.Store.NO));
-        doc.add (new StringField ("string", "b"+i, Field.Store.NO));
-        writer.addDocument (doc);
+        doc.add(new StringField("string", "a"+i, Field.Store.NO));
+        doc.add(new StringField("string", "b"+i, Field.Store.NO));
+        writer.addDocument(doc);
     }
     writer.forceMerge(1); // enforce one segment to have a higher unique term count in all cases
     writer.close();
     sort.setSort(
         new SortField("string", SortField.Type.STRING),
-        SortField.FIELD_DOC );
+        SortField.FIELD_DOC);
     // this should not throw AIOOBE or RuntimeEx
     IndexReader reader = DirectoryReader.open(indexStore);
     IndexSearcher searcher = new IndexSearcher(reader);
@@ -1333,11 +1383,11 @@
   public void testCountingCollector() throws Exception {
     Directory indexStore = newDirectory();
     RandomIndexWriter writer = new RandomIndexWriter(random(), indexStore);
-    for (int i=0; i<5; i++) {
+    for(int i=0; i<5; i++) {
       Document doc = new Document();
-      doc.add (new StringField ("string", "a"+i, Field.Store.NO));
-      doc.add (new StringField ("string", "b"+i, Field.Store.NO));
-      writer.addDocument (doc);
+      doc.add(new StringField("string", "a"+i, Field.Store.NO));
+      doc.add(new StringField("string", "b"+i, Field.Store.NO));
+      writer.addDocument(doc);
     }
     IndexReader reader = writer.getReader();
     writer.close();
@@ -1366,14 +1416,14 @@
     @Override
     public DocIdSet getDocIdSet(AtomicReaderContext context, Bits acceptDocs) throws IOException {
       final int maxDoc = context.reader().maxDoc();
-      final DocValues.Source idSource = context.reader().docValues("id").getSource();
+      final FieldCache.Ints idSource = FieldCache.DEFAULT.getInts(context.reader(), "id", false);
       assertNotNull(idSource);
       final FixedBitSet bits = new FixedBitSet(maxDoc);
       for(int docID=0;docID<maxDoc;docID++) {
         if (random.nextFloat() <= density && (acceptDocs == null || acceptDocs.get(docID))) {
           bits.set(docID);
           //System.out.println("  acc id=" + idSource.getInt(docID) + " docID=" + docID);
-          matchValues.add(docValues.get((int) idSource.getInt(docID)));
+          matchValues.add(docValues.get(idSource.get(docID)));
         }
       }
 
@@ -1418,9 +1468,9 @@
       }
       
       final Document doc = new Document();
-      doc.add(new SortedBytesDocValuesField("stringdv", br));
+      doc.add(new SortedDocValuesField("stringdv", br));
       doc.add(newStringField("string", s, Field.Store.NO));
-      doc.add(new PackedLongDocValuesField("id", numDocs));
+      doc.add(new NumericDocValuesField("id", numDocs));
       docValues.add(br);
       writer.addDocument(doc);
       numDocs++;
@@ -1445,7 +1495,6 @@
       final SortField sf;
       if (random.nextBoolean()) {
         sf = new SortField("stringdv", SortField.Type.STRING, reverse);
-        sf.setUseIndexValues(true);
       } else {
         sf = new SortField("string", SortField.Type.STRING, reverse);
       }
Index: lucene/core/src/test/org/apache/lucene/search/payloads/TestPayloadNearQuery.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/search/payloads/TestPayloadNearQuery.java	(revision 1442822)
+++ lucene/core/src/test/org/apache/lucene/search/payloads/TestPayloadNearQuery.java	(working copy)
@@ -24,7 +24,6 @@
 import org.apache.lucene.document.Field;
 import org.apache.lucene.index.FieldInvertState;
 import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.Norm;
 import org.apache.lucene.index.RandomIndexWriter;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.search.CollectionStatistics;
Index: lucene/core/src/test/org/apache/lucene/search/payloads/TestPayloadTermQuery.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/search/payloads/TestPayloadTermQuery.java	(revision 1442822)
+++ lucene/core/src/test/org/apache/lucene/search/payloads/TestPayloadTermQuery.java	(working copy)
@@ -36,7 +36,6 @@
 import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.FieldInvertState;
 import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.Norm;
 import org.apache.lucene.index.RandomIndexWriter;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.store.Directory;
Index: lucene/core/src/test/org/apache/lucene/search/TestDocValuesScoring.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/search/TestDocValuesScoring.java	(revision 1442822)
+++ lucene/core/src/test/org/apache/lucene/search/TestDocValuesScoring.java	(working copy)
@@ -23,10 +23,8 @@
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.FloatDocValuesField;
 import org.apache.lucene.index.AtomicReaderContext;
-import org.apache.lucene.index.DocValues.Source;
 import org.apache.lucene.index.FieldInvertState;
 import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.Norm;
 import org.apache.lucene.index.RandomIndexWriter;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.search.similarities.PerFieldSimilarityWrapper;
@@ -50,7 +48,7 @@
     Document doc = new Document();
     Field field = newTextField("foo", "", Field.Store.NO);
     doc.add(field);
-    Field dvField = new FloatDocValuesField("foo_boost", 0.0f);
+    Field dvField = new FloatDocValuesField("foo_boost", 0.0F);
     doc.add(dvField);
     Field field2 = newTextField("bar", "", Field.Store.NO);
     doc.add(field2);
@@ -67,10 +65,10 @@
     iw.close();
     
     // no boosting
-    IndexSearcher searcher1 = newSearcher(ir);
+    IndexSearcher searcher1 = newSearcher(ir, false);
     final Similarity base = searcher1.getSimilarity();
     // boosting
-    IndexSearcher searcher2 = newSearcher(ir);
+    IndexSearcher searcher2 = newSearcher(ir, false);
     searcher2.setSimilarity(new PerFieldSimilarityWrapper() {
       final Similarity fooSim = new BoostingSimilarity(base, "foo_boost");
 
@@ -148,8 +146,8 @@
     }
     
     @Override
-    public void computeNorm(FieldInvertState state, Norm norm) {
-      sim.computeNorm(state, norm);
+    public long computeNorm(FieldInvertState state) {
+      return sim.computeNorm(state);
     }
 
     @Override
@@ -160,17 +158,17 @@
     @Override
     public ExactSimScorer exactSimScorer(SimWeight stats, AtomicReaderContext context) throws IOException {
       final ExactSimScorer sub = sim.exactSimScorer(stats, context);
-      final Source values = context.reader().docValues(boostField).getSource();
+      final FieldCache.Floats values = FieldCache.DEFAULT.getFloats(context.reader(), boostField, false);
 
       return new ExactSimScorer() {
         @Override
         public float score(int doc, int freq) {
-          return (float) values.getFloat(doc) * sub.score(doc, freq);
+          return values.get(doc) * sub.score(doc, freq);
         }
 
         @Override
         public Explanation explain(int doc, Explanation freq) {
-          Explanation boostExplanation = new Explanation((float) values.getFloat(doc), "indexDocValue(" + boostField + ")");
+          Explanation boostExplanation = new Explanation(values.get(doc), "indexDocValue(" + boostField + ")");
           Explanation simExplanation = sub.explain(doc, freq);
           Explanation expl = new Explanation(boostExplanation.getValue() * simExplanation.getValue(), "product of:");
           expl.addDetail(boostExplanation);
@@ -183,12 +181,12 @@
     @Override
     public SloppySimScorer sloppySimScorer(SimWeight stats, AtomicReaderContext context) throws IOException {
       final SloppySimScorer sub = sim.sloppySimScorer(stats, context);
-      final Source values = context.reader().docValues(boostField).getSource();
+      final FieldCache.Floats values = FieldCache.DEFAULT.getFloats(context.reader(), boostField, false);
       
       return new SloppySimScorer() {
         @Override
         public float score(int doc, float freq) {
-          return (float) values.getFloat(doc) * sub.score(doc, freq);
+          return values.get(doc) * sub.score(doc, freq);
         }
         
         @Override
@@ -203,7 +201,7 @@
 
         @Override
         public Explanation explain(int doc, Explanation freq) {
-          Explanation boostExplanation = new Explanation((float) values.getFloat(doc), "indexDocValue(" + boostField + ")");
+          Explanation boostExplanation = new Explanation(values.get(doc), "indexDocValue(" + boostField + ")");
           Explanation simExplanation = sub.explain(doc, freq);
           Explanation expl = new Explanation(boostExplanation.getValue() * simExplanation.getValue(), "product of:");
           expl.addDetail(boostExplanation);
Index: lucene/core/src/test/org/apache/lucene/search/TestSameScoresWithThreads.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/search/TestSameScoresWithThreads.java	(revision 0)
+++ lucene/core/src/test/org/apache/lucene/search/TestSameScoresWithThreads.java	(working copy)
@@ -0,0 +1,123 @@
+package org.apache.lucene.search;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.util.ArrayList;
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+import java.util.concurrent.CountDownLatch;
+
+import org.apache.lucene.document.Document;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.MultiFields;
+import org.apache.lucene.index.RandomIndexWriter;
+import org.apache.lucene.index.Term;
+import org.apache.lucene.index.Terms;
+import org.apache.lucene.index.TermsEnum;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.LineFileDocs;
+import org.apache.lucene.util.LuceneTestCase;
+import org.apache.lucene.util.RamUsageEstimator;
+import org.apache.lucene.util._TestUtil;
+
+public class TestSameScoresWithThreads extends LuceneTestCase {
+
+  public void test() throws Exception {
+    final Directory dir = newDirectory();
+    final RandomIndexWriter w = new RandomIndexWriter(random(), dir);
+    LineFileDocs docs = new LineFileDocs(random());
+    int charsToIndex = atLeast(100000);
+    int charsIndexed = 0;
+    //System.out.println("bytesToIndex=" + charsToIndex);
+    while(charsIndexed < charsToIndex) {
+      Document doc = docs.nextDoc();
+      charsIndexed += doc.get("body").length();
+      w.addDocument(doc);
+      //System.out.println("  bytes=" + charsIndexed + " add: " + doc);
+    }
+    IndexReader r = w.getReader();
+    //System.out.println("numDocs=" + r.numDocs());
+    w.close();
+
+    final IndexSearcher s = new IndexSearcher(r);
+    Terms terms = MultiFields.getFields(r).terms("body");
+    int termCount = 0;
+    TermsEnum termsEnum = terms.iterator(null);
+    while(termsEnum.next() != null) {
+      termCount++;
+    }
+    assertTrue(termCount > 0);
+    
+    // Target ~10 terms to search:
+    double chance = 10.0 / termCount;
+    termsEnum = terms.iterator(termsEnum);
+    final Map<BytesRef,TopDocs> answers = new HashMap<BytesRef,TopDocs>();
+    while(termsEnum.next() != null) {
+      if (random().nextDouble() <= chance) {
+        BytesRef term = BytesRef.deepCopyOf(termsEnum.term());
+        answers.put(term,
+                    s.search(new TermQuery(new Term("body", term)), 100));
+      }
+    }
+
+    if (!answers.isEmpty()) {
+      final CountDownLatch startingGun = new CountDownLatch(1);
+      int numThreads = _TestUtil.nextInt(random(), 2, 5);
+      Thread[] threads = new Thread[numThreads];
+      for(int threadID=0;threadID<numThreads;threadID++) {
+        Thread thread = new Thread() {
+            @Override
+            public void run() {
+              try {
+                startingGun.await();
+                for(int i=0;i<20;i++) {
+                  List<Map.Entry<BytesRef,TopDocs>> shuffled = new ArrayList<Map.Entry<BytesRef,TopDocs>>(answers.entrySet());
+                  Collections.shuffle(shuffled);
+                  for(Map.Entry<BytesRef,TopDocs> ent : shuffled) {
+                    TopDocs actual = s.search(new TermQuery(new Term("body", ent.getKey())), 100);
+                    TopDocs expected = ent.getValue();
+                    assertEquals(expected.totalHits, actual.totalHits);
+                    assertEquals("query=" + ent.getKey().utf8ToString(), expected.scoreDocs.length, actual.scoreDocs.length);
+                    for(int hit=0;hit<expected.scoreDocs.length;hit++) {
+                      assertEquals(expected.scoreDocs[hit].doc, actual.scoreDocs[hit].doc);
+                      // Floats really should be identical:
+                      assertTrue(expected.scoreDocs[hit].score == actual.scoreDocs[hit].score);
+                    }
+                  }
+                }
+              } catch (Exception e) {
+                throw new RuntimeException(e);
+              }
+            }
+          };
+        threads[threadID] = thread;
+        thread.start();
+      }
+      startingGun.countDown();
+      for(Thread thread : threads) {
+        thread.join();
+      }
+      r.close();
+    }
+
+    dir.close();
+  }
+}

Property changes on: lucene/core/src/test/org/apache/lucene/search/TestSameScoresWithThreads.java
___________________________________________________________________
Added: svn:eol-style
## -0,0 +1 ##
+native
Index: lucene/core/src/test/org/apache/lucene/index/TestDocTermOrds.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/index/TestDocTermOrds.java	(revision 1442822)
+++ lucene/core/src/test/org/apache/lucene/index/TestDocTermOrds.java	(working copy)
@@ -32,7 +32,6 @@
 import org.apache.lucene.index.DocTermOrds.TermOrdsIterator;
 import org.apache.lucene.search.FieldCache;
 import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.MockDirectoryWrapper;
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.LuceneTestCase;
 import org.apache.lucene.util.StringHelper;
@@ -303,7 +302,7 @@
                                             _TestUtil.nextInt(random(), 2, 10));
                                             
 
-    final int[] docIDToID = FieldCache.DEFAULT.getInts(r, "id", false);
+    final FieldCache.Ints docIDToID = FieldCache.DEFAULT.getInts(r, "id", false);
     /*
       for(int docID=0;docID<subR.maxDoc();docID++) {
       System.out.println("  docID=" + docID + " id=" + docIDToID[docID]);
@@ -357,10 +356,10 @@
     final int[] buffer = new int[5];
     for(int docID=0;docID<r.maxDoc();docID++) {
       if (VERBOSE) {
-        System.out.println("TEST: docID=" + docID + " of " + r.maxDoc() + " (id=" + docIDToID[docID] + ")");
+        System.out.println("TEST: docID=" + docID + " of " + r.maxDoc() + " (id=" + docIDToID.get(docID) + ")");
       }
       iter = dto.lookup(docID, iter);
-      final int[] answers = idToOrds[docIDToID[docID]];
+      final int[] answers = idToOrds[docIDToID.get(docID)];
       int upto = 0;
       while(true) {
         final int chunk = iter.read(buffer);
Index: lucene/core/src/test/org/apache/lucene/index/TestDocValuesWithThreads.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/index/TestDocValuesWithThreads.java	(revision 0)
+++ lucene/core/src/test/org/apache/lucene/index/TestDocValuesWithThreads.java	(working copy)
@@ -0,0 +1,230 @@
+package org.apache.lucene.index;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.HashSet;
+import java.util.List;
+import java.util.Random;
+import java.util.Set;
+import java.util.concurrent.CountDownLatch;
+
+import org.apache.lucene.analysis.MockAnalyzer;
+import org.apache.lucene.document.BinaryDocValuesField;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.NumericDocValuesField;
+import org.apache.lucene.document.SortedDocValuesField;
+import org.apache.lucene.search.FieldCache;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.LuceneTestCase;
+import org.apache.lucene.util._TestUtil;
+
+public class TestDocValuesWithThreads extends LuceneTestCase {
+
+  public void test() throws Exception {
+    Directory dir = newDirectory();
+    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(newLogMergePolicy()));
+
+    final List<Long> numbers = new ArrayList<Long>();
+    final List<BytesRef> binary = new ArrayList<BytesRef>();
+    final List<BytesRef> sorted = new ArrayList<BytesRef>();
+    final int numDocs = atLeast(100);
+    for(int i=0;i<numDocs;i++) {
+      Document d = new Document();
+      long number = random().nextLong();
+      d.add(new NumericDocValuesField("number", number));
+      BytesRef bytes = new BytesRef(_TestUtil.randomRealisticUnicodeString(random()));
+      d.add(new BinaryDocValuesField("bytes", bytes));
+      binary.add(bytes);
+      bytes = new BytesRef(_TestUtil.randomRealisticUnicodeString(random()));
+      d.add(new SortedDocValuesField("sorted", bytes));
+      sorted.add(bytes);
+      w.addDocument(d);
+      numbers.add(number);
+    }
+
+    w.forceMerge(1);
+    final IndexReader r = w.getReader();
+    w.close();
+
+    assertEquals(1, r.leaves().size());
+    final AtomicReader ar = r.leaves().get(0).reader();
+
+    int numThreads = _TestUtil.nextInt(random(), 2, 5);
+    List<Thread> threads = new ArrayList<Thread>();
+    final CountDownLatch startingGun = new CountDownLatch(1);
+    for(int t=0;t<numThreads;t++) {
+      final Random threadRandom = new Random(random().nextLong());
+      Thread thread = new Thread() {
+          @Override
+          public void run() {
+            try {
+              //NumericDocValues ndv = ar.getNumericDocValues("number");
+              FieldCache.Longs ndv = FieldCache.DEFAULT.getLongs(ar, "number", false);
+              //BinaryDocValues bdv = ar.getBinaryDocValues("bytes");
+              BinaryDocValues bdv = FieldCache.DEFAULT.getTerms(ar, "bytes");
+              SortedDocValues sdv = FieldCache.DEFAULT.getTermsIndex(ar, "sorted");
+              startingGun.await();
+              int iters = atLeast(1000);
+              BytesRef scratch = new BytesRef();
+              BytesRef scratch2 = new BytesRef();
+              for(int iter=0;iter<iters;iter++) {
+                int docID = threadRandom.nextInt(numDocs);
+                switch(threadRandom.nextInt(6)) {
+                case 0:
+                  assertEquals((byte) numbers.get(docID).longValue(), FieldCache.DEFAULT.getBytes(ar, "number", false).get(docID));
+                  break;
+                case 1:
+                  assertEquals((short) numbers.get(docID).longValue(), FieldCache.DEFAULT.getShorts(ar, "number", false).get(docID));
+                  break;
+                case 2:
+                  assertEquals((int) numbers.get(docID).longValue(), FieldCache.DEFAULT.getInts(ar, "number", false).get(docID));
+                  break;
+                case 3:
+                  assertEquals(numbers.get(docID).longValue(), FieldCache.DEFAULT.getLongs(ar, "number", false).get(docID));
+                  break;
+                case 4:
+                  assertEquals(Float.intBitsToFloat((int) numbers.get(docID).longValue()), FieldCache.DEFAULT.getFloats(ar, "number", false).get(docID), 0.0f);
+                  break;
+                case 5:
+                  assertEquals(Double.longBitsToDouble(numbers.get(docID).longValue()), FieldCache.DEFAULT.getDoubles(ar, "number", false).get(docID), 0.0);
+                  break;
+                }
+                bdv.get(docID, scratch);
+                assertEquals(binary.get(docID), scratch);
+                // Cannot share a single scratch against two "sources":
+                sdv.get(docID, scratch2);
+                assertEquals(sorted.get(docID), scratch2);
+              }
+            } catch (Exception e) {
+              throw new RuntimeException(e);
+            }
+          }
+        };
+      thread.start();
+      threads.add(thread);
+    }
+
+    startingGun.countDown();
+
+    for(Thread thread : threads) {
+      thread.join();
+    }
+
+    r.close();
+    dir.close();
+  }
+  
+  public void test2() throws Exception {
+    Random random = random();
+    final int NUM_DOCS = atLeast(100);
+    final Directory dir = newDirectory();
+    final RandomIndexWriter writer = new RandomIndexWriter(random, dir);
+    final boolean allowDups = random.nextBoolean();
+    final Set<String> seen = new HashSet<String>();
+    if (VERBOSE) {
+      System.out.println("TEST: NUM_DOCS=" + NUM_DOCS + " allowDups=" + allowDups);
+    }
+    int numDocs = 0;
+    final List<BytesRef> docValues = new ArrayList<BytesRef>();
+
+    // TODO: deletions
+    while (numDocs < NUM_DOCS) {
+      final String s;
+      if (random.nextBoolean()) {
+        s = _TestUtil.randomSimpleString(random);
+      } else {
+        s = _TestUtil.randomUnicodeString(random);
+      }
+      final BytesRef br = new BytesRef(s);
+
+      if (!allowDups) {
+        if (seen.contains(s)) {
+          continue;
+        }
+        seen.add(s);
+      }
+
+      if (VERBOSE) {
+        System.out.println("  " + numDocs + ": s=" + s);
+      }
+      
+      final Document doc = new Document();
+      doc.add(new SortedDocValuesField("stringdv", br));
+      doc.add(new NumericDocValuesField("id", numDocs));
+      docValues.add(br);
+      writer.addDocument(doc);
+      numDocs++;
+
+      if (random.nextInt(40) == 17) {
+        // force flush
+        writer.getReader().close();
+      }
+    }
+
+    writer.forceMerge(1);
+    final DirectoryReader r = writer.getReader();
+    writer.close();
+    
+    final AtomicReader sr = getOnlySegmentReader(r);
+
+    final long END_TIME = System.currentTimeMillis() + (TEST_NIGHTLY ? 30 : 1);
+
+    final int NUM_THREADS = _TestUtil.nextInt(random(), 1, 10);
+    Thread[] threads = new Thread[NUM_THREADS];
+    for(int thread=0;thread<NUM_THREADS;thread++) {
+      threads[thread] = new Thread() {
+          @Override
+          public void run() {
+            Random random = random();            
+            final SortedDocValues stringDVDirect;
+            final NumericDocValues docIDToID;
+            try {
+              stringDVDirect = sr.getSortedDocValues("stringdv");
+              docIDToID = sr.getNumericDocValues("id");
+              assertNotNull(stringDVDirect);
+            } catch (IOException ioe) {
+              throw new RuntimeException(ioe);
+            }
+            while(System.currentTimeMillis() < END_TIME) {
+              final SortedDocValues source;
+              source = stringDVDirect;
+              final BytesRef scratch = new BytesRef();
+
+              for(int iter=0;iter<100;iter++) {
+                final int docID = random.nextInt(sr.maxDoc());
+                source.get(docID, scratch);
+                assertEquals(docValues.get((int) docIDToID.get(docID)), scratch);
+              }
+            }
+          }
+        };
+      threads[thread].start();
+    }
+
+    for(Thread thread : threads) {
+      thread.join();
+    }
+
+    r.close();
+    dir.close();
+  }
+
+}

Property changes on: lucene/core/src/test/org/apache/lucene/index/TestDocValuesWithThreads.java
___________________________________________________________________
Added: svn:eol-style
## -0,0 +1 ##
+native
Index: lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger.java	(revision 1442822)
+++ lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger.java	(working copy)
@@ -31,7 +31,6 @@
 import org.apache.lucene.util._TestUtil;
 import org.apache.lucene.util.packed.PackedInts;
 
-
 public class TestSegmentMerger extends LuceneTestCase {
   //The variables for the new merged segment
   private Directory mergedDir;
Index: lucene/core/src/test/org/apache/lucene/index/TestCodecHoldsOpenFiles.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/index/TestCodecHoldsOpenFiles.java	(revision 0)
+++ lucene/core/src/test/org/apache/lucene/index/TestCodecHoldsOpenFiles.java	(working copy)
@@ -0,0 +1,58 @@
+package org.apache.lucene.index;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.TextField;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.util.LuceneTestCase;
+import org.apache.lucene.util._TestUtil;
+
+public class TestCodecHoldsOpenFiles extends LuceneTestCase {
+  public void test() throws Exception {
+    Directory d = newDirectory();
+    RandomIndexWriter w = new RandomIndexWriter(random(), d);
+    int numDocs = atLeast(100);
+    for(int i=0;i<numDocs;i++) {
+      Document doc = new Document();
+      doc.add(newField("foo", "bar", TextField.TYPE_NOT_STORED));
+      w.addDocument(doc);
+    }
+
+    IndexReader r = w.getReader();
+    w.close();
+
+    for(String fileName : d.listAll()) {
+      try {
+        d.deleteFile(fileName);
+      } catch (IOException ioe) {
+        // ignore: this means codec (correctly) is holding
+        // the file open
+      }
+    }
+
+    for(AtomicReaderContext cxt : r.leaves()) {
+      _TestUtil.checkReader(cxt.reader());
+    }
+
+    r.close();
+    d.close();
+  }
+}

Property changes on: lucene/core/src/test/org/apache/lucene/index/TestCodecHoldsOpenFiles.java
___________________________________________________________________
Added: svn:eol-style
## -0,0 +1 ##
+native
Index: lucene/core/src/test/org/apache/lucene/index/TestMultiDocValues.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/index/TestMultiDocValues.java	(revision 0)
+++ lucene/core/src/test/org/apache/lucene/index/TestMultiDocValues.java	(working copy)
@@ -0,0 +1,192 @@
+package org.apache.lucene.index;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.document.BinaryDocValuesField;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.document.NumericDocValuesField;
+import org.apache.lucene.document.SortedDocValuesField;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.LuceneTestCase;
+import org.apache.lucene.util._TestUtil;
+
+/** Tests MultiDocValues versus ordinary segment merging */
+public class TestMultiDocValues extends LuceneTestCase {
+  
+  public void testNumerics() throws Exception {
+    Directory dir = newDirectory();
+    Document doc = new Document();
+    Field field = new NumericDocValuesField("numbers", 0);
+    doc.add(field);
+    
+    IndexWriterConfig iwc = newIndexWriterConfig(random(), TEST_VERSION_CURRENT, null);
+    iwc.setMergePolicy(newLogMergePolicy());
+    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwc);
+
+    int numDocs = atLeast(500);
+    for (int i = 0; i < numDocs; i++) {
+      field.setLongValue(random().nextLong());
+      iw.addDocument(doc);
+      if (random().nextInt(17) == 0) {
+        iw.commit();
+      }
+    }
+    DirectoryReader ir = iw.getReader();
+    iw.forceMerge(1);
+    DirectoryReader ir2 = iw.getReader();
+    AtomicReader merged = getOnlySegmentReader(ir2);
+    iw.close();
+    
+    NumericDocValues multi = MultiDocValues.getNumericValues(ir, "numbers");
+    NumericDocValues single = merged.getNumericDocValues("numbers");
+    for (int i = 0; i < numDocs; i++) {
+      assertEquals(single.get(i), multi.get(i));
+    }
+    ir.close();
+    ir2.close();
+    dir.close();
+  }
+  
+  public void testBinary() throws Exception {
+    Directory dir = newDirectory();
+    Document doc = new Document();
+    BytesRef ref = new BytesRef();
+    Field field = new BinaryDocValuesField("bytes", ref);
+    doc.add(field);
+    
+    IndexWriterConfig iwc = newIndexWriterConfig(random(), TEST_VERSION_CURRENT, null);
+    iwc.setMergePolicy(newLogMergePolicy());
+    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwc);
+
+    int numDocs = atLeast(500);
+    for (int i = 0; i < numDocs; i++) {
+      ref.copyChars(_TestUtil.randomUnicodeString(random()));
+      iw.addDocument(doc);
+      if (random().nextInt(17) == 0) {
+        iw.commit();
+      }
+    }
+    DirectoryReader ir = iw.getReader();
+    iw.forceMerge(1);
+    DirectoryReader ir2 = iw.getReader();
+    AtomicReader merged = getOnlySegmentReader(ir2);
+    iw.close();
+    
+    BinaryDocValues multi = MultiDocValues.getBinaryValues(ir, "bytes");
+    BinaryDocValues single = merged.getBinaryDocValues("bytes");
+    BytesRef actual = new BytesRef();
+    BytesRef expected = new BytesRef();
+    for (int i = 0; i < numDocs; i++) {
+      single.get(i, expected);
+      multi.get(i, actual);
+      assertEquals(expected, actual);
+    }
+    ir.close();
+    ir2.close();
+    dir.close();
+  }
+  
+  public void testSorted() throws Exception {
+    Directory dir = newDirectory();
+    Document doc = new Document();
+    BytesRef ref = new BytesRef();
+    Field field = new SortedDocValuesField("bytes", ref);
+    doc.add(field);
+    
+    IndexWriterConfig iwc = newIndexWriterConfig(random(), TEST_VERSION_CURRENT, null);
+    iwc.setMergePolicy(newLogMergePolicy());
+    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwc);
+
+    int numDocs = atLeast(500);
+    for (int i = 0; i < numDocs; i++) {
+      ref.copyChars(_TestUtil.randomUnicodeString(random()));
+      iw.addDocument(doc);
+      if (random().nextInt(17) == 0) {
+        iw.commit();
+      }
+    }
+    DirectoryReader ir = iw.getReader();
+    iw.forceMerge(1);
+    DirectoryReader ir2 = iw.getReader();
+    AtomicReader merged = getOnlySegmentReader(ir2);
+    iw.close();
+    
+    SortedDocValues multi = MultiDocValues.getSortedValues(ir, "bytes");
+    SortedDocValues single = merged.getSortedDocValues("bytes");
+    assertEquals(single.getValueCount(), multi.getValueCount());
+    BytesRef actual = new BytesRef();
+    BytesRef expected = new BytesRef();
+    for (int i = 0; i < numDocs; i++) {
+      // check ord
+      assertEquals(single.getOrd(i), multi.getOrd(i));
+      // check ord value
+      single.get(i, expected);
+      multi.get(i, actual);
+      assertEquals(expected, actual);
+    }
+    ir.close();
+    ir2.close();
+    dir.close();
+  }
+  
+  // tries to make more dups than testSorted
+  public void testSortedWithLotsOfDups() throws Exception {
+    Directory dir = newDirectory();
+    Document doc = new Document();
+    BytesRef ref = new BytesRef();
+    Field field = new SortedDocValuesField("bytes", ref);
+    doc.add(field);
+    
+    IndexWriterConfig iwc = newIndexWriterConfig(random(), TEST_VERSION_CURRENT, null);
+    iwc.setMergePolicy(newLogMergePolicy());
+    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwc);
+
+    int numDocs = atLeast(500);
+    for (int i = 0; i < numDocs; i++) {
+      ref.copyChars(_TestUtil.randomSimpleString(random(), 2));
+      iw.addDocument(doc);
+      if (random().nextInt(17) == 0) {
+        iw.commit();
+      }
+    }
+    DirectoryReader ir = iw.getReader();
+    iw.forceMerge(1);
+    DirectoryReader ir2 = iw.getReader();
+    AtomicReader merged = getOnlySegmentReader(ir2);
+    iw.close();
+    
+    SortedDocValues multi = MultiDocValues.getSortedValues(ir, "bytes");
+    SortedDocValues single = merged.getSortedDocValues("bytes");
+    assertEquals(single.getValueCount(), multi.getValueCount());
+    BytesRef actual = new BytesRef();
+    BytesRef expected = new BytesRef();
+    for (int i = 0; i < numDocs; i++) {
+      // check ord
+      assertEquals(single.getOrd(i), multi.getOrd(i));
+      // check ord value
+      single.get(i, expected);
+      multi.get(i, actual);
+      assertEquals(expected, actual);
+    }
+    ir.close();
+    ir2.close();
+    dir.close();
+  }
+}

Property changes on: lucene/core/src/test/org/apache/lucene/index/TestMultiDocValues.java
___________________________________________________________________
Added: svn:eol-style
## -0,0 +1 ##
+native
Index: lucene/core/src/test/org/apache/lucene/index/TestIndexWriterWithThreads.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/index/TestIndexWriterWithThreads.java	(revision 1442822)
+++ lucene/core/src/test/org/apache/lucene/index/TestIndexWriterWithThreads.java	(working copy)
@@ -28,6 +28,7 @@
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.FieldType;
+import org.apache.lucene.document.NumericDocValuesField;
 import org.apache.lucene.document.TextField;
 import org.apache.lucene.search.DocIdSetIterator;
 import org.apache.lucene.store.AlreadyClosedException;
@@ -74,6 +75,7 @@
       customType.setStoreTermVectorOffsets(true);
       
       doc.add(newField("field", "aaa bbb ccc ddd eee fff ggg hhh iii jjj", customType));
+      doc.add(new NumericDocValuesField("dv", 5));
 
       int idUpto = 0;
       int fullCount = 0;
Index: lucene/core/src/test/org/apache/lucene/index/TestTypePromotion.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/index/TestTypePromotion.java	(revision 1442822)
+++ lucene/core/src/test/org/apache/lucene/index/TestTypePromotion.java	(working copy)
@@ -1,385 +0,0 @@
-package org.apache.lucene.index;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to You under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-
-import java.io.IOException;
-import java.util.EnumSet;
-import java.util.List;
-import java.util.Random;
-
-import org.apache.lucene.analysis.MockAnalyzer;
-import org.apache.lucene.document.ByteDocValuesField;
-import org.apache.lucene.document.DerefBytesDocValuesField;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.document.DoubleDocValuesField;
-import org.apache.lucene.document.Field;
-import org.apache.lucene.document.FloatDocValuesField;
-import org.apache.lucene.document.IntDocValuesField;
-import org.apache.lucene.document.LongDocValuesField;
-import org.apache.lucene.document.PackedLongDocValuesField;
-import org.apache.lucene.document.ShortDocValuesField;
-import org.apache.lucene.document.SortedBytesDocValuesField;
-import org.apache.lucene.document.StraightBytesDocValuesField;
-import org.apache.lucene.document.TextField;
-import org.apache.lucene.index.DocValues.Source;
-import org.apache.lucene.index.DocValues.Type;
-import org.apache.lucene.store.ByteArrayDataOutput;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.LuceneTestCase;
-
-public class TestTypePromotion extends LuceneTestCase {
-
-  private static EnumSet<Type> INTEGERS = EnumSet.of(Type.VAR_INTS,
-      Type.FIXED_INTS_16, Type.FIXED_INTS_32,
-      Type.FIXED_INTS_64, Type.FIXED_INTS_8);
-
-  private static EnumSet<Type> FLOATS = EnumSet.of(Type.FLOAT_32,
-      Type.FLOAT_64, Type.FIXED_INTS_8);
-
-  private static EnumSet<Type> UNSORTED_BYTES = EnumSet.of(
-      Type.BYTES_FIXED_DEREF, Type.BYTES_FIXED_STRAIGHT,
-      Type.BYTES_VAR_STRAIGHT, Type.BYTES_VAR_DEREF);
-
-  private static EnumSet<Type> SORTED_BYTES = EnumSet.of(
-      Type.BYTES_FIXED_SORTED, Type.BYTES_VAR_SORTED);
-  
-  public Type randomValueType(EnumSet<Type> typeEnum, Random random) {
-    Type[] array = typeEnum.toArray(new Type[0]);
-    return array[random.nextInt(array.length)];
-  }
-  
-  private static enum TestType {
-    Int, Float, Byte
-  }
-
-  private void runTest(EnumSet<Type> types, TestType type) throws IOException {
-    Directory dir = newDirectory();
-    IndexWriter writer = new IndexWriter(dir,
-        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));
-    int num_1 = atLeast(200);
-    int num_2 = atLeast(200);
-    int num_3 = atLeast(200);
-    long[] values = new long[num_1 + num_2 + num_3];
-    Type[] sourceType = new Type[num_1 + num_2 + num_3];
-    index(writer,
-        randomValueType(types, random()), values, sourceType, 0, num_1);
-    writer.commit();
-    
-    index(writer,
-        randomValueType(types, random()), values, sourceType, num_1, num_2);
-    writer.commit();
-    
-    if (random().nextInt(4) == 0) {
-      // once in a while use addIndexes
-      writer.forceMerge(1);
-      
-      Directory dir_2 = newDirectory() ;
-      IndexWriter writer_2 = new IndexWriter(dir_2,
-          newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));
-      index(writer_2,
-          randomValueType(types, random()), values, sourceType, num_1 + num_2, num_3);
-      writer_2.commit();
-      writer_2.close();
-      if (rarely()) {
-        writer.addIndexes(dir_2);
-      } else {
-        // do a real merge here
-        IndexReader open = maybeWrapReader(DirectoryReader.open(dir_2));
-        writer.addIndexes(open);
-        open.close();
-      }
-      dir_2.close();
-    } else {
-      index(writer,
-          randomValueType(types, random()), values, sourceType, num_1 + num_2, num_3);
-    }
-
-    writer.forceMerge(1);
-    writer.close();
-    assertValues(type, dir, values, sourceType);
-    dir.close();
-  }
-
-  
-  private void assertValues(TestType type, Directory dir, long[] values, Type[] sourceType)
-      throws IOException {
-    DirectoryReader reader = DirectoryReader.open(dir);
-    assertEquals(1, reader.leaves().size());
-    IndexReaderContext topReaderContext = reader.getContext();
-    List<AtomicReaderContext> leaves = topReaderContext.leaves();
-    assertEquals(1, leaves.size());
-    DocValues docValues = leaves.get(0).reader().docValues("promote");
-    Source directSource = docValues.getDirectSource();
-    for (int i = 0; i < values.length; i++) {
-      int id = Integer.parseInt(reader.document(i).get("id"));
-      String msg = "id: " + id + " doc: " + i;
-      switch (type) {
-      case Byte:
-        BytesRef bytes = directSource.getBytes(i, new BytesRef());
-        long value = 0;
-        switch(bytes.length) {
-        case 1:
-          value = bytes.bytes[bytes.offset];
-          break;
-        case 2:
-          value = ((bytes.bytes[bytes.offset] & 0xFF) << 8) | (bytes.bytes[bytes.offset+1] & 0xFF);
-          break;
-        case 4:
-          value = ((bytes.bytes[bytes.offset] & 0xFF) << 24)  | ((bytes.bytes[bytes.offset+1] & 0xFF) << 16)
-                | ((bytes.bytes[bytes.offset+2] & 0xFF) << 8) | (bytes.bytes[bytes.offset+3] & 0xFF);
-          break;
-        case 8:
-          value =  (((long)(bytes.bytes[bytes.offset] & 0xff) << 56) | ((long)(bytes.bytes[bytes.offset+1] & 0xff) << 48) |
-                  ((long)(bytes.bytes[bytes.offset+2] & 0xff) << 40) | ((long)(bytes.bytes[bytes.offset+3] & 0xff) << 32) |
-                  ((long)(bytes.bytes[bytes.offset+4] & 0xff) << 24) | ((long)(bytes.bytes[bytes.offset+5] & 0xff) << 16) |
-                  ((long)(bytes.bytes[bytes.offset+6] & 0xff) <<  8) | ((long)(bytes.bytes[bytes.offset+7] & 0xff)));
-          break;
-          
-        default:
-          fail(msg + " bytessize: " + bytes.length);
-        }
-        
-        assertEquals(msg  + " byteSize: " + bytes.length, values[id], value);
-        break;
-      case Float:
-          if (sourceType[id] == Type.FLOAT_32
-              || sourceType[id] == Type.FLOAT_64) {
-            assertEquals(msg, values[id],
-                Double.doubleToRawLongBits(directSource.getFloat(i)));
-          } else {
-            assertEquals(msg, values[id], directSource.getFloat(i), 0.0d);
-          }
-        break;
-      case Int:
-        assertEquals(msg, values[id], directSource.getInt(i));
-        break;
-      default:
-        break;
-      }
-
-    }
-    docValues.close();
-    reader.close();
-  }
-
-  public void index(IndexWriter writer,
-      Type valueType, long[] values, Type[] sourceTypes, int offset, int num)
-      throws IOException {
-    final Field valField;
-
-    if (VERBOSE) {
-      System.out.println("TEST: add docs " + offset + "-" + (offset+num) + " valType=" + valueType);
-    }
-
-    switch(valueType) {
-    case VAR_INTS:
-      valField = new PackedLongDocValuesField("promote", (long) 0);
-      break;
-    case FIXED_INTS_8:
-      valField = new ByteDocValuesField("promote", (byte) 0);
-      break;
-    case FIXED_INTS_16:
-      valField = new ShortDocValuesField("promote", (short) 0);
-      break;
-    case FIXED_INTS_32:
-      valField = new IntDocValuesField("promote", 0);
-      break;
-    case FIXED_INTS_64:
-      valField = new LongDocValuesField("promote", (byte) 0);
-      break;
-    case FLOAT_32:
-      valField = new FloatDocValuesField("promote", 0f);
-      break;
-    case FLOAT_64:
-      valField = new DoubleDocValuesField("promote", 0d);
-      break;
-    case BYTES_FIXED_STRAIGHT:
-      valField = new StraightBytesDocValuesField("promote", new BytesRef(), true);
-      break;
-    case BYTES_VAR_STRAIGHT:
-      valField = new StraightBytesDocValuesField("promote", new BytesRef(), false);
-      break;
-    case BYTES_FIXED_DEREF:
-      valField = new DerefBytesDocValuesField("promote", new BytesRef(), true);
-      break;
-    case BYTES_VAR_DEREF:
-      valField = new DerefBytesDocValuesField("promote", new BytesRef(), false);
-      break;
-    case BYTES_FIXED_SORTED:
-      valField = new SortedBytesDocValuesField("promote", new BytesRef(), true);
-      break;
-    case BYTES_VAR_SORTED:
-      valField = new SortedBytesDocValuesField("promote", new BytesRef(), false);
-      break;
-    default:
-      throw new IllegalStateException("unknown Type: " + valueType);
-    }
-
-    for (int i = offset; i < offset + num; i++) {
-      Document doc = new Document();
-      doc.add(new TextField("id", i + "", Field.Store.YES));
-      sourceTypes[i] = valueType;
-      switch (valueType) {
-      case VAR_INTS:
-        // TODO: can we do nextLong()?
-        values[i] = random().nextInt();
-        valField.setLongValue(values[i]);
-        break;
-      case FIXED_INTS_16:
-        // TODO: negatives too?
-        values[i] = random().nextInt(Short.MAX_VALUE);
-        valField.setShortValue((short) values[i]);
-        break;
-      case FIXED_INTS_32:
-        values[i] = random().nextInt();
-        valField.setIntValue((int) values[i]);
-        break;
-      case FIXED_INTS_64:
-        values[i] = random().nextLong();
-        valField.setLongValue(values[i]);
-        break;
-      case FLOAT_64:
-        final double nextDouble = random().nextDouble();
-        values[i] = Double.doubleToRawLongBits(nextDouble);
-        valField.setDoubleValue(nextDouble);
-        break;
-      case FLOAT_32:
-        final float nextFloat = random().nextFloat();
-        values[i] = Double.doubleToRawLongBits(nextFloat);
-        valField.setFloatValue(nextFloat);
-        break;
-      case FIXED_INTS_8:
-        values[i] = (byte) i;
-        valField.setByteValue((byte)values[i]);
-        break;
-      case BYTES_FIXED_DEREF:
-      case BYTES_FIXED_SORTED:
-      case BYTES_FIXED_STRAIGHT:
-        values[i] = random().nextLong();
-        byte bytes[] = new byte[8];
-        ByteArrayDataOutput out = new ByteArrayDataOutput(bytes, 0, 8);
-        out.writeLong(values[i]);
-        valField.setBytesValue(new BytesRef(bytes));
-        break;
-      case BYTES_VAR_DEREF:
-      case BYTES_VAR_SORTED:
-      case BYTES_VAR_STRAIGHT:
-        byte lbytes[] = new byte[8];
-        ByteArrayDataOutput lout = new ByteArrayDataOutput(lbytes, 0, 8);
-        final int len;
-        if (random().nextBoolean()) {
-          values[i] = random().nextInt();
-          lout.writeInt((int)values[i]);
-          len = 4;
-        } else {
-          values[i] = random().nextLong();
-          lout.writeLong(values[i]);
-          len = 8;
-        }
-        valField.setBytesValue(new BytesRef(lbytes, 0, len));
-        break;
-
-      default:
-        fail("unexpected value " + valueType);
-      }
-      if (VERBOSE) {
-        System.out.println("  doc " + i + " has val=" + valField);
-      }
-      doc.add(valField);
-      writer.addDocument(doc);
-      if (random().nextInt(10) == 0) {
-        writer.commit();
-      }
-    }
-  }
-
-  public void testPromoteBytes() throws IOException {
-    runTest(UNSORTED_BYTES, TestType.Byte);
-  }
-  
-  public void testSortedPromoteBytes() throws IOException {
-    runTest(SORTED_BYTES, TestType.Byte);
-  }
-
-  public void testPromoteInteger() throws IOException {
-    runTest(INTEGERS, TestType.Int);
-  }
-
-  public void testPromotFloatingPoint() throws IOException {
-    runTest(FLOATS, TestType.Float);
-  }
-  
-  public void testMergeIncompatibleTypes() throws IOException {
-    Directory dir = newDirectory();
-    IndexWriterConfig writerConfig = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
-    writerConfig.setMergePolicy(NoMergePolicy.NO_COMPOUND_FILES); // no merges until we are done with adding values
-    IndexWriter writer = new IndexWriter(dir, writerConfig);
-    int num_1 = atLeast(200);
-    int num_2 = atLeast(200);
-    long[] values = new long[num_1 + num_2];
-    Type[] sourceType = new Type[num_1 + num_2];
-    index(writer,
-        randomValueType(INTEGERS, random()), values, sourceType, 0, num_1);
-    writer.commit();
-    
-    if (random().nextInt(4) == 0) {
-      // once in a while use addIndexes
-      Directory dir_2 = newDirectory() ;
-      IndexWriter writer_2 = new IndexWriter(dir_2,
-                       newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));
-      index(writer_2,
-          randomValueType(random().nextBoolean() ? UNSORTED_BYTES : SORTED_BYTES, random()), values, sourceType, num_1, num_2);
-      writer_2.commit();
-      writer_2.close();
-      if (random().nextBoolean()) {
-        writer.addIndexes(dir_2);
-      } else {
-        // do a real merge here
-        IndexReader open = DirectoryReader.open(dir_2);
-        writer.addIndexes(open);
-        open.close();
-      }
-      dir_2.close();
-    } else {
-      index(writer,
-          randomValueType(random().nextBoolean() ? UNSORTED_BYTES : SORTED_BYTES, random()), values, sourceType, num_1, num_2);
-      writer.commit();
-    }
-    writer.close();
-    writerConfig = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
-    if (writerConfig.getMergePolicy() instanceof NoMergePolicy) {
-      writerConfig.setMergePolicy(newLogMergePolicy()); // make sure we merge to one segment (merge everything together)
-    }
-    writer = new IndexWriter(dir, writerConfig);
-    // now merge
-    writer.forceMerge(1);
-    writer.close();
-    DirectoryReader reader = DirectoryReader.open(dir);
-    assertEquals(1, reader.leaves().size());
-    IndexReaderContext topReaderContext = reader.getContext();
-    List<AtomicReaderContext> leaves = topReaderContext.leaves();
-    DocValues docValues = leaves.get(0).reader().docValues("promote");
-    assertNotNull(docValues);
-    assertValues(TestType.Byte, dir, values, sourceType);
-    assertEquals(Type.BYTES_VAR_STRAIGHT, docValues.getType());
-    reader.close();
-    dir.close();
-  }
-
-}
\ No newline at end of file
Index: lucene/core/src/test/org/apache/lucene/index/TestCustomNorms.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/index/TestCustomNorms.java	(revision 1442822)
+++ lucene/core/src/test/org/apache/lucene/index/TestCustomNorms.java	(working copy)
@@ -17,25 +17,17 @@
  * limitations under the License.
  */
 import java.io.IOException;
-import java.util.Random;
 
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.TextField;
-import org.apache.lucene.index.DocValues.Source;
-import org.apache.lucene.index.DocValues.Type;
 import org.apache.lucene.search.CollectionStatistics;
 import org.apache.lucene.search.TermStatistics;
 import org.apache.lucene.search.similarities.DefaultSimilarity;
 import org.apache.lucene.search.similarities.PerFieldSimilarityWrapper;
 import org.apache.lucene.search.similarities.Similarity;
-import org.apache.lucene.search.similarities.Similarity.ExactSimScorer;
-import org.apache.lucene.search.similarities.Similarity.SimWeight;
-import org.apache.lucene.search.similarities.Similarity.SloppySimScorer;
 import org.apache.lucene.store.Directory;
-import org.apache.lucene.util.Bits;
-import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.LineFileDocs;
 import org.apache.lucene.util.LuceneTestCase;
 
@@ -72,106 +64,18 @@
     writer.commit();
     writer.close();
     AtomicReader open = SlowCompositeReaderWrapper.wrap(DirectoryReader.open(dir));
-    DocValues normValues = open.normValues(floatTestField);
-    assertNotNull(normValues);
-    Source source = normValues.getSource();
-    assertTrue(source.hasArray());
-    assertEquals(Type.FLOAT_32, normValues.getType());
-    float[] norms = (float[]) source.getArray();
+    NumericDocValues norms = open.getNormValues(floatTestField);
+    assertNotNull(norms);
     for (int i = 0; i < open.maxDoc(); i++) {
       StoredDocument document = open.document(i);
       float expected = Float.parseFloat(document.get(floatTestField));
-      assertEquals(expected, norms[i], 0.0f);
+      assertEquals(expected, Float.intBitsToFloat((int)norms.get(i)), 0.0f);
     }
     open.close();
     dir.close();
     docs.close();
   }
 
-  public void testExceptionOnRandomType() throws IOException {
-    Directory dir = newDirectory();
-    IndexWriterConfig config = newIndexWriterConfig(TEST_VERSION_CURRENT,
-        new MockAnalyzer(random()));
-    Similarity provider = new MySimProvider();
-    config.setSimilarity(provider);
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, config);
-    final LineFileDocs docs = new LineFileDocs(random());
-    int num = atLeast(100);
-    try {
-      for (int i = 0; i < num; i++) {
-        Document doc = docs.nextDoc();
-        float nextFloat = random().nextFloat();
-        Field f = new TextField(exceptionTestField, "" + nextFloat, Field.Store.YES);
-        f.setBoost(nextFloat);
-
-        doc.add(f);
-        writer.addDocument(doc);
-        doc.removeField(exceptionTestField);
-        if (rarely()) {
-          writer.commit();
-        }
-      }
-      fail("expected exception - incompatible types");
-    } catch (IllegalArgumentException e) {
-      // expected
-    }
-    writer.commit();
-    writer.close();
-    dir.close();
-    docs.close();
-
-  }
-  
-  public void testIllegalCustomEncoder() throws Exception {
-    Directory dir = newDirectory();
-    IllegalCustomEncodingSimilarity similarity = new IllegalCustomEncodingSimilarity();
-    IndexWriterConfig config = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
-    config.setSimilarity(similarity);
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, config);
-    Document doc = new Document();
-    Field foo = newTextField("foo", "", Field.Store.NO);
-    Field bar = newTextField("bar", "", Field.Store.NO);
-    doc.add(foo);
-    doc.add(bar);
-    
-    int numAdded = 0;
-    for (int i = 0; i < 100; i++) {
-      try {
-        bar.setStringValue("singleton");
-        similarity.useByte = random().nextBoolean();
-        writer.addDocument(doc);
-        numAdded++;
-      } catch (IllegalArgumentException e) {}
-    }
-    
-    
-    IndexReader reader = writer.getReader();
-    writer.close();
-    assertEquals(numAdded, reader.numDocs());
-    IndexReaderContext topReaderContext = reader.getContext();
-    for (final AtomicReaderContext ctx : topReaderContext.leaves()) {
-      AtomicReader atomicReader = ctx.reader();
-      Source source = random().nextBoolean() ? atomicReader.normValues("foo").getSource() : atomicReader.normValues("foo").getDirectSource();
-      Bits liveDocs = atomicReader.getLiveDocs();
-      Type t = source.getType();
-      for (int i = 0; i < atomicReader.maxDoc(); i++) {
-          assertEquals(0, source.getFloat(i), 0.000f);
-      }
-      
-  
-      source = random().nextBoolean() ? atomicReader.normValues("bar").getSource() : atomicReader.normValues("bar").getDirectSource();
-      for (int i = 0; i < atomicReader.maxDoc(); i++) {
-        if (liveDocs == null || liveDocs.get(i)) {
-          assertEquals("type: " + t, 1, source.getFloat(i), 0.000f);
-        } else {
-          assertEquals("type: " + t, 0, source.getFloat(i), 0.000f);
-        }
-      }
-    }
-    reader.close();
-    dir.close();
-  }
-
   public class MySimProvider extends PerFieldSimilarityWrapper {
     Similarity delegate = new DefaultSimilarity();
 
@@ -184,8 +88,6 @@
     public Similarity get(String field) {
       if (floatTestField.equals(field)) {
         return new FloatEncodingBoostSimilarity();
-      } else if (exceptionTestField.equals(field)) {
-        return new RandomTypeSimilarity(random());
       } else {
         return delegate;
       }
@@ -200,9 +102,8 @@
   public static class FloatEncodingBoostSimilarity extends Similarity {
 
     @Override
-    public void computeNorm(FieldInvertState state, Norm norm) {
-      float boost = state.getBoost();
-      norm.setFloat(boost);
+    public long computeNorm(FieldInvertState state) {
+      return Float.floatToIntBits(state.getBoost());
     }
     
     @Override
@@ -220,87 +121,4 @@
       throw new UnsupportedOperationException();
     }
   }
-
-  public static class RandomTypeSimilarity extends Similarity {
-
-    private final Random random;
-    
-    public RandomTypeSimilarity(Random random) {
-      this.random = random;
-    }
-
-    @Override
-    public void computeNorm(FieldInvertState state, Norm norm) {
-      float boost = state.getBoost();
-      int nextInt = random.nextInt(10);
-      switch (nextInt) {
-      case 0:
-        norm.setDouble((double) boost);
-        break;
-      case 1:
-        norm.setFloat(boost);
-        break;
-      case 2:
-        norm.setLong((long) boost);
-        break;
-      case 3:
-        norm.setBytes(new BytesRef(new byte[6]));
-        break;
-      case 4:
-        norm.setInt((int) boost);
-        break;
-      case 5:
-        norm.setShort((short) boost);
-        break;
-      default:
-        norm.setByte((byte) boost);
-      }
-
-    }
-
-    @Override
-    public SimWeight computeWeight(float queryBoost, CollectionStatistics collectionStats, TermStatistics... termStats) {
-      throw new UnsupportedOperationException();
-    }
-
-    @Override
-    public ExactSimScorer exactSimScorer(SimWeight weight, AtomicReaderContext context) throws IOException {
-      throw new UnsupportedOperationException();
-    }
-
-    @Override
-    public SloppySimScorer sloppySimScorer(SimWeight weight, AtomicReaderContext context) throws IOException {
-      throw new UnsupportedOperationException();
-    }
-  }
-  
-  class IllegalCustomEncodingSimilarity extends Similarity {
-    
-    public boolean useByte = false;
-
-    @Override
-    public void computeNorm(FieldInvertState state, Norm norm) {
-      if (useByte) {
-        norm.setByte((byte)state.getLength());
-      } else {
-        norm.setFloat((float)state.getLength());
-      }
-    }
-
-    @Override
-    public SimWeight computeWeight(float queryBoost, CollectionStatistics collectionStats, TermStatistics... termStats) {
-      throw new UnsupportedOperationException();
-    }
-
-    @Override
-    public ExactSimScorer exactSimScorer(SimWeight weight, AtomicReaderContext context) throws IOException {
-      throw new UnsupportedOperationException();
-    }
-
-    @Override
-    public SloppySimScorer sloppySimScorer(SimWeight weight, AtomicReaderContext context) throws IOException {
-      throw new UnsupportedOperationException();
-    }
-  }
-
 }
Index: lucene/core/src/test/org/apache/lucene/index/TestParallelAtomicReader.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/index/TestParallelAtomicReader.java	(revision 1442822)
+++ lucene/core/src/test/org/apache/lucene/index/TestParallelAtomicReader.java	(working copy)
@@ -27,6 +27,7 @@
 import org.apache.lucene.search.*;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.util.LuceneTestCase;
+import org.apache.lucene.util._TestUtil;
 
 public class TestParallelAtomicReader extends LuceneTestCase {
 
@@ -262,6 +263,7 @@
     ParallelAtomicReader pr = new ParallelAtomicReader(
         SlowCompositeReaderWrapper.wrap(DirectoryReader.open(dir1)),
         SlowCompositeReaderWrapper.wrap(DirectoryReader.open(dir2)));
+    _TestUtil.checkReader(pr);
     return newSearcher(pr);
   }
 
Index: lucene/core/src/test/org/apache/lucene/index/TestIndexWriterDelete.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/index/TestIndexWriterDelete.java	(revision 1442822)
+++ lucene/core/src/test/org/apache/lucene/index/TestIndexWriterDelete.java	(working copy)
@@ -32,6 +32,7 @@
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.FieldType;
+import org.apache.lucene.document.NumericDocValuesField;
 import org.apache.lucene.document.StringField;
 import org.apache.lucene.search.IndexSearcher;
 import org.apache.lucene.search.ScoreDoc;
@@ -389,6 +390,7 @@
     doc.add(newTextField("content", "aaa", Field.Store.NO));
     doc.add(newStringField("id", String.valueOf(id), Field.Store.YES));
     doc.add(newStringField("value", String.valueOf(value), Field.Store.NO));
+    doc.add(new NumericDocValuesField("dv", value));
     modifier.updateDocument(new Term("id", String.valueOf(id)), doc);
   }
 
@@ -399,6 +401,7 @@
     doc.add(newTextField("content", "aaa", Field.Store.NO));
     doc.add(newStringField("id", String.valueOf(id), Field.Store.YES));
     doc.add(newStringField("value", String.valueOf(value), Field.Store.NO));
+    doc.add(new NumericDocValuesField("dv", value));
     modifier.addDocument(doc);
   }
 
@@ -437,6 +440,7 @@
       Document d = new Document();
       d.add(newStringField("id", Integer.toString(i), Field.Store.YES));
       d.add(newTextField("content", "aaa " + i, Field.Store.NO));
+      d.add(new NumericDocValuesField("dv", i));
       writer.addDocument(d);
     }
     writer.close();
@@ -515,6 +519,7 @@
                 Document d = new Document();
                 d.add(newStringField("id", Integer.toString(i), Field.Store.YES));
                 d.add(newTextField("content", "bbb " + i, Field.Store.NO));
+                d.add(new NumericDocValuesField("dv", i));
                 modifier.updateDocument(new Term("id", Integer.toString(docId)), d);
               } else { // deletes
                 modifier.deleteDocuments(new Term("id", Integer.toString(docId)));
Index: lucene/core/src/test/org/apache/lucene/index/TestDirectoryReader.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/index/TestDirectoryReader.java	(revision 1442822)
+++ lucene/core/src/test/org/apache/lucene/index/TestDirectoryReader.java	(working copy)
@@ -39,7 +39,6 @@
 import org.apache.lucene.search.DocIdSetIterator;
 import org.apache.lucene.search.FieldCache;
 import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.MockDirectoryWrapper;
 import org.apache.lucene.store.NoSuchDirectoryException;
 import org.apache.lucene.util.Bits;
 import org.apache.lucene.util.BytesRef;
@@ -368,57 +367,57 @@
 
   
   public void testBinaryFields() throws IOException {
-      Directory dir = newDirectory();
-      byte[] bin = new byte[]{0, 1, 2, 3, 4, 5, 6, 7, 8, 9};
+    Directory dir = newDirectory();
+    byte[] bin = new byte[]{0, 1, 2, 3, 4, 5, 6, 7, 8, 9};
       
-      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(newLogMergePolicy()));
+    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(newLogMergePolicy()));
       
-      for (int i = 0; i < 10; i++) {
-        addDoc(writer, "document number " + (i + 1));
-        addDocumentWithFields(writer);
-        addDocumentWithDifferentFields(writer);
-        addDocumentWithTermVectorFields(writer);
-      }
-      writer.close();
-      writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setOpenMode(OpenMode.APPEND).setMergePolicy(newLogMergePolicy()));
-      Document doc = new Document();
-      doc.add(new StoredField("bin1", bin));
-      doc.add(new TextField("junk", "junk text", Field.Store.NO));
-      writer.addDocument(doc);
-      writer.close();
-      DirectoryReader reader = DirectoryReader.open(dir);
-      StoredDocument doc2 = reader.document(reader.maxDoc() - 1);
-      StorableField[] fields = doc2.getFields("bin1");
-      assertNotNull(fields);
-      assertEquals(1, fields.length);
-      StorableField b1 = fields[0];
-      assertTrue(b1.binaryValue() != null);
-      BytesRef bytesRef = b1.binaryValue();
-      assertEquals(bin.length, bytesRef.length);
-      for (int i = 0; i < bin.length; i++) {
-        assertEquals(bin[i], bytesRef.bytes[i + bytesRef.offset]);
-      }
-      reader.close();
-      // force merge
+    for (int i = 0; i < 10; i++) {
+      addDoc(writer, "document number " + (i + 1));
+      addDocumentWithFields(writer);
+      addDocumentWithDifferentFields(writer);
+      addDocumentWithTermVectorFields(writer);
+    }
+    writer.close();
+    writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setOpenMode(OpenMode.APPEND).setMergePolicy(newLogMergePolicy()));
+    Document doc = new Document();
+    doc.add(new StoredField("bin1", bin));
+    doc.add(new TextField("junk", "junk text", Field.Store.NO));
+    writer.addDocument(doc);
+    writer.close();
+    DirectoryReader reader = DirectoryReader.open(dir);
+    StoredDocument doc2 = reader.document(reader.maxDoc() - 1);
+    StorableField[] fields = doc2.getFields("bin1");
+    assertNotNull(fields);
+    assertEquals(1, fields.length);
+    StorableField b1 = fields[0];
+    assertTrue(b1.binaryValue() != null);
+    BytesRef bytesRef = b1.binaryValue();
+    assertEquals(bin.length, bytesRef.length);
+    for (int i = 0; i < bin.length; i++) {
+      assertEquals(bin[i], bytesRef.bytes[i + bytesRef.offset]);
+    }
+    reader.close();
+    // force merge
 
 
-      writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setOpenMode(OpenMode.APPEND).setMergePolicy(newLogMergePolicy()));
-      writer.forceMerge(1);
-      writer.close();
-      reader = DirectoryReader.open(dir);
-      doc2 = reader.document(reader.maxDoc() - 1);
-      fields = doc2.getFields("bin1");
-      assertNotNull(fields);
-      assertEquals(1, fields.length);
-      b1 = fields[0];
-      assertTrue(b1.binaryValue() != null);
-      bytesRef = b1.binaryValue();
-      assertEquals(bin.length, bytesRef.length);
-      for (int i = 0; i < bin.length; i++) {
-        assertEquals(bin[i], bytesRef.bytes[i + bytesRef.offset]);
-      }
-      reader.close();
-      dir.close();
+    writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setOpenMode(OpenMode.APPEND).setMergePolicy(newLogMergePolicy()));
+    writer.forceMerge(1);
+    writer.close();
+    reader = DirectoryReader.open(dir);
+    doc2 = reader.document(reader.maxDoc() - 1);
+    fields = doc2.getFields("bin1");
+    assertNotNull(fields);
+    assertEquals(1, fields.length);
+    b1 = fields[0];
+    assertTrue(b1.binaryValue() != null);
+    bytesRef = b1.binaryValue();
+    assertEquals(bin.length, bytesRef.length);
+    for (int i = 0; i < bin.length; i++) {
+      assertEquals(bin[i], bytesRef.bytes[i + bytesRef.offset]);
+    }
+    reader.close();
+    dir.close();
   }
 
   /* ??? public void testOpenEmptyDirectory() throws IOException{
@@ -551,7 +550,7 @@
     assertEquals("IndexReaders have different values for maxDoc.", index1.maxDoc(), index2.maxDoc());
     assertEquals("Only one IndexReader has deletions.", index1.hasDeletions(), index2.hasDeletions());
     assertEquals("Single segment test differs.", index1.leaves().size() == 1, index2.leaves().size() == 1);
-    
+
     // check field names
     FieldInfos fieldInfos1 = MultiFields.getMergedFieldInfos(index1);
     FieldInfos fieldInfos2 = MultiFields.getMergedFieldInfos(index2);
@@ -566,22 +565,17 @@
     // check norms
     for(FieldInfo fieldInfo : fieldInfos1) {
       String curField = fieldInfo.name;
-      DocValues norms1 = MultiDocValues.getNormDocValues(index1, curField);
-      DocValues norms2 = MultiDocValues.getNormDocValues(index2, curField);
-      if (norms1 != null && norms2 != null)
-      {
+      NumericDocValues norms1 = MultiDocValues.getNormValues(index1, curField);
+      NumericDocValues norms2 = MultiDocValues.getNormValues(index2, curField);
+      if (norms1 != null && norms2 != null) {
         // todo: generalize this (like TestDuelingCodecs assert)
-        byte[] b1 = (byte[]) norms1.getSource().getArray();
-        byte[] b2 = (byte[]) norms2.getSource().getArray();
-        assertEquals(b1.length, b2.length);
-        for (int i = 0; i < b1.length; i++) {
-          assertEquals("Norm different for doc " + i + " and field '" + curField + "'.", b1[i], b2[i]);
+        for (int i = 0; i < index1.maxDoc(); i++) {
+          assertEquals("Norm different for doc " + i + " and field '" + curField + "'.", norms1.get(i), norms2.get(i));
         }
+      } else {
+        assertNull(norms1);
+        assertNull(norms2);
       }
-      else
-      {
-        assertSame(norms1, norms2);
-      }
     }
     
     // check deletions
@@ -776,9 +770,8 @@
     // Open reader1
     DirectoryReader r = DirectoryReader.open(dir);
     AtomicReader r1 = getOnlySegmentReader(r);
-    final int[] ints = FieldCache.DEFAULT.getInts(r1, "number", false);
-    assertEquals(1, ints.length);
-    assertEquals(17, ints[0]);
+    final FieldCache.Ints ints = FieldCache.DEFAULT.getInts(r1, "number", false);
+    assertEquals(17, ints.get(0));
   
     // Add new segment
     writer.addDocument(doc);
@@ -789,7 +782,7 @@
     assertNotNull(r2);
     r.close();
     AtomicReader sub0 = r2.leaves().get(0).reader();
-    final int[] ints2 = FieldCache.DEFAULT.getInts(sub0, "number", false);
+    final FieldCache.Ints ints2 = FieldCache.DEFAULT.getInts(sub0, "number", false);
     r2.close();
     assertTrue(ints == ints2);
   
Index: lucene/core/src/test/org/apache/lucene/index/TestDocValuesIndexing.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/index/TestDocValuesIndexing.java	(revision 1442822)
+++ lucene/core/src/test/org/apache/lucene/index/TestDocValuesIndexing.java	(working copy)
@@ -16,57 +16,28 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
-import java.io.Closeable;
+
 import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.Collections;
-import java.util.EnumSet;
-import java.util.HashMap;
-import java.util.HashSet;
-import java.util.List;
-import java.util.Map;
-import java.util.Map.Entry;
-import java.util.Random;
-import java.util.Set;
+import java.util.concurrent.CountDownLatch;
+import java.util.concurrent.atomic.AtomicBoolean;
 
+import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.MockAnalyzer;
-import org.apache.lucene.document.ByteDocValuesField;
-import org.apache.lucene.document.DerefBytesDocValuesField;
+import org.apache.lucene.document.BinaryDocValuesField;
 import org.apache.lucene.document.Document;
-import org.apache.lucene.document.DoubleDocValuesField;
 import org.apache.lucene.document.Field;
-import org.apache.lucene.document.FloatDocValuesField;
-import org.apache.lucene.document.IntDocValuesField;
-import org.apache.lucene.document.LongDocValuesField;
-import org.apache.lucene.document.PackedLongDocValuesField;
-import org.apache.lucene.document.ShortDocValuesField;
-import org.apache.lucene.document.SortedBytesDocValuesField;
-import org.apache.lucene.document.StraightBytesDocValuesField;
-import org.apache.lucene.document.StringField;
+import org.apache.lucene.document.NumericDocValuesField;
+import org.apache.lucene.document.SortedDocValuesField;
 import org.apache.lucene.document.TextField;
-import org.apache.lucene.index.DocValues.SortedSource;
-import org.apache.lucene.index.DocValues.Source;
-import org.apache.lucene.index.DocValues.SourceCache;
-import org.apache.lucene.index.DocValues.Type;
-import org.apache.lucene.index.DocValues.SourceCache.DirectSourceCache;
-import org.apache.lucene.search.BooleanClause;
-import org.apache.lucene.search.BooleanQuery;
-import org.apache.lucene.search.DocIdSetIterator;
-import org.apache.lucene.search.IndexSearcher;
-import org.apache.lucene.search.ScoreDoc;
-import org.apache.lucene.search.TermQuery;
-import org.apache.lucene.search.TopDocs;
+import org.apache.lucene.search.FieldCache;
 import org.apache.lucene.store.Directory;
+import org.apache.lucene.util.Bits;
 import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.BytesRefHash;
-import org.apache.lucene.util.FixedBitSet;
 import org.apache.lucene.util.LuceneTestCase;
-import org.apache.lucene.util._TestUtil;
 
 /**
  * 
- * Tests DocValues integration into IndexWriter & Codecs
+ * Tests DocValues integration into IndexWriter
  * 
  */
 public class TestDocValuesIndexing extends LuceneTestCase {
@@ -75,70 +46,12 @@
    * - add multithreaded tests / integrate into stress indexing?
    */
   
-  /*
-   * Simple test case to show how to use the API
-   */
-  public void testDocValuesSimple() throws IOException {
-    Directory dir = newDirectory();
-    IndexWriter writer = new IndexWriter(dir, writerConfig(false));
-    for (int i = 0; i < 5; i++) {
-      Document doc = new Document();
-      doc.add(new PackedLongDocValuesField("docId", i));
-      doc.add(new TextField("docId", "" + i, Field.Store.NO));
-      writer.addDocument(doc);
-    }
-    writer.commit();
-    writer.forceMerge(1, true);
-
-    writer.close(true);
-
-    DirectoryReader reader = DirectoryReader.open(dir, 1);
-    assertEquals(1, reader.leaves().size());
-
-    IndexSearcher searcher = new IndexSearcher(reader);
-
-    BooleanQuery query = new BooleanQuery();
-    query.add(new TermQuery(new Term("docId", "0")), BooleanClause.Occur.SHOULD);
-    query.add(new TermQuery(new Term("docId", "1")), BooleanClause.Occur.SHOULD);
-    query.add(new TermQuery(new Term("docId", "2")), BooleanClause.Occur.SHOULD);
-    query.add(new TermQuery(new Term("docId", "3")), BooleanClause.Occur.SHOULD);
-    query.add(new TermQuery(new Term("docId", "4")), BooleanClause.Occur.SHOULD);
-
-    TopDocs search = searcher.search(query, 10);
-    assertEquals(5, search.totalHits);
-    ScoreDoc[] scoreDocs = search.scoreDocs;
-    DocValues docValues = MultiDocValues.getDocValues(reader, "docId");
-    Source source = docValues.getSource();
-    for (int i = 0; i < scoreDocs.length; i++) {
-      assertEquals(i, scoreDocs[i].doc);
-      assertEquals(i, source.getInt(scoreDocs[i].doc));
-    }
-    reader.close();
-    dir.close();
-  }
-
-  public void testIndexBytesNoDeletes() throws IOException {
-    runTestIndexBytes(writerConfig(random().nextBoolean()), false);
-  }
-
-  public void testIndexBytesDeletes() throws IOException {
-    runTestIndexBytes(writerConfig(random().nextBoolean()), true);
-  }
-
-  public void testIndexNumericsNoDeletes() throws IOException {
-    runTestNumerics(writerConfig(random().nextBoolean()), false);
-  }
-
-  public void testIndexNumericsDeletes() throws IOException {
-    runTestNumerics(writerConfig(random().nextBoolean()), true);
-  }
-
   public void testAddIndexes() throws IOException {
     Directory d1 = newDirectory();
     RandomIndexWriter w = new RandomIndexWriter(random(), d1);
     Document doc = new Document();
     doc.add(newStringField("id", "1", Field.Store.YES));
-    doc.add(new PackedLongDocValuesField("dv", 1));
+    doc.add(new NumericDocValuesField("dv", 1));
     w.addDocument(doc);
     IndexReader r1 = w.getReader();
     w.close();
@@ -147,7 +60,7 @@
     w = new RandomIndexWriter(random(), d2);
     doc = new Document();
     doc.add(newStringField("id", "2", Field.Store.YES));
-    doc.add(new PackedLongDocValuesField("dv", 2));
+    doc.add(new NumericDocValuesField("dv", 2));
     w.addDocument(doc);
     IndexReader r2 = w.getReader();
     w.close();
@@ -165,601 +78,17 @@
     w.close();
     AtomicReader sr = getOnlySegmentReader(r3);
     assertEquals(2, sr.numDocs());
-    DocValues docValues = sr.docValues("dv");
+    NumericDocValues docValues = sr.getNumericDocValues("dv");
     assertNotNull(docValues);
     r3.close();
     d3.close();
   }
 
-  public void testAddIndexesRandom() throws IOException {
-    int valuesPerIndex = 10;
-    List<Type> values = Arrays.asList(Type.values());
-    Collections.shuffle(values, random());
-    Type first = values.get(0);
-    Type second = values.get(1);
-    // index first index
-    Directory d_1 = newDirectory();
-    IndexWriter w_1 = new IndexWriter(d_1, writerConfig(random().nextBoolean()));
-    indexValues(w_1, valuesPerIndex, first, values, false, 7);
-    w_1.commit();
-    assertEquals(valuesPerIndex, w_1.maxDoc());
-    _TestUtil.checkIndex(d_1);
-
-    // index second index
-    Directory d_2 = newDirectory();
-    IndexWriter w_2 = new IndexWriter(d_2, writerConfig(random().nextBoolean()));
-    indexValues(w_2, valuesPerIndex, second, values, false, 7);
-    w_2.commit();
-    assertEquals(valuesPerIndex, w_2.maxDoc());
-    _TestUtil.checkIndex(d_2);
-
-    Directory target = newDirectory();
-    IndexWriter w = new IndexWriter(target, writerConfig(random().nextBoolean()));
-    DirectoryReader r_1 = DirectoryReader.open(w_1, true);
-    DirectoryReader r_2 = DirectoryReader.open(w_2, true);
-    if (random().nextBoolean()) {
-      w.addIndexes(d_1, d_2);
-    } else {
-      w.addIndexes(r_1, r_2);
-    }
-    w.forceMerge(1, true);
-    w.commit();
-    
-    _TestUtil.checkIndex(target);
-    assertEquals(valuesPerIndex * 2, w.maxDoc());
-
-    // check values
-    
-    DirectoryReader merged = DirectoryReader.open(w, true);
-    Source source_1 = getSource(getDocValues(r_1, first.name()));
-    Source source_2 = getSource(getDocValues(r_2, second.name()));
-    Source source_1_merged = getSource(getDocValues(merged, first.name()));
-    Source source_2_merged = getSource(getDocValues(merged, second
-        .name()));
-    for (int i = 0; i < r_1.maxDoc(); i++) {
-      switch (first) {
-      case BYTES_FIXED_DEREF:
-      case BYTES_FIXED_STRAIGHT:
-      case BYTES_VAR_DEREF:
-      case BYTES_VAR_STRAIGHT:
-      case BYTES_FIXED_SORTED:
-      case BYTES_VAR_SORTED:
-        assertEquals(source_1.getBytes(i, new BytesRef()),
-            source_1_merged.getBytes(i, new BytesRef()));
-        break;
-      case FIXED_INTS_16:
-      case FIXED_INTS_32:
-      case FIXED_INTS_64:
-      case FIXED_INTS_8:
-      case VAR_INTS:
-        assertEquals(source_1.getInt(i), source_1_merged.getInt(i));
-        break;
-      case FLOAT_32:
-      case FLOAT_64:
-        assertEquals(source_1.getFloat(i), source_1_merged.getFloat(i), 0.0d);
-        break;
-      default:
-        fail("unkonwn " + first);
-      }
-    }
-
-    for (int i = r_1.maxDoc(); i < merged.maxDoc(); i++) {
-      switch (second) {
-      case BYTES_FIXED_DEREF:
-      case BYTES_FIXED_STRAIGHT:
-      case BYTES_VAR_DEREF:
-      case BYTES_VAR_STRAIGHT:
-      case BYTES_FIXED_SORTED:
-      case BYTES_VAR_SORTED:
-        assertEquals(source_2.getBytes(i - r_1.maxDoc(), new BytesRef()),
-            source_2_merged.getBytes(i, new BytesRef()));
-        break;
-      case FIXED_INTS_16:
-      case FIXED_INTS_32:
-      case FIXED_INTS_64:
-      case FIXED_INTS_8:
-      case VAR_INTS:
-        assertEquals(source_2.getInt(i - r_1.maxDoc()),
-            source_2_merged.getInt(i));
-        break;
-      case FLOAT_32:
-      case FLOAT_64:
-        assertEquals(source_2.getFloat(i - r_1.maxDoc()),
-            source_2_merged.getFloat(i), 0.0d);
-        break;
-      default:
-        fail("unkonwn " + first);
-      }
-    }
-    // close resources
-    r_1.close();
-    r_2.close();
-    merged.close();
-    w_1.close(true);
-    w_2.close(true);
-    w.close(true);
-    d_1.close();
-    d_2.close();
-    target.close();
-  }
-
-  private IndexWriterConfig writerConfig(boolean useCompoundFile) {
-    final IndexWriterConfig cfg = newIndexWriterConfig(TEST_VERSION_CURRENT,
-        new MockAnalyzer(random()));
-    cfg.setMergePolicy(newLogMergePolicy(random()));
-    LogMergePolicy policy = new LogDocMergePolicy();
-    cfg.setMergePolicy(policy);
-    policy.setUseCompoundFile(useCompoundFile);
-    return cfg;
-  }
-
-  @SuppressWarnings("fallthrough")
-  public void runTestNumerics(IndexWriterConfig cfg, boolean withDeletions)
-      throws IOException {
-    Directory d = newDirectory();
-    IndexWriter w = new IndexWriter(d, cfg);
-    final int numValues = 50 + atLeast(10);
-    final List<Type> numVariantList = new ArrayList<Type>(NUMERICS);
-
-    // run in random order to test if fill works correctly during merges
-    Collections.shuffle(numVariantList, random());
-    for (Type val : numVariantList) {
-      FixedBitSet deleted = indexValues(w, numValues, val, numVariantList,
-          withDeletions, 7);
-      List<Closeable> closeables = new ArrayList<Closeable>();
-      DirectoryReader r = DirectoryReader.open(w, true);
-      final int numRemainingValues = numValues - deleted.cardinality();
-      final int base = r.numDocs() - numRemainingValues;
-      // for FIXED_INTS_8 we use value mod 128 - to enable testing in 
-      // one go we simply use numValues as the mod for all other INT types
-      int mod = numValues;
-      switch (val) {
-      case FIXED_INTS_8:
-        mod = 128;
-      case FIXED_INTS_16:
-      case FIXED_INTS_32:
-      case FIXED_INTS_64:
-      case VAR_INTS: {
-        DocValues intsReader = getDocValues(r, val.name());
-        assertNotNull(intsReader);
-
-        Source ints = getSource(intsReader);
-
-        for (int i = 0; i < base; i++) {
-          long value = ints.getInt(i);
-          assertEquals("index " + i, 0, value);
-        }
-
-        int expected = 0;
-        for (int i = base; i < r.numDocs(); i++, expected++) {
-          while (deleted.get(expected)) {
-            expected++;
-          }
-          assertEquals(val + " mod: " + mod + " index: " +  i, expected%mod, ints.getInt(i));
-        }
-      }
-        break;
-      case FLOAT_32:
-      case FLOAT_64: {
-        DocValues floatReader = getDocValues(r, val.name());
-        assertNotNull(floatReader);
-        Source floats = getSource(floatReader);
-        for (int i = 0; i < base; i++) {
-          double value = floats.getFloat(i);
-          assertEquals(val + " failed for doc: " + i + " base: " + base,
-              0.0d, value, 0.0d);
-        }
-        int expected = 0;
-        for (int i = base; i < r.numDocs(); i++, expected++) {
-          while (deleted.get(expected)) {
-            expected++;
-          }
-          assertEquals("index " + i, 2.0 * expected, floats.getFloat(i),
-              0.00001);
-        }
-      }
-        break;
-      default:
-        fail("unexpected value " + val);
-      }
-
-      closeables.add(r);
-      for (Closeable toClose : closeables) {
-        toClose.close();
-      }
-    }
-    w.close();
-    d.close();
-  }
-  
-  public void runTestIndexBytes(IndexWriterConfig cfg, boolean withDeletions)
-      throws IOException {
-    final Directory d = newDirectory();
-    IndexWriter w = new IndexWriter(d, cfg);
-    final List<Type> byteVariantList = new ArrayList<Type>(BYTES);
-    // run in random order to test if fill works correctly during merges
-    Collections.shuffle(byteVariantList, random());
-    final int numValues = 50 + atLeast(10);
-    for (Type byteIndexValue : byteVariantList) {
-      List<Closeable> closeables = new ArrayList<Closeable>();
-      final int bytesSize = 1 + atLeast(50);
-      FixedBitSet deleted = indexValues(w, numValues, byteIndexValue,
-          byteVariantList, withDeletions, bytesSize);
-      final DirectoryReader r = DirectoryReader.open(w, withDeletions);
-      assertEquals(0, r.numDeletedDocs());
-      final int numRemainingValues = numValues - deleted.cardinality();
-      final int base = r.numDocs() - numRemainingValues;
-      DocValues bytesReader = getDocValues(r, byteIndexValue.name());
-      assertNotNull("field " + byteIndexValue.name()
-          + " returned null reader - maybe merged failed", bytesReader);
-      Source bytes = getSource(bytesReader);
-      byte upto = 0;
-
-      // test the filled up slots for correctness
-      for (int i = 0; i < base; i++) {
-
-        BytesRef br = bytes.getBytes(i, new BytesRef());
-        String msg = " field: " + byteIndexValue.name() + " at index: " + i
-            + " base: " + base + " numDocs:" + r.numDocs();
-        switch (byteIndexValue) {
-        case BYTES_VAR_STRAIGHT:
-        case BYTES_FIXED_STRAIGHT:
-        case BYTES_FIXED_DEREF:
-        case BYTES_FIXED_SORTED:
-          // fixed straight returns bytesref with zero bytes all of fixed
-          // length
-          assertNotNull("expected none null - " + msg, br);
-          if (br.length != 0) {
-            assertEquals("expected zero bytes of length " + bytesSize + " - "
-                + msg + br.utf8ToString(), bytesSize, br.length);
-            for (int j = 0; j < br.length; j++) {
-              assertEquals("Byte at index " + j + " doesn't match - " + msg, 0,
-                  br.bytes[br.offset + j]);
-            }
-          }
-          break;
-        default:
-          assertNotNull("expected none null - " + msg, br);
-          assertEquals(byteIndexValue + "", 0, br.length);
-          // make sure we advance at least until base
-        }
-      }
-
-      // test the actual doc values added in this iteration
-      assertEquals(base + numRemainingValues, r.numDocs());
-      int v = 0;
-      for (int i = base; i < r.numDocs(); i++) {
-        String msg = " field: " + byteIndexValue.name() + " at index: " + i
-            + " base: " + base + " numDocs:" + r.numDocs() + " bytesSize: "
-            + bytesSize + " src: " + bytes;
-        while (withDeletions && deleted.get(v++)) {
-          upto += bytesSize;
-        }
-        BytesRef br = bytes.getBytes(i, new BytesRef());
-        assertTrue(msg, br.length > 0);
-        for (int j = 0; j < br.length; j++, upto++) {
-          if (!(br.bytes.length > br.offset + j))
-            br = bytes.getBytes(i, new BytesRef());
-          assertTrue("BytesRef index exceeded [" + msg + "] offset: "
-              + br.offset + " length: " + br.length + " index: "
-              + (br.offset + j), br.bytes.length > br.offset + j);
-          assertEquals("SourceRef Byte at index " + j + " doesn't match - "
-              + msg, upto, br.bytes[br.offset + j]);
-        }
-      }
-
-      // clean up
-      closeables.add(r);
-      for (Closeable toClose : closeables) {
-        toClose.close();
-      }
-    }
-
-    w.close();
-    d.close();
-  }
-  
-  public void testGetArrayNumerics() throws IOException {
-    Directory d = newDirectory();
-    IndexWriterConfig cfg = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
-    IndexWriter w = new IndexWriter(d, cfg);
-    final int numValues = 50 + atLeast(10);
-    final List<Type> numVariantList = new ArrayList<Type>(NUMERICS);
-    Collections.shuffle(numVariantList, random());
-    for (Type val : numVariantList) {
-      indexValues(w, numValues, val, numVariantList,
-          false, 7);
-      DirectoryReader r = DirectoryReader.open(w, true);
-      DocValues docValues = getDocValues(r, val.name());
-      assertNotNull(docValues);
-      // make sure we don't get a direct source since they don't support getArray()
-      Source source = docValues.getSource();
-      switch (source.getType()) {
-      case FIXED_INTS_8:
-      {
-        assertTrue(source.hasArray());
-        byte[] values = (byte[]) source.getArray();
-        for (int i = 0; i < numValues; i++) {
-          assertEquals((long)values[i], source.getInt(i));
-        }
-      }
-      break;
-      case FIXED_INTS_16:
-      {
-        assertTrue(source.hasArray());
-        short[] values = (short[]) source.getArray();
-        for (int i = 0; i < numValues; i++) {
-          assertEquals((long)values[i], source.getInt(i));
-        }
-      }
-      break;
-      case FIXED_INTS_32:
-      {
-        assertTrue(source.hasArray());
-        int[] values = (int[]) source.getArray();
-        for (int i = 0; i < numValues; i++) {
-          assertEquals((long)values[i], source.getInt(i));
-        }
-      }
-      break;
-      case FIXED_INTS_64:
-      {
-        assertTrue(source.hasArray());
-        long[] values = (long[]) source.getArray();
-        for (int i = 0; i < numValues; i++) {
-          assertEquals(values[i], source.getInt(i));
-        }
-      }
-      break;
-      case VAR_INTS:
-        assertFalse(source.hasArray());
-        break;
-      case FLOAT_32:
-      {
-        assertTrue(source.hasArray());
-        float[] values = (float[]) source.getArray();
-        for (int i = 0; i < numValues; i++) {
-          assertEquals((double)values[i], source.getFloat(i), 0.0d);
-        }
-      }
-      break;
-      case FLOAT_64:
-      {
-        assertTrue(source.hasArray());
-        double[] values = (double[]) source.getArray();
-        for (int i = 0; i < numValues; i++) {
-          assertEquals(values[i], source.getFloat(i), 0.0d);
-        }
-      }
-        break;
-      default:
-        fail("unexpected value " + source.getType());
-      }
-      r.close();
-    }
-    w.close();
-    d.close();
-  }
-  
-  public void testGetArrayBytes() throws IOException {
-    Directory d = newDirectory();
-    IndexWriterConfig cfg = newIndexWriterConfig(TEST_VERSION_CURRENT,
-        new MockAnalyzer(random()));
-    IndexWriter w = new IndexWriter(d, cfg);
-    final int numValues = 50 + atLeast(10);
-    // only single byte fixed straight supports getArray()
-    indexValues(w, numValues, Type.BYTES_FIXED_STRAIGHT, null, false, 1);
-    DirectoryReader r = DirectoryReader.open(w, true);
-    DocValues docValues = getDocValues(r, Type.BYTES_FIXED_STRAIGHT.name());
-    assertNotNull(docValues);
-    // make sure we don't get a direct source since they don't support
-    // getArray()
-    Source source = docValues.getSource();
-
-    switch (source.getType()) {
-    case BYTES_FIXED_STRAIGHT: {
-      BytesRef ref = new BytesRef();
-      if (source.hasArray()) {
-        byte[] values = (byte[]) source.getArray();
-        for (int i = 0; i < numValues; i++) {
-          source.getBytes(i, ref);
-          assertEquals(1, ref.length);
-          assertEquals(values[i], ref.bytes[ref.offset]);
-        }
-      }
-    }
-      break;
-    default:
-      fail("unexpected value " + source.getType());
-    }
-    r.close();
-    w.close();
-    d.close();
-  }
-
-  private DocValues getDocValues(IndexReader reader, String field) throws IOException {
-    final DocValues docValues = MultiDocValues.getDocValues(reader, field);
-    if (docValues == null) {
-      return docValues;
-    }
-    if (rarely()) {
-      docValues.setCache(new NotCachingSourceCache());
-    } else {
-      if (!(docValues.getCache() instanceof DirectSourceCache))  {
-        docValues.setCache(new DirectSourceCache());
-      }
-    }
-    return docValues;
-    }
-
-  @SuppressWarnings("fallthrough")
-  private Source getSource(DocValues values) throws IOException {
-    // getSource uses cache internally
-    switch(random().nextInt(5)) {
-    case 3:
-      return values.loadSource();
-    case 2:
-      return values.getDirectSource();
-    case 1:
-      if(values.getType() == Type.BYTES_VAR_SORTED || values.getType() == Type.BYTES_FIXED_SORTED) {
-        return values.getSource().asSortedSource();
-      }
-    default:
-      return values.getSource();
-    }
-  }
-
-
-  private static EnumSet<Type> BYTES = EnumSet.of(Type.BYTES_FIXED_DEREF,
-      Type.BYTES_FIXED_STRAIGHT, Type.BYTES_VAR_DEREF,
-      Type.BYTES_VAR_STRAIGHT, Type.BYTES_FIXED_SORTED, Type.BYTES_VAR_SORTED);
-
-  private static EnumSet<Type> NUMERICS = EnumSet.of(Type.VAR_INTS,
-      Type.FIXED_INTS_16, Type.FIXED_INTS_32,
-      Type.FIXED_INTS_64, 
-      Type.FIXED_INTS_8,
-      Type.FLOAT_32,
-      Type.FLOAT_64);
-
-  private FixedBitSet indexValues(IndexWriter w, int numValues, Type valueType,
-      List<Type> valueVarList, boolean withDeletions, int bytesSize)
-      throws IOException {
-    final boolean isNumeric = NUMERICS.contains(valueType);
-    FixedBitSet deleted = new FixedBitSet(numValues);
-    Document doc = new Document();
-    final Field valField;
-    if (isNumeric) {
-      switch (valueType) {
-      case VAR_INTS:
-        valField = new PackedLongDocValuesField(valueType.name(), (long) 0);
-        break;
-      case FIXED_INTS_16:
-        valField = new ShortDocValuesField(valueType.name(), (short) 0);
-        break;
-      case FIXED_INTS_32:
-        valField = new IntDocValuesField(valueType.name(), 0);
-        break;
-      case FIXED_INTS_64:
-        valField = new LongDocValuesField(valueType.name(), (long) 0);
-        break;
-      case FIXED_INTS_8:
-        valField = new ByteDocValuesField(valueType.name(), (byte) 0);
-        break;
-      case FLOAT_32:
-        valField = new FloatDocValuesField(valueType.name(), (float) 0);
-        break;
-      case FLOAT_64:
-        valField = new DoubleDocValuesField(valueType.name(), (double) 0);
-        break;
-      default:
-        valField = null;
-        fail("unhandled case");
-      }
-    } else {
-      switch (valueType) {
-      case BYTES_FIXED_STRAIGHT:
-        valField = new StraightBytesDocValuesField(valueType.name(), new BytesRef(), true);
-        break;
-      case BYTES_VAR_STRAIGHT:
-        valField = new StraightBytesDocValuesField(valueType.name(), new BytesRef(), false);
-        break;
-      case BYTES_FIXED_DEREF:
-        valField = new DerefBytesDocValuesField(valueType.name(), new BytesRef(), true);
-        break;
-      case BYTES_VAR_DEREF:
-        valField = new DerefBytesDocValuesField(valueType.name(), new BytesRef(), false);
-        break;
-      case BYTES_FIXED_SORTED:
-        valField = new SortedBytesDocValuesField(valueType.name(), new BytesRef(), true);
-        break;
-      case BYTES_VAR_SORTED:
-        valField = new SortedBytesDocValuesField(valueType.name(), new BytesRef(), false);
-        break;
-      default:
-        valField = null;
-        fail("unhandled case");
-      }
-    }
-    doc.add(valField);
-    final BytesRef bytesRef = new BytesRef();
-
-    final String idBase = valueType.name() + "_";
-    final byte[] b = new byte[bytesSize];
-    if (bytesRef != null) {
-      bytesRef.bytes = b;
-      bytesRef.length = b.length;
-      bytesRef.offset = 0;
-    }
-    byte upto = 0;
-    for (int i = 0; i < numValues; i++) {
-      if (isNumeric) {
-        switch (valueType) {
-        case VAR_INTS:
-          valField.setLongValue((long)i);
-          break;
-        case FIXED_INTS_16:
-          valField.setShortValue((short)i);
-          break;
-        case FIXED_INTS_32:
-          valField.setIntValue(i);
-          break;
-        case FIXED_INTS_64:
-          valField.setLongValue((long)i);
-          break;
-        case FIXED_INTS_8:
-          valField.setByteValue((byte)(0xFF & (i % 128)));
-          break;
-        case FLOAT_32:
-          valField.setFloatValue(2.0f * i);
-          break;
-        case FLOAT_64:
-          valField.setDoubleValue(2.0d * i);
-          break;
-        default:
-          fail("unexpected value " + valueType);
-        }
-      } else {
-        for (int j = 0; j < b.length; j++) {
-          b[j] = upto++;
-        }
-        if (bytesRef != null) {
-          valField.setBytesValue(bytesRef);
-        }
-      }
-      doc.removeFields("id");
-      doc.add(new StringField("id", idBase + i, Field.Store.YES));
-      w.addDocument(doc);
-
-      if (i % 7 == 0) {
-        if (withDeletions && random().nextBoolean()) {
-          Type val = valueVarList.get(random().nextInt(1 + valueVarList
-              .indexOf(valueType)));
-          final int randInt = val == valueType ? random().nextInt(1 + i) : random()
-              .nextInt(numValues);
-          w.deleteDocuments(new Term("id", val.name() + "_" + randInt));
-          if (val == valueType) {
-            deleted.set(randInt);
-          }
-        }
-        if (random().nextInt(10) == 0) {
-          w.commit();
-        }
-      }
-    }
-    w.commit();
-
-    // TODO test multi seg with deletions
-    if (withDeletions || random().nextBoolean()) {
-      w.forceMerge(1, true);
-    }
-    return deleted;
-  }
-
   public void testMultiValuedDocValuesField() throws Exception {
     Directory d = newDirectory();
     RandomIndexWriter w = new RandomIndexWriter(random(), d);
     Document doc = new Document();
-    Field f = new PackedLongDocValuesField("field", 17);
+    Field f = new NumericDocValuesField("field", 17);
     // Index doc values are single-valued so we should not
     // be able to add same field more than once:
     doc.add(f);
@@ -777,7 +106,7 @@
     w.forceMerge(1);
     DirectoryReader r = w.getReader();
     w.close();
-    assertEquals(17, getOnlySegmentReader(r).docValues("field").loadSource().getInt(0));
+    assertEquals(17, FieldCache.DEFAULT.getInts(getOnlySegmentReader(r), "field", false).get(0));
     r.close();
     d.close();
   }
@@ -789,8 +118,8 @@
     // Index doc values are single-valued so we should not
     // be able to add same field more than once:
     Field f;
-    doc.add(f = new PackedLongDocValuesField("field", 17));
-    doc.add(new FloatDocValuesField("field", 22.0f));
+    doc.add(f = new NumericDocValuesField("field", 17));
+    doc.add(new BinaryDocValuesField("field", new BytesRef("blah")));
     try {
       w.addDocument(doc);
       fail("didn't hit expected exception");
@@ -804,208 +133,35 @@
     w.forceMerge(1);
     DirectoryReader r = w.getReader();
     w.close();
-    assertEquals(17, getOnlySegmentReader(r).docValues("field").loadSource().getInt(0));
+    assertEquals(17, FieldCache.DEFAULT.getInts(getOnlySegmentReader(r), "field", false).get(0));
     r.close();
     d.close();
   }
-  
-  public void testSortedBytes() throws IOException {
-    Type[] types = new Type[] { Type.BYTES_FIXED_SORTED, Type.BYTES_VAR_SORTED };
-    for (Type type : types) {
-      boolean fixed = type == Type.BYTES_FIXED_SORTED;
-      final Directory d = newDirectory();
-      IndexWriterConfig cfg = newIndexWriterConfig(TEST_VERSION_CURRENT,
-          new MockAnalyzer(random()));
-      IndexWriter w = new IndexWriter(d, cfg);
-      int numDocs = atLeast(100);
-      BytesRefHash hash = new BytesRefHash();
-      Map<String, String> docToString = new HashMap<String, String>();
-      int len = 1 + random().nextInt(50);
-      for (int i = 0; i < numDocs; i++) {
-        Document doc = new Document();
-        doc.add(newTextField("id", "" + i, Field.Store.YES));
-        String string = fixed ? _TestUtil.randomFixedByteLengthUnicodeString(random(),
-            len) : _TestUtil.randomRealisticUnicodeString(random(), 1, len);
-        BytesRef br = new BytesRef(string);
-        doc.add(new SortedBytesDocValuesField("field", br, type == Type.BYTES_FIXED_SORTED));
-        hash.add(br);
-        docToString.put("" + i, string);
-        w.addDocument(doc);
-      }
-      if (rarely()) {
-        w.commit();
-      }
-      int numDocsNoValue = atLeast(10);
-      for (int i = 0; i < numDocsNoValue; i++) {
-        Document doc = new Document();
-        doc.add(newTextField("id", "noValue", Field.Store.YES));
-        w.addDocument(doc);
-      }
-      BytesRef bytesRef = new BytesRef(fixed ? len : 0);
-      bytesRef.offset = 0;
-      bytesRef.length = fixed ? len : 0;
-      hash.add(bytesRef); // add empty value for the gaps
-      if (rarely()) {
-        w.commit();
-      }
-      for (int i = 0; i < numDocs; i++) {
-        Document doc = new Document();
-        String id = "" + i + numDocs;
-        doc.add(newTextField("id", id, Field.Store.YES));
-        String string = fixed ? _TestUtil.randomFixedByteLengthUnicodeString(random(),
-            len) : _TestUtil.randomRealisticUnicodeString(random(), 1, len);
-        BytesRef br = new BytesRef(string);
-        hash.add(br);
-        docToString.put(id, string);
-        doc.add(new SortedBytesDocValuesField("field", br, type == Type.BYTES_FIXED_SORTED));
-        w.addDocument(doc);
-      }
-      w.commit();
-      IndexReader reader = w.getReader();
-      DocValues docValues = MultiDocValues.getDocValues(reader, "field");
-      Source source = getSource(docValues);
-      SortedSource asSortedSource = source.asSortedSource();
-      int[] sort = hash.sort(BytesRef.getUTF8SortedAsUnicodeComparator());
-      BytesRef expected = new BytesRef();
-      BytesRef actual = new BytesRef();
-      assertEquals(hash.size(), asSortedSource.getValueCount());
-      for (int i = 0; i < hash.size(); i++) {
-        hash.get(sort[i], expected);
-        asSortedSource.getByOrd(i, actual);
-        assertEquals(expected.utf8ToString(), actual.utf8ToString());
-        int ord = asSortedSource.getOrdByValue(expected, actual);
-        assertEquals(i, ord);
-      }
-      AtomicReader slowR = SlowCompositeReaderWrapper.wrap(reader);
-      Set<Entry<String, String>> entrySet = docToString.entrySet();
 
-      for (Entry<String, String> entry : entrySet) {
-        int docId = docId(slowR, new Term("id", entry.getKey()));
-        expected = new BytesRef(entry.getValue());
-        assertEquals(expected, asSortedSource.getBytes(docId, actual));
-      }
-
-      reader.close();
-      w.close();
-      d.close();
+  public void testDifferentTypedDocValuesField2() throws Exception {
+    Directory d = newDirectory();
+    RandomIndexWriter w = new RandomIndexWriter(random(), d);
+    Document doc = new Document();
+    // Index doc values are single-valued so we should not
+    // be able to add same field more than once:
+    Field f = new NumericDocValuesField("field", 17);
+    doc.add(f);
+    doc.add(new SortedDocValuesField("field", new BytesRef("hello")));
+    try {
+      w.addDocument(doc);
+      fail("didn't hit expected exception");
+    } catch (IllegalArgumentException iae) {
+      // expected
     }
-  }
-  
-  public int docId(AtomicReader reader, Term term) throws IOException {
-    int docFreq = reader.docFreq(term);
-    assertEquals(1, docFreq);
-    DocsEnum termDocsEnum = reader.termDocsEnum(term);
-    int nextDoc = termDocsEnum.nextDoc();
-    assertEquals(DocIdSetIterator.NO_MORE_DOCS, termDocsEnum.nextDoc());
-    return nextDoc;
-  }
-
-  public void testWithThreads() throws Exception {
-    Random random = random();
-    final int NUM_DOCS = atLeast(100);
-    final Directory dir = newDirectory();
-    final RandomIndexWriter writer = new RandomIndexWriter(random, dir);
-    final boolean allowDups = random.nextBoolean();
-    final Set<String> seen = new HashSet<String>();
-    if (VERBOSE) {
-      System.out.println("TEST: NUM_DOCS=" + NUM_DOCS + " allowDups=" + allowDups);
-    }
-    int numDocs = 0;
-    final List<BytesRef> docValues = new ArrayList<BytesRef>();
-
-    // TODO: deletions
-    while (numDocs < NUM_DOCS) {
-      final String s;
-      if (random.nextBoolean()) {
-        s = _TestUtil.randomSimpleString(random);
-      } else {
-        s = _TestUtil.randomUnicodeString(random);
-      }
-      final BytesRef br = new BytesRef(s);
-
-      if (!allowDups) {
-        if (seen.contains(s)) {
-          continue;
-        }
-        seen.add(s);
-      }
-
-      if (VERBOSE) {
-        System.out.println("  " + numDocs + ": s=" + s);
-      }
-      
-      final Document doc = new Document();
-      doc.add(new SortedBytesDocValuesField("stringdv", br));
-      doc.add(new PackedLongDocValuesField("id", numDocs));
-      docValues.add(br);
-      writer.addDocument(doc);
-      numDocs++;
-
-      if (random.nextInt(40) == 17) {
-        // force flush
-        writer.getReader().close();
-      }
-    }
-
-    writer.forceMerge(1);
-    final DirectoryReader r = writer.getReader();
-    writer.close();
-    
-    final AtomicReader sr = getOnlySegmentReader(r);
-    final DocValues dv = sr.docValues("stringdv");
-    assertNotNull(dv);
-
-    final long END_TIME = System.currentTimeMillis() + (TEST_NIGHTLY ? 30 : 1);
-
-    final DocValues.Source docIDToID = sr.docValues("id").getSource();
-
-    final int NUM_THREADS = _TestUtil.nextInt(random(), 1, 10);
-    Thread[] threads = new Thread[NUM_THREADS];
-    for(int thread=0;thread<NUM_THREADS;thread++) {
-      threads[thread] = new Thread() {
-          @Override
-          public void run() {
-            Random random = random();            
-            final DocValues.Source stringDVSource;
-            final DocValues.Source stringDVDirectSource;
-            try {
-              stringDVSource = dv.getSource();
-              assertNotNull(stringDVSource);
-              stringDVDirectSource = dv.getDirectSource();
-              assertNotNull(stringDVDirectSource);
-            } catch (IOException ioe) {
-              throw new RuntimeException(ioe);
-            }
-            while(System.currentTimeMillis() < END_TIME) {
-              final DocValues.Source source;
-              if (random.nextBoolean()) {
-                source = stringDVSource;
-              } else {
-                source = stringDVDirectSource;
-              }
-
-              final DocValues.SortedSource sortedSource = source.asSortedSource();
-              assertNotNull(sortedSource);
-
-              final BytesRef scratch = new BytesRef();
-
-              for(int iter=0;iter<100;iter++) {
-                final int docID = random.nextInt(sr.maxDoc());
-                final BytesRef br = sortedSource.getBytes(docID, scratch);
-                assertEquals(docValues.get((int) docIDToID.getInt(docID)), br);
-              }
-            }
-          }
-        };
-      threads[thread].start();
-    }
-
-    for(Thread thread : threads) {
-      thread.join();
-    }
-
+    doc = new Document();
+    doc.add(f);
+    w.addDocument(doc);
+    w.forceMerge(1);
+    DirectoryReader r = w.getReader();
+    assertEquals(17, getOnlySegmentReader(r).getNumericDocValues("field").get(0));
     r.close();
-    dir.close();
+    w.close();
+    d.close();
   }
 
   // LUCENE-3870
@@ -1017,20 +173,21 @@
     BytesRef b = new BytesRef();
     b.bytes = bytes;
     b.length = bytes.length;
-    doc.add(new DerefBytesDocValuesField("field", b));
+    doc.add(new SortedDocValuesField("field", b));
     w.addDocument(doc);
     bytes[0] = 1;
     w.addDocument(doc);
     w.forceMerge(1);
     DirectoryReader r = w.getReader();
-    Source s = getOnlySegmentReader(r).docValues("field").getSource();
+    BinaryDocValues s = FieldCache.DEFAULT.getTerms(getOnlySegmentReader(r), "field");
 
-    BytesRef bytes1 = s.getBytes(0, new BytesRef());
+    BytesRef bytes1 = new BytesRef();
+    s.get(0, bytes1);
     assertEquals(bytes.length, bytes1.length);
     bytes[0] = 0;
     assertEquals(b, bytes1);
     
-    bytes1 = s.getBytes(1, new BytesRef());
+    s.get(1, bytes1);
     assertEquals(bytes.length, bytes1.length);
     bytes[0] = 1;
     assertEquals(b, bytes1);
@@ -1038,24 +195,7 @@
     w.close();
     d.close();
   }
-  
-  public void testFixedLengthNotReallyFixed() throws IOException {
-    Directory d = newDirectory();
-    IndexWriter w = new IndexWriter(d, new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));
-    Document doc = new Document();
-    doc.add(new DerefBytesDocValuesField("foo", new BytesRef("bar"), true));
-    w.addDocument(doc);
-    doc = new Document();
-    doc.add(new DerefBytesDocValuesField("foo", new BytesRef("bazz"), true));
-    try {
-      w.addDocument(doc);
-    } catch (IllegalArgumentException expected) {
-      // expected
-    }
-    w.close();
-    d.close();
-  }
-  
+
   public void testDocValuesUnstored() throws IOException {
     Directory dir = newDirectory();
     IndexWriterConfig iwconfig = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
@@ -1063,7 +203,7 @@
     IndexWriter writer = new IndexWriter(dir, iwconfig);
     for (int i = 0; i < 50; i++) {
       Document doc = new Document();
-      doc.add(new PackedLongDocValuesField("dv", i));
+      doc.add(new NumericDocValuesField("dv", i));
       doc.add(new TextField("docId", "" + i, Field.Store.YES));
       writer.addDocument(doc);
     }
@@ -1072,10 +212,9 @@
     FieldInfos fi = slow.getFieldInfos();
     FieldInfo dvInfo = fi.fieldInfo("dv");
     assertTrue(dvInfo.hasDocValues());
-    DocValues dv = slow.docValues("dv");
-    Source source = dv.getDirectSource();
+    NumericDocValues dv = slow.getNumericDocValues("dv");
     for (int i = 0; i < 50; i++) {
-      assertEquals(i, source.getInt(i));
+      assertEquals(i, dv.get(i));
       StoredDocument d = slow.document(i);
       // cannot use d.get("dv") due to another bug!
       assertNull(d.getField("dv"));
@@ -1085,24 +224,530 @@
     writer.close();
     dir.close();
   }
+
+  // Same field in one document as different types:
+  public void testMixedTypesSameDocument() throws Exception {
+    Directory dir = newDirectory();
+    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));
+    Document doc = new Document();
+    doc.add(new NumericDocValuesField("foo", 0));
+    doc.add(new SortedDocValuesField("foo", new BytesRef("hello")));
+    try {
+      w.addDocument(doc);
+    } catch (IllegalArgumentException iae) {
+      // expected
+    }
+    w.close();
+    dir.close();
+  }
+
+  // Two documents with same field as different types:
+  public void testMixedTypesDifferentDocuments() throws Exception {
+    Directory dir = newDirectory();
+    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));
+    Document doc = new Document();
+    doc.add(new NumericDocValuesField("foo", 0));
+    w.addDocument(doc);
+
+    doc = new Document();
+    doc.add(new SortedDocValuesField("foo", new BytesRef("hello")));
+    try {
+      w.addDocument(doc);
+    } catch (IllegalArgumentException iae) {
+      // expected
+    }
+    w.close();
+    dir.close();
+  }
   
-  /**
-  *
-  */
- public static class NotCachingSourceCache extends SourceCache {
-   
-   @Override
-   public Source load(DocValues values) throws IOException {
-     return values.loadSource();
-   }
-   
-   @Override
-   public Source loadDirect(DocValues values) throws IOException {
-     return values.loadDirectSource();
-   }
-   
-   @Override
-   public void invalidate(DocValues values) {}
- }
- 
+  public void testAddSortedTwice() throws IOException {
+    Analyzer analyzer = new MockAnalyzer(random());
+
+    Directory directory = newDirectory();
+    // we don't use RandomIndexWriter because it might add more docvalues than we expect !!!!1
+    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);
+    iwc.setMergePolicy(newLogMergePolicy());
+    IndexWriter iwriter = new IndexWriter(directory, iwc);
+    Document doc = new Document();
+    doc.add(new SortedDocValuesField("dv", new BytesRef("foo!")));
+    doc.add(new SortedDocValuesField("dv", new BytesRef("bar!")));
+    try {
+      iwriter.addDocument(doc);
+      fail("didn't hit expected exception");
+    } catch (IllegalArgumentException expected) {
+      // expected
+    }
+    
+    iwriter.close();
+    directory.close();
+  }
+  
+  public void testAddBinaryTwice() throws IOException {
+    Analyzer analyzer = new MockAnalyzer(random());
+
+    Directory directory = newDirectory();
+    // we don't use RandomIndexWriter because it might add more docvalues than we expect !!!!1
+    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);
+    iwc.setMergePolicy(newLogMergePolicy());
+    IndexWriter iwriter = new IndexWriter(directory, iwc);
+    Document doc = new Document();
+    doc.add(new BinaryDocValuesField("dv", new BytesRef("foo!")));
+    doc.add(new BinaryDocValuesField("dv", new BytesRef("bar!")));
+    try {
+      iwriter.addDocument(doc);
+      fail("didn't hit expected exception");
+    } catch (IllegalArgumentException expected) {
+      // expected
+    }
+    
+    iwriter.close();
+    directory.close();
+  }
+  
+  public void testAddNumericTwice() throws IOException {
+    Analyzer analyzer = new MockAnalyzer(random());
+
+    Directory directory = newDirectory();
+    // we don't use RandomIndexWriter because it might add more docvalues than we expect !!!!1
+    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);
+    iwc.setMergePolicy(newLogMergePolicy());
+    IndexWriter iwriter = new IndexWriter(directory, iwc);
+    Document doc = new Document();
+    doc.add(new NumericDocValuesField("dv", 1));
+    doc.add(new NumericDocValuesField("dv", 2));
+    try {
+      iwriter.addDocument(doc);
+      fail("didn't hit expected exception");
+    } catch (IllegalArgumentException expected) {
+      // expected
+    }
+    
+    iwriter.close();
+    directory.close();
+  }
+  
+  public void testTooLargeBytes() throws IOException {
+    Analyzer analyzer = new MockAnalyzer(random());
+
+    Directory directory = newDirectory();
+    // we don't use RandomIndexWriter because it might add more docvalues than we expect !!!!1
+    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);
+    iwc.setMergePolicy(newLogMergePolicy());
+    IndexWriter iwriter = new IndexWriter(directory, iwc);
+    Document doc = new Document();
+    byte bytes[] = new byte[100000];
+    BytesRef b = new BytesRef(bytes);
+    random().nextBytes(bytes);
+    doc.add(new BinaryDocValuesField("dv", b));
+    try {
+      iwriter.addDocument(doc);
+      fail("did not get expected exception");
+    } catch (IllegalArgumentException expected) {
+      // expected
+    }
+    iwriter.close();
+
+    directory.close();
+  }
+  
+  public void testTooLargeSortedBytes() throws IOException {
+    Analyzer analyzer = new MockAnalyzer(random());
+
+    Directory directory = newDirectory();
+    // we don't use RandomIndexWriter because it might add more docvalues than we expect !!!!1
+    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);
+    iwc.setMergePolicy(newLogMergePolicy());
+    IndexWriter iwriter = new IndexWriter(directory, iwc);
+    Document doc = new Document();
+    byte bytes[] = new byte[100000];
+    BytesRef b = new BytesRef(bytes);
+    random().nextBytes(bytes);
+    doc.add(new SortedDocValuesField("dv", b));
+    try {
+      iwriter.addDocument(doc);
+      fail("did not get expected exception");
+    } catch (IllegalArgumentException expected) {
+      // expected
+    }
+    iwriter.close();
+    directory.close();
+  }
+
+  // Two documents across segments
+  public void testMixedTypesDifferentSegments() throws Exception {
+    Directory dir = newDirectory();
+    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));
+    Document doc = new Document();
+    doc.add(new NumericDocValuesField("foo", 0));
+    w.addDocument(doc);
+    w.commit();
+
+    doc = new Document();
+    doc.add(new SortedDocValuesField("foo", new BytesRef("hello")));
+    try {
+      w.addDocument(doc);
+    } catch (IllegalArgumentException iae) {
+      // expected
+    }
+    w.close();
+    dir.close();
+  }
+
+  // Add inconsistent document after deleteAll
+  public void testMixedTypesAfterDeleteAll() throws Exception {
+    Directory dir = newDirectory();
+    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));
+    Document doc = new Document();
+    doc.add(new NumericDocValuesField("foo", 0));
+    w.addDocument(doc);
+    w.deleteAll();
+
+    doc = new Document();
+    doc.add(new SortedDocValuesField("foo", new BytesRef("hello")));
+    w.addDocument(doc);
+    w.close();
+    dir.close();
+  }
+
+  // Add inconsistent document after reopening IW w/ create
+  public void testMixedTypesAfterReopenCreate() throws Exception {
+    Directory dir = newDirectory();
+    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));
+    Document doc = new Document();
+    doc.add(new NumericDocValuesField("foo", 0));
+    w.addDocument(doc);
+    w.close();
+
+    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
+    iwc.setOpenMode(IndexWriterConfig.OpenMode.CREATE);
+    w = new IndexWriter(dir, iwc);
+    doc = new Document();
+    doc.add(new SortedDocValuesField("foo", new BytesRef("hello")));
+    w.addDocument(doc);
+    w.close();
+    dir.close();
+  }
+
+  // Two documents with same field as different types, added
+  // from separate threads:
+  public void testMixedTypesDifferentThreads() throws Exception {
+    Directory dir = newDirectory();
+    final IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));
+
+    final CountDownLatch startingGun = new CountDownLatch(1);
+    final AtomicBoolean hitExc = new AtomicBoolean();
+    Thread[] threads = new Thread[3];
+    for(int i=0;i<3;i++) {
+      Field field;
+      if (i == 0) {
+        field = new SortedDocValuesField("foo", new BytesRef("hello"));
+      } else if (i == 1) {
+        field = new NumericDocValuesField("foo", 0);
+      } else {
+        field = new BinaryDocValuesField("foo", new BytesRef("bazz"));
+      }
+      final Document doc = new Document();
+      doc.add(field);
+
+      threads[i] = new Thread() {
+          @Override
+          public void run() {
+            try {
+              startingGun.await();
+              w.addDocument(doc);
+            } catch (IllegalArgumentException iae) {
+              // expected
+              hitExc.set(true);
+            } catch (Exception e) {
+              throw new RuntimeException(e);
+            }
+          }
+        };
+      threads[i].start();
+    }
+
+    startingGun.countDown();
+
+    for(Thread t : threads) {
+      t.join();
+    }
+    assertTrue(hitExc.get());
+    w.close();
+    dir.close();
+  }
+
+  // Adding documents via addIndexes
+  public void testMixedTypesViaAddIndexes() throws Exception {
+    Directory dir = newDirectory();
+    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));
+    Document doc = new Document();
+    doc.add(new NumericDocValuesField("foo", 0));
+    w.addDocument(doc);
+
+    // Make 2nd index w/ inconsistent field
+    Directory dir2 = newDirectory();
+    IndexWriter w2 = new IndexWriter(dir2, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));
+    doc = new Document();
+    doc.add(new SortedDocValuesField("foo", new BytesRef("hello")));
+    w2.addDocument(doc);
+    w2.close();
+
+    try {
+      w.addIndexes(new Directory[] {dir2});
+    } catch (IllegalArgumentException iae) {
+      // expected
+    }
+
+    IndexReader r = DirectoryReader.open(dir2);
+    try {
+      w.addIndexes(new IndexReader[] {r});
+    } catch (IllegalArgumentException iae) {
+      // expected
+    }
+
+    r.close();
+    dir2.close();
+    w.close();
+    dir.close();
+  }
+
+  public void testIllegalTypeChange() throws Exception {
+    Directory dir = newDirectory();
+    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
+    IndexWriter writer = new IndexWriter(dir, conf);
+    Document doc = new Document();
+    doc.add(new NumericDocValuesField("dv", 0L));
+    writer.addDocument(doc);
+    doc = new Document();
+    doc.add(new SortedDocValuesField("dv", new BytesRef("foo")));
+    try {
+      writer.addDocument(doc);
+      fail("did not hit exception");
+    } catch (IllegalArgumentException iae) {
+      // expected
+    }
+    writer.close();
+    dir.close();
+  }
+
+  public void testIllegalTypeChangeAcrossSegments() throws Exception {
+    Directory dir = newDirectory();
+    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
+    IndexWriter writer = new IndexWriter(dir, conf);
+    Document doc = new Document();
+    doc.add(new NumericDocValuesField("dv", 0L));
+    writer.addDocument(doc);
+    writer.close();
+
+    writer = new IndexWriter(dir, conf);
+    doc = new Document();
+    doc.add(new SortedDocValuesField("dv", new BytesRef("foo")));
+    try {
+      writer.addDocument(doc);
+      fail("did not hit exception");
+    } catch (IllegalArgumentException iae) {
+      // expected
+    }
+    writer.close();
+    dir.close();
+  }
+
+  public void testTypeChangeAfterCloseAndDeleteAll() throws Exception {
+    Directory dir = newDirectory();
+    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
+    IndexWriter writer = new IndexWriter(dir, conf);
+    Document doc = new Document();
+    doc.add(new NumericDocValuesField("dv", 0L));
+    writer.addDocument(doc);
+    writer.close();
+
+    writer = new IndexWriter(dir, conf);
+    writer.deleteAll();
+    doc = new Document();
+    doc.add(new SortedDocValuesField("dv", new BytesRef("foo")));
+    writer.addDocument(doc);
+    writer.close();
+    dir.close();
+  }
+
+  public void testTypeChangeAfterDeleteAll() throws Exception {
+    Directory dir = newDirectory();
+    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
+    IndexWriter writer = new IndexWriter(dir, conf);
+    Document doc = new Document();
+    doc.add(new NumericDocValuesField("dv", 0L));
+    writer.addDocument(doc);
+    writer.deleteAll();
+    doc = new Document();
+    doc.add(new SortedDocValuesField("dv", new BytesRef("foo")));
+    writer.addDocument(doc);
+    writer.close();
+    dir.close();
+  }
+
+  public void testTypeChangeAfterCommitAndDeleteAll() throws Exception {
+    Directory dir = newDirectory();
+    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
+    IndexWriter writer = new IndexWriter(dir, conf);
+    Document doc = new Document();
+    doc.add(new NumericDocValuesField("dv", 0L));
+    writer.addDocument(doc);
+    writer.commit();
+    writer.deleteAll();
+    doc = new Document();
+    doc.add(new SortedDocValuesField("dv", new BytesRef("foo")));
+    writer.addDocument(doc);
+    writer.close();
+    dir.close();
+  }
+
+  public void testTypeChangeAfterOpenCreate() throws Exception {
+    Directory dir = newDirectory();
+    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
+    IndexWriter writer = new IndexWriter(dir, conf);
+    Document doc = new Document();
+    doc.add(new NumericDocValuesField("dv", 0L));
+    writer.addDocument(doc);
+    writer.close();
+    conf.setOpenMode(IndexWriterConfig.OpenMode.CREATE);
+    writer = new IndexWriter(dir, conf);
+    doc = new Document();
+    doc.add(new SortedDocValuesField("dv", new BytesRef("foo")));
+    writer.addDocument(doc);
+    writer.close();
+    dir.close();
+  }
+
+  public void testTypeChangeViaAddIndexes() throws Exception {
+    Directory dir = newDirectory();
+    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
+    IndexWriter writer = new IndexWriter(dir, conf);
+    Document doc = new Document();
+    doc.add(new NumericDocValuesField("dv", 0L));
+    writer.addDocument(doc);
+    writer.close();
+
+    Directory dir2 = newDirectory();
+    writer = new IndexWriter(dir2, conf);
+    doc = new Document();
+    doc.add(new SortedDocValuesField("dv", new BytesRef("foo")));
+    writer.addDocument(doc);
+    try {
+      writer.addIndexes(dir);
+      fail("did not hit exception");
+    } catch (IllegalArgumentException iae) {
+      // expected
+    }
+    writer.close();
+
+    dir.close();
+    dir2.close();
+  }
+
+  public void testTypeChangeViaAddIndexesIR() throws Exception {
+    Directory dir = newDirectory();
+    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
+    IndexWriter writer = new IndexWriter(dir, conf);
+    Document doc = new Document();
+    doc.add(new NumericDocValuesField("dv", 0L));
+    writer.addDocument(doc);
+    writer.close();
+
+    Directory dir2 = newDirectory();
+    writer = new IndexWriter(dir2, conf);
+    doc = new Document();
+    doc.add(new SortedDocValuesField("dv", new BytesRef("foo")));
+    writer.addDocument(doc);
+    IndexReader[] readers = new IndexReader[] {DirectoryReader.open(dir)};
+    try {
+      writer.addIndexes(readers);
+      fail("did not hit exception");
+    } catch (IllegalArgumentException iae) {
+      // expected
+    }
+    readers[0].close();
+    writer.close();
+
+    dir.close();
+    dir2.close();
+  }
+
+  public void testTypeChangeViaAddIndexes2() throws Exception {
+    Directory dir = newDirectory();
+    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
+    IndexWriter writer = new IndexWriter(dir, conf);
+    Document doc = new Document();
+    doc.add(new NumericDocValuesField("dv", 0L));
+    writer.addDocument(doc);
+    writer.close();
+
+    Directory dir2 = newDirectory();
+    writer = new IndexWriter(dir2, conf);
+    writer.addIndexes(dir);
+    doc = new Document();
+    doc.add(new SortedDocValuesField("dv", new BytesRef("foo")));
+    try {
+      writer.addDocument(doc);
+      fail("did not hit exception");
+    } catch (IllegalArgumentException iae) {
+      // expected
+    }
+    writer.close();
+    dir2.close();
+    dir.close();
+  }
+
+  public void testTypeChangeViaAddIndexesIR2() throws Exception {
+    Directory dir = newDirectory();
+    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
+    IndexWriter writer = new IndexWriter(dir, conf);
+    Document doc = new Document();
+    doc.add(new NumericDocValuesField("dv", 0L));
+    writer.addDocument(doc);
+    writer.close();
+
+    Directory dir2 = newDirectory();
+    writer = new IndexWriter(dir2, conf);
+    IndexReader[] readers = new IndexReader[] {DirectoryReader.open(dir)};
+    writer.addIndexes(readers);
+    readers[0].close();
+    doc = new Document();
+    doc.add(new SortedDocValuesField("dv", new BytesRef("foo")));
+    try {
+      writer.addDocument(doc);
+      fail("did not hit exception");
+    } catch (IllegalArgumentException iae) {
+      // expected
+    }
+    writer.close();
+    dir2.close();
+    dir.close();
+  }
+
+  public void testDocsWithField() throws Exception {
+    Directory dir = newDirectory();
+    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
+    IndexWriter writer = new IndexWriter(dir, conf);
+    Document doc = new Document();
+    doc.add(new NumericDocValuesField("dv", 0L));
+    writer.addDocument(doc);
+
+    doc = new Document();
+    doc.add(new TextField("dv", "some text", Field.Store.NO));
+    doc.add(new NumericDocValuesField("dv", 0L));
+    writer.addDocument(doc);
+    
+    DirectoryReader r = writer.getReader();
+    writer.close();
+
+    AtomicReader subR = r.leaves().get(0).reader();
+    assertEquals(2, subR.numDocs());
+
+    Bits bits = FieldCache.DEFAULT.getDocsWithField(subR, "dv");
+    assertTrue(bits.get(0));
+    assertTrue(bits.get(1));
+    r.close();
+    dir.close();
+  }
+
 }
Index: lucene/core/src/test/org/apache/lucene/index/Test2BBinaryDocValues.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/index/Test2BBinaryDocValues.java	(revision 0)
+++ lucene/core/src/test/org/apache/lucene/index/Test2BBinaryDocValues.java	(working copy)
@@ -0,0 +1,157 @@
+package org.apache.lucene.index;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.analysis.MockAnalyzer;
+import org.apache.lucene.document.BinaryDocValuesField;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.store.BaseDirectoryWrapper;
+import org.apache.lucene.store.ByteArrayDataInput;
+import org.apache.lucene.store.ByteArrayDataOutput;
+import org.apache.lucene.store.MockDirectoryWrapper;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.LuceneTestCase;
+import org.apache.lucene.util.TimeUnits;
+import org.apache.lucene.util._TestUtil;
+import org.junit.Ignore;
+
+import com.carrotsearch.randomizedtesting.annotations.TimeoutSuite;
+
+@TimeoutSuite(millis = 80 * TimeUnits.HOUR)
+@Ignore("takes ~ 45 minutes")
+public class Test2BBinaryDocValues extends LuceneTestCase {
+  
+  // indexes Integer.MAX_VALUE docs with a fixed binary field
+  public void testFixedBinary() throws Exception {
+    BaseDirectoryWrapper dir = newFSDirectory(_TestUtil.getTempDir("2BFixedBinary"));
+    if (dir instanceof MockDirectoryWrapper) {
+      ((MockDirectoryWrapper)dir).setThrottling(MockDirectoryWrapper.Throttling.NEVER);
+    }
+    
+    IndexWriter w = new IndexWriter(dir,
+        new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()))
+        .setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH)
+        .setRAMBufferSizeMB(256.0)
+        .setMergeScheduler(new ConcurrentMergeScheduler())
+        .setMergePolicy(newLogMergePolicy(false, 10))
+        .setOpenMode(IndexWriterConfig.OpenMode.CREATE));
+
+    Document doc = new Document();
+    byte bytes[] = new byte[4];
+    BytesRef data = new BytesRef(bytes);
+    BinaryDocValuesField dvField = new BinaryDocValuesField("dv", data);
+    doc.add(dvField);
+    
+    for (int i = 0; i < Integer.MAX_VALUE; i++) {
+      bytes[0] = (byte)(i >> 24);
+      bytes[1] = (byte)(i >> 16);
+      bytes[2] = (byte)(i >> 8);
+      bytes[3] = (byte) i;
+      w.addDocument(doc);
+      if (i % 100000 == 0) {
+        System.out.println("indexed: " + i);
+        System.out.flush();
+      }
+    }
+    
+    w.forceMerge(1);
+    w.close();
+    
+    System.out.println("verifying...");
+    System.out.flush();
+    
+    DirectoryReader r = DirectoryReader.open(dir);
+    int expectedValue = 0;
+    for (AtomicReaderContext context : r.leaves()) {
+      AtomicReader reader = context.reader();
+      BytesRef scratch = new BytesRef();
+      BinaryDocValues dv = reader.getBinaryDocValues("dv");
+      for (int i = 0; i < reader.maxDoc(); i++) {
+        bytes[0] = (byte)(expectedValue >> 24);
+        bytes[1] = (byte)(expectedValue >> 16);
+        bytes[2] = (byte)(expectedValue >> 8);
+        bytes[3] = (byte) expectedValue;
+        dv.get(i, scratch);
+        assertEquals(data, scratch);
+        expectedValue++;
+      }
+    }
+    
+    r.close();
+    dir.close();
+  }
+  
+  // indexes Integer.MAX_VALUE docs with a variable binary field
+  public void testVariableBinary() throws Exception {
+    BaseDirectoryWrapper dir = newFSDirectory(_TestUtil.getTempDir("2BVariableBinary"));
+    if (dir instanceof MockDirectoryWrapper) {
+      ((MockDirectoryWrapper)dir).setThrottling(MockDirectoryWrapper.Throttling.NEVER);
+    }
+    
+    IndexWriter w = new IndexWriter(dir,
+        new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()))
+        .setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH)
+        .setRAMBufferSizeMB(256.0)
+        .setMergeScheduler(new ConcurrentMergeScheduler())
+        .setMergePolicy(newLogMergePolicy(false, 10))
+        .setOpenMode(IndexWriterConfig.OpenMode.CREATE));
+
+    Document doc = new Document();
+    byte bytes[] = new byte[4];
+    ByteArrayDataOutput encoder = new ByteArrayDataOutput(bytes);
+    BytesRef data = new BytesRef(bytes);
+    BinaryDocValuesField dvField = new BinaryDocValuesField("dv", data);
+    doc.add(dvField);
+    
+    for (int i = 0; i < Integer.MAX_VALUE; i++) {
+      encoder.reset(bytes);
+      encoder.writeVInt(i % 65535); // 1, 2, or 3 bytes
+      data.length = encoder.getPosition();
+      w.addDocument(doc);
+      if (i % 100000 == 0) {
+        System.out.println("indexed: " + i);
+        System.out.flush();
+      }
+    }
+    
+    w.forceMerge(1);
+    w.close();
+    
+    System.out.println("verifying...");
+    System.out.flush();
+    
+    DirectoryReader r = DirectoryReader.open(dir);
+    int expectedValue = 0;
+    ByteArrayDataInput input = new ByteArrayDataInput();
+    for (AtomicReaderContext context : r.leaves()) {
+      AtomicReader reader = context.reader();
+      BytesRef scratch = new BytesRef(bytes);
+      BinaryDocValues dv = reader.getBinaryDocValues("dv");
+      for (int i = 0; i < reader.maxDoc(); i++) {
+        dv.get(i, scratch);
+        input.reset(scratch.bytes, scratch.offset, scratch.length);
+        assertEquals(expectedValue % 65535, input.readVInt());
+        assertTrue(input.eof());
+        expectedValue++;
+      }
+    }
+    
+    r.close();
+    dir.close();
+  }
+}

Property changes on: lucene/core/src/test/org/apache/lucene/index/Test2BBinaryDocValues.java
___________________________________________________________________
Added: svn:eol-style
## -0,0 +1 ##
+native
Index: lucene/core/src/test/org/apache/lucene/index/TestSegmentReader.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/index/TestSegmentReader.java	(revision 1442822)
+++ lucene/core/src/test/org/apache/lucene/index/TestSegmentReader.java	(working copy)
@@ -29,6 +29,7 @@
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.LuceneTestCase;
 import org.apache.lucene.util._TestUtil;
+import org.junit.Assume;
 
 public class TestSegmentReader extends LuceneTestCase {
   private Directory dir;
@@ -173,15 +174,15 @@
   }
 
   public static void checkNorms(AtomicReader reader) throws IOException {
-        // test omit norms
+    // test omit norms
     for (int i=0; i<DocHelper.fields.length; i++) {
       IndexableField f = DocHelper.fields[i];
       if (f.fieldType().indexed()) {
-        assertEquals(reader.normValues(f.name()) != null, !f.fieldType().omitNorms());
-        assertEquals(reader.normValues(f.name()) != null, !DocHelper.noNorms.containsKey(f.name()));
-        if (reader.normValues(f.name()) == null) {
+        assertEquals(reader.getNormValues(f.name()) != null, !f.fieldType().omitNorms());
+        assertEquals(reader.getNormValues(f.name()) != null, !DocHelper.noNorms.containsKey(f.name()));
+        if (reader.getNormValues(f.name()) == null) {
           // test for norms of null
-          DocValues norms = MultiDocValues.getNormDocValues(reader, f.name());
+          NumericDocValues norms = MultiDocValues.getNormValues(reader, f.name());
           assertNull(norms);
         }
       }
Index: lucene/core/src/test/org/apache/lucene/index/TestOmitTf.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/index/TestOmitTf.java	(revision 1442822)
+++ lucene/core/src/test/org/apache/lucene/index/TestOmitTf.java	(working copy)
@@ -314,7 +314,7 @@
                       public final void collect(int doc) throws IOException {
                         //System.out.println("Q1: Doc=" + doc + " score=" + score);
                         float score = scorer.score();
-                        assertTrue(score==1.0f);
+                        assertTrue("got score=" + score, score==1.0f);
                         super.collect(doc);
                       }
                     });
Index: lucene/core/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull.java	(revision 1442822)
+++ lucene/core/src/test/org/apache/lucene/index/TestIndexWriterOnDiskFull.java	(working copy)
@@ -24,6 +24,7 @@
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.FieldType;
+import org.apache.lucene.document.NumericDocValuesField;
 import org.apache.lucene.document.TextField;
 import org.apache.lucene.index.IndexWriterConfig.OpenMode;
 import org.apache.lucene.search.IndexSearcher;
@@ -557,6 +558,7 @@
   {
       Document doc = new Document();
       doc.add(newTextField("content", "aaa", Field.Store.NO));
+      doc.add(new NumericDocValuesField("numericdv", 1));
       writer.addDocument(doc);
   }
   
@@ -565,6 +567,7 @@
       Document doc = new Document();
       doc.add(newTextField("content", "aaa " + index, Field.Store.NO));
       doc.add(newTextField("id", "" + index, Field.Store.NO));
+      doc.add(new NumericDocValuesField("numericdv", 1));
       writer.addDocument(doc);
   }
 }
Index: lucene/core/src/test/org/apache/lucene/index/TestIndexWriter.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/index/TestIndexWriter.java	(revision 1442822)
+++ lucene/core/src/test/org/apache/lucene/index/TestIndexWriter.java	(working copy)
@@ -34,9 +34,12 @@
 import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;
 import org.apache.lucene.codecs.Codec;
 import org.apache.lucene.codecs.simpletext.SimpleTextCodec;
+import org.apache.lucene.document.BinaryDocValuesField;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.FieldType;
+import org.apache.lucene.document.NumericDocValuesField;
+import org.apache.lucene.document.SortedDocValuesField;
 import org.apache.lucene.document.StoredField;
 import org.apache.lucene.document.StringField;
 import org.apache.lucene.document.TextField;
@@ -1017,10 +1020,16 @@
       Document doc = new Document();
       doc.add(newStringField(random, "id", "500", Field.Store.NO));
       doc.add(newField(random, "field", "some prepackaged text contents", storedTextType));
+      doc.add(new BinaryDocValuesField("binarydv", new BytesRef("500")));
+      doc.add(new NumericDocValuesField("numericdv", 500));
+      doc.add(new SortedDocValuesField("sorteddv", new BytesRef("500")));
       w.addDocument(doc);
       doc = new Document();
       doc.add(newStringField(random, "id", "501", Field.Store.NO));
       doc.add(newField(random, "field", "some more contents", storedTextType));
+      doc.add(new BinaryDocValuesField("binarydv", new BytesRef("501")));
+      doc.add(new NumericDocValuesField("numericdv", 501));
+      doc.add(new SortedDocValuesField("sorteddv", new BytesRef("501")));
       w.addDocument(doc);
       w.deleteDocuments(new Term("id", "500"));
       w.close();
@@ -1045,10 +1054,19 @@
 
             Document doc = new Document();
             Field idField = newStringField(random, "id", "", Field.Store.NO);
+            Field binaryDVField = new BinaryDocValuesField("binarydv", new BytesRef());
+            Field numericDVField = new NumericDocValuesField("numericdv", 0);
+            Field sortedDVField = new SortedDocValuesField("sorteddv", new BytesRef());
             doc.add(idField);
             doc.add(newField(random, "field", "some text contents", storedTextType));
+            doc.add(binaryDVField);
+            doc.add(numericDVField);
+            doc.add(sortedDVField);
             for(int i=0;i<100;i++) {
               idField.setStringValue(Integer.toString(i));
+              binaryDVField.setBytesValue(new BytesRef(idField.stringValue()));
+              numericDVField.setLongValue(i);
+              sortedDVField.setBytesValue(new BytesRef(idField.stringValue()));
               int action = random.nextInt(100);
               if (action == 17) {
                 w.addIndexes(adder);
@@ -1694,10 +1712,11 @@
     w.close();
     assertEquals(1, reader.docFreq(new Term("content", bigTerm)));
 
-    FieldCache.DocTermsIndex dti = FieldCache.DEFAULT.getTermsIndex(SlowCompositeReaderWrapper.wrap(reader), "content", random().nextFloat() * PackedInts.FAST);
-    assertEquals(5, dti.numOrd());                // +1 for null ord
-    assertEquals(4, dti.size());
-    assertEquals(bigTermBytesRef, dti.lookup(3, new BytesRef()));
+    SortedDocValues dti = FieldCache.DEFAULT.getTermsIndex(SlowCompositeReaderWrapper.wrap(reader), "content", random().nextFloat() * PackedInts.FAST);
+    assertEquals(4, dti.getValueCount());
+    BytesRef br = new BytesRef();
+    dti.lookupOrd(2, br);
+    assertEquals(bigTermBytesRef, br);
     reader.close();
     dir.close();
   }
Index: lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions.java	(revision 1442822)
+++ lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions.java	(working copy)
@@ -27,9 +27,12 @@
 import java.util.concurrent.atomic.AtomicBoolean;
 
 import org.apache.lucene.analysis.*;
+import org.apache.lucene.document.BinaryDocValuesField;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.FieldType;
+import org.apache.lucene.document.NumericDocValuesField;
+import org.apache.lucene.document.SortedDocValuesField;
 import org.apache.lucene.document.StringField;
 import org.apache.lucene.document.TextField;
 import org.apache.lucene.index.IndexWriterConfig.OpenMode;
@@ -137,6 +140,9 @@
 
       doc.add(newTextField(r, "content4", "aaa bbb ccc ddd", Field.Store.NO));
       doc.add(newStringField(r, "content5", "aaa bbb ccc ddd", Field.Store.NO));
+      doc.add(new NumericDocValuesField("numericdv", 5));
+      doc.add(new BinaryDocValuesField("binarydv", new BytesRef("hello")));
+      doc.add(new SortedDocValuesField("sorteddv", new BytesRef("world")));
 
       doc.add(newField(r, "content7", "aaa bbb ccc ddd", DocCopyIterator.custom4));
 
Index: lucene/core/src/test/org/apache/lucene/index/Test2BNumericDocValues.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/index/Test2BNumericDocValues.java	(revision 0)
+++ lucene/core/src/test/org/apache/lucene/index/Test2BNumericDocValues.java	(working copy)
@@ -0,0 +1,84 @@
+package org.apache.lucene.index;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.analysis.MockAnalyzer;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.NumericDocValuesField;
+import org.apache.lucene.store.BaseDirectoryWrapper;
+import org.apache.lucene.store.MockDirectoryWrapper;
+import org.apache.lucene.util.LuceneTestCase;
+import org.apache.lucene.util.TimeUnits;
+import org.apache.lucene.util._TestUtil;
+import org.junit.Ignore;
+
+import com.carrotsearch.randomizedtesting.annotations.TimeoutSuite;
+
+@TimeoutSuite(millis = 80 * TimeUnits.HOUR)
+@Ignore("takes ~ 30 minutes")
+public class Test2BNumericDocValues extends LuceneTestCase {
+  
+  // indexes Integer.MAX_VALUE docs with an increasing dv field
+  public void testNumerics() throws Exception {
+    BaseDirectoryWrapper dir = newFSDirectory(_TestUtil.getTempDir("2BNumerics"));
+    if (dir instanceof MockDirectoryWrapper) {
+      ((MockDirectoryWrapper)dir).setThrottling(MockDirectoryWrapper.Throttling.NEVER);
+    }
+    
+    IndexWriter w = new IndexWriter(dir,
+        new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()))
+        .setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH)
+        .setRAMBufferSizeMB(256.0)
+        .setMergeScheduler(new ConcurrentMergeScheduler())
+        .setMergePolicy(newLogMergePolicy(false, 10))
+        .setOpenMode(IndexWriterConfig.OpenMode.CREATE));
+
+    Document doc = new Document();
+    NumericDocValuesField dvField = new NumericDocValuesField("dv", 0);
+    doc.add(dvField);
+    
+    for (int i = 0; i < Integer.MAX_VALUE; i++) {
+      dvField.setLongValue(i);
+      w.addDocument(doc);
+      if (i % 100000 == 0) {
+        System.out.println("indexed: " + i);
+        System.out.flush();
+      }
+    }
+    
+    w.forceMerge(1);
+    w.close();
+    
+    System.out.println("verifying...");
+    System.out.flush();
+    
+    DirectoryReader r = DirectoryReader.open(dir);
+    long expectedValue = 0;
+    for (AtomicReaderContext context : r.leaves()) {
+      AtomicReader reader = context.reader();
+      NumericDocValues dv = reader.getNumericDocValues("dv");
+      for (int i = 0; i < reader.maxDoc(); i++) {
+        assertEquals(expectedValue, dv.get(i));
+        expectedValue++;
+      }
+    }
+    
+    r.close();
+    dir.close();
+  }
+}

Property changes on: lucene/core/src/test/org/apache/lucene/index/Test2BNumericDocValues.java
___________________________________________________________________
Added: svn:eol-style
## -0,0 +1 ##
+native
Index: lucene/core/src/test/org/apache/lucene/index/TestOmitNorms.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/index/TestOmitNorms.java	(revision 1442822)
+++ lucene/core/src/test/org/apache/lucene/index/TestOmitNorms.java	(working copy)
@@ -19,8 +19,6 @@
 
 import java.io.IOException;
 
-import org.apache.lucene.util.LuceneTestCase;
-import org.apache.lucene.util._TestUtil;
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.document.Document;
@@ -28,6 +26,8 @@
 import org.apache.lucene.document.FieldType;
 import org.apache.lucene.document.TextField;
 import org.apache.lucene.store.Directory;
+import org.apache.lucene.util.LuceneTestCase;
+import org.apache.lucene.util._TestUtil;
 
 public class TestOmitNorms extends LuceneTestCase {
   // Tests whether the DocumentWriter correctly enable the
@@ -265,7 +265,7 @@
    * Indexes at least 1 document with f1, and at least 1 document with f2.
    * returns the norms for "field".
    */
-  byte[] getNorms(String field, Field f1, Field f2) throws IOException {
+  NumericDocValues getNorms(String field, Field f1, Field f2) throws IOException {
     Directory dir = newDirectory();
     IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(newLogMergePolicy());
     RandomIndexWriter riw = new RandomIndexWriter(random(), dir, iwc);
@@ -290,16 +290,20 @@
 
     IndexReader ir1 = riw.getReader();
     // todo: generalize
-    DocValues dv1 = MultiDocValues.getNormDocValues(ir1, field);
-    byte[] norms1 = dv1 == null ? null : (byte[]) dv1.getSource().getArray();
+    NumericDocValues norms1 = MultiDocValues.getNormValues(ir1, field);
     
     // fully merge and validate MultiNorms against single segment.
     riw.forceMerge(1);
     DirectoryReader ir2 = riw.getReader();
-    DocValues dv2 = getOnlySegmentReader(ir2).normValues(field);
-    byte[] norms2 = dv2 == null ? null : (byte[]) dv2.getSource().getArray();
-    
-    assertArrayEquals(norms1, norms2);
+    NumericDocValues norms2 = getOnlySegmentReader(ir2).getNormValues(field);
+
+    if (norms1 == null) {
+      assertNull(norms2);
+    } else {
+      for(int docID=0;docID<ir1.maxDoc();docID++) {
+        assertEquals(norms1.get(docID), norms2.get(docID));
+      }
+    }
     ir1.close();
     ir2.close();
     riw.close();
Index: lucene/core/src/test/org/apache/lucene/index/TestDirectoryReaderReopen.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/index/TestDirectoryReaderReopen.java	(revision 1442822)
+++ lucene/core/src/test/org/apache/lucene/index/TestDirectoryReaderReopen.java	(working copy)
@@ -16,7 +16,6 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
-
 import java.io.IOException;
 import java.util.ArrayList;
 import java.util.Collection;
@@ -38,7 +37,6 @@
 import org.apache.lucene.search.ScoreDoc;
 import org.apache.lucene.search.TermQuery;
 import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.MockDirectoryWrapper;
 import org.apache.lucene.util.LuceneTestCase;
 import org.apache.lucene.util._TestUtil;
 
Index: lucene/core/src/test/org/apache/lucene/index/TestTermsEnum.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/index/TestTermsEnum.java	(revision 1442822)
+++ lucene/core/src/test/org/apache/lucene/index/TestTermsEnum.java	(working copy)
@@ -225,7 +225,7 @@
     w.close();
 
     // NOTE: intentional insanity!!
-    final int[] docIDToID = FieldCache.DEFAULT.getInts(SlowCompositeReaderWrapper.wrap(r), "id", false);
+    final FieldCache.Ints docIDToID = FieldCache.DEFAULT.getInts(SlowCompositeReaderWrapper.wrap(r), "id", false);
 
     for(int iter=0;iter<10*RANDOM_MULTIPLIER;iter++) {
 
@@ -335,7 +335,7 @@
           docsEnum = _TestUtil.docs(random(), te, null, docsEnum, DocsEnum.FLAG_NONE);
           final int docID = docsEnum.nextDoc();
           assertTrue(docID != DocIdSetIterator.NO_MORE_DOCS);
-          assertEquals(docIDToID[docID], termToID.get(expected).intValue());
+          assertEquals(docIDToID.get(docID), termToID.get(expected).intValue());
           do {
             loc++;
           } while (loc < termsArray.length && !acceptTermsSet.contains(termsArray[loc]));
Index: lucene/core/src/test/org/apache/lucene/index/TestNorms.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/index/TestNorms.java	(revision 1442822)
+++ lucene/core/src/test/org/apache/lucene/index/TestNorms.java	(working copy)
@@ -24,8 +24,6 @@
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.TextField;
-import org.apache.lucene.index.DocValues.Source;
-import org.apache.lucene.index.DocValues.Type;
 import org.apache.lucene.search.CollectionStatistics;
 import org.apache.lucene.search.TermStatistics;
 import org.apache.lucene.search.similarities.DefaultSimilarity;
@@ -42,7 +40,7 @@
  * Test that norms info is preserved during index life - including
  * separate norms, addDocument, addIndexes, forceMerge.
  */
-@SuppressCodecs({ "SimpleText", "Memory", "Direct" })
+@SuppressCodecs({ "Memory", "Direct", "SimpleText" })
 @Slow
 public class TestNorms extends LuceneTestCase {
   final String byteTestField = "normsTestByte";
@@ -84,13 +82,15 @@
     IndexReader reader = writer.getReader();
     writer.close();
     
-    byte fooNorms[] = (byte[]) MultiDocValues.getNormDocValues(reader, "foo").getSource().getArray();
-    for (int i = 0; i < reader.maxDoc(); i++)
-      assertEquals(0, fooNorms[i]);
+    NumericDocValues fooNorms = MultiDocValues.getNormValues(reader, "foo");
+    for (int i = 0; i < reader.maxDoc(); i++) {
+      assertEquals(0, fooNorms.get(i));
+    }
     
-    byte barNorms[] = (byte[]) MultiDocValues.getNormDocValues(reader, "bar").getSource().getArray();
-    for (int i = 0; i < reader.maxDoc(); i++)
-      assertEquals(1, barNorms[i]);
+    NumericDocValues barNorms = MultiDocValues.getNormValues(reader, "bar");
+    for (int i = 0; i < reader.maxDoc(); i++) {
+      assertEquals(1, barNorms.get(i));
+    }
     
     reader.close();
     dir.close();
@@ -98,99 +98,33 @@
   
   public void testMaxByteNorms() throws IOException {
     Directory dir = newFSDirectory(_TestUtil.getTempDir("TestNorms.testMaxByteNorms"));
-    buildIndex(dir, true);
+    buildIndex(dir);
     AtomicReader open = SlowCompositeReaderWrapper.wrap(DirectoryReader.open(dir));
-    DocValues normValues = open.normValues(byteTestField);
+    NumericDocValues normValues = open.getNormValues(byteTestField);
     assertNotNull(normValues);
-    Source source = normValues.getSource();
-    assertTrue(source.hasArray());
-    assertEquals(Type.FIXED_INTS_8, normValues.getType());
-    byte[] norms = (byte[]) source.getArray();
     for (int i = 0; i < open.maxDoc(); i++) {
       StoredDocument document = open.document(i);
       int expected = Integer.parseInt(document.get(byteTestField));
-      assertEquals((byte)expected, norms[i]);
+      assertEquals(expected, normValues.get(i));
     }
     open.close();
     dir.close();
   }
   
-  /**
-   * this test randomly creates segments with or without norms but not omitting
-   * norms. The similarity used doesn't write a norm value if writeNorms = false is
-   * passed. This differs from omitNorm since norms are simply not written for this segment
-   * while merging fills in default values based on the Norm {@link Type}
-   */
-  public void testNormsNotPresent() throws IOException {
-    Directory dir = newFSDirectory(_TestUtil.getTempDir("TestNorms.testNormsNotPresent.1"));
-    boolean firstWriteNorm = random().nextBoolean();
-    buildIndex(dir, firstWriteNorm);
+  // TODO: create a testNormsNotPresent ourselves by adding/deleting/merging docs
 
-    Directory otherDir = newFSDirectory(_TestUtil.getTempDir("TestNorms.testNormsNotPresent.2"));
-    boolean secondWriteNorm = random().nextBoolean();
-    buildIndex(otherDir, secondWriteNorm);
-
-    AtomicReader reader = SlowCompositeReaderWrapper.wrap(DirectoryReader.open(otherDir));
-    FieldInfos fieldInfos = reader.getFieldInfos();
-    FieldInfo fieldInfo = fieldInfos.fieldInfo(byteTestField);
-    assertFalse(fieldInfo.omitsNorms());
-    assertTrue(fieldInfo.isIndexed());
-    if (secondWriteNorm) {
-      assertTrue(fieldInfo.hasNorms());
-    } else {
-      assertFalse(fieldInfo.hasNorms());  
-    }
-    
-    IndexWriterConfig config = newIndexWriterConfig(TEST_VERSION_CURRENT,
-        new MockAnalyzer(random()));
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, config);
-    writer.addIndexes(reader);
-    AtomicReader mergedReader = SlowCompositeReaderWrapper.wrap(writer.getReader());
-    if (!firstWriteNorm && !secondWriteNorm) {
-      DocValues normValues = mergedReader.normValues(byteTestField);
-      assertNull(normValues);
-      FieldInfo fi = mergedReader.getFieldInfos().fieldInfo(byteTestField);
-      assertFalse(fi.omitsNorms());
-      assertTrue(fi.isIndexed());
-      assertFalse(fi.hasNorms());
-    } else {
-      FieldInfo fi = mergedReader.getFieldInfos().fieldInfo(byteTestField);
-      assertFalse(fi.omitsNorms());
-      assertTrue(fi.isIndexed());
-      assertTrue(fi.hasNorms());
-      
-      DocValues normValues = mergedReader.normValues(byteTestField);
-      assertNotNull(normValues);
-      Source source = normValues.getSource();
-      assertTrue(source.hasArray());
-      assertEquals(Type.FIXED_INTS_8, normValues.getType());
-      byte[] norms = (byte[]) source.getArray();
-      for (int i = 0; i < mergedReader.maxDoc(); i++) {
-        StoredDocument document = mergedReader.document(i);
-        int expected = Integer.parseInt(document.get(byteTestField));
-        assertEquals((byte) expected, norms[i]);
-      }
-    }
-    mergedReader.close();
-    reader.close();
-
-    writer.close();
-    dir.close();
-    otherDir.close();
-  }
-
-  public void buildIndex(Directory dir, boolean writeNorms) throws IOException {
+  public void buildIndex(Directory dir) throws IOException {
     Random random = random();
     IndexWriterConfig config = newIndexWriterConfig(TEST_VERSION_CURRENT,
         new MockAnalyzer(random()));
-    Similarity provider = new MySimProvider(writeNorms);
+    Similarity provider = new MySimProvider();
     config.setSimilarity(provider);
     RandomIndexWriter writer = new RandomIndexWriter(random, dir, config);
     final LineFileDocs docs = new LineFileDocs(random, true);
     int num = atLeast(100);
     for (int i = 0; i < num; i++) {
       Document doc = docs.nextDoc();
-      int boost = writeNorms ? 1 + random().nextInt(255) : 0;
+      int boost = random().nextInt(255);
       Field f = new TextField(byteTestField, "" + boost, Field.Store.YES);
       f.setBoost(boost);
       doc.add(f);
@@ -208,10 +142,7 @@
 
   public class MySimProvider extends PerFieldSimilarityWrapper {
     Similarity delegate = new DefaultSimilarity();
-    private boolean writeNorms;
-    public MySimProvider(boolean writeNorms) {
-      this.writeNorms = writeNorms;
-    }
+
     @Override
     public float queryNorm(float sumOfSquaredWeights) {
 
@@ -221,7 +152,7 @@
     @Override
     public Similarity get(String field) {
       if (byteTestField.equals(field)) {
-        return new ByteEncodingBoostSimilarity(writeNorms);
+        return new ByteEncodingBoostSimilarity();
       } else {
         return delegate;
       }
@@ -236,18 +167,10 @@
   
   public static class ByteEncodingBoostSimilarity extends Similarity {
 
-    private boolean writeNorms;
-
-    public ByteEncodingBoostSimilarity(boolean writeNorms) {
-      this.writeNorms = writeNorms;
-    }
-
     @Override
-    public void computeNorm(FieldInvertState state, Norm norm) {
-      if (writeNorms) {
-        int boost = (int) state.getBoost();
-        norm.setByte((byte) (0xFF & boost));
-      }
+    public long computeNorm(FieldInvertState state) {
+      int boost = (int) state.getBoost();
+      return (0xFF & boost);
     }
 
     @Override
Index: lucene/core/src/test/org/apache/lucene/index/TestMaxTermFrequency.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/index/TestMaxTermFrequency.java	(revision 1442822)
+++ lucene/core/src/test/org/apache/lucene/index/TestMaxTermFrequency.java	(working copy)
@@ -67,9 +67,10 @@
   }
   
   public void test() throws Exception {
-    byte fooNorms[] = (byte[])MultiDocValues.getNormDocValues(reader, "foo").getSource().getArray();
-    for (int i = 0; i < reader.maxDoc(); i++)
-      assertEquals(expected.get(i).intValue(), fooNorms[i] & 0xff);
+    NumericDocValues fooNorms = MultiDocValues.getNormValues(reader, "foo");
+    for (int i = 0; i < reader.maxDoc(); i++) {
+      assertEquals(expected.get(i).intValue(), fooNorms.get(i) & 0xff);
+    }
   }
 
   /**
Index: lucene/core/src/test/org/apache/lucene/index/TestCodecs.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/index/TestCodecs.java	(revision 1442822)
+++ lucene/core/src/test/org/apache/lucene/index/TestCodecs.java	(working copy)
@@ -36,6 +36,7 @@
 import org.apache.lucene.document.FieldType;
 import org.apache.lucene.document.StringField;
 import org.apache.lucene.document.TextField;
+import org.apache.lucene.index.FieldInfo.DocValuesType;
 import org.apache.lucene.index.FieldInfo.IndexOptions;
 import org.apache.lucene.search.DocIdSetIterator;
 import org.apache.lucene.search.IndexSearcher;
@@ -94,8 +95,41 @@
       this.omitTF = omitTF;
       this.storePayloads = storePayloads;
       // TODO: change this test to use all three
-      fieldInfos.addOrUpdate(name, true, false, false, storePayloads, omitTF ? IndexOptions.DOCS_ONLY : IndexOptions.DOCS_AND_FREQS_AND_POSITIONS, null, null);
-      fieldInfo = fieldInfos.fieldInfo(name);
+      fieldInfo = fieldInfos.addOrUpdate(name, new IndexableFieldType() {
+
+        @Override
+        public boolean indexed() { return true; }
+
+        @Override
+        public boolean stored() { return false; }
+
+        @Override
+        public boolean tokenized() { return false; }
+
+        @Override
+        public boolean storeTermVectors() { return false; }
+
+        @Override
+        public boolean storeTermVectorOffsets() { return false; }
+
+        @Override
+        public boolean storeTermVectorPositions() { return false; }
+
+        @Override
+        public boolean storeTermVectorPayloads() { return false; }
+
+        @Override
+        public boolean omitNorms() { return false; }
+
+        @Override
+        public IndexOptions indexOptions() { return omitTF ? IndexOptions.DOCS_ONLY : IndexOptions.DOCS_AND_FREQS_AND_POSITIONS; }
+
+        @Override
+        public DocValuesType docValueType() { return null; }
+      });
+      if (storePayloads) {
+        fieldInfo.setStorePayloads();
+      }
       this.terms = terms;
       for(int i=0;i<terms.length;i++)
         terms[i].field = this;
Index: lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets.java	(revision 1442822)
+++ lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets.java	(working copy)
@@ -298,7 +298,7 @@
       DocsEnum docs = null;
       DocsAndPositionsEnum docsAndPositions = null;
       DocsAndPositionsEnum docsAndPositionsAndOffsets = null;
-      final int docIDToID[] = FieldCache.DEFAULT.getInts(sub, "id", false);
+      final FieldCache.Ints docIDToID = FieldCache.DEFAULT.getInts(sub, "id", false);
       for(String term : terms) {
         //System.out.println("  term=" + term);
         if (termsEnum.seekExact(new BytesRef(term), random().nextBoolean())) {
@@ -307,8 +307,8 @@
           int doc;
           //System.out.println("    doc/freq");
           while((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {
-            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);
-            //System.out.println("      doc=" + docIDToID[doc] + " docID=" + doc + " " + expected.size() + " freq");
+            final List<Token> expected = actualTokens.get(term).get(docIDToID.get(doc));
+            //System.out.println("      doc=" + docIDToID.get(doc) + " docID=" + doc + " " + expected.size() + " freq");
             assertNotNull(expected);
             assertEquals(expected.size(), docs.freq());
           }
@@ -318,8 +318,8 @@
           assertNotNull(docsAndPositions);
           //System.out.println("    doc/freq/pos");
           while((doc = docsAndPositions.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {
-            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);
-            //System.out.println("      doc=" + docIDToID[doc] + " " + expected.size() + " freq");
+            final List<Token> expected = actualTokens.get(term).get(docIDToID.get(doc));
+            //System.out.println("      doc=" + docIDToID.get(doc) + " " + expected.size() + " freq");
             assertNotNull(expected);
             assertEquals(expected.size(), docsAndPositions.freq());
             for(Token token : expected) {
@@ -333,8 +333,8 @@
           assertNotNull(docsAndPositionsAndOffsets);
           //System.out.println("    doc/freq/pos/offs");
           while((doc = docsAndPositionsAndOffsets.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {
-            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);
-            //System.out.println("      doc=" + docIDToID[doc] + " " + expected.size() + " freq");
+            final List<Token> expected = actualTokens.get(term).get(docIDToID.get(doc));
+            //System.out.println("      doc=" + docIDToID.get(doc) + " " + expected.size() + " freq");
             assertNotNull(expected);
             assertEquals(expected.size(), docsAndPositionsAndOffsets.freq());
             for(Token token : expected) {
Index: lucene/core/src/test/org/apache/lucene/index/TestDuelingCodecs.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/index/TestDuelingCodecs.java	(revision 1442822)
+++ lucene/core/src/test/org/apache/lucene/index/TestDuelingCodecs.java	(working copy)
@@ -65,12 +65,9 @@
     // as this gives the best overall coverage. when we have more
     // codecs we should probably pick 2 from Codec.availableCodecs()
     
-    // TODO: it would also be nice to support preflex, but it doesn't
-    // support a lot of the current feature set (docvalues, statistics)
-    // so this would make assertEquals complicated.
-
     leftCodec = Codec.forName("SimpleText");
     rightCodec = new RandomCodec(random());
+
     leftDir = newDirectory();
     rightDir = newDirectory();
 
@@ -110,10 +107,19 @@
   
   @Override
   public void tearDown() throws Exception {
-    leftReader.close();
-    rightReader.close();   
-    leftDir.close();
-    rightDir.close();
+    if (leftReader != null) {
+      leftReader.close();
+    }
+    if (rightReader != null) {
+      rightReader.close();   
+    }
+
+    if (leftDir != null) {
+      leftDir.close();
+    }
+    if (rightDir != null) {
+      rightDir.close();
+    }
     
     super.tearDown();
   }
@@ -536,13 +542,13 @@
     }
     
     for (String field : leftFields) {
-      DocValues leftNorms = MultiDocValues.getNormDocValues(leftReader, field);
-      DocValues rightNorms = MultiDocValues.getNormDocValues(rightReader, field);
+      NumericDocValues leftNorms = MultiDocValues.getNormValues(leftReader, field);
+      NumericDocValues rightNorms = MultiDocValues.getNormValues(rightReader, field);
       if (leftNorms != null && rightNorms != null) {
-        assertDocValues(leftNorms, rightNorms);
+        assertDocValues(leftReader.maxDoc(), leftNorms, rightNorms);
       } else {
-        assertNull(leftNorms);
-        assertNull(rightNorms);
+        assertNull(info, leftNorms);
+        assertNull(info, rightNorms);
       }
     }
   }
@@ -618,68 +624,74 @@
    * checks that docvalues across all fields are equivalent
    */
   public void assertDocValues(IndexReader leftReader, IndexReader rightReader) throws Exception {
-    Set<String> leftValues = getDVFields(leftReader);
-    Set<String> rightValues = getDVFields(rightReader);
-    assertEquals(info, leftValues, rightValues);
+    Set<String> leftFields = getDVFields(leftReader);
+    Set<String> rightFields = getDVFields(rightReader);
+    assertEquals(info, leftFields, rightFields);
 
-    for (String field : leftValues) {
-      DocValues leftDocValues = MultiDocValues.getDocValues(leftReader, field);
-      DocValues rightDocValues = MultiDocValues.getDocValues(rightReader, field);
-      if (leftDocValues != null && rightDocValues != null) {
-        assertDocValues(leftDocValues, rightDocValues);
-      } else {
-        assertNull(leftDocValues);
-        assertNull(rightDocValues);
+    for (String field : leftFields) {
+      // TODO: clean this up... very messy
+      {
+        NumericDocValues leftValues = MultiDocValues.getNumericValues(leftReader, field);
+        NumericDocValues rightValues = MultiDocValues.getNumericValues(rightReader, field);
+        if (leftValues != null && rightValues != null) {
+          assertDocValues(leftReader.maxDoc(), leftValues, rightValues);
+        } else {
+          assertNull(info, leftValues);
+          assertNull(info, rightValues);
+        }
       }
+
+      {
+        BinaryDocValues leftValues = MultiDocValues.getBinaryValues(leftReader, field);
+        BinaryDocValues rightValues = MultiDocValues.getBinaryValues(rightReader, field);
+        if (leftValues != null && rightValues != null) {
+          BytesRef scratchLeft = new BytesRef();
+          BytesRef scratchRight = new BytesRef();
+          for(int docID=0;docID<leftReader.maxDoc();docID++) {
+            leftValues.get(docID, scratchLeft);
+            rightValues.get(docID, scratchRight);
+            assertEquals(info, scratchLeft, scratchRight);
+          }
+        } else {
+          assertNull(info, leftValues);
+          assertNull(info, rightValues);
+        }
+      }
+      
+      {
+        SortedDocValues leftValues = MultiDocValues.getSortedValues(leftReader, field);
+        SortedDocValues rightValues = MultiDocValues.getSortedValues(rightReader, field);
+        if (leftValues != null && rightValues != null) {
+          // numOrds
+          assertEquals(info, leftValues.getValueCount(), rightValues.getValueCount());
+          // ords
+          BytesRef scratchLeft = new BytesRef();
+          BytesRef scratchRight = new BytesRef();
+          for (int i = 0; i < leftValues.getValueCount(); i++) {
+            leftValues.lookupOrd(i, scratchLeft);
+            rightValues.lookupOrd(i, scratchRight);
+            assertEquals(info, scratchLeft, scratchRight);
+          }
+          // bytes
+          for(int docID=0;docID<leftReader.maxDoc();docID++) {
+            leftValues.get(docID, scratchLeft);
+            rightValues.get(docID, scratchRight);
+            assertEquals(info, scratchLeft, scratchRight);
+          }
+        } else {
+          assertNull(info, leftValues);
+          assertNull(info, rightValues);
+        }
+      }
     }
   }
   
-  public void assertDocValues(DocValues leftDocValues, DocValues rightDocValues) throws Exception {
+  public void assertDocValues(int num, NumericDocValues leftDocValues, NumericDocValues rightDocValues) throws Exception {
     assertNotNull(info, leftDocValues);
     assertNotNull(info, rightDocValues);
-    assertEquals(info, leftDocValues.getType(), rightDocValues.getType());
-    assertEquals(info, leftDocValues.getValueSize(), rightDocValues.getValueSize());
-    assertDocValuesSource(leftDocValues.getDirectSource(), rightDocValues.getDirectSource());
-    assertDocValuesSource(leftDocValues.getSource(), rightDocValues.getSource());
-  }
-  
-  /**
-   * checks source API
-   */
-  public void assertDocValuesSource(DocValues.Source left, DocValues.Source right) throws Exception {
-    DocValues.Type leftType = left.getType();
-    assertEquals(info, leftType, right.getType());
-    switch(leftType) {
-      case VAR_INTS:
-      case FIXED_INTS_8:
-      case FIXED_INTS_16:
-      case FIXED_INTS_32:
-      case FIXED_INTS_64:
-        for (int i = 0; i < leftReader.maxDoc(); i++) {
-          assertEquals(info, left.getInt(i), right.getInt(i));
-        }
-        break;
-      case FLOAT_32:
-      case FLOAT_64:
-        for (int i = 0; i < leftReader.maxDoc(); i++) {
-          assertEquals(info, left.getFloat(i), right.getFloat(i), 0F);
-        }
-        break;
-      case BYTES_FIXED_STRAIGHT:
-      case BYTES_FIXED_DEREF:
-      case BYTES_VAR_STRAIGHT:
-      case BYTES_VAR_DEREF:
-        BytesRef b1 = new BytesRef();
-        BytesRef b2 = new BytesRef();
-        for (int i = 0; i < leftReader.maxDoc(); i++) {
-          left.getBytes(i, b1);
-          right.getBytes(i, b2);
-          assertEquals(info, b1, b2);
-        }
-        break;
-      // TODO: can we test these?
-      case BYTES_VAR_SORTED:
-      case BYTES_FIXED_SORTED:
+    for(int docID=0;docID<num;docID++) {
+      assertEquals(leftDocValues.get(docID),
+                   rightDocValues.get(docID));
     }
   }
   
Index: lucene/core/src/test/org/apache/lucene/index/TestIndexableField.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/index/TestIndexableField.java	(revision 1442822)
+++ lucene/core/src/test/org/apache/lucene/index/TestIndexableField.java	(working copy)
@@ -25,7 +25,7 @@
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.document.Field;
-import org.apache.lucene.index.DocValues.Type;
+import org.apache.lucene.index.FieldInfo.DocValuesType;
 import org.apache.lucene.search.BooleanClause;
 import org.apache.lucene.search.BooleanQuery;
 import org.apache.lucene.search.DocIdSetIterator;
@@ -89,7 +89,7 @@
       }
 
       @Override
-      public Type docValueType() {
+      public DocValuesType docValueType() {
         return null;
       }
     };
Index: lucene/core/src/test/org/apache/lucene/index/TestDocumentWriter.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/index/TestDocumentWriter.java	(revision 1442822)
+++ lucene/core/src/test/org/apache/lucene/index/TestDocumentWriter.java	(working copy)
@@ -97,7 +97,7 @@
     // omitNorms is true
     for (FieldInfo fi : reader.getFieldInfos()) {
       if (fi.isIndexed()) {
-        assertTrue(fi.omitsNorms() == (reader.normValues(fi.name) == null));
+        assertTrue(fi.omitsNorms() == (reader.getNormValues(fi.name) == null));
       }
     }
     reader.close();
Index: lucene/core/src/test/org/apache/lucene/index/TestDocValuesFormat.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/index/TestDocValuesFormat.java	(revision 0)
+++ lucene/core/src/test/org/apache/lucene/index/TestDocValuesFormat.java	(working copy)
@@ -0,0 +1,31 @@
+package org.apache.lucene.index;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.codecs.Codec;
+
+/** Tests the codec configuration defined by LuceneTestCase randomly
+ *  (typically a mix across different fields).
+ */
+public class TestDocValuesFormat extends BaseDocValuesFormatTestCase {
+
+  @Override
+  protected Codec getCodec() {
+    return Codec.getDefault();
+  }
+}

Property changes on: lucene/core/src/test/org/apache/lucene/index/TestDocValuesFormat.java
___________________________________________________________________
Added: svn:eol-style
## -0,0 +1 ##
+native
Index: lucene/core/src/test/org/apache/lucene/index/TestFieldsReader.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/index/TestFieldsReader.java	(revision 1442822)
+++ lucene/core/src/test/org/apache/lucene/index/TestFieldsReader.java	(working copy)
@@ -48,11 +48,12 @@
 
 public class TestFieldsReader extends LuceneTestCase {
   private static Directory dir;
-  private static Document testDoc = new Document();
+  private static Document testDoc;
   private static FieldInfos.Builder fieldInfos = null;
 
   @BeforeClass
   public static void beforeClass() throws Exception {
+    testDoc = new Document();
     fieldInfos = new FieldInfos.Builder();
     DocHelper.setupDoc(testDoc);
     for (IndexableField field : testDoc.getFields()) {
@@ -291,12 +292,12 @@
 
     for(AtomicReaderContext ctx : r.leaves()) {
       final AtomicReader sub = ctx.reader();
-      final int[] ids = FieldCache.DEFAULT.getInts(sub, "id", false);
+      final FieldCache.Ints ids = FieldCache.DEFAULT.getInts(sub, "id", false);
       for(int docID=0;docID<sub.numDocs();docID++) {
         final StoredDocument doc = sub.document(docID);
         final Field f = (Field) doc.getField("nf");
         assertTrue("got f=" + f, f instanceof StoredField);
-        assertEquals(answers[ids[docID]], f.numericValue());
+        assertEquals(answers[ids.get(docID)], f.numericValue());
       }
     }
     r.close();
Index: lucene/core/src/test/org/apache/lucene/index/TestDocValuesTypeCompatibility.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/index/TestDocValuesTypeCompatibility.java	(revision 1442822)
+++ lucene/core/src/test/org/apache/lucene/index/TestDocValuesTypeCompatibility.java	(working copy)
@@ -1,313 +0,0 @@
-package org.apache.lucene.index;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.analysis.MockAnalyzer;
-import org.apache.lucene.document.ByteDocValuesField;
-import org.apache.lucene.document.DerefBytesDocValuesField;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.document.DoubleDocValuesField;
-import org.apache.lucene.document.Field;
-import org.apache.lucene.document.FloatDocValuesField;
-import org.apache.lucene.document.IntDocValuesField;
-import org.apache.lucene.document.LongDocValuesField;
-import org.apache.lucene.document.PackedLongDocValuesField;
-import org.apache.lucene.document.ShortDocValuesField;
-import org.apache.lucene.document.SortedBytesDocValuesField;
-import org.apache.lucene.document.StraightBytesDocValuesField;
-import org.apache.lucene.index.DocValues.Type;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.LuceneTestCase;
-
-/**
- * Tests compatibility of {@link DocValues.Type} during indexing
- */
-public class TestDocValuesTypeCompatibility extends LuceneTestCase {
-  
-  public void testAddCompatibleIntTypes() throws IOException {
-    int numIter = atLeast(10);
-    for (int i = 0; i < numIter; i++) {
-      Directory dir = newDirectory();
-      IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT,
-          new MockAnalyzer(random()));
-      int numDocs = atLeast(100);
-      
-      iwc.setMaxBufferedDocs(2 * numDocs); // make sure we hit the same DWPT
-                                           // here
-      iwc.setRAMBufferSizeMB(IndexWriterConfig.DISABLE_AUTO_FLUSH);
-      iwc.setRAMPerThreadHardLimitMB(2000);
-      IndexWriter writer = new IndexWriter(dir, iwc);
-      Type[] types = new Type[] {Type.VAR_INTS, Type.FIXED_INTS_16,
-          Type.FIXED_INTS_64, Type.FIXED_INTS_16, Type.FIXED_INTS_8};
-      Type maxType = types[random().nextInt(types.length)];
-      for (int j = 0; j < numDocs; j++) {
-        addDoc(writer, getRandomIntsField(maxType, j == 0));
-      }
-      writer.close();
-      dir.close();
-    }
-    
-  }
-  
-  @SuppressWarnings("fallthrough")
-  public Field getRandomIntsField(Type maxType, boolean force) {
-    switch (maxType) {
-    
-      case VAR_INTS:
-        if (random().nextInt(5) == 0 || force) {
-          return new PackedLongDocValuesField("f", 1);
-        }
-      case FIXED_INTS_64:
-        if (random().nextInt(4) == 0 || force) {
-          return new LongDocValuesField("f", 1);
-        }
-      case FIXED_INTS_32:
-        if (random().nextInt(3) == 0 || force) {
-          return new IntDocValuesField("f", 1);
-        }
-      case FIXED_INTS_16:
-        if (random().nextInt(2) == 0 || force) {
-          return new ShortDocValuesField("f", (short) 1);
-        }
-      case FIXED_INTS_8:
-        return new ByteDocValuesField("f", (byte) 1);
-        
-      default:
-        throw new IllegalArgumentException();
-        
-    }
-  }
-  
-  public void testAddCompatibleDoubleTypes() throws IOException {
-    int numIter = atLeast(10);
-    for (int i = 0; i < numIter; i++) {
-      Directory dir = newDirectory();
-      IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT,
-          new MockAnalyzer(random()));
-      int numDocs = atLeast(100);
-      
-      iwc.setMaxBufferedDocs(2 * numDocs); // make sure we hit the same DWPT
-                                           // here
-      iwc.setRAMBufferSizeMB(IndexWriterConfig.DISABLE_AUTO_FLUSH);
-      iwc.setRAMPerThreadHardLimitMB(2000);
-      IndexWriter writer = new IndexWriter(dir, iwc);
-      
-      Type[] types = new Type[] {Type.FLOAT_64, Type.FLOAT_32};
-      Type maxType = types[random().nextInt(types.length)];
-      for (int j = 0; j < numDocs; j++) {
-        addDoc(writer, getRandomFloatField(maxType, j == 0));
-      }
-      writer.close();
-      dir.close();
-    }
-    
-  }
-  @SuppressWarnings("fallthrough")
-  public Field getRandomFloatField(Type maxType, boolean force) {
-    switch (maxType) {
-    
-      case FLOAT_64:
-        if (random().nextInt(5) == 0 || force) {
-          return new PackedLongDocValuesField("f", 1);
-        }
-      case FIXED_INTS_32:
-        if (random().nextInt(4) == 0 || force) {
-          return new LongDocValuesField("f", 1);
-        }
-      case FLOAT_32:
-        if (random().nextInt(3) == 0 || force) {
-          return new IntDocValuesField("f", 1);
-        }
-      case FIXED_INTS_16:
-        if (random().nextInt(2) == 0 || force) {
-          return new ShortDocValuesField("f", (short) 1);
-        }
-      case FIXED_INTS_8:
-        return new ByteDocValuesField("f", (byte) 1);
-        
-      default:
-        throw new IllegalArgumentException();
-        
-    }
-  }
-  
-  public void testAddCompatibleDoubleTypes2() throws IOException {
-    int numIter = atLeast(10);
-    for (int i = 0; i < numIter; i++) {
-      Directory dir = newDirectory();
-      IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT,
-          new MockAnalyzer(random()));
-      int numDocs = atLeast(100);
-      
-      iwc.setMaxBufferedDocs(2 * numDocs); // make sure we hit the same DWPT
-                                           // here
-      iwc.setRAMBufferSizeMB(IndexWriterConfig.DISABLE_AUTO_FLUSH);
-      iwc.setRAMPerThreadHardLimitMB(2000);
-      IndexWriter writer = new IndexWriter(dir, iwc);
-      Field[] fields = new Field[] {
-          new DoubleDocValuesField("f", 1.0), new IntDocValuesField("f", 1),
-          new ShortDocValuesField("f", (short) 1),
-          new ByteDocValuesField("f", (byte) 1)};
-      int base = random().nextInt(fields.length - 1);
-      
-      addDoc(writer, fields[base]);
-      
-      for (int j = 0; j < numDocs; j++) {
-        int f = base + random().nextInt(fields.length - base);
-        addDoc(writer, fields[f]);
-      }
-      writer.close();
-      dir.close();
-    }
-    
-  }
-  
-  public void testAddCompatibleByteTypes() throws IOException {
-    int numIter = atLeast(10);
-    for (int i = 0; i < numIter; i++) {
-      Directory dir = newDirectory();
-      IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT,
-          new MockAnalyzer(random()));
-      int numDocs = atLeast(100);
-      
-      iwc.setMaxBufferedDocs(2 * numDocs); // make sure we hit the same DWPT
-                                           // here
-      iwc.setRAMBufferSizeMB(IndexWriterConfig.DISABLE_AUTO_FLUSH);
-      iwc.setRAMPerThreadHardLimitMB(2000);
-      IndexWriter writer = new IndexWriter(dir, iwc);
-      boolean mustBeFixed = random().nextBoolean();
-      int maxSize = 2 + random().nextInt(15);
-      Field bytesField = getRandomBytesField(mustBeFixed, maxSize,
-          true);
-      addDoc(writer, bytesField);
-      for (int j = 0; j < numDocs; j++) {
-        bytesField = getRandomBytesField(mustBeFixed, maxSize, false);
-        addDoc(writer, bytesField);
-        
-      }
-      writer.close();
-      dir.close();
-    }
-  }
-  
-  public Field getRandomBytesField(boolean mustBeFixed, int maxSize,
-      boolean mustBeVariableIfNotFixed) {
-    int size = mustBeFixed ? maxSize : random().nextInt(maxSize) + 1;
-    StringBuilder s = new StringBuilder();
-    for (int i = 0; i < size; i++) {
-      s.append("a");
-    }
-    BytesRef bytesRef = new BytesRef(s);
-    boolean fixed = mustBeFixed ? true : mustBeVariableIfNotFixed ? false
-        : random().nextBoolean();
-    switch (random().nextInt(3)) {
-      case 0:
-        return new SortedBytesDocValuesField("f", bytesRef, fixed);
-      case 1:
-        return new DerefBytesDocValuesField("f", bytesRef, fixed);
-      default:
-        return new StraightBytesDocValuesField("f", bytesRef, fixed);
-    }
-  }
-  
-  public void testIncompatibleTypesBytes() throws IOException {
-    Directory dir = newDirectory();
-    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT,
-        new MockAnalyzer(random()));
-    int numDocs = atLeast(100);
-
-    iwc.setMaxBufferedDocs(numDocs); // make sure we hit the same DWPT
-    iwc.setRAMBufferSizeMB(IndexWriterConfig.DISABLE_AUTO_FLUSH);
-    iwc.setRAMPerThreadHardLimitMB(2000);
-    IndexWriter writer = new IndexWriter(dir, iwc);
-    
-    int numDocsIndexed = 0;
-    for (int j = 1; j < numDocs; j++) {
-      try {
-        addDoc(writer, getRandomIndexableDVField());
-        numDocsIndexed++;
-      } catch (IllegalArgumentException e) {
-        assertTrue(e.getMessage().startsWith("Incompatible DocValues type:"));
-      }
-    }
-    
-    writer.commit();
-    DirectoryReader open = DirectoryReader.open(dir);
-    assertEquals(numDocsIndexed, open.numDocs());
-    open.close();
-    writer.close();
-    dir.close();
-  }
-  
-  private void addDoc(IndexWriter writer, Field... fields)
-      throws IOException {
-    Document doc = new Document();
-    for (Field indexableField : fields) {
-      doc.add(indexableField);
-    }
-    writer.addDocument(doc);
-  }
-  
-  public Field getRandomIndexableDVField() {
-    int size = random().nextInt(100) + 1;
-    StringBuilder s = new StringBuilder();
-    for (int i = 0; i < size; i++) {
-      s.append("a");
-    }
-    BytesRef bytesRef = new BytesRef(s);
-    
-    Type[] values = Type.values();
-    Type t = values[random().nextInt(values.length)];
-    switch (t) {
-      case BYTES_FIXED_DEREF:
-        return new DerefBytesDocValuesField("f", bytesRef, true);
-      case BYTES_FIXED_SORTED:
-        return new SortedBytesDocValuesField("f", bytesRef, true);
-      case BYTES_FIXED_STRAIGHT:
-        return new StraightBytesDocValuesField("f", bytesRef, true);
-      case BYTES_VAR_DEREF:
-        return new DerefBytesDocValuesField("f", bytesRef, false);
-      case BYTES_VAR_SORTED:
-        return new SortedBytesDocValuesField("f", bytesRef, false);
-      case BYTES_VAR_STRAIGHT:
-        return new StraightBytesDocValuesField("f", bytesRef, false);
-      case FIXED_INTS_16:
-        return new ShortDocValuesField("f", (short) 1);
-      case FIXED_INTS_32:
-        return new IntDocValuesField("f", 1);
-      case FIXED_INTS_64:
-        return new LongDocValuesField("f", 1);
-      case FIXED_INTS_8:
-        return new ByteDocValuesField("f", (byte) 1);
-      case FLOAT_32:
-        return new FloatDocValuesField("f", 1.0f);
-      case FLOAT_64:
-        return new DoubleDocValuesField("f", 1.0f);
-      case VAR_INTS:
-        return new PackedLongDocValuesField("f", 1);
-      default:
-        throw new IllegalArgumentException();
-        
-    }
-    
-  }
-  
-}
Index: lucene/core/src/test/org/apache/lucene/index/Test2BSortedDocValues.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/index/Test2BSortedDocValues.java	(revision 0)
+++ lucene/core/src/test/org/apache/lucene/index/Test2BSortedDocValues.java	(working copy)
@@ -0,0 +1,153 @@
+package org.apache.lucene.index;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.util.Random;
+
+import org.apache.lucene.analysis.MockAnalyzer;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.SortedDocValuesField;
+import org.apache.lucene.store.BaseDirectoryWrapper;
+import org.apache.lucene.store.MockDirectoryWrapper;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.LuceneTestCase;
+import org.apache.lucene.util.TimeUnits;
+import org.apache.lucene.util._TestUtil;
+import org.junit.Ignore;
+
+import com.carrotsearch.randomizedtesting.annotations.TimeoutSuite;
+
+@TimeoutSuite(millis = 80 * TimeUnits.HOUR)
+@Ignore("very slow")
+public class Test2BSortedDocValues extends LuceneTestCase {
+  
+  // indexes Integer.MAX_VALUE docs with a fixed binary field
+  public void testFixedSorted() throws Exception {
+    BaseDirectoryWrapper dir = newFSDirectory(_TestUtil.getTempDir("2BFixedSorted"));
+    if (dir instanceof MockDirectoryWrapper) {
+      ((MockDirectoryWrapper)dir).setThrottling(MockDirectoryWrapper.Throttling.NEVER);
+    }
+    
+    IndexWriter w = new IndexWriter(dir,
+        new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()))
+        .setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH)
+        .setRAMBufferSizeMB(256.0)
+        .setMergeScheduler(new ConcurrentMergeScheduler())
+        .setMergePolicy(newLogMergePolicy(false, 10))
+        .setOpenMode(IndexWriterConfig.OpenMode.CREATE));
+
+    Document doc = new Document();
+    byte bytes[] = new byte[2];
+    BytesRef data = new BytesRef(bytes);
+    SortedDocValuesField dvField = new SortedDocValuesField("dv", data);
+    doc.add(dvField);
+    
+    for (int i = 0; i < Integer.MAX_VALUE; i++) {
+      bytes[0] = (byte)(i >> 8);
+      bytes[1] = (byte) i;
+      w.addDocument(doc);
+      if (i % 100000 == 0) {
+        System.out.println("indexed: " + i);
+        System.out.flush();
+      }
+    }
+    
+    w.forceMerge(1);
+    w.close();
+    
+    System.out.println("verifying...");
+    System.out.flush();
+    
+    DirectoryReader r = DirectoryReader.open(dir);
+    int expectedValue = 0;
+    for (AtomicReaderContext context : r.leaves()) {
+      AtomicReader reader = context.reader();
+      BytesRef scratch = new BytesRef();
+      BinaryDocValues dv = reader.getSortedDocValues("dv");
+      for (int i = 0; i < reader.maxDoc(); i++) {
+        bytes[0] = (byte)(expectedValue >> 8);
+        bytes[1] = (byte) expectedValue;
+        dv.get(i, scratch);
+        assertEquals(data, scratch);
+        expectedValue++;
+      }
+    }
+    
+    r.close();
+    dir.close();
+  }
+  
+  // indexes Integer.MAX_VALUE docs with a fixed binary field
+  // TODO: must use random.nextBytes (like Test2BTerms) to avoid BytesRefHash probing issues
+  public void test2BOrds() throws Exception {
+    BaseDirectoryWrapper dir = newFSDirectory(_TestUtil.getTempDir("2BOrds"));
+    if (dir instanceof MockDirectoryWrapper) {
+      ((MockDirectoryWrapper)dir).setThrottling(MockDirectoryWrapper.Throttling.NEVER);
+    }
+    
+    IndexWriter w = new IndexWriter(dir,
+        new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()))
+        .setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH)
+        .setRAMBufferSizeMB(256.0)
+        .setMergeScheduler(new ConcurrentMergeScheduler())
+        .setMergePolicy(newLogMergePolicy(false, 10))
+        .setOpenMode(IndexWriterConfig.OpenMode.CREATE));
+
+    Document doc = new Document();
+    byte bytes[] = new byte[4];
+    BytesRef data = new BytesRef(bytes);
+    SortedDocValuesField dvField = new SortedDocValuesField("dv", data);
+    doc.add(dvField);
+    
+    long seed = random().nextLong();
+    Random random = new Random(seed);
+    
+    for (int i = 0; i < Integer.MAX_VALUE; i++) {
+      random.nextBytes(bytes);
+      w.addDocument(doc);
+      if (i % 100000 == 0) {
+        System.out.println("indexed: " + i);
+        System.out.flush();
+      }
+    }
+    
+    w.forceMerge(1);
+    w.close();
+    
+    System.out.println("verifying...");
+    System.out.flush();
+    
+    DirectoryReader r = DirectoryReader.open(dir);
+    random.setSeed(seed);
+    for (AtomicReaderContext context : r.leaves()) {
+      AtomicReader reader = context.reader();
+      BytesRef scratch = new BytesRef();
+      BinaryDocValues dv = reader.getSortedDocValues("dv");
+      for (int i = 0; i < reader.maxDoc(); i++) {
+        random.nextBytes(bytes);
+        dv.get(i, scratch);
+        assertEquals(data, scratch);
+      }
+    }
+    
+    r.close();
+    dir.close();
+  }
+  
+  // TODO: variable
+}

Property changes on: lucene/core/src/test/org/apache/lucene/index/Test2BSortedDocValues.java
___________________________________________________________________
Added: svn:eol-style
## -0,0 +1 ##
+native
Index: lucene/core/src/test/org/apache/lucene/index/TestBackwardsCompatibility.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/index/TestBackwardsCompatibility.java	(revision 1442822)
+++ lucene/core/src/test/org/apache/lucene/index/TestBackwardsCompatibility.java	(working copy)
@@ -30,24 +30,18 @@
 import java.util.Random;
 
 import org.apache.lucene.analysis.MockAnalyzer;
-import org.apache.lucene.document.ByteDocValuesField;
-import org.apache.lucene.document.DerefBytesDocValuesField;
+import org.apache.lucene.document.BinaryDocValuesField;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.DoubleDocValuesField;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.FieldType;
 import org.apache.lucene.document.FloatDocValuesField;
-import org.apache.lucene.document.IntDocValuesField;
 import org.apache.lucene.document.IntField;
-import org.apache.lucene.document.LongDocValuesField;
+import org.apache.lucene.document.NumericDocValuesField;
 import org.apache.lucene.document.LongField;
-import org.apache.lucene.document.PackedLongDocValuesField;
-import org.apache.lucene.document.ShortDocValuesField;
-import org.apache.lucene.document.SortedBytesDocValuesField;
-import org.apache.lucene.document.StraightBytesDocValuesField;
+import org.apache.lucene.document.SortedDocValuesField;
 import org.apache.lucene.document.StringField;
 import org.apache.lucene.document.TextField;
-import org.apache.lucene.index.DocValues.Source;
 import org.apache.lucene.index.FieldInfo.IndexOptions;
 import org.apache.lucene.index.IndexWriterConfig.OpenMode;
 import org.apache.lucene.search.DocIdSetIterator;
@@ -62,12 +56,13 @@
 import org.apache.lucene.util.Bits;
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.Constants;
+import org.apache.lucene.util.LuceneTestCase.SuppressCodecs;
 import org.apache.lucene.util.LuceneTestCase;
 import org.apache.lucene.util.StringHelper;
 import org.apache.lucene.util._TestUtil;
-import org.apache.lucene.util.LuceneTestCase.SuppressCodecs;
 import org.junit.AfterClass;
 import org.junit.BeforeClass;
+import org.junit.Ignore;
 
 /*
   Verify we can read the pre-5.0 file format, do searches
@@ -403,26 +398,26 @@
         assertEquals(7, i);
       }
     }
-    
+
     if (is40Index) {
       // check docvalues fields
-      Source dvByte = MultiDocValues.getDocValues(reader, "dvByte").getSource();
-      Source dvBytesDerefFixed = MultiDocValues.getDocValues(reader, "dvBytesDerefFixed").getSource();
-      Source dvBytesDerefVar = MultiDocValues.getDocValues(reader, "dvBytesDerefVar").getSource();
-      Source dvBytesSortedFixed = MultiDocValues.getDocValues(reader, "dvBytesSortedFixed").getSource();
-      Source dvBytesSortedVar = MultiDocValues.getDocValues(reader, "dvBytesSortedVar").getSource();
-      Source dvBytesStraightFixed = MultiDocValues.getDocValues(reader, "dvBytesStraightFixed").getSource();
-      Source dvBytesStraightVar = MultiDocValues.getDocValues(reader, "dvBytesStraightVar").getSource();
-      Source dvDouble = MultiDocValues.getDocValues(reader, "dvDouble").getSource();
-      Source dvFloat = MultiDocValues.getDocValues(reader, "dvFloat").getSource();
-      Source dvInt = MultiDocValues.getDocValues(reader, "dvInt").getSource();
-      Source dvLong = MultiDocValues.getDocValues(reader, "dvLong").getSource();
-      Source dvPacked = MultiDocValues.getDocValues(reader, "dvPacked").getSource();
-      Source dvShort = MultiDocValues.getDocValues(reader, "dvShort").getSource();
+      NumericDocValues dvByte = MultiDocValues.getNumericValues(reader, "dvByte");
+      BinaryDocValues dvBytesDerefFixed = MultiDocValues.getBinaryValues(reader, "dvBytesDerefFixed");
+      BinaryDocValues dvBytesDerefVar = MultiDocValues.getBinaryValues(reader, "dvBytesDerefVar");
+      SortedDocValues dvBytesSortedFixed = MultiDocValues.getSortedValues(reader, "dvBytesSortedFixed");
+      SortedDocValues dvBytesSortedVar = MultiDocValues.getSortedValues(reader, "dvBytesSortedVar");
+      BinaryDocValues dvBytesStraightFixed = MultiDocValues.getBinaryValues(reader, "dvBytesStraightFixed");
+      BinaryDocValues dvBytesStraightVar = MultiDocValues.getBinaryValues(reader, "dvBytesStraightVar");
+      NumericDocValues dvDouble = MultiDocValues.getNumericValues(reader, "dvDouble");
+      NumericDocValues dvFloat = MultiDocValues.getNumericValues(reader, "dvFloat");
+      NumericDocValues dvInt = MultiDocValues.getNumericValues(reader, "dvInt");
+      NumericDocValues dvLong = MultiDocValues.getNumericValues(reader, "dvLong");
+      NumericDocValues dvPacked = MultiDocValues.getNumericValues(reader, "dvPacked");
+      NumericDocValues dvShort = MultiDocValues.getNumericValues(reader, "dvShort");
       
       for (int i=0;i<35;i++) {
         int id = Integer.parseInt(reader.document(i).get("id"));
-        assertEquals((byte)id, dvByte.getInt(i));
+        assertEquals(id, dvByte.get(i));
         
         byte bytes[] = new byte[] {
             (byte)(id >>> 24), (byte)(id >>> 16),(byte)(id >>> 8),(byte)id
@@ -430,19 +425,25 @@
         BytesRef expectedRef = new BytesRef(bytes);
         BytesRef scratch = new BytesRef();
         
-        assertEquals(expectedRef, dvBytesDerefFixed.getBytes(i, scratch));
-        assertEquals(expectedRef, dvBytesDerefVar.getBytes(i, scratch));
-        assertEquals(expectedRef, dvBytesSortedFixed.getBytes(i, scratch));
-        assertEquals(expectedRef, dvBytesSortedVar.getBytes(i, scratch));
-        assertEquals(expectedRef, dvBytesStraightFixed.getBytes(i, scratch));
-        assertEquals(expectedRef, dvBytesStraightVar.getBytes(i, scratch));
+        dvBytesDerefFixed.get(i, scratch);
+        assertEquals(expectedRef, scratch);
+        dvBytesDerefVar.get(i, scratch);
+        assertEquals(expectedRef, scratch);
+        dvBytesSortedFixed.get(i, scratch);
+        assertEquals(expectedRef, scratch);
+        dvBytesSortedVar.get(i, scratch);
+        assertEquals(expectedRef, scratch);
+        dvBytesStraightFixed.get(i, scratch);
+        assertEquals(expectedRef, scratch);
+        dvBytesStraightVar.get(i, scratch);
+        assertEquals(expectedRef, scratch);
         
-        assertEquals((double)id, dvDouble.getFloat(i), 0D);
-        assertEquals((float)id, dvFloat.getFloat(i), 0F);
-        assertEquals(id, dvInt.getInt(i));
-        assertEquals(id, dvLong.getInt(i));
-        assertEquals(id, dvPacked.getInt(i));
-        assertEquals(id, dvShort.getInt(i));
+        assertEquals((double)id, Double.longBitsToDouble(dvDouble.get(i)), 0D);
+        assertEquals((float)id, Float.intBitsToFloat((int)dvFloat.get(i)), 0F);
+        assertEquals(id, dvInt.get(i));
+        assertEquals(id, dvLong.get(i));
+        assertEquals(id, dvPacked.get(i));
+        assertEquals(id, dvShort.get(i));
       }
     }
     
@@ -689,23 +690,23 @@
     doc.add(new IntField("trieInt", id, Field.Store.NO));
     doc.add(new LongField("trieLong", (long) id, Field.Store.NO));
     // add docvalues fields
-    doc.add(new ByteDocValuesField("dvByte", (byte) id));
+    doc.add(new NumericDocValuesField("dvByte", (byte) id));
     byte bytes[] = new byte[] {
       (byte)(id >>> 24), (byte)(id >>> 16),(byte)(id >>> 8),(byte)id
     };
     BytesRef ref = new BytesRef(bytes);
-    doc.add(new DerefBytesDocValuesField("dvBytesDerefFixed", ref, true));
-    doc.add(new DerefBytesDocValuesField("dvBytesDerefVar", ref, false));
-    doc.add(new SortedBytesDocValuesField("dvBytesSortedFixed", ref, true));
-    doc.add(new SortedBytesDocValuesField("dvBytesSortedVar", ref, false));
-    doc.add(new StraightBytesDocValuesField("dvBytesStraightFixed", ref, true));
-    doc.add(new StraightBytesDocValuesField("dvBytesStraightVar", ref, false));
+    doc.add(new BinaryDocValuesField("dvBytesDerefFixed", ref));
+    doc.add(new BinaryDocValuesField("dvBytesDerefVar", ref));
+    doc.add(new SortedDocValuesField("dvBytesSortedFixed", ref));
+    doc.add(new SortedDocValuesField("dvBytesSortedVar", ref));
+    doc.add(new BinaryDocValuesField("dvBytesStraightFixed", ref));
+    doc.add(new BinaryDocValuesField("dvBytesStraightVar", ref));
     doc.add(new DoubleDocValuesField("dvDouble", (double)id));
     doc.add(new FloatDocValuesField("dvFloat", (float)id));
-    doc.add(new IntDocValuesField("dvInt", id));
-    doc.add(new LongDocValuesField("dvLong", id));
-    doc.add(new PackedLongDocValuesField("dvPacked", id));
-    doc.add(new ShortDocValuesField("dvShort", (short)id));
+    doc.add(new NumericDocValuesField("dvInt", id));
+    doc.add(new NumericDocValuesField("dvLong", id));
+    doc.add(new NumericDocValuesField("dvPacked", id));
+    doc.add(new NumericDocValuesField("dvShort", (short)id));
     // a field with both offsets and term vectors for a cross-check
     FieldType customType3 = new FieldType(TextField.TYPE_STORED);
     customType3.setStoreTermVectors(true);
@@ -851,13 +852,16 @@
       assertEquals("wrong number of hits", 34, hits.length);
       
       // check decoding into field cache
-      int[] fci = FieldCache.DEFAULT.getInts(SlowCompositeReaderWrapper.wrap(searcher.getIndexReader()), "trieInt", false);
-      for (int val : fci) {
+      FieldCache.Ints fci = FieldCache.DEFAULT.getInts(SlowCompositeReaderWrapper.wrap(searcher.getIndexReader()), "trieInt", false);
+      int maxDoc = searcher.getIndexReader().maxDoc();
+      for(int doc=0;doc<maxDoc;doc++) {
+        int val = fci.get(doc);
         assertTrue("value in id bounds", val >= 0 && val < 35);
       }
       
-      long[] fcl = FieldCache.DEFAULT.getLongs(SlowCompositeReaderWrapper.wrap(searcher.getIndexReader()), "trieLong", false);
-      for (long val : fcl) {
+      FieldCache.Longs fcl = FieldCache.DEFAULT.getLongs(SlowCompositeReaderWrapper.wrap(searcher.getIndexReader()), "trieLong", false);
+      for(int doc=0;doc<maxDoc;doc++) {
+        long val = fcl.get(doc);
         assertTrue("value in id bounds", val >= 0L && val < 35L);
       }
       
Index: lucene/core/src/test/org/apache/lucene/index/TestUniqueTermCount.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/index/TestUniqueTermCount.java	(revision 1442822)
+++ lucene/core/src/test/org/apache/lucene/index/TestUniqueTermCount.java	(working copy)
@@ -27,7 +27,6 @@
 import org.apache.lucene.document.Field;
 import org.apache.lucene.search.CollectionStatistics;
 import org.apache.lucene.search.TermStatistics;
-import org.apache.lucene.search.similarities.DefaultSimilarity;
 import org.apache.lucene.search.similarities.Similarity;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.util.LuceneTestCase;
@@ -46,8 +45,9 @@
   public void setUp() throws Exception {
     super.setUp();
     dir = newDirectory();
-    IndexWriterConfig config = newIndexWriterConfig(TEST_VERSION_CURRENT, 
-                                                    new MockAnalyzer(random(), MockTokenizer.SIMPLE, true)).setMergePolicy(newLogMergePolicy());
+    MockAnalyzer analyzer = new MockAnalyzer(random(), MockTokenizer.SIMPLE, true);
+    IndexWriterConfig config = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);
+    config.setMergePolicy(newLogMergePolicy());
     config.setSimilarity(new TestSimilarity());
     RandomIndexWriter writer = new RandomIndexWriter(random(), dir, config);
     Document doc = new Document();
@@ -69,9 +69,11 @@
   }
   
   public void test() throws Exception {
-    byte fooNorms[] = (byte[])MultiDocValues.getNormDocValues(reader, "foo").getSource().getArray();
-    for (int i = 0; i < reader.maxDoc(); i++)
-      assertEquals(expected.get(i).intValue(), fooNorms[i] & 0xff);
+    NumericDocValues fooNorms = MultiDocValues.getNormValues(reader, "foo");
+    assertNotNull(fooNorms);
+    for (int i = 0; i < reader.maxDoc(); i++) {
+      assertEquals(expected.get(i).longValue(), fooNorms.get(i));
+    }
   }
 
   /**
@@ -93,13 +95,13 @@
   }
   
   /**
-   * Simple similarity that encodes maxTermFrequency directly as a byte
+   * Simple similarity that encodes maxTermFrequency directly
    */
   class TestSimilarity extends Similarity {
 
     @Override
-    public void computeNorm(FieldInvertState state, Norm norm) {
-      norm.setByte((byte) state.getUniqueTermCount());
+    public long computeNorm(FieldInvertState state) {
+      return state.getUniqueTermCount();
     }
 
     @Override
Index: lucene/core/src/test/org/apache/lucene/util/Test2BPagedBytes.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/util/Test2BPagedBytes.java	(revision 1442822)
+++ lucene/core/src/test/org/apache/lucene/util/Test2BPagedBytes.java	(working copy)
@@ -17,52 +17,59 @@
  * limitations under the License.
  */
 
-import java.util.Arrays;
 import java.util.Random;
 
-import org.apache.lucene.util.PagedBytes.PagedBytesDataInput;
-import org.apache.lucene.util.PagedBytes.PagedBytesDataOutput;
+import org.apache.lucene.store.BaseDirectoryWrapper;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.store.MockDirectoryWrapper;
 import org.junit.Ignore;
 
 @Ignore("You must increase heap to > 2 G to run this")
 public class Test2BPagedBytes extends LuceneTestCase {
 
   public void test() throws Exception {
+    BaseDirectoryWrapper dir = newFSDirectory(_TestUtil.getTempDir("test2BPagedBytes"));
+    if (dir instanceof MockDirectoryWrapper) {
+      ((MockDirectoryWrapper)dir).setThrottling(MockDirectoryWrapper.Throttling.NEVER);
+    }
     PagedBytes pb = new PagedBytes(15);
-    PagedBytesDataOutput dataOutput = pb.getDataOutput();
+    IndexOutput dataOutput = dir.createOutput("foo", IOContext.DEFAULT);
     long netBytes = 0;
     long seed = random().nextLong();
     long lastFP = 0;
     Random r2 = new Random(seed);
     while(netBytes < 1.1*Integer.MAX_VALUE) {
-      int numBytes = _TestUtil.nextInt(r2, 1, 100000);
+      int numBytes = _TestUtil.nextInt(r2, 1, 32768);
       byte[] bytes = new byte[numBytes];
       r2.nextBytes(bytes);
       dataOutput.writeBytes(bytes, bytes.length);
-      long fp = dataOutput.getPosition();
+      long fp = dataOutput.getFilePointer();
       assert fp == lastFP + numBytes;
       lastFP = fp;
       netBytes += numBytes;
     }
-    pb.freeze(true);
+    dataOutput.close();
+    IndexInput input = dir.openInput("foo", IOContext.DEFAULT);
+    pb.copy(input, input.length());
+    input.close();
+    PagedBytes.Reader reader = pb.freeze(true);
 
-    PagedBytesDataInput dataInput = pb.getDataInput();
-    lastFP = 0;
     r2 = new Random(seed);
     netBytes = 0;
     while(netBytes < 1.1*Integer.MAX_VALUE) {
-      int numBytes = _TestUtil.nextInt(r2, 1, 100000);
+      int numBytes = _TestUtil.nextInt(r2, 1, 32768);
       byte[] bytes = new byte[numBytes];
       r2.nextBytes(bytes);
+      BytesRef expected = new BytesRef(bytes);
 
-      byte[] bytesIn = new byte[numBytes];
-      dataInput.readBytes(bytesIn, 0, numBytes);
-      assertTrue(Arrays.equals(bytes, bytesIn));
+      BytesRef actual = new BytesRef();
+      reader.fillSlice(actual, netBytes, numBytes);
+      assertEquals(expected, actual);
 
-      long fp = dataInput.getPosition();
-      assert fp == lastFP + numBytes;
-      lastFP = fp;
       netBytes += numBytes;
     }
+    dir.close();
   }
 }
Index: lucene/core/src/test/org/apache/lucene/util/packed/TestPackedInts.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/util/packed/TestPackedInts.java	(revision 1442822)
+++ lucene/core/src/test/org/apache/lucene/util/packed/TestPackedInts.java	(working copy)
@@ -37,9 +37,9 @@
 import org.apache.lucene.util.LongsRef;
 import org.apache.lucene.util.LuceneTestCase;
 import org.apache.lucene.util.LuceneTestCase.Slow;
+import org.apache.lucene.util.RamUsageEstimator;
 import org.apache.lucene.util._TestUtil;
 import org.apache.lucene.util.packed.PackedInts.Reader;
-
 import org.junit.Ignore;
 
 import com.carrotsearch.randomizedtesting.generators.RandomInts;
@@ -103,7 +103,11 @@
         final int actualValueCount = random().nextBoolean() ? valueCount : _TestUtil.nextInt(random(), 0, valueCount);
         final long[] values = new long[valueCount];
         for(int i=0;i<actualValueCount;i++) {
-          values[i] = _TestUtil.nextLong(random(), 0, maxValue);
+          if (nbits == 64) {
+            values[i] = random().nextLong();
+          } else {
+            values[i] = _TestUtil.nextLong(random(), 0, maxValue);
+          }
           w.add(values[i]);
         }
         w.finish();
@@ -135,6 +139,11 @@
                     + r.getClass().getSimpleName(), values[i], r.get(i));
           }
           in.close();
+
+          final long expectedBytesUsed = RamUsageEstimator.sizeOf(r);
+          final long computedBytesUsed = r.ramBytesUsed();
+          assertEquals(r.getClass() + "expected " + expectedBytesUsed + ", got: " + computedBytesUsed,
+              expectedBytesUsed, computedBytesUsed);
         }
 
         { // test reader iterator next
@@ -795,6 +804,46 @@
     return true;
   }
 
+  public void testAppendingLongBuffer() {
+    final long[] arr = new long[RandomInts.randomIntBetween(random(), 1, 2000000)];
+    for (int bpv : new int[] {0, 1, 63, 64, RandomInts.randomIntBetween(random(), 2, 61)}) {
+      if (bpv == 0) {
+        Arrays.fill(arr, random().nextLong());
+      } else if (bpv == 64) {
+        for (int i = 0; i < arr.length; ++i) {
+          arr[i] = random().nextLong();
+        }
+      } else {
+        final long minValue = _TestUtil.nextLong(random(), Long.MIN_VALUE, Long.MAX_VALUE - PackedInts.maxValue(bpv));
+        for (int i = 0; i < arr.length; ++i) {
+          arr[i] = minValue + random().nextLong() & PackedInts.maxValue(bpv); // _TestUtil.nextLong is too slow
+        }
+      }
+      AppendingLongBuffer buf = new AppendingLongBuffer();
+      for (int i = 0; i < arr.length; ++i) {
+        buf.add(arr[i]);
+      }
+      assertEquals(arr.length, buf.size());
+      final AppendingLongBuffer.Iterator it = buf.iterator();
+      for (int i = 0; i < arr.length; ++i) {
+        if (random().nextBoolean()) {
+          assertTrue(it.hasNext());
+        }
+        assertEquals(arr[i], it.next());
+      }
+      assertFalse(it.hasNext());
+      
+      for (int i = 0; i < arr.length; ++i) {
+        assertEquals(arr[i], buf.get(i));
+      }
+
+      final long expectedBytesUsed = RamUsageEstimator.sizeOf(buf);
+      final long computedBytesUsed = buf.ramBytesUsed();
+      assertEquals("got " + computedBytesUsed + ", expected: " + expectedBytesUsed,
+          expectedBytesUsed, computedBytesUsed);
+    }
+  }
+
   public void testPackedInputOutput() throws IOException {
     final long[] longs = new long[random().nextInt(8192)];
     final int[] bitsPerValues = new int[longs.length];
@@ -841,7 +890,7 @@
   public void testBlockPackedReaderWriter() throws IOException {
     final int iters = atLeast(2);
     for (int iter = 0; iter < iters; ++iter) {
-      final int blockSize = 64 * _TestUtil.nextInt(random(), 1, 1 << 12);
+      final int blockSize = 1 << _TestUtil.nextInt(random(), 6, 18);
       final int valueCount = random().nextInt(1 << 18);
       final long[] values = new long[valueCount];
       long minValue = 0;
@@ -873,30 +922,29 @@
       final long fp = out.getFilePointer();
       out.close();
 
-      DataInput in = dir.openInput("out.bin", IOContext.DEFAULT);
-      if (random().nextBoolean()) {
-        byte[] buf = new byte[(int) fp];
-        in.readBytes(buf, 0, (int) fp);
-        ((IndexInput) in).close();
-        in = new ByteArrayDataInput(buf);
-      }
-      final BlockPackedReader reader = new BlockPackedReader(in, PackedInts.VERSION_CURRENT, blockSize, valueCount);
+      IndexInput in1 = dir.openInput("out.bin", IOContext.DEFAULT);
+      byte[] buf = new byte[(int) fp];
+      in1.readBytes(buf, 0, (int) fp);
+      in1.seek(0L);
+      ByteArrayDataInput in2 = new ByteArrayDataInput(buf);
+      final DataInput in = random().nextBoolean() ? in1 : in2;
+      final BlockPackedReaderIterator it = new BlockPackedReaderIterator(in, PackedInts.VERSION_CURRENT, blockSize, valueCount);
       for (int i = 0; i < valueCount; ) {
         if (random().nextBoolean()) {
-          assertEquals("" + i, values[i], reader.next());
+          assertEquals("" + i, values[i], it.next());
           ++i;
         } else {
-          final LongsRef nextValues = reader.next(_TestUtil.nextInt(random(), 1, 1024));
+          final LongsRef nextValues = it.next(_TestUtil.nextInt(random(), 1, 1024));
           for (int j = 0; j < nextValues.length; ++j) {
             assertEquals("" + (i + j), values[i + j], nextValues.longs[nextValues.offset + j]);
           }
           i += nextValues.length;
         }
-        assertEquals(i, reader.ord());
+        assertEquals(i, it.ord());
       }
       assertEquals(fp, in instanceof ByteArrayDataInput ? ((ByteArrayDataInput) in).getPosition() : ((IndexInput) in).getFilePointer());
       try {
-        reader.next();
+        it.next();
         assertTrue(false);
       } catch (IOException e) {
         // OK
@@ -907,33 +955,121 @@
       } else {
         ((IndexInput) in).seek(0L);
       }
-      final BlockPackedReader reader2 = new BlockPackedReader(in, PackedInts.VERSION_CURRENT, blockSize, valueCount);
+      final BlockPackedReaderIterator it2 = new BlockPackedReaderIterator(in, PackedInts.VERSION_CURRENT, blockSize, valueCount);
       int i = 0;
       while (true) {
         final int skip = _TestUtil.nextInt(random(), 0, valueCount - i);
-        reader2.skip(skip);
+        it2.skip(skip);
         i += skip;
-        assertEquals(i, reader2.ord());
+        assertEquals(i, it2.ord());
         if (i == valueCount) {
           break;
         } else {
-          assertEquals(values[i], reader2.next());
+          assertEquals(values[i], it2.next());
           ++i;
         }
       }
       assertEquals(fp, in instanceof ByteArrayDataInput ? ((ByteArrayDataInput) in).getPosition() : ((IndexInput) in).getFilePointer());
       try {
-        reader2.skip(1);
+        it2.skip(1);
         assertTrue(false);
       } catch (IOException e) {
         // OK
       }
 
-      if (in instanceof IndexInput) {
-        ((IndexInput) in).close();
+      in1.seek(0L);
+      final BlockPackedReader reader = new BlockPackedReader(in1, PackedInts.VERSION_CURRENT, blockSize, valueCount, random().nextBoolean());
+      assertEquals(in1.getFilePointer(), in1.length());
+      for (i = 0; i < valueCount; ++i) {
+        assertEquals("i=" + i, values[i], reader.get(i));
       }
+      in1.close();
       dir.close();
     }
   }
 
+  public void testMonotonicBlockPackedReaderWriter() throws IOException {
+    final int iters = atLeast(2);
+    for (int iter = 0; iter < iters; ++iter) {
+      final int blockSize = 1 << _TestUtil.nextInt(random(), 6, 18);
+      final int valueCount = random().nextInt(1 << 18);
+      final long[] values = new long[valueCount];
+      if (valueCount > 0) {
+        values[0] = random().nextBoolean() ? random().nextInt(10) : random().nextInt(Integer.MAX_VALUE);
+        int maxDelta = random().nextInt(64);
+        for (int i = 1; i < valueCount; ++i) {
+          if (random().nextDouble() < 0.1d) {
+            maxDelta = random().nextInt(64);
+          }
+          values[i] = Math.max(0, values[i-1] + _TestUtil.nextInt(random(), -16, maxDelta));
+        }
+      }
+
+      final Directory dir = newDirectory();
+      final IndexOutput out = dir.createOutput("out.bin", IOContext.DEFAULT);
+      final MonotonicBlockPackedWriter writer = new MonotonicBlockPackedWriter(out, blockSize);
+      for (int i = 0; i < valueCount; ++i) {
+        assertEquals(i, writer.ord());
+        writer.add(values[i]);
+      }
+      assertEquals(valueCount, writer.ord());
+      writer.finish();
+      assertEquals(valueCount, writer.ord());
+      final long fp = out.getFilePointer();
+      out.close();
+
+      final IndexInput in = dir.openInput("out.bin", IOContext.DEFAULT);
+      final MonotonicBlockPackedReader reader = new MonotonicBlockPackedReader(in, PackedInts.VERSION_CURRENT, blockSize, valueCount, random().nextBoolean());
+      assertEquals(fp, in.getFilePointer());
+      for (int i = 0; i < valueCount; ++i) {
+        assertEquals("i=" +i, values[i], reader.get(i));
+      }
+      in.close();
+      dir.close();
+    }
+  }
+
+  @Nightly
+  public void testBlockReaderOverflow() throws IOException {
+    final long valueCount = _TestUtil.nextLong(random(), 1L + Integer.MAX_VALUE, (long) Integer.MAX_VALUE * 2);
+    final int blockSize = 1 << _TestUtil.nextInt(random(), 20, 22);
+    final Directory dir = newDirectory();
+    final IndexOutput out = dir.createOutput("out.bin", IOContext.DEFAULT);
+    final BlockPackedWriter writer = new BlockPackedWriter(out, blockSize);
+    long value = random().nextInt() & 0xFFFFFFFFL;
+    long valueOffset = _TestUtil.nextLong(random(), 0, valueCount - 1);
+    for (long i = 0; i < valueCount; ) {
+      assertEquals(i, writer.ord());
+      if ((i & (blockSize - 1)) == 0 && (i + blockSize < valueOffset || i > valueOffset && i + blockSize < valueCount)) {
+        writer.addBlockOfZeros();
+        i += blockSize;
+      } else if (i == valueOffset) {
+        writer.add(value);
+        ++i;
+      } else {
+        writer.add(0);
+        ++i;
+      }
+    }
+    writer.finish();
+    out.close();
+    final IndexInput in = dir.openInput("out.bin", IOContext.DEFAULT);
+    final BlockPackedReaderIterator it = new BlockPackedReaderIterator(in, PackedInts.VERSION_CURRENT, blockSize, valueCount);
+    it.skip(valueOffset);
+    assertEquals(value, it.next());
+    in.seek(0L);
+    final BlockPackedReader reader = new BlockPackedReader(in, PackedInts.VERSION_CURRENT, blockSize, valueCount, random().nextBoolean());
+    assertEquals(value, reader.get(valueOffset));
+    for (int i = 0; i < 5; ++i) {
+      final long offset = _TestUtil.nextLong(random(), 0, valueCount - 1);
+      if (offset == valueOffset) {
+        assertEquals(value, reader.get(offset));
+      } else {
+        assertEquals(0, reader.get(offset));
+      }
+    }
+    in.close();
+    dir.close();
+  }
+
 }
Index: lucene/core/src/test/org/apache/lucene/util/junitcompat/TestFailIfDirectoryNotClosed.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/util/junitcompat/TestFailIfDirectoryNotClosed.java	(revision 1442822)
+++ lucene/core/src/test/org/apache/lucene/util/junitcompat/TestFailIfDirectoryNotClosed.java	(working copy)
@@ -30,7 +30,7 @@
   }
   
   public static class Nested1 extends WithNestedTests.AbstractNestedTest {
-    public void testDummy() {
+    public void testDummy() throws Exception {
       Directory dir = newDirectory();
       System.out.println(dir.toString());
     }
Index: lucene/core/src/test/org/apache/lucene/util/junitcompat/TestFailOnFieldCacheInsanity.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/util/junitcompat/TestFailOnFieldCacheInsanity.java	(revision 0)
+++ lucene/core/src/test/org/apache/lucene/util/junitcompat/TestFailOnFieldCacheInsanity.java	(working copy)
@@ -0,0 +1,76 @@
+package org.apache.lucene.util.junitcompat;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.StringField;
+import org.apache.lucene.index.AtomicReader;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.RandomIndexWriter;
+import org.apache.lucene.search.FieldCache;
+import org.apache.lucene.store.Directory;
+import org.junit.Assert;
+import org.junit.Test;
+import org.junit.runner.JUnitCore;
+import org.junit.runner.Result;
+import org.junit.runner.notification.Failure;
+
+public class TestFailOnFieldCacheInsanity extends WithNestedTests {
+  public TestFailOnFieldCacheInsanity() {
+    super(true);
+  }
+  
+  public static class Nested1 extends WithNestedTests.AbstractNestedTest {
+    private Directory d;
+    private IndexReader r;
+    private AtomicReader subR;
+
+    private void makeIndex() throws Exception {
+      d = newDirectory();
+      RandomIndexWriter w = new RandomIndexWriter(random(), d);
+      Document doc = new Document();
+      doc.add(newField("ints", "1", StringField.TYPE_NOT_STORED));
+      w.addDocument(doc);
+      w.forceMerge(1);
+      r = w.getReader();
+      w.close();
+
+      subR = r.leaves().get(0).reader();
+    }
+
+    public void testDummy() throws Exception {
+      makeIndex();
+      assertNotNull(FieldCache.DEFAULT.getTermsIndex(subR, "ints"));
+      assertNotNull(FieldCache.DEFAULT.getTerms(subR, "ints"));
+      // NOTE: do not close reader/directory, else it
+      // purges FC entries
+    }
+  }
+
+  @Test
+  public void testFailOnFieldCacheInsanity() {
+    Result r = JUnitCore.runClasses(Nested1.class);
+    boolean insane = false;
+    for(Failure f : r.getFailures()) {
+      if (f.getMessage().indexOf("Insane") != -1) {
+        insane = true;
+      }
+    }
+    Assert.assertTrue(insane);
+  }
+}

Property changes on: lucene/core/src/test/org/apache/lucene/util/junitcompat/TestFailOnFieldCacheInsanity.java
___________________________________________________________________
Added: svn:eol-style
## -0,0 +1 ##
+native
Index: lucene/core/src/test/org/apache/lucene/util/TestPagedBytes.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/util/TestPagedBytes.java	(revision 1442822)
+++ lucene/core/src/test/org/apache/lucene/util/TestPagedBytes.java	(working copy)
@@ -20,10 +20,12 @@
 import java.io.IOException;
 import java.util.*;
 
+import org.apache.lucene.store.BaseDirectoryWrapper;
 import org.apache.lucene.store.DataInput;
-import org.apache.lucene.store.DataOutput;
-import org.apache.lucene.util.PagedBytes.PagedBytesDataInput;
-import org.apache.lucene.util.PagedBytes.PagedBytesDataOutput;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.store.MockDirectoryWrapper;
 import org.junit.Ignore;
 
 public class TestPagedBytes extends LuceneTestCase {
@@ -31,10 +33,14 @@
   public void testDataInputOutput() throws Exception {
     Random random = random();
     for(int iter=0;iter<5*RANDOM_MULTIPLIER;iter++) {
+      BaseDirectoryWrapper dir = newFSDirectory(_TestUtil.getTempDir("testOverflow"));
+      if (dir instanceof MockDirectoryWrapper) {
+        ((MockDirectoryWrapper)dir).setThrottling(MockDirectoryWrapper.Throttling.NEVER);
+      }
       final int blockBits = _TestUtil.nextInt(random, 1, 20);
       final int blockSize = 1 << blockBits;
       final PagedBytes p = new PagedBytes(blockBits);
-      final DataOutput out = p.getDataOutput();
+      final IndexOutput out = dir.createOutput("foo", IOContext.DEFAULT);
       final int numBytes = _TestUtil.nextInt(random(), 2, 10000000);
 
       final byte[] answer = new byte[numBytes];
@@ -49,11 +55,14 @@
           written += chunk;
         }
       }
-
+      
+      out.close();
+      final IndexInput input = dir.openInput("foo", IOContext.DEFAULT);
+      final DataInput in = input.clone();
+      
+      p.copy(input, input.length());
       final PagedBytes.Reader reader = p.freeze(random.nextBoolean());
 
-      final DataInput in = p.getDataInput();
-
       final byte[] verify = new byte[numBytes];
       int read = 0;
       while(read < numBytes) {
@@ -76,68 +85,17 @@
           assertEquals(answer[pos + byteUpto], slice.bytes[slice.offset + byteUpto]);
         }
       }
+      input.close();
+      dir.close();
     }
   }
 
-  public void testLengthPrefix() throws Exception {
-    Random random = random();
-    for(int iter=0;iter<5*RANDOM_MULTIPLIER;iter++) {
-      final int blockBits = _TestUtil.nextInt(random, 2, 20);
-      final int blockSize = 1 << blockBits;
-      final PagedBytes p = new PagedBytes(blockBits);
-      final List<Integer> addresses = new ArrayList<Integer>();
-      final List<BytesRef> answers = new ArrayList<BytesRef>();
-      int totBytes = 0;
-      while(totBytes < 10000000 && answers.size() < 100000) {
-        final int len = random.nextInt(Math.min(blockSize-2, 32768));
-        final BytesRef b = new BytesRef();
-        b.bytes = new byte[len];
-        b.length = len;
-        b.offset = 0;
-        random.nextBytes(b.bytes);
-        answers.add(b);
-        addresses.add((int) p.copyUsingLengthPrefix(b));
-
-        totBytes += len;
-      }
-
-      final PagedBytes.Reader reader = p.freeze(random.nextBoolean());
-
-      final BytesRef slice = new BytesRef();
-
-      for(int idx=0;idx<answers.size();idx++) {
-        reader.fillSliceWithPrefix(slice, addresses.get(idx));
-        assertEquals(answers.get(idx), slice);
-      }
-    }
-  }
-
-  // LUCENE-3841: even though
-  // copyUsingLengthPrefix will never span two blocks, make
-  // sure if caller writes their own prefix followed by the
-  // bytes, it still works:
-  public void testLengthPrefixAcrossTwoBlocks() throws Exception {
-    Random random = random();
-    final PagedBytes p = new PagedBytes(10);
-    final DataOutput out = p.getDataOutput();
-    final byte[] bytes1 = new byte[1000];
-    random.nextBytes(bytes1);
-    out.writeBytes(bytes1, 0, bytes1.length);
-    out.writeByte((byte) 40);
-    final byte[] bytes2 = new byte[40];
-    random.nextBytes(bytes2);
-    out.writeBytes(bytes2, 0, bytes2.length);
-
-    final PagedBytes.Reader reader = p.freeze(random.nextBoolean());
-    BytesRef answer = reader.fillSliceWithPrefix(new BytesRef(), 1000);
-    assertEquals(40, answer.length);
-    for(int i=0;i<40;i++) {
-      assertEquals(bytes2[i], answer.bytes[answer.offset + i]);
-    }
-  }
-
   @Ignore // memory hole
   public void testOverflow() throws IOException {
+    BaseDirectoryWrapper dir = newFSDirectory(_TestUtil.getTempDir("testOverflow"));
+    if (dir instanceof MockDirectoryWrapper) {
+      ((MockDirectoryWrapper)dir).setThrottling(MockDirectoryWrapper.Throttling.NEVER);
+    }
     final int blockBits = _TestUtil.nextInt(random(), 14, 28);
     final int blockSize = 1 << blockBits;
     byte[] arr = new byte[_TestUtil.nextInt(random(), blockSize / 2, blockSize * 2)];
@@ -146,23 +104,26 @@
     }
     final long numBytes = (1L << 31) + _TestUtil.nextInt(random(), 1, blockSize * 3);
     final PagedBytes p = new PagedBytes(blockBits);
-    final PagedBytesDataOutput out = p.getDataOutput();
+    final IndexOutput out = dir.createOutput("foo", IOContext.DEFAULT);
     for (long i = 0; i < numBytes; ) {
-      assertEquals(i, out.getPosition());
+      assertEquals(i, out.getFilePointer());
       final int len = (int) Math.min(arr.length, numBytes - i);
       out.writeBytes(arr, len);
       i += len;
     }
-    assertEquals(numBytes, out.getPosition());
-    p.freeze(random().nextBoolean());
-    final PagedBytesDataInput in = p.getDataInput();
+    assertEquals(numBytes, out.getFilePointer());
+    out.close();
+    final IndexInput in = dir.openInput("foo", IOContext.DEFAULT);
+    p.copy(in, numBytes);
+    final PagedBytes.Reader reader = p.freeze(random().nextBoolean());
 
     for (long offset : new long[] {0L, Integer.MAX_VALUE, numBytes - 1,
         _TestUtil.nextLong(random(), 1, numBytes - 2)}) {
-      in.setPosition(offset);
-      assertEquals(offset, in.getPosition());
-      assertEquals(arr[(int) (offset % arr.length)], in.readByte());
-      assertEquals(offset+1, in.getPosition());
+      BytesRef b = new BytesRef();
+      reader.fillSlice(b, offset, 1);
+      assertEquals(arr[(int) (offset % arr.length)], b.bytes[b.offset]);
     }
+    in.close();
+    dir.close();
   }
 }
Index: lucene/core/src/test/org/apache/lucene/util/TestByteBlockPool.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/util/TestByteBlockPool.java	(revision 1442822)
+++ lucene/core/src/test/org/apache/lucene/util/TestByteBlockPool.java	(working copy)
@@ -1,14 +1,9 @@
 package org.apache.lucene.util;
 
-import java.io.EOFException;
 import java.io.IOException;
 import java.util.ArrayList;
 import java.util.List;
 
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.store.RAMDirectory;
-
 /*
  * Licensed to the Apache Software Foundation (ASF) under one or more
  * contributor license agreements. See the NOTICE file distributed with this
@@ -27,46 +22,33 @@
  */
 public class TestByteBlockPool extends LuceneTestCase {
 
-  public void testCopyRefAndWrite() throws IOException {
+  public void testReadAndWrite() throws IOException {
     Counter bytesUsed = Counter.newCounter();
     ByteBlockPool pool = new ByteBlockPool(new ByteBlockPool.DirectTrackingAllocator(bytesUsed));
     pool.nextBuffer();
     boolean reuseFirst = random().nextBoolean();
     for (int j = 0; j < 2; j++) {
         
-      List<String> list = new ArrayList<String>();
+      List<BytesRef> list = new ArrayList<BytesRef>();
       int maxLength = atLeast(500);
       final int numValues = atLeast(100);
       BytesRef ref = new BytesRef();
       for (int i = 0; i < numValues; i++) {
         final String value = _TestUtil.randomRealisticUnicodeString(random(),
             maxLength);
-        list.add(value);
+        list.add(new BytesRef(value));
         ref.copyChars(value);
-        pool.copy(ref);
+        pool.append(ref);
       }
-      RAMDirectory dir = new RAMDirectory();
-      IndexOutput stream = dir.createOutput("foo.txt", newIOContext(random()));
-      pool.writePool(stream);
-      stream.flush();
-      stream.close();
-      IndexInput input = dir.openInput("foo.txt", newIOContext(random()));
-      assertEquals(pool.byteOffset + pool.byteUpto, stream.length());
-      BytesRef expected = new BytesRef();
-      BytesRef actual = new BytesRef();
-      for (String string : list) {
-        expected.copyChars(string);
-        actual.grow(expected.length);
-        actual.length = expected.length;
-        input.readBytes(actual.bytes, 0, actual.length);
-        assertEquals(expected, actual);
+      // verify
+      long position = 0;
+      for (BytesRef expected : list) {
+        ref.grow(expected.length);
+        ref.length = expected.length;
+        pool.readBytes(position, ref.bytes, ref.offset, ref.length);
+        assertEquals(expected, ref);
+        position += ref.length;
       }
-      try {
-        input.readByte();
-        fail("must be EOF");
-      } catch (EOFException e) {
-        // expected - read past EOF
-      }
       pool.reset(random().nextBoolean(), reuseFirst);
       if (reuseFirst) {
         assertEquals(ByteBlockPool.BYTE_BLOCK_SIZE, bytesUsed.get());
@@ -74,7 +56,6 @@
         assertEquals(0, bytesUsed.get());
         pool.nextBuffer(); // prepare for next iter
       }
-      dir.close();
     }
-  }
+  } 
 }
Index: lucene/core/src/test/org/apache/lucene/document/TestField.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/document/TestField.java	(revision 1442822)
+++ lucene/core/src/test/org/apache/lucene/document/TestField.java	(working copy)
@@ -29,44 +29,6 @@
 // sanity check some basics of fields
 public class TestField extends LuceneTestCase {
   
-  public void testByteDocValuesField() throws Exception {
-    ByteDocValuesField field = new ByteDocValuesField("foo", (byte) 5);
-
-    trySetBoost(field);
-    field.setByteValue((byte) 6); // ok
-    trySetBytesValue(field);
-    trySetBytesRefValue(field);
-    trySetDoubleValue(field);
-    trySetIntValue(field);
-    trySetFloatValue(field);
-    trySetLongValue(field);
-    trySetReaderValue(field);
-    trySetShortValue(field);
-    trySetStringValue(field);
-    trySetTokenStreamValue(field);
-    
-    assertEquals(6, field.numericValue().byteValue());
-  }
-  
-  public void testDerefBytesDocValuesField() throws Exception {
-    DerefBytesDocValuesField field = new DerefBytesDocValuesField("foo", new BytesRef("bar"));
-
-    trySetBoost(field);
-    trySetByteValue(field);
-    field.setBytesValue("fubar".getBytes("UTF-8"));
-    field.setBytesValue(new BytesRef("baz"));
-    trySetDoubleValue(field);
-    trySetIntValue(field);
-    trySetFloatValue(field);
-    trySetLongValue(field);
-    trySetReaderValue(field);
-    trySetShortValue(field);
-    trySetStringValue(field);
-    trySetTokenStreamValue(field);
-    
-    assertEquals(new BytesRef("baz"), field.binaryValue());
-  }
-  
   public void testDoubleField() throws Exception {
     Field fields[] = new Field[] {
         new DoubleField("foo", 5d, Field.Store.NO),
@@ -107,7 +69,7 @@
     trySetStringValue(field);
     trySetTokenStreamValue(field);
     
-    assertEquals(6d, field.numericValue().doubleValue(), 0.0d);
+    assertEquals(6d, Double.longBitsToDouble(field.numericValue().longValue()), 0.0d);
   }
   
   public void testFloatDocValuesField() throws Exception {
@@ -126,7 +88,7 @@
     trySetStringValue(field);
     trySetTokenStreamValue(field);
     
-    assertEquals(6f, field.numericValue().floatValue(), 0.0f);
+    assertEquals(6f, Float.intBitsToFloat(field.numericValue().intValue()), 0.0f);
   }
   
   public void testFloatField() throws Exception {
@@ -153,25 +115,6 @@
     }
   }
   
-  public void testIntDocValuesField() throws Exception {
-    IntDocValuesField field = new IntDocValuesField("foo", 5);
-
-    trySetBoost(field);
-    trySetByteValue(field);
-    trySetBytesValue(field);
-    trySetBytesRefValue(field);
-    trySetDoubleValue(field);
-    field.setIntValue(6); // ok
-    trySetFloatValue(field);
-    trySetLongValue(field);
-    trySetReaderValue(field);
-    trySetShortValue(field);
-    trySetStringValue(field);
-    trySetTokenStreamValue(field);
-    
-    assertEquals(6, field.numericValue().intValue());
-  }
-  
   public void testIntField() throws Exception {
     Field fields[] = new Field[] {
         new IntField("foo", 5, Field.Store.NO),
@@ -196,8 +139,8 @@
     }
   }
   
-  public void testLongDocValuesField() throws Exception {
-    LongDocValuesField field = new LongDocValuesField("foo", 5L);
+  public void testNumericDocValuesField() throws Exception {
+    NumericDocValuesField field = new NumericDocValuesField("foo", 5L);
 
     trySetBoost(field);
     trySetByteValue(field);
@@ -239,46 +182,8 @@
     }
   }
   
-  public void testPackedLongDocValuesField() throws Exception {
-    PackedLongDocValuesField field = new PackedLongDocValuesField("foo", 5L);
-
-    trySetBoost(field);
-    trySetByteValue(field);
-    trySetBytesValue(field);
-    trySetBytesRefValue(field);
-    trySetDoubleValue(field);
-    trySetIntValue(field);
-    trySetFloatValue(field);
-    field.setLongValue(6); // ok
-    trySetReaderValue(field);
-    trySetShortValue(field);
-    trySetStringValue(field);
-    trySetTokenStreamValue(field);
-    
-    assertEquals(6L, field.numericValue().longValue());
-  }
-  
-  public void testShortDocValuesField() throws Exception {
-    ShortDocValuesField field = new ShortDocValuesField("foo", (short)5);
-
-    trySetBoost(field);
-    trySetByteValue(field);
-    trySetBytesValue(field);
-    trySetBytesRefValue(field);
-    trySetDoubleValue(field);
-    trySetIntValue(field);
-    trySetFloatValue(field);
-    trySetLongValue(field);
-    trySetReaderValue(field);
-    field.setShortValue((short) 6); // ok
-    trySetStringValue(field);
-    trySetTokenStreamValue(field);
-    
-    assertEquals((short)6, field.numericValue().shortValue());
-  }
-  
   public void testSortedBytesDocValuesField() throws Exception {
-    SortedBytesDocValuesField field = new SortedBytesDocValuesField("foo", new BytesRef("bar"));
+    SortedDocValuesField field = new SortedDocValuesField("foo", new BytesRef("bar"));
 
     trySetBoost(field);
     trySetByteValue(field);
@@ -296,8 +201,8 @@
     assertEquals(new BytesRef("baz"), field.binaryValue());
   }
   
-  public void testStraightBytesDocValuesField() throws Exception {
-    StraightBytesDocValuesField field = new StraightBytesDocValuesField("foo", new BytesRef("bar"));
+  public void testBinaryDocValuesField() throws Exception {
+    BinaryDocValuesField field = new BinaryDocValuesField("foo", new BytesRef("bar"));
 
     trySetBoost(field);
     trySetByteValue(field);
Index: lucene/core/src/java/org/apache/lucene/store/CompoundFileWriter.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/store/CompoundFileWriter.java	(revision 1442822)
+++ lucene/core/src/java/org/apache/lucene/store/CompoundFileWriter.java	(working copy)
@@ -220,6 +220,7 @@
       assert !seenIDs.contains(id): "file=\"" + name + "\" maps to id=\"" + id + "\", which was already written";
       seenIDs.add(id);
       final DirectCFSIndexOutput out;
+
       if ((outputLocked = outputTaken.compareAndSet(false, true))) {
         out = new DirectCFSIndexOutput(getOutput(), entry, false);
       } else {
Index: lucene/core/src/java/org/apache/lucene/codecs/lucene42/Lucene42FieldInfosReader.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/codecs/lucene42/Lucene42FieldInfosReader.java	(revision 0)
+++ lucene/core/src/java/org/apache/lucene/codecs/lucene42/Lucene42FieldInfosReader.java	(working copy)
@@ -0,0 +1,121 @@
+package org.apache.lucene.codecs.lucene42;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Collections;
+import java.util.Map;
+
+import org.apache.lucene.codecs.CodecUtil;
+import org.apache.lucene.codecs.FieldInfosReader;
+import org.apache.lucene.index.CorruptIndexException;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.FieldInfo.DocValuesType;
+import org.apache.lucene.index.FieldInfo.IndexOptions;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.util.IOUtils;
+
+/**
+ * Lucene 4.2 FieldInfos reader.
+ * 
+ * @lucene.experimental
+ * @see Lucene42FieldInfosFormat
+ */
+final class Lucene42FieldInfosReader extends FieldInfosReader {
+
+  /** Sole constructor. */
+  public Lucene42FieldInfosReader() {
+  }
+
+  @Override
+  public FieldInfos read(Directory directory, String segmentName, IOContext iocontext) throws IOException {
+    final String fileName = IndexFileNames.segmentFileName(segmentName, "", Lucene42FieldInfosFormat.EXTENSION);
+    IndexInput input = directory.openInput(fileName, iocontext);
+    
+    boolean success = false;
+    try {
+      CodecUtil.checkHeader(input, Lucene42FieldInfosFormat.CODEC_NAME, 
+                                   Lucene42FieldInfosFormat.FORMAT_START, 
+                                   Lucene42FieldInfosFormat.FORMAT_CURRENT);
+
+      final int size = input.readVInt(); //read in the size
+      FieldInfo infos[] = new FieldInfo[size];
+
+      for (int i = 0; i < size; i++) {
+        String name = input.readString();
+        final int fieldNumber = input.readVInt();
+        byte bits = input.readByte();
+        boolean isIndexed = (bits & Lucene42FieldInfosFormat.IS_INDEXED) != 0;
+        boolean storeTermVector = (bits & Lucene42FieldInfosFormat.STORE_TERMVECTOR) != 0;
+        boolean omitNorms = (bits & Lucene42FieldInfosFormat.OMIT_NORMS) != 0;
+        boolean storePayloads = (bits & Lucene42FieldInfosFormat.STORE_PAYLOADS) != 0;
+        final IndexOptions indexOptions;
+        if (!isIndexed) {
+          indexOptions = null;
+        } else if ((bits & Lucene42FieldInfosFormat.OMIT_TERM_FREQ_AND_POSITIONS) != 0) {
+          indexOptions = IndexOptions.DOCS_ONLY;
+        } else if ((bits & Lucene42FieldInfosFormat.OMIT_POSITIONS) != 0) {
+          indexOptions = IndexOptions.DOCS_AND_FREQS;
+        } else if ((bits & Lucene42FieldInfosFormat.STORE_OFFSETS_IN_POSTINGS) != 0) {
+          indexOptions = IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS;
+        } else {
+          indexOptions = IndexOptions.DOCS_AND_FREQS_AND_POSITIONS;
+        }
+
+        // DV Types are packed in one byte
+        byte val = input.readByte();
+        final DocValuesType docValuesType = getDocValuesType(input, (byte) (val & 0x0F));
+        final DocValuesType normsType = getDocValuesType(input, (byte) ((val >>> 4) & 0x0F));
+        final Map<String,String> attributes = input.readStringStringMap();
+        infos[i] = new FieldInfo(name, isIndexed, fieldNumber, storeTermVector, 
+          omitNorms, storePayloads, indexOptions, docValuesType, normsType, Collections.unmodifiableMap(attributes));
+      }
+
+      if (input.getFilePointer() != input.length()) {
+        throw new CorruptIndexException("did not read all bytes from file \"" + fileName + "\": read " + input.getFilePointer() + " vs size " + input.length() + " (resource: " + input + ")");
+      }
+      FieldInfos fieldInfos = new FieldInfos(infos);
+      success = true;
+      return fieldInfos;
+    } finally {
+      if (success) {
+        input.close();
+      } else {
+        IOUtils.closeWhileHandlingException(input);
+      }
+    }
+  }
+  
+  private static DocValuesType getDocValuesType(IndexInput input, byte b) throws IOException {
+    if (b == 0) {
+      return null;
+    } else if (b == 1) {
+      return DocValuesType.NUMERIC;
+    } else if (b == 2) {
+      return DocValuesType.BINARY;
+    } else if (b == 3) {
+      return DocValuesType.SORTED;
+    } else {
+      throw new CorruptIndexException("invalid docvalues byte: " + b + " (resource=" + input + ")");
+    }
+  }
+}

Property changes on: lucene/core/src/java/org/apache/lucene/codecs/lucene42/Lucene42FieldInfosReader.java
___________________________________________________________________
Added: svn:eol-style
## -0,0 +1 ##
+native
Index: lucene/core/src/java/org/apache/lucene/codecs/lucene42/Lucene42NormsFormat.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/codecs/lucene42/Lucene42NormsFormat.java	(revision 0)
+++ lucene/core/src/java/org/apache/lucene/codecs/lucene42/Lucene42NormsFormat.java	(working copy)
@@ -0,0 +1,64 @@
+package org.apache.lucene.codecs.lucene42;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.codecs.DocValuesConsumer;
+import org.apache.lucene.codecs.DocValuesProducer;
+import org.apache.lucene.codecs.NormsFormat;
+import org.apache.lucene.index.SegmentReadState;
+import org.apache.lucene.index.SegmentWriteState;
+import org.apache.lucene.util.packed.PackedInts;
+
+/**
+ * Lucene 4.2 score normalization format.
+ * <p>
+ * NOTE: this uses the same format as {@link Lucene42DocValuesFormat}
+ * Numeric DocValues, but with different file extensions, and passing
+ * {@link PackedInts#FASTEST} for uncompressed encoding: trading off
+ * space for performance.
+ * <p>
+ * Files:
+ * <ul>
+ *   <li><tt>.nvd</tt>: DocValues data</li>
+ *   <li><tt>.nvm</tt>: DocValues metadata</li>
+ * </ul>
+ * @see Lucene42DocValuesFormat
+ */
+public final class Lucene42NormsFormat extends NormsFormat {
+
+  /** Sole constructor */
+  public Lucene42NormsFormat() {}
+  
+  @Override
+  public DocValuesConsumer normsConsumer(SegmentWriteState state) throws IOException {
+    // note: we choose FASTEST here (otherwise our norms are half as big but 15% slower than previous lucene)
+    return new Lucene42DocValuesConsumer(state, DATA_CODEC, DATA_EXTENSION, METADATA_CODEC, METADATA_EXTENSION, PackedInts.FASTEST);
+  }
+  
+  @Override
+  public DocValuesProducer normsProducer(SegmentReadState state) throws IOException {
+    return new Lucene42DocValuesProducer(state, DATA_CODEC, DATA_EXTENSION, METADATA_CODEC, METADATA_EXTENSION);
+  }
+  
+  private static final String DATA_CODEC = "Lucene41NormsData";
+  private static final String DATA_EXTENSION = "nvd";
+  private static final String METADATA_CODEC = "Lucene41NormsMetadata";
+  private static final String METADATA_EXTENSION = "nvm";
+}

Property changes on: lucene/core/src/java/org/apache/lucene/codecs/lucene42/Lucene42NormsFormat.java
___________________________________________________________________
Added: svn:eol-style
## -0,0 +1 ##
+native
Index: lucene/core/src/java/org/apache/lucene/codecs/lucene42/Lucene42Codec.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/codecs/lucene42/Lucene42Codec.java	(revision 1442822)
+++ lucene/core/src/java/org/apache/lucene/codecs/lucene42/Lucene42Codec.java	(working copy)
@@ -27,16 +27,15 @@
 import org.apache.lucene.codecs.SegmentInfoFormat;
 import org.apache.lucene.codecs.StoredFieldsFormat;
 import org.apache.lucene.codecs.TermVectorsFormat;
-import org.apache.lucene.codecs.lucene40.Lucene40DocValuesFormat;
-import org.apache.lucene.codecs.lucene40.Lucene40FieldInfosFormat;
 import org.apache.lucene.codecs.lucene40.Lucene40LiveDocsFormat;
-import org.apache.lucene.codecs.lucene40.Lucene40NormsFormat;
 import org.apache.lucene.codecs.lucene40.Lucene40SegmentInfoFormat;
 import org.apache.lucene.codecs.lucene41.Lucene41StoredFieldsFormat;
+import org.apache.lucene.codecs.perfield.PerFieldDocValuesFormat;
 import org.apache.lucene.codecs.perfield.PerFieldPostingsFormat;
 
 /**
- * Implements the Lucene 4.2 index format, with configurable per-field postings formats.
+ * Implements the Lucene 4.2 index format, with configurable per-field postings
+ * and docvalues formats.
  * <p>
  * If you want to reuse functionality of this codec in another codec, extend
  * {@link FilterCodec}.
@@ -50,7 +49,7 @@
 public class Lucene42Codec extends Codec {
   private final StoredFieldsFormat fieldsFormat = new Lucene41StoredFieldsFormat();
   private final TermVectorsFormat vectorsFormat = new Lucene42TermVectorsFormat();
-  private final FieldInfosFormat fieldInfosFormat = new Lucene40FieldInfosFormat();
+  private final FieldInfosFormat fieldInfosFormat = new Lucene42FieldInfosFormat();
   private final SegmentInfoFormat infosFormat = new Lucene40SegmentInfoFormat();
   private final LiveDocsFormat liveDocsFormat = new Lucene40LiveDocsFormat();
   
@@ -60,6 +59,14 @@
       return Lucene42Codec.this.getPostingsFormatForField(field);
     }
   };
+  
+  
+  private final DocValuesFormat docValuesFormat = new PerFieldDocValuesFormat() {
+    @Override
+    public DocValuesFormat getDocValuesFormatForField(String field) {
+      return Lucene42Codec.this.getDocValuesFormatForField(field);
+    }
+  };
 
   /** Sole constructor. */
   public Lucene42Codec() {
@@ -105,15 +112,24 @@
     return defaultFormat;
   }
   
+  /** Returns the docvalues format that should be used for writing 
+   *  new segments of <code>field</code>.
+   *  
+   *  The default implementation always returns "Lucene42"
+   */
+  public DocValuesFormat getDocValuesFormatForField(String field) {
+    return defaultDVFormat;
+  }
+  
   @Override
   public final DocValuesFormat docValuesFormat() {
     return docValuesFormat;
   }
 
   private final PostingsFormat defaultFormat = PostingsFormat.forName("Lucene41");
-  private final DocValuesFormat docValuesFormat = new Lucene40DocValuesFormat();
+  private final DocValuesFormat defaultDVFormat = DocValuesFormat.forName("Lucene42");
 
-  private final NormsFormat normsFormat = new Lucene40NormsFormat();
+  private final NormsFormat normsFormat = new Lucene42NormsFormat();
 
   @Override
   public final NormsFormat normsFormat() {
Index: lucene/core/src/java/org/apache/lucene/codecs/lucene42/Lucene42FieldInfosWriter.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/codecs/lucene42/Lucene42FieldInfosWriter.java	(revision 0)
+++ lucene/core/src/java/org/apache/lucene/codecs/lucene42/Lucene42FieldInfosWriter.java	(working copy)
@@ -0,0 +1,106 @@
+package org.apache.lucene.codecs.lucene42;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.codecs.CodecUtil;
+import org.apache.lucene.codecs.FieldInfosWriter;
+import org.apache.lucene.index.FieldInfo.DocValuesType;
+import org.apache.lucene.index.FieldInfo.IndexOptions;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.util.IOUtils;
+
+/**
+ * Lucene 4.2 FieldInfos writer.
+ * 
+ * @see Lucene42FieldInfosFormat
+ * @lucene.experimental
+ */
+final class Lucene42FieldInfosWriter extends FieldInfosWriter {
+  
+  /** Sole constructor. */
+  public Lucene42FieldInfosWriter() {
+  }
+  
+  @Override
+  public void write(Directory directory, String segmentName, FieldInfos infos, IOContext context) throws IOException {
+    final String fileName = IndexFileNames.segmentFileName(segmentName, "", Lucene42FieldInfosFormat.EXTENSION);
+    IndexOutput output = directory.createOutput(fileName, context);
+    boolean success = false;
+    try {
+      CodecUtil.writeHeader(output, Lucene42FieldInfosFormat.CODEC_NAME, Lucene42FieldInfosFormat.FORMAT_CURRENT);
+      output.writeVInt(infos.size());
+      for (FieldInfo fi : infos) {
+        IndexOptions indexOptions = fi.getIndexOptions();
+        byte bits = 0x0;
+        if (fi.hasVectors()) bits |= Lucene42FieldInfosFormat.STORE_TERMVECTOR;
+        if (fi.omitsNorms()) bits |= Lucene42FieldInfosFormat.OMIT_NORMS;
+        if (fi.hasPayloads()) bits |= Lucene42FieldInfosFormat.STORE_PAYLOADS;
+        if (fi.isIndexed()) {
+          bits |= Lucene42FieldInfosFormat.IS_INDEXED;
+          assert indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 || !fi.hasPayloads();
+          if (indexOptions == IndexOptions.DOCS_ONLY) {
+            bits |= Lucene42FieldInfosFormat.OMIT_TERM_FREQ_AND_POSITIONS;
+          } else if (indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) {
+            bits |= Lucene42FieldInfosFormat.STORE_OFFSETS_IN_POSTINGS;
+          } else if (indexOptions == IndexOptions.DOCS_AND_FREQS) {
+            bits |= Lucene42FieldInfosFormat.OMIT_POSITIONS;
+          }
+        }
+        output.writeString(fi.name);
+        output.writeVInt(fi.number);
+        output.writeByte(bits);
+
+        // pack the DV types in one byte
+        final byte dv = docValuesByte(fi.getDocValuesType());
+        final byte nrm = docValuesByte(fi.getNormType());
+        assert (dv & (~0xF)) == 0 && (nrm & (~0x0F)) == 0;
+        byte val = (byte) (0xff & ((nrm << 4) | dv));
+        output.writeByte(val);
+        output.writeStringStringMap(fi.attributes());
+      }
+      success = true;
+    } finally {
+      if (success) {
+        output.close();
+      } else {
+        IOUtils.closeWhileHandlingException(output);
+      }
+    }
+  }
+  
+  private static byte docValuesByte(DocValuesType type) {
+    if (type == null) {
+      return 0;
+    } else if (type == DocValuesType.NUMERIC) {
+      return 1;
+    } else if (type == DocValuesType.BINARY) {
+      return 2;
+    } else if (type == DocValuesType.SORTED) {
+      return 3;
+    } else {
+      throw new AssertionError();
+    }
+  }  
+}

Property changes on: lucene/core/src/java/org/apache/lucene/codecs/lucene42/Lucene42FieldInfosWriter.java
___________________________________________________________________
Added: svn:eol-style
## -0,0 +1 ##
+native
Index: lucene/core/src/java/org/apache/lucene/codecs/lucene42/Lucene42DocValuesProducer.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/codecs/lucene42/Lucene42DocValuesProducer.java	(revision 0)
+++ lucene/core/src/java/org/apache/lucene/codecs/lucene42/Lucene42DocValuesProducer.java	(working copy)
@@ -0,0 +1,313 @@
+package org.apache.lucene.codecs.lucene42;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.HashMap;
+import java.util.Map;
+
+import org.apache.lucene.codecs.CodecUtil;
+import org.apache.lucene.codecs.DocValuesProducer;
+import org.apache.lucene.index.BinaryDocValues;
+import org.apache.lucene.index.CorruptIndexException;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.NumericDocValues;
+import org.apache.lucene.index.SegmentReadState;
+import org.apache.lucene.index.SortedDocValues;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util.IntsRef;
+import org.apache.lucene.util.PagedBytes;
+import org.apache.lucene.util.fst.BytesRefFSTEnum;
+import org.apache.lucene.util.fst.BytesRefFSTEnum.InputOutput;
+import org.apache.lucene.util.fst.FST;
+import org.apache.lucene.util.fst.FST.Arc;
+import org.apache.lucene.util.fst.FST.BytesReader;
+import org.apache.lucene.util.fst.PositiveIntOutputs;
+import org.apache.lucene.util.fst.Util;
+import org.apache.lucene.util.packed.BlockPackedReader;
+import org.apache.lucene.util.packed.MonotonicBlockPackedReader;
+import org.apache.lucene.util.packed.PackedInts;
+
+/**
+ * Reader for {@link Lucene42DocValuesFormat}
+ */
+class Lucene42DocValuesProducer extends DocValuesProducer {
+  // metadata maps (just file pointers and minimal stuff)
+  private final Map<Integer,NumericEntry> numerics;
+  private final Map<Integer,BinaryEntry> binaries;
+  private final Map<Integer,FSTEntry> fsts;
+  private final IndexInput data;
+  
+  // ram instances we have already loaded
+  private final Map<Integer,NumericDocValues> numericInstances = 
+      new HashMap<Integer,NumericDocValues>();
+  private final Map<Integer,BinaryDocValues> binaryInstances =
+      new HashMap<Integer,BinaryDocValues>();
+  private final Map<Integer,FST<Long>> fstInstances =
+      new HashMap<Integer,FST<Long>>();
+  
+  private final int maxDoc;
+    
+  Lucene42DocValuesProducer(SegmentReadState state, String dataCodec, String dataExtension, String metaCodec, String metaExtension) throws IOException {
+    maxDoc = state.segmentInfo.getDocCount();
+    String metaName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, metaExtension);
+    // read in the entries from the metadata file.
+    IndexInput in = state.directory.openInput(metaName, state.context);
+    boolean success = false;
+    try {
+      CodecUtil.checkHeader(in, metaCodec, 
+                                Lucene42DocValuesConsumer.VERSION_START,
+                                Lucene42DocValuesConsumer.VERSION_START);
+      numerics = new HashMap<Integer,NumericEntry>();
+      binaries = new HashMap<Integer,BinaryEntry>();
+      fsts = new HashMap<Integer,FSTEntry>();
+      readFields(in, state.fieldInfos);
+      success = true;
+    } finally {
+      if (success) {
+        IOUtils.close(in);
+      } else {
+        IOUtils.closeWhileHandlingException(in);
+      }
+    }
+    
+    String dataName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, dataExtension);
+    data = state.directory.openInput(dataName, state.context);
+    CodecUtil.checkHeader(data, dataCodec, 
+                                Lucene42DocValuesConsumer.VERSION_START,
+                                Lucene42DocValuesConsumer.VERSION_START);
+  }
+  
+  private void readFields(IndexInput meta, FieldInfos infos) throws IOException {
+    int fieldNumber = meta.readVInt();
+    while (fieldNumber != -1) {
+      int fieldType = meta.readByte();
+      if (fieldType == Lucene42DocValuesConsumer.NUMBER) {
+        NumericEntry entry = new NumericEntry();
+        entry.offset = meta.readLong();
+        entry.format = meta.readByte();
+        if (entry.format != Lucene42DocValuesConsumer.UNCOMPRESSED) {
+          entry.packedIntsVersion = meta.readVInt();
+        }
+        numerics.put(fieldNumber, entry);
+      } else if (fieldType == Lucene42DocValuesConsumer.BYTES) {
+        BinaryEntry entry = new BinaryEntry();
+        entry.offset = meta.readLong();
+        entry.numBytes = meta.readLong();
+        entry.minLength = meta.readVInt();
+        entry.maxLength = meta.readVInt();
+        if (entry.minLength != entry.maxLength) {
+          entry.packedIntsVersion = meta.readVInt();
+          entry.blockSize = meta.readVInt();
+        }
+        binaries.put(fieldNumber, entry);
+      } else if (fieldType == Lucene42DocValuesConsumer.FST) {
+        FSTEntry entry = new FSTEntry();
+        entry.offset = meta.readLong();
+        entry.numOrds = meta.readVInt();
+        fsts.put(fieldNumber, entry);
+      } else {
+        throw new CorruptIndexException("invalid entry type: " + fieldType + ", input=" + meta);
+      }
+      fieldNumber = meta.readVInt();
+    }
+  }
+
+  @Override
+  public synchronized NumericDocValues getNumeric(FieldInfo field) throws IOException {
+    NumericDocValues instance = numericInstances.get(field.number);
+    if (instance == null) {
+      instance = loadNumeric(field);
+      numericInstances.put(field.number, instance);
+    }
+    return instance;
+  }
+  
+  private NumericDocValues loadNumeric(FieldInfo field) throws IOException {
+    NumericEntry entry = numerics.get(field.number);
+    data.seek(entry.offset);
+    if (entry.format == Lucene42DocValuesConsumer.TABLE_COMPRESSED) {
+      int size = data.readVInt();
+      final long decode[] = new long[size];
+      for (int i = 0; i < decode.length; i++) {
+        decode[i] = data.readLong();
+      }
+      final int formatID = data.readVInt();
+      final int bitsPerValue = data.readVInt();
+      final PackedInts.Reader reader = PackedInts.getReaderNoHeader(data, PackedInts.Format.byId(formatID), entry.packedIntsVersion, maxDoc, bitsPerValue);
+      return new NumericDocValues() {
+        @Override
+        public long get(int docID) {
+          return decode[(int)reader.get(docID)];
+        }
+      };
+    } else if (entry.format == Lucene42DocValuesConsumer.DELTA_COMPRESSED) {
+      final int blockSize = data.readVInt();
+      final BlockPackedReader reader = new BlockPackedReader(data, entry.packedIntsVersion, blockSize, maxDoc, false);
+      return new NumericDocValues() {
+        @Override
+        public long get(int docID) {
+          return reader.get(docID);
+        }
+      };
+    } else if (entry.format == Lucene42DocValuesConsumer.UNCOMPRESSED) {
+      final byte bytes[] = new byte[maxDoc];
+      data.readBytes(bytes, 0, bytes.length);
+      return new NumericDocValues() {
+        @Override
+        public long get(int docID) {
+          return bytes[docID];
+        }
+      };
+    } else {
+      throw new IllegalStateException();
+    }
+  }
+
+  @Override
+  public synchronized BinaryDocValues getBinary(FieldInfo field) throws IOException {
+    BinaryDocValues instance = binaryInstances.get(field.number);
+    if (instance == null) {
+      instance = loadBinary(field);
+      binaryInstances.put(field.number, instance);
+    }
+    return instance;
+  }
+  
+  private BinaryDocValues loadBinary(FieldInfo field) throws IOException {
+    BinaryEntry entry = binaries.get(field.number);
+    data.seek(entry.offset);
+    PagedBytes bytes = new PagedBytes(16);
+    bytes.copy(data, entry.numBytes);
+    final PagedBytes.Reader bytesReader = bytes.freeze(true);
+    if (entry.minLength == entry.maxLength) {
+      final int fixedLength = entry.minLength;
+      return new BinaryDocValues() {
+        @Override
+        public void get(int docID, BytesRef result) {
+          bytesReader.fillSlice(result, fixedLength * (long)docID, fixedLength);
+        }
+      };
+    } else {
+      final MonotonicBlockPackedReader addresses = new MonotonicBlockPackedReader(data, entry.packedIntsVersion, entry.blockSize, maxDoc, false);
+      return new BinaryDocValues() {
+        @Override
+        public void get(int docID, BytesRef result) {
+          long startAddress = docID == 0 ? 0 : addresses.get(docID-1);
+          long endAddress = addresses.get(docID); 
+          bytesReader.fillSlice(result, startAddress, (int) (endAddress - startAddress));
+        }
+      };
+    }
+  }
+  
+  @Override
+  public SortedDocValues getSorted(FieldInfo field) throws IOException {
+    final FSTEntry entry = fsts.get(field.number);
+    FST<Long> instance;
+    synchronized(this) {
+      instance = fstInstances.get(field.number);
+      if (instance == null) {
+        data.seek(entry.offset);
+        instance = new FST<Long>(data, PositiveIntOutputs.getSingleton(true));
+        fstInstances.put(field.number, instance);
+      }
+    }
+    final NumericDocValues docToOrd = getNumeric(field);
+    final FST<Long> fst = instance;
+    
+    // per-thread resources
+    final BytesReader in = fst.getBytesReader();
+    final Arc<Long> firstArc = new Arc<Long>();
+    final Arc<Long> scratchArc = new Arc<Long>();
+    final IntsRef scratchInts = new IntsRef();
+    final BytesRefFSTEnum<Long> fstEnum = new BytesRefFSTEnum<Long>(fst); 
+    
+    return new SortedDocValues() {
+      @Override
+      public int getOrd(int docID) {
+        return (int) docToOrd.get(docID);
+      }
+
+      @Override
+      public void lookupOrd(int ord, BytesRef result) {
+        try {
+          in.setPosition(0);
+          fst.getFirstArc(firstArc);
+          IntsRef output = Util.getByOutput(fst, ord, in, firstArc, scratchArc, scratchInts);
+          result.bytes = new byte[output.length];
+          result.offset = 0;
+          result.length = 0;
+          Util.toBytesRef(output, result);
+        } catch (IOException bogus) {
+          throw new RuntimeException(bogus);
+        }
+      }
+
+      @Override
+      public int lookupTerm(BytesRef key) {
+        try {
+          InputOutput<Long> o = fstEnum.seekCeil(key);
+          if (o == null) {
+            return -getValueCount()-1;
+          } else if (o.input.equals(key)) {
+            return o.output.intValue();
+          } else {
+            return (int) -o.output-1;
+          }
+        } catch (IOException bogus) {
+          throw new RuntimeException(bogus);
+        }
+      }
+
+      @Override
+      public int getValueCount() {
+        return entry.numOrds;
+      }
+    };
+  }
+  
+  @Override
+  public void close() throws IOException {
+    data.close();
+  }
+  
+  static class NumericEntry {
+    long offset;
+    byte format;
+    int packedIntsVersion;
+  }
+  
+  static class BinaryEntry {
+    long offset;
+    long numBytes;
+    int minLength;
+    int maxLength;
+    int packedIntsVersion;
+    int blockSize;
+  }
+  
+  static class FSTEntry {
+    long offset;
+    int numOrds;
+  }
+}

Property changes on: lucene/core/src/java/org/apache/lucene/codecs/lucene42/Lucene42DocValuesProducer.java
___________________________________________________________________
Added: svn:eol-style
## -0,0 +1 ##
+native
Index: lucene/core/src/java/org/apache/lucene/codecs/lucene42/Lucene42FieldInfosFormat.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/codecs/lucene42/Lucene42FieldInfosFormat.java	(revision 0)
+++ lucene/core/src/java/org/apache/lucene/codecs/lucene42/Lucene42FieldInfosFormat.java	(working copy)
@@ -0,0 +1,121 @@
+package org.apache.lucene.codecs.lucene42;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.codecs.CodecUtil;
+import org.apache.lucene.codecs.FieldInfosFormat;
+import org.apache.lucene.codecs.FieldInfosReader;
+import org.apache.lucene.codecs.FieldInfosWriter;
+import org.apache.lucene.index.FieldInfo.DocValuesType; // javadoc
+import org.apache.lucene.store.DataOutput; // javadoc
+
+/**
+ * Lucene 4.2 Field Infos format.
+ * <p>
+ * <p>Field names are stored in the field info file, with suffix <tt>.fnm</tt>.</p>
+ * <p>FieldInfos (.fnm) --&gt; Header,FieldsCount, &lt;FieldName,FieldNumber,
+ * FieldBits,DocValuesBits,Attributes&gt; <sup>FieldsCount</sup></p>
+ * <p>Data types:
+ * <ul>
+ *   <li>Header --&gt; {@link CodecUtil#checkHeader CodecHeader}</li>
+ *   <li>FieldsCount --&gt; {@link DataOutput#writeVInt VInt}</li>
+ *   <li>FieldName --&gt; {@link DataOutput#writeString String}</li>
+ *   <li>FieldBits, DocValuesBits --&gt; {@link DataOutput#writeByte Byte}</li>
+ *   <li>FieldNumber --&gt; {@link DataOutput#writeInt VInt}</li>
+ *   <li>Attributes --&gt; {@link DataOutput#writeStringStringMap Map&lt;String,String&gt;}</li>
+ * </ul>
+ * </p>
+ * Field Descriptions:
+ * <ul>
+ *   <li>FieldsCount: the number of fields in this file.</li>
+ *   <li>FieldName: name of the field as a UTF-8 String.</li>
+ *   <li>FieldNumber: the field's number. Note that unlike previous versions of
+ *       Lucene, the fields are not numbered implicitly by their order in the
+ *       file, instead explicitly.</li>
+ *   <li>FieldBits: a byte containing field options.
+ *       <ul>
+ *         <li>The low-order bit is one for indexed fields, and zero for non-indexed
+ *             fields.</li>
+ *         <li>The second lowest-order bit is one for fields that have term vectors
+ *             stored, and zero for fields without term vectors.</li>
+ *         <li>If the third lowest order-bit is set (0x4), offsets are stored into
+ *             the postings list in addition to positions.</li>
+ *         <li>Fourth bit is unused.</li>
+ *         <li>If the fifth lowest-order bit is set (0x10), norms are omitted for the
+ *             indexed field.</li>
+ *         <li>If the sixth lowest-order bit is set (0x20), payloads are stored for the
+ *             indexed field.</li>
+ *         <li>If the seventh lowest-order bit is set (0x40), term frequencies and
+ *             positions omitted for the indexed field.</li>
+ *         <li>If the eighth lowest-order bit is set (0x80), positions are omitted for the
+ *             indexed field.</li>
+ *       </ul>
+ *    </li>
+ *    <li>DocValuesBits: a byte containing per-document value types. The type
+ *        recorded as two four-bit integers, with the high-order bits representing
+ *        <code>norms</code> options, and the low-order bits representing 
+ *        {@code DocValues} options. Each four-bit integer can be decoded as such:
+ *        <ul>
+ *          <li>0: no DocValues for this field.</li>
+ *          <li>1: NumericDocValues. ({@link DocValuesType#NUMERIC})</li>
+ *          <li>2: BinaryDocValues. ({@code DocValuesType#BINARY})</li>
+ *          <li>3: SortedDocValues. ({@code DocValuesType#SORTED})</li>
+ *        </ul>
+ *    </li>
+ *    <li>Attributes: a key-value map of codec-private attributes.</li>
+ * </ul>
+ *
+ * @lucene.experimental
+ */
+public final class Lucene42FieldInfosFormat extends FieldInfosFormat {
+  private final FieldInfosReader reader = new Lucene42FieldInfosReader();
+  private final FieldInfosWriter writer = new Lucene42FieldInfosWriter();
+  
+  /** Sole constructor. */
+  public Lucene42FieldInfosFormat() {
+  }
+
+  @Override
+  public FieldInfosReader getFieldInfosReader() throws IOException {
+    return reader;
+  }
+
+  @Override
+  public FieldInfosWriter getFieldInfosWriter() throws IOException {
+    return writer;
+  }
+  
+  /** Extension of field infos */
+  static final String EXTENSION = "fnm";
+  
+  // Codec header
+  static final String CODEC_NAME = "Lucene42FieldInfos";
+  static final int FORMAT_START = 0;
+  static final int FORMAT_CURRENT = FORMAT_START;
+  
+  // Field flags
+  static final byte IS_INDEXED = 0x1;
+  static final byte STORE_TERMVECTOR = 0x2;
+  static final byte STORE_OFFSETS_IN_POSTINGS = 0x4;
+  static final byte OMIT_NORMS = 0x10;
+  static final byte STORE_PAYLOADS = 0x20;
+  static final byte OMIT_TERM_FREQ_AND_POSITIONS = 0x40;
+  static final byte OMIT_POSITIONS = -128;
+}

Property changes on: lucene/core/src/java/org/apache/lucene/codecs/lucene42/Lucene42FieldInfosFormat.java
___________________________________________________________________
Added: svn:eol-style
## -0,0 +1 ##
+native
Index: lucene/core/src/java/org/apache/lucene/codecs/lucene42/Lucene42DocValuesFormat.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/codecs/lucene42/Lucene42DocValuesFormat.java	(revision 0)
+++ lucene/core/src/java/org/apache/lucene/codecs/lucene42/Lucene42DocValuesFormat.java	(working copy)
@@ -0,0 +1,134 @@
+package org.apache.lucene.codecs.lucene42;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.codecs.CodecUtil;
+import org.apache.lucene.codecs.DocValuesConsumer;
+import org.apache.lucene.codecs.DocValuesProducer;
+import org.apache.lucene.codecs.DocValuesFormat;
+import org.apache.lucene.index.SegmentReadState;
+import org.apache.lucene.index.SegmentWriteState;
+import org.apache.lucene.store.DataOutput;
+import org.apache.lucene.util.fst.FST;
+import org.apache.lucene.util.packed.MonotonicBlockPackedWriter;
+import org.apache.lucene.util.packed.PackedInts;
+import org.apache.lucene.util.packed.BlockPackedWriter;
+
+/**
+ * Lucene 4.2 DocValues format.
+ * <p>
+ * Encodes the three per-document value types (Numeric,Binary,Sorted) with five basic strategies.
+ * <p>
+ * <ul>
+ *    <li>Delta-compressed Numerics: per-document integers written in blocks of 4096. For each block
+ *        the minimum value is encoded, and each entry is a delta from that minimum value.
+ *    <li>Table-compressed Numerics: when the number of unique values is very small, a lookup table
+ *        is written instead. Each per-document entry is instead the ordinal to this table.
+ *    <li>Uncompressed Numerics: when all values would fit into a single byte, and the 
+ *        <code>acceptableOverheadRatio</code> would pack values into 8 bits per value anyway, they
+ *        are written as absolute values (with no indirection or packing) for performance.
+ *    <li>Fixed-width Binary: one large concatenated byte[] is written, along with the fixed length.
+ *        Each document's value can be addressed by maxDoc*length. 
+ *    <li>Variable-width Binary: one large concatenated byte[] is written, along with end addresses 
+ *        for each document. The addresses are written in blocks of 4096, with the current absolute
+ *        start for the block, and the average (expected) delta per entry. For each document the 
+ *        deviation from the delta (actual - expected) is written.
+ *    <li>Sorted: an FST mapping deduplicated terms to ordinals is written, along with the per-document
+ *        ordinals written using one of the numeric stratgies above.
+ * </ul>
+ * <p>
+ * Files:
+ * <ol>
+ *   <li><tt>.dvd</tt>: DocValues data</li>
+ *   <li><tt>.dvm</tt>: DocValues metadata</li>
+ * </ol>
+ * <ol>
+ *   <li><a name="dvm" id="dvm"></a>
+ *   <p>The DocValues metadata or .dvm file.</p>
+ *   <p>For DocValues field, this stores metadata, such as the offset into the 
+ *      DocValues data (.dvd)</p>
+ *   <p>DocValues metadata (.dvm) --&gt; Header,&lt;FieldNumber,EntryType,Entry&gt;<sup>NumFields</sup></p>
+ *   <ul>
+ *     <li>Entry --&gt; NumericEntry | BinaryEntry | SortedEntry</li>
+ *     <li>NumericEntry --&gt; DataOffset,CompressionType,PackedVersion</li>
+ *     <li>BinaryEntry --&gt; DataOffset,DataLength,MinLength,MaxLength,PackedVersion?,BlockSize?</li>
+ *     <li>SortedEntry --&gt; DataOffset,ValueCount</li>
+ *     <li>FieldNumber,PackedVersion,MinLength,MaxLength,BlockSize,ValueCount --&gt; {@link DataOutput#writeVInt VInt}</li>
+ *     <li>DataOffset,DataLength --&gt; {@link DataOutput#writeLong Int64}</li>
+ *     <li>EntryType,CompressionType --&gt; {@link DataOutput#writeByte Byte}</li>
+ *     <li>Header --&gt; {@link CodecUtil#writeHeader CodecHeader}</li>
+ *   </ul>
+ *   <p>Sorted fields have two entries: a SortedEntry with the FST metadata,
+ *      and an ordinary NumericEntry for the document-to-ord metadata.</p>
+ *   <p>FieldNumber of -1 indicates the end of metadata.</p>
+ *   <p>EntryType is a 0 (NumericEntry), 1 (BinaryEntry, or 2 (SortedEntry)</p>
+ *   <p>DataOffset is the pointer to the start of the data in the DocValues data (.dvd)</p>
+ *   <p>CompressionType indicates how Numeric values will be compressed:
+ *      <ul>
+ *         <li>0 --&gt; delta-compressed. For each block of 4096 integers, every integer is delta-encoded
+ *             from the minimum value within the block. 
+ *         <li>1 --&gt; table-compressed. When the number of unique numeric values is small and it would save space,
+ *             a lookup table of unique values is written, followed by the ordinal for each document.
+ *         <li>2 --&gt; uncompressed. When the <code>acceptableOverheadRatio</code> parameter would upgrade the number
+ *             of bits required to 8, and all values fit in a byte, these are written as absolute binary values
+ *             for performance.
+ *      </ul>
+ *   <p>MinLength and MaxLength represent the min and max byte[] value lengths for Binary values.
+ *      If they are equal, then all values are of a fixed size, and can be addressed as DataOffset + (docID * length).
+ *      Otherwise, the binary values are of variable size, and packed integer metadata (PackedVersion,BlockSize)
+ *      is written for the addresses.
+ *   <li><a name="dvd" id="dvd"></a>
+ *   <p>The DocValues data or .dvd file.</p>
+ *   <p>For DocValues field, this stores the actual per-document data (the heavy-lifting)</p>
+ *   <p>DocValues data (.dvd) --&gt; Header,&lt;NumericData | BinaryData | SortedData&gt;<sup>NumFields</sup></p>
+ *   <ul>
+ *     <li>NumericData --&gt; DeltaCompressedNumerics | TableCompressedNumerics | UncompressedNumerics</li>
+ *     <li>BinaryData --&gt;  {@link DataOutput#writeByte Byte}<sup>DataLength</sup>,Addresses</li>
+ *     <li>SortedData --&gt; {@link FST FST&lt;Int64&gt;}</li>
+ *     <li>DeltaCompressedNumerics --&gt; {@link BlockPackedWriter BlockPackedInts(blockSize=4096)}</li>
+ *     <li>TableCompressedNumerics --&gt; TableSize,{@link DataOutput#writeLong Int64}<sup>TableSize</sup>,{@link PackedInts PackedInts}</li>
+ *     <li>UncompressedNumerics --&gt; {@link DataOutput#writeByte Byte}<sup>maxdoc</sup></li>
+ *     <li>Addresses --&gt; {@link MonotonicBlockPackedWriter MonotonicBlockPackedInts(blockSize=4096)}</li>
+ *   </ul>
+ * </ol>
+ */
+public final class Lucene42DocValuesFormat extends DocValuesFormat {
+
+  /** Sole constructor */
+  public Lucene42DocValuesFormat() {
+    super("Lucene42");
+  }
+
+  @Override
+  public DocValuesConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
+    // note: we choose DEFAULT here (its reasonably fast, and for small bpv has tiny waste)
+    return new Lucene42DocValuesConsumer(state, DATA_CODEC, DATA_EXTENSION, METADATA_CODEC, METADATA_EXTENSION, PackedInts.DEFAULT);
+  }
+  
+  @Override
+  public DocValuesProducer fieldsProducer(SegmentReadState state) throws IOException {
+    return new Lucene42DocValuesProducer(state, DATA_CODEC, DATA_EXTENSION, METADATA_CODEC, METADATA_EXTENSION);
+  }
+  
+  private static final String DATA_CODEC = "Lucene42DocValuesData";
+  private static final String DATA_EXTENSION = "dvd";
+  private static final String METADATA_CODEC = "Lucene42DocValuesMetadata";
+  private static final String METADATA_EXTENSION = "dvm";
+}

Property changes on: lucene/core/src/java/org/apache/lucene/codecs/lucene42/Lucene42DocValuesFormat.java
___________________________________________________________________
Added: svn:eol-style
## -0,0 +1 ##
+native
Index: lucene/core/src/java/org/apache/lucene/codecs/lucene42/Lucene42DocValuesConsumer.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/codecs/lucene42/Lucene42DocValuesConsumer.java	(revision 0)
+++ lucene/core/src/java/org/apache/lucene/codecs/lucene42/Lucene42DocValuesConsumer.java	(working copy)
@@ -0,0 +1,220 @@
+package org.apache.lucene.codecs.lucene42;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.HashMap;
+import java.util.HashSet;
+
+import org.apache.lucene.codecs.CodecUtil;
+import org.apache.lucene.codecs.DocValuesConsumer;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.SegmentWriteState;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util.IntsRef;
+import org.apache.lucene.util.fst.Builder;
+import org.apache.lucene.util.fst.FST;
+import org.apache.lucene.util.fst.FST.INPUT_TYPE;
+import org.apache.lucene.util.fst.PositiveIntOutputs;
+import org.apache.lucene.util.fst.Util;
+import org.apache.lucene.util.packed.BlockPackedWriter;
+import org.apache.lucene.util.packed.MonotonicBlockPackedWriter;
+import org.apache.lucene.util.packed.PackedInts;
+import org.apache.lucene.util.packed.PackedInts.FormatAndBits;
+
+/**
+ * Writer for {@link Lucene42DocValuesFormat}
+ */
+class Lucene42DocValuesConsumer extends DocValuesConsumer {
+  static final int VERSION_START = 0;
+  static final int VERSION_CURRENT = VERSION_START;
+  
+  static final byte NUMBER = 0;
+  static final byte BYTES = 1;
+  static final byte FST = 2;
+
+  static final int BLOCK_SIZE = 4096;
+  
+  static final byte DELTA_COMPRESSED = 0;
+  static final byte TABLE_COMPRESSED = 1;
+  static final byte UNCOMPRESSED = 2;
+
+  final IndexOutput data, meta;
+  final int maxDoc;
+  final float acceptableOverheadRatio;
+  
+  Lucene42DocValuesConsumer(SegmentWriteState state, String dataCodec, String dataExtension, String metaCodec, String metaExtension, float acceptableOverheadRatio) throws IOException {
+    this.acceptableOverheadRatio = acceptableOverheadRatio;
+    maxDoc = state.segmentInfo.getDocCount();
+    boolean success = false;
+    try {
+      String dataName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, dataExtension);
+      data = state.directory.createOutput(dataName, state.context);
+      CodecUtil.writeHeader(data, dataCodec, VERSION_CURRENT);
+      String metaName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, metaExtension);
+      meta = state.directory.createOutput(metaName, state.context);
+      CodecUtil.writeHeader(meta, metaCodec, VERSION_CURRENT);
+      success = true;
+    } finally {
+      if (!success) {
+        IOUtils.closeWhileHandlingException(this);
+      }
+    }
+  }
+  
+  @Override
+  public void addNumericField(FieldInfo field, Iterable<Number> values) throws IOException {
+    meta.writeVInt(field.number);
+    meta.writeByte(NUMBER);
+    meta.writeLong(data.getFilePointer());
+    long minValue = Long.MAX_VALUE;
+    long maxValue = Long.MIN_VALUE;
+    // TODO: more efficient?
+    HashSet<Long> uniqueValues = new HashSet<Long>();
+    for(Number nv : values) {
+      long v = nv.longValue();
+      minValue = Math.min(minValue, v);
+      maxValue = Math.max(maxValue, v);
+      if (uniqueValues != null) {
+        if (uniqueValues.add(v)) {
+          if (uniqueValues.size() > 256) {
+            uniqueValues = null;
+          }
+        }
+      }
+    }
+
+    if (uniqueValues != null) {
+      // small number of unique values
+      final int bitsPerValue = PackedInts.bitsRequired(uniqueValues.size()-1);
+      FormatAndBits formatAndBits = PackedInts.fastestFormatAndBits(maxDoc, bitsPerValue, acceptableOverheadRatio);
+      if (formatAndBits.bitsPerValue == 8 && minValue >= Byte.MIN_VALUE && maxValue <= Byte.MAX_VALUE) {
+        meta.writeByte(UNCOMPRESSED); // uncompressed
+        for (Number nv : values) {
+          data.writeByte((byte) nv.longValue());
+        }
+      } else {
+        meta.writeByte(TABLE_COMPRESSED); // table-compressed
+        Long[] decode = uniqueValues.toArray(new Long[uniqueValues.size()]);
+        final HashMap<Long,Integer> encode = new HashMap<Long,Integer>();
+        data.writeVInt(decode.length);
+        for (int i = 0; i < decode.length; i++) {
+          data.writeLong(decode[i]);
+          encode.put(decode[i], i);
+        }
+
+        meta.writeVInt(PackedInts.VERSION_CURRENT);
+        data.writeVInt(formatAndBits.format.getId());
+        data.writeVInt(formatAndBits.bitsPerValue);
+
+        final PackedInts.Writer writer = PackedInts.getWriterNoHeader(data, formatAndBits.format, maxDoc, formatAndBits.bitsPerValue, PackedInts.DEFAULT_BUFFER_SIZE);
+        for(Number nv : values) {
+          writer.add(encode.get(nv.longValue()));
+        }
+        writer.finish();
+      }
+    } else {
+      meta.writeByte(DELTA_COMPRESSED); // delta-compressed
+
+      meta.writeVInt(PackedInts.VERSION_CURRENT);
+      data.writeVInt(BLOCK_SIZE);
+
+      final BlockPackedWriter writer = new BlockPackedWriter(data, BLOCK_SIZE);
+      for (Number nv : values) {
+        writer.add(nv.longValue());
+      }
+      writer.finish();
+    }
+  }
+  
+  @Override
+  public void close() throws IOException {
+    boolean success = false;
+    try {
+      if (meta != null) {
+        meta.writeVInt(-1); // write EOF marker
+      }
+      success = true;
+    } finally {
+      if (success) {
+        IOUtils.close(data, meta);
+      } else {
+        IOUtils.closeWhileHandlingException(data, meta);
+      }
+    }
+  }
+
+  @Override
+  public void addBinaryField(FieldInfo field, final Iterable<BytesRef> values) throws IOException {
+    // write the byte[] data
+    meta.writeVInt(field.number);
+    meta.writeByte(BYTES);
+    int minLength = Integer.MAX_VALUE;
+    int maxLength = Integer.MIN_VALUE;
+    final long startFP = data.getFilePointer();
+    for(BytesRef v : values) {
+      minLength = Math.min(minLength, v.length);
+      maxLength = Math.max(maxLength, v.length);
+      data.writeBytes(v.bytes, v.offset, v.length);
+    }
+    meta.writeLong(startFP);
+    meta.writeLong(data.getFilePointer() - startFP);
+    meta.writeVInt(minLength);
+    meta.writeVInt(maxLength);
+    
+    // if minLength == maxLength, its a fixed-length byte[], we are done (the addresses are implicit)
+    // otherwise, we need to record the length fields...
+    if (minLength != maxLength) {
+      meta.writeVInt(PackedInts.VERSION_CURRENT);
+      meta.writeVInt(BLOCK_SIZE);
+
+      final MonotonicBlockPackedWriter writer = new MonotonicBlockPackedWriter(data, BLOCK_SIZE);
+      long addr = 0;
+      for (BytesRef v : values) {
+        addr += v.length;
+        writer.add(addr);
+      }
+      writer.finish();
+    }
+  }
+
+  @Override
+  public void addSortedField(FieldInfo field, Iterable<BytesRef> values, Iterable<Number> docToOrd) throws IOException {
+    // write the ordinals as numerics
+    addNumericField(field, docToOrd);
+    
+    // write the values as FST
+    meta.writeVInt(field.number);
+    meta.writeByte(FST);
+    meta.writeLong(data.getFilePointer());
+    PositiveIntOutputs outputs = PositiveIntOutputs.getSingleton(true);
+    Builder<Long> builder = new Builder<Long>(INPUT_TYPE.BYTE1, outputs);
+    IntsRef scratch = new IntsRef();
+    long ord = 0;
+    for (BytesRef v : values) {
+      builder.add(Util.toIntsRef(v, scratch), ord);
+      ord++;
+    }
+    FST<Long> fst = builder.finish();
+    fst.save(data);
+    meta.writeVInt((int)ord);
+  }
+}

Property changes on: lucene/core/src/java/org/apache/lucene/codecs/lucene42/Lucene42DocValuesConsumer.java
___________________________________________________________________
Added: svn:eol-style
## -0,0 +1 ##
+native
Index: lucene/core/src/java/org/apache/lucene/codecs/DocValuesConsumer.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/codecs/DocValuesConsumer.java	(revision 1442822)
+++ lucene/core/src/java/org/apache/lucene/codecs/DocValuesConsumer.java	(working copy)
@@ -16,235 +16,478 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
+
+import java.io.Closeable;
 import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Iterator;
+import java.util.List;
+import java.util.NoSuchElementException;
 
-import org.apache.lucene.document.ByteDocValuesField;
-import org.apache.lucene.document.DerefBytesDocValuesField;
-import org.apache.lucene.document.DoubleDocValuesField;
-import org.apache.lucene.document.Field;
-import org.apache.lucene.document.FloatDocValuesField;
-import org.apache.lucene.document.IntDocValuesField;
-import org.apache.lucene.document.LongDocValuesField;
-import org.apache.lucene.document.PackedLongDocValuesField;
-import org.apache.lucene.document.ShortDocValuesField;
-import org.apache.lucene.document.SortedBytesDocValuesField;
-import org.apache.lucene.document.StoredField;
-import org.apache.lucene.document.StraightBytesDocValuesField;
 import org.apache.lucene.index.AtomicReader;
-import org.apache.lucene.index.DocValues.Source;
-import org.apache.lucene.index.DocValues.Type;
-import org.apache.lucene.index.DocValues;
-import org.apache.lucene.index.IndexableField;
-import org.apache.lucene.index.StorableField;
+import org.apache.lucene.index.BinaryDocValues;
+import org.apache.lucene.index.FieldInfo;
 import org.apache.lucene.index.MergeState;
+import org.apache.lucene.index.NumericDocValues;
+import org.apache.lucene.index.SegmentWriteState;
+import org.apache.lucene.index.SortedDocValues;
 import org.apache.lucene.util.Bits;
 import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.FixedBitSet;
+import org.apache.lucene.util.PriorityQueue;
+import org.apache.lucene.util.packed.AppendingLongBuffer;
 
-/**
- * Abstract API that consumes {@link StorableField}s.
- * {@link DocValuesConsumer} are always associated with a specific field and
- * segments. Concrete implementations of this API write the given
- * {@link StorableField} into a implementation specific format depending on
- * the fields meta-data.
- * 
+/** 
+ * Abstract API that consumes numeric, binary and
+ * sorted docvalues.  Concrete implementations of this
+ * actually do "something" with the docvalues (write it into
+ * the index in a specific format).
+ * <p>
+ * The lifecycle is:
+ * <ol>
+ *   <li>DocValuesConsumer is created by 
+ *       {@link DocValuesFormat#fieldsConsumer(SegmentWriteState)} or
+ *       {@link NormsFormat#normsConsumer(SegmentWriteState)}.
+ *   <li>{@link #addNumericField}, {@link #addBinaryField},
+ *       or {@link #addSortedField} are called for each Numeric,
+ *       Binary, or Sorted docvalues field. The API is a "pull" rather
+ *       than "push", and the implementation is free to iterate over the 
+ *       values multiple times ({@link Iterable#iterator()}).
+ *   <li>After all fields are added, the consumer is {@link #close}d.
+ * </ol>
+ *
  * @lucene.experimental
  */
-public abstract class DocValuesConsumer {
-
-  /** Spare {@link BytesRef} that subclasses can reuse. */
-  protected final BytesRef spare = new BytesRef();
-
-  /** Returns the {@link Type} of this consumer. */
-  protected abstract Type getType();
-
+public abstract class DocValuesConsumer implements Closeable {
+  
   /** Sole constructor. (For invocation by subclass 
    *  constructors, typically implicit.) */
-  protected DocValuesConsumer() {
-  }
+  protected DocValuesConsumer() {}
 
   /**
-   * Adds the given {@link StorableField} instance to this
-   * {@link DocValuesConsumer}
-   * 
-   * @param docID
-   *          the document ID to add the value for. The docID must always
-   *          increase or be <tt>0</tt> if it is the first call to this method.
-   * @param value
-   *          the value to add
-   * @throws IOException
-   *           if an {@link IOException} occurs
+   * Writes numeric docvalues for a field.
+   * @param field field information
+   * @param values Iterable of numeric values (one for each document).
+   * @throws IOException if an I/O error occurred.
    */
-  public abstract void add(int docID, StorableField value)
-      throws IOException;
+  public abstract void addNumericField(FieldInfo field, Iterable<Number> values) throws IOException;    
 
   /**
-   * Called when the consumer of this API is done adding values.
-   * 
-   * @param docCount
-   *          the total number of documents in this {@link DocValuesConsumer}.
-   *          Must be greater than or equal the last given docID to
-   *          {@link #add(int, StorableField)}.
-   * @throws IOException If an I/O error occurs
+   * Writes binary docvalues for a field.
+   * @param field field information
+   * @param values Iterable of binary values (one for each document).
+   * @throws IOException if an I/O error occurred.
    */
-  public abstract void finish(int docCount) throws IOException;
-  
-  
+  public abstract void addBinaryField(FieldInfo field, Iterable<BytesRef> values) throws IOException;
+
   /**
-   * Returns the value size this consumer accepts or <tt>-1</tt> iff this
-   * consumer is value size agnostic ie. accepts variable length values.
+   * Writes pre-sorted binary docvalues for a field.
+   * @param field field information
+   * @param values Iterable of binary values in sorted order (deduplicated).
+   * @param docToOrd Iterable of ordinals (one for each document).
+   * @throws IOException if an I/O error occurred.
+   */
+  public abstract void addSortedField(FieldInfo field, Iterable<BytesRef> values, Iterable<Number> docToOrd) throws IOException;
+
+  /**
+   * Merges the numeric docvalues from <code>toMerge</code>.
    * <p>
-   * NOTE: the return value is undefined until the consumer has successfully
-   * consumed at least one value.
-   * 
-   * @return the value size this consumer accepts or <tt>-1</tt> iff this
-   *         consumer is value size agnostic ie. accepts variable length values.
+   * The default implementation calls {@link #addNumericField}, passing
+   * an Iterable that merges and filters deleted documents on the fly.
    */
-  public abstract int getValueSize();
+  public void mergeNumericField(FieldInfo fieldInfo, final MergeState mergeState, final List<NumericDocValues> toMerge) throws IOException {
+
+    addNumericField(fieldInfo,
+                    new Iterable<Number>() {
+                      @Override
+                      public Iterator<Number> iterator() {
+                        return new Iterator<Number>() {
+                          int readerUpto = -1;
+                          int docIDUpto;
+                          long nextValue;
+                          AtomicReader currentReader;
+                          NumericDocValues currentValues;
+                          Bits currentLiveDocs;
+                          boolean nextIsSet;
+
+                          @Override
+                          public boolean hasNext() {
+                            return nextIsSet || setNext();
+                          }
+
+                          @Override
+                          public void remove() {
+                            throw new UnsupportedOperationException();
+                          }
+
+                          @Override
+                          public Number next() {
+                            if (!hasNext()) {
+                              throw new NoSuchElementException();
+                            }
+                            assert nextIsSet;
+                            nextIsSet = false;
+                            // TODO: make a mutable number
+                            return nextValue;
+                          }
+
+                          private boolean setNext() {
+                            while (true) {
+                              if (readerUpto == toMerge.size()) {
+                                return false;
+                              }
+
+                              if (currentReader == null || docIDUpto == currentReader.maxDoc()) {
+                                readerUpto++;
+                                if (readerUpto < toMerge.size()) {
+                                  currentReader = mergeState.readers.get(readerUpto);
+                                  currentValues = toMerge.get(readerUpto);
+                                  currentLiveDocs = currentReader.getLiveDocs();
+                                }
+                                docIDUpto = 0;
+                                continue;
+                              }
+
+                              if (currentLiveDocs == null || currentLiveDocs.get(docIDUpto)) {
+                                nextIsSet = true;
+                                nextValue = currentValues.get(docIDUpto);
+                                docIDUpto++;
+                                return true;
+                              }
+
+                              docIDUpto++;
+                            }
+                          }
+                        };
+                      }
+                    });
+  }
   
   /**
-   * Merges the given {@link org.apache.lucene.index.MergeState} into
-   * this {@link DocValuesConsumer}.
-   * 
-   * @param mergeState
-   *          the state to merge
-   * @param docValues docValues array containing one instance per reader (
-   *          {@link org.apache.lucene.index.MergeState#readers}) or <code>null</code> if the reader has
-   *          no {@link DocValues} instance.
-   * @throws IOException
-   *           if an {@link IOException} occurs
+   * Merges the binary docvalues from <code>toMerge</code>.
+   * <p>
+   * The default implementation calls {@link #addBinaryField}, passing
+   * an Iterable that merges and filters deleted documents on the fly.
    */
-  public void merge(MergeState mergeState, DocValues[] docValues) throws IOException {
-    assert mergeState != null;
-    boolean hasMerged = false;
-    for(int readerIDX=0;readerIDX<mergeState.readers.size();readerIDX++) {
-      final AtomicReader reader = mergeState.readers.get(readerIDX);
-      if (docValues[readerIDX] != null) {
-        hasMerged = true;
-        merge(docValues[readerIDX], mergeState.docBase[readerIDX],
-              reader.maxDoc(), reader.getLiveDocs());
-        mergeState.checkAbort.work(reader.maxDoc());
+  public void mergeBinaryField(FieldInfo fieldInfo, final MergeState mergeState, final List<BinaryDocValues> toMerge) throws IOException {
+
+    addBinaryField(fieldInfo,
+                   new Iterable<BytesRef>() {
+                     @Override
+                     public Iterator<BytesRef> iterator() {
+                       return new Iterator<BytesRef>() {
+                         int readerUpto = -1;
+                         int docIDUpto;
+                         BytesRef nextValue = new BytesRef();
+                         AtomicReader currentReader;
+                         BinaryDocValues currentValues;
+                         Bits currentLiveDocs;
+                         boolean nextIsSet;
+
+                         @Override
+                         public boolean hasNext() {
+                           return nextIsSet || setNext();
+                         }
+
+                         @Override
+                         public void remove() {
+                           throw new UnsupportedOperationException();
+                         }
+
+                         @Override
+                         public BytesRef next() {
+                           if (!hasNext()) {
+                             throw new NoSuchElementException();
+                           }
+                           assert nextIsSet;
+                           nextIsSet = false;
+                           // TODO: make a mutable number
+                           return nextValue;
+                         }
+
+                         private boolean setNext() {
+                           while (true) {
+                             if (readerUpto == toMerge.size()) {
+                               return false;
+                             }
+
+                             if (currentReader == null || docIDUpto == currentReader.maxDoc()) {
+                               readerUpto++;
+                               if (readerUpto < toMerge.size()) {
+                                 currentReader = mergeState.readers.get(readerUpto);
+                                 currentValues = toMerge.get(readerUpto);
+                                 currentLiveDocs = currentReader.getLiveDocs();
+                               }
+                               docIDUpto = 0;
+                               continue;
+                             }
+
+                             if (currentLiveDocs == null || currentLiveDocs.get(docIDUpto)) {
+                               nextIsSet = true;
+                               currentValues.get(docIDUpto, nextValue);
+                               docIDUpto++;
+                               return true;
+                             }
+
+                             docIDUpto++;
+                           }
+                         }
+                       };
+                     }
+                   });
+  }
+
+  static class SortedBytesMerger {
+
+    public int numMergedTerms;
+
+    final AppendingLongBuffer ordToReaderId = new AppendingLongBuffer();
+    final List<SegmentState> segStates = new ArrayList<SegmentState>();
+
+    private static class SegmentState {
+      int segmentID;
+      AtomicReader reader;
+      FixedBitSet liveTerms;
+      int ord = -1;
+      SortedDocValues values;
+      BytesRef scratch = new BytesRef();
+      AppendingLongBuffer ordDeltas = new AppendingLongBuffer();
+
+      // TODO: use another scheme?
+      // currently we +/- delta merged-ord from segment-ord (is this good? makes sense to me?)
+      // but we have a good idea "roughly" what
+      // the ord should be (linear projection) so we only
+      // need to encode the delta from that ...:        
+      AppendingLongBuffer segOrdToMergedOrd = new AppendingLongBuffer();
+
+      public BytesRef nextTerm() {
+        while (ord < values.getValueCount()-1) {
+          ord++;
+          if (liveTerms == null || liveTerms.get(ord)) {
+            values.lookupOrd(ord, scratch);
+            return scratch;
+          }
+        }
+
+        return null;
       }
     }
-    // only finish if no exception is thrown!
-    if (hasMerged) {
-      finish(mergeState.segmentInfo.getDocCount());
+
+    private static class TermMergeQueue extends PriorityQueue<SegmentState> {
+      public TermMergeQueue(int maxSize) {
+        super(maxSize);
+      }
+
+      @Override
+      protected boolean lessThan(SegmentState a, SegmentState b) {
+        return a.scratch.compareTo(b.scratch) <= 0;
+      }
     }
-  }
 
-  /**
-   * Merges the given {@link DocValues} into this {@link DocValuesConsumer}.
-   * 
-   * @throws IOException
-   *           if an {@link IOException} occurs
-   */
-  protected void merge(DocValues reader, int docBase, int docCount, Bits liveDocs) throws IOException {
-    // This enables bulk copies in subclasses per MergeState, subclasses can
-    // simply override this and decide if they want to merge
-    // segments using this generic implementation or if a bulk merge is possible
-    // / feasible.
-    final Source source = reader.getDirectSource();
-    assert source != null;
-    int docID = docBase;
-    final Type type = getType();
-    final StoredField scratchField;
-    switch(type) {
-    case VAR_INTS:
-      scratchField = new PackedLongDocValuesField("", (long) 0);
-      break;
-    case FIXED_INTS_8:
-      scratchField = new ByteDocValuesField("", (byte) 0);
-      break;
-    case FIXED_INTS_16:
-      scratchField = new ShortDocValuesField("", (short) 0);
-      break;
-    case FIXED_INTS_32:
-      scratchField = new IntDocValuesField("", 0);
-      break;
-    case FIXED_INTS_64:
-      scratchField = new LongDocValuesField("", (long) 0);
-      break;
-    case FLOAT_32:
-      scratchField = new FloatDocValuesField("", 0f);
-      break;
-    case FLOAT_64:
-      scratchField = new DoubleDocValuesField("", 0d);
-      break;
-    case BYTES_FIXED_STRAIGHT:
-      scratchField = new StraightBytesDocValuesField("", new BytesRef(), true);
-      break;
-    case BYTES_VAR_STRAIGHT:
-      scratchField = new StraightBytesDocValuesField("", new BytesRef(), false);
-      break;
-    case BYTES_FIXED_DEREF:
-      scratchField = new DerefBytesDocValuesField("", new BytesRef(), true);
-      break;
-    case BYTES_VAR_DEREF:
-      scratchField = new DerefBytesDocValuesField("", new BytesRef(), false);
-      break;
-    case BYTES_FIXED_SORTED:
-      scratchField = new SortedBytesDocValuesField("", new BytesRef(), true);
-      break;
-    case BYTES_VAR_SORTED:
-      scratchField = new SortedBytesDocValuesField("", new BytesRef(), false);
-      break;
-    default:
-      throw new IllegalStateException("unknown Type: " + type);
-    }
-    for (int i = 0; i < docCount; i++) {
-      if (liveDocs == null || liveDocs.get(i)) {
-        mergeDoc(scratchField, source, docID++, i);
+    public void merge(MergeState mergeState, List<SortedDocValues> toMerge) throws IOException {
+
+      // First pass: mark "live" terms
+      for (int readerIDX=0;readerIDX<toMerge.size();readerIDX++) {
+        AtomicReader reader = mergeState.readers.get(readerIDX);      
+        int maxDoc = reader.maxDoc();
+
+        SegmentState state = new SegmentState();
+        state.segmentID = readerIDX;
+        state.reader = reader;
+        state.values = toMerge.get(readerIDX);
+
+        segStates.add(state);
+        assert state.values.getValueCount() < Integer.MAX_VALUE;
+        if (reader.hasDeletions()) {
+          state.liveTerms = new FixedBitSet(state.values.getValueCount());
+          Bits liveDocs = reader.getLiveDocs();
+          assert liveDocs != null;
+          for(int docID=0;docID<maxDoc;docID++) {
+            if (liveDocs.get(docID)) {
+              state.liveTerms.set(state.values.getOrd(docID));
+            }
+          }
+        }
+
+        // TODO: we can unload the bits/packed ints to disk to reduce
+        // transient ram spike... most of these just require iterators
       }
+
+      // Second pass: merge only the live terms
+
+      TermMergeQueue q = new TermMergeQueue(segStates.size());
+      for(SegmentState segState : segStates) {
+        if (segState.nextTerm() != null) {
+          q.add(segState);
+        }
+      }
+
+      int lastOrds[] = new int[segStates.size()];
+      BytesRef lastTerm = null;
+      int ord = 0;
+      while (q.size() != 0) {
+        SegmentState top = q.top();
+        if (lastTerm == null || !lastTerm.equals(top.scratch)) {
+          // a new unique term: record its segment ID / sourceOrd pair
+          int readerId = top.segmentID;
+          ordToReaderId.add(readerId);
+
+          int sourceOrd = top.ord;             
+          int delta = sourceOrd - lastOrds[readerId];
+          lastOrds[readerId] = sourceOrd;
+          top.ordDeltas.add(delta);
+          
+          if (lastTerm == null) {
+            lastTerm = BytesRef.deepCopyOf(top.scratch);
+          } else {
+            lastTerm.copyBytes(top.scratch);
+          }
+          ord++;
+        }
+
+        long signedDelta = (ord-1) - top.ord; // global ord space - segment ord space
+        // fill in any holes for unused ords, then finally the value we want (segOrdToMergedOrd[top.ord])
+        // TODO: is there a better way...
+        while (top.segOrdToMergedOrd.size() <= top.ord) {
+          top.segOrdToMergedOrd.add(signedDelta);
+        }
+        if (top.nextTerm() == null) {
+          q.pop();
+        } else {
+          q.updateTop();
+        }
+      }
+
+      numMergedTerms = ord;
+      // clear our bitsets for GC: we dont need them anymore (e.g. while flushing merged stuff to codec)
+      for (SegmentState state : segStates) {
+        state.liveTerms = null;
+      }
     }
   }
 
   /**
-   * Merges a document with the given <code>docID</code>. The methods
-   * implementation obtains the value for the <i>sourceDoc</i> id from the
-   * current {@link Source}.
+   * Merges the sorted docvalues from <code>toMerge</code>.
    * <p>
-   * This method is used during merging to provide implementation agnostic
-   * default merge implementation.
-   * </p>
-   * <p>
-   * All documents IDs between the given ID and the previously given ID or
-   * <tt>0</tt> if the method is call the first time are filled with default
-   * values depending on the implementation. The given document
-   * ID must always be greater than the previous ID or <tt>0</tt> if called the
-   * first time.
+   * The default implementation calls {@link #addSortedField}, passing
+   * an Iterable that merges ordinals and values and filters deleted documents .
    */
-  protected void mergeDoc(StoredField scratchField, Source source, int docID, int sourceDoc)
-      throws IOException {
-    switch(getType()) {
-    case BYTES_FIXED_DEREF:
-    case BYTES_FIXED_SORTED:
-    case BYTES_FIXED_STRAIGHT:
-    case BYTES_VAR_DEREF:
-    case BYTES_VAR_SORTED:
-    case BYTES_VAR_STRAIGHT:
-      scratchField.setBytesValue(source.getBytes(sourceDoc, spare));
-      break;
-    case FIXED_INTS_8:
-      scratchField.setByteValue((byte) source.getInt(sourceDoc));
-      break;
-    case FIXED_INTS_16:
-      scratchField.setShortValue((short) source.getInt(sourceDoc));
-      break;
-    case FIXED_INTS_32:
-      scratchField.setIntValue((int) source.getInt(sourceDoc));
-      break;
-    case FIXED_INTS_64:
-      scratchField.setLongValue(source.getInt(sourceDoc));
-      break;
-    case VAR_INTS:
-      scratchField.setLongValue(source.getInt(sourceDoc));
-      break;
-    case FLOAT_32:
-      scratchField.setFloatValue((float) source.getFloat(sourceDoc));
-      break;
-    case FLOAT_64:
-      scratchField.setDoubleValue(source.getFloat(sourceDoc));
-      break;
-    }
-    add(docID, scratchField);
+  public void mergeSortedField(FieldInfo fieldInfo, final MergeState mergeState, List<SortedDocValues> toMerge) throws IOException {
+    final SortedBytesMerger merger = new SortedBytesMerger();
+
+    // Does the heavy lifting to merge sort all "live" ords:
+    merger.merge(mergeState, toMerge);
+
+    addSortedField(fieldInfo,
+
+                   // ord -> value
+                   new Iterable<BytesRef>() {
+                     @Override
+                     public Iterator<BytesRef> iterator() {
+                       // for each next(), tells us what reader to go to
+                       final AppendingLongBuffer.Iterator readerIDs = merger.ordToReaderId.iterator();
+                       // for each next(), gives us the original ord
+                       final AppendingLongBuffer.Iterator ordDeltas[] = new AppendingLongBuffer.Iterator[merger.segStates.size()];
+                       final int lastOrds[] = new int[ordDeltas.length];
+                       
+                       for (int i = 0; i < ordDeltas.length; i++) {
+                         ordDeltas[i] = merger.segStates.get(i).ordDeltas.iterator();
+                       }
+
+                       final BytesRef scratch = new BytesRef();
+                       
+                       return new Iterator<BytesRef>() {
+                         int ordUpto;
+
+                         @Override
+                         public boolean hasNext() {
+                           return ordUpto < merger.numMergedTerms;
+                         }
+
+                         @Override
+                         public void remove() {
+                           throw new UnsupportedOperationException();
+                         }
+
+                         @Override
+                         public BytesRef next() {
+                           if (!hasNext()) {
+                             throw new NoSuchElementException();
+                           }
+                           int readerID = (int) readerIDs.next();
+                           int ord = lastOrds[readerID] + (int) ordDeltas[readerID].next();
+                           merger.segStates.get(readerID).values.lookupOrd(ord, scratch);
+                           lastOrds[readerID] = ord;
+                           ordUpto++;
+                           return scratch;
+                         }
+                       };
+                     }
+                   },
+
+                   // doc -> ord
+                    new Iterable<Number>() {
+                      @Override
+                      public Iterator<Number> iterator() {
+                        return new Iterator<Number>() {
+                          int readerUpto = -1;
+                          int docIDUpto;
+                          int nextValue;
+                          SortedBytesMerger.SegmentState currentReader;
+                          Bits currentLiveDocs;
+                          boolean nextIsSet;
+
+                          @Override
+                          public boolean hasNext() {
+                            return nextIsSet || setNext();
+                          }
+
+                          @Override
+                          public void remove() {
+                            throw new UnsupportedOperationException();
+                          }
+
+                          @Override
+                          public Number next() {
+                            if (!hasNext()) {
+                              throw new NoSuchElementException();
+                            }
+                            assert nextIsSet;
+                            nextIsSet = false;
+                            // TODO make a mutable number
+                            return nextValue;
+                          }
+
+                          private boolean setNext() {
+                            while (true) {
+                              if (readerUpto == merger.segStates.size()) {
+                                return false;
+                              }
+
+                              if (currentReader == null || docIDUpto == currentReader.reader.maxDoc()) {
+                                readerUpto++;
+                                if (readerUpto < merger.segStates.size()) {
+                                  currentReader = merger.segStates.get(readerUpto);
+                                  currentLiveDocs = currentReader.reader.getLiveDocs();
+                                }
+                                docIDUpto = 0;
+                                continue;
+                              }
+
+                              if (currentLiveDocs == null || currentLiveDocs.get(docIDUpto)) {
+                                nextIsSet = true;
+                                int segOrd = currentReader.values.getOrd(docIDUpto);
+                                nextValue = (int) (segOrd + currentReader.segOrdToMergedOrd.get(segOrd));
+                                docIDUpto++;
+                                return true;
+                              }
+
+                              docIDUpto++;
+                            }
+                          }
+                        };
+                      }
+                    });
+
   }
 }
Index: lucene/core/src/java/org/apache/lucene/codecs/DocValuesArraySource.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/codecs/DocValuesArraySource.java	(revision 1442822)
+++ lucene/core/src/java/org/apache/lucene/codecs/DocValuesArraySource.java	(working copy)
@@ -1,545 +0,0 @@
-package org.apache.lucene.codecs;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to You under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-
-import java.io.IOException;
-import java.util.Collections;
-import java.util.EnumMap;
-import java.util.Map;
-
-import org.apache.lucene.index.DocValues.Source;
-import org.apache.lucene.index.DocValues.Type;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.RamUsageEstimator;
-
-/**
- * DocValues {@link Source} implementation backed by
- * simple arrays.
- * 
- * @lucene.experimental
- * @lucene.internal
- */
-public abstract class DocValuesArraySource extends Source {
-
-  private static final Map<Type, DocValuesArraySource> TEMPLATES;
-
-  static {
-    EnumMap<Type, DocValuesArraySource> templates = new EnumMap<Type, DocValuesArraySource>(
-        Type.class);
-    templates.put(Type.FIXED_INTS_16, new ShortValues());
-    templates.put(Type.FIXED_INTS_32, new IntValues());
-    templates.put(Type.FIXED_INTS_64, new LongValues());
-    templates.put(Type.FIXED_INTS_8, new ByteValues());
-    templates.put(Type.FLOAT_32, new FloatValues());
-    templates.put(Type.FLOAT_64, new DoubleValues());
-    TEMPLATES = Collections.unmodifiableMap(templates);
-  }
-
-  /** Returns the {@link DocValuesArraySource} for the given
-   *  {@link Type}. */
-  public static DocValuesArraySource forType(Type type) {
-    return TEMPLATES.get(type);
-  }
-
-  /** Number of bytes to encode each doc value. */
-  protected final int bytesPerValue;
-
-  DocValuesArraySource(int bytesPerValue, Type type) {
-    super(type);
-    this.bytesPerValue = bytesPerValue;
-  }
-
-  @Override
-  public abstract BytesRef getBytes(int docID, BytesRef ref);
-
-  
-  /** Creates a {@link DocValuesArraySource} by loading a
-   *  previously saved one from an {@link IndexInput}. */
-  public abstract DocValuesArraySource newFromInput(IndexInput input, int numDocs)
-      throws IOException;
-
-  /** Creates {@link DocValuesArraySource} from a native
-   *  array. */
-  public abstract DocValuesArraySource newFromArray(Object array);
-
-  @Override
-  public final boolean hasArray() {
-    return true;
-  }
-
-  /** Encode a long value into the provided {@link
-   *  BytesRef}. */
-  public void toBytes(long value, BytesRef bytesRef) {
-    copyLong(bytesRef, value);
-  }
-
-  /** Encode a double value into the provided {@link
-   *  BytesRef}. */
-  public void toBytes(double value, BytesRef bytesRef) {
-    copyLong(bytesRef, Double.doubleToRawLongBits(value));
-  }
-
-  final static class ByteValues extends DocValuesArraySource {
-    private final byte[] values;
-    
-    ByteValues() {
-      super(1, Type.FIXED_INTS_8);
-      values = new byte[0];
-    }
-    private ByteValues(byte[] array) {
-      super(1, Type.FIXED_INTS_8);
-      values = array;
-    }
-
-    private ByteValues(IndexInput input, int numDocs) throws IOException {
-      super(1, Type.FIXED_INTS_8);
-      values = new byte[numDocs];
-      input.readBytes(values, 0, values.length, false);
-    }
-
-    @Override
-    public byte[] getArray() {
-      return values;
-    }
-    
-    @Override
-    public double getFloat(int docID) {
-      return getInt(docID);
-    }
-
-    @Override
-    public long getInt(int docID) {
-      assert docID >= 0 && docID < values.length;
-      return values[docID];
-    }
-
-    @Override
-    public DocValuesArraySource newFromInput(IndexInput input, int numDocs)
-        throws IOException {
-      return new ByteValues(input, numDocs);
-    }
-    
-    @Override
-    public DocValuesArraySource newFromArray(Object array) {
-      assert array instanceof byte[];
-      return new ByteValues((byte[]) array);
-    }
-
-    @Override
-    public void toBytes(long value, BytesRef bytesRef) {
-      if (bytesRef.bytes.length == 0) {
-        bytesRef.bytes = new byte[1];
-      }
-      bytesRef.bytes[0] = (byte) (0xFFL & value);
-      bytesRef.offset = 0;
-      bytesRef.length = 1;
-    }
-    
-    @Override
-    public BytesRef getBytes(int docID, BytesRef ref) {
-      toBytes(getInt(docID), ref);
-      return ref;
-    }
-
-  };
-
-  final static class ShortValues extends DocValuesArraySource {
-    private final short[] values;
-
-    ShortValues() {
-      super(RamUsageEstimator.NUM_BYTES_SHORT, Type.FIXED_INTS_16);
-      values = new short[0];
-    }
-    
-    private ShortValues(short[] array) {
-      super(RamUsageEstimator.NUM_BYTES_SHORT, Type.FIXED_INTS_16);
-      values = array;
-    }
-
-    private ShortValues(IndexInput input, int numDocs) throws IOException {
-      super(RamUsageEstimator.NUM_BYTES_SHORT, Type.FIXED_INTS_16);
-      values = new short[numDocs];
-      for (int i = 0; i < values.length; i++) {
-        values[i] = input.readShort();
-      }
-    }
-
-    @Override
-    public short[] getArray() {
-      return values;
-    }
-    
-    @Override
-    public double getFloat(int docID) {
-      return getInt(docID);
-    }
-
-    @Override
-    public long getInt(int docID) {
-      assert docID >= 0 && docID < values.length;
-      return values[docID];
-    }
-
-    @Override
-    public DocValuesArraySource newFromInput(IndexInput input, int numDocs)
-        throws IOException {
-      return new ShortValues(input, numDocs);
-    }
-
-    @Override
-    public void toBytes(long value, BytesRef bytesRef) {
-      copyShort(bytesRef, (short) (0xFFFFL & value));
-    }
-
-    @Override
-    public DocValuesArraySource newFromArray(Object array) {
-      assert array instanceof short[];
-      return new ShortValues((short[]) array);
-    }
-    
-    @Override
-    public BytesRef getBytes(int docID, BytesRef ref) {
-      toBytes(getInt(docID), ref);
-      return ref;
-    }
-
-  };
-
-  final static class IntValues extends DocValuesArraySource {
-    private final int[] values;
-
-    IntValues() {
-      super(RamUsageEstimator.NUM_BYTES_INT, Type.FIXED_INTS_32);
-      values = new int[0];
-    }
-
-    private IntValues(IndexInput input, int numDocs) throws IOException {
-      super(RamUsageEstimator.NUM_BYTES_INT, Type.FIXED_INTS_32);
-      values = new int[numDocs];
-      for (int i = 0; i < values.length; i++) {
-        values[i] = input.readInt();
-      }
-    }
-
-    private IntValues(int[] array) {
-      super(RamUsageEstimator.NUM_BYTES_INT, Type.FIXED_INTS_32);
-      values = array;
-    }
-
-    @Override
-    public int[] getArray() {
-      return values;
-    }
-    
-    @Override
-    public double getFloat(int docID) {
-      return getInt(docID);
-    }
-
-    @Override
-    public long getInt(int docID) {
-      assert docID >= 0 && docID < values.length;
-      return 0xFFFFFFFF & values[docID];
-    }
-
-    @Override
-    public DocValuesArraySource newFromInput(IndexInput input, int numDocs)
-        throws IOException {
-      return new IntValues(input, numDocs);
-    }
-
-    @Override
-    public void toBytes(long value, BytesRef bytesRef) {
-      copyInt(bytesRef, (int) (0xFFFFFFFF & value));
-    }
-
-    @Override
-    public DocValuesArraySource newFromArray(Object array) {
-      assert array instanceof int[];
-      return new IntValues((int[]) array);
-    }
-    
-    @Override
-    public BytesRef getBytes(int docID, BytesRef ref) {
-      toBytes(getInt(docID), ref);
-      return ref;
-    }
-
-  };
-
-  final static class LongValues extends DocValuesArraySource {
-    private final long[] values;
-
-    LongValues() {
-      super(RamUsageEstimator.NUM_BYTES_LONG, Type.FIXED_INTS_64);
-      values = new long[0];
-    }
-
-    private LongValues(IndexInput input, int numDocs) throws IOException {
-      super(RamUsageEstimator.NUM_BYTES_LONG, Type.FIXED_INTS_64);
-      values = new long[numDocs];
-      for (int i = 0; i < values.length; i++) {
-        values[i] = input.readLong();
-      }
-    }
-
-    private LongValues(long[] array) {
-      super(RamUsageEstimator.NUM_BYTES_LONG, Type.FIXED_INTS_64);
-      values = array;
-    }
-
-    @Override
-    public long[] getArray() {
-      return values;
-    }
-
-    @Override
-    public long getInt(int docID) {
-      assert docID >= 0 && docID < values.length;
-      return values[docID];
-    }
-
-    @Override
-    public DocValuesArraySource newFromInput(IndexInput input, int numDocs)
-        throws IOException {
-      return new LongValues(input, numDocs);
-    }
-
-    @Override
-    public DocValuesArraySource newFromArray(Object array) {
-      assert array instanceof long[];
-      return new LongValues((long[])array);
-    }
-    
-    @Override
-    public BytesRef getBytes(int docID, BytesRef ref) {
-      toBytes(getInt(docID), ref);
-      return ref;
-    }
-
-  };
-
-  final static class FloatValues extends DocValuesArraySource {
-    private final float[] values;
-
-    FloatValues() {
-      super(RamUsageEstimator.NUM_BYTES_FLOAT, Type.FLOAT_32);
-      values = new float[0];
-    }
-
-    private FloatValues(IndexInput input, int numDocs) throws IOException {
-      super(RamUsageEstimator.NUM_BYTES_FLOAT, Type.FLOAT_32);
-      values = new float[numDocs];
-      /*
-       * we always read BIG_ENDIAN here since the writer serialized plain bytes
-       * we can simply read the ints / longs back in using readInt / readLong
-       */
-      for (int i = 0; i < values.length; i++) {
-        values[i] = Float.intBitsToFloat(input.readInt());
-      }
-    }
-
-    private FloatValues(float[] array) {
-      super(RamUsageEstimator.NUM_BYTES_FLOAT, Type.FLOAT_32);
-      values = array;
-    }
-
-    @Override
-    public float[] getArray() {
-      return values;
-    }
-
-    @Override
-    public double getFloat(int docID) {
-      assert docID >= 0 && docID < values.length;
-      return values[docID];
-    }
-    
-    @Override
-    public void toBytes(double value, BytesRef bytesRef) {
-      copyInt(bytesRef, Float.floatToRawIntBits((float)value));
-
-    }
-
-    @Override
-    public DocValuesArraySource newFromInput(IndexInput input, int numDocs)
-        throws IOException {
-      return new FloatValues(input, numDocs);
-    }
-
-    @Override
-    public DocValuesArraySource newFromArray(Object array) {
-      assert array instanceof float[];
-      return new FloatValues((float[]) array);
-    }
-    
-    @Override
-    public BytesRef getBytes(int docID, BytesRef ref) {
-      toBytes(getFloat(docID), ref);
-      return ref;
-    }
-  };
-
-  final static class DoubleValues extends DocValuesArraySource {
-    private final double[] values;
-
-    DoubleValues() {
-      super(RamUsageEstimator.NUM_BYTES_DOUBLE, Type.FLOAT_64);
-      values = new double[0];
-    }
-
-    private DoubleValues(IndexInput input, int numDocs) throws IOException {
-      super(RamUsageEstimator.NUM_BYTES_DOUBLE, Type.FLOAT_64);
-      values = new double[numDocs];
-      /*
-       * we always read BIG_ENDIAN here since the writer serialized plain bytes
-       * we can simply read the ints / longs back in using readInt / readLong
-       */
-      for (int i = 0; i < values.length; i++) {
-        values[i] = Double.longBitsToDouble(input.readLong());
-      }
-    }
-
-    private DoubleValues(double[] array) {
-      super(RamUsageEstimator.NUM_BYTES_DOUBLE, Type.FLOAT_64);
-      values = array;
-    }
-
-    @Override
-    public double[] getArray() {
-      return values;
-    }
-
-    @Override
-    public double getFloat(int docID) {
-      assert docID >= 0 && docID < values.length;
-      return values[docID];
-    }
-
-    @Override
-    public DocValuesArraySource newFromInput(IndexInput input, int numDocs)
-        throws IOException {
-      return new DoubleValues(input, numDocs);
-    }
-
-    @Override
-    public DocValuesArraySource newFromArray(Object array) {
-      assert array instanceof double[];
-      return new DoubleValues((double[]) array);
-    }
-    
-    @Override
-    public BytesRef getBytes(int docID, BytesRef ref) {
-      toBytes(getFloat(docID), ref);
-      return ref;
-    }
-
-  };
-  
-  /**
-   * Copies the given long value and encodes it as 8 byte Big-Endian.
-   * <p>
-   * NOTE: this method resets the offset to 0, length to 8 and resizes the
-   * reference array if needed.
-   */
-  public static void copyLong(BytesRef ref, long value) {
-    if (ref.bytes.length < 8) {
-      ref.bytes = new byte[8];
-    }
-    copyInternal(ref, (int) (value >> 32), ref.offset = 0);
-    copyInternal(ref, (int) value, 4);
-    ref.length = 8;
-  }
-
-  /**
-   * Copies the given int value and encodes it as 4 byte Big-Endian.
-   * <p>
-   * NOTE: this method resets the offset to 0, length to 4 and resizes the
-   * reference array if needed.
-   */
-  public static void copyInt(BytesRef ref, int value) {
-    if (ref.bytes.length < 4) {
-      ref.bytes = new byte[4];
-    }
-    copyInternal(ref, value, ref.offset = 0);
-    ref.length = 4;
-    
-  }
-
-  /**
-   * Copies the given short value and encodes it as a 2 byte Big-Endian.
-   * <p>
-   * NOTE: this method resets the offset to 0, length to 2 and resizes the
-   * reference array if needed.
-   */
-  public static void copyShort(BytesRef ref, short value) {
-    if (ref.bytes.length < 2) {
-      ref.bytes = new byte[2];
-    }
-    ref.offset = 0;
-    ref.bytes[ref.offset] = (byte) (value >> 8);
-    ref.bytes[ref.offset + 1] = (byte) (value);
-    ref.length = 2;
-  }
-
-  private static void copyInternal(BytesRef ref, int value, int startOffset) {
-    ref.bytes[startOffset] = (byte) (value >> 24);
-    ref.bytes[startOffset + 1] = (byte) (value >> 16);
-    ref.bytes[startOffset + 2] = (byte) (value >> 8);
-    ref.bytes[startOffset + 3] = (byte) (value);
-  }
-
-  /**
-   * Converts 2 consecutive bytes from the current offset to a short. Bytes are
-   * interpreted as Big-Endian (most significant bit first)
-   * <p>
-   * NOTE: this method does <b>NOT</b> check the bounds of the referenced array.
-   */
-  public static short asShort(BytesRef b) {
-    return (short) (0xFFFF & ((b.bytes[b.offset] & 0xFF) << 8) | (b.bytes[b.offset + 1] & 0xFF));
-  }
-
-  /**
-   * Converts 4 consecutive bytes from the current offset to an int. Bytes are
-   * interpreted as Big-Endian (most significant bit first)
-   * <p>
-   * NOTE: this method does <b>NOT</b> check the bounds of the referenced array.
-   */
-  public static int asInt(BytesRef b) {
-    return asIntInternal(b, b.offset);
-  }
-
-  /**
-   * Converts 8 consecutive bytes from the current offset to a long. Bytes are
-   * interpreted as Big-Endian (most significant bit first)
-   * <p>
-   * NOTE: this method does <b>NOT</b> check the bounds of the referenced array.
-   */
-  public static long asLong(BytesRef b) {
-    return (((long) asIntInternal(b, b.offset) << 32) | asIntInternal(b,
-        b.offset + 4) & 0xFFFFFFFFL);
-  }
-
-  private static int asIntInternal(BytesRef b, int pos) {
-    return ((b.bytes[pos++] & 0xFF) << 24) | ((b.bytes[pos++] & 0xFF) << 16)
-        | ((b.bytes[pos++] & 0xFF) << 8) | (b.bytes[pos] & 0xFF);
-  }
-
-
-}
\ No newline at end of file
Index: lucene/core/src/java/org/apache/lucene/codecs/PostingsConsumer.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/codecs/PostingsConsumer.java	(revision 1442822)
+++ lucene/core/src/java/org/apache/lucene/codecs/PostingsConsumer.java	(working copy)
@@ -73,12 +73,11 @@
 
   /** Default merge impl: append documents, mapping around
    *  deletes */
-  public TermStats merge(final MergeState mergeState, final DocsEnum postings, final FixedBitSet visitedDocs) throws IOException {
+  public TermStats merge(final MergeState mergeState, IndexOptions indexOptions, final DocsEnum postings, final FixedBitSet visitedDocs) throws IOException {
 
     int df = 0;
     long totTF = 0;
 
-    IndexOptions indexOptions = mergeState.fieldInfo.getIndexOptions();
     if (indexOptions == IndexOptions.DOCS_ONLY) {
       while(true) {
         final int doc = postings.nextDoc();
Index: lucene/core/src/java/org/apache/lucene/codecs/TermsConsumer.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/codecs/TermsConsumer.java	(revision 1442822)
+++ lucene/core/src/java/org/apache/lucene/codecs/TermsConsumer.java	(working copy)
@@ -83,7 +83,7 @@
   private MappingMultiDocsAndPositionsEnum postingsEnum;
 
   /** Default merge impl */
-  public void merge(MergeState mergeState, TermsEnum termsEnum) throws IOException {
+  public void merge(MergeState mergeState, IndexOptions indexOptions, TermsEnum termsEnum) throws IOException {
 
     BytesRef term;
     assert termsEnum != null;
@@ -92,7 +92,6 @@
     long sumDFsinceLastAbortCheck = 0;
     FixedBitSet visitedDocs = new FixedBitSet(mergeState.segmentInfo.getDocCount());
 
-    IndexOptions indexOptions = mergeState.fieldInfo.getIndexOptions();
     if (indexOptions == IndexOptions.DOCS_ONLY) {
       if (docsEnum == null) {
         docsEnum = new MappingMultiDocsEnum();
@@ -108,7 +107,7 @@
         if (docsEnumIn != null) {
           docsEnum.reset(docsEnumIn);
           final PostingsConsumer postingsConsumer = startTerm(term);
-          final TermStats stats = postingsConsumer.merge(mergeState, docsEnum, visitedDocs);
+          final TermStats stats = postingsConsumer.merge(mergeState, indexOptions, docsEnum, visitedDocs);
           if (stats.docFreq > 0) {
             finishTerm(term, stats);
             sumTotalTermFreq += stats.docFreq;
@@ -136,7 +135,7 @@
         assert docsAndFreqsEnumIn != null;
         docsAndFreqsEnum.reset(docsAndFreqsEnumIn);
         final PostingsConsumer postingsConsumer = startTerm(term);
-        final TermStats stats = postingsConsumer.merge(mergeState, docsAndFreqsEnum, visitedDocs);
+        final TermStats stats = postingsConsumer.merge(mergeState, indexOptions, docsAndFreqsEnum, visitedDocs);
         if (stats.docFreq > 0) {
           finishTerm(term, stats);
           sumTotalTermFreq += stats.totalTermFreq;
@@ -162,7 +161,7 @@
         postingsEnum.reset(postingsEnumIn);
 
         final PostingsConsumer postingsConsumer = startTerm(term);
-        final TermStats stats = postingsConsumer.merge(mergeState, postingsEnum, visitedDocs);
+        final TermStats stats = postingsConsumer.merge(mergeState, indexOptions, postingsEnum, visitedDocs);
         if (stats.docFreq > 0) {
           finishTerm(term, stats);
           sumTotalTermFreq += stats.totalTermFreq;
@@ -189,7 +188,7 @@
         postingsEnum.reset(postingsEnumIn);
 
         final PostingsConsumer postingsConsumer = startTerm(term);
-        final TermStats stats = postingsConsumer.merge(mergeState, postingsEnum, visitedDocs);
+        final TermStats stats = postingsConsumer.merge(mergeState, indexOptions, postingsEnum, visitedDocs);
         if (stats.docFreq > 0) {
           finishTerm(term, stats);
           sumTotalTermFreq += stats.totalTermFreq;
Index: lucene/core/src/java/org/apache/lucene/codecs/PerDocProducer.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/codecs/PerDocProducer.java	(revision 1442822)
+++ lucene/core/src/java/org/apache/lucene/codecs/PerDocProducer.java	(working copy)
@@ -1,54 +0,0 @@
-package org.apache.lucene.codecs;
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-import java.io.Closeable;
-import java.io.IOException;
-
-import org.apache.lucene.index.DocValues;
-
-/**
- * Abstract API that provides access to one or more per-document storage
- * features. The concrete implementations provide access to the underlying
- * storage on a per-document basis corresponding to their actual
- * {@link PerDocConsumer} counterpart.
- * <p>
- * The {@link PerDocProducer} API is accessible through the
- * {@link PostingsFormat} - API providing per field consumers and producers for inverted
- * data (terms, postings) as well as per-document data.
- * 
- * @lucene.experimental
- */
-public abstract class PerDocProducer implements Closeable {
-  /** Sole constructor. (For invocation by subclass 
-   *  constructors, typically implicit.) */
-  protected PerDocProducer() {
-  }
-
-  /**
-   * Returns {@link DocValues} for the current field.
-   * 
-   * @param field
-   *          the field name
-   * @return the {@link DocValues} for this field or <code>null</code> if not
-   *         applicable.
-   * @throws IOException If an I/O error occurs
-   */
-  public abstract DocValues docValues(String field) throws IOException;
-
-  @Override
-  public abstract void close() throws IOException;
-}
Index: lucene/core/src/java/org/apache/lucene/codecs/DocValuesProducer.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/codecs/DocValuesProducer.java	(revision 0)
+++ lucene/core/src/java/org/apache/lucene/codecs/DocValuesProducer.java	(working copy)
@@ -0,0 +1,53 @@
+package org.apache.lucene.codecs;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.Closeable;
+import java.io.IOException;
+
+import org.apache.lucene.index.BinaryDocValues;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.NumericDocValues;
+import org.apache.lucene.index.SortedDocValues;
+
+/** Abstract API that produces numeric, binary and
+ * sorted docvalues.
+ *
+ * @lucene.experimental
+ */
+public abstract class DocValuesProducer implements Closeable {
+  
+  /** Sole constructor. (For invocation by subclass 
+   *  constructors, typically implicit.) */
+  protected DocValuesProducer() {}
+
+  /** Returns {@link NumericDocValues} for this field.
+   *  The returned instance need not be thread-safe: it will only be
+   *  used by a single thread. */
+  public abstract NumericDocValues getNumeric(FieldInfo field) throws IOException;
+
+  /** Returns {@link BinaryDocValues} for this field.
+   *  The returned instance need not be thread-safe: it will only be
+   *  used by a single thread. */
+  public abstract BinaryDocValues getBinary(FieldInfo field) throws IOException;
+
+  /** Returns {@link SortedDocValues} for this field.
+   *  The returned instance need not be thread-safe: it will only be
+   *  used by a single thread. */
+  public abstract SortedDocValues getSorted(FieldInfo field) throws IOException;
+}

Property changes on: lucene/core/src/java/org/apache/lucene/codecs/DocValuesProducer.java
___________________________________________________________________
Added: svn:eol-style
## -0,0 +1 ##
+native
Index: lucene/core/src/java/org/apache/lucene/codecs/DocValuesFormat.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/codecs/DocValuesFormat.java	(revision 1442822)
+++ lucene/core/src/java/org/apache/lucene/codecs/DocValuesFormat.java	(working copy)
@@ -18,24 +18,106 @@
  */
 
 import java.io.IOException;
+import java.util.ServiceLoader;
+import java.util.Set;
 
-import org.apache.lucene.index.DocValues; // javadocs
-import org.apache.lucene.index.PerDocWriteState;
 import org.apache.lucene.index.SegmentReadState;
+import org.apache.lucene.index.SegmentWriteState;
+import org.apache.lucene.util.NamedSPILoader;
 
-/**
- * Encodes/decodes {@link DocValues}
- * @lucene.experimental
- */
-public abstract class DocValuesFormat {
-  /** Sole constructor. (For invocation by subclass 
-   *  constructors, typically implicit.) */
-  protected DocValuesFormat() {
+/** 
+ * Encodes/decodes per-document values.
+ * <p>
+ * Note, when extending this class, the name ({@link #getName}) may
+ * written into the index in certain configurations. In order for the segment 
+ * to be read, the name must resolve to your implementation via {@link #forName(String)}.
+ * This method uses Java's 
+ * {@link ServiceLoader Service Provider Interface} (SPI) to resolve format names.
+ * <p>
+ * If you implement your own format, make sure that it has a no-arg constructor
+ * so SPI can load it.
+ * @see ServiceLoader
+ * @lucene.experimental */
+public abstract class DocValuesFormat implements NamedSPILoader.NamedSPI {
+  
+  private static final NamedSPILoader<DocValuesFormat> loader =
+      new NamedSPILoader<DocValuesFormat>(DocValuesFormat.class);
+  
+  /** Unique name that's used to retrieve this format when
+   *  reading the index.
+   */
+  private final String name;
+
+  /**
+   * Creates a new docvalues format.
+   * <p>
+   * The provided name will be written into the index segment in some configurations
+   * (such as when using {@code PerFieldDocValuesFormat}): in such configurations,
+   * for the segment to be read this class should be registered with Java's
+   * SPI mechanism (registered in META-INF/ of your jar file, etc).
+   * @param name must be all ascii alphanumeric, and less than 128 characters in length.
+   */
+  protected DocValuesFormat(String name) {
+    NamedSPILoader.checkServiceName(name);
+    this.name = name;
   }
 
-  /** Consumes (writes) doc values during indexing. */
-  public abstract PerDocConsumer docsConsumer(PerDocWriteState state) throws IOException;
+  /** Returns a {@link DocValuesConsumer} to write docvalues to the
+   *  index. */
+  public abstract DocValuesConsumer fieldsConsumer(SegmentWriteState state) throws IOException;
 
-  /** Produces (reads) doc values during reading/searching. */
-  public abstract PerDocProducer docsProducer(SegmentReadState state) throws IOException;
+  /** 
+   * Returns a {@link DocValuesProducer} to read docvalues from the index. 
+   * <p>
+   * NOTE: by the time this call returns, it must hold open any files it will 
+   * need to use; else, those files may be deleted. Additionally, required files 
+   * may be deleted during the execution of this call before there is a chance 
+   * to open them. Under these circumstances an IOException should be thrown by 
+   * the implementation. IOExceptions are expected and will automatically cause 
+   * a retry of the segment opening logic with the newly revised segments.
+   */
+  public abstract DocValuesProducer fieldsProducer(SegmentReadState state) throws IOException;
+
+  @Override
+  public final String getName() {
+    return name;
+  }
+  
+  @Override
+  public String toString() {
+    return "DocValuesFormat(name=" + name + ")";
+  }
+  
+  /** looks up a format by name */
+  public static DocValuesFormat forName(String name) {
+    if (loader == null) {
+      throw new IllegalStateException("You called DocValuesFormat.forName() before all formats could be initialized. "+
+          "This likely happens if you call it from a DocValuesFormat's ctor.");
+    }
+    return loader.lookup(name);
+  }
+  
+  /** returns a list of all available format names */
+  public static Set<String> availableDocValuesFormats() {
+    if (loader == null) {
+      throw new IllegalStateException("You called DocValuesFormat.availableDocValuesFormats() before all formats could be initialized. "+
+          "This likely happens if you call it from a DocValuesFormat's ctor.");
+    }
+    return loader.availableServices();
+  }
+  
+  /** 
+   * Reloads the DocValues format list from the given {@link ClassLoader}.
+   * Changes to the docvalues formats are visible after the method ends, all
+   * iterators ({@link #availableDocValuesFormats()},...) stay consistent. 
+   * 
+   * <p><b>NOTE:</b> Only new docvalues formats are added, existing ones are
+   * never removed or replaced.
+   * 
+   * <p><em>This method is expensive and should only be called for discovery
+   * of new docvalues formats on the given classpath/classloader!</em>
+   */
+  public static void reloadDocValuesFormats(ClassLoader classloader) {
+    loader.reload(classloader);
+  }
 }
Index: lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingTermVectorsReader.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingTermVectorsReader.java	(revision 1442822)
+++ lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingTermVectorsReader.java	(working copy)
@@ -57,7 +57,7 @@
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.IOUtils;
 import org.apache.lucene.util.LongsRef;
-import org.apache.lucene.util.packed.BlockPackedReader;
+import org.apache.lucene.util.packed.BlockPackedReaderIterator;
 import org.apache.lucene.util.packed.PackedInts;
 
 
@@ -76,7 +76,7 @@
   private final int chunkSize;
   private final int numDocs;
   private boolean closed;
-  private final BlockPackedReader reader;
+  private final BlockPackedReaderIterator reader;
 
   // used by clone
   private CompressingTermVectorsReader(CompressingTermVectorsReader reader) {
@@ -88,7 +88,7 @@
     this.decompressor = reader.decompressor.clone();
     this.chunkSize = reader.chunkSize;
     this.numDocs = reader.numDocs;
-    this.reader = new BlockPackedReader(vectorsStream, packedIntsVersion, BLOCK_SIZE, 0);
+    this.reader = new BlockPackedReaderIterator(vectorsStream, packedIntsVersion, BLOCK_SIZE, 0);
     this.closed = false;
   }
 
@@ -119,7 +119,7 @@
       packedIntsVersion = vectorsStream.readVInt();
       chunkSize = vectorsStream.readVInt();
       decompressor = compressionMode.newDecompressor();
-      this.reader = new BlockPackedReader(vectorsStream, packedIntsVersion, BLOCK_SIZE, 0);
+      this.reader = new BlockPackedReaderIterator(vectorsStream, packedIntsVersion, BLOCK_SIZE, 0);
 
       success = true;
     } finally {
Index: lucene/core/src/java/org/apache/lucene/codecs/FieldsConsumer.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/codecs/FieldsConsumer.java	(revision 1442822)
+++ lucene/core/src/java/org/apache/lucene/codecs/FieldsConsumer.java	(working copy)
@@ -64,12 +64,12 @@
    *  implementation to do its own merging. */
   public void merge(MergeState mergeState, Fields fields) throws IOException {
     for (String field : fields) {
-      mergeState.fieldInfo = mergeState.fieldInfos.fieldInfo(field);
-      assert mergeState.fieldInfo != null : "FieldInfo for field is null: "+ field;
+      FieldInfo info = mergeState.fieldInfos.fieldInfo(field);
+      assert info != null : "FieldInfo for field is null: "+ field;
       Terms terms = fields.terms(field);
       if (terms != null) {
-        final TermsConsumer termsConsumer = addField(mergeState.fieldInfo);
-        termsConsumer.merge(mergeState, terms.iterator(null));
+        final TermsConsumer termsConsumer = addField(info);
+        termsConsumer.merge(mergeState, info.getIndexOptions(), terms.iterator(null));
       }
     }
   }
Index: lucene/core/src/java/org/apache/lucene/codecs/NormsFormat.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/codecs/NormsFormat.java	(revision 1442822)
+++ lucene/core/src/java/org/apache/lucene/codecs/NormsFormat.java	(working copy)
@@ -19,11 +19,11 @@
 
 import java.io.IOException;
 
-import org.apache.lucene.index.PerDocWriteState;
 import org.apache.lucene.index.SegmentReadState;
+import org.apache.lucene.index.SegmentWriteState;
 
 /**
- * format for normalization factors
+ * Encodes/decodes per-document score normalization values.
  */
 public abstract class NormsFormat {
   /** Sole constructor. (For invocation by subclass 
@@ -31,11 +31,19 @@
   protected NormsFormat() {
   }
 
-  /** Returns a {@link PerDocConsumer} to write norms to the
+  /** Returns a {@link DocValuesConsumer} to write norms to the
    *  index. */
-  public abstract PerDocConsumer docsConsumer(PerDocWriteState state) throws IOException;
+  public abstract DocValuesConsumer normsConsumer(SegmentWriteState state) throws IOException;
 
-  /** Returns a {@link PerDocProducer} to read norms from the
-   *  index. */
-  public abstract PerDocProducer docsProducer(SegmentReadState state) throws IOException;
+  /** 
+   * Returns a {@link DocValuesProducer} to read norms from the index. 
+   * <p>
+   * NOTE: by the time this call returns, it must hold open any files it will 
+   * need to use; else, those files may be deleted. Additionally, required files 
+   * may be deleted during the execution of this call before there is a chance 
+   * to open them. Under these circumstances an IOException should be thrown by 
+   * the implementation. IOExceptions are expected and will automatically cause 
+   * a retry of the segment opening logic with the newly revised segments.
+   */
+  public abstract DocValuesProducer normsProducer(SegmentReadState state) throws IOException;
 }
Index: lucene/core/src/java/org/apache/lucene/codecs/perfield/PerFieldDocValuesFormat.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/codecs/perfield/PerFieldDocValuesFormat.java	(revision 0)
+++ lucene/core/src/java/org/apache/lucene/codecs/perfield/PerFieldDocValuesFormat.java	(working copy)
@@ -0,0 +1,280 @@
+package org.apache.lucene.codecs.perfield;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.Closeable;
+import java.io.IOException;
+import java.util.HashMap;
+import java.util.IdentityHashMap;
+import java.util.Map;
+import java.util.ServiceLoader; // javadocs
+import java.util.TreeMap;
+
+import org.apache.lucene.codecs.PostingsFormat;
+import org.apache.lucene.codecs.DocValuesConsumer;
+import org.apache.lucene.codecs.DocValuesProducer;
+import org.apache.lucene.codecs.DocValuesFormat;
+import org.apache.lucene.index.BinaryDocValues;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.NumericDocValues;
+import org.apache.lucene.index.SegmentReadState;
+import org.apache.lucene.index.SegmentWriteState;
+import org.apache.lucene.index.SortedDocValues;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IOUtils;
+
+/**
+ * Enables per field docvalues support.
+ * <p>
+ * Note, when extending this class, the name ({@link #getName}) is 
+ * written into the index. In order for the field to be read, the
+ * name must resolve to your implementation via {@link #forName(String)}.
+ * This method uses Java's 
+ * {@link ServiceLoader Service Provider Interface} to resolve format names.
+ * <p>
+ * Files written by each docvalues format have an additional suffix containing the 
+ * format name. For example, in a per-field configuration instead of <tt>_1.dat</tt> 
+ * filenames would look like <tt>_1_Lucene40_0.dat</tt>.
+ * @see ServiceLoader
+ * @lucene.experimental
+ */
+
+public abstract class PerFieldDocValuesFormat extends DocValuesFormat {
+  /** Name of this {@link PostingsFormat}. */
+  public static final String PER_FIELD_NAME = "PerFieldDV40";
+
+  /** {@link FieldInfo} attribute name used to store the
+   *  format name for each field. */
+  public static final String PER_FIELD_FORMAT_KEY = PerFieldDocValuesFormat.class.getSimpleName() + ".format";
+
+  /** {@link FieldInfo} attribute name used to store the
+   *  segment suffix name for each field. */
+  public static final String PER_FIELD_SUFFIX_KEY = PerFieldDocValuesFormat.class.getSimpleName() + ".suffix";
+
+  
+  /** Sole constructor. */
+  public PerFieldDocValuesFormat() {
+    super(PER_FIELD_NAME);
+  }
+
+  @Override
+  public final DocValuesConsumer fieldsConsumer(SegmentWriteState state)
+      throws IOException {
+    return new FieldsWriter(state);
+  }
+  
+  static class ConsumerAndSuffix implements Closeable {
+    DocValuesConsumer consumer;
+    int suffix;
+    
+    @Override
+    public void close() throws IOException {
+      consumer.close();
+    }
+  }
+    
+  private class FieldsWriter extends DocValuesConsumer {
+
+    private final Map<DocValuesFormat,ConsumerAndSuffix> formats = new HashMap<DocValuesFormat,ConsumerAndSuffix>();
+    private final Map<String,Integer> suffixes = new HashMap<String,Integer>();
+    
+    private final SegmentWriteState segmentWriteState;
+
+    public FieldsWriter(SegmentWriteState state) {
+      segmentWriteState = state;
+    }
+    
+    @Override
+    public void addNumericField(FieldInfo field, Iterable<Number> values) throws IOException {
+      getInstance(field).addNumericField(field, values);
+    }
+
+    @Override
+    public void addBinaryField(FieldInfo field, Iterable<BytesRef> values) throws IOException {
+      getInstance(field).addBinaryField(field, values);
+    }
+
+    @Override
+    public void addSortedField(FieldInfo field, Iterable<BytesRef> values, Iterable<Number> docToOrd) throws IOException {
+      getInstance(field).addSortedField(field, values, docToOrd);
+    }
+
+    private DocValuesConsumer getInstance(FieldInfo field) throws IOException {
+      final DocValuesFormat format = getDocValuesFormatForField(field.name);
+      if (format == null) {
+        throw new IllegalStateException("invalid null DocValuesFormat for field=\"" + field.name + "\"");
+      }
+      final String formatName = format.getName();
+      
+      String previousValue = field.putAttribute(PER_FIELD_FORMAT_KEY, formatName);
+      assert previousValue == null: "formatName=" + formatName + " prevValue=" + previousValue;
+      
+      Integer suffix;
+      
+      ConsumerAndSuffix consumer = formats.get(format);
+      if (consumer == null) {
+        // First time we are seeing this format; create a new instance
+        
+        // bump the suffix
+        suffix = suffixes.get(formatName);
+        if (suffix == null) {
+          suffix = 0;
+        } else {
+          suffix = suffix + 1;
+        }
+        suffixes.put(formatName, suffix);
+        
+        final String segmentSuffix = getFullSegmentSuffix(field.name,
+                                                          segmentWriteState.segmentSuffix,
+                                                          getSuffix(formatName, Integer.toString(suffix)));
+        consumer = new ConsumerAndSuffix();
+        consumer.consumer = format.fieldsConsumer(new SegmentWriteState(segmentWriteState, segmentSuffix));
+        consumer.suffix = suffix;
+        formats.put(format, consumer);
+      } else {
+        // we've already seen this format, so just grab its suffix
+        assert suffixes.containsKey(formatName);
+        suffix = consumer.suffix;
+      }
+      
+      previousValue = field.putAttribute(PER_FIELD_SUFFIX_KEY, Integer.toString(suffix));
+      assert previousValue == null;
+
+      // TODO: we should only provide the "slice" of FIS
+      // that this PF actually sees ...
+      return consumer.consumer;
+    }
+
+    @Override
+    public void close() throws IOException {
+      // Close all subs
+      IOUtils.close(formats.values());
+    }
+  }
+  
+  static String getSuffix(String formatName, String suffix) {
+    return formatName + "_" + suffix;
+  }
+
+  static String getFullSegmentSuffix(String fieldName, String outerSegmentSuffix, String segmentSuffix) {
+    if (outerSegmentSuffix.length() == 0) {
+      return segmentSuffix;
+    } else {
+      // TODO: support embedding; I think it should work but
+      // we need a test confirm to confirm
+      // return outerSegmentSuffix + "_" + segmentSuffix;
+      throw new IllegalStateException("cannot embed PerFieldPostingsFormat inside itself (field \"" + fieldName + "\" returned PerFieldPostingsFormat)");
+    }
+  }
+
+  private class FieldsReader extends DocValuesProducer {
+
+    private final Map<String,DocValuesProducer> fields = new TreeMap<String,DocValuesProducer>();
+    private final Map<String,DocValuesProducer> formats = new HashMap<String,DocValuesProducer>();
+
+    public FieldsReader(final SegmentReadState readState) throws IOException {
+
+      // Read _X.per and init each format:
+      boolean success = false;
+      try {
+        // Read field name -> format name
+        for (FieldInfo fi : readState.fieldInfos) {
+          if (fi.hasDocValues()) {
+            final String fieldName = fi.name;
+            final String formatName = fi.getAttribute(PER_FIELD_FORMAT_KEY);
+            if (formatName != null) {
+              // null formatName means the field is in fieldInfos, but has no docvalues!
+              final String suffix = fi.getAttribute(PER_FIELD_SUFFIX_KEY);
+              assert suffix != null;
+              DocValuesFormat format = DocValuesFormat.forName(formatName);
+              String segmentSuffix = getSuffix(formatName, suffix);
+              if (!formats.containsKey(segmentSuffix)) {
+                formats.put(segmentSuffix, format.fieldsProducer(new SegmentReadState(readState, segmentSuffix)));
+              }
+              fields.put(fieldName, formats.get(segmentSuffix));
+            }
+          }
+        }
+        success = true;
+      } finally {
+        if (!success) {
+          IOUtils.closeWhileHandlingException(formats.values());
+        }
+      }
+    }
+
+    private FieldsReader(FieldsReader other) {
+
+      Map<DocValuesProducer,DocValuesProducer> oldToNew = new IdentityHashMap<DocValuesProducer,DocValuesProducer>();
+      // First clone all formats
+      for(Map.Entry<String,DocValuesProducer> ent : other.formats.entrySet()) {
+        DocValuesProducer values = ent.getValue();
+        formats.put(ent.getKey(), values);
+        oldToNew.put(ent.getValue(), values);
+      }
+
+      // Then rebuild fields:
+      for(Map.Entry<String,DocValuesProducer> ent : other.fields.entrySet()) {
+        DocValuesProducer producer = oldToNew.get(ent.getValue());
+        assert producer != null;
+        fields.put(ent.getKey(), producer);
+      }
+    }
+
+    @Override
+    public NumericDocValues getNumeric(FieldInfo field) throws IOException {
+      DocValuesProducer producer = fields.get(field.name);
+      return producer == null ? null : producer.getNumeric(field);
+    }
+
+    @Override
+    public BinaryDocValues getBinary(FieldInfo field) throws IOException {
+      DocValuesProducer producer = fields.get(field.name);
+      return producer == null ? null : producer.getBinary(field);
+    }
+
+    @Override
+    public SortedDocValues getSorted(FieldInfo field) throws IOException {
+      DocValuesProducer producer = fields.get(field.name);
+      return producer == null ? null : producer.getSorted(field);
+    }
+
+    @Override
+    public void close() throws IOException {
+      IOUtils.close(formats.values());
+    }
+
+    @Override
+    public DocValuesProducer clone() {
+      return new FieldsReader(this);
+    }
+  }
+
+  @Override
+  public final DocValuesProducer fieldsProducer(SegmentReadState state) throws IOException {
+    return new FieldsReader(state);
+  }
+
+  /** 
+   * Returns the doc values format that should be used for writing 
+   * new segments of <code>field</code>.
+   * <p>
+   * The field to format mapping is written to the index, so
+   * this method is only invoked when writing, not when reading. */
+  public abstract DocValuesFormat getDocValuesFormatForField(String field);
+}

Property changes on: lucene/core/src/java/org/apache/lucene/codecs/perfield/PerFieldDocValuesFormat.java
___________________________________________________________________
Added: svn:eol-style
## -0,0 +1 ##
+native
Index: lucene/core/src/java/org/apache/lucene/codecs/perfield/PerFieldPostingsFormat.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/codecs/perfield/PerFieldPostingsFormat.java	(revision 1442822)
+++ lucene/core/src/java/org/apache/lucene/codecs/perfield/PerFieldPostingsFormat.java	(working copy)
@@ -37,7 +37,7 @@
 import org.apache.lucene.util.IOUtils;
 
 /**
- * Enables per field format support.
+ * Enables per field postings support.
  * <p>
  * Note, when extending this class, the name ({@link #getName}) is 
  * written into the index. In order for the field to be read, the
Index: lucene/core/src/java/org/apache/lucene/codecs/PerDocProducerBase.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/codecs/PerDocProducerBase.java	(revision 1442822)
+++ lucene/core/src/java/org/apache/lucene/codecs/PerDocProducerBase.java	(working copy)
@@ -1,138 +0,0 @@
-package org.apache.lucene.codecs;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.Closeable;
-import java.io.IOException;
-import java.util.Collection;
-import java.util.Comparator;
-import java.util.Map;
-import java.util.TreeMap;
-
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.DocValues;
-import org.apache.lucene.index.DocValues.Type; // javadocs
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.util.BytesRef;
-
-/**
- * Abstract base class for PerDocProducer implementations
- * @lucene.experimental
- */
-public abstract class PerDocProducerBase extends PerDocProducer {
-
-  /** Closes provided Closables. */
-  protected abstract void closeInternal(Collection<? extends Closeable> closeables) throws IOException;
-
-  /** Returns a map, mapping field names to doc values. */
-  protected abstract Map<String, DocValues> docValues();
-  
-  /** Sole constructor. (For invocation by subclass 
-   *  constructors, typically implicit.) */
-  protected PerDocProducerBase() {
-  }
-
-  @Override
-  public void close() throws IOException {
-    closeInternal(docValues().values());
-  }
-  
-  @Override
-  public DocValues docValues(String field) throws IOException {
-    return docValues().get(field);
-  }
-
-  /** Returns the comparator used to sort {@link BytesRef} values. */
-  public Comparator<BytesRef> getComparator() throws IOException {
-    return BytesRef.getUTF8SortedAsUnicodeComparator();
-  }
-
-  /** Only opens files... doesn't actually load any values. */
-  protected TreeMap<String, DocValues> load(FieldInfos fieldInfos,
-      String segment, int docCount, Directory dir, IOContext context)
-      throws IOException {
-    TreeMap<String, DocValues> values = new TreeMap<String, DocValues>();
-    boolean success = false;
-    try {
-
-      for (FieldInfo fieldInfo : fieldInfos) {
-        if (canLoad(fieldInfo)) {
-          final String field = fieldInfo.name;
-          final String id = docValuesId(segment,
-              fieldInfo.number);
-          values.put(field,
-              loadDocValues(docCount, dir, id, getDocValuesType(fieldInfo), context));
-        }
-      }
-      success = true;
-    } finally {
-      if (!success) {
-        // if we fail we must close all opened resources if there are any
-        try {
-          closeInternal(values.values());
-        } catch (Throwable t) {} // keep our original exception
-      }
-    }
-    return values;
-  }
-
-  /** Returns true if this field indexed doc values. */
-  protected boolean canLoad(FieldInfo info) {
-    return info.hasDocValues();
-  }
-  
-  /** Returns the doc values type for this field. */
-  protected Type getDocValuesType(FieldInfo info) {
-    return info.getDocValuesType();
-  }
-  
-  /** Returns true if any fields indexed doc values. */
-  protected boolean anyDocValuesFields(FieldInfos infos) {
-    return infos.hasDocValues();
-  }
-
-  /** Returns the unique segment and field id for any
-   *  per-field files this implementation needs to write. */
-  public static String docValuesId(String segmentsName, int fieldId) {
-    return segmentsName + "_" + fieldId;
-  }
-  
-  /**
-   * Loads a {@link DocValues} instance depending on the given {@link Type}.
-   * Codecs that use different implementations for a certain {@link Type} can
-   * simply override this method and return their custom implementations.
-   * 
-   * @param docCount
-   *          number of documents in the segment
-   * @param dir
-   *          the {@link Directory} to load the {@link DocValues} from
-   * @param id
-   *          the unique file ID within the segment
-   * @param type
-   *          the type to load
-   * @return a {@link DocValues} instance for the given type
-   * @throws IOException
-   *           if an {@link IOException} occurs
-   * @throws IllegalArgumentException
-   *           if the given {@link Type} is not supported
-   */
-  protected abstract DocValues loadDocValues(int docCount, Directory dir, String id,
-      DocValues.Type type, IOContext context) throws IOException;
-}
Index: lucene/core/src/java/org/apache/lucene/codecs/Codec.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/codecs/Codec.java	(revision 1442822)
+++ lucene/core/src/java/org/apache/lucene/codecs/Codec.java	(working copy)
@@ -64,7 +64,7 @@
   
   /** Encodes/decodes postings */
   public abstract PostingsFormat postingsFormat();
-  
+
   /** Encodes/decodes docvalues */
   public abstract DocValuesFormat docValuesFormat();
   
@@ -82,7 +82,7 @@
   
   /** Encodes/decodes document normalization values */
   public abstract NormsFormat normsFormat();
-  
+
   /** Encodes/decodes live docs */
   public abstract LiveDocsFormat liveDocsFormat();
   
Index: lucene/core/src/java/org/apache/lucene/codecs/PerDocConsumer.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/codecs/PerDocConsumer.java	(revision 1442822)
+++ lucene/core/src/java/org/apache/lucene/codecs/PerDocConsumer.java	(working copy)
@@ -1,115 +0,0 @@
-package org.apache.lucene.codecs;
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to You under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-import java.io.Closeable;
-import java.io.IOException;
-
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.DocValues;
-import org.apache.lucene.index.AtomicReader;
-import org.apache.lucene.index.MergeState;
-import org.apache.lucene.index.DocValues.Type;
-
-/**
- * Abstract API that consumes per document values. Concrete implementations of
- * this convert field values into a Codec specific format during indexing.
- * <p>
- * The {@link PerDocConsumer} API is accessible through the
- * {@link PostingsFormat} - API providing per field consumers and producers for inverted
- * data (terms, postings) as well as per-document data.
- * 
- * @lucene.experimental
- */
-public abstract class PerDocConsumer implements Closeable {
-  /** Sole constructor. (For invocation by subclass 
-   *  constructors, typically implicit.) */
-  protected PerDocConsumer() {
-  }
-
-  /** Adds a new DocValuesField */
-  public abstract DocValuesConsumer addValuesField(DocValues.Type type, FieldInfo field)
-      throws IOException;
-
-  /**
-   * Consumes and merges the given {@link PerDocProducer} producer
-   * into this consumers format.   
-   */
-  public void merge(MergeState mergeState) throws IOException {
-    final DocValues[] docValues = new DocValues[mergeState.readers.size()];
-
-    for (FieldInfo fieldInfo : mergeState.fieldInfos) {
-      mergeState.fieldInfo = fieldInfo; // set the field we are merging
-      if (canMerge(fieldInfo)) {
-        for (int i = 0; i < docValues.length; i++) {
-          docValues[i] = getDocValuesForMerge(mergeState.readers.get(i), fieldInfo);
-        }
-        Type docValuesType = getDocValuesType(fieldInfo);
-        assert docValuesType != null;
-        
-        final DocValuesConsumer docValuesConsumer = addValuesField(docValuesType, fieldInfo);
-        assert docValuesConsumer != null;
-        docValuesConsumer.merge(mergeState, docValues);
-      }
-    }
-  }
-
-  /**
-   * Returns a {@link DocValues} instance for merging from the given reader for the given
-   * {@link FieldInfo}. This method is used for merging and uses
-   * {@link AtomicReader#docValues(String)} by default.
-   * <p>
-   * To enable {@link DocValues} merging for different {@link DocValues} than
-   * the default override this method accordingly.
-   * <p>
-   */
-  protected DocValues getDocValuesForMerge(AtomicReader reader, FieldInfo info) throws IOException {
-    return reader.docValues(info.name);
-  }
-  
-  /**
-   * Returns <code>true</code> iff the given field can be merged ie. has {@link DocValues}.
-   * By default this method uses {@link FieldInfo#hasDocValues()}.
-   * <p>
-   * To enable {@link DocValues} merging for different {@link DocValues} than
-   * the default override this method accordingly.
-   * <p>
-   */
-  protected boolean canMerge(FieldInfo info) {
-    return info.hasDocValues();
-  }
-  
-  /**
-   * Returns the {@link DocValues} {@link Type} for the given {@link FieldInfo}.
-   * By default this method uses {@link FieldInfo#getDocValuesType()}.
-   * <p>
-   * To enable {@link DocValues} merging for different {@link DocValues} than
-   * the default override this method accordingly.
-   * <p>
-   */
-  protected Type getDocValuesType(FieldInfo info) {
-    return info.getDocValuesType();
-  }
-  
-  /**
-   * Called during indexing if the indexing session is aborted due to a unrecoverable exception.
-   * This method should cleanup all resources.
-   */
-  public abstract void abort();
-
-  @Override
-  public abstract void close() throws IOException;
-}
Index: lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40FieldInfosFormat.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40FieldInfosFormat.java	(revision 1442822)
+++ lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40FieldInfosFormat.java	(working copy)
@@ -23,8 +23,6 @@
 import org.apache.lucene.codecs.FieldInfosFormat;
 import org.apache.lucene.codecs.FieldInfosReader;
 import org.apache.lucene.codecs.FieldInfosWriter;
-import org.apache.lucene.index.DocValues; // javadoc
-import org.apache.lucene.index.DocValues.Type; // javadoc
 import org.apache.lucene.store.DataOutput; // javadoc
 
 /**
@@ -72,32 +70,33 @@
  *    <li>DocValuesBits: a byte containing per-document value types. The type
  *        recorded as two four-bit integers, with the high-order bits representing
  *        <code>norms</code> options, and the low-order bits representing 
- *        {@link DocValues} options. Each four-bit integer can be decoded as such:
+ *        {@code DocValues} options. Each four-bit integer can be decoded as such:
  *        <ul>
  *          <li>0: no DocValues for this field.</li>
- *          <li>1: variable-width signed integers. ({@link Type#VAR_INTS VAR_INTS})</li>
- *          <li>2: 32-bit floating point values. ({@link Type#FLOAT_32 FLOAT_32})</li>
- *          <li>3: 64-bit floating point values. ({@link Type#FLOAT_64 FLOAT_64})</li>
- *          <li>4: fixed-length byte array values. ({@link Type#BYTES_FIXED_STRAIGHT BYTES_FIXED_STRAIGHT})</li>
- *          <li>5: fixed-length dereferenced byte array values. ({@link Type#BYTES_FIXED_DEREF BYTES_FIXED_DEREF})</li>
- *          <li>6: variable-length byte array values. ({@link Type#BYTES_VAR_STRAIGHT BYTES_VAR_STRAIGHT})</li>
- *          <li>7: variable-length dereferenced byte array values. ({@link Type#BYTES_VAR_DEREF BYTES_VAR_DEREF})</li>
- *          <li>8: 16-bit signed integers. ({@link Type#FIXED_INTS_16 FIXED_INTS_16})</li>
- *          <li>9: 32-bit signed integers. ({@link Type#FIXED_INTS_32 FIXED_INTS_32})</li>
- *          <li>10: 64-bit signed integers. ({@link Type#FIXED_INTS_64 FIXED_INTS_64})</li>
- *          <li>11: 8-bit signed integers. ({@link Type#FIXED_INTS_8 FIXED_INTS_8})</li>
- *          <li>12: fixed-length sorted byte array values. ({@link Type#BYTES_FIXED_SORTED BYTES_FIXED_SORTED})</li>
- *          <li>13: variable-length sorted byte array values. ({@link Type#BYTES_VAR_SORTED BYTES_VAR_SORTED})</li>
+ *          <li>1: variable-width signed integers. ({@code Type#VAR_INTS VAR_INTS})</li>
+ *          <li>2: 32-bit floating point values. ({@code Type#FLOAT_32 FLOAT_32})</li>
+ *          <li>3: 64-bit floating point values. ({@code Type#FLOAT_64 FLOAT_64})</li>
+ *          <li>4: fixed-length byte array values. ({@code Type#BYTES_FIXED_STRAIGHT BYTES_FIXED_STRAIGHT})</li>
+ *          <li>5: fixed-length dereferenced byte array values. ({@code Type#BYTES_FIXED_DEREF BYTES_FIXED_DEREF})</li>
+ *          <li>6: variable-length byte array values. ({@code Type#BYTES_VAR_STRAIGHT BYTES_VAR_STRAIGHT})</li>
+ *          <li>7: variable-length dereferenced byte array values. ({@code Type#BYTES_VAR_DEREF BYTES_VAR_DEREF})</li>
+ *          <li>8: 16-bit signed integers. ({@code Type#FIXED_INTS_16 FIXED_INTS_16})</li>
+ *          <li>9: 32-bit signed integers. ({@code Type#FIXED_INTS_32 FIXED_INTS_32})</li>
+ *          <li>10: 64-bit signed integers. ({@code Type#FIXED_INTS_64 FIXED_INTS_64})</li>
+ *          <li>11: 8-bit signed integers. ({@code Type#FIXED_INTS_8 FIXED_INTS_8})</li>
+ *          <li>12: fixed-length sorted byte array values. ({@code Type#BYTES_FIXED_SORTED BYTES_FIXED_SORTED})</li>
+ *          <li>13: variable-length sorted byte array values. ({@code Type#BYTES_VAR_SORTED BYTES_VAR_SORTED})</li>
  *        </ul>
  *    </li>
  *    <li>Attributes: a key-value map of codec-private attributes.</li>
  * </ul>
  *
  * @lucene.experimental
+ * @deprecated Only for reading old 4.0 and 4.1 segments
  */
+@Deprecated
 public class Lucene40FieldInfosFormat extends FieldInfosFormat {
   private final FieldInfosReader reader = new Lucene40FieldInfosReader();
-  private final FieldInfosWriter writer = new Lucene40FieldInfosWriter();
   
   /** Sole constructor. */
   public Lucene40FieldInfosFormat() {
@@ -110,6 +109,21 @@
 
   @Override
   public FieldInfosWriter getFieldInfosWriter() throws IOException {
-    return writer;
+    throw new UnsupportedOperationException("this codec can only be used for reading");
   }
+  
+  /** Extension of field infos */
+  static final String FIELD_INFOS_EXTENSION = "fnm";
+  
+  static final String CODEC_NAME = "Lucene40FieldInfos";
+  static final int FORMAT_START = 0;
+  static final int FORMAT_CURRENT = FORMAT_START;
+  
+  static final byte IS_INDEXED = 0x1;
+  static final byte STORE_TERMVECTOR = 0x2;
+  static final byte STORE_OFFSETS_IN_POSTINGS = 0x4;
+  static final byte OMIT_NORMS = 0x10;
+  static final byte STORE_PAYLOADS = 0x20;
+  static final byte OMIT_TERM_FREQ_AND_POSITIONS = 0x40;
+  static final byte OMIT_POSITIONS = -128;
 }
Index: lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40FieldInfosReader.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40FieldInfosReader.java	(revision 1442822)
+++ lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40FieldInfosReader.java	(working copy)
@@ -27,8 +27,8 @@
 import org.apache.lucene.index.FieldInfo;
 import org.apache.lucene.index.FieldInfos;
 import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.FieldInfo.DocValuesType;
 import org.apache.lucene.index.FieldInfo.IndexOptions;
-import org.apache.lucene.index.DocValues;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.IOContext;
 import org.apache.lucene.store.IndexInput;
@@ -39,8 +39,10 @@
  * 
  * @lucene.experimental
  * @see Lucene40FieldInfosFormat
+ * @deprecated Only for reading old 4.0 and 4.1 segments
  */
-public class Lucene40FieldInfosReader extends FieldInfosReader {
+@Deprecated
+class Lucene40FieldInfosReader extends FieldInfosReader {
 
   /** Sole constructor. */
   public Lucene40FieldInfosReader() {
@@ -48,14 +50,14 @@
 
   @Override
   public FieldInfos read(Directory directory, String segmentName, IOContext iocontext) throws IOException {
-    final String fileName = IndexFileNames.segmentFileName(segmentName, "", Lucene40FieldInfosWriter.FIELD_INFOS_EXTENSION);
+    final String fileName = IndexFileNames.segmentFileName(segmentName, "", Lucene40FieldInfosFormat.FIELD_INFOS_EXTENSION);
     IndexInput input = directory.openInput(fileName, iocontext);
     
     boolean success = false;
     try {
-      CodecUtil.checkHeader(input, Lucene40FieldInfosWriter.CODEC_NAME, 
-                                   Lucene40FieldInfosWriter.FORMAT_START, 
-                                   Lucene40FieldInfosWriter.FORMAT_CURRENT);
+      CodecUtil.checkHeader(input, Lucene40FieldInfosFormat.CODEC_NAME, 
+                                   Lucene40FieldInfosFormat.FORMAT_START, 
+                                   Lucene40FieldInfosFormat.FORMAT_CURRENT);
 
       final int size = input.readVInt(); //read in the size
       FieldInfo infos[] = new FieldInfo[size];
@@ -64,18 +66,18 @@
         String name = input.readString();
         final int fieldNumber = input.readVInt();
         byte bits = input.readByte();
-        boolean isIndexed = (bits & Lucene40FieldInfosWriter.IS_INDEXED) != 0;
-        boolean storeTermVector = (bits & Lucene40FieldInfosWriter.STORE_TERMVECTOR) != 0;
-        boolean omitNorms = (bits & Lucene40FieldInfosWriter.OMIT_NORMS) != 0;
-        boolean storePayloads = (bits & Lucene40FieldInfosWriter.STORE_PAYLOADS) != 0;
+        boolean isIndexed = (bits & Lucene40FieldInfosFormat.IS_INDEXED) != 0;
+        boolean storeTermVector = (bits & Lucene40FieldInfosFormat.STORE_TERMVECTOR) != 0;
+        boolean omitNorms = (bits & Lucene40FieldInfosFormat.OMIT_NORMS) != 0;
+        boolean storePayloads = (bits & Lucene40FieldInfosFormat.STORE_PAYLOADS) != 0;
         final IndexOptions indexOptions;
         if (!isIndexed) {
           indexOptions = null;
-        } else if ((bits & Lucene40FieldInfosWriter.OMIT_TERM_FREQ_AND_POSITIONS) != 0) {
+        } else if ((bits & Lucene40FieldInfosFormat.OMIT_TERM_FREQ_AND_POSITIONS) != 0) {
           indexOptions = IndexOptions.DOCS_ONLY;
-        } else if ((bits & Lucene40FieldInfosWriter.OMIT_POSITIONS) != 0) {
+        } else if ((bits & Lucene40FieldInfosFormat.OMIT_POSITIONS) != 0) {
           indexOptions = IndexOptions.DOCS_AND_FREQS;
-        } else if ((bits & Lucene40FieldInfosWriter.STORE_OFFSETS_IN_POSTINGS) != 0) {
+        } else if ((bits & Lucene40FieldInfosFormat.STORE_OFFSETS_IN_POSTINGS) != 0) {
           indexOptions = IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS;
         } else {
           indexOptions = IndexOptions.DOCS_AND_FREQS_AND_POSITIONS;
@@ -89,11 +91,20 @@
         }
         // DV Types are packed in one byte
         byte val = input.readByte();
-        final DocValues.Type docValuesType = getDocValuesType((byte) (val & 0x0F));
-        final DocValues.Type normsType = getDocValuesType((byte) ((val >>> 4) & 0x0F));
-        final Map<String,String> attributes = input.readStringStringMap();
+        final LegacyDocValuesType oldValuesType = getDocValuesType((byte) (val & 0x0F));
+        final LegacyDocValuesType oldNormsType = getDocValuesType((byte) ((val >>> 4) & 0x0F));
+        final Map<String,String> attributes = input.readStringStringMap();;
+        if (oldValuesType.mapping != null) {
+          attributes.put(LEGACY_DV_TYPE_KEY, oldValuesType.name());
+        }
+        if (oldNormsType.mapping != null) {
+          if (oldNormsType.mapping != DocValuesType.NUMERIC) {
+            throw new CorruptIndexException("invalid norm type: " + oldNormsType);
+          }
+          attributes.put(LEGACY_NORM_TYPE_KEY, oldNormsType.name());
+        }
         infos[i] = new FieldInfo(name, isIndexed, fieldNumber, storeTermVector, 
-          omitNorms, storePayloads, indexOptions, docValuesType, normsType, Collections.unmodifiableMap(attributes));
+          omitNorms, storePayloads, indexOptions, oldValuesType.mapping, oldNormsType.mapping, Collections.unmodifiableMap(attributes));
       }
 
       if (input.getFilePointer() != input.length()) {
@@ -110,39 +121,35 @@
       }
     }
   }
-
-  private static DocValues.Type getDocValuesType(final byte b) {
-    switch(b) {
-      case 0:
-        return null;
-      case 1:
-        return DocValues.Type.VAR_INTS;
-      case 2:
-        return DocValues.Type.FLOAT_32;
-      case 3:
-        return DocValues.Type.FLOAT_64;
-      case 4:
-        return DocValues.Type.BYTES_FIXED_STRAIGHT;
-      case 5:
-        return DocValues.Type.BYTES_FIXED_DEREF;
-      case 6:
-        return DocValues.Type.BYTES_VAR_STRAIGHT;
-      case 7:
-        return DocValues.Type.BYTES_VAR_DEREF;
-      case 8:
-        return DocValues.Type.FIXED_INTS_16;
-      case 9:
-        return DocValues.Type.FIXED_INTS_32;
-      case 10:
-        return DocValues.Type.FIXED_INTS_64;
-      case 11:
-        return DocValues.Type.FIXED_INTS_8;
-      case 12:
-        return DocValues.Type.BYTES_FIXED_SORTED;
-      case 13:
-        return DocValues.Type.BYTES_VAR_SORTED;
-      default:
-        throw new IllegalStateException("unhandled indexValues type " + b);
+  
+  static final String LEGACY_DV_TYPE_KEY = Lucene40FieldInfosReader.class.getSimpleName() + ".dvtype";
+  static final String LEGACY_NORM_TYPE_KEY = Lucene40FieldInfosReader.class.getSimpleName() + ".normtype";
+  
+  // mapping of 4.0 types -> 4.2 types
+  static enum LegacyDocValuesType {
+    NONE(null),
+    VAR_INTS(DocValuesType.NUMERIC),
+    FLOAT_32(DocValuesType.NUMERIC),
+    FLOAT_64(DocValuesType.NUMERIC),
+    BYTES_FIXED_STRAIGHT(DocValuesType.BINARY),
+    BYTES_FIXED_DEREF(DocValuesType.BINARY),
+    BYTES_VAR_STRAIGHT(DocValuesType.BINARY),
+    BYTES_VAR_DEREF(DocValuesType.BINARY),
+    FIXED_INTS_16(DocValuesType.NUMERIC),
+    FIXED_INTS_32(DocValuesType.NUMERIC),
+    FIXED_INTS_64(DocValuesType.NUMERIC),
+    FIXED_INTS_8(DocValuesType.NUMERIC),
+    BYTES_FIXED_SORTED(DocValuesType.SORTED),
+    BYTES_VAR_SORTED(DocValuesType.SORTED);
+    
+    final DocValuesType mapping;
+    LegacyDocValuesType(DocValuesType mapping) {
+      this.mapping = mapping;
     }
   }
+  
+  // decodes a 4.0 type
+  private static LegacyDocValuesType getDocValuesType(byte b) {
+    return LegacyDocValuesType.values()[b];
+  }
 }
Index: lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40NormsFormat.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40NormsFormat.java	(revision 1442822)
+++ lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40NormsFormat.java	(working copy)
@@ -19,17 +19,13 @@
 
 import java.io.IOException;
 
+import org.apache.lucene.codecs.DocValuesConsumer;
+import org.apache.lucene.codecs.DocValuesProducer;
 import org.apache.lucene.codecs.NormsFormat;
-import org.apache.lucene.codecs.PerDocConsumer;
-import org.apache.lucene.codecs.PerDocProducer;
-import org.apache.lucene.index.DocValues;
-import org.apache.lucene.index.DocValues.Type;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.AtomicReader;
-import org.apache.lucene.index.PerDocWriteState;
+import org.apache.lucene.index.IndexFileNames;
 import org.apache.lucene.index.SegmentReadState;
-import org.apache.lucene.store.CompoundFileDirectory; // javadocs
+import org.apache.lucene.index.SegmentWriteState;
+import org.apache.lucene.store.CompoundFileDirectory;
 
 /**
  * Lucene 4.0 Norms Format.
@@ -44,82 +40,24 @@
  * 
  * @see Lucene40DocValuesFormat
  * @lucene.experimental
+ * @deprecated Only for reading old 4.0 and 4.1 segments
  */
+@Deprecated
 public class Lucene40NormsFormat extends NormsFormat {
-  private final static String NORMS_SEGMENT_SUFFIX = "nrm";
 
   /** Sole constructor. */
-  public Lucene40NormsFormat() {
-  }
+  public Lucene40NormsFormat() {}
   
   @Override
-  public PerDocConsumer docsConsumer(PerDocWriteState state) throws IOException {
-    return new Lucene40NormsDocValuesConsumer(state, NORMS_SEGMENT_SUFFIX);
+  public DocValuesConsumer normsConsumer(SegmentWriteState state) throws IOException {
+    throw new UnsupportedOperationException("this codec can only be used for reading");
   }
 
   @Override
-  public PerDocProducer docsProducer(SegmentReadState state) throws IOException {
-    return new Lucene40NormsDocValuesProducer(state, NORMS_SEGMENT_SUFFIX);
+  public DocValuesProducer normsProducer(SegmentReadState state) throws IOException {
+    String filename = IndexFileNames.segmentFileName(state.segmentInfo.name, 
+                                                     "nrm", 
+                                                     IndexFileNames.COMPOUND_FILE_EXTENSION);
+    return new Lucene40DocValuesReader(state, filename, Lucene40FieldInfosReader.LEGACY_NORM_TYPE_KEY);
   }
-
-  /**
-   * Lucene 4.0 PerDocProducer implementation that uses compound file.
-   * 
-   * @see Lucene40DocValuesFormat
-   */
-  public static class Lucene40NormsDocValuesProducer extends Lucene40DocValuesProducer {
-
-    /** Sole constructor. */
-    public Lucene40NormsDocValuesProducer(SegmentReadState state,
-        String segmentSuffix) throws IOException {
-      super(state, segmentSuffix);
-    }
-
-    @Override
-    protected boolean canLoad(FieldInfo info) {
-      return info.hasNorms();
-    }
-
-    @Override
-    protected Type getDocValuesType(FieldInfo info) {
-      return info.getNormType();
-    }
-
-    @Override
-    protected boolean anyDocValuesFields(FieldInfos infos) {
-      return infos.hasNorms();
-    }
-    
-  }
-  
-  /**
-   * Lucene 4.0 PerDocConsumer implementation that uses compound file.
-   * 
-   * @see Lucene40DocValuesFormat
-   * @lucene.experimental
-   */
-  public static class Lucene40NormsDocValuesConsumer extends Lucene40DocValuesConsumer {
-
-    /** Sole constructor. */
-    public Lucene40NormsDocValuesConsumer(PerDocWriteState state,
-        String segmentSuffix) {
-      super(state, segmentSuffix);
-    }
-
-    @Override
-    protected DocValues getDocValuesForMerge(AtomicReader reader, FieldInfo info)
-        throws IOException {
-      return reader.normValues(info.name);
-    }
-
-    @Override
-    protected boolean canMerge(FieldInfo info) {
-      return info.hasNorms();
-    }
-
-    @Override
-    protected Type getDocValuesType(FieldInfo info) {
-      return info.getNormType();
-    }
-  }
 }
Index: lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40PostingsBaseFormat.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40PostingsBaseFormat.java	(revision 1442822)
+++ lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40PostingsBaseFormat.java	(working copy)
@@ -42,7 +42,7 @@
 
   @Override
   public PostingsReaderBase postingsReaderBase(SegmentReadState state) throws IOException {
-    return new Lucene40PostingsReader(state.dir, state.fieldInfos, state.segmentInfo, state.context, state.segmentSuffix);
+    return new Lucene40PostingsReader(state.directory, state.fieldInfos, state.segmentInfo, state.context, state.segmentSuffix);
   }
 
   @Override
Index: lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40Codec.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40Codec.java	(revision 1442822)
+++ lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40Codec.java	(working copy)
@@ -18,15 +18,16 @@
  */
 
 import org.apache.lucene.codecs.Codec;
-import org.apache.lucene.codecs.DocValuesFormat;
 import org.apache.lucene.codecs.FieldInfosFormat;
 import org.apache.lucene.codecs.FilterCodec;
 import org.apache.lucene.codecs.LiveDocsFormat;
-import org.apache.lucene.codecs.NormsFormat;
 import org.apache.lucene.codecs.PostingsFormat;
 import org.apache.lucene.codecs.SegmentInfoFormat;
+import org.apache.lucene.codecs.DocValuesFormat;
+import org.apache.lucene.codecs.NormsFormat;
 import org.apache.lucene.codecs.StoredFieldsFormat;
 import org.apache.lucene.codecs.TermVectorsFormat;
+import org.apache.lucene.codecs.lucene42.Lucene42NormsFormat;
 import org.apache.lucene.codecs.perfield.PerFieldPostingsFormat;
 
 /**
@@ -42,13 +43,11 @@
 // if they are backwards compatible or smallish we can probably do the backwards in the postingsreader
 // (it writes a minor version, etc).
 @Deprecated
-public final class Lucene40Codec extends Codec {
+public class Lucene40Codec extends Codec {
   private final StoredFieldsFormat fieldsFormat = new Lucene40StoredFieldsFormat();
   private final TermVectorsFormat vectorsFormat = new Lucene40TermVectorsFormat();
   private final FieldInfosFormat fieldInfosFormat = new Lucene40FieldInfosFormat();
-  private final DocValuesFormat docValuesFormat = new Lucene40DocValuesFormat();
   private final SegmentInfoFormat infosFormat = new Lucene40SegmentInfoFormat();
-  private final NormsFormat normsFormat = new Lucene40NormsFormat();
   private final LiveDocsFormat liveDocsFormat = new Lucene40LiveDocsFormat();
   
   private final PostingsFormat postingsFormat = new PerFieldPostingsFormat() {
@@ -74,17 +73,12 @@
   }
 
   @Override
-  public final DocValuesFormat docValuesFormat() {
-    return docValuesFormat;
-  }
-
-  @Override
   public final PostingsFormat postingsFormat() {
     return postingsFormat;
   }
   
   @Override
-  public final FieldInfosFormat fieldInfosFormat() {
+  public FieldInfosFormat fieldInfosFormat() {
     return fieldInfosFormat;
   }
   
@@ -92,12 +86,21 @@
   public final SegmentInfoFormat segmentInfoFormat() {
     return infosFormat;
   }
+  
+  private final DocValuesFormat defaultDVFormat = new Lucene40DocValuesFormat();
 
   @Override
-  public final NormsFormat normsFormat() {
+  public DocValuesFormat docValuesFormat() {
+    return defaultDVFormat;
+  }
+
+  private final NormsFormat normsFormat = new Lucene40NormsFormat();
+
+  @Override
+  public NormsFormat normsFormat() {
     return normsFormat;
   }
-  
+
   @Override
   public final LiveDocsFormat liveDocsFormat() {
     return liveDocsFormat;
Index: lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40FieldInfosWriter.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40FieldInfosWriter.java	(revision 1442822)
+++ lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40FieldInfosWriter.java	(working copy)
@@ -1,146 +0,0 @@
-package org.apache.lucene.codecs.lucene40;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-import java.io.IOException;
-
-import org.apache.lucene.codecs.CodecUtil;
-import org.apache.lucene.codecs.FieldInfosWriter;
-import org.apache.lucene.index.DocValues.Type;
-import org.apache.lucene.index.FieldInfo.IndexOptions;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.util.IOUtils;
-
-/**
- * Lucene 4.0 FieldInfos writer.
- * 
- * @see Lucene40FieldInfosFormat
- * @lucene.experimental
- */
-public class Lucene40FieldInfosWriter extends FieldInfosWriter {
-  
-  /** Extension of field infos */
-  static final String FIELD_INFOS_EXTENSION = "fnm";
-  
-  static final String CODEC_NAME = "Lucene40FieldInfos";
-  static final int FORMAT_START = 0;
-  static final int FORMAT_CURRENT = FORMAT_START;
-  
-  static final byte IS_INDEXED = 0x1;
-  static final byte STORE_TERMVECTOR = 0x2;
-  static final byte STORE_OFFSETS_IN_POSTINGS = 0x4;
-  static final byte OMIT_NORMS = 0x10;
-  static final byte STORE_PAYLOADS = 0x20;
-  static final byte OMIT_TERM_FREQ_AND_POSITIONS = 0x40;
-  static final byte OMIT_POSITIONS = -128;
-
-  /** Sole constructor. */
-  public Lucene40FieldInfosWriter() {
-  }
-  
-  @Override
-  public void write(Directory directory, String segmentName, FieldInfos infos, IOContext context) throws IOException {
-    final String fileName = IndexFileNames.segmentFileName(segmentName, "", FIELD_INFOS_EXTENSION);
-    IndexOutput output = directory.createOutput(fileName, context);
-    boolean success = false;
-    try {
-      CodecUtil.writeHeader(output, CODEC_NAME, FORMAT_CURRENT);
-      output.writeVInt(infos.size());
-      for (FieldInfo fi : infos) {
-        IndexOptions indexOptions = fi.getIndexOptions();
-        byte bits = 0x0;
-        if (fi.hasVectors()) bits |= STORE_TERMVECTOR;
-        if (fi.omitsNorms()) bits |= OMIT_NORMS;
-        if (fi.hasPayloads()) bits |= STORE_PAYLOADS;
-        if (fi.isIndexed()) {
-          bits |= IS_INDEXED;
-          assert indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 || !fi.hasPayloads();
-          if (indexOptions == IndexOptions.DOCS_ONLY) {
-            bits |= OMIT_TERM_FREQ_AND_POSITIONS;
-          } else if (indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) {
-            bits |= STORE_OFFSETS_IN_POSTINGS;
-          } else if (indexOptions == IndexOptions.DOCS_AND_FREQS) {
-            bits |= OMIT_POSITIONS;
-          }
-        }
-        output.writeString(fi.name);
-        output.writeVInt(fi.number);
-        output.writeByte(bits);
-
-        // pack the DV types in one byte
-        final byte dv = docValuesByte(fi.getDocValuesType());
-        final byte nrm = docValuesByte(fi.getNormType());
-        assert (dv & (~0xF)) == 0 && (nrm & (~0x0F)) == 0;
-        byte val = (byte) (0xff & ((nrm << 4) | dv));
-        output.writeByte(val);
-        output.writeStringStringMap(fi.attributes());
-      }
-      success = true;
-    } finally {
-      if (success) {
-        output.close();
-      } else {
-        IOUtils.closeWhileHandlingException(output);
-      }
-    }
-  }
-
-  /** Returns the byte used to encode the {@link
-   *  Type} for each field. */
-  public byte docValuesByte(Type type) {
-    if (type == null) {
-      return 0;
-    } else {
-      switch(type) {
-      case VAR_INTS:
-        return 1;
-      case FLOAT_32:
-        return 2;
-      case FLOAT_64:
-        return 3;
-      case BYTES_FIXED_STRAIGHT:
-        return 4;
-      case BYTES_FIXED_DEREF:
-        return 5;
-      case BYTES_VAR_STRAIGHT:
-        return 6;
-      case BYTES_VAR_DEREF:
-        return 7;
-      case FIXED_INTS_16:
-        return 8;
-      case FIXED_INTS_32:
-        return 9;
-      case FIXED_INTS_64:
-        return 10;
-      case FIXED_INTS_8:
-        return 11;
-      case BYTES_FIXED_SORTED:
-        return 12;
-      case BYTES_VAR_SORTED:
-        return 13;
-      default:
-        throw new IllegalStateException("unhandled indexValues type " + type);
-      }
-    }
-  }
-  
-}
Index: lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40PostingsFormat.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40PostingsFormat.java	(revision 1442822)
+++ lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40PostingsFormat.java	(working copy)
@@ -248,12 +248,12 @@
 
   @Override
   public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {
-    PostingsReaderBase postings = new Lucene40PostingsReader(state.dir, state.fieldInfos, state.segmentInfo, state.context, state.segmentSuffix);
+    PostingsReaderBase postings = new Lucene40PostingsReader(state.directory, state.fieldInfos, state.segmentInfo, state.context, state.segmentSuffix);
 
     boolean success = false;
     try {
       FieldsProducer ret = new BlockTreeTermsReader(
-                                                    state.dir,
+                                                    state.directory,
                                                     state.fieldInfos,
                                                     state.segmentInfo,
                                                     postings,
Index: lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40DocValuesProducer.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40DocValuesProducer.java	(revision 1442822)
+++ lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40DocValuesProducer.java	(working copy)
@@ -1,113 +0,0 @@
-package org.apache.lucene.codecs.lucene40;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.Closeable;
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Collection;
-import java.util.Map;
-import java.util.TreeMap;
-
-import org.apache.lucene.codecs.PerDocProducerBase;
-import org.apache.lucene.codecs.lucene40.values.Bytes;
-import org.apache.lucene.codecs.lucene40.values.Floats;
-import org.apache.lucene.codecs.lucene40.values.Ints;
-import org.apache.lucene.index.DocValues;
-import org.apache.lucene.index.DocValues.Type;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.SegmentReadState;
-import org.apache.lucene.store.CompoundFileDirectory;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.util.IOUtils;
-
-/**
- * Lucene 4.0 PerDocProducer implementation that uses compound file.
- * 
- * @see Lucene40DocValuesFormat
- * @lucene.experimental
- */
-public class Lucene40DocValuesProducer extends PerDocProducerBase {
-  /** Maps field name to {@link DocValues} instance. */
-  protected final TreeMap<String,DocValues> docValues;
-  private final Directory cfs;
-  /**
-   * Creates a new {@link Lucene40DocValuesProducer} instance and loads all
-   * {@link DocValues} instances for this segment and codec.
-   */
-  public Lucene40DocValuesProducer(SegmentReadState state, String segmentSuffix) throws IOException {
-    if (anyDocValuesFields(state.fieldInfos)) {
-      cfs = new CompoundFileDirectory(state.dir, 
-                                      IndexFileNames.segmentFileName(state.segmentInfo.name,
-                                                                     segmentSuffix, IndexFileNames.COMPOUND_FILE_EXTENSION), 
-                                      state.context, false);
-      docValues = load(state.fieldInfos, state.segmentInfo.name, state.segmentInfo.getDocCount(), cfs, state.context);
-    } else {
-      cfs = null;
-      docValues = new TreeMap<String,DocValues>();
-    }
-  }
-  
-  @Override
-  protected Map<String,DocValues> docValues() {
-    return docValues;
-  }
-
-  @Override
-  protected void closeInternal(Collection<? extends Closeable> closeables) throws IOException {
-    if (cfs != null) {
-      final ArrayList<Closeable> list = new ArrayList<Closeable>(closeables);
-      list.add(cfs);
-      IOUtils.close(list);
-    } else {
-      IOUtils.close(closeables);
-    }
-  }
-
-  @Override
-  protected DocValues loadDocValues(int docCount, Directory dir, String id,
-      Type type, IOContext context) throws IOException {
-      switch (type) {
-      case FIXED_INTS_16:
-      case FIXED_INTS_32:
-      case FIXED_INTS_64:
-      case FIXED_INTS_8:
-      case VAR_INTS:
-        return Ints.getValues(dir, id, docCount, type, context);
-      case FLOAT_32:
-        return Floats.getValues(dir, id, docCount, context, type);
-      case FLOAT_64:
-        return Floats.getValues(dir, id, docCount, context, type);
-      case BYTES_FIXED_STRAIGHT:
-        return Bytes.getValues(dir, id, Bytes.Mode.STRAIGHT, true, docCount, getComparator(), context);
-      case BYTES_FIXED_DEREF:
-        return Bytes.getValues(dir, id, Bytes.Mode.DEREF, true, docCount, getComparator(), context);
-      case BYTES_FIXED_SORTED:
-        return Bytes.getValues(dir, id, Bytes.Mode.SORTED, true, docCount, getComparator(), context);
-      case BYTES_VAR_STRAIGHT:
-        return Bytes.getValues(dir, id, Bytes.Mode.STRAIGHT, false, docCount, getComparator(), context);
-      case BYTES_VAR_DEREF:
-        return Bytes.getValues(dir, id, Bytes.Mode.DEREF, false, docCount, getComparator(), context);
-      case BYTES_VAR_SORTED:
-        return Bytes.getValues(dir, id, Bytes.Mode.SORTED, false, docCount, getComparator(), context);
-      default:
-        throw new IllegalStateException("unrecognized index values mode " + type);
-      }
-    }
-}
Index: lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40DocValuesFormat.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40DocValuesFormat.java	(revision 1442822)
+++ lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40DocValuesFormat.java	(working copy)
@@ -20,16 +20,15 @@
 import java.io.IOException;
 
 import org.apache.lucene.codecs.CodecUtil;
+import org.apache.lucene.codecs.DocValuesConsumer;
 import org.apache.lucene.codecs.DocValuesFormat;
-import org.apache.lucene.codecs.PerDocConsumer;
-import org.apache.lucene.codecs.PerDocProducer;
-import org.apache.lucene.index.DocValues; // javadocs
-import org.apache.lucene.index.DocValues.Type; // javadocs
-import org.apache.lucene.index.PerDocWriteState;
+import org.apache.lucene.codecs.DocValuesProducer;
+import org.apache.lucene.index.IndexFileNames;
 import org.apache.lucene.index.SegmentReadState;
-import org.apache.lucene.store.CompoundFileDirectory; // javadocs
-import org.apache.lucene.store.DataOutput; // javadocs
-import org.apache.lucene.util.packed.PackedInts; // javadocs
+import org.apache.lucene.index.SegmentWriteState;
+import org.apache.lucene.store.CompoundFileDirectory;
+import org.apache.lucene.store.DataOutput;
+import org.apache.lucene.util.packed.PackedInts;
 
 /**
  * Lucene 4.0 DocValues format.
@@ -45,7 +44,7 @@
  *   <li><tt>&lt;segment&gt;_&lt;fieldNumber&gt;.idx</tt>: index into the .dat for DEREF types</li>
  * </ul>
  * <p>
- * There are several many types of {@link DocValues} with different encodings.
+ * There are several many types of {@code DocValues} with different encodings.
  * From the perspective of filenames, all types store their values in <tt>.dat</tt>
  * entries within the compound file. In the case of dereferenced/sorted types, the <tt>.dat</tt>
  * actually contains only the unique values, and an additional <tt>.idx</tt> file contains
@@ -53,42 +52,34 @@
  * </p>
  * Formats:
  * <ul>
- *    <li>{@link Type#VAR_INTS VAR_INTS} .dat --&gt; Header, PackedType, MinValue, 
+ *    <li>{@code VAR_INTS} .dat --&gt; Header, PackedType, MinValue, 
  *        DefaultValue, PackedStream</li>
- *    <li>{@link Type#FIXED_INTS_8 FIXED_INTS_8} .dat --&gt; Header, ValueSize, 
+ *    <li>{@code FIXED_INTS_8} .dat --&gt; Header, ValueSize, 
  *        {@link DataOutput#writeByte Byte}<sup>maxdoc</sup></li>
- *    <li>{@link Type#FIXED_INTS_16 FIXED_INTS_16} .dat --&gt; Header, ValueSize,
+ *    <li>{@code FIXED_INTS_16} .dat --&gt; Header, ValueSize,
  *        {@link DataOutput#writeShort Short}<sup>maxdoc</sup></li>
- *    <li>{@link Type#FIXED_INTS_32 FIXED_INTS_32} .dat --&gt; Header, ValueSize,
+ *    <li>{@code FIXED_INTS_32} .dat --&gt; Header, ValueSize,
  *        {@link DataOutput#writeInt Int32}<sup>maxdoc</sup></li>
- *    <li>{@link Type#FIXED_INTS_64 FIXED_INTS_64} .dat --&gt; Header, ValueSize,
+ *    <li>{@code FIXED_INTS_64} .dat --&gt; Header, ValueSize,
  *        {@link DataOutput#writeLong Int64}<sup>maxdoc</sup></li>
- *    <li>{@link Type#FLOAT_32 FLOAT_32} .dat --&gt; Header, ValueSize,
- *        Float32<sup>maxdoc</sup></li>
- *    <li>{@link Type#FLOAT_64 FLOAT_64} .dat --&gt; Header, ValueSize,
- *        Float64<sup>maxdoc</sup></li>
- *    <li>{@link Type#BYTES_FIXED_STRAIGHT BYTES_FIXED_STRAIGHT} .dat --&gt; Header, ValueSize,
+ *    <li>{@code FLOAT_32} .dat --&gt; Header, ValueSize, Float32<sup>maxdoc</sup></li>
+ *    <li>{@code FLOAT_64} .dat --&gt; Header, ValueSize, Float64<sup>maxdoc</sup></li>
+ *    <li>{@code BYTES_FIXED_STRAIGHT} .dat --&gt; Header, ValueSize,
  *        ({@link DataOutput#writeByte Byte} * ValueSize)<sup>maxdoc</sup></li>
- *    <li>{@link Type#BYTES_VAR_STRAIGHT BYTES_VAR_STRAIGHT} .idx --&gt; Header, MaxAddress,
- *        Addresses</li>
- *    <li>{@link Type#BYTES_VAR_STRAIGHT BYTES_VAR_STRAIGHT} .dat --&gt; Header, TotalBytes,
- *        Addresses, ({@link DataOutput#writeByte Byte} *
- *        <i>variable ValueSize</i>)<sup>maxdoc</sup></li>
- *    <li>{@link Type#BYTES_FIXED_DEREF BYTES_FIXED_DEREF} .idx --&gt; Header, NumValues,
- *        Addresses</li>
- *    <li>{@link Type#BYTES_FIXED_DEREF BYTES_FIXED_DEREF} .dat --&gt; Header, ValueSize,
+ *    <li>{@code BYTES_VAR_STRAIGHT} .idx --&gt; Header, TotalBytes, Addresses</li>
+ *    <li>{@code BYTES_VAR_STRAIGHT} .dat --&gt; Header,
+          ({@link DataOutput#writeByte Byte} * <i>variable ValueSize</i>)<sup>maxdoc</sup></li>
+ *    <li>{@code BYTES_FIXED_DEREF} .idx --&gt; Header, NumValues, Addresses</li>
+ *    <li>{@code BYTES_FIXED_DEREF} .dat --&gt; Header, ValueSize,
  *        ({@link DataOutput#writeByte Byte} * ValueSize)<sup>NumValues</sup></li>
- *    <li>{@link Type#BYTES_VAR_DEREF BYTES_VAR_DEREF} .idx --&gt; Header, TotalVarBytes,
- *        Addresses</li>
- *    <li>{@link Type#BYTES_VAR_DEREF BYTES_VAR_DEREF} .dat --&gt; Header,
+ *    <li>{@code BYTES_VAR_DEREF} .idx --&gt; Header, TotalVarBytes, Addresses</li>
+ *    <li>{@code BYTES_VAR_DEREF} .dat --&gt; Header,
  *        (LengthPrefix + {@link DataOutput#writeByte Byte} * <i>variable ValueSize</i>)<sup>NumValues</sup></li>
- *    <li>{@link Type#BYTES_FIXED_SORTED BYTES_FIXED_SORTED} .idx --&gt; Header, NumValues,
- *        Ordinals</li>
- *    <li>{@link Type#BYTES_FIXED_SORTED BYTES_FIXED_SORTED} .dat --&gt; Header, ValueSize,
+ *    <li>{@code BYTES_FIXED_SORTED} .idx --&gt; Header, NumValues, Ordinals</li>
+ *    <li>{@code BYTES_FIXED_SORTED} .dat --&gt; Header, ValueSize,
  *        ({@link DataOutput#writeByte Byte} * ValueSize)<sup>NumValues</sup></li>
- *    <li>{@link Type#BYTES_VAR_SORTED BYTES_VAR_SORTED} .idx --&gt; Header, TotalVarBytes,
- *        Addresses, Ordinals</li>
- *    <li>{@link Type#BYTES_VAR_SORTED BYTES_VAR_SORTED} .dat --&gt; Header,
+ *    <li>{@code BYTES_VAR_SORTED} .idx --&gt; Header, TotalVarBytes, Addresses, Ordinals</li>
+ *    <li>{@code BYTES_VAR_SORTED} .dat --&gt; Header,
  *        ({@link DataOutput#writeByte Byte} * <i>variable ValueSize</i>)<sup>NumValues</sup></li>
  * </ul>
  * Data Types:
@@ -122,25 +113,86 @@
  *        In the VAR_SORTED case, there is double indirection (docid -> ordinal -> address), but
  *        an additional sentinel ordinal+address is always written (so there are NumValues+1 ordinals). To
  *        determine the length, ord+1's address is looked up as well.</li>
- *    <li>{@link Type#BYTES_VAR_STRAIGHT BYTES_VAR_STRAIGHT} in contrast to other straight 
+ *    <li>{@code BYTES_VAR_STRAIGHT BYTES_VAR_STRAIGHT} in contrast to other straight 
  *        variants uses a <tt>.idx</tt> file to improve lookup perfromance. In contrast to 
- *        {@link Type#BYTES_VAR_DEREF BYTES_VAR_DEREF} it doesn't apply deduplication of the document values.
+ *        {@code BYTES_VAR_DEREF BYTES_VAR_DEREF} it doesn't apply deduplication of the document values.
  *    </li>
  * </ul>
+ * @deprecated Only for reading old 4.0 and 4.1 segments
  */
+@Deprecated
+// NOTE: not registered in SPI, doesnt respect segment suffix, etc
+// for back compat only!
 public class Lucene40DocValuesFormat extends DocValuesFormat {
-
+  
   /** Sole constructor. */
   public Lucene40DocValuesFormat() {
+    super("Lucene40");
   }
-
+  
   @Override
-  public PerDocConsumer docsConsumer(PerDocWriteState state) throws IOException {
-    return new Lucene40DocValuesConsumer(state, Lucene40DocValuesConsumer.DOC_VALUES_SEGMENT_SUFFIX);
+  public DocValuesConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
+    throw new UnsupportedOperationException("this codec can only be used for reading");
   }
-
+  
   @Override
-  public PerDocProducer docsProducer(SegmentReadState state) throws IOException {
-    return new Lucene40DocValuesProducer(state, Lucene40DocValuesConsumer.DOC_VALUES_SEGMENT_SUFFIX);
+  public DocValuesProducer fieldsProducer(SegmentReadState state) throws IOException {
+    String filename = IndexFileNames.segmentFileName(state.segmentInfo.name, 
+                                                     "dv", 
+                                                     IndexFileNames.COMPOUND_FILE_EXTENSION);
+    return new Lucene40DocValuesReader(state, filename, Lucene40FieldInfosReader.LEGACY_DV_TYPE_KEY);
   }
+  
+  // constants for VAR_INTS
+  static final String VAR_INTS_CODEC_NAME = "PackedInts";
+  static final int VAR_INTS_VERSION_START = 0;
+  static final int VAR_INTS_VERSION_CURRENT = VAR_INTS_VERSION_START;
+  static final byte VAR_INTS_PACKED = 0x00;
+  static final byte VAR_INTS_FIXED_64 = 0x01;
+  
+  // constants for FIXED_INTS_8, FIXED_INTS_16, FIXED_INTS_32, FIXED_INTS_64
+  static final String INTS_CODEC_NAME = "Ints";
+  static final int INTS_VERSION_START = 0;
+  static final int INTS_VERSION_CURRENT = INTS_VERSION_START;
+  
+  // constants for FLOAT_32, FLOAT_64
+  static final String FLOATS_CODEC_NAME = "Floats";
+  static final int FLOATS_VERSION_START = 0;
+  static final int FLOATS_VERSION_CURRENT = FLOATS_VERSION_START;
+  
+  // constants for BYTES_FIXED_STRAIGHT
+  static final String BYTES_FIXED_STRAIGHT_CODEC_NAME = "FixedStraightBytes";
+  static final int BYTES_FIXED_STRAIGHT_VERSION_START = 0;
+  static final int BYTES_FIXED_STRAIGHT_VERSION_CURRENT = BYTES_FIXED_STRAIGHT_VERSION_START;
+  
+  // constants for BYTES_VAR_STRAIGHT
+  static final String BYTES_VAR_STRAIGHT_CODEC_NAME_IDX = "VarStraightBytesIdx";
+  static final String BYTES_VAR_STRAIGHT_CODEC_NAME_DAT = "VarStraightBytesDat";
+  static final int BYTES_VAR_STRAIGHT_VERSION_START = 0;
+  static final int BYTES_VAR_STRAIGHT_VERSION_CURRENT = BYTES_VAR_STRAIGHT_VERSION_START;
+  
+  // constants for BYTES_FIXED_DEREF
+  static final String BYTES_FIXED_DEREF_CODEC_NAME_IDX = "FixedDerefBytesIdx";
+  static final String BYTES_FIXED_DEREF_CODEC_NAME_DAT = "FixedDerefBytesDat";
+  static final int BYTES_FIXED_DEREF_VERSION_START = 0;
+  static final int BYTES_FIXED_DEREF_VERSION_CURRENT = BYTES_FIXED_DEREF_VERSION_START;
+  
+  // constants for BYTES_VAR_DEREF
+  static final String BYTES_VAR_DEREF_CODEC_NAME_IDX = "VarDerefBytesIdx";
+  static final String BYTES_VAR_DEREF_CODEC_NAME_DAT = "VarDerefBytesDat";
+  static final int BYTES_VAR_DEREF_VERSION_START = 0;
+  static final int BYTES_VAR_DEREF_VERSION_CURRENT = BYTES_VAR_DEREF_VERSION_START;
+  
+  // constants for BYTES_FIXED_SORTED
+  static final String BYTES_FIXED_SORTED_CODEC_NAME_IDX = "FixedSortedBytesIdx";
+  static final String BYTES_FIXED_SORTED_CODEC_NAME_DAT = "FixedSortedBytesDat";
+  static final int BYTES_FIXED_SORTED_VERSION_START = 0;
+  static final int BYTES_FIXED_SORTED_VERSION_CURRENT = BYTES_FIXED_SORTED_VERSION_START;
+  
+  // constants for BYTES_VAR_SORTED
+  // NOTE THIS IS NOT A BUG! 4.0 actually screwed this up (VAR_SORTED and VAR_DEREF have same codec header)
+  static final String BYTES_VAR_SORTED_CODEC_NAME_IDX = "VarDerefBytesIdx";
+  static final String BYTES_VAR_SORTED_CODEC_NAME_DAT = "VarDerefBytesDat";
+  static final int BYTES_VAR_SORTED_VERSION_START = 0;
+  static final int BYTES_VAR_SORTED_VERSION_CURRENT = BYTES_VAR_SORTED_VERSION_START;
 }
Index: lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40DocValuesConsumer.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40DocValuesConsumer.java	(revision 1442822)
+++ lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40DocValuesConsumer.java	(working copy)
@@ -1,86 +0,0 @@
-package org.apache.lucene.codecs.lucene40;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.codecs.lucene40.values.DocValuesWriterBase;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.PerDocWriteState;
-import org.apache.lucene.store.CompoundFileDirectory;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.util.IOUtils;
-
-/**
- * Lucene 4.0 PerDocConsumer implementation that uses compound file.
- * 
- * @see Lucene40DocValuesFormat
- * @lucene.experimental
- */
-public class Lucene40DocValuesConsumer extends DocValuesWriterBase {
-  private final Directory mainDirectory;
-  private Directory directory;
-  private final String segmentSuffix;
-
-  /** Segment suffix used when writing doc values index files. */
-  public final static String DOC_VALUES_SEGMENT_SUFFIX = "dv";
-
-  /** Sole constructor. */
-  public Lucene40DocValuesConsumer(PerDocWriteState state, String segmentSuffix) {
-    super(state);
-    this.segmentSuffix = segmentSuffix;
-    mainDirectory = state.directory;
-    //TODO maybe we should enable a global CFS that all codecs can pull on demand to further reduce the number of files?
-  }
-  
-  @Override
-  protected Directory getDirectory() throws IOException {
-    // lazy init
-    if (directory == null) {
-      directory = new CompoundFileDirectory(mainDirectory,
-                                            IndexFileNames.segmentFileName(segmentName, segmentSuffix,
-                                                                           IndexFileNames.COMPOUND_FILE_EXTENSION), context, true);
-    }
-    return directory;
-  }
-
-  @Override
-  public void close() throws IOException {
-    if (directory != null) {
-      directory.close();
-    }
-  }
-
-  @Override
-  public void abort() {
-    try {
-      close();
-    } catch (Throwable t) {
-      // ignore
-    } finally {
-      // TODO: why the inconsistency here? we do this, but not SimpleText (which says IFD
-      // will do it).
-      // TODO: check that IFD really does this always, even if codec abort() throws a 
-      // RuntimeException (e.g. ThreadInterruptedException)
-      IOUtils.deleteFilesIgnoringExceptions(mainDirectory, IndexFileNames.segmentFileName(
-        segmentName, segmentSuffix, IndexFileNames.COMPOUND_FILE_EXTENSION),
-        IndexFileNames.segmentFileName(segmentName, segmentSuffix,
-            IndexFileNames.COMPOUND_FILE_ENTRIES_EXTENSION));
-    }
-  }
-}
Index: lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40DocValuesReader.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40DocValuesReader.java	(revision 0)
+++ lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40DocValuesReader.java	(working copy)
@@ -0,0 +1,621 @@
+package org.apache.lucene.codecs.lucene40;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.HashMap;
+import java.util.Map;
+
+import org.apache.lucene.codecs.CodecUtil;
+import org.apache.lucene.codecs.DocValuesProducer;
+import org.apache.lucene.codecs.lucene40.Lucene40FieldInfosReader.LegacyDocValuesType;
+import org.apache.lucene.index.BinaryDocValues;
+import org.apache.lucene.index.CorruptIndexException;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.NumericDocValues;
+import org.apache.lucene.index.SegmentReadState;
+import org.apache.lucene.index.SortedDocValues;
+import org.apache.lucene.store.CompoundFileDirectory;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util.PagedBytes;
+import org.apache.lucene.util.packed.PackedInts;
+
+/**
+ * Reads the 4.0 format of norms/docvalues
+ * @lucene.experimental
+ * @deprecated Only for reading old 4.0 and 4.1 segments
+ */
+@Deprecated
+final class Lucene40DocValuesReader extends DocValuesProducer {
+  private final Directory dir;
+  private final SegmentReadState state;
+  private final String legacyKey;
+  private static final String segmentSuffix = "dv";
+
+  // ram instances we have already loaded
+  private final Map<Integer,NumericDocValues> numericInstances = 
+      new HashMap<Integer,NumericDocValues>();
+  private final Map<Integer,BinaryDocValues> binaryInstances = 
+      new HashMap<Integer,BinaryDocValues>();
+  private final Map<Integer,SortedDocValues> sortedInstances = 
+      new HashMap<Integer,SortedDocValues>();
+  
+  Lucene40DocValuesReader(SegmentReadState state, String filename, String legacyKey) throws IOException {
+    this.state = state;
+    this.legacyKey = legacyKey;
+    this.dir = new CompoundFileDirectory(state.directory, filename, state.context, false);
+  }
+  
+  @Override
+  public synchronized NumericDocValues getNumeric(FieldInfo field) throws IOException {
+    NumericDocValues instance = numericInstances.get(field.number);
+    if (instance == null) {
+      String fileName = IndexFileNames.segmentFileName(state.segmentInfo.name + "_" + Integer.toString(field.number), segmentSuffix, "dat");
+      IndexInput input = dir.openInput(fileName, state.context);
+      boolean success = false;
+      try {
+        switch(LegacyDocValuesType.valueOf(field.getAttribute(legacyKey))) {
+          case VAR_INTS:
+            instance = loadVarIntsField(field, input);
+            break;
+          case FIXED_INTS_8:
+            instance = loadByteField(field, input);
+            break;
+          case FIXED_INTS_16:
+            instance = loadShortField(field, input);
+            break;
+          case FIXED_INTS_32:
+            instance = loadIntField(field, input);
+            break;
+          case FIXED_INTS_64:
+            instance = loadLongField(field, input);
+            break;
+          case FLOAT_32:
+            instance = loadFloatField(field, input);
+            break;
+          case FLOAT_64:
+            instance = loadDoubleField(field, input);
+            break;
+          default: 
+            throw new AssertionError();
+        }
+        if (input.getFilePointer() != input.length()) {
+          throw new CorruptIndexException("did not read all bytes from file \"" + fileName + "\": read " + input.getFilePointer() + " vs size " + input.length() + " (resource: " + input + ")");
+        }
+        success = true;
+      } finally {
+        if (success) {
+          IOUtils.close(input);
+        } else {
+          IOUtils.closeWhileHandlingException(input);
+        }
+      }
+      numericInstances.put(field.number, instance);
+    }
+    return instance;
+  }
+  
+  private NumericDocValues loadVarIntsField(FieldInfo field, IndexInput input) throws IOException {
+    CodecUtil.checkHeader(input, Lucene40DocValuesFormat.VAR_INTS_CODEC_NAME, 
+                                 Lucene40DocValuesFormat.VAR_INTS_VERSION_START, 
+                                 Lucene40DocValuesFormat.VAR_INTS_VERSION_CURRENT);
+    byte header = input.readByte();
+    if (header == Lucene40DocValuesFormat.VAR_INTS_FIXED_64) {
+      int maxDoc = state.segmentInfo.getDocCount();
+      final long values[] = new long[maxDoc];
+      for (int i = 0; i < values.length; i++) {
+        values[i] = input.readLong();
+      }
+      return new NumericDocValues() {
+        @Override
+        public long get(int docID) {
+          return values[docID];
+        }
+      };
+    } else if (header == Lucene40DocValuesFormat.VAR_INTS_PACKED) {
+      final long minValue = input.readLong();
+      final long defaultValue = input.readLong();
+      final PackedInts.Reader reader = PackedInts.getReader(input);
+      return new NumericDocValues() {
+        @Override
+        public long get(int docID) {
+          final long value = reader.get(docID);
+          if (value == defaultValue) {
+            return 0;
+          } else {
+            return minValue + value;
+          }
+        }
+      };
+    } else {
+      throw new CorruptIndexException("invalid VAR_INTS header byte: " + header + " (resource=" + input + ")");
+    }
+  }
+  
+  private NumericDocValues loadByteField(FieldInfo field, IndexInput input) throws IOException {
+    CodecUtil.checkHeader(input, Lucene40DocValuesFormat.INTS_CODEC_NAME, 
+                                 Lucene40DocValuesFormat.INTS_VERSION_START, 
+                                 Lucene40DocValuesFormat.INTS_VERSION_CURRENT);
+    int valueSize = input.readInt();
+    if (valueSize != 1) {
+      throw new CorruptIndexException("invalid valueSize: " + valueSize);
+    }
+    int maxDoc = state.segmentInfo.getDocCount();
+    final byte values[] = new byte[maxDoc];
+    input.readBytes(values, 0, values.length);
+    return new NumericDocValues() {
+      @Override
+      public long get(int docID) {
+        return values[docID];
+      }
+    };
+  }
+  
+  private NumericDocValues loadShortField(FieldInfo field, IndexInput input) throws IOException {
+    CodecUtil.checkHeader(input, Lucene40DocValuesFormat.INTS_CODEC_NAME, 
+                                 Lucene40DocValuesFormat.INTS_VERSION_START, 
+                                 Lucene40DocValuesFormat.INTS_VERSION_CURRENT);
+    int valueSize = input.readInt();
+    if (valueSize != 2) {
+      throw new CorruptIndexException("invalid valueSize: " + valueSize);
+    }
+    int maxDoc = state.segmentInfo.getDocCount();
+    final short values[] = new short[maxDoc];
+    for (int i = 0; i < values.length; i++) {
+      values[i] = input.readShort();
+    }
+    return new NumericDocValues() {
+      @Override
+      public long get(int docID) {
+        return values[docID];
+      }
+    };
+  }
+  
+  private NumericDocValues loadIntField(FieldInfo field, IndexInput input) throws IOException {
+    CodecUtil.checkHeader(input, Lucene40DocValuesFormat.INTS_CODEC_NAME, 
+                                 Lucene40DocValuesFormat.INTS_VERSION_START, 
+                                 Lucene40DocValuesFormat.INTS_VERSION_CURRENT);
+    int valueSize = input.readInt();
+    if (valueSize != 4) {
+      throw new CorruptIndexException("invalid valueSize: " + valueSize);
+    }
+    int maxDoc = state.segmentInfo.getDocCount();
+    final int values[] = new int[maxDoc];
+    for (int i = 0; i < values.length; i++) {
+      values[i] = input.readInt();
+    }
+    return new NumericDocValues() {
+      @Override
+      public long get(int docID) {
+        return values[docID];
+      }
+    };
+  }
+  
+  private NumericDocValues loadLongField(FieldInfo field, IndexInput input) throws IOException {
+    CodecUtil.checkHeader(input, Lucene40DocValuesFormat.INTS_CODEC_NAME, 
+                                 Lucene40DocValuesFormat.INTS_VERSION_START, 
+                                 Lucene40DocValuesFormat.INTS_VERSION_CURRENT);
+    int valueSize = input.readInt();
+    if (valueSize != 8) {
+      throw new CorruptIndexException("invalid valueSize: " + valueSize);
+    }
+    int maxDoc = state.segmentInfo.getDocCount();
+    final long values[] = new long[maxDoc];
+    for (int i = 0; i < values.length; i++) {
+      values[i] = input.readLong();
+    }
+    return new NumericDocValues() {
+      @Override
+      public long get(int docID) {
+        return values[docID];
+      }
+    };
+  }
+  
+  private NumericDocValues loadFloatField(FieldInfo field, IndexInput input) throws IOException {
+    CodecUtil.checkHeader(input, Lucene40DocValuesFormat.FLOATS_CODEC_NAME, 
+                                 Lucene40DocValuesFormat.FLOATS_VERSION_START, 
+                                 Lucene40DocValuesFormat.FLOATS_VERSION_CURRENT);
+    int valueSize = input.readInt();
+    if (valueSize != 4) {
+      throw new CorruptIndexException("invalid valueSize: " + valueSize);
+    }
+    int maxDoc = state.segmentInfo.getDocCount();
+    final int values[] = new int[maxDoc];
+    for (int i = 0; i < values.length; i++) {
+      values[i] = input.readInt();
+    }
+    return new NumericDocValues() {
+      @Override
+      public long get(int docID) {
+        return values[docID];
+      }
+    };
+  }
+  
+  private NumericDocValues loadDoubleField(FieldInfo field, IndexInput input) throws IOException {
+    CodecUtil.checkHeader(input, Lucene40DocValuesFormat.FLOATS_CODEC_NAME, 
+                                 Lucene40DocValuesFormat.FLOATS_VERSION_START, 
+                                 Lucene40DocValuesFormat.FLOATS_VERSION_CURRENT);
+    int valueSize = input.readInt();
+    if (valueSize != 8) {
+      throw new CorruptIndexException("invalid valueSize: " + valueSize);
+    }
+    int maxDoc = state.segmentInfo.getDocCount();
+    final long values[] = new long[maxDoc];
+    for (int i = 0; i < values.length; i++) {
+      values[i] = input.readLong();
+    }
+    return new NumericDocValues() {
+      @Override
+      public long get(int docID) {
+        return values[docID];
+      }
+    };
+  }
+
+  @Override
+  public synchronized BinaryDocValues getBinary(FieldInfo field) throws IOException {
+    BinaryDocValues instance = binaryInstances.get(field.number);
+    if (instance == null) {
+      switch(LegacyDocValuesType.valueOf(field.getAttribute(legacyKey))) {
+        case BYTES_FIXED_STRAIGHT:
+          instance = loadBytesFixedStraight(field);
+          break;
+        case BYTES_VAR_STRAIGHT:
+          instance = loadBytesVarStraight(field);
+          break;
+        case BYTES_FIXED_DEREF:
+          instance = loadBytesFixedDeref(field);
+          break;
+        case BYTES_VAR_DEREF:
+          instance = loadBytesVarDeref(field);
+          break;
+        default:
+          throw new AssertionError();
+      }
+      binaryInstances.put(field.number, instance);
+    }
+    return instance;
+  }
+  
+  private BinaryDocValues loadBytesFixedStraight(FieldInfo field) throws IOException {
+    String fileName = IndexFileNames.segmentFileName(state.segmentInfo.name + "_" + Integer.toString(field.number), segmentSuffix, "dat");
+    IndexInput input = dir.openInput(fileName, state.context);
+    boolean success = false;
+    try {
+      CodecUtil.checkHeader(input, Lucene40DocValuesFormat.BYTES_FIXED_STRAIGHT_CODEC_NAME, 
+                                   Lucene40DocValuesFormat.BYTES_FIXED_STRAIGHT_VERSION_START, 
+                                   Lucene40DocValuesFormat.BYTES_FIXED_STRAIGHT_VERSION_CURRENT);
+      final int fixedLength = input.readInt();
+      PagedBytes bytes = new PagedBytes(16);
+      bytes.copy(input, fixedLength * (long)state.segmentInfo.getDocCount());
+      final PagedBytes.Reader bytesReader = bytes.freeze(true);
+      if (input.getFilePointer() != input.length()) {
+        throw new CorruptIndexException("did not read all bytes from file \"" + fileName + "\": read " + input.getFilePointer() + " vs size " + input.length() + " (resource: " + input + ")");
+      }
+      success = true;
+      return new BinaryDocValues() {
+        @Override
+        public void get(int docID, BytesRef result) {
+          bytesReader.fillSlice(result, fixedLength * (long)docID, fixedLength);
+        }
+      };
+    } finally {
+      if (success) {
+        IOUtils.close(input);
+      } else {
+        IOUtils.closeWhileHandlingException(input);
+      }
+    }
+  }
+  
+  private BinaryDocValues loadBytesVarStraight(FieldInfo field) throws IOException {
+    String dataName = IndexFileNames.segmentFileName(state.segmentInfo.name + "_" + Integer.toString(field.number), segmentSuffix, "dat");
+    String indexName = IndexFileNames.segmentFileName(state.segmentInfo.name + "_" + Integer.toString(field.number), segmentSuffix, "idx");
+    IndexInput data = null;
+    IndexInput index = null;
+    boolean success = false;
+    try {
+      data = dir.openInput(dataName, state.context);
+      CodecUtil.checkHeader(data, Lucene40DocValuesFormat.BYTES_VAR_STRAIGHT_CODEC_NAME_DAT, 
+                                  Lucene40DocValuesFormat.BYTES_VAR_STRAIGHT_VERSION_START, 
+                                  Lucene40DocValuesFormat.BYTES_VAR_STRAIGHT_VERSION_CURRENT);
+      index = dir.openInput(indexName, state.context);
+      CodecUtil.checkHeader(index, Lucene40DocValuesFormat.BYTES_VAR_STRAIGHT_CODEC_NAME_IDX, 
+                                   Lucene40DocValuesFormat.BYTES_VAR_STRAIGHT_VERSION_START, 
+                                   Lucene40DocValuesFormat.BYTES_VAR_STRAIGHT_VERSION_CURRENT);
+      long totalBytes = index.readVLong();
+      PagedBytes bytes = new PagedBytes(16);
+      bytes.copy(data, totalBytes);
+      final PagedBytes.Reader bytesReader = bytes.freeze(true);
+      final PackedInts.Reader reader = PackedInts.getReader(index);
+      if (data.getFilePointer() != data.length()) {
+        throw new CorruptIndexException("did not read all bytes from file \"" + dataName + "\": read " + data.getFilePointer() + " vs size " + data.length() + " (resource: " + data + ")");
+      }
+      if (index.getFilePointer() != index.length()) {
+        throw new CorruptIndexException("did not read all bytes from file \"" + indexName + "\": read " + index.getFilePointer() + " vs size " + index.length() + " (resource: " + index + ")");
+      }
+      success = true;
+      return new BinaryDocValues() {
+        @Override
+        public void get(int docID, BytesRef result) {
+          long startAddress = reader.get(docID);
+          long endAddress = reader.get(docID+1);
+          bytesReader.fillSlice(result, startAddress, (int)(endAddress - startAddress));
+        }
+      };
+    } finally {
+      if (success) {
+        IOUtils.close(data, index);
+      } else {
+        IOUtils.closeWhileHandlingException(data, index);
+      }
+    }
+  }
+  
+  private BinaryDocValues loadBytesFixedDeref(FieldInfo field) throws IOException {
+    String dataName = IndexFileNames.segmentFileName(state.segmentInfo.name + "_" + Integer.toString(field.number), segmentSuffix, "dat");
+    String indexName = IndexFileNames.segmentFileName(state.segmentInfo.name + "_" + Integer.toString(field.number), segmentSuffix, "idx");
+    IndexInput data = null;
+    IndexInput index = null;
+    boolean success = false;
+    try {
+      data = dir.openInput(dataName, state.context);
+      CodecUtil.checkHeader(data, Lucene40DocValuesFormat.BYTES_FIXED_DEREF_CODEC_NAME_DAT, 
+                                  Lucene40DocValuesFormat.BYTES_FIXED_DEREF_VERSION_START, 
+                                  Lucene40DocValuesFormat.BYTES_FIXED_DEREF_VERSION_CURRENT);
+      index = dir.openInput(indexName, state.context);
+      CodecUtil.checkHeader(index, Lucene40DocValuesFormat.BYTES_FIXED_DEREF_CODEC_NAME_IDX, 
+                                   Lucene40DocValuesFormat.BYTES_FIXED_DEREF_VERSION_START, 
+                                   Lucene40DocValuesFormat.BYTES_FIXED_DEREF_VERSION_CURRENT);
+      
+      final int fixedLength = data.readInt();
+      final int valueCount = index.readInt();
+      PagedBytes bytes = new PagedBytes(16);
+      bytes.copy(data, fixedLength * (long) valueCount);
+      final PagedBytes.Reader bytesReader = bytes.freeze(true);
+      final PackedInts.Reader reader = PackedInts.getReader(index);
+      if (data.getFilePointer() != data.length()) {
+        throw new CorruptIndexException("did not read all bytes from file \"" + dataName + "\": read " + data.getFilePointer() + " vs size " + data.length() + " (resource: " + data + ")");
+      }
+      if (index.getFilePointer() != index.length()) {
+        throw new CorruptIndexException("did not read all bytes from file \"" + indexName + "\": read " + index.getFilePointer() + " vs size " + index.length() + " (resource: " + index + ")");
+      }
+      success = true;
+      return new BinaryDocValues() {
+        @Override
+        public void get(int docID, BytesRef result) {
+          final long offset = fixedLength * reader.get(docID);
+          bytesReader.fillSlice(result, offset, fixedLength);
+        }
+      };
+    } finally {
+      if (success) {
+        IOUtils.close(data, index);
+      } else {
+        IOUtils.closeWhileHandlingException(data, index);
+      }
+    }
+  }
+  
+  private BinaryDocValues loadBytesVarDeref(FieldInfo field) throws IOException {
+    String dataName = IndexFileNames.segmentFileName(state.segmentInfo.name + "_" + Integer.toString(field.number), segmentSuffix, "dat");
+    String indexName = IndexFileNames.segmentFileName(state.segmentInfo.name + "_" + Integer.toString(field.number), segmentSuffix, "idx");
+    IndexInput data = null;
+    IndexInput index = null;
+    boolean success = false;
+    try {
+      data = dir.openInput(dataName, state.context);
+      CodecUtil.checkHeader(data, Lucene40DocValuesFormat.BYTES_VAR_DEREF_CODEC_NAME_DAT, 
+                                  Lucene40DocValuesFormat.BYTES_VAR_DEREF_VERSION_START, 
+                                  Lucene40DocValuesFormat.BYTES_VAR_DEREF_VERSION_CURRENT);
+      index = dir.openInput(indexName, state.context);
+      CodecUtil.checkHeader(index, Lucene40DocValuesFormat.BYTES_VAR_DEREF_CODEC_NAME_IDX, 
+                                   Lucene40DocValuesFormat.BYTES_VAR_DEREF_VERSION_START, 
+                                   Lucene40DocValuesFormat.BYTES_VAR_DEREF_VERSION_CURRENT);
+      
+      final long totalBytes = index.readLong();
+      final PagedBytes bytes = new PagedBytes(16);
+      bytes.copy(data, totalBytes);
+      final PagedBytes.Reader bytesReader = bytes.freeze(true);
+      final PackedInts.Reader reader = PackedInts.getReader(index);
+      if (data.getFilePointer() != data.length()) {
+        throw new CorruptIndexException("did not read all bytes from file \"" + dataName + "\": read " + data.getFilePointer() + " vs size " + data.length() + " (resource: " + data + ")");
+      }
+      if (index.getFilePointer() != index.length()) {
+        throw new CorruptIndexException("did not read all bytes from file \"" + indexName + "\": read " + index.getFilePointer() + " vs size " + index.length() + " (resource: " + index + ")");
+      }
+      success = true;
+      return new BinaryDocValues() {
+        @Override
+        public void get(int docID, BytesRef result) {
+          long startAddress = reader.get(docID);
+          BytesRef lengthBytes = new BytesRef();
+          bytesReader.fillSlice(lengthBytes, startAddress, 1);
+          byte code = lengthBytes.bytes[lengthBytes.offset];
+          if ((code & 128) == 0) {
+            // length is 1 byte
+            bytesReader.fillSlice(result, startAddress + 1, (int) code);
+          } else {
+            bytesReader.fillSlice(lengthBytes, startAddress + 1, 1);
+            int length = ((code & 0x7f) << 8) | (lengthBytes.bytes[lengthBytes.offset] & 0xff);
+            bytesReader.fillSlice(result, startAddress + 2, length);
+          }
+        }
+      };
+    } finally {
+      if (success) {
+        IOUtils.close(data, index);
+      } else {
+        IOUtils.closeWhileHandlingException(data, index);
+      }
+    }
+  }
+
+  @Override
+  public synchronized SortedDocValues getSorted(FieldInfo field) throws IOException {
+    SortedDocValues instance = sortedInstances.get(field.number);
+    if (instance == null) {
+      String dataName = IndexFileNames.segmentFileName(state.segmentInfo.name + "_" + Integer.toString(field.number), segmentSuffix, "dat");
+      String indexName = IndexFileNames.segmentFileName(state.segmentInfo.name + "_" + Integer.toString(field.number), segmentSuffix, "idx");
+      IndexInput data = null;
+      IndexInput index = null;
+      boolean success = false;
+      try {
+        data = dir.openInput(dataName, state.context);
+        index = dir.openInput(indexName, state.context);
+        switch(LegacyDocValuesType.valueOf(field.getAttribute(legacyKey))) {
+          case BYTES_FIXED_SORTED:
+            instance = loadBytesFixedSorted(field, data, index);
+            break;
+          case BYTES_VAR_SORTED:
+            instance = loadBytesVarSorted(field, data, index);
+            break;
+          default:
+            throw new AssertionError();
+        }
+        if (data.getFilePointer() != data.length()) {
+          throw new CorruptIndexException("did not read all bytes from file \"" + dataName + "\": read " + data.getFilePointer() + " vs size " + data.length() + " (resource: " + data + ")");
+        }
+        if (index.getFilePointer() != index.length()) {
+          throw new CorruptIndexException("did not read all bytes from file \"" + indexName + "\": read " + index.getFilePointer() + " vs size " + index.length() + " (resource: " + index + ")");
+        }
+        success = true;
+      } finally {
+        if (success) {
+          IOUtils.close(data, index);
+        } else {
+          IOUtils.closeWhileHandlingException(data, index);
+        }
+      }
+      sortedInstances.put(field.number, instance);
+    }
+    return instance;
+  }
+  
+  private SortedDocValues loadBytesFixedSorted(FieldInfo field, IndexInput data, IndexInput index) throws IOException {
+    CodecUtil.checkHeader(data, Lucene40DocValuesFormat.BYTES_FIXED_SORTED_CODEC_NAME_DAT, 
+                                Lucene40DocValuesFormat.BYTES_FIXED_SORTED_VERSION_START, 
+                                Lucene40DocValuesFormat.BYTES_FIXED_SORTED_VERSION_CURRENT);
+    CodecUtil.checkHeader(index, Lucene40DocValuesFormat.BYTES_FIXED_SORTED_CODEC_NAME_IDX, 
+                                 Lucene40DocValuesFormat.BYTES_FIXED_SORTED_VERSION_START, 
+                                 Lucene40DocValuesFormat.BYTES_FIXED_SORTED_VERSION_CURRENT);
+    
+    final int fixedLength = data.readInt();
+    final int valueCount = index.readInt();
+    
+    PagedBytes bytes = new PagedBytes(16);
+    bytes.copy(data, fixedLength * (long) valueCount);
+    final PagedBytes.Reader bytesReader = bytes.freeze(true);
+    final PackedInts.Reader reader = PackedInts.getReader(index);
+    
+    return correctBuggyOrds(new SortedDocValues() {
+      @Override
+      public int getOrd(int docID) {
+        return (int) reader.get(docID);
+      }
+
+      @Override
+      public void lookupOrd(int ord, BytesRef result) {
+        bytesReader.fillSlice(result, fixedLength * (long) ord, fixedLength);
+      }
+
+      @Override
+      public int getValueCount() {
+        return valueCount;
+      }
+    });
+  }
+  
+  private SortedDocValues loadBytesVarSorted(FieldInfo field, IndexInput data, IndexInput index) throws IOException {
+    CodecUtil.checkHeader(data, Lucene40DocValuesFormat.BYTES_VAR_SORTED_CODEC_NAME_DAT, 
+                                Lucene40DocValuesFormat.BYTES_VAR_SORTED_VERSION_START, 
+                                Lucene40DocValuesFormat.BYTES_VAR_SORTED_VERSION_CURRENT);
+    CodecUtil.checkHeader(index, Lucene40DocValuesFormat.BYTES_VAR_SORTED_CODEC_NAME_IDX, 
+                                 Lucene40DocValuesFormat.BYTES_VAR_SORTED_VERSION_START, 
+                                 Lucene40DocValuesFormat.BYTES_VAR_SORTED_VERSION_CURRENT);
+  
+    long maxAddress = index.readLong();
+    PagedBytes bytes = new PagedBytes(16);
+    bytes.copy(data, maxAddress);
+    final PagedBytes.Reader bytesReader = bytes.freeze(true);
+    final PackedInts.Reader addressReader = PackedInts.getReader(index);
+    final PackedInts.Reader ordsReader = PackedInts.getReader(index);
+    
+    final int valueCount = addressReader.size() - 1;
+    
+    return correctBuggyOrds(new SortedDocValues() {
+      @Override
+      public int getOrd(int docID) {
+        return (int)ordsReader.get(docID);
+      }
+
+      @Override
+      public void lookupOrd(int ord, BytesRef result) {
+        long startAddress = addressReader.get(ord);
+        long endAddress = addressReader.get(ord+1);
+        bytesReader.fillSlice(result, startAddress, (int)(endAddress - startAddress));
+      }
+
+      @Override
+      public int getValueCount() {
+        return valueCount;
+      }
+    });
+  }
+  
+  // detects and corrects LUCENE-4717 in old indexes
+  private SortedDocValues correctBuggyOrds(final SortedDocValues in) {
+    final int maxDoc = state.segmentInfo.getDocCount();
+    for (int i = 0; i < maxDoc; i++) {
+      if (in.getOrd(i) == 0) {
+        return in; // ok
+      }
+    }
+    
+    // we had ord holes, return an ord-shifting-impl that corrects the bug
+    return new SortedDocValues() {
+      @Override
+      public int getOrd(int docID) {
+        return in.getOrd(docID) - 1;
+      }
+
+      @Override
+      public void lookupOrd(int ord, BytesRef result) {
+        in.lookupOrd(ord+1, result);
+      }
+
+      @Override
+      public int getValueCount() {
+        return in.getValueCount() - 1;
+      }
+    };
+  }
+  
+  @Override
+  public void close() throws IOException {
+    dir.close();
+  }
+}

Property changes on: lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40DocValuesReader.java
___________________________________________________________________
Added: svn:eol-style
## -0,0 +1 ##
+native
Index: lucene/core/src/java/org/apache/lucene/codecs/lucene40/values/Writer.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/codecs/lucene40/values/Writer.java	(revision 1442822)
+++ lucene/core/src/java/org/apache/lucene/codecs/lucene40/values/Writer.java	(working copy)
@@ -1,126 +0,0 @@
-package org.apache.lucene.codecs.lucene40.values;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-import java.io.IOException;
-import java.util.Comparator;
-
-import org.apache.lucene.codecs.DocValuesConsumer;
-import org.apache.lucene.index.DocValues.Type;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.Counter;
-import org.apache.lucene.util.packed.PackedInts;
-
-/**
- * Abstract API for per-document stored primitive values of type <tt>byte[]</tt>
- * , <tt>long</tt> or <tt>double</tt>. The API accepts a single value for each
- * document. The underlying storage mechanism, file formats, data-structures and
- * representations depend on the actual implementation.
- * <p>
- * Document IDs passed to this API must always be increasing unless stated
- * otherwise.
- * </p>
- * 
- * @lucene.experimental
- */
-abstract class Writer extends DocValuesConsumer {
-  protected final Counter bytesUsed;
-  protected Type type;
-
-  /**
-   * Creates a new {@link Writer}.
-   * 
-   * @param bytesUsed
-   *          bytes-usage tracking reference used by implementation to track
-   *          internally allocated memory. All tracked bytes must be released
-   *          once {@link #finish(int)} has been called.
-   */
-  protected Writer(Counter bytesUsed, Type type) {
-    this.bytesUsed = bytesUsed;
-    this.type = type;
-  }
-  
-  
-
-  @Override
-  protected Type getType() {
-    return type;
-  }
-
-
-
-  /**
-   * Factory method to create a {@link Writer} instance for a given type. This
-   * method returns default implementations for each of the different types
-   * defined in the {@link Type} enumeration.
-   * 
-   * @param type
-   *          the {@link Type} to create the {@link Writer} for
-   * @param id
-   *          the file name id used to create files within the writer.
-   * @param directory
-   *          the {@link Directory} to create the files from.
-   * @param bytesUsed
-   *          a byte-usage tracking reference
-   * @param acceptableOverheadRatio
-   *          how to trade space for speed. This option is only applicable for
-   *          docvalues of type {@link Type#BYTES_FIXED_SORTED} and
-   *          {@link Type#BYTES_VAR_SORTED}.
-   * @return a new {@link Writer} instance for the given {@link Type}
-   * @see PackedInts#getReader(org.apache.lucene.store.DataInput)
-   */
-  public static DocValuesConsumer create(Type type, String id, Directory directory,
-      Comparator<BytesRef> comp, Counter bytesUsed, IOContext context, float acceptableOverheadRatio) {
-    if (comp == null) {
-      comp = BytesRef.getUTF8SortedAsUnicodeComparator();
-    }
-    switch (type) {
-    case FIXED_INTS_16:
-    case FIXED_INTS_32:
-    case FIXED_INTS_64:
-    case FIXED_INTS_8:
-    case VAR_INTS:
-      return Ints.getWriter(directory, id, bytesUsed, type, context);
-    case FLOAT_32:
-      return Floats.getWriter(directory, id, bytesUsed, context, type);
-    case FLOAT_64:
-      return Floats.getWriter(directory, id, bytesUsed, context, type);
-    case BYTES_FIXED_STRAIGHT:
-      return Bytes.getWriter(directory, id, Bytes.Mode.STRAIGHT, true, comp,
-          bytesUsed, context, acceptableOverheadRatio);
-    case BYTES_FIXED_DEREF:
-      return Bytes.getWriter(directory, id, Bytes.Mode.DEREF, true, comp,
-          bytesUsed, context, acceptableOverheadRatio);
-    case BYTES_FIXED_SORTED:
-      return Bytes.getWriter(directory, id, Bytes.Mode.SORTED, true, comp,
-          bytesUsed, context, acceptableOverheadRatio);
-    case BYTES_VAR_STRAIGHT:
-      return Bytes.getWriter(directory, id, Bytes.Mode.STRAIGHT, false, comp,
-          bytesUsed, context, acceptableOverheadRatio);
-    case BYTES_VAR_DEREF:
-      return Bytes.getWriter(directory, id, Bytes.Mode.DEREF, false, comp,
-          bytesUsed, context, acceptableOverheadRatio);
-    case BYTES_VAR_SORTED:
-      return Bytes.getWriter(directory, id, Bytes.Mode.SORTED, false, comp,
-          bytesUsed, context, acceptableOverheadRatio);
-    default:
-      throw new IllegalArgumentException("Unknown Values: " + type);
-    }
-  }
-}
Index: lucene/core/src/java/org/apache/lucene/codecs/lucene40/values/Ints.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/codecs/lucene40/values/Ints.java	(revision 1442822)
+++ lucene/core/src/java/org/apache/lucene/codecs/lucene40/values/Ints.java	(working copy)
@@ -1,161 +0,0 @@
-package org.apache.lucene.codecs.lucene40.values;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.codecs.DocValuesArraySource;
-import org.apache.lucene.codecs.DocValuesConsumer;
-import org.apache.lucene.index.DocValues.Source;
-import org.apache.lucene.index.DocValues.Type;
-import org.apache.lucene.index.DocValues;
-import org.apache.lucene.index.IndexableField;
-import org.apache.lucene.index.StorableField;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.Counter;
-import org.apache.lucene.util.IOUtils;
-
-/**
- * Stores ints packed and fixed with fixed-bit precision.
- * 
- * @lucene.experimental
- */
-public final class Ints {
-  /** Codec name, written in the header. */
-  protected static final String CODEC_NAME = "Ints";
-
-  /** Initial version. */
-  protected static final int VERSION_START = 0;
-
-  /** Current version. */
-  protected static final int VERSION_CURRENT = VERSION_START;
-
-  /** Sole constructor. */
-  private Ints() {
-  }
-  
-  /** Creates and returns a {@link DocValuesConsumer} to
-   *  write int values. */
-  public static DocValuesConsumer getWriter(Directory dir, String id, Counter bytesUsed,
-      Type type, IOContext context) {
-    return type == Type.VAR_INTS ? new PackedIntValues.PackedIntsWriter(dir, id,
-        bytesUsed, context) : new IntsWriter(dir, id, bytesUsed, context, type);
-  }
-
-  /** Creates and returns a {@link DocValues} to
-   *  read previously written int values. */
-  public static DocValues getValues(Directory dir, String id, int numDocs,
-      Type type, IOContext context) throws IOException {
-    return type == Type.VAR_INTS ? new PackedIntValues.PackedIntsReader(dir, id,
-        numDocs, context) : new IntsReader(dir, id, numDocs, context, type);
-  }
-  
-  private static Type sizeToType(int size) {
-    switch (size) {
-    case 1:
-      return Type.FIXED_INTS_8;
-    case 2:
-      return Type.FIXED_INTS_16;
-    case 4:
-      return Type.FIXED_INTS_32;
-    case 8:
-      return Type.FIXED_INTS_64;
-    default:
-      throw new IllegalStateException("illegal size " + size);
-    }
-  }
-  
-  private static int typeToSize(Type type) {
-    switch (type) {
-    case FIXED_INTS_16:
-      return 2;
-    case FIXED_INTS_32:
-      return 4;
-    case FIXED_INTS_64:
-      return 8;
-    case FIXED_INTS_8:
-      return 1;
-    default:
-      throw new IllegalStateException("illegal type " + type);
-    }
-  }
-
-
-  static class IntsWriter extends FixedStraightBytesImpl.Writer {
-    private final DocValuesArraySource template;
-
-    public IntsWriter(Directory dir, String id, Counter bytesUsed,
-        IOContext context, Type valueType) {
-      this(dir, id, CODEC_NAME, VERSION_CURRENT, bytesUsed, context, valueType);
-    }
-
-    protected IntsWriter(Directory dir, String id, String codecName,
-        int version, Counter bytesUsed, IOContext context, Type valueType) {
-      super(dir, id, codecName, version, bytesUsed, context);
-      size = typeToSize(valueType);
-      this.bytesRef = new BytesRef(size);
-      bytesRef.length = size;
-      template = DocValuesArraySource.forType(valueType);
-    }
-    
-    @Override
-    protected void setMergeBytes(Source source, int sourceDoc) {
-      final long value = source.getInt(sourceDoc);
-      template.toBytes(value, bytesRef);
-    }
-    
-    @Override
-    public void add(int docID, StorableField value) throws IOException {
-      template.toBytes(value.numericValue().longValue(), bytesRef);
-      bytesSpareField.setBytesValue(bytesRef);
-      super.add(docID, bytesSpareField);
-    }
-
-    @Override
-    protected boolean tryBulkMerge(DocValues docValues) {
-      // only bulk merge if value type is the same otherwise size differs
-      return super.tryBulkMerge(docValues) && docValues.getType() == template.getType();
-    }
-  }
-  
-  final static class IntsReader extends FixedStraightBytesImpl.FixedStraightReader {
-    private final DocValuesArraySource arrayTemplate;
-
-    IntsReader(Directory dir, String id, int maxDoc, IOContext context, Type type)
-        throws IOException {
-      super(dir, id, CODEC_NAME, VERSION_CURRENT, maxDoc,
-          context, type);
-      arrayTemplate = DocValuesArraySource.forType(type);
-      assert arrayTemplate != null;
-      assert type == sizeToType(size);
-    }
-
-    @Override
-    protected Source loadSource() throws IOException {
-      final IndexInput indexInput = cloneData();
-      try {
-        return arrayTemplate.newFromInput(indexInput, maxDoc);
-      } finally {
-        IOUtils.close(indexInput);
-      }
-    }
-  }
-}
Index: lucene/core/src/java/org/apache/lucene/codecs/lucene40/values/DocValuesWriterBase.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/codecs/lucene40/values/DocValuesWriterBase.java	(revision 1442822)
+++ lucene/core/src/java/org/apache/lucene/codecs/lucene40/values/DocValuesWriterBase.java	(working copy)
@@ -1,107 +0,0 @@
-package org.apache.lucene.codecs.lucene40.values;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Comparator;
-
-import org.apache.lucene.codecs.DocValuesConsumer;
-import org.apache.lucene.codecs.PerDocProducerBase;
-import org.apache.lucene.codecs.PerDocConsumer;
-import org.apache.lucene.codecs.lucene40.values.Writer;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.PerDocWriteState;
-import org.apache.lucene.index.DocValues.Type; // javadoc
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.Counter;
-import org.apache.lucene.util.packed.PackedInts;
-
-/**
- * Abstract base class for PerDocConsumer implementations
- *
- * @lucene.experimental
- */
-public abstract class DocValuesWriterBase extends PerDocConsumer {
-  /** Segment name to use when writing files. */
-  protected final String segmentName;
-  private final Counter bytesUsed;
-
-  /** {@link IOContext} to use when writing files. */
-  protected final IOContext context;
-
-  private final float acceptableOverheadRatio;
-
-  /**
-   * Filename extension for index files
-   */
-  public static final String INDEX_EXTENSION = "idx";
-  
-  /**
-   * Filename extension for data files.
-   */
-  public static final String DATA_EXTENSION = "dat";
-
-  /**
-   * Creates {@code DocValuesWriterBase}, using {@link
-   * PackedInts#FAST}.
-   * @param state The state to initiate a {@link PerDocConsumer} instance
-   */
-  protected DocValuesWriterBase(PerDocWriteState state) {
-    this(state, PackedInts.FAST);
-  }
-
-  /**
-   * Creates {@code DocValuesWriterBase}.
-   * @param state The state to initiate a {@link PerDocConsumer} instance
-   * @param acceptableOverheadRatio
-   *          how to trade space for speed. This option is only applicable for
-   *          docvalues of type {@link Type#BYTES_FIXED_SORTED} and
-   *          {@link Type#BYTES_VAR_SORTED}.
-   * @see PackedInts#getReader(org.apache.lucene.store.DataInput)
-   */
-  protected DocValuesWriterBase(PerDocWriteState state, float acceptableOverheadRatio) {
-    this.segmentName = state.segmentInfo.name;
-    this.bytesUsed = state.bytesUsed;
-    this.context = state.context;
-    this.acceptableOverheadRatio = acceptableOverheadRatio;
-  }
-
-  /** Returns the {@link Directory} that files should be
-   *  written to. */
-  protected abstract Directory getDirectory() throws IOException;
-  
-  @Override
-  public void close() throws IOException {   
-  }
-
-  @Override
-  public DocValuesConsumer addValuesField(Type valueType, FieldInfo field) throws IOException {
-    return Writer.create(valueType,
-        PerDocProducerBase.docValuesId(segmentName, field.number), 
-        getDirectory(), getComparator(), bytesUsed, context, acceptableOverheadRatio);
-  }
-
-
-  /** Returns the comparator used to sort {@link BytesRef}
-   *  values. */
-  public Comparator<BytesRef> getComparator() throws IOException {
-    return BytesRef.getUTF8SortedAsUnicodeComparator();
-  }
-}
Index: lucene/core/src/java/org/apache/lucene/codecs/lucene40/values/FixedStraightBytesImpl.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/codecs/lucene40/values/FixedStraightBytesImpl.java	(revision 1442822)
+++ lucene/core/src/java/org/apache/lucene/codecs/lucene40/values/FixedStraightBytesImpl.java	(working copy)
@@ -1,369 +0,0 @@
-package org.apache.lucene.codecs.lucene40.values;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.codecs.lucene40.values.Bytes.BytesReaderBase;
-import org.apache.lucene.codecs.lucene40.values.Bytes.BytesSourceBase;
-import org.apache.lucene.codecs.lucene40.values.Bytes.BytesWriterBase;
-import org.apache.lucene.document.StoredField;
-import org.apache.lucene.document.StraightBytesDocValuesField;
-import org.apache.lucene.index.DocValues.Source;
-import org.apache.lucene.index.DocValues.Type;
-import org.apache.lucene.index.DocValues;
-import org.apache.lucene.index.StorableField;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.util.Bits;
-import org.apache.lucene.util.ByteBlockPool.DirectTrackingAllocator;
-import org.apache.lucene.util.ByteBlockPool;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.Counter;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.PagedBytes;
-
-import static org.apache.lucene.util.ByteBlockPool.BYTE_BLOCK_SIZE;
-
-// Simplest storage: stores fixed length byte[] per
-// document, with no dedup and no sorting.
-/**
- * @lucene.experimental
- */
-class FixedStraightBytesImpl {
-
-  static final String CODEC_NAME = "FixedStraightBytes";
-  static final int VERSION_START = 0;
-  static final int VERSION_CURRENT = VERSION_START;
-  
-  static abstract class FixedBytesWriterBase extends BytesWriterBase {
-    protected final StraightBytesDocValuesField bytesSpareField = new StraightBytesDocValuesField("", new BytesRef(), true);
-    protected int lastDocID = -1;
-    // start at -1 if the first added value is > 0
-    protected int size = -1;
-    private final int byteBlockSize = BYTE_BLOCK_SIZE;
-    private final ByteBlockPool pool;
-
-    protected FixedBytesWriterBase(Directory dir, String id, String codecNameDat,
-        int version, Counter bytesUsed, IOContext context) {
-     this(dir, id, codecNameDat, version, bytesUsed, context, Type.BYTES_FIXED_STRAIGHT);
-    }
-    
-    protected FixedBytesWriterBase(Directory dir, String id, String codecNameDat,
-        int version, Counter bytesUsed, IOContext context, Type type) {
-      super(dir, id, null, codecNameDat, version, bytesUsed, context, type);
-      pool = new ByteBlockPool(new DirectTrackingAllocator(bytesUsed));
-      pool.nextBuffer();
-    }
-    
-    @Override
-    public void add(int docID, StorableField value) throws IOException {
-      final BytesRef bytes = value.binaryValue();
-      assert bytes != null;
-      assert lastDocID < docID;
-
-      if (size == -1) {
-        if (bytes.length > BYTE_BLOCK_SIZE) {
-          throw new IllegalArgumentException("bytes arrays > " + BYTE_BLOCK_SIZE + " are not supported");
-        }
-        size = bytes.length;
-      } else if (bytes.length != size) {
-        throw new IllegalArgumentException("byte[] length changed for BYTES_FIXED_STRAIGHT type (before=" + size + " now=" + bytes.length);
-      }
-      if (lastDocID+1 < docID) {
-        advancePool(docID);
-      }
-      pool.copy(bytes);
-      lastDocID = docID;
-    }
-    
-    private final void advancePool(int docID) {
-      long numBytes = (docID - (lastDocID+1))*size;
-      while(numBytes > 0) {
-        if (numBytes + pool.byteUpto < byteBlockSize) {
-          pool.byteUpto += numBytes;
-          numBytes = 0;
-        } else {
-          numBytes -= byteBlockSize - pool.byteUpto;
-          pool.nextBuffer();
-        }
-      }
-      assert numBytes == 0;
-    }
-    
-    protected void set(BytesRef ref, int docId) {
-      assert BYTE_BLOCK_SIZE % size == 0 : "BYTE_BLOCK_SIZE ("+ BYTE_BLOCK_SIZE + ") must be a multiple of the size: " + size;
-      ref.offset = docId*size;
-      ref.length = size;
-      pool.deref(ref);
-    }
-    
-    protected void resetPool() {
-      pool.reset(false, false);
-    }
-    
-    protected void writeData(IndexOutput out) throws IOException {
-      pool.writePool(out);
-    }
-    
-    protected void writeZeros(int num, IndexOutput out) throws IOException {
-      final byte[] zeros = new byte[size];
-      for (int i = 0; i < num; i++) {
-        out.writeBytes(zeros, zeros.length);
-      }
-    }
-    
-    @Override
-    public int getValueSize() {
-      return size;
-    }
-  }
-
-  static class Writer extends FixedBytesWriterBase {
-    private boolean hasMerged;
-    private IndexOutput datOut;
-    
-    public Writer(Directory dir, String id, Counter bytesUsed, IOContext context) {
-      super(dir, id, CODEC_NAME, VERSION_CURRENT, bytesUsed, context);
-    }
-
-    public Writer(Directory dir, String id, String codecNameDat, int version, Counter bytesUsed, IOContext context) {
-      super(dir, id, codecNameDat, version, bytesUsed, context);
-    }
-
-
-    @Override
-    protected void merge(DocValues readerIn, int docBase, int docCount, Bits liveDocs) throws IOException {
-      datOut = getOrCreateDataOut();
-      boolean success = false;
-      try {
-        if (!hasMerged && size != -1) {
-          datOut.writeInt(size);
-        }
-
-        if (liveDocs == null && tryBulkMerge(readerIn)) {
-          FixedStraightReader reader = (FixedStraightReader) readerIn;
-          final int maxDocs = reader.maxDoc;
-          if (maxDocs == 0) {
-            return;
-          }
-          if (size == -1) {
-            size = reader.size;
-            datOut.writeInt(size);
-          } else if (size != reader.size) {
-            throw new IllegalArgumentException("expected bytes size=" + size
-                + " but got " + reader.size);
-           }
-          if (lastDocID+1 < docBase) {
-            fill(datOut, docBase);
-            lastDocID = docBase-1;
-          }
-          // TODO should we add a transfer to API to each reader?
-          final IndexInput cloneData = reader.cloneData();
-          try {
-            datOut.copyBytes(cloneData, size * maxDocs);
-          } finally {
-            IOUtils.close(cloneData);  
-          }
-        
-          lastDocID += maxDocs;
-        } else {
-          super.merge(readerIn, docBase, docCount, liveDocs);
-        }
-        success = true;
-      } finally {
-        if (!success) {
-          IOUtils.closeWhileHandlingException(datOut);
-        }
-        hasMerged = true;
-      }
-    }
-    
-    protected boolean tryBulkMerge(DocValues docValues) {
-      return docValues instanceof FixedStraightReader;
-    }
-    
-    @Override
-    protected void mergeDoc(StoredField scratchField, Source source, int docID, int sourceDoc) throws IOException {
-      assert lastDocID < docID;
-      setMergeBytes(source, sourceDoc);
-      if (size == -1) {
-        size = bytesRef.length;
-        datOut.writeInt(size);
-      }
-      assert size == bytesRef.length : "size: " + size + " ref: " + bytesRef.length;
-      if (lastDocID+1 < docID) {
-        fill(datOut, docID);
-      }
-      datOut.writeBytes(bytesRef.bytes, bytesRef.offset, bytesRef.length);
-      lastDocID = docID;
-    }
-    
-    protected void setMergeBytes(Source source, int sourceDoc) {
-      source.getBytes(sourceDoc, bytesRef);
-    }
-
-    // Fills up to but not including this docID
-    private void fill(IndexOutput datOut, int docID) throws IOException {
-      assert size >= 0;
-      writeZeros((docID - (lastDocID+1)), datOut);
-    }
-
-    @Override
-    public void finish(int docCount) throws IOException {
-      boolean success = false;
-      try {
-        if (!hasMerged) {
-          // indexing path - no disk IO until here
-          assert datOut == null;
-          datOut = getOrCreateDataOut();
-          if (size == -1) {
-            datOut.writeInt(0);
-          } else {
-            datOut.writeInt(size);
-            writeData(datOut);
-          }
-          if (lastDocID + 1 < docCount) {
-            fill(datOut, docCount);
-          }
-        } else {
-          // merge path - datOut should be initialized
-          assert datOut != null;
-          if (size == -1) {// no data added
-            datOut.writeInt(0);
-          } else {
-            fill(datOut, docCount);
-          }
-        }
-        success = true;
-      } finally {
-        resetPool();
-        if (success) {
-          IOUtils.close(datOut);
-        } else {
-          IOUtils.closeWhileHandlingException(datOut);
-        }
-      }
-    }
-  
-  }
-  
-  public static class FixedStraightReader extends BytesReaderBase {
-    protected final int size;
-    protected final int maxDoc;
-    
-    FixedStraightReader(Directory dir, String id, int maxDoc, IOContext context) throws IOException {
-      this(dir, id, CODEC_NAME, VERSION_CURRENT, maxDoc, context, Type.BYTES_FIXED_STRAIGHT);
-    }
-
-    protected FixedStraightReader(Directory dir, String id, String codecNameDat, int version, int maxDoc, IOContext context, Type type) throws IOException {
-      super(dir, id, null, codecNameDat, version, false, context, type);
-      size = datIn.readInt();
-      this.maxDoc = maxDoc;
-    }
-
-    @Override
-    protected Source loadSource() throws IOException {
-      return size == 1 ? new SingleByteSource(cloneData(), maxDoc) : 
-        new FixedStraightSource(cloneData(), size, maxDoc, type);
-    }
-
-    @Override
-    public void close() throws IOException {
-      datIn.close();
-    }
-   
-    @Override
-    protected Source loadDirectSource() throws IOException {
-      return new DirectFixedStraightSource(cloneData(), size, getType());
-    }
-    
-    @Override
-    public int getValueSize() {
-      return size;
-    }
-  }
-  
-  // specialized version for single bytes
-  private static final class SingleByteSource extends Source {
-    private final byte[] data;
-
-    public SingleByteSource(IndexInput datIn, int maxDoc) throws IOException {
-      super(Type.BYTES_FIXED_STRAIGHT);
-      try {
-        data = new byte[maxDoc];
-        datIn.readBytes(data, 0, data.length, false);
-      } finally {
-        IOUtils.close(datIn);
-      }
-    }
-    
-    @Override
-    public boolean hasArray() {
-      return true;
-    }
-
-    @Override
-    public Object getArray() {
-      return data;
-    }
-
-    @Override
-    public BytesRef getBytes(int docID, BytesRef bytesRef) {
-      bytesRef.length = 1;
-      bytesRef.bytes = data;
-      bytesRef.offset = docID;
-      return bytesRef;
-    }
-  }
-
-  
-  private final static class FixedStraightSource extends BytesSourceBase {
-    private final int size;
-
-    public FixedStraightSource(IndexInput datIn, int size, int maxDoc, Type type)
-        throws IOException {
-      super(datIn, null, new PagedBytes(PAGED_BYTES_BITS), size * maxDoc,
-          type);
-      this.size = size;
-    }
-
-    @Override
-    public BytesRef getBytes(int docID, BytesRef bytesRef) {
-      return data.fillSlice(bytesRef, size * ((long) docID), size);
-    }
-  }
-  
-  public final static class DirectFixedStraightSource extends DirectSource {
-    private final int size;
-
-    DirectFixedStraightSource(IndexInput input, int size, Type type) {
-      super(input, type);
-      this.size = size;
-    }
-
-    @Override
-    protected int position(int docID) throws IOException {
-      data.seek(baseOffset + size * ((long) docID));
-      return size;
-    }
-
-  }
-}
Index: lucene/core/src/java/org/apache/lucene/codecs/lucene40/values/FixedDerefBytesImpl.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/codecs/lucene40/values/FixedDerefBytesImpl.java	(revision 1442822)
+++ lucene/core/src/java/org/apache/lucene/codecs/lucene40/values/FixedDerefBytesImpl.java	(working copy)
@@ -1,135 +0,0 @@
-package org.apache.lucene.codecs.lucene40.values;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.codecs.lucene40.values.Bytes.BytesReaderBase;
-import org.apache.lucene.codecs.lucene40.values.Bytes.BytesSourceBase;
-import org.apache.lucene.codecs.lucene40.values.Bytes.DerefBytesWriterBase;
-import org.apache.lucene.index.DocValues.Type;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.Counter;
-import org.apache.lucene.util.PagedBytes;
-import org.apache.lucene.util.packed.PackedInts;
-
-// Stores fixed-length byte[] by deref, ie when two docs
-// have the same value, they store only 1 byte[]
-/**
- * @lucene.experimental
- */
-class FixedDerefBytesImpl {
-
-  static final String CODEC_NAME_IDX = "FixedDerefBytesIdx";
-  static final String CODEC_NAME_DAT = "FixedDerefBytesDat";
-
-  static final int VERSION_START = 0;
-  static final int VERSION_CURRENT = VERSION_START;
-
-  public static class Writer extends DerefBytesWriterBase {
-    public Writer(Directory dir, String id, Counter bytesUsed, IOContext context) {
-      super(dir, id, CODEC_NAME_IDX, CODEC_NAME_DAT, VERSION_CURRENT, bytesUsed, context, Type.BYTES_FIXED_DEREF);
-    }
-
-    @Override
-    protected void finishInternal(int docCount) throws IOException {
-      final int numValues = hash.size();
-      final IndexOutput datOut = getOrCreateDataOut();
-      datOut.writeInt(size);
-      if (size != -1) {
-        final BytesRef bytesRef = new BytesRef(size);
-        for (int i = 0; i < numValues; i++) {
-          hash.get(i, bytesRef);
-          datOut.writeBytes(bytesRef.bytes, bytesRef.offset, bytesRef.length);
-        }
-      }
-      final IndexOutput idxOut = getOrCreateIndexOut();
-      idxOut.writeInt(numValues);
-      writeIndex(idxOut, docCount, numValues, docToEntry);
-    }
-
-  }
-
-  public static class FixedDerefReader extends BytesReaderBase {
-    private final int size;
-    private final int numValuesStored;
-    FixedDerefReader(Directory dir, String id, int maxDoc, IOContext context) throws IOException {
-      super(dir, id, CODEC_NAME_IDX, CODEC_NAME_DAT, VERSION_START, true, context, Type.BYTES_FIXED_DEREF);
-      size = datIn.readInt();
-      numValuesStored = idxIn.readInt();
-    }
-
-    @Override
-    protected Source loadSource() throws IOException {
-      return new FixedDerefSource(cloneData(), cloneIndex(), size, numValuesStored);
-    }
-
-    @Override
-    protected Source loadDirectSource()
-        throws IOException {
-      return new DirectFixedDerefSource(cloneData(), cloneIndex(), size, getType());
-    }
-
-    @Override
-    public int getValueSize() {
-      return size;
-    }
-    
-  }
-  
-  static final class FixedDerefSource extends BytesSourceBase {
-    private final int size;
-    private final PackedInts.Reader addresses;
-
-    protected FixedDerefSource(IndexInput datIn, IndexInput idxIn, int size, long numValues) throws IOException {
-      super(datIn, idxIn, new PagedBytes(PAGED_BYTES_BITS), size * numValues,
-          Type.BYTES_FIXED_DEREF);
-      this.size = size;
-      addresses = PackedInts.getReader(idxIn);
-    }
-
-    @Override
-    public BytesRef getBytes(int docID, BytesRef bytesRef) {
-      return data.fillSlice(bytesRef, addresses.get(docID) * size, size);
-    }
-
-  }
-  
-  final static class DirectFixedDerefSource extends DirectSource {
-    private final PackedInts.Reader index;
-    private final int size;
-
-    DirectFixedDerefSource(IndexInput data, IndexInput index, int size, Type type)
-        throws IOException {
-      super(data, type);
-      this.size = size;
-      this.index = PackedInts.getDirectReader(index);
-    }
-
-    @Override
-    protected int position(int docID) throws IOException {
-      data.seek(baseOffset + index.get(docID) * size);
-      return size;
-    }
-  }
-
-}
Index: lucene/core/src/java/org/apache/lucene/codecs/lucene40/values/VarSortedBytesImpl.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/codecs/lucene40/values/VarSortedBytesImpl.java	(revision 1442822)
+++ lucene/core/src/java/org/apache/lucene/codecs/lucene40/values/VarSortedBytesImpl.java	(working copy)
@@ -1,258 +0,0 @@
-package org.apache.lucene.codecs.lucene40.values;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Comparator;
-import java.util.List;
-
-import org.apache.lucene.codecs.lucene40.values.Bytes.BytesReaderBase;
-import org.apache.lucene.codecs.lucene40.values.Bytes.BytesSortedSourceBase;
-import org.apache.lucene.codecs.lucene40.values.Bytes.DerefBytesWriterBase;
-import org.apache.lucene.index.DocValues;
-import org.apache.lucene.index.SortedBytesMergeUtils;
-import org.apache.lucene.index.DocValues.SortedSource;
-import org.apache.lucene.index.DocValues.Type;
-import org.apache.lucene.index.SortedBytesMergeUtils.IndexOutputBytesRefConsumer;
-import org.apache.lucene.index.SortedBytesMergeUtils.MergeContext;
-import org.apache.lucene.index.SortedBytesMergeUtils.SortedSourceSlice;
-import org.apache.lucene.index.MergeState;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.Counter;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.packed.PackedInts;
-
-// Stores variable-length byte[] by deref, ie when two docs
-// have the same value, they store only 1 byte[] and both
-// docs reference that single source
-
-/**
- * @lucene.experimental
- */
-final class VarSortedBytesImpl {
-
-  static final String CODEC_NAME_IDX = "VarDerefBytesIdx";
-  static final String CODEC_NAME_DAT = "VarDerefBytesDat";
-
-  static final int VERSION_START = 0;
-  static final int VERSION_CURRENT = VERSION_START;
-
-  final static class Writer extends DerefBytesWriterBase {
-    private final Comparator<BytesRef> comp;
-
-    public Writer(Directory dir, String id, Comparator<BytesRef> comp,
-        Counter bytesUsed, IOContext context, float acceptableOverheadRatio) {
-      super(dir, id, CODEC_NAME_IDX, CODEC_NAME_DAT, VERSION_CURRENT, bytesUsed, context, acceptableOverheadRatio, Type.BYTES_VAR_SORTED);
-      this.comp = comp;
-      size = 0;
-    }
-
-    @Override
-    public void merge(MergeState mergeState, DocValues[] docValues)
-        throws IOException {
-      boolean success = false;
-      try {
-        MergeContext ctx = SortedBytesMergeUtils.init(Type.BYTES_VAR_SORTED, docValues, comp, mergeState.segmentInfo.getDocCount());
-        final List<SortedSourceSlice> slices = SortedBytesMergeUtils.buildSlices(mergeState.docBase, mergeState.docMaps, docValues, ctx);
-        IndexOutput datOut = getOrCreateDataOut();
-        
-        ctx.offsets = new long[1];
-        final int maxOrd = SortedBytesMergeUtils.mergeRecords(ctx, new IndexOutputBytesRefConsumer(datOut), slices);
-        final long[] offsets = ctx.offsets;
-        maxBytes = offsets[maxOrd-1];
-        final IndexOutput idxOut = getOrCreateIndexOut();
-        
-        idxOut.writeLong(maxBytes);
-        final PackedInts.Writer offsetWriter = PackedInts.getWriter(idxOut, maxOrd+1,
-            PackedInts.bitsRequired(maxBytes), PackedInts.DEFAULT);
-        offsetWriter.add(0);
-        for (int i = 0; i < maxOrd; i++) {
-          offsetWriter.add(offsets[i]);
-        }
-        offsetWriter.finish();
-        
-        final PackedInts.Writer ordsWriter = PackedInts.getWriter(idxOut, ctx.docToEntry.length,
-            PackedInts.bitsRequired(maxOrd-1), PackedInts.DEFAULT);
-        for (SortedSourceSlice slice : slices) {
-          slice.writeOrds(ordsWriter);
-        }
-        ordsWriter.finish();
-        success = true;
-      } finally {
-        releaseResources();
-        if (success) {
-          IOUtils.close(getIndexOut(), getDataOut());
-        } else {
-          IOUtils.closeWhileHandlingException(getIndexOut(), getDataOut());
-        }
-
-      }
-    }
-
-    @Override
-    protected void checkSize(BytesRef bytes) {
-      // allow var bytes sizes
-    }
-
-    // Important that we get docCount, in case there were
-    // some last docs that we didn't see
-    @Override
-    public void finishInternal(int docCount) throws IOException {
-      fillDefault(docCount);
-      final int count = hash.size();
-      final IndexOutput datOut = getOrCreateDataOut();
-      final IndexOutput idxOut = getOrCreateIndexOut();
-      long offset = 0;
-      final int[] index = new int[count];
-      final int[] sortedEntries = hash.sort(comp);
-      // total bytes of data
-      idxOut.writeLong(maxBytes);
-      PackedInts.Writer offsetWriter = PackedInts.getWriter(idxOut, count+1,
-          PackedInts.bitsRequired(maxBytes), PackedInts.DEFAULT);
-      // first dump bytes data, recording index & write offset as
-      // we go
-      final BytesRef spare = new BytesRef();
-      for (int i = 0; i < count; i++) {
-        final int e = sortedEntries[i];
-        offsetWriter.add(offset);
-        index[e] = i;
-        final BytesRef bytes = hash.get(e, spare);
-        // TODO: we could prefix code...
-        datOut.writeBytes(bytes.bytes, bytes.offset, bytes.length);
-        offset += bytes.length;
-      }
-      // write sentinel
-      offsetWriter.add(offset);
-      offsetWriter.finish();
-      // write index
-      writeIndex(idxOut, docCount, count, index, docToEntry);
-
-    }
-  }
-
-  public static class Reader extends BytesReaderBase {
-
-    private final Comparator<BytesRef> comparator;
-
-    Reader(Directory dir, String id, int maxDoc,
-        IOContext context, Type type, Comparator<BytesRef> comparator)
-        throws IOException {
-      super(dir, id, CODEC_NAME_IDX, CODEC_NAME_DAT, VERSION_START, true, context, type);
-      this.comparator = comparator;
-    }
-
-    @Override
-    public org.apache.lucene.index.DocValues.Source loadSource()
-        throws IOException {
-      return new VarSortedSource(cloneData(), cloneIndex(), comparator);
-    }
-
-    @Override
-    protected Source loadDirectSource() throws IOException {
-      return new DirectSortedSource(cloneData(), cloneIndex(), comparator, getType());
-    }
-    
-  }
-  private static final class VarSortedSource extends BytesSortedSourceBase {
-    private final int valueCount;
-
-    VarSortedSource(IndexInput datIn, IndexInput idxIn,
-        Comparator<BytesRef> comp) throws IOException {
-      super(datIn, idxIn, comp, idxIn.readLong(), Type.BYTES_VAR_SORTED, true);
-      valueCount = ordToOffsetIndex.size()-1; // the last value here is just a dummy value to get the length of the last value
-      closeIndexInput();
-    }
-
-    @Override
-    public BytesRef getByOrd(int ord, BytesRef bytesRef) {
-      final long offset = ordToOffsetIndex.get(ord);
-      final long nextOffset = ordToOffsetIndex.get(1 + ord);
-      data.fillSlice(bytesRef, offset, (int) (nextOffset - offset));
-      return bytesRef;
-    }
-
-    @Override
-    public int getValueCount() {
-      return valueCount;
-    }
-  }
-
-  private static final class DirectSortedSource extends SortedSource {
-    private final PackedInts.Reader docToOrdIndex;
-    private final PackedInts.Reader ordToOffsetIndex;
-    private final IndexInput datIn;
-    private final long basePointer;
-    private final int valueCount;
-    
-    DirectSortedSource(IndexInput datIn, IndexInput idxIn,
-        Comparator<BytesRef> comparator, Type type) throws IOException {
-      super(type, comparator);
-      idxIn.readLong();
-      ordToOffsetIndex = PackedInts.getDirectReader(idxIn);
-      valueCount = ordToOffsetIndex.size()-1; // the last value here is just a dummy value to get the length of the last value
-      // advance this iterator to the end and clone the stream once it points to the docToOrdIndex header
-      ordToOffsetIndex.get(valueCount);
-      docToOrdIndex = PackedInts.getDirectReader(idxIn.clone()); // read the ords in to prevent too many random disk seeks
-      basePointer = datIn.getFilePointer();
-      this.datIn = datIn;
-    }
-
-    @Override
-    public int ord(int docID) {
-      return (int) docToOrdIndex.get(docID);
-    }
-
-    @Override
-    public boolean hasPackedDocToOrd() {
-      return true;
-    }
-
-    @Override
-    public PackedInts.Reader getDocToOrd() {
-      return docToOrdIndex;
-    }
-
-    @Override
-    public BytesRef getByOrd(int ord, BytesRef bytesRef) {
-      try {
-        final long offset = ordToOffsetIndex.get(ord);
-        // 1+ord is safe because we write a sentinel at the end
-        final long nextOffset = ordToOffsetIndex.get(1+ord);
-        datIn.seek(basePointer + offset);
-        final int length = (int) (nextOffset - offset);
-        bytesRef.offset = 0;
-        bytesRef.grow(length);
-        datIn.readBytes(bytesRef.bytes, 0, length);
-        bytesRef.length = length;
-        return bytesRef;
-      } catch (IOException ex) {
-        throw new IllegalStateException("failed", ex);
-      }
-    }
-    
-    @Override
-    public int getValueCount() {
-      return valueCount;
-    }
-
-  }
-}
Index: lucene/core/src/java/org/apache/lucene/codecs/lucene40/values/PackedIntValues.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/codecs/lucene40/values/PackedIntValues.java	(revision 1442822)
+++ lucene/core/src/java/org/apache/lucene/codecs/lucene40/values/PackedIntValues.java	(working copy)
@@ -1,256 +0,0 @@
-package org.apache.lucene.codecs.lucene40.values;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-import java.io.IOException;
-
-import org.apache.lucene.codecs.CodecUtil;
-import org.apache.lucene.codecs.DocValuesArraySource;
-import org.apache.lucene.codecs.lucene40.values.FixedStraightBytesImpl.FixedBytesWriterBase;
-import org.apache.lucene.index.DocValues.Source;
-import org.apache.lucene.index.DocValues.Type;
-import org.apache.lucene.index.DocValues;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.IndexableField;
-import org.apache.lucene.index.StorableField;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.Counter;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.packed.PackedInts;
-
-/**
- * Stores integers using {@link PackedInts}
- * 
- * @lucene.experimental
- * */
-class PackedIntValues {
-
-  private static final String CODEC_NAME = "PackedInts";
-  private static final byte PACKED = 0x00;
-  private static final byte FIXED_64 = 0x01;
-
-  static final int VERSION_START = 0;
-  static final int VERSION_CURRENT = VERSION_START;
-
-  static class PackedIntsWriter extends FixedBytesWriterBase {
-
-    private long minValue;
-    private long maxValue;
-    private boolean started;
-    private int lastDocId = -1;
-
-    protected PackedIntsWriter(Directory dir, String id, Counter bytesUsed,
-        IOContext context) {
-      super(dir, id, CODEC_NAME, VERSION_CURRENT, bytesUsed, context, Type.VAR_INTS);
-      bytesRef = new BytesRef(8);
-    }
-    
-    @Override
-    public void finish(int docCount) throws IOException {
-      boolean success = false;
-      final IndexOutput dataOut = getOrCreateDataOut();
-      try {
-        if (!started) {
-          minValue = maxValue = 0;
-        }
-        final long delta = maxValue - minValue;
-        // if we exceed the range of positive longs we must switch to fixed
-        // ints
-        if (delta <= (maxValue >= 0 && minValue <= 0 ? Long.MAX_VALUE
-            : Long.MAX_VALUE - 1) && delta >= 0) {
-          dataOut.writeByte(PACKED);
-          writePackedInts(dataOut, docCount);
-          return; // done
-        } else {
-          dataOut.writeByte(FIXED_64);
-        }
-        writeData(dataOut);
-        writeZeros(docCount - (lastDocID + 1), dataOut);
-        success = true;
-      } finally {
-        resetPool();
-        if (success) {
-          IOUtils.close(dataOut);
-        } else {
-          IOUtils.closeWhileHandlingException(dataOut);
-        }
-      }
-    }
-
-    private void writePackedInts(IndexOutput datOut, int docCount) throws IOException {
-      datOut.writeLong(minValue);
-      
-      // write a default value to recognize docs without a value for that
-      // field
-      final long defaultValue = maxValue >= 0 && minValue <= 0 ? 0 - minValue
-          : ++maxValue - minValue;
-      datOut.writeLong(defaultValue);
-      PackedInts.Writer w = PackedInts.getWriter(datOut, docCount,
-          PackedInts.bitsRequired(maxValue - minValue), PackedInts.DEFAULT);
-      for (int i = 0; i < lastDocID + 1; i++) {
-        set(bytesRef, i);
-        byte[] bytes = bytesRef.bytes;
-        int offset = bytesRef.offset;
-        long asLong =  
-           (((long)(bytes[offset+0] & 0xff) << 56) |
-            ((long)(bytes[offset+1] & 0xff) << 48) |
-            ((long)(bytes[offset+2] & 0xff) << 40) |
-            ((long)(bytes[offset+3] & 0xff) << 32) |
-            ((long)(bytes[offset+4] & 0xff) << 24) |
-            ((long)(bytes[offset+5] & 0xff) << 16) |
-            ((long)(bytes[offset+6] & 0xff) <<  8) |
-            ((long)(bytes[offset+7] & 0xff)));
-        w.add(asLong == 0 ? defaultValue : asLong - minValue);
-      }
-      for (int i = lastDocID + 1; i < docCount; i++) {
-        w.add(defaultValue);
-      }
-      w.finish();
-    }
-    
-    @Override
-    public void add(int docID, StorableField docValue) throws IOException {
-      final long v = docValue.numericValue().longValue();
-      assert lastDocId < docID;
-      if (!started) {
-        started = true;
-        minValue = maxValue = v;
-      } else {
-        if (v < minValue) {
-          minValue = v;
-        } else if (v > maxValue) {
-          maxValue = v;
-        }
-      }
-      lastDocId = docID;
-      DocValuesArraySource.copyLong(bytesRef, v);
-      bytesSpareField.setBytesValue(bytesRef);
-      super.add(docID, bytesSpareField);
-    }
-  }
-
-  /**
-   * Opens all necessary files, but does not read any data in until you call
-   * {@link #loadSource}.
-   */
-  static class PackedIntsReader extends DocValues {
-    private final IndexInput datIn;
-    private final byte type;
-    private final int numDocs;
-    private final DocValuesArraySource values;
-
-    protected PackedIntsReader(Directory dir, String id, int numDocs,
-        IOContext context) throws IOException {
-      datIn = dir.openInput(
-                IndexFileNames.segmentFileName(id, Bytes.DV_SEGMENT_SUFFIX, DocValuesWriterBase.DATA_EXTENSION),
-          context);
-      this.numDocs = numDocs;
-      boolean success = false;
-      try {
-        CodecUtil.checkHeader(datIn, CODEC_NAME, VERSION_START, VERSION_START);
-        type = datIn.readByte();
-        values = type == FIXED_64 ?  DocValuesArraySource.forType(Type.FIXED_INTS_64) : null;
-        success = true;
-      } finally {
-        if (!success) {
-          IOUtils.closeWhileHandlingException(datIn);
-        }
-      }
-    }
-
-
-    /**
-     * Loads the actual values. You may call this more than once, eg if you
-     * already previously loaded but then discarded the Source.
-     */
-    @Override
-    protected Source loadSource() throws IOException {
-      boolean success = false;
-      final Source source;
-      IndexInput input = null;
-      try {
-        input = datIn.clone();
-        
-        if (values == null) {
-          source = new PackedIntsSource(input, false);
-        } else {
-          source = values.newFromInput(input, numDocs);
-        }
-        success = true;
-        return source;
-      } finally {
-        if (!success) {
-          IOUtils.closeWhileHandlingException(input, datIn);
-        }
-      }
-    }
-
-    @Override
-    public void close() throws IOException {
-      super.close();
-      datIn.close();
-    }
-
-
-    @Override
-    public Type getType() {
-      return Type.VAR_INTS;
-    }
-
-
-    @Override
-    protected Source loadDirectSource() throws IOException {
-      return values != null ? new FixedStraightBytesImpl.DirectFixedStraightSource(datIn.clone(), 8, Type.FIXED_INTS_64) : new PackedIntsSource(datIn.clone(), true);
-    }
-  }
-
-  
-  static class PackedIntsSource extends Source {
-    private final long minValue;
-    private final long defaultValue;
-    private final PackedInts.Reader values;
-
-    public PackedIntsSource(IndexInput dataIn, boolean direct) throws IOException {
-      super(Type.VAR_INTS);
-      minValue = dataIn.readLong();
-      defaultValue = dataIn.readLong();
-      values = direct ? PackedInts.getDirectReader(dataIn) : PackedInts.getReader(dataIn);
-    }
-    
-    @Override
-    public BytesRef getBytes(int docID, BytesRef ref) {
-      ref.grow(8);
-      DocValuesArraySource.copyLong(ref, getInt(docID));
-      return ref;
-    }
-
-    @Override
-    public long getInt(int docID) {
-      // TODO -- can we somehow avoid 2X method calls
-      // on each get? must push minValue down, and make
-      // PackedInts implement Ints.Source
-      assert docID >= 0;
-      final long value = values.get(docID);
-      return value == defaultValue ? 0 : minValue + value;
-    }
-  }
-
-}
\ No newline at end of file
Index: lucene/core/src/java/org/apache/lucene/codecs/lucene40/values/Bytes.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/codecs/lucene40/values/Bytes.java	(revision 1442822)
+++ lucene/core/src/java/org/apache/lucene/codecs/lucene40/values/Bytes.java	(working copy)
@@ -1,611 +0,0 @@
-package org.apache.lucene.codecs.lucene40.values;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/** Base class for specific Bytes Reader/Writer implementations */
-import java.io.IOException;
-import java.util.Comparator;
-import java.util.concurrent.atomic.AtomicLong;
-
-import org.apache.lucene.codecs.CodecUtil;
-import org.apache.lucene.codecs.DocValuesConsumer;
-import org.apache.lucene.index.DocValues.SortedSource;
-import org.apache.lucene.index.DocValues.Source;
-import org.apache.lucene.index.DocValues.Type;
-import org.apache.lucene.index.DocValues;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.IndexableField;
-import org.apache.lucene.index.StorableField;
-import org.apache.lucene.store.DataOutput;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.util.ArrayUtil;
-import org.apache.lucene.util.ByteBlockPool.Allocator;
-import org.apache.lucene.util.ByteBlockPool.DirectTrackingAllocator;
-import org.apache.lucene.util.ByteBlockPool;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.BytesRefHash.TrackingDirectBytesStartArray;
-import org.apache.lucene.util.BytesRefHash;
-import org.apache.lucene.util.Counter;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.PagedBytes;
-import org.apache.lucene.util.RamUsageEstimator;
-import org.apache.lucene.util.packed.PackedInts;
-
-/**
- * Provides concrete Writer/Reader implementations for <tt>byte[]</tt> value per
- * document. There are 6 package-private default implementations of this, for
- * all combinations of {@link Mode#DEREF}/{@link Mode#STRAIGHT} x fixed-length/variable-length.
- * 
- * <p>
- * NOTE: Currently the total amount of byte[] data stored (across a single
- * segment) cannot exceed 2GB.
- * </p>
- * <p>
- * NOTE: Each byte[] must be <= 32768 bytes in length
- * </p>
- * 
- * @lucene.experimental
- */
-public final class Bytes {
-
-  static final String DV_SEGMENT_SUFFIX = "dv";
-
-  // TODO - add bulk copy where possible
-  private Bytes() { /* don't instantiate! */
-  }
-
-  /**
-   * Defines the {@link Writer}s store mode. The writer will either store the
-   * bytes sequentially ({@link #STRAIGHT}, dereferenced ({@link #DEREF}) or
-   * sorted ({@link #SORTED})
-   * 
-   * @lucene.experimental
-   */
-  public static enum Mode {
-    /**
-     * Mode for sequentially stored bytes
-     */
-    STRAIGHT,
-    /**
-     * Mode for dereferenced stored bytes
-     */
-    DEREF,
-    /**
-     * Mode for sorted stored bytes
-     */
-    SORTED
-  };
-
-  /**
-   * Creates a new <tt>byte[]</tt> {@link Writer} instances for the given
-   * directory.
-   * 
-   * @param dir
-   *          the directory to write the values to
-   * @param id
-   *          the id used to create a unique file name. Usually composed out of
-   *          the segment name and a unique id per segment.
-   * @param mode
-   *          the writers store mode
-   * @param fixedSize
-   *          <code>true</code> if all bytes subsequently passed to the
-   *          {@link Writer} will have the same length
-   * @param sortComparator {@link BytesRef} comparator used by sorted variants. 
-   *        If <code>null</code> {@link BytesRef#getUTF8SortedAsUnicodeComparator()}
-   *        is used instead
-   * @param bytesUsed
-   *          an {@link AtomicLong} instance to track the used bytes within the
-   *          {@link Writer}. A call to {@link Writer#finish(int)} will release
-   *          all internally used resources and frees the memory tracking
-   *          reference.
-   * @param acceptableOverheadRatio
-   *          how to trade space for speed. This option is only applicable for
-   *          docvalues of type {@link Type#BYTES_FIXED_SORTED} and
-   *          {@link Type#BYTES_VAR_SORTED}.
-   * @param context I/O Context
-   * @return a new {@link Writer} instance
-   * @see PackedInts#getReader(org.apache.lucene.store.DataInput)
-   */
-  public static DocValuesConsumer getWriter(Directory dir, String id, Mode mode,
-      boolean fixedSize, Comparator<BytesRef> sortComparator,
-      Counter bytesUsed, IOContext context, float acceptableOverheadRatio) {
-    // TODO -- i shouldn't have to specify fixed? can
-    // track itself & do the write thing at write time?
-    if (sortComparator == null) {
-      sortComparator = BytesRef.getUTF8SortedAsUnicodeComparator();
-    }
-
-    if (fixedSize) {
-      if (mode == Mode.STRAIGHT) {
-        return new FixedStraightBytesImpl.Writer(dir, id, bytesUsed, context);
-      } else if (mode == Mode.DEREF) {
-        return new FixedDerefBytesImpl.Writer(dir, id, bytesUsed, context);
-      } else if (mode == Mode.SORTED) {
-        return new FixedSortedBytesImpl.Writer(dir, id, sortComparator, bytesUsed, context, acceptableOverheadRatio);
-      }
-    } else {
-      if (mode == Mode.STRAIGHT) {
-        return new VarStraightBytesImpl.Writer(dir, id, bytesUsed, context);
-      } else if (mode == Mode.DEREF) {
-        return new VarDerefBytesImpl.Writer(dir, id, bytesUsed, context);
-      } else if (mode == Mode.SORTED) {
-        return new VarSortedBytesImpl.Writer(dir, id, sortComparator, bytesUsed, context, acceptableOverheadRatio);
-      }
-    }
-
-    throw new IllegalArgumentException("");
-  }
-
-  /**
-   * Creates a new {@link DocValues} instance that provides either memory
-   * resident or iterative access to a per-document stored <tt>byte[]</tt>
-   * value. The returned {@link DocValues} instance will be initialized without
-   * consuming a significant amount of memory.
-   * 
-   * @param dir
-   *          the directory to load the {@link DocValues} from.
-   * @param id
-   *          the file ID in the {@link Directory} to load the values from.
-   * @param mode
-   *          the mode used to store the values
-   * @param fixedSize
-   *          <code>true</code> iff the values are stored with fixed-size,
-   *          otherwise <code>false</code>
-   * @param maxDoc
-   *          the number of document values stored for the given ID
-   * @param sortComparator {@link BytesRef} comparator used by sorted variants. 
-   *        If <code>null</code> {@link BytesRef#getUTF8SortedAsUnicodeComparator()}
-   *        is used instead
-   * @return an initialized {@link DocValues} instance.
-   * @throws IOException
-   *           if an {@link IOException} occurs
-   */
-  public static DocValues getValues(Directory dir, String id, Mode mode,
-      boolean fixedSize, int maxDoc, Comparator<BytesRef> sortComparator, IOContext context) throws IOException {
-    if (sortComparator == null) {
-      sortComparator = BytesRef.getUTF8SortedAsUnicodeComparator();
-    }
-    // TODO -- I can peek @ header to determing fixed/mode?
-    if (fixedSize) {
-      if (mode == Mode.STRAIGHT) {
-        return new FixedStraightBytesImpl.FixedStraightReader(dir, id, maxDoc, context);
-      } else if (mode == Mode.DEREF) {
-        return new FixedDerefBytesImpl.FixedDerefReader(dir, id, maxDoc, context);
-      } else if (mode == Mode.SORTED) {
-        return new FixedSortedBytesImpl.Reader(dir, id, maxDoc, context, Type.BYTES_FIXED_SORTED, sortComparator);
-      }
-    } else {
-      if (mode == Mode.STRAIGHT) {
-        return new VarStraightBytesImpl.VarStraightReader(dir, id, maxDoc, context);
-      } else if (mode == Mode.DEREF) {
-        return new VarDerefBytesImpl.VarDerefReader(dir, id, maxDoc, context);
-      } else if (mode == Mode.SORTED) {
-        return new VarSortedBytesImpl.Reader(dir, id, maxDoc,context, Type.BYTES_VAR_SORTED, sortComparator);
-      }
-    }
-
-    throw new IllegalArgumentException("Illegal Mode: " + mode);
-  }
-
-  // TODO open up this API?
-  static abstract class BytesSourceBase extends Source {
-    private final PagedBytes pagedBytes;
-    protected final IndexInput datIn;
-    protected final IndexInput idxIn;
-    protected final static int PAGED_BYTES_BITS = 15;
-    protected final PagedBytes.Reader data;
-    protected final long totalLengthInBytes;
-    
-
-    protected BytesSourceBase(IndexInput datIn, IndexInput idxIn,
-        PagedBytes pagedBytes, long bytesToRead, Type type) throws IOException {
-      super(type);
-      assert bytesToRead <= datIn.length() : " file size is less than the expected size diff: "
-          + (bytesToRead - datIn.length()) + " pos: " + datIn.getFilePointer();
-      this.datIn = datIn;
-      this.totalLengthInBytes = bytesToRead;
-      this.pagedBytes = pagedBytes;
-      this.pagedBytes.copy(datIn, bytesToRead);
-      data = pagedBytes.freeze(true);
-      this.idxIn = idxIn;
-    }
-  }
-  
-  // TODO: open up this API?!
-  static abstract class BytesWriterBase extends Writer {
-    private final String id;
-    private IndexOutput idxOut;
-    private IndexOutput datOut;
-    protected BytesRef bytesRef = new BytesRef();
-    private final Directory dir;
-    private final String codecNameIdx;
-    private final String codecNameDat;
-    private final int version;
-    private final IOContext context;
-
-    protected BytesWriterBase(Directory dir, String id, String codecNameIdx, String codecNameDat,
-        int version, Counter bytesUsed, IOContext context, Type type) {
-      super(bytesUsed, type);
-      this.id = id;
-      this.dir = dir;
-      this.codecNameIdx = codecNameIdx;
-      this.codecNameDat = codecNameDat;
-      this.version = version;
-      this.context = context;
-      assert codecNameDat != null || codecNameIdx != null: "both codec names are null";
-      assert (codecNameDat != null && !codecNameDat.equals(codecNameIdx)) 
-      || (codecNameIdx != null && !codecNameIdx.equals(codecNameDat)):
-        "index and data codec names must not be equal";
-    }
-    
-    protected IndexOutput getOrCreateDataOut() throws IOException {
-      if (datOut == null) {
-        boolean success = false;
-        assert codecNameDat != null;
-        try {
-          datOut = dir.createOutput(IndexFileNames.segmentFileName(id, DV_SEGMENT_SUFFIX,
-              DocValuesWriterBase.DATA_EXTENSION), context);
-          CodecUtil.writeHeader(datOut, codecNameDat, version);
-          success = true;
-        } finally {
-          if (!success) {
-            IOUtils.closeWhileHandlingException(datOut);
-          }
-        }
-      }
-      return datOut;
-    }
-    
-    protected IndexOutput getIndexOut() {
-      return idxOut;
-    }
-    
-    protected IndexOutput getDataOut() {
-      return datOut;
-    }
-
-    protected IndexOutput getOrCreateIndexOut() throws IOException {
-      boolean success = false;
-      try {
-        if (idxOut == null) {
-          assert codecNameIdx != null;
-          idxOut = dir.createOutput(IndexFileNames.segmentFileName(id, DV_SEGMENT_SUFFIX,
-              DocValuesWriterBase.INDEX_EXTENSION), context);
-          CodecUtil.writeHeader(idxOut, codecNameIdx, version);
-        }
-        success = true;
-      } finally {
-        if (!success) {
-          IOUtils.closeWhileHandlingException(idxOut);
-        }
-      }
-      return idxOut;
-    }
-
-
-    @Override
-    public abstract void finish(int docCount) throws IOException;
-
-  }
-
-  /**
-   * Opens all necessary files, but does not read any data in until you call
-   * {@link #loadSource}.
-   */
-  static abstract class BytesReaderBase extends DocValues {
-    protected final IndexInput idxIn;
-    protected final IndexInput datIn;
-    protected final int version;
-    protected final String id;
-    protected final Type type;
-    
-    protected BytesReaderBase(Directory dir, String id, String codecNameIdx, String codecNameDat,
-        int maxVersion, boolean doIndex, IOContext context, Type type) throws IOException {
-      IndexInput dataIn = null;
-      IndexInput indexIn = null;
-      boolean success = false;
-      try {
-        dataIn = dir.openInput(IndexFileNames.segmentFileName(id, DV_SEGMENT_SUFFIX,
-                                                              DocValuesWriterBase.DATA_EXTENSION), context);
-        version = CodecUtil.checkHeader(dataIn, codecNameDat, maxVersion, maxVersion);
-        if (doIndex) {
-          indexIn = dir.openInput(IndexFileNames.segmentFileName(id, DV_SEGMENT_SUFFIX,
-                                                                 DocValuesWriterBase.INDEX_EXTENSION), context);
-          final int version2 = CodecUtil.checkHeader(indexIn, codecNameIdx,
-                                                     maxVersion, maxVersion);
-          assert version == version2;
-        }
-        success = true;
-      } finally {
-        if (!success) {
-          IOUtils.closeWhileHandlingException(dataIn, indexIn);
-        }
-      }
-      datIn = dataIn;
-      idxIn = indexIn;
-      this.type = type;
-      this.id = id;
-    }
-
-    /**
-     * clones and returns the data {@link IndexInput}
-     */
-    protected final IndexInput cloneData() {
-      assert datIn != null;
-      return datIn.clone();
-    }
-
-    /**
-     * clones and returns the indexing {@link IndexInput}
-     */
-    protected final IndexInput cloneIndex() {
-      assert idxIn != null;
-      return idxIn.clone();
-    }
-
-    @Override
-    public void close() throws IOException {
-      try {
-        super.close();
-      } finally {
-         IOUtils.close(datIn, idxIn);
-      }
-    }
-
-    @Override
-    public Type getType() {
-      return type;
-    }
-    
-  }
-  
-  static abstract class DerefBytesWriterBase extends BytesWriterBase {
-    protected int size = -1;
-    protected int lastDocId = -1;
-    protected int[] docToEntry;
-    protected final BytesRefHash hash;
-    protected final float acceptableOverheadRatio;
-    protected long maxBytes = 0;
-    
-    protected DerefBytesWriterBase(Directory dir, String id, String codecNameIdx, String codecNameDat,
-        int codecVersion, Counter bytesUsed, IOContext context, Type type) {
-      this(dir, id, codecNameIdx, codecNameDat, codecVersion, new DirectTrackingAllocator(
-          ByteBlockPool.BYTE_BLOCK_SIZE, bytesUsed), bytesUsed, context, PackedInts.DEFAULT, type);
-    }
-
-    protected DerefBytesWriterBase(Directory dir, String id, String codecNameIdx, String codecNameDat,
-                                   int codecVersion, Counter bytesUsed, IOContext context, float acceptableOverheadRatio, Type type) {
-      this(dir, id, codecNameIdx, codecNameDat, codecVersion, new DirectTrackingAllocator(
-          ByteBlockPool.BYTE_BLOCK_SIZE, bytesUsed), bytesUsed, context, acceptableOverheadRatio, type);
-    }
-
-    protected DerefBytesWriterBase(Directory dir, String id, String codecNameIdx, String codecNameDat, int codecVersion, Allocator allocator,
-        Counter bytesUsed, IOContext context, float acceptableOverheadRatio, Type type) {
-      super(dir, id, codecNameIdx, codecNameDat, codecVersion, bytesUsed, context, type);
-      hash = new BytesRefHash(new ByteBlockPool(allocator),
-          BytesRefHash.DEFAULT_CAPACITY, new TrackingDirectBytesStartArray(
-              BytesRefHash.DEFAULT_CAPACITY, bytesUsed));
-      docToEntry = new int[1];
-      bytesUsed.addAndGet(RamUsageEstimator.NUM_BYTES_INT);
-      this.acceptableOverheadRatio = acceptableOverheadRatio;
-    }
-    
-    protected static int writePrefixLength(DataOutput datOut, BytesRef bytes)
-        throws IOException {
-      if (bytes.length < 128) {
-        datOut.writeByte((byte) bytes.length);
-        return 1;
-      } else {
-        datOut.writeByte((byte) (0x80 | (bytes.length >> 8)));
-        datOut.writeByte((byte) (bytes.length & 0xff));
-        return 2;
-      }
-    }
-
-    @Override
-    public void add(int docID, StorableField value) throws IOException {
-      BytesRef bytes = value.binaryValue();
-      assert bytes != null;
-      if (bytes.length == 0) { // default value - skip it
-        return;
-      }
-      checkSize(bytes);
-      fillDefault(docID);
-      int ord = hash.add(bytes);
-      if (ord < 0) {
-        ord = (-ord) - 1;
-      } else {
-        maxBytes += bytes.length;
-      }
-      
-      
-      docToEntry[docID] = ord;
-      lastDocId = docID;
-    }
-    
-    protected void fillDefault(int docID) {
-      if (docID >= docToEntry.length) {
-        final int size = docToEntry.length;
-        docToEntry = ArrayUtil.grow(docToEntry, 1 + docID);
-        bytesUsed.addAndGet((docToEntry.length - size)
-            * RamUsageEstimator.NUM_BYTES_INT);
-      }
-      assert size >= 0;
-      BytesRef ref = new BytesRef(size);
-      ref.length = size;
-      int ord = hash.add(ref);
-      if (ord < 0) {
-        ord = (-ord) - 1;
-      }
-      for (int i = lastDocId+1; i < docID; i++) {
-        docToEntry[i] = ord;
-      }
-    }
-    
-    protected void checkSize(BytesRef bytes) {
-      if (size == -1) {
-        size = bytes.length;
-      } else if (bytes.length != size) {
-        throw new IllegalArgumentException("expected bytes size=" + size
-            + " but got " + bytes.length);
-      }
-    }
-    
-    @Override
-    public int getValueSize() {
-      return size;
-    }
-    
-    // Important that we get docCount, in case there were
-    // some last docs that we didn't see
-    @Override
-    public void finish(int docCount) throws IOException {
-      boolean success = false;
-      try {
-        finishInternal(docCount);
-        success = true;
-      } finally {
-        releaseResources();
-        if (success) {
-          IOUtils.close(getIndexOut(), getDataOut());
-        } else {
-          IOUtils.closeWhileHandlingException(getIndexOut(), getDataOut());
-        }
-        
-      }
-    }
-    
-    protected abstract void finishInternal(int docCount) throws IOException;
-    
-    protected void releaseResources() {
-      hash.close();
-      bytesUsed.addAndGet((-docToEntry.length) * RamUsageEstimator.NUM_BYTES_INT);
-      docToEntry = null;
-    }
-    
-    protected void writeIndex(IndexOutput idxOut, int docCount,
-        long maxValue, int[] toEntry) throws IOException {
-      writeIndex(idxOut, docCount, maxValue, (int[])null, toEntry);
-    }
-    
-    protected void writeIndex(IndexOutput idxOut, int docCount,
-        long maxValue, int[] addresses, int[] toEntry) throws IOException {
-      final PackedInts.Writer w = PackedInts.getWriter(idxOut, docCount,
-          PackedInts.bitsRequired(maxValue), acceptableOverheadRatio);
-      final int limit = docCount > docToEntry.length ? docToEntry.length
-          : docCount;
-      assert toEntry.length >= limit -1;
-      if (addresses != null) {
-        for (int i = 0; i < limit; i++) {
-          assert addresses[toEntry[i]] >= 0;
-          w.add(addresses[toEntry[i]]);
-        }
-      } else {
-        for (int i = 0; i < limit; i++) {
-          assert toEntry[i] >= 0;
-          w.add(toEntry[i]);
-        }
-      }
-      for (int i = limit; i < docCount; i++) {
-        w.add(0);
-      }
-      w.finish();
-    }
-    
-    protected void writeIndex(IndexOutput idxOut, int docCount,
-        long maxValue, long[] addresses, int[] toEntry) throws IOException {
-      final PackedInts.Writer w = PackedInts.getWriter(idxOut, docCount,
-          PackedInts.bitsRequired(maxValue), acceptableOverheadRatio);
-      final int limit = docCount > docToEntry.length ? docToEntry.length
-          : docCount;
-      assert toEntry.length >= limit -1;
-      if (addresses != null) {
-        for (int i = 0; i < limit; i++) {
-          assert addresses[toEntry[i]] >= 0;
-          w.add(addresses[toEntry[i]]);
-        }
-      } else {
-        for (int i = 0; i < limit; i++) {
-          assert toEntry[i] >= 0;
-          w.add(toEntry[i]);
-        }
-      }
-      for (int i = limit; i < docCount; i++) {
-        w.add(0);
-      }
-      w.finish();
-    }
-    
-  }
-  
-  static abstract class BytesSortedSourceBase extends SortedSource {
-    private final PagedBytes pagedBytes;
-    
-    protected final PackedInts.Reader docToOrdIndex;
-    protected final PackedInts.Reader ordToOffsetIndex;
-
-    protected final IndexInput datIn;
-    protected final IndexInput idxIn;
-    protected final BytesRef defaultValue = new BytesRef();
-    protected final static int PAGED_BYTES_BITS = 15;
-    protected final PagedBytes.Reader data;
-
-    protected BytesSortedSourceBase(IndexInput datIn, IndexInput idxIn,
-        Comparator<BytesRef> comp, long bytesToRead, Type type, boolean hasOffsets) throws IOException {
-      this(datIn, idxIn, comp, new PagedBytes(PAGED_BYTES_BITS), bytesToRead, type, hasOffsets);
-    }
-    
-    protected BytesSortedSourceBase(IndexInput datIn, IndexInput idxIn,
-        Comparator<BytesRef> comp, PagedBytes pagedBytes, long bytesToRead, Type type, boolean hasOffsets)
-        throws IOException {
-      super(type, comp);
-      assert bytesToRead <= datIn.length() : " file size is less than the expected size diff: "
-          + (bytesToRead - datIn.length()) + " pos: " + datIn.getFilePointer();
-      this.datIn = datIn;
-      this.pagedBytes = pagedBytes;
-      this.pagedBytes.copy(datIn, bytesToRead);
-      data = pagedBytes.freeze(true);
-      this.idxIn = idxIn;
-      ordToOffsetIndex = hasOffsets ? PackedInts.getReader(idxIn) : null; 
-      docToOrdIndex = PackedInts.getReader(idxIn);
-    }
-
-    @Override
-    public boolean hasPackedDocToOrd() {
-      return true;
-    }
-
-    @Override
-    public PackedInts.Reader getDocToOrd() {
-      return docToOrdIndex;
-    }
-    
-    @Override
-    public int ord(int docID) {
-      assert docToOrdIndex.get(docID) < getValueCount();
-      return (int) docToOrdIndex.get(docID);
-    }
-
-    protected void closeIndexInput() throws IOException {
-      IOUtils.close(datIn, idxIn);
-    }
-  }
-}
Index: lucene/core/src/java/org/apache/lucene/codecs/lucene40/values/VarStraightBytesImpl.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/codecs/lucene40/values/VarStraightBytesImpl.java	(revision 1442822)
+++ lucene/core/src/java/org/apache/lucene/codecs/lucene40/values/VarStraightBytesImpl.java	(working copy)
@@ -1,297 +0,0 @@
-package org.apache.lucene.codecs.lucene40.values;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.codecs.lucene40.values.Bytes.BytesReaderBase;
-import org.apache.lucene.codecs.lucene40.values.Bytes.BytesSourceBase;
-import org.apache.lucene.codecs.lucene40.values.Bytes.BytesWriterBase;
-import org.apache.lucene.document.StoredField;
-import org.apache.lucene.index.DocValues.Source;
-import org.apache.lucene.index.DocValues.Type;
-import org.apache.lucene.index.DocValues;
-import org.apache.lucene.index.StorableField;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.util.ArrayUtil;
-import org.apache.lucene.util.Bits;
-import org.apache.lucene.util.ByteBlockPool.DirectTrackingAllocator;
-import org.apache.lucene.util.ByteBlockPool;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.Counter;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.PagedBytes;
-import org.apache.lucene.util.RamUsageEstimator;
-import org.apache.lucene.util.packed.PackedInts.ReaderIterator;
-import org.apache.lucene.util.packed.PackedInts;
-
-// Variable length byte[] per document, no sharing
-
-/**
- * @lucene.experimental
- */
-class VarStraightBytesImpl {
-
-  static final String CODEC_NAME_IDX = "VarStraightBytesIdx";
-  static final String CODEC_NAME_DAT = "VarStraightBytesDat";
-
-  static final int VERSION_START = 0;
-  static final int VERSION_CURRENT = VERSION_START;
-
-  static class Writer extends BytesWriterBase {
-    private long address;
-    // start at -1 if the first added value is > 0
-    private int lastDocID = -1;
-    private long[] docToAddress;
-    private final ByteBlockPool pool;
-    private IndexOutput datOut;
-    private boolean merge = false;
-    public Writer(Directory dir, String id, Counter bytesUsed, IOContext context) {
-      super(dir, id, CODEC_NAME_IDX, CODEC_NAME_DAT, VERSION_CURRENT, bytesUsed, context, Type.BYTES_VAR_STRAIGHT);
-      pool = new ByteBlockPool(new DirectTrackingAllocator(bytesUsed));
-      docToAddress = new long[1];
-      pool.nextBuffer(); // init
-      bytesUsed.addAndGet(RamUsageEstimator.NUM_BYTES_INT);
-    }
-
-    // Fills up to but not including this docID
-    private void fill(final int docID, final long nextAddress) {
-      if (docID >= docToAddress.length) {
-        int oldSize = docToAddress.length;
-        docToAddress = ArrayUtil.grow(docToAddress, 1 + docID);
-        bytesUsed.addAndGet((docToAddress.length - oldSize)
-            * RamUsageEstimator.NUM_BYTES_INT);
-      }
-      for (int i = lastDocID + 1; i < docID; i++) {
-        docToAddress[i] = nextAddress;
-      }
-    }
-
-    @Override
-    public void add(int docID, StorableField value) throws IOException {
-      final BytesRef bytes = value.binaryValue();
-      assert bytes != null;
-      assert !merge;
-      if (bytes.length == 0) {
-        return; // default
-      }
-      fill(docID, address);
-      docToAddress[docID] = address;
-      pool.copy(bytes);
-      address += bytes.length;
-      lastDocID = docID;
-    }
-    
-    @Override
-    protected void merge(DocValues readerIn, int docBase, int docCount, Bits liveDocs) throws IOException {
-      merge = true;
-      datOut = getOrCreateDataOut();
-      boolean success = false;
-      try {
-        if (liveDocs == null && readerIn instanceof VarStraightReader) {
-          // bulk merge since we don't have any deletes
-          VarStraightReader reader = (VarStraightReader) readerIn;
-          final int maxDocs = reader.maxDoc;
-          if (maxDocs == 0) {
-            return;
-          }
-          if (lastDocID+1 < docBase) {
-            fill(docBase, address);
-            lastDocID = docBase-1;
-          }
-          final long numDataBytes;
-          final IndexInput cloneIdx = reader.cloneIndex();
-          try {
-            numDataBytes = cloneIdx.readVLong();
-            final ReaderIterator iter = PackedInts.getReaderIterator(cloneIdx, PackedInts.DEFAULT_BUFFER_SIZE);
-            for (int i = 0; i < maxDocs; i++) {
-              long offset = iter.next();
-              ++lastDocID;
-              if (lastDocID >= docToAddress.length) {
-                int oldSize = docToAddress.length;
-                docToAddress = ArrayUtil.grow(docToAddress, 1 + lastDocID);
-                bytesUsed.addAndGet((docToAddress.length - oldSize)
-                    * RamUsageEstimator.NUM_BYTES_INT);
-              }
-              docToAddress[lastDocID] = address + offset;
-            }
-            address += numDataBytes; // this is the address after all addr pointers are updated
-          } finally {
-            IOUtils.close(cloneIdx);
-          }
-          final IndexInput cloneData = reader.cloneData();
-          try {
-            datOut.copyBytes(cloneData, numDataBytes);
-          } finally {
-            IOUtils.close(cloneData);  
-          }
-        } else {
-          super.merge(readerIn, docBase, docCount, liveDocs);
-        }
-        success = true;
-      } finally {
-        if (!success) {
-          IOUtils.closeWhileHandlingException(datOut);
-        }
-      }
-    }
-    
-    @Override
-    protected void mergeDoc(StoredField scratchField, Source source, int docID, int sourceDoc) throws IOException {
-      assert merge;
-      assert lastDocID < docID;
-      source.getBytes(sourceDoc, bytesRef);
-      if (bytesRef.length == 0) {
-        return; // default
-      }
-      fill(docID, address);
-      datOut.writeBytes(bytesRef.bytes, bytesRef.offset, bytesRef.length);
-      docToAddress[docID] = address;
-      address += bytesRef.length;
-      lastDocID = docID;
-    }
-    
-
-    @Override
-    public void finish(int docCount) throws IOException {
-      boolean success = false;
-      assert (!merge && datOut == null) || (merge && datOut != null); 
-      final IndexOutput datOut = getOrCreateDataOut();
-      try {
-        if (!merge) {
-          // header is already written in getDataOut()
-          pool.writePool(datOut);
-        }
-        success = true;
-      } finally {
-        if (success) {
-          IOUtils.close(datOut);
-        } else {
-          IOUtils.closeWhileHandlingException(datOut);
-        }
-        pool.reset(false, false);
-      }
-
-      success = false;
-      final IndexOutput idxOut = getOrCreateIndexOut();
-      try {
-        if (lastDocID == -1) {
-          idxOut.writeVLong(0);
-          final PackedInts.Writer w = PackedInts.getWriter(idxOut, docCount+1,
-              PackedInts.bitsRequired(0), PackedInts.DEFAULT);
-          // docCount+1 so we write sentinel
-          for (int i = 0; i < docCount+1; i++) {
-            w.add(0);
-          }
-          w.finish();
-        } else {
-          fill(docCount, address);
-          idxOut.writeVLong(address);
-          final PackedInts.Writer w = PackedInts.getWriter(idxOut, docCount+1,
-              PackedInts.bitsRequired(address), PackedInts.DEFAULT);
-          for (int i = 0; i < docCount; i++) {
-            w.add(docToAddress[i]);
-          }
-          // write sentinel
-          w.add(address);
-          w.finish();
-        }
-        success = true;
-      } finally {
-        bytesUsed.addAndGet(-(docToAddress.length)
-            * RamUsageEstimator.NUM_BYTES_INT);
-        docToAddress = null;
-        if (success) {
-          IOUtils.close(idxOut);
-        } else {
-          IOUtils.closeWhileHandlingException(idxOut);
-        }
-      }
-    }
-
-    public long ramBytesUsed() {
-      return bytesUsed.get();
-    }
-
-    @Override
-    public int getValueSize() {
-      return -1;
-    }
-  }
-
-  public static class VarStraightReader extends BytesReaderBase {
-    final int maxDoc;
-
-    VarStraightReader(Directory dir, String id, int maxDoc, IOContext context) throws IOException {
-      super(dir, id, CODEC_NAME_IDX, CODEC_NAME_DAT, VERSION_START, true, context, Type.BYTES_VAR_STRAIGHT);
-      this.maxDoc = maxDoc;
-    }
-
-    @Override
-    protected Source loadSource() throws IOException {
-      return new VarStraightSource(cloneData(), cloneIndex());
-    }
-
-    @Override
-    protected Source loadDirectSource()
-        throws IOException {
-      return new DirectVarStraightSource(cloneData(), cloneIndex(), getType());
-    }
-  }
-  
-  private static final class VarStraightSource extends BytesSourceBase {
-    private final PackedInts.Reader addresses;
-
-    public VarStraightSource(IndexInput datIn, IndexInput idxIn) throws IOException {
-      super(datIn, idxIn, new PagedBytes(PAGED_BYTES_BITS), idxIn.readVLong(),
-          Type.BYTES_VAR_STRAIGHT);
-      addresses = PackedInts.getReader(idxIn);
-    }
-
-    @Override
-    public BytesRef getBytes(int docID, BytesRef bytesRef) {
-      final long address = addresses.get(docID);
-      return data.fillSlice(bytesRef, address,
-          (int) (addresses.get(docID + 1) - address));
-    }
-  }
-  
-  public final static class DirectVarStraightSource extends DirectSource {
-
-    private final PackedInts.Reader index;
-
-    DirectVarStraightSource(IndexInput data, IndexInput index, Type type)
-        throws IOException {
-      super(data, type);
-      index.readVLong();
-      this.index = PackedInts.getDirectReader(index);
-    }
-
-    @Override
-    protected int position(int docID) throws IOException {
-      final long offset = index.get(docID);
-      data.seek(baseOffset + offset);
-      // Safe to do 1+docID because we write sentinel at the end:
-      final long nextOffset = index.get(1+docID);
-      return (int) (nextOffset - offset);
-    }
-  }
-}
Index: lucene/core/src/java/org/apache/lucene/codecs/lucene40/values/VarDerefBytesImpl.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/codecs/lucene40/values/VarDerefBytesImpl.java	(revision 1442822)
+++ lucene/core/src/java/org/apache/lucene/codecs/lucene40/values/VarDerefBytesImpl.java	(working copy)
@@ -1,152 +0,0 @@
-package org.apache.lucene.codecs.lucene40.values;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.codecs.lucene40.values.Bytes.BytesReaderBase;
-import org.apache.lucene.codecs.lucene40.values.Bytes.BytesSourceBase;
-import org.apache.lucene.codecs.lucene40.values.Bytes.DerefBytesWriterBase;
-import org.apache.lucene.index.DocValues.Type;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.Counter;
-import org.apache.lucene.util.PagedBytes;
-import org.apache.lucene.util.packed.PackedInts;
-
-// Stores variable-length byte[] by deref, ie when two docs
-// have the same value, they store only 1 byte[] and both
-// docs reference that single source
-
-/**
- * @lucene.experimental
- */
-class VarDerefBytesImpl {
-
-  static final String CODEC_NAME_IDX = "VarDerefBytesIdx";
-  static final String CODEC_NAME_DAT = "VarDerefBytesDat";
-
-  static final int VERSION_START = 0;
-  static final int VERSION_CURRENT = VERSION_START;
-
-  /*
-   * TODO: if impls like this are merged we are bound to the amount of memory we
-   * can store into a BytesRefHash and therefore how much memory a ByteBlockPool
-   * can address. This is currently limited to 2GB. While we could extend that
-   * and use 64bit for addressing this still limits us to the existing main
-   * memory as all distinct bytes will be loaded up into main memory. We could
-   * move the byte[] writing to #finish(int) and store the bytes in sorted
-   * order and merge them in a streamed fashion. 
-   */
-  static class Writer extends DerefBytesWriterBase {
-    public Writer(Directory dir, String id, Counter bytesUsed, IOContext context) {
-      super(dir, id, CODEC_NAME_IDX, CODEC_NAME_DAT, VERSION_CURRENT, bytesUsed, context, Type.BYTES_VAR_DEREF);
-      size = 0;
-    }
-    
-    @Override
-    protected void checkSize(BytesRef bytes) {
-      // allow var bytes sizes
-    }
-
-    // Important that we get docCount, in case there were
-    // some last docs that we didn't see
-    @Override
-    public void finishInternal(int docCount) throws IOException {
-      fillDefault(docCount);
-      final int size = hash.size();
-      final long[] addresses = new long[size];
-      final IndexOutput datOut = getOrCreateDataOut();
-      int addr = 0;
-      final BytesRef bytesRef = new BytesRef();
-      for (int i = 0; i < size; i++) {
-        hash.get(i, bytesRef);
-        addresses[i] = addr;
-        addr += writePrefixLength(datOut, bytesRef) + bytesRef.length;
-        datOut.writeBytes(bytesRef.bytes, bytesRef.offset, bytesRef.length);
-      }
-
-      final IndexOutput idxOut = getOrCreateIndexOut();
-      // write the max address to read directly on source load
-      idxOut.writeLong(addr);
-      writeIndex(idxOut, docCount, addresses[addresses.length-1], addresses, docToEntry);
-    }
-  }
-
-  public static class VarDerefReader extends BytesReaderBase {
-    private final long totalBytes;
-    VarDerefReader(Directory dir, String id, int maxDoc, IOContext context) throws IOException {
-      super(dir, id, CODEC_NAME_IDX, CODEC_NAME_DAT, VERSION_START, true, context, Type.BYTES_VAR_DEREF);
-      totalBytes = idxIn.readLong();
-    }
-
-    @Override
-    protected Source loadSource() throws IOException {
-      return new VarDerefSource(cloneData(), cloneIndex(), totalBytes);
-    }
-   
-    @Override
-    protected Source loadDirectSource()
-        throws IOException {
-      return new DirectVarDerefSource(cloneData(), cloneIndex(), getType());
-    }
-  }
-  
-  final static class VarDerefSource extends BytesSourceBase {
-    private final PackedInts.Reader addresses;
-
-    public VarDerefSource(IndexInput datIn, IndexInput idxIn, long totalBytes)
-        throws IOException {
-      super(datIn, idxIn, new PagedBytes(PAGED_BYTES_BITS), totalBytes,
-          Type.BYTES_VAR_DEREF);
-      addresses = PackedInts.getReader(idxIn);
-    }
-
-    @Override
-    public BytesRef getBytes(int docID, BytesRef bytesRef) {
-      return data.fillSliceWithPrefix(bytesRef,
-          addresses.get(docID));
-    }
-  }
-
-  
-  final static class DirectVarDerefSource extends DirectSource {
-    private final PackedInts.Reader index;
-
-    DirectVarDerefSource(IndexInput data, IndexInput index, Type type)
-        throws IOException {
-      super(data, type);
-      this.index = PackedInts.getDirectReader(index);
-    }
-    
-    @Override
-    protected int position(int docID) throws IOException {
-      data.seek(baseOffset + index.get(docID));
-      final byte sizeByte = data.readByte();
-      if ((sizeByte & 128) == 0) {
-        // length is 1 byte
-        return sizeByte;
-      } else {
-        return ((sizeByte & 0x7f) << 8) | ((data.readByte() & 0xff));
-      }
-    }
-  }
-}
Index: lucene/core/src/java/org/apache/lucene/codecs/lucene40/values/Floats.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/codecs/lucene40/values/Floats.java	(revision 1442822)
+++ lucene/core/src/java/org/apache/lucene/codecs/lucene40/values/Floats.java	(working copy)
@@ -1,139 +0,0 @@
-package org.apache.lucene.codecs.lucene40.values;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-import java.io.IOException;
-
-import org.apache.lucene.codecs.DocValuesArraySource;
-import org.apache.lucene.codecs.DocValuesConsumer;
-import org.apache.lucene.index.DocValues.Source;
-import org.apache.lucene.index.DocValues.Type;
-import org.apache.lucene.index.DocValues;
-import org.apache.lucene.index.IndexableField;
-import org.apache.lucene.index.StorableField;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.Counter;
-import org.apache.lucene.util.IOUtils;
-
-/**
- * Exposes {@link Writer} and reader ({@link Source}) for 32 bit and 64 bit
- * floating point values.
- * <p>
- * Current implementations store either 4 byte or 8 byte floating points with
- * full precision without any compression.
- * 
- * @lucene.experimental
- */
-public class Floats {
-
-  /** Codec name, written in the header. */
-  protected static final String CODEC_NAME = "Floats";
-
-  /** Initial version. */
-  protected static final int VERSION_START = 0;
-
-  /** Current version. */
-  protected static final int VERSION_CURRENT = VERSION_START;
-
-  /** Sole constructor. */
-  private Floats() {
-  }
-
-  /** Creates and returns a {@link DocValuesConsumer} to
-   *  write float values. */
-  public static DocValuesConsumer getWriter(Directory dir, String id, Counter bytesUsed,
-      IOContext context, Type type) {
-    return new FloatsWriter(dir, id, bytesUsed, context, type);
-  }
-
-  /** Creates and returns a {@link DocValues} to
-   *  read previously written float values. */
-  public static DocValues getValues(Directory dir, String id, int maxDoc, IOContext context, Type type)
-      throws IOException {
-    return new FloatsReader(dir, id, maxDoc, context, type);
-  }
-  
-  private static int typeToSize(Type type) {
-    switch (type) {
-    case FLOAT_32:
-      return 4;
-    case FLOAT_64:
-      return 8;
-    default:
-      throw new IllegalStateException("illegal type " + type);
-    }
-  }
-  
-  final static class FloatsWriter extends FixedStraightBytesImpl.Writer {
-   
-    private final int size; 
-    private final DocValuesArraySource template;
-    public FloatsWriter(Directory dir, String id, Counter bytesUsed,
-        IOContext context, Type type) {
-      super(dir, id, CODEC_NAME, VERSION_CURRENT, bytesUsed, context);
-      size = typeToSize(type);
-      this.bytesRef = new BytesRef(size);
-      bytesRef.length = size;
-      template = DocValuesArraySource.forType(type);
-      assert template != null;
-    }
-    
-    @Override
-    protected boolean tryBulkMerge(DocValues docValues) {
-      // only bulk merge if value type is the same otherwise size differs
-      return super.tryBulkMerge(docValues) && docValues.getType() == template.getType();
-    }
-    
-    @Override
-    public void add(int docID, StorableField value) throws IOException {
-      template.toBytes(value.numericValue().doubleValue(), bytesRef);
-      bytesSpareField.setBytesValue(bytesRef);
-      super.add(docID, bytesSpareField);
-    }
-    
-    @Override
-    protected void setMergeBytes(Source source, int sourceDoc) {
-      final double value = source.getFloat(sourceDoc);
-      template.toBytes(value, bytesRef);
-    }
-  }
-  
-  final static class FloatsReader extends FixedStraightBytesImpl.FixedStraightReader {
-    final DocValuesArraySource arrayTemplate;
-    FloatsReader(Directory dir, String id, int maxDoc, IOContext context, Type type)
-        throws IOException {
-      super(dir, id, CODEC_NAME, VERSION_CURRENT, maxDoc, context, type);
-      arrayTemplate = DocValuesArraySource.forType(type);
-      assert size == 4 || size == 8: "wrong size=" + size + " type=" + type + " id=" + id;
-    }
-    
-    @Override
-    protected Source loadSource() throws IOException {
-      final IndexInput indexInput = cloneData();
-      try {
-        return arrayTemplate.newFromInput(indexInput, maxDoc);
-      } finally {
-        IOUtils.close(indexInput);
-      }
-    }
-    
-  }
-
-}
\ No newline at end of file
Index: lucene/core/src/java/org/apache/lucene/codecs/lucene40/values/FixedSortedBytesImpl.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/codecs/lucene40/values/FixedSortedBytesImpl.java	(revision 1442822)
+++ lucene/core/src/java/org/apache/lucene/codecs/lucene40/values/FixedSortedBytesImpl.java	(working copy)
@@ -1,232 +0,0 @@
-package org.apache.lucene.codecs.lucene40.values;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Comparator;
-import java.util.List;
-
-import org.apache.lucene.codecs.lucene40.values.Bytes.BytesReaderBase;
-import org.apache.lucene.codecs.lucene40.values.Bytes.BytesSortedSourceBase;
-import org.apache.lucene.codecs.lucene40.values.Bytes.DerefBytesWriterBase;
-import org.apache.lucene.index.DocValues;
-import org.apache.lucene.index.SortedBytesMergeUtils;
-import org.apache.lucene.index.DocValues.SortedSource;
-import org.apache.lucene.index.DocValues.Type;
-import org.apache.lucene.index.SortedBytesMergeUtils.IndexOutputBytesRefConsumer;
-import org.apache.lucene.index.SortedBytesMergeUtils.MergeContext;
-import org.apache.lucene.index.SortedBytesMergeUtils.SortedSourceSlice;
-import org.apache.lucene.index.MergeState;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.Counter;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.packed.PackedInts;
-
-// Stores fixed-length byte[] by deref, ie when two docs
-// have the same value, they store only 1 byte[]
-
-/**
- * @lucene.experimental
- */
-class FixedSortedBytesImpl {
-
-  static final String CODEC_NAME_IDX = "FixedSortedBytesIdx";
-  static final String CODEC_NAME_DAT = "FixedSortedBytesDat";
-  static final int VERSION_START = 0;
-  static final int VERSION_CURRENT = VERSION_START;
-
-  static final class Writer extends DerefBytesWriterBase {
-    private final Comparator<BytesRef> comp;
-
-    public Writer(Directory dir, String id, Comparator<BytesRef> comp,
-        Counter bytesUsed, IOContext context, float acceptableOverheadRatio) {
-      super(dir, id, CODEC_NAME_IDX, CODEC_NAME_DAT, VERSION_CURRENT, bytesUsed, context, acceptableOverheadRatio, Type.BYTES_FIXED_SORTED);
-      this.comp = comp;
-    }
-
-    @Override
-    public void merge(MergeState mergeState, DocValues[] docValues)
-        throws IOException {
-      boolean success = false;
-      try {
-        final MergeContext ctx = SortedBytesMergeUtils.init(Type.BYTES_FIXED_SORTED, docValues, comp, mergeState.segmentInfo.getDocCount());
-        List<SortedSourceSlice> slices = SortedBytesMergeUtils.buildSlices(mergeState.docBase, mergeState.docMaps, docValues, ctx);
-        final IndexOutput datOut = getOrCreateDataOut();
-        datOut.writeInt(ctx.sizePerValues);
-        final int maxOrd = SortedBytesMergeUtils.mergeRecords(ctx, new IndexOutputBytesRefConsumer(datOut), slices);
-        
-        final IndexOutput idxOut = getOrCreateIndexOut();
-        idxOut.writeInt(maxOrd);
-        final PackedInts.Writer ordsWriter = PackedInts.getWriter(idxOut, ctx.docToEntry.length,
-            PackedInts.bitsRequired(maxOrd), PackedInts.DEFAULT);
-        for (SortedSourceSlice slice : slices) {
-          slice.writeOrds(ordsWriter);
-        }
-        ordsWriter.finish();
-        success = true;
-      } finally {
-        releaseResources();
-        if (success) {
-          IOUtils.close(getIndexOut(), getDataOut());
-        } else {
-          IOUtils.closeWhileHandlingException(getIndexOut(), getDataOut());
-        }
-
-      }
-    }
-
-    // Important that we get docCount, in case there were
-    // some last docs that we didn't see
-    @Override
-    public void finishInternal(int docCount) throws IOException {
-      fillDefault(docCount);
-      final IndexOutput datOut = getOrCreateDataOut();
-      final int count = hash.size();
-      final int[] address = new int[count];
-      datOut.writeInt(size);
-      if (size != -1) {
-        final int[] sortedEntries = hash.sort(comp);
-        // first dump bytes data, recording address as we go
-        final BytesRef spare = new BytesRef(size);
-        for (int i = 0; i < count; i++) {
-          final int e = sortedEntries[i];
-          final BytesRef bytes = hash.get(e, spare);
-          assert bytes.length == size;
-          datOut.writeBytes(bytes.bytes, bytes.offset, bytes.length);
-          address[e] = i;
-        }
-      }
-      final IndexOutput idxOut = getOrCreateIndexOut();
-      idxOut.writeInt(count);
-      writeIndex(idxOut, docCount, count, address, docToEntry);
-    }
-  }
-
-  static final class Reader extends BytesReaderBase {
-    private final int size;
-    private final int valueCount;
-    private final Comparator<BytesRef> comparator;
-
-    public Reader(Directory dir, String id, int maxDoc, IOContext context,
-        Type type, Comparator<BytesRef> comparator) throws IOException {
-      super(dir, id, CODEC_NAME_IDX, CODEC_NAME_DAT, VERSION_START, true, context, type);
-      size = datIn.readInt();
-      valueCount = idxIn.readInt();
-      this.comparator = comparator;
-    }
-
-    @Override
-    protected Source loadSource() throws IOException {
-      return new FixedSortedSource(cloneData(), cloneIndex(), size, valueCount,
-          comparator);
-    }
-
-    @Override
-    protected Source loadDirectSource() throws IOException {
-      return new DirectFixedSortedSource(cloneData(), cloneIndex(), size,
-          valueCount, comparator, type);
-    }
-
-    @Override
-    public int getValueSize() {
-      return size;
-    }
-  }
-
-  static final class FixedSortedSource extends BytesSortedSourceBase {
-    private final int valueCount;
-    private final int size;
-
-    FixedSortedSource(IndexInput datIn, IndexInput idxIn, int size,
-        int numValues, Comparator<BytesRef> comp) throws IOException {
-      super(datIn, idxIn, comp, size * numValues, Type.BYTES_FIXED_SORTED,
-          false);
-      this.size = size;
-      this.valueCount = numValues;
-      closeIndexInput();
-    }
-
-    @Override
-    public int getValueCount() {
-      return valueCount;
-    }
-
-    @Override
-    public BytesRef getByOrd(int ord, BytesRef bytesRef) {
-      return data.fillSlice(bytesRef, (ord * size), size);
-    }
-  }
-
-  static final class DirectFixedSortedSource extends SortedSource {
-    final PackedInts.Reader docToOrdIndex;
-    private final IndexInput datIn;
-    private final long basePointer;
-    private final int size;
-    private final int valueCount;
-
-    DirectFixedSortedSource(IndexInput datIn, IndexInput idxIn, int size,
-        int valueCount, Comparator<BytesRef> comp, Type type)
-        throws IOException {
-      super(type, comp);
-      docToOrdIndex = PackedInts.getDirectReader(idxIn);
-      basePointer = datIn.getFilePointer();
-      this.datIn = datIn;
-      this.size = size;
-      this.valueCount = valueCount;
-    }
-
-    @Override
-    public int ord(int docID) {
-      return (int) docToOrdIndex.get(docID);
-    }
-
-    @Override
-    public boolean hasPackedDocToOrd() {
-      return true;
-    }
-
-    @Override
-    public PackedInts.Reader getDocToOrd() {
-      return docToOrdIndex;
-    }
-
-    @Override
-    public BytesRef getByOrd(int ord, BytesRef bytesRef) {
-      try {
-        datIn.seek(basePointer + size * ord);
-        bytesRef.offset = 0;
-        bytesRef.grow(size);
-        datIn.readBytes(bytesRef.bytes, 0, size);
-        bytesRef.length = size;
-        return bytesRef;
-      } catch (IOException ex) {
-        throw new IllegalStateException("failed to getByOrd", ex);
-      }
-    }
-
-    @Override
-    public int getValueCount() {
-      return valueCount;
-    }
-  }
-
-}
Index: lucene/core/src/java/org/apache/lucene/codecs/lucene40/values/DirectSource.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/codecs/lucene40/values/DirectSource.java	(revision 1442822)
+++ lucene/core/src/java/org/apache/lucene/codecs/lucene40/values/DirectSource.java	(working copy)
@@ -1,164 +0,0 @@
-package org.apache.lucene.codecs.lucene40.values;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.index.DocValues.Source;
-import org.apache.lucene.index.DocValues.Type;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.util.BytesRef;
-
-/**
- * Base class for disk resident source implementations
- * @lucene.internal
- */
-abstract class DirectSource extends Source {
-
-  protected final IndexInput data;
-  private final ToNumeric toNumeric;
-  protected final long baseOffset;
-
-  public DirectSource(IndexInput input, Type type) {
-    super(type);
-    this.data = input;
-    baseOffset = input.getFilePointer();
-    switch (type) {
-    case FIXED_INTS_16:
-      toNumeric = new ShortToLong();
-      break;
-    case FLOAT_32:
-      toNumeric = new BytesToFloat();
-      break;
-    case FLOAT_64:
-      toNumeric = new BytesToDouble();
-      break;
-    case FIXED_INTS_32:
-      toNumeric = new IntToLong();
-      break;
-    case FIXED_INTS_8:
-      toNumeric = new ByteToLong();
-      break;
-    default:
-      toNumeric = new LongToLong();
-    }
-  }
-
-  @Override
-  public BytesRef getBytes(int docID, BytesRef ref) {
-    try {
-      final int sizeToRead = position(docID);
-      ref.offset = 0;
-      ref.grow(sizeToRead);
-      data.readBytes(ref.bytes, 0, sizeToRead);
-      ref.length = sizeToRead;
-      return ref;
-    } catch (IOException ex) {
-      throw new IllegalStateException("failed to get value for docID: " + docID, ex);
-    }
-  }
-
-  @Override
-  public long getInt(int docID) {
-    try {
-      position(docID);
-      return toNumeric.toLong(data);
-    } catch (IOException ex) {
-      throw new IllegalStateException("failed to get value for docID: " + docID, ex);
-    }
-  }
-
-  @Override
-  public double getFloat(int docID) {
-    try {
-      position(docID);
-      return toNumeric.toDouble(data);
-    } catch (IOException ex) {
-      throw new IllegalStateException("failed to get value for docID: " + docID, ex);
-    }
-  }
-
-  protected abstract int position(int docID) throws IOException;
-
-  private abstract static class ToNumeric {
-    abstract long toLong(IndexInput input) throws IOException;
-
-    double toDouble(IndexInput input) throws IOException {
-      return toLong(input);
-    }
-  }
-
-  private static final class ByteToLong extends ToNumeric {
-    @Override
-    long toLong(IndexInput input) throws IOException {
-      return input.readByte();
-    }
-  }
-
-  private static final class ShortToLong extends ToNumeric {
-    @Override
-    long toLong(IndexInput input) throws IOException {
-      return input.readShort();
-    }
-  }
-
-  private static final class IntToLong extends ToNumeric {
-    @Override
-    long toLong(IndexInput input) throws IOException {
-      return input.readInt();
-    }
-  }
-  
-  private static final class BytesToFloat extends ToNumeric {
-    @Override
-    long toLong(IndexInput input) {
-      throw new UnsupportedOperationException("ints are not supported");
-    }
-
-    @Override
-    double toDouble(IndexInput input) throws IOException {
-      return Float.intBitsToFloat(input.readInt());
-    }
-  }
-  
-  private static final class BytesToDouble extends ToNumeric {
-    @Override
-    long toLong(IndexInput input) {
-      throw new UnsupportedOperationException("ints are not supported");
-    }
-
-    @Override
-    double toDouble(IndexInput input) throws IOException {
-      return Double.longBitsToDouble(input.readLong());
-    }
-  }
-
-
-  private static final class LongToLong extends ToNumeric {
-    @Override
-    long toLong(IndexInput input) throws IOException {
-      return input.readLong();
-    }
-
-    @Override
-    double toDouble(IndexInput input) {
-      throw new UnsupportedOperationException("doubles are not supported");
-    }
-  }
-
-}
Index: lucene/core/src/java/org/apache/lucene/codecs/lucene41/Lucene41PostingsBaseFormat.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/codecs/lucene41/Lucene41PostingsBaseFormat.java	(revision 1442822)
+++ lucene/core/src/java/org/apache/lucene/codecs/lucene41/Lucene41PostingsBaseFormat.java	(working copy)
@@ -41,7 +41,7 @@
 
   @Override
   public PostingsReaderBase postingsReaderBase(SegmentReadState state) throws IOException {
-    return new Lucene41PostingsReader(state.dir, state.fieldInfos, state.segmentInfo, state.context, state.segmentSuffix);
+    return new Lucene41PostingsReader(state.directory, state.fieldInfos, state.segmentInfo, state.context, state.segmentSuffix);
   }
 
   @Override
Index: lucene/core/src/java/org/apache/lucene/codecs/lucene41/Lucene41Codec.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/codecs/lucene41/Lucene41Codec.java	(revision 1442822)
+++ lucene/core/src/java/org/apache/lucene/codecs/lucene41/Lucene41Codec.java	(working copy)
@@ -20,19 +20,19 @@
 import java.io.IOException;
 
 import org.apache.lucene.codecs.Codec;
-import org.apache.lucene.codecs.DocValuesFormat;
 import org.apache.lucene.codecs.FieldInfosFormat;
 import org.apache.lucene.codecs.FilterCodec;
 import org.apache.lucene.codecs.LiveDocsFormat;
-import org.apache.lucene.codecs.NormsFormat;
 import org.apache.lucene.codecs.PostingsFormat;
 import org.apache.lucene.codecs.SegmentInfoFormat;
+import org.apache.lucene.codecs.DocValuesFormat;
+import org.apache.lucene.codecs.NormsFormat;
 import org.apache.lucene.codecs.StoredFieldsFormat;
 import org.apache.lucene.codecs.StoredFieldsWriter;
 import org.apache.lucene.codecs.TermVectorsFormat;
-import org.apache.lucene.codecs.lucene40.Lucene40DocValuesFormat;
 import org.apache.lucene.codecs.compressing.CompressingStoredFieldsFormat;
 import org.apache.lucene.codecs.compressing.CompressionMode;
+import org.apache.lucene.codecs.lucene40.Lucene40DocValuesFormat;
 import org.apache.lucene.codecs.lucene40.Lucene40FieldInfosFormat;
 import org.apache.lucene.codecs.lucene40.Lucene40LiveDocsFormat;
 import org.apache.lucene.codecs.lucene40.Lucene40NormsFormat;
@@ -64,9 +64,7 @@
   };
   private final TermVectorsFormat vectorsFormat = new Lucene40TermVectorsFormat();
   private final FieldInfosFormat fieldInfosFormat = new Lucene40FieldInfosFormat();
-  private final DocValuesFormat docValuesFormat = new Lucene40DocValuesFormat();
   private final SegmentInfoFormat infosFormat = new Lucene40SegmentInfoFormat();
-  private final NormsFormat normsFormat = new Lucene40NormsFormat();
   private final LiveDocsFormat liveDocsFormat = new Lucene40LiveDocsFormat();
   
   private final PostingsFormat postingsFormat = new PerFieldPostingsFormat() {
@@ -93,17 +91,12 @@
   }
 
   @Override
-  public final DocValuesFormat docValuesFormat() {
-    return docValuesFormat;
-  }
-
-  @Override
   public final PostingsFormat postingsFormat() {
     return postingsFormat;
   }
   
   @Override
-  public final FieldInfosFormat fieldInfosFormat() {
+  public FieldInfosFormat fieldInfosFormat() {
     return fieldInfosFormat;
   }
   
@@ -111,11 +104,6 @@
   public final SegmentInfoFormat segmentInfoFormat() {
     return infosFormat;
   }
-
-  @Override
-  public final NormsFormat normsFormat() {
-    return normsFormat;
-  }
   
   @Override
   public final LiveDocsFormat liveDocsFormat() {
@@ -131,5 +119,17 @@
     return defaultFormat;
   }
   
+  @Override
+  public DocValuesFormat docValuesFormat() {
+    return dvFormat;
+  }
+
   private final PostingsFormat defaultFormat = PostingsFormat.forName("Lucene41");
+  private final DocValuesFormat dvFormat = new Lucene40DocValuesFormat();
+  private final NormsFormat normsFormat = new Lucene40NormsFormat();
+
+  @Override
+  public NormsFormat normsFormat() {
+    return normsFormat;
+  }
 }
Index: lucene/core/src/java/org/apache/lucene/codecs/lucene41/Lucene41PostingsFormat.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/codecs/lucene41/Lucene41PostingsFormat.java	(revision 1442822)
+++ lucene/core/src/java/org/apache/lucene/codecs/lucene41/Lucene41PostingsFormat.java	(working copy)
@@ -427,14 +427,14 @@
 
   @Override
   public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {
-    PostingsReaderBase postingsReader = new Lucene41PostingsReader(state.dir,
+    PostingsReaderBase postingsReader = new Lucene41PostingsReader(state.directory,
                                                                 state.fieldInfos,
                                                                 state.segmentInfo,
                                                                 state.context,
                                                                 state.segmentSuffix);
     boolean success = false;
     try {
-      FieldsProducer ret = new BlockTreeTermsReader(state.dir,
+      FieldsProducer ret = new BlockTreeTermsReader(state.directory,
                                                     state.fieldInfos,
                                                     state.segmentInfo,
                                                     postingsReader,
Index: lucene/core/src/java/org/apache/lucene/search/FieldCacheRangeFilter.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/search/FieldCacheRangeFilter.java	(revision 1442822)
+++ lucene/core/src/java/org/apache/lucene/search/FieldCacheRangeFilter.java	(working copy)
@@ -18,15 +18,16 @@
 
 import java.io.IOException;
 
+import org.apache.lucene.document.DoubleField; // for javadocs
+import org.apache.lucene.document.FloatField; // for javadocs
+import org.apache.lucene.document.IntField; // for javadocs
+import org.apache.lucene.document.LongField; // for javadocs
 import org.apache.lucene.index.AtomicReader; // for javadocs
 import org.apache.lucene.index.AtomicReaderContext;
-import org.apache.lucene.document.IntField; // for javadocs
-import org.apache.lucene.document.FloatField; // for javadocs
-import org.apache.lucene.document.LongField; // for javadocs
-import org.apache.lucene.document.DoubleField; // for javadocs
-import org.apache.lucene.util.NumericUtils;
+import org.apache.lucene.index.SortedDocValues;
 import org.apache.lucene.util.Bits;
 import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.NumericUtils;
 
 /**
  * A range filter built on top of a cached single term field (in {@link FieldCache}).
@@ -89,43 +90,41 @@
     return new FieldCacheRangeFilter<String>(field, null, lowerVal, upperVal, includeLower, includeUpper) {
       @Override
       public DocIdSet getDocIdSet(AtomicReaderContext context, Bits acceptDocs) throws IOException {
-        final FieldCache.DocTermsIndex fcsi = FieldCache.DEFAULT.getTermsIndex(context.reader(), field);
-        final BytesRef spare = new BytesRef();
-        final int lowerPoint = fcsi.binarySearchLookup(lowerVal == null ? null : new BytesRef(lowerVal), spare);
-        final int upperPoint = fcsi.binarySearchLookup(upperVal == null ? null : new BytesRef(upperVal), spare);
+        final SortedDocValues fcsi = FieldCache.DEFAULT.getTermsIndex(context.reader(), field);
+        final int lowerPoint = lowerVal == null ? -1 : fcsi.lookupTerm(new BytesRef(lowerVal));
+        final int upperPoint = upperVal == null ? -1 : fcsi.lookupTerm(new BytesRef(upperVal));
 
         final int inclusiveLowerPoint, inclusiveUpperPoint;
 
         // Hints:
-        // * binarySearchLookup returns 0, if value was null.
+        // * binarySearchLookup returns -1, if value was null.
         // * the value is <0 if no exact hit was found, the returned value
         //   is (-(insertion point) - 1)
-        if (lowerPoint == 0) {
-          assert lowerVal == null;
-          inclusiveLowerPoint = 1;
-        } else if (includeLower && lowerPoint > 0) {
+        if (lowerPoint == -1 && lowerVal == null) {
+          inclusiveLowerPoint = 0;
+        } else if (includeLower && lowerPoint >= 0) {
           inclusiveLowerPoint = lowerPoint;
-        } else if (lowerPoint > 0) {
+        } else if (lowerPoint >= 0) {
           inclusiveLowerPoint = lowerPoint + 1;
         } else {
-          inclusiveLowerPoint = Math.max(1, -lowerPoint - 1);
+          inclusiveLowerPoint = Math.max(0, -lowerPoint - 1);
         }
         
-        if (upperPoint == 0) {
-          assert upperVal == null;
+        if (upperPoint == -1 && upperVal == null) {
           inclusiveUpperPoint = Integer.MAX_VALUE;  
-        } else if (includeUpper && upperPoint > 0) {
+        } else if (includeUpper && upperPoint >= 0) {
           inclusiveUpperPoint = upperPoint;
-        } else if (upperPoint > 0) {
+        } else if (upperPoint >= 0) {
           inclusiveUpperPoint = upperPoint - 1;
         } else {
           inclusiveUpperPoint = -upperPoint - 2;
         }      
 
-        if (inclusiveUpperPoint <= 0 || inclusiveLowerPoint > inclusiveUpperPoint)
+        if (inclusiveUpperPoint < 0 || inclusiveLowerPoint > inclusiveUpperPoint) {
           return DocIdSet.EMPTY_DOCIDSET;
+        }
         
-        assert inclusiveLowerPoint > 0 && inclusiveUpperPoint > 0;
+        assert inclusiveLowerPoint >= 0 && inclusiveUpperPoint >= 0;
         
         return new FieldCacheDocIdSet(context.reader().maxDoc(), acceptDocs) {
           @Override
@@ -139,6 +138,63 @@
   }
   
   /**
+   * Creates a BytesRef range filter using {@link FieldCache#getTermsIndex}. This works with all
+   * fields containing zero or one term in the field. The range can be half-open by setting one
+   * of the values to <code>null</code>.
+   */
+  // TODO: bogus that newStringRange doesnt share this code... generics hell
+  public static FieldCacheRangeFilter<BytesRef> newBytesRefRange(String field, BytesRef lowerVal, BytesRef upperVal, boolean includeLower, boolean includeUpper) {
+    return new FieldCacheRangeFilter<BytesRef>(field, null, lowerVal, upperVal, includeLower, includeUpper) {
+      @Override
+      public DocIdSet getDocIdSet(AtomicReaderContext context, Bits acceptDocs) throws IOException {
+        final SortedDocValues fcsi = FieldCache.DEFAULT.getTermsIndex(context.reader(), field);
+        final int lowerPoint = lowerVal == null ? -1 : fcsi.lookupTerm(lowerVal);
+        final int upperPoint = upperVal == null ? -1 : fcsi.lookupTerm(upperVal);
+
+        final int inclusiveLowerPoint, inclusiveUpperPoint;
+
+        // Hints:
+        // * binarySearchLookup returns -1, if value was null.
+        // * the value is <0 if no exact hit was found, the returned value
+        //   is (-(insertion point) - 1)
+        if (lowerPoint == -1 && lowerVal == null) {
+          inclusiveLowerPoint = 0;
+        } else if (includeLower && lowerPoint >= 0) {
+          inclusiveLowerPoint = lowerPoint;
+        } else if (lowerPoint >= 0) {
+          inclusiveLowerPoint = lowerPoint + 1;
+        } else {
+          inclusiveLowerPoint = Math.max(0, -lowerPoint - 1);
+        }
+        
+        if (upperPoint == -1 && upperVal == null) {
+          inclusiveUpperPoint = Integer.MAX_VALUE;  
+        } else if (includeUpper && upperPoint >= 0) {
+          inclusiveUpperPoint = upperPoint;
+        } else if (upperPoint >= 0) {
+          inclusiveUpperPoint = upperPoint - 1;
+        } else {
+          inclusiveUpperPoint = -upperPoint - 2;
+        }      
+
+        if (inclusiveUpperPoint < 0 || inclusiveLowerPoint > inclusiveUpperPoint) {
+          return DocIdSet.EMPTY_DOCIDSET;
+        }
+        
+        assert inclusiveLowerPoint >= 0 && inclusiveUpperPoint >= 0;
+        
+        return new FieldCacheDocIdSet(context.reader().maxDoc(), acceptDocs) {
+          @Override
+          protected final boolean matchDoc(int doc) {
+            final int docOrd = fcsi.getOrd(doc);
+            return docOrd >= inclusiveLowerPoint && docOrd <= inclusiveUpperPoint;
+          }
+        };
+      }
+    };
+  }
+  
+  /**
    * Creates a numeric range filter using {@link FieldCache#getBytes(AtomicReader,String,boolean)}. This works with all
    * byte fields containing exactly one numeric term in the field. The range can be half-open by setting one
    * of the values to <code>null</code>.
@@ -177,11 +233,12 @@
         if (inclusiveLowerPoint > inclusiveUpperPoint)
           return DocIdSet.EMPTY_DOCIDSET;
         
-        final byte[] values = FieldCache.DEFAULT.getBytes(context.reader(), field, (FieldCache.ByteParser) parser, false);
+        final FieldCache.Bytes values = FieldCache.DEFAULT.getBytes(context.reader(), field, (FieldCache.ByteParser) parser, false);
         return new FieldCacheDocIdSet(context.reader().maxDoc(), acceptDocs) {
           @Override
           protected boolean matchDoc(int doc) {
-            return values[doc] >= inclusiveLowerPoint && values[doc] <= inclusiveUpperPoint;
+            final byte value = values.get(doc);
+            return value >= inclusiveLowerPoint && value <= inclusiveUpperPoint;
           }
         };
       }
@@ -227,11 +284,12 @@
         if (inclusiveLowerPoint > inclusiveUpperPoint)
           return DocIdSet.EMPTY_DOCIDSET;
         
-        final short[] values = FieldCache.DEFAULT.getShorts(context.reader(), field, (FieldCache.ShortParser) parser, false);
+        final FieldCache.Shorts values = FieldCache.DEFAULT.getShorts(context.reader(), field, (FieldCache.ShortParser) parser, false);
         return new FieldCacheDocIdSet(context.reader().maxDoc(), acceptDocs) {
           @Override
           protected boolean matchDoc(int doc) {
-            return values[doc] >= inclusiveLowerPoint && values[doc] <= inclusiveUpperPoint;
+            final short value = values.get(doc);
+            return value >= inclusiveLowerPoint && value <= inclusiveUpperPoint;
           }
         };
       }
@@ -277,11 +335,12 @@
         if (inclusiveLowerPoint > inclusiveUpperPoint)
           return DocIdSet.EMPTY_DOCIDSET;
         
-        final int[] values = FieldCache.DEFAULT.getInts(context.reader(), field, (FieldCache.IntParser) parser, false);
+        final FieldCache.Ints values = FieldCache.DEFAULT.getInts(context.reader(), field, (FieldCache.IntParser) parser, false);
         return new FieldCacheDocIdSet(context.reader().maxDoc(), acceptDocs) {
           @Override
           protected boolean matchDoc(int doc) {
-            return values[doc] >= inclusiveLowerPoint && values[doc] <= inclusiveUpperPoint;
+            final int value = values.get(doc);
+            return value >= inclusiveLowerPoint && value <= inclusiveUpperPoint;
           }
         };
       }
@@ -327,11 +386,12 @@
         if (inclusiveLowerPoint > inclusiveUpperPoint)
           return DocIdSet.EMPTY_DOCIDSET;
         
-        final long[] values = FieldCache.DEFAULT.getLongs(context.reader(), field, (FieldCache.LongParser) parser, false);
+        final FieldCache.Longs values = FieldCache.DEFAULT.getLongs(context.reader(), field, (FieldCache.LongParser) parser, false);
         return new FieldCacheDocIdSet(context.reader().maxDoc(), acceptDocs) {
           @Override
           protected boolean matchDoc(int doc) {
-            return values[doc] >= inclusiveLowerPoint && values[doc] <= inclusiveUpperPoint;
+            final long value = values.get(doc);
+            return value >= inclusiveLowerPoint && value <= inclusiveUpperPoint;
           }
         };
       }
@@ -381,11 +441,12 @@
         if (inclusiveLowerPoint > inclusiveUpperPoint)
           return DocIdSet.EMPTY_DOCIDSET;
         
-        final float[] values = FieldCache.DEFAULT.getFloats(context.reader(), field, (FieldCache.FloatParser) parser, false);
+        final FieldCache.Floats values = FieldCache.DEFAULT.getFloats(context.reader(), field, (FieldCache.FloatParser) parser, false);
         return new FieldCacheDocIdSet(context.reader().maxDoc(), acceptDocs) {
           @Override
           protected boolean matchDoc(int doc) {
-            return values[doc] >= inclusiveLowerPoint && values[doc] <= inclusiveUpperPoint;
+            final float value = values.get(doc);
+            return value >= inclusiveLowerPoint && value <= inclusiveUpperPoint;
           }
         };
       }
@@ -435,12 +496,13 @@
         if (inclusiveLowerPoint > inclusiveUpperPoint)
           return DocIdSet.EMPTY_DOCIDSET;
         
-        final double[] values = FieldCache.DEFAULT.getDoubles(context.reader(), field, (FieldCache.DoubleParser) parser, false);
+        final FieldCache.Doubles values = FieldCache.DEFAULT.getDoubles(context.reader(), field, (FieldCache.DoubleParser) parser, false);
         // ignore deleted docs if range doesn't contain 0
         return new FieldCacheDocIdSet(context.reader().maxDoc(), acceptDocs) {
           @Override
           protected boolean matchDoc(int doc) {
-            return values[doc] >= inclusiveLowerPoint && values[doc] <= inclusiveUpperPoint;
+            final double value = values.get(doc);
+            return value >= inclusiveLowerPoint && value <= inclusiveUpperPoint;
           }
         };
       }
Index: lucene/core/src/java/org/apache/lucene/search/FieldCacheImpl.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/search/FieldCacheImpl.java	(revision 1442822)
+++ lucene/core/src/java/org/apache/lucene/search/FieldCacheImpl.java	(working copy)
@@ -20,20 +20,20 @@
 import java.io.IOException;
 import java.io.PrintStream;
 import java.util.ArrayList;
-import java.util.Comparator;
 import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
 import java.util.WeakHashMap;
 
+import org.apache.lucene.index.AtomicReader;
+import org.apache.lucene.index.BinaryDocValues;
 import org.apache.lucene.index.DocTermOrds;
-import org.apache.lucene.index.DocsAndPositionsEnum;
 import org.apache.lucene.index.DocsEnum;
-import org.apache.lucene.index.AtomicReader;
+import org.apache.lucene.index.FieldInfo;
 import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.OrdTermState;
+import org.apache.lucene.index.NumericDocValues;
 import org.apache.lucene.index.SegmentReader;
-import org.apache.lucene.index.TermState;
+import org.apache.lucene.index.SortedDocValues;
 import org.apache.lucene.index.Terms;
 import org.apache.lucene.index.TermsEnum;
 import org.apache.lucene.util.ArrayUtil;
@@ -57,6 +57,7 @@
   FieldCacheImpl() {
     init();
   }
+
   private synchronized void init() {
     caches = new HashMap<Class<?>,Cache>(9);
     caches.put(Byte.TYPE, new ByteCache(this));
@@ -65,8 +66,8 @@
     caches.put(Float.TYPE, new FloatCache(this));
     caches.put(Long.TYPE, new LongCache(this));
     caches.put(Double.TYPE, new DoubleCache(this));
-    caches.put(DocTerms.class, new DocTermsCache(this));
-    caches.put(DocTermsIndex.class, new DocTermsIndexCache(this));
+    caches.put(BinaryDocValues.class, new BinaryDocValuesCache(this));
+    caches.put(SortedDocValues.class, new SortedDocValuesCache(this));
     caches.put(DocTermOrds.class, new DocTermOrdsCache(this));
     caches.put(DocsWithFieldCache.class, new DocsWithFieldCache(this));
   }
@@ -82,7 +83,7 @@
       c.purge(r);
     }
   }
-  
+
   @Override
   public synchronized CacheEntry[] getCacheEntries() {
     List<CacheEntry> result = new ArrayList<CacheEntry>(17);
@@ -90,57 +91,22 @@
       final Cache cache = cacheEntry.getValue();
       final Class<?> cacheType = cacheEntry.getKey();
       synchronized(cache.readerCache) {
-        for (final Map.Entry<Object,Map<Entry, Object>> readerCacheEntry : cache.readerCache.entrySet()) {
+        for (final Map.Entry<Object,Map<CacheKey, Object>> readerCacheEntry : cache.readerCache.entrySet()) {
           final Object readerKey = readerCacheEntry.getKey();
           if (readerKey == null) continue;
-          final Map<Entry, Object> innerCache = readerCacheEntry.getValue();
-          for (final Map.Entry<Entry, Object> mapEntry : innerCache.entrySet()) {
-            Entry entry = mapEntry.getKey();
-            result.add(new CacheEntryImpl(readerKey, entry.field,
-                                          cacheType, entry.custom,
-                                          mapEntry.getValue()));
+          final Map<CacheKey, Object> innerCache = readerCacheEntry.getValue();
+          for (final Map.Entry<CacheKey, Object> mapEntry : innerCache.entrySet()) {
+            CacheKey entry = mapEntry.getKey();
+            result.add(new CacheEntry(readerKey, entry.field,
+                                      cacheType, entry.custom,
+                                      mapEntry.getValue()));
           }
         }
       }
     }
     return result.toArray(new CacheEntry[result.size()]);
   }
-  
-  private static final class CacheEntryImpl extends CacheEntry {
-    private final Object readerKey;
-    private final String fieldName;
-    private final Class<?> cacheType;
-    private final Object custom;
-    private final Object value;
-    CacheEntryImpl(Object readerKey, String fieldName,
-                   Class<?> cacheType,
-                   Object custom,
-                   Object value) {
-        this.readerKey = readerKey;
-        this.fieldName = fieldName;
-        this.cacheType = cacheType;
-        this.custom = custom;
-        this.value = value;
 
-        // :HACK: for testing.
-//         if (null != locale || SortField.CUSTOM != sortFieldType) {
-//           throw new RuntimeException("Locale/sortFieldType: " + this);
-//         }
-
-    }
-    @Override
-    public Object getReaderKey() { return readerKey; }
-    @Override
-    public String getFieldName() { return fieldName; }
-    @Override
-    public Class<?> getCacheType() { return cacheType; }
-    @Override
-    public Object getCustom() { return custom; }
-    @Override
-    public Object getValue() { return value; }
-  }
-
-  
   // per-segment fieldcaches don't purge until the shared core closes.
   final SegmentReader.CoreClosedListener purgeCore = new SegmentReader.CoreClosedListener() {
     @Override
@@ -183,9 +149,9 @@
 
     final FieldCacheImpl wrapper;
 
-    final Map<Object,Map<Entry,Object>> readerCache = new WeakHashMap<Object,Map<Entry,Object>>();
+    final Map<Object,Map<CacheKey,Object>> readerCache = new WeakHashMap<Object,Map<CacheKey,Object>>();
     
-    protected abstract Object createValue(AtomicReader reader, Entry key, boolean setDocsWithField)
+    protected abstract Object createValue(AtomicReader reader, CacheKey key, boolean setDocsWithField)
         throws IOException;
 
     /** Remove this reader from the cache, if present. */
@@ -198,13 +164,13 @@
 
     /** Sets the key to the value for the provided reader;
      *  if the key is already set then this doesn't change it. */
-    public void put(AtomicReader reader, Entry key, Object value) {
+    public void put(AtomicReader reader, CacheKey key, Object value) {
       final Object readerKey = reader.getCoreCacheKey();
       synchronized (readerCache) {
-        Map<Entry,Object> innerCache = readerCache.get(readerKey);
+        Map<CacheKey,Object> innerCache = readerCache.get(readerKey);
         if (innerCache == null) {
           // First time this reader is using FieldCache
-          innerCache = new HashMap<Entry,Object>();
+          innerCache = new HashMap<CacheKey,Object>();
           readerCache.put(readerKey, innerCache);
           wrapper.initReader(reader);
         }
@@ -217,15 +183,15 @@
       }
     }
 
-    public Object get(AtomicReader reader, Entry key, boolean setDocsWithField) throws IOException {
-      Map<Entry,Object> innerCache;
+    public Object get(AtomicReader reader, CacheKey key, boolean setDocsWithField) throws IOException {
+      Map<CacheKey,Object> innerCache;
       Object value;
       final Object readerKey = reader.getCoreCacheKey();
       synchronized (readerCache) {
         innerCache = readerCache.get(readerKey);
         if (innerCache == null) {
           // First time this reader is using FieldCache
-          innerCache = new HashMap<Entry,Object>();
+          innerCache = new HashMap<CacheKey,Object>();
           readerCache.put(readerKey, innerCache);
           wrapper.initReader(reader);
           value = null;
@@ -281,12 +247,12 @@
   }
 
   /** Expert: Every composite-key in the internal cache is of this type. */
-  static class Entry {
+  static class CacheKey {
     final String field;        // which Field
     final Object custom;       // which custom comparator or parser
 
     /** Creates one of these objects for a custom comparator/parser. */
-    Entry (String field, Object custom) {
+    CacheKey(String field, Object custom) {
       this.field = field;
       this.custom = custom;
     }
@@ -294,8 +260,8 @@
     /** Two of these are equal iff they reference the same field and type. */
     @Override
     public boolean equals (Object o) {
-      if (o instanceof Entry) {
-        Entry other = (Entry) o;
+      if (o instanceof CacheKey) {
+        CacheKey other = (CacheKey) o;
         if (other.field.equals(field)) {
           if (other.custom == null) {
             if (custom == null) return true;
@@ -314,264 +280,366 @@
     }
   }
 
-  // inherit javadocs
-  @Override
-  public byte[] getBytes (AtomicReader reader, String field, boolean setDocsWithField) throws IOException {
-    return getBytes(reader, field, null, setDocsWithField);
-  }
+  private static abstract class Uninvert {
 
-  // inherit javadocs
-  @Override
-  public byte[] getBytes(AtomicReader reader, String field, ByteParser parser, boolean setDocsWithField)
-      throws IOException {
-    return (byte[]) caches.get(Byte.TYPE).get(reader, new Entry(field, parser), setDocsWithField);
-  }
+    public Bits docsWithField;
 
-  static final class ByteCache extends Cache {
-    ByteCache(FieldCacheImpl wrapper) {
-      super(wrapper);
-    }
-    @Override
-    protected Object createValue(AtomicReader reader, Entry entryKey, boolean setDocsWithField)
-        throws IOException {
-      String field = entryKey.field;
-      ByteParser parser = (ByteParser) entryKey.custom;
-      if (parser == null) {
-        return wrapper.getBytes(reader, field, FieldCache.DEFAULT_BYTE_PARSER, setDocsWithField);
-      }
+    public void uninvert(AtomicReader reader, String field, boolean setDocsWithField) throws IOException {
       final int maxDoc = reader.maxDoc();
-      final byte[] retArray = new byte[maxDoc];
       Terms terms = reader.terms(field);
-      FixedBitSet docsWithField = null;
       if (terms != null) {
         if (setDocsWithField) {
           final int termsDocCount = terms.getDocCount();
           assert termsDocCount <= maxDoc;
           if (termsDocCount == maxDoc) {
             // Fast case: all docs have this field:
-            wrapper.setDocsWithField(reader, field, new Bits.MatchAllBits(maxDoc));
+            docsWithField = new Bits.MatchAllBits(maxDoc);
             setDocsWithField = false;
           }
         }
-        final TermsEnum termsEnum = parser.termsEnum(terms);
-        assert termsEnum != null : "TermsEnum must not be null";
+
+        final TermsEnum termsEnum = termsEnum(terms);
+
         DocsEnum docs = null;
+        FixedBitSet docsWithField = null;
         while(true) {
           final BytesRef term = termsEnum.next();
           if (term == null) {
             break;
           }
-          final byte termval = parser.parseByte(term);
+          visitTerm(term);
           docs = termsEnum.docs(null, docs, DocsEnum.FLAG_NONE);
           while (true) {
             final int docID = docs.nextDoc();
             if (docID == DocIdSetIterator.NO_MORE_DOCS) {
               break;
             }
-            retArray[docID] = termval;
+            visitDoc(docID);
             if (setDocsWithField) {
               if (docsWithField == null) {
                 // Lazy init
-                docsWithField = new FixedBitSet(maxDoc);
+                this.docsWithField = docsWithField = new FixedBitSet(maxDoc);
               }
               docsWithField.set(docID);
             }
           }
         }
       }
+    }
+
+    protected abstract TermsEnum termsEnum(Terms terms) throws IOException;
+    protected abstract void visitTerm(BytesRef term);
+    protected abstract void visitDoc(int docID);
+  }
+
+  // null Bits means no docs matched
+  void setDocsWithField(AtomicReader reader, String field, Bits docsWithField) {
+    final int maxDoc = reader.maxDoc();
+    final Bits bits;
+    if (docsWithField == null) {
+      bits = new Bits.MatchNoBits(maxDoc);
+    } else if (docsWithField instanceof FixedBitSet) {
+      final int numSet = ((FixedBitSet) docsWithField).cardinality();
+      if (numSet >= maxDoc) {
+        // The cardinality of the BitSet is maxDoc if all documents have a value.
+        assert numSet == maxDoc;
+        bits = new Bits.MatchAllBits(maxDoc);
+      } else {
+        bits = docsWithField;
+      }
+    } else {
+      bits = docsWithField;
+    }
+    caches.get(DocsWithFieldCache.class).put(reader, new CacheKey(field, null), bits);
+  }
+  
+  // inherit javadocs
+  public Bytes getBytes (AtomicReader reader, String field, boolean setDocsWithField) throws IOException {
+    return getBytes(reader, field, null, setDocsWithField);
+  }
+
+  // inherit javadocs
+  public Bytes getBytes(AtomicReader reader, String field, ByteParser parser, boolean setDocsWithField)
+      throws IOException {
+    final NumericDocValues valuesIn = reader.getNumericDocValues(field);
+    if (valuesIn != null) {
+      // Not cached here by FieldCacheImpl (cached instead
+      // per-thread by SegmentReader):
+      return new Bytes() {
+        @Override
+        public byte get(int docID) {
+          return (byte) valuesIn.get(docID);
+        }
+      };
+    } else {
+      return (Bytes) caches.get(Byte.TYPE).get(reader, new CacheKey(field, parser), setDocsWithField);
+    }
+  }
+
+  static class BytesFromArray extends Bytes {
+    private final byte[] values;
+
+    public BytesFromArray(byte[] values) {
+      this.values = values;
+    }
+    
+    @Override
+    public byte get(int docID) {
+      return values[docID];
+    }
+  }
+
+  static final class ByteCache extends Cache {
+    ByteCache(FieldCacheImpl wrapper) {
+      super(wrapper);
+    }
+
+    @Override
+    protected Object createValue(AtomicReader reader, CacheKey key, boolean setDocsWithField)
+        throws IOException {
+
+      int maxDoc = reader.maxDoc();
+      final byte[] values;
+      final ByteParser parser = (ByteParser) key.custom;
+      if (parser == null) {
+        // Confusing: must delegate to wrapper (vs simply
+        // setting parser = DEFAULT_SHORT_PARSER) so cache
+        // key includes DEFAULT_SHORT_PARSER:
+        return wrapper.getBytes(reader, key.field, DEFAULT_BYTE_PARSER, setDocsWithField);
+      }
+
+      values = new byte[maxDoc];
+
+      Uninvert u = new Uninvert() {
+          private byte currentValue;
+
+          @Override
+          public void visitTerm(BytesRef term) {
+            currentValue = parser.parseByte(term);
+          }
+
+          @Override
+          public void visitDoc(int docID) {
+            values[docID] = currentValue;
+          }
+
+          @Override
+          protected TermsEnum termsEnum(Terms terms) throws IOException {
+            return parser.termsEnum(terms);
+          }
+        };
+
+      u.uninvert(reader, key.field, setDocsWithField);
+
       if (setDocsWithField) {
-        wrapper.setDocsWithField(reader, field, docsWithField);
+        wrapper.setDocsWithField(reader, key.field, u.docsWithField);
       }
-      return retArray;
+
+      return new BytesFromArray(values);
     }
   }
   
   // inherit javadocs
-  @Override
-  public short[] getShorts (AtomicReader reader, String field, boolean setDocsWithField) throws IOException {
+  public Shorts getShorts (AtomicReader reader, String field, boolean setDocsWithField) throws IOException {
     return getShorts(reader, field, null, setDocsWithField);
   }
 
   // inherit javadocs
-  @Override
-  public short[] getShorts(AtomicReader reader, String field, ShortParser parser, boolean setDocsWithField)
+  public Shorts getShorts(AtomicReader reader, String field, ShortParser parser, boolean setDocsWithField)
       throws IOException {
-    return (short[]) caches.get(Short.TYPE).get(reader, new Entry(field, parser), setDocsWithField);
+    final NumericDocValues valuesIn = reader.getNumericDocValues(field);
+    if (valuesIn != null) {
+      // Not cached here by FieldCacheImpl (cached instead
+      // per-thread by SegmentReader):
+      return new Shorts() {
+        @Override
+        public short get(int docID) {
+          return (short) valuesIn.get(docID);
+        }
+      };
+    } else {
+      return (Shorts) caches.get(Short.TYPE).get(reader, new CacheKey(field, parser), setDocsWithField);
+    }
   }
 
+  static class ShortsFromArray extends Shorts {
+    private final short[] values;
+
+    public ShortsFromArray(short[] values) {
+      this.values = values;
+    }
+    
+    @Override
+    public short get(int docID) {
+      return values[docID];
+    }
+  }
+
   static final class ShortCache extends Cache {
     ShortCache(FieldCacheImpl wrapper) {
       super(wrapper);
     }
 
     @Override
-    protected Object createValue(AtomicReader reader, Entry entryKey, boolean setDocsWithField)
+    protected Object createValue(AtomicReader reader, CacheKey key, boolean setDocsWithField)
         throws IOException {
-      String field = entryKey.field;
-      ShortParser parser = (ShortParser) entryKey.custom;
+
+      int maxDoc = reader.maxDoc();
+      final short[] values;
+      final ShortParser parser = (ShortParser) key.custom;
       if (parser == null) {
-        return wrapper.getShorts(reader, field, FieldCache.DEFAULT_SHORT_PARSER, setDocsWithField);
+        // Confusing: must delegate to wrapper (vs simply
+        // setting parser = DEFAULT_SHORT_PARSER) so cache
+        // key includes DEFAULT_SHORT_PARSER:
+        return wrapper.getShorts(reader, key.field, DEFAULT_SHORT_PARSER, setDocsWithField);
       }
-      final int maxDoc = reader.maxDoc();
-      final short[] retArray = new short[maxDoc];
-      Terms terms = reader.terms(field);
-      FixedBitSet docsWithField = null;
-      if (terms != null) {
-        if (setDocsWithField) {
-          final int termsDocCount = terms.getDocCount();
-          assert termsDocCount <= maxDoc;
-          if (termsDocCount == maxDoc) {
-            // Fast case: all docs have this field:
-            wrapper.setDocsWithField(reader, field, new Bits.MatchAllBits(maxDoc));
-            setDocsWithField = false;
+
+      values = new short[maxDoc];
+      Uninvert u = new Uninvert() {
+          private short currentValue;
+
+          @Override
+          public void visitTerm(BytesRef term) {
+            currentValue = parser.parseShort(term);
           }
-        }
-        final TermsEnum termsEnum = parser.termsEnum(terms);
-        assert termsEnum != null : "TermsEnum must not be null";
-        DocsEnum docs = null;
-        while(true) {
-          final BytesRef term = termsEnum.next();
-          if (term == null) {
-            break;
+
+          @Override
+          public void visitDoc(int docID) {
+            values[docID] = currentValue;
           }
-          final short termval = parser.parseShort(term);
-          docs = termsEnum.docs(null, docs, DocsEnum.FLAG_NONE);
-          while (true) {
-            final int docID = docs.nextDoc();
-            if (docID == DocIdSetIterator.NO_MORE_DOCS) {
-              break;
-            }
-            retArray[docID] = termval;
-            if (setDocsWithField) {
-              if (docsWithField == null) {
-                // Lazy init
-                docsWithField = new FixedBitSet(maxDoc);
-              }
-              docsWithField.set(docID);
-            }
+          
+          @Override
+          protected TermsEnum termsEnum(Terms terms) throws IOException {
+            return parser.termsEnum(terms);
           }
-        }
-      }
+        };
+
+      u.uninvert(reader, key.field, setDocsWithField);
+
       if (setDocsWithField) {
-        wrapper.setDocsWithField(reader, field, docsWithField);
+        wrapper.setDocsWithField(reader, key.field, u.docsWithField);
       }
-      return retArray;
+      return new ShortsFromArray(values);
     }
   }
 
-  // null Bits means no docs matched
-  void setDocsWithField(AtomicReader reader, String field, Bits docsWithField) {
-    final int maxDoc = reader.maxDoc();
-    final Bits bits;
-    if (docsWithField == null) {
-      bits = new Bits.MatchNoBits(maxDoc);
-    } else if (docsWithField instanceof FixedBitSet) {
-      final int numSet = ((FixedBitSet) docsWithField).cardinality();
-      if (numSet >= maxDoc) {
-        // The cardinality of the BitSet is maxDoc if all documents have a value.
-        assert numSet == maxDoc;
-        bits = new Bits.MatchAllBits(maxDoc);
-      } else {
-        bits = docsWithField;
-      }
-    } else {
-      bits = docsWithField;
-    }
-    caches.get(DocsWithFieldCache.class).put(reader, new Entry(field, null), bits);
-  }
-  
   // inherit javadocs
-  @Override
-  public int[] getInts (AtomicReader reader, String field, boolean setDocsWithField) throws IOException {
+  public Ints getInts (AtomicReader reader, String field, boolean setDocsWithField) throws IOException {
     return getInts(reader, field, null, setDocsWithField);
   }
 
   // inherit javadocs
-  @Override
-  public int[] getInts(AtomicReader reader, String field, IntParser parser, boolean setDocsWithField)
+  public Ints getInts(AtomicReader reader, String field, IntParser parser, boolean setDocsWithField)
       throws IOException {
-    return (int[]) caches.get(Integer.TYPE).get(reader, new Entry(field, parser), setDocsWithField);
+    final NumericDocValues valuesIn = reader.getNumericDocValues(field);
+    if (valuesIn != null) {
+      // Not cached here by FieldCacheImpl (cached instead
+      // per-thread by SegmentReader):
+      return new Ints() {
+        @Override
+        public int get(int docID) {
+          return (int) valuesIn.get(docID);
+        }
+      };
+    } else {
+      return (Ints) caches.get(Integer.TYPE).get(reader, new CacheKey(field, parser), setDocsWithField);
+    }
   }
 
+  static class IntsFromArray extends Ints {
+    private final int[] values;
+
+    public IntsFromArray(int[] values) {
+      this.values = values;
+    }
+    
+    @Override
+    public int get(int docID) {
+      return values[docID];
+    }
+  }
+
+  private static class HoldsOneThing<T> {
+    private T it;
+
+    public void set(T it) {
+      this.it = it;
+    }
+
+    public T get() {
+      return it;
+    }
+  }
+
   static final class IntCache extends Cache {
     IntCache(FieldCacheImpl wrapper) {
       super(wrapper);
     }
 
     @Override
-    protected Object createValue(AtomicReader reader, Entry entryKey, boolean setDocsWithField)
+    protected Object createValue(final AtomicReader reader, CacheKey key, boolean setDocsWithField)
         throws IOException {
-      String field = entryKey.field;
-      IntParser parser = (IntParser) entryKey.custom;
+
+      final IntParser parser = (IntParser) key.custom;
       if (parser == null) {
+        // Confusing: must delegate to wrapper (vs simply
+        // setting parser =
+        // DEFAULT_INT_PARSER/NUMERIC_UTILS_INT_PARSER) so
+        // cache key includes
+        // DEFAULT_INT_PARSER/NUMERIC_UTILS_INT_PARSER:
         try {
-          return wrapper.getInts(reader, field, DEFAULT_INT_PARSER, setDocsWithField);
+          return wrapper.getInts(reader, key.field, DEFAULT_INT_PARSER, setDocsWithField);
         } catch (NumberFormatException ne) {
-          return wrapper.getInts(reader, field, NUMERIC_UTILS_INT_PARSER, setDocsWithField);
+          return wrapper.getInts(reader, key.field, NUMERIC_UTILS_INT_PARSER, setDocsWithField);
         }
       }
-      final int maxDoc = reader.maxDoc();
-      int[] retArray = null;
 
-      Terms terms = reader.terms(field);
-      FixedBitSet docsWithField = null;
-      if (terms != null) {
-        if (setDocsWithField) {
-          final int termsDocCount = terms.getDocCount();
-          assert termsDocCount <= maxDoc;
-          if (termsDocCount == maxDoc) {
-            // Fast case: all docs have this field:
-            wrapper.setDocsWithField(reader, field, new Bits.MatchAllBits(maxDoc));
-            setDocsWithField = false;
+      final HoldsOneThing<int[]> valuesRef = new HoldsOneThing<int[]>();
+
+      Uninvert u = new Uninvert() {
+          private int currentValue;
+          private int[] values;
+
+          @Override
+          public void visitTerm(BytesRef term) {
+            currentValue = parser.parseInt(term);
+            if (values == null) {
+              // Lazy alloc so for the numeric field case
+              // (which will hit a NumberFormatException
+              // when we first try the DEFAULT_INT_PARSER),
+              // we don't double-alloc:
+              values = new int[reader.maxDoc()];
+              valuesRef.set(values);
+            }
           }
-        }
-        final TermsEnum termsEnum = parser.termsEnum(terms);
-        assert termsEnum != null : "TermsEnum must not be null";
-        DocsEnum docs = null;
-        while(true) {
-          final BytesRef term = termsEnum.next();
-          if (term == null) {
-            break;
+
+          @Override
+          public void visitDoc(int docID) {
+            values[docID] = currentValue;
           }
-          final int termval = parser.parseInt(term);
-          if (retArray == null) {
-            // late init so numeric fields don't double allocate
-            retArray = new int[maxDoc];
+          
+          @Override
+          protected TermsEnum termsEnum(Terms terms) throws IOException {
+            return parser.termsEnum(terms);
           }
+        };
 
-          docs = termsEnum.docs(null, docs, DocsEnum.FLAG_NONE);
-          while (true) {
-            final int docID = docs.nextDoc();
-            if (docID == DocIdSetIterator.NO_MORE_DOCS) {
-              break;
-            }
-            retArray[docID] = termval;
-            if (setDocsWithField) {
-              if (docsWithField == null) {
-                // Lazy init
-                docsWithField = new FixedBitSet(maxDoc);
-              }
-              docsWithField.set(docID);
-            }
-          }
-        }
-      }
+      u.uninvert(reader, key.field, setDocsWithField);
 
-      if (retArray == null) {
-        // no values
-        retArray = new int[maxDoc];
-      }
       if (setDocsWithField) {
-        wrapper.setDocsWithField(reader, field, docsWithField);
+        wrapper.setDocsWithField(reader, key.field, u.docsWithField);
       }
-      return retArray;
+      int[] values = valuesRef.get();
+      if (values == null) {
+        values = new int[reader.maxDoc()];
+      }
+      return new IntsFromArray(values);
     }
   }
-  
-  @Override
+
   public Bits getDocsWithField(AtomicReader reader, String field)
       throws IOException {
-    return (Bits) caches.get(DocsWithFieldCache.class).get(reader, new Entry(field, null), false);
+    return (Bits) caches.get(DocsWithFieldCache.class).get(reader, new CacheKey(field, null), false);
   }
 
   static final class DocsWithFieldCache extends Cache {
@@ -580,12 +648,23 @@
     }
     
     @Override
-      protected Object createValue(AtomicReader reader, Entry entryKey, boolean setDocsWithField /* ignored */)
+    protected Object createValue(AtomicReader reader, CacheKey key, boolean setDocsWithField /* ignored */)
     throws IOException {
-      final String field = entryKey.field;      
+      final String field = key.field;
+      final FieldInfo fieldInfo = reader.getFieldInfos().fieldInfo(field);
+      final int maxDoc = reader.maxDoc();
+
+      if (fieldInfo == null) {
+        // field does not exist or has no value
+        return new Bits.MatchNoBits(maxDoc);
+      } else if (fieldInfo.hasDocValues()) {
+        // doc values are dense
+        return new Bits.MatchAllBits(maxDoc);
+      }
+
+      // Visit all docs that have terms for this field
       FixedBitSet res = null;
       Terms terms = reader.terms(field);
-      final int maxDoc = reader.maxDoc();
       if (terms != null) {
         final int termsDocCount = terms.getDocCount();
         assert termsDocCount <= maxDoc;
@@ -630,18 +709,40 @@
   }
 
   // inherit javadocs
-  @Override
-  public float[] getFloats (AtomicReader reader, String field, boolean setDocsWithField)
+  public Floats getFloats (AtomicReader reader, String field, boolean setDocsWithField)
     throws IOException {
     return getFloats(reader, field, null, setDocsWithField);
   }
 
   // inherit javadocs
-  @Override
-  public float[] getFloats(AtomicReader reader, String field, FloatParser parser, boolean setDocsWithField)
+  public Floats getFloats(AtomicReader reader, String field, FloatParser parser, boolean setDocsWithField)
     throws IOException {
+    final NumericDocValues valuesIn = reader.getNumericDocValues(field);
+    if (valuesIn != null) {
+      // Not cached here by FieldCacheImpl (cached instead
+      // per-thread by SegmentReader):
+      return new Floats() {
+        @Override
+        public float get(int docID) {
+          return Float.intBitsToFloat((int) valuesIn.get(docID));
+        }
+      };
+    } else {
+      return (Floats) caches.get(Float.TYPE).get(reader, new CacheKey(field, parser), setDocsWithField);
+    }
+  }
 
-    return (float[]) caches.get(Float.TYPE).get(reader, new Entry(field, parser), setDocsWithField);
+  static class FloatsFromArray extends Floats {
+    private final float[] values;
+
+    public FloatsFromArray(float[] values) {
+      this.values = values;
+    }
+    
+    @Override
+    public float get(int docID) {
+      return values[docID];
+    }
   }
 
   static final class FloatCache extends Cache {
@@ -650,257 +751,279 @@
     }
 
     @Override
-    protected Object createValue(AtomicReader reader, Entry entryKey, boolean setDocsWithField)
+    protected Object createValue(final AtomicReader reader, CacheKey key, boolean setDocsWithField)
         throws IOException {
-      String field = entryKey.field;
-      FloatParser parser = (FloatParser) entryKey.custom;
+
+      final FloatParser parser = (FloatParser) key.custom;
       if (parser == null) {
+        // Confusing: must delegate to wrapper (vs simply
+        // setting parser =
+        // DEFAULT_FLOAT_PARSER/NUMERIC_UTILS_FLOAT_PARSER) so
+        // cache key includes
+        // DEFAULT_FLOAT_PARSER/NUMERIC_UTILS_FLOAT_PARSER:
         try {
-          return wrapper.getFloats(reader, field, DEFAULT_FLOAT_PARSER, setDocsWithField);
+          return wrapper.getFloats(reader, key.field, DEFAULT_FLOAT_PARSER, setDocsWithField);
         } catch (NumberFormatException ne) {
-          return wrapper.getFloats(reader, field, NUMERIC_UTILS_FLOAT_PARSER, setDocsWithField);
+          return wrapper.getFloats(reader, key.field, NUMERIC_UTILS_FLOAT_PARSER, setDocsWithField);
         }
       }
-      final int maxDoc = reader.maxDoc();
-      float[] retArray = null;
 
-      Terms terms = reader.terms(field);
-      FixedBitSet docsWithField = null;
-      if (terms != null) {
-        if (setDocsWithField) {
-          final int termsDocCount = terms.getDocCount();
-          assert termsDocCount <= maxDoc;
-          if (termsDocCount == maxDoc) {
-            // Fast case: all docs have this field:
-            wrapper.setDocsWithField(reader, field, new Bits.MatchAllBits(maxDoc));
-            setDocsWithField = false;
+      final HoldsOneThing<float[]> valuesRef = new HoldsOneThing<float[]>();
+
+      Uninvert u = new Uninvert() {
+          private float currentValue;
+          private float[] values;
+
+          @Override
+          public void visitTerm(BytesRef term) {
+            currentValue = parser.parseFloat(term);
+            if (values == null) {
+              // Lazy alloc so for the numeric field case
+              // (which will hit a NumberFormatException
+              // when we first try the DEFAULT_INT_PARSER),
+              // we don't double-alloc:
+              values = new float[reader.maxDoc()];
+              valuesRef.set(values);
+            }
           }
-        }
-        final TermsEnum termsEnum = parser.termsEnum(terms);
-        assert termsEnum != null : "TermsEnum must not be null";
-        DocsEnum docs = null;
-        while(true) {
-          final BytesRef term = termsEnum.next();
-          if (term == null) {
-            break;
+
+          @Override
+          public void visitDoc(int docID) {
+            values[docID] = currentValue;
           }
-          final float termval = parser.parseFloat(term);
-          if (retArray == null) {
-            // late init so numeric fields don't double allocate
-            retArray = new float[maxDoc];
-          }
           
-          docs = termsEnum.docs(null, docs, DocsEnum.FLAG_NONE);
-          while (true) {
-            final int docID = docs.nextDoc();
-            if (docID == DocIdSetIterator.NO_MORE_DOCS) {
-              break;
-            }
-            retArray[docID] = termval;
-            if (setDocsWithField) {
-              if (docsWithField == null) {
-                // Lazy init
-                docsWithField = new FixedBitSet(maxDoc);
-              }
-              docsWithField.set(docID);
-            }
+          @Override
+          protected TermsEnum termsEnum(Terms terms) throws IOException {
+            return parser.termsEnum(terms);
           }
-        }
+        };
+
+      u.uninvert(reader, key.field, setDocsWithField);
+
+      if (setDocsWithField) {
+        wrapper.setDocsWithField(reader, key.field, u.docsWithField);
       }
 
-      if (retArray == null) {
-        // no values
-        retArray = new float[maxDoc];
+      float[] values = valuesRef.get();
+      if (values == null) {
+        values = new float[reader.maxDoc()];
       }
-      if (setDocsWithField) {
-        wrapper.setDocsWithField(reader, field, docsWithField);
-      }
-      return retArray;
+      return new FloatsFromArray(values);
     }
   }
 
-
-  @Override
-  public long[] getLongs(AtomicReader reader, String field, boolean setDocsWithField) throws IOException {
+  // inherit javadocs
+  public Longs getLongs(AtomicReader reader, String field, boolean setDocsWithField) throws IOException {
     return getLongs(reader, field, null, setDocsWithField);
   }
   
   // inherit javadocs
-  @Override
-  public long[] getLongs(AtomicReader reader, String field, FieldCache.LongParser parser, boolean setDocsWithField)
+  public Longs getLongs(AtomicReader reader, String field, FieldCache.LongParser parser, boolean setDocsWithField)
       throws IOException {
-    return (long[]) caches.get(Long.TYPE).get(reader, new Entry(field, parser), setDocsWithField);
+    final NumericDocValues valuesIn = reader.getNumericDocValues(field);
+    if (valuesIn != null) {
+      // Not cached here by FieldCacheImpl (cached instead
+      // per-thread by SegmentReader):
+      return new Longs() {
+        @Override
+        public long get(int docID) {
+          return valuesIn.get(docID);
+        }
+      };
+    } else {
+      return (Longs) caches.get(Long.TYPE).get(reader, new CacheKey(field, parser), setDocsWithField);
+    }
   }
 
+  static class LongsFromArray extends Longs {
+    private final long[] values;
+
+    public LongsFromArray(long[] values) {
+      this.values = values;
+    }
+    
+    @Override
+    public long get(int docID) {
+      return values[docID];
+    }
+  }
+
   static final class LongCache extends Cache {
     LongCache(FieldCacheImpl wrapper) {
       super(wrapper);
     }
 
     @Override
-    protected Object createValue(AtomicReader reader, Entry entryKey, boolean setDocsWithField)
+    protected Object createValue(final AtomicReader reader, CacheKey key, boolean setDocsWithField)
         throws IOException {
-      String field = entryKey.field;
-      FieldCache.LongParser parser = (FieldCache.LongParser) entryKey.custom;
+
+      final LongParser parser = (LongParser) key.custom;
       if (parser == null) {
+        // Confusing: must delegate to wrapper (vs simply
+        // setting parser =
+        // DEFAULT_LONG_PARSER/NUMERIC_UTILS_LONG_PARSER) so
+        // cache key includes
+        // DEFAULT_LONG_PARSER/NUMERIC_UTILS_LONG_PARSER:
         try {
-          return wrapper.getLongs(reader, field, DEFAULT_LONG_PARSER, setDocsWithField);
+          return wrapper.getLongs(reader, key.field, DEFAULT_LONG_PARSER, setDocsWithField);
         } catch (NumberFormatException ne) {
-          return wrapper.getLongs(reader, field, NUMERIC_UTILS_LONG_PARSER, setDocsWithField);
+          return wrapper.getLongs(reader, key.field, NUMERIC_UTILS_LONG_PARSER, setDocsWithField);
         }
       }
-      final int maxDoc = reader.maxDoc();
-      long[] retArray = null;
 
-      Terms terms = reader.terms(field);
-      FixedBitSet docsWithField = null;
-      if (terms != null) {
-        if (setDocsWithField) {
-          final int termsDocCount = terms.getDocCount();
-          assert termsDocCount <= maxDoc;
-          if (termsDocCount == maxDoc) {
-            // Fast case: all docs have this field:
-            wrapper.setDocsWithField(reader, field, new Bits.MatchAllBits(maxDoc));
-            setDocsWithField = false;
+      final HoldsOneThing<long[]> valuesRef = new HoldsOneThing<long[]>();
+
+      Uninvert u = new Uninvert() {
+          private long currentValue;
+          private long[] values;
+
+          @Override
+          public void visitTerm(BytesRef term) {
+            currentValue = parser.parseLong(term);
+            if (values == null) {
+              // Lazy alloc so for the numeric field case
+              // (which will hit a NumberFormatException
+              // when we first try the DEFAULT_INT_PARSER),
+              // we don't double-alloc:
+              values = new long[reader.maxDoc()];
+              valuesRef.set(values);
+            }
           }
-        }
-        final TermsEnum termsEnum = parser.termsEnum(terms);
-        assert termsEnum != null : "TermsEnum must not be null";
-        DocsEnum docs = null;
-        while(true) {
-          final BytesRef term = termsEnum.next();
-          if (term == null) {
-            break;
+
+          @Override
+          public void visitDoc(int docID) {
+            values[docID] = currentValue;
           }
-          final long termval = parser.parseLong(term);
-          if (retArray == null) {
-            // late init so numeric fields don't double allocate
-            retArray = new long[maxDoc];
+          
+          @Override
+          protected TermsEnum termsEnum(Terms terms) throws IOException {
+            return parser.termsEnum(terms);
           }
+        };
 
-          docs = termsEnum.docs(null, docs, DocsEnum.FLAG_NONE);
-          while (true) {
-            final int docID = docs.nextDoc();
-            if (docID == DocIdSetIterator.NO_MORE_DOCS) {
-              break;
-            }
-            retArray[docID] = termval;
-            if (setDocsWithField) {
-              if (docsWithField == null) {
-                // Lazy init
-                docsWithField = new FixedBitSet(maxDoc);
-              }
-              docsWithField.set(docID);
-            }
-          }
-        }
-      }
+      u.uninvert(reader, key.field, setDocsWithField);
 
-      if (retArray == null) {
-        // no values
-        retArray = new long[maxDoc];
-      }
       if (setDocsWithField) {
-        wrapper.setDocsWithField(reader, field, docsWithField);
+        wrapper.setDocsWithField(reader, key.field, u.docsWithField);
       }
-      return retArray;
+      long[] values = valuesRef.get();
+      if (values == null) {
+        values = new long[reader.maxDoc()];
+      }
+      return new LongsFromArray(values);
     }
   }
 
   // inherit javadocs
-  @Override
-  public double[] getDoubles(AtomicReader reader, String field, boolean setDocsWithField)
+  public Doubles getDoubles(AtomicReader reader, String field, boolean setDocsWithField)
     throws IOException {
     return getDoubles(reader, field, null, setDocsWithField);
   }
 
   // inherit javadocs
-  @Override
-  public double[] getDoubles(AtomicReader reader, String field, FieldCache.DoubleParser parser, boolean setDocsWithField)
+  public Doubles getDoubles(AtomicReader reader, String field, FieldCache.DoubleParser parser, boolean setDocsWithField)
       throws IOException {
-    return (double[]) caches.get(Double.TYPE).get(reader, new Entry(field, parser), setDocsWithField);
+    final NumericDocValues valuesIn = reader.getNumericDocValues(field);
+    if (valuesIn != null) {
+      // Not cached here by FieldCacheImpl (cached instead
+      // per-thread by SegmentReader):
+      return new Doubles() {
+        @Override
+        public double get(int docID) {
+          return Double.longBitsToDouble(valuesIn.get(docID));
+        }
+      };
+    } else {
+      return (Doubles) caches.get(Double.TYPE).get(reader, new CacheKey(field, parser), setDocsWithField);
+    }
   }
 
+  static class DoublesFromArray extends Doubles {
+    private final double[] values;
+
+    public DoublesFromArray(double[] values) {
+      this.values = values;
+    }
+    
+    @Override
+    public double get(int docID) {
+      return values[docID];
+    }
+  }
+
   static final class DoubleCache extends Cache {
     DoubleCache(FieldCacheImpl wrapper) {
       super(wrapper);
     }
 
     @Override
-    protected Object createValue(AtomicReader reader, Entry entryKey, boolean setDocsWithField)
+    protected Object createValue(final AtomicReader reader, CacheKey key, boolean setDocsWithField)
         throws IOException {
-      String field = entryKey.field;
-      FieldCache.DoubleParser parser = (FieldCache.DoubleParser) entryKey.custom;
+
+      final DoubleParser parser = (DoubleParser) key.custom;
       if (parser == null) {
+        // Confusing: must delegate to wrapper (vs simply
+        // setting parser =
+        // DEFAULT_DOUBLE_PARSER/NUMERIC_UTILS_DOUBLE_PARSER) so
+        // cache key includes
+        // DEFAULT_DOUBLE_PARSER/NUMERIC_UTILS_DOUBLE_PARSER:
         try {
-          return wrapper.getDoubles(reader, field, DEFAULT_DOUBLE_PARSER, setDocsWithField);
+          return wrapper.getDoubles(reader, key.field, DEFAULT_DOUBLE_PARSER, setDocsWithField);
         } catch (NumberFormatException ne) {
-          return wrapper.getDoubles(reader, field, NUMERIC_UTILS_DOUBLE_PARSER, setDocsWithField);
+          return wrapper.getDoubles(reader, key.field, NUMERIC_UTILS_DOUBLE_PARSER, setDocsWithField);
         }
       }
-      final int maxDoc = reader.maxDoc();
-      double[] retArray = null;
 
-      Terms terms = reader.terms(field);
-      FixedBitSet docsWithField = null;
-      if (terms != null) {
-        if (setDocsWithField) {
-          final int termsDocCount = terms.getDocCount();
-          assert termsDocCount <= maxDoc;
-          if (termsDocCount == maxDoc) {
-            // Fast case: all docs have this field:
-            wrapper.setDocsWithField(reader, field, new Bits.MatchAllBits(maxDoc));
-            setDocsWithField = false;
+      final HoldsOneThing<double[]> valuesRef = new HoldsOneThing<double[]>();
+
+      Uninvert u = new Uninvert() {
+          private double currentValue;
+          private double[] values;
+
+          @Override
+          public void visitTerm(BytesRef term) {
+            currentValue = parser.parseDouble(term);
+            if (values == null) {
+              // Lazy alloc so for the numeric field case
+              // (which will hit a NumberFormatException
+              // when we first try the DEFAULT_INT_PARSER),
+              // we don't double-alloc:
+              values = new double[reader.maxDoc()];
+              valuesRef.set(values);
+            }
           }
-        }
-        final TermsEnum termsEnum = parser.termsEnum(terms);
-        assert termsEnum != null : "TermsEnum must not be null";
-        DocsEnum docs = null;
-        while(true) {
-          final BytesRef term = termsEnum.next();
-          if (term == null) {
-            break;
+
+          @Override
+          public void visitDoc(int docID) {
+            values[docID] = currentValue;
           }
-          final double termval = parser.parseDouble(term);
-          if (retArray == null) {
-            // late init so numeric fields don't double allocate
-            retArray = new double[maxDoc];
+          
+          @Override
+          protected TermsEnum termsEnum(Terms terms) throws IOException {
+            return parser.termsEnum(terms);
           }
+        };
 
-          docs = termsEnum.docs(null, docs, DocsEnum.FLAG_NONE);
-          while (true) {
-            final int docID = docs.nextDoc();
-            if (docID == DocIdSetIterator.NO_MORE_DOCS) {
-              break;
-            }
-            retArray[docID] = termval;
-            if (setDocsWithField) {
-              if (docsWithField == null) {
-                // Lazy init
-                docsWithField = new FixedBitSet(maxDoc);
-              }
-              docsWithField.set(docID);
-            }
-          }
-        }
+      u.uninvert(reader, key.field, setDocsWithField);
+
+      if (setDocsWithField) {
+        wrapper.setDocsWithField(reader, key.field, u.docsWithField);
       }
-      if (retArray == null) { // no values
-        retArray = new double[maxDoc];
+      double[] values = valuesRef.get();
+      if (values == null) {
+        values = new double[reader.maxDoc()];
       }
-      if (setDocsWithField) {
-        wrapper.setDocsWithField(reader, field, docsWithField);
-      }
-      return retArray;
+      return new DoublesFromArray(values);
     }
   }
 
-  public static class DocTermsIndexImpl extends DocTermsIndex {
+  public static class SortedDocValuesImpl extends SortedDocValues {
     private final PagedBytes.Reader bytes;
     private final PackedInts.Reader termOrdToBytesOffset;
     private final PackedInts.Reader docToTermOrd;
     private final int numOrd;
 
-    public DocTermsIndexImpl(PagedBytes.Reader bytes, PackedInts.Reader termOrdToBytesOffset, PackedInts.Reader docToTermOrd, int numOrd) {
+    public SortedDocValuesImpl(PagedBytes.Reader bytes, PackedInts.Reader termOrdToBytesOffset, PackedInts.Reader docToTermOrd, int numOrd) {
       this.bytes = bytes;
       this.docToTermOrd = docToTermOrd;
       this.termOrdToBytesOffset = termOrdToBytesOffset;
@@ -908,197 +1031,70 @@
     }
 
     @Override
-    public PackedInts.Reader getDocToOrd() {
-      return docToTermOrd;
-    }
-
-    @Override
-    public int numOrd() {
+    public int getValueCount() {
       return numOrd;
     }
 
     @Override
     public int getOrd(int docID) {
-      return (int) docToTermOrd.get(docID);
+      // Subtract 1, matching the 1+ord we did when
+      // storing, so that missing values, which are 0 in the
+      // packed ints, are returned as -1 ord:
+      return (int) docToTermOrd.get(docID)-1;
     }
 
     @Override
-    public int size() {
-      return docToTermOrd.size();
-    }
-
-    @Override
-    public BytesRef lookup(int ord, BytesRef ret) {
-      return bytes.fill(ret, termOrdToBytesOffset.get(ord));
-    }
-
-    @Override
-    public TermsEnum getTermsEnum() {
-      return this.new DocTermsIndexEnum();
-    }
-
-    class DocTermsIndexEnum extends TermsEnum {
-      int currentOrd;
-      int currentBlockNumber;
-      int end;  // end position in the current block
-      final byte[][] blocks;
-      final int[] blockEnds;
-
-      final BytesRef term = new BytesRef();
-
-      public DocTermsIndexEnum() {
-        currentOrd = 0;
-        currentBlockNumber = 0;
-        blocks = bytes.getBlocks();
-        blockEnds = bytes.getBlockEnds();
-        currentBlockNumber = bytes.fillAndGetIndex(term, termOrdToBytesOffset.get(0));
-        end = blockEnds[currentBlockNumber];
+    public void lookupOrd(int ord, BytesRef ret) {
+      if (ord < 0) {
+        throw new IllegalArgumentException("ord must be >=0 (got ord=" + ord + ")");
       }
-
-      @Override
-      public SeekStatus seekCeil(BytesRef text, boolean useCache /* ignored */) throws IOException {
-        int low = 1;
-        int high = numOrd-1;
-        
-        while (low <= high) {
-          int mid = (low + high) >>> 1;
-          seekExact(mid);
-          int cmp = term.compareTo(text);
-
-          if (cmp < 0)
-            low = mid + 1;
-          else if (cmp > 0)
-            high = mid - 1;
-          else
-            return SeekStatus.FOUND; // key found
-        }
-        
-        if (low == numOrd) {
-          return SeekStatus.END;
-        } else {
-          seekExact(low);
-          return SeekStatus.NOT_FOUND;
-        }
-      }
-
-      @Override
-      public void seekExact(long ord) throws IOException {
-        assert(ord >= 0 && ord <= numOrd);
-        // TODO: if gap is small, could iterate from current position?  Or let user decide that?
-        currentBlockNumber = bytes.fillAndGetIndex(term, termOrdToBytesOffset.get((int)ord));
-        end = blockEnds[currentBlockNumber];
-        currentOrd = (int)ord;
-      }
-
-      @Override
-      public BytesRef next() throws IOException {
-        int start = term.offset + term.length;
-        if (start >= end) {
-          // switch byte blocks
-          if (currentBlockNumber +1 >= blocks.length) {
-            return null;
-          }
-          currentBlockNumber++;
-          term.bytes = blocks[currentBlockNumber];
-          end = blockEnds[currentBlockNumber];
-          start = 0;
-          if (end<=0) return null;  // special case of empty last array
-        }
-
-        currentOrd++;
-
-        byte[] block = term.bytes;
-        if ((block[start] & 128) == 0) {
-          term.length = block[start];
-          term.offset = start+1;
-        } else {
-          term.length = (((block[start] & 0x7f)) << 8) | (block[1+start] & 0xff);
-          term.offset = start+2;
-        }
-
-        return term;
-      }
-
-      @Override
-      public BytesRef term() throws IOException {
-        return term;
-      }
-
-      @Override
-      public long ord() throws IOException {
-        return currentOrd;
-      }
-
-      @Override
-      public int docFreq() {
-        throw new UnsupportedOperationException();
-      }
-
-      @Override
-      public long totalTermFreq() {
-        return -1;
-      }
-
-      @Override
-      public DocsEnum docs(Bits liveDocs, DocsEnum reuse, int flags) throws IOException {
-        throw new UnsupportedOperationException();
-      }
-
-      @Override
-      public DocsAndPositionsEnum docsAndPositions(Bits liveDocs, DocsAndPositionsEnum reuse, int flags) throws IOException {
-        throw new UnsupportedOperationException();
-      }
-
-      @Override
-      public Comparator<BytesRef> getComparator() {
-        return BytesRef.getUTF8SortedAsUnicodeComparator();
-      }
-
-      @Override
-      public void seekExact(BytesRef term, TermState state) throws IOException {
-        assert state != null && state instanceof OrdTermState;
-        this.seekExact(((OrdTermState)state).ord);
-      }
-
-      @Override
-      public TermState termState() throws IOException {
-        OrdTermState state = new OrdTermState();
-        state.ord = currentOrd;
-        return state;
-      }
+      bytes.fill(ret, termOrdToBytesOffset.get(ord));
     }
   }
 
-  @Override
-  public DocTermsIndex getTermsIndex(AtomicReader reader, String field) throws IOException {
+  public SortedDocValues getTermsIndex(AtomicReader reader, String field) throws IOException {
     return getTermsIndex(reader, field, PackedInts.FAST);
   }
 
-  @Override
-  public DocTermsIndex getTermsIndex(AtomicReader reader, String field, float acceptableOverheadRatio) throws IOException {
-    return (DocTermsIndex) caches.get(DocTermsIndex.class).get(reader, new Entry(field, acceptableOverheadRatio), false);
+  public SortedDocValues getTermsIndex(AtomicReader reader, String field, float acceptableOverheadRatio) throws IOException {
+    SortedDocValues valuesIn = reader.getSortedDocValues(field);
+    if (valuesIn != null) {
+      // Not cached here by FieldCacheImpl (cached instead
+      // per-thread by SegmentReader):
+      return valuesIn;
+    } else {
+      final FieldInfo info = reader.getFieldInfos().fieldInfo(field);
+      if (info != null && !info.isIndexed() && info.hasDocValues()) {
+        // we don't try to build a sorted instance from numeric/binary doc
+        // values because dedup can be very costly
+        throw new IllegalArgumentException("Cannot get terms index for \"" + field
+            + "\": it isn't indexed and doesn't have sorted doc values");
+      }
+      return (SortedDocValues) caches.get(SortedDocValues.class).get(reader, new CacheKey(field, acceptableOverheadRatio), false);
+    }
   }
 
-  static class DocTermsIndexCache extends Cache {
-    DocTermsIndexCache(FieldCacheImpl wrapper) {
+  static class SortedDocValuesCache extends Cache {
+    SortedDocValuesCache(FieldCacheImpl wrapper) {
       super(wrapper);
     }
 
     @Override
-    protected Object createValue(AtomicReader reader, Entry entryKey, boolean setDocsWithField /* ignored */)
+    protected Object createValue(AtomicReader reader, CacheKey key, boolean setDocsWithField /* ignored */)
         throws IOException {
 
-      Terms terms = reader.terms(entryKey.field);
+      final int maxDoc = reader.maxDoc();
 
-      final float acceptableOverheadRatio = ((Float) entryKey.custom).floatValue();
+      Terms terms = reader.terms(key.field);
 
+      final float acceptableOverheadRatio = ((Float) key.custom).floatValue();
+
       final PagedBytes bytes = new PagedBytes(15);
 
       int startBytesBPV;
       int startTermsBPV;
       int startNumUniqueTerms;
 
-      int maxDoc = reader.maxDoc();
       final int termCountHardLimit;
       if (maxDoc == Integer.MAX_VALUE) {
         termCountHardLimit = Integer.MAX_VALUE;
@@ -1106,6 +1102,7 @@
         termCountHardLimit = maxDoc+1;
       }
 
+      // TODO: use Uninvert?
       if (terms != null) {
         // Try for coarse estimate for number of bits; this
         // should be an underestimate most of the time, which
@@ -1137,10 +1134,10 @@
       GrowableWriter termOrdToBytesOffset = new GrowableWriter(startBytesBPV, 1+startNumUniqueTerms, acceptableOverheadRatio);
       final GrowableWriter docToTermOrd = new GrowableWriter(startTermsBPV, maxDoc, acceptableOverheadRatio);
 
-      // 0 is reserved for "unset"
-      bytes.copyUsingLengthPrefix(new BytesRef());
-      int termOrd = 1;
+      int termOrd = 0;
 
+      // TODO: use Uninvert?
+
       if (terms != null) {
         final TermsEnum termsEnum = terms.iterator(null);
         DocsEnum docs = null;
@@ -1167,7 +1164,8 @@
             if (docID == DocIdSetIterator.NO_MORE_DOCS) {
               break;
             }
-            docToTermOrd.set(docID, termOrd);
+            // Store 1+ ord into packed bits
+            docToTermOrd.set(docID, 1+termOrd);
           }
           termOrd++;
         }
@@ -1178,63 +1176,73 @@
       }
 
       // maybe an int-only impl?
-      return new DocTermsIndexImpl(bytes.freeze(true), termOrdToBytesOffset.getMutable(), docToTermOrd.getMutable(), termOrd);
+      return new SortedDocValuesImpl(bytes.freeze(true), termOrdToBytesOffset.getMutable(), docToTermOrd.getMutable(), termOrd);
     }
   }
 
-  private static class DocTermsImpl extends DocTerms {
+  private static class BinaryDocValuesImpl extends BinaryDocValues {
     private final PagedBytes.Reader bytes;
     private final PackedInts.Reader docToOffset;
 
-    public DocTermsImpl(PagedBytes.Reader bytes, PackedInts.Reader docToOffset) {
+    public BinaryDocValuesImpl(PagedBytes.Reader bytes, PackedInts.Reader docToOffset) {
       this.bytes = bytes;
       this.docToOffset = docToOffset;
     }
 
     @Override
-    public int size() {
-      return docToOffset.size();
+    public void get(int docID, BytesRef ret) {
+      final int pointer = (int) docToOffset.get(docID);
+      if (pointer == 0) {
+        ret.bytes = MISSING;
+        ret.offset = 0;
+        ret.length = 0;
+      } else {
+        bytes.fill(ret, pointer);
+      }
     }
-
-    @Override
-    public boolean exists(int docID) {
-      return docToOffset.get(docID) == 0;
-    }
-
-    @Override
-    public BytesRef getTerm(int docID, BytesRef ret) {
-      final int pointer = (int) docToOffset.get(docID);
-      return bytes.fill(ret, pointer);
-    }      
   }
 
   // TODO: this if DocTermsIndex was already created, we
   // should share it...
-  @Override
-  public DocTerms getTerms(AtomicReader reader, String field) throws IOException {
+  public BinaryDocValues getTerms(AtomicReader reader, String field) throws IOException {
     return getTerms(reader, field, PackedInts.FAST);
   }
 
-  @Override
-  public DocTerms getTerms(AtomicReader reader, String field, float acceptableOverheadRatio) throws IOException {
-    return (DocTerms) caches.get(DocTerms.class).get(reader, new Entry(field, acceptableOverheadRatio), false);
+  public BinaryDocValues getTerms(AtomicReader reader, String field, float acceptableOverheadRatio) throws IOException {
+    BinaryDocValues valuesIn = reader.getBinaryDocValues(field);
+    if (valuesIn == null) {
+      valuesIn = reader.getSortedDocValues(field);
+    }
+
+    if (valuesIn != null) {
+      // Not cached here by FieldCacheImpl (cached instead
+      // per-thread by SegmentReader):
+      return valuesIn;
+    }
+
+    return (BinaryDocValues) caches.get(BinaryDocValues.class).get(reader, new CacheKey(field, acceptableOverheadRatio), false);
   }
 
-  static final class DocTermsCache extends Cache {
-    DocTermsCache(FieldCacheImpl wrapper) {
+  static final class BinaryDocValuesCache extends Cache {
+    BinaryDocValuesCache(FieldCacheImpl wrapper) {
       super(wrapper);
     }
 
     @Override
-    protected Object createValue(AtomicReader reader, Entry entryKey, boolean setDocsWithField /* ignored */)
+    protected Object createValue(AtomicReader reader, CacheKey key, boolean setDocsWithField /* ignored */)
         throws IOException {
 
-      Terms terms = reader.terms(entryKey.field);
+      // TODO: would be nice to first check if DocTermsIndex
+      // was already cached for this field and then return
+      // that instead, to avoid insanity
 
-      final float acceptableOverheadRatio = ((Float) entryKey.custom).floatValue();
+      final int maxDoc = reader.maxDoc();
+      Terms terms = reader.terms(key.field);
 
-      final int termCountHardLimit = reader.maxDoc();
+      final float acceptableOverheadRatio = ((Float) key.custom).floatValue();
 
+      final int termCountHardLimit = maxDoc;
+
       // Holds the actual term data, expanded.
       final PagedBytes bytes = new PagedBytes(15);
 
@@ -1257,7 +1265,7 @@
         startBPV = 1;
       }
 
-      final GrowableWriter docToOffset = new GrowableWriter(startBPV, reader.maxDoc(), acceptableOverheadRatio);
+      final GrowableWriter docToOffset = new GrowableWriter(startBPV, maxDoc, acceptableOverheadRatio);
       
       // pointer==0 means not set
       bytes.copyUsingLengthPrefix(new BytesRef());
@@ -1291,13 +1299,12 @@
       }
 
       // maybe an int-only impl?
-      return new DocTermsImpl(bytes.freeze(true), docToOffset.getMutable());
+      return new BinaryDocValuesImpl(bytes.freeze(true), docToOffset.getMutable());
     }
   }
 
-  @Override
   public DocTermOrds getDocTermOrds(AtomicReader reader, String field) throws IOException {
-    return (DocTermOrds) caches.get(DocTermOrds.class).get(reader, new Entry(field, null), false);
+    return (DocTermOrds) caches.get(DocTermOrds.class).get(reader, new CacheKey(field, null), false);
   }
 
   static final class DocTermOrdsCache extends Cache {
@@ -1306,20 +1313,19 @@
     }
 
     @Override
-    protected Object createValue(AtomicReader reader, Entry entryKey, boolean setDocsWithField /* ignored */)
+    protected Object createValue(AtomicReader reader, CacheKey key, boolean setDocsWithField /* ignored */)
         throws IOException {
-      return new DocTermOrds(reader, entryKey.field);
+      // No DocValues impl yet (DocValues are single valued...):
+      return new DocTermOrds(reader, key.field);
     }
   }
 
   private volatile PrintStream infoStream;
 
-  @Override
   public void setInfoStream(PrintStream stream) {
     infoStream = stream;
   }
 
-  @Override
   public PrintStream getInfoStream() {
     return infoStream;
   }
Index: lucene/core/src/java/org/apache/lucene/search/SortField.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/search/SortField.java	(revision 1442822)
+++ lucene/core/src/java/org/apache/lucene/search/SortField.java	(working copy)
@@ -257,7 +257,6 @@
   @Override
   public String toString() {
     StringBuilder buffer = new StringBuilder();
-    String dv = useIndexValues ? " [dv]" : "";
     switch (type) {
       case SCORE:
         buffer.append("<score>");
@@ -268,11 +267,11 @@
         break;
 
       case STRING:
-        buffer.append("<string" + dv + ": \"").append(field).append("\">");
+        buffer.append("<string" + ": \"").append(field).append("\">");
         break;
 
       case STRING_VAL:
-        buffer.append("<string_val" + dv + ": \"").append(field).append("\">");
+        buffer.append("<string_val" + ": \"").append(field).append("\">");
         break;
 
       case BYTE:
@@ -284,7 +283,7 @@
         break;
 
       case INT:
-        buffer.append("<int" + dv + ": \"").append(field).append("\">");
+        buffer.append("<int" + ": \"").append(field).append("\">");
         break;
 
       case LONG:
@@ -292,11 +291,11 @@
         break;
 
       case FLOAT:
-        buffer.append("<float" + dv + ": \"").append(field).append("\">");
+        buffer.append("<float" + ": \"").append(field).append("\">");
         break;
 
       case DOUBLE:
-        buffer.append("<double" + dv + ": \"").append(field).append("\">");
+        buffer.append("<double" + ": \"").append(field).append("\">");
         break;
 
       case CUSTOM:
@@ -347,16 +346,6 @@
     return hash;
   }
 
-  private boolean useIndexValues;
-
-  public void setUseIndexValues(boolean b) {
-    useIndexValues = b;
-  }
-
-  public boolean getUseIndexValues() {
-    return useIndexValues;
-  }
-
   private Comparator<BytesRef> bytesComparator = BytesRef.getUTF8SortedAsUnicodeComparator();
 
   public void setBytesComparator(Comparator<BytesRef> b) {
@@ -389,18 +378,10 @@
       return new FieldComparator.DocComparator(numHits);
 
     case INT:
-      if (useIndexValues) {
-        return new FieldComparator.IntDocValuesComparator(numHits, field);
-      } else {
-        return new FieldComparator.IntComparator(numHits, field, parser, (Integer) missingValue);
-      }
+      return new FieldComparator.IntComparator(numHits, field, parser, (Integer) missingValue);
 
     case FLOAT:
-      if (useIndexValues) {
-        return new FieldComparator.FloatDocValuesComparator(numHits, field);
-      } else {
-        return new FieldComparator.FloatComparator(numHits, field, parser, (Float) missingValue);
-      }
+      return new FieldComparator.FloatComparator(numHits, field, parser, (Float) missingValue);
 
     case LONG:
       return new FieldComparator.LongComparator(numHits, field, parser, (Long) missingValue);
@@ -419,18 +400,10 @@
       return comparatorSource.newComparator(field, numHits, sortPos, reverse);
 
     case STRING:
-      if (useIndexValues) {
-        return new FieldComparator.TermOrdValDocValuesComparator(numHits, field);
-      } else {
-        return new FieldComparator.TermOrdValComparator(numHits, field);
-      }
+      return new FieldComparator.TermOrdValComparator(numHits, field);
 
     case STRING_VAL:
-      if (useIndexValues) {
-        return new FieldComparator.TermValDocValuesComparator(numHits, field);
-      } else {
-        return new FieldComparator.TermValComparator(numHits, field);
-      }
+      return new FieldComparator.TermValComparator(numHits, field);
 
     case REWRITEABLE:
       throw new IllegalStateException("SortField needs to be rewritten through Sort.rewrite(..) and SortField.rewrite(..)");
Index: lucene/core/src/java/org/apache/lucene/search/similarities/BM25Similarity.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/search/similarities/BM25Similarity.java	(revision 1442822)
+++ lucene/core/src/java/org/apache/lucene/search/similarities/BM25Similarity.java	(working copy)
@@ -20,9 +20,8 @@
 import java.io.IOException;
 
 import org.apache.lucene.index.AtomicReaderContext;
-import org.apache.lucene.index.DocValues;
 import org.apache.lucene.index.FieldInvertState;
-import org.apache.lucene.index.Norm;
+import org.apache.lucene.index.NumericDocValues;
 import org.apache.lucene.search.CollectionStatistics;
 import org.apache.lucene.search.Explanation;
 import org.apache.lucene.search.TermStatistics;
@@ -136,9 +135,9 @@
 
 
   @Override
-  public final void computeNorm(FieldInvertState state, Norm norm) {
+  public final long computeNorm(FieldInvertState state) {
     final int numTerms = discountOverlaps ? state.getLength() - state.getNumOverlap() : state.getLength();
-    norm.setByte(encodeNormValue(state.getBoost(), numTerms));
+    return encodeNormValue(state.getBoost(), numTerms);
   }
 
   /**
@@ -215,7 +214,7 @@
   @Override
   public final ExactSimScorer exactSimScorer(SimWeight stats, AtomicReaderContext context) throws IOException {
     BM25Stats bm25stats = (BM25Stats) stats;
-    final DocValues norms = context.reader().normValues(bm25stats.field);
+    final NumericDocValues norms = context.reader().getNormValues(bm25stats.field);
     return norms == null 
       ? new ExactBM25DocScorerNoNorms(bm25stats)
       : new ExactBM25DocScorer(bm25stats, norms);
@@ -224,26 +223,26 @@
   @Override
   public final SloppySimScorer sloppySimScorer(SimWeight stats, AtomicReaderContext context) throws IOException {
     BM25Stats bm25stats = (BM25Stats) stats;
-    return new SloppyBM25DocScorer(bm25stats, context.reader().normValues(bm25stats.field));
+    return new SloppyBM25DocScorer(bm25stats, context.reader().getNormValues(bm25stats.field));
   }
   
   private class ExactBM25DocScorer extends ExactSimScorer {
     private final BM25Stats stats;
     private final float weightValue;
-    private final byte[] norms;
+    private final NumericDocValues norms;
     private final float[] cache;
     
-    ExactBM25DocScorer(BM25Stats stats, DocValues norms) throws IOException {
+    ExactBM25DocScorer(BM25Stats stats, NumericDocValues norms) throws IOException {
       assert norms != null;
       this.stats = stats;
       this.weightValue = stats.weight * (k1 + 1); // boost * idf * (k1 + 1)
       this.cache = stats.cache;
-      this.norms = (byte[])norms.getSource().getArray();
+      this.norms = norms;
     }
     
     @Override
     public float score(int doc, int freq) {
-      return weightValue * freq / (freq + cache[norms[doc] & 0xFF]);
+      return weightValue * freq / (freq + cache[(byte)norms.get(doc) & 0xFF]);
     }
     
     @Override
@@ -283,20 +282,20 @@
   private class SloppyBM25DocScorer extends SloppySimScorer {
     private final BM25Stats stats;
     private final float weightValue; // boost * idf * (k1 + 1)
-    private final byte[] norms;
+    private final NumericDocValues norms;
     private final float[] cache;
     
-    SloppyBM25DocScorer(BM25Stats stats, DocValues norms) throws IOException {
+    SloppyBM25DocScorer(BM25Stats stats, NumericDocValues norms) throws IOException {
       this.stats = stats;
       this.weightValue = stats.weight * (k1 + 1);
       this.cache = stats.cache;
-      this.norms = norms == null ? null : (byte[])norms.getSource().getArray();
+      this.norms = norms;
     }
     
     @Override
     public float score(int doc, float freq) {
       // if there are no norms, we act as if b=0
-      float norm = norms == null ? k1 : cache[norms[doc] & 0xFF];
+      float norm = norms == null ? k1 : cache[(byte)norms.get(doc) & 0xFF];
       return weightValue * freq / (freq + norm);
     }
     
@@ -356,7 +355,7 @@
     } 
   }
   
-  private Explanation explainScore(int doc, Explanation freq, BM25Stats stats, byte[] norms) {
+  private Explanation explainScore(int doc, Explanation freq, BM25Stats stats, NumericDocValues norms) {
     Explanation result = new Explanation();
     result.setDescription("score(doc="+doc+",freq="+freq+"), product of:");
     
@@ -374,7 +373,7 @@
       tfNormExpl.addDetail(new Explanation(0, "parameter b (norms omitted for field)"));
       tfNormExpl.setValue((freq.getValue() * (k1 + 1)) / (freq.getValue() + k1));
     } else {
-      float doclen = decodeNormValue(norms[doc]);
+      float doclen = decodeNormValue((byte)norms.get(doc));
       tfNormExpl.addDetail(new Explanation(b, "parameter b"));
       tfNormExpl.addDetail(new Explanation(stats.avgdl, "avgFieldLength"));
       tfNormExpl.addDetail(new Explanation(doclen, "fieldLength"));
Index: lucene/core/src/java/org/apache/lucene/search/similarities/Similarity.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/search/similarities/Similarity.java	(revision 1442822)
+++ lucene/core/src/java/org/apache/lucene/search/similarities/Similarity.java	(working copy)
@@ -19,12 +19,9 @@
 
 import java.io.IOException;
 
-import org.apache.lucene.document.ByteDocValuesField; // javadoc
-import org.apache.lucene.document.FloatDocValuesField; // javadoc
 import org.apache.lucene.index.AtomicReader; // javadoc
 import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.FieldInvertState;
-import org.apache.lucene.index.Norm;
 import org.apache.lucene.search.BooleanQuery;
 import org.apache.lucene.search.CollectionStatistics;
 import org.apache.lucene.search.Explanation;
@@ -52,9 +49,9 @@
  * <a href="#querytime">query-time</a>.
  * <p>
  * <a name="indextime"/>
- * At indexing time, the indexer calls {@link #computeNorm(FieldInvertState, Norm)}, allowing
+ * At indexing time, the indexer calls {@link #computeNorm(FieldInvertState)}, allowing
  * the Similarity implementation to set a per-document value for the field that will 
- * be later accessible via {@link AtomicReader#normValues(String)}.  Lucene makes no assumption
+ * be later accessible via {@link AtomicReader#getNormValues(String)}.  Lucene makes no assumption
  * about what is in this norm, but it is most useful for encoding length normalization 
  * information.
  * <p>
@@ -69,9 +66,8 @@
  * depending upon whether the average should reflect field sparsity.
  * <p>
  * Additional scoring factors can be stored in named
- * <code>*DocValuesField</code>s (such as {@link
- * ByteDocValuesField} or {@link FloatDocValuesField}), and accessed
- * at query-time with {@link AtomicReader#docValues(String)}.
+ * <code>NumericDocValuesField</code>s and accessed
+ * at query-time with {@link AtomicReader#getNumericDocValues(String)}.
  * <p>
  * Finally, using index-time boosts (either via folding into the normalization byte or
  * via DocValues), is an inefficient way to boost the scores of different fields if the
@@ -149,9 +145,6 @@
   /**
    * Computes the normalization value for a field, given the accumulated
    * state of term processing for this field (see {@link FieldInvertState}).
-   * 
-   * <p>Implementations should calculate a norm value based on the field
-   * state and set that value to the given {@link Norm}.
    *
    * <p>Matches in longer fields are less precise, so implementations of this
    * method usually set smaller values when <code>state.getLength()</code> is large,
@@ -160,10 +153,10 @@
    * @lucene.experimental
    * 
    * @param state current processing state for this field
-   * @param norm holds the computed norm value when this method returns
+   * @return computed norm value
    */
-  public abstract void computeNorm(FieldInvertState state, Norm norm);
-  
+  public abstract long computeNorm(FieldInvertState state);
+
   /**
    * Compute any collection-level weight (e.g. IDF, average document length, etc) needed for scoring a query.
    *
Index: lucene/core/src/java/org/apache/lucene/search/similarities/DefaultSimilarity.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/search/similarities/DefaultSimilarity.java	(revision 1442822)
+++ lucene/core/src/java/org/apache/lucene/search/similarities/DefaultSimilarity.java	(working copy)
@@ -18,7 +18,6 @@
  */
 
 import org.apache.lucene.index.FieldInvertState;
-import org.apache.lucene.index.Norm;
 import org.apache.lucene.util.BytesRef;
 
 /** Expert: Default scoring implementation. */
Index: lucene/core/src/java/org/apache/lucene/search/similarities/TFIDFSimilarity.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/search/similarities/TFIDFSimilarity.java	(revision 1442822)
+++ lucene/core/src/java/org/apache/lucene/search/similarities/TFIDFSimilarity.java	(working copy)
@@ -17,13 +17,11 @@
  * limitations under the License.
  */
 
-
 import java.io.IOException;
 
 import org.apache.lucene.index.AtomicReaderContext;
-import org.apache.lucene.index.DocValues;
 import org.apache.lucene.index.FieldInvertState;
-import org.apache.lucene.index.Norm;
+import org.apache.lucene.index.NumericDocValues;
 import org.apache.lucene.search.CollectionStatistics;
 import org.apache.lucene.search.Explanation;
 import org.apache.lucene.search.IndexSearcher;
@@ -682,17 +680,18 @@
   public abstract float lengthNorm(FieldInvertState state);
   
   @Override
-  public final void computeNorm(FieldInvertState state, Norm norm) {
+  public final long computeNorm(FieldInvertState state) {
     float normValue = lengthNorm(state);
-    norm.setByte(encodeNormValue(normValue));
+    return encodeNormValue(normValue);
   }
   
   /** Cache of decoded bytes. */
   private static final float[] NORM_TABLE = new float[256];
 
   static {
-    for (int i = 0; i < 256; i++)
+    for (int i = 0; i < 256; i++) {
       NORM_TABLE[i] = SmallFloat.byte315ToFloat((byte)i);
+    }
   }
 
   /** Decodes a normalization factor stored in an index.
@@ -758,13 +757,13 @@
   @Override
   public final ExactSimScorer exactSimScorer(SimWeight stats, AtomicReaderContext context) throws IOException {
     IDFStats idfstats = (IDFStats) stats;
-    return new ExactTFIDFDocScorer(idfstats, context.reader().normValues(idfstats.field));
+    return new ExactTFIDFDocScorer(idfstats, context.reader().getNormValues(idfstats.field));
   }
 
   @Override
   public final SloppySimScorer sloppySimScorer(SimWeight stats, AtomicReaderContext context) throws IOException {
     IDFStats idfstats = (IDFStats) stats;
-    return new SloppyTFIDFDocScorer(idfstats, context.reader().normValues(idfstats.field));
+    return new SloppyTFIDFDocScorer(idfstats, context.reader().getNormValues(idfstats.field));
   }
   
   // TODO: we can specialize these for omitNorms up front, but we should test that it doesn't confuse stupid hotspot.
@@ -772,26 +771,19 @@
   private final class ExactTFIDFDocScorer extends ExactSimScorer {
     private final IDFStats stats;
     private final float weightValue;
-    private final byte[] norms;
-    private static final int SCORE_CACHE_SIZE = 32;
-    private float[] scoreCache = new float[SCORE_CACHE_SIZE];
+    private final NumericDocValues norms;
     
-    ExactTFIDFDocScorer(IDFStats stats, DocValues norms) throws IOException {
+    ExactTFIDFDocScorer(IDFStats stats, NumericDocValues norms) throws IOException {
       this.stats = stats;
       this.weightValue = stats.value;
-      this.norms = norms == null ? null : (byte[])norms.getSource().getArray(); 
-      for (int i = 0; i < SCORE_CACHE_SIZE; i++)
-        scoreCache[i] = tf(i) * weightValue;
+      this.norms = norms; 
     }
     
     @Override
     public float score(int doc, int freq) {
-      final float raw =                                // compute tf(f)*weight
-        freq < SCORE_CACHE_SIZE                        // check cache
-        ? scoreCache[freq]                             // cache hit
-        : tf(freq)*weightValue;        // cache miss
+      final float raw = tf(freq)*weightValue;  // compute tf(f)*weight
 
-      return norms == null ? raw : raw * decodeNormValue(norms[doc]); // normalize for field
+      return norms == null ? raw : raw * decodeNormValue((byte)norms.get(doc)); // normalize for field
     }
 
     @Override
@@ -803,19 +795,19 @@
   private final class SloppyTFIDFDocScorer extends SloppySimScorer {
     private final IDFStats stats;
     private final float weightValue;
-    private final byte[] norms;
+    private final NumericDocValues norms;
     
-    SloppyTFIDFDocScorer(IDFStats stats, DocValues norms) throws IOException {
+    SloppyTFIDFDocScorer(IDFStats stats, NumericDocValues norms) throws IOException {
       this.stats = stats;
       this.weightValue = stats.value;
-      this.norms = norms == null ? null : (byte[])norms.getSource().getArray();
+      this.norms = norms;
     }
     
     @Override
     public float score(int doc, float freq) {
       final float raw = tf(freq) * weightValue; // compute tf(f)*weight
       
-      return norms == null ? raw : raw * decodeNormValue(norms[doc]);  // normalize for field
+      return norms == null ? raw : raw * decodeNormValue((byte)norms.get(doc));  // normalize for field
     }
     
     @Override
@@ -865,9 +857,9 @@
       queryWeight *= this.queryNorm;              // normalize query weight
       value = queryWeight * idf.getValue();         // idf for document
     }
-  }
-  
-  private Explanation explainScore(int doc, Explanation freq, IDFStats stats, byte[] norms) {
+  }  
+
+  private Explanation explainScore(int doc, Explanation freq, IDFStats stats, NumericDocValues norms) {
     Explanation result = new Explanation();
     result.setDescription("score(doc="+doc+",freq="+freq+"), product of:");
 
@@ -903,7 +895,7 @@
 
     Explanation fieldNormExpl = new Explanation();
     float fieldNorm =
-      norms!=null ? decodeNormValue(norms[doc]) : 1.0f;
+      norms!=null ? decodeNormValue((byte) norms.get(doc)) : 1.0f;
     fieldNormExpl.setValue(fieldNorm);
     fieldNormExpl.setDescription("fieldNorm(doc="+doc+")");
     fieldExpl.addDetail(fieldNormExpl);
Index: lucene/core/src/java/org/apache/lucene/search/similarities/SimilarityBase.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/search/similarities/SimilarityBase.java	(revision 1442822)
+++ lucene/core/src/java/org/apache/lucene/search/similarities/SimilarityBase.java	(working copy)
@@ -20,9 +20,8 @@
 import java.io.IOException;
 
 import org.apache.lucene.index.AtomicReaderContext;
-import org.apache.lucene.index.DocValues;
 import org.apache.lucene.index.FieldInvertState;
-import org.apache.lucene.index.Norm;
+import org.apache.lucene.index.NumericDocValues;
 import org.apache.lucene.search.CollectionStatistics;
 import org.apache.lucene.search.Explanation;
 import org.apache.lucene.search.TermStatistics;
@@ -199,12 +198,12 @@
       ExactSimScorer subScorers[] = new ExactSimScorer[subStats.length];
       for (int i = 0; i < subScorers.length; i++) {
         BasicStats basicstats = (BasicStats) subStats[i];
-        subScorers[i] = new BasicExactDocScorer(basicstats, context.reader().normValues(basicstats.field));
+        subScorers[i] = new BasicExactDocScorer(basicstats, context.reader().getNormValues(basicstats.field));
       }
       return new MultiSimilarity.MultiExactDocScorer(subScorers);
     } else {
       BasicStats basicstats = (BasicStats) stats;
-      return new BasicExactDocScorer(basicstats, context.reader().normValues(basicstats.field));
+      return new BasicExactDocScorer(basicstats, context.reader().getNormValues(basicstats.field));
     }
   }
   
@@ -217,12 +216,12 @@
       SloppySimScorer subScorers[] = new SloppySimScorer[subStats.length];
       for (int i = 0; i < subScorers.length; i++) {
         BasicStats basicstats = (BasicStats) subStats[i];
-        subScorers[i] = new BasicSloppyDocScorer(basicstats, context.reader().normValues(basicstats.field));
+        subScorers[i] = new BasicSloppyDocScorer(basicstats, context.reader().getNormValues(basicstats.field));
       }
       return new MultiSimilarity.MultiSloppyDocScorer(subScorers);
     } else {
       BasicStats basicstats = (BasicStats) stats;
-      return new BasicSloppyDocScorer(basicstats, context.reader().normValues(basicstats.field));
+      return new BasicSloppyDocScorer(basicstats, context.reader().getNormValues(basicstats.field));
     }
   }
   
@@ -247,13 +246,13 @@
 
   /** Encodes the document length in the same way as {@link TFIDFSimilarity}. */
   @Override
-  public void computeNorm(FieldInvertState state, Norm norm) {
+  public long computeNorm(FieldInvertState state) {
     final float numTerms;
     if (discountOverlaps)
       numTerms = state.getLength() - state.getNumOverlap();
     else
       numTerms = state.getLength() / state.getBoost();
-    norm.setByte(encodeNormValue(state.getBoost(), numTerms));
+    return encodeNormValue(state.getBoost(), numTerms);
   }
   
   /** Decodes a normalization factor (document length) stored in an index.
@@ -286,24 +285,24 @@
    */
   private class BasicExactDocScorer extends ExactSimScorer {
     private final BasicStats stats;
-    private final byte[] norms;
+    private final NumericDocValues norms;
     
-    BasicExactDocScorer(BasicStats stats, DocValues norms) throws IOException {
+    BasicExactDocScorer(BasicStats stats, NumericDocValues norms) throws IOException {
       this.stats = stats;
-      this.norms = norms == null ? null : (byte[])norms.getSource().getArray();
+      this.norms = norms;
     }
     
     @Override
     public float score(int doc, int freq) {
       // We have to supply something in case norms are omitted
       return SimilarityBase.this.score(stats, freq,
-          norms == null ? 1F : decodeNormValue(norms[doc]));
+          norms == null ? 1F : decodeNormValue((byte)norms.get(doc)));
     }
     
     @Override
     public Explanation explain(int doc, Explanation freq) {
       return SimilarityBase.this.explain(stats, doc, freq,
-          norms == null ? 1F : decodeNormValue(norms[doc]));
+          norms == null ? 1F : decodeNormValue((byte)norms.get(doc)));
     }
   }
   
@@ -315,23 +314,23 @@
    */
   private class BasicSloppyDocScorer extends SloppySimScorer {
     private final BasicStats stats;
-    private final byte[] norms;
+    private final NumericDocValues norms;
     
-    BasicSloppyDocScorer(BasicStats stats, DocValues norms) throws IOException {
+    BasicSloppyDocScorer(BasicStats stats, NumericDocValues norms) throws IOException {
       this.stats = stats;
-      this.norms = norms == null ? null : (byte[])norms.getSource().getArray();
+      this.norms = norms;
     }
     
     @Override
     public float score(int doc, float freq) {
       // We have to supply something in case norms are omitted
       return SimilarityBase.this.score(stats, freq,
-          norms == null ? 1F : decodeNormValue(norms[doc]));
+          norms == null ? 1F : decodeNormValue((byte)norms.get(doc)));
     }
     @Override
     public Explanation explain(int doc, Explanation freq) {
       return SimilarityBase.this.explain(stats, doc, freq,
-          norms == null ? 1F : decodeNormValue(norms[doc]));
+          norms == null ? 1F : decodeNormValue((byte)norms.get(doc)));
     }
 
     @Override
Index: lucene/core/src/java/org/apache/lucene/search/similarities/PerFieldSimilarityWrapper.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/search/similarities/PerFieldSimilarityWrapper.java	(revision 1442822)
+++ lucene/core/src/java/org/apache/lucene/search/similarities/PerFieldSimilarityWrapper.java	(working copy)
@@ -21,7 +21,6 @@
 
 import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.FieldInvertState;
-import org.apache.lucene.index.Norm;
 import org.apache.lucene.search.CollectionStatistics;
 import org.apache.lucene.search.TermStatistics;
 
@@ -42,8 +41,8 @@
   public PerFieldSimilarityWrapper() {}
 
   @Override
-  public final void computeNorm(FieldInvertState state, Norm norm) {
-    get(state.getName()).computeNorm(state, norm);
+  public final long computeNorm(FieldInvertState state) {
+    return get(state.getName()).computeNorm(state);
   }
 
   @Override
Index: lucene/core/src/java/org/apache/lucene/search/similarities/MultiSimilarity.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/search/similarities/MultiSimilarity.java	(revision 1442822)
+++ lucene/core/src/java/org/apache/lucene/search/similarities/MultiSimilarity.java	(working copy)
@@ -21,7 +21,6 @@
 
 import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.FieldInvertState;
-import org.apache.lucene.index.Norm;
 import org.apache.lucene.search.CollectionStatistics;
 import org.apache.lucene.search.Explanation;
 import org.apache.lucene.search.TermStatistics;
@@ -44,8 +43,8 @@
   }
   
   @Override
-  public void computeNorm(FieldInvertState state, Norm norm) {
-    sims[0].computeNorm(state, norm);
+  public long computeNorm(FieldInvertState state) {
+    return sims[0].computeNorm(state);
   }
 
   @Override
Index: lucene/core/src/java/org/apache/lucene/search/FieldCacheTermsFilter.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/search/FieldCacheTermsFilter.java	(revision 1442822)
+++ lucene/core/src/java/org/apache/lucene/search/FieldCacheTermsFilter.java	(working copy)
@@ -22,9 +22,10 @@
 import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.DocsEnum; // javadoc @link
 import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.util.FixedBitSet;
+import org.apache.lucene.index.SortedDocValues;
 import org.apache.lucene.util.Bits;
 import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.FixedBitSet;
 
 /**
  * A {@link Filter} that only accepts documents whose single
@@ -43,7 +44,7 @@
  * <p/>
  * 
  * The first invocation of this filter on a given field will
- * be slower, since a {@link FieldCache.DocTermsIndex} must be
+ * be slower, since a {@link SortedDocValues} must be
  * created.  Subsequent invocations using the same field
  * will re-use this cache.  However, as with all
  * functionality based on {@link FieldCache}, persistent RAM
@@ -118,19 +119,24 @@
 
   @Override
   public DocIdSet getDocIdSet(AtomicReaderContext context, Bits acceptDocs) throws IOException {
-    final FieldCache.DocTermsIndex fcsi = getFieldCache().getTermsIndex(context.reader(), field);
-    final FixedBitSet bits = new FixedBitSet(fcsi.numOrd());
-    final BytesRef spare = new BytesRef();
+    final SortedDocValues fcsi = getFieldCache().getTermsIndex(context.reader(), field);
+    final FixedBitSet bits = new FixedBitSet(fcsi.getValueCount());
     for (int i=0;i<terms.length;i++) {
-      int termNumber = fcsi.binarySearchLookup(terms[i], spare);
-      if (termNumber > 0) {
-        bits.set(termNumber);
+      int ord = fcsi.lookupTerm(terms[i]);
+      if (ord >= 0) {
+        bits.set(ord);
       }
     }
     return new FieldCacheDocIdSet(context.reader().maxDoc(), acceptDocs) {
       @Override
       protected final boolean matchDoc(int doc) {
-        return bits.get(fcsi.getOrd(doc));
+        int ord = fcsi.getOrd(doc);
+        if (ord == -1) {
+          // missing
+          return false;
+        } else {
+          return bits.get(ord);
+        }
       }
     };
   }
Index: lucene/core/src/java/org/apache/lucene/search/FieldCache.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/search/FieldCache.java	(revision 1442822)
+++ lucene/core/src/java/org/apache/lucene/search/FieldCache.java	(working copy)
@@ -21,19 +21,20 @@
 import java.io.PrintStream;
 
 import org.apache.lucene.analysis.NumericTokenStream; // for javadocs
+import org.apache.lucene.document.DoubleField; // for javadocs
+import org.apache.lucene.document.FloatField; // for javadocs
 import org.apache.lucene.document.IntField; // for javadocs
-import org.apache.lucene.document.FloatField; // for javadocs
 import org.apache.lucene.document.LongField; // for javadocs
-import org.apache.lucene.document.DoubleField; // for javadocs
+import org.apache.lucene.index.AtomicReader;
+import org.apache.lucene.index.BinaryDocValues;
 import org.apache.lucene.index.DocTermOrds;
-import org.apache.lucene.index.AtomicReader;
+import org.apache.lucene.index.SortedDocValues;
 import org.apache.lucene.index.Terms;
 import org.apache.lucene.index.TermsEnum;
 import org.apache.lucene.util.Bits;
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.NumericUtils;
 import org.apache.lucene.util.RamUsageEstimator;
-import org.apache.lucene.util.packed.PackedInts;
 
 /**
  * Expert: Maintains caches of term values.
@@ -42,9 +43,47 @@
  *
  * @since   lucene 1.4
  * @see org.apache.lucene.util.FieldCacheSanityChecker
+ *
+ * @lucene.internal
  */
 public interface FieldCache {
 
+  /** Field values as 8-bit signed bytes */
+  public static abstract class Bytes {
+    /** Return a single Byte representation of this field's value. */
+    public abstract byte get(int docID);
+  }
+
+  /** Field values as 16-bit signed shorts */
+  public static abstract class Shorts {
+    /** Return a short representation of this field's value. */
+    public abstract short get(int docID);
+  }
+
+  /** Field values as 32-bit signed integers */
+  public static abstract class Ints {
+    /** Return an integer representation of this field's value. */
+    public abstract int get(int docID);
+  }
+
+  /** Field values as 32-bit signed long integers */
+  public static abstract class Longs {
+    /** Return an long representation of this field's value. */
+    public abstract long get(int docID);
+  }
+
+  /** Field values as 32-bit floats */
+  public static abstract class Floats {
+    /** Return an float representation of this field's value. */
+    public abstract float get(int docID);
+  }
+
+  /** Field values as 64-bit doubles */
+  public static abstract class Doubles {
+    /** Return an double representation of this field's value. */
+    public abstract double get(int docID);
+  }
+
   /**
    * Placeholder indicating creation of this cache is currently in-progress.
    */
@@ -114,7 +153,7 @@
    * @see FieldCache#getDoubles(AtomicReader, String, FieldCache.DoubleParser, boolean)
    */
   public interface DoubleParser extends Parser {
-    /** Return an long representation of this field's value. */
+    /** Return an double representation of this field's value. */
     public double parseDouble(BytesRef term);
   }
 
@@ -333,12 +372,13 @@
   
  
   /** Checks the internal cache for an appropriate entry, and if none is found,
-   * reads the terms in <code>field</code> and returns a bit set at the size of
-   * <code>reader.maxDoc()</code>, with turned on bits for each docid that 
-   * does have a value for this field.
+   *  reads the terms in <code>field</code> and returns a bit set at the size of
+   *  <code>reader.maxDoc()</code>, with turned on bits for each docid that 
+   *  does have a value for this field.  Note that if the field was only indexed
+   *  as DocValues then this method will not work (it will return a Bits stating
+   *  that no documents contain the field).
    */
-  public Bits getDocsWithField(AtomicReader reader, String field) 
-  throws IOException;
+  public Bits getDocsWithField(AtomicReader reader, String field) throws IOException;
 
   /** Checks the internal cache for an appropriate entry, and if none is
    * found, reads the terms in <code>field</code> as a single byte and returns an array
@@ -351,8 +391,7 @@
    * @return The values in the given field for each document.
    * @throws IOException  If any error occurs.
    */
-  public byte[] getBytes (AtomicReader reader, String field, boolean setDocsWithField)
-  throws IOException;
+  public Bytes getBytes(AtomicReader reader, String field, boolean setDocsWithField) throws IOException;
 
   /** Checks the internal cache for an appropriate entry, and if none is found,
    * reads the terms in <code>field</code> as bytes and returns an array of
@@ -366,8 +405,7 @@
    * @return The values in the given field for each document.
    * @throws IOException  If any error occurs.
    */
-  public byte[] getBytes (AtomicReader reader, String field, ByteParser parser, boolean setDocsWithField)
-  throws IOException;
+  public Bytes getBytes(AtomicReader reader, String field, ByteParser parser, boolean setDocsWithField) throws IOException;
 
   /** Checks the internal cache for an appropriate entry, and if none is
    * found, reads the terms in <code>field</code> as shorts and returns an array
@@ -380,8 +418,7 @@
    * @return The values in the given field for each document.
    * @throws IOException  If any error occurs.
    */
-  public short[] getShorts (AtomicReader reader, String field, boolean setDocsWithField)
-  throws IOException;
+  public Shorts getShorts (AtomicReader reader, String field, boolean setDocsWithField) throws IOException;
 
   /** Checks the internal cache for an appropriate entry, and if none is found,
    * reads the terms in <code>field</code> as shorts and returns an array of
@@ -395,8 +432,7 @@
    * @return The values in the given field for each document.
    * @throws IOException  If any error occurs.
    */
-  public short[] getShorts (AtomicReader reader, String field, ShortParser parser, boolean setDocsWithField)
-  throws IOException;
+  public Shorts getShorts (AtomicReader reader, String field, ShortParser parser, boolean setDocsWithField) throws IOException;
   
   /** Checks the internal cache for an appropriate entry, and if none is
    * found, reads the terms in <code>field</code> as integers and returns an array
@@ -409,8 +445,7 @@
    * @return The values in the given field for each document.
    * @throws IOException  If any error occurs.
    */
-  public int[] getInts (AtomicReader reader, String field, boolean setDocsWithField)
-  throws IOException;
+  public Ints getInts (AtomicReader reader, String field, boolean setDocsWithField) throws IOException;
 
   /** Checks the internal cache for an appropriate entry, and if none is found,
    * reads the terms in <code>field</code> as integers and returns an array of
@@ -424,8 +459,7 @@
    * @return The values in the given field for each document.
    * @throws IOException  If any error occurs.
    */
-  public int[] getInts (AtomicReader reader, String field, IntParser parser, boolean setDocsWithField)
-  throws IOException;
+  public Ints getInts (AtomicReader reader, String field, IntParser parser, boolean setDocsWithField) throws IOException;
 
   /** Checks the internal cache for an appropriate entry, and if
    * none is found, reads the terms in <code>field</code> as floats and returns an array
@@ -438,8 +472,7 @@
    * @return The values in the given field for each document.
    * @throws IOException  If any error occurs.
    */
-  public float[] getFloats (AtomicReader reader, String field, boolean setDocsWithField)
-  throws IOException;
+  public Floats getFloats (AtomicReader reader, String field, boolean setDocsWithField) throws IOException;
 
   /** Checks the internal cache for an appropriate entry, and if
    * none is found, reads the terms in <code>field</code> as floats and returns an array
@@ -453,8 +486,7 @@
    * @return The values in the given field for each document.
    * @throws IOException  If any error occurs.
    */
-  public float[] getFloats (AtomicReader reader, String field,
-                            FloatParser parser, boolean setDocsWithField) throws IOException;
+  public Floats getFloats (AtomicReader reader, String field, FloatParser parser, boolean setDocsWithField) throws IOException;
 
   /**
    * Checks the internal cache for an appropriate entry, and if none is
@@ -469,7 +501,7 @@
    * @return The values in the given field for each document.
    * @throws java.io.IOException If any error occurs.
    */
-  public long[] getLongs(AtomicReader reader, String field, boolean setDocsWithField)
+  public Longs getLongs(AtomicReader reader, String field, boolean setDocsWithField)
           throws IOException;
 
   /**
@@ -486,7 +518,7 @@
    * @return The values in the given field for each document.
    * @throws IOException If any error occurs.
    */
-  public long[] getLongs(AtomicReader reader, String field, LongParser parser, boolean setDocsWithField)
+  public Longs getLongs(AtomicReader reader, String field, LongParser parser, boolean setDocsWithField)
           throws IOException;
 
   /**
@@ -502,7 +534,7 @@
    * @return The values in the given field for each document.
    * @throws IOException If any error occurs.
    */
-  public double[] getDoubles(AtomicReader reader, String field, boolean setDocsWithField)
+  public Doubles getDoubles(AtomicReader reader, String field, boolean setDocsWithField)
           throws IOException;
 
   /**
@@ -519,35 +551,18 @@
    * @return The values in the given field for each document.
    * @throws IOException If any error occurs.
    */
-  public double[] getDoubles(AtomicReader reader, String field, DoubleParser parser, boolean setDocsWithField)
-          throws IOException;
+  public Doubles getDoubles(AtomicReader reader, String field, DoubleParser parser, boolean setDocsWithField) throws IOException;
 
-  /** Returned by {@link #getTerms} */
-  public abstract static class DocTerms {
-    /** The BytesRef argument must not be null; the method
-     *  returns the same BytesRef, or an empty (length=0)
-     *  BytesRef if the doc did not have this field or was
-     *  deleted. */
-    public abstract BytesRef getTerm(int docID, BytesRef ret);
-
-    /** Returns true if this doc has this field and is not
-     *  deleted. */
-    public abstract boolean exists(int docID);
-
-    /** Number of documents */
-    public abstract int size();
-  }
-
   /** Checks the internal cache for an appropriate entry, and if none
    * is found, reads the term values in <code>field</code>
-   * and returns a {@link DocTerms} instance, providing a
+   * and returns a {@link BinaryDocValues} instance, providing a
    * method to retrieve the term (as a BytesRef) per document.
    * @param reader  Used to get field values.
    * @param field   Which field contains the strings.
    * @return The values in the given field for each document.
    * @throws IOException  If any error occurs.
    */
-  public DocTerms getTerms (AtomicReader reader, String field)
+  public BinaryDocValues getTerms (AtomicReader reader, String field)
   throws IOException;
 
   /** Expert: just like {@link #getTerms(AtomicReader,String)},
@@ -555,76 +570,19 @@
    *  faster lookups (default is "true").  Note that the
    *  first call for a given reader and field "wins",
    *  subsequent calls will share the same cache entry. */
-  public DocTerms getTerms (AtomicReader reader, String field, float acceptableOverheadRatio)
-  throws IOException;
+  public BinaryDocValues getTerms (AtomicReader reader, String field, float acceptableOverheadRatio) throws IOException;
 
-  /** Returned by {@link #getTermsIndex} */
-  public abstract static class DocTermsIndex {
-
-    public int binarySearchLookup(BytesRef key, BytesRef spare) {
-      // this special case is the reason that Arrays.binarySearch() isn't useful.
-      if (key == null)
-        return 0;
-  
-      int low = 1;
-      int high = numOrd()-1;
-
-      while (low <= high) {
-        int mid = (low + high) >>> 1;
-        int cmp = lookup(mid, spare).compareTo(key);
-
-        if (cmp < 0)
-          low = mid + 1;
-        else if (cmp > 0)
-          high = mid - 1;
-        else
-          return mid; // key found
-      }
-      return -(low + 1);  // key not found.
-    }
-
-    /** The BytesRef argument must not be null; the method
-     *  returns the same BytesRef, or an empty (length=0)
-     *  BytesRef if this ord is the null ord (0). */
-    public abstract BytesRef lookup(int ord, BytesRef reuse);
-
-    /** Convenience method, to lookup the Term for a doc.
-     *  If this doc is deleted or did not have this field,
-     *  this will return an empty (length=0) BytesRef. */
-    public BytesRef getTerm(int docID, BytesRef reuse) {
-      return lookup(getOrd(docID), reuse);
-    }
-
-    /** Returns sort ord for this document.  Ord 0 is
-     *  reserved for docs that are deleted or did not have
-     *  this field.  */
-    public abstract int getOrd(int docID);
-
-    /** Returns total unique ord count; this includes +1 for
-     *  the null ord (always 0). */
-    public abstract int numOrd();
-
-    /** Number of documents */
-    public abstract int size();
-
-    /** Returns a TermsEnum that can iterate over the values in this index entry */
-    public abstract TermsEnum getTermsEnum();
-
-    /** @lucene.internal */
-    public abstract PackedInts.Reader getDocToOrd();
-  }
-
   /** Checks the internal cache for an appropriate entry, and if none
    * is found, reads the term values in <code>field</code>
-   * and returns a {@link DocTerms} instance, providing a
-   * method to retrieve the term (as a BytesRef) per document.
+   * and returns a {@link SortedDocValues} instance,
+   * providing methods to retrieve sort ordinals and terms
+   * (as a ByteRef) per document.
    * @param reader  Used to get field values.
    * @param field   Which field contains the strings.
    * @return The values in the given field for each document.
    * @throws IOException  If any error occurs.
    */
-  public DocTermsIndex getTermsIndex (AtomicReader reader, String field)
-  throws IOException;
+  public SortedDocValues getTermsIndex (AtomicReader reader, String field) throws IOException;
 
   /** Expert: just like {@link
    *  #getTermsIndex(AtomicReader,String)}, but you can specify
@@ -632,8 +590,7 @@
    *  faster lookups (default is "true").  Note that the
    *  first call for a given reader and field "wins",
    *  subsequent calls will share the same cache entry. */
-  public DocTermsIndex getTermsIndex (AtomicReader reader, String field, float acceptableOverheadRatio)
-  throws IOException;
+  public SortedDocValues getTermsIndex (AtomicReader reader, String field, float acceptableOverheadRatio) throws IOException;
 
   /**
    * Checks the internal cache for an appropriate entry, and if none is found, reads the term values
@@ -652,35 +609,63 @@
    * Can be useful for logging/debugging.
    * @lucene.experimental
    */
-  public static abstract class CacheEntry {
-    public abstract Object getReaderKey();
-    public abstract String getFieldName();
-    public abstract Class<?> getCacheType();
-    public abstract Object getCustom();
-    public abstract Object getValue();
-    private String size = null;
-    protected final void setEstimatedSize(String size) {
-      this.size = size;
+  public final class CacheEntry {
+
+    private final Object readerKey;
+    private final String fieldName;
+    private final Class<?> cacheType;
+    private final Object custom;
+    private final Object value;
+    private String size;
+
+    public CacheEntry(Object readerKey, String fieldName,
+                      Class<?> cacheType,
+                      Object custom,
+                      Object value) {
+      this.readerKey = readerKey;
+      this.fieldName = fieldName;
+      this.cacheType = cacheType;
+      this.custom = custom;
+      this.value = value;
     }
 
+    public Object getReaderKey() {
+      return readerKey;
+    }
+
+    public String getFieldName() {
+      return fieldName;
+    }
+
+    public Class<?> getCacheType() {
+      return cacheType;
+    }
+
+    public Object getCustom() {
+      return custom;
+    }
+
+    public Object getValue() {
+      return value;
+    }
+
     /** 
      * Computes (and stores) the estimated size of the cache Value 
      * @see #getEstimatedSize
      */
     public void estimateSize() {
-      long size = RamUsageEstimator.sizeOf(getValue());
-      setEstimatedSize(RamUsageEstimator.humanReadableUnits(size));
+      long bytesUsed = RamUsageEstimator.sizeOf(getValue());
+      size = RamUsageEstimator.humanReadableUnits(bytesUsed);
     }
 
     /**
      * The most recently estimated size of the value, null unless 
      * estimateSize has been called.
      */
-    public final String getEstimatedSize() {
+    public String getEstimatedSize() {
       return size;
     }
     
-    
     @Override
     public String toString() {
       StringBuilder b = new StringBuilder();
@@ -697,7 +682,6 @@
 
       return b.toString();
     }
-  
   }
   
   /**
Index: lucene/core/src/java/org/apache/lucene/search/FieldComparator.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/search/FieldComparator.java	(revision 1442822)
+++ lucene/core/src/java/org/apache/lucene/search/FieldComparator.java	(working copy)
@@ -18,14 +18,12 @@
  */
 
 import java.io.IOException;
-import java.util.Comparator;
 
 import org.apache.lucene.index.AtomicReader; // javadocs
 import org.apache.lucene.index.AtomicReaderContext;
-import org.apache.lucene.index.DocValues;
+import org.apache.lucene.index.BinaryDocValues;
+import org.apache.lucene.index.SortedDocValues;
 import org.apache.lucene.search.FieldCache.ByteParser;
-import org.apache.lucene.search.FieldCache.DocTerms;
-import org.apache.lucene.search.FieldCache.DocTermsIndex;
 import org.apache.lucene.search.FieldCache.DoubleParser;
 import org.apache.lucene.search.FieldCache.FloatParser;
 import org.apache.lucene.search.FieldCache.IntParser;
@@ -33,7 +31,6 @@
 import org.apache.lucene.search.FieldCache.ShortParser;
 import org.apache.lucene.util.Bits;
 import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.packed.PackedInts;
 
 /**
  * Expert: a FieldComparator compares hits so as to determine their
@@ -227,7 +224,7 @@
   public static final class ByteComparator extends NumericComparator<Byte> {
     private final byte[] values;
     private final ByteParser parser;
-    private byte[] currentReaderValues;
+    private FieldCache.Bytes currentReaderValues;
     private byte bottom;
 
     ByteComparator(int numHits, String field, FieldCache.Parser parser, Byte missingValue) {
@@ -243,7 +240,7 @@
 
     @Override
     public int compareBottom(int doc) {
-      byte v2 = currentReaderValues[doc];
+      byte v2 = currentReaderValues.get(doc);
       // Test for v2 == 0 to save Bits.get method call for
       // the common case (doc has value and value is non-zero):
       if (docsWithField != null && v2 == 0 && !docsWithField.get(doc)) {
@@ -255,7 +252,7 @@
 
     @Override
     public void copy(int slot, int doc) {
-      byte v2 = currentReaderValues[doc];
+      byte v2 = currentReaderValues.get(doc);
       // Test for v2 == 0 to save Bits.get method call for
       // the common case (doc has value and value is non-zero):
       if (docsWithField != null && v2 == 0 && !docsWithField.get(doc)) {
@@ -284,7 +281,7 @@
 
     @Override
     public int compareDocToValue(int doc, Byte value) {
-      byte docValue = currentReaderValues[doc];
+      byte docValue = currentReaderValues.get(doc);
       // Test for docValue == 0 to save Bits.get method call for
       // the common case (doc has value and value is non-zero):
       if (docsWithField != null && docValue == 0 && !docsWithField.get(doc)) {
@@ -299,7 +296,7 @@
   public static final class DoubleComparator extends NumericComparator<Double> {
     private final double[] values;
     private final DoubleParser parser;
-    private double[] currentReaderValues;
+    private FieldCache.Doubles currentReaderValues;
     private double bottom;
 
     DoubleComparator(int numHits, String field, FieldCache.Parser parser, Double missingValue) {
@@ -323,7 +320,7 @@
 
     @Override
     public int compareBottom(int doc) {
-      double v2 = currentReaderValues[doc];
+      double v2 = currentReaderValues.get(doc);
       // Test for v2 == 0 to save Bits.get method call for
       // the common case (doc has value and value is non-zero):
       if (docsWithField != null && v2 == 0 && !docsWithField.get(doc)) {
@@ -341,7 +338,7 @@
 
     @Override
     public void copy(int slot, int doc) {
-      double v2 = currentReaderValues[doc];
+      double v2 = currentReaderValues.get(doc);
       // Test for v2 == 0 to save Bits.get method call for
       // the common case (doc has value and value is non-zero):
       if (docsWithField != null && v2 == 0 && !docsWithField.get(doc)) {
@@ -372,7 +369,7 @@
     @Override
     public int compareDocToValue(int doc, Double valueObj) {
       final double value = valueObj.doubleValue();
-      double docValue = currentReaderValues[doc];
+      double docValue = currentReaderValues.get(doc);
       // Test for docValue == 0 to save Bits.get method call for
       // the common case (doc has value and value is non-zero):
       if (docsWithField != null && docValue == 0 && !docsWithField.get(doc)) {
@@ -388,89 +385,12 @@
     }
   }
 
-  /** Uses float index values to sort by ascending value */
-  public static final class FloatDocValuesComparator extends FieldComparator<Double> {
-    private final double[] values;
-    private final String field;
-    private DocValues.Source currentReaderValues;
-    private double bottom;
-
-    FloatDocValuesComparator(int numHits, String field) {
-      values = new double[numHits];
-      this.field = field;
-    }
-
-    @Override
-    public int compare(int slot1, int slot2) {
-      final double v1 = values[slot1];
-      final double v2 = values[slot2];
-      if (v1 > v2) {
-        return 1;
-      } else if (v1 < v2) {
-        return -1;
-      } else {
-        return 0;
-      }
-    }
-
-    @Override
-    public int compareBottom(int doc) {
-      final double v2 = currentReaderValues.getFloat(doc);
-      if (bottom > v2) {
-        return 1;
-      } else if (bottom < v2) {
-        return -1;
-      } else {
-        return 0;
-      }
-    }
-
-    @Override
-    public void copy(int slot, int doc) {
-      values[slot] = currentReaderValues.getFloat(doc); 
-    }
-
-    @Override
-    public FieldComparator<Double> setNextReader(AtomicReaderContext context) throws IOException {
-      final DocValues docValues = context.reader().docValues(field);
-      if (docValues != null) {
-        currentReaderValues = docValues.getSource(); 
-      } else {
-        currentReaderValues = DocValues.getDefaultSource(DocValues.Type.FLOAT_64);
-      }
-      return this;
-    }
-    
-    @Override
-    public void setBottom(final int bottom) {
-      this.bottom = values[bottom];
-    }
-
-    @Override
-    public Double value(int slot) {
-      return Double.valueOf(values[slot]);
-    }
-
-    @Override
-    public int compareDocToValue(int doc, Double valueObj) {
-      final double value = valueObj.doubleValue();
-      final double docValue = currentReaderValues.getFloat(doc);
-      if (docValue < value) {
-        return -1;
-      } else if (docValue > value) {
-        return 1;
-      } else {
-        return 0;
-      }
-    }
-  }
-
   /** Parses field's values as float (using {@link
    *  FieldCache#getFloats} and sorts by ascending value */
   public static final class FloatComparator extends NumericComparator<Float> {
     private final float[] values;
     private final FloatParser parser;
-    private float[] currentReaderValues;
+    private FieldCache.Floats currentReaderValues;
     private float bottom;
 
     FloatComparator(int numHits, String field, FieldCache.Parser parser, Float missingValue) {
@@ -497,7 +417,7 @@
     @Override
     public int compareBottom(int doc) {
       // TODO: are there sneaky non-branch ways to compute sign of float?
-      float v2 = currentReaderValues[doc];
+      float v2 = currentReaderValues.get(doc);
       // Test for v2 == 0 to save Bits.get method call for
       // the common case (doc has value and value is non-zero):
       if (docsWithField != null && v2 == 0 && !docsWithField.get(doc)) {
@@ -515,7 +435,7 @@
 
     @Override
     public void copy(int slot, int doc) {
-      float v2 = currentReaderValues[doc];
+      float v2 = currentReaderValues.get(doc);
       // Test for v2 == 0 to save Bits.get method call for
       // the common case (doc has value and value is non-zero):
       if (docsWithField != null && v2 == 0 && !docsWithField.get(doc)) {
@@ -546,7 +466,7 @@
     @Override
     public int compareDocToValue(int doc, Float valueObj) {
       final float value = valueObj.floatValue();
-      float docValue = currentReaderValues[doc];
+      float docValue = currentReaderValues.get(doc);
       // Test for docValue == 0 to save Bits.get method call for
       // the common case (doc has value and value is non-zero):
       if (docsWithField != null && docValue == 0 && !docsWithField.get(doc)) {
@@ -567,7 +487,7 @@
   public static final class ShortComparator extends NumericComparator<Short> {
     private final short[] values;
     private final ShortParser parser;
-    private short[] currentReaderValues;
+    private FieldCache.Shorts currentReaderValues;
     private short bottom;
 
     ShortComparator(int numHits, String field, FieldCache.Parser parser, Short missingValue) {
@@ -583,7 +503,7 @@
 
     @Override
     public int compareBottom(int doc) {
-      short v2 = currentReaderValues[doc];
+      short v2 = currentReaderValues.get(doc);
       // Test for v2 == 0 to save Bits.get method call for
       // the common case (doc has value and value is non-zero):
       if (docsWithField != null && v2 == 0 && !docsWithField.get(doc)) {
@@ -595,7 +515,7 @@
 
     @Override
     public void copy(int slot, int doc) {
-      short v2 = currentReaderValues[doc];
+      short v2 = currentReaderValues.get(doc);
       // Test for v2 == 0 to save Bits.get method call for
       // the common case (doc has value and value is non-zero):
       if (docsWithField != null && v2 == 0 && !docsWithField.get(doc)) {
@@ -626,7 +546,7 @@
     @Override
     public int compareDocToValue(int doc, Short valueObj) {
       final short value = valueObj.shortValue();
-      short docValue = currentReaderValues[doc];
+      short docValue = currentReaderValues.get(doc);
       // Test for docValue == 0 to save Bits.get method call for
       // the common case (doc has value and value is non-zero):
       if (docsWithField != null && docValue == 0 && !docsWithField.get(doc)) {
@@ -641,7 +561,7 @@
   public static final class IntComparator extends NumericComparator<Integer> {
     private final int[] values;
     private final IntParser parser;
-    private int[] currentReaderValues;
+    private FieldCache.Ints currentReaderValues;
     private int bottom;                           // Value of bottom of queue
 
     IntComparator(int numHits, String field, FieldCache.Parser parser, Integer missingValue) {
@@ -673,7 +593,7 @@
       // -1/+1/0 sign
       // Cannot return bottom - values[slot2] because that
       // may overflow
-      int v2 = currentReaderValues[doc];
+      int v2 = currentReaderValues.get(doc);
       // Test for v2 == 0 to save Bits.get method call for
       // the common case (doc has value and value is non-zero):
       if (docsWithField != null && v2 == 0 && !docsWithField.get(doc)) {
@@ -691,7 +611,7 @@
 
     @Override
     public void copy(int slot, int doc) {
-      int v2 = currentReaderValues[doc];
+      int v2 = currentReaderValues.get(doc);
       // Test for v2 == 0 to save Bits.get method call for
       // the common case (doc has value and value is non-zero):
       if (docsWithField != null && v2 == 0 && !docsWithField.get(doc)) {
@@ -722,7 +642,7 @@
     @Override
     public int compareDocToValue(int doc, Integer valueObj) {
       final int value = valueObj.intValue();
-      int docValue = currentReaderValues[doc];
+      int docValue = currentReaderValues.get(doc);
       // Test for docValue == 0 to save Bits.get method call for
       // the common case (doc has value and value is non-zero):
       if (docsWithField != null && docValue == 0 && !docsWithField.get(doc)) {
@@ -738,93 +658,12 @@
     }
   }
 
-  /** Loads int index values and sorts by ascending value. */
-  public static final class IntDocValuesComparator extends FieldComparator<Long> {
-    private final long[] values;
-    private DocValues.Source currentReaderValues;
-    private final String field;
-    private long bottom;
-
-    IntDocValuesComparator(int numHits, String field) {
-      values = new long[numHits];
-      this.field = field;
-    }
-
-    @Override
-    public int compare(int slot1, int slot2) {
-      // TODO: there are sneaky non-branch ways to compute
-      // -1/+1/0 sign
-      final long v1 = values[slot1];
-      final long v2 = values[slot2];
-      if (v1 > v2) {
-        return 1;
-      } else if (v1 < v2) {
-        return -1;
-      } else {
-        return 0;
-      }
-    }
-
-    @Override
-    public int compareBottom(int doc) {
-      // TODO: there are sneaky non-branch ways to compute
-      // -1/+1/0 sign
-      final long v2 = currentReaderValues.getInt(doc);
-      if (bottom > v2) {
-        return 1;
-      } else if (bottom < v2) {
-        return -1;
-      } else {
-        return 0;
-      }
-    }
-
-    @Override
-    public void copy(int slot, int doc) {
-      values[slot] = currentReaderValues.getInt(doc); 
-    }
-
-    @Override
-    public FieldComparator<Long> setNextReader(AtomicReaderContext context) throws IOException {
-      DocValues docValues = context.reader().docValues(field);
-      if (docValues != null) {
-        currentReaderValues = docValues.getSource();
-      } else {
-        currentReaderValues = DocValues.getDefaultSource(DocValues.Type.FIXED_INTS_64);
-      }
-      return this;
-    }
-    
-    @Override
-    public void setBottom(final int bottom) {
-      this.bottom = values[bottom];
-    }
-
-    @Override
-    public Long value(int slot) {
-      return Long.valueOf(values[slot]);
-    }
-
-    @Override
-    public int compareDocToValue(int doc, Long valueObj) {
-      final long value = valueObj.longValue();
-      final long docValue = currentReaderValues.getInt(doc);
-      if (docValue < value) {
-        return -1;
-      } else if (docValue > value) {
-        return 1;
-      } else {
-        return 0;
-      }
-    }
-  }
-
   /** Parses field's values as long (using {@link
    *  FieldCache#getLongs} and sorts by ascending value */
   public static final class LongComparator extends NumericComparator<Long> {
     private final long[] values;
     private final LongParser parser;
-    private long[] currentReaderValues;
+    private FieldCache.Longs currentReaderValues;
     private long bottom;
 
     LongComparator(int numHits, String field, FieldCache.Parser parser, Long missingValue) {
@@ -852,7 +691,7 @@
     public int compareBottom(int doc) {
       // TODO: there are sneaky non-branch ways to compute
       // -1/+1/0 sign
-      long v2 = currentReaderValues[doc];
+      long v2 = currentReaderValues.get(doc);
       // Test for v2 == 0 to save Bits.get method call for
       // the common case (doc has value and value is non-zero):
       if (docsWithField != null && v2 == 0 && !docsWithField.get(doc)) {
@@ -870,7 +709,7 @@
 
     @Override
     public void copy(int slot, int doc) {
-      long v2 = currentReaderValues[doc];
+      long v2 = currentReaderValues.get(doc);
       // Test for v2 == 0 to save Bits.get method call for
       // the common case (doc has value and value is non-zero):
       if (docsWithField != null && v2 == 0 && !docsWithField.get(doc)) {
@@ -901,7 +740,7 @@
     @Override
     public int compareDocToValue(int doc, Long valueObj) {
       final long value = valueObj.longValue();
-      long docValue = currentReaderValues[doc];
+      long docValue = currentReaderValues.get(doc);
       // Test for docValue == 0 to save Bits.get method call for
       // the common case (doc has value and value is non-zero):
       if (docsWithField != null && docValue == 0 && !docsWithField.get(doc)) {
@@ -1095,7 +934,7 @@
 
     /* Current reader's doc ord/values.
        @lucene.internal */
-    DocTermsIndex termsIndex;
+    SortedDocValues termsIndex;
 
     private final String field;
 
@@ -1158,8 +997,8 @@
 
     @Override
     public int compareDocToValue(int doc, BytesRef value) {
-      BytesRef docValue = termsIndex.getTerm(doc, tempBR);
-      if (docValue == null) {
+      int ord = termsIndex.getOrd(doc);
+      if (ord == -1) {
         if (value == null) {
           return 0;
         }
@@ -1167,7 +1006,8 @@
       } else if (value == null) {
         return 1;
       }
-      return docValue.compareTo(value);
+      termsIndex.lookupOrd(ord, tempBR);
+      return tempBR.compareTo(value);
     }
 
     /** Base class for specialized (per bit width of the
@@ -1217,153 +1057,12 @@
       }
     }
 
-    // Used per-segment when bit width of doc->ord is 8:
-    private final class ByteOrdComparator extends PerSegmentComparator {
-      private final byte[] readerOrds;
-      private final DocTermsIndex termsIndex;
-      private final int docBase;
-
-      public ByteOrdComparator(byte[] readerOrds, DocTermsIndex termsIndex, int docBase) {
-        this.readerOrds = readerOrds;
-        this.termsIndex = termsIndex;
-        this.docBase = docBase;
-      }
-
-      @Override
-      public int compareBottom(int doc) {
-        assert bottomSlot != -1;
-        final int docOrd = (readerOrds[doc]&0xFF);
-        if (bottomSameReader) {
-          // ord is precisely comparable, even in the equal case
-          return bottomOrd - docOrd;
-        } else if (bottomOrd >= docOrd) {
-          // the equals case always means bottom is > doc
-          // (because we set bottomOrd to the lower bound in
-          // setBottom):
-          return 1;
-        } else {
-          return -1;
-        }
-      }
-
-      @Override
-      public void copy(int slot, int doc) {
-        final int ord = readerOrds[doc]&0xFF;
-        ords[slot] = ord;
-        if (ord == 0) {
-          values[slot] = null;
-        } else {
-          assert ord > 0;
-          if (values[slot] == null) {
-            values[slot] = new BytesRef();
-          }
-          termsIndex.lookup(ord, values[slot]);
-        }
-        readerGen[slot] = currentReaderGen;
-      }
-    }
-
-    // Used per-segment when bit width of doc->ord is 16:
-    private final class ShortOrdComparator extends PerSegmentComparator {
-      private final short[] readerOrds;
-      private final DocTermsIndex termsIndex;
-      private final int docBase;
-
-      public ShortOrdComparator(short[] readerOrds, DocTermsIndex termsIndex, int docBase) {
-        this.readerOrds = readerOrds;
-        this.termsIndex = termsIndex;
-        this.docBase = docBase;
-      }
-
-      @Override
-      public int compareBottom(int doc) {
-        assert bottomSlot != -1;
-        final int docOrd = (readerOrds[doc]&0xFFFF);
-        if (bottomSameReader) {
-          // ord is precisely comparable, even in the equal case
-          return bottomOrd - docOrd;
-        } else if (bottomOrd >= docOrd) {
-          // the equals case always means bottom is > doc
-          // (because we set bottomOrd to the lower bound in
-          // setBottom):
-          return 1;
-        } else {
-          return -1;
-        }
-      }
-
-      @Override
-      public void copy(int slot, int doc) {
-        final int ord = readerOrds[doc]&0xFFFF;
-        ords[slot] = ord;
-        if (ord == 0) {
-          values[slot] = null;
-        } else {
-          assert ord > 0;
-          if (values[slot] == null) {
-            values[slot] = new BytesRef();
-          }
-          termsIndex.lookup(ord, values[slot]);
-        }
-        readerGen[slot] = currentReaderGen;
-      }
-    }
-
-    // Used per-segment when bit width of doc->ord is 32:
-    private final class IntOrdComparator extends PerSegmentComparator {
-      private final int[] readerOrds;
-      private final DocTermsIndex termsIndex;
-      private final int docBase;
-
-      public IntOrdComparator(int[] readerOrds, DocTermsIndex termsIndex, int docBase) {
-        this.readerOrds = readerOrds;
-        this.termsIndex = termsIndex;
-        this.docBase = docBase;
-      }
-
-      @Override
-      public int compareBottom(int doc) {
-        assert bottomSlot != -1;
-        final int docOrd = readerOrds[doc];
-        if (bottomSameReader) {
-          // ord is precisely comparable, even in the equal case
-          return bottomOrd - docOrd;
-        } else if (bottomOrd >= docOrd) {
-          // the equals case always means bottom is > doc
-          // (because we set bottomOrd to the lower bound in
-          // setBottom):
-          return 1;
-        } else {
-          return -1;
-        }
-      }
-
-      @Override
-      public void copy(int slot, int doc) {
-        final int ord = readerOrds[doc];
-        ords[slot] = ord;
-        if (ord == 0) {
-          values[slot] = null;
-        } else {
-          assert ord > 0;
-          if (values[slot] == null) {
-            values[slot] = new BytesRef();
-          }
-          termsIndex.lookup(ord, values[slot]);
-        }
-        readerGen[slot] = currentReaderGen;
-      }
-    }
-
-    // Used per-segment when bit width is not a native array
-    // size (8, 16, 32):
+    // Used per-segment when docToOrd is null:
     private final class AnyOrdComparator extends PerSegmentComparator {
-      private final PackedInts.Reader readerOrds;
-      private final DocTermsIndex termsIndex;
+      private final SortedDocValues termsIndex;
       private final int docBase;
 
-      public AnyOrdComparator(PackedInts.Reader readerOrds, DocTermsIndex termsIndex, int docBase) {
-        this.readerOrds = readerOrds;
+      public AnyOrdComparator(SortedDocValues termsIndex, int docBase) {
         this.termsIndex = termsIndex;
         this.docBase = docBase;
       }
@@ -1371,7 +1070,7 @@
       @Override
       public int compareBottom(int doc) {
         assert bottomSlot != -1;
-        final int docOrd = (int) readerOrds.get(doc);
+        final int docOrd = termsIndex.getOrd(doc);
         if (bottomSameReader) {
           // ord is precisely comparable, even in the equal case
           return bottomOrd - docOrd;
@@ -1387,16 +1086,16 @@
 
       @Override
       public void copy(int slot, int doc) {
-        final int ord = (int) readerOrds.get(doc);
+        final int ord = termsIndex.getOrd(doc);
         ords[slot] = ord;
-        if (ord == 0) {
+        if (ord == -1) {
           values[slot] = null;
         } else {
-          assert ord > 0;
+          assert ord >= 0;
           if (values[slot] == null) {
             values[slot] = new BytesRef();
           }
-          termsIndex.lookup(ord, values[slot]);
+          termsIndex.lookupOrd(ord, values[slot]);
         }
         readerGen[slot] = currentReaderGen;
       }
@@ -1406,25 +1105,7 @@
     public FieldComparator<BytesRef> setNextReader(AtomicReaderContext context) throws IOException {
       final int docBase = context.docBase;
       termsIndex = FieldCache.DEFAULT.getTermsIndex(context.reader(), field);
-      final PackedInts.Reader docToOrd = termsIndex.getDocToOrd();
-      FieldComparator<BytesRef> perSegComp = null;
-      if (docToOrd.hasArray()) {
-        final Object arr = docToOrd.getArray();
-        if (arr instanceof byte[]) {
-          perSegComp = new ByteOrdComparator((byte[]) arr, termsIndex, docBase);
-        } else if (arr instanceof short[]) {
-          perSegComp = new ShortOrdComparator((short[]) arr, termsIndex, docBase);
-        } else if (arr instanceof int[]) {
-          perSegComp = new IntOrdComparator((int[]) arr, termsIndex, docBase);
-        }
-        // Don't specialize the long[] case since it's not
-        // possible, ie, worse case is MAX_INT-1 docs with
-        // every one having a unique value.
-      }
-      if (perSegComp == null) {
-        perSegComp = new AnyOrdComparator(docToOrd, termsIndex, docBase);
-      }
-
+      FieldComparator<BytesRef> perSegComp = new AnyOrdComparator(termsIndex, docBase);
       currentReaderGen++;
       if (bottomSlot != -1) {
         perSegComp.setBottom(bottomSlot);
@@ -1443,13 +1124,13 @@
         bottomSameReader = true;
       } else {
         if (bottomValue == null) {
-          // 0 ord is null for all segments
-          assert ords[bottomSlot] == 0;
-          bottomOrd = 0;
+          // -1 ord is null for all segments
+          assert ords[bottomSlot] == -1;
+          bottomOrd = -1;
           bottomSameReader = true;
           readerGen[bottomSlot] = currentReaderGen;
         } else {
-          final int index = binarySearch(tempBR, termsIndex, bottomValue);
+          final int index = termsIndex.lookupTerm(bottomValue);
           if (index < 0) {
             bottomOrd = -index - 2;
             bottomSameReader = false;
@@ -1470,441 +1151,6 @@
     }
   }
 
-  /** Sorts by field's natural Term sort order, using
-   *  ordinals; this is just like {@link
-   *  org.apache.lucene.search.FieldComparator.TermValComparator} except it uses DocValues to
-   *  retrieve the sort ords saved during indexing. */
-  public static final class TermOrdValDocValuesComparator extends FieldComparator<BytesRef> {
-    /* Ords for each slot.
-       @lucene.internal */
-    final int[] ords;
-
-    /* Values for each slot.
-       @lucene.internal */
-    final BytesRef[] values;
-
-    /* Which reader last copied a value into the slot. When
-       we compare two slots, we just compare-by-ord if the
-       readerGen is the same; else we must compare the
-       values (slower).
-       @lucene.internal */
-    final int[] readerGen;
-
-    /* Gen of current reader we are on.
-       @lucene.internal */
-    int currentReaderGen = -1;
-
-    /* Current reader's doc ord/values.
-       @lucene.internal */
-    DocValues.SortedSource termsIndex;
-
-    /* Comparator for comparing by value.
-       @lucene.internal */
-    Comparator<BytesRef> comp;
-
-    private final String field;
-
-    /* Bottom slot, or -1 if queue isn't full yet
-       @lucene.internal */
-    int bottomSlot = -1;
-
-    /* Bottom ord (same as ords[bottomSlot] once bottomSlot
-       is set).  Cached for faster compares.
-       @lucene.internal */
-    int bottomOrd;
-
-    /* True if current bottom slot matches the current
-       reader.
-       @lucene.internal */
-    boolean bottomSameReader;
-
-    /* Bottom value (same as values[bottomSlot] once
-       bottomSlot is set).  Cached for faster compares.
-      @lucene.internal */
-    BytesRef bottomValue;
-
-    /** @lucene.internal */
-    final BytesRef tempBR = new BytesRef();
-
-    public TermOrdValDocValuesComparator(int numHits, String field) {
-      ords = new int[numHits];
-      values = new BytesRef[numHits];
-      readerGen = new int[numHits];
-      this.field = field;
-    }
-
-    @Override
-    public int compare(int slot1, int slot2) {
-      if (readerGen[slot1] == readerGen[slot2]) {
-        return ords[slot1] - ords[slot2];
-      }
-
-      final BytesRef val1 = values[slot1];
-      final BytesRef val2 = values[slot2];
-      if (val1 == null) {
-        if (val2 == null) {
-          return 0;
-        }
-        return -1;
-      } else if (val2 == null) {
-        return 1;
-      }
-      return comp.compare(val1, val2);
-    }
-
-    @Override
-    public int compareBottom(int doc) {
-      throw new UnsupportedOperationException();
-    }
-
-    @Override
-    public void copy(int slot, int doc) {
-      throw new UnsupportedOperationException();
-    }
-
-    @Override
-    public int compareDocToValue(int doc, BytesRef value) {
-      return termsIndex.getBytes(doc, tempBR).compareTo(value);
-    }
-
-    // TODO: would be nice to share these specialized impls
-    // w/ TermOrdValComparator
-
-    /** Base class for specialized (per bit width of the
-     * ords) per-segment comparator.  NOTE: this is messy;
-     * we do this only because hotspot can't reliably inline
-     * the underlying array access when looking up doc->ord
-     * @lucene.internal
-     */
-    abstract class PerSegmentComparator extends FieldComparator<BytesRef> {
-      
-      @Override
-      public FieldComparator<BytesRef> setNextReader(AtomicReaderContext context) throws IOException {
-        return TermOrdValDocValuesComparator.this.setNextReader(context);
-      }
-
-      @Override
-      public int compare(int slot1, int slot2) {
-        return TermOrdValDocValuesComparator.this.compare(slot1, slot2);
-      }
-
-      @Override
-      public void setBottom(final int bottom) {
-        TermOrdValDocValuesComparator.this.setBottom(bottom);
-      }
-
-      @Override
-      public BytesRef value(int slot) {
-        return TermOrdValDocValuesComparator.this.value(slot);
-      }
-
-      @Override
-      public int compareValues(BytesRef val1, BytesRef val2) {
-        assert val1 != null;
-        assert val2 != null;
-        return comp.compare(val1, val2);
-      }
-
-      @Override
-      public int compareDocToValue(int doc, BytesRef value) {
-        return TermOrdValDocValuesComparator.this.compareDocToValue(doc, value);
-      }
-    }
-
-    // Used per-segment when bit width of doc->ord is 8:
-    private final class ByteOrdComparator extends PerSegmentComparator {
-      private final byte[] readerOrds;
-      private final DocValues.SortedSource termsIndex;
-      private final int docBase;
-
-      public ByteOrdComparator(byte[] readerOrds, DocValues.SortedSource termsIndex, int docBase) {
-        this.readerOrds = readerOrds;
-        this.termsIndex = termsIndex;
-        this.docBase = docBase;
-      }
-
-      @Override
-      public int compareBottom(int doc) {
-        assert bottomSlot != -1;
-        final int docOrd = readerOrds[doc]&0xFF;
-        if (bottomSameReader) {
-          // ord is precisely comparable, even in the equal case
-          return bottomOrd - docOrd;
-        } else if (bottomOrd >= docOrd) {
-          // the equals case always means bottom is > doc
-          // (because we set bottomOrd to the lower bound in
-          // setBottom):
-          return 1;
-        } else {
-          return -1;
-        }
-      }
-
-      @Override
-      public void copy(int slot, int doc) {
-        final int ord = readerOrds[doc]&0xFF;
-        ords[slot] = ord;
-        if (values[slot] == null) {
-          values[slot] = new BytesRef();
-        }
-        termsIndex.getByOrd(ord, values[slot]);
-        readerGen[slot] = currentReaderGen;
-      }
-    }
-
-    // Used per-segment when bit width of doc->ord is 16:
-    private final class ShortOrdComparator extends PerSegmentComparator {
-      private final short[] readerOrds;
-      private final DocValues.SortedSource termsIndex;
-      private final int docBase;
-
-      public ShortOrdComparator(short[] readerOrds, DocValues.SortedSource termsIndex, int docBase) {
-        this.readerOrds = readerOrds;
-        this.termsIndex = termsIndex;
-        this.docBase = docBase;
-      }
-
-      @Override
-      public int compareBottom(int doc) {
-        assert bottomSlot != -1;
-        final int docOrd = readerOrds[doc]&0xFFFF;
-        if (bottomSameReader) {
-          // ord is precisely comparable, even in the equal case
-          return bottomOrd - docOrd;
-        } else if (bottomOrd >= docOrd) {
-          // the equals case always means bottom is > doc
-          // (because we set bottomOrd to the lower bound in
-          // setBottom):
-          return 1;
-        } else {
-          return -1;
-        }
-      }
-
-      @Override
-      public void copy(int slot, int doc) {
-        final int ord = readerOrds[doc]&0xFFFF;
-        ords[slot] = ord;
-        if (values[slot] == null) {
-          values[slot] = new BytesRef();
-        }
-        termsIndex.getByOrd(ord, values[slot]);
-        readerGen[slot] = currentReaderGen;
-      }
-    }
-
-    // Used per-segment when bit width of doc->ord is 32:
-    private final class IntOrdComparator extends PerSegmentComparator {
-      private final int[] readerOrds;
-      private final DocValues.SortedSource termsIndex;
-      private final int docBase;
-
-      public IntOrdComparator(int[] readerOrds, DocValues.SortedSource termsIndex, int docBase) {
-        this.readerOrds = readerOrds;
-        this.termsIndex = termsIndex;
-        this.docBase = docBase;
-      }
-
-      @Override
-      public int compareBottom(int doc) {
-        assert bottomSlot != -1;
-        final int docOrd = readerOrds[doc];
-        if (bottomSameReader) {
-          // ord is precisely comparable, even in the equal case
-          return bottomOrd - docOrd;
-        } else if (bottomOrd >= docOrd) {
-          // the equals case always means bottom is > doc
-          // (because we set bottomOrd to the lower bound in
-          // setBottom):
-          return 1;
-        } else {
-          return -1;
-        }
-      }
-
-      @Override
-      public void copy(int slot, int doc) {
-        final int ord = readerOrds[doc];
-        ords[slot] = ord;
-        if (values[slot] == null) {
-          values[slot] = new BytesRef();
-        }
-        termsIndex.getByOrd(ord, values[slot]);
-        readerGen[slot] = currentReaderGen;
-      }
-    }
-
-    // Used per-segment when bit width is not a native array
-    // size (8, 16, 32):
-    private final class AnyPackedDocToOrdComparator extends PerSegmentComparator {
-      private final PackedInts.Reader readerOrds;
-      private final int docBase;
-
-      public AnyPackedDocToOrdComparator(PackedInts.Reader readerOrds, int docBase) {
-        this.readerOrds = readerOrds;
-        this.docBase = docBase;
-      }
-
-      @Override
-      public int compareBottom(int doc) {
-        assert bottomSlot != -1;
-        final int docOrd = (int) readerOrds.get(doc);
-        if (bottomSameReader) {
-          // ord is precisely comparable, even in the equal case
-          return bottomOrd - docOrd;
-        } else if (bottomOrd >= docOrd) {
-          // the equals case always means bottom is > doc
-          // (because we set bottomOrd to the lower bound in
-          // setBottom):
-          return 1;
-        } else {
-          return -1;
-        }
-      }
-
-      @Override
-      public void copy(int slot, int doc) {
-        final int ord = (int) readerOrds.get(doc);
-        ords[slot] = ord;
-        if (values[slot] == null) {
-          values[slot] = new BytesRef();
-        }
-        termsIndex.getByOrd(ord, values[slot]);
-        readerGen[slot] = currentReaderGen;
-      }
-    }
-
-    // Used per-segment when DV doesn't use packed ints for
-    // docToOrds:
-    private final class AnyOrdComparator extends PerSegmentComparator {
-      private final int docBase;
-
-      public AnyOrdComparator(int docBase) {
-        this.docBase = docBase;
-      }
-
-      @Override
-      public int compareBottom(int doc) {
-        final int docOrd = termsIndex.ord(doc);
-        if (bottomSameReader) {
-          // ord is precisely comparable, even in the equal case
-          return bottomOrd - docOrd;
-        } else if (bottomOrd >= docOrd) {
-          // the equals case always means bottom is > doc
-          // (because we set bottomOrd to the lower bound in
-          // setBottom):
-          return 1;
-        } else {
-          return -1;
-        }
-      }
-
-      @Override
-      public void copy(int slot, int doc) {
-        final int ord = termsIndex.ord(doc);
-        ords[slot] = ord;
-        if (values[slot] == null) {
-          values[slot] = new BytesRef();
-        }
-        termsIndex.getByOrd(ord, values[slot]);
-        readerGen[slot] = currentReaderGen;
-      }
-    }
-
-    @Override
-    public FieldComparator<BytesRef> setNextReader(AtomicReaderContext context) throws IOException {
-      final int docBase = context.docBase;
-
-      final DocValues dv = context.reader().docValues(field);
-      if (dv == null) {
-        // This may mean entire segment had no docs with
-        // this DV field; use default field value (empty
-        // byte[]) in this case:
-        termsIndex = DocValues.getDefaultSortedSource(DocValues.Type.BYTES_VAR_SORTED, context.reader().maxDoc());
-      } else {
-        termsIndex = dv.getSource().asSortedSource();
-        if (termsIndex == null) {
-          // This means segment has doc values, but they are
-          // not able to provide a sorted source; consider
-          // this a hard error:
-          throw new IllegalStateException("DocValues exist for field \"" + field + "\", but not as a sorted source: type=" + dv.getSource().getType() + " reader=" + context.reader());
-        }
-      }
-
-      comp = termsIndex.getComparator();
-
-      FieldComparator<BytesRef> perSegComp = null;
-      if (termsIndex.hasPackedDocToOrd()) {
-        final PackedInts.Reader docToOrd = termsIndex.getDocToOrd();
-        if (docToOrd.hasArray()) {
-          final Object arr = docToOrd.getArray();
-          assert arr != null;
-          if (arr instanceof byte[]) {
-            // 8 bit packed
-            perSegComp = new ByteOrdComparator((byte[]) arr, termsIndex, docBase);
-          } else if (arr instanceof short[]) {
-            // 16 bit packed
-            perSegComp = new ShortOrdComparator((short[]) arr, termsIndex, docBase);
-          } else if (arr instanceof int[]) {
-            // 32 bit packed
-            perSegComp = new IntOrdComparator((int[]) arr, termsIndex, docBase);
-          }
-        }
-
-        if (perSegComp == null) {
-          perSegComp = new AnyPackedDocToOrdComparator(docToOrd, docBase);
-        }
-      } else {
-        if (perSegComp == null) {
-          perSegComp = new AnyOrdComparator(docBase);
-        }
-      }
-        
-      currentReaderGen++;
-      if (bottomSlot != -1) {
-        perSegComp.setBottom(bottomSlot);
-      }
-
-      return perSegComp;
-    }
-    
-    @Override
-    public void setBottom(final int bottom) {
-      bottomSlot = bottom;
-
-      bottomValue = values[bottomSlot];
-      if (currentReaderGen == readerGen[bottomSlot]) {
-        bottomOrd = ords[bottomSlot];
-        bottomSameReader = true;
-      } else {
-        if (bottomValue == null) {
-          // 0 ord is null for all segments
-          assert ords[bottomSlot] == 0;
-          bottomOrd = 0;
-          bottomSameReader = true;
-          readerGen[bottomSlot] = currentReaderGen;
-        } else {
-          final int index = termsIndex.getOrdByValue(bottomValue, tempBR);
-          if (index < 0) {
-            bottomOrd = -index - 2;
-            bottomSameReader = false;
-          } else {
-            bottomOrd = index;
-            // exact value match
-            bottomSameReader = true;
-            readerGen[bottomSlot] = currentReaderGen;            
-            ords[bottomSlot] = bottomOrd;
-          }
-        }
-      }
-    }
-
-    @Override
-    public BytesRef value(int slot) {
-      return values[slot];
-    }
-  }
-
   /** Sorts by field's natural Term sort order.  All
    *  comparisons are done using BytesRef.compareTo, which is
    *  slow for medium to large result sets but possibly
@@ -1912,7 +1158,7 @@
   public static final class TermValComparator extends FieldComparator<BytesRef> {
 
     private BytesRef[] values;
-    private DocTerms docTerms;
+    private BinaryDocValues docTerms;
     private final String field;
     private BytesRef bottom;
     private final BytesRef tempBR = new BytesRef();
@@ -1940,16 +1186,16 @@
 
     @Override
     public int compareBottom(int doc) {
-      BytesRef val2 = docTerms.getTerm(doc, tempBR);
-      if (bottom == null) {
-        if (val2 == null) {
+      docTerms.get(doc, tempBR);
+      if (bottom.bytes == BinaryDocValues.MISSING) {
+        if (tempBR.bytes == BinaryDocValues.MISSING) {
           return 0;
         }
         return -1;
-      } else if (val2 == null) {
+      } else if (tempBR.bytes == BinaryDocValues.MISSING) {
         return 1;
       }
-      return bottom.compareTo(val2);
+      return bottom.compareTo(tempBR);
     }
 
     @Override
@@ -1957,7 +1203,7 @@
       if (values[slot] == null) {
         values[slot] = new BytesRef();
       }
-      docTerms.getTerm(doc, values[slot]);
+      docTerms.get(doc, values[slot]);
     }
 
     @Override
@@ -1991,106 +1237,8 @@
 
     @Override
     public int compareDocToValue(int doc, BytesRef value) {
-      return docTerms.getTerm(doc, tempBR).compareTo(value);
+      docTerms.get(doc, tempBR);
+      return tempBR.compareTo(value);
     }
   }
-
-  /** Sorts by field's natural Term sort order.  All
-   *  comparisons are done using BytesRef.compareTo, which is
-   *  slow for medium to large result sets but possibly
-   *  very fast for very small results sets.  The BytesRef
-   *  values are obtained using {@link AtomicReader#docValues}. */
-  public static final class TermValDocValuesComparator extends FieldComparator<BytesRef> {
-
-    private BytesRef[] values;
-    private DocValues.Source docTerms;
-    private final String field;
-    private BytesRef bottom;
-    private final BytesRef tempBR = new BytesRef();
-
-    TermValDocValuesComparator(int numHits, String field) {
-      values = new BytesRef[numHits];
-      this.field = field;
-    }
-
-    @Override
-    public int compare(int slot1, int slot2) {
-      assert values[slot1] != null;
-      assert values[slot2] != null;
-      return values[slot1].compareTo(values[slot2]);
-    }
-
-    @Override
-    public int compareBottom(int doc) {
-      assert bottom != null;
-      return bottom.compareTo(docTerms.getBytes(doc, tempBR));
-    }
-
-    @Override
-    public void copy(int slot, int doc) {
-      if (values[slot] == null) {
-        values[slot] = new BytesRef();
-      }
-      docTerms.getBytes(doc, values[slot]);
-    }
-
-    @Override
-    public FieldComparator<BytesRef> setNextReader(AtomicReaderContext context) throws IOException {
-      final DocValues dv = context.reader().docValues(field);
-      if (dv != null) {
-        docTerms = dv.getSource();
-      } else {
-        docTerms = DocValues.getDefaultSource(DocValues.Type.BYTES_VAR_DEREF);
-      }
-      return this;
-    }
-    
-    @Override
-    public void setBottom(final int bottom) {
-      this.bottom = values[bottom];
-    }
-
-    @Override
-    public BytesRef value(int slot) {
-      return values[slot];
-    }
-
-    @Override
-    public int compareValues(BytesRef val1, BytesRef val2) {
-      assert val1 != null;
-      assert val2 != null;
-      return val1.compareTo(val2);
-    }
-
-    @Override
-    public int compareDocToValue(int doc, BytesRef value) {
-      return docTerms.getBytes(doc, tempBR).compareTo(value);
-    }
-  }
-
-  final protected static int binarySearch(BytesRef br, DocTermsIndex a, BytesRef key) {
-    return binarySearch(br, a, key, 1, a.numOrd()-1);
-  }
-
-  final protected static int binarySearch(BytesRef br, DocTermsIndex a, BytesRef key, int low, int high) {
-
-    while (low <= high) {
-      int mid = (low + high) >>> 1;
-      BytesRef midVal = a.lookup(mid, br);
-      int cmp;
-      if (midVal != null) {
-        cmp = midVal.compareTo(key);
-      } else {
-        cmp = -1;
-      }
-
-      if (cmp < 0)
-        low = mid + 1;
-      else if (cmp > 0)
-        high = mid - 1;
-      else
-        return mid;
-    }
-    return -(low + 1);
-  }
 }
Index: lucene/core/src/java/org/apache/lucene/index/SortedBytesMergeUtils.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/index/SortedBytesMergeUtils.java	(revision 1442822)
+++ lucene/core/src/java/org/apache/lucene/index/SortedBytesMergeUtils.java	(working copy)
@@ -1,435 +0,0 @@
-package org.apache.lucene.index;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Collections;
-import java.util.Comparator;
-import java.util.List;
-
-import org.apache.lucene.index.DocValues.SortedSource;
-import org.apache.lucene.index.DocValues.Source;
-import org.apache.lucene.index.DocValues.Type;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.util.ArrayUtil;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.PriorityQueue;
-import org.apache.lucene.util.packed.PackedInts;
-
-/**
- * Utility class for merging SortedBytes DocValues
- * instances.
- *  
- * @lucene.internal
- */
-public final class SortedBytesMergeUtils {
-
-  private SortedBytesMergeUtils() {
-    // no instance
-  }
-
-  /** Creates the {@link MergeContext} necessary for merging
-   *  the ordinals. */
-  public static MergeContext init(Type type, DocValues[] docValues,
-      Comparator<BytesRef> comp, int mergeDocCount) {
-    int size = -1;
-    if (type == Type.BYTES_FIXED_SORTED) {
-      for (DocValues indexDocValues : docValues) {
-        if (indexDocValues != null) {
-          size = indexDocValues.getValueSize();
-          break;
-        }
-      }
-      assert size >= 0;
-    }
-    return new MergeContext(comp, mergeDocCount, size, type);
-  }
-  /**
-   * Encapsulates contextual information about the merge. 
-   * This class holds document id to ordinal mappings, offsets for
-   * variable length values and the comparator to sort the merged
-   * bytes.
-   * 
-   * @lucene.internal
-   */
-  public static final class MergeContext {
-    private final Comparator<BytesRef> comp;
-    private final BytesRef missingValue = new BytesRef();
-
-    /** How many bytes each value occupies, or -1 if it
-     *  varies. */
-    public final int sizePerValues; // -1 if var length
-
-    final Type type;
-
-    /** Maps each document to the ordinal for its value. */
-    public final int[] docToEntry;
-
-    /** File-offset for each document; will be null if it's
-     *  not needed (eg fixed-size values). */
-    public long[] offsets; // if non-null #mergeRecords collects byte offsets here
-
-    /** Sole constructor. */
-    public MergeContext(Comparator<BytesRef> comp, int mergeDocCount,
-        int size, Type type) {
-      assert type == Type.BYTES_FIXED_SORTED || type == Type.BYTES_VAR_SORTED;
-      this.comp = comp;
-      this.sizePerValues = size;
-      this.type = type;
-      if (size > 0) {
-        missingValue.grow(size);
-        missingValue.length = size;
-      }
-      docToEntry = new int[mergeDocCount];
-    }
-
-    /** Returns number of documents merged. */
-    public int getMergeDocCount() {
-      return docToEntry.length;
-    }
-  }
-
-  /** Creates the {@link SortedSourceSlice}s for
-   *  merging. */
-  public static List<SortedSourceSlice> buildSlices(
-      int[] docBases, MergeState.DocMap[] docMaps,
-      DocValues[] docValues, MergeContext ctx) throws IOException {
-    final List<SortedSourceSlice> slices = new ArrayList<SortedSourceSlice>();
-    for (int i = 0; i < docValues.length; i++) {
-      final SortedSourceSlice nextSlice;
-      final Source directSource;
-      if (docValues[i] != null
-          && (directSource = docValues[i].getDirectSource()) != null) {
-        final SortedSourceSlice slice = new SortedSourceSlice(i, directSource
-            .asSortedSource(), docBases, ctx.getMergeDocCount(), ctx.docToEntry);
-        nextSlice = slice;
-      } else {
-        nextSlice = new SortedSourceSlice(i, new MissingValueSource(ctx),
-            docBases, ctx.getMergeDocCount(), ctx.docToEntry);
-      }
-      createOrdMapping(docBases, docMaps, nextSlice);
-      slices.add(nextSlice);
-    }
-    return Collections.unmodifiableList(slices);
-  }
-
-  /*
-   * In order to merge we need to map the ords used in each segment to the new
-   * global ords in the new segment. Additionally we need to drop values that
-   * are not referenced anymore due to deleted documents. This method walks all
-   * live documents and fetches their current ordinal. We store this ordinal per
-   * slice and (SortedSourceSlice#ordMapping) and remember the doc to ord
-   * mapping in docIDToRelativeOrd. After the merge SortedSourceSlice#ordMapping
-   * contains the new global ordinals for the relative index.
-   */
-  private static void createOrdMapping(int[] docBases, MergeState.DocMap[] docMaps,
-      SortedSourceSlice currentSlice) {
-    final int readerIdx = currentSlice.readerIdx;
-    final MergeState.DocMap currentDocMap = docMaps[readerIdx];
-    final int docBase = currentSlice.docToOrdStart;
-    assert docBase == docBases[readerIdx];
-    if (currentDocMap != null && currentDocMap.hasDeletions()) { // we have deletes
-      for (int i = 0; i < currentDocMap.maxDoc(); i++) {
-        final int doc = currentDocMap.get(i);
-        if (doc != -1) { // not deleted
-          final int ord = currentSlice.source.ord(i); // collect ords strictly
-                                                      // increasing
-          currentSlice.docIDToRelativeOrd[docBase + doc] = ord;
-          // use ord + 1 to identify unreferenced values (ie. == 0)
-          currentSlice.ordMapping[ord] = ord + 1;
-        }
-      }
-    } else { // no deletes
-      final int numDocs = currentSlice.docToOrdEnd - currentSlice.docToOrdStart;
-      for (int doc = 0; doc < numDocs; doc++) {
-        final int ord = currentSlice.source.ord(doc);
-        currentSlice.docIDToRelativeOrd[docBase + doc] = ord;
-        // use ord + 1 to identify unreferenced values (ie. == 0)
-        currentSlice.ordMapping[ord] = ord + 1;
-      }
-    }
-  }
-
-  /** Does the "real work" of merging the slices and
-   *  computing the ord mapping. */
-  public static int mergeRecords(MergeContext ctx, BytesRefConsumer consumer,
-      List<SortedSourceSlice> slices) throws IOException {
-    final RecordMerger merger = new RecordMerger(new MergeQueue(slices.size(),
-        ctx.comp), slices.toArray(new SortedSourceSlice[0]));
-    long[] offsets = ctx.offsets;
-    final boolean recordOffsets = offsets != null;
-    long offset = 0;
-    BytesRef currentMergedBytes;
-    merger.pushTop();
-    while (merger.queue.size() > 0) {
-      merger.pullTop();
-      currentMergedBytes = merger.current;
-      assert ctx.sizePerValues == -1 || ctx.sizePerValues == currentMergedBytes.length : "size: "
-          + ctx.sizePerValues + " spare: " + currentMergedBytes.length;
-      offset += currentMergedBytes.length;
-      if (recordOffsets) {
-        if (merger.currentOrd >= offsets.length) {
-          offsets = ArrayUtil.grow(offsets, merger.currentOrd + 1);
-        }
-        offsets[merger.currentOrd] = offset;
-      }
-      consumer.consume(currentMergedBytes, merger.currentOrd, offset);
-      merger.pushTop();
-    }
-    ctx.offsets = offsets;
-    assert offsets == null || offsets[merger.currentOrd - 1] == offset;
-    return merger.currentOrd;
-  }
-  
-  /**
-   * Implementation of this interface consume the merged bytes with their
-   * corresponding ordinal and byte offset. The offset is the byte offset in
-   * target sorted source where the currently merged {@link BytesRef} instance
-   * should be stored at.
-   */
-  public static interface BytesRefConsumer {
-    
-    /**
-     * Consumes a single {@link BytesRef}. The provided {@link BytesRef}
-     * instances are strictly increasing with respect to the used
-     * {@link Comparator} used for merging
-     * 
-     * @param ref
-     *          the {@link BytesRef} to consume
-     * @param ord
-     *          the ordinal of the given {@link BytesRef} in the merge target
-     * @param offset
-     *          the byte offset of the given {@link BytesRef} in the merge
-     *          target
-     * @throws IOException
-     *           if an {@link IOException} occurs
-     */
-    public void consume(BytesRef ref, int ord, long offset) throws IOException;
-  }
-  
-  /**
-   * A simple {@link BytesRefConsumer} that writes the merged {@link BytesRef}
-   * instances sequentially to an {@link IndexOutput}.
-   */
-  public static final class IndexOutputBytesRefConsumer implements BytesRefConsumer {
-    private final IndexOutput datOut;
-    
-    /** Sole constructor. */
-    public IndexOutputBytesRefConsumer(IndexOutput datOut) {
-      this.datOut = datOut;
-    }
-
-    @Override
-    public void consume(BytesRef currentMergedBytes, int ord, long offset) throws IOException {
-      datOut.writeBytes(currentMergedBytes.bytes, currentMergedBytes.offset,
-          currentMergedBytes.length);      
-    }
-  }
-  
-  /**
-   * {@link RecordMerger} merges a list of {@link SortedSourceSlice} lazily by
-   * consuming the sorted source records one by one and de-duplicates records
-   * that are shared across slices. The algorithm is based on a lazy priority queue
-   * that prevents reading merge sources into heap memory. 
-   * 
-   * @lucene.internal
-   */
-  private static final class RecordMerger {
-    private final MergeQueue queue;
-    private final SortedSourceSlice[] top;
-    private int numTop;
-    BytesRef current;
-    int currentOrd = -1;
-
-    RecordMerger(MergeQueue queue, SortedSourceSlice[] top) {
-      super();
-      this.queue = queue;
-      this.top = top;
-      this.numTop = top.length;
-    }
-
-    private void pullTop() {
-      // extract all subs from the queue that have the same
-      // top record
-      assert numTop == 0;
-      assert currentOrd >= 0;
-      while (true) {
-        final SortedSourceSlice popped = top[numTop++] = queue.pop();
-        // use ord + 1 to identify unreferenced values (ie. == 0)
-        popped.ordMapping[popped.relativeOrd] = currentOrd + 1;
-        if (queue.size() == 0
-            || !(queue.top()).current.bytesEquals(top[0].current)) {
-          break;
-        }
-      }
-      current = top[0].current;
-    }
-
-    private void pushTop() {
-      // call next() on each top, and put back into queue
-      for (int i = 0; i < numTop; i++) {
-        top[i].current = top[i].next();
-        if (top[i].current != null) {
-          queue.add(top[i]);
-        }
-      }
-      currentOrd++;
-      numTop = 0;
-    }
-  }
-
-  /**
-   * {@link SortedSourceSlice} represents a single {@link SortedSource} merge candidate.
-   * It encapsulates ordinal and pre-calculated target doc id to ordinal mappings.
-   * This class also holds state private to the merge process.
-   * @lucene.internal
-   */
-  public static class SortedSourceSlice {
-    final SortedSource source;
-    final int readerIdx;
-    /* global array indexed by docID containg the relative ord for the doc */
-    final int[] docIDToRelativeOrd;
-    /*
-     * maps relative ords to merged global ords - index is relative ord value
-     * new global ord this map gets updates as we merge ords. later we use the
-     * docIDtoRelativeOrd to get the previous relative ord to get the new ord
-     * from the relative ord map.
-     */
-    final int[] ordMapping;
-
-    /* start index into docIDToRelativeOrd */
-    final int docToOrdStart;
-    /* end index into docIDToRelativeOrd */
-    final int docToOrdEnd;
-    BytesRef current = new BytesRef();
-    /* the currently merged relative ordinal */
-    int relativeOrd = -1;
-
-    SortedSourceSlice(int readerIdx, SortedSource source, int[] docBase, int mergeDocCount,
-        int[] docToOrd) {
-      super();
-      this.readerIdx = readerIdx;
-      this.source = source;
-      this.docIDToRelativeOrd = docToOrd;
-      this.ordMapping = new int[source.getValueCount()];
-      this.docToOrdStart = docBase[readerIdx];
-      this.docToOrdEnd = this.docToOrdStart + numDocs(docBase, mergeDocCount, readerIdx);
-    }
-
-    private static int numDocs(int[] docBase, int mergedDocCount, int readerIndex) {
-      if (readerIndex == docBase.length - 1) {
-        return mergedDocCount - docBase[readerIndex];
-      }
-      return docBase[readerIndex + 1] - docBase[readerIndex];
-    }
-
-    BytesRef next() {
-      for (int i = relativeOrd + 1; i < ordMapping.length; i++) {
-        if (ordMapping[i] != 0) { // skip ords that are not referenced anymore
-          source.getByOrd(i, current);
-          relativeOrd = i;
-          return current;
-        }
-      }
-      return null;
-    }
-
-    /** Fills in the absolute ords for this slice. 
-     * 
-     * @return the provided {@code docToOrd} */
-    public int[] toAbsolutOrds(int[] docToOrd) {
-      for (int i = docToOrdStart; i < docToOrdEnd; i++) {
-        final int mappedOrd = docIDToRelativeOrd[i];
-        assert mappedOrd < ordMapping.length;
-        assert ordMapping[mappedOrd] > 0 : "illegal mapping ord maps to an unreferenced value";
-        docToOrd[i] = ordMapping[mappedOrd] -1;
-      }
-      return docToOrd;
-    }
-
-    /** Writes ords for this slice. */
-    public void writeOrds(PackedInts.Writer writer) throws IOException {
-      for (int i = docToOrdStart; i < docToOrdEnd; i++) {
-        final int mappedOrd = docIDToRelativeOrd[i];
-        assert mappedOrd < ordMapping.length;
-        assert ordMapping[mappedOrd] > 0 : "illegal mapping ord maps to an unreferenced value";
-        writer.add(ordMapping[mappedOrd] - 1);
-      }
-    }
-  }
-
-  /*
-   * if a segment has no values at all we use this source to fill in the missing
-   * value in the right place (depending on the comparator used)
-   */
-  private static final class MissingValueSource extends SortedSource {
-
-    private BytesRef missingValue;
-
-    public MissingValueSource(MergeContext ctx) {
-      super(ctx.type, ctx.comp);
-      this.missingValue = ctx.missingValue;
-    }
-
-    @Override
-    public int ord(int docID) {
-      return 0;
-    }
-
-    @Override
-    public BytesRef getByOrd(int ord, BytesRef bytesRef) {
-      bytesRef.copyBytes(missingValue);
-      return bytesRef;
-    }
-
-    @Override
-    public PackedInts.Reader getDocToOrd() {
-      return null;
-    }
-
-    @Override
-    public int getValueCount() {
-      return 1;
-    }
-
-  }
-
-  /*
-   * merge queue
-   */
-  private static final class MergeQueue extends
-      PriorityQueue<SortedSourceSlice> {
-    final Comparator<BytesRef> comp;
-
-    public MergeQueue(int maxSize, Comparator<BytesRef> comp) {
-      super(maxSize);
-      this.comp = comp;
-    }
-
-    @Override
-    protected boolean lessThan(SortedSourceSlice a, SortedSourceSlice b) {
-      int cmp = comp.compare(a.current, b.current);
-      if (cmp != 0) {
-        return cmp < 0;
-      } else { // just a tie-breaker
-        return a.docToOrdStart < b.docToOrdStart;
-      }
-    }
-
-  }
-}
Index: lucene/core/src/java/org/apache/lucene/index/SegmentMerger.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/index/SegmentMerger.java	(revision 1442822)
+++ lucene/core/src/java/org/apache/lucene/index/SegmentMerger.java	(working copy)
@@ -26,9 +26,10 @@
 import org.apache.lucene.codecs.Codec;
 import org.apache.lucene.codecs.FieldInfosWriter;
 import org.apache.lucene.codecs.FieldsConsumer;
-import org.apache.lucene.codecs.PerDocConsumer;
+import org.apache.lucene.codecs.DocValuesConsumer;
 import org.apache.lucene.codecs.StoredFieldsWriter;
 import org.apache.lucene.codecs.TermVectorsWriter;
+import org.apache.lucene.index.FieldInfo.DocValuesType;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.IOContext;
 import org.apache.lucene.util.IOUtils;
@@ -70,14 +71,14 @@
   /**
    * Add an IndexReader to the collection of readers that are to be merged
    */
-  final void add(IndexReader reader) {
+  void add(IndexReader reader) {
     for (final AtomicReaderContext ctx : reader.leaves()) {
       final AtomicReader r = ctx.reader();
       mergeState.readers.add(r);
     }
   }
 
-  final void add(SegmentReader reader) {
+  void add(SegmentReader reader) {
     mergeState.readers.add(reader);
   }
 
@@ -87,7 +88,7 @@
    * @throws CorruptIndexException if the index is corrupt
    * @throws IOException if there is a low-level IO error
    */
-  final MergeState merge() throws IOException {
+  MergeState merge() throws IOException {
     // NOTE: it's important to add calls to
     // checkAbort.work(...) if you make any changes to this
     // method that will spend alot of time.  The frequency
@@ -96,7 +97,7 @@
     // threads.
     
     mergeState.segmentInfo.setDocCount(setDocMaps());
-    mergeDocValuesAndNormsFieldInfos();
+    mergeFieldInfos();
     setMatchingSegmentReaders();
     long t0 = 0;
     if (mergeState.infoStream.isEnabled("SM")) {
@@ -123,7 +124,9 @@
     if (mergeState.infoStream.isEnabled("SM")) {
       t0 = System.nanoTime();
     }
-    mergePerDoc(segmentWriteState);
+    if (mergeState.fieldInfos.hasDocValues()) {
+      mergeSimpleDocValues(segmentWriteState);
+    }
     if (mergeState.infoStream.isEnabled("SM")) {
       long t1 = System.nanoTime();
       mergeState.infoStream.message("SM", ((t1-t0)/1000000) + " msec to merge doc values [" + numMerged + " docs]");
@@ -133,7 +136,7 @@
       if (mergeState.infoStream.isEnabled("SM")) {
         t0 = System.nanoTime();
       }
-      mergeNorms(segmentWriteState);
+      mergeSimpleNorms(segmentWriteState);
       if (mergeState.infoStream.isEnabled("SM")) {
         long t1 = System.nanoTime();
         mergeState.infoStream.message("SM", ((t1-t0)/1000000) + " msec to merge norms [" + numMerged + " docs]");
@@ -159,6 +162,90 @@
     return mergeState;
   }
 
+  private void mergeSimpleDocValues(SegmentWriteState segmentWriteState) throws IOException {
+
+    if (codec.docValuesFormat() != null) {
+      DocValuesConsumer consumer = codec.docValuesFormat().fieldsConsumer(segmentWriteState);
+      boolean success = false;
+      try {
+        for (FieldInfo field : mergeState.fieldInfos) {
+          DocValuesType type = field.getDocValuesType();
+          if (type != null) {
+            if (type == DocValuesType.NUMERIC) {
+              List<NumericDocValues> toMerge = new ArrayList<NumericDocValues>();
+              for (AtomicReader reader : mergeState.readers) {
+                NumericDocValues values = reader.getNumericDocValues(field.name);
+                if (values == null) {
+                  values = NumericDocValues.EMPTY;
+                }
+                toMerge.add(values);
+              }
+              consumer.mergeNumericField(field, mergeState, toMerge);
+            } else if (type == DocValuesType.BINARY) {
+              List<BinaryDocValues> toMerge = new ArrayList<BinaryDocValues>();
+              for (AtomicReader reader : mergeState.readers) {
+                BinaryDocValues values = reader.getBinaryDocValues(field.name);
+                if (values == null) {
+                  values = BinaryDocValues.EMPTY;
+                }
+                toMerge.add(values);
+              }
+              consumer.mergeBinaryField(field, mergeState, toMerge);
+            } else if (type == DocValuesType.SORTED) {
+              List<SortedDocValues> toMerge = new ArrayList<SortedDocValues>();
+              for (AtomicReader reader : mergeState.readers) {
+                SortedDocValues values = reader.getSortedDocValues(field.name);
+                if (values == null) {
+                  values = SortedDocValues.EMPTY;
+                }
+                toMerge.add(values);
+              }
+              consumer.mergeSortedField(field, mergeState, toMerge);
+            } else {
+              throw new AssertionError("type=" + type);
+            }
+          }
+        }
+        success = true;
+      } finally {
+        if (success) {
+          IOUtils.close(consumer);
+        } else {
+          IOUtils.closeWhileHandlingException(consumer);            
+        }
+      }
+    }
+  }
+
+  private void mergeSimpleNorms(SegmentWriteState segmentWriteState) throws IOException {
+    if (codec.normsFormat() != null) {
+      DocValuesConsumer consumer = codec.normsFormat().normsConsumer(segmentWriteState);
+      boolean success = false;
+      try {
+        for (FieldInfo field : mergeState.fieldInfos) {
+          if (field.hasNorms()) {
+            List<NumericDocValues> toMerge = new ArrayList<NumericDocValues>();
+            for (AtomicReader reader : mergeState.readers) {
+              NumericDocValues norms = reader.getNormValues(field.name);
+              if (norms == null) {
+                norms = NumericDocValues.EMPTY;
+              }
+              toMerge.add(norms);
+            }
+            consumer.mergeNumericField(field, mergeState, toMerge);
+          }
+        }
+        success = true;
+      } finally {
+        if (success) {
+          IOUtils.close(consumer);
+        } else {
+          IOUtils.closeWhileHandlingException(consumer);            
+        }
+      }
+    }
+  }
+
   private void setMatchingSegmentReaders() {
     // If the i'th reader is a SegmentReader and has
     // identical fieldName -> number mapping, then this
@@ -203,73 +290,16 @@
     }
   }
   
-  // returns an updated typepromoter (tracking type and size) given a previous one,
-  // and a newly encountered docvalues
-  private TypePromoter mergeDocValuesType(TypePromoter previous, DocValues docValues) {
-    TypePromoter incoming = TypePromoter.create(docValues.getType(),  docValues.getValueSize());
-    if (previous == null) {
-      previous = TypePromoter.getIdentityPromoter();
-    }
-    return previous.promote(incoming);
-  }
-
-  // NOTE: this is actually merging all the fieldinfos
-  public void mergeDocValuesAndNormsFieldInfos() throws IOException {
-    // mapping from all docvalues fields found to their promoted types
-    // this is because FieldInfos does not store the
-    // valueSize
-    Map<FieldInfo,TypePromoter> docValuesTypes = new HashMap<FieldInfo,TypePromoter>();
-    Map<FieldInfo,TypePromoter> normValuesTypes = new HashMap<FieldInfo,TypePromoter>();
-
+  public void mergeFieldInfos() throws IOException {
     for (AtomicReader reader : mergeState.readers) {
       FieldInfos readerFieldInfos = reader.getFieldInfos();
       for (FieldInfo fi : readerFieldInfos) {
-        FieldInfo merged = fieldInfosBuilder.add(fi);
-        // update the type promotion mapping for this reader
-        if (fi.hasDocValues()) {
-          TypePromoter previous = docValuesTypes.get(merged);
-          docValuesTypes.put(merged, mergeDocValuesType(previous, reader.docValues(fi.name))); 
-        }
-        if (fi.hasNorms()) {
-          TypePromoter previous = normValuesTypes.get(merged);
-          normValuesTypes.put(merged, mergeDocValuesType(previous, reader.normValues(fi.name))); 
-        }
+        fieldInfosBuilder.add(fi);
       }
     }
-    updatePromoted(normValuesTypes, true);
-    updatePromoted(docValuesTypes, false);
     mergeState.fieldInfos = fieldInfosBuilder.finish();
   }
-  
-  protected void updatePromoted(Map<FieldInfo,TypePromoter> infoAndPromoter, boolean norms) {
-    // update any promoted doc values types:
-    for (Map.Entry<FieldInfo,TypePromoter> e : infoAndPromoter.entrySet()) {
-      FieldInfo fi = e.getKey();
-      TypePromoter promoter = e.getValue();
-      if (promoter == null) {
-        if (norms) {
-          fi.setNormValueType(null);
-        } else {
-          fi.setDocValuesType(null);
-        }
-      } else {
-        assert promoter != TypePromoter.getIdentityPromoter();
-        if (norms) {
-          if (fi.getNormType() != promoter.type() && !fi.omitsNorms()) {
-            // reset the type if we got promoted
-            fi.setNormValueType(promoter.type());
-          }  
-        } else {
-          if (fi.getDocValuesType() != promoter.type()) {
-            // reset the type if we got promoted
-            fi.setDocValuesType(promoter.type());
-          }
-        }
-      }
-    }
-  }
 
-
   /**
    *
    * @return The number of documents in all of the readers
@@ -290,7 +320,7 @@
    * Merge the TermVectors from each of the segments into the new one.
    * @throws IOException if there is a low-level IO error
    */
-  private final int mergeVectors() throws IOException {
+  private int mergeVectors() throws IOException {
     final TermVectorsWriter termVectorsWriter = codec.termVectorsFormat().vectorsWriter(directory, mergeState.segmentInfo, context);
     
     try {
@@ -326,7 +356,7 @@
     return docBase;
   }
 
-  private final void mergeTerms(SegmentWriteState segmentWriteState) throws IOException {
+  private void mergeTerms(SegmentWriteState segmentWriteState) throws IOException {
     
     final List<Fields> fields = new ArrayList<Fields>();
     final List<ReaderSlice> slices = new ArrayList<ReaderSlice>();
@@ -359,38 +389,4 @@
       }
     }
   }
-
-  private void mergePerDoc(SegmentWriteState segmentWriteState) throws IOException {
-      final PerDocConsumer docsConsumer = codec.docValuesFormat()
-          .docsConsumer(new PerDocWriteState(segmentWriteState));
-      assert docsConsumer != null;
-      boolean success = false;
-      try {
-        docsConsumer.merge(mergeState);
-        success = true;
-      } finally {
-        if (success) {
-          IOUtils.close(docsConsumer);
-        } else {
-          IOUtils.closeWhileHandlingException(docsConsumer);
-        }
-      }
-  }
-  
-  private void mergeNorms(SegmentWriteState segmentWriteState) throws IOException {
-    final PerDocConsumer docsConsumer = codec.normsFormat()
-        .docsConsumer(new PerDocWriteState(segmentWriteState));
-    assert docsConsumer != null;
-    boolean success = false;
-    try {
-      docsConsumer.merge(mergeState);
-      success = true;
-    } finally {
-      if (success) {
-        IOUtils.close(docsConsumer);
-      } else {
-        IOUtils.closeWhileHandlingException(docsConsumer);
-      }
-    }
-  }
 }
Index: lucene/core/src/java/org/apache/lucene/index/MultiTermsEnum.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/index/MultiTermsEnum.java	(revision 1442822)
+++ lucene/core/src/java/org/apache/lucene/index/MultiTermsEnum.java	(working copy)
@@ -495,7 +495,7 @@
     }
   }
 
-  private final static class TermsEnumWithSlice {
+  final static class TermsEnumWithSlice {
     private final ReaderSlice subSlice;
     private TermsEnum terms;
     public BytesRef current;
Index: lucene/core/src/java/org/apache/lucene/index/CheckIndex.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/index/CheckIndex.java	(revision 1442822)
+++ lucene/core/src/java/org/apache/lucene/index/CheckIndex.java	(working copy)
@@ -31,10 +31,7 @@
 import org.apache.lucene.codecs.BlockTreeTermsReader;
 import org.apache.lucene.codecs.Codec;
 import org.apache.lucene.codecs.PostingsFormat; // javadocs
-import org.apache.lucene.document.Document;
 import org.apache.lucene.document.FieldType; // for javadocs
-import org.apache.lucene.index.DocValues.SortedSource;
-import org.apache.lucene.index.DocValues.Source;
 import org.apache.lucene.index.FieldInfo.IndexOptions;
 import org.apache.lucene.search.DocIdSetIterator;
 import org.apache.lucene.store.Directory;
@@ -183,7 +180,7 @@
       /** Number of deleted documents. */
       public int numDeleted;
 
-      /** True if we were able to open a SegmentReader on this
+      /** True if we were able to open an AtomicReader on this
        *  segment. */
       public boolean openReaderPassed;
 
@@ -343,9 +340,9 @@
     setInfoStream(out, false);
   }
 
-  private void msg(String msg) {
-    if (infoStream != null)
-      infoStream.println(msg);
+  private static void msg(PrintStream out, String msg) {
+    if (out != null)
+      out.println(msg);
   }
 
   /** Returns a {@link Status} instance detailing
@@ -381,7 +378,7 @@
     try {
       sis.read(dir);
     } catch (Throwable t) {
-      msg("ERROR: could not read any segments file in directory");
+      msg(infoStream, "ERROR: could not read any segments file in directory");
       result.missingSegments = true;
       if (infoStream != null)
         t.printStackTrace(infoStream);
@@ -416,7 +413,7 @@
     try {
       input = dir.openInput(segmentsFileName, IOContext.DEFAULT);
     } catch (Throwable t) {
-      msg("ERROR: could not open segments file in directory");
+      msg(infoStream, "ERROR: could not open segments file in directory");
       if (infoStream != null)
         t.printStackTrace(infoStream);
       result.cantOpenSegments = true;
@@ -426,7 +423,7 @@
     try {
       format = input.readInt();
     } catch (Throwable t) {
-      msg("ERROR: could not read segment file version in directory");
+      msg(infoStream, "ERROR: could not read segment file version in directory");
       if (infoStream != null)
         t.printStackTrace(infoStream);
       result.missingSegmentVersion = true;
@@ -460,7 +457,7 @@
       versionString = oldest.equals(newest) ? ( "version=" + oldest ) : ("versions=[" + oldest + " .. " + newest + "]");
     }
 
-    msg("Segments file=" + segmentsFileName + " numSegments=" + numSegments
+    msg(infoStream, "Segments file=" + segmentsFileName + " numSegments=" + numSegments
         + " " + versionString + " format=" + sFormat + userDataString);
 
     if (onlySegments != null) {
@@ -472,11 +469,11 @@
           infoStream.print(" " + s);
       }
       result.segmentsChecked.addAll(onlySegments);
-      msg(":");
+      msg(infoStream, ":");
     }
 
     if (skip) {
-      msg("\nERROR: this index appears to be created by a newer version of Lucene than this tool was compiled on; please re-compile this tool on the matching version of Lucene; exiting");
+      msg(infoStream, "\nERROR: this index appears to be created by a newer version of Lucene than this tool was compiled on; please re-compile this tool on the matching version of Lucene; exiting");
       result.toolOutOfDate = true;
       return result;
     }
@@ -497,38 +494,41 @@
       }
       Status.SegmentInfoStatus segInfoStat = new Status.SegmentInfoStatus();
       result.segmentInfos.add(segInfoStat);
-      msg("  " + (1+i) + " of " + numSegments + ": name=" + info.info.name + " docCount=" + info.info.getDocCount());
+      msg(infoStream, "  " + (1+i) + " of " + numSegments + ": name=" + info.info.name + " docCount=" + info.info.getDocCount());
       segInfoStat.name = info.info.name;
       segInfoStat.docCount = info.info.getDocCount();
 
       int toLoseDocCount = info.info.getDocCount();
 
-      SegmentReader reader = null;
+      AtomicReader reader = null;
 
       try {
         final Codec codec = info.info.getCodec();
-        msg("    codec=" + codec);
+        msg(infoStream, "    codec=" + codec);
         segInfoStat.codec = codec;
-        msg("    compound=" + info.info.getUseCompoundFile());
+        msg(infoStream, "    compound=" + info.info.getUseCompoundFile());
         segInfoStat.compound = info.info.getUseCompoundFile();
-        msg("    numFiles=" + info.files().size());
+        msg(infoStream, "    numFiles=" + info.files().size());
         segInfoStat.numFiles = info.files().size();
         segInfoStat.sizeMB = info.sizeInBytes()/(1024.*1024.);
-        msg("    size (MB)=" + nf.format(segInfoStat.sizeMB));
+        msg(infoStream, "    size (MB)=" + nf.format(segInfoStat.sizeMB));
         Map<String,String> diagnostics = info.info.getDiagnostics();
         segInfoStat.diagnostics = diagnostics;
         if (diagnostics.size() > 0) {
-          msg("    diagnostics = " + diagnostics);
+          msg(infoStream, "    diagnostics = " + diagnostics);
         }
 
-        // TODO: we could append the info attributes() to the msg?
+        Map<String,String> atts = info.info.attributes();
+        if (atts != null && !atts.isEmpty()) {
+          msg(infoStream, "    attributes = " + atts);
+        }
 
         if (!info.hasDeletions()) {
-          msg("    no deletions");
+          msg(infoStream, "    no deletions");
           segInfoStat.hasDeletions = false;
         }
         else{
-          msg("    has deletions [delGen=" + info.getDelGen() + "]");
+          msg(infoStream, "    has deletions [delGen=" + info.getDelGen() + "]");
           segInfoStat.hasDeletions = true;
           segInfoStat.deletionsGen = info.getDelGen();
         }
@@ -566,7 +566,7 @@
           }
           
           segInfoStat.numDeleted = info.info.getDocCount() - numDocs;
-          msg("OK [" + (segInfoStat.numDeleted) + " deleted docs]");
+          msg(infoStream, "OK [" + (segInfoStat.numDeleted) + " deleted docs]");
         } else {
           if (info.getDelCount() != 0) {
             throw new RuntimeException("delete count mismatch: info=" + info.getDelCount() + " vs reader=" + (info.info.getDocCount() - numDocs));
@@ -580,7 +580,7 @@
               }
             }
           }
-          msg("OK");
+          msg(infoStream, "OK");
         }
         if (reader.maxDoc() != info.info.getDocCount()) {
           throw new RuntimeException("SegmentReader.maxDoc() " + reader.maxDoc() + " != SegmentInfos.docCount " + info.info.getDocCount());
@@ -591,23 +591,23 @@
           infoStream.print("    test: fields..............");
         }         
         FieldInfos fieldInfos = reader.getFieldInfos();
-        msg("OK [" + fieldInfos.size() + " fields]");
+        msg(infoStream, "OK [" + fieldInfos.size() + " fields]");
         segInfoStat.numFields = fieldInfos.size();
         
         // Test Field Norms
-        segInfoStat.fieldNormStatus = testFieldNorms(fieldInfos, reader);
+        segInfoStat.fieldNormStatus = testFieldNorms(reader, infoStream);
 
         // Test the Term Index
-        segInfoStat.termIndexStatus = testPostings(fieldInfos, reader);
+        segInfoStat.termIndexStatus = testPostings(reader, infoStream, verbose);
 
         // Test Stored Fields
-        segInfoStat.storedFieldStatus = testStoredFields(info, reader, nf);
+        segInfoStat.storedFieldStatus = testStoredFields(reader, infoStream);
 
         // Test Term Vectors
-        segInfoStat.termVectorStatus = testTermVectors(fieldInfos, info, reader, nf);
-        
-        segInfoStat.docValuesStatus = testDocValues(info, fieldInfos, reader);
+        segInfoStat.termVectorStatus = testTermVectors(reader, infoStream, verbose, crossCheckTermVectors);
 
+        segInfoStat.docValuesStatus = testDocValues(reader, infoStream);
+
         // Rethrow the first exception we encountered
         //  This will cause stats for failed segments to be incremented properly
         if (segInfoStat.fieldNormStatus.error != null) {
@@ -622,16 +622,16 @@
           throw new RuntimeException("DocValues test failed");
         }
 
-        msg("");
+        msg(infoStream, "");
 
       } catch (Throwable t) {
-        msg("FAILED");
+        msg(infoStream, "FAILED");
         String comment;
         comment = "fixIndex() would remove reference to this segment";
-        msg("    WARNING: " + comment + "; full exception:");
+        msg(infoStream, "    WARNING: " + comment + "; full exception:");
         if (infoStream != null)
           t.printStackTrace(infoStream);
-        msg("");
+        msg(infoStream, "");
         result.totLoseDocCount += toLoseDocCount;
         result.numBadSegments++;
         continue;
@@ -647,16 +647,16 @@
     if (0 == result.numBadSegments) {
       result.clean = true;
     } else
-      msg("WARNING: " + result.numBadSegments + " broken segments (containing " + result.totLoseDocCount + " documents) detected");
+      msg(infoStream, "WARNING: " + result.numBadSegments + " broken segments (containing " + result.totLoseDocCount + " documents) detected");
 
     if ( ! (result.validCounter = (result.maxSegmentName < sis.counter))) {
       result.clean = false;
       result.newSegments.counter = result.maxSegmentName + 1; 
-      msg("ERROR: Next segment name counter " + sis.counter + " is not greater than max segment name " + result.maxSegmentName);
+      msg(infoStream, "ERROR: Next segment name counter " + sis.counter + " is not greater than max segment name " + result.maxSegmentName);
     }
     
     if (result.clean) {
-      msg("No problems were detected with this index.\n");
+      msg(infoStream, "No problems were detected with this index.\n");
     }
 
     return result;
@@ -664,8 +664,9 @@
 
   /**
    * Test field norms.
+   * @lucene.experimental
    */
-  private Status.FieldNormStatus testFieldNorms(FieldInfos fieldInfos, SegmentReader reader) {
+  public static Status.FieldNormStatus testFieldNorms(AtomicReader reader, PrintStream infoStream) {
     final Status.FieldNormStatus status = new Status.FieldNormStatus();
 
     try {
@@ -673,21 +674,20 @@
       if (infoStream != null) {
         infoStream.print("    test: field norms.........");
       }
-      for (FieldInfo info : fieldInfos) {
+      for (FieldInfo info : reader.getFieldInfos()) {
         if (info.hasNorms()) {
-          DocValues dv = reader.normValues(info.name);
-          checkDocValues(dv, info.name, info.getNormType(), reader.maxDoc());
+          checkNorms(info, reader, infoStream);
           ++status.totFields;
         } else {
-          if (reader.normValues(info.name) != null) {
+          if (reader.getNormValues(info.name) != null) {
             throw new RuntimeException("field: " + info.name + " should omit norms but has them!");
           }
         }
       }
 
-      msg("OK [" + status.totFields + " fields]");
+      msg(infoStream, "OK [" + status.totFields + " fields]");
     } catch (Throwable e) {
-      msg("ERROR [" + String.valueOf(e.getMessage()) + "]");
+      msg(infoStream, "ERROR [" + String.valueOf(e.getMessage()) + "]");
       status.error = e;
       if (infoStream != null) {
         e.printStackTrace(infoStream);
@@ -701,14 +701,14 @@
    * checks Fields api is consistent with itself.
    * searcher is optional, to verify with queries. Can be null.
    */
-  private Status.TermIndexStatus checkFields(Fields fields, Bits liveDocs, int maxDoc, FieldInfos fieldInfos, boolean doPrint, boolean isVectors) throws IOException {
+  private static Status.TermIndexStatus checkFields(Fields fields, Bits liveDocs, int maxDoc, FieldInfos fieldInfos, boolean doPrint, boolean isVectors, PrintStream infoStream, boolean verbose) throws IOException {
     // TODO: we should probably return our own stats thing...?!
     
     final Status.TermIndexStatus status = new Status.TermIndexStatus();
     int computedFieldCount = 0;
     
     if (fields == null) {
-      msg("OK [no fields/terms]");
+      msg(infoStream, "OK [no fields/terms]");
       return status;
     }
     
@@ -769,7 +769,7 @@
           break;
         }
         
-        checkBounds(term);
+        assert term.isValid();
         
         // make sure terms arrive in order according to
         // the comp
@@ -859,7 +859,7 @@
               lastPos = pos;
               BytesRef payload = postings.getPayload();
               if (payload != null) {
-                checkBounds(payload);
+                assert payload.isValid();
               }
               if (payload != null && payload.length < 1) {
                 throw new RuntimeException("term " + term + ": doc " + doc + ": pos " + pos + " payload length is out of bounds " + payload.length);
@@ -1155,7 +1155,7 @@
     }
 
     if (doPrint) {
-      msg("OK [" + status.termCount + " terms; " + status.totFreq + " terms/docs pairs; " + status.totPos + " tokens]");
+      msg(infoStream, "OK [" + status.termCount + " terms; " + status.totFreq + " terms/docs pairs; " + status.totPos + " tokens]");
     }
     
     if (verbose && status.blockTreeStats != null && infoStream != null && status.termCount > 0) {
@@ -1170,8 +1170,17 @@
 
   /**
    * Test the term index.
+   * @lucene.experimental
    */
-  private Status.TermIndexStatus testPostings(FieldInfos fieldInfos, SegmentReader reader) {
+  public static Status.TermIndexStatus testPostings(AtomicReader reader, PrintStream infoStream) {
+    return testPostings(reader, infoStream, false);
+  }
+  
+  /**
+   * Test the term index.
+   * @lucene.experimental
+   */
+  public static Status.TermIndexStatus testPostings(AtomicReader reader, PrintStream infoStream, boolean verbose) {
 
     // TODO: we should go and verify term vectors match, if
     // crossCheckTermVectors is on...
@@ -1186,15 +1195,16 @@
       }
 
       final Fields fields = reader.fields();
-      status = checkFields(fields, liveDocs, maxDoc, fieldInfos, true, false);
+      final FieldInfos fieldInfos = reader.getFieldInfos();
+      status = checkFields(fields, liveDocs, maxDoc, fieldInfos, true, false, infoStream, verbose);
       if (liveDocs != null) {
         if (infoStream != null) {
           infoStream.print("    test (ignoring deletes): terms, freq, prox...");
         }
-        checkFields(fields, null, maxDoc, fieldInfos, true, false);
+        checkFields(fields, null, maxDoc, fieldInfos, true, false, infoStream, verbose);
       }
     } catch (Throwable e) {
-      msg("ERROR: " + e);
+      msg(infoStream, "ERROR: " + e);
       status = new Status.TermIndexStatus();
       status.error = e;
       if (infoStream != null) {
@@ -1206,9 +1216,10 @@
   }
   
   /**
-   * Test stored fields for a segment.
+   * Test stored fields.
+   * @lucene.experimental
    */
-  private Status.StoredFieldStatus testStoredFields(SegmentInfoPerCommit info, SegmentReader reader, NumberFormat format) {
+  public static Status.StoredFieldStatus testStoredFields(AtomicReader reader, PrintStream infoStream) {
     final Status.StoredFieldStatus status = new Status.StoredFieldStatus();
 
     try {
@@ -1218,7 +1229,7 @@
 
       // Scan stored fields for all documents
       final Bits liveDocs = reader.getLiveDocs();
-      for (int j = 0; j < info.info.getDocCount(); ++j) {
+      for (int j = 0; j < reader.maxDoc(); ++j) {
         // Intentionally pull even deleted documents to
         // make sure they too are not corrupt:
         StoredDocument doc = reader.document(j);
@@ -1233,10 +1244,10 @@
         throw new RuntimeException("docCount=" + status.docCount + " but saw " + status.docCount + " undeleted docs");
       }
 
-      msg("OK [" + status.totFields + " total field count; avg " + 
-          format.format((((float) status.totFields)/status.docCount)) + " fields per doc]");      
+      msg(infoStream, "OK [" + status.totFields + " total field count; avg " + 
+          NumberFormat.getInstance(Locale.ROOT).format((((float) status.totFields)/status.docCount)) + " fields per doc]");      
     } catch (Throwable e) {
-      msg("ERROR [" + String.valueOf(e.getMessage()) + "]");
+      msg(infoStream, "ERROR [" + String.valueOf(e.getMessage()) + "]");
       status.error = e;
       if (infoStream != null) {
         e.printStackTrace(infoStream);
@@ -1246,124 +1257,33 @@
     return status;
   }
   
-  /** Helper method to verify values (either docvalues or norms), also checking
-   *  type and size against fieldinfos/segmentinfo
+  /**
+   * Test docvalues.
+   * @lucene.experimental
    */
-  private void checkDocValues(DocValues docValues, String fieldName, DocValues.Type expectedType, int expectedDocs) throws IOException {
-    if (docValues == null) {
-      throw new RuntimeException("field: " + fieldName + " omits docvalues but should have them!");
-    }
-    DocValues.Type type = docValues.getType();
-    if (type != expectedType) {
-      throw new RuntimeException("field: " + fieldName + " has type: " + type + " but fieldInfos says:" + expectedType);
-    }
-    final Source values = docValues.getDirectSource();
-    int size = docValues.getValueSize();
-    for (int i = 0; i < expectedDocs; i++) {
-      switch (type) {
-      case BYTES_FIXED_SORTED:
-      case BYTES_VAR_SORTED:
-      case BYTES_FIXED_DEREF:
-      case BYTES_FIXED_STRAIGHT:
-      case BYTES_VAR_DEREF:
-      case BYTES_VAR_STRAIGHT:
-        BytesRef bytes = new BytesRef();
-        values.getBytes(i, bytes);
-        if (size != -1 && size != bytes.length) {
-          throw new RuntimeException("field: " + fieldName + " returned wrongly sized bytes, was: " + bytes.length + " should be: " + size);
-        }
-        break;
-      case FLOAT_32:
-        assert size == 4;
-        values.getFloat(i);
-        break;
-      case FLOAT_64:
-        assert size == 8;
-        values.getFloat(i);
-        break;
-      case VAR_INTS:
-        assert size == -1;
-        values.getInt(i);
-        break;
-      case FIXED_INTS_16:
-        assert size == 2;
-        values.getInt(i);
-        break;
-      case FIXED_INTS_32:
-        assert size == 4;
-        values.getInt(i);
-        break;
-      case FIXED_INTS_64:
-        assert size == 8;
-        values.getInt(i);
-        break;
-      case FIXED_INTS_8:
-        assert size == 1;
-        values.getInt(i);
-        break;
-      default:
-        throw new IllegalArgumentException("Field: " + fieldName
-                    + " - no such DocValues type: " + type);
-      }
-    }
-    if (type == DocValues.Type.BYTES_FIXED_SORTED || type == DocValues.Type.BYTES_VAR_SORTED) {
-      // check sorted bytes
-      SortedSource sortedValues = values.asSortedSource();
-      Comparator<BytesRef> comparator = sortedValues.getComparator();
-      int maxOrd = sortedValues.getValueCount() - 1;
-      FixedBitSet seenOrds = new FixedBitSet(sortedValues.getValueCount());
-      int lastOrd = -1;
-      BytesRef lastBytes = new BytesRef();
-      for (int i = 0; i < expectedDocs; i++) {
-        int ord = sortedValues.ord(i);
-        if (ord < 0 || ord > maxOrd) {
-          throw new RuntimeException("field: " + fieldName + " ord is out of bounds: " + ord);
-        }
-        BytesRef bytes = new BytesRef();
-        sortedValues.getByOrd(ord, bytes);
-        if (lastOrd != -1) {
-          int ordComp = Integer.signum(new Integer(ord).compareTo(new Integer(lastOrd)));
-          int bytesComp = Integer.signum(comparator.compare(bytes, lastBytes));
-          if (ordComp != bytesComp) {
-            throw new RuntimeException("field: " + fieldName + " ord comparison is wrong: " + ordComp + " comparator claims: " + bytesComp);
-          }
-        }
-        lastOrd = ord;
-        lastBytes = bytes;
-        seenOrds.set(ord);
-      }
-      if (seenOrds.cardinality() != sortedValues.getValueCount()) {
-        // TODO: find the bug here and figure out a workaround (we can implement in LUCENE-4547's back compat layer maybe)
-        // basically ord 0 is unused by any docs: so the sortedbytes ords are all off-by-one
-        // does it always happen? e.g. maybe only if there are missing values? or a bug in its merge optimizations?
-        // throw new RuntimeException("dv for field: " + fieldName + " has holes in its ords, valueCount=" + sortedValues.getValueCount() + " but only used: " + seenOrds.cardinality());
-      }
-    }
-  }
-  
-  private Status.DocValuesStatus testDocValues(SegmentInfoPerCommit info,
-                                               FieldInfos fieldInfos,
-                                               SegmentReader reader) {
+  public static Status.DocValuesStatus testDocValues(AtomicReader reader,
+                                                     PrintStream infoStream) {
     final Status.DocValuesStatus status = new Status.DocValuesStatus();
     try {
       if (infoStream != null) {
         infoStream.print("    test: docvalues...........");
       }
-      for (FieldInfo fieldInfo : fieldInfos) {
+      for (FieldInfo fieldInfo : reader.getFieldInfos()) {
         if (fieldInfo.hasDocValues()) {
           status.totalValueFields++;
-          final DocValues docValues = reader.docValues(fieldInfo.name);
-          checkDocValues(docValues, fieldInfo.name, fieldInfo.getDocValuesType(), reader.maxDoc());
+          checkDocValues(fieldInfo, reader, infoStream);
         } else {
-          if (reader.docValues(fieldInfo.name) != null) {
+          if (reader.getBinaryDocValues(fieldInfo.name) != null ||
+              reader.getNumericDocValues(fieldInfo.name) != null ||
+              reader.getSortedDocValues(fieldInfo.name) != null) {
             throw new RuntimeException("field: " + fieldInfo.name + " has docvalues but should omit them!");
           }
         }
       }
 
-      msg("OK [" + status.docCount + " total doc count; " + status.totalValueFields + " docvalues fields]");
+      msg(infoStream, "OK [" + status.docCount + " total doc count; " + status.totalValueFields + " docvalues fields]");
     } catch (Throwable e) {
-      msg("ERROR [" + String.valueOf(e.getMessage()) + "]");
+      msg(infoStream, "ERROR [" + String.valueOf(e.getMessage()) + "]");
       status.error = e;
       if (infoStream != null) {
         e.printStackTrace(infoStream);
@@ -1372,37 +1292,94 @@
     return status;
   }
   
-  // basic value checks
-  private static void checkBounds(BytesRef b) {
-    if (b.bytes == null) {
-      throw new RuntimeException("bytes is null");
+  private static void checkBinaryDocValues(String fieldName, AtomicReader reader, BinaryDocValues dv) {
+    BytesRef scratch = new BytesRef();
+    for (int i = 0; i < reader.maxDoc(); i++) {
+      dv.get(i, scratch);
+      assert scratch.isValid();
     }
-    if (b.length < 0) {
-      throw new RuntimeException("length is negative: " + b.length);
+  }
+  
+  private static void checkSortedDocValues(String fieldName, AtomicReader reader, SortedDocValues dv) {
+    checkBinaryDocValues(fieldName, reader, dv);
+    final int maxOrd = dv.getValueCount()-1;
+    FixedBitSet seenOrds = new FixedBitSet(dv.getValueCount());
+    int maxOrd2 = -1;
+    for (int i = 0; i < reader.maxDoc(); i++) {
+      int ord = dv.getOrd(i);
+      if (ord < 0 || ord > maxOrd) {
+        throw new RuntimeException("ord out of bounds: " + ord);
+      }
+      maxOrd2 = Math.max(maxOrd2, ord);
+      seenOrds.set(ord);
     }
-    if (b.length > b.bytes.length) {
-      throw new RuntimeException("length is out of bounds: " + b.length + ", bytes.length=" + b.bytes.length);
+    if (maxOrd != maxOrd2) {
+      throw new RuntimeException("dv for field: " + fieldName + " reports wrong maxOrd=" + maxOrd + " but this is not the case: " + maxOrd2);
     }
-    if (b.offset < 0) {
-      throw new RuntimeException("offset is negative: " + b.offset);
+    if (seenOrds.cardinality() != dv.getValueCount()) {
+      throw new RuntimeException("dv for field: " + fieldName + " has holes in its ords, valueCount=" + dv.getValueCount() + " but only used: " + seenOrds.cardinality());
     }
-    if (b.offset > b.bytes.length) {
-      throw new RuntimeException("offset out of bounds: " + b.offset + ", length=" + b.length);
+    BytesRef lastValue = null;
+    BytesRef scratch = new BytesRef();
+    for (int i = 0; i <= maxOrd; i++) {
+      dv.lookupOrd(i, scratch);
+      assert scratch.isValid();
+      if (lastValue != null) {
+        if (scratch.compareTo(lastValue) <= 0) {
+          throw new RuntimeException("dv for field: " + fieldName + " has ords out of order: " + lastValue + " >=" + scratch);
+        }
+      }
+      lastValue = BytesRef.deepCopyOf(scratch);
     }
-    if (b.offset + b.length < 0) {
-      throw new RuntimeException("offset+length is negative: offset=" + b.offset + ",length=" + b.length);
+  }
+  
+  private static void checkNumericDocValues(String fieldName, AtomicReader reader, NumericDocValues ndv) {
+    for (int i = 0; i < reader.maxDoc(); i++) {
+      ndv.get(i);
     }
-    if (b.offset + b.length > b.bytes.length) {
-      throw new RuntimeException("offset+length out of bounds: offset=" + b.offset + ",length=" + b.length + ",bytes.length=" + b.bytes.length);
+  }
+  
+  private static void checkDocValues(FieldInfo fi, AtomicReader reader, PrintStream infoStream) throws Exception {
+    switch(fi.getDocValuesType()) {
+      case SORTED:
+        checkSortedDocValues(fi.name, reader, reader.getSortedDocValues(fi.name));
+        break;
+      case BINARY:
+        checkBinaryDocValues(fi.name, reader, reader.getBinaryDocValues(fi.name));
+        break;
+      case NUMERIC:
+        checkNumericDocValues(fi.name, reader, reader.getNumericDocValues(fi.name));
+        break;
+      default:
+        throw new AssertionError();
     }
   }
+  
+  private static void checkNorms(FieldInfo fi, AtomicReader reader, PrintStream infoStream) throws IOException {
+    switch(fi.getNormType()) {
+      case NUMERIC:
+        checkNumericDocValues(fi.name, reader, reader.getNormValues(fi.name));
+        break;
+      default:
+        throw new AssertionError("wtf: " + fi.getNormType());
+    }
+  }
 
   /**
-   * Test term vectors for a segment.
+   * Test term vectors.
+   * @lucene.experimental
    */
-  private Status.TermVectorStatus testTermVectors(FieldInfos fieldInfos, SegmentInfoPerCommit info, SegmentReader reader, NumberFormat format) {
+  public static Status.TermVectorStatus testTermVectors(AtomicReader reader, PrintStream infoStream) {
+    return testTermVectors(reader, infoStream, false, false);
+  }
+
+  /**
+   * Test term vectors.
+   * @lucene.experimental
+   */
+  public static Status.TermVectorStatus testTermVectors(AtomicReader reader, PrintStream infoStream, boolean verbose, boolean crossCheckTermVectors) {
     final Status.TermVectorStatus status = new Status.TermVectorStatus();
-
+    final FieldInfos fieldInfos = reader.getFieldInfos();
     final Bits onlyDocIsDeleted = new FixedBitSet(1);
     
     try {
@@ -1430,7 +1407,7 @@
       TermsEnum termsEnum = null;
       TermsEnum postingsTermsEnum = null;
 
-      for (int j = 0; j < info.info.getDocCount(); ++j) {
+      for (int j = 0; j < reader.maxDoc(); ++j) {
         // Intentionally pull/visit (but don't count in
         // stats) deleted documents to make sure they too
         // are not corrupt:
@@ -1441,10 +1418,10 @@
 
         if (tfv != null) {
           // First run with no deletions:
-          checkFields(tfv, null, 1, fieldInfos, false, true);
+          checkFields(tfv, null, 1, fieldInfos, false, true, infoStream, verbose);
 
           // Again, with the one doc deleted:
-          checkFields(tfv, onlyDocIsDeleted, 1, fieldInfos, false, true);
+          checkFields(tfv, onlyDocIsDeleted, 1, fieldInfos, false, true, infoStream, verbose);
 
           // Only agg stats if the doc is live:
           final boolean doStats = liveDocs == null || liveDocs.get(j);
@@ -1608,10 +1585,10 @@
         }
       }
       float vectorAvg = status.docCount == 0 ? 0 : status.totVectors / (float)status.docCount;
-      msg("OK [" + status.totVectors + " total vector count; avg " + 
-          format.format(vectorAvg) + " term/freq vector fields per doc]");
+      msg(infoStream, "OK [" + status.totVectors + " total vector count; avg " + 
+          NumberFormat.getInstance(Locale.ROOT).format(vectorAvg) + " term/freq vector fields per doc]");
     } catch (Throwable e) {
-      msg("ERROR [" + String.valueOf(e.getMessage()) + "]");
+      msg(infoStream, "ERROR [" + String.valueOf(e.getMessage()) + "]");
       status.error = e;
       if (infoStream != null) {
         e.printStackTrace(infoStream);
Index: lucene/core/src/java/org/apache/lucene/index/DocValuesProcessor.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/index/DocValuesProcessor.java	(revision 0)
+++ lucene/core/src/java/org/apache/lucene/index/DocValuesProcessor.java	(working copy)
@@ -0,0 +1,161 @@
+package org.apache.lucene.index;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.HashMap;
+import java.util.Map;
+
+import org.apache.lucene.codecs.DocValuesConsumer;
+import org.apache.lucene.codecs.DocValuesFormat;
+import org.apache.lucene.index.FieldInfo.DocValuesType;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.Counter;
+import org.apache.lucene.util.IOUtils;
+
+final class DocValuesProcessor extends StoredFieldsConsumer {
+
+  // TODO: somewhat wasteful we also keep a map here; would
+  // be more efficient if we could "reuse" the map/hash
+  // lookup DocFieldProcessor already did "above"
+  private final Map<String,DocValuesWriter> writers = new HashMap<String,DocValuesWriter>();
+  private final Counter bytesUsed;
+
+  public DocValuesProcessor(Counter bytesUsed) {
+    this.bytesUsed = bytesUsed;
+  }
+
+  @Override
+  void startDocument() {
+  }
+
+  @Override
+  void finishDocument() {
+  }
+
+  @Override
+  public void addField(int docID, StorableField field, FieldInfo fieldInfo) {
+    final DocValuesType dvType = field.fieldType().docValueType();
+    if (dvType != null) {
+      fieldInfo.setDocValuesType(dvType);
+      if (dvType == DocValuesType.BINARY) {
+        addBinaryField(fieldInfo, docID, field.binaryValue());
+      } else if (dvType == DocValuesType.SORTED) {
+        addSortedField(fieldInfo, docID, field.binaryValue());
+      } else if (dvType == DocValuesType.NUMERIC) {
+        if (!(field.numericValue() instanceof Long)) {
+          throw new IllegalArgumentException("illegal type " + field.numericValue().getClass() + ": DocValues types must be Long");
+        }
+        addNumericField(fieldInfo, docID, field.numericValue().longValue());
+      } else {
+        assert false: "unrecognized DocValues.Type: " + dvType;
+      }
+    }
+  }
+
+  @Override
+  void flush(SegmentWriteState state) throws IOException {
+    if (!writers.isEmpty()) {
+      DocValuesFormat fmt = state.segmentInfo.getCodec().docValuesFormat();
+      DocValuesConsumer dvConsumer = fmt.fieldsConsumer(state);
+      boolean success = false;
+      try {
+        for(DocValuesWriter writer : writers.values()) {
+          writer.finish(state.segmentInfo.getDocCount());
+          writer.flush(state, dvConsumer);
+        }
+        // TODO: catch missing DV fields here?  else we have
+        // null/"" depending on how docs landed in segments?
+        // but we can't detect all cases, and we should leave
+        // this behavior undefined. dv is not "schemaless": its column-stride.
+        writers.clear();
+        success = true;
+      } finally {
+        if (success) {
+          IOUtils.close(dvConsumer);
+        } else {
+          IOUtils.closeWhileHandlingException(dvConsumer);
+        }
+      }
+    }
+  }
+
+  void addBinaryField(FieldInfo fieldInfo, int docID, BytesRef value) {
+    DocValuesWriter writer = writers.get(fieldInfo.name);
+    BinaryDocValuesWriter binaryWriter;
+    if (writer == null) {
+      binaryWriter = new BinaryDocValuesWriter(fieldInfo, bytesUsed);
+      writers.put(fieldInfo.name, binaryWriter);
+    } else if (!(writer instanceof BinaryDocValuesWriter)) {
+      throw new IllegalArgumentException("Incompatible DocValues type: field \"" + fieldInfo.name + "\" changed from " + getTypeDesc(writer) + " to binary");
+    } else {
+      binaryWriter = (BinaryDocValuesWriter) writer;
+    }
+    binaryWriter.addValue(docID, value);
+  }
+
+  void addSortedField(FieldInfo fieldInfo, int docID, BytesRef value) {
+    DocValuesWriter writer = writers.get(fieldInfo.name);
+    SortedDocValuesWriter sortedWriter;
+    if (writer == null) {
+      sortedWriter = new SortedDocValuesWriter(fieldInfo, bytesUsed);
+      writers.put(fieldInfo.name, sortedWriter);
+    } else if (!(writer instanceof SortedDocValuesWriter)) {
+      throw new IllegalArgumentException("Incompatible DocValues type: field \"" + fieldInfo.name + "\" changed from " + getTypeDesc(writer) + " to sorted");
+    } else {
+      sortedWriter = (SortedDocValuesWriter) writer;
+    }
+    sortedWriter.addValue(docID, value);
+  }
+
+  void addNumericField(FieldInfo fieldInfo, int docID, long value) {
+    DocValuesWriter writer = writers.get(fieldInfo.name);
+    NumericDocValuesWriter numericWriter;
+    if (writer == null) {
+      numericWriter = new NumericDocValuesWriter(fieldInfo, bytesUsed);
+      writers.put(fieldInfo.name, numericWriter);
+    } else if (!(writer instanceof NumericDocValuesWriter)) {
+      throw new IllegalArgumentException("Incompatible DocValues type: field \"" + fieldInfo.name + "\" changed from " + getTypeDesc(writer) + " to numeric");
+    } else {
+      numericWriter = (NumericDocValuesWriter) writer;
+    }
+    numericWriter.addValue(docID, value);
+  }
+
+  private String getTypeDesc(DocValuesWriter obj) {
+    if (obj instanceof BinaryDocValuesWriter) {
+      return "binary";
+    } else if (obj instanceof NumericDocValuesWriter) {
+      return "numeric";
+    } else {
+      assert obj instanceof SortedDocValuesWriter;
+      return "sorted";
+    }
+  }
+
+  @Override
+  public void abort() throws IOException {
+    for(DocValuesWriter writer : writers.values()) {
+      try {
+        writer.abort();
+      } catch (Throwable t) {
+      }
+    }
+    writers.clear();
+  }
+}

Property changes on: lucene/core/src/java/org/apache/lucene/index/DocValuesProcessor.java
___________________________________________________________________
Added: svn:eol-style
## -0,0 +1 ##
+native
Index: lucene/core/src/java/org/apache/lucene/index/SlowCompositeReaderWrapper.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/index/SlowCompositeReaderWrapper.java	(revision 1442822)
+++ lucene/core/src/java/org/apache/lucene/index/SlowCompositeReaderWrapper.java	(working copy)
@@ -24,6 +24,8 @@
 import org.apache.lucene.util.Bits;
 
 import org.apache.lucene.index.DirectoryReader; // javadoc
+import org.apache.lucene.index.MultiDocValues.MultiSortedDocValues;
+import org.apache.lucene.index.MultiDocValues.OrdinalMap;
 import org.apache.lucene.index.MultiReader; // javadoc
 
 /**
@@ -44,7 +46,6 @@
 public final class SlowCompositeReaderWrapper extends AtomicReader {
 
   private final CompositeReader in;
-  private final Map<String, DocValues> normsCache = new HashMap<String, DocValues>();
   private final Fields fields;
   private final Bits liveDocs;
   
@@ -83,26 +84,66 @@
   }
 
   @Override
-  public DocValues docValues(String field) throws IOException {
+  public NumericDocValues getNumericDocValues(String field) throws IOException {
     ensureOpen();
-    return MultiDocValues.getDocValues(in, field);
+    return MultiDocValues.getNumericValues(in, field);
   }
-  
+
   @Override
-  public synchronized DocValues normValues(String field) throws IOException {
+  public BinaryDocValues getBinaryDocValues(String field) throws IOException {
     ensureOpen();
-    DocValues values = normsCache.get(field);
-    if (values == null) {
-      values = MultiDocValues.getNormDocValues(in, field);
-      normsCache.put(field, values);
+    return MultiDocValues.getBinaryValues(in, field);
+  }
+
+  @Override
+  public SortedDocValues getSortedDocValues(String field) throws IOException {
+    ensureOpen();
+    OrdinalMap map = null;
+    synchronized (cachedOrdMaps) {
+      map = cachedOrdMaps.get(field);
+      if (map == null) {
+        // uncached, or not a multi dv
+        SortedDocValues dv = MultiDocValues.getSortedValues(in, field);
+        if (dv instanceof MultiSortedDocValues) {
+          map = ((MultiSortedDocValues)dv).mapping;
+          if (map.owner == getCoreCacheKey()) {
+            cachedOrdMaps.put(field, map);
+          }
+        }
+        return dv;
+      }
     }
-    return values;
+    // cached multi dv
+    assert map != null;
+    int size = in.leaves().size();
+    final SortedDocValues[] values = new SortedDocValues[size];
+    final int[] starts = new int[size+1];
+    for (int i = 0; i < size; i++) {
+      AtomicReaderContext context = in.leaves().get(i);
+      SortedDocValues v = context.reader().getSortedDocValues(field);
+      if (v == null) {
+        v = SortedDocValues.EMPTY;
+      }
+      values[i] = v;
+      starts[i] = context.docBase;
+    }
+    starts[size] = maxDoc();
+    return new MultiSortedDocValues(values, starts, map);
   }
   
+  // TODO: this could really be a weak map somewhere else on the coreCacheKey,
+  // but do we really need to optimize slow-wrapper any more?
+  private final Map<String,OrdinalMap> cachedOrdMaps = new HashMap<String,OrdinalMap>();
+
   @Override
-  public Fields getTermVectors(int docID)
-          throws IOException {
+  public NumericDocValues getNormValues(String field) throws IOException {
     ensureOpen();
+    return MultiDocValues.getNormValues(in, field);
+  }
+  
+  @Override
+  public Fields getTermVectors(int docID) throws IOException {
+    ensureOpen();
     return in.getTermVectors(docID);
   }
 
Index: lucene/core/src/java/org/apache/lucene/index/MultiDocValues.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/index/MultiDocValues.java	(revision 1442822)
+++ lucene/core/src/java/org/apache/lucene/index/MultiDocValues.java	(working copy)
@@ -16,593 +16,292 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
+
 import java.io.IOException;
-import java.lang.reflect.Array;
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.Comparator;
 import java.util.List;
 
-import org.apache.lucene.index.SortedBytesMergeUtils.MergeContext;
-import org.apache.lucene.index.SortedBytesMergeUtils.SortedSourceSlice;
-import org.apache.lucene.util.ArrayUtil;
+import org.apache.lucene.index.MultiTermsEnum.TermsEnumIndex;
+import org.apache.lucene.index.MultiTermsEnum.TermsEnumWithSlice;
 import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.PagedBytes;
-import org.apache.lucene.util.packed.PackedInts.Reader;
+import org.apache.lucene.util.packed.AppendingLongBuffer;
 
 /**
- * A wrapper for CompositeIndexReader providing access to per segment
- * {@link DocValues}
+ * A wrapper for CompositeIndexReader providing access to DocValues.
  * 
  * <p><b>NOTE</b>: for multi readers, you'll get better
  * performance by gathering the sub readers using
  * {@link IndexReader#getContext()} to get the
  * atomic leaves and then operate per-AtomicReader,
  * instead of using this class.
+ * 
+ * <p><b>NOTE</b>: This is very costly.
  *
  * @lucene.experimental
  * @lucene.internal
  */
-class MultiDocValues extends DocValues {
+public class MultiDocValues {
   
-  private static DocValuesPuller DEFAULT_PULLER = new DocValuesPuller();
-  private static final DocValuesPuller NORMS_PULLER = new DocValuesPuller() {
-    @Override
-    public DocValues pull(AtomicReader reader, String field) throws IOException {
-      return reader.normValues(field);
-    }
-    
-    @Override
-    public boolean stopLoadingOnNull(AtomicReader reader, String field) {
-      // for norms we drop all norms if one leaf reader has no norms and the field is present
-      FieldInfos fieldInfos = reader.getFieldInfos();
-      FieldInfo fieldInfo = fieldInfos.fieldInfo(field);
-      return fieldInfo != null && fieldInfo.omitsNorms();
-    }
-  };
-
-  public static class DocValuesSlice {
-    public final static DocValuesSlice[] EMPTY_ARRAY = new DocValuesSlice[0];
-    final int start;
-    final int length;
-    DocValues docValues;
-
-    public DocValuesSlice(DocValues docValues, int start, int length) {
-      this.docValues = docValues;
-      this.start = start;
-      this.length = length;
-    }
-  }
+  /** No instantiation */
+  private MultiDocValues() {}
   
-  private static class DocValuesPuller {
-    public DocValuesPuller() {}
-
-    public DocValues pull(AtomicReader reader, String field) throws IOException {
-      return reader.docValues(field);
-    }
-    
-    public boolean stopLoadingOnNull(AtomicReader reader, String field) {
-      return false;
-    }
-  }
-
-  private DocValuesSlice[] slices;
-  private int[] starts;
-  private Type type;
-  private int valueSize;
-
-  private MultiDocValues(DocValuesSlice[] slices, int[] starts, TypePromoter promotedType) {
-    this.starts = starts;
-    this.slices = slices;
-    this.type = promotedType.type();
-    this.valueSize = promotedType.getValueSize();
-  }
-  /**
-   * Returns a single {@link DocValues} instance for this field, merging
-   * their values on the fly.
-   * 
+  /** Returns a NumericDocValues for a reader's norms (potentially merging on-the-fly).
    * <p>
-   * <b>NOTE</b>: this is a slow way to access DocValues.
-   * It's better to get the sub-readers and iterate through them
-   * yourself.
+   * This is a slow way to access normalization values. Instead, access them per-segment
+   * with {@link AtomicReader#getNormValues(String)}
+   * </p> 
    */
-  public static DocValues getDocValues(IndexReader r, final String field) throws IOException {
-    return getDocValues(r, field, DEFAULT_PULLER);
-  }
-  
-  /**
-   * Returns a single {@link DocValues} instance for this norms field, merging
-   * their values on the fly.
-   * 
-   * <p>
-   * <b>NOTE</b>: this is a slow way to access DocValues.
-   * It's better to get the sub-readers and iterate through them
-   * yourself.
-   */
-  public static DocValues getNormDocValues(IndexReader r, final String field) throws IOException {
-    return getDocValues(r, field, NORMS_PULLER);
-  }
-  
- 
-  private static DocValues getDocValues(IndexReader reader, final String field, final DocValuesPuller puller) throws IOException {
-    if (reader instanceof AtomicReader) {
-      // already an atomic reader
-      return puller.pull((AtomicReader) reader, field);
+  public static NumericDocValues getNormValues(final IndexReader r, final String field) throws IOException {
+    final List<AtomicReaderContext> leaves = r.leaves();
+    final int size = leaves.size();
+    if (size == 0) {
+      return null;
+    } else if (size == 1) {
+      return leaves.get(0).reader().getNormValues(field);
     }
-    assert reader instanceof CompositeReader;
-    final List<AtomicReaderContext> leaves = reader.leaves();
-    switch (leaves.size()) {
-      case 0:
-        // no fields
-        return null;
-      case 1:
-        // already an atomic reader / reader with one leave
-        return getDocValues(leaves.get(0).reader(), field, puller);
-      default:
-        final List<DocValuesSlice> slices = new ArrayList<DocValuesSlice>();
-        
-        TypePromoter promotedType =  TypePromoter.getIdentityPromoter();
-        
-        // gather all docvalues fields, accumulating a promoted type across 
-        // potentially incompatible types
-        for (final AtomicReaderContext ctx : leaves) {
-          final AtomicReader r = ctx.reader();
-          final DocValues d = puller.pull(r, field);
-          if (d != null) {
-            TypePromoter incoming = TypePromoter.create(d.getType(), d.getValueSize());
-            promotedType = promotedType.promote(incoming);
-          } else if (puller.stopLoadingOnNull(r, field)){
-            return null;
-          }
-          slices.add(new DocValuesSlice(d, ctx.docBase, r.maxDoc()));
-        }
-        
-        // return null if no docvalues encountered anywhere
-        if (promotedType == TypePromoter.getIdentityPromoter()) {
-          return null;
-        }
-             
-        // populate starts and fill gaps with empty docvalues 
-        int starts[] = new int[slices.size()];
-        for (int i = 0; i < slices.size(); i++) {
-          DocValuesSlice slice = slices.get(i);
-          starts[i] = slice.start;
-          if (slice.docValues == null) {
-            Type promoted = promotedType.type();
-            switch(promoted) {
-              case BYTES_FIXED_DEREF:
-              case BYTES_FIXED_STRAIGHT:
-              case BYTES_FIXED_SORTED:
-                assert promotedType.getValueSize() >= 0;
-                slice.docValues = new EmptyFixedDocValues(slice.length, promoted, promotedType.getValueSize());
-                break;
-              default:
-                slice.docValues = new EmptyDocValues(slice.length, promoted);
-            }
-          }
-        }
-        
-        return new MultiDocValues(slices.toArray(new DocValuesSlice[slices.size()]), starts, promotedType);
+    FieldInfo fi = MultiFields.getMergedFieldInfos(r).fieldInfo(field);
+    if (fi == null || fi.hasNorms() == false) {
+      return null;
     }
-  }
 
-  @Override
-  protected Source loadSource() throws IOException {
-    return new MultiSource(slices, starts, false, type);
-  }
-
-  public static class EmptyDocValues extends DocValues {
-    final int maxDoc;
-    final Source emptySource;
-
-    public EmptyDocValues(int maxDoc, Type type) {
-      this.maxDoc = maxDoc;
-      this.emptySource = new EmptySource(type);
+    boolean anyReal = false;
+    final NumericDocValues[] values = new NumericDocValues[size];
+    final int[] starts = new int[size+1];
+    for (int i = 0; i < size; i++) {
+      AtomicReaderContext context = leaves.get(i);
+      NumericDocValues v = context.reader().getNormValues(field);
+      if (v == null) {
+        v = NumericDocValues.EMPTY;
+      } else {
+        anyReal = true;
+      }
+      values[i] = v;
+      starts[i] = context.docBase;
     }
+    starts[size] = r.maxDoc();
+    
+    assert anyReal;
 
-    @Override
-    protected Source loadSource() throws IOException {
-      return emptySource;
-    }
-
-    @Override
-    public Type getType() {
-      return emptySource.getType();
-    }
-
-    @Override
-    protected Source loadDirectSource() throws IOException {
-      return emptySource;
-    }
+    return new NumericDocValues() {
+      @Override
+      public long get(int docID) {
+        int subIndex = ReaderUtil.subIndex(docID, starts);
+        return values[subIndex].get(docID - starts[subIndex]);
+      }
+    };
   }
-  
-  public static class EmptyFixedDocValues extends DocValues {
-    final int maxDoc;
-    final Source emptyFixedSource;
-    final int valueSize;
 
-    public EmptyFixedDocValues(int maxDoc, Type type, int valueSize) {
-      this.maxDoc = maxDoc;
-      this.emptyFixedSource = new EmptyFixedSource(type, valueSize);
-      this.valueSize = valueSize;
+  /** Returns a NumericDocValues for a reader's docvalues (potentially merging on-the-fly) 
+   * <p>
+   * This is a slow way to access numeric values. Instead, access them per-segment
+   * with {@link AtomicReader#getNumericDocValues(String)}
+   * </p> 
+   * */
+  public static NumericDocValues getNumericValues(final IndexReader r, final String field) throws IOException {
+    final List<AtomicReaderContext> leaves = r.leaves();
+    final int size = leaves.size();
+    if (size == 0) {
+      return null;
+    } else if (size == 1) {
+      return leaves.get(0).reader().getNumericDocValues(field);
     }
 
-    @Override
-    protected Source loadSource() throws IOException {
-      return emptyFixedSource;
-    }
-
-    @Override
-    public Type getType() {
-      return emptyFixedSource.getType();
-    }
-
-    @Override
-    public int getValueSize() {
-      return valueSize;
-    }
-
-    @Override
-    protected Source loadDirectSource() throws IOException {
-      return emptyFixedSource;
-    }
-  }
-
-  private static class MultiSource extends Source {
-    private int numDocs = 0;
-    private int start = 0;
-    private Source current;
-    private final int[] starts;
-    private final DocValuesSlice[] slices;
-    private boolean direct;
-    private Object cachedArray; // cached array if supported
-
-    public MultiSource(DocValuesSlice[] slices, int[] starts, boolean direct, Type type) {
-      super(type);
-      this.slices = slices;
-      this.starts = starts;
-      assert slices.length != 0;
-      this.direct = direct;
-    }
-
-    @Override
-    public long getInt(int docID) {
-      final int doc = ensureSource(docID);
-      return current.getInt(doc);
-    }
-
-    private final int ensureSource(int docID) {
-      if (docID >= start && docID < start+numDocs) {
-        return docID - start;
+    boolean anyReal = false;
+    final NumericDocValues[] values = new NumericDocValues[size];
+    final int[] starts = new int[size+1];
+    for (int i = 0; i < size; i++) {
+      AtomicReaderContext context = leaves.get(i);
+      NumericDocValues v = context.reader().getNumericDocValues(field);
+      if (v == null) {
+        v = NumericDocValues.EMPTY;
       } else {
-        final int idx = ReaderUtil.subIndex(docID, starts);
-        assert idx >= 0 && idx < slices.length : "idx was " + idx
-            + " for doc id: " + docID + " slices : " + Arrays.toString(starts);
-        assert slices[idx] != null;
-        try {
-          if (direct) {
-            current = slices[idx].docValues.getDirectSource();
-          } else {
-            current = slices[idx].docValues.getSource();
-          }
-        } catch (IOException e) {
-          throw new RuntimeException("load failed", e); // TODO how should we
-          // handle this
-        }
-
-        start = slices[idx].start;
-        numDocs = slices[idx].length;
-        return docID - start;
+        anyReal = true;
       }
+      values[i] = v;
+      starts[i] = context.docBase;
     }
+    starts[size] = r.maxDoc();
 
-    @Override
-    public double getFloat(int docID) {
-      final int doc = ensureSource(docID);
-      return current.getFloat(doc);
+    if (!anyReal) {
+      return null;
+    } else {
+      return new NumericDocValues() {
+        @Override
+        public long get(int docID) {
+          int subIndex = ReaderUtil.subIndex(docID, starts);
+          return values[subIndex].get(docID - starts[subIndex]);
+        }
+      };
     }
+  }
 
-    @Override
-    public BytesRef getBytes(int docID, BytesRef bytesRef) {
-      final int doc = ensureSource(docID);
-      return current.getBytes(doc, bytesRef);
-    }
-
-    @Override
-    public SortedSource asSortedSource() {
-      try {
-        if (type == Type.BYTES_FIXED_SORTED || type == Type.BYTES_VAR_SORTED) {
-          DocValues[] values = new DocValues[slices.length];
-          Comparator<BytesRef> comp = null;
-          for (int i = 0; i < values.length; i++) {
-            values[i] = slices[i].docValues;
-            if (!(values[i] instanceof EmptyDocValues)) {
-              Comparator<BytesRef> comparator = values[i].getDirectSource()
-                  .asSortedSource().getComparator();
-              assert comp == null || comp == comparator;
-              comp = comparator;
-            }
-          }
-          assert comp != null;
-          final int globalNumDocs = globalNumDocs();
-          final MergeContext ctx = SortedBytesMergeUtils.init(type, values,
-              comp, globalNumDocs);
-          List<SortedSourceSlice> slices = SortedBytesMergeUtils.buildSlices(
-              docBases(), new MergeState.DocMap[values.length], values, ctx);
-          RecordingBytesRefConsumer consumer = new RecordingBytesRefConsumer(
-              type);
-          final int maxOrd = SortedBytesMergeUtils.mergeRecords(ctx, consumer,
-              slices);
-          final int[] docToOrd = new int[globalNumDocs];
-          for (SortedSourceSlice slice : slices) {
-            slice.toAbsolutOrds(docToOrd);
-          }
-          return new MultiSortedSource(type, comp, consumer.pagedBytes,
-              ctx.sizePerValues, maxOrd, docToOrd, consumer.ordToOffset);
-        }
-      } catch (IOException e) {
-        throw new RuntimeException("load failed", e);
-      }
-      return super.asSortedSource();
-    }
+  /** Returns a BinaryDocValues for a reader's docvalues (potentially merging on-the-fly)
+   * <p>
+   * This is a slow way to access binary values. Instead, access them per-segment
+   * with {@link AtomicReader#getBinaryDocValues(String)}
+   * </p>  
+   */
+  public static BinaryDocValues getBinaryValues(final IndexReader r, final String field) throws IOException {
+    final List<AtomicReaderContext> leaves = r.leaves();
+    final int size = leaves.size();
     
-    private int globalNumDocs() {
-      int docs = 0;
-      for (int i = 0; i < slices.length; i++) {
-        docs += slices[i].length;
-      }
-      return docs;
+    if (size == 0) {
+      return null;
+    } else if (size == 1) {
+      return leaves.get(0).reader().getBinaryDocValues(field);
     }
     
-    private int[] docBases() {
-      int[] docBases = new int[slices.length];
-      for (int i = 0; i < slices.length; i++) {
-        docBases[i] = slices[i].start;
+    boolean anyReal = false;
+    final BinaryDocValues[] values = new BinaryDocValues[size];
+    final int[] starts = new int[size+1];
+    for (int i = 0; i < size; i++) {
+      AtomicReaderContext context = leaves.get(i);
+      BinaryDocValues v = context.reader().getBinaryDocValues(field);
+      if (v == null) {
+        v = BinaryDocValues.EMPTY;
+      } else {
+        anyReal = true;
       }
-      return docBases;
+      values[i] = v;
+      starts[i] = context.docBase;
     }
+    starts[size] = r.maxDoc();
     
-    @Override
-    public boolean hasArray() {
-      boolean oneRealSource = false;
-      for (DocValuesSlice slice : slices) {
-        try {
-          Source source = slice.docValues.getSource();
-          if (source instanceof EmptySource) {
-            /*
-             * empty source marks a gap in the array skip if we encounter one
-             */
-            continue;
-          }
-          oneRealSource = true;
-          if (!source.hasArray()) {
-            return false;
-          }
-        } catch (IOException e) {
-          throw new RuntimeException("load failed", e);
+    if (!anyReal) {
+      return null;
+    } else {
+      return new BinaryDocValues() {
+        @Override
+        public void get(int docID, BytesRef result) {
+          int subIndex = ReaderUtil.subIndex(docID, starts);
+          values[subIndex].get(docID - starts[subIndex], result);
         }
-      }
-      return oneRealSource;
+      };
     }
-
-    @Override
-    public Object getArray() {
-      if (!hasArray()) {
-        return null;
-      }
-      try {
-        Class<?> componentType = null;
-        Object[] arrays = new Object[slices.length];
-        int numDocs = 0;
-        for (int i = 0; i < slices.length; i++) {
-          DocValuesSlice slice = slices[i];
-          Source source = slice.docValues.getSource();
-          Object array = null;
-          if (!(source instanceof EmptySource)) {
-            // EmptySource is skipped - marks a gap in the array
-            array = source.getArray();
-          }
-          numDocs += slice.length;
-          if (array != null) {
-            if (componentType == null) {
-              componentType = array.getClass().getComponentType();
-            }
-            assert componentType == array.getClass().getComponentType();
-          }
-          arrays[i] = array;
-        }
-        assert componentType != null;
-        synchronized (this) {
-          if (cachedArray != null) {
-            return cachedArray;
-          }
-          final Object globalArray = Array.newInstance(componentType, numDocs);
-
-          for (int i = 0; i < slices.length; i++) {
-            DocValuesSlice slice = slices[i];
-            if (arrays[i] != null) {
-              assert slice.length == Array.getLength(arrays[i]);
-              System.arraycopy(arrays[i], 0, globalArray, slice.start,
-                  slice.length);
-            }
-          }
-          return cachedArray = globalArray;
-        }
-      } catch (IOException e) {
-        throw new RuntimeException("load failed", e);
-      }
-    }
   }
   
-  private static final class RecordingBytesRefConsumer implements SortedBytesMergeUtils.BytesRefConsumer {
-    private final static int PAGED_BYTES_BITS = 15;
-    final PagedBytes pagedBytes = new PagedBytes(PAGED_BYTES_BITS);
-    long[] ordToOffset;
+  /** Returns a SortedDocValues for a reader's docvalues (potentially doing extremely slow things).
+   * <p>
+   * This is an extremely slow way to access sorted values. Instead, access them per-segment
+   * with {@link AtomicReader#getSortedDocValues(String)}
+   * </p>  
+   */
+  public static SortedDocValues getSortedValues(final IndexReader r, final String field) throws IOException {
+    final List<AtomicReaderContext> leaves = r.leaves();
+    final int size = leaves.size();
     
-    public RecordingBytesRefConsumer(Type type) {
-      ordToOffset = type == Type.BYTES_VAR_SORTED ? new long[2] : null;
+    if (size == 0) {
+      return null;
+    } else if (size == 1) {
+      return leaves.get(0).reader().getSortedDocValues(field);
     }
-    @Override
-    public void consume(BytesRef ref, int ord, long offset) {
-      pagedBytes.copy(ref);
-      if (ordToOffset != null) {
-        if (ord+1 >= ordToOffset.length) {
-          ordToOffset = ArrayUtil.grow(ordToOffset, ord + 2);
-        }
-        ordToOffset[ord+1] = offset;
+    
+    boolean anyReal = false;
+    final SortedDocValues[] values = new SortedDocValues[size];
+    final int[] starts = new int[size+1];
+    for (int i = 0; i < size; i++) {
+      AtomicReaderContext context = leaves.get(i);
+      SortedDocValues v = context.reader().getSortedDocValues(field);
+      if (v == null) {
+        v = SortedDocValues.EMPTY;
+      } else {
+        anyReal = true;
       }
+      values[i] = v;
+      starts[i] = context.docBase;
     }
+    starts[size] = r.maxDoc();
     
-  }
-  
-  private static final class MultiSortedSource extends SortedSource {
-    private final PagedBytes.Reader data;
-    private final int[] docToOrd;
-    private final long[] ordToOffset;
-    private int size;
-    private int valueCount;
-    public MultiSortedSource(Type type, Comparator<BytesRef> comparator, PagedBytes pagedBytes, int size, int numValues, int[] docToOrd, long[] ordToOffset) {
-      super(type, comparator);
-      data = pagedBytes.freeze(true);
-      this.size = size;
-      this.valueCount = numValues;
-      this.docToOrd = docToOrd;
-      this.ordToOffset = ordToOffset;
-    }
-
-    @Override
-    public int ord(int docID) {
-      return docToOrd[docID];
-    }
-
-    @Override
-    public BytesRef getByOrd(int ord, BytesRef bytesRef) {
-      int size = this.size;
-      long offset = (ord*size);
-      if (ordToOffset != null) {
-        offset =  ordToOffset[ord];
-        size = (int) (ordToOffset[1 + ord] - offset);
-      }
-      assert size >=0;
-      return data.fillSlice(bytesRef, offset, size);
-     }
-
-    @Override
-    public Reader getDocToOrd() {
+    if (!anyReal) {
       return null;
+    } else {
+      OrdinalMap mapping = new OrdinalMap(r.getCoreCacheKey(), values);
+      return new MultiSortedDocValues(values, starts, mapping);
     }
-
-    @Override
-    public int getValueCount() {
-      return valueCount;
-    }
   }
-
-  // TODO: this is dup of DocValues.getDefaultSource()?
-  private static class EmptySource extends SortedSource {
-
-    public EmptySource(Type type) {
-      super(type, BytesRef.getUTF8SortedAsUnicodeComparator());
-    }
-
-    @Override
-    public BytesRef getBytes(int docID, BytesRef ref) {
-      ref.length = 0;
-      return ref;
-    }
-
-    @Override
-    public double getFloat(int docID) {
-      return 0d;
-    }
-
-    @Override
-    public long getInt(int docID) {
-      return 0;
-    }
-
-    @Override
-    public SortedSource asSortedSource() {
-      if (getType() == Type.BYTES_FIXED_SORTED || getType() == Type.BYTES_VAR_SORTED) {
-        
+  
+  /** maps per-segment ordinals to/from global ordinal space */
+  // TODO: use more efficient packed ints structures (these are all positive values!)
+  static class OrdinalMap {
+    // cache key of whoever asked for this aweful thing
+    final Object owner;
+    // globalOrd -> (globalOrd - segmentOrd)
+    final AppendingLongBuffer globalOrdDeltas;
+    // globalOrd -> sub index
+    final AppendingLongBuffer subIndexes;
+    // segmentOrd -> (globalOrd - segmentOrd)
+    final AppendingLongBuffer ordDeltas[];
+    
+    OrdinalMap(Object owner, SortedDocValues subs[]) throws IOException {
+      // create the ordinal mappings by pulling a termsenum over each sub's 
+      // unique terms, and walking a multitermsenum over those
+      this.owner = owner;
+      globalOrdDeltas = new AppendingLongBuffer();
+      subIndexes = new AppendingLongBuffer();
+      ordDeltas = new AppendingLongBuffer[subs.length];
+      for (int i = 0; i < ordDeltas.length; i++) {
+        ordDeltas[i] = new AppendingLongBuffer();
       }
-      return super.asSortedSource();
+      int segmentOrds[] = new int[subs.length];
+      ReaderSlice slices[] = new ReaderSlice[subs.length];
+      TermsEnumIndex indexes[] = new TermsEnumIndex[slices.length];
+      for (int i = 0; i < slices.length; i++) {
+        slices[i] = new ReaderSlice(0, 0, i);
+        indexes[i] = new TermsEnumIndex(new SortedDocValuesTermsEnum(subs[i]), i);
+      }
+      MultiTermsEnum mte = new MultiTermsEnum(slices);
+      mte.reset(indexes);
+      int globalOrd = 0;
+      while (mte.next() != null) {        
+        TermsEnumWithSlice matches[] = mte.getMatchArray();
+        for (int i = 0; i < mte.getMatchCount(); i++) {
+          int subIndex = matches[i].index;
+          int delta = globalOrd - segmentOrds[subIndex];
+          assert delta >= 0;
+          // for each unique term, just mark the first subindex/delta where it occurs
+          if (i == 0) {
+            subIndexes.add(subIndex);
+            globalOrdDeltas.add(delta);
+          }
+          // for each per-segment ord, map it back to the global term.
+          ordDeltas[subIndex].add(delta);
+          segmentOrds[subIndex]++;
+        }
+        globalOrd++;
+      }
     }
-
-    @Override
-    public int ord(int docID) {
-      return 0;
-    }
-
-    @Override
-    public BytesRef getByOrd(int ord, BytesRef bytesRef) {
-      bytesRef.length = 0;
-      bytesRef.offset = 0;
-      return bytesRef;
-    }
-
-    @Override
-    public Reader getDocToOrd() {
-      return null;
-    }
-
-    @Override
-    public int getValueCount() {
-      return 1;
-    }
-    
   }
   
-  private static class EmptyFixedSource extends EmptySource {
-    private final int valueSize;
-    private final byte[] valueArray;
-    public EmptyFixedSource(Type type, int valueSize) {
-      super(type);
-      this.valueSize = valueSize;
-      valueArray = new byte[valueSize];
+  /** implements SortedDocValues over n subs, using an OrdinalMap */
+  static class MultiSortedDocValues extends SortedDocValues {
+    final int docStarts[];
+    final SortedDocValues values[];
+    final OrdinalMap mapping;
+  
+    MultiSortedDocValues(SortedDocValues values[], int docStarts[], OrdinalMap mapping) throws IOException {
+      assert values.length == mapping.ordDeltas.length;
+      assert docStarts.length == values.length + 1;
+      this.values = values;
+      this.docStarts = docStarts;
+      this.mapping = mapping;
     }
-
+       
     @Override
-    public BytesRef getBytes(int docID, BytesRef ref) {
-      ref.grow(valueSize);
-      ref.length = valueSize;
-      Arrays.fill(ref.bytes, ref.offset, ref.offset+valueSize, (byte)0);
-      return ref;
+    public int getOrd(int docID) {
+      int subIndex = ReaderUtil.subIndex(docID, docStarts);
+      int segmentOrd = values[subIndex].getOrd(docID - docStarts[subIndex]);
+      return (int) (segmentOrd + mapping.ordDeltas[subIndex].get(segmentOrd));
     }
-
+ 
     @Override
-    public double getFloat(int docID) {
-      return 0d;
+    public void lookupOrd(int ord, BytesRef result) {
+      int subIndex = (int) mapping.subIndexes.get(ord);
+      int segmentOrd = (int) (ord - mapping.globalOrdDeltas.get(ord));
+      values[subIndex].lookupOrd(segmentOrd, result);
     }
-
+ 
     @Override
-    public long getInt(int docID) {
-      return 0;
+    public int getValueCount() {
+      return mapping.globalOrdDeltas.size();
     }
-    
-    @Override
-    public BytesRef getByOrd(int ord, BytesRef bytesRef) {
-      bytesRef.bytes = valueArray;
-      bytesRef.length = valueSize;
-      bytesRef.offset = 0;
-      return bytesRef;
-    }
   }
-
-  @Override
-  public Type getType() {
-    return type;
-  }
-
-  @Override
-  public int getValueSize() {
-    return valueSize;
-  }
-
-  @Override
-  protected Source loadDirectSource() throws IOException {
-    return new MultiSource(slices, starts, true, type);
-  }
-  
-  
 }
Index: lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriter.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriter.java	(revision 1442822)
+++ lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriter.java	(working copy)
@@ -45,10 +45,10 @@
     List<FreqProxTermsWriterPerField> allFields = new ArrayList<FreqProxTermsWriterPerField>();
 
     for (TermsHashConsumerPerField f : fieldsToFlush.values()) {
-        final FreqProxTermsWriterPerField perField = (FreqProxTermsWriterPerField) f;
-        if (perField.termsHashPerField.bytesHash.size() > 0) {
-          allFields.add(perField);
-        }
+      final FreqProxTermsWriterPerField perField = (FreqProxTermsWriterPerField) f;
+      if (perField.termsHashPerField.bytesHash.size() > 0) {
+        allFields.add(perField);
+      }
     }
 
     final int numAllFields = allFields.size();
Index: lucene/core/src/java/org/apache/lucene/index/MergeState.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/index/MergeState.java	(revision 1442822)
+++ lucene/core/src/java/org/apache/lucene/index/MergeState.java	(working copy)
@@ -217,9 +217,6 @@
   /** InfoStream for debugging messages. */
   public InfoStream infoStream;
 
-  /** Current field being merged. */
-  public FieldInfo fieldInfo;
-  
   // TODO: get rid of this? it tells you which segments are 'aligned' (e.g. for bulk merging)
   // but is this really so expensive to compute again in different components, versus once in SM?
   
Index: lucene/core/src/java/org/apache/lucene/index/TwoStoredFieldsConsumers.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/index/TwoStoredFieldsConsumers.java	(revision 0)
+++ lucene/core/src/java/org/apache/lucene/index/TwoStoredFieldsConsumers.java	(working copy)
@@ -0,0 +1,68 @@
+package org.apache.lucene.index;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+/** Just switches between two {@link DocFieldConsumer}s. */
+
+class TwoStoredFieldsConsumers extends StoredFieldsConsumer {
+  private final StoredFieldsConsumer first;
+  private final StoredFieldsConsumer second;
+
+  public TwoStoredFieldsConsumers(StoredFieldsConsumer first, StoredFieldsConsumer second) {
+    this.first = first;
+    this.second = second;
+  }
+
+  @Override
+  public void addField(int docID, StorableField field, FieldInfo fieldInfo) throws IOException {
+    first.addField(docID, field, fieldInfo);
+    second.addField(docID, field, fieldInfo);
+  }
+
+  @Override
+  void flush(SegmentWriteState state) throws IOException {
+    first.flush(state);
+    second.flush(state);
+  }
+
+  @Override
+  void abort() {
+    try {
+      first.abort();
+    } catch (Throwable t) {
+    }
+    try {
+      second.abort();
+    } catch (Throwable t) {
+    }
+  }
+
+  @Override
+  void startDocument() throws IOException {
+    first.startDocument();
+    second.startDocument();
+  }
+
+  @Override
+  void finishDocument() throws IOException {
+    first.finishDocument();
+    second.finishDocument();
+  }
+}

Property changes on: lucene/core/src/java/org/apache/lucene/index/TwoStoredFieldsConsumers.java
___________________________________________________________________
Added: svn:eol-style
## -0,0 +1 ##
+native
Index: lucene/core/src/java/org/apache/lucene/index/StoredFieldsConsumer.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/index/StoredFieldsConsumer.java	(revision 1442822)
+++ lucene/core/src/java/org/apache/lucene/index/StoredFieldsConsumer.java	(working copy)
@@ -19,131 +19,10 @@
 
 import java.io.IOException;
 
-import org.apache.lucene.codecs.Codec;
-import org.apache.lucene.codecs.StoredFieldsWriter;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.util.ArrayUtil;
-import org.apache.lucene.util.RamUsageEstimator;
-
-/** This is a DocFieldConsumer that writes stored fields. */
-final class StoredFieldsConsumer {
-
-  StoredFieldsWriter fieldsWriter;
-  final DocumentsWriterPerThread docWriter;
-  int lastDocID;
-
-  int freeCount;
-
-  final DocumentsWriterPerThread.DocState docState;
-  final Codec codec;
-
-  public StoredFieldsConsumer(DocumentsWriterPerThread docWriter) {
-    this.docWriter = docWriter;
-    this.docState = docWriter.docState;
-    this.codec = docWriter.codec;
-  }
-
-  private int numStoredFields;
-  private StorableField[] storedFields;
-  private FieldInfo[] fieldInfos;
-
-  public void reset() {
-    numStoredFields = 0;
-    storedFields = new StorableField[1];
-    fieldInfos = new FieldInfo[1];
-  }
-
-  public void startDocument() {
-    reset();
-  }
-
-  public void flush(SegmentWriteState state) throws IOException {
-    int numDocs = state.segmentInfo.getDocCount();
-
-    if (numDocs > 0) {
-      // It's possible that all documents seen in this segment
-      // hit non-aborting exceptions, in which case we will
-      // not have yet init'd the FieldsWriter:
-      initFieldsWriter(state.context);
-      fill(numDocs);
-    }
-
-    if (fieldsWriter != null) {
-      try {
-        fieldsWriter.finish(state.fieldInfos, numDocs);
-      } finally {
-        fieldsWriter.close();
-        fieldsWriter = null;
-        lastDocID = 0;
-      }
-    }
-  }
-
-  private synchronized void initFieldsWriter(IOContext context) throws IOException {
-    if (fieldsWriter == null) {
-      fieldsWriter = codec.storedFieldsFormat().fieldsWriter(docWriter.directory, docWriter.getSegmentInfo(), context);
-      lastDocID = 0;
-    }
-  }
-
-  int allocCount;
-
-  void abort() {
-    reset();
-
-    if (fieldsWriter != null) {
-      fieldsWriter.abort();
-      fieldsWriter = null;
-      lastDocID = 0;
-    }
-  }
-
-  /** Fills in any hole in the docIDs */
-  void fill(int docID) throws IOException {
-    // We must "catch up" for all docs before us
-    // that had no stored fields:
-    while(lastDocID < docID) {
-      fieldsWriter.startDocument(0);
-      lastDocID++;
-      fieldsWriter.finishDocument();
-    }
-  }
-
-  void finishDocument() throws IOException {
-    assert docWriter.writer.testPoint("StoredFieldsWriter.finishDocument start");
-
-    initFieldsWriter(IOContext.DEFAULT);
-    fill(docState.docID);
-
-    if (fieldsWriter != null && numStoredFields > 0) {
-      fieldsWriter.startDocument(numStoredFields);
-      for (int i = 0; i < numStoredFields; i++) {
-        fieldsWriter.writeField(fieldInfos[i], storedFields[i]);
-      }
-      fieldsWriter.finishDocument();
-      lastDocID++;
-    }
-
-    reset();
-    assert docWriter.writer.testPoint("StoredFieldsWriter.finishDocument end");
-  }
-
-  public void addField(StorableField field, FieldInfo fieldInfo) {
-    if (numStoredFields == storedFields.length) {
-      int newSize = ArrayUtil.oversize(numStoredFields + 1, RamUsageEstimator.NUM_BYTES_OBJECT_REF);
-      StorableField[] newArray = new StorableField[newSize];
-      System.arraycopy(storedFields, 0, newArray, 0, numStoredFields);
-      storedFields = newArray;
-      
-      FieldInfo[] newInfoArray = new FieldInfo[newSize];
-      System.arraycopy(fieldInfos, 0, newInfoArray, 0, numStoredFields);
-      fieldInfos = newInfoArray;
-    }
-
-    storedFields[numStoredFields] = field;
-    fieldInfos[numStoredFields] = fieldInfo;
-    numStoredFields++;
-
-    assert docState.testPoint("StoredFieldsWriterPerThread.processFields.writeField");
-  }
+abstract class StoredFieldsConsumer {
+  abstract void addField(int docID, StorableField field, FieldInfo fieldInfo) throws IOException;
+  abstract void flush(SegmentWriteState state) throws IOException;
+  abstract void abort() throws IOException;
+  abstract void startDocument() throws IOException;
+  abstract void finishDocument() throws IOException;
 }
Index: lucene/core/src/java/org/apache/lucene/index/IndexWriter.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/index/IndexWriter.java	(revision 1442822)
+++ lucene/core/src/java/org/apache/lucene/index/IndexWriter.java	(working copy)
@@ -761,25 +761,14 @@
    * If this {@link SegmentInfos} has no global field number map the returned instance is empty
    */
   private FieldNumbers getFieldNumberMap() throws IOException {
-    final FieldNumbers map  = new FieldNumbers();
+    final FieldNumbers map = new FieldNumbers();
 
-    SegmentInfoPerCommit biggest = null;
     for(SegmentInfoPerCommit info : segmentInfos) {
-      if (biggest == null || (info.info.getDocCount()-info.getDelCount()) > (biggest.info.getDocCount()-biggest.getDelCount())) {
-        biggest = info;
+      for(FieldInfo fi : getFieldInfos(info.info)) {
+        map.addOrGet(fi.name, fi.number, fi.getDocValuesType());
       }
     }
 
-    if (biggest != null) {
-      for(FieldInfo fi : getFieldInfos(biggest.info)) {
-        map.addOrGet(fi.name, fi.number);
-      }
-    }
-
-    // TODO: we could also pull DV type of each field here,
-    // and use that to make sure new segment(s) don't change
-    // the type...
-
     return map;
   }
   
@@ -1964,7 +1953,6 @@
       infoStream.message("IW", "rollback");
     }
     
-
     try {
       synchronized(this) {
         finishMerges(false);
@@ -2069,6 +2057,8 @@
       deleter.checkpoint(segmentInfos, false);
       deleter.refresh();
 
+      globalFieldNumberMap.clear();
+
       // Don't bother saving any changes in our segmentInfos
       readerPool.dropAll(false);
 
@@ -2312,7 +2302,10 @@
             }
 
             IOContext context = new IOContext(new MergeInfo(info.info.getDocCount(), info.info.sizeInBytes(), true, -1));
-          
+
+            for(FieldInfo fi : getFieldInfos(info.info)) {
+              globalFieldNumberMap.addOrGet(fi.name, fi.number, fi.getDocValuesType());
+            }
             infos.add(copySegmentAsIs(info, newSegName, context));
           }
         }
Index: lucene/core/src/java/org/apache/lucene/index/NumericDocValues.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/index/NumericDocValues.java	(revision 0)
+++ lucene/core/src/java/org/apache/lucene/index/NumericDocValues.java	(working copy)
@@ -0,0 +1,43 @@
+package org.apache.lucene.index;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * A per-document numeric value.
+ */
+public abstract class NumericDocValues {
+  
+  /** Sole constructor. (For invocation by subclass 
+   * constructors, typically implicit.) */
+  protected NumericDocValues() {}
+
+  /**
+   * Returns the numeric value for the specified document ID.
+   * @param docID document ID to lookup
+   * @return numeric value
+   */
+  public abstract long get(int docID);
+
+  /** An empty NumericDocValues which returns zero for every document */
+  public static final NumericDocValues EMPTY = new NumericDocValues() {
+    @Override
+    public long get(int docID) {
+      return 0;
+    }
+  };
+}

Property changes on: lucene/core/src/java/org/apache/lucene/index/NumericDocValues.java
___________________________________________________________________
Added: svn:eol-style
## -0,0 +1 ##
+native
Index: lucene/core/src/java/org/apache/lucene/index/DocInverter.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/index/DocInverter.java	(revision 1442822)
+++ lucene/core/src/java/org/apache/lucene/index/DocInverter.java	(working copy)
@@ -78,13 +78,7 @@
   }
 
   @Override
-  public boolean freeRAM() {
-    return consumer.freeRAM();
-  }
-
-  @Override
   public DocFieldConsumerPerField addField(FieldInfo fi) {
     return new DocInverterPerField(this, fi);
   }
-
 }
Index: lucene/core/src/java/org/apache/lucene/index/DocFieldProcessorPerField.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/index/DocFieldProcessorPerField.java	(revision 1442822)
+++ lucene/core/src/java/org/apache/lucene/index/DocFieldProcessorPerField.java	(working copy)
@@ -17,7 +17,12 @@
  * limitations under the License.
  */
 
+import java.util.HashMap;
+import java.util.Map;
+
 import org.apache.lucene.util.ArrayUtil;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.Counter;
 import org.apache.lucene.util.RamUsageEstimator;
 
 /**
@@ -28,16 +33,19 @@
 
   final DocFieldConsumerPerField consumer;
   final FieldInfo fieldInfo;
+  private final Counter bytesUsed;
 
   DocFieldProcessorPerField next;
   int lastGen = -1;
 
   int fieldCount;
   IndexableField[] fields = new IndexableField[1];
+  private final Map<FieldInfo,String> dvFields = new HashMap<FieldInfo,String>();
 
   public DocFieldProcessorPerField(final DocFieldProcessor docFieldProcessor, final FieldInfo fieldInfo) {
     this.consumer = docFieldProcessor.consumer.addField(fieldInfo);
     this.fieldInfo = fieldInfo;
+    this.bytesUsed = docFieldProcessor.bytesUsed;
   }
 
   public void addField(IndexableField field) {
Index: lucene/core/src/java/org/apache/lucene/index/SegmentReader.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/index/SegmentReader.java	(revision 1442822)
+++ lucene/core/src/java/org/apache/lucene/index/SegmentReader.java	(working copy)
@@ -20,7 +20,6 @@
 import java.io.IOException;
 
 import org.apache.lucene.store.Directory;
-import org.apache.lucene.codecs.PerDocProducer;
 import org.apache.lucene.codecs.StoredFieldsReader;
 import org.apache.lucene.codecs.TermVectorsReader;
 import org.apache.lucene.search.FieldCache; // javadocs
@@ -229,28 +228,31 @@
   public int getTermInfosIndexDivisor() {
     return core.termsIndexDivisor;
   }
-  
+
   @Override
-  public DocValues docValues(String field) throws IOException {
+  public NumericDocValues getNumericDocValues(String field) throws IOException {
     ensureOpen();
-    final PerDocProducer perDoc = core.perDocProducer;
-    if (perDoc == null) {
-      return null;
-    }
-    return perDoc.docValues(field);
+    return core.getNumericDocValues(field);
   }
-  
+
   @Override
-  public DocValues normValues(String field) throws IOException {
+  public BinaryDocValues getBinaryDocValues(String field) throws IOException {
     ensureOpen();
-    final PerDocProducer perDoc = core.norms;
-    if (perDoc == null) {
-      return null;
-    }
-    return perDoc.docValues(field);
+    return core.getBinaryDocValues(field);
   }
-  
 
+  @Override
+  public SortedDocValues getSortedDocValues(String field) throws IOException {
+    ensureOpen();
+    return core.getSortedDocValues(field);
+  }
+
+  @Override
+  public NumericDocValues getNormValues(String field) throws IOException {
+    ensureOpen();
+    return core.getNormValues(field);
+  }
+
   /**
    * Called when the shared core for this SegmentReader
    * is closed.
Index: lucene/core/src/java/org/apache/lucene/index/StoredFieldsProcessor.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/index/StoredFieldsProcessor.java	(revision 0)
+++ lucene/core/src/java/org/apache/lucene/index/StoredFieldsProcessor.java	(working copy)
@@ -0,0 +1,156 @@
+package org.apache.lucene.index;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.codecs.Codec;
+import org.apache.lucene.codecs.StoredFieldsWriter;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.util.ArrayUtil;
+import org.apache.lucene.util.RamUsageEstimator;
+
+/** This is a StoredFieldsConsumer that writes stored fields. */
+final class StoredFieldsProcessor extends StoredFieldsConsumer {
+
+  StoredFieldsWriter fieldsWriter;
+  final DocumentsWriterPerThread docWriter;
+  int lastDocID;
+
+  int freeCount;
+
+  final DocumentsWriterPerThread.DocState docState;
+  final Codec codec;
+
+  public StoredFieldsProcessor(DocumentsWriterPerThread docWriter) {
+    this.docWriter = docWriter;
+    this.docState = docWriter.docState;
+    this.codec = docWriter.codec;
+  }
+
+  private int numStoredFields;
+  private StorableField[] storedFields;
+  private FieldInfo[] fieldInfos;
+
+  public void reset() {
+    numStoredFields = 0;
+    storedFields = new StorableField[1];
+    fieldInfos = new FieldInfo[1];
+  }
+  
+  @Override
+  public void startDocument() {
+    reset();
+  }
+
+  @Override
+  public void flush(SegmentWriteState state) throws IOException {
+    int numDocs = state.segmentInfo.getDocCount();
+
+    if (numDocs > 0) {
+      // It's possible that all documents seen in this segment
+      // hit non-aborting exceptions, in which case we will
+      // not have yet init'd the FieldsWriter:
+      initFieldsWriter(state.context);
+      fill(numDocs);
+    }
+
+    if (fieldsWriter != null) {
+      try {
+        fieldsWriter.finish(state.fieldInfos, numDocs);
+      } finally {
+        fieldsWriter.close();
+        fieldsWriter = null;
+        lastDocID = 0;
+      }
+    }
+  }
+
+  private synchronized void initFieldsWriter(IOContext context) throws IOException {
+    if (fieldsWriter == null) {
+      fieldsWriter = codec.storedFieldsFormat().fieldsWriter(docWriter.directory, docWriter.getSegmentInfo(), context);
+      lastDocID = 0;
+    }
+  }
+
+  int allocCount;
+
+  @Override
+  void abort() {
+    reset();
+
+    if (fieldsWriter != null) {
+      fieldsWriter.abort();
+      fieldsWriter = null;
+      lastDocID = 0;
+    }
+  }
+
+  /** Fills in any hole in the docIDs */
+  void fill(int docID) throws IOException {
+    // We must "catch up" for all docs before us
+    // that had no stored fields:
+    while(lastDocID < docID) {
+      fieldsWriter.startDocument(0);
+      lastDocID++;
+      fieldsWriter.finishDocument();
+    }
+  }
+
+  @Override
+  void finishDocument() throws IOException {
+    assert docWriter.writer.testPoint("StoredFieldsWriter.finishDocument start");
+
+    initFieldsWriter(IOContext.DEFAULT);
+    fill(docState.docID);
+
+    if (fieldsWriter != null && numStoredFields > 0) {
+      fieldsWriter.startDocument(numStoredFields);
+      for (int i = 0; i < numStoredFields; i++) {
+        fieldsWriter.writeField(fieldInfos[i], storedFields[i]);
+      }
+      fieldsWriter.finishDocument();
+      lastDocID++;
+    }
+
+    reset();
+    assert docWriter.writer.testPoint("StoredFieldsWriter.finishDocument end");
+  }
+
+  @Override
+  public void addField(int docID, StorableField field, FieldInfo fieldInfo) {
+    if (field.fieldType().stored()) {
+      if (numStoredFields == storedFields.length) {
+        int newSize = ArrayUtil.oversize(numStoredFields + 1, RamUsageEstimator.NUM_BYTES_OBJECT_REF);
+        StorableField[] newArray = new StorableField[newSize];
+        System.arraycopy(storedFields, 0, newArray, 0, numStoredFields);
+        storedFields = newArray;
+      
+        FieldInfo[] newInfoArray = new FieldInfo[newSize];
+        System.arraycopy(fieldInfos, 0, newInfoArray, 0, numStoredFields);
+        fieldInfos = newInfoArray;
+      }
+
+      storedFields[numStoredFields] = field;
+      fieldInfos[numStoredFields] = fieldInfo;
+      numStoredFields++;
+
+      assert docState.testPoint("StoredFieldsWriterPerThread.processFields.writeField");
+    }
+  }
+}

Property changes on: lucene/core/src/java/org/apache/lucene/index/StoredFieldsProcessor.java
___________________________________________________________________
Added: svn:eol-style
## -0,0 +1 ##
+native
Index: lucene/core/src/java/org/apache/lucene/index/BinaryDocValues.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/index/BinaryDocValues.java	(revision 0)
+++ lucene/core/src/java/org/apache/lucene/index/BinaryDocValues.java	(working copy)
@@ -0,0 +1,48 @@
+package org.apache.lucene.index;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.util.BytesRef;
+
+/**
+ * A per-document byte[]
+ */
+public abstract class BinaryDocValues {
+  
+  /** Sole constructor. (For invocation by subclass 
+   * constructors, typically implicit.) */
+  protected BinaryDocValues() {}
+
+  /** Lookup the value for document. */
+  public abstract void get(int docID, BytesRef result);
+
+  /**
+   * Indicates the value was missing for the document.
+   */
+  public static final byte[] MISSING = new byte[0];
+  
+  /** An empty BinaryDocValues which returns {@link #MISSING} for every document */
+  public static final BinaryDocValues EMPTY = new BinaryDocValues() {
+    @Override
+    public void get(int docID, BytesRef result) {
+      result.bytes = MISSING;
+      result.offset = 0;
+      result.length = 0;
+    }
+  };
+}

Property changes on: lucene/core/src/java/org/apache/lucene/index/BinaryDocValues.java
___________________________________________________________________
Added: svn:eol-style
## -0,0 +1 ##
+native
Index: lucene/core/src/java/org/apache/lucene/index/DocValues.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/index/DocValues.java	(revision 1442822)
+++ lucene/core/src/java/org/apache/lucene/index/DocValues.java	(working copy)
@@ -1,815 +0,0 @@
-package org.apache.lucene.index;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-import java.io.Closeable;
-import java.io.IOException;
-import java.util.Arrays;
-import java.util.Comparator;
-
-import org.apache.lucene.codecs.DocValuesFormat;
-import org.apache.lucene.document.ByteDocValuesField; // javadocs
-import org.apache.lucene.document.DerefBytesDocValuesField; // javadocs
-import org.apache.lucene.document.DoubleDocValuesField; // javadocs
-import org.apache.lucene.document.Field; // javadocs
-import org.apache.lucene.document.FloatDocValuesField; // javadocs
-import org.apache.lucene.document.IntDocValuesField; // javadocs
-import org.apache.lucene.document.LongDocValuesField; // javadocs
-import org.apache.lucene.document.PackedLongDocValuesField; // javadocs
-import org.apache.lucene.document.ShortDocValuesField; // javadocs
-import org.apache.lucene.document.SortedBytesDocValuesField; // javadocs
-import org.apache.lucene.document.StraightBytesDocValuesField; // javadocs
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.CloseableThreadLocal;
-import org.apache.lucene.util.packed.PackedInts;
-
-/**
- * {@link DocValues} provides a dense per-document typed storage for fast
- * value access based on the lucene internal document id. {@link DocValues}
- * exposes two distinct APIs:
- * <ul>
- * <li>via {@link #getSource()} providing RAM resident random access</li>
- * <li>via {@link #getDirectSource()} providing on disk random access</li>
- * </ul> {@link DocValues} are exposed via
- * {@link AtomicReader#docValues(String)} on a per-segment basis. For best
- * performance {@link DocValues} should be consumed per-segment just like
- * IndexReader.
- * <p>
- * {@link DocValues} are fully integrated into the {@link DocValuesFormat} API.
- * <p>
- * NOTE: DocValues is a strongly typed per-field API. Type changes within an
- * indexing session can result in exceptions if the type has changed in a way that
- * the previously give type for a field can't promote the value without losing
- * information. For instance a field initially indexed with {@link Type#FIXED_INTS_32}
- * can promote a value with {@link Type#FIXED_INTS_8} but can't promote
- * {@link Type#FIXED_INTS_64}. During segment merging type-promotion exceptions are suppressed. 
- * Fields will be promoted to their common denominator or automatically transformed
- * into a 3rd type like {@link Type#BYTES_VAR_STRAIGHT} to prevent data loss and merge exceptions.
- * This behavior is considered <i>best-effort</i> might change in future releases.
- * </p>
-* <p>
- * DocValues are exposed via the {@link Field} API with type safe
- * specializations for each type variant:
- * <ul>
- * <li> {@link ByteDocValuesField} - for adding byte values to the index</li>
- * <li> {@link ShortDocValuesField} - for adding short values to the index</li>
- * <li> {@link IntDocValuesField} - for adding int values to the index</li>
- * <li> {@link LongDocValuesField} - for adding long values to the index</li>
- * <li> {@link FloatDocValuesField} - for adding float values to the index</li>
- * <li> {@link DoubleDocValuesField} - for adding double values to the index</li>
- * <li> {@link PackedLongDocValuesField} - for adding packed long values to the
- * index</li>
- * <li> {@link SortedBytesDocValuesField} - for adding sorted {@link BytesRef}
- * values to the index</li>
- * <li> {@link StraightBytesDocValuesField} - for adding straight
- * {@link BytesRef} values to the index</li>
- * <li> {@link DerefBytesDocValuesField} - for adding deref {@link BytesRef}
- * values to the index</li>
- * </ul>
- * See {@link Type} for limitations of each type variant.
- * <p> 
- * <p>
- * 
- * @see DocValuesFormat#docsConsumer(org.apache.lucene.index.PerDocWriteState)
- *      
- * @lucene.experimental
- */
-public abstract class DocValues implements Closeable {
-
-  /** Zero length DocValues array. */
-  public static final DocValues[] EMPTY_ARRAY = new DocValues[0];
-
-  private volatile SourceCache cache = new SourceCache.DirectSourceCache();
-  private final Object cacheLock = new Object();
-  /** Sole constructor. (For invocation by subclass 
-   *  constructors, typically implicit.) */
-  protected DocValues() {
-  }
-
-  /**
-   * Loads a new {@link Source} instance for this {@link DocValues} field
-   * instance. Source instances returned from this method are not cached. It is
-   * the callers responsibility to maintain the instance and release its
-   * resources once the source is not needed anymore.
-   * <p>
-   * For managed {@link Source} instances see {@link #getSource()}.
-   * 
-   * @see #getSource()
-   * @see #setCache(SourceCache)
-   */
-  protected abstract Source loadSource() throws IOException;
-
-  /**
-   * Returns a {@link Source} instance through the current {@link SourceCache}.
-   * Iff no {@link Source} has been loaded into the cache so far the source will
-   * be loaded through {@link #loadSource()} and passed to the {@link SourceCache}.
-   * The caller of this method should not close the obtained {@link Source}
-   * instance unless it is not needed for the rest of its life time.
-   * <p>
-   * {@link Source} instances obtained from this method are closed / released
-   * from the cache once this {@link DocValues} instance is closed by the
-   * {@link IndexReader}, {@link Fields} or the
-   * {@link DocValues} was created from.
-   */
-  public Source getSource() throws IOException {
-    return cache.load(this);
-  }
-  
-  /**
-   * Returns a disk resident {@link Source} instance through the current
-   * {@link SourceCache}. Direct Sources are cached per thread in the
-   * {@link SourceCache}. The obtained instance should not be shared with other
-   * threads.
-   */
-  public Source getDirectSource() throws IOException {
-    return this.cache.loadDirect(this);
-  }
-  
-
-  /**
-   * Loads a new {@link Source direct source} instance from this {@link DocValues} field
-   * instance. Source instances returned from this method are not cached. It is
-   * the callers responsibility to maintain the instance and release its
-   * resources once the source is not needed anymore.
-   * <p>
-   * For managed {@link Source direct source} instances see {@link #getDirectSource()}.
-   * 
-   * @see #getDirectSource()
-   * @see #setCache(SourceCache)
-   */
-  protected abstract Source loadDirectSource() throws IOException;
-
-  /**
-   * Returns the {@link Type} of this {@link DocValues} instance
-   */
-  public abstract Type getType();
-
-  /**
-   * Closes this {@link DocValues} instance. This method should only be called
-   * by the creator of this {@link DocValues} instance. API users should not
-   * close {@link DocValues} instances.
-   */
-  @Override
-  public void close() throws IOException {
-    cache.close(this);
-  }
-
-  /**
-   * Returns the size per value in bytes or <code>-1</code> iff size per value
-   * is variable.
-   * 
-   * @return the size per value in bytes or <code>-1</code> iff size per value
-   * is variable.
-   */
-  public int getValueSize() {
-    return -1;
-  }
-
-  /**
-   * Sets the {@link SourceCache} used by this {@link DocValues} instance. This
-   * method should be called before {@link #loadSource()} is called. All {@link Source} instances in the currently used cache will be closed
-   * before the new cache is installed.
-   * <p>
-   * Note: All instances previously obtained from {@link #loadSource()} will be lost.
-   * 
-   * @throws IllegalArgumentException
-   *           if the given cache is <code>null</code>
-   * 
-   */
-  public void setCache(SourceCache cache) {
-    if (cache == null)
-      throw new IllegalArgumentException("cache must not be null");
-    synchronized (cacheLock) {
-      SourceCache toClose = this.cache;
-      this.cache = cache;
-      toClose.close(this);
-    }
-  }
-  /**
-   * Returns the currently used cache instance;
-   * @see #setCache(SourceCache)
-   */
-  // for tests
-  SourceCache getCache() {
-    return cache;
-  }
-
-  /**
-   * Source of per document values like long, double or {@link BytesRef}
-   * depending on the {@link DocValues} fields {@link Type}. Source
-   * implementations provide random access semantics similar to array lookups
-   * <p>
-   * @see DocValues#getSource()
-   * @see DocValues#getDirectSource()
-   */
-  public static abstract class Source {
-
-    /** {@link Type} of this {@code Source}. */
-    protected final Type type;
-
-    /** Sole constructor. (For invocation by subclass 
-     *  constructors, typically implicit.) */
-    protected Source(Type type) {
-      this.type = type;
-    }
-
-    /**
-     * Returns a <tt>long</tt> for the given document id or throws an
-     * {@link UnsupportedOperationException} if this source doesn't support
-     * <tt>long</tt> values.
-     * 
-     * @throws UnsupportedOperationException
-     *           if this source doesn't support <tt>long</tt> values.
-     */
-    public long getInt(int docID) {
-      throw new UnsupportedOperationException("ints are not supported");
-    }
-
-    /**
-     * Returns a <tt>double</tt> for the given document id or throws an
-     * {@link UnsupportedOperationException} if this source doesn't support
-     * <tt>double</tt> values.
-     * 
-     * @throws UnsupportedOperationException
-     *           if this source doesn't support <tt>double</tt> values.
-     */
-    public double getFloat(int docID) {
-      throw new UnsupportedOperationException("floats are not supported");
-    }
-
-    /**
-     * Returns a {@link BytesRef} for the given document id or throws an
-     * {@link UnsupportedOperationException} if this source doesn't support
-     * <tt>byte[]</tt> values.
-     * 
-     * @throws UnsupportedOperationException
-     *           if this source doesn't support <tt>byte[]</tt> values.
-     */
-    public BytesRef getBytes(int docID, BytesRef ref) {
-      throw new UnsupportedOperationException("bytes are not supported");
-    }
-
-    /**
-     * Returns the {@link Type} of this source.
-     * 
-     * @return the {@link Type} of this source.
-     */
-    public Type getType() {
-      return type;
-    }
-
-    /**
-     * Returns <code>true</code> iff this {@link Source} exposes an array via
-     * {@link #getArray()} otherwise <code>false</code>.
-     * 
-     * @return <code>true</code> iff this {@link Source} exposes an array via
-     *         {@link #getArray()} otherwise <code>false</code>.
-     */
-    public boolean hasArray() {
-      return false;
-    }
-
-    /**
-     * Returns the internal array representation iff this {@link Source} uses an
-     * array as its inner representation, otherwise <code>UOE</code>.
-     */
-    public Object getArray() {
-      throw new UnsupportedOperationException("getArray is not supported");
-    }
-    
-    /**
-     * If this {@link Source} is sorted this method will return an instance of
-     * {@link SortedSource} otherwise <code>UOE</code>
-     */
-    public SortedSource asSortedSource() {
-      throw new UnsupportedOperationException("asSortedSource is not supported");
-    }
-  }
-
-  /**
-   * A sorted variant of {@link Source} for <tt>byte[]</tt> values per document.
-   * <p>
-   */
-  public static abstract class SortedSource extends Source {
-
-    private final Comparator<BytesRef> comparator;
-
-    /** Sole constructor. (For invocation by subclass 
-     * constructors, typically implicit.) */
-    protected SortedSource(Type type, Comparator<BytesRef> comparator) {
-      super(type);
-      this.comparator = comparator;
-    }
-
-    @Override
-    public BytesRef getBytes(int docID, BytesRef bytesRef) {
-      final int ord = ord(docID);
-      if (ord < 0) {
-        // Negative ord means doc was missing?
-        bytesRef.length = 0;
-      } else {
-        getByOrd(ord, bytesRef);
-      }
-      return bytesRef;
-    }
-
-    /**
-     * Returns ord for specified docID. Ord is dense, ie, starts at 0, then increments by 1
-     * for the next (as defined by {@link Comparator} value.
-     */
-    public abstract int ord(int docID);
-
-    /** Returns value for specified ord. */
-    public abstract BytesRef getByOrd(int ord, BytesRef result);
-
-    /** Return true if it's safe to call {@link
-     *  #getDocToOrd}. */
-    public boolean hasPackedDocToOrd() {
-      return false;
-    }
-
-    /**
-     * Returns the PackedInts.Reader impl that maps document to ord.
-     */
-    public abstract PackedInts.Reader getDocToOrd();
-    
-    /**
-     * Returns the comparator used to order the BytesRefs.
-     */
-    public Comparator<BytesRef> getComparator() {
-      return comparator;
-    }
-
-    /**
-     * Lookup ord by value.
-     * 
-     * @param value
-     *          the value to look up
-     * @param spare
-     *          a spare {@link BytesRef} instance used to compare internal
-     *          values to the given value. Must not be <code>null</code>
-     * @return the given values ordinal if found or otherwise
-     *         <code>(-(ord)-1)</code>, defined as the ordinal of the first
-     *         element that is greater than the given value (the insertion
-     *         point). This guarantees that the return value will always be
-     *         &gt;= 0 if the given value is found.
-     */
-    public int getOrdByValue(BytesRef value, BytesRef spare) {
-      return binarySearch(value, spare, 0, getValueCount() - 1);
-    }    
-
-    private int binarySearch(BytesRef b, BytesRef bytesRef, int low,
-        int high) {
-      int mid = 0;
-      while (low <= high) {
-        mid = (low + high) >>> 1;
-        getByOrd(mid, bytesRef);
-        final int cmp = comparator.compare(bytesRef, b);
-        if (cmp < 0) {
-          low = mid + 1;
-        } else if (cmp > 0) {
-          high = mid - 1;
-        } else {
-          return mid;
-        }
-      }
-      assert comparator.compare(bytesRef, b) != 0;
-      return -(low + 1);
-    }
-    
-    @Override
-    public SortedSource asSortedSource() {
-      return this;
-    }
-    
-    /**
-     * Returns the number of unique values in this sorted source
-     */
-    public abstract int getValueCount();
-  }
-
-  /** Returns a Source that always returns default (missing)
-   *  values for all documents. */
-  public static Source getDefaultSource(final Type type) {
-    return new Source(type) {
-      @Override
-      public long getInt(int docID) {
-        return 0;
-      }
-
-      @Override
-      public double getFloat(int docID) {
-        return 0.0;
-      }
-
-      @Override
-      public BytesRef getBytes(int docID, BytesRef ref) {
-        ref.length = 0;
-        return ref;
-      }
-    };
-  }
-
-  /** Returns a SortedSource that always returns default (missing)
-   *  values for all documents. */
-  public static SortedSource getDefaultSortedSource(final Type type, final int size) {
-
-    final PackedInts.Reader docToOrd = new PackedInts.Reader() {
-      @Override
-      public long get(int index) {
-        return 0;
-      }
-
-      @Override
-      public int getBitsPerValue() {
-        return 0;
-      }
-
-      @Override
-      public int size() {
-        return size;
-      }
-
-      @Override
-      public boolean hasArray() {
-        return false;
-      }
-
-      @Override
-      public Object getArray() {
-        return null;
-      }
-
-      @Override
-      public int get(int index, long[] arr, int off, int len) {
-        len = Math.min(len, size() - index);
-        Arrays.fill(arr, off, off+len, 0);
-        return len;
-      }
-
-      @Override
-      public long ramBytesUsed() {
-        return 0;
-      }
-    };
-
-    return new SortedSource(type, BytesRef.getUTF8SortedAsUnicodeComparator()) {
-
-      @Override
-      public BytesRef getBytes(int docID, BytesRef ref) {
-        ref.length = 0;
-        return ref;
-      }
-
-      @Override
-      public int ord(int docID) {
-        return 0;
-      }
-
-      @Override
-      public BytesRef getByOrd(int ord, BytesRef bytesRef) {
-        assert ord == 0;
-        bytesRef.length = 0;
-        return bytesRef;
-      }
-
-      @Override
-      public boolean hasPackedDocToOrd() {
-        return true;
-      }
-
-      @Override
-      public PackedInts.Reader getDocToOrd() {
-        return docToOrd;
-      }
-
-      @Override
-      public int getOrdByValue(BytesRef value, BytesRef spare) {
-        if (value.length == 0) {
-          return 0;
-        } else {
-          return -1;
-        }
-      }
-
-      @Override
-      public int getValueCount() {
-        return 1;
-      }
-    };
-  }
-  
-  /**
-   * <code>Type</code> specifies the {@link DocValues} type for a
-   * certain field. A <code>Type</code> only defines the data type for a field
-   * while the actual implementation used to encode and decode the values depends
-   * on the the {@link DocValuesFormat#docsConsumer} and {@link DocValuesFormat#docsProducer} methods.
-   * 
-   * @lucene.experimental
-   */
-  public static enum Type {
-
-    /**
-     * A variable bit signed integer value. By default this type uses
-     * {@link PackedInts} to compress the values, as an offset
-     * from the minimum value, as long as the value range
-     * fits into 2<sup>63</sup>-1. Otherwise,
-     * the default implementation falls back to fixed size 64bit
-     * integers ({@link #FIXED_INTS_64}).
-     * <p>
-     * NOTE: this type uses <tt>0</tt> as the default value without any
-     * distinction between provided <tt>0</tt> values during indexing. All
-     * documents without an explicit value will use <tt>0</tt> instead.
-     * Custom default values must be assigned explicitly.
-     * </p>
-     */
-    VAR_INTS,
-    
-    /**
-     * A 8 bit signed integer value. {@link Source} instances of
-     * this type return a <tt>byte</tt> array from {@link Source#getArray()}
-     * <p>
-     * NOTE: this type uses <tt>0</tt> as the default value without any
-     * distinction between provided <tt>0</tt> values during indexing. All
-     * documents without an explicit value will use <tt>0</tt> instead.
-     * Custom default values must be assigned explicitly.
-     * </p>
-     */
-    FIXED_INTS_8,
-    
-    /**
-     * A 16 bit signed integer value. {@link Source} instances of
-     * this type return a <tt>short</tt> array from {@link Source#getArray()}
-     * <p>
-     * NOTE: this type uses <tt>0</tt> as the default value without any
-     * distinction between provided <tt>0</tt> values during indexing. All
-     * documents without an explicit value will use <tt>0</tt> instead.
-     * Custom default values must be assigned explicitly.
-     * </p>
-     */
-    FIXED_INTS_16,
-    
-    /**
-     * A 32 bit signed integer value. {@link Source} instances of
-     * this type return a <tt>int</tt> array from {@link Source#getArray()}
-     * <p>
-     * NOTE: this type uses <tt>0</tt> as the default value without any
-     * distinction between provided <tt>0</tt> values during indexing. All
-     * documents without an explicit value will use <tt>0</tt> instead. 
-     * Custom default values must be assigned explicitly.
-     * </p>
-     */
-    FIXED_INTS_32,
-
-    /**
-     * A 64 bit signed integer value. {@link Source} instances of
-     * this type return a <tt>long</tt> array from {@link Source#getArray()}
-     * <p>
-     * NOTE: this type uses <tt>0</tt> as the default value without any
-     * distinction between provided <tt>0</tt> values during indexing. All
-     * documents without an explicit value will use <tt>0</tt> instead.
-     * Custom default values must be assigned explicitly.
-     * </p>
-     */
-    FIXED_INTS_64,
-
-    /**
-     * A 32 bit floating point value. By default there is no compression
-     * applied. To fit custom float values into less than 32bit either a custom
-     * implementation is needed or values must be encoded into a
-     * {@link #BYTES_FIXED_STRAIGHT} type. {@link Source} instances of
-     * this type return a <tt>float</tt> array from {@link Source#getArray()}
-     * <p>
-     * NOTE: this type uses <tt>0.0f</tt> as the default value without any
-     * distinction between provided <tt>0.0f</tt> values during indexing. All
-     * documents without an explicit value will use <tt>0.0f</tt> instead.
-     * Custom default values must be assigned explicitly.
-     * </p>
-     */
-    FLOAT_32,
-
-    /**
-     * 
-     * A 64 bit floating point value. By default there is no compression
-     * applied. To fit custom float values into less than 64bit either a custom
-     * implementation is needed or values must be encoded into a
-     * {@link #BYTES_FIXED_STRAIGHT} type. {@link Source} instances of
-     * this type return a <tt>double</tt> array from {@link Source#getArray()}
-     * <p>
-     * NOTE: this type uses <tt>0.0d</tt> as the default value without any
-     * distinction between provided <tt>0.0d</tt> values during indexing. All
-     * documents without an explicit value will use <tt>0.0d</tt> instead.
-     * Custom default values must be assigned explicitly.
-     * </p>
-     */
-    FLOAT_64,
-
-    // TODO(simonw): -- shouldn't lucene decide/detect straight vs
-    // deref, as well fixed vs var?
-    /**
-     * A fixed length straight byte[]. All values added to
-     * such a field must be of the same length. All bytes are stored sequentially
-     * for fast offset access.
-     * <p>
-     * NOTE: this type uses <tt>0 byte</tt> filled byte[] based on the length of the first seen
-     * value as the default value without any distinction between explicitly
-     * provided values during indexing. All documents without an explicit value
-     * will use the default instead.Custom default values must be assigned explicitly.
-     * </p>
-     */
-    BYTES_FIXED_STRAIGHT,
-
-    /**
-     * A fixed length dereferenced byte[] variant. Fields with
-     * this type only store distinct byte values and store an additional offset
-     * pointer per document to dereference the shared byte[].
-     * Use this type if your documents may share the same byte[].
-     * <p>
-     * NOTE: Fields of this type will not store values for documents without an
-     * explicitly provided value. If a documents value is accessed while no
-     * explicit value is stored the returned {@link BytesRef} will be a 0-length
-     * reference. Custom default values must be assigned explicitly.
-     * </p>
-     */
-    BYTES_FIXED_DEREF,
-
-    /**
-     * Variable length straight stored byte[] variant. All bytes are
-     * stored sequentially for compactness. Usage of this type via the
-     * disk-resident API might yield performance degradation since no additional
-     * index is used to advance by more than one document value at a time.
-     * <p>
-     * NOTE: Fields of this type will not store values for documents without an
-     * explicitly provided value. If a documents value is accessed while no
-     * explicit value is stored the returned {@link BytesRef} will be a 0-length
-     * byte[] reference. Custom default values must be assigned explicitly.
-     * </p>
-     */
-    BYTES_VAR_STRAIGHT,
-
-    /**
-     * A variable length dereferenced byte[]. Just like
-     * {@link #BYTES_FIXED_DEREF}, but allowing each
-     * document's value to be a different length.
-     * <p>
-     * NOTE: Fields of this type will not store values for documents without an
-     * explicitly provided value. If a documents value is accessed while no
-     * explicit value is stored the returned {@link BytesRef} will be a 0-length
-     * reference. Custom default values must be assigned explicitly.
-     * </p>
-     */
-    BYTES_VAR_DEREF,
-
-
-    /**
-     * A variable length pre-sorted byte[] variant. Just like
-     * {@link #BYTES_FIXED_SORTED}, but allowing each
-     * document's value to be a different length.
-     * <p>
-     * NOTE: Fields of this type will not store values for documents without an
-     * explicitly provided value. If a documents value is accessed while no
-     * explicit value is stored the returned {@link BytesRef} will be a 0-length
-     * reference.Custom default values must be assigned explicitly.
-     * </p>
-     * 
-     * @see SortedSource
-     */
-    BYTES_VAR_SORTED,
-    
-    /**
-     * A fixed length pre-sorted byte[] variant. Fields with this type only
-     * store distinct byte values and store an additional offset pointer per
-     * document to dereference the shared byte[]. The stored
-     * byte[] is presorted, by default by unsigned byte order,
-     * and allows access via document id, ordinal and by-value.
-     * Use this type if your documents may share the same byte[].
-     * <p>
-     * NOTE: Fields of this type will not store values for documents without an
-     * explicitly provided value. If a documents value is accessed while no
-     * explicit value is stored the returned {@link BytesRef} will be a 0-length
-     * reference. Custom default values must be assigned
-     * explicitly.
-     * </p>
-     * 
-     * @see SortedSource
-     */
-    BYTES_FIXED_SORTED
-  }
-  
-  /**
-   * Abstract base class for {@link DocValues} {@link Source} cache.
-   * <p>
-   * {@link Source} instances loaded via {@link DocValues#loadSource()} are entirely memory resident
-   * and need to be maintained by the caller. Each call to
-   * {@link DocValues#loadSource()} will cause an entire reload of
-   * the underlying data. Source instances obtained from
-   * {@link DocValues#getSource()} and {@link DocValues#getSource()}
-   * respectively are maintained by a {@link SourceCache} that is closed (
-   * {@link #close(DocValues)}) once the {@link IndexReader} that created the
-   * {@link DocValues} instance is closed.
-   * <p>
-   * Unless {@link Source} instances are managed by another entity it is
-   * recommended to use the cached variants to obtain a source instance.
-   * <p>
-   * Implementation of this API must be thread-safe.
-   * 
-   * @see DocValues#setCache(SourceCache)
-   * @see DocValues#getSource()
-   * 
-   * @lucene.experimental
-   */
-  public static abstract class SourceCache {
-
-    /** Sole constructor. (For invocation by subclass 
-     * constructors, typically implicit.) */
-    protected SourceCache() {
-    }
-
-    /**
-     * Atomically loads a {@link Source} into the cache from the given
-     * {@link DocValues} and returns it iff no other {@link Source} has already
-     * been cached. Otherwise the cached source is returned.
-     * <p>
-     * This method will not return <code>null</code>
-     */
-    public abstract Source load(DocValues values) throws IOException;
-    
-    /**
-     * Atomically loads a {@link Source direct source} into the per-thread cache from the given
-     * {@link DocValues} and returns it iff no other {@link Source direct source} has already
-     * been cached. Otherwise the cached source is returned.
-     * <p>
-     * This method will not return <code>null</code>
-     */
-    public abstract Source loadDirect(DocValues values) throws IOException;
-
-    /**
-     * Atomically invalidates the cached {@link Source} 
-     * instances if any and empties the cache.
-     */
-    public abstract void invalidate(DocValues values);
-
-    /**
-     * Atomically closes the cache and frees all resources.
-     */
-    public synchronized void close(DocValues values) {
-      invalidate(values);
-    }
-
-    /**
-     * Simple per {@link DocValues} instance cache implementation that holds a
-     * {@link Source} a member variable.
-     * <p>
-     * If a {@link DirectSourceCache} instance is closed or invalidated the cached
-     * reference are simply set to <code>null</code>
-     */
-    public static final class DirectSourceCache extends SourceCache {
-      private Source ref;
-      private final CloseableThreadLocal<Source> directSourceCache = new CloseableThreadLocal<Source>();
-      
-      /** Sole constructor. */
-      public DirectSourceCache() {
-      }
-
-      @Override
-      public synchronized Source load(DocValues values) throws IOException {
-        if (ref == null) {
-          ref = values.loadSource();
-        }
-        return ref;
-      }
-
-      @Override
-      public synchronized void invalidate(DocValues values) {
-        ref = null;
-        directSourceCache.close();
-      }
-
-      @Override
-      public synchronized Source loadDirect(DocValues values) throws IOException {
-        final Source source = directSourceCache.get();
-        if (source == null) {
-          final Source loadDirectSource = values.loadDirectSource();
-          directSourceCache.set(loadDirectSource);
-          return loadDirectSource;
-        } else {
-          return source;
-        }
-      }
-    }
-  }
-}
Index: lucene/core/src/java/org/apache/lucene/index/SortedDocValuesTermsEnum.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/index/SortedDocValuesTermsEnum.java	(revision 0)
+++ lucene/core/src/java/org/apache/lucene/index/SortedDocValuesTermsEnum.java	(working copy)
@@ -0,0 +1,139 @@
+package org.apache.lucene.index;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Comparator;
+
+import org.apache.lucene.util.Bits;
+import org.apache.lucene.util.BytesRef;
+
+/** Implements a {@link TermsEnum} wrapping a provided
+ * {@link SortedDocValues}. */
+
+public class SortedDocValuesTermsEnum extends TermsEnum {
+  private final SortedDocValues values;
+  private int currentOrd = -1;
+  private final BytesRef term = new BytesRef();
+
+  /** Creates a new TermsEnum over the provided values */
+  public SortedDocValuesTermsEnum(SortedDocValues values) {
+    this.values = values;
+  }
+
+  @Override
+  public SeekStatus seekCeil(BytesRef text, boolean useCache /* ignored */) throws IOException {
+    int ord = values.lookupTerm(text);
+    if (ord >= 0) {
+      currentOrd = ord;
+      term.offset = 0;
+      // TODO: is there a cleaner way?
+      // term.bytes may be pointing to codec-private byte[]
+      // storage, so we must force new byte[] allocation:
+      term.bytes = new byte[text.length];
+      term.copyBytes(text);
+      return SeekStatus.FOUND;
+    } else {
+      currentOrd = -ord-1;
+      if (currentOrd == values.getValueCount()) {
+        return SeekStatus.END;
+      } else {
+        // TODO: hmm can we avoid this "extra" lookup?:
+        values.lookupOrd(currentOrd, term);
+        return SeekStatus.NOT_FOUND;
+      }
+    }
+  }
+
+  @Override
+  public boolean seekExact(BytesRef text, boolean useCache) throws IOException {
+    int ord = values.lookupTerm(text);
+    if (ord >= 0) {
+      currentOrd = ord;
+      return true;
+    } else {
+      return false;
+    }
+  }
+
+  @Override
+  public void seekExact(long ord) throws IOException {
+    assert ord >= 0 && ord < values.getValueCount();
+    currentOrd = (int) ord;
+    values.lookupOrd(currentOrd, term);
+  }
+
+  @Override
+  public BytesRef next() throws IOException {
+    currentOrd++;
+    if (currentOrd >= values.getValueCount()) {
+      return null;
+    }
+    values.lookupOrd(currentOrd, term);
+    return term;
+  }
+
+  @Override
+  public BytesRef term() throws IOException {
+    return term;
+  }
+
+  @Override
+  public long ord() throws IOException {
+    return currentOrd;
+  }
+
+  @Override
+  public int docFreq() {
+    throw new UnsupportedOperationException();
+  }
+
+  @Override
+  public long totalTermFreq() {
+    return -1;
+  }
+
+  @Override
+  public DocsEnum docs(Bits liveDocs, DocsEnum reuse, int flags) throws IOException {
+    throw new UnsupportedOperationException();
+  }
+
+  @Override
+  public DocsAndPositionsEnum docsAndPositions(Bits liveDocs, DocsAndPositionsEnum reuse, int flags) throws IOException {
+    throw new UnsupportedOperationException();
+  }
+
+  @Override
+  public Comparator<BytesRef> getComparator() {
+    return BytesRef.getUTF8SortedAsUnicodeComparator();
+  }
+
+  @Override
+  public void seekExact(BytesRef term, TermState state) throws IOException {
+    assert state != null && state instanceof OrdTermState;
+    this.seekExact(((OrdTermState)state).ord);
+  }
+
+  @Override
+  public TermState termState() throws IOException {
+    OrdTermState state = new OrdTermState();
+    state.ord = currentOrd;
+    return state;
+  }
+}
+

Property changes on: lucene/core/src/java/org/apache/lucene/index/SortedDocValuesTermsEnum.java
___________________________________________________________________
Added: svn:eol-style
## -0,0 +1 ##
+native
Index: lucene/core/src/java/org/apache/lucene/index/NormsConsumer.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/index/NormsConsumer.java	(revision 1442822)
+++ lucene/core/src/java/org/apache/lucene/index/NormsConsumer.java	(working copy)
@@ -22,8 +22,7 @@
 
 import org.apache.lucene.codecs.DocValuesConsumer;
 import org.apache.lucene.codecs.NormsFormat;
-import org.apache.lucene.codecs.PerDocConsumer;
-import org.apache.lucene.index.DocValues.Type;
+import org.apache.lucene.index.FieldInfo.DocValuesType;
 import org.apache.lucene.util.IOUtils;
 
 // TODO FI: norms could actually be stored as doc store
@@ -34,54 +33,40 @@
  */
 
 final class NormsConsumer extends InvertedDocEndConsumer {
-  private final NormsFormat normsFormat;
-  private PerDocConsumer consumer;
-  
-  public NormsConsumer(DocumentsWriterPerThread dwpt) {
-    normsFormat = dwpt.codec.normsFormat();
-  }
 
   @Override
-  public void abort(){
-    if (consumer != null) {
-      consumer.abort();
-    }
-  }
-
-  /** Produce _X.nrm if any document had a field with norms
-   *  not disabled */
+  void abort() {}
+  
   @Override
   public void flush(Map<String,InvertedDocEndConsumerPerField> fieldsToFlush, SegmentWriteState state) throws IOException {
     boolean success = false;
-    boolean anythingFlushed = false;
+    DocValuesConsumer normsConsumer = null;
     try {
       if (state.fieldInfos.hasNorms()) {
+        NormsFormat normsFormat = state.segmentInfo.getCodec().normsFormat();
+        assert normsFormat != null;
+        normsConsumer = normsFormat.normsConsumer(state);
+
         for (FieldInfo fi : state.fieldInfos) {
           final NormsConsumerPerField toWrite = (NormsConsumerPerField) fieldsToFlush.get(fi.name);
           // we must check the final value of omitNorms for the fieldinfo, it could have 
           // changed for this field since the first time we added it.
           if (!fi.omitsNorms()) {
-            if (toWrite != null && toWrite.initialized()) {
-              anythingFlushed = true;
-              final Type type = toWrite.flush(state.segmentInfo.getDocCount());
-              assert fi.getNormType() == type;
+            if (toWrite != null && !toWrite.isEmpty()) {
+              toWrite.flush(state, normsConsumer);
+              assert fi.getNormType() == DocValuesType.NUMERIC;
             } else if (fi.isIndexed()) {
-              anythingFlushed = true;
               assert fi.getNormType() == null: "got " + fi.getNormType() + "; field=" + fi.name;
             }
           }
         }
-      } 
-      
+      }
       success = true;
-      if (!anythingFlushed && consumer != null) {
-        consumer.abort();
-      }
     } finally {
       if (success) {
-        IOUtils.close(consumer);
+        IOUtils.close(normsConsumer);
       } else {
-        IOUtils.closeWhileHandlingException(consumer);
+        IOUtils.closeWhileHandlingException(normsConsumer);
       }
     }
   }
@@ -93,18 +78,7 @@
   void startDocument() {}
 
   @Override
-  InvertedDocEndConsumerPerField addField(DocInverterPerField docInverterPerField,
-      FieldInfo fieldInfo) {
+  InvertedDocEndConsumerPerField addField(DocInverterPerField docInverterPerField, FieldInfo fieldInfo) {
     return new NormsConsumerPerField(docInverterPerField, fieldInfo, this);
   }
-  
-  DocValuesConsumer newConsumer(PerDocWriteState perDocWriteState,
-      FieldInfo fieldInfo, Type type) throws IOException {
-    if (consumer == null) {
-      consumer = normsFormat.docsConsumer(perDocWriteState);
-    }
-    DocValuesConsumer addValuesField = consumer.addValuesField(type, fieldInfo);
-    return addValuesField;
-  }
-  
 }
Index: lucene/core/src/java/org/apache/lucene/index/FieldInfos.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/index/FieldInfos.java	(revision 1442822)
+++ lucene/core/src/java/org/apache/lucene/index/FieldInfos.java	(working copy)
@@ -25,6 +25,7 @@
 import java.util.SortedMap;
 import java.util.TreeMap;
 
+import org.apache.lucene.index.FieldInfo.DocValuesType;
 import org.apache.lucene.index.FieldInfo.IndexOptions;
 
 /** 
@@ -162,11 +163,20 @@
     
     private final Map<Integer,String> numberToName;
     private final Map<String,Integer> nameToNumber;
+    // We use this to enforce that a given field never
+    // changes DV type, even across segments / IndexWriter
+    // sessions:
+    private final Map<String,DocValuesType> docValuesType;
+
+    // TODO: we should similarly catch an attempt to turn
+    // norms back on after they were already ommitted; today
+    // we silently discard the norm but this is badly trappy
     private int lowestUnassignedFieldNumber = -1;
     
     FieldNumbers() {
       this.nameToNumber = new HashMap<String, Integer>();
       this.numberToName = new HashMap<Integer, String>();
+      this.docValuesType = new HashMap<String,DocValuesType>();
     }
     
     /**
@@ -175,7 +185,15 @@
      * number assigned if possible otherwise the first unassigned field number
      * is used as the field number.
      */
-    synchronized int addOrGet(String fieldName, int preferredFieldNumber) {
+    synchronized int addOrGet(String fieldName, int preferredFieldNumber, DocValuesType dvType) {
+      if (dvType != null) {
+        DocValuesType currentDVType = docValuesType.get(fieldName);
+        if (currentDVType == null) {
+          docValuesType.put(fieldName, dvType);
+        } else if (currentDVType != null && currentDVType != dvType) {
+          throw new IllegalArgumentException("cannot change DocValues type from " + currentDVType + " to " + dvType + " for field \"" + fieldName + "\"");
+        }
+      }
       Integer fieldNumber = nameToNumber.get(fieldName);
       if (fieldNumber == null) {
         final Integer preferredBoxed = Integer.valueOf(preferredFieldNumber);
@@ -198,25 +216,18 @@
       return fieldNumber.intValue();
     }
 
-    /**
-     * Sets the given field number and name if not yet set. 
-     */
-    synchronized void setIfNotSet(int fieldNumber, String fieldName) {
-      final Integer boxedFieldNumber = Integer.valueOf(fieldNumber);
-      if (!numberToName.containsKey(boxedFieldNumber)
-          && !nameToNumber.containsKey(fieldName)) {
-        numberToName.put(boxedFieldNumber, fieldName);
-        nameToNumber.put(fieldName, boxedFieldNumber);
-      } else {
-        assert containsConsistent(boxedFieldNumber, fieldName);
-      }
-    }
-    
     // used by assert
-    synchronized boolean containsConsistent(Integer number, String name) {
+    synchronized boolean containsConsistent(Integer number, String name, DocValuesType dvType) {
       return name.equals(numberToName.get(number))
-          && number.equals(nameToNumber.get(name));
+          && number.equals(nameToNumber.get(name)) &&
+        (dvType == null || docValuesType.get(name) == null || dvType == docValuesType.get(name));
     }
+
+    synchronized void clear() {
+      numberToName.clear();
+      nameToNumber.clear();
+      docValuesType.clear();
+    }
   }
   
   static final class Builder {
@@ -241,35 +252,6 @@
       }
     }
    
-    /**
-     * adds the given field to this FieldInfos name / number mapping. The given FI
-     * must be present in the global field number mapping before this method it
-     * called
-     */
-    private void putInternal(FieldInfo fi) {
-      assert !byName.containsKey(fi.name);
-      assert globalFieldNumbers.containsConsistent(Integer.valueOf(fi.number), fi.name);
-      byName.put(fi.name, fi);
-    }
-    
-    /** If the field is not yet known, adds it. If it is known, checks to make
-     *  sure that the isIndexed flag is the same as was given previously for this
-     *  field. If not - marks it as being indexed.  Same goes for the TermVector
-     * parameters.
-     *
-     * @param name The name of the field
-     * @param isIndexed true if the field is indexed
-     * @param storeTermVector true if the term vector should be stored
-     * @param omitNorms true if the norms for the indexed field should be omitted
-     * @param storePayloads true if payloads should be stored for this field
-     * @param indexOptions if term freqs should be omitted for this field
-     */
-    // TODO: fix testCodecs to do this another way, its the only user of this
-    FieldInfo addOrUpdate(String name, boolean isIndexed, boolean storeTermVector,
-                         boolean omitNorms, boolean storePayloads, IndexOptions indexOptions, DocValues.Type docValues, DocValues.Type normType) {
-      return addOrUpdateInternal(name, -1, isIndexed, storeTermVector, omitNorms, storePayloads, indexOptions, docValues, normType);
-    }
-
     /** NOTE: this method does not carry over termVector
      *  booleans nor docValuesType; the indexer chain
      *  (TermVectorsConsumerPerField, DocFieldProcessor) must
@@ -283,22 +265,31 @@
       // be updated by maybe FreqProxTermsWriterPerField:
       return addOrUpdateInternal(name, -1, fieldType.indexed(), false,
                                  fieldType.omitNorms(), false,
-                                 fieldType.indexOptions(), null, null);
+                                 fieldType.indexOptions(), fieldType.docValueType(), null);
     }
 
     private FieldInfo addOrUpdateInternal(String name, int preferredFieldNumber, boolean isIndexed,
         boolean storeTermVector,
-        boolean omitNorms, boolean storePayloads, IndexOptions indexOptions, DocValues.Type docValues, DocValues.Type normType) {
+        boolean omitNorms, boolean storePayloads, IndexOptions indexOptions, DocValuesType docValues, DocValuesType normType) {
       FieldInfo fi = fieldInfo(name);
       if (fi == null) {
-        // get a global number for this field
-        final int fieldNumber = globalFieldNumbers.addOrGet(name, preferredFieldNumber);
-        fi = addInternal(name, fieldNumber, isIndexed, storeTermVector, omitNorms, storePayloads, indexOptions, docValues, normType);
+        // This field wasn't yet added to this in-RAM
+        // segment's FieldInfo, so now we get a global
+        // number for this field.  If the field was seen
+        // before then we'll get the same name and number,
+        // else we'll allocate a new one:
+        final int fieldNumber = globalFieldNumbers.addOrGet(name, preferredFieldNumber, docValues);
+        fi = new FieldInfo(name, isIndexed, fieldNumber, storeTermVector, omitNorms, storePayloads, indexOptions, docValues, normType, null);
+        assert !byName.containsKey(fi.name);
+        assert globalFieldNumbers.containsConsistent(Integer.valueOf(fi.number), fi.name, fi.getDocValuesType());
+        byName.put(fi.name, fi);
       } else {
         fi.update(isIndexed, storeTermVector, omitNorms, storePayloads, indexOptions);
+
         if (docValues != null) {
           fi.setDocValuesType(docValues);
         }
+
         if (!fi.omitsNorms() && normType != null) {
           fi.setNormValueType(normType);
         }
@@ -313,15 +304,6 @@
                  fi.getIndexOptions(), fi.getDocValuesType(), fi.getNormType());
     }
     
-    private FieldInfo addInternal(String name, int fieldNumber, boolean isIndexed,
-                                  boolean storeTermVector, boolean omitNorms, boolean storePayloads,
-                                  IndexOptions indexOptions, DocValues.Type docValuesType, DocValues.Type normType) {
-      globalFieldNumbers.setIfNotSet(fieldNumber, name);
-      final FieldInfo fi = new FieldInfo(name, isIndexed, fieldNumber, storeTermVector, omitNorms, storePayloads, indexOptions, docValuesType, normType, null);
-      putInternal(fi);
-      return fi;
-    }
-
     public FieldInfo fieldInfo(String fieldName) {
       return byName.get(fieldName);
     }
Index: lucene/core/src/java/org/apache/lucene/index/InvertedDocConsumer.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/index/InvertedDocConsumer.java	(revision 1442822)
+++ lucene/core/src/java/org/apache/lucene/index/InvertedDocConsumer.java	(working copy)
@@ -33,8 +33,4 @@
   abstract void startDocument() throws IOException;
 
   abstract void finishDocument() throws IOException;
-
-  /** Attempt to free RAM, returning true if any RAM was
-   *  freed */
-  abstract boolean freeRAM();
-  }
+}
Index: lucene/core/src/java/org/apache/lucene/index/NormsConsumerPerField.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/index/NormsConsumerPerField.java	(revision 1442822)
+++ lucene/core/src/java/org/apache/lucene/index/NormsConsumerPerField.java	(working copy)
@@ -18,7 +18,6 @@
 import java.io.IOException;
 
 import org.apache.lucene.codecs.DocValuesConsumer;
-import org.apache.lucene.index.DocValues.Type;
 import org.apache.lucene.search.similarities.Similarity;
 
 final class NormsConsumerPerField extends InvertedDocEndConsumerPerField implements Comparable<NormsConsumerPerField> {
@@ -26,18 +25,13 @@
   private final DocumentsWriterPerThread.DocState docState;
   private final Similarity similarity;
   private final FieldInvertState fieldState;
-  private DocValuesConsumer consumer;
-  private final Norm norm;
-  private final NormsConsumer parent;
-  private Type initType;
+  private NumericDocValuesWriter consumer;
   
   public NormsConsumerPerField(final DocInverterPerField docInverterPerField, final FieldInfo fieldInfo, NormsConsumer parent) {
     this.fieldInfo = fieldInfo;
-    this.parent = parent;
     docState = docInverterPerField.docState;
     fieldState = docInverterPerField.fieldState;
     similarity = docState.similarity;
-    norm = new Norm();
   }
 
   @Override
@@ -48,45 +42,31 @@
   @Override
   void finish() throws IOException {
     if (fieldInfo.isIndexed() && !fieldInfo.omitsNorms()) {
-      similarity.computeNorm(fieldState, norm);
-      
-      if (norm.type() != null) {
-        StorableField field = norm.field();
-        // some similarity might not compute any norms
-        DocValuesConsumer consumer = getConsumer(norm.type());
-        consumer.add(docState.docID, field);
+      if (consumer == null) {
+        fieldInfo.setNormValueType(FieldInfo.DocValuesType.NUMERIC);
+        consumer = new NumericDocValuesWriter(fieldInfo, docState.docWriter.bytesUsed);
       }
-    }    
-  }
-  
-  Type flush(int docCount) throws IOException {
-    if (!initialized()) {
-      return null; // null type - not omitted but not written
+      consumer.addValue(docState.docID, similarity.computeNorm(fieldState));
     }
-    consumer.finish(docCount);
-    return initType;
   }
   
-  private DocValuesConsumer getConsumer(Type type) throws IOException {
+  void flush(SegmentWriteState state, DocValuesConsumer normsWriter) throws IOException {
+    int docCount = state.segmentInfo.getDocCount();
     if (consumer == null) {
-      assert fieldInfo.getNormType() == null || fieldInfo.getNormType() == type;
-      fieldInfo.setNormValueType(type);
-      consumer = parent.newConsumer(docState.docWriter.newPerDocWriteState(""), fieldInfo, type);
-      this.initType = type;
+      return; // null type - not omitted but not written -
+              // meaning the only docs that had
+              // norms hit exceptions (but indexed=true is set...)
     }
-    if (initType != type) {
-      throw new IllegalArgumentException("NormTypes for field: " + fieldInfo.name + " doesn't match " + initType + " != " + type);
-    }
-    return consumer;
+    consumer.finish(docCount);
+    consumer.flush(state, normsWriter);
   }
   
-  boolean initialized() {
-    return consumer != null;
+  boolean isEmpty() {
+    return consumer == null;
   }
 
   @Override
   void abort() {
     //
   }
-
 }
Index: lucene/core/src/java/org/apache/lucene/index/IndexWriterConfig.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/index/IndexWriterConfig.java	(revision 1442822)
+++ lucene/core/src/java/org/apache/lucene/index/IndexWriterConfig.java	(working copy)
@@ -288,6 +288,9 @@
    * Only takes effect when IndexWriter is first created.
    */
   public IndexWriterConfig setCodec(Codec codec) {
+    if (codec == null) {
+      throw new NullPointerException();
+    }
     this.codec = codec;
     return this;
   }
Index: lucene/core/src/java/org/apache/lucene/index/AtomicReader.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/index/AtomicReader.java	(revision 1442822)
+++ lucene/core/src/java/org/apache/lucene/index/AtomicReader.java	(working copy)
@@ -156,20 +156,31 @@
     }
     return null;
   }
-  
-  /**
-   * Returns {@link DocValues} for this field.
-   * This method may return null if the reader has no per-document
-   * values stored.
-   */
-  public abstract DocValues docValues(String field) throws IOException;
-  
-  /**
-   * Returns {@link DocValues} for this field's normalization values.
-   * This method may return null if the field has no norms.
-   */
-  public abstract DocValues normValues(String field) throws IOException;
 
+  /** Returns {@link NumericDocValues} for this field, or
+   *  null if no {@link NumericDocValues} were indexed for
+   *  this field.  The returned instance should only be
+   *  used by a single thread. */
+  public abstract NumericDocValues getNumericDocValues(String field) throws IOException;
+
+  /** Returns {@link BinaryDocValues} for this field, or
+   *  null if no {@link BinaryDocValues} were indexed for
+   *  this field.  The returned instance should only be
+   *  used by a single thread. */
+  public abstract BinaryDocValues getBinaryDocValues(String field) throws IOException;
+
+  /** Returns {@link SortedDocValues} for this field, or
+   *  null if no {@link SortedDocValues} were indexed for
+   *  this field.  The returned instance should only be
+   *  used by a single thread. */
+  public abstract SortedDocValues getSortedDocValues(String field) throws IOException;
+
+  /** Returns {@link NumericDocValues} representing norms
+   *  for this field, or null if no {@link NumericDocValues}
+   *  were indexed. The returned instance should only be
+   *  used by a single thread. */
+  public abstract NumericDocValues getNormValues(String field) throws IOException;
+
   /**
    * Get the {@link FieldInfos} describing all fields in
    * this reader.
Index: lucene/core/src/java/org/apache/lucene/index/BinaryDocValuesWriter.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/index/BinaryDocValuesWriter.java	(revision 0)
+++ lucene/core/src/java/org/apache/lucene/index/BinaryDocValuesWriter.java	(working copy)
@@ -0,0 +1,133 @@
+package org.apache.lucene.index;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Iterator;
+import java.util.NoSuchElementException;
+
+import org.apache.lucene.codecs.DocValuesConsumer;
+import org.apache.lucene.util.ByteBlockPool.DirectTrackingAllocator;
+import org.apache.lucene.util.ByteBlockPool;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.Counter;
+import org.apache.lucene.util.packed.AppendingLongBuffer;
+
+import static org.apache.lucene.util.ByteBlockPool.BYTE_BLOCK_SIZE;
+
+
+/** Buffers up pending byte[] per doc, then flushes when
+ *  segment flushes. */
+class BinaryDocValuesWriter extends DocValuesWriter {
+
+  private final ByteBlockPool pool;
+  private final AppendingLongBuffer lengths;
+  private final FieldInfo fieldInfo;
+  private int addedValues = 0;
+
+  public BinaryDocValuesWriter(FieldInfo fieldInfo, Counter iwBytesUsed) {
+    this.fieldInfo = fieldInfo;
+    this.pool = new ByteBlockPool(new DirectTrackingAllocator(iwBytesUsed));
+    this.lengths = new AppendingLongBuffer();
+  }
+
+  public void addValue(int docID, BytesRef value) {
+    if (docID < addedValues) {
+      throw new IllegalArgumentException("DocValuesField \"" + fieldInfo.name + "\" appears more than once in this document (only one value is allowed per field)");
+    }
+    if (value == null) {
+      throw new IllegalArgumentException("field=\"" + fieldInfo.name + "\": null value not allowed");
+    }
+    if (value.length > (BYTE_BLOCK_SIZE - 2)) {
+      throw new IllegalArgumentException("DocValuesField \"" + fieldInfo.name + "\" is too large, must be <= " + (BYTE_BLOCK_SIZE - 2));
+    }
+    
+    // Fill in any holes:
+    while(addedValues < docID) {
+      addedValues++;
+      lengths.add(0);
+    }
+    addedValues++;
+    lengths.add(value.length);
+    pool.append(value);
+  }
+
+  @Override
+  public void finish(int maxDoc) {
+  }
+
+  @Override
+  public void flush(SegmentWriteState state, DocValuesConsumer dvConsumer) throws IOException {
+    final int maxDoc = state.segmentInfo.getDocCount();
+    dvConsumer.addBinaryField(fieldInfo,
+                              new Iterable<BytesRef>() {
+                                @Override
+                                public Iterator<BytesRef> iterator() {
+                                   return new BytesIterator(maxDoc);                                 
+                                }
+                              });
+  }
+
+  @Override
+  public void abort() {
+  }
+  
+  // iterates over the values we have in ram
+  private class BytesIterator implements Iterator<BytesRef> {
+    final BytesRef value = new BytesRef();
+    final AppendingLongBuffer.Iterator lengthsIterator = lengths.iterator();
+    final int size = lengths.size();
+    final int maxDoc;
+    int upto;
+    long byteOffset;
+    
+    BytesIterator(int maxDoc) {
+      this.maxDoc = maxDoc;
+    }
+    
+    @Override
+    public boolean hasNext() {
+      return upto < maxDoc;
+    }
+
+    @Override
+    public BytesRef next() {
+      if (!hasNext()) {
+        throw new NoSuchElementException();
+      }
+      if (upto < size) {
+        int length = (int) lengthsIterator.next();
+        value.grow(length);
+        value.length = length;
+        pool.readBytes(byteOffset, value.bytes, value.offset, value.length);
+        byteOffset += length;
+      } else {
+        // This is to handle last N documents not having
+        // this DV field in the end of the segment:
+        value.length = 0;
+      }
+      upto++;
+      return value;
+    }
+
+    @Override
+    public void remove() {
+      throw new UnsupportedOperationException();
+    }
+  }
+}

Property changes on: lucene/core/src/java/org/apache/lucene/index/BinaryDocValuesWriter.java
___________________________________________________________________
Added: svn:eol-style
## -0,0 +1 ##
+native
Index: lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThread.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThread.java	(revision 1442822)
+++ lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThread.java	(working copy)
@@ -63,18 +63,21 @@
       This is the current indexing chain:
 
       DocConsumer / DocConsumerPerThread
-        --> code: DocFieldProcessor / DocFieldProcessorPerThread
-          --> DocFieldConsumer / DocFieldConsumerPerThread / DocFieldConsumerPerField
-            --> code: DocFieldConsumers / DocFieldConsumersPerThread / DocFieldConsumersPerField
-              --> code: DocInverter / DocInverterPerThread / DocInverterPerField
-                --> InvertedDocConsumer / InvertedDocConsumerPerThread / InvertedDocConsumerPerField
-                  --> code: TermsHash / TermsHashPerThread / TermsHashPerField
-                    --> TermsHashConsumer / TermsHashConsumerPerThread / TermsHashConsumerPerField
-                      --> code: FreqProxTermsWriter / FreqProxTermsWriterPerThread / FreqProxTermsWriterPerField
-                      --> code: TermVectorsTermsWriter / TermVectorsTermsWriterPerThread / TermVectorsTermsWriterPerField
-                --> InvertedDocEndConsumer / InvertedDocConsumerPerThread / InvertedDocConsumerPerField
-                  --> code: NormsWriter / NormsWriterPerThread / NormsWriterPerField
-              --> code: StoredFieldsWriter / StoredFieldsWriterPerThread / StoredFieldsWriterPerField
+        --> code: DocFieldProcessor
+          --> DocFieldConsumer / DocFieldConsumerPerField
+            --> code: DocFieldConsumers / DocFieldConsumersPerField
+              --> code: DocInverter / DocInverterPerField
+                --> InvertedDocConsumer / InvertedDocConsumerPerField
+                  --> code: TermsHash / TermsHashPerField
+                    --> TermsHashConsumer / TermsHashConsumerPerField
+                      --> code: FreqProxTermsWriter / FreqProxTermsWriterPerField
+                      --> code: TermVectorsTermsWriter / TermVectorsTermsWriterPerField
+                --> InvertedDocEndConsumer / InvertedDocConsumerPerField
+                  --> code: NormsConsumer / NormsConsumerPerField
+          --> StoredFieldsConsumer
+            --> TwoStoredFieldConsumers
+              -> code: StoredFieldsProcessor
+              -> code: DocValuesProcessor
     */
 
     // Build up indexing chain:
@@ -82,11 +85,14 @@
       final TermsHashConsumer termVectorsWriter = new TermVectorsConsumer(documentsWriterPerThread);
       final TermsHashConsumer freqProxWriter = new FreqProxTermsWriter();
 
-      final InvertedDocConsumer  termsHash = new TermsHash(documentsWriterPerThread, freqProxWriter, true,
-                                                           new TermsHash(documentsWriterPerThread, termVectorsWriter, false, null));
-      final NormsConsumer normsWriter = new NormsConsumer(documentsWriterPerThread);
+      final InvertedDocConsumer termsHash = new TermsHash(documentsWriterPerThread, freqProxWriter, true,
+                                                          new TermsHash(documentsWriterPerThread, termVectorsWriter, false, null));
+      final NormsConsumer normsWriter = new NormsConsumer();
       final DocInverter docInverter = new DocInverter(documentsWriterPerThread.docState, termsHash, normsWriter);
-      return new DocFieldProcessor(documentsWriterPerThread, docInverter);
+      final StoredFieldsConsumer storedFields = new TwoStoredFieldsConsumers(
+                                                      new StoredFieldsProcessor(documentsWriterPerThread),
+                                                      new DocValuesProcessor(documentsWriterPerThread.bytesUsed));
+      return new DocFieldProcessor(documentsWriterPerThread, docInverter, storedFields);
     }
   };
 
@@ -651,10 +657,6 @@
     }
     
   }
-  PerDocWriteState newPerDocWriteState(String segmentSuffix) {
-    assert segmentInfo != null;
-    return new PerDocWriteState(infoStream, directory, segmentInfo, bytesUsed, segmentSuffix, IOContext.DEFAULT);
-  }
   
   @Override
   public String toString() {
Index: lucene/core/src/java/org/apache/lucene/index/StorableFieldType.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/index/StorableFieldType.java	(revision 1442822)
+++ lucene/core/src/java/org/apache/lucene/index/StorableFieldType.java	(working copy)
@@ -1,5 +1,7 @@
 package org.apache.lucene.index;
 
+import org.apache.lucene.index.FieldInfo.DocValuesType;
+
 /*
  * Licensed to the Apache Software Foundation (ASF) under one or more
  * contributor license agreements.  See the NOTICE file distributed with
@@ -27,5 +29,5 @@
 
   /** DocValues type; if non-null then the field's value
    *  will be indexed into docValues */
-  public DocValues.Type docValueType();
+  public DocValuesType docValueType();
 }
Index: lucene/core/src/java/org/apache/lucene/index/DocFieldConsumer.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/index/DocFieldConsumer.java	(revision 1442822)
+++ lucene/core/src/java/org/apache/lucene/index/DocFieldConsumer.java	(working copy)
@@ -28,15 +28,9 @@
   /** Called when an aborting exception is hit */
   abstract void abort();
 
-  /** Called when DocumentsWriterPerThread is using too much RAM.
-   *  The consumer should free RAM, if possible, returning
-   *  true if any RAM was in fact freed. */
-  abstract boolean freeRAM();
-
   abstract void startDocument() throws IOException;
 
   abstract DocFieldConsumerPerField addField(FieldInfo fi);
 
   abstract void finishDocument() throws IOException;
-
 }
Index: lucene/core/src/java/org/apache/lucene/index/ParallelAtomicReader.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/index/ParallelAtomicReader.java	(revision 1442822)
+++ lucene/core/src/java/org/apache/lucene/index/ParallelAtomicReader.java	(working copy)
@@ -262,18 +262,33 @@
     // throw the first exception
     if (ioe != null) throw ioe;
   }
+
+  @Override
+  public NumericDocValues getNumericDocValues(String field) throws IOException {
+    ensureOpen();
+    AtomicReader reader = fieldToReader.get(field);
+    return reader == null ? null : reader.getNumericDocValues(field);
+  }
   
   @Override
-  public DocValues docValues(String field) throws IOException {
+  public BinaryDocValues getBinaryDocValues(String field) throws IOException {
     ensureOpen();
     AtomicReader reader = fieldToReader.get(field);
-    return reader == null ? null : reader.docValues(field);
+    return reader == null ? null : reader.getBinaryDocValues(field);
   }
   
   @Override
-  public DocValues normValues(String field) throws IOException {
+  public SortedDocValues getSortedDocValues(String field) throws IOException {
     ensureOpen();
     AtomicReader reader = fieldToReader.get(field);
-    return reader == null ? null : reader.normValues(field);
+    return reader == null ? null : reader.getSortedDocValues(field);
   }
+
+  @Override
+  public NumericDocValues getNormValues(String field) throws IOException {
+    ensureOpen();
+    AtomicReader reader = fieldToReader.get(field);
+    NumericDocValues values = reader == null ? null : reader.getNormValues(field);
+    return values;
+  }
 }
Index: lucene/core/src/java/org/apache/lucene/index/SegmentReadState.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/index/SegmentReadState.java	(revision 1442822)
+++ lucene/core/src/java/org/apache/lucene/index/SegmentReadState.java	(working copy)
@@ -28,7 +28,7 @@
  */
 public class SegmentReadState {
   /** {@link Directory} where this segment is read from. */ 
-  public final Directory dir;
+  public final Directory directory;
 
   /** {@link SegmentInfo} describing this segment. */
   public final SegmentInfo segmentInfo;
@@ -73,7 +73,7 @@
                           IOContext context,
                           int termsIndexDivisor,
                           String segmentSuffix) {
-    this.dir = dir;
+    this.directory = dir;
     this.segmentInfo = info;
     this.fieldInfos = fieldInfos;
     this.context = context;
@@ -84,7 +84,7 @@
   /** Create a {@code SegmentReadState}. */
   public SegmentReadState(SegmentReadState other,
                           String newSegmentSuffix) {
-    this.dir = other.dir;
+    this.directory = other.directory;
     this.segmentInfo = other.segmentInfo;
     this.fieldInfos = other.fieldInfos;
     this.context = other.context;
Index: lucene/core/src/java/org/apache/lucene/index/SortedDocValuesWriter.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/index/SortedDocValuesWriter.java	(revision 0)
+++ lucene/core/src/java/org/apache/lucene/index/SortedDocValuesWriter.java	(working copy)
@@ -0,0 +1,214 @@
+package org.apache.lucene.index;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import static org.apache.lucene.util.ByteBlockPool.BYTE_BLOCK_SIZE;
+
+import java.io.IOException;
+import java.util.Iterator;
+import java.util.NoSuchElementException;
+
+import org.apache.lucene.codecs.DocValuesConsumer;
+import org.apache.lucene.util.ArrayUtil;
+import org.apache.lucene.util.ByteBlockPool;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.BytesRefHash.DirectBytesStartArray;
+import org.apache.lucene.util.BytesRefHash;
+import org.apache.lucene.util.Counter;
+import org.apache.lucene.util.RamUsageEstimator;
+import org.apache.lucene.util.packed.AppendingLongBuffer;
+
+
+/** Buffers up pending byte[] per doc, deref and sorting via
+ *  int ord, then flushes when segment flushes. */
+class SortedDocValuesWriter extends DocValuesWriter {
+  final BytesRefHash hash;
+  private AppendingLongBuffer pending;
+  private final Counter iwBytesUsed;
+  private long bytesUsed; // this currently only tracks differences in 'pending'
+  private final FieldInfo fieldInfo;
+
+  private static final BytesRef EMPTY = new BytesRef(BytesRef.EMPTY_BYTES);
+
+  public SortedDocValuesWriter(FieldInfo fieldInfo, Counter iwBytesUsed) {
+    this.fieldInfo = fieldInfo;
+    this.iwBytesUsed = iwBytesUsed;
+    hash = new BytesRefHash(
+        new ByteBlockPool(
+            new ByteBlockPool.DirectTrackingAllocator(iwBytesUsed)),
+            BytesRefHash.DEFAULT_CAPACITY,
+            new DirectBytesStartArray(BytesRefHash.DEFAULT_CAPACITY, iwBytesUsed));
+    pending = new AppendingLongBuffer();
+    bytesUsed = pending.ramBytesUsed();
+    iwBytesUsed.addAndGet(bytesUsed);
+  }
+
+  public void addValue(int docID, BytesRef value) {
+    if (docID < pending.size()) {
+      throw new IllegalArgumentException("DocValuesField \"" + fieldInfo.name + "\" appears more than once in this document (only one value is allowed per field)");
+    }
+    if (value == null) {
+      throw new IllegalArgumentException("field \"" + fieldInfo.name + "\": null value not allowed");
+    }
+    if (value.length > (BYTE_BLOCK_SIZE - 2)) {
+      throw new IllegalArgumentException("DocValuesField \"" + fieldInfo.name + "\" is too large, must be <= " + (BYTE_BLOCK_SIZE - 2));
+    }
+
+    // Fill in any holes:
+    while(pending.size() < docID) {
+      addOneValue(EMPTY);
+    }
+
+    addOneValue(value);
+  }
+
+  @Override
+  public void finish(int maxDoc) {
+    while(pending.size() < maxDoc) {
+      addOneValue(EMPTY);
+    }
+  }
+
+  private void addOneValue(BytesRef value) {
+    int ord = hash.add(value);
+    if (ord < 0) {
+      ord = -ord-1;
+    } else {
+      // reserve additional space for each unique value:
+      // 1. when indexing, when hash is 50% full, rehash() suddenly needs 2*size ints.
+      //    TODO: can this same OOM happen in THPF?
+      // 2. when flushing, we need 1 int per value (slot in the ordMap).
+      iwBytesUsed.addAndGet(2 * RamUsageEstimator.NUM_BYTES_INT);
+    }
+    
+    pending.add(ord);
+    updateBytesUsed();
+  }
+  
+  private void updateBytesUsed() {
+    final long newBytesUsed = pending.ramBytesUsed();
+    iwBytesUsed.addAndGet(newBytesUsed - bytesUsed);
+    bytesUsed = newBytesUsed;
+  }
+
+  @Override
+  public void flush(SegmentWriteState state, DocValuesConsumer dvConsumer) throws IOException {
+    final int maxDoc = state.segmentInfo.getDocCount();
+
+    final int emptyOrd;
+    assert pending.size() == maxDoc;
+    final int valueCount = hash.size();
+
+    final int[] sortedValues = hash.sort(BytesRef.getUTF8SortedAsUnicodeComparator());
+    final int[] ordMap = new int[valueCount];
+
+    for(int ord=0;ord<valueCount;ord++) {
+      ordMap[sortedValues[ord]] = ord;
+    }
+
+    dvConsumer.addSortedField(fieldInfo,
+
+                              // ord -> value
+                              new Iterable<BytesRef>() {
+                                @Override
+                                public Iterator<BytesRef> iterator() {
+                                  return new ValuesIterator(sortedValues, valueCount);
+                                }
+                              },
+
+                              // doc -> ord
+                              new Iterable<Number>() {
+                                @Override
+                                public Iterator<Number> iterator() {
+                                  return new OrdsIterator(ordMap, maxDoc);
+                                }
+                              });
+  }
+
+  @Override
+  public void abort() {
+  }
+  
+  // iterates over the unique values we have in ram
+  private class ValuesIterator implements Iterator<BytesRef> {
+    final int sortedValues[];
+    final BytesRef scratch = new BytesRef();
+    final int valueCount;
+    int ordUpto;
+    
+    ValuesIterator(int sortedValues[], int valueCount) {
+      this.sortedValues = sortedValues;
+      this.valueCount = valueCount;
+    }
+
+    @Override
+    public boolean hasNext() {
+      return ordUpto < valueCount;
+    }
+
+    @Override
+    public BytesRef next() {
+      if (!hasNext()) {
+        throw new NoSuchElementException();
+      }
+      hash.get(sortedValues[ordUpto], scratch);
+      ordUpto++;
+      return scratch;
+    }
+
+    @Override
+    public void remove() {
+      throw new UnsupportedOperationException();
+    }
+  }
+  
+  // iterates over the ords for each doc we have in ram
+  private class OrdsIterator implements Iterator<Number> {
+    final AppendingLongBuffer.Iterator iter = pending.iterator();
+    final int ordMap[];
+    final int maxDoc;
+    int docUpto;
+    
+    OrdsIterator(int ordMap[], int maxDoc) {
+      this.ordMap = ordMap;
+      this.maxDoc = maxDoc;
+      assert pending.size() == maxDoc;
+    }
+    
+    @Override
+    public boolean hasNext() {
+      return docUpto < maxDoc;
+    }
+
+    @Override
+    public Number next() {
+      if (!hasNext()) {
+        throw new NoSuchElementException();
+      }
+      int ord = (int) iter.next();
+      docUpto++;
+      // TODO: make reusable Number
+      return ordMap[ord];
+    }
+
+    @Override
+    public void remove() {
+      throw new UnsupportedOperationException();
+    }
+  }
+}

Property changes on: lucene/core/src/java/org/apache/lucene/index/SortedDocValuesWriter.java
___________________________________________________________________
Added: svn:eol-style
## -0,0 +1 ##
+native
Index: lucene/core/src/java/org/apache/lucene/index/LiveIndexWriterConfig.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/index/LiveIndexWriterConfig.java	(revision 1442822)
+++ lucene/core/src/java/org/apache/lucene/index/LiveIndexWriterConfig.java	(working copy)
@@ -116,6 +116,9 @@
     writeLockTimeout = IndexWriterConfig.WRITE_LOCK_TIMEOUT;
     indexingChain = DocumentsWriterPerThread.defaultIndexingChain;
     codec = Codec.getDefault();
+    if (codec == null) {
+      throw new NullPointerException();
+    }
     infoStream = InfoStream.getDefault();
     mergePolicy = new TieredMergePolicy();
     flushPolicy = new FlushByRamOrCountsPolicy();
Index: lucene/core/src/java/org/apache/lucene/index/Norm.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/index/Norm.java	(revision 1442822)
+++ lucene/core/src/java/org/apache/lucene/index/Norm.java	(working copy)
@@ -1,189 +0,0 @@
-package org.apache.lucene.index;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-import org.apache.lucene.document.ByteDocValuesField;
-import org.apache.lucene.document.DerefBytesDocValuesField;
-import org.apache.lucene.document.DoubleDocValuesField;
-import org.apache.lucene.document.Field;
-import org.apache.lucene.document.FloatDocValuesField;
-import org.apache.lucene.document.IntDocValuesField;
-import org.apache.lucene.document.LongDocValuesField;
-import org.apache.lucene.document.PackedLongDocValuesField;
-import org.apache.lucene.document.ShortDocValuesField;
-import org.apache.lucene.document.SortedBytesDocValuesField;
-import org.apache.lucene.document.StoredField;
-import org.apache.lucene.document.StraightBytesDocValuesField;
-import org.apache.lucene.index.DocValues.Type;
-import org.apache.lucene.search.similarities.Similarity;
-import org.apache.lucene.util.BytesRef;
-
-/**
- * Stores the normalization value with {@link StorableField} computed in
- * {@link Similarity#computeNorm(FieldInvertState, Norm)} per field.
- * Normalization values must be consistent within a single field, different
- * value types are not permitted within a single field. All values set must be
- * fixed size values ie. all values passed to {@link Norm#setBytes(BytesRef)}
- * must have the same length per field.
- * 
- * @lucene.experimental
- * @lucene.internal
- */
-public final class Norm  {
-  private StoredField field;
-  private BytesRef spare;
-  
-  /** Sole constructor. */
-  public Norm() {
-  }
-
-  /**
-   * Returns the {@link StorableField} representation for this norm
-   */
-  public StorableField field() {
-    return field;
-  }
-  
-  /**
-   * Returns the {@link Type} for this norm.
-   */
-  public Type type() {
-    return field == null? null : field.fieldType().docValueType();
-  }
-  
-  /**
-   * Returns a spare {@link BytesRef} 
-   */
-  public BytesRef getSpare() {
-    if (spare == null) {
-      spare = new BytesRef();
-    }
-    return spare;
-  }
-
-  /**
-   * Sets a float norm value
-   */
-  public void setFloat(float norm) {
-    setType(Type.FLOAT_32);
-    this.field.setFloatValue(norm);
-  }
-
-  /**
-   * Sets a double norm value
-   */
-  public void setDouble(double norm) {
-    setType(Type.FLOAT_64);
-    this.field.setDoubleValue(norm);
-  }
-
-  /**
-   * Sets a short norm value
-   */
-  public void setShort(short norm) {
-    setType(Type.FIXED_INTS_16);
-    this.field.setShortValue(norm);
-    
-  }
-
-  /**
-   * Sets a int norm value
-   */
-  public void setInt(int norm) {
-    setType(Type.FIXED_INTS_32);
-    this.field.setIntValue(norm);
-  }
-
-  /**
-   * Sets a long norm value
-   */
-  public void setLong(long norm) {
-    setType(Type.FIXED_INTS_64);
-    this.field.setLongValue(norm);
-  }
-
-  /**
-   * Sets a byte norm value
-   */
-  public void setByte(byte norm) {
-    setType(Type.FIXED_INTS_8);
-    this.field.setByteValue(norm);
-  }
-
-  /**
-   * Sets a fixed byte array norm value
-   */
-  public void setBytes(BytesRef norm) {
-    setType(Type.BYTES_FIXED_STRAIGHT);
-    this.field.setBytesValue(norm);
-  }
-
-  
-  private void setType(Type type) {
-    if (field != null) {
-      if (type != field.fieldType().docValueType()) {
-        throw new IllegalArgumentException("FieldType missmatch - expected "+type+" but was " + field.fieldType().docValueType());
-      }
-    } else {
-
-      switch(type) {
-      case VAR_INTS:
-        field = new PackedLongDocValuesField("", (long) 0);
-        break;
-      case FIXED_INTS_8:
-        field = new ByteDocValuesField("", (byte) 0);
-        break;
-      case FIXED_INTS_16:
-        field = new ShortDocValuesField("", (short) 0);
-        break;
-      case FIXED_INTS_32:
-        field = new IntDocValuesField("", 0);
-        break;
-      case FIXED_INTS_64:
-        field = new LongDocValuesField("", (byte) 0);
-        break;
-      case FLOAT_32:
-        field = new FloatDocValuesField("", 0f);
-        break;
-      case FLOAT_64:
-        field = new DoubleDocValuesField("", 0d);
-        break;
-      case BYTES_FIXED_STRAIGHT:
-        field = new StraightBytesDocValuesField("", new BytesRef(), true);
-        break;
-      case BYTES_VAR_STRAIGHT:
-        field = new StraightBytesDocValuesField("", new BytesRef(), false);
-        break;
-      case BYTES_FIXED_DEREF:
-        field = new DerefBytesDocValuesField("", new BytesRef(), true);
-        break;
-      case BYTES_VAR_DEREF:
-        field = new DerefBytesDocValuesField("", new BytesRef(), false);
-        break;
-      case BYTES_FIXED_SORTED:
-        field = new SortedBytesDocValuesField("", new BytesRef(), true);
-        break;
-      case BYTES_VAR_SORTED:
-        field = new SortedBytesDocValuesField("", new BytesRef(), false);
-        break;
-      default:
-        throw new IllegalArgumentException("unknown Type: " + type);
-      }
-    }
-  }
-
-}
\ No newline at end of file
Index: lucene/core/src/java/org/apache/lucene/index/DocValuesWriter.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/index/DocValuesWriter.java	(revision 0)
+++ lucene/core/src/java/org/apache/lucene/index/DocValuesWriter.java	(working copy)
@@ -0,0 +1,28 @@
+package org.apache.lucene.index;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.codecs.DocValuesConsumer;
+
+abstract class DocValuesWriter {
+  abstract void abort() throws IOException;
+  abstract void finish(int numDoc);
+  abstract void flush(SegmentWriteState state, DocValuesConsumer consumer) throws IOException;
+}

Property changes on: lucene/core/src/java/org/apache/lucene/index/DocValuesWriter.java
___________________________________________________________________
Added: svn:eol-style
## -0,0 +1 ##
+native
Index: lucene/core/src/java/org/apache/lucene/index/PerDocWriteState.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/index/PerDocWriteState.java	(revision 1442822)
+++ lucene/core/src/java/org/apache/lucene/index/PerDocWriteState.java	(working copy)
@@ -1,86 +0,0 @@
-package org.apache.lucene.index;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.codecs.PerDocConsumer;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.util.Counter;
-import org.apache.lucene.util.InfoStream;
-
-/**
- * Encapsulates all necessary state to initiate a {@link PerDocConsumer} and
- * create all necessary files in order to consume and merge per-document values.
- * 
- * @lucene.experimental
- */
-public class PerDocWriteState {
-  /** InfoStream used for debugging. */
-  public final InfoStream infoStream;
-
-  /** {@link Directory} to write all files to. */
-  public final Directory directory;
-
-  /** {@link SegmentInfo} describing this segment. */
-  public final SegmentInfo segmentInfo;
-
-  /** Number of bytes allocated in RAM to hold this state. */
-  public final Counter bytesUsed;
-
-  /** Segment suffix to pass to {@link
-   * IndexFileNames#segmentFileName(String,String,String)}. */
-  public final String segmentSuffix;
-
-  /** {@link IOContext} to use for all file writing. */
-  public final IOContext context;
-
-  /** Creates a {@code PerDocWriteState}. */
-  public PerDocWriteState(InfoStream infoStream, Directory directory,
-      SegmentInfo segmentInfo, Counter bytesUsed,
-      String segmentSuffix, IOContext context) {
-    this.infoStream = infoStream;
-    this.directory = directory;
-    this.segmentInfo = segmentInfo;
-    this.segmentSuffix = segmentSuffix;
-    this.bytesUsed = bytesUsed;
-    this.context = context;
-  }
-
-  /** Creates a {@code PerDocWriteState}, copying fields
-   *  from another and allocating a new {@link #bytesUsed}. */
-  public PerDocWriteState(SegmentWriteState state) {
-    infoStream = state.infoStream;
-    directory = state.directory;
-    segmentInfo = state.segmentInfo;
-    segmentSuffix = state.segmentSuffix;
-    bytesUsed = Counter.newCounter();
-    context = state.context;
-  }
-
-  /** Creates a {@code PerDocWriteState}, copying fields
-   *  from another (copy constructor) but setting a new
-   *  {@link #segmentSuffix}. */
-  public PerDocWriteState(PerDocWriteState state, String segmentSuffix) {
-    this.infoStream = state.infoStream;
-    this.directory = state.directory;
-    this.segmentInfo = state.segmentInfo;
-    this.segmentSuffix = segmentSuffix;
-    this.bytesUsed = state.bytesUsed;
-    this.context = state.context;
-  }
-}
Index: lucene/core/src/java/org/apache/lucene/index/TermsHash.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/index/TermsHash.java	(revision 1442822)
+++ lucene/core/src/java/org/apache/lucene/index/TermsHash.java	(working copy)
@@ -126,11 +126,6 @@
   }
 
   @Override
-  public boolean freeRAM() {
-    return false;
-  }
-
-  @Override
   void finishDocument() throws IOException {
     consumer.finishDocument(this);
     if (nextTermsHash != null) {
Index: lucene/core/src/java/org/apache/lucene/index/NumericDocValuesWriter.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/index/NumericDocValuesWriter.java	(revision 0)
+++ lucene/core/src/java/org/apache/lucene/index/NumericDocValuesWriter.java	(working copy)
@@ -0,0 +1,127 @@
+package org.apache.lucene.index;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Iterator;
+import java.util.NoSuchElementException;
+
+import org.apache.lucene.codecs.DocValuesConsumer;
+import org.apache.lucene.util.Counter;
+import org.apache.lucene.util.packed.AppendingLongBuffer;
+
+/** Buffers up pending long per doc, then flushes when
+ *  segment flushes. */
+class NumericDocValuesWriter extends DocValuesWriter {
+
+  private final static long MISSING = 0L;
+
+  private AppendingLongBuffer pending;
+  private final Counter iwBytesUsed;
+  private long bytesUsed;
+  private final FieldInfo fieldInfo;
+
+  public NumericDocValuesWriter(FieldInfo fieldInfo, Counter iwBytesUsed) {
+    pending = new AppendingLongBuffer();
+    bytesUsed = pending.ramBytesUsed();
+    this.fieldInfo = fieldInfo;
+    this.iwBytesUsed = iwBytesUsed;
+    iwBytesUsed.addAndGet(bytesUsed);
+  }
+
+  public void addValue(int docID, long value) {
+    if (docID < pending.size()) {
+      throw new IllegalArgumentException("DocValuesField \"" + fieldInfo.name + "\" appears more than once in this document (only one value is allowed per field)");
+    }
+
+    // Fill in any holes:
+    for (int i = pending.size(); i < docID; ++i) {
+      pending.add(MISSING);
+    }
+
+    pending.add(value);
+
+    updateBytesUsed();
+  }
+
+  private void updateBytesUsed() {
+    final long newBytesUsed = pending.ramBytesUsed();
+    iwBytesUsed.addAndGet(newBytesUsed - bytesUsed);
+    bytesUsed = newBytesUsed;
+  }
+
+  @Override
+  public void finish(int maxDoc) {
+  }
+
+  @Override
+  public void flush(SegmentWriteState state, DocValuesConsumer dvConsumer) throws IOException {
+
+    final int maxDoc = state.segmentInfo.getDocCount();
+
+    dvConsumer.addNumericField(fieldInfo,
+                               new Iterable<Number>() {
+                                 @Override
+                                 public Iterator<Number> iterator() {
+                                   return new NumericIterator(maxDoc);
+                                 }
+                               });
+  }
+
+  @Override
+  public void abort() {
+  }
+  
+  // iterates over the values we have in ram
+  private class NumericIterator implements Iterator<Number> {
+    final AppendingLongBuffer.Iterator iter = pending.iterator();
+    final int size = pending.size();
+    final int maxDoc;
+    int upto;
+    
+    NumericIterator(int maxDoc) {
+      this.maxDoc = maxDoc;
+    }
+    
+    @Override
+    public boolean hasNext() {
+      return upto < maxDoc;
+    }
+
+    @Override
+    public Number next() {
+      if (!hasNext()) {
+        throw new NoSuchElementException();
+      }
+      long value;
+      if (upto < size) {
+        value = iter.next();
+      } else {
+        value = 0;
+      }
+      upto++;
+      // TODO: make reusable Number
+      return value;
+    }
+
+    @Override
+    public void remove() {
+      throw new UnsupportedOperationException();
+    }
+  }
+}

Property changes on: lucene/core/src/java/org/apache/lucene/index/NumericDocValuesWriter.java
___________________________________________________________________
Added: svn:eol-style
## -0,0 +1 ##
+native
Index: lucene/core/src/java/org/apache/lucene/index/DocFieldProcessor.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/index/DocFieldProcessor.java	(revision 1442822)
+++ lucene/core/src/java/org/apache/lucene/index/DocFieldProcessor.java	(working copy)
@@ -25,17 +25,11 @@
 import java.util.Map;
 
 import org.apache.lucene.codecs.Codec;
-import org.apache.lucene.codecs.DocValuesConsumer;
 import org.apache.lucene.codecs.FieldInfosWriter;
-import org.apache.lucene.codecs.PerDocConsumer;
-import org.apache.lucene.document.FieldType;
-import org.apache.lucene.index.DocumentsWriterPerThread.DocState;
-import org.apache.lucene.index.TypePromoter.TypeCompatibility;
 import org.apache.lucene.store.IOContext;
 import org.apache.lucene.util.ArrayUtil;
-import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util.Counter;
 
-
 /**
  * This is a DocConsumer that gathers all fields under the
  * same name, and calls per-field consumers to process field
@@ -47,7 +41,7 @@
 final class DocFieldProcessor extends DocConsumer {
 
   final DocFieldConsumer consumer;
-  final StoredFieldsConsumer fieldsWriter;
+  final StoredFieldsConsumer storedConsumer;
   final Codec codec;
 
   // Holds all fields seen in current doc
@@ -62,11 +56,14 @@
   int fieldGen;
   final DocumentsWriterPerThread.DocState docState;
 
-  public DocFieldProcessor(DocumentsWriterPerThread docWriter, DocFieldConsumer consumer) {
+  final Counter bytesUsed;
+
+  public DocFieldProcessor(DocumentsWriterPerThread docWriter, DocFieldConsumer consumer, StoredFieldsConsumer storedConsumer) {
     this.docState = docWriter.docState;
     this.codec = docWriter.codec;
+    this.bytesUsed = docWriter.bytesUsed;
     this.consumer = consumer;
-    fieldsWriter = new StoredFieldsConsumer(docWriter);
+    this.storedConsumer = storedConsumer;
   }
 
   @Override
@@ -78,16 +75,11 @@
       childFields.put(f.getFieldInfo().name, f);
     }
 
-    fieldsWriter.flush(state);
+    assert fields.size() == totalFieldCount;
+
+    storedConsumer.flush(state);
     consumer.flush(childFields, state);
 
-    for (DocValuesConsumerHolder consumer : docValues.values()) {
-      consumer.docValuesConsumer.finish(state.segmentInfo.getDocCount());
-    }
-    
-    // close perDocConsumer during flush to ensure all files are flushed due to PerCodec CFS
-    IOUtils.close(perDocConsumer);
-
     // Important to save after asking consumer to flush so
     // consumer can alter the FieldInfo* if necessary.  EG,
     // FreqProxTermsWriter does this with
@@ -113,11 +105,9 @@
         field = next;
       }
     }
-    IOUtils.closeWhileHandlingException(perDocConsumer);
-    // TODO add abort to PerDocConsumer!
     
     try {
-      fieldsWriter.abort();
+      storedConsumer.abort();
     } catch (Throwable t) {
       if (th == null) {
         th = t;
@@ -132,16 +122,6 @@
       }
     }
     
-    try {
-      if (perDocConsumer != null) {
-        perDocConsumer.abort();  
-      }
-    } catch (Throwable t) {
-      if (th == null) {
-        th = t;
-      }
-    }
-    
     // If any errors occured, throw it.
     if (th != null) {
       if (th instanceof RuntimeException) throw (RuntimeException) th;
@@ -151,11 +131,6 @@
     }
   }
 
-  @Override
-  public boolean freeRAM() {
-    return consumer.freeRAM();
-  }
-
   public Collection<DocFieldConsumerPerField> fields() {
     Collection<DocFieldConsumerPerField> fields = new HashSet<DocFieldConsumerPerField>();
     for(int i=0;i<fieldHash.length;i++) {
@@ -176,8 +151,6 @@
     fieldHash = new DocFieldProcessorPerField[2];
     hashMask = 1;
     totalFieldCount = 0;
-    perDocConsumer = null;
-    docValues.clear();
   }
 
   private void rehash() {
@@ -207,7 +180,7 @@
   public void processDocument(FieldInfos.Builder fieldInfos) throws IOException {
 
     consumer.startDocument();
-    fieldsWriter.startDocument();
+    storedConsumer.startDocument();
 
     fieldCount = 0;
 
@@ -226,38 +199,12 @@
 
       fp.addField(field);
     }
+
     for (StorableField field: docState.doc.storableFields()) {
       final String fieldName = field.name();
       IndexableFieldType ft = field.fieldType();
-
-      DocFieldProcessorPerField fp = processField(fieldInfos, thisFieldGen, fieldName, ft);
-      if (ft.stored()) {
-        fieldsWriter.addField(field, fp.fieldInfo);
-      }
-      
-      final DocValues.Type dvType = ft.docValueType();
-      if (dvType != null) {
-        DocValuesConsumerHolder docValuesConsumer = docValuesConsumer(dvType,
-            docState, fp.fieldInfo);
-        DocValuesConsumer consumer = docValuesConsumer.docValuesConsumer;
-        if (docValuesConsumer.compatibility == null) {
-          consumer.add(docState.docID, field);
-          docValuesConsumer.compatibility = new TypeCompatibility(dvType,
-              consumer.getValueSize());
-        } else if (docValuesConsumer.compatibility.isCompatible(dvType,
-            TypePromoter.getValueSize(dvType, field.binaryValue()))) {
-          consumer.add(docState.docID, field);
-        } else {
-          docValuesConsumer.compatibility.isCompatible(dvType,
-              TypePromoter.getValueSize(dvType, field.binaryValue()));
-          TypeCompatibility compatibility = docValuesConsumer.compatibility;
-          throw new IllegalArgumentException("Incompatible DocValues type: "
-              + dvType.name() + " size: "
-              + TypePromoter.getValueSize(dvType, field.binaryValue())
-              + " expected: " + " type: " + compatibility.getBaseType()
-              + " size: " + compatibility.getBaseSize());
-        }
-      }
+      FieldInfo fieldInfo = fieldInfos.addOrUpdate(fieldName, ft);
+      storedConsumer.addField(docState.docID, field, fieldInfo);
     }
 
     // If we are writing vectors then we must visit
@@ -280,6 +227,7 @@
 
   private DocFieldProcessorPerField processField(FieldInfos.Builder fieldInfos,
       final int thisFieldGen, final String fieldName, IndexableFieldType ft) {
+
     // Make sure we have a PerField allocated
     final int hashPos = fieldName.hashCode() & hashMask;
     DocFieldProcessorPerField fp = fieldHash[hashPos];
@@ -305,7 +253,7 @@
         rehash();
       }
     } else {
-      fieldInfos.addOrUpdate(fp.fieldInfo.name, ft);
+      fp.fieldInfo.update(ft);
     }
 
     if (thisFieldGen != fp.lastGen) {
@@ -336,54 +284,9 @@
   @Override
   void finishDocument() throws IOException {
     try {
-      fieldsWriter.finishDocument();
+      storedConsumer.finishDocument();
     } finally {
       consumer.finishDocument();
     }
   }
-
-  private static class DocValuesConsumerHolder {
-    // Only used to enforce that same DV field name is never
-    // added more than once per doc:
-    int docID;
-    final DocValuesConsumer docValuesConsumer;
-    TypeCompatibility compatibility;
-
-    public DocValuesConsumerHolder(DocValuesConsumer docValuesConsumer) {
-      this.docValuesConsumer = docValuesConsumer;
-    }
-  }
-
-  final private Map<String, DocValuesConsumerHolder> docValues = new HashMap<String, DocValuesConsumerHolder>();
-  private PerDocConsumer perDocConsumer;
-
-  DocValuesConsumerHolder docValuesConsumer(DocValues.Type valueType, DocState docState, FieldInfo fieldInfo) 
-      throws IOException {
-    DocValuesConsumerHolder docValuesConsumerAndDocID = docValues.get(fieldInfo.name);
-    if (docValuesConsumerAndDocID != null) {
-      if (docState.docID == docValuesConsumerAndDocID.docID) {
-        throw new IllegalArgumentException("DocValuesField \"" + fieldInfo.name + "\" appears more than once in this document (only one value is allowed, per field)");
-      }
-      assert docValuesConsumerAndDocID.docID < docState.docID;
-      docValuesConsumerAndDocID.docID = docState.docID;
-      return docValuesConsumerAndDocID;
-    }
-
-    if (perDocConsumer == null) {
-      PerDocWriteState perDocWriteState = docState.docWriter.newPerDocWriteState("");
-      perDocConsumer = docState.docWriter.codec.docValuesFormat().docsConsumer(perDocWriteState);
-      if (perDocConsumer == null) {
-        throw new IllegalStateException("codec=" +  docState.docWriter.codec + " does not support docValues: from docValuesFormat().docsConsumer(...) returned null; field=" + fieldInfo.name);
-      }
-    }
-    DocValuesConsumer docValuesConsumer = perDocConsumer.addValuesField(valueType, fieldInfo);
-    assert fieldInfo.getDocValuesType() == null || fieldInfo.getDocValuesType() == valueType;
-    fieldInfo.setDocValuesType(valueType);
-
-    docValuesConsumerAndDocID = new DocValuesConsumerHolder(docValuesConsumer);
-    docValuesConsumerAndDocID.docID = docState.docID;
-    docValues.put(fieldInfo.name, docValuesConsumerAndDocID);
-    return docValuesConsumerAndDocID;
-  }
-  
 }
Index: lucene/core/src/java/org/apache/lucene/index/DocConsumer.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/index/DocConsumer.java	(revision 1442822)
+++ lucene/core/src/java/org/apache/lucene/index/DocConsumer.java	(working copy)
@@ -24,6 +24,5 @@
   abstract void finishDocument() throws IOException;
   abstract void flush(final SegmentWriteState state) throws IOException;
   abstract void abort();
-  abstract boolean freeRAM();
   abstract void doAfterFlush();
 }
Index: lucene/core/src/java/org/apache/lucene/index/FieldInfo.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/index/FieldInfo.java	(revision 1442822)
+++ lucene/core/src/java/org/apache/lucene/index/FieldInfo.java	(working copy)
@@ -20,8 +20,6 @@
 import java.util.HashMap;
 import java.util.Map;
 
-import org.apache.lucene.index.DocValues.Type;
-
 /**
  *  Access to the Field Info file that describes document fields and whether or
  *  not they are indexed. Each segment has a separate Field Info file. Objects
@@ -37,12 +35,12 @@
   public final int number;
 
   private boolean indexed;
-  private DocValues.Type docValueType;
+  private DocValuesType docValueType;
 
   // True if any document indexed term vectors
   private boolean storeTermVector;
 
-  private DocValues.Type normType;
+  private DocValuesType normType;
   private boolean omitNorms; // omit norms associated with indexed fields  
   private IndexOptions indexOptions;
   private boolean storePayloads; // whether this field stores payloads together with term positions
@@ -82,6 +80,29 @@
      */
     DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS,
   };
+  
+  /**
+   * DocValues types.
+   * Note that DocValues is strongly typed, so a field cannot have different types
+   * across different documents.
+   */
+  public static enum DocValuesType {
+    /** 
+     * A per-document Number
+     */
+    NUMERIC,
+    /**
+     * A per-document byte[].
+     */
+    BINARY,
+    /** 
+     * A pre-sorted byte[]. Fields with this type only store distinct byte values 
+     * and store an additional offset pointer per document to dereference the shared 
+     * byte[]. The stored byte[] is presorted and allows access via document id, 
+     * ordinal and by-value.
+     */
+    SORTED
+  };
 
   /**
    * Sole Constructor.
@@ -89,7 +110,7 @@
    * @lucene.experimental
    */
   public FieldInfo(String name, boolean indexed, int number, boolean storeTermVector, 
-            boolean omitNorms, boolean storePayloads, IndexOptions indexOptions, DocValues.Type docValues, DocValues.Type normsType, Map<String,String> attributes) {
+            boolean omitNorms, boolean storePayloads, IndexOptions indexOptions, DocValuesType docValues, DocValuesType normsType, Map<String,String> attributes) {
     this.name = name;
     this.indexed = indexed;
     this.number = number;
@@ -130,9 +151,13 @@
     return true;
   }
 
+  void update(IndexableFieldType ft) {
+    update(ft.indexed(), false, ft.omitNorms(), false, ft.indexOptions());
+  }
+
   // should only be called by FieldInfos#addOrUpdate
   void update(boolean indexed, boolean storeTermVector, boolean omitNorms, boolean storePayloads, IndexOptions indexOptions) {
-
+    //System.out.println("FI.update field=" + name + " indexed=" + indexed + " omitNorms=" + omitNorms + " this.omitNorms=" + this.omitNorms);
     if (this.indexed != indexed) {
       this.indexed = true;                      // once indexed, always index
     }
@@ -163,7 +188,10 @@
     assert checkConsistency();
   }
 
-  void setDocValuesType(DocValues.Type type) {
+  void setDocValuesType(DocValuesType type) {
+    if (docValueType != null && docValueType != type) {
+      throw new IllegalArgumentException("cannot change DocValues type from " + docValueType + " to " + type + " for field \"" + name + "\"");
+    }
     docValueType = type;
     assert checkConsistency();
   }
@@ -181,16 +209,16 @@
   }
 
   /**
-   * Returns {@link DocValues.Type} of the docValues. this may be null if the field has no docvalues.
+   * Returns {@link DocValuesType} of the docValues. this may be null if the field has no docvalues.
    */
-  public DocValues.Type getDocValuesType() {
+  public DocValuesType getDocValuesType() {
     return docValueType;
   }
   
   /**
-   * Returns {@link DocValues.Type} of the norm. this may be null if the field has no norms.
+   * Returns {@link DocValuesType} of the norm. this may be null if the field has no norms.
    */
-  public DocValues.Type getNormType() {
+  public DocValuesType getNormType() {
     return normType;
   }
 
@@ -206,7 +234,10 @@
     assert checkConsistency();
   }
 
-  void setNormValueType(Type type) {
+  void setNormValueType(DocValuesType type) {
+    if (normType != null && normType != type) {
+      throw new IllegalArgumentException("cannot change Norm type from " + normType + " to " + type + " for field \"" + name + "\"");
+    }
     normType = type;
     assert checkConsistency();
   }
Index: lucene/core/src/java/org/apache/lucene/index/IndexableField.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/index/IndexableField.java	(revision 1442822)
+++ lucene/core/src/java/org/apache/lucene/index/IndexableField.java	(working copy)
@@ -55,7 +55,7 @@
    * a "document boost", then you must pre-multiply it across all the
    * relevant fields yourself. 
    * <p>The boost is used to compute the norm factor for the field.  By
-   * default, in the {@link Similarity#computeNorm(FieldInvertState, Norm)} method, 
+   * default, in the {@link Similarity#computeNorm(FieldInvertState)} method, 
    * the boost value is multiplied by the length normalization factor and then
    * rounded by {@link DefaultSimilarity#encodeNormValue(float)} before it is stored in the
    * index.  One should attempt to ensure that this product does not overflow
@@ -65,7 +65,7 @@
    * indexed ({@link IndexableFieldType#indexed()} is false) or omits normalization values
    * ({@link IndexableFieldType#omitNorms()} returns true).
    *
-   * @see Similarity#computeNorm(FieldInvertState, Norm)
+   * @see Similarity#computeNorm(FieldInvertState)
    * @see DefaultSimilarity#encodeNormValue(float)
    */
   public float boost();
Index: lucene/core/src/java/org/apache/lucene/index/FilterAtomicReader.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/index/FilterAtomicReader.java	(revision 1442822)
+++ lucene/core/src/java/org/apache/lucene/index/FilterAtomicReader.java	(working copy)
@@ -406,14 +406,26 @@
   }
 
   @Override
-  public DocValues docValues(String field) throws IOException {
+  public NumericDocValues getNumericDocValues(String field) throws IOException {
     ensureOpen();
-    return in.docValues(field);
+    return in.getNumericDocValues(field);
   }
   
   @Override
-  public DocValues normValues(String field) throws IOException {
+  public BinaryDocValues getBinaryDocValues(String field) throws IOException {
     ensureOpen();
-    return in.normValues(field);
+    return in.getBinaryDocValues(field);
   }
+
+  @Override
+  public SortedDocValues getSortedDocValues(String field) throws IOException {
+    ensureOpen();
+    return in.getSortedDocValues(field);
+  }
+
+  @Override
+  public NumericDocValues getNormValues(String field) throws IOException {
+    ensureOpen();
+    return in.getNormValues(field);
+  }
 }
Index: lucene/core/src/java/org/apache/lucene/index/TypePromoter.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/index/TypePromoter.java	(revision 1442822)
+++ lucene/core/src/java/org/apache/lucene/index/TypePromoter.java	(working copy)
@@ -1,362 +0,0 @@
-package org.apache.lucene.index;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-import java.util.HashMap;
-import java.util.Map;
-
-import org.apache.lucene.index.DocValues.Type;
-import org.apache.lucene.util.BytesRef;
-
-// TODO: maybe we should not automagically promote
-// types... and instead require a given field always has the
-// same type?
-
-/**
- * Type promoter that promotes {@link DocValues} during merge based on their
- * {@link Type} and {@link #getValueSize()}
- * 
- * @lucene.internal
- */
-class TypePromoter {
-  
-  private final static Map<Integer,Type> FLAGS_MAP = new HashMap<Integer,Type>();
-  private static final TypePromoter IDENTITY_PROMOTER = new IdentityTypePromoter();
-  public static final int VAR_TYPE_VALUE_SIZE = -1;
-  
-  private static final int IS_INT = 1 << 0 | 1 << 2;
-  private static final int IS_BYTE = 1 << 1;
-  private static final int IS_FLOAT = 1 << 2 ;
-  /* VAR & FIXED == VAR */
-  private static final int IS_VAR = 1 << 3;
-  private static final int IS_FIXED = 1 << 3 | 1 << 4;
-  /* if we have FIXED & FIXED with different size we promote to VAR */
-  private static final int PROMOTE_TO_VAR_SIZE_MASK = ~(1 << 3);
-  /* STRAIGHT & DEREF == STRAIGHT (dense values win) */
-  private static final int IS_STRAIGHT = 1 << 5;
-  private static final int IS_DEREF = 1 << 5 | 1 << 6;
-  private static final int IS_SORTED = 1 << 7;
-  /* more bits wins (int16 & int32 == int32) */
-  private static final int IS_8_BIT = 1 << 8 | 1 << 9 | 1 << 10 | 1 << 11 | 1 << 12 | 1 << 13; // 8
-  private static final int IS_16_BIT = 1 << 9 | 1 << 10 | 1 << 11 | 1 << 12 | 1 << 13; // 9
-  private static final int IS_32_BIT = 1 << 10 | 1 << 11 | 1 << 13;
-  private static final int IS_64_BIT = 1 << 11;
-  private static final int IS_32_BIT_FLOAT = 1 << 12 | 1 << 13;
-  private static final int IS_64_BIT_FLOAT = 1 << 13;
-  
-  private Type type;
-  private int flags;
-  private int valueSize;
-  
-  /**
-   * Returns a positive value size if this {@link TypePromoter} represents a
-   * fixed variant, otherwise <code>-1</code>
-   * 
-   * @return a positive value size if this {@link TypePromoter} represents a
-   *         fixed variant, otherwise <code>-1</code>
-   */
-  public int getValueSize() {
-    return valueSize;
-  }
-  
-  static {
-    for (Type type : Type.values()) {
-      TypePromoter create = create(type, VAR_TYPE_VALUE_SIZE);
-      FLAGS_MAP.put(create.flags, type);
-    }
-  }
-  
-  /**
-   * Creates a new {@link TypePromoter}
-   * 
-   */
-  protected TypePromoter() {}
-  
-  /**
-   * Creates a new {@link TypePromoter}
-   * 
-   * @param type
-   *          the {@link Type} this promoter represents
-   * 
-   * @param flags
-   *          the promoters flags
-   * @param valueSize
-   *          the value size if {@link #IS_FIXED} or <code>-1</code> otherwise.
-   */
-  protected TypePromoter(Type type, int flags, int valueSize) {
-    this.type = type;
-    this.flags = flags;
-    this.valueSize = valueSize;
-  }
-  
-  /**
-   * Resets the {@link TypePromoter}
-   * 
-   * @param type
-   *          the {@link Type} this promoter represents
-   * 
-   * @param flags
-   *          the promoters flags
-   * @param valueSize
-   *          the value size if {@link #IS_FIXED} or <code>-1</code> otherwise.
-   */
-  protected TypePromoter set(Type type, int flags, int valueSize) {
-    this.type = type;
-    this.flags = flags;
-    this.valueSize = valueSize;
-    return this;
-  }
-  
-  /**
-   * Creates a new promoted {@link TypePromoter} based on this and the given
-   * {@link TypePromoter} or <code>null</code> iff the {@link TypePromoter} 
-   * aren't compatible.
-   * 
-   * @param promoter
-   *          the incoming promoter
-   * @return a new promoted {@link TypePromoter} based on this and the given
-   *         {@link TypePromoter} or <code>null</code> iff the
-   *         {@link TypePromoter} aren't compatible.
-   */
-  public TypePromoter promote(TypePromoter promoter) {
-    return promote(promoter, newPromoter());
-  }
-  
-  private TypePromoter promote(TypePromoter promoter, TypePromoter spare) {
-    int promotedFlags = promoter.flags & this.flags;
-    TypePromoter promoted = reset(FLAGS_MAP.get(promotedFlags), valueSize,
-        spare);
-    if (promoted == null) {
-      return TypePromoter.create(DocValues.Type.BYTES_VAR_STRAIGHT,
-          TypePromoter.VAR_TYPE_VALUE_SIZE);
-    }
-    if ((promoted.flags & IS_BYTE) != 0
-        && (promoted.flags & IS_FIXED) == IS_FIXED) {
-      if (this.valueSize == promoter.valueSize) {
-        return promoted;
-      }
-      return reset(FLAGS_MAP.get(promoted.flags & PROMOTE_TO_VAR_SIZE_MASK),
-          VAR_TYPE_VALUE_SIZE, spare);
-    }
-    
-    return promoted;
-  }
-  
-  /**
-   * Returns the {@link Type} of this {@link TypePromoter}
-   * 
-   * @return the {@link Type} of this {@link TypePromoter}
-   */
-  public Type type() {
-    return type;
-  }
-  
-  private boolean isTypeCompatible(TypePromoter promoter) {
-    int promotedFlags = promoter.flags & this.flags;
-    return (promotedFlags & 0x7) > 0;
-  }
-  
-  private boolean isBytesCompatible(TypePromoter promoter) {
-    int promotedFlags = promoter.flags & this.flags;
-    return (promotedFlags & IS_BYTE) > 0
-        && (promotedFlags & (IS_FIXED | IS_VAR)) > 0;
-  }
-  
-  private boolean isNumericSizeCompatible(TypePromoter promoter) {
-    int promotedFlags = promoter.flags & this.flags;
-    return (promotedFlags & IS_BYTE) == 0
-        && (((promotedFlags & IS_FIXED) > 0 && (promotedFlags & (IS_8_BIT)) > 0) || (promotedFlags & IS_VAR) > 0);
-  }
-  
-  @Override
-  public String toString() {
-    return "TypePromoter [type=" + type + ", sizeInBytes=" + valueSize + "]";
-  }
-  
-  @Override
-  public int hashCode() {
-    final int prime = 31;
-    int result = 1;
-    result = prime * result + flags;
-    result = prime * result + ((type == null) ? 0 : type.hashCode());
-    result = prime * result + valueSize;
-    return result;
-  }
-  
-  @Override
-  public boolean equals(Object obj) {
-    if (this == obj) return true;
-    if (obj == null) return false;
-    if (getClass() != obj.getClass()) return false;
-    TypePromoter other = (TypePromoter) obj;
-    if (flags != other.flags) return false;
-    if (type != other.type) return false;
-    if (valueSize != other.valueSize) return false;
-    return true;
-  }
-  
-  /**
-   * Creates a new {@link TypePromoter} for the given type and size per value.
-   * 
-   * @param type
-   *          the {@link Type} to create the promoter for
-   * @param valueSize
-   *          the size per value in bytes or <code>-1</code> iff the types have
-   *          variable length.
-   * @return a new {@link TypePromoter}
-   */
-  public static TypePromoter create(Type type, int valueSize) {
-    return reset(type, valueSize, new TypePromoter());
-  }
-  
-  private static TypePromoter reset(Type type, int valueSize,
-      TypePromoter promoter) {
-    if (type == null) {
-      return null;
-    }
-    switch (type) {
-      case BYTES_FIXED_DEREF:
-        return promoter.set(type, IS_BYTE | IS_FIXED | IS_DEREF, valueSize);
-      case BYTES_FIXED_SORTED:
-        return promoter.set(type, IS_BYTE | IS_FIXED | IS_SORTED, valueSize);
-      case BYTES_FIXED_STRAIGHT:
-        return promoter.set(type, IS_BYTE | IS_FIXED | IS_STRAIGHT, valueSize);
-      case BYTES_VAR_DEREF:
-        return promoter.set(type, IS_BYTE | IS_VAR | IS_DEREF,
-            VAR_TYPE_VALUE_SIZE);
-      case BYTES_VAR_SORTED:
-        return promoter.set(type, IS_BYTE | IS_VAR | IS_SORTED,
-            VAR_TYPE_VALUE_SIZE);
-      case BYTES_VAR_STRAIGHT:
-        return promoter.set(type, IS_BYTE | IS_VAR | IS_STRAIGHT,
-            VAR_TYPE_VALUE_SIZE);
-      case FIXED_INTS_16:
-        return promoter.set(type, IS_INT | IS_FIXED | IS_STRAIGHT | IS_16_BIT,
-            valueSize);
-      case FIXED_INTS_32:
-        return promoter.set(type, IS_INT | IS_FIXED | IS_STRAIGHT | IS_32_BIT,
-            valueSize);
-      case FIXED_INTS_64:
-        return promoter.set(type, IS_INT | IS_FIXED | IS_STRAIGHT | IS_64_BIT,
-            valueSize);
-      case FIXED_INTS_8:
-        return promoter.set(type, IS_INT | IS_FIXED | IS_STRAIGHT | IS_8_BIT,
-            valueSize);
-      case FLOAT_32:
-        return promoter.set(type,
-            IS_FLOAT | IS_FIXED | IS_STRAIGHT | IS_32_BIT_FLOAT, valueSize);
-      case FLOAT_64:
-        return promoter.set(type,
-            IS_FLOAT | IS_FIXED | IS_STRAIGHT | IS_64_BIT_FLOAT, valueSize);
-      case VAR_INTS:
-        return promoter.set(type, IS_INT | IS_VAR | IS_STRAIGHT,
-            VAR_TYPE_VALUE_SIZE);
-      default:
-        throw new IllegalStateException();
-    }
-  }
-  
-  public static int getValueSize(DocValues.Type type, BytesRef ref) {
-    switch (type) {
-      case VAR_INTS:
-      case BYTES_VAR_DEREF:
-      case BYTES_VAR_SORTED:
-      case BYTES_VAR_STRAIGHT:
-        return -1;
-      case BYTES_FIXED_DEREF:
-      case BYTES_FIXED_SORTED:
-      case BYTES_FIXED_STRAIGHT:
-        assert ref != null;
-        return ref.length;
-      case FIXED_INTS_16:
-        return 2;
-      case FLOAT_32:
-      case FIXED_INTS_32:
-        return 4;
-      case FLOAT_64:
-      case FIXED_INTS_64:
-        return 8;
-      case FIXED_INTS_8:
-        return 1;
-      default:
-        throw new IllegalArgumentException("unknonw docvalues type: "
-            + type.name());
-    }
-  }
-  
-  /**
-   * Returns a {@link TypePromoter} that always promotes to the type provided to
-   * {@link #promote(TypePromoter)}
-   */
-  public static TypePromoter getIdentityPromoter() {
-    return IDENTITY_PROMOTER;
-  }
-  
-  private static TypePromoter newPromoter() {
-    return new TypePromoter(null, 0, -1);
-  }
-  
-  private static class IdentityTypePromoter extends TypePromoter {
-    
-    public IdentityTypePromoter() {
-      super(null, 0, -1);
-    }
-    
-    @Override
-    protected TypePromoter set(Type type, int flags, int valueSize) {
-      throw new UnsupportedOperationException("can not reset IdendityPromotoer");
-    }
-    
-    @Override
-    public TypePromoter promote(TypePromoter promoter) {
-      return promoter;
-    }
-  }
-  
-  static class TypeCompatibility {
-    private final TypePromoter base;
-    private final TypePromoter spare;
-    
-    TypeCompatibility(Type type, int valueSize) {
-      this.base = create(type, valueSize);
-      spare = newPromoter();
-    }
-    
-    boolean isCompatible(Type type, int valueSize) {
-      TypePromoter reset = reset(type, valueSize, spare);
-      if (base.isTypeCompatible(reset)) {
-        if (base.isBytesCompatible(reset)) {
-          return base.valueSize == -1 || base.valueSize == valueSize;
-        } else if (base.flags == reset.flags) {
-          return true;
-        } else if (base.isNumericSizeCompatible(reset)) {
-          return base.valueSize == -1
-              || (base.valueSize > valueSize && valueSize > 0);
-        }
-      }
-      return false;
-    }
-    
-    Type getBaseType() {
-      return base.type();
-    }
-    
-    int getBaseSize() {
-      return base.valueSize;
-    }
-  }
-}
\ No newline at end of file
Index: lucene/core/src/java/org/apache/lucene/index/SegmentCoreReaders.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/index/SegmentCoreReaders.java	(revision 1442822)
+++ lucene/core/src/java/org/apache/lucene/index/SegmentCoreReaders.java	(working copy)
@@ -19,16 +19,19 @@
 
 import java.io.IOException;
 import java.util.Collections;
+import java.util.HashMap;
 import java.util.LinkedHashSet;
+import java.util.Map;
 import java.util.Set;
 import java.util.concurrent.atomic.AtomicInteger;
 
 import org.apache.lucene.codecs.Codec;
 import org.apache.lucene.codecs.FieldsProducer;
-import org.apache.lucene.codecs.PerDocProducer;
 import org.apache.lucene.codecs.PostingsFormat;
+import org.apache.lucene.codecs.DocValuesProducer;
 import org.apache.lucene.codecs.StoredFieldsReader;
 import org.apache.lucene.codecs.TermVectorsReader;
+import org.apache.lucene.index.FieldInfo.DocValuesType;
 import org.apache.lucene.index.SegmentReader.CoreClosedListener;
 import org.apache.lucene.store.CompoundFileDirectory;
 import org.apache.lucene.store.Directory;
@@ -51,8 +54,8 @@
   final FieldInfos fieldInfos;
   
   final FieldsProducer fields;
-  final PerDocProducer perDocProducer;
-  final PerDocProducer norms;
+  final DocValuesProducer dvProducer;
+  final DocValuesProducer normsProducer;
 
   final int termsIndexDivisor;
   
@@ -62,6 +65,10 @@
   final TermVectorsReader termVectorsReaderOrig;
   final CompoundFileDirectory cfsReader;
 
+  // TODO: make a single thread local w/ a
+  // Thingy class holding fieldsReader, termVectorsReader,
+  // normsProducer, dvProducer
+
   final CloseableThreadLocal<StoredFieldsReader> fieldsReaderLocal = new CloseableThreadLocal<StoredFieldsReader>() {
     @Override
     protected StoredFieldsReader initialValue() {
@@ -72,11 +79,24 @@
   final CloseableThreadLocal<TermVectorsReader> termVectorsLocal = new CloseableThreadLocal<TermVectorsReader>() {
     @Override
     protected TermVectorsReader initialValue() {
-      return (termVectorsReaderOrig == null) ?
-        null : termVectorsReaderOrig.clone();
+      return (termVectorsReaderOrig == null) ? null : termVectorsReaderOrig.clone();
     }
   };
-  
+
+  final CloseableThreadLocal<Map<String,Object>> docValuesLocal = new CloseableThreadLocal<Map<String,Object>>() {
+    @Override
+    protected Map<String,Object> initialValue() {
+      return new HashMap<String,Object>();
+    }
+  };
+
+  final CloseableThreadLocal<Map<String,Object>> normsLocal = new CloseableThreadLocal<Map<String,Object>>() {
+    @Override
+    protected Map<String,Object> initialValue() {
+      return new HashMap<String,Object>();
+    }
+  };
+
   private final Set<CoreClosedListener> coreClosedListeners = 
       Collections.synchronizedSet(new LinkedHashSet<CoreClosedListener>());
   
@@ -109,8 +129,20 @@
       // ask codec for its Norms: 
       // TODO: since we don't write any norms file if there are no norms,
       // kinda jaky to assume the codec handles the case of no norms file at all gracefully?!
-      norms = codec.normsFormat().docsProducer(segmentReadState);
-      perDocProducer = codec.docValuesFormat().docsProducer(segmentReadState);
+
+      if (fieldInfos.hasDocValues()) {
+        dvProducer = codec.docValuesFormat().fieldsProducer(segmentReadState);
+        assert dvProducer != null;
+      } else {
+        dvProducer = null;
+      }
+
+      if (fieldInfos.hasNorms()) {
+        normsProducer = codec.normsFormat().normsProducer(segmentReadState);
+        assert normsProducer != null;
+      } else {
+        normsProducer = null;
+      }
   
       fieldsReaderOrig = si.info.getCodec().storedFieldsFormat().fieldsReader(cfsDir, si.info, fieldInfos, context);
 
@@ -137,17 +169,123 @@
   void incRef() {
     ref.incrementAndGet();
   }
-  
+
+  NumericDocValues getNumericDocValues(String field) throws IOException {
+    FieldInfo fi = fieldInfos.fieldInfo(field);
+    if (fi == null) {
+      // Field does not exist
+      return null;
+    }
+    if (fi.getDocValuesType() == null) {
+      // Field was not indexed with doc values
+      return null;
+    }
+    if (fi.getDocValuesType() != DocValuesType.NUMERIC) {
+      // DocValues were not numeric
+      return null;
+    }
+
+    assert dvProducer != null;
+
+    Map<String,Object> dvFields = docValuesLocal.get();
+
+    NumericDocValues dvs = (NumericDocValues) dvFields.get(field);
+    if (dvs == null) {
+      dvs = dvProducer.getNumeric(fi);
+      dvFields.put(field, dvs);
+    }
+
+    return dvs;
+  }
+
+  BinaryDocValues getBinaryDocValues(String field) throws IOException {
+    FieldInfo fi = fieldInfos.fieldInfo(field);
+    if (fi == null) {
+      // Field does not exist
+      return null;
+    }
+    if (fi.getDocValuesType() == null) {
+      // Field was not indexed with doc values
+      return null;
+    }
+    if (fi.getDocValuesType() != DocValuesType.BINARY) {
+      // DocValues were not binary
+      return null;
+    }
+
+    assert dvProducer != null;
+
+    Map<String,Object> dvFields = docValuesLocal.get();
+
+    BinaryDocValues dvs = (BinaryDocValues) dvFields.get(field);
+    if (dvs == null) {
+      dvs = dvProducer.getBinary(fi);
+      dvFields.put(field, dvs);
+    }
+
+    return dvs;
+  }
+
+  SortedDocValues getSortedDocValues(String field) throws IOException {
+    FieldInfo fi = fieldInfos.fieldInfo(field);
+    if (fi == null) {
+      // Field does not exist
+      return null;
+    }
+    if (fi.getDocValuesType() == null) {
+      // Field was not indexed with doc values
+      return null;
+    }
+    if (fi.getDocValuesType() != DocValuesType.SORTED) {
+      // DocValues were not sorted
+      return null;
+    }
+
+    assert dvProducer != null;
+
+    Map<String,Object> dvFields = docValuesLocal.get();
+
+    SortedDocValues dvs = (SortedDocValues) dvFields.get(field);
+    if (dvs == null) {
+      dvs = dvProducer.getSorted(fi);
+      dvFields.put(field, dvs);
+    }
+
+    return dvs;
+  }
+
+  NumericDocValues getNormValues(String field) throws IOException {
+    FieldInfo fi = fieldInfos.fieldInfo(field);
+    if (fi == null) {
+      // Field does not exist
+      return null;
+    }
+    if (!fi.hasNorms()) {
+      return null;
+    }
+   
+    assert normsProducer != null;
+
+    Map<String,Object> normFields = normsLocal.get();
+
+    NumericDocValues norms = (NumericDocValues) normFields.get(field);
+    if (norms == null) {
+      norms = normsProducer.getNumeric(fi);
+      normFields.put(field, norms);
+    }
+
+    return norms;
+  }
+
   void decRef() throws IOException {
-    //System.out.println("core.decRef seg=" + owner.getSegmentInfo() + " rc=" + ref);
     if (ref.decrementAndGet() == 0) {
-      IOUtils.close(termVectorsLocal, fieldsReaderLocal, fields, perDocProducer,
-        termVectorsReaderOrig, fieldsReaderOrig, cfsReader, norms);
+      IOUtils.close(termVectorsLocal, fieldsReaderLocal, docValuesLocal, normsLocal, fields, dvProducer,
+                    termVectorsReaderOrig, fieldsReaderOrig, cfsReader, normsProducer);
       notifyCoreClosedListeners();
     }
   }
   
-  private final void notifyCoreClosedListeners() {
+  private void notifyCoreClosedListeners() {
     synchronized(coreClosedListeners) {
       for (CoreClosedListener listener : coreClosedListeners) {
         listener.onClose(owner);
Index: lucene/core/src/java/org/apache/lucene/index/IndexableFieldType.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/index/IndexableFieldType.java	(revision 1442822)
+++ lucene/core/src/java/org/apache/lucene/index/IndexableFieldType.java	(working copy)
@@ -18,6 +18,7 @@
  */
 
 import org.apache.lucene.analysis.Analyzer; // javadocs
+import org.apache.lucene.index.FieldInfo.DocValuesType;
 import org.apache.lucene.index.FieldInfo.IndexOptions;
 
 /** 
@@ -92,8 +93,8 @@
   public IndexOptions indexOptions();
 
   /** 
-   * DocValues {@link DocValues.Type}: if non-null then the field's value
+   * DocValues {@link DocValuesType}: if non-null then the field's value
    * will be indexed into docValues.
    */
-  public DocValues.Type docValueType();  
+  public DocValuesType docValueType();  
 }
Index: lucene/core/src/java/org/apache/lucene/index/TermVectorsConsumerPerField.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/index/TermVectorsConsumerPerField.java	(revision 1442822)
+++ lucene/core/src/java/org/apache/lucene/index/TermVectorsConsumerPerField.java	(working copy)
@@ -188,7 +188,6 @@
 
     termsHashPerField.reset();
 
-    // commit the termVectors once successful - FI will otherwise reset them
     fieldInfo.setStoreTermVectors();
   }
 
Index: lucene/core/src/java/org/apache/lucene/index/SortedDocValues.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/index/SortedDocValues.java	(revision 0)
+++ lucene/core/src/java/org/apache/lucene/index/SortedDocValues.java	(working copy)
@@ -0,0 +1,117 @@
+package org.apache.lucene.index;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.util.BytesRef;
+
+/**
+ * A per-document byte[] with presorted values.
+ * <p>
+ * Per-Document values in a SortedDocValues are deduplicated, dereferenced,
+ * and sorted into a dictionary of unique values. A pointer to the
+ * dictionary value (ordinal) can be retrieved for each document. Ordinals
+ * are dense and in increasing sorted order.
+ */
+public abstract class SortedDocValues extends BinaryDocValues {
+  
+  /** Sole constructor. (For invocation by subclass 
+   * constructors, typically implicit.) */
+  protected SortedDocValues() {}
+
+  /**
+   * Returns the ordinal for the specified docID.
+   * @param  docID document ID to lookup
+   * @return ordinal for the document: this is dense, starts at 0, then
+   *         increments by 1 for the next value in sorted order. 
+   */
+  public abstract int getOrd(int docID);
+
+  /** Retrieves the value for the specified ordinal.
+   * @param ord ordinal to lookup
+   * @param result will be populated with the ordinal's value
+   * @see #getOrd(int) 
+   */
+  public abstract void lookupOrd(int ord, BytesRef result);
+
+  /**
+   * Returns the number of unique values.
+   * @return number of unique values in this SortedDocValues. This is
+   *         also equivalent to one plus the maximum ordinal.
+   */
+  public abstract int getValueCount();
+
+  @Override
+  public void get(int docID, BytesRef result) {
+    int ord = getOrd(docID);
+    if (ord == -1) {
+      result.bytes = MISSING;
+      result.length = 0;
+      result.offset = 0;
+    } else {
+      lookupOrd(ord, result);
+    }
+  }
+
+  /** An empty SortedDocValues which returns {@link #MISSING} for every document */
+  public static final SortedDocValues EMPTY = new SortedDocValues() {
+    @Override
+    public int getOrd(int docID) {
+      return 0;
+    }
+
+    @Override
+    public void lookupOrd(int ord, BytesRef result) {
+      result.bytes = MISSING;
+      result.offset = 0;
+      result.length = 0;
+    }
+
+    @Override
+    public int getValueCount() {
+      return 1;
+    }
+  };
+
+  /** If {@code key} exists, returns its ordinal, else
+   *  returns {@code -insertionPoint-1}, like {@code
+   *  Arrays.binarySearch}.
+   *
+   *  @param key Key to look up
+   **/
+  public int lookupTerm(BytesRef key) {
+    BytesRef spare = new BytesRef();
+    int low = 0;
+    int high = getValueCount()-1;
+
+    while (low <= high) {
+      int mid = (low + high) >>> 1;
+      lookupOrd(mid, spare);
+      int cmp = spare.compareTo(key);
+
+      if (cmp < 0) {
+        low = mid + 1;
+      } else if (cmp > 0) {
+        high = mid - 1;
+      } else {
+        return mid; // key found
+      }
+    }
+
+    return -(low + 1);  // key not found.
+  }
+}

Property changes on: lucene/core/src/java/org/apache/lucene/index/SortedDocValues.java
___________________________________________________________________
Added: svn:eol-style
## -0,0 +1 ##
+native
Index: lucene/core/src/java/org/apache/lucene/util/BytesRefHash.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/util/BytesRefHash.java	(revision 1442822)
+++ lucene/core/src/java/org/apache/lucene/util/BytesRefHash.java	(working copy)
@@ -69,7 +69,7 @@
   public BytesRefHash() { 
     this(new ByteBlockPool(new DirectAllocator()));
   }
-
+  
   /**
    * Creates a new {@link BytesRefHash}
    */
@@ -118,7 +118,8 @@
   public BytesRef get(int ord, BytesRef ref) {
     assert bytesStart != null : "bytesStart is null - not initialized";
     assert ord < bytesStart.length: "ord exceeds byteStart len: " + bytesStart.length;
-    return pool.setBytesRef(ref, bytesStart[ord]);
+    pool.setBytesRef(ref, bytesStart[ord]);
+    return ref;
   }
 
   /**
@@ -129,7 +130,7 @@
    * order to reuse this {@link BytesRefHash} instance.
    * </p>
    */
-  public int[] compact() {
+  int[] compact() {
     assert bytesStart != null : "Bytesstart is null - not initialized";
     int upto = 0;
     for (int i = 0; i < hashSize; i++) {
@@ -171,8 +172,9 @@
       protected int compare(int i, int j) {
         final int ord1 = compact[i], ord2 = compact[j];
         assert bytesStart.length > ord1 && bytesStart.length > ord2;
-        return comp.compare(pool.setBytesRef(scratch1, bytesStart[ord1]),
-          pool.setBytesRef(scratch2, bytesStart[ord2]));
+        pool.setBytesRef(scratch1, bytesStart[ord1]);
+        pool.setBytesRef(scratch2, bytesStart[ord2]);
+        return comp.compare(scratch1, scratch2);
       }
 
       @Override
@@ -186,8 +188,8 @@
       protected int comparePivot(int j) {
         final int ord = compact[j];
         assert bytesStart.length > ord;
-        return comp.compare(pivot,
-          pool.setBytesRef(scratch2, bytesStart[ord]));
+        pool.setBytesRef(scratch2, bytesStart[ord]);
+        return comp.compare(pivot, scratch2);
       }
       
       private final BytesRef pivot = new BytesRef(),
@@ -197,7 +199,8 @@
   }
 
   private boolean equals(int ord, BytesRef b) {
-    return pool.setBytesRef(scratch1, bytesStart[ord]).bytesEquals(b);
+    pool.setBytesRef(scratch1, bytesStart[ord]);
+    return scratch1.bytesEquals(b);
   }
 
   private boolean shrink(int targetSize) {
@@ -208,8 +211,7 @@
       newSize /= 2;
     }
     if (newSize != hashSize) {
-      bytesUsed.addAndGet(RamUsageEstimator.NUM_BYTES_INT
-          * -(hashSize - newSize));
+      bytesUsed.addAndGet(RamUsageEstimator.NUM_BYTES_INT * -(hashSize - newSize));
       hashSize = newSize;
       ords = new int[hashSize];
       Arrays.fill(ords, -1);
@@ -248,8 +250,7 @@
   public void close() {
     clear(true);
     ords = null;
-    bytesUsed.addAndGet(RamUsageEstimator.NUM_BYTES_INT
-        * -hashSize);
+    bytesUsed.addAndGet(RamUsageEstimator.NUM_BYTES_INT * -hashSize);
   }
 
   /**
@@ -533,51 +534,7 @@
      */
     public abstract Counter bytesUsed();
   }
-  
-  /** A simple {@link BytesStartArray} that tracks all
-   *  memory allocation using a shared {@link Counter}
-   *  instance.  */
-  public static class TrackingDirectBytesStartArray extends BytesStartArray {
-    protected final int initSize;
-    private int[] bytesStart;
-    protected final Counter bytesUsed;
-    
-    public TrackingDirectBytesStartArray(int initSize, Counter bytesUsed) {
-      this.initSize = initSize;
-      this.bytesUsed = bytesUsed;
-    }
 
-    @Override
-    public int[] clear() {
-      if (bytesStart != null) {
-        bytesUsed.addAndGet(-bytesStart.length * RamUsageEstimator.NUM_BYTES_INT);
-      }
-      return bytesStart = null;
-    }
-
-    @Override
-    public int[] grow() {
-      assert bytesStart != null;
-      final int oldSize = bytesStart.length;
-      bytesStart = ArrayUtil.grow(bytesStart, bytesStart.length + 1);
-      bytesUsed.addAndGet((bytesStart.length - oldSize) * RamUsageEstimator.NUM_BYTES_INT);
-      return bytesStart;
-    }
-
-    @Override
-    public int[] init() {
-      bytesStart = new int[ArrayUtil.oversize(initSize,
-          RamUsageEstimator.NUM_BYTES_INT)];
-      bytesUsed.addAndGet((bytesStart.length) * RamUsageEstimator.NUM_BYTES_INT);
-      return bytesStart;
-    }
-
-    @Override
-    public Counter bytesUsed() {
-      return bytesUsed;
-    }
-  }
-
   /** A simple {@link BytesStartArray} that tracks
    *  memory allocation using a private {@link AtomicLong}
    *  instance.  */
@@ -590,9 +547,13 @@
     private int[] bytesStart;
     private final Counter bytesUsed;
     
+    public DirectBytesStartArray(int initSize, Counter counter) {
+      this.bytesUsed = counter;
+      this.initSize = initSize;      
+    }
+    
     public DirectBytesStartArray(int initSize) {
-      this.bytesUsed = Counter.newCounter();
-      this.initSize = initSize;
+      this(initSize, Counter.newCounter());
     }
 
     @Override
Index: lucene/core/src/java/org/apache/lucene/util/packed/Packed64.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/util/packed/Packed64.java	(revision 1442822)
+++ lucene/core/src/java/org/apache/lucene/util/packed/Packed64.java	(working copy)
@@ -244,7 +244,12 @@
 
   @Override
   public long ramBytesUsed() {
-    return RamUsageEstimator.sizeOf(blocks);
+    return RamUsageEstimator.alignObjectSize(
+        RamUsageEstimator.NUM_BYTES_OBJECT_HEADER
+        + 3 * RamUsageEstimator.NUM_BYTES_INT     // bpvMinusBlockSize,valueCount,bitsPerValue
+        + RamUsageEstimator.NUM_BYTES_LONG        // maskRight
+        + RamUsageEstimator.NUM_BYTES_OBJECT_REF) // blocks ref
+        + RamUsageEstimator.sizeOf(blocks);
   }
 
   @Override
Index: lucene/core/src/java/org/apache/lucene/util/packed/Packed64SingleBlock.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/util/packed/Packed64SingleBlock.java	(revision 1442822)
+++ lucene/core/src/java/org/apache/lucene/util/packed/Packed64SingleBlock.java	(working copy)
@@ -60,7 +60,11 @@
 
   @Override
   public long ramBytesUsed() {
-    return RamUsageEstimator.sizeOf(blocks);
+    return RamUsageEstimator.alignObjectSize(
+        RamUsageEstimator.NUM_BYTES_OBJECT_HEADER
+        + 2 * RamUsageEstimator.NUM_BYTES_INT     // valueCount,bitsPerValue
+        + RamUsageEstimator.NUM_BYTES_OBJECT_REF) // blocks ref
+        + RamUsageEstimator.sizeOf(blocks);
   }
 
   @Override
Index: lucene/core/src/java/org/apache/lucene/util/packed/Direct32.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/util/packed/Direct32.java	(revision 1442822)
+++ lucene/core/src/java/org/apache/lucene/util/packed/Direct32.java	(working copy)
@@ -61,7 +61,11 @@
 
   @Override
   public long ramBytesUsed() {
-    return RamUsageEstimator.sizeOf(values);
+    return RamUsageEstimator.alignObjectSize(
+        RamUsageEstimator.NUM_BYTES_OBJECT_HEADER
+        + 2 * RamUsageEstimator.NUM_BYTES_INT     // valueCount,bitsPerValue
+        + RamUsageEstimator.NUM_BYTES_OBJECT_REF) // values ref
+        + RamUsageEstimator.sizeOf(values);
   }
 
   @Override
Index: lucene/core/src/java/org/apache/lucene/util/packed/Direct16.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/util/packed/Direct16.java	(revision 1442822)
+++ lucene/core/src/java/org/apache/lucene/util/packed/Direct16.java	(working copy)
@@ -61,7 +61,11 @@
 
   @Override
   public long ramBytesUsed() {
-    return RamUsageEstimator.sizeOf(values);
+    return RamUsageEstimator.alignObjectSize(
+        RamUsageEstimator.NUM_BYTES_OBJECT_HEADER
+        + 2 * RamUsageEstimator.NUM_BYTES_INT     // valueCount,bitsPerValue
+        + RamUsageEstimator.NUM_BYTES_OBJECT_REF) // values ref
+        + RamUsageEstimator.sizeOf(values);
   }
 
   @Override
Index: lucene/core/src/java/org/apache/lucene/util/packed/PackedWriter.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/util/packed/PackedWriter.java	(revision 1442822)
+++ lucene/core/src/java/org/apache/lucene/util/packed/PackedWriter.java	(working copy)
@@ -56,7 +56,7 @@
 
   @Override
   public void add(long v) throws IOException {
-    assert v >= 0 && v <= PackedInts.maxValue(bitsPerValue);
+    assert bitsPerValue == 64 || (v >= 0 && v <= PackedInts.maxValue(bitsPerValue)) : bitsPerValue;
     assert !finished;
     if (valueCount != -1 && written >= valueCount) {
       throw new EOFException("Writing past end of stream");
Index: lucene/core/src/java/org/apache/lucene/util/packed/Direct64.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/util/packed/Direct64.java	(revision 1442822)
+++ lucene/core/src/java/org/apache/lucene/util/packed/Direct64.java	(working copy)
@@ -56,7 +56,11 @@
 
   @Override
   public long ramBytesUsed() {
-    return RamUsageEstimator.sizeOf(values);
+    return RamUsageEstimator.alignObjectSize(
+        RamUsageEstimator.NUM_BYTES_OBJECT_HEADER
+        + 2 * RamUsageEstimator.NUM_BYTES_INT     // valueCount,bitsPerValue
+        + RamUsageEstimator.NUM_BYTES_OBJECT_REF) // values ref
+        + RamUsageEstimator.sizeOf(values);
   }
 
   @Override
Index: lucene/core/src/java/org/apache/lucene/util/packed/PackedInts.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/util/packed/PackedInts.java	(revision 1442822)
+++ lucene/core/src/java/org/apache/lucene/util/packed/PackedInts.java	(working copy)
@@ -681,6 +681,53 @@
     }
   }
 
+  /** A {@link Reader} which has all its values equal to 0 (bitsPerValue = 0). */
+  public static final class NullReader implements Reader {
+
+    private final int valueCount;
+
+    /** Sole constructor. */
+    public NullReader(int valueCount) {
+      this.valueCount = valueCount;
+    }
+
+    @Override
+    public long get(int index) {
+      return 0;
+    }
+
+    @Override
+    public int get(int index, long[] arr, int off, int len) {
+      return 0;
+    }
+
+    @Override
+    public int getBitsPerValue() {
+      return 0;
+    }
+
+    @Override
+    public int size() {
+      return valueCount;
+    }
+
+    @Override
+    public long ramBytesUsed() {
+      return 0;
+    }
+
+    @Override
+    public Object getArray() {
+      return null;
+    }
+
+    @Override
+    public boolean hasArray() {
+      return false;
+    }
+
+  }
+
   /** A write-once Writer.
    * @lucene.internal
    */
@@ -800,6 +847,22 @@
         throw new AssertionError("Unknown Writer format: " + format);
     }
   }
+  
+  /**
+   * Expert: Restore a {@link Reader} from a stream without reading metadata at
+   * the beginning of the stream. This method is useful to restore data when
+   * metadata has been previously read using {@link #readHeader(DataInput)}.
+   *
+   * @param in           the stream to read data from, positioned at the beginning of the packed values
+   * @param header       metadata result from <code>readHeader()</code>
+   * @return             a Reader
+   * @throws IOException If there is a low-level I/O error
+   * @see #readHeader(DataInput)
+   * @lucene.internal
+   */
+  public static Reader getReaderNoHeader(DataInput in, Header header) throws IOException {
+    return getReaderNoHeader(in, header.format, header.version, header.valueCount, header.bitsPerValue);
+  }
 
   /**
    * Restore a {@link Reader} from a stream.
@@ -912,6 +975,23 @@
         throw new AssertionError("Unknwown format: " + format);
     }
   }
+  
+  /**
+   * Expert: Construct a direct {@link Reader} from an {@link IndexInput} 
+   * without reading metadata at the beginning of the stream. This method is 
+   * useful to restore data when metadata has been previously read using 
+   * {@link #readHeader(DataInput)}.
+   *
+   * @param in           the stream to read data from, positioned at the beginning of the packed values
+   * @param header       metadata result from <code>readHeader()</code>
+   * @return             a Reader
+   * @throws IOException If there is a low-level I/O error
+   * @see #readHeader(DataInput)
+   * @lucene.internal
+   */
+  public static Reader getDirectReaderNoHeader(IndexInput in, Header header) throws IOException {
+    return getDirectReaderNoHeader(in, header.format, header.version, header.valueCount, header.bitsPerValue);
+  }
 
   /**
    * Construct a direct {@link Reader} from an {@link IndexInput}. This method
@@ -1144,5 +1224,41 @@
       }
     }
   }
+  
+  /**
+   * Expert: reads only the metadata from a stream. This is useful to later
+   * restore a stream or open a direct reader via 
+   * {@link #getReaderNoHeader(DataInput, Header)}
+   * or {@link #getDirectReaderNoHeader(IndexInput, Header)}.
+   * @param    in the stream to read data
+   * @return   packed integer metadata.
+   * @throws   IOException If there is a low-level I/O error
+   * @see #getReaderNoHeader(DataInput, Header)
+   * @see #getDirectReaderNoHeader(IndexInput, Header)
+   */
+  public static Header readHeader(DataInput in) throws IOException {
+    final int version = CodecUtil.checkHeader(in, CODEC_NAME, VERSION_START, VERSION_CURRENT);
+    final int bitsPerValue = in.readVInt();
+    assert bitsPerValue > 0 && bitsPerValue <= 64: "bitsPerValue=" + bitsPerValue;
+    final int valueCount = in.readVInt();
+    final Format format = Format.byId(in.readVInt());
+    return new Header(format, valueCount, bitsPerValue, version);
+  }
+  
+  /** Header identifying the structure of a packed integer array. */
+  public static class Header {
 
+    private final Format format;
+    private final int valueCount;
+    private final int bitsPerValue;
+    private final int version;
+
+    public Header(Format format, int valueCount, int bitsPerValue, int version) {
+      this.format = format;
+      this.valueCount = valueCount;
+      this.bitsPerValue = bitsPerValue;
+      this.version = version;
+    }    
+  }
+
 }
\ No newline at end of file
Index: lucene/core/src/java/org/apache/lucene/util/packed/BlockPackedReaderIterator.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/util/packed/BlockPackedReaderIterator.java	(revision 0)
+++ lucene/core/src/java/org/apache/lucene/util/packed/BlockPackedReaderIterator.java	(working copy)
@@ -0,0 +1,241 @@
+package org.apache.lucene.util.packed;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import static org.apache.lucene.util.packed.BlockPackedWriter.BPV_SHIFT;
+import static org.apache.lucene.util.packed.BlockPackedWriter.MIN_VALUE_EQUALS_0;
+import static org.apache.lucene.util.packed.BlockPackedWriter.checkBlockSize;
+
+import java.io.EOFException;
+import java.io.IOException;
+import java.util.Arrays;
+
+import org.apache.lucene.store.DataInput;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.util.LongsRef;
+
+/**
+ * Reader for sequences of longs written with {@link BlockPackedWriter}.
+ * @see BlockPackedWriter
+ * @lucene.internal
+ */
+public final class BlockPackedReaderIterator {
+
+  static long zigZagDecode(long n) {
+    return ((n >>> 1) ^ -(n & 1));
+  }
+
+  // same as DataInput.readVLong but supports negative values
+  static long readVLong(DataInput in) throws IOException {
+    byte b = in.readByte();
+    if (b >= 0) return b;
+    long i = b & 0x7FL;
+    b = in.readByte();
+    i |= (b & 0x7FL) << 7;
+    if (b >= 0) return i;
+    b = in.readByte();
+    i |= (b & 0x7FL) << 14;
+    if (b >= 0) return i;
+    b = in.readByte();
+    i |= (b & 0x7FL) << 21;
+    if (b >= 0) return i;
+    b = in.readByte();
+    i |= (b & 0x7FL) << 28;
+    if (b >= 0) return i;
+    b = in.readByte();
+    i |= (b & 0x7FL) << 35;
+    if (b >= 0) return i;
+    b = in.readByte();
+    i |= (b & 0x7FL) << 42;
+    if (b >= 0) return i;
+    b = in.readByte();
+    i |= (b & 0x7FL) << 49;
+    if (b >= 0) return i;
+    b = in.readByte();
+    i |= (b & 0xFFL) << 56;
+    return i;
+  }
+
+  DataInput in;
+  final int packedIntsVersion;
+  long valueCount;
+  final int blockSize;
+  final long[] values;
+  final LongsRef valuesRef;
+  byte[] blocks;
+  int off;
+  long ord;
+
+  /** Sole constructor.
+   * @param blockSize the number of values of a block, must be equal to the
+   *                  block size of the {@link BlockPackedWriter} which has
+   *                  been used to write the stream
+   */
+  public BlockPackedReaderIterator(DataInput in, int packedIntsVersion, int blockSize, long valueCount) {
+    checkBlockSize(blockSize);
+    this.packedIntsVersion = packedIntsVersion;
+    this.blockSize = blockSize;
+    this.values = new long[blockSize];
+    this.valuesRef = new LongsRef(this.values, 0, 0);
+    reset(in, valueCount);
+  }
+
+  /** Reset the current reader to wrap a stream of <code>valueCount</code>
+   * values contained in <code>in</code>. The block size remains unchanged. */
+  public void reset(DataInput in, long valueCount) {
+    this.in = in;
+    assert valueCount >= 0;
+    this.valueCount = valueCount;
+    off = blockSize;
+    ord = 0;
+  }
+
+  /** Skip exactly <code>count</code> values. */
+  public void skip(long count) throws IOException {
+    assert count >= 0;
+    if (ord + count > valueCount || ord + count < 0) {
+      throw new EOFException();
+    }
+
+    // 1. skip buffered values
+    final int skipBuffer = (int) Math.min(count, blockSize - off);
+    off += skipBuffer;
+    ord += skipBuffer;
+    count -= skipBuffer;
+    if (count == 0L) {
+      return;
+    }
+
+    // 2. skip as many blocks as necessary
+    assert off == blockSize;
+    while (count >= blockSize) {
+      final int token = in.readByte() & 0xFF;
+      final int bitsPerValue = token >>> BPV_SHIFT;
+      if (bitsPerValue > 64) {
+        throw new IOException("Corrupted");
+      }
+      if ((token & MIN_VALUE_EQUALS_0) == 0) {
+        readVLong(in);
+      }
+      final long blockBytes = PackedInts.Format.PACKED.byteCount(packedIntsVersion, blockSize, bitsPerValue);
+      skipBytes(blockBytes);
+      ord += blockSize;
+      count -= blockSize;
+    }
+    if (count == 0L) {
+      return;
+    }
+
+    // 3. skip last values
+    assert count < blockSize;
+    refill();
+    ord += count;
+    off += count;
+  }
+
+  private void skipBytes(long count) throws IOException {
+    if (in instanceof IndexInput) {
+      final IndexInput iin = (IndexInput) in;
+      iin.seek(iin.getFilePointer() + count);
+    } else {
+      if (blocks == null) {
+        blocks = new byte[blockSize];
+      }
+      long skipped = 0;
+      while (skipped < count) {
+        final int toSkip = (int) Math.min(blocks.length, count - skipped);
+        in.readBytes(blocks, 0, toSkip);
+        skipped += toSkip;
+      }
+    }
+  }
+
+  /** Read the next value. */
+  public long next() throws IOException {
+    if (ord == valueCount) {
+      throw new EOFException();
+    }
+    if (off == blockSize) {
+      refill();
+    }
+    final long value = values[off++];
+    ++ord;
+    return value;
+  }
+
+  /** Read between <tt>1</tt> and <code>count</code> values. */
+  public LongsRef next(int count) throws IOException {
+    assert count > 0;
+    if (ord == valueCount) {
+      throw new EOFException();
+    }
+    if (off == blockSize) {
+      refill();
+    }
+
+    count = Math.min(count, blockSize - off);
+    count = (int) Math.min(count, valueCount - ord);
+
+    valuesRef.offset = off;
+    valuesRef.length = count;
+    off += count;
+    ord += count;
+    return valuesRef;
+  }
+
+  private void refill() throws IOException {
+    final int token = in.readByte() & 0xFF;
+    final boolean minEquals0 = (token & MIN_VALUE_EQUALS_0) != 0;
+    final int bitsPerValue = token >>> BPV_SHIFT;
+    if (bitsPerValue > 64) {
+      throw new IOException("Corrupted");
+    }
+    final long minValue = minEquals0 ? 0L : zigZagDecode(1L + readVLong(in));
+    assert minEquals0 || minValue != 0;
+
+    if (bitsPerValue == 0) {
+      Arrays.fill(values, minValue);
+    } else {
+      final PackedInts.Decoder decoder = PackedInts.getDecoder(PackedInts.Format.PACKED, packedIntsVersion, bitsPerValue);
+      final int iterations = blockSize / decoder.byteValueCount();
+      final int blocksSize = iterations * decoder.byteBlockCount();
+      if (blocks == null || blocks.length < blocksSize) {
+        blocks = new byte[blocksSize];
+      }
+
+      final int valueCount = (int) Math.min(this.valueCount - ord, blockSize);
+      final int blocksCount = (int) PackedInts.Format.PACKED.byteCount(packedIntsVersion, valueCount, bitsPerValue);
+      in.readBytes(blocks, 0, blocksCount);
+
+      decoder.decode(blocks, 0, values, 0, iterations);
+
+      if (minValue != 0) {
+        for (int i = 0; i < valueCount; ++i) {
+          values[i] += minValue;
+        }
+      }
+    }
+    off = 0;
+  }
+
+  /** Return the offset of the next value to read. */
+  public long ord() {
+    return ord;
+  }
+
+}

Property changes on: lucene/core/src/java/org/apache/lucene/util/packed/BlockPackedReaderIterator.java
___________________________________________________________________
Added: svn:eol-style
## -0,0 +1 ##
+native
Index: lucene/core/src/java/org/apache/lucene/util/packed/Direct8.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/util/packed/Direct8.java	(revision 1442822)
+++ lucene/core/src/java/org/apache/lucene/util/packed/Direct8.java	(working copy)
@@ -59,7 +59,11 @@
 
   @Override
   public long ramBytesUsed() {
-    return RamUsageEstimator.sizeOf(values);
+    return RamUsageEstimator.alignObjectSize(
+        RamUsageEstimator.NUM_BYTES_OBJECT_HEADER
+        + 2 * RamUsageEstimator.NUM_BYTES_INT     // valueCount,bitsPerValue
+        + RamUsageEstimator.NUM_BYTES_OBJECT_REF) // values ref
+        + RamUsageEstimator.sizeOf(values);
   }
 
   @Override
Index: lucene/core/src/java/org/apache/lucene/util/packed/MonotonicBlockPackedReader.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/util/packed/MonotonicBlockPackedReader.java	(revision 0)
+++ lucene/core/src/java/org/apache/lucene/util/packed/MonotonicBlockPackedReader.java	(working copy)
@@ -0,0 +1,83 @@
+package org.apache.lucene.util.packed;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import static org.apache.lucene.util.packed.AbstractBlockPackedWriter.checkBlockSize;
+import static org.apache.lucene.util.packed.BlockPackedReaderIterator.zigZagDecode;
+
+import java.io.IOException;
+
+import org.apache.lucene.store.IndexInput;
+
+/**
+ * Provides random access to a stream written with
+ * {@link MonotonicBlockPackedWriter}.
+ * @lucene.internal
+ */
+public final class MonotonicBlockPackedReader {
+
+  private final int blockShift, blockMask;
+  private final long valueCount;
+  private final long[] minValues;
+  private final float[] averages;
+  private final PackedInts.Reader[] subReaders;
+
+  /** Sole constructor. */
+  public MonotonicBlockPackedReader(IndexInput in, int packedIntsVersion, int blockSize, long valueCount, boolean direct) throws IOException {
+    checkBlockSize(blockSize);
+    this.valueCount = valueCount;
+    blockShift = Integer.numberOfTrailingZeros(blockSize);
+    blockMask = blockSize - 1;
+    final int numBlocks = (int) (valueCount / blockSize) + (valueCount % blockSize == 0 ? 0 : 1);
+    if ((long) numBlocks * blockSize < valueCount) {
+      throw new IllegalArgumentException("valueCount is too large for this block size");
+    }
+    minValues = new long[numBlocks];
+    averages = new float[numBlocks];
+    subReaders = new PackedInts.Reader[numBlocks];
+    for (int i = 0; i < numBlocks; ++i) {
+      minValues[i] = in.readVLong();
+      averages[i] = Float.intBitsToFloat(in.readInt());
+      final int bitsPerValue = in.readVInt();
+      if (bitsPerValue > 64) {
+        throw new IOException("Corrupted");
+      }
+      if (bitsPerValue == 0) {
+        subReaders[i] = new PackedInts.NullReader(blockSize);
+      } else {
+        final int size = (int) Math.min(blockSize, valueCount - (long) i * blockSize);
+        if (direct) {
+          final long pointer = in.getFilePointer();
+          subReaders[i] = PackedInts.getDirectReaderNoHeader(in, PackedInts.Format.PACKED, packedIntsVersion, size, bitsPerValue);
+          in.seek(pointer + PackedInts.Format.PACKED.byteCount(packedIntsVersion, size, bitsPerValue));
+        } else {
+          subReaders[i] = PackedInts.getReaderNoHeader(in, PackedInts.Format.PACKED, packedIntsVersion, size, bitsPerValue);
+        }
+      }
+    }
+  }
+
+  /** Get value at <code>index</code>. */
+  public long get(long index) {
+    assert index >= 0 && index < valueCount;
+    final int block = (int) (index >>> blockShift);
+    final int idx = (int) (index & blockMask);
+    return minValues[block] + (long) (idx * averages[block]) + zigZagDecode(subReaders[block].get(idx));
+  }
+
+}

Property changes on: lucene/core/src/java/org/apache/lucene/util/packed/MonotonicBlockPackedReader.java
___________________________________________________________________
Added: svn:eol-style
## -0,0 +1 ##
+native
Index: lucene/core/src/java/org/apache/lucene/util/packed/AppendingLongBuffer.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/util/packed/AppendingLongBuffer.java	(revision 0)
+++ lucene/core/src/java/org/apache/lucene/util/packed/AppendingLongBuffer.java	(working copy)
@@ -0,0 +1,196 @@
+package org.apache.lucene.util.packed;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.util.Arrays;
+
+import org.apache.lucene.util.ArrayUtil;
+import org.apache.lucene.util.RamUsageEstimator;
+
+/**
+ * Utility class to buffer a list of signed longs in memory. This class only
+ * supports appending.
+ * @lucene.internal
+ */
+public class AppendingLongBuffer {
+
+  private static final int BLOCK_BITS = 10;
+  private static final int MAX_PENDING_COUNT = 1 << BLOCK_BITS;
+  private static final int BLOCK_MASK = MAX_PENDING_COUNT - 1;
+
+  private long[] minValues;
+  private PackedInts.Reader[] values;
+  private long valuesBytes;
+  private int valuesOff;
+  private long[] pending;
+  private int pendingOff;
+
+  /** Sole constructor. */
+  public AppendingLongBuffer() {
+    minValues = new long[16];
+    values = new PackedInts.Reader[16];
+    pending = new long[MAX_PENDING_COUNT];
+    valuesOff = 0;
+    pendingOff = 0;
+  }
+
+  /** Append a value to this buffer. */
+  public void add(long l) {
+    if (pendingOff == MAX_PENDING_COUNT) {
+      packPendingValues();
+    }
+    pending[pendingOff++] = l;
+  }
+  
+  /** Get a value from this buffer. 
+   *  <p>
+   *  <b>NOTE</b>: This class is not really designed for random access!
+   *  You will likely get better performance by using packed ints in another way! */
+  public long get(int index) {
+    assert index < size(); // TODO: do a better check, and throw IndexOutOfBoundsException?
+                           // This class is currently only used by the indexer.
+    int block = index >> BLOCK_BITS;
+    int element = index & BLOCK_MASK;
+    if (block == valuesOff) {
+      return pending[element];
+    } else if (values[block] == null) {
+      return minValues[block];
+    } else {
+      return minValues[block] + values[block].get(element);
+    }
+  }
+
+  private void packPendingValues() {
+    assert pendingOff == MAX_PENDING_COUNT;
+
+    // check size
+    if (values.length == valuesOff) {
+      final int newLength = ArrayUtil.oversize(valuesOff + 1, 8);
+      minValues = Arrays.copyOf(minValues, newLength);
+      values = Arrays.copyOf(values, newLength);
+    }
+
+    // compute max delta
+    long minValue = pending[0];
+    long maxValue = pending[0];
+    for (int i = 1; i < pendingOff; ++i) {
+      minValue = Math.min(minValue, pending[i]);
+      maxValue = Math.max(maxValue, pending[i]);
+    }
+    final long delta = maxValue - minValue;
+
+    minValues[valuesOff] = minValue;
+    if (delta != 0) {
+      // build a new packed reader
+      final int bitsRequired = delta < 0 ? 64 : PackedInts.bitsRequired(delta);
+      for (int i = 0; i < pendingOff; ++i) {
+        pending[i] -= minValue;
+      }
+      final PackedInts.Mutable mutable = PackedInts.getMutable(pendingOff, bitsRequired, PackedInts.COMPACT);
+      for (int i = 0; i < pendingOff; ) {
+        i += mutable.set(i, pending, i, pendingOff - i);
+      }
+      values[valuesOff] = mutable;
+      valuesBytes += mutable.ramBytesUsed();
+    }
+    ++valuesOff;
+
+    // reset pending buffer
+    pendingOff = 0;
+  }
+
+  /** Get the number of values that have been added to the buffer. */
+  public int size() {
+    return valuesOff * MAX_PENDING_COUNT + pendingOff;
+  }
+
+  /** Return an iterator over the values of this buffer. */
+  public Iterator iterator() {
+    return new Iterator();
+  }
+
+  /** A long iterator. */
+  public class Iterator {
+
+    long[] currentValues;
+    int vOff, pOff;
+
+    private Iterator() {
+      vOff = pOff = 0;
+      if (valuesOff == 0) {
+        currentValues = pending;
+      } else {
+        currentValues = new long[MAX_PENDING_COUNT];
+        fillValues();
+      }
+    }
+
+    private void fillValues() {
+      if (vOff == valuesOff) {
+        currentValues = pending;
+      } else if (values[vOff] == null) {
+        Arrays.fill(currentValues, minValues[vOff]);
+      } else {
+        for (int k = 0; k < MAX_PENDING_COUNT; ) {
+          k += values[vOff].get(k, currentValues, k, MAX_PENDING_COUNT - k);
+        }
+        for (int k = 0; k < MAX_PENDING_COUNT; ++k) {
+          currentValues[k] += minValues[vOff];
+        }
+      }
+    }
+
+    /** Whether or not there are remaining values. */
+    public boolean hasNext() {
+      return vOff < valuesOff || (vOff == valuesOff && pOff < pendingOff);
+    }
+
+    /** Return the next long in the buffer. */
+    public long next() {
+      assert hasNext();
+      long result = currentValues[pOff++];
+      if (pOff == MAX_PENDING_COUNT) {
+        vOff += 1;
+        pOff = 0;
+        if (vOff <= valuesOff) {
+          fillValues();
+        }
+      }
+      return result;
+    }
+
+  }
+
+  /**
+   * Return the number of bytes used by this instance.
+   */
+  public long ramBytesUsed() {
+    // TODO: this is called per-doc-per-norms/dv-field, can we optimize this?
+    long bytesUsed = RamUsageEstimator.alignObjectSize(
+        RamUsageEstimator.NUM_BYTES_OBJECT_HEADER
+        + 3 * RamUsageEstimator.NUM_BYTES_OBJECT_REF // the 3 arrays
+        + 2 * RamUsageEstimator.NUM_BYTES_INT) // the 2 offsets
+        + RamUsageEstimator.NUM_BYTES_LONG // valuesBytes
+        + RamUsageEstimator.sizeOf(pending)
+        + RamUsageEstimator.sizeOf(minValues)
+        + RamUsageEstimator.alignObjectSize(RamUsageEstimator.NUM_BYTES_ARRAY_HEADER + (long) RamUsageEstimator.NUM_BYTES_OBJECT_REF * values.length); // values
+
+    return bytesUsed + valuesBytes;
+  }
+
+}

Property changes on: lucene/core/src/java/org/apache/lucene/util/packed/AppendingLongBuffer.java
___________________________________________________________________
Added: svn:eol-style
## -0,0 +1 ##
+native
Index: lucene/core/src/java/org/apache/lucene/util/packed/AbstractBlockPackedWriter.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/util/packed/AbstractBlockPackedWriter.java	(revision 0)
+++ lucene/core/src/java/org/apache/lucene/util/packed/AbstractBlockPackedWriter.java	(working copy)
@@ -0,0 +1,146 @@
+package org.apache.lucene.util.packed;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Arrays;
+
+import org.apache.lucene.store.DataOutput;
+
+abstract class AbstractBlockPackedWriter {
+
+  static final int MAX_BLOCK_SIZE = 1 << (30 - 3);
+  static final int MIN_VALUE_EQUALS_0 = 1 << 0;
+  static final int BPV_SHIFT = 1;
+
+  static void checkBlockSize(int blockSize) {
+    if (blockSize <= 0 || blockSize > MAX_BLOCK_SIZE) {
+      throw new IllegalArgumentException("blockSize must be > 0 and < " + MAX_BLOCK_SIZE + ", got " + blockSize);
+    }
+    if (blockSize < 64) {
+      throw new IllegalArgumentException("blockSize must be >= 64, got " + blockSize);
+    }
+    if ((blockSize & (blockSize - 1)) != 0) {
+      throw new IllegalArgumentException("blockSize must be a power of two, got " + blockSize);
+    }
+  }
+
+  static long zigZagEncode(long n) {
+    return (n >> 63) ^ (n << 1);
+  }
+
+  // same as DataOutput.writeVLong but accepts negative values
+  static void writeVLong(DataOutput out, long i) throws IOException {
+    int k = 0;
+    while ((i & ~0x7FL) != 0L && k++ < 8) {
+      out.writeByte((byte)((i & 0x7FL) | 0x80L));
+      i >>>= 7;
+    }
+    out.writeByte((byte) i);
+  }
+
+  protected DataOutput out;
+  protected final long[] values;
+  protected byte[] blocks;
+  protected int off;
+  protected long ord;
+  protected boolean finished;
+
+  /**
+   * Sole constructor.
+   * @param blockSize the number of values of a single block, must be a multiple of <tt>64</tt>
+   */
+  public AbstractBlockPackedWriter(DataOutput out, int blockSize) {
+    checkBlockSize(blockSize);
+    reset(out);
+    values = new long[blockSize];
+  }
+
+  /** Reset this writer to wrap <code>out</code>. The block size remains unchanged. */
+  public void reset(DataOutput out) {
+    assert out != null;
+    this.out = out;
+    off = 0;
+    ord = 0L;
+    finished = false;
+  }
+
+  private void checkNotFinished() {
+    if (finished) {
+      throw new IllegalStateException("Already finished");
+    }
+  }
+
+  /** Append a new long. */
+  public void add(long l) throws IOException {
+    checkNotFinished();
+    if (off == values.length) {
+      flush();
+    }
+    values[off++] = l;
+    ++ord;
+  }
+
+  // For testing only
+  void addBlockOfZeros() throws IOException {
+    checkNotFinished();
+    if (off != 0 && off != values.length) {
+      throw new IllegalStateException("" + off);
+    }
+    if (off == values.length) {
+      flush();
+    }
+    Arrays.fill(values, 0);
+    off = values.length;
+    ord += values.length;
+  }
+
+  /** Flush all buffered data to disk. This instance is not usable anymore
+   *  after this method has been called until {@link #reset(DataOutput)} has
+   *  been called. */
+  public void finish() throws IOException {
+    checkNotFinished();
+    if (off > 0) {
+      flush();
+    }
+    finished = true;
+  }
+
+  /** Return the number of values which have been added. */
+  public long ord() {
+    return ord;
+  }
+
+  protected abstract void flush() throws IOException;
+
+  protected final void writeValues(int bitsRequired) throws IOException {
+    final PackedInts.Encoder encoder = PackedInts.getEncoder(PackedInts.Format.PACKED, PackedInts.VERSION_CURRENT, bitsRequired);
+    final int iterations = values.length / encoder.byteValueCount();
+    final int blockSize = encoder.byteBlockCount() * iterations;
+    if (blocks == null || blocks.length < blockSize) {
+      blocks = new byte[blockSize];
+    }
+    if (off < values.length) {
+      Arrays.fill(values, off, values.length, 0L);
+    }
+    encoder.encode(values, 0, blocks, 0, iterations);
+    final int blockCount = (int) PackedInts.Format.PACKED.byteCount(PackedInts.VERSION_CURRENT, off, bitsRequired);
+    out.writeBytes(blocks, blockCount);
+  }
+
+}

Property changes on: lucene/core/src/java/org/apache/lucene/util/packed/AbstractBlockPackedWriter.java
___________________________________________________________________
Added: svn:eol-style
## -0,0 +1 ##
+native
Index: lucene/core/src/java/org/apache/lucene/util/packed/MonotonicBlockPackedWriter.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/util/packed/MonotonicBlockPackedWriter.java	(revision 0)
+++ lucene/core/src/java/org/apache/lucene/util/packed/MonotonicBlockPackedWriter.java	(working copy)
@@ -0,0 +1,96 @@
+package org.apache.lucene.util.packed;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.store.DataOutput;
+
+/**
+ * A writer for large monotonically increasing sequences of positive longs.
+ * <p>
+ * The sequence is divided into fixed-size blocks and for each block, values
+ * are modeled after a linear function f: x &rarr; A &times; x + B. The block
+ * encodes deltas from the expected values computed from this function using as
+ * few bits as possible. Each block has an overhead between 6 and 14 bytes.
+ * <p>
+ * Format:
+ * <ul>
+ * <li>&lt;BLock&gt;<sup>BlockCount</sup>
+ * <li>BlockCount: &lceil; ValueCount / BlockSize &rceil;
+ * <li>Block: &lt;Header, (Ints)&gt;
+ * <li>Header: &lt;B, A, BitsPerValue&gt;
+ * <li>B: the B from f: x &rarr; A &times; x + B using a
+ *     {@link DataOutput#writeVLong(long) variable-length long}
+ * <li>A: the A from f: x &rarr; A &times; x + B encoded using
+ *     {@link Float#floatToIntBits(float)} on
+ *     {@link DataOutput#writeInt(int) 4 bytes}
+ * <li>BitsPerValue: a {@link DataOutput#writeVInt(int) variable-length int}
+ * <li>Ints: if BitsPerValue is <tt>0</tt>, then there is nothing to read and
+ *     all values perfectly match the result of the function. Otherwise, these
+ *     are the
+ *     <a href="https://developers.google.com/protocol-buffers/docs/encoding#types">zigzag-encoded</a>
+ *     {@link PackedInts packed} deltas from the expected value (computed from
+ *     the function) using exaclty BitsPerValue bits per value
+ * </ul>
+ * @see MonotonicBlockPackedReader
+ * @lucene.internal
+ */
+public final class MonotonicBlockPackedWriter extends AbstractBlockPackedWriter {
+
+  /**
+   * Sole constructor.
+   * @param blockSize the number of values of a single block, must be a power of 2
+   */
+  public MonotonicBlockPackedWriter(DataOutput out, int blockSize) {
+    super(out, blockSize);
+  }
+
+  @Override
+  public void add(long l) throws IOException {
+    assert l >= 0;
+    super.add(l);
+  }
+
+  protected void flush() throws IOException {
+    assert off > 0;
+
+    // TODO: perform a true linear regression?
+    final long min = values[0];
+    final float avg = off == 1 ? 0f : (float) (values[off - 1] - min) / (off - 1);
+
+    long maxZigZagDelta = 0;
+    for (int i = 0; i < off; ++i) {
+      values[i] = zigZagEncode(values[i] - min - (long) (avg * i));
+      maxZigZagDelta = Math.max(maxZigZagDelta, values[i]);
+    }
+
+    out.writeVLong(min);
+    out.writeInt(Float.floatToIntBits(avg));
+    if (maxZigZagDelta == 0) {
+      out.writeVInt(0);
+    } else {
+      final int bitsRequired = PackedInts.bitsRequired(maxZigZagDelta);
+      out.writeVInt(bitsRequired);
+      writeValues(bitsRequired);
+    }
+
+    off = 0;
+  }
+
+}

Property changes on: lucene/core/src/java/org/apache/lucene/util/packed/MonotonicBlockPackedWriter.java
___________________________________________________________________
Added: svn:eol-style
## -0,0 +1 ##
+native
Index: lucene/core/src/java/org/apache/lucene/util/packed/Packed8ThreeBlocks.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/util/packed/Packed8ThreeBlocks.java	(revision 1442822)
+++ lucene/core/src/java/org/apache/lucene/util/packed/Packed8ThreeBlocks.java	(working copy)
@@ -114,7 +114,11 @@
 
   @Override
   public long ramBytesUsed() {
-    return RamUsageEstimator.sizeOf(blocks);
+    return RamUsageEstimator.alignObjectSize(
+        RamUsageEstimator.NUM_BYTES_OBJECT_HEADER
+        + 2 * RamUsageEstimator.NUM_BYTES_INT     // valueCount,bitsPerValue
+        + RamUsageEstimator.NUM_BYTES_OBJECT_REF) // blocks ref
+        + RamUsageEstimator.sizeOf(blocks);
   }
 
   @Override
Index: lucene/core/src/java/org/apache/lucene/util/packed/BlockPackedReader.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/util/packed/BlockPackedReader.java	(revision 1442822)
+++ lucene/core/src/java/org/apache/lucene/util/packed/BlockPackedReader.java	(working copy)
@@ -17,225 +17,73 @@
  * limitations under the License.
  */
 
+import static org.apache.lucene.util.packed.BlockPackedReaderIterator.readVLong;
+import static org.apache.lucene.util.packed.BlockPackedReaderIterator.zigZagDecode;
 import static org.apache.lucene.util.packed.BlockPackedWriter.BPV_SHIFT;
 import static org.apache.lucene.util.packed.BlockPackedWriter.MIN_VALUE_EQUALS_0;
 import static org.apache.lucene.util.packed.BlockPackedWriter.checkBlockSize;
 
-import java.io.EOFException;
 import java.io.IOException;
-import java.util.Arrays;
 
-import org.apache.lucene.store.DataInput;
 import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.util.LongsRef;
 
 /**
- * Reader for sequences of longs written with {@link BlockPackedWriter}.
- * @see BlockPackedWriter
+ * Provides random access to a stream written with {@link BlockPackedWriter}.
  * @lucene.internal
  */
 public final class BlockPackedReader {
 
-  static long zigZagDecode(long n) {
-    return ((n >>> 1) ^ -(n & 1));
-  }
+  private final int blockShift, blockMask;
+  private final long valueCount;
+  private final long[] minValues;
+  private final PackedInts.Reader[] subReaders;
 
-  // same as DataInput.readVLong but supports negative values
-  static long readVLong(DataInput in) throws IOException {
-    byte b = in.readByte();
-    if (b >= 0) return b;
-    long i = b & 0x7FL;
-    b = in.readByte();
-    i |= (b & 0x7FL) << 7;
-    if (b >= 0) return i;
-    b = in.readByte();
-    i |= (b & 0x7FL) << 14;
-    if (b >= 0) return i;
-    b = in.readByte();
-    i |= (b & 0x7FL) << 21;
-    if (b >= 0) return i;
-    b = in.readByte();
-    i |= (b & 0x7FL) << 28;
-    if (b >= 0) return i;
-    b = in.readByte();
-    i |= (b & 0x7FL) << 35;
-    if (b >= 0) return i;
-    b = in.readByte();
-    i |= (b & 0x7FL) << 42;
-    if (b >= 0) return i;
-    b = in.readByte();
-    i |= (b & 0x7FL) << 49;
-    if (b >= 0) return i;
-    b = in.readByte();
-    i |= (b & 0xFFL) << 56;
-    return i;
-  }
-
-  DataInput in;
-  final int packedIntsVersion;
-  long valueCount;
-  final int blockSize;
-  final long[] values;
-  final LongsRef valuesRef;
-  byte[] blocks;
-  int off;
-  long ord;
-
-  /** Sole constructor.
-   * @param blockSize the number of values of a block, must be equal to the
-   *                  block size of the {@link BlockPackedWriter} which has
-   *                  been used to write the stream
-   */
-  public BlockPackedReader(DataInput in, int packedIntsVersion, int blockSize, long valueCount) {
+  /** Sole constructor. */
+  public BlockPackedReader(IndexInput in, int packedIntsVersion, int blockSize, long valueCount, boolean direct) throws IOException {
     checkBlockSize(blockSize);
-    this.packedIntsVersion = packedIntsVersion;
-    this.blockSize = blockSize;
-    this.values = new long[blockSize];
-    this.valuesRef = new LongsRef(this.values, 0, 0);
-    reset(in, valueCount);
-  }
-
-  /** Reset the current reader to wrap a stream of <code>valueCount</code>
-   * values contained in <code>in</code>. The block size remains unchanged. */
-  public void reset(DataInput in, long valueCount) {
-    this.in = in;
-    assert valueCount >= 0;
     this.valueCount = valueCount;
-    off = blockSize;
-    ord = 0;
-  }
-
-  /** Skip exactly <code>count</code> values. */
-  public void skip(long count) throws IOException {
-    assert count >= 0;
-    if (ord + count > valueCount || ord + count < 0) {
-      throw new EOFException();
+    blockShift = Integer.numberOfTrailingZeros(blockSize);
+    blockMask = blockSize - 1;
+    final int numBlocks = (int) (valueCount / blockSize) + (valueCount % blockSize == 0 ? 0 : 1);
+    if ((long) numBlocks * blockSize < valueCount) {
+      throw new IllegalArgumentException("valueCount is too large for this block size");
     }
-
-    // 1. skip buffered values
-    final int skipBuffer = (int) Math.min(count, blockSize - off);
-    off += skipBuffer;
-    ord += skipBuffer;
-    count -= skipBuffer;
-    if (count == 0L) {
-      return;
-    }
-
-    // 2. skip as many blocks as necessary
-    assert off == blockSize;
-    while (count >= blockSize) {
+    long[] minValues = null;
+    subReaders = new PackedInts.Reader[numBlocks];
+    for (int i = 0; i < numBlocks; ++i) {
       final int token = in.readByte() & 0xFF;
       final int bitsPerValue = token >>> BPV_SHIFT;
       if (bitsPerValue > 64) {
         throw new IOException("Corrupted");
       }
       if ((token & MIN_VALUE_EQUALS_0) == 0) {
-        readVLong(in);
+        if (minValues == null) {
+          minValues = new long[numBlocks];
+        }
+        minValues[i] = zigZagDecode(1L + readVLong(in));
       }
-      final long blockBytes = PackedInts.Format.PACKED.byteCount(packedIntsVersion, blockSize, bitsPerValue);
-      skipBytes(blockBytes);
-      ord += blockSize;
-      count -= blockSize;
-    }
-    if (count == 0L) {
-      return;
-    }
-
-    // 3. skip last values
-    assert count < blockSize;
-    refill();
-    ord += count;
-    off += count;
-  }
-
-  private void skipBytes(long count) throws IOException {
-    if (in instanceof IndexInput) {
-      final IndexInput iin = (IndexInput) in;
-      iin.seek(iin.getFilePointer() + count);
-    } else {
-      if (blocks == null) {
-        blocks = new byte[blockSize];
-      }
-      long skipped = 0;
-      while (skipped < count) {
-        final int toSkip = (int) Math.min(blocks.length, count - skipped);
-        in.readBytes(blocks, 0, toSkip);
-        skipped += toSkip;
-      }
-    }
-  }
-
-  /** Read the next value. */
-  public long next() throws IOException {
-    if (ord == valueCount) {
-      throw new EOFException();
-    }
-    if (off == blockSize) {
-      refill();
-    }
-    final long value = values[off++];
-    ++ord;
-    return value;
-  }
-
-  /** Read between <tt>1</tt> and <code>count</code> values. */
-  public LongsRef next(int count) throws IOException {
-    assert count > 0;
-    if (ord == valueCount) {
-      throw new EOFException();
-    }
-    if (off == blockSize) {
-      refill();
-    }
-
-    count = Math.min(count, blockSize - off);
-    count = (int) Math.min(count, valueCount - ord);
-
-    valuesRef.offset = off;
-    valuesRef.length = count;
-    off += count;
-    ord += count;
-    return valuesRef;
-  }
-
-  private void refill() throws IOException {
-    final int token = in.readByte() & 0xFF;
-    final boolean minEquals0 = (token & MIN_VALUE_EQUALS_0) != 0;
-    final int bitsPerValue = token >>> BPV_SHIFT;
-    if (bitsPerValue > 64) {
-      throw new IOException("Corrupted");
-    }
-    final long minValue = minEquals0 ? 0L : zigZagDecode(1L + readVLong(in));
-    assert minEquals0 || minValue != 0;
-
-    if (bitsPerValue == 0) {
-      Arrays.fill(values, minValue);
-    } else {
-      final PackedInts.Decoder decoder = PackedInts.getDecoder(PackedInts.Format.PACKED, packedIntsVersion, bitsPerValue);
-      final int iterations = blockSize / decoder.byteValueCount();
-      final int blocksSize = iterations * decoder.byteBlockCount();
-      if (blocks == null || blocks.length < blocksSize) {
-        blocks = new byte[blocksSize];
-      }
-
-      final int valueCount = (int) Math.min(this.valueCount - ord, blockSize);
-      final int blocksCount = (int) PackedInts.Format.PACKED.byteCount(packedIntsVersion, valueCount, bitsPerValue);
-      in.readBytes(blocks, 0, blocksCount);
-
-      decoder.decode(blocks, 0, values, 0, iterations);
-
-      if (minValue != 0) {
-        for (int i = 0; i < valueCount; ++i) {
-          values[i] += minValue;
+      if (bitsPerValue == 0) {
+        subReaders[i] = new PackedInts.NullReader(blockSize);
+      } else {
+        final int size = (int) Math.min(blockSize, valueCount - (long) i * blockSize);
+        if (direct) {
+          final long pointer = in.getFilePointer();
+          subReaders[i] = PackedInts.getDirectReaderNoHeader(in, PackedInts.Format.PACKED, packedIntsVersion, size, bitsPerValue);
+          in.seek(pointer + PackedInts.Format.PACKED.byteCount(packedIntsVersion, size, bitsPerValue));
+        } else {
+          subReaders[i] = PackedInts.getReaderNoHeader(in, PackedInts.Format.PACKED, packedIntsVersion, size, bitsPerValue);
         }
       }
     }
-    off = 0;
+    this.minValues = minValues;
   }
 
-  /** Return the offset of the next value to read. */
-  public long ord() {
-    return ord;
+  /** Get value at <code>index</code>. */
+  public long get(long index) {
+    assert index >= 0 && index < valueCount;
+    final int block = (int) (index >>> blockShift);
+    final int idx = (int) (index & blockMask);
+    return (minValues == null ? 0 : minValues[block]) + subReaders[block].get(idx);
   }
 
 }
Index: lucene/core/src/java/org/apache/lucene/util/packed/BlockPackedWriter.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/util/packed/BlockPackedWriter.java	(revision 1442822)
+++ lucene/core/src/java/org/apache/lucene/util/packed/BlockPackedWriter.java	(working copy)
@@ -18,7 +18,6 @@
  */
 
 import java.io.IOException;
-import java.util.Arrays;
 
 import org.apache.lucene.store.DataOutput;
 
@@ -30,92 +29,43 @@
  * using as few bits as possible. Memory usage of this class is proportional to
  * the block size. Each block has an overhead between 1 and 10 bytes to store
  * the minimum value and the number of bits per value of the block.
+ * <p>
+ * Format:
+ * <ul>
+ * <li>&lt;BLock&gt;<sup>BlockCount</sup>
+ * <li>BlockCount: &lceil; ValueCount / BlockSize &rceil;
+ * <li>Block: &lt;Header, (Ints)&gt;
+ * <li>Header: &lt;Token, (MinValue)&gt;
+ * <li>Token: a {@link DataOutput#writeByte(byte) byte}, first 7 bits are the
+ *     number of bits per value (<tt>bitsPerValue</tt>). If the 8th bit is 1,
+ *     then MinValue (see next) is <tt>0</tt>, otherwise MinValue and needs to
+ *     be decoded
+ * <li>MinValue: a
+ *     <a href="https://developers.google.com/protocol-buffers/docs/encoding#types">zigzag-encoded</a>
+ *     {@link DataOutput#writeVLong(long) variable-length long} whose value
+ *     should be added to every int from the block to restore the original
+ *     values
+ * <li>Ints: If the number of bits per value is <tt>0</tt>, then there is
+ *     nothing to decode and all ints are equal to MinValue. Otherwise: BlockSize
+ *     {@link PackedInts packed ints} encoded on exactly <tt>bitsPerValue</tt>
+ *     bits per value. They are the subtraction of the original values and
+ *     MinValue
+ * </ul>
+ * @see BlockPackedReaderIterator
  * @see BlockPackedReader
  * @lucene.internal
  */
-public final class BlockPackedWriter {
+public final class BlockPackedWriter extends AbstractBlockPackedWriter {
 
-  static final int MAX_BLOCK_SIZE = 1 << (30 - 3);
-  static final int MIN_VALUE_EQUALS_0 = 1 << 0;
-  static final int BPV_SHIFT = 1;
-
-  static void checkBlockSize(int blockSize) {
-    if (blockSize <= 0 || blockSize > MAX_BLOCK_SIZE) {
-      throw new IllegalArgumentException("blockSize must be > 0 and < " + MAX_BLOCK_SIZE + ", got " + blockSize);
-    }
-    if (blockSize % 64 != 0) {
-      throw new IllegalArgumentException("blockSize must be a multiple of 64, got " + blockSize);
-    }
-  }
-
-  static long zigZagEncode(long n) {
-    return (n >> 63) ^ (n << 1);
-  }
-
-  // same as DataOutput.writeVLong but accepts negative values
-  static void writeVLong(DataOutput out, long i) throws IOException {
-    int k = 0;
-    while ((i & ~0x7FL) != 0L && k++ < 8) {
-      out.writeByte((byte)((i & 0x7FL) | 0x80L));
-      i >>>= 7;
-    }
-    out.writeByte((byte) i);
-  }
-
-  DataOutput out;
-  final long[] values;
-  byte[] blocks;
-  int off;
-  long ord;
-  boolean finished;
-
   /**
    * Sole constructor.
-   * @param blockSize the number of values of a single block, must be a multiple of <tt>64</tt>
+   * @param blockSize the number of values of a single block, must be a power of 2
    */
   public BlockPackedWriter(DataOutput out, int blockSize) {
-    checkBlockSize(blockSize);
-    reset(out);
-    values = new long[blockSize];
+    super(out, blockSize);
   }
 
-  /** Reset this writer to wrap <code>out</code>. The block size remains unchanged. */
-  public void reset(DataOutput out) {
-    assert out != null;
-    this.out = out;
-    off = 0;
-    ord = 0L;
-    finished = false;
-  }
-
-  private void checkNotFinished() {
-    if (finished) {
-      throw new IllegalStateException("Already finished");
-    }
-  }
-
-  /** Append a new long. */
-  public void add(long l) throws IOException {
-    checkNotFinished();
-    if (off == values.length) {
-      flush();
-    }
-    values[off++] = l;
-    ++ord;
-  }
-
-  /** Flush all buffered data to disk. This instance is not usable anymore
-   *  after this method has been called until {@link #reset(DataOutput)} has
-   *  been called. */
-  public void finish() throws IOException {
-    checkNotFinished();
-    if (off > 0) {
-      flush();
-    }
-    finished = true;
-  }
-
-  private void flush() throws IOException {
+  protected void flush() throws IOException {
     assert off > 0;
     long min = Long.MAX_VALUE, max = Long.MIN_VALUE;
     for (int i = 0; i < off; ++i) {
@@ -146,26 +96,10 @@
           values[i] -= min;
         }
       }
-      final PackedInts.Encoder encoder = PackedInts.getEncoder(PackedInts.Format.PACKED, PackedInts.VERSION_CURRENT, bitsRequired);
-      final int iterations = values.length / encoder.byteValueCount();
-      final int blockSize = encoder.byteBlockCount() * iterations;
-      if (blocks == null || blocks.length < blockSize) {
-        blocks = new byte[blockSize];
-      }
-      if (off < values.length) {
-        Arrays.fill(values, off, values.length, 0L);
-      }
-      encoder.encode(values, 0, blocks, 0, iterations);
-      final int blockCount = (int) PackedInts.Format.PACKED.byteCount(PackedInts.VERSION_CURRENT, off, bitsRequired);
-      out.writeBytes(blocks, blockCount);
+      writeValues(bitsRequired);
     }
 
     off = 0;
   }
 
-  /** Return the number of values which have been added. */
-  public long ord() {
-    return ord;
-  }
-
 }
Index: lucene/core/src/java/org/apache/lucene/util/packed/DirectPackedReader.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/util/packed/DirectPackedReader.java	(revision 1442822)
+++ lucene/core/src/java/org/apache/lucene/util/packed/DirectPackedReader.java	(working copy)
@@ -26,20 +26,10 @@
   private final IndexInput in;
   private final long startPointer;
 
-  // masks[n-1] masks for bottom n bits
-  private final long[] masks;
-
   public DirectPackedReader(int bitsPerValue, int valueCount, IndexInput in) {
     super(valueCount, bitsPerValue);
     this.in = in;
 
-    long v = 1;
-    masks = new long[bitsPerValue];
-    for (int i = 0; i < bitsPerValue; i++) {
-      v *= 2;
-      masks[i] = v - 1;
-    }
-
     startPointer = in.getFilePointer();
   }
 
Index: lucene/core/src/java/org/apache/lucene/util/packed/Packed16ThreeBlocks.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/util/packed/Packed16ThreeBlocks.java	(revision 1442822)
+++ lucene/core/src/java/org/apache/lucene/util/packed/Packed16ThreeBlocks.java	(working copy)
@@ -116,7 +116,11 @@
 
   @Override
   public long ramBytesUsed() {
-    return RamUsageEstimator.sizeOf(blocks);
+    return RamUsageEstimator.alignObjectSize(
+        RamUsageEstimator.NUM_BYTES_OBJECT_HEADER
+        + 2 * RamUsageEstimator.NUM_BYTES_INT     // valueCount,bitsPerValue
+        + RamUsageEstimator.NUM_BYTES_OBJECT_REF) // blocks ref
+        + RamUsageEstimator.sizeOf(blocks);
   }
 
   @Override
Index: lucene/core/src/java/org/apache/lucene/util/IntsRef.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/util/IntsRef.java	(revision 1442822)
+++ lucene/core/src/java/org/apache/lucene/util/IntsRef.java	(working copy)
@@ -50,13 +50,10 @@
    * ints should not be null.
    */
   public IntsRef(int[] ints, int offset, int length) {
-    assert ints != null;
-    assert offset >= 0;
-    assert length >= 0;
-    assert ints.length >= offset + length;
     this.ints = ints;
     this.offset = offset;
     this.length = length;
+    assert isValid();
   }
 
   @Override
@@ -176,4 +173,33 @@
     clone.copyInts(other);
     return clone;
   }
+  
+  /** 
+   * Performs internal consistency checks.
+   * Always returns true (or throws IllegalStateException) 
+   */
+  public boolean isValid() {
+    if (ints == null) {
+      throw new IllegalStateException("ints is null");
+    }
+    if (length < 0) {
+      throw new IllegalStateException("length is negative: " + length);
+    }
+    if (length > ints.length) {
+      throw new IllegalStateException("length is out of bounds: " + length + ",ints.length=" + ints.length);
+    }
+    if (offset < 0) {
+      throw new IllegalStateException("offset is negative: " + offset);
+    }
+    if (offset > ints.length) {
+      throw new IllegalStateException("offset out of bounds: " + offset + ",ints.length=" + ints.length);
+    }
+    if (offset + length < 0) {
+      throw new IllegalStateException("offset+length is negative: offset=" + offset + ",length=" + length);
+    }
+    if (offset + length > ints.length) {
+      throw new IllegalStateException("offset+length out of bounds: offset=" + offset + ",length=" + length + ",ints.length=" + ints.length);
+    }
+    return true;
+  }
 }
Index: lucene/core/src/java/org/apache/lucene/util/fst/Util.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/util/fst/Util.java	(revision 1442822)
+++ lucene/core/src/java/org/apache/lucene/util/fst/Util.java	(working copy)
@@ -109,7 +109,15 @@
     FST.Arc<Long> scratchArc = new FST.Arc<Long>();
 
     final IntsRef result = new IntsRef();
-
+    
+    return getByOutput(fst, targetOutput, in, arc, scratchArc, result);
+  }
+    
+  /** 
+   * Expert: like {@link Util#getByOutput(FST, long)} except reusing 
+   * BytesReader, initial and scratch Arc, and result.
+   */
+  public static IntsRef getByOutput(FST<Long> fst, long targetOutput, BytesReader in, Arc<Long> arc, Arc<Long> scratchArc, IntsRef result) throws IOException {
     long output = arc.output;
     int upto = 0;
 
Index: lucene/core/src/java/org/apache/lucene/util/ByteBlockPool.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/util/ByteBlockPool.java	(revision 1442822)
+++ lucene/core/src/java/org/apache/lucene/util/ByteBlockPool.java	(working copy)
@@ -16,12 +16,10 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
-import java.io.IOException;
+
 import java.util.Arrays;
 import java.util.List;
 
-import org.apache.lucene.store.DataOutput;
-
 import static org.apache.lucene.util.RamUsageEstimator.NUM_BYTES_OBJECT_REF;
 
 /** 
@@ -255,8 +253,9 @@
     final int newSize = LEVEL_SIZE_ARRAY[newLevel];
 
     // Maybe allocate another block
-    if (byteUpto > BYTE_BLOCK_SIZE-newSize)
+    if (byteUpto > BYTE_BLOCK_SIZE-newSize) {
       nextBuffer();
+    }
 
     final int newUpto = byteUpto;
     final int offset = newUpto + byteOffset;
@@ -282,7 +281,7 @@
 
   // Fill in a BytesRef from term's length & bytes encoded in
   // byte block
-  public final BytesRef setBytesRef(BytesRef term, int textStart) {
+  public void setBytesRef(BytesRef term, int textStart) {
     final byte[] bytes = term.bytes = buffers[textStart >> BYTE_BLOCK_SHIFT];
     int pos = textStart & BYTE_BLOCK_MASK;
     if ((bytes[pos] & 0x80) == 0) {
@@ -295,27 +294,17 @@
       term.offset = pos+2;
     }
     assert term.length >= 0;
-    return term;
   }
-  /**
-   * Dereferences the byte block according to {@link BytesRef} offset. The offset 
-   * is interpreted as the absolute offset into the {@link ByteBlockPool}.
-   */
-  public final BytesRef deref(BytesRef bytes) {
-    final int offset = bytes.offset;
-    byte[] buffer = buffers[offset >> BYTE_BLOCK_SHIFT];
-    int pos = offset & BYTE_BLOCK_MASK;
-    bytes.bytes = buffer;
-    bytes.offset = pos;
-    return bytes;
-  }
   
   /**
-   * Copies the given {@link BytesRef} at the current positions (
-   * {@link #byteUpto} across buffer boundaries
+   * Appends the bytes in the provided {@link BytesRef} at
+   * the current position.
    */
-  public final void copy(final BytesRef bytes) {
+  public void append(final BytesRef bytes) {
     int length = bytes.length;
+    if (length == 0) {
+      return;
+    }
     int offset = bytes.offset;
     int overflow = (length + byteUpto) - BYTE_BLOCK_SIZE;
     do {
@@ -325,9 +314,11 @@
         break;
       } else {
         final int bytesToCopy = length-overflow;
-        System.arraycopy(bytes.bytes, offset, buffer, byteUpto, bytesToCopy);
-        offset += bytesToCopy;
-        length -= bytesToCopy;
+        if (bytesToCopy > 0) {
+          System.arraycopy(bytes.bytes, offset, buffer, byteUpto, bytesToCopy);
+          offset += bytesToCopy;
+          length -= bytesToCopy;
+        }
         nextBuffer();
         overflow = overflow - BYTE_BLOCK_SIZE;
       }
@@ -335,48 +326,34 @@
   }
   
   /**
-   * Copies bytes from the pool starting at the given offset with the given  
-   * length into the given {@link BytesRef} at offset <tt>0</tt> and returns it.
+   * Reads bytes bytes out of the pool starting at the given offset with the given  
+   * length into the given byte array at offset <tt>off</tt>.
    * <p>Note: this method allows to copy across block boundaries.</p>
    */
-  public final BytesRef copyFrom(final BytesRef bytes, final int offset, final int length) {
-    bytes.offset = 0;
-    bytes.grow(length);
-    bytes.length = length;
-    int bufferIndex = offset >> BYTE_BLOCK_SHIFT;
+  public void readBytes(final long offset, final byte bytes[], final int off, final int length) {
+    if (length == 0) {
+      return;
+    }
+    int bytesOffset = off;
+    int bytesLength = length;
+    int bufferIndex = (int) (offset >> BYTE_BLOCK_SHIFT);
     byte[] buffer = buffers[bufferIndex];
-    int pos = offset & BYTE_BLOCK_MASK;
+    int pos = (int) (offset & BYTE_BLOCK_MASK);
     int overflow = (pos + length) - BYTE_BLOCK_SIZE;
     do {
       if (overflow <= 0) {
-        System.arraycopy(buffer, pos, bytes.bytes, bytes.offset, bytes.length);
-        bytes.length = length;
-        bytes.offset = 0;
+        System.arraycopy(buffer, pos, bytes, bytesOffset, bytesLength);
         break;
       } else {
         final int bytesToCopy = length - overflow;
-        System.arraycopy(buffer, pos, bytes.bytes, bytes.offset, bytesToCopy);
+        System.arraycopy(buffer, pos, bytes, bytesOffset, bytesToCopy);
         pos = 0;
-        bytes.length -= bytesToCopy;
-        bytes.offset += bytesToCopy;
+        bytesLength -= bytesToCopy;
+        bytesOffset += bytesToCopy;
         buffer = buffers[++bufferIndex];
         overflow = overflow - BYTE_BLOCK_SIZE;
       }
     } while (true);
-    return bytes;
   }
-  
-  /**
-   * Writes the pools content to the given {@link DataOutput}
-   */
-  public final void writePool(final DataOutput out) throws IOException {
-    int bytesOffset = byteOffset;
-    int block = 0;
-    while (bytesOffset > 0) {
-      out.writeBytes(buffers[block++], BYTE_BLOCK_SIZE);
-      bytesOffset -= BYTE_BLOCK_SIZE;
-    }
-    out.writeBytes(buffers[block], byteUpto);
-  }
 }
 
Index: lucene/core/src/java/org/apache/lucene/util/PagedBytes.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/util/PagedBytes.java	(revision 1442822)
+++ lucene/core/src/java/org/apache/lucene/util/PagedBytes.java	(working copy)
@@ -21,8 +21,6 @@
 import java.util.ArrayList;
 import java.util.List;
 
-import org.apache.lucene.store.DataInput;
-import org.apache.lucene.store.DataOutput;
 import org.apache.lucene.store.IndexInput;
 
 /** Represents a logical byte[] as a series of pages.  You
@@ -32,6 +30,8 @@
  *
  * @lucene.internal
  **/
+// TODO: refactor this, byteblockpool, fst.bytestore, and any
+// other "shift/mask big arrays". there are too many of these classes!
 public final class PagedBytes {
   private final List<byte[]> blocks = new ArrayList<byte[]>();
   private final List<Integer> blockEnd = new ArrayList<Integer>();
@@ -56,7 +56,7 @@
     private final int blockMask;
     private final int blockSize;
 
-    public Reader(PagedBytes pagedBytes) {
+    private Reader(PagedBytes pagedBytes) {
       blocks = new byte[pagedBytes.blocks.size()][];
       for(int i=0;i<blocks.length;i++) {
         blocks[i] = pagedBytes.blocks.get(i);
@@ -79,7 +79,7 @@
      * </p>
      * @lucene.internal 
      **/
-    public BytesRef fillSlice(BytesRef b, long start, int length) {
+    public void fillSlice(BytesRef b, long start, int length) {
       assert length >= 0: "length=" + length;
       assert length <= blockSize+1;
       final int index = (int) (start >> blockBits);
@@ -96,7 +96,6 @@
         System.arraycopy(blocks[index], offset, b.bytes, 0, blockSize-offset);
         System.arraycopy(blocks[1+index], 0, b.bytes, blockSize-offset, length-(blockSize-offset));
       }
-      return b;
     }
     
     /**
@@ -106,11 +105,10 @@
      * borders.
      * </p>
      * 
-     * @return the given {@link BytesRef}
-     * 
      * @lucene.internal
      **/
-    public BytesRef fill(BytesRef b, long start) {
+    // TODO: this really needs to be refactored into fieldcacheimpl
+    public void fill(BytesRef b, long start) {
       final int index = (int) (start >> blockBits);
       final int offset = (int) (start & blockMask);
       final byte[] block = b.bytes = blocks[index];
@@ -123,134 +121,7 @@
         b.offset = offset+2;
         assert b.length > 0;
       }
-      return b;
     }
-
-    /**
-     * Reads length as 1 or 2 byte vInt prefix, starting at <i>start</i>. *
-     * <p>
-     * <b>Note:</b> this method does not support slices spanning across block
-     * borders.
-     * </p>
-     * 
-     * @return the internal block number of the slice.
-     * @lucene.internal
-     **/
-    public int fillAndGetIndex(BytesRef b, long start) {
-      final int index = (int) (start >> blockBits);
-      final int offset = (int) (start & blockMask);
-      final byte[] block = b.bytes = blocks[index];
-
-      if ((block[offset] & 128) == 0) {
-        b.length = block[offset];
-        b.offset = offset+1;
-      } else {
-        b.length = ((block[offset] & 0x7f) << 8) | (block[1+offset] & 0xff);
-        b.offset = offset+2;
-        assert b.length > 0;
-      }
-      return index;
-    }
-
-    /**
-     * Reads length as 1 or 2 byte vInt prefix, starting at <i>start</i> and
-     * returns the start offset of the next part, suitable as start parameter on
-     * next call to sequentially read all {@link BytesRef}.
-     * 
-     * <p>
-     * <b>Note:</b> this method does not support slices spanning across block
-     * borders.
-     * </p>
-     * 
-     * @return the start offset of the next part, suitable as start parameter on
-     *         next call to sequentially read all {@link BytesRef}.
-     * @lucene.internal
-     **/
-    public long fillAndGetStart(BytesRef b, long start) {
-      final int index = (int) (start >> blockBits);
-      final int offset = (int) (start & blockMask);
-      final byte[] block = b.bytes = blocks[index];
-
-      if ((block[offset] & 128) == 0) {
-        b.length = block[offset];
-        b.offset = offset+1;
-        start += 1L + b.length;
-      } else {
-        b.length = ((block[offset] & 0x7f) << 8) | (block[1+offset] & 0xff);
-        b.offset = offset+2;
-        start += 2L + b.length;
-        assert b.length > 0;
-      }
-      return start;
-    }
-    
-  
-    /**
-     * Gets a slice out of {@link PagedBytes} starting at <i>start</i>, the
-     * length is read as 1 or 2 byte vInt prefix. Iff the slice spans across a
-     * block border this method will allocate sufficient resources and copy the
-     * paged data.
-     * <p>
-     * Slices spanning more than one block are not supported.
-     * </p>
-     * 
-     * @lucene.internal
-     **/
-    public BytesRef fillSliceWithPrefix(BytesRef b, long start) {
-      int index = (int) (start >> blockBits);
-      int offset = (int) (start & blockMask);
-      byte[] block = blocks[index];
-      final int length;
-      assert offset <= block.length-1;
-      if ((block[offset] & 128) == 0) {
-        length = block[offset];
-        offset = offset+1;
-      } else {
-        if (offset==block.length-1) {
-          final byte[] nextBlock = blocks[++index];
-          length = ((block[offset] & 0x7f) << 8) | (nextBlock[0] & 0xff);
-          offset = 1;
-          block = nextBlock;
-          assert length > 0; 
-        } else {
-          assert offset < block.length-1;
-          length = ((block[offset] & 0x7f) << 8) | (block[1+offset] & 0xff);
-          offset = offset+2;
-          assert length > 0;
-        }
-      }
-      assert length >= 0: "length=" + length;
-      b.length = length;
-
-      // NOTE: even though copyUsingLengthPrefix always
-      // allocs a new block if the byte[] to be added won't
-      // fit in current block,
-      // VarDerefBytesImpl.finishInternal does its own
-      // prefix + byte[] writing which can span two blocks,
-      // so we support that here on decode:
-      if (blockSize - offset >= length) {
-        // Within block
-        b.offset = offset;
-        b.bytes = blocks[index];
-      } else {
-        // Split
-        b.bytes = new byte[length];
-        b.offset = 0;
-        System.arraycopy(blocks[index], offset, b.bytes, 0, blockSize-offset);
-        System.arraycopy(blocks[1+index], 0, b.bytes, blockSize-offset, length-(blockSize-offset));
-      }
-      return b;
-    }
-
-    /** @lucene.internal */
-    public byte[][] getBlocks() {
-      return blocks;
-    }
-
-    /** @lucene.internal */
-    public int[] getBlockEnds() {
-      return blockEnds;
-    }
   }
 
   /** 1&lt;&lt;blockBits must be bigger than biggest single
@@ -288,34 +159,6 @@
     }
   }
 
-  /** Copy BytesRef in */
-  public void copy(BytesRef bytes) {
-    int byteCount = bytes.length;
-    int bytesUpto = bytes.offset;
-    while (byteCount > 0) {
-      int left = blockSize - upto;
-      if (left == 0) {
-        if (currentBlock != null) {
-          blocks.add(currentBlock);
-          blockEnd.add(upto);          
-        }
-        currentBlock = new byte[blockSize];
-        upto = 0;
-        left = blockSize;
-      }
-      if (left < byteCount) {
-        System.arraycopy(bytes.bytes, bytesUpto, currentBlock, upto, left);
-        upto = blockSize;
-        byteCount -= left;
-        bytesUpto += left;
-      } else {
-        System.arraycopy(bytes.bytes, bytesUpto, currentBlock, upto, byteCount);
-        upto += byteCount;
-        break;
-      }
-    }
-  }
-
   /** Copy BytesRef in, setting BytesRef out to the result.
    * Do not use this if you will use freeze(true).
    * This only supports bytes.length <= blockSize */
@@ -362,7 +205,7 @@
     blockEnd.add(upto); 
     frozen = true;
     currentBlock = null;
-    return new Reader(this);
+    return new PagedBytes.Reader(this);
   }
 
   public long getPointer() {
@@ -375,6 +218,7 @@
 
   /** Copy bytes in, writing the length as a 1 or 2 byte
    *  vInt prefix. */
+  // TODO: this really needs to be refactored into fieldcacheimpl!
   public long copyUsingLengthPrefix(BytesRef bytes) {
     if (bytes.length >= 32768) {
       throw new IllegalArgumentException("max length is 32767 (got " + bytes.length + ")");
@@ -405,148 +249,4 @@
 
     return pointer;
   }
-
-  public final class PagedBytesDataInput extends DataInput {
-    private int currentBlockIndex;
-    private int currentBlockUpto;
-    private byte[] currentBlock;
-
-    PagedBytesDataInput() {
-      currentBlock = blocks.get(0);
-    }
-
-    @Override
-    public PagedBytesDataInput clone() {
-      PagedBytesDataInput clone = getDataInput();
-      clone.setPosition(getPosition());
-      return clone;
-    }
-
-    /** Returns the current byte position. */
-    public long getPosition() {
-      return (long) currentBlockIndex * blockSize + currentBlockUpto;
-    }
-  
-    /** Seek to a position previously obtained from
-     *  {@link #getPosition}. */
-    public void setPosition(long pos) {
-      currentBlockIndex = (int) (pos >> blockBits);
-      currentBlock = blocks.get(currentBlockIndex);
-      currentBlockUpto = (int) (pos & blockMask);
-    }
-
-    @Override
-    public byte readByte() {
-      if (currentBlockUpto == blockSize) {
-        nextBlock();
-      }
-      return currentBlock[currentBlockUpto++];
-    }
-
-    @Override
-    public void readBytes(byte[] b, int offset, int len) {
-      assert b.length >= offset + len;
-      final int offsetEnd = offset + len;
-      while (true) {
-        final int blockLeft = blockSize - currentBlockUpto;
-        final int left = offsetEnd - offset;
-        if (blockLeft < left) {
-          System.arraycopy(currentBlock, currentBlockUpto,
-                           b, offset,
-                           blockLeft);
-          nextBlock();
-          offset += blockLeft;
-        } else {
-          // Last block
-          System.arraycopy(currentBlock, currentBlockUpto,
-                           b, offset,
-                           left);
-          currentBlockUpto += left;
-          break;
-        }
-      }
-    }
-
-    private void nextBlock() {
-      currentBlockIndex++;
-      currentBlockUpto = 0;
-      currentBlock = blocks.get(currentBlockIndex);
-    }
-  }
-
-  public final class PagedBytesDataOutput extends DataOutput {
-    @Override
-    public void writeByte(byte b) {
-      if (upto == blockSize) {
-        if (currentBlock != null) {
-          blocks.add(currentBlock);
-          blockEnd.add(upto);
-        }
-        currentBlock = new byte[blockSize];
-        upto = 0;
-      }
-      currentBlock[upto++] = b;
-    }
-
-    @Override
-    public void writeBytes(byte[] b, int offset, int length) {
-      assert b.length >= offset + length;
-      if (length == 0) {
-        return;
-      }
-
-      if (upto == blockSize) {
-        if (currentBlock != null) {
-          blocks.add(currentBlock);
-          blockEnd.add(upto);
-        }
-        currentBlock = new byte[blockSize];
-        upto = 0;
-      }
-          
-      final int offsetEnd = offset + length;
-      while(true) {
-        final int left = offsetEnd - offset;
-        final int blockLeft = blockSize - upto;
-        if (blockLeft < left) {
-          System.arraycopy(b, offset, currentBlock, upto, blockLeft);
-          blocks.add(currentBlock);
-          blockEnd.add(blockSize);
-          currentBlock = new byte[blockSize];
-          upto = 0;
-          offset += blockLeft;
-        } else {
-          // Last block
-          System.arraycopy(b, offset, currentBlock, upto, left);
-          upto += left;
-          break;
-        }
-      }
-    }
-
-    /** Return the current byte position. */
-    public long getPosition() {
-      return getPointer();
-    }
-  }
-
-  /** Returns a DataInput to read values from this
-   *  PagedBytes instance. */
-  public PagedBytesDataInput getDataInput() {
-    if (!frozen) {
-      throw new IllegalStateException("must call freeze() before getDataInput");
-    }
-    return new PagedBytesDataInput();
-  }
-
-  /** Returns a DataOutput that you may use to write into
-   *  this PagedBytes instance.  If you do this, you should
-   *  not call the other writing methods (eg, copy);
-   *  results are undefined. */
-  public PagedBytesDataOutput getDataOutput() {
-    if (frozen) {
-      throw new IllegalStateException("cannot get DataOutput after freeze()");
-    }
-    return new PagedBytesDataOutput();
-  }
 }
Index: lucene/core/src/java/org/apache/lucene/util/CharsRef.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/util/CharsRef.java	(revision 1442822)
+++ lucene/core/src/java/org/apache/lucene/util/CharsRef.java	(working copy)
@@ -55,13 +55,10 @@
    * length
    */
   public CharsRef(char[] chars, int offset, int length) {
-    assert chars != null;
-    assert offset >= 0;
-    assert length >= 0;
-    assert chars.length >= offset + length;
     this.chars = chars;
     this.offset = offset;
     this.length = length;
+    assert isValid();
   }
 
   /**
@@ -292,4 +289,33 @@
     clone.copyChars(other);
     return clone;
   }
+  
+  /** 
+   * Performs internal consistency checks.
+   * Always returns true (or throws IllegalStateException) 
+   */
+  public boolean isValid() {
+    if (chars == null) {
+      throw new IllegalStateException("chars is null");
+    }
+    if (length < 0) {
+      throw new IllegalStateException("length is negative: " + length);
+    }
+    if (length > chars.length) {
+      throw new IllegalStateException("length is out of bounds: " + length + ",chars.length=" + chars.length);
+    }
+    if (offset < 0) {
+      throw new IllegalStateException("offset is negative: " + offset);
+    }
+    if (offset > chars.length) {
+      throw new IllegalStateException("offset out of bounds: " + offset + ",chars.length=" + chars.length);
+    }
+    if (offset + length < 0) {
+      throw new IllegalStateException("offset+length is negative: offset=" + offset + ",length=" + length);
+    }
+    if (offset + length > chars.length) {
+      throw new IllegalStateException("offset+length out of bounds: offset=" + offset + ",length=" + length + ",chars.length=" + chars.length);
+    }
+    return true;
+  }
 }
\ No newline at end of file
Index: lucene/core/src/java/org/apache/lucene/util/LongsRef.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/util/LongsRef.java	(revision 1442822)
+++ lucene/core/src/java/org/apache/lucene/util/LongsRef.java	(working copy)
@@ -49,13 +49,10 @@
   /** This instance will directly reference longs w/o making a copy.
    * longs should not be null */
   public LongsRef(long[] longs, int offset, int length) {
-    assert longs != null;
-    assert offset >= 0;
-    assert length >= 0;
-    assert longs.length >= offset + length;
     this.longs = longs;
     this.offset = offset;
     this.length = length;
+    assert isValid();
   }
 
   @Override
@@ -175,4 +172,33 @@
     clone.copyLongs(other);
     return clone;
   }
+  
+  /** 
+   * Performs internal consistency checks.
+   * Always returns true (or throws IllegalStateException) 
+   */
+  public boolean isValid() {
+    if (longs == null) {
+      throw new IllegalStateException("longs is null");
+    }
+    if (length < 0) {
+      throw new IllegalStateException("length is negative: " + length);
+    }
+    if (length > longs.length) {
+      throw new IllegalStateException("length is out of bounds: " + length + ",longs.length=" + longs.length);
+    }
+    if (offset < 0) {
+      throw new IllegalStateException("offset is negative: " + offset);
+    }
+    if (offset > longs.length) {
+      throw new IllegalStateException("offset out of bounds: " + offset + ",longs.length=" + longs.length);
+    }
+    if (offset + length < 0) {
+      throw new IllegalStateException("offset+length is negative: offset=" + offset + ",length=" + length);
+    }
+    if (offset + length > longs.length) {
+      throw new IllegalStateException("offset+length out of bounds: offset=" + offset + ",length=" + length + ",longs.length=" + longs.length);
+    }
+    return true;
+  }
 }
Index: lucene/core/src/java/org/apache/lucene/util/BytesRef.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/util/BytesRef.java	(revision 1442822)
+++ lucene/core/src/java/org/apache/lucene/util/BytesRef.java	(working copy)
@@ -52,13 +52,10 @@
    * bytes should not be null.
    */
   public BytesRef(byte[] bytes, int offset, int length) {
-    assert bytes != null;
-    assert offset >= 0;
-    assert length >= 0;
-    assert bytes.length >= offset + length;
     this.bytes = bytes;
     this.offset = offset;
     this.length = length;
+    assert isValid();
   }
 
   /** This instance will directly reference bytes w/o making a copy.
@@ -340,4 +337,33 @@
     copy.copyBytes(other);
     return copy;
   }
+  
+  /** 
+   * Performs internal consistency checks.
+   * Always returns true (or throws IllegalStateException) 
+   */
+  public boolean isValid() {
+    if (bytes == null) {
+      throw new IllegalStateException("bytes is null");
+    }
+    if (length < 0) {
+      throw new IllegalStateException("length is negative: " + length);
+    }
+    if (length > bytes.length) {
+      throw new IllegalStateException("length is out of bounds: " + length + ",bytes.length=" + bytes.length);
+    }
+    if (offset < 0) {
+      throw new IllegalStateException("offset is negative: " + offset);
+    }
+    if (offset > bytes.length) {
+      throw new IllegalStateException("offset out of bounds: " + offset + ",bytes.length=" + bytes.length);
+    }
+    if (offset + length < 0) {
+      throw new IllegalStateException("offset+length is negative: offset=" + offset + ",length=" + length);
+    }
+    if (offset + length > bytes.length) {
+      throw new IllegalStateException("offset+length out of bounds: offset=" + offset + ",length=" + length + ",bytes.length=" + bytes.length);
+    }
+    return true;
+  }
 }
Index: lucene/core/src/java/org/apache/lucene/document/IntDocValuesField.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/document/IntDocValuesField.java	(revision 1442822)
+++ lucene/core/src/java/org/apache/lucene/document/IntDocValuesField.java	(working copy)
@@ -1,58 +0,0 @@
-package org.apache.lucene.document;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.index.DocValues;
-
-/**
- * <p>
- * Field that stores a per-document <code>int</code> value for scoring, 
- * sorting or value retrieval. Here's an example usage:
- * 
- * <pre class="prettyprint">
- *   document.add(new IntDocValuesField(name, 22));
- * </pre>
- * 
- * <p>
- * If you also need to store the value, you should add a
- * separate {@link StoredField} instance.
- * @see DocValues
- * */
-
-public class IntDocValuesField extends StoredField {
-
-  /**
-   * Type for 32-bit integer DocValues.
-   */
-  public static final FieldType TYPE = new FieldType();
-  static {
-    TYPE.setDocValueType(DocValues.Type.FIXED_INTS_32);
-    TYPE.freeze();
-  }
-
-  /** 
-   * Creates a new DocValues field with the specified 32-bit integer value 
-   * @param name field name
-   * @param value 32-bit integer value
-   * @throws IllegalArgumentException if the field name is null
-   */
-  public IntDocValuesField(String name, int value) {
-    super(name, TYPE);
-    fieldsData = Integer.valueOf(value);
-  }
-}
Index: lucene/core/src/java/org/apache/lucene/document/DerefBytesDocValuesField.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/document/DerefBytesDocValuesField.java	(revision 1442822)
+++ lucene/core/src/java/org/apache/lucene/document/DerefBytesDocValuesField.java	(working copy)
@@ -1,94 +0,0 @@
-package org.apache.lucene.document;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.index.DocValues;
-import org.apache.lucene.util.BytesRef;
-
-/**
- * <p>
- * Field that stores
- * a per-document {@link BytesRef} value.  The values are
- * stored indirectly, such that many documents sharing the
- * same value all point to a single copy of the value, which
- * is a good fit when the fields share values.  If values
- * are (mostly) unique it's better to use {@link
- * StraightBytesDocValuesField}.  Here's an example usage: 
- * 
- * <pre class="prettyprint">
- *   document.add(new DerefBytesDocValuesField(name, new BytesRef("hello")));
- * </pre>
- * 
- * <p>
- * If you also need to store the value, you should add a
- * separate {@link StoredField} instance.
- * 
- * @see DocValues
- * */
-
-public class DerefBytesDocValuesField extends StoredField {
-
-  // TODO: ideally indexer figures out var vs fixed on its own!?
-  /**
-   * Type for indirect bytes DocValues: all with the same length
-   */
-  public static final FieldType TYPE_FIXED_LEN = new FieldType();
-  static {
-    TYPE_FIXED_LEN.setDocValueType(DocValues.Type.BYTES_FIXED_DEREF);
-    TYPE_FIXED_LEN.freeze();
-  }
-
-  /**
-   * Type for indirect bytes DocValues: can have variable lengths
-   */
-  public static final FieldType TYPE_VAR_LEN = new FieldType();
-  static {
-    TYPE_VAR_LEN.setDocValueType(DocValues.Type.BYTES_VAR_DEREF);
-    TYPE_VAR_LEN.freeze();
-  }
-
-  /**
-   * Create a new variable-length indirect DocValues field.
-   * <p>
-   * This calls 
-   * {@link DerefBytesDocValuesField#DerefBytesDocValuesField(String, BytesRef, boolean)
-   *  DerefBytesDocValuesField(name, bytes, false}, meaning by default
-   * it allows for values of different lengths. If your values are all 
-   * the same length, use that constructor instead.
-   * @param name field name
-   * @param bytes binary content
-   * @throws IllegalArgumentException if the field name is null
-   */
-  public DerefBytesDocValuesField(String name, BytesRef bytes) {
-    super(name, TYPE_VAR_LEN);
-    fieldsData = bytes;
-  }
-
-  /**
-   * Create a new fixed or variable length indirect DocValues field.
-   * <p>
-   * @param name field name
-   * @param bytes binary content
-   * @param isFixedLength true if all values have the same length.
-   * @throws IllegalArgumentException if the field name is null
-   */
-  public DerefBytesDocValuesField(String name, BytesRef bytes, boolean isFixedLength) {
-    super(name, isFixedLength ? TYPE_FIXED_LEN : TYPE_VAR_LEN);
-    fieldsData = bytes;
-  }
-}
Index: lucene/core/src/java/org/apache/lucene/document/ShortDocValuesField.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/document/ShortDocValuesField.java	(revision 1442822)
+++ lucene/core/src/java/org/apache/lucene/document/ShortDocValuesField.java	(working copy)
@@ -1,59 +0,0 @@
-package org.apache.lucene.document;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.index.DocValues;
-
-/**
- * <p>
- * Field that stores a per-document <code>short</code> value for scoring, 
- * sorting or value retrieval. Here's an example usage:
- * 
- * <pre class="prettyprint">
- *   document.add(new ShortDocValuesField(name, (short) 22));
- * </pre>
- * 
- * <p>
- * If you also need to store the value, you should add a
- * separate {@link StoredField} instance.
- * 
- * @see DocValues
- * */
-
-public class ShortDocValuesField extends StoredField {
-
-  /**
-   * Type for 16-bit short DocValues.
-   */
-  public static final FieldType TYPE = new FieldType();
-  static {
-    TYPE.setDocValueType(DocValues.Type.FIXED_INTS_16);
-    TYPE.freeze();
-  }
-
-  /** 
-   * Creates a new DocValues field with the specified 16-bit short value 
-   * @param name field name
-   * @param value 16-bit short value
-   * @throws IllegalArgumentException if the field name is null
-   */
-  public ShortDocValuesField(String name, short value) {
-    super(name, TYPE);
-    fieldsData = Short.valueOf(value);
-  }
-}
Index: lucene/core/src/java/org/apache/lucene/document/LongDocValuesField.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/document/LongDocValuesField.java	(revision 1442822)
+++ lucene/core/src/java/org/apache/lucene/document/LongDocValuesField.java	(working copy)
@@ -1,58 +0,0 @@
-package org.apache.lucene.document;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.index.DocValues;
-
-/**
- * <p>
- * Field that stores a per-document <code>long</code> value for scoring, 
- * sorting or value retrieval. Here's an example usage:
- * 
- * <pre class="prettyprint">
- *   document.add(new LongDocValuesField(name, 22L));
- * </pre>
- * 
- * <p>
- * If you also need to store the value, you should add a
- * separate {@link StoredField} instance.
- * @see DocValues
- * */
-
-public class LongDocValuesField extends StoredField {
-
-  /**
-   * Type for 64-bit long DocValues.
-   */
-  public static final FieldType TYPE = new FieldType();
-  static {
-    TYPE.setDocValueType(DocValues.Type.FIXED_INTS_64);
-    TYPE.freeze();
-  }
-
-  /** 
-   * Creates a new DocValues field with the specified 64-bit long value 
-   * @param name field name
-   * @param value 64-bit long value
-   * @throws IllegalArgumentException if the field name is null
-   */
-  public LongDocValuesField(String name, long value) {
-    super(name, TYPE);
-    fieldsData = Long.valueOf(value);
-  }
-}
Index: lucene/core/src/java/org/apache/lucene/document/SortedDocValuesField.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/document/SortedDocValuesField.java	(revision 0)
+++ lucene/core/src/java/org/apache/lucene/document/SortedDocValuesField.java	(working copy)
@@ -0,0 +1,60 @@
+package org.apache.lucene.document;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.util.BytesRef;
+
+/**
+ * <p>
+ * Field that stores
+ * a per-document {@link BytesRef} value, indexed for
+ * sorting.  Here's an example usage:
+ * 
+ * <pre class="prettyprint">
+ *   document.add(new SortedDocValuesField(name, new BytesRef("hello")));
+ * </pre>
+ * 
+ * <p>
+ * If you also need to store the value, you should add a
+ * separate {@link StoredField} instance.
+ * 
+ * */
+
+public class SortedDocValuesField extends StoredField {
+
+  /**
+   * Type for sorted bytes DocValues
+   */
+  public static final FieldType TYPE = new FieldType();
+  static {
+    TYPE.setDocValueType(FieldInfo.DocValuesType.SORTED);
+    TYPE.freeze();
+  }
+
+  /**
+   * Create a new sorted DocValues field.
+   * @param name field name
+   * @param bytes binary content
+   * @throws IllegalArgumentException if the field name is null
+   */
+  public SortedDocValuesField(String name, BytesRef bytes) {
+    super(name, TYPE);
+    fieldsData = bytes;
+  }
+}

Property changes on: lucene/core/src/java/org/apache/lucene/document/SortedDocValuesField.java
___________________________________________________________________
Added: svn:eol-style
## -0,0 +1 ##
+native
Index: lucene/core/src/java/org/apache/lucene/document/NumericDocValuesField.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/document/NumericDocValuesField.java	(revision 0)
+++ lucene/core/src/java/org/apache/lucene/document/NumericDocValuesField.java	(working copy)
@@ -0,0 +1,57 @@
+package org.apache.lucene.document;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.index.FieldInfo;
+
+/**
+ * <p>
+ * Field that stores a per-document <code>long</code> value for scoring, 
+ * sorting or value retrieval. Here's an example usage:
+ * 
+ * <pre class="prettyprint">
+ *   document.add(new NumericDocValuesField(name, 22L));
+ * </pre>
+ * 
+ * <p>
+ * If you also need to store the value, you should add a
+ * separate {@link StoredField} instance.
+ * */
+
+public class NumericDocValuesField extends StoredField {
+
+  /**
+   * Type for numeric DocValues.
+   */
+  public static final FieldType TYPE = new FieldType();
+  static {
+    TYPE.setDocValueType(FieldInfo.DocValuesType.NUMERIC);
+    TYPE.freeze();
+  }
+
+  /** 
+   * Creates a new DocValues field with the specified 64-bit long value 
+   * @param name field name
+   * @param value 64-bit long value
+   * @throws IllegalArgumentException if the field name is null
+   */
+  public NumericDocValuesField(String name, long value) {
+    super(name, TYPE);
+    fieldsData = Long.valueOf(value);
+  }
+}

Property changes on: lucene/core/src/java/org/apache/lucene/document/NumericDocValuesField.java
___________________________________________________________________
Added: svn:eol-style
## -0,0 +1 ##
+native
Index: lucene/core/src/java/org/apache/lucene/document/ByteDocValuesField.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/document/ByteDocValuesField.java	(revision 1442822)
+++ lucene/core/src/java/org/apache/lucene/document/ByteDocValuesField.java	(working copy)
@@ -1,59 +0,0 @@
-package org.apache.lucene.document;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.index.DocValues;
-
-/**
- * <p>
- * Field that stores a per-document <code>byte</code> value for scoring, 
- * sorting or value retrieval. Here's an example usage:
- * 
- * <pre class="prettyprint">
- *   document.add(new ByteDocValuesField(name, (byte) 22));
- * </pre>
- * 
- * <p>
- * If you also need to store the value, you should add a
- * separate {@link StoredField} instance.
- * 
- * @see DocValues
- * */
-
-public class ByteDocValuesField extends StoredField {
-
-  /**
-   * Type for 8-bit byte DocValues.
-   */
-  public static final FieldType TYPE = new FieldType();
-  static {
-    TYPE.setDocValueType(DocValues.Type.FIXED_INTS_8);
-    TYPE.freeze();
-  }
-
-  /** 
-   * Creates a new DocValues field with the specified 8-bit byte value 
-   * @param name field name
-   * @param value 8-bit byte value
-   * @throws IllegalArgumentException if the field name is null.
-   */
-  public ByteDocValuesField(String name, byte value) {
-    super(name, TYPE);
-    fieldsData = Byte.valueOf(value);
-  }
-}
Index: lucene/core/src/java/org/apache/lucene/document/FloatDocValuesField.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/document/FloatDocValuesField.java	(revision 1442822)
+++ lucene/core/src/java/org/apache/lucene/document/FloatDocValuesField.java	(working copy)
@@ -17,34 +17,22 @@
  * limitations under the License.
  */
 
-import org.apache.lucene.index.DocValues;
+import org.apache.lucene.index.AtomicReader; // javadocs
+import org.apache.lucene.search.FieldCache; // javadocs
 
 /**
+ * Syntactic sugar for encoding floats as NumericDocValues
+ * via {@link Float#floatToRawIntBits(float)}.
  * <p>
- * Field that stores a per-document <code>float</code> value for scoring, 
- * sorting or value retrieval. Here's an example usage:
- * 
- * <pre class="prettyprint">
- *   document.add(new FloatDocValuesField(name, 22f));
- * </pre>
- * 
+ * Per-document floating point values can be retrieved via
+ * {@link FieldCache#getFloats(AtomicReader, String, boolean)}.
  * <p>
- * If you also need to store the value, you should add a
- * separate {@link StoredField} instance.
- * @see DocValues
- * */
+ * <b>NOTE</b>: In most all cases this will be rather inefficient,
+ * requiring four bytes per document. Consider encoding floating
+ * point values yourself with only as much precision as you require.
+ */
+public class FloatDocValuesField extends NumericDocValuesField {
 
-public class FloatDocValuesField extends StoredField {
-
-  /**
-   * Type for 32-bit float DocValues.
-   */
-  public static final FieldType TYPE = new FieldType();
-  static {
-    TYPE.setDocValueType(DocValues.Type.FLOAT_32);
-    TYPE.freeze();
-  }
-
   /** 
    * Creates a new DocValues field with the specified 32-bit float value 
    * @param name field name
@@ -52,7 +40,16 @@
    * @throws IllegalArgumentException if the field name is null
    */
   public FloatDocValuesField(String name, float value) {
-    super(name, TYPE);
-    fieldsData = Float.valueOf(value);
+    super(name, Float.floatToRawIntBits(value));
   }
+
+  @Override
+  public void setFloatValue(float value) {
+    super.setLongValue(Float.floatToRawIntBits(value));
+  }
+  
+  @Override
+  public void setLongValue(long value) {
+    throw new IllegalArgumentException("cannot change value type from Float to Long");
+  }
 }
Index: lucene/core/src/java/org/apache/lucene/document/SortedBytesDocValuesField.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/document/SortedBytesDocValuesField.java	(revision 1442822)
+++ lucene/core/src/java/org/apache/lucene/document/SortedBytesDocValuesField.java	(working copy)
@@ -1,88 +0,0 @@
-package org.apache.lucene.document;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.index.DocValues;
-import org.apache.lucene.util.BytesRef;
-
-/**
- * <p>
- * Field that stores
- * a per-document {@link BytesRef} value, indexed for
- * sorting.  Here's an example usage:
- * 
- * <pre class="prettyprint">
- *   document.add(new SortedBytesDocValuesField(name, new BytesRef("hello")));
- * </pre>
- * 
- * <p>
- * If you also need to store the value, you should add a
- * separate {@link StoredField} instance.
- * 
- * @see DocValues
- * */
-
-public class SortedBytesDocValuesField extends StoredField {
-
-  // TODO: ideally indexer figures out var vs fixed on its own!?
-  /**
-   * Type for sorted bytes DocValues: all with the same length
-   */
-  public static final FieldType TYPE_FIXED_LEN = new FieldType();
-  static {
-    TYPE_FIXED_LEN.setDocValueType(DocValues.Type.BYTES_FIXED_SORTED);
-    TYPE_FIXED_LEN.freeze();
-  }
-
-  /**
-   * Type for sorted bytes DocValues: can have variable lengths
-   */
-  public static final FieldType TYPE_VAR_LEN = new FieldType();
-  static {
-    TYPE_VAR_LEN.setDocValueType(DocValues.Type.BYTES_VAR_SORTED);
-    TYPE_VAR_LEN.freeze();
-  }
-
-  /**
-   * Create a new variable-length sorted DocValues field.
-   * <p>
-   * This calls 
-   * {@link SortedBytesDocValuesField#SortedBytesDocValuesField(String, BytesRef, boolean)
-   *  SortedBytesDocValuesField(name, bytes, false}, meaning by default
-   * it allows for values of different lengths. If your values are all 
-   * the same length, use that constructor instead.
-   * @param name field name
-   * @param bytes binary content
-   * @throws IllegalArgumentException if the field name is null
-   */
-  public SortedBytesDocValuesField(String name, BytesRef bytes) {
-    this(name, bytes, false);
-  }
-
-  /**
-   * Create a new fixed or variable length sorted DocValues field.
-   * @param name field name
-   * @param bytes binary content
-   * @param isFixedLength true if all values have the same length.
-   * @throws IllegalArgumentException if the field name is null
-   */
-  public SortedBytesDocValuesField(String name, BytesRef bytes, boolean isFixedLength) {
-    super(name, isFixedLength ? TYPE_FIXED_LEN : TYPE_VAR_LEN);
-    fieldsData = bytes;
-  }
-}
Index: lucene/core/src/java/org/apache/lucene/document/PackedLongDocValuesField.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/document/PackedLongDocValuesField.java	(revision 1442822)
+++ lucene/core/src/java/org/apache/lucene/document/PackedLongDocValuesField.java	(working copy)
@@ -1,63 +0,0 @@
-package org.apache.lucene.document;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.index.DocValues;
-import org.apache.lucene.index.AtomicReader;      // javadocs
-
-/**
- * <p>
- * Field that stores a per-document <code>long</code> value 
- * for scoring, sorting or value retrieval.  The values are 
- * encoded in the index an in RAM (when loaded via 
- * {@link AtomicReader#docValues})
- * using packed ints. Here's an example usage:
- * 
- * <pre class="prettyprint">
- *   document.add(new PackedLongDocValuesField(name, 22L));
- * </pre>
- * 
- * <p>
- * If you also need to store the value, you should add a
- * separate {@link StoredField} instance.
- * 
- * @see DocValues
- * */
-
-public class PackedLongDocValuesField extends StoredField {
-
-  /**
-   * Type for packed long DocValues.
-   */
-  public static final FieldType TYPE = new FieldType();
-  static {
-    TYPE.setDocValueType(DocValues.Type.VAR_INTS);
-    TYPE.freeze();
-  }
-
-  /** 
-   * Creates a new DocValues field with the specified long value 
-   * @param name field name
-   * @param value 64-bit long value
-   * @throws IllegalArgumentException if the field name is null
-   */
-  public PackedLongDocValuesField(String name, long value) {
-    super(name, TYPE);
-    fieldsData = Long.valueOf(value);
-  }
-}
Index: lucene/core/src/java/org/apache/lucene/document/DoubleDocValuesField.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/document/DoubleDocValuesField.java	(revision 1442822)
+++ lucene/core/src/java/org/apache/lucene/document/DoubleDocValuesField.java	(working copy)
@@ -17,35 +17,22 @@
  * limitations under the License.
  */
 
-import org.apache.lucene.index.DocValues;
+import org.apache.lucene.index.AtomicReader; // javadocs
+import org.apache.lucene.search.FieldCache; // javadocs
 
 /**
+ * Syntactic sugar for encoding doubles as NumericDocValues
+ * via {@link Double#doubleToRawLongBits(double)}.
  * <p>
- * Field that stores a per-document <code>double</code> value for scoring, 
- * sorting or value retrieval. Here's an example usage:
- * 
- * <pre class="prettyprint">
- *   document.add(new DoubleDocValuesField(name, 22.0));
- * </pre>
- * 
+ * Per-document double values can be retrieved via
+ * {@link FieldCache#getDoubles(AtomicReader, String, boolean)}.
  * <p>
- * If you also need to store the value, you should add a
- * separate {@link StoredField} instance.
- * 
- * @see DocValues
- * */
+ * <b>NOTE</b>: In most all cases this will be rather inefficient,
+ * requiring eight bytes per document. Consider encoding double
+ * values yourself with only as much precision as you require.
+ */
+public class DoubleDocValuesField extends NumericDocValuesField {
 
-public class DoubleDocValuesField extends StoredField {
-
-  /**
-   * Type for 64-bit double DocValues.
-   */
-  public static final FieldType TYPE = new FieldType();
-  static {
-    TYPE.setDocValueType(DocValues.Type.FLOAT_64);
-    TYPE.freeze();
-  }
-
   /** 
    * Creates a new DocValues field with the specified 64-bit double value 
    * @param name field name
@@ -53,7 +40,16 @@
    * @throws IllegalArgumentException if the field name is null
    */
   public DoubleDocValuesField(String name, double value) {
-    super(name, TYPE);
-    fieldsData = Double.valueOf(value);
+    super(name, Double.doubleToRawLongBits(value));
   }
+
+  @Override
+  public void setDoubleValue(double value) {
+    super.setLongValue(Double.doubleToRawLongBits(value));
+  }
+  
+  @Override
+  public void setLongValue(long value) {
+    throw new IllegalArgumentException("cannot change value type from Double to Long");
+  }
 }
Index: lucene/core/src/java/org/apache/lucene/document/FieldType.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/document/FieldType.java	(revision 1442822)
+++ lucene/core/src/java/org/apache/lucene/document/FieldType.java	(working copy)
@@ -18,7 +18,7 @@
  */
 
 import org.apache.lucene.analysis.Analyzer; // javadocs
-import org.apache.lucene.index.DocValues;
+import org.apache.lucene.index.FieldInfo.DocValuesType;
 import org.apache.lucene.index.FieldInfo.IndexOptions;
 import org.apache.lucene.index.IndexableFieldType;
 import org.apache.lucene.search.NumericRangeQuery; // javadocs
@@ -55,7 +55,7 @@
   private NumericType numericType;
   private boolean frozen;
   private int numericPrecisionStep = NumericUtils.PRECISION_STEP_DEFAULT;
-  private DocValues.Type docValueType;
+  private DocValuesType docValueType;
 
   /**
    * Create a new mutable FieldType with all of the properties from <code>ref</code>
@@ -416,21 +416,21 @@
    * {@inheritDoc}
    * <p>
    * The default is <code>null</code> (no docValues) 
-   * @see #setDocValueType(DocValues.Type)
+   * @see #setDocValueType(org.apache.lucene.index.FieldInfo.DocValuesType)
    */
   @Override
-  public DocValues.Type docValueType() {
+  public DocValuesType docValueType() {
     return docValueType;
   }
 
   /**
-   * Set's the field's DocValues.Type
+   * Set's the field's DocValuesType
    * @param type DocValues type, or null if no DocValues should be stored.
    * @throws IllegalStateException if this FieldType is frozen against
    *         future modifications.
    * @see #docValueType()
    */
-  public void setDocValueType(DocValues.Type type) {
+  public void setDocValueType(DocValuesType type) {
     checkIfFrozen();
     docValueType = type;
   }
Index: lucene/core/src/java/org/apache/lucene/document/StraightBytesDocValuesField.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/document/StraightBytesDocValuesField.java	(revision 1442822)
+++ lucene/core/src/java/org/apache/lucene/document/StraightBytesDocValuesField.java	(working copy)
@@ -1,93 +0,0 @@
-package org.apache.lucene.document;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.index.DocValues;
-import org.apache.lucene.util.BytesRef;
-
-/**
- * <p>
- * Field that stores
- * a per-document {@link BytesRef} value.  The values are
- * stored directly with no sharing, which is a good fit when
- * the fields don't share (many) values, such as a title
- * field.  If values may be shared it's better to use {@link
- * DerefBytesDocValuesField}.  Here's an example usage:
- * 
- * <pre class="prettyprint">
- *   document.add(new StraightBytesDocValuesField(name, new BytesRef("hello")));
- * </pre>
- * 
- * <p>
- * If you also need to store the value, you should add a
- * separate {@link StoredField} instance.
- * 
- * @see DocValues
- * */
-
-public class StraightBytesDocValuesField extends StoredField {
-
-  // TODO: ideally indexer figures out var vs fixed on its own!?
-  /**
-   * Type for direct bytes DocValues: all with the same length
-   */
-  public static final FieldType TYPE_FIXED_LEN = new FieldType();
-  static {
-    TYPE_FIXED_LEN.setDocValueType(DocValues.Type.BYTES_FIXED_STRAIGHT);
-    TYPE_FIXED_LEN.freeze();
-  }
-
-  /**
-   * Type for direct bytes DocValues: can have variable lengths
-   */
-  public static final FieldType TYPE_VAR_LEN = new FieldType();
-  static {
-    TYPE_VAR_LEN.setDocValueType(DocValues.Type.BYTES_VAR_STRAIGHT);
-    TYPE_VAR_LEN.freeze();
-  }
-
-  /**
-   * Create a new variable-length direct DocValues field.
-   * <p>
-   * This calls 
-   * {@link StraightBytesDocValuesField#StraightBytesDocValuesField(String, BytesRef, boolean)
-   *  StraightBytesDocValuesField(name, bytes, false}, meaning by default
-   * it allows for values of different lengths. If your values are all 
-   * the same length, use that constructor instead.
-   * @param name field name
-   * @param bytes binary content
-   * @throws IllegalArgumentException if the field name is null
-   */
-  public StraightBytesDocValuesField(String name, BytesRef bytes) {
-    super(name, TYPE_VAR_LEN);
-    fieldsData = bytes;
-  }
-
-  /**
-   * Create a new fixed or variable length direct DocValues field.
-   * <p>
-   * @param name field name
-   * @param bytes binary content
-   * @param isFixedLength true if all values have the same length.
-   * @throws IllegalArgumentException if the field name is null
-   */
-  public StraightBytesDocValuesField(String name, BytesRef bytes, boolean isFixedLength) {
-    super(name, isFixedLength ? TYPE_FIXED_LEN : TYPE_VAR_LEN);
-    fieldsData = bytes;
-  }
-}
Index: lucene/core/src/java/org/apache/lucene/document/Field.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/document/Field.java	(revision 1442822)
+++ lucene/core/src/java/org/apache/lucene/document/Field.java	(working copy)
@@ -29,7 +29,6 @@
 import org.apache.lucene.index.IndexWriter; // javadocs
 import org.apache.lucene.index.IndexableField;
 import org.apache.lucene.index.IndexableFieldType;
-import org.apache.lucene.index.Norm; // javadocs
 import org.apache.lucene.index.StorableField;
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.index.FieldInvertState; // javadocs
@@ -38,13 +37,8 @@
  * Expert: directly create a field for a document.  Most
  * users should use one of the sugar subclasses: {@link
  * IntField}, {@link LongField}, {@link FloatField}, {@link
- * DoubleField}, {@link ByteDocValuesField}, {@link
- * ShortDocValuesField}, {@link IntDocValuesField}, {@link
- * LongDocValuesField}, {@link PackedLongDocValuesField},
- * {@link FloatDocValuesField}, {@link
- * DoubleDocValuesField}, {@link SortedBytesDocValuesField},
- * {@link DerefBytesDocValuesField}, {@link
- * StraightBytesDocValuesField}, {@link
+ * DoubleField}, {@link BinaryDocValuesField}, {@link
+ * NumericDocValuesField}, {@link SortedDocValuesField}, {@link
  * StringField}, {@link TextField}, {@link StoredField}.
  *
  * <p/> A field is a section of a Document. Each field has three
Index: lucene/core/src/java/org/apache/lucene/document/BinaryDocValuesField.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/document/BinaryDocValuesField.java	(revision 0)
+++ lucene/core/src/java/org/apache/lucene/document/BinaryDocValuesField.java	(working copy)
@@ -0,0 +1,63 @@
+package org.apache.lucene.document;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.index.BinaryDocValues;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.util.BytesRef;
+
+/**
+ * Field that stores a per-document {@link BytesRef} value.  
+ * <p>
+ * The values are stored directly with no sharing, which is a good fit when
+ * the fields don't share (many) values, such as a title field.  If values 
+ * may be shared and sorted it's better to use {@link SortedDocValuesField}.  
+ * Here's an example usage:
+ * 
+ * <pre class="prettyprint">
+ *   document.add(new BinaryDocValuesField(name, new BytesRef("hello")));
+ * </pre>
+ * 
+ * <p>
+ * If you also need to store the value, you should add a
+ * separate {@link StoredField} instance.
+ * 
+ * @see BinaryDocValues
+ * */
+public class BinaryDocValuesField extends StoredField {
+  
+  /**
+   * Type for straight bytes DocValues.
+   */
+  public static final FieldType TYPE = new FieldType();
+  static {
+    TYPE.setDocValueType(FieldInfo.DocValuesType.BINARY);
+    TYPE.freeze();
+  }
+  
+  /**
+   * Create a new binary DocValues field.
+   * @param name field name
+   * @param value binary content
+   * @throws IllegalArgumentException if the field name is null
+   */
+  public BinaryDocValuesField(String name, BytesRef value) {
+    super(name, TYPE);
+    fieldsData = value;
+  }
+}

Property changes on: lucene/core/src/java/org/apache/lucene/document/BinaryDocValuesField.java
___________________________________________________________________
Added: svn:eol-style
## -0,0 +1 ##
+native
Index: lucene/core/src/resources/META-INF/services/org.apache.lucene.codecs.DocValuesFormat
===================================================================
--- lucene/core/src/resources/META-INF/services/org.apache.lucene.codecs.DocValuesFormat	(revision 0)
+++ lucene/core/src/resources/META-INF/services/org.apache.lucene.codecs.DocValuesFormat	(working copy)
@@ -0,0 +1,16 @@
+#  Licensed to the Apache Software Foundation (ASF) under one or more
+#  contributor license agreements.  See the NOTICE file distributed with
+#  this work for additional information regarding copyright ownership.
+#  The ASF licenses this file to You under the Apache License, Version 2.0
+#  (the "License"); you may not use this file except in compliance with
+#  the License.  You may obtain a copy of the License at
+#
+#       http://www.apache.org/licenses/LICENSE-2.0
+#
+#  Unless required by applicable law or agreed to in writing, software
+#  distributed under the License is distributed on an "AS IS" BASIS,
+#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+#  See the License for the specific language governing permissions and
+#  limitations under the License.
+
+org.apache.lucene.codecs.lucene42.Lucene42DocValuesFormat
Index: lucene/benchmark/src/test/org/apache/lucene/benchmark/byTask/TestPerfTasksLogic.java
===================================================================
--- lucene/benchmark/src/test/org/apache/lucene/benchmark/byTask/TestPerfTasksLogic.java	(revision 1442822)
+++ lucene/benchmark/src/test/org/apache/lucene/benchmark/byTask/TestPerfTasksLogic.java	(working copy)
@@ -53,10 +53,10 @@
 import org.apache.lucene.index.SegmentInfos;
 import org.apache.lucene.index.SerialMergeScheduler;
 import org.apache.lucene.index.SlowCompositeReaderWrapper;
+import org.apache.lucene.index.SortedDocValues;
 import org.apache.lucene.index.Terms;
 import org.apache.lucene.index.TermsEnum;
 import org.apache.lucene.search.DocIdSetIterator;
-import org.apache.lucene.search.FieldCache.DocTermsIndex;
 import org.apache.lucene.search.FieldCache;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.util.BytesRef;
@@ -342,12 +342,11 @@
     Benchmark benchmark = execBenchmark(algLines);
 
     DirectoryReader r = DirectoryReader.open(benchmark.getRunData().getDirectory());
-    DocTermsIndex idx = FieldCache.DEFAULT.getTermsIndex(new SlowCompositeReaderWrapper(r), "country");
+    SortedDocValues idx = FieldCache.DEFAULT.getTermsIndex(new SlowCompositeReaderWrapper(r), "country");
     final int maxDoc = r.maxDoc();
     assertEquals(1000, maxDoc);
-    BytesRef br = new BytesRef();
     for(int i=0;i<1000;i++) {
-      assertNotNull("doc " + i + " has null country", idx.getTerm(i, br));
+      assertTrue("doc " + i + " has null country", idx.getOrd(i) != -1);
     }
     r.close();
   }
Index: lucene/join/src/test/org/apache/lucene/search/join/TestJoinUtil.java
===================================================================
--- lucene/join/src/test/org/apache/lucene/search/join/TestJoinUtil.java	(revision 1442822)
+++ lucene/join/src/test/org/apache/lucene/search/join/TestJoinUtil.java	(working copy)
@@ -17,6 +17,9 @@
  * limitations under the License.
  */
 
+import java.io.IOException;
+import java.util.*;
+
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.analysis.MockTokenizer;
 import org.apache.lucene.document.Document;
@@ -24,6 +27,7 @@
 import org.apache.lucene.document.TextField;
 import org.apache.lucene.index.AtomicReader;
 import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.BinaryDocValues;
 import org.apache.lucene.index.DocTermOrds;
 import org.apache.lucene.index.DocsEnum;
 import org.apache.lucene.index.IndexReader;
@@ -49,14 +53,11 @@
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.FixedBitSet;
+import org.apache.lucene.util.LuceneTestCase.Slow;
 import org.apache.lucene.util.LuceneTestCase;
 import org.apache.lucene.util._TestUtil;
-import org.apache.lucene.util.LuceneTestCase.Slow;
 import org.junit.Test;
 
-import java.io.IOException;
-import java.util.*;
-
 public class TestJoinUtil extends LuceneTestCase {
 
   public void testSimple() throws Exception {
@@ -523,13 +524,14 @@
         fromSearcher.search(new TermQuery(new Term("value", uniqueRandomValue)), new Collector() {
 
           private Scorer scorer;
-          private FieldCache.DocTerms terms;
+          private BinaryDocValues terms;
           private final BytesRef spare = new BytesRef();
 
           @Override
           public void collect(int doc) throws IOException {
-            BytesRef joinValue = terms.getTerm(doc, spare);
-            if (joinValue == null) {
+            terms.get(doc, spare);
+            BytesRef joinValue = spare;
+            if (joinValue.bytes == BinaryDocValues.MISSING) {
               return;
             }
 
@@ -641,13 +643,14 @@
       } else {
         toSearcher.search(new MatchAllDocsQuery(), new Collector() {
 
-          private FieldCache.DocTerms terms;
+          private BinaryDocValues terms;
           private int docBase;
           private final BytesRef spare = new BytesRef();
 
           @Override
           public void collect(int doc) {
-            JoinScore joinScore = joinValueToJoinScores.get(terms.getTerm(doc, spare));
+            terms.get(doc, spare);
+            JoinScore joinScore = joinValueToJoinScores.get(spare);
             if (joinScore == null) {
               return;
             }
Index: lucene/join/src/java/org/apache/lucene/search/join/TermsCollector.java
===================================================================
--- lucene/join/src/java/org/apache/lucene/search/join/TermsCollector.java	(revision 1442822)
+++ lucene/join/src/java/org/apache/lucene/search/join/TermsCollector.java	(working copy)
@@ -17,7 +17,10 @@
  * limitations under the License.
  */
 
+import java.io.IOException;
+
 import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.BinaryDocValues;
 import org.apache.lucene.index.DocTermOrds;
 import org.apache.lucene.index.TermsEnum;
 import org.apache.lucene.search.Collector;
@@ -26,8 +29,6 @@
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.BytesRefHash;
 
-import java.io.IOException;
-
 /**
  * A collector that collects all terms from a specified field matching the query.
  *
@@ -109,7 +110,7 @@
   static class SV extends TermsCollector {
 
     final BytesRef spare = new BytesRef();
-    private FieldCache.DocTerms fromDocTerms;
+    private BinaryDocValues fromDocTerms;
 
     SV(String field) {
       super(field);
@@ -117,7 +118,8 @@
 
     @Override
     public void collect(int doc) throws IOException {
-      collectorTerms.add(fromDocTerms.getTerm(doc, spare));
+      fromDocTerms.get(doc, spare);
+      collectorTerms.add(spare);
     }
 
     @Override
Index: lucene/join/src/java/org/apache/lucene/search/join/TermsWithScoreCollector.java
===================================================================
--- lucene/join/src/java/org/apache/lucene/search/join/TermsWithScoreCollector.java	(revision 1442822)
+++ lucene/join/src/java/org/apache/lucene/search/join/TermsWithScoreCollector.java	(working copy)
@@ -17,7 +17,10 @@
  * limitations under the License.
  */
 
+import java.io.IOException;
+
 import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.BinaryDocValues;
 import org.apache.lucene.index.DocTermOrds;
 import org.apache.lucene.index.TermsEnum;
 import org.apache.lucene.search.Collector;
@@ -27,8 +30,6 @@
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.BytesRefHash;
 
-import java.io.IOException;
-
 abstract class TermsWithScoreCollector extends Collector {
 
   private final static int INITIAL_ARRAY_SIZE = 256;
@@ -92,7 +93,7 @@
   static class SV extends TermsWithScoreCollector {
 
     final BytesRef spare = new BytesRef();
-    FieldCache.DocTerms fromDocTerms;
+    BinaryDocValues fromDocTerms;
 
     SV(String field, ScoreMode scoreMode) {
       super(field, scoreMode);
@@ -100,7 +101,8 @@
 
     @Override
     public void collect(int doc) throws IOException {
-      int ord = collectedTerms.add(fromDocTerms.getTerm(doc, spare));
+      fromDocTerms.get(doc, spare);
+      int ord = collectedTerms.add(spare);
       if (ord < 0) {
         ord = -ord - 1;
       } else {
@@ -141,7 +143,8 @@
 
       @Override
       public void collect(int doc) throws IOException {
-        int ord = collectedTerms.add(fromDocTerms.getTerm(doc, spare));
+        fromDocTerms.get(doc, spare);
+        int ord = collectedTerms.add(spare);
         if (ord < 0) {
           ord = -ord - 1;
         } else {
Index: solr/core/src/test/org/apache/solr/update/DocumentBuilderTest.java
===================================================================
--- solr/core/src/test/org/apache/solr/update/DocumentBuilderTest.java	(revision 1442822)
+++ solr/core/src/test/org/apache/solr/update/DocumentBuilderTest.java	(working copy)
@@ -20,6 +20,7 @@
 import org.apache.lucene.document.Document;
 import org.apache.lucene.index.IndexableField;
 import org.apache.lucene.index.AtomicReader;
+import org.apache.lucene.index.NumericDocValues;
 import org.apache.lucene.index.SlowCompositeReaderWrapper;
 import org.apache.lucene.search.similarities.DefaultSimilarity;
 import org.apache.solr.SolrTestCaseJ4;
@@ -332,19 +333,19 @@
 
       DefaultSimilarity sim = (DefaultSimilarity) searcher.getSimilarity();
       
-      byte[] titleNorms = (byte[]) reader.normValues("title").getSource().getArray();
-      byte[] fooNorms = (byte[]) reader.normValues("foo_t").getSource().getArray();
-      byte[] textNorms = (byte[]) reader.normValues("text").getSource().getArray();
+      NumericDocValues titleNorms = reader.getNormValues("title");
+      NumericDocValues fooNorms = reader.getNormValues("foo_t");
+      NumericDocValues textNorms =  reader.getNormValues("text");
 
       assertEquals(expectedNorm(sim, 2, TITLE_BOOST * DOC_BOOST),
-                   titleNorms[docid]);
+                   titleNorms.get(docid));
 
       assertEquals(expectedNorm(sim, 8-3, FOO_BOOST * DOC_BOOST),
-                   fooNorms[docid]);
+                   fooNorms.get(docid));
 
       assertEquals(expectedNorm(sim, 2 + 8-3, 
                                 TITLE_BOOST * FOO_BOOST * DOC_BOOST),
-                   textNorms[docid]);
+                   textNorms.get(docid));
 
     } finally {
       req.close();
Index: solr/core/src/test/org/apache/solr/search/TestDocSet.java
===================================================================
--- solr/core/src/test/org/apache/solr/search/TestDocSet.java	(revision 1442822)
+++ solr/core/src/test/org/apache/solr/search/TestDocSet.java	(working copy)
@@ -22,21 +22,23 @@
 import java.util.List;
 import java.util.Random;
 
+import org.apache.lucene.index.AtomicReader;
+import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.BinaryDocValues;
 import org.apache.lucene.index.FieldInfo;
 import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.DocValues;
-import org.apache.lucene.index.StoredFieldVisitor;
 import org.apache.lucene.index.Fields;
-import org.apache.lucene.index.AtomicReader;
 import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.IndexReaderContext;
 import org.apache.lucene.index.MultiReader;
-import org.apache.lucene.index.IndexReaderContext;
+import org.apache.lucene.index.NumericDocValues;
+import org.apache.lucene.index.SortedDocValues;
+import org.apache.lucene.index.StoredFieldVisitor;
 import org.apache.lucene.search.DocIdSet;
 import org.apache.lucene.search.DocIdSetIterator;
 import org.apache.lucene.search.Filter;
+import org.apache.lucene.util.Bits;
 import org.apache.lucene.util.LuceneTestCase;
-import org.apache.lucene.util.Bits;
 import org.apache.lucene.util.OpenBitSet;
 import org.apache.lucene.util.OpenBitSetIterator;
 
@@ -386,16 +388,26 @@
       }
 
       @Override
-      public DocValues normValues(String field) {
+      public NumericDocValues getNumericDocValues(String field) {
         return null;
       }
 
       @Override
-      public DocValues docValues(String field) {
+      public BinaryDocValues getBinaryDocValues(String field) {
         return null;
       }
 
       @Override
+      public SortedDocValues getSortedDocValues(String field) {
+        return null;
+      }
+
+      @Override
+      public NumericDocValues getNormValues(String field) {
+        return null;
+      }
+
+      @Override
       protected void doClose() {
       }
 
Index: solr/core/src/test/org/apache/solr/search/function/TestFunctionQuery.java
===================================================================
--- solr/core/src/test/org/apache/solr/search/function/TestFunctionQuery.java	(revision 1442822)
+++ solr/core/src/test/org/apache/solr/search/function/TestFunctionQuery.java	(working copy)
@@ -18,7 +18,6 @@
 package org.apache.solr.search.function;
 
 import org.apache.lucene.codecs.Codec;
-import org.apache.lucene.index.Norm;
 import org.apache.lucene.index.FieldInvertState;
 import org.apache.lucene.search.FieldCache;
 import org.apache.lucene.search.similarities.DefaultSimilarity;
@@ -347,18 +346,17 @@
     FieldInvertState state = new FieldInvertState("a_t");
     state.setBoost(1.0f);
     state.setLength(4);
-    Norm norm = new Norm();
-    similarity.computeNorm(state, norm);
-    float nrm = similarity.decodeNormValue(norm.field().numericValue().byteValue());
+    long norm = similarity.computeNorm(state);
+    float nrm = similarity.decodeNormValue((byte) norm);
     assertQ(req("fl","*,score","q", "{!func}norm(a_t)", "fq","id:2"),
         "//float[@name='score']='" + nrm  + "'");  // sqrt(4)==2 and is exactly representable when quantized to a byte
 
     // test that ord and rord are working on a global index basis, not just
     // at the segment level (since Lucene 2.9 has switched to per-segment searching)
-    assertQ(req("fl","*,score","q", "{!func}ord(id)", "fq","id:6"), "//float[@name='score']='6.0'");
-    assertQ(req("fl","*,score","q", "{!func}top(ord(id))", "fq","id:6"), "//float[@name='score']='6.0'");
-    assertQ(req("fl","*,score","q", "{!func}rord(id)", "fq","id:1"),"//float[@name='score']='6.0'");
-    assertQ(req("fl","*,score","q", "{!func}top(rord(id))", "fq","id:1"),"//float[@name='score']='6.0'");
+    assertQ(req("fl","*,score","q", "{!func}ord(id)", "fq","id:6"), "//float[@name='score']='5.0'");
+    assertQ(req("fl","*,score","q", "{!func}top(ord(id))", "fq","id:6"), "//float[@name='score']='5.0'");
+    assertQ(req("fl","*,score","q", "{!func}rord(id)", "fq","id:1"),"//float[@name='score']='5.0'");
+    assertQ(req("fl","*,score","q", "{!func}top(rord(id))", "fq","id:1"),"//float[@name='score']='5.0'");
 
 
     // test that we can subtract dates to millisecond precision
Index: solr/core/src/java/org/apache/solr/search/MissingStringLastComparatorSource.java
===================================================================
--- solr/core/src/java/org/apache/solr/search/MissingStringLastComparatorSource.java	(revision 1442822)
+++ solr/core/src/java/org/apache/solr/search/MissingStringLastComparatorSource.java	(working copy)
@@ -17,8 +17,11 @@
 
 package org.apache.solr.search;
 
+import java.io.IOException;
+
 import org.apache.lucene.index.AtomicReader;
 import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.SortedDocValues;
 import org.apache.lucene.search.FieldCache;
 import org.apache.lucene.search.FieldComparator;
 import org.apache.lucene.search.FieldComparatorSource;
@@ -26,9 +29,7 @@
 import org.apache.lucene.util.UnicodeUtil;
 import org.apache.lucene.util.packed.PackedInts;
 
-import java.io.IOException;
 
-
 public class MissingStringLastComparatorSource extends FieldComparatorSource {
   private final BytesRef missingValueProxy;
 
@@ -56,13 +57,13 @@
 // Copied from Lucene's TermOrdValComparator and modified since the Lucene version couldn't
 // be extended.
 class TermOrdValComparator_SML extends FieldComparator<Comparable> {
-  private static final int NULL_ORD = Integer.MAX_VALUE;
+  private static final int NULL_ORD = Integer.MAX_VALUE-1;
 
   private final int[] ords;
   private final BytesRef[] values;
   private final int[] readerGen;
 
-  private FieldCache.DocTermsIndex termsIndex;
+  private SortedDocValues termsIndex;
   private final String field;
 
   private final BytesRef NULL_VAL;
@@ -137,7 +138,7 @@
     protected final int[] readerGen;
 
     protected int currentReaderGen = -1;
-    protected FieldCache.DocTermsIndex termsIndex;
+    protected SortedDocValues termsIndex;
 
     protected int bottomSlot = -1;
     protected int bottomOrd;
@@ -196,13 +197,13 @@
         bottomSameReader = true;
       } else {
         if (bottomValue == null) {
-          // 0 ord is null for all segments
+          // -1 ord is null for all segments
           assert ords[bottomSlot] == NULL_ORD;
           bottomOrd = NULL_ORD;
           bottomSameReader = true;
           readerGen[bottomSlot] = currentReaderGen;
         } else {
-          final int index = binarySearch(tempBR, termsIndex, bottomValue);
+          final int index = termsIndex.lookupTerm(bottomValue);
           if (index < 0) {
             bottomOrd = -index - 2;
             bottomSameReader = false;
@@ -224,8 +225,8 @@
 
     @Override
     public int compareDocToValue(int doc, BytesRef value) {
-      final BytesRef docValue = termsIndex.getTerm(doc, tempBR);
-      if (docValue == null) {
+      int docOrd = termsIndex.getOrd(doc);
+      if (docOrd == -1) {
         if (value == null) {
           return 0;
         }
@@ -233,219 +234,63 @@
       } else if (value == null) {
         return -1;
       }
-      return docValue.compareTo(value);
+      termsIndex.lookupOrd(docOrd, tempBR);
+      return tempBR.compareTo(value);
     }
   }
 
-  // Used per-segment when bit width of doc->ord is 8:
-  private static final class ByteOrdComparator extends PerSegmentComparator {
-    private final byte[] readerOrds;
-
-    public ByteOrdComparator(byte[] readerOrds, TermOrdValComparator_SML parent) {
+  private static final class AnyOrdComparator extends PerSegmentComparator {
+    public AnyOrdComparator(TermOrdValComparator_SML parent) {
       super(parent);
-      this.readerOrds = readerOrds;
     }
 
     @Override
     public int compareBottom(int doc) {
       assert bottomSlot != -1;
-      int order = readerOrds[doc]&0xFF;
-      if (order == 0) order = NULL_ORD;
+      int order = termsIndex.getOrd(doc);
+      if (order == -1) order = NULL_ORD;
       if (bottomSameReader) {
-        // ord is precisely comparable, even in the equal case
+        // ord is precisely comparable, even in the equal
+        // case
         return bottomOrd - order;
       } else {
         // ord is only approx comparable: if they are not
         // equal, we can use that; if they are equal, we
         // must fallback to compare by value
-        final int cmp = bottomOrd - order;
-        if (cmp != 0) {
-          return cmp;
-        }
 
-        // take care of the case where both vals are null
-        if (order == NULL_ORD) return 0;
-
-        // and at this point we know that neither value is null, so safe to compare
-        termsIndex.lookup(order, tempBR);
-        return bottomValue.compareTo(tempBR);
-      }
-    }
-
-    @Override
-    public void copy(int slot, int doc) {
-      int ord = readerOrds[doc]&0xFF;
-      if (ord == 0) {
-        ords[slot] = NULL_ORD;
-        values[slot] = null;
-      } else {
-        ords[slot] = ord;
-        assert ord > 0;
-        if (values[slot] == null) {
-          values[slot] = new BytesRef();
-        }
-        termsIndex.lookup(ord, values[slot]);
-      }
-      readerGen[slot] = currentReaderGen;
-    }
-  }
-
-  // Used per-segment when bit width of doc->ord is 16:
-  private static final class ShortOrdComparator extends PerSegmentComparator {
-    private final short[] readerOrds;
-
-    public ShortOrdComparator(short[] readerOrds, TermOrdValComparator_SML parent) {
-      super(parent);
-      this.readerOrds = readerOrds;
-    }
-
-    @Override
-    public int compareBottom(int doc) {
-      assert bottomSlot != -1;
-      int order = readerOrds[doc]&0xFFFF;
-      if (order == 0) order = NULL_ORD;
-      if (bottomSameReader) {
-        // ord is precisely comparable, even in the equal case
-        return bottomOrd - order;
-      } else {
-        // ord is only approx comparable: if they are not
-        // equal, we can use that; if they are equal, we
-        // must fallback to compare by value
         final int cmp = bottomOrd - order;
         if (cmp != 0) {
           return cmp;
         }
 
         // take care of the case where both vals are null
-        if (order == NULL_ORD) return 0;
-
-        // and at this point we know that neither value is null, so safe to compare
-        termsIndex.lookup(order, tempBR);
-        return bottomValue.compareTo(tempBR);
-      }
-    }
-
-    @Override
-    public void copy(int slot, int doc) {
-      int ord = readerOrds[doc]&0xFFFF;
-      if (ord == 0) {
-        ords[slot] = NULL_ORD;
-        values[slot] = null;
-      } else {
-        ords[slot] = ord;
-        assert ord > 0;
-        if (values[slot] == null) {
-          values[slot] = new BytesRef();
+        if (order == NULL_ORD) {
+          return 0;
         }
-        termsIndex.lookup(ord, values[slot]);
-      }
-      readerGen[slot] = currentReaderGen;
-    }
-  }
 
-  // Used per-segment when bit width of doc->ord is 32:
-  private static final class IntOrdComparator extends PerSegmentComparator {
-    private final int[] readerOrds;
-
-    public IntOrdComparator(int[] readerOrds, TermOrdValComparator_SML parent) {
-      super(parent);
-      this.readerOrds = readerOrds;
-    }
-
-    @Override
-    public int compareBottom(int doc) {
-      assert bottomSlot != -1;
-      int order = readerOrds[doc];
-      if (order == 0) order = NULL_ORD;
-      if (bottomSameReader) {
-        // ord is precisely comparable, even in the equal case
-        return bottomOrd - order;
-      } else {
-        // ord is only approx comparable: if they are not
-        // equal, we can use that; if they are equal, we
-        // must fallback to compare by value
-        final int cmp = bottomOrd - order;
-        if (cmp != 0) {
-          return cmp;
-        }
-
-        // take care of the case where both vals are null
-        if (order == NULL_ORD) return 0;
-
         // and at this point we know that neither value is null, so safe to compare
-        termsIndex.lookup(order, tempBR);
-        return bottomValue.compareTo(tempBR);
-      }
-    }
-
-    @Override
-    public void copy(int slot, int doc) {
-      int ord = readerOrds[doc];
-      if (ord == 0) {
-        ords[slot] = NULL_ORD;
-        values[slot] = null;
-      } else {
-        ords[slot] = ord;
-        assert ord > 0;
-        if (values[slot] == null) {
-          values[slot] = new BytesRef();
+        if (order == NULL_ORD) {
+          return bottomValue.compareTo(parent.NULL_VAL);
+        } else {
+          termsIndex.lookupOrd(order, tempBR);
+          return bottomValue.compareTo(tempBR);
         }
-        termsIndex.lookup(ord, values[slot]);
       }
-      readerGen[slot] = currentReaderGen;
     }
-  }
 
-  // Used per-segment when bit width is not a native array
-  // size (8, 16, 32):
-  private static final class AnyOrdComparator extends PerSegmentComparator {
-    private final PackedInts.Reader readerOrds;
-
-    public AnyOrdComparator(PackedInts.Reader readerOrds, TermOrdValComparator_SML parent) {
-      super(parent);
-      this.readerOrds = readerOrds;
-    }
-
     @Override
-    public int compareBottom(int doc) {
-      assert bottomSlot != -1;
-      int order = (int) readerOrds.get(doc);
-      if (order == 0) order = NULL_ORD;
-      if (bottomSameReader) {
-        // ord is precisely comparable, even in the equal case
-        return bottomOrd - order;
-      } else {
-        // ord is only approx comparable: if they are not
-        // equal, we can use that; if they are equal, we
-        // must fallback to compare by value
-        final int cmp = bottomOrd - order;
-        if (cmp != 0) {
-          return cmp;
-        }
-
-        // take care of the case where both vals are null
-        if (order == NULL_ORD) return 0;
-
-        // and at this point we know that neither value is null, so safe to compare
-        termsIndex.lookup(order, tempBR);
-        return bottomValue.compareTo(tempBR);
-      }
-
-    }
-
-    @Override
     public void copy(int slot, int doc) {
-      int ord = (int) readerOrds.get(doc);
-      if (ord == 0) {
+      int ord = termsIndex.getOrd(doc);
+      if (ord == -1) {
         ords[slot] = NULL_ORD;
         values[slot] = null;
       } else {
         ords[slot] = ord;
-        assert ord > 0;
+        assert ord >= 0;
         if (values[slot] == null) {
           values[slot] = new BytesRef();
         }
-        termsIndex.lookup(ord, values[slot]);
+        termsIndex.lookupOrd(ord, values[slot]);
       }
       readerGen[slot] = currentReaderGen;
     }
@@ -453,23 +298,8 @@
 
   public static FieldComparator createComparator(AtomicReader reader, TermOrdValComparator_SML parent) throws IOException {
     parent.termsIndex = FieldCache.DEFAULT.getTermsIndex(reader, parent.field);
-    final PackedInts.Reader docToOrd = parent.termsIndex.getDocToOrd();
-    PerSegmentComparator perSegComp = null;
-    if (docToOrd.hasArray()) {
-      final Object arr = docToOrd.getArray();
-      if (arr instanceof byte[]) {
-        perSegComp = new ByteOrdComparator((byte[]) arr, parent);
-      } else if (arr instanceof short[]) {
-        perSegComp = new ShortOrdComparator((short[]) arr, parent);
-      } else if (arr instanceof int[]) {
-        perSegComp = new IntOrdComparator((int[]) arr, parent);
-      }
-    }
+    PerSegmentComparator perSegComp = new AnyOrdComparator(parent);
 
-    if (perSegComp == null) {
-      perSegComp = new AnyOrdComparator(docToOrd, parent);
-    }
-
     if (perSegComp.bottomSlot != -1) {
       perSegComp.setBottom(perSegComp.bottomSlot);
     }
Index: solr/core/src/java/org/apache/solr/handler/component/FieldFacetStats.java
===================================================================
--- solr/core/src/java/org/apache/solr/handler/component/FieldFacetStats.java	(revision 1442822)
+++ solr/core/src/java/org/apache/solr/handler/component/FieldFacetStats.java	(working copy)
@@ -16,17 +16,18 @@
  * limitations under the License.
  */
 
+import java.util.ArrayList;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+
+import org.apache.lucene.index.SortedDocValues;
 import org.apache.lucene.search.FieldCache;
 import org.apache.lucene.util.BytesRef;
 import org.apache.solr.schema.FieldType;
 import org.apache.solr.schema.SchemaField;
 
-import java.util.ArrayList;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
 
-
 /**
  * FieldFacetStats is a utility to accumulate statistics on a set of values in one field,
  * for facet values present in another field.
@@ -39,7 +40,7 @@
 
 public class FieldFacetStats {
   public final String name;
-  final FieldCache.DocTermsIndex si;
+  final SortedDocValues si;
   final SchemaField facet_sf;
   final SchemaField field_sf;
 
@@ -55,15 +56,15 @@
 
   private final BytesRef tempBR = new BytesRef();
 
-  public FieldFacetStats(String name, FieldCache.DocTermsIndex si, SchemaField field_sf, SchemaField facet_sf, int numStatsTerms) {
+  public FieldFacetStats(String name, SortedDocValues si, SchemaField field_sf, SchemaField facet_sf, int numStatsTerms) {
     this.name = name;
     this.si = si;
     this.field_sf = field_sf;
     this.facet_sf = facet_sf;
     this.numStatsTerms = numStatsTerms;
 
-    startTermIndex = 1;
-    endTermIndex = si.numOrd();
+    startTermIndex = 0;
+    endTermIndex = si.getValueCount();
     nTerms = endTermIndex - startTermIndex;
 
     facetStatsValues = new HashMap<String, StatsValues>();
@@ -79,10 +80,11 @@
 
   BytesRef getTermText(int docID, BytesRef ret) {
     final int ord = si.getOrd(docID);
-    if (ord == 0) {
+    if (ord == -1) {
       return null;
     } else {
-      return si.lookup(ord, ret);
+      si.lookupOrd(ord, ret);
+      return ret;
     }
   }
 
@@ -90,7 +92,14 @@
     int term = si.getOrd(docID);
     int arrIdx = term - startTermIndex;
     if (arrIdx >= 0 && arrIdx < nTerms) {
-      final BytesRef br = si.lookup(term, tempBR);
+      
+      final BytesRef br;
+      if (term == -1) {
+        br = null;
+      } else {
+        br = tempBR;
+        si.lookupOrd(term, tempBR);
+      }
       String key = (br == null)?null:facet_sf.getType().indexedToReadable(br.utf8ToString());
       StatsValues stats = facetStatsValues.get(key);
       if (stats == null) {
@@ -117,7 +126,13 @@
     int term = si.getOrd(docID);
     int arrIdx = term - startTermIndex;
     if (arrIdx >= 0 && arrIdx < nTerms) {
-      final BytesRef br = si.lookup(term, tempBR);
+      final BytesRef br;
+      if (term == -1) {
+        br = null;
+      } else {
+        br = tempBR;
+        si.lookupOrd(term, tempBR);
+      }
       String key = br == null ? null : br.utf8ToString();
       HashMap<String, Integer> statsTermCounts = facetStatsTerms.get(statsTermNum);
       Integer statsTermCount = statsTermCounts.get(key);
Index: solr/core/src/java/org/apache/solr/handler/component/StatsComponent.java
===================================================================
--- solr/core/src/java/org/apache/solr/handler/component/StatsComponent.java	(revision 1442822)
+++ solr/core/src/java/org/apache/solr/handler/component/StatsComponent.java	(working copy)
@@ -23,22 +23,23 @@
 import java.util.List;
 import java.util.Map;
 
+import org.apache.lucene.index.SortedDocValues;
 import org.apache.lucene.search.FieldCache;
 import org.apache.lucene.util.BytesRef;
 import org.apache.solr.common.SolrException;
+import org.apache.solr.common.params.ShardParams;
 import org.apache.solr.common.params.SolrParams;
 import org.apache.solr.common.params.StatsParams;
-import org.apache.solr.common.params.ShardParams;
 import org.apache.solr.common.util.NamedList;
 import org.apache.solr.common.util.SimpleOrderedMap;
 import org.apache.solr.request.SolrQueryRequest;
+import org.apache.solr.request.UnInvertedField;
 import org.apache.solr.schema.FieldType;
 import org.apache.solr.schema.SchemaField;
 import org.apache.solr.schema.TrieField;
 import org.apache.solr.search.DocIterator;
 import org.apache.solr.search.DocSet;
 import org.apache.solr.search.SolrIndexSearcher;
-import org.apache.solr.request.UnInvertedField;
 
 /**
  * Stats component calculates simple statistics on numeric field values
@@ -240,7 +241,7 @@
   public NamedList<?> getFieldCacheStats(String fieldName, String[] facet ) {
     SchemaField sf = searcher.getSchema().getField(fieldName);
     
-    FieldCache.DocTermsIndex si;
+    SortedDocValues si;
     try {
       si = FieldCache.DEFAULT.getTermsIndex(searcher.getAtomicReader(), fieldName);
     } 
@@ -248,12 +249,12 @@
       throw new RuntimeException( "failed to open field cache for: "+fieldName, e );
     }
     StatsValues allstats = StatsValuesFactory.createStatsValues(sf);
-    final int nTerms = si.numOrd();
+    final int nTerms = si.getValueCount();
     if ( nTerms <= 0 || docs.size() <= 0 ) return allstats.getStatsValues();
 
     // don't worry about faceting if no documents match...
     List<FieldFacetStats> facetStats = new ArrayList<FieldFacetStats>();
-    FieldCache.DocTermsIndex facetTermsIndex;
+    SortedDocValues facetTermsIndex;
     for( String facetField : facet ) {
       SchemaField fsf = searcher.getSchema().getField(facetField);
 
@@ -261,7 +262,7 @@
         throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,
           "Stats can only facet on single-valued fields, not: " + facetField );
       }
-
+      
       try {
         facetTermsIndex = FieldCache.DEFAULT.getTermsIndex(searcher.getAtomicReader(), facetField);
       }
@@ -276,11 +277,20 @@
     DocIterator iter = docs.iterator();
     while (iter.hasNext()) {
       int docID = iter.nextDoc();
-      BytesRef raw = si.lookup(si.getOrd(docID), tempBR);
-      if( raw.length > 0 ) {
-        allstats.accumulate(raw);
+      int docOrd = si.getOrd(docID);
+      BytesRef raw;
+      if (docOrd == -1) {
+        allstats.missing();
+        tempBR.length = 0;
+        raw = tempBR;
       } else {
-        allstats.missing();
+        raw = tempBR;
+        si.lookupOrd(docOrd, tempBR);
+        if( tempBR.length > 0 ) {
+          allstats.accumulate(tempBR);
+        } else {
+          allstats.missing();
+        }
       }
 
       // now update the facets
Index: solr/core/src/java/org/apache/solr/request/PerSegmentSingleValuedFaceting.java
===================================================================
--- solr/core/src/java/org/apache/solr/request/PerSegmentSingleValuedFaceting.java	(revision 1442822)
+++ solr/core/src/java/org/apache/solr/request/PerSegmentSingleValuedFaceting.java	(working copy)
@@ -17,15 +17,21 @@
 
 package org.apache.solr.request;
 
+import java.io.IOException;
+import java.util.*;
+import java.util.concurrent.*;
+
 import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.SortedDocValues;
+import org.apache.lucene.index.SortedDocValuesTermsEnum;
 import org.apache.lucene.index.TermsEnum;
 import org.apache.lucene.search.DocIdSet;
 import org.apache.lucene.search.DocIdSetIterator;
 import org.apache.lucene.search.FieldCache;
 import org.apache.lucene.search.Filter;
+import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.CharsRef;
 import org.apache.lucene.util.PriorityQueue;
-import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.UnicodeUtil;
 import org.apache.lucene.util.packed.PackedInts;
 import org.apache.solr.common.SolrException;
@@ -36,11 +42,7 @@
 import org.apache.solr.search.SolrIndexSearcher;
 import org.apache.solr.util.BoundedTreeSet;
 
-import java.io.IOException;
-import java.util.*;
-import java.util.concurrent.*;
 
-
 class PerSegmentSingleValuedFaceting {
 
   // input params
@@ -145,15 +147,15 @@
 
 
       if (seg.startTermIndex < seg.endTermIndex) {
-        if (seg.startTermIndex==0) {
+        if (seg.startTermIndex==-1) {
           hasMissingCount=true;
           missingCount += seg.counts[0];
-          seg.pos = 1;
+          seg.pos = 0;
         } else {
           seg.pos = seg.startTermIndex;
         }
         if (seg.pos < seg.endTermIndex) {
-          seg.tenum = seg.si.getTermsEnum();          
+          seg.tenum = new SortedDocValuesTermsEnum(seg.si);
           seg.tenum.seekExact(seg.pos);
           seg.tempBR = seg.tenum.term();
           queue.add(seg);
@@ -224,7 +226,7 @@
       this.context = context;
     }
     
-    FieldCache.DocTermsIndex si;
+    SortedDocValues si;
     int startTermIndex;
     int endTermIndex;
     int[] counts;
@@ -240,16 +242,16 @@
 
       if (prefix!=null) {
         BytesRef prefixRef = new BytesRef(prefix);
-        startTermIndex = si.binarySearchLookup(prefixRef, tempBR);
+        startTermIndex = si.lookupTerm(prefixRef);
         if (startTermIndex<0) startTermIndex=-startTermIndex-1;
         prefixRef.append(UnicodeUtil.BIG_TERM);
         // TODO: we could constrain the lower endpoint if we had a binarySearch method that allowed passing start/end
-        endTermIndex = si.binarySearchLookup(prefixRef, tempBR);
+        endTermIndex = si.lookupTerm(prefixRef);
         assert endTermIndex < 0;
         endTermIndex = -endTermIndex-1;
       } else {
-        startTermIndex=0;
-        endTermIndex=si.numOrd();
+        startTermIndex=-1;
+        endTermIndex=si.getValueCount();
       }
 
       final int nTerms=endTermIndex-startTermIndex;
@@ -262,68 +264,19 @@
 
 
         ////
-        PackedInts.Reader ordReader = si.getDocToOrd();
         int doc;
 
-        final Object arr;
-        if (ordReader.hasArray()) {
-          arr = ordReader.getArray();
-        } else {
-          arr = null;
-        }
-
-        if (arr instanceof int[]) {
-          int[] ords = (int[]) arr;
-          if (prefix==null) {
-            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {
-              counts[ords[doc]]++;
-            }
-          } else {
-            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {
-              int term = ords[doc];
-              int arrIdx = term-startTermIndex;
-              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;
-            }
+        if (prefix==null) {
+          // specialized version when collecting counts for all terms
+          while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {
+            counts[1+si.getOrd(doc)]++;
           }
-        } else if (arr instanceof short[]) {
-          short[] ords = (short[]) arr;
-          if (prefix==null) {
-            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {
-              counts[ords[doc] & 0xffff]++;
-            }
-          } else {
-            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {
-              int term = ords[doc] & 0xffff;
-              int arrIdx = term-startTermIndex;
-              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;
-            }
-          }
-        } else if (arr instanceof byte[]) {
-          byte[] ords = (byte[]) arr;
-          if (prefix==null) {
-            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {
-              counts[ords[doc] & 0xff]++;
-            }
-          } else {
-            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {
-              int term = ords[doc] & 0xff;
-              int arrIdx = term-startTermIndex;
-              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;
-            }
-          }
         } else {
-          if (prefix==null) {
-            // specialized version when collecting counts for all terms
-            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {
-              counts[si.getOrd(doc)]++;
-            }
-          } else {
-            // version that adjusts term numbers because we aren't collecting the full range
-            while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {
-              int term = si.getOrd(doc);
-              int arrIdx = term-startTermIndex;
-              if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;
-            }
+          // version that adjusts term numbers because we aren't collecting the full range
+          while ((doc = iter.nextDoc()) < DocIdSetIterator.NO_MORE_DOCS) {
+            int term = si.getOrd(doc);
+            int arrIdx = term-startTermIndex;
+            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;
           }
         }
       }
Index: solr/core/src/java/org/apache/solr/request/SimpleFacets.java
===================================================================
--- solr/core/src/java/org/apache/solr/request/SimpleFacets.java	(revision 1442822)
+++ solr/core/src/java/org/apache/solr/request/SimpleFacets.java	(working copy)
@@ -478,8 +478,10 @@
     FieldType ft = searcher.getSchema().getFieldType(fieldName);
     NamedList<Integer> res = new NamedList<Integer>();
 
-    FieldCache.DocTermsIndex si = FieldCache.DEFAULT.getTermsIndex(searcher.getAtomicReader(), fieldName);
+    SortedDocValues si = FieldCache.DEFAULT.getTermsIndex(searcher.getAtomicReader(), fieldName);
 
+    final BytesRef br = new BytesRef();
+
     final BytesRef prefixRef;
     if (prefix == null) {
       prefixRef = null;
@@ -490,19 +492,17 @@
       prefixRef = new BytesRef(prefix);
     }
 
-    final BytesRef br = new BytesRef();
-
     int startTermIndex, endTermIndex;
     if (prefix!=null) {
-      startTermIndex = si.binarySearchLookup(prefixRef, br);
+      startTermIndex = si.lookupTerm(prefixRef);
       if (startTermIndex<0) startTermIndex=-startTermIndex-1;
       prefixRef.append(UnicodeUtil.BIG_TERM);
-      endTermIndex = si.binarySearchLookup(prefixRef, br);
+      endTermIndex = si.lookupTerm(prefixRef);
       assert endTermIndex < 0;
       endTermIndex = -endTermIndex-1;
     } else {
-      startTermIndex=0;
-      endTermIndex=si.numOrd();
+      startTermIndex=-1;
+      endTermIndex=si.getValueCount();
     }
 
     final int nTerms=endTermIndex-startTermIndex;
@@ -516,62 +516,13 @@
 
       DocIterator iter = docs.iterator();
 
-      PackedInts.Reader ordReader = si.getDocToOrd();
-      final Object arr;
-      if (ordReader.hasArray()) {
-        arr = ordReader.getArray();
-      } else {
-        arr = null;
+      while (iter.hasNext()) {
+        int term = si.getOrd(iter.nextDoc());
+        int arrIdx = term-startTermIndex;
+        if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;
       }
 
-      if (arr instanceof int[]) {
-        int[] ords = (int[]) arr;
-        if (prefix==null) {
-          while (iter.hasNext()) {
-            counts[ords[iter.nextDoc()]]++;
-          }
-        } else {
-          while (iter.hasNext()) {
-            int term = ords[iter.nextDoc()];
-            int arrIdx = term-startTermIndex;
-            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;
-          }
-        }
-      } else if (arr instanceof short[]) {
-        short[] ords = (short[]) arr;
-        if (prefix==null) {
-          while (iter.hasNext()) {
-            counts[ords[iter.nextDoc()] & 0xffff]++;
-          }
-        } else {
-          while (iter.hasNext()) {
-            int term = ords[iter.nextDoc()] & 0xffff;
-            int arrIdx = term-startTermIndex;
-            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;
-          }
-        }
-      } else if (arr instanceof byte[]) {
-        byte[] ords = (byte[]) arr;
-        if (prefix==null) {
-          while (iter.hasNext()) {
-            counts[ords[iter.nextDoc()] & 0xff]++;
-          }
-        } else {
-          while (iter.hasNext()) {
-            int term = ords[iter.nextDoc()] & 0xff;
-            int arrIdx = term-startTermIndex;
-            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;
-          }
-        }
-      } else {
-        while (iter.hasNext()) {
-          int term = si.getOrd(iter.nextDoc());
-          int arrIdx = term-startTermIndex;
-          if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;
-        }
-      }
-
-      if (startTermIndex == 0) {
+      if (startTermIndex == -1) {
         missingCount = counts[0];
       }
 
@@ -587,7 +538,7 @@
         LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);
 
         int min=mincount-1;  // the smallest value in the top 'N' values
-        for (int i=(startTermIndex==0)?1:0; i<nTerms; i++) {
+        for (int i=(startTermIndex==-1)?1:0; i<nTerms; i++) {
           int c = counts[i];
           if (c>min) {
             // NOTE: we use c>min rather than c>=min as an optimization because we are going in
@@ -614,13 +565,14 @@
           long pair = sorted[i];
           int c = (int)(pair >>> 32);
           int tnum = Integer.MAX_VALUE - (int)pair;
-          ft.indexedToReadable(si.lookup(startTermIndex+tnum, br), charsRef);
+          si.lookupOrd(startTermIndex+tnum, br);
+          ft.indexedToReadable(br, charsRef);
           res.add(charsRef.toString(), c);
         }
       
       } else {
         // add results in index order
-        int i=(startTermIndex==0)?1:0;
+        int i=(startTermIndex==-1)?1:0;
         if (mincount<=0) {
           // if mincount<=0, then we won't discard any terms and we know exactly
           // where to start.
@@ -632,7 +584,8 @@
           int c = counts[i];
           if (c<mincount || --off>=0) continue;
           if (--lim<0) break;
-          ft.indexedToReadable(si.lookup(startTermIndex+i, br), charsRef);
+          si.lookupOrd(startTermIndex+i, br);
+          ft.indexedToReadable(br, charsRef);
           res.add(charsRef.toString(), c);
         }
       }
Index: solr/core/src/java/org/apache/solr/request/UnInvertedField.java
===================================================================
--- solr/core/src/java/org/apache/solr/request/UnInvertedField.java	(revision 1442822)
+++ solr/core/src/java/org/apache/solr/request/UnInvertedField.java	(working copy)
@@ -17,38 +17,37 @@
 
 package org.apache.solr.request;
 
-import org.apache.lucene.search.FieldCache;
+import java.io.IOException;
+import java.util.HashMap;
+import java.util.LinkedHashMap;
+import java.util.Map;
+import java.util.concurrent.atomic.AtomicLong;
+
 import org.apache.lucene.index.DocTermOrds;
+import org.apache.lucene.index.SortedDocValues;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.index.TermsEnum;
+import org.apache.lucene.search.FieldCache;
 import org.apache.lucene.search.TermQuery;
 import org.apache.lucene.search.TermRangeQuery;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.CharsRef;
+import org.apache.lucene.util.OpenBitSet;
+import org.apache.lucene.util.UnicodeUtil;
+import org.apache.solr.common.SolrException;
 import org.apache.solr.common.params.FacetParams;
 import org.apache.solr.common.util.NamedList;
-import org.apache.solr.common.SolrException;
 import org.apache.solr.core.SolrCore;
-
+import org.apache.solr.handler.component.FieldFacetStats;
+import org.apache.solr.handler.component.StatsValues;
+import org.apache.solr.handler.component.StatsValuesFactory;
 import org.apache.solr.schema.FieldType;
 import org.apache.solr.schema.SchemaField;
 import org.apache.solr.schema.TrieField;
 import org.apache.solr.search.*;
 import org.apache.solr.util.LongPriorityQueue;
 import org.apache.solr.util.PrimUtils;
-import org.apache.solr.handler.component.StatsValues;
-import org.apache.solr.handler.component.StatsValuesFactory;
-import org.apache.solr.handler.component.FieldFacetStats;
-import org.apache.lucene.util.CharsRef;
-import org.apache.lucene.util.OpenBitSet;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.UnicodeUtil;
 
-import java.io.IOException;
-import java.util.HashMap;
-import java.util.LinkedHashMap;
-import java.util.Map;
-
-import java.util.concurrent.atomic.AtomicLong;
-
 /**
  *
  * Final form of the un-inverted field:
@@ -481,7 +480,7 @@
     int i = 0;
     final FieldFacetStats[] finfo = new FieldFacetStats[facet.length];
     //Initialize facetstats, if facets have been passed in
-    FieldCache.DocTermsIndex si;
+    SortedDocValues si;
     for (String f : facet) {
       SchemaField facet_sf = searcher.getSchema().getField(f);
       try {
Index: solr/core/src/java/org/apache/solr/schema/SortableIntField.java
===================================================================
--- solr/core/src/java/org/apache/solr/schema/SortableIntField.java	(revision 1442822)
+++ solr/core/src/java/org/apache/solr/schema/SortableIntField.java	(working copy)
@@ -145,7 +145,12 @@
       @Override
       public int intVal(int doc) {
         int ord=termsIndex.getOrd(doc);
-        return ord==0 ? def  : NumberUtils.SortableStr2int(termsIndex.lookup(ord, spare),0,3);
+        if (ord==-1) {
+          return def;
+        } else {
+          termsIndex.lookupOrd(ord, spare);
+          return NumberUtils.SortableStr2int(spare,0,3);
+        }
       }
 
       @Override
@@ -171,7 +176,12 @@
       @Override
       public Object objectVal(int doc) {
         int ord=termsIndex.getOrd(doc);
-        return ord==0 ? null  : NumberUtils.SortableStr2int(termsIndex.lookup(ord, spare));
+        if (ord==-1) {
+          return null;
+        } else {
+          termsIndex.lookupOrd(ord, spare);
+          return NumberUtils.SortableStr2int(spare);
+        }
       }
 
       @Override
@@ -187,11 +197,12 @@
           @Override
           public void fillValue(int doc) {
             int ord=termsIndex.getOrd(doc);
-            if (ord == 0) {
+            if (ord == -1) {
               mval.value = def;
               mval.exists = false;
             } else {
-              mval.value = NumberUtils.SortableStr2int(termsIndex.lookup(ord, spare),0,3);
+              termsIndex.lookupOrd(ord, spare);
+              mval.value = NumberUtils.SortableStr2int(spare,0,3);
               mval.exists = true;
             }
           }
Index: solr/core/src/java/org/apache/solr/schema/StrFieldSource.java
===================================================================
--- solr/core/src/java/org/apache/solr/schema/StrFieldSource.java	(revision 1442822)
+++ solr/core/src/java/org/apache/solr/schema/StrFieldSource.java	(working copy)
@@ -52,7 +52,7 @@
 
       @Override
       public int numOrd() {
-        return termsIndex.numOrd();
+        return termsIndex.getValueCount();
       }
 
       @Override
Index: solr/core/src/java/org/apache/solr/schema/SortableDoubleField.java
===================================================================
--- solr/core/src/java/org/apache/solr/schema/SortableDoubleField.java	(revision 1442822)
+++ solr/core/src/java/org/apache/solr/schema/SortableDoubleField.java	(working copy)
@@ -149,7 +149,12 @@
       @Override
       public double doubleVal(int doc) {
         int ord=termsIndex.getOrd(doc);
-        return ord==0 ? def  : NumberUtils.SortableStr2double(termsIndex.lookup(ord, spare));
+        if (ord == -1) {
+          return def;
+        } else {
+          termsIndex.lookupOrd(ord, spare);
+          return NumberUtils.SortableStr2double(spare);
+        }
       }
 
       @Override
@@ -160,7 +165,12 @@
       @Override
       public Object objectVal(int doc) {
         int ord=termsIndex.getOrd(doc);
-        return ord==0 ? null  : NumberUtils.SortableStr2double(termsIndex.lookup(ord, spare));
+        if (ord==-1) {
+          return null;
+        } else {
+          termsIndex.lookupOrd(ord, spare);
+          return NumberUtils.SortableStr2double(spare);
+        }
       }
 
       @Override
@@ -181,11 +191,12 @@
           @Override
           public void fillValue(int doc) {
             int ord=termsIndex.getOrd(doc);
-            if (ord == 0) {
+            if (ord == -1) {
               mval.value = def;
               mval.exists = false;
             } else {
-              mval.value = NumberUtils.SortableStr2double(termsIndex.lookup(ord, spare));
+              termsIndex.lookupOrd(ord, spare);
+              mval.value = NumberUtils.SortableStr2double(spare);
               mval.exists = true;
             }
           }
Index: solr/core/src/java/org/apache/solr/schema/DateField.java
===================================================================
--- solr/core/src/java/org/apache/solr/schema/DateField.java	(revision 1442822)
+++ solr/core/src/java/org/apache/solr/schema/DateField.java	(working copy)
@@ -498,21 +498,22 @@
       @Override
       public String strVal(int doc) {
         int ord=termsIndex.getOrd(doc);
-        if (ord == 0) {
+        if (ord == -1) {
           return null;
         } else {
-          final BytesRef br = termsIndex.lookup(ord, spare);
-          return ft.indexedToReadable(br, spareChars).toString();
+          termsIndex.lookupOrd(ord, spare);
+          return ft.indexedToReadable(spare, spareChars).toString();
         }
       }
 
       @Override
       public Object objectVal(int doc) {
         int ord=termsIndex.getOrd(doc);
-        if (ord == 0) {
+        if (ord == -1) {
           return null;
         } else {
-          final BytesRef br = termsIndex.lookup(ord, new BytesRef());
+          final BytesRef br = new BytesRef();
+          termsIndex.lookupOrd(ord, br);
           return ft.toObject(null, br);
         }
       }
Index: solr/core/src/java/org/apache/solr/schema/SortableLongField.java
===================================================================
--- solr/core/src/java/org/apache/solr/schema/SortableLongField.java	(revision 1442822)
+++ solr/core/src/java/org/apache/solr/schema/SortableLongField.java	(working copy)
@@ -148,7 +148,12 @@
       @Override
       public long longVal(int doc) {
         int ord=termsIndex.getOrd(doc);
-        return ord==0 ? def  : NumberUtils.SortableStr2long(termsIndex.lookup(ord, spare),0,5);
+        if (ord==-1) {
+          return def;
+        } else {
+          termsIndex.lookupOrd(ord, spare);
+          return NumberUtils.SortableStr2long(spare,0,5);
+        }
       }
 
       @Override
@@ -164,7 +169,12 @@
       @Override
       public Object objectVal(int doc) {
         int ord=termsIndex.getOrd(doc);
-        return ord==0 ? null  : NumberUtils.SortableStr2long(termsIndex.lookup(ord, spare));
+        if (ord==-1) {
+          return null;
+        } else {
+          termsIndex.lookupOrd(ord, spare);
+          return NumberUtils.SortableStr2long(spare);
+        }
       }
 
       @Override
@@ -185,11 +195,12 @@
           @Override
           public void fillValue(int doc) {
             int ord=termsIndex.getOrd(doc);
-            if (ord == 0) {
+            if (ord == -1) {
               mval.value = def;
               mval.exists = false;
             } else {
-              mval.value = NumberUtils.SortableStr2long(termsIndex.lookup(ord, spare),0,5);
+              termsIndex.lookupOrd(ord, spare);
+              mval.value = NumberUtils.SortableStr2long(spare,0,5);
               mval.exists = true;
             }
           }
Index: solr/core/src/java/org/apache/solr/schema/BoolField.java
===================================================================
--- solr/core/src/java/org/apache/solr/schema/BoolField.java	(revision 1442822)
+++ solr/core/src/java/org/apache/solr/schema/BoolField.java	(working copy)
@@ -17,29 +17,30 @@
 
 package org.apache.solr.schema;
 
+import java.io.IOException;
+import java.io.Reader;
+import java.util.Map;
+
+import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.analysis.Tokenizer;
+import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
 import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.GeneralField;
+import org.apache.lucene.index.SortedDocValues;
 import org.apache.lucene.index.StorableField;
-import org.apache.lucene.search.FieldCache;
-import org.apache.lucene.search.SortField;
-import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.queries.function.docvalues.BoolDocValues;
 import org.apache.lucene.queries.function.valuesource.OrdFieldSource;
+import org.apache.lucene.search.FieldCache;
+import org.apache.lucene.search.SortField;
+import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.CharsRef;
 import org.apache.lucene.util.mutable.MutableValue;
 import org.apache.lucene.util.mutable.MutableValueBool;
+import org.apache.solr.analysis.SolrAnalyzer;
+import org.apache.solr.response.TextResponseWriter;
 import org.apache.solr.search.QParser;
-import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
-import org.apache.solr.response.TextResponseWriter;
-import org.apache.solr.analysis.SolrAnalyzer;
-
-import java.util.Map;
-import java.io.Reader;
-import java.io.IOException;
 /**
  *
  */
@@ -167,14 +168,14 @@
 
   @Override
   public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
-    final FieldCache.DocTermsIndex sindex = FieldCache.DEFAULT.getTermsIndex(readerContext.reader(), field);
+    final SortedDocValues sindex = FieldCache.DEFAULT.getTermsIndex(readerContext.reader(), field);
 
     // figure out what ord maps to true
-    int nord = sindex.numOrd();
+    int nord = sindex.getValueCount();
     BytesRef br = new BytesRef();
     int tord = -1;
-    for (int i=1; i<nord; i++) {
-      sindex.lookup(i, br);
+    for (int i=0; i<nord; i++) {
+      sindex.lookupOrd(i, br);
       if (br.length==1 && br.bytes[br.offset]=='T') {
         tord = i;
         break;
@@ -191,7 +192,7 @@
 
       @Override
       public boolean exists(int doc) {
-        return sindex.getOrd(doc) != 0;
+        return sindex.getOrd(doc) != -1;
       }
 
       @Override
@@ -208,7 +209,7 @@
           public void fillValue(int doc) {
             int ord = sindex.getOrd(doc);
             mval.value = (ord == trueOrd);
-            mval.exists = (ord != 0);
+            mval.exists = (ord != -1);
           }
         };
       }
Index: solr/core/src/java/org/apache/solr/schema/SortableFloatField.java
===================================================================
--- solr/core/src/java/org/apache/solr/schema/SortableFloatField.java	(revision 1442822)
+++ solr/core/src/java/org/apache/solr/schema/SortableFloatField.java	(working copy)
@@ -138,7 +138,12 @@
       @Override
       public float floatVal(int doc) {
         int ord=termsIndex.getOrd(doc);
-        return ord==0 ? def  : NumberUtils.SortableStr2float(termsIndex.lookup(ord, spare));
+        if (ord==-1) {
+          return def;
+        } else {
+          termsIndex.lookupOrd(ord, spare);
+          return NumberUtils.SortableStr2float(spare);
+        }
       }
 
       @Override
@@ -169,7 +174,12 @@
       @Override
       public Object objectVal(int doc) {
         int ord=termsIndex.getOrd(doc);
-        return ord==0 ? null  : NumberUtils.SortableStr2float(termsIndex.lookup(ord, spare));
+        if (ord==-1) {
+          return null;
+        } else {
+          termsIndex.lookupOrd(ord, spare);
+          return NumberUtils.SortableStr2float(spare);
+        }
       }
 
       @Override
@@ -185,11 +195,12 @@
           @Override
           public void fillValue(int doc) {
             int ord=termsIndex.getOrd(doc);
-            if (ord == 0) {
+            if (ord == -1) {
               mval.value = def;
               mval.exists = false;
             } else {
-              mval.value = NumberUtils.SortableStr2float(termsIndex.lookup(ord, spare));
+              termsIndex.lookupOrd(ord, spare);
+              mval.value = NumberUtils.SortableStr2float(spare);
               mval.exists = true;
             }
           }
Index: solr/core/src/java/org/apache/solr/core/SchemaCodecFactory.java
===================================================================
--- solr/core/src/java/org/apache/solr/core/SchemaCodecFactory.java	(revision 1442822)
+++ solr/core/src/java/org/apache/solr/core/SchemaCodecFactory.java	(working copy)
@@ -55,6 +55,7 @@
         }
         return super.getPostingsFormatForField(field);
       }
+      // TODO: when dv support is added to solr, add it here too
     };
   }
 
Index: solr/core/src/java/org/apache/solr/core/SolrResourceLoader.java
===================================================================
--- solr/core/src/java/org/apache/solr/core/SolrResourceLoader.java	(revision 1442822)
+++ solr/core/src/java/org/apache/solr/core/SolrResourceLoader.java	(working copy)
@@ -36,6 +36,7 @@
 import org.apache.lucene.analysis.util.TokenizerFactory;
 import org.apache.lucene.codecs.Codec;
 import org.apache.lucene.codecs.PostingsFormat;
+import org.apache.lucene.codecs.DocValuesFormat;
 import org.apache.lucene.analysis.util.WordlistLoader;
 import org.apache.solr.common.ResourceLoader;
 import org.apache.solr.handler.admin.CoreAdminHandler;
@@ -178,6 +179,7 @@
   void reloadLuceneSPI() {
     // Codecs:
     PostingsFormat.reloadPostingsFormats(this.classLoader);
+    DocValuesFormat.reloadDocValuesFormats(this.classLoader);
     Codec.reloadCodecs(this.classLoader);
     // Analysis:
     CharFilterFactory.reloadCharFilters(this.classLoader);
