Index: tags/lucene_2_9_back_compat_tests_20090930a/src/test/org/apache/lucene/search/TestSort.java
===================================================================
--- tags/lucene_2_9_back_compat_tests_20090930a/src/test/org/apache/lucene/search/TestSort.java	(revision 821733)
+++ tags/lucene_2_9_back_compat_tests_20090930a/src/test/org/apache/lucene/search/TestSort.java	(working copy)
@@ -35,6 +35,7 @@
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexWriter;
 import org.apache.lucene.index.Term;
+import org.apache.lucene.index.TermRef;
 import org.apache.lucene.queryParser.ParseException;
 import org.apache.lucene.search.BooleanClause.Occur;
 import org.apache.lucene.store.LockObtainFailedException;
@@ -333,54 +334,78 @@
 
 
     sort.setSort (new SortField[] { new SortField ("parser", new FieldCache.IntParser(){
-      public final int parseInt(final String val) {
-        return (val.charAt(0)-'A') * 123456;
+      public final int parseInt(final String term) {
+        // dummy
+        return 0;
       }
+      public final int parseInt(final TermRef term) {
+        return (term.bytes[term.offset]-'A') * 123456;
+      }
     }), SortField.FIELD_DOC });
     assertMatches (full, queryA, sort, "JIHGFEDCBA");
     assertSaneFieldCaches(getName() + " IntParser");
     fc.purgeAllCaches();
 
     sort.setSort (new SortField[] { new SortField ("parser", new FieldCache.FloatParser(){
-      public final float parseFloat(final String val) {
-        return (float) Math.sqrt( val.charAt(0) );
+      public final float parseFloat(final String term) {
+        // dummy
+        return 0;
       }
+      public final float parseFloat(final TermRef term) {
+        return (float) Math.sqrt( term.bytes[term.offset] );
+      }
     }), SortField.FIELD_DOC });
     assertMatches (full, queryA, sort, "JIHGFEDCBA");
     assertSaneFieldCaches(getName() + " FloatParser");
     fc.purgeAllCaches();
 
     sort.setSort (new SortField[] { new SortField ("parser", new FieldCache.LongParser(){
-      public final long parseLong(final String val) {
-        return (val.charAt(0)-'A') * 1234567890L;
+      public final long parseLong(final String term) {
+        // dummy
+        return 0;
       }
+      public final long parseLong(final TermRef term) {
+        return (term.bytes[term.offset]-'A') * 1234567890L;
+      }
     }), SortField.FIELD_DOC });
     assertMatches (full, queryA, sort, "JIHGFEDCBA");
     assertSaneFieldCaches(getName() + " LongParser");
     fc.purgeAllCaches();
 
     sort.setSort (new SortField[] { new SortField ("parser", new FieldCache.DoubleParser(){
-      public final double parseDouble(final String val) {
-        return Math.pow( val.charAt(0), (val.charAt(0)-'A') );
+      public final double parseDouble(final String term) {
+        // dummy
+        return 0;
       }
+      public final double parseDouble(final TermRef term) {
+        return Math.pow( term.bytes[term.offset], (term.bytes[term.offset]-'A') );
+      }
     }), SortField.FIELD_DOC });
     assertMatches (full, queryA, sort, "JIHGFEDCBA");
     assertSaneFieldCaches(getName() + " DoubleParser");
     fc.purgeAllCaches();
 
     sort.setSort (new SortField[] { new SortField ("parser", new FieldCache.ByteParser(){
-      public final byte parseByte(final String val) {
-        return (byte) (val.charAt(0)-'A');
+      public final byte parseByte(final String term) {
+        // dummy
+        return 0;
       }
+      public final byte parseByte(final TermRef term) {
+        return (byte) (term.bytes[term.offset]-'A');
+      }
     }), SortField.FIELD_DOC });
     assertMatches (full, queryA, sort, "JIHGFEDCBA");
     assertSaneFieldCaches(getName() + " ByteParser");
     fc.purgeAllCaches();
 
     sort.setSort (new SortField[] { new SortField ("parser", new FieldCache.ShortParser(){
-      public final short parseShort(final String val) {
-        return (short) (val.charAt(0)-'A');
+      public final short parseShort(final String term) {
+        // dummy
+        return 0;
       }
+      public final short parseShort(final TermRef term) {
+        return (short) (term.bytes[term.offset]-'A');
+      }
     }), SortField.FIELD_DOC });
     assertMatches (full, queryA, sort, "JIHGFEDCBA");
     assertSaneFieldCaches(getName() + " ShortParser");
@@ -434,9 +459,13 @@
 
     public void setNextReader(IndexReader reader, int docBase) throws IOException {
       docValues = FieldCache.DEFAULT.getInts(reader, "parser", new FieldCache.IntParser() {
-          public final int parseInt(final String val) {
-            return (val.charAt(0)-'A') * 123456;
+          public final int parseInt(final String term) {
+            // dummy
+            return 0;
           }
+          public final int parseInt(final TermRef term) {
+            return (term.bytes[term.offset]-'A') * 123456;
+          }
         });
     }
 
Index: tags/lucene_2_9_back_compat_tests_20090930a/src/test/org/apache/lucene/search/TestTermScorer.java
===================================================================
--- tags/lucene_2_9_back_compat_tests_20090930a/src/test/org/apache/lucene/search/TestTermScorer.java	(revision 821733)
+++ tags/lucene_2_9_back_compat_tests_20090930a/src/test/org/apache/lucene/search/TestTermScorer.java	(working copy)
@@ -71,9 +71,9 @@
 
         Weight weight = termQuery.weight(indexSearcher);
 
-        TermScorer ts = new TermScorer(weight,
-                                       indexReader.termDocs(allTerm), indexSearcher.getSimilarity(),
-                                       indexReader.norms(FIELD));
+        Scorer ts = weight.scorer(indexSearcher.getIndexReader(),
+                                  true, true);
+
         //we have 2 documents with the term all in them, one document for all the other values
         final List docs = new ArrayList();
         //must call next first
@@ -133,9 +133,9 @@
 
         Weight weight = termQuery.weight(indexSearcher);
 
-        TermScorer ts = new TermScorer(weight,
-                                       indexReader.termDocs(allTerm), indexSearcher.getSimilarity(),
-                                       indexReader.norms(FIELD));
+        Scorer ts = weight.scorer(indexSearcher.getIndexReader(),
+                                  true, true);
+ 
         assertTrue("next did not return a doc", ts.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);
         assertTrue("score is not correct", ts.score() == 1.6931472f);
         assertTrue("next did not return a doc", ts.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);
@@ -150,9 +150,9 @@
 
         Weight weight = termQuery.weight(indexSearcher);
 
-        TermScorer ts = new TermScorer(weight,
-                                       indexReader.termDocs(allTerm), indexSearcher.getSimilarity(),
-                                       indexReader.norms(FIELD));
+        Scorer ts = weight.scorer(indexSearcher.getIndexReader(),
+                                  true, true);
+
         assertTrue("Didn't skip", ts.advance(3) != DocIdSetIterator.NO_MORE_DOCS);
         //The next doc should be doc 5
         assertTrue("doc should be number 5", ts.docID() == 5);
@@ -165,9 +165,9 @@
 
         Weight weight = termQuery.weight(indexSearcher);
 
-        TermScorer ts = new TermScorer(weight,
-                                       indexReader.termDocs(allTerm), indexSearcher.getSimilarity(),
-                                       indexReader.norms(FIELD));
+        Scorer ts = weight.scorer(indexSearcher.getIndexReader(),
+                                  true, true);
+
         Explanation explanation = ts.explain(0);
         assertTrue("explanation is null and it shouldn't be", explanation != null);
         //System.out.println("Explanation: " + explanation.toString());
@@ -183,8 +183,9 @@
         termQuery = new TermQuery(dogsTerm);
         weight = termQuery.weight(indexSearcher);
 
-        ts = new TermScorer(weight, indexReader.termDocs(dogsTerm), indexSearcher.getSimilarity(),
-                                       indexReader.norms(FIELD));
+        ts = weight.scorer(indexSearcher.getIndexReader(),
+                           true, true);
+
         explanation = ts.explain(1);
         assertTrue("explanation is null and it shouldn't be", explanation != null);
         //System.out.println("Explanation: " + explanation.toString());
Index: tags/lucene_2_9_back_compat_tests_20090930a/src/test/org/apache/lucene/index/TestSegmentTermEnum.java
===================================================================
--- tags/lucene_2_9_back_compat_tests_20090930a/src/test/org/apache/lucene/index/TestSegmentTermEnum.java	(revision 821733)
+++ tags/lucene_2_9_back_compat_tests_20090930a/src/test/org/apache/lucene/index/TestSegmentTermEnum.java	(working copy)
@@ -61,23 +61,6 @@
     verifyDocFreq();
   }
 
-  public void testPrevTermAtEnd() throws IOException
-  {
-    Directory dir = new MockRAMDirectory();
-    IndexWriter writer  = new IndexWriter(dir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
-    addDoc(writer, "aaa bbb");
-    writer.close();
-    SegmentReader reader = SegmentReader.getOnlySegmentReader(dir);
-    SegmentTermEnum termEnum = (SegmentTermEnum) reader.terms();
-    assertTrue(termEnum.next());
-    assertEquals("aaa", termEnum.term().text());
-    assertTrue(termEnum.next());
-    assertEquals("aaa", termEnum.prev().text());
-    assertEquals("bbb", termEnum.term().text());
-    assertFalse(termEnum.next());
-    assertEquals("bbb", termEnum.prev().text());
-  }
-
   private void verifyDocFreq()
       throws IOException
   {
Index: tags/lucene_2_9_back_compat_tests_20090930a/src/test/org/apache/lucene/index/TestIndexReader.java
===================================================================
--- tags/lucene_2_9_back_compat_tests_20090930a/src/test/org/apache/lucene/index/TestIndexReader.java	(revision 821733)
+++ tags/lucene_2_9_back_compat_tests_20090930a/src/test/org/apache/lucene/index/TestIndexReader.java	(working copy)
@@ -1009,30 +1009,8 @@
           // new IndexFileDeleter, have it delete
           // unreferenced files, then verify that in fact
           // no files were deleted:
-          String[] startFiles = dir.listAll();
-          SegmentInfos infos = new SegmentInfos();
-          infos.read(dir);
-          new IndexFileDeleter(dir, new KeepOnlyLastCommitDeletionPolicy(), infos, null, null);
-          String[] endFiles = dir.listAll();
+          TestIndexWriter.assertNoUnreferencedFiles(dir, "reader.close() failed to delete unreferenced files");
 
-          Arrays.sort(startFiles);
-          Arrays.sort(endFiles);
-
-          //for(int i=0;i<startFiles.length;i++) {
-          //  System.out.println("  startFiles: " + i + ": " + startFiles[i]);
-          //}
-
-          if (!Arrays.equals(startFiles, endFiles)) {
-            String successStr;
-            if (success) {
-              successStr = "success";
-            } else {
-              successStr = "IOException";
-              err.printStackTrace();
-            }
-            fail("reader.close() failed to delete unreferenced files after " + successStr + " (" + diskFree + " bytes): before delete:\n    " + arrayToString(startFiles) + "\n  after delete:\n    " + arrayToString(endFiles));
-          }
-
           // Finally, verify index is not corrupt, and, if
           // we succeeded, we see all docs changed, and if
           // we failed, we see either all docs or no docs
@@ -1819,7 +1797,6 @@
     } catch (IllegalStateException ise) {
       // expected
     }
-    assertFalse(((SegmentReader) r.getSequentialSubReaders()[0]).termsIndexLoaded());
 
     assertEquals(-1, r.getTermInfosIndexDivisor());
     writer = new IndexWriter(dir, new WhitespaceAnalyzer(), IndexWriter.MaxFieldLength.UNLIMITED);
@@ -1832,7 +1809,12 @@
     IndexReader[] subReaders = r2.getSequentialSubReaders();
     assertEquals(2, subReaders.length);
     for(int i=0;i<2;i++) {
-      assertFalse(((SegmentReader) subReaders[i]).termsIndexLoaded());
+      try {
+        subReaders[i].docFreq(new Term("field", "f"));
+        fail("did not hit expected exception");
+      } catch (IllegalStateException ise) {
+        // expected
+      }
     }
     r2.close();
     dir.close();
Index: tags/lucene_2_9_back_compat_tests_20090930a/src/test/org/apache/lucene/index/TestSegmentTermDocs.java
===================================================================
--- tags/lucene_2_9_back_compat_tests_20090930a/src/test/org/apache/lucene/index/TestSegmentTermDocs.java	(revision 821733)
+++ tags/lucene_2_9_back_compat_tests_20090930a/src/test/org/apache/lucene/index/TestSegmentTermDocs.java	(working copy)
@@ -55,14 +55,13 @@
     SegmentReader reader = SegmentReader.get(true, info, indexDivisor);
     assertTrue(reader != null);
     assertEquals(indexDivisor, reader.getTermInfosIndexDivisor());
-    SegmentTermDocs segTermDocs = new SegmentTermDocs(reader);
-    assertTrue(segTermDocs != null);
-    segTermDocs.seek(new Term(DocHelper.TEXT_FIELD_2_KEY, "field"));
-    if (segTermDocs.next() == true)
-    {
-      int docId = segTermDocs.doc();
+    TermDocs termDocs = reader.termDocs();
+    assertTrue(termDocs != null);
+    termDocs.seek(new Term(DocHelper.TEXT_FIELD_2_KEY, "field"));
+    if (termDocs.next() == true) {
+      int docId = termDocs.doc();
       assertTrue(docId == 0);
-      int freq = segTermDocs.freq();
+      int freq = termDocs.freq();
       assertTrue(freq == 3);  
     }
     reader.close();
@@ -77,20 +76,20 @@
       //After adding the document, we should be able to read it back in
       SegmentReader reader = SegmentReader.get(true, info, indexDivisor);
       assertTrue(reader != null);
-      SegmentTermDocs segTermDocs = new SegmentTermDocs(reader);
-      assertTrue(segTermDocs != null);
-      segTermDocs.seek(new Term("textField2", "bad"));
-      assertTrue(segTermDocs.next() == false);
+      TermDocs termDocs = reader.termDocs();
+      assertTrue(termDocs != null);
+      termDocs.seek(new Term("textField2", "bad"));
+      assertTrue(termDocs.next() == false);
       reader.close();
     }
     {
       //After adding the document, we should be able to read it back in
       SegmentReader reader = SegmentReader.get(true, info, indexDivisor);
       assertTrue(reader != null);
-      SegmentTermDocs segTermDocs = new SegmentTermDocs(reader);
-      assertTrue(segTermDocs != null);
-      segTermDocs.seek(new Term("junk", "bad"));
-      assertTrue(segTermDocs.next() == false);
+      TermDocs termDocs = reader.termDocs();
+      assertTrue(termDocs != null);
+      termDocs.seek(new Term("junk", "bad"));
+      assertTrue(termDocs.next() == false);
       reader.close();
     }
   }
Index: tags/lucene_2_9_back_compat_tests_20090930a/src/test/org/apache/lucene/index/TestIndexWriter.java
===================================================================
--- tags/lucene_2_9_back_compat_tests_20090930a/src/test/org/apache/lucene/index/TestIndexWriter.java	(revision 821733)
+++ tags/lucene_2_9_back_compat_tests_20090930a/src/test/org/apache/lucene/index/TestIndexWriter.java	(working copy)
@@ -61,6 +61,7 @@
 import org.apache.lucene.store.IndexOutput;
 import org.apache.lucene.store.Lock;
 import org.apache.lucene.store.LockFactory;
+import org.apache.lucene.store.NoLockFactory;
 import org.apache.lucene.store.MockRAMDirectory;
 import org.apache.lucene.store.RAMDirectory;
 import org.apache.lucene.store.SingleInstanceLockFactory;
@@ -554,10 +555,15 @@
     }                                               
 
     public static void assertNoUnreferencedFiles(Directory dir, String message) throws IOException {
-      String[] startFiles = dir.listAll();
-      SegmentInfos infos = new SegmentInfos();
-      infos.read(dir);
-      new IndexFileDeleter(dir, new KeepOnlyLastCommitDeletionPolicy(), infos, null, null);
+      final LockFactory lf = dir.getLockFactory();
+      String[] startFiles;
+      try {
+        dir.setLockFactory(new NoLockFactory());
+        startFiles = dir.list();
+        new IndexWriter(dir, new WhitespaceAnalyzer(), IndexWriter.MaxFieldLength.UNLIMITED).close();
+      } finally {
+        dir.setLockFactory(lf);
+      }
       String[] endFiles = dir.listAll();
 
       Arrays.sort(startFiles);
@@ -3563,6 +3569,7 @@
     w.addDocument(doc);
     w.commit();
 
+    System.out.println("\nTEST: now search");
     IndexSearcher s = new IndexSearcher(dir);
     PhraseQuery pq = new PhraseQuery();
     pq.add(new Term("field", "a"));
@@ -4420,10 +4427,6 @@
 
       assertTrue(dir.fileExists("myrandomfile"));
 
-      // Make sure this does not copy myrandomfile:
-      Directory dir2 = new RAMDirectory(dir);
-      assertTrue(!dir2.fileExists("myrandomfile"));
-
     } finally {
       dir.close();
       _TestUtil.rmDir(indexDir);
Index: tags/lucene_2_9_back_compat_tests_20090930a/src/test/org/apache/lucene/index/TestMultiLevelSkipList.java
===================================================================
--- tags/lucene_2_9_back_compat_tests_20090930a/src/test/org/apache/lucene/index/TestMultiLevelSkipList.java	(revision 821733)
+++ tags/lucene_2_9_back_compat_tests_20090930a/src/test/org/apache/lucene/index/TestMultiLevelSkipList.java	(working copy)
@@ -30,7 +30,8 @@
 import org.apache.lucene.document.Field.Index;
 import org.apache.lucene.document.Field.Store;
 import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.store.RAMDirectory;
+import org.apache.lucene.store.MockRAMDirectory;
+import org.apache.lucene.store.Directory;
 import org.apache.lucene.util.LuceneTestCase;
 
 /**
@@ -42,8 +43,18 @@
  * 
  */
 public class TestMultiLevelSkipList extends LuceneTestCase {
+
+  class CountingRAMDirectory extends MockRAMDirectory {
+    public IndexInput openInput(String fileName) throws IOException {
+      IndexInput in = super.openInput(fileName);
+      if (fileName.endsWith(".frq"))
+        in = new CountingStream(in);
+      return in;
+    }
+  }
+
   public void testSimpleSkip() throws IOException {
-    RAMDirectory dir = new RAMDirectory();
+    Directory dir = new CountingRAMDirectory();
     IndexWriter writer = new IndexWriter(dir, new PayloadAnalyzer(), true,
                                          IndexWriter.MaxFieldLength.LIMITED);
     Term term = new Term("test", "a");
@@ -57,8 +68,7 @@
     writer.close();
 
     IndexReader reader = SegmentReader.getOnlySegmentReader(dir);
-    SegmentTermPositions tp = (SegmentTermPositions) reader.termPositions();
-    tp.freqStream = new CountingStream(tp.freqStream);
+    TermPositions tp = reader.termPositions();
 
     for (int i = 0; i < 2; i++) {
       counter = 0;
Index: tags/lucene_2_9_back_compat_tests_20090930a/src/test/org/apache/lucene/index/TestIndexWriterDelete.java
===================================================================
--- tags/lucene_2_9_back_compat_tests_20090930a/src/test/org/apache/lucene/index/TestIndexWriterDelete.java	(revision 821733)
+++ tags/lucene_2_9_back_compat_tests_20090930a/src/test/org/apache/lucene/index/TestIndexWriterDelete.java	(working copy)
@@ -812,18 +812,7 @@
         }
       }
 
-      String[] startFiles = dir.listAll();
-      SegmentInfos infos = new SegmentInfos();
-      infos.read(dir);
-      new IndexFileDeleter(dir, new KeepOnlyLastCommitDeletionPolicy(), infos, null, null);
-      String[] endFiles = dir.listAll();
-
-      if (!Arrays.equals(startFiles, endFiles)) {
-        fail("docswriter abort() failed to delete unreferenced files:\n  before delete:\n    "
-             + arrayToString(startFiles) + "\n  after delete:\n    "
-             + arrayToString(endFiles));
-      }
-
+      TestIndexWriter.assertNoUnreferencedFiles(dir, "docsWriter.abort() failed to delete unreferenced files");
       modifier.close();
 
     }
Index: tags/lucene_2_9_back_compat_tests_20090930a/src/test/org/apache/lucene/index/TestPayloads.java
===================================================================
--- tags/lucene_2_9_back_compat_tests_20090930a/src/test/org/apache/lucene/index/TestPayloads.java	(revision 821733)
+++ tags/lucene_2_9_back_compat_tests_20090930a/src/test/org/apache/lucene/index/TestPayloads.java	(working copy)
@@ -257,10 +257,12 @@
         tp.next();
         tp.nextPosition();
         // now we don't read this payload
+        tp.next();
         tp.nextPosition();
         assertEquals("Wrong payload length.", 1, tp.getPayloadLength());
         byte[] payload = tp.getPayload(null, 0);
         assertEquals(payload[0], payloadData[numTerms]);
+        tp.next();
         tp.nextPosition();
         
         // we don't read this payload and skip to a different document
Index: tags/lucene_2_9_back_compat_tests_20090930a/src/test/org/apache/lucene/index/TestSegmentReader.java
===================================================================
--- tags/lucene_2_9_back_compat_tests_20090930a/src/test/org/apache/lucene/index/TestSegmentReader.java	(revision 821733)
+++ tags/lucene_2_9_back_compat_tests_20090930a/src/test/org/apache/lucene/index/TestSegmentReader.java	(working copy)
@@ -136,6 +136,7 @@
     TermPositions positions = reader.termPositions();
     positions.seek(new Term(DocHelper.TEXT_FIELD_1_KEY, "field"));
     assertTrue(positions != null);
+    assertTrue(positions.next());
     assertTrue(positions.doc() == 0);
     assertTrue(positions.nextPosition() >= 0);
   }    
Index: tags/lucene_2_9_back_compat_tests_20090930a/src/test/org/apache/lucene/index/TestLazyProxSkipping.java
===================================================================
--- tags/lucene_2_9_back_compat_tests_20090930a/src/test/org/apache/lucene/index/TestLazyProxSkipping.java	(revision 821733)
+++ tags/lucene_2_9_back_compat_tests_20090930a/src/test/org/apache/lucene/index/TestLazyProxSkipping.java	(working copy)
@@ -47,7 +47,7 @@
     private class SeekCountingDirectory extends RAMDirectory {
       public IndexInput openInput(String name) throws IOException {
         IndexInput ii = super.openInput(name);
-        if (name.endsWith(".prx")) {
+        if (name.endsWith(".prx") || name.endsWith(".pos")) {
           // we decorate the proxStream with a wrapper class that allows to count the number of calls of seek()
           ii = new SeeksCountingStream(ii);
         }
