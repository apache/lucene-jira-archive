diff --git a/lucene/core/src/java/org/apache/lucene/codecs/blocktree/BlockTreeTermsWriter.java b/lucene/core/src/java/org/apache/lucene/codecs/blocktree/BlockTreeTermsWriter.java
index eb242d8..4c965f5 100644
--- a/lucene/core/src/java/org/apache/lucene/codecs/blocktree/BlockTreeTermsWriter.java
+++ b/lucene/core/src/java/org/apache/lucene/codecs/blocktree/BlockTreeTermsWriter.java
@@ -35,6 +35,8 @@ import org.apache.lucene.index.PostingsEnum;
 import org.apache.lucene.index.SegmentWriteState;
 import org.apache.lucene.index.Terms;
 import org.apache.lucene.index.TermsEnum;
+import org.apache.lucene.store.ByteArrayDataInput;
+import org.apache.lucene.store.ByteArrayDataOutput;
 import org.apache.lucene.store.DataOutput;
 import org.apache.lucene.store.IndexOutput;
 import org.apache.lucene.store.RAMOutputStream;
@@ -541,16 +543,58 @@ public final class BlockTreeTermsWriter extends FieldsConsumer {
     return brToString(new BytesRef(b));
   }
 
+  @SuppressWarnings("unchecked")
+  static FST<BytesRef> indexToFST(Object _index) throws IOException {
+    if (_index instanceof FST) {
+      // already done
+      return (FST<BytesRef>) _index;
+    }
+    IntsRefBuilder scratchIntsRef = new IntsRefBuilder();
+    ByteArrayDataOutput index = (ByteArrayDataOutput) _index;
+    int length = index.getPosition();
+    ByteArrayDataInput in = new ByteArrayDataInput(index.getBytes(), 0, length);
+
+    ByteSequenceOutputs outputs = ByteSequenceOutputs.getSingleton();
+    Builder<BytesRef> builder = new Builder<>(FST.INPUT_TYPE.BYTE1,
+                                              0, 0, true, false, Integer.MAX_VALUE,
+                                              outputs, false,
+                                              PackedInts.COMPACT, true, 15);
+    BytesRefBuilder prefix = new BytesRefBuilder();
+    BytesRefBuilder output = new BytesRefBuilder();
+    System.out.println("TO FST: " + length);
+
+    while (in.getPosition() != length) {
+      int len = in.readVInt();
+
+      prefix.grow(len);
+      prefix.clear();
+      in.readBytes(prefix.bytes(), 0, len);
+      prefix.setLength(len);
+
+      len = in.readVInt();
+      output.grow(len);
+      output.clear();
+      in.readBytes(output.bytes(), 0, len);
+      output.setLength(len);
+      
+      builder.add(Util.toIntsRef(prefix.get(), scratchIntsRef), BytesRef.deepCopyOf(output.get()));
+    }
+
+    return builder.finish();
+  }
+
   private static final class PendingBlock extends PendingEntry {
     public final BytesRef prefix;
     public final long fp;
-    public FST<BytesRef> index;
-    public List<FST<BytesRef>> subIndices;
+    // Either an FST or a RAMOutputStream:
+    public Object index;
+    // List of <either an FST or a RAMOutputStream>:
+    public List<Object> subIndices;
     public final boolean hasTerms;
     public final boolean isFloor;
     public final int floorLeadByte;
 
-    public PendingBlock(BytesRef prefix, long fp, boolean hasTerms, boolean isFloor, int floorLeadByte, List<FST<BytesRef>> subIndices) {
+    public PendingBlock(BytesRef prefix, long fp, boolean hasTerms, boolean isFloor, int floorLeadByte, List<Object> subIndices) {
       super(false);
       this.prefix = prefix;
       this.fp = fp;
@@ -567,6 +611,8 @@ public final class BlockTreeTermsWriter extends FieldsConsumer {
 
     public void compileIndex(List<PendingBlock> blocks, RAMOutputStream scratchBytes, IntsRefBuilder scratchIntsRef) throws IOException {
 
+      boolean useFST = false;
+
       assert (isFloor && blocks.size() > 1) || (isFloor == false && blocks.size() == 1): "isFloor=" + isFloor + " blocks=" + blocks;
       assert this == blocks.get(0);
 
@@ -590,32 +636,64 @@ public final class BlockTreeTermsWriter extends FieldsConsumer {
         }
       }
 
-      final ByteSequenceOutputs outputs = ByteSequenceOutputs.getSingleton();
-      final Builder<BytesRef> indexBuilder = new Builder<>(FST.INPUT_TYPE.BYTE1,
-                                                           0, 0, true, false, Integer.MAX_VALUE,
-                                                           outputs, false,
-                                                           PackedInts.COMPACT, true, 15);
-      //if (DEBUG) {
-      //  System.out.println("  compile index for prefix=" + prefix);
-      //}
-      //indexBuilder.DEBUG = false;
-      final byte[] bytes = new byte[(int) scratchBytes.getFilePointer()];
-      assert bytes.length > 0;
-      scratchBytes.writeTo(bytes, 0);
-      indexBuilder.add(Util.toIntsRef(prefix, scratchIntsRef), new BytesRef(bytes, 0, bytes.length));
-      scratchBytes.reset();
-
-      // Copy over index for all sub-blocks
-      for(PendingBlock block : blocks) {
-        if (block.subIndices != null) {
-          for(FST<BytesRef> subIndex : block.subIndices) {
-            append(indexBuilder, subIndex, scratchIntsRef);
+      // Logically, we just need to hold onto List<Pair<byte[], byte[]>>:
+      if (useFST) {
+
+        final ByteSequenceOutputs outputs = ByteSequenceOutputs.getSingleton();
+        final Builder<BytesRef> indexBuilder = new Builder<>(FST.INPUT_TYPE.BYTE1,
+                                                             0, 0, true, false, Integer.MAX_VALUE,
+                                                             outputs, false,
+                                                             PackedInts.COMPACT, true, 15);
+        final byte[] bytes = new byte[(int) scratchBytes.getFilePointer()];
+        assert bytes.length > 0;
+        scratchBytes.writeTo(bytes, 0);
+        scratchBytes.reset();
+
+        indexBuilder.add(Util.toIntsRef(prefix, scratchIntsRef), new BytesRef(bytes, 0, bytes.length));
+
+        // Copy over index for all sub-blocks
+        for(PendingBlock block : blocks) {
+          if (block.subIndices != null) {
+            for(Object subIndex : block.subIndices) {
+              append(indexBuilder, indexToFST(subIndex), scratchIntsRef);
+            }
+            block.subIndices = null;
           }
-          block.subIndices = null;
         }
-      }
 
-      index = indexBuilder.finish();
+        index = indexBuilder.finish();
+
+      } else {
+        byte[] bytes = new byte[16];
+        ByteArrayDataOutput bytesOut = new ByteArrayDataOutput(bytes);
+
+        writeVInt(bytesOut, prefix.length);
+
+        writeBytes(bytesOut, prefix.bytes, prefix.offset, prefix.length);
+
+        writeVInt(bytesOut, (int) scratchBytes.getFilePointer());
+
+        byte[] tempBytes = new byte[(int) scratchBytes.getFilePointer()];
+        assert tempBytes.length > 0;
+        scratchBytes.writeTo(tempBytes, 0);
+        scratchBytes.reset();
+
+        writeBytes(bytesOut, tempBytes, 0, tempBytes.length);
+
+        // Copy over index for all sub-blocks
+        for(PendingBlock block : blocks) {
+          if (block.subIndices != null) {
+            for(Object subIndex : block.subIndices) {
+              assert subIndex instanceof ByteArrayDataOutput;
+              ByteArrayDataOutput bytesIn = (ByteArrayDataOutput) subIndex;
+              writeBytes(bytesOut, bytesIn.getBytes(), 0, bytesIn.getPosition());
+            }
+            block.subIndices = null;
+          }
+        }
+
+        index = bytesOut;
+      }
 
       assert subIndices == null;
 
@@ -642,6 +720,26 @@ public final class BlockTreeTermsWriter extends FieldsConsumer {
     }
   }
 
+  static void writeVInt(ByteArrayDataOutput bytesOut, int x) throws IOException {
+    int pos = bytesOut.getPosition();
+    int lenNeeded = pos + 5;
+    if (lenNeeded > bytesOut.getBytes().length) {
+      byte[] newBytes = ArrayUtil.grow(bytesOut.getBytes(), lenNeeded);
+      bytesOut.reset(newBytes, pos, newBytes.length - pos);
+    }
+    bytesOut.writeVInt(x);
+  }
+
+  static void writeBytes(ByteArrayDataOutput bytesOut, byte[] bytes, int offset, int length) throws IOException {
+    int pos = bytesOut.getPosition();
+    int lenNeeded = pos + length;
+    if (lenNeeded > bytesOut.getBytes().length) {
+      byte[] newBytes = ArrayUtil.grow(bytesOut.getBytes(), lenNeeded);
+      bytesOut.reset(newBytes, pos, newBytes.length - pos);
+    }
+    bytesOut.writeBytes(bytes, offset, length);
+  }
+
   private final RAMOutputStream scratchBytes = new RAMOutputStream();
   private final IntsRefBuilder scratchIntsRef = new IntsRefBuilder();
 
@@ -827,7 +925,7 @@ public final class BlockTreeTermsWriter extends FieldsConsumer {
 
       //System.out.println("  isLeaf=" + isLeafBlock);
 
-      final List<FST<BytesRef>> subIndices;
+      final List<Object> subIndices;
 
       boolean absolute = true;
 
@@ -1117,18 +1215,21 @@ public final class BlockTreeTermsWriter extends FieldsConsumer {
         assert pending.size() == 1 && !pending.get(0).isTerm: "pending.size()=" + pending.size() + " pending=" + pending;
         final PendingBlock root = (PendingBlock) pending.get(0);
         assert root.prefix.length == 0;
-        assert root.index.getEmptyOutput() != null;
+
+        FST<BytesRef> index = indexToFST(root.index);
+        assert index.getEmptyOutput() != null;
+        assert index.getEmptyOutput().length != 0;
 
         // Write FST to index
         indexStartFP = indexOut.getFilePointer();
-        root.index.save(indexOut);
+        index.save(indexOut);
         //System.out.println("  write FST " + indexStartFP + " field=" + fieldInfo.name);
 
         /*
         if (DEBUG) {
           final String dotFileName = segment + "_" + fieldInfo.name + ".dot";
           Writer w = new OutputStreamWriter(new FileOutputStream(dotFileName));
-          Util.toDot(root.index, w, false, false);
+          Util.toDot(index, w, false, false);
           System.out.println("SAVED to " + dotFileName);
           w.close();
         }
@@ -1140,7 +1241,7 @@ public final class BlockTreeTermsWriter extends FieldsConsumer {
         BytesRef maxTerm = new BytesRef(lastPendingTerm.termBytes);
 
         fields.add(new FieldMetaData(fieldInfo,
-                                     ((PendingBlock) pending.get(0)).index.getEmptyOutput(),
+                                     index.getEmptyOutput(),
                                      numTerms,
                                      indexStartFP,
                                      sumTotalTermFreq,
diff --git a/lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain.java b/lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain.java
index 788fa29..011b690 100644
--- a/lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain.java
+++ b/lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain.java
@@ -91,16 +91,35 @@ final class DefaultIndexingChain extends DocConsumer {
     // aborting on any exception from this method
 
     int maxDoc = state.segmentInfo.maxDoc();
+    long t0 = System.nanoTime();
     writeNorms(state);
+    if (docState.infoStream.isEnabled("IW")) {
+      docState.infoStream.message("IW", ((System.nanoTime()-t0)/1000000) + " msec to write norms for segment=" + docWriter.getSegmentInfo().name);
+    }
+    
+    t0 = System.nanoTime();
     writeDocValues(state);
+    if (docState.infoStream.isEnabled("IW")) {
+      docState.infoStream.message("IW", ((System.nanoTime()-t0)/1000000) + " msec to write docValues for segment=" + docWriter.getSegmentInfo().name);
+    }
+
+    t0 = System.nanoTime();
     writePoints(state);
+    if (docState.infoStream.isEnabled("IW")) {
+      docState.infoStream.message("IW", ((System.nanoTime()-t0)/1000000) + " msec to write points for segment=" + docWriter.getSegmentInfo().name);
+    }
     
     // it's possible all docs hit non-aborting exceptions...
+    t0 = System.nanoTime();
     initStoredFieldsWriter();
     fillStoredFields(maxDoc);
     storedFieldsWriter.finish(state.fieldInfos, maxDoc);
     storedFieldsWriter.close();
+    if (docState.infoStream.isEnabled("IW")) {
+      docState.infoStream.message("IW", ((System.nanoTime()-t0)/1000000) + " msec to finish stored fields for segment=" + docWriter.getSegmentInfo().name);
+    }
 
+    t0 = System.nanoTime();
     Map<String,TermsHashPerField> fieldsToFlush = new HashMap<>();
     for (int i=0;i<fieldHash.length;i++) {
       PerField perField = fieldHash[i];
@@ -113,12 +132,19 @@ final class DefaultIndexingChain extends DocConsumer {
     }
 
     termsHash.flush(fieldsToFlush, state);
+    if (docState.infoStream.isEnabled("IW")) {
+      docState.infoStream.message("IW", ((System.nanoTime()-t0)/1000000) + " msec to write postings for segment=" + docWriter.getSegmentInfo().name);
+    }
 
     // Important to save after asking consumer to flush so
     // consumer can alter the FieldInfo* if necessary.  EG,
     // FreqProxTermsWriter does this with
     // FieldInfo.storePayload.
+    t0 = System.nanoTime();
     docWriter.codec.fieldInfosFormat().write(state.directory, state.segmentInfo, "", state.fieldInfos, IOContext.DEFAULT);
+    if (docState.infoStream.isEnabled("IW")) {
+      docState.infoStream.message("IW", ((System.nanoTime()-t0)/1000000) + " msec to write fieldInfos for segment=" + docWriter.getSegmentInfo().name);
+    }
   }
 
   /** Writes all buffered points. */
diff --git a/lucene/core/src/java/org/apache/lucene/store/ByteArrayDataOutput.java b/lucene/core/src/java/org/apache/lucene/store/ByteArrayDataOutput.java
index 4a6599f..a10db09 100644
--- a/lucene/core/src/java/org/apache/lucene/store/ByteArrayDataOutput.java
+++ b/lucene/core/src/java/org/apache/lucene/store/ByteArrayDataOutput.java
@@ -43,6 +43,10 @@ public class ByteArrayDataOutput extends DataOutput {
     reset(BytesRef.EMPTY_BYTES);
   }
 
+  public byte[] getBytes() {
+    return bytes;
+  }
+
   public void reset(byte[] bytes) {
     reset(bytes, 0, bytes.length);
   }
diff --git a/lucene/core/src/java/org/apache/lucene/store/RAMOutputStream.java b/lucene/core/src/java/org/apache/lucene/store/RAMOutputStream.java
index 5bf8079..aefe16d 100644
--- a/lucene/core/src/java/org/apache/lucene/store/RAMOutputStream.java
+++ b/lucene/core/src/java/org/apache/lucene/store/RAMOutputStream.java
@@ -71,7 +71,7 @@ public class RAMOutputStream extends IndexOutput implements Accountable {
     }
   }
 
-  /** Copy the current contents of this buffer to the named output. */
+  /** Copy the current contents of this buffer to the provided {@link DataOutput}. */
   public void writeTo(DataOutput out) throws IOException {
     flush();
     final long end = file.length;
