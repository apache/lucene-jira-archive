Index: lucene/contrib/spatial/src/java/org/apache/lucene/spatial/tier/LatLongDistanceFilter.java
===================================================================
--- lucene/contrib/spatial/src/java/org/apache/lucene/spatial/tier/LatLongDistanceFilter.java	(revision 1177602)
+++ lucene/contrib/spatial/src/java/org/apache/lucene/spatial/tier/LatLongDistanceFilter.java	(revision )
@@ -66,8 +66,8 @@
   @Override
   public DocIdSet getDocIdSet(IndexReader reader) throws IOException {
 
-    final double[] latIndex = FieldCache.DEFAULT.getDoubles(reader, latField);
-    final double[] lngIndex = FieldCache.DEFAULT.getDoubles(reader, lngField);
+    final double[] latIndex = reader.getFieldCache().getDoubles(latField);
+    final double[] lngIndex = reader.getFieldCache().getDoubles(lngField);
 
     final int docBase = nextDocBase;
     nextDocBase += reader.maxDoc();
Index: lucene/src/java/org/apache/lucene/search/FieldCacheImpl.java
===================================================================
--- lucene/src/java/org/apache/lucene/search/FieldCacheImpl.java	(revision 1177602)
+++ lucene/src/java/org/apache/lucene/search/FieldCacheImpl.java	(revision )
@@ -26,10 +26,7 @@
 import java.util.Map;
 import java.util.WeakHashMap;
 
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.Term;
-import org.apache.lucene.index.TermDocs;
-import org.apache.lucene.index.TermEnum;
+import org.apache.lucene.index.*;
 import org.apache.lucene.util.Bits;
 import org.apache.lucene.util.FixedBitSet;
 import org.apache.lucene.util.StringHelper;
@@ -43,706 +40,114 @@
  *
  * @since   lucene 1.4
  */
+@Deprecated
 class FieldCacheImpl implements FieldCache {
 	
-  private Map<Class<?>,Cache> caches;
   FieldCacheImpl() {
-    init();
   }
-  private synchronized void init() {
-    caches = new HashMap<Class<?>,Cache>(9);
-    caches.put(Byte.TYPE, new ByteCache(this));
-    caches.put(Short.TYPE, new ShortCache(this));
-    caches.put(Integer.TYPE, new IntCache(this));
-    caches.put(Float.TYPE, new FloatCache(this));
-    caches.put(Long.TYPE, new LongCache(this));
-    caches.put(Double.TYPE, new DoubleCache(this));
-    caches.put(String.class, new StringCache(this));
-    caches.put(StringIndex.class, new StringIndexCache(this));
-    caches.put(DocsWithFieldCache.class, new DocsWithFieldCache(this));
-  }
 
   public synchronized void purgeAllCaches() {
-    init();
+    SlowMultiReaderWrapper.getNonAtomicFieldCache().purgeAllCaches();
   }
 
   public synchronized void purge(IndexReader r) {
-    for(Cache c : caches.values()) {
-      c.purge(r);
+    new SlowMultiReaderWrapper(r).getFieldCache().purgeCache();
-    }
+  }
-  }
-  
+
   public synchronized CacheEntry[] getCacheEntries() {
-    List<CacheEntry> result = new ArrayList<CacheEntry>(17);
-    for(final Map.Entry<Class<?>,Cache> cacheEntry: caches.entrySet()) {
-      final Cache cache = cacheEntry.getValue();
-      final Class<?> cacheType = cacheEntry.getKey();
-      synchronized(cache.readerCache) {
-        for (final Map.Entry<Object,Map<Entry, Object>> readerCacheEntry : cache.readerCache.entrySet()) {
-          final Object readerKey = readerCacheEntry.getKey();
-          if (readerKey == null) continue;
-          final Map<Entry, Object> innerCache = readerCacheEntry.getValue();
-          for (final Map.Entry<Entry, Object> mapEntry : innerCache.entrySet()) {
-            Entry entry = mapEntry.getKey();
-            result.add(new CacheEntryImpl(readerKey, entry.field,
-                                          cacheType, entry.custom,
-                                          mapEntry.getValue()));
+    return SlowMultiReaderWrapper.getCacheEntries();
-          }
+  }
-        }
-      }
-    }
-    return result.toArray(new CacheEntry[result.size()]);
-  }
-  
+
-  private static final class CacheEntryImpl extends CacheEntry {
-    private final Object readerKey;
-    private final String fieldName;
-    private final Class<?> cacheType;
-    private final Object custom;
-    private final Object value;
-    CacheEntryImpl(Object readerKey, String fieldName,
-                   Class<?> cacheType,
-                   Object custom,
-                   Object value) {
-        this.readerKey = readerKey;
-        this.fieldName = fieldName;
-        this.cacheType = cacheType;
-        this.custom = custom;
-        this.value = value;
-
-        // :HACK: for testing.
-//         if (null != locale || SortField.CUSTOM != sortFieldType) {
-//           throw new RuntimeException("Locale/sortFieldType: " + this);
-//         }
-
-    }
-    @Override
-    public Object getReaderKey() { return readerKey; }
-    @Override
-    public String getFieldName() { return fieldName; }
-    @Override
-    public Class<?> getCacheType() { return cacheType; }
-    @Override
-    public Object getCustom() { return custom; }
-    @Override
-    public Object getValue() { return value; }
-  }
-
-  /**
-   * Hack: When thrown from a Parser (NUMERIC_UTILS_* ones), this stops
-   * processing terms and returns the current FieldCache
-   * array.
-   */
-  static final class StopFillCacheException extends RuntimeException {
-  }
-
-  final static IndexReader.ReaderFinishedListener purgeReader = new IndexReader.ReaderFinishedListener() {
-    // @Override -- not until Java 1.6
-    public void finished(IndexReader reader) {
-      FieldCache.DEFAULT.purge(reader);
-    }
-  };
-
-  /** Expert: Internal cache. */
-  abstract static class Cache {
-    Cache() {
-      this.wrapper = null;
-    }
-
-    Cache(FieldCache wrapper) {
-      this.wrapper = wrapper;
-    }
-
-    final FieldCache wrapper;
-
-    final Map<Object,Map<Entry,Object>> readerCache = new WeakHashMap<Object,Map<Entry,Object>>();
-    
-    protected abstract Object createValue(IndexReader reader, Entry key)
-        throws IOException;
-
-    /** Remove this reader from the cache, if present. */
-    public void purge(IndexReader r) {
-      Object readerKey = r.getCoreCacheKey();
-      synchronized(readerCache) {
-        readerCache.remove(readerKey);
-      }
-    }
-
-    public Object get(IndexReader reader, Entry key) throws IOException {
-      Map<Entry,Object> innerCache;
-      Object value;
-      final Object readerKey = reader.getCoreCacheKey();
-      synchronized (readerCache) {
-        innerCache = readerCache.get(readerKey);
-        if (innerCache == null) {
-          // First time this reader is using FieldCache
-          innerCache = new HashMap<Entry,Object>();
-          readerCache.put(readerKey, innerCache);
-          reader.addReaderFinishedListener(purgeReader);
-          value = null;
-        } else {
-          value = innerCache.get(key);
-        }
-        if (value == null) {
-          value = new CreationPlaceholder();
-          innerCache.put(key, value);
-        }
-      }
-      if (value instanceof CreationPlaceholder) {
-        synchronized (value) {
-          CreationPlaceholder progress = (CreationPlaceholder) value;
-          if (progress.value == null) {
-            progress.value = createValue(reader, key);
-            synchronized (readerCache) {
-              innerCache.put(key, progress.value);
-            }
-
-            // Only check if key.custom (the parser) is
-            // non-null; else, we check twice for a single
-            // call to FieldCache.getXXX
-            if (key.custom != null && wrapper != null) {
-              final PrintStream infoStream = wrapper.getInfoStream();
-              if (infoStream != null) {
-                printNewInsanity(infoStream, progress.value);
-              }
-            }
-          }
-          return progress.value;
-        }
-      }
-      return value;
-    }
-
-    private void printNewInsanity(PrintStream infoStream, Object value) {
-      final FieldCacheSanityChecker.Insanity[] insanities = FieldCacheSanityChecker.checkSanity(wrapper);
-      for(int i=0;i<insanities.length;i++) {
-        final FieldCacheSanityChecker.Insanity insanity = insanities[i];
-        final CacheEntry[] entries = insanity.getCacheEntries();
-        for(int j=0;j<entries.length;j++) {
-          if (entries[j].getValue() == value) {
-            // OK this insanity involves our entry
-            infoStream.println("WARNING: new FieldCache insanity created\nDetails: " + insanity.toString());
-            infoStream.println("\nStack:\n");
-            new Throwable().printStackTrace(infoStream);
-            break;
-          }
-        }
-      }
-    }
-  }
-
-  /** Expert: Every composite-key in the internal cache is of this type. */
-  static class Entry {
-    final String field;        // which Fieldable
-    final Object custom;       // which custom comparator or parser
-
-    /** Creates one of these objects for a custom comparator/parser. */
-    Entry (String field, Object custom) {
-      this.field = StringHelper.intern(field);
-      this.custom = custom;
-    }
-
-    /** Two of these are equal iff they reference the same field and type. */
-    @Override
-    public boolean equals (Object o) {
-      if (o instanceof Entry) {
-        Entry other = (Entry) o;
-        if (other.field == field) {
-          if (other.custom == null) {
-            if (custom == null) return true;
-          } else if (other.custom.equals (custom)) {
-            return true;
-          }
-        }
-      }
-      return false;
-    }
-
-    /** Composes a hashcode based on the field and type. */
-    @Override
-    public int hashCode() {
-      return field.hashCode() ^ (custom==null ? 0 : custom.hashCode());
-    }
-  }
-
   // inherit javadocs
   public byte[] getBytes (IndexReader reader, String field) throws IOException {
-    return getBytes(reader, field, null);
+    return new SlowMultiReaderWrapper(reader).getFieldCache().getBytes(field);
   }
 
   // inherit javadocs
   public byte[] getBytes(IndexReader reader, String field, ByteParser parser)
       throws IOException {
-    return (byte[]) caches.get(Byte.TYPE).get(reader, new Entry(field, parser));
+    return new SlowMultiReaderWrapper(reader).getFieldCache().getBytes(field, parser);
   }
-
+  
-  static final class ByteCache extends Cache {
-    ByteCache(FieldCache wrapper) {
-      super(wrapper);
-    }
-    @Override
-    protected Object createValue(IndexReader reader, Entry entryKey)
-        throws IOException {
-      Entry entry = entryKey;
-      String field = entry.field;
-      ByteParser parser = (ByteParser) entry.custom;
-      if (parser == null) {
-        return wrapper.getBytes(reader, field, FieldCache.DEFAULT_BYTE_PARSER);
-      }
-      final byte[] retArray = new byte[reader.maxDoc()];
-      TermDocs termDocs = reader.termDocs();
-      TermEnum termEnum = reader.terms (new Term (field));
-      try {
-        do {
-          Term term = termEnum.term();
-          if (term==null || term.field() != field) break;
-          byte termval = parser.parseByte(term.text());
-          termDocs.seek (termEnum);
-          while (termDocs.next()) {
-            retArray[termDocs.doc()] = termval;
-          }
-        } while (termEnum.next());
-      } catch (StopFillCacheException stop) {
-      } finally {
-        termDocs.close();
-        termEnum.close();
-      }
-      return retArray;
-    }
-  }
-  
   // inherit javadocs
   public short[] getShorts (IndexReader reader, String field) throws IOException {
-    return getShorts(reader, field, null);
+    return new SlowMultiReaderWrapper(reader).getFieldCache().getShorts(field);
   }
 
   // inherit javadocs
   public short[] getShorts(IndexReader reader, String field, ShortParser parser)
       throws IOException {
-    return (short[]) caches.get(Short.TYPE).get(reader, new Entry(field, parser));
+    return new SlowMultiReaderWrapper(reader).getFieldCache().getShorts(field, parser);
   }
-
+  
-  static final class ShortCache extends Cache {
-    ShortCache(FieldCache wrapper) {
-      super(wrapper);
-    }
-
-    @Override
-    protected Object createValue(IndexReader reader, Entry entryKey)
-        throws IOException {
-      Entry entry =  entryKey;
-      String field = entry.field;
-      ShortParser parser = (ShortParser) entry.custom;
-      if (parser == null) {
-        return wrapper.getShorts(reader, field, FieldCache.DEFAULT_SHORT_PARSER);
-      }
-      final short[] retArray = new short[reader.maxDoc()];
-      TermDocs termDocs = reader.termDocs();
-      TermEnum termEnum = reader.terms (new Term (field));
-      try {
-        do {
-          Term term = termEnum.term();
-          if (term==null || term.field() != field) break;
-          short termval = parser.parseShort(term.text());
-          termDocs.seek (termEnum);
-          while (termDocs.next()) {
-            retArray[termDocs.doc()] = termval;
-          }
-        } while (termEnum.next());
-      } catch (StopFillCacheException stop) {
-      } finally {
-        termDocs.close();
-        termEnum.close();
-      }
-      return retArray;
-    }
-  }
-  
   // inherit javadocs
   public int[] getInts (IndexReader reader, String field) throws IOException {
-    return getInts(reader, field, null);
+    return new SlowMultiReaderWrapper(reader).getFieldCache().getInts(field);
   }
 
   // inherit javadocs
   public int[] getInts(IndexReader reader, String field, IntParser parser)
       throws IOException {
-    return (int[]) caches.get(Integer.TYPE).get(reader, new Entry(field, parser));
+    return new SlowMultiReaderWrapper(reader).getFieldCache().getInts(field, parser);
   }
-
+  
-  static final class IntCache extends Cache {
-    IntCache(FieldCache wrapper) {
-      super(wrapper);
-    }
-
-    @Override
-    protected Object createValue(IndexReader reader, Entry entryKey)
-        throws IOException {
-      Entry entry = entryKey;
-      String field = entry.field;
-      IntParser parser = (IntParser) entry.custom;
-      if (parser == null) {
-        try {
-          return wrapper.getInts(reader, field, DEFAULT_INT_PARSER);
-        } catch (NumberFormatException ne) {
-          return wrapper.getInts(reader, field, NUMERIC_UTILS_INT_PARSER);      
-        }
-      }
-      int[] retArray = null;
-      TermDocs termDocs = reader.termDocs();
-      TermEnum termEnum = reader.terms (new Term (field));
-      try {
-        do {
-          Term term = termEnum.term();
-          if (term==null || term.field() != field) break;
-          int termval = parser.parseInt(term.text());
-          if (retArray == null) // late init
-            retArray = new int[reader.maxDoc()];
-          termDocs.seek (termEnum);
-          while (termDocs.next()) {
-            retArray[termDocs.doc()] = termval;
-          }
-        } while (termEnum.next());
-      } catch (StopFillCacheException stop) {
-      } finally {
-        termDocs.close();
-        termEnum.close();
-      }
-      if (retArray == null) // no values
-        retArray = new int[reader.maxDoc()];
-      return retArray;
-    }
-  }
-  
   public Bits getDocsWithField(IndexReader reader, String field)
       throws IOException {
-    return (Bits) caches.get(DocsWithFieldCache.class).get(reader, new Entry(field, null));
+    return new SlowMultiReaderWrapper(reader).getFieldCache().getDocsWithField(field);
   }
 
-  static final class DocsWithFieldCache extends Cache {
-    DocsWithFieldCache(FieldCache wrapper) {
-      super(wrapper);
-    }
-    
-    @Override
-    protected Object createValue(IndexReader reader, Entry entryKey)
-    throws IOException {
-      final Entry entry = entryKey;
-      final String field = entry.field;      
-      FixedBitSet res = null;
-      final TermDocs termDocs = reader.termDocs();
-      final TermEnum termEnum = reader.terms(new Term(field));
-      try {
-        do {
-          final Term term = termEnum.term();
-          if (term == null || term.field() != field) break;
-          if (res == null) // late init
-            res = new FixedBitSet(reader.maxDoc());
-          termDocs.seek(termEnum);
-          while (termDocs.next()) {
-            res.set(termDocs.doc());
-          }
-        } while (termEnum.next());
-      } finally {
-        termDocs.close();
-        termEnum.close();
-      }
-      if (res == null)
-        return new Bits.MatchNoBits(reader.maxDoc());
-      final int numSet = res.cardinality();
-      if (numSet >= reader.numDocs()) {
-        // The cardinality of the BitSet is numDocs if all documents have a value.
-        // As deleted docs are not in TermDocs, this is always true
-        assert numSet == reader.numDocs();
-        return new Bits.MatchAllBits(reader.maxDoc());
-      }
-      return res;
-    }
-  }
-
   // inherit javadocs
   public float[] getFloats (IndexReader reader, String field)
     throws IOException {
-    return getFloats(reader, field, null);
+    return new SlowMultiReaderWrapper(reader).getFieldCache().getFloats(field);
   }
 
   // inherit javadocs
   public float[] getFloats(IndexReader reader, String field, FloatParser parser)
     throws IOException {
-
-    return (float[]) caches.get(Float.TYPE).get(reader, new Entry(field, parser));
+    return new SlowMultiReaderWrapper(reader).getFieldCache().getFloats(field, parser);
   }
 
-  static final class FloatCache extends Cache {
-    FloatCache(FieldCache wrapper) {
-      super(wrapper);
-    }
-
-    @Override
-    protected Object createValue(IndexReader reader, Entry entryKey)
-        throws IOException {
-      Entry entry = entryKey;
-      String field = entry.field;
-      FloatParser parser = (FloatParser) entry.custom;
-      if (parser == null) {
-        try {
-          return wrapper.getFloats(reader, field, DEFAULT_FLOAT_PARSER);
-        } catch (NumberFormatException ne) {
-          return wrapper.getFloats(reader, field, NUMERIC_UTILS_FLOAT_PARSER);      
-        }
-    }
-      float[] retArray = null;
-      TermDocs termDocs = reader.termDocs();
-      TermEnum termEnum = reader.terms (new Term (field));
-      try {
-        do {
-          Term term = termEnum.term();
-          if (term==null || term.field() != field) break;
-          float termval = parser.parseFloat(term.text());
-          if (retArray == null) // late init
-            retArray = new float[reader.maxDoc()];
-          termDocs.seek (termEnum);
-          while (termDocs.next()) {
-            retArray[termDocs.doc()] = termval;
-          }
-        } while (termEnum.next());
-      } catch (StopFillCacheException stop) {
-      } finally {
-        termDocs.close();
-        termEnum.close();
-      }
-      if (retArray == null) // no values
-        retArray = new float[reader.maxDoc()];
-      return retArray;
-    }
-  }
-
-
   public long[] getLongs(IndexReader reader, String field) throws IOException {
-    return getLongs(reader, field, null);
+    return new SlowMultiReaderWrapper(reader).getFieldCache().getLongs(field);
   }
-  
+
   // inherit javadocs
   public long[] getLongs(IndexReader reader, String field, FieldCache.LongParser parser)
       throws IOException {
-    return (long[]) caches.get(Long.TYPE).get(reader, new Entry(field, parser));
+    return new SlowMultiReaderWrapper(reader).getFieldCache().getLongs(field, parser);
   }
 
-  static final class LongCache extends Cache {
-    LongCache(FieldCache wrapper) {
-      super(wrapper);
-    }
-
-    @Override
-    protected Object createValue(IndexReader reader, Entry entry)
-        throws IOException {
-      String field = entry.field;
-      FieldCache.LongParser parser = (FieldCache.LongParser) entry.custom;
-      if (parser == null) {
-        try {
-          return wrapper.getLongs(reader, field, DEFAULT_LONG_PARSER);
-        } catch (NumberFormatException ne) {
-          return wrapper.getLongs(reader, field, NUMERIC_UTILS_LONG_PARSER);      
-        }
-      }
-      long[] retArray = null;
-      TermDocs termDocs = reader.termDocs();
-      TermEnum termEnum = reader.terms (new Term(field));
-      try {
-        do {
-          Term term = termEnum.term();
-          if (term==null || term.field() != field) break;
-          long termval = parser.parseLong(term.text());
-          if (retArray == null) // late init
-            retArray = new long[reader.maxDoc()];
-          termDocs.seek (termEnum);
-          while (termDocs.next()) {
-            retArray[termDocs.doc()] = termval;
-          }
-        } while (termEnum.next());
-      } catch (StopFillCacheException stop) {
-      } finally {
-        termDocs.close();
-        termEnum.close();
-      }
-      if (retArray == null) // no values
-        retArray = new long[reader.maxDoc()];
-      return retArray;
-    }
-  }
-
   // inherit javadocs
   public double[] getDoubles(IndexReader reader, String field)
     throws IOException {
-    return getDoubles(reader, field, null);
+    return new SlowMultiReaderWrapper(reader).getFieldCache().getDoubles(field);
   }
 
   // inherit javadocs
   public double[] getDoubles(IndexReader reader, String field, FieldCache.DoubleParser parser)
       throws IOException {
-    return (double[]) caches.get(Double.TYPE).get(reader, new Entry(field, parser));
+    return new SlowMultiReaderWrapper(reader).getFieldCache().getDoubles(field, parser);
   }
 
-  static final class DoubleCache extends Cache {
-    DoubleCache(FieldCache wrapper) {
-      super(wrapper);
-    }
-
-    @Override
-    protected Object createValue(IndexReader reader, Entry entryKey)
-        throws IOException {
-      Entry entry = entryKey;
-      String field = entry.field;
-      FieldCache.DoubleParser parser = (FieldCache.DoubleParser) entry.custom;
-      if (parser == null) {
-        try {
-          return wrapper.getDoubles(reader, field, DEFAULT_DOUBLE_PARSER);
-        } catch (NumberFormatException ne) {
-          return wrapper.getDoubles(reader, field, NUMERIC_UTILS_DOUBLE_PARSER);      
-        }
-      }
-      double[] retArray = null;
-      TermDocs termDocs = reader.termDocs();
-      TermEnum termEnum = reader.terms (new Term (field));
-      try {
-        do {
-          Term term = termEnum.term();
-          if (term==null || term.field() != field) break;
-          double termval = parser.parseDouble(term.text());
-          if (retArray == null) // late init
-            retArray = new double[reader.maxDoc()];
-          termDocs.seek (termEnum);
-          while (termDocs.next()) {
-            retArray[termDocs.doc()] = termval;
-          }
-        } while (termEnum.next());
-      } catch (StopFillCacheException stop) {
-      } finally {
-        termDocs.close();
-        termEnum.close();
-      }
-      if (retArray == null) // no values
-        retArray = new double[reader.maxDoc()];
-      return retArray;
-    }
-  }
-
   // inherit javadocs
   public String[] getStrings(IndexReader reader, String field)
       throws IOException {
-    return (String[]) caches.get(String.class).get(reader, new Entry(field, (Parser)null));
+    return new SlowMultiReaderWrapper(reader).getFieldCache().getStrings(field);
   }
 
-  static final class StringCache extends Cache {
-    StringCache(FieldCache wrapper) {
-      super(wrapper);
-    }
-
-    @Override
-    protected Object createValue(IndexReader reader, Entry entryKey)
-        throws IOException {
-      String field = StringHelper.intern(entryKey.field);
-      final String[] retArray = new String[reader.maxDoc()];
-      TermDocs termDocs = reader.termDocs();
-      TermEnum termEnum = reader.terms (new Term (field));
-      final int termCountHardLimit = reader.maxDoc();
-      int termCount = 0;
-      try {
-        do {
-          if (termCount++ == termCountHardLimit) {
-            // app is misusing the API (there is more than
-            // one term per doc); in this case we make best
-            // effort to load what we can (see LUCENE-2142)
-            break;
-          }
-
-          Term term = termEnum.term();
-          if (term==null || term.field() != field) break;
-          String termval = term.text();
-          termDocs.seek (termEnum);
-          while (termDocs.next()) {
-            retArray[termDocs.doc()] = termval;
-          }
-        } while (termEnum.next());
-      } finally {
-        termDocs.close();
-        termEnum.close();
-      }
-      return retArray;
-    }
-  }
-
   // inherit javadocs
   public StringIndex getStringIndex(IndexReader reader, String field)
       throws IOException {
-    return (StringIndex) caches.get(StringIndex.class).get(reader, new Entry(field, (Parser)null));
+    return new SlowMultiReaderWrapper(reader).getFieldCache().getStringIndex(field);
   }
 
-  static final class StringIndexCache extends Cache {
-    StringIndexCache(FieldCache wrapper) {
-      super(wrapper);
-    }
-
-    @Override
-    protected Object createValue(IndexReader reader, Entry entryKey)
-        throws IOException {
-      String field = StringHelper.intern(entryKey.field);
-      final int[] retArray = new int[reader.maxDoc()];
-      String[] mterms = new String[reader.maxDoc()+1];
-      TermDocs termDocs = reader.termDocs();
-      TermEnum termEnum = reader.terms (new Term (field));
-      int t = 0;  // current term number
-
-      // an entry for documents that have no terms in this field
-      // should a document with no terms be at top or bottom?
-      // this puts them at the top - if it is changed, FieldDocSortedHitQueue
-      // needs to change as well.
-      mterms[t++] = null;
-
-      try {
-        do {
-          Term term = termEnum.term();
-          if (term==null || term.field() != field || t >= mterms.length) break;
-
-          // store term text
-          mterms[t] = term.text();
-
-          termDocs.seek (termEnum);
-          while (termDocs.next()) {
-            retArray[termDocs.doc()] = t;
-          }
-
-          t++;
-        } while (termEnum.next());
-      } finally {
-        termDocs.close();
-        termEnum.close();
-      }
-
-      if (t == 0) {
-        // if there are no terms, make the term array
-        // have a single null entry
-        mterms = new String[1];
-      } else if (t < mterms.length) {
-        // if there are less terms than documents,
-        // trim off the dead array space
-        String[] terms = new String[t];
-        System.arraycopy (mterms, 0, terms, 0, t);
-        mterms = terms;
-      }
-
-      StringIndex value = new StringIndex (retArray, mterms);
-      return value;
-    }
-  }
-
-  private volatile PrintStream infoStream;
-
   public void setInfoStream(PrintStream stream) {
-    infoStream = stream;
+    SlowMultiReaderWrapper.setInfoStream(stream);
   }
 
   public PrintStream getInfoStream() {
-    return infoStream;
+    return SlowMultiReaderWrapper.getInfoStream();
   }
 }
 
Index: lucene/contrib/grouping/src/java/org/apache/lucene/search/grouping/TermAllGroupHeadsCollector.java
===================================================================
--- lucene/contrib/grouping/src/java/org/apache/lucene/search/grouping/TermAllGroupHeadsCollector.java	(revision 1177602)
+++ lucene/contrib/grouping/src/java/org/apache/lucene/search/grouping/TermAllGroupHeadsCollector.java	(revision )
@@ -141,7 +141,7 @@
     public void setNextReader(IndexReader reader, int docBase) throws IOException {
       this.indexReader = reader;
       this.docBase = docBase;
-      groupIndex = FieldCache.DEFAULT.getStringIndex(reader, groupField);
+      groupIndex = reader.getFieldCache().getStringIndex(groupField);
 
       for (GroupHead groupHead : groups.values()) {
         for (int i = 0; i < groupHead.comparators.length; i++) {
@@ -244,13 +244,13 @@
     public void setNextReader(IndexReader reader, int docBase) throws IOException {
       this.indexReader = reader;
       this.docBase = docBase;
-      groupIndex = FieldCache.DEFAULT.getStringIndex(reader, groupField);
+      groupIndex = reader.getFieldCache().getStringIndex(groupField);
       for (int i = 0; i < fields.length; i++) {
         if (fields[i].getType() == SortField.SCORE) {
           continue;
         }
 
-        sortsIndex[i] = FieldCache.DEFAULT.getStringIndex(reader, fields[i].getField());
+        sortsIndex[i] = reader.getFieldCache().getStringIndex(fields[i].getField());
       }
 
       // Clear ordSet and fill it with previous encountered groups that can occur in the current segment.
@@ -392,9 +392,9 @@
     public void setNextReader(IndexReader reader, int docBase) throws IOException {
       this.indexReader = reader;
       this.docBase = docBase;
-      groupIndex = FieldCache.DEFAULT.getStringIndex(reader, groupField);
+      groupIndex = reader.getFieldCache().getStringIndex(groupField);
       for (int i = 0; i < fields.length; i++) {
-        sortsIndex[i] = FieldCache.DEFAULT.getStringIndex(reader, fields[i].getField());
+        sortsIndex[i] = reader.getFieldCache().getStringIndex(fields[i].getField());
       }
 
       // Clear ordSet and fill it with previous encountered groups that can occur in the current segment.
@@ -511,7 +511,7 @@
     public void setNextReader(IndexReader reader, int docBase) throws IOException {
       this.indexReader = reader;
       this.docBase = docBase;
-      groupIndex = FieldCache.DEFAULT.getStringIndex(reader, groupField);
+      groupIndex = reader.getFieldCache().getStringIndex(groupField);
 
       // Clear ordSet and fill it with previous encountered groups that can occur in the current segment.
       ordSet.clear();
Index: lucene/src/java/org/apache/lucene/search/FieldCache.java
===================================================================
--- lucene/src/java/org/apache/lucene/search/FieldCache.java	(revision 1177602)
+++ lucene/src/java/org/apache/lucene/search/FieldCache.java	(revision )
@@ -18,6 +18,7 @@
  */
 
 import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.search.cache.StopFillCacheException;
 import org.apache.lucene.util.Bits;
 import org.apache.lucene.util.NumericUtils;
 import org.apache.lucene.util.RamUsageEstimator;
@@ -41,7 +42,7 @@
 public interface FieldCache {
 
   public static final class CreationPlaceholder {
-    Object value;
+    public Object value;
   }
 
   /** Indicator for StringIndex values in the cache. */
@@ -145,6 +146,7 @@
   }
 
   /** Expert: The cache used internally by sorting and range query classes. */
+  @Deprecated
   public static FieldCache DEFAULT = new FieldCacheImpl();
   
   /** The default parser for byte values, which are encoded by {@link Byte#toString(byte)} */
@@ -239,7 +241,7 @@
     public int parseInt(String val) {
       final int shift = val.charAt(0)-NumericUtils.SHIFT_START_INT;
       if (shift>0 && shift<=31)
-        throw new FieldCacheImpl.StopFillCacheException();
+        throw new StopFillCacheException();
       return NumericUtils.prefixCodedToInt(val);
     }
     protected Object readResolve() {
@@ -259,7 +261,7 @@
     public float parseFloat(String val) {
       final int shift = val.charAt(0)-NumericUtils.SHIFT_START_INT;
       if (shift>0 && shift<=31)
-        throw new FieldCacheImpl.StopFillCacheException();
+        throw new StopFillCacheException();
       return NumericUtils.sortableIntToFloat(NumericUtils.prefixCodedToInt(val));
     }
     protected Object readResolve() {
@@ -279,7 +281,7 @@
     public long parseLong(String val) {
       final int shift = val.charAt(0)-NumericUtils.SHIFT_START_LONG;
       if (shift>0 && shift<=63)
-        throw new FieldCacheImpl.StopFillCacheException();
+        throw new StopFillCacheException();
       return NumericUtils.prefixCodedToLong(val);
     }
     protected Object readResolve() {
@@ -299,7 +301,7 @@
     public double parseDouble(String val) {
       final int shift = val.charAt(0)-NumericUtils.SHIFT_START_LONG;
       if (shift>0 && shift<=63)
-        throw new FieldCacheImpl.StopFillCacheException();
+        throw new StopFillCacheException();
       return NumericUtils.sortableLongToDouble(NumericUtils.prefixCodedToLong(val));
     }
     protected Object readResolve() {
@@ -316,7 +318,7 @@
    * <code>reader.maxDoc()</code>, with turned on bits for each docid that 
    * does have a value for this field.
    */
-  public Bits getDocsWithField(IndexReader reader, String field) 
+  public Bits getDocsWithField(IndexReader reader, String field)
   throws IOException;
   
   /** Checks the internal cache for an appropriate entry, and if none is
Index: lucene/contrib/grouping/src/java/org/apache/lucene/search/grouping/TermFirstPassGroupingCollector.java
===================================================================
--- lucene/contrib/grouping/src/java/org/apache/lucene/search/grouping/TermFirstPassGroupingCollector.java	(revision 1177602)
+++ lucene/contrib/grouping/src/java/org/apache/lucene/search/grouping/TermFirstPassGroupingCollector.java	(revision )
@@ -71,6 +71,6 @@
   @Override
   public void setNextReader(IndexReader reader, int docBase) throws IOException {
     super.setNextReader(reader, docBase);
-    index = FieldCache.DEFAULT.getStringIndex(reader, groupField);
+    index = reader.getFieldCache().getStringIndex(groupField);
   }
 }
Index: lucene/src/java/org/apache/lucene/search/cache/StopFillCacheException.java
===================================================================
--- lucene/src/java/org/apache/lucene/search/cache/StopFillCacheException.java	(revision )
+++ lucene/src/java/org/apache/lucene/search/cache/StopFillCacheException.java	(revision )
@@ -0,0 +1,26 @@
+package org.apache.lucene.search.cache;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * Hack: When thrown from a Parser (NUMERIC_UTILS_* ones), this stops
+ * processing terms and returns the current FieldCache
+ * array.
+ */
+public class StopFillCacheException extends RuntimeException {
+}
Index: lucene/src/java/org/apache/lucene/search/FieldComparator.java
===================================================================
--- lucene/src/java/org/apache/lucene/search/FieldComparator.java	(revision 1177602)
+++ lucene/src/java/org/apache/lucene/search/FieldComparator.java	(revision )
@@ -197,7 +197,7 @@
     @Override
     public void setNextReader(IndexReader reader, int docBase) throws IOException {
       if (missingValue != null) {
-        docsWithField = FieldCache.DEFAULT.getDocsWithField(reader, field);
+        docsWithField = reader.getFieldCache().getDocsWithField(field);
         // optimization to remove unneeded checks on the bit interface:
         if (docsWithField instanceof Bits.MatchAllBits)
           docsWithField = null;
@@ -249,7 +249,7 @@
 
     @Override
     public void setNextReader(IndexReader reader, int docBase) throws IOException {
-      currentReaderValues = FieldCache.DEFAULT.getBytes(reader, field, parser);
+      currentReaderValues = reader.getFieldCache().getBytes(field, parser);
       super.setNextReader(reader, docBase);
     }
     
@@ -365,7 +365,7 @@
 
     @Override
     public void setNextReader(IndexReader reader, int docBase) throws IOException {
-      currentReaderValues = FieldCache.DEFAULT.getDoubles(reader, field, parser);
+      currentReaderValues = reader.getFieldCache().getDoubles(field, parser);
       super.setNextReader(reader, docBase);
     }
     
@@ -439,7 +439,7 @@
 
     @Override
     public void setNextReader(IndexReader reader, int docBase) throws IOException {
-      currentReaderValues = FieldCache.DEFAULT.getFloats(reader, field, parser);
+      currentReaderValues = reader.getFieldCache().getFloats(field, parser);
       super.setNextReader(reader, docBase);
     }
     
@@ -517,7 +517,7 @@
 
     @Override
     public void setNextReader(IndexReader reader, int docBase) throws IOException {
-      currentReaderValues = FieldCache.DEFAULT.getInts(reader, field, parser);
+      currentReaderValues = reader.getFieldCache().getInts(field, parser);
       super.setNextReader(reader, docBase);
     }
     
@@ -591,7 +591,7 @@
 
     @Override
     public void setNextReader(IndexReader reader, int docBase) throws IOException {
-      currentReaderValues = FieldCache.DEFAULT.getLongs(reader, field, parser);
+      currentReaderValues = reader.getFieldCache().getLongs(field, parser);
       super.setNextReader(reader, docBase);
     }
     
@@ -715,7 +715,7 @@
 
     @Override
     public void setNextReader(IndexReader reader, int docBase) throws IOException {
-      currentReaderValues = FieldCache.DEFAULT.getShorts(reader, field, parser);
+      currentReaderValues = reader.getFieldCache().getShorts(field, parser);
       super.setNextReader(reader, docBase);
     }
     
@@ -782,7 +782,7 @@
 
     @Override
     public void setNextReader(IndexReader reader, int docBase) throws IOException {
-      currentReaderValues = FieldCache.DEFAULT.getStrings(reader, field);
+      currentReaderValues = reader.getFieldCache().getStrings(field);
     }
     
     @Override
@@ -902,7 +902,7 @@
 
     @Override
     public void setNextReader(IndexReader reader, int docBase) throws IOException {
-      StringIndex currentReaderValues = FieldCache.DEFAULT.getStringIndex(reader, field);
+      StringIndex currentReaderValues = reader.getFieldCache().getStringIndex(field);
       currentReaderGen++;
       order = currentReaderValues.order;
       lookup = currentReaderValues.lookup;
@@ -1026,7 +1026,7 @@
 
     @Override
     public void setNextReader(IndexReader reader, int docBase) throws IOException {
-      currentReaderValues = FieldCache.DEFAULT.getStrings(reader, field);
+      currentReaderValues = reader.getFieldCache().getStrings(field);
     }
     
     @Override
Index: lucene/contrib/grouping/src/java/org/apache/lucene/search/grouping/TermAllGroupsCollector.java
===================================================================
--- lucene/contrib/grouping/src/java/org/apache/lucene/search/grouping/TermAllGroupsCollector.java	(revision 1177602)
+++ lucene/contrib/grouping/src/java/org/apache/lucene/search/grouping/TermAllGroupsCollector.java	(revision )
@@ -94,7 +94,7 @@
   }
 
   public void setNextReader(IndexReader reader, int docBase) throws IOException {
-    index = FieldCache.DEFAULT.getStringIndex(reader, groupField);
+    index = reader.getFieldCache().getStringIndex(groupField);
 
     // Clear ordSet and fill it with previous encountered groups that can occur in the current segment.
     ordSet.clear();
Index: lucene/src/java/org/apache/lucene/search/function/IntFieldSource.java
===================================================================
--- lucene/src/java/org/apache/lucene/search/function/IntFieldSource.java	(revision 1177602)
+++ lucene/src/java/org/apache/lucene/search/function/IntFieldSource.java	(revision )
@@ -19,7 +19,7 @@
 
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.search.FieldCache;
-import org.apache.lucene.search.function.DocValues;
+import org.apache.lucene.search.cache.AtomicFieldCache;
 
 import java.io.IOException;
 
@@ -67,8 +67,8 @@
 
   /*(non-Javadoc) @see org.apache.lucene.search.function.FieldCacheSource#getCachedValues(org.apache.lucene.search.FieldCache, java.lang.String, org.apache.lucene.index.IndexReader) */
   @Override
-  public DocValues getCachedFieldValues (FieldCache cache, String field, IndexReader reader) throws IOException {
-    final int[] arr = cache.getInts(reader, field, parser);
+  public DocValues getCachedFieldValues (AtomicFieldCache cache, String field, IndexReader reader) throws IOException {
+    final int[] arr = cache.getInts(field, parser);
     return new DocValues() {
       /*(non-Javadoc) @see org.apache.lucene.search.function.DocValues#floatVal(int) */
       @Override
Index: lucene/src/test/org/apache/lucene/search/function/JustCompileSearchSpans.java
===================================================================
--- lucene/src/test/org/apache/lucene/search/function/JustCompileSearchSpans.java	(revision 1177602)
+++ lucene/src/test/org/apache/lucene/search/function/JustCompileSearchSpans.java	(revision )
@@ -18,7 +18,7 @@
  */
 
 import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.search.FieldCache;
+import org.apache.lucene.search.cache.AtomicFieldCache;
 
 import java.io.IOException;
 
@@ -63,7 +63,7 @@
     }
 
     @Override
-    public DocValues getCachedFieldValues(FieldCache cache, String field,
+    public DocValues getCachedFieldValues(AtomicFieldCache cache, String field,
                                           IndexReader reader) throws IOException {
       throw new UnsupportedOperationException(UNSUPPORTED_MSG);
     }
Index: lucene/src/test/org/apache/lucene/index/TestSegmentFieldCacheImpl.java
===================================================================
--- lucene/src/test/org/apache/lucene/index/TestSegmentFieldCacheImpl.java	(revision )
+++ lucene/src/test/org/apache/lucene/index/TestSegmentFieldCacheImpl.java	(revision )
@@ -0,0 +1,290 @@
+package org.apache.lucene.index;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.analysis.MockAnalyzer;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.search.FieldCache;
+import org.apache.lucene.search.cache.AtomicFieldCache;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.LuceneTestCase;
+import org.apache.lucene.util.ReaderUtil;
+import org.apache.lucene.util._TestUtil;
+
+import java.io.ByteArrayOutputStream;
+import java.io.IOException;
+import java.io.PrintStream;
+import java.util.*;
+
+/**
+ *
+ */
+public class TestSegmentFieldCacheImpl extends LuceneTestCase {
+
+  protected IndexReader reader;
+  private String[] unicodeStrings;
+  private Directory directory;
+
+  @Override
+  public void setUp() throws Exception {
+    super.setUp();
+    int numDocs = atLeast(1000);
+    directory = newDirectory();
+    RandomIndexWriter writer= new RandomIndexWriter(random, directory, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));
+    long theLong = Long.MAX_VALUE;
+    double theDouble = Double.MAX_VALUE;
+    byte theByte = Byte.MAX_VALUE;
+    short theShort = Short.MAX_VALUE;
+    int theInt = Integer.MAX_VALUE;
+    float theFloat = Float.MAX_VALUE;
+    unicodeStrings = new String[numDocs];
+    if (VERBOSE) {
+      System.out.println("TEST: setUp");
+    }
+    writer.w.setInfoStream(VERBOSE ? System.out : null);
+    for (int i = 0; i < numDocs; i++){
+      Document doc = new Document();
+      doc.add(newField("theLong", String.valueOf(theLong--), Field.Store.NO, Field.Index.NOT_ANALYZED));
+      doc.add(newField("theDouble", String.valueOf(theDouble--), Field.Store.NO, Field.Index.NOT_ANALYZED));
+      doc.add(newField("theByte", String.valueOf(theByte--), Field.Store.NO, Field.Index.NOT_ANALYZED));
+      doc.add(newField("theShort", String.valueOf(theShort--), Field.Store.NO, Field.Index.NOT_ANALYZED));
+      doc.add(newField("theInt", String.valueOf(theInt--), Field.Store.NO, Field.Index.NOT_ANALYZED));
+      doc.add(newField("theFloat", String.valueOf(theFloat--), Field.Store.NO, Field.Index.NOT_ANALYZED));
+
+      // sometimes skip the field:
+      if (random.nextInt(40) != 17) {
+        unicodeStrings[i] = generateString(i);
+        doc.add(newField("theRandomUnicodeString", unicodeStrings[i], Field.Store.YES, Field.Index.NOT_ANALYZED_NO_NORMS));
+      }
+      writer.addDocument(doc);
+    }
+    reader = writer.getReader();
+    writer.close();
+  }
+
+  @Override
+  public void tearDown() throws Exception {
+    reader.close();
+    directory.close();
+    super.tearDown();
+  }
+
+  public void testInfoStream() throws Exception {
+    List<IndexReader> subReaders = new LinkedList<IndexReader>();
+    ReaderUtil.gatherSubReaders(subReaders, reader);
+    AtomicFieldCache cache = subReaders.get(0).getFieldCache();
+    try {
+      ByteArrayOutputStream bos = new ByteArrayOutputStream(1024);
+      cache.setInfoStream(new PrintStream(bos));
+      cache.getDoubles("theDouble");
+      cache.getFloats("theDouble");
+      assertTrue(bos.toString().indexOf("WARNING") != -1);
+    } finally {
+      purgeFieldCache(cache);
+    }
+  }
+
+  public void test() throws IOException {
+    List<IndexReader> subReaders = new ArrayList<IndexReader>();
+    ReaderUtil.gatherSubReaders(subReaders, reader);
+    int start = 0;
+    for (IndexReader subReader : subReaders) {
+      AtomicFieldCache atomicCache = subReader.getFieldCache();
+      double [] doubles = atomicCache.getDoubles("theDouble");
+      assertSame("Second request to cache return same array", doubles, atomicCache.getDoubles("theDouble"));
+      assertSame("Second request with explicit parser return same array", doubles, atomicCache.getDoubles("theDouble", FieldCache.DEFAULT_DOUBLE_PARSER));
+      int expectedLength = subReader.maxDoc();
+      assertTrue("doubles Size: " + doubles.length + " is not: " + expectedLength, doubles.length == expectedLength);
+      for (int i = 0; i < doubles.length; i++) {
+        int j = start + i;
+        assertTrue(doubles[i] + " does not equal: " + (Double.MAX_VALUE - j), doubles[i] == (Double.MAX_VALUE - j));
+      }
+      start += expectedLength;
+    }
+
+    start = 0;
+    for (IndexReader subReader : subReaders) {
+      AtomicFieldCache atomicCache = subReader.getFieldCache();
+      long [] longs = atomicCache.getLongs("theLong");
+      assertSame("Second request to cache return same array", longs, atomicCache.getLongs("theLong"));
+      assertSame("Second request with explicit parser return same array", longs, atomicCache.getLongs("theLong", FieldCache.DEFAULT_LONG_PARSER));
+      int expectedLength = subReader.maxDoc();
+      assertTrue("longs Size: " + longs.length + " is not: " + expectedLength, longs.length == expectedLength);
+      for (int i = 0; i < longs.length; i++) {
+        int j = start + i;
+        assertTrue(longs[i] + " does not equal: " + (Long.MAX_VALUE - j) + " i=" + i, longs[i] == (Long.MAX_VALUE - j));
+      }
+      start += expectedLength;
+    }
+
+    start = 0;
+    for (IndexReader subReader : subReaders) {
+      AtomicFieldCache atomicCache = subReader.getFieldCache();
+      byte [] bytes = atomicCache.getBytes("theByte");
+      assertSame("Second request to cache return same array", bytes, atomicCache.getBytes("theByte"));
+      assertSame("Second request with explicit parser return same array", bytes, atomicCache.getBytes("theByte", FieldCache.DEFAULT_BYTE_PARSER));
+      int expectedLength = subReader.maxDoc();
+      assertTrue("bytes Size: " + bytes.length + " is not: " + expectedLength, bytes.length == expectedLength);
+      for (int i = 0; i < bytes.length; i++) {
+        int j = start + i;
+        assertTrue(bytes[i] + " does not equal: " + (Byte.MAX_VALUE - j), bytes[i] == (byte) (Byte.MAX_VALUE - j));
+      }
+      start += expectedLength;
+    }
+
+    start = 0;
+    for (IndexReader subReader : subReaders) {
+      AtomicFieldCache atomicCache = subReader.getFieldCache();
+      short [] shorts = atomicCache.getShorts("theShort");
+      assertSame("Second request to cache return same array", shorts, atomicCache.getShorts("theShort"));
+      assertSame("Second request with explicit parser return same array", shorts, atomicCache.getShorts("theShort", FieldCache.DEFAULT_SHORT_PARSER));
+      int expectedLength = subReader.maxDoc();
+      assertTrue("shorts Size: " + shorts.length + " is not: " + expectedLength, shorts.length == expectedLength);
+      for (int i = 0; i < shorts.length; i++) {
+        int j = start + i;
+        assertTrue(shorts[i] + " does not equal: " + (Short.MAX_VALUE - j), shorts[i] == (short) (Short.MAX_VALUE - j));
+      }
+      start += expectedLength;
+    }
+
+    start = 0;
+    for (IndexReader subReader : subReaders) {
+      AtomicFieldCache atomicCache = subReader.getFieldCache();
+      int [] ints = atomicCache.getInts("theInt");
+      assertSame("Second request to cache return same array", ints, atomicCache.getInts("theInt"));
+      assertSame("Second request with explicit parser return same array", ints, atomicCache.getInts("theInt", FieldCache.DEFAULT_INT_PARSER));
+      int expectedLength = subReader.maxDoc();
+      assertTrue("ints Size: " + ints.length + " is not: " + expectedLength, ints.length == expectedLength);
+      for (int i = 0; i < ints.length; i++) {
+        int j = start + i;
+        assertTrue(ints[i] + " does not equal: " + (Integer.MAX_VALUE - j), ints[i] == (Integer.MAX_VALUE - j));
+      }
+      start += expectedLength;
+    }
+
+    start = 0;
+    for (IndexReader subReader : subReaders) {
+      AtomicFieldCache atomicCache = subReader.getFieldCache();
+      float [] floats = atomicCache.getFloats("theFloat");
+      assertSame("Second request to cache return same array", floats, atomicCache.getFloats("theFloat"));
+      assertSame("Second request with explicit parser return same array", floats, atomicCache.getFloats("theFloat", FieldCache.DEFAULT_FLOAT_PARSER));
+      int expectedLength = subReader.maxDoc();
+      assertTrue("floats Size: " + floats.length + " is not: " + expectedLength, floats.length == expectedLength);
+      for (int i = 0; i < floats.length; i++) {
+        int j = start + i;
+        assertTrue(floats[i] + " does not equal: " + (Float.MAX_VALUE - j), floats[i] == (Float.MAX_VALUE - j));
+
+      }
+      start += expectedLength;
+    }
+
+    // getTerms
+    start = 0;
+    for (IndexReader subReader : subReaders) {
+      AtomicFieldCache atomicCache = subReader.getFieldCache();
+      String[] terms = atomicCache.getStrings("theRandomUnicodeString");
+      assertSame("Second request to cache return same array", terms, atomicCache.getStrings("theRandomUnicodeString"));
+      int expectedLength = subReader.maxDoc();
+      assertTrue("doubles Size: " + terms.length+ " is not: " + expectedLength, terms.length == expectedLength);
+      for (int i = 0; i < expectedLength; i++) {
+        int j = start + i;
+        final String s = terms[i];
+        assertTrue("for doc " + i + ": " + s + " does not equal: " + unicodeStrings[j], unicodeStrings[j] == null || unicodeStrings[j].equals(s));
+      }
+
+      // test bad field
+      terms = atomicCache.getStrings("bogusfield");
+      start += expectedLength;
+    }
+
+    // getTermsIndex
+    start = 0;
+    for (IndexReader subReader : subReaders) {
+      AtomicFieldCache atomicCache = subReader.getFieldCache();
+      FieldCache.StringIndex termsIndex = atomicCache.getStringIndex("theRandomUnicodeString");
+      assertSame("Second request to cache return same array", termsIndex, atomicCache.getStringIndex("theRandomUnicodeString"));
+      int expectedLength = subReader.maxDoc();
+      assertTrue("doubles Size: " + termsIndex.order.length + " is not: " + expectedLength, termsIndex.order.length == expectedLength);
+      for (int i = 0; i < expectedLength; i++) {
+        int j = start + i;
+        final String s = termsIndex.lookup[termsIndex.order[i]];
+        assertTrue("for doc " + i + ": " + s + " does not equal: " + unicodeStrings[j], unicodeStrings[j] == null || unicodeStrings[j].equals(s));
+      }
+
+      int nTerms = termsIndex.lookup.length;
+      // System.out.println("nTerms="+nTerms);
+
+      TermEnum tenum = subReader.terms();
+      for (int i=1; i<nTerms; i++) {
+        String actual = termsIndex.lookup[i];
+        Term term = subReader.terms(new Term("theRandomUnicodeString", actual)).term();
+        String expected = term != null ? term.text() : null;
+        // System.out.println("i="+i);
+        assertEquals(actual, expected);
+      }
+
+      // seek the enum around (note this isn't a great test here)
+      int num = atLeast(100);
+      for (int i = 0; i < num; i++) {
+        int k = _TestUtil.nextInt(random, 1, nTerms - 1);
+        String val1 = termsIndex.lookup[k];
+
+        tenum = subReader.terms(new Term("theRandomUnicodeString", val1));
+        Term term = tenum.term();
+        assertEquals(val1, term != null ? term.text() : null);
+      }
+
+      // test bad field
+      termsIndex = atomicCache.getStringIndex("bogusfield");
+      start += expectedLength;
+    }
+  }
+
+  public void testEmptyIndex() throws Exception {
+    Directory dir = newDirectory();
+    IndexWriter writer= new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMaxBufferedDocs(500));
+    IndexReader r = IndexReader.open(writer, true);
+    List<IndexReader> subReaders = new LinkedList<IndexReader>();
+    ReaderUtil.gatherSubReaders(subReaders, reader);
+    AtomicFieldCache cache = subReaders.get(0).getFieldCache();
+    String[] terms = cache.getStrings("foobar");
+    FieldCache.StringIndex termsIndex = cache.getStringIndex("foobar");
+    writer.close();
+    r.close();
+    dir.close();
+  }
+
+  private String generateString(int i) {
+    String s = null;
+    if (i > 0 && random.nextInt(3) == 1) {
+      // reuse past string -- try to find one that's not null
+      for(int iter = 0; iter < 10 && s == null;iter++) {
+        s = unicodeStrings[random.nextInt(i)];
+      }
+      if (s == null) {
+        s = _TestUtil.randomUnicodeString(random);
+      }
+    } else {
+      s = _TestUtil.randomUnicodeString(random);
+    }
+    return s;
+  }
+
+}
Index: lucene/src/java/org/apache/lucene/index/SegmentReader.java
===================================================================
--- lucene/src/java/org/apache/lucene/index/SegmentReader.java	(revision 1177602)
+++ lucene/src/java/org/apache/lucene/index/SegmentReader.java	(revision )
@@ -28,16 +28,19 @@
 import java.util.Map;
 import java.util.Set;
 
+import java.util.concurrent.ConcurrentHashMap;
 import java.util.concurrent.atomic.AtomicInteger;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.FieldSelector;
 import org.apache.lucene.search.Similarity;
 import org.apache.lucene.index.FieldInfo.IndexOptions;
+import org.apache.lucene.search.cache.AtomicFieldCache;
 import org.apache.lucene.store.BufferedIndexInput;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.IndexInput;
 import org.apache.lucene.util.BitVector;
 import org.apache.lucene.util.CloseableThreadLocal;
+import org.apache.lucene.util.MapBackedSet;
 import org.apache.lucene.util.StringHelper;
 
 /**
@@ -73,6 +76,33 @@
 
   SegmentCoreReaders core;
 
+  private final SegmentFieldCacheImpl segmentCache;
+
+  public SegmentReader() {
+    this.segmentCache = new SegmentFieldCacheImpl(this);
+    this.readerFinishedListeners = new MapBackedSet<ReaderFinishedListener>(new ConcurrentHashMap<ReaderFinishedListener,Boolean>());
+    readerFinishedListeners.add(new ReaderFinishedListener() {
+
+      public void finished(IndexReader reader) {
+        segmentCache.purgeCache();
+      }
+
+    });
+  }
+
+  public SegmentReader(SegmentFieldCacheImpl segmentCache) {
+    this.segmentCache = segmentCache;
+    this.readerFinishedListeners = new MapBackedSet<ReaderFinishedListener>(new ConcurrentHashMap<ReaderFinishedListener,Boolean>());
+    readerFinishedListeners.add(new ReaderFinishedListener() {
+
+      public void finished(IndexReader reader) {
+        SegmentReader.this.segmentCache.purgeCache();
+      }
+
+    });
+    segmentCache.updateIndexReader(this);
+  }
+
   /**
    * Sets the initial value 
    */
@@ -240,6 +270,11 @@
 
     // clone reader
     SegmentReader clone = openReadOnly ? new ReadOnlySegmentReader() : new SegmentReader();
+    if (openReadOnly) {
+      clone = new ReadOnlySegmentReader();
+    } else {
+      clone = deletionsUpToDate ? new SegmentReader(segmentCache) : new SegmentReader();
+    }
 
     boolean success = false;
     try {
@@ -921,6 +956,11 @@
   }
 
   @Override
+  public AtomicFieldCache getFieldCache() {
+    return segmentCache;
+  }
+
+  @Override
   public int getTermInfosIndexDivisor() {
     return core.termsIndexDivisor;
   }
Index: solr/core/src/java/org/apache/solr/handler/component/StatsComponent.java
===================================================================
--- solr/core/src/java/org/apache/solr/handler/component/StatsComponent.java	(revision 1177602)
+++ solr/core/src/java/org/apache/solr/handler/component/StatsComponent.java	(revision )
@@ -21,6 +21,7 @@
 import java.util.HashMap;
 import java.util.Map;
 
+import org.apache.lucene.index.SlowMultiReaderWrapper;
 import org.apache.lucene.search.FieldCache;
 import org.apache.solr.common.params.SolrParams;
 import org.apache.solr.common.params.StatsParams;
@@ -248,8 +249,8 @@
 
     FieldCache.StringIndex si = null;
     try {
-      si = FieldCache.DEFAULT.getStringIndex(searcher.getReader(), fieldName);
+      si = new SlowMultiReaderWrapper(searcher.getReader()).getFieldCache().getStringIndex(fieldName);
-    } 
+    }
     catch (IOException e) {
       throw new RuntimeException( "failed to open field cache for: "+fieldName, e );
     }
@@ -263,8 +264,8 @@
     for( String f : facet ) {
       ft = searcher.getSchema().getFieldType(f);
       try {
-        si = FieldCache.DEFAULT.getStringIndex(searcher.getReader(), f);
+        si = new SlowMultiReaderWrapper(searcher.getReader()).getFieldCache().getStringIndex(f);
-      } 
+      }
       catch (IOException e) {
         throw new RuntimeException( "failed to open field cache for: "+f, e );
       }
Index: lucene/src/java/org/apache/lucene/index/IndexReader.java
===================================================================
--- lucene/src/java/org/apache/lucene/index/IndexReader.java	(revision 1177602)
+++ lucene/src/java/org/apache/lucene/index/IndexReader.java	(revision )
@@ -21,6 +21,7 @@
 import org.apache.lucene.document.FieldSelector;
 import org.apache.lucene.search.FieldCache; // javadocs
 import org.apache.lucene.search.Similarity;
+import org.apache.lucene.search.cache.AtomicFieldCache;
 import org.apache.lucene.store.*;
 import org.apache.lucene.util.ArrayUtil;
 
@@ -1424,4 +1425,15 @@
   public int getTermInfosIndexDivisor() {
     throw new UnsupportedOperationException("This reader does not support this method.");
   }
+
+  /**
+   * Returns an {@link org.apache.lucene.search.cache.AtomicFieldCache} instance for this reader.
+   * Not all {@link IndexReader} subclasses implements this method.
+   *
+   * @return {@link org.apache.lucene.search.cache.AtomicFieldCache} instance for this reader
+   */
+  public AtomicFieldCache getFieldCache() {
+    throw new UnsupportedOperationException("This reader does not support this method.");
-}
+  }
+
+}
Index: solr/core/src/java/org/apache/solr/request/UnInvertedField.java
===================================================================
--- solr/core/src/java/org/apache/solr/request/UnInvertedField.java	(revision 1177602)
+++ solr/core/src/java/org/apache/solr/request/UnInvertedField.java	(revision )
@@ -17,11 +17,8 @@
 
 package org.apache.solr.request;
 
+import org.apache.lucene.index.*;
 import org.apache.lucene.search.FieldCache;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.Term;
-import org.apache.lucene.index.TermDocs;
-import org.apache.lucene.index.TermEnum;
 import org.apache.lucene.search.TermQuery;
 import org.apache.lucene.search.TermRangeQuery;
 import org.apache.solr.common.params.FacetParams;
@@ -688,7 +685,7 @@
     for (String f : facet) {
       FieldType facet_ft = searcher.getSchema().getFieldType(f);
       try {
-        si = FieldCache.DEFAULT.getStringIndex(searcher.getReader(), f);
+        si = new SlowMultiReaderWrapper(searcher.getReader()).getFieldCache().getStringIndex(f);
       }
       catch (IOException e) {
         throw new RuntimeException("failed to open field cache for: " + f, e);
Index: solr/core/src/java/org/apache/solr/search/function/StringIndexDocValues.java
===================================================================
--- solr/core/src/java/org/apache/solr/search/function/StringIndexDocValues.java	(revision 1177602)
+++ solr/core/src/java/org/apache/solr/search/function/StringIndexDocValues.java	(revision )
@@ -33,7 +33,7 @@
 
     public StringIndexDocValues(ValueSource vs, IndexReader reader, String field) throws IOException {
       try {
-        index = FieldCache.DEFAULT.getStringIndex(reader, field);
+        index = reader.getFieldCache().getStringIndex(field);
       } catch (RuntimeException e) {
         throw new StringIndexException(field, e);
       }
Index: lucene/src/test-framework/org/apache/lucene/index/SlowMultiReaderWrapper.java
===================================================================
--- lucene/src/test-framework/org/apache/lucene/index/SlowMultiReaderWrapper.java	(revision 1177602)
+++ lucene/src/java/org/apache/lucene/index/SlowMultiReaderWrapper.java	(revision )
@@ -17,20 +17,43 @@
  * limitations under the License.
  */
 
-import java.util.ArrayList;
+import java.io.IOException;
+import java.io.PrintStream;
+import java.util.*;
+import java.util.concurrent.ConcurrentHashMap;
 
-import org.apache.lucene.util.ReaderUtil;
+import org.apache.lucene.search.DocIdSet;
+import org.apache.lucene.search.FieldCache;
+import org.apache.lucene.search.cache.AtomicFieldCache;
+import org.apache.lucene.search.cache.StopFillCacheException;
+import org.apache.lucene.util.*;
 
 /**
  * Acts like Lucene 4.x's SlowMultiReaderWrapper for testing 
  * of top-level MultiTermEnum, MultiTermDocs, ...
+ *
+ * @lucene.insane
  */
 public class SlowMultiReaderWrapper extends MultiReader {
 
+  private final IndexReader reader;
+  private final static InsaneFieldCache insaneFieldCache = new InsaneFieldCache();
+  // Same instance for every SlowMultiReaderWrapper instance works well with MapBackedSet
+  private final static InsaneReaderFinishedListener insaneReaderFinishedListener = new InsaneReaderFinishedListener();
+
   public SlowMultiReaderWrapper(IndexReader reader) {
-    super(subReaders(reader));
+    super(reader != null ? subReaders(reader): new IndexReader[0]);
+    this.reader = reader;
+    if (reader == null) {
+      return;
-  }
-  
+    }
+
+    if (this.reader.readerFinishedListeners == null) {
+      this.reader.readerFinishedListeners = new MapBackedSet<ReaderFinishedListener>(new ConcurrentHashMap<ReaderFinishedListener, Boolean>());
+    }
+    this.reader.addReaderFinishedListener(insaneReaderFinishedListener);
+  }
+  
   private static IndexReader[] subReaders(IndexReader reader) {
     ArrayList<IndexReader> list = new ArrayList<IndexReader>();
     ReaderUtil.gatherSubReaders(list, reader);
@@ -46,4 +69,814 @@
   public String toString() {
     return "SlowMultiReaderWrapper(" + super.toString() + ")";
   }
+
+  public static void setInfoStream(PrintStream stream) {
+    insaneFieldCache.setInfoStream(stream);
-}
+  }
+
+  public static PrintStream getInfoStream() {
+    return insaneFieldCache.getInfoStream();
+  }
+
+  public static FieldCache.CacheEntry[] getCacheEntries() {
+    return insaneFieldCache.getCacheEntries();
+  }
+
+  public static void purgeAllCaches() {
+    insaneFieldCache.purgeAllCaches();
+  }
+
+  @Override
+  public AtomicFieldCache getFieldCache() {
+    return new InsaneNonAtomicFieldCache() {
+
+      public byte[] getBytes(String field) throws IOException {
+        return insaneFieldCache.getBytes(reader, field);
+      }
+
+      public byte[] getBytes(String field, FieldCache.ByteParser parser) throws IOException {
+        return insaneFieldCache.getBytes(reader, field, parser);
+      }
+
+      public short[] getShorts(String field) throws IOException {
+        return insaneFieldCache.getShorts(reader, field);
+      }
+
+      public short[] getShorts(String field, FieldCache.ShortParser parser) throws IOException {
+        return insaneFieldCache.getShorts(reader, field);
+      }
+
+      public int[] getInts(String field) throws IOException {
+        return insaneFieldCache.getInts(reader, field);
+      }
+
+      public int[] getInts(String field, FieldCache.IntParser parser) throws IOException {
+        return insaneFieldCache.getInts(reader, field, parser);
+      }
+
+      public float[] getFloats(String field) throws IOException {
+        return insaneFieldCache.getFloats(reader, field);
+      }
+
+      public float[] getFloats(String field, FieldCache.FloatParser parser) throws IOException {
+        return insaneFieldCache.getFloats(reader, field, parser);
+      }
+
+      public long[] getLongs(String field) throws IOException {
+        return insaneFieldCache.getLongs(reader, field);
+      }
+
+      public long[] getLongs(String field, FieldCache.LongParser parser) throws IOException {
+        return insaneFieldCache.getLongs(reader, field, parser);
+      }
+
+      public double[] getDoubles(String field) throws IOException {
+        return insaneFieldCache.getDoubles(reader, field);
+      }
+
+      public double[] getDoubles(String field, FieldCache.DoubleParser parser) throws IOException {
+        return insaneFieldCache.getDoubles(reader, field, parser);
+      }
+
+      public String[] getStrings(String field) throws IOException {
+        return insaneFieldCache.getStrings(reader, field);
+      }
+
+      public FieldCache.StringIndex getStringIndex(String field) throws IOException {
+        return insaneFieldCache.getStringIndex(reader, field);
+      }
+
+      public FieldCache.CacheEntry[] getCacheEntries() {
+        return insaneFieldCache.getCacheEntries();
+      }
+
+      public void purgeCache() {
+        insaneFieldCache.purge(reader);
+      }
+
+      public void purgeAllCaches() {
+        insaneFieldCache.purgeAllCaches();
+      }
+
+      public Bits getDocsWithField(String field) throws IOException {
+        return insaneFieldCache.getDocsWithField(reader, field);
+      }
+
+      public void setInfoStream(PrintStream stream) {
+        insaneFieldCache.setInfoStream(stream);
+      }
+
+      public PrintStream getInfoStream() {
+        return insaneFieldCache.getInfoStream();
+      }
+    };
+  }
+
+  public static InsaneNonAtomicFieldCache getNonAtomicFieldCache() {
+    return (InsaneNonAtomicFieldCache) new SlowMultiReaderWrapper(null).getFieldCache();
+  }
+
+  /**
+   * Concrete subclasses maintain field values cache for more than one reader.
+   *
+   * @lucene.insane
+   */
+  public interface InsaneNonAtomicFieldCache extends AtomicFieldCache {
+
+    /**
+     * Purges all cache entries for all IndexReader keys.
+     */
+    public void purgeAllCaches();
+
+
+  }
+
+  private static class InsaneReaderFinishedListener implements ReaderFinishedListener {
+
+    public void finished(IndexReader reader) {
+      insaneFieldCache.purge(reader);
+    }
+
+  }
+
+  private static class InsaneFieldCache {
+
+    private Map<Class<?>, Cache> caches;
+
+    InsaneFieldCache() {
+      init();
+    }
+
+    private synchronized void init() {
+      caches = new HashMap<Class<?>,Cache>(9);
+      caches.put(Byte.TYPE, new ByteCache(this));
+      caches.put(Short.TYPE, new ShortCache(this));
+      caches.put(Integer.TYPE, new IntCache(this));
+      caches.put(Float.TYPE, new FloatCache(this));
+      caches.put(Long.TYPE, new LongCache(this));
+      caches.put(Double.TYPE, new DoubleCache(this));
+      caches.put(String.class, new StringCache(this));
+      caches.put(FieldCache.StringIndex.class, new StringIndexCache(this));
+      caches.put(DocsWithFieldCache.class, new DocsWithFieldCache(this));
+    }
+
+    public synchronized void purgeAllCaches() {
+      init();
+    }
+
+    public synchronized void purge(IndexReader r) {
+      for (Cache c : caches.values()) {
+        c.purge(r);
+      }
+    }
+
+    public synchronized FieldCache.CacheEntry[] getCacheEntries() {
+      List<FieldCache.CacheEntry> result = new ArrayList<FieldCache.CacheEntry>(17);
+      for(final Map.Entry<Class<?>,Cache> cacheEntry: caches.entrySet()) {
+        final Cache cache = cacheEntry.getValue();
+        final Class<?> cacheType = cacheEntry.getKey();
+        synchronized(cache.readerCache) {
+          for (final Map.Entry<Object,Map<Entry, Object>> readerCacheEntry : cache.readerCache.entrySet()) {
+            final Object readerKey = readerCacheEntry.getKey();
+            if (readerKey == null) continue;
+            final Map<Entry, Object> innerCache = readerCacheEntry.getValue();
+            for (final Map.Entry<Entry, Object> mapEntry : innerCache.entrySet()) {
+              Entry entry = mapEntry.getKey();
+              result.add(new CacheEntryImpl(readerKey, entry.field,
+                                            cacheType, entry.custom,
+                                            mapEntry.getValue()));
+            }
+          }
+        }
+      }
+      return result.toArray(new FieldCache.CacheEntry[result.size()]);
+    }
+
+    private static final class CacheEntryImpl extends FieldCache.CacheEntry {
+      private final Object readerKey;
+      private final String fieldName;
+      private final Class<?> cacheType;
+      private final Object custom;
+      private final Object value;
+      CacheEntryImpl(Object readerKey, String fieldName,
+                     Class<?> cacheType,
+                     Object custom,
+                     Object value) {
+          this.readerKey = readerKey;
+          this.fieldName = fieldName;
+          this.cacheType = cacheType;
+          this.custom = custom;
+          this.value = value;
+
+          // :HACK: for testing.
+  //         if (null != locale || SortField.CUSTOM != sortFieldType) {
+  //           throw new RuntimeException("Locale/sortFieldType: " + this);
+  //         }
+
+      }
+      @Override
+      public Object getReaderKey() { return readerKey; }
+      @Override
+      public String getFieldName() { return fieldName; }
+      @Override
+      public Class<?> getCacheType() { return cacheType; }
+      @Override
+      public Object getCustom() { return custom; }
+      @Override
+      public Object getValue() { return value; }
+    }
+
+    final static IndexReader.ReaderFinishedListener purgeReader = new IndexReader.ReaderFinishedListener() {
+
+      public void finished(IndexReader reader) {
+        insaneFieldCache.purge(reader);
+      }
+
+    };
+
+    /** Expert: Internal cache. */
+    abstract static class Cache {
+      Cache() {
+        this.wrapper = null;
+      }
+
+      Cache(InsaneFieldCache wrapper) {
+        this.wrapper = wrapper;
+      }
+
+      final InsaneFieldCache wrapper;
+
+      final Map<Object,Map<Entry,Object>> readerCache = new WeakHashMap<Object,Map<Entry,Object>>();
+
+      protected abstract Object createValue(IndexReader reader, Entry key)
+          throws IOException;
+
+      /** Remove this reader from the cache, if present. */
+      public void purge(IndexReader r) {
+        Object readerKey = r.getCoreCacheKey();
+        synchronized(readerCache) {
+          readerCache.remove(readerKey);
+        }
+      }
+
+      public Object get(IndexReader reader, Entry key) throws IOException {
+        Map<Entry,Object> innerCache;
+        Object value;
+        final Object readerKey = reader.getCoreCacheKey();
+        synchronized (readerCache) {
+          innerCache = readerCache.get(readerKey);
+          if (innerCache == null) {
+            // First time this reader is using FieldCache
+            innerCache = new HashMap<Entry,Object>();
+            readerCache.put(readerKey, innerCache);
+            reader.addReaderFinishedListener(purgeReader);
+            value = null;
+          } else {
+            value = innerCache.get(key);
+          }
+          if (value == null) {
+            value = new FieldCache.CreationPlaceholder();
+            innerCache.put(key, value);
+          }
+        }
+        if (value instanceof FieldCache.CreationPlaceholder) {
+          synchronized (value) {
+            FieldCache.CreationPlaceholder progress = (FieldCache.CreationPlaceholder) value;
+            if (progress.value == null) {
+              progress.value = createValue(reader, key);
+              synchronized (readerCache) {
+                innerCache.put(key, progress.value);
+              }
+
+              // Only check if key.custom (the parser) is
+              // non-null; else, we check twice for a single
+              // call to FieldCache.getXXX
+              if (key.custom != null && wrapper != null) {
+                final PrintStream infoStream = wrapper.getInfoStream();
+                if (infoStream != null) {
+                  printNewInsanity(infoStream, progress.value);
+                }
+              }
+            }
+            return progress.value;
+          }
+        }
+        return value;
+      }
+
+      private void printNewInsanity(PrintStream infoStream, Object value) {
+        final FieldCacheSanityChecker.Insanity[] insanities = FieldCacheSanityChecker.checkSanity(wrapper.getCacheEntries());
+        for(int i=0;i<insanities.length;i++) {
+          final FieldCacheSanityChecker.Insanity insanity = insanities[i];
+          final FieldCache.CacheEntry[] entries = insanity.getCacheEntries();
+          for(int j=0;j<entries.length;j++) {
+            if (entries[j].getValue() == value) {
+              // OK this insanity involves our entry
+              infoStream.println("WARNING: new FieldCache insanity created\nDetails: " + insanity.toString());
+              infoStream.println("\nStack:\n");
+              new Throwable().printStackTrace(infoStream);
+              break;
+            }
+          }
+        }
+      }
+    }
+
+    /** Expert: Every composite-key in the internal cache is of this type. */
+    static class Entry {
+      final String field;        // which Fieldable
+      final Object custom;       // which custom comparator or parser
+
+      /** Creates one of these objects for a custom comparator/parser. */
+      Entry (String field, Object custom) {
+        this.field = StringHelper.intern(field);
+        this.custom = custom;
+      }
+
+      /** Two of these are equal iff they reference the same field and type. */
+      @Override
+      public boolean equals (Object o) {
+        if (o instanceof Entry) {
+          Entry other = (Entry) o;
+          if (other.field == field) {
+            if (other.custom == null) {
+              if (custom == null) return true;
+            } else if (other.custom.equals (custom)) {
+              return true;
+            }
+          }
+        }
+        return false;
+      }
+
+      /** Composes a hashcode based on the field and type. */
+      @Override
+      public int hashCode() {
+        return field.hashCode() ^ (custom==null ? 0 : custom.hashCode());
+      }
+    }
+
+    public byte[] getBytes(IndexReader reader, String field) throws IOException {
+      return getBytes(reader, field, null);
+    }
+
+    public byte[] getBytes(IndexReader reader, String field, FieldCache.ByteParser parser) throws IOException {
+      return (byte[]) caches.get(Byte.TYPE).get(reader, new Entry(field, parser));
+    }
+
+    static final class ByteCache extends Cache {
+      ByteCache(InsaneFieldCache wrapper) {
+        super(wrapper);
+      }
+      @Override
+      protected Object createValue(IndexReader reader, Entry entryKey)
+          throws IOException {
+        Entry entry = entryKey;
+        String field = entry.field;
+        FieldCache.ByteParser parser = (FieldCache.ByteParser) entry.custom;
+        if (parser == null) {
+          return wrapper.getBytes(reader, field, FieldCache.DEFAULT_BYTE_PARSER);
+        }
+        final byte[] retArray = new byte[reader.maxDoc()];
+        TermDocs termDocs = reader.termDocs();
+        TermEnum termEnum = reader.terms (new Term (field));
+        try {
+          do {
+            Term term = termEnum.term();
+            if (term==null || term.field() != field) break;
+            byte termval = parser.parseByte(term.text());
+            termDocs.seek (termEnum);
+            while (termDocs.next()) {
+              retArray[termDocs.doc()] = termval;
+            }
+          } while (termEnum.next());
+        } catch (StopFillCacheException stop) {
+        } finally {
+          termDocs.close();
+          termEnum.close();
+        }
+        return retArray;
+      }
+    }
+
+    public short[] getShorts(IndexReader reader, String field) throws IOException {
+      return getShorts(reader, field, null);
+    }
+
+    public short[] getShorts(IndexReader reader, String field, FieldCache.ShortParser parser) throws IOException {
+      return (short[]) caches.get(Short.TYPE).get(reader, new Entry(field, parser));
+    }
+
+    static final class ShortCache extends Cache {
+      ShortCache(InsaneFieldCache wrapper) {
+        super(wrapper);
+      }
+
+      @Override
+      protected Object createValue(IndexReader reader, Entry entryKey)
+          throws IOException {
+        Entry entry =  entryKey;
+        String field = entry.field;
+        FieldCache.ShortParser parser = (FieldCache.ShortParser) entry.custom;
+        if (parser == null) {
+          return wrapper.getShorts(reader, field, FieldCache.DEFAULT_SHORT_PARSER);
+        }
+        final short[] retArray = new short[reader.maxDoc()];
+        TermDocs termDocs = reader.termDocs();
+        TermEnum termEnum = reader.terms (new Term (field));
+        try {
+          do {
+            Term term = termEnum.term();
+            if (term==null || term.field() != field) break;
+            short termval = parser.parseShort(term.text());
+            termDocs.seek (termEnum);
+            while (termDocs.next()) {
+              retArray[termDocs.doc()] = termval;
+            }
+          } while (termEnum.next());
+        } catch (StopFillCacheException stop) {
+        } finally {
+          termDocs.close();
+          termEnum.close();
+        }
+        return retArray;
+      }
+    }
+
+    public int[] getInts(IndexReader reader, String field) throws IOException {
+      return getInts(reader, field, null);
+    }
+
+    public int[] getInts(IndexReader reader, String field, FieldCache.IntParser parser) throws IOException {
+      return (int[]) caches.get(Integer.TYPE).get(reader, new Entry(field, parser));
+    }
+
+    static final class IntCache extends Cache {
+      IntCache(InsaneFieldCache wrapper) {
+        super(wrapper);
+      }
+
+      @Override
+      protected Object createValue(IndexReader reader, Entry entryKey)
+          throws IOException {
+        Entry entry = entryKey;
+        String field = entry.field;
+        FieldCache.IntParser parser = (FieldCache.IntParser) entry.custom;
+        if (parser == null) {
+          try {
+            return wrapper.getInts(reader, field, FieldCache.DEFAULT_INT_PARSER);
+          } catch (NumberFormatException ne) {
+            return wrapper.getInts(reader, field, FieldCache.NUMERIC_UTILS_INT_PARSER);
+          }
+        }
+        int[] retArray = null;
+        TermDocs termDocs = reader.termDocs();
+        TermEnum termEnum = reader.terms (new Term (field));
+        try {
+          do {
+            Term term = termEnum.term();
+            if (term==null || term.field() != field) break;
+            int termval = parser.parseInt(term.text());
+            if (retArray == null) // late init
+              retArray = new int[reader.maxDoc()];
+            termDocs.seek (termEnum);
+            while (termDocs.next()) {
+              retArray[termDocs.doc()] = termval;
+            }
+          } while (termEnum.next());
+        } catch (StopFillCacheException stop) {
+        } finally {
+          termDocs.close();
+          termEnum.close();
+        }
+        if (retArray == null) // no values
+          retArray = new int[reader.maxDoc()];
+        return retArray;
+      }
+    }
+
+    public float[] getFloats(IndexReader reader, String field) throws IOException {
+      return getFloats(reader, field, null);
+    }
+
+    public float[] getFloats(IndexReader reader, String field, FieldCache.FloatParser parser) throws IOException {
+      return (float[]) caches.get(Float.TYPE).get(reader, new Entry(field, parser));
+    }
+
+    static final class FloatCache extends Cache {
+      FloatCache(InsaneFieldCache wrapper) {
+        super(wrapper);
+      }
+
+      @Override
+      protected Object createValue(IndexReader reader, Entry entryKey)
+          throws IOException {
+        Entry entry = entryKey;
+        String field = entry.field;
+        FieldCache.FloatParser parser = (FieldCache.FloatParser) entry.custom;
+        if (parser == null) {
+          try {
+            return wrapper.getFloats(reader, field, FieldCache.DEFAULT_FLOAT_PARSER);
+          } catch (NumberFormatException ne) {
+            return wrapper.getFloats(reader, field, FieldCache.NUMERIC_UTILS_FLOAT_PARSER);
+          }
+      }
+        float[] retArray = null;
+        TermDocs termDocs = reader.termDocs();
+        TermEnum termEnum = reader.terms (new Term (field));
+        try {
+          do {
+            Term term = termEnum.term();
+            if (term==null || term.field() != field) break;
+            float termval = parser.parseFloat(term.text());
+            if (retArray == null) // late init
+              retArray = new float[reader.maxDoc()];
+            termDocs.seek (termEnum);
+            while (termDocs.next()) {
+              retArray[termDocs.doc()] = termval;
+            }
+          } while (termEnum.next());
+        } catch (StopFillCacheException stop) {
+        } finally {
+          termDocs.close();
+          termEnum.close();
+        }
+        if (retArray == null) // no values
+          retArray = new float[reader.maxDoc()];
+        return retArray;
+      }
+    }
+
+    public long[] getLongs(IndexReader reader, String field) throws IOException {
+      return getLongs(reader, field, null);
+    }
+
+    public long[] getLongs(IndexReader reader, String field, FieldCache.LongParser parser) throws IOException {
+      return (long[]) caches.get(Long.TYPE).get(reader, new Entry(field, parser));
+    }
+
+    static final class LongCache extends Cache {
+      LongCache(InsaneFieldCache wrapper) {
+        super(wrapper);
+      }
+
+      @Override
+      protected Object createValue(IndexReader reader, Entry entry)
+          throws IOException {
+        String field = entry.field;
+        FieldCache.LongParser parser = (FieldCache.LongParser) entry.custom;
+        if (parser == null) {
+          try {
+            return wrapper.getLongs(reader, field, FieldCache.DEFAULT_LONG_PARSER);
+          } catch (NumberFormatException ne) {
+            return wrapper.getLongs(reader, field, FieldCache.NUMERIC_UTILS_LONG_PARSER);
+          }
+        }
+        long[] retArray = null;
+        TermDocs termDocs = reader.termDocs();
+        TermEnum termEnum = reader.terms (new Term(field));
+        try {
+          do {
+            Term term = termEnum.term();
+            if (term==null || term.field() != field) break;
+            long termval = parser.parseLong(term.text());
+            if (retArray == null) // late init
+              retArray = new long[reader.maxDoc()];
+            termDocs.seek (termEnum);
+            while (termDocs.next()) {
+              retArray[termDocs.doc()] = termval;
+            }
+          } while (termEnum.next());
+        } catch (StopFillCacheException stop) {
+        } finally {
+          termDocs.close();
+          termEnum.close();
+        }
+        if (retArray == null) // no values
+          retArray = new long[reader.maxDoc()];
+        return retArray;
+      }
+    }
+
+    // inherit javadocs
+    public double[] getDoubles(IndexReader reader, String field)
+      throws IOException {
+      return getDoubles(reader, field, null);
+    }
+
+    // inherit javadocs
+    public double[] getDoubles(IndexReader reader, String field, FieldCache.DoubleParser parser)
+        throws IOException {
+      return (double[]) caches.get(Double.TYPE).get(reader, new Entry(field, parser));
+    }
+
+    static final class DoubleCache extends Cache {
+      DoubleCache(InsaneFieldCache wrapper) {
+        super(wrapper);
+      }
+
+      @Override
+      protected Object createValue(IndexReader reader, Entry entryKey)
+          throws IOException {
+        Entry entry = entryKey;
+        String field = entry.field;
+        FieldCache.DoubleParser parser = (FieldCache.DoubleParser) entry.custom;
+        if (parser == null) {
+          try {
+            return wrapper.getDoubles(reader, field, FieldCache.DEFAULT_DOUBLE_PARSER);
+          } catch (NumberFormatException ne) {
+            return wrapper.getDoubles(reader, field, FieldCache.NUMERIC_UTILS_DOUBLE_PARSER);
+          }
+        }
+        double[] retArray = null;
+        TermDocs termDocs = reader.termDocs();
+        TermEnum termEnum = reader.terms (new Term (field));
+        try {
+          do {
+            Term term = termEnum.term();
+            if (term==null || term.field() != field) break;
+            double termval = parser.parseDouble(term.text());
+            if (retArray == null) // late init
+              retArray = new double[reader.maxDoc()];
+            termDocs.seek (termEnum);
+            while (termDocs.next()) {
+              retArray[termDocs.doc()] = termval;
+            }
+          } while (termEnum.next());
+        } catch (StopFillCacheException stop) {
+        } finally {
+          termDocs.close();
+          termEnum.close();
+        }
+        if (retArray == null) // no values
+          retArray = new double[reader.maxDoc()];
+        return retArray;
+      }
+    }
+
+    public String[] getStrings(IndexReader reader, String field)
+        throws IOException {
+      return (String[]) caches.get(String.class).get(reader, new Entry(field, null));
+    }
+
+    static final class StringCache extends Cache {
+      StringCache(InsaneFieldCache wrapper) {
+        super(wrapper);
+      }
+
+      @Override
+      protected Object createValue(IndexReader reader, Entry entryKey)
+          throws IOException {
+        String field = StringHelper.intern(entryKey.field);
+        final String[] retArray = new String[reader.maxDoc()];
+        TermDocs termDocs = reader.termDocs();
+        TermEnum termEnum = reader.terms (new Term (field));
+        final int termCountHardLimit = reader.maxDoc();
+        int termCount = 0;
+        try {
+          do {
+            if (termCount++ == termCountHardLimit) {
+              // app is misusing the API (there is more than
+              // one term per doc); in this case we make best
+              // effort to load what we can (see LUCENE-2142)
+              break;
+            }
+
+            Term term = termEnum.term();
+            if (term==null || term.field() != field) break;
+            String termval = term.text();
+            termDocs.seek (termEnum);
+            while (termDocs.next()) {
+              retArray[termDocs.doc()] = termval;
+            }
+          } while (termEnum.next());
+        } finally {
+          termDocs.close();
+          termEnum.close();
+        }
+        return retArray;
+      }
+    }
+
+    public FieldCache.StringIndex getStringIndex(IndexReader reader, String field)
+        throws IOException {
+      return (FieldCache.StringIndex) caches.get(FieldCache.StringIndex.class).get(reader, new Entry(field, null));
+    }
+
+    static final class StringIndexCache extends Cache {
+      StringIndexCache(InsaneFieldCache wrapper) {
+        super(wrapper);
+      }
+
+      @Override
+      protected Object createValue(IndexReader reader, Entry entryKey)
+          throws IOException {
+        String field = StringHelper.intern(entryKey.field);
+        final int[] retArray = new int[reader.maxDoc()];
+        String[] mterms = new String[reader.maxDoc()+1];
+        TermDocs termDocs = reader.termDocs();
+        TermEnum termEnum = reader.terms (new Term (field));
+        int t = 0;  // current term number
+
+        // an entry for documents that have no terms in this field
+        // should a document with no terms be at top or bottom?
+        // this puts them at the top - if it is changed, FieldDocSortedHitQueue
+        // needs to change as well.
+        mterms[t++] = null;
+
+        try {
+          do {
+            Term term = termEnum.term();
+            if (term==null || term.field() != field || t >= mterms.length) break;
+
+            // store term text
+            mterms[t] = term.text();
+
+            termDocs.seek (termEnum);
+            while (termDocs.next()) {
+              retArray[termDocs.doc()] = t;
+            }
+
+            t++;
+          } while (termEnum.next());
+        } finally {
+          termDocs.close();
+          termEnum.close();
+        }
+
+        if (t == 0) {
+          // if there are no terms, make the term array
+          // have a single null entry
+          mterms = new String[1];
+        } else if (t < mterms.length) {
+          // if there are less terms than documents,
+          // trim off the dead array space
+          String[] terms = new String[t];
+          System.arraycopy (mterms, 0, terms, 0, t);
+          mterms = terms;
+        }
+
+        FieldCache.StringIndex value = new FieldCache.StringIndex(retArray, mterms);
+        return value;
+      }
+    }
+
+    static final class DocsWithFieldCache extends Cache {
+
+      DocsWithFieldCache(InsaneFieldCache wrapper) {
+        super(wrapper);
+      }
+
+      @Override
+      protected Object createValue(IndexReader reader, Entry entryKey)
+      throws IOException {
+        final Entry entry = entryKey;
+        final String field = entry.field;
+        FixedBitSet res = null;
+        final TermDocs termDocs = reader.termDocs();
+        final TermEnum termEnum = reader.terms(new Term(field));
+        try {
+          do {
+            final Term term = termEnum.term();
+            if (term == null || term.field() != field) break;
+            if (res == null) // late init
+              res = new FixedBitSet(reader.maxDoc());
+            termDocs.seek(termEnum);
+            while (termDocs.next()) {
+              res.set(termDocs.doc());
+            }
+          } while (termEnum.next());
+        } finally {
+          termDocs.close();
+          termEnum.close();
+        }
+        if (res == null)
+          return new Bits.MatchNoBits(reader.maxDoc());
+        final int numSet = res.cardinality();
+        if (numSet >= reader.numDocs()) {
+          // The cardinality of the BitSet is numDocs if all documents have a value.
+          // As deleted docs are not in TermDocs, this is always true
+          assert numSet == reader.numDocs();
+          return new Bits.MatchAllBits(reader.maxDoc());
+        }
+        return res;
+      }
+    }
+
+    public Bits getDocsWithField(IndexReader reader, String field) throws IOException {
+      return (Bits) caches.get(DocsWithFieldCache.class).get(reader, new Entry(field, null));
+    }
+
+    private volatile PrintStream infoStream;
+
+    public void setInfoStream(PrintStream stream) {
+      infoStream = stream;
+    }
+
+    public PrintStream getInfoStream() {
+      return infoStream;
+    }
+
+  }
+
+}
Index: solr/core/src/java/org/apache/solr/search/MissingStringLastComparatorSource.java
===================================================================
--- solr/core/src/java/org/apache/solr/search/MissingStringLastComparatorSource.java	(revision 1177602)
+++ solr/core/src/java/org/apache/solr/search/MissingStringLastComparatorSource.java	(revision )
@@ -177,7 +177,7 @@
 
     @Override
     public void setNextReader(IndexReader reader, int docBase) throws IOException {
-      FieldCache.StringIndex currentReaderValues = FieldCache.DEFAULT.getStringIndex(reader, field);
+      FieldCache.StringIndex currentReaderValues = reader.getFieldCache().getStringIndex(field);
       currentReaderGen++;
       order = currentReaderValues.order;
       lookup = currentReaderValues.lookup;
Index: solr/core/src/java/org/apache/solr/search/SolrIndexReader.java
===================================================================
--- solr/core/src/java/org/apache/solr/search/SolrIndexReader.java	(revision 1177602)
+++ solr/core/src/java/org/apache/solr/search/SolrIndexReader.java	(revision )
@@ -19,6 +19,7 @@
 
 
 import org.apache.lucene.index.*;
+import org.apache.lucene.search.cache.AtomicFieldCache;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.LockObtainFailedException;
 import org.apache.lucene.document.Document;
@@ -494,10 +495,16 @@
   public int getTermInfosIndexDivisor() {
     return in.getTermInfosIndexDivisor();
   }
+
+  @Override
+  public AtomicFieldCache getFieldCache() {
+    return new SlowMultiReaderWrapper(this).getFieldCache();
-}
+  }
 
+}
 
 
+
 /** SolrReaderInfo contains information that is the same for
  * every SolrIndexReader that wraps the same IndexReader.
  * Multiple SolrIndexReader instances will be accessing this
@@ -510,4 +517,4 @@
   }
   public IndexReader getReader() { return reader; }
 
-}
\ No newline at end of file
+}
Index: lucene/contrib/spatial/src/java/org/apache/lucene/spatial/geohash/GeoHashDistanceFilter.java
===================================================================
--- lucene/contrib/spatial/src/java/org/apache/lucene/spatial/geohash/GeoHashDistanceFilter.java	(revision 1177602)
+++ lucene/contrib/spatial/src/java/org/apache/lucene/spatial/geohash/GeoHashDistanceFilter.java	(revision )
@@ -61,9 +61,8 @@
 
   @Override
   public DocIdSet getDocIdSet(IndexReader reader) throws IOException {
+    final String[] geoHashValues = reader.getFieldCache().getStrings(geoHashField);
 
-    final String[] geoHashValues = FieldCache.DEFAULT.getStrings(reader, geoHashField);
-
     final int docBase = nextDocBase;
     nextDocBase += reader.maxDoc();
 
Index: lucene/src/java/org/apache/lucene/search/FieldCacheRangeFilter.java
===================================================================
--- lucene/src/java/org/apache/lucene/search/FieldCacheRangeFilter.java	(revision 1177602)
+++ lucene/src/java/org/apache/lucene/search/FieldCacheRangeFilter.java	(revision )
@@ -82,7 +82,7 @@
     return new FieldCacheRangeFilter<String>(field, null, lowerVal, upperVal, includeLower, includeUpper) {
       @Override
       public DocIdSet getDocIdSet(IndexReader reader) throws IOException {
-        final FieldCache.StringIndex fcsi = FieldCache.DEFAULT.getStringIndex(reader, field);
+        final FieldCache.StringIndex fcsi = reader.getFieldCache().getStringIndex(field);
         final int lowerPoint = fcsi.binarySearchLookup(lowerVal);
         final int upperPoint = fcsi.binarySearchLookup(upperVal);
         
@@ -170,7 +170,7 @@
         if (inclusiveLowerPoint > inclusiveUpperPoint)
           return DocIdSet.EMPTY_DOCIDSET;
         
-        final byte[] values = FieldCache.DEFAULT.getBytes(reader, field, (FieldCache.ByteParser) parser);
+        final byte[] values = reader.getFieldCache().getBytes(field, (FieldCache.ByteParser) parser);
         // we only request the usage of termDocs, if the range contains 0
         return new FieldCacheDocIdSet(reader, (inclusiveLowerPoint <= 0 && inclusiveUpperPoint >= 0)) {
           @Override
@@ -221,7 +221,7 @@
         if (inclusiveLowerPoint > inclusiveUpperPoint)
           return DocIdSet.EMPTY_DOCIDSET;
         
-        final short[] values = FieldCache.DEFAULT.getShorts(reader, field, (FieldCache.ShortParser) parser);
+        final short[] values = reader.getFieldCache().getShorts(field, (FieldCache.ShortParser) parser);
         // we only request the usage of termDocs, if the range contains 0
         return new FieldCacheDocIdSet(reader, (inclusiveLowerPoint <= 0 && inclusiveUpperPoint >= 0)) {
           @Override
@@ -272,7 +272,7 @@
         if (inclusiveLowerPoint > inclusiveUpperPoint)
           return DocIdSet.EMPTY_DOCIDSET;
         
-        final int[] values = FieldCache.DEFAULT.getInts(reader, field, (FieldCache.IntParser) parser);
+        final int[] values = reader.getFieldCache().getInts(field, (FieldCache.IntParser) parser);
         // we only request the usage of termDocs, if the range contains 0
         return new FieldCacheDocIdSet(reader, (inclusiveLowerPoint <= 0 && inclusiveUpperPoint >= 0)) {
           @Override
@@ -323,7 +323,7 @@
         if (inclusiveLowerPoint > inclusiveUpperPoint)
           return DocIdSet.EMPTY_DOCIDSET;
         
-        final long[] values = FieldCache.DEFAULT.getLongs(reader, field, (FieldCache.LongParser) parser);
+        final long[] values = reader.getFieldCache().getLongs(field, (FieldCache.LongParser) parser);
         // we only request the usage of termDocs, if the range contains 0
         return new FieldCacheDocIdSet(reader, (inclusiveLowerPoint <= 0L && inclusiveUpperPoint >= 0L)) {
           @Override
@@ -378,7 +378,7 @@
         if (inclusiveLowerPoint > inclusiveUpperPoint)
           return DocIdSet.EMPTY_DOCIDSET;
         
-        final float[] values = FieldCache.DEFAULT.getFloats(reader, field, (FieldCache.FloatParser) parser);
+        final float[] values = reader.getFieldCache().getFloats(field, (FieldCache.FloatParser) parser);
         // we only request the usage of termDocs, if the range contains 0
         return new FieldCacheDocIdSet(reader, (inclusiveLowerPoint <= 0.0f && inclusiveUpperPoint >= 0.0f)) {
           @Override
@@ -433,7 +433,7 @@
         if (inclusiveLowerPoint > inclusiveUpperPoint)
           return DocIdSet.EMPTY_DOCIDSET;
         
-        final double[] values = FieldCache.DEFAULT.getDoubles(reader, field, (FieldCache.DoubleParser) parser);
+        final double[] values = reader.getFieldCache().getDoubles(field, (FieldCache.DoubleParser) parser);
         // we only request the usage of termDocs, if the range contains 0
         return new FieldCacheDocIdSet(reader, (inclusiveLowerPoint <= 0.0 && inclusiveUpperPoint >= 0.0)) {
           @Override
Index: lucene/src/java/org/apache/lucene/search/function/ShortFieldSource.java
===================================================================
--- lucene/src/java/org/apache/lucene/search/function/ShortFieldSource.java	(revision 1177602)
+++ lucene/src/java/org/apache/lucene/search/function/ShortFieldSource.java	(revision )
@@ -19,7 +19,7 @@
 
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.search.FieldCache;
-import org.apache.lucene.search.function.DocValues;
+import org.apache.lucene.search.cache.AtomicFieldCache;
 
 import java.io.IOException;
 
@@ -67,8 +67,8 @@
 
   /*(non-Javadoc) @see org.apache.lucene.search.function.FieldCacheSource#getCachedValues(org.apache.lucene.search.FieldCache, java.lang.String, org.apache.lucene.index.IndexReader) */
   @Override
-  public DocValues getCachedFieldValues (FieldCache cache, String field, IndexReader reader) throws IOException {
-    final short[] arr = cache.getShorts(reader, field, parser);
+  public DocValues getCachedFieldValues (AtomicFieldCache cache, String field, IndexReader reader) throws IOException {
+    final short[] arr = cache.getShorts(field, parser);
     return new DocValues() {
       /*(non-Javadoc) @see org.apache.lucene.search.function.DocValues#floatVal(int) */
       @Override
Index: lucene/src/java/org/apache/lucene/search/function/ReverseOrdFieldSource.java
===================================================================
--- lucene/src/java/org/apache/lucene/search/function/ReverseOrdFieldSource.java	(revision 1177602)
+++ lucene/src/java/org/apache/lucene/search/function/ReverseOrdFieldSource.java	(revision )
@@ -18,6 +18,7 @@
 package org.apache.lucene.search.function;
 
 import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.SlowMultiReaderWrapper;
 import org.apache.lucene.search.FieldCache;
 
 import java.io.IOException;
@@ -70,7 +71,7 @@
   /*(non-Javadoc) @see org.apache.lucene.search.function.ValueSource#getValues(org.apache.lucene.index.IndexReader) */
   @Override
   public DocValues getValues(IndexReader reader) throws IOException {
-    final FieldCache.StringIndex sindex = FieldCache.DEFAULT.getStringIndex(reader, field);
+    final FieldCache.StringIndex sindex = new SlowMultiReaderWrapper(reader).getFieldCache().getStringIndex(field);
 
     final int arr[] = sindex.order;
     final int end = sindex.lookup.length;
Index: lucene/src/java/org/apache/lucene/search/function/FieldCacheSource.java
===================================================================
--- lucene/src/java/org/apache/lucene/search/function/FieldCacheSource.java	(revision 1177602)
+++ lucene/src/java/org/apache/lucene/search/function/FieldCacheSource.java	(revision )
@@ -21,6 +21,7 @@
 
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.search.FieldCache;
+import org.apache.lucene.search.cache.AtomicFieldCache;
 
 /**
  * Expert: A base class for ValueSource implementations that retrieve values for
@@ -56,7 +57,7 @@
   /* (non-Javadoc) @see org.apache.lucene.search.function.ValueSource#getValues(org.apache.lucene.index.IndexReader) */
   @Override
   public final DocValues getValues(IndexReader reader) throws IOException {
-    return getCachedFieldValues(FieldCache.DEFAULT, field, reader);
+    return getCachedFieldValues(reader.getFieldCache(), field, reader);
   }
 
   /* (non-Javadoc) @see org.apache.lucene.search.function.ValueSource#description() */
@@ -67,11 +68,12 @@
 
   /**
    * Return cached DocValues for input field and reader.
+   *
    * @param cache FieldCache so that values of a field are loaded once per reader (RAM allowing)
    * @param field Field for which values are required.
    * @see ValueSource
    */
-  public abstract DocValues getCachedFieldValues(FieldCache cache, String field, IndexReader reader) throws IOException;
+  public abstract DocValues getCachedFieldValues(AtomicFieldCache cache, String field, IndexReader reader) throws IOException;
 
   /*(non-Javadoc) @see java.lang.Object#equals(java.lang.Object) */
   @Override
Index: solr/core/src/java/org/apache/solr/request/SimpleFacets.java
===================================================================
--- solr/core/src/java/org/apache/solr/request/SimpleFacets.java	(revision 1177602)
+++ solr/core/src/java/org/apache/solr/request/SimpleFacets.java	(revision )
@@ -17,10 +17,7 @@
 
 package org.apache.solr.request;
 
-import org.apache.lucene.index.Term;
-import org.apache.lucene.index.TermEnum;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.TermDocs;
+import org.apache.lucene.index.*;
 import org.apache.lucene.queryParser.ParseException;
 import org.apache.lucene.search.*;
 import org.apache.lucene.search.grouping.AbstractAllGroupHeadsCollector;
@@ -383,7 +380,7 @@
     FieldType ft = searcher.getSchema().getFieldType(fieldName);
     NamedList res = new NamedList();
 
-    FieldCache.StringIndex si = FieldCache.DEFAULT.getStringIndex(searcher.getReader(), fieldName);
+    FieldCache.StringIndex si = new SlowMultiReaderWrapper(searcher.getReader()).getFieldCache().getStringIndex(fieldName);
     final String[] terms = si.lookup;
     final int[] termNum = si.order;
 
Index: solr/core/src/java/org/apache/solr/core/SolrCore.java
===================================================================
--- solr/core/src/java/org/apache/solr/core/SolrCore.java	(revision 1177602)
+++ solr/core/src/java/org/apache/solr/core/SolrCore.java	(revision )
@@ -535,8 +535,6 @@
       infoRegistry = new ConcurrentHashMap<String, SolrInfoMBean>();
     }
 
-    infoRegistry.put("fieldCache", new SolrFieldCacheMBean());
-
     this.schema = schema;
     this.dataDir = dataDir;
     this.solrConfig = config;
Index: lucene/src/java/org/apache/lucene/search/function/OrdFieldSource.java
===================================================================
--- lucene/src/java/org/apache/lucene/search/function/OrdFieldSource.java	(revision 1177602)
+++ lucene/src/java/org/apache/lucene/search/function/OrdFieldSource.java	(revision )
@@ -18,6 +18,7 @@
 package org.apache.lucene.search.function;
 
 import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.SlowMultiReaderWrapper;
 import org.apache.lucene.search.FieldCache;
 
 import java.io.IOException;
@@ -69,7 +70,7 @@
   /*(non-Javadoc) @see org.apache.lucene.search.function.ValueSource#getValues(org.apache.lucene.index.IndexReader) */
   @Override
   public DocValues getValues(IndexReader reader) throws IOException {
-    final int[] arr = FieldCache.DEFAULT.getStringIndex(reader, field).order;
+    final int[] arr = new SlowMultiReaderWrapper(reader).getFieldCache().getStringIndex(field).order;
     return new DocValues() {
       /*(non-Javadoc) @see org.apache.lucene.search.function.DocValues#floatVal(int) */
       @Override
Index: solr/core/src/java/org/apache/solr/search/SolrIndexSearcher.java
===================================================================
--- solr/core/src/java/org/apache/solr/search/SolrIndexSearcher.java	(revision 1177602)
+++ solr/core/src/java/org/apache/solr/search/SolrIndexSearcher.java	(revision )
@@ -223,6 +223,7 @@
       cache.setState(SolrCache.State.LIVE);
       core.getInfoRegistry().put(cache.name(), cache);
     }
+    core.getInfoRegistry().put("fieldCache", new SolrFieldCacheMBean(this));
     registerTime=System.currentTimeMillis();
   }
 
Index: lucene/src/java/org/apache/lucene/search/FieldCacheTermsFilter.java
===================================================================
--- lucene/src/java/org/apache/lucene/search/FieldCacheTermsFilter.java	(revision 1177602)
+++ lucene/src/java/org/apache/lucene/search/FieldCacheTermsFilter.java	(revision )
@@ -20,6 +20,7 @@
 import java.io.IOException;
 
 import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.search.cache.AtomicFieldCache;
 import org.apache.lucene.util.FixedBitSet;
 import org.apache.lucene.index.TermDocs;  // for javadocs
 
@@ -102,13 +103,18 @@
     this.terms = terms;
   }
 
+  @Deprecated
   public FieldCache getFieldCache() {
     return FieldCache.DEFAULT;
   }
 
+  public AtomicFieldCache getAtomicFieldCache(IndexReader ir) {
+    return ir.getFieldCache();
+  }
+
   @Override
   public DocIdSet getDocIdSet(IndexReader reader) throws IOException {
-    return new FieldCacheTermsFilterDocIdSet(getFieldCache().getStringIndex(reader, field));
+    return new FieldCacheTermsFilterDocIdSet(getAtomicFieldCache(reader).getStringIndex(field));
   }
 
   protected class FieldCacheTermsFilterDocIdSet extends DocIdSet {
Index: solr/core/src/java/org/apache/solr/handler/component/QueryElevationComponent.java
===================================================================
--- solr/core/src/java/org/apache/solr/handler/component/QueryElevationComponent.java	(revision 1177602)
+++ solr/core/src/java/org/apache/solr/handler/component/QueryElevationComponent.java	(revision )
@@ -497,7 +497,7 @@
 
       @Override
       public void setNextReader(IndexReader reader, int docBase) throws IOException {
-        idIndex = FieldCache.DEFAULT.getStringIndex(reader, fieldname);
+        idIndex = reader.getFieldCache().getStringIndex(fieldname);
       }
 
       @Override
Index: lucene/src/java/org/apache/lucene/search/function/FloatFieldSource.java
===================================================================
--- lucene/src/java/org/apache/lucene/search/function/FloatFieldSource.java	(revision 1177602)
+++ lucene/src/java/org/apache/lucene/search/function/FloatFieldSource.java	(revision )
@@ -19,7 +19,7 @@
 
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.search.FieldCache;
-import org.apache.lucene.search.function.DocValues;
+import org.apache.lucene.search.cache.AtomicFieldCache;
 
 import java.io.IOException;
 
@@ -67,8 +67,8 @@
 
   /*(non-Javadoc) @see org.apache.lucene.search.function.FieldCacheSource#getCachedValues(org.apache.lucene.search.FieldCache, java.lang.String, org.apache.lucene.index.IndexReader) */
   @Override
-  public DocValues getCachedFieldValues (FieldCache cache, String field, IndexReader reader) throws IOException {
-    final float[] arr = cache.getFloats(reader, field, parser);
+  public DocValues getCachedFieldValues (AtomicFieldCache cache, String field, IndexReader reader) throws IOException {
+    final float[] arr = cache.getFloats(field, parser);
     return new DocValues() {
       /*(non-Javadoc) @see org.apache.lucene.search.function.DocValues#floatVal(int) */
       @Override
@@ -106,4 +106,4 @@
     return parser==null ? 
       Float.class.hashCode() : parser.getClass().hashCode();
   }
-}
\ No newline at end of file
+}
Index: lucene/contrib/grouping/src/java/org/apache/lucene/search/grouping/TermSecondPassGroupingCollector.java
===================================================================
--- lucene/contrib/grouping/src/java/org/apache/lucene/search/grouping/TermSecondPassGroupingCollector.java	(revision 1177602)
+++ lucene/contrib/grouping/src/java/org/apache/lucene/search/grouping/TermSecondPassGroupingCollector.java	(revision )
@@ -50,7 +50,7 @@
   @Override
   public void setNextReader(IndexReader reader, int docBase) throws IOException {
     super.setNextReader(reader, docBase);
-    index = FieldCache.DEFAULT.getStringIndex(reader, groupField);
+    index = reader.getFieldCache().getStringIndex(groupField);
 
     // Rebuild ordSet
     ordSet.clear();
@@ -71,4 +71,4 @@
     }
     return null;
   }
-}
\ No newline at end of file
+}
Index: solr/core/src/java/org/apache/solr/search/SolrFieldCacheMBean.java
===================================================================
--- solr/core/src/java/org/apache/solr/search/SolrFieldCacheMBean.java	(revision 1177602)
+++ solr/core/src/java/org/apache/solr/search/SolrFieldCacheMBean.java	(revision )
@@ -18,7 +18,12 @@
 package org.apache.solr.search;
 
 import java.net.URL;
+import java.util.ArrayList;
+import java.util.List;
 
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.SlowMultiReaderWrapper;
+import org.apache.lucene.util.ReaderUtil;
 import org.apache.solr.common.util.NamedList;
 import org.apache.solr.common.util.SimpleOrderedMap;
 
@@ -37,8 +42,13 @@
  */
 public class SolrFieldCacheMBean implements SolrInfoMBean {
 
+  private final SolrIndexSearcher indexSearcher;
   protected FieldCacheSanityChecker checker = new FieldCacheSanityChecker();
 
+  public SolrFieldCacheMBean(SolrIndexSearcher indexSearcher) {
+    this.indexSearcher = indexSearcher;
+  }
+
   public String getName() { return this.getClass().getName(); }
   public String getVersion() { return SolrCore.version; }
   public String getDescription() {
@@ -56,17 +66,20 @@
     return null;
   }
   public NamedList getStatistics() {
-    NamedList stats = new SimpleOrderedMap();
-    CacheEntry[] entries = FieldCache.DEFAULT.getCacheEntries();
-    stats.add("entries_count", entries.length);
+    NamedList<Object> stats = new SimpleOrderedMap<Object>();
+
+    NamedList<Object> topLevelStats = new SimpleOrderedMap<Object>();
+    IndexReader topReader = indexSearcher.getReader();
+    CacheEntry[] entries = new SlowMultiReaderWrapper(topReader).getFieldCache().getCacheEntries();
+    topLevelStats.add("entries_count", entries.length);
     for (int i = 0; i < entries.length; i++) {
       CacheEntry e = entries[i];
-      stats.add("entry#" + i, e.toString());
+      topLevelStats.add("entry#" + i, e.toString());
     }
 
     Insanity[] insanity = checker.check(entries);
 
-    stats.add("insanity_count", insanity.length);
+    topLevelStats.add("insanity_count", insanity.length);
     for (int i = 0; i < insanity.length; i++) {
 
       /** RAM estimation is both CPU and memory intensive... we don't want to do it unless asked.
@@ -76,9 +89,35 @@
         if (null == e.getEstimatedSize()) e.estimateSize();
       }
       **/
-      
+
-      stats.add("insanity#" + i, insanity[i].toString());
+      topLevelStats.add("insanity#" + i, insanity[i].toString());
     }
+    stats.add("top_entries", topLevelStats);
+
+    NamedList<Object> leaveEntriesStats = new SimpleOrderedMap<Object>();
+    List<IndexReader> subReaders = new ArrayList<IndexReader>();
+    ReaderUtil.gatherSubReaders(subReaders, indexSearcher.getReader());
+    for (IndexReader leave : subReaders) {
+      NamedList<Object> leaveEntryStats = new SimpleOrderedMap<Object>();
+
+      CacheEntry[] leaveEntries = leave.getFieldCache().getCacheEntries();
+      leaveEntryStats.add("entries_count", leaveEntries.length);
+      for (int i = 0; i < leaveEntries.length; i++) {
+        CacheEntry e = leaveEntries[i];
+        leaveEntryStats.add("entry#" + i, e.toString());
+      }
+
+      Insanity[] leaveInsanity = checker.check(leaveEntries);
+
+      leaveEntryStats.add("insanity_count", leaveInsanity.length);
+      for (int i = 0; i < leaveInsanity.length; i++) {
+        leaveEntryStats.add("insanity#" + i, leaveInsanity[i].toString());
+      }
+
+
+      leaveEntriesStats.add(leave.getCoreCacheKey().toString(), leaveEntryStats);
+    }
+    stats.add("leave_entries", leaveEntriesStats);
     return stats;
   }
 
Index: lucene/src/java/org/apache/lucene/search/cache/AtomicFieldCache.java
===================================================================
--- lucene/src/java/org/apache/lucene/search/cache/AtomicFieldCache.java	(revision )
+++ lucene/src/java/org/apache/lucene/search/cache/AtomicFieldCache.java	(revision )
@@ -0,0 +1,228 @@
+package org.apache.lucene.search.cache;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.search.DocIdSet;
+import org.apache.lucene.util.Bits;
+
+import java.io.IOException;
+import java.io.PrintStream;
+
+import static org.apache.lucene.search.FieldCache.*;
+
+/**
+ * Expert: Maintains caches of term values.
+ */
+public interface AtomicFieldCache {
+
+  /** Checks the internal cache for an appropriate entry, and if none is
+   * found, reads the terms in <code>field</code> as a single byte and returns an array
+   * of size <code>reader.maxDoc()</code> of the value each document
+   * has in the given field.
+   * @param field   Which field contains the single byte values.
+   * @return The values in the given field for each document.
+   * @throws java.io.IOException  If any error occurs.
+   */
+  public byte[] getBytes (String field) throws IOException;
+
+  /** Checks the internal cache for an appropriate entry, and if none is found,
+   * reads the terms in <code>field</code> as bytes and returns an array of
+   * size <code>reader.maxDoc()</code> of the value each document has in the
+   * given field.
+   * @param field   Which field contains the bytes.
+   * @param parser  Computes byte for string values.
+   * @return The values in the given field for each document.
+   * @throws IOException  If any error occurs.
+   */
+  public byte[] getBytes (String field, ByteParser parser) throws IOException;
+
+  /** Checks the internal cache for an appropriate entry, and if none is
+   * found, reads the terms in <code>field</code> as shorts and returns an array
+   * of size <code>reader.maxDoc()</code> of the value each document
+   * has in the given field.
+   * @param field   Which field contains the shorts.
+   * @return The values in the given field for each document.
+   * @throws IOException  If any error occurs.
+   */
+  public short[] getShorts(String field) throws IOException;
+
+  /** Checks the internal cache for an appropriate entry, and if none is found,
+   * reads the terms in <code>field</code> as shorts and returns an array of
+   * size <code>reader.maxDoc()</code> of the value each document has in the
+   * given field.
+   * @param field   Which field contains the shorts.
+   * @param parser  Computes short for string values.
+   * @return The values in the given field for each document.
+   * @throws IOException  If any error occurs.
+   */
+  public short[] getShorts(String field, ShortParser parser) throws IOException;
+
+
+  /** Checks the internal cache for an appropriate entry, and if none is
+   * found, reads the terms in <code>field</code> as integers and returns an array
+   * of size <code>reader.maxDoc()</code> of the value each document
+   * has in the given field.
+   * @param field   Which field contains the integers.
+   * @return The values in the given field for each document.
+   * @throws IOException  If any error occurs.
+   */
+  public int[] getInts(String field) throws IOException;
+
+  /** Checks the internal cache for an appropriate entry, and if none is found,
+   * reads the terms in <code>field</code> as integers and returns an array of
+   * size <code>reader.maxDoc()</code> of the value each document has in the
+   * given field.
+   * @param field   Which field contains the integers.
+   * @param parser  Computes integer for string values.
+   * @return The values in the given field for each document.
+   * @throws IOException  If any error occurs.
+   */
+  public int[] getInts(String field, IntParser parser) throws IOException;
+
+
+  /** Checks the internal cache for an appropriate entry, and if
+   * none is found, reads the terms in <code>field</code> as floats and returns an array
+   * of size <code>reader.maxDoc()</code> of the value each document
+   * has in the given field.
+   * @param field   Which field contains the floats.
+   * @return The values in the given field for each document.
+   * @throws IOException  If any error occurs.
+   */
+  public float[] getFloats(String field) throws IOException;
+
+  /** Checks the internal cache for an appropriate entry, and if
+   * none is found, reads the terms in <code>field</code> as floats and returns an array
+   * of size <code>reader.maxDoc()</code> of the value each document
+   * has in the given field.
+   * @param field   Which field contains the floats.
+   * @param parser  Computes float for string values.
+   * @return The values in the given field for each document.
+   * @throws IOException  If any error occurs.
+   */
+  public float[] getFloats(String field, FloatParser parser) throws IOException;
+
+
+  /**
+   * Checks the internal cache for an appropriate entry, and if none is
+   * found, reads the terms in <code>field</code> as longs and returns an array
+   * of size <code>reader.maxDoc()</code> of the value each document
+   * has in the given field.
+   *
+   * @param field  Which field contains the longs.
+   * @return The values in the given field for each document.
+   * @throws java.io.IOException If any error occurs.
+   */
+  public long[] getLongs(String field) throws IOException;
+
+  /**
+   * Checks the internal cache for an appropriate entry, and if none is found,
+   * reads the terms in <code>field</code> as longs and returns an array of
+   * size <code>reader.maxDoc()</code> of the value each document has in the
+   * given field.
+   *
+   * @param field  Which field contains the longs.
+   * @param parser Computes integer for string values.
+   * @return The values in the given field for each document.
+   * @throws IOException If any error occurs.
+   */
+  public long[] getLongs(String field, LongParser parser) throws IOException;
+
+
+  /**
+   * Checks the internal cache for an appropriate entry, and if none is
+   * found, reads the terms in <code>field</code> as integers and returns an array
+   * of size <code>reader.maxDoc()</code> of the value each document
+   * has in the given field.
+   *
+   * @param field  Which field contains the doubles.
+   * @return The values in the given field for each document.
+   * @throws IOException If any error occurs.
+   */
+  public double[] getDoubles(String field) throws IOException;
+
+  /**
+   * Checks the internal cache for an appropriate entry, and if none is found,
+   * reads the terms in <code>field</code> as doubles and returns an array of
+   * size <code>reader.maxDoc()</code> of the value each document has in the
+   * given field.
+   *
+   * @param field  Which field contains the doubles.
+   * @param parser Computes integer for string values.
+   * @return The values in the given field for each document.
+   * @throws IOException If any error occurs.
+   */
+  public double[] getDoubles(String field, DoubleParser parser) throws IOException;
+
+  /** Checks the internal cache for an appropriate entry, and if none
+   * is found, reads the term values in <code>field</code> and returns an array
+   * of size <code>reader.maxDoc()</code> containing the value each document
+   * has in the given field.
+   * @param field   Which field contains the strings.
+   * @return The values in the given field for each document.
+   * @throws IOException  If any error occurs.
+   */
+  public String[] getStrings (String field) throws IOException;
+
+  /** Checks the internal cache for an appropriate entry, and if none
+   * is found reads the term values in <code>field</code> and returns
+   * an array of them in natural order, along with an array telling
+   * which element in the term array each document uses.
+   * @param field   Which field contains the strings.
+   * @return Array of terms and index into the array for each document.
+   * @throws IOException  If any error occurs.
+   */
+  public StringIndex getStringIndex (String field) throws IOException;
+
+  /**
+   * EXPERT: Generates an array of CacheEntry objects representing all items
+   * currently in the FieldCache.
+   * <p>
+   * NOTE: These CacheEntry objects maintain a strong reference to the
+   * Cached Values.  Maintaining references to a CacheEntry the IndexReader
+   * associated with it has garbage collected will prevent the Value itself
+   * from being garbage collected when the Cache drops the WeakReference.
+   * </p>
+   * @lucene.experimental
+   */
+  public abstract CacheEntry[] getCacheEntries();
+
+  /**
+   * Expert: drops all cache entries associated with this
+   * field cache.
+   */
+  public abstract void purgeCache();
+
+  /** Checks the internal cache for an appropriate entry, and if none is found,
+   * reads the terms in <code>field</code> and returns a bit set at the size of
+   * <code>reader.maxDoc()</code>, with turned on bits for each docid that
+   * does have a value for this field.
+   */
+  public Bits getDocsWithField(String field) throws IOException;
+
+  /**
+   * If non-null, FieldCacheImpl will warn whenever
+   * entries are created that are not sane according to
+   * {@link org.apache.lucene.util.FieldCacheSanityChecker}.
+   */
+  public void setInfoStream(PrintStream stream);
+
+  /** counterpart of {@link #setInfoStream(PrintStream)} */
+  public PrintStream getInfoStream();
+
+}
Index: lucene/src/test-framework/org/apache/lucene/util/LuceneTestCase.java
===================================================================
--- lucene/src/test-framework/org/apache/lucene/util/LuceneTestCase.java	(revision 1177602)
+++ lucene/src/test-framework/org/apache/lucene/util/LuceneTestCase.java	(revision )
@@ -50,6 +50,7 @@
 import org.apache.lucene.search.AssertingIndexSearcher;
 import org.apache.lucene.search.FieldCache;
 import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.cache.AtomicFieldCache;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.FSDirectory;
 import org.apache.lucene.store.LockFactory;
@@ -480,6 +481,14 @@
     fc.purgeAllCaches();
   }
 
+  protected void purgeFieldCache(final AtomicFieldCache fc) {
+     if (SlowMultiReaderWrapper.InsaneNonAtomicFieldCache.class.isInstance(fc)) {
+      ((SlowMultiReaderWrapper.InsaneNonAtomicFieldCache) fc).purgeAllCaches();
+    } else {
+      fc.purgeCache();
+    }
+  }
+
   protected String getTestLabel() {
     return getClass().getName() + "." + getName();
   }
Index: lucene/src/java/org/apache/lucene/index/SegmentFieldCacheImpl.java
===================================================================
--- lucene/src/java/org/apache/lucene/index/SegmentFieldCacheImpl.java	(revision )
+++ lucene/src/java/org/apache/lucene/index/SegmentFieldCacheImpl.java	(revision )
@@ -0,0 +1,715 @@
+package org.apache.lucene.index;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.search.DocIdSet;
+import org.apache.lucene.search.FieldCache;
+import org.apache.lucene.search.cache.AtomicFieldCache;
+import org.apache.lucene.search.cache.StopFillCacheException;
+import org.apache.lucene.util.*;
+
+import java.io.IOException;
+import java.io.PrintStream;
+import java.util.ArrayList;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+
+/**
+ *
+ */
+class SegmentFieldCacheImpl implements AtomicFieldCache {
+
+  private IndexReader indexReader;
+  private final Map<Class<?>, Cache> cache; 
+
+  SegmentFieldCacheImpl(IndexReader indexReader) {
+    if (indexReader == null) {
+      throw new IllegalArgumentException("Supplied indexReader cannot be null");
+    }
+
+    this.indexReader = indexReader;
+    cache = new HashMap<Class<?>, Cache>(9);
+    initCache();
+  }
+
+  private void initCache() {
+    cache.put(Byte.TYPE, new ByteCache(this, indexReader));
+    cache.put(Short.TYPE, new ShortCache(this, indexReader));
+    cache.put(Integer.TYPE, new IntCache(this, indexReader));
+    cache.put(Float.TYPE, new FloatCache(this, indexReader));
+    cache.put(Long.TYPE, new LongCache(this, indexReader));
+    cache.put(Double.TYPE, new DoubleCache(this, indexReader));
+    cache.put(String.class, new StringCache(this, indexReader));
+    cache.put(FieldCache.StringIndex.class, new StringIndexCache(this, indexReader));
+    cache.put(DocsWithFieldCache.class, new DocsWithFieldCache(this, indexReader));
+  }
+
+  // Invoked incase of IR reopen / clone. Sometimes the IR instance change, so we need to update.
+  void updateIndexReader(IndexReader ir) {
+    this.indexReader = ir;
+    for (Cache c : cache.values()) {
+      c.updateIR(ir);
+    }
+  }
+
+  /**
+   * {@inheritDoc}
+   */
+  public byte[] getBytes(String field) throws IOException {
+    return getBytes(field, null);
+  }
+
+  // inherit javadocs
+  public byte[] getBytes(String field, FieldCache.ByteParser parser) throws IOException {
+    return (byte[]) cache.get(Byte.TYPE).get(new Entry(field, parser));
+  }
+
+  // inherit javadocs
+  public short[] getShorts(String field) throws IOException {
+    return getShorts(field, null);
+  }
+
+  // inherit javadocs
+  public short[] getShorts(String field, FieldCache.ShortParser parser) throws IOException {
+    return (short[]) cache.get(Short.TYPE).get(new Entry(field, parser));
+  }
+
+  // inherit javadocs
+  public int[] getInts(String field) throws IOException {
+    return getInts(field, null);
+  }
+
+  // inherit javadocs
+  public int[] getInts(String field, FieldCache.IntParser parser) throws IOException {
+    return (int[]) cache.get(Integer.TYPE).get(new Entry(field, parser));
+  }
+
+  // inherit javadocs
+  public float[] getFloats(String field) throws IOException {
+    return getFloats(field, null);
+  }
+
+  // inherit javadocs
+  public float[] getFloats(String field, FieldCache.FloatParser parser) throws IOException {
+    return (float[]) cache.get(Float.TYPE).get(new Entry(field, parser));
+  }
+
+  public long[] getLongs(String field) throws IOException {
+    return getLongs(field, null);
+  }
+
+  // inherit javadocs
+  public long[] getLongs(String field, FieldCache.LongParser parser) throws IOException {
+    return (long[]) cache.get(Long.TYPE).get(new Entry(field, parser));
+  }
+
+  // inherit javadocs
+  public double[] getDoubles(String field) throws IOException {
+    return getDoubles(field, null);
+  }
+
+  // inherit javadocs
+  public double[] getDoubles(String field, FieldCache.DoubleParser parser) throws IOException {
+    return (double[]) cache.get(Double.TYPE).get(new Entry(field, parser));
+  }
+
+  // inherit javadocs
+  public String[] getStrings(String field) throws IOException {
+    return (String[]) cache.get(String.class).get(new Entry(field, null));
+  }
+
+  // inherit javadocs
+  public FieldCache.StringIndex getStringIndex(String field) throws IOException {
+    return (FieldCache.StringIndex) cache.get(FieldCache.StringIndex.class).get(new Entry(field, null));
+  }
+
+  public Bits getDocsWithField(String field)
+      throws IOException {
+    return (Bits) cache.get(DocsWithFieldCache.class).get(new Entry(field, null));
+  }
+
+  private volatile PrintStream infoStream;
+
+  public void setInfoStream(PrintStream stream) {
+    infoStream = stream;
+  }
+
+  public PrintStream getInfoStream() {
+    return infoStream;
+  }
+
+  public FieldCache.CacheEntry[] getCacheEntries() {
+    List<FieldCache.CacheEntry> result = new ArrayList<FieldCache.CacheEntry>(17);
+    for (final Map.Entry<Class<?>, Cache> cacheEntry : cache.entrySet()) {
+      final Class<?> cacheType = cacheEntry.getKey();
+      final Cache<?> cache = cacheEntry.getValue();
+      synchronized (cache.readerCache) {
+        for (final Map.Entry<?, Object> mapEntry : cache.readerCache.entrySet()) {
+          Entry entry = (Entry) mapEntry.getKey();
+          result.add(new CacheEntryImpl(indexReader, entry.field, cacheType, entry.custom, mapEntry.getValue()));
+        }
+      }
+    }
+    return result.toArray(new FieldCache.CacheEntry[result.size()]);
+  }
+
+  public void purgeCache() {
+    cache.clear();
+    initCache();
+  }
+
+  static abstract class Cache<T> {
+
+    final AtomicFieldCache wrapper;
+    private IndexReader indexReader;
+    private final Map<Entry,Object> readerCache;
+
+    Cache(AtomicFieldCache wrapper, IndexReader indexReader) {
+      this.wrapper = wrapper;
+      this.indexReader = indexReader;
+      this.readerCache = new HashMap<Entry,Object>();
+    }
+
+    void updateIR(IndexReader ir) {
+      this.indexReader = ir;
+    }
+
+    protected abstract Object createValue(IndexReader reader, Entry entryKey) throws IOException;
+
+    @SuppressWarnings("unchecked")
+    public Object get(Entry key) throws IOException {
+      Object value;
+
+      synchronized (readerCache) {
+        value = readerCache.get(key);
+        if (value == null) {
+          value = new FieldCache.CreationPlaceholder();
+          readerCache.put(key, value);
+        }
+      }
+      if (value instanceof FieldCache.CreationPlaceholder) {
+        synchronized (value) {
+          FieldCache.CreationPlaceholder progress = (FieldCache.CreationPlaceholder) value;
+          if (progress.value != null) {
+            return progress.value;
+          }
+          progress.value = createValue(indexReader, key);
+          synchronized (readerCache) {
+            readerCache.put(key, progress.value);
+          }
+
+          // Only check if key.custom (the parser) is
+          // non-null; else, we check twice for a single
+          // call to FieldCache.getXXX
+          if (key.custom != null && wrapper != null) {
+            final PrintStream infoStream = wrapper.getInfoStream();
+            if (infoStream != null) {
+              printNewInsanity(infoStream, progress.value);
+            }
+          }
+          return progress.value;
+        }
+      }
+
+      return value;
+    }
+
+    private void printNewInsanity(PrintStream infoStream, Object value) {
+      final FieldCacheSanityChecker.Insanity[] insanities = FieldCacheSanityChecker.checkSanity(wrapper.getCacheEntries());
+      for(int i=0;i<insanities.length;i++) {
+        final FieldCacheSanityChecker.Insanity insanity = insanities[i];
+        final FieldCache.CacheEntry[] entries = insanity.getCacheEntries();
+        for(int j=0;j<entries.length;j++) {
+          if (entries[j].getValue() == value) {
+            // OK this insanity involves our entry
+            infoStream.println("WARNING: new FieldCache insanity created\nDetails: " + insanity.toString());
+            infoStream.println("\nStack:\n");
+            new Throwable().printStackTrace(infoStream);
+            break;
+          }
+        }
+      }
+    }
+  }
+
+  /** Expert: Every composite-key in the internal cache is of this type. */
+  static class Entry {
+    final String field;        // which Fieldable
+    final Object custom;       // which custom comparator or parser
+
+    /** Creates one of these objects for a custom comparator/parser. */
+    Entry (String field, Object custom) {
+      this.field = StringHelper.intern(field);
+      this.custom = custom;
+    }
+
+    /** Two of these are equal iff they reference the same field and type. */
+    @Override
+    public boolean equals (Object o) {
+      if (o instanceof Entry) {
+        Entry other = (Entry) o;
+        if (other.field == field) {
+          if (other.custom == null) {
+            if (custom == null) return true;
+          } else if (other.custom.equals (custom)) {
+            return true;
+          }
+        }
+      }
+      return false;
+    }
+
+    /** Composes a hashcode based on the field and type. */
+    @Override
+    public int hashCode() {
+      return field.hashCode() ^ (custom==null ? 0 : custom.hashCode());
+    }
+  }
+
+  private static class CacheEntryImpl extends FieldCache.CacheEntry {
+
+    private final IndexReader indexReader;
+    private final String fieldName;
+    private final Class<?> cacheType;
+    private final Object custom;
+    private final Object value;
+
+    CacheEntryImpl(IndexReader indexReader,
+                   String fieldName,
+                   Class<?> cacheType,
+                   Object custom,
+                   Object value) {
+        this.indexReader = indexReader;
+        this.fieldName = fieldName;
+        this.cacheType = cacheType;
+        this.custom = custom;
+        this.value = value;
+    }
+
+    public Object getReaderKey() {
+      return indexReader;
+    }
+
+    public String getFieldName() {
+      return fieldName;
+    }
+
+    public Class<?> getCacheType() {
+      return cacheType;
+    }
+
+    public Object getCustom() {
+      return custom;
+    }
+
+    public Object getValue() {
+      return value;
+    }
+  }
+
+  static final class ByteCache extends Cache {
+    ByteCache(AtomicFieldCache wrapper, IndexReader indexReader) {
+      super(wrapper, indexReader);
+    }
+    @Override
+    protected Object createValue(IndexReader reader, Entry entryKey)
+        throws IOException {
+      Entry entry = entryKey;
+      String field = entry.field;
+      FieldCache.ByteParser parser = (FieldCache.ByteParser) entry.custom;
+      if (parser == null) {
+        return wrapper.getBytes(field, FieldCache.DEFAULT_BYTE_PARSER);
+      }
+      final byte[] retArray = new byte[reader.maxDoc()];
+      TermDocs termDocs = reader.termDocs();
+      TermEnum termEnum = reader.terms (new Term (field));
+      try {
+        do {
+          Term term = termEnum.term();
+          if (term==null || term.field() != field) break;
+          byte termval = parser.parseByte(term.text());
+          termDocs.seek (termEnum);
+          while (termDocs.next()) {
+            retArray[termDocs.doc()] = termval;
+          }
+        } while (termEnum.next());
+      } catch (StopFillCacheException stop) {
+      } finally {
+        termDocs.close();
+        termEnum.close();
+      }
+      return retArray;
+    }
+  }
+
+  static final class ShortCache extends Cache {
+    ShortCache(AtomicFieldCache wrapper, IndexReader indexReader) {
+      super(wrapper, indexReader);
+    }
+
+    @Override
+    protected Object createValue(IndexReader reader, Entry entryKey)
+        throws IOException {
+      Entry entry =  entryKey;
+      String field = entry.field;
+      FieldCache.ShortParser parser = (FieldCache.ShortParser) entry.custom;
+      if (parser == null) {
+        return wrapper.getShorts(field, FieldCache.DEFAULT_SHORT_PARSER);
+      }
+      final short[] retArray = new short[reader.maxDoc()];
+      TermDocs termDocs = reader.termDocs();
+      TermEnum termEnum = reader.terms (new Term (field));
+      try {
+        do {
+          Term term = termEnum.term();
+          if (term==null || term.field() != field) break;
+          short termval = parser.parseShort(term.text());
+          termDocs.seek (termEnum);
+          while (termDocs.next()) {
+            retArray[termDocs.doc()] = termval;
+          }
+        } while (termEnum.next());
+      } catch (StopFillCacheException stop) {
+      } finally {
+        termDocs.close();
+        termEnum.close();
+      }
+      return retArray;
+    }
+  }
+
+  static final class IntCache extends Cache {
+
+    IntCache(AtomicFieldCache wrapper, IndexReader indexReader) {
+      super(wrapper, indexReader);
+    }
+
+    @Override
+    protected Object createValue(IndexReader reader, Entry entryKey)
+        throws IOException {
+      Entry entry = entryKey;
+      String field = entry.field;
+      FieldCache.IntParser parser = (FieldCache.IntParser) entry.custom;
+      if (parser == null) {
+        try {
+          return wrapper.getInts(field, FieldCache.DEFAULT_INT_PARSER);
+        } catch (NumberFormatException ne) {
+          return wrapper.getInts(field, FieldCache.NUMERIC_UTILS_INT_PARSER);
+        }
+      }
+      int[] retArray = null;
+      TermDocs termDocs = reader.termDocs();
+      TermEnum termEnum = reader.terms (new Term (field));
+      try {
+        do {
+          Term term = termEnum.term();
+          if (term==null || term.field() != field) break;
+          int termval = parser.parseInt(term.text());
+          if (retArray == null) // late init
+            retArray = new int[reader.maxDoc()];
+          termDocs.seek (termEnum);
+          while (termDocs.next()) {
+            retArray[termDocs.doc()] = termval;
+          }
+        } while (termEnum.next());
+      } catch (StopFillCacheException stop) {
+      } finally {
+        termDocs.close();
+        termEnum.close();
+      }
+      if (retArray == null) // no values
+        retArray = new int[reader.maxDoc()];
+      return retArray;
+    }
+  }
+
+  static final class FloatCache extends Cache {
+
+    FloatCache(AtomicFieldCache wrapper, IndexReader indexReader) {
+      super(wrapper, indexReader);
+    }
+
+    @Override
+    protected Object createValue(IndexReader reader, Entry entryKey)
+        throws IOException {
+      Entry entry = entryKey;
+      String field = entry.field;
+      FieldCache.FloatParser parser = (FieldCache.FloatParser) entry.custom;
+      if (parser == null) {
+        try {
+          return wrapper.getFloats(field, FieldCache.DEFAULT_FLOAT_PARSER);
+        } catch (NumberFormatException ne) {
+          return wrapper.getFloats(field, FieldCache.NUMERIC_UTILS_FLOAT_PARSER);
+        }
+    }
+      float[] retArray = null;
+      TermDocs termDocs = reader.termDocs();
+      TermEnum termEnum = reader.terms (new Term (field));
+      try {
+        do {
+          Term term = termEnum.term();
+          if (term==null || term.field() != field) break;
+          float termval = parser.parseFloat(term.text());
+          if (retArray == null) // late init
+            retArray = new float[reader.maxDoc()];
+          termDocs.seek (termEnum);
+          while (termDocs.next()) {
+            retArray[termDocs.doc()] = termval;
+          }
+        } while (termEnum.next());
+      } catch (StopFillCacheException stop) {
+      } finally {
+        termDocs.close();
+        termEnum.close();
+      }
+      if (retArray == null) // no values
+        retArray = new float[reader.maxDoc()];
+      return retArray;
+    }
+  }
+
+  static final class LongCache extends Cache {
+
+    LongCache(AtomicFieldCache wrapper, IndexReader indexReader) {
+      super(wrapper, indexReader);
+    }
+
+    @Override
+    protected Object createValue(IndexReader reader, Entry entry)
+        throws IOException {
+      String field = entry.field;
+      FieldCache.LongParser parser = (FieldCache.LongParser) entry.custom;
+      if (parser == null) {
+        try {
+          return wrapper.getLongs(field, FieldCache.DEFAULT_LONG_PARSER);
+        } catch (NumberFormatException ne) {
+          return wrapper.getLongs(field, FieldCache.NUMERIC_UTILS_LONG_PARSER);
+        }
+      }
+      long[] retArray = null;
+      TermDocs termDocs = reader.termDocs();
+      TermEnum termEnum = reader.terms (new Term(field));
+      try {
+        do {
+          Term term = termEnum.term();
+          if (term==null || term.field() != field) break;
+          long termval = parser.parseLong(term.text());
+          if (retArray == null) // late init
+            retArray = new long[reader.maxDoc()];
+          termDocs.seek (termEnum);
+          while (termDocs.next()) {
+            retArray[termDocs.doc()] = termval;
+          }
+        } while (termEnum.next());
+      } catch (StopFillCacheException stop) {
+      } finally {
+        termDocs.close();
+        termEnum.close();
+      }
+      if (retArray == null) // no values
+        retArray = new long[reader.maxDoc()];
+      return retArray;
+    }
+  }
+
+  static final class DoubleCache extends Cache {
+
+    DoubleCache(AtomicFieldCache wrapper, IndexReader indexReader) {
+      super(wrapper, indexReader);
+    }
+
+    @Override
+    protected Object createValue(IndexReader reader, Entry entryKey)
+        throws IOException {
+      Entry entry = entryKey;
+      String field = entry.field;
+      FieldCache.DoubleParser parser = (FieldCache.DoubleParser) entry.custom;
+      if (parser == null) {
+        try {
+          return wrapper.getDoubles(field, FieldCache.DEFAULT_DOUBLE_PARSER);
+        } catch (NumberFormatException ne) {
+          return wrapper.getDoubles(field, FieldCache.NUMERIC_UTILS_DOUBLE_PARSER);
+        }
+      }
+      double[] retArray = null;
+      TermDocs termDocs = reader.termDocs();
+      TermEnum termEnum = reader.terms (new Term (field));
+      try {
+        do {
+          Term term = termEnum.term();
+          if (term==null || term.field() != field) break;
+          double termval = parser.parseDouble(term.text());
+          if (retArray == null) // late init
+            retArray = new double[reader.maxDoc()];
+          termDocs.seek (termEnum);
+          while (termDocs.next()) {
+            retArray[termDocs.doc()] = termval;
+          }
+        } while (termEnum.next());
+      } catch (StopFillCacheException stop) {
+      } finally {
+        termDocs.close();
+        termEnum.close();
+      }
+      if (retArray == null) // no values
+        retArray = new double[reader.maxDoc()];
+      return retArray;
+    }
+  }
+
+  static final class DocsWithFieldCache extends Cache {
+    DocsWithFieldCache(AtomicFieldCache wrapper, IndexReader indexReader) {
+      super(wrapper, indexReader);
+    }
+
+    @Override
+    protected Object createValue(IndexReader reader, Entry entryKey)
+    throws IOException {
+      final Entry entry = entryKey;
+      final String field = entry.field;
+      FixedBitSet res = null;
+      final TermDocs termDocs = reader.termDocs();
+      final TermEnum termEnum = reader.terms(new Term(field));
+      try {
+        do {
+          final Term term = termEnum.term();
+          if (term == null || term.field() != field) break;
+          if (res == null) // late init
+            res = new FixedBitSet(reader.maxDoc());
+          termDocs.seek(termEnum);
+          while (termDocs.next()) {
+            res.set(termDocs.doc());
+          }
+        } while (termEnum.next());
+      } finally {
+        termDocs.close();
+        termEnum.close();
+      }
+      if (res == null)
+        return new Bits.MatchNoBits(reader.maxDoc());
+      final int numSet = res.cardinality();
+      if (numSet >= reader.numDocs()) {
+        // The cardinality of the BitSet is numDocs if all documents have a value.
+        // As deleted docs are not in TermDocs, this is always true
+        assert numSet == reader.numDocs();
+        return new Bits.MatchAllBits(reader.maxDoc());
+      }
+      return res;
+    }
+  }
+
+  static final class StringCache extends Cache {
+
+    StringCache(AtomicFieldCache wrapper, IndexReader indexReader) {
+      super(wrapper, indexReader);
+    }
+
+    @Override
+    protected Object createValue(IndexReader reader, Entry entryKey)
+        throws IOException {
+      String field = StringHelper.intern(entryKey.field);
+      final String[] retArray = new String[reader.maxDoc()];
+      TermDocs termDocs = reader.termDocs();
+      TermEnum termEnum = reader.terms (new Term (field));
+      final int termCountHardLimit = reader.maxDoc();
+      int termCount = 0;
+      try {
+        do {
+          if (termCount++ == termCountHardLimit) {
+            // app is misusing the API (there is more than
+            // one term per doc); in this case we make best
+            // effort to load what we can (see LUCENE-2142)
+            break;
+          }
+
+          Term term = termEnum.term();
+          if (term==null || term.field() != field) break;
+          String termval = term.text();
+          termDocs.seek (termEnum);
+          while (termDocs.next()) {
+            retArray[termDocs.doc()] = termval;
+          }
+        } while (termEnum.next());
+      } finally {
+        termDocs.close();
+        termEnum.close();
+      }
+      return retArray;
+    }
+  }
+
+  static final class StringIndexCache extends Cache {
+    StringIndexCache(AtomicFieldCache wrapper, IndexReader indexReader) {
+      super(wrapper, indexReader);
+    }
+
+    @Override
+    protected Object createValue(IndexReader reader, Entry entryKey)
+        throws IOException {
+      String field = StringHelper.intern(entryKey.field);
+      final int[] retArray = new int[reader.maxDoc()];
+      String[] mterms = new String[reader.maxDoc()+1];
+      TermDocs termDocs = reader.termDocs();
+      TermEnum termEnum = reader.terms (new Term (field));
+      int t = 0;  // current term number
+
+      // an entry for documents that have no terms in this field
+      // should a document with no terms be at top or bottom?
+      // this puts them at the top - if it is changed, FieldDocSortedHitQueue
+      // needs to change as well.
+      mterms[t++] = null;
+
+      try {
+        do {
+          Term term = termEnum.term();
+          if (term==null || term.field() != field || t >= mterms.length) break;
+
+          // store term text
+          mterms[t] = term.text();
+
+          termDocs.seek (termEnum);
+          while (termDocs.next()) {
+            retArray[termDocs.doc()] = t;
+          }
+
+          t++;
+        } while (termEnum.next());
+      } finally {
+        termDocs.close();
+        termEnum.close();
+      }
+
+      if (t == 0) {
+        // if there are no terms, make the term array
+        // have a single null entry
+        mterms = new String[1];
+      } else if (t < mterms.length) {
+        // if there are less terms than documents,
+        // trim off the dead array space
+        String[] terms = new String[t];
+        System.arraycopy (mterms, 0, terms, 0, t);
+        mterms = terms;
+      }
+
+      FieldCache.StringIndex value = new FieldCache.StringIndex(retArray, mterms);
+      return value;
+    }
+  }
+
+}
Index: lucene/src/java/org/apache/lucene/search/function/ByteFieldSource.java
===================================================================
--- lucene/src/java/org/apache/lucene/search/function/ByteFieldSource.java	(revision 1177602)
+++ lucene/src/java/org/apache/lucene/search/function/ByteFieldSource.java	(revision )
@@ -19,7 +19,7 @@
 
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.search.FieldCache;
-import org.apache.lucene.search.function.DocValues;
+import org.apache.lucene.search.cache.AtomicFieldCache;
 
 import java.io.IOException;
 
@@ -67,8 +67,8 @@
 
   /*(non-Javadoc) @see org.apache.lucene.search.function.FieldCacheSource#getCachedValues(org.apache.lucene.search.FieldCache, java.lang.String, org.apache.lucene.index.IndexReader) */
   @Override
-  public DocValues getCachedFieldValues (FieldCache cache, String field, IndexReader reader) throws IOException {
-    final byte[] arr = cache.getBytes(reader, field, parser);
+  public DocValues getCachedFieldValues (AtomicFieldCache cache, String field, IndexReader reader) throws IOException {
+    final byte[] arr = cache.getBytes(field, parser);
     return new DocValues() {
       /*(non-Javadoc) @see org.apache.lucene.search.function.DocValues#floatVal(int) */
       @Override
