diff --git a/lucene/core/src/java/org/apache/lucene/codecs/lucene70/IndexedDISI.java b/lucene/core/src/java/org/apache/lucene/codecs/lucene70/IndexedDISI.java
index 24eaf7a..97a258c 100644
--- a/lucene/core/src/java/org/apache/lucene/codecs/lucene70/IndexedDISI.java
+++ b/lucene/core/src/java/org/apache/lucene/codecs/lucene70/IndexedDISI.java
@@ -50,6 +50,12 @@ import org.apache.lucene.util.RoaringDocIdSet;
 final class IndexedDISI extends DocIdSetIterator {
 
   static final int MAX_ARRAY_LENGTH = (1 << 12) - 1;
+  static final String NO_NAME = "n/a";
+  public static boolean CACHING_ENABLED = true; // TODO (Toke): Primarily a default for Proof Of Concept
+
+  // If true, IndexedDISI from Lucene70NormsProducer are also cached. If CACHING_ENABLED is false, this has no effect
+  public static boolean ALSO_CACHE_NORMS = true; // TODO (Toke): Primarily a default for Proof Of Concept
+  public final String name;
 
   private static void flush(int block, FixedBitSet buffer, int cardinality, IndexOutput out) throws IOException {
     assert block >= 0 && block < 65536;
@@ -98,13 +104,69 @@ final class IndexedDISI extends DocIdSetIterator {
   /** The slice that stores the {@link DocIdSetIterator}. */
   private final IndexInput slice;
   private final long cost;
+  private final IndexedDISICache cache;
 
   IndexedDISI(IndexInput in, long offset, long length, long cost) throws IOException {
-    this.slice = in.slice("docs", offset, length);
+    this(in, offset, length, cost, NO_NAME);
+  }
+
+  IndexedDISI(IndexInput in, long offset, long length, long cost, String name) throws IOException {
+    this(in, offset, length, cost, CACHING_ENABLED, name);
+  }
+
+  IndexedDISI(IndexInput in, long offset, long length, long cost, boolean useCaching) throws IOException {
+    this(in, offset, length, cost, useCaching, NO_NAME);
+  }
+
+  IndexedDISI(IndexInput in, long offset, long length, long cost, boolean useCaching, String name) throws IOException {
+    this(in, offset, length, cost, useCaching ? IndexedDISICacheFactory.getCache(in, offset, length, cost, name) : null, name);
+  }
+
+  IndexedDISI(IndexInput in, long offset, long length, long cost, IndexedDISICache cache) throws IOException {
+    this(in, offset, length, cost, cache, NO_NAME);
+  }
+
+  IndexedDISI(IndexInput in, long offset, long length, long cost, IndexedDISICache cache, String name) throws IOException {
+    this(in.slice("docs", offset, length), cost, cache, name);
+  }
+
+  IndexedDISI(IndexInput slice, long cost) throws IOException {
+    this(slice, cost, NO_NAME);
+  }
+  // This constructor allows to pass the slice directly in case it helps reuse
+  // see eg. Lucene70 norms producer's merge instance
+  IndexedDISI(IndexInput slice, long cost, String name) throws IOException {
+    this(slice, cost, null, name);
+    IndexedDISICacheFactory.debug(
+        "Non-cached direct slice IndexedDISI with length " + slice.length() + ": " + slice.toString());
+  }
+
+  IndexedDISI(int hash, IndexInput slice, long cost) throws IOException {
+    this(hash, slice, cost, NO_NAME);
+  }
+  IndexedDISI(int hash, IndexInput slice, long cost, String name) throws IOException {
+    this(hash, slice, cost, CACHING_ENABLED && ALSO_CACHE_NORMS, name);
+  }
+  IndexedDISI(int hash, IndexInput slice, long cost, boolean useCaching) throws IOException {
+    this(hash, slice, cost, useCaching, NO_NAME);
+  }
+  IndexedDISI(int hash, IndexInput slice, long cost, boolean useCaching, String name) throws IOException {
+    this(slice, cost, useCaching ? IndexedDISICacheFactory.getCache(hash, slice, cost, name) : null, name);
+  }
+  IndexedDISI(IndexInput slice, long cost, IndexedDISICache cache) throws IOException {
+    this(slice, cost, cache, NO_NAME);
+  }
+  // This constructor allows to pass the slice directly in case it helps reuse
+  // see eg. Lucene70 norms producer's merge instance
+  IndexedDISI(IndexInput slice, long cost, IndexedDISICache cache, String name) {
+    this.name = name;
+    this.slice = slice;
     this.cost = cost;
+    this.cache = cache == null ? IndexedDISICache.EMPTY : cache;
   }
 
   private int block = -1;
+  private long blockStart; // Used with the DENSE cache
   private long blockEnd;
   private int nextBlockIndex = -1;
   Method method;
@@ -120,6 +182,8 @@ final class IndexedDISI extends DocIdSetIterator {
   private int wordIndex = -1;
   // number of one bits encountered so far, including those of `word`
   private int numberOfOnes;
+  // Used with rank for jumps inside of DENSE
+  private int denseOrigoIndex;
 
   // ALL variables
   private int gap;
@@ -157,6 +221,17 @@ final class IndexedDISI extends DocIdSetIterator {
   }
 
   private void advanceBlock(int targetBlock) throws IOException {
+    long offset = cache.getFilePointerForBlock(targetBlock>>IndexedDISICache.BLOCK_BITS);
+    int origo = cache.getIndexForBlock(targetBlock>>IndexedDISICache.BLOCK_BITS);
+    if (origo != -1 && offset != -1 && offset > slice.getFilePointer()) {
+   //   System.out.println("Seeking to " + offset + " for targetBlock " + (targetBlock>>IndexedDISICache.BLOCK_BITS) + " with origo " + origo);
+      this.nextBlockIndex = origo-1; // -1 to compensate for the always-added 1 in readBlockHeader
+      slice.seek(offset);
+      readBlockHeader();
+      return;
+    }
+
+    // Fallback to non-cached
     do {
       slice.seek(blockEnd);
       readBlockHeader();
@@ -164,6 +239,7 @@ final class IndexedDISI extends DocIdSetIterator {
   }
 
   private void readBlockHeader() throws IOException {
+    blockStart = slice.getFilePointer();
     block = Short.toUnsignedInt(slice.readShort()) << 16;
     assert block >= 0;
     final int numValues = 1 + Short.toUnsignedInt(slice.readShort());
@@ -181,6 +257,7 @@ final class IndexedDISI extends DocIdSetIterator {
       blockEnd = slice.getFilePointer() + (1 << 13);
       wordIndex = -1;
       numberOfOnes = index + 1;
+      denseOrigoIndex = numberOfOnes;
     }
   }
 
@@ -244,6 +321,10 @@ final class IndexedDISI extends DocIdSetIterator {
       boolean advanceWithinBlock(IndexedDISI disi, int target) throws IOException {
         final int targetInBlock = target & 0xFFFF;
         final int targetWordIndex = targetInBlock >>> 6;
+
+        // If possible, skip ahead using the rank cache
+        disi.rankSkip(disi, target);
+
         for (int i = disi.wordIndex + 1; i <= targetWordIndex; ++i) {
           disi.word = disi.slice.readLong();
           disi.numberOfOnes += Long.bitCount(disi.word);
@@ -257,7 +338,10 @@ final class IndexedDISI extends DocIdSetIterator {
           return true;
         }
 
+        // There were no set bits at the wanted position. Move forward until one is reached
         while (++disi.wordIndex < 1024) {
+          // This could use the rank cache to skip empty spaces >= 512 bits, but it seems unrealistic
+          // that such blocks would be DENSE
           disi.word = disi.slice.readLong();
           if (disi.word != 0) {
             disi.index = disi.numberOfOnes;
@@ -266,12 +350,18 @@ final class IndexedDISI extends DocIdSetIterator {
             return true;
           }
         }
+        // No set bits in the block at or after the wanted position.
         return false;
       }
+
       @Override
       boolean advanceExactWithinBlock(IndexedDISI disi, int target) throws IOException {
         final int targetInBlock = target & 0xFFFF;
         final int targetWordIndex = targetInBlock >>> 6;
+
+        // If possible, skip ahead using the rank cache
+        disi.rankSkip(disi, target);
+
         for (int i = disi.wordIndex + 1; i <= targetWordIndex; ++i) {
           disi.word = disi.slice.readLong();
           disi.numberOfOnes += Long.bitCount(disi.word);
@@ -282,6 +372,8 @@ final class IndexedDISI extends DocIdSetIterator {
         disi.index = disi.numberOfOnes - Long.bitCount(leftBits);
         return (leftBits & 1L) != 0;
       }
+
+
     },
     ALL {
       @Override
@@ -306,4 +398,53 @@ final class IndexedDISI extends DocIdSetIterator {
     abstract boolean advanceExactWithinBlock(IndexedDISI disi, int target) throws IOException;
   }
 
+  /**
+   * If the distance between the current position and the target is > 8 words, the rank cache will
+   * be used to guarantee a worst-case of 1 rank-lookup and 7 word-read-and-count-bits operations.
+   * Note: This does not guarantee a skip up to target, only up to nearest rank boundary. It is the
+   * responsibility of the caller to iterate further to reach target.
+   * @param disi standard DISI.
+   * @param target the wanted docID for which to calculate set-flag and index.
+   * @throws IOException if a disi seek failed.
+   */
+  private void rankSkip(IndexedDISI disi, int target) throws IOException {
+    final int targetInBlock = target & 0xFFFF;
+    final int targetWordIndex = targetInBlock >>> 6;
+
+    // If the distance between the current position and the target is >= 8
+    // then it pays to use the rank to jump
+    if (!(disi.cache.hasRank() && targetWordIndex - disi.wordIndex >= IndexedDISICache.RANK_BLOCK_LONGS)) {
+      return;
+    }
+
+    int rankPos = disi.cache.denseRankPosition(target);
+    if (rankPos == -1) {
+      return;
+    }
+    int rank = disi.cache.getRankInBlock(rankPos);
+    if (rank == -1) {
+      System.out.println("Rank -1 for target=" + target);
+      return;
+    }
+    int rankIndex = disi.denseOrigoIndex + rank;
+    int rankWordIndex = (rankPos & 0xFFFF) >> 6;
+    long rankOffset = disi.blockStart + 4 + (rankWordIndex * 8);
+
+    long mark = disi.slice.getFilePointer();
+    disi.slice.seek(rankOffset);
+    long rankWord = disi.slice.readLong();
+    int rankNOO = rankIndex + Long.bitCount(rankWord);
+    rankOffset += Long.BYTES;
+
+
+    //disi.slice.seek(mark);
+    disi.wordIndex = rankWordIndex;
+    disi.word = rankWord;
+    disi.numberOfOnes = rankNOO;
+
+//            System.out.println("> rank denseOrigoIndex=" + disi.denseOrigoIndex +
+//                ", rank[" + (target >> IndexedDISICache.RANK_BLOCK_BITS) + "]=" + disi.cache.getRankInBlock(rankPos) +
+//                ", rankWordIndex=" + rankWordIndex + ", twi=" + targetWordIndex +
+//               ", offset=" + rankOffset + ", rankOnes=" + rankNOO + ", endIndex=" + rankIndex);
+  }
 }
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/lucene70/IndexedDISICache.java b/lucene/core/src/java/org/apache/lucene/codecs/lucene70/IndexedDISICache.java
new file mode 100644
index 0000000..48b41be
--- /dev/null
+++ b/lucene/core/src/java/org/apache/lucene/codecs/lucene70/IndexedDISICache.java
@@ -0,0 +1,331 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.lucene.codecs.lucene70;
+
+import java.io.IOException;
+import java.util.Arrays;
+import java.util.concurrent.atomic.AtomicInteger;
+
+import org.apache.lucene.search.DocIdSetIterator;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.util.Accountable;
+import org.apache.lucene.util.ArrayUtil;
+import org.apache.lucene.util.RamUsageEstimator;
+
+import static org.apache.lucene.codecs.lucene70.IndexedDISI.MAX_ARRAY_LENGTH;
+
+/**
+ * Caching of IndexedDISI with two strategies:
+ *
+ * A lookup table for block blockCache and index and a rank structure for DENSE block lookups.
+ *
+ * The lookup table is an array of {@code long}s with an entry for each block (65536 bits).
+ * Each long entry consists of 2 logical parts:
+ * The first 31 bits holds the index up to just before the wanted block.
+ * The next 33 bits holds the offset into the underlying slice.
+ * As there is a maximum of 2^16 blocks, it follows that the maximum size of any block must
+ * not exceed 2^17 bits to avoid  overflow. This is currently the case, with the largest
+ * block being DENSE and using 2^16 + 32 bits, and is likely to continue to hold as using
+ * more than double the amount of bits is unlikely to be an efficient representation.
+ * The alternative to using the lookup table is iteration of all blocks up to the wanted one.
+ * The cache overhead is numDocs/1024 bytes.
+ *
+ * Note: There are 4 types of blocks: ALL, DENSE, SPARSE and non-existing (0 set bits).
+ * In the case of non-existing blocks, the entry in the lookup table has index equal to the
+ * previous entry and offset equal to the next non-empty block.
+ *
+ * The rank structure for DENSE blocks is an array of unsigned {@code short}s with an entry
+ * or each sub-block of 512 bits out of the 65536 bits in the outer block.
+ * Each rank-entry states the number of set bits within the block up to the bit before the
+ * bit positioned at the start of the sub-block.
+ * Note that that the rank entry of the first sub-block is always 0 and that the last entry can
+ * at most be 65536-512 = 65024 and thus will always fit into an unsigned short.
+ * See https://en.wikipedia.org/wiki/Succinct_data_structure for details on rank structures.
+ * The alternative to using the rank structure is iteration and summing of set bits for all
+ * entries in the DENSE sub-block up until the wanted bit, with a worst-case of 1024 entries.
+ * The rank cache overhead for a single DENSE block is 128 shorts (128*16 = 2048 bits) or
+ * 1/32th.
+ *
+ * The total overhead for the rank cache is currently also numDocs/32 bits or numDocs/8 bytes
+ * as the rank-representation is not sparse itself, using empty entries for sub-blocks of type
+ * ALL or SPARSE. // TODO: Support sparse rank structures to avoid overhead for non-DENSE blocks
+ *
+ * See https://issues.apache.org/jira/browse/LUCENE-8374 for details
+ */
+public class IndexedDISICache implements Accountable {
+
+  public static final int BLOCK = 65536;
+  public static final int BLOCK_BITS = 16;
+  public static final long BLOCK_INDEX_SHIFT = 33;
+  public static final long BLOCK_INDEX_MASK = ~0L << BLOCK_INDEX_SHIFT;
+  public static final long BLOCK_LOOKUP_MASK = ~BLOCK_INDEX_MASK;
+
+  public static final int RANK_BLOCK = 512;
+  public static final int RANK_BLOCK_LONGS = 512/Long.SIZE;
+  public static final int RANK_BLOCK_BITS = 9;
+  public static final int RANKS_PER_BLOCK = BLOCK/RANK_BLOCK;
+
+  private long[] blockCache = null; // One every 65536 docs, contains index & slice position
+  private char[] rank;              // One every 512 docs
+  public String creationStats = ""; // TODO: Definitely not the way to keep the stats, but where to send them?
+  public String name; // Identifier for debug, log & inspection
+
+  // Flags for not-yet-defined-values used during building
+  private static final long BLOCK_EMPTY_INDEX = ~0L << BLOCK_INDEX_SHIFT;
+  private static final long BLOCK_EMPTY_LOOKUP = BLOCK_LOOKUP_MASK;
+  private static final long BLOCK_EMPTY = BLOCK_EMPTY_INDEX | BLOCK_EMPTY_LOOKUP;
+
+  /**
+   * Builds the stated caches for the given Indexed
+   *
+   * @param in positioned at the start of the logical underlying bitmap.
+   */
+  IndexedDISICache(IndexInput in, boolean createBlockCache, boolean createRankCache, String name) throws IOException {
+    if (createBlockCache) {
+      blockCache = new long[16];    // Will be extended when needed
+      Arrays.fill(blockCache, BLOCK_EMPTY);
+    }
+    rank = createRankCache ? new char[256] : null; // Will be extended when needed
+    if (!createBlockCache && !createRankCache) {
+      return; // Nothing to do
+    }
+    this.name = name;
+    updateCaches(in, createBlockCache, createRankCache);
+  }
+
+  private IndexedDISICache() {
+    this.blockCache = null;
+    this.rank = null;
+    this.name = "";
+  }
+  // TODO: EMPTY works poorly with name, but creating multiple empty's with different names seems wasteful
+  public static final IndexedDISICache EMPTY = new IndexedDISICache();
+
+  /**
+   * If available, returns a position within the underlying {@link IndexInput} for the start of the block
+   * containing the wanted bit (the target) or the next non-EMPTY block, if the block representing the bit is empty.
+   * @param targetBlock the index for the block to resolve (docID / 65536).
+   * @return the offset for the block for target or -1 if it cannot be resolved.
+   */
+  public long getFilePointerForBlock(int targetBlock) {
+    long offset = blockCache == null || blockCache.length <= targetBlock ? -1 : blockCache[targetBlock] & BLOCK_LOOKUP_MASK;
+    return offset == BLOCK_EMPTY_LOOKUP ? -1 : offset;
+  }
+
+  /**
+   * If available, returns the index; number of set bits before the wanted block.
+   * @param targetBlock the block to resolve (docID / 65536).
+   * @return the index for the block or -1 if it cannot be resolved.
+   */
+  public int getIndexForBlock(int targetBlock) {
+    if (blockCache == null || blockCache.length <= targetBlock) {
+      return -1;
+    }
+    return (blockCache[targetBlock] & BLOCK_INDEX_MASK) == BLOCK_EMPTY_INDEX ? -1 : (int)(blockCache[targetBlock] >>> BLOCK_INDEX_SHIFT);
+  }
+
+  /**
+   * Given a target (docID), this method returns the docID
+   * @param target the docID for which an index is wanted.
+   * @return the docID where the rank is known. This will be lte target.
+   */
+  // TODO: This method requires way too much knowledge of the intrinsics of the cache. Usage should be simplified
+  public int denseRankPosition(int target) {
+       return target >> RANK_BLOCK_BITS << RANK_BLOCK_BITS;
+  }
+
+  public boolean hasOffsets() {
+    return blockCache != null;
+  }
+
+  public boolean hasRank() {
+    return rank != null;
+  }
+  
+  /**
+   * Get the rank (index) for all set bits up to just before the given rankPosition in the block.
+   * The caller is responsible for deriving the count of bits up to the docID target from the rankPosition.
+   * The caller is also responsible for keeping track of set bits up to the current block.
+   * Important: This only accepts rankPositions that aligns to {@link #RANK_BLOCK} boundaries.
+   * Note 1: Use {@link #denseRankPosition(int)} to obtain a calid rankPosition for a wanted docID.
+   * Note 2: The caller should seek to the rankPosition in the underlying slice to keep everything in sync.
+   * @param rankPosition a docID target that aligns to {@link #RANK_BLOCK}.
+   * @return the rank (index / set bits count) up to just before the given rankPosition.
+   *         If rank is disabled, -1 is returned.
+   */
+  // TODO: This method requires way too much knowledge of the intrinsics of the cache. Usage should be simplified
+  public int getRankInBlock(int rankPosition) {
+    assert rankPosition == denseRankPosition(rankPosition);
+    int rankIndex = rankPosition >> RANK_BLOCK_BITS;
+    return rank == null || rankIndex >= rank.length ? -1 : rank[rankIndex];
+  }
+
+  private void updateCaches(IndexInput slice, boolean fillBlockCache, boolean fillRankCache)
+      throws IOException {
+    final long startOffset = slice.getFilePointer();
+
+    final long startTime = System.nanoTime();
+    AtomicInteger statBlockALL = new AtomicInteger(0);
+    AtomicInteger statBlockDENSE = new AtomicInteger(0);
+    AtomicInteger statBlockSPARSE = new AtomicInteger(0);
+
+    // Fill phase
+    int largestBlock;
+    try {
+      largestBlock = fillCache(slice, fillBlockCache, fillRankCache, statBlockALL, statBlockDENSE, statBlockSPARSE);
+    } catch (Exception e) { // TODO (Toke): Development debug only. Remove when stable
+      creationStats = "Exception filling cache with slice of length " + slice.getFilePointer();
+      System.err.println(creationStats);
+      e.printStackTrace();
+      blockCache = null;
+      rank = null;
+      slice.seek(startOffset); // Leave it as we found it
+      return;
+    }
+
+    freezeCaches(fillBlockCache, fillRankCache, largestBlock);
+
+    slice.seek(startOffset); // Leave it as we found it
+    creationStats = String.format(
+        "name=%s, blocks=%d (ALL=%d, DENSE=%d, SPARSE=%d, EMPTY=%d), time=%dms, block=%b, rank=%b",
+        name,
+        largestBlock+1, statBlockALL.get(), statBlockDENSE.get(), statBlockSPARSE.get(),
+        (largestBlock+1-statBlockALL.get()-statBlockDENSE.get()-statBlockSPARSE.get()),
+        (System.nanoTime()-startTime)/1000000, fillBlockCache, fillRankCache);
+  }
+
+  private int fillCache(IndexInput slice, boolean fillBlockCache, boolean fillRankCache, AtomicInteger statBlockALL, AtomicInteger statBlockDENSE, AtomicInteger statBlockSPARSE) throws IOException {
+    int largestBlock = -1;
+    long index = 0;
+    while (slice.getFilePointer() < slice.length()) {
+      final long startFilePointer = slice.getFilePointer();
+
+      final int blockIndex = Short.toUnsignedInt(slice.readShort());
+      final int numValues = 1 + Short.toUnsignedInt(slice.readShort());
+
+      assert blockIndex > largestBlock;
+      if (blockIndex == DocIdSetIterator.NO_MORE_DOCS >>> 16) { // End reached
+        assert Short.toUnsignedInt(slice.readShort()) == (DocIdSetIterator.NO_MORE_DOCS & 0xFFFF);
+        break;
+      }
+      largestBlock = blockIndex;
+
+      if (fillBlockCache) {
+        blockCache = ArrayUtil.grow(blockCache, blockIndex+1); // No-op if large enough
+        blockCache[blockIndex] = (index << BLOCK_INDEX_SHIFT) | startFilePointer;
+      }
+      index += numValues;
+
+      if (numValues <= MAX_ARRAY_LENGTH) { // SPARSE
+        statBlockSPARSE.incrementAndGet();
+        slice.seek(slice.getFilePointer() + (numValues << 1));
+        continue;
+      }
+      if (numValues == 65536) { // ALL
+        statBlockALL.incrementAndGet();
+        // Already at next block offset
+        continue;
+      }
+
+      // The block is DENSE
+      statBlockDENSE.incrementAndGet();
+      long nextBlockOffset = slice.getFilePointer() + (1 << 13);
+      if (fillRankCache) {
+        int setBits = 0;
+        int rankOrigo = blockIndex << 16 >> 9; // Double shift for clarity: The compiler will simplify it
+        for (int rankDelta = 0 ; rankDelta < RANKS_PER_BLOCK ; rankDelta++) { // 128 rank-entries in a block
+          final int rankIndex = rankOrigo + rankDelta;
+          rank = ArrayUtil.grow(rank, rankIndex+1);
+          rank[rankIndex] = (char)setBits;
+          for (int i = 0 ; i < 512/64 ; i++) { // 8 longs for each rank-entry
+            setBits += Long.bitCount(slice.readLong());
+          }
+        }
+        assert slice.getFilePointer() == nextBlockOffset;
+      } else {
+        slice.seek(nextBlockOffset);
+      }
+    }
+    //maxDocID = ((largestBlock+1) << BLOCK_BITS)-1;
+    return largestBlock;
+  }
+
+  private void freezeCaches(boolean fillBlockCache, boolean fillRankCache, int largestBlock) {
+    if (largestBlock == -1) { // No set bit: Disable the caches
+      blockCache = null;
+      rank = null;
+      return;
+    }
+
+    // Reduce size to minimum
+    if (fillBlockCache && blockCache.length-1 > largestBlock) {
+      long[] newBC = new long[Math.max(largestBlock - 1, 1)];
+      System.arraycopy(blockCache, 0, newBC, 0, newBC.length);
+      blockCache = newBC;
+    }
+    if (fillRankCache && rank.length > (largestBlock+1)*RANKS_PER_BLOCK) {
+      char[] newRank = new char[(largestBlock+1)*RANKS_PER_BLOCK];
+      System.arraycopy(rank, 0, newRank, 0, newRank.length);
+      rank = newRank;
+    }
+
+    // Replace non-defined values with usable ones
+    if (fillBlockCache) {
+
+      // Set non-defined blockCache entries (caused by blocks with 0 set bits) to the subsequently defined one
+      long latest = BLOCK_EMPTY;
+      for (int i = blockCache.length-1; i >= 0 ; i--) {
+        long current = blockCache[i];
+        if (current == BLOCK_EMPTY) {
+          blockCache[i] = latest;
+        } else {
+          latest = current;
+        }
+      }
+
+/*      // Set non-defined blockCache entries (caused by blocks with 0 set bits) to the next defined offset
+      long latestLookup = BLOCK_EMPTY_LOOKUP;
+      for (int i = blockCache.length-1; i >= 0 ; i--) {
+        long currentLookup = blockCache[i] & BLOCK_LOOKUP_MASK;
+        if (currentLookup == BLOCK_EMPTY_LOOKUP) { // If empty, set the pointer to the sub-sequent defined one
+          blockCache[i] = (blockCache[i] & BLOCK_INDEX_MASK) | (latestLookup & BLOCK_LOOKUP_MASK);
+        } else {
+          latestLookup = currentLookup;
+        }
+      }
+  */
+/*      // Set non-defined index (caused by blocks with 0 set bits) to the previous origo
+      long lastIndex = 0L;
+      for (int i = 0 ; i < blockCache.length ; i++) {
+        long currentIndex = blockCache[i] & BLOCK_INDEX_MASK;
+        if (currentIndex == BLOCK_EMPTY_INDEX) {
+          blockCache[i] = lastIndex | (blockCache[i] & BLOCK_LOOKUP_MASK);
+        } else {
+          lastIndex = currentIndex;
+        }
+      }*/
+    }
+  }
+
+  @Override
+  public long ramBytesUsed() {
+    return (blockCache == null ? 0 : RamUsageEstimator.sizeOf(blockCache)) +
+        (rank == null ? 0 : RamUsageEstimator.sizeOf(rank)) +
+        RamUsageEstimator.NUM_BYTES_OBJECT_REF*3 +
+        RamUsageEstimator.NUM_BYTES_OBJECT_HEADER + creationStats.length()*2;
+  }
+}
\ No newline at end of file
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/lucene70/IndexedDISICacheFactory.java b/lucene/core/src/java/org/apache/lucene/codecs/lucene70/IndexedDISICacheFactory.java
new file mode 100644
index 0000000..7c8e78e
--- /dev/null
+++ b/lucene/core/src/java/org/apache/lucene/codecs/lucene70/IndexedDISICacheFactory.java
@@ -0,0 +1,243 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.lucene.codecs.lucene70;
+
+
+import java.io.IOException;
+import java.util.Collection;
+import java.util.HashMap;
+import java.util.Map;
+
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.store.RandomAccessInput;
+import org.apache.lucene.util.Accountable;
+import org.apache.lucene.util.ArrayUtil;
+import org.apache.lucene.util.RamUsageEstimator;
+
+/**
+ * Creates and stores caches for {@link IndexedDISI}.
+ *
+ * See https://issues.apache.org/jira/browse/LUCENE-8374 for details
+ */
+// Note: This was hacked together with little understanding of overall caching principles for Lucene.
+// It probably belongs somewhere else and hopefully someone with a better understanding will refactor the code.
+public class IndexedDISICacheFactory implements Accountable {
+  public static int MIN_LENGTH_FOR_CACHING = 50; // Set this very low: Could be 9 EMPTY followed by a SPARSE
+  public static boolean BLOCK_CACHING_ENABLED = true;
+  public static boolean DENSE_CACHING_ENABLED = true;
+  public static boolean VARYINGBPV_CACHING_ENABLED = true;
+
+  public static boolean DEBUG = true; // TODO (Toke): Remove this when code has stabilized
+
+  // Map<IndexInput.hashCode, Map<key, cache>>
+  private static final Map<Integer, Map<Long, IndexedDISICache>> disiPool = new HashMap<>();
+  private static final Map<Integer, Map<String, VaryingBPVJumpTable>> vBPVPool = new HashMap<>();
+
+  static {
+    if (DEBUG) {
+      System.out.println(IndexedDISICacheFactory.class.getSimpleName() +
+          ": LUCENE-8374 beta patch enabled with block_caching=" + BLOCK_CACHING_ENABLED +
+          ", dense_caching=" + DENSE_CACHING_ENABLED + ", cBPV_caching=" + VARYINGBPV_CACHING_ENABLED);
+    }
+  }
+
+  /**
+   * Releases all caches associated with the given data.
+   * @param data with {@link IndexedDISICache}s.
+   */
+  public static void release(IndexInput data) {
+    if (disiPool.remove(data.hashCode()) != null) {
+      //debug("Release cache called for disiPool data " + data.hashCode() + " with existing cache");
+    }
+    if (vBPVPool.remove(data.hashCode()) != null) {
+      //debug("Release cache called for vBPVPool data " + data.hashCode() + " with existing cache");
+    }
+  }
+
+  /**
+   * Creates a cache (jump table) if not already present and returns it.
+   * @param indexInputHash hash for the outer IndexInput. Used for cache invalidation.
+   * @param name the name for the cache, typically the field name. Used as key for later retrieval.
+   * @param slice the long values with varying bits per value.
+   * @param valuesLength the length in bytes of the slice.
+   * @return a jump table for the longs in the given slice or null if the structure is not suitable for caching.
+   */
+  public static VaryingBPVJumpTable getVBPVJumpTable(
+      int indexInputHash, String name, RandomAccessInput slice, long valuesLength) throws IOException {
+    Map<String, VaryingBPVJumpTable> jumpTables = vBPVPool.computeIfAbsent(indexInputHash, poolHash -> new HashMap<>());
+    VaryingBPVJumpTable jumpTable = jumpTables.get(name);
+    if (jumpTable == null) {
+      // TODO: Avoid overlapping builds of the same jump table
+      jumpTable = new VaryingBPVJumpTable(slice, name, valuesLength);
+      jumpTables.put(name, jumpTable);
+      debug("Created packed numeric jump table for " + name + ": " + jumpTable.creationStats + " (" + jumpTable.ramBytesUsed() + " bytes)");
+    }
+    return jumpTable;
+  }
+
+  public static long getDISIBlocksWithOffsetsCount() {
+    return disiPool.values().stream().map(Map::values).flatMap(Collection::stream).
+        filter(IndexedDISICache::hasOffsets).count();
+  }
+
+  public static long getDISIBlocksWithRankCount() {
+    return disiPool.values().stream().map(Map::values).flatMap(Collection::stream).
+        filter(IndexedDISICache::hasRank).count();
+  }
+
+  public static long getVaryingBPVCount() {
+    return vBPVPool.values().stream().map(Map::values).count();
+  }
+
+  /**
+   * Creates a cache if not already present and returns it.
+   * @param data   the slice to create a cache for.
+   * @param offset same as the offset that will also be used for creating an {@link IndexedDISI}.
+   * @param length same af the length that will also be used for creating an {@link IndexedDISI}.
+   * @param cost same af the cost that will also be used for creating an {@link IndexedDISI}.
+   * @param name human readable designation, typically a field name. Used for debug, log and inspection.
+   * @return a cache for the given slice+offset+length or null if not suitable for caching.
+   */
+  public static IndexedDISICache getCache(IndexInput data, long offset, long length, long cost, String name) throws IOException {
+    if (length < MIN_LENGTH_FOR_CACHING) {
+      return null;
+    }
+
+    Map<Long, IndexedDISICache> caches = disiPool.computeIfAbsent(data.hashCode(), poolHash -> new HashMap<>());
+    long key = data.hashCode() + offset + length + cost;
+    IndexedDISICache cache = caches.get(key);
+    if (cache == null) {
+      // TODO: Avoid overlapping builds of the same cache
+      cache = new IndexedDISICache(data.slice("docs", offset, length),
+          BLOCK_CACHING_ENABLED, DENSE_CACHING_ENABLED, name);
+      caches.put(key, cache);
+      debug("Created IndexedDISI cache for " + data.toString() + ": " + cache.creationStats + " (" + cache.ramBytesUsed() + " bytes)");
+    }
+    return cache;
+  }
+
+  /**
+   * Creates a cache if not already present and returns it.
+   * @param poolHash the key for the map of caches in the {@link #disiPool}.
+   * @param slice    the input slice.
+   * @param cost     same af the cost that will also be used for creating an {@link IndexedDISI}.
+   * @param name human readable designation, typically a field name. Used for debug, log and inspection.
+   * @return a cache for the given slice+offset+length or null if not suitable for caching.
+   */
+  public static IndexedDISICache getCache(int poolHash, IndexInput slice, long cost, String name) throws IOException {
+    final long offset = slice.getFilePointer();
+    final long length = slice.length();
+    if (length < MIN_LENGTH_FOR_CACHING) {
+      return null;
+    }
+    final long cacheHash = poolHash + offset + length + cost;
+
+    Map<Long, IndexedDISICache> caches = disiPool.computeIfAbsent(poolHash, key -> new HashMap<>());
+
+    IndexedDISICache cache = caches.get(cacheHash);
+    if (cache == null) {
+      // TODO: Avoid overlapping builds of the same cache
+      cache = new IndexedDISICache(slice, BLOCK_CACHING_ENABLED, DENSE_CACHING_ENABLED, name);
+      caches.put(cacheHash, cache);
+      debug("Created cache for " + slice.toString() + ": " + cache.creationStats + " (" + cache.ramBytesUsed() + " bytes)");
+    }
+    return cache;
+  }
+
+  // TODO (Toke): Definitely not the way to do it. Connect to InputStream or just remove it fully when IndexedDISICache is stable
+  public static void debug(String message) {
+    if (DEBUG) {
+      System.out.println(IndexedDISICacheFactory.class.getSimpleName() + ": " + message);
+    }
+  }
+
+  @Override
+  public long ramBytesUsed() {
+    long mem = RamUsageEstimator.NUM_BYTES_OBJECT_REF + RamUsageEstimator.shallowSizeOf(disiPool);
+    for (Map.Entry<Integer, Map<Long, IndexedDISICache>> entry: disiPool.entrySet()) {
+      mem += RamUsageEstimator.shallowSizeOf(entry);
+      for (Map.Entry<Long, IndexedDISICache> cacheEntry: entry.getValue().entrySet()) {
+        mem += RamUsageEstimator.shallowSizeOf(cacheEntry);
+        mem += cacheEntry.getValue().ramBytesUsed();
+      }
+    }
+    for (Map.Entry<Integer, Map<String, VaryingBPVJumpTable>> entry: vBPVPool.entrySet()) {
+      mem += RamUsageEstimator.shallowSizeOf(entry);
+      for (Map.Entry<String, VaryingBPVJumpTable> cacheEntry: entry.getValue().entrySet()) {
+        mem += RamUsageEstimator.shallowSizeOf(cacheEntry);
+        mem += cacheEntry.getValue().ramBytesUsed();
+      }
+    }
+    return mem;
+  }
+
+  /**
+   * Jump table used by Lucene70DocValuesProducer.VaryingBPVReader to avoid iterating all blocks from
+   * current to wanted index. The jump table holds offsets for all blocks.
+   */
+  public static class VaryingBPVJumpTable implements Accountable {
+    // TODO: It is much too heavy to use longs here for practically all indexes. Maybe a PackedInts representation?
+    long[] offsets = new long[10];
+    final String creationStats;
+
+    public VaryingBPVJumpTable(RandomAccessInput slice, String name, long valuesLength) throws IOException {
+      final long startTime = System.nanoTime();
+
+      int block = -1;
+      long offset;
+      long blockEndOffset = 0;
+
+      int bitsPerValue;
+      // TODO (Toke): Introduce jump table
+      do {
+        offset = blockEndOffset;
+
+        offsets = ArrayUtil.grow(offsets, block+2); // No-op if large enough
+        offsets[block+1] = offset;
+
+        bitsPerValue = slice.readByte(offset++);
+        offset += Long.BYTES; // Skip over delta as we do not resolve the values themselves at this point
+        if (bitsPerValue == 0) {
+          blockEndOffset = offset;
+        } else {
+          final int length = slice.readInt(offset);
+          offset += Integer.BYTES;
+          blockEndOffset = offset + length;
+        }
+        block++;
+      } while (blockEndOffset < valuesLength-Byte.BYTES-Long.BYTES);
+      long[] newOffsets = new long[block+1];
+      System.arraycopy(offsets, 0, newOffsets, 0, block+1);
+      offsets = newOffsets;
+      creationStats = String.format(
+          "name=%s, blocks=%d, time=%dms",
+          name, offsets.length, (System.nanoTime()-startTime)/1000000);
+    }
+
+    public long getBlockOffset(long block) {
+      // Technically a limitation in caching vs. VaryingBPVReader to limit to 2b blocks
+      return offsets[(int) block];
+    }
+
+    @Override
+    public long ramBytesUsed() {
+      return (offsets == null ? 0 : RamUsageEstimator.sizeOf(offsets)) +
+          RamUsageEstimator.NUM_BYTES_OBJECT_REF*3 +
+          RamUsageEstimator.NUM_BYTES_OBJECT_HEADER + creationStats.length()*2;
+    }
+  }
+}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/lucene70/Lucene70DocValuesProducer.java b/lucene/core/src/java/org/apache/lucene/codecs/lucene70/Lucene70DocValuesProducer.java
index 386655e..89076cd 100644
--- a/lucene/core/src/java/org/apache/lucene/codecs/lucene70/Lucene70DocValuesProducer.java
+++ b/lucene/core/src/java/org/apache/lucene/codecs/lucene70/Lucene70DocValuesProducer.java
@@ -118,23 +118,23 @@ final class Lucene70DocValuesProducer extends DocValuesProducer implements Close
       }
       byte type = meta.readByte();
       if (type == Lucene70DocValuesFormat.NUMERIC) {
-        numerics.put(info.name, readNumeric(meta));
+        numerics.put(info.name, readNumeric(meta, info.name));
       } else if (type == Lucene70DocValuesFormat.BINARY) {
-        binaries.put(info.name, readBinary(meta));
+        binaries.put(info.name, readBinary(meta, info.name));
       } else if (type == Lucene70DocValuesFormat.SORTED) {
-        sorted.put(info.name, readSorted(meta));
+        sorted.put(info.name, readSorted(meta, info.name));
       } else if (type == Lucene70DocValuesFormat.SORTED_SET) {
-        sortedSets.put(info.name, readSortedSet(meta));
+        sortedSets.put(info.name, readSortedSet(meta, info.name));
       } else if (type == Lucene70DocValuesFormat.SORTED_NUMERIC) {
-        sortedNumerics.put(info.name, readSortedNumeric(meta));
+        sortedNumerics.put(info.name, readSortedNumeric(meta, info.name));
       } else {
         throw new CorruptIndexException("invalid type: " + type, meta);
       }
     }
   }
 
-  private NumericEntry readNumeric(ChecksumIndexInput meta) throws IOException {
-    NumericEntry entry = new NumericEntry();
+  private NumericEntry readNumeric(ChecksumIndexInput meta, String name) throws IOException {
+    NumericEntry entry = new NumericEntry(name);
     readNumeric(meta, entry);
     return entry;
   }
@@ -166,8 +166,8 @@ final class Lucene70DocValuesProducer extends DocValuesProducer implements Close
     entry.valuesLength = meta.readLong();
   }
 
-  private BinaryEntry readBinary(ChecksumIndexInput meta) throws IOException {
-    BinaryEntry entry = new BinaryEntry();
+  private BinaryEntry readBinary(ChecksumIndexInput meta, String name) throws IOException {
+    BinaryEntry entry = new BinaryEntry(name);
     entry.dataOffset = meta.readLong();
     entry.dataLength = meta.readLong();
     entry.docsWithFieldOffset = meta.readLong();
@@ -185,8 +185,8 @@ final class Lucene70DocValuesProducer extends DocValuesProducer implements Close
     return entry;
   }
 
-  private SortedEntry readSorted(ChecksumIndexInput meta) throws IOException {
-    SortedEntry entry = new SortedEntry();
+  private SortedEntry readSorted(ChecksumIndexInput meta, String name) throws IOException {
+    SortedEntry entry = new SortedEntry(name);
     entry.docsWithFieldOffset = meta.readLong();
     entry.docsWithFieldLength = meta.readLong();
     entry.numDocsWithField = meta.readInt();
@@ -197,12 +197,12 @@ final class Lucene70DocValuesProducer extends DocValuesProducer implements Close
     return entry;
   }
 
-  private SortedSetEntry readSortedSet(ChecksumIndexInput meta) throws IOException {
-    SortedSetEntry entry = new SortedSetEntry();
+  private SortedSetEntry readSortedSet(ChecksumIndexInput meta, String name) throws IOException {
+    SortedSetEntry entry = new SortedSetEntry(name);
     byte multiValued = meta.readByte();
     switch (multiValued) {
       case 0: // singlevalued
-        entry.singleValueEntry = readSorted(meta);
+        entry.singleValueEntry = readSorted(meta, name);
         return entry;
       case 1: // multivalued
         break;
@@ -244,8 +244,8 @@ final class Lucene70DocValuesProducer extends DocValuesProducer implements Close
     entry.termsIndexAddressesLength = meta.readLong();
   }
 
-  private SortedNumericEntry readSortedNumeric(ChecksumIndexInput meta) throws IOException {
-    SortedNumericEntry entry = new SortedNumericEntry();
+  private SortedNumericEntry readSortedNumeric(ChecksumIndexInput meta, String name) throws IOException {
+    SortedNumericEntry entry = new SortedNumericEntry(name);
     readNumeric(meta, entry);
     entry.numDocsWithField = meta.readInt();
     if (entry.numDocsWithField != entry.numValues) {
@@ -261,9 +261,23 @@ final class Lucene70DocValuesProducer extends DocValuesProducer implements Close
   @Override
   public void close() throws IOException {
     data.close();
+    IndexedDISICacheFactory.release(data);
   }
 
-  private static class NumericEntry {
+  // Highly debatable if this is a sane construct as the name is only used for debug/logging/inspection purposes
+  // This was introduced in LUCENE-8374
+  private static class EntryImpl {
+    final String name;
+
+    public EntryImpl(String name) {
+      this.name = name;
+    }
+  }
+
+  private static class NumericEntry extends EntryImpl {
+    public NumericEntry(String name) {
+      super(name);
+    }
     long[] table;
     int blockShift;
     byte bitsPerValue;
@@ -276,7 +290,10 @@ final class Lucene70DocValuesProducer extends DocValuesProducer implements Close
     long valuesLength;
   }
 
-  private static class BinaryEntry {
+  private static class BinaryEntry extends EntryImpl {
+    public BinaryEntry(String name) {
+      super(name);
+    }
     long dataOffset;
     long dataLength;
     long docsWithFieldOffset;
@@ -289,7 +306,10 @@ final class Lucene70DocValuesProducer extends DocValuesProducer implements Close
     DirectMonotonicReader.Meta addressesMeta;
   }
 
-  private static class TermsDictEntry {
+  private static class TermsDictEntry extends EntryImpl {
+    public TermsDictEntry(String name) {
+      super(name);
+    }
     long termsDictSize;
     int termsDictBlockShift;
     DirectMonotonicReader.Meta termsAddressesMeta;
@@ -307,6 +327,9 @@ final class Lucene70DocValuesProducer extends DocValuesProducer implements Close
   }
 
   private static class SortedEntry extends TermsDictEntry {
+    public SortedEntry(String name) {
+      super(name);
+    }
     long docsWithFieldOffset;
     long docsWithFieldLength;
     int numDocsWithField;
@@ -316,6 +339,9 @@ final class Lucene70DocValuesProducer extends DocValuesProducer implements Close
   }
 
   private static class SortedSetEntry extends TermsDictEntry {
+    public SortedSetEntry(String name) {
+      super(name);
+    }
     SortedEntry singleValueEntry;
     long docsWithFieldOffset;
     long docsWithFieldLength;
@@ -329,6 +355,9 @@ final class Lucene70DocValuesProducer extends DocValuesProducer implements Close
   }
 
   private static class SortedNumericEntry extends NumericEntry {
+    public SortedNumericEntry(String name) {
+      super(name);
+    }
     int numDocsWithField;
     DirectMonotonicReader.Meta addressesMeta;
     long addressesOffset;
@@ -434,44 +463,18 @@ final class Lucene70DocValuesProducer extends DocValuesProducer implements Close
           }
         };
       } else {
-        final RandomAccessInput slice = data.randomAccessSlice(entry.valuesOffset, entry.valuesLength);
         if (entry.blockShift >= 0) {
           // dense but split into blocks of different bits per value
-          final int shift = entry.blockShift;
-          final long mul = entry.gcd;
-          final int mask = (1 << shift) - 1;
           return new DenseNumericDocValues(maxDoc) {
-            int block = -1;
-            long delta;
-            long offset;
-            long blockEndOffset;
-            LongValues values;
+            final VaryingBPVReader vBPVReader = new VaryingBPVReader(entry);
 
             @Override
             public long longValue() throws IOException {
-              final int block = doc >>> shift;
-              if (this.block != block) {
-                int bitsPerValue;
-                do {
-                  offset = blockEndOffset;
-                  bitsPerValue = slice.readByte(offset++);
-                  delta = slice.readLong(offset);
-                  offset += Long.BYTES;
-                  if (bitsPerValue == 0) {
-                    blockEndOffset = offset;
-                  } else {
-                    final int length = slice.readInt(offset);
-                    offset += Integer.BYTES;
-                    blockEndOffset = offset + length;
-                  }
-                  this.block ++;
-                } while (this.block != block);
-                values = bitsPerValue == 0 ? LongValues.ZEROES : DirectReader.getInstance(slice, bitsPerValue, offset);
-              }
-              return mul * values.get(doc & mask) + delta;
+              return vBPVReader.getLongValue(doc);
             }
           };
         } else {
+          final RandomAccessInput slice = data.randomAccessSlice(entry.valuesOffset, entry.valuesLength);
           final LongValues values = DirectReader.getInstance(slice, entry.bitsPerValue);
           if (entry.table != null) {
             final long[] table = entry.table;
@@ -495,7 +498,8 @@ final class Lucene70DocValuesProducer extends DocValuesProducer implements Close
       }
     } else {
       // sparse
-      final IndexedDISI disi = new IndexedDISI(data, entry.docsWithFieldOffset, entry.docsWithFieldLength, entry.numValues);
+      final IndexedDISI disi = new IndexedDISI(
+          data, entry.docsWithFieldOffset, entry.docsWithFieldLength, entry.numValues, entry.name);
       if (entry.bitsPerValue == 0) {
         return new SparseNumericDocValues(disi) {
           @Override
@@ -504,45 +508,19 @@ final class Lucene70DocValuesProducer extends DocValuesProducer implements Close
           }
         };
       } else {
-        final RandomAccessInput slice = data.randomAccessSlice(entry.valuesOffset, entry.valuesLength);
         if (entry.blockShift >= 0) {
           // sparse and split into blocks of different bits per value
-          final int shift = entry.blockShift;
-          final long mul = entry.gcd;
-          final int mask = (1 << shift) - 1;
           return new SparseNumericDocValues(disi) {
-            int block = -1;
-            long delta;
-            long offset;
-            long blockEndOffset;
-            LongValues values;
+            final VaryingBPVReader vBPVReader = new VaryingBPVReader(entry);
 
             @Override
             public long longValue() throws IOException {
               final int index = disi.index();
-              final int block = index >>> shift;
-              if (this.block != block) {
-                int bitsPerValue;
-                do {
-                  offset = blockEndOffset;
-                  bitsPerValue = slice.readByte(offset++);
-                  delta = slice.readLong(offset);
-                  offset += Long.BYTES;
-                  if (bitsPerValue == 0) {
-                    blockEndOffset = offset;
-                  } else {
-                    final int length = slice.readInt(offset);
-                    offset += Integer.BYTES;
-                    blockEndOffset = offset + length;
-                  }
-                  this.block ++;
-                } while (this.block != block);
-                values = bitsPerValue == 0 ? LongValues.ZEROES : DirectReader.getInstance(slice, bitsPerValue, offset);
-              }
-              return mul * values.get(index & mask) + delta;
+              return vBPVReader.getLongValue(index);
             }
           };
         } else {
+          final RandomAccessInput slice = data.randomAccessSlice(entry.valuesOffset, entry.valuesLength);
           final LongValues values = DirectReader.getInstance(slice, entry.bitsPerValue);
           if (entry.table != null) {
             final long[] table = entry.table;
@@ -576,47 +554,20 @@ final class Lucene70DocValuesProducer extends DocValuesProducer implements Close
         }
       };
     } else {
-      final RandomAccessInput slice = data.randomAccessSlice(entry.valuesOffset, entry.valuesLength);
       if (entry.blockShift >= 0) {
-        final int shift = entry.blockShift;
-        final long mul = entry.gcd;
-        final long mask = (1L << shift) - 1;
         return new LongValues() {
-          long block = -1;
-          long delta;
-          long offset;
-          long blockEndOffset;
-          LongValues values;
-
+          final VaryingBPVReader vBPVReader = new VaryingBPVReader(entry);
+          @Override
           public long get(long index) {
-            final long block = index >>> shift;
-            if (this.block != block) {
-              assert block > this.block : "Reading backwards is illegal: " + this.block + " < " + block;
-              int bitsPerValue;
-              do {
-                offset = blockEndOffset;
                 try {
-                  bitsPerValue = slice.readByte(offset++);
-                  delta = slice.readLong(offset);
-                  offset += Long.BYTES;
-                  if (bitsPerValue == 0) {
-                    blockEndOffset = offset;
-                  } else {
-                    final int length = slice.readInt(offset);
-                    offset += Integer.BYTES;
-                    blockEndOffset = offset + length;
-                  }
+              return vBPVReader.getLongValue(index);
                 } catch (IOException e) {
                   throw new RuntimeException(e);
                 }
-                this.block ++;
-              } while (this.block != block);
-              values = bitsPerValue == 0 ? LongValues.ZEROES : DirectReader.getInstance(slice, bitsPerValue, offset);
-            }
-            return mul * values.get(index & mask) + delta;
           }
         };
       } else {
+        final RandomAccessInput slice = data.randomAccessSlice(entry.valuesOffset, entry.valuesLength);
         final LongValues values = DirectReader.getInstance(slice, entry.bitsPerValue);
         if (entry.table != null) {
           final long[] table = entry.table;
@@ -766,7 +717,8 @@ final class Lucene70DocValuesProducer extends DocValuesProducer implements Close
       }
     } else {
       // sparse
-      final IndexedDISI disi = new IndexedDISI(data, entry.docsWithFieldOffset, entry.docsWithFieldLength, entry.numDocsWithField);
+      final IndexedDISI disi = new IndexedDISI(
+          data, entry.docsWithFieldOffset, entry.docsWithFieldLength, entry.numDocsWithField, entry.name);
       if (entry.minLength == entry.maxLength) {
         // fixed length
         final int length = entry.maxLength;
@@ -867,7 +819,8 @@ final class Lucene70DocValuesProducer extends DocValuesProducer implements Close
       };
     } else {
       // sparse
-      final IndexedDISI disi = new IndexedDISI(data, entry.docsWithFieldOffset, entry.docsWithFieldLength, entry.numDocsWithField);
+      final IndexedDISI disi = new IndexedDISI(
+          data, entry.docsWithFieldOffset, entry.docsWithFieldLength, entry.numDocsWithField, entry.name);
       return new BaseSortedDocValues(entry, data) {
 
         @Override
@@ -1230,7 +1183,8 @@ final class Lucene70DocValuesProducer extends DocValuesProducer implements Close
       };
     } else {
       // sparse
-      final IndexedDISI disi = new IndexedDISI(data, entry.docsWithFieldOffset, entry.docsWithFieldLength, entry.numDocsWithField);
+      final IndexedDISI disi = new IndexedDISI(
+          data, entry.docsWithFieldOffset, entry.docsWithFieldLength, entry.numDocsWithField, field.name);
       return new SortedNumericDocValues() {
 
         boolean set;
@@ -1356,7 +1310,8 @@ final class Lucene70DocValuesProducer extends DocValuesProducer implements Close
       };
     } else {
       // sparse
-      final IndexedDISI disi = new IndexedDISI(data, entry.docsWithFieldOffset, entry.docsWithFieldLength, entry.numDocsWithField);
+      final IndexedDISI disi = new IndexedDISI(
+          data, entry.docsWithFieldOffset, entry.docsWithFieldLength, entry.numDocsWithField, field.name);
       return new BaseSortedSetDocValues(entry, data) {
 
         boolean set;
@@ -1416,4 +1371,64 @@ final class Lucene70DocValuesProducer extends DocValuesProducer implements Close
     CodecUtil.checksumEntireFile(data);
   }
 
+  /**
+   * Reader for longs split into blocks of different bits per values.
+   * The longs are requested by index and must be accessed in monotonically increasing order.
+   */
+  // Note: The order requirement goes away when using caching.
+  private class VaryingBPVReader {
+    final RandomAccessInput slice;
+    final NumericEntry entry;
+    final int shift;
+    final long mul;
+    final int mask;
+
+    long block = -1;
+    long delta;
+    long offset;
+    long blockEndOffset;
+    LongValues values;
+
+    VaryingBPVReader(NumericEntry entry) throws IOException {
+      this.entry = entry;
+      slice = data.randomAccessSlice(entry.valuesOffset, entry.valuesLength);
+      shift = entry.blockShift;
+      mul = entry.gcd;
+      mask = (1 << shift) - 1;
+    }
+
+    long getLongValue(long index) throws IOException {
+      final long block = index >>> shift;
+      if (this.block != block) {
+        int bitsPerValue;
+        // TODO (Toke): Introduce jump table
+        do {
+          // We delay cache generation to access-time to avoid non-used cashes
+          // Unfortunately merging causes the cache to be created - can this be avoided?
+          IndexedDISICacheFactory.VaryingBPVJumpTable cache =
+              IndexedDISICacheFactory.VARYINGBPV_CACHING_ENABLED ?
+              IndexedDISICacheFactory.getVBPVJumpTable(data.hashCode(), entry.name, slice, entry.valuesLength) :
+              null;
+          if (cache != null) {
+            blockEndOffset = cache.getBlockOffset(block);
+            this.block = block-1;
+          }
+          offset = blockEndOffset;
+          bitsPerValue = slice.readByte(offset++);
+          delta = slice.readLong(offset);
+          offset += Long.BYTES;
+          if (bitsPerValue == 0) {
+            blockEndOffset = offset;
+          } else {
+            final int length = slice.readInt(offset);
+            offset += Integer.BYTES;
+            blockEndOffset = offset + length;
+          }
+          this.block++;
+        } while (this.block != block);
+        values = bitsPerValue == 0 ? LongValues.ZEROES : DirectReader.getInstance(slice, bitsPerValue, offset);
+      }
+      return mul * values.get(index & mask) + delta;
+    }
+  }
 }
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/lucene70/Lucene70NormsProducer.java b/lucene/core/src/java/org/apache/lucene/codecs/lucene70/Lucene70NormsProducer.java
index eb7c41a..dd97e97 100644
--- a/lucene/core/src/java/org/apache/lucene/codecs/lucene70/Lucene70NormsProducer.java
+++ b/lucene/core/src/java/org/apache/lucene/codecs/lucene70/Lucene70NormsProducer.java
@@ -43,8 +43,12 @@ import org.apache.lucene.util.IOUtils;
 final class Lucene70NormsProducer extends NormsProducer {
   // metadata maps (just file pointers and minimal stuff)
   private final Map<Integer,NormsEntry> norms = new HashMap<>();
-  private final IndexInput data;
   private final int maxDoc;
+  private IndexInput data;
+  private boolean merging;
+  private Map<Integer, IndexInput> disiInputs;
+  private Map<Integer, RandomAccessInput> dataInputs;
+
 
   Lucene70NormsProducer(SegmentReadState state, String dataCodec, String dataExtension, String metaCodec, String metaExtension) throws IOException {
     maxDoc = state.segmentInfo.maxDoc();
@@ -245,7 +249,8 @@ final class Lucene70NormsProducer extends NormsProducer {
       }
     } else {
       // sparse
-      final IndexedDISI disi = new IndexedDISI(data, entry.docsWithFieldOffset, entry.docsWithFieldLength, entry.numDocsWithField);
+      // TODO (Toke): Review if it makes sense to use caching here - aren't there already skip structures in place?
+      final IndexedDISI disi = new IndexedDISI(data, entry.docsWithFieldOffset, entry.docsWithFieldLength, entry.numDocsWithField, field.name);
       if (entry.bytesPerNorm == 0) {
         return new SparseNormsIterator(disi) {
           @Override
@@ -294,6 +299,7 @@ final class Lucene70NormsProducer extends NormsProducer {
   @Override
   public void close() throws IOException {
     data.close();
+    IndexedDISICacheFactory.release(data);
   }
 
   @Override
diff --git a/lucene/core/src/test/org/apache/lucene/codecs/lucene70/TestIndexedDISI.java b/lucene/core/src/test/org/apache/lucene/codecs/lucene70/TestIndexedDISI.java
index 64bfbd5..63ea626 100644
--- a/lucene/core/src/test/org/apache/lucene/codecs/lucene70/TestIndexedDISI.java
+++ b/lucene/core/src/test/org/apache/lucene/codecs/lucene70/TestIndexedDISI.java
@@ -19,6 +19,7 @@ package org.apache.lucene.codecs.lucene70;
 import java.io.IOException;
 import java.util.ArrayList;
 import java.util.List;
+import java.util.Locale;
 
 import org.apache.lucene.search.DocIdSetIterator;
 import org.apache.lucene.store.Directory;
@@ -150,6 +151,179 @@ public class TestIndexedDISI extends LuceneTestCase {
       }
     }
   }
+  public void testDenseMultiBlock() throws IOException {
+    try (Directory dir = newDirectory()) {
+      int maxDoc = 10 * 65536; // 10 blocks
+      FixedBitSet set = new FixedBitSet(maxDoc);
+      for (int i = 0; i < maxDoc; i += 2) { // Set every other to ensure dense
+        set.set(i);
+      }
+      doTest(set, dir);
+    }
+  }
+
+  public void testOneDocMissingFixed() throws IOException {
+    int maxDoc = 9699;
+    FixedBitSet set = new FixedBitSet(maxDoc);
+    set.set(0, maxDoc);
+    set.clear(1345);
+    try (Directory dir = newDirectory()) {
+
+      final int cardinality = set.cardinality();
+      long length;
+      try (IndexOutput out = dir.createOutput("foo", IOContext.DEFAULT)) {
+        IndexedDISI.writeBitSet(new BitSetIterator(set, cardinality), out);
+        length = out.getFilePointer();
+      }
+
+      int step = 16000;
+      try (IndexInput in = dir.openInput("foo", IOContext.DEFAULT)) {
+        IndexedDISI disi = new IndexedDISI(in, 0L, length, cardinality);
+        BitSetIterator disi2 = new BitSetIterator(set, cardinality);
+        assertAdvanceEquality(disi, disi2, step);
+      }
+    }
+  }
+
+  final int[] SIZES = new int[]{10_000, 100_000, 1_000_000, 10_000_000, 100_000_000};
+  final int[] STEPS = new int[]{1, 9, 99, 999, 9_999, 99_999, 999_999, 9_999_999, 99_999_999};
+  public void testCacheSpeedDense() throws IOException {
+    IndexedDISICacheFactory.DEBUG = false; // No chattiness as we want a clean output
+    for (int size: SIZES) {
+      FixedBitSet set = new FixedBitSet(size);
+      for (int i = 0; i < size; i += 3) {
+        set.set(i); // Quite DENSE
+      }
+      measureCacheSpeed("Dense", set, STEPS);
+    }
+  }
+
+  public void testCacheVaryingDensity() throws IOException {
+    IndexedDISICacheFactory.DEBUG = false; // No chattiness as we want a clean output
+    for (int size: SIZES) {
+      FixedBitSet set = new FixedBitSet(size);
+      set.clear(0, set.length());
+      for (int maxSkip: STEPS) {
+        for (int i = 0; i < size; i += random().nextInt(maxSkip)+1) {
+          set.set(i);
+        }
+        measureCacheSpeed("R-dense(" + (maxSkip+1) + ")", set, STEPS);
+      }
+    }
+  }
+
+  public void testCacheSpeedNearFull() throws IOException {
+    IndexedDISICacheFactory.DEBUG = false; // No chattiness as we want a clean output
+    for (int size: SIZES) {
+      FixedBitSet set = new FixedBitSet(size);
+      set.set(0, set.length());
+      set.clear(set.length() - 1);
+      measureCacheSpeed("Near full", set, STEPS);
+    }
+  }
+
+  private void measureCacheSpeed(String designation, FixedBitSet set, int[] steps) throws IOException {
+    for (int step: new int[]{1, 9, 99, 999, 9_999, 99_999, 999_999, 9_999_999, 99_999_999}) {
+      if (step > set.length()) {
+        continue;
+      }
+      measureCacheSpeed(designation, set, step);
+    }
+
+  }
+
+  // TODO (Toke): Remove when stable
+  // This microbenchmark is just for sanity checking and not intended to provide realistic measurements
+  private void measureCacheSpeed(String designation, FixedBitSet set, int step) throws IOException {
+    final double MSD = 1000000.0;
+    final int cardinality = set.cardinality();
+    long length;
+    try (Directory dir = newDirectory()) {
+      try (IndexOutput out = dir.createOutput("foo", IOContext.DEFAULT)) {
+        IndexedDISI.writeBitSet(new BitSetIterator(set, cardinality), out);
+        length = out.getFilePointer();
+      }
+
+      try (IndexInput inVanilla1 = dir.openInput("foo", IOContext.DEFAULT);
+           IndexInput inCached1 = dir.openInput("foo", IOContext.DEFAULT);
+           IndexInput inVanilla2 = dir.openInput("foo", IOContext.DEFAULT);
+           IndexInput inCached2 = dir.openInput("foo", IOContext.DEFAULT)) {
+        IndexedDISI vanilla1 = new IndexedDISI(inVanilla1, 0L, length, cardinality, false);
+        IndexedDISI cached1 = new IndexedDISI(inCached1, 0L, length, cardinality, true);
+        IndexedDISI vanilla2 = new IndexedDISI(inVanilla2, 0L, length, cardinality, false);
+        IndexedDISI cached2 = new IndexedDISI(inCached2, 0L, length, cardinality, true);
+
+        final long c1NS = stepTime(cached1, step);
+        final long v1NS = stepTime(vanilla1, step);
+        final long c2NS = stepTime(cached2, step);
+        final long v2NS = stepTime(vanilla2, step);
+        System.out.println(String.format(Locale.ENGLISH,
+            "%-20s: length=%10d, step=%10d, v1=[%6.2f, %6.2f]ms, c1=[%6.2f, %6.2f]ms, c2/v2=%4d%%",
+            designation, set.length(), step, v1NS/MSD, v2NS/MSD, c1NS/MSD, c2NS/MSD, c2NS*100/v2NS));
+      }
+    }
+  }
+
+  // TODO (Toke): Remove when stable
+  private long stepTime(IndexedDISI disi, int step) throws IOException {
+    final long startTime = System.nanoTime();
+
+    int target = 0;
+    int doc;
+    while ((doc = disi.advance(target)) != DocIdSetIterator.NO_MORE_DOCS) {
+      target = doc + step;
+    }
+
+    return System.nanoTime()-startTime;
+  }
+
+  // TODO (Toke): Remove when stable
+  public void testExplorativeTestCachedDense() throws IOException {
+    try (Directory dir = newDirectory()) {
+      int maxDoc = 100*65536; // 10 blocks
+      FixedBitSet set = new FixedBitSet(maxDoc);
+      for (int block = 0 ; block < 100 ; block++) {
+        switch (block % 4) {
+          case 0: break; // EMPTY
+          case 1: {
+            for (int i = 0; i < 65536; i++) { // ALL
+              set.set((block << 16) | i);
+            }
+            break;
+          }
+          case 2: {
+            for (int i = 0; i < 65536; i += 2) { // DENSE
+              set.set((block << 16) | i);
+            }
+            break;
+          }
+          case 3: {
+            for (int i = 0; i < 65536; i += 5000) { // SPARSE
+              set.set((block << 16) | i);
+            }
+            break;
+          }
+        }
+      }
+
+      final int cardinality = set.cardinality();
+      long length;
+      try (IndexOutput out = dir.createOutput("foo", IOContext.DEFAULT)) {
+        IndexedDISI.writeBitSet(new BitSetIterator(set, cardinality), out);
+        length = out.getFilePointer();
+      }
+
+      for (int step: new int[] {1, 10, 100, 1000, 10000, 100000}) {
+        try (IndexInput in = dir.openInput("foo", IOContext.DEFAULT)) {
+//        IndexedDISICache cache = new IndexedDISICache(in.slice("docs", 0L, length), true, true);
+          IndexedDISI disi = new IndexedDISI(in, 0L, length, cardinality, true);
+          BitSetIterator disi2 = new BitSetIterator(set, cardinality);
+          assertAdvanceEquality(disi, disi2, step);
+//        assertAdvanceExactRandomized(disi, disi2, set.cardinality(), step);
+        }
+      }
+    }
+  }
 
   public void testRandom() throws IOException {
     try (Directory dir = newDirectory()) {
@@ -246,4 +420,21 @@ public class TestIndexedDISI extends LuceneTestCase {
     dir.deleteFile("foo");
   }
 
+  private void assertAdvanceEquality(IndexedDISI disi, BitSetIterator disi2, int step) throws IOException {
+    int index = -1;
+    while (true) {
+      int target = disi2.docID() + step;
+      int doc;
+      do {
+        doc = disi2.nextDoc();
+        index++;
+      } while (doc < target);
+      assertEquals(doc, disi.advance(target));
+      if (doc == DocIdSetIterator.NO_MORE_DOCS) {
+        break;
+      }
+      assertEquals("Expected equality using step " + step + " at docID " + doc, index, disi.index());
+    }
+  }
+
 }
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestDocValues.java b/lucene/core/src/test/org/apache/lucene/index/TestDocValues.java
index 0214e54..e016bfb 100644
--- a/lucene/core/src/test/org/apache/lucene/index/TestDocValues.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestDocValues.java
@@ -18,7 +18,11 @@ package org.apache.lucene.index;
 
 
 import java.io.IOException;
+import java.nio.file.Paths;
 
+import org.apache.lucene.analysis.standard.StandardAnalyzer;
+import org.apache.lucene.codecs.Codec;
+import org.apache.lucene.codecs.lucene70.IndexedDISICacheFactory;
 import org.apache.lucene.document.BinaryDocValuesField;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
@@ -27,7 +31,9 @@ import org.apache.lucene.document.SortedDocValuesField;
 import org.apache.lucene.document.SortedNumericDocValuesField;
 import org.apache.lucene.document.SortedSetDocValuesField;
 import org.apache.lucene.document.StringField;
+import org.apache.lucene.search.DocIdSetIterator;
 import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.MMapDirectory;
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.IOUtils;
 import org.apache.lucene.util.LuceneTestCase;
@@ -125,6 +131,226 @@ public class TestDocValues extends LuceneTestCase {
   }
   
   /** 
+   * Triggers varying bits per value codec representation for numeric.
+   */
+  public void testNumericFieldVaryingBPV() throws Exception {
+    Directory dir = newDirectory();
+    IndexWriter iw = new IndexWriter(dir, newIndexWriterConfig(null));
+    long generatedSum = 0;
+    for (int bpv = 2 ; bpv < 24 ; bpv+=3) {
+      for (int i = 0 ; i < 66000 ; i++) {
+        Document doc = new Document();
+        int max = 1 << (bpv - 1);
+        int value =  random().nextInt(max) | max;
+        generatedSum += value;
+        //System.out.println("--- " + value);
+        doc.add(new NumericDocValuesField("foo", value));
+        iw.addDocument(doc);
+      }
+    }
+    iw.flush();
+    iw.forceMerge(1, true);
+    iw.commit();
+    DirectoryReader dr = DirectoryReader.open(iw);
+    LeafReader r = getOnlyLeafReader(dr);
+
+    // ok
+    NumericDocValues numDV = DocValues.getNumeric(r, "foo");
+
+    assertNotNull(numDV);
+    long sum = 0;
+    while (numDV.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {
+      sum += numDV.longValue();
+    }
+    assertEquals("The sum of retrieved values should match the input", generatedSum, sum);
+
+//    assertNotNull(DocValues.getSortedNumeric(r, "foo"));
+
+    dr.close();
+    iw.close();
+    dir.close();
+  }
+
+  // TODO (Toke): Remove this when LUCENE-8374 is ready for release
+  // IMPORTANT: This _does not yet_ guarantee triggering of the varying_BPC codec part, so that part is rarely measured
+  public void testNumericRetrievalSpeed() throws IOException {
+    final int MAJOR_RUNS = 1;
+    final int INNER_RUNS = 10;
+    final int[] DOCS_PER_BPV = new int[]{100, 10_000, 500_000, 2_000_000};
+    final int[] QUERIES = new int[]{10, 100, 1_000, 10_000, 100_000};
+
+    boolean oldDebug = IndexedDISICacheFactory.DEBUG;
+
+    for (int docsPerBPV: DOCS_PER_BPV) {
+      IndexedDISICacheFactory.DEBUG = false;
+      int estSize = (24-2)/3*docsPerBPV;
+      System.out.println("Generating plain & optimized indexes with ~" + estSize + " documents");
+      Directory dirPlain = new MMapDirectory(Paths.get(System.getProperty("java.io.tmpdir"), "plain_" + random().nextInt()));
+      generateVaryingBPVIndex(dirPlain, 2, 3, 24, docsPerBPV, false);
+
+      Directory dirOptimize = new MMapDirectory(Paths.get(System.getProperty("java.io.tmpdir"), "optimized_" + random().nextInt()));
+      generateVaryingBPVIndex(dirOptimize, 2, 3, 24, docsPerBPV, true);
+
+      // Disk cache warm
+      final double[] NONE = new double[]{-1d, -1d};
+
+      System.out.println("Warming disk cache plain");
+      numericRetrievalSpeed(dirPlain, 5, 1000, true, true, true, false, NONE);
+      System.out.println("Warming disk cache optimized");
+      numericRetrievalSpeed(dirOptimize, 5, 1000, true, true, true, false, NONE);
+
+      System.out.println("Running performance tests");
+      cacheNote("Multi-segment", dirPlain);
+      cacheNote("Single-segment", dirOptimize);
+
+      for (int run = 0; run < MAJOR_RUNS; run++) {
+        System.out.println(DV_PERFORMANCE_HEADER);
+        for (int queries: QUERIES) {
+          for (boolean optimize: new boolean[]{false, true}) {
+            Directory dir = optimize ? dirOptimize : dirPlain;
+            double[] basePlain = numericRetrievalSpeed(dir, INNER_RUNS, queries, false, false, false, true, NONE);
+            numericRetrievalSpeed(dir, INNER_RUNS, queries, true, false, false, true, basePlain);
+            numericRetrievalSpeed(dir, INNER_RUNS, queries, false, true, false, true, basePlain);
+            numericRetrievalSpeed(dir, INNER_RUNS, queries, false, false, true, true, basePlain);
+            numericRetrievalSpeed(dir, INNER_RUNS, queries, true, true, true, true, basePlain);
+            // Run baseline again and compare to old to observe measuring skews due to warming and chance
+            numericRetrievalSpeed(dir, INNER_RUNS, queries, false, false, false, true, basePlain);
+            System.out.println("");
+          }
+        }
+        System.out.println("----------------------");
+      }
+
+      dirPlain.close();
+      dirOptimize.close();
+    }
+    IndexedDISICacheFactory.DEBUG = oldDebug;
+  }
+
+  private void cacheNote(String designation, Directory dir) throws IOException {
+    boolean[] capabilities = getCacheability(dir);
+    if (!(capabilities[0] && capabilities[1] && capabilities[2])) {
+      System.out.println(String.format(
+          "* Note: %s index can only get caches for block=%b, dense=%b, vBPV=%b",
+          designation, capabilities[0], capabilities[1], capabilities[2]));
+    }
+  }
+
+  public static final String DV_PERFORMANCE_HEADER = "  docs segments requests block dense  vBPV worst_r/s best_r/s  worst/base best/base";
+  public static final String DV_PERFORMANCE_PATTERN = "%6s %8s %8s %5s %5s %5s %9s %8s %10.0f%% %9.0f%%";
+
+  // Returns [worst, best] docs/s
+  private double[] numericRetrievalSpeed(
+      Directory dir, int runs, int requests, boolean block, boolean dense, boolean vBPV, boolean print, double[] base) throws IOException {
+
+    IndexedDISICacheFactory.BLOCK_CACHING_ENABLED = block;
+    IndexedDISICacheFactory.DENSE_CACHING_ENABLED = dense;
+    IndexedDISICacheFactory.VARYINGBPV_CACHING_ENABLED = vBPV;
+
+    DirectoryReader dr = DirectoryReader.open(dir);
+    int maxDoc = dr.maxDoc();
+
+    long best = Long.MAX_VALUE;
+    long worst = -1;
+    long sum = -1;
+    for (int run = 0 ; run < runs ; run++) {
+      long runTime = -System.nanoTime();
+      for (int q = 0 ; q < requests ; q++) {
+        final int docID = random().nextInt(maxDoc-1);
+
+        int readerIndex = dr.readerIndex(docID);
+        LeafReader reader = dr.leaves().get(readerIndex).reader();
+        NumericDocValues numDV = reader.getNumericDocValues("dv");
+        if (!numDV.advanceExact(docID-dr.readerBase(readerIndex))) {
+          //System.err.println("Expected numeric doc value for docID=" + docID);
+          continue;
+        }
+        sum += numDV.longValue();
+      }
+      runTime += System.nanoTime();
+      best = Math.min(best, runTime);
+      worst = Math.max(worst, runTime);
+    }
+    double worstDPS = requests / (worst/1000000.0/1000);
+    double bestDPS = requests / (best/1000000.0/1000);
+    double worstRelative = base[0] < 0 ? 100 : worstDPS*100/base[0];
+    double bestRelative = base[1] < 0 ? 100 : bestDPS*100/base[1];
+    if (print) {
+      System.out.println(String.format(DV_PERFORMANCE_PATTERN,
+          shorten(maxDoc), dr.leaves().size(), shorten(requests),
+          block ? "block" : "", dense ? "dense" : "", vBPV ? "vBPV" : "",
+          shortenKB((int) worstDPS), shortenKB((int) bestDPS), worstRelative, bestRelative));
+    }
+    assertFalse("There should be at least 1 long value", sum == -1);
+
+    dr.close();
+    return new double[]{worstDPS, bestDPS};
+  }
+
+  // returns [block, dense, vBPV]
+  private boolean[] getCacheability(Directory dir) throws IOException {
+
+    IndexedDISICacheFactory.BLOCK_CACHING_ENABLED = true;
+    IndexedDISICacheFactory.DENSE_CACHING_ENABLED = true;
+    IndexedDISICacheFactory.VARYINGBPV_CACHING_ENABLED = true;
+
+    try (DirectoryReader dr = DirectoryReader.open(dir)) {
+      int maxDoc = dr.maxDoc();
+
+      for (int run = 0; run < 100; run++) {
+        final int docID = run * 100 / maxDoc;
+
+        int readerIndex = dr.readerIndex(docID);
+        LeafReader reader = dr.leaves().get(readerIndex).reader();
+        NumericDocValues numDV = reader.getNumericDocValues("dv");
+        if (numDV.advanceExact(docID - dr.readerBase(readerIndex))) {
+          numDV.longValue();
+        }
+
+      }
+      return new boolean[]{
+          IndexedDISICacheFactory.getDISIBlocksWithOffsetsCount() > 0,
+          IndexedDISICacheFactory.getDISIBlocksWithRankCount() > 0,
+          IndexedDISICacheFactory.getVaryingBPVCount() > 0};
+    }
+  }
+
+  private String shortenKB(int requests) {
+    return requests >= 1_000 ? requests/1_000+"K" : requests+"";
+  }
+  private String shorten(int requests) {
+    return requests >= 1_000_000 ? requests/1_000_000+"M" : requests >= 1_000 ? requests/1_000+"K" : requests+"";
+  }
+
+  private void generateVaryingBPVIndex(
+      Directory dir, int bpvMin, int bpvStep, int bpvMax, int docsPerBPV, boolean optimize) throws IOException {
+
+    IndexWriterConfig iwc = new IndexWriterConfig(new StandardAnalyzer());
+    iwc.setCodec(Codec.forName("Lucene70"));
+    IndexWriter iw = new IndexWriter(dir, iwc);
+
+    int id = 0;
+    for (int bpv = bpvMin ; bpv < bpvMax+1 ; bpv += bpvStep) {
+      for (int i = 0 ; i < docsPerBPV ; i++) {
+        Document doc = new Document();
+        int max = 1 << (bpv - 1);
+        int value =  random().nextInt(max) | max;
+        doc.add(new StringField("id", Integer.toString(id++), Field.Store.YES));
+        if (id % 87 != 0) { // Ensure sparse
+          doc.add(new NumericDocValuesField("dv", value));
+        }
+        iw.addDocument(doc);
+      }
+    }
+    iw.flush();
+    if (optimize) {
+      iw.forceMerge(1, true);
+    }
+    iw.commit();
+    iw.close();
+  }
+
+  /**
    * field with binary docvalues
    */
   public void testBinaryField() throws Exception {
