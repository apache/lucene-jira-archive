Index: solr/contrib/extraction/src/test/resources/solr/conf/schema.xml
===================================================================
--- solr/contrib/extraction/src/test/resources/solr/conf/schema.xml	(revision 947868)
+++ solr/contrib/extraction/src/test/resources/solr/conf/schema.xml	(working copy)
@@ -127,7 +127,7 @@
 
 
     <fieldtype name="nametext" class="solr.TextField">
-      <analyzer class="org.apache.lucene.analysis.WhitespaceAnalyzer"/>
+      <analyzer class="org.apache.lucene.analysis.core.WhitespaceAnalyzer"/>
     </fieldtype>
 
     <fieldtype name="teststop" class="solr.TextField">
Index: solr/src/test/test-files/solr/conf/schema-required-fields.xml
===================================================================
--- solr/src/test/test-files/solr/conf/schema-required-fields.xml	(revision 947868)
+++ solr/src/test/test-files/solr/conf/schema-required-fields.xml	(working copy)
@@ -110,7 +110,7 @@
 
 
     <fieldtype name="nametext" class="solr.TextField">
-      <analyzer class="org.apache.lucene.analysis.WhitespaceAnalyzer"/>
+      <analyzer class="org.apache.lucene.analysis.core.WhitespaceAnalyzer"/>
     </fieldtype>
 
     <fieldtype name="teststop" class="solr.TextField">
Index: solr/src/test/test-files/solr/conf/schema.xml
===================================================================
--- solr/src/test/test-files/solr/conf/schema.xml	(revision 947868)
+++ solr/src/test/test-files/solr/conf/schema.xml	(working copy)
@@ -140,7 +140,7 @@
 
 
     <fieldtype name="nametext" class="solr.TextField">
-      <analyzer class="org.apache.lucene.analysis.WhitespaceAnalyzer"/>
+      <analyzer class="org.apache.lucene.analysis.core.WhitespaceAnalyzer"/>
     </fieldtype>
 
     <fieldtype name="teststop" class="solr.TextField">
Index: solr/src/test/test-files/solr/conf/schema12.xml
===================================================================
--- solr/src/test/test-files/solr/conf/schema12.xml	(revision 947868)
+++ solr/src/test/test-files/solr/conf/schema12.xml	(working copy)
@@ -146,7 +146,7 @@
 
 
     <fieldtype name="nametext" class="solr.TextField">
-      <analyzer class="org.apache.lucene.analysis.WhitespaceAnalyzer"/>
+      <analyzer class="org.apache.lucene.analysis.core.WhitespaceAnalyzer"/>
     </fieldtype>
 
     <fieldtype name="teststop" class="solr.TextField">
Index: solr/src/test/test-files/solr/conf/schema-copyfield-test.xml
===================================================================
--- solr/src/test/test-files/solr/conf/schema-copyfield-test.xml	(revision 947868)
+++ solr/src/test/test-files/solr/conf/schema-copyfield-test.xml	(working copy)
@@ -119,7 +119,7 @@
 
 
     <fieldtype name="nametext" class="solr.TextField">
-      <analyzer class="org.apache.lucene.analysis.WhitespaceAnalyzer"/>
+      <analyzer class="org.apache.lucene.analysis.core.WhitespaceAnalyzer"/>
     </fieldtype>
 
     <fieldtype name="teststop" class="solr.TextField">
Index: solr/src/test/org/apache/solr/analysis/TestBufferedTokenStream.java
===================================================================
--- solr/src/test/org/apache/solr/analysis/TestBufferedTokenStream.java	(revision 947868)
+++ solr/src/test/org/apache/solr/analysis/TestBufferedTokenStream.java	(working copy)
@@ -20,7 +20,7 @@
 import org.apache.lucene.analysis.Token;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
 
 import java.io.IOException;
Index: solr/src/test/org/apache/solr/analysis/TestGermanStemFilterFactory.java
===================================================================
--- solr/src/test/org/apache/solr/analysis/TestGermanStemFilterFactory.java	(revision 947868)
+++ solr/src/test/org/apache/solr/analysis/TestGermanStemFilterFactory.java	(working copy)
@@ -22,7 +22,7 @@
 
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 
 /**
  * Simple tests to ensure the German stem filter factory is working.
Index: solr/src/test/org/apache/solr/analysis/TestPorterStemFilterFactory.java
===================================================================
--- solr/src/test/org/apache/solr/analysis/TestPorterStemFilterFactory.java	(revision 947868)
+++ solr/src/test/org/apache/solr/analysis/TestPorterStemFilterFactory.java	(working copy)
@@ -22,7 +22,7 @@
 
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 
 /**
  * Simple tests to ensure the Porter stem filter factory is working.
Index: solr/src/test/org/apache/solr/analysis/TestPatternReplaceCharFilterFactory.java
===================================================================
--- solr/src/test/org/apache/solr/analysis/TestPatternReplaceCharFilterFactory.java	(revision 947868)
+++ solr/src/test/org/apache/solr/analysis/TestPatternReplaceCharFilterFactory.java	(working copy)
@@ -25,7 +25,7 @@
 import org.apache.lucene.analysis.CharReader;
 import org.apache.lucene.analysis.CharStream;
 import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 
 /**
  * Simple tests to ensure this factory is working
Index: solr/src/test/org/apache/solr/analysis/TestMultiWordSynonyms.java
===================================================================
--- solr/src/test/org/apache/solr/analysis/TestMultiWordSynonyms.java	(revision 947868)
+++ solr/src/test/org/apache/solr/analysis/TestMultiWordSynonyms.java	(working copy)
@@ -1,6 +1,6 @@
 package org.apache.solr.analysis;
 
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 import org.apache.lucene.analysis.synonym.SynonymFilter;
 import org.apache.lucene.analysis.synonym.SynonymMap;
 import org.junit.Test;
Index: solr/src/test/org/apache/solr/analysis/TestPersianNormalizationFilterFactory.java
===================================================================
--- solr/src/test/org/apache/solr/analysis/TestPersianNormalizationFilterFactory.java	(revision 947868)
+++ solr/src/test/org/apache/solr/analysis/TestPersianNormalizationFilterFactory.java	(working copy)
@@ -22,7 +22,7 @@
 
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 
 /**
  * Simple tests to ensure the Persian normalization factory is working.
Index: solr/src/test/org/apache/solr/analysis/TestPatternReplaceFilterFactory.java
===================================================================
--- solr/src/test/org/apache/solr/analysis/TestPatternReplaceFilterFactory.java	(revision 947868)
+++ solr/src/test/org/apache/solr/analysis/TestPatternReplaceFilterFactory.java	(working copy)
@@ -18,7 +18,7 @@
 package org.apache.solr.analysis;
 
 import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 
 import java.io.StringReader;
 import java.util.HashMap;
Index: solr/src/test/org/apache/solr/analysis/DoubleMetaphoneFilterFactoryTest.java
===================================================================
--- solr/src/test/org/apache/solr/analysis/DoubleMetaphoneFilterFactoryTest.java	(revision 947868)
+++ solr/src/test/org/apache/solr/analysis/DoubleMetaphoneFilterFactoryTest.java	(working copy)
@@ -21,7 +21,7 @@
 import java.util.Map;
 
 import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
 
 public class DoubleMetaphoneFilterFactoryTest extends BaseTokenTestCase {
Index: solr/src/test/org/apache/solr/analysis/TestTrimFilterFactory.java
===================================================================
--- solr/src/test/org/apache/solr/analysis/TestTrimFilterFactory.java	(revision 947868)
+++ solr/src/test/org/apache/solr/analysis/TestTrimFilterFactory.java	(working copy)
@@ -23,9 +23,9 @@
 import java.util.HashMap;
 import java.util.Map;
 
-import org.apache.lucene.analysis.KeywordTokenizer;
 import org.apache.lucene.analysis.Token;
 import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.analysis.core.KeywordTokenizer;
 import org.apache.lucene.analysis.tokenattributes.FlagsAttribute;
 import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
 import org.apache.lucene.analysis.tokenattributes.PayloadAttribute;
Index: solr/src/test/org/apache/solr/analysis/TestIndonesianStemFilterFactory.java
===================================================================
--- solr/src/test/org/apache/solr/analysis/TestIndonesianStemFilterFactory.java	(revision 947868)
+++ solr/src/test/org/apache/solr/analysis/TestIndonesianStemFilterFactory.java	(working copy)
@@ -24,7 +24,7 @@
 
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 
 /**
  * Simple tests to ensure the Indonesian stem filter factory is working.
Index: solr/src/test/org/apache/solr/analysis/CommonGramsFilterFactoryTest.java
===================================================================
--- solr/src/test/org/apache/solr/analysis/CommonGramsFilterFactoryTest.java	(revision 947868)
+++ solr/src/test/org/apache/solr/analysis/CommonGramsFilterFactoryTest.java	(working copy)
@@ -19,7 +19,7 @@
 
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 import org.apache.solr.common.ResourceLoader;
 import org.apache.solr.core.SolrResourceLoader;
 
Index: solr/src/test/org/apache/solr/analysis/DoubleMetaphoneFilterTest.java
===================================================================
--- solr/src/test/org/apache/solr/analysis/DoubleMetaphoneFilterTest.java	(revision 947868)
+++ solr/src/test/org/apache/solr/analysis/DoubleMetaphoneFilterTest.java	(working copy)
@@ -19,7 +19,7 @@
 import java.io.StringReader;
 
 import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 
 public class DoubleMetaphoneFilterTest extends BaseTokenTestCase {
 
Index: solr/src/test/org/apache/solr/analysis/TestCapitalizationFilter.java
===================================================================
--- solr/src/test/org/apache/solr/analysis/TestCapitalizationFilter.java	(revision 947868)
+++ solr/src/test/org/apache/solr/analysis/TestCapitalizationFilter.java	(working copy)
@@ -21,10 +21,10 @@
 import java.util.HashMap;
 import java.util.Map;
 
-import org.apache.lucene.analysis.KeywordTokenizer;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.KeywordTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 
 
 /**
Index: solr/src/test/org/apache/solr/analysis/EnglishPorterFilterFactoryTest.java
===================================================================
--- solr/src/test/org/apache/solr/analysis/EnglishPorterFilterFactoryTest.java	(revision 947868)
+++ solr/src/test/org/apache/solr/analysis/EnglishPorterFilterFactoryTest.java	(working copy)
@@ -18,7 +18,7 @@
 
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 import org.apache.solr.common.ResourceLoader;
 import org.apache.solr.common.util.StrUtils;
 import org.tartarus.snowball.ext.EnglishStemmer;
Index: solr/src/test/org/apache/solr/analysis/TestKeywordMarkerFilterFactory.java
===================================================================
--- solr/src/test/org/apache/solr/analysis/TestKeywordMarkerFilterFactory.java	(revision 947868)
+++ solr/src/test/org/apache/solr/analysis/TestKeywordMarkerFilterFactory.java	(working copy)
@@ -23,10 +23,10 @@
 import java.util.HashMap;
 import java.util.Map;
 
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 import org.apache.lucene.analysis.en.PorterStemFilter;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
 import org.apache.solr.common.ResourceLoader;
 import org.apache.solr.core.SolrResourceLoader;
 
Index: solr/src/test/org/apache/solr/analysis/TestReversedWildcardFilterFactory.java
===================================================================
--- solr/src/test/org/apache/solr/analysis/TestReversedWildcardFilterFactory.java	(revision 947868)
+++ solr/src/test/org/apache/solr/analysis/TestReversedWildcardFilterFactory.java	(working copy)
@@ -25,7 +25,7 @@
 
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 import org.apache.lucene.queryParser.ParseException;
 import org.apache.lucene.search.Query;
 import org.apache.solr.SolrTestCaseJ4;
Index: solr/src/test/org/apache/solr/analysis/TestShingleFilterFactory.java
===================================================================
--- solr/src/test/org/apache/solr/analysis/TestShingleFilterFactory.java	(revision 947868)
+++ solr/src/test/org/apache/solr/analysis/TestShingleFilterFactory.java	(working copy)
@@ -23,7 +23,7 @@
 import java.util.Map;
 
 import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 
 /**
  * Simple tests to ensure the Shingle filter factory works.
Index: solr/src/test/org/apache/solr/analysis/TestCollationKeyFilterFactory.java
===================================================================
--- solr/src/test/org/apache/solr/analysis/TestCollationKeyFilterFactory.java	(revision 947868)
+++ solr/src/test/org/apache/solr/analysis/TestCollationKeyFilterFactory.java	(working copy)
@@ -28,8 +28,8 @@
 import java.util.Locale;
 import java.util.Map;
 
-import org.apache.lucene.analysis.KeywordTokenizer;
 import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.analysis.core.KeywordTokenizer;
 import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
 import org.apache.solr.common.ResourceLoader;
 
Index: solr/src/test/org/apache/solr/analysis/TestBulgarianStemFilterFactory.java
===================================================================
--- solr/src/test/org/apache/solr/analysis/TestBulgarianStemFilterFactory.java	(revision 947868)
+++ solr/src/test/org/apache/solr/analysis/TestBulgarianStemFilterFactory.java	(working copy)
@@ -22,7 +22,7 @@
 
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 
 /**
  * Simple tests to ensure the Bulgarian stem filter factory is working.
Index: solr/src/test/org/apache/solr/analysis/TestTurkishLowerCaseFilterFactory.java
===================================================================
--- solr/src/test/org/apache/solr/analysis/TestTurkishLowerCaseFilterFactory.java	(revision 947868)
+++ solr/src/test/org/apache/solr/analysis/TestTurkishLowerCaseFilterFactory.java	(working copy)
@@ -22,7 +22,7 @@
 
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 
 /**
  * Simple tests to ensure the Turkish lowercase filter factory is working.
Index: solr/src/test/org/apache/solr/analysis/TestDutchStemFilterFactory.java
===================================================================
--- solr/src/test/org/apache/solr/analysis/TestDutchStemFilterFactory.java	(revision 947868)
+++ solr/src/test/org/apache/solr/analysis/TestDutchStemFilterFactory.java	(working copy)
@@ -22,7 +22,7 @@
 
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 
 /**
  * Simple tests to ensure the Dutch stem filter factory is working.
Index: solr/src/test/org/apache/solr/analysis/TestElisionFilterFactory.java
===================================================================
--- solr/src/test/org/apache/solr/analysis/TestElisionFilterFactory.java	(revision 947868)
+++ solr/src/test/org/apache/solr/analysis/TestElisionFilterFactory.java	(working copy)
@@ -24,7 +24,7 @@
 
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 import org.apache.solr.common.ResourceLoader;
 import org.apache.solr.core.SolrResourceLoader;
 
Index: solr/src/test/org/apache/solr/analysis/TestGreekLowerCaseFilterFactory.java
===================================================================
--- solr/src/test/org/apache/solr/analysis/TestGreekLowerCaseFilterFactory.java	(revision 947868)
+++ solr/src/test/org/apache/solr/analysis/TestGreekLowerCaseFilterFactory.java	(working copy)
@@ -22,7 +22,7 @@
 
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 
 /**
  * Simple tests to ensure the Greek lowercase filter factory is working.
Index: solr/src/test/org/apache/solr/analysis/TestReverseStringFilterFactory.java
===================================================================
--- solr/src/test/org/apache/solr/analysis/TestReverseStringFilterFactory.java	(revision 947868)
+++ solr/src/test/org/apache/solr/analysis/TestReverseStringFilterFactory.java	(working copy)
@@ -22,7 +22,7 @@
 
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 
 /**
  * Simple tests to ensure the Reverse string filter factory is working.
Index: solr/src/test/org/apache/solr/analysis/TestStandardFactories.java
===================================================================
--- solr/src/test/org/apache/solr/analysis/TestStandardFactories.java	(revision 947868)
+++ solr/src/test/org/apache/solr/analysis/TestStandardFactories.java	(working copy)
@@ -22,7 +22,7 @@
 
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 
 /**
  * Simple tests to ensure the standard lucene factories are working.
Index: solr/src/test/org/apache/solr/analysis/TestBrazilianStemFilterFactory.java
===================================================================
--- solr/src/test/org/apache/solr/analysis/TestBrazilianStemFilterFactory.java	(revision 947868)
+++ solr/src/test/org/apache/solr/analysis/TestBrazilianStemFilterFactory.java	(working copy)
@@ -22,7 +22,7 @@
 
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 
 /**
  * Simple tests to ensure the Brazilian stem filter factory is working.
Index: solr/src/test/org/apache/solr/analysis/TestDictionaryCompoundWordTokenFilterFactory.java
===================================================================
--- solr/src/test/org/apache/solr/analysis/TestDictionaryCompoundWordTokenFilterFactory.java	(revision 947868)
+++ solr/src/test/org/apache/solr/analysis/TestDictionaryCompoundWordTokenFilterFactory.java	(working copy)
@@ -24,7 +24,7 @@
 
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 import org.apache.solr.common.ResourceLoader;
 import org.apache.solr.core.SolrResourceLoader;
 
Index: solr/src/test/org/apache/solr/analysis/CommonGramsQueryFilterFactoryTest.java
===================================================================
--- solr/src/test/org/apache/solr/analysis/CommonGramsQueryFilterFactoryTest.java	(revision 947868)
+++ solr/src/test/org/apache/solr/analysis/CommonGramsQueryFilterFactoryTest.java	(working copy)
@@ -18,7 +18,7 @@
 
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 import org.apache.solr.common.ResourceLoader;
 import org.apache.solr.core.SolrResourceLoader;
 
Index: solr/src/test/org/apache/solr/analysis/TestDelimitedPayloadTokenFilterFactory.java
===================================================================
--- solr/src/test/org/apache/solr/analysis/TestDelimitedPayloadTokenFilterFactory.java	(revision 947868)
+++ solr/src/test/org/apache/solr/analysis/TestDelimitedPayloadTokenFilterFactory.java	(working copy)
@@ -22,7 +22,7 @@
 import java.util.Map;
 
 import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 import org.apache.lucene.analysis.payloads.DelimitedPayloadTokenFilter;
 import org.apache.lucene.analysis.payloads.FloatEncoder;
 import org.apache.lucene.analysis.payloads.PayloadHelper;
Index: solr/src/test/org/apache/solr/analysis/TestStemmerOverrideFilterFactory.java
===================================================================
--- solr/src/test/org/apache/solr/analysis/TestStemmerOverrideFilterFactory.java	(revision 947868)
+++ solr/src/test/org/apache/solr/analysis/TestStemmerOverrideFilterFactory.java	(working copy)
@@ -23,10 +23,10 @@
 import java.util.HashMap;
 import java.util.Map;
 
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 import org.apache.lucene.analysis.en.PorterStemFilter;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
 import org.apache.solr.common.ResourceLoader;
 import org.apache.solr.core.SolrResourceLoader;
 
Index: solr/src/test/org/apache/solr/analysis/SnowballPorterFilterFactoryTest.java
===================================================================
--- solr/src/test/org/apache/solr/analysis/SnowballPorterFilterFactoryTest.java	(revision 947868)
+++ solr/src/test/org/apache/solr/analysis/SnowballPorterFilterFactoryTest.java	(working copy)
@@ -18,7 +18,7 @@
 
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 import org.apache.solr.common.ResourceLoader;
 import org.apache.solr.common.util.StrUtils;
 import org.apache.solr.core.SolrResourceLoader;
Index: solr/src/test/org/apache/solr/analysis/TestChineseFilterFactory.java
===================================================================
--- solr/src/test/org/apache/solr/analysis/TestChineseFilterFactory.java	(revision 947868)
+++ solr/src/test/org/apache/solr/analysis/TestChineseFilterFactory.java	(working copy)
@@ -22,7 +22,7 @@
 
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 
 /**
  * Simple tests to ensure the Chinese filter factory is working.
Index: solr/src/test/org/apache/solr/analysis/TestPhoneticFilter.java
===================================================================
--- solr/src/test/org/apache/solr/analysis/TestPhoneticFilter.java	(revision 948011)
+++ solr/src/test/org/apache/solr/analysis/TestPhoneticFilter.java	(working copy)
@@ -24,7 +24,7 @@
 import org.apache.commons.codec.language.Metaphone;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 
 
 /**
Index: solr/src/test/org/apache/solr/analysis/TestThaiWordFilterFactory.java
===================================================================
--- solr/src/test/org/apache/solr/analysis/TestThaiWordFilterFactory.java	(revision 947868)
+++ solr/src/test/org/apache/solr/analysis/TestThaiWordFilterFactory.java	(working copy)
@@ -22,7 +22,7 @@
 
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 
 /**
  * Simple tests to ensure the Thai word filter factory is working.
Index: solr/src/test/org/apache/solr/analysis/TestCzechStemFilterFactory.java
===================================================================
--- solr/src/test/org/apache/solr/analysis/TestCzechStemFilterFactory.java	(revision 947868)
+++ solr/src/test/org/apache/solr/analysis/TestCzechStemFilterFactory.java	(working copy)
@@ -22,7 +22,7 @@
 
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 
 /**
  * Simple tests to ensure the Czech stem filter factory is working.
Index: solr/src/test/org/apache/solr/analysis/TestGreekStemFilterFactory.java
===================================================================
--- solr/src/test/org/apache/solr/analysis/TestGreekStemFilterFactory.java	(revision 947868)
+++ solr/src/test/org/apache/solr/analysis/TestGreekStemFilterFactory.java	(working copy)
@@ -5,7 +5,7 @@
 
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 import org.apache.lucene.analysis.el.GreekLowerCaseFilter;
 
 /**
Index: solr/src/test/org/apache/solr/analysis/LengthFilterTest.java
===================================================================
--- solr/src/test/org/apache/solr/analysis/LengthFilterTest.java	(revision 947868)
+++ solr/src/test/org/apache/solr/analysis/LengthFilterTest.java	(working copy)
@@ -22,7 +22,7 @@
 import java.util.Map;
 
 import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 
 public class LengthFilterTest extends BaseTokenTestCase {
 
Index: solr/src/test/org/apache/solr/analysis/TestFrenchStemFilterFactory.java
===================================================================
--- solr/src/test/org/apache/solr/analysis/TestFrenchStemFilterFactory.java	(revision 947868)
+++ solr/src/test/org/apache/solr/analysis/TestFrenchStemFilterFactory.java	(working copy)
@@ -22,7 +22,7 @@
 
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 
 /**
  * Simple tests to ensure the French stem filter factory is working.
Index: solr/src/test/org/apache/solr/analysis/TestNGramFilters.java
===================================================================
--- solr/src/test/org/apache/solr/analysis/TestNGramFilters.java	(revision 947868)
+++ solr/src/test/org/apache/solr/analysis/TestNGramFilters.java	(working copy)
@@ -24,7 +24,7 @@
 
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 
 /**
  * Simple tests to ensure the NGram filter factories are working.
Index: solr/src/test/org/apache/solr/search/TestSort.java
===================================================================
--- solr/src/test/org/apache/solr/search/TestSort.java	(revision 947868)
+++ solr/src/test/org/apache/solr/search/TestSort.java	(working copy)
@@ -17,7 +17,7 @@
 
 package org.apache.solr.search;
 
-import org.apache.lucene.analysis.SimpleAnalyzer;
+import org.apache.lucene.analysis.core.SimpleAnalyzer;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.index.IndexReader;
Index: solr/src/test/org/apache/solr/highlight/HighlighterTest.java
===================================================================
--- solr/src/test/org/apache/solr/highlight/HighlighterTest.java	(revision 947868)
+++ solr/src/test/org/apache/solr/highlight/HighlighterTest.java	(working copy)
@@ -20,7 +20,7 @@
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.Token;
 import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.WhitespaceAnalyzer;
+import org.apache.lucene.analysis.core.WhitespaceAnalyzer;
 import org.apache.solr.SolrTestCaseJ4;
 import org.apache.solr.core.SolrCore;
 import org.apache.solr.request.SolrQueryRequest;
Index: solr/src/test/org/apache/solr/spelling/SimpleQueryConverter.java
===================================================================
--- solr/src/test/org/apache/solr/spelling/SimpleQueryConverter.java	(revision 947868)
+++ solr/src/test/org/apache/solr/spelling/SimpleQueryConverter.java	(working copy)
@@ -17,8 +17,8 @@
 package org.apache.solr.spelling;
 
 import org.apache.lucene.analysis.Token;
-import org.apache.lucene.analysis.WhitespaceAnalyzer;
 import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.analysis.core.WhitespaceAnalyzer;
 import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
 import org.apache.lucene.analysis.tokenattributes.FlagsAttribute;
 import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
Index: solr/src/test/org/apache/solr/spelling/IndexBasedSpellCheckerTest.java
===================================================================
--- solr/src/test/org/apache/solr/spelling/IndexBasedSpellCheckerTest.java	(revision 947868)
+++ solr/src/test/org/apache/solr/spelling/IndexBasedSpellCheckerTest.java	(working copy)
@@ -19,7 +19,7 @@
 import static org.junit.Assert.*;
 
 import org.apache.lucene.analysis.Token;
-import org.apache.lucene.analysis.WhitespaceAnalyzer;
+import org.apache.lucene.analysis.core.WhitespaceAnalyzer;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.index.IndexReader;
Index: solr/src/test/org/apache/solr/spelling/SpellingQueryConverterTest.java
===================================================================
--- solr/src/test/org/apache/solr/spelling/SpellingQueryConverterTest.java	(revision 947868)
+++ solr/src/test/org/apache/solr/spelling/SpellingQueryConverterTest.java	(working copy)
@@ -18,7 +18,7 @@
 package org.apache.solr.spelling;
 
 import org.apache.lucene.analysis.Token;
-import org.apache.lucene.analysis.WhitespaceAnalyzer;
+import org.apache.lucene.analysis.core.WhitespaceAnalyzer;
 import org.apache.solr.common.util.NamedList;
 import static org.junit.Assert.assertTrue;
 import static org.junit.Assert.assertEquals;
Index: solr/src/test/org/apache/solr/handler/DocumentAnalysisRequestHandlerTest.java
===================================================================
--- solr/src/test/org/apache/solr/handler/DocumentAnalysisRequestHandlerTest.java	(revision 947868)
+++ solr/src/test/org/apache/solr/handler/DocumentAnalysisRequestHandlerTest.java	(working copy)
@@ -158,7 +158,7 @@
     NamedList<NamedList<Object>> whitetokResult = documentResult.get("whitetok");
     assertNotNull("an analysis for the 'whitetok' field should be returned", whitetokResult);
     queryResult = whitetokResult.get("query");
-    tokenList = (List<NamedList>) queryResult.get("org.apache.lucene.analysis.WhitespaceTokenizer");
+    tokenList = (List<NamedList>) queryResult.get("org.apache.lucene.analysis.core.WhitespaceTokenizer");
     assertNotNull("Expecting the 'WhitespaceTokenizer' to be applied on the query for the 'whitetok' field", tokenList);
     assertEquals("Query has only one token", 1, tokenList.size());
     assertToken(tokenList.get(0), new TokenInfo("JUMPING", null, "word", 0, 7, 1, null, false));
@@ -182,11 +182,11 @@
     assertNotNull("Expecting the 'StandardFilter' to be applied on the query for the 'text' field", tokenList);
     assertEquals("Query has only one token", 1, tokenList.size());
     assertToken(tokenList.get(0), new TokenInfo("JUMPING", null, "<ALPHANUM>", 0, 7, 1, null, false));
-    tokenList = (List<NamedList>) queryResult.get("org.apache.lucene.analysis.LowerCaseFilter");
+    tokenList = (List<NamedList>) queryResult.get("org.apache.lucene.analysis.core.LowerCaseFilter");
     assertNotNull("Expecting the 'LowerCaseFilter' to be applied on the query for the 'text' field", tokenList);
     assertEquals("Query has only one token", 1, tokenList.size());
     assertToken(tokenList.get(0), new TokenInfo("jumping", null, "<ALPHANUM>", 0, 7, 1, null, false));
-    tokenList = (List<NamedList>) queryResult.get("org.apache.lucene.analysis.StopFilter");
+    tokenList = (List<NamedList>) queryResult.get("org.apache.lucene.analysis.core.StopFilter");
     assertNotNull("Expecting the 'StopFilter' to be applied on the query for the 'text' field", tokenList);
     assertEquals("Query has only one token", 1, tokenList.size());
     assertToken(tokenList.get(0), new TokenInfo("jumping", null, "<ALPHANUM>", 0, 7, 1, null, false));
@@ -215,7 +215,7 @@
     assertToken(tokenList.get(3), new TokenInfo("Over", null, "<ALPHANUM>", 15, 19, 4, null, false));
     assertToken(tokenList.get(4), new TokenInfo("The", null, "<ALPHANUM>", 20, 23, 5, null, false));
     assertToken(tokenList.get(5), new TokenInfo("Dogs", null, "<ALPHANUM>", 24, 28, 6, null, false));
-    tokenList = valueResult.get("org.apache.lucene.analysis.LowerCaseFilter");
+    tokenList = valueResult.get("org.apache.lucene.analysis.core.LowerCaseFilter");
     assertNotNull("Expecting the 'LowerCaseFilter' to be applied on the index for the 'text' field", tokenList);
     assertEquals("Expecting 6 tokens", 6, tokenList.size());
     assertToken(tokenList.get(0), new TokenInfo("the", null, "<ALPHANUM>", 0, 3, 1, null, false));
@@ -224,7 +224,7 @@
     assertToken(tokenList.get(3), new TokenInfo("over", null, "<ALPHANUM>", 15, 19, 4, null, false));
     assertToken(tokenList.get(4), new TokenInfo("the", null, "<ALPHANUM>", 20, 23, 5, null, false));
     assertToken(tokenList.get(5), new TokenInfo("dogs", null, "<ALPHANUM>", 24, 28, 6, null, false));
-    tokenList = valueResult.get("org.apache.lucene.analysis.StopFilter");
+    tokenList = valueResult.get("org.apache.lucene.analysis.core.StopFilter");
     assertNotNull("Expecting the 'StopFilter' to be applied on the index for the 'text' field", tokenList);
     assertEquals("Expecting 4 tokens after stop word removal", 4, tokenList.size());
     assertToken(tokenList.get(0), new TokenInfo("fox", null, "<ALPHANUM>", 4, 7, 1, null, false));
Index: solr/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest.java
===================================================================
--- solr/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest.java	(revision 947868)
+++ solr/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest.java	(working copy)
@@ -17,8 +17,8 @@
 
 package org.apache.solr.handler;
 
-import org.apache.lucene.analysis.KeywordTokenizer;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.KeywordTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 import org.apache.solr.common.params.AnalysisParams;
 import org.apache.solr.common.params.CommonParams;
 import org.apache.solr.common.params.ModifiableSolrParams;
@@ -149,7 +149,7 @@
     assertToken(tokenList.get(7), new TokenInfo("lazy", null, "<ALPHANUM>", 34, 38, 8, null, false));
     assertToken(tokenList.get(8), new TokenInfo("brown", null, "<ALPHANUM>", 39, 44, 9, null, true));
     assertToken(tokenList.get(9), new TokenInfo("dogs", null, "<ALPHANUM>", 45, 49, 10, null, false));
-    tokenList = indexPart.get("org.apache.lucene.analysis.LowerCaseFilter");
+    tokenList = indexPart.get("org.apache.lucene.analysis.core.LowerCaseFilter");
     assertNotNull("Expcting LowerCaseFilter analysis breakdown", tokenList);
     assertEquals(tokenList.size(), 10);
     assertToken(tokenList.get(0), new TokenInfo("the", null, "<ALPHANUM>", 0, 3, 1, null, false));
@@ -162,7 +162,7 @@
     assertToken(tokenList.get(7), new TokenInfo("lazy", null, "<ALPHANUM>", 34, 38, 8, null, false));
     assertToken(tokenList.get(8), new TokenInfo("brown", null, "<ALPHANUM>", 39, 44, 9, null, true));
     assertToken(tokenList.get(9), new TokenInfo("dogs", null, "<ALPHANUM>", 45, 49, 10, null, false));
-    tokenList = indexPart.get("org.apache.lucene.analysis.StopFilter");
+    tokenList = indexPart.get("org.apache.lucene.analysis.core.StopFilter");
     assertNotNull("Expcting StopFilter analysis breakdown", tokenList);
     assertEquals(tokenList.size(), 8);
     assertToken(tokenList.get(0), new TokenInfo("quick", null, "<ALPHANUM>", 4, 9, 1, null, false));
@@ -198,12 +198,12 @@
     assertEquals(2, tokenList.size());
     assertToken(tokenList.get(0), new TokenInfo("fox", null, "<ALPHANUM>", 0, 3, 1, null, false));
     assertToken(tokenList.get(1), new TokenInfo("brown", null, "<ALPHANUM>", 4, 9, 2, null, false));
-    tokenList = queryPart.get("org.apache.lucene.analysis.LowerCaseFilter");
+    tokenList = queryPart.get("org.apache.lucene.analysis.core.LowerCaseFilter");
     assertNotNull("Expcting LowerCaseFilter analysis breakdown", tokenList);
     assertEquals(2, tokenList.size());
     assertToken(tokenList.get(0), new TokenInfo("fox", null, "<ALPHANUM>", 0, 3, 1, null, false));
     assertToken(tokenList.get(1), new TokenInfo("brown", null, "<ALPHANUM>", 4, 9, 2, null, false));
-    tokenList = queryPart.get("org.apache.lucene.analysis.StopFilter");
+    tokenList = queryPart.get("org.apache.lucene.analysis.core.StopFilter");
     assertNotNull("Expcting StopFilter analysis breakdown", tokenList);
     assertEquals(2, tokenList.size());
     assertToken(tokenList.get(0), new TokenInfo("fox", null, "<ALPHANUM>", 0, 3, 1, null, false));
@@ -220,7 +220,7 @@
     indexPart = nameTextType.get("index");
     assertNotNull("expecting an index token analysis for field type 'nametext'", indexPart);
 
-    tokenList = indexPart.get("org.apache.lucene.analysis.WhitespaceTokenizer");
+    tokenList = indexPart.get("org.apache.lucene.analysis.core.WhitespaceTokenizer");
     assertNotNull("Expcting WhitespaceTokenizer analysis breakdown", tokenList);
     assertEquals(10, tokenList.size());
     assertToken(tokenList.get(0), new TokenInfo("the", null, "word", 0, 3, 1, null, false));
@@ -314,7 +314,7 @@
     assertEquals("  whátëvêr  ", indexPart.get("org.apache.lucene.analysis.charfilter.HTMLStripCharFilter"));
     assertEquals("  whatever  ", indexPart.get("org.apache.lucene.analysis.charfilter.MappingCharFilter"));
 
-    List<NamedList> tokenList = (List<NamedList>)indexPart.get("org.apache.lucene.analysis.WhitespaceTokenizer");
+    List<NamedList> tokenList = (List<NamedList>)indexPart.get("org.apache.lucene.analysis.core.WhitespaceTokenizer");
     assertNotNull("Expecting WhitespaceTokenizer analysis breakdown", tokenList);
     assertEquals(tokenList.size(), 1);
     assertToken(tokenList.get(0), new TokenInfo("whatever", null, "word", 12, 20, 1, null, false));
Index: solr/src/java/org/apache/solr/analysis/CommonGramsFilterFactory.java
===================================================================
--- solr/src/java/org/apache/solr/analysis/CommonGramsFilterFactory.java	(revision 947868)
+++ solr/src/java/org/apache/solr/analysis/CommonGramsFilterFactory.java	(working copy)
@@ -20,9 +20,9 @@
 import java.util.Set;
 
 import org.apache.lucene.analysis.CharArraySet;
-import org.apache.lucene.analysis.StopAnalyzer;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.commongrams.CommonGramsFilter;
+import org.apache.lucene.analysis.core.StopAnalyzer;
 import org.apache.solr.common.ResourceLoader;
 import org.apache.solr.util.plugin.ResourceLoaderAware;
 
Index: solr/src/java/org/apache/solr/analysis/LowerCaseTokenizerFactory.java
===================================================================
--- solr/src/java/org/apache/solr/analysis/LowerCaseTokenizerFactory.java	(revision 947868)
+++ solr/src/java/org/apache/solr/analysis/LowerCaseTokenizerFactory.java	(working copy)
@@ -17,7 +17,7 @@
 
 package org.apache.solr.analysis;
 
-import org.apache.lucene.analysis.LowerCaseTokenizer;
+import org.apache.lucene.analysis.core.LowerCaseTokenizer;
 
 import java.io.Reader;
 import java.util.Map;
Index: solr/src/java/org/apache/solr/analysis/BaseTokenStreamFactory.java
===================================================================
--- solr/src/java/org/apache/solr/analysis/BaseTokenStreamFactory.java	(revision 947868)
+++ solr/src/java/org/apache/solr/analysis/BaseTokenStreamFactory.java	(working copy)
@@ -27,7 +27,7 @@
 import java.util.Map;
 
 import org.apache.lucene.analysis.CharArraySet;
-import org.apache.lucene.analysis.StopFilter;
+import org.apache.lucene.analysis.core.StopFilter;
 import org.apache.lucene.util.Version;
 
 
Index: solr/src/java/org/apache/solr/analysis/RussianLowerCaseFilterFactory.java
===================================================================
--- solr/src/java/org/apache/solr/analysis/RussianLowerCaseFilterFactory.java	(revision 947868)
+++ solr/src/java/org/apache/solr/analysis/RussianLowerCaseFilterFactory.java	(working copy)
@@ -19,9 +19,9 @@
 
 import java.util.Map;
 
-import org.apache.lucene.analysis.LowerCaseFilter;
 import org.apache.lucene.analysis.TokenFilter;
 import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.analysis.core.LowerCaseFilter;
 import org.apache.lucene.util.Version;
 import org.apache.solr.common.SolrException;
 import org.apache.solr.common.SolrException.ErrorCode;
Index: solr/src/java/org/apache/solr/analysis/KeywordTokenizerFactory.java
===================================================================
--- solr/src/java/org/apache/solr/analysis/KeywordTokenizerFactory.java	(revision 947868)
+++ solr/src/java/org/apache/solr/analysis/KeywordTokenizerFactory.java	(working copy)
@@ -17,7 +17,7 @@
 
 package org.apache.solr.analysis;
 
-import org.apache.lucene.analysis.KeywordTokenizer;
+import org.apache.lucene.analysis.core.KeywordTokenizer;
 
 import java.io.Reader;
 
Index: solr/src/java/org/apache/solr/analysis/LetterTokenizerFactory.java
===================================================================
--- solr/src/java/org/apache/solr/analysis/LetterTokenizerFactory.java	(revision 947868)
+++ solr/src/java/org/apache/solr/analysis/LetterTokenizerFactory.java	(working copy)
@@ -17,7 +17,7 @@
 
 package org.apache.solr.analysis;
 
-import org.apache.lucene.analysis.LetterTokenizer;
+import org.apache.lucene.analysis.core.LetterTokenizer;
 
 import java.io.Reader;
 import java.util.Map;
Index: solr/src/java/org/apache/solr/analysis/LowerCaseFilterFactory.java
===================================================================
--- solr/src/java/org/apache/solr/analysis/LowerCaseFilterFactory.java	(revision 947868)
+++ solr/src/java/org/apache/solr/analysis/LowerCaseFilterFactory.java	(working copy)
@@ -20,7 +20,7 @@
 import java.util.Map;
 
 import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.LowerCaseFilter;
+import org.apache.lucene.analysis.core.LowerCaseFilter;
 
 /**
  * @version $Id$
Index: solr/src/java/org/apache/solr/analysis/CommonGramsQueryFilterFactory.java
===================================================================
--- solr/src/java/org/apache/solr/analysis/CommonGramsQueryFilterFactory.java	(revision 947868)
+++ solr/src/java/org/apache/solr/analysis/CommonGramsQueryFilterFactory.java	(working copy)
@@ -21,10 +21,10 @@
 import java.util.Set;
 
 import org.apache.lucene.analysis.CharArraySet;
-import org.apache.lucene.analysis.StopAnalyzer;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.commongrams.CommonGramsFilter;
 import org.apache.lucene.analysis.commongrams.CommonGramsQueryFilter;
+import org.apache.lucene.analysis.core.StopAnalyzer;
 import org.apache.solr.common.ResourceLoader;
 import org.apache.solr.util.plugin.ResourceLoaderAware;
 
Index: solr/src/java/org/apache/solr/analysis/WhitespaceTokenizerFactory.java
===================================================================
--- solr/src/java/org/apache/solr/analysis/WhitespaceTokenizerFactory.java	(revision 947868)
+++ solr/src/java/org/apache/solr/analysis/WhitespaceTokenizerFactory.java	(working copy)
@@ -17,7 +17,7 @@
 
 package org.apache.solr.analysis;
 
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 
 import java.io.Reader;
 import java.util.Map;
Index: solr/src/java/org/apache/solr/analysis/StopFilterFactory.java
===================================================================
--- solr/src/java/org/apache/solr/analysis/StopFilterFactory.java	(revision 947868)
+++ solr/src/java/org/apache/solr/analysis/StopFilterFactory.java	(working copy)
@@ -19,10 +19,10 @@
 
 import org.apache.solr.common.ResourceLoader;
 import org.apache.solr.util.plugin.ResourceLoaderAware;
-import org.apache.lucene.analysis.StopFilter;
-import org.apache.lucene.analysis.StopAnalyzer;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.CharArraySet;
+import org.apache.lucene.analysis.core.StopAnalyzer;
+import org.apache.lucene.analysis.core.StopFilter;
 
 import java.util.Map;
 import java.util.Set;
Index: solr/src/java/org/apache/solr/handler/component/SpellCheckComponent.java
===================================================================
--- solr/src/java/org/apache/solr/handler/component/SpellCheckComponent.java	(revision 947868)
+++ solr/src/java/org/apache/solr/handler/component/SpellCheckComponent.java	(working copy)
@@ -32,7 +32,7 @@
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.Token;
 import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.WhitespaceAnalyzer;
+import org.apache.lucene.analysis.core.WhitespaceAnalyzer;
 import org.apache.lucene.analysis.tokenattributes.FlagsAttribute;
 import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
 import org.apache.lucene.analysis.tokenattributes.PayloadAttribute;
Index: solr/src/java/org/apache/solr/spelling/AbstractLuceneSpellChecker.java
===================================================================
--- solr/src/java/org/apache/solr/spelling/AbstractLuceneSpellChecker.java	(revision 947868)
+++ solr/src/java/org/apache/solr/spelling/AbstractLuceneSpellChecker.java	(working copy)
@@ -27,7 +27,7 @@
 import org.slf4j.LoggerFactory;
 
 import org.apache.lucene.analysis.Token;
-import org.apache.lucene.analysis.WhitespaceAnalyzer;
+import org.apache.lucene.analysis.core.WhitespaceAnalyzer;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.search.spell.Dictionary;
Index: modules/analysis/icu/src/test/org/apache/lucene/analysis/icu/TestICUTransformFilter.java
===================================================================
--- modules/analysis/icu/src/test/org/apache/lucene/analysis/icu/TestICUTransformFilter.java	(revision 947868)
+++ modules/analysis/icu/src/test/org/apache/lucene/analysis/icu/TestICUTransformFilter.java	(working copy)
@@ -21,7 +21,7 @@
 import java.io.StringReader;
 
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
-import org.apache.lucene.analysis.KeywordTokenizer;
+import org.apache.lucene.analysis.core.KeywordTokenizer;
 import org.apache.lucene.analysis.TokenStream;
 
 import com.ibm.icu.text.Transliterator;
Index: modules/analysis/icu/src/test/org/apache/lucene/analysis/icu/TestICUFoldingFilter.java
===================================================================
--- modules/analysis/icu/src/test/org/apache/lucene/analysis/icu/TestICUFoldingFilter.java	(revision 947868)
+++ modules/analysis/icu/src/test/org/apache/lucene/analysis/icu/TestICUFoldingFilter.java	(working copy)
@@ -23,7 +23,7 @@
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 
 /**
  * Tests ICUFoldingFilter
Index: modules/analysis/icu/src/test/org/apache/lucene/analysis/icu/TestICUNormalizer2Filter.java
===================================================================
--- modules/analysis/icu/src/test/org/apache/lucene/analysis/icu/TestICUNormalizer2Filter.java	(revision 947868)
+++ modules/analysis/icu/src/test/org/apache/lucene/analysis/icu/TestICUNormalizer2Filter.java	(working copy)
@@ -23,7 +23,7 @@
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 
 import com.ibm.icu.text.Normalizer2;
 
Index: modules/analysis/icu/src/test/org/apache/lucene/analysis/icu/segmentation/TestICUTokenizer.java
===================================================================
--- modules/analysis/icu/src/test/org/apache/lucene/analysis/icu/segmentation/TestICUTokenizer.java	(revision 947868)
+++ modules/analysis/icu/src/test/org/apache/lucene/analysis/icu/segmentation/TestICUTokenizer.java	(working copy)
@@ -23,7 +23,7 @@
 
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
-import org.apache.lucene.analysis.ReusableAnalyzerBase;
+import org.apache.lucene.analysis.util.ReusableAnalyzerBase;
 import org.apache.lucene.analysis.TokenFilter;
 import org.apache.lucene.analysis.Tokenizer;
 import org.apache.lucene.analysis.icu.ICUNormalizer2Filter;
Index: modules/analysis/icu/src/test/org/apache/lucene/collation/TestICUCollationKeyFilter.java
===================================================================
--- modules/analysis/icu/src/test/org/apache/lucene/collation/TestICUCollationKeyFilter.java	(revision 947868)
+++ modules/analysis/icu/src/test/org/apache/lucene/collation/TestICUCollationKeyFilter.java	(working copy)
@@ -21,7 +21,7 @@
 import com.ibm.icu.text.Collator;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.analysis.KeywordTokenizer;
+import org.apache.lucene.analysis.core.KeywordTokenizer;
 
 import java.io.Reader;
 import java.util.Locale;
Index: modules/analysis/icu/src/java/org/apache/lucene/collation/ICUCollationKeyAnalyzer.java
===================================================================
--- modules/analysis/icu/src/java/org/apache/lucene/collation/ICUCollationKeyAnalyzer.java	(revision 947868)
+++ modules/analysis/icu/src/java/org/apache/lucene/collation/ICUCollationKeyAnalyzer.java	(working copy)
@@ -21,7 +21,7 @@
 import com.ibm.icu.text.Collator;
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.KeywordTokenizer;
+import org.apache.lucene.analysis.core.KeywordTokenizer;
 import org.apache.lucene.analysis.Tokenizer;
 
 import java.io.Reader;
Index: modules/analysis/icu/build.xml
===================================================================
--- modules/analysis/icu/build.xml	(revision 947868)
+++ modules/analysis/icu/build.xml	(working copy)
@@ -38,6 +38,30 @@
 
   <import file="../../../lucene/contrib/contrib-build.xml"/>
 
+  <module-uptodate name="analysis/common" jarfile="../build/common/lucene-analyzers-common-${version}.jar"
+    property="analyzers-common.uptodate" classpath.property="analyzers-common.jar"/>
+
+  <path id="classpath">
+    <pathelement path="${analyzers-common.jar}"/>
+    <path refid="base.classpath"/>
+  </path>
+
+  <path id="test.classpath">
+  	<pathelement path="${analyzers-common.jar}"/>
+    <path refid="classpath"/>
+    <pathelement location="../../../lucene/build/classes/test/"/>
+  	<pathelement location="../build/common/classes/test/"/>
+    <path refid="junit-path"/>
+    <pathelement location="${build.dir}/classes/java"/>
+  </path>
+
+  <target name="compile-core" depends="build-analyzers-common, common.compile-core" />
+
+  <target name="build-analyzers-common" unless="analyzers-common.uptodate">
+    <echo>ICU building dependency ${analyzers-common.jar}</echo>
+    <ant antfile="../common/build.xml" target="default" inheritall="false" dir="../common" />
+  </target>
+
   <property name="gennorm2.src.dir" value="src/data/utr30"/>
   <property name="gennorm2.src.files" 
   	value="nfkc.txt nfkc_cf.txt BasicFoldings.txt DiacriticFolding.txt DingbatFolding.txt HanRadicalFolding.txt NativeDigitFolding.txt"/>
Index: modules/analysis/smartcn/src/java/org/apache/lucene/analysis/cn/smart/SmartChineseAnalyzer.java
===================================================================
--- modules/analysis/smartcn/src/java/org/apache/lucene/analysis/cn/smart/SmartChineseAnalyzer.java	(revision 947868)
+++ modules/analysis/smartcn/src/java/org/apache/lucene/analysis/cn/smart/SmartChineseAnalyzer.java	(working copy)
@@ -26,12 +26,12 @@
 
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.en.PorterStemFilter;
-import org.apache.lucene.analysis.StopFilter;
+import org.apache.lucene.analysis.util.WordlistLoader;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.WordlistLoader;
 import org.apache.lucene.analysis.cn.smart.SentenceTokenizer;
 import org.apache.lucene.analysis.cn.smart.WordTokenFilter;
+import org.apache.lucene.analysis.core.StopFilter;
 import org.apache.lucene.util.Version;
 
 /**
Index: modules/analysis/common/src/test/org/apache/lucene/analysis/commongrams/CommonGramsFilterTest.java
===================================================================
--- modules/analysis/common/src/test/org/apache/lucene/analysis/commongrams/CommonGramsFilterTest.java	(revision 947868)
+++ modules/analysis/common/src/test/org/apache/lucene/analysis/commongrams/CommonGramsFilterTest.java	(working copy)
@@ -24,7 +24,7 @@
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.TokenFilter;
 import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
 
 /**
Index: modules/analysis/common/src/test/org/apache/lucene/analysis/charfilter/TestMappingCharFilter.java
===================================================================
--- modules/analysis/common/src/test/org/apache/lucene/analysis/charfilter/TestMappingCharFilter.java	(revision 947868)
+++ modules/analysis/common/src/test/org/apache/lucene/analysis/charfilter/TestMappingCharFilter.java	(working copy)
@@ -23,7 +23,7 @@
 import org.apache.lucene.analysis.CharReader;
 import org.apache.lucene.analysis.CharStream;
 import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 
 public class TestMappingCharFilter extends BaseTokenStreamTestCase {
 
Index: modules/analysis/common/src/test/org/apache/lucene/analysis/compound/TestCompoundWordTokenFilter.java
===================================================================
--- modules/analysis/common/src/test/org/apache/lucene/analysis/compound/TestCompoundWordTokenFilter.java	(revision 947868)
+++ modules/analysis/common/src/test/org/apache/lucene/analysis/compound/TestCompoundWordTokenFilter.java	(working copy)
@@ -25,8 +25,8 @@
 
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
 import org.apache.lucene.analysis.compound.hyphenation.HyphenationTree;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 import org.apache.lucene.analysis.tokenattributes.TermAttribute;
 
 public class TestCompoundWordTokenFilter extends BaseTokenStreamTestCase {
Index: modules/analysis/common/src/test/org/apache/lucene/analysis/bg/TestBulgarianStemmer.java
===================================================================
--- modules/analysis/common/src/test/org/apache/lucene/analysis/bg/TestBulgarianStemmer.java	(revision 947868)
+++ modules/analysis/common/src/test/org/apache/lucene/analysis/bg/TestBulgarianStemmer.java	(working copy)
@@ -22,8 +22,8 @@
 
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.CharArraySet;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 import org.apache.lucene.analysis.miscellaneous.KeywordMarkerFilter;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
 import org.apache.lucene.util.Version;
 
 /**
Index: modules/analysis/common/src/test/org/apache/lucene/analysis/de/TestGermanAnalyzer.java
===================================================================
--- modules/analysis/common/src/test/org/apache/lucene/analysis/de/TestGermanAnalyzer.java	(revision 947868)
+++ modules/analysis/common/src/test/org/apache/lucene/analysis/de/TestGermanAnalyzer.java	(working copy)
@@ -23,8 +23,8 @@
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.CharArraySet;
+import org.apache.lucene.analysis.core.LowerCaseTokenizer;
 import org.apache.lucene.analysis.miscellaneous.KeywordMarkerFilter;
-import org.apache.lucene.analysis.LowerCaseTokenizer;
 import org.apache.lucene.util.Version;
 
 public class TestGermanAnalyzer extends BaseTokenStreamTestCase {
Index: modules/analysis/common/src/test/org/apache/lucene/analysis/de/TestGermanStemFilter.java
===================================================================
--- modules/analysis/common/src/test/org/apache/lucene/analysis/de/TestGermanStemFilter.java	(revision 947868)
+++ modules/analysis/common/src/test/org/apache/lucene/analysis/de/TestGermanStemFilter.java	(working copy)
@@ -24,10 +24,10 @@
 import java.io.StringReader;
 
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
-import org.apache.lucene.analysis.KeywordTokenizer;
-import org.apache.lucene.analysis.LowerCaseFilter;
 import org.apache.lucene.analysis.TokenFilter;
 import org.apache.lucene.analysis.Tokenizer;
+import org.apache.lucene.analysis.core.KeywordTokenizer;
+import org.apache.lucene.analysis.core.LowerCaseFilter;
 
 /**
  * Test the German stemmer. The stemming algorithm is known to work less 
Index: modules/analysis/common/src/test/org/apache/lucene/analysis/core/TestStandardAnalyzer.java
===================================================================
--- modules/analysis/common/src/test/org/apache/lucene/analysis/core/TestStandardAnalyzer.java	(revision 0)
+++ modules/analysis/common/src/test/org/apache/lucene/analysis/core/TestStandardAnalyzer.java	(working copy)
@@ -1,7 +1,20 @@
-package org.apache.lucene.analysis;
+package org.apache.lucene.analysis.core;
 
+import java.io.IOException;
+import java.util.Arrays;
+
+import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.standard.StandardAnalyzer;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.index.IndexWriterConfig;
+import org.apache.lucene.index.Term;
+import org.apache.lucene.index.TermPositions;
 
+import org.apache.lucene.store.RAMDirectory;
 import org.apache.lucene.util.Version;
 
 
@@ -232,4 +245,64 @@
     assertAnalyzesTo(sa, "test\u02C6test", new String[] { "test\u02C6test" });
   }
 
+  /**
+   * Make sure we skip wicked long terms.
+  */
+  public void testWickedLongTerm() throws IOException {
+    RAMDirectory dir = new RAMDirectory();
+    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(
+      TEST_VERSION_CURRENT, new StandardAnalyzer(TEST_VERSION_CURRENT)));
+
+    char[] chars = new char[IndexWriter.MAX_TERM_LENGTH];
+    Arrays.fill(chars, 'x');
+    Document doc = new Document();
+    final String bigTerm = new String(chars);
+
+    // This produces a too-long term:
+    String contents = "abc xyz x" + bigTerm + " another term";
+    doc.add(new Field("content", contents, Field.Store.NO, Field.Index.ANALYZED));
+    writer.addDocument(doc);
+
+    // Make sure we can add another normal document
+    doc = new Document();
+    doc.add(new Field("content", "abc bbb ccc", Field.Store.NO, Field.Index.ANALYZED));
+    writer.addDocument(doc);
+    writer.close();
+
+    IndexReader reader = IndexReader.open(dir, true);
+
+    // Make sure all terms < max size were indexed
+    assertEquals(2, reader.docFreq(new Term("content", "abc")));
+    assertEquals(1, reader.docFreq(new Term("content", "bbb")));
+    assertEquals(1, reader.docFreq(new Term("content", "term")));
+    assertEquals(1, reader.docFreq(new Term("content", "another")));
+
+    // Make sure position is still incremented when
+    // massive term is skipped:
+    TermPositions tps = reader.termPositions(new Term("content", "another"));
+    assertTrue(tps.next());
+    assertEquals(1, tps.freq());
+    assertEquals(3, tps.nextPosition());
+
+    // Make sure the doc that has the massive term is in
+    // the index:
+    assertEquals("document with wicked long term should is not in the index!", 2, reader.numDocs());
+
+    reader.close();
+
+    // Make sure we can add a document with exactly the
+    // maximum length term, and search on that term:
+    doc = new Document();
+    doc.add(new Field("content", bigTerm, Field.Store.NO, Field.Index.ANALYZED));
+    StandardAnalyzer sa = new StandardAnalyzer(TEST_VERSION_CURRENT);
+    sa.setMaxTokenLength(100000);
+    writer  = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, sa));
+    writer.addDocument(doc);
+    writer.close();
+    reader = IndexReader.open(dir, true);
+    assertEquals(1, reader.docFreq(new Term("content", bigTerm)));
+    reader.close();
+
+    dir.close();
+  }
 }
Index: modules/analysis/common/src/test/org/apache/lucene/analysis/core/TestAnalyzers.java
===================================================================
--- modules/analysis/common/src/test/org/apache/lucene/analysis/core/TestAnalyzers.java	(revision 0)
+++ modules/analysis/common/src/test/org/apache/lucene/analysis/core/TestAnalyzers.java	(working copy)
@@ -1,4 +1,4 @@
-package org.apache.lucene.analysis;
+package org.apache.lucene.analysis.core;
 
 /**
  * Licensed to the Apache Software Foundation (ASF) under one or more
@@ -21,11 +21,21 @@
 import java.io.StringReader;
 import java.io.Reader;
 
+import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
+import org.apache.lucene.analysis.core.LowerCaseTokenizer;
+import org.apache.lucene.analysis.TokenFilter;
+import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.analysis.core.LowerCaseFilter;
+import org.apache.lucene.analysis.core.SimpleAnalyzer;
+import org.apache.lucene.analysis.core.StopAnalyzer;
+import org.apache.lucene.analysis.core.WhitespaceAnalyzer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 import org.apache.lucene.analysis.standard.StandardTokenizer;
-import org.apache.lucene.analysis.standard.StandardAnalyzer;
 import org.apache.lucene.analysis.tokenattributes.PayloadAttribute;
 import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
 import org.apache.lucene.index.Payload;
+import org.apache.lucene.util.Version;
 
 public class TestAnalyzers extends BaseTokenStreamTestCase {
 
@@ -214,6 +224,38 @@
         new String [] { "abac\uDC16adaba" });
   }
   
+  public void testLowerCaseTokenizer() throws IOException {
+    StringReader reader = new StringReader("Tokenizer \ud801\udc1ctest");
+    LowerCaseTokenizer tokenizer = new LowerCaseTokenizer(TEST_VERSION_CURRENT,
+        reader);
+    assertTokenStreamContents(tokenizer, new String[] { "tokenizer",
+        "\ud801\udc44test" });
+  }
+
+  @Deprecated
+  public void testLowerCaseTokenizerBWCompat() throws IOException {
+    StringReader reader = new StringReader("Tokenizer \ud801\udc1ctest");
+    LowerCaseTokenizer tokenizer = new LowerCaseTokenizer(Version.LUCENE_30,
+        reader);
+    assertTokenStreamContents(tokenizer, new String[] { "tokenizer", "test" });
+  }
+  
+  public void testWhitespaceTokenizer() throws IOException {
+    StringReader reader = new StringReader("Tokenizer \ud801\udc1ctest");
+    WhitespaceTokenizer tokenizer = new WhitespaceTokenizer(TEST_VERSION_CURRENT,
+        reader);
+    assertTokenStreamContents(tokenizer, new String[] { "Tokenizer",
+        "\ud801\udc1ctest" });
+  }
+
+  @Deprecated
+  public void testWhitespaceTokenizerBWCompat() throws IOException {
+    StringReader reader = new StringReader("Tokenizer \ud801\udc1ctest");
+    WhitespaceTokenizer tokenizer = new WhitespaceTokenizer(Version.LUCENE_30,
+        reader);
+    assertTokenStreamContents(tokenizer, new String[] { "Tokenizer",
+        "\ud801\udc1ctest" });
+  }
 }
 
 final class PayloadSetter extends TokenFilter {
Index: modules/analysis/common/src/test/org/apache/lucene/analysis/core/TestKeywordAnalyzer.java
===================================================================
--- modules/analysis/common/src/test/org/apache/lucene/analysis/core/TestKeywordAnalyzer.java	(revision 0)
+++ modules/analysis/common/src/test/org/apache/lucene/analysis/core/TestKeywordAnalyzer.java	(working copy)
@@ -1,4 +1,4 @@
-package org.apache.lucene.analysis;
+package org.apache.lucene.analysis.core;
 
 /**
  * Licensed to the Apache Software Foundation (ASF) under one or more
@@ -19,6 +19,9 @@
 
 import java.io.StringReader;
 
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
+import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.analysis.core.KeywordAnalyzer;
 import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
@@ -27,10 +30,7 @@
 import org.apache.lucene.index.IndexWriterConfig;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.index.TermDocs;
-import org.apache.lucene.queryParser.QueryParser;
 import org.apache.lucene.search.IndexSearcher;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.ScoreDoc;
 import org.apache.lucene.store.RAMDirectory;
 
 public class TestKeywordAnalyzer extends BaseTokenStreamTestCase {
@@ -43,8 +43,7 @@
     super.setUp();
     directory = new RAMDirectory();
     IndexWriter writer = new IndexWriter(directory, new IndexWriterConfig(
-        TEST_VERSION_CURRENT, new SimpleAnalyzer(
-        TEST_VERSION_CURRENT)));
+        TEST_VERSION_CURRENT, new SimpleAnalyzer(TEST_VERSION_CURRENT)));
 
     Document doc = new Document();
     doc.add(new Field("partnum", "Q36", Field.Store.YES, Field.Index.NOT_ANALYZED));
Index: modules/analysis/common/src/test/org/apache/lucene/analysis/core/TestStopAnalyzer.java
===================================================================
--- modules/analysis/common/src/test/org/apache/lucene/analysis/core/TestStopAnalyzer.java	(revision 0)
+++ modules/analysis/common/src/test/org/apache/lucene/analysis/core/TestStopAnalyzer.java	(working copy)
@@ -1,4 +1,4 @@
-package org.apache.lucene.analysis;
+package org.apache.lucene.analysis.core;
 
 /**
  * Licensed to the Apache Software Foundation (ASF) under one or more
@@ -17,6 +17,9 @@
  * limitations under the License.
  */
 
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
+import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.analysis.core.StopAnalyzer;
 import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;
 import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
 import org.apache.lucene.util.Version;
Index: modules/analysis/common/src/test/org/apache/lucene/analysis/core/TestStopFilter.java
===================================================================
--- modules/analysis/common/src/test/org/apache/lucene/analysis/core/TestStopFilter.java	(revision 0)
+++ modules/analysis/common/src/test/org/apache/lucene/analysis/core/TestStopFilter.java	(working copy)
@@ -1,4 +1,4 @@
-package org.apache.lucene.analysis;
+package org.apache.lucene.analysis.core;
 
 /**
  * Copyright 2005 The Apache Software Foundation
@@ -16,6 +16,10 @@
  * limitations under the License.
  */
 
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
+import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.analysis.core.StopFilter;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;
 import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
 import org.apache.lucene.util.English;
Index: modules/analysis/common/src/test/org/apache/lucene/analysis/hi/TestHindiStemmer.java
===================================================================
--- modules/analysis/common/src/test/org/apache/lucene/analysis/hi/TestHindiStemmer.java	(revision 947868)
+++ modules/analysis/common/src/test/org/apache/lucene/analysis/hi/TestHindiStemmer.java	(working copy)
@@ -23,7 +23,7 @@
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.TokenFilter;
 import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 
 /**
  * Test HindiStemmer
Index: modules/analysis/common/src/test/org/apache/lucene/analysis/hi/TestHindiNormalizer.java
===================================================================
--- modules/analysis/common/src/test/org/apache/lucene/analysis/hi/TestHindiNormalizer.java	(revision 947868)
+++ modules/analysis/common/src/test/org/apache/lucene/analysis/hi/TestHindiNormalizer.java	(working copy)
@@ -23,7 +23,7 @@
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.TokenFilter;
 import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 
 /**
  * Test HindiNormalizer
Index: modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestPerFieldAnalzyerWrapper.java
===================================================================
--- modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestPerFieldAnalzyerWrapper.java	(revision 947868)
+++ modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestPerFieldAnalzyerWrapper.java	(working copy)
@@ -3,6 +3,8 @@
 import java.io.StringReader;
 
 import org.apache.lucene.analysis.*;
+import org.apache.lucene.analysis.core.SimpleAnalyzer;
+import org.apache.lucene.analysis.core.WhitespaceAnalyzer;
 import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
 
 /**
Index: modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestASCIIFoldingFilter.java
===================================================================
--- modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestASCIIFoldingFilter.java	(revision 947868)
+++ modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestASCIIFoldingFilter.java	(working copy)
@@ -19,7 +19,7 @@
 
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
 import java.io.StringReader;
 import java.util.List;
Index: modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestHyphenatedWordsFilter.java
===================================================================
--- modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestHyphenatedWordsFilter.java	(revision 947868)
+++ modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestHyphenatedWordsFilter.java	(working copy)
@@ -21,7 +21,7 @@
 
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 
 /**
  * HyphenatedWordsFilter test
Index: modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestWordDelimiterFilter.java
===================================================================
--- modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestWordDelimiterFilter.java	(revision 947868)
+++ modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestWordDelimiterFilter.java	(working copy)
@@ -20,12 +20,12 @@
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.CharArraySet;
-import org.apache.lucene.analysis.KeywordTokenizer;
-import org.apache.lucene.analysis.StopFilter;
 import org.apache.lucene.analysis.TokenFilter;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Token;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.KeywordTokenizer;
+import org.apache.lucene.analysis.core.StopFilter;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 import org.apache.lucene.analysis.miscellaneous.SingleTokenTokenStream;
 import org.apache.lucene.analysis.standard.StandardAnalyzer;
 import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;
Index: modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestStemmerOverrideFilter.java
===================================================================
--- modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestStemmerOverrideFilter.java	(revision 947868)
+++ modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestStemmerOverrideFilter.java	(working copy)
@@ -6,7 +6,7 @@
 import java.util.Map;
 
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
-import org.apache.lucene.analysis.KeywordTokenizer;
+import org.apache.lucene.analysis.core.KeywordTokenizer;
 import org.apache.lucene.analysis.en.PorterStemFilter;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
Index: modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/PatternAnalyzerTest.java
===================================================================
--- modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/PatternAnalyzerTest.java	(revision 947868)
+++ modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/PatternAnalyzerTest.java	(working copy)
@@ -22,8 +22,8 @@
 import java.util.regex.Pattern;
 
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
-import org.apache.lucene.analysis.StopAnalyzer;
 import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.analysis.core.StopAnalyzer;
 
 /**
  * Verifies the behavior of PatternAnalyzer.
Index: modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestPrefixAwareTokenFilter.java
===================================================================
--- modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestPrefixAwareTokenFilter.java	(revision 947868)
+++ modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestPrefixAwareTokenFilter.java	(working copy)
@@ -19,7 +19,7 @@
 
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.Token;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 
 import java.io.IOException;
 import java.io.StringReader;
Index: modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestKeepWordFilter.java
===================================================================
--- modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestKeepWordFilter.java	(revision 947868)
+++ modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestKeepWordFilter.java	(working copy)
@@ -23,7 +23,7 @@
 
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 
 /** Test {@link KeepWordFilter} */
 public class TestKeepWordFilter extends BaseTokenStreamTestCase {
Index: modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestLengthFilter.java
===================================================================
--- modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestLengthFilter.java	(revision 947868)
+++ modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestLengthFilter.java	(working copy)
@@ -18,6 +18,7 @@
  */
 
 import org.apache.lucene.analysis.*;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
 import java.io.StringReader;
 
Index: modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestISOLatin1AccentFilter.java
===================================================================
--- modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestISOLatin1AccentFilter.java	(revision 947868)
+++ modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestISOLatin1AccentFilter.java	(working copy)
@@ -19,7 +19,7 @@
 
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
 import java.io.StringReader;
 
Index: modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestPrefixAndSuffixAwareTokenFilter.java
===================================================================
--- modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestPrefixAndSuffixAwareTokenFilter.java	(revision 947868)
+++ modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestPrefixAndSuffixAwareTokenFilter.java	(working copy)
@@ -19,7 +19,7 @@
 
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.Token;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 
 import java.io.IOException;
 import java.io.StringReader;
Index: modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestKeywordMarkerFilter.java
===================================================================
--- modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestKeywordMarkerFilter.java	(revision 947868)
+++ modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestKeywordMarkerFilter.java	(working copy)
@@ -10,7 +10,7 @@
 import org.apache.lucene.analysis.CharArraySet;
 import org.apache.lucene.analysis.TokenFilter;
 import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 import org.apache.lucene.analysis.tokenattributes.KeywordAttribute;
 import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
 import org.junit.Test;
Index: modules/analysis/common/src/test/org/apache/lucene/analysis/br/TestBrazilianStemmer.java
===================================================================
--- modules/analysis/common/src/test/org/apache/lucene/analysis/br/TestBrazilianStemmer.java	(revision 947868)
+++ modules/analysis/common/src/test/org/apache/lucene/analysis/br/TestBrazilianStemmer.java	(working copy)
@@ -23,8 +23,8 @@
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.CharArraySet;
+import org.apache.lucene.analysis.core.LowerCaseTokenizer;
 import org.apache.lucene.analysis.miscellaneous.KeywordMarkerFilter;
-import org.apache.lucene.analysis.LowerCaseTokenizer;
 
 /**
  * Test the Brazilian Stem Filter, which only modifies the term text.
Index: modules/analysis/common/src/test/org/apache/lucene/analysis/ngram/EdgeNGramTokenFilterTest.java
===================================================================
--- modules/analysis/common/src/test/org/apache/lucene/analysis/ngram/EdgeNGramTokenFilterTest.java	(revision 947868)
+++ modules/analysis/common/src/test/org/apache/lucene/analysis/ngram/EdgeNGramTokenFilterTest.java	(working copy)
@@ -18,8 +18,8 @@
  */
 
 import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 
 import java.io.StringReader;
 
Index: modules/analysis/common/src/test/org/apache/lucene/analysis/ngram/NGramTokenFilterTest.java
===================================================================
--- modules/analysis/common/src/test/org/apache/lucene/analysis/ngram/NGramTokenFilterTest.java	(revision 947868)
+++ modules/analysis/common/src/test/org/apache/lucene/analysis/ngram/NGramTokenFilterTest.java	(working copy)
@@ -18,8 +18,8 @@
  */
 
 import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 
 import java.io.StringReader;
 
Index: modules/analysis/common/src/test/org/apache/lucene/analysis/query/QueryAutoStopWordAnalyzerTest.java
===================================================================
--- modules/analysis/common/src/test/org/apache/lucene/analysis/query/QueryAutoStopWordAnalyzerTest.java	(revision 947868)
+++ modules/analysis/common/src/test/org/apache/lucene/analysis/query/QueryAutoStopWordAnalyzerTest.java	(working copy)
@@ -22,10 +22,10 @@
 
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
-import org.apache.lucene.analysis.LetterTokenizer;
 import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.WhitespaceAnalyzer;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.LetterTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceAnalyzer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 import org.apache.lucene.analysis.tokenattributes.TermAttribute;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
Index: modules/analysis/common/src/test/org/apache/lucene/analysis/reverse/TestReverseStringFilter.java
===================================================================
--- modules/analysis/common/src/test/org/apache/lucene/analysis/reverse/TestReverseStringFilter.java	(revision 947868)
+++ modules/analysis/common/src/test/org/apache/lucene/analysis/reverse/TestReverseStringFilter.java	(working copy)
@@ -20,7 +20,7 @@
 import java.io.StringReader;
 
 import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 import org.apache.lucene.analysis.tokenattributes.TermAttribute;
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.util.Version;
Index: modules/analysis/common/src/test/org/apache/lucene/analysis/payloads/DelimitedPayloadTokenFilterTest.java
===================================================================
--- modules/analysis/common/src/test/org/apache/lucene/analysis/payloads/DelimitedPayloadTokenFilterTest.java	(revision 947868)
+++ modules/analysis/common/src/test/org/apache/lucene/analysis/payloads/DelimitedPayloadTokenFilterTest.java	(working copy)
@@ -17,7 +17,7 @@
  */
 
 import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 import org.apache.lucene.analysis.tokenattributes.PayloadAttribute;
 import org.apache.lucene.analysis.tokenattributes.TermAttribute;
 import org.apache.lucene.index.Payload;
Index: modules/analysis/common/src/test/org/apache/lucene/analysis/payloads/NumericPayloadTokenFilterTest.java
===================================================================
--- modules/analysis/common/src/test/org/apache/lucene/analysis/payloads/NumericPayloadTokenFilterTest.java	(revision 947868)
+++ modules/analysis/common/src/test/org/apache/lucene/analysis/payloads/NumericPayloadTokenFilterTest.java	(working copy)
@@ -19,7 +19,7 @@
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.TokenFilter;
 import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 import org.apache.lucene.analysis.tokenattributes.PayloadAttribute;
 import org.apache.lucene.analysis.tokenattributes.TermAttribute;
 import org.apache.lucene.analysis.tokenattributes.TypeAttribute;
Index: modules/analysis/common/src/test/org/apache/lucene/analysis/payloads/TypeAsPayloadTokenFilterTest.java
===================================================================
--- modules/analysis/common/src/test/org/apache/lucene/analysis/payloads/TypeAsPayloadTokenFilterTest.java	(revision 947868)
+++ modules/analysis/common/src/test/org/apache/lucene/analysis/payloads/TypeAsPayloadTokenFilterTest.java	(working copy)
@@ -19,7 +19,7 @@
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.TokenFilter;
 import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 import org.apache.lucene.analysis.tokenattributes.PayloadAttribute;
 import org.apache.lucene.analysis.tokenattributes.TermAttribute;
 import org.apache.lucene.analysis.tokenattributes.TypeAttribute;
Index: modules/analysis/common/src/test/org/apache/lucene/analysis/payloads/TokenOffsetPayloadTokenFilterTest.java
===================================================================
--- modules/analysis/common/src/test/org/apache/lucene/analysis/payloads/TokenOffsetPayloadTokenFilterTest.java	(revision 947868)
+++ modules/analysis/common/src/test/org/apache/lucene/analysis/payloads/TokenOffsetPayloadTokenFilterTest.java	(working copy)
@@ -17,7 +17,7 @@
  */
 
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
 import org.apache.lucene.analysis.tokenattributes.PayloadAttribute;
 import org.apache.lucene.index.Payload;
Index: modules/analysis/common/src/test/org/apache/lucene/analysis/pattern/TestPatternReplaceFilter.java
===================================================================
--- modules/analysis/common/src/test/org/apache/lucene/analysis/pattern/TestPatternReplaceFilter.java	(revision 947868)
+++ modules/analysis/common/src/test/org/apache/lucene/analysis/pattern/TestPatternReplaceFilter.java	(working copy)
@@ -19,7 +19,7 @@
 
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 
 import java.io.StringReader;
 import java.util.regex.Pattern;
Index: modules/analysis/common/src/test/org/apache/lucene/analysis/pattern/TestPatternReplaceCharFilter.java
===================================================================
--- modules/analysis/common/src/test/org/apache/lucene/analysis/pattern/TestPatternReplaceCharFilter.java	(revision 947868)
+++ modules/analysis/common/src/test/org/apache/lucene/analysis/pattern/TestPatternReplaceCharFilter.java	(working copy)
@@ -25,7 +25,7 @@
 import org.apache.lucene.analysis.CharReader;
 import org.apache.lucene.analysis.CharStream;
 import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 
 /**
  * Tests {@link PatternReplaceCharFilter}
Index: modules/analysis/common/src/test/org/apache/lucene/analysis/snowball/TestSnowballVocab.java
===================================================================
--- modules/analysis/common/src/test/org/apache/lucene/analysis/snowball/TestSnowballVocab.java	(revision 947868)
+++ modules/analysis/common/src/test/org/apache/lucene/analysis/snowball/TestSnowballVocab.java	(working copy)
@@ -25,9 +25,9 @@
 import java.util.zip.ZipFile;
 
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
-import org.apache.lucene.analysis.KeywordTokenizer;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
+import org.apache.lucene.analysis.core.KeywordTokenizer;
 
 /**
  * Test the snowball filters against the snowball data tests
Index: modules/analysis/common/src/test/org/apache/lucene/analysis/tr/TestTurkishLowerCaseFilter.java
===================================================================
--- modules/analysis/common/src/test/org/apache/lucene/analysis/tr/TestTurkishLowerCaseFilter.java	(revision 947868)
+++ modules/analysis/common/src/test/org/apache/lucene/analysis/tr/TestTurkishLowerCaseFilter.java	(working copy)
@@ -21,7 +21,7 @@
 
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 
 /**
  * Test the Turkish lowercase filter.
Index: modules/analysis/common/src/test/org/apache/lucene/analysis/sinks/TokenTypeSinkTokenizerTest.java
===================================================================
--- modules/analysis/common/src/test/org/apache/lucene/analysis/sinks/TokenTypeSinkTokenizerTest.java	(revision 947868)
+++ modules/analysis/common/src/test/org/apache/lucene/analysis/sinks/TokenTypeSinkTokenizerTest.java	(working copy)
@@ -22,7 +22,7 @@
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.TokenFilter;
 import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 import org.apache.lucene.analysis.tokenattributes.TermAttribute;
 import org.apache.lucene.analysis.tokenattributes.TypeAttribute;
 
Index: modules/analysis/common/src/test/org/apache/lucene/analysis/sinks/DateRecognizerSinkTokenizerTest.java
===================================================================
--- modules/analysis/common/src/test/org/apache/lucene/analysis/sinks/DateRecognizerSinkTokenizerTest.java	(revision 947868)
+++ modules/analysis/common/src/test/org/apache/lucene/analysis/sinks/DateRecognizerSinkTokenizerTest.java	(working copy)
@@ -22,7 +22,7 @@
 import java.util.Locale;
 
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 
 public class DateRecognizerSinkTokenizerTest extends BaseTokenStreamTestCase {
 
Index: modules/analysis/common/src/test/org/apache/lucene/analysis/sinks/TestTeeSinkTokenFilter.java
===================================================================
--- modules/analysis/common/src/test/org/apache/lucene/analysis/sinks/TestTeeSinkTokenFilter.java	(revision 947868)
+++ modules/analysis/common/src/test/org/apache/lucene/analysis/sinks/TestTeeSinkTokenFilter.java	(working copy)
@@ -19,11 +19,11 @@
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.CachingTokenFilter;
-import org.apache.lucene.analysis.LowerCaseFilter;
 import org.apache.lucene.analysis.TokenFilter;
 import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.WhitespaceAnalyzer;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.LowerCaseFilter;
+import org.apache.lucene.analysis.core.WhitespaceAnalyzer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 import org.apache.lucene.analysis.standard.StandardFilter;
 import org.apache.lucene.analysis.standard.StandardTokenizer;
 import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;
Index: modules/analysis/common/src/test/org/apache/lucene/analysis/sinks/TokenRangeSinkTokenizerTest.java
===================================================================
--- modules/analysis/common/src/test/org/apache/lucene/analysis/sinks/TokenRangeSinkTokenizerTest.java	(revision 947868)
+++ modules/analysis/common/src/test/org/apache/lucene/analysis/sinks/TokenRangeSinkTokenizerTest.java	(working copy)
@@ -20,7 +20,7 @@
 import java.io.StringReader;
 
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 
 public class TokenRangeSinkTokenizerTest extends BaseTokenStreamTestCase {
 
Index: modules/analysis/common/src/test/org/apache/lucene/analysis/shingle/ShingleFilterTest.java
===================================================================
--- modules/analysis/common/src/test/org/apache/lucene/analysis/shingle/ShingleFilterTest.java	(revision 947868)
+++ modules/analysis/common/src/test/org/apache/lucene/analysis/shingle/ShingleFilterTest.java	(working copy)
@@ -24,7 +24,7 @@
 import org.apache.lucene.analysis.Token;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 import org.apache.lucene.analysis.tokenattributes.*;
 
 public class ShingleFilterTest extends BaseTokenStreamTestCase {
Index: modules/analysis/common/src/test/org/apache/lucene/analysis/shingle/ShingleAnalyzerWrapperTest.java
===================================================================
--- modules/analysis/common/src/test/org/apache/lucene/analysis/shingle/ShingleAnalyzerWrapperTest.java	(revision 947868)
+++ modules/analysis/common/src/test/org/apache/lucene/analysis/shingle/ShingleAnalyzerWrapperTest.java	(working copy)
@@ -22,10 +22,10 @@
 
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
-import org.apache.lucene.analysis.LetterTokenizer;
 import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.WhitespaceAnalyzer;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.LetterTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceAnalyzer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;
 import org.apache.lucene.analysis.tokenattributes.TermAttribute;
 import org.apache.lucene.document.Document;
Index: modules/analysis/common/src/test/org/apache/lucene/analysis/shingle/TestShingleMatrixFilter.java
===================================================================
--- modules/analysis/common/src/test/org/apache/lucene/analysis/shingle/TestShingleMatrixFilter.java	(revision 947868)
+++ modules/analysis/common/src/test/org/apache/lucene/analysis/shingle/TestShingleMatrixFilter.java	(working copy)
@@ -24,6 +24,7 @@
 import java.util.LinkedList;
 
 import org.apache.lucene.analysis.*;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 import org.apache.lucene.analysis.miscellaneous.EmptyTokenStream;
 import org.apache.lucene.analysis.miscellaneous.PrefixAndSuffixAwareTokenFilter;
 import org.apache.lucene.analysis.miscellaneous.SingleTokenTokenStream;
Index: modules/analysis/common/src/test/org/apache/lucene/analysis/id/TestIndonesianStemmer.java
===================================================================
--- modules/analysis/common/src/test/org/apache/lucene/analysis/id/TestIndonesianStemmer.java	(revision 947868)
+++ modules/analysis/common/src/test/org/apache/lucene/analysis/id/TestIndonesianStemmer.java	(working copy)
@@ -22,9 +22,9 @@
 
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
-import org.apache.lucene.analysis.KeywordTokenizer;
-import org.apache.lucene.analysis.ReusableAnalyzerBase;
 import org.apache.lucene.analysis.Tokenizer;
+import org.apache.lucene.analysis.core.KeywordTokenizer;
+import org.apache.lucene.analysis.util.ReusableAnalyzerBase;
 
 /**
  * Tests {@link IndonesianStemmer}
Index: modules/analysis/common/src/test/org/apache/lucene/analysis/cn/TestChineseTokenizer.java
===================================================================
--- modules/analysis/common/src/test/org/apache/lucene/analysis/cn/TestChineseTokenizer.java	(revision 947868)
+++ modules/analysis/common/src/test/org/apache/lucene/analysis/cn/TestChineseTokenizer.java	(working copy)
@@ -24,7 +24,7 @@
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
 import org.apache.lucene.util.Version;
 
Index: modules/analysis/common/src/test/org/apache/lucene/analysis/en/TestPorterStemFilter.java
===================================================================
--- modules/analysis/common/src/test/org/apache/lucene/analysis/en/TestPorterStemFilter.java	(revision 947868)
+++ modules/analysis/common/src/test/org/apache/lucene/analysis/en/TestPorterStemFilter.java	(working copy)
@@ -26,11 +26,11 @@
 
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.CharArraySet;
+import org.apache.lucene.analysis.core.KeywordTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 import org.apache.lucene.analysis.miscellaneous.KeywordMarkerFilter;
-import org.apache.lucene.analysis.KeywordTokenizer;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
 
 /**
  * Test the PorterStemFilter with Martin Porter's test data.
Index: modules/analysis/common/src/test/org/apache/lucene/analysis/in/TestIndicNormalizer.java
===================================================================
--- modules/analysis/common/src/test/org/apache/lucene/analysis/in/TestIndicNormalizer.java	(revision 947868)
+++ modules/analysis/common/src/test/org/apache/lucene/analysis/in/TestIndicNormalizer.java	(working copy)
@@ -23,7 +23,7 @@
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.TokenFilter;
 import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 
 /**
  * Test IndicNormalizer
Index: modules/analysis/common/src/test/org/apache/lucene/analysis/cz/TestCzechStemmer.java
===================================================================
--- modules/analysis/common/src/test/org/apache/lucene/analysis/cz/TestCzechStemmer.java	(revision 947868)
+++ modules/analysis/common/src/test/org/apache/lucene/analysis/cz/TestCzechStemmer.java	(working copy)
@@ -22,8 +22,8 @@
 
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.CharArraySet;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 import org.apache.lucene.analysis.miscellaneous.KeywordMarkerFilter;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
 
 /**
  * Test the Czech Stemmer.
Index: modules/analysis/common/src/test/org/apache/lucene/analysis/synonym/TestSynonymFilter.java
===================================================================
--- modules/analysis/common/src/test/org/apache/lucene/analysis/synonym/TestSynonymFilter.java	(revision 947868)
+++ modules/analysis/common/src/test/org/apache/lucene/analysis/synonym/TestSynonymFilter.java	(working copy)
@@ -21,7 +21,7 @@
 import org.apache.lucene.analysis.Token;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 import org.apache.lucene.analysis.tokenattributes.FlagsAttribute;
 import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
 import org.apache.lucene.analysis.tokenattributes.PayloadAttribute;
Index: modules/analysis/common/src/test/org/apache/lucene/analysis/util/TestWordlistLoader.java
===================================================================
--- modules/analysis/common/src/test/org/apache/lucene/analysis/util/TestWordlistLoader.java	(revision 0)
+++ modules/analysis/common/src/test/org/apache/lucene/analysis/util/TestWordlistLoader.java	(working copy)
@@ -1,4 +1,4 @@
-package org.apache.lucene.index;
+package org.apache.lucene.analysis.util;
 
 /**
  * Licensed to the Apache Software Foundation (ASF) under one or more
@@ -25,7 +25,7 @@
 
 import org.apache.lucene.util.LuceneTestCase;
 
-import org.apache.lucene.analysis.WordlistLoader;
+import org.apache.lucene.analysis.util.WordlistLoader;
 
 public class TestWordlistLoader extends LuceneTestCase {
 
Index: modules/analysis/common/src/test/org/apache/lucene/collation/TestCollationKeyFilter.java
===================================================================
--- modules/analysis/common/src/test/org/apache/lucene/collation/TestCollationKeyFilter.java	(revision 947868)
+++ modules/analysis/common/src/test/org/apache/lucene/collation/TestCollationKeyFilter.java	(working copy)
@@ -20,7 +20,7 @@
 
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.analysis.KeywordTokenizer;
+import org.apache.lucene.analysis.core.KeywordTokenizer;
 
 import java.text.Collator;
 import java.util.Locale;
Index: modules/analysis/common/src/java/org/apache/lucene/analysis/da/DanishAnalyzer.java
===================================================================
--- modules/analysis/common/src/java/org/apache/lucene/analysis/da/DanishAnalyzer.java	(revision 947868)
+++ modules/analysis/common/src/java/org/apache/lucene/analysis/da/DanishAnalyzer.java	(working copy)
@@ -23,16 +23,16 @@
 
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.CharArraySet;
+import org.apache.lucene.analysis.core.LowerCaseFilter;
+import org.apache.lucene.analysis.core.StopFilter;
 import org.apache.lucene.analysis.miscellaneous.KeywordMarkerFilter;
-import org.apache.lucene.analysis.LowerCaseFilter;
-import org.apache.lucene.analysis.StopFilter;
-import org.apache.lucene.analysis.StopwordAnalyzerBase;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.WordlistLoader;
 import org.apache.lucene.analysis.snowball.SnowballFilter;
 import org.apache.lucene.analysis.standard.StandardFilter;
 import org.apache.lucene.analysis.standard.StandardTokenizer;
+import org.apache.lucene.analysis.util.StopwordAnalyzerBase;
+import org.apache.lucene.analysis.util.WordlistLoader;
 import org.apache.lucene.util.Version;
 import org.tartarus.snowball.ext.DanishStemmer;
 
@@ -106,11 +106,11 @@
 
   /**
    * Creates a
-   * {@link org.apache.lucene.analysis.ReusableAnalyzerBase.TokenStreamComponents}
+   * {@link org.apache.lucene.analysis.util.ReusableAnalyzerBase.TokenStreamComponents}
    * which tokenizes all the text in the provided {@link Reader}.
    * 
    * @return A
-   *         {@link org.apache.lucene.analysis.ReusableAnalyzerBase.TokenStreamComponents}
+   *         {@link org.apache.lucene.analysis.util.ReusableAnalyzerBase.TokenStreamComponents}
    *         built from an {@link StandardTokenizer} filtered with
    *         {@link StandardFilter}, {@link LowerCaseFilter}, {@link StopFilter}
    *         , {@link KeywordMarkerFilter} if a stem exclusion set is
Index: modules/analysis/common/src/java/org/apache/lucene/analysis/fa/PersianAnalyzer.java
===================================================================
--- modules/analysis/common/src/java/org/apache/lucene/analysis/fa/PersianAnalyzer.java	(revision 947868)
+++ modules/analysis/common/src/java/org/apache/lucene/analysis/fa/PersianAnalyzer.java	(working copy)
@@ -24,14 +24,14 @@
 import java.util.Set;
 
 import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.analysis.LowerCaseFilter;
-import org.apache.lucene.analysis.StopFilter;
-import org.apache.lucene.analysis.StopwordAnalyzerBase;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.WordlistLoader;
 import org.apache.lucene.analysis.ar.ArabicLetterTokenizer;
 import org.apache.lucene.analysis.ar.ArabicNormalizationFilter;
+import org.apache.lucene.analysis.core.LowerCaseFilter;
+import org.apache.lucene.analysis.core.StopFilter;
+import org.apache.lucene.analysis.util.StopwordAnalyzerBase;
+import org.apache.lucene.analysis.util.WordlistLoader;
 import org.apache.lucene.util.Version;
 
 /**
@@ -136,10 +136,10 @@
 
   /**
    * Creates
-   * {@link org.apache.lucene.analysis.ReusableAnalyzerBase.TokenStreamComponents}
+   * {@link org.apache.lucene.analysis.util.ReusableAnalyzerBase.TokenStreamComponents}
    * used to tokenize all the text in the provided {@link Reader}.
    * 
-   * @return {@link org.apache.lucene.analysis.ReusableAnalyzerBase.TokenStreamComponents}
+   * @return {@link org.apache.lucene.analysis.util.ReusableAnalyzerBase.TokenStreamComponents}
    *         built from a {@link ArabicLetterTokenizer} filtered with
    *         {@link LowerCaseFilter}, {@link ArabicNormalizationFilter},
    *         {@link PersianNormalizationFilter} and Persian Stop words
Index: modules/analysis/common/src/java/org/apache/lucene/analysis/bg/BulgarianAnalyzer.java
===================================================================
--- modules/analysis/common/src/java/org/apache/lucene/analysis/bg/BulgarianAnalyzer.java	(revision 947868)
+++ modules/analysis/common/src/java/org/apache/lucene/analysis/bg/BulgarianAnalyzer.java	(working copy)
@@ -23,16 +23,16 @@
 import java.util.Set;
 
 import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.analysis.LowerCaseFilter;
 import org.apache.lucene.analysis.CharArraySet;
+import org.apache.lucene.analysis.core.LowerCaseFilter;
+import org.apache.lucene.analysis.core.StopFilter;
 import org.apache.lucene.analysis.miscellaneous.KeywordMarkerFilter;
-import org.apache.lucene.analysis.StopFilter;
-import org.apache.lucene.analysis.StopwordAnalyzerBase;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.WordlistLoader;
 import org.apache.lucene.analysis.standard.StandardFilter;
 import org.apache.lucene.analysis.standard.StandardTokenizer;
+import org.apache.lucene.analysis.util.StopwordAnalyzerBase;
+import org.apache.lucene.analysis.util.WordlistLoader;
 import org.apache.lucene.util.Version;
 
 /**
@@ -119,11 +119,11 @@
 
   /**
    * Creates a
-   * {@link org.apache.lucene.analysis.ReusableAnalyzerBase.TokenStreamComponents}
+   * {@link org.apache.lucene.analysis.util.ReusableAnalyzerBase.TokenStreamComponents}
    * which tokenizes all the text in the provided {@link Reader}.
    * 
    * @return A
-   *         {@link org.apache.lucene.analysis.ReusableAnalyzerBase.TokenStreamComponents}
+   *         {@link org.apache.lucene.analysis.util.ReusableAnalyzerBase.TokenStreamComponents}
    *         built from an {@link StandardTokenizer} filtered with
    *         {@link StandardFilter}, {@link LowerCaseFilter}, {@link StopFilter}
    *         , {@link KeywordMarkerFilter} if a stem exclusion set is
Index: modules/analysis/common/src/java/org/apache/lucene/analysis/de/GermanAnalyzer.java
===================================================================
--- modules/analysis/common/src/java/org/apache/lucene/analysis/de/GermanAnalyzer.java	(revision 947868)
+++ modules/analysis/common/src/java/org/apache/lucene/analysis/de/GermanAnalyzer.java	(working copy)
@@ -28,17 +28,17 @@
 
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.CharArraySet;
-import org.apache.lucene.analysis.LowerCaseFilter;
+import org.apache.lucene.analysis.core.LowerCaseFilter;
+import org.apache.lucene.analysis.core.StopFilter;
 import org.apache.lucene.analysis.miscellaneous.KeywordMarkerFilter;
-import org.apache.lucene.analysis.StopFilter;
-import org.apache.lucene.analysis.StopwordAnalyzerBase;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.WordlistLoader;
 import org.apache.lucene.analysis.snowball.SnowballFilter;
 import org.apache.lucene.analysis.standard.StandardAnalyzer;
 import org.apache.lucene.analysis.standard.StandardFilter;
 import org.apache.lucene.analysis.standard.StandardTokenizer;
+import org.apache.lucene.analysis.util.StopwordAnalyzerBase;
+import org.apache.lucene.analysis.util.WordlistLoader;
 import org.apache.lucene.util.Version;
 import org.tartarus.snowball.ext.German2Stemmer;
 
@@ -224,10 +224,10 @@
 
   /**
    * Creates
-   * {@link org.apache.lucene.analysis.ReusableAnalyzerBase.TokenStreamComponents}
+   * {@link org.apache.lucene.analysis.util.ReusableAnalyzerBase.TokenStreamComponents}
    * used to tokenize all the text in the provided {@link Reader}.
    * 
-   * @return {@link org.apache.lucene.analysis.ReusableAnalyzerBase.TokenStreamComponents}
+   * @return {@link org.apache.lucene.analysis.util.ReusableAnalyzerBase.TokenStreamComponents}
    *         built from a {@link StandardTokenizer} filtered with
    *         {@link StandardFilter}, {@link LowerCaseFilter}, {@link StopFilter}
    *         , {@link KeywordMarkerFilter} if a stem exclusion set is
Index: modules/analysis/common/src/java/org/apache/lucene/analysis/sv/SwedishAnalyzer.java
===================================================================
--- modules/analysis/common/src/java/org/apache/lucene/analysis/sv/SwedishAnalyzer.java	(revision 947868)
+++ modules/analysis/common/src/java/org/apache/lucene/analysis/sv/SwedishAnalyzer.java	(working copy)
@@ -23,16 +23,16 @@
 
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.CharArraySet;
+import org.apache.lucene.analysis.core.LowerCaseFilter;
+import org.apache.lucene.analysis.core.StopFilter;
 import org.apache.lucene.analysis.miscellaneous.KeywordMarkerFilter;
-import org.apache.lucene.analysis.LowerCaseFilter;
-import org.apache.lucene.analysis.StopFilter;
-import org.apache.lucene.analysis.StopwordAnalyzerBase;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.WordlistLoader;
 import org.apache.lucene.analysis.snowball.SnowballFilter;
 import org.apache.lucene.analysis.standard.StandardFilter;
 import org.apache.lucene.analysis.standard.StandardTokenizer;
+import org.apache.lucene.analysis.util.StopwordAnalyzerBase;
+import org.apache.lucene.analysis.util.WordlistLoader;
 import org.apache.lucene.util.Version;
 import org.tartarus.snowball.ext.SwedishStemmer;
 
@@ -106,11 +106,11 @@
 
   /**
    * Creates a
-   * {@link org.apache.lucene.analysis.ReusableAnalyzerBase.TokenStreamComponents}
+   * {@link org.apache.lucene.analysis.util.ReusableAnalyzerBase.TokenStreamComponents}
    * which tokenizes all the text in the provided {@link Reader}.
    * 
    * @return A
-   *         {@link org.apache.lucene.analysis.ReusableAnalyzerBase.TokenStreamComponents}
+   *         {@link org.apache.lucene.analysis.util.ReusableAnalyzerBase.TokenStreamComponents}
    *         built from an {@link StandardTokenizer} filtered with
    *         {@link StandardFilter}, {@link LowerCaseFilter}, {@link StopFilter}
    *         , {@link KeywordMarkerFilter} if a stem exclusion set is
Index: modules/analysis/common/src/java/org/apache/lucene/analysis/core/SimpleAnalyzer.java
===================================================================
--- modules/analysis/common/src/java/org/apache/lucene/analysis/core/SimpleAnalyzer.java	(revision 0)
+++ modules/analysis/common/src/java/org/apache/lucene/analysis/core/SimpleAnalyzer.java	(working copy)
@@ -1,4 +1,4 @@
-package org.apache.lucene.analysis;
+package org.apache.lucene.analysis.core;
 
 /**
  * Licensed to the Apache Software Foundation (ASF) under one or more
@@ -19,6 +19,10 @@
 
 import java.io.Reader;
 
+import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.analysis.CharTokenizer;
+import org.apache.lucene.analysis.util.ReusableAnalyzerBase;
+import org.apache.lucene.analysis.util.ReusableAnalyzerBase.TokenStreamComponents;
 import org.apache.lucene.util.Version;
 
 /** An {@link Analyzer} that filters {@link LetterTokenizer} 
Index: modules/analysis/common/src/java/org/apache/lucene/analysis/core/LowerCaseTokenizer.java
===================================================================
--- modules/analysis/common/src/java/org/apache/lucene/analysis/core/LowerCaseTokenizer.java	(revision 0)
+++ modules/analysis/common/src/java/org/apache/lucene/analysis/core/LowerCaseTokenizer.java	(working copy)
@@ -1,4 +1,4 @@
-package org.apache.lucene.analysis;
+package org.apache.lucene.analysis.core;
 
 /**
  * Licensed to the Apache Software Foundation (ASF) under one or more
@@ -19,6 +19,8 @@
 
 import java.io.Reader;
 
+import org.apache.lucene.analysis.CharTokenizer;
+import org.apache.lucene.analysis.Tokenizer;
 import org.apache.lucene.util.AttributeSource;
 import org.apache.lucene.util.Version;
 
Index: modules/analysis/common/src/java/org/apache/lucene/analysis/core/KeywordTokenizer.java
===================================================================
--- modules/analysis/common/src/java/org/apache/lucene/analysis/core/KeywordTokenizer.java	(revision 0)
+++ modules/analysis/common/src/java/org/apache/lucene/analysis/core/KeywordTokenizer.java	(working copy)
@@ -1,4 +1,4 @@
-package org.apache.lucene.analysis;
+package org.apache.lucene.analysis.core;
 
 /**
  * Licensed to the Apache Software Foundation (ASF) under one or more
@@ -20,6 +20,7 @@
 import java.io.IOException;
 import java.io.Reader;
 
+import org.apache.lucene.analysis.Tokenizer;
 import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
 import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
 import org.apache.lucene.util.AttributeSource;
Index: modules/analysis/common/src/java/org/apache/lucene/analysis/core/WhitespaceAnalyzer.java
===================================================================
--- modules/analysis/common/src/java/org/apache/lucene/analysis/core/WhitespaceAnalyzer.java	(revision 0)
+++ modules/analysis/common/src/java/org/apache/lucene/analysis/core/WhitespaceAnalyzer.java	(working copy)
@@ -1,4 +1,4 @@
-package org.apache.lucene.analysis;
+package org.apache.lucene.analysis.core;
 
 /**
  * Licensed to the Apache Software Foundation (ASF) under one or more
@@ -19,6 +19,9 @@
 
 import java.io.Reader;
 
+import org.apache.lucene.analysis.CharTokenizer;
+import org.apache.lucene.analysis.util.ReusableAnalyzerBase;
+import org.apache.lucene.analysis.util.ReusableAnalyzerBase.TokenStreamComponents;
 import org.apache.lucene.util.Version;
 
 /**
Index: modules/analysis/common/src/java/org/apache/lucene/analysis/core/WhitespaceTokenizer.java
===================================================================
--- modules/analysis/common/src/java/org/apache/lucene/analysis/core/WhitespaceTokenizer.java	(revision 0)
+++ modules/analysis/common/src/java/org/apache/lucene/analysis/core/WhitespaceTokenizer.java	(working copy)
@@ -1,4 +1,4 @@
-package org.apache.lucene.analysis;
+package org.apache.lucene.analysis.core;
 
 /**
  * Licensed to the Apache Software Foundation (ASF) under one or more
@@ -19,6 +19,8 @@
 
 import java.io.Reader;
 
+import org.apache.lucene.analysis.CharTokenizer;
+import org.apache.lucene.analysis.Tokenizer;
 import org.apache.lucene.util.AttributeSource;
 import org.apache.lucene.util.Version;
 
Index: modules/analysis/common/src/java/org/apache/lucene/analysis/core/LowerCaseFilter.java
===================================================================
--- modules/analysis/common/src/java/org/apache/lucene/analysis/core/LowerCaseFilter.java	(revision 0)
+++ modules/analysis/common/src/java/org/apache/lucene/analysis/core/LowerCaseFilter.java	(working copy)
@@ -1,4 +1,4 @@
-package org.apache.lucene.analysis;
+package org.apache.lucene.analysis.core;
 
 /**
  * Licensed to the Apache Software Foundation (ASF) under one or more
@@ -19,6 +19,8 @@
 
 import java.io.IOException;
 
+import org.apache.lucene.analysis.TokenFilter;
+import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
 import org.apache.lucene.util.CharacterUtils;
 import org.apache.lucene.util.Version;
Index: modules/analysis/common/src/java/org/apache/lucene/analysis/core/LetterTokenizer.java
===================================================================
--- modules/analysis/common/src/java/org/apache/lucene/analysis/core/LetterTokenizer.java	(revision 0)
+++ modules/analysis/common/src/java/org/apache/lucene/analysis/core/LetterTokenizer.java	(working copy)
@@ -1,4 +1,4 @@
-package org.apache.lucene.analysis;
+package org.apache.lucene.analysis.core;
 
 /**
  * Licensed to the Apache Software Foundation (ASF) under one or more
@@ -19,6 +19,8 @@
 
 import java.io.Reader;
 
+import org.apache.lucene.analysis.CharTokenizer;
+import org.apache.lucene.analysis.Tokenizer;
 import org.apache.lucene.util.AttributeSource;
 import org.apache.lucene.util.Version;
 
Index: modules/analysis/common/src/java/org/apache/lucene/analysis/core/KeywordAnalyzer.java
===================================================================
--- modules/analysis/common/src/java/org/apache/lucene/analysis/core/KeywordAnalyzer.java	(revision 0)
+++ modules/analysis/common/src/java/org/apache/lucene/analysis/core/KeywordAnalyzer.java	(working copy)
@@ -1,4 +1,4 @@
-package org.apache.lucene.analysis;
+package org.apache.lucene.analysis.core;
 
 /**
  * Licensed to the Apache Software Foundation (ASF) under one or more
@@ -19,6 +19,8 @@
 
 import java.io.Reader;
 
+import org.apache.lucene.analysis.util.ReusableAnalyzerBase;
+
 /**
  * "Tokenizes" the entire stream as a single token. This is useful
  * for data like zip codes, ids, and some product names.
Index: modules/analysis/common/src/java/org/apache/lucene/analysis/core/StopAnalyzer.java
===================================================================
--- modules/analysis/common/src/java/org/apache/lucene/analysis/core/StopAnalyzer.java	(revision 0)
+++ modules/analysis/common/src/java/org/apache/lucene/analysis/core/StopAnalyzer.java	(working copy)
@@ -1,4 +1,4 @@
-package org.apache.lucene.analysis;
+package org.apache.lucene.analysis.core;
 
 /**
  * Licensed to the Apache Software Foundation (ASF) under one or more
@@ -24,6 +24,12 @@
 import java.util.Set;
 import java.util.List;
 
+import org.apache.lucene.analysis.CharArraySet;
+import org.apache.lucene.analysis.Tokenizer;
+import org.apache.lucene.analysis.util.ReusableAnalyzerBase;
+import org.apache.lucene.analysis.util.StopwordAnalyzerBase;
+import org.apache.lucene.analysis.util.WordlistLoader;
+import org.apache.lucene.analysis.util.ReusableAnalyzerBase.TokenStreamComponents;
 import org.apache.lucene.util.Version;
 
 /** Filters {@link LetterTokenizer} with {@link LowerCaseFilter} and {@link StopFilter}.
@@ -91,10 +97,10 @@
 
   /**
    * Creates
-   * {@link org.apache.lucene.analysis.ReusableAnalyzerBase.TokenStreamComponents}
+   * {@link org.apache.lucene.analysis.util.ReusableAnalyzerBase.TokenStreamComponents}
    * used to tokenize all the text in the provided {@link Reader}.
    * 
-   * @return {@link org.apache.lucene.analysis.ReusableAnalyzerBase.TokenStreamComponents}
+   * @return {@link org.apache.lucene.analysis.util.ReusableAnalyzerBase.TokenStreamComponents}
    *         built from a {@link LowerCaseTokenizer} filtered with
    *         {@link StopFilter}
    */
Index: modules/analysis/common/src/java/org/apache/lucene/analysis/core/StopFilter.java
===================================================================
--- modules/analysis/common/src/java/org/apache/lucene/analysis/core/StopFilter.java	(revision 0)
+++ modules/analysis/common/src/java/org/apache/lucene/analysis/core/StopFilter.java	(working copy)
@@ -1,4 +1,4 @@
-package org.apache.lucene.analysis;
+package org.apache.lucene.analysis.core;
 
 /**
  * Licensed to the Apache Software Foundation (ASF) under one or more
@@ -22,6 +22,9 @@
 import java.util.Set;
 import java.util.List;
 
+import org.apache.lucene.analysis.CharArraySet;
+import org.apache.lucene.analysis.TokenFilter;
+import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;
 import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
 import org.apache.lucene.queryParser.QueryParser; // for javadoc
Index: modules/analysis/common/src/java/org/apache/lucene/analysis/fi/FinnishAnalyzer.java
===================================================================
--- modules/analysis/common/src/java/org/apache/lucene/analysis/fi/FinnishAnalyzer.java	(revision 947868)
+++ modules/analysis/common/src/java/org/apache/lucene/analysis/fi/FinnishAnalyzer.java	(working copy)
@@ -23,16 +23,16 @@
 
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.CharArraySet;
+import org.apache.lucene.analysis.core.LowerCaseFilter;
+import org.apache.lucene.analysis.core.StopFilter;
 import org.apache.lucene.analysis.miscellaneous.KeywordMarkerFilter;
-import org.apache.lucene.analysis.LowerCaseFilter;
-import org.apache.lucene.analysis.StopFilter;
-import org.apache.lucene.analysis.StopwordAnalyzerBase;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.WordlistLoader;
 import org.apache.lucene.analysis.snowball.SnowballFilter;
 import org.apache.lucene.analysis.standard.StandardFilter;
 import org.apache.lucene.analysis.standard.StandardTokenizer;
+import org.apache.lucene.analysis.util.StopwordAnalyzerBase;
+import org.apache.lucene.analysis.util.WordlistLoader;
 import org.apache.lucene.util.Version;
 import org.tartarus.snowball.ext.FinnishStemmer;
 
@@ -106,11 +106,11 @@
 
   /**
    * Creates a
-   * {@link org.apache.lucene.analysis.ReusableAnalyzerBase.TokenStreamComponents}
+   * {@link org.apache.lucene.analysis.util.ReusableAnalyzerBase.TokenStreamComponents}
    * which tokenizes all the text in the provided {@link Reader}.
    * 
    * @return A
-   *         {@link org.apache.lucene.analysis.ReusableAnalyzerBase.TokenStreamComponents}
+   *         {@link org.apache.lucene.analysis.util.ReusableAnalyzerBase.TokenStreamComponents}
    *         built from an {@link StandardTokenizer} filtered with
    *         {@link StandardFilter}, {@link LowerCaseFilter}, {@link StopFilter}
    *         , {@link KeywordMarkerFilter} if a stem exclusion set is
Index: modules/analysis/common/src/java/org/apache/lucene/analysis/hi/HindiAnalyzer.java
===================================================================
--- modules/analysis/common/src/java/org/apache/lucene/analysis/hi/HindiAnalyzer.java	(revision 947868)
+++ modules/analysis/common/src/java/org/apache/lucene/analysis/hi/HindiAnalyzer.java	(working copy)
@@ -21,13 +21,13 @@
 import java.io.Reader;
 import java.util.Set;
 
-import org.apache.lucene.analysis.LowerCaseFilter;
 import org.apache.lucene.analysis.CharArraySet;
 import org.apache.lucene.analysis.miscellaneous.KeywordMarkerFilter;
-import org.apache.lucene.analysis.StopFilter;
-import org.apache.lucene.analysis.StopwordAnalyzerBase;
+import org.apache.lucene.analysis.util.StopwordAnalyzerBase;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
+import org.apache.lucene.analysis.core.LowerCaseFilter;
+import org.apache.lucene.analysis.core.StopFilter;
 import org.apache.lucene.analysis.in.IndicNormalizationFilter;
 import org.apache.lucene.analysis.in.IndicTokenizer;
 import org.apache.lucene.util.Version;
@@ -106,10 +106,10 @@
 
   /**
    * Creates
-   * {@link org.apache.lucene.analysis.ReusableAnalyzerBase.TokenStreamComponents}
+   * {@link org.apache.lucene.analysis.util.ReusableAnalyzerBase.TokenStreamComponents}
    * used to tokenize all the text in the provided {@link Reader}.
    * 
-   * @return {@link org.apache.lucene.analysis.ReusableAnalyzerBase.TokenStreamComponents}
+   * @return {@link org.apache.lucene.analysis.util.ReusableAnalyzerBase.TokenStreamComponents}
    *         built from a {@link IndicTokenizer} filtered with
    *         {@link LowerCaseFilter}, {@link IndicNormalizationFilter},
    *         {@link HindiNormalizationFilter}, {@link KeywordMarkerFilter}
Index: modules/analysis/common/src/java/org/apache/lucene/analysis/standard/StandardAnalyzer.java
===================================================================
--- modules/analysis/common/src/java/org/apache/lucene/analysis/standard/StandardAnalyzer.java	(revision 947868)
+++ modules/analysis/common/src/java/org/apache/lucene/analysis/standard/StandardAnalyzer.java	(working copy)
@@ -18,6 +18,11 @@
  */
 
 import org.apache.lucene.analysis.*;
+import org.apache.lucene.analysis.core.LowerCaseFilter;
+import org.apache.lucene.analysis.core.StopAnalyzer;
+import org.apache.lucene.analysis.core.StopFilter;
+import org.apache.lucene.analysis.util.StopwordAnalyzerBase;
+import org.apache.lucene.analysis.util.WordlistLoader;
 import org.apache.lucene.util.Version;
 
 import java.io.File;
Index: modules/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/PatternAnalyzer.java
===================================================================
--- modules/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/PatternAnalyzer.java	(revision 947868)
+++ modules/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/PatternAnalyzer.java	(working copy)
@@ -28,9 +28,9 @@
 
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.CharArraySet;
-import org.apache.lucene.analysis.StopAnalyzer;
-import org.apache.lucene.analysis.StopFilter;
 import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.analysis.core.StopAnalyzer;
+import org.apache.lucene.analysis.core.StopFilter;
 import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
 import org.apache.lucene.analysis.tokenattributes.TermAttribute;
 import org.apache.lucene.util.Version;
@@ -40,10 +40,10 @@
  * {@link java.io.Reader}, that can flexibly separate text into terms via a regular expression {@link Pattern}
  * (with behaviour identical to {@link String#split(String)}),
  * and that combines the functionality of
- * {@link org.apache.lucene.analysis.LetterTokenizer},
- * {@link org.apache.lucene.analysis.LowerCaseTokenizer},
- * {@link org.apache.lucene.analysis.WhitespaceTokenizer},
- * {@link org.apache.lucene.analysis.StopFilter} into a single efficient
+ * {@link org.apache.lucene.analysis.core.LetterTokenizer},
+ * {@link org.apache.lucene.analysis.core.LowerCaseTokenizer},
+ * {@link org.apache.lucene.analysis.core.WhitespaceTokenizer},
+ * {@link org.apache.lucene.analysis.core.StopFilter} into a single efficient
  * multi-purpose class.
  * <p>
  * If you are unsure how exactly a regular expression should look like, consider 
@@ -157,7 +157,7 @@
    *            given stop set (after previously having applied toLowerCase()
    *            if applicable). For example, created via
    *            {@link StopFilter#makeStopSet(Version, String[])}and/or
-   *            {@link org.apache.lucene.analysis.WordlistLoader}as in
+   *            {@link org.apache.lucene.analysis.util.WordlistLoader}as in
    *            <code>WordlistLoader.getWordSet(new File("samples/fulltext/stopwords.txt")</code>
    *            or <a href="http://www.unine.ch/info/clef/">other stop words
    *            lists </a>.
Index: modules/analysis/common/src/java/org/apache/lucene/analysis/br/BrazilianAnalyzer.java
===================================================================
--- modules/analysis/common/src/java/org/apache/lucene/analysis/br/BrazilianAnalyzer.java	(revision 947868)
+++ modules/analysis/common/src/java/org/apache/lucene/analysis/br/BrazilianAnalyzer.java	(working copy)
@@ -28,16 +28,16 @@
 
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.CharArraySet;
-import org.apache.lucene.analysis.LowerCaseFilter;
+import org.apache.lucene.analysis.core.LowerCaseFilter;
+import org.apache.lucene.analysis.core.StopFilter;
 import org.apache.lucene.analysis.miscellaneous.KeywordMarkerFilter;
-import org.apache.lucene.analysis.StopFilter;
-import org.apache.lucene.analysis.StopwordAnalyzerBase;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.WordlistLoader;
 import org.apache.lucene.analysis.standard.StandardAnalyzer;
 import org.apache.lucene.analysis.standard.StandardFilter;
 import org.apache.lucene.analysis.standard.StandardTokenizer;
+import org.apache.lucene.analysis.util.StopwordAnalyzerBase;
+import org.apache.lucene.analysis.util.WordlistLoader;
 import org.apache.lucene.util.Version;
 
 /**
@@ -193,10 +193,10 @@
 
   /**
    * Creates
-   * {@link org.apache.lucene.analysis.ReusableAnalyzerBase.TokenStreamComponents}
+   * {@link org.apache.lucene.analysis.util.ReusableAnalyzerBase.TokenStreamComponents}
    * used to tokenize all the text in the provided {@link Reader}.
    * 
-   * @return {@link org.apache.lucene.analysis.ReusableAnalyzerBase.TokenStreamComponents}
+   * @return {@link org.apache.lucene.analysis.util.ReusableAnalyzerBase.TokenStreamComponents}
    *         built from a {@link StandardTokenizer} filtered with
    *         {@link LowerCaseFilter}, {@link StandardFilter}, {@link StopFilter}
    *         , and {@link BrazilianStemFilter}.
Index: modules/analysis/common/src/java/org/apache/lucene/analysis/query/QueryAutoStopWordAnalyzer.java
===================================================================
--- modules/analysis/common/src/java/org/apache/lucene/analysis/query/QueryAutoStopWordAnalyzer.java	(revision 947868)
+++ modules/analysis/common/src/java/org/apache/lucene/analysis/query/QueryAutoStopWordAnalyzer.java	(working copy)
@@ -21,7 +21,7 @@
 import org.apache.lucene.index.TermEnum;
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.StopFilter;
+import org.apache.lucene.analysis.core.StopFilter;
 import org.apache.lucene.util.StringHelper;
 import org.apache.lucene.util.Version;
 
Index: modules/analysis/common/src/java/org/apache/lucene/analysis/fr/FrenchAnalyzer.java
===================================================================
--- modules/analysis/common/src/java/org/apache/lucene/analysis/fr/FrenchAnalyzer.java	(revision 947868)
+++ modules/analysis/common/src/java/org/apache/lucene/analysis/fr/FrenchAnalyzer.java	(working copy)
@@ -19,17 +19,17 @@
 
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.CharArraySet;
-import org.apache.lucene.analysis.LowerCaseFilter;
+import org.apache.lucene.analysis.core.LowerCaseFilter;
+import org.apache.lucene.analysis.core.StopFilter;
 import org.apache.lucene.analysis.miscellaneous.KeywordMarkerFilter;
-import org.apache.lucene.analysis.StopFilter;
-import org.apache.lucene.analysis.StopwordAnalyzerBase;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.WordlistLoader;
 import org.apache.lucene.analysis.snowball.SnowballFilter;
 import org.apache.lucene.analysis.standard.StandardFilter;
 import org.apache.lucene.analysis.standard.StandardTokenizer;
 import org.apache.lucene.analysis.standard.StandardAnalyzer;  // for javadoc
+import org.apache.lucene.analysis.util.StopwordAnalyzerBase;
+import org.apache.lucene.analysis.util.WordlistLoader;
 import org.apache.lucene.util.Version;
 
 import java.io.File;
@@ -225,10 +225,10 @@
 
   /**
    * Creates
-   * {@link org.apache.lucene.analysis.ReusableAnalyzerBase.TokenStreamComponents}
+   * {@link org.apache.lucene.analysis.util.ReusableAnalyzerBase.TokenStreamComponents}
    * used to tokenize all the text in the provided {@link Reader}.
    * 
-   * @return {@link org.apache.lucene.analysis.ReusableAnalyzerBase.TokenStreamComponents}
+   * @return {@link org.apache.lucene.analysis.util.ReusableAnalyzerBase.TokenStreamComponents}
    *         built from a {@link StandardTokenizer} filtered with
    *         {@link StandardFilter}, {@link ElisionFilter},
    *         {@link LowerCaseFilter}, {@link StopFilter},
Index: modules/analysis/common/src/java/org/apache/lucene/analysis/nl/DutchAnalyzer.java
===================================================================
--- modules/analysis/common/src/java/org/apache/lucene/analysis/nl/DutchAnalyzer.java	(revision 947868)
+++ modules/analysis/common/src/java/org/apache/lucene/analysis/nl/DutchAnalyzer.java	(working copy)
@@ -19,18 +19,18 @@
 
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.CharArraySet;
+import org.apache.lucene.analysis.core.LowerCaseFilter;
+import org.apache.lucene.analysis.core.StopFilter;
 import org.apache.lucene.analysis.miscellaneous.KeywordMarkerFilter;
-import org.apache.lucene.analysis.LowerCaseFilter;
-import org.apache.lucene.analysis.ReusableAnalyzerBase;
-import org.apache.lucene.analysis.StopFilter;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.WordlistLoader;
 import org.apache.lucene.analysis.miscellaneous.StemmerOverrideFilter;
 import org.apache.lucene.analysis.snowball.SnowballFilter;
 import org.apache.lucene.analysis.standard.StandardFilter;
 import org.apache.lucene.analysis.standard.StandardTokenizer;
 import org.apache.lucene.analysis.standard.StandardAnalyzer;  // for javadoc
+import org.apache.lucene.analysis.util.ReusableAnalyzerBase;
+import org.apache.lucene.analysis.util.WordlistLoader;
 import org.apache.lucene.util.Version;
 
 import java.io.File;
@@ -171,7 +171,7 @@
   public DutchAnalyzer(Version matchVersion, File stopwords) {
     // this is completely broken!
     try {
-      stoptable = org.apache.lucene.analysis.WordlistLoader.getWordSet(stopwords);
+      stoptable = org.apache.lucene.analysis.util.WordlistLoader.getWordSet(stopwords);
     } catch (IOException e) {
       // TODO: throw IOException
       throw new RuntimeException(e);
@@ -208,7 +208,7 @@
   @Deprecated
   public void setStemExclusionTable(File exclusionlist) {
     try {
-      excltable = org.apache.lucene.analysis.WordlistLoader.getWordSet(exclusionlist);
+      excltable = org.apache.lucene.analysis.util.WordlistLoader.getWordSet(exclusionlist);
       setPreviousTokenStream(null); // force a new stemmer to be created
     } catch (IOException e) {
       // TODO: throw IOException
Index: modules/analysis/common/src/java/org/apache/lucene/analysis/th/ThaiWordFilter.java
===================================================================
--- modules/analysis/common/src/java/org/apache/lucene/analysis/th/ThaiWordFilter.java	(revision 947868)
+++ modules/analysis/common/src/java/org/apache/lucene/analysis/th/ThaiWordFilter.java	(working copy)
@@ -24,7 +24,7 @@
 
 import org.apache.lucene.analysis.TokenFilter;
 import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.LowerCaseFilter;
+import org.apache.lucene.analysis.core.LowerCaseFilter;
 import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;
 import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
 import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
Index: modules/analysis/common/src/java/org/apache/lucene/analysis/th/ThaiAnalyzer.java
===================================================================
--- modules/analysis/common/src/java/org/apache/lucene/analysis/th/ThaiAnalyzer.java	(revision 947868)
+++ modules/analysis/common/src/java/org/apache/lucene/analysis/th/ThaiAnalyzer.java	(working copy)
@@ -18,16 +18,16 @@
 
 import java.io.Reader;
 
-import org.apache.lucene.analysis.ReusableAnalyzerBase;
 import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.analysis.StopAnalyzer;
-import org.apache.lucene.analysis.StopFilter;
-import org.apache.lucene.analysis.LowerCaseFilter;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
+import org.apache.lucene.analysis.core.LowerCaseFilter;
+import org.apache.lucene.analysis.core.StopAnalyzer;
+import org.apache.lucene.analysis.core.StopFilter;
 import org.apache.lucene.analysis.standard.StandardAnalyzer;
 import org.apache.lucene.analysis.standard.StandardFilter;
 import org.apache.lucene.analysis.standard.StandardTokenizer;
+import org.apache.lucene.analysis.util.ReusableAnalyzerBase;
 import org.apache.lucene.util.Version;
 
 /**
@@ -46,10 +46,10 @@
 
   /**
    * Creates
-   * {@link org.apache.lucene.analysis.ReusableAnalyzerBase.TokenStreamComponents}
+   * {@link org.apache.lucene.analysis.util.ReusableAnalyzerBase.TokenStreamComponents}
    * used to tokenize all the text in the provided {@link Reader}.
    * 
-   * @return {@link org.apache.lucene.analysis.ReusableAnalyzerBase.TokenStreamComponents}
+   * @return {@link org.apache.lucene.analysis.util.ReusableAnalyzerBase.TokenStreamComponents}
    *         built from a {@link StandardTokenizer} filtered with
    *         {@link StandardFilter}, {@link LowerCaseFilter}, {@link ThaiWordFilter}, and
    *         {@link StopFilter}
Index: modules/analysis/common/src/java/org/apache/lucene/analysis/hu/HungarianAnalyzer.java
===================================================================
--- modules/analysis/common/src/java/org/apache/lucene/analysis/hu/HungarianAnalyzer.java	(revision 947868)
+++ modules/analysis/common/src/java/org/apache/lucene/analysis/hu/HungarianAnalyzer.java	(working copy)
@@ -23,16 +23,16 @@
 
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.CharArraySet;
+import org.apache.lucene.analysis.core.LowerCaseFilter;
+import org.apache.lucene.analysis.core.StopFilter;
 import org.apache.lucene.analysis.miscellaneous.KeywordMarkerFilter;
-import org.apache.lucene.analysis.LowerCaseFilter;
-import org.apache.lucene.analysis.StopFilter;
-import org.apache.lucene.analysis.StopwordAnalyzerBase;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.WordlistLoader;
 import org.apache.lucene.analysis.snowball.SnowballFilter;
 import org.apache.lucene.analysis.standard.StandardFilter;
 import org.apache.lucene.analysis.standard.StandardTokenizer;
+import org.apache.lucene.analysis.util.StopwordAnalyzerBase;
+import org.apache.lucene.analysis.util.WordlistLoader;
 import org.apache.lucene.util.Version;
 import org.tartarus.snowball.ext.HungarianStemmer;
 
@@ -106,11 +106,11 @@
 
   /**
    * Creates a
-   * {@link org.apache.lucene.analysis.ReusableAnalyzerBase.TokenStreamComponents}
+   * {@link org.apache.lucene.analysis.util.ReusableAnalyzerBase.TokenStreamComponents}
    * which tokenizes all the text in the provided {@link Reader}.
    * 
    * @return A
-   *         {@link org.apache.lucene.analysis.ReusableAnalyzerBase.TokenStreamComponents}
+   *         {@link org.apache.lucene.analysis.util.ReusableAnalyzerBase.TokenStreamComponents}
    *         built from an {@link StandardTokenizer} filtered with
    *         {@link StandardFilter}, {@link LowerCaseFilter}, {@link StopFilter}
    *         , {@link KeywordMarkerFilter} if a stem exclusion set is
Index: modules/analysis/common/src/java/org/apache/lucene/analysis/no/NorwegianAnalyzer.java
===================================================================
--- modules/analysis/common/src/java/org/apache/lucene/analysis/no/NorwegianAnalyzer.java	(revision 947868)
+++ modules/analysis/common/src/java/org/apache/lucene/analysis/no/NorwegianAnalyzer.java	(working copy)
@@ -23,16 +23,16 @@
 
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.CharArraySet;
+import org.apache.lucene.analysis.core.LowerCaseFilter;
+import org.apache.lucene.analysis.core.StopFilter;
 import org.apache.lucene.analysis.miscellaneous.KeywordMarkerFilter;
-import org.apache.lucene.analysis.LowerCaseFilter;
-import org.apache.lucene.analysis.StopFilter;
-import org.apache.lucene.analysis.StopwordAnalyzerBase;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.WordlistLoader;
 import org.apache.lucene.analysis.snowball.SnowballFilter;
 import org.apache.lucene.analysis.standard.StandardFilter;
 import org.apache.lucene.analysis.standard.StandardTokenizer;
+import org.apache.lucene.analysis.util.StopwordAnalyzerBase;
+import org.apache.lucene.analysis.util.WordlistLoader;
 import org.apache.lucene.util.Version;
 import org.tartarus.snowball.ext.NorwegianStemmer;
 
@@ -106,11 +106,11 @@
 
   /**
    * Creates a
-   * {@link org.apache.lucene.analysis.ReusableAnalyzerBase.TokenStreamComponents}
+   * {@link org.apache.lucene.analysis.util.ReusableAnalyzerBase.TokenStreamComponents}
    * which tokenizes all the text in the provided {@link Reader}.
    * 
    * @return A
-   *         {@link org.apache.lucene.analysis.ReusableAnalyzerBase.TokenStreamComponents}
+   *         {@link org.apache.lucene.analysis.util.ReusableAnalyzerBase.TokenStreamComponents}
    *         built from an {@link StandardTokenizer} filtered with
    *         {@link StandardFilter}, {@link LowerCaseFilter}, {@link StopFilter}
    *         , {@link KeywordMarkerFilter} if a stem exclusion set is
Index: modules/analysis/common/src/java/org/apache/lucene/analysis/ro/RomanianAnalyzer.java
===================================================================
--- modules/analysis/common/src/java/org/apache/lucene/analysis/ro/RomanianAnalyzer.java	(revision 947868)
+++ modules/analysis/common/src/java/org/apache/lucene/analysis/ro/RomanianAnalyzer.java	(working copy)
@@ -23,15 +23,15 @@
 
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.CharArraySet;
+import org.apache.lucene.analysis.core.LowerCaseFilter;
+import org.apache.lucene.analysis.core.StopFilter;
 import org.apache.lucene.analysis.miscellaneous.KeywordMarkerFilter;
-import org.apache.lucene.analysis.LowerCaseFilter;
-import org.apache.lucene.analysis.StopFilter;
-import org.apache.lucene.analysis.StopwordAnalyzerBase;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
 import org.apache.lucene.analysis.snowball.SnowballFilter;
 import org.apache.lucene.analysis.standard.StandardFilter;
 import org.apache.lucene.analysis.standard.StandardTokenizer;
+import org.apache.lucene.analysis.util.StopwordAnalyzerBase;
 import org.apache.lucene.util.Version;
 import org.tartarus.snowball.ext.RomanianStemmer;
 
@@ -110,11 +110,11 @@
 
   /**
    * Creates a
-   * {@link org.apache.lucene.analysis.ReusableAnalyzerBase.TokenStreamComponents}
+   * {@link org.apache.lucene.analysis.util.ReusableAnalyzerBase.TokenStreamComponents}
    * which tokenizes all the text in the provided {@link Reader}.
    * 
    * @return A
-   *         {@link org.apache.lucene.analysis.ReusableAnalyzerBase.TokenStreamComponents}
+   *         {@link org.apache.lucene.analysis.util.ReusableAnalyzerBase.TokenStreamComponents}
    *         built from an {@link StandardTokenizer} filtered with
    *         {@link StandardFilter}, {@link LowerCaseFilter}, {@link StopFilter}
    *         , {@link KeywordMarkerFilter} if a stem exclusion set is
Index: modules/analysis/common/src/java/org/apache/lucene/analysis/snowball/SnowballAnalyzer.java
===================================================================
--- modules/analysis/common/src/java/org/apache/lucene/analysis/snowball/SnowballAnalyzer.java	(revision 947868)
+++ modules/analysis/common/src/java/org/apache/lucene/analysis/snowball/SnowballAnalyzer.java	(working copy)
@@ -18,6 +18,8 @@
  */
 
 import org.apache.lucene.analysis.*;
+import org.apache.lucene.analysis.core.LowerCaseFilter;
+import org.apache.lucene.analysis.core.StopFilter;
 import org.apache.lucene.analysis.standard.*;
 import org.apache.lucene.analysis.tr.TurkishLowerCaseFilter;
 import org.apache.lucene.util.Version;
Index: modules/analysis/common/src/java/org/apache/lucene/analysis/snowball/SnowballFilter.java
===================================================================
--- modules/analysis/common/src/java/org/apache/lucene/analysis/snowball/SnowballFilter.java	(revision 947868)
+++ modules/analysis/common/src/java/org/apache/lucene/analysis/snowball/SnowballFilter.java	(working copy)
@@ -21,10 +21,10 @@
 
 import org.apache.lucene.analysis.TokenFilter;
 import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.analysis.core.LowerCaseFilter;
 import org.apache.lucene.analysis.tokenattributes.KeywordAttribute;
 import org.apache.lucene.analysis.tokenattributes.TermAttribute;
 import org.apache.lucene.analysis.tr.TurkishLowerCaseFilter; // javadoc @link
-import org.apache.lucene.analysis.LowerCaseFilter; // javadoc @link
 import org.tartarus.snowball.SnowballProgram;
 
 /**
Index: modules/analysis/common/src/java/org/apache/lucene/analysis/pt/PortugueseAnalyzer.java
===================================================================
--- modules/analysis/common/src/java/org/apache/lucene/analysis/pt/PortugueseAnalyzer.java	(revision 947868)
+++ modules/analysis/common/src/java/org/apache/lucene/analysis/pt/PortugueseAnalyzer.java	(working copy)
@@ -23,16 +23,16 @@
 
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.CharArraySet;
+import org.apache.lucene.analysis.core.LowerCaseFilter;
+import org.apache.lucene.analysis.core.StopFilter;
 import org.apache.lucene.analysis.miscellaneous.KeywordMarkerFilter;
-import org.apache.lucene.analysis.LowerCaseFilter;
-import org.apache.lucene.analysis.StopFilter;
-import org.apache.lucene.analysis.StopwordAnalyzerBase;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.WordlistLoader;
 import org.apache.lucene.analysis.snowball.SnowballFilter;
 import org.apache.lucene.analysis.standard.StandardFilter;
 import org.apache.lucene.analysis.standard.StandardTokenizer;
+import org.apache.lucene.analysis.util.StopwordAnalyzerBase;
+import org.apache.lucene.analysis.util.WordlistLoader;
 import org.apache.lucene.util.Version;
 import org.tartarus.snowball.ext.PortugueseStemmer;
 
@@ -106,11 +106,11 @@
 
   /**
    * Creates a
-   * {@link org.apache.lucene.analysis.ReusableAnalyzerBase.TokenStreamComponents}
+   * {@link org.apache.lucene.analysis.util.ReusableAnalyzerBase.TokenStreamComponents}
    * which tokenizes all the text in the provided {@link Reader}.
    * 
    * @return A
-   *         {@link org.apache.lucene.analysis.ReusableAnalyzerBase.TokenStreamComponents}
+   *         {@link org.apache.lucene.analysis.util.ReusableAnalyzerBase.TokenStreamComponents}
    *         built from an {@link StandardTokenizer} filtered with
    *         {@link StandardFilter}, {@link LowerCaseFilter}, {@link StopFilter}
    *         , {@link KeywordMarkerFilter} if a stem exclusion set is
Index: modules/analysis/common/src/java/org/apache/lucene/analysis/tr/TurkishAnalyzer.java
===================================================================
--- modules/analysis/common/src/java/org/apache/lucene/analysis/tr/TurkishAnalyzer.java	(revision 947868)
+++ modules/analysis/common/src/java/org/apache/lucene/analysis/tr/TurkishAnalyzer.java	(working copy)
@@ -23,14 +23,14 @@
 
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.CharArraySet;
+import org.apache.lucene.analysis.core.StopFilter;
 import org.apache.lucene.analysis.miscellaneous.KeywordMarkerFilter;
-import org.apache.lucene.analysis.StopFilter;
-import org.apache.lucene.analysis.StopwordAnalyzerBase;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
 import org.apache.lucene.analysis.snowball.SnowballFilter;
 import org.apache.lucene.analysis.standard.StandardFilter;
 import org.apache.lucene.analysis.standard.StandardTokenizer;
+import org.apache.lucene.analysis.util.StopwordAnalyzerBase;
 import org.apache.lucene.util.Version;
 import org.tartarus.snowball.ext.TurkishStemmer;
 
@@ -109,11 +109,11 @@
 
   /**
    * Creates a
-   * {@link org.apache.lucene.analysis.ReusableAnalyzerBase.TokenStreamComponents}
+   * {@link org.apache.lucene.analysis.util.ReusableAnalyzerBase.TokenStreamComponents}
    * which tokenizes all the text in the provided {@link Reader}.
    * 
    * @return A
-   *         {@link org.apache.lucene.analysis.ReusableAnalyzerBase.TokenStreamComponents}
+   *         {@link org.apache.lucene.analysis.util.ReusableAnalyzerBase.TokenStreamComponents}
    *         built from an {@link StandardTokenizer} filtered with
    *         {@link StandardFilter}, {@link TurkishLowerCaseFilter},
    *         {@link StopFilter}, {@link KeywordMarkerFilter} if a stem
Index: modules/analysis/common/src/java/org/apache/lucene/analysis/ru/RussianLetterTokenizer.java
===================================================================
--- modules/analysis/common/src/java/org/apache/lucene/analysis/ru/RussianLetterTokenizer.java	(revision 947868)
+++ modules/analysis/common/src/java/org/apache/lucene/analysis/ru/RussianLetterTokenizer.java	(working copy)
@@ -20,7 +20,7 @@
 import java.io.Reader;
 import org.apache.lucene.analysis.CharTokenizer;
 import org.apache.lucene.analysis.Tokenizer; // for javadocs
-import org.apache.lucene.analysis.LetterTokenizer; // for javadocs
+import org.apache.lucene.analysis.core.LetterTokenizer;
 import org.apache.lucene.analysis.standard.StandardTokenizer; // for javadocs
 import org.apache.lucene.util.AttributeSource;
 import org.apache.lucene.util.Version;
Index: modules/analysis/common/src/java/org/apache/lucene/analysis/ru/RussianLowerCaseFilter.java
===================================================================
--- modules/analysis/common/src/java/org/apache/lucene/analysis/ru/RussianLowerCaseFilter.java	(revision 947868)
+++ modules/analysis/common/src/java/org/apache/lucene/analysis/ru/RussianLowerCaseFilter.java	(working copy)
@@ -19,9 +19,9 @@
 
 import java.io.IOException;
 
-import org.apache.lucene.analysis.LowerCaseFilter; // for javadoc
 import org.apache.lucene.analysis.TokenFilter;
 import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.analysis.core.LowerCaseFilter;
 import org.apache.lucene.analysis.tokenattributes.TermAttribute;
 
 /**
Index: modules/analysis/common/src/java/org/apache/lucene/analysis/ru/RussianStemFilter.java
===================================================================
--- modules/analysis/common/src/java/org/apache/lucene/analysis/ru/RussianStemFilter.java	(revision 947868)
+++ modules/analysis/common/src/java/org/apache/lucene/analysis/ru/RussianStemFilter.java	(working copy)
@@ -17,8 +17,8 @@
  * limitations under the License.
  */
 
+import org.apache.lucene.analysis.core.LowerCaseFilter;
 import org.apache.lucene.analysis.miscellaneous.KeywordMarkerFilter; // for javadoc
-import org.apache.lucene.analysis.LowerCaseFilter; // for javadoc
 import org.apache.lucene.analysis.TokenFilter;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.tokenattributes.KeywordAttribute;
Index: modules/analysis/common/src/java/org/apache/lucene/analysis/ru/RussianAnalyzer.java
===================================================================
--- modules/analysis/common/src/java/org/apache/lucene/analysis/ru/RussianAnalyzer.java	(revision 947868)
+++ modules/analysis/common/src/java/org/apache/lucene/analysis/ru/RussianAnalyzer.java	(working copy)
@@ -25,16 +25,16 @@
 
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.CharArraySet;
-import org.apache.lucene.analysis.LowerCaseFilter;
 import org.apache.lucene.analysis.snowball.SnowballFilter;
 import org.apache.lucene.analysis.standard.StandardFilter;
 import org.apache.lucene.analysis.standard.StandardTokenizer;
+import org.apache.lucene.analysis.util.StopwordAnalyzerBase;
+import org.apache.lucene.analysis.util.WordlistLoader;
+import org.apache.lucene.analysis.core.LowerCaseFilter;
+import org.apache.lucene.analysis.core.StopFilter;
 import org.apache.lucene.analysis.miscellaneous.KeywordMarkerFilter;
-import org.apache.lucene.analysis.StopFilter;
-import org.apache.lucene.analysis.StopwordAnalyzerBase;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.WordlistLoader;
 import org.apache.lucene.util.Version;
 
 /**
@@ -161,10 +161,10 @@
 
   /**
    * Creates
-   * {@link org.apache.lucene.analysis.ReusableAnalyzerBase.TokenStreamComponents}
+   * {@link org.apache.lucene.analysis.util.ReusableAnalyzerBase.TokenStreamComponents}
    * used to tokenize all the text in the provided {@link Reader}.
    * 
-   * @return {@link org.apache.lucene.analysis.ReusableAnalyzerBase.TokenStreamComponents}
+   * @return {@link org.apache.lucene.analysis.util.ReusableAnalyzerBase.TokenStreamComponents}
    *         built from a {@link StandardTokenizer} filtered with
    *         {@link StandardFilter}, {@link LowerCaseFilter}, {@link StopFilter}
    *         , {@link KeywordMarkerFilter} if a stem exclusion set is
Index: modules/analysis/common/src/java/org/apache/lucene/analysis/id/IndonesianAnalyzer.java
===================================================================
--- modules/analysis/common/src/java/org/apache/lucene/analysis/id/IndonesianAnalyzer.java	(revision 947868)
+++ modules/analysis/common/src/java/org/apache/lucene/analysis/id/IndonesianAnalyzer.java	(working copy)
@@ -22,14 +22,14 @@
 import java.util.Set;
 
 import org.apache.lucene.analysis.CharArraySet;
+import org.apache.lucene.analysis.core.LowerCaseFilter;
+import org.apache.lucene.analysis.core.StopFilter;
 import org.apache.lucene.analysis.miscellaneous.KeywordMarkerFilter;
-import org.apache.lucene.analysis.LowerCaseFilter;
-import org.apache.lucene.analysis.StopFilter;
-import org.apache.lucene.analysis.StopwordAnalyzerBase;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
 import org.apache.lucene.analysis.standard.StandardFilter;
 import org.apache.lucene.analysis.standard.StandardTokenizer;
+import org.apache.lucene.analysis.util.StopwordAnalyzerBase;
 import org.apache.lucene.util.Version;
 
 /**
@@ -106,10 +106,10 @@
 
   /**
    * Creates
-   * {@link org.apache.lucene.analysis.ReusableAnalyzerBase.TokenStreamComponents}
+   * {@link org.apache.lucene.analysis.util.ReusableAnalyzerBase.TokenStreamComponents}
    * used to tokenize all the text in the provided {@link Reader}.
    * 
-   * @return {@link org.apache.lucene.analysis.ReusableAnalyzerBase.TokenStreamComponents}
+   * @return {@link org.apache.lucene.analysis.util.ReusableAnalyzerBase.TokenStreamComponents}
    *         built from an {@link StandardTokenizer} filtered with
    *         {@link StandardFilter}, {@link LowerCaseFilter},
    *         {@link StopFilter}, {@link KeywordMarkerFilter}
Index: modules/analysis/common/src/java/org/apache/lucene/analysis/cn/ChineseAnalyzer.java
===================================================================
--- modules/analysis/common/src/java/org/apache/lucene/analysis/cn/ChineseAnalyzer.java	(revision 947868)
+++ modules/analysis/common/src/java/org/apache/lucene/analysis/cn/ChineseAnalyzer.java	(working copy)
@@ -19,8 +19,8 @@
 
 import java.io.Reader;
 
-import org.apache.lucene.analysis.ReusableAnalyzerBase;
 import org.apache.lucene.analysis.standard.StandardAnalyzer; // javadoc @link
+import org.apache.lucene.analysis.util.ReusableAnalyzerBase;
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.Tokenizer;
 
@@ -35,10 +35,10 @@
 
   /**
    * Creates
-   * {@link org.apache.lucene.analysis.ReusableAnalyzerBase.TokenStreamComponents}
+   * {@link org.apache.lucene.analysis.util.ReusableAnalyzerBase.TokenStreamComponents}
    * used to tokenize all the text in the provided {@link Reader}.
    * 
-   * @return {@link org.apache.lucene.analysis.ReusableAnalyzerBase.TokenStreamComponents}
+   * @return {@link org.apache.lucene.analysis.util.ReusableAnalyzerBase.TokenStreamComponents}
    *         built from a {@link ChineseTokenizer} filtered with
    *         {@link ChineseFilter}
    */
Index: modules/analysis/common/src/java/org/apache/lucene/analysis/cn/ChineseFilter.java
===================================================================
--- modules/analysis/common/src/java/org/apache/lucene/analysis/cn/ChineseFilter.java	(revision 947868)
+++ modules/analysis/common/src/java/org/apache/lucene/analysis/cn/ChineseFilter.java	(working copy)
@@ -23,7 +23,7 @@
 import org.apache.lucene.analysis.CharArraySet;
 import org.apache.lucene.analysis.TokenFilter;
 import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.StopFilter;
+import org.apache.lucene.analysis.core.StopFilter;
 import org.apache.lucene.analysis.tokenattributes.TermAttribute;
 import org.apache.lucene.util.Version;
 
Index: modules/analysis/common/src/java/org/apache/lucene/analysis/el/GreekAnalyzer.java
===================================================================
--- modules/analysis/common/src/java/org/apache/lucene/analysis/el/GreekAnalyzer.java	(revision 947868)
+++ modules/analysis/common/src/java/org/apache/lucene/analysis/el/GreekAnalyzer.java	(working copy)
@@ -17,13 +17,13 @@
  */
 
 import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.analysis.StopFilter;
-import org.apache.lucene.analysis.StopwordAnalyzerBase;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
+import org.apache.lucene.analysis.core.StopFilter;
 import org.apache.lucene.analysis.standard.StandardFilter;
 import org.apache.lucene.analysis.standard.StandardTokenizer;
 import org.apache.lucene.analysis.standard.StandardAnalyzer;  // for javadoc
+import org.apache.lucene.analysis.util.StopwordAnalyzerBase;
 import org.apache.lucene.util.Version;
 
 import java.io.IOException;
@@ -121,10 +121,10 @@
   
   /**
    * Creates
-   * {@link org.apache.lucene.analysis.ReusableAnalyzerBase.TokenStreamComponents}
+   * {@link org.apache.lucene.analysis.util.ReusableAnalyzerBase.TokenStreamComponents}
    * used to tokenize all the text in the provided {@link Reader}.
    * 
-   * @return {@link org.apache.lucene.analysis.ReusableAnalyzerBase.TokenStreamComponents}
+   * @return {@link org.apache.lucene.analysis.util.ReusableAnalyzerBase.TokenStreamComponents}
    *         built from a {@link StandardTokenizer} filtered with
    *         {@link GreekLowerCaseFilter}, {@link StandardFilter},
    *         {@link StopFilter}, and {@link GreekStemFilter}
Index: modules/analysis/common/src/java/org/apache/lucene/analysis/ar/ArabicAnalyzer.java
===================================================================
--- modules/analysis/common/src/java/org/apache/lucene/analysis/ar/ArabicAnalyzer.java	(revision 947868)
+++ modules/analysis/common/src/java/org/apache/lucene/analysis/ar/ArabicAnalyzer.java	(working copy)
@@ -24,14 +24,14 @@
 import java.util.Set;
 
 import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.analysis.LowerCaseFilter;
 import org.apache.lucene.analysis.CharArraySet;
+import org.apache.lucene.analysis.core.LowerCaseFilter;
+import org.apache.lucene.analysis.core.StopFilter;
 import org.apache.lucene.analysis.miscellaneous.KeywordMarkerFilter;
-import org.apache.lucene.analysis.StopFilter;
-import org.apache.lucene.analysis.StopwordAnalyzerBase;
+import org.apache.lucene.analysis.util.StopwordAnalyzerBase;
+import org.apache.lucene.analysis.util.WordlistLoader;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.WordlistLoader;
 import org.apache.lucene.util.Version;
 
 /**
@@ -163,10 +163,10 @@
 
   /**
    * Creates
-   * {@link org.apache.lucene.analysis.ReusableAnalyzerBase.TokenStreamComponents}
+   * {@link org.apache.lucene.analysis.util.ReusableAnalyzerBase.TokenStreamComponents}
    * used to tokenize all the text in the provided {@link Reader}.
    * 
-   * @return {@link org.apache.lucene.analysis.ReusableAnalyzerBase.TokenStreamComponents}
+   * @return {@link org.apache.lucene.analysis.util.ReusableAnalyzerBase.TokenStreamComponents}
    *         built from an {@link ArabicLetterTokenizer} filtered with
    *         {@link LowerCaseFilter}, {@link StopFilter},
    *         {@link ArabicNormalizationFilter}, {@link KeywordMarkerFilter}
Index: modules/analysis/common/src/java/org/apache/lucene/analysis/ar/ArabicLetterTokenizer.java
===================================================================
--- modules/analysis/common/src/java/org/apache/lucene/analysis/ar/ArabicLetterTokenizer.java	(revision 947868)
+++ modules/analysis/common/src/java/org/apache/lucene/analysis/ar/ArabicLetterTokenizer.java	(working copy)
@@ -19,7 +19,7 @@
 import java.io.Reader;
 
 import org.apache.lucene.analysis.CharTokenizer;
-import org.apache.lucene.analysis.LetterTokenizer;
+import org.apache.lucene.analysis.core.LetterTokenizer;
 import org.apache.lucene.util.AttributeSource;
 import org.apache.lucene.util.Version;
 
@@ -120,7 +120,7 @@
   
   /** 
    * Allows for Letter category or NonspacingMark category
-   * @see org.apache.lucene.analysis.LetterTokenizer#isTokenChar(int)
+   * @see org.apache.lucene.analysis.core.LetterTokenizer#isTokenChar(int)
    */
   @Override
   protected boolean isTokenChar(int c) {
Index: modules/analysis/common/src/java/org/apache/lucene/analysis/en/EnglishAnalyzer.java
===================================================================
--- modules/analysis/common/src/java/org/apache/lucene/analysis/en/EnglishAnalyzer.java	(revision 947868)
+++ modules/analysis/common/src/java/org/apache/lucene/analysis/en/EnglishAnalyzer.java	(working copy)
@@ -22,15 +22,15 @@
 
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.CharArraySet;
+import org.apache.lucene.analysis.core.LowerCaseFilter;
+import org.apache.lucene.analysis.core.StopFilter;
 import org.apache.lucene.analysis.miscellaneous.KeywordMarkerFilter;
-import org.apache.lucene.analysis.LowerCaseFilter;
-import org.apache.lucene.analysis.StopFilter;
-import org.apache.lucene.analysis.StopwordAnalyzerBase;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
 import org.apache.lucene.analysis.standard.StandardAnalyzer;
 import org.apache.lucene.analysis.standard.StandardFilter;
 import org.apache.lucene.analysis.standard.StandardTokenizer;
+import org.apache.lucene.analysis.util.StopwordAnalyzerBase;
 import org.apache.lucene.util.Version;
 
 /**
@@ -89,11 +89,11 @@
 
   /**
    * Creates a
-   * {@link org.apache.lucene.analysis.ReusableAnalyzerBase.TokenStreamComponents}
+   * {@link org.apache.lucene.analysis.util.ReusableAnalyzerBase.TokenStreamComponents}
    * which tokenizes all the text in the provided {@link Reader}.
    * 
    * @return A
-   *         {@link org.apache.lucene.analysis.ReusableAnalyzerBase.TokenStreamComponents}
+   *         {@link org.apache.lucene.analysis.util.ReusableAnalyzerBase.TokenStreamComponents}
    *         built from an {@link StandardTokenizer} filtered with
    *         {@link StandardFilter}, {@link LowerCaseFilter}, {@link StopFilter}
    *         , {@link KeywordMarkerFilter} if a stem exclusion set is
Index: modules/analysis/common/src/java/org/apache/lucene/analysis/cjk/CJKAnalyzer.java
===================================================================
--- modules/analysis/common/src/java/org/apache/lucene/analysis/cjk/CJKAnalyzer.java	(revision 947868)
+++ modules/analysis/common/src/java/org/apache/lucene/analysis/cjk/CJKAnalyzer.java	(working copy)
@@ -19,9 +19,9 @@
 
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.CharArraySet;
-import org.apache.lucene.analysis.StopFilter;
-import org.apache.lucene.analysis.StopwordAnalyzerBase;
 import org.apache.lucene.analysis.Tokenizer;
+import org.apache.lucene.analysis.core.StopFilter;
+import org.apache.lucene.analysis.util.StopwordAnalyzerBase;
 import org.apache.lucene.util.Version;
 
 import java.io.Reader;
Index: modules/analysis/common/src/java/org/apache/lucene/analysis/es/SpanishAnalyzer.java
===================================================================
--- modules/analysis/common/src/java/org/apache/lucene/analysis/es/SpanishAnalyzer.java	(revision 947868)
+++ modules/analysis/common/src/java/org/apache/lucene/analysis/es/SpanishAnalyzer.java	(working copy)
@@ -23,16 +23,16 @@
 
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.CharArraySet;
+import org.apache.lucene.analysis.core.LowerCaseFilter;
+import org.apache.lucene.analysis.core.StopFilter;
 import org.apache.lucene.analysis.miscellaneous.KeywordMarkerFilter;
-import org.apache.lucene.analysis.LowerCaseFilter;
-import org.apache.lucene.analysis.StopFilter;
-import org.apache.lucene.analysis.StopwordAnalyzerBase;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.WordlistLoader;
 import org.apache.lucene.analysis.snowball.SnowballFilter;
 import org.apache.lucene.analysis.standard.StandardFilter;
 import org.apache.lucene.analysis.standard.StandardTokenizer;
+import org.apache.lucene.analysis.util.StopwordAnalyzerBase;
+import org.apache.lucene.analysis.util.WordlistLoader;
 import org.apache.lucene.util.Version;
 import org.tartarus.snowball.ext.SpanishStemmer;
 
@@ -106,11 +106,11 @@
 
   /**
    * Creates a
-   * {@link org.apache.lucene.analysis.ReusableAnalyzerBase.TokenStreamComponents}
+   * {@link org.apache.lucene.analysis.util.ReusableAnalyzerBase.TokenStreamComponents}
    * which tokenizes all the text in the provided {@link Reader}.
    * 
    * @return A
-   *         {@link org.apache.lucene.analysis.ReusableAnalyzerBase.TokenStreamComponents}
+   *         {@link org.apache.lucene.analysis.util.ReusableAnalyzerBase.TokenStreamComponents}
    *         built from an {@link StandardTokenizer} filtered with
    *         {@link StandardFilter}, {@link LowerCaseFilter}, {@link StopFilter}
    *         , {@link KeywordMarkerFilter} if a stem exclusion set is
Index: modules/analysis/common/src/java/org/apache/lucene/analysis/it/ItalianAnalyzer.java
===================================================================
--- modules/analysis/common/src/java/org/apache/lucene/analysis/it/ItalianAnalyzer.java	(revision 947868)
+++ modules/analysis/common/src/java/org/apache/lucene/analysis/it/ItalianAnalyzer.java	(working copy)
@@ -23,16 +23,16 @@
 
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.CharArraySet;
+import org.apache.lucene.analysis.core.LowerCaseFilter;
+import org.apache.lucene.analysis.core.StopFilter;
 import org.apache.lucene.analysis.miscellaneous.KeywordMarkerFilter;
-import org.apache.lucene.analysis.LowerCaseFilter;
-import org.apache.lucene.analysis.StopFilter;
-import org.apache.lucene.analysis.StopwordAnalyzerBase;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.WordlistLoader;
 import org.apache.lucene.analysis.snowball.SnowballFilter;
 import org.apache.lucene.analysis.standard.StandardFilter;
 import org.apache.lucene.analysis.standard.StandardTokenizer;
+import org.apache.lucene.analysis.util.StopwordAnalyzerBase;
+import org.apache.lucene.analysis.util.WordlistLoader;
 import org.apache.lucene.util.Version;
 import org.tartarus.snowball.ext.ItalianStemmer;
 
@@ -106,11 +106,11 @@
 
   /**
    * Creates a
-   * {@link org.apache.lucene.analysis.ReusableAnalyzerBase.TokenStreamComponents}
+   * {@link org.apache.lucene.analysis.util.ReusableAnalyzerBase.TokenStreamComponents}
    * which tokenizes all the text in the provided {@link Reader}.
    * 
    * @return A
-   *         {@link org.apache.lucene.analysis.ReusableAnalyzerBase.TokenStreamComponents}
+   *         {@link org.apache.lucene.analysis.util.ReusableAnalyzerBase.TokenStreamComponents}
    *         built from an {@link StandardTokenizer} filtered with
    *         {@link StandardFilter}, {@link LowerCaseFilter}, {@link StopFilter}
    *         , {@link KeywordMarkerFilter} if a stem exclusion set is
Index: modules/analysis/common/src/java/org/apache/lucene/analysis/cz/CzechAnalyzer.java
===================================================================
--- modules/analysis/common/src/java/org/apache/lucene/analysis/cz/CzechAnalyzer.java	(revision 947868)
+++ modules/analysis/common/src/java/org/apache/lucene/analysis/cz/CzechAnalyzer.java	(working copy)
@@ -17,17 +17,17 @@
  * limitations under the License.
  */
 
-import org.apache.lucene.analysis.ReusableAnalyzerBase;
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.CharArraySet;
+import org.apache.lucene.analysis.core.LowerCaseFilter;
+import org.apache.lucene.analysis.core.StopFilter;
 import org.apache.lucene.analysis.miscellaneous.KeywordMarkerFilter;
-import org.apache.lucene.analysis.LowerCaseFilter;
-import org.apache.lucene.analysis.StopFilter;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.WordlistLoader;
 import org.apache.lucene.analysis.standard.StandardFilter;
 import org.apache.lucene.analysis.standard.StandardTokenizer;
+import org.apache.lucene.analysis.util.ReusableAnalyzerBase;
+import org.apache.lucene.analysis.util.WordlistLoader;
 import org.apache.lucene.util.Version;
 
 import java.io.*;
@@ -218,10 +218,10 @@
 
   /**
    * Creates
-   * {@link org.apache.lucene.analysis.ReusableAnalyzerBase.TokenStreamComponents}
+   * {@link org.apache.lucene.analysis.util.ReusableAnalyzerBase.TokenStreamComponents}
    * used to tokenize all the text in the provided {@link Reader}.
    * 
-   * @return {@link org.apache.lucene.analysis.ReusableAnalyzerBase.TokenStreamComponents}
+   * @return {@link org.apache.lucene.analysis.util.ReusableAnalyzerBase.TokenStreamComponents}
    *         built from a {@link StandardTokenizer} filtered with
    *         {@link StandardFilter}, {@link LowerCaseFilter}, {@link StopFilter}
    *         , and {@link CzechStemFilter} (only if version is >= LUCENE_31). If
Index: modules/analysis/common/src/java/org/apache/lucene/analysis/util/StopwordAnalyzerBase.java
===================================================================
--- modules/analysis/common/src/java/org/apache/lucene/analysis/util/StopwordAnalyzerBase.java	(revision 0)
+++ modules/analysis/common/src/java/org/apache/lucene/analysis/util/StopwordAnalyzerBase.java	(working copy)
@@ -15,14 +15,14 @@
  * limitations under the License.
  */
 
-package org.apache.lucene.analysis;
+package org.apache.lucene.analysis.util;
 
 import java.io.IOException;
 import java.util.Set;
 
 import org.apache.lucene.analysis.CharArraySet;
-import org.apache.lucene.analysis.ReusableAnalyzerBase;
-import org.apache.lucene.analysis.WordlistLoader;
+import org.apache.lucene.analysis.util.ReusableAnalyzerBase;
+import org.apache.lucene.analysis.util.WordlistLoader;
 import org.apache.lucene.util.Version;
 
 /**
Index: modules/analysis/common/src/java/org/apache/lucene/analysis/util/ReusableAnalyzerBase.java
===================================================================
--- modules/analysis/common/src/java/org/apache/lucene/analysis/util/ReusableAnalyzerBase.java	(revision 0)
+++ modules/analysis/common/src/java/org/apache/lucene/analysis/util/ReusableAnalyzerBase.java	(working copy)
@@ -15,11 +15,16 @@
  * limitations under the License.
  */
 
-package org.apache.lucene.analysis;
+package org.apache.lucene.analysis.util;
 
 import java.io.IOException;
 import java.io.Reader;
 
+import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.analysis.TokenFilter;
+import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.analysis.Tokenizer;
+
 /**
  * An convenience subclass of Analyzer that makes it easy to implement
  * {@link TokenStream} reuse.
Index: modules/analysis/common/src/java/org/apache/lucene/analysis/util/WordlistLoader.java
===================================================================
--- modules/analysis/common/src/java/org/apache/lucene/analysis/util/WordlistLoader.java	(revision 0)
+++ modules/analysis/common/src/java/org/apache/lucene/analysis/util/WordlistLoader.java	(working copy)
@@ -1,4 +1,4 @@
-package org.apache.lucene.analysis;
+package org.apache.lucene.analysis.util;
 
 /**
  * Licensed to the Apache Software Foundation (ASF) under one or more
Index: modules/analysis/common/src/java/org/apache/lucene/collation/CollationKeyAnalyzer.java
===================================================================
--- modules/analysis/common/src/java/org/apache/lucene/collation/CollationKeyAnalyzer.java	(revision 947868)
+++ modules/analysis/common/src/java/org/apache/lucene/collation/CollationKeyAnalyzer.java	(working copy)
@@ -20,8 +20,8 @@
 
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.KeywordTokenizer;
 import org.apache.lucene.analysis.Tokenizer;
+import org.apache.lucene.analysis.core.KeywordTokenizer;
 
 import java.text.Collator;
 import java.io.Reader;
Index: modules/analysis/common/build.xml
===================================================================
--- modules/analysis/common/build.xml	(revision 947868)
+++ modules/analysis/common/build.xml	(working copy)
@@ -38,7 +38,7 @@
 
   <target name="compile-core" depends="jflex-notice, common.compile-core"/>
 
-  <target name="jflex" depends="jflex-check,clean-jflex,jflex-wiki-tokenizer"/>
+  <target name="jflex" depends="jflex-check,clean-jflex,jflex-StandardAnalyzer,jflex-wiki-tokenizer"/>
 
   <target name="jflex-wiki-tokenizer" depends="init,jflex-check" if="jflex.present">
     <taskdef classname="jflex.anttask.JFlexTask" name="jflex">
@@ -49,11 +49,27 @@
            nobak="on"/>
   </target>
 
+  <target name="jflex-StandardAnalyzer" depends="init,jflex-check" if="jflex.present">
+    <taskdef classname="jflex.anttask.JFlexTask" name="jflex">
+			<classpath refid="jflex.classpath"/>
+    </taskdef>
+
+    <jflex file="src/java/org/apache/lucene/analysis/standard/StandardTokenizerImplOrig.jflex"
+           outdir="src/java/org/apache/lucene/analysis/standard"
+           nobak="on" />
+    <jflex file="src/java/org/apache/lucene/analysis/standard/StandardTokenizerImpl31.jflex"
+           outdir="src/java/org/apache/lucene/analysis/standard"
+           nobak="on" />
+  </target>
+
   <target name="clean-jflex">
     <delete>
       <fileset dir="src/java/org/apache/lucene/analysis/wikipedia" includes="*.java">
         <containsregexp expression="generated.*by.*JFlex"/>
       </fileset>
+      <fileset dir="src/java/org/apache/lucene/analysis/standard" includes="*.java">
+    	<containsregexp expression="generated.*by.*JFlex"/>
+      </fileset>
     </delete>
   </target>
 </project>
Index: modules/analysis/stempel/src/java/org/apache/lucene/analysis/pl/PolishAnalyzer.java
===================================================================
--- modules/analysis/stempel/src/java/org/apache/lucene/analysis/pl/PolishAnalyzer.java	(revision 947868)
+++ modules/analysis/stempel/src/java/org/apache/lucene/analysis/pl/PolishAnalyzer.java	(working copy)
@@ -26,17 +26,17 @@
 
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.CharArraySet;
+import org.apache.lucene.analysis.core.LowerCaseFilter;
+import org.apache.lucene.analysis.core.StopFilter;
 import org.apache.lucene.analysis.miscellaneous.KeywordMarkerFilter;
-import org.apache.lucene.analysis.LowerCaseFilter;
-import org.apache.lucene.analysis.StopFilter;
-import org.apache.lucene.analysis.StopwordAnalyzerBase;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.WordlistLoader;
 import org.apache.lucene.analysis.standard.StandardFilter;
 import org.apache.lucene.analysis.standard.StandardTokenizer;
 import org.apache.lucene.analysis.stempel.StempelStemmer;
 import org.apache.lucene.analysis.stempel.StempelFilter;
+import org.apache.lucene.analysis.util.StopwordAnalyzerBase;
+import org.apache.lucene.analysis.util.WordlistLoader;
 import org.apache.lucene.util.Version;
 import org.egothor.stemmer.Trie;
 
@@ -129,11 +129,11 @@
 
   /**
    * Creates a
-   * {@link org.apache.lucene.analysis.ReusableAnalyzerBase.TokenStreamComponents}
+   * {@link org.apache.lucene.analysis.util.ReusableAnalyzerBase.TokenStreamComponents}
    * which tokenizes all the text in the provided {@link Reader}.
    * 
    * @return A
-   *         {@link org.apache.lucene.analysis.ReusableAnalyzerBase.TokenStreamComponents}
+   *         {@link org.apache.lucene.analysis.util.ReusableAnalyzerBase.TokenStreamComponents}
    *         built from an {@link StandardTokenizer} filtered with
    *         {@link StandardFilter}, {@link LowerCaseFilter}, {@link StopFilter}
    *         , {@link KeywordMarkerFilter} if a stem exclusion set is
Index: lucene/src/test/org/apache/lucene/queryParser/TestQueryParser.java
===================================================================
--- lucene/src/test/org/apache/lucene/queryParser/TestQueryParser.java	(revision 947868)
+++ lucene/src/test/org/apache/lucene/queryParser/TestQueryParser.java	(working copy)
@@ -29,7 +29,6 @@
 import java.util.Locale;
 
 import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.analysis.KeywordAnalyzer;
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.analysis.MockTokenFilter;
 import org.apache.lucene.analysis.MockTokenizer;
@@ -255,8 +254,10 @@
     assertQueryEquals("türm term term", new MockAnalyzer(), "türm term term");
     assertQueryEquals("ümlaut", new MockAnalyzer(), "ümlaut");
 
-    assertQueryEquals("\"\"", new KeywordAnalyzer(), "");
-    assertQueryEquals("foo:\"\"", new KeywordAnalyzer(), "foo:");
+    // FIXME: enhance MockAnalyzer to be able to support this
+    // it must no longer extend CharTokenizer
+    //assertQueryEquals("\"\"", new KeywordAnalyzer(), "");
+    //assertQueryEquals("foo:\"\"", new KeywordAnalyzer(), "foo:");
 
     assertQueryEquals("a AND b", null, "+a +b");
     assertQueryEquals("(a AND b)", null, "+a +b");
Index: lucene/src/test/org/apache/lucene/analysis/TestStandardAnalyzer.java (deleted)
===================================================================
Index: lucene/src/test/org/apache/lucene/analysis/TestAnalyzers.java (deleted)
===================================================================
Index: lucene/src/test/org/apache/lucene/analysis/TestToken.java
===================================================================
--- lucene/src/test/org/apache/lucene/analysis/TestToken.java	(revision 947868)
+++ lucene/src/test/org/apache/lucene/analysis/TestToken.java	(working copy)
@@ -239,7 +239,7 @@
   }
 
   public void testTokenAttributeFactory() throws Exception {
-    TokenStream ts = new WhitespaceTokenizer(Token.TOKEN_ATTRIBUTE_FACTORY, new StringReader("foo bar"));
+    TokenStream ts = new MockTokenizer(Token.TOKEN_ATTRIBUTE_FACTORY, new StringReader("foo bar"), MockTokenizer.WHITESPACE, false);
     
     assertTrue("TypeAttribute is not implemented by SenselessAttributeImpl",
       ts.addAttribute(SenselessAttribute.class) instanceof SenselessAttributeImpl);
Index: lucene/src/test/org/apache/lucene/analysis/MockTokenizer.java
===================================================================
--- lucene/src/test/org/apache/lucene/analysis/MockTokenizer.java	(revision 947868)
+++ lucene/src/test/org/apache/lucene/analysis/MockTokenizer.java	(working copy)
@@ -21,6 +21,7 @@
 import java.io.Reader;
 
 import org.apache.lucene.util.Version;
+import org.apache.lucene.util.AttributeSource.AttributeFactory;
 import org.apache.lucene.util.automaton.CharacterRunAutomaton;
 import org.apache.lucene.util.automaton.RegExp;
 
@@ -45,6 +46,13 @@
   private final boolean lowerCase;
   private int state;
 
+  public MockTokenizer(AttributeFactory factory, Reader input, CharacterRunAutomaton runAutomaton, boolean lowerCase) {
+    super(Version.LUCENE_CURRENT, factory, input);
+    this.runAutomaton = runAutomaton;
+    this.lowerCase = lowerCase;
+    this.state = runAutomaton.getInitialState();
+  }
+
   public MockTokenizer(Reader input, CharacterRunAutomaton runAutomaton, boolean lowerCase) {
     super(Version.LUCENE_CURRENT, input);
     this.runAutomaton = runAutomaton;
Index: lucene/src/test/org/apache/lucene/analysis/TestKeywordAnalyzer.java (deleted)
===================================================================
Index: lucene/src/test/org/apache/lucene/analysis/TestStopFilter.java (deleted)
===================================================================
Index: lucene/src/test/org/apache/lucene/analysis/TestStopAnalyzer.java (deleted)
===================================================================
Index: lucene/src/test/org/apache/lucene/analysis/TestCharTokenizers.java
===================================================================
--- lucene/src/test/org/apache/lucene/analysis/TestCharTokenizers.java	(revision 947868)
+++ lucene/src/test/org/apache/lucene/analysis/TestCharTokenizers.java	(working copy)
@@ -46,8 +46,7 @@
     }
     // internal buffer size is 1024 make sure we have a surrogate pair right at the border
     builder.insert(1023, "\ud801\udc1c");
-    LowerCaseTokenizer tokenizer = new LowerCaseTokenizer(
-        TEST_VERSION_CURRENT, new StringReader(builder.toString()));
+    MockTokenizer tokenizer = new MockTokenizer(new StringReader(builder.toString()), MockTokenizer.SIMPLE, true);
     assertTokenStreamContents(tokenizer, builder.toString().toLowerCase().split(" "));
   }
   
@@ -64,8 +63,7 @@
         builder.append("a");
       }
       builder.append("\ud801\udc1cabc");
-      LowerCaseTokenizer tokenizer = new LowerCaseTokenizer(
-          TEST_VERSION_CURRENT, new StringReader(builder.toString()));
+      MockTokenizer tokenizer = new MockTokenizer(new StringReader(builder.toString()), MockTokenizer.SIMPLE, true);
       assertTokenStreamContents(tokenizer, new String[] {builder.toString().toLowerCase()});
     }
   }
@@ -79,8 +77,7 @@
     for (int i = 0; i < 255; i++) {
       builder.append("A");
     }
-    LowerCaseTokenizer tokenizer = new LowerCaseTokenizer(
-        TEST_VERSION_CURRENT, new StringReader(builder.toString() + builder.toString()));
+    MockTokenizer tokenizer = new MockTokenizer(new StringReader(builder.toString() + builder.toString()), MockTokenizer.SIMPLE, true);
     assertTokenStreamContents(tokenizer, new String[] {builder.toString().toLowerCase(), builder.toString().toLowerCase()});
   }
   
@@ -94,42 +91,10 @@
       builder.append("A");
     }
     builder.append("\ud801\udc1c");
-    LowerCaseTokenizer tokenizer = new LowerCaseTokenizer(
-        TEST_VERSION_CURRENT, new StringReader(builder.toString() + builder.toString()));
+    MockTokenizer tokenizer = new MockTokenizer(new StringReader(builder.toString() + builder.toString()), MockTokenizer.SIMPLE, true);
     assertTokenStreamContents(tokenizer, new String[] {builder.toString().toLowerCase(), builder.toString().toLowerCase()});
   }
 
-  public void testLowerCaseTokenizer() throws IOException {
-    StringReader reader = new StringReader("Tokenizer \ud801\udc1ctest");
-    LowerCaseTokenizer tokenizer = new LowerCaseTokenizer(TEST_VERSION_CURRENT,
-        reader);
-    assertTokenStreamContents(tokenizer, new String[] { "tokenizer",
-        "\ud801\udc44test" });
-  }
-
-  public void testLowerCaseTokenizerBWCompat() throws IOException {
-    StringReader reader = new StringReader("Tokenizer \ud801\udc1ctest");
-    LowerCaseTokenizer tokenizer = new LowerCaseTokenizer(Version.LUCENE_30,
-        reader);
-    assertTokenStreamContents(tokenizer, new String[] { "tokenizer", "test" });
-  }
-
-  public void testWhitespaceTokenizer() throws IOException {
-    StringReader reader = new StringReader("Tokenizer \ud801\udc1ctest");
-    WhitespaceTokenizer tokenizer = new WhitespaceTokenizer(TEST_VERSION_CURRENT,
-        reader);
-    assertTokenStreamContents(tokenizer, new String[] { "Tokenizer",
-        "\ud801\udc1ctest" });
-  }
-
-  public void testWhitespaceTokenizerBWCompat() throws IOException {
-    StringReader reader = new StringReader("Tokenizer \ud801\udc1ctest");
-    WhitespaceTokenizer tokenizer = new WhitespaceTokenizer(Version.LUCENE_30,
-        reader);
-    assertTokenStreamContents(tokenizer, new String[] { "Tokenizer",
-        "\ud801\udc1ctest" });
-  }
-
   public void testIsTokenCharCharInSubclass() {
     new TestingCharTokenizer(Version.LUCENE_30, new StringReader(""));
     try {
Index: lucene/src/test/org/apache/lucene/collation/CollationTestBase.java (deleted)
===================================================================
Index: lucene/src/test/org/apache/lucene/collation/TestCollationKeyAnalyzer.java (deleted)
===================================================================
Index: lucene/src/test/org/apache/lucene/collation/TestCollationKeyFilter.java (deleted)
===================================================================
Index: lucene/src/test/org/apache/lucene/search/TestPositionIncrement.java
===================================================================
--- lucene/src/test/org/apache/lucene/search/TestPositionIncrement.java	(revision 947868)
+++ lucene/src/test/org/apache/lucene/search/TestPositionIncrement.java	(working copy)
@@ -21,19 +21,15 @@
 import java.io.IOException;
 import java.io.StringReader;
 import java.util.Collection;
-import java.util.Collections;
-import java.util.Iterator;
 
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.MockTokenizer;
-import org.apache.lucene.analysis.StopFilter;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
 import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;
 import org.apache.lucene.analysis.tokenattributes.PayloadAttribute;
 import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
-import org.apache.lucene.analysis.CharArraySet;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.index.IndexWriter;
@@ -44,7 +40,6 @@
 import org.apache.lucene.queryParser.QueryParser;
 import org.apache.lucene.store.MockRAMDirectory;
 import org.apache.lucene.store.Directory;
-import org.apache.lucene.analysis.LowerCaseTokenizer;
 import org.apache.lucene.analysis.TokenFilter;
 import org.apache.lucene.index.Payload;
 import org.apache.lucene.search.payloads.PayloadSpanUtil;
@@ -52,9 +47,7 @@
 import org.apache.lucene.search.spans.SpanQuery;
 import org.apache.lucene.search.spans.SpanTermQuery;
 import org.apache.lucene.search.spans.Spans;
-import org.apache.lucene.util.Version;
 import org.apache.lucene.util.LuceneTestCase;
-import org.apache.lucene.util.automaton.BasicAutomata;
 import org.apache.lucene.util.automaton.CharacterRunAutomaton;
 import org.apache.lucene.util.automaton.RegExp;
 
Index: lucene/src/test/org/apache/lucene/search/TestSloppyPhraseQuery.java
===================================================================
--- lucene/src/test/org/apache/lucene/search/TestSloppyPhraseQuery.java	(revision 947868)
+++ lucene/src/test/org/apache/lucene/search/TestSloppyPhraseQuery.java	(working copy)
@@ -20,7 +20,6 @@
 import org.apache.lucene.util.LuceneTestCase;
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.analysis.MockTokenizer;
-import org.apache.lucene.analysis.WhitespaceAnalyzer;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.index.IndexWriter;
Index: lucene/src/test/org/apache/lucene/search/TestMultiTermConstantScore.java
===================================================================
--- lucene/src/test/org/apache/lucene/search/TestMultiTermConstantScore.java	(revision 947868)
+++ lucene/src/test/org/apache/lucene/search/TestMultiTermConstantScore.java	(working copy)
@@ -19,8 +19,6 @@
 
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.analysis.MockTokenizer;
-import org.apache.lucene.analysis.SimpleAnalyzer;
-import org.apache.lucene.analysis.WhitespaceAnalyzer;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.index.IndexReader;
Index: lucene/src/test/org/apache/lucene/search/TestTermVectors.java
===================================================================
--- lucene/src/test/org/apache/lucene/search/TestTermVectors.java	(revision 947868)
+++ lucene/src/test/org/apache/lucene/search/TestTermVectors.java	(working copy)
@@ -20,7 +20,6 @@
 import org.apache.lucene.util.LuceneTestCase;
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.analysis.MockTokenizer;
-import org.apache.lucene.analysis.SimpleAnalyzer;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.index.*;
Index: lucene/src/test/org/apache/lucene/search/TestRegexpRandom2.java
===================================================================
--- lucene/src/test/org/apache/lucene/search/TestRegexpRandom2.java	(revision 947868)
+++ lucene/src/test/org/apache/lucene/search/TestRegexpRandom2.java	(working copy)
@@ -20,7 +20,8 @@
 import java.io.IOException;
 import java.util.Random;
 
-import org.apache.lucene.analysis.KeywordAnalyzer;
+import org.apache.lucene.analysis.MockAnalyzer;
+import org.apache.lucene.analysis.MockTokenizer;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.index.IndexReader;
@@ -50,7 +51,7 @@
     super.setUp();
     random = newRandom();
     RAMDirectory dir = new RAMDirectory();
-    IndexWriter writer = new IndexWriter(dir, new KeywordAnalyzer(),
+    IndexWriter writer = new IndexWriter(dir, new MockAnalyzer(MockTokenizer.KEYWORD, false),
         IndexWriter.MaxFieldLength.UNLIMITED);
     
     Document doc = new Document();
Index: lucene/src/test/org/apache/lucene/index/TestWordlistLoader.java (deleted)
===================================================================
Index: lucene/src/test/org/apache/lucene/index/TestIndexWriter.java
===================================================================
--- lucene/src/test/org/apache/lucene/index/TestIndexWriter.java	(revision 947868)
+++ lucene/src/test/org/apache/lucene/index/TestIndexWriter.java	(working copy)
@@ -42,7 +42,6 @@
 import org.apache.lucene.analysis.MockTokenizer;
 import org.apache.lucene.analysis.TokenFilter;
 import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.standard.StandardAnalyzer;
 import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
 import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;
 import org.apache.lucene.document.Document;
@@ -538,67 +537,6 @@
       }
     }
 
-    /**
-     * Make sure we skip wicked long terms.
-    */
-    public void testWickedLongTerm() throws IOException {
-      RAMDirectory dir = new RAMDirectory();
-      IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(
-        TEST_VERSION_CURRENT, new StandardAnalyzer(TEST_VERSION_CURRENT)));
-
-      char[] chars = new char[DocumentsWriter.MAX_TERM_LENGTH_UTF8];
-      Arrays.fill(chars, 'x');
-      Document doc = new Document();
-      final String bigTerm = new String(chars);
-
-      // This produces a too-long term:
-      String contents = "abc xyz x" + bigTerm + " another term";
-      doc.add(new Field("content", contents, Field.Store.NO, Field.Index.ANALYZED));
-      writer.addDocument(doc);
-
-      // Make sure we can add another normal document
-      doc = new Document();
-      doc.add(new Field("content", "abc bbb ccc", Field.Store.NO, Field.Index.ANALYZED));
-      writer.addDocument(doc);
-      writer.close();
-
-      IndexReader reader = IndexReader.open(dir, true);
-
-      // Make sure all terms < max size were indexed
-      assertEquals(2, reader.docFreq(new Term("content", "abc")));
-      assertEquals(1, reader.docFreq(new Term("content", "bbb")));
-      assertEquals(1, reader.docFreq(new Term("content", "term")));
-      assertEquals(1, reader.docFreq(new Term("content", "another")));
-
-      // Make sure position is still incremented when
-      // massive term is skipped:
-      TermPositions tps = reader.termPositions(new Term("content", "another"));
-      assertTrue(tps.next());
-      assertEquals(1, tps.freq());
-      assertEquals(3, tps.nextPosition());
-
-      // Make sure the doc that has the massive term is in
-      // the index:
-      assertEquals("document with wicked long term should is not in the index!", 2, reader.numDocs());
-
-      reader.close();
-
-      // Make sure we can add a document with exactly the
-      // maximum length term, and search on that term:
-      doc = new Document();
-      doc.add(new Field("content", bigTerm, Field.Store.NO, Field.Index.ANALYZED));
-      StandardAnalyzer sa = new StandardAnalyzer(TEST_VERSION_CURRENT);
-      sa.setMaxTokenLength(100000);
-      writer  = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, sa));
-      writer.addDocument(doc);
-      writer.close();
-      reader = IndexReader.open(dir, true);
-      assertEquals(1, reader.docFreq(new Term("content", bigTerm)));
-      reader.close();
-
-      dir.close();
-    }
-
     public void testOptimizeMaxNumSegments() throws IOException {
 
       MockRAMDirectory dir = new MockRAMDirectory();
Index: lucene/src/test/org/apache/lucene/index/TestDocumentWriter.java
===================================================================
--- lucene/src/test/org/apache/lucene/index/TestDocumentWriter.java	(revision 947868)
+++ lucene/src/test/org/apache/lucene/index/TestDocumentWriter.java	(working copy)
@@ -25,8 +25,6 @@
 import org.apache.lucene.analysis.MockTokenizer;
 import org.apache.lucene.analysis.TokenFilter;
 import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.MockAnalyzer;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
 import org.apache.lucene.analysis.tokenattributes.PayloadAttribute;
 import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;
 import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
Index: lucene/src/test/org/apache/lucene/index/TestPayloads.java
===================================================================
--- lucene/src/test/org/apache/lucene/index/TestPayloads.java	(revision 947868)
+++ lucene/src/test/org/apache/lucene/index/TestPayloads.java	(working copy)
@@ -32,7 +32,6 @@
 import org.apache.lucene.analysis.TokenFilter;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.MockAnalyzer;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
 import org.apache.lucene.analysis.tokenattributes.PayloadAttribute;
 import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
 import org.apache.lucene.document.Document;
Index: lucene/src/java/org/apache/lucene/queryParser/QueryParser.java
===================================================================
--- lucene/src/java/org/apache/lucene/queryParser/QueryParser.java	(revision 947868)
+++ lucene/src/java/org/apache/lucene/queryParser/QueryParser.java	(working copy)
@@ -1082,22 +1082,6 @@
     return sb.toString();
   }
 
-  /**
-   * Command line tool to test QueryParser, using {@link org.apache.lucene.analysis.SimpleAnalyzer}.
-   * Usage:<br>
-   * <code>java org.apache.lucene.queryParser.QueryParser &lt;input&gt;</code>
-   */
-  public static void main(String[] args) throws Exception {
-    if (args.length == 0) {
-      System.out.println("Usage: java org.apache.lucene.queryParser.QueryParser <input>");
-      System.exit(0);
-    }
-    QueryParser qp = new QueryParser(Version.LUCENE_CURRENT, "field",
-                           new org.apache.lucene.analysis.SimpleAnalyzer());
-    Query q = qp.parse(args[0]);
-    System.out.println(q.toString("field"));
-  }
-
 // *   Query  ::= ( Clause )*
 // *   Clause ::= ["+", "-"] [<TERM> ":"] ( <TERM> | "(" Query ")" )
   final public int Conjunction() throws ParseException {
@@ -1802,4 +1786,19 @@
     JJCalls next;
   }
 
+  /**
+   * Command line tool to test QueryParser, using {@link org.apache.lucene.analysis.SimpleAnalyzer}.
+   * Usage:<br>
+   * <code>java org.apache.lucene.queryParser.QueryParser &lt;input&gt;</code>
+   */
+//  public static void main(String[] args) throws Exception {
+//    if (args.length == 0) {
+//      System.out.println("Usage: java org.apache.lucene.queryParser.QueryParser <input>");
+//      System.exit(0);
+//    }
+//    QueryParser qp = new QueryParser(Version.LUCENE_CURRENT, "field",
+//                           new org.apache.lucene.analysis.SimpleAnalyzer());
+//    Query q = qp.parse(args[0]);
+//    System.out.println(q.toString("field"));
+//  }
 }
Index: lucene/src/java/org/apache/lucene/queryParser/QueryParser.jj
===================================================================
--- lucene/src/java/org/apache/lucene/queryParser/QueryParser.jj	(revision 947868)
+++ lucene/src/java/org/apache/lucene/queryParser/QueryParser.jj	(working copy)
@@ -1111,16 +1111,16 @@
    * Usage:<br>
    * <code>java org.apache.lucene.queryParser.QueryParser &lt;input&gt;</code>
    */
-  public static void main(String[] args) throws Exception {
-    if (args.length == 0) {
-      System.out.println("Usage: java org.apache.lucene.queryParser.QueryParser <input>");
-      System.exit(0);
-    }
-    QueryParser qp = new QueryParser(Version.LUCENE_CURRENT, "field",
-                           new org.apache.lucene.analysis.SimpleAnalyzer());
-    Query q = qp.parse(args[0]);
-    System.out.println(q.toString("field"));
-  }
+//  public static void main(String[] args) throws Exception {
+//    if (args.length == 0) {
+//      System.out.println("Usage: java org.apache.lucene.queryParser.QueryParser <input>");
+//      System.exit(0);
+//    }
+//    QueryParser qp = new QueryParser(Version.LUCENE_CURRENT, "field",
+//                           new org.apache.lucene.analysis.SimpleAnalyzer());
+//    Query q = qp.parse(args[0]);
+//    System.out.println(q.toString("field"));
+//  }
 }
 
 PARSER_END(QueryParser)
Index: lucene/src/java/org/apache/lucene/analysis/SimpleAnalyzer.java (deleted)
===================================================================
Index: lucene/src/java/org/apache/lucene/analysis/WhitespaceAnalyzer.java (deleted)
===================================================================
Index: lucene/src/java/org/apache/lucene/analysis/ReusableAnalyzerBase.java (deleted)
===================================================================
Index: lucene/src/java/org/apache/lucene/analysis/KeywordTokenizer.java (deleted)
===================================================================
Index: lucene/src/java/org/apache/lucene/analysis/LowerCaseTokenizer.java (deleted)
===================================================================
Index: lucene/src/java/org/apache/lucene/analysis/standard/StandardTokenizerImpl31.java (deleted)
===================================================================
Index: lucene/src/java/org/apache/lucene/analysis/standard/READ_BEFORE_REGENERATING.txt (deleted)
===================================================================
Index: lucene/src/java/org/apache/lucene/analysis/standard/StandardTokenizerImplOrig.jflex (deleted)
===================================================================
Index: lucene/src/java/org/apache/lucene/analysis/standard/StandardAnalyzer.java (deleted)
===================================================================
Index: lucene/src/java/org/apache/lucene/analysis/standard/StandardFilter.java (deleted)
===================================================================
Index: lucene/src/java/org/apache/lucene/analysis/standard/StandardTokenizerImpl31.jflex (deleted)
===================================================================
Index: lucene/src/java/org/apache/lucene/analysis/standard/StandardTokenizer.java (deleted)
===================================================================
Index: lucene/src/java/org/apache/lucene/analysis/standard/StandardTokenizerInterface.java (deleted)
===================================================================
Index: lucene/src/java/org/apache/lucene/analysis/standard/package.html (deleted)
===================================================================
Index: lucene/src/java/org/apache/lucene/analysis/standard/StandardTokenizerImplOrig.java (deleted)
===================================================================
Index: lucene/src/java/org/apache/lucene/analysis/StopwordAnalyzerBase.java (deleted)
===================================================================
Index: lucene/src/java/org/apache/lucene/analysis/WhitespaceTokenizer.java (deleted)
===================================================================
Index: lucene/src/java/org/apache/lucene/analysis/KeywordAnalyzer.java (deleted)
===================================================================
Index: lucene/src/java/org/apache/lucene/analysis/LetterTokenizer.java (deleted)
===================================================================
Index: lucene/src/java/org/apache/lucene/analysis/LowerCaseFilter.java (deleted)
===================================================================
Index: lucene/src/java/org/apache/lucene/analysis/StopAnalyzer.java (deleted)
===================================================================
Index: lucene/src/java/org/apache/lucene/analysis/StopFilter.java (deleted)
===================================================================
Index: lucene/src/java/org/apache/lucene/analysis/TokenStream.java
===================================================================
--- lucene/src/java/org/apache/lucene/analysis/TokenStream.java	(revision 947868)
+++ lucene/src/java/org/apache/lucene/analysis/TokenStream.java	(working copy)
@@ -156,7 +156,7 @@
    * This method can be used to perform any end-of-stream operations, such as
    * setting the final offset of a stream. The final offset of a stream might
    * differ from the offset of the last token eg in case one or more whitespaces
-   * followed after the last token, but a {@link WhitespaceTokenizer} was used.
+   * followed after the last token, but a WhitespaceTokenizer was used.
    * 
    * @throws IOException
    */
Index: lucene/src/java/org/apache/lucene/analysis/WordlistLoader.java (deleted)
===================================================================
Index: lucene/src/java/org/apache/lucene/collation/CollationKeyAnalyzer.java (deleted)
===================================================================
Index: lucene/src/java/org/apache/lucene/collation/CollationKeyFilter.java (deleted)
===================================================================
Index: lucene/src/java/org/apache/lucene/collation/package.html (deleted)
===================================================================
Index: lucene/src/demo/org/apache/lucene/demo/HTMLDocument.java (deleted)
===================================================================
Index: lucene/src/demo/org/apache/lucene/demo/html/Test.java (deleted)
===================================================================
Index: lucene/src/demo/org/apache/lucene/demo/html/Token.java (deleted)
===================================================================
Index: lucene/src/demo/org/apache/lucene/demo/html/TokenMgrError.java (deleted)
===================================================================
Index: lucene/src/demo/org/apache/lucene/demo/html/HTMLParser.java (deleted)
===================================================================
Index: lucene/src/demo/org/apache/lucene/demo/html/HTMLParser.jj (deleted)
===================================================================
Index: lucene/src/demo/org/apache/lucene/demo/html/SimpleCharStream.java (deleted)
===================================================================
Index: lucene/src/demo/org/apache/lucene/demo/html/Entities.java (deleted)
===================================================================
Index: lucene/src/demo/org/apache/lucene/demo/html/ParserThread.java (deleted)
===================================================================
Index: lucene/src/demo/org/apache/lucene/demo/html/ParseException.java (deleted)
===================================================================
Index: lucene/src/demo/org/apache/lucene/demo/html/HTMLParserTokenManager.java (deleted)
===================================================================
Index: lucene/src/demo/org/apache/lucene/demo/html/HTMLParserConstants.java (deleted)
===================================================================
Index: lucene/src/demo/org/apache/lucene/demo/html/Tags.java (deleted)
===================================================================
Index: lucene/src/demo/org/apache/lucene/demo/DeleteFiles.java (deleted)
===================================================================
Index: lucene/src/demo/org/apache/lucene/demo/SearchFiles.java (deleted)
===================================================================
Index: lucene/src/demo/org/apache/lucene/demo/IndexFiles.java (deleted)
===================================================================
Index: lucene/src/demo/org/apache/lucene/demo/IndexHTML.java (deleted)
===================================================================
Index: lucene/src/demo/org/apache/lucene/demo/FileDocument.java (deleted)
===================================================================
Index: lucene/src/demo/demo-build.template (deleted)
===================================================================
Index: lucene/src/jsp/configuration.jsp (deleted)
===================================================================
Index: lucene/src/jsp/index.jsp (deleted)
===================================================================
Index: lucene/src/jsp/header.jsp (deleted)
===================================================================
Index: lucene/src/jsp/WEB-INF/web.xml (deleted)
===================================================================
Index: lucene/src/jsp/footer.jsp (deleted)
===================================================================
Index: lucene/src/jsp/README.txt (deleted)
===================================================================
Index: lucene/src/jsp/results.jsp (deleted)
===================================================================
Index: lucene/build.xml
===================================================================
--- lucene/build.xml	(revision 947868)
+++ lucene/build.xml	(working copy)
@@ -23,21 +23,11 @@
 
   <import file="common-build.xml"/>
 
-  <property name="build.demo.template" value="src/demo/demo-build.template"/> 
-
-  <property name="demo.name" value="lucene-demos-${version}"/>
-  <property name="demo.war.name" value="luceneweb"/>
-
   <!-- Build classpath -->
   <path id="classpath">
     <pathelement location="${build.dir}/classes/java"/>
   </path>
 
-  <path id="demo.classpath">
-    <path refid="classpath"/>
-    <pathelement location="${build.dir}/classes/demo"/>
-  </path>
-  
   <path id="test.classpath">
   	<path refid="classpath"/>
     <path refid="junit-path"/>
@@ -57,10 +47,10 @@
               excludes="contrib/db/*/lib/,contrib/*/ext-libs/,src/site/build/,contrib/benchmark/temp/,contrib/benchmark/work/"
   />
   <patternset id="binary.build.dist.patterns"
-              includes="${final.name}.jar,${demo.war.name}.war,${demo.name}.jar,docs/,contrib/*/*.jar,contrib/*/*.war, contrib/*/*/*.jar"
+              includes="${final.name}.jar,docs/,contrib/*/*.jar,contrib/*/*.war, contrib/*/*/*.jar"
   />
   <patternset id="binary.root.dist.patterns"
-              includes="src/demo/,src/jsp/,docs/,*.txt,contrib/*/README*,**/CHANGES.txt,lib/servlet-api-*.jar"
+              includes="docs/,*.txt,contrib/*/README*,**/CHANGES.txt"
               excludes="${build.demo.template}"
   />
 
@@ -177,71 +167,8 @@
   <!--                                                                    -->
   <!-- ================================================================== -->
 
-  <target name="jar-demo" depends="compile-demo">
-  	<sequential>
-  	  <build-manifest title="Lucene Search Engine: demos"/>
-      <jar
-        destfile="${build.dir}/${demo.name}.jar"
-        basedir="${build.dir}/classes/demo"
-        excludes="**/*.java"
-      	manifest="${manifest.file}">
-        <metainf dir="${common.dir}">
-          <include name="LICENSE.txt"/>
-          <include name="NOTICE.txt"/>
-        </metainf>
-      </jar>
-  	</sequential>
-  </target>
-
-  <target name="jar-demo-src" depends="compile-demo">
-  	<sequential>
-  	  <build-manifest title="Lucene Search Engine: demos"/>
-      <jar
-        destfile="${build.dir}/${demo.name}-src.jar"
-        basedir="src/demo"
-      	manifest="${manifest.file}">
-        <metainf dir="${common.dir}">
-          <include name="LICENSE.txt"/>
-          <include name="NOTICE.txt"/>
-        </metainf>
-      </jar>
-  	</sequential>
-  </target>
-
-  <target name="war-demo" depends="jar-core,jar-demo">
-    <sequential>
-      <build-manifest title="Lucene Search Engine: demos"/>
-  	  <war destfile="${build.dir}/${demo.war.name}.war"
-           webxml="src/jsp/WEB-INF/web.xml"
-      	   manifest="${manifest.file}">
-        <fileset dir="src/jsp" excludes="WEB-INF/web.xml"/>
-        <lib dir="${build.dir}" includes="${demo.name}.jar"/>
-        <lib dir="${build.dir}" includes="${final.name}.jar"/>
-        <metainf dir="${common.dir}">
-          <include name="LICENSE.txt"/>
-          <include name="NOTICE.txt"/>
-        </metainf>
-      </war>
-    </sequential>
-  </target>
-
   <target name="compile-core" depends="jflex-notice, javacc-notice, common.compile-core"/>
-  
-  <!-- ================================================================== -->
-  <!-- B U I L D  D E M O                                                 -->
-  <!-- ================================================================== -->
-  <!--                                                                    -->
-  <!-- ================================================================== -->
-  <target name="compile-demo" depends="compile-core">
-    <mkdir dir="${build.dir}/classes/demo"/>
 
-    <compile
-      srcdir="src/demo"
-      destdir="${build.dir}/classes/demo">
-      <classpath refid="demo.classpath"/>
-    </compile>
-  </target>
-
   <!-- ================================================================== -->
   <!-- D O C U M E N T A T I O N                                          -->
   <!-- ================================================================== -->
@@ -252,7 +179,7 @@
   </target>
 
   <target name="javadocs" description="Generate javadoc" 
-          depends="javadocs-all, javadocs-core, javadocs-demo, javadocs-contrib">
+          depends="javadocs-all, javadocs-core, javadocs-contrib">
     <echo file="${javadoc.dir}/index.html" append="false">
 <![CDATA[<html><head><title>${Name} ${version} Javadoc Index</title></head>
 <body>
@@ -266,7 +193,6 @@
     <contrib-crawl target="javadocs-index.html" failonerror="false"/>
     <echo file="${javadoc.dir}/index.html" append="true"><![CDATA[
   </ul>
-  <li><a href="demo/index.html">Demo</a></li>
 </ul></body>]]></echo>
   </target>
 	
@@ -285,27 +211,12 @@
     </sequential>
   </target>
 
-  <target name="javadocs-demo" description="Generate javadoc for demo classes">
-  	<sequential>
-      <mkdir dir="${javadoc.dir}/demo"/>
-      <invoke-javadoc
-        destdir="${javadoc.dir}/demo"
-      	title="${Name} ${version} demo API">
-        <sources>
-          <packageset dir="src/demo"/>
-          <link href=""/>
-        </sources>
-      </invoke-javadoc>
-      <jarify basedir="${javadoc.dir}/demo" destfile="${build.dir}/${demo.name}-javadoc.jar"/>
-    </sequential>
-  </target>
-	
   <target name="javadocs-contrib" description="Generate javadoc for contrib classes">
     <contrib-crawl target="javadocs"
                    failonerror="false"/>
   </target>
   	
-  <target name="javadocs-all" description="Generate javadoc for core, demo and contrib classes" depends="build-contrib">
+  <target name="javadocs-all" description="Generate javadoc for core and contrib classes" depends="build-contrib">
   	<sequential>
       <mkdir dir="${javadoc.dir}/all"/>
       <invoke-javadoc
@@ -314,8 +225,6 @@
           <!-- TODO: find a dynamic way to do include multiple source roots -->
           <packageset dir="src/java"/>
 
-          <packageset dir="src/demo"/>
-
           <!-- please keep this list up to date, and in alpha order...   -->
         
           <!-- ie: `find contrib/* -path \*src/java | sort` -->
@@ -348,11 +257,10 @@
           <!-- packages are not being matched by any of these rules   -->
   
           <group title="Core" packages="org.apache.*:org.apache.lucene.analysis:org.apache.lucene.analysis.standard*:org.apache.lucene.analysis.tokenattributes*"/>
-  
-          <group title="Demo" packages="org.apache.lucene.demo*"/>
-  
+    
           <group title="contrib: Ant" packages="org.apache.lucene.ant*"/>
           <group title="contrib: Benchmark" packages="org.apache.lucene.benchmark*"/>
+          <group title="contrib: Demo" packages="org.apache.lucene.demo*"/>
           <group title="contrib: ICU" packages="org.apache.lucene.collation*"/>
           <group title="contrib: DB" packages="org.apache.lucene.store.db*:org.apache.lucene.store.je*:com.sleepycat*"/>
           <group title="contrib: Highlighter" packages="org.apache.lucene.search.highlight:*org.apache.lucene.search.vectorhighlight*"/>
@@ -379,7 +287,7 @@
   <!-- ================================================================== -->
   <!--                                                                    -->
   <!-- ================================================================== -->
-  <target name="package" depends="jar-core, javadocs, war-demo, build-contrib, init-dist, changes-to-html">
+  <target name="package" depends="jar-core, javadocs, build-contrib, init-dist, changes-to-html">
      <copy file="${build.demo.template}" tofile="${build.dir}/build-demo.xml">
         <filterset begintoken="@PLACEHOLDER_" endtoken="@"> 
 	  <filter token="version" value="${version}"/>
@@ -518,7 +426,7 @@
 
   <target name="dist-all" depends="dist, dist-src"/>
 
-  <target name="generate-maven-artifacts" depends="maven.ant.tasks-check, package, jar-src, jar-demo-src, javadocs">
+  <target name="generate-maven-artifacts" depends="maven.ant.tasks-check, package, jar-src, javadocs">
     <sequential>
       <m2-deploy pom.xml="lucene-parent-pom.xml.template"/>
       <m2-deploy pom.xml="lucene-core-pom.xml.template">
@@ -529,16 +437,7 @@
                   classifier="javadoc"/>
         </artifact-attachments>
       </m2-deploy>
-      
-      <m2-deploy pom.xml="lucene-demos-pom.xml.template">
-        <artifact-attachments>
-          <attach file="${build.dir}/${demo.name}-src.jar"
-                  classifier="sources"/>
-          <attach file="${build.dir}/${demo.name}-javadoc.jar"
-                  classifier="javadoc"/>
-        </artifact-attachments>
-      </m2-deploy>
-      
+           
       <m2-deploy pom.xml="lucene-contrib-pom.xml.template"/>
       <contrib-crawl target="dist-maven"/>
     </sequential>
@@ -604,13 +503,10 @@
       <fileset dir="contrib/queryparser/src/java/org/apache/lucene/queryParser/standard/parser" includes="*.java">
         <containsregexp expression="Generated.*By.*JavaCC"/>
       </fileset>
-      <fileset dir="src/demo/org/apache/lucene/demo/html" includes="*.java">
-        <containsregexp expression="Generated.*By.*JavaCC"/>
-      </fileset>
     </delete>
   </target>
 
-  <target name="javacc" depends="init,javacc-check,clean-javacc,javacc-QueryParser,javacc-HTMLParser,javacc-contrib-queryparser"/>
+  <target name="javacc" depends="init,javacc-check,clean-javacc,javacc-QueryParser,javacc-contrib-queryparser"/>
 
   <target name="javacc-QueryParser" depends="init,javacc-check" if="javacc.present">
     <sequential>
@@ -629,12 +525,6 @@
 
     </sequential>
   </target>	
-  
-  <target name="javacc-HTMLParser" depends="init,javacc-check" if="javacc.present">
-    <invoke-javacc target="src/demo/org/apache/lucene/demo/html/HTMLParser.jj"
-                   outputDir="src/demo/org/apache/lucene/demo/html"
-    />
-  </target>
 	
   <target name="javacc-contrib-queryparser" depends="init,javacc-check" if="javacc.present">
     <ant target="javacc"
@@ -642,34 +532,7 @@
       antfile="build.xml" 
     />
   </target>
-  
-  <!-- ================================================================== -->
-  <!-- Build the JFlex files into the source tree                         -->
-  <!-- ================================================================== -->
 
-  <target name="jflex" depends="jflex-check, clean-jflex,jflex-StandardAnalyzer" />
-
-  <target name="jflex-StandardAnalyzer" depends="init,jflex-check" if="jflex.present">
-    <taskdef classname="jflex.anttask.JFlexTask" name="jflex">
-			<classpath refid="jflex.classpath"/>
-    </taskdef>
-
-    <jflex file="src/java/org/apache/lucene/analysis/standard/StandardTokenizerImplOrig.jflex"
-           outdir="src/java/org/apache/lucene/analysis/standard"
-           nobak="on" />
-    <jflex file="src/java/org/apache/lucene/analysis/standard/StandardTokenizerImpl31.jflex"
-           outdir="src/java/org/apache/lucene/analysis/standard"
-           nobak="on" />
-  </target>
-
-  <target name="clean-jflex">
-    <delete>
-      <fileset dir="src/java/org/apache/lucene/analysis/standard" includes="*.java">
-        <containsregexp expression="generated.*by.*JFlex"/>
-      </fileset>
-    </delete>
-  </target>
-
   <macrodef name="createLevAutomaton">
   	<attribute name="n"/>
   	<sequential>
Index: lucene/contrib/queryparser/src/test/org/apache/lucene/queryParser/standard/TestQPHelper.java
===================================================================
--- lucene/contrib/queryparser/src/test/org/apache/lucene/queryParser/standard/TestQPHelper.java	(revision 947868)
+++ lucene/contrib/queryparser/src/test/org/apache/lucene/queryParser/standard/TestQPHelper.java	(working copy)
@@ -32,7 +32,6 @@
 import java.util.Map;
 
 import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.analysis.KeywordAnalyzer;
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.analysis.MockTokenFilter;
 import org.apache.lucene.analysis.MockTokenizer;
@@ -341,8 +340,9 @@
         "t�rm term term");
     assertQueryEquals("�mlaut", new MockAnalyzer(MockTokenizer.WHITESPACE, false), "�mlaut");
 
-    assertQueryEquals("\"\"", new KeywordAnalyzer(), "");
-    assertQueryEquals("foo:\"\"", new KeywordAnalyzer(), "foo:");
+    // FIXME: change MockAnalyzer to not extend CharTokenizer for this test
+    //assertQueryEquals("\"\"", new KeywordAnalyzer(), "");
+    //assertQueryEquals("foo:\"\"", new KeywordAnalyzer(), "foo:");
 
     assertQueryEquals("a AND b", null, "+a +b");
     assertQueryEquals("(a AND b)", null, "+a +b");
Index: lucene/contrib/queryparser/src/test/org/apache/lucene/queryParser/standard/TestMultiAnalyzerQPHelper.java
===================================================================
--- lucene/contrib/queryparser/src/test/org/apache/lucene/queryParser/standard/TestMultiAnalyzerQPHelper.java	(revision 947868)
+++ lucene/contrib/queryparser/src/test/org/apache/lucene/queryParser/standard/TestMultiAnalyzerQPHelper.java	(working copy)
@@ -20,11 +20,9 @@
 import java.io.Reader;
 
 import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.analysis.LowerCaseFilter;
 import org.apache.lucene.analysis.MockTokenizer;
 import org.apache.lucene.analysis.TokenFilter;
 import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.standard.StandardTokenizer;
 import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
 import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;
 import org.apache.lucene.analysis.tokenattributes.TermAttribute;
Index: lucene/contrib/queryparser/src/test/org/apache/lucene/queryParser/standard/TestQueryParserWrapper.java
===================================================================
--- lucene/contrib/queryparser/src/test/org/apache/lucene/queryParser/standard/TestQueryParserWrapper.java	(revision 947868)
+++ lucene/contrib/queryparser/src/test/org/apache/lucene/queryParser/standard/TestQueryParserWrapper.java	(working copy)
@@ -30,7 +30,6 @@
 import java.util.Locale;
 
 import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.analysis.KeywordAnalyzer;
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.analysis.MockTokenFilter;
 import org.apache.lucene.analysis.MockTokenizer;
@@ -333,8 +332,9 @@
         "t�rm term term");
     assertQueryEquals("�mlaut", new MockAnalyzer(MockTokenizer.WHITESPACE, false), "�mlaut");
 
-    assertQueryEquals("\"\"", new KeywordAnalyzer(), "");
-    assertQueryEquals("foo:\"\"", new KeywordAnalyzer(), "foo:");
+    //FIXME: Change MockAnalyzer to not extend CharTokenizer for this test
+    //assertQueryEquals("\"\"", new KeywordAnalyzer(), "");
+    //assertQueryEquals("foo:\"\"", new KeywordAnalyzer(), "foo:");
 
     assertQueryEquals("a AND b", null, "+a +b");
     assertQueryEquals("(a AND b)", null, "+a +b");
Index: lucene/contrib/queryparser/src/java/org/apache/lucene/queryParser/precedence/PrecedenceQueryParser.java
===================================================================
--- lucene/contrib/queryparser/src/java/org/apache/lucene/queryParser/precedence/PrecedenceQueryParser.java	(revision 947868)
+++ lucene/contrib/queryparser/src/java/org/apache/lucene/queryParser/precedence/PrecedenceQueryParser.java	(working copy)
@@ -577,22 +577,6 @@
     return sb.toString();
   }
 
-  /**
-   * Command line tool to test QueryParser, using {@link org.apache.lucene.analysis.SimpleAnalyzer}.
-   * Usage:<br>
-   * <code>java org.apache.lucene.queryParser.QueryParser &lt;input&gt;</code>
-   */
-  public static void main(String[] args) throws Exception {
-    if (args.length == 0) {
-      System.out.println("Usage: java org.apache.lucene.queryParser.QueryParser <input>");
-      System.exit(0);
-    }
-    PrecedenceQueryParser qp = new PrecedenceQueryParser("field",
-                           new org.apache.lucene.analysis.SimpleAnalyzer());
-    Query q = qp.parse(args[0]);
-    System.out.println(q.toString("field"));
-  }
-
 // *   Query  ::= ( Clause )*
 // *   Clause ::= ["+", "-"] [<TERM> ":"] ( <TERM> | "(" Query ")" )
   final public int Conjunction() throws ParseException {
@@ -1290,4 +1274,19 @@
     JJCalls next;
   }
 
+  /**
+   * Command line tool to test QueryParser, using {@link org.apache.lucene.analysis.SimpleAnalyzer}.
+   * Usage:<br>
+   * <code>java org.apache.lucene.queryParser.QueryParser &lt;input&gt;</code>
+   */
+//  public static void main(String[] args) throws Exception {
+//    if (args.length == 0) {
+//      System.out.println("Usage: java org.apache.lucene.queryParser.QueryParser <input>");
+//      System.exit(0);
+//    }
+//    PrecedenceQueryParser qp = new PrecedenceQueryParser("field",
+//                           new org.apache.lucene.analysis.SimpleAnalyzer());
+//    Query q = qp.parse(args[0]);
+//    System.out.println(q.toString("field"));
+//  }
 }
Index: lucene/contrib/queryparser/src/java/org/apache/lucene/queryParser/precedence/PrecedenceQueryParser.jj
===================================================================
--- lucene/contrib/queryparser/src/java/org/apache/lucene/queryParser/precedence/PrecedenceQueryParser.jj	(revision 947868)
+++ lucene/contrib/queryparser/src/java/org/apache/lucene/queryParser/precedence/PrecedenceQueryParser.jj	(working copy)
@@ -606,16 +606,16 @@
    * Usage:<br>
    * <code>java org.apache.lucene.queryParser.QueryParser &lt;input&gt;</code>
    */
-  public static void main(String[] args) throws Exception {
-    if (args.length == 0) {
-      System.out.println("Usage: java org.apache.lucene.queryParser.QueryParser <input>");
-      System.exit(0);
-    }
-    PrecedenceQueryParser qp = new PrecedenceQueryParser("field",
-                           new org.apache.lucene.analysis.SimpleAnalyzer());
-    Query q = qp.parse(args[0]);
-    System.out.println(q.toString("field"));
-  }
+//  public static void main(String[] args) throws Exception {
+//    if (args.length == 0) {
+//      System.out.println("Usage: java org.apache.lucene.queryParser.QueryParser <input>");
+//      System.exit(0);
+//    }
+//    PrecedenceQueryParser qp = new PrecedenceQueryParser("field",
+//                           new org.apache.lucene.analysis.SimpleAnalyzer());
+//    Query q = qp.parse(args[0]);
+//    System.out.println(q.toString("field"));
+//  }
 }
 
 PARSER_END(PrecedenceQueryParser)
Index: lucene/contrib/ant/src/test/org/apache/lucene/ant/IndexTaskTest.java
===================================================================
--- lucene/contrib/ant/src/test/org/apache/lucene/ant/IndexTaskTest.java	(revision 947868)
+++ lucene/contrib/ant/src/test/org/apache/lucene/ant/IndexTaskTest.java	(working copy)
@@ -21,7 +21,7 @@
 import java.io.IOException;
 
 import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.analysis.StopAnalyzer;
+import org.apache.lucene.analysis.core.StopAnalyzer;
 import org.apache.lucene.queryParser.QueryParser;
 import org.apache.lucene.search.IndexSearcher;
 import org.apache.lucene.search.Query;
Index: lucene/contrib/ant/src/java/org/apache/lucene/ant/IndexTask.java
===================================================================
--- lucene/contrib/ant/src/java/org/apache/lucene/ant/IndexTask.java	(revision 947868)
+++ lucene/contrib/ant/src/java/org/apache/lucene/ant/IndexTask.java	(working copy)
@@ -30,9 +30,9 @@
 import java.lang.reflect.Constructor;
 
 import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.analysis.SimpleAnalyzer;
-import org.apache.lucene.analysis.StopAnalyzer;
-import org.apache.lucene.analysis.WhitespaceAnalyzer;
+import org.apache.lucene.analysis.core.SimpleAnalyzer;
+import org.apache.lucene.analysis.core.StopAnalyzer;
+import org.apache.lucene.analysis.core.WhitespaceAnalyzer;
 import org.apache.lucene.analysis.standard.StandardAnalyzer;
 import org.apache.lucene.document.DateTools;
 import org.apache.lucene.document.Document;
Index: lucene/contrib/ant/build.xml
===================================================================
--- lucene/contrib/ant/build.xml	(revision 947868)
+++ lucene/contrib/ant/build.xml	(working copy)
@@ -34,4 +34,21 @@
   />
 
   <import file="../contrib-build.xml"/>
+	
+  <module-uptodate name="analysis/common" jarfile="${common.dir}/../modules/analysis/build/common/lucene-analyzers-common-${version}.jar"
+      property="analyzers-common.uptodate" classpath.property="analyzers-common.jar"/>
+
+  <path id="classpath">
+	 <pathelement path="${analyzers-common.jar}"/>
+	 <path refid="base.classpath"/>
+  </path>
+
+  <target name="compile-core" depends="compile-analyzers-common, common.compile-core" />
+
+  <target name="compile-analyzers-common" unless="analyzers-common.uptodate">
+    <subant target="default">
+      <fileset dir="${common.dir}/../modules/analysis/common" includes="build.xml"/>
+    </subant>
+  </target>
+
 </project>
Index: lucene/contrib/wordnet/src/test/org/apache/lucene/wordnet/TestSynonymTokenFilter.java
===================================================================
--- lucene/contrib/wordnet/src/test/org/apache/lucene/wordnet/TestSynonymTokenFilter.java	(revision 947868)
+++ lucene/contrib/wordnet/src/test/org/apache/lucene/wordnet/TestSynonymTokenFilter.java	(working copy)
@@ -23,12 +23,12 @@
 import java.io.Reader;
 
 import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.analysis.LowerCaseFilter;
 import org.apache.lucene.analysis.MockTokenizer;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
+import org.apache.lucene.analysis.core.LowerCaseFilter;
+import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 
 public class TestSynonymTokenFilter extends BaseTokenStreamTestCase {
   final String testFile = "testSynonyms.txt";
Index: lucene/contrib/wordnet/build.xml
===================================================================
--- lucene/contrib/wordnet/build.xml	(revision 947868)
+++ lucene/contrib/wordnet/build.xml	(working copy)
@@ -30,6 +30,22 @@
 
   <import file="../contrib-build.xml"/>
 
+  <module-uptodate name="analysis/common" jarfile="${common.dir}/../modules/analysis/build/common/lucene-analyzers-common-${version}.jar"
+      property="analyzers-common.uptodate" classpath.property="analyzers-common.jar"/>
+
+  <path id="classpath">
+	 <pathelement path="${analyzers-common.jar}"/>
+	 <path refid="base.classpath"/>
+  </path>
+
+  <target name="compile-core" depends="compile-analyzers-common, common.compile-core" />
+
+  <target name="compile-analyzers-common" unless="analyzers-common.uptodate">
+    <subant target="default">
+      <fileset dir="${common.dir}/../modules/analysis/common" includes="build.xml"/>
+    </subant>
+  </target>
+	
   <target name="index" depends="compile" description="Build WordNet index">
     <fail if="synindex.exists">
       Index already exists - must remove first.
Index: lucene/contrib/demo/build.xml
===================================================================
--- lucene/contrib/demo/build.xml	(revision 0)
+++ lucene/contrib/demo/build.xml	(revision 0)
@@ -0,0 +1,78 @@
+<?xml version="1.0"?>
+
+<!--
+    Licensed to the Apache Software Foundation (ASF) under one or more
+    contributor license agreements.  See the NOTICE file distributed with
+    this work for additional information regarding copyright ownership.
+    The ASF licenses this file to You under the Apache License, Version 2.0
+    the "License"); you may not use this file except in compliance with
+    the License.  You may obtain a copy of the License at
+ 
+        http://www.apache.org/licenses/LICENSE-2.0
+ 
+    Unless required by applicable law or agreed to in writing, software
+    distributed under the License is distributed on an "AS IS" BASIS,
+    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+    See the License for the specific language governing permissions and
+    limitations under the License.
+ -->
+
+<project name="demo" default="default">
+
+  <description>
+    Lucene Demo
+  </description>
+
+  <property name="build.demo.template" value="src/java/demo-build.template"/> 
+
+  <property name="demo.name" value="lucene-demos-${version}"/>
+  <property name="demo.war.name" value="luceneweb"/>
+
+  <import file="../contrib-build.xml"/>
+
+  <module-uptodate name="analysis/common" jarfile="${common.dir}/../modules/analysis/build/common/lucene-analyzers-common-${version}.jar"
+      property="analyzers-common.uptodate" classpath.property="analyzers-common.jar"/>
+
+  <path id="classpath">
+	 <pathelement path="${analyzers-common.jar}"/>
+	 <path refid="base.classpath"/>
+  </path>
+
+  <target name="compile-core" depends="compile-analyzers-common, common.compile-core" />
+
+  <target name="compile-analyzers-common" unless="analyzers-common.uptodate">
+    <subant target="default">
+      <fileset dir="${common.dir}/../modules/analysis/common" includes="build.xml"/>
+    </subant>
+  </target>
+
+  <target name="war-demo" depends="jar-core">
+    <sequential>
+      <build-manifest title="Lucene Search Engine: demos"/>
+  	  <war destfile="${build.dir}/${demo.war.name}.war"
+           webxml="src/jsp/WEB-INF/web.xml"
+      	   manifest="${manifest.file}">
+        <fileset dir="src/jsp" excludes="WEB-INF/web.xml"/>
+        <lib dir="${build.dir}/../.." includes="lucene-core-${version}.jar"/>
+  	  	<lib dir="${common.dir}/../modules/analysis/build/common" includes="lucene-analyzers-common-${version}.jar"/>
+        <lib dir="${build.dir}" includes="${final.name}.jar"/>
+        <metainf dir="${common.dir}">
+          <include name="LICENSE.txt"/>
+          <include name="NOTICE.txt"/>
+        </metainf>
+      </war>
+    </sequential>
+  </target>
+	
+  <target name="clean-javacc">
+    <fileset dir="src/demo/org/apache/lucene/demo/html" includes="*.java">
+      <containsregexp expression="Generated.*By.*JavaCC"/>
+    </fileset>
+  </target>
+	
+  <target name="javacc" depends="init,javacc-check" if="javacc.present">
+    <invoke-javacc target="src/demo/org/apache/lucene/demo/html/HTMLParser.jj"
+                     outputDir="src/demo/org/apache/lucene/demo/html"
+    />
+  </target>
+</project>

Property changes on: lucene\contrib\demo\build.xml
___________________________________________________________________
Added: svn:eol-style
   + native

Index: lucene/contrib/swing/src/java/org/apache/lucene/swing/models/TableSearcher.java
===================================================================
--- lucene/contrib/swing/src/java/org/apache/lucene/swing/models/TableSearcher.java	(revision 947868)
+++ lucene/contrib/swing/src/java/org/apache/lucene/swing/models/TableSearcher.java	(working copy)
@@ -24,7 +24,7 @@
 import javax.swing.table.TableModel;
 
 import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.analysis.WhitespaceAnalyzer;
+import org.apache.lucene.analysis.core.WhitespaceAnalyzer;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.Fieldable;
Index: lucene/contrib/swing/src/java/org/apache/lucene/swing/models/ListSearcher.java
===================================================================
--- lucene/contrib/swing/src/java/org/apache/lucene/swing/models/ListSearcher.java	(revision 947868)
+++ lucene/contrib/swing/src/java/org/apache/lucene/swing/models/ListSearcher.java	(working copy)
@@ -25,7 +25,7 @@
 import javax.swing.event.ListDataListener;
 
 import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.analysis.WhitespaceAnalyzer;
+import org.apache.lucene.analysis.core.WhitespaceAnalyzer;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.Fieldable;
Index: lucene/contrib/swing/build.xml
===================================================================
--- lucene/contrib/swing/build.xml	(revision 947868)
+++ lucene/contrib/swing/build.xml	(working copy)
@@ -25,6 +25,22 @@
 
   <import file="../contrib-build.xml"/>
 
+  <module-uptodate name="analysis/common" jarfile="${common.dir}/../modules/analysis/build/common/lucene-analyzers-common-${version}.jar"
+      property="analyzers-common.uptodate" classpath.property="analyzers-common.jar"/>
+
+  <path id="classpath">
+	 <pathelement path="${analyzers-common.jar}"/>
+	 <path refid="base.classpath"/>
+  </path>
+
+  <target name="compile-core" depends="compile-analyzers-common, common.compile-core" />
+
+  <target name="compile-analyzers-common" unless="analyzers-common.uptodate">
+    <subant target="default">
+      <fileset dir="${common.dir}/../modules/analysis/common" includes="build.xml"/>
+    </subant>
+  </target>
+
   <target name="list-demo" depends="compile">
     <java classname="org.apache.lucene.swing.models.ListSearcherSimulator"
           fork="yes" spawn="yes"
Index: lucene/contrib/benchmark/build.xml
===================================================================
--- lucene/contrib/benchmark/build.xml	(revision 947868)
+++ lucene/contrib/benchmark/build.xml	(working copy)
@@ -18,6 +18,7 @@
     <module-uptodate name="analysis/common" jarfile="${common.dir}/../modules/analysis/build/common/lucene-analyzers-common-${version}.jar"
       property="analyzers-common.uptodate" classpath.property="analyzers-common.jar"/>
     <contrib-uptodate name="memory" property="memory.uptodate" classpath.property="memory.jar"/>
+    <contrib-uptodate name="demo" property="demo.uptodate" classpath.property="demo.jar"/>
 
     <target name="check-files">
         <available file="temp/news20.tar.gz" property="news20.exists"/>
@@ -139,8 +140,8 @@
       <pathelement path="${memory.jar}"/>
       <pathelement path="${highlighter.jar}"/>
       <pathelement path="${analyzers-common.jar}"/>
+      <pathelement path="${demo.jar}"/>
       <path refid="base.classpath"/>
-      <pathelement path="${common.dir}/build/classes/demo"/>
     	<fileset dir="lib">
     		<include name="**/*.jar"/>
     	</fileset>
@@ -228,9 +229,9 @@
       <echo>Benchmark output in JIRA table format is in file: ${shingle.jira.output.file}</echo>
     </target>
 
-    <target name="compile-demo">
-      <subant target="compile-demo">
-         <fileset dir="${common.dir}" includes="build.xml"/>
+    <target name="compile-demo" unless="demo.uptodate">
+      <subant target="default">
+         <fileset dir="${common.dir}/contrib/demo" includes="build.xml"/>
       </subant>
     </target>
     <target name="compile-highlighter" unless="highlighter.uptodate">
Index: lucene/contrib/misc/src/java/org/apache/lucene/index/MultiPassIndexSplitter.java
===================================================================
--- lucene/contrib/misc/src/java/org/apache/lucene/index/MultiPassIndexSplitter.java	(revision 947868)
+++ lucene/contrib/misc/src/java/org/apache/lucene/index/MultiPassIndexSplitter.java	(working copy)
@@ -21,7 +21,7 @@
 import java.io.IOException;
 import java.util.ArrayList;
 
-import org.apache.lucene.analysis.WhitespaceAnalyzer;
+import org.apache.lucene.analysis.core.WhitespaceAnalyzer;
 import org.apache.lucene.index.IndexWriterConfig.OpenMode;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.FSDirectory;
Index: lucene/contrib/misc/src/java/org/apache/lucene/misc/IndexMergeTool.java
===================================================================
--- lucene/contrib/misc/src/java/org/apache/lucene/misc/IndexMergeTool.java	(revision 947868)
+++ lucene/contrib/misc/src/java/org/apache/lucene/misc/IndexMergeTool.java	(working copy)
@@ -16,7 +16,7 @@
   * limitations under the License.
   */
 
-import org.apache.lucene.analysis.WhitespaceAnalyzer;
+import org.apache.lucene.analysis.core.WhitespaceAnalyzer;
 import org.apache.lucene.index.IndexWriter;
 import org.apache.lucene.index.IndexWriterConfig;
 import org.apache.lucene.index.IndexWriterConfig.OpenMode;
Index: lucene/contrib/misc/build.xml
===================================================================
--- lucene/contrib/misc/build.xml	(revision 947868)
+++ lucene/contrib/misc/build.xml	(working copy)
@@ -27,4 +27,19 @@
 
   <import file="../contrib-build.xml"/>
 
+  <module-uptodate name="analysis/common" jarfile="${common.dir}/../modules/analysis/build/common/lucene-analyzers-common-${version}.jar"
+      property="analyzers-common.uptodate" classpath.property="analyzers-common.jar"/>
+
+  <path id="classpath">
+	 <pathelement path="${analyzers-common.jar}"/>
+	 <path refid="base.classpath"/>
+  </path>
+
+  <target name="compile-core" depends="compile-analyzers-common, common.compile-core" />
+
+  <target name="compile-analyzers-common" unless="analyzers-common.uptodate">
+    <subant target="default">
+      <fileset dir="${common.dir}/../modules/analysis/common" includes="build.xml"/>
+    </subant>
+  </target>
 </project>
Index: lucene/contrib/highlighter/src/java/org/apache/lucene/search/vectorhighlight/FieldTermStack.java
===================================================================
--- lucene/contrib/highlighter/src/java/org/apache/lucene/search/vectorhighlight/FieldTermStack.java	(revision 947868)
+++ lucene/contrib/highlighter/src/java/org/apache/lucene/search/vectorhighlight/FieldTermStack.java	(working copy)
@@ -21,24 +21,10 @@
 import java.util.LinkedList;
 import java.util.Set;
 
-import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.analysis.WhitespaceAnalyzer;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.document.Field;
-import org.apache.lucene.document.Field.Index;
-import org.apache.lucene.document.Field.Store;
-import org.apache.lucene.document.Field.TermVector;
 import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.IndexWriter;
-import org.apache.lucene.index.IndexWriterConfig;
 import org.apache.lucene.index.TermFreqVector;
 import org.apache.lucene.index.TermPositionVector;
 import org.apache.lucene.index.TermVectorOffsetInfo;
-import org.apache.lucene.queryParser.QueryParser;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.RAMDirectory;
-import org.apache.lucene.util.Version;
 
 /**
  * <code>FieldTermStack</code> is a stack that keeps query terms in the specified field
@@ -49,24 +35,24 @@
   private final String fieldName;
   LinkedList<TermInfo> termList = new LinkedList<TermInfo>();
   
-  public static void main( String[] args ) throws Exception {
-    Analyzer analyzer = new WhitespaceAnalyzer(Version.LUCENE_CURRENT);
-    QueryParser parser = new QueryParser(Version.LUCENE_CURRENT,  "f", analyzer );
-    Query query = parser.parse( "a x:b" );
-    FieldQuery fieldQuery = new FieldQuery( query, true, false );
+  //public static void main( String[] args ) throws Exception {
+  //  Analyzer analyzer = new WhitespaceAnalyzer(Version.LUCENE_CURRENT);
+  //  QueryParser parser = new QueryParser(Version.LUCENE_CURRENT,  "f", analyzer );
+  //  Query query = parser.parse( "a x:b" );
+  //  FieldQuery fieldQuery = new FieldQuery( query, true, false );
     
-    Directory dir = new RAMDirectory();
-    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(Version.LUCENE_CURRENT, analyzer));
-    Document doc = new Document();
-    doc.add( new Field( "f", "a a a b b c a b b c d e f", Store.YES, Index.ANALYZED, TermVector.WITH_POSITIONS_OFFSETS ) );
-    doc.add( new Field( "f", "b a b a f", Store.YES, Index.ANALYZED, TermVector.WITH_POSITIONS_OFFSETS ) );
-    writer.addDocument( doc );
-    writer.close();
+  //  Directory dir = new RAMDirectory();
+  //  IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(Version.LUCENE_CURRENT, analyzer));
+  //  Document doc = new Document();
+  //  doc.add( new Field( "f", "a a a b b c a b b c d e f", Store.YES, Index.ANALYZED, TermVector.WITH_POSITIONS_OFFSETS ) );
+  //  doc.add( new Field( "f", "b a b a f", Store.YES, Index.ANALYZED, TermVector.WITH_POSITIONS_OFFSETS ) );
+  //  writer.addDocument( doc );
+  //  writer.close();
     
-    IndexReader reader = IndexReader.open( dir, true );
-    new FieldTermStack( reader, 0, "f", fieldQuery );
-    reader.close();
-  }
+  //  IndexReader reader = IndexReader.open( dir, true );
+  //  new FieldTermStack( reader, 0, "f", fieldQuery );
+  //  reader.close();
+  //}
 
   /**
    * a constructor.
Index: lucene/contrib/spellchecker/src/test/org/apache/lucene/search/spell/TestLuceneDictionary.java
===================================================================
--- lucene/contrib/spellchecker/src/test/org/apache/lucene/search/spell/TestLuceneDictionary.java	(revision 947868)
+++ lucene/contrib/spellchecker/src/test/org/apache/lucene/search/spell/TestLuceneDictionary.java	(working copy)
@@ -22,7 +22,7 @@
 
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.analysis.MockTokenizer;
-import org.apache.lucene.analysis.WhitespaceAnalyzer;
+import org.apache.lucene.analysis.core.WhitespaceAnalyzer;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.index.IndexReader;
Index: lucene/contrib/spellchecker/src/java/org/apache/lucene/search/spell/SpellChecker.java
===================================================================
--- lucene/contrib/spellchecker/src/java/org/apache/lucene/search/spell/SpellChecker.java	(revision 947868)
+++ lucene/contrib/spellchecker/src/java/org/apache/lucene/search/spell/SpellChecker.java	(working copy)
@@ -20,7 +20,7 @@
 import java.io.IOException;
 import java.util.Iterator;
 
-import org.apache.lucene.analysis.WhitespaceAnalyzer;
+import org.apache.lucene.analysis.core.WhitespaceAnalyzer;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.index.IndexReader;
Index: lucene/contrib/spellchecker/build.xml
===================================================================
--- lucene/contrib/spellchecker/build.xml	(revision 947868)
+++ lucene/contrib/spellchecker/build.xml	(working copy)
@@ -24,4 +24,20 @@
   </description>
 
   <import file="../contrib-build.xml"/>
+
+  <module-uptodate name="analysis/common" jarfile="${common.dir}/../modules/analysis/build/common/lucene-analyzers-common-${version}.jar"
+      property="analyzers-common.uptodate" classpath.property="analyzers-common.jar"/>
+
+  <path id="classpath">
+	 <pathelement path="${analyzers-common.jar}"/>
+	 <path refid="base.classpath"/>
+  </path>
+
+  <target name="compile-core" depends="compile-analyzers-common, common.compile-core" />
+
+  <target name="compile-analyzers-common" unless="analyzers-common.uptodate">
+    <subant target="default">
+      <fileset dir="${common.dir}/../modules/analysis/common" includes="build.xml"/>
+    </subant>
+  </target>
 </project>
Index: lucene/contrib/memory/src/test/org/apache/lucene/index/memory/MemoryIndexTest.java
===================================================================
--- lucene/contrib/memory/src/test/org/apache/lucene/index/memory/MemoryIndexTest.java	(revision 947868)
+++ lucene/contrib/memory/src/test/org/apache/lucene/index/memory/MemoryIndexTest.java	(working copy)
@@ -27,13 +27,9 @@
 
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
-import org.apache.lucene.analysis.KeywordAnalyzer;
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.analysis.MockTokenFilter;
 import org.apache.lucene.analysis.MockTokenizer;
-import org.apache.lucene.analysis.SimpleAnalyzer;
-import org.apache.lucene.analysis.StopAnalyzer;
-import org.apache.lucene.analysis.standard.StandardAnalyzer;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.index.IndexWriter;
Index: lucene/contrib/lucli/build.xml
===================================================================
--- lucene/contrib/lucli/build.xml	(revision 947868)
+++ lucene/contrib/lucli/build.xml	(working copy)
@@ -38,6 +38,22 @@
 
   <import file="../contrib-build.xml"/>
 
+  <module-uptodate name="analysis/common" jarfile="${common.dir}/../modules/analysis/build/common/lucene-analyzers-common-${version}.jar"
+      property="analyzers-common.uptodate" classpath.property="analyzers-common.jar"/>
+
+  <path id="classpath">
+	 <pathelement path="${analyzers-common.jar}"/>
+	 <path refid="base.classpath"/>
+  </path>
+
+  <target name="compile-core" depends="compile-analyzers-common, common.compile-core" />
+
+  <target name="compile-analyzers-common" unless="analyzers-common.uptodate">
+    <subant target="default">
+      <fileset dir="${common.dir}/../modules/analysis/common" includes="build.xml"/>
+    </subant>
+  </target>
+	
   <target name="jar" depends="compile" description="Create JAR">
     <jarify>
       <manifest-attributes>
Index: lucene/contrib/queries/src/test/org/apache/lucene/search/similar/TestMoreLikeThis.java
===================================================================
--- lucene/contrib/queries/src/test/org/apache/lucene/search/similar/TestMoreLikeThis.java	(revision 947868)
+++ lucene/contrib/queries/src/test/org/apache/lucene/search/similar/TestMoreLikeThis.java	(working copy)
@@ -24,6 +24,7 @@
 import java.util.Map;
 
 import org.apache.lucene.analysis.MockAnalyzer;
+import org.apache.lucene.analysis.MockTokenizer;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.index.IndexReader;
@@ -76,6 +77,7 @@
 
 	MoreLikeThis mlt = new MoreLikeThis(
 		reader);
+	mlt.setAnalyzer(new MockAnalyzer(MockTokenizer.WHITESPACE, false));
 	mlt.setMinDocFreq(1);
 	mlt.setMinTermFreq(1);
 	mlt.setMinWordLen(1);
@@ -110,6 +112,7 @@
     private Map<String,Float> getOriginalValues() throws IOException {
 	Map<String,Float> originalValues = new HashMap<String,Float>();
 	MoreLikeThis mlt = new MoreLikeThis(reader);
+	mlt.setAnalyzer(new MockAnalyzer(MockTokenizer.WHITESPACE, false));
 	mlt.setMinDocFreq(1);
 	mlt.setMinTermFreq(1);
 	mlt.setMinWordLen(1);
Index: lucene/contrib/queries/src/test/org/apache/lucene/search/BooleanFilterTest.java
===================================================================
--- lucene/contrib/queries/src/test/org/apache/lucene/search/BooleanFilterTest.java	(revision 947868)
+++ lucene/contrib/queries/src/test/org/apache/lucene/search/BooleanFilterTest.java	(working copy)
@@ -21,7 +21,6 @@
 
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.analysis.MockTokenizer;
-import org.apache.lucene.analysis.WhitespaceAnalyzer;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.index.IndexReader;
Index: lucene/contrib/queries/src/java/org/apache/lucene/search/similar/MoreLikeThis.java
===================================================================
--- lucene/contrib/queries/src/java/org/apache/lucene/search/similar/MoreLikeThis.java	(revision 947868)
+++ lucene/contrib/queries/src/java/org/apache/lucene/search/similar/MoreLikeThis.java	(working copy)
@@ -32,7 +32,6 @@
 
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.standard.StandardAnalyzer;
 import org.apache.lucene.analysis.tokenattributes.TermAttribute;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.index.IndexReader;
@@ -49,7 +48,6 @@
 import org.apache.lucene.search.TopDocs;
 import org.apache.lucene.store.FSDirectory;
 import org.apache.lucene.util.PriorityQueue;
-import org.apache.lucene.util.Version;
 
 
 /**
@@ -158,13 +156,6 @@
 	 */
     public static final int DEFAULT_MAX_NUM_TOKENS_PARSED=5000;
        
-
-	/**
-     * Default analyzer to parse source doc with.
-	 * @see #getAnalyzer
-     */
-    public static final Analyzer DEFAULT_ANALYZER = new StandardAnalyzer(Version.LUCENE_CURRENT);
-
     /**
      * Ignore terms with less than this frequency in the source doc.
 	 * @see #getMinTermFreq
@@ -240,7 +231,7 @@
     /**
      * Analyzer that will be used to parse the doc.
      */
-    private Analyzer analyzer = DEFAULT_ANALYZER;
+    private Analyzer analyzer = null;
 
     /**
      * Ignore words less frequent that this.
@@ -343,10 +334,9 @@
 
   /**
      * Returns an analyzer that will be used to parse source doc with. The default analyzer
-     * is the {@link #DEFAULT_ANALYZER}.
+     * is not set.
      *
      * @return the analyzer that will be used to parse source doc with.
-	 * @see #DEFAULT_ANALYZER
      */
     public Analyzer getAnalyzer() {
         return analyzer;
@@ -887,6 +877,10 @@
 	private void addTermFrequencies(Reader r, Map<String,Int> termFreqMap, String fieldName)
 		throws IOException
 	{
+	  if (analyzer == null) {
+	    throw new UnsupportedOperationException("To use MoreLikeThis without " +
+	    		"term vectors, you must provide an Analyzer");
+	  }
 		   TokenStream ts = analyzer.tokenStream(fieldName, r);
 			int tokenCount=0;
 			// for every token
Index: lucene/lib/servlet-api-2.4.jar (deleted)
===================================================================
