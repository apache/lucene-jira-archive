Index: contrib/miscellaneous/src/test/org/apache/lucene/index/TestFieldNormModifier.java
===================================================================
--- contrib/miscellaneous/src/test/org/apache/lucene/index/TestFieldNormModifier.java	(revision 495338)
+++ contrib/miscellaneous/src/test/org/apache/lucene/index/TestFieldNormModifier.java	(working copy)
@@ -18,23 +18,19 @@
  */
 
 import java.io.IOException;
-import java.util.Arrays;
 
 import junit.framework.TestCase;
 
-import org.apache.lucene.index.Term;
-import org.apache.lucene.index.IndexWriter;
-import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.analysis.SimpleAnalyzer;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.search.DefaultSimilarity;
+import org.apache.lucene.search.HitCollector;
 import org.apache.lucene.search.IndexSearcher;
 import org.apache.lucene.search.Similarity;
-import org.apache.lucene.search.DefaultSimilarity;
 import org.apache.lucene.search.TermQuery;
-import org.apache.lucene.search.HitCollector;
+import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.RAMDirectory;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.analysis.SimpleAnalyzer;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.document.Field;
 
 /**
  * Tests changing of field norms with a custom similarity and with fake norms.
@@ -42,6 +38,10 @@
  * @version $Id$
  */
 public class TestFieldNormModifier extends TestCase {
+  private static final int N_FIELDS_WITH_NUM = 10;
+
+  private static final String FIELD_WITH_NUM = "fieldNum";
+
   public TestFieldNormModifier(String name) {
     super(name);
   }
@@ -58,6 +58,14 @@
       return (float)numTokens;
     }
   };
+  /** adds e.g. 7 for field called fieldNum7 */
+  public static Similarity simWithDelta = new DefaultSimilarity() {
+    public float lengthNorm(String fieldName, int numTokens) {
+      int k = fieldName.indexOf(FIELD_WITH_NUM);
+      int delta = (k<0 ? 0 : Integer.parseInt(fieldName.substring(FIELD_WITH_NUM.length())));
+      return (float)(delta+numTokens);
+    }
+  };
   
   public void setUp() throws Exception {
     IndexWriter writer = new IndexWriter(store, new SimpleAnalyzer(), true);
@@ -66,11 +74,17 @@
       Document d = new Document();
       d.add(new Field("field", "word", Field.Store.YES, Field.Index.TOKENIZED));
       d.add(new Field("nonorm", "word", Field.Store.YES, Field.Index.NO_NORMS));
-      d.add(new Field("untokfield", "20061212 20071212", Field.Store.YES, Field.Index.TOKENIZED));
+      d.add(new Field("untokfield", "20061212 20071212", Field.Store.YES, Field.Index.UN_TOKENIZED));
+      for (int k = 0; k < N_FIELDS_WITH_NUM; k++) {
+        d.add(new Field(FIELD_WITH_NUM+k, "word", Field.Store.YES, Field.Index.UN_TOKENIZED));
+      }
       
       for (int j = 1; j <= i; j++) {
         d.add(new Field("field", "crap", Field.Store.YES, Field.Index.TOKENIZED));
         d.add(new Field("nonorm", "more words", Field.Store.YES, Field.Index.NO_NORMS));
+        for (int k = 0; k < N_FIELDS_WITH_NUM; k++) {
+          d.add(new Field(FIELD_WITH_NUM+k, "word", Field.Store.YES, Field.Index.UN_TOKENIZED));
+        }
       }
       writer.addDocument(d);
     }
@@ -162,21 +176,54 @@
     }
   }
 
-  public void testNormKiller() throws IOException {
+  public void testTokenizedFieldNormKiller() throws IOException {
+      IndexReader r = IndexReader.open(store);
+      assertTrue(r.hasNorms("field"));
+      r.close();
+      
+      FieldNormModifier fnm = new FieldNormModifier(store, null);
+      fnm.killNorms("field");
 
+      r = IndexReader.open(store);
+      assertFalse(r.hasNorms("field"));
+      r.close();
+
+      // verify that we still get documents in the same order as originally
+      IndexSearcher searcher = new IndexSearcher(store);
+      final float[] scores = new float[NUM_DOCS];
+      float lastScore = 0.0f;
+      
+      // default similarity should return the same score for all documents for this query
+      searcher.search(new TermQuery(new Term("field", "word")), new HitCollector() {
+        public final void collect(int doc, float score) {
+          scores[doc] = score;
+        }
+      });
+      searcher.close();
+      
+      lastScore = scores[0];
+      for (int i = 0; i < NUM_DOCS; i++) {
+        String msg = "i=" + i + ", " + scores[i] + " == " + lastScore;
+        assertTrue(msg, scores[i] == lastScore);
+        //System.out.println(msg);
+        lastScore = scores[i];
+      } 
+  }
+
+  public void testUnTokenizedFieldNormKiller() throws IOException {
     IndexReader r = IndexReader.open(store);
-    byte[] oldNorms = r.norms("untokfield");    
+//    byte[] oldNorms = r.norms("untokfield");
+    assertTrue(r.hasNorms("untokfield"));
     r.close();
     
-    FieldNormModifier fnm = new FieldNormModifier(store, s);
-    fnm.reSetNorms("untokfield");
+    FieldNormModifier fnm = new FieldNormModifier(store, null);
+    fnm.killNorms("untokfield");
 
     r = IndexReader.open(store);
-    byte[] newNorms = r.norms("untokfield");
+//    byte[] newNorms = r.norms("untokfield");
+    assertFalse(r.hasNorms("untokfield"));
     r.close();
-    assertFalse(Arrays.equals(oldNorms, newNorms));    
 
-    
     // verify that we still get documents in the same order as originally
     IndexSearcher searcher = new IndexSearcher(store);
     final float[] scores = new float[NUM_DOCS];
@@ -198,4 +245,57 @@
       lastScore = scores[i];
     }
   }
+  
+  public void testModifiedNormValuesCombinedWithKill() throws Exception {
+    //verify initial norms 
+    Similarity ds = new DefaultSimilarity();
+    IndexReader reader = IndexReader.open(store);
+    for (int i=0; i<N_FIELDS_WITH_NUM; i++) {
+      String fname = FIELD_WITH_NUM+i;
+      byte[] norms = reader.norms(fname);
+      for (int j = 0; j < norms.length; j++) {
+        float expectedVal = Similarity.decodeNorm(Similarity.encodeNorm(ds.lengthNorm(fname,j+1)));
+        float foundVal = Similarity.decodeNorm(norms[j]);
+        //System.out.println("f="+fname+" doc="+j+" expectedNorm="+expectedVal+" foundNorm="+foundVal);
+        assertEquals("Was the wrong norm saved? f="+fname+" doc="+j,expectedVal, foundVal, 0.0001);
+      }
+    }
+    reader.close();
+
+    // modify the norms of "numbered" fields:
+    // - some by using s - by tf
+    // - some by using smWithDelta - i.e. by tf+fieldNum
+    // - some norms would be ommited
+    reader = IndexReader.open(store);
+    FieldNormModifier fnm[] = {null,  new FieldNormModifier(store,s),  new FieldNormModifier(store,simWithDelta)};
+    for (int i=0; i<N_FIELDS_WITH_NUM; i++) {
+      String fname = FIELD_WITH_NUM+i;
+      if ((i%3)==0) {
+        fnm[1].killNorms(fname);
+      } else {
+        fnm[i%3].reSetNorms(fname);
+      }
+    }
+    
+    // verify norms modified as expected
+    reader = IndexReader.open(store);
+    for (int i=0; i<N_FIELDS_WITH_NUM; i++) {
+      String fname = FIELD_WITH_NUM+i;
+      if ((i%3)==0) {
+        //System.out.println("hasNorms("+fname+")="+reader.hasNorms("fname"));
+        assertFalse(reader.hasNorms("fname"));
+      } else {
+        byte[] norms = reader.norms(fname);
+        for (int j = 0; j < norms.length; j++) {
+          float expectedVal = Similarity.decodeNorm(Similarity.encodeNorm((i%3==1 ? j+1 : j+1+i)));
+          float foundVal = Similarity.decodeNorm(norms[j]);
+          //System.out.println("f="+fname+" doc="+j+" expectedNorm="+expectedVal+" foundNorm="+foundVal);
+          assertEquals("Was the wrong norm saved? f="+fname+" doc="+j,expectedVal, foundVal, 0.0001);
+        }
+      }
+    }
+    reader.close();
+    
+  }
+
 }
Index: contrib/miscellaneous/src/java/org/apache/lucene/index/FieldNormModifier.java
===================================================================
--- contrib/miscellaneous/src/java/org/apache/lucene/index/FieldNormModifier.java	(revision 495338)
+++ contrib/miscellaneous/src/java/org/apache/lucene/index/FieldNormModifier.java	(working copy)
@@ -19,9 +19,12 @@
 import java.io.IOException;
 import java.util.Date;
 
+import org.apache.lucene.analysis.SimpleAnalyzer;
+import org.apache.lucene.index.IndexReader.FieldOption;
 import org.apache.lucene.search.Similarity;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.FSDirectory;
+import org.apache.lucene.store.IndexOutput;
 
 /**
  * Given a directory and a list of fields, updates the fieldNorms in place for every document.
@@ -67,6 +70,9 @@
 
     for (int i = 2; i < args.length; i++) {
       System.out.print("Updating field: " + args[i] + " " + (new Date()).toString() + " ... ");
+      if (s == null)
+        fnm.killNorms(args[i]);
+      else
       fnm.reSetNorms(args[i]);
       System.out.println(new Date().toString());
     }
@@ -94,9 +100,8 @@
    * Resets the norms for the specified field.
    *
    * <p>
-   * Opens a new IndexReader on the Directory given to this instance,
-   * modifies the norms (either using the Similarity given to this instance, or by using fake norms,
-   * and closes the IndexReader.
+   * Opens a new IndexReader on the Directory given to this instance and
+   * modifies the norms using the Similarity specified in the call to the constructor.
    * </p>
    *
    * @param field the field whose norms should be reset
@@ -104,7 +109,6 @@
   public void reSetNorms(String field) throws IOException {
     String fieldName = field.intern();
     int[] termCounts = new int[0];
-    byte[] fakeNorms = new byte[0];
     
     IndexReader reader = null;
     TermEnum termEnum = null;
@@ -112,9 +116,6 @@
     try {
       reader = IndexReader.open(dir);
       termCounts = new int[reader.maxDoc()];
-      // if we are killing norms, get fake ones
-      if (sim == null)
-        fakeNorms = SegmentReader.createFakeNorms(reader.maxDoc());
       try {
         termEnum = reader.terms(new Term(field,""));
         try {
@@ -135,24 +136,121 @@
       } finally {
         if (null != termEnum) termEnum.close();
       }
+      
+      for (int d = 0; d < termCounts.length; d++) {
+        if (! reader.isDeleted(d))
+          reader.setNorm(d, fieldName, sim.encodeNorm(sim.lengthNorm(fieldName, termCounts[d])));
+      }
     } finally {
       if (null != reader) reader.close();
     }
+  }
+
+  /**
+   * Removes norms for the given field.  The index is optimized and expanded into
+   * a multi-file index first.  After the forms are removed, the index is packed
+   * back into a compound index, if that was its original format.
+   * @param fieldName the field whose norms should be removed
+   * @throws IOException
+   */
+  public void killNorms(String fieldName) throws IOException {
+    // figure out if the index is a CFS index or not
+    SegmentInfos sis = new SegmentInfos(); 
+    sis.read(dir);
+    SegmentInfo si = sis.info(0);
+    boolean isCompound = SegmentReader.usesCompoundFile(si);
+
+    // ensure there is only one segment, and that the index is expanded if it's CFS
+    IndexWriter writer = new IndexWriter(dir, new SimpleAnalyzer(), false);
+    writer.setUseCompoundFile(false);
+    writer.forceOptimize();
+    writer.close();
+
+    // find the .fnm file and .nrm files
+    String fnm = null;
+    String nrm = null;
+    String[] files = dir.list();
+    for (int i = 0; i < files.length; i++) {
+      if (files[i].endsWith(".fnm")) {
+        fnm = files[i];
+        System.out.println("FieldInfo file: " + fnm);
+      } else if (files[i].endsWith(".nrm")) {
+        nrm = files[i];
+        System.out.println("Norms file: " + nrm);
+      }
+      if (fnm!=null && nrm!=null)
+        break; // only 1 .fnm and 1 .nrm per optimized index
+    }
+
+    // switch from .nrm file to .Fn files for being able to kill norms
+    // 1. save all norms in .fN files
+    sis = new SegmentInfos(); 
+    sis.read(dir);
+    si = sis.info(0);
+    IndexReader reader = IndexReader.open(dir);
+    FieldInfos fis = new FieldInfos(dir,fnm);
+    for (int i=0; i<fis.size(); i++) {
+      FieldInfo fi = fis.fieldInfo(i);
+      if (!fi.omitNorms) {
+        byte b[] = reader.norms(fi.name);
+        String ff = si.name + ".f" + fi.number; 
+        //System.out.println("writing norms in: "+ff);
+        IndexOutput out = dir.createOutput(ff);
+        out.writeBytes(b, b.length);
+        out.close();
+      }
+    }
+    reader.close();
+    // 2. remove the .nrm file
+    dir.deleteFile(nrm);
+    // 3. make sure SegmentInfo look fo .fN files
+    SegmentInfo si2 = new SegmentInfo(si.name, si.docCount, dir, false, false);
+    sis.set(0,si2);
+    sis.write(dir);
+
+    // now can omit norms
+    ChangeableFieldInfos cfi = new ChangeableFieldInfos(dir, fnm);
+    cfi.change(fieldName, true);
+
+    // if the index was a CFS index, pack it back
+    if (isCompound) {
+      writer = new IndexWriter(dir, new SimpleAnalyzer(), false);
+      writer.setUseCompoundFile(true);
+      writer.forceOptimize();
+      writer.close();        
+    }
+  }
+
+  class ChangeableFieldInfos extends FieldInfos {
+    private Directory dir;
+    private String fileName;
     
-    try {
-      reader = IndexReader.open(dir); 
-      for (int d = 0; d < termCounts.length; d++) {
-        if (! reader.isDeleted(d)) {
-          if (sim == null)
-            reader.setNorm(d, fieldName, fakeNorms[0]);
-          else
-            reader.setNorm(d, fieldName, sim.encodeNorm(sim.lengthNorm(fieldName, termCounts[d])));
+    ChangeableFieldInfos(Directory dir, String fileName) throws IOException {
+      super(dir, fileName);
+      this.dir = dir;
+      this.fileName = fileName;
+    }
+    
+    void change(String fieldName, boolean omitNorms) throws IOException {
+      // set the omitNorms bit
+      FieldInfo fi = fieldInfo(fieldName);
+      fi.omitNorms = omitNorms;
+
+      System.out.println("Write file: " + fileName);
+      IndexOutput output = dir.createOutput(fileName);
+      write(output);
+      output.close();
+
+      // find and delete all .fN file files for the field
+      int fieldNumber = fieldNumber(fieldName);
+      String[] files = dir.list();
+      for (int i = 0; i < files.length; i++) {
+        if (files[i].endsWith(".f" + fieldNumber)) {
+          String normsFile = files[i];
+          System.out.println("Removing field norms file: " + normsFile);
+          dir.deleteFile(normsFile);
         }
-      }
-      
-    } finally {
-      if (null != reader) reader.close();
+      }      
     }
   }
-  
 }
