diff --git a/lucene/core/src/java/org/apache/lucene/index/SegmentMerger.java b/lucene/core/src/java/org/apache/lucene/index/SegmentMerger.java
index ad60a94..e4cbece 100644
--- a/lucene/core/src/java/org/apache/lucene/index/SegmentMerger.java
+++ b/lucene/core/src/java/org/apache/lucene/index/SegmentMerger.java
@@ -18,6 +18,8 @@ package org.apache.lucene.index;
 
 
 import java.io.IOException;
+import java.io.OutputStream;
+import java.io.PrintStream;
 import java.util.List;
 
 import org.apache.lucene.codecs.Codec;
@@ -28,6 +30,15 @@ import org.apache.lucene.codecs.NormsProducer;
 import org.apache.lucene.codecs.PointsWriter;
 import org.apache.lucene.codecs.StoredFieldsWriter;
 import org.apache.lucene.codecs.TermVectorsWriter;
+import org.apache.lucene.index.CheckIndex.Status.DocValuesStatus;
+import org.apache.lucene.index.CheckIndex.Status.FieldInfoStatus;
+import org.apache.lucene.index.CheckIndex.Status.FieldNormStatus;
+import org.apache.lucene.index.CheckIndex.Status.IndexSortStatus;
+import org.apache.lucene.index.CheckIndex.Status.LiveDocStatus;
+import org.apache.lucene.index.CheckIndex.Status.PointsStatus;
+import org.apache.lucene.index.CheckIndex.Status.StoredFieldStatus;
+import org.apache.lucene.index.CheckIndex.Status.TermIndexStatus;
+import org.apache.lucene.index.CheckIndex.Status.TermVectorStatus;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.IOContext;
 import org.apache.lucene.util.InfoStream;
@@ -56,6 +67,9 @@ final class SegmentMerger {
     if (context.context != IOContext.Context.MERGE) {
       throw new IllegalArgumentException("IOContext.context should be MERGE; got: " + context.context);
     }
+    for (CodecReader reader : readers) {
+      testReader(reader);
+    }
     mergeState = new MergeState(readers, segmentInfo, infoStream);
     directory = dir;
     this.codec = segmentInfo.getCodec();
@@ -80,7 +94,84 @@ final class SegmentMerger {
       }
     }
   }
+
+  private static void testReader(CodecReader reader) throws IOException {
+    if (reader instanceof SegmentReader) {
+      // We trust existing segment readers. This is the common case and they
+      // are expected to already have gone through sanity checks: documents
+      // have been validated at flush time (eg. positions are lte
+      // IndexWriter.MAX_POSITION) and then the verification of checksums at
+      // merge time should have prevented corruptions from being introduced.
+      return;
+    }
+
+    // Now we either have a view over an existing reader or a completely
+    // made-up CodecReader: let's verify it passes CheckIndex.
+
+    // No-op print stream
+    final PrintStream printStream = new PrintStream(new OutputStream() {
+      @Override
+      public void write(int b) throws IOException {}
+    });
+    final boolean failFast = true;
+    final boolean verbose = false;
+    final boolean doSlowChecks = false;
+
+    // Test Livedocs
+    LiveDocStatus liveDocStatus = CheckIndex.testLiveDocs(reader, printStream, failFast);
+    if (liveDocStatus.error != null) {
+      throw new IllegalArgumentException("Codec reader " + reader + " has corrupt live docs", liveDocStatus.error);
+    }
+
+    // Test Fieldinfos
+    FieldInfoStatus fieldInfoStatus = CheckIndex.testFieldInfos(reader, printStream, failFast);
+    if (fieldInfoStatus.error != null) {
+      throw new IllegalArgumentException("Codec reader " + reader + " has corrupt field infos", fieldInfoStatus.error);
+    }
   
+    // Test Field Norms
+    FieldNormStatus fieldNormStatus = CheckIndex.testFieldNorms(reader, printStream, failFast);
+    if (fieldNormStatus.error != null) {
+      throw new IllegalArgumentException("Codec reader " + reader + " has corrupt norms", fieldNormStatus.error);
+    }
+
+    // Test the Term Index
+    TermIndexStatus termIndexStatus = CheckIndex.testPostings(reader, printStream, verbose, doSlowChecks, failFast);
+    if (termIndexStatus.error != null) {
+      throw new IllegalArgumentException("Codec reader " + reader + " has corrupt terms/postings", termIndexStatus.error);
+    }
+
+    // Test Stored Fields
+    StoredFieldStatus storedFieldStatus = CheckIndex.testStoredFields(reader, printStream, failFast);
+    if (storedFieldStatus.error != null) {
+      throw new IllegalArgumentException("Codec reader " + reader + " has corrupt stored fields", storedFieldStatus.error);
+    }
+
+    // Test Term Vectors
+    TermVectorStatus termVectorStatus = CheckIndex.testTermVectors(reader, printStream, verbose, doSlowChecks, failFast);
+    if (termVectorStatus.error != null) {
+      throw new IllegalArgumentException("Codec reader " + reader + " has corrupt term vectors", termVectorStatus.error);
+    }
+
+    // Test Docvalues
+    DocValuesStatus docValuesStatus = CheckIndex.testDocValues(reader, printStream, failFast);
+    if (docValuesStatus.error != null) {
+      throw new IllegalArgumentException("Codec reader " + reader + " has corrupt doc values", docValuesStatus.error);
+    }
+
+    // Test PointValues
+    PointsStatus pointsStatus = CheckIndex.testPoints(reader, printStream, failFast);
+    if (pointsStatus.error != null) {
+      throw new IllegalArgumentException("Codec reader " + reader + " has corrupt points", pointsStatus.error);
+    }
+
+    // Test index sort
+    IndexSortStatus indexSortStatus = CheckIndex.testSort(reader, reader.getMetaData().getSort(), printStream, failFast);
+    if (indexSortStatus.error != null) {
+      throw new IllegalArgumentException("Codec reader " + reader + " has corrupt index sort", indexSortStatus.error);
+    }
+  }
+
   /** True if any merging should happen */
   boolean shouldMerge() {
     return mergeState.segmentInfo.maxDoc() > 0;
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestFilterLeafReader.java b/lucene/core/src/test/org/apache/lucene/index/TestFilterLeafReader.java
index de06f90..3b69a70 100644
--- a/lucene/core/src/test/org/apache/lucene/index/TestFilterLeafReader.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestFilterLeafReader.java
@@ -21,6 +21,7 @@ package org.apache.lucene.index;
 import java.io.IOException;
 import java.lang.reflect.Method;
 import java.lang.reflect.Modifier;
+import java.util.BitSet;
 
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.document.Document;
@@ -36,14 +37,53 @@ public class TestFilterLeafReader extends LuceneTestCase {
   private static class TestReader extends FilterLeafReader {
 
     private static class TestTerms extends FilterTerms {
-      TestTerms(Terms in) {
+
+      private final int docCount;
+      private final long sumDocFreq, sumTotalTermFreq;
+
+      TestTerms(Terms in) throws IOException {
         super(in);
+        BitSet docsWithTerms = new BitSet();
+        long sumDocFreq = 0, sumTotalTermFreq = 0;
+        TermsEnum te = iterator();
+        PostingsEnum pe = null;
+        while (te.next() != null) {
+          sumDocFreq += te.docFreq();
+          sumTotalTermFreq += te.totalTermFreq();
+          pe = te.postings(pe, PostingsEnum.NONE);
+          for (int doc = pe.nextDoc(); doc != DocIdSetIterator.NO_MORE_DOCS; doc = pe.nextDoc()) {
+            docsWithTerms.set(doc);
+          }
+        }
+        this.docCount = docsWithTerms.cardinality();
+        this.sumDocFreq = sumDocFreq;
+        this.sumTotalTermFreq = sumTotalTermFreq;
       }
 
       @Override
       public TermsEnum iterator() throws IOException {
         return new TestTermsEnum(super.iterator());
       }
+
+      @Override
+      public int getDocCount() {
+        return docCount;
+      }
+
+      @Override
+      public long getSumDocFreq() {
+        return sumDocFreq;
+      }
+
+      @Override
+      public long getSumTotalTermFreq() {
+        return sumTotalTermFreq;
+      }
+
+      @Override
+      public long size() {
+        return -1;
+      }
     }
 
     private static class TestTermsEnum extends FilterTermsEnum {
@@ -66,6 +106,26 @@ public class TestFilterLeafReader extends LuceneTestCase {
       public PostingsEnum postings(PostingsEnum reuse, int flags) throws IOException {
         return new TestPositions(super.postings(reuse == null ? null : ((FilterPostingsEnum) reuse).in, flags));
       }
+
+      @Override
+      public int docFreq() throws IOException {
+        int df = 0;
+        PostingsEnum pe = postings(null, PostingsEnum.NONE);
+        for (int doc = pe.nextDoc(); doc != DocIdSetIterator.NO_MORE_DOCS; doc = pe.nextDoc()) {
+          df++;
+        }
+        return df;
+      }
+
+      @Override
+      public long totalTermFreq() throws IOException {
+        long ttf = 0;
+        PostingsEnum pe = postings(null, PostingsEnum.FREQS);
+        for (int doc = pe.nextDoc(); doc != DocIdSetIterator.NO_MORE_DOCS; doc = pe.nextDoc()) {
+          ttf += pe.freq();
+        }
+        return ttf;
+      }
     }
 
     /** Filter that only returns odd numbered documents. */
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestMultiTermsEnum.java b/lucene/core/src/test/org/apache/lucene/index/TestMultiTermsEnum.java
index dbd685a..835f6dc 100644
--- a/lucene/core/src/test/org/apache/lucene/index/TestMultiTermsEnum.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestMultiTermsEnum.java
@@ -148,22 +148,34 @@ public class TestMultiTermsEnum extends LuceneTestCase {
 
         @Override
         public long size() throws IOException {
-          throw new UnsupportedOperationException();
+          return 1;
         }
 
         @Override
         public long getSumTotalTermFreq() throws IOException {
-          throw new UnsupportedOperationException();
+          TermsEnum iterator = iterator();
+          BytesRef b = iterator.next();
+          if (b == null) {
+            return 0;
+          } else {
+            return iterator.totalTermFreq();
+          }
         }
 
         @Override
         public long getSumDocFreq() throws IOException {
-          throw new UnsupportedOperationException();
+          return getDocCount();
         }
 
         @Override
         public int getDocCount() throws IOException {
-          throw new UnsupportedOperationException();
+          TermsEnum iterator = iterator();
+          BytesRef b = iterator.next();
+          if (b == null) {
+            return 0;
+          } else {
+            return iterator.docFreq();
+          }
         }
 
         @Override
