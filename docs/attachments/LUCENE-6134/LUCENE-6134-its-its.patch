Index: dev-tools/scripts/checkJavadocLinks.py
===================================================================
--- dev-tools/scripts/checkJavadocLinks.py	(revision 1647726)
+++ dev-tools/scripts/checkJavadocLinks.py	(working copy)
@@ -40,7 +40,7 @@
 
   def handle_starttag(self, tag, attrs):
     # NOTE: I don't think 'a' should be in here. But try debugging 
-    # NumericRangeQuery.html. (Could be javadocs bug, its a generic type...)
+    # NumericRangeQuery.html. (Could be javadocs bug, it's a generic type...)
     if tag not in ('link', 'meta', 'frame', 'br', 'hr', 'p', 'li', 'img', 'col', 'a'):
       self.stack.append(tag)
     if tag == 'a':
Index: dev-tools/scripts/smokeTestRelease.py
===================================================================
--- dev-tools/scripts/smokeTestRelease.py	(revision 1647726)
+++ dev-tools/scripts/smokeTestRelease.py	(working copy)
@@ -402,7 +402,7 @@
       logFile = '%s/%s.%s.gpg.verify.log' % (tmpDir, project, artifact)
       run('gpg --homedir %s --verify %s %s' % (gpgHomeDir, sigFile, artifactFile),
           logFile)
-      # Forward any GPG warnings, except the expected one (since its a clean world)
+      # Forward any GPG warnings, except the expected one (since it's a clean world)
       f = open(logFile, encoding='UTF-8')
       for line in f.readlines():
         if line.lower().find('warning') != -1 \
@@ -1132,7 +1132,7 @@
       logFile = '%s/%s.%s.gpg.verify.log' % (tmpDir, project, artifact)
       run('gpg --homedir %s --verify %s %s' % (gpgHomeDir, sigFile, artifactFile),
           logFile)
-      # Forward any GPG warnings, except the expected one (since its a clean world)
+      # Forward any GPG warnings, except the expected one (since it's a clean world)
       f = open(logFile, encoding='UTF-8')
       for line in f.readlines():
         if line.lower().find('warning') != -1 \
Index: lucene/CHANGES.txt
===================================================================
--- lucene/CHANGES.txt	(revision 1647726)
+++ lucene/CHANGES.txt	(working copy)
@@ -549,7 +549,7 @@
 * LUCENE-5825: Benchmark module can use custom postings format, e.g.:
  codec.postingsFormat=Memory (Varun Shenoy, David Smiley)
 
-* LUCENE-5842: When opening large files (where its to expensive to compare
+* LUCENE-5842: When opening large files (where it's too expensive to compare
   checksum against all the bytes), retrieve checksum to validate structure
   of footer, this can detect some forms of corruption such as truncation.
   (Robert Muir)
@@ -1228,7 +1228,7 @@
 
 * LUCENE-5483: Fix inaccuracies in HunspellStemFilter. Multi-stage affix-stripping,
   prefix-suffix dependencies, and COMPLEXPREFIXES now work correctly according
-  to the hunspell algorithm. Removed recursionCap parameter, as its no longer needed, rules for
+  to the hunspell algorithm. Removed recursionCap parameter, as it's no longer needed, rules for
   recursive affix application are driven correctly by continuation classes in the affix file.
   (Robert Muir)
 
@@ -1258,7 +1258,7 @@
 
 * LUCENE-5612: NativeFSLockFactory no longer deletes its lock file. This cannot be done
   safely without the risk of deleting someone else's lock file. If you use NativeFSLockFactory,
-  you may see write.lock hanging around from time to time: its harmless.  
+  you may see write.lock hanging around from time to time: it's harmless.  
   (Uwe Schindler, Mike McCandless, Robert Muir)
 
 * LUCENE-5624: Ensure NativeFSLockFactory does not leak file handles if it is unable
@@ -2269,7 +2269,7 @@
 * LUCENE-4933: Replace ExactSimScorer/SloppySimScorer with just SimScorer. Previously
   there were 2 implementations as a performance hack to support tableization of
   sqrt(), but this caching is removed, as sqrt is implemented in hardware with modern 
-  jvms and its faster not to cache.  (Robert Muir)
+  jvms and it's faster not to cache.  (Robert Muir)
 
 * LUCENE-5038: MergePolicy now has a default implementation for useCompoundFile based
   on segment size and noCFSRatio. The default implemantion was pulled up from
@@ -2365,7 +2365,7 @@
   SortedSetDocValuesReaderState and SortedSetDocValuesAccumulator.
   (Robert Muir, Mike McCandless)
 
-* LUCENE-5120: AnalyzingSuggester modifed it's FST's cached root arc if payloads
+* LUCENE-5120: AnalyzingSuggester modified its FST's cached root arc if payloads
   are used and the entire output resided on the root arc on the first access. This
   caused subsequent suggest calls to fail. (Simon Willnauer)
 
@@ -2939,9 +2939,9 @@
     use a different DocValuesFormat per field (like postings).
   - Unified with FieldCache api: DocValues can be accessed via FieldCache API,
     so it works automatically with grouping/join/sort/function queries, etc.
-  - Simplified types: There are only 3 types (NUMERIC, BINARY, SORTED), so its
+  - Simplified types: There are only 3 types (NUMERIC, BINARY, SORTED), so it's
     not necessary to specify for example that all of your binary values have
-    the same length. Instead its easy for the Codec API to optimize encoding
+    the same length. Instead it's easy for the Codec API to optimize encoding
     based on any properties of the content.
    (Simon Willnauer, Adrien Grand, Mike McCandless, Robert Muir)
 
@@ -3561,7 +3561,7 @@
   ForUtil.skipBlock.  (Robert Muir)
 
 * LUCENE-4497: Don't write PosVIntCount to the positions file in 
-  Lucene41PostingsFormat, as its always totalTermFreq % BLOCK_SIZE. (Robert Muir)
+  Lucene41PostingsFormat, as it's always totalTermFreq % BLOCK_SIZE. (Robert Muir)
 
 * LUCENE-4498: In Lucene41PostingsFormat, when a term appears in only one document, 
   Instead of writing a file pointer to a VIntBlock containing the doc id, just 
@@ -3675,7 +3675,7 @@
   (Uwe Schindler, Robert Muir)
 
 * LUCENE-4304: removed PayloadProcessorProvider. If you want to change
-  payloads (or other things) when merging indexes, its recommended
+  payloads (or other things) when merging indexes, it's recommended
   to just use a FilterAtomicReader + IndexWriter.addIndexes. See the
   OrdinalMappingAtomicReader and TaxonomyMergeUtils in the facets
   module if you want an example of this.
@@ -3699,7 +3699,7 @@
   overdelegates (read(), read(char[], int, int), skip, etc). This made it
   hard to implement CharFilters that were correct. Instead only close() is
   delegated by default: read(char[], int, int) and correct(int) are abstract
-  so that its obvious which methods you should implement.  The protected 
+  so that it's obvious which methods you should implement.  The protected 
   inner Reader is 'input' like CharFilter in the 3.x series, instead of 'in'.  
   (Dawid Weiss, Uwe Schindler, Robert Muir)
 
@@ -3755,7 +3755,7 @@
 
 * SOLR-3737: StempelPolishStemFilterFactory loaded its stemmer table incorrectly.
   Also, ensure immutability and use only one instance of this table in RAM (lazy
-  loaded) since its quite large. (sausarkar, Steven Rowe, Robert Muir)
+  loaded) since it's quite large. (sausarkar, Steven Rowe, Robert Muir)
 
 * LUCENE-4310: MappingCharFilter was failing to match input strings
   containing non-BMP Unicode characters.  (Dawid Weiss, Robert Muir,
@@ -3897,7 +3897,7 @@
   of embedded morfologik dictionaries to version 1.9. (Dawid Weiss)
 
 * LUCENE-4178: set 'tokenized' to true on FieldType by default, so that if you
-  make a custom FieldType and set indexed = true, its analyzed by the analyzer.
+  make a custom FieldType and set indexed = true, it's analyzed by the analyzer.
   (Robert Muir)
 
 * LUCENE-4220: Removed the buggy JavaCC-based HTML parser in the benchmark
@@ -4218,7 +4218,7 @@
   migration notes in MIGRATE.txt for more details.  (Robert Muir, Doron Cohen)
 
 * LUCENE-2315: AttributeSource's methods for accessing attributes are now final,
-  else its easy to corrupt the internal states.  (Uwe Schindler)
+  else it's easy to corrupt the internal states.  (Uwe Schindler)
 
 * LUCENE-2814: The IndexWriter.flush method no longer takes "boolean
   flushDocStores" argument, as we now always flush doc stores (index
@@ -4782,7 +4782,7 @@
     DefaultSimilarity, so you can easily try these out/switch back and 
     forth/run experiments and comparisons without reindexing. Note: most of 
     the models do rely upon index statistics that are new in Lucene 4.0, so 
-    for existing 3.x indexes its a good idea to upgrade your index to the 
+    for existing 3.x indexes it's a good idea to upgrade your index to the 
     new format with IndexUpgrader first.
 
   - Added a new subclass SimilarityBase which provides a simplified API 
@@ -6698,7 +6698,7 @@
 
 * LUCENE-2754, LUCENE-2757: Added a wrapper around MultiTermQueries
   to add span support: SpanMultiTermQueryWrapper<Q extends MultiTermQuery>.
-  Using this wrapper its easy to add fuzzy/wildcard to e.g. a SpanNearQuery.
+  Using this wrapper it's easy to add fuzzy/wildcard to e.g. a SpanNearQuery.
   (Robert Muir, Uwe Schindler)
 
 * LUCENE-2838: ConstantScoreQuery now directly supports wrapping a Query
@@ -8363,7 +8363,7 @@
     diagnostic information about the possibility of inconsistent
     FieldCache usage.  Namely: FieldCache entries for the same field
     with different datatypes or parsers; and FieldCache entries for
-    the same field in both a reader, and one of it's (descendant) sub
+    the same field in both a reader, and one of its (descendant) sub
     readers. 
     (Chris Hostetter, Mark Miller)
 
@@ -8584,7 +8584,7 @@
 1. LUCENE-1340: In a minor change to Lucene's backward compatibility
    policy, we are now allowing the Fieldable interface to have
    changes, within reason, and made on a case-by-case basis.  If an
-   application implements it's own Fieldable, please be aware of
+   application implements its own Fieldable, please be aware of
    this.  Otherwise, no need to be concerned.  This is in effect for
    all 2.X releases, starting with 2.4.  Also note, that in all
    likelihood, Fieldable will be changed in 3.0.
Index: lucene/analysis/common/src/java/org/apache/lucene/analysis/cjk/CJKBigramFilter.java
===================================================================
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/cjk/CJKBigramFilter.java	(revision 1647726)
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/cjk/CJKBigramFilter.java	(working copy)
@@ -56,9 +56,9 @@
   /** bigram flag for Hangul */
   public static final int HANGUL = 8;
 
-  /** when we emit a bigram, its then marked as this type */
+  /** when we emit a bigram, it's then marked as this type */
   public static final String DOUBLE_TYPE = "<DOUBLE>";
-  /** when we emit a unigram, its then marked as this type */
+  /** when we emit a unigram, it's then marked as this type */
   public static final String SINGLE_TYPE = "<SINGLE>";
 
   // the types from standardtokenizer
@@ -199,7 +199,7 @@
           if (hasBufferedUnigram()) {
             
             // we have a buffered unigram, and we peeked ahead to see if we could form
-            // a bigram, but we can't, because its not a CJK type. capture the state 
+            // a bigram, but we can't, because it's not a CJK type. capture the state 
             // of this peeked data to be revisited next time thru the loop, and dump our unigram.
             
             loneState = captureState();
@@ -213,7 +213,7 @@
         // case 3: we have only zero or 1 codepoints buffered, 
         // so not enough to form a bigram. But, we also have no
         // more input. So if we have a buffered codepoint, emit
-        // a unigram, otherwise, its end of stream.
+        // a unigram, otherwise, it's end of stream.
         
         if (hasBufferedUnigram()) {
           flushUnigram(); // flush our remaining unigram
@@ -345,7 +345,7 @@
       // when outputting unigrams always
       return bufferLen - index == 1;
     } else {
-      // otherwise its only when we have a lone CJK character
+      // otherwise it's only when we have a lone CJK character
       return bufferLen == 1 && index == 0;
     }
   }
Index: lucene/analysis/common/src/java/org/apache/lucene/analysis/commongrams/CommonGramsFilter.java
===================================================================
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/commongrams/CommonGramsFilter.java	(revision 1647726)
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/commongrams/CommonGramsFilter.java	(working copy)
@@ -112,7 +112,7 @@
     
     /* We build n-grams before and after stopwords. 
      * When valid, the buffer always contains at least the separator.
-     * If its empty, there is nothing before this stopword.
+     * If it's empty, there is nothing before this stopword.
      */
     if (lastWasCommon || (isCommon() && buffer.length() > 0)) {
       savedState = captureState();
Index: lucene/analysis/common/src/java/org/apache/lucene/analysis/de/GermanStemmer.java
===================================================================
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/de/GermanStemmer.java	(revision 1647726)
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/de/GermanStemmer.java	(working copy)
@@ -137,7 +137,7 @@
         strip( buffer );
       }
       // Additional step for irregular plural nouns like "Matrizen -> Matrix".
-      // NOTE: this length constraint is probably not a great value, its just to prevent AIOOBE on empty terms
+      // NOTE: this length constraint is probably not a great value, it's just to prevent AIOOBE on empty terms
       if ( buffer.length() > 0 && buffer.charAt( buffer.length() - 1 ) == ( 'z' ) ) {
         buffer.setCharAt( buffer.length() - 1, 'x' );
       }
Index: lucene/analysis/common/src/java/org/apache/lucene/analysis/el/GreekStemmer.java
===================================================================
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/el/GreekStemmer.java	(revision 1647726)
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/el/GreekStemmer.java	(working copy)
@@ -34,7 +34,7 @@
   
  /**
    * Stems a word contained in a leading portion of a char[] array.
-   * The word is passed through a number of rules that modify it's length.
+   * The word is passed through a number of rules that modify its length.
    * 
    * @param s A char[] array that contains the word to be stemmed.
    * @param len The length of the char[] array.
@@ -327,7 +327,7 @@
     }
     
     if (removed && exc8a.contains(s, 0, len)) {
-      // add -αγαν (we removed > 4 chars so its safe)
+      // add -αγαν (we removed > 4 chars so it's safe)
       len += 4;
       s[len - 4] = 'α';
       s[len - 3] = 'γ';
Index: lucene/analysis/common/src/java/org/apache/lucene/analysis/hunspell/Dictionary.java
===================================================================
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/hunspell/Dictionary.java	(revision 1647726)
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/hunspell/Dictionary.java	(working copy)
@@ -555,7 +555,7 @@
         // already exists in our hash
         appendFlagsOrd = (-appendFlagsOrd)-1;
       } else if (appendFlagsOrd > Short.MAX_VALUE) {
-        // this limit is probably flexible, but its a good sanity check too
+        // this limit is probably flexible, but it's a good sanity check too
         throw new UnsupportedOperationException("Too many unique append flags, please report this to dev@lucene.apache.org");
       }
       
@@ -1027,7 +1027,7 @@
   }
   
   private String parseStemException(String morphData) {
-    // first see if its an alias
+    // first see if it's an alias
     if (morphAliasCount > 0) {
       try {
         int alias = Integer.parseInt(morphData.trim());
Index: lucene/analysis/common/src/java/org/apache/lucene/analysis/hunspell/Stemmer.java
===================================================================
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/hunspell/Stemmer.java	(revision 1647726)
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/hunspell/Stemmer.java	(working copy)
@@ -46,7 +46,7 @@
   private final StringBuilder scratchSegment = new StringBuilder();
   private char scratchBuffer[] = new char[32];
   
-  // its '1' if we have no stem exceptions, otherwise every other form
+  // it's '1' if we have no stem exceptions, otherwise every other form
   // is really an ID pointing to the exception table
   private final int formStep;
   
@@ -134,7 +134,7 @@
       return EXACT_CASE;
     }
     
-    // determine if we are title or lowercase (or something funky, in which its exact)
+    // determine if we are title or lowercase (or something funky, in which it's exact)
     boolean seenUpper = false;
     boolean seenLower = false;
     for (int i = 1; i < length; i++) {
@@ -183,7 +183,7 @@
           if (checkKeepCase && Dictionary.hasFlag(wordFlags, (char)dictionary.keepcase)) {
             continue;
           }
-          // we can't add this form, its a pseudostem requiring an affix
+          // we can't add this form, it's a pseudostem requiring an affix
           if (checkNeedAffix && Dictionary.hasFlag(wordFlags, (char)dictionary.needaffix)) {
             continue;
           }
@@ -280,12 +280,12 @@
    * @param word Word to generate the stems for
    * @param previous previous affix that was removed (so we dont remove same one twice)
    * @param prevFlag Flag from a previous stemming step that need to be cross-checked with any affixes in this recursive step
-   * @param prefixFlag flag of the most inner removed prefix, so that when removing a suffix, its also checked against the word
+   * @param prefixFlag flag of the most inner removed prefix, so that when removing a suffix, it's also checked against the word
    * @param recursionDepth current recursiondepth
    * @param doPrefix true if we should remove prefixes
    * @param doSuffix true if we should remove suffixes
    * @param previousWasPrefix true if the previous removal was a prefix:
-   *        if we are removing a suffix, and it has no continuation requirements, its ok.
+   *        if we are removing a suffix, and it has no continuation requirements, it's ok.
    *        but two prefixes (COMPLEXPREFIXES) or two suffixes must have continuation requirements to recurse. 
    * @param circumfix true if the previous prefix removal was signed as a circumfix
    *        this means inner most suffix must also contain circumfix flag.
@@ -501,7 +501,7 @@
    * @param prefixFlag when we already stripped a prefix, we cant simply recurse and check the suffix, unless both are compatible
    *                   so we must check dictionary form against both to add it as a stem!
    * @param recursionDepth current recursion depth
-   * @param prefix true if we are removing a prefix (false if its a suffix)
+   * @param prefix true if we are removing a prefix (false if it's a suffix)
    * @return List of stems for the word, or an empty list if none are found
    */
   List<CharsRef> applyAffix(char strippedWord[], int length, int affix, int prefixFlag, int recursionDepth, boolean prefix, boolean circumfix, boolean caseVariant) throws IOException {    
Index: lucene/analysis/common/src/java/org/apache/lucene/analysis/lv/LatvianStemmer.java
===================================================================
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/lv/LatvianStemmer.java	(revision 1647726)
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/lv/LatvianStemmer.java	(working copy)
@@ -94,8 +94,8 @@
    * </ul>
    */
   private int unpalatalize(char s[], int len) {
-    // we check the character removed: if its -u then 
-    // its 2,5, or 6 gen pl., and these two can only apply then.
+    // we check the character removed: if it's -u then 
+    // it's 2,5, or 6 gen pl., and these two can only apply then.
     if (s[len] == 'u') {
       // kš -> kst
       if (endsWith(s, len, "kš")) {
Index: lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/LimitTokenCountFilterFactory.java
===================================================================
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/LimitTokenCountFilterFactory.java	(revision 1647726)
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/LimitTokenCountFilterFactory.java	(working copy)
@@ -33,7 +33,7 @@
  * &lt;/fieldType&gt;</pre>
  * <p>
  * The {@code consumeAllTokens} property is optional and defaults to {@code false}.  
- * See {@link LimitTokenCountFilter} for an explanation of it's use.
+ * See {@link LimitTokenCountFilter} for an explanation of its use.
  */
 public class LimitTokenCountFilterFactory extends TokenFilterFactory {
 
Index: lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/ScandinavianFoldingFilter.java
===================================================================
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/ScandinavianFoldingFilter.java	(revision 1647726)
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/ScandinavianFoldingFilter.java	(working copy)
@@ -28,7 +28,7 @@
  * This filter folds Scandinavian characters åÅäæÄÆ-&gt;a and öÖøØ-&gt;o.
  * It also discriminate against use of double vowels aa, ae, ao, oe and oo, leaving just the first one.
  * <p/>
- * It's is a semantically more destructive solution than {@link ScandinavianNormalizationFilter} but
+ * It's a semantically more destructive solution than {@link ScandinavianNormalizationFilter} but
  * can in addition help with matching raksmorgas as räksmörgås.
  * <p/>
  * blåbærsyltetøj == blåbärsyltetöj == blaabaarsyltetoej == blabarsyltetoj
Index: lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/StemmerOverrideFilter.java
===================================================================
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/StemmerOverrideFilter.java	(revision 1647726)
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/StemmerOverrideFilter.java	(working copy)
@@ -168,7 +168,7 @@
     }
     
     /**
-     * Adds an input string and it's stemmer override output to this builder.
+     * Adds an input string and its stemmer override output to this builder.
      * 
      * @param input the input char sequence 
      * @param output the stemmer override output char sequence
Index: lucene/analysis/common/src/java/org/apache/lucene/analysis/path/ReversePathHierarchyTokenizer.java
===================================================================
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/path/ReversePathHierarchyTokenizer.java	(revision 1647726)
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/path/ReversePathHierarchyTokenizer.java	(working copy)
@@ -154,7 +154,7 @@
       resultToken.setLength(0);
       int idx = delimitersCount-1 - skip;
       if (idx >= 0) {
-        // otherwise its ok, because we will skip and return false
+        // otherwise it's ok, because we will skip and return false
         endPosition = delimiterPositions.get(idx);
       }
       finalOffset = correctOffset(length);
Index: lucene/analysis/common/src/java/org/apache/lucene/analysis/synonym/SolrSynonymParser.java
===================================================================
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/synonym/SolrSynonymParser.java	(revision 1647726)
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/synonym/SolrSynonymParser.java	(working copy)
@@ -120,7 +120,7 @@
       
       // currently we include the term itself in the map,
       // and use includeOrig = false always.
-      // this is how the existing filter does it, but its actually a bug,
+      // this is how the existing filter does it, but it's actually a bug,
       // especially if combined with ignoreCase = true
       for (int i = 0; i < inputs.length; i++) {
         for (int j = 0; j < outputs.length; j++) {
Index: lucene/analysis/common/src/java/org/apache/lucene/analysis/synonym/SynonymFilter.java
===================================================================
--- lucene/analysis/common/src/java/org/apache/lucene/analysis/synonym/SynonymFilter.java	(revision 1647726)
+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/synonym/SynonymFilter.java	(working copy)
@@ -258,7 +258,7 @@
    * @param input input tokenstream
    * @param synonyms synonym map
    * @param ignoreCase case-folds input for matching with {@link Character#toLowerCase(int)}.
-   *                   Note, if you set this to true, its your responsibility to lowercase
+   *                   Note, if you set this to true, it's your responsibility to lowercase
    *                   the input entries when you create the {@link SynonymMap}
    */
   public SynonymFilter(TokenStream input, SynonymMap synonyms, boolean ignoreCase) {
Index: lucene/analysis/common/src/java/org/apache/lucene/collation/CollationKeyAnalyzer.java
===================================================================
--- lucene/analysis/common/src/java/org/apache/lucene/collation/CollationKeyAnalyzer.java	(revision 1647726)
+++ lucene/analysis/common/src/java/org/apache/lucene/collation/CollationKeyAnalyzer.java	(working copy)
@@ -56,7 +56,7 @@
  * </ol> 
  * <p>
  *   The <code>ICUCollationKeyAnalyzer</code> in the analysis-icu package 
- *   uses ICU4J's Collator, which makes its
+ *   uses ICU4J's Collator, which makes
  *   its version available, thus allowing collation to be versioned
  *   independently from the JVM.  ICUCollationKeyAnalyzer is also significantly
  *   faster and generates significantly shorter keys than CollationKeyAnalyzer.
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/compound/TestHyphenationCompoundWordTokenFilterFactory.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/compound/TestHyphenationCompoundWordTokenFilterFactory.java	(revision 1647726)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/compound/TestHyphenationCompoundWordTokenFilterFactory.java	(working copy)
@@ -49,7 +49,7 @@
   /**
    * Ensure the factory works with no dictionary: using hyphenation grammar only.
    * Also change the min/max subword sizes from the default. When using no dictionary,
-   * its generally necessary to tweak these, or you get lots of expansions.
+   * it's generally necessary to tweak these, or you get lots of expansions.
    */
   public void testHyphenationOnly() throws Exception {
     Reader reader = new StringReader("basketballkurv");
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestDuelingAnalyzers.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestDuelingAnalyzers.java	(revision 1647726)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestDuelingAnalyzers.java	(working copy)
@@ -80,7 +80,7 @@
     }
   }
   
-  // not so useful since its all one token?!
+  // not so useful since it's all one token?!
   public void testLetterAsciiHuge() throws Exception {
     Random random = random();
     int maxLength = 8192; // CharTokenizer.IO_BUFFER_SIZE*2
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestFactories.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestFactories.java	(revision 1647726)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestFactories.java	(working copy)
@@ -71,7 +71,7 @@
       if (factory instanceof MultiTermAwareComponent) {
         AbstractAnalysisFactory mtc = ((MultiTermAwareComponent) factory).getMultiTermComponent();
         assertNotNull(mtc);
-        // its not ok to return e.g. a charfilter here: but a tokenizer could wrap a filter around it
+        // it's not ok to return e.g. a charfilter here: but a tokenizer could wrap a filter around it
         assertFalse(mtc instanceof CharFilterFactory);
       }
       
@@ -91,7 +91,7 @@
       if (factory instanceof MultiTermAwareComponent) {
         AbstractAnalysisFactory mtc = ((MultiTermAwareComponent) factory).getMultiTermComponent();
         assertNotNull(mtc);
-        // its not ok to return a charfilter or tokenizer here, this makes no sense
+        // it's not ok to return a charfilter or tokenizer here, this makes no sense
         assertTrue(mtc instanceof TokenFilterFactory);
       }
       
@@ -111,7 +111,7 @@
       if (factory instanceof MultiTermAwareComponent) {
         AbstractAnalysisFactory mtc = ((MultiTermAwareComponent) factory).getMultiTermComponent();
         assertNotNull(mtc);
-        // its not ok to return a tokenizer or tokenfilter here, this makes no sense
+        // it's not ok to return a tokenizer or tokenfilter here, this makes no sense
         assertTrue(mtc instanceof CharFilterFactory);
       }
       
@@ -141,7 +141,7 @@
       throw new RuntimeException(e);
     } catch (InvocationTargetException e) {
       if (e.getCause() instanceof IllegalArgumentException) {
-        // its ok if we dont provide the right parameters to throw this
+        // it's ok if we dont provide the right parameters to throw this
         return null;
       }
     }
@@ -150,7 +150,7 @@
       try {
         ((ResourceLoaderAware) factory).inform(new StringMockResourceLoader(""));
       } catch (IOException ignored) {
-        // its ok if the right files arent available or whatever to throw this
+        // it's ok if the right files arent available or whatever to throw this
       } catch (IllegalArgumentException ignored) {
         // is this ok? I guess so
       }
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/cz/TestCzechStemmer.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/cz/TestCzechStemmer.java	(revision 1647726)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/cz/TestCzechStemmer.java	(working copy)
@@ -32,7 +32,7 @@
 /**
  * Test the Czech Stemmer.
  * 
- * Note: its algorithmic, so some stems are nonsense
+ * Note: it's algorithmic, so some stems are nonsense
  *
  */
 public class TestCzechStemmer extends BaseTokenStreamTestCase {
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestLimitTokenCountAnalyzer.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestLimitTokenCountAnalyzer.java	(revision 1647726)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestLimitTokenCountAnalyzer.java	(working copy)
@@ -43,7 +43,7 @@
       mock.setEnableChecks(consumeAll);
       Analyzer a = new LimitTokenCountAnalyzer(mock, 2, consumeAll);
     
-      // dont use assertAnalyzesTo here, as the end offset is not the end of the string (unless consumeAll is true, in which case its correct)!
+      // dont use assertAnalyzesTo here, as the end offset is not the end of the string (unless consumeAll is true, in which case it's correct)!
       assertTokenStreamContents(a.tokenStream("dummy", "1  2     3  4  5"), new String[] { "1", "2" }, new int[] { 0, 3 }, new int[] { 1, 4 }, consumeAll ? 16 : null);
       assertTokenStreamContents(a.tokenStream("dummy", "1 2 3 4 5"), new String[] { "1", "2" }, new int[] { 0, 2 }, new int[] { 1, 3 }, consumeAll ? 9 : null);
       
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestLimitTokenPositionFilter.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestLimitTokenPositionFilter.java	(revision 1647726)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestLimitTokenPositionFilter.java	(working copy)
@@ -43,7 +43,7 @@
         }
       };
 
-      // don't use assertAnalyzesTo here, as the end offset is not the end of the string (unless consumeAll is true, in which case its correct)!
+      // don't use assertAnalyzesTo here, as the end offset is not the end of the string (unless consumeAll is true, in which case it's correct)!
       assertTokenStreamContents(a.tokenStream("dummy", "1  2     3  4  5"),
           new String[]{"1", "2"}, new int[]{0, 3}, new int[]{1, 4}, consumeAll ? 16 : null);
       assertTokenStreamContents(a.tokenStream("dummy", new StringReader("1 2 3 4 5")),
Index: lucene/analysis/common/src/test/org/apache/lucene/analysis/util/TestCharArrayIterator.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/analysis/util/TestCharArrayIterator.java	(revision 1647726)
+++ lucene/analysis/common/src/test/org/apache/lucene/analysis/util/TestCharArrayIterator.java	(working copy)
@@ -31,7 +31,7 @@
   }
   
   public void testConsumeWordInstance() {
-    // we use the default locale, as its randomized by LuceneTestCase
+    // we use the default locale, as it's randomized by LuceneTestCase
     BreakIterator bi = BreakIterator.getWordInstance(Locale.getDefault());
     CharArrayIterator ci = CharArrayIterator.newWordInstance();
     for (int i = 0; i < 10000; i++) {
@@ -43,7 +43,7 @@
   
   /* run this to test if your JRE is buggy
   public void testWordInstanceJREBUG() {
-    // we use the default locale, as its randomized by LuceneTestCase
+    // we use the default locale, as it's randomized by LuceneTestCase
     BreakIterator bi = BreakIterator.getWordInstance(Locale.getDefault());
     Segment ci = new Segment();
     for (int i = 0; i < 10000; i++) {
@@ -61,7 +61,7 @@
   }
   
   public void testConsumeSentenceInstance() {
-    // we use the default locale, as its randomized by LuceneTestCase
+    // we use the default locale, as it's randomized by LuceneTestCase
     BreakIterator bi = BreakIterator.getSentenceInstance(Locale.getDefault());
     CharArrayIterator ci = CharArrayIterator.newSentenceInstance();
     for (int i = 0; i < 10000; i++) {
@@ -73,7 +73,7 @@
   
   /* run this to test if your JRE is buggy
   public void testSentenceInstanceJREBUG() {
-    // we use the default locale, as its randomized by LuceneTestCase
+    // we use the default locale, as it's randomized by LuceneTestCase
     BreakIterator bi = BreakIterator.getSentenceInstance(Locale.getDefault());
     Segment ci = new Segment();
     for (int i = 0; i < 10000; i++) {
Index: lucene/analysis/common/src/test/org/apache/lucene/collation/TestCollationKeyAnalyzer.java
===================================================================
--- lucene/analysis/common/src/test/org/apache/lucene/collation/TestCollationKeyAnalyzer.java	(revision 1647726)
+++ lucene/analysis/common/src/test/org/apache/lucene/collation/TestCollationKeyAnalyzer.java	(working copy)
@@ -28,7 +28,7 @@
 public class TestCollationKeyAnalyzer extends CollationTestBase {
   // the sort order of Ø versus U depends on the version of the rules being used
   // for the inherited root locale: Ø's order isnt specified in Locale.US since 
-  // its not used in english.
+  // it's not used in english.
   private boolean oStrokeFirst = Collator.getInstance(new Locale("")).compare("Ø", "U") < 0;
   
   // Neither Java 1.4.2 nor 1.5.0 has Farsi Locale collation available in
Index: lucene/analysis/icu/src/java/org/apache/lucene/analysis/icu/segmentation/BreakIteratorWrapper.java
===================================================================
--- lucene/analysis/icu/src/java/org/apache/lucene/analysis/icu/segmentation/BreakIteratorWrapper.java	(revision 1647726)
+++ lucene/analysis/icu/src/java/org/apache/lucene/analysis/icu/segmentation/BreakIteratorWrapper.java	(working copy)
@@ -59,7 +59,7 @@
   }
 
   /**
-   * If its a RuleBasedBreakIterator, the rule status can be used for token type. If its
+   * If it's a RuleBasedBreakIterator, the rule status can be used for token type. If it's
    * any other BreakIterator, the rulestatus method is not available, so treat
    * it like a generic BreakIterator.
    */
@@ -71,7 +71,7 @@
   }
 
   /**
-   * RuleBasedBreakIterator wrapper: RuleBasedBreakIterator (as long as its not
+   * RuleBasedBreakIterator wrapper: RuleBasedBreakIterator (as long as it's not
    * a DictionaryBasedBreakIterator) behaves correctly.
    */
   static final class RBBIWrapper extends BreakIteratorWrapper {
Index: lucene/analysis/icu/src/java/org/apache/lucene/analysis/icu/segmentation/ScriptIterator.java
===================================================================
--- lucene/analysis/icu/src/java/org/apache/lucene/analysis/icu/segmentation/ScriptIterator.java	(revision 1647726)
+++ lucene/analysis/icu/src/java/org/apache/lucene/analysis/icu/segmentation/ScriptIterator.java	(working copy)
@@ -44,7 +44,7 @@
  * differences:
  * <ul>
  *  <li>Doesn't attempt to match paired punctuation. For tokenization purposes, this
- * is not necessary. Its also quite expensive. 
+ * is not necessary. It's also quite expensive. 
  *  <li>Non-spacing marks inherit the script of their base character, following 
  *  recommendations from UTR #24.
  * </ul>
Index: lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/JapaneseReadingFormFilter.java
===================================================================
--- lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/JapaneseReadingFormFilter.java	(revision 1647726)
+++ lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/JapaneseReadingFormFilter.java	(working copy)
@@ -54,7 +54,7 @@
       
       if (useRomaji) {
         if (reading == null) {
-          // if its an OOV term, just try the term text
+          // if it's an OOV term, just try the term text
           buffer.setLength(0);
           ToStringUtil.getRomanization(buffer, termAttr);
           termAttr.setEmpty().append(buffer);
Index: lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/dict/BinaryDictionary.java
===================================================================
--- lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/dict/BinaryDictionary.java	(revision 1647726)
+++ lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/dict/BinaryDictionary.java	(working copy)
@@ -289,7 +289,7 @@
     return new String(text);
   }
   
-  /** flag that the entry has baseform data. otherwise its not inflected (same as surface form) */
+  /** flag that the entry has baseform data. otherwise it's not inflected (same as surface form) */
   public static final int HAS_BASEFORM = 1;
   /** flag that the entry has reading data. otherwise reading is surface form converted to katakana */
   public static final int HAS_READING = 2;
Index: lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/dict/UserDictionary.java
===================================================================
--- lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/dict/UserDictionary.java	(revision 1647726)
+++ lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/dict/UserDictionary.java	(working copy)
@@ -77,7 +77,7 @@
     }
     
     // TODO: should we allow multiple segmentations per input 'phrase'?
-    // the old treemap didn't support this either, and i'm not sure if its needed/useful?
+    // the old treemap didn't support this either, and i'm not sure if it's needed/useful?
 
     Collections.sort(featureEntries, new Comparator<String[]>() {
       @Override
Index: lucene/analysis/kuromoji/src/test/org/apache/lucene/analysis/ja/dict/TestTokenInfoDictionary.java
===================================================================
--- lucene/analysis/kuromoji/src/test/org/apache/lucene/analysis/ja/dict/TestTokenInfoDictionary.java	(revision 1647726)
+++ lucene/analysis/kuromoji/src/test/org/apache/lucene/analysis/ja/dict/TestTokenInfoDictionary.java	(working copy)
@@ -67,14 +67,14 @@
         String inflectionForm = tid.getInflectionForm(wordId);
         assertTrue(inflectionForm == null || UnicodeUtil.validUTF16String(inflectionForm));
         if (inflectionForm != null) {
-          // check that its actually an ipadic inflection form
+          // check that it's actually an ipadic inflection form
           assertNotNull(ToStringUtil.getInflectedFormTranslation(inflectionForm));          
         }
         
         String inflectionType = tid.getInflectionType(wordId);
         assertTrue(inflectionType == null || UnicodeUtil.validUTF16String(inflectionType));
         if (inflectionType != null) {
-          // check that its actually an ipadic inflection type
+          // check that it's actually an ipadic inflection type
           assertNotNull(ToStringUtil.getInflectionTypeTranslation(inflectionType));
         }
         
@@ -88,7 +88,7 @@
         String pos = tid.getPartOfSpeech(wordId);
         assertNotNull(pos);
         assertTrue(UnicodeUtil.validUTF16String(pos));
-        // check that its actually an ipadic pos tag
+        // check that it's actually an ipadic pos tag
         assertNotNull(ToStringUtil.getPOSTranslation(pos));
         
         String pronunciation = tid.getPronunciation(wordId, chars, 0, chars.length);
Index: lucene/analysis/phonetic/src/java/org/apache/lucene/analysis/phonetic/BeiderMorseFilter.java
===================================================================
--- lucene/analysis/phonetic/src/java/org/apache/lucene/analysis/phonetic/BeiderMorseFilter.java	(revision 1647726)
+++ lucene/analysis/phonetic/src/java/org/apache/lucene/analysis/phonetic/BeiderMorseFilter.java	(working copy)
@@ -39,7 +39,7 @@
   private final LanguageSet languages;
   
   // output is a string such as ab|ac|...
-  // in complex cases like d'angelo its (anZelo|andZelo|...)-(danZelo|...)
+  // in complex cases like d'angelo it's (anZelo|andZelo|...)-(danZelo|...)
   // if there are multiple 's, it starts to nest...
   private static final Pattern pattern = Pattern.compile("([^()|-]+)");
   
Index: lucene/analysis/smartcn/src/java/org/apache/lucene/analysis/cn/smart/WordSegmenter.java
===================================================================
--- lucene/analysis/smartcn/src/java/org/apache/lucene/analysis/cn/smart/WordSegmenter.java	(revision 1647726)
+++ lucene/analysis/smartcn/src/java/org/apache/lucene/analysis/cn/smart/WordSegmenter.java	(working copy)
@@ -47,7 +47,7 @@
     // tokens from sentence, excluding WordType.SENTENCE_BEGIN and WordType.SENTENCE_END
     List<SegToken> result = Collections.emptyList();
     
-    if (segTokenList.size() > 2) // if its not an empty sentence
+    if (segTokenList.size() > 2) // if it's not an empty sentence
       result = segTokenList.subList(1, segTokenList.size() - 1);
     
     for (SegToken st : result)
Index: lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/SearchTravRetLoadFieldSelectorTask.java
===================================================================
--- lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/SearchTravRetLoadFieldSelectorTask.java	(revision 1647726)
+++ lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/SearchTravRetLoadFieldSelectorTask.java	(working copy)
@@ -67,7 +67,7 @@
 
   @Override
   public void setParams(String params) {
-    this.params = params; // cannot just call super.setParams(), b/c it's params differ.
+    this.params = params; // cannot just call super.setParams(), b/c its params differ.
     fieldsToLoad = new HashSet<>();
     for (StringTokenizer tokenizer = new StringTokenizer(params, ","); tokenizer.hasMoreTokens();) {
       String s = tokenizer.nextToken();
Index: lucene/classification/src/java/org/apache/lucene/classification/CachingNaiveBayesClassifier.java
===================================================================
--- lucene/classification/src/java/org/apache/lucene/classification/CachingNaiveBayesClassifier.java	(revision 1647726)
+++ lucene/classification/src/java/org/apache/lucene/classification/CachingNaiveBayesClassifier.java	(working copy)
@@ -50,7 +50,7 @@
 public class CachingNaiveBayesClassifier extends SimpleNaiveBayesClassifier {
   //for caching classes this will be the classification class list
   private ArrayList<BytesRef> cclasses = new ArrayList<>();
-  // its a term-inmap style map, where the inmap contains class-hit pairs to the
+  // it's a term-inmap style map, where the inmap contains class-hit pairs to the
   // upper term
   private Map<String, Map<BytesRef, Integer>> termCClassHitCache = new HashMap<>();
   // the term frequency in classes
@@ -185,7 +185,7 @@
 
     Map<BytesRef, Integer> searched = new ConcurrentHashMap<>();
 
-    // if we dont get the answer, but its relevant we must search it and insert to the cache
+    // if we dont get the answer, but it's relevant we must search it and insert to the cache
     if (insertPoint != null || !justCachedTerms) {
       for (BytesRef cclass : cclasses) {
         BooleanQuery booleanQuery = new BooleanQuery();
Index: lucene/codecs/src/java/org/apache/lucene/codecs/memory/DirectDocValuesConsumer.java
===================================================================
--- lucene/codecs/src/java/org/apache/lucene/codecs/memory/DirectDocValuesConsumer.java	(revision 1647726)
+++ lucene/codecs/src/java/org/apache/lucene/codecs/memory/DirectDocValuesConsumer.java	(working copy)
@@ -262,7 +262,7 @@
     }
   }
 
-  // note: this might not be the most efficient... but its fairly simple
+  // note: this might not be the most efficient... but it's fairly simple
   @Override
   public void addSortedSetField(FieldInfo field, Iterable<BytesRef> values, final Iterable<Number> docToOrdCount, final Iterable<Number> ords) throws IOException {
     meta.writeVInt(field.number);
Index: lucene/codecs/src/java/org/apache/lucene/codecs/memory/MemoryDocValuesConsumer.java
===================================================================
--- lucene/codecs/src/java/org/apache/lucene/codecs/memory/MemoryDocValuesConsumer.java	(revision 1647726)
+++ lucene/codecs/src/java/org/apache/lucene/codecs/memory/MemoryDocValuesConsumer.java	(working copy)
@@ -196,7 +196,7 @@
       int numBlocks = maxDoc / BLOCK_SIZE;
       float avgBPV = blockSum / (float)numBlocks;
       // just a heuristic, with tiny amounts of blocks our estimate is skewed as we ignore the final "incomplete" block.
-      // with at least 4 blocks its pretty accurate. The difference must also be significant (according to acceptable overhead).
+      // with at least 4 blocks it's pretty accurate. The difference must also be significant (according to acceptable overhead).
       if (numBlocks >= 4 && (avgBPV+avgBPV*acceptableOverheadRatio) < deltaBPV.bitsPerValue) {
         doBlock = true;
       }
@@ -335,7 +335,7 @@
     meta.writeVInt(minLength);
     meta.writeVInt(maxLength);
     
-    // if minLength == maxLength, its a fixed-length byte[], we are done (the addresses are implicit)
+    // if minLength == maxLength, it's a fixed-length byte[], we are done (the addresses are implicit)
     // otherwise, we need to record the length fields...
     if (minLength != maxLength) {
       meta.writeVInt(PackedInts.VERSION_CURRENT);
@@ -432,7 +432,7 @@
     }
   }
 
-  // note: this might not be the most efficient... but its fairly simple
+  // note: this might not be the most efficient... but it's fairly simple
   @Override
   public void addSortedSetField(FieldInfo field, Iterable<BytesRef> values, final Iterable<Number> docToOrdCount, final Iterable<Number> ords) throws IOException {
     meta.writeVInt(field.number);
Index: lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextDocValuesFormat.java
===================================================================
--- lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextDocValuesFormat.java	(revision 1647726)
+++ lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextDocValuesFormat.java	(working copy)
@@ -111,11 +111,11 @@
  *  </pre>
  *  so the "ord section" begins at startOffset + (9+pattern.length+maxlength)*numValues.
  *  a document's ord list can be retrieved by seeking to "ord section" + (1+ordpattern.length())*docid
- *  this is a comma-separated list, and its padded with spaces to be fixed width. so trim() and split() it.
+ *  this is a comma-separated list, and it's padded with spaces to be fixed width. so trim() and split() it.
  *  and beware the empty string!
  *  an ord's value can be retrieved by seeking to startOffset + (9+pattern.length+maxlength)*ord
  *  
- *  for sorted numerics, its encoded (not very creatively) as a comma-separated list of strings the same as binary.
+ *  for sorted numerics, it's encoded (not very creatively) as a comma-separated list of strings the same as binary.
  *  beware the empty string!
  *   
  *  the reader can just scan this file when it opens, skipping over the data blocks
Index: lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextDocValuesReader.java
===================================================================
--- lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextDocValuesReader.java	(revision 1647726)
+++ lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextDocValuesReader.java	(working copy)
@@ -180,7 +180,7 @@
           } catch (ParseException pe) {
             throw new CorruptIndexException("failed to parse BigDecimal value", in, pe);
           }
-          SimpleTextUtil.readLine(in, scratch); // read the line telling us if its real or not
+          SimpleTextUtil.readLine(in, scratch); // read the line telling us if it's real or not
           return BigInteger.valueOf(field.minValue).add(bd.toBigIntegerExact()).longValue();
         } catch (IOException ioe) {
           throw new RuntimeException(ioe);
Index: lucene/core/src/java/org/apache/lucene/analysis/Analyzer.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/analysis/Analyzer.java	(revision 1647726)
+++ lucene/core/src/java/org/apache/lucene/analysis/Analyzer.java	(working copy)
@@ -88,7 +88,7 @@
   /**
    * Expert: create a new Analyzer with a custom {@link ReuseStrategy}.
    * <p>
-   * NOTE: if you just want to reuse on a per-field basis, its easier to
+   * NOTE: if you just want to reuse on a per-field basis, it's easier to
    * use a subclass of {@link AnalyzerWrapper} such as 
    * <a href="{@docRoot}/../analyzers-common/org/apache/lucene/analysis/miscellaneous/PerFieldAnalyzerWrapper.html">
    * PerFieldAnalyerWrapper</a> instead.
Index: lucene/core/src/java/org/apache/lucene/analysis/tokenattributes/PayloadAttribute.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/analysis/tokenattributes/PayloadAttribute.java	(revision 1647726)
+++ lucene/core/src/java/org/apache/lucene/analysis/tokenattributes/PayloadAttribute.java	(working copy)
@@ -29,7 +29,7 @@
  * in the {@link org.apache.lucene.search.payloads} and
  * {@link org.apache.lucene.search.spans} packages.
  * <p>
- * NOTE: because the payload will be stored at each position, its usually
+ * NOTE: because the payload will be stored at each position, it's usually
  * best to use the minimum number of bytes necessary. Some codec implementations
  * may optimize payload storage when all payloads have the same length.
  * 
Index: lucene/core/src/java/org/apache/lucene/codecs/TermVectorsWriter.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/codecs/TermVectorsWriter.java	(revision 1647726)
+++ lucene/core/src/java/org/apache/lucene/codecs/TermVectorsWriter.java	(working copy)
@@ -116,7 +116,7 @@
    * @lucene.internal
    */
   // TODO: we should probably nuke this and make a more efficient 4.x format
-  // PreFlex-RW could then be slow and buffer (its only used in tests...)
+  // PreFlex-RW could then be slow and buffer (it's only used in tests...)
   public void addProx(int numProx, DataInput positions, DataInput offsets) throws IOException {
     int position = 0;
     int lastOffset = 0;
Index: lucene/core/src/java/org/apache/lucene/codecs/blocktree/BlockTreeTermsReader.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/codecs/blocktree/BlockTreeTermsReader.java	(revision 1647726)
+++ lucene/core/src/java/org/apache/lucene/codecs/blocktree/BlockTreeTermsReader.java	(working copy)
@@ -48,7 +48,7 @@
  *  approach is that seekExact is often able to
  *  determine a term cannot exist without doing any IO, and
  *  intersection with Automata is very fast.  Note that this
- *  terms dictionary has it's own fixed terms index (ie, it
+ *  terms dictionary has its own fixed terms index (ie, it
  *  does not support a pluggable terms index
  *  implementation).
  *
Index: lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressionMode.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressionMode.java	(revision 1647726)
+++ lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressionMode.java	(working copy)
@@ -199,7 +199,7 @@
       }
       final int compressedLength = in.readVInt();
       // pad with extra "dummy byte": see javadocs for using Inflater(true)
-      // we do it for compliance, but its unnecessary for years in zlib.
+      // we do it for compliance, but it's unnecessary for years in zlib.
       final int paddedLength = compressedLength + 1;
       if (paddedLength > compressed.length) {
         compressed = new byte[ArrayUtil.oversize(paddedLength, 1)];
Index: lucene/core/src/java/org/apache/lucene/codecs/lucene50/Lucene50DocValuesConsumer.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/codecs/lucene50/Lucene50DocValuesConsumer.java	(revision 1647726)
+++ lucene/core/src/java/org/apache/lucene/codecs/lucene50/Lucene50DocValuesConsumer.java	(working copy)
@@ -324,7 +324,7 @@
     meta.writeVLong(count);
     meta.writeLong(startFP);
     
-    // if minLength == maxLength, its a fixed-length byte[], we are done (the addresses are implicit)
+    // if minLength == maxLength, it's a fixed-length byte[], we are done (the addresses are implicit)
     // otherwise, we need to record the length fields...
     if (minLength != maxLength) {
       meta.writeLong(data.getFilePointer());
@@ -346,7 +346,7 @@
   
   /** expert: writes a value dictionary for a sorted/sortedset field */
   private void addTermsDict(FieldInfo field, final Iterable<BytesRef> values) throws IOException {
-    // first check if its a "fixed-length" terms dict
+    // first check if it's a "fixed-length" terms dict
     int minLength = Integer.MAX_VALUE;
     int maxLength = Integer.MIN_VALUE;
     long numValues = 0;
@@ -371,7 +371,7 @@
       // now write the bytes: sharing prefixes within a block
       final long startFP = data.getFilePointer();
       // currently, we have to store the delta from expected for every 1/nth term
-      // we could avoid this, but its not much and less overall RAM than the previous approach!
+      // we could avoid this, but it's not much and less overall RAM than the previous approach!
       RAMOutputStream addressBuffer = new RAMOutputStream();
       MonotonicBlockPackedWriter termAddresses = new MonotonicBlockPackedWriter(addressBuffer, BLOCK_SIZE);
       // buffers up 16 terms
Index: lucene/core/src/java/org/apache/lucene/codecs/lucene50/Lucene50DocValuesFormat.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/codecs/lucene50/Lucene50DocValuesFormat.java	(revision 1647726)
+++ lucene/core/src/java/org/apache/lucene/codecs/lucene50/Lucene50DocValuesFormat.java	(working copy)
@@ -149,7 +149,7 @@
  *      Otherwise, the binary values are of variable size, and packed integer metadata (PackedVersion,BlockSize)
  *      is written for the addresses.
  *   <p>MissingOffset points to a byte[] containing a bitset of all documents that had a value for the field.
- *      If its -1, then there are no missing values. If its -2, all values are missing.
+ *      If it's -1, then there are no missing values. If it's -2, all values are missing.
  *   <li><a name="dvd" id="dvd"></a>
  *   <p>The DocValues data or .dvd file.</p>
  *   <p>For DocValues field, this stores the actual per-document data (the heavy-lifting)</p>
Index: lucene/core/src/java/org/apache/lucene/codecs/lucene50/Lucene50NormsConsumer.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/codecs/lucene50/Lucene50NormsConsumer.java	(revision 1647726)
+++ lucene/core/src/java/org/apache/lucene/codecs/lucene50/Lucene50NormsConsumer.java	(working copy)
@@ -349,7 +349,7 @@
 
   // specialized deduplication of long->ord for norms: 99.99999% of the time this will be a single-byte range.
   static class NormMap {
-    // we use short: at most we will add 257 values to this map before its rejected as too big above.
+    // we use short: at most we will add 257 values to this map before it's rejected as too big above.
     private final short[] ords = new short[256];
     final int[] freqs = new int[257];
     final byte[] values = new byte[257];
Index: lucene/core/src/java/org/apache/lucene/index/AutomatonTermsEnum.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/index/AutomatonTermsEnum.java	(revision 1647726)
+++ lucene/core/src/java/org/apache/lucene/index/AutomatonTermsEnum.java	(working copy)
@@ -110,7 +110,7 @@
     //System.out.println("ATE.nextSeekTerm term=" + term);
     if (term == null) {
       assert seekBytesRef.length() == 0;
-      // return the empty term, as its valid
+      // return the empty term, as it's valid
       if (runAutomaton.isAccept(runAutomaton.getInitialState())) {   
         return seekBytesRef.get();
       }
@@ -310,7 +310,7 @@
   private int backtrack(int position) {
     while (position-- > 0) {
       int nextChar = seekBytesRef.byteAt(position) & 0xff;
-      // if a character is 0xff its a dead-end too,
+      // if a character is 0xff it's a dead-end too,
       // because there is no higher character in binary sort order.
       if (nextChar++ != 0xff) {
         seekBytesRef.setByteAt(position, (byte) nextChar);
Index: lucene/core/src/java/org/apache/lucene/index/CheckIndex.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/index/CheckIndex.java	(revision 1647726)
+++ lucene/core/src/java/org/apache/lucene/index/CheckIndex.java	(working copy)
@@ -778,7 +778,7 @@
       } else {
         Bits liveDocs = reader.getLiveDocs();
         if (liveDocs != null) {
-          // its ok for it to be non-null here, as long as none are set right?
+          // it's ok for it to be non-null here, as long as none are set right?
           for (int j = 0; j < liveDocs.length(); j++) {
             if (!liveDocs.get(j)) {
               throw new RuntimeException("liveDocs mismatch: info says no deletions but doc " + j + " is deleted.");
Index: lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain.java	(revision 1647726)
+++ lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain.java	(working copy)
@@ -92,7 +92,7 @@
     writeNorms(state);
     writeDocValues(state);
     
-    // its possible all docs hit non-aborting exceptions...
+    // it's possible all docs hit non-aborting exceptions...
     initStoredFieldsWriter();
     fillStoredFields(numDocs);
     storedFieldsWriter.finish(state.fieldInfos, numDocs);
@@ -152,7 +152,7 @@
       // TODO: catch missing DV fields here?  else we have
       // null/"" depending on how docs landed in segments?
       // but we can't detect all cases, and we should leave
-      // this behavior undefined. dv is not "schemaless": its column-stride.
+      // this behavior undefined. dv is not "schemaless": it's column-stride.
       success = true;
     } finally {
       if (success) {
@@ -662,7 +662,7 @@
         // trigger streams to perform end-of-stream operations
         stream.end();
 
-        // TODO: maybe add some safety? then again, its already checked 
+        // TODO: maybe add some safety? then again, it's already checked 
         // when we come back around to the field...
         invertState.position += invertState.posIncrAttribute.getPositionIncrement();
         invertState.offset += invertState.offsetAttribute.endOffset();
Index: lucene/core/src/java/org/apache/lucene/index/DocumentsWriterFlushControl.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/index/DocumentsWriterFlushControl.java	(revision 1647726)
+++ lucene/core/src/java/org/apache/lucene/index/DocumentsWriterFlushControl.java	(working copy)
@@ -582,7 +582,7 @@
         assert !flushingWriters.containsKey(blockedFlush.dwpt) : "DWPT is already flushing";
         // Record the flushing DWPT to reduce flushBytes in doAfterFlush
         flushingWriters.put(blockedFlush.dwpt, Long.valueOf(blockedFlush.bytes));
-        // don't decr pending here - its already done when DWPT is blocked
+        // don't decr pending here - it's already done when DWPT is blocked
         flushQueue.add(blockedFlush.dwpt);
       }
     }
Index: lucene/core/src/java/org/apache/lucene/index/DocumentsWriterFlushQueue.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/index/DocumentsWriterFlushQueue.java	(revision 1647726)
+++ lucene/core/src/java/org/apache/lucene/index/DocumentsWriterFlushQueue.java	(working copy)
@@ -224,7 +224,7 @@
     protected void publish(IndexWriter writer) throws IOException {
       assert !published : "ticket was already publised - can not publish twice";
       published = true;
-      // its a global ticket - no segment to publish
+      // it's a global ticket - no segment to publish
       finishFlush(writer, null, frozenUpdates);
     }
 
Index: lucene/core/src/java/org/apache/lucene/index/IndexWriter.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/index/IndexWriter.java	(revision 1647726)
+++ lucene/core/src/java/org/apache/lucene/index/IndexWriter.java	(working copy)
@@ -4412,8 +4412,8 @@
         try {
           rollback();
         } catch (Throwable ignored) {
-          // it would be confusing to addSuppressed here, its unrelated to the disaster,
-          // and its possible our internal state is amiss anyway.
+          // it would be confusing to addSuppressed here, it's unrelated to the disaster,
+          // and it's possible our internal state is amiss anyway.
         }
       }
     }
@@ -4598,7 +4598,7 @@
   
   /**
    * Interface for internal atomic events. See {@link DocumentsWriter} for details. Events are executed concurrently and no order is guaranteed.
-   * Each event should only rely on the serializeability within it's process method. All actions that must happen before or after a certain action must be
+   * Each event should only rely on the serializeability within its process method. All actions that must happen before or after a certain action must be
    * encoded inside the {@link #process(IndexWriter, boolean, boolean)} method.
    *
    */
Index: lucene/core/src/java/org/apache/lucene/index/IndexableField.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/index/IndexableField.java	(revision 1647726)
+++ lucene/core/src/java/org/apache/lucene/index/IndexableField.java	(working copy)
@@ -42,7 +42,7 @@
    *              custom field types (like StringField and NumericField) that do not use
    *              the analyzer to still have good performance. Note: the passed-in type
    *              may be inappropriate, for example if you mix up different types of Fields
-   *              for the same field name. So its the responsibility of the implementation to
+   *              for the same field name. So it's the responsibility of the implementation to
    *              check.
    * @return TokenStream value for indexing the document.  Should always return
    *         a non-null value if the field is to be indexed
Index: lucene/core/src/java/org/apache/lucene/index/MultiDocValues.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/index/MultiDocValues.java	(revision 1647726)
+++ lucene/core/src/java/org/apache/lucene/index/MultiDocValues.java	(working copy)
@@ -374,7 +374,7 @@
   /** maps per-segment ordinals to/from global ordinal space */
   // TODO: we could also have a utility method to merge Terms[] and use size() as a weight when we need it
   // TODO: use more efficient packed ints structures?
-  // TODO: pull this out? its pretty generic (maps between N ord()-enabled TermsEnums) 
+  // TODO: pull this out? it's pretty generic (maps between N ord()-enabled TermsEnums) 
   public static class OrdinalMap implements Accountable {
 
     private static class SegmentMap implements Accountable {
Index: lucene/core/src/java/org/apache/lucene/index/OrdTermState.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/index/OrdTermState.java	(revision 1647726)
+++ lucene/core/src/java/org/apache/lucene/index/OrdTermState.java	(working copy)
@@ -23,7 +23,7 @@
  * @lucene.experimental
  */
 public class OrdTermState extends TermState {
-  /** Term ordinal, i.e. it's position in the full list of
+  /** Term ordinal, i.e. its position in the full list of
    *  sorted terms. */
   public long ord;
 
Index: lucene/core/src/java/org/apache/lucene/index/SegmentCoreReaders.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/index/SegmentCoreReaders.java	(revision 1647726)
+++ lucene/core/src/java/org/apache/lucene/index/SegmentCoreReaders.java	(working copy)
@@ -97,7 +97,7 @@
   SegmentCoreReaders(SegmentReader owner, Directory dir, SegmentCommitInfo si, IOContext context) throws IOException {
 
     final Codec codec = si.info.getCodec();
-    final Directory cfsDir; // confusing name: if (cfs) its the cfsdir, otherwise its the segment's directory.
+    final Directory cfsDir; // confusing name: if (cfs) it's the cfsdir, otherwise it's the segment's directory.
 
     boolean success = false;
     
Index: lucene/core/src/java/org/apache/lucene/index/SegmentInfo.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/index/SegmentInfo.java	(revision 1647726)
+++ lucene/core/src/java/org/apache/lucene/index/SegmentInfo.java	(working copy)
@@ -35,7 +35,7 @@
 import org.apache.lucene.util.Version;
 
 /**
- * Information about a segment such as it's name, directory, and files related
+ * Information about a segment such as its name, directory, and files related
  * to the segment.
  *
  * @lucene.experimental
Index: lucene/core/src/java/org/apache/lucene/index/SegmentWriteState.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/index/SegmentWriteState.java	(revision 1647726)
+++ lucene/core/src/java/org/apache/lucene/index/SegmentWriteState.java	(working copy)
@@ -112,7 +112,7 @@
   }
   
   // currently only used by assert? clean up and make real check?
-  // either its a segment suffix (_X_Y) or its a parseable generation
+  // either it's a segment suffix (_X_Y) or it's a parseable generation
   // TODO: this is very confusing how ReadersAndUpdates passes generations via
   // this mechanism, maybe add 'generation' explicitly to ctor create the 'actual suffix' here?
   private boolean assertSegmentSuffix(String segmentSuffix) {
Index: lucene/core/src/java/org/apache/lucene/index/SortedSetDocValuesWriter.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/index/SortedSetDocValuesWriter.java	(revision 1647726)
+++ lucene/core/src/java/org/apache/lucene/index/SortedSetDocValuesWriter.java	(working copy)
@@ -92,7 +92,7 @@
     int count = 0;
     for (int i = 0; i < currentUpto; i++) {
       int termID = currentValues[i];
-      // if its not a duplicate
+      // if it's not a duplicate
       if (termID != lastValue) {
         pending.add(termID); // record the term id
         count++;
Index: lucene/core/src/java/org/apache/lucene/index/package.html
===================================================================
--- lucene/core/src/java/org/apache/lucene/index/package.html	(revision 1647726)
+++ lucene/core/src/java/org/apache/lucene/index/package.html	(working copy)
@@ -58,7 +58,7 @@
 // access term vector fields for a specified document
 Fields fields = reader.getTermVectors(docid);
 </pre>
-Fields implements Java's Iterable interface, so its easy to enumerate the
+Fields implements Java's Iterable interface, so it's easy to enumerate the
 list of fields:
 <pre class="prettyprint">
 // enumerate list of fields
Index: lucene/core/src/java/org/apache/lucene/search/BooleanQuery.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/search/BooleanQuery.java	(revision 1647726)
+++ lucene/core/src/java/org/apache/lucene/search/BooleanQuery.java	(working copy)
@@ -213,7 +213,7 @@
     public float coord(int overlap, int maxOverlap) {
       // LUCENE-4300: in most cases of maxOverlap=1, BQ rewrites itself away,
       // so coord() is not applied. But when BQ cannot optimize itself away
-      // for a single clause (minNrShouldMatch, prohibited clauses, etc), its
+      // for a single clause (minNrShouldMatch, prohibited clauses, etc), it's
       // important not to apply coord(1,1) for consistency, it might not be 1.0F
       return maxOverlap == 1 ? 1F : similarity.coord(overlap, maxOverlap);
     }
@@ -400,14 +400,14 @@
       
       // conjunction-disjunction mix:
       // we create the required and optional pieces with coord disabled, and then
-      // combine the two: if minNrShouldMatch > 0, then its a conjunction: because the
-      // optional side must match. otherwise its required + optional, factoring the
+      // combine the two: if minNrShouldMatch > 0, then it's a conjunction: because the
+      // optional side must match. otherwise it's required + optional, factoring the
       // number of optional terms into the coord calculation
       
       Scorer req = excl(req(required, true), prohibited);
       Scorer opt = opt(optional, minShouldMatch, true);
 
-      // TODO: clean this up: its horrible
+      // TODO: clean this up: it's horrible
       if (disableCoord) {
         if (minShouldMatch > 0) {
           return new ConjunctionScorer(this, new Scorer[] { req, opt }, 1F);
Index: lucene/core/src/java/org/apache/lucene/search/ControlledRealTimeReopenThread.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/search/ControlledRealTimeReopenThread.java	(revision 1647726)
+++ lucene/core/src/java/org/apache/lucene/search/ControlledRealTimeReopenThread.java	(working copy)
@@ -165,7 +165,7 @@
       // not sleep for much or any longer before reopening:
       reopenLock.lock();
 
-      // Need to find waitingGen inside lock as its used to determine
+      // Need to find waitingGen inside lock as it's used to determine
       // stale time
       waitingGen = Math.max(waitingGen, targetGen);
 
Index: lucene/core/src/java/org/apache/lucene/search/package.html
===================================================================
--- lucene/core/src/java/org/apache/lucene/search/package.html	(revision 1647726)
+++ lucene/core/src/java/org/apache/lucene/search/package.html	(working copy)
@@ -521,7 +521,7 @@
         back
         out of Lucene (similar to Doug adding SpanQuery functionality).</p>
 
-<!-- TODO: integrate this better, its better served as an intro than an appendix -->
+<!-- TODO: integrate this better, it's better served as an intro than an appendix -->
 
 
 <a name="algorithm"></a>
Index: lucene/core/src/java/org/apache/lucene/search/similarities/Similarity.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/search/similarities/Similarity.java	(revision 1647726)
+++ lucene/core/src/java/org/apache/lucene/search/similarities/Similarity.java	(working copy)
@@ -233,7 +233,7 @@
     /** The value for normalization of contained query clauses (e.g. sum of squared weights).
      * <p>
      * NOTE: a Similarity implementation might not use any query normalization at all,
-     * its not required. However, if it wants to participate in query normalization,
+     * it's not required. However, if it wants to participate in query normalization,
      * it can return a value here.
      */
     public abstract float getValueForNormalization();
@@ -241,7 +241,7 @@
     /** Assigns the query normalization factor and boost from parent queries to this.
      * <p>
      * NOTE: a Similarity implementation might not use this normalized value at all,
-     * its not required. However, its usually a good idea to at least incorporate 
+     * it's not required. However, it's usually a good idea to at least incorporate 
      * the topLevelBoost (e.g. from an outer BooleanQuery) into its score.
      */
     public abstract void normalize(float queryNorm, float topLevelBoost);
Index: lucene/core/src/java/org/apache/lucene/search/spans/SpanTermQuery.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/search/spans/SpanTermQuery.java	(revision 1647726)
+++ lucene/core/src/java/org/apache/lucene/search/spans/SpanTermQuery.java	(working copy)
@@ -92,7 +92,7 @@
     final TermState state;
     if (termContext == null) {
       // this happens with span-not query, as it doesn't include the NOT side in extractTerms()
-      // so we seek to the term now in this segment..., this sucks because its ugly mostly!
+      // so we seek to the term now in this segment..., this sucks because it's ugly mostly!
       final Terms terms = context.reader().terms(term.field());
       if (terms != null) {
         final TermsEnum termsEnum = terms.iterator(null);
Index: lucene/core/src/java/org/apache/lucene/store/ByteBufferIndexInput.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/store/ByteBufferIndexInput.java	(revision 1647726)
+++ lucene/core/src/java/org/apache/lucene/store/ByteBufferIndexInput.java	(working copy)
@@ -213,7 +213,7 @@
     try {
       return buffers[bi].getShort((int) (pos & chunkSizeMask));
     } catch (IndexOutOfBoundsException ioobe) {
-      // either its a boundary, or read past EOF, fall back:
+      // either it's a boundary, or read past EOF, fall back:
       setPos(pos, bi);
       return readShort();
     } catch (NullPointerException npe) {
@@ -227,7 +227,7 @@
     try {
       return buffers[bi].getInt((int) (pos & chunkSizeMask));
     } catch (IndexOutOfBoundsException ioobe) {
-      // either its a boundary, or read past EOF, fall back:
+      // either it's a boundary, or read past EOF, fall back:
       setPos(pos, bi);
       return readInt();
     } catch (NullPointerException npe) {
@@ -241,7 +241,7 @@
     try {
       return buffers[bi].getLong((int) (pos & chunkSizeMask));
     } catch (IndexOutOfBoundsException ioobe) {
-      // either its a boundary, or read past EOF, fall back:
+      // either it's a boundary, or read past EOF, fall back:
       setPos(pos, bi);
       return readLong();
     } catch (NullPointerException npe) {
Index: lucene/core/src/java/org/apache/lucene/store/MMapDirectory.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/store/MMapDirectory.java	(revision 1647726)
+++ lucene/core/src/java/org/apache/lucene/store/MMapDirectory.java	(working copy)
@@ -244,7 +244,7 @@
     final String originalMessage;
     final Throwable originalCause;
     if (ioe.getCause() instanceof OutOfMemoryError) {
-      // nested OOM confuses users, because its "incorrect", just print a plain message:
+      // nested OOM confuses users, because it's "incorrect", just print a plain message:
       originalMessage = "Map failed";
       originalCause = null;
     } else {
Index: lucene/core/src/java/org/apache/lucene/store/NativeFSLockFactory.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/store/NativeFSLockFactory.java	(revision 1647726)
+++ lucene/core/src/java/org/apache/lucene/store/NativeFSLockFactory.java	(working copy)
@@ -114,7 +114,7 @@
         Files.createFile(path);
       } catch (IOException ignore) {
         // we must create the file to have a truly canonical path.
-        // if its already created, we don't care. if it cant be created, it will fail below.
+        // if it's already created, we don't care. if it cant be created, it will fail below.
       }
       final Path canonicalPath = path.toRealPath();
       // Make sure nobody else in-process has this lock held
Index: lucene/core/src/java/org/apache/lucene/store/RAMDirectory.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/store/RAMDirectory.java	(revision 1647726)
+++ lucene/core/src/java/org/apache/lucene/store/RAMDirectory.java	(working copy)
@@ -105,7 +105,7 @@
   public final String[] listAll() {
     ensureOpen();
     // NOTE: this returns a "weakly consistent view". Unless we change Dir API, keep this,
-    // and do not synchronize or anything stronger. its great for testing!
+    // and do not synchronize or anything stronger. it's great for testing!
     // NOTE: fileMap.keySet().toArray(new String[0]) is broken in non Sun JDKs,
     // and the code below is resilient to map changes during the array population.
     Set<String> fileNames = fileMap.keySet();
Index: lucene/core/src/java/org/apache/lucene/util/IOUtils.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/util/IOUtils.java	(revision 1647726)
+++ lucene/core/src/java/org/apache/lucene/util/IOUtils.java	(working copy)
@@ -140,7 +140,7 @@
    * the read charset doesn't match the expected {@link Charset}. 
    * <p>
    * Decoding readers are useful to load configuration files, stopword lists or synonym files
-   * to detect character set problems. However, its not recommended to use as a common purpose 
+   * to detect character set problems. However, it's not recommended to use as a common purpose 
    * reader.
    * 
    * @param stream the stream to wrap in a reader
@@ -160,7 +160,7 @@
    * the read charset doesn't match the expected {@link Charset}. 
    * <p>
    * Decoding readers are useful to load configuration files, stopword lists or synonym files
-   * to detect character set problems. However, its not recommended to use as a common purpose 
+   * to detect character set problems. However, it's not recommended to use as a common purpose 
    * reader.
    * @param clazz the class used to locate the resource
    * @param resource the resource name to load
@@ -505,7 +505,7 @@
       devinfo = sysinfo.resolve(devName);
     }
     
-    // read first byte from rotational, its a 1 if it spins.
+    // read first byte from rotational, it's a 1 if it spins.
     Path info = devinfo.resolve("queue/rotational");
     try (InputStream stream = Files.newInputStream(info)) {
       return stream.read() == '1'; 
@@ -518,7 +518,7 @@
     FileStore store = Files.getFileStore(path);
     String mount = getMountPoint(store);
 
-    // find the "matching" FileStore from system list, its the one we want.
+    // find the "matching" FileStore from system list, it's the one we want.
     for (FileStore fs : path.getFileSystem().getFileStores()) {
       if (mount.equals(getMountPoint(fs))) {
         return fs;
Index: lucene/core/src/java/org/apache/lucene/util/automaton/createLevAutomata.py
===================================================================
--- lucene/core/src/java/org/apache/lucene/util/automaton/createLevAutomata.py	(revision 1647726)
+++ lucene/core/src/java/org/apache/lucene/util/automaton/createLevAutomata.py	(working copy)
@@ -325,7 +325,7 @@
   minErrors = []
   for i in xrange(len(stateMap2)-1):
     w('//   %s -> %s' % (i, stateMap2[i]))
-    # we replace t-notation as its not relevant here
+    # we replace t-notation as it's not relevant here
     st = stateMap2[i].replace('t', '')
     
     v = eval(st)
Index: lucene/core/src/test/org/apache/lucene/codecs/lucene50/TestBlockPostingsFormat3.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/codecs/lucene50/TestBlockPostingsFormat3.java	(revision 1647726)
+++ lucene/core/src/test/org/apache/lucene/codecs/lucene50/TestBlockPostingsFormat3.java	(working copy)
@@ -492,7 +492,7 @@
       assertEquals(freq, rightDocs.freq());
       for (int i = 0; i < freq; i++) {
         assertEquals(leftDocs.nextPosition(), rightDocs.nextPosition());
-        // we don't compare the payloads, its allowed that one is empty etc
+        // we don't compare the payloads, it's allowed that one is empty etc
       }
     }
   }
Index: lucene/core/src/test/org/apache/lucene/document/TestBinaryDocument.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/document/TestBinaryDocument.java	(revision 1647726)
+++ lucene/core/src/test/org/apache/lucene/document/TestBinaryDocument.java	(working copy)
@@ -61,13 +61,13 @@
     StoredDocument docFromReader = reader.document(0);
     assertTrue(docFromReader != null);
     
-    /** fetch the binary stored field and compare it's content with the original one */
+    /** fetch the binary stored field and compare its content with the original one */
     BytesRef bytes = docFromReader.getBinaryValue("binaryStored");
     assertNotNull(bytes);
     String binaryFldStoredTest = new String(bytes.bytes, bytes.offset, bytes.length, StandardCharsets.UTF_8);
     assertTrue(binaryFldStoredTest.equals(binaryValStored));
     
-    /** fetch the string field and compare it's content with the original one */
+    /** fetch the string field and compare its content with the original one */
     String stringFldStoredTest = docFromReader.get("stringStored");
     assertTrue(stringFldStoredTest.equals(binaryValStored));
     
@@ -95,7 +95,7 @@
     StoredDocument docFromReader = reader.document(0);
     assertTrue(docFromReader != null);
     
-    /** fetch the binary compressed field and compare it's content with the original one */
+    /** fetch the binary compressed field and compare its content with the original one */
     String binaryFldCompressedTest = new String(CompressionTools.decompress(docFromReader.getBinaryValue("binaryCompressed")), StandardCharsets.UTF_8);
     assertTrue(binaryFldCompressedTest.equals(binaryValCompressed));
     assertTrue(CompressionTools.decompressString(docFromReader.getBinaryValue("stringCompressed")).equals(binaryValCompressed));
Index: lucene/core/src/test/org/apache/lucene/index/TestBagOfPositions.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/index/TestBagOfPositions.java	(revision 1647726)
+++ lucene/core/src/test/org/apache/lucene/index/TestBagOfPositions.java	(working copy)
@@ -38,7 +38,7 @@
  * Simple test that adds numeric terms, where each term has the 
  * totalTermFreq of its integer value, and checks that the totalTermFreq is correct. 
  */
-// TODO: somehow factor this with BagOfPostings? its almost the same
+// TODO: somehow factor this with BagOfPostings? it's almost the same
 @SuppressCodecs({"Direct", "Memory"}) // at night this makes like 200k/300k docs and will make Direct's heart beat!
 public class TestBagOfPositions extends LuceneTestCase {
   public void test() throws Exception {
Index: lucene/core/src/test/org/apache/lucene/index/TestDirectoryReader.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/index/TestDirectoryReader.java	(revision 1647726)
+++ lucene/core/src/test/org/apache/lucene/index/TestDirectoryReader.java	(working copy)
@@ -939,7 +939,7 @@
   
     reader.close();
   
-    // Close the top reader, its the only one that should be closed
+    // Close the top reader, it's the only one that should be closed
     assertEquals(1, closeCount[0]);
     writer.close();
   
Index: lucene/core/src/test/org/apache/lucene/index/TestDuelingCodecs.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/index/TestDuelingCodecs.java	(revision 1647726)
+++ lucene/core/src/test/org/apache/lucene/index/TestDuelingCodecs.java	(working copy)
@@ -50,7 +50,7 @@
   public void setUp() throws Exception {
     super.setUp();
 
-    // for now its SimpleText vs Default(random postings format)
+    // for now it's SimpleText vs Default(random postings format)
     // as this gives the best overall coverage. when we have more
     // codecs we should probably pick 2 from Codec.availableCodecs()
     
@@ -125,7 +125,7 @@
    */
   public static void createRandomIndex(int numdocs, RandomIndexWriter writer, long seed) throws IOException {
     Random random = new Random(seed);
-    // primary source for our data is from linefiledocs, its realistic.
+    // primary source for our data is from linefiledocs, it's realistic.
     LineFileDocs lineFileDocs = new LineFileDocs(random);
 
     // TODO: we should add other fields that use things like docs&freqs but omit positions,
Index: lucene/core/src/test/org/apache/lucene/index/TestFieldReuse.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/index/TestFieldReuse.java	(revision 1647726)
+++ lucene/core/src/test/org/apache/lucene/index/TestFieldReuse.java	(working copy)
@@ -57,7 +57,7 @@
         new int[]    { 3 }
     );
     
-    // pass a bogus stream and ensure its still ok
+    // pass a bogus stream and ensure it's still ok
     stringField = new StringField("foo", "beer", Field.Store.NO);
     TokenStream bogus = new NumericTokenStream();
     ts = stringField.tokenStream(null, bogus);
@@ -84,7 +84,7 @@
     assertSame(ts, ts2);
     assertNumericContents(20, ts);
     
-    // pass a bogus stream and ensure its still ok
+    // pass a bogus stream and ensure it's still ok
     intField = new IntField("foo", 2343, Field.Store.NO);
     TokenStream bogus = new CannedTokenStream(new Token("bogus", 0, 5));
     ts = intField.tokenStream(null, bogus);
Index: lucene/core/src/test/org/apache/lucene/index/TestIndexWriter.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/index/TestIndexWriter.java	(revision 1647726)
+++ lucene/core/src/test/org/apache/lucene/index/TestIndexWriter.java	(working copy)
@@ -165,7 +165,7 @@
 
 
 
-    // TODO: we have the logic in MDW to do this check, and its better, because it knows about files it tried
+    // TODO: we have the logic in MDW to do this check, and it's better, because it knows about files it tried
     // to delete but couldn't: we should replace this!!!!
     public static void assertNoUnreferencedFiles(Directory dir, String message) throws IOException {
       if (dir instanceof MockDirectoryWrapper) {
Index: lucene/core/src/test/org/apache/lucene/index/TestIndexWriterConfig.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/index/TestIndexWriterConfig.java	(revision 1647726)
+++ lucene/core/src/test/org/apache/lucene/index/TestIndexWriterConfig.java	(working copy)
@@ -235,7 +235,7 @@
     }
 
     // Test Similarity: 
-    // we shouldnt assert what the default is, just that its not null.
+    // we shouldnt assert what the default is, just that it's not null.
     assertTrue(IndexSearcher.getDefaultSimilarity() == conf.getSimilarity());
     conf.setSimilarity(new MySimilarity());
     assertEquals(MySimilarity.class, conf.getSimilarity().getClass());
Index: lucene/core/src/test/org/apache/lucene/index/TestIndexWriterOnJRECrash.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/index/TestIndexWriterOnJRECrash.java	(revision 1647726)
+++ lucene/core/src/test/org/apache/lucene/index/TestIndexWriterOnJRECrash.java	(working copy)
@@ -40,7 +40,7 @@
 import com.carrotsearch.randomizedtesting.SeedUtils;
 /**
  * Runs TestNRTThreads in a separate process, crashes the JRE in the middle
- * of execution, then runs checkindex to make sure its not corrupt.
+ * of execution, then runs checkindex to make sure it's not corrupt.
  */
 public class TestIndexWriterOnJRECrash extends TestNRTThreads {
   private Path tempDir;
Index: lucene/core/src/test/org/apache/lucene/search/TestFieldCacheRangeFilter.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/search/TestFieldCacheRangeFilter.java	(revision 1647726)
+++ lucene/core/src/test/org/apache/lucene/search/TestFieldCacheRangeFilter.java	(working copy)
@@ -372,7 +372,7 @@
     assertEquals("inverse range", 0, result.length);
   }
   
-  // float and double tests are a bit minimalistic, but its complicated, because missing precision
+  // float and double tests are a bit minimalistic, but it's complicated, because missing precision
   
   @Test
   public void testFieldCacheRangeFilterFloats() throws IOException {
Index: lucene/core/src/test/org/apache/lucene/search/TestRegexpQuery.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/search/TestRegexpQuery.java	(revision 1647726)
+++ lucene/core/src/test/org/apache/lucene/search/TestRegexpQuery.java	(working copy)
@@ -117,7 +117,7 @@
   
   /**
    * Test a corner case for backtracking: In this case the term dictionary has
-   * 493432 followed by 49344. When backtracking from 49343... to 4934, its
+   * 493432 followed by 49344. When backtracking from 49343... to 4934, it's
    * necessary to test that 4934 itself is ok before trying to append more
    * characters.
    */
Index: lucene/core/src/test/org/apache/lucene/search/TestSubScorerFreqs.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/search/TestSubScorerFreqs.java	(revision 1647726)
+++ lucene/core/src/test/org/apache/lucene/search/TestSubScorerFreqs.java	(working copy)
@@ -171,7 +171,7 @@
       boolean includeOptional = occur.contains("SHOULD");
       for (int i = 0; i < maxDocs; i++) {
         Map<Query, Float> doc0 = c.docCounts.get(i);
-        // Y doesnt exist in the index, so its not in the scorer tree
+        // Y doesnt exist in the index, so it's not in the scorer tree
         assertEquals(4, doc0.size());
         assertEquals(1.0F, doc0.get(aQuery), FLOAT_TOLERANCE);
         assertEquals(4.0F, doc0.get(dQuery), FLOAT_TOLERANCE);
@@ -180,7 +180,7 @@
         }
 
         Map<Query, Float> doc1 = c.docCounts.get(++i);
-        // Y doesnt exist in the index, so its not in the scorer tree
+        // Y doesnt exist in the index, so it's not in the scorer tree
         assertEquals(4, doc1.size());
         assertEquals(1.0F, doc1.get(aQuery), FLOAT_TOLERANCE);
         assertEquals(1.0F, doc1.get(dQuery), FLOAT_TOLERANCE);
Index: lucene/core/src/test/org/apache/lucene/search/similarities/TestSimilarity2.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/search/similarities/TestSimilarity2.java	(revision 1647726)
+++ lucene/core/src/test/org/apache/lucene/search/similarities/TestSimilarity2.java	(working copy)
@@ -71,7 +71,7 @@
     sims.add(new LMJelinekMercerSimilarity(0.7f));
   }
   
-  /** because of stupid things like querynorm, its possible we computeStats on a field that doesnt exist at all
+  /** because of stupid things like querynorm, it's possible we computeStats on a field that doesnt exist at all
    *  test this against a totally empty index, to make sure sims handle it
    */
   public void testEmptyIndex() throws Exception {
Index: lucene/core/src/test/org/apache/lucene/store/TestNRTCachingDirectory.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/store/TestNRTCachingDirectory.java	(revision 1647726)
+++ lucene/core/src/test/org/apache/lucene/store/TestNRTCachingDirectory.java	(working copy)
@@ -39,7 +39,7 @@
 
 public class TestNRTCachingDirectory extends BaseDirectoryTestCase {
 
-  // TODO: RAMDir used here, because its still too slow to use e.g. SimpleFS
+  // TODO: RAMDir used here, because it's still too slow to use e.g. SimpleFS
   // for the threads tests... maybe because of the synchronization in listAll?
   // would be good to investigate further...
   @Override
Index: lucene/core/src/test/org/apache/lucene/util/TestIOUtils.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/util/TestIOUtils.java	(revision 1647726)
+++ lucene/core/src/test/org/apache/lucene/util/TestIOUtils.java	(working copy)
@@ -214,7 +214,7 @@
     
     @Override
     public void checkAccess(Path path, AccessMode... modes) throws IOException {
-      // TODO: kinda screwed up how we do this, but its easy to get lost. just unravel completely.
+      // TODO: kinda screwed up how we do this, but it's easy to get lost. just unravel completely.
       delegate.checkAccess(FilterPath.unwrap(path), modes);
     }
 
Index: lucene/core/src/test/org/apache/lucene/util/TestNamedSPILoader.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/util/TestNamedSPILoader.java	(revision 1647726)
+++ lucene/core/src/test/org/apache/lucene/util/TestNamedSPILoader.java	(working copy)
@@ -21,7 +21,7 @@
  * limitations under the License.
  */
 
-// TODO: maybe we should test this with mocks, but its easy
+// TODO: maybe we should test this with mocks, but it's easy
 // enough to test the basics via Codec
 public class TestNamedSPILoader extends LuceneTestCase {
   
@@ -31,7 +31,7 @@
     assertEquals(currentName, codec.getName());
   }
   
-  // we want an exception if its not found.
+  // we want an exception if it's not found.
   public void testBogusLookup() {
     try {
       Codec.forName("dskfdskfsdfksdfdsf");
Index: lucene/core/src/test/org/apache/lucene/util/fst/TestFSTs.java
===================================================================
--- lucene/core/src/test/org/apache/lucene/util/fst/TestFSTs.java	(revision 1647726)
+++ lucene/core/src/test/org/apache/lucene/util/fst/TestFSTs.java	(working copy)
@@ -1417,10 +1417,10 @@
       Util.TopResults<Long> r = Util.shortestPaths(fst, arc, fst.outputs.getNoOutput(), minLongComparator, topN, true);
       assertTrue(r.isComplete);
 
-      // 2. go thru whole treemap (slowCompletor) and check its actually the best suggestion
+      // 2. go thru whole treemap (slowCompletor) and check it's actually the best suggestion
       final List<Result<Long>> matches = new ArrayList<>();
 
-      // TODO: could be faster... but its slowCompletor for a reason
+      // TODO: could be faster... but it's slowCompletor for a reason
       for (Map.Entry<String,Long> e : slowCompletor.entrySet()) {
         if (e.getKey().startsWith(prefix)) {
           //System.out.println("  consider " + e.getKey());
@@ -1538,10 +1538,10 @@
 
       Util.TopResults<Pair<Long,Long>> r = Util.shortestPaths(fst, arc, fst.outputs.getNoOutput(), minPairWeightComparator, topN, true);
       assertTrue(r.isComplete);
-      // 2. go thru whole treemap (slowCompletor) and check its actually the best suggestion
+      // 2. go thru whole treemap (slowCompletor) and check it's actually the best suggestion
       final List<Result<Pair<Long,Long>>> matches = new ArrayList<>();
 
-      // TODO: could be faster... but its slowCompletor for a reason
+      // TODO: could be faster... but it's slowCompletor for a reason
       for (Map.Entry<String,TwoLongs> e : slowCompletor.entrySet()) {
         if (e.getKey().startsWith(prefix)) {
           //System.out.println("  consider " + e.getKey());
Index: lucene/demo/build.xml
===================================================================
--- lucene/demo/build.xml	(revision 1647726)
+++ lucene/demo/build.xml	(working copy)
@@ -42,7 +42,7 @@
 
   <target name="javadocs" depends="javadocs-analyzers-common,javadocs-queryparser,javadocs-facet,javadocs-expressions,compile-core,check-javadocs-uptodate"
           unless="javadocs-uptodate-${name}">
-    <!-- we link the example source in the javadocs, as its ref'ed elsewhere -->
+    <!-- we link the example source in the javadocs, as it's ref'ed elsewhere -->
     <invoke-module-javadoc linksource="yes">
       <links>
         <link href="../analyzers-common"/>
Index: lucene/expressions/src/java/org/apache/lucene/expressions/js/JavascriptCompiler.java
===================================================================
--- lucene/expressions/src/java/org/apache/lucene/expressions/js/JavascriptCompiler.java	(revision 1647726)
+++ lucene/expressions/src/java/org/apache/lucene/expressions/js/JavascriptCompiler.java	(working copy)
@@ -104,7 +104,7 @@
     return org.objectweb.asm.commons.Method.getMethod(method);
   }
   
-  // This maximum length is theoretically 65535 bytes, but as its CESU-8 encoded we dont know how large it is in bytes, so be safe
+  // This maximum length is theoretically 65535 bytes, but as it's CESU-8 encoded we dont know how large it is in bytes, so be safe
   // rcmuir: "If your ranking function is that large you need to check yourself into a mental institution!"
   private static final int MAX_SOURCE_LENGTH = 16384;
   
Index: lucene/facet/src/test/org/apache/lucene/facet/taxonomy/TestTaxonomyCombined.java
===================================================================
--- lucene/facet/src/test/org/apache/lucene/facet/taxonomy/TestTaxonomyCombined.java	(revision 1647726)
+++ lucene/facet/src/test/org/apache/lucene/facet/taxonomy/TestTaxonomyCombined.java	(working copy)
@@ -224,7 +224,7 @@
     // Now, open the same taxonomy and add the same categories again.
     // After a few categories, the LuceneTaxonomyWriter implementation
     // will stop looking for each category on disk, and rather read them
-    // all into memory and close it's reader. The bug was that it closed
+    // all into memory and close its reader. The bug was that it closed
     // the reader, but forgot that it did (because it didn't set the reader
     // reference to null).
     tw = new DirectoryTaxonomyWriter(indexDir);
Index: lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/DefaultPassageFormatter.java
===================================================================
--- lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/DefaultPassageFormatter.java	(revision 1647726)
+++ lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/DefaultPassageFormatter.java	(working copy)
@@ -62,7 +62,7 @@
     StringBuilder sb = new StringBuilder();
     int pos = 0;
     for (Passage passage : passages) {
-      // don't add ellipsis if its the first one, or if its connected.
+      // don't add ellipsis if it's the first one, or if it's connected.
       if (passage.startOffset > pos && pos > 0) {
         sb.append(ellipsis);
       }
@@ -70,7 +70,7 @@
       for (int i = 0; i < passage.numMatches; i++) {
         int start = passage.matchStarts[i];
         int end = passage.matchEnds[i];
-        // its possible to have overlapping terms
+        // it's possible to have overlapping terms
         if (start > pos) {
           append(sb, content, pos, start);
         }
@@ -81,7 +81,7 @@
           pos = end;
         }
       }
-      // its possible a "term" from the analyzer could span a sentence boundary.
+      // it's possible a "term" from the analyzer could span a sentence boundary.
       append(sb, content, pos, Math.max(pos, passage.endOffset));
       pos = passage.endOffset;
     }
Index: lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/MultiTermHighlighting.java
===================================================================
--- lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/MultiTermHighlighting.java	(revision 1647726)
+++ lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/MultiTermHighlighting.java	(working copy)
@@ -157,7 +157,7 @@
         final CharsRef scratch = new CharsRef();
         final Comparator<CharsRef> comparator = CharsRef.getUTF16SortedAsUTF8Comparator();
         
-        // this is *not* an automaton, but its very simple
+        // this is *not* an automaton, but it's very simple
         list.add(new CharacterRunAutomaton(Automata.makeEmpty()) {
           @Override
           public boolean run(char[] s, int offset, int length) {
Index: lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/Passage.java
===================================================================
--- lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/Passage.java	(revision 1647726)
+++ lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/Passage.java	(working copy)
@@ -141,7 +141,7 @@
   /**
    * End offsets of the term matches, corresponding with {@link #getMatchStarts}. 
    * <p>
-   * Only {@link #getNumMatches} are valid. Note that its possible that an end offset 
+   * Only {@link #getNumMatches} are valid. Note that it's possible that an end offset 
    * could exceed beyond the bounds of the passage ({@link #getEndOffset()}), if the 
    * Analyzer produced a term which spans a passage boundary.
    */
Index: lucene/highlighter/src/java/org/apache/lucene/search/vectorhighlight/SimpleBoundaryScanner.java
===================================================================
--- lucene/highlighter/src/java/org/apache/lucene/search/vectorhighlight/SimpleBoundaryScanner.java	(revision 1647726)
+++ lucene/highlighter/src/java/org/apache/lucene/search/vectorhighlight/SimpleBoundaryScanner.java	(working copy)
@@ -66,7 +66,7 @@
       if( boundaryChars.contains( buffer.charAt( offset - 1 ) ) ) return offset;
       offset--;
     }
-    // if we scanned up to the start of the text, return it, its a "boundary"
+    // if we scanned up to the start of the text, return it, it's a "boundary"
     if (offset == 0) {
       return 0;
     }
Index: lucene/highlighter/src/test/org/apache/lucene/search/postingshighlight/TestMultiTermHighlighting.java
===================================================================
--- lucene/highlighter/src/test/org/apache/lucene/search/postingshighlight/TestMultiTermHighlighting.java	(revision 1647726)
+++ lucene/highlighter/src/test/org/apache/lucene/search/postingshighlight/TestMultiTermHighlighting.java	(working copy)
@@ -837,7 +837,7 @@
             StringBuilder sb = new StringBuilder();
             int pos = 0;
             for (Passage passage : passages) {
-              // don't add ellipsis if its the first one, or if its connected.
+              // don't add ellipsis if it's the first one, or if it's connected.
               if (passage.startOffset > pos && pos > 0) {
                 sb.append("... ");
               }
@@ -845,7 +845,7 @@
               for (int i = 0; i < passage.numMatches; i++) {
                 int start = passage.matchStarts[i];
                 int end = passage.matchEnds[i];
-                // its possible to have overlapping terms
+                // it's possible to have overlapping terms
                 if (start > pos) {
                   sb.append(content, pos, start);
                 }
@@ -859,7 +859,7 @@
                   pos = end;
                 }
               }
-              // its possible a "term" from the analyzer could span a sentence boundary.
+              // it's possible a "term" from the analyzer could span a sentence boundary.
               sb.append(content, pos, Math.max(pos, passage.endOffset));
               pos = passage.endOffset;
             }
Index: lucene/highlighter/src/test/org/apache/lucene/search/vectorhighlight/BreakIteratorBoundaryScannerTest.java
===================================================================
--- lucene/highlighter/src/test/org/apache/lucene/search/vectorhighlight/BreakIteratorBoundaryScannerTest.java	(revision 1647726)
+++ lucene/highlighter/src/test/org/apache/lucene/search/vectorhighlight/BreakIteratorBoundaryScannerTest.java	(working copy)
@@ -57,7 +57,7 @@
 
   public void testSentenceBoundary() throws Exception {
     StringBuilder text = new StringBuilder(TEXT);
-    // we test this with default locale, its randomized by LuceneTestCase
+    // we test this with default locale, it's randomized by LuceneTestCase
     BreakIterator bi = BreakIterator.getSentenceInstance(Locale.getDefault());
     BoundaryScanner scanner = new BreakIteratorBoundaryScanner(bi);
     
@@ -71,7 +71,7 @@
 
   public void testLineBoundary() throws Exception {
     StringBuilder text = new StringBuilder(TEXT);
-    // we test this with default locale, its randomized by LuceneTestCase
+    // we test this with default locale, it's randomized by LuceneTestCase
     BreakIterator bi = BreakIterator.getLineInstance(Locale.getDefault());
     BoundaryScanner scanner = new BreakIteratorBoundaryScanner(bi);
     
Index: lucene/misc/src/java/org/apache/lucene/index/SortingLeafReader.java
===================================================================
--- lucene/misc/src/java/org/apache/lucene/index/SortingLeafReader.java	(revision 1647726)
+++ lucene/misc/src/java/org/apache/lucene/index/SortingLeafReader.java	(working copy)
@@ -510,7 +510,7 @@
     
     /**
      * A {@link TimSorter} which sorts two parallel arrays of doc IDs and
-     * offsets in one go. Everytime a doc ID is 'swapped', its correponding offset
+     * offsets in one go. Everytime a doc ID is 'swapped', its corresponding offset
      * is swapped too.
      */
     private static final class DocOffsetSorter extends TimSorter {
Index: lucene/misc/src/java/org/apache/lucene/uninverting/DocTermOrds.java
===================================================================
--- lucene/misc/src/java/org/apache/lucene/uninverting/DocTermOrds.java	(revision 1647726)
+++ lucene/misc/src/java/org/apache/lucene/uninverting/DocTermOrds.java	(working copy)
@@ -96,7 +96,7 @@
  *
  *   There are actually 256 byte arrays, to compensate for the fact that the pointers
  *   into the byte arrays are only 3 bytes long.  The correct byte array for a document
- *   is a function of it's id.
+ *   is a function of its id.
  *
  *   To save space and speed up faceting, any term that matches enough documents will
  *   not be un-inverted... it will be skipped while building the un-inverted field structure,
@@ -105,7 +105,7 @@
  *   To further save memory, the terms (the actual string values) are not all stored in
  *   memory, but a TermIndex is used to convert term numbers to term values only
  *   for the terms needed after faceting has completed.  Only every 128th term value
- *   is stored, along with it's corresponding term number, and this is used as an
+ *   is stored, along with its corresponding term number, and this is used as an
  *   index to find the closest term and iterate until the desired number is hit (very
  *   much like Lucene's own internal term index).
  *
@@ -314,7 +314,7 @@
     //
     // During this intermediate form, every document has a (potential) byte[]
     // and the int[maxDoc()] array either contains the termNumber list directly
-    // or the *end* offset of the termNumber list in it's byte array (for faster
+    // or the *end* offset of the termNumber list in its byte array (for faster
     // appending and faster creation of the final form).
     //
     // idea... if things are too large while building, we could do a range of docs
Index: lucene/misc/src/java/org/apache/lucene/uninverting/FieldCacheImpl.java
===================================================================
--- lucene/misc/src/java/org/apache/lucene/uninverting/FieldCacheImpl.java	(revision 1647726)
+++ lucene/misc/src/java/org/apache/lucene/uninverting/FieldCacheImpl.java	(working copy)
@@ -911,8 +911,8 @@
       return DocValues.emptySortedSet();
     } else {
       // if #postings = #docswithfield we know that the field is "single valued enough".
-      // its possible the same term might appear twice in the same document, but SORTED_SET discards frequency.
-      // its still ok with filtering (which we limit to numerics), it just means precisionStep = Inf
+      // it's possible the same term might appear twice in the same document, but SORTED_SET discards frequency.
+      // it's still ok with filtering (which we limit to numerics), it just means precisionStep = Inf
       long numPostings = terms.getSumDocFreq();
       if (numPostings != -1 && numPostings == terms.getDocCount()) {
         return DocValues.singleton(getTermsIndex(reader, field));
Index: lucene/misc/src/java/org/apache/lucene/uninverting/FieldCacheSanityChecker.java
===================================================================
--- lucene/misc/src/java/org/apache/lucene/uninverting/FieldCacheSanityChecker.java	(revision 1647726)
+++ lucene/misc/src/java/org/apache/lucene/uninverting/FieldCacheSanityChecker.java	(working copy)
@@ -348,7 +348,7 @@
     public CacheEntry[] getCacheEntries() { return entries; }
     /**
      * Multi-Line representation of this Insanity object, starting with 
-     * the Type and Msg, followed by each CacheEntry.toString() on it's 
+     * the Type and Msg, followed by each CacheEntry.toString() on its 
      * own line prefaced by a tab character
      */
     @Override
Index: lucene/module-build.xml
===================================================================
--- lucene/module-build.xml	(revision 1647726)
+++ lucene/module-build.xml	(working copy)
@@ -68,7 +68,7 @@
   <macrodef name="invoke-module-javadoc">
     <!-- additional links for dependencies to other modules -->
       <element name="links" optional="yes"/>
-    <!-- link source (don't do this unless its example code) -->
+    <!-- link source (don't do this unless it's example code) -->
       <attribute name="linksource" default="no"/>
     <sequential>
       <mkdir dir="${javadoc.dir}/${name}"/>
Index: lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/MaxFloatFunction.java
===================================================================
--- lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/MaxFloatFunction.java	(revision 1647726)
+++ lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/MaxFloatFunction.java	(working copy)
@@ -21,7 +21,7 @@
 import org.apache.lucene.queries.function.ValueSource;
 
 /**
- * <code>MaxFloatFunction</code> returns the max of it's components.
+ * <code>MaxFloatFunction</code> returns the max of its components.
  */
 public class MaxFloatFunction extends MultiFloatFunction {
   public MaxFloatFunction(ValueSource[] sources) {
Index: lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/MinFloatFunction.java
===================================================================
--- lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/MinFloatFunction.java	(revision 1647726)
+++ lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/MinFloatFunction.java	(working copy)
@@ -21,7 +21,7 @@
 import org.apache.lucene.queries.function.ValueSource;
 
 /**
- * <code>MinFloatFunction</code> returns the min of it's components.
+ * <code>MinFloatFunction</code> returns the min of its components.
  */
 public class MinFloatFunction extends MultiFloatFunction {
   public MinFloatFunction(ValueSource[] sources) {
Index: lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/ProductFloatFunction.java
===================================================================
--- lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/ProductFloatFunction.java	(revision 1647726)
+++ lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/ProductFloatFunction.java	(working copy)
@@ -21,7 +21,7 @@
 import org.apache.lucene.queries.function.ValueSource;
 
 /**
- * <code>ProductFloatFunction</code> returns the product of it's components.
+ * <code>ProductFloatFunction</code> returns the product of its components.
  */
 public class ProductFloatFunction extends MultiFloatFunction {
   public ProductFloatFunction(ValueSource[] sources) {
Index: lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/SumFloatFunction.java
===================================================================
--- lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/SumFloatFunction.java	(revision 1647726)
+++ lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/SumFloatFunction.java	(working copy)
@@ -21,7 +21,7 @@
 import org.apache.lucene.queries.function.ValueSource;
 
 /**
- * <code>SumFloatFunction</code> returns the sum of it's components.
+ * <code>SumFloatFunction</code> returns the sum of its components.
  */
 public class SumFloatFunction extends MultiFloatFunction {
   public SumFloatFunction(ValueSource[] sources) {
Index: lucene/queries/src/test/org/apache/lucene/queries/CommonTermsQueryTest.java
===================================================================
--- lucene/queries/src/test/org/apache/lucene/queries/CommonTermsQueryTest.java	(revision 1647726)
+++ lucene/queries/src/test/org/apache/lucene/queries/CommonTermsQueryTest.java	(working copy)
@@ -531,7 +531,7 @@
   public static void createRandomIndex(int numdocs, RandomIndexWriter writer,
       long seed) throws IOException {
     Random random = new Random(seed);
-    // primary source for our data is from linefiledocs, its realistic.
+    // primary source for our data is from linefiledocs, it's realistic.
     LineFileDocs lineFileDocs = new LineFileDocs(random);
     
     // TODO: we should add other fields that use things like docs&freqs but omit
Index: lucene/queryparser/docs/xml/LuceneContribQuery.dtd.html
===================================================================
--- lucene/queryparser/docs/xml/LuceneContribQuery.dtd.html	(revision 1647726)
+++ lucene/queryparser/docs/xml/LuceneContribQuery.dtd.html	(working copy)
@@ -219,7 +219,7 @@
 <li> as a Clause in a BooleanQuery who's only other clause
 is a "mustNot" match (Lucene requires at least one positive clause) and..</li>
 <li> in a FilteredQuery where a Filter tag is effectively being
-used to select content rather than it's usual role of filtering the results of a query.</li>
+used to select content rather than its usual role of filtering the results of a query.</li>
 </ol></p><p><span class='inTextTitle'>Example:</span> <em>Effectively use a Filter as a query </em>
 </p><pre>	          
                &lt;FilteredQuery&gt;
Index: lucene/queryparser/docs/xml/LuceneCoreQuery.dtd.html
===================================================================
--- lucene/queryparser/docs/xml/LuceneCoreQuery.dtd.html	(revision 1647726)
+++ lucene/queryparser/docs/xml/LuceneCoreQuery.dtd.html	(working copy)
@@ -225,7 +225,7 @@
 <li> as a Clause in a BooleanQuery who's only other clause
 is a "mustNot" match (Lucene requires at least one positive clause) and..</li>
 <li> in a FilteredQuery where a Filter tag is effectively being
-used to select content rather than it's usual role of filtering the results of a query.</li>
+used to select content rather than its usual role of filtering the results of a query.</li>
 </ol></p><p><span class='inTextTitle'>Example:</span> <em>Effectively use a Filter as a query </em>
 </p><pre>	          
                &lt;FilteredQuery&gt;
Index: lucene/queryparser/docs/xml/LuceneCoreQuery.dtd.org.html
===================================================================
--- lucene/queryparser/docs/xml/LuceneCoreQuery.dtd.org.html	(revision 1647726)
+++ lucene/queryparser/docs/xml/LuceneCoreQuery.dtd.org.html	(working copy)
@@ -159,7 +159,7 @@
 <span class="dtd_comment">    &lt;li&gt; as a Clause in a BooleanQuery who's only other clause</span>
 <span class="dtd_comment">    is a &quot;mustNot&quot; match (Lucene requires at least one positive clause) and..&lt;/li&gt;</span>
 <span class="dtd_comment">    &lt;li&gt; in a FilteredQuery where a Filter tag is effectively being </span>
-<span class="dtd_comment">    used to select content rather than it's usual role of filtering the results of a query.&lt;/li&gt;</span>
+<span class="dtd_comment">    used to select content rather than its usual role of filtering the results of a query.&lt;/li&gt;</span>
 <span class="dtd_comment">    &lt;/ol&gt;</span>
 <span class="dtd_comment">    </span>
 <span class="dtd_comment">    </span><span class="dtd_dtddoc_tag">@example</span><span class="dtd_comment"> </span>
Index: lucene/queryparser/src/java/org/apache/lucene/queryparser/surround/query/SpanNearClauseFactory.java
===================================================================
--- lucene/queryparser/src/java/org/apache/lucene/queryparser/surround/query/SpanNearClauseFactory.java	(revision 1647726)
+++ lucene/queryparser/src/java/org/apache/lucene/queryparser/surround/query/SpanNearClauseFactory.java	(working copy)
@@ -46,7 +46,7 @@
    Are SpanQuery weights handled correctly during search by Lucene?
    Should the resulting SpanOrQuery be sorted?
    Could other SpanQueries be added for use in this factory:
-   - SpanOrQuery: in principle yes, but it only has access to it's terms
+   - SpanOrQuery: in principle yes, but it only has access to its terms
                   via getTerms(); are the corresponding weights available?
    - SpanFirstQuery: treat similar to subquery SpanNearQuery. (ok?)
    - SpanNotQuery: treat similar to subquery SpanNearQuery. (ok?)
Index: lucene/queryparser/src/resources/org/apache/lucene/queryparser/xml/LuceneCoreQuery.dtd
===================================================================
--- lucene/queryparser/src/resources/org/apache/lucene/queryparser/xml/LuceneCoreQuery.dtd	(revision 1647726)
+++ lucene/queryparser/src/resources/org/apache/lucene/queryparser/xml/LuceneCoreQuery.dtd	(working copy)
@@ -153,7 +153,7 @@
 	<li> as a Clause in a BooleanQuery who's only other clause
 	is a "mustNot" match (Lucene requires at least one positive clause) and..</li>
 	<li> in a FilteredQuery where a Filter tag is effectively being 
-	used to select content rather than it's usual role of filtering the results of a query.</li>
+	used to select content rather than its usual role of filtering the results of a query.</li>
 	</ol>
 	
 	@example 
Index: lucene/spatial/src/java/org/apache/lucene/spatial/bbox/BBoxOverlapRatioValueSource.java
===================================================================
--- lucene/spatial/src/java/org/apache/lucene/spatial/bbox/BBoxOverlapRatioValueSource.java	(revision 1647726)
+++ lucene/spatial/src/java/org/apache/lucene/spatial/bbox/BBoxOverlapRatioValueSource.java	(working copy)
@@ -51,7 +51,7 @@
  * Originally based on Geoportal's
  * <a href="http://geoportal.svn.sourceforge.net/svnroot/geoportal/Geoportal/trunk/src/com/esri/gpt/catalog/lucene/SpatialRankingValueSource.java">
  *   SpatialRankingValueSource</a> but modified quite a bit. GeoPortal's algorithm will yield a score of 0
- * if either a line or point is compared, and it's doesn't output a 0-1 normalized score (it multiplies the factors),
+ * if either a line or point is compared, and it doesn't output a 0-1 normalized score (it multiplies the factors),
  * and it doesn't support minSideLength, and it had dateline bugs.
  *
  * @lucene.experimental
Index: lucene/spatial/src/java/org/apache/lucene/spatial/prefix/CellTokenStream.java
===================================================================
--- lucene/spatial/src/java/org/apache/lucene/spatial/prefix/CellTokenStream.java	(revision 1647726)
+++ lucene/spatial/src/java/org/apache/lucene/spatial/prefix/CellTokenStream.java	(working copy)
@@ -147,7 +147,7 @@
     cellAtt.setOmitLeafByte(false);
   }
 
-  /** Outputs the token of a cell, and if its a leaf, outputs it again with the leaf byte. */
+  /** Outputs the token of a cell, and if it's a leaf, outputs it again with the leaf byte. */
   @Override
   public final boolean incrementToken() {
     if (iter == null)
Index: lucene/spatial/src/java/org/apache/lucene/spatial/prefix/tree/Cell.java
===================================================================
--- lucene/spatial/src/java/org/apache/lucene/spatial/prefix/tree/Cell.java	(revision 1647726)
+++ lucene/spatial/src/java/org/apache/lucene/spatial/prefix/tree/Cell.java	(working copy)
@@ -63,7 +63,7 @@
 
   /**
    * Returns the bytes for this cell, with a leaf byte if this is a leaf cell.
-   * The result param is used to save object allocation, though it's bytes aren't used.
+   * The result param is used to save object allocation, though its bytes aren't used.
    * @param result where the result goes, or null to create new
    */
   BytesRef getTokenBytesWithLeaf(BytesRef result);
@@ -71,7 +71,7 @@
   /**
    * Returns the bytes for this cell, without leaf set. The bytes should sort before
    * {@link #getTokenBytesWithLeaf(org.apache.lucene.util.BytesRef)}.
-   * The result param is used to save object allocation, though it's bytes aren't used.
+   * The result param is used to save object allocation, though its bytes aren't used.
    * @param result where the result goes, or null to create new
    */
   BytesRef getTokenBytesNoLeaf(BytesRef result);
Index: lucene/spatial/src/java/org/apache/lucene/spatial/prefix/tree/SpatialPrefixTreeFactory.java
===================================================================
--- lucene/spatial/src/java/org/apache/lucene/spatial/prefix/tree/SpatialPrefixTreeFactory.java	(revision 1647726)
+++ lucene/spatial/src/java/org/apache/lucene/spatial/prefix/tree/SpatialPrefixTreeFactory.java	(working copy)
@@ -41,7 +41,7 @@
 
   /**
    * The factory  is looked up via "prefixTree" in args, expecting "geohash" or "quad".
-   * If its neither of these, then "geohash" is chosen for a geo context, otherwise "quad" is chosen.
+   * If it's neither of these, then "geohash" is chosen for a geo context, otherwise "quad" is chosen.
    */
   public static SpatialPrefixTree makeSPT(Map<String,String> args, ClassLoader classLoader, SpatialContext ctx) {
     SpatialPrefixTreeFactory instance;
Index: lucene/spatial/src/java/org/apache/lucene/spatial/util/ShapePredicateValueSource.java
===================================================================
--- lucene/spatial/src/java/org/apache/lucene/spatial/util/ShapePredicateValueSource.java	(revision 1647726)
+++ lucene/spatial/src/java/org/apache/lucene/spatial/util/ShapePredicateValueSource.java	(working copy)
@@ -42,7 +42,7 @@
 
   /**
    *
-   * @param shapeValuesource Must yield {@link Shape} instances from it's objectVal(doc). If null
+   * @param shapeValuesource Must yield {@link Shape} instances from its objectVal(doc). If null
    *                         then the result is false. This is the left-hand (indexed) side.
    * @param op the predicate
    * @param queryShape The shape on the right-hand (query) side.
Index: lucene/spatial/src/test/org/apache/lucene/spatial/prefix/RandomSpatialOpFuzzyPrefixTreeTest.java
===================================================================
--- lucene/spatial/src/test/org/apache/lucene/spatial/prefix/RandomSpatialOpFuzzyPrefixTreeTest.java	(revision 1647726)
+++ lucene/spatial/src/test/org/apache/lucene/spatial/prefix/RandomSpatialOpFuzzyPrefixTreeTest.java	(working copy)
@@ -180,7 +180,7 @@
         new SpatialArgs(SpatialOperation.IsWithin, ctx.makeRectangle(38, 192, -72, 56))
     ), 1).numFound==0);//no-match
 
-    //this time the rect is a little bigger and is considered a match. It's a
+    //this time the rect is a little bigger and is considered a match. It's
     // an acceptable false-positive because of the grid approximation.
     assertTrue(executeQuery(strategy.makeQuery(
         new SpatialArgs(SpatialOperation.IsWithin, ctx.makeRectangle(38, 192, -72, 80))
@@ -462,7 +462,7 @@
         return r;
       //test all 4 corners
       // Note: awkwardly, we use a non-geo context for this because in geo, -180 & +180 are the same place, which means
-      //  that "other" might wrap the world horizontally and yet all it's corners could be in shape1 (or shape2) even
+      //  that "other" might wrap the world horizontally and yet all its corners could be in shape1 (or shape2) even
       //  though shape1 is only adjacent to the dateline. I couldn't think of a better way to handle this.
       Rectangle oRect = (Rectangle)other;
       if (cornerContainsNonGeo(oRect.getMinX(), oRect.getMinY())
Index: lucene/suggest/src/java/org/apache/lucene/search/spell/DirectSpellChecker.java
===================================================================
--- lucene/suggest/src/java/org/apache/lucene/search/spell/DirectSpellChecker.java	(revision 1647726)
+++ lucene/suggest/src/java/org/apache/lucene/search/spell/DirectSpellChecker.java	(working copy)
@@ -236,7 +236,7 @@
    * True if the spellchecker should lowercase terms (default: true)
    * <p>
    * This is a convenience method, if your index field has more complicated
-   * analysis (such as StandardTokenizer removing punctuation), its probably
+   * analysis (such as StandardTokenizer removing punctuation), it's probably
    * better to turn this off, and instead run your query terms through your
    * Analyzer first.
    * <p>
Index: lucene/suggest/src/java/org/apache/lucene/search/spell/LuceneLevenshteinDistance.java
===================================================================
--- lucene/suggest/src/java/org/apache/lucene/search/spell/LuceneLevenshteinDistance.java	(revision 1647726)
+++ lucene/suggest/src/java/org/apache/lucene/search/spell/LuceneLevenshteinDistance.java	(working copy)
@@ -55,8 +55,8 @@
     // NOTE: if we cared, we could 3*m space instead of m*n space, similar to 
     // what LevenshteinDistance does, except cycling thru a ring of three 
     // horizontal cost arrays... but this comparator is never actually used by 
-    // DirectSpellChecker, its only used for merging results from multiple shards 
-    // in "distributed spellcheck", and its inefficient in other ways too...
+    // DirectSpellChecker, it's only used for merging results from multiple shards 
+    // in "distributed spellcheck", and it's inefficient in other ways too...
 
     // cheaper to do this up front once
     targetPoints = toIntsRef(target);
Index: lucene/suggest/src/java/org/apache/lucene/search/spell/SpellChecker.java
===================================================================
--- lucene/suggest/src/java/org/apache/lucene/search/spell/SpellChecker.java	(revision 1647726)
+++ lucene/suggest/src/java/org/apache/lucene/search/spell/SpellChecker.java	(working copy)
@@ -568,7 +568,7 @@
 
   private static Document createDocument(String text, int ng1, int ng2) {
     Document doc = new Document();
-    // the word field is never queried on... its indexed so it can be quickly
+    // the word field is never queried on... it's indexed so it can be quickly
     // checked for rebuild (and stored for retrieval). Doesn't need norms or TF/pos
     Field f = new StringField(F_WORD, text, Field.Store.YES);
     doc.add(f); // orig term
Index: lucene/suggest/src/java/org/apache/lucene/search/suggest/DocumentDictionary.java
===================================================================
--- lucene/suggest/src/java/org/apache/lucene/search/suggest/DocumentDictionary.java	(revision 1647726)
+++ lucene/suggest/src/java/org/apache/lucene/search/suggest/DocumentDictionary.java	(working copy)
@@ -235,8 +235,8 @@
     
     /** 
      * Returns the value of the <code>weightField</code> for the current document.
-     * Retrieves the value for the <code>weightField</code> if its stored (using <code>doc</code>)
-     * or if its indexed as {@link NumericDocValues} (using <code>docId</code>) for the document.
+     * Retrieves the value for the <code>weightField</code> if it's stored (using <code>doc</code>)
+     * or if it's indexed as {@link NumericDocValues} (using <code>docId</code>) for the document.
      * If no value is found, then the weight is 0.
      */
     protected long getWeight(StoredDocument doc, int docId) {
Index: lucene/suggest/src/test/org/apache/lucene/search/suggest/analyzing/AnalyzingSuggesterTest.java
===================================================================
--- lucene/suggest/src/test/org/apache/lucene/search/suggest/analyzing/AnalyzingSuggesterTest.java	(revision 1647726)
+++ lucene/suggest/src/test/org/apache/lucene/search/suggest/analyzing/AnalyzingSuggesterTest.java	(working copy)
@@ -221,13 +221,13 @@
     assertEquals("the ghost of christmas past", results.get(0).key.toString());
     assertEquals(50, results.get(0).value, 0.01F);
 
-    // omit the 'the' since its a stopword, its suggested anyway
+    // omit the 'the' since it's a stopword, it's suggested anyway
     results = suggester.lookup(TestUtil.stringToCharSequence("ghost of chris", random()), false, 1);
     assertEquals(1, results.size());
     assertEquals("the ghost of christmas past", results.get(0).key.toString());
     assertEquals(50, results.get(0).value, 0.01F);
 
-    // omit the 'the' and 'of' since they are stopwords, its suggested anyway
+    // omit the 'the' and 'of' since they are stopwords, it's suggested anyway
     results = suggester.lookup(TestUtil.stringToCharSequence("ghost chris", random()), false, 1);
     assertEquals(1, results.size());
     assertEquals("the ghost of christmas past", results.get(0).key.toString());
@@ -817,7 +817,7 @@
         System.out.println("  analyzed: " + analyzedKey);
       }
 
-      // TODO: could be faster... but its slowCompletor for a reason
+      // TODO: could be faster... but it's slowCompletor for a reason
       for (TermFreq2 e : slowCompletor) {
         if (e.analyzedForm.startsWith(analyzedKey)) {
           matches.add(e);
Index: lucene/suggest/src/test/org/apache/lucene/search/suggest/analyzing/FuzzySuggesterTest.java
===================================================================
--- lucene/suggest/src/test/org/apache/lucene/search/suggest/analyzing/FuzzySuggesterTest.java	(revision 1647726)
+++ lucene/suggest/src/test/org/apache/lucene/search/suggest/analyzing/FuzzySuggesterTest.java	(working copy)
@@ -186,13 +186,13 @@
     assertEquals("the ghost of christmas past", results.get(0).key.toString());
     assertEquals(50, results.get(0).value, 0.01F);
 
-    // omit the 'the' since its a stopword, its suggested anyway
+    // omit the 'the' since it's a stopword, it's suggested anyway
     results = suggester.lookup(TestUtil.stringToCharSequence("ghost of chris", random()), false, 1);
     assertEquals(1, results.size());
     assertEquals("the ghost of christmas past", results.get(0).key.toString());
     assertEquals(50, results.get(0).value, 0.01F);
 
-    // omit the 'the' and 'of' since they are stopwords, its suggested anyway
+    // omit the 'the' and 'of' since they are stopwords, it's suggested anyway
     results = suggester.lookup(TestUtil.stringToCharSequence("ghost chris", random()), false, 1);
     assertEquals(1, results.size());
     assertEquals("the ghost of christmas past", results.get(0).key.toString());
@@ -755,7 +755,7 @@
       Automaton automaton = suggester.convertAutomaton(suggester.toLevenshteinAutomata(suggester.toLookupAutomaton(analyzedKey)));
       assertTrue(automaton.isDeterministic());
 
-      // TODO: could be faster... but its slowCompletor for a reason
+      // TODO: could be faster... but it's slowCompletor for a reason
       BytesRefBuilder spare = new BytesRefBuilder();
       for (TermFreqPayload2 e : slowCompletor) {
         spare.copyChars(e.analyzedForm);
@@ -1114,8 +1114,8 @@
     // NOTE: if we cared, we could 3*m space instead of m*n space, similar to 
     // what LevenshteinDistance does, except cycling thru a ring of three 
     // horizontal cost arrays... but this comparator is never actually used by 
-    // DirectSpellChecker, its only used for merging results from multiple shards 
-    // in "distributed spellcheck", and its inefficient in other ways too...
+    // DirectSpellChecker, it's only used for merging results from multiple shards 
+    // in "distributed spellcheck", and it's inefficient in other ways too...
 
     // cheaper to do this up front once
     targetPoints = toIntsRef(target);
Index: lucene/suggest/src/test/org/apache/lucene/search/suggest/fst/WFSTCompletionTest.java
===================================================================
--- lucene/suggest/src/test/org/apache/lucene/search/suggest/fst/WFSTCompletionTest.java	(revision 1647726)
+++ lucene/suggest/src/test/org/apache/lucene/search/suggest/fst/WFSTCompletionTest.java	(working copy)
@@ -162,10 +162,10 @@
       final int topN = TestUtil.nextInt(random, 1, 10);
       List<LookupResult> r = suggester.lookup(TestUtil.stringToCharSequence(prefix, random), false, topN);
 
-      // 2. go thru whole treemap (slowCompletor) and check its actually the best suggestion
+      // 2. go thru whole treemap (slowCompletor) and check it's actually the best suggestion
       final List<LookupResult> matches = new ArrayList<>();
 
-      // TODO: could be faster... but its slowCompletor for a reason
+      // TODO: could be faster... but it's slowCompletor for a reason
       for (Map.Entry<String,Long> e : slowCompletor.entrySet()) {
         if (e.getKey().startsWith(prefix)) {
           matches.add(new LookupResult(e.getKey(), e.getValue().longValue()));
Index: lucene/test-framework/build.xml
===================================================================
--- lucene/test-framework/build.xml	(revision 1647726)
+++ lucene/test-framework/build.xml	(working copy)
@@ -46,7 +46,7 @@
   <!-- redefine the clover setup, because we dont want to run clover for the test-framework -->
   <target name="-clover.setup" if="run.clover"/>
 
-  <!-- redefine the test compilation, so its just a no-op -->
+  <!-- redefine the test compilation, so it's just a no-op -->
   <target name="compile-test"/>
   
   <!-- redefine the forbidden apis for tests, as we check ourselves - no sysout testing -->
Index: lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase.java
===================================================================
--- lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase.java	(revision 1647726)
+++ lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase.java	(working copy)
@@ -49,7 +49,7 @@
 /** 
  * Base class for all Lucene unit tests that use TokenStreams. 
  * <p>
- * When writing unit tests for analysis components, its highly recommended
+ * When writing unit tests for analysis components, it's highly recommended
  * to use the helper methods here (especially in conjunction with {@link MockAnalyzer} or
  * {@link MockTokenizer}), as they contain many assertions and checks to 
  * catch bugs.
@@ -508,7 +508,7 @@
     try {
       checkRandomData(new Random(seed), a, iterations, maxWordLength, useCharFilter, simple, offsetsAreCorrect, iw);
       // now test with multiple threads: note we do the EXACT same thing we did before in each thread,
-      // so this should only really fail from another thread if its an actual thread problem
+      // so this should only really fail from another thread if it's an actual thread problem
       int numThreads = TestUtil.nextInt(random, 2, 4);
       final CountDownLatch startingGun = new CountDownLatch(1);
       AnalysisThread threads[] = new AnalysisThread[numThreads];
Index: lucene/test-framework/src/java/org/apache/lucene/analysis/MockAnalyzer.java
===================================================================
--- lucene/test-framework/src/java/org/apache/lucene/analysis/MockAnalyzer.java	(revision 1647726)
+++ lucene/test-framework/src/java/org/apache/lucene/analysis/MockAnalyzer.java	(working copy)
@@ -30,7 +30,7 @@
  * <p>
  * This analyzer is a replacement for Whitespace/Simple/KeywordAnalyzers
  * for unit tests. If you are testing a custom component such as a queryparser
- * or analyzer-wrapper that consumes analysis streams, its a great idea to test
+ * or analyzer-wrapper that consumes analysis streams, it's a great idea to test
  * it with this analyzer instead. MockAnalyzer has the following behavior:
  * <ul>
  *   <li>By default, the assertions in {@link MockTokenizer} are turned on for extra
Index: lucene/test-framework/src/java/org/apache/lucene/analysis/MockTokenizer.java
===================================================================
--- lucene/test-framework/src/java/org/apache/lucene/analysis/MockTokenizer.java	(revision 1647726)
+++ lucene/test-framework/src/java/org/apache/lucene/analysis/MockTokenizer.java	(working copy)
@@ -33,7 +33,7 @@
  * Tokenizer for testing.
  * <p>
  * This tokenizer is a replacement for {@link #WHITESPACE}, {@link #SIMPLE}, and {@link #KEYWORD}
- * tokenizers. If you are writing a component such as a TokenFilter, its a great idea to test
+ * tokenizers. If you are writing a component such as a TokenFilter, it's a great idea to test
  * it wrapping this tokenizer instead for extra checks. This tokenizer has the following behavior:
  * <ul>
  *   <li>An internal state-machine is used for checking consumer consistency. These checks can
@@ -66,7 +66,7 @@
   int off = 0;
   
   // buffered state (previous codepoint and offset). we replay this once we
-  // hit a reject state in case its permissible as the start of a new term.
+  // hit a reject state in case it's permissible as the start of a new term.
   int bufferedCodePoint = -1; // -1 indicates empty buffer
   int bufferedOff = -1;
 
@@ -169,7 +169,7 @@
           bufferedCodePoint = cp;
           bufferedOff = endOffset;
         } else {
-          // otherwise, its because we hit term limit.
+          // otherwise, it's because we hit term limit.
           bufferedCodePoint = -1;
         }
         int correctedStartOffset = correctOffset(startOffset);
Index: lucene/test-framework/src/java/org/apache/lucene/analysis/package.html
===================================================================
--- lucene/test-framework/src/java/org/apache/lucene/analysis/package.html	(revision 1647726)
+++ lucene/test-framework/src/java/org/apache/lucene/analysis/package.html	(working copy)
@@ -30,11 +30,11 @@
        as it contains many assertions and checks to catch bugs. </li>
    <li>{@link org.apache.lucene.analysis.MockTokenizer}: Tokenizer for testing.
        Tokenizer that serves as a replacement for WHITESPACE, SIMPLE, and KEYWORD
-       tokenizers. If you are writing a component such as a TokenFilter, its a great idea to test
+       tokenizers. If you are writing a component such as a TokenFilter, it's a great idea to test
        it wrapping this tokenizer instead for extra checks. </li>
    <li>{@link org.apache.lucene.analysis.MockAnalyzer}: Analyzer for testing.
        Analyzer that uses MockTokenizer for additional verification. If you are testing a custom 
-       component such as a queryparser or analyzer-wrapper that consumes analysis streams, its a great 
+       component such as a queryparser or analyzer-wrapper that consumes analysis streams, it's a great 
        idea to test it with this analyzer instead. </li>
 </ul>
 </p>
Index: lucene/test-framework/src/java/org/apache/lucene/codecs/compressing/HighCompressionCompressingCodec.java
===================================================================
--- lucene/test-framework/src/java/org/apache/lucene/codecs/compressing/HighCompressionCompressingCodec.java	(revision 1647726)
+++ lucene/test-framework/src/java/org/apache/lucene/codecs/compressing/HighCompressionCompressingCodec.java	(working copy)
@@ -29,7 +29,7 @@
 
   /** Default constructor. */
   public HighCompressionCompressingCodec() {
-    // we don't worry about zlib block overhead as its
+    // we don't worry about zlib block overhead as it's
     // not bad and try to save space instead:
     this(61440, 512, false);
   }
Index: lucene/test-framework/src/java/org/apache/lucene/codecs/mockrandom/MockRandomPostingsFormat.java
===================================================================
--- lucene/test-framework/src/java/org/apache/lucene/codecs/mockrandom/MockRandomPostingsFormat.java	(revision 1647726)
+++ lucene/test-framework/src/java/org/apache/lucene/codecs/mockrandom/MockRandomPostingsFormat.java	(working copy)
@@ -91,7 +91,7 @@
       minSkipInterval = 2;
     }
 
-    // we pull this before the seed intentionally: because its not consumed at runtime
+    // we pull this before the seed intentionally: because it's not consumed at runtime
     // (the skipInterval is written into postings header).
     // NOTE: Currently not passed to postings writer.
     //       before, it was being passed in wrongly as acceptableOverhead!
Index: lucene/test-framework/src/java/org/apache/lucene/index/AlcoholicMergePolicy.java
===================================================================
--- lucene/test-framework/src/java/org/apache/lucene/index/AlcoholicMergePolicy.java	(revision 1647726)
+++ lucene/test-framework/src/java/org/apache/lucene/index/AlcoholicMergePolicy.java	(working copy)
@@ -58,7 +58,7 @@
     int hourOfDay = calendar.get(Calendar.HOUR_OF_DAY);
     if (hourOfDay < 6 || 
         hourOfDay > 20 || 
-        // its 5 o'clock somewhere
+        // it's 5 o'clock somewhere
         random.nextInt(23) == 5) {
       
       Drink[] values = Drink.values();
Index: lucene/test-framework/src/java/org/apache/lucene/index/RandomCodec.java
===================================================================
--- lucene/test-framework/src/java/org/apache/lucene/index/RandomCodec.java	(revision 1647726)
+++ lucene/test-framework/src/java/org/apache/lucene/index/RandomCodec.java	(working copy)
@@ -74,7 +74,7 @@
   public final Set<String> avoidCodecs;
 
   /** memorized field to postingsformat mappings */
-  // note: we have to sync this map even though its just for debugging/toString, 
+  // note: we have to sync this map even though it's just for debugging/toString, 
   // otherwise DWPT's .toString() calls that iterate over the map can 
   // cause concurrentmodificationexception if indexwriter's infostream is on
   private Map<String,PostingsFormat> previousMappings = Collections.synchronizedMap(new HashMap<String,PostingsFormat>());
Index: lucene/test-framework/src/java/org/apache/lucene/mockfile/LeakFS.java
===================================================================
--- lucene/test-framework/src/java/org/apache/lucene/mockfile/LeakFS.java	(revision 1647726)
+++ lucene/test-framework/src/java/org/apache/lucene/mockfile/LeakFS.java	(working copy)
@@ -55,7 +55,7 @@
   @Override
   public synchronized void onClose() {
     if (!openHandles.isEmpty()) {
-      // print the first one as its very verbose otherwise
+      // print the first one as it's very verbose otherwise
       Exception cause = null;
       Iterator<Exception> stacktraces = openHandles.values().iterator();
       if (stacktraces.hasNext()) {
Index: lucene/test-framework/src/java/org/apache/lucene/mockfile/WindowsFS.java
===================================================================
--- lucene/test-framework/src/java/org/apache/lucene/mockfile/WindowsFS.java	(revision 1647726)
+++ lucene/test-framework/src/java/org/apache/lucene/mockfile/WindowsFS.java	(working copy)
@@ -89,7 +89,7 @@
   }
   
   /** 
-   * Checks that its ok to delete {@code Path}. If the file
+   * Checks that it's ok to delete {@code Path}. If the file
    * is still open, it throws IOException("access denied").
    */
   private void checkDeleteAccess(Path path) throws IOException {
Index: lucene/test-framework/src/java/org/apache/lucene/search/CheckHits.java
===================================================================
--- lucene/test-framework/src/java/org/apache/lucene/search/CheckHits.java	(revision 1647726)
+++ lucene/test-framework/src/java/org/apache/lucene/search/CheckHits.java	(working copy)
@@ -342,13 +342,13 @@
     if (!deep) return;
 
     Explanation detail[] = expl.getDetails();
-    // TODO: can we improve this entire method? its really geared to work only with TF/IDF
+    // TODO: can we improve this entire method? it's really geared to work only with TF/IDF
     if (expl.getDescription().endsWith("computed from:")) {
       return; // something more complicated.
     }
     if (detail!=null) {
       if (detail.length==1) {
-        // simple containment, unless its a freq of: (which lets a query explain how the freq is calculated), 
+        // simple containment, unless it's a freq of: (which lets a query explain how the freq is calculated), 
         // just verify contained expl has same score
         if (!expl.getDescription().endsWith("with freq of:"))
           verifyExplanation(q,doc,score,deep,detail[0]);
Index: lucene/test-framework/src/java/org/apache/lucene/store/BaseDirectoryTestCase.java
===================================================================
--- lucene/test-framework/src/java/org/apache/lucene/store/BaseDirectoryTestCase.java	(revision 1647726)
+++ lucene/test-framework/src/java/org/apache/lucene/store/BaseDirectoryTestCase.java	(working copy)
@@ -735,7 +735,7 @@
     
     // this test backdoors the directory via the filesystem. so it must be an FSDir (for now)
     // TODO: figure a way to test this better/clean it up. E.g. we should be testing for FileSwitchDir,
-    // if its using two FSdirs and so on
+    // if it's using two FSdirs and so on
     if (fsdir instanceof FSDirectory == false) {
       fsdir.close();
       assumeTrue("test only works for FSDirectory subclasses", false);
Index: lucene/test-framework/src/java/org/apache/lucene/store/MockDirectoryWrapper.java
===================================================================
--- lucene/test-framework/src/java/org/apache/lucene/store/MockDirectoryWrapper.java	(revision 1647726)
+++ lucene/test-framework/src/java/org/apache/lucene/store/MockDirectoryWrapper.java	(working copy)
@@ -269,7 +269,7 @@
       success = true;
     } finally {
       if (success) {
-        // we don't do this stuff with lucene's commit, but its just for completeness
+        // we don't do this stuff with lucene's commit, but it's just for completeness
         if (unSyncedFiles.contains(source)) {
           unSyncedFiles.remove(source);
           unSyncedFiles.add(dest);
@@ -751,7 +751,7 @@
         openFilesDeleted = new HashSet<>();
       }
       if (openFiles.size() > 0) {
-        // print the first one as its very verbose otherwise
+        // print the first one as it's very verbose otherwise
         Exception cause = null;
         Iterator<Exception> stacktraces = openFileHandles.values().iterator();
         if (stacktraces.hasNext()) {
@@ -806,7 +806,7 @@
               }
             }
             
-            // its possible we cannot delete the segments_N on windows if someone has it open and
+            // it's possible we cannot delete the segments_N on windows if someone has it open and
             // maybe other files too, depending on timing. normally someone on windows wouldnt have
             // an issue (IFD would nuke this stuff eventually), but we pass NoDeletionPolicy...
             for (String file : pendingDeletions) {
Index: lucene/test-framework/src/java/org/apache/lucene/util/LineFileDocs.java
===================================================================
--- lucene/test-framework/src/java/org/apache/lucene/util/LineFileDocs.java	(revision 1647726)
+++ lucene/test-framework/src/java/org/apache/lucene/util/LineFileDocs.java	(working copy)
@@ -90,7 +90,7 @@
     boolean needSkip = true;
     long size = 0L, seekTo = 0L;
     if (is == null) {
-      // if its not in classpath, we load it as absolute filesystem path (e.g. Hudson's home dir)
+      // if it's not in classpath, we load it as absolute filesystem path (e.g. Hudson's home dir)
       Path file = Paths.get(path);
       size = Files.size(file);
       if (path.endsWith(".gz")) {
Index: lucene/test-framework/src/java/org/apache/lucene/util/TestRuleSetupAndRestoreClassEnv.java
===================================================================
--- lucene/test-framework/src/java/org/apache/lucene/util/TestRuleSetupAndRestoreClassEnv.java	(revision 1647726)
+++ lucene/test-framework/src/java/org/apache/lucene/util/TestRuleSetupAndRestoreClassEnv.java	(working copy)
@@ -113,7 +113,7 @@
 
   @Override
   protected void before() throws Exception {
-    // enable this by default, for IDE consistency with ant tests (as its the default from ant)
+    // enable this by default, for IDE consistency with ant tests (as it's the default from ant)
     // TODO: really should be in solr base classes, but some extend LTC directly.
     // we do this in beforeClass, because some tests currently disable it
     restoreProperties.put("solr.directoryFactory", System.getProperty("solr.directoryFactory"));
Index: lucene/test-framework/src/java/org/apache/lucene/util/TestUtil.java
===================================================================
--- lucene/test-framework/src/java/org/apache/lucene/util/TestUtil.java	(revision 1647726)
+++ lucene/test-framework/src/java/org/apache/lucene/util/TestUtil.java	(working copy)
@@ -847,7 +847,7 @@
     }
     MergeScheduler ms = w.getConfig().getMergeScheduler();
     if (ms instanceof ConcurrentMergeScheduler) {
-      // wtf... shouldnt it be even lower since its 1 by default?!?!
+      // wtf... shouldnt it be even lower since it's 1 by default?!?!
       ((ConcurrentMergeScheduler) ms).setMaxMergesAndThreads(3, 2);
     }
   }
Index: lucene/tools/build.xml
===================================================================
--- lucene/tools/build.xml	(revision 1647726)
+++ lucene/tools/build.xml	(working copy)
@@ -33,7 +33,7 @@
 
   <path id="test.classpath"/>
 
-  <!-- redefine the test compilation, so its just a no-op -->
+  <!-- redefine the test compilation, so it's just a no-op -->
   <target name="compile-test"/>
   
   <!-- redefine the forbidden apis to be no-ops -->
Index: lucene/tools/custom-tasks.xml
===================================================================
--- lucene/tools/custom-tasks.xml	(revision 1647726)
+++ lucene/tools/custom-tasks.xml	(working copy)
@@ -47,7 +47,7 @@
     
     <replaceregex pattern="[-]tests$" replace="-tests" flags="gi" />
 
-    <!-- git hashcode pattern: its always 40 chars right? -->
+    <!-- git hashcode pattern: it's always 40 chars right? -->
     <replaceregex pattern="\-[a-z0-9]{40,40}$" replace="" flags="gi" />
   </filtermapper>
 
Index: solr/CHANGES.txt
===================================================================
--- solr/CHANGES.txt	(revision 1647726)
+++ solr/CHANGES.txt	(working copy)
@@ -916,9 +916,9 @@
 * SOLR-6393: TransactionLog replay performance on HDFS is very poor. (Mark Miller)  
 
 * SOLR-6268: HdfsUpdateLog has a race condition that can expose a closed HDFS FileSystem instance and should 
-  close it's FileSystem instance if either inherited close method is called. (Mark Miller)
+  close its FileSystem instance if either inherited close method is called. (Mark Miller)
 
-* SOLR-6089: When using the HDFS block cache, when a file is deleted, it's underlying data entries in the 
+* SOLR-6089: When using the HDFS block cache, when a file is deleted, its underlying data entries in the 
   block cache are not removed, which is a problem with the global block cache option. 
   (Mark Miller, Patrick Hunt)
 
@@ -994,7 +994,7 @@
 
 * SOLR-6270: Increased timeouts for MultiThreadedOCPTest. (shalin)
 
-* SOLR-6274: UpdateShardHandler should log the params used to configure it's
+* SOLR-6274: UpdateShardHandler should log the params used to configure its
   HttpClient. (Ramkumar Aiyengar via Mark Miller)
 
 * SOLR-6194: Opened up "public" access to DataSource, DocBuilder, and EntityProcessorWrapper
@@ -1100,7 +1100,7 @@
   (Timothy Potter) 
 
 * SOLR-6002: Fix a couple of ugly issues around SolrIndexWriter close and 
-  rollback as well as how SolrIndexWriter manages it's ref counted directory
+  rollback as well as how SolrIndexWriter manages its ref counted directory
   instance. (Mark Miller, Gregory Chanan)
 
 * SOLR-6015: Better way to handle managed synonyms when ignoreCase=true
@@ -1529,7 +1529,7 @@
 * SOLR-5647: The lib paths in example-schemaless will now load correctly.
   (Paul Westin via Shawn Heisey)
 
-* SOLR-5770: All attempts to match a SolrCore with it's state in clusterstate.json
+* SOLR-5770: All attempts to match a SolrCore with its state in clusterstate.json
   should be done with the CoreNodeName. (Steve Davids via Mark Miller)
 
 * SOLR-5875: QueryComponent.mergeIds() unmarshals all docs' sort field values once
@@ -1590,7 +1590,7 @@
   problem if you hit a bad work item. (Mark Miller)
 
 * SOLR-5796: Increase how long we are willing to wait for a core to see the ZK
-  advertised leader in it's local state. (Timothy Potter, Mark Miller)  
+  advertised leader in its local state. (Timothy Potter, Mark Miller)  
 
 * SOLR-5834: Overseer threads are only being interrupted and not closed.
   (hossman, Mark Miller)
@@ -1618,7 +1618,7 @@
 ---------------------
 
 * SOLR-5796: Make how long we are willing to wait for a core to see the ZK
-  advertised leader in it's local state configurable. 
+  advertised leader in its local state configurable. 
   (Timothy Potter via Mark Miller)
 
 ==================  4.7.0 ==================
@@ -1758,8 +1758,8 @@
 
 * SOLR-4612: Admin UI - Analysis Screen contains empty table-columns (steffkes)
 
-* SOLR-5451: SyncStrategy closes it's http connection manager before the
-  executor that uses it in it's close method. (Mark Miller)
+* SOLR-5451: SyncStrategy closes its http connection manager before the
+  executor that uses it in its close method. (Mark Miller)
 
 * SOLR-5460: SolrDispatchFilter#sendError can get a SolrCore that it does not 
   close. (Mark Miller)
@@ -1767,7 +1767,7 @@
 * SOLR-5461: Request proxying should only set con.setDoOutput(true) if the
   request is a post. (Mark Miller)
 
-* SOLR-5481: SolrCmdDistributor should not let the http client do it's own 
+* SOLR-5481: SolrCmdDistributor should not let the http client do its own 
   retries. (Mark Miller)
 
 * LUCENE-5347: Fixed Solr's Zookeeper Client to copy files to Zookeeper using
@@ -1848,7 +1848,7 @@
   of multiValued string fields. (Andreas Hubold, Vitaliy Zhovtyuk via shalin)
 
 * SOLR-5593: Replicas should accept the last updates from a leader that has just 
-  lost it's connection to ZooKeeper. (Christine Poerschke via Mark Miller)
+  lost its connection to ZooKeeper. (Christine Poerschke via Mark Miller)
 
 * SOLR-5678: SolrZkClient should throw a SolrException when connect times out
   rather than a RuntimeException. (Karl Wright, Anshum Gupta, Mark Miller)
@@ -2460,7 +2460,7 @@
   where items are preserved across commits.  (Robert Muir)
 
 * SOLR-4249: UniqFieldsUpdateProcessorFactory now extends 
-  FieldMutatingUpdateProcessorFactory and supports all of it's selector options. Use
+  FieldMutatingUpdateProcessorFactory and supports all of its selector options. Use
   of the "fields" init param is now deprecated in favor of "fieldName" (hossman)
   
 * SOLR-2548: Allow multiple threads to be specified for faceting. When threading, one
@@ -3277,7 +3277,7 @@
   fullpath not path. (Mark Miller)
 
 * SOLR-4555: When forceNew is used with CachingDirectoryFactory#get, the old
-  CachValue should give up it's path as it will be used by a new Directory
+  CachValue should give up its path as it will be used by a new Directory
   instance. (Mark Miller)
 
 * SOLR-4578: CoreAdminHandler#handleCreateAction gets a SolrCore and does not
@@ -3298,7 +3298,7 @@
   working correctly. (Mark Miller)
 
 * SOLR-4570: Even if an explicit shard id is used, ZkController#preRegister 
-  should still wait to see the shard id in it's current ClusterState.
+  should still wait to see the shard id in its current ClusterState.
   (Mark Miller)
 
 * SOLR-4585: The Collections API validates numShards with < 0 but should use 
@@ -3326,7 +3326,7 @@
   Directory has a refCnt of 0, but it should call closeDirectory(CacheValue).
   (Mark Miller)
 
-* SOLR-4602: ZkController#unregister should cancel it's election participation 
+* SOLR-4602: ZkController#unregister should cancel its election participation 
   before asking the Overseer to delete the SolrCore information. (Mark Miller)
 
 * SOLR-4601: A Collection that is only partially created and then deleted will 
@@ -3355,7 +3355,7 @@
   when used in field:value queries in the lucene QParser.  (hossman, yonik)
 
 * SOLR-4617: SolrCore#reload needs to pass the deletion policy to the next 
-  SolrCore through it's constructor rather than setting a field after.
+  SolrCore through its constructor rather than setting a field after.
   (Mark Miller)
     
 * SOLR-4589: Fixed CPU spikes and poor performance in lazy field loading 
@@ -3810,7 +3810,7 @@
 * SOLR-4271: Add support for PostingsHighlighter.  (Robert Muir)
 
 * SOLR-4255: The new Solr 4 spatial fields now have a 'filter' boolean local-param
-  that can be set to false to not filter. Its useful when there is already a spatial
+  that can be set to false to not filter. It's useful when there is already a spatial
   filter query but you also need to sort or boost by distance. (David Smiley)
 
 * SOLR-4265, SOLR-4283: Solr now parses request parameters (in URL or sent with POST
@@ -4020,7 +4020,7 @@
 * SOLR-3959: Ensure the internal comma separator of poly fields is escaped
   for CSVResponseWriter.  (Areek Zillur via Robert Muir)
   
-* SOLR-4075: A logical shard that has had all of it's SolrCores unloaded should 
+* SOLR-4075: A logical shard that has had all of its SolrCores unloaded should 
   be removed from the cluster state. (Mark Miller, Gilles Comeau)
   
 * SOLR-4034: Check if a collection already exists before trying to create a
@@ -4030,7 +4030,7 @@
   (Mark Miller)
   
 * SOLR-4099: Allow the collection api work queue to make forward progress even
-  when it's watcher is not fired for some reason. (Raintung Li via Mark Miller)
+  when its watcher is not fired for some reason. (Raintung Li via Mark Miller)
 
 * SOLR-3960: Fixed a bug where Distributed Grouping ignored PostFilters
   (Nathan Visagan, hossman)
@@ -4046,7 +4046,7 @@
   options from being respected in some <fieldType/> declarations (hossman)
 
 * SOLR-4159: When we are starting a shard from rest, a potential leader should 
-  not consider it's last published state when deciding if it can be the new 
+  not consider its last published state when deciding if it can be the new 
   leader. (Mark Miller)
 
 * SOLR-4158: When a core is registering in ZooKeeper it may not wait long 
@@ -4078,7 +4078,7 @@
   (steffkes via hossman)
 
 * SOLR-4178: ReplicationHandler should abort any current pulls and wait for 
-  it's executor to stop during core close. (Mark Miller)
+  its executor to stop during core close. (Mark Miller)
 
 * SOLR-3918: Fixed the 'dist-war-excl-slf4j' ant target to exclude all
   slf4j jars, so that the resulting war is usable as is provided the servlet 
@@ -4278,7 +4278,7 @@
 In order to better support distributed search mode, the TermVectorComponent's
 response format has been changed so that if the schema defines a 
 uniqueKeyField, then that field value is used as the "key" for each document in
-it's response section, instead of the internal lucene doc id.  Users w/o a 
+its response section, instead of the internal lucene doc id.  Users w/o a 
 uniqueKeyField will continue to see the same response format.  See SOLR-3229
 for more details.
 
@@ -4572,7 +4572,7 @@
 
 * SOLR-3783: Fixed Pivot Faceting to work with facet.missing=true (hossman)
 
-* SOLR-3869: A PeerSync attempt to it's replicas by a candidate leader should
+* SOLR-3869: A PeerSync attempt to its replicas by a candidate leader should
   not fail on o.a.http.conn.ConnectTimeoutException. (Mark Miller)
 
 * SOLR-3875: Fixed index boosts on multi-valued fields when docBoost is used 
@@ -6505,7 +6505,7 @@
   the terms component. Example: fq={!term f=weight}1.5   (hossman, yonik) 
 
 * SOLR-1915: DebugComponent now supports using a NamedList to model
-  Explanation objects in it's responses instead of
+  Explanation objects in its responses instead of
   Explanation.toString  (hossman)
 
 * SOLR-2448: Search results clustering updates: bisecting k-means
@@ -6539,7 +6539,7 @@
   commit point on server startup is never removed. (yonik)
 
 * SOLR-2466: SolrJ's CommonsHttpSolrServer would retry requests on failure, regardless
-  of the configured maxRetries, due to HttpClient having it's own retry mechanism
+  of the configured maxRetries, due to HttpClient having its own retry mechanism
   by default.  The retryCount of HttpClient is now set to 0, and SolrJ does
   the retry.  (yonik)
 
@@ -7449,7 +7449,7 @@
 schema.xml, they must support reusability.  If your Tokenizer or TokenFilter
 maintains state, it should implement reset().  If your TokenFilteFactory does
 not return a subclass of TokenFilter, then it should implement reset() and call
-reset() on it's input TokenStream.  TokenizerFactory implementations must
+reset() on its input TokenStream.  TokenizerFactory implementations must
 now return a Tokenizer rather than a TokenStream.
 
 New users of Solr 1.4 will have omitTermFreqAndPositions enabled for non-text
@@ -7687,8 +7687,8 @@
 47. SOLR-1106: Made CoreAdminHandler Actions pluggable so that additional actions may be plugged in or the existing
     ones can be overridden if needed. (Kay Kay, Noble Paul, shalin)
 
-48. SOLR-1124: Add a top() function query that causes it's argument to
-    have it's values derived from the top level IndexReader, even when
+48. SOLR-1124: Add a top() function query that causes its argument to
+    have its values derived from the top level IndexReader, even when
     invoked from a sub-reader.  top() is implicitly used for the
     ord() and rord() functions.  (yonik)
 
@@ -9440,7 +9440,7 @@
  3. A new method "getSolrQueryParser" has been added to the IndexSchema
     class for retrieving a new SolrQueryParser instance with all options
     specified in the schema.xml's <solrQueryParser> block set.  The
-    documentation for the SolrQueryParser constructor and it's use of
+    documentation for the SolrQueryParser constructor and its use of
     IndexSchema have also been clarified.
     (Erik Hatcher and hossman)
 
Index: solr/contrib/analysis-extras/src/test/org/apache/solr/schema/TestICUCollationField.java
===================================================================
--- solr/contrib/analysis-extras/src/test/org/apache/solr/schema/TestICUCollationField.java	(revision 1647726)
+++ solr/contrib/analysis-extras/src/test/org/apache/solr/schema/TestICUCollationField.java	(working copy)
@@ -60,7 +60,7 @@
    * Ugly: but what to do? We want to test custom sort, which reads rules in as a resource.
    * These are largish files, and jvm-specific (as our documentation says, you should always
    * look out for jvm differences with collation).
-   * So its preferable to create this file on-the-fly.
+   * So it's preferable to create this file on-the-fly.
    */
   public static String setupSolrHome() throws Exception {
     String tmpFile = createTempDir().toFile().getAbsolutePath();
Index: solr/contrib/analysis-extras/src/test/org/apache/solr/schema/TestICUCollationFieldDocValues.java
===================================================================
--- solr/contrib/analysis-extras/src/test/org/apache/solr/schema/TestICUCollationFieldDocValues.java	(revision 1647726)
+++ solr/contrib/analysis-extras/src/test/org/apache/solr/schema/TestICUCollationFieldDocValues.java	(working copy)
@@ -58,7 +58,7 @@
    * Ugly: but what to do? We want to test custom sort, which reads rules in as a resource.
    * These are largish files, and jvm-specific (as our documentation says, you should always
    * look out for jvm differences with collation).
-   * So its preferable to create this file on-the-fly.
+   * So it's preferable to create this file on-the-fly.
    */
   public static String setupSolrHome() throws Exception {
     File tmpFile = createTempDir().toFile();
Index: solr/contrib/analytics/src/java/org/apache/solr/analytics/expression/MultiDelegateExpression.java
===================================================================
--- solr/contrib/analytics/src/java/org/apache/solr/analytics/expression/MultiDelegateExpression.java	(revision 1647726)
+++ solr/contrib/analytics/src/java/org/apache/solr/analytics/expression/MultiDelegateExpression.java	(working copy)
@@ -33,7 +33,7 @@
   }
 }
 /**
- * <code>AddExpression</code> returns the sum of it's components' values.
+ * <code>AddExpression</code> returns the sum of its components' values.
  */
 class AddExpression extends MultiDelegateExpression {
   public AddExpression(Expression[] delegates) {
@@ -56,7 +56,7 @@
   }
 }
 /**
- * <code>MultiplyExpression</code> returns the product of it's delegates' values.
+ * <code>MultiplyExpression</code> returns the product of its delegates' values.
  */
 class MultiplyExpression extends MultiDelegateExpression {
   public MultiplyExpression(Expression[] delegates) {
@@ -110,7 +110,7 @@
   }
 }
 /**
- * <code>ConcatenateExpression</code> returns the concatenation of it's delegates' values in the order given.
+ * <code>ConcatenateExpression</code> returns the concatenation of its delegates' values in the order given.
  */
 class ConcatenateExpression extends MultiDelegateExpression {
   public ConcatenateExpression(Expression[] delegates) {
Index: solr/contrib/analytics/src/java/org/apache/solr/analytics/util/valuesource/AddDoubleFunction.java
===================================================================
--- solr/contrib/analytics/src/java/org/apache/solr/analytics/util/valuesource/AddDoubleFunction.java	(revision 1647726)
+++ solr/contrib/analytics/src/java/org/apache/solr/analytics/util/valuesource/AddDoubleFunction.java	(working copy)
@@ -22,7 +22,7 @@
 import org.apache.solr.analytics.util.AnalyticsParams;
 
 /**
- * <code>AddDoubleFunction</code> returns the sum of it's components.
+ * <code>AddDoubleFunction</code> returns the sum of its components.
  */
 public class AddDoubleFunction extends MultiDoubleFunction {
   public final static String NAME = AnalyticsParams.ADD;
Index: solr/contrib/analytics/src/java/org/apache/solr/analytics/util/valuesource/MultiplyDoubleFunction.java
===================================================================
--- solr/contrib/analytics/src/java/org/apache/solr/analytics/util/valuesource/MultiplyDoubleFunction.java	(revision 1647726)
+++ solr/contrib/analytics/src/java/org/apache/solr/analytics/util/valuesource/MultiplyDoubleFunction.java	(working copy)
@@ -22,7 +22,7 @@
 import org.apache.solr.analytics.util.AnalyticsParams;
 
 /**
- * <code>MultiplyDoubleFunction</code> returns the product of it's components.
+ * <code>MultiplyDoubleFunction</code> returns the product of its components.
  */
 public class MultiplyDoubleFunction extends MultiDoubleFunction {
   public final static String NAME = AnalyticsParams.MULTIPLY;
Index: solr/contrib/clustering/src/test-files/clustering/solr/collection1/conf/solrconfig.xml
===================================================================
--- solr/contrib/clustering/src/test-files/clustering/solr/collection1/conf/solrconfig.xml	(revision 1647726)
+++ solr/contrib/clustering/src/test-files/clustering/solr/collection1/conf/solrconfig.xml	(working copy)
@@ -305,7 +305,7 @@
   </requestHandler>
 
   <!-- DisMaxRequestHandler allows easy searching across multiple fields
-       for simple user-entered phrases.  It's implementation is now
+       for simple user-entered phrases.  Its implementation is now
        just the standard SearchHandler with a default query parser
        of "dismax". 
        see http://wiki.apache.org/solr/DisMaxRequestHandler
Index: solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/TestXPathRecordReader.java
===================================================================
--- solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/TestXPathRecordReader.java	(revision 1647726)
+++ solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/TestXPathRecordReader.java	(working copy)
@@ -394,7 +394,7 @@
              + "  this <boo>top level</boo> is ignored because it is external to the forEach\n"
              + "  <status>as is <boo>this element</boo></status>\n"
              + "  <contenido id=\"10097\" idioma=\"cat\">\n"
-             + "    This one is <boo>not ignored as its</boo> inside a forEach\n"
+             + "    This one is <boo>not ignored as it's</boo> inside a forEach\n"
              + "    <antetitulo><i> big <boo>antler</boo></i></antetitulo>\n"
              + "    <titulo>  My <i>flattened <boo>title</boo></i> </titulo>\n"
              + "    <resumen> My summary <i>skip this!</i>  </resumen>\n"
@@ -407,15 +407,15 @@
     assertEquals(1, l.size());
     Map<String, Object> m = l.get(0);
     assertEquals("This one is  inside a forEach", m.get("cont").toString().trim());
-    assertEquals("10097"             ,m.get("id"));
-    assertEquals("My flattened title",m.get("title").toString().trim());
-    assertEquals("My summary"        ,m.get("resume").toString().trim());
-    assertEquals("My text"           ,m.get("text").toString().trim());
-    assertEquals("not ignored as its",(String) ((List) m.get("descdend")).get(0) );
-    assertEquals("antler"            ,(String) ((List) m.get("descdend")).get(1) );
-    assertEquals("Within the body of",(String) ((List) m.get("descdend")).get(2) );
-    assertEquals("inner  as well"    ,(String) ((List) m.get("descdend")).get(3) );
-    assertEquals("sub clauses"       ,m.get("inr_descd").toString().trim());
+    assertEquals("10097"              ,m.get("id"));
+    assertEquals("My flattened title" ,m.get("title").toString().trim());
+    assertEquals("My summary"         ,m.get("resume").toString().trim());
+    assertEquals("My text"            ,m.get("text").toString().trim());
+    assertEquals("not ignored as it's",(String) ((List) m.get("descdend")).get(0) );
+    assertEquals("antler"             ,(String) ((List) m.get("descdend")).get(1) );
+    assertEquals("Within the body of" ,(String) ((List) m.get("descdend")).get(2) );
+    assertEquals("inner  as well"     ,(String) ((List) m.get("descdend")).get(3) );
+    assertEquals("sub clauses"        ,m.get("inr_descd").toString().trim());
   }
 
   @Test
@@ -428,7 +428,7 @@
              + "  this <boo>top level</boo> is ignored because it is external to the forEach\n"
              + "  <status>as is <boo>this element</boo></status>\n"
              + "  <contenido id=\"10097\" idioma=\"cat\">\n"
-             + "    This one is <boo>not ignored as its</boo> inside a forEach\n"
+             + "    This one is <boo>not ignored as it's</boo> inside a forEach\n"
              + "    <antetitulo><i> big <boo>antler</boo></i></antetitulo>\n"
              + "    <titulo>  My <i>flattened <boo>title</boo></i> </titulo>\n"
              + "    <resumen> My summary <i>skip this!</i>  </resumen>\n"
@@ -440,13 +440,13 @@
     List<Map<String, Object>> l = rr.getAllRecords(new StringReader(xml));
     assertEquals(1, l.size());
     Map<String, Object> m = l.get(0);
-    assertEquals("top level"         ,(String) ((List) m.get("descdend")).get(0) );
-    assertEquals("this element"      ,(String) ((List) m.get("descdend")).get(1) );
-    assertEquals("not ignored as its",(String) ((List) m.get("descdend")).get(2) );
-    assertEquals("antler"            ,(String) ((List) m.get("descdend")).get(3) );
-    assertEquals("title"             ,(String) ((List) m.get("descdend")).get(4) );
-    assertEquals("Within the body of",(String) ((List) m.get("descdend")).get(5) );
-    assertEquals("inner  as well"    ,(String) ((List) m.get("descdend")).get(6) );
+    assertEquals("top level"          ,(String) ((List) m.get("descdend")).get(0) );
+    assertEquals("this element"       ,(String) ((List) m.get("descdend")).get(1) );
+    assertEquals("not ignored as it's",(String) ((List) m.get("descdend")).get(2) );
+    assertEquals("antler"             ,(String) ((List) m.get("descdend")).get(3) );
+    assertEquals("title"              ,(String) ((List) m.get("descdend")).get(4) );
+    assertEquals("Within the body of" ,(String) ((List) m.get("descdend")).get(5) );
+    assertEquals("inner  as well"     ,(String) ((List) m.get("descdend")).get(6) );
   }
 
   @Test
@@ -459,7 +459,7 @@
              + "  this <boo>top level</boo> is ignored because it is external to the forEach\n"
              + "  <status>as is <boo>this element</boo></status>\n"
              + "  <contenido id=\"10097\" idioma=\"cat\">\n"
-             + "    This one is <boo>not ignored as its</boo> inside a forEach\n"
+             + "    This one is <boo>not ignored as it's</boo> inside a forEach\n"
              + "    <antetitulo><i> big <boo>antler</boo></i></antetitulo>\n"
              + "    <titulo>  My <i>flattened <boo>title</boo></i> </titulo>\n"
              + "    <resumen> My summary <i>skip this!</i>  </resumen>\n"
@@ -471,11 +471,11 @@
     List<Map<String, Object>> l = rr.getAllRecords(new StringReader(xml));
     assertEquals(1, l.size());
     Map<String, Object> m = l.get(0);
-    assertEquals("not ignored as its",((List) m.get("descdend")).get(0) );
-    assertEquals("antler"            ,((List) m.get("descdend")).get(1) );
-    assertEquals("title"             ,((List) m.get("descdend")).get(2) );
-    assertEquals("Within the body of",((List) m.get("descdend")).get(3) );
-    assertEquals("inner  as well"    ,((List) m.get("descdend")).get(4) );
+    assertEquals("not ignored as it's",((List) m.get("descdend")).get(0) );
+    assertEquals("antler"             ,((List) m.get("descdend")).get(1) );
+    assertEquals("title"              ,((List) m.get("descdend")).get(2) );
+    assertEquals("Within the body of" ,((List) m.get("descdend")).get(3) );
+    assertEquals("inner  as well"     ,((List) m.get("descdend")).get(4) );
   }
   
   @Test
Index: solr/contrib/dataimporthandler-extras/src/java/org/apache/solr/handler/dataimport/MailEntityProcessor.java
===================================================================
--- solr/contrib/dataimporthandler-extras/src/java/org/apache/solr/handler/dataimport/MailEntityProcessor.java	(revision 1647726)
+++ solr/contrib/dataimporthandler-extras/src/java/org/apache/solr/handler/dataimport/MailEntityProcessor.java	(working copy)
@@ -534,7 +534,7 @@
     }
     
     public void remove() {
-      throw new UnsupportedOperationException("Its read only mode...");
+      throw new UnsupportedOperationException("It's read only mode...");
     }
     
     private void getTopLevelFolders(Store mailBox) {
@@ -544,7 +544,7 @@
         try {
           folders.add(mailbox.getFolder(topLevelFolders.get(i)));
         } catch (MessagingException e) {
-          // skip bad ones unless its the last one and still no good folder
+          // skip bad ones unless it's the last one and still no good folder
           if (folders.size() == 0 && i == topLevelFolders.size() - 1) throw new DataImportHandlerException(
               DataImportHandlerException.SEVERE, "Folder retreival failed");
         }
@@ -705,7 +705,7 @@
     }
     
     public void remove() {
-      throw new UnsupportedOperationException("Its read only mode...");
+      throw new UnsupportedOperationException("It's read only mode...");
     }
     
     private SearchTerm getSearchTerm() {
Index: solr/contrib/morphlines-core/src/test-files/solr/collection1/conf/solrconfig.xml
===================================================================
--- solr/contrib/morphlines-core/src/test-files/solr/collection1/conf/solrconfig.xml	(revision 1647726)
+++ solr/contrib/morphlines-core/src/test-files/solr/collection1/conf/solrconfig.xml	(working copy)
@@ -122,7 +122,7 @@
        index format, but hooks into the schema to provide per-field customization of
        the postings lists and per-document values in the fieldType element
        (postingsFormat/docValuesFormat). Note that most of the alternative implementations
-       are experimental, so if you choose to customize the index format, its a good
+       are experimental, so if you choose to customize the index format, it's a good
        idea to convert back to the official format e.g. via IndexWriter.addIndexes(IndexReader)
        before upgrading to a newer version to avoid unnecessary reindexing.
   -->
Index: solr/contrib/morphlines-core/src/test-files/solr/minimr/conf/solrconfig.xml
===================================================================
--- solr/contrib/morphlines-core/src/test-files/solr/minimr/conf/solrconfig.xml	(revision 1647726)
+++ solr/contrib/morphlines-core/src/test-files/solr/minimr/conf/solrconfig.xml	(working copy)
@@ -138,7 +138,7 @@
        index format, but hooks into the schema to provide per-field customization of
        the postings lists and per-document values in the fieldType element
        (postingsFormat/docValuesFormat). Note that most of the alternative implementations
-       are experimental, so if you choose to customize the index format, its a good
+       are experimental, so if you choose to customize the index format, it's a good
        idea to convert back to the official format e.g. via IndexWriter.addIndexes(IndexReader)
        before upgrading to a newer version to avoid unnecessary reindexing.
   -->
Index: solr/contrib/morphlines-core/src/test-files/solr/mrunit/conf/solrconfig.xml
===================================================================
--- solr/contrib/morphlines-core/src/test-files/solr/mrunit/conf/solrconfig.xml	(revision 1647726)
+++ solr/contrib/morphlines-core/src/test-files/solr/mrunit/conf/solrconfig.xml	(working copy)
@@ -140,7 +140,7 @@
        index format, but hooks into the schema to provide per-field customization of
        the postings lists and per-document values in the fieldType element
        (postingsFormat/docValuesFormat). Note that most of the alternative implementations
-       are experimental, so if you choose to customize the index format, its a good
+       are experimental, so if you choose to customize the index format, it's a good
        idea to convert back to the official format e.g. via IndexWriter.addIndexes(IndexReader)
        before upgrading to a newer version to avoid unnecessary reindexing.
   -->
Index: solr/contrib/morphlines-core/src/test-files/solr/solrcelltest/collection1/conf/solrconfig.xml
===================================================================
--- solr/contrib/morphlines-core/src/test-files/solr/solrcelltest/collection1/conf/solrconfig.xml	(revision 1647726)
+++ solr/contrib/morphlines-core/src/test-files/solr/solrcelltest/collection1/conf/solrconfig.xml	(working copy)
@@ -122,7 +122,7 @@
        index format, but hooks into the schema to provide per-field customization of
        the postings lists and per-document values in the fieldType element
        (postingsFormat/docValuesFormat). Note that most of the alternative implementations
-       are experimental, so if you choose to customize the index format, its a good
+       are experimental, so if you choose to customize the index format, it's a good
        idea to convert back to the official format e.g. via IndexWriter.addIndexes(IndexReader)
        before upgrading to a newer version to avoid unnecessary reindexing.
   -->
Index: solr/contrib/morphlines-core/src/test-files/solr/solrcloud/conf/solrconfig.xml
===================================================================
--- solr/contrib/morphlines-core/src/test-files/solr/solrcloud/conf/solrconfig.xml	(revision 1647726)
+++ solr/contrib/morphlines-core/src/test-files/solr/solrcloud/conf/solrconfig.xml	(working copy)
@@ -141,7 +141,7 @@
        index format, but hooks into the schema to provide per-field customization of
        the postings lists and per-document values in the fieldType element
        (postingsFormat/docValuesFormat). Note that most of the alternative implementations
-       are experimental, so if you choose to customize the index format, its a good
+       are experimental, so if you choose to customize the index format, it's a good
        idea to convert back to the official format e.g. via IndexWriter.addIndexes(IndexReader)
        before upgrading to a newer version to avoid unnecessary reindexing.
   -->
Index: solr/contrib/morphlines-core/src/test-files/test-morphlines/tutorialReadAvroContainer.conf
===================================================================
--- solr/contrib/morphlines-core/src/test-files/test-morphlines/tutorialReadAvroContainer.conf	(revision 1647726)
+++ solr/contrib/morphlines-core/src/test-files/test-morphlines/tutorialReadAvroContainer.conf	(working copy)
@@ -34,7 +34,7 @@
 # transformation chain. A morphline consists of one or more (potentially 
 # nested) commands. A morphline is a way to consume records (e.g. Flume events, 
 # HDFS files or blocks), turn them into a stream of records, and pipe the stream 
-# of records through a set of easily configurable transformations on it's way to 
+# of records through a set of easily configurable transformations on its way to 
 # Solr.
 morphlines : [
   {
Index: solr/contrib/uima/src/test-files/uima/solr/collection1/conf/solrconfig.xml
===================================================================
--- solr/contrib/uima/src/test-files/uima/solr/collection1/conf/solrconfig.xml	(revision 1647726)
+++ solr/contrib/uima/src/test-files/uima/solr/collection1/conf/solrconfig.xml	(working copy)
@@ -398,7 +398,7 @@
 
   <!--
     DisMaxRequestHandler allows easy searching across multiple fields
-    for simple user-entered phrases. It's implementation is now just the
+    for simple user-entered phrases. Its implementation is now just the
     standard SearchHandler with a default query parser of "dismax". see
     http://wiki.apache.org/solr/DisMaxRequestHandler
   -->
Index: solr/contrib/uima/src/test-files/uima/uima-tokenizers-solrconfig.xml
===================================================================
--- solr/contrib/uima/src/test-files/uima/uima-tokenizers-solrconfig.xml	(revision 1647726)
+++ solr/contrib/uima/src/test-files/uima/uima-tokenizers-solrconfig.xml	(working copy)
@@ -397,7 +397,7 @@
 
   <!--
     DisMaxRequestHandler allows easy searching across multiple fields
-    for simple user-entered phrases. It's implementation is now just the
+    for simple user-entered phrases. Its implementation is now just the
     standard SearchHandler with a default query parser of "dismax". see
     http://wiki.apache.org/solr/DisMaxRequestHandler
   -->
Index: solr/core/src/java/org/apache/solr/cloud/Assign.java
===================================================================
--- solr/core/src/java/org/apache/solr/cloud/Assign.java	(revision 1647726)
+++ solr/core/src/java/org/apache/solr/cloud/Assign.java	(working copy)
@@ -174,7 +174,7 @@
           + collectionName
           + " is higher than or equal to the number of Solr instances currently live or part of your " + CREATE_NODE_SET + "("
           + nodeList.size()
-          + "). Its unusual to run two replica of the same slice on the same Solr-instance.");
+          + "). It's unusual to run two replica of the same slice on the same Solr-instance.");
     }
 
     int maxCoresAllowedToCreate = maxShardsPerNode * nodeList.size();
Index: solr/core/src/java/org/apache/solr/cloud/OverseerAutoReplicaFailoverThread.java
===================================================================
--- solr/core/src/java/org/apache/solr/cloud/OverseerAutoReplicaFailoverThread.java	(revision 1647726)
+++ solr/core/src/java/org/apache/solr/cloud/OverseerAutoReplicaFailoverThread.java	(working copy)
@@ -128,7 +128,7 @@
         doWork();
       } catch (Exception e) {
         SolrException.log(log, this.getClass().getSimpleName()
-            + " had an error it's thread work loop.", e);
+            + " had an error in its thread work loop.", e);
       }
       
       if (!this.isClosed) {
Index: solr/core/src/java/org/apache/solr/cloud/OverseerCollectionProcessor.java
===================================================================
--- solr/core/src/java/org/apache/solr/cloud/OverseerCollectionProcessor.java	(revision 1647726)
+++ solr/core/src/java/org/apache/solr/cloud/OverseerCollectionProcessor.java	(working copy)
@@ -2378,7 +2378,7 @@
             + collectionName
             + " is higher than or equal to the number of Solr instances currently live or live and part of your " + CREATE_NODE_SET + "("
             + nodeList.size()
-            + "). Its unusual to run two replica of the same slice on the same Solr-instance.");
+            + "). It's unusual to run two replica of the same slice on the same Solr-instance.");
       }
       
       int maxShardsAllowedToCreate = maxShardsPerNode * nodeList.size();
Index: solr/core/src/java/org/apache/solr/cloud/ZkController.java
===================================================================
--- solr/core/src/java/org/apache/solr/cloud/ZkController.java	(revision 1647726)
+++ solr/core/src/java/org/apache/solr/cloud/ZkController.java	(working copy)
@@ -774,7 +774,7 @@
       }
       zkClient.makePath(nodePath, CreateMode.EPHEMERAL, true);
     } catch (KeeperException e) {
-      // its okay if the node already exists
+      // it's okay if the node already exists
       if (e.code() != KeeperException.Code.NODEEXISTS) {
         throw e;
       }
@@ -1314,7 +1314,7 @@
           zkClient.makePath(collectionPath, ZkStateReader.toJSON(zkProps), CreateMode.PERSISTENT, null, true);
 
         } catch (KeeperException e) {
-          // its okay if the node already exists
+          // it's okay if the node already exists
           if (e.code() != KeeperException.Code.NODEEXISTS) {
             throw e;
           }
@@ -1324,7 +1324,7 @@
       }
       
     } catch (KeeperException e) {
-      // its okay if another beats us creating the node
+      // it's okay if another beats us creating the node
       if (e.code() == KeeperException.Code.NODEEXISTS) {
         return;
       }
@@ -1707,7 +1707,7 @@
         zkClient.makePath(path, ZkStateReader.toJSON(props),
             CreateMode.PERSISTENT, null, true);
       } catch (KeeperException e2) {
-        // its okay if the node already exists
+        // it's okay if the node already exists
         if (e2.code() != KeeperException.Code.NODEEXISTS) {
           throw e;
         }
@@ -1811,7 +1811,7 @@
   
   /**
    * Utility method for trimming and leading and/or trailing slashes from 
-   * it's input.  May return the empty string.  May return null if and only 
+   * its input.  May return the empty string.  May return null if and only 
    * if the input is null.
    */
   public static String trimLeadingAndTrailingSlashes(final String in) {
@@ -2057,7 +2057,7 @@
       stateObj = ZkNodeProps.makeMap();
 
     stateObj.put("state", state);
-    // only update the createdBy value if its not set
+    // only update the createdBy value if it's not set
     if (stateObj.get("createdByNodeName") == null)
       stateObj.put("createdByNodeName", String.valueOf(this.nodeName));
 
Index: solr/core/src/java/org/apache/solr/core/CloseHook.java
===================================================================
--- solr/core/src/java/org/apache/solr/core/CloseHook.java	(revision 1647726)
+++ solr/core/src/java/org/apache/solr/core/CloseHook.java	(working copy)
@@ -45,7 +45,7 @@
    * <br/>
    * Use this method for post-close clean up operations e.g. deleting the index from disk.
    * <br/>
-   * <b>The core's passed to the method is already closed and therefore, it's update handler or searcher should *NOT* be used</b>
+   * <b>The core's passed to the method is already closed and therefore, its update handler or searcher should *NOT* be used</b>
    *
    * <b>Important:</b> Keep the method implementation as short as possible. If it were to use any heavy i/o , network connections -
    * it might be a better idea to launch in a separate Thread so as to not to block the process of
Index: solr/core/src/java/org/apache/solr/core/DirectoryFactory.java
===================================================================
--- solr/core/src/java/org/apache/solr/core/DirectoryFactory.java	(revision 1647726)
+++ solr/core/src/java/org/apache/solr/core/DirectoryFactory.java	(working copy)
@@ -51,7 +51,7 @@
   private static final Logger log = LoggerFactory.getLogger(DirectoryFactory.class.getName());
   
   /**
-   * Indicates a Directory will no longer be used, and when it's ref count
+   * Indicates a Directory will no longer be used, and when its ref count
    * hits 0, it can be closed. On close all directories will be closed
    * whether this has been called or not. This is simply to allow early cleanup.
    * 
Index: solr/core/src/java/org/apache/solr/core/JmxMonitoredMap.java
===================================================================
--- solr/core/src/java/org/apache/solr/core/JmxMonitoredMap.java	(revision 1647726)
+++ solr/core/src/java/org/apache/solr/core/JmxMonitoredMap.java	(working copy)
@@ -335,13 +335,13 @@
       }
 
       if (val != null) {
-        // Its String or one of the simple types, just return it as JMX suggests direct support for such types
+        // It's String or one of the simple types, just return it as JMX suggests direct support for such types
         for (String simpleTypeName : SimpleType.ALLOWED_CLASSNAMES_LIST) {
           if (val.getClass().getName().equals(simpleTypeName)) {
             return val;
           }
         }
-        // Its an arbitrary object which could be something complex and odd, return its toString, assuming that is
+        // It's an arbitrary object which could be something complex and odd, return its toString, assuming that is
         // a workable representation of the object
         return val.toString();
       }
Index: solr/core/src/java/org/apache/solr/core/RequestHandlers.java
===================================================================
--- solr/core/src/java/org/apache/solr/core/RequestHandlers.java	(revision 1647726)
+++ solr/core/src/java/org/apache/solr/core/RequestHandlers.java	(working copy)
@@ -70,7 +70,7 @@
   public static final boolean disableExternalLib = Boolean.parseBoolean(System.getProperty("disable.external.lib", "false"));
 
   /**
-   * Trim the trailing '/' if its there, and convert null to empty string.
+   * Trim the trailing '/' if it's there, and convert null to empty string.
    * 
    * we want:
    *  /update/csv   and
Index: solr/core/src/java/org/apache/solr/core/SolrCore.java
===================================================================
--- solr/core/src/java/org/apache/solr/core/SolrCore.java	(revision 1647726)
+++ solr/core/src/java/org/apache/solr/core/SolrCore.java	(working copy)
@@ -1473,7 +1473,7 @@
   }
 
 
-  /** Opens a new searcher and returns a RefCounted&lt;SolrIndexSearcher&gt; with it's reference incremented.
+  /** Opens a new searcher and returns a RefCounted&lt;SolrIndexSearcher&gt; with its reference incremented.
    *
    * "realtime" means that we need to open quickly for a realtime view of the index, hence don't do any
    * autowarming and add to the _realtimeSearchers queue rather than the _searchers queue (so it won't
Index: solr/core/src/java/org/apache/solr/core/SolrResourceLoader.java
===================================================================
--- solr/core/src/java/org/apache/solr/core/SolrResourceLoader.java	(revision 1647726)
+++ solr/core/src/java/org/apache/solr/core/SolrResourceLoader.java	(working copy)
@@ -162,7 +162,7 @@
    * Adds every file/dir found in the baseDir which passes the specified Filter
    * to the ClassLoader used by this ResourceLoader.  This method <b>MUST</b>
    * only be called prior to using this ResourceLoader to get any resources, otherwise
-   * it's behavior will be non-deterministic. You also have to {link @reloadLuceneSPI}
+   * its behavior will be non-deterministic. You also have to {link @reloadLuceneSPI}
    * before using this ResourceLoader.
    * 
    * <p>This method will quietly ignore missing or non-directory <code>baseDir</code>
@@ -424,7 +424,7 @@
   }
   
   /**
-   * This method loads a class either with it's FQN or a short-name (solr.class-simplename or class-simplename).
+   * This method loads a class either with its FQN or a short-name (solr.class-simplename or class-simplename).
    * It tries to load the class with the name that is given first and if it fails, it tries all the known
    * solr packages. This method caches the FQN of a short-name in a static map in-order to make subsequent lookups
    * for the same class faster. The caching is done only if the class is loaded by the webapp classloader and it
Index: solr/core/src/java/org/apache/solr/handler/PingRequestHandler.java
===================================================================
--- solr/core/src/java/org/apache/solr/handler/PingRequestHandler.java	(revision 1647726)
+++ solr/core/src/java/org/apache/solr/handler/PingRequestHandler.java	(working copy)
@@ -50,7 +50,7 @@
  * </p>
  * 
  * <p> 
- * In it's simplest form, the PingRequestHandler should be
+ * In its simplest form, the PingRequestHandler should be
  * configured with some defaults indicating a request that should be
  * executed.  If the request succeeds, then the PingRequestHandler
  * will respond back with a simple "OK" status.  If the request fails,
Index: solr/core/src/java/org/apache/solr/handler/component/SpellCheckComponent.java
===================================================================
--- solr/core/src/java/org/apache/solr/handler/component/SpellCheckComponent.java	(revision 1647726)
+++ solr/core/src/java/org/apache/solr/handler/component/SpellCheckComponent.java	(working copy)
@@ -651,7 +651,7 @@
           NamedList spellchecker = (NamedList) initParams.getVal(i);
           String className = (String) spellchecker.get("classname");
           // TODO: this is a little bit sneaky: warn if class isnt supplied
-          // so that its mandatory in a future release?
+          // so that it's mandatory in a future release?
           if (className == null)
             className = IndexBasedSpellChecker.class.getName();
           SolrResourceLoader loader = core.getResourceLoader();
Index: solr/core/src/java/org/apache/solr/highlight/DefaultSolrHighlighter.java
===================================================================
--- solr/core/src/java/org/apache/solr/highlight/DefaultSolrHighlighter.java	(revision 1647726)
+++ solr/core/src/java/org/apache/solr/highlight/DefaultSolrHighlighter.java	(working copy)
@@ -759,7 +759,7 @@
   int startOffset;
 }
 
-/** For use with term vectors of multi-valued fields. We want an offset based window into it's TokenStream. */
+/** For use with term vectors of multi-valued fields. We want an offset based window into its TokenStream. */
 final class OffsetWindowTokenFilter extends TokenFilter {
 
   private final OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);
Index: solr/core/src/java/org/apache/solr/highlight/PostingsSolrHighlighter.java
===================================================================
--- solr/core/src/java/org/apache/solr/highlight/PostingsSolrHighlighter.java	(revision 1647726)
+++ solr/core/src/java/org/apache/solr/highlight/PostingsSolrHighlighter.java	(working copy)
@@ -161,7 +161,7 @@
       for (String field : fieldNames) {
         String snippet = snippets.get(field)[i];
         // box in an array to match the format of existing highlighters, 
-        // even though its always one element.
+        // even though it's always one element.
         if (snippet == null) {
           summary.add(field, new String[0]);
         } else {
Index: solr/core/src/java/org/apache/solr/internal/csv/CSVParser.java
===================================================================
--- solr/core/src/java/org/apache/solr/internal/csv/CSVParser.java	(revision 1647726)
+++ solr/core/src/java/org/apache/solr/internal/csv/CSVParser.java	(working copy)
@@ -55,7 +55,7 @@
   private static final int INITIAL_TOKEN_LENGTH = 50;
   
   // the token types
-  /** Token has no valid content, i.e. is in its initilized state. */
+  /** Token has no valid content, i.e. is in its initialized state. */
   protected static final int TT_INVALID = -1;
   /** Token with content, at beginning or in the middle of a line. */
   protected static final int TT_TOKEN = 0;
Index: solr/core/src/java/org/apache/solr/parser/SolrQueryParserBase.java
===================================================================
--- solr/core/src/java/org/apache/solr/parser/SolrQueryParserBase.java	(revision 1647726)
+++ solr/core/src/java/org/apache/solr/parser/SolrQueryParserBase.java	(working copy)
@@ -791,7 +791,7 @@
         automaton = Operations.minus(automaton, falsePositives, Operations.DEFAULT_MAX_DETERMINIZED_STATES);
       }
       return new AutomatonQuery(term, automaton) {
-        // override toString so its completely transparent
+        // override toString so it's completely transparent
         @Override
         public String toString(String field) {
           StringBuilder buffer = new StringBuilder();
Index: solr/core/src/java/org/apache/solr/request/SimpleFacets.java
===================================================================
--- solr/core/src/java/org/apache/solr/request/SimpleFacets.java	(revision 1647726)
+++ solr/core/src/java/org/apache/solr/request/SimpleFacets.java	(working copy)
@@ -99,7 +99,7 @@
  * A class that generates simple Facet information for a request.
  *
  * More advanced facet implementations may compose or subclass this class 
- * to leverage any of it's functionality.
+ * to leverage any of its functionality.
  */
 public class SimpleFacets {
 
@@ -492,8 +492,8 @@
     SchemaField sf = searcher.getSchema().getFieldOrNull(groupField);
     
     if (sf != null && sf.hasDocValues() == false && sf.multiValued() == false && sf.getType().getNumericType() != null) {
-      // its a single-valued numeric field: we must currently create insanity :(
-      // there isnt a GroupedFacetCollector that works on numerics right now...
+      // it's a single-valued numeric field: we must currently create insanity :(
+      // there isn't a GroupedFacetCollector that works on numerics right now...
       searcher.search(new MatchAllDocsQuery(), base.getTopFilter(), new FilterCollector(collector) {
         @Override
         public LeafCollector getLeafCollector(LeafReaderContext context) throws IOException {
Index: solr/core/src/java/org/apache/solr/request/UnInvertedField.java
===================================================================
--- solr/core/src/java/org/apache/solr/request/UnInvertedField.java	(revision 1647726)
+++ solr/core/src/java/org/apache/solr/request/UnInvertedField.java	(working copy)
@@ -69,7 +69,7 @@
  *
  *   There are actually 256 byte arrays, to compensate for the fact that the pointers
  *   into the byte arrays are only 3 bytes long.  The correct byte array for a document
- *   is a function of it's id.
+ *   is a function of its id.
  *
  *   To save space and speed up faceting, any term that matches enough documents will
  *   not be un-inverted... it will be skipped while building the un-inverted field structure,
@@ -78,7 +78,7 @@
  *   To further save memory, the terms (the actual string values) are not all stored in
  *   memory, but a TermIndex is used to convert term numbers to term values only
  *   for the terms needed after faceting has completed.  Only every 128th term value
- *   is stored, along with it's corresponding term number, and this is used as an
+ *   is stored, along with its corresponding term number, and this is used as an
  *   index to find the closest term and iterate until the desired number is hit (very
  *   much like Lucene's own internal term index).
  *
@@ -234,7 +234,7 @@
       final int[] counts = new int[numTermsInField + 1];
 
       //
-      // If there is prefix, find it's start and end term numbers
+      // If there is prefix, find its start and end term numbers
       //
       int startTerm = 0;
       int endTerm = numTermsInField;  // one past the end
Index: solr/core/src/java/org/apache/solr/response/CSVResponseWriter.java
===================================================================
--- solr/core/src/java/org/apache/solr/response/CSVResponseWriter.java	(revision 1647726)
+++ solr/core/src/java/org/apache/solr/response/CSVResponseWriter.java	(working copy)
@@ -88,7 +88,7 @@
 
   char[] sharedCSVBuf = new char[8192];
 
-  // prevent each instance from creating it's own buffer
+  // prevent each instance from creating its own buffer
   class CSVSharedBufPrinter extends CSVPrinter {
     public CSVSharedBufPrinter(Writer out, CSVStrategy strategy) {
       super(out, strategy);
Index: solr/core/src/java/org/apache/solr/response/PythonResponseWriter.java
===================================================================
--- solr/core/src/java/org/apache/solr/response/PythonResponseWriter.java	(revision 1647726)
+++ solr/core/src/java/org/apache/solr/response/PythonResponseWriter.java	(working copy)
@@ -83,7 +83,7 @@
     }
 
     // use python unicode strings...
-    // python doesn't tolerate newlines in strings in it's eval(), so we must escape them.
+    // python doesn't tolerate newlines in strings in its eval(), so we must escape them.
 
     StringBuilder sb = new StringBuilder(val.length());
     boolean needUnicode=false;
@@ -120,7 +120,7 @@
   old version that always used unicode
   public void writeStr(String name, String val, boolean needsEscaping) throws IOException {
     // use python unicode strings...
-    // python doesn't tolerate newlines in strings in it's eval(), so we must escape them.
+    // python doesn't tolerate newlines in strings in its eval(), so we must escape them.
     writer.write("u'");
     // it might be more efficient to use a stringbuilder or write substrings
     // if writing chars to the stream is slow.
Index: solr/core/src/java/org/apache/solr/response/RawResponseWriter.java
===================================================================
--- solr/core/src/java/org/apache/solr/response/RawResponseWriter.java	(revision 1647726)
+++ solr/core/src/java/org/apache/solr/response/RawResponseWriter.java	(working copy)
@@ -35,7 +35,7 @@
  * This writer is a special case that extends and alters the
  * QueryResponseWriter contract.  If SolrQueryResponse contains a
  * ContentStream added with the key {@link #CONTENT}
- * then this writer will output that stream exactly as is (with it's
+ * then this writer will output that stream exactly as is (with its
  * Content-Type).  if no such ContentStream has been added, then a
  * "base" QueryResponseWriter will be used to write the response
  * according to the usual contract.  The name of the "base" writer can
Index: solr/core/src/java/org/apache/solr/schema/CollationField.java
===================================================================
--- solr/core/src/java/org/apache/solr/schema/CollationField.java	(revision 1647726)
+++ solr/core/src/java/org/apache/solr/schema/CollationField.java	(working copy)
@@ -223,7 +223,7 @@
   /**
    * analyze the range with the analyzer, instead of the collator.
    * because jdk collators might not be thread safe (when they are
-   * its just that all methods are synced), this keeps things 
+   * it's just that all methods are synced), this keeps things 
    * simple (we already have a threadlocal clone in the reused TS)
    */
   private BytesRef getCollationKey(String field, String text) {     
Index: solr/core/src/java/org/apache/solr/schema/CurrencyField.java
===================================================================
--- solr/core/src/java/org/apache/solr/schema/CurrencyField.java	(revision 1647726)
+++ solr/core/src/java/org/apache/solr/schema/CurrencyField.java	(working copy)
@@ -252,7 +252,7 @@
    * <p>
    * Returns a ValueSource over this field in which the numeric value for 
    * each document represents the indexed value as converted to the default 
-   * currency for the field, normalized to it's most granular form based 
+   * currency for the field, normalized to its most granular form based 
    * on the default fractional digits.
    * </p>
    * <p>
Index: solr/core/src/java/org/apache/solr/schema/FieldType.java
===================================================================
--- solr/core/src/java/org/apache/solr/schema/FieldType.java	(revision 1647726)
+++ solr/core/src/java/org/apache/solr/schema/FieldType.java	(working copy)
@@ -450,7 +450,7 @@
   }
   
   /**
-   * DocValues is not enabled for a field, but its indexed, docvalues can be constructed 
+   * DocValues is not enabled for a field, but it's indexed, docvalues can be constructed 
    * on the fly (uninverted, aka fieldcache) on the first request to sort, facet, etc. 
    * This specifies the structure to use.
    * 
@@ -755,7 +755,7 @@
    *
    * <p>
    * This method is called by the <code>SchemaField</code> constructor to 
-   * check that it's initialization does not violate any fundemental 
+   * check that its initialization does not violate any fundemental 
    * requirements of the <code>FieldType</code>.  The default implementation 
    * does nothing, but subclasses may chose to throw a {@link SolrException}  
    * if invariants are violated by the <code>SchemaField.</code>
Index: solr/core/src/java/org/apache/solr/schema/TextField.java
===================================================================
--- solr/core/src/java/org/apache/solr/schema/TextField.java	(revision 1647726)
+++ solr/core/src/java/org/apache/solr/schema/TextField.java	(working copy)
@@ -96,7 +96,7 @@
 
   @Override
   public SortField getSortField(SchemaField field, boolean reverse) {
-    /* :TODO: maybe warn if isTokenized(), but doesn't use LimitTokenCountFilter in it's chain? */
+    /* :TODO: maybe warn if isTokenized(), but doesn't use LimitTokenCountFilter in its chain? */
     field.checkSortability();
     return Sorting.getTextSortField(field.getName(), reverse, field.sortMissingLast(), field.sortMissingFirst());
   }
Index: solr/core/src/java/org/apache/solr/search/BitDocSet.java
===================================================================
--- solr/core/src/java/org/apache/solr/search/BitDocSet.java	(revision 1647726)
+++ solr/core/src/java/org/apache/solr/search/BitDocSet.java	(working copy)
@@ -321,7 +321,7 @@
               @Override
               public long cost() {
                 // we don't want to actually compute cardinality, but
-                // if its already been computed, we use it (pro-rated for the segment)
+                // if it's already been computed, we use it (pro-rated for the segment)
                 if (size != -1) {
                   return (long)(size * ((FixedBitSet.bits2words(maxDoc)<<6) / (float)bs.length()));
                 } else {
Index: solr/core/src/java/org/apache/solr/search/CollapsingQParserPlugin.java
===================================================================
--- solr/core/src/java/org/apache/solr/search/CollapsingQParserPlugin.java	(revision 1647726)
+++ solr/core/src/java/org/apache/solr/search/CollapsingQParserPlugin.java	(working copy)
@@ -617,7 +617,7 @@
           if(ord > -1) {
             dummy.score = scores[ord];
           } else if (boostDocs != null && boostDocs.containsKey(docId)) {
-            //Its an elevated doc so no score is needed
+            //It's an elevated doc so no score is needed
             dummy.score = 0F;
           } else if (nullPolicy == CollapsingPostFilter.NULL_POLICY_COLLAPSE) {
             dummy.score = nullScore;
Index: solr/core/src/java/org/apache/solr/search/DisMaxQParser.java
===================================================================
--- solr/core/src/java/org/apache/solr/search/DisMaxQParser.java	(revision 1647726)
+++ solr/core/src/java/org/apache/solr/search/DisMaxQParser.java	(working copy)
@@ -168,7 +168,7 @@
     }
   }
 
-  /** Adds the main query to the query argument. If its blank then false is returned. */
+  /** Adds the main query to the query argument. If it's blank then false is returned. */
   protected boolean addMainQuery(BooleanQuery query, SolrParams solrParams) throws SyntaxError {
     Map<String, Float> phraseFields = SolrPluginUtils.parseFieldBoosts(solrParams.getParams(DisMaxParams.PF));
     float tiebreaker = solrParams.getFloat(DisMaxParams.TIE, 0.0f);
Index: solr/core/src/java/org/apache/solr/search/DocSet.java
===================================================================
--- solr/core/src/java/org/apache/solr/search/DocSet.java	(revision 1647726)
+++ solr/core/src/java/org/apache/solr/search/DocSet.java	(working copy)
@@ -93,7 +93,7 @@
 
   /**
    * Returns the number of documents of the intersection of this set with another set.
-   * May be more efficient than actually creating the intersection and then getting it's size.
+   * May be more efficient than actually creating the intersection and then getting its size.
    */
   public int intersectionSize(DocSet other);
 
@@ -109,7 +109,7 @@
 
   /**
    * Returns the number of documents of the union of this set with another set.
-   * May be more efficient than actually creating the union and then getting it's size.
+   * May be more efficient than actually creating the union and then getting its size.
    */
   public int unionSize(DocSet other);
 
Index: solr/core/src/java/org/apache/solr/search/DocSetBase.java
===================================================================
--- solr/core/src/java/org/apache/solr/search/DocSetBase.java	(revision 1647726)
+++ solr/core/src/java/org/apache/solr/search/DocSetBase.java	(working copy)
@@ -139,7 +139,7 @@
     if (!(other instanceof BitDocSet)) {
       return other.intersectionSize(this);
     }
-    // less efficient way: do the intersection then get it's size
+    // less efficient way: do the intersection then get its size
     return intersection(other).size();
   }
 
Index: solr/core/src/java/org/apache/solr/search/ExtendedDismaxQParser.java
===================================================================
--- solr/core/src/java/org/apache/solr/search/ExtendedDismaxQParser.java	(revision 1647726)
+++ solr/core/src/java/org/apache/solr/search/ExtendedDismaxQParser.java	(working copy)
@@ -840,7 +840,7 @@
         // special syntax in a string isn't special
         clause.hasSpecialSyntax = false;        
       } else {
-        // an empty clause... must be just a + or - on it's own
+        // an empty clause... must be just a + or - on its own
         if (clause.val.length() == 0) {
           clause.syntaxError = true;
           if (clause.must != 0) {
@@ -997,7 +997,7 @@
     
     public ExtendedSolrQueryParser(QParser parser, String defaultField) {
       super(parser, defaultField);
-      // don't trust that our parent class won't ever change it's default
+      // don't trust that our parent class won't ever change its default
       setDefaultOperator(QueryParser.Operator.OR);
     }
     
@@ -1227,7 +1227,7 @@
             Query query = super.getFieldQuery(field, val, type == QType.PHRASE);
             // A BooleanQuery is only possible from getFieldQuery if it came from
             // a single whitespace separated term. In this case, check the coordination
-            // factor on the query: if its enabled, that means we aren't a set of synonyms
+            // factor on the query: if it's enabled, that means we aren't a set of synonyms
             // but instead multiple terms from one whitespace-separated term, we must
             // apply minShouldMatch here so that it works correctly with other things
             // like aliasing.
Index: solr/core/src/java/org/apache/solr/search/NestedQParserPlugin.java
===================================================================
--- solr/core/src/java/org/apache/solr/search/NestedQParserPlugin.java	(revision 1647726)
+++ solr/core/src/java/org/apache/solr/search/NestedQParserPlugin.java	(working copy)
@@ -23,7 +23,7 @@
 import org.apache.solr.request.SolrQueryRequest;
 
 /**
- * Create a nested query, with the ability of that query to redefine it's type via
+ * Create a nested query, with the ability of that query to redefine its type via
  * local parameters.  This is useful in specifying defaults in configuration and
  * letting clients indirectly reference them.
  * <br>Example: <code>{!query defType=func v=$q1}</code>
Index: solr/core/src/java/org/apache/solr/search/PostFilter.java
===================================================================
--- solr/core/src/java/org/apache/solr/search/PostFilter.java	(revision 1647726)
+++ solr/core/src/java/org/apache/solr/search/PostFilter.java	(working copy)
@@ -42,6 +42,6 @@
  */
 public interface PostFilter extends ExtendedQuery {
 
-  /** Returns a DelegatingCollector to be run after the main query and all of it's filters, but before any sorting or grouping collectors */
+  /** Returns a DelegatingCollector to be run after the main query and all of its filters, but before any sorting or grouping collectors */
   public DelegatingCollector getFilterCollector(IndexSearcher searcher);
 }
Index: solr/core/src/java/org/apache/solr/search/SolrIndexSearcher.java
===================================================================
--- solr/core/src/java/org/apache/solr/search/SolrIndexSearcher.java	(revision 1647726)
+++ solr/core/src/java/org/apache/solr/search/SolrIndexSearcher.java	(working copy)
@@ -1604,7 +1604,7 @@
       final Sort weightedSort = weightSort(cmd.getSort());
       final CursorMark cursor = cmd.getCursorMark();
 
-      // :TODO: make fillFields it's own QueryCommand flag? ...
+      // :TODO: make fillFields its own QueryCommand flag? ...
       // ... see comments in populateNextCursorMarkFromTopDocs for cache issues (SOLR-5595)
       final boolean fillFields = (null != cursor);
       final FieldDoc searchAfter = (null != cursor ? cursor.getSearchAfterFieldDoc() : null);
Index: solr/core/src/java/org/apache/solr/search/function/OrdFieldSource.java
===================================================================
--- solr/core/src/java/org/apache/solr/search/function/OrdFieldSource.java	(revision 1647726)
+++ solr/core/src/java/org/apache/solr/search/function/OrdFieldSource.java	(working copy)
@@ -79,7 +79,7 @@
       SolrIndexSearcher is = (SolrIndexSearcher) o;
       SchemaField sf = is.getSchema().getFieldOrNull(field);
       if (sf != null && sf.hasDocValues() == false && sf.multiValued() == false && sf.getType().getNumericType() != null) {
-        // its a single-valued numeric field: we must currently create insanity :(
+        // it's a single-valued numeric field: we must currently create insanity :(
         List<LeafReaderContext> leaves = is.getIndexReader().leaves();
         LeafReader insaneLeaves[] = new LeafReader[leaves.size()];
         int upto = 0;
@@ -95,7 +95,7 @@
       IndexReader topReader = ReaderUtil.getTopLevelContext(readerContext).reader();
       r = SlowCompositeReaderWrapper.wrap(topReader);
     }
-    // if its e.g. tokenized/multivalued, emulate old behavior of single-valued fc
+    // if it's e.g. tokenized/multivalued, emulate old behavior of single-valued fc
     final SortedDocValues sindex = SortedSetSelector.wrap(DocValues.getSortedSet(r, field), SortedSetSelector.Type.MIN);
     return new IntDocValues(this) {
       protected String toTerm(String readableValue) {
Index: solr/core/src/java/org/apache/solr/search/function/ReverseOrdFieldSource.java
===================================================================
--- solr/core/src/java/org/apache/solr/search/function/ReverseOrdFieldSource.java	(revision 1647726)
+++ solr/core/src/java/org/apache/solr/search/function/ReverseOrdFieldSource.java	(working copy)
@@ -79,7 +79,7 @@
       SolrIndexSearcher is = (SolrIndexSearcher) o;
       SchemaField sf = is.getSchema().getFieldOrNull(field);
       if (sf != null && sf.hasDocValues() == false && sf.multiValued() == false && sf.getType().getNumericType() != null) {
-        // its a single-valued numeric field: we must currently create insanity :(
+        // it's a single-valued numeric field: we must currently create insanity :(
         List<LeafReaderContext> leaves = is.getIndexReader().leaves();
         LeafReader insaneLeaves[] = new LeafReader[leaves.size()];
         int upto = 0;
@@ -95,7 +95,7 @@
       IndexReader topReader = ReaderUtil.getTopLevelContext(readerContext).reader();
       r = SlowCompositeReaderWrapper.wrap(topReader);
     }
-    // if its e.g. tokenized/multivalued, emulate old behavior of single-valued fc
+    // if it's e.g. tokenized/multivalued, emulate old behavior of single-valued fc
     final SortedDocValues sindex = SortedSetSelector.wrap(DocValues.getSortedSet(r, field), SortedSetSelector.Type.MIN);
     final int end = sindex.getValueCount();
 
Index: solr/core/src/java/org/apache/solr/servlet/SolrRequestParsers.java
===================================================================
--- solr/core/src/java/org/apache/solr/servlet/SolrRequestParsers.java	(revision 1647726)
+++ solr/core/src/java/org/apache/solr/servlet/SolrRequestParsers.java	(working copy)
@@ -548,7 +548,7 @@
       while (iter.hasNext()) {
           FileItem item = (FileItem) iter.next();
 
-          // If its a form field, put it in our parameter map
+          // If it's a form field, put it in our parameter map
           if (item.isFormField()) {
             MultiMapSolrParams.addParam( 
               item.getFieldName(), 
@@ -592,7 +592,7 @@
         parseQueryString(qs, map);
       }
       
-      // may be -1, so we check again later. But if its already greater we can stop processing!
+      // may be -1, so we check again later. But if it's already greater we can stop processing!
       final long totalLength = req.getContentLength();
       final long maxLength = ((long) uploadLimitKB) * 1024L;
       if (totalLength > maxLength) {
Index: solr/core/src/java/org/apache/solr/update/DefaultSolrCoreState.java
===================================================================
--- solr/core/src/java/org/apache/solr/update/DefaultSolrCoreState.java	(revision 1647726)
+++ solr/core/src/java/org/apache/solr/update/DefaultSolrCoreState.java	(working copy)
@@ -143,7 +143,7 @@
       // we need to wait for the Writer to fall out of use
       // first lets stop it from being lent out
       pauseWriter = true;
-      // then lets wait until its out of use
+      // then lets wait until it's out of use
       log.info("Waiting until IndexWriter is unused... core=" + coreName);
       
       while (!writerFree) {
@@ -201,7 +201,7 @@
       // we need to wait for the Writer to fall out of use
       // first lets stop it from being lent out
       pauseWriter = true;
-      // then lets wait until its out of use
+      // then lets wait until it's out of use
       log.info("Waiting until IndexWriter is unused... core=" + coreName);
       
       while (!writerFree) {
Index: solr/core/src/java/org/apache/solr/update/DirectUpdateHandler2.java
===================================================================
--- solr/core/src/java/org/apache/solr/update/DirectUpdateHandler2.java	(revision 1647726)
+++ solr/core/src/java/org/apache/solr/update/DirectUpdateHandler2.java	(working copy)
@@ -136,7 +136,7 @@
 
     UpdateLog existingLog = updateHandler.getUpdateLog();
     if (this.ulog != null && this.ulog == existingLog) {
-      // If we are reusing the existing update log, inform the log that it's update handler has changed.
+      // If we are reusing the existing update log, inform the log that its update handler has changed.
       // We do this as late as possible.
       this.ulog.init(this, core);
     }
Index: solr/core/src/java/org/apache/solr/update/PeerSync.java
===================================================================
--- solr/core/src/java/org/apache/solr/update/PeerSync.java	(revision 1647726)
+++ solr/core/src/java/org/apache/solr/update/PeerSync.java	(working copy)
@@ -168,7 +168,7 @@
   }
 
   /** Returns true if peer sync was successful, meaning that this core may not be considered to have the latest updates
-   *  when considering the last N updates between it and it's peers.
+   *  when considering the last N updates between it and its peers.
    *  A commit is not performed.
    */
   public boolean sync() {
Index: solr/core/src/java/org/apache/solr/update/SolrCmdDistributor.java
===================================================================
--- solr/core/src/java/org/apache/solr/update/SolrCmdDistributor.java	(revision 1647726)
+++ solr/core/src/java/org/apache/solr/update/SolrCmdDistributor.java	(working copy)
@@ -118,7 +118,7 @@
             doRetry = true;
           }
           
-          // if its a connect exception, lets try again
+          // if it's a connect exception, lets try again
           if (err.e instanceof SolrServerException) {
             if (((SolrServerException) err.e).getRootCause() instanceof ConnectException) {
               doRetry = true;
Index: solr/core/src/java/org/apache/solr/update/UpdateLog.java
===================================================================
--- solr/core/src/java/org/apache/solr/update/UpdateLog.java	(revision 1647726)
+++ solr/core/src/java/org/apache/solr/update/UpdateLog.java	(working copy)
@@ -317,7 +317,7 @@
   }
 
   /* Takes over ownership of the log, keeping it until no longer needed
-     and then decrementing it's reference and dropping it.
+     and then decrementing its reference and dropping it.
    */
   protected synchronized void addOldLog(TransactionLog oldLog, boolean removeOld) {
     if (oldLog == null) return;
Index: solr/core/src/java/org/apache/solr/update/processor/DistributedUpdateProcessor.java
===================================================================
--- solr/core/src/java/org/apache/solr/update/processor/DistributedUpdateProcessor.java	(revision 1647726)
+++ solr/core/src/java/org/apache/solr/update/processor/DistributedUpdateProcessor.java	(working copy)
@@ -372,7 +372,7 @@
         doDefensiveChecks(phase);
 
         // if request is coming from another collection then we want it to be sent to all replicas
-        // even if it's phase is FROMLEADER
+        // even if its phase is FROMLEADER
         String fromCollection = updateCommand.getReq().getParams().get(DISTRIB_FROM_COLLECTION);
 
         if (DistribPhase.FROMLEADER == phase && !isSubShardLeader && fromCollection == null) {
@@ -774,7 +774,7 @@
     List<Error> errors = cmdDistrib.getErrors();
     // TODO - we may need to tell about more than one error...
     
-    // if its a forward, any fail is a problem - 
+    // if it's a forward, any fail is a problem - 
     // otherwise we assume things are fine if we got it locally
     // until we start allowing min replication param
     if (errors.size() > 0) {
Index: solr/core/src/java/org/apache/solr/util/ConcurrentLRUCache.java
===================================================================
--- solr/core/src/java/org/apache/solr/util/ConcurrentLRUCache.java	(revision 1647726)
+++ solr/core/src/java/org/apache/solr/util/ConcurrentLRUCache.java	(working copy)
@@ -245,7 +245,7 @@
             // this entry is guaranteed not to be in the bottom
             // group, so do nothing but remove it from the eset.
             numKept++;
-            // remove the entry by moving the last element to it's position
+            // remove the entry by moving the last element to its position
             eset[i] = eset[eSize-1];
             eSize--;
 
@@ -258,7 +258,7 @@
             evictEntry(ce.key);
             numRemoved++;
 
-            // remove the entry by moving the last element to it's position
+            // remove the entry by moving the last element to its position
             eset[i] = eset[eSize-1];
             eSize--;
           } else {
Index: solr/core/src/java/org/apache/solr/util/SimplePostTool.java
===================================================================
--- solr/core/src/java/org/apache/solr/util/SimplePostTool.java	(revision 1647726)
+++ solr/core/src/java/org/apache/solr/util/SimplePostTool.java	(working copy)
@@ -762,7 +762,7 @@
   }
 
   /**
-   * Opens the file and posts it's contents to the solrUrl,
+   * Opens the file and posts its contents to the solrUrl,
    * writes to response to output. 
    */
   public void postFile(File file, OutputStream output, String type) {
Index: solr/core/src/java/org/apache/solr/util/SolrCLI.java
===================================================================
--- solr/core/src/java/org/apache/solr/util/SolrCLI.java	(revision 1647726)
+++ solr/core/src/java/org/apache/solr/util/SolrCLI.java	(working copy)
@@ -924,7 +924,7 @@
           String coreUrl = replicaCoreProps.getCoreUrl();
           boolean isLeader = coreUrl.equals(leaderUrl);
 
-          // if replica's node is not live, it's status is DOWN
+          // if replica's node is not live, its status is DOWN
           String nodeName = replicaCoreProps.getNodeName();
           if (nodeName == null || !liveNodes.contains(nodeName)) {
             replicaStatus = ZkStateReader.DOWN;
Index: solr/core/src/java/org/apache/solr/util/SolrPluginUtils.java
===================================================================
--- solr/core/src/java/org/apache/solr/util/SolrPluginUtils.java	(revision 1647726)
+++ solr/core/src/java/org/apache/solr/util/SolrPluginUtils.java	(working copy)
@@ -266,7 +266,7 @@
    * <li>parsedquery - the main query executed formated by the Solr
    *     QueryParsing utils class (which knows about field types)
    * </li>
-   * <li>parsedquery_toString - the main query executed formated by it's
+   * <li>parsedquery_toString - the main query executed formatted by its
    *     own toString method (in case it has internal state Solr
    *     doesn't know about)
    * </li>
@@ -701,7 +701,7 @@
   }
 
   /**
-   * Returns it's input if there is an even (ie: balanced) number of
+   * Returns its input if there is an even (ie: balanced) number of
    * '"' characters -- otherwise returns a String in which all '"'
    * characters are striped out.
    */
@@ -758,7 +758,7 @@
     protected Map<String,Alias> aliases = new HashMap<>(3);
     public DisjunctionMaxQueryParser(QParser qp, String defaultField) {
       super(qp,defaultField);
-      // don't trust that our parent class won't ever change it's default
+      // don't trust that our parent class won't ever change its default
       setDefaultOperator(QueryParser.Operator.OR);
     }
 
Index: solr/core/src/test/org/apache/solr/BasicFunctionalityTest.java
===================================================================
--- solr/core/src/test/org/apache/solr/BasicFunctionalityTest.java	(revision 1647726)
+++ solr/core/src/test/org/apache/solr/BasicFunctionalityTest.java	(working copy)
@@ -625,7 +625,7 @@
             ,"*[count(//doc)=2]"
             ,"//arr[@name='multiDefault']"
             );
-    assertQ("1 doc should have it's explicit multiDefault",
+    assertQ("1 doc should have its explicit multiDefault",
             req("multiDefault:a")
             ,"*[count(//doc)=1]"
             );
@@ -634,7 +634,7 @@
             req("intDefault:42")
             ,"*[count(//doc)=2]"
             );
-    assertQ("1 doc should have it's explicit intDefault",
+    assertQ("1 doc should have its explicit intDefault",
             req("intDefault:[3 TO 5]")
             ,"*[count(//doc)=1]"
             );
Index: solr/core/src/test/org/apache/solr/cloud/BasicDistributedZk2Test.java
===================================================================
--- solr/core/src/test/org/apache/solr/cloud/BasicDistributedZk2Test.java	(revision 1647726)
+++ solr/core/src/test/org/apache/solr/cloud/BasicDistributedZk2Test.java	(working copy)
@@ -125,7 +125,7 @@
         // expected
       }
       
-      // TODO: bring this to it's own method?
+      // TODO: bring this to its own method?
       // try indexing to a leader that has no replicas up
       ZkStateReader zkStateReader = cloudClient.getZkStateReader();
       ZkNodeProps leaderProps = zkStateReader.getLeaderRetry(
Index: solr/core/src/test/org/apache/solr/cloud/CollectionsAPIDistributedZkTest.java
===================================================================
--- solr/core/src/test/org/apache/solr/cloud/CollectionsAPIDistributedZkTest.java	(revision 1647726)
+++ solr/core/src/test/org/apache/solr/cloud/CollectionsAPIDistributedZkTest.java	(working copy)
@@ -287,7 +287,7 @@
   
   private void deleteCollectionWithDownNodes() throws Exception {
     String baseUrl = getBaseUrl((HttpSolrServer) clients.get(0));
-    // now try to remove a collection when a couple of it's nodes are down
+    // now try to remove a collection when a couple of its nodes are down
     if (secondConfigSet) {
       createCollection(null, "halfdeletedcollection2", 3, 3, 6,
           createNewSolrServer("", baseUrl), null, "conf2");
Index: solr/core/src/test/org/apache/solr/cloud/LeaderInitiatedRecoveryOnCommitTest.java
===================================================================
--- solr/core/src/test/org/apache/solr/cloud/LeaderInitiatedRecoveryOnCommitTest.java	(revision 1647726)
+++ solr/core/src/test/org/apache/solr/cloud/LeaderInitiatedRecoveryOnCommitTest.java	(working copy)
@@ -84,7 +84,7 @@
             + printClusterStateInfo(),
         notLeaders.size() == 1);
 
-    // let's put the leader in it's own partition, no replicas can contact it now
+    // let's put the leader in its own partition, no replicas can contact it now
     Replica leader = cloudClient.getZkStateReader().getLeaderRetry(testCollectionName, "shard1");
     SocketProxy leaderProxy = getProxyForReplica(leader);
     leaderProxy.close();
@@ -127,7 +127,7 @@
             + printClusterStateInfo(),
         notLeaders.size() == 2);
 
-    // let's put the leader in it's own partition, no replicas can contact it now
+    // let's put the leader in its own partition, no replicas can contact it now
     Replica leader = cloudClient.getZkStateReader().getLeaderRetry(testCollectionName, "shard1");
     SocketProxy leaderProxy = getProxyForReplica(leader);
     leaderProxy.close();
Index: solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsWriteToMultipleCollectionsTest.java
===================================================================
--- solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsWriteToMultipleCollectionsTest.java	(revision 1647726)
+++ solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsWriteToMultipleCollectionsTest.java	(working copy)
@@ -146,7 +146,7 @@
             // see SOLR-6424
             assertFalse(blockDirectory.isBlockCacheWriteEnabled());
             Cache cache = blockDirectory.getCache();
-            // we know its a BlockDirectoryCache, but future proof
+            // we know it's a BlockDirectoryCache, but future proof
             assertTrue(cache instanceof BlockDirectoryCache);
             BlockCache blockCache = ((BlockDirectoryCache) cache)
                 .getBlockCache();
Index: solr/core/src/test/org/apache/solr/core/SolrCoreCheckLockOnStartupTest.java
===================================================================
--- solr/core/src/test/org/apache/solr/core/SolrCoreCheckLockOnStartupTest.java	(revision 1647726)
+++ solr/core/src/test/org/apache/solr/core/SolrCoreCheckLockOnStartupTest.java	(working copy)
@@ -41,7 +41,7 @@
     System.setProperty("solr.directoryFactory", "org.apache.solr.core.SimpleFSDirectoryFactory");
     // test tests native and simple in the same jvm in the same exact directory:
     // the file will remain after the native test (it cannot safely be deleted without the risk of deleting another guys lock)
-    // its ok, these aren't "compatible" anyway: really this test should not re-use the same directory at all.
+    // it's ok, these aren't "compatible" anyway: really this test should not re-use the same directory at all.
     Files.deleteIfExists(new File(new File(initCoreDataDir, "index"), IndexWriter.WRITE_LOCK_NAME).toPath());
   }
 
Index: solr/core/src/test/org/apache/solr/handler/admin/CoreAdminCreateDiscoverTest.java
===================================================================
--- solr/core/src/test/org/apache/solr/handler/admin/CoreAdminCreateDiscoverTest.java	(revision 1647726)
+++ solr/core/src/test/org/apache/solr/handler/admin/CoreAdminCreateDiscoverTest.java	(working copy)
@@ -82,7 +82,7 @@
     setupCore(coreSysProps, true);
 
     // create a new core (using CoreAdminHandler) w/ properties
-    // Just to be sure its NOT written to the core.properties file
+    // Just to be sure it's NOT written to the core.properties file
     File workDir = new File(solrHomeDirectory, coreSysProps);
     System.setProperty("INSTDIR_TEST", workDir.getAbsolutePath());
     System.setProperty("CONFIG_TEST", "solrconfig_ren.xml");
@@ -227,7 +227,7 @@
     setupCore(coreNormal, true);
 
     // create a new core (using CoreAdminHandler) w/ properties
-    // Just to be sure its NOT written to the core.properties file
+    // Just to be sure it's NOT written to the core.properties file
     File workDir = new File(solrHomeDirectory, coreNormal);
     File data = new File(workDir, "data");
 
Index: solr/core/src/test/org/apache/solr/handler/component/DebugComponentTest.java
===================================================================
--- solr/core/src/test/org/apache/solr/handler/component/DebugComponentTest.java	(revision 1647726)
+++ solr/core/src/test/org/apache/solr/handler/component/DebugComponentTest.java	(working copy)
@@ -61,7 +61,7 @@
             "//lst[@name='explain']/str[@name='3']",
             "//str[@name='QParser']",// make sure the QParser is specified
             "count(//lst[@name='timing']/*)=3", //should be three pieces to timings
-            "//lst[@name='timing']/double[@name='time']", //make sure we have a time value, but don't specify it's result
+            "//lst[@name='timing']/double[@name='time']", //make sure we have a time value, but don't specify its result
             "count(//lst[@name='prepare']/*)>0",
             "//lst[@name='prepare']/double[@name='time']",
             "count(//lst[@name='process']/*)>0",
@@ -85,7 +85,7 @@
             "//lst[@name='explain']/str[@name='2']",
             "//lst[@name='explain']/str[@name='3']",
             "count(//lst[@name='timing']/*)=3", //should be three pieces to timings
-            "//lst[@name='timing']/double[@name='time']", //make sure we have a time value, but don't specify it's result
+            "//lst[@name='timing']/double[@name='time']", //make sure we have a time value, but don't specify its result
             "count(//lst[@name='prepare']/*)>0",
             "//lst[@name='prepare']/double[@name='time']",
             "count(//lst[@name='process']/*)>0",
@@ -100,7 +100,7 @@
             "count(//lst[@name='explain']/*)=0",
             "count(//str[@name='QParser'])=0",// make sure the QParser is specified
             "count(//lst[@name='timing']/*)=3", //should be three pieces to timings
-            "//lst[@name='timing']/double[@name='time']", //make sure we have a time value, but don't specify it's result
+            "//lst[@name='timing']/double[@name='time']", //make sure we have a time value, but don't specify its result
             "count(//lst[@name='prepare']/*)>0",
             "//lst[@name='prepare']/double[@name='time']",
             "count(//lst[@name='process']/*)>0",
Index: solr/core/src/test/org/apache/solr/handler/component/DistributedFacetPivotLargeTest.java
===================================================================
--- solr/core/src/test/org/apache/solr/handler/component/DistributedFacetPivotLargeTest.java	(revision 1647726)
+++ solr/core/src/test/org/apache/solr/handler/component/DistributedFacetPivotLargeTest.java	(working copy)
@@ -178,7 +178,7 @@
     //
     // This is tricky, here's what i think is happening.... 
     // - "company:honda" only exists on twoShard, and only w/ "place:cardiff"
-    // - twoShard has no other places in it's docs
+    // - twoShard has no other places in its docs
     // - twoShard can't return any other places to w/ honda as a count=0 sub-value
     // - if we refined all other companies places, would twoShard return honda==0 ?
     //   ... but there's no refinement since mincount==0
Index: solr/core/src/test/org/apache/solr/internal/csv/CSVParserTest.java
===================================================================
--- solr/core/src/test/org/apache/solr/internal/csv/CSVParserTest.java	(revision 1647726)
+++ solr/core/src/test/org/apache/solr/internal/csv/CSVParserTest.java	(working copy)
@@ -29,7 +29,7 @@
  * The test are organized in three different sections:
  * The 'setter/getter' section, the lexer section and finally the parser 
  * section. In case a test fails, you should follow a top-down approach for 
- * fixing a potential bug (its likely that the parser itself fails if the lexer
+ * fixing a potential bug (it's likely that the parser itself fails if the lexer
  * has problems...).
  */
 public class CSVParserTest extends TestCase {
Index: solr/core/src/test/org/apache/solr/internal/csv/CSVStrategyTest.java
===================================================================
--- solr/core/src/test/org/apache/solr/internal/csv/CSVStrategyTest.java	(revision 1647726)
+++ solr/core/src/test/org/apache/solr/internal/csv/CSVStrategyTest.java	(working copy)
@@ -26,7 +26,7 @@
  * The test are organized in three different sections:
  * The 'setter/getter' section, the lexer section and finally the strategy 
  * section. In case a test fails, you should follow a top-down approach for 
- * fixing a potential bug (its likely that the strategy itself fails if the lexer
+ * fixing a potential bug (it's likely that the strategy itself fails if the lexer
  * has problems...).
  */
 public class CSVStrategyTest extends TestCase {
Index: solr/core/src/test/org/apache/solr/response/TestRawResponseWriter.java
===================================================================
--- solr/core/src/test/org/apache/solr/response/TestRawResponseWriter.java	(revision 1647726)
+++ solr/core/src/test/org/apache/solr/response/TestRawResponseWriter.java	(working copy)
@@ -50,8 +50,8 @@
 
   @BeforeClass
   public static void setupCoreAndWriters() throws Exception {
-    // we don't directly use this core or it's config, we use
-    // QueryResponseWriters' constructed programaticly,
+    // we don't directly use this core or its config, we use
+    // QueryResponseWriters' constructed programmatically,
     // but we do use this core for managing the life cycle of the requests
     // we spin up.
     initCore("solrconfig.xml","schema.xml");
Index: solr/core/src/test/org/apache/solr/schema/TestCollationField.java
===================================================================
--- solr/core/src/test/org/apache/solr/schema/TestCollationField.java	(revision 1647726)
+++ solr/core/src/test/org/apache/solr/schema/TestCollationField.java	(working copy)
@@ -59,7 +59,7 @@
    * Ugly: but what to do? We want to test custom sort, which reads rules in as a resource.
    * These are largish files, and jvm-specific (as our documentation says, you should always
    * look out for jvm differences with collation).
-   * So its preferable to create this file on-the-fly.
+   * So it's preferable to create this file on-the-fly.
    */
   public static String setupSolrHome() throws Exception {
     // make a solr home underneath the test's TEMP_DIR
Index: solr/core/src/test/org/apache/solr/schema/TestCollationFieldDocValues.java
===================================================================
--- solr/core/src/test/org/apache/solr/schema/TestCollationFieldDocValues.java	(revision 1647726)
+++ solr/core/src/test/org/apache/solr/schema/TestCollationFieldDocValues.java	(working copy)
@@ -57,7 +57,7 @@
    * Ugly: but what to do? We want to test custom sort, which reads rules in as a resource.
    * These are largish files, and jvm-specific (as our documentation says, you should always
    * look out for jvm differences with collation).
-   * So its preferable to create this file on-the-fly.
+   * So it's preferable to create this file on-the-fly.
    */
   public static String setupSolrHome() throws Exception {
     // make a solr home underneath the test's TEMP_DIR
Index: solr/core/src/test/org/apache/solr/search/TestStressLucene.java
===================================================================
--- solr/core/src/test/org/apache/solr/search/TestStressLucene.java	(revision 1647726)
+++ solr/core/src/test/org/apache/solr/search/TestStressLucene.java	(working copy)
@@ -317,7 +317,7 @@
               int docid = getFirstMatch(r, new Term("id",Integer.toString(id)));
 
               if (docid < 0 && tombstones) {
-                // if we couldn't find the doc, look for it's tombstone
+                // if we couldn't find the doc, look for its tombstone
                 docid = getFirstMatch(r, new Term("id","-"+Integer.toString(id)));
                 if (docid < 0) {
                   if (val == -1L) {
@@ -336,7 +336,7 @@
                 if (docid < 0) {
                   verbose("ERROR: Couldn't find a doc for id", id, "using reader",r);
                 }
-                assertTrue(docid >= 0);   // we should have found the document, or it's tombstone
+                assertTrue(docid >= 0);   // we should have found the document, or its tombstone
                 StoredDocument doc = r.document(docid);
                 long foundVal = Long.parseLong(doc.get(field));
                 if (foundVal < Math.abs(val)) {
Index: solr/core/src/test/org/apache/solr/update/DocumentBuilderTest.java
===================================================================
--- solr/core/src/test/org/apache/solr/update/DocumentBuilderTest.java	(revision 1647726)
+++ solr/core/src/test/org/apache/solr/update/DocumentBuilderTest.java	(working copy)
@@ -210,7 +210,7 @@
   }
   
   /**
-   * Its ok to boost a field if it has norms
+   * It's ok to boost a field if it has norms
    */
   public void testBoost() throws Exception {
     XmlDoc xml = new XmlDoc();
@@ -385,7 +385,7 @@
   }
   
   /**
-   * Its ok to supply a document boost even if a field omits norms
+   * It's ok to supply a document boost even if a field omits norms
    */
   public void testDocumentBoostOmitNorms() throws Exception {
     XmlDoc xml = new XmlDoc();
Index: solr/core/src/test/org/apache/solr/update/PeerSyncTest.java
===================================================================
--- solr/core/src/test/org/apache/solr/update/PeerSyncTest.java	(revision 1647726)
+++ solr/core/src/test/org/apache/solr/update/PeerSyncTest.java	(working copy)
@@ -65,7 +65,7 @@
     long v = 0;
     add(client0, seenLeader, sdoc("id","1","_version_",++v));
 
-    // this fails because client0 has no context (i.e. no updates of it's own to judge if applying the updates
+    // this fails because client0 has no context (i.e. no updates of its own to judge if applying the updates
     // from client1 will bring it into sync with client1)
     assertSync(client1, numVersions, false, shardsArr[0]);
 
Index: solr/core/src/test-files/solr/collection1/conf/solrconfig-externalversionconstraint.xml
===================================================================
Cannot display: file marked as a binary type.
svn:mime-type = application/xml
Index: solr/core/src/test-files/solr/collection1/conf/solrconfig.xml
===================================================================
--- solr/core/src/test-files/solr/collection1/conf/solrconfig.xml	(revision 1647726)
+++ solr/core/src/test-files/solr/collection1/conf/solrconfig.xml	(working copy)
@@ -587,7 +587,7 @@
   <restManager>
     <!-- 
     IMPORTANT: Due to the Lucene SecurityManager, tests can only write to their runtime directory or below.
-    But its easier to just keep everything in memory for testing so no remnants are left behind.
+    But it's easier to just keep everything in memory for testing so no remnants are left behind.
     -->
     <str name="storageIO">org.apache.solr.rest.ManagedResourceStorage$InMemoryStorageIO</str>
   </restManager>
Index: solr/example/example-DIH/solr/db/conf/solrconfig.xml
===================================================================
--- solr/example/example-DIH/solr/db/conf/solrconfig.xml	(revision 1647726)
+++ solr/example/example-DIH/solr/db/conf/solrconfig.xml	(working copy)
@@ -142,7 +142,7 @@
        index format, but hooks into the schema to provide per-field customization of
        the postings lists and per-document values in the fieldType element
        (postingsFormat/docValuesFormat). Note that most of the alternative implementations
-       are experimental, so if you choose to customize the index format, its a good
+       are experimental, so if you choose to customize the index format, it's a good
        idea to convert back to the official format e.g. via IndexWriter.addIndexes(IndexReader)
        before upgrading to a newer version to avoid unnecessary reindexing.
   -->
Index: solr/example/example-DIH/solr/mail/conf/solrconfig.xml
===================================================================
--- solr/example/example-DIH/solr/mail/conf/solrconfig.xml	(revision 1647726)
+++ solr/example/example-DIH/solr/mail/conf/solrconfig.xml	(working copy)
@@ -145,7 +145,7 @@
        index format, but hooks into the schema to provide per-field customization of
        the postings lists and per-document values in the fieldType element
        (postingsFormat/docValuesFormat). Note that most of the alternative implementations
-       are experimental, so if you choose to customize the index format, its a good
+       are experimental, so if you choose to customize the index format, it's a good
        idea to convert back to the official format e.g. via IndexWriter.addIndexes(IndexReader)
        before upgrading to a newer version to avoid unnecessary reindexing.
   -->
Index: solr/example/example-DIH/solr/rss/conf/solrconfig.xml
===================================================================
--- solr/example/example-DIH/solr/rss/conf/solrconfig.xml	(revision 1647726)
+++ solr/example/example-DIH/solr/rss/conf/solrconfig.xml	(working copy)
@@ -142,7 +142,7 @@
        index format, but hooks into the schema to provide per-field customization of
        the postings lists and per-document values in the fieldType element
        (postingsFormat/docValuesFormat). Note that most of the alternative implementations
-       are experimental, so if you choose to customize the index format, its a good
+       are experimental, so if you choose to customize the index format, it's a good
        idea to convert back to the official format e.g. via IndexWriter.addIndexes(IndexReader)
        before upgrading to a newer version to avoid unnecessary reindexing.
   -->
Index: solr/example/example-DIH/solr/solr/conf/solrconfig.xml
===================================================================
--- solr/example/example-DIH/solr/solr/conf/solrconfig.xml	(revision 1647726)
+++ solr/example/example-DIH/solr/solr/conf/solrconfig.xml	(working copy)
@@ -142,7 +142,7 @@
        index format, but hooks into the schema to provide per-field customization of
        the postings lists and per-document values in the fieldType element
        (postingsFormat/docValuesFormat). Note that most of the alternative implementations
-       are experimental, so if you choose to customize the index format, its a good
+       are experimental, so if you choose to customize the index format, it's a good
        idea to convert back to the official format e.g. via IndexWriter.addIndexes(IndexReader)
        before upgrading to a newer version to avoid unnecessary reindexing.
   -->
Index: solr/example/example-DIH/solr/tika/conf/solrconfig.xml
===================================================================
--- solr/example/example-DIH/solr/tika/conf/solrconfig.xml	(revision 1647726)
+++ solr/example/example-DIH/solr/tika/conf/solrconfig.xml	(working copy)
@@ -143,7 +143,7 @@
        index format, but hooks into the schema to provide per-field customization of
        the postings lists and per-document values in the fieldType element
        (postingsFormat/docValuesFormat). Note that most of the alternative implementations
-       are experimental, so if you choose to customize the index format, its a good
+       are experimental, so if you choose to customize the index format, it's a good
        idea to convert back to the official format e.g. via IndexWriter.addIndexes(IndexReader)
        before upgrading to a newer version to avoid unnecessary reindexing.
   -->
Index: solr/server/etc/webdefault.xml
===================================================================
--- solr/server/etc/webdefault.xml	(revision 1647726)
+++ solr/server/etc/webdefault.xml	(working copy)
@@ -27,7 +27,7 @@
 
   <description>
     Default web.xml file.  
-    This file is applied to a Web application before it's own WEB_INF/web.xml file
+    This file is applied to a Web application before its own WEB_INF/web.xml file
   </description>
 
   <!-- ==================================================================== -->
Index: solr/server/solr/configsets/basic_configs/conf/solrconfig.xml
===================================================================
--- solr/server/solr/configsets/basic_configs/conf/solrconfig.xml	(revision 1647726)
+++ solr/server/solr/configsets/basic_configs/conf/solrconfig.xml	(working copy)
@@ -70,7 +70,7 @@
        index format, but hooks into the schema to provide per-field customization of
        the postings lists and per-document values in the fieldType element
        (postingsFormat/docValuesFormat). Note that most of the alternative implementations
-       are experimental, so if you choose to customize the index format, its a good
+       are experimental, so if you choose to customize the index format, it's a good
        idea to convert back to the official format e.g. via IndexWriter.addIndexes(IndexReader)
        before upgrading to a newer version to avoid unnecessary reindexing.
   -->
Index: solr/server/solr/configsets/data_driven_schema_configs/conf/solrconfig.xml
===================================================================
--- solr/server/solr/configsets/data_driven_schema_configs/conf/solrconfig.xml	(revision 1647726)
+++ solr/server/solr/configsets/data_driven_schema_configs/conf/solrconfig.xml	(working copy)
@@ -123,7 +123,7 @@
        index format, but hooks into the schema to provide per-field customization of
        the postings lists and per-document values in the fieldType element
        (postingsFormat/docValuesFormat). Note that most of the alternative implementations
-       are experimental, so if you choose to customize the index format, its a good
+       are experimental, so if you choose to customize the index format, it's a good
        idea to convert back to the official format e.g. via IndexWriter.addIndexes(IndexReader)
        before upgrading to a newer version to avoid unnecessary reindexing.
   -->
Index: solr/server/solr/configsets/sample_techproducts_configs/conf/solrconfig.xml
===================================================================
--- solr/server/solr/configsets/sample_techproducts_configs/conf/solrconfig.xml	(revision 1647726)
+++ solr/server/solr/configsets/sample_techproducts_configs/conf/solrconfig.xml	(working copy)
@@ -140,7 +140,7 @@
        index format, but hooks into the schema to provide per-field customization of
        the postings lists and per-document values in the fieldType element
        (postingsFormat/docValuesFormat). Note that most of the alternative implementations
-       are experimental, so if you choose to customize the index format, its a good
+       are experimental, so if you choose to customize the index format, it's a good
        idea to convert back to the official format e.g. via IndexWriter.addIndexes(IndexReader)
        before upgrading to a newer version to avoid unnecessary reindexing.
   -->
Index: solr/solrj/build.xml
===================================================================
--- solr/solrj/build.xml	(revision 1647726)
+++ solr/solrj/build.xml	(working copy)
@@ -66,7 +66,7 @@
 
   <!-- Specialized to use lucene's classpath too, because it refs e.g. qp syntax 
        (even though it doesnt compile with it) 
-       TODO: would be nice to fix this up better, but its hard because of
+       TODO: would be nice to fix this up better, but it's hard because of
        the different ways solr links to lucene javadocs -->
   <target name="-ecj-javadoc-lint-src" depends="-ecj-resolve">
     <ecj-macro srcdir="${src.dir}" configuration="${common.dir}/tools/javadoc/ecj.javadocs.prefs">
Index: solr/solrj/src/java/org/apache/solr/client/solrj/ResponseParser.java
===================================================================
--- solr/solrj/src/java/org/apache/solr/client/solrj/ResponseParser.java	(revision 1647726)
+++ solr/solrj/src/java/org/apache/solr/client/solrj/ResponseParser.java	(working copy)
@@ -35,7 +35,7 @@
   public abstract NamedList<Object> processResponse(Reader reader);
   
   /**
-   * A well behaved ResponseParser will return it's content-type.
+   * A well behaved ResponseParser will return its content-type.
    * 
    * @return the content-type this parser expects to parse
    */
Index: solr/solrj/src/java/org/apache/solr/client/solrj/SolrQuery.java
===================================================================
--- solr/solrj/src/java/org/apache/solr/client/solrj/SolrQuery.java	(revision 1647726)
+++ solr/solrj/src/java/org/apache/solr/client/solrj/SolrQuery.java	(working copy)
@@ -730,7 +730,7 @@
   /**
    * Updates or adds a single sort field specification to the current sort
    * information. If the sort field already exist in the sort information map,
-   * it's position is unchanged and the sort order is set; if it does not exist,
+   * its position is unchanged and the sort order is set; if it does not exist,
    * it is appended at the end with the specified order..
    *
    * @return the modified SolrQuery object, for easy chaining
Index: solr/solrj/src/java/org/apache/solr/common/cloud/ZkCmdExecutor.java
===================================================================
--- solr/solrj/src/java/org/apache/solr/common/cloud/ZkCmdExecutor.java	(revision 1647726)
+++ solr/solrj/src/java/org/apache/solr/common/cloud/ZkCmdExecutor.java	(working copy)
@@ -29,7 +29,7 @@
   
   /**
    * TODO: At this point, this should probably take a SolrZkClient in
-   * it's constructor.
+   * its constructor.
    * 
    * @param timeoutms
    *          the client timeout for the ZooKeeper clients that will be used
@@ -93,7 +93,7 @@
     try {
       zkClient.makePath(path, data, true);
     } catch (NodeExistsException e) {
-      // its okay if another beats us creating the node
+      // it's okay if another beats us creating the node
     }
     
   }
Index: solr/solrj/src/java/org/apache/solr/common/params/RequiredSolrParams.java
===================================================================
--- solr/solrj/src/java/org/apache/solr/common/params/RequiredSolrParams.java	(revision 1647726)
+++ solr/solrj/src/java/org/apache/solr/common/params/RequiredSolrParams.java	(working copy)
@@ -106,7 +106,7 @@
 
   //----------------------------------------------------------
   // Functions with a default value - pass directly to the
-  // wrapped SolrParams (they won't return null - unless its the default)
+  // wrapped SolrParams (they won't return null - unless it's the default)
   //----------------------------------------------------------
 
   @Override
Index: solr/solrj/src/java/org/apache/solr/common/util/Hash.java
===================================================================
--- solr/solrj/src/java/org/apache/solr/common/util/Hash.java	(revision 1647726)
+++ solr/solrj/src/java/org/apache/solr/common/util/Hash.java	(working copy)
@@ -118,7 +118,7 @@
 
   /**
    * <p>The hash value of a character sequence is defined to be the hash of
-   * it's unicode code points, according to {@link #lookup3ycs(int[] k, int offset, int length, int initval)}
+   * its unicode code points, according to {@link #lookup3ycs(int[] k, int offset, int length, int initval)}
    * </p>
    * <p>If you know the number of code points in the {@code CharSequence}, you can
    * generate the same hash as the original lookup3
Index: solr/solrj/src/java/org/apache/solr/common/util/NamedList.java
===================================================================
--- solr/solrj/src/java/org/apache/solr/common/util/NamedList.java	(revision 1647726)
+++ solr/solrj/src/java/org/apache/solr/common/util/NamedList.java	(working copy)
@@ -455,7 +455,7 @@
   }
 
   /**
-   * Iterates over the Map and sequentially adds it's key/value pairs
+   * Iterates over the Map and sequentially adds its key/value pairs
    */
   public boolean addAll(Map<String,T> args) {
     for (Map.Entry<String, T> entry : args.entrySet() ) {
Index: solr/solrj/src/java/org/apache/solr/common/util/SimpleOrderedMap.java
===================================================================
--- solr/solrj/src/java/org/apache/solr/common/util/SimpleOrderedMap.java	(revision 1647726)
+++ solr/solrj/src/java/org/apache/solr/common/util/SimpleOrderedMap.java	(working copy)
@@ -32,7 +32,7 @@
  * the same way.
  * </p>
  * <p>
- * This class does not provide efficient lookup by key, it's main purpose is
+ * This class does not provide efficient lookup by key, its main purpose is
  * to hold data to be serialized.  It aims to minimize overhead and to be
  * efficient at adding new elements.
  * </p>
Index: solr/test-framework/build.xml
===================================================================
--- solr/test-framework/build.xml	(revision 1647726)
+++ solr/test-framework/build.xml	(working copy)
@@ -46,7 +46,7 @@
   <!-- redefine the clover setup, because we dont want to run clover for the test-framework -->
   <target name="-clover.setup" if="run.clover"/>
 
-  <!-- redefine the test compilation, so its just a no-op -->
+  <!-- redefine the test compilation, so it's just a no-op -->
   <target name="compile-test"/>
   
   <!-- redefine the forbidden apis for tests, as we check ourselves -->
Index: solr/test-framework/src/java/org/apache/solr/SolrTestCaseJ4.java
===================================================================
--- solr/test-framework/src/java/org/apache/solr/SolrTestCaseJ4.java	(revision 1647726)
+++ solr/test-framework/src/java/org/apache/solr/SolrTestCaseJ4.java	(working copy)
@@ -440,7 +440,7 @@
      if (endNumOpens-numOpens != endNumCloses-numCloses) {
        String msg = "ERROR: SolrIndexSearcher opens=" + (endNumOpens-numOpens) + " closes=" + (endNumCloses-numCloses);
        log.error(msg);
-       // if its TestReplicationHandler, ignore it. the test is broken and gets no love
+       // if it's TestReplicationHandler, ignore it. the test is broken and gets no love
        if ("TestReplicationHandler".equals(RandomizedContext.current().getTargetClass().getSimpleName())) {
          log.warn("TestReplicationHandler wants to fail!: " + msg);
        } else {
Index: solr/test-framework/src/java/org/apache/solr/util/TestHarness.java
===================================================================
--- solr/test-framework/src/java/org/apache/solr/util/TestHarness.java	(revision 1647726)
+++ solr/test-framework/src/java/org/apache/solr/util/TestHarness.java	(working copy)
@@ -200,7 +200,7 @@
     return container;
   }
 
-  /** Gets a core that does not have it's refcount incremented (i.e. there is no need to
+  /** Gets a core that does not have its refcount incremented (i.e. there is no need to
    * close when done).  This is not MT safe in conjunction with reloads!
    */
   public SolrCore getCore() {
@@ -212,7 +212,7 @@
     return core;
   }
 
-  /** Gets the core with it's reference count incremented.
+  /** Gets the core with its reference count incremented.
    * You must call core.close() when done!
    */
   public SolrCore getCoreInc() {
Index: solr/webapp/web/js/lib/jquery.ajaxfileupload.js
===================================================================
--- solr/webapp/web/js/lib/jquery.ajaxfileupload.js	(revision 1647726)
+++ solr/webapp/web/js/lib/jquery.ajaxfileupload.js	(working copy)
@@ -128,7 +128,7 @@
           wrapElement($element);
 
           // Call user-supplied (or default) onStart(), setting
-          //  it's this context to the file DOM element
+          //  its this context to the file DOM element
           var ret = settings.onStart.apply($element, [settings.params]);
 
           // let onStart have the option to cancel the upload
Index: solr/webapp/web/js/lib/jquery.sammy.js
===================================================================
--- solr/webapp/web/js/lib/jquery.sammy.js	(revision 1647726)
+++ solr/webapp/web/js/lib/jquery.sammy.js	(working copy)
@@ -48,7 +48,7 @@
 
 
   // `Sammy` (also aliased as $.sammy) is not only the namespace for a
-  // number of prototypes, its also a top level method that allows for easy
+  // number of prototypes, it's also a top level method that allows for easy
   // creation/management of `Sammy.Application` instances. There are a
   // number of different forms for `Sammy()` but each returns an instance
   // of `Sammy.Application`. When a new instance is created using
@@ -1446,7 +1446,7 @@
         }
         if (callback) { this.then(callback); }
         if (typeof location === 'string') {
-          // its a path
+          // it's a path
           is_json      = (location.match(/\.json$/) || options.json);
           should_cache = ((is_json && options.cache === true) || options.cache !== false);
           context.next_engine = context.event_context.engineFor(location);
@@ -1474,12 +1474,12 @@
           }, options));
           return false;
         } else {
-          // its a dom/jQuery
+          // it's a dom/jQuery
           if (location.nodeType) {
             return location.innerHTML;
           }
           if (location.selector) {
-            // its a jQuery
+            // it's a jQuery
             context.next_engine = location.attr('data-engine');
             if (options.clone === false) {
               return location.remove()[0].innerHTML.toString();
Index: solr/webapp/web/js/require.js
===================================================================
--- solr/webapp/web/js/require.js	(revision 1647726)
+++ solr/webapp/web/js/require.js	(working copy)
@@ -4618,7 +4618,7 @@
 					jQuery.error( "type property can't be changed" );
 				} else if ( !jQuery.support.radioValue && value === "radio" && jQuery.nodeName(elem, "input") ) {
 					// Setting the type on a radio button after the value resets the value in IE6-9
-					// Reset value to it's default in case type is set after value
+					// Reset value to its default in case type is set after value
 					// This is for element creation
 					var val = elem.value;
 					elem.setAttribute( "type", value );
@@ -6277,7 +6277,7 @@
 				// Replace IE's carriage returns
 				return elem.innerText.replace( rReturn, '' );
 			} else {
-				// Traverse it's children
+				// Traverse its children
 				for ( elem = elem.firstChild; elem; elem = elem.nextSibling) {
 					ret += getText( elem );
 				}
