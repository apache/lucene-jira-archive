Index: src/test/org/apache/lucene/index/TestDoc.java
===================================================================
--- src/test/org/apache/lucene/index/TestDoc.java	(revision 824874)
+++ src/test/org/apache/lucene/index/TestDoc.java	(working copy)
@@ -169,15 +169,15 @@
       Document doc = new Document();
       doc.add(new Field("contents", new FileReader(file)));
       writer.addDocument(doc);
-      writer.flush();
+      writer.commit();
       return writer.newestSegment();
    }
 
 
    private SegmentInfo merge(SegmentInfo si1, SegmentInfo si2, String merged, boolean useCompoundFile)
    throws Exception {
-      SegmentReader r1 = SegmentReader.get(si1);
-      SegmentReader r2 = SegmentReader.get(si2);
+      SegmentReader r1 = SegmentReader.get(true, si1, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR);
+      SegmentReader r2 = SegmentReader.get(true, si2, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR);
 
       SegmentMerger merger = new SegmentMerger(si1.dir, merged);
 
@@ -198,7 +198,7 @@
 
    private void printSegment(PrintWriter out, SegmentInfo si)
    throws Exception {
-      SegmentReader reader = SegmentReader.get(si);
+      SegmentReader reader = SegmentReader.get(true, si, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR);
 
       for (int i = 0; i < reader.numDocs(); i++)
         out.println(reader.document(i));
Index: src/test/org/apache/lucene/index/TestTermVectorsReader.java
===================================================================
--- src/test/org/apache/lucene/index/TestTermVectorsReader.java	(revision 824874)
+++ src/test/org/apache/lucene/index/TestTermVectorsReader.java	(working copy)
@@ -111,7 +111,7 @@
     //terms
     for(int j=0;j<5;j++)
       writer.addDocument(doc);
-    writer.flush();
+    writer.commit();
     seg = writer.newestSegment().name;
     writer.close();
 
Index: src/test/org/apache/lucene/index/TestIndexWriter.java
===================================================================
--- src/test/org/apache/lucene/index/TestIndexWriter.java	(revision 824874)
+++ src/test/org/apache/lucene/index/TestIndexWriter.java	(working copy)
@@ -1133,7 +1133,7 @@
     public void testIndexNoDocuments() throws IOException {
       RAMDirectory dir = new RAMDirectory();      
       IndexWriter writer  = new IndexWriter(dir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
-      writer.flush();
+      writer.commit();
       writer.close();
 
       IndexReader reader = IndexReader.open(dir, true);
@@ -1142,7 +1142,7 @@
       reader.close();
 
       writer  = new IndexWriter(dir, new WhitespaceAnalyzer(), false, IndexWriter.MaxFieldLength.LIMITED);
-      writer.flush();
+      writer.commit();
       writer.close();
 
       reader = IndexReader.open(dir, true);
@@ -1503,7 +1503,7 @@
       Document doc = new Document();
       doc.add(new Field("field", "aaa", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));
       writer.addDocument(doc);
-      writer.flush();
+      writer.commit();
       writer.addDocument(new Document());
       writer.close();
       _TestUtil.checkIndex(dir);
@@ -1595,13 +1595,13 @@
                            Field.TermVector.NO));
     iw.addDocument(document);
     // Make first segment
-    iw.flush();
+    iw.commit();
 
     document.add(new Field("tvtest", "a b c", Field.Store.NO, Field.Index.ANALYZED,
         Field.TermVector.YES));
     iw.addDocument(document);
     // Make 2nd segment
-    iw.flush();
+    iw.commit();
 
     iw.optimize();
     iw.close();
@@ -1616,14 +1616,14 @@
     document.add(new Field("tvtest", "a b c", Field.Store.NO, Field.Index.ANALYZED,
         Field.TermVector.YES));
     iw.addDocument(document);
-    iw.flush();
+    iw.commit();
 
     document = new Document();
     document.add(new Field("tvtest", "x y z", Field.Store.NO, Field.Index.ANALYZED,
                            Field.TermVector.NO));
     iw.addDocument(document);
     // Make first segment
-    iw.flush();
+    iw.commit();
 
     iw.optimize();
 
@@ -1631,7 +1631,7 @@
         Field.TermVector.YES));
     iw.addDocument(document);
     // Make 2nd segment
-    iw.flush();
+    iw.commit();
     iw.optimize();
 
     iw.close();
Index: src/test/org/apache/lucene/index/TestMultiLevelSkipList.java
===================================================================
--- src/test/org/apache/lucene/index/TestMultiLevelSkipList.java	(revision 824874)
+++ src/test/org/apache/lucene/index/TestMultiLevelSkipList.java	(working copy)
@@ -52,7 +52,7 @@
       d1.add(new Field(term.field(), term.text(), Store.NO, Index.ANALYZED));
       writer.addDocument(d1);
     }
-    writer.flush();
+    writer.commit();
     writer.optimize();
     writer.close();
 
Index: src/test/org/apache/lucene/index/TestDocumentWriter.java
===================================================================
--- src/test/org/apache/lucene/index/TestDocumentWriter.java	(revision 824874)
+++ src/test/org/apache/lucene/index/TestDocumentWriter.java	(working copy)
@@ -63,11 +63,11 @@
     Analyzer analyzer = new WhitespaceAnalyzer();
     IndexWriter writer = new IndexWriter(dir, analyzer, true, IndexWriter.MaxFieldLength.LIMITED);
     writer.addDocument(testDoc);
-    writer.flush();
+    writer.commit();
     SegmentInfo info = writer.newestSegment();
     writer.close();
     //After adding the document, we should be able to read it back in
-    SegmentReader reader = SegmentReader.get(info);
+    SegmentReader reader = SegmentReader.get(true, info, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR);
     assertTrue(reader != null);
     Document doc = reader.document(0);
     assertTrue(doc != null);
@@ -123,10 +123,10 @@
     doc.add(new Field("repeated", "repeated two", Field.Store.YES, Field.Index.ANALYZED));
 
     writer.addDocument(doc);
-    writer.flush();
+    writer.commit();
     SegmentInfo info = writer.newestSegment();
     writer.close();
-    SegmentReader reader = SegmentReader.get(info);
+    SegmentReader reader = SegmentReader.get(true, info, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR);
 
     TermPositions termPositions = reader.termPositions(new Term("repeated", "repeated"));
     assertTrue(termPositions.next());
@@ -183,10 +183,10 @@
     doc.add(new Field("f1", "a 5 a a", Field.Store.YES, Field.Index.ANALYZED));
 
     writer.addDocument(doc);
-    writer.flush();
+    writer.commit();
     SegmentInfo info = writer.newestSegment();
     writer.close();
-    SegmentReader reader = SegmentReader.get(info);
+    SegmentReader reader = SegmentReader.get(true, info, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR);
 
     TermPositions termPositions = reader.termPositions(new Term("f1", "a"));
     assertTrue(termPositions.next());
@@ -223,10 +223,10 @@
     }, TermVector.NO));
     
     writer.addDocument(doc);
-    writer.flush();
+    writer.commit();
     SegmentInfo info = writer.newestSegment();
     writer.close();
-    SegmentReader reader = SegmentReader.get(info);
+    SegmentReader reader = SegmentReader.get(true, info, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR);
 
     TermPositions termPositions = reader.termPositions(new Term("preanalyzed", "term1"));
     assertTrue(termPositions.next());
Index: src/test/org/apache/lucene/index/TestConcurrentMergeScheduler.java
===================================================================
--- src/test/org/apache/lucene/index/TestConcurrentMergeScheduler.java	(revision 824874)
+++ src/test/org/apache/lucene/index/TestConcurrentMergeScheduler.java	(working copy)
@@ -88,7 +88,7 @@
         writer.addDocument(doc);
         failure.setDoFail();
         try {
-          writer.flush();
+          writer.flush(true, false, true);
           if (failure.hitExc) {
             fail("failed to hit IOException");
           }
@@ -140,7 +140,7 @@
         delID += 10;
       }
 
-      writer.flush();
+      writer.commit();
     }
 
     writer.close();
@@ -210,7 +210,7 @@
       // stress out aborting them on close:
       writer.setMergeFactor(3);
       writer.addDocument(doc);
-      writer.flush();
+      writer.commit();
 
       writer.close(false);
 
Index: src/test/org/apache/lucene/index/DocHelper.java
===================================================================
--- src/test/org/apache/lucene/index/DocHelper.java	(revision 824874)
+++ src/test/org/apache/lucene/index/DocHelper.java	(working copy)
@@ -238,7 +238,7 @@
     writer.setSimilarity(similarity);
     //writer.setUseCompoundFile(false);
     writer.addDocument(doc);
-    writer.flush();
+    writer.commit();
     SegmentInfo info = writer.newestSegment();
     writer.close();
     return info;
Index: src/test/org/apache/lucene/index/TestMultiReader.java
===================================================================
--- src/test/org/apache/lucene/index/TestMultiReader.java	(revision 824874)
+++ src/test/org/apache/lucene/index/TestMultiReader.java	(working copy)
@@ -28,8 +28,8 @@
     IndexReader reader;
 
     sis.read(dir);
-    SegmentReader reader1 = SegmentReader.get(sis.info(0));
-    SegmentReader reader2 = SegmentReader.get(sis.info(1));
+    SegmentReader reader1 = SegmentReader.get(false, sis.info(0), IndexReader.DEFAULT_TERMS_INDEX_DIVISOR);
+    SegmentReader reader2 = SegmentReader.get(false, sis.info(1), IndexReader.DEFAULT_TERMS_INDEX_DIVISOR);
     readers[0] = reader1;
     readers[1] = reader2;
     assertTrue(reader1 != null);
Index: src/test/org/apache/lucene/index/TestIndexWriterDelete.java
===================================================================
--- src/test/org/apache/lucene/index/TestIndexWriterDelete.java	(revision 824874)
+++ src/test/org/apache/lucene/index/TestIndexWriterDelete.java	(working copy)
@@ -152,7 +152,7 @@
 
       addDoc(modifier, ++id, value);
       assertEquals(0, modifier.getSegmentCount());
-      modifier.flush();
+      modifier.commit();
 
       modifier.commit();
 
Index: src/test/org/apache/lucene/index/TestIndexReaderReopen.java
===================================================================
--- src/test/org/apache/lucene/index/TestIndexReaderReopen.java	(revision 824874)
+++ src/test/org/apache/lucene/index/TestIndexReaderReopen.java	(working copy)
@@ -935,7 +935,7 @@
     for (int i = 0; i < 100; i++) {
       w.addDocument(createDocument(i, 4));
       if (multiSegment && (i % 10) == 0) {
-        w.flush();
+        w.commit();
       }
     }
     
Index: src/test/org/apache/lucene/index/TestSegmentMerger.java
===================================================================
--- src/test/org/apache/lucene/index/TestSegmentMerger.java	(revision 824874)
+++ src/test/org/apache/lucene/index/TestSegmentMerger.java	(working copy)
@@ -49,8 +49,8 @@
     SegmentInfo info1 = DocHelper.writeDoc(merge1Dir, doc1);
     DocHelper.setupDoc(doc2);
     SegmentInfo info2 = DocHelper.writeDoc(merge2Dir, doc2);
-    reader1 = SegmentReader.get(info1);
-    reader2 = SegmentReader.get(info2);
+    reader1 = SegmentReader.get(true, info1, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR);
+    reader2 = SegmentReader.get(true, info2, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR);
   }
 
   public void test() {
@@ -69,7 +69,7 @@
     merger.closeReaders();
     assertTrue(docsMerged == 2);
     //Should be able to open a new SegmentReader against the new directory
-    SegmentReader mergedReader = SegmentReader.get(new SegmentInfo(mergedSegment, docsMerged, mergedDir, false, true));
+    SegmentReader mergedReader = SegmentReader.get(true, new SegmentInfo(mergedSegment, docsMerged, mergedDir, false, true), IndexReader.DEFAULT_TERMS_INDEX_DIVISOR);
     assertTrue(mergedReader != null);
     assertTrue(mergedReader.numDocs() == 2);
     Document newDoc1 = mergedReader.document(0);
Index: src/test/org/apache/lucene/index/TestPayloads.java
===================================================================
--- src/test/org/apache/lucene/index/TestPayloads.java	(revision 824874)
+++ src/test/org/apache/lucene/index/TestPayloads.java	(working copy)
@@ -202,7 +202,7 @@
         }
         
         // make sure we create more than one segment to test merging
-        writer.flush();
+        writer.commit();
         
         // now we make sure to have different payload lengths next at the next skip point        
         for (int i = 0; i < numDocs; i++) {
Index: src/test/org/apache/lucene/index/TestSegmentReader.java
===================================================================
--- src/test/org/apache/lucene/index/TestSegmentReader.java	(revision 824874)
+++ src/test/org/apache/lucene/index/TestSegmentReader.java	(working copy)
@@ -43,7 +43,7 @@
     super.setUp();
     DocHelper.setupDoc(testDoc);
     SegmentInfo info = DocHelper.writeDoc(dir, testDoc);
-    reader = SegmentReader.get(info);
+    reader = SegmentReader.get(true, info, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR);
   }
 
   public void test() {
@@ -73,7 +73,7 @@
     Document docToDelete = new Document();
     DocHelper.setupDoc(docToDelete);
     SegmentInfo info = DocHelper.writeDoc(dir, docToDelete);
-    SegmentReader deleteReader = SegmentReader.get(info);
+    SegmentReader deleteReader = SegmentReader.get(false, info, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR);
     assertTrue(deleteReader != null);
     assertTrue(deleteReader.numDocs() == 1);
     deleteReader.deleteDocument(0);
@@ -167,14 +167,7 @@
           // test for fake norms of 1.0 or null depending on the flag
           byte [] norms = reader.norms(f.name());
           byte norm1 = DefaultSimilarity.encodeNorm(1.0f);
-          if (reader.getDisableFakeNorms())
-            assertNull(norms);
-          else {
-            assertEquals(norms.length,reader.maxDoc());
-            for (int j=0; j<reader.maxDoc(); j++) {
-              assertEquals(norms[j], norm1);
-            }
-          }
+          assertNull(norms);
           norms = new byte[reader.maxDoc()];
           reader.norms(f.name(),norms, 0);
           for (int j=0; j<reader.maxDoc(); j++) {
Index: src/java/org/apache/lucene/index/SegmentReader.java
===================================================================
--- src/java/org/apache/lucene/index/SegmentReader.java	(revision 824874)
+++ src/java/org/apache/lucene/index/SegmentReader.java	(working copy)
@@ -585,16 +585,7 @@
   /**
    * @throws CorruptIndexException if the index is corrupt
    * @throws IOException if there is a low-level IO error
-   * @deprecated
    */
-  public static SegmentReader get(SegmentInfo si) throws CorruptIndexException, IOException {
-    return get(false, si.dir, si, BufferedIndexInput.BUFFER_SIZE, true, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR);
-  }
-
-  /**
-   * @throws CorruptIndexException if the index is corrupt
-   * @throws IOException if there is a low-level IO error
-   */
   public static SegmentReader get(boolean readOnly, SegmentInfo si, int termInfosIndexDivisor) throws CorruptIndexException, IOException {
     return get(readOnly, si.dir, si, BufferedIndexInput.BUFFER_SIZE, true, termInfosIndexDivisor);
   }
@@ -602,16 +593,7 @@
   /**
    * @throws CorruptIndexException if the index is corrupt
    * @throws IOException if there is a low-level IO error
-   * @deprecated
    */
-  static SegmentReader get(SegmentInfo si, int readBufferSize, boolean doOpenStores, int termInfosIndexDivisor) throws CorruptIndexException, IOException {
-    return get(false, si.dir, si, readBufferSize, doOpenStores, termInfosIndexDivisor);
-  }
-
-  /**
-   * @throws CorruptIndexException if the index is corrupt
-   * @throws IOException if there is a low-level IO error
-   */
   public static SegmentReader get(boolean readOnly,
                                   Directory dir,
                                   SegmentInfo si,
@@ -780,7 +762,6 @@
         }
       }
 
-      clone.setDisableFakeNorms(getDisableFakeNorms());
       clone.norms = new HashMap();
 
       // Clone norms
@@ -1055,11 +1036,6 @@
   }
 
   private byte[] ones;
-  private byte[] fakeNorms() {
-    assert !getDisableFakeNorms();
-    if (ones==null) ones=createFakeNorms(maxDoc());
-    return ones;
-  }
 
   // can return null if norms aren't stored
   protected synchronized byte[] getNorms(String field) throws IOException {
@@ -1072,7 +1048,6 @@
   public synchronized byte[] norms(String field) throws IOException {
     ensureOpen();
     byte[] bytes = getNorms(field);
-    if (bytes==null && !getDisableFakeNorms()) bytes=fakeNorms();
     return bytes;
   }
 
Index: src/java/org/apache/lucene/index/SnapshotDeletionPolicy.java
===================================================================
--- src/java/org/apache/lucene/index/SnapshotDeletionPolicy.java	(revision 824874)
+++ src/java/org/apache/lucene/index/SnapshotDeletionPolicy.java	(working copy)
@@ -71,8 +71,7 @@
    *  you call optimize()) then in the worst case this could
    *  consume an extra 1X of your total index size, until
    *  you release the snapshot. */
-  // TODO 3.0: change this to return IndexCommit instead
-  public synchronized IndexCommitPoint snapshot() {
+  public synchronized IndexCommit snapshot() {
     if (snapshot == null)
       snapshot = lastCommit.getSegmentsFileName();
     else
Index: src/java/org/apache/lucene/index/CheckIndex.java
===================================================================
--- src/java/org/apache/lucene/index/CheckIndex.java	(revision 824874)
+++ src/java/org/apache/lucene/index/CheckIndex.java	(working copy)
@@ -289,23 +289,6 @@
     }
   }
 
-  /** Returns true if index is clean, else false. 
-   *  @deprecated Please instantiate a CheckIndex and then use {@link #checkIndex()} instead */
-  public static boolean check(Directory dir, boolean doFix) throws IOException {
-    return check(dir, doFix, null);
-  }
-
-  /** Returns true if index is clean, else false.
-   *  @deprecated Please instantiate a CheckIndex and then use {@link #checkIndex(List)} instead */
-  public static boolean check(Directory dir, boolean doFix, List onlySegments) throws IOException {
-    CheckIndex checker = new CheckIndex(dir);
-    Status status = checker.checkIndex(onlySegments);
-    if (doFix && !status.clean)
-      checker.fixIndex(status);
-
-    return status.clean;
-  }
-
   /** Returns a {@link Status} instance detailing
    *  the state of the index.
    *
@@ -488,7 +471,7 @@
         }
         if (infoStream != null)
           infoStream.print("    test: open reader.........");
-        reader = SegmentReader.get(info);
+        reader = SegmentReader.get(true, info, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR);
 
         segInfoStat.openReaderPassed = true;
 
Index: src/java/org/apache/lucene/index/MultiReader.java
===================================================================
--- src/java/org/apache/lucene/index/MultiReader.java	(revision 824874)
+++ src/java/org/apache/lucene/index/MultiReader.java	(working copy)
@@ -179,7 +179,6 @@
       }
       MultiReader mr = new MultiReader(newSubReaders);
       mr.decrefOnClose = newDecrefOnClose;
-      mr.setDisableFakeNorms(getDisableFakeNorms());
       return mr;
     } else {
       return this;
@@ -289,7 +288,7 @@
     if (bytes != null)
       return bytes;          // cache hit
     if (!hasNorms(field))
-      return getDisableFakeNorms() ? null : fakeNorms();
+      return null;
 
     bytes = new byte[maxDoc()];
     for (int i = 0; i < subReaders.length; i++)
Index: src/java/org/apache/lucene/index/IndexCommit.java
===================================================================
--- src/java/org/apache/lucene/index/IndexCommit.java	(revision 824874)
+++ src/java/org/apache/lucene/index/IndexCommit.java	(working copy)
@@ -41,7 +41,7 @@
  * may suddenly change. </p>
 */
 
-public abstract class IndexCommit implements IndexCommitPoint {
+public abstract class IndexCommit {
 
   /**
    * Get the segments file (<code>segments_N</code>) associated 
Index: src/java/org/apache/lucene/index/DirectoryReader.java
===================================================================
--- src/java/org/apache/lucene/index/DirectoryReader.java	(revision 824874)
+++ src/java/org/apache/lucene/index/DirectoryReader.java	(working copy)
@@ -17,25 +17,25 @@
  * limitations under the License.
  */
 
-import java.io.IOException;
 import java.io.FileNotFoundException;
+import java.io.IOException;
+import java.util.ArrayList;
 import java.util.Arrays;
 import java.util.Collection;
+import java.util.Collections;
 import java.util.HashMap;
 import java.util.HashSet;
 import java.util.Iterator;
 import java.util.Map;
 import java.util.Set;
-import java.util.Collections;
-import java.util.ArrayList;
 
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.FieldSelector;
 import org.apache.lucene.search.DefaultSimilarity;
+import org.apache.lucene.store.AlreadyClosedException;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.Lock;
 import org.apache.lucene.store.LockObtainFailedException;
-import org.apache.lucene.store.AlreadyClosedException;
 
 /** 
  * An IndexReader which reads indexes with multiple segments.
@@ -380,7 +380,6 @@
       // the future we could have write make some effort to
       // detect that no changes have occurred
       IndexReader reader = writer.getReader();
-      reader.setDisableFakeNorms(getDisableFakeNorms());
       return reader;
     }
 
@@ -436,7 +435,6 @@
     } else {
       reader = new DirectoryReader(directory, infos, subReaders, starts, normsCache, false, doClone, termInfosIndexDivisor);
     }
-    reader.setDisableFakeNorms(getDisableFakeNorms());
     return reader;
   }
 
@@ -564,18 +562,13 @@
   }
 
   private byte[] ones;
-  private byte[] fakeNorms() {
-    if (ones==null) ones=SegmentReader.createFakeNorms(maxDoc());
-    return ones;
-  }
-
   public synchronized byte[] norms(String field) throws IOException {
     ensureOpen();
     byte[] bytes = (byte[])normsCache.get(field);
     if (bytes != null)
       return bytes;          // cache hit
     if (!hasNorms(field))
-      return getDisableFakeNorms() ? null : fakeNorms();
+      return null;
 
     bytes = new byte[maxDoc()];
     for (int i = 0; i < subReaders.length; i++)
@@ -679,11 +672,6 @@
     }
   }
 
-  /** @deprecated  */
-  protected void doCommit() throws IOException {
-    doCommit(null);
-  }
-
   /**
    * Commit changes resulting from delete, undeleteAll, or setNorm operations
    * <p/>
@@ -832,12 +820,6 @@
     return subReaders;
   }
 
-  public void setDisableFakeNorms(boolean disableFakeNorms) {
-    super.setDisableFakeNorms(disableFakeNorms);
-    for (int i = 0; i < subReaders.length; i++)
-        subReaders[i].setDisableFakeNorms(disableFakeNorms);
-  }
-
   /** Returns the directory this index resides in. */
   public Directory directory() {
     // Don't ensureOpen here -- in certain cases, when a
Index: src/java/org/apache/lucene/index/IndexReader.java
===================================================================
--- src/java/org/apache/lucene/index/IndexReader.java	(revision 824874)
+++ src/java/org/apache/lucene/index/IndexReader.java	(working copy)
@@ -980,18 +980,9 @@
     hasChanges = false;
   }
 
-  /** Implements commit.
-   *  @deprecated Please implement {@link #doCommit(Map)
-   *  instead}. */
-  protected abstract void doCommit() throws IOException;
-
   /** Implements commit.  NOTE: subclasses should override
    *  this.  In 3.0 this will become an abstract method. */
-  void doCommit(Map commitUserData) throws IOException {
-    // Default impl discards commitUserData; all Lucene
-    // subclasses override this (do not discard it).
-    doCommit();
-  }
+  protected abstract void doCommit(Map commitUserData) throws IOException;
 
   /**
    * Closes files associated with this index.
@@ -1145,8 +1136,7 @@
     return null;
   }
 
-  /** Expert    
-   *  @deprecated */
+  /** Expert */
   public Object getFieldCacheKey() {
     return this;
   }
@@ -1166,26 +1156,4 @@
   public long getUniqueTermCount() throws IOException {
     throw new UnsupportedOperationException("this reader does not implement getUniqueTermCount()");
   }
-
-  /** Expert: Return the state of the flag that disables fakes norms in favor of representing the absence of field norms with null.
-   * @return true if fake norms are disabled
-   * @deprecated This currently defaults to false (to remain
-   * back-compatible), but in 3.0 it will be hardwired to
-   * true, meaning the norms() methods will return null for
-   * fields that had disabled norms.
-   */
-  public boolean getDisableFakeNorms() {
-    return disableFakeNorms;
-  }
-
-  /** Expert: Set the state of the flag that disables fakes norms in favor of representing the absence of field norms with null.
-   * @param disableFakeNorms true to disable fake norms, false to preserve the legacy behavior
-   * @deprecated This currently defaults to false (to remain
-   * back-compatible), but in 3.0 it will be hardwired to
-   * true, meaning the norms() methods will return null for
-   * fields that had disabled norms.
-   */
-  public void setDisableFakeNorms(boolean disableFakeNorms) {
-    this.disableFakeNorms = disableFakeNorms;
- }
 }
Index: src/java/org/apache/lucene/index/IndexWriter.java
===================================================================
--- src/java/org/apache/lucene/index/IndexWriter.java	(revision 824874)
+++ src/java/org/apache/lucene/index/IndexWriter.java	(working copy)
@@ -611,7 +611,7 @@
         // TODO: we may want to avoid doing this while
         // synchronized
         // Returns a ref, which we xfer to readerMap:
-        sr = SegmentReader.get(info, readBufferSize, doOpenStores, termsIndexDivisor);
+        sr = SegmentReader.get(false, info.dir, info, readBufferSize, doOpenStores, termsIndexDivisor);
         readerMap.put(info, sr);
       } else {
         if (doOpenStores) {
@@ -3410,30 +3410,6 @@
     throws IOException {
   }
 
-  /**
-   * Flush all in-memory buffered updates (adds and deletes)
-   * to the Directory. 
-   * <p>Note: while this will force buffered docs to be
-   * pushed into the index, it will not make these docs
-   * visible to a reader.  Use {@link #commit()} instead
-   *
-   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError
-   * you should immediately close the writer.  See <a
-   * href="#OOME">above</a> for details.</p>
-   *
-   * @deprecated please call {@link #commit()}) instead
-   *
-   * @throws CorruptIndexException if the index is corrupt
-   * @throws IOException if there is a low-level IO error
-   */
-  public final void flush() throws CorruptIndexException, IOException {  
-    if (hitOOM) {
-      throw new IllegalStateException("this writer hit an OutOfMemoryError; cannot flush");
-    }
-
-    flush(true, false, true);
-  }
-
   /** Expert: prepare for commit.
    *
    * <p><b>NOTE</b>: if this method hits an OutOfMemoryError
Index: contrib/instantiated/src/test/org/apache/lucene/store/instantiated/TestEmptyIndex.java
===================================================================
--- contrib/instantiated/src/test/org/apache/lucene/store/instantiated/TestEmptyIndex.java	(revision 824874)
+++ contrib/instantiated/src/test/org/apache/lucene/store/instantiated/TestEmptyIndex.java	(working copy)
@@ -71,8 +71,7 @@
   private void testNorms(IndexReader r) throws IOException {
     byte[] norms;
     norms = r.norms("foo");
-    if (!r.getDisableFakeNorms()) {
-      assertNotNull(norms);
+    if (norms != null) {
       assertEquals(0, norms.length);
       norms = new byte[10];
       Arrays.fill(norms, (byte)10);
Index: contrib/instantiated/src/test/org/apache/lucene/store/instantiated/TestIndicesEquals.java
===================================================================
--- contrib/instantiated/src/test/org/apache/lucene/store/instantiated/TestIndicesEquals.java	(revision 824874)
+++ contrib/instantiated/src/test/org/apache/lucene/store/instantiated/TestIndicesEquals.java	(working copy)
@@ -253,7 +253,7 @@
       byte[] aprioriNorms = aprioriReader.norms((String) field);
       byte[] testNorms = testReader.norms((String) field);
 
-      if (!aprioriReader.getDisableFakeNorms()) {
+      if (aprioriNorms != null) {
         assertEquals(aprioriNorms.length, testNorms.length);
 
         for (int i = 0; i < aprioriNorms.length; i++) {
Index: contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedIndexReader.java
===================================================================
--- contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedIndexReader.java	(revision 824874)
+++ contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedIndexReader.java	(working copy)
@@ -144,7 +144,7 @@
     deletedDocuments.clear();
   }
 
-  protected void doCommit() throws IOException {
+  protected void doCommit(Map commitUserData) throws IOException {
     // todo: read/write lock
 
     boolean updated = false;
Index: contrib/misc/src/test/org/apache/lucene/index/TestFieldNormModifier.java
===================================================================
--- contrib/misc/src/test/org/apache/lucene/index/TestFieldNormModifier.java	(revision 824874)
+++ contrib/misc/src/test/org/apache/lucene/index/TestFieldNormModifier.java	(working copy)
@@ -92,13 +92,7 @@
     
     // sanity check, norms should all be 1
     assertTrue("Whoops we have norms?", !r.hasNorms("nonorm"));
-    if (!r.getDisableFakeNorms()) {
-      for (int i = 0; i< norms.length; i++) {
-        assertEquals(""+i, DEFAULT_NORM, norms[i]);
-      }
-    } else {
-      assertNull(norms);
-    }
+    assertNull(norms);
     
     r.close();
     
@@ -114,13 +108,7 @@
     
     norms = r.norms("nonorm");
     assertTrue("Whoops we have norms?", !r.hasNorms("nonorm"));
-    if (!r.getDisableFakeNorms()) {
-      for (int i = 0; i< norms.length; i++) {
-        assertEquals(""+i, DEFAULT_NORM, norms[i]);
-      }
-    } else {
-      assertNull(norms);
-    }
+    assertNull(norms);
 
     r.close();
   }
Index: contrib/misc/src/test/org/apache/lucene/misc/TestLengthNormModifier.java
===================================================================
--- contrib/misc/src/test/org/apache/lucene/misc/TestLengthNormModifier.java	(revision 824874)
+++ contrib/misc/src/test/org/apache/lucene/misc/TestLengthNormModifier.java	(working copy)
@@ -98,13 +98,7 @@
 
 	// sanity check, norms should all be 1
 	assertTrue("Whoops we have norms?", !r.hasNorms("nonorm"));
-        if (!r.getDisableFakeNorms()) {
-          for (int i = 0; i< norms.length; i++) {
-	    assertEquals(""+i, DEFAULT_NORM, norms[i]);
-          }
-        } else {
-          assertNull(norms);
-        }
+	assertNull(norms);
 
 	r.close();
 	
@@ -120,13 +114,7 @@
 	
 	norms = r.norms("nonorm");
 	assertTrue("Whoops we have norms?", !r.hasNorms("nonorm"));
-        if (!r.getDisableFakeNorms()) {
-          for (int i = 0; i< norms.length; i++) {
-	    assertEquals(""+i, DEFAULT_NORM, norms[i]);
-          }
-        } else {
-          assertNull(norms);
-        }
+  assertNull(norms);
 
 	r.close();
 	
Index: contrib/spatial/src/test/org/apache/lucene/spatial/tier/TestDistance.java
===================================================================
--- contrib/spatial/src/test/org/apache/lucene/spatial/tier/TestDistance.java	(revision 824874)
+++ contrib/spatial/src/test/org/apache/lucene/spatial/tier/TestDistance.java	(working copy)
@@ -94,7 +94,7 @@
     addPoint(writer,"Iota Club and Cafe",38.8890000,-77.0923000);
     addPoint(writer,"Hilton Washington Embassy Row",38.9103000,-77.0451000);
     addPoint(writer,"HorseFeathers, Bar & Grill", 39.01220000000001, -77.3942);
-    writer.flush();
+    writer.commit();
   }
 
   public void testLatLongFilterOnDeletedDocs() throws Exception {
Index: contrib/memory/src/java/org/apache/lucene/index/memory/MemoryIndex.java
===================================================================
--- contrib/memory/src/java/org/apache/lucene/index/memory/MemoryIndex.java	(revision 824874)
+++ contrib/memory/src/java/org/apache/lucene/index/memory/MemoryIndex.java	(working copy)
@@ -1152,7 +1152,7 @@
       throw new UnsupportedOperationException();
     }
   
-    protected void doCommit() {
+    protected void doCommit(Map commitUserData) {
       if (DEBUG) System.err.println("MemoryIndexReader.doCommit");
     }
   
