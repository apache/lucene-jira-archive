

diff -ruN -x .svn -x build lucene-clean-trunk/lucene/CHANGES.txt lucene-3622/lucene/CHANGES.txt
--- lucene-clean-trunk/lucene/CHANGES.txt	2011-12-12 12:27:14.309443853 -0500
+++ lucene-3622/lucene/CHANGES.txt	2011-12-12 15:02:05.841605660 -0500
@@ -505,23 +505,23 @@
   bytes in RAM. (Mike McCandless)
   
 * LUCENE-3108, LUCENE-2935, LUCENE-2168, LUCENE-1231: Changes from 
-  IndexDocValues (ColumnStrideFields):
+  DocValues (ColumnStrideFields):
   
   - IndexWriter now supports typesafe dense per-document values stored in
-    a column like storage. IndexDocValues are stored on a per-document
+    a column like storage. DocValues are stored on a per-document
     basis where each documents field can hold exactly one value of a given
-    type. IndexDocValues are provided via Fieldable and can be used in
+    type. DocValues are provided via Fieldable and can be used in
     conjunction with stored and indexed values.
      
-  - IndexDocValues provides an entirely RAM resident document id to value
+  - DocValues provides an entirely RAM resident document id to value
     mapping per field as well as a DocIdSetIterator based disk-resident
     sequential access API relying on filesystem-caches.
     
   - Both APIs are exposed via IndexReader and the Codec / Flex API allowing
-    expert users to integrate customized IndexDocValues reader and writer
+    expert users to integrate customized DocValues reader and writer
     implementations by extending existing Codecs.
     
-  - IndexDocValues provides implementations for primitive datatypes like int,
+  - DocValues provides implementations for primitive datatypes like int,
     long, float, double and arrays of byte. Byte based implementations further
     provide storage variants like straight or dereferenced stored bytes, fixed
     and variable length bytes as well as index time sorted based on 
@@ -731,10 +731,6 @@
 * LUCENE-3641: Fixed MultiReader to correctly propagate readerFinishedListeners
   to clones/reopened readers.  (Uwe Schindler)
 
-* LUCENE-3642: Fixed bugs in CharTokenizer, n-gram filters, and smart chinese 
-  where they would create invalid offsets in some situations, leading to problems
-  in highlighting. (Max Beutel via Robert Muir)
-
 Documentation
 
 * LUCENE-3597: Fixed incorrect grouping documentation. (Martijn van Groningen, Robert Muir)


diff -ruN -x .svn -x build lucene-clean-trunk/lucene/contrib/memory/src/java/org/apache/lucene/index/memory/MemoryIndex.java lucene-3622/lucene/contrib/memory/src/java/org/apache/lucene/index/memory/MemoryIndex.java
--- lucene-clean-trunk/lucene/contrib/memory/src/java/org/apache/lucene/index/memory/MemoryIndex.java	2011-12-09 08:23:19.812675175 -0500
+++ lucene-3622/lucene/contrib/memory/src/java/org/apache/lucene/index/memory/MemoryIndex.java	2011-12-10 10:57:43.022341097 -0500
@@ -41,12 +41,12 @@
 import org.apache.lucene.index.Fields;
 import org.apache.lucene.index.FieldsEnum;
 import org.apache.lucene.index.IndexReader.AtomicReaderContext;
+import org.apache.lucene.index.DocValues;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.OrdTermState;
 import org.apache.lucene.index.StoredFieldVisitor;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.index.TermState;
-import org.apache.lucene.index.codecs.PerDocValues;
 import org.apache.lucene.index.Terms;
 import org.apache.lucene.index.TermsEnum;
 import org.apache.lucene.search.Collector;
@@ -1159,7 +1159,7 @@
     }
 
     @Override
-    public PerDocValues perDocValues() throws IOException {
+    public DocValues docValues(String field) throws IOException {
       return null;
     }
   }


diff -ruN -x .svn -x build lucene-clean-trunk/lucene/contrib/misc/src/java/org/apache/lucene/document/LazyDocument.java lucene-3622/lucene/contrib/misc/src/java/org/apache/lucene/document/LazyDocument.java
--- lucene-clean-trunk/lucene/contrib/misc/src/java/org/apache/lucene/document/LazyDocument.java	2011-12-06 18:45:01.240810934 -0500
+++ lucene-3622/lucene/contrib/misc/src/java/org/apache/lucene/document/LazyDocument.java	2011-12-10 13:07:49.326477040 -0500
@@ -28,8 +28,8 @@
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexableField;
 import org.apache.lucene.index.IndexableFieldType;
-import org.apache.lucene.index.values.PerDocFieldValues;
-import org.apache.lucene.index.values.ValueType;
+import org.apache.lucene.index.DocValue;
+import org.apache.lucene.index.DocValues;
 import org.apache.lucene.util.BytesRef;
 
 /** Defers actually loading a field's value until you ask
@@ -157,20 +157,20 @@
     }
 
     @Override
-    public PerDocFieldValues docValues() {
+    public DocValue docValue() {
       if (num == 0) {
-        return getDocument().getField(name).docValues();
+        return getDocument().getField(name).docValue();
       } else {
-        return getDocument().getFields(name)[num].docValues();
+        return getDocument().getFields(name)[num].docValue();
       }
     }
 
     @Override
-    public ValueType docValuesType() {
+    public DocValues.Type docValueType() {
       if (num == 0) {
-        return getDocument().getField(name).docValuesType();
+        return getDocument().getField(name).docValueType();
       } else {
-        return getDocument().getFields(name)[num].docValuesType();
+        return getDocument().getFields(name)[num].docValueType();
       }
     }
 


diff -ruN -x .svn -x build lucene-clean-trunk/lucene/src/java/org/apache/lucene/document/DocValuesField.java lucene-3622/lucene/src/java/org/apache/lucene/document/DocValuesField.java
--- lucene-clean-trunk/lucene/src/java/org/apache/lucene/document/DocValuesField.java	1969-12-31 19:00:00.000000000 -0500
+++ lucene-3622/lucene/src/java/org/apache/lucene/document/DocValuesField.java	2011-12-10 13:07:49.306477040 -0500
@@ -0,0 +1,394 @@
+package org.apache.lucene.document;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+import java.io.Reader;
+import java.util.Comparator;
+
+import org.apache.lucene.index.IndexableFieldType;
+import org.apache.lucene.index.DocValue;
+import org.apache.lucene.index.DocValues;
+import org.apache.lucene.index.DocValues.Type; // javadocs
+import org.apache.lucene.util.BytesRef;
+
+/**
+ * <p>
+ * This class provides a {@link Field} that enables storing of typed
+ * per-document values for scoring, sorting or value retrieval. Here's an
+ * example usage, adding an int value:
+ * 
+ * <pre>
+ * document.add(new DocValuesField(name).setInt(value));
+ * </pre>
+ * 
+ * For optimal performance, re-use the <code>DocValuesField</code> and
+ * {@link Document} instance for more than one document:
+ * 
+ * <pre>
+ *  DocValuesField field = new DocValuesField(name);
+ *  Document document = new Document();
+ *  document.add(field);
+ * 
+ *  for(all documents) {
+ *    ...
+ *    field.setInt(value)
+ *    writer.addDocument(document);
+ *    ...
+ *  }
+ * </pre>
+ * 
+ * <p>
+ * If doc values are stored in addition to an indexed ({@link FieldType#setIndexed(boolean)}) or stored
+ * ({@link FieldType#setStored(boolean)}) value it's recommended to pass the appropriate {@link FieldType}
+ * when creating the field:
+ * 
+ * <pre>
+ *  DocValuesField field = new DocValuesField(name, StringField.TYPE_STORED);
+ *  Document document = new Document();
+ *  document.add(field);
+ *  for(all documents) {
+ *    ...
+ *    field.setInt(value)
+ *    writer.addDocument(document);
+ *    ...
+ *  }
+ * </pre>
+ * 
+ * */
+public class DocValuesField extends Field implements DocValue {
+
+  protected BytesRef bytes;
+  protected double doubleValue;
+  protected long longValue;
+  protected DocValues.Type type;
+  protected Comparator<BytesRef> bytesComparator;
+
+  /**
+   * Creates a new {@link DocValuesField} with the given name.
+   */
+  public DocValuesField(String name) {
+    this(name, new FieldType());
+  }
+
+  public DocValuesField(String name, IndexableFieldType type) {
+    this(name, type, null);
+  }
+
+  public DocValuesField(String name, IndexableFieldType type, String value) {
+    super(name, type);
+    fieldsData = value;
+  }
+
+  @Override
+  public DocValue docValue() {
+    return this;
+  }
+
+  /**
+   * Sets the given <code>long</code> value and sets the field's {@link Type} to
+   * {@link Type#VAR_INTS} unless already set. If you want to change the
+   * default type use {@link #setDocValuesType(DocValues.Type)}.
+   */
+  public void setInt(long value) {
+    setInt(value, false);
+  }
+  
+  /**
+   * Sets the given <code>long</code> value as a 64 bit signed integer.
+   * 
+   * @param value
+   *          the value to set
+   * @param fixed
+   *          if <code>true</code> {@link Type#FIXED_INTS_64} is used
+   *          otherwise {@link Type#VAR_INTS}
+   */
+  public void setInt(long value, boolean fixed) {
+    if (type == null) {
+      type = fixed ? DocValues.Type.FIXED_INTS_64 : DocValues.Type.VAR_INTS;
+    }
+    longValue = value;
+  }
+
+  /**
+   * Sets the given <code>int</code> value and sets the field's {@link Type} to
+   * {@link Type#VAR_INTS} unless already set. If you want to change the
+   * default type use {@link #setDocValuesType(DocValues.Type)}.
+   */
+  public void setInt(int value) {
+    setInt(value, false);
+  }
+
+  /**
+   * Sets the given <code>int</code> value as a 32 bit signed integer.
+   * 
+   * @param value
+   *          the value to set
+   * @param fixed
+   *          if <code>true</code> {@link Type#FIXED_INTS_32} is used
+   *          otherwise {@link Type#VAR_INTS}
+   */
+  public void setInt(int value, boolean fixed) {
+    if (type == null) {
+      type = fixed ? DocValues.Type.FIXED_INTS_32 : DocValues.Type.VAR_INTS;
+    }
+    longValue = value;
+  }
+
+  /**
+   * Sets the given <code>short</code> value and sets the field's {@link Type} to
+   * {@link Type#VAR_INTS} unless already set. If you want to change the
+   * default type use {@link #setDocValuesType(DocValues.Type)}.
+   */
+  public void setInt(short value) {
+    setInt(value, false);
+  }
+
+  /**
+   * Sets the given <code>short</code> value as a 16 bit signed integer.
+   * 
+   * @param value
+   *          the value to set
+   * @param fixed
+   *          if <code>true</code> {@link Type#FIXED_INTS_16} is used
+   *          otherwise {@link Type#VAR_INTS}
+   */
+  public void setInt(short value, boolean fixed) {
+    if (type == null) {
+      type = fixed ? DocValues.Type.FIXED_INTS_16 : DocValues.Type.VAR_INTS;
+    }
+    longValue = value;
+  }
+
+  /**
+   * Sets the given <code>byte</code> value and sets the field's {@link Type} to
+   * {@link Type#VAR_INTS} unless already set. If you want to change the
+   * default type use {@link #setDocValuesType(DocValues.Type)}.
+   */
+  public void setInt(byte value) {
+    setInt(value, false);
+  }
+
+  /**
+   * Sets the given <code>byte</code> value as a 8 bit signed integer.
+   * 
+   * @param value
+   *          the value to set
+   * @param fixed
+   *          if <code>true</code> {@link Type#FIXED_INTS_8} is used
+   *          otherwise {@link Type#VAR_INTS}
+   */
+  public void setInt(byte value, boolean fixed) {
+    if (type == null) {
+      type = fixed ? DocValues.Type.FIXED_INTS_8 : DocValues.Type.VAR_INTS;
+    }
+    longValue = value;
+  }
+
+  /**
+   * Sets the given <code>float</code> value and sets the field's {@link Type}
+   * to {@link Type#FLOAT_32} unless already set. If you want to
+   * change the type use {@link #setDocValuesType(DocValues.Type)}.
+   */
+  public void setFloat(float value) {
+    if (type == null) {
+      type = DocValues.Type.FLOAT_32;
+    }
+    doubleValue = value;
+  }
+
+  /**
+   * Sets the given <code>double</code> value and sets the field's {@link Type}
+   * to {@link Type#FLOAT_64} unless already set. If you want to
+   * change the default type use {@link #setDocValuesType(DocValues.Type)}.
+   */
+  public void setFloat(double value) {
+    if (type == null) {
+      type = DocValues.Type.FLOAT_64;
+    }
+    doubleValue = value;
+  }
+
+  /**
+   * Sets the given {@link BytesRef} value and the field's {@link Type}. The
+   * comparator for this field is set to <code>null</code>. If a
+   * <code>null</code> comparator is set the default comparator for the given
+   * {@link Type} is used.
+   */
+  public void setBytes(BytesRef value, DocValues.Type type) {
+    setBytes(value, type, null);
+  }
+
+  /**
+   * Sets the given {@link BytesRef} value, the field's {@link Type} and the
+   * field's comparator. If the {@link Comparator} is set to <code>null</code>
+   * the default for the given {@link Type} is used instead.
+   * 
+   * @throws IllegalArgumentException
+   *           if the value or the type are null
+   */
+  public void setBytes(BytesRef value, DocValues.Type type, Comparator<BytesRef> comp) {
+    if (value == null) {
+      throw new IllegalArgumentException("value must not be null");
+    }
+    setDocValuesType(type);
+    if (bytes == null) {
+      bytes = BytesRef.deepCopyOf(value);
+    } else {
+      bytes.copyBytes(value);
+    }
+    bytesComparator = comp;
+  }
+
+  /**
+   * Returns the set {@link BytesRef} or <code>null</code> if not set.
+   */
+  public BytesRef getBytes() {
+    return bytes;
+  }
+
+  /**
+   * Returns the set {@link BytesRef} comparator or <code>null</code> if not set
+   */
+  public Comparator<BytesRef> bytesComparator() {
+    return bytesComparator;
+  }
+
+  /**
+   * Returns the set floating point value or <code>0.0d</code> if not set.
+   */
+  public double getFloat() {
+    return doubleValue;
+  }
+
+  /**
+   * Returns the set <code>long</code> value of <code>0</code> if not set.
+   */
+  public long getInt() {
+    return longValue;
+  }
+
+  /**
+   * Sets the {@link BytesRef} comparator for this field. If the field has a
+   * numeric {@link Type} the comparator will be ignored.
+   */
+  public void setBytesComparator(Comparator<BytesRef> comp) {
+    this.bytesComparator = comp;
+  }
+
+  /**
+   * Sets the {@link Type} for this field.
+   */
+  public void setDocValuesType(DocValues.Type type) {
+    if (type == null) {
+      throw new IllegalArgumentException("Type must not be null");
+    }
+    this.type = type;
+  }
+
+  /**
+   * Returns always <code>null</code>
+   */
+  public Reader readerValue() {
+    return null;
+  }
+
+  @Override
+  public DocValues.Type docValueType() {
+    return type;
+  }
+
+  @Override
+  public String toString() {
+    final String value;
+    switch (type) {
+    case BYTES_FIXED_DEREF:
+    case BYTES_FIXED_STRAIGHT:
+    case BYTES_VAR_DEREF:
+    case BYTES_VAR_STRAIGHT:
+    case BYTES_FIXED_SORTED:
+    case BYTES_VAR_SORTED:
+      // don't use to unicode string this is not necessarily unicode here
+      value = "bytes: " + bytes.toString();
+      break;
+    case FIXED_INTS_16:
+      value = "int16: " + longValue;
+      break;
+    case FIXED_INTS_32:
+      value = "int32: " + longValue;
+      break;
+    case FIXED_INTS_64:
+      value = "int64: " + longValue;
+      break;
+    case FIXED_INTS_8:
+      value = "int8: " + longValue;
+      break;
+    case VAR_INTS:
+      value = "vint: " + longValue;
+      break;
+    case FLOAT_32:
+      value = "float32: " + doubleValue;
+      break;
+    case FLOAT_64:
+      value = "float64: " + doubleValue;
+      break;
+    default:
+      throw new IllegalArgumentException("unknown type: " + type);
+    }
+    return "<" + name() + ": DocValuesField " + value + ">";
+  }
+
+  /**
+   * Returns an DocValuesField holding the value from
+   * the provided string field, as the specified type.  The
+   * incoming field must have a string value.  The name, {@link
+   * FieldType} and string value are carried over from the
+   * incoming Field.
+   */
+  public static DocValuesField build(Field field, DocValues.Type type) {
+    if (field instanceof DocValuesField) {
+      return (DocValuesField) field;
+    }
+    final DocValuesField valField = new DocValuesField(field.name(), field.fieldType(), field.stringValue());
+    switch (type) {
+    case BYTES_FIXED_DEREF:
+    case BYTES_FIXED_STRAIGHT:
+    case BYTES_VAR_DEREF:
+    case BYTES_VAR_STRAIGHT:
+    case BYTES_FIXED_SORTED:
+    case BYTES_VAR_SORTED:
+      BytesRef ref = field.isBinary() ? field.binaryValue() : new BytesRef(field.stringValue());
+      valField.setBytes(ref, type);
+      break;
+    case FIXED_INTS_16:
+    case FIXED_INTS_32:
+    case FIXED_INTS_64:
+    case FIXED_INTS_8:
+    case VAR_INTS:
+      valField.setInt(Long.parseLong(field.stringValue()));
+      break;
+    case FLOAT_32:
+      valField.setFloat(Float.parseFloat(field.stringValue()));
+      break;
+    case FLOAT_64:
+      valField.setFloat(Double.parseDouble(field.stringValue()));
+      break;
+    default:
+      throw new IllegalArgumentException("unknown type: " + type);
+    }
+    return valField;
+  }
+}


diff -ruN -x .svn -x build lucene-clean-trunk/lucene/src/java/org/apache/lucene/document/Field.java lucene-3622/lucene/src/java/org/apache/lucene/document/Field.java
--- lucene-clean-trunk/lucene/src/java/org/apache/lucene/document/Field.java	2011-12-06 18:45:04.320810987 -0500
+++ lucene-3622/lucene/src/java/org/apache/lucene/document/Field.java	2011-12-10 13:16:21.458485958 -0500
@@ -25,10 +25,10 @@
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
 import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
+import org.apache.lucene.index.DocValues;
 import org.apache.lucene.index.IndexableFieldType;
 import org.apache.lucene.index.IndexableField;
-import org.apache.lucene.index.values.PerDocFieldValues;
-import org.apache.lucene.index.values.ValueType;
+import org.apache.lucene.index.DocValue;
 import org.apache.lucene.util.BytesRef;
 
 /**
@@ -51,7 +51,7 @@
   // pre-analyzed tokenStream for indexed fields
   protected TokenStream tokenStream;
   // length/offset for all primitive types
-  protected PerDocFieldValues docValues;
+  protected DocValue docValue;
   
   protected float boost = 1.0f;
 
@@ -292,17 +292,17 @@
     return result.toString();
   }
   
-  public void setDocValues(PerDocFieldValues docValues) {
-    this.docValues = docValues;
+  public void setDocValue(DocValue docValue) {
+    this.docValue = docValue;
   }
 
   @Override
-  public PerDocFieldValues docValues() {
+  public DocValue docValue() {
     return null;
   }
   
   @Override
-  public ValueType docValuesType() {
+  public DocValues.Type docValueType() {
     return null;
   }
 


diff -ruN -x .svn -x build lucene-clean-trunk/lucene/src/java/org/apache/lucene/document/IndexDocValuesField.java lucene-3622/lucene/src/java/org/apache/lucene/document/IndexDocValuesField.java
--- lucene-clean-trunk/lucene/src/java/org/apache/lucene/document/IndexDocValuesField.java	2011-12-06 18:45:04.316810986 -0500
+++ lucene-3622/lucene/src/java/org/apache/lucene/document/IndexDocValuesField.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,394 +0,0 @@
-package org.apache.lucene.document;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-import java.io.Reader;
-import java.util.Comparator;
-
-import org.apache.lucene.index.IndexableFieldType;
-import org.apache.lucene.index.values.PerDocFieldValues;
-import org.apache.lucene.index.values.ValueType;
-import org.apache.lucene.util.BytesRef;
-
-/**
- * <p>
- * This class provides a {@link Field} that enables storing of typed
- * per-document values for scoring, sorting or value retrieval. Here's an
- * example usage, adding an int value:
- * 
- * <pre>
- * document.add(new IndexDocValuesField(name).setInt(value));
- * </pre>
- * 
- * For optimal performance, re-use the <code>DocValuesField</code> and
- * {@link Document} instance for more than one document:
- * 
- * <pre>
- *  IndexDocValuesField field = new IndexDocValuesField(name);
- *  Document document = new Document();
- *  document.add(field);
- * 
- *  for(all documents) {
- *    ...
- *    field.setInt(value)
- *    writer.addDocument(document);
- *    ...
- *  }
- * </pre>
- * 
- * <p>
- * If doc values are stored in addition to an indexed ({@link FieldType#setIndexed(boolean)}) or stored
- * ({@link FieldType#setStored(boolean)}) value it's recommended to pass the appropriate {@link FieldType}
- * when creating the field:
- * 
- * <pre>
- *  IndexDocValuesField field = new IndexDocValuesField(name, StringField.TYPE_STORED);
- *  Document document = new Document();
- *  document.add(field);
- *  for(all documents) {
- *    ...
- *    field.setInt(value)
- *    writer.addDocument(document);
- *    ...
- *  }
- * </pre>
- * 
- * */
-// TODO: maybe rename to DocValuesField?
-public class IndexDocValuesField extends Field implements PerDocFieldValues {
-
-  protected BytesRef bytes;
-  protected double doubleValue;
-  protected long longValue;
-  protected ValueType type;
-  protected Comparator<BytesRef> bytesComparator;
-
-  /**
-   * Creates a new {@link IndexDocValuesField} with the given name.
-   */
-  public IndexDocValuesField(String name) {
-    this(name, new FieldType());
-  }
-
-  public IndexDocValuesField(String name, IndexableFieldType type) {
-    this(name, type, null);
-  }
-
-  public IndexDocValuesField(String name, IndexableFieldType type, String value) {
-    super(name, type);
-    fieldsData = value;
-  }
-
-  @Override
-  public PerDocFieldValues docValues() {
-    return this;
-  }
-
-  /**
-   * Sets the given <code>long</code> value and sets the field's {@link ValueType} to
-   * {@link ValueType#VAR_INTS} unless already set. If you want to change the
-   * default type use {@link #setDocValuesType(ValueType)}.
-   */
-  public void setInt(long value) {
-    setInt(value, false);
-  }
-  
-  /**
-   * Sets the given <code>long</code> value as a 64 bit signed integer.
-   * 
-   * @param value
-   *          the value to set
-   * @param fixed
-   *          if <code>true</code> {@link ValueType#FIXED_INTS_64} is used
-   *          otherwise {@link ValueType#VAR_INTS}
-   */
-  public void setInt(long value, boolean fixed) {
-    if (type == null) {
-      type = fixed ? ValueType.FIXED_INTS_64 : ValueType.VAR_INTS;
-    }
-    longValue = value;
-  }
-
-  /**
-   * Sets the given <code>int</code> value and sets the field's {@link ValueType} to
-   * {@link ValueType#VAR_INTS} unless already set. If you want to change the
-   * default type use {@link #setDocValuesType(ValueType)}.
-   */
-  public void setInt(int value) {
-    setInt(value, false);
-  }
-
-  /**
-   * Sets the given <code>int</code> value as a 32 bit signed integer.
-   * 
-   * @param value
-   *          the value to set
-   * @param fixed
-   *          if <code>true</code> {@link ValueType#FIXED_INTS_32} is used
-   *          otherwise {@link ValueType#VAR_INTS}
-   */
-  public void setInt(int value, boolean fixed) {
-    if (type == null) {
-      type = fixed ? ValueType.FIXED_INTS_32 : ValueType.VAR_INTS;
-    }
-    longValue = value;
-  }
-
-  /**
-   * Sets the given <code>short</code> value and sets the field's {@link ValueType} to
-   * {@link ValueType#VAR_INTS} unless already set. If you want to change the
-   * default type use {@link #setDocValuesType(ValueType)}.
-   */
-  public void setInt(short value) {
-    setInt(value, false);
-  }
-
-  /**
-   * Sets the given <code>short</code> value as a 16 bit signed integer.
-   * 
-   * @param value
-   *          the value to set
-   * @param fixed
-   *          if <code>true</code> {@link ValueType#FIXED_INTS_16} is used
-   *          otherwise {@link ValueType#VAR_INTS}
-   */
-  public void setInt(short value, boolean fixed) {
-    if (type == null) {
-      type = fixed ? ValueType.FIXED_INTS_16 : ValueType.VAR_INTS;
-    }
-    longValue = value;
-  }
-
-  /**
-   * Sets the given <code>byte</code> value and sets the field's {@link ValueType} to
-   * {@link ValueType#VAR_INTS} unless already set. If you want to change the
-   * default type use {@link #setDocValuesType(ValueType)}.
-   */
-  public void setInt(byte value) {
-    setInt(value, false);
-  }
-
-  /**
-   * Sets the given <code>byte</code> value as a 8 bit signed integer.
-   * 
-   * @param value
-   *          the value to set
-   * @param fixed
-   *          if <code>true</code> {@link ValueType#FIXED_INTS_8} is used
-   *          otherwise {@link ValueType#VAR_INTS}
-   */
-  public void setInt(byte value, boolean fixed) {
-    if (type == null) {
-      type = fixed ? ValueType.FIXED_INTS_8 : ValueType.VAR_INTS;
-    }
-    longValue = value;
-  }
-
-  /**
-   * Sets the given <code>float</code> value and sets the field's {@link ValueType}
-   * to {@link ValueType#FLOAT_32} unless already set. If you want to
-   * change the type use {@link #setDocValuesType(ValueType)}.
-   */
-  public void setFloat(float value) {
-    if (type == null) {
-      type = ValueType.FLOAT_32;
-    }
-    doubleValue = value;
-  }
-
-  /**
-   * Sets the given <code>double</code> value and sets the field's {@link ValueType}
-   * to {@link ValueType#FLOAT_64} unless already set. If you want to
-   * change the default type use {@link #setDocValuesType(ValueType)}.
-   */
-  public void setFloat(double value) {
-    if (type == null) {
-      type = ValueType.FLOAT_64;
-    }
-    doubleValue = value;
-  }
-
-  /**
-   * Sets the given {@link BytesRef} value and the field's {@link ValueType}. The
-   * comparator for this field is set to <code>null</code>. If a
-   * <code>null</code> comparator is set the default comparator for the given
-   * {@link ValueType} is used.
-   */
-  public void setBytes(BytesRef value, ValueType type) {
-    setBytes(value, type, null);
-  }
-
-  /**
-   * Sets the given {@link BytesRef} value, the field's {@link ValueType} and the
-   * field's comparator. If the {@link Comparator} is set to <code>null</code>
-   * the default for the given {@link ValueType} is used instead.
-   * 
-   * @throws IllegalArgumentException
-   *           if the value or the type are null
-   */
-  public void setBytes(BytesRef value, ValueType type, Comparator<BytesRef> comp) {
-    if (value == null) {
-      throw new IllegalArgumentException("value must not be null");
-    }
-    setDocValuesType(type);
-    if (bytes == null) {
-      bytes = BytesRef.deepCopyOf(value);
-    } else {
-      bytes.copyBytes(value);
-    }
-    bytesComparator = comp;
-  }
-
-  /**
-   * Returns the set {@link BytesRef} or <code>null</code> if not set.
-   */
-  public BytesRef getBytes() {
-    return bytes;
-  }
-
-  /**
-   * Returns the set {@link BytesRef} comparator or <code>null</code> if not set
-   */
-  public Comparator<BytesRef> bytesComparator() {
-    return bytesComparator;
-  }
-
-  /**
-   * Returns the set floating point value or <code>0.0d</code> if not set.
-   */
-  public double getFloat() {
-    return doubleValue;
-  }
-
-  /**
-   * Returns the set <code>long</code> value of <code>0</code> if not set.
-   */
-  public long getInt() {
-    return longValue;
-  }
-
-  /**
-   * Sets the {@link BytesRef} comparator for this field. If the field has a
-   * numeric {@link ValueType} the comparator will be ignored.
-   */
-  public void setBytesComparator(Comparator<BytesRef> comp) {
-    this.bytesComparator = comp;
-  }
-
-  /**
-   * Sets the {@link ValueType} for this field.
-   */
-  public void setDocValuesType(ValueType type) {
-    if (type == null) {
-      throw new IllegalArgumentException("Type must not be null");
-    }
-    this.type = type;
-  }
-
-  /**
-   * Returns always <code>null</code>
-   */
-  public Reader readerValue() {
-    return null;
-  }
-
-  @Override
-  public ValueType docValuesType() {
-    return type;
-  }
-
-  @Override
-  public String toString() {
-    final String value;
-    switch (type) {
-    case BYTES_FIXED_DEREF:
-    case BYTES_FIXED_STRAIGHT:
-    case BYTES_VAR_DEREF:
-    case BYTES_VAR_STRAIGHT:
-    case BYTES_FIXED_SORTED:
-    case BYTES_VAR_SORTED:
-      // don't use to unicode string this is not necessarily unicode here
-      value = "bytes: " + bytes.toString();
-      break;
-    case FIXED_INTS_16:
-      value = "int16: " + longValue;
-      break;
-    case FIXED_INTS_32:
-      value = "int32: " + longValue;
-      break;
-    case FIXED_INTS_64:
-      value = "int64: " + longValue;
-      break;
-    case FIXED_INTS_8:
-      value = "int8: " + longValue;
-      break;
-    case VAR_INTS:
-      value = "vint: " + longValue;
-      break;
-    case FLOAT_32:
-      value = "float32: " + doubleValue;
-      break;
-    case FLOAT_64:
-      value = "float64: " + doubleValue;
-      break;
-    default:
-      throw new IllegalArgumentException("unknown type: " + type);
-    }
-    return "<" + name() + ": IndexDocValuesField " + value + ">";
-  }
-
-  /**
-   * Returns an IndexDocValuesField holding the value from
-   * the provided string field, as the specified type.  The
-   * incoming field must have a string value.  The name, {@link
-   * FieldType} and string value are carried over from the
-   * incoming Field.
-   */
-  public static IndexDocValuesField build(Field field, ValueType type) {
-    if (field instanceof IndexDocValuesField) {
-      return (IndexDocValuesField) field;
-    }
-    final IndexDocValuesField valField = new IndexDocValuesField(field.name(), field.fieldType(), field.stringValue());
-    switch (type) {
-    case BYTES_FIXED_DEREF:
-    case BYTES_FIXED_STRAIGHT:
-    case BYTES_VAR_DEREF:
-    case BYTES_VAR_STRAIGHT:
-    case BYTES_FIXED_SORTED:
-    case BYTES_VAR_SORTED:
-      BytesRef ref = field.isBinary() ? field.binaryValue() : new BytesRef(field.stringValue());
-      valField.setBytes(ref, type);
-      break;
-    case FIXED_INTS_16:
-    case FIXED_INTS_32:
-    case FIXED_INTS_64:
-    case FIXED_INTS_8:
-    case VAR_INTS:
-      valField.setInt(Long.parseLong(field.stringValue()));
-      break;
-    case FLOAT_32:
-      valField.setFloat(Float.parseFloat(field.stringValue()));
-      break;
-    case FLOAT_64:
-      valField.setFloat(Double.parseDouble(field.stringValue()));
-      break;
-    default:
-      throw new IllegalArgumentException("unknown type: " + type);
-    }
-    return valField;
-  }
-}


diff -ruN -x .svn -x build lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/BaseMultiReader.java lucene-3622/lucene/src/java/org/apache/lucene/index/BaseMultiReader.java
--- lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/BaseMultiReader.java	2011-12-11 12:21:46.807933541 -0500
+++ lucene-3622/lucene/src/java/org/apache/lucene/index/BaseMultiReader.java	2011-12-12 15:03:57.677607607 -0500
@@ -22,7 +22,6 @@
 import java.util.HashSet;
 import java.util.Set;
 
-import org.apache.lucene.index.codecs.PerDocValues;
 import org.apache.lucene.util.Bits;
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.ReaderUtil;
@@ -156,9 +155,9 @@
   public ReaderContext getTopReaderContext() {
     return topLevelContext;
   }
-
+  
   @Override
-  public PerDocValues perDocValues() throws IOException {
-    throw new UnsupportedOperationException("please use MultiPerDocValues.getPerDocs, or wrap your IndexReader with SlowMultiReaderWrapper, if you really need a top level Fields");
+  public DocValues docValues(String field) throws IOException {
+    throw new UnsupportedOperationException("please use MultiDocValues#getDocValues, or wrap your IndexReader with SlowMultiReaderWrapper, if you really need a top level DocValues");
   }
 }


diff -ruN -x .svn -x build lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/CheckIndex.java lucene-3622/lucene/src/java/org/apache/lucene/index/CheckIndex.java
--- lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/CheckIndex.java	2011-12-09 08:23:19.924675176 -0500
+++ lucene-3622/lucene/src/java/org/apache/lucene/index/CheckIndex.java	2011-12-10 16:42:16.106701108 -0500
@@ -25,7 +25,11 @@
 import org.apache.lucene.store.IOContext;
 import org.apache.lucene.store.IndexInput;
 import org.apache.lucene.document.Document;
+import org.apache.lucene.index.DocValues.SortedSource;
+import org.apache.lucene.index.DocValues.Source;
 import org.apache.lucene.index.codecs.Codec;
+import org.apache.lucene.index.codecs.PerDocProducer;
+
 import java.io.File;
 import java.io.IOException;
 import java.io.PrintStream;
@@ -38,9 +42,6 @@
 import java.util.Map;
 
 import org.apache.lucene.index.codecs.BlockTreeTermsReader;
-import org.apache.lucene.index.codecs.PerDocValues;
-import org.apache.lucene.index.values.IndexDocValues;
-import org.apache.lucene.index.values.IndexDocValues.Source;
 import org.apache.lucene.store.FSDirectory;
 import org.apache.lucene.util.Bits;
 import org.apache.lucene.util.BytesRef;
@@ -638,11 +639,26 @@
       if (infoStream != null) {
         infoStream.print("    test: field norms.........");
       }
+      FieldInfos infos = reader.fieldInfos();
       byte[] b;
       for (final String fieldName : fieldNames) {
+        FieldInfo info = infos.fieldInfo(fieldName);
         if (reader.hasNorms(fieldName)) {
           b = reader.norms(fieldName);
+          if (b.length != reader.maxDoc()) {
+            throw new RuntimeException("norms for field: " + fieldName + " are of the wrong size");
+          }
+          if (!info.isIndexed || info.omitNorms) {
+            throw new RuntimeException("field: " + fieldName + " should omit norms but has them!");
+          }
           ++status.totFields;
+        } else {
+          if (reader.norms(fieldName) != null) {
+            throw new RuntimeException("field: " + fieldName + " should omit norms but has them!");
+          }
+          if (info.isIndexed && !info.omitNorms) {
+            throw new RuntimeException("field: " + fieldName + " should have norms but omits them!");
+          }
         }
       }
 
@@ -1132,39 +1148,92 @@
       for (FieldInfo fieldInfo : fieldInfos) {
         if (fieldInfo.hasDocValues()) {
           status.totalValueFields++;
-          final PerDocValues perDocValues = reader.perDocValues();
-          final IndexDocValues docValues = perDocValues.docValues(fieldInfo.name);
+          final DocValues docValues = reader.docValues(fieldInfo.name);
           if (docValues == null) {
-            continue;
+            throw new RuntimeException("field: " + fieldInfo.name + " omits docvalues but should have them!");
+          }
+          DocValues.Type type = docValues.type();
+          if (type != fieldInfo.getDocValuesType()) {
+            throw new RuntimeException("field: " + fieldInfo.name + " has type: " + type + " but fieldInfos says:" + fieldInfo.getDocValuesType());
           }
           final Source values = docValues.getDirectSource();
           final int maxDoc = reader.maxDoc();
+          int size = docValues.getValueSize();
           for (int i = 0; i < maxDoc; i++) {
-            switch (fieldInfo.docValues) {
+            switch (fieldInfo.getDocValuesType()) {
             case BYTES_FIXED_SORTED:
             case BYTES_VAR_SORTED:
             case BYTES_FIXED_DEREF:
             case BYTES_FIXED_STRAIGHT:
             case BYTES_VAR_DEREF:
             case BYTES_VAR_STRAIGHT:
-              values.getBytes(i, new BytesRef());
+              BytesRef bytes = new BytesRef();
+              values.getBytes(i, bytes);
+              if (size != -1 && size != bytes.length) {
+                throw new RuntimeException("field: " + fieldInfo.name + " returned wrongly sized bytes, was: " + bytes.length + " should be: " + size);
+              }
               break;
             case FLOAT_32:
+              assert size == 4;
+              values.getFloat(i);
+              break;
             case FLOAT_64:
+              assert size == 8;
               values.getFloat(i);
               break;
             case VAR_INTS:
+              assert size == -1;
+              values.getInt(i);
+              break;
             case FIXED_INTS_16:
+              assert size == 2;
+              values.getInt(i);
+              break;
             case FIXED_INTS_32:
+              assert size == 4;
+              values.getInt(i);
+              break;
             case FIXED_INTS_64:
+              assert size == 8;
+              values.getInt(i);
+              break;
             case FIXED_INTS_8:
+              assert size == 1;
               values.getInt(i);
               break;
             default:
               throw new IllegalArgumentException("Field: " + fieldInfo.name
-                  + " - no such DocValues type: " + fieldInfo.docValues);
+                          + " - no such DocValues type: " + fieldInfo.getDocValuesType());
+            }
+          }
+          if (type == DocValues.Type.BYTES_FIXED_SORTED || type == DocValues.Type.BYTES_VAR_SORTED) {
+            // check sorted bytes
+            SortedSource sortedValues = values.asSortedSource();
+            Comparator<BytesRef> comparator = sortedValues.getComparator();
+            int lastOrd = -1;
+            BytesRef lastBytes = new BytesRef();
+            for (int i = 0; i < maxDoc; i++) {
+              int ord = sortedValues.ord(i);
+              if (ord < 0 || ord > maxDoc) {
+                throw new RuntimeException("field: " + fieldInfo.name + " ord is out of bounds: " + ord);
+              }
+              BytesRef bytes = new BytesRef();
+              sortedValues.getByOrd(ord, bytes);
+              if (lastOrd != -1) {
+                int ordComp = Integer.signum(new Integer(ord).compareTo(new Integer(lastOrd)));
+                int bytesComp = Integer.signum(comparator.compare(bytes, lastBytes));
+                if (ordComp != bytesComp) {
+                  throw new RuntimeException("field: " + fieldInfo.name + " ord comparison is wrong: " + ordComp + " comparator claims: " + bytesComp);
+                }
+              }
+              lastOrd = ord;
+              lastBytes = bytes;
             }
           }
+        } else {
+          if (reader.docValues(fieldInfo.name) != null) {
+            throw new RuntimeException("field: " + fieldInfo.name + " has docvalues but should omit them!");
+          }
         }
       }
 


diff -ruN -x .svn -x build lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/codecs/DocValuesConsumer.java lucene-3622/lucene/src/java/org/apache/lucene/index/codecs/DocValuesConsumer.java
--- lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/codecs/DocValuesConsumer.java	2011-12-06 18:45:03.996810982 -0500
+++ lucene-3622/lucene/src/java/org/apache/lucene/index/codecs/DocValuesConsumer.java	2011-12-12 15:12:57.673617011 -0500
@@ -17,77 +17,52 @@
  * limitations under the License.
  */
 import java.io.IOException;
-import java.util.Collection;
 
+import org.apache.lucene.index.DocValues;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.MergeState;
-import org.apache.lucene.index.values.IndexDocValues;
-import org.apache.lucene.index.values.PerDocFieldValues;
-import org.apache.lucene.index.values.Writer;
+import org.apache.lucene.index.DocValue;
 import org.apache.lucene.util.Bits;
-import org.apache.lucene.util.Counter;
 
 /**
- * Abstract API that consumes {@link PerDocFieldValues}.
+ * Abstract API that consumes {@link DocValue}s.
  * {@link DocValuesConsumer} are always associated with a specific field and
  * segments. Concrete implementations of this API write the given
- * {@link PerDocFieldValues} into a implementation specific format depending on
+ * {@link DocValue} into a implementation specific format depending on
  * the fields meta-data.
  * 
  * @lucene.experimental
  */
 public abstract class DocValuesConsumer {
 
-  protected final Counter bytesUsed;
-
-  /**
-   * Creates a new {@link DocValuesConsumer}.
-   * 
-   * @param bytesUsed
-   *          bytes-usage tracking reference used by implementation to track
-   *          internally allocated memory. All tracked bytes must be released
-   *          once {@link #finish(int)} has been called.
-   */
-  protected DocValuesConsumer(Counter bytesUsed) {
-    this.bytesUsed = bytesUsed == null ? Counter.newCounter() : bytesUsed;
-  }
-
   /**
-   * Adds the given {@link PerDocFieldValues} instance to this
+   * Adds the given {@link DocValue} instance to this
    * {@link DocValuesConsumer}
    * 
    * @param docID
    *          the document ID to add the value for. The docID must always
    *          increase or be <tt>0</tt> if it is the first call to this method.
-   * @param docValues
-   *          the values to add
+   * @param docValue
+   *          the value to add
    * @throws IOException
    *           if an {@link IOException} occurs
    */
-  public abstract void add(int docID, PerDocFieldValues docValues)
+  public abstract void add(int docID, DocValue docValue)
       throws IOException;
 
   /**
    * Called when the consumer of this API is doc with adding
-   * {@link PerDocFieldValues} to this {@link DocValuesConsumer}
+   * {@link DocValue} to this {@link DocValuesConsumer}
    * 
    * @param docCount
    *          the total number of documents in this {@link DocValuesConsumer}.
    *          Must be greater than or equal the last given docID to
-   *          {@link #add(int, PerDocFieldValues)}.
+   *          {@link #add(int, DocValue)}.
    * @throws IOException
    */
   public abstract void finish(int docCount) throws IOException;
 
   /**
-   * Gathers files associated with this {@link DocValuesConsumer}
-   * 
-   * @param files
-   *          the of files to add the consumers files to.
-   */
-  public abstract void files(Collection<String> files) throws IOException;
-
-  /**
    * Merges the given {@link org.apache.lucene.index.MergeState} into
    * this {@link DocValuesConsumer}.
    * 
@@ -95,18 +70,18 @@
    *          the state to merge
    * @param docValues docValues array containing one instance per reader (
    *          {@link org.apache.lucene.index.MergeState#readers}) or <code>null</code> if the reader has
-   *          no {@link IndexDocValues} instance.
+   *          no {@link DocValues} instance.
    * @throws IOException
    *           if an {@link IOException} occurs
    */
-  public void merge(MergeState mergeState, IndexDocValues[] docValues) throws IOException {
+  public void merge(MergeState mergeState, DocValues[] docValues) throws IOException {
     assert mergeState != null;
     boolean hasMerged = false;
     for(int readerIDX=0;readerIDX<mergeState.readers.size();readerIDX++) {
       final org.apache.lucene.index.MergeState.IndexReaderAndLiveDocs reader = mergeState.readers.get(readerIDX);
       if (docValues[readerIDX] != null) {
         hasMerged = true;
-        merge(new Writer.SingleSubMergeState(docValues[readerIDX], mergeState.docBase[readerIDX], reader.reader.maxDoc(),
+        merge(new SingleSubMergeState(docValues[readerIDX], mergeState.docBase[readerIDX], reader.reader.maxDoc(),
                                     reader.liveDocs));
       }
     }
@@ -124,6 +99,9 @@
    * @throws IOException
    *           if an {@link IOException} occurs
    */
+  // TODO: can't we have a default implementation here that merges naively with our apis?
+  // this is how stored fields and term vectors work. its a pain to have to impl merging
+  // (should be an optimization to override it)
   protected abstract void merge(SingleSubMergeState mergeState) throws IOException;
 
   /**
@@ -137,7 +115,7 @@
      * the source reader for this MergeState - merged values should be read from
      * this instance
      */
-    public final IndexDocValues reader;
+    public final DocValues reader;
     /** the absolute docBase for this MergeState within the resulting segment */
     public final int docBase;
     /** the number of documents in this MergeState */
@@ -145,7 +123,7 @@
     /** the not deleted bits for this MergeState */
     public final Bits liveDocs;
 
-    public SingleSubMergeState(IndexDocValues reader, int docBase, int docCount, Bits liveDocs) {
+    public SingleSubMergeState(DocValues reader, int docBase, int docCount, Bits liveDocs) {
       assert reader != null;
       this.reader = reader;
       this.docBase = docBase;


diff -ruN -x .svn -x build lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/codecs/DocValuesFormat.java lucene-3622/lucene/src/java/org/apache/lucene/index/codecs/DocValuesFormat.java
--- lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/codecs/DocValuesFormat.java	2011-12-06 18:45:04.100810984 -0500
+++ lucene-3622/lucene/src/java/org/apache/lucene/index/codecs/DocValuesFormat.java	2011-12-10 10:29:15.266311358 -0500
@@ -27,6 +27,6 @@
 
 public abstract class DocValuesFormat {
   public abstract PerDocConsumer docsConsumer(PerDocWriteState state) throws IOException;
-  public abstract PerDocValues docsProducer(SegmentReadState state) throws IOException;
+  public abstract PerDocProducer docsProducer(SegmentReadState state) throws IOException;
   public abstract void files(Directory dir, SegmentInfo info, Set<String> files) throws IOException;
 }


diff -ruN -x .svn -x build lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/codecs/DocValuesReaderBase.java lucene-3622/lucene/src/java/org/apache/lucene/index/codecs/DocValuesReaderBase.java
--- lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/codecs/DocValuesReaderBase.java	2011-12-06 18:45:04.020810982 -0500
+++ lucene-3622/lucene/src/java/org/apache/lucene/index/codecs/DocValuesReaderBase.java	2011-12-10 10:29:15.270311358 -0500
@@ -26,23 +26,24 @@
 
 import org.apache.lucene.index.FieldInfo;
 import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.values.Bytes;
-import org.apache.lucene.index.values.Floats;
-import org.apache.lucene.index.values.IndexDocValues;
-import org.apache.lucene.index.values.Ints;
-import org.apache.lucene.index.values.ValueType;
+import org.apache.lucene.index.DocValues;
+import org.apache.lucene.index.DocValues.Type; // javadocs
+import org.apache.lucene.index.codecs.lucene40.values.Bytes;
+import org.apache.lucene.index.codecs.lucene40.values.Floats;
+import org.apache.lucene.index.codecs.lucene40.values.Ints;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.IOContext;
 import org.apache.lucene.util.BytesRef;
 
 /**
- * Abstract base class for PerDocValues implementations
+ * Abstract base class for PerDocProducer implementations
  * @lucene.experimental
  */
-public abstract class DocValuesReaderBase extends PerDocValues {
+// TODO: this needs to go under lucene40 codec (its specific to its impl)
+public abstract class DocValuesReaderBase extends PerDocProducer {
   
   protected abstract void closeInternal(Collection<? extends Closeable> closeables) throws IOException;
-  protected abstract Map<String, IndexDocValues> docValues();
+  protected abstract Map<String, DocValues> docValues();
   
   @Override
   public void close() throws IOException {
@@ -50,24 +51,19 @@
   }
   
   @Override
-  public IndexDocValues docValues(String field) throws IOException {
+  public DocValues docValues(String field) throws IOException {
     return docValues().get(field);
   }
-
-  @Override
-  public Collection<String> fields() {
-    return docValues().keySet();
-  }
   
   public Comparator<BytesRef> getComparator() throws IOException {
     return BytesRef.getUTF8SortedAsUnicodeComparator();
   }
 
   // Only opens files... doesn't actually load any values
-  protected TreeMap<String, IndexDocValues> load(FieldInfos fieldInfos,
+  protected TreeMap<String, DocValues> load(FieldInfos fieldInfos,
       String segment, int docCount, Directory dir, IOContext context)
       throws IOException {
-    TreeMap<String, IndexDocValues> values = new TreeMap<String, IndexDocValues>();
+    TreeMap<String, DocValues> values = new TreeMap<String, DocValues>();
     boolean success = false;
     try {
 
@@ -79,7 +75,7 @@
           final String id = DocValuesWriterBase.docValuesId(segment,
               fieldInfo.number);
           values.put(field,
-              loadDocValues(docCount, dir, id, fieldInfo.getDocValues(), context));
+              loadDocValues(docCount, dir, id, fieldInfo.getDocValuesType(), context));
         }
       }
       success = true;
@@ -93,26 +89,26 @@
   }
   
   /**
-   * Loads a {@link IndexDocValues} instance depending on the given {@link ValueType}.
-   * Codecs that use different implementations for a certain {@link ValueType} can
+   * Loads a {@link DocValues} instance depending on the given {@link Type}.
+   * Codecs that use different implementations for a certain {@link Type} can
    * simply override this method and return their custom implementations.
    * 
    * @param docCount
    *          number of documents in the segment
    * @param dir
-   *          the {@link Directory} to load the {@link IndexDocValues} from
+   *          the {@link Directory} to load the {@link DocValues} from
    * @param id
    *          the unique file ID within the segment
    * @param type
    *          the type to load
-   * @return a {@link IndexDocValues} instance for the given type
+   * @return a {@link DocValues} instance for the given type
    * @throws IOException
    *           if an {@link IOException} occurs
    * @throws IllegalArgumentException
-   *           if the given {@link ValueType} is not supported
+   *           if the given {@link Type} is not supported
    */
-  protected IndexDocValues loadDocValues(int docCount, Directory dir, String id,
-      ValueType type, IOContext context) throws IOException {
+  protected DocValues loadDocValues(int docCount, Directory dir, String id,
+      DocValues.Type type, IOContext context) throws IOException {
     switch (type) {
     case FIXED_INTS_16:
     case FIXED_INTS_32:


diff -ruN -x .svn -x build lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/codecs/DocValuesWriterBase.java lucene-3622/lucene/src/java/org/apache/lucene/index/codecs/DocValuesWriterBase.java
--- lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/codecs/DocValuesWriterBase.java	2011-12-06 18:45:04.020810982 -0500
+++ lucene-3622/lucene/src/java/org/apache/lucene/index/codecs/DocValuesWriterBase.java	2011-12-09 11:10:18.636849648 -0500
@@ -22,7 +22,8 @@
 
 import org.apache.lucene.index.FieldInfo;
 import org.apache.lucene.index.PerDocWriteState;
-import org.apache.lucene.index.values.Writer;
+import org.apache.lucene.index.codecs.lucene40.values.Writer;
+import org.apache.lucene.index.DocValues;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.IOContext;
 import org.apache.lucene.util.BytesRef;
@@ -32,6 +33,7 @@
  * Abstract base class for PerDocConsumer implementations
  * @lucene.experimental
  */
+//TODO: this needs to go under lucene40 codec (its specific to its impl)
 public abstract class DocValuesWriterBase extends PerDocConsumer {
   protected final String segmentName;
   protected final String segmentSuffix;
@@ -52,8 +54,8 @@
   }
 
   @Override
-  public DocValuesConsumer addValuesField(FieldInfo field) throws IOException {
-    return Writer.create(field.getDocValues(),
+  public DocValuesConsumer addValuesField(DocValues.Type valueType, FieldInfo field) throws IOException {
+    return Writer.create(valueType,
         docValuesId(segmentName, field.number), 
         getDirectory(), getComparator(), bytesUsed, context);
   }


diff -ruN -x .svn -x build lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/codecs/lucene3x/Lucene3xCodec.java lucene-3622/lucene/src/java/org/apache/lucene/index/codecs/lucene3x/Lucene3xCodec.java
--- lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/codecs/lucene3x/Lucene3xCodec.java	2011-12-09 08:23:19.928675177 -0500
+++ lucene-3622/lucene/src/java/org/apache/lucene/index/codecs/lucene3x/Lucene3xCodec.java	2011-12-10 10:29:15.282311358 -0500
@@ -27,9 +27,9 @@
 import org.apache.lucene.index.codecs.DocValuesFormat;
 import org.apache.lucene.index.codecs.FieldInfosFormat;
 import org.apache.lucene.index.codecs.NormsFormat;
+import org.apache.lucene.index.codecs.PerDocProducer;
 import org.apache.lucene.index.codecs.StoredFieldsFormat;
 import org.apache.lucene.index.codecs.PerDocConsumer;
-import org.apache.lucene.index.codecs.PerDocValues;
 import org.apache.lucene.index.codecs.PostingsFormat;
 import org.apache.lucene.index.codecs.SegmentInfosFormat;
 import org.apache.lucene.index.codecs.TermVectorsFormat;
@@ -75,7 +75,7 @@
     }
 
     @Override
-    public PerDocValues docsProducer(SegmentReadState state) throws IOException {
+    public PerDocProducer docsProducer(SegmentReadState state) throws IOException {
       return null;
     }
 


diff -ruN -x .svn -x build lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/codecs/lucene40/Lucene40DocValuesFormat.java lucene-3622/lucene/src/java/org/apache/lucene/index/codecs/lucene40/Lucene40DocValuesFormat.java
--- lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/codecs/lucene40/Lucene40DocValuesFormat.java	2011-12-06 18:45:03.992810982 -0500
+++ lucene-3622/lucene/src/java/org/apache/lucene/index/codecs/lucene40/Lucene40DocValuesFormat.java	2011-12-10 10:29:15.282311358 -0500
@@ -25,7 +25,7 @@
 import org.apache.lucene.index.SegmentReadState;
 import org.apache.lucene.index.codecs.DocValuesFormat;
 import org.apache.lucene.index.codecs.PerDocConsumer;
-import org.apache.lucene.index.codecs.PerDocValues;
+import org.apache.lucene.index.codecs.PerDocProducer;
 import org.apache.lucene.store.Directory;
 
 public class Lucene40DocValuesFormat extends DocValuesFormat {
@@ -36,7 +36,7 @@
   }
 
   @Override
-  public PerDocValues docsProducer(SegmentReadState state) throws IOException {
+  public PerDocProducer docsProducer(SegmentReadState state) throws IOException {
     return new Lucene40DocValuesProducer(state);
   }
 


diff -ruN -x .svn -x build lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/codecs/lucene40/Lucene40DocValuesProducer.java lucene-3622/lucene/src/java/org/apache/lucene/index/codecs/lucene40/Lucene40DocValuesProducer.java
--- lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/codecs/lucene40/Lucene40DocValuesProducer.java	2011-12-06 18:45:03.992810982 -0500
+++ lucene-3622/lucene/src/java/org/apache/lucene/index/codecs/lucene40/Lucene40DocValuesProducer.java	2011-12-10 10:29:15.286311358 -0500
@@ -24,25 +24,25 @@
 import java.util.Map;
 import java.util.TreeMap;
 
+import org.apache.lucene.index.DocValues;
 import org.apache.lucene.index.IndexFileNames;
 import org.apache.lucene.index.SegmentReadState;
 import org.apache.lucene.index.codecs.DocValuesReaderBase;
-import org.apache.lucene.index.values.IndexDocValues;
 import org.apache.lucene.store.CompoundFileDirectory;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.util.IOUtils;
 
 /**
- * Default PerDocValues implementation that uses compound file.
+ * Default PerDocProducer implementation that uses compound file.
  * @lucene.experimental
  */
 public class Lucene40DocValuesProducer extends DocValuesReaderBase {
-  protected final TreeMap<String,IndexDocValues> docValues;
+  protected final TreeMap<String,DocValues> docValues;
   private final Directory cfs;
 
   /**
    * Creates a new {@link Lucene40DocValuesProducer} instance and loads all
-   * {@link IndexDocValues} instances for this segment and codec.
+   * {@link DocValues} instances for this segment and codec.
    */
   public Lucene40DocValuesProducer(SegmentReadState state) throws IOException {
     if (state.fieldInfos.anyDocValuesFields()) {
@@ -53,12 +53,12 @@
       docValues = load(state.fieldInfos, state.segmentInfo.name, state.segmentInfo.docCount, cfs, state.context);
     } else {
       cfs = null;
-      docValues = new TreeMap<String,IndexDocValues>();
+      docValues = new TreeMap<String,DocValues>();
     }
   }
   
   @Override
-  protected Map<String,IndexDocValues> docValues() {
+  protected Map<String,DocValues> docValues() {
     return docValues;
   }
 


diff -ruN -x .svn -x build lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/codecs/lucene40/Lucene40FieldInfosReader.java lucene-3622/lucene/src/java/org/apache/lucene/index/codecs/lucene40/Lucene40FieldInfosReader.java
--- lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/codecs/lucene40/Lucene40FieldInfosReader.java	2011-12-06 18:45:03.988810982 -0500
+++ lucene-3622/lucene/src/java/org/apache/lucene/index/codecs/lucene40/Lucene40FieldInfosReader.java	2011-12-09 11:10:18.732849649 -0500
@@ -12,7 +12,7 @@
 import org.apache.lucene.index.SegmentInfo;
 import org.apache.lucene.index.FieldInfo.IndexOptions;
 import org.apache.lucene.index.codecs.FieldInfosReader;
-import org.apache.lucene.index.values.ValueType;
+import org.apache.lucene.index.DocValues;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.IOContext;
 import org.apache.lucene.store.IndexInput;
@@ -95,7 +95,7 @@
         hasVectors |= storeTermVector;
         hasProx |= isIndexed && indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS;
         hasFreq |= isIndexed && indexOptions != IndexOptions.DOCS_ONLY;
-        ValueType docValuesType = null;
+        DocValues.Type docValuesType = null;
         if (format <= Lucene40FieldInfosWriter.FORMAT_FLEX) {
           final byte b = input.readByte();
           switch(b) {
@@ -103,43 +103,43 @@
               docValuesType = null;
               break;
             case 1:
-              docValuesType = ValueType.VAR_INTS;
+              docValuesType = DocValues.Type.VAR_INTS;
               break;
             case 2:
-              docValuesType = ValueType.FLOAT_32;
+              docValuesType = DocValues.Type.FLOAT_32;
               break;
             case 3:
-              docValuesType = ValueType.FLOAT_64;
+              docValuesType = DocValues.Type.FLOAT_64;
               break;
             case 4:
-              docValuesType = ValueType.BYTES_FIXED_STRAIGHT;
+              docValuesType = DocValues.Type.BYTES_FIXED_STRAIGHT;
               break;
             case 5:
-              docValuesType = ValueType.BYTES_FIXED_DEREF;
+              docValuesType = DocValues.Type.BYTES_FIXED_DEREF;
               break;
             case 6:
-              docValuesType = ValueType.BYTES_VAR_STRAIGHT;
+              docValuesType = DocValues.Type.BYTES_VAR_STRAIGHT;
               break;
             case 7:
-              docValuesType = ValueType.BYTES_VAR_DEREF;
+              docValuesType = DocValues.Type.BYTES_VAR_DEREF;
               break;
             case 8:
-              docValuesType = ValueType.FIXED_INTS_16;
+              docValuesType = DocValues.Type.FIXED_INTS_16;
               break;
             case 9:
-              docValuesType = ValueType.FIXED_INTS_32;
+              docValuesType = DocValues.Type.FIXED_INTS_32;
               break;
             case 10:
-              docValuesType = ValueType.FIXED_INTS_64;
+              docValuesType = DocValues.Type.FIXED_INTS_64;
               break;
             case 11:
-              docValuesType = ValueType.FIXED_INTS_8;
+              docValuesType = DocValues.Type.FIXED_INTS_8;
               break;
             case 12:
-              docValuesType = ValueType.BYTES_FIXED_SORTED;
+              docValuesType = DocValues.Type.BYTES_FIXED_SORTED;
               break;
             case 13:
-              docValuesType = ValueType.BYTES_VAR_SORTED;
+              docValuesType = DocValues.Type.BYTES_VAR_SORTED;
               break;
         
             default:


diff -ruN -x .svn -x build lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/codecs/lucene40/Lucene40FieldInfosWriter.java lucene-3622/lucene/src/java/org/apache/lucene/index/codecs/lucene40/Lucene40FieldInfosWriter.java
--- lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/codecs/lucene40/Lucene40FieldInfosWriter.java	2011-12-06 18:45:03.988810982 -0500
+++ lucene-3622/lucene/src/java/org/apache/lucene/index/codecs/lucene40/Lucene40FieldInfosWriter.java	2011-12-09 08:23:09.856675002 -0500
@@ -83,7 +83,7 @@
         if (!fi.hasDocValues()) {
           b = 0;
         } else {
-          switch(fi.getDocValues()) {
+          switch(fi.getDocValuesType()) {
           case VAR_INTS:
             b = 1;
             break;
@@ -124,7 +124,7 @@
             b = 13;
             break;
           default:
-            throw new IllegalStateException("unhandled indexValues type " + fi.getDocValues());
+            throw new IllegalStateException("unhandled indexValues type " + fi.getDocValuesType());
           }
         }
         output.writeByte(b);


diff -ruN -x .svn -x build lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/codecs/lucene40/values/Bytes.java lucene-3622/lucene/src/java/org/apache/lucene/index/codecs/lucene40/values/Bytes.java
--- lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/codecs/lucene40/values/Bytes.java	1969-12-31 19:00:00.000000000 -0500
+++ lucene-3622/lucene/src/java/org/apache/lucene/index/codecs/lucene40/values/Bytes.java	2011-12-10 13:11:35.282480975 -0500
@@ -0,0 +1,601 @@
+package org.apache.lucene.index.codecs.lucene40.values;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/** Base class for specific Bytes Reader/Writer implementations */
+import java.io.IOException;
+import java.util.Comparator;
+import java.util.concurrent.atomic.AtomicLong;
+
+import org.apache.lucene.index.DocValues;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.DocValue;
+import org.apache.lucene.index.DocValues.SortedSource;
+import org.apache.lucene.index.DocValues.Source;
+import org.apache.lucene.index.DocValues.Type;
+import org.apache.lucene.store.DataOutput;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.util.ArrayUtil;
+import org.apache.lucene.util.ByteBlockPool.Allocator;
+import org.apache.lucene.util.ByteBlockPool.DirectTrackingAllocator;
+import org.apache.lucene.util.ByteBlockPool;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.BytesRefHash.TrackingDirectBytesStartArray;
+import org.apache.lucene.util.BytesRefHash;
+import org.apache.lucene.util.CodecUtil;
+import org.apache.lucene.util.Counter;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util.PagedBytes;
+import org.apache.lucene.util.RamUsageEstimator;
+import org.apache.lucene.util.packed.PackedInts;
+
+/**
+ * Provides concrete Writer/Reader implementations for <tt>byte[]</tt> value per
+ * document. There are 6 package-private default implementations of this, for
+ * all combinations of {@link Mode#DEREF}/{@link Mode#STRAIGHT} x fixed-length/variable-length.
+ * 
+ * <p>
+ * NOTE: Currently the total amount of byte[] data stored (across a single
+ * segment) cannot exceed 2GB.
+ * </p>
+ * <p>
+ * NOTE: Each byte[] must be <= 32768 bytes in length
+ * </p>
+ * 
+ * @lucene.experimental
+ */
+public final class Bytes {
+
+  static final String DV_SEGMENT_SUFFIX = "dv";
+
+  // TODO - add bulk copy where possible
+  private Bytes() { /* don't instantiate! */
+  }
+
+  /**
+   * Defines the {@link Writer}s store mode. The writer will either store the
+   * bytes sequentially ({@link #STRAIGHT}, dereferenced ({@link #DEREF}) or
+   * sorted ({@link #SORTED})
+   * 
+   * @lucene.experimental
+   */
+  public static enum Mode {
+    /**
+     * Mode for sequentially stored bytes
+     */
+    STRAIGHT,
+    /**
+     * Mode for dereferenced stored bytes
+     */
+    DEREF,
+    /**
+     * Mode for sorted stored bytes
+     */
+    SORTED
+  };
+
+  /**
+   * Creates a new <tt>byte[]</tt> {@link Writer} instances for the given
+   * directory.
+   * 
+   * @param dir
+   *          the directory to write the values to
+   * @param id
+   *          the id used to create a unique file name. Usually composed out of
+   *          the segment name and a unique id per segment.
+   * @param mode
+   *          the writers store mode
+   * @param fixedSize
+   *          <code>true</code> if all bytes subsequently passed to the
+   *          {@link Writer} will have the same length
+   * @param sortComparator {@link BytesRef} comparator used by sorted variants. 
+   *        If <code>null</code> {@link BytesRef#getUTF8SortedAsUnicodeComparator()}
+   *        is used instead
+   * @param bytesUsed
+   *          an {@link AtomicLong} instance to track the used bytes within the
+   *          {@link Writer}. A call to {@link Writer#finish(int)} will release
+   *          all internally used resources and frees the memory tracking
+   *          reference.
+   * @param context 
+   * @return a new {@link Writer} instance
+   * @throws IOException
+   *           if the files for the writer can not be created.
+   */
+  public static Writer getWriter(Directory dir, String id, Mode mode,
+      boolean fixedSize, Comparator<BytesRef> sortComparator, Counter bytesUsed, IOContext context)
+      throws IOException {
+    // TODO -- i shouldn't have to specify fixed? can
+    // track itself & do the write thing at write time?
+    if (sortComparator == null) {
+      sortComparator = BytesRef.getUTF8SortedAsUnicodeComparator();
+    }
+
+    if (fixedSize) {
+      if (mode == Mode.STRAIGHT) {
+        return new FixedStraightBytesImpl.Writer(dir, id, bytesUsed, context);
+      } else if (mode == Mode.DEREF) {
+        return new FixedDerefBytesImpl.Writer(dir, id, bytesUsed, context);
+      } else if (mode == Mode.SORTED) {
+        return new FixedSortedBytesImpl.Writer(dir, id, sortComparator, bytesUsed, context);
+      }
+    } else {
+      if (mode == Mode.STRAIGHT) {
+        return new VarStraightBytesImpl.Writer(dir, id, bytesUsed, context);
+      } else if (mode == Mode.DEREF) {
+        return new VarDerefBytesImpl.Writer(dir, id, bytesUsed, context);
+      } else if (mode == Mode.SORTED) {
+        return new VarSortedBytesImpl.Writer(dir, id, sortComparator, bytesUsed, context);
+      }
+    }
+
+    throw new IllegalArgumentException("");
+  }
+
+  /**
+   * Creates a new {@link DocValues} instance that provides either memory
+   * resident or iterative access to a per-document stored <tt>byte[]</tt>
+   * value. The returned {@link DocValues} instance will be initialized without
+   * consuming a significant amount of memory.
+   * 
+   * @param dir
+   *          the directory to load the {@link DocValues} from.
+   * @param id
+   *          the file ID in the {@link Directory} to load the values from.
+   * @param mode
+   *          the mode used to store the values
+   * @param fixedSize
+   *          <code>true</code> iff the values are stored with fixed-size,
+   *          otherwise <code>false</code>
+   * @param maxDoc
+   *          the number of document values stored for the given ID
+   * @param sortComparator {@link BytesRef} comparator used by sorted variants. 
+   *        If <code>null</code> {@link BytesRef#getUTF8SortedAsUnicodeComparator()}
+   *        is used instead
+   * @return an initialized {@link DocValues} instance.
+   * @throws IOException
+   *           if an {@link IOException} occurs
+   */
+  public static DocValues getValues(Directory dir, String id, Mode mode,
+      boolean fixedSize, int maxDoc, Comparator<BytesRef> sortComparator, IOContext context) throws IOException {
+    if (sortComparator == null) {
+      sortComparator = BytesRef.getUTF8SortedAsUnicodeComparator();
+    }
+    // TODO -- I can peek @ header to determing fixed/mode?
+    if (fixedSize) {
+      if (mode == Mode.STRAIGHT) {
+        return new FixedStraightBytesImpl.FixedStraightReader(dir, id, maxDoc, context);
+      } else if (mode == Mode.DEREF) {
+        return new FixedDerefBytesImpl.FixedDerefReader(dir, id, maxDoc, context);
+      } else if (mode == Mode.SORTED) {
+        return new FixedSortedBytesImpl.Reader(dir, id, maxDoc, context, Type.BYTES_FIXED_SORTED, sortComparator);
+      }
+    } else {
+      if (mode == Mode.STRAIGHT) {
+        return new VarStraightBytesImpl.VarStraightReader(dir, id, maxDoc, context);
+      } else if (mode == Mode.DEREF) {
+        return new VarDerefBytesImpl.VarDerefReader(dir, id, maxDoc, context);
+      } else if (mode == Mode.SORTED) {
+        return new VarSortedBytesImpl.Reader(dir, id, maxDoc,context, Type.BYTES_VAR_SORTED, sortComparator);
+      }
+    }
+
+    throw new IllegalArgumentException("Illegal Mode: " + mode);
+  }
+
+  // TODO open up this API?
+  static abstract class BytesSourceBase extends Source {
+    private final PagedBytes pagedBytes;
+    protected final IndexInput datIn;
+    protected final IndexInput idxIn;
+    protected final static int PAGED_BYTES_BITS = 15;
+    protected final PagedBytes.Reader data;
+    protected final long totalLengthInBytes;
+    
+
+    protected BytesSourceBase(IndexInput datIn, IndexInput idxIn,
+        PagedBytes pagedBytes, long bytesToRead, Type type) throws IOException {
+      super(type);
+      assert bytesToRead <= datIn.length() : " file size is less than the expected size diff: "
+          + (bytesToRead - datIn.length()) + " pos: " + datIn.getFilePointer();
+      this.datIn = datIn;
+      this.totalLengthInBytes = bytesToRead;
+      this.pagedBytes = pagedBytes;
+      this.pagedBytes.copy(datIn, bytesToRead);
+      data = pagedBytes.freeze(true);
+      this.idxIn = idxIn;
+    }
+  }
+  
+  // TODO: open up this API?!
+  static abstract class BytesWriterBase extends Writer {
+    private final String id;
+    private IndexOutput idxOut;
+    private IndexOutput datOut;
+    protected BytesRef bytesRef = new BytesRef();
+    private final Directory dir;
+    private final String codecName;
+    private final int version;
+    private final IOContext context;
+
+    protected BytesWriterBase(Directory dir, String id, String codecName,
+        int version, Counter bytesUsed, IOContext context) throws IOException {
+      super(bytesUsed);
+      this.id = id;
+      this.dir = dir;
+      this.codecName = codecName;
+      this.version = version;
+      this.context = context;
+    }
+    
+    protected IndexOutput getOrCreateDataOut() throws IOException {
+      if (datOut == null) {
+        boolean success = false;
+        try {
+          datOut = dir.createOutput(IndexFileNames.segmentFileName(id, DV_SEGMENT_SUFFIX,
+              DATA_EXTENSION), context);
+          CodecUtil.writeHeader(datOut, codecName, version);
+          success = true;
+        } finally {
+          if (!success) {
+            IOUtils.closeWhileHandlingException(datOut);
+          }
+        }
+      }
+      return datOut;
+    }
+    
+    protected IndexOutput getIndexOut() {
+      return idxOut;
+    }
+    
+    protected IndexOutput getDataOut() {
+      return datOut;
+    }
+
+    protected IndexOutput getOrCreateIndexOut() throws IOException {
+      boolean success = false;
+      try {
+        if (idxOut == null) {
+          idxOut = dir.createOutput(IndexFileNames.segmentFileName(id, DV_SEGMENT_SUFFIX,
+              INDEX_EXTENSION), context);
+          CodecUtil.writeHeader(idxOut, codecName, version);
+        }
+        success = true;
+      } finally {
+        if (!success) {
+          IOUtils.closeWhileHandlingException(idxOut);
+        }
+      }
+      return idxOut;
+    }
+    /**
+     * Must be called only with increasing docIDs. It's OK for some docIDs to be
+     * skipped; they will be filled with 0 bytes.
+     */
+    @Override
+    public abstract void add(int docID, BytesRef bytes) throws IOException;
+
+    @Override
+    public abstract void finish(int docCount) throws IOException;
+
+    @Override
+    protected void mergeDoc(int docID, int sourceDoc) throws IOException {
+      add(docID, currentMergeSource.getBytes(sourceDoc, bytesRef));
+    }
+
+    @Override
+    public void add(int docID, DocValue docValue) throws IOException {
+      final BytesRef ref;
+      if ((ref = docValue.getBytes()) != null) {
+        add(docID, ref);
+      }
+    }
+  }
+
+  /**
+   * Opens all necessary files, but does not read any data in until you call
+   * {@link #load}.
+   */
+  static abstract class BytesReaderBase extends DocValues {
+    protected final IndexInput idxIn;
+    protected final IndexInput datIn;
+    protected final int version;
+    protected final String id;
+    protected final Type type;
+
+    protected BytesReaderBase(Directory dir, String id, String codecName,
+        int maxVersion, boolean doIndex, IOContext context, Type type) throws IOException {
+      IndexInput dataIn = null;
+      IndexInput indexIn = null;
+      boolean success = false;
+      try {
+        dataIn = dir.openInput(IndexFileNames.segmentFileName(id, DV_SEGMENT_SUFFIX,
+                                                              Writer.DATA_EXTENSION), context);
+        version = CodecUtil.checkHeader(dataIn, codecName, maxVersion, maxVersion);
+        if (doIndex) {
+          indexIn = dir.openInput(IndexFileNames.segmentFileName(id, DV_SEGMENT_SUFFIX,
+                                                                 Writer.INDEX_EXTENSION), context);
+          final int version2 = CodecUtil.checkHeader(indexIn, codecName,
+                                                     maxVersion, maxVersion);
+          assert version == version2;
+        }
+        success = true;
+      } finally {
+        if (!success) {
+          IOUtils.closeWhileHandlingException(dataIn, indexIn);
+        }
+      }
+      datIn = dataIn;
+      idxIn = indexIn;
+      this.type = type;
+      this.id = id;
+    }
+
+    /**
+     * clones and returns the data {@link IndexInput}
+     */
+    protected final IndexInput cloneData() {
+      assert datIn != null;
+      return (IndexInput) datIn.clone();
+    }
+
+    /**
+     * clones and returns the indexing {@link IndexInput}
+     */
+    protected final IndexInput cloneIndex() {
+      assert idxIn != null;
+      return (IndexInput) idxIn.clone();
+    }
+
+    @Override
+    public void close() throws IOException {
+      try {
+        super.close();
+      } finally {
+         IOUtils.close(datIn, idxIn);
+      }
+    }
+
+    @Override
+    public Type type() {
+      return type;
+    }
+    
+  }
+  
+  static abstract class DerefBytesWriterBase extends BytesWriterBase {
+    protected int size = -1;
+    protected int lastDocId = -1;
+    protected int[] docToEntry;
+    protected final BytesRefHash hash;
+    protected long maxBytes = 0;
+    
+    protected DerefBytesWriterBase(Directory dir, String id, String codecName,
+        int codecVersion, Counter bytesUsed, IOContext context)
+        throws IOException {
+      this(dir, id, codecName, codecVersion, new DirectTrackingAllocator(
+          ByteBlockPool.BYTE_BLOCK_SIZE, bytesUsed), bytesUsed, context);
+    }
+
+    protected DerefBytesWriterBase(Directory dir, String id, String codecName, int codecVersion, Allocator allocator,
+        Counter bytesUsed, IOContext context) throws IOException {
+      super(dir, id, codecName, codecVersion, bytesUsed, context);
+      hash = new BytesRefHash(new ByteBlockPool(allocator),
+          BytesRefHash.DEFAULT_CAPACITY, new TrackingDirectBytesStartArray(
+              BytesRefHash.DEFAULT_CAPACITY, bytesUsed));
+      docToEntry = new int[1];
+      bytesUsed.addAndGet(RamUsageEstimator.NUM_BYTES_INT);
+    }
+    
+    protected static int writePrefixLength(DataOutput datOut, BytesRef bytes)
+        throws IOException {
+      if (bytes.length < 128) {
+        datOut.writeByte((byte) bytes.length);
+        return 1;
+      } else {
+        datOut.writeByte((byte) (0x80 | (bytes.length >> 8)));
+        datOut.writeByte((byte) (bytes.length & 0xff));
+        return 2;
+      }
+    }
+
+    @Override
+    public void add(int docID, BytesRef bytes) throws IOException {
+      if (bytes.length == 0) { // default value - skip it
+        return;
+      }
+      checkSize(bytes);
+      fillDefault(docID);
+      int ord = hash.add(bytes);
+      if (ord < 0) {
+        ord = (-ord) - 1;
+      } else {
+        maxBytes += bytes.length;
+      }
+      
+      
+      docToEntry[docID] = ord;
+      lastDocId = docID;
+    }
+    
+    protected void fillDefault(int docID) {
+      if (docID >= docToEntry.length) {
+        final int size = docToEntry.length;
+        docToEntry = ArrayUtil.grow(docToEntry, 1 + docID);
+        bytesUsed.addAndGet((docToEntry.length - size)
+            * RamUsageEstimator.NUM_BYTES_INT);
+      }
+      assert size >= 0;
+      BytesRef ref = new BytesRef(size);
+      ref.length = size;
+      int ord = hash.add(ref);
+      if (ord < 0) {
+        ord = (-ord) - 1;
+      }
+      for (int i = lastDocId+1; i < docID; i++) {
+        docToEntry[i] = ord;
+      }
+    }
+    
+    protected void checkSize(BytesRef bytes) {
+      if (size == -1) {
+        size = bytes.length;
+      } else if (bytes.length != size) {
+        throw new IllegalArgumentException("expected bytes size=" + size
+            + " but got " + bytes.length);
+      }
+    }
+    
+    // Important that we get docCount, in case there were
+    // some last docs that we didn't see
+    @Override
+    public void finish(int docCount) throws IOException {
+      boolean success = false;
+      try {
+        finishInternal(docCount);
+        success = true;
+      } finally {
+        releaseResources();
+        if (success) {
+          IOUtils.close(getIndexOut(), getDataOut());
+        } else {
+          IOUtils.closeWhileHandlingException(getIndexOut(), getDataOut());
+        }
+        
+      }
+    }
+    
+    protected abstract void finishInternal(int docCount) throws IOException;
+    
+    protected void releaseResources() {
+      hash.close();
+      bytesUsed.addAndGet((-docToEntry.length) * RamUsageEstimator.NUM_BYTES_INT);
+      docToEntry = null;
+    }
+    
+    protected void writeIndex(IndexOutput idxOut, int docCount,
+        long maxValue, int[] toEntry) throws IOException {
+      writeIndex(idxOut, docCount, maxValue, (int[])null, toEntry);
+    }
+    
+    protected void writeIndex(IndexOutput idxOut, int docCount,
+        long maxValue, int[] addresses, int[] toEntry) throws IOException {
+      final PackedInts.Writer w = PackedInts.getWriter(idxOut, docCount,
+          PackedInts.bitsRequired(maxValue));
+      final int limit = docCount > docToEntry.length ? docToEntry.length
+          : docCount;
+      assert toEntry.length >= limit -1;
+      if (addresses != null) {
+        for (int i = 0; i < limit; i++) {
+          assert addresses[toEntry[i]] >= 0;
+          w.add(addresses[toEntry[i]]);
+        }
+      } else {
+        for (int i = 0; i < limit; i++) {
+          assert toEntry[i] >= 0;
+          w.add(toEntry[i]);
+        }
+      }
+      for (int i = limit; i < docCount; i++) {
+        w.add(0);
+      }
+      w.finish();
+    }
+    
+    protected void writeIndex(IndexOutput idxOut, int docCount,
+        long maxValue, long[] addresses, int[] toEntry) throws IOException {
+      final PackedInts.Writer w = PackedInts.getWriter(idxOut, docCount,
+          PackedInts.bitsRequired(maxValue));
+      final int limit = docCount > docToEntry.length ? docToEntry.length
+          : docCount;
+      assert toEntry.length >= limit -1;
+      if (addresses != null) {
+        for (int i = 0; i < limit; i++) {
+          assert addresses[toEntry[i]] >= 0;
+          w.add(addresses[toEntry[i]]);
+        }
+      } else {
+        for (int i = 0; i < limit; i++) {
+          assert toEntry[i] >= 0;
+          w.add(toEntry[i]);
+        }
+      }
+      for (int i = limit; i < docCount; i++) {
+        w.add(0);
+      }
+      w.finish();
+    }
+    
+  }
+  
+  static abstract class BytesSortedSourceBase extends SortedSource {
+    private final PagedBytes pagedBytes;
+    
+    protected final PackedInts.Reader docToOrdIndex;
+    protected final PackedInts.Reader ordToOffsetIndex;
+
+    protected final IndexInput datIn;
+    protected final IndexInput idxIn;
+    protected final BytesRef defaultValue = new BytesRef();
+    protected final static int PAGED_BYTES_BITS = 15;
+    protected final PagedBytes.Reader data;
+
+    protected BytesSortedSourceBase(IndexInput datIn, IndexInput idxIn,
+        Comparator<BytesRef> comp, long bytesToRead, Type type, boolean hasOffsets) throws IOException {
+      this(datIn, idxIn, comp, new PagedBytes(PAGED_BYTES_BITS), bytesToRead, type, hasOffsets);
+    }
+    
+    protected BytesSortedSourceBase(IndexInput datIn, IndexInput idxIn,
+        Comparator<BytesRef> comp, PagedBytes pagedBytes, long bytesToRead, Type type, boolean hasOffsets)
+        throws IOException {
+      super(type, comp);
+      assert bytesToRead <= datIn.length() : " file size is less than the expected size diff: "
+          + (bytesToRead - datIn.length()) + " pos: " + datIn.getFilePointer();
+      this.datIn = datIn;
+      this.pagedBytes = pagedBytes;
+      this.pagedBytes.copy(datIn, bytesToRead);
+      data = pagedBytes.freeze(true);
+      this.idxIn = idxIn;
+      ordToOffsetIndex = hasOffsets ? PackedInts.getReader(idxIn) : null; 
+      docToOrdIndex = PackedInts.getReader(idxIn);
+    }
+
+    @Override
+    public boolean hasPackedDocToOrd() {
+      return true;
+    }
+
+    @Override
+    public PackedInts.Reader getDocToOrd() {
+      return docToOrdIndex;
+    }
+    
+    @Override
+    public int ord(int docID) {
+      assert docToOrdIndex.get(docID) < getValueCount();
+      return (int) docToOrdIndex.get(docID);
+    }
+
+    protected void closeIndexInput() throws IOException {
+      IOUtils.close(datIn, idxIn);
+    }
+  }
+}


diff -ruN -x .svn -x build lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/codecs/lucene40/values/BytesRefUtils.java lucene-3622/lucene/src/java/org/apache/lucene/index/codecs/lucene40/values/BytesRefUtils.java
--- lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/codecs/lucene40/values/BytesRefUtils.java	1969-12-31 19:00:00.000000000 -0500
+++ lucene-3622/lucene/src/java/org/apache/lucene/index/codecs/lucene40/values/BytesRefUtils.java	2011-12-06 18:19:37.232784394 -0500
@@ -0,0 +1,120 @@
+package org.apache.lucene.index.codecs.lucene40.values;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements. See the NOTICE file distributed with this
+ * work for additional information regarding copyright ownership. The ASF
+ * licenses this file to You under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ * 
+ * http://www.apache.org/licenses/LICENSE-2.0
+ * 
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
+ * License for the specific language governing permissions and limitations under
+ * the License.
+ */
+
+import org.apache.lucene.util.BytesRef;
+
+/**
+ * Package private BytesRefUtils - can move this into the o.a.l.utils package if
+ * needed.
+ * 
+ * @lucene.internal
+ */
+public final class BytesRefUtils {
+
+  private BytesRefUtils() {
+  }
+
+  /**
+   * Copies the given long value and encodes it as 8 byte Big-Endian.
+   * <p>
+   * NOTE: this method resets the offset to 0, length to 8 and resizes the
+   * reference array if needed.
+   */
+  public static void copyLong(BytesRef ref, long value) {
+    if (ref.bytes.length < 8) {
+      ref.bytes = new byte[8];
+    }
+    copyInternal(ref, (int) (value >> 32), ref.offset = 0);
+    copyInternal(ref, (int) value, 4);
+    ref.length = 8;
+  }
+
+  /**
+   * Copies the given int value and encodes it as 4 byte Big-Endian.
+   * <p>
+   * NOTE: this method resets the offset to 0, length to 4 and resizes the
+   * reference array if needed.
+   */
+  public static void copyInt(BytesRef ref, int value) {
+    if (ref.bytes.length < 4) {
+      ref.bytes = new byte[4];
+    }
+    copyInternal(ref, value, ref.offset = 0);
+    ref.length = 4;
+  }
+
+  /**
+   * Copies the given short value and encodes it as a 2 byte Big-Endian.
+   * <p>
+   * NOTE: this method resets the offset to 0, length to 2 and resizes the
+   * reference array if needed.
+   */
+  public static void copyShort(BytesRef ref, short value) {
+    if (ref.bytes.length < 2) {
+      ref.bytes = new byte[2];
+    }
+    ref.bytes[ref.offset] = (byte) (value >> 8);
+    ref.bytes[ref.offset + 1] = (byte) (value);
+    ref.length = 2;
+  }
+
+  private static void copyInternal(BytesRef ref, int value, int startOffset) {
+    ref.bytes[startOffset] = (byte) (value >> 24);
+    ref.bytes[startOffset + 1] = (byte) (value >> 16);
+    ref.bytes[startOffset + 2] = (byte) (value >> 8);
+    ref.bytes[startOffset + 3] = (byte) (value);
+  }
+
+  /**
+   * Converts 2 consecutive bytes from the current offset to a short. Bytes are
+   * interpreted as Big-Endian (most significant bit first)
+   * <p>
+   * NOTE: this method does <b>NOT</b> check the bounds of the referenced array.
+   */
+  public static short asShort(BytesRef b) {
+    return (short) (0xFFFF & ((b.bytes[b.offset] & 0xFF) << 8) | (b.bytes[b.offset + 1] & 0xFF));
+  }
+
+  /**
+   * Converts 4 consecutive bytes from the current offset to an int. Bytes are
+   * interpreted as Big-Endian (most significant bit first)
+   * <p>
+   * NOTE: this method does <b>NOT</b> check the bounds of the referenced array.
+   */
+  public static int asInt(BytesRef b) {
+    return asIntInternal(b, b.offset);
+  }
+
+  /**
+   * Converts 8 consecutive bytes from the current offset to a long. Bytes are
+   * interpreted as Big-Endian (most significant bit first)
+   * <p>
+   * NOTE: this method does <b>NOT</b> check the bounds of the referenced array.
+   */
+  public static long asLong(BytesRef b) {
+    return (((long) asIntInternal(b, b.offset) << 32) | asIntInternal(b,
+        b.offset + 4) & 0xFFFFFFFFL);
+  }
+
+  private static int asIntInternal(BytesRef b, int pos) {
+    return ((b.bytes[pos++] & 0xFF) << 24) | ((b.bytes[pos++] & 0xFF) << 16)
+        | ((b.bytes[pos++] & 0xFF) << 8) | (b.bytes[pos] & 0xFF);
+  }
+
+}


diff -ruN -x .svn -x build lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/codecs/lucene40/values/DirectSource.java lucene-3622/lucene/src/java/org/apache/lucene/index/codecs/lucene40/values/DirectSource.java
--- lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/codecs/lucene40/values/DirectSource.java	1969-12-31 19:00:00.000000000 -0500
+++ lucene-3622/lucene/src/java/org/apache/lucene/index/codecs/lucene40/values/DirectSource.java	2011-12-09 11:10:18.628849648 -0500
@@ -0,0 +1,139 @@
+package org.apache.lucene.index.codecs.lucene40.values;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.index.DocValues;
+import org.apache.lucene.index.DocValues.Source;
+import org.apache.lucene.index.DocValues.Type;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.util.BytesRef;
+
+/**
+ * Base class for disk resident source implementations
+ * @lucene.internal
+ */
+public abstract class DirectSource extends Source {
+
+  protected final IndexInput data;
+  private final ToNumeric toNumeric;
+  protected final long baseOffset;
+
+  public DirectSource(IndexInput input, Type type) {
+    super(type);
+    this.data = input;
+    baseOffset = input.getFilePointer();
+    switch (type) {
+    case FIXED_INTS_16:
+      toNumeric = new ShortToLong();
+      break;
+    case FLOAT_32:
+    case FIXED_INTS_32:
+      toNumeric = new IntToLong();
+      break;
+    case FIXED_INTS_8:
+      toNumeric = new ByteToLong();
+      break;
+    default:
+      toNumeric = new LongToLong();
+    }
+  }
+
+  @Override
+  public BytesRef getBytes(int docID, BytesRef ref) {
+    try {
+      final int sizeToRead = position(docID);
+      ref.grow(sizeToRead);
+      data.readBytes(ref.bytes, 0, sizeToRead);
+      ref.length = sizeToRead;
+      ref.offset = 0;
+      return ref;
+    } catch (IOException ex) {
+      throw new IllegalStateException("failed to get value for docID: " + docID, ex);
+    }
+  }
+
+  @Override
+  public long getInt(int docID) {
+    try {
+      position(docID);
+      return toNumeric.toLong(data);
+    } catch (IOException ex) {
+      throw new IllegalStateException("failed to get value for docID: " + docID, ex);
+    }
+  }
+
+  @Override
+  public double getFloat(int docID) {
+    try {
+      position(docID);
+      return toNumeric.toDouble(data);
+    } catch (IOException ex) {
+      throw new IllegalStateException("failed to get value for docID: " + docID, ex);
+    }
+  }
+
+  protected abstract int position(int docID) throws IOException;
+
+  private abstract static class ToNumeric {
+    abstract long toLong(IndexInput input) throws IOException;
+
+    double toDouble(IndexInput input) throws IOException {
+      return toLong(input);
+    }
+  }
+
+  private static final class ByteToLong extends ToNumeric {
+    @Override
+    long toLong(IndexInput input) throws IOException {
+      return input.readByte();
+    }
+
+  }
+
+  private static final class ShortToLong extends ToNumeric {
+    @Override
+    long toLong(IndexInput input) throws IOException {
+      return input.readShort();
+    }
+  }
+
+  private static final class IntToLong extends ToNumeric {
+    @Override
+    long toLong(IndexInput input) throws IOException {
+      return input.readInt();
+    }
+
+    double toDouble(IndexInput input) throws IOException {
+      return Float.intBitsToFloat(input.readInt());
+    }
+  }
+
+  private static final class LongToLong extends ToNumeric {
+    @Override
+    long toLong(IndexInput input) throws IOException {
+      return input.readLong();
+    }
+
+    double toDouble(IndexInput input) throws IOException {
+      return Double.longBitsToDouble(input.readLong());
+    }
+  }
+
+}


diff -ruN -x .svn -x build lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/codecs/lucene40/values/DocValuesArray.java lucene-3622/lucene/src/java/org/apache/lucene/index/codecs/lucene40/values/DocValuesArray.java
--- lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/codecs/lucene40/values/DocValuesArray.java	1969-12-31 19:00:00.000000000 -0500
+++ lucene-3622/lucene/src/java/org/apache/lucene/index/codecs/lucene40/values/DocValuesArray.java	2011-12-09 11:14:29.392854015 -0500
@@ -0,0 +1,306 @@
+package org.apache.lucene.index.codecs.lucene40.values;
+
+import java.io.IOException;
+import java.util.Collections;
+import java.util.EnumMap;
+import java.util.Map;
+
+import org.apache.lucene.index.DocValues.Source;
+import org.apache.lucene.index.DocValues.Type;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.RamUsageEstimator;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements. See the NOTICE file distributed with this
+ * work for additional information regarding copyright ownership. The ASF
+ * licenses this file to You under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ * 
+ * http://www.apache.org/licenses/LICENSE-2.0
+ * 
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
+ * License for the specific language governing permissions and limitations under
+ * the License.
+ */
+
+/**
+ * @lucene.experimental
+ */
+abstract class DocValuesArray extends Source {
+
+  static final Map<Type, DocValuesArray> TEMPLATES;
+
+  static {
+    EnumMap<Type, DocValuesArray> templates = new EnumMap<Type, DocValuesArray>(
+        Type.class);
+    templates.put(Type.FIXED_INTS_16, new ShortValues());
+    templates.put(Type.FIXED_INTS_32, new IntValues());
+    templates.put(Type.FIXED_INTS_64, new LongValues());
+    templates.put(Type.FIXED_INTS_8, new ByteValues());
+    templates.put(Type.FLOAT_32, new FloatValues());
+    templates.put(Type.FLOAT_64, new DoubleValues());
+    TEMPLATES = Collections.unmodifiableMap(templates);
+  }
+
+  protected final int bytesPerValue;
+
+  DocValuesArray(int bytesPerValue, Type type) {
+    super(type);
+    this.bytesPerValue = bytesPerValue;
+  }
+
+  public abstract DocValuesArray newFromInput(IndexInput input, int numDocs)
+      throws IOException;
+
+  @Override
+  public final boolean hasArray() {
+    return true;
+  }
+
+  void toBytes(long value, BytesRef bytesRef) {
+    BytesRefUtils.copyLong(bytesRef, value);
+  }
+
+  void toBytes(double value, BytesRef bytesRef) {
+    BytesRefUtils.copyLong(bytesRef, Double.doubleToRawLongBits(value));
+  }
+
+  final static class ByteValues extends DocValuesArray {
+    private final byte[] values;
+
+    ByteValues() {
+      super(1, Type.FIXED_INTS_8);
+      values = new byte[0];
+    }
+
+    private ByteValues(IndexInput input, int numDocs) throws IOException {
+      super(1, Type.FIXED_INTS_8);
+      values = new byte[numDocs];
+      input.readBytes(values, 0, values.length, false);
+    }
+
+    @Override
+    public byte[] getArray() {
+      return values;
+    }
+
+    @Override
+    public long getInt(int docID) {
+      assert docID >= 0 && docID < values.length;
+      return values[docID];
+    }
+
+    @Override
+    public DocValuesArray newFromInput(IndexInput input, int numDocs)
+        throws IOException {
+      return new ByteValues(input, numDocs);
+    }
+
+    void toBytes(long value, BytesRef bytesRef) {
+      bytesRef.bytes[0] = (byte) (0xFFL & value);
+    }
+
+  };
+
+  final static class ShortValues extends DocValuesArray {
+    private final short[] values;
+
+    ShortValues() {
+      super(RamUsageEstimator.NUM_BYTES_SHORT, Type.FIXED_INTS_16);
+      values = new short[0];
+    }
+
+    private ShortValues(IndexInput input, int numDocs) throws IOException {
+      super(RamUsageEstimator.NUM_BYTES_SHORT, Type.FIXED_INTS_16);
+      values = new short[numDocs];
+      for (int i = 0; i < values.length; i++) {
+        values[i] = input.readShort();
+      }
+    }
+
+    @Override
+    public short[] getArray() {
+      return values;
+    }
+
+    @Override
+    public long getInt(int docID) {
+      assert docID >= 0 && docID < values.length;
+      return values[docID];
+    }
+
+    @Override
+    public DocValuesArray newFromInput(IndexInput input, int numDocs)
+        throws IOException {
+      return new ShortValues(input, numDocs);
+    }
+
+    void toBytes(long value, BytesRef bytesRef) {
+      BytesRefUtils.copyShort(bytesRef, (short) (0xFFFFL & value));
+    }
+
+  };
+
+  final static class IntValues extends DocValuesArray {
+    private final int[] values;
+
+    IntValues() {
+      super(RamUsageEstimator.NUM_BYTES_INT, Type.FIXED_INTS_32);
+      values = new int[0];
+    }
+
+    private IntValues(IndexInput input, int numDocs) throws IOException {
+      super(RamUsageEstimator.NUM_BYTES_INT, Type.FIXED_INTS_32);
+      values = new int[numDocs];
+      for (int i = 0; i < values.length; i++) {
+        values[i] = input.readInt();
+      }
+    }
+
+    @Override
+    public int[] getArray() {
+      return values;
+    }
+
+    @Override
+    public long getInt(int docID) {
+      assert docID >= 0 && docID < values.length;
+      return 0xFFFFFFFF & values[docID];
+    }
+
+    @Override
+    public DocValuesArray newFromInput(IndexInput input, int numDocs)
+        throws IOException {
+      return new IntValues(input, numDocs);
+    }
+
+    void toBytes(long value, BytesRef bytesRef) {
+      BytesRefUtils.copyInt(bytesRef, (int) (0xFFFFFFFF & value));
+    }
+
+  };
+
+  final static class LongValues extends DocValuesArray {
+    private final long[] values;
+
+    LongValues() {
+      super(RamUsageEstimator.NUM_BYTES_LONG, Type.FIXED_INTS_64);
+      values = new long[0];
+    }
+
+    private LongValues(IndexInput input, int numDocs) throws IOException {
+      super(RamUsageEstimator.NUM_BYTES_LONG, Type.FIXED_INTS_64);
+      values = new long[numDocs];
+      for (int i = 0; i < values.length; i++) {
+        values[i] = input.readLong();
+      }
+    }
+
+    @Override
+    public long[] getArray() {
+      return values;
+    }
+
+    @Override
+    public long getInt(int docID) {
+      assert docID >= 0 && docID < values.length;
+      return values[docID];
+    }
+
+    @Override
+    public DocValuesArray newFromInput(IndexInput input, int numDocs)
+        throws IOException {
+      return new LongValues(input, numDocs);
+    }
+
+  };
+
+  final static class FloatValues extends DocValuesArray {
+    private final float[] values;
+
+    FloatValues() {
+      super(RamUsageEstimator.NUM_BYTES_FLOAT, Type.FLOAT_32);
+      values = new float[0];
+    }
+
+    private FloatValues(IndexInput input, int numDocs) throws IOException {
+      super(RamUsageEstimator.NUM_BYTES_FLOAT, Type.FLOAT_32);
+      values = new float[numDocs];
+      /*
+       * we always read BIG_ENDIAN here since the writer serialized plain bytes
+       * we can simply read the ints / longs back in using readInt / readLong
+       */
+      for (int i = 0; i < values.length; i++) {
+        values[i] = Float.intBitsToFloat(input.readInt());
+      }
+    }
+
+    @Override
+    public float[] getArray() {
+      return values;
+    }
+
+    @Override
+    public double getFloat(int docID) {
+      assert docID >= 0 && docID < values.length;
+      return values[docID];
+    }
+    
+    @Override
+    void toBytes(double value, BytesRef bytesRef) {
+      BytesRefUtils.copyInt(bytesRef, Float.floatToRawIntBits((float)value));
+
+    }
+
+    @Override
+    public DocValuesArray newFromInput(IndexInput input, int numDocs)
+        throws IOException {
+      return new FloatValues(input, numDocs);
+    }
+  };
+
+  final static class DoubleValues extends DocValuesArray {
+    private final double[] values;
+
+    DoubleValues() {
+      super(RamUsageEstimator.NUM_BYTES_DOUBLE, Type.FLOAT_64);
+      values = new double[0];
+    }
+
+    private DoubleValues(IndexInput input, int numDocs) throws IOException {
+      super(RamUsageEstimator.NUM_BYTES_DOUBLE, Type.FLOAT_64);
+      values = new double[numDocs];
+      /*
+       * we always read BIG_ENDIAN here since the writer serialized plain bytes
+       * we can simply read the ints / longs back in using readInt / readLong
+       */
+      for (int i = 0; i < values.length; i++) {
+        values[i] = Double.longBitsToDouble(input.readLong());
+      }
+    }
+
+    @Override
+    public double[] getArray() {
+      return values;
+    }
+
+    @Override
+    public double getFloat(int docID) {
+      assert docID >= 0 && docID < values.length;
+      return values[docID];
+    }
+
+    @Override
+    public DocValuesArray newFromInput(IndexInput input, int numDocs)
+        throws IOException {
+      return new DoubleValues(input, numDocs);
+    }
+
+  };
+
+}


diff -ruN -x .svn -x build lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/codecs/lucene40/values/FixedDerefBytesImpl.java lucene-3622/lucene/src/java/org/apache/lucene/index/codecs/lucene40/values/FixedDerefBytesImpl.java
--- lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/codecs/lucene40/values/FixedDerefBytesImpl.java	1969-12-31 19:00:00.000000000 -0500
+++ lucene-3622/lucene/src/java/org/apache/lucene/index/codecs/lucene40/values/FixedDerefBytesImpl.java	2011-12-09 11:10:18.676849648 -0500
@@ -0,0 +1,134 @@
+package org.apache.lucene.index.codecs.lucene40.values;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.index.codecs.lucene40.values.Bytes.BytesReaderBase;
+import org.apache.lucene.index.codecs.lucene40.values.Bytes.BytesSourceBase;
+import org.apache.lucene.index.codecs.lucene40.values.Bytes.DerefBytesWriterBase;
+import org.apache.lucene.index.DocValues.Type;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.Counter;
+import org.apache.lucene.util.PagedBytes;
+import org.apache.lucene.util.packed.PackedInts;
+
+// Stores fixed-length byte[] by deref, ie when two docs
+// have the same value, they store only 1 byte[]
+/**
+ * @lucene.experimental
+ */
+class FixedDerefBytesImpl {
+
+  static final String CODEC_NAME = "FixedDerefBytes";
+  static final int VERSION_START = 0;
+  static final int VERSION_CURRENT = VERSION_START;
+
+  public static class Writer extends DerefBytesWriterBase {
+    public Writer(Directory dir, String id, Counter bytesUsed, IOContext context)
+        throws IOException {
+      super(dir, id, CODEC_NAME, VERSION_CURRENT, bytesUsed, context);
+    }
+
+    @Override
+    protected void finishInternal(int docCount) throws IOException {
+      final int numValues = hash.size();
+      final IndexOutput datOut = getOrCreateDataOut();
+      datOut.writeInt(size);
+      if (size != -1) {
+        final BytesRef bytesRef = new BytesRef(size);
+        for (int i = 0; i < numValues; i++) {
+          hash.get(i, bytesRef);
+          datOut.writeBytes(bytesRef.bytes, bytesRef.offset, bytesRef.length);
+        }
+      }
+      final IndexOutput idxOut = getOrCreateIndexOut();
+      idxOut.writeInt(numValues);
+      writeIndex(idxOut, docCount, numValues, docToEntry);
+    }
+  }
+
+  public static class FixedDerefReader extends BytesReaderBase {
+    private final int size;
+    private final int numValuesStored;
+    FixedDerefReader(Directory dir, String id, int maxDoc, IOContext context) throws IOException {
+      super(dir, id, CODEC_NAME, VERSION_START, true, context, Type.BYTES_FIXED_DEREF);
+      size = datIn.readInt();
+      numValuesStored = idxIn.readInt();
+    }
+
+    @Override
+    public Source load() throws IOException {
+      return new FixedDerefSource(cloneData(), cloneIndex(), size, numValuesStored);
+    }
+
+    @Override
+    public Source getDirectSource()
+        throws IOException {
+      return new DirectFixedDerefSource(cloneData(), cloneIndex(), size, type());
+    }
+
+    @Override
+    public int getValueSize() {
+      return size;
+    }
+    
+  }
+  
+  static final class FixedDerefSource extends BytesSourceBase {
+    private final int size;
+    private final PackedInts.Reader addresses;
+
+    protected FixedDerefSource(IndexInput datIn, IndexInput idxIn, int size, long numValues) throws IOException {
+      super(datIn, idxIn, new PagedBytes(PAGED_BYTES_BITS), size * numValues,
+          Type.BYTES_FIXED_DEREF);
+      this.size = size;
+      addresses = PackedInts.getReader(idxIn);
+    }
+
+    @Override
+    public BytesRef getBytes(int docID, BytesRef bytesRef) {
+      final int id = (int) addresses.get(docID);
+      return data.fillSlice(bytesRef, (id * size), size);
+    }
+
+  }
+  
+  final static class DirectFixedDerefSource extends DirectSource {
+    private final PackedInts.Reader index;
+    private final int size;
+
+    DirectFixedDerefSource(IndexInput data, IndexInput index, int size, Type type)
+        throws IOException {
+      super(data, type);
+      this.size = size;
+      this.index = PackedInts.getDirectReader(index);
+    }
+
+    @Override
+    protected int position(int docID) throws IOException {
+      data.seek(baseOffset + index.get(docID) * size);
+      return size;
+    }
+  }
+
+}


diff -ruN -x .svn -x build lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/codecs/lucene40/values/FixedSortedBytesImpl.java lucene-3622/lucene/src/java/org/apache/lucene/index/codecs/lucene40/values/FixedSortedBytesImpl.java
--- lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/codecs/lucene40/values/FixedSortedBytesImpl.java	1969-12-31 19:00:00.000000000 -0500
+++ lucene-3622/lucene/src/java/org/apache/lucene/index/codecs/lucene40/values/FixedSortedBytesImpl.java	2011-12-10 14:27:19.286560106 -0500
@@ -0,0 +1,230 @@
+package org.apache.lucene.index.codecs.lucene40.values;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Comparator;
+import java.util.List;
+
+import org.apache.lucene.index.DocValues;
+import org.apache.lucene.index.SortedBytesMergeUtils;
+import org.apache.lucene.index.DocValues.SortedSource;
+import org.apache.lucene.index.DocValues.Type;
+import org.apache.lucene.index.SortedBytesMergeUtils.MergeContext;
+import org.apache.lucene.index.SortedBytesMergeUtils.SortedSourceSlice;
+import org.apache.lucene.index.MergeState;
+import org.apache.lucene.index.codecs.lucene40.values.Bytes.BytesReaderBase;
+import org.apache.lucene.index.codecs.lucene40.values.Bytes.BytesSortedSourceBase;
+import org.apache.lucene.index.codecs.lucene40.values.Bytes.DerefBytesWriterBase;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.Counter;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util.packed.PackedInts;
+
+// Stores fixed-length byte[] by deref, ie when two docs
+// have the same value, they store only 1 byte[]
+
+/**
+ * @lucene.experimental
+ */
+class FixedSortedBytesImpl {
+
+  static final String CODEC_NAME = "FixedSortedBytes";
+  static final int VERSION_START = 0;
+  static final int VERSION_CURRENT = VERSION_START;
+
+  static final class Writer extends DerefBytesWriterBase {
+    private final Comparator<BytesRef> comp;
+
+    public Writer(Directory dir, String id, Comparator<BytesRef> comp,
+        Counter bytesUsed, IOContext context) throws IOException {
+      super(dir, id, CODEC_NAME, VERSION_CURRENT, bytesUsed, context);
+      this.comp = comp;
+    }
+
+    @Override
+    public void merge(MergeState mergeState, DocValues[] docValues)
+        throws IOException {
+      boolean success = false;
+      try {
+        final MergeContext ctx = SortedBytesMergeUtils.init(Type.BYTES_FIXED_SORTED, docValues, comp, mergeState);
+        List<SortedSourceSlice> slices = SortedBytesMergeUtils.buildSlices(mergeState, docValues, ctx);
+        final IndexOutput datOut = getOrCreateDataOut();
+        datOut.writeInt(ctx.sizePerValues);
+        final int maxOrd = SortedBytesMergeUtils.mergeRecords(ctx, datOut, slices);
+        
+        final IndexOutput idxOut = getOrCreateIndexOut();
+        idxOut.writeInt(maxOrd);
+        final PackedInts.Writer ordsWriter = PackedInts.getWriter(idxOut, ctx.docToEntry.length,
+            PackedInts.bitsRequired(maxOrd));
+        for (SortedSourceSlice slice : slices) {
+          slice.writeOrds(ordsWriter);
+        }
+        ordsWriter.finish();
+        success = true;
+      } finally {
+        releaseResources();
+        if (success) {
+          IOUtils.close(getIndexOut(), getDataOut());
+        } else {
+          IOUtils.closeWhileHandlingException(getIndexOut(), getDataOut());
+        }
+
+      }
+    }
+
+    // Important that we get docCount, in case there were
+    // some last docs that we didn't see
+    @Override
+    public void finishInternal(int docCount) throws IOException {
+      fillDefault(docCount);
+      final IndexOutput datOut = getOrCreateDataOut();
+      final int count = hash.size();
+      final int[] address = new int[count];
+      datOut.writeInt(size);
+      if (size != -1) {
+        final int[] sortedEntries = hash.sort(comp);
+        // first dump bytes data, recording address as we go
+        final BytesRef spare = new BytesRef(size);
+        for (int i = 0; i < count; i++) {
+          final int e = sortedEntries[i];
+          final BytesRef bytes = hash.get(e, spare);
+          assert bytes.length == size;
+          datOut.writeBytes(bytes.bytes, bytes.offset, bytes.length);
+          address[e] = i;
+        }
+      }
+      final IndexOutput idxOut = getOrCreateIndexOut();
+      idxOut.writeInt(count);
+      writeIndex(idxOut, docCount, count, address, docToEntry);
+    }
+  }
+
+  static final class Reader extends BytesReaderBase {
+    private final int size;
+    private final int valueCount;
+    private final Comparator<BytesRef> comparator;
+
+    public Reader(Directory dir, String id, int maxDoc, IOContext context,
+        Type type, Comparator<BytesRef> comparator) throws IOException {
+      super(dir, id, CODEC_NAME, VERSION_START, true, context, type);
+      size = datIn.readInt();
+      valueCount = idxIn.readInt();
+      this.comparator = comparator;
+    }
+
+    @Override
+    public Source load() throws IOException {
+      return new FixedSortedSource(cloneData(), cloneIndex(), size, valueCount,
+          comparator);
+    }
+
+    @Override
+    public Source getDirectSource() throws IOException {
+      return new DirectFixedSortedSource(cloneData(), cloneIndex(), size,
+          valueCount, comparator, type);
+    }
+
+    @Override
+    public int getValueSize() {
+      return size;
+    }
+  }
+
+  static final class FixedSortedSource extends BytesSortedSourceBase {
+    private final int valueCount;
+    private final int size;
+
+    FixedSortedSource(IndexInput datIn, IndexInput idxIn, int size,
+        int numValues, Comparator<BytesRef> comp) throws IOException {
+      super(datIn, idxIn, comp, size * numValues, Type.BYTES_FIXED_SORTED,
+          false);
+      this.size = size;
+      this.valueCount = numValues;
+      closeIndexInput();
+    }
+
+    @Override
+    public int getValueCount() {
+      return valueCount;
+    }
+
+    @Override
+    public BytesRef getByOrd(int ord, BytesRef bytesRef) {
+      return data.fillSlice(bytesRef, (ord * size), size);
+    }
+  }
+
+  static final class DirectFixedSortedSource extends SortedSource {
+    final PackedInts.Reader docToOrdIndex;
+    private final IndexInput datIn;
+    private final long basePointer;
+    private final int size;
+    private final int valueCount;
+
+    DirectFixedSortedSource(IndexInput datIn, IndexInput idxIn, int size,
+        int valueCount, Comparator<BytesRef> comp, Type type)
+        throws IOException {
+      super(type, comp);
+      docToOrdIndex = PackedInts.getDirectReader(idxIn);
+      basePointer = datIn.getFilePointer();
+      this.datIn = datIn;
+      this.size = size;
+      this.valueCount = valueCount;
+    }
+
+    @Override
+    public int ord(int docID) {
+      return (int) docToOrdIndex.get(docID);
+    }
+
+    @Override
+    public boolean hasPackedDocToOrd() {
+      return true;
+    }
+
+    @Override
+    public PackedInts.Reader getDocToOrd() {
+      return docToOrdIndex;
+    }
+
+    @Override
+    public BytesRef getByOrd(int ord, BytesRef bytesRef) {
+      try {
+        datIn.seek(basePointer + size * ord);
+        bytesRef.grow(size);
+        datIn.readBytes(bytesRef.bytes, 0, size);
+        bytesRef.length = size;
+        bytesRef.offset = 0;
+        return bytesRef;
+      } catch (IOException ex) {
+        throw new IllegalStateException("failed to getByOrd", ex);
+      }
+    }
+
+    @Override
+    public int getValueCount() {
+      return valueCount;
+    }
+  }
+
+}


diff -ruN -x .svn -x build lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/codecs/lucene40/values/FixedStraightBytesImpl.java lucene-3622/lucene/src/java/org/apache/lucene/index/codecs/lucene40/values/FixedStraightBytesImpl.java
--- lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/codecs/lucene40/values/FixedStraightBytesImpl.java	1969-12-31 19:00:00.000000000 -0500
+++ lucene-3622/lucene/src/java/org/apache/lucene/index/codecs/lucene40/values/FixedStraightBytesImpl.java	2011-12-09 11:10:18.680849649 -0500
@@ -0,0 +1,355 @@
+package org.apache.lucene.index.codecs.lucene40.values;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import static org.apache.lucene.util.ByteBlockPool.BYTE_BLOCK_SIZE;
+
+import java.io.IOException;
+
+import org.apache.lucene.index.DocValues;
+import org.apache.lucene.index.DocValues.Source;
+import org.apache.lucene.index.DocValues.Type;
+import org.apache.lucene.index.codecs.lucene40.values.Bytes.BytesReaderBase;
+import org.apache.lucene.index.codecs.lucene40.values.Bytes.BytesSourceBase;
+import org.apache.lucene.index.codecs.lucene40.values.Bytes.BytesWriterBase;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.util.ByteBlockPool;
+import org.apache.lucene.util.ByteBlockPool.DirectTrackingAllocator;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.Counter;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util.PagedBytes;
+
+// Simplest storage: stores fixed length byte[] per
+// document, with no dedup and no sorting.
+/**
+ * @lucene.experimental
+ */
+class FixedStraightBytesImpl {
+
+  static final String CODEC_NAME = "FixedStraightBytes";
+  static final int VERSION_START = 0;
+  static final int VERSION_CURRENT = VERSION_START;
+  
+  static abstract class FixedBytesWriterBase extends BytesWriterBase {
+    protected int lastDocID = -1;
+    // start at -1 if the first added value is > 0
+    protected int size = -1;
+    private final int byteBlockSize = BYTE_BLOCK_SIZE;
+    private final ByteBlockPool pool;
+
+    protected FixedBytesWriterBase(Directory dir, String id, String codecName,
+        int version, Counter bytesUsed, IOContext context) throws IOException {
+      super(dir, id, codecName, version, bytesUsed, context);
+      pool = new ByteBlockPool(new DirectTrackingAllocator(bytesUsed));
+      pool.nextBuffer();
+    }
+    
+    @Override
+    public void add(int docID, BytesRef bytes) throws IOException {
+      assert lastDocID < docID;
+
+      if (size == -1) {
+        if (bytes.length > BYTE_BLOCK_SIZE) {
+          throw new IllegalArgumentException("bytes arrays > " + Short.MAX_VALUE + " are not supported");
+        }
+        size = bytes.length;
+      } else if (bytes.length != size) {
+        throw new IllegalArgumentException("expected bytes size=" + size
+            + " but got " + bytes.length);
+      }
+      if (lastDocID+1 < docID) {
+        advancePool(docID);
+      }
+      pool.copy(bytes);
+      lastDocID = docID;
+    }
+    
+    private final void advancePool(int docID) {
+      long numBytes = (docID - (lastDocID+1))*size;
+      while(numBytes > 0) {
+        if (numBytes + pool.byteUpto < byteBlockSize) {
+          pool.byteUpto += numBytes;
+          numBytes = 0;
+        } else {
+          numBytes -= byteBlockSize - pool.byteUpto;
+          pool.nextBuffer();
+        }
+      }
+      assert numBytes == 0;
+    }
+    
+    protected void set(BytesRef ref, int docId) {
+      assert BYTE_BLOCK_SIZE % size == 0 : "BYTE_BLOCK_SIZE ("+ BYTE_BLOCK_SIZE + ") must be a multiple of the size: " + size;
+      ref.offset = docId*size;
+      ref.length = size;
+      pool.deref(ref);
+    }
+    
+    protected void resetPool() {
+      pool.dropBuffersAndReset();
+    }
+    
+    protected void writeData(IndexOutput out) throws IOException {
+      pool.writePool(out);
+    }
+    
+    protected void writeZeros(int num, IndexOutput out) throws IOException {
+      final byte[] zeros = new byte[size];
+      for (int i = 0; i < num; i++) {
+        out.writeBytes(zeros, zeros.length);
+      }
+    }
+  }
+
+  static class Writer extends FixedBytesWriterBase {
+    private boolean hasMerged;
+    private IndexOutput datOut;
+    
+    public Writer(Directory dir, String id, Counter bytesUsed, IOContext context) throws IOException {
+      super(dir, id, CODEC_NAME, VERSION_CURRENT, bytesUsed, context);
+    }
+
+    public Writer(Directory dir, String id, String codecName, int version, Counter bytesUsed, IOContext context) throws IOException {
+      super(dir, id, codecName, version, bytesUsed, context);
+    }
+
+
+    @Override
+    protected void merge(SingleSubMergeState state) throws IOException {
+      datOut = getOrCreateDataOut();
+      boolean success = false;
+      try {
+        if (!hasMerged && size != -1) {
+          datOut.writeInt(size);
+        }
+
+        if (state.liveDocs == null && tryBulkMerge(state.reader)) {
+          FixedStraightReader reader = (FixedStraightReader) state.reader;
+          final int maxDocs = reader.maxDoc;
+          if (maxDocs == 0) {
+            return;
+          }
+          if (size == -1) {
+            size = reader.size;
+            datOut.writeInt(size);
+          } else if (size != reader.size) {
+            throw new IllegalArgumentException("expected bytes size=" + size
+                + " but got " + reader.size);
+           }
+          if (lastDocID+1 < state.docBase) {
+            fill(datOut, state.docBase);
+            lastDocID = state.docBase-1;
+          }
+          // TODO should we add a transfer to API to each reader?
+          final IndexInput cloneData = reader.cloneData();
+          try {
+            datOut.copyBytes(cloneData, size * maxDocs);
+          } finally {
+            IOUtils.close(cloneData);  
+          }
+        
+          lastDocID += maxDocs;
+        } else {
+          super.merge(state);
+        }
+        success = true;
+      } finally {
+        if (!success) {
+          IOUtils.closeWhileHandlingException(datOut);
+        }
+        hasMerged = true;
+      }
+    }
+    
+    protected boolean tryBulkMerge(DocValues docValues) {
+      return docValues instanceof FixedStraightReader;
+    }
+    
+    @Override
+    protected void mergeDoc(int docID, int sourceDoc) throws IOException {
+      assert lastDocID < docID;
+      setMergeBytes(sourceDoc);
+      if (size == -1) {
+        size = bytesRef.length;
+        datOut.writeInt(size);
+      }
+      assert size == bytesRef.length : "size: " + size + " ref: " + bytesRef.length;
+      if (lastDocID+1 < docID) {
+        fill(datOut, docID);
+      }
+      datOut.writeBytes(bytesRef.bytes, bytesRef.offset, bytesRef.length);
+      lastDocID = docID;
+    }
+    
+    protected void setMergeBytes(int sourceDoc) {
+      currentMergeSource.getBytes(sourceDoc, bytesRef);
+    }
+
+
+
+    // Fills up to but not including this docID
+    private void fill(IndexOutput datOut, int docID) throws IOException {
+      assert size >= 0;
+      writeZeros((docID - (lastDocID+1)), datOut);
+    }
+
+    @Override
+    public void finish(int docCount) throws IOException {
+      boolean success = false;
+      try {
+        if (!hasMerged) {
+          // indexing path - no disk IO until here
+          assert datOut == null;
+          datOut = getOrCreateDataOut();
+          if (size == -1) {
+            datOut.writeInt(0);
+          } else {
+            datOut.writeInt(size);
+            writeData(datOut);
+          }
+          if (lastDocID + 1 < docCount) {
+            fill(datOut, docCount);
+          }
+        } else {
+          // merge path - datOut should be initialized
+          assert datOut != null;
+          if (size == -1) {// no data added
+            datOut.writeInt(0);
+          } else {
+            fill(datOut, docCount);
+          }
+        }
+        success = true;
+      } finally {
+        resetPool();
+        if (success) {
+          IOUtils.close(datOut);
+        } else {
+          IOUtils.closeWhileHandlingException(datOut);
+        }
+      }
+    }
+  
+  }
+  
+  public static class FixedStraightReader extends BytesReaderBase {
+    protected final int size;
+    protected final int maxDoc;
+    
+    FixedStraightReader(Directory dir, String id, int maxDoc, IOContext context) throws IOException {
+      this(dir, id, CODEC_NAME, VERSION_CURRENT, maxDoc, context, Type.BYTES_FIXED_STRAIGHT);
+    }
+
+    protected FixedStraightReader(Directory dir, String id, String codec, int version, int maxDoc, IOContext context, Type type) throws IOException {
+      super(dir, id, codec, version, false, context, type);
+      size = datIn.readInt();
+      this.maxDoc = maxDoc;
+    }
+
+    @Override
+    public Source load() throws IOException {
+      return size == 1 ? new SingleByteSource(cloneData(), maxDoc) : 
+        new FixedStraightSource(cloneData(), size, maxDoc, type);
+    }
+
+    @Override
+    public void close() throws IOException {
+      datIn.close();
+    }
+   
+    @Override
+    public Source getDirectSource() throws IOException {
+      return new DirectFixedStraightSource(cloneData(), size, type());
+    }
+    
+    @Override
+    public int getValueSize() {
+      return size;
+    }
+  }
+  
+  // specialized version for single bytes
+  private static final class SingleByteSource extends Source {
+    private final byte[] data;
+
+    public SingleByteSource(IndexInput datIn, int maxDoc) throws IOException {
+      super(Type.BYTES_FIXED_STRAIGHT);
+      try {
+        data = new byte[maxDoc];
+        datIn.readBytes(data, 0, data.length, false);
+      } finally {
+        IOUtils.close(datIn);
+      }
+    }
+    
+    @Override
+    public boolean hasArray() {
+      return true;
+    }
+
+    @Override
+    public Object getArray() {
+      return data;
+    }
+
+    @Override
+    public BytesRef getBytes(int docID, BytesRef bytesRef) {
+      bytesRef.length = 1;
+      bytesRef.bytes = data;
+      bytesRef.offset = docID;
+      return bytesRef;
+    }
+  }
+
+  
+  private final static class FixedStraightSource extends BytesSourceBase {
+    private final int size;
+
+    public FixedStraightSource(IndexInput datIn, int size, int maxDoc, Type type)
+        throws IOException {
+      super(datIn, null, new PagedBytes(PAGED_BYTES_BITS), size * maxDoc,
+          type);
+      this.size = size;
+    }
+
+    @Override
+    public BytesRef getBytes(int docID, BytesRef bytesRef) {
+      return data.fillSlice(bytesRef, docID * size, size);
+    }
+  }
+  
+  public final static class DirectFixedStraightSource extends DirectSource {
+    private final int size;
+
+    DirectFixedStraightSource(IndexInput input, int size, Type type) {
+      super(input, type);
+      this.size = size;
+    }
+
+    @Override
+    protected int position(int docID) throws IOException {
+      data.seek(baseOffset + size * docID);
+      return size;
+    }
+
+  }
+}


diff -ruN -x .svn -x build lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/codecs/lucene40/values/Floats.java lucene-3622/lucene/src/java/org/apache/lucene/index/codecs/lucene40/values/Floats.java
--- lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/codecs/lucene40/values/Floats.java	1969-12-31 19:00:00.000000000 -0500
+++ lucene-3622/lucene/src/java/org/apache/lucene/index/codecs/lucene40/values/Floats.java	2011-12-10 13:12:09.950481579 -0500
@@ -0,0 +1,126 @@
+package org.apache.lucene.index.codecs.lucene40.values;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+import java.io.IOException;
+
+import org.apache.lucene.index.DocValues;
+import org.apache.lucene.index.DocValue;
+import org.apache.lucene.index.DocValues.Source;
+import org.apache.lucene.index.DocValues.Type;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.Counter;
+import org.apache.lucene.util.IOUtils;
+
+/**
+ * Exposes {@link Writer} and reader ({@link Source}) for 32 bit and 64 bit
+ * floating point values.
+ * <p>
+ * Current implementations store either 4 byte or 8 byte floating points with
+ * full precision without any compression.
+ * 
+ * @lucene.experimental
+ */
+public class Floats {
+  
+  protected static final String CODEC_NAME = "Floats";
+  protected static final int VERSION_START = 0;
+  protected static final int VERSION_CURRENT = VERSION_START;
+  
+  public static Writer getWriter(Directory dir, String id, Counter bytesUsed,
+      IOContext context, Type type) throws IOException {
+    return new FloatsWriter(dir, id, bytesUsed, context, type);
+  }
+
+  public static DocValues getValues(Directory dir, String id, int maxDoc, IOContext context, Type type)
+      throws IOException {
+    return new FloatsReader(dir, id, maxDoc, context, type);
+  }
+  
+  private static int typeToSize(Type type) {
+    switch (type) {
+    case FLOAT_32:
+      return 4;
+    case FLOAT_64:
+      return 8;
+    default:
+      throw new IllegalStateException("illegal type " + type);
+    }
+  }
+  
+  final static class FloatsWriter extends FixedStraightBytesImpl.Writer {
+   
+    private final int size; 
+    private final DocValuesArray template;
+    public FloatsWriter(Directory dir, String id, Counter bytesUsed,
+        IOContext context, Type type) throws IOException {
+      super(dir, id, CODEC_NAME, VERSION_CURRENT, bytesUsed, context);
+      size = typeToSize(type);
+      this.bytesRef = new BytesRef(size);
+      bytesRef.length = size;
+      template = DocValuesArray.TEMPLATES.get(type);
+      assert template != null;
+    }
+    
+    public void add(int docID, double v) throws IOException {
+      template.toBytes(v, bytesRef);
+      add(docID, bytesRef);
+    }
+    
+    @Override
+    public void add(int docID, DocValue docValue) throws IOException {
+      add(docID, docValue.getFloat());
+    }
+    
+    @Override
+    protected boolean tryBulkMerge(DocValues docValues) {
+      // only bulk merge if value type is the same otherwise size differs
+      return super.tryBulkMerge(docValues) && docValues.type() == template.type();
+    }
+    
+    @Override
+    protected void setMergeBytes(int sourceDoc) {
+      final double value = currentMergeSource.getFloat(sourceDoc);
+      template.toBytes(value, bytesRef);
+    }
+  }
+  
+  final static class FloatsReader extends FixedStraightBytesImpl.FixedStraightReader {
+    final DocValuesArray arrayTemplate;
+    FloatsReader(Directory dir, String id, int maxDoc, IOContext context, Type type)
+        throws IOException {
+      super(dir, id, CODEC_NAME, VERSION_CURRENT, maxDoc, context, type);
+      arrayTemplate = DocValuesArray.TEMPLATES.get(type);
+      assert size == 4 || size == 8;
+    }
+    
+    @Override
+    public Source load() throws IOException {
+      final IndexInput indexInput = cloneData();
+      try {
+        return arrayTemplate.newFromInput(indexInput, maxDoc);
+      } finally {
+        IOUtils.close(indexInput);
+      }
+    }
+    
+  }
+
+}
\ No newline at end of file


diff -ruN -x .svn -x build lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/codecs/lucene40/values/Ints.java lucene-3622/lucene/src/java/org/apache/lucene/index/codecs/lucene40/values/Ints.java
--- lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/codecs/lucene40/values/Ints.java	1969-12-31 19:00:00.000000000 -0500
+++ lucene-3622/lucene/src/java/org/apache/lucene/index/codecs/lucene40/values/Ints.java	2011-12-10 13:10:38.810479992 -0500
@@ -0,0 +1,151 @@
+package org.apache.lucene.index.codecs.lucene40.values;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.index.DocValues;
+import org.apache.lucene.index.DocValues.Type;
+import org.apache.lucene.index.DocValue;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.Counter;
+import org.apache.lucene.util.IOUtils;
+
+/**
+ * Stores ints packed and fixed with fixed-bit precision.
+ * 
+ * @lucene.experimental
+ */
+public final class Ints {
+  protected static final String CODEC_NAME = "Ints";
+  protected static final int VERSION_START = 0;
+  protected static final int VERSION_CURRENT = VERSION_START;
+
+  private Ints() {
+  }
+  
+  public static Writer getWriter(Directory dir, String id, Counter bytesUsed,
+      Type type, IOContext context) throws IOException {
+    return type == Type.VAR_INTS ? new PackedIntValues.PackedIntsWriter(dir, id,
+        bytesUsed, context) : new IntsWriter(dir, id, bytesUsed, context, type);
+  }
+
+  public static DocValues getValues(Directory dir, String id, int numDocs,
+      Type type, IOContext context) throws IOException {
+    return type == Type.VAR_INTS ? new PackedIntValues.PackedIntsReader(dir, id,
+        numDocs, context) : new IntsReader(dir, id, numDocs, context, type);
+  }
+  
+  private static Type sizeToType(int size) {
+    switch (size) {
+    case 1:
+      return Type.FIXED_INTS_8;
+    case 2:
+      return Type.FIXED_INTS_16;
+    case 4:
+      return Type.FIXED_INTS_32;
+    case 8:
+      return Type.FIXED_INTS_64;
+    default:
+      throw new IllegalStateException("illegal size " + size);
+    }
+  }
+  
+  private static int typeToSize(Type type) {
+    switch (type) {
+    case FIXED_INTS_16:
+      return 2;
+    case FIXED_INTS_32:
+      return 4;
+    case FIXED_INTS_64:
+      return 8;
+    case FIXED_INTS_8:
+      return 1;
+    default:
+      throw new IllegalStateException("illegal type " + type);
+    }
+  }
+
+
+  static class IntsWriter extends FixedStraightBytesImpl.Writer {
+    private final DocValuesArray template;
+
+    public IntsWriter(Directory dir, String id, Counter bytesUsed,
+        IOContext context, Type valueType) throws IOException {
+      this(dir, id, CODEC_NAME, VERSION_CURRENT, bytesUsed, context, valueType);
+    }
+
+    protected IntsWriter(Directory dir, String id, String codecName,
+        int version, Counter bytesUsed, IOContext context, Type valueType) throws IOException {
+      super(dir, id, codecName, version, bytesUsed, context);
+      size = typeToSize(valueType);
+      this.bytesRef = new BytesRef(size);
+      bytesRef.length = size;
+      template = DocValuesArray.TEMPLATES.get(valueType);
+    }
+    
+    @Override
+    public void add(int docID, long v) throws IOException {
+      template.toBytes(v, bytesRef);
+      add(docID, bytesRef);
+    }
+
+    @Override
+    public void add(int docID, DocValue docValue) throws IOException {
+      add(docID, docValue.getInt());
+    }
+    
+    @Override
+    protected void setMergeBytes(int sourceDoc) {
+      final long value = currentMergeSource.getInt(sourceDoc);
+      template.toBytes(value, bytesRef);
+    }
+    
+    @Override
+    protected boolean tryBulkMerge(DocValues docValues) {
+      // only bulk merge if value type is the same otherwise size differs
+      return super.tryBulkMerge(docValues) && docValues.type() == template.type();
+    }
+  }
+  
+  final static class IntsReader extends FixedStraightBytesImpl.FixedStraightReader {
+    private final DocValuesArray arrayTemplate;
+
+    IntsReader(Directory dir, String id, int maxDoc, IOContext context, Type type)
+        throws IOException {
+      super(dir, id, CODEC_NAME, VERSION_CURRENT, maxDoc,
+          context, type);
+      arrayTemplate = DocValuesArray.TEMPLATES.get(type);
+      assert arrayTemplate != null;
+      assert type == sizeToType(size);
+    }
+
+    @Override
+    public Source load() throws IOException {
+      final IndexInput indexInput = cloneData();
+      try {
+        return arrayTemplate.newFromInput(indexInput, maxDoc);
+      } finally {
+        IOUtils.close(indexInput);
+      }
+    }
+  }
+}


diff -ruN -x .svn -x build lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/codecs/lucene40/values/PackedIntValues.java lucene-3622/lucene/src/java/org/apache/lucene/index/codecs/lucene40/values/PackedIntValues.java
--- lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/codecs/lucene40/values/PackedIntValues.java	1969-12-31 19:00:00.000000000 -0500
+++ lucene-3622/lucene/src/java/org/apache/lucene/index/codecs/lucene40/values/PackedIntValues.java	2011-12-10 13:11:08.202480504 -0500
@@ -0,0 +1,265 @@
+package org.apache.lucene.index.codecs.lucene40.values;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+import java.io.IOException;
+
+import org.apache.lucene.index.DocValues;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.DocValue;
+import org.apache.lucene.index.DocValues.Source;
+import org.apache.lucene.index.DocValues.Type;
+import org.apache.lucene.index.codecs.lucene40.values.FixedStraightBytesImpl.FixedBytesWriterBase;
+import org.apache.lucene.index.codecs.lucene40.values.DocValuesArray.LongValues;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.CodecUtil;
+import org.apache.lucene.util.Counter;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util.packed.PackedInts;
+
+/**
+ * Stores integers using {@link PackedInts}
+ * 
+ * @lucene.experimental
+ * */
+class PackedIntValues {
+
+  private static final String CODEC_NAME = "PackedInts";
+  private static final byte PACKED = 0x00;
+  private static final byte FIXED_64 = 0x01;
+
+  static final int VERSION_START = 0;
+  static final int VERSION_CURRENT = VERSION_START;
+
+  static class PackedIntsWriter extends FixedBytesWriterBase {
+
+    private long minValue;
+    private long maxValue;
+    private boolean started;
+    private int lastDocId = -1;
+
+    protected PackedIntsWriter(Directory dir, String id, Counter bytesUsed,
+        IOContext context) throws IOException {
+      super(dir, id, CODEC_NAME, VERSION_CURRENT, bytesUsed, context);
+      bytesRef = new BytesRef(8);
+    }
+
+    @Override
+    public void add(int docID, long v) throws IOException {
+      assert lastDocId < docID;
+      if (!started) {
+        started = true;
+        minValue = maxValue = v;
+      } else {
+        if (v < minValue) {
+          minValue = v;
+        } else if (v > maxValue) {
+          maxValue = v;
+        }
+      }
+      lastDocId = docID;
+      BytesRefUtils.copyLong(bytesRef, v);
+      add(docID, bytesRef);
+    }
+
+    @Override
+    public void finish(int docCount) throws IOException {
+      boolean success = false;
+      final IndexOutput dataOut = getOrCreateDataOut();
+      try {
+        if (!started) {
+          minValue = maxValue = 0;
+        }
+        final long delta = maxValue - minValue;
+        // if we exceed the range of positive longs we must switch to fixed
+        // ints
+        if (delta <= (maxValue >= 0 && minValue <= 0 ? Long.MAX_VALUE
+            : Long.MAX_VALUE - 1) && delta >= 0) {
+          dataOut.writeByte(PACKED);
+          writePackedInts(dataOut, docCount);
+          return; // done
+        } else {
+          dataOut.writeByte(FIXED_64);
+        }
+        writeData(dataOut);
+        writeZeros(docCount - (lastDocID + 1), dataOut);
+        success = true;
+      } finally {
+        resetPool();
+        if (success) {
+          IOUtils.close(dataOut);
+        } else {
+          IOUtils.closeWhileHandlingException(dataOut);
+        }
+      }
+    }
+
+    @Override
+    protected void mergeDoc(int docID, int sourceDoc) throws IOException {
+      assert docID > lastDocId : "docID: " + docID
+          + " must be greater than the last added doc id: " + lastDocId;
+        add(docID, currentMergeSource.getInt(sourceDoc));
+    }
+
+    private void writePackedInts(IndexOutput datOut, int docCount) throws IOException {
+      datOut.writeLong(minValue);
+      
+      // write a default value to recognize docs without a value for that
+      // field
+      final long defaultValue = maxValue >= 0 && minValue <= 0 ? 0 - minValue
+          : ++maxValue - minValue;
+      datOut.writeLong(defaultValue);
+      PackedInts.Writer w = PackedInts.getWriter(datOut, docCount,
+          PackedInts.bitsRequired(maxValue - minValue));
+      for (int i = 0; i < lastDocID + 1; i++) {
+        set(bytesRef, i);
+        byte[] bytes = bytesRef.bytes;
+        int offset = bytesRef.offset;
+        long asLong =  
+           (((long)(bytes[offset+0] & 0xff) << 56) |
+            ((long)(bytes[offset+1] & 0xff) << 48) |
+            ((long)(bytes[offset+2] & 0xff) << 40) |
+            ((long)(bytes[offset+3] & 0xff) << 32) |
+            ((long)(bytes[offset+4] & 0xff) << 24) |
+            ((long)(bytes[offset+5] & 0xff) << 16) |
+            ((long)(bytes[offset+6] & 0xff) <<  8) |
+            ((long)(bytes[offset+7] & 0xff)));
+        w.add(asLong == 0 ? defaultValue : asLong - minValue);
+      }
+      for (int i = lastDocID + 1; i < docCount; i++) {
+        w.add(defaultValue);
+      }
+      w.finish();
+    }
+
+    @Override
+    public void add(int docID, DocValue docValue) throws IOException {
+      add(docID, docValue.getInt());
+    }
+  }
+
+  /**
+   * Opens all necessary files, but does not read any data in until you call
+   * {@link #load}.
+   */
+  static class PackedIntsReader extends DocValues {
+    private final IndexInput datIn;
+    private final byte type;
+    private final int numDocs;
+    private final LongValues values;
+
+    protected PackedIntsReader(Directory dir, String id, int numDocs,
+        IOContext context) throws IOException {
+      datIn = dir.openInput(
+                IndexFileNames.segmentFileName(id, Bytes.DV_SEGMENT_SUFFIX, Writer.DATA_EXTENSION),
+          context);
+      this.numDocs = numDocs;
+      boolean success = false;
+      try {
+        CodecUtil.checkHeader(datIn, CODEC_NAME, VERSION_START, VERSION_START);
+        type = datIn.readByte();
+        values = type == FIXED_64 ? new LongValues() : null;
+        success = true;
+      } finally {
+        if (!success) {
+          IOUtils.closeWhileHandlingException(datIn);
+        }
+      }
+    }
+
+
+    /**
+     * Loads the actual values. You may call this more than once, eg if you
+     * already previously loaded but then discarded the Source.
+     */
+    @Override
+    public Source load() throws IOException {
+      boolean success = false;
+      final Source source;
+      IndexInput input = null;
+      try {
+        input = (IndexInput) datIn.clone();
+        
+        if (values == null) {
+          source = new PackedIntsSource(input, false);
+        } else {
+          source = values.newFromInput(input, numDocs);
+        }
+        success = true;
+        return source;
+      } finally {
+        if (!success) {
+          IOUtils.closeWhileHandlingException(input, datIn);
+        }
+      }
+    }
+
+    @Override
+    public void close() throws IOException {
+      super.close();
+      datIn.close();
+    }
+
+
+    @Override
+    public Type type() {
+      return Type.VAR_INTS;
+    }
+
+
+    @Override
+    public Source getDirectSource() throws IOException {
+      return values != null ? new FixedStraightBytesImpl.DirectFixedStraightSource((IndexInput) datIn.clone(), 8, Type.FIXED_INTS_64) : new PackedIntsSource((IndexInput) datIn.clone(), true);
+    }
+  }
+
+  
+  static class PackedIntsSource extends Source {
+    private final long minValue;
+    private final long defaultValue;
+    private final PackedInts.Reader values;
+
+    public PackedIntsSource(IndexInput dataIn, boolean direct) throws IOException {
+      super(Type.VAR_INTS);
+      minValue = dataIn.readLong();
+      defaultValue = dataIn.readLong();
+      values = direct ? PackedInts.getDirectReader(dataIn) : PackedInts.getReader(dataIn);
+    }
+    
+    @Override
+    public BytesRef getBytes(int docID, BytesRef ref) {
+      ref.grow(8);
+      BytesRefUtils.copyLong(ref, getInt(docID));
+      return ref;
+    }
+
+    @Override
+    public long getInt(int docID) {
+      // TODO -- can we somehow avoid 2X method calls
+      // on each get? must push minValue down, and make
+      // PackedInts implement Ints.Source
+      assert docID >= 0;
+      final long value = values.get(docID);
+      return value == defaultValue ? 0 : minValue + value;
+    }
+  }
+
+}
\ No newline at end of file


diff -ruN -x .svn -x build lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/codecs/lucene40/values/VarDerefBytesImpl.java lucene-3622/lucene/src/java/org/apache/lucene/index/codecs/lucene40/values/VarDerefBytesImpl.java
--- lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/codecs/lucene40/values/VarDerefBytesImpl.java	1969-12-31 19:00:00.000000000 -0500
+++ lucene-3622/lucene/src/java/org/apache/lucene/index/codecs/lucene40/values/VarDerefBytesImpl.java	2011-12-09 11:10:18.876849652 -0500
@@ -0,0 +1,151 @@
+package org.apache.lucene.index.codecs.lucene40.values;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.index.codecs.lucene40.values.Bytes.BytesReaderBase;
+import org.apache.lucene.index.codecs.lucene40.values.Bytes.BytesSourceBase;
+import org.apache.lucene.index.codecs.lucene40.values.Bytes.DerefBytesWriterBase;
+import org.apache.lucene.index.DocValues.Type;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.Counter;
+import org.apache.lucene.util.PagedBytes;
+import org.apache.lucene.util.packed.PackedInts;
+
+// Stores variable-length byte[] by deref, ie when two docs
+// have the same value, they store only 1 byte[] and both
+// docs reference that single source
+
+/**
+ * @lucene.experimental
+ */
+class VarDerefBytesImpl {
+
+  static final String CODEC_NAME = "VarDerefBytes";
+  static final int VERSION_START = 0;
+  static final int VERSION_CURRENT = VERSION_START;
+
+  /*
+   * TODO: if impls like this are merged we are bound to the amount of memory we
+   * can store into a BytesRefHash and therefore how much memory a ByteBlockPool
+   * can address. This is currently limited to 2GB. While we could extend that
+   * and use 64bit for addressing this still limits us to the existing main
+   * memory as all distinct bytes will be loaded up into main memory. We could
+   * move the byte[] writing to #finish(int) and store the bytes in sorted
+   * order and merge them in a streamed fashion. 
+   */
+  static class Writer extends DerefBytesWriterBase {
+    public Writer(Directory dir, String id, Counter bytesUsed, IOContext context)
+        throws IOException {
+      super(dir, id, CODEC_NAME, VERSION_CURRENT, bytesUsed, context);
+      size = 0;
+    }
+    
+    @Override
+    protected void checkSize(BytesRef bytes) {
+      // allow var bytes sizes
+    }
+
+    // Important that we get docCount, in case there were
+    // some last docs that we didn't see
+    @Override
+    public void finishInternal(int docCount) throws IOException {
+      fillDefault(docCount);
+      final int size = hash.size();
+      final long[] addresses = new long[size];
+      final IndexOutput datOut = getOrCreateDataOut();
+      int addr = 0;
+      final BytesRef bytesRef = new BytesRef();
+      for (int i = 0; i < size; i++) {
+        hash.get(i, bytesRef);
+        addresses[i] = addr;
+        addr += writePrefixLength(datOut, bytesRef) + bytesRef.length;
+        datOut.writeBytes(bytesRef.bytes, bytesRef.offset, bytesRef.length);
+      }
+
+      final IndexOutput idxOut = getOrCreateIndexOut();
+      // write the max address to read directly on source load
+      idxOut.writeLong(addr);
+      writeIndex(idxOut, docCount, addresses[addresses.length-1], addresses, docToEntry);
+    }
+  }
+
+  public static class VarDerefReader extends BytesReaderBase {
+    private final long totalBytes;
+    VarDerefReader(Directory dir, String id, int maxDoc, IOContext context) throws IOException {
+      super(dir, id, CODEC_NAME, VERSION_START, true, context, Type.BYTES_VAR_DEREF);
+      totalBytes = idxIn.readLong();
+    }
+
+    @Override
+    public Source load() throws IOException {
+      return new VarDerefSource(cloneData(), cloneIndex(), totalBytes);
+    }
+   
+    @Override
+    public Source getDirectSource()
+        throws IOException {
+      return new DirectVarDerefSource(cloneData(), cloneIndex(), type());
+    }
+  }
+  
+  final static class VarDerefSource extends BytesSourceBase {
+    private final PackedInts.Reader addresses;
+
+    public VarDerefSource(IndexInput datIn, IndexInput idxIn, long totalBytes)
+        throws IOException {
+      super(datIn, idxIn, new PagedBytes(PAGED_BYTES_BITS), totalBytes,
+          Type.BYTES_VAR_DEREF);
+      addresses = PackedInts.getReader(idxIn);
+    }
+
+    @Override
+    public BytesRef getBytes(int docID, BytesRef bytesRef) {
+      return data.fillSliceWithPrefix(bytesRef,
+          addresses.get(docID));
+    }
+  }
+
+  
+  final static class DirectVarDerefSource extends DirectSource {
+    private final PackedInts.Reader index;
+
+    DirectVarDerefSource(IndexInput data, IndexInput index, Type type)
+        throws IOException {
+      super(data, type);
+      this.index = PackedInts.getDirectReader(index);
+    }
+    
+    @Override
+    protected int position(int docID) throws IOException {
+      data.seek(baseOffset + index.get(docID));
+      final byte sizeByte = data.readByte();
+      if ((sizeByte & 128) == 0) {
+        // length is 1 byte
+        return sizeByte;
+      } else {
+        return ((sizeByte & 0x7f) << 8) | ((data.readByte() & 0xff));
+      }
+    }
+  }
+}


diff -ruN -x .svn -x build lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/codecs/lucene40/values/VarSortedBytesImpl.java lucene-3622/lucene/src/java/org/apache/lucene/index/codecs/lucene40/values/VarSortedBytesImpl.java
--- lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/codecs/lucene40/values/VarSortedBytesImpl.java	1969-12-31 19:00:00.000000000 -0500
+++ lucene-3622/lucene/src/java/org/apache/lucene/index/codecs/lucene40/values/VarSortedBytesImpl.java	2011-12-10 14:27:19.314560107 -0500
@@ -0,0 +1,254 @@
+package org.apache.lucene.index.codecs.lucene40.values;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Comparator;
+import java.util.List;
+
+import org.apache.lucene.index.DocValues;
+import org.apache.lucene.index.SortedBytesMergeUtils;
+import org.apache.lucene.index.DocValues.SortedSource;
+import org.apache.lucene.index.DocValues.Type;
+import org.apache.lucene.index.SortedBytesMergeUtils.MergeContext;
+import org.apache.lucene.index.SortedBytesMergeUtils.SortedSourceSlice;
+import org.apache.lucene.index.MergeState;
+import org.apache.lucene.index.codecs.lucene40.values.Bytes.BytesReaderBase;
+import org.apache.lucene.index.codecs.lucene40.values.Bytes.BytesSortedSourceBase;
+import org.apache.lucene.index.codecs.lucene40.values.Bytes.DerefBytesWriterBase;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.Counter;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util.packed.PackedInts;
+
+// Stores variable-length byte[] by deref, ie when two docs
+// have the same value, they store only 1 byte[] and both
+// docs reference that single source
+
+/**
+ * @lucene.experimental
+ */
+final class VarSortedBytesImpl {
+
+  static final String CODEC_NAME = "VarDerefBytes";
+  static final int VERSION_START = 0;
+  static final int VERSION_CURRENT = VERSION_START;
+
+  final static class Writer extends DerefBytesWriterBase {
+    private final Comparator<BytesRef> comp;
+
+    public Writer(Directory dir, String id, Comparator<BytesRef> comp,
+        Counter bytesUsed, IOContext context) throws IOException {
+      super(dir, id, CODEC_NAME, VERSION_CURRENT, bytesUsed, context);
+      this.comp = comp;
+      size = 0;
+    }
+    @Override
+    public void merge(MergeState mergeState, DocValues[] docValues)
+        throws IOException {
+      boolean success = false;
+      try {
+        MergeContext ctx = SortedBytesMergeUtils.init(Type.BYTES_VAR_SORTED, docValues, comp, mergeState);
+        final List<SortedSourceSlice> slices = SortedBytesMergeUtils.buildSlices(mergeState, docValues, ctx);
+        IndexOutput datOut = getOrCreateDataOut();
+        
+        ctx.offsets = new long[1];
+        final int maxOrd = SortedBytesMergeUtils.mergeRecords(ctx, datOut, slices);
+        final long[] offsets = ctx.offsets;
+        maxBytes = offsets[maxOrd-1];
+        final IndexOutput idxOut = getOrCreateIndexOut();
+        
+        idxOut.writeLong(maxBytes);
+        final PackedInts.Writer offsetWriter = PackedInts.getWriter(idxOut, maxOrd+1,
+            PackedInts.bitsRequired(maxBytes));
+        offsetWriter.add(0);
+        for (int i = 0; i < maxOrd; i++) {
+          offsetWriter.add(offsets[i]);
+        }
+        offsetWriter.finish();
+        
+        final PackedInts.Writer ordsWriter = PackedInts.getWriter(idxOut, ctx.docToEntry.length,
+            PackedInts.bitsRequired(maxOrd-1));
+        for (SortedSourceSlice slice : slices) {
+          slice.writeOrds(ordsWriter);
+        }
+        ordsWriter.finish();
+        success = true;
+      } finally {
+        releaseResources();
+        if (success) {
+          IOUtils.close(getIndexOut(), getDataOut());
+        } else {
+          IOUtils.closeWhileHandlingException(getIndexOut(), getDataOut());
+        }
+
+      }
+    }
+
+    @Override
+    protected void checkSize(BytesRef bytes) {
+      // allow var bytes sizes
+    }
+
+    // Important that we get docCount, in case there were
+    // some last docs that we didn't see
+    @Override
+    public void finishInternal(int docCount) throws IOException {
+      fillDefault(docCount);
+      final int count = hash.size();
+      final IndexOutput datOut = getOrCreateDataOut();
+      final IndexOutput idxOut = getOrCreateIndexOut();
+      long offset = 0;
+      final int[] index = new int[count];
+      final int[] sortedEntries = hash.sort(comp);
+      // total bytes of data
+      idxOut.writeLong(maxBytes);
+      PackedInts.Writer offsetWriter = PackedInts.getWriter(idxOut, count+1,
+          PackedInts.bitsRequired(maxBytes));
+      // first dump bytes data, recording index & write offset as
+      // we go
+      final BytesRef spare = new BytesRef();
+      for (int i = 0; i < count; i++) {
+        final int e = sortedEntries[i];
+        offsetWriter.add(offset);
+        index[e] = i;
+        final BytesRef bytes = hash.get(e, spare);
+        // TODO: we could prefix code...
+        datOut.writeBytes(bytes.bytes, bytes.offset, bytes.length);
+        offset += bytes.length;
+      }
+      // write sentinel
+      offsetWriter.add(offset);
+      offsetWriter.finish();
+      // write index
+      writeIndex(idxOut, docCount, count, index, docToEntry);
+
+    }
+  }
+
+  public static class Reader extends BytesReaderBase {
+
+    private final Comparator<BytesRef> comparator;
+
+    Reader(Directory dir, String id, int maxDoc,
+        IOContext context, Type type, Comparator<BytesRef> comparator)
+        throws IOException {
+      super(dir, id, CODEC_NAME, VERSION_START, true, context, type);
+      this.comparator = comparator;
+    }
+
+    @Override
+    public org.apache.lucene.index.DocValues.Source load()
+        throws IOException {
+      return new VarSortedSource(cloneData(), cloneIndex(), comparator);
+    }
+
+    @Override
+    public Source getDirectSource() throws IOException {
+      return new DirectSortedSource(cloneData(), cloneIndex(), comparator, type());
+    }
+    
+  }
+  private static final class VarSortedSource extends BytesSortedSourceBase {
+    private final int valueCount;
+
+    VarSortedSource(IndexInput datIn, IndexInput idxIn,
+        Comparator<BytesRef> comp) throws IOException {
+      super(datIn, idxIn, comp, idxIn.readLong(), Type.BYTES_VAR_SORTED, true);
+      valueCount = ordToOffsetIndex.size()-1; // the last value here is just a dummy value to get the length of the last value
+      closeIndexInput();
+    }
+
+    @Override
+    public BytesRef getByOrd(int ord, BytesRef bytesRef) {
+      final long offset = ordToOffsetIndex.get(ord);
+      final long nextOffset = ordToOffsetIndex.get(1 + ord);
+      data.fillSlice(bytesRef, offset, (int) (nextOffset - offset));
+      return bytesRef;
+    }
+
+    @Override
+    public int getValueCount() {
+      return valueCount;
+    }
+  }
+
+  private static final class DirectSortedSource extends SortedSource {
+    private final PackedInts.Reader docToOrdIndex;
+    private final PackedInts.Reader ordToOffsetIndex;
+    private final IndexInput datIn;
+    private final long basePointer;
+    private final int valueCount;
+    
+    DirectSortedSource(IndexInput datIn, IndexInput idxIn,
+        Comparator<BytesRef> comparator, Type type) throws IOException {
+      super(type, comparator);
+      idxIn.readLong();
+      ordToOffsetIndex = PackedInts.getDirectReader(idxIn);
+      valueCount = ordToOffsetIndex.size()-1; // the last value here is just a dummy value to get the length of the last value
+      // advance this iterator to the end and clone the stream once it points to the docToOrdIndex header
+      ordToOffsetIndex.get(valueCount);
+      docToOrdIndex = PackedInts.getDirectReader((IndexInput) idxIn.clone()); // read the ords in to prevent too many random disk seeks
+      basePointer = datIn.getFilePointer();
+      this.datIn = datIn;
+    }
+
+    @Override
+    public int ord(int docID) {
+      return (int) docToOrdIndex.get(docID);
+    }
+
+    @Override
+    public boolean hasPackedDocToOrd() {
+      return true;
+    }
+
+    @Override
+    public PackedInts.Reader getDocToOrd() {
+      return docToOrdIndex;
+    }
+
+    @Override
+    public BytesRef getByOrd(int ord, BytesRef bytesRef) {
+      try {
+        final long offset = ordToOffsetIndex.get(ord);
+        // 1+ord is safe because we write a sentinel at the end
+        final long nextOffset = ordToOffsetIndex.get(1+ord);
+        datIn.seek(basePointer + offset);
+        final int length = (int) (nextOffset - offset);
+        bytesRef.grow(length);
+        datIn.readBytes(bytesRef.bytes, 0, length);
+        bytesRef.length = length;
+        bytesRef.offset = 0;
+        return bytesRef;
+      } catch (IOException ex) {
+        throw new IllegalStateException("failed", ex);
+      }
+    }
+    
+    @Override
+    public int getValueCount() {
+      return valueCount;
+    }
+
+  }
+}


diff -ruN -x .svn -x build lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/codecs/lucene40/values/VarStraightBytesImpl.java lucene-3622/lucene/src/java/org/apache/lucene/index/codecs/lucene40/values/VarStraightBytesImpl.java
--- lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/codecs/lucene40/values/VarStraightBytesImpl.java	1969-12-31 19:00:00.000000000 -0500
+++ lucene-3622/lucene/src/java/org/apache/lucene/index/codecs/lucene40/values/VarStraightBytesImpl.java	2011-12-09 11:10:18.880849652 -0500
@@ -0,0 +1,285 @@
+package org.apache.lucene.index.codecs.lucene40.values;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.index.codecs.lucene40.values.Bytes.BytesReaderBase;
+import org.apache.lucene.index.codecs.lucene40.values.Bytes.BytesSourceBase;
+import org.apache.lucene.index.codecs.lucene40.values.Bytes.BytesWriterBase;
+import org.apache.lucene.index.DocValues.Type;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.util.ArrayUtil;
+import org.apache.lucene.util.ByteBlockPool;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.Counter;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util.PagedBytes;
+import org.apache.lucene.util.RamUsageEstimator;
+import org.apache.lucene.util.ByteBlockPool.DirectTrackingAllocator;
+import org.apache.lucene.util.packed.PackedInts;
+import org.apache.lucene.util.packed.PackedInts.ReaderIterator;
+
+// Variable length byte[] per document, no sharing
+
+/**
+ * @lucene.experimental
+ */
+class VarStraightBytesImpl {
+
+  static final String CODEC_NAME = "VarStraightBytes";
+  static final int VERSION_START = 0;
+  static final int VERSION_CURRENT = VERSION_START;
+
+  static class Writer extends BytesWriterBase {
+    private long address;
+    // start at -1 if the first added value is > 0
+    private int lastDocID = -1;
+    private long[] docToAddress;
+    private final ByteBlockPool pool;
+    private IndexOutput datOut;
+    private boolean merge = false;
+    public Writer(Directory dir, String id, Counter bytesUsed, IOContext context)
+        throws IOException {
+      super(dir, id, CODEC_NAME, VERSION_CURRENT, bytesUsed, context);
+      pool = new ByteBlockPool(new DirectTrackingAllocator(bytesUsed));
+      docToAddress = new long[1];
+      pool.nextBuffer(); // init
+      bytesUsed.addAndGet(RamUsageEstimator.NUM_BYTES_INT);
+    }
+
+    // Fills up to but not including this docID
+    private void fill(final int docID, final long nextAddress) {
+      if (docID >= docToAddress.length) {
+        int oldSize = docToAddress.length;
+        docToAddress = ArrayUtil.grow(docToAddress, 1 + docID);
+        bytesUsed.addAndGet((docToAddress.length - oldSize)
+            * RamUsageEstimator.NUM_BYTES_INT);
+      }
+      for (int i = lastDocID + 1; i < docID; i++) {
+        docToAddress[i] = nextAddress;
+      }
+    }
+
+    @Override
+    public void add(int docID, BytesRef bytes) throws IOException {
+      assert !merge;
+      if (bytes.length == 0) {
+        return; // default
+      }
+      fill(docID, address);
+      docToAddress[docID] = address;
+      pool.copy(bytes);
+      address += bytes.length;
+      lastDocID = docID;
+    }
+    
+    @Override
+    protected void merge(SingleSubMergeState state) throws IOException {
+      merge = true;
+      datOut = getOrCreateDataOut();
+      boolean success = false;
+      try {
+        if (state.liveDocs == null && state.reader instanceof VarStraightReader) {
+          // bulk merge since we don't have any deletes
+          VarStraightReader reader = (VarStraightReader) state.reader;
+          final int maxDocs = reader.maxDoc;
+          if (maxDocs == 0) {
+            return;
+          }
+          if (lastDocID+1 < state.docBase) {
+            fill(state.docBase, address);
+            lastDocID = state.docBase-1;
+          }
+          final long numDataBytes;
+          final IndexInput cloneIdx = reader.cloneIndex();
+          try {
+            numDataBytes = cloneIdx.readVLong();
+            final ReaderIterator iter = PackedInts.getReaderIterator(cloneIdx);
+            for (int i = 0; i < maxDocs; i++) {
+              long offset = iter.next();
+              ++lastDocID;
+              if (lastDocID >= docToAddress.length) {
+                int oldSize = docToAddress.length;
+                docToAddress = ArrayUtil.grow(docToAddress, 1 + lastDocID);
+                bytesUsed.addAndGet((docToAddress.length - oldSize)
+                    * RamUsageEstimator.NUM_BYTES_INT);
+              }
+              docToAddress[lastDocID] = address + offset;
+            }
+            address += numDataBytes; // this is the address after all addr pointers are updated
+            iter.close();
+          } finally {
+            IOUtils.close(cloneIdx);
+          }
+          final IndexInput cloneData = reader.cloneData();
+          try {
+            datOut.copyBytes(cloneData, numDataBytes);
+          } finally {
+            IOUtils.close(cloneData);  
+          }
+        } else {
+          super.merge(state);
+        }
+        success = true;
+      } finally {
+        if (!success) {
+          IOUtils.closeWhileHandlingException(datOut);
+        }
+      }
+    }
+    
+    @Override
+    protected void mergeDoc(int docID, int sourceDoc) throws IOException {
+      assert merge;
+      assert lastDocID < docID;
+      currentMergeSource.getBytes(sourceDoc, bytesRef);
+      if (bytesRef.length == 0) {
+        return; // default
+      }
+      fill(docID, address);
+      datOut.writeBytes(bytesRef.bytes, bytesRef.offset, bytesRef.length);
+      docToAddress[docID] = address;
+      address += bytesRef.length;
+      lastDocID = docID;
+    }
+    
+
+    @Override
+    public void finish(int docCount) throws IOException {
+      boolean success = false;
+      assert (!merge && datOut == null) || (merge && datOut != null); 
+      final IndexOutput datOut = getOrCreateDataOut();
+      try {
+        if (!merge) {
+          // header is already written in getDataOut()
+          pool.writePool(datOut);
+        }
+        success = true;
+      } finally {
+        if (success) {
+          IOUtils.close(datOut);
+        } else {
+          IOUtils.closeWhileHandlingException(datOut);
+        }
+        pool.dropBuffersAndReset();
+      }
+
+      success = false;
+      final IndexOutput idxOut = getOrCreateIndexOut();
+      try {
+        if (lastDocID == -1) {
+          idxOut.writeVLong(0);
+          final PackedInts.Writer w = PackedInts.getWriter(idxOut, docCount+1,
+              PackedInts.bitsRequired(0));
+          // docCount+1 so we write sentinel
+          for (int i = 0; i < docCount+1; i++) {
+            w.add(0);
+          }
+          w.finish();
+        } else {
+          fill(docCount, address);
+          idxOut.writeVLong(address);
+          final PackedInts.Writer w = PackedInts.getWriter(idxOut, docCount+1,
+              PackedInts.bitsRequired(address));
+          for (int i = 0; i < docCount; i++) {
+            w.add(docToAddress[i]);
+          }
+          // write sentinel
+          w.add(address);
+          w.finish();
+        }
+        success = true;
+      } finally {
+        bytesUsed.addAndGet(-(docToAddress.length)
+            * RamUsageEstimator.NUM_BYTES_INT);
+        docToAddress = null;
+        if (success) {
+          IOUtils.close(idxOut);
+        } else {
+          IOUtils.closeWhileHandlingException(idxOut);
+        }
+      }
+    }
+
+    public long ramBytesUsed() {
+      return bytesUsed.get();
+    }
+  }
+
+  public static class VarStraightReader extends BytesReaderBase {
+    private final int maxDoc;
+
+    VarStraightReader(Directory dir, String id, int maxDoc, IOContext context) throws IOException {
+      super(dir, id, CODEC_NAME, VERSION_START, true, context, Type.BYTES_VAR_STRAIGHT);
+      this.maxDoc = maxDoc;
+    }
+
+    @Override
+    public Source load() throws IOException {
+      return new VarStraightSource(cloneData(), cloneIndex());
+    }
+
+    @Override
+    public Source getDirectSource()
+        throws IOException {
+      return new DirectVarStraightSource(cloneData(), cloneIndex(), type());
+    }
+  }
+  
+  private static final class VarStraightSource extends BytesSourceBase {
+    private final PackedInts.Reader addresses;
+
+    public VarStraightSource(IndexInput datIn, IndexInput idxIn) throws IOException {
+      super(datIn, idxIn, new PagedBytes(PAGED_BYTES_BITS), idxIn.readVLong(),
+          Type.BYTES_VAR_STRAIGHT);
+      addresses = PackedInts.getReader(idxIn);
+    }
+
+    @Override
+    public BytesRef getBytes(int docID, BytesRef bytesRef) {
+      final long address = addresses.get(docID);
+      return data.fillSlice(bytesRef, address,
+          (int) (addresses.get(docID + 1) - address));
+    }
+  }
+  
+  public final static class DirectVarStraightSource extends DirectSource {
+
+    private final PackedInts.Reader index;
+
+    DirectVarStraightSource(IndexInput data, IndexInput index, Type type)
+        throws IOException {
+      super(data, type);
+      index.readVLong();
+      this.index = PackedInts.getDirectReader(index);
+    }
+
+    @Override
+    protected int position(int docID) throws IOException {
+      final long offset = index.get(docID);
+      data.seek(baseOffset + offset);
+      // Safe to do 1+docID because we write sentinel at the end:
+      final long nextOffset = index.get(1+docID);
+      return (int) (nextOffset - offset);
+    }
+  }
+}


diff -ruN -x .svn -x build lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/codecs/lucene40/values/Writer.java lucene-3622/lucene/src/java/org/apache/lucene/index/codecs/lucene40/values/Writer.java
--- lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/codecs/lucene40/values/Writer.java	1969-12-31 19:00:00.000000000 -0500
+++ lucene-3622/lucene/src/java/org/apache/lucene/index/codecs/lucene40/values/Writer.java	2011-12-09 11:10:18.884849652 -0500
@@ -0,0 +1,219 @@
+package org.apache.lucene.index.codecs.lucene40.values;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+import java.io.IOException;
+import java.util.Comparator;
+
+import org.apache.lucene.index.DocValues.Source;
+import org.apache.lucene.index.DocValues.Type;
+import org.apache.lucene.index.codecs.DocValuesConsumer;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.util.Bits;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.Counter;
+
+/**
+ * Abstract API for per-document stored primitive values of type <tt>byte[]</tt>
+ * , <tt>long</tt> or <tt>double</tt>. The API accepts a single value for each
+ * document. The underlying storage mechanism, file formats, data-structures and
+ * representations depend on the actual implementation.
+ * <p>
+ * Document IDs passed to this API must always be increasing unless stated
+ * otherwise.
+ * </p>
+ * 
+ * @lucene.experimental
+ */
+public abstract class Writer extends DocValuesConsumer {
+  protected Source currentMergeSource;
+  protected final Counter bytesUsed;
+
+  /**
+   * Creates a new {@link Writer}.
+   * 
+   * @param bytesUsed
+   *          bytes-usage tracking reference used by implementation to track
+   *          internally allocated memory. All tracked bytes must be released
+   *          once {@link #finish(int)} has been called.
+   */
+  protected Writer(Counter bytesUsed) {
+    this.bytesUsed = bytesUsed;
+  }
+
+  /**
+   * Filename extension for index files
+   */
+  public static final String INDEX_EXTENSION = "idx";
+  
+  /**
+   * Filename extension for data files.
+   */
+  public static final String DATA_EXTENSION = "dat";
+
+  /**
+   * Records the specified <tt>long</tt> value for the docID or throws an
+   * {@link UnsupportedOperationException} if this {@link Writer} doesn't record
+   * <tt>long</tt> values.
+   * 
+   * @throws UnsupportedOperationException
+   *           if this writer doesn't record <tt>long</tt> values
+   */
+  public void add(int docID, long value) throws IOException {
+    throw new UnsupportedOperationException();
+  }
+
+  /**
+   * Records the specified <tt>double</tt> value for the docID or throws an
+   * {@link UnsupportedOperationException} if this {@link Writer} doesn't record
+   * <tt>double</tt> values.
+   * 
+   * @throws UnsupportedOperationException
+   *           if this writer doesn't record <tt>double</tt> values
+   */
+  public void add(int docID, double value) throws IOException {
+    throw new UnsupportedOperationException();
+  }
+
+  /**
+   * Records the specified {@link BytesRef} value for the docID or throws an
+   * {@link UnsupportedOperationException} if this {@link Writer} doesn't record
+   * {@link BytesRef} values.
+   * 
+   * @throws UnsupportedOperationException
+   *           if this writer doesn't record {@link BytesRef} values
+   */
+  public void add(int docID, BytesRef value) throws IOException {
+    throw new UnsupportedOperationException();
+  }
+
+  /**
+   * Merges a document with the given <code>docID</code>. The methods
+   * implementation obtains the value for the <i>sourceDoc</i> id from the
+   * current {@link Source} set to <i>setNextMergeSource(Source)</i>.
+   * <p>
+   * This method is used during merging to provide implementation agnostic
+   * default merge implementation.
+   * </p>
+   * <p>
+   * All documents IDs between the given ID and the previously given ID or
+   * <tt>0</tt> if the method is call the first time are filled with default
+   * values depending on the {@link Writer} implementation. The given document
+   * ID must always be greater than the previous ID or <tt>0</tt> if called the
+   * first time.
+   */
+  protected abstract void mergeDoc(int docID, int sourceDoc) throws IOException;
+
+  /**
+   * Sets the next {@link Source} to consume values from on calls to
+   * {@link #mergeDoc(int, int)}
+   * 
+   * @param mergeSource
+   *          the next {@link Source}, this must not be null
+   */
+  protected void setNextMergeSource(Source mergeSource) {
+    currentMergeSource = mergeSource;
+  }
+
+  /**
+   * Finish writing and close any files and resources used by this Writer.
+   * 
+   * @param docCount
+   *          the total number of documents for this writer. This must be
+   *          greater that or equal to the largest document id passed to one of
+   *          the add methods after the {@link Writer} was created.
+   */
+  public abstract void finish(int docCount) throws IOException;
+
+  @Override
+  protected void merge(SingleSubMergeState state) throws IOException {
+    // This enables bulk copies in subclasses per MergeState, subclasses can
+    // simply override this and decide if they want to merge
+    // segments using this generic implementation or if a bulk merge is possible
+    // / feasible.
+    final Source source = state.reader.getDirectSource();
+    assert source != null;
+    setNextMergeSource(source); // set the current enum we are working on - the
+    // impl. will get the correct reference for the type
+    // it supports
+    int docID = state.docBase;
+    final Bits liveDocs = state.liveDocs;
+    final int docCount = state.docCount;
+    for (int i = 0; i < docCount; i++) {
+      if (liveDocs == null || liveDocs.get(i)) {
+        mergeDoc(docID++, i);
+      }
+    }
+    
+  }
+
+  /**
+   * Factory method to create a {@link Writer} instance for a given type. This
+   * method returns default implementations for each of the different types
+   * defined in the {@link Type} enumeration.
+   * 
+   * @param type
+   *          the {@link Type} to create the {@link Writer} for
+   * @param id
+   *          the file name id used to create files within the writer.
+   * @param directory
+   *          the {@link Directory} to create the files from.
+   * @param bytesUsed
+   *          a byte-usage tracking reference
+   * @return a new {@link Writer} instance for the given {@link Type}
+   * @throws IOException
+   */
+  public static Writer create(Type type, String id, Directory directory,
+      Comparator<BytesRef> comp, Counter bytesUsed, IOContext context) throws IOException {
+    if (comp == null) {
+      comp = BytesRef.getUTF8SortedAsUnicodeComparator();
+    }
+    switch (type) {
+    case FIXED_INTS_16:
+    case FIXED_INTS_32:
+    case FIXED_INTS_64:
+    case FIXED_INTS_8:
+    case VAR_INTS:
+      return Ints.getWriter(directory, id, bytesUsed, type, context);
+    case FLOAT_32:
+      return Floats.getWriter(directory, id, bytesUsed, context, type);
+    case FLOAT_64:
+      return Floats.getWriter(directory, id, bytesUsed, context, type);
+    case BYTES_FIXED_STRAIGHT:
+      return Bytes.getWriter(directory, id, Bytes.Mode.STRAIGHT, true, comp,
+          bytesUsed, context);
+    case BYTES_FIXED_DEREF:
+      return Bytes.getWriter(directory, id, Bytes.Mode.DEREF, true, comp,
+          bytesUsed, context);
+    case BYTES_FIXED_SORTED:
+      return Bytes.getWriter(directory, id, Bytes.Mode.SORTED, true, comp,
+          bytesUsed, context);
+    case BYTES_VAR_STRAIGHT:
+      return Bytes.getWriter(directory, id, Bytes.Mode.STRAIGHT, false, comp,
+          bytesUsed, context);
+    case BYTES_VAR_DEREF:
+      return Bytes.getWriter(directory, id, Bytes.Mode.DEREF, false, comp,
+          bytesUsed, context);
+    case BYTES_VAR_SORTED:
+      return Bytes.getWriter(directory, id, Bytes.Mode.SORTED, false, comp,
+          bytesUsed, context);
+    default:
+      throw new IllegalArgumentException("Unknown Values: " + type);
+    }
+  }
+}


diff -ruN -x .svn -x build lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/codecs/PerDocConsumer.java lucene-3622/lucene/src/java/org/apache/lucene/index/codecs/PerDocConsumer.java
--- lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/codecs/PerDocConsumer.java	2011-12-06 18:45:04.100810984 -0500
+++ lucene-3622/lucene/src/java/org/apache/lucene/index/codecs/PerDocConsumer.java	2011-12-10 10:34:57.134317312 -0500
@@ -19,11 +19,8 @@
 import java.io.IOException;
 
 import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.DocValues;
 import org.apache.lucene.index.MergeState;
-import org.apache.lucene.index.values.IndexDocValues;
-import org.apache.lucene.index.values.TypePromoter;
-import org.apache.lucene.index.values.ValueType;
 
 /**
  * Abstract API that consumes per document values. Concrete implementations of
@@ -37,82 +34,26 @@
  */
 public abstract class PerDocConsumer implements Closeable{
   /** Adds a new DocValuesField */
-  public abstract DocValuesConsumer addValuesField(FieldInfo field)
+  public abstract DocValuesConsumer addValuesField(DocValues.Type type, FieldInfo field)
       throws IOException;
 
   /**
-   * Consumes and merges the given {@link PerDocValues} producer
+   * Consumes and merges the given {@link PerDocProducer} producer
    * into this consumers format.   
    */
-  public void merge(MergeState mergeState)
-      throws IOException {
-    final FieldInfos fieldInfos = mergeState.fieldInfos;
-    final IndexDocValues[] docValues = new IndexDocValues[mergeState.readers.size()];
-    final PerDocValues[] perDocValues = new PerDocValues[mergeState.readers.size()];
-    // pull all PerDocValues 
-    for (int i = 0; i < perDocValues.length; i++) {
-      perDocValues[i] =  mergeState.readers.get(i).reader.perDocValues();
-    }
-    for (FieldInfo fieldInfo : fieldInfos) {
-      mergeState.fieldInfo = fieldInfo;
-      TypePromoter currentPromoter = TypePromoter.getIdentityPromoter();
+  public void merge(MergeState mergeState) throws IOException {
+    final DocValues[] docValues = new DocValues[mergeState.readers.size()];
+
+    for (FieldInfo fieldInfo : mergeState.fieldInfos) {
+      mergeState.fieldInfo = fieldInfo; // set the field we are merging
       if (fieldInfo.hasDocValues()) {
-        for (int i = 0; i < perDocValues.length; i++) {
-          if (perDocValues[i] != null) { // get all IDV to merge
-            docValues[i] = perDocValues[i].docValues(fieldInfo.name);
-            if (docValues[i] != null) {
-              currentPromoter = promoteValueType(fieldInfo, docValues[i], currentPromoter);
-              if (currentPromoter == null) {
-                break;
-              }     
-            }
-          }
-        }
-        
-        if (currentPromoter == null) {
-          fieldInfo.resetDocValues(null);
-          continue;
-        }
-        assert currentPromoter != TypePromoter.getIdentityPromoter();
-        if (fieldInfo.getDocValues() != currentPromoter.type()) {
-          // reset the type if we got promoted
-          fieldInfo.resetDocValues(currentPromoter.type());
+        for (int i = 0; i < docValues.length; i++) {
+          docValues[i] = mergeState.readers.get(i).reader.docValues(fieldInfo.name);
         }
-        
-        final DocValuesConsumer docValuesConsumer = addValuesField(mergeState.fieldInfo);
+        final DocValuesConsumer docValuesConsumer = addValuesField(fieldInfo.getDocValuesType(), fieldInfo);
         assert docValuesConsumer != null;
         docValuesConsumer.merge(mergeState, docValues);
       }
     }
-    /* NOTE: don't close the perDocProducers here since they are private segment producers
-     * and will be closed once the SegmentReader goes out of scope */ 
-  }
-
-  protected TypePromoter promoteValueType(final FieldInfo fieldInfo, final IndexDocValues docValues,
-      TypePromoter currentPromoter) {
-    assert currentPromoter != null;
-    final TypePromoter incomingPromoter = TypePromoter.create(docValues.type(),  docValues.getValueSize());
-    assert incomingPromoter != null;
-    final TypePromoter newPromoter = currentPromoter.promote(incomingPromoter);
-    return newPromoter == null ? handleIncompatibleValueType(fieldInfo, incomingPromoter, currentPromoter) : newPromoter;    
-  }
-
-  /**
-   * Resolves a conflicts of incompatible {@link TypePromoter}s. The default
-   * implementation promotes incompatible types to
-   * {@link ValueType#BYTES_VAR_STRAIGHT} and preserves all values. If this
-   * method returns <code>null</code> all docvalues for the given
-   * {@link FieldInfo} are dropped and all values are lost.
-   * 
-   * @param incomingPromoter
-   *          the incompatible incoming promoter
-   * @param currentPromoter
-   *          the current promoter
-   * @return a promoted {@link TypePromoter} or <code>null</code> iff this index
-   *         docvalues should be dropped for this field.
-   */
-  protected TypePromoter handleIncompatibleValueType(FieldInfo fieldInfo, TypePromoter incomingPromoter, TypePromoter currentPromoter) {
-    return TypePromoter.create(ValueType.BYTES_VAR_STRAIGHT, TypePromoter.VAR_TYPE_VALUE_SIZE);
-  }
-  
+  }  
 }


diff -ruN -x .svn -x build lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/codecs/PerDocProducer.java lucene-3622/lucene/src/java/org/apache/lucene/index/codecs/PerDocProducer.java
--- lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/codecs/PerDocProducer.java	1969-12-31 19:00:00.000000000 -0500
+++ lucene-3622/lucene/src/java/org/apache/lucene/index/codecs/PerDocProducer.java	2011-12-10 10:29:15.410311361 -0500
@@ -0,0 +1,48 @@
+package org.apache.lucene.index.codecs;
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+import java.io.Closeable;
+import java.io.IOException;
+
+import org.apache.lucene.index.DocValues;
+
+/**
+ * Abstract API that provides access to one or more per-document storage
+ * features. The concrete implementations provide access to the underlying
+ * storage on a per-document basis corresponding to their actual
+ * {@link PerDocConsumer} counterpart.
+ * <p>
+ * The {@link PerDocProducer} API is accessible through the
+ * {@link PostingsFormat} - API providing per field consumers and producers for inverted
+ * data (terms, postings) as well as per-document data.
+ * 
+ * @lucene.experimental
+ */
+public abstract class PerDocProducer implements Closeable {
+  /**
+   * Returns {@link DocValues} for the current field.
+   * 
+   * @param field
+   *          the field name
+   * @return the {@link DocValues} for this field or <code>null</code> if not
+   *         applicable.
+   * @throws IOException
+   */
+  public abstract DocValues docValues(String field) throws IOException;
+
+  public static final PerDocProducer[] EMPTY_ARRAY = new PerDocProducer[0];
+}


diff -ruN -x .svn -x build lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/codecs/PerDocValues.java lucene-3622/lucene/src/java/org/apache/lucene/index/codecs/PerDocValues.java
--- lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/codecs/PerDocValues.java	2011-12-06 18:45:04.068810983 -0500
+++ lucene-3622/lucene/src/java/org/apache/lucene/index/codecs/PerDocValues.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,54 +0,0 @@
-package org.apache.lucene.index.codecs;
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-import java.io.Closeable;
-import java.io.IOException;
-import java.util.Collection;
-
-import org.apache.lucene.index.values.IndexDocValues;
-
-/**
- * Abstract API that provides access to one or more per-document storage
- * features. The concrete implementations provide access to the underlying
- * storage on a per-document basis corresponding to their actual
- * {@link PerDocConsumer} counterpart.
- * <p>
- * The {@link PerDocValues} API is accessible through the
- * {@link PostingsFormat} - API providing per field consumers and producers for inverted
- * data (terms, postings) as well as per-document data.
- * 
- * @lucene.experimental
- */
-public abstract class PerDocValues implements Closeable {
-  /**
-   * Returns {@link IndexDocValues} for the current field.
-   * 
-   * @param field
-   *          the field name
-   * @return the {@link IndexDocValues} for this field or <code>null</code> if not
-   *         applicable.
-   * @throws IOException
-   */
-  public abstract IndexDocValues docValues(String field) throws IOException;
-
-  public static final PerDocValues[] EMPTY_ARRAY = new PerDocValues[0];
-
-  /**
-   * Returns all fields this {@link PerDocValues} contains values for.
-   */
-  public abstract Collection<String> fields();
-}


diff -ruN -x .svn -x build lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/codecs/sep/SepDocValuesConsumer.java lucene-3622/lucene/src/java/org/apache/lucene/index/codecs/sep/SepDocValuesConsumer.java
--- lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/codecs/sep/SepDocValuesConsumer.java	2011-12-06 18:45:04.084810983 -0500
+++ lucene-3622/lucene/src/java/org/apache/lucene/index/codecs/sep/SepDocValuesConsumer.java	2011-12-09 08:23:09.860675003 -0500
@@ -26,7 +26,7 @@
 import org.apache.lucene.index.PerDocWriteState;
 import org.apache.lucene.index.SegmentInfo;
 import org.apache.lucene.index.codecs.DocValuesWriterBase;
-import org.apache.lucene.index.values.Writer;
+import org.apache.lucene.index.codecs.lucene40.values.Writer;
 import org.apache.lucene.store.Directory;
 
 /**
@@ -53,7 +53,7 @@
     for (FieldInfo fieldInfo : fieldInfos) {
       if (fieldInfo.hasDocValues()) {
         String filename = docValuesId(segmentInfo.name, fieldInfo.number);
-        switch (fieldInfo.getDocValues()) {
+        switch (fieldInfo.getDocValuesType()) {
           case BYTES_FIXED_DEREF:
           case BYTES_VAR_DEREF:
           case BYTES_VAR_STRAIGHT:


diff -ruN -x .svn -x build lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/codecs/sep/SepDocValuesProducer.java lucene-3622/lucene/src/java/org/apache/lucene/index/codecs/sep/SepDocValuesProducer.java
--- lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/codecs/sep/SepDocValuesProducer.java	2011-12-06 18:45:04.084810983 -0500
+++ lucene-3622/lucene/src/java/org/apache/lucene/index/codecs/sep/SepDocValuesProducer.java	2011-12-10 10:29:15.374311360 -0500
@@ -22,28 +22,28 @@
 import java.util.Map;
 import java.util.TreeMap;
 
+import org.apache.lucene.index.DocValues;
 import org.apache.lucene.index.SegmentReadState;
 import org.apache.lucene.index.codecs.DocValuesReaderBase;
-import org.apache.lucene.index.values.IndexDocValues;
 import org.apache.lucene.util.IOUtils;
 
 /**
- * Implementation of PerDocValues that uses separate files.
+ * Implementation of PerDocProducer that uses separate files.
  * @lucene.experimental
  */
 public class SepDocValuesProducer extends DocValuesReaderBase {
-  private final TreeMap<String, IndexDocValues> docValues;
+  private final TreeMap<String, DocValues> docValues;
 
   /**
    * Creates a new {@link SepDocValuesProducer} instance and loads all
-   * {@link IndexDocValues} instances for this segment and codec.
+   * {@link DocValues} instances for this segment and codec.
    */
   public SepDocValuesProducer(SegmentReadState state) throws IOException {
     docValues = load(state.fieldInfos, state.segmentInfo.name, state.segmentInfo.docCount, state.dir, state.context);
   }
   
   @Override
-  protected Map<String,IndexDocValues> docValues() {
+  protected Map<String,DocValues> docValues() {
     return docValues;
   }
   


diff -ruN -x .svn -x build lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/codecs/simpletext/SimpleTextFieldInfosReader.java lucene-3622/lucene/src/java/org/apache/lucene/index/codecs/simpletext/SimpleTextFieldInfosReader.java
--- lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/codecs/simpletext/SimpleTextFieldInfosReader.java	2011-12-06 18:45:04.024810982 -0500
+++ lucene-3622/lucene/src/java/org/apache/lucene/index/codecs/simpletext/SimpleTextFieldInfosReader.java	2011-12-09 11:10:18.772849650 -0500
@@ -26,8 +26,8 @@
 import org.apache.lucene.index.IndexFileNames;
 import org.apache.lucene.index.SegmentInfo;
 import org.apache.lucene.index.FieldInfo.IndexOptions;
+import org.apache.lucene.index.DocValues;
 import org.apache.lucene.index.codecs.FieldInfosReader;
-import org.apache.lucene.index.values.ValueType;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.IOContext;
 import org.apache.lucene.store.IndexInput;
@@ -98,12 +98,12 @@
         SimpleTextUtil.readLine(input, scratch);
         assert StringHelper.startsWith(scratch, DOCVALUES);
         String dvType = readString(DOCVALUES.length, scratch);
-        final ValueType docValuesType;
+        final DocValues.Type docValuesType;
         
         if ("false".equals(dvType)) {
           docValuesType = null;
         } else {
-          docValuesType = ValueType.valueOf(dvType);
+          docValuesType = DocValues.Type.valueOf(dvType);
         }
         
         SimpleTextUtil.readLine(input, scratch);


diff -ruN -x .svn -x build lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/codecs/simpletext/SimpleTextFieldInfosWriter.java lucene-3622/lucene/src/java/org/apache/lucene/index/codecs/simpletext/SimpleTextFieldInfosWriter.java
--- lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/codecs/simpletext/SimpleTextFieldInfosWriter.java	2011-12-06 18:45:04.028810982 -0500
+++ lucene-3622/lucene/src/java/org/apache/lucene/index/codecs/simpletext/SimpleTextFieldInfosWriter.java	2011-12-09 08:23:09.860675003 -0500
@@ -100,7 +100,7 @@
         if (!fi.hasDocValues()) {
           SimpleTextUtil.write(out, "false", scratch);
         } else {
-          SimpleTextUtil.write(out, fi.getDocValues().toString(), scratch);
+          SimpleTextUtil.write(out, fi.getDocValuesType().toString(), scratch);
         }
         SimpleTextUtil.writeNewline(out);
         


diff -ruN -x .svn -x build lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/DirectoryReader.java lucene-3622/lucene/src/java/org/apache/lucene/index/DirectoryReader.java
--- lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/DirectoryReader.java	2011-12-11 12:21:46.803933541 -0500
+++ lucene-3622/lucene/src/java/org/apache/lucene/index/DirectoryReader.java	2011-12-12 15:04:20.237608000 -0500
@@ -29,8 +29,6 @@
 
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.IOContext;
-import org.apache.lucene.util.Bits;
-import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.IOUtils;
 import org.apache.lucene.util.MapBackedSet;
 


diff -ruN -x .svn -x build lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/DocFieldProcessor.java lucene-3622/lucene/src/java/org/apache/lucene/index/DocFieldProcessor.java
--- lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/DocFieldProcessor.java	2011-12-11 17:20:17.380245444 -0500
+++ lucene-3622/lucene/src/java/org/apache/lucene/index/DocFieldProcessor.java	2011-12-12 15:02:46.777606373 -0500
@@ -26,11 +26,11 @@
 
 import org.apache.lucene.index.DocumentsWriterPerThread.DocState;
 import org.apache.lucene.index.codecs.Codec;
-import org.apache.lucene.index.codecs.DocValuesFormat;
 import org.apache.lucene.index.codecs.DocValuesConsumer;
+import org.apache.lucene.index.codecs.DocValuesFormat;
 import org.apache.lucene.index.codecs.FieldInfosWriter;
 import org.apache.lucene.index.codecs.PerDocConsumer;
-import org.apache.lucene.index.values.PerDocFieldValues;
+import org.apache.lucene.index.DocValues;
 import org.apache.lucene.store.IOContext;
 import org.apache.lucene.util.ArrayUtil;
 import org.apache.lucene.util.IOUtils;
@@ -224,7 +224,7 @@
         // needs to be more "pluggable" such that if I want
         // to have a new "thing" my Fields can do, I can
         // easily add it
-        FieldInfo fi = fieldInfos.addOrUpdate(fieldName, field.fieldType(), false, field.docValuesType());
+        FieldInfo fi = fieldInfos.addOrUpdate(fieldName, field.fieldType());
 
         fp = new DocFieldProcessorPerField(this, fi);
         fp.next = fieldHash[hashPos];
@@ -235,7 +235,7 @@
           rehash();
         }
       } else {
-        fieldInfos.addOrUpdate(fp.fieldInfo.name, field.fieldType(), false, field.docValuesType());
+        fieldInfos.addOrUpdate(fp.fieldInfo.name, field.fieldType());
       }
 
       if (thisFieldGen != fp.lastGen) {
@@ -259,9 +259,9 @@
       if (field.fieldType().stored()) {
         fieldsWriter.addField(field, fp.fieldInfo);
       }
-      final PerDocFieldValues docValues = field.docValues();
-      if (docValues != null) {
-        docValuesConsumer(docState, fp.fieldInfo).add(docState.docID, docValues);
+      final DocValue docValue = field.docValue();
+      if (docValue != null) {
+        docValuesConsumer(field.docValueType(), docState, fp.fieldInfo).add(docState.docID, docValue);
       }
     }
 
@@ -310,12 +310,12 @@
   final private Map<String, DocValuesConsumerAndDocID> docValues = new HashMap<String, DocValuesConsumerAndDocID>();
   final private Map<Integer, PerDocConsumer> perDocConsumers = new HashMap<Integer, PerDocConsumer>();
 
-  DocValuesConsumer docValuesConsumer(DocState docState, FieldInfo fieldInfo) 
+  DocValuesConsumer docValuesConsumer(DocValues.Type valueType, DocState docState, FieldInfo fieldInfo) 
       throws IOException {
     DocValuesConsumerAndDocID docValuesConsumerAndDocID = docValues.get(fieldInfo.name);
     if (docValuesConsumerAndDocID != null) {
       if (docState.docID == docValuesConsumerAndDocID.docID) {
-        throw new IllegalArgumentException("IndexDocValuesField \"" + fieldInfo.name + "\" appears more than once in this document (only one value is allowed, per field)");
+        throw new IllegalArgumentException("DocValuesField \"" + fieldInfo.name + "\" appears more than once in this document (only one value is allowed, per field)");
       }
       assert docValuesConsumerAndDocID.docID < docState.docID;
       docValuesConsumerAndDocID.docID = docState.docID;
@@ -329,17 +329,9 @@
       perDocConsumer = dvFormat.docsConsumer(perDocWriteState);
       perDocConsumers.put(0, perDocConsumer);
     }
-    boolean success = false;
-    DocValuesConsumer docValuesConsumer = null;
-    try {
-      docValuesConsumer = perDocConsumer.addValuesField(fieldInfo);
-      fieldInfo.commitDocValues();
-      success = true;
-    } finally {
-      if (!success) {
-        fieldInfo.revertUncommitted();
-      }
-    }
+
+    DocValuesConsumer docValuesConsumer = perDocConsumer.addValuesField(valueType, fieldInfo);
+    fieldInfo.setDocValuesType(valueType);
 
     docValuesConsumerAndDocID = new DocValuesConsumerAndDocID(docValuesConsumer);
     docValuesConsumerAndDocID.docID = docState.docID;


diff -ruN -x .svn -x build lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/DocumentsWriterPerThread.java lucene-3622/lucene/src/java/org/apache/lucene/index/DocumentsWriterPerThread.java
--- lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/DocumentsWriterPerThread.java	2011-12-11 17:20:17.380245444 -0500
+++ lucene-3622/lucene/src/java/org/apache/lucene/index/DocumentsWriterPerThread.java	2011-12-12 15:02:46.489606367 -0500
@@ -248,7 +248,6 @@
           // mark document as deleted
           deleteDocID(docState.docID);
           numDocsInRAM++;
-          fieldInfos.revertUncommitted();
         } else {
           abort();
         }
@@ -312,7 +311,6 @@
               // Incr here because finishDocument will not
               // be called (because an exc is being thrown):
               numDocsInRAM++;
-              fieldInfos.revertUncommitted();
             } else {
               abort();
             }


diff -ruN -x .svn -x build lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/DocValue.java lucene-3622/lucene/src/java/org/apache/lucene/index/DocValue.java
--- lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/DocValue.java	1969-12-31 19:00:00.000000000 -0500
+++ lucene-3622/lucene/src/java/org/apache/lucene/index/DocValue.java	2011-12-10 13:07:49.298477039 -0500
@@ -0,0 +1,100 @@
+package org.apache.lucene.index;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+import java.util.Comparator;
+
+import org.apache.lucene.document.DocValuesField;
+import org.apache.lucene.index.DocValues;
+import org.apache.lucene.index.DocValues.Type; // javadocs
+import org.apache.lucene.index.codecs.DocValuesConsumer; // javadocs
+import org.apache.lucene.util.BytesRef;
+
+/**
+ * Per document and field values consumed by {@link DocValuesConsumer}. 
+ * @see DocValuesField
+ * 
+ * @lucene.experimental
+ */
+public interface DocValue {
+
+  /**
+   * Sets the given <code>long</code> value.
+   */
+  public void setInt(long value);
+
+  /**
+   * Sets the given <code>float</code> value.
+   */
+  public void setFloat(float value);
+
+  /**
+   * Sets the given <code>double</code> value.
+   */
+  public void setFloat(double value);
+
+  /**
+   * Sets the given {@link BytesRef} value and the field's {@link Type}. The
+   * comparator for this field is set to <code>null</code>. If a
+   * <code>null</code> comparator is set the default comparator for the given
+   * {@link Type} is used.
+   */
+  public void setBytes(BytesRef value, DocValues.Type type);
+
+  /**
+   * Sets the given {@link BytesRef} value, the field's {@link Type} and the
+   * field's comparator. If the {@link Comparator} is set to <code>null</code>
+   * the default for the given {@link Type} is used instead.
+   */
+  public void setBytes(BytesRef value, DocValues.Type type, Comparator<BytesRef> comp);
+
+  /**
+   * Returns the set {@link BytesRef} or <code>null</code> if not set.
+   */
+  public BytesRef getBytes();
+
+  /**
+   * Returns the set {@link BytesRef} comparator or <code>null</code> if not set
+   */
+  public Comparator<BytesRef> bytesComparator();
+
+  /**
+   * Returns the set floating point value or <code>0.0d</code> if not set.
+   */
+  public double getFloat();
+
+  /**
+   * Returns the set <code>long</code> value of <code>0</code> if not set.
+   */
+  public long getInt();
+
+  /**
+   * Sets the {@link BytesRef} comparator for this field. If the field has a
+   * numeric {@link Type} the comparator will be ignored.
+   */
+  public void setBytesComparator(Comparator<BytesRef> comp);
+
+  /**
+   * Sets the {@link Type}
+   */
+  public void setDocValuesType(DocValues.Type type);
+
+  /**
+  * Returns the {@link Type}
+  */
+  public DocValues.Type docValueType();
+}


diff -ruN -x .svn -x build lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/DocValues.java lucene-3622/lucene/src/java/org/apache/lucene/index/DocValues.java
--- lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/DocValues.java	1969-12-31 19:00:00.000000000 -0500
+++ lucene-3622/lucene/src/java/org/apache/lucene/index/DocValues.java	2011-12-09 11:13:38.768853133 -0500
@@ -0,0 +1,687 @@
+package org.apache.lucene.index;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+import java.io.Closeable;
+import java.io.IOException;
+import java.util.Comparator;
+
+import org.apache.lucene.document.DocValuesField;
+import org.apache.lucene.index.codecs.DocValuesFormat;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.packed.PackedInts;
+
+/**
+ * {@link DocValues} provides a dense per-document typed storage for fast
+ * value access based on the lucene internal document id. {@link DocValues}
+ * exposes two distinct APIs:
+ * <ul>
+ * <li>via {@link #getSource()} providing RAM resident random access</li>
+ * <li>via {@link #getDirectSource()} providing on disk random access</li>
+ * </ul> {@link DocValues} are exposed via
+ * {@link IndexReader#perDocValues()} on a per-segment basis. For best
+ * performance {@link DocValues} should be consumed per-segment just like
+ * IndexReader.
+ * <p>
+ * {@link DocValues} are fully integrated into the {@link DocValuesFormat} API.
+ * 
+ * @see Type for limitations and default implementation documentation
+ * @see DocValuesField for adding values to the index
+ * @see DocValuesFormat#docsConsumer(org.apache.lucene.index.PerDocWriteState) for
+ *      customization
+ * @lucene.experimental
+ */
+public abstract class DocValues implements Closeable {
+
+  public static final DocValues[] EMPTY_ARRAY = new DocValues[0];
+
+  private volatile SourceCache cache = new SourceCache.DirectSourceCache();
+  private final Object cacheLock = new Object();
+  
+  /**
+   * Loads a new {@link Source} instance for this {@link DocValues} field
+   * instance. Source instances returned from this method are not cached. It is
+   * the callers responsibility to maintain the instance and release its
+   * resources once the source is not needed anymore.
+   * <p>
+   * For managed {@link Source} instances see {@link #getSource()}.
+   * 
+   * @see #getSource()
+   * @see #setCache(SourceCache)
+   */
+  public abstract Source load() throws IOException;
+
+  /**
+   * Returns a {@link Source} instance through the current {@link SourceCache}.
+   * Iff no {@link Source} has been loaded into the cache so far the source will
+   * be loaded through {@link #load()} and passed to the {@link SourceCache}.
+   * The caller of this method should not close the obtained {@link Source}
+   * instance unless it is not needed for the rest of its life time.
+   * <p>
+   * {@link Source} instances obtained from this method are closed / released
+   * from the cache once this {@link DocValues} instance is closed by the
+   * {@link IndexReader}, {@link Fields} or {@link FieldsEnum} the
+   * {@link DocValues} was created from.
+   */
+  public Source getSource() throws IOException {
+    return cache.load(this);
+  }
+
+  /**
+   * Returns a disk resident {@link Source} instance. Direct Sources are not
+   * cached in the {@link SourceCache} and should not be shared between threads.
+   */
+  public abstract Source getDirectSource() throws IOException;
+
+  /**
+   * Returns the {@link Type} of this {@link DocValues} instance
+   */
+  public abstract Type type();
+
+  /**
+   * Closes this {@link DocValues} instance. This method should only be called
+   * by the creator of this {@link DocValues} instance. API users should not
+   * close {@link DocValues} instances.
+   */
+  public void close() throws IOException {
+    cache.close(this);
+  }
+
+  /**
+   * Returns the size per value in bytes or <code>-1</code> iff size per value
+   * is variable.
+   * 
+   * @return the size per value in bytes or <code>-1</code> iff size per value
+   * is variable.
+   */
+  public int getValueSize() {
+    return -1;
+  }
+
+  /**
+   * Sets the {@link SourceCache} used by this {@link DocValues} instance. This
+   * method should be called before {@link #load()} is called. All {@link Source} instances in the currently used cache will be closed
+   * before the new cache is installed.
+   * <p>
+   * Note: All instances previously obtained from {@link #load()} will be lost.
+   * 
+   * @throws IllegalArgumentException
+   *           if the given cache is <code>null</code>
+   * 
+   */
+  public void setCache(SourceCache cache) {
+    if (cache == null)
+      throw new IllegalArgumentException("cache must not be null");
+    synchronized (cacheLock) {
+      SourceCache toClose = this.cache;
+      this.cache = cache;
+      toClose.close(this);
+    }
+  }
+
+  /**
+   * Source of per document values like long, double or {@link BytesRef}
+   * depending on the {@link DocValues} fields {@link Type}. Source
+   * implementations provide random access semantics similar to array lookups
+   * <p>
+   * @see DocValues#getSource()
+   * @see DocValues#getDirectSource()
+   */
+  public static abstract class Source {
+    
+    protected final Type type;
+
+    protected Source(Type type) {
+      this.type = type;
+    }
+    /**
+     * Returns a <tt>long</tt> for the given document id or throws an
+     * {@link UnsupportedOperationException} if this source doesn't support
+     * <tt>long</tt> values.
+     * 
+     * @throws UnsupportedOperationException
+     *           if this source doesn't support <tt>long</tt> values.
+     */
+    public long getInt(int docID) {
+      throw new UnsupportedOperationException("ints are not supported");
+    }
+
+    /**
+     * Returns a <tt>double</tt> for the given document id or throws an
+     * {@link UnsupportedOperationException} if this source doesn't support
+     * <tt>double</tt> values.
+     * 
+     * @throws UnsupportedOperationException
+     *           if this source doesn't support <tt>double</tt> values.
+     */
+    public double getFloat(int docID) {
+      throw new UnsupportedOperationException("floats are not supported");
+    }
+
+    /**
+     * Returns a {@link BytesRef} for the given document id or throws an
+     * {@link UnsupportedOperationException} if this source doesn't support
+     * <tt>byte[]</tt> values.
+     * @throws IOException 
+     * 
+     * @throws UnsupportedOperationException
+     *           if this source doesn't support <tt>byte[]</tt> values.
+     */
+    public BytesRef getBytes(int docID, BytesRef ref) {
+      throw new UnsupportedOperationException("bytes are not supported");
+    }
+
+    /**
+     * Returns the {@link Type} of this source.
+     * 
+     * @return the {@link Type} of this source.
+     */
+    public Type type() {
+      return type;
+    }
+
+    /**
+     * Returns <code>true</code> iff this {@link Source} exposes an array via
+     * {@link #getArray()} otherwise <code>false</code>.
+     * 
+     * @return <code>true</code> iff this {@link Source} exposes an array via
+     *         {@link #getArray()} otherwise <code>false</code>.
+     */
+    public boolean hasArray() {
+      return false;
+    }
+
+    /**
+     * Returns the internal array representation iff this {@link Source} uses an
+     * array as its inner representation, otherwise <code>UOE</code>.
+     */
+    public Object getArray() {
+      throw new UnsupportedOperationException("getArray is not supported");
+    }
+    
+    /**
+     * If this {@link Source} is sorted this method will return an instance of
+     * {@link SortedSource} otherwise <code>UOE</code>
+     */
+    public SortedSource asSortedSource() {
+      throw new UnsupportedOperationException("asSortedSource is not supported");
+    }
+  }
+
+  /**
+   * A sorted variant of {@link Source} for <tt>byte[]</tt> values per document.
+   * <p>
+   */
+  public static abstract class SortedSource extends Source {
+
+    private final Comparator<BytesRef> comparator;
+
+    protected SortedSource(Type type, Comparator<BytesRef> comparator) {
+      super(type);
+      this.comparator = comparator;
+    }
+
+    @Override
+    public BytesRef getBytes(int docID, BytesRef bytesRef) {
+      final int ord = ord(docID);
+      if (ord < 0) {
+        bytesRef.length = 0;
+      } else {
+        getByOrd(ord , bytesRef);
+      }
+      return bytesRef;
+    }
+
+    /**
+     * Returns ord for specified docID. Ord is dense, ie, starts at 0, then increments by 1
+     * for the next (as defined by {@link Comparator} value.
+     */
+    public abstract int ord(int docID);
+
+    /** Returns value for specified ord. */
+    public abstract BytesRef getByOrd(int ord, BytesRef bytesRef);
+
+    /** Return true if it's safe to call {@link
+     *  #getDocToOrd}. */
+    public boolean hasPackedDocToOrd() {
+      return false;
+    }
+
+    /**
+     * Returns the PackedInts.Reader impl that maps document to ord.
+     */
+    public abstract PackedInts.Reader getDocToOrd();
+    
+    /**
+     * Returns the comparator used to order the BytesRefs.
+     */
+    public Comparator<BytesRef> getComparator() {
+      return comparator;
+    }
+
+    /**
+     * Performs a lookup by value.
+     * 
+     * @param value
+     *          the value to look up
+     * @param spare
+     *          a spare {@link BytesRef} instance used to compare internal
+     *          values to the given value. Must not be <code>null</code>
+     * @return the given values ordinal if found or otherwise
+     *         <code>(-(ord)-1)</code>, defined as the ordinal of the first
+     *         element that is greater than the given value. This guarantees
+     *         that the return value will always be &gt;= 0 if the given value
+     *         is found.
+     */
+    public int getByValue(BytesRef value, BytesRef spare) {
+      return binarySearch(value, spare, 0, getValueCount() - 1);
+    }    
+
+    private int binarySearch(BytesRef b, BytesRef bytesRef, int low,
+        int high) {
+      int mid = 0;
+      while (low <= high) {
+        mid = (low + high) >>> 1;
+        getByOrd(mid, bytesRef);
+        final int cmp = comparator.compare(bytesRef, b);
+        if (cmp < 0) {
+          low = mid + 1;
+        } else if (cmp > 0) {
+          high = mid - 1;
+        } else {
+          return mid;
+        }
+      }
+      assert comparator.compare(bytesRef, b) != 0;
+      return -(low + 1);
+    }
+    
+    @Override
+    public SortedSource asSortedSource() {
+      return this;
+    }
+    
+    /**
+     * Returns the number of unique values in this sorted source
+     */
+    public abstract int getValueCount();
+  }
+
+  /** Returns a Source that always returns default (missing)
+   *  values for all documents. */
+  public static Source getDefaultSource(final Type type) {
+    return new Source(type) {
+      @Override
+      public long getInt(int docID) {
+        return 0;
+      }
+
+      @Override
+      public double getFloat(int docID) {
+        return 0.0;
+      }
+
+      @Override
+      public BytesRef getBytes(int docID, BytesRef ref) {
+        ref.length = 0;
+        return ref;
+      }
+    };
+  }
+
+  /** Returns a SortedSource that always returns default (missing)
+   *  values for all documents. */
+  public static SortedSource getDefaultSortedSource(final Type type, final int size) {
+
+    final PackedInts.Reader docToOrd = new PackedInts.Reader() {
+      @Override
+      public long get(int index) {
+        return 0;
+      }
+
+      @Override
+      public int getBitsPerValue() {
+        return 0;
+      }
+
+      @Override
+      public int size() {
+        return size;
+      }
+
+      @Override
+      public boolean hasArray() {
+        return false;
+      }
+
+      @Override
+      public Object getArray() {
+        return null;
+      }
+    };
+
+    return new SortedSource(type, BytesRef.getUTF8SortedAsUnicodeComparator()) {
+
+      @Override
+      public BytesRef getBytes(int docID, BytesRef ref) {
+        ref.length = 0;
+        return ref;
+      }
+
+      @Override
+      public int ord(int docID) {
+        return 0;
+      }
+
+      @Override
+      public BytesRef getByOrd(int ord, BytesRef bytesRef) {
+        assert ord == 0;
+        bytesRef.length = 0;
+        return bytesRef;
+      }
+
+      @Override
+      public boolean hasPackedDocToOrd() {
+        return true;
+      }
+
+      @Override
+      public PackedInts.Reader getDocToOrd() {
+        return docToOrd;
+      }
+
+      @Override
+      public int getByValue(BytesRef value, BytesRef spare) {
+        if (value.length == 0) {
+          return 0;
+        } else {
+          return -1;
+        }
+      }
+
+      @Override
+        public int getValueCount() {
+        return 1;
+      }
+    };
+  }
+  
+  /**
+   * <code>Type</code> specifies the {@link DocValues} type for a
+   * certain field. A <code>Type</code> only defines the data type for a field
+   * while the actual implementation used to encode and decode the values depends
+   * on the the {@link DocValuesFormat#docsConsumer} and {@link DocValuesFormat#docsProducer} methods.
+   * 
+   * @lucene.experimental
+   */
+  public static enum Type {
+
+    /**
+     * A variable bit signed integer value. By default this type uses
+     * {@link PackedInts} to compress the values, as an offset
+     * from the minimum value, as long as the value range
+     * fits into 2<sup>63</sup>-1. Otherwise,
+     * the default implementation falls back to fixed size 64bit
+     * integers ({@link #FIXED_INTS_64}).
+     * <p>
+     * NOTE: this type uses <tt>0</tt> as the default value without any
+     * distinction between provided <tt>0</tt> values during indexing. All
+     * documents without an explicit value will use <tt>0</tt> instead.
+     * Custom default values must be assigned explicitly.
+     * </p>
+     */
+    VAR_INTS,
+    
+    /**
+     * A 8 bit signed integer value. {@link Source} instances of
+     * this type return a <tt>byte</tt> array from {@link Source#getArray()}
+     * <p>
+     * NOTE: this type uses <tt>0</tt> as the default value without any
+     * distinction between provided <tt>0</tt> values during indexing. All
+     * documents without an explicit value will use <tt>0</tt> instead.
+     * Custom default values must be assigned explicitly.
+     * </p>
+     */
+    FIXED_INTS_8,
+    
+    /**
+     * A 16 bit signed integer value. {@link Source} instances of
+     * this type return a <tt>short</tt> array from {@link Source#getArray()}
+     * <p>
+     * NOTE: this type uses <tt>0</tt> as the default value without any
+     * distinction between provided <tt>0</tt> values during indexing. All
+     * documents without an explicit value will use <tt>0</tt> instead.
+     * Custom default values must be assigned explicitly.
+     * </p>
+     */
+    FIXED_INTS_16,
+    
+    /**
+     * A 32 bit signed integer value. {@link Source} instances of
+     * this type return a <tt>int</tt> array from {@link Source#getArray()}
+     * <p>
+     * NOTE: this type uses <tt>0</tt> as the default value without any
+     * distinction between provided <tt>0</tt> values during indexing. All
+     * documents without an explicit value will use <tt>0</tt> instead. 
+     * Custom default values must be assigned explicitly.
+     * </p>
+     */
+    FIXED_INTS_32,
+
+    /**
+     * A 64 bit signed integer value. {@link Source} instances of
+     * this type return a <tt>long</tt> array from {@link Source#getArray()}
+     * <p>
+     * NOTE: this type uses <tt>0</tt> as the default value without any
+     * distinction between provided <tt>0</tt> values during indexing. All
+     * documents without an explicit value will use <tt>0</tt> instead.
+     * Custom default values must be assigned explicitly.
+     * </p>
+     */
+    FIXED_INTS_64,
+    /**
+     * A 32 bit floating point value. By default there is no compression
+     * applied. To fit custom float values into less than 32bit either a custom
+     * implementation is needed or values must be encoded into a
+     * {@link #BYTES_FIXED_STRAIGHT} type. {@link Source} instances of
+     * this type return a <tt>float</tt> array from {@link Source#getArray()}
+     * <p>
+     * NOTE: this type uses <tt>0.0f</tt> as the default value without any
+     * distinction between provided <tt>0.0f</tt> values during indexing. All
+     * documents without an explicit value will use <tt>0.0f</tt> instead.
+     * Custom default values must be assigned explicitly.
+     * </p>
+     */
+    FLOAT_32,
+    /**
+     * 
+     * A 64 bit floating point value. By default there is no compression
+     * applied. To fit custom float values into less than 64bit either a custom
+     * implementation is needed or values must be encoded into a
+     * {@link #BYTES_FIXED_STRAIGHT} type. {@link Source} instances of
+     * this type return a <tt>double</tt> array from {@link Source#getArray()}
+     * <p>
+     * NOTE: this type uses <tt>0.0d</tt> as the default value without any
+     * distinction between provided <tt>0.0d</tt> values during indexing. All
+     * documents without an explicit value will use <tt>0.0d</tt> instead.
+     * Custom default values must be assigned explicitly.
+     * </p>
+     */
+    FLOAT_64,
+
+    // TODO(simonw): -- shouldn't lucene decide/detect straight vs
+    // deref, as well fixed vs var?
+    /**
+     * A fixed length straight byte[]. All values added to
+     * such a field must be of the same length. All bytes are stored sequentially
+     * for fast offset access.
+     * <p>
+     * NOTE: this type uses <tt>0 byte</tt> filled byte[] based on the length of the first seen
+     * value as the default value without any distinction between explicitly
+     * provided values during indexing. All documents without an explicit value
+     * will use the default instead.Custom default values must be assigned explicitly.
+     * </p>
+     */
+    BYTES_FIXED_STRAIGHT,
+
+    /**
+     * A fixed length dereferenced byte[] variant. Fields with
+     * this type only store distinct byte values and store an additional offset
+     * pointer per document to dereference the shared byte[].
+     * Use this type if your documents may share the same byte[].
+     * <p>
+     * NOTE: Fields of this type will not store values for documents without and
+     * explicitly provided value. If a documents value is accessed while no
+     * explicit value is stored the returned {@link BytesRef} will be a 0-length
+     * reference. Custom default values must be assigned explicitly.
+     * </p>
+     */
+    BYTES_FIXED_DEREF,
+
+    /**
+     * Variable length straight stored byte[] variant. All bytes are
+     * stored sequentially for compactness. Usage of this type via the
+     * disk-resident API might yield performance degradation since no additional
+     * index is used to advance by more than one document value at a time.
+     * <p>
+     * NOTE: Fields of this type will not store values for documents without an
+     * explicitly provided value. If a documents value is accessed while no
+     * explicit value is stored the returned {@link BytesRef} will be a 0-length
+     * byte[] reference. Custom default values must be assigned explicitly.
+     * </p>
+     */
+    BYTES_VAR_STRAIGHT,
+
+    /**
+     * A variable length dereferenced byte[]. Just like
+     * {@link #BYTES_FIXED_DEREF}, but allowing each
+     * document's value to be a different length.
+     * <p>
+     * NOTE: Fields of this type will not store values for documents without and
+     * explicitly provided value. If a documents value is accessed while no
+     * explicit value is stored the returned {@link BytesRef} will be a 0-length
+     * reference. Custom default values must be assigned explicitly.
+     * </p>
+     */
+    BYTES_VAR_DEREF,
+
+
+    /**
+     * A variable length pre-sorted byte[] variant. Just like
+     * {@link #BYTES_FIXED_SORTED}, but allowing each
+     * document's value to be a different length.
+     * <p>
+     * NOTE: Fields of this type will not store values for documents without and
+     * explicitly provided value. If a documents value is accessed while no
+     * explicit value is stored the returned {@link BytesRef} will be a 0-length
+     * reference.Custom default values must be assigned explicitly.
+     * </p>
+     * 
+     * @see SortedSource
+     */
+    BYTES_VAR_SORTED,
+    
+    /**
+     * A fixed length pre-sorted byte[] variant. Fields with this type only
+     * store distinct byte values and store an additional offset pointer per
+     * document to dereference the shared byte[]. The stored
+     * byte[] is presorted, by default by unsigned byte order,
+     * and allows access via document id, ordinal and by-value.
+     * Use this type if your documents may share the same byte[].
+     * <p>
+     * NOTE: Fields of this type will not store values for documents without and
+     * explicitly provided value. If a documents value is accessed while no
+     * explicit value is stored the returned {@link BytesRef} will be a 0-length
+     * reference. Custom default values must be assigned
+     * explicitly.
+     * </p>
+     * 
+     * @see SortedSource
+     */
+    BYTES_FIXED_SORTED
+    
+  }
+  
+  /**
+   * Abstract base class for {@link DocValues} {@link Source} cache.
+   * <p>
+   * {@link Source} instances loaded via {@link DocValues#load()} are entirely memory resident
+   * and need to be maintained by the caller. Each call to
+   * {@link DocValues#load()} will cause an entire reload of
+   * the underlying data. Source instances obtained from
+   * {@link DocValues#getSource()} and {@link DocValues#getSource()}
+   * respectively are maintained by a {@link SourceCache} that is closed (
+   * {@link #close(DocValues)}) once the {@link IndexReader} that created the
+   * {@link DocValues} instance is closed.
+   * <p>
+   * Unless {@link Source} instances are managed by another entity it is
+   * recommended to use the cached variants to obtain a source instance.
+   * <p>
+   * Implementation of this API must be thread-safe.
+   * 
+   * @see DocValues#setCache(SourceCache)
+   * @see DocValues#getSource()
+   * 
+   * @lucene.experimental
+   */
+  public static abstract class SourceCache {
+
+    /**
+     * Atomically loads a {@link Source} into the cache from the given
+     * {@link DocValues} and returns it iff no other {@link Source} has already
+     * been cached. Otherwise the cached source is returned.
+     * <p>
+     * This method will not return <code>null</code>
+     */
+    public abstract Source load(DocValues values) throws IOException;
+
+    /**
+     * Atomically invalidates the cached {@link Source} 
+     * instances if any and empties the cache.
+     */
+    public abstract void invalidate(DocValues values);
+
+    /**
+     * Atomically closes the cache and frees all resources.
+     */
+    public synchronized void close(DocValues values) {
+      invalidate(values);
+    }
+
+    /**
+     * Simple per {@link DocValues} instance cache implementation that holds a
+     * {@link Source} a member variable.
+     * <p>
+     * If a {@link DirectSourceCache} instance is closed or invalidated the cached
+     * reference are simply set to <code>null</code>
+     */
+    public static final class DirectSourceCache extends SourceCache {
+      private Source ref;
+
+      public synchronized Source load(DocValues values) throws IOException {
+        if (ref == null) {
+          ref = values.load();
+        }
+        return ref;
+      }
+
+      public synchronized void invalidate(DocValues values) {
+        ref = null;
+      }
+    }
+  }
+}


diff -ruN -x .svn -x build lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/FieldInfo.java lucene-3622/lucene/src/java/org/apache/lucene/index/FieldInfo.java
--- lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/FieldInfo.java	2011-12-06 22:37:24.949053756 -0500
+++ lucene-3622/lucene/src/java/org/apache/lucene/index/FieldInfo.java	2011-12-09 11:10:18.668849648 -0500
@@ -1,6 +1,6 @@
 package org.apache.lucene.index;
 
-import org.apache.lucene.index.values.ValueType;
+import org.apache.lucene.index.DocValues;
 
 /**
  * Licensed to the Apache Software Foundation (ASF) under one or more
@@ -25,7 +25,7 @@
   public final int number;
 
   public boolean isIndexed;
-  ValueType docValues;
+  private DocValues.Type docValues;
 
 
   // true if term vector for this field should be stored
@@ -57,7 +57,7 @@
    */
   public FieldInfo(String name, boolean isIndexed, int number, boolean storeTermVector, 
             boolean storePositionWithTermVector,  boolean storeOffsetWithTermVector, 
-            boolean omitNorms, boolean storePayloads, IndexOptions indexOptions, ValueType docValues) {
+            boolean omitNorms, boolean storePayloads, IndexOptions indexOptions, DocValues.Type docValues) {
     this.name = name;
     this.isIndexed = isIndexed;
     this.number = number;
@@ -118,13 +118,14 @@
     }
     assert this.indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS || !this.storePayloads;
   }
-  void setDocValues(ValueType v) {
+
+  void setDocValuesType(DocValues.Type v) {
     if (docValues == null) {
       docValues = v;
     }
   }
   
-  public void resetDocValues(ValueType v) {
+  public void resetDocValuesType(DocValues.Type v) {
     if (docValues != null) {
       docValues = v;
     }
@@ -134,42 +135,13 @@
     return docValues != null;
   }
 
-  public ValueType getDocValues() {
+  public DocValues.Type getDocValuesType() {
     return docValues;
   }
-  
-  private boolean vectorsCommitted;
-  private boolean docValuesCommitted;
- 
-  /**
-   * Reverts all uncommitted changes on this {@link FieldInfo}
-   * @see #commitVectors()
-   */
-  void revertUncommitted() {
-    if (storeTermVector && !vectorsCommitted) {
-      storeOffsetWithTermVector = false;
-      storePositionWithTermVector = false;
-      storeTermVector = false;  
-    }
-    
-    if (docValues != null && !docValuesCommitted) {
-      docValues = null;
-    }
-  }
 
-  /**
-   * Commits term vector modifications. Changes to term-vectors must be
-   * explicitly committed once the necessary files are created. If those changes
-   * are not committed subsequent {@link #revertUncommitted()} will reset the
-   * all term-vector flags before the next document.
-   */
-  void commitVectors() {
-    assert storeTermVector;
-    vectorsCommitted = true;
-  }
-  
-  void commitDocValues() {
-    assert hasDocValues();
-    docValuesCommitted = true;
+  public void setStoreTermVectors(boolean withPositions, boolean withOffsets) {
+    storeTermVector = true;
+    storePositionWithTermVector |= withPositions;
+    storeOffsetWithTermVector |= withOffsets;
   }
 }


diff -ruN -x .svn -x build lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/FieldInfos.java lucene-3622/lucene/src/java/org/apache/lucene/index/FieldInfos.java
--- lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/FieldInfos.java	2011-12-09 08:23:19.920675177 -0500
+++ lucene-3622/lucene/src/java/org/apache/lucene/index/FieldInfos.java	2011-12-09 11:10:18.672849647 -0500
@@ -25,7 +25,7 @@
 import java.util.TreeMap;
 
 import org.apache.lucene.index.FieldInfo.IndexOptions;
-import org.apache.lucene.index.values.ValueType;
+import org.apache.lucene.index.DocValues;
 
 /** Access to the Field Info file that describes document fields and whether or
  *  not they are indexed. Each segment has a separate Field Info file. Objects
@@ -299,20 +299,30 @@
    */
   synchronized public FieldInfo addOrUpdate(String name, boolean isIndexed, boolean storeTermVector,
                        boolean storePositionWithTermVector, boolean storeOffsetWithTermVector,
-                       boolean omitNorms, boolean storePayloads, IndexOptions indexOptions, ValueType docValues) {
+                       boolean omitNorms, boolean storePayloads, IndexOptions indexOptions, DocValues.Type docValues) {
     return addOrUpdateInternal(name, -1, isIndexed, storeTermVector, storePositionWithTermVector,
                                storeOffsetWithTermVector, omitNorms, storePayloads, indexOptions, docValues);
   }
 
-  synchronized public FieldInfo addOrUpdate(String name, IndexableFieldType fieldType, boolean scorePayloads, ValueType docValues) {
-    return addOrUpdateInternal(name, -1, fieldType.indexed(), fieldType.storeTermVectors(),
-        fieldType.storeTermVectorPositions(), fieldType.storeTermVectorOffsets(), fieldType.omitNorms(), scorePayloads,
-        fieldType.indexOptions(), docValues);
+  // NOTE: this method does not carry over termVector
+  // booleans nor docValuesType; the indexer chain
+  // (TermVectorsConsumerPerField, DocFieldProcessor) must
+  // set these fields when they succeed in consuming
+  // the document:
+  public FieldInfo addOrUpdate(String name, IndexableFieldType fieldType) {
+    // TODO: really, indexer shouldn't even call this
+    // method (it's only called from DocFieldProcessor);
+    // rather, each component in the chain should update
+    // what it "owns".  EG fieldType.indexOptions() should
+    // be updated by maybe FreqProxTermsWriterPerField:
+    return addOrUpdateInternal(name, -1, fieldType.indexed(), false, false, false,
+                               fieldType.omitNorms(), false,
+                               fieldType.indexOptions(), null);
   }
 
   synchronized private FieldInfo addOrUpdateInternal(String name, int preferredFieldNumber, boolean isIndexed,
       boolean storeTermVector, boolean storePositionWithTermVector, boolean storeOffsetWithTermVector,
-      boolean omitNorms, boolean storePayloads, IndexOptions indexOptions, ValueType docValues) {
+      boolean omitNorms, boolean storePayloads, IndexOptions indexOptions, DocValues.Type docValues) {
     if (globalFieldNumbers == null) {
       throw new IllegalStateException("FieldInfos are read-only, create a new instance with a global field map to make modifications to FieldInfos");
     }
@@ -322,7 +332,7 @@
       fi = addInternal(name, fieldNumber, isIndexed, storeTermVector, storePositionWithTermVector, storeOffsetWithTermVector, omitNorms, storePayloads, indexOptions, docValues);
     } else {
       fi.update(isIndexed, storeTermVector, storePositionWithTermVector, storeOffsetWithTermVector, omitNorms, storePayloads, indexOptions);
-      fi.setDocValues(docValues);
+      fi.setDocValuesType(docValues);
     }
     version++;
     return fi;
@@ -333,7 +343,7 @@
     return addOrUpdateInternal(fi.name, fi.number, fi.isIndexed, fi.storeTermVector,
                fi.storePositionWithTermVector, fi.storeOffsetWithTermVector,
                fi.omitNorms, fi.storePayloads,
-               fi.indexOptions, fi.docValues);
+               fi.indexOptions, fi.getDocValuesType());
   }
   
   /*
@@ -341,7 +351,7 @@
    */
   private FieldInfo addInternal(String name, int fieldNumber, boolean isIndexed,
                                 boolean storeTermVector, boolean storePositionWithTermVector, 
-                                boolean storeOffsetWithTermVector, boolean omitNorms, boolean storePayloads, IndexOptions indexOptions, ValueType docValuesType) {
+                                boolean storeOffsetWithTermVector, boolean omitNorms, boolean storePayloads, IndexOptions indexOptions, DocValues.Type docValuesType) {
     // don't check modifiable here since we use that to initially build up FIs
     if (globalFieldNumbers != null) {
       globalFieldNumbers.setIfNotSet(fieldNumber, name);
@@ -429,16 +439,6 @@
     return version;
   }
   
-  /**
-   * Reverts all uncommitted changes 
-   * @see FieldInfo#revertUncommitted()
-   */
-  void revertUncommitted() {
-    for (FieldInfo fieldInfo : this) {
-      fieldInfo.revertUncommitted();
-    }
-  }
-  
   final FieldInfos asReadOnly() {
     if (isReadOnly()) {
       return this;


diff -ruN -x .svn -x build lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/FilterIndexReader.java lucene-3622/lucene/src/java/org/apache/lucene/index/FilterIndexReader.java
--- lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/FilterIndexReader.java	2011-12-09 08:23:19.920675177 -0500
+++ lucene-3622/lucene/src/java/org/apache/lucene/index/FilterIndexReader.java	2011-12-10 10:39:08.598321690 -0500
@@ -17,7 +17,6 @@
  * limitations under the License.
  */
 
-import org.apache.lucene.index.codecs.PerDocValues;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.util.Bits;
 import org.apache.lucene.util.BytesRef;
@@ -422,9 +421,9 @@
   }
 
   @Override
-  public PerDocValues perDocValues() throws IOException {
+  public DocValues docValues(String field) throws IOException {
     ensureOpen();
-    return in.perDocValues();
+    return in.docValues(field);
   }
 
   @Override


diff -ruN -x .svn -x build lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/IndexableField.java lucene-3622/lucene/src/java/org/apache/lucene/index/IndexableField.java
--- lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/IndexableField.java	2011-12-06 18:45:03.980810982 -0500
+++ lucene-3622/lucene/src/java/org/apache/lucene/index/IndexableField.java	2011-12-10 13:07:57.586477184 -0500
@@ -23,8 +23,7 @@
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.document.NumericField;
-import org.apache.lucene.index.values.PerDocFieldValues;
-import org.apache.lucene.index.values.ValueType;
+import org.apache.lucene.index.DocValues;
 import org.apache.lucene.util.BytesRef;
 
 // TODO: how to handle versioning here...?
@@ -77,10 +76,10 @@
   public IndexableFieldType fieldType();
   
   /** Non-null if doc values should be indexed */
-  public PerDocFieldValues docValues();
+  public DocValue docValue();
 
-  /** DocValues type; only used if docValues is non-null */
-  public ValueType docValuesType();
+  /** DocValues type; only used if docValue is non-null */
+  public DocValues.Type docValueType();
 
   /**
    * Creates the TokenStream used for indexing this field.  If appropriate,


diff -ruN -x .svn -x build lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/IndexReader.java lucene-3622/lucene/src/java/org/apache/lucene/index/IndexReader.java
--- lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/IndexReader.java	2011-12-11 19:00:23.532350038 -0500
+++ lucene-3622/lucene/src/java/org/apache/lucene/index/IndexReader.java	2011-12-12 15:02:47.925606393 -0500
@@ -28,8 +28,6 @@
 
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.DocumentStoredFieldVisitor;
-import org.apache.lucene.index.codecs.PerDocValues;
-import org.apache.lucene.index.values.IndexDocValues;
 import org.apache.lucene.search.FieldCache; // javadocs
 import org.apache.lucene.search.SearcherManager; // javadocs
 import org.apache.lucene.store.*;
@@ -771,21 +769,6 @@
    * through them yourself. */
   public abstract Fields fields() throws IOException;
   
-  /**
-   * Returns {@link PerDocValues} for this reader.
-   * This method may return null if the reader has no per-document
-   * values stored.
-   *
-   * <p><b>NOTE</b>: if this is a multi reader ({@link
-   * #getSequentialSubReaders} is not null) then this
-   * method will throw UnsupportedOperationException.  If
-   * you really need {@link PerDocValues} for such a reader,
-   * use {@link MultiPerDocValues#getPerDocs(IndexReader)}.  However, for
-   * performance reasons, it's best to get all sub-readers
-   * using {@link ReaderUtil#gatherSubReaders} and iterate
-   * through them yourself. */
-  public abstract PerDocValues perDocValues() throws IOException;
-
   public final int docFreq(Term term) throws IOException {
     return docFreq(term.field(), term.bytes());
   }
@@ -1144,14 +1127,20 @@
     throw new UnsupportedOperationException("This reader does not support this method.");
   }
   
-  public final IndexDocValues docValues(String field) throws IOException {
-    ensureOpen();
-    final PerDocValues perDoc = perDocValues();
-    if (perDoc == null) {
-      return null;
-    }
-    return perDoc.docValues(field);
-  }
+  /**
+   * Returns {@link DocValues} for this field.
+   * This method may return null if the reader has no per-document
+   * values stored.
+   *
+   * <p><b>NOTE</b>: if this is a multi reader ({@link
+   * #getSequentialSubReaders} is not null) then this
+   * method will throw UnsupportedOperationException.  If
+   * you really need {@link DocValues} for such a reader,
+   * use {@link MultiDocValues#getDocValues(IndexReader,String)}.  However, for
+   * performance reasons, it's best to get all sub-readers
+   * using {@link ReaderUtil#gatherSubReaders} and iterate
+   * through them yourself. */
+  public abstract DocValues docValues(String field) throws IOException;
 
   private volatile Fields fields;
 
@@ -1167,21 +1156,6 @@
     return fields;
   }
   
-  private volatile PerDocValues perDocValues;
-  
-  /** @lucene.internal */
-  void storePerDoc(PerDocValues perDocValues) {
-    ensureOpen();
-    this.perDocValues = perDocValues;
-  }
-
-  /** @lucene.internal */
-  PerDocValues retrievePerDoc() {
-    ensureOpen();
-    return perDocValues;
-  }  
-  
-
   /**
    * A struct like class that represents a hierarchical relationship between
    * {@link IndexReader} instances. 


diff -ruN -x .svn -x build lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/MultiDocValues.java lucene-3622/lucene/src/java/org/apache/lucene/index/MultiDocValues.java
--- lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/MultiDocValues.java	1969-12-31 19:00:00.000000000 -0500
+++ lucene-3622/lucene/src/java/org/apache/lucene/index/MultiDocValues.java	2011-12-10 12:20:56.402428054 -0500
@@ -0,0 +1,247 @@
+package org.apache.lucene.index;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.List;
+
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.ReaderUtil;
+import org.apache.lucene.util.ReaderUtil.Gather;
+
+/**
+ * A wrapper for compound IndexReader providing access to per segment
+ * {@link DocValues}
+ * 
+ * @lucene.experimental
+ * @lucene.internal
+ */
+public class MultiDocValues extends DocValues {
+
+  public static class DocValuesSlice {
+    public final static DocValuesSlice[] EMPTY_ARRAY = new DocValuesSlice[0];
+    final int start;
+    final int length;
+    DocValues docValues;
+
+    public DocValuesSlice(DocValues docValues, int start, int length) {
+      this.docValues = docValues;
+      this.start = start;
+      this.length = length;
+    }
+  }
+
+  private DocValuesSlice[] slices;
+  private int[] starts;
+  private Type type;
+  private int valueSize;
+
+  private MultiDocValues(DocValuesSlice[] slices, int[] starts, TypePromoter promotedType) {
+    this.starts = starts;
+    this.slices = slices;
+    this.type = promotedType.type();
+    this.valueSize = promotedType.getValueSize();
+  }
+  
+  /**
+   * Returns a single {@link DocValues} instance for this field, merging
+   * their values on the fly.
+   * 
+   * <p>
+   * <b>NOTE</b>: this is a slow way to access DocValues. It's better to get the
+   * sub-readers (using {@link Gather}) and iterate through them yourself.
+   */
+  public static DocValues getDocValues(IndexReader r, final String field) throws IOException {
+    final IndexReader[] subs = r.getSequentialSubReaders();
+    if (subs == null) {
+      // already an atomic reader
+      return r.docValues(field);
+    } else if (subs.length == 0) {
+      // no fields
+      return null;
+    } else if (subs.length == 1) {
+      return getDocValues(subs[0], field);
+    } else {      
+      final List<DocValuesSlice> slices = new ArrayList<DocValuesSlice>();
+      
+      final TypePromoter promotedType[] = new TypePromoter[1];
+      promotedType[0] = TypePromoter.getIdentityPromoter();
+      
+      // gather all docvalues fields, accumulating a promoted type across 
+      // potentially incompatible types
+      
+      new ReaderUtil.Gather(r) {
+        @Override
+        protected void add(int base, IndexReader r) throws IOException {
+          final DocValues d = r.docValues(field);
+          if (d != null) {
+            TypePromoter incoming = TypePromoter.create(d.type(), d.getValueSize());
+            promotedType[0] = promotedType[0].promote(incoming);
+          }
+          slices.add(new DocValuesSlice(d, base, r.maxDoc()));
+        }
+      }.run();
+      
+      // return null if no docvalues encountered anywhere
+      if (promotedType[0] == TypePromoter.getIdentityPromoter()) {
+        return null;
+      }
+           
+      // populate starts and fill gaps with empty docvalues 
+      int starts[] = new int[slices.size()];
+      for (int i = 0; i < slices.size(); i++) {
+        DocValuesSlice slice = slices.get(i);
+        starts[i] = slice.start;
+        if (slice.docValues == null) {
+          slice.docValues = new EmptyDocValues(slice.length, promotedType[0].type());
+        }
+      }
+      
+      return new MultiDocValues(slices.toArray(new DocValuesSlice[slices.size()]), starts, promotedType[0]);
+    }
+  }
+
+  @Override
+  public Source load() throws IOException {
+    return new MultiSource(slices, starts, false, type);
+  }
+
+  public static class EmptyDocValues extends DocValues {
+    final int maxDoc;
+    final Source emptySource;
+
+    public EmptyDocValues(int maxDoc, Type type) {
+      this.maxDoc = maxDoc;
+      this.emptySource = new EmptySource(type);
+    }
+
+    @Override
+    public Source load() throws IOException {
+      return emptySource;
+    }
+
+    @Override
+    public Type type() {
+      return emptySource.type();
+    }
+
+
+    @Override
+    public Source getDirectSource() throws IOException {
+      return emptySource;
+    }
+  }
+
+  private static class MultiSource extends Source {
+    private int numDocs = 0;
+    private int start = 0;
+    private Source current;
+    private final int[] starts;
+    private final DocValuesSlice[] slices;
+    private boolean direct;
+
+    public MultiSource(DocValuesSlice[] slices, int[] starts, boolean direct, Type type) {
+      super(type);
+      this.slices = slices;
+      this.starts = starts;
+      assert slices.length != 0;
+      this.direct = direct;
+    }
+
+    public long getInt(int docID) {
+      final int doc = ensureSource(docID);
+      return current.getInt(doc);
+    }
+
+    private final int ensureSource(int docID) {
+      if (docID >= start && docID < start+numDocs) {
+        return docID - start;
+      } else {
+        final int idx = ReaderUtil.subIndex(docID, starts);
+        assert idx >= 0 && idx < slices.length : "idx was " + idx
+            + " for doc id: " + docID + " slices : " + Arrays.toString(starts);
+        assert slices[idx] != null;
+        try {
+          if (direct) {
+            current = slices[idx].docValues.getDirectSource();
+          } else {
+            current = slices[idx].docValues.getSource();
+          }
+        } catch (IOException e) {
+          throw new RuntimeException("load failed", e); // TODO how should we
+          // handle this
+        }
+
+        start = slices[idx].start;
+        numDocs = slices[idx].length;
+        return docID - start;
+      }
+    }
+
+    public double getFloat(int docID) {
+      final int doc = ensureSource(docID);
+      return current.getFloat(doc);
+    }
+
+    public BytesRef getBytes(int docID, BytesRef bytesRef) {
+      final int doc = ensureSource(docID);
+      return current.getBytes(doc, bytesRef);
+    }
+  }
+
+  // TODO: this is dup of DocValues.getDefaultSource()?
+  private static class EmptySource extends Source {
+
+    public EmptySource(Type type) {
+      super(type);
+    }
+
+    @Override
+    public BytesRef getBytes(int docID, BytesRef ref) {
+      ref.length = 0;
+      return ref;
+
+    }
+
+    @Override
+    public double getFloat(int docID) {
+      return 0d;
+    }
+
+    @Override
+    public long getInt(int docID) {
+      return 0;
+    }
+  }
+
+  @Override
+  public Type type() {
+    return type;
+  }
+
+  @Override
+  public int getValueSize() {
+    return valueSize;
+  }
+
+  @Override
+  public Source getDirectSource() throws IOException {
+    return new MultiSource(slices, starts, true, type);
+  }
+}


diff -ruN -x .svn -x build lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/MultiPerDocValues.java lucene-3622/lucene/src/java/org/apache/lucene/index/MultiPerDocValues.java
--- lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/MultiPerDocValues.java	2011-12-06 18:45:03.856810979 -0500
+++ lucene-3622/lucene/src/java/org/apache/lucene/index/MultiPerDocValues.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,162 +0,0 @@
-package org.apache.lucene.index;
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Collection;
-import java.util.List;
-import java.util.Map;
-import java.util.TreeSet;
-import java.util.concurrent.ConcurrentHashMap;
-
-import org.apache.lucene.index.codecs.PerDocValues;
-import org.apache.lucene.index.values.IndexDocValues;
-import org.apache.lucene.index.values.MultiIndexDocValues;
-import org.apache.lucene.index.values.ValueType;
-import org.apache.lucene.index.values.MultiIndexDocValues.DocValuesIndex;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.ReaderUtil;
-import org.apache.lucene.util.ReaderUtil.Gather;
-
-/**
- * Exposes per-document values, merged from per-document values API of
- * sub-segments. This is useful when you're interacting with an {@link IndexReader}
- * implementation that consists of sequential sub-readers (eg DirectoryReader
- * or {@link MultiReader}). 
- * 
- * <p>
- * <b>NOTE</b>: for multi readers, you'll get better performance by gathering
- * the sub readers using {@link ReaderUtil#gatherSubReaders} and then operate
- * per-reader, instead of using this class.
- * 
- * @lucene.experimental
- */
-public class MultiPerDocValues extends PerDocValues {
-  private final PerDocValues[] subs;
-  private final ReaderUtil.Slice[] subSlices;
-  private final Map<String, IndexDocValues> docValues = new ConcurrentHashMap<String, IndexDocValues>();
-  private final TreeSet<String> fields;
-
-  public MultiPerDocValues(PerDocValues[] subs, ReaderUtil.Slice[] subSlices) {
-    this.subs = subs;
-    this.subSlices = subSlices;
-    fields = new TreeSet<String>();
-    for (PerDocValues sub : subs) {
-      fields.addAll(sub.fields());
-    }
-  }
-
-  /**
-   * Returns a single {@link PerDocValues} instance for this reader, merging
-   * their values on the fly. This method will not return <code>null</code>.
-   * 
-   * <p>
-   * <b>NOTE</b>: this is a slow way to access postings. It's better to get the
-   * sub-readers (using {@link Gather}) and iterate through them yourself.
-   */
-  public static PerDocValues getPerDocs(IndexReader r) throws IOException {
-    final IndexReader[] subs = r.getSequentialSubReaders();
-    if (subs == null) {
-      // already an atomic reader
-      return r.perDocValues();
-    } else if (subs.length == 0) {
-      // no fields
-      return null;
-    } else if (subs.length == 1) {
-      return getPerDocs(subs[0]);
-    }
-    PerDocValues perDocValues = r.retrievePerDoc();
-    if (perDocValues == null) {
-
-      final List<PerDocValues> producer = new ArrayList<PerDocValues>();
-      final List<ReaderUtil.Slice> slices = new ArrayList<ReaderUtil.Slice>();
-
-      new ReaderUtil.Gather(r) {
-        @Override
-        protected void add(int base, IndexReader r) throws IOException {
-          final PerDocValues f = r.perDocValues();
-          if (f != null) {
-            producer.add(f);
-            slices
-                .add(new ReaderUtil.Slice(base, r.maxDoc(), producer.size() - 1));
-          }
-        }
-      }.run();
-
-      if (producer.size() == 0) {
-        return null;
-      } else if (producer.size() == 1) {
-        perDocValues = producer.get(0);
-      } else {
-        perDocValues = new MultiPerDocValues(
-            producer.toArray(PerDocValues.EMPTY_ARRAY),
-            slices.toArray(ReaderUtil.Slice.EMPTY_ARRAY));
-      }
-      r.storePerDoc(perDocValues);
-    }
-    return perDocValues;
-  }
-
-  public IndexDocValues docValues(String field) throws IOException {
-    IndexDocValues result = docValues.get(field);
-    if (result == null) {
-      // Lazy init: first time this field is requested, we
-      // create & add to docValues:
-      final List<MultiIndexDocValues.DocValuesIndex> docValuesIndex = new ArrayList<MultiIndexDocValues.DocValuesIndex>();
-      int docsUpto = 0;
-      ValueType type = null;
-      // Gather all sub-readers that share this field
-      for (int i = 0; i < subs.length; i++) {
-        IndexDocValues values = subs[i].docValues(field);
-        final int start = subSlices[i].start;
-        final int length = subSlices[i].length;
-        if (values != null) {
-          if (docsUpto != start) {
-            type = values.type();
-            docValuesIndex.add(new MultiIndexDocValues.DocValuesIndex(
-                new MultiIndexDocValues.EmptyDocValues(start, type), docsUpto, start
-                    - docsUpto));
-          }
-          docValuesIndex.add(new MultiIndexDocValues.DocValuesIndex(values, start,
-              length));
-          docsUpto = start + length;
-
-        } else if (i + 1 == subs.length && !docValuesIndex.isEmpty()) {
-          docValuesIndex.add(new MultiIndexDocValues.DocValuesIndex(
-              new MultiIndexDocValues.EmptyDocValues(start, type), docsUpto, start
-                  - docsUpto));
-        }
-      }
-      if (docValuesIndex.isEmpty()) {
-        return null;
-      }
-      result = new MultiIndexDocValues(
-          docValuesIndex.toArray(DocValuesIndex.EMPTY_ARRAY));
-      docValues.put(field, result);
-    }
-    return result;
-  }
-
-  public void close() throws IOException {
-    IOUtils.close(this.subs);
-  }
-
-  @Override
-  public Collection<String> fields() {
-    return fields;
-  }
-}


diff -ruN -x .svn -x build lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/MultiReader.java lucene-3622/lucene/src/java/org/apache/lucene/index/MultiReader.java
--- lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/MultiReader.java	2011-12-11 19:00:23.532350038 -0500
+++ lucene-3622/lucene/src/java/org/apache/lucene/index/MultiReader.java	2011-12-12 15:04:58.317608663 -0500
@@ -21,8 +21,6 @@
 import java.util.Collection;
 import java.util.concurrent.ConcurrentHashMap;
 
-import org.apache.lucene.util.Bits;
-import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.MapBackedSet;
 
 /** An IndexReader which reads multiple indexes, appending
@@ -187,5 +185,4 @@
       sub.removeReaderFinishedListener(listener);
     }
   }
-
 }


diff -ruN -x .svn -x build lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/ParallelReader.java lucene-3622/lucene/src/java/org/apache/lucene/index/ParallelReader.java
--- lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/ParallelReader.java	2011-12-10 12:55:54.390464590 -0500
+++ lucene-3622/lucene/src/java/org/apache/lucene/index/ParallelReader.java	2011-12-10 12:56:23.746465101 -0500
@@ -17,8 +17,6 @@
  * limitations under the License.
  */
 
-import org.apache.lucene.index.codecs.PerDocValues;
-import org.apache.lucene.index.values.IndexDocValues;
 import org.apache.lucene.util.Bits;
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.MapBackedSet;
@@ -59,7 +57,6 @@
   private boolean hasDeletions;
 
   private final ParallelFields fields = new ParallelFields();
-  private final ParallelPerDocs perDocs = new ParallelPerDocs();
 
  /** Construct a ParallelReader. 
   * <p>Note that all subreaders are closed if this ParallelReader is closed.</p>
@@ -132,7 +129,6 @@
       if (fieldToReader.get(field) == null) {
         fieldToReader.put(field, reader);
         this.fields.addField(field, MultiFields.getFields(reader).terms(field));
-        this.perDocs.addField(field, reader);
       }
     }
 
@@ -464,41 +460,10 @@
     }
   }
 
+  // TODO: I suspect this is completely untested!!!!!
   @Override
-  public PerDocValues perDocValues() throws IOException {
-    ensureOpen();
-    return perDocs;
-  }
-  
-  // Single instance of this, per ParallelReader instance
-  private static final class ParallelPerDocs extends PerDocValues {
-    final TreeMap<String,IndexDocValues> fields = new TreeMap<String,IndexDocValues>();
-
-    void addField(String field, IndexReader r) throws IOException {
-      PerDocValues perDocs = MultiPerDocValues.getPerDocs(r);
-      if (perDocs != null) {
-        fields.put(field, perDocs.docValues(field));
-      }
-    }
-
-    @Override
-    public void close() throws IOException {
-      // nothing to do here
-    }
-
-    @Override
-    public IndexDocValues docValues(String field) throws IOException {
-      return fields.get(field);
-    }
-
-    @Override
-    public Collection<String> fields() {
-      return fields.keySet();
-    }
+  public DocValues docValues(String field) throws IOException {
+    IndexReader reader = fieldToReader.get(field);
+    return reader == null ? null : MultiDocValues.getDocValues(reader, field);
   }
 }
-
-
-
-
-


diff -ruN -x .svn -x build lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/SegmentCoreReaders.java lucene-3622/lucene/src/java/org/apache/lucene/index/SegmentCoreReaders.java
--- lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/SegmentCoreReaders.java	2011-12-09 08:23:19.932675178 -0500
+++ lucene-3622/lucene/src/java/org/apache/lucene/index/SegmentCoreReaders.java	2011-12-10 10:29:15.354311359 -0500
@@ -22,10 +22,10 @@
 
 import org.apache.lucene.index.codecs.Codec;
 import org.apache.lucene.index.codecs.NormsReader;
+import org.apache.lucene.index.codecs.PerDocProducer;
 import org.apache.lucene.index.codecs.PostingsFormat;
 import org.apache.lucene.index.codecs.FieldsProducer;
 import org.apache.lucene.index.codecs.StoredFieldsReader;
-import org.apache.lucene.index.codecs.PerDocValues;
 import org.apache.lucene.index.codecs.TermVectorsReader;
 import org.apache.lucene.store.CompoundFileDirectory;
 import org.apache.lucene.store.Directory;
@@ -48,7 +48,7 @@
   final FieldInfos fieldInfos;
   
   final FieldsProducer fields;
-  final PerDocValues perDocProducer;
+  final PerDocProducer perDocProducer;
   final NormsReader norms;
 
   final Directory dir;


diff -ruN -x .svn -x build lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/SegmentMerger.java lucene-3622/lucene/src/java/org/apache/lucene/index/SegmentMerger.java
--- lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/SegmentMerger.java	2011-12-09 08:23:19.920675177 -0500
+++ lucene-3622/lucene/src/java/org/apache/lucene/index/SegmentMerger.java	2011-12-09 11:10:18.756849650 -0500
@@ -20,10 +20,13 @@
 import java.io.IOException;
 import java.util.ArrayList;
 import java.util.Collection;
+import java.util.HashMap;
 import java.util.List;
+import java.util.Map;
 
 import org.apache.lucene.index.FieldInfo.IndexOptions;
 import org.apache.lucene.index.IndexReader.FieldOption;
+import org.apache.lucene.index.DocValues;
 import org.apache.lucene.index.codecs.Codec;
 import org.apache.lucene.index.codecs.FieldInfosWriter;
 import org.apache.lucene.index.codecs.FieldsConsumer;
@@ -31,7 +34,6 @@
 import org.apache.lucene.index.codecs.StoredFieldsWriter;
 import org.apache.lucene.index.codecs.PerDocConsumer;
 import org.apache.lucene.index.codecs.TermVectorsWriter;
-import org.apache.lucene.index.values.ValueType;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.IOContext;
 import org.apache.lucene.util.Bits;
@@ -131,9 +133,7 @@
       int numMerged = mergeVectors();
       assert numMerged == mergeState.mergedDocCount;
     }
-    // write FIS once merge is done. IDV might change types or drops fields
-    FieldInfosWriter fieldInfosWriter = codec.fieldInfosFormat().getFieldInfosWriter();
-    fieldInfosWriter.write(directory, segment, mergeState.fieldInfos, context);
+
     return mergeState;
   }
 
@@ -186,15 +186,40 @@
       }
     }
   }
+  
+  // returns an updated typepromoter (tracking type and size) given a previous one,
+  // and a newly encountered docvalues
+  private TypePromoter mergeDocValuesType(TypePromoter previous, DocValues docValues) {
+    TypePromoter incoming = TypePromoter.create(docValues.type(),  docValues.getValueSize());
+    if (previous == null) {
+      previous = TypePromoter.getIdentityPromoter();
+    }
+    TypePromoter promoted = previous.promote(incoming);
+    if (promoted == null) {
+      // type is incompatible: promote to BYTES_VAR_STRAIGHT
+      return TypePromoter.create(DocValues.Type.BYTES_VAR_STRAIGHT, TypePromoter.VAR_TYPE_VALUE_SIZE);
+    } else {
+      return promoted;
+    }
+  }
 
   private void mergeFieldInfos() throws IOException {
+    // mapping from all docvalues fields found to their promoted types
+    // this is because FieldInfos does not store the valueSize
+    Map<FieldInfo,TypePromoter> docValuesTypes = new HashMap<FieldInfo,TypePromoter>();
+
     for (MergeState.IndexReaderAndLiveDocs readerAndLiveDocs : mergeState.readers) {
       final IndexReader reader = readerAndLiveDocs.reader;
       if (reader instanceof SegmentReader) {
         SegmentReader segmentReader = (SegmentReader) reader;
         FieldInfos readerFieldInfos = segmentReader.fieldInfos();
         for (FieldInfo fi : readerFieldInfos) {
-          mergeState.fieldInfos.add(fi);
+          FieldInfo merged = mergeState.fieldInfos.add(fi);
+          // update the type promotion mapping for this reader
+          if (fi.hasDocValues()) {
+            TypePromoter previous = docValuesTypes.get(merged);
+            docValuesTypes.put(merged, mergeDocValuesType(previous, reader.docValues(fi.name))); 
+          }
         }
       } else {
         addIndexed(reader, mergeState.fieldInfos, reader.getFieldNames(FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false, IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);
@@ -209,10 +234,33 @@
         Collection<String> dvNames = reader.getFieldNames(FieldOption.DOC_VALUES);
         mergeState.fieldInfos.addOrUpdate(dvNames, false);
         for (String dvName : dvNames) {
-          mergeState.fieldInfos.fieldInfo(dvName).setDocValues(reader.docValues(dvName).type());
+          FieldInfo merged = mergeState.fieldInfos.fieldInfo(dvName);
+          DocValues docValues = reader.docValues(dvName);
+          merged.setDocValuesType(docValues.type());
+          TypePromoter previous = docValuesTypes.get(merged);
+          docValuesTypes.put(merged, mergeDocValuesType(previous, docValues));
         }
       }
     }
+    
+    // update any promoted doc values types:
+    for (Map.Entry<FieldInfo,TypePromoter> e : docValuesTypes.entrySet()) {
+      FieldInfo fi = e.getKey();
+      TypePromoter promoter = e.getValue();
+      if (promoter == null) {
+        fi.resetDocValuesType(null);
+      } else {
+        assert promoter != TypePromoter.getIdentityPromoter();
+        if (fi.getDocValuesType() != promoter.type()) {
+          // reset the type if we got promoted
+          fi.resetDocValuesType(promoter.type());
+        }
+      }
+    }
+    
+    // write the merged infos
+    FieldInfosWriter fieldInfosWriter = codec.fieldInfosFormat().getFieldInfosWriter();
+    fieldInfosWriter.write(directory, segment, mergeState.fieldInfos, context);
   }
 
   /**


diff -ruN -x .svn -x build lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/SegmentReader.java lucene-3622/lucene/src/java/org/apache/lucene/index/SegmentReader.java
--- lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/SegmentReader.java	2011-12-09 08:23:19.920675177 -0500
+++ lucene-3622/lucene/src/java/org/apache/lucene/index/SegmentReader.java	2011-12-10 10:32:10.814314415 -0500
@@ -27,8 +27,8 @@
 
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.index.FieldInfo.IndexOptions;
+import org.apache.lucene.index.codecs.PerDocProducer;
 import org.apache.lucene.index.codecs.StoredFieldsReader;
-import org.apache.lucene.index.codecs.PerDocValues;
 import org.apache.lucene.index.codecs.TermVectorsReader;
 import org.apache.lucene.store.IOContext;
 import org.apache.lucene.util.BitVector;
@@ -425,9 +425,13 @@
   }
   
   @Override
-  public PerDocValues perDocValues() throws IOException {
+  public DocValues docValues(String field) throws IOException {
     ensureOpen();
-    return core.perDocProducer;
+    final PerDocProducer perDoc = core.perDocProducer;
+    if (perDoc == null) {
+      return null;
+    }
+    return perDoc.docValues(field);
   }
 
   /**


diff -ruN -x .svn -x build lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/SlowMultiReaderWrapper.java lucene-3622/lucene/src/java/org/apache/lucene/index/SlowMultiReaderWrapper.java
--- lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/SlowMultiReaderWrapper.java	2011-12-09 08:23:19.932675178 -0500
+++ lucene-3622/lucene/src/java/org/apache/lucene/index/SlowMultiReaderWrapper.java	2011-12-10 12:38:56.218446859 -0500
@@ -26,7 +26,6 @@
 
 import org.apache.lucene.index.DirectoryReader; // javadoc
 import org.apache.lucene.index.MultiReader; // javadoc
-import org.apache.lucene.index.codecs.PerDocValues;
 
 /**
  * This class forces a composite reader (eg a {@link
@@ -35,11 +34,12 @@
  * IndexReader#getSequentialSubReaders}) to emulate an
  * atomic reader.  This requires implementing the postings
  * APIs on-the-fly, using the static methods in {@link
- * MultiFields}, by stepping through the sub-readers to
- * merge fields/terms, appending docs, etc.
+ * MultiFields}, {@link MultiNorms}, {@link MultiDocValues}, 
+ * by stepping through the sub-readers to merge fields/terms, 
+ * appending docs, etc.
  *
  * <p>If you ever hit an UnsupportedOperationException saying
- * "please use MultiFields.XXX instead", the simple
+ * "please use MultiXXX.YYY instead", the simple
  * but non-performant workaround is to wrap your reader
  * using this class.</p>
  *
@@ -72,9 +72,9 @@
   }
 
   @Override
-  public PerDocValues perDocValues() throws IOException {
+  public DocValues docValues(String field) throws IOException {
     ensureOpen();
-    return MultiPerDocValues.getPerDocs(in);
+    return MultiDocValues.getDocValues(in, field);
   }
 
   @Override


diff -ruN -x .svn -x build lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/SortedBytesMergeUtils.java lucene-3622/lucene/src/java/org/apache/lucene/index/SortedBytesMergeUtils.java
--- lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/SortedBytesMergeUtils.java	1969-12-31 19:00:00.000000000 -0500
+++ lucene-3622/lucene/src/java/org/apache/lucene/index/SortedBytesMergeUtils.java	2011-12-10 14:37:05.334570312 -0500
@@ -0,0 +1,342 @@
+package org.apache.lucene.index;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Collections;
+import java.util.Comparator;
+import java.util.List;
+
+import org.apache.lucene.index.DocValues.SortedSource;
+import org.apache.lucene.index.DocValues.Source;
+import org.apache.lucene.index.DocValues.Type;
+import org.apache.lucene.index.MergeState.IndexReaderAndLiveDocs;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.util.ArrayUtil;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.PriorityQueue;
+import org.apache.lucene.util.packed.PackedInts;
+
+/**
+ * @lucene.internal
+ */
+// TODO: generalize this a bit more:
+//       * remove writing (like indexoutput) from here
+//       * just take IndexReaders (not IR&LiveDocs), doesnt care about liveDocs
+//       * hook into MultiDocValues to make a MultiSortedSource
+//       * maybe DV merging should then just use MultiDocValues for simplicity?
+public final class SortedBytesMergeUtils {
+
+  private SortedBytesMergeUtils() {
+    // no instance
+  }
+
+  public static MergeContext init(Type type, DocValues[] docValues,
+      Comparator<BytesRef> comp, MergeState mergeState) {
+    int size = -1;
+    if (type == Type.BYTES_FIXED_SORTED) {
+      for (DocValues indexDocValues : docValues) {
+        if (indexDocValues != null) {
+          size = indexDocValues.getValueSize();
+          break;
+        }
+      }
+      assert size >= 0;
+    }
+    return new MergeContext(comp, mergeState, size, type);
+  }
+
+  public static final class MergeContext {
+    private final Comparator<BytesRef> comp;
+    private final BytesRef missingValue = new BytesRef();
+    public final int sizePerValues; // -1 if var length
+    final Type type;
+    public final int[] docToEntry;
+    public long[] offsets; // if non-null #mergeRecords collects byte offsets here
+
+    public MergeContext(Comparator<BytesRef> comp, MergeState mergeState,
+        int size, Type type) {
+      assert type == Type.BYTES_FIXED_SORTED || type == Type.BYTES_VAR_SORTED;
+      this.comp = comp;
+      this.sizePerValues = size;
+      this.type = type;
+      if (size > 0) {
+        missingValue.grow(size);
+        missingValue.length = size;
+      }
+      docToEntry = new int[mergeState.mergedDocCount];
+    }
+  }
+
+  public static List<SortedSourceSlice> buildSlices(MergeState mergeState,
+      DocValues[] docValues, MergeContext ctx) throws IOException {
+    final List<SortedSourceSlice> slices = new ArrayList<SortedSourceSlice>();
+    for (int i = 0; i < docValues.length; i++) {
+      final SortedSourceSlice nextSlice;
+      final Source directSource;
+      if (docValues[i] != null
+          && (directSource = docValues[i].getDirectSource()) != null) {
+        final SortedSourceSlice slice = new SortedSourceSlice(i, directSource
+            .asSortedSource(), mergeState, ctx.docToEntry);
+        nextSlice = slice;
+      } else {
+        nextSlice = new SortedSourceSlice(i, new MissingValueSource(ctx),
+            mergeState, ctx.docToEntry);
+      }
+      createOrdMapping(mergeState, nextSlice);
+      slices.add(nextSlice);
+    }
+    return Collections.unmodifiableList(slices);
+  }
+
+  /*
+   * In order to merge we need to map the ords used in each segment to the new
+   * global ords in the new segment. Additionally we need to drop values that
+   * are not referenced anymore due to deleted documents. This method walks all
+   * live documents and fetches their current ordinal. We store this ordinal per
+   * slice and (SortedSourceSlice#ordMapping) and remember the doc to ord
+   * mapping in docIDToRelativeOrd. After the merge SortedSourceSlice#ordMapping
+   * contains the new global ordinals for the relative index.
+   */
+  private static void createOrdMapping(MergeState mergeState,
+      SortedSourceSlice currentSlice) {
+    final int readerIdx = currentSlice.readerIdx;
+    final int[] currentDocMap = mergeState.docMaps[readerIdx];
+    final int docBase = currentSlice.docToOrdStart;
+    assert docBase == mergeState.docBase[readerIdx];
+    if (currentDocMap != null) { // we have deletes
+      for (int i = 0; i < currentDocMap.length; i++) {
+        final int doc = currentDocMap[i];
+        if (doc != -1) { // not deleted
+          final int ord = currentSlice.source.ord(i); // collect ords strictly
+                                                      // increasing
+          currentSlice.docIDToRelativeOrd[docBase + doc] = ord;
+          // use ord + 1 to identify unreferenced values (ie. == 0)
+          currentSlice.ordMapping[ord] = ord + 1;
+        }
+      }
+    } else { // no deletes
+      final IndexReaderAndLiveDocs indexReaderAndLiveDocs = mergeState.readers
+          .get(readerIdx);
+      final int numDocs = indexReaderAndLiveDocs.reader.numDocs();
+      assert indexReaderAndLiveDocs.liveDocs == null;
+      assert currentSlice.docToOrdEnd - currentSlice.docToOrdStart == numDocs;
+      for (int doc = 0; doc < numDocs; doc++) {
+        final int ord = currentSlice.source.ord(doc);
+        currentSlice.docIDToRelativeOrd[docBase + doc] = ord;
+        // use ord + 1 to identify unreferenced values (ie. == 0)
+        currentSlice.ordMapping[ord] = ord + 1;
+      }
+    }
+  }
+
+  public static int mergeRecords(MergeContext ctx, IndexOutput datOut,
+      List<SortedSourceSlice> slices) throws IOException {
+    final RecordMerger merger = new RecordMerger(new MergeQueue(slices.size(),
+        ctx.comp), slices.toArray(new SortedSourceSlice[0]));
+    long[] offsets = ctx.offsets;
+    final boolean recordOffsets = offsets != null;
+    long offset = 0;
+    BytesRef currentMergedBytes;
+    merger.pushTop();
+    while (merger.queue.size() > 0) {
+      merger.pullTop();
+      currentMergedBytes = merger.current;
+      assert ctx.sizePerValues == -1 || ctx.sizePerValues == currentMergedBytes.length : "size: "
+          + ctx.sizePerValues + " spare: " + currentMergedBytes.length;
+
+      if (recordOffsets) {
+        offset += currentMergedBytes.length;
+        if (merger.currentOrd >= offsets.length) {
+          offsets = ArrayUtil.grow(offsets, merger.currentOrd + 1);
+        }
+        offsets[merger.currentOrd] = offset;
+      }
+      datOut.writeBytes(currentMergedBytes.bytes, currentMergedBytes.offset,
+          currentMergedBytes.length);
+      merger.pushTop();
+    }
+    ctx.offsets = offsets;
+    assert offsets == null || offsets[merger.currentOrd - 1] == offset;
+    return merger.currentOrd;
+  }
+
+  private static final class RecordMerger {
+    private final MergeQueue queue;
+    private final SortedSourceSlice[] top;
+    private int numTop;
+    BytesRef current;
+    int currentOrd = -1;
+
+    RecordMerger(MergeQueue queue, SortedSourceSlice[] top) {
+      super();
+      this.queue = queue;
+      this.top = top;
+      this.numTop = top.length;
+    }
+
+    private void pullTop() {
+      // extract all subs from the queue that have the same
+      // top record
+      assert numTop == 0;
+      assert currentOrd >= 0;
+      while (true) {
+        final SortedSourceSlice popped = top[numTop++] = queue.pop();
+        // use ord + 1 to identify unreferenced values (ie. == 0)
+        popped.ordMapping[popped.relativeOrd] = currentOrd + 1;
+        if (queue.size() == 0
+            || !(queue.top()).current.bytesEquals(top[0].current)) {
+          break;
+        }
+      }
+      current = top[0].current;
+    }
+
+    private void pushTop() throws IOException {
+      // call next() on each top, and put back into queue
+      for (int i = 0; i < numTop; i++) {
+        top[i].current = top[i].next();
+        if (top[i].current != null) {
+          queue.add(top[i]);
+        }
+      }
+      currentOrd++;
+      numTop = 0;
+    }
+  }
+
+  public static class SortedSourceSlice {
+    final SortedSource source;
+    final int readerIdx;
+    /* global array indexed by docID containg the relative ord for the doc */
+    final int[] docIDToRelativeOrd;
+    /*
+     * maps relative ords to merged global ords - index is relative ord value
+     * new global ord this map gets updates as we merge ords. later we use the
+     * docIDtoRelativeOrd to get the previous relative ord to get the new ord
+     * from the relative ord map.
+     */
+    final int[] ordMapping;
+
+    /* start index into docIDToRelativeOrd */
+    final int docToOrdStart;
+    /* end index into docIDToRelativeOrd */
+    final int docToOrdEnd;
+    BytesRef current = new BytesRef();
+    /* the currently merged relative ordinal */
+    int relativeOrd = -1;
+
+    SortedSourceSlice(int readerIdx, SortedSource source, MergeState state,
+        int[] docToOrd) {
+      super();
+      this.readerIdx = readerIdx;
+      this.source = source;
+      this.docIDToRelativeOrd = docToOrd;
+      this.ordMapping = new int[source.getValueCount()];
+      this.docToOrdStart = state.docBase[readerIdx];
+      this.docToOrdEnd = this.docToOrdStart + numDocs(state, readerIdx);
+    }
+
+    private static int numDocs(MergeState state, int readerIndex) {
+      if (readerIndex == state.docBase.length - 1) {
+        return state.mergedDocCount - state.docBase[readerIndex];
+      }
+      return state.docBase[readerIndex + 1] - state.docBase[readerIndex];
+    }
+
+    BytesRef next() {
+      for (int i = relativeOrd + 1; i < ordMapping.length; i++) {
+        if (ordMapping[i] != 0) { // skip ords that are not referenced anymore
+          source.getByOrd(i, current);
+          relativeOrd = i;
+          return current;
+        }
+      }
+      return null;
+    }
+
+    public void writeOrds(PackedInts.Writer writer) throws IOException {
+      for (int i = docToOrdStart; i < docToOrdEnd; i++) {
+        final int mappedOrd = docIDToRelativeOrd[i];
+        assert mappedOrd < ordMapping.length;
+        assert ordMapping[mappedOrd] > 0 : "illegal mapping ord maps to an unreferenced value";
+        writer.add(ordMapping[mappedOrd] - 1);
+      }
+    }
+  }
+
+  /*
+   * if a segment has no values at all we use this source to fill in the missing
+   * value in the right place (depending on the comparator used)
+   */
+  private static final class MissingValueSource extends SortedSource {
+
+    private BytesRef missingValue;
+
+    public MissingValueSource(MergeContext ctx) {
+      super(ctx.type, ctx.comp);
+      this.missingValue = ctx.missingValue;
+    }
+
+    @Override
+    public int ord(int docID) {
+      return 0;
+    }
+
+    @Override
+    public BytesRef getByOrd(int ord, BytesRef bytesRef) {
+      bytesRef.copyBytes(missingValue);
+      return bytesRef;
+    }
+
+    @Override
+    public PackedInts.Reader getDocToOrd() {
+      return null;
+    }
+
+    @Override
+    public int getValueCount() {
+      return 1;
+    }
+
+  }
+
+  /*
+   * merge queue
+   */
+  private static final class MergeQueue extends
+      PriorityQueue<SortedSourceSlice> {
+    final Comparator<BytesRef> comp;
+
+    public MergeQueue(int maxSize, Comparator<BytesRef> comp) {
+      super(maxSize);
+      this.comp = comp;
+    }
+
+    @Override
+    protected boolean lessThan(SortedSourceSlice a, SortedSourceSlice b) {
+      int cmp = comp.compare(a.current, b.current);
+      if (cmp != 0) {
+        return cmp < 0;
+      } else { // just a tie-breaker
+        return a.docToOrdStart < b.docToOrdStart;
+      }
+    }
+
+  }
+}


diff -ruN -x .svn -x build lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/TermVectorsConsumerPerField.java lucene-3622/lucene/src/java/org/apache/lucene/index/TermVectorsConsumerPerField.java
--- lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/TermVectorsConsumerPerField.java	2011-12-06 18:45:03.852810979 -0500
+++ lucene-3622/lucene/src/java/org/apache/lucene/index/TermVectorsConsumerPerField.java	2011-12-09 08:23:09.856675002 -0500
@@ -113,7 +113,6 @@
     // of a given field in the doc.  At this point we flush
     // our hash into the DocWriter.
 
-    assert fieldInfo.storeTermVector;
     assert termsWriter.vectorFieldsInOrder(fieldInfo);
 
     TermVectorsPostingsArray postings = (TermVectorsPostingsArray) termsHashPerField.postingsArray;
@@ -150,8 +149,9 @@
     }
 
     termsHashPerField.reset();
+
     // commit the termVectors once successful success - FI will otherwise reset them
-    fieldInfo.commitVectors();
+    fieldInfo.setStoreTermVectors(doVectorPositions, doVectorOffsets);
   }
 
   void shrinkHash() {


diff -ruN -x .svn -x build lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/TypePromoter.java lucene-3622/lucene/src/java/org/apache/lucene/index/TypePromoter.java
--- lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/TypePromoter.java	1969-12-31 19:00:00.000000000 -0500
+++ lucene-3622/lucene/src/java/org/apache/lucene/index/TypePromoter.java	2011-12-09 11:10:18.868849652 -0500
@@ -0,0 +1,206 @@
+package org.apache.lucene.index;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+import java.util.HashMap;
+import java.util.Map;
+
+import org.apache.lucene.index.DocValues.Type;
+
+/**
+ * Type promoter that promotes {@link DocValues} during merge based on
+ * their {@link Type} and {@link #getValueSize()}
+ * 
+ * @lucene.internal
+ */
+class TypePromoter {
+
+  private final static Map<Integer, Type> FLAGS_MAP = new HashMap<Integer, Type>();
+  private static final TypePromoter IDENTITY_PROMOTER = new IdentityTypePromoter();
+  public static final int VAR_TYPE_VALUE_SIZE = -1;
+
+  private static final int IS_INT = 1 << 0;
+  private static final int IS_BYTE = 1 << 1;
+  private static final int IS_FLOAT = 1 << 2;
+  /* VAR & FIXED == VAR */
+  private static final int IS_VAR = 1 << 3;
+  private static final int IS_FIXED = 1 << 3 | 1 << 4;
+  /* if we have FIXED & FIXED with different size we promote to VAR */
+  private static final int PROMOTE_TO_VAR_SIZE_MASK = ~(1 << 3);
+  /* STRAIGHT & DEREF == STRAIGHT (dense values win) */
+  private static final int IS_STRAIGHT = 1 << 5;
+  private static final int IS_DEREF = 1 << 5 | 1 << 6;
+  private static final int IS_SORTED = 1 << 7;
+  /* more bits wins (int16 & int32 == int32) */
+  private static final int IS_8_BIT = 1 << 8 | 1 << 9 | 1 << 10 | 1 << 11;
+  private static final int IS_16_BIT = 1 << 9 | 1 << 10 | 1 << 11;
+  private static final int IS_32_BIT = 1 << 10 | 1 << 11;
+  private static final int IS_64_BIT = 1 << 11;
+
+  private final Type type;
+  private final int flags;
+  private final int valueSize;
+
+  /**
+   * Returns a positive value size if this {@link TypePromoter} represents a
+   * fixed variant, otherwise <code>-1</code>
+   * 
+   * @return a positive value size if this {@link TypePromoter} represents a
+   *         fixed variant, otherwise <code>-1</code>
+   */
+  public int getValueSize() {
+    return valueSize;
+  }
+
+  static {
+    for (Type type : Type.values()) {
+      TypePromoter create = create(type, VAR_TYPE_VALUE_SIZE);
+      FLAGS_MAP.put(create.flags, type);
+    }
+  }
+
+  /**
+   * Creates a new {@link TypePromoter}
+   * 
+   * @param type
+   *          the {@link Type} this promoter represents
+   * @param flags
+   *          the promoters flags
+   * @param valueSize
+   *          the value size if {@link #IS_FIXED} or <code>-1</code> otherwise.
+   */
+  protected TypePromoter(Type type, int flags, int valueSize) {
+    this.type = type;
+    this.flags = flags;
+    this.valueSize = valueSize;
+  }
+
+  /**
+   * Creates a new promoted {@link TypePromoter} based on this and the given
+   * {@link TypePromoter} or <code>null</code> iff the {@link TypePromoter} 
+   * aren't compatible.
+   * 
+   * @param promoter
+   *          the incoming promoter
+   * @return a new promoted {@link TypePromoter} based on this and the given
+   *         {@link TypePromoter} or <code>null</code> iff the
+   *         {@link TypePromoter} aren't compatible.
+   */
+  public TypePromoter promote(TypePromoter promoter) {
+
+    int promotedFlags = promoter.flags & this.flags;
+    TypePromoter promoted = create(FLAGS_MAP.get(promotedFlags), valueSize);
+    if (promoted == null) {
+      return promoted;
+    }
+    if ((promoted.flags & IS_BYTE) != 0 && (promoted.flags & IS_FIXED) == IS_FIXED) {
+      if (this.valueSize == promoter.valueSize) {
+        return promoted;
+      }
+      return create(FLAGS_MAP.get(promoted.flags & PROMOTE_TO_VAR_SIZE_MASK),
+          VAR_TYPE_VALUE_SIZE);
+    }
+    return promoted;
+
+  }
+
+  /**
+   * Returns the {@link Type} of this {@link TypePromoter}
+   * 
+   * @return the {@link Type} of this {@link TypePromoter}
+   */
+  public Type type() {
+    return type;
+  }
+
+  @Override
+  public String toString() {
+    return "TypePromoter [type=" + type + ", sizeInBytes=" + valueSize + "]";
+  }
+
+  /**
+   * Creates a new {@link TypePromoter} for the given type and size per value.
+   * 
+   * @param type
+   *          the {@link Type} to create the promoter for
+   * @param valueSize
+   *          the size per value in bytes or <code>-1</code> iff the types have
+   *          variable length.
+   * @return a new {@link TypePromoter}
+   */
+  public static TypePromoter create(Type type, int valueSize) {
+    if (type == null) {
+      return null;
+    }
+    switch (type) {
+    case BYTES_FIXED_DEREF:
+      return new TypePromoter(type, IS_BYTE | IS_FIXED | IS_DEREF, valueSize);
+    case BYTES_FIXED_SORTED:
+      return new TypePromoter(type, IS_BYTE | IS_FIXED | IS_SORTED, valueSize);
+    case BYTES_FIXED_STRAIGHT:
+      return new TypePromoter(type, IS_BYTE | IS_FIXED | IS_STRAIGHT, valueSize);
+    case BYTES_VAR_DEREF:
+      return new TypePromoter(type, IS_BYTE | IS_VAR | IS_DEREF, VAR_TYPE_VALUE_SIZE);
+    case BYTES_VAR_SORTED:
+      return new TypePromoter(type, IS_BYTE | IS_VAR | IS_SORTED, VAR_TYPE_VALUE_SIZE);
+    case BYTES_VAR_STRAIGHT:
+      return new TypePromoter(type, IS_BYTE | IS_VAR | IS_STRAIGHT, VAR_TYPE_VALUE_SIZE);
+    case FIXED_INTS_16:
+      return new TypePromoter(type,
+          IS_INT | IS_FIXED | IS_STRAIGHT | IS_16_BIT, valueSize);
+    case FIXED_INTS_32:
+      return new TypePromoter(type,
+          IS_INT | IS_FIXED | IS_STRAIGHT | IS_32_BIT, valueSize);
+    case FIXED_INTS_64:
+      return new TypePromoter(type,
+          IS_INT | IS_FIXED | IS_STRAIGHT | IS_64_BIT, valueSize);
+    case FIXED_INTS_8:
+      return new TypePromoter(type, IS_INT | IS_FIXED | IS_STRAIGHT | IS_8_BIT,
+          valueSize);
+    case FLOAT_32:
+      return new TypePromoter(type, IS_FLOAT | IS_FIXED | IS_STRAIGHT
+          | IS_32_BIT, valueSize);
+    case FLOAT_64:
+      return new TypePromoter(type, IS_FLOAT | IS_FIXED | IS_STRAIGHT
+          | IS_64_BIT, valueSize);
+    case VAR_INTS:
+      return new TypePromoter(type, IS_INT | IS_VAR | IS_STRAIGHT, VAR_TYPE_VALUE_SIZE);
+    default:
+      throw new IllegalStateException();
+    }
+  }
+
+  /**
+   * Returns a {@link TypePromoter} that always promotes to the type provided to
+   * {@link #promote(TypePromoter)}
+   */
+  public static TypePromoter getIdentityPromoter() {
+    return IDENTITY_PROMOTER;
+  }
+
+  private static class IdentityTypePromoter extends TypePromoter {
+
+    public IdentityTypePromoter() {
+      super(null, 0, -1);
+    }
+
+    @Override
+    public TypePromoter promote(TypePromoter promoter) {
+      return promoter;
+    }
+  }
+}
\ No newline at end of file


diff -ruN -x .svn -x build lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/values/Bytes.java lucene-3622/lucene/src/java/org/apache/lucene/index/values/Bytes.java
--- lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/values/Bytes.java	2011-12-06 18:45:03.864810979 -0500
+++ lucene-3622/lucene/src/java/org/apache/lucene/index/values/Bytes.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,606 +0,0 @@
-package org.apache.lucene.index.values;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/** Base class for specific Bytes Reader/Writer implementations */
-import java.io.IOException;
-import java.util.Collection;
-import java.util.Comparator;
-import java.util.concurrent.atomic.AtomicLong;
-
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.values.IndexDocValues.SortedSource;
-import org.apache.lucene.index.values.IndexDocValues.Source;
-import org.apache.lucene.store.DataOutput;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.util.ArrayUtil;
-import org.apache.lucene.util.ByteBlockPool.Allocator;
-import org.apache.lucene.util.ByteBlockPool.DirectTrackingAllocator;
-import org.apache.lucene.util.ByteBlockPool;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.BytesRefHash.TrackingDirectBytesStartArray;
-import org.apache.lucene.util.BytesRefHash;
-import org.apache.lucene.util.CodecUtil;
-import org.apache.lucene.util.Counter;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.PagedBytes;
-import org.apache.lucene.util.RamUsageEstimator;
-import org.apache.lucene.util.packed.PackedInts;
-
-/**
- * Provides concrete Writer/Reader implementations for <tt>byte[]</tt> value per
- * document. There are 6 package-private default implementations of this, for
- * all combinations of {@link Mode#DEREF}/{@link Mode#STRAIGHT} x fixed-length/variable-length.
- * 
- * <p>
- * NOTE: Currently the total amount of byte[] data stored (across a single
- * segment) cannot exceed 2GB.
- * </p>
- * <p>
- * NOTE: Each byte[] must be <= 32768 bytes in length
- * </p>
- * 
- * @lucene.experimental
- */
-public final class Bytes {
-
-  static final String DV_SEGMENT_SUFFIX = "dv";
-
-  // TODO - add bulk copy where possible
-  private Bytes() { /* don't instantiate! */
-  }
-
-  /**
-   * Defines the {@link Writer}s store mode. The writer will either store the
-   * bytes sequentially ({@link #STRAIGHT}, dereferenced ({@link #DEREF}) or
-   * sorted ({@link #SORTED})
-   * 
-   * @lucene.experimental
-   */
-  public static enum Mode {
-    /**
-     * Mode for sequentially stored bytes
-     */
-    STRAIGHT,
-    /**
-     * Mode for dereferenced stored bytes
-     */
-    DEREF,
-    /**
-     * Mode for sorted stored bytes
-     */
-    SORTED
-  };
-
-  /**
-   * Creates a new <tt>byte[]</tt> {@link Writer} instances for the given
-   * directory.
-   * 
-   * @param dir
-   *          the directory to write the values to
-   * @param id
-   *          the id used to create a unique file name. Usually composed out of
-   *          the segment name and a unique id per segment.
-   * @param mode
-   *          the writers store mode
-   * @param fixedSize
-   *          <code>true</code> if all bytes subsequently passed to the
-   *          {@link Writer} will have the same length
-   * @param sortComparator {@link BytesRef} comparator used by sorted variants. 
-   *        If <code>null</code> {@link BytesRef#getUTF8SortedAsUnicodeComparator()}
-   *        is used instead
-   * @param bytesUsed
-   *          an {@link AtomicLong} instance to track the used bytes within the
-   *          {@link Writer}. A call to {@link Writer#finish(int)} will release
-   *          all internally used resources and frees the memory tracking
-   *          reference.
-   * @param context 
-   * @return a new {@link Writer} instance
-   * @throws IOException
-   *           if the files for the writer can not be created.
-   */
-  public static Writer getWriter(Directory dir, String id, Mode mode,
-      boolean fixedSize, Comparator<BytesRef> sortComparator, Counter bytesUsed, IOContext context)
-      throws IOException {
-    // TODO -- i shouldn't have to specify fixed? can
-    // track itself & do the write thing at write time?
-    if (sortComparator == null) {
-      sortComparator = BytesRef.getUTF8SortedAsUnicodeComparator();
-    }
-
-    if (fixedSize) {
-      if (mode == Mode.STRAIGHT) {
-        return new FixedStraightBytesImpl.Writer(dir, id, bytesUsed, context);
-      } else if (mode == Mode.DEREF) {
-        return new FixedDerefBytesImpl.Writer(dir, id, bytesUsed, context);
-      } else if (mode == Mode.SORTED) {
-        return new FixedSortedBytesImpl.Writer(dir, id, sortComparator, bytesUsed, context);
-      }
-    } else {
-      if (mode == Mode.STRAIGHT) {
-        return new VarStraightBytesImpl.Writer(dir, id, bytesUsed, context);
-      } else if (mode == Mode.DEREF) {
-        return new VarDerefBytesImpl.Writer(dir, id, bytesUsed, context);
-      } else if (mode == Mode.SORTED) {
-        return new VarSortedBytesImpl.Writer(dir, id, sortComparator, bytesUsed, context);
-      }
-    }
-
-    throw new IllegalArgumentException("");
-  }
-
-  /**
-   * Creates a new {@link IndexDocValues} instance that provides either memory
-   * resident or iterative access to a per-document stored <tt>byte[]</tt>
-   * value. The returned {@link IndexDocValues} instance will be initialized without
-   * consuming a significant amount of memory.
-   * 
-   * @param dir
-   *          the directory to load the {@link IndexDocValues} from.
-   * @param id
-   *          the file ID in the {@link Directory} to load the values from.
-   * @param mode
-   *          the mode used to store the values
-   * @param fixedSize
-   *          <code>true</code> iff the values are stored with fixed-size,
-   *          otherwise <code>false</code>
-   * @param maxDoc
-   *          the number of document values stored for the given ID
-   * @param sortComparator {@link BytesRef} comparator used by sorted variants. 
-   *        If <code>null</code> {@link BytesRef#getUTF8SortedAsUnicodeComparator()}
-   *        is used instead
-   * @return an initialized {@link IndexDocValues} instance.
-   * @throws IOException
-   *           if an {@link IOException} occurs
-   */
-  public static IndexDocValues getValues(Directory dir, String id, Mode mode,
-      boolean fixedSize, int maxDoc, Comparator<BytesRef> sortComparator, IOContext context) throws IOException {
-    if (sortComparator == null) {
-      sortComparator = BytesRef.getUTF8SortedAsUnicodeComparator();
-    }
-    // TODO -- I can peek @ header to determing fixed/mode?
-    if (fixedSize) {
-      if (mode == Mode.STRAIGHT) {
-        return new FixedStraightBytesImpl.FixedStraightReader(dir, id, maxDoc, context);
-      } else if (mode == Mode.DEREF) {
-        return new FixedDerefBytesImpl.FixedDerefReader(dir, id, maxDoc, context);
-      } else if (mode == Mode.SORTED) {
-        return new FixedSortedBytesImpl.Reader(dir, id, maxDoc, context, ValueType.BYTES_FIXED_SORTED, sortComparator);
-      }
-    } else {
-      if (mode == Mode.STRAIGHT) {
-        return new VarStraightBytesImpl.VarStraightReader(dir, id, maxDoc, context);
-      } else if (mode == Mode.DEREF) {
-        return new VarDerefBytesImpl.VarDerefReader(dir, id, maxDoc, context);
-      } else if (mode == Mode.SORTED) {
-        return new VarSortedBytesImpl.Reader(dir, id, maxDoc,context, ValueType.BYTES_VAR_SORTED, sortComparator);
-      }
-    }
-
-    throw new IllegalArgumentException("Illegal Mode: " + mode);
-  }
-
-  // TODO open up this API?
-  static abstract class BytesSourceBase extends Source {
-    private final PagedBytes pagedBytes;
-    protected final IndexInput datIn;
-    protected final IndexInput idxIn;
-    protected final static int PAGED_BYTES_BITS = 15;
-    protected final PagedBytes.Reader data;
-    protected final long totalLengthInBytes;
-    
-
-    protected BytesSourceBase(IndexInput datIn, IndexInput idxIn,
-        PagedBytes pagedBytes, long bytesToRead, ValueType type) throws IOException {
-      super(type);
-      assert bytesToRead <= datIn.length() : " file size is less than the expected size diff: "
-          + (bytesToRead - datIn.length()) + " pos: " + datIn.getFilePointer();
-      this.datIn = datIn;
-      this.totalLengthInBytes = bytesToRead;
-      this.pagedBytes = pagedBytes;
-      this.pagedBytes.copy(datIn, bytesToRead);
-      data = pagedBytes.freeze(true);
-      this.idxIn = idxIn;
-    }
-  }
-  
-  // TODO: open up this API?!
-  static abstract class BytesWriterBase extends Writer {
-    private final String id;
-    private IndexOutput idxOut;
-    private IndexOutput datOut;
-    protected BytesRef bytesRef = new BytesRef();
-    private final Directory dir;
-    private final String codecName;
-    private final int version;
-    private final IOContext context;
-
-    protected BytesWriterBase(Directory dir, String id, String codecName,
-        int version, Counter bytesUsed, IOContext context) throws IOException {
-      super(bytesUsed);
-      this.id = id;
-      this.dir = dir;
-      this.codecName = codecName;
-      this.version = version;
-      this.context = context;
-    }
-    
-    protected IndexOutput getOrCreateDataOut() throws IOException {
-      if (datOut == null) {
-        boolean success = false;
-        try {
-          datOut = dir.createOutput(IndexFileNames.segmentFileName(id, DV_SEGMENT_SUFFIX,
-              DATA_EXTENSION), context);
-          CodecUtil.writeHeader(datOut, codecName, version);
-          success = true;
-        } finally {
-          if (!success) {
-            IOUtils.closeWhileHandlingException(datOut);
-          }
-        }
-      }
-      return datOut;
-    }
-    
-    protected IndexOutput getIndexOut() {
-      return idxOut;
-    }
-    
-    protected IndexOutput getDataOut() {
-      return datOut;
-    }
-
-    protected IndexOutput getOrCreateIndexOut() throws IOException {
-      boolean success = false;
-      try {
-        if (idxOut == null) {
-          idxOut = dir.createOutput(IndexFileNames.segmentFileName(id, DV_SEGMENT_SUFFIX,
-              INDEX_EXTENSION), context);
-          CodecUtil.writeHeader(idxOut, codecName, version);
-        }
-        success = true;
-      } finally {
-        if (!success) {
-          IOUtils.closeWhileHandlingException(idxOut);
-        }
-      }
-      return idxOut;
-    }
-    /**
-     * Must be called only with increasing docIDs. It's OK for some docIDs to be
-     * skipped; they will be filled with 0 bytes.
-     */
-    @Override
-    public abstract void add(int docID, BytesRef bytes) throws IOException;
-
-    @Override
-    public abstract void finish(int docCount) throws IOException;
-
-    @Override
-    protected void mergeDoc(int docID, int sourceDoc) throws IOException {
-      add(docID, currentMergeSource.getBytes(sourceDoc, bytesRef));
-    }
-
-    @Override
-    public void add(int docID, PerDocFieldValues docValues) throws IOException {
-      final BytesRef ref;
-      if ((ref = docValues.getBytes()) != null) {
-        add(docID, ref);
-      }
-    }
-
-    @Override
-    public void files(Collection<String> files) throws IOException {
-      assert datOut != null;
-      files.add(IndexFileNames.segmentFileName(id, DV_SEGMENT_SUFFIX, DATA_EXTENSION));
-      if (idxOut != null) { // called after flush - so this must be initialized
-        // if needed or present
-        final String idxFile = IndexFileNames.segmentFileName(id, DV_SEGMENT_SUFFIX,
-            INDEX_EXTENSION);
-        files.add(idxFile);
-      }
-    }
-  }
-
-  /**
-   * Opens all necessary files, but does not read any data in until you call
-   * {@link #load}.
-   */
-  static abstract class BytesReaderBase extends IndexDocValues {
-    protected final IndexInput idxIn;
-    protected final IndexInput datIn;
-    protected final int version;
-    protected final String id;
-    protected final ValueType type;
-
-    protected BytesReaderBase(Directory dir, String id, String codecName,
-        int maxVersion, boolean doIndex, IOContext context, ValueType type) throws IOException {
-      IndexInput dataIn = null;
-      IndexInput indexIn = null;
-      boolean success = false;
-      try {
-        dataIn = dir.openInput(IndexFileNames.segmentFileName(id, DV_SEGMENT_SUFFIX,
-                                                              Writer.DATA_EXTENSION), context);
-        version = CodecUtil.checkHeader(dataIn, codecName, maxVersion, maxVersion);
-        if (doIndex) {
-          indexIn = dir.openInput(IndexFileNames.segmentFileName(id, DV_SEGMENT_SUFFIX,
-                                                                 Writer.INDEX_EXTENSION), context);
-          final int version2 = CodecUtil.checkHeader(indexIn, codecName,
-                                                     maxVersion, maxVersion);
-          assert version == version2;
-        }
-        success = true;
-      } finally {
-        if (!success) {
-          IOUtils.closeWhileHandlingException(dataIn, indexIn);
-        }
-      }
-      datIn = dataIn;
-      idxIn = indexIn;
-      this.type = type;
-      this.id = id;
-    }
-
-    /**
-     * clones and returns the data {@link IndexInput}
-     */
-    protected final IndexInput cloneData() {
-      assert datIn != null;
-      return (IndexInput) datIn.clone();
-    }
-
-    /**
-     * clones and returns the indexing {@link IndexInput}
-     */
-    protected final IndexInput cloneIndex() {
-      assert idxIn != null;
-      return (IndexInput) idxIn.clone();
-    }
-
-    @Override
-    public void close() throws IOException {
-      try {
-        super.close();
-      } finally {
-         IOUtils.close(datIn, idxIn);
-      }
-    }
-
-    @Override
-    public ValueType type() {
-      return type;
-    }
-    
-  }
-  
-  static abstract class DerefBytesWriterBase extends BytesWriterBase {
-    protected int size = -1;
-    protected int lastDocId = -1;
-    protected int[] docToEntry;
-    protected final BytesRefHash hash;
-    protected long maxBytes = 0;
-    
-    protected DerefBytesWriterBase(Directory dir, String id, String codecName,
-        int codecVersion, Counter bytesUsed, IOContext context)
-        throws IOException {
-      this(dir, id, codecName, codecVersion, new DirectTrackingAllocator(
-          ByteBlockPool.BYTE_BLOCK_SIZE, bytesUsed), bytesUsed, context);
-    }
-
-    protected DerefBytesWriterBase(Directory dir, String id, String codecName, int codecVersion, Allocator allocator,
-        Counter bytesUsed, IOContext context) throws IOException {
-      super(dir, id, codecName, codecVersion, bytesUsed, context);
-      hash = new BytesRefHash(new ByteBlockPool(allocator),
-          BytesRefHash.DEFAULT_CAPACITY, new TrackingDirectBytesStartArray(
-              BytesRefHash.DEFAULT_CAPACITY, bytesUsed));
-      docToEntry = new int[1];
-      bytesUsed.addAndGet(RamUsageEstimator.NUM_BYTES_INT);
-    }
-    
-    protected static int writePrefixLength(DataOutput datOut, BytesRef bytes)
-        throws IOException {
-      if (bytes.length < 128) {
-        datOut.writeByte((byte) bytes.length);
-        return 1;
-      } else {
-        datOut.writeByte((byte) (0x80 | (bytes.length >> 8)));
-        datOut.writeByte((byte) (bytes.length & 0xff));
-        return 2;
-      }
-    }
-
-    @Override
-    public void add(int docID, BytesRef bytes) throws IOException {
-      if (bytes.length == 0) { // default value - skip it
-        return;
-      }
-      checkSize(bytes);
-      fillDefault(docID);
-      int ord = hash.add(bytes);
-      if (ord < 0) {
-        ord = (-ord) - 1;
-      } else {
-        maxBytes += bytes.length;
-      }
-      
-      
-      docToEntry[docID] = ord;
-      lastDocId = docID;
-    }
-    
-    protected void fillDefault(int docID) {
-      if (docID >= docToEntry.length) {
-        final int size = docToEntry.length;
-        docToEntry = ArrayUtil.grow(docToEntry, 1 + docID);
-        bytesUsed.addAndGet((docToEntry.length - size)
-            * RamUsageEstimator.NUM_BYTES_INT);
-      }
-      assert size >= 0;
-      BytesRef ref = new BytesRef(size);
-      ref.length = size;
-      int ord = hash.add(ref);
-      if (ord < 0) {
-        ord = (-ord) - 1;
-      }
-      for (int i = lastDocId+1; i < docID; i++) {
-        docToEntry[i] = ord;
-      }
-    }
-    
-    protected void checkSize(BytesRef bytes) {
-      if (size == -1) {
-        size = bytes.length;
-      } else if (bytes.length != size) {
-        throw new IllegalArgumentException("expected bytes size=" + size
-            + " but got " + bytes.length);
-      }
-    }
-    
-    // Important that we get docCount, in case there were
-    // some last docs that we didn't see
-    @Override
-    public void finish(int docCount) throws IOException {
-      boolean success = false;
-      try {
-        finishInternal(docCount);
-        success = true;
-      } finally {
-        releaseResources();
-        if (success) {
-          IOUtils.close(getIndexOut(), getDataOut());
-        } else {
-          IOUtils.closeWhileHandlingException(getIndexOut(), getDataOut());
-        }
-        
-      }
-    }
-    
-    protected abstract void finishInternal(int docCount) throws IOException;
-    
-    protected void releaseResources() {
-      hash.close();
-      bytesUsed.addAndGet((-docToEntry.length) * RamUsageEstimator.NUM_BYTES_INT);
-      docToEntry = null;
-    }
-    
-    protected void writeIndex(IndexOutput idxOut, int docCount,
-        long maxValue, int[] toEntry) throws IOException {
-      writeIndex(idxOut, docCount, maxValue, (int[])null, toEntry);
-    }
-    
-    protected void writeIndex(IndexOutput idxOut, int docCount,
-        long maxValue, int[] addresses, int[] toEntry) throws IOException {
-      final PackedInts.Writer w = PackedInts.getWriter(idxOut, docCount,
-          PackedInts.bitsRequired(maxValue));
-      final int limit = docCount > docToEntry.length ? docToEntry.length
-          : docCount;
-      assert toEntry.length >= limit -1;
-      if (addresses != null) {
-        for (int i = 0; i < limit; i++) {
-          assert addresses[toEntry[i]] >= 0;
-          w.add(addresses[toEntry[i]]);
-        }
-      } else {
-        for (int i = 0; i < limit; i++) {
-          assert toEntry[i] >= 0;
-          w.add(toEntry[i]);
-        }
-      }
-      for (int i = limit; i < docCount; i++) {
-        w.add(0);
-      }
-      w.finish();
-    }
-    
-    protected void writeIndex(IndexOutput idxOut, int docCount,
-        long maxValue, long[] addresses, int[] toEntry) throws IOException {
-      final PackedInts.Writer w = PackedInts.getWriter(idxOut, docCount,
-          PackedInts.bitsRequired(maxValue));
-      final int limit = docCount > docToEntry.length ? docToEntry.length
-          : docCount;
-      assert toEntry.length >= limit -1;
-      if (addresses != null) {
-        for (int i = 0; i < limit; i++) {
-          assert addresses[toEntry[i]] >= 0;
-          w.add(addresses[toEntry[i]]);
-        }
-      } else {
-        for (int i = 0; i < limit; i++) {
-          assert toEntry[i] >= 0;
-          w.add(toEntry[i]);
-        }
-      }
-      for (int i = limit; i < docCount; i++) {
-        w.add(0);
-      }
-      w.finish();
-    }
-    
-  }
-  
-  static abstract class BytesSortedSourceBase extends SortedSource {
-    private final PagedBytes pagedBytes;
-    
-    protected final PackedInts.Reader docToOrdIndex;
-    protected final PackedInts.Reader ordToOffsetIndex;
-
-    protected final IndexInput datIn;
-    protected final IndexInput idxIn;
-    protected final BytesRef defaultValue = new BytesRef();
-    protected final static int PAGED_BYTES_BITS = 15;
-    protected final PagedBytes.Reader data;
-
-    protected BytesSortedSourceBase(IndexInput datIn, IndexInput idxIn,
-        Comparator<BytesRef> comp, long bytesToRead, ValueType type, boolean hasOffsets) throws IOException {
-      this(datIn, idxIn, comp, new PagedBytes(PAGED_BYTES_BITS), bytesToRead, type, hasOffsets);
-    }
-    
-    protected BytesSortedSourceBase(IndexInput datIn, IndexInput idxIn,
-        Comparator<BytesRef> comp, PagedBytes pagedBytes, long bytesToRead, ValueType type, boolean hasOffsets)
-        throws IOException {
-      super(type, comp);
-      assert bytesToRead <= datIn.length() : " file size is less than the expected size diff: "
-          + (bytesToRead - datIn.length()) + " pos: " + datIn.getFilePointer();
-      this.datIn = datIn;
-      this.pagedBytes = pagedBytes;
-      this.pagedBytes.copy(datIn, bytesToRead);
-      data = pagedBytes.freeze(true);
-      this.idxIn = idxIn;
-      ordToOffsetIndex = hasOffsets ? PackedInts.getReader(idxIn) : null; 
-      docToOrdIndex = PackedInts.getReader(idxIn);
-    }
-
-    @Override
-    public PackedInts.Reader getDocToOrd() {
-      return docToOrdIndex;
-    }
-    
-    @Override
-    public int ord(int docID) {
-      assert docToOrdIndex.get(docID) < getValueCount();
-      return (int) docToOrdIndex.get(docID);
-    }
-
-    protected void closeIndexInput() throws IOException {
-      IOUtils.close(datIn, idxIn);
-    }
-  }
-}


diff -ruN -x .svn -x build lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/values/BytesRefUtils.java lucene-3622/lucene/src/java/org/apache/lucene/index/values/BytesRefUtils.java
--- lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/values/BytesRefUtils.java	2011-12-06 18:45:03.860810978 -0500
+++ lucene-3622/lucene/src/java/org/apache/lucene/index/values/BytesRefUtils.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,120 +0,0 @@
-package org.apache.lucene.index.values;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to You under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-
-import org.apache.lucene.util.BytesRef;
-
-/**
- * Package private BytesRefUtils - can move this into the o.a.l.utils package if
- * needed.
- * 
- * @lucene.internal
- */
-final class BytesRefUtils {
-
-  private BytesRefUtils() {
-  }
-
-  /**
-   * Copies the given long value and encodes it as 8 byte Big-Endian.
-   * <p>
-   * NOTE: this method resets the offset to 0, length to 8 and resizes the
-   * reference array if needed.
-   */
-  public static void copyLong(BytesRef ref, long value) {
-    if (ref.bytes.length < 8) {
-      ref.bytes = new byte[8];
-    }
-    copyInternal(ref, (int) (value >> 32), ref.offset = 0);
-    copyInternal(ref, (int) value, 4);
-    ref.length = 8;
-  }
-
-  /**
-   * Copies the given int value and encodes it as 4 byte Big-Endian.
-   * <p>
-   * NOTE: this method resets the offset to 0, length to 4 and resizes the
-   * reference array if needed.
-   */
-  public static void copyInt(BytesRef ref, int value) {
-    if (ref.bytes.length < 4) {
-      ref.bytes = new byte[4];
-    }
-    copyInternal(ref, value, ref.offset = 0);
-    ref.length = 4;
-  }
-
-  /**
-   * Copies the given short value and encodes it as a 2 byte Big-Endian.
-   * <p>
-   * NOTE: this method resets the offset to 0, length to 2 and resizes the
-   * reference array if needed.
-   */
-  public static void copyShort(BytesRef ref, short value) {
-    if (ref.bytes.length < 2) {
-      ref.bytes = new byte[2];
-    }
-    ref.bytes[ref.offset] = (byte) (value >> 8);
-    ref.bytes[ref.offset + 1] = (byte) (value);
-    ref.length = 2;
-  }
-
-  private static void copyInternal(BytesRef ref, int value, int startOffset) {
-    ref.bytes[startOffset] = (byte) (value >> 24);
-    ref.bytes[startOffset + 1] = (byte) (value >> 16);
-    ref.bytes[startOffset + 2] = (byte) (value >> 8);
-    ref.bytes[startOffset + 3] = (byte) (value);
-  }
-
-  /**
-   * Converts 2 consecutive bytes from the current offset to a short. Bytes are
-   * interpreted as Big-Endian (most significant bit first)
-   * <p>
-   * NOTE: this method does <b>NOT</b> check the bounds of the referenced array.
-   */
-  public static short asShort(BytesRef b) {
-    return (short) (0xFFFF & ((b.bytes[b.offset] & 0xFF) << 8) | (b.bytes[b.offset + 1] & 0xFF));
-  }
-
-  /**
-   * Converts 4 consecutive bytes from the current offset to an int. Bytes are
-   * interpreted as Big-Endian (most significant bit first)
-   * <p>
-   * NOTE: this method does <b>NOT</b> check the bounds of the referenced array.
-   */
-  public static int asInt(BytesRef b) {
-    return asIntInternal(b, b.offset);
-  }
-
-  /**
-   * Converts 8 consecutive bytes from the current offset to a long. Bytes are
-   * interpreted as Big-Endian (most significant bit first)
-   * <p>
-   * NOTE: this method does <b>NOT</b> check the bounds of the referenced array.
-   */
-  public static long asLong(BytesRef b) {
-    return (((long) asIntInternal(b, b.offset) << 32) | asIntInternal(b,
-        b.offset + 4) & 0xFFFFFFFFL);
-  }
-
-  private static int asIntInternal(BytesRef b, int pos) {
-    return ((b.bytes[pos++] & 0xFF) << 24) | ((b.bytes[pos++] & 0xFF) << 16)
-        | ((b.bytes[pos++] & 0xFF) << 8) | (b.bytes[pos] & 0xFF);
-  }
-
-}


diff -ruN -x .svn -x build lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/values/DirectSource.java lucene-3622/lucene/src/java/org/apache/lucene/index/values/DirectSource.java
--- lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/values/DirectSource.java	2011-12-06 18:45:03.864810979 -0500
+++ lucene-3622/lucene/src/java/org/apache/lucene/index/values/DirectSource.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,137 +0,0 @@
-package org.apache.lucene.index.values;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.index.values.IndexDocValues.Source;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.util.BytesRef;
-
-/**
- * Base class for disk resident source implementations
- * @lucene.internal
- */
-abstract class DirectSource extends Source {
-
-  protected final IndexInput data;
-  private final ToNumeric toNumeric;
-  protected final long baseOffset;
-
-  DirectSource(IndexInput input, ValueType type) {
-    super(type);
-    this.data = input;
-    baseOffset = input.getFilePointer();
-    switch (type) {
-    case FIXED_INTS_16:
-      toNumeric = new ShortToLong();
-      break;
-    case FLOAT_32:
-    case FIXED_INTS_32:
-      toNumeric = new IntToLong();
-      break;
-    case FIXED_INTS_8:
-      toNumeric = new ByteToLong();
-      break;
-    default:
-      toNumeric = new LongToLong();
-    }
-  }
-
-  @Override
-  public BytesRef getBytes(int docID, BytesRef ref) {
-    try {
-      final int sizeToRead = position(docID);
-      ref.grow(sizeToRead);
-      data.readBytes(ref.bytes, 0, sizeToRead);
-      ref.length = sizeToRead;
-      ref.offset = 0;
-      return ref;
-    } catch (IOException ex) {
-      throw new IllegalStateException("failed to get value for docID: " + docID, ex);
-    }
-  }
-
-  @Override
-  public long getInt(int docID) {
-    try {
-      position(docID);
-      return toNumeric.toLong(data);
-    } catch (IOException ex) {
-      throw new IllegalStateException("failed to get value for docID: " + docID, ex);
-    }
-  }
-
-  @Override
-  public double getFloat(int docID) {
-    try {
-      position(docID);
-      return toNumeric.toDouble(data);
-    } catch (IOException ex) {
-      throw new IllegalStateException("failed to get value for docID: " + docID, ex);
-    }
-  }
-
-  protected abstract int position(int docID) throws IOException;
-
-  private abstract static class ToNumeric {
-    abstract long toLong(IndexInput input) throws IOException;
-
-    double toDouble(IndexInput input) throws IOException {
-      return toLong(input);
-    }
-  }
-
-  private static final class ByteToLong extends ToNumeric {
-    @Override
-    long toLong(IndexInput input) throws IOException {
-      return input.readByte();
-    }
-
-  }
-
-  private static final class ShortToLong extends ToNumeric {
-    @Override
-    long toLong(IndexInput input) throws IOException {
-      return input.readShort();
-    }
-  }
-
-  private static final class IntToLong extends ToNumeric {
-    @Override
-    long toLong(IndexInput input) throws IOException {
-      return input.readInt();
-    }
-
-    double toDouble(IndexInput input) throws IOException {
-      return Float.intBitsToFloat(input.readInt());
-    }
-  }
-
-  private static final class LongToLong extends ToNumeric {
-    @Override
-    long toLong(IndexInput input) throws IOException {
-      return input.readLong();
-    }
-
-    double toDouble(IndexInput input) throws IOException {
-      return Double.longBitsToDouble(input.readLong());
-    }
-  }
-
-}


diff -ruN -x .svn -x build lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/values/FixedDerefBytesImpl.java lucene-3622/lucene/src/java/org/apache/lucene/index/values/FixedDerefBytesImpl.java
--- lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/values/FixedDerefBytesImpl.java	2011-12-06 18:45:03.864810979 -0500
+++ lucene-3622/lucene/src/java/org/apache/lucene/index/values/FixedDerefBytesImpl.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,133 +0,0 @@
-package org.apache.lucene.index.values;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.index.values.Bytes.BytesReaderBase;
-import org.apache.lucene.index.values.Bytes.BytesSourceBase;
-import org.apache.lucene.index.values.Bytes.DerefBytesWriterBase;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.Counter;
-import org.apache.lucene.util.PagedBytes;
-import org.apache.lucene.util.packed.PackedInts;
-
-// Stores fixed-length byte[] by deref, ie when two docs
-// have the same value, they store only 1 byte[]
-/**
- * @lucene.experimental
- */
-class FixedDerefBytesImpl {
-
-  static final String CODEC_NAME = "FixedDerefBytes";
-  static final int VERSION_START = 0;
-  static final int VERSION_CURRENT = VERSION_START;
-
-  public static class Writer extends DerefBytesWriterBase {
-    public Writer(Directory dir, String id, Counter bytesUsed, IOContext context)
-        throws IOException {
-      super(dir, id, CODEC_NAME, VERSION_CURRENT, bytesUsed, context);
-    }
-
-    @Override
-    protected void finishInternal(int docCount) throws IOException {
-      final int numValues = hash.size();
-      final IndexOutput datOut = getOrCreateDataOut();
-      datOut.writeInt(size);
-      if (size != -1) {
-        final BytesRef bytesRef = new BytesRef(size);
-        for (int i = 0; i < numValues; i++) {
-          hash.get(i, bytesRef);
-          datOut.writeBytes(bytesRef.bytes, bytesRef.offset, bytesRef.length);
-        }
-      }
-      final IndexOutput idxOut = getOrCreateIndexOut();
-      idxOut.writeInt(numValues);
-      writeIndex(idxOut, docCount, numValues, docToEntry);
-    }
-  }
-
-  public static class FixedDerefReader extends BytesReaderBase {
-    private final int size;
-    private final int numValuesStored;
-    FixedDerefReader(Directory dir, String id, int maxDoc, IOContext context) throws IOException {
-      super(dir, id, CODEC_NAME, VERSION_START, true, context, ValueType.BYTES_FIXED_DEREF);
-      size = datIn.readInt();
-      numValuesStored = idxIn.readInt();
-    }
-
-    @Override
-    public Source load() throws IOException {
-      return new FixedDerefSource(cloneData(), cloneIndex(), size, numValuesStored);
-    }
-
-    @Override
-    public Source getDirectSource()
-        throws IOException {
-      return new DirectFixedDerefSource(cloneData(), cloneIndex(), size, type());
-    }
-
-    @Override
-    public int getValueSize() {
-      return size;
-    }
-    
-  }
-  
-  static final class FixedDerefSource extends BytesSourceBase {
-    private final int size;
-    private final PackedInts.Reader addresses;
-
-    protected FixedDerefSource(IndexInput datIn, IndexInput idxIn, int size, long numValues) throws IOException {
-      super(datIn, idxIn, new PagedBytes(PAGED_BYTES_BITS), size * numValues,
-          ValueType.BYTES_FIXED_DEREF);
-      this.size = size;
-      addresses = PackedInts.getReader(idxIn);
-    }
-
-    @Override
-    public BytesRef getBytes(int docID, BytesRef bytesRef) {
-      final int id = (int) addresses.get(docID);
-      return data.fillSlice(bytesRef, (id * size), size);
-    }
-
-  }
-  
-  final static class DirectFixedDerefSource extends DirectSource {
-    private final PackedInts.Reader index;
-    private final int size;
-
-    DirectFixedDerefSource(IndexInput data, IndexInput index, int size, ValueType type)
-        throws IOException {
-      super(data, type);
-      this.size = size;
-      this.index = PackedInts.getDirectReader(index);
-    }
-
-    @Override
-    protected int position(int docID) throws IOException {
-      data.seek(baseOffset + index.get(docID) * size);
-      return size;
-    }
-  }
-
-}


diff -ruN -x .svn -x build lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/values/FixedSortedBytesImpl.java lucene-3622/lucene/src/java/org/apache/lucene/index/values/FixedSortedBytesImpl.java
--- lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/values/FixedSortedBytesImpl.java	2011-12-06 18:45:03.860810978 -0500
+++ lucene-3622/lucene/src/java/org/apache/lucene/index/values/FixedSortedBytesImpl.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,222 +0,0 @@
-package org.apache.lucene.index.values;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Comparator;
-import java.util.List;
-
-import org.apache.lucene.index.MergeState;
-import org.apache.lucene.index.values.Bytes.BytesReaderBase;
-import org.apache.lucene.index.values.Bytes.BytesSortedSourceBase;
-import org.apache.lucene.index.values.Bytes.DerefBytesWriterBase;
-import org.apache.lucene.index.values.IndexDocValues.SortedSource;
-import org.apache.lucene.index.values.SortedBytesMergeUtils.MergeContext;
-import org.apache.lucene.index.values.SortedBytesMergeUtils.SortedSourceSlice;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.Counter;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.packed.PackedInts;
-
-// Stores fixed-length byte[] by deref, ie when two docs
-// have the same value, they store only 1 byte[]
-
-/**
- * @lucene.experimental
- */
-class FixedSortedBytesImpl {
-
-  static final String CODEC_NAME = "FixedSortedBytes";
-  static final int VERSION_START = 0;
-  static final int VERSION_CURRENT = VERSION_START;
-
-  static final class Writer extends DerefBytesWriterBase {
-    private final Comparator<BytesRef> comp;
-
-    public Writer(Directory dir, String id, Comparator<BytesRef> comp,
-        Counter bytesUsed, IOContext context) throws IOException {
-      super(dir, id, CODEC_NAME, VERSION_CURRENT, bytesUsed, context);
-      this.comp = comp;
-    }
-
-    @Override
-    public void merge(MergeState mergeState, IndexDocValues[] docValues)
-        throws IOException {
-      boolean success = false;
-      try {
-        final MergeContext ctx = SortedBytesMergeUtils.init(ValueType.BYTES_FIXED_SORTED, docValues, comp, mergeState);
-        List<SortedSourceSlice> slices = SortedBytesMergeUtils.buildSlices(mergeState, docValues, ctx);
-        final IndexOutput datOut = getOrCreateDataOut();
-        datOut.writeInt(ctx.sizePerValues);
-        final int maxOrd = SortedBytesMergeUtils.mergeRecords(ctx, datOut, slices);
-        
-        final IndexOutput idxOut = getOrCreateIndexOut();
-        idxOut.writeInt(maxOrd);
-        final PackedInts.Writer ordsWriter = PackedInts.getWriter(idxOut, ctx.docToEntry.length,
-            PackedInts.bitsRequired(maxOrd));
-        for (SortedSourceSlice slice : slices) {
-          slice.writeOrds(ordsWriter);
-        }
-        ordsWriter.finish();
-        success = true;
-      } finally {
-        releaseResources();
-        if (success) {
-          IOUtils.close(getIndexOut(), getDataOut());
-        } else {
-          IOUtils.closeWhileHandlingException(getIndexOut(), getDataOut());
-        }
-
-      }
-    }
-
-    // Important that we get docCount, in case there were
-    // some last docs that we didn't see
-    @Override
-    public void finishInternal(int docCount) throws IOException {
-      fillDefault(docCount);
-      final IndexOutput datOut = getOrCreateDataOut();
-      final int count = hash.size();
-      final int[] address = new int[count];
-      datOut.writeInt(size);
-      if (size != -1) {
-        final int[] sortedEntries = hash.sort(comp);
-        // first dump bytes data, recording address as we go
-        final BytesRef spare = new BytesRef(size);
-        for (int i = 0; i < count; i++) {
-          final int e = sortedEntries[i];
-          final BytesRef bytes = hash.get(e, spare);
-          assert bytes.length == size;
-          datOut.writeBytes(bytes.bytes, bytes.offset, bytes.length);
-          address[e] = i;
-        }
-      }
-      final IndexOutput idxOut = getOrCreateIndexOut();
-      idxOut.writeInt(count);
-      writeIndex(idxOut, docCount, count, address, docToEntry);
-    }
-  }
-
-  static final class Reader extends BytesReaderBase {
-    private final int size;
-    private final int valueCount;
-    private final Comparator<BytesRef> comparator;
-
-    public Reader(Directory dir, String id, int maxDoc, IOContext context,
-        ValueType type, Comparator<BytesRef> comparator) throws IOException {
-      super(dir, id, CODEC_NAME, VERSION_START, true, context, type);
-      size = datIn.readInt();
-      valueCount = idxIn.readInt();
-      this.comparator = comparator;
-    }
-
-    @Override
-    public Source load() throws IOException {
-      return new FixedSortedSource(cloneData(), cloneIndex(), size, valueCount,
-          comparator);
-    }
-
-    @Override
-    public Source getDirectSource() throws IOException {
-      return new DirectFixedSortedSource(cloneData(), cloneIndex(), size,
-          valueCount, comparator, type);
-    }
-
-    @Override
-    public int getValueSize() {
-      return size;
-    }
-  }
-
-  static final class FixedSortedSource extends BytesSortedSourceBase {
-    private final int valueCount;
-    private final int size;
-
-    FixedSortedSource(IndexInput datIn, IndexInput idxIn, int size,
-        int numValues, Comparator<BytesRef> comp) throws IOException {
-      super(datIn, idxIn, comp, size * numValues, ValueType.BYTES_FIXED_SORTED,
-          false);
-      this.size = size;
-      this.valueCount = numValues;
-      closeIndexInput();
-    }
-
-    @Override
-    public int getValueCount() {
-      return valueCount;
-    }
-
-    @Override
-    public BytesRef getByOrd(int ord, BytesRef bytesRef) {
-      return data.fillSlice(bytesRef, (ord * size), size);
-    }
-  }
-
-  static final class DirectFixedSortedSource extends SortedSource {
-    final PackedInts.Reader docToOrdIndex;
-    private final IndexInput datIn;
-    private final long basePointer;
-    private final int size;
-    private final int valueCount;
-
-    DirectFixedSortedSource(IndexInput datIn, IndexInput idxIn, int size,
-        int valueCount, Comparator<BytesRef> comp, ValueType type)
-        throws IOException {
-      super(type, comp);
-      docToOrdIndex = PackedInts.getDirectReader(idxIn);
-      basePointer = datIn.getFilePointer();
-      this.datIn = datIn;
-      this.size = size;
-      this.valueCount = valueCount;
-    }
-
-    @Override
-    public int ord(int docID) {
-      return (int) docToOrdIndex.get(docID);
-    }
-
-    @Override
-    public PackedInts.Reader getDocToOrd() {
-      return docToOrdIndex;
-    }
-
-    @Override
-    public BytesRef getByOrd(int ord, BytesRef bytesRef) {
-      try {
-        datIn.seek(basePointer + size * ord);
-        bytesRef.grow(size);
-        datIn.readBytes(bytesRef.bytes, 0, size);
-        bytesRef.length = size;
-        bytesRef.offset = 0;
-        return bytesRef;
-      } catch (IOException ex) {
-        throw new IllegalStateException("failed to getByOrd", ex);
-      }
-    }
-
-    @Override
-    public int getValueCount() {
-      return valueCount;
-    }
-  }
-
-}


diff -ruN -x .svn -x build lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/values/FixedStraightBytesImpl.java lucene-3622/lucene/src/java/org/apache/lucene/index/values/FixedStraightBytesImpl.java
--- lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/values/FixedStraightBytesImpl.java	2011-12-06 18:45:03.864810979 -0500
+++ lucene-3622/lucene/src/java/org/apache/lucene/index/values/FixedStraightBytesImpl.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,354 +0,0 @@
-package org.apache.lucene.index.values;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import static org.apache.lucene.util.ByteBlockPool.BYTE_BLOCK_SIZE;
-
-import java.io.IOException;
-
-import org.apache.lucene.index.values.Bytes.BytesSourceBase;
-import org.apache.lucene.index.values.Bytes.BytesReaderBase;
-import org.apache.lucene.index.values.Bytes.BytesWriterBase;
-import org.apache.lucene.index.values.DirectSource;
-import org.apache.lucene.index.values.IndexDocValues.Source;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.util.ByteBlockPool;
-import org.apache.lucene.util.ByteBlockPool.DirectTrackingAllocator;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.Counter;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.PagedBytes;
-
-// Simplest storage: stores fixed length byte[] per
-// document, with no dedup and no sorting.
-/**
- * @lucene.experimental
- */
-class FixedStraightBytesImpl {
-
-  static final String CODEC_NAME = "FixedStraightBytes";
-  static final int VERSION_START = 0;
-  static final int VERSION_CURRENT = VERSION_START;
-  
-  static abstract class FixedBytesWriterBase extends BytesWriterBase {
-    protected int lastDocID = -1;
-    // start at -1 if the first added value is > 0
-    protected int size = -1;
-    private final int byteBlockSize = BYTE_BLOCK_SIZE;
-    private final ByteBlockPool pool;
-
-    protected FixedBytesWriterBase(Directory dir, String id, String codecName,
-        int version, Counter bytesUsed, IOContext context) throws IOException {
-      super(dir, id, codecName, version, bytesUsed, context);
-      pool = new ByteBlockPool(new DirectTrackingAllocator(bytesUsed));
-      pool.nextBuffer();
-    }
-    
-    @Override
-    public void add(int docID, BytesRef bytes) throws IOException {
-      assert lastDocID < docID;
-
-      if (size == -1) {
-        if (bytes.length > BYTE_BLOCK_SIZE) {
-          throw new IllegalArgumentException("bytes arrays > " + Short.MAX_VALUE + " are not supported");
-        }
-        size = bytes.length;
-      } else if (bytes.length != size) {
-        throw new IllegalArgumentException("expected bytes size=" + size
-            + " but got " + bytes.length);
-      }
-      if (lastDocID+1 < docID) {
-        advancePool(docID);
-      }
-      pool.copy(bytes);
-      lastDocID = docID;
-    }
-    
-    private final void advancePool(int docID) {
-      long numBytes = (docID - (lastDocID+1))*size;
-      while(numBytes > 0) {
-        if (numBytes + pool.byteUpto < byteBlockSize) {
-          pool.byteUpto += numBytes;
-          numBytes = 0;
-        } else {
-          numBytes -= byteBlockSize - pool.byteUpto;
-          pool.nextBuffer();
-        }
-      }
-      assert numBytes == 0;
-    }
-    
-    protected void set(BytesRef ref, int docId) {
-      assert BYTE_BLOCK_SIZE % size == 0 : "BYTE_BLOCK_SIZE ("+ BYTE_BLOCK_SIZE + ") must be a multiple of the size: " + size;
-      ref.offset = docId*size;
-      ref.length = size;
-      pool.deref(ref);
-    }
-    
-    protected void resetPool() {
-      pool.dropBuffersAndReset();
-    }
-    
-    protected void writeData(IndexOutput out) throws IOException {
-      pool.writePool(out);
-    }
-    
-    protected void writeZeros(int num, IndexOutput out) throws IOException {
-      final byte[] zeros = new byte[size];
-      for (int i = 0; i < num; i++) {
-        out.writeBytes(zeros, zeros.length);
-      }
-    }
-  }
-
-  static class Writer extends FixedBytesWriterBase {
-    private boolean hasMerged;
-    private IndexOutput datOut;
-    
-    public Writer(Directory dir, String id, Counter bytesUsed, IOContext context) throws IOException {
-      super(dir, id, CODEC_NAME, VERSION_CURRENT, bytesUsed, context);
-    }
-
-    public Writer(Directory dir, String id, String codecName, int version, Counter bytesUsed, IOContext context) throws IOException {
-      super(dir, id, codecName, version, bytesUsed, context);
-    }
-
-
-    @Override
-    protected void merge(SingleSubMergeState state) throws IOException {
-      datOut = getOrCreateDataOut();
-      boolean success = false;
-      try {
-        if (!hasMerged && size != -1) {
-          datOut.writeInt(size);
-        }
-
-        if (state.liveDocs == null && tryBulkMerge(state.reader)) {
-          FixedStraightReader reader = (FixedStraightReader) state.reader;
-          final int maxDocs = reader.maxDoc;
-          if (maxDocs == 0) {
-            return;
-          }
-          if (size == -1) {
-            size = reader.size;
-            datOut.writeInt(size);
-          } else if (size != reader.size) {
-            throw new IllegalArgumentException("expected bytes size=" + size
-                + " but got " + reader.size);
-           }
-          if (lastDocID+1 < state.docBase) {
-            fill(datOut, state.docBase);
-            lastDocID = state.docBase-1;
-          }
-          // TODO should we add a transfer to API to each reader?
-          final IndexInput cloneData = reader.cloneData();
-          try {
-            datOut.copyBytes(cloneData, size * maxDocs);
-          } finally {
-            IOUtils.close(cloneData);  
-          }
-        
-          lastDocID += maxDocs;
-        } else {
-          super.merge(state);
-        }
-        success = true;
-      } finally {
-        if (!success) {
-          IOUtils.closeWhileHandlingException(datOut);
-        }
-        hasMerged = true;
-      }
-    }
-    
-    protected boolean tryBulkMerge(IndexDocValues docValues) {
-      return docValues instanceof FixedStraightReader;
-    }
-    
-    @Override
-    protected void mergeDoc(int docID, int sourceDoc) throws IOException {
-      assert lastDocID < docID;
-      setMergeBytes(sourceDoc);
-      if (size == -1) {
-        size = bytesRef.length;
-        datOut.writeInt(size);
-      }
-      assert size == bytesRef.length : "size: " + size + " ref: " + bytesRef.length;
-      if (lastDocID+1 < docID) {
-        fill(datOut, docID);
-      }
-      datOut.writeBytes(bytesRef.bytes, bytesRef.offset, bytesRef.length);
-      lastDocID = docID;
-    }
-    
-    protected void setMergeBytes(int sourceDoc) {
-      currentMergeSource.getBytes(sourceDoc, bytesRef);
-    }
-
-
-
-    // Fills up to but not including this docID
-    private void fill(IndexOutput datOut, int docID) throws IOException {
-      assert size >= 0;
-      writeZeros((docID - (lastDocID+1)), datOut);
-    }
-
-    @Override
-    public void finish(int docCount) throws IOException {
-      boolean success = false;
-      try {
-        if (!hasMerged) {
-          // indexing path - no disk IO until here
-          assert datOut == null;
-          datOut = getOrCreateDataOut();
-          if (size == -1) {
-            datOut.writeInt(0);
-          } else {
-            datOut.writeInt(size);
-            writeData(datOut);
-          }
-          if (lastDocID + 1 < docCount) {
-            fill(datOut, docCount);
-          }
-        } else {
-          // merge path - datOut should be initialized
-          assert datOut != null;
-          if (size == -1) {// no data added
-            datOut.writeInt(0);
-          } else {
-            fill(datOut, docCount);
-          }
-        }
-        success = true;
-      } finally {
-        resetPool();
-        if (success) {
-          IOUtils.close(datOut);
-        } else {
-          IOUtils.closeWhileHandlingException(datOut);
-        }
-      }
-    }
-  
-  }
-  
-  public static class FixedStraightReader extends BytesReaderBase {
-    protected final int size;
-    protected final int maxDoc;
-    
-    FixedStraightReader(Directory dir, String id, int maxDoc, IOContext context) throws IOException {
-      this(dir, id, CODEC_NAME, VERSION_CURRENT, maxDoc, context, ValueType.BYTES_FIXED_STRAIGHT);
-    }
-
-    protected FixedStraightReader(Directory dir, String id, String codec, int version, int maxDoc, IOContext context, ValueType type) throws IOException {
-      super(dir, id, codec, version, false, context, type);
-      size = datIn.readInt();
-      this.maxDoc = maxDoc;
-    }
-
-    @Override
-    public Source load() throws IOException {
-      return size == 1 ? new SingleByteSource(cloneData(), maxDoc) : 
-        new FixedStraightSource(cloneData(), size, maxDoc, type);
-    }
-
-    @Override
-    public void close() throws IOException {
-      datIn.close();
-    }
-   
-    @Override
-    public Source getDirectSource() throws IOException {
-      return new DirectFixedStraightSource(cloneData(), size, type());
-    }
-    
-    @Override
-    public int getValueSize() {
-      return size;
-    }
-  }
-  
-  // specialized version for single bytes
-  private static final class SingleByteSource extends Source {
-    private final byte[] data;
-
-    public SingleByteSource(IndexInput datIn, int maxDoc) throws IOException {
-      super(ValueType.BYTES_FIXED_STRAIGHT);
-      try {
-        data = new byte[maxDoc];
-        datIn.readBytes(data, 0, data.length, false);
-      } finally {
-        IOUtils.close(datIn);
-      }
-    }
-    
-    @Override
-    public boolean hasArray() {
-      return true;
-    }
-
-    @Override
-    public Object getArray() {
-      return data;
-    }
-
-    @Override
-    public BytesRef getBytes(int docID, BytesRef bytesRef) {
-      bytesRef.length = 1;
-      bytesRef.bytes = data;
-      bytesRef.offset = docID;
-      return bytesRef;
-    }
-  }
-
-  
-  private final static class FixedStraightSource extends BytesSourceBase {
-    private final int size;
-
-    public FixedStraightSource(IndexInput datIn, int size, int maxDoc, ValueType type)
-        throws IOException {
-      super(datIn, null, new PagedBytes(PAGED_BYTES_BITS), size * maxDoc,
-          type);
-      this.size = size;
-    }
-
-    @Override
-    public BytesRef getBytes(int docID, BytesRef bytesRef) {
-      return data.fillSlice(bytesRef, docID * size, size);
-    }
-  }
-  
-  public final static class DirectFixedStraightSource extends DirectSource {
-    private final int size;
-
-    DirectFixedStraightSource(IndexInput input, int size, ValueType type) {
-      super(input, type);
-      this.size = size;
-    }
-
-    @Override
-    protected int position(int docID) throws IOException {
-      data.seek(baseOffset + size * docID);
-      return size;
-    }
-
-  }
-}


diff -ruN -x .svn -x build lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/values/Floats.java lucene-3622/lucene/src/java/org/apache/lucene/index/values/Floats.java
--- lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/values/Floats.java	2011-12-06 18:45:03.864810979 -0500
+++ lucene-3622/lucene/src/java/org/apache/lucene/index/values/Floats.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,123 +0,0 @@
-package org.apache.lucene.index.values;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-import java.io.IOException;
-
-import org.apache.lucene.index.values.IndexDocValues.Source;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.Counter;
-import org.apache.lucene.util.IOUtils;
-
-/**
- * Exposes {@link Writer} and reader ({@link Source}) for 32 bit and 64 bit
- * floating point values.
- * <p>
- * Current implementations store either 4 byte or 8 byte floating points with
- * full precision without any compression.
- * 
- * @lucene.experimental
- */
-public class Floats {
-  
-  protected static final String CODEC_NAME = "Floats";
-  protected static final int VERSION_START = 0;
-  protected static final int VERSION_CURRENT = VERSION_START;
-  
-  public static Writer getWriter(Directory dir, String id, Counter bytesUsed,
-      IOContext context, ValueType type) throws IOException {
-    return new FloatsWriter(dir, id, bytesUsed, context, type);
-  }
-
-  public static IndexDocValues getValues(Directory dir, String id, int maxDoc, IOContext context, ValueType type)
-      throws IOException {
-    return new FloatsReader(dir, id, maxDoc, context, type);
-  }
-  
-  private static int typeToSize(ValueType type) {
-    switch (type) {
-    case FLOAT_32:
-      return 4;
-    case FLOAT_64:
-      return 8;
-    default:
-      throw new IllegalStateException("illegal type " + type);
-    }
-  }
-  
-  final static class FloatsWriter extends FixedStraightBytesImpl.Writer {
-   
-    private final int size; 
-    private final IndexDocValuesArray template;
-    public FloatsWriter(Directory dir, String id, Counter bytesUsed,
-        IOContext context, ValueType type) throws IOException {
-      super(dir, id, CODEC_NAME, VERSION_CURRENT, bytesUsed, context);
-      size = typeToSize(type);
-      this.bytesRef = new BytesRef(size);
-      bytesRef.length = size;
-      template = IndexDocValuesArray.TEMPLATES.get(type);
-      assert template != null;
-    }
-    
-    public void add(int docID, double v) throws IOException {
-      template.toBytes(v, bytesRef);
-      add(docID, bytesRef);
-    }
-    
-    @Override
-    public void add(int docID, PerDocFieldValues docValues) throws IOException {
-      add(docID, docValues.getFloat());
-    }
-    
-    @Override
-    protected boolean tryBulkMerge(IndexDocValues docValues) {
-      // only bulk merge if value type is the same otherwise size differs
-      return super.tryBulkMerge(docValues) && docValues.type() == template.type();
-    }
-    
-    @Override
-    protected void setMergeBytes(int sourceDoc) {
-      final double value = currentMergeSource.getFloat(sourceDoc);
-      template.toBytes(value, bytesRef);
-    }
-  }
-  
-  final static class FloatsReader extends FixedStraightBytesImpl.FixedStraightReader {
-    final IndexDocValuesArray arrayTemplate;
-    FloatsReader(Directory dir, String id, int maxDoc, IOContext context, ValueType type)
-        throws IOException {
-      super(dir, id, CODEC_NAME, VERSION_CURRENT, maxDoc, context, type);
-      arrayTemplate = IndexDocValuesArray.TEMPLATES.get(type);
-      assert size == 4 || size == 8;
-    }
-    
-    @Override
-    public Source load() throws IOException {
-      final IndexInput indexInput = cloneData();
-      try {
-        return arrayTemplate.newFromInput(indexInput, maxDoc);
-      } finally {
-        IOUtils.close(indexInput);
-      }
-    }
-    
-  }
-
-}
\ No newline at end of file


diff -ruN -x .svn -x build lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/values/IndexDocValuesArray.java lucene-3622/lucene/src/java/org/apache/lucene/index/values/IndexDocValuesArray.java
--- lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/values/IndexDocValuesArray.java	2011-12-06 18:45:03.860810978 -0500
+++ lucene-3622/lucene/src/java/org/apache/lucene/index/values/IndexDocValuesArray.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,305 +0,0 @@
-package org.apache.lucene.index.values;
-
-import java.io.IOException;
-import java.util.Collections;
-import java.util.EnumMap;
-import java.util.Map;
-
-import org.apache.lucene.index.values.IndexDocValues.Source;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.RamUsageEstimator;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to You under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-
-/**
- * @lucene.experimental
- */
-abstract class IndexDocValuesArray extends Source {
-
-  static final Map<ValueType, IndexDocValuesArray> TEMPLATES;
-
-  static {
-    EnumMap<ValueType, IndexDocValuesArray> templates = new EnumMap<ValueType, IndexDocValuesArray>(
-        ValueType.class);
-    templates.put(ValueType.FIXED_INTS_16, new ShortValues());
-    templates.put(ValueType.FIXED_INTS_32, new IntValues());
-    templates.put(ValueType.FIXED_INTS_64, new LongValues());
-    templates.put(ValueType.FIXED_INTS_8, new ByteValues());
-    templates.put(ValueType.FLOAT_32, new FloatValues());
-    templates.put(ValueType.FLOAT_64, new DoubleValues());
-    TEMPLATES = Collections.unmodifiableMap(templates);
-  }
-
-  protected final int bytesPerValue;
-
-  IndexDocValuesArray(int bytesPerValue, ValueType type) {
-    super(type);
-    this.bytesPerValue = bytesPerValue;
-  }
-
-  public abstract IndexDocValuesArray newFromInput(IndexInput input, int numDocs)
-      throws IOException;
-
-  @Override
-  public final boolean hasArray() {
-    return true;
-  }
-
-  void toBytes(long value, BytesRef bytesRef) {
-    BytesRefUtils.copyLong(bytesRef, value);
-  }
-
-  void toBytes(double value, BytesRef bytesRef) {
-    BytesRefUtils.copyLong(bytesRef, Double.doubleToRawLongBits(value));
-  }
-
-  final static class ByteValues extends IndexDocValuesArray {
-    private final byte[] values;
-
-    ByteValues() {
-      super(1, ValueType.FIXED_INTS_8);
-      values = new byte[0];
-    }
-
-    private ByteValues(IndexInput input, int numDocs) throws IOException {
-      super(1, ValueType.FIXED_INTS_8);
-      values = new byte[numDocs];
-      input.readBytes(values, 0, values.length, false);
-    }
-
-    @Override
-    public byte[] getArray() {
-      return values;
-    }
-
-    @Override
-    public long getInt(int docID) {
-      assert docID >= 0 && docID < values.length;
-      return values[docID];
-    }
-
-    @Override
-    public IndexDocValuesArray newFromInput(IndexInput input, int numDocs)
-        throws IOException {
-      return new ByteValues(input, numDocs);
-    }
-
-    void toBytes(long value, BytesRef bytesRef) {
-      bytesRef.bytes[0] = (byte) (0xFFL & value);
-    }
-
-  };
-
-  final static class ShortValues extends IndexDocValuesArray {
-    private final short[] values;
-
-    ShortValues() {
-      super(RamUsageEstimator.NUM_BYTES_SHORT, ValueType.FIXED_INTS_16);
-      values = new short[0];
-    }
-
-    private ShortValues(IndexInput input, int numDocs) throws IOException {
-      super(RamUsageEstimator.NUM_BYTES_SHORT, ValueType.FIXED_INTS_16);
-      values = new short[numDocs];
-      for (int i = 0; i < values.length; i++) {
-        values[i] = input.readShort();
-      }
-    }
-
-    @Override
-    public short[] getArray() {
-      return values;
-    }
-
-    @Override
-    public long getInt(int docID) {
-      assert docID >= 0 && docID < values.length;
-      return values[docID];
-    }
-
-    @Override
-    public IndexDocValuesArray newFromInput(IndexInput input, int numDocs)
-        throws IOException {
-      return new ShortValues(input, numDocs);
-    }
-
-    void toBytes(long value, BytesRef bytesRef) {
-      BytesRefUtils.copyShort(bytesRef, (short) (0xFFFFL & value));
-    }
-
-  };
-
-  final static class IntValues extends IndexDocValuesArray {
-    private final int[] values;
-
-    IntValues() {
-      super(RamUsageEstimator.NUM_BYTES_INT, ValueType.FIXED_INTS_32);
-      values = new int[0];
-    }
-
-    private IntValues(IndexInput input, int numDocs) throws IOException {
-      super(RamUsageEstimator.NUM_BYTES_INT, ValueType.FIXED_INTS_32);
-      values = new int[numDocs];
-      for (int i = 0; i < values.length; i++) {
-        values[i] = input.readInt();
-      }
-    }
-
-    @Override
-    public int[] getArray() {
-      return values;
-    }
-
-    @Override
-    public long getInt(int docID) {
-      assert docID >= 0 && docID < values.length;
-      return 0xFFFFFFFF & values[docID];
-    }
-
-    @Override
-    public IndexDocValuesArray newFromInput(IndexInput input, int numDocs)
-        throws IOException {
-      return new IntValues(input, numDocs);
-    }
-
-    void toBytes(long value, BytesRef bytesRef) {
-      BytesRefUtils.copyInt(bytesRef, (int) (0xFFFFFFFF & value));
-    }
-
-  };
-
-  final static class LongValues extends IndexDocValuesArray {
-    private final long[] values;
-
-    LongValues() {
-      super(RamUsageEstimator.NUM_BYTES_LONG, ValueType.FIXED_INTS_64);
-      values = new long[0];
-    }
-
-    private LongValues(IndexInput input, int numDocs) throws IOException {
-      super(RamUsageEstimator.NUM_BYTES_LONG, ValueType.FIXED_INTS_64);
-      values = new long[numDocs];
-      for (int i = 0; i < values.length; i++) {
-        values[i] = input.readLong();
-      }
-    }
-
-    @Override
-    public long[] getArray() {
-      return values;
-    }
-
-    @Override
-    public long getInt(int docID) {
-      assert docID >= 0 && docID < values.length;
-      return values[docID];
-    }
-
-    @Override
-    public IndexDocValuesArray newFromInput(IndexInput input, int numDocs)
-        throws IOException {
-      return new LongValues(input, numDocs);
-    }
-
-  };
-
-  final static class FloatValues extends IndexDocValuesArray {
-    private final float[] values;
-
-    FloatValues() {
-      super(RamUsageEstimator.NUM_BYTES_FLOAT, ValueType.FLOAT_32);
-      values = new float[0];
-    }
-
-    private FloatValues(IndexInput input, int numDocs) throws IOException {
-      super(RamUsageEstimator.NUM_BYTES_FLOAT, ValueType.FLOAT_32);
-      values = new float[numDocs];
-      /*
-       * we always read BIG_ENDIAN here since the writer serialized plain bytes
-       * we can simply read the ints / longs back in using readInt / readLong
-       */
-      for (int i = 0; i < values.length; i++) {
-        values[i] = Float.intBitsToFloat(input.readInt());
-      }
-    }
-
-    @Override
-    public float[] getArray() {
-      return values;
-    }
-
-    @Override
-    public double getFloat(int docID) {
-      assert docID >= 0 && docID < values.length;
-      return values[docID];
-    }
-    
-    @Override
-    void toBytes(double value, BytesRef bytesRef) {
-      BytesRefUtils.copyInt(bytesRef, Float.floatToRawIntBits((float)value));
-
-    }
-
-    @Override
-    public IndexDocValuesArray newFromInput(IndexInput input, int numDocs)
-        throws IOException {
-      return new FloatValues(input, numDocs);
-    }
-  };
-
-  final static class DoubleValues extends IndexDocValuesArray {
-    private final double[] values;
-
-    DoubleValues() {
-      super(RamUsageEstimator.NUM_BYTES_DOUBLE, ValueType.FLOAT_64);
-      values = new double[0];
-    }
-
-    private DoubleValues(IndexInput input, int numDocs) throws IOException {
-      super(RamUsageEstimator.NUM_BYTES_DOUBLE, ValueType.FLOAT_64);
-      values = new double[numDocs];
-      /*
-       * we always read BIG_ENDIAN here since the writer serialized plain bytes
-       * we can simply read the ints / longs back in using readInt / readLong
-       */
-      for (int i = 0; i < values.length; i++) {
-        values[i] = Double.longBitsToDouble(input.readLong());
-      }
-    }
-
-    @Override
-    public double[] getArray() {
-      return values;
-    }
-
-    @Override
-    public double getFloat(int docID) {
-      assert docID >= 0 && docID < values.length;
-      return values[docID];
-    }
-
-    @Override
-    public IndexDocValuesArray newFromInput(IndexInput input, int numDocs)
-        throws IOException {
-      return new DoubleValues(input, numDocs);
-    }
-
-  };
-
-}


diff -ruN -x .svn -x build lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/values/IndexDocValues.java lucene-3622/lucene/src/java/org/apache/lucene/index/values/IndexDocValues.java
--- lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/values/IndexDocValues.java	2011-12-07 10:48:20.389817474 -0500
+++ lucene-3622/lucene/src/java/org/apache/lucene/index/values/IndexDocValues.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,414 +0,0 @@
-package org.apache.lucene.index.values;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-import java.io.Closeable;
-import java.io.IOException;
-import java.util.Comparator;
-
-import org.apache.lucene.document.IndexDocValuesField;
-import org.apache.lucene.index.Fields;
-import org.apache.lucene.index.FieldsEnum;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.codecs.DocValuesFormat;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.packed.PackedInts;
-
-/**
- * {@link IndexDocValues} provides a dense per-document typed storage for fast
- * value access based on the lucene internal document id. {@link IndexDocValues}
- * exposes two distinct APIs:
- * <ul>
- * <li>via {@link #getSource()} providing RAM resident random access</li>
- * <li>via {@link #getDirectSource()} providing on disk random access</li>
- * </ul> {@link IndexDocValues} are exposed via
- * {@link IndexReader#perDocValues()} on a per-segment basis. For best
- * performance {@link IndexDocValues} should be consumed per-segment just like
- * IndexReader.
- * <p>
- * {@link IndexDocValues} are fully integrated into the {@link DocValuesFormat} API.
- * 
- * @see ValueType for limitations and default implementation documentation
- * @see IndexDocValuesField for adding values to the index
- * @see DocValuesFormat#docsConsumer(org.apache.lucene.index.PerDocWriteState) for
- *      customization
- * @lucene.experimental
- */
-public abstract class IndexDocValues implements Closeable {
-
-  public static final IndexDocValues[] EMPTY_ARRAY = new IndexDocValues[0];
-
-  private volatile SourceCache cache = new SourceCache.DirectSourceCache();
-  private final Object cacheLock = new Object();
-  
-  /**
-   * Loads a new {@link Source} instance for this {@link IndexDocValues} field
-   * instance. Source instances returned from this method are not cached. It is
-   * the callers responsibility to maintain the instance and release its
-   * resources once the source is not needed anymore.
-   * <p>
-   * For managed {@link Source} instances see {@link #getSource()}.
-   * 
-   * @see #getSource()
-   * @see #setCache(SourceCache)
-   */
-  public abstract Source load() throws IOException;
-
-  /**
-   * Returns a {@link Source} instance through the current {@link SourceCache}.
-   * Iff no {@link Source} has been loaded into the cache so far the source will
-   * be loaded through {@link #load()} and passed to the {@link SourceCache}.
-   * The caller of this method should not close the obtained {@link Source}
-   * instance unless it is not needed for the rest of its life time.
-   * <p>
-   * {@link Source} instances obtained from this method are closed / released
-   * from the cache once this {@link IndexDocValues} instance is closed by the
-   * {@link IndexReader}, {@link Fields} or {@link FieldsEnum} the
-   * {@link IndexDocValues} was created from.
-   */
-  public Source getSource() throws IOException {
-    return cache.load(this);
-  }
-
-  /**
-   * Returns a disk resident {@link Source} instance. Direct Sources are not
-   * cached in the {@link SourceCache} and should not be shared between threads.
-   */
-  public abstract Source getDirectSource() throws IOException;
-
-  /**
-   * Returns the {@link ValueType} of this {@link IndexDocValues} instance
-   */
-  public abstract ValueType type();
-
-  /**
-   * Closes this {@link IndexDocValues} instance. This method should only be called
-   * by the creator of this {@link IndexDocValues} instance. API users should not
-   * close {@link IndexDocValues} instances.
-   */
-  public void close() throws IOException {
-    cache.close(this);
-  }
-
-  /**
-   * Returns the size per value in bytes or <code>-1</code> iff size per value
-   * is variable.
-   * 
-   * @return the size per value in bytes or <code>-1</code> iff size per value
-   * is variable.
-   */
-  public int getValueSize() {
-    return -1;
-  }
-
-  /**
-   * Sets the {@link SourceCache} used by this {@link IndexDocValues} instance. This
-   * method should be called before {@link #load()} is called. All {@link Source} instances in the currently used cache will be closed
-   * before the new cache is installed.
-   * <p>
-   * Note: All instances previously obtained from {@link #load()} will be lost.
-   * 
-   * @throws IllegalArgumentException
-   *           if the given cache is <code>null</code>
-   * 
-   */
-  public void setCache(SourceCache cache) {
-    if (cache == null)
-      throw new IllegalArgumentException("cache must not be null");
-    synchronized (cacheLock) {
-      SourceCache toClose = this.cache;
-      this.cache = cache;
-      toClose.close(this);
-    }
-  }
-
-  /**
-   * Source of per document values like long, double or {@link BytesRef}
-   * depending on the {@link IndexDocValues} fields {@link ValueType}. Source
-   * implementations provide random access semantics similar to array lookups
-   * <p>
-   * @see IndexDocValues#getSource()
-   * @see IndexDocValues#getDirectSource()
-   */
-  public static abstract class Source {
-    
-    protected final ValueType type;
-
-    protected Source(ValueType type) {
-      this.type = type;
-    }
-    /**
-     * Returns a <tt>long</tt> for the given document id or throws an
-     * {@link UnsupportedOperationException} if this source doesn't support
-     * <tt>long</tt> values.
-     * 
-     * @throws UnsupportedOperationException
-     *           if this source doesn't support <tt>long</tt> values.
-     */
-    public long getInt(int docID) {
-      throw new UnsupportedOperationException("ints are not supported");
-    }
-
-    /**
-     * Returns a <tt>double</tt> for the given document id or throws an
-     * {@link UnsupportedOperationException} if this source doesn't support
-     * <tt>double</tt> values.
-     * 
-     * @throws UnsupportedOperationException
-     *           if this source doesn't support <tt>double</tt> values.
-     */
-    public double getFloat(int docID) {
-      throw new UnsupportedOperationException("floats are not supported");
-    }
-
-    /**
-     * Returns a {@link BytesRef} for the given document id or throws an
-     * {@link UnsupportedOperationException} if this source doesn't support
-     * <tt>byte[]</tt> values.
-     * @throws IOException 
-     * 
-     * @throws UnsupportedOperationException
-     *           if this source doesn't support <tt>byte[]</tt> values.
-     */
-    public BytesRef getBytes(int docID, BytesRef ref) {
-      throw new UnsupportedOperationException("bytes are not supported");
-    }
-
-    /**
-     * Returns the {@link ValueType} of this source.
-     * 
-     * @return the {@link ValueType} of this source.
-     */
-    public ValueType type() {
-      return type;
-    }
-
-    /**
-     * Returns <code>true</code> iff this {@link Source} exposes an array via
-     * {@link #getArray()} otherwise <code>false</code>.
-     * 
-     * @return <code>true</code> iff this {@link Source} exposes an array via
-     *         {@link #getArray()} otherwise <code>false</code>.
-     */
-    public boolean hasArray() {
-      return false;
-    }
-
-    /**
-     * Returns the internal array representation iff this {@link Source} uses an
-     * array as its inner representation, otherwise <code>UOE</code>.
-     */
-    public Object getArray() {
-      throw new UnsupportedOperationException("getArray is not supported");
-    }
-    
-    /**
-     * If this {@link Source} is sorted this method will return an instance of
-     * {@link SortedSource} otherwise <code>UOE</code>
-     */
-    public SortedSource asSortedSource() {
-      throw new UnsupportedOperationException("asSortedSource is not supported");
-    }
-  }
-
-  /**
-   * A sorted variant of {@link Source} for <tt>byte[]</tt> values per document.
-   * <p>
-   */
-  public static abstract class SortedSource extends Source {
-
-    private final Comparator<BytesRef> comparator;
-
-    protected SortedSource(ValueType type, Comparator<BytesRef> comparator) {
-      super(type);
-      this.comparator = comparator;
-    }
-
-    @Override
-    public BytesRef getBytes(int docID, BytesRef bytesRef) {
-      final int ord = ord(docID);
-      if (ord < 0) {
-        bytesRef.length = 0;
-      } else {
-        getByOrd(ord , bytesRef);
-      }
-      return bytesRef;
-    }
-
-    /**
-     * Returns ord for specified docID. Ord is dense, ie, starts at 0, then increments by 1
-     * for the next (as defined by {@link Comparator} value.
-     */
-    public abstract int ord(int docID);
-
-    /** Returns value for specified ord. */
-    public abstract BytesRef getByOrd(int ord, BytesRef bytesRef);
-
-    /**
-     * Returns the PackedInts.Reader impl that maps document to ord.
-     */
-    public abstract PackedInts.Reader getDocToOrd();
-    
-    /**
-     * Returns the comparator used to order the BytesRefs.
-     */
-    public Comparator<BytesRef> getComparator() {
-      return comparator;
-    }
-
-    /**
-     * Performs a lookup by value.
-     * 
-     * @param value
-     *          the value to look up
-     * @param spare
-     *          a spare {@link BytesRef} instance used to compare internal
-     *          values to the given value. Must not be <code>null</code>
-     * @return the given values ordinal if found or otherwise
-     *         <code>(-(ord)-1)</code>, defined as the ordinal of the first
-     *         element that is greater than the given value. This guarantees
-     *         that the return value will always be &gt;= 0 if the given value
-     *         is found.
-     */
-    public int getByValue(BytesRef value, BytesRef spare) {
-      return binarySearch(value, spare, 0, getValueCount() - 1);
-    }    
-
-    private int binarySearch(BytesRef b, BytesRef bytesRef, int low,
-        int high) {
-      int mid = 0;
-      while (low <= high) {
-        mid = (low + high) >>> 1;
-        getByOrd(mid, bytesRef);
-        final int cmp = comparator.compare(bytesRef, b);
-        if (cmp < 0) {
-          low = mid + 1;
-        } else if (cmp > 0) {
-          high = mid - 1;
-        } else {
-          return mid;
-        }
-      }
-      assert comparator.compare(bytesRef, b) != 0;
-      return -(low + 1);
-    }
-    
-    @Override
-    public SortedSource asSortedSource() {
-      return this;
-    }
-    
-    /**
-     * Returns the number of unique values in this sorted source
-     */
-    public abstract int getValueCount();
-  }
-
-  /** Returns a Source that always returns default (missing)
-   *  values for all documents. */
-  public static Source getDefaultSource(final ValueType type) {
-    return new Source(type) {
-      @Override
-      public long getInt(int docID) {
-        return 0;
-      }
-
-      @Override
-      public double getFloat(int docID) {
-        return 0.0;
-      }
-
-      @Override
-      public BytesRef getBytes(int docID, BytesRef ref) {
-        ref.length = 0;
-        return ref;
-      }
-    };
-  }
-
-  /** Returns a SortedSource that always returns default (missing)
-   *  values for all documents. */
-  public static SortedSource getDefaultSortedSource(final ValueType type, final int size) {
-
-    final PackedInts.Reader docToOrd = new PackedInts.Reader() {
-      @Override
-      public long get(int index) {
-        return 0;
-      }
-
-      @Override
-      public int getBitsPerValue() {
-        return 0;
-      }
-
-      @Override
-      public int size() {
-        return size;
-      }
-
-      @Override
-      public boolean hasArray() {
-        return false;
-      }
-
-      @Override
-      public Object getArray() {
-        return null;
-      }
-    };
-
-    return new SortedSource(type, BytesRef.getUTF8SortedAsUnicodeComparator()) {
-
-      @Override
-      public BytesRef getBytes(int docID, BytesRef ref) {
-        ref.length = 0;
-        return ref;
-      }
-
-      @Override
-      public int ord(int docID) {
-        return 0;
-      }
-
-      @Override
-      public BytesRef getByOrd(int ord, BytesRef bytesRef) {
-        assert ord == 0;
-        bytesRef.length = 0;
-        return bytesRef;
-      }
-
-      @Override
-      public PackedInts.Reader getDocToOrd() {
-        return docToOrd;
-      }
-
-      @Override
-      public int getByValue(BytesRef value, BytesRef spare) {
-        if (value.length == 0) {
-          return 0;
-        } else {
-          return -1;
-        }
-      }
-
-      @Override
-        public int getValueCount() {
-        return 1;
-      }
-    };
-  }
-}


diff -ruN -x .svn -x build lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/values/Ints.java lucene-3622/lucene/src/java/org/apache/lucene/index/values/Ints.java
--- lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/values/Ints.java	2011-12-06 18:45:03.860810978 -0500
+++ lucene-3622/lucene/src/java/org/apache/lucene/index/values/Ints.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,148 +0,0 @@
-package org.apache.lucene.index.values;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.Counter;
-import org.apache.lucene.util.IOUtils;
-
-/**
- * Stores ints packed and fixed with fixed-bit precision.
- * 
- * @lucene.experimental
- */
-public final class Ints {
-  protected static final String CODEC_NAME = "Ints";
-  protected static final int VERSION_START = 0;
-  protected static final int VERSION_CURRENT = VERSION_START;
-
-  private Ints() {
-  }
-  
-  public static Writer getWriter(Directory dir, String id, Counter bytesUsed,
-      ValueType type, IOContext context) throws IOException {
-    return type == ValueType.VAR_INTS ? new PackedIntValues.PackedIntsWriter(dir, id,
-        bytesUsed, context) : new IntsWriter(dir, id, bytesUsed, context, type);
-  }
-
-  public static IndexDocValues getValues(Directory dir, String id, int numDocs,
-      ValueType type, IOContext context) throws IOException {
-    return type == ValueType.VAR_INTS ? new PackedIntValues.PackedIntsReader(dir, id,
-        numDocs, context) : new IntsReader(dir, id, numDocs, context, type);
-  }
-  
-  private static ValueType sizeToType(int size) {
-    switch (size) {
-    case 1:
-      return ValueType.FIXED_INTS_8;
-    case 2:
-      return ValueType.FIXED_INTS_16;
-    case 4:
-      return ValueType.FIXED_INTS_32;
-    case 8:
-      return ValueType.FIXED_INTS_64;
-    default:
-      throw new IllegalStateException("illegal size " + size);
-    }
-  }
-  
-  private static int typeToSize(ValueType type) {
-    switch (type) {
-    case FIXED_INTS_16:
-      return 2;
-    case FIXED_INTS_32:
-      return 4;
-    case FIXED_INTS_64:
-      return 8;
-    case FIXED_INTS_8:
-      return 1;
-    default:
-      throw new IllegalStateException("illegal type " + type);
-    }
-  }
-
-
-  static class IntsWriter extends FixedStraightBytesImpl.Writer {
-    private final IndexDocValuesArray template;
-
-    public IntsWriter(Directory dir, String id, Counter bytesUsed,
-        IOContext context, ValueType valueType) throws IOException {
-      this(dir, id, CODEC_NAME, VERSION_CURRENT, bytesUsed, context, valueType);
-    }
-
-    protected IntsWriter(Directory dir, String id, String codecName,
-        int version, Counter bytesUsed, IOContext context, ValueType valueType) throws IOException {
-      super(dir, id, codecName, version, bytesUsed, context);
-      size = typeToSize(valueType);
-      this.bytesRef = new BytesRef(size);
-      bytesRef.length = size;
-      template = IndexDocValuesArray.TEMPLATES.get(valueType);
-    }
-    
-    @Override
-    public void add(int docID, long v) throws IOException {
-      template.toBytes(v, bytesRef);
-      add(docID, bytesRef);
-    }
-
-    @Override
-    public void add(int docID, PerDocFieldValues docValues) throws IOException {
-      add(docID, docValues.getInt());
-    }
-    
-    @Override
-    protected void setMergeBytes(int sourceDoc) {
-      final long value = currentMergeSource.getInt(sourceDoc);
-      template.toBytes(value, bytesRef);
-    }
-    
-    @Override
-    protected boolean tryBulkMerge(IndexDocValues docValues) {
-      // only bulk merge if value type is the same otherwise size differs
-      return super.tryBulkMerge(docValues) && docValues.type() == template.type();
-    }
-  }
-  
-  final static class IntsReader extends FixedStraightBytesImpl.FixedStraightReader {
-    private final IndexDocValuesArray arrayTemplate;
-
-    IntsReader(Directory dir, String id, int maxDoc, IOContext context, ValueType type)
-        throws IOException {
-      super(dir, id, CODEC_NAME, VERSION_CURRENT, maxDoc,
-          context, type);
-      arrayTemplate = IndexDocValuesArray.TEMPLATES.get(type);
-      assert arrayTemplate != null;
-      assert type == sizeToType(size);
-    }
-
-    @Override
-    public Source load() throws IOException {
-      final IndexInput indexInput = cloneData();
-      try {
-        return arrayTemplate.newFromInput(indexInput, maxDoc);
-      } finally {
-        IOUtils.close(indexInput);
-      }
-    }
-  }
-}


diff -ruN -x .svn -x build lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/values/MultiIndexDocValues.java lucene-3622/lucene/src/java/org/apache/lucene/index/values/MultiIndexDocValues.java
--- lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/values/MultiIndexDocValues.java	2011-12-07 10:44:32.325813503 -0500
+++ lucene-3622/lucene/src/java/org/apache/lucene/index/values/MultiIndexDocValues.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,211 +0,0 @@
-package org.apache.lucene.index.values;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-import java.io.IOException;
-import java.util.Arrays;
-
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.ReaderUtil;
-
-/**
- * A wrapper for compound IndexReader providing access to per segment
- * {@link IndexDocValues}
- * 
- * @lucene.experimental
- * @lucene.internal
- */
-public class MultiIndexDocValues extends IndexDocValues {
-
-  public static class DocValuesIndex {
-    public final static DocValuesIndex[] EMPTY_ARRAY = new DocValuesIndex[0];
-    final int start;
-    final int length;
-    final IndexDocValues docValues;
-
-    public DocValuesIndex(IndexDocValues docValues, int start, int length) {
-      this.docValues = docValues;
-      this.start = start;
-      this.length = length;
-    }
-  }
-
-  private DocValuesIndex[] docValuesIdx;
-  private int[] starts;
-  private ValueType type;
-  private int valueSize;
-
-  public MultiIndexDocValues() {
-    starts = new int[0];
-    docValuesIdx = new DocValuesIndex[0];
-  }
-
-  public MultiIndexDocValues(DocValuesIndex[] docValuesIdx) {
-    reset(docValuesIdx);
-  }
-
-  @Override
-  public Source load() throws IOException {
-    return new MultiSource(docValuesIdx, starts, false);
-  }
-
-  public IndexDocValues reset(DocValuesIndex[] docValuesIdx) {
-    final int[] start = new int[docValuesIdx.length];
-    TypePromoter promoter = TypePromoter.getIdentityPromoter();
-    for (int i = 0; i < docValuesIdx.length; i++) {
-      start[i] = docValuesIdx[i].start;
-      if (!(docValuesIdx[i].docValues instanceof EmptyDocValues)) {
-        // only promote if not a dummy
-        final TypePromoter incomingPromoter = TypePromoter.create(
-            docValuesIdx[i].docValues.type(),
-            docValuesIdx[i].docValues.getValueSize());
-        promoter = promoter.promote(incomingPromoter);
-        if (promoter == null) {
-          throw new IllegalStateException("Can not promote " + incomingPromoter);
-        }
-      }
-    }
-    this.type = promoter.type();
-    this.valueSize = promoter.getValueSize();
-    this.starts = start;
-    this.docValuesIdx = docValuesIdx;
-    return this;
-  }
-
-  public static class EmptyDocValues extends IndexDocValues {
-    final int maxDoc;
-    final Source emptySource;
-
-    public EmptyDocValues(int maxDoc, ValueType type) {
-      this.maxDoc = maxDoc;
-      this.emptySource = new EmptySource(type);
-    }
-
-    @Override
-    public Source load() throws IOException {
-      return emptySource;
-    }
-
-    @Override
-    public ValueType type() {
-      return emptySource.type();
-    }
-
-
-    @Override
-    public Source getDirectSource() throws IOException {
-      return emptySource;
-    }
-  }
-
-  private static class MultiSource extends Source {
-    private int numDocs = 0;
-    private int start = 0;
-    private Source current;
-    private final int[] starts;
-    private final DocValuesIndex[] docValuesIdx;
-    private boolean direct;
-
-    public MultiSource(DocValuesIndex[] docValuesIdx, int[] starts, boolean direct) {
-      super(docValuesIdx[0].docValues.type());
-      this.docValuesIdx = docValuesIdx;
-      this.starts = starts;
-      assert docValuesIdx.length != 0;
-      this.direct = direct;
-    }
-
-    public long getInt(int docID) {
-      final int doc = ensureSource(docID);
-      return current.getInt(doc);
-    }
-
-    private final int ensureSource(int docID) {
-      if (docID >= start && docID < start+numDocs) {
-        return docID - start;
-      } else {
-        final int idx = ReaderUtil.subIndex(docID, starts);
-        assert idx >= 0 && idx < docValuesIdx.length : "idx was " + idx
-            + " for doc id: " + docID + " slices : " + Arrays.toString(starts);
-        assert docValuesIdx[idx] != null;
-        try {
-          if (direct) {
-            current = docValuesIdx[idx].docValues.getDirectSource();
-          } else {
-            current = docValuesIdx[idx].docValues.getSource();
-          }
-        } catch (IOException e) {
-          throw new RuntimeException("load failed", e); // TODO how should we
-          // handle this
-        }
-
-        start = docValuesIdx[idx].start;
-        numDocs = docValuesIdx[idx].length;
-        return docID - start;
-      }
-    }
-
-    public double getFloat(int docID) {
-      final int doc = ensureSource(docID);
-      return current.getFloat(doc);
-    }
-
-    public BytesRef getBytes(int docID, BytesRef bytesRef) {
-      final int doc = ensureSource(docID);
-      return current.getBytes(doc, bytesRef);
-    }
-  }
-
-  // TODO: this is dup of IndexDocValues.getDefaultSource()?
-  private static class EmptySource extends Source {
-
-    public EmptySource(ValueType type) {
-      super(type);
-    }
-
-    @Override
-    public BytesRef getBytes(int docID, BytesRef ref) {
-      ref.length = 0;
-      return ref;
-
-    }
-
-    @Override
-    public double getFloat(int docID) {
-      return 0d;
-    }
-
-    @Override
-    public long getInt(int docID) {
-      return 0;
-    }
-  }
-
-  @Override
-  public ValueType type() {
-    return type;
-  }
-
-  @Override
-  public int getValueSize() {
-    return valueSize;
-  }
-
-  @Override
-  public Source getDirectSource() throws IOException {
-    return new MultiSource(docValuesIdx, starts, true);
-  }
-}


diff -ruN -x .svn -x build lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/values/PackedIntValues.java lucene-3622/lucene/src/java/org/apache/lucene/index/values/PackedIntValues.java
--- lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/values/PackedIntValues.java	2011-12-06 18:45:03.860810978 -0500
+++ lucene-3622/lucene/src/java/org/apache/lucene/index/values/PackedIntValues.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,262 +0,0 @@
-package org.apache.lucene.index.values;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-import java.io.IOException;
-
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.values.FixedStraightBytesImpl.FixedBytesWriterBase;
-import org.apache.lucene.index.values.IndexDocValues.Source;
-import org.apache.lucene.index.values.IndexDocValuesArray.LongValues;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.CodecUtil;
-import org.apache.lucene.util.Counter;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.packed.PackedInts;
-
-/**
- * Stores integers using {@link PackedInts}
- * 
- * @lucene.experimental
- * */
-class PackedIntValues {
-
-  private static final String CODEC_NAME = "PackedInts";
-  private static final byte PACKED = 0x00;
-  private static final byte FIXED_64 = 0x01;
-
-  static final int VERSION_START = 0;
-  static final int VERSION_CURRENT = VERSION_START;
-
-  static class PackedIntsWriter extends FixedBytesWriterBase {
-
-    private long minValue;
-    private long maxValue;
-    private boolean started;
-    private int lastDocId = -1;
-
-    protected PackedIntsWriter(Directory dir, String id, Counter bytesUsed,
-        IOContext context) throws IOException {
-      super(dir, id, CODEC_NAME, VERSION_CURRENT, bytesUsed, context);
-      bytesRef = new BytesRef(8);
-    }
-
-    @Override
-    public void add(int docID, long v) throws IOException {
-      assert lastDocId < docID;
-      if (!started) {
-        started = true;
-        minValue = maxValue = v;
-      } else {
-        if (v < minValue) {
-          minValue = v;
-        } else if (v > maxValue) {
-          maxValue = v;
-        }
-      }
-      lastDocId = docID;
-      BytesRefUtils.copyLong(bytesRef, v);
-      add(docID, bytesRef);
-    }
-
-    @Override
-    public void finish(int docCount) throws IOException {
-      boolean success = false;
-      final IndexOutput dataOut = getOrCreateDataOut();
-      try {
-        if (!started) {
-          minValue = maxValue = 0;
-        }
-        final long delta = maxValue - minValue;
-        // if we exceed the range of positive longs we must switch to fixed
-        // ints
-        if (delta <= (maxValue >= 0 && minValue <= 0 ? Long.MAX_VALUE
-            : Long.MAX_VALUE - 1) && delta >= 0) {
-          dataOut.writeByte(PACKED);
-          writePackedInts(dataOut, docCount);
-          return; // done
-        } else {
-          dataOut.writeByte(FIXED_64);
-        }
-        writeData(dataOut);
-        writeZeros(docCount - (lastDocID + 1), dataOut);
-        success = true;
-      } finally {
-        resetPool();
-        if (success) {
-          IOUtils.close(dataOut);
-        } else {
-          IOUtils.closeWhileHandlingException(dataOut);
-        }
-      }
-    }
-
-    @Override
-    protected void mergeDoc(int docID, int sourceDoc) throws IOException {
-      assert docID > lastDocId : "docID: " + docID
-          + " must be greater than the last added doc id: " + lastDocId;
-        add(docID, currentMergeSource.getInt(sourceDoc));
-    }
-
-    private void writePackedInts(IndexOutput datOut, int docCount) throws IOException {
-      datOut.writeLong(minValue);
-      
-      // write a default value to recognize docs without a value for that
-      // field
-      final long defaultValue = maxValue >= 0 && minValue <= 0 ? 0 - minValue
-          : ++maxValue - minValue;
-      datOut.writeLong(defaultValue);
-      PackedInts.Writer w = PackedInts.getWriter(datOut, docCount,
-          PackedInts.bitsRequired(maxValue - minValue));
-      for (int i = 0; i < lastDocID + 1; i++) {
-        set(bytesRef, i);
-        byte[] bytes = bytesRef.bytes;
-        int offset = bytesRef.offset;
-        long asLong =  
-           (((long)(bytes[offset+0] & 0xff) << 56) |
-            ((long)(bytes[offset+1] & 0xff) << 48) |
-            ((long)(bytes[offset+2] & 0xff) << 40) |
-            ((long)(bytes[offset+3] & 0xff) << 32) |
-            ((long)(bytes[offset+4] & 0xff) << 24) |
-            ((long)(bytes[offset+5] & 0xff) << 16) |
-            ((long)(bytes[offset+6] & 0xff) <<  8) |
-            ((long)(bytes[offset+7] & 0xff)));
-        w.add(asLong == 0 ? defaultValue : asLong - minValue);
-      }
-      for (int i = lastDocID + 1; i < docCount; i++) {
-        w.add(defaultValue);
-      }
-      w.finish();
-    }
-
-    @Override
-    public void add(int docID, PerDocFieldValues docValues) throws IOException {
-      add(docID, docValues.getInt());
-    }
-  }
-
-  /**
-   * Opens all necessary files, but does not read any data in until you call
-   * {@link #load}.
-   */
-  static class PackedIntsReader extends IndexDocValues {
-    private final IndexInput datIn;
-    private final byte type;
-    private final int numDocs;
-    private final LongValues values;
-
-    protected PackedIntsReader(Directory dir, String id, int numDocs,
-        IOContext context) throws IOException {
-      datIn = dir.openInput(
-                IndexFileNames.segmentFileName(id, Bytes.DV_SEGMENT_SUFFIX, Writer.DATA_EXTENSION),
-          context);
-      this.numDocs = numDocs;
-      boolean success = false;
-      try {
-        CodecUtil.checkHeader(datIn, CODEC_NAME, VERSION_START, VERSION_START);
-        type = datIn.readByte();
-        values = type == FIXED_64 ? new LongValues() : null;
-        success = true;
-      } finally {
-        if (!success) {
-          IOUtils.closeWhileHandlingException(datIn);
-        }
-      }
-    }
-
-
-    /**
-     * Loads the actual values. You may call this more than once, eg if you
-     * already previously loaded but then discarded the Source.
-     */
-    @Override
-    public Source load() throws IOException {
-      boolean success = false;
-      final Source source;
-      IndexInput input = null;
-      try {
-        input = (IndexInput) datIn.clone();
-        
-        if (values == null) {
-          source = new PackedIntsSource(input, false);
-        } else {
-          source = values.newFromInput(input, numDocs);
-        }
-        success = true;
-        return source;
-      } finally {
-        if (!success) {
-          IOUtils.closeWhileHandlingException(input, datIn);
-        }
-      }
-    }
-
-    @Override
-    public void close() throws IOException {
-      super.close();
-      datIn.close();
-    }
-
-
-    @Override
-    public ValueType type() {
-      return ValueType.VAR_INTS;
-    }
-
-
-    @Override
-    public Source getDirectSource() throws IOException {
-      return values != null ? new FixedStraightBytesImpl.DirectFixedStraightSource((IndexInput) datIn.clone(), 8, ValueType.FIXED_INTS_64) : new PackedIntsSource((IndexInput) datIn.clone(), true);
-    }
-  }
-
-  
-  static class PackedIntsSource extends Source {
-    private final long minValue;
-    private final long defaultValue;
-    private final PackedInts.Reader values;
-
-    public PackedIntsSource(IndexInput dataIn, boolean direct) throws IOException {
-      super(ValueType.VAR_INTS);
-      minValue = dataIn.readLong();
-      defaultValue = dataIn.readLong();
-      values = direct ? PackedInts.getDirectReader(dataIn) : PackedInts.getReader(dataIn);
-    }
-    
-    @Override
-    public BytesRef getBytes(int docID, BytesRef ref) {
-      ref.grow(8);
-      BytesRefUtils.copyLong(ref, getInt(docID));
-      return ref;
-    }
-
-    @Override
-    public long getInt(int docID) {
-      // TODO -- can we somehow avoid 2X method calls
-      // on each get? must push minValue down, and make
-      // PackedInts implement Ints.Source
-      assert docID >= 0;
-      final long value = values.get(docID);
-      return value == defaultValue ? 0 : minValue + value;
-    }
-  }
-
-}
\ No newline at end of file


diff -ruN -x .svn -x build lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/values/PerDocFieldValues.java lucene-3622/lucene/src/java/org/apache/lucene/index/values/PerDocFieldValues.java
--- lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/values/PerDocFieldValues.java	2011-12-06 18:45:03.860810978 -0500
+++ lucene-3622/lucene/src/java/org/apache/lucene/index/values/PerDocFieldValues.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,98 +0,0 @@
-package org.apache.lucene.index.values;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-import java.util.Comparator;
-
-import org.apache.lucene.document.IndexDocValuesField;
-import org.apache.lucene.index.codecs.DocValuesConsumer;
-import org.apache.lucene.util.BytesRef;
-
-/**
- * Per document and field values consumed by {@link DocValuesConsumer}. 
- * @see IndexDocValuesField
- * 
- * @lucene.experimental
- */
-public interface PerDocFieldValues {
-
-  /**
-   * Sets the given <code>long</code> value.
-   */
-  public void setInt(long value);
-
-  /**
-   * Sets the given <code>float</code> value.
-   */
-  public void setFloat(float value);
-
-  /**
-   * Sets the given <code>double</code> value.
-   */
-  public void setFloat(double value);
-
-  /**
-   * Sets the given {@link BytesRef} value and the field's {@link ValueType}. The
-   * comparator for this field is set to <code>null</code>. If a
-   * <code>null</code> comparator is set the default comparator for the given
-   * {@link ValueType} is used.
-   */
-  public void setBytes(BytesRef value, ValueType type);
-
-  /**
-   * Sets the given {@link BytesRef} value, the field's {@link ValueType} and the
-   * field's comparator. If the {@link Comparator} is set to <code>null</code>
-   * the default for the given {@link ValueType} is used instead.
-   */
-  public void setBytes(BytesRef value, ValueType type, Comparator<BytesRef> comp);
-
-  /**
-   * Returns the set {@link BytesRef} or <code>null</code> if not set.
-   */
-  public BytesRef getBytes();
-
-  /**
-   * Returns the set {@link BytesRef} comparator or <code>null</code> if not set
-   */
-  public Comparator<BytesRef> bytesComparator();
-
-  /**
-   * Returns the set floating point value or <code>0.0d</code> if not set.
-   */
-  public double getFloat();
-
-  /**
-   * Returns the set <code>long</code> value of <code>0</code> if not set.
-   */
-  public long getInt();
-
-  /**
-   * Sets the {@link BytesRef} comparator for this field. If the field has a
-   * numeric {@link ValueType} the comparator will be ignored.
-   */
-  public void setBytesComparator(Comparator<BytesRef> comp);
-
-  /**
-   * Sets the {@link ValueType}
-   */
-  public void setDocValuesType(ValueType type);
-
-  /**
-  * Returns the {@link ValueType}
-  */
-  public ValueType docValuesType();
-}


diff -ruN -x .svn -x build lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/values/SortedBytesMergeUtils.java lucene-3622/lucene/src/java/org/apache/lucene/index/values/SortedBytesMergeUtils.java
--- lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/values/SortedBytesMergeUtils.java	2011-12-06 18:45:03.860810978 -0500
+++ lucene-3622/lucene/src/java/org/apache/lucene/index/values/SortedBytesMergeUtils.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,337 +0,0 @@
-package org.apache.lucene.index.values;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Collections;
-import java.util.Comparator;
-import java.util.List;
-
-import org.apache.lucene.index.MergeState;
-import org.apache.lucene.index.MergeState.IndexReaderAndLiveDocs;
-import org.apache.lucene.index.values.IndexDocValues.SortedSource;
-import org.apache.lucene.index.values.IndexDocValues.Source;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.util.ArrayUtil;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.PriorityQueue;
-import org.apache.lucene.util.packed.PackedInts;
-
-/**
- * @lucene.internal
- */
-final class SortedBytesMergeUtils {
-
-  private SortedBytesMergeUtils() {
-    // no instance
-  }
-
-  static MergeContext init(ValueType type, IndexDocValues[] docValues,
-      Comparator<BytesRef> comp, MergeState mergeState) {
-    int size = -1;
-    if (type == ValueType.BYTES_FIXED_SORTED) {
-      for (IndexDocValues indexDocValues : docValues) {
-        if (indexDocValues != null) {
-          size = indexDocValues.getValueSize();
-          break;
-        }
-      }
-      assert size >= 0;
-    }
-    return new MergeContext(comp, mergeState, size, type);
-  }
-
-  public static final class MergeContext {
-    private final Comparator<BytesRef> comp;
-    private final BytesRef missingValue = new BytesRef();
-    final int sizePerValues; // -1 if var length
-    final ValueType type;
-    final int[] docToEntry;
-    long[] offsets; // if non-null #mergeRecords collects byte offsets here
-
-    public MergeContext(Comparator<BytesRef> comp, MergeState mergeState,
-        int size, ValueType type) {
-      assert type == ValueType.BYTES_FIXED_SORTED || type == ValueType.BYTES_VAR_SORTED;
-      this.comp = comp;
-      this.sizePerValues = size;
-      this.type = type;
-      if (size > 0) {
-        missingValue.grow(size);
-        missingValue.length = size;
-      }
-      docToEntry = new int[mergeState.mergedDocCount];
-    }
-  }
-
-  static List<SortedSourceSlice> buildSlices(MergeState mergeState,
-      IndexDocValues[] docValues, MergeContext ctx) throws IOException {
-    final List<SortedSourceSlice> slices = new ArrayList<SortedSourceSlice>();
-    for (int i = 0; i < docValues.length; i++) {
-      final SortedSourceSlice nextSlice;
-      final Source directSource;
-      if (docValues[i] != null
-          && (directSource = docValues[i].getDirectSource()) != null) {
-        final SortedSourceSlice slice = new SortedSourceSlice(i, directSource
-            .asSortedSource(), mergeState, ctx.docToEntry);
-        nextSlice = slice;
-      } else {
-        nextSlice = new SortedSourceSlice(i, new MissingValueSource(ctx),
-            mergeState, ctx.docToEntry);
-      }
-      createOrdMapping(mergeState, nextSlice);
-      slices.add(nextSlice);
-    }
-    return Collections.unmodifiableList(slices);
-  }
-
-  /*
-   * In order to merge we need to map the ords used in each segment to the new
-   * global ords in the new segment. Additionally we need to drop values that
-   * are not referenced anymore due to deleted documents. This method walks all
-   * live documents and fetches their current ordinal. We store this ordinal per
-   * slice and (SortedSourceSlice#ordMapping) and remember the doc to ord
-   * mapping in docIDToRelativeOrd. After the merge SortedSourceSlice#ordMapping
-   * contains the new global ordinals for the relative index.
-   */
-  private static void createOrdMapping(MergeState mergeState,
-      SortedSourceSlice currentSlice) {
-    final int readerIdx = currentSlice.readerIdx;
-    final int[] currentDocMap = mergeState.docMaps[readerIdx];
-    final int docBase = currentSlice.docToOrdStart;
-    assert docBase == mergeState.docBase[readerIdx];
-    if (currentDocMap != null) { // we have deletes
-      for (int i = 0; i < currentDocMap.length; i++) {
-        final int doc = currentDocMap[i];
-        if (doc != -1) { // not deleted
-          final int ord = currentSlice.source.ord(i); // collect ords strictly
-                                                      // increasing
-          currentSlice.docIDToRelativeOrd[docBase + doc] = ord;
-          // use ord + 1 to identify unreferenced values (ie. == 0)
-          currentSlice.ordMapping[ord] = ord + 1;
-        }
-      }
-    } else { // no deletes
-      final IndexReaderAndLiveDocs indexReaderAndLiveDocs = mergeState.readers
-          .get(readerIdx);
-      final int numDocs = indexReaderAndLiveDocs.reader.numDocs();
-      assert indexReaderAndLiveDocs.liveDocs == null;
-      assert currentSlice.docToOrdEnd - currentSlice.docToOrdStart == numDocs;
-      for (int doc = 0; doc < numDocs; doc++) {
-        final int ord = currentSlice.source.ord(doc);
-        currentSlice.docIDToRelativeOrd[docBase + doc] = ord;
-        // use ord + 1 to identify unreferenced values (ie. == 0)
-        currentSlice.ordMapping[ord] = ord + 1;
-      }
-    }
-  }
-
-  static int mergeRecords(MergeContext ctx, IndexOutput datOut,
-      List<SortedSourceSlice> slices) throws IOException {
-    final RecordMerger merger = new RecordMerger(new MergeQueue(slices.size(),
-        ctx.comp), slices.toArray(new SortedSourceSlice[0]));
-    long[] offsets = ctx.offsets;
-    final boolean recordOffsets = offsets != null;
-    long offset = 0;
-    BytesRef currentMergedBytes;
-    merger.pushTop();
-    while (merger.queue.size() > 0) {
-      merger.pullTop();
-      currentMergedBytes = merger.current;
-      assert ctx.sizePerValues == -1 || ctx.sizePerValues == currentMergedBytes.length : "size: "
-          + ctx.sizePerValues + " spare: " + currentMergedBytes.length;
-
-      if (recordOffsets) {
-        offset += currentMergedBytes.length;
-        if (merger.currentOrd >= offsets.length) {
-          offsets = ArrayUtil.grow(offsets, merger.currentOrd + 1);
-        }
-        offsets[merger.currentOrd] = offset;
-      }
-      datOut.writeBytes(currentMergedBytes.bytes, currentMergedBytes.offset,
-          currentMergedBytes.length);
-      merger.pushTop();
-    }
-    ctx.offsets = offsets;
-    assert offsets == null || offsets[merger.currentOrd - 1] == offset;
-    return merger.currentOrd;
-  }
-
-  private static final class RecordMerger {
-    private final MergeQueue queue;
-    private final SortedSourceSlice[] top;
-    private int numTop;
-    BytesRef current;
-    int currentOrd = -1;
-
-    RecordMerger(MergeQueue queue, SortedSourceSlice[] top) {
-      super();
-      this.queue = queue;
-      this.top = top;
-      this.numTop = top.length;
-    }
-
-    private void pullTop() {
-      // extract all subs from the queue that have the same
-      // top record
-      assert numTop == 0;
-      assert currentOrd >= 0;
-      while (true) {
-        final SortedSourceSlice popped = top[numTop++] = queue.pop();
-        // use ord + 1 to identify unreferenced values (ie. == 0)
-        popped.ordMapping[popped.relativeOrd] = currentOrd + 1;
-        if (queue.size() == 0
-            || !(queue.top()).current.bytesEquals(top[0].current)) {
-          break;
-        }
-      }
-      current = top[0].current;
-    }
-
-    private void pushTop() throws IOException {
-      // call next() on each top, and put back into queue
-      for (int i = 0; i < numTop; i++) {
-        top[i].current = top[i].next();
-        if (top[i].current != null) {
-          queue.add(top[i]);
-        }
-      }
-      currentOrd++;
-      numTop = 0;
-    }
-  }
-
-  static class SortedSourceSlice {
-    final SortedSource source;
-    final int readerIdx;
-    /* global array indexed by docID containg the relative ord for the doc */
-    final int[] docIDToRelativeOrd;
-    /*
-     * maps relative ords to merged global ords - index is relative ord value
-     * new global ord this map gets updates as we merge ords. later we use the
-     * docIDtoRelativeOrd to get the previous relative ord to get the new ord
-     * from the relative ord map.
-     */
-    final int[] ordMapping;
-
-    /* start index into docIDToRelativeOrd */
-    final int docToOrdStart;
-    /* end index into docIDToRelativeOrd */
-    final int docToOrdEnd;
-    BytesRef current = new BytesRef();
-    /* the currently merged relative ordinal */
-    int relativeOrd = -1;
-
-    SortedSourceSlice(int readerIdx, SortedSource source, MergeState state,
-        int[] docToOrd) {
-      super();
-      this.readerIdx = readerIdx;
-      this.source = source;
-      this.docIDToRelativeOrd = docToOrd;
-      this.ordMapping = new int[source.getValueCount()];
-      this.docToOrdStart = state.docBase[readerIdx];
-      this.docToOrdEnd = this.docToOrdStart + numDocs(state, readerIdx);
-    }
-
-    private static int numDocs(MergeState state, int readerIndex) {
-      if (readerIndex == state.docBase.length - 1) {
-        return state.mergedDocCount - state.docBase[readerIndex];
-      }
-      return state.docBase[readerIndex + 1] - state.docBase[readerIndex];
-    }
-
-    BytesRef next() {
-      for (int i = relativeOrd + 1; i < ordMapping.length; i++) {
-        if (ordMapping[i] != 0) { // skip ords that are not referenced anymore
-          source.getByOrd(i, current);
-          relativeOrd = i;
-          return current;
-        }
-      }
-      return null;
-    }
-
-    void writeOrds(PackedInts.Writer writer) throws IOException {
-      for (int i = docToOrdStart; i < docToOrdEnd; i++) {
-        final int mappedOrd = docIDToRelativeOrd[i];
-        assert mappedOrd < ordMapping.length;
-        assert ordMapping[mappedOrd] > 0 : "illegal mapping ord maps to an unreferenced value";
-        writer.add(ordMapping[mappedOrd] - 1);
-      }
-    }
-  }
-
-  /*
-   * if a segment has no values at all we use this source to fill in the missing
-   * value in the right place (depending on the comparator used)
-   */
-  private static final class MissingValueSource extends SortedSource {
-
-    private BytesRef missingValue;
-
-    public MissingValueSource(MergeContext ctx) {
-      super(ctx.type, ctx.comp);
-      this.missingValue = ctx.missingValue;
-    }
-
-    @Override
-    public int ord(int docID) {
-      return 0;
-    }
-
-    @Override
-    public BytesRef getByOrd(int ord, BytesRef bytesRef) {
-      bytesRef.copyBytes(missingValue);
-      return bytesRef;
-    }
-
-    @Override
-    public PackedInts.Reader getDocToOrd() {
-      return null;
-    }
-
-    @Override
-    public int getValueCount() {
-      return 1;
-    }
-
-  }
-
-  /*
-   * merge queue
-   */
-  private static final class MergeQueue extends
-      PriorityQueue<SortedSourceSlice> {
-    final Comparator<BytesRef> comp;
-
-    public MergeQueue(int maxSize, Comparator<BytesRef> comp) {
-      super(maxSize);
-      this.comp = comp;
-    }
-
-    @Override
-    protected boolean lessThan(SortedSourceSlice a, SortedSourceSlice b) {
-      int cmp = comp.compare(a.current, b.current);
-      if (cmp != 0) {
-        return cmp < 0;
-      } else { // just a tie-breaker
-        return a.docToOrdStart < b.docToOrdStart;
-      }
-    }
-
-  }
-}


diff -ruN -x .svn -x build lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/values/SourceCache.java lucene-3622/lucene/src/java/org/apache/lucene/index/values/SourceCache.java
--- lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/values/SourceCache.java	2011-12-06 18:45:03.860810978 -0500
+++ lucene-3622/lucene/src/java/org/apache/lucene/index/values/SourceCache.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,93 +0,0 @@
-package org.apache.lucene.index.values;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.values.IndexDocValues.Source;
-
-/**
- * Abstract base class for {@link IndexDocValues} {@link Source} cache.
- * <p>
- * {@link Source} instances loaded via {@link IndexDocValues#load()} are entirely memory resident
- * and need to be maintained by the caller. Each call to
- * {@link IndexDocValues#load()} will cause an entire reload of
- * the underlying data. Source instances obtained from
- * {@link IndexDocValues#getSource()} and {@link IndexDocValues#getSource()}
- * respectively are maintained by a {@link SourceCache} that is closed (
- * {@link #close(IndexDocValues)}) once the {@link IndexReader} that created the
- * {@link IndexDocValues} instance is closed.
- * <p>
- * Unless {@link Source} instances are managed by another entity it is
- * recommended to use the cached variants to obtain a source instance.
- * <p>
- * Implementation of this API must be thread-safe.
- * 
- * @see IndexDocValues#setCache(SourceCache)
- * @see IndexDocValues#getSource()
- * 
- * @lucene.experimental
- */
-public abstract class SourceCache {
-
-  /**
-   * Atomically loads a {@link Source} into the cache from the given
-   * {@link IndexDocValues} and returns it iff no other {@link Source} has already
-   * been cached. Otherwise the cached source is returned.
-   * <p>
-   * This method will not return <code>null</code>
-   */
-  public abstract Source load(IndexDocValues values) throws IOException;
-
-  /**
-   * Atomically invalidates the cached {@link Source} 
-   * instances if any and empties the cache.
-   */
-  public abstract void invalidate(IndexDocValues values);
-
-  /**
-   * Atomically closes the cache and frees all resources.
-   */
-  public synchronized void close(IndexDocValues values) {
-    invalidate(values);
-  }
-
-  /**
-   * Simple per {@link IndexDocValues} instance cache implementation that holds a
-   * {@link Source} a member variable.
-   * <p>
-   * If a {@link DirectSourceCache} instance is closed or invalidated the cached
-   * reference are simply set to <code>null</code>
-   */
-  public static final class DirectSourceCache extends SourceCache {
-    private Source ref;
-
-    public synchronized Source load(IndexDocValues values) throws IOException {
-      if (ref == null) {
-        ref = values.load();
-      }
-      return ref;
-    }
-
-    public synchronized void invalidate(IndexDocValues values) {
-      ref = null;
-    }
-  }
-
-}


diff -ruN -x .svn -x build lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/values/TypePromoter.java lucene-3622/lucene/src/java/org/apache/lucene/index/values/TypePromoter.java
--- lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/values/TypePromoter.java	2011-12-06 18:45:03.860810978 -0500
+++ lucene-3622/lucene/src/java/org/apache/lucene/index/values/TypePromoter.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,204 +0,0 @@
-package org.apache.lucene.index.values;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-import java.util.HashMap;
-import java.util.Map;
-
-/**
- * Type promoter that promotes {@link IndexDocValues} during merge based on
- * their {@link ValueType} and {@link #getValueSize()}
- * 
- * @lucene.internal
- */
-public class TypePromoter {
-
-  private final static Map<Integer, ValueType> FLAGS_MAP = new HashMap<Integer, ValueType>();
-  private static final TypePromoter IDENTITY_PROMOTER = new IdentityTypePromoter();
-  public static final int VAR_TYPE_VALUE_SIZE = -1;
-
-  private static final int IS_INT = 1 << 0;
-  private static final int IS_BYTE = 1 << 1;
-  private static final int IS_FLOAT = 1 << 2;
-  /* VAR & FIXED == VAR */
-  private static final int IS_VAR = 1 << 3;
-  private static final int IS_FIXED = 1 << 3 | 1 << 4;
-  /* if we have FIXED & FIXED with different size we promote to VAR */
-  private static final int PROMOTE_TO_VAR_SIZE_MASK = ~(1 << 3);
-  /* STRAIGHT & DEREF == STRAIGHT (dense values win) */
-  private static final int IS_STRAIGHT = 1 << 5;
-  private static final int IS_DEREF = 1 << 5 | 1 << 6;
-  private static final int IS_SORTED = 1 << 7;
-  /* more bits wins (int16 & int32 == int32) */
-  private static final int IS_8_BIT = 1 << 8 | 1 << 9 | 1 << 10 | 1 << 11;
-  private static final int IS_16_BIT = 1 << 9 | 1 << 10 | 1 << 11;
-  private static final int IS_32_BIT = 1 << 10 | 1 << 11;
-  private static final int IS_64_BIT = 1 << 11;
-
-  private final ValueType type;
-  private final int flags;
-  private final int valueSize;
-
-  /**
-   * Returns a positive value size if this {@link TypePromoter} represents a
-   * fixed variant, otherwise <code>-1</code>
-   * 
-   * @return a positive value size if this {@link TypePromoter} represents a
-   *         fixed variant, otherwise <code>-1</code>
-   */
-  public int getValueSize() {
-    return valueSize;
-  }
-
-  static {
-    for (ValueType type : ValueType.values()) {
-      TypePromoter create = create(type, VAR_TYPE_VALUE_SIZE);
-      FLAGS_MAP.put(create.flags, type);
-    }
-  }
-
-  /**
-   * Creates a new {@link TypePromoter}
-   * 
-   * @param type
-   *          the {@link ValueType} this promoter represents
-   * @param flags
-   *          the promoters flags
-   * @param valueSize
-   *          the value size if {@link #IS_FIXED} or <code>-1</code> otherwise.
-   */
-  protected TypePromoter(ValueType type, int flags, int valueSize) {
-    this.type = type;
-    this.flags = flags;
-    this.valueSize = valueSize;
-  }
-
-  /**
-   * Creates a new promoted {@link TypePromoter} based on this and the given
-   * {@link TypePromoter} or <code>null</code> iff the {@link TypePromoter} 
-   * aren't compatible.
-   * 
-   * @param promoter
-   *          the incoming promoter
-   * @return a new promoted {@link TypePromoter} based on this and the given
-   *         {@link TypePromoter} or <code>null</code> iff the
-   *         {@link TypePromoter} aren't compatible.
-   */
-  public TypePromoter promote(TypePromoter promoter) {
-
-    int promotedFlags = promoter.flags & this.flags;
-    TypePromoter promoted = create(FLAGS_MAP.get(promotedFlags), valueSize);
-    if (promoted == null) {
-      return promoted;
-    }
-    if ((promoted.flags & IS_BYTE) != 0 && (promoted.flags & IS_FIXED) == IS_FIXED) {
-      if (this.valueSize == promoter.valueSize) {
-        return promoted;
-      }
-      return create(FLAGS_MAP.get(promoted.flags & PROMOTE_TO_VAR_SIZE_MASK),
-          VAR_TYPE_VALUE_SIZE);
-    }
-    return promoted;
-
-  }
-
-  /**
-   * Returns the {@link ValueType} of this {@link TypePromoter}
-   * 
-   * @return the {@link ValueType} of this {@link TypePromoter}
-   */
-  public ValueType type() {
-    return type;
-  }
-
-  @Override
-  public String toString() {
-    return "TypePromoter [type=" + type + ", sizeInBytes=" + valueSize + "]";
-  }
-
-  /**
-   * Creates a new {@link TypePromoter} for the given type and size per value.
-   * 
-   * @param type
-   *          the {@link ValueType} to create the promoter for
-   * @param valueSize
-   *          the size per value in bytes or <code>-1</code> iff the types have
-   *          variable length.
-   * @return a new {@link TypePromoter}
-   */
-  public static TypePromoter create(ValueType type, int valueSize) {
-    if (type == null) {
-      return null;
-    }
-    switch (type) {
-    case BYTES_FIXED_DEREF:
-      return new TypePromoter(type, IS_BYTE | IS_FIXED | IS_DEREF, valueSize);
-    case BYTES_FIXED_SORTED:
-      return new TypePromoter(type, IS_BYTE | IS_FIXED | IS_SORTED, valueSize);
-    case BYTES_FIXED_STRAIGHT:
-      return new TypePromoter(type, IS_BYTE | IS_FIXED | IS_STRAIGHT, valueSize);
-    case BYTES_VAR_DEREF:
-      return new TypePromoter(type, IS_BYTE | IS_VAR | IS_DEREF, VAR_TYPE_VALUE_SIZE);
-    case BYTES_VAR_SORTED:
-      return new TypePromoter(type, IS_BYTE | IS_VAR | IS_SORTED, VAR_TYPE_VALUE_SIZE);
-    case BYTES_VAR_STRAIGHT:
-      return new TypePromoter(type, IS_BYTE | IS_VAR | IS_STRAIGHT, VAR_TYPE_VALUE_SIZE);
-    case FIXED_INTS_16:
-      return new TypePromoter(type,
-          IS_INT | IS_FIXED | IS_STRAIGHT | IS_16_BIT, valueSize);
-    case FIXED_INTS_32:
-      return new TypePromoter(type,
-          IS_INT | IS_FIXED | IS_STRAIGHT | IS_32_BIT, valueSize);
-    case FIXED_INTS_64:
-      return new TypePromoter(type,
-          IS_INT | IS_FIXED | IS_STRAIGHT | IS_64_BIT, valueSize);
-    case FIXED_INTS_8:
-      return new TypePromoter(type, IS_INT | IS_FIXED | IS_STRAIGHT | IS_8_BIT,
-          valueSize);
-    case FLOAT_32:
-      return new TypePromoter(type, IS_FLOAT | IS_FIXED | IS_STRAIGHT
-          | IS_32_BIT, valueSize);
-    case FLOAT_64:
-      return new TypePromoter(type, IS_FLOAT | IS_FIXED | IS_STRAIGHT
-          | IS_64_BIT, valueSize);
-    case VAR_INTS:
-      return new TypePromoter(type, IS_INT | IS_VAR | IS_STRAIGHT, VAR_TYPE_VALUE_SIZE);
-    default:
-      throw new IllegalStateException();
-    }
-  }
-
-  /**
-   * Returns a {@link TypePromoter} that always promotes to the type provided to
-   * {@link #promote(TypePromoter)}
-   */
-  public static TypePromoter getIdentityPromoter() {
-    return IDENTITY_PROMOTER;
-  }
-
-  private static class IdentityTypePromoter extends TypePromoter {
-
-    public IdentityTypePromoter() {
-      super(null, 0, -1);
-    }
-
-    @Override
-    public TypePromoter promote(TypePromoter promoter) {
-      return promoter;
-    }
-  }
-}
\ No newline at end of file


diff -ruN -x .svn -x build lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/values/ValueType.java lucene-3622/lucene/src/java/org/apache/lucene/index/values/ValueType.java
--- lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/values/ValueType.java	2011-12-06 18:45:03.860810978 -0500
+++ lucene-3622/lucene/src/java/org/apache/lucene/index/values/ValueType.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,220 +0,0 @@
-package org.apache.lucene.index.values;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.index.codecs.DocValuesFormat;
-import org.apache.lucene.index.values.IndexDocValues.SortedSource;
-import org.apache.lucene.index.values.IndexDocValues.Source;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.packed.PackedInts;
-
-/**
- * <code>ValueType</code> specifies the {@link IndexDocValues} type for a
- * certain field. A <code>ValueType</code> only defines the data type for a field
- * while the actual implementation used to encode and decode the values depends
- * on the the {@link DocValuesFormat#docsConsumer} and {@link DocValuesFormat#docsProducer} methods.
- * 
- * @lucene.experimental
- */
-public enum ValueType {
-
-  /**
-   * A variable bit signed integer value. By default this type uses
-   * {@link PackedInts} to compress the values, as an offset
-   * from the minimum value, as long as the value range
-   * fits into 2<sup>63</sup>-1. Otherwise,
-   * the default implementation falls back to fixed size 64bit
-   * integers ({@link #FIXED_INTS_64}).
-   * <p>
-   * NOTE: this type uses <tt>0</tt> as the default value without any
-   * distinction between provided <tt>0</tt> values during indexing. All
-   * documents without an explicit value will use <tt>0</tt> instead.
-   * Custom default values must be assigned explicitly.
-   * </p>
-   */
-  VAR_INTS,
-  
-  /**
-   * A 8 bit signed integer value. {@link Source} instances of
-   * this type return a <tt>byte</tt> array from {@link Source#getArray()}
-   * <p>
-   * NOTE: this type uses <tt>0</tt> as the default value without any
-   * distinction between provided <tt>0</tt> values during indexing. All
-   * documents without an explicit value will use <tt>0</tt> instead.
-   * Custom default values must be assigned explicitly.
-   * </p>
-   */
-  FIXED_INTS_8,
-  
-  /**
-   * A 16 bit signed integer value. {@link Source} instances of
-   * this type return a <tt>short</tt> array from {@link Source#getArray()}
-   * <p>
-   * NOTE: this type uses <tt>0</tt> as the default value without any
-   * distinction between provided <tt>0</tt> values during indexing. All
-   * documents without an explicit value will use <tt>0</tt> instead.
-   * Custom default values must be assigned explicitly.
-   * </p>
-   */
-  FIXED_INTS_16,
-  
-  /**
-   * A 32 bit signed integer value. {@link Source} instances of
-   * this type return a <tt>int</tt> array from {@link Source#getArray()}
-   * <p>
-   * NOTE: this type uses <tt>0</tt> as the default value without any
-   * distinction between provided <tt>0</tt> values during indexing. All
-   * documents without an explicit value will use <tt>0</tt> instead. 
-   * Custom default values must be assigned explicitly.
-   * </p>
-   */
-  FIXED_INTS_32,
-
-  /**
-   * A 64 bit signed integer value. {@link Source} instances of
-   * this type return a <tt>long</tt> array from {@link Source#getArray()}
-   * <p>
-   * NOTE: this type uses <tt>0</tt> as the default value without any
-   * distinction between provided <tt>0</tt> values during indexing. All
-   * documents without an explicit value will use <tt>0</tt> instead.
-   * Custom default values must be assigned explicitly.
-   * </p>
-   */
-  FIXED_INTS_64,
-  /**
-   * A 32 bit floating point value. By default there is no compression
-   * applied. To fit custom float values into less than 32bit either a custom
-   * implementation is needed or values must be encoded into a
-   * {@link #BYTES_FIXED_STRAIGHT} type. {@link Source} instances of
-   * this type return a <tt>float</tt> array from {@link Source#getArray()}
-   * <p>
-   * NOTE: this type uses <tt>0.0f</tt> as the default value without any
-   * distinction between provided <tt>0.0f</tt> values during indexing. All
-   * documents without an explicit value will use <tt>0.0f</tt> instead.
-   * Custom default values must be assigned explicitly.
-   * </p>
-   */
-  FLOAT_32,
-  /**
-   * 
-   * A 64 bit floating point value. By default there is no compression
-   * applied. To fit custom float values into less than 64bit either a custom
-   * implementation is needed or values must be encoded into a
-   * {@link #BYTES_FIXED_STRAIGHT} type. {@link Source} instances of
-   * this type return a <tt>double</tt> array from {@link Source#getArray()}
-   * <p>
-   * NOTE: this type uses <tt>0.0d</tt> as the default value without any
-   * distinction between provided <tt>0.0d</tt> values during indexing. All
-   * documents without an explicit value will use <tt>0.0d</tt> instead.
-   * Custom default values must be assigned explicitly.
-   * </p>
-   */
-  FLOAT_64,
-
-  // TODO(simonw): -- shouldn't lucene decide/detect straight vs
-  // deref, as well fixed vs var?
-  /**
-   * A fixed length straight byte[]. All values added to
-   * such a field must be of the same length. All bytes are stored sequentially
-   * for fast offset access.
-   * <p>
-   * NOTE: this type uses <tt>0 byte</tt> filled byte[] based on the length of the first seen
-   * value as the default value without any distinction between explicitly
-   * provided values during indexing. All documents without an explicit value
-   * will use the default instead.Custom default values must be assigned explicitly.
-   * </p>
-   */
-  BYTES_FIXED_STRAIGHT,
-
-  /**
-   * A fixed length dereferenced byte[] variant. Fields with
-   * this type only store distinct byte values and store an additional offset
-   * pointer per document to dereference the shared byte[].
-   * Use this type if your documents may share the same byte[].
-   * <p>
-   * NOTE: Fields of this type will not store values for documents without and
-   * explicitly provided value. If a documents value is accessed while no
-   * explicit value is stored the returned {@link BytesRef} will be a 0-length
-   * reference. Custom default values must be assigned explicitly.
-   * </p>
-   */
-  BYTES_FIXED_DEREF,
-
-  /**
-   * Variable length straight stored byte[] variant. All bytes are
-   * stored sequentially for compactness. Usage of this type via the
-   * disk-resident API might yield performance degradation since no additional
-   * index is used to advance by more than one document value at a time.
-   * <p>
-   * NOTE: Fields of this type will not store values for documents without an
-   * explicitly provided value. If a documents value is accessed while no
-   * explicit value is stored the returned {@link BytesRef} will be a 0-length
-   * byte[] reference. Custom default values must be assigned explicitly.
-   * </p>
-   */
-  BYTES_VAR_STRAIGHT,
-
-  /**
-   * A variable length dereferenced byte[]. Just like
-   * {@link #BYTES_FIXED_DEREF}, but allowing each
-   * document's value to be a different length.
-   * <p>
-   * NOTE: Fields of this type will not store values for documents without and
-   * explicitly provided value. If a documents value is accessed while no
-   * explicit value is stored the returned {@link BytesRef} will be a 0-length
-   * reference. Custom default values must be assigned explicitly.
-   * </p>
-   */
-  BYTES_VAR_DEREF,
-
-
-  /**
-   * A variable length pre-sorted byte[] variant. Just like
-   * {@link #BYTES_FIXED_SORTED}, but allowing each
-   * document's value to be a different length.
-   * <p>
-   * NOTE: Fields of this type will not store values for documents without and
-   * explicitly provided value. If a documents value is accessed while no
-   * explicit value is stored the returned {@link BytesRef} will be a 0-length
-   * reference.Custom default values must be assigned explicitly.
-   * </p>
-   * 
-   * @see SortedSource
-   */
-  BYTES_VAR_SORTED,
-  
-  /**
-   * A fixed length pre-sorted byte[] variant. Fields with this type only
-   * store distinct byte values and store an additional offset pointer per
-   * document to dereference the shared byte[]. The stored
-   * byte[] is presorted, by default by unsigned byte order,
-   * and allows access via document id, ordinal and by-value.
-   * Use this type if your documents may share the same byte[].
-   * <p>
-   * NOTE: Fields of this type will not store values for documents without and
-   * explicitly provided value. If a documents value is accessed while no
-   * explicit value is stored the returned {@link BytesRef} will be a 0-length
-   * reference. Custom default values must be assigned
-   * explicitly.
-   * </p>
-   * 
-   * @see SortedSource
-   */
-  BYTES_FIXED_SORTED
-  
-}


diff -ruN -x .svn -x build lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/values/VarDerefBytesImpl.java lucene-3622/lucene/src/java/org/apache/lucene/index/values/VarDerefBytesImpl.java
--- lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/values/VarDerefBytesImpl.java	2011-12-06 18:45:03.864810979 -0500
+++ lucene-3622/lucene/src/java/org/apache/lucene/index/values/VarDerefBytesImpl.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,150 +0,0 @@
-package org.apache.lucene.index.values;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.index.values.Bytes.BytesReaderBase;
-import org.apache.lucene.index.values.Bytes.BytesSourceBase;
-import org.apache.lucene.index.values.Bytes.DerefBytesWriterBase;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.Counter;
-import org.apache.lucene.util.PagedBytes;
-import org.apache.lucene.util.packed.PackedInts;
-
-// Stores variable-length byte[] by deref, ie when two docs
-// have the same value, they store only 1 byte[] and both
-// docs reference that single source
-
-/**
- * @lucene.experimental
- */
-class VarDerefBytesImpl {
-
-  static final String CODEC_NAME = "VarDerefBytes";
-  static final int VERSION_START = 0;
-  static final int VERSION_CURRENT = VERSION_START;
-
-  /*
-   * TODO: if impls like this are merged we are bound to the amount of memory we
-   * can store into a BytesRefHash and therefore how much memory a ByteBlockPool
-   * can address. This is currently limited to 2GB. While we could extend that
-   * and use 64bit for addressing this still limits us to the existing main
-   * memory as all distinct bytes will be loaded up into main memory. We could
-   * move the byte[] writing to #finish(int) and store the bytes in sorted
-   * order and merge them in a streamed fashion. 
-   */
-  static class Writer extends DerefBytesWriterBase {
-    public Writer(Directory dir, String id, Counter bytesUsed, IOContext context)
-        throws IOException {
-      super(dir, id, CODEC_NAME, VERSION_CURRENT, bytesUsed, context);
-      size = 0;
-    }
-    
-    @Override
-    protected void checkSize(BytesRef bytes) {
-      // allow var bytes sizes
-    }
-
-    // Important that we get docCount, in case there were
-    // some last docs that we didn't see
-    @Override
-    public void finishInternal(int docCount) throws IOException {
-      fillDefault(docCount);
-      final int size = hash.size();
-      final long[] addresses = new long[size];
-      final IndexOutput datOut = getOrCreateDataOut();
-      int addr = 0;
-      final BytesRef bytesRef = new BytesRef();
-      for (int i = 0; i < size; i++) {
-        hash.get(i, bytesRef);
-        addresses[i] = addr;
-        addr += writePrefixLength(datOut, bytesRef) + bytesRef.length;
-        datOut.writeBytes(bytesRef.bytes, bytesRef.offset, bytesRef.length);
-      }
-
-      final IndexOutput idxOut = getOrCreateIndexOut();
-      // write the max address to read directly on source load
-      idxOut.writeLong(addr);
-      writeIndex(idxOut, docCount, addresses[addresses.length-1], addresses, docToEntry);
-    }
-  }
-
-  public static class VarDerefReader extends BytesReaderBase {
-    private final long totalBytes;
-    VarDerefReader(Directory dir, String id, int maxDoc, IOContext context) throws IOException {
-      super(dir, id, CODEC_NAME, VERSION_START, true, context, ValueType.BYTES_VAR_DEREF);
-      totalBytes = idxIn.readLong();
-    }
-
-    @Override
-    public Source load() throws IOException {
-      return new VarDerefSource(cloneData(), cloneIndex(), totalBytes);
-    }
-   
-    @Override
-    public Source getDirectSource()
-        throws IOException {
-      return new DirectVarDerefSource(cloneData(), cloneIndex(), type());
-    }
-  }
-  
-  final static class VarDerefSource extends BytesSourceBase {
-    private final PackedInts.Reader addresses;
-
-    public VarDerefSource(IndexInput datIn, IndexInput idxIn, long totalBytes)
-        throws IOException {
-      super(datIn, idxIn, new PagedBytes(PAGED_BYTES_BITS), totalBytes,
-          ValueType.BYTES_VAR_DEREF);
-      addresses = PackedInts.getReader(idxIn);
-    }
-
-    @Override
-    public BytesRef getBytes(int docID, BytesRef bytesRef) {
-      return data.fillSliceWithPrefix(bytesRef,
-          addresses.get(docID));
-    }
-  }
-
-  
-  final static class DirectVarDerefSource extends DirectSource {
-    private final PackedInts.Reader index;
-
-    DirectVarDerefSource(IndexInput data, IndexInput index, ValueType type)
-        throws IOException {
-      super(data, type);
-      this.index = PackedInts.getDirectReader(index);
-    }
-    
-    @Override
-    protected int position(int docID) throws IOException {
-      data.seek(baseOffset + index.get(docID));
-      final byte sizeByte = data.readByte();
-      if ((sizeByte & 128) == 0) {
-        // length is 1 byte
-        return sizeByte;
-      } else {
-        return ((sizeByte & 0x7f) << 8) | ((data.readByte() & 0xff));
-      }
-    }
-  }
-}


diff -ruN -x .svn -x build lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/values/VarSortedBytesImpl.java lucene-3622/lucene/src/java/org/apache/lucene/index/values/VarSortedBytesImpl.java
--- lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/values/VarSortedBytesImpl.java	2011-12-06 18:45:03.860810978 -0500
+++ lucene-3622/lucene/src/java/org/apache/lucene/index/values/VarSortedBytesImpl.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,246 +0,0 @@
-package org.apache.lucene.index.values;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Comparator;
-import java.util.List;
-
-import org.apache.lucene.index.MergeState;
-import org.apache.lucene.index.values.Bytes.BytesSortedSourceBase;
-import org.apache.lucene.index.values.Bytes.BytesReaderBase;
-import org.apache.lucene.index.values.Bytes.DerefBytesWriterBase;
-import org.apache.lucene.index.values.IndexDocValues.SortedSource;
-import org.apache.lucene.index.values.SortedBytesMergeUtils.MergeContext;
-import org.apache.lucene.index.values.SortedBytesMergeUtils.SortedSourceSlice;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.Counter;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.packed.PackedInts;
-
-// Stores variable-length byte[] by deref, ie when two docs
-// have the same value, they store only 1 byte[] and both
-// docs reference that single source
-
-/**
- * @lucene.experimental
- */
-final class VarSortedBytesImpl {
-
-  static final String CODEC_NAME = "VarDerefBytes";
-  static final int VERSION_START = 0;
-  static final int VERSION_CURRENT = VERSION_START;
-
-  final static class Writer extends DerefBytesWriterBase {
-    private final Comparator<BytesRef> comp;
-
-    public Writer(Directory dir, String id, Comparator<BytesRef> comp,
-        Counter bytesUsed, IOContext context) throws IOException {
-      super(dir, id, CODEC_NAME, VERSION_CURRENT, bytesUsed, context);
-      this.comp = comp;
-      size = 0;
-    }
-    @Override
-    public void merge(MergeState mergeState, IndexDocValues[] docValues)
-        throws IOException {
-      boolean success = false;
-      try {
-        MergeContext ctx = SortedBytesMergeUtils.init(ValueType.BYTES_VAR_SORTED, docValues, comp, mergeState);
-        final List<SortedSourceSlice> slices = SortedBytesMergeUtils.buildSlices(mergeState, docValues, ctx);
-        IndexOutput datOut = getOrCreateDataOut();
-        
-        ctx.offsets = new long[1];
-        final int maxOrd = SortedBytesMergeUtils.mergeRecords(ctx, datOut, slices);
-        final long[] offsets = ctx.offsets;
-        maxBytes = offsets[maxOrd-1];
-        final IndexOutput idxOut = getOrCreateIndexOut();
-        
-        idxOut.writeLong(maxBytes);
-        final PackedInts.Writer offsetWriter = PackedInts.getWriter(idxOut, maxOrd+1,
-            PackedInts.bitsRequired(maxBytes));
-        offsetWriter.add(0);
-        for (int i = 0; i < maxOrd; i++) {
-          offsetWriter.add(offsets[i]);
-        }
-        offsetWriter.finish();
-        
-        final PackedInts.Writer ordsWriter = PackedInts.getWriter(idxOut, ctx.docToEntry.length,
-            PackedInts.bitsRequired(maxOrd-1));
-        for (SortedSourceSlice slice : slices) {
-          slice.writeOrds(ordsWriter);
-        }
-        ordsWriter.finish();
-        success = true;
-      } finally {
-        releaseResources();
-        if (success) {
-          IOUtils.close(getIndexOut(), getDataOut());
-        } else {
-          IOUtils.closeWhileHandlingException(getIndexOut(), getDataOut());
-        }
-
-      }
-    }
-
-    @Override
-    protected void checkSize(BytesRef bytes) {
-      // allow var bytes sizes
-    }
-
-    // Important that we get docCount, in case there were
-    // some last docs that we didn't see
-    @Override
-    public void finishInternal(int docCount) throws IOException {
-      fillDefault(docCount);
-      final int count = hash.size();
-      final IndexOutput datOut = getOrCreateDataOut();
-      final IndexOutput idxOut = getOrCreateIndexOut();
-      long offset = 0;
-      final int[] index = new int[count];
-      final int[] sortedEntries = hash.sort(comp);
-      // total bytes of data
-      idxOut.writeLong(maxBytes);
-      PackedInts.Writer offsetWriter = PackedInts.getWriter(idxOut, count+1,
-          PackedInts.bitsRequired(maxBytes));
-      // first dump bytes data, recording index & write offset as
-      // we go
-      final BytesRef spare = new BytesRef();
-      for (int i = 0; i < count; i++) {
-        final int e = sortedEntries[i];
-        offsetWriter.add(offset);
-        index[e] = i;
-        final BytesRef bytes = hash.get(e, spare);
-        // TODO: we could prefix code...
-        datOut.writeBytes(bytes.bytes, bytes.offset, bytes.length);
-        offset += bytes.length;
-      }
-      // write sentinel
-      offsetWriter.add(offset);
-      offsetWriter.finish();
-      // write index
-      writeIndex(idxOut, docCount, count, index, docToEntry);
-
-    }
-  }
-
-  public static class Reader extends BytesReaderBase {
-
-    private final Comparator<BytesRef> comparator;
-
-    Reader(Directory dir, String id, int maxDoc,
-        IOContext context, ValueType type, Comparator<BytesRef> comparator)
-        throws IOException {
-      super(dir, id, CODEC_NAME, VERSION_START, true, context, type);
-      this.comparator = comparator;
-    }
-
-    @Override
-    public org.apache.lucene.index.values.IndexDocValues.Source load()
-        throws IOException {
-      return new VarSortedSource(cloneData(), cloneIndex(), comparator);
-    }
-
-    @Override
-    public Source getDirectSource() throws IOException {
-      return new DirectSortedSource(cloneData(), cloneIndex(), comparator, type());
-    }
-    
-  }
-  private static final class VarSortedSource extends BytesSortedSourceBase {
-    private final int valueCount;
-
-    VarSortedSource(IndexInput datIn, IndexInput idxIn,
-        Comparator<BytesRef> comp) throws IOException {
-      super(datIn, idxIn, comp, idxIn.readLong(), ValueType.BYTES_VAR_SORTED, true);
-      valueCount = ordToOffsetIndex.size()-1; // the last value here is just a dummy value to get the length of the last value
-      closeIndexInput();
-    }
-
-    @Override
-    public BytesRef getByOrd(int ord, BytesRef bytesRef) {
-      final long offset = ordToOffsetIndex.get(ord);
-      final long nextOffset = ordToOffsetIndex.get(1 + ord);
-      data.fillSlice(bytesRef, offset, (int) (nextOffset - offset));
-      return bytesRef;
-    }
-
-    @Override
-    public int getValueCount() {
-      return valueCount;
-    }
-  }
-
-  private static final class DirectSortedSource extends SortedSource {
-    private final PackedInts.Reader docToOrdIndex;
-    private final PackedInts.Reader ordToOffsetIndex;
-    private final IndexInput datIn;
-    private final long basePointer;
-    private final int valueCount;
-    
-    DirectSortedSource(IndexInput datIn, IndexInput idxIn,
-        Comparator<BytesRef> comparator, ValueType type) throws IOException {
-      super(type, comparator);
-      idxIn.readLong();
-      ordToOffsetIndex = PackedInts.getDirectReader(idxIn);
-      valueCount = ordToOffsetIndex.size()-1; // the last value here is just a dummy value to get the length of the last value
-      // advance this iterator to the end and clone the stream once it points to the docToOrdIndex header
-      ordToOffsetIndex.get(valueCount);
-      docToOrdIndex = PackedInts.getDirectReader((IndexInput) idxIn.clone()); // read the ords in to prevent too many random disk seeks
-      basePointer = datIn.getFilePointer();
-      this.datIn = datIn;
-    }
-
-    @Override
-    public int ord(int docID) {
-      return (int) docToOrdIndex.get(docID);
-    }
-
-    @Override
-    public PackedInts.Reader getDocToOrd() {
-      return docToOrdIndex;
-    }
-
-    @Override
-    public BytesRef getByOrd(int ord, BytesRef bytesRef) {
-      try {
-        final long offset = ordToOffsetIndex.get(ord);
-        // 1+ord is safe because we write a sentinel at the end
-        final long nextOffset = ordToOffsetIndex.get(1+ord);
-        datIn.seek(basePointer + offset);
-        final int length = (int) (nextOffset - offset);
-        bytesRef.grow(length);
-        datIn.readBytes(bytesRef.bytes, 0, length);
-        bytesRef.length = length;
-        bytesRef.offset = 0;
-        return bytesRef;
-      } catch (IOException ex) {
-        throw new IllegalStateException("failed", ex);
-      }
-    }
-    
-    @Override
-    public int getValueCount() {
-      return valueCount;
-    }
-
-  }
-}


diff -ruN -x .svn -x build lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/values/VarStraightBytesImpl.java lucene-3622/lucene/src/java/org/apache/lucene/index/values/VarStraightBytesImpl.java
--- lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/values/VarStraightBytesImpl.java	2011-12-06 18:45:03.864810979 -0500
+++ lucene-3622/lucene/src/java/org/apache/lucene/index/values/VarStraightBytesImpl.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,284 +0,0 @@
-package org.apache.lucene.index.values;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.index.values.Bytes.BytesReaderBase;
-import org.apache.lucene.index.values.Bytes.BytesSourceBase;
-import org.apache.lucene.index.values.Bytes.BytesWriterBase;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.util.ArrayUtil;
-import org.apache.lucene.util.ByteBlockPool;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.Counter;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.PagedBytes;
-import org.apache.lucene.util.RamUsageEstimator;
-import org.apache.lucene.util.ByteBlockPool.DirectTrackingAllocator;
-import org.apache.lucene.util.packed.PackedInts;
-import org.apache.lucene.util.packed.PackedInts.ReaderIterator;
-
-// Variable length byte[] per document, no sharing
-
-/**
- * @lucene.experimental
- */
-class VarStraightBytesImpl {
-
-  static final String CODEC_NAME = "VarStraightBytes";
-  static final int VERSION_START = 0;
-  static final int VERSION_CURRENT = VERSION_START;
-
-  static class Writer extends BytesWriterBase {
-    private long address;
-    // start at -1 if the first added value is > 0
-    private int lastDocID = -1;
-    private long[] docToAddress;
-    private final ByteBlockPool pool;
-    private IndexOutput datOut;
-    private boolean merge = false;
-    public Writer(Directory dir, String id, Counter bytesUsed, IOContext context)
-        throws IOException {
-      super(dir, id, CODEC_NAME, VERSION_CURRENT, bytesUsed, context);
-      pool = new ByteBlockPool(new DirectTrackingAllocator(bytesUsed));
-      docToAddress = new long[1];
-      pool.nextBuffer(); // init
-      bytesUsed.addAndGet(RamUsageEstimator.NUM_BYTES_INT);
-    }
-
-    // Fills up to but not including this docID
-    private void fill(final int docID, final long nextAddress) {
-      if (docID >= docToAddress.length) {
-        int oldSize = docToAddress.length;
-        docToAddress = ArrayUtil.grow(docToAddress, 1 + docID);
-        bytesUsed.addAndGet((docToAddress.length - oldSize)
-            * RamUsageEstimator.NUM_BYTES_INT);
-      }
-      for (int i = lastDocID + 1; i < docID; i++) {
-        docToAddress[i] = nextAddress;
-      }
-    }
-
-    @Override
-    public void add(int docID, BytesRef bytes) throws IOException {
-      assert !merge;
-      if (bytes.length == 0) {
-        return; // default
-      }
-      fill(docID, address);
-      docToAddress[docID] = address;
-      pool.copy(bytes);
-      address += bytes.length;
-      lastDocID = docID;
-    }
-    
-    @Override
-    protected void merge(SingleSubMergeState state) throws IOException {
-      merge = true;
-      datOut = getOrCreateDataOut();
-      boolean success = false;
-      try {
-        if (state.liveDocs == null && state.reader instanceof VarStraightReader) {
-          // bulk merge since we don't have any deletes
-          VarStraightReader reader = (VarStraightReader) state.reader;
-          final int maxDocs = reader.maxDoc;
-          if (maxDocs == 0) {
-            return;
-          }
-          if (lastDocID+1 < state.docBase) {
-            fill(state.docBase, address);
-            lastDocID = state.docBase-1;
-          }
-          final long numDataBytes;
-          final IndexInput cloneIdx = reader.cloneIndex();
-          try {
-            numDataBytes = cloneIdx.readVLong();
-            final ReaderIterator iter = PackedInts.getReaderIterator(cloneIdx);
-            for (int i = 0; i < maxDocs; i++) {
-              long offset = iter.next();
-              ++lastDocID;
-              if (lastDocID >= docToAddress.length) {
-                int oldSize = docToAddress.length;
-                docToAddress = ArrayUtil.grow(docToAddress, 1 + lastDocID);
-                bytesUsed.addAndGet((docToAddress.length - oldSize)
-                    * RamUsageEstimator.NUM_BYTES_INT);
-              }
-              docToAddress[lastDocID] = address + offset;
-            }
-            address += numDataBytes; // this is the address after all addr pointers are updated
-            iter.close();
-          } finally {
-            IOUtils.close(cloneIdx);
-          }
-          final IndexInput cloneData = reader.cloneData();
-          try {
-            datOut.copyBytes(cloneData, numDataBytes);
-          } finally {
-            IOUtils.close(cloneData);  
-          }
-        } else {
-          super.merge(state);
-        }
-        success = true;
-      } finally {
-        if (!success) {
-          IOUtils.closeWhileHandlingException(datOut);
-        }
-      }
-    }
-    
-    @Override
-    protected void mergeDoc(int docID, int sourceDoc) throws IOException {
-      assert merge;
-      assert lastDocID < docID;
-      currentMergeSource.getBytes(sourceDoc, bytesRef);
-      if (bytesRef.length == 0) {
-        return; // default
-      }
-      fill(docID, address);
-      datOut.writeBytes(bytesRef.bytes, bytesRef.offset, bytesRef.length);
-      docToAddress[docID] = address;
-      address += bytesRef.length;
-      lastDocID = docID;
-    }
-    
-
-    @Override
-    public void finish(int docCount) throws IOException {
-      boolean success = false;
-      assert (!merge && datOut == null) || (merge && datOut != null); 
-      final IndexOutput datOut = getOrCreateDataOut();
-      try {
-        if (!merge) {
-          // header is already written in getDataOut()
-          pool.writePool(datOut);
-        }
-        success = true;
-      } finally {
-        if (success) {
-          IOUtils.close(datOut);
-        } else {
-          IOUtils.closeWhileHandlingException(datOut);
-        }
-        pool.dropBuffersAndReset();
-      }
-
-      success = false;
-      final IndexOutput idxOut = getOrCreateIndexOut();
-      try {
-        if (lastDocID == -1) {
-          idxOut.writeVLong(0);
-          final PackedInts.Writer w = PackedInts.getWriter(idxOut, docCount+1,
-              PackedInts.bitsRequired(0));
-          // docCount+1 so we write sentinel
-          for (int i = 0; i < docCount+1; i++) {
-            w.add(0);
-          }
-          w.finish();
-        } else {
-          fill(docCount, address);
-          idxOut.writeVLong(address);
-          final PackedInts.Writer w = PackedInts.getWriter(idxOut, docCount+1,
-              PackedInts.bitsRequired(address));
-          for (int i = 0; i < docCount; i++) {
-            w.add(docToAddress[i]);
-          }
-          // write sentinel
-          w.add(address);
-          w.finish();
-        }
-        success = true;
-      } finally {
-        bytesUsed.addAndGet(-(docToAddress.length)
-            * RamUsageEstimator.NUM_BYTES_INT);
-        docToAddress = null;
-        if (success) {
-          IOUtils.close(idxOut);
-        } else {
-          IOUtils.closeWhileHandlingException(idxOut);
-        }
-      }
-    }
-
-    public long ramBytesUsed() {
-      return bytesUsed.get();
-    }
-  }
-
-  public static class VarStraightReader extends BytesReaderBase {
-    private final int maxDoc;
-
-    VarStraightReader(Directory dir, String id, int maxDoc, IOContext context) throws IOException {
-      super(dir, id, CODEC_NAME, VERSION_START, true, context, ValueType.BYTES_VAR_STRAIGHT);
-      this.maxDoc = maxDoc;
-    }
-
-    @Override
-    public Source load() throws IOException {
-      return new VarStraightSource(cloneData(), cloneIndex());
-    }
-
-    @Override
-    public Source getDirectSource()
-        throws IOException {
-      return new DirectVarStraightSource(cloneData(), cloneIndex(), type());
-    }
-  }
-  
-  private static final class VarStraightSource extends BytesSourceBase {
-    private final PackedInts.Reader addresses;
-
-    public VarStraightSource(IndexInput datIn, IndexInput idxIn) throws IOException {
-      super(datIn, idxIn, new PagedBytes(PAGED_BYTES_BITS), idxIn.readVLong(),
-          ValueType.BYTES_VAR_STRAIGHT);
-      addresses = PackedInts.getReader(idxIn);
-    }
-
-    @Override
-    public BytesRef getBytes(int docID, BytesRef bytesRef) {
-      final long address = addresses.get(docID);
-      return data.fillSlice(bytesRef, address,
-          (int) (addresses.get(docID + 1) - address));
-    }
-  }
-  
-  public final static class DirectVarStraightSource extends DirectSource {
-
-    private final PackedInts.Reader index;
-
-    DirectVarStraightSource(IndexInput data, IndexInput index, ValueType type)
-        throws IOException {
-      super(data, type);
-      index.readVLong();
-      this.index = PackedInts.getDirectReader(index);
-    }
-
-    @Override
-    protected int position(int docID) throws IOException {
-      final long offset = index.get(docID);
-      data.seek(baseOffset + offset);
-      // Safe to do 1+docID because we write sentinel at the end:
-      final long nextOffset = index.get(1+docID);
-      return (int) (nextOffset - offset);
-    }
-  }
-}


diff -ruN -x .svn -x build lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/values/Writer.java lucene-3622/lucene/src/java/org/apache/lucene/index/values/Writer.java
--- lucene-clean-trunk/lucene/src/java/org/apache/lucene/index/values/Writer.java	2011-12-06 18:45:03.860810978 -0500
+++ lucene-3622/lucene/src/java/org/apache/lucene/index/values/Writer.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,216 +0,0 @@
-package org.apache.lucene.index.values;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-import java.io.IOException;
-import java.util.Comparator;
-
-import org.apache.lucene.index.codecs.DocValuesConsumer;
-import org.apache.lucene.index.values.IndexDocValues.Source;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.util.Bits;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.Counter;
-
-/**
- * Abstract API for per-document stored primitive values of type <tt>byte[]</tt>
- * , <tt>long</tt> or <tt>double</tt>. The API accepts a single value for each
- * document. The underlying storage mechanism, file formats, data-structures and
- * representations depend on the actual implementation.
- * <p>
- * Document IDs passed to this API must always be increasing unless stated
- * otherwise.
- * </p>
- * 
- * @lucene.experimental
- */
-public abstract class Writer extends DocValuesConsumer {
-  protected Source currentMergeSource;
-  /**
-   * Creates a new {@link Writer}.
-   * 
-   * @param bytesUsed
-   *          bytes-usage tracking reference used by implementation to track
-   *          internally allocated memory. All tracked bytes must be released
-   *          once {@link #finish(int)} has been called.
-   */
-  protected Writer(Counter bytesUsed) {
-    super(bytesUsed);
-  }
-
-  /**
-   * Filename extension for index files
-   */
-  public static final String INDEX_EXTENSION = "idx";
-  
-  /**
-   * Filename extension for data files.
-   */
-  public static final String DATA_EXTENSION = "dat";
-
-  /**
-   * Records the specified <tt>long</tt> value for the docID or throws an
-   * {@link UnsupportedOperationException} if this {@link Writer} doesn't record
-   * <tt>long</tt> values.
-   * 
-   * @throws UnsupportedOperationException
-   *           if this writer doesn't record <tt>long</tt> values
-   */
-  public void add(int docID, long value) throws IOException {
-    throw new UnsupportedOperationException();
-  }
-
-  /**
-   * Records the specified <tt>double</tt> value for the docID or throws an
-   * {@link UnsupportedOperationException} if this {@link Writer} doesn't record
-   * <tt>double</tt> values.
-   * 
-   * @throws UnsupportedOperationException
-   *           if this writer doesn't record <tt>double</tt> values
-   */
-  public void add(int docID, double value) throws IOException {
-    throw new UnsupportedOperationException();
-  }
-
-  /**
-   * Records the specified {@link BytesRef} value for the docID or throws an
-   * {@link UnsupportedOperationException} if this {@link Writer} doesn't record
-   * {@link BytesRef} values.
-   * 
-   * @throws UnsupportedOperationException
-   *           if this writer doesn't record {@link BytesRef} values
-   */
-  public void add(int docID, BytesRef value) throws IOException {
-    throw new UnsupportedOperationException();
-  }
-
-  /**
-   * Merges a document with the given <code>docID</code>. The methods
-   * implementation obtains the value for the <i>sourceDoc</i> id from the
-   * current {@link Source} set to <i>setNextMergeSource(Source)</i>.
-   * <p>
-   * This method is used during merging to provide implementation agnostic
-   * default merge implementation.
-   * </p>
-   * <p>
-   * All documents IDs between the given ID and the previously given ID or
-   * <tt>0</tt> if the method is call the first time are filled with default
-   * values depending on the {@link Writer} implementation. The given document
-   * ID must always be greater than the previous ID or <tt>0</tt> if called the
-   * first time.
-   */
-  protected abstract void mergeDoc(int docID, int sourceDoc) throws IOException;
-
-  /**
-   * Sets the next {@link Source} to consume values from on calls to
-   * {@link #mergeDoc(int, int)}
-   * 
-   * @param mergeSource
-   *          the next {@link Source}, this must not be null
-   */
-  protected void setNextMergeSource(Source mergeSource) {
-    currentMergeSource = mergeSource;
-  }
-
-  /**
-   * Finish writing and close any files and resources used by this Writer.
-   * 
-   * @param docCount
-   *          the total number of documents for this writer. This must be
-   *          greater that or equal to the largest document id passed to one of
-   *          the add methods after the {@link Writer} was created.
-   */
-  public abstract void finish(int docCount) throws IOException;
-
-  @Override
-  protected void merge(SingleSubMergeState state) throws IOException {
-    // This enables bulk copies in subclasses per MergeState, subclasses can
-    // simply override this and decide if they want to merge
-    // segments using this generic implementation or if a bulk merge is possible
-    // / feasible.
-    final Source source = state.reader.getDirectSource();
-    assert source != null;
-    setNextMergeSource(source); // set the current enum we are working on - the
-    // impl. will get the correct reference for the type
-    // it supports
-    int docID = state.docBase;
-    final Bits liveDocs = state.liveDocs;
-    final int docCount = state.docCount;
-    for (int i = 0; i < docCount; i++) {
-      if (liveDocs == null || liveDocs.get(i)) {
-        mergeDoc(docID++, i);
-      }
-    }
-    
-  }
-
-  /**
-   * Factory method to create a {@link Writer} instance for a given type. This
-   * method returns default implementations for each of the different types
-   * defined in the {@link ValueType} enumeration.
-   * 
-   * @param type
-   *          the {@link ValueType} to create the {@link Writer} for
-   * @param id
-   *          the file name id used to create files within the writer.
-   * @param directory
-   *          the {@link Directory} to create the files from.
-   * @param bytesUsed
-   *          a byte-usage tracking reference
-   * @return a new {@link Writer} instance for the given {@link ValueType}
-   * @throws IOException
-   */
-  public static Writer create(ValueType type, String id, Directory directory,
-      Comparator<BytesRef> comp, Counter bytesUsed, IOContext context) throws IOException {
-    if (comp == null) {
-      comp = BytesRef.getUTF8SortedAsUnicodeComparator();
-    }
-    switch (type) {
-    case FIXED_INTS_16:
-    case FIXED_INTS_32:
-    case FIXED_INTS_64:
-    case FIXED_INTS_8:
-    case VAR_INTS:
-      return Ints.getWriter(directory, id, bytesUsed, type, context);
-    case FLOAT_32:
-      return Floats.getWriter(directory, id, bytesUsed, context, type);
-    case FLOAT_64:
-      return Floats.getWriter(directory, id, bytesUsed, context, type);
-    case BYTES_FIXED_STRAIGHT:
-      return Bytes.getWriter(directory, id, Bytes.Mode.STRAIGHT, true, comp,
-          bytesUsed, context);
-    case BYTES_FIXED_DEREF:
-      return Bytes.getWriter(directory, id, Bytes.Mode.DEREF, true, comp,
-          bytesUsed, context);
-    case BYTES_FIXED_SORTED:
-      return Bytes.getWriter(directory, id, Bytes.Mode.SORTED, true, comp,
-          bytesUsed, context);
-    case BYTES_VAR_STRAIGHT:
-      return Bytes.getWriter(directory, id, Bytes.Mode.STRAIGHT, false, comp,
-          bytesUsed, context);
-    case BYTES_VAR_DEREF:
-      return Bytes.getWriter(directory, id, Bytes.Mode.DEREF, false, comp,
-          bytesUsed, context);
-    case BYTES_VAR_SORTED:
-      return Bytes.getWriter(directory, id, Bytes.Mode.SORTED, false, comp,
-          bytesUsed, context);
-    default:
-      throw new IllegalArgumentException("Unknown Values: " + type);
-    }
-  }
-}


diff -ruN -x .svn -x build lucene-clean-trunk/lucene/src/java/org/apache/lucene/search/FieldComparator.java lucene-3622/lucene/src/java/org/apache/lucene/search/FieldComparator.java
--- lucene-clean-trunk/lucene/src/java/org/apache/lucene/search/FieldComparator.java	2011-12-06 18:45:03.728810977 -0500
+++ lucene-3622/lucene/src/java/org/apache/lucene/search/FieldComparator.java	2011-12-09 11:10:18.668849648 -0500
@@ -21,11 +21,8 @@
 import java.util.Comparator;
 
 import org.apache.lucene.index.IndexReader.AtomicReaderContext;
+import org.apache.lucene.index.DocValues;
 import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.values.IndexDocValues.SortedSource;
-import org.apache.lucene.index.values.IndexDocValues.Source;
-import org.apache.lucene.index.values.IndexDocValues;
-import org.apache.lucene.index.values.ValueType;
 import org.apache.lucene.search.FieldCache.ByteParser;
 import org.apache.lucene.search.FieldCache.DocTerms;
 import org.apache.lucene.search.FieldCache.DocTermsIndex;
@@ -360,7 +357,7 @@
   public static final class FloatDocValuesComparator extends FieldComparator<Double> {
     private final double[] values;
     private final String field;
-    private Source currentReaderValues;
+    private DocValues.Source currentReaderValues;
     private double bottom;
 
     FloatDocValuesComparator(int numHits, String field) {
@@ -400,11 +397,11 @@
 
     @Override
     public FieldComparator setNextReader(AtomicReaderContext context) throws IOException {
-      final IndexDocValues docValues = context.reader.docValues(field);
+      final DocValues docValues = context.reader.docValues(field);
       if (docValues != null) {
         currentReaderValues = docValues.getSource(); 
       } else {
-        currentReaderValues = IndexDocValues.getDefaultSource(ValueType.FLOAT_64);
+        currentReaderValues = DocValues.getDefaultSource(DocValues.Type.FLOAT_64);
       }
       return this;
     }
@@ -648,7 +645,7 @@
   /** Loads int index values and sorts by ascending value. */
   public static final class IntDocValuesComparator extends FieldComparator<Long> {
     private final long[] values;
-    private Source currentReaderValues;
+    private DocValues.Source currentReaderValues;
     private final String field;
     private long bottom;
 
@@ -693,11 +690,11 @@
 
     @Override
     public FieldComparator setNextReader(AtomicReaderContext context) throws IOException {
-      IndexDocValues docValues = context.reader.docValues(field);
+      DocValues docValues = context.reader.docValues(field);
       if (docValues != null) {
         currentReaderValues = docValues.getSource();
       } else {
-        currentReaderValues = IndexDocValues.getDefaultSource(ValueType.FIXED_INTS_64);
+        currentReaderValues = DocValues.getDefaultSource(DocValues.Type.FIXED_INTS_64);
       }
       return this;
     }
@@ -1382,7 +1379,7 @@
 
     /* Current reader's doc ord/values.
        @lucene.internal */
-    SortedSource termsIndex;
+    DocValues.SortedSource termsIndex;
 
     /* Comparator for comparing by value.
        @lucene.internal */
@@ -1490,10 +1487,10 @@
     // Used per-segment when bit width of doc->ord is 8:
     private final class ByteOrdComparator extends PerSegmentComparator {
       private final byte[] readerOrds;
-      private final SortedSource termsIndex;
+      private final DocValues.SortedSource termsIndex;
       private final int docBase;
 
-      public ByteOrdComparator(byte[] readerOrds, SortedSource termsIndex, int docBase) {
+      public ByteOrdComparator(byte[] readerOrds, DocValues.SortedSource termsIndex, int docBase) {
         this.readerOrds = readerOrds;
         this.termsIndex = termsIndex;
         this.docBase = docBase;
@@ -1535,10 +1532,10 @@
     // Used per-segment when bit width of doc->ord is 16:
     private final class ShortOrdComparator extends PerSegmentComparator {
       private final short[] readerOrds;
-      private final SortedSource termsIndex;
+      private final DocValues.SortedSource termsIndex;
       private final int docBase;
 
-      public ShortOrdComparator(short[] readerOrds, SortedSource termsIndex, int docBase) {
+      public ShortOrdComparator(short[] readerOrds, DocValues.SortedSource termsIndex, int docBase) {
         this.readerOrds = readerOrds;
         this.termsIndex = termsIndex;
         this.docBase = docBase;
@@ -1580,10 +1577,10 @@
     // Used per-segment when bit width of doc->ord is 32:
     private final class IntOrdComparator extends PerSegmentComparator {
       private final int[] readerOrds;
-      private final SortedSource termsIndex;
+      private final DocValues.SortedSource termsIndex;
       private final int docBase;
 
-      public IntOrdComparator(int[] readerOrds, SortedSource termsIndex, int docBase) {
+      public IntOrdComparator(int[] readerOrds, DocValues.SortedSource termsIndex, int docBase) {
         this.readerOrds = readerOrds;
         this.termsIndex = termsIndex;
         this.docBase = docBase;
@@ -1623,11 +1620,11 @@
 
     // Used per-segment when bit width is not a native array
     // size (8, 16, 32):
-    private final class AnyOrdComparator extends PerSegmentComparator {
+    private final class AnyPackedDocToOrdComparator extends PerSegmentComparator {
       private final PackedInts.Reader readerOrds;
       private final int docBase;
 
-      public AnyOrdComparator(PackedInts.Reader readerOrds, int docBase) {
+      public AnyPackedDocToOrdComparator(PackedInts.Reader readerOrds, int docBase) {
         this.readerOrds = readerOrds;
         this.docBase = docBase;
       }
@@ -1664,16 +1661,57 @@
       }
     }
 
+    // Used per-segment when DV doesn't use packed ints for
+    // docToOrds:
+    private final class AnyOrdComparator extends PerSegmentComparator {
+      private final int docBase;
+
+      public AnyOrdComparator(int docBase) {
+        this.docBase = docBase;
+      }
+
+      @Override
+      public int compareBottom(int doc) {
+        assert bottomSlot != -1;
+        if (bottomSameReader) {
+          // ord is precisely comparable, even in the equal case
+          return bottomOrd - termsIndex.ord(doc);
+        } else {
+          // ord is only approx comparable: if they are not
+          // equal, we can use that; if they are equal, we
+          // must fallback to compare by value
+          final int order = termsIndex.ord(doc);
+          final int cmp = bottomOrd - order;
+          if (cmp != 0) {
+            return cmp;
+          }
+          termsIndex.getByOrd(order, tempBR);
+          return comp.compare(bottomValue, tempBR);
+        }
+      }
+
+      @Override
+      public void copy(int slot, int doc) {
+        final int ord = termsIndex.ord(doc);
+        ords[slot] = ord;
+        if (values[slot] == null) {
+          values[slot] = new BytesRef();
+        }
+        termsIndex.getByOrd(ord, values[slot]);
+        readerGen[slot] = currentReaderGen;
+      }
+    }
+
     @Override
     public FieldComparator setNextReader(AtomicReaderContext context) throws IOException {
       final int docBase = context.docBase;
 
-      final IndexDocValues dv = context.reader.docValues(field);
+      final DocValues dv = context.reader.docValues(field);
       if (dv == null) {
         // This may mean entire segment had no docs with
         // this DV field; use default field value (empty
         // byte[]) in this case:
-        termsIndex = IndexDocValues.getDefaultSortedSource(ValueType.BYTES_VAR_SORTED, context.reader.maxDoc());
+        termsIndex = DocValues.getDefaultSortedSource(DocValues.Type.BYTES_VAR_SORTED, context.reader.maxDoc());
       } else {
         termsIndex = dv.getSource().asSortedSource();
         if (termsIndex == null) {
@@ -1687,24 +1725,30 @@
       comp = termsIndex.getComparator();
 
       FieldComparator perSegComp = null;
-      final PackedInts.Reader docToOrd = termsIndex.getDocToOrd();
-      if (docToOrd.hasArray()) {
-        final Object arr = docToOrd.getArray();
-        assert arr != null;
-        if (arr instanceof byte[]) {
-          // 8 bit packed
-          perSegComp = new ByteOrdComparator((byte[]) arr, termsIndex, docBase);
-        } else if (arr instanceof short[]) {
-          // 16 bit packed
-          perSegComp = new ShortOrdComparator((short[]) arr, termsIndex, docBase);
-        } else if (arr instanceof int[]) {
-          // 32 bit packed
-          perSegComp = new IntOrdComparator((int[]) arr, termsIndex, docBase);
+      if (termsIndex.hasPackedDocToOrd()) {
+        final PackedInts.Reader docToOrd = termsIndex.getDocToOrd();
+        if (docToOrd.hasArray()) {
+          final Object arr = docToOrd.getArray();
+          assert arr != null;
+          if (arr instanceof byte[]) {
+            // 8 bit packed
+            perSegComp = new ByteOrdComparator((byte[]) arr, termsIndex, docBase);
+          } else if (arr instanceof short[]) {
+            // 16 bit packed
+            perSegComp = new ShortOrdComparator((short[]) arr, termsIndex, docBase);
+          } else if (arr instanceof int[]) {
+            // 32 bit packed
+            perSegComp = new IntOrdComparator((int[]) arr, termsIndex, docBase);
+          }
         }
-      }
 
-      if (perSegComp == null) {
-        perSegComp = new AnyOrdComparator(docToOrd, docBase);
+        if (perSegComp == null) {
+          perSegComp = new AnyPackedDocToOrdComparator(docToOrd, docBase);
+        }
+      } else {
+        if (perSegComp == null) {
+          perSegComp = new AnyOrdComparator(docBase);
+        }
       }
         
       currentReaderGen++;
@@ -1845,7 +1889,7 @@
   public static final class TermValDocValuesComparator extends FieldComparator<BytesRef> {
 
     private BytesRef[] values;
-    private Source docTerms;
+    private DocValues.Source docTerms;
     private final String field;
     private BytesRef bottom;
     private final BytesRef tempBR = new BytesRef();
@@ -1878,11 +1922,11 @@
 
     @Override
     public FieldComparator setNextReader(AtomicReaderContext context) throws IOException {
-      final IndexDocValues dv = context.reader.docValues(field);
+      final DocValues dv = context.reader.docValues(field);
       if (dv != null) {
         docTerms = dv.getSource();
       } else {
-        docTerms = IndexDocValues.getDefaultSource(ValueType.BYTES_VAR_DEREF);
+        docTerms = DocValues.getDefaultSource(DocValues.Type.BYTES_VAR_DEREF);
       }
       return this;
     }


diff -ruN -x .svn -x build lucene-clean-trunk/lucene/src/java/org/apache/lucene/search/similarities/Similarity.java lucene-3622/lucene/src/java/org/apache/lucene/search/similarities/Similarity.java
--- lucene-clean-trunk/lucene/src/java/org/apache/lucene/search/similarities/Similarity.java	2011-12-06 18:45:03.740810977 -0500
+++ lucene-3622/lucene/src/java/org/apache/lucene/search/similarities/Similarity.java	2011-12-09 11:13:38.796853133 -0500
@@ -20,7 +20,7 @@
 
 import java.io.IOException;
 
-import org.apache.lucene.document.IndexDocValuesField; // javadoc
+import org.apache.lucene.document.DocValuesField; // javadoc
 import org.apache.lucene.index.FieldInvertState;
 import org.apache.lucene.index.IndexReader; // javadoc
 import org.apache.lucene.index.IndexReader.AtomicReaderContext;
@@ -71,11 +71,11 @@
  * <p>
  * Because index-time boost is handled entirely at the application level anyway,
  * an application can alternatively store the index-time boost separately using an 
- * {@link IndexDocValuesField}, and access this at query-time with 
+ * {@link DocValuesField}, and access this at query-time with 
  * {@link IndexReader#docValues(String)}.
  * <p>
  * Finally, using index-time boosts (either via folding into the normalization byte or
- * via IndexDocValues), is an inefficient way to boost the scores of different fields if the
+ * via DocValues), is an inefficient way to boost the scores of different fields if the
  * boost will be the same for every document, instead the Similarity can simply take a constant
  * boost parameter <i>C</i>, and the SimilarityProvider can return different instances with
  * different boosts depending upon field name.


diff -ruN -x .svn -x build lucene-clean-trunk/lucene/src/test/org/apache/lucene/index/TestAddIndexes.java lucene-3622/lucene/src/test/org/apache/lucene/index/TestAddIndexes.java
--- lucene-clean-trunk/lucene/src/test/org/apache/lucene/index/TestAddIndexes.java	2011-12-09 08:23:19.884675177 -0500
+++ lucene-3622/lucene/src/test/org/apache/lucene/index/TestAddIndexes.java	2011-12-10 10:59:27.322342914 -0500
@@ -27,7 +27,7 @@
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.FieldType;
-import org.apache.lucene.document.IndexDocValuesField;
+import org.apache.lucene.document.DocValuesField;
 import org.apache.lucene.document.StringField;
 import org.apache.lucene.document.TextField;
 import org.apache.lucene.index.IndexWriterConfig.OpenMode;
@@ -47,7 +47,6 @@
 import org.apache.lucene.index.codecs.lucene40.Lucene40StoredFieldsFormat;
 import org.apache.lucene.index.codecs.lucene40.Lucene40TermVectorsFormat;
 import org.apache.lucene.index.codecs.pulsing.Pulsing40PostingsFormat;
-import org.apache.lucene.index.values.IndexDocValues;
 import org.apache.lucene.search.DocIdSetIterator;
 import org.apache.lucene.search.PhraseQuery;
 import org.apache.lucene.store.AlreadyClosedException;
@@ -1254,7 +1253,7 @@
     RandomIndexWriter w = new RandomIndexWriter(random, d1);
     Document doc = new Document();
     doc.add(newField("id", "1", StringField.TYPE_STORED));
-    IndexDocValuesField dv = new IndexDocValuesField("dv");
+    DocValuesField dv = new DocValuesField("dv");
     dv.setInt(1);
     doc.add(dv);
     w.addDocument(doc);
@@ -1265,7 +1264,7 @@
     w = new RandomIndexWriter(random, d2);
     doc = new Document();
     doc.add(newField("id", "2", StringField.TYPE_STORED));
-    dv = new IndexDocValuesField("dv");
+    dv = new DocValuesField("dv");
     dv.setInt(2);
     doc.add(dv);
     w.addDocument(doc);
@@ -1285,7 +1284,7 @@
     w.close();
     IndexReader sr = getOnlySegmentReader(r3);
     assertEquals(2, sr.numDocs());
-    IndexDocValues docValues = sr.perDocValues().docValues("dv");
+    DocValues docValues = sr.docValues("dv");
     assertNotNull(docValues);
     r3.close();
     d3.close();


diff -ruN -x .svn -x build lucene-clean-trunk/lucene/src/test/org/apache/lucene/index/TestDuelingCodecs.java lucene-3622/lucene/src/test/org/apache/lucene/index/TestDuelingCodecs.java
--- lucene-clean-trunk/lucene/src/test/org/apache/lucene/index/TestDuelingCodecs.java	2011-12-06 18:45:03.448810972 -0500
+++ lucene-3622/lucene/src/test/org/apache/lucene/index/TestDuelingCodecs.java	2011-12-10 12:36:13.606444027 -0500
@@ -18,17 +18,16 @@
  */
 
 import java.io.IOException;
+import java.util.HashSet;
 import java.util.Iterator;
 import java.util.Random;
+import java.util.Set;
 
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.document.Document;
+import org.apache.lucene.index.IndexReader.FieldOption;
 import org.apache.lucene.index.codecs.Codec;
-import org.apache.lucene.index.codecs.PerDocValues;
-import org.apache.lucene.index.values.IndexDocValues;
-import org.apache.lucene.index.values.IndexDocValues.Source;
-import org.apache.lucene.index.values.ValueType;
 import org.apache.lucene.search.DocIdSetIterator;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.util.Bits;
@@ -520,29 +519,15 @@
    * checks that docvalues across all fields are equivalent
    */
   public void assertDocValues(IndexReader leftReader, IndexReader rightReader) throws Exception {
-    PerDocValues leftPerDoc = MultiPerDocValues.getPerDocs(leftReader);
-    PerDocValues rightPerDoc = MultiPerDocValues.getPerDocs(rightReader);
-    
-    Fields leftFields = MultiFields.getFields(leftReader);
-    Fields rightFields = MultiFields.getFields(rightReader);
-    // Fields could be null if there are no postings,
-    // but then it must be null for both
-    if (leftFields == null || rightFields == null) {
-      assertNull(info, leftFields);
-      assertNull(info, rightFields);
-      return;
-    }
-    
-    FieldsEnum fieldsEnum = leftFields.iterator();
-    String field;
-    while ((field = fieldsEnum.next()) != null) {
-      IndexDocValues leftDocValues = leftPerDoc.docValues(field);
-      IndexDocValues rightDocValues = rightPerDoc.docValues(field);
-      if (leftDocValues == null || rightDocValues == null) {
-        assertNull(info, leftDocValues);
-        assertNull(info, rightDocValues);
-        continue;
-      }
+    Set<String> leftValues = new HashSet<String>(leftReader.getFieldNames(FieldOption.DOC_VALUES));
+    Set<String> rightValues = new HashSet<String>(rightReader.getFieldNames(FieldOption.DOC_VALUES));
+    assertEquals(info, leftValues, rightValues);
+
+    for (String field : leftValues) {
+      DocValues leftDocValues = MultiDocValues.getDocValues(leftReader, field);
+      DocValues rightDocValues = MultiDocValues.getDocValues(rightReader, field);
+      assertNotNull(info, leftDocValues);
+      assertNotNull(info, rightDocValues);
       assertDocValuesSource(leftDocValues.getDirectSource(), rightDocValues.getDirectSource());
       assertDocValuesSource(leftDocValues.getSource(), rightDocValues.getSource());
     }
@@ -551,8 +536,8 @@
   /**
    * checks source API
    */
-  public void assertDocValuesSource(Source left, Source right) throws Exception {
-    ValueType leftType = left.type();
+  public void assertDocValuesSource(DocValues.Source left, DocValues.Source right) throws Exception {
+    DocValues.Type leftType = left.type();
     assertEquals(info, leftType, right.type());
     switch(leftType) {
       case VAR_INTS:


diff -ruN -x .svn -x build lucene-clean-trunk/lucene/src/test/org/apache/lucene/index/TestFieldInfos.java lucene-3622/lucene/src/test/org/apache/lucene/index/TestFieldInfos.java
--- lucene-clean-trunk/lucene/src/test/org/apache/lucene/index/TestFieldInfos.java	2011-12-06 18:45:03.452810972 -0500
+++ lucene-3622/lucene/src/test/org/apache/lucene/index/TestFieldInfos.java	2011-12-09 08:23:09.844675001 -0500
@@ -27,7 +27,6 @@
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.IOContext;
 import org.apache.lucene.store.IndexOutput;
-import org.junit.Ignore;
 
 import java.io.IOException;
 import java.util.Arrays;
@@ -82,7 +81,6 @@
 
     info = readIn.fieldInfo("textField2");
     assertTrue(info != null);
-    assertTrue(info.storeTermVector == true);
     assertTrue(info.omitNorms == false);
 
     info = readIn.fieldInfo("textField3");


diff -ruN -x .svn -x build lucene-clean-trunk/lucene/src/test/org/apache/lucene/index/TestIndexableField.java lucene-3622/lucene/src/test/org/apache/lucene/index/TestIndexableField.java
--- lucene-clean-trunk/lucene/src/test/org/apache/lucene/index/TestIndexableField.java	2011-12-06 18:45:03.304810970 -0500
+++ lucene-3622/lucene/src/test/org/apache/lucene/index/TestIndexableField.java	2011-12-10 13:07:49.338477040 -0500
@@ -28,8 +28,7 @@
 import org.apache.lucene.document.NumericField.DataType;
 import org.apache.lucene.document.NumericField;
 import org.apache.lucene.document.StringField;
-import org.apache.lucene.index.values.PerDocFieldValues;
-import org.apache.lucene.index.values.ValueType;
+import org.apache.lucene.index.DocValues;
 import org.apache.lucene.search.BooleanClause;
 import org.apache.lucene.search.BooleanQuery;
 import org.apache.lucene.search.IndexSearcher;
@@ -157,12 +156,12 @@
 
     // TODO: randomly enable doc values
     @Override
-    public PerDocFieldValues docValues() {
+    public DocValue docValue() {
       return null;
     }
 
     @Override
-    public ValueType docValuesType() {
+    public DocValues.Type docValueType() {
       return null;
     }
 


diff -ruN -x .svn -x build lucene-clean-trunk/lucene/src/test/org/apache/lucene/index/values/TestDocValuesIndexing.java lucene-3622/lucene/src/test/org/apache/lucene/index/values/TestDocValuesIndexing.java
--- lucene-clean-trunk/lucene/src/test/org/apache/lucene/index/values/TestDocValuesIndexing.java	2011-12-09 08:23:19.856675176 -0500
+++ lucene-3622/lucene/src/test/org/apache/lucene/index/values/TestDocValuesIndexing.java	2011-12-10 12:33:54.782441610 -0500
@@ -27,21 +27,23 @@
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
-import org.apache.lucene.document.IndexDocValuesField;
+import org.apache.lucene.document.DocValuesField;
 import org.apache.lucene.document.StringField;
 import org.apache.lucene.document.TextField;
 import org.apache.lucene.index.CorruptIndexException;
+import org.apache.lucene.index.DocValues;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexWriter;
 import org.apache.lucene.index.IndexWriterConfig;
 import org.apache.lucene.index.LogDocMergePolicy;
 import org.apache.lucene.index.LogMergePolicy;
-import org.apache.lucene.index.MultiPerDocValues;
+import org.apache.lucene.index.MultiDocValues;
 import org.apache.lucene.index.RandomIndexWriter;
 import org.apache.lucene.index.Term;
+import org.apache.lucene.index.DocValues.Source;
+import org.apache.lucene.index.DocValues.Type;
 import org.apache.lucene.index.codecs.Codec;
-import org.apache.lucene.index.codecs.PerDocValues;
-import org.apache.lucene.index.values.IndexDocValues.Source;
+import org.apache.lucene.index.codecs.PerDocProducer;
 import org.apache.lucene.search.*;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.LockObtainFailedException;
@@ -76,7 +78,7 @@
     IndexWriter writer = new IndexWriter(dir, writerConfig(false));
     for (int i = 0; i < 5; i++) {
       Document doc = new Document();
-      IndexDocValuesField valuesField = new IndexDocValuesField("docId");
+      DocValuesField valuesField = new DocValuesField("docId");
       valuesField.setInt(i);
       doc.add(valuesField);
       doc.add(new TextField("docId", "" + i));
@@ -102,7 +104,7 @@
     TopDocs search = searcher.search(query, 10);
     assertEquals(5, search.totalHits);
     ScoreDoc[] scoreDocs = search.scoreDocs;
-    IndexDocValues docValues = MultiPerDocValues.getPerDocs(reader).docValues("docId");
+    DocValues docValues = MultiDocValues.getDocValues(reader, "docId");
     Source source = docValues.getSource();
     for (int i = 0; i < scoreDocs.length; i++) {
       assertEquals(i, scoreDocs[i].doc);
@@ -130,10 +132,10 @@
 
   public void testAddIndexes() throws IOException {
     int valuesPerIndex = 10;
-    List<ValueType> values = Arrays.asList(ValueType.values());
+    List<Type> values = Arrays.asList(Type.values());
     Collections.shuffle(values, random);
-    ValueType first = values.get(0);
-    ValueType second = values.get(1);
+    Type first = values.get(0);
+    Type second = values.get(1);
     // index first index
     Directory d_1 = newDirectory();
     IndexWriter w_1 = new IndexWriter(d_1, writerConfig(random.nextBoolean()));
@@ -256,11 +258,11 @@
     Directory d = newDirectory();
     IndexWriter w = new IndexWriter(d, cfg);
     final int numValues = 50 + atLeast(10);
-    final List<ValueType> numVariantList = new ArrayList<ValueType>(NUMERICS);
+    final List<Type> numVariantList = new ArrayList<Type>(NUMERICS);
 
     // run in random order to test if fill works correctly during merges
     Collections.shuffle(numVariantList, random);
-    for (ValueType val : numVariantList) {
+    for (Type val : numVariantList) {
       FixedBitSet deleted = indexValues(w, numValues, val, numVariantList,
           withDeletions, 7);
       List<Closeable> closeables = new ArrayList<Closeable>();
@@ -277,7 +279,7 @@
       case FIXED_INTS_32:
       case FIXED_INTS_64:
       case VAR_INTS: {
-        IndexDocValues intsReader = getDocValues(r, val.name());
+        DocValues intsReader = getDocValues(r, val.name());
         assertNotNull(intsReader);
 
         Source ints = getSource(intsReader);
@@ -298,7 +300,7 @@
         break;
       case FLOAT_32:
       case FLOAT_64: {
-        IndexDocValues floatReader = getDocValues(r, val.name());
+        DocValues floatReader = getDocValues(r, val.name());
         assertNotNull(floatReader);
         Source floats = getSource(floatReader);
         for (int i = 0; i < base; i++) {
@@ -333,11 +335,11 @@
       throws CorruptIndexException, LockObtainFailedException, IOException {
     final Directory d = newDirectory();
     IndexWriter w = new IndexWriter(d, cfg);
-    final List<ValueType> byteVariantList = new ArrayList<ValueType>(BYTES);
+    final List<Type> byteVariantList = new ArrayList<Type>(BYTES);
     // run in random order to test if fill works correctly during merges
     Collections.shuffle(byteVariantList, random);
     final int numValues = 50 + atLeast(10);
-    for (ValueType byteIndexValue : byteVariantList) {
+    for (Type byteIndexValue : byteVariantList) {
       List<Closeable> closeables = new ArrayList<Closeable>();
       final int bytesSize = 1 + atLeast(50);
       FixedBitSet deleted = indexValues(w, numValues, byteIndexValue,
@@ -346,7 +348,7 @@
       assertEquals(0, r.numDeletedDocs());
       final int numRemainingValues = numValues - deleted.cardinality();
       final int base = r.numDocs() - numRemainingValues;
-      IndexDocValues bytesReader = getDocValues(r, byteIndexValue.name());
+      DocValues bytesReader = getDocValues(r, byteIndexValue.name());
       assertNotNull("field " + byteIndexValue.name()
           + " returned null reader - maybe merged failed", bytesReader);
       Source bytes = getSource(bytesReader);
@@ -416,27 +418,11 @@
     d.close();
   }
 
-  private IndexDocValues getDocValues(IndexReader reader, String field)
-      throws IOException {
-    boolean singleSeg = reader.getSequentialSubReaders().length == 1;
-    PerDocValues perDoc = singleSeg ? reader.getSequentialSubReaders()[0].perDocValues()
-        : MultiPerDocValues.getPerDocs(reader);
-    switch (random.nextInt(singleSeg ? 3 : 2)) { // case 2 only if single seg
-    case 0:
-      return perDoc.docValues(field);
-    case 1:
-      IndexDocValues docValues = perDoc.docValues(field);
-      if (docValues != null) {
-        return docValues;
-      }
-      throw new RuntimeException("no such field " + field);
-    case 2:// this only works if we are on a single seg index!
-      return reader.getSequentialSubReaders()[0].docValues(field);
-    }
-    throw new RuntimeException();
+  private DocValues getDocValues(IndexReader reader, String field) throws IOException {
+    return MultiDocValues.getDocValues(reader, field);
   }
 
-  private Source getSource(IndexDocValues values) throws IOException {
+  private Source getSource(DocValues values) throws IOException {
     // getSource uses cache internally
     switch(random.nextInt(5)) {
     case 3:
@@ -451,24 +437,24 @@
   }
 
 
-  private static EnumSet<ValueType> BYTES = EnumSet.of(ValueType.BYTES_FIXED_DEREF,
-      ValueType.BYTES_FIXED_STRAIGHT, ValueType.BYTES_VAR_DEREF,
-      ValueType.BYTES_VAR_STRAIGHT, ValueType.BYTES_FIXED_SORTED, ValueType.BYTES_VAR_SORTED);
-
-  private static EnumSet<ValueType> NUMERICS = EnumSet.of(ValueType.VAR_INTS,
-      ValueType.FIXED_INTS_16, ValueType.FIXED_INTS_32,
-      ValueType.FIXED_INTS_64, 
-      ValueType.FIXED_INTS_8,
-      ValueType.FLOAT_32,
-      ValueType.FLOAT_64);
+  private static EnumSet<Type> BYTES = EnumSet.of(Type.BYTES_FIXED_DEREF,
+      Type.BYTES_FIXED_STRAIGHT, Type.BYTES_VAR_DEREF,
+      Type.BYTES_VAR_STRAIGHT, Type.BYTES_FIXED_SORTED, Type.BYTES_VAR_SORTED);
+
+  private static EnumSet<Type> NUMERICS = EnumSet.of(Type.VAR_INTS,
+      Type.FIXED_INTS_16, Type.FIXED_INTS_32,
+      Type.FIXED_INTS_64, 
+      Type.FIXED_INTS_8,
+      Type.FLOAT_32,
+      Type.FLOAT_64);
 
-  private FixedBitSet indexValues(IndexWriter w, int numValues, ValueType value,
-      List<ValueType> valueVarList, boolean withDeletions, int bytesSize)
+  private FixedBitSet indexValues(IndexWriter w, int numValues, Type value,
+      List<Type> valueVarList, boolean withDeletions, int bytesSize)
       throws CorruptIndexException, IOException {
     final boolean isNumeric = NUMERICS.contains(value);
     FixedBitSet deleted = new FixedBitSet(numValues);
     Document doc = new Document();
-    IndexDocValuesField valField = new IndexDocValuesField(value.name());
+    DocValuesField valField = new DocValuesField(value.name());
     doc.add(valField);
     final BytesRef bytesRef = new BytesRef();
 
@@ -522,7 +508,7 @@
 
       if (i % 7 == 0) {
         if (withDeletions && random.nextBoolean()) {
-          ValueType val = valueVarList.get(random.nextInt(1 + valueVarList
+          Type val = valueVarList.get(random.nextInt(1 + valueVarList
               .indexOf(value)));
           final int randInt = val == value ? random.nextInt(1 + i) : random
               .nextInt(numValues);
@@ -545,11 +531,11 @@
     return deleted;
   }
 
-  public void testMultiValuedIndexDocValuesField() throws Exception {
+  public void testMultiValuedDocValuesField() throws Exception {
     Directory d = newDirectory();
     RandomIndexWriter w = new RandomIndexWriter(random, d);
     Document doc = new Document();
-    IndexDocValuesField f = new IndexDocValuesField("field");
+    DocValuesField f = new DocValuesField("field");
     f.setInt(17);
     // Index doc values are single-valued so we should not
     // be able to add same field more than once:
@@ -568,7 +554,7 @@
     w.forceMerge(1);
     IndexReader r = w.getReader();
     w.close();
-    assertEquals(17, r.getSequentialSubReaders()[0].perDocValues().docValues("field").load().getInt(0));
+    assertEquals(17, r.getSequentialSubReaders()[0].docValues("field").load().getInt(0));
     r.close();
     d.close();
   }
@@ -577,12 +563,12 @@
     Directory d = newDirectory();
     RandomIndexWriter w = new RandomIndexWriter(random, d);
     Document doc = new Document();
-    IndexDocValuesField f = new IndexDocValuesField("field");
+    DocValuesField f = new DocValuesField("field");
     f.setInt(17);
     // Index doc values are single-valued so we should not
     // be able to add same field more than once:
     doc.add(f);
-    IndexDocValuesField f2 = new IndexDocValuesField("field");
+    DocValuesField f2 = new DocValuesField("field");
     f2.setFloat(22.0);
     doc.add(f2);
     try {
@@ -598,7 +584,7 @@
     w.forceMerge(1);
     IndexReader r = w.getReader();
     w.close();
-    assertEquals(17, r.getSequentialSubReaders()[0].perDocValues().docValues("field").load().getInt(0));
+    assertEquals(17, r.getSequentialSubReaders()[0].docValues("field").load().getInt(0));
     r.close();
     d.close();
   }


diff -ruN -x .svn -x build lucene-clean-trunk/lucene/src/test/org/apache/lucene/index/values/TestDocValues.java lucene-3622/lucene/src/test/org/apache/lucene/index/values/TestDocValues.java
--- lucene-clean-trunk/lucene/src/test/org/apache/lucene/index/values/TestDocValues.java	2011-12-06 18:45:03.304810970 -0500
+++ lucene-3622/lucene/src/test/org/apache/lucene/index/values/TestDocValues.java	2011-12-09 11:10:18.792849650 -0500
@@ -20,8 +20,14 @@
 import java.io.IOException;
 import java.util.Comparator;
 
-import org.apache.lucene.index.values.IndexDocValues.SortedSource;
-import org.apache.lucene.index.values.IndexDocValues.Source;
+import org.apache.lucene.index.DocValues;
+import org.apache.lucene.index.DocValues.SortedSource;
+import org.apache.lucene.index.DocValues.Source;
+import org.apache.lucene.index.DocValues.Type;
+import org.apache.lucene.index.codecs.lucene40.values.Bytes;
+import org.apache.lucene.index.codecs.lucene40.values.Floats;
+import org.apache.lucene.index.codecs.lucene40.values.Ints;
+import org.apache.lucene.index.codecs.lucene40.values.Writer;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.Counter;
@@ -29,6 +35,7 @@
 import org.apache.lucene.util.UnicodeUtil;
 import org.apache.lucene.util._TestUtil;
 
+// TODO: some of this should be under lucene40 codec tests? is talking to codec directly?f
 public class TestDocValues extends LuceneTestCase {
   private static final Comparator<BytesRef> COMP = BytesRef.getUTF8SortedAsUnicodeComparator();
   // TODO -- for sorted test, do our own Sort of the
@@ -76,12 +83,12 @@
     w.finish(maxDoc);
     assertEquals(0, trackBytes.get());
 
-    IndexDocValues r = Bytes.getValues(dir, "test", mode, fixedSize, maxDoc, COMP, newIOContext(random));
+    DocValues r = Bytes.getValues(dir, "test", mode, fixedSize, maxDoc, COMP, newIOContext(random));
 
     // Verify we can load source twice:
     for (int iter = 0; iter < 2; iter++) {
       Source s;
-      IndexDocValues.SortedSource ss;
+      DocValues.SortedSource ss;
       if (mode == Bytes.Mode.SORTED) {
         // default is unicode so we can simply pass null here
         s = ss = getSortedSource(r);  
@@ -155,19 +162,19 @@
         { Long.MIN_VALUE + 1, 1 }, { -1, Long.MAX_VALUE },
         { Long.MIN_VALUE, -1 }, { 1, Long.MAX_VALUE },
         { -1, Long.MAX_VALUE - 1 }, { Long.MIN_VALUE + 2, 1 }, };
-    ValueType[] expectedTypes = new ValueType[] { ValueType.FIXED_INTS_64,
-        ValueType.FIXED_INTS_64, ValueType.FIXED_INTS_64,
-        ValueType.FIXED_INTS_64, ValueType.VAR_INTS, ValueType.VAR_INTS,
-        ValueType.VAR_INTS, };
+    Type[] expectedTypes = new Type[] { Type.FIXED_INTS_64,
+        Type.FIXED_INTS_64, Type.FIXED_INTS_64,
+        Type.FIXED_INTS_64, Type.VAR_INTS, Type.VAR_INTS,
+        Type.VAR_INTS, };
     for (int i = 0; i < minMax.length; i++) {
       Directory dir = newDirectory();
       final Counter trackBytes = Counter.newCounter();
-      Writer w = Ints.getWriter(dir, "test", trackBytes, ValueType.VAR_INTS, newIOContext(random));
+      Writer w = Ints.getWriter(dir, "test", trackBytes, Type.VAR_INTS, newIOContext(random));
       w.add(0, minMax[i][0]);
       w.add(1, minMax[i][1]);
       w.finish(2);
       assertEquals(0, trackBytes.get());
-      IndexDocValues r = Ints.getValues(dir, "test", 2,  ValueType.VAR_INTS, newIOContext(random));
+      DocValues r = Ints.getValues(dir, "test", 2,  Type.VAR_INTS, newIOContext(random));
       Source source = getSource(r);
       assertEquals(i + " with min: " + minMax[i][0] + " max: " + minMax[i][1],
           expectedTypes[i], source.type());
@@ -180,14 +187,14 @@
   }
   
   public void testVInts() throws IOException {
-    testInts(ValueType.VAR_INTS, 63);
+    testInts(Type.VAR_INTS, 63);
   }
   
   public void testFixedInts() throws IOException {
-    testInts(ValueType.FIXED_INTS_64, 63);
-    testInts(ValueType.FIXED_INTS_32, 31);
-    testInts(ValueType.FIXED_INTS_16, 15);
-    testInts(ValueType.FIXED_INTS_8, 7);
+    testInts(Type.FIXED_INTS_64, 63);
+    testInts(Type.FIXED_INTS_32, 31);
+    testInts(Type.FIXED_INTS_16, 15);
+    testInts(Type.FIXED_INTS_8, 7);
 
   }
   
@@ -195,12 +202,12 @@
     byte[] sourceArray = new byte[] {1,2,3};
     Directory dir = newDirectory();
     final Counter trackBytes = Counter.newCounter();
-    Writer w = Ints.getWriter(dir, "test", trackBytes, ValueType.FIXED_INTS_8, newIOContext(random));
+    Writer w = Ints.getWriter(dir, "test", trackBytes, Type.FIXED_INTS_8, newIOContext(random));
     for (int i = 0; i < sourceArray.length; i++) {
       w.add(i, (long) sourceArray[i]);
     }
     w.finish(sourceArray.length);
-    IndexDocValues r = Ints.getValues(dir, "test", sourceArray.length, ValueType.FIXED_INTS_8, newIOContext(random));
+    DocValues r = Ints.getValues(dir, "test", sourceArray.length, Type.FIXED_INTS_8, newIOContext(random));
     Source source = r.getSource();
     assertTrue(source.hasArray());
     byte[] loaded = ((byte[])source.getArray());
@@ -216,12 +223,12 @@
     short[] sourceArray = new short[] {1,2,3};
     Directory dir = newDirectory();
     final Counter trackBytes = Counter.newCounter();
-    Writer w = Ints.getWriter(dir, "test", trackBytes, ValueType.FIXED_INTS_16, newIOContext(random));
+    Writer w = Ints.getWriter(dir, "test", trackBytes, Type.FIXED_INTS_16, newIOContext(random));
     for (int i = 0; i < sourceArray.length; i++) {
       w.add(i, (long) sourceArray[i]);
     }
     w.finish(sourceArray.length);
-    IndexDocValues r = Ints.getValues(dir, "test", sourceArray.length, ValueType.FIXED_INTS_16, newIOContext(random));
+    DocValues r = Ints.getValues(dir, "test", sourceArray.length, Type.FIXED_INTS_16, newIOContext(random));
     Source source = r.getSource();
     assertTrue(source.hasArray());
     short[] loaded = ((short[])source.getArray());
@@ -237,12 +244,12 @@
     long[] sourceArray = new long[] {1,2,3};
     Directory dir = newDirectory();
     final Counter trackBytes = Counter.newCounter();
-    Writer w = Ints.getWriter(dir, "test", trackBytes, ValueType.FIXED_INTS_64, newIOContext(random));
+    Writer w = Ints.getWriter(dir, "test", trackBytes, Type.FIXED_INTS_64, newIOContext(random));
     for (int i = 0; i < sourceArray.length; i++) {
       w.add(i, sourceArray[i]);
     }
     w.finish(sourceArray.length);
-    IndexDocValues r = Ints.getValues(dir, "test", sourceArray.length, ValueType.FIXED_INTS_64, newIOContext(random));
+    DocValues r = Ints.getValues(dir, "test", sourceArray.length, Type.FIXED_INTS_64, newIOContext(random));
     Source source = r.getSource();
     assertTrue(source.hasArray());
     long[] loaded = ((long[])source.getArray());
@@ -258,12 +265,12 @@
     int[] sourceArray = new int[] {1,2,3};
     Directory dir = newDirectory();
     final Counter trackBytes = Counter.newCounter();
-    Writer w = Ints.getWriter(dir, "test", trackBytes, ValueType.FIXED_INTS_32, newIOContext(random));
+    Writer w = Ints.getWriter(dir, "test", trackBytes, Type.FIXED_INTS_32, newIOContext(random));
     for (int i = 0; i < sourceArray.length; i++) {
       w.add(i, (long) sourceArray[i]);
     }
     w.finish(sourceArray.length);
-    IndexDocValues r = Ints.getValues(dir, "test", sourceArray.length, ValueType.FIXED_INTS_32, newIOContext(random));
+    DocValues r = Ints.getValues(dir, "test", sourceArray.length, Type.FIXED_INTS_32, newIOContext(random));
     Source source = r.getSource();
     assertTrue(source.hasArray());
     int[] loaded = ((int[])source.getArray());
@@ -279,12 +286,12 @@
     float[] sourceArray = new float[] {1,2,3};
     Directory dir = newDirectory();
     final Counter trackBytes = Counter.newCounter();
-    Writer w = Floats.getWriter(dir, "test", trackBytes, newIOContext(random), ValueType.FLOAT_32);
+    Writer w = Floats.getWriter(dir, "test", trackBytes, newIOContext(random), Type.FLOAT_32);
     for (int i = 0; i < sourceArray.length; i++) {
       w.add(i, sourceArray[i]);
     }
     w.finish(sourceArray.length);
-    IndexDocValues r = Floats.getValues(dir, "test", 3, newIOContext(random), ValueType.FLOAT_32);
+    DocValues r = Floats.getValues(dir, "test", 3, newIOContext(random), Type.FLOAT_32);
     Source source = r.getSource();
     assertTrue(source.hasArray());
     float[] loaded = ((float[])source.getArray());
@@ -300,12 +307,12 @@
     double[] sourceArray = new double[] {1,2,3};
     Directory dir = newDirectory();
     final Counter trackBytes = Counter.newCounter();
-    Writer w = Floats.getWriter(dir, "test", trackBytes, newIOContext(random), ValueType.FLOAT_64);
+    Writer w = Floats.getWriter(dir, "test", trackBytes, newIOContext(random), Type.FLOAT_64);
     for (int i = 0; i < sourceArray.length; i++) {
       w.add(i, sourceArray[i]);
     }
     w.finish(sourceArray.length);
-    IndexDocValues r = Floats.getValues(dir, "test", 3, newIOContext(random), ValueType.FLOAT_64);
+    DocValues r = Floats.getValues(dir, "test", 3, newIOContext(random), Type.FLOAT_64);
     Source source = r.getSource();
     assertTrue(source.hasArray());
     double[] loaded = ((double[])source.getArray());
@@ -317,7 +324,7 @@
     dir.close();
   }
 
-  private void testInts(ValueType type, int maxBit) throws IOException {
+  private void testInts(Type type, int maxBit) throws IOException {
     long maxV = 1;
     final int NUM_VALUES = 333 + random.nextInt(333);
     final long[] values = new long[NUM_VALUES];
@@ -334,7 +341,7 @@
       w.finish(NUM_VALUES + additionalDocs);
       assertEquals(0, trackBytes.get());
 
-      IndexDocValues r = Ints.getValues(dir, "test", NUM_VALUES + additionalDocs, type, newIOContext(random));
+      DocValues r = Ints.getValues(dir, "test", NUM_VALUES + additionalDocs, type, newIOContext(random));
       for (int iter = 0; iter < 2; iter++) {
         Source s = getSource(r);
         assertEquals(type, s.type());
@@ -350,17 +357,17 @@
   }
 
   public void testFloats4() throws IOException {
-    runTestFloats(ValueType.FLOAT_32, 0.00001);
+    runTestFloats(Type.FLOAT_32, 0.00001);
   }
 
-  private void runTestFloats(ValueType type, double delta) throws IOException {
+  private void runTestFloats(Type type, double delta) throws IOException {
     Directory dir = newDirectory();
     final Counter trackBytes = Counter.newCounter();
     Writer w = Floats.getWriter(dir, "test", trackBytes, newIOContext(random), type);
     final int NUM_VALUES = 777 + random.nextInt(777);;
     final double[] values = new double[NUM_VALUES];
     for (int i = 0; i < NUM_VALUES; i++) {
-      final double v = type == ValueType.FLOAT_32 ? random.nextFloat() : random
+      final double v = type == Type.FLOAT_32 ? random.nextFloat() : random
           .nextDouble();
       values[i] = v;
       w.add(i, v);
@@ -369,7 +376,7 @@
     w.finish(NUM_VALUES + additionalValues);
     assertEquals(0, trackBytes.get());
 
-    IndexDocValues r = Floats.getValues(dir, "test", NUM_VALUES + additionalValues, newIOContext(random), type);
+    DocValues r = Floats.getValues(dir, "test", NUM_VALUES + additionalValues, newIOContext(random), type);
     for (int iter = 0; iter < 2; iter++) {
       Source s = getSource(r);
       for (int i = 0; i < NUM_VALUES; i++) {
@@ -381,11 +388,11 @@
   }
 
   public void testFloats8() throws IOException {
-    runTestFloats(ValueType.FLOAT_64, 0.0);
+    runTestFloats(Type.FLOAT_64, 0.0);
   }
   
 
-  private Source getSource(IndexDocValues values) throws IOException {
+  private Source getSource(DocValues values) throws IOException {
     // getSource uses cache internally
     switch(random.nextInt(5)) {
     case 3:
@@ -399,7 +406,7 @@
     }
   }
   
-  private SortedSource getSortedSource(IndexDocValues values) throws IOException {
+  private SortedSource getSortedSource(DocValues values) throws IOException {
     return getSource(values).asSortedSource();
   }
   


diff -ruN -x .svn -x build lucene-clean-trunk/lucene/src/test/org/apache/lucene/index/values/TestTypePromotion.java lucene-3622/lucene/src/test/org/apache/lucene/index/values/TestTypePromotion.java
--- lucene-clean-trunk/lucene/src/test/org/apache/lucene/index/values/TestTypePromotion.java	2011-12-07 10:51:25.825820702 -0500
+++ lucene-3622/lucene/src/test/org/apache/lucene/index/values/TestTypePromotion.java	2011-12-09 11:13:38.844853134 -0500
@@ -7,17 +7,20 @@
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
-import org.apache.lucene.document.IndexDocValuesField;
+import org.apache.lucene.document.DocValuesField;
 import org.apache.lucene.document.TextField;
 import org.apache.lucene.index.CorruptIndexException;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexReader.ReaderContext;
+import org.apache.lucene.index.DocValues;
+import org.apache.lucene.index.DocValues.Source;
+import org.apache.lucene.index.DocValues.Type;
 import org.apache.lucene.index.IndexWriter;
 import org.apache.lucene.index.IndexWriterConfig;
 import org.apache.lucene.index.NoMergePolicy;
 import org.apache.lucene.index.SlowMultiReaderWrapper;
 import org.apache.lucene.index.codecs.Codec;
-import org.apache.lucene.index.values.IndexDocValues.Source;
+import org.apache.lucene.index.codecs.lucene40.values.BytesRefUtils;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.LuceneTestCase;
@@ -46,22 +49,22 @@
     assumeFalse("cannot work with preflex codec", Codec.getDefault().getName().equals("Lucene3x"));
   }
 
-  private static EnumSet<ValueType> INTEGERS = EnumSet.of(ValueType.VAR_INTS,
-      ValueType.FIXED_INTS_16, ValueType.FIXED_INTS_32,
-      ValueType.FIXED_INTS_64, ValueType.FIXED_INTS_8);
-
-  private static EnumSet<ValueType> FLOATS = EnumSet.of(ValueType.FLOAT_32,
-      ValueType.FLOAT_64);
-
-  private static EnumSet<ValueType> UNSORTED_BYTES = EnumSet.of(
-      ValueType.BYTES_FIXED_DEREF, ValueType.BYTES_FIXED_STRAIGHT,
-      ValueType.BYTES_VAR_STRAIGHT, ValueType.BYTES_VAR_DEREF);
+  private static EnumSet<Type> INTEGERS = EnumSet.of(Type.VAR_INTS,
+      Type.FIXED_INTS_16, Type.FIXED_INTS_32,
+      Type.FIXED_INTS_64, Type.FIXED_INTS_8);
+
+  private static EnumSet<Type> FLOATS = EnumSet.of(Type.FLOAT_32,
+      Type.FLOAT_64);
+
+  private static EnumSet<Type> UNSORTED_BYTES = EnumSet.of(
+      Type.BYTES_FIXED_DEREF, Type.BYTES_FIXED_STRAIGHT,
+      Type.BYTES_VAR_STRAIGHT, Type.BYTES_VAR_DEREF);
 
-  private static EnumSet<ValueType> SORTED_BYTES = EnumSet.of(
-      ValueType.BYTES_FIXED_SORTED, ValueType.BYTES_VAR_SORTED);
+  private static EnumSet<Type> SORTED_BYTES = EnumSet.of(
+      Type.BYTES_FIXED_SORTED, Type.BYTES_VAR_SORTED);
   
-  public ValueType randomValueType(EnumSet<ValueType> typeEnum, Random random) {
-    ValueType[] array = typeEnum.toArray(new ValueType[0]);
+  public Type randomValueType(EnumSet<Type> typeEnum, Random random) {
+    Type[] array = typeEnum.toArray(new Type[0]);
     return array[random.nextInt(array.length)];
   }
   
@@ -69,7 +72,7 @@
     Int, Float, Byte
   }
 
-  private void runTest(EnumSet<ValueType> types, TestType type)
+  private void runTest(EnumSet<Type> types, TestType type)
       throws CorruptIndexException, IOException {
     Directory dir = newDirectory();
     IndexWriter writer = new IndexWriter(dir,
@@ -78,11 +81,11 @@
     int num_2 = atLeast(200);
     int num_3 = atLeast(200);
     long[] values = new long[num_1 + num_2 + num_3];
-    index(writer, new IndexDocValuesField("promote"),
+    index(writer, new DocValuesField("promote"),
         randomValueType(types, random), values, 0, num_1);
     writer.commit();
     
-    index(writer, new IndexDocValuesField("promote"),
+    index(writer, new DocValuesField("promote"),
         randomValueType(types, random), values, num_1, num_2);
     writer.commit();
     
@@ -93,7 +96,7 @@
       Directory dir_2 = newDirectory() ;
       IndexWriter writer_2 = new IndexWriter(dir_2,
           newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)));
-      index(writer_2, new IndexDocValuesField("promote"),
+      index(writer_2, new DocValuesField("promote"),
           randomValueType(types, random), values, num_1 + num_2, num_3);
       writer_2.commit();
       writer_2.close();
@@ -109,7 +112,7 @@
       }
       dir_2.close();
     } else {
-      index(writer, new IndexDocValuesField("promote"),
+      index(writer, new DocValuesField("promote"),
           randomValueType(types, random), values, num_1 + num_2, num_3);
     }
 
@@ -126,7 +129,7 @@
     assertEquals(1, reader.getSequentialSubReaders().length);
     ReaderContext topReaderContext = reader.getTopReaderContext();
     ReaderContext[] children = topReaderContext.children();
-    IndexDocValues docValues = children[0].reader.docValues("promote");
+    DocValues docValues = children[0].reader.docValues("promote");
     assertEquals(1, children.length);
     Source directSource = docValues.getDirectSource();
     for (int i = 0; i < values.length; i++) {
@@ -171,8 +174,8 @@
     reader.close();
   }
 
-  public void index(IndexWriter writer, IndexDocValuesField valField,
-      ValueType valueType, long[] values, int offset, int num)
+  public void index(IndexWriter writer, DocValuesField valField,
+      Type valueType, long[] values, int offset, int num)
       throws CorruptIndexException, IOException {
     BytesRef ref = new BytesRef(new byte[] { 1, 2, 3, 4 });
     for (int i = offset; i < offset + num; i++) {
@@ -266,7 +269,7 @@
     int num_1 = atLeast(200);
     int num_2 = atLeast(200);
     long[] values = new long[num_1 + num_2];
-    index(writer, new IndexDocValuesField("promote"),
+    index(writer, new DocValuesField("promote"),
         randomValueType(INTEGERS, random), values, 0, num_1);
     writer.commit();
     
@@ -275,7 +278,7 @@
       Directory dir_2 = newDirectory() ;
       IndexWriter writer_2 = new IndexWriter(dir_2,
           newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)));
-      index(writer_2, new IndexDocValuesField("promote"),
+      index(writer_2, new DocValuesField("promote"),
           randomValueType(random.nextBoolean() ? UNSORTED_BYTES : SORTED_BYTES, random), values, num_1, num_2);
       writer_2.commit();
       writer_2.close();
@@ -289,7 +292,7 @@
       }
       dir_2.close();
     } else {
-      index(writer, new IndexDocValuesField("promote"),
+      index(writer, new DocValuesField("promote"),
           randomValueType(random.nextBoolean() ? UNSORTED_BYTES : SORTED_BYTES, random), values, num_1, num_2);
       writer.commit();
     }
@@ -306,10 +309,10 @@
     assertEquals(1, reader.getSequentialSubReaders().length);
     ReaderContext topReaderContext = reader.getTopReaderContext();
     ReaderContext[] children = topReaderContext.children();
-    IndexDocValues docValues = children[0].reader.docValues("promote");
+    DocValues docValues = children[0].reader.docValues("promote");
     assertNotNull(docValues);
     assertValues(TestType.Byte, dir, values);
-    assertEquals(ValueType.BYTES_VAR_STRAIGHT, docValues.type());
+    assertEquals(Type.BYTES_VAR_STRAIGHT, docValues.type());
     reader.close();
     dir.close();
   }


diff -ruN -x .svn -x build lucene-clean-trunk/lucene/src/test/org/apache/lucene/search/TestDocValuesScoring.java lucene-3622/lucene/src/test/org/apache/lucene/search/TestDocValuesScoring.java
--- lucene-clean-trunk/lucene/src/test/org/apache/lucene/search/TestDocValuesScoring.java	2011-12-11 17:18:24.932243485 -0500
+++ lucene-3622/lucene/src/test/org/apache/lucene/search/TestDocValuesScoring.java	2011-12-12 15:02:16.729605849 -0500
@@ -21,15 +21,15 @@
 
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
-import org.apache.lucene.document.IndexDocValuesField;
+import org.apache.lucene.document.DocValuesField;
 import org.apache.lucene.document.TextField;
 import org.apache.lucene.index.FieldInvertState;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.RandomIndexWriter;
 import org.apache.lucene.index.Term;
+import org.apache.lucene.index.DocValues.Source;
 import org.apache.lucene.index.IndexReader.AtomicReaderContext;
 import org.apache.lucene.index.codecs.Codec;
-import org.apache.lucene.index.values.IndexDocValues.Source;
 import org.apache.lucene.search.similarities.DefaultSimilarityProvider;
 import org.apache.lucene.search.similarities.Similarity;
 import org.apache.lucene.search.similarities.SimilarityProvider;
@@ -48,7 +48,7 @@
   private static final float SCORE_EPSILON = 0.001f; /* for comparing floats */
 
   public void testSimple() throws Exception {
-    assumeFalse("PreFlex codec cannot work with IndexDocValues!", 
+    assumeFalse("PreFlex codec cannot work with DocValues!", 
         "Lucene3x".equals(Codec.getDefault().getName()));
     
     Directory dir = newDirectory();
@@ -56,7 +56,7 @@
     Document doc = new Document();
     Field field = newField("foo", "", TextField.TYPE_UNSTORED);
     doc.add(field);
-    IndexDocValuesField dvField = new IndexDocValuesField("foo_boost");
+    DocValuesField dvField = new DocValuesField("foo_boost");
     doc.add(dvField);
     Field field2 = newField("bar", "", TextField.TYPE_UNSTORED);
     doc.add(field2);


diff -ruN -x .svn -x build lucene-clean-trunk/lucene/src/test/org/apache/lucene/search/TestSort.java lucene-3622/lucene/src/test/org/apache/lucene/search/TestSort.java
--- lucene-clean-trunk/lucene/src/test/org/apache/lucene/search/TestSort.java	2011-12-11 17:15:16.032240198 -0500
+++ lucene-3622/lucene/src/test/org/apache/lucene/search/TestSort.java	2011-12-12 15:02:11.753605763 -0500
@@ -28,7 +28,7 @@
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.FieldType;
-import org.apache.lucene.document.IndexDocValuesField;
+import org.apache.lucene.document.DocValuesField;
 import org.apache.lucene.document.StringField;
 import org.apache.lucene.document.TextField;
 import org.apache.lucene.index.CorruptIndexException;
@@ -42,7 +42,7 @@
 import org.apache.lucene.index.SlowMultiReaderWrapper;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.index.codecs.Codec;
-import org.apache.lucene.index.values.ValueType;
+import org.apache.lucene.index.DocValues;
 import org.apache.lucene.search.BooleanClause.Occur;
 import org.apache.lucene.search.FieldValueHitQueue.Entry;
 import org.apache.lucene.store.Directory;
@@ -121,18 +121,18 @@
     dirs.add(indexStore);
     RandomIndexWriter writer = new RandomIndexWriter(random, indexStore, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));
 
-    final ValueType stringDVType;
+    final DocValues.Type stringDVType;
     if (dvStringSorted) {
       // Index sorted
-      stringDVType = random.nextBoolean() ? ValueType.BYTES_VAR_SORTED : ValueType.BYTES_FIXED_SORTED;
+      stringDVType = random.nextBoolean() ? DocValues.Type.BYTES_VAR_SORTED : DocValues.Type.BYTES_FIXED_SORTED;
     } else {
       // Index non-sorted
       if (random.nextBoolean()) {
         // Fixed
-        stringDVType = random.nextBoolean() ? ValueType.BYTES_FIXED_STRAIGHT : ValueType.BYTES_FIXED_DEREF;
+        stringDVType = random.nextBoolean() ? DocValues.Type.BYTES_FIXED_STRAIGHT : DocValues.Type.BYTES_FIXED_DEREF;
       } else {
         // Var
-        stringDVType = random.nextBoolean() ? ValueType.BYTES_VAR_STRAIGHT : ValueType.BYTES_VAR_DEREF;
+        stringDVType = random.nextBoolean() ? DocValues.Type.BYTES_VAR_STRAIGHT : DocValues.Type.BYTES_VAR_DEREF;
       }
     }
 
@@ -148,21 +148,21 @@
         if (data[i][2] != null) {
           Field f = new StringField ("int", data[i][2]);
           if (supportsDocValues) {
-            f = IndexDocValuesField.build(f, ValueType.VAR_INTS);
+            f = DocValuesField.build(f, DocValues.Type.VAR_INTS);
           }
           doc.add(f);
         }
         if (data[i][3] != null) {
           Field f = new StringField ("float", data[i][3]);
           if (supportsDocValues) {
-            f = IndexDocValuesField.build(f, ValueType.FLOAT_32);
+            f = DocValuesField.build(f, DocValues.Type.FLOAT_32);
           }
           doc.add(f);
         }
         if (data[i][4] != null) {
           Field f = new StringField ("string", data[i][4]);
           if (supportsDocValues) {
-            f = IndexDocValuesField.build(f, stringDVType);
+            f = DocValuesField.build(f, stringDVType);
           }
           doc.add(f);
         }
@@ -172,7 +172,7 @@
         if (data[i][8] != null) {
           Field f = new StringField ("double", data[i][8]);
           if (supportsDocValues) {
-            f = IndexDocValuesField.build(f, ValueType.FLOAT_64);
+            f = DocValuesField.build(f, DocValues.Type.FLOAT_64);
           }
           doc.add(f);
         }
@@ -219,13 +219,13 @@
       //doc.add (new Field ("contents", Integer.toString(i), Field.Store.NO, Field.Index.ANALYZED));
       Field f = new StringField("string", num);
       if (supportsDocValues) {
-        f = IndexDocValuesField.build(f, ValueType.BYTES_VAR_SORTED);
+        f = DocValuesField.build(f, DocValues.Type.BYTES_VAR_SORTED);
       }
       doc.add (f);
       String num2 = getRandomCharString(getRandomNumber(1, 4), 48, 50);
       f = new StringField ("string2", num2);
       if (supportsDocValues) {
-        f = IndexDocValuesField.build(f, ValueType.BYTES_VAR_SORTED);
+        f = DocValuesField.build(f, DocValues.Type.BYTES_VAR_SORTED);
       }
       doc.add (f);
       doc.add (new Field ("tracer2", num2, onlyStored));
@@ -238,13 +238,13 @@
       //doc.add (new Field ("contents", Integer.toString(i), Field.Store.NO, Field.Index.ANALYZED));
       f = new StringField("string_fixed", numFixed);
       if (supportsDocValues) {
-        f = IndexDocValuesField.build(f, ValueType.BYTES_FIXED_SORTED);
+        f = DocValuesField.build(f, DocValues.Type.BYTES_FIXED_SORTED);
       }
       doc.add (f);
       String num2Fixed = getRandomCharString(fixedLen2, 48, 52);
       f = new StringField ("string2_fixed", num2Fixed);
       if (supportsDocValues) {
-        f = IndexDocValuesField.build(f, ValueType.BYTES_FIXED_SORTED);
+        f = DocValuesField.build(f, DocValues.Type.BYTES_FIXED_SORTED);
       }
       doc.add (f);
       doc.add (new Field ("tracer2_fixed", num2Fixed, onlyStored));


diff -ruN -x .svn -x build lucene-clean-trunk/lucene/src/test-framework/java/org/apache/lucene/analysis/BaseTokenStreamTestCase.java lucene-3622/lucene/src/test-framework/java/org/apache/lucene/analysis/BaseTokenStreamTestCase.java
--- lucene-clean-trunk/lucene/src/test-framework/java/org/apache/lucene/analysis/BaseTokenStreamTestCase.java	2011-12-12 08:15:49.481181159 -0500
+++ lucene-3622/lucene/src/test-framework/java/org/apache/lucene/analysis/BaseTokenStreamTestCase.java	2011-11-04 11:34:34.084494177 -0400
@@ -135,10 +135,6 @@
         assertTrue("startOffset must be >= 0", offsetAtt.startOffset() >= 0);
         assertTrue("endOffset must be >= 0", offsetAtt.endOffset() >= 0);
         assertTrue("endOffset must be >= startOffset", offsetAtt.endOffset() >= offsetAtt.startOffset());
-        if (finalOffset != null) {
-          assertTrue("startOffset must be <= finalOffset", offsetAtt.startOffset() <= finalOffset.intValue());
-          assertTrue("endOffset must be <= finalOffset", offsetAtt.endOffset() <= finalOffset.intValue());
-        }
       }
       if (posIncrAtt != null) {
         assertTrue("posIncrement must be >= 0", posIncrAtt.getPositionIncrement() >= 0);


diff -ruN -x .svn -x build lucene-clean-trunk/lucene/src/test-framework/java/org/apache/lucene/index/codecs/mocksep/MockSepDocValuesFormat.java lucene-3622/lucene/src/test-framework/java/org/apache/lucene/index/codecs/mocksep/MockSepDocValuesFormat.java
--- lucene-clean-trunk/lucene/src/test-framework/java/org/apache/lucene/index/codecs/mocksep/MockSepDocValuesFormat.java	2011-12-06 18:45:04.532810991 -0500
+++ lucene-3622/lucene/src/test-framework/java/org/apache/lucene/index/codecs/mocksep/MockSepDocValuesFormat.java	2011-12-10 10:29:15.290311359 -0500
@@ -25,7 +25,7 @@
 import org.apache.lucene.index.SegmentReadState;
 import org.apache.lucene.index.codecs.DocValuesFormat;
 import org.apache.lucene.index.codecs.PerDocConsumer;
-import org.apache.lucene.index.codecs.PerDocValues;
+import org.apache.lucene.index.codecs.PerDocProducer;
 import org.apache.lucene.index.codecs.sep.SepDocValuesConsumer;
 import org.apache.lucene.index.codecs.sep.SepDocValuesProducer;
 import org.apache.lucene.store.Directory;
@@ -43,7 +43,7 @@
   }
 
   @Override
-  public PerDocValues docsProducer(SegmentReadState state) throws IOException {
+  public PerDocProducer docsProducer(SegmentReadState state) throws IOException {
     return new SepDocValuesProducer(state);
   }
 


diff -ruN -x .svn -x build lucene-clean-trunk/lucene/src/test-framework/java/org/apache/lucene/index/RandomIndexWriter.java lucene-3622/lucene/src/test-framework/java/org/apache/lucene/index/RandomIndexWriter.java
--- lucene-clean-trunk/lucene/src/test-framework/java/org/apache/lucene/index/RandomIndexWriter.java	2011-12-09 08:23:19.936675178 -0500
+++ lucene-3622/lucene/src/test-framework/java/org/apache/lucene/index/RandomIndexWriter.java	2011-12-10 16:53:06.334712432 -0500
@@ -24,11 +24,11 @@
 
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.MockAnalyzer;
-import org.apache.lucene.document.IndexDocValuesField;
+import org.apache.lucene.document.DocValuesField;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.index.IndexWriter; // javadoc
 import org.apache.lucene.index.codecs.Codec;
-import org.apache.lucene.index.values.ValueType;
+import org.apache.lucene.index.DocValues;
 import org.apache.lucene.search.Query;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.util.BytesRef;
@@ -167,12 +167,12 @@
   
   private void randomPerDocFieldValues(Random random, Document doc) {
     
-    ValueType[] values = ValueType.values();
-    ValueType type = values[random.nextInt(values.length)];
+    DocValues.Type[] values = DocValues.Type.values();
+    DocValues.Type type = values[random.nextInt(values.length)];
     String name = "random_" + type.name() + "" + docValuesFieldPrefix;
     if ("Lucene3x".equals(codec.getName()) || doc.getField(name) != null)
         return;
-    IndexDocValuesField docValuesField = new IndexDocValuesField(name);
+    DocValuesField docValuesField = new DocValuesField(name);
     switch (type) {
     case BYTES_FIXED_DEREF:
     case BYTES_FIXED_STRAIGHT:


diff -ruN -x .svn -x build lucene-clean-trunk/lucene/src/test-framework/java/org/apache/lucene/util/_TestUtil.java lucene-3622/lucene/src/test-framework/java/org/apache/lucene/util/_TestUtil.java
--- lucene-clean-trunk/lucene/src/test-framework/java/org/apache/lucene/util/_TestUtil.java	2011-12-12 11:04:56.277357860 -0500
+++ lucene-3622/lucene/src/test-framework/java/org/apache/lucene/util/_TestUtil.java	2011-12-09 08:23:09.864675003 -0500
@@ -440,7 +440,7 @@
   /** Adds field info for a Document. */
   public static void add(Document doc, FieldInfos fieldInfos) {
     for (IndexableField field : doc) {
-      fieldInfos.addOrUpdate(field.name(), field.fieldType(), false, field.docValuesType());
+      fieldInfos.addOrUpdate(field.name(), field.fieldType());
     }
   }
   


diff -ruN -x .svn -x build lucene-clean-trunk/modules/analysis/common/src/java/org/apache/lucene/analysis/ngram/EdgeNGramTokenFilter.java lucene-3622/modules/analysis/common/src/java/org/apache/lucene/analysis/ngram/EdgeNGramTokenFilter.java
--- lucene-clean-trunk/modules/analysis/common/src/java/org/apache/lucene/analysis/ngram/EdgeNGramTokenFilter.java	2011-12-12 09:08:48.177236514 -0500
+++ lucene-3622/modules/analysis/common/src/java/org/apache/lucene/analysis/ngram/EdgeNGramTokenFilter.java	2011-05-08 10:52:09.620078711 -0400
@@ -71,8 +71,6 @@
   private int curTermLength;
   private int curGramSize;
   private int tokStart;
-  private int tokEnd; // only used if the length changed before this filter
-  private boolean hasIllegalOffsets; // only if the length changed before this filter
   
   private final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);
   private final OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);
@@ -128,10 +126,6 @@
           curTermLength = termAtt.length();
           curGramSize = minGram;
           tokStart = offsetAtt.startOffset();
-          tokEnd = offsetAtt.endOffset();
-          // if length by start + end offsets doesn't match the term text then assume
-          // this is a synonym and don't adjust the offsets.
-          hasIllegalOffsets = (tokStart + curTermLength) != tokEnd;
         }
       }
       if (curGramSize <= maxGram) {
@@ -141,11 +135,7 @@
           int start = side == Side.FRONT ? 0 : curTermLength - curGramSize;
           int end = start + curGramSize;
           clearAttributes();
-          if (hasIllegalOffsets) {
-            offsetAtt.setOffset(tokStart, tokEnd);
-          } else {
-            offsetAtt.setOffset(tokStart + start, tokStart + end);
-          }
+          offsetAtt.setOffset(tokStart + start, tokStart + end);
           termAtt.copyBuffer(curTermBuffer, start, curGramSize);
           curGramSize++;
           return true;


diff -ruN -x .svn -x build lucene-clean-trunk/modules/analysis/common/src/java/org/apache/lucene/analysis/ngram/NGramTokenFilter.java lucene-3622/modules/analysis/common/src/java/org/apache/lucene/analysis/ngram/NGramTokenFilter.java
--- lucene-clean-trunk/modules/analysis/common/src/java/org/apache/lucene/analysis/ngram/NGramTokenFilter.java	2011-12-12 09:13:56.353241881 -0500
+++ lucene-3622/modules/analysis/common/src/java/org/apache/lucene/analysis/ngram/NGramTokenFilter.java	2011-05-08 10:52:09.620078711 -0400
@@ -38,8 +38,6 @@
   private int curGramSize;
   private int curPos;
   private int tokStart;
-  private int tokEnd; // only used if the length changed before this filter
-  private boolean hasIllegalOffsets; // only if the length changed before this filter
   
   private final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);
   private final OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);
@@ -83,21 +81,13 @@
           curGramSize = minGram;
           curPos = 0;
           tokStart = offsetAtt.startOffset();
-          tokEnd = offsetAtt.endOffset();
-          // if length by start + end offsets doesn't match the term text then assume
-          // this is a synonym and don't adjust the offsets.
-          hasIllegalOffsets = (tokStart + curTermLength) != tokEnd;
         }
       }
       while (curGramSize <= maxGram) {
         while (curPos+curGramSize <= curTermLength) {     // while there is input
           clearAttributes();
           termAtt.copyBuffer(curTermBuffer, curPos, curGramSize);
-          if (hasIllegalOffsets) {
-            offsetAtt.setOffset(tokStart, tokEnd);
-          } else {
-            offsetAtt.setOffset(tokStart + curPos, tokStart + curPos + curGramSize);
-          }
+          offsetAtt.setOffset(tokStart + curPos, tokStart + curPos + curGramSize);
           curPos++;
           return true;
         }


diff -ruN -x .svn -x build lucene-clean-trunk/modules/analysis/common/src/java/org/apache/lucene/analysis/util/CharTokenizer.java lucene-3622/modules/analysis/common/src/java/org/apache/lucene/analysis/util/CharTokenizer.java
--- lucene-clean-trunk/modules/analysis/common/src/java/org/apache/lucene/analysis/util/CharTokenizer.java	2011-12-12 12:14:29.441430533 -0500
+++ lucene-3622/modules/analysis/common/src/java/org/apache/lucene/analysis/util/CharTokenizer.java	2011-11-02 10:34:14.648585064 -0400
@@ -144,7 +144,6 @@
     clearAttributes();
     int length = 0;
     int start = -1; // this variable is always initialized
-    int end = -1;
     char[] buffer = termAtt.buffer();
     while (true) {
       if (bufferIndex >= dataLen) {
@@ -163,18 +162,15 @@
       }
       // use CharacterUtils here to support < 3.1 UTF-16 code unit behavior if the char based methods are gone
       final int c = charUtils.codePointAt(ioBuffer.getBuffer(), bufferIndex);
-      final int charCount = Character.charCount(c);
-      bufferIndex += charCount;
+      bufferIndex += Character.charCount(c);
 
       if (isTokenChar(c)) {               // if it's a token char
         if (length == 0) {                // start of token
           assert start == -1;
-          start = offset + bufferIndex - charCount;
-          end = start;
+          start = offset + bufferIndex - 1;
         } else if (length >= buffer.length-1) { // check if a supplementary could run out of bounds
           buffer = termAtt.resizeBuffer(2+length); // make sure a supplementary fits in the buffer
         }
-        end += charCount;
         length += Character.toChars(normalize(c), buffer, length); // buffer it, normalized
         if (length >= MAX_WORD_LEN) // buffer overflow! make sure to check for >= surrogate pair could break == test
           break;
@@ -184,7 +180,7 @@
 
     termAtt.setLength(length);
     assert start != -1;
-    offsetAtt.setOffset(correctOffset(start), finalOffset = correctOffset(end));
+    offsetAtt.setOffset(correctOffset(start), finalOffset = correctOffset(start+length));
     return true;
     
   }


diff -ruN -x .svn -x build lucene-clean-trunk/modules/analysis/common/src/test/org/apache/lucene/analysis/core/TestDuelingAnalyzers.java lucene-3622/modules/analysis/common/src/test/org/apache/lucene/analysis/core/TestDuelingAnalyzers.java
--- lucene-clean-trunk/modules/analysis/common/src/test/org/apache/lucene/analysis/core/TestDuelingAnalyzers.java	2011-12-12 11:20:56.445374579 -0500
+++ lucene-3622/modules/analysis/common/src/test/org/apache/lucene/analysis/core/TestDuelingAnalyzers.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,123 +0,0 @@
-package org.apache.lucene.analysis.core;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.Reader;
-import java.io.StringReader;
-
-import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.analysis.MockAnalyzer;
-import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
-import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
-import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;
-import org.apache.lucene.util.LuceneTestCase;
-import org.apache.lucene.util._TestUtil;
-import org.apache.lucene.util.automaton.Automaton;
-import org.apache.lucene.util.automaton.BasicOperations;
-import org.apache.lucene.util.automaton.CharacterRunAutomaton;
-import org.apache.lucene.util.automaton.State;
-import org.apache.lucene.util.automaton.Transition;
-
-/**
- * Compares MockTokenizer (which is simple with no optimizations) with equivalent 
- * core tokenizers (that have optimizations like buffering).
- * 
- * Any tests here need to probably consider unicode version of the JRE (it could
- * cause false fails).
- */
-public class TestDuelingAnalyzers extends LuceneTestCase {
-  private CharacterRunAutomaton jvmLetter;
-  
-  public void setUp() throws Exception {
-    super.setUp();
-    // build an automaton matching this jvm's letter definition
-    State initial = new State();
-    State accept = new State();
-    accept.setAccept(true);
-    for (int i = 0; i <= 0x10FFFF; i++) {
-      if (Character.isLetter(i)) {
-        initial.addTransition(new Transition(i, i, accept));
-      }
-    }
-    Automaton single = new Automaton(initial);
-    single.reduce();
-    Automaton repeat = BasicOperations.repeat(single);
-    jvmLetter = new CharacterRunAutomaton(repeat);
-  }
-  
-  public void testLetterAscii() throws Exception {
-    Analyzer left = new MockAnalyzer(random, jvmLetter, false);
-    Analyzer right = new Analyzer() {
-      @Override
-      protected TokenStreamComponents createComponents(String fieldName, Reader reader) {
-        Tokenizer tokenizer = new LetterTokenizer(TEST_VERSION_CURRENT, reader);
-        return new TokenStreamComponents(tokenizer, tokenizer);
-      }
-    };
-    for (int i = 0; i < 10000; i++) {
-      String s = _TestUtil.randomSimpleString(random);
-      assertEquals(s, left.tokenStream("foo", new StringReader(s)), 
-                   right.tokenStream("foo", new StringReader(s)));
-    }
-  }
-  
-  public void testLetterUnicode() throws Exception {
-    Analyzer left = new MockAnalyzer(random, jvmLetter, false);
-    Analyzer right = new Analyzer() {
-      @Override
-      protected TokenStreamComponents createComponents(String fieldName, Reader reader) {
-        Tokenizer tokenizer = new LetterTokenizer(TEST_VERSION_CURRENT, reader);
-        return new TokenStreamComponents(tokenizer, tokenizer);
-      }
-    };
-    for (int i = 0; i < 10000; i++) {
-      String s = _TestUtil.randomUnicodeString(random);
-      assertEquals(s, left.tokenStream("foo", new StringReader(s)), 
-                   right.tokenStream("foo", new StringReader(s)));
-    }
-  }
-  
-  // we only check a few core attributes here.
-  // TODO: test other things
-  public void assertEquals(String s, TokenStream left, TokenStream right) throws Exception {
-    left.reset();
-    right.reset();
-    CharTermAttribute leftTerm = left.addAttribute(CharTermAttribute.class);
-    CharTermAttribute rightTerm = right.addAttribute(CharTermAttribute.class);
-    OffsetAttribute leftOffset = left.addAttribute(OffsetAttribute.class);
-    OffsetAttribute rightOffset = right.addAttribute(OffsetAttribute.class);
-    PositionIncrementAttribute leftPos = left.addAttribute(PositionIncrementAttribute.class);
-    PositionIncrementAttribute rightPos = right.addAttribute(PositionIncrementAttribute.class);
-    
-    while (left.incrementToken()) {
-      assertTrue("wrong number of tokens for input: " + s, right.incrementToken());
-      assertEquals("wrong term text for input: " + s, leftTerm.toString(), rightTerm.toString());
-      assertEquals("wrong position for input: " + s, leftPos.getPositionIncrement(), rightPos.getPositionIncrement());
-      assertEquals("wrong start offset for input: " + s, leftOffset.startOffset(), rightOffset.startOffset());
-      assertEquals("wrong end offset for input: " + s, leftOffset.endOffset(), rightOffset.endOffset());
-    };
-    assertFalse("wrong number of tokens for input: " + s, right.incrementToken());
-    left.end();
-    right.end();
-    assertEquals("wrong final offset for input: " + s, leftOffset.endOffset(), rightOffset.endOffset());
-    left.close();
-    right.close();
-  }
-}


diff -ruN -x .svn -x build lucene-clean-trunk/modules/analysis/common/src/test/org/apache/lucene/analysis/ngram/EdgeNGramTokenFilterTest.java lucene-3622/modules/analysis/common/src/test/org/apache/lucene/analysis/ngram/EdgeNGramTokenFilterTest.java
--- lucene-clean-trunk/modules/analysis/common/src/test/org/apache/lucene/analysis/ngram/EdgeNGramTokenFilterTest.java	2011-12-12 09:09:45.525237513 -0500
+++ lucene-3622/modules/analysis/common/src/test/org/apache/lucene/analysis/ngram/EdgeNGramTokenFilterTest.java	2011-11-02 10:34:14.632585067 -0400
@@ -17,16 +17,11 @@
  * limitations under the License.
  */
 
-import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.MockTokenizer;
-import org.apache.lucene.analysis.TokenFilter;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
-import org.apache.lucene.analysis.Tokenizer;
 import org.apache.lucene.analysis.core.WhitespaceTokenizer;
-import org.apache.lucene.analysis.miscellaneous.ASCIIFoldingFilter;
 
-import java.io.Reader;
 import java.io.StringReader;
 
 /**
@@ -109,24 +104,4 @@
     tokenizer.reset(new StringReader("abcde"));
     assertTokenStreamContents(filter, new String[]{"a","ab","abc"}, new int[]{0,0,0}, new int[]{1,2,3});
   }
-  
-  // LUCENE-3642
-  // EdgeNgram blindly adds term length to offset, but this can take things out of bounds
-  // wrt original text if a previous filter increases the length of the word (in this case æ -> ae)
-  // so in this case we behave like WDF, and preserve any modified offsets
-  public void testInvalidOffsets() throws Exception {
-    Analyzer analyzer = new Analyzer() {
-      @Override
-      protected TokenStreamComponents createComponents(String fieldName, Reader reader) {
-        Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);
-        TokenFilter filters = new ASCIIFoldingFilter(tokenizer);
-        filters = new EdgeNGramTokenFilter(filters, EdgeNGramTokenFilter.Side.FRONT, 2, 15);
-        return new TokenStreamComponents(tokenizer, filters);
-      }
-    };
-    assertAnalyzesTo(analyzer, "mosfellsbær",
-        new String[] { "mo", "mos", "mosf", "mosfe", "mosfel", "mosfell", "mosfells", "mosfellsb", "mosfellsba", "mosfellsbae", "mosfellsbaer" },
-        new int[]    {    0,     0,      0,       0,        0,         0,          0,           0,            0,             0,              0 },
-        new int[]    {   11,    11,     11,      11,       11,        11,         11,          11,           11,            11,             11 });
-  }
 }


diff -ruN -x .svn -x build lucene-clean-trunk/modules/analysis/common/src/test/org/apache/lucene/analysis/ngram/NGramTokenFilterTest.java lucene-3622/modules/analysis/common/src/test/org/apache/lucene/analysis/ngram/NGramTokenFilterTest.java
--- lucene-clean-trunk/modules/analysis/common/src/test/org/apache/lucene/analysis/ngram/NGramTokenFilterTest.java	2011-12-12 09:12:30.761240390 -0500
+++ lucene-3622/modules/analysis/common/src/test/org/apache/lucene/analysis/ngram/NGramTokenFilterTest.java	2011-11-02 10:34:14.632585067 -0400
@@ -17,16 +17,11 @@
  * limitations under the License.
  */
 
-import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.MockTokenizer;
-import org.apache.lucene.analysis.TokenFilter;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
-import org.apache.lucene.analysis.Tokenizer;
 import org.apache.lucene.analysis.core.WhitespaceTokenizer;
-import org.apache.lucene.analysis.miscellaneous.ASCIIFoldingFilter;
 
-import java.io.Reader;
 import java.io.StringReader;
 
 /**
@@ -98,24 +93,4 @@
       tokenizer.reset(new StringReader("abcde"));
       assertTokenStreamContents(filter, new String[]{"a","b","c","d","e"}, new int[]{0,1,2,3,4}, new int[]{1,2,3,4,5});
     }
-    
-    // LUCENE-3642
-    // EdgeNgram blindly adds term length to offset, but this can take things out of bounds
-    // wrt original text if a previous filter increases the length of the word (in this case æ -> ae)
-    // so in this case we behave like WDF, and preserve any modified offsets
-    public void testInvalidOffsets() throws Exception {
-      Analyzer analyzer = new Analyzer() {
-        @Override
-        protected TokenStreamComponents createComponents(String fieldName, Reader reader) {
-          Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);
-          TokenFilter filters = new ASCIIFoldingFilter(tokenizer);
-          filters = new NGramTokenFilter(filters, 2, 2);
-          return new TokenStreamComponents(tokenizer, filters);
-        }
-      };
-      assertAnalyzesTo(analyzer, "mosfellsbær",
-          new String[] { "mo", "os", "sf", "fe", "el", "ll", "ls", "sb", "ba", "ae", "er" },
-          new int[]    {    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0 },
-          new int[]    {   11,   11,   11,   11,   11,   11,   11,   11,   11,   11,   11 });
-    }
 }


diff -ruN -x .svn -x build lucene-clean-trunk/modules/analysis/common/src/test/org/apache/lucene/analysis/util/TestCharTokenizers.java lucene-3622/modules/analysis/common/src/test/org/apache/lucene/analysis/util/TestCharTokenizers.java
--- lucene-clean-trunk/modules/analysis/common/src/test/org/apache/lucene/analysis/util/TestCharTokenizers.java	2011-12-12 12:17:00.225433159 -0500
+++ lucene-3622/modules/analysis/common/src/test/org/apache/lucene/analysis/util/TestCharTokenizers.java	2011-05-08 10:52:09.420078711 -0400
@@ -18,17 +18,11 @@
  */
 
 import java.io.IOException;
-import java.io.Reader;
 import java.io.StringReader;
 
-import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
-import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.core.LetterTokenizer;
 import org.apache.lucene.analysis.core.LowerCaseTokenizer;
-import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
-import org.apache.lucene.util._TestUtil;
 
 
 /**
@@ -100,80 +94,4 @@
     Tokenizer tokenizer = new LowerCaseTokenizer(TEST_VERSION_CURRENT, new StringReader(builder.toString() + builder.toString()));
     assertTokenStreamContents(tokenizer, new String[] {builder.toString().toLowerCase(), builder.toString().toLowerCase()});
   }
-  
-  // LUCENE-3642: normalize SMP->BMP and check that offsets are correct
-  public void testCrossPlaneNormalization() throws IOException {
-    Analyzer analyzer = new Analyzer() {
-      @Override
-      protected TokenStreamComponents createComponents(String fieldName, Reader reader) {
-        Tokenizer tokenizer = new LetterTokenizer(TEST_VERSION_CURRENT, reader) {
-          @Override
-          protected int normalize(int c) {
-            if (c > 0xffff) {
-              return 'δ';
-            } else {
-              return c;
-            }
-          }
-        };
-        return new TokenStreamComponents(tokenizer, tokenizer);
-      }
-    };
-    int num = 10000 * RANDOM_MULTIPLIER;
-    for (int i = 0; i < num; i++) {
-      String s = _TestUtil.randomUnicodeString(random);
-      TokenStream ts = analyzer.tokenStream("foo", new StringReader(s));
-      ts.reset();
-      OffsetAttribute offsetAtt = ts.addAttribute(OffsetAttribute.class);
-      while (ts.incrementToken()) {
-        String highlightedText = s.substring(offsetAtt.startOffset(), offsetAtt.endOffset());
-        for (int j = 0, cp = 0; j < highlightedText.length(); j += Character.charCount(cp)) {
-          cp = highlightedText.codePointAt(j);
-          assertTrue("non-letter:" + Integer.toHexString(cp), Character.isLetter(cp));
-        }
-      }
-      ts.end();
-      ts.close();
-    }
-    // just for fun
-    checkRandomData(random, analyzer, num);
-  }
-  
-  // LUCENE-3642: normalize BMP->SMP and check that offsets are correct
-  public void testCrossPlaneNormalization2() throws IOException {
-    Analyzer analyzer = new Analyzer() {
-      @Override
-      protected TokenStreamComponents createComponents(String fieldName, Reader reader) {
-        Tokenizer tokenizer = new LetterTokenizer(TEST_VERSION_CURRENT, reader) {
-          @Override
-          protected int normalize(int c) {
-            if (c <= 0xffff) {
-              return 0x1043C;
-            } else {
-              return c;
-            }
-          }
-        };
-        return new TokenStreamComponents(tokenizer, tokenizer);
-      }
-    };
-    int num = 10000 * RANDOM_MULTIPLIER;
-    for (int i = 0; i < num; i++) {
-      String s = _TestUtil.randomUnicodeString(random);
-      TokenStream ts = analyzer.tokenStream("foo", new StringReader(s));
-      ts.reset();
-      OffsetAttribute offsetAtt = ts.addAttribute(OffsetAttribute.class);
-      while (ts.incrementToken()) {
-        String highlightedText = s.substring(offsetAtt.startOffset(), offsetAtt.endOffset());
-        for (int j = 0, cp = 0; j < highlightedText.length(); j += Character.charCount(cp)) {
-          cp = highlightedText.codePointAt(j);
-          assertTrue("non-letter:" + Integer.toHexString(cp), Character.isLetter(cp));
-        }
-      }
-      ts.end();
-      ts.close();
-    }
-    // just for fun
-    checkRandomData(random, analyzer, num);
-  }
 }


diff -ruN -x .svn -x build lucene-clean-trunk/modules/analysis/smartcn/src/java/org/apache/lucene/analysis/cn/smart/WordTokenFilter.java lucene-3622/modules/analysis/smartcn/src/java/org/apache/lucene/analysis/cn/smart/WordTokenFilter.java
--- lucene-clean-trunk/modules/analysis/smartcn/src/java/org/apache/lucene/analysis/cn/smart/WordTokenFilter.java	2011-12-12 09:28:34.565257174 -0500
+++ lucene-3622/modules/analysis/smartcn/src/java/org/apache/lucene/analysis/cn/smart/WordTokenFilter.java	2011-05-08 10:52:04.440078712 -0400
@@ -43,10 +43,6 @@
   private final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);
   private final OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);
   private final TypeAttribute typeAtt = addAttribute(TypeAttribute.class);
-  
-  private int tokStart; // only used if the length changed before this filter
-  private int tokEnd; // only used if the length changed before this filter
-  private boolean hasIllegalOffsets; // only if the length changed before this filter
 
   /**
    * Construct a new WordTokenizer.
@@ -63,11 +59,6 @@
     if (tokenIter == null || !tokenIter.hasNext()) {
       // there are no remaining tokens from the current sentence... are there more sentences?
       if (input.incrementToken()) {
-        tokStart = offsetAtt.startOffset();
-        tokEnd = offsetAtt.endOffset();
-        // if length by start + end offsets doesn't match the term text then assume
-        // this is a synonym and don't adjust the offsets.
-        hasIllegalOffsets = (tokStart + termAtt.length()) != tokEnd;
         // a new sentence is available: process it.
         tokenBuffer = wordSegmenter.segmentSentence(termAtt.toString(), offsetAtt.startOffset());
         tokenIter = tokenBuffer.iterator();
@@ -86,11 +77,7 @@
     // There are remaining tokens from the current sentence, return the next one. 
     SegToken nextWord = tokenIter.next();
     termAtt.copyBuffer(nextWord.charArray, 0, nextWord.charArray.length);
-    if (hasIllegalOffsets) {
-      offsetAtt.setOffset(tokStart, tokEnd);
-    } else {
-      offsetAtt.setOffset(nextWord.startOffset, nextWord.endOffset);
-    }
+    offsetAtt.setOffset(nextWord.startOffset, nextWord.endOffset);
     typeAtt.setType("word");
     return true;
   }


diff -ruN -x .svn -x build lucene-clean-trunk/modules/analysis/smartcn/src/test/org/apache/lucene/analysis/cn/smart/TestSmartChineseAnalyzer.java lucene-3622/modules/analysis/smartcn/src/test/org/apache/lucene/analysis/cn/smart/TestSmartChineseAnalyzer.java
--- lucene-clean-trunk/modules/analysis/smartcn/src/test/org/apache/lucene/analysis/cn/smart/TestSmartChineseAnalyzer.java	2011-12-12 09:26:29.885255004 -0500
+++ lucene-3622/modules/analysis/smartcn/src/test/org/apache/lucene/analysis/cn/smart/TestSmartChineseAnalyzer.java	2011-09-28 16:10:03.658773855 -0400
@@ -17,16 +17,11 @@
 
 package org.apache.lucene.analysis.cn.smart;
 
-import java.io.Reader;
 import java.io.StringReader;
 
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.analysis.MockTokenizer;
-import org.apache.lucene.analysis.TokenFilter;
 import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.miscellaneous.ASCIIFoldingFilter;
 import org.apache.lucene.util.Version;
 
 public class TestSmartChineseAnalyzer extends BaseTokenStreamTestCase {
@@ -201,24 +196,6 @@
     }
   }
   
-  // LUCENE-3642
-  public void testInvalidOffset() throws Exception {
-    Analyzer analyzer = new Analyzer() {
-      @Override
-      protected TokenStreamComponents createComponents(String fieldName, Reader reader) {
-        Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);
-        TokenFilter filters = new ASCIIFoldingFilter(tokenizer);
-        filters = new WordTokenFilter(filters);
-        return new TokenStreamComponents(tokenizer, filters);
-      }
-    };
-    
-    assertAnalyzesTo(analyzer, "mosfellsbær", 
-        new String[] { "mosfellsbaer" },
-        new int[]    { 0 },
-        new int[]    { 11 });
-  }
-  
   /** blast some random strings through the analyzer */
   public void testRandomStrings() throws Exception {
     checkRandomData(random, new SmartChineseAnalyzer(TEST_VERSION_CURRENT), 10000*RANDOM_MULTIPLIER);


diff -ruN -x .svn -x build lucene-clean-trunk/modules/grouping/CHANGES.txt lucene-3622/modules/grouping/CHANGES.txt
--- lucene-clean-trunk/modules/grouping/CHANGES.txt	2011-12-06 18:45:01.012810930 -0500
+++ lucene-3622/modules/grouping/CHANGES.txt	2011-12-09 11:16:02.576855638 -0500
@@ -17,4 +17,4 @@
 
 New features
 
-LUCENE-3496: Support grouping by IndexDocValues. (Martijn van Groningen)
\ No newline at end of file
+LUCENE-3496: Support grouping by DocValues. (Martijn van Groningen)
\ No newline at end of file


diff -ruN -x .svn -x build lucene-clean-trunk/modules/grouping/src/java/org/apache/lucene/search/grouping/dv/DVAllGroupHeadsCollector.java lucene-3622/modules/grouping/src/java/org/apache/lucene/search/grouping/dv/DVAllGroupHeadsCollector.java
--- lucene-clean-trunk/modules/grouping/src/java/org/apache/lucene/search/grouping/dv/DVAllGroupHeadsCollector.java	2011-12-06 18:45:01.036810930 -0500
+++ lucene-3622/modules/grouping/src/java/org/apache/lucene/search/grouping/dv/DVAllGroupHeadsCollector.java	2011-12-09 11:33:19.592873695 -0500
@@ -17,9 +17,9 @@
  * limitations under the License.
  */
 
+import org.apache.lucene.index.DocValues;
+import org.apache.lucene.index.DocValues.Type; // javadocs
 import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.values.IndexDocValues;
-import org.apache.lucene.index.values.ValueType;
 import org.apache.lucene.search.*;
 import org.apache.lucene.search.grouping.AbstractAllGroupHeadsCollector;
 import org.apache.lucene.util.BytesRef;
@@ -40,13 +40,13 @@
 
   final String groupField;
   final boolean diskResident;
-  final ValueType valueType;
+  final DocValues.Type valueType;
   final BytesRef scratchBytesRef = new BytesRef();
 
   IndexReader.AtomicReaderContext readerContext;
   Scorer scorer;
 
-  DVAllGroupHeadsCollector(String groupField, ValueType valueType, int numberOfSorts, boolean diskResident) {
+  DVAllGroupHeadsCollector(String groupField, DocValues.Type valueType, int numberOfSorts, boolean diskResident) {
     super(numberOfSorts);
     this.groupField = groupField;
     this.valueType = valueType;
@@ -59,12 +59,12 @@
    *
    * @param groupField      The field to group by
    * @param sortWithinGroup The sort within each group
-   * @param type The {@link ValueType} which is used to select a concrete implementation.
+   * @param type The {@link Type} which is used to select a concrete implementation.
    * @param diskResident Whether the values to group by should be disk resident
    * @return an <code>AbstractAllGroupHeadsCollector</code> instance based on the supplied arguments
    * @throws IOException If I/O related errors occur
    */
-  public static AbstractAllGroupHeadsCollector create(String groupField, Sort sortWithinGroup, ValueType type, boolean diskResident) throws IOException {
+  public static AbstractAllGroupHeadsCollector create(String groupField, Sort sortWithinGroup, DocValues.Type type, boolean diskResident) throws IOException {
     switch (type) {
       case VAR_INTS:
       case FIXED_INTS_8:
@@ -126,8 +126,8 @@
   public void setNextReader(IndexReader.AtomicReaderContext readerContext) throws IOException {
     this.readerContext = readerContext;
 
-    final IndexDocValues dv = readerContext.reader.docValues(groupField);
-    final IndexDocValues.Source dvSource;
+    final DocValues dv = readerContext.reader.docValues(groupField);
+    final DocValues.Source dvSource;
     if (dv != null) {
       dvSource = diskResident ? dv.getDirectSource() : dv.getSource();
     } else {
@@ -141,14 +141,14 @@
    *
    * @param source The idv source to be used by concrete implementations
    */
-  protected abstract void setDocValuesSources(IndexDocValues.Source source);
+  protected abstract void setDocValuesSources(DocValues.Source source);
 
   /**
    * @return The default source when no doc values are available.
    * @param readerContext The current reader context
    */
-  protected IndexDocValues.Source getDefaultSource(IndexReader.AtomicReaderContext readerContext) {
-    return IndexDocValues.getDefaultSource(valueType);
+  protected DocValues.Source getDefaultSource(IndexReader.AtomicReaderContext readerContext) {
+    return DocValues.getDefaultSource(valueType);
   }
 
   // A general impl that works for any group sort.
@@ -157,7 +157,7 @@
     private final Sort sortWithinGroup;
     private final Map<Comparable, GroupHead> groups;
 
-    GeneralAllGroupHeadsCollector(String groupField, ValueType valueType, Sort sortWithinGroup, boolean diskResident) throws IOException {
+    GeneralAllGroupHeadsCollector(String groupField, DocValues.Type valueType, Sort sortWithinGroup, boolean diskResident) throws IOException {
       super(groupField, valueType, sortWithinGroup.getSort().length, diskResident);
       this.sortWithinGroup = sortWithinGroup;
       groups = new HashMap<Comparable, GroupHead>();
@@ -211,9 +211,9 @@
 
     static class SortedBR extends GeneralAllGroupHeadsCollector {
 
-      private IndexDocValues.SortedSource source;
+      private DocValues.SortedSource source;
 
-      SortedBR(String groupField, ValueType valueType, Sort sortWithinGroup, boolean diskResident) throws IOException {
+      SortedBR(String groupField, DocValues.Type valueType, Sort sortWithinGroup, boolean diskResident) throws IOException {
         super(groupField, valueType, sortWithinGroup, diskResident);
       }
 
@@ -225,21 +225,21 @@
         return BytesRef.deepCopyOf((BytesRef) value);
       }
 
-      protected void setDocValuesSources(IndexDocValues.Source source) {
+      protected void setDocValuesSources(DocValues.Source source) {
         this.source = source.asSortedSource();
       }
 
       @Override
-      protected IndexDocValues.Source getDefaultSource(IndexReader.AtomicReaderContext readerContext) {
-        return IndexDocValues.getDefaultSortedSource(valueType, readerContext.reader.maxDoc());
+      protected DocValues.Source getDefaultSource(IndexReader.AtomicReaderContext readerContext) {
+        return DocValues.getDefaultSortedSource(valueType, readerContext.reader.maxDoc());
       }
     }
 
     static class BR extends GeneralAllGroupHeadsCollector {
 
-      private IndexDocValues.Source source;
+      private DocValues.Source source;
 
-      BR(String groupField, ValueType valueType, Sort sortWithinGroup, boolean diskResident) throws IOException {
+      BR(String groupField, DocValues.Type valueType, Sort sortWithinGroup, boolean diskResident) throws IOException {
         super(groupField, valueType, sortWithinGroup, diskResident);
       }
 
@@ -251,7 +251,7 @@
         return BytesRef.deepCopyOf((BytesRef) value);
       }
 
-      protected void setDocValuesSources(IndexDocValues.Source source) {
+      protected void setDocValuesSources(DocValues.Source source) {
         this.source = source;
       }
 
@@ -259,9 +259,9 @@
 
     static class Lng extends GeneralAllGroupHeadsCollector {
 
-      private IndexDocValues.Source source;
+      private DocValues.Source source;
 
-      Lng(String groupField, ValueType valueType, Sort sortWithinGroup, boolean diskResident) throws IOException {
+      Lng(String groupField, DocValues.Type valueType, Sort sortWithinGroup, boolean diskResident) throws IOException {
         super(groupField, valueType, sortWithinGroup, diskResident);
       }
 
@@ -273,16 +273,16 @@
         return value;
       }
 
-      protected void setDocValuesSources(IndexDocValues.Source source) {
+      protected void setDocValuesSources(DocValues.Source source) {
         this.source = source;
       }
     }
 
     static class Dbl extends GeneralAllGroupHeadsCollector {
 
-      private IndexDocValues.Source source;
+      private DocValues.Source source;
 
-      Dbl(String groupField, ValueType valueType, Sort sortWithinGroup, boolean diskResident) throws IOException {
+      Dbl(String groupField, DocValues.Type valueType, Sort sortWithinGroup, boolean diskResident) throws IOException {
         super(groupField, valueType, sortWithinGroup, diskResident);
       }
 
@@ -294,7 +294,7 @@
         return value;
       }
 
-      protected void setDocValuesSources(IndexDocValues.Source source) {
+      protected void setDocValuesSources(DocValues.Source source) {
         this.source = source;
       }
 


diff -ruN -x .svn -x build lucene-clean-trunk/modules/grouping/src/java/org/apache/lucene/search/grouping/dv/DVAllGroupsCollector.java lucene-3622/modules/grouping/src/java/org/apache/lucene/search/grouping/dv/DVAllGroupsCollector.java
--- lucene-clean-trunk/modules/grouping/src/java/org/apache/lucene/search/grouping/dv/DVAllGroupsCollector.java	2011-12-06 18:45:01.036810930 -0500
+++ lucene-3622/modules/grouping/src/java/org/apache/lucene/search/grouping/dv/DVAllGroupsCollector.java	2011-12-09 11:41:26.916882183 -0500
@@ -17,9 +17,9 @@
  * limitations under the License.
  */
 
+import org.apache.lucene.index.DocValues;
+import org.apache.lucene.index.DocValues.Type; // javadocs
 import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.values.IndexDocValues;
-import org.apache.lucene.index.values.ValueType;
 import org.apache.lucene.search.grouping.AbstractAllGroupsCollector;
 import org.apache.lucene.search.grouping.SentinelIntSet;
 import org.apache.lucene.util.BytesRef;
@@ -29,7 +29,7 @@
 
 /**
  * Implementation of {@link AbstractAllGroupsCollector} that groups documents based on
- * {@link IndexDocValues} fields.
+ * {@link DocValues} fields.
  *
  * @lucene.experimental
  */
@@ -39,20 +39,20 @@
 
   /**
    * Expert: Constructs a {@link DVAllGroupsCollector}.
-   * Selects and constructs the most optimal all groups collector implementation for grouping by {@link IndexDocValues}.
+   * Selects and constructs the most optimal all groups collector implementation for grouping by {@link DocValues}.
    * 
    *
    * @param groupField  The field to group by
-   * @param type The {@link ValueType} which is used to select a concrete implementation.
+   * @param type The {@link Type} which is used to select a concrete implementation.
    * @param diskResident Whether the values to group by should be disk resident
    * @param initialSize The initial allocation size of the
    *                    internal int set and group list
    *                    which should roughly match the total
    *                    number of expected unique groups. Be aware that the
    *                    heap usage is 4 bytes * initialSize. Not all concrete implementions use this!
-   * @return the most optimal all groups collector implementation for grouping by {@link IndexDocValues}
+   * @return the most optimal all groups collector implementation for grouping by {@link DocValues}
    */
-  public static DVAllGroupsCollector create(String groupField, ValueType type, boolean diskResident, int initialSize) {
+  public static DVAllGroupsCollector create(String groupField, DocValues.Type type, boolean diskResident, int initialSize) {
     switch (type) {
       case VAR_INTS:
       case FIXED_INTS_8:
@@ -78,25 +78,25 @@
 
   /**
    * Constructs a {@link DVAllGroupsCollector}.
-   * Selects and constructs the most optimal all groups collector implementation for grouping by {@link IndexDocValues}.
+   * Selects and constructs the most optimal all groups collector implementation for grouping by {@link DocValues}.
    * If implementations require an initial allocation size then this will be set to 128.
    *
    *
    * @param groupField  The field to group by
-   * @param type The {@link ValueType} which is used to select a concrete implementation.
+   * @param type The {@link Type} which is used to select a concrete implementation.
    * @param diskResident Wether the values to group by should be disk resident
-   * @return the most optimal all groups collector implementation for grouping by {@link IndexDocValues}
+   * @return the most optimal all groups collector implementation for grouping by {@link DocValues}
    */
-  public static DVAllGroupsCollector create(String groupField, ValueType type, boolean diskResident) {
+  public static DVAllGroupsCollector create(String groupField, DocValues.Type type, boolean diskResident) {
     return create(groupField, type, diskResident, DEFAULT_INITIAL_SIZE);
   }
 
   final String groupField;
-  final ValueType valueType;
+  final DocValues.Type valueType;
   final boolean diskResident;
   final Collection<GROUP_VALUE_TYPE> groups;
 
-  DVAllGroupsCollector(String groupField, ValueType valueType, boolean diskResident, Collection<GROUP_VALUE_TYPE> groups) {
+  DVAllGroupsCollector(String groupField, DocValues.Type valueType, boolean diskResident, Collection<GROUP_VALUE_TYPE> groups) {
     this.groupField = groupField;
     this.valueType = valueType;
     this.diskResident = diskResident;
@@ -105,8 +105,8 @@
 
   @Override
   public void setNextReader(IndexReader.AtomicReaderContext readerContext) throws IOException {
-    final IndexDocValues dv = readerContext.reader.docValues(groupField);
-    final IndexDocValues.Source dvSource;
+    final DocValues dv = readerContext.reader.docValues(groupField);
+    final DocValues.Source dvSource;
     if (dv != null) {
       dvSource = diskResident ? dv.getDirectSource() : dv.getSource();
     } else {
@@ -121,21 +121,21 @@
    * @param source The idv source to be used by concrete implementations
    * @param readerContext The current reader context
    */
-  protected abstract void setDocValuesSources(IndexDocValues.Source source, IndexReader.AtomicReaderContext readerContext);
+  protected abstract void setDocValuesSources(DocValues.Source source, IndexReader.AtomicReaderContext readerContext);
 
   /**
    * @return The default source when no doc values are available.
    * @param readerContext The current reader context
    */
-  protected IndexDocValues.Source getDefaultSource(IndexReader.AtomicReaderContext readerContext) {
-    return IndexDocValues.getDefaultSource(valueType);
+  protected DocValues.Source getDefaultSource(IndexReader.AtomicReaderContext readerContext) {
+    return DocValues.getDefaultSource(valueType);
   }
 
   static class Lng extends DVAllGroupsCollector<Long> {
 
-    private IndexDocValues.Source source;
+    private DocValues.Source source;
 
-    Lng(String groupField, ValueType valueType, boolean diskResident) {
+    Lng(String groupField, DocValues.Type valueType, boolean diskResident) {
       super(groupField, valueType, diskResident, new TreeSet<Long>());
     }
 
@@ -150,7 +150,7 @@
       return groups;
     }
 
-    protected void setDocValuesSources(IndexDocValues.Source source, IndexReader.AtomicReaderContext readerContext) {
+    protected void setDocValuesSources(DocValues.Source source, IndexReader.AtomicReaderContext readerContext) {
       this.source = source;
     }
 
@@ -158,9 +158,9 @@
 
   static class Dbl extends DVAllGroupsCollector<Double> {
 
-    private IndexDocValues.Source source;
+    private DocValues.Source source;
 
-    Dbl(String groupField, ValueType valueType, boolean diskResident) {
+    Dbl(String groupField, DocValues.Type valueType, boolean diskResident) {
       super(groupField, valueType, diskResident, new TreeSet<Double>());
     }
 
@@ -175,7 +175,7 @@
       return groups;
     }
 
-    protected void setDocValuesSources(IndexDocValues.Source source, IndexReader.AtomicReaderContext readerContext) {
+    protected void setDocValuesSources(DocValues.Source source, IndexReader.AtomicReaderContext readerContext) {
       this.source = source;
     }
 
@@ -185,9 +185,9 @@
 
     private final BytesRef spare = new BytesRef();
 
-    private IndexDocValues.Source source;
+    private DocValues.Source source;
 
-    BR(String groupField, ValueType valueType, boolean diskResident) {
+    BR(String groupField, DocValues.Type valueType, boolean diskResident) {
       super(groupField, valueType, diskResident, new TreeSet<BytesRef>());
     }
 
@@ -202,7 +202,7 @@
       return groups;
     }
 
-    protected void setDocValuesSources(IndexDocValues.Source source, IndexReader.AtomicReaderContext readerContext) {
+    protected void setDocValuesSources(DocValues.Source source, IndexReader.AtomicReaderContext readerContext) {
       this.source = source;
     }
 
@@ -213,9 +213,9 @@
     private final SentinelIntSet ordSet;
     private final BytesRef spare = new BytesRef();
 
-    private IndexDocValues.SortedSource source;
+    private DocValues.SortedSource source;
 
-    SortedBR(String groupField, ValueType valueType, boolean diskResident, int initialSize) {
+    SortedBR(String groupField, DocValues.Type valueType, boolean diskResident, int initialSize) {
       super(groupField, valueType, diskResident, new ArrayList<BytesRef>(initialSize));
       ordSet = new SentinelIntSet(initialSize, -1);
     }
@@ -233,7 +233,7 @@
       return groups;
     }
 
-    protected void setDocValuesSources(IndexDocValues.Source source, IndexReader.AtomicReaderContext readerContext) {
+    protected void setDocValuesSources(DocValues.Source source, IndexReader.AtomicReaderContext readerContext) {
       this.source = source.asSortedSource();
 
       ordSet.clear();
@@ -246,8 +246,8 @@
     }
 
     @Override
-    protected IndexDocValues.Source getDefaultSource(IndexReader.AtomicReaderContext readerContext) {
-      return IndexDocValues.getDefaultSortedSource(valueType, readerContext.reader.maxDoc());
+    protected DocValues.Source getDefaultSource(IndexReader.AtomicReaderContext readerContext) {
+      return DocValues.getDefaultSortedSource(valueType, readerContext.reader.maxDoc());
     }
 
   }


diff -ruN -x .svn -x build lucene-clean-trunk/modules/grouping/src/java/org/apache/lucene/search/grouping/dv/DVFirstPassGroupingCollector.java lucene-3622/modules/grouping/src/java/org/apache/lucene/search/grouping/dv/DVFirstPassGroupingCollector.java
--- lucene-clean-trunk/modules/grouping/src/java/org/apache/lucene/search/grouping/dv/DVFirstPassGroupingCollector.java	2011-12-06 18:45:01.036810930 -0500
+++ lucene-3622/modules/grouping/src/java/org/apache/lucene/search/grouping/dv/DVFirstPassGroupingCollector.java	2011-12-09 11:33:31.608873906 -0500
@@ -17,9 +17,9 @@
  * limitations under the License.
  */
 
+import org.apache.lucene.index.DocValues;
+import org.apache.lucene.index.DocValues.Type; // javadocs
 import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.values.IndexDocValues;
-import org.apache.lucene.index.values.ValueType;
 import org.apache.lucene.search.Sort;
 import org.apache.lucene.search.grouping.AbstractFirstPassGroupingCollector;
 import org.apache.lucene.util.BytesRef;
@@ -35,9 +35,9 @@
 
   final String groupField;
   final boolean diskResident;
-  final ValueType valueType;
+  final DocValues.Type valueType;
 
-  public static DVFirstPassGroupingCollector create(Sort groupSort, int topNGroups, String groupField, ValueType type, boolean diskResident) throws IOException {
+  public static DVFirstPassGroupingCollector create(Sort groupSort, int topNGroups, String groupField, DocValues.Type type, boolean diskResident) throws IOException {
     switch (type) {
       case VAR_INTS:
       case FIXED_INTS_8:
@@ -61,7 +61,7 @@
     }
   }
 
-  DVFirstPassGroupingCollector(Sort groupSort, int topNGroups, String groupField, boolean diskResident, ValueType valueType) throws IOException {
+  DVFirstPassGroupingCollector(Sort groupSort, int topNGroups, String groupField, boolean diskResident, DocValues.Type valueType) throws IOException {
     super(groupSort, topNGroups);
     this.groupField = groupField;
     this.diskResident = diskResident;
@@ -72,8 +72,8 @@
   public void setNextReader(IndexReader.AtomicReaderContext readerContext) throws IOException {
     super.setNextReader(readerContext);
 
-    final IndexDocValues dv = readerContext.reader.docValues(groupField);
-    final IndexDocValues.Source dvSource;
+    final DocValues dv = readerContext.reader.docValues(groupField);
+    final DocValues.Source dvSource;
     if (dv != null) {
       dvSource = diskResident ? dv.getDirectSource() : dv.getSource();
     } else {
@@ -87,21 +87,21 @@
    *
    * @param source The idv source to be used by concrete implementations
    */
-  protected abstract void setDocValuesSources(IndexDocValues.Source source);
+  protected abstract void setDocValuesSources(DocValues.Source source);
 
   /**
    * @return The default source when no doc values are available.
    * @param readerContext The current reader context
    */
-  protected IndexDocValues.Source getDefaultSource(IndexReader.AtomicReaderContext readerContext) {
-    return IndexDocValues.getDefaultSource(valueType);
+  protected DocValues.Source getDefaultSource(IndexReader.AtomicReaderContext readerContext) {
+    return DocValues.getDefaultSource(valueType);
   }
 
   static class Lng extends DVFirstPassGroupingCollector<Long> {
 
-    private IndexDocValues.Source source;
+    private DocValues.Source source;
 
-    Lng(Sort groupSort, int topNGroups, String groupField, boolean diskResident, ValueType type) throws IOException {
+    Lng(Sort groupSort, int topNGroups, String groupField, boolean diskResident, DocValues.Type type) throws IOException {
       super(groupSort, topNGroups, groupField, diskResident, type);
     }
 
@@ -113,16 +113,16 @@
       return groupValue;
     }
 
-    protected void setDocValuesSources(IndexDocValues.Source source) {
+    protected void setDocValuesSources(DocValues.Source source) {
       this.source = source;
     }
   }
 
   static class Dbl extends DVFirstPassGroupingCollector<Double> {
 
-    private IndexDocValues.Source source;
+    private DocValues.Source source;
 
-    Dbl(Sort groupSort, int topNGroups, String groupField, boolean diskResident, ValueType type) throws IOException {
+    Dbl(Sort groupSort, int topNGroups, String groupField, boolean diskResident, DocValues.Type type) throws IOException {
       super(groupSort, topNGroups, groupField, diskResident, type);
     }
 
@@ -134,17 +134,17 @@
       return groupValue;
     }
 
-    protected void setDocValuesSources(IndexDocValues.Source source) {
+    protected void setDocValuesSources(DocValues.Source source) {
       this.source = source;
     }
   }
 
   static class BR extends DVFirstPassGroupingCollector<BytesRef> {
 
-    private IndexDocValues.Source source;
+    private DocValues.Source source;
     private final BytesRef spare = new BytesRef();
 
-    BR(Sort groupSort, int topNGroups, String groupField, boolean diskResident, ValueType type) throws IOException {
+    BR(Sort groupSort, int topNGroups, String groupField, boolean diskResident, DocValues.Type type) throws IOException {
       super(groupSort, topNGroups, groupField, diskResident, type);
     }
 
@@ -162,17 +162,17 @@
     }
 
     @Override
-    protected void setDocValuesSources(IndexDocValues.Source source) {
+    protected void setDocValuesSources(DocValues.Source source) {
       this.source = source;
     }
   }
 
   static class SortedBR extends DVFirstPassGroupingCollector<BytesRef> {
 
-    private IndexDocValues.SortedSource sortedSource;
+    private DocValues.SortedSource sortedSource;
     private final BytesRef spare = new BytesRef();
 
-    SortedBR(Sort groupSort, int topNGroups, String groupField, boolean diskResident, ValueType type) throws IOException {
+    SortedBR(Sort groupSort, int topNGroups, String groupField, boolean diskResident, DocValues.Type type) throws IOException {
       super(groupSort, topNGroups, groupField, diskResident, type);
     }
 
@@ -192,13 +192,13 @@
     }
 
     @Override
-    protected void setDocValuesSources(IndexDocValues.Source source) {
+    protected void setDocValuesSources(DocValues.Source source) {
       this.sortedSource = source.asSortedSource();
     }
 
     @Override
-    protected IndexDocValues.Source getDefaultSource(IndexReader.AtomicReaderContext readerContext) {
-      return IndexDocValues.getDefaultSortedSource(valueType, readerContext.reader.maxDoc());
+    protected DocValues.Source getDefaultSource(IndexReader.AtomicReaderContext readerContext) {
+      return DocValues.getDefaultSortedSource(valueType, readerContext.reader.maxDoc());
     }
   }
 


diff -ruN -x .svn -x build lucene-clean-trunk/modules/grouping/src/java/org/apache/lucene/search/grouping/dv/DVSecondPassGroupingCollector.java lucene-3622/modules/grouping/src/java/org/apache/lucene/search/grouping/dv/DVSecondPassGroupingCollector.java
--- lucene-clean-trunk/modules/grouping/src/java/org/apache/lucene/search/grouping/dv/DVSecondPassGroupingCollector.java	2011-12-06 18:45:01.036810930 -0500
+++ lucene-3622/modules/grouping/src/java/org/apache/lucene/search/grouping/dv/DVSecondPassGroupingCollector.java	2011-12-09 11:41:39.980882410 -0500
@@ -17,9 +17,9 @@
  * limitations under the License.
  */
 
+import org.apache.lucene.index.DocValues;
+import org.apache.lucene.index.DocValues.Type; // javadocs
 import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.values.IndexDocValues;
-import org.apache.lucene.index.values.ValueType;
 import org.apache.lucene.search.Sort;
 import org.apache.lucene.search.grouping.AbstractSecondPassGroupingCollector;
 import org.apache.lucene.search.grouping.SearchGroup;
@@ -38,11 +38,11 @@
 
   /**
    * Constructs a {@link DVSecondPassGroupingCollector}.
-   * Selects and constructs the most optimal second pass collector implementation for grouping by {@link IndexDocValues}.
+   * Selects and constructs the most optimal second pass collector implementation for grouping by {@link DocValues}.
    *
    * @param groupField      The field to group by
    * @param diskResident    Whether the values to group by should be disk resident
-   * @param type            The {@link org.apache.lucene.index.values.ValueType} which is used to select a concrete implementation.
+   * @param type            The {@link Type} which is used to select a concrete implementation.
    * @param searchGroups    The groups from the first phase search
    * @param groupSort       The sort used for the groups
    * @param withinGroupSort The sort used for documents inside a group
@@ -50,13 +50,13 @@
    * @param getScores       Whether to include scores for the documents inside a group
    * @param getMaxScores    Whether to keep track of the higest score per group
    * @param fillSortFields  Whether to include the sort values
-   * @return the most optimal second pass collector implementation for grouping by {@link IndexDocValues}
+   * @return the most optimal second pass collector implementation for grouping by {@link DocValues}
    * @throws IOException    If I/O related errors occur
    */
   @SuppressWarnings("unchecked")
   public static DVSecondPassGroupingCollector create(String groupField,
                                                      boolean diskResident,
-                                                     ValueType type,
+                                                     DocValues.Type type,
                                                      Collection<SearchGroup> searchGroups,
                                                      Sort groupSort,
                                                      Sort withinGroupSort,
@@ -92,10 +92,10 @@
   }
 
   final String groupField;
-  final ValueType valueType;
+  final DocValues.Type valueType;
   final boolean diskResident;
 
-  DVSecondPassGroupingCollector(String groupField, ValueType valueType, boolean diskResident, Collection<SearchGroup<GROUP_VALUE>> searchGroups, Sort groupSort, Sort withinGroupSort, int maxDocsPerGroup, boolean getScores, boolean getMaxScores, boolean fillSortFields) throws IOException {
+  DVSecondPassGroupingCollector(String groupField, DocValues.Type valueType, boolean diskResident, Collection<SearchGroup<GROUP_VALUE>> searchGroups, Sort groupSort, Sort withinGroupSort, int maxDocsPerGroup, boolean getScores, boolean getMaxScores, boolean fillSortFields) throws IOException {
     super(searchGroups, groupSort, withinGroupSort, maxDocsPerGroup, getScores, getMaxScores, fillSortFields);
     this.groupField = groupField;
     this.valueType = valueType;
@@ -106,8 +106,8 @@
   public void setNextReader(IndexReader.AtomicReaderContext readerContext) throws IOException {
     super.setNextReader(readerContext);
 
-    final IndexDocValues dv = readerContext.reader.docValues(groupField);
-    final IndexDocValues.Source dvSource;
+    final DocValues dv = readerContext.reader.docValues(groupField);
+    final DocValues.Source dvSource;
     if (dv != null) {
       dvSource = diskResident ? dv.getDirectSource() : dv.getSource();
     } else {
@@ -122,21 +122,21 @@
    * @param source The idv source to be used by concrete implementations
    * @param readerContext The current reader context
    */
-  protected abstract void setDocValuesSources(IndexDocValues.Source source, IndexReader.AtomicReaderContext readerContext);
+  protected abstract void setDocValuesSources(DocValues.Source source, IndexReader.AtomicReaderContext readerContext);
 
   /**
    * @return The default source when no doc values are available.
    * @param readerContext The current reader context
    */
-  protected IndexDocValues.Source getDefaultSource(IndexReader.AtomicReaderContext readerContext) {
-    return IndexDocValues.getDefaultSource(valueType);
+  protected DocValues.Source getDefaultSource(IndexReader.AtomicReaderContext readerContext) {
+    return DocValues.getDefaultSource(valueType);
   }
 
   static class Lng extends DVSecondPassGroupingCollector<Long> {
 
-    private IndexDocValues.Source source;
+    private DocValues.Source source;
 
-    Lng(String groupField, ValueType valueType, boolean diskResident, Collection<SearchGroup<Long>> searchGroups, Sort groupSort, Sort withinGroupSort, int maxDocsPerGroup, boolean getScores, boolean getMaxScores, boolean fillSortFields) throws IOException {
+    Lng(String groupField, DocValues.Type valueType, boolean diskResident, Collection<SearchGroup<Long>> searchGroups, Sort groupSort, Sort withinGroupSort, int maxDocsPerGroup, boolean getScores, boolean getMaxScores, boolean fillSortFields) throws IOException {
       super(groupField, valueType, diskResident, searchGroups, groupSort, withinGroupSort, maxDocsPerGroup, getScores, getMaxScores, fillSortFields);
     }
 
@@ -144,16 +144,16 @@
       return groupMap.get(source.getInt(doc));
     }
 
-    protected void setDocValuesSources(IndexDocValues.Source source, IndexReader.AtomicReaderContext readerContext) {
+    protected void setDocValuesSources(DocValues.Source source, IndexReader.AtomicReaderContext readerContext) {
       this.source = source;
     }
   }
 
   static class Dbl extends DVSecondPassGroupingCollector<Double> {
 
-    private IndexDocValues.Source source;
+    private DocValues.Source source;
 
-    Dbl(String groupField, ValueType valueType, boolean diskResident, Collection<SearchGroup<Double>> searchGroups, Sort groupSort, Sort withinGroupSort, int maxDocsPerGroup, boolean getScores, boolean getMaxScores, boolean fillSortFields) throws IOException {
+    Dbl(String groupField, DocValues.Type valueType, boolean diskResident, Collection<SearchGroup<Double>> searchGroups, Sort groupSort, Sort withinGroupSort, int maxDocsPerGroup, boolean getScores, boolean getMaxScores, boolean fillSortFields) throws IOException {
       super(groupField, valueType, diskResident, searchGroups, groupSort, withinGroupSort, maxDocsPerGroup, getScores, getMaxScores, fillSortFields);
     }
 
@@ -161,17 +161,17 @@
       return groupMap.get(source.getFloat(doc));
     }
 
-    protected void setDocValuesSources(IndexDocValues.Source source, IndexReader.AtomicReaderContext readerContext) {
+    protected void setDocValuesSources(DocValues.Source source, IndexReader.AtomicReaderContext readerContext) {
       this.source = source;
     }
   }
 
   static class BR extends DVSecondPassGroupingCollector<BytesRef> {
 
-    private IndexDocValues.Source source;
+    private DocValues.Source source;
     private final BytesRef spare = new BytesRef();
 
-    BR(String groupField, ValueType valueType, boolean diskResident, Collection<SearchGroup<BytesRef>> searchGroups, Sort groupSort, Sort withinGroupSort, int maxDocsPerGroup, boolean getScores, boolean getMaxScores, boolean fillSortFields) throws IOException {
+    BR(String groupField, DocValues.Type valueType, boolean diskResident, Collection<SearchGroup<BytesRef>> searchGroups, Sort groupSort, Sort withinGroupSort, int maxDocsPerGroup, boolean getScores, boolean getMaxScores, boolean fillSortFields) throws IOException {
       super(groupField, valueType, diskResident, searchGroups, groupSort, withinGroupSort, maxDocsPerGroup, getScores, getMaxScores, fillSortFields);
     }
 
@@ -180,7 +180,7 @@
     }
 
     @Override
-    protected void setDocValuesSources(IndexDocValues.Source source, IndexReader.AtomicReaderContext readerContext) {
+    protected void setDocValuesSources(DocValues.Source source, IndexReader.AtomicReaderContext readerContext) {
       this.source = source;
     }
 
@@ -188,12 +188,12 @@
 
   static class SortedBR extends DVSecondPassGroupingCollector<BytesRef> {
 
-    private IndexDocValues.SortedSource source;
+    private DocValues.SortedSource source;
     private final BytesRef spare = new BytesRef();
     private final SentinelIntSet ordSet;
 
     @SuppressWarnings("unchecked")
-    SortedBR(String groupField,  ValueType valueType, boolean diskResident, Collection<SearchGroup<BytesRef>> searchGroups, Sort groupSort, Sort withinGroupSort, int maxDocsPerGroup, boolean getScores, boolean getMaxScores, boolean fillSortFields) throws IOException {
+    SortedBR(String groupField,  DocValues.Type valueType, boolean diskResident, Collection<SearchGroup<BytesRef>> searchGroups, Sort groupSort, Sort withinGroupSort, int maxDocsPerGroup, boolean getScores, boolean getMaxScores, boolean fillSortFields) throws IOException {
       super(groupField, valueType, diskResident, searchGroups, groupSort, withinGroupSort, maxDocsPerGroup, getScores, getMaxScores, fillSortFields);
       ordSet = new SentinelIntSet(groupMap.size(), -1);
       groupDocs = (SearchGroupDocs<BytesRef>[]) new SearchGroupDocs[ordSet.keys.length];
@@ -209,7 +209,7 @@
     }
 
     @Override
-    protected void setDocValuesSources(IndexDocValues.Source source, IndexReader.AtomicReaderContext readerContext) {
+    protected void setDocValuesSources(DocValues.Source source, IndexReader.AtomicReaderContext readerContext) {
       this.source = source.asSortedSource();
 
       ordSet.clear();
@@ -222,8 +222,8 @@
     }
 
     @Override
-    protected IndexDocValues.Source getDefaultSource(IndexReader.AtomicReaderContext readerContext) {
-      return IndexDocValues.getDefaultSortedSource(valueType, readerContext.reader.maxDoc());
+    protected DocValues.Source getDefaultSource(IndexReader.AtomicReaderContext readerContext) {
+      return DocValues.getDefaultSortedSource(valueType, readerContext.reader.maxDoc());
     }
   }
 


diff -ruN -x .svn -x build lucene-clean-trunk/modules/grouping/src/java/org/apache/lucene/search/grouping/function/FunctionAllGroupHeadsCollector.java lucene-3622/modules/grouping/src/java/org/apache/lucene/search/grouping/function/FunctionAllGroupHeadsCollector.java
--- lucene-clean-trunk/modules/grouping/src/java/org/apache/lucene/search/grouping/function/FunctionAllGroupHeadsCollector.java	2011-12-06 18:45:01.024810930 -0500
+++ lucene-3622/modules/grouping/src/java/org/apache/lucene/search/grouping/function/FunctionAllGroupHeadsCollector.java	2011-12-12 15:20:30.585624898 -0500
@@ -18,7 +18,7 @@
  */
 
 import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.queries.function.DocValues;
+import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.search.FieldComparator;
 import org.apache.lucene.search.Scorer;
@@ -45,7 +45,7 @@
   private final Map<MutableValue, GroupHead> groups;
   private final Sort sortWithinGroup;
 
-  private DocValues.ValueFiller filler;
+  private FunctionValues.ValueFiller filler;
   private MutableValue mval;
   private IndexReader.AtomicReaderContext readerContext;
   private Scorer scorer;
@@ -105,7 +105,7 @@
 
   public void setNextReader(IndexReader.AtomicReaderContext context) throws IOException {
     this.readerContext = context;
-    DocValues docValues = groupBy.getValues(vsContext, context);
+    FunctionValues docValues = groupBy.getValues(vsContext, context);
     filler = docValues.getValueFiller();
     mval = filler.getValue();
 


diff -ruN -x .svn -x build lucene-clean-trunk/modules/grouping/src/java/org/apache/lucene/search/grouping/function/FunctionAllGroupsCollector.java lucene-3622/modules/grouping/src/java/org/apache/lucene/search/grouping/function/FunctionAllGroupsCollector.java
--- lucene-clean-trunk/modules/grouping/src/java/org/apache/lucene/search/grouping/function/FunctionAllGroupsCollector.java	2011-12-06 18:45:01.024810930 -0500
+++ lucene-3622/modules/grouping/src/java/org/apache/lucene/search/grouping/function/FunctionAllGroupsCollector.java	2011-12-12 15:20:30.589624898 -0500
@@ -18,7 +18,7 @@
  */
 
 import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.queries.function.DocValues;
+import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.search.grouping.AbstractAllGroupsCollector;
 import org.apache.lucene.util.mutable.MutableValue;
@@ -36,7 +36,7 @@
  * the most relevant document of a group.
  *
  * <p/>
- * Implementation detail: Uses {@link ValueSource} and {@link DocValues} to retrieve the
+ * Implementation detail: Uses {@link ValueSource} and {@link FunctionValues} to retrieve the
  * field values to group by.
  *
  * @lucene.experimental
@@ -47,7 +47,7 @@
   private final ValueSource groupBy;
   private final SortedSet<MutableValue> groups = new TreeSet<MutableValue>();
 
-  private DocValues.ValueFiller filler;
+  private FunctionValues.ValueFiller filler;
   private MutableValue mval;
 
   /**
@@ -79,7 +79,7 @@
    * {@inheritDoc}
    */
   public void setNextReader(IndexReader.AtomicReaderContext context) throws IOException {
-    DocValues docValues = groupBy.getValues(vsContext, context);
+    FunctionValues docValues = groupBy.getValues(vsContext, context);
     filler = docValues.getValueFiller();
     mval = filler.getValue();
   }


diff -ruN -x .svn -x build lucene-clean-trunk/modules/grouping/src/java/org/apache/lucene/search/grouping/function/FunctionFirstPassGroupingCollector.java lucene-3622/modules/grouping/src/java/org/apache/lucene/search/grouping/function/FunctionFirstPassGroupingCollector.java
--- lucene-clean-trunk/modules/grouping/src/java/org/apache/lucene/search/grouping/function/FunctionFirstPassGroupingCollector.java	2011-12-06 18:45:01.024810930 -0500
+++ lucene-3622/modules/grouping/src/java/org/apache/lucene/search/grouping/function/FunctionFirstPassGroupingCollector.java	2011-12-12 15:20:30.589624898 -0500
@@ -18,7 +18,7 @@
  */
 
 import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.queries.function.DocValues;
+import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.search.Sort;
 import org.apache.lucene.search.grouping.AbstractFirstPassGroupingCollector;
@@ -38,8 +38,8 @@
   private final ValueSource groupByVS;
   private final Map vsContext;
 
-  private DocValues docValues;
-  private DocValues.ValueFiller filler;
+  private FunctionValues docValues;
+  private FunctionValues.ValueFiller filler;
   private MutableValue mval;
 
   /**


diff -ruN -x .svn -x build lucene-clean-trunk/modules/grouping/src/java/org/apache/lucene/search/grouping/function/FunctionSecondPassGroupingCollector.java lucene-3622/modules/grouping/src/java/org/apache/lucene/search/grouping/function/FunctionSecondPassGroupingCollector.java
--- lucene-clean-trunk/modules/grouping/src/java/org/apache/lucene/search/grouping/function/FunctionSecondPassGroupingCollector.java	2011-12-06 18:45:01.024810930 -0500
+++ lucene-3622/modules/grouping/src/java/org/apache/lucene/search/grouping/function/FunctionSecondPassGroupingCollector.java	2011-12-12 15:20:30.593624898 -0500
@@ -18,7 +18,7 @@
  */
 
 import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.queries.function.DocValues;
+import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.search.Sort;
 import org.apache.lucene.search.grouping.AbstractSecondPassGroupingCollector;
@@ -41,7 +41,7 @@
   private final ValueSource groupByVS;
   private final Map vsContext;
 
-  private DocValues.ValueFiller filler;
+  private FunctionValues.ValueFiller filler;
   private MutableValue mval;
 
   /**
@@ -77,7 +77,7 @@
    */
   public void setNextReader(IndexReader.AtomicReaderContext readerContext) throws IOException {
     super.setNextReader(readerContext);
-    DocValues docValues = groupByVS.getValues(vsContext, readerContext);
+    FunctionValues docValues = groupByVS.getValues(vsContext, readerContext);
     filler = docValues.getValueFiller();
     mval = filler.getValue();
   }


diff -ruN -x .svn -x build lucene-clean-trunk/modules/grouping/src/test/org/apache/lucene/search/grouping/AllGroupHeadsCollectorTest.java lucene-3622/modules/grouping/src/test/org/apache/lucene/search/grouping/AllGroupHeadsCollectorTest.java
--- lucene-clean-trunk/modules/grouping/src/test/org/apache/lucene/search/grouping/AllGroupHeadsCollectorTest.java	2011-12-11 16:52:05.840215987 -0500
+++ lucene-3622/modules/grouping/src/test/org/apache/lucene/search/grouping/AllGroupHeadsCollectorTest.java	2011-12-12 15:01:52.857605434 -0500
@@ -23,7 +23,7 @@
 import org.apache.lucene.index.RandomIndexWriter;
 import org.apache.lucene.index.SlowMultiReaderWrapper;
 import org.apache.lucene.index.Term;
-import org.apache.lucene.index.values.ValueType;
+import org.apache.lucene.index.DocValues.Type;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.queries.function.valuesource.BytesRefFieldSource;
 import org.apache.lucene.search.*;
@@ -41,8 +41,8 @@
 
 public class AllGroupHeadsCollectorTest extends LuceneTestCase {
 
-  private static final ValueType[] vts = new ValueType[]{
-      ValueType.BYTES_VAR_DEREF, ValueType.BYTES_VAR_STRAIGHT, ValueType.BYTES_VAR_SORTED
+  private static final Type[] vts = new Type[]{
+      Type.BYTES_VAR_DEREF, Type.BYTES_VAR_STRAIGHT, Type.BYTES_VAR_SORTED
   };
 
   public void testBasic() throws Exception {
@@ -54,7 +54,7 @@
         newIndexWriterConfig(TEST_VERSION_CURRENT,
             new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));
     boolean canUseIDV = !"Lucene3x".equals(w.w.getConfig().getCodec().getName());
-    ValueType valueType = vts[random.nextInt(vts.length)];
+    Type valueType = vts[random.nextInt(vts.length)];
 
     // 0
     Document doc = new Document();
@@ -203,15 +203,15 @@
               new MockAnalyzer(random)));
       boolean preFlex = "Lucene3x".equals(w.w.getConfig().getCodec().getName());
       boolean canUseIDV = !preFlex;
-      ValueType valueType = vts[random.nextInt(vts.length)];
+      Type valueType = vts[random.nextInt(vts.length)];
 
       Document doc = new Document();
       Document docNoGroup = new Document();
       Field group = newField("group", "", StringField.TYPE_UNSTORED);
       doc.add(group);
-      IndexDocValuesField valuesField = null;
+      DocValuesField valuesField = null;
       if (canUseIDV) {
-        valuesField = new IndexDocValuesField("group");
+        valuesField = new DocValuesField("group");
         doc.add(valuesField);
       }
       Field sort1 = newField("sort1", "", StringField.TYPE_UNSTORED);
@@ -505,7 +505,7 @@
     };
   }
 
-  private AbstractAllGroupHeadsCollector createRandomCollector(String groupField, Sort sortWithinGroup, boolean canUseIDV, ValueType valueType) throws IOException {
+  private AbstractAllGroupHeadsCollector createRandomCollector(String groupField, Sort sortWithinGroup, boolean canUseIDV, Type valueType) throws IOException {
     AbstractAllGroupHeadsCollector collector;
     if (random.nextBoolean()) {
       ValueSource vs = new BytesRefFieldSource(groupField);
@@ -524,10 +524,10 @@
     return collector;
   }
 
-  private void addGroupField(Document doc, String groupField, String value, boolean canUseIDV, ValueType valueType) {
+  private void addGroupField(Document doc, String groupField, String value, boolean canUseIDV, Type valueType) {
     doc.add(new Field(groupField, value, TextField.TYPE_STORED));
     if (canUseIDV) {
-      IndexDocValuesField valuesField = new IndexDocValuesField(groupField);
+      DocValuesField valuesField = new DocValuesField(groupField);
       valuesField.setBytes(new BytesRef(value), valueType);
       doc.add(valuesField);
     }


diff -ruN -x .svn -x build lucene-clean-trunk/modules/grouping/src/test/org/apache/lucene/search/grouping/AllGroupsCollectorTest.java lucene-3622/modules/grouping/src/test/org/apache/lucene/search/grouping/AllGroupsCollectorTest.java
--- lucene-clean-trunk/modules/grouping/src/test/org/apache/lucene/search/grouping/AllGroupsCollectorTest.java	2011-12-06 18:45:01.012810930 -0500
+++ lucene-3622/modules/grouping/src/test/org/apache/lucene/search/grouping/AllGroupsCollectorTest.java	2011-12-09 11:13:38.740853132 -0500
@@ -21,7 +21,7 @@
 import org.apache.lucene.document.*;
 import org.apache.lucene.index.RandomIndexWriter;
 import org.apache.lucene.index.Term;
-import org.apache.lucene.index.values.ValueType;
+import org.apache.lucene.index.DocValues.Type;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.queries.function.valuesource.BytesRefFieldSource;
 import org.apache.lucene.search.IndexSearcher;
@@ -123,8 +123,8 @@
   private void addGroupField(Document doc, String groupField, String value, boolean canUseIDV) {
     doc.add(new Field(groupField, value, TextField.TYPE_STORED));
     if (canUseIDV) {
-      IndexDocValuesField valuesField = new IndexDocValuesField(groupField);
-      valuesField.setBytes(new BytesRef(value), ValueType.BYTES_VAR_SORTED);
+      DocValuesField valuesField = new DocValuesField(groupField);
+      valuesField.setBytes(new BytesRef(value), Type.BYTES_VAR_SORTED);
       doc.add(valuesField);
     }
   }
@@ -133,7 +133,7 @@
     AbstractAllGroupsCollector selected;
     if (random.nextBoolean() && canUseIDV) {
       boolean diskResident = random.nextBoolean();
-      selected = DVAllGroupsCollector.create(groupField, ValueType.BYTES_VAR_SORTED, diskResident);
+      selected = DVAllGroupsCollector.create(groupField, Type.BYTES_VAR_SORTED, diskResident);
     } else if (random.nextBoolean()) {
       selected = new TermAllGroupsCollector(groupField);
     } else {


diff -ruN -x .svn -x build lucene-clean-trunk/modules/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping.java lucene-3622/modules/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping.java
--- lucene-clean-trunk/modules/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping.java	2011-12-11 16:52:15.552216153 -0500
+++ lucene-3622/modules/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping.java	2011-12-12 15:01:53.157605439 -0500
@@ -24,7 +24,7 @@
 import org.apache.lucene.index.RandomIndexWriter;
 import org.apache.lucene.index.SlowMultiReaderWrapper;
 import org.apache.lucene.index.Term;
-import org.apache.lucene.index.values.ValueType;
+import org.apache.lucene.index.DocValues.Type;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.queries.function.valuesource.BytesRefFieldSource;
 import org.apache.lucene.search.*;
@@ -171,8 +171,8 @@
   private void addGroupField(Document doc, String groupField, String value, boolean canUseIDV) {
     doc.add(new Field(groupField, value, TextField.TYPE_STORED));
     if (canUseIDV) {
-      IndexDocValuesField valuesField = new IndexDocValuesField(groupField);
-      valuesField.setBytes(new BytesRef(value), ValueType.BYTES_VAR_SORTED);
+      DocValuesField valuesField = new DocValuesField(groupField);
+      valuesField.setBytes(new BytesRef(value), Type.BYTES_VAR_SORTED);
       doc.add(valuesField);
     }
   }
@@ -181,7 +181,7 @@
     AbstractFirstPassGroupingCollector selected;
     if (canUseIDV && random.nextBoolean()) {
       boolean diskResident = random.nextBoolean();
-      selected = DVFirstPassGroupingCollector.create(groupSort, topDocs, groupField, ValueType.BYTES_VAR_SORTED, diskResident);
+      selected = DVFirstPassGroupingCollector.create(groupSort, topDocs, groupField, Type.BYTES_VAR_SORTED, diskResident);
     } else if (random.nextBoolean()) {
       ValueSource vs = new BytesRefFieldSource(groupField);
       selected = new FunctionFirstPassGroupingCollector(vs, new HashMap(), groupSort, topDocs);
@@ -197,7 +197,7 @@
   private AbstractFirstPassGroupingCollector createFirstPassCollector(String groupField, Sort groupSort, int topDocs, AbstractFirstPassGroupingCollector firstPassGroupingCollector) throws IOException {
     if (DVFirstPassGroupingCollector.class.isAssignableFrom(firstPassGroupingCollector.getClass())) {
       boolean diskResident = random.nextBoolean();
-      return DVFirstPassGroupingCollector.create(groupSort, topDocs, groupField, ValueType.BYTES_VAR_SORTED, diskResident);
+      return DVFirstPassGroupingCollector.create(groupSort, topDocs, groupField, Type.BYTES_VAR_SORTED, diskResident);
     } else if (TermFirstPassGroupingCollector.class.isAssignableFrom(firstPassGroupingCollector.getClass())) {
       ValueSource vs = new BytesRefFieldSource(groupField);
       return new FunctionFirstPassGroupingCollector(vs, new HashMap(), groupSort, topDocs);
@@ -220,7 +220,7 @@
     if (DVFirstPassGroupingCollector.class.isAssignableFrom(firstPassGroupingCollector.getClass())) {
       boolean diskResident = random.nextBoolean();
       Collection<SearchGroup> searchGroups = firstPassGroupingCollector.getTopGroups(groupOffset, fillSortFields);
-      return DVSecondPassGroupingCollector.create(groupField, diskResident, ValueType.BYTES_VAR_SORTED, searchGroups, groupSort, sortWithinGroup, maxDocsPerGroup, getScores, getMaxScores, fillSortFields);
+      return DVSecondPassGroupingCollector.create(groupField, diskResident, Type.BYTES_VAR_SORTED, searchGroups, groupSort, sortWithinGroup, maxDocsPerGroup, getScores, getMaxScores, fillSortFields);
     } else if (TermFirstPassGroupingCollector.class.isAssignableFrom(firstPassGroupingCollector.getClass())) {
       Collection<SearchGroup<BytesRef>> searchGroups = firstPassGroupingCollector.getTopGroups(groupOffset, fillSortFields);
       return new TermSecondPassGroupingCollector(groupField, searchGroups, groupSort, sortWithinGroup, maxDocsPerGroup , getScores, getMaxScores, fillSortFields);
@@ -244,7 +244,7 @@
                                                                         boolean fillSortFields) throws IOException {
     if (DVFirstPassGroupingCollector.class.isAssignableFrom(firstPassGroupingCollector.getClass())) {
       boolean diskResident = random.nextBoolean();
-      return DVSecondPassGroupingCollector.create(groupField, diskResident, ValueType.BYTES_VAR_SORTED, (Collection) searchGroups, groupSort, sortWithinGroup, maxDocsPerGroup, getScores, getMaxScores, fillSortFields);
+      return DVSecondPassGroupingCollector.create(groupField, diskResident, Type.BYTES_VAR_SORTED, (Collection) searchGroups, groupSort, sortWithinGroup, maxDocsPerGroup, getScores, getMaxScores, fillSortFields);
     } else if (firstPassGroupingCollector.getClass().isAssignableFrom(TermFirstPassGroupingCollector.class)) {
       return new TermSecondPassGroupingCollector(groupField, searchGroups, groupSort, sortWithinGroup, maxDocsPerGroup , getScores, getMaxScores, fillSortFields);
     } else {
@@ -274,7 +274,7 @@
       return new TermAllGroupsCollector(groupField);
     } else if (firstPassGroupingCollector.getClass().isAssignableFrom(DVFirstPassGroupingCollector.class)) {
       boolean diskResident = random.nextBoolean();
-      return DVAllGroupsCollector.create(groupField, ValueType.BYTES_VAR_SORTED, diskResident);
+      return DVAllGroupsCollector.create(groupField, Type.BYTES_VAR_SORTED, diskResident);
     } else {
       ValueSource vs = new BytesRefFieldSource(groupField);
       return new FunctionAllGroupsCollector(vs, new HashMap());
@@ -705,7 +705,7 @@
 
       Document doc = new Document();
       Document docNoGroup = new Document();
-      IndexDocValuesField idvGroupField = new IndexDocValuesField("group");
+      DocValuesField idvGroupField = new DocValuesField("group");
       if (canUseIDV) {
         doc.add(idvGroupField);
       }
@@ -747,7 +747,7 @@
         if (groupDoc.group != null) {
           group.setValue(groupDoc.group.utf8ToString());
           if (canUseIDV) {
-            idvGroupField.setBytes(BytesRef.deepCopyOf(groupDoc.group), ValueType.BYTES_VAR_SORTED);
+            idvGroupField.setBytes(BytesRef.deepCopyOf(groupDoc.group), Type.BYTES_VAR_SORTED);
           }
         }
         sort1.setValue(groupDoc.sort1.utf8ToString());


diff -ruN -x .svn -x build lucene-clean-trunk/modules/queries/src/java/org/apache/lucene/queries/function/BoostedQuery.java lucene-3622/modules/queries/src/java/org/apache/lucene/queries/function/BoostedQuery.java
--- lucene-clean-trunk/modules/queries/src/java/org/apache/lucene/queries/function/BoostedQuery.java	2011-12-06 18:45:01.072810931 -0500
+++ lucene-3622/modules/queries/src/java/org/apache/lucene/queries/function/BoostedQuery.java	2011-12-12 15:20:30.505624897 -0500
@@ -108,7 +108,7 @@
       if (!subQueryExpl.isMatch()) {
         return subQueryExpl;
       }
-      DocValues vals = boostVal.getValues(fcontext, readerContext);
+      FunctionValues vals = boostVal.getValues(fcontext, readerContext);
       float sc = subQueryExpl.getValue() * vals.floatVal(doc);
       Explanation res = new ComplexExplanation(
         true, sc, BoostedQuery.this.toString() + ", product of:");
@@ -123,7 +123,7 @@
     private final BoostedQuery.BoostedWeight weight;
     private final float qWeight;
     private final Scorer scorer;
-    private final DocValues vals;
+    private final FunctionValues vals;
     private final AtomicReaderContext readerContext;
 
     private CustomScorer(AtomicReaderContext readerContext, BoostedQuery.BoostedWeight w, float qWeight,


diff -ruN -x .svn -x build lucene-clean-trunk/modules/queries/src/java/org/apache/lucene/queries/function/docvalues/BoolDocValues.java lucene-3622/modules/queries/src/java/org/apache/lucene/queries/function/docvalues/BoolDocValues.java
--- lucene-clean-trunk/modules/queries/src/java/org/apache/lucene/queries/function/docvalues/BoolDocValues.java	2011-12-06 18:45:01.072810931 -0500
+++ lucene-3622/modules/queries/src/java/org/apache/lucene/queries/function/docvalues/BoolDocValues.java	2011-12-12 15:20:30.485624895 -0500
@@ -1,12 +1,12 @@
 package org.apache.lucene.queries.function.docvalues;
 
-import org.apache.lucene.queries.function.DocValues;
+import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.util.mutable.MutableValue;
 import org.apache.lucene.util.mutable.MutableValueBool;
 
 
-public abstract class BoolDocValues extends DocValues {
+public abstract class BoolDocValues extends FunctionValues {
   protected final ValueSource vs;
 
   public BoolDocValues(ValueSource vs) {


diff -ruN -x .svn -x build lucene-clean-trunk/modules/queries/src/java/org/apache/lucene/queries/function/docvalues/DoubleDocValues.java lucene-3622/modules/queries/src/java/org/apache/lucene/queries/function/docvalues/DoubleDocValues.java
--- lucene-clean-trunk/modules/queries/src/java/org/apache/lucene/queries/function/docvalues/DoubleDocValues.java	2011-12-06 18:45:01.072810931 -0500
+++ lucene-3622/modules/queries/src/java/org/apache/lucene/queries/function/docvalues/DoubleDocValues.java	2011-12-12 15:20:30.565624898 -0500
@@ -1,11 +1,11 @@
 package org.apache.lucene.queries.function.docvalues;
 
-import org.apache.lucene.queries.function.DocValues;
+import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.util.mutable.MutableValue;
 import org.apache.lucene.util.mutable.MutableValueDouble;
 
-public abstract class DoubleDocValues extends DocValues {
+public abstract class DoubleDocValues extends FunctionValues {
   protected final ValueSource vs;
 
   public DoubleDocValues(ValueSource vs) {


diff -ruN -x .svn -x build lucene-clean-trunk/modules/queries/src/java/org/apache/lucene/queries/function/docvalues/FloatDocValues.java lucene-3622/modules/queries/src/java/org/apache/lucene/queries/function/docvalues/FloatDocValues.java
--- lucene-clean-trunk/modules/queries/src/java/org/apache/lucene/queries/function/docvalues/FloatDocValues.java	2011-12-06 18:45:01.076810931 -0500
+++ lucene-3622/modules/queries/src/java/org/apache/lucene/queries/function/docvalues/FloatDocValues.java	2011-12-12 15:20:30.581624898 -0500
@@ -1,11 +1,11 @@
 package org.apache.lucene.queries.function.docvalues;
 
-import org.apache.lucene.queries.function.DocValues;
+import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.util.mutable.MutableValue;
 import org.apache.lucene.util.mutable.MutableValueFloat;
 
-public abstract class FloatDocValues extends DocValues {
+public abstract class FloatDocValues extends FunctionValues {
   protected final ValueSource vs;
 
   public FloatDocValues(ValueSource vs) {


diff -ruN -x .svn -x build lucene-clean-trunk/modules/queries/src/java/org/apache/lucene/queries/function/docvalues/IntDocValues.java lucene-3622/modules/queries/src/java/org/apache/lucene/queries/function/docvalues/IntDocValues.java
--- lucene-clean-trunk/modules/queries/src/java/org/apache/lucene/queries/function/docvalues/IntDocValues.java	2011-12-06 18:45:01.072810931 -0500
+++ lucene-3622/modules/queries/src/java/org/apache/lucene/queries/function/docvalues/IntDocValues.java	2011-12-12 15:20:30.605624898 -0500
@@ -1,12 +1,12 @@
 package org.apache.lucene.queries.function.docvalues;
 
-import org.apache.lucene.queries.function.DocValues;
+import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.util.mutable.MutableValue;
 import org.apache.lucene.util.mutable.MutableValueInt;
 
 
-public abstract class IntDocValues extends DocValues {
+public abstract class IntDocValues extends FunctionValues {
   protected final ValueSource vs;
 
   public IntDocValues(ValueSource vs) {


diff -ruN -x .svn -x build lucene-clean-trunk/modules/queries/src/java/org/apache/lucene/queries/function/docvalues/LongDocValues.java lucene-3622/modules/queries/src/java/org/apache/lucene/queries/function/docvalues/LongDocValues.java
--- lucene-clean-trunk/modules/queries/src/java/org/apache/lucene/queries/function/docvalues/LongDocValues.java	2011-12-06 18:45:01.072810931 -0500
+++ lucene-3622/modules/queries/src/java/org/apache/lucene/queries/function/docvalues/LongDocValues.java	2011-12-12 15:20:30.621624899 -0500
@@ -1,12 +1,12 @@
 package org.apache.lucene.queries.function.docvalues;
 
-import org.apache.lucene.queries.function.DocValues;
+import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.util.mutable.MutableValue;
 import org.apache.lucene.util.mutable.MutableValueLong;
 
 
-public abstract class LongDocValues extends DocValues {
+public abstract class LongDocValues extends FunctionValues {
   protected final ValueSource vs;
 
   public LongDocValues(ValueSource vs) {


diff -ruN -x .svn -x build lucene-clean-trunk/modules/queries/src/java/org/apache/lucene/queries/function/docvalues/StrDocValues.java lucene-3622/modules/queries/src/java/org/apache/lucene/queries/function/docvalues/StrDocValues.java
--- lucene-clean-trunk/modules/queries/src/java/org/apache/lucene/queries/function/docvalues/StrDocValues.java	2011-12-06 18:45:01.076810931 -0500
+++ lucene-3622/modules/queries/src/java/org/apache/lucene/queries/function/docvalues/StrDocValues.java	2011-12-12 15:20:30.669624900 -0500
@@ -1,11 +1,11 @@
 package org.apache.lucene.queries.function.docvalues;
 
-import org.apache.lucene.queries.function.DocValues;
+import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.util.mutable.MutableValue;
 import org.apache.lucene.util.mutable.MutableValueStr;
 
-public abstract class StrDocValues extends DocValues {
+public abstract class StrDocValues extends FunctionValues {
   protected final ValueSource vs;
 
   public StrDocValues(ValueSource vs) {


diff -ruN -x .svn -x build lucene-clean-trunk/modules/queries/src/java/org/apache/lucene/queries/function/docvalues/StringIndexDocValues.java lucene-3622/modules/queries/src/java/org/apache/lucene/queries/function/docvalues/StringIndexDocValues.java
--- lucene-clean-trunk/modules/queries/src/java/org/apache/lucene/queries/function/docvalues/StringIndexDocValues.java	2011-12-06 18:45:01.072810931 -0500
+++ lucene-3622/modules/queries/src/java/org/apache/lucene/queries/function/docvalues/StringIndexDocValues.java	2011-12-12 15:20:30.673624900 -0500
@@ -17,7 +17,7 @@
 
 package org.apache.lucene.queries.function.docvalues;
 
-import org.apache.lucene.queries.function.DocValues;
+import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.queries.function.ValueSourceScorer;
 import org.apache.lucene.search.FieldCache;
@@ -32,9 +32,9 @@
 import java.io.IOException;
 
 /** Internal class, subject to change.
- *  Serves as base class for DocValues based on StringIndex 
+ *  Serves as base class for FunctionValues based on StringIndex 
  **/
-public abstract class StringIndexDocValues extends DocValues {
+public abstract class StringIndexDocValues extends FunctionValues {
   protected final FieldCache.DocTermsIndex termsIndex;
   protected final ValueSource vs;
   protected final MutableValueStr val = new MutableValueStr();
@@ -158,7 +158,7 @@
     public StringIndexException(final String fieldName,
                                 final RuntimeException cause) {
       super("Can't initialize StringIndex to generate (function) " +
-              "DocValues for field: " + fieldName, cause);
+              "FunctionValues for field: " + fieldName, cause);
     }
   }
 


diff -ruN -x .svn -x build lucene-clean-trunk/modules/queries/src/java/org/apache/lucene/queries/function/DocValues.java lucene-3622/modules/queries/src/java/org/apache/lucene/queries/function/DocValues.java
--- lucene-clean-trunk/modules/queries/src/java/org/apache/lucene/queries/function/DocValues.java	2011-12-06 18:45:01.072810931 -0500
+++ lucene-3622/modules/queries/src/java/org/apache/lucene/queries/function/DocValues.java	1969-12-31 19:00:00.000000000 -0500
@@ -1,198 +0,0 @@
-package org.apache.lucene.queries.function;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.search.*;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.mutable.MutableValue;
-import org.apache.lucene.util.mutable.MutableValueFloat;
-
-/**
- * Represents field values as different types.
- * Normally created via a {@link ValueSource} for a particular field and reader.
- *
- *
- */
-
-// DocValues is distinct from ValueSource because
-// there needs to be an object created at query evaluation time that
-// is not referenced by the query itself because:
-// - Query objects should be MT safe
-// - For caching, Query objects are often used as keys... you don't
-//   want the Query carrying around big objects
-public abstract class DocValues {
-
-  public byte byteVal(int doc) { throw new UnsupportedOperationException(); }
-  public short shortVal(int doc) { throw new UnsupportedOperationException(); }
-
-  public float floatVal(int doc) { throw new UnsupportedOperationException(); }
-  public int intVal(int doc) { throw new UnsupportedOperationException(); }
-  public long longVal(int doc) { throw new UnsupportedOperationException(); }
-  public double doubleVal(int doc) { throw new UnsupportedOperationException(); }
-  // TODO: should we make a termVal, returns BytesRef?
-  public String strVal(int doc) { throw new UnsupportedOperationException(); }
-
-  public boolean boolVal(int doc) {
-    return intVal(doc) != 0;
-  }
-
-  /** returns the bytes representation of the string val - TODO: should this return the indexed raw bytes not? */
-  public boolean bytesVal(int doc, BytesRef target) {
-    String s = strVal(doc);
-    if (s==null) {
-      target.length = 0;
-      return false;
-    }
-    target.copyChars(s);
-    return true;
-  };
-
-  /** Native Java Object representation of the value */
-  public Object objectVal(int doc) {
-    // most DocValues are functions, so by default return a Float()
-    return floatVal(doc);
-  }
-
-  /** Returns true if there is a value for this document */
-  public boolean exists(int doc) {
-    return true;
-  }
-
-  /**
-   * @param doc The doc to retrieve to sort ordinal for
-   * @return the sort ordinal for the specified doc
-   * TODO: Maybe we can just use intVal for this...
-   */
-  public int ordVal(int doc) { throw new UnsupportedOperationException(); }
-
-  /**
-   * @return the number of unique sort ordinals this instance has
-   */
-  public int numOrd() { throw new UnsupportedOperationException(); }
-  public abstract String toString(int doc);
-
-  /** @lucene.experimental  */
-  public static abstract class ValueFiller {
-    /** MutableValue will be reused across calls */
-    public abstract MutableValue getValue();
-
-    /** MutableValue will be reused across calls.  Returns true if the value exists. */
-    public abstract void fillValue(int doc);
-  }
-
-  /** @lucene.experimental  */
-  public ValueFiller getValueFiller() {
-    return new ValueFiller() {
-      private final MutableValueFloat mval = new MutableValueFloat();
-
-      @Override
-      public MutableValue getValue() {
-        return mval;
-      }
-
-      @Override
-      public void fillValue(int doc) {
-        mval.value = floatVal(doc);
-      }
-    };
-  }
-
-  //For Functions that can work with multiple values from the same document.  This does not apply to all functions
-  public void byteVal(int doc, byte [] vals) { throw new UnsupportedOperationException(); }
-  public void shortVal(int doc, short [] vals) { throw new UnsupportedOperationException(); }
-
-  public void floatVal(int doc, float [] vals) { throw new UnsupportedOperationException(); }
-  public void intVal(int doc, int [] vals) { throw new UnsupportedOperationException(); }
-  public void longVal(int doc, long [] vals) { throw new UnsupportedOperationException(); }
-  public void doubleVal(int doc, double [] vals) { throw new UnsupportedOperationException(); }
-
-  // TODO: should we make a termVal, fills BytesRef[]?
-  public void strVal(int doc, String [] vals) { throw new UnsupportedOperationException(); }
-
-  public Explanation explain(int doc) {
-    return new Explanation(floatVal(doc), toString(doc));
-  }
-
-  public ValueSourceScorer getScorer(IndexReader reader) {
-    return new ValueSourceScorer(reader, this);
-  }
-
-  // A RangeValueSource can't easily be a ValueSource that takes another ValueSource
-  // because it needs different behavior depending on the type of fields.  There is also
-  // a setup cost - parsing and normalizing params, and doing a binary search on the StringIndex.
-  // TODO: change "reader" to AtomicReaderContext
-  public ValueSourceScorer getRangeScorer(IndexReader reader, String lowerVal, String upperVal, boolean includeLower, boolean includeUpper) {
-    float lower;
-    float upper;
-
-    if (lowerVal == null) {
-      lower = Float.NEGATIVE_INFINITY;
-    } else {
-      lower = Float.parseFloat(lowerVal);
-    }
-    if (upperVal == null) {
-      upper = Float.POSITIVE_INFINITY;
-    } else {
-      upper = Float.parseFloat(upperVal);
-    }
-
-    final float l = lower;
-    final float u = upper;
-
-    if (includeLower && includeUpper) {
-      return new ValueSourceScorer(reader, this) {
-        @Override
-        public boolean matchesValue(int doc) {
-          float docVal = floatVal(doc);
-          return docVal >= l && docVal <= u;
-        }
-      };
-    }
-    else if (includeLower && !includeUpper) {
-       return new ValueSourceScorer(reader, this) {
-        @Override
-        public boolean matchesValue(int doc) {
-          float docVal = floatVal(doc);
-          return docVal >= l && docVal < u;
-        }
-      };
-    }
-    else if (!includeLower && includeUpper) {
-       return new ValueSourceScorer(reader, this) {
-        @Override
-        public boolean matchesValue(int doc) {
-          float docVal = floatVal(doc);
-          return docVal > l && docVal <= u;
-        }
-      };
-    }
-    else {
-       return new ValueSourceScorer(reader, this) {
-        @Override
-        public boolean matchesValue(int doc) {
-          float docVal = floatVal(doc);
-          return docVal > l && docVal < u;
-        }
-      };
-    }
-  }
-}
-
-
-


diff -ruN -x .svn -x build lucene-clean-trunk/modules/queries/src/java/org/apache/lucene/queries/function/FunctionQuery.java lucene-3622/modules/queries/src/java/org/apache/lucene/queries/function/FunctionQuery.java
--- lucene-clean-trunk/modules/queries/src/java/org/apache/lucene/queries/function/FunctionQuery.java	2011-12-06 18:45:01.112810932 -0500
+++ lucene-3622/modules/queries/src/java/org/apache/lucene/queries/function/FunctionQuery.java	2011-12-12 15:20:30.589624898 -0500
@@ -106,7 +106,7 @@
     final int maxDoc;
     final float qWeight;
     int doc=-1;
-    final DocValues vals;
+    final FunctionValues vals;
     final Bits liveDocs;
 
     public AllScorer(AtomicReaderContext context, Bits acceptDocs, FunctionWeight w, float qWeight) throws IOException {


diff -ruN -x .svn -x build lucene-clean-trunk/modules/queries/src/java/org/apache/lucene/queries/function/FunctionValues.java lucene-3622/modules/queries/src/java/org/apache/lucene/queries/function/FunctionValues.java
--- lucene-clean-trunk/modules/queries/src/java/org/apache/lucene/queries/function/FunctionValues.java	1969-12-31 19:00:00.000000000 -0500
+++ lucene-3622/modules/queries/src/java/org/apache/lucene/queries/function/FunctionValues.java	2011-12-12 15:20:30.801624902 -0500
@@ -0,0 +1,198 @@
+package org.apache.lucene.queries.function;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.search.*;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.mutable.MutableValue;
+import org.apache.lucene.util.mutable.MutableValueFloat;
+
+/**
+ * Represents field values as different types.
+ * Normally created via a {@link ValueSource} for a particular field and reader.
+ *
+ *
+ */
+
+// FunctionValues is distinct from ValueSource because
+// there needs to be an object created at query evaluation time that
+// is not referenced by the query itself because:
+// - Query objects should be MT safe
+// - For caching, Query objects are often used as keys... you don't
+//   want the Query carrying around big objects
+public abstract class FunctionValues {
+
+  public byte byteVal(int doc) { throw new UnsupportedOperationException(); }
+  public short shortVal(int doc) { throw new UnsupportedOperationException(); }
+
+  public float floatVal(int doc) { throw new UnsupportedOperationException(); }
+  public int intVal(int doc) { throw new UnsupportedOperationException(); }
+  public long longVal(int doc) { throw new UnsupportedOperationException(); }
+  public double doubleVal(int doc) { throw new UnsupportedOperationException(); }
+  // TODO: should we make a termVal, returns BytesRef?
+  public String strVal(int doc) { throw new UnsupportedOperationException(); }
+
+  public boolean boolVal(int doc) {
+    return intVal(doc) != 0;
+  }
+
+  /** returns the bytes representation of the string val - TODO: should this return the indexed raw bytes not? */
+  public boolean bytesVal(int doc, BytesRef target) {
+    String s = strVal(doc);
+    if (s==null) {
+      target.length = 0;
+      return false;
+    }
+    target.copyChars(s);
+    return true;
+  };
+
+  /** Native Java Object representation of the value */
+  public Object objectVal(int doc) {
+    // most FunctionValues are functions, so by default return a Float()
+    return floatVal(doc);
+  }
+
+  /** Returns true if there is a value for this document */
+  public boolean exists(int doc) {
+    return true;
+  }
+
+  /**
+   * @param doc The doc to retrieve to sort ordinal for
+   * @return the sort ordinal for the specified doc
+   * TODO: Maybe we can just use intVal for this...
+   */
+  public int ordVal(int doc) { throw new UnsupportedOperationException(); }
+
+  /**
+   * @return the number of unique sort ordinals this instance has
+   */
+  public int numOrd() { throw new UnsupportedOperationException(); }
+  public abstract String toString(int doc);
+
+  /** @lucene.experimental  */
+  public static abstract class ValueFiller {
+    /** MutableValue will be reused across calls */
+    public abstract MutableValue getValue();
+
+    /** MutableValue will be reused across calls.  Returns true if the value exists. */
+    public abstract void fillValue(int doc);
+  }
+
+  /** @lucene.experimental  */
+  public ValueFiller getValueFiller() {
+    return new ValueFiller() {
+      private final MutableValueFloat mval = new MutableValueFloat();
+
+      @Override
+      public MutableValue getValue() {
+        return mval;
+      }
+
+      @Override
+      public void fillValue(int doc) {
+        mval.value = floatVal(doc);
+      }
+    };
+  }
+
+  //For Functions that can work with multiple values from the same document.  This does not apply to all functions
+  public void byteVal(int doc, byte [] vals) { throw new UnsupportedOperationException(); }
+  public void shortVal(int doc, short [] vals) { throw new UnsupportedOperationException(); }
+
+  public void floatVal(int doc, float [] vals) { throw new UnsupportedOperationException(); }
+  public void intVal(int doc, int [] vals) { throw new UnsupportedOperationException(); }
+  public void longVal(int doc, long [] vals) { throw new UnsupportedOperationException(); }
+  public void doubleVal(int doc, double [] vals) { throw new UnsupportedOperationException(); }
+
+  // TODO: should we make a termVal, fills BytesRef[]?
+  public void strVal(int doc, String [] vals) { throw new UnsupportedOperationException(); }
+
+  public Explanation explain(int doc) {
+    return new Explanation(floatVal(doc), toString(doc));
+  }
+
+  public ValueSourceScorer getScorer(IndexReader reader) {
+    return new ValueSourceScorer(reader, this);
+  }
+
+  // A RangeValueSource can't easily be a ValueSource that takes another ValueSource
+  // because it needs different behavior depending on the type of fields.  There is also
+  // a setup cost - parsing and normalizing params, and doing a binary search on the StringIndex.
+  // TODO: change "reader" to AtomicReaderContext
+  public ValueSourceScorer getRangeScorer(IndexReader reader, String lowerVal, String upperVal, boolean includeLower, boolean includeUpper) {
+    float lower;
+    float upper;
+
+    if (lowerVal == null) {
+      lower = Float.NEGATIVE_INFINITY;
+    } else {
+      lower = Float.parseFloat(lowerVal);
+    }
+    if (upperVal == null) {
+      upper = Float.POSITIVE_INFINITY;
+    } else {
+      upper = Float.parseFloat(upperVal);
+    }
+
+    final float l = lower;
+    final float u = upper;
+
+    if (includeLower && includeUpper) {
+      return new ValueSourceScorer(reader, this) {
+        @Override
+        public boolean matchesValue(int doc) {
+          float docVal = floatVal(doc);
+          return docVal >= l && docVal <= u;
+        }
+      };
+    }
+    else if (includeLower && !includeUpper) {
+       return new ValueSourceScorer(reader, this) {
+        @Override
+        public boolean matchesValue(int doc) {
+          float docVal = floatVal(doc);
+          return docVal >= l && docVal < u;
+        }
+      };
+    }
+    else if (!includeLower && includeUpper) {
+       return new ValueSourceScorer(reader, this) {
+        @Override
+        public boolean matchesValue(int doc) {
+          float docVal = floatVal(doc);
+          return docVal > l && docVal <= u;
+        }
+      };
+    }
+    else {
+       return new ValueSourceScorer(reader, this) {
+        @Override
+        public boolean matchesValue(int doc) {
+          float docVal = floatVal(doc);
+          return docVal > l && docVal < u;
+        }
+      };
+    }
+  }
+}
+
+
+


diff -ruN -x .svn -x build lucene-clean-trunk/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/ByteFieldSource.java lucene-3622/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/ByteFieldSource.java
--- lucene-clean-trunk/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/ByteFieldSource.java	2011-12-06 18:45:01.120810932 -0500
+++ lucene-3622/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/ByteFieldSource.java	2011-12-12 15:20:30.513624897 -0500
@@ -20,7 +20,7 @@
 import java.util.Map;
 
 import org.apache.lucene.index.IndexReader.AtomicReaderContext;
-import org.apache.lucene.queries.function.DocValues;
+import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.search.FieldCache;
 
 /**
@@ -50,10 +50,10 @@
   }
 
   @Override
-  public DocValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
+  public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
     final byte[] arr = cache.getBytes(readerContext.reader, field, parser, false);
     
-    return new DocValues() {
+    return new FunctionValues() {
       @Override
       public byte byteVal(int doc) {
         return arr[doc];


diff -ruN -x .svn -x build lucene-clean-trunk/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/BytesRefFieldSource.java lucene-3622/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/BytesRefFieldSource.java
--- lucene-clean-trunk/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/BytesRefFieldSource.java	2011-12-06 18:45:01.124810932 -0500
+++ lucene-3622/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/BytesRefFieldSource.java	2011-12-12 15:20:30.517624897 -0500
@@ -18,7 +18,7 @@
  */
 
 import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.queries.function.DocValues;
+import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.docvalues.StringIndexDocValues;
 import org.apache.lucene.queries.function.ValueSource; //javadoc
 
@@ -26,7 +26,7 @@
 import java.util.Map;
 
 /**
- * An implementation for retrieving {@link DocValues} instances for string based fields.
+ * An implementation for retrieving {@link FunctionValues} instances for string based fields.
  */
 public class BytesRefFieldSource extends FieldCacheSource {
 
@@ -35,7 +35,7 @@
   }
 
   @Override
-  public DocValues getValues(Map context, IndexReader.AtomicReaderContext readerContext) throws IOException {
+  public FunctionValues getValues(Map context, IndexReader.AtomicReaderContext readerContext) throws IOException {
     return new StringIndexDocValues(this, readerContext, field) {
 
       @Override


diff -ruN -x .svn -x build lucene-clean-trunk/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/ConstValueSource.java lucene-3622/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/ConstValueSource.java
--- lucene-clean-trunk/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/ConstValueSource.java	2011-12-06 18:45:01.120810932 -0500
+++ lucene-3622/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/ConstValueSource.java	2011-12-12 15:20:30.521624897 -0500
@@ -18,7 +18,7 @@
 package org.apache.lucene.queries.function.valuesource;
 
 import org.apache.lucene.index.IndexReader.AtomicReaderContext;
-import org.apache.lucene.queries.function.DocValues;
+import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.docvalues.FloatDocValues;
 
 import java.io.IOException;
@@ -42,7 +42,7 @@
   }
 
   @Override
-  public DocValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
+  public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
     return new FloatDocValues(this) {
       @Override
       public float floatVal(int doc) {


diff -ruN -x .svn -x build lucene-clean-trunk/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/DefFunction.java lucene-3622/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/DefFunction.java
--- lucene-clean-trunk/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/DefFunction.java	2011-12-06 18:45:01.112810932 -0500
+++ lucene-3622/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/DefFunction.java	2011-12-12 15:20:30.533624897 -0500
@@ -17,7 +17,7 @@
  */
 
 import org.apache.lucene.index.IndexReader.AtomicReaderContext;
-import org.apache.lucene.queries.function.DocValues;
+import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.search.IndexSearcher;
 import org.apache.lucene.util.BytesRef;
@@ -39,15 +39,15 @@
 
 
   @Override
-  public DocValues getValues(Map fcontext, AtomicReaderContext readerContext) throws IOException {
+  public FunctionValues getValues(Map fcontext, AtomicReaderContext readerContext) throws IOException {
 
 
     return new Values(valsArr(sources, fcontext, readerContext)) {
       final int upto = valsArr.length - 1;
 
-      private DocValues get(int doc) {
+      private FunctionValues get(int doc) {
         for (int i=0; i<upto; i++) {
-          DocValues vals = valsArr[i];
+          FunctionValues vals = valsArr[i];
           if (vals.exists(doc)) {
             return vals;
           }
@@ -108,7 +108,7 @@
       @Override
       public boolean exists(int doc) {
         // return true if any source is exists?
-        for (DocValues vals : valsArr) {
+        for (FunctionValues vals : valsArr) {
           if (vals.exists(doc)) {
             return true;
           }


diff -ruN -x .svn -x build lucene-clean-trunk/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/DivFloatFunction.java lucene-3622/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/DivFloatFunction.java
--- lucene-clean-trunk/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/DivFloatFunction.java	2011-12-06 18:45:01.116810932 -0500
+++ lucene-3622/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/DivFloatFunction.java	2011-12-12 15:20:30.533624897 -0500
@@ -17,7 +17,7 @@
 
 package org.apache.lucene.queries.function.valuesource;
 
-import org.apache.lucene.queries.function.DocValues;
+import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 
 /** Function to divide "a" by "b"
@@ -37,7 +37,7 @@
   }
 
   @Override
-  protected float func(int doc, DocValues aVals, DocValues bVals) {
+  protected float func(int doc, FunctionValues aVals, FunctionValues bVals) {
     return aVals.floatVal(doc) / bVals.floatVal(doc);
   }
 }


diff -ruN -x .svn -x build lucene-clean-trunk/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/DocFreqValueSource.java lucene-3622/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/DocFreqValueSource.java
--- lucene-clean-trunk/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/DocFreqValueSource.java	2011-12-06 18:45:01.112810932 -0500
+++ lucene-3622/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/DocFreqValueSource.java	2011-12-12 15:20:30.537624897 -0500
@@ -19,7 +19,7 @@
 
 import org.apache.lucene.index.IndexReader.AtomicReaderContext;
 import org.apache.lucene.index.Term;
-import org.apache.lucene.queries.function.DocValues;
+import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.queries.function.docvalues.DoubleDocValues;
 import org.apache.lucene.queries.function.docvalues.IntDocValues;
@@ -146,7 +146,7 @@
   }
 
   @Override
-  public DocValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
+  public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
     IndexSearcher searcher = (IndexSearcher)context.get("searcher");
     int docfreq = searcher.getIndexReader().docFreq(new Term(indexedField, indexedBytes));
     return new ConstIntDocValues(docfreq, this);


diff -ruN -x .svn -x build lucene-clean-trunk/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/DoubleConstValueSource.java lucene-3622/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/DoubleConstValueSource.java
--- lucene-clean-trunk/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/DoubleConstValueSource.java	2011-12-06 18:45:01.112810932 -0500
+++ lucene-3622/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/DoubleConstValueSource.java	2011-12-12 15:20:30.565624898 -0500
@@ -18,7 +18,7 @@
 package org.apache.lucene.queries.function.valuesource;
 
 import org.apache.lucene.index.IndexReader.AtomicReaderContext;
-import org.apache.lucene.queries.function.DocValues;
+import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.docvalues.DoubleDocValues;
 
 import java.io.IOException;
@@ -41,7 +41,7 @@
   }
 
   @Override
-  public DocValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
+  public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
     return new DoubleDocValues(this) {
       @Override
       public float floatVal(int doc) {


diff -ruN -x .svn -x build lucene-clean-trunk/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/DoubleFieldSource.java lucene-3622/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/DoubleFieldSource.java
--- lucene-clean-trunk/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/DoubleFieldSource.java	2011-12-06 18:45:01.112810932 -0500
+++ lucene-3622/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/DoubleFieldSource.java	2011-12-12 15:20:30.569624898 -0500
@@ -22,7 +22,7 @@
 
 import org.apache.lucene.index.IndexReader.AtomicReaderContext;
 import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.queries.function.DocValues;
+import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSourceScorer;
 import org.apache.lucene.queries.function.docvalues.DoubleDocValues;
 import org.apache.lucene.search.FieldCache;
@@ -56,7 +56,7 @@
   }
 
   @Override
-  public DocValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
+  public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
     final double[] arr = cache.getDoubles(readerContext.reader, field, parser, true);
     final Bits valid = cache.getDocsWithField(readerContext.reader, field);
     return new DoubleDocValues(this) {


diff -ruN -x .svn -x build lucene-clean-trunk/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/DualFloatFunction.java lucene-3622/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/DualFloatFunction.java
--- lucene-clean-trunk/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/DualFloatFunction.java	2011-12-06 18:45:01.124810932 -0500
+++ lucene-3622/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/DualFloatFunction.java	2011-12-12 15:20:30.573624898 -0500
@@ -18,7 +18,7 @@
 package org.apache.lucene.queries.function.valuesource;
 
 import org.apache.lucene.index.IndexReader.AtomicReaderContext;
-import org.apache.lucene.queries.function.DocValues;
+import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.queries.function.docvalues.FloatDocValues;
 import org.apache.lucene.search.IndexSearcher;
@@ -40,7 +40,7 @@
   }
 
   protected abstract String name();
-  protected abstract float func(int doc, DocValues aVals, DocValues bVals);
+  protected abstract float func(int doc, FunctionValues aVals, FunctionValues bVals);
 
   @Override
   public String description() {
@@ -48,9 +48,9 @@
   }
 
   @Override
-  public DocValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
-    final DocValues aVals =  a.getValues(context, readerContext);
-    final DocValues bVals =  b.getValues(context, readerContext);
+  public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
+    final FunctionValues aVals =  a.getValues(context, readerContext);
+    final FunctionValues bVals =  b.getValues(context, readerContext);
     return new FloatDocValues(this) {
       @Override
       public float floatVal(int doc) {


diff -ruN -x .svn -x build lucene-clean-trunk/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/FloatFieldSource.java lucene-3622/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/FloatFieldSource.java
--- lucene-clean-trunk/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/FloatFieldSource.java	2011-12-06 18:45:01.120810932 -0500
+++ lucene-3622/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/FloatFieldSource.java	2011-12-12 15:20:30.585624898 -0500
@@ -21,7 +21,7 @@
 import java.util.Map;
 
 import org.apache.lucene.index.IndexReader.AtomicReaderContext;
-import org.apache.lucene.queries.function.DocValues;
+import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.docvalues.FloatDocValues;
 import org.apache.lucene.search.FieldCache;
 import org.apache.lucene.util.Bits;
@@ -54,7 +54,7 @@
   }
 
   @Override
-  public DocValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
+  public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
     final float[] arr = cache.getFloats(readerContext.reader, field, parser, true);
     final Bits valid = cache.getDocsWithField(readerContext.reader, field);
 


diff -ruN -x .svn -x build lucene-clean-trunk/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/IDFValueSource.java lucene-3622/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/IDFValueSource.java
--- lucene-clean-trunk/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/IDFValueSource.java	2011-12-06 18:45:01.124810932 -0500
+++ lucene-3622/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/IDFValueSource.java	2011-12-12 15:20:30.601624897 -0500
@@ -19,7 +19,7 @@
 
 import org.apache.lucene.index.*;
 import org.apache.lucene.index.IndexReader.AtomicReaderContext;
-import org.apache.lucene.queries.function.DocValues;
+import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.search.IndexSearcher;
 import org.apache.lucene.search.similarities.Similarity;
 import org.apache.lucene.search.similarities.TFIDFSimilarity;
@@ -40,7 +40,7 @@
   }
 
   @Override
-  public DocValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
+  public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
     IndexSearcher searcher = (IndexSearcher)context.get("searcher");
     Similarity sim = searcher.getSimilarityProvider().get(field);
     if (!(sim instanceof TFIDFSimilarity)) {


diff -ruN -x .svn -x build lucene-clean-trunk/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/IfFunction.java lucene-3622/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/IfFunction.java
--- lucene-clean-trunk/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/IfFunction.java	2011-12-06 18:45:01.120810932 -0500
+++ lucene-3622/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/IfFunction.java	2011-12-12 15:20:30.605624898 -0500
@@ -19,7 +19,7 @@
 
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexReader.AtomicReaderContext;
-import org.apache.lucene.queries.function.DocValues;
+import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.search.Explanation;
 import org.apache.lucene.search.IndexSearcher;
@@ -43,12 +43,12 @@
   }
 
   @Override
-  public DocValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
-    final DocValues ifVals = ifSource.getValues(context, readerContext);
-    final DocValues trueVals = trueSource.getValues(context, readerContext);
-    final DocValues falseVals = falseSource.getValues(context, readerContext);
+  public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
+    final FunctionValues ifVals = ifSource.getValues(context, readerContext);
+    final FunctionValues trueVals = trueSource.getValues(context, readerContext);
+    final FunctionValues falseVals = falseSource.getValues(context, readerContext);
 
-    return new DocValues() {
+    return new FunctionValues() {
       @Override
       public byte byteVal(int doc) {
         return ifVals.boolVal(doc) ? trueVals.byteVal(doc) : falseVals.byteVal(doc);


diff -ruN -x .svn -x build lucene-clean-trunk/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/IntFieldSource.java lucene-3622/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/IntFieldSource.java
--- lucene-clean-trunk/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/IntFieldSource.java	2011-12-06 18:45:01.124810932 -0500
+++ lucene-3622/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/IntFieldSource.java	2011-12-12 15:20:30.609624899 -0500
@@ -22,7 +22,7 @@
 
 import org.apache.lucene.index.IndexReader.AtomicReaderContext;
 import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.queries.function.DocValues;
+import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSourceScorer;
 import org.apache.lucene.queries.function.docvalues.IntDocValues;
 import org.apache.lucene.search.FieldCache;
@@ -56,7 +56,7 @@
 
 
   @Override
-  public DocValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
+  public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
     final int[] arr = cache.getInts(readerContext.reader, field, parser, true);
     final Bits valid = cache.getDocsWithField(readerContext.reader, field);
     


diff -ruN -x .svn -x build lucene-clean-trunk/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/JoinDocFreqValueSource.java lucene-3622/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/JoinDocFreqValueSource.java
--- lucene-clean-trunk/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/JoinDocFreqValueSource.java	2011-12-06 18:45:01.124810932 -0500
+++ lucene-3622/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/JoinDocFreqValueSource.java	2011-12-12 15:20:30.613624899 -0500
@@ -22,7 +22,7 @@
 
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexReader.AtomicReaderContext;
-import org.apache.lucene.queries.function.DocValues;
+import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.docvalues.IntDocValues;
 import org.apache.lucene.search.FieldCache.DocTerms;
 import org.apache.lucene.util.BytesRef;
@@ -50,7 +50,7 @@
   }
 
   @Override
-  public DocValues getValues(Map context, AtomicReaderContext readerContext) throws IOException
+  public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException
   {
     final DocTerms terms = cache.getTerms(readerContext.reader, field, true );
     final IndexReader top = ReaderUtil.getTopLevelContext(readerContext).reader;


diff -ruN -x .svn -x build lucene-clean-trunk/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/LinearFloatFunction.java lucene-3622/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/LinearFloatFunction.java
--- lucene-clean-trunk/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/LinearFloatFunction.java	2011-12-06 18:45:01.112810932 -0500
+++ lucene-3622/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/LinearFloatFunction.java	2011-12-12 15:20:30.621624899 -0500
@@ -18,7 +18,7 @@
 package org.apache.lucene.queries.function.valuesource;
 
 import org.apache.lucene.index.IndexReader.AtomicReaderContext;
-import org.apache.lucene.queries.function.DocValues;
+import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.queries.function.docvalues.FloatDocValues;
 import org.apache.lucene.search.IndexSearcher;
@@ -51,8 +51,8 @@
   }
 
   @Override
-  public DocValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
-    final DocValues vals =  source.getValues(context, readerContext);
+  public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
+    final FunctionValues vals =  source.getValues(context, readerContext);
     return new FloatDocValues(this) {
       @Override
       public float floatVal(int doc) {


diff -ruN -x .svn -x build lucene-clean-trunk/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/LiteralValueSource.java lucene-3622/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/LiteralValueSource.java
--- lucene-clean-trunk/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/LiteralValueSource.java	2011-12-06 18:45:01.116810932 -0500
+++ lucene-3622/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/LiteralValueSource.java	2011-12-12 15:20:30.621624899 -0500
@@ -17,7 +17,7 @@
  */
 
 import org.apache.lucene.index.IndexReader.AtomicReaderContext;
-import org.apache.lucene.queries.function.DocValues;
+import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.queries.function.docvalues.StrDocValues;
 import org.apache.lucene.util.BytesRef;
@@ -45,7 +45,7 @@
   }
 
   @Override
-  public DocValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
+  public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
 
     return new StrDocValues(this) {
       @Override


diff -ruN -x .svn -x build lucene-clean-trunk/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/LongFieldSource.java lucene-3622/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/LongFieldSource.java
--- lucene-clean-trunk/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/LongFieldSource.java	2011-12-06 18:45:01.116810932 -0500
+++ lucene-3622/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/LongFieldSource.java	2011-12-12 15:20:30.625624899 -0500
@@ -22,7 +22,7 @@
 
 import org.apache.lucene.index.IndexReader.AtomicReaderContext;
 import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.queries.function.DocValues;
+import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSourceScorer;
 import org.apache.lucene.queries.function.docvalues.LongDocValues;
 import org.apache.lucene.search.FieldCache;
@@ -65,7 +65,7 @@
   }
 
   @Override
-  public DocValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
+  public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
     final long[] arr = cache.getLongs(readerContext.reader, field, parser, true);
     final Bits valid = cache.getDocsWithField(readerContext.reader, field);
     


diff -ruN -x .svn -x build lucene-clean-trunk/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/MaxDocValueSource.java lucene-3622/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/MaxDocValueSource.java
--- lucene-clean-trunk/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/MaxDocValueSource.java	2011-12-06 18:45:01.120810932 -0500
+++ lucene-3622/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/MaxDocValueSource.java	2011-12-12 15:20:30.625624899 -0500
@@ -17,7 +17,7 @@
 package org.apache.lucene.queries.function.valuesource;
 
 import org.apache.lucene.index.IndexReader.AtomicReaderContext;
-import org.apache.lucene.queries.function.DocValues;
+import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.search.IndexSearcher;
 
@@ -40,7 +40,7 @@
   }
 
   @Override
-  public DocValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
+  public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
     IndexSearcher searcher = (IndexSearcher)context.get("searcher");
     return new ConstIntDocValues(searcher.getIndexReader().maxDoc(), this);
   }


diff -ruN -x .svn -x build lucene-clean-trunk/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/MaxFloatFunction.java lucene-3622/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/MaxFloatFunction.java
--- lucene-clean-trunk/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/MaxFloatFunction.java	2011-12-06 18:45:01.116810932 -0500
+++ lucene-3622/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/MaxFloatFunction.java	2011-12-12 15:20:30.625624899 -0500
@@ -17,7 +17,7 @@
 
 package org.apache.lucene.queries.function.valuesource;
 
-import org.apache.lucene.queries.function.DocValues;
+import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 
 /**
@@ -34,10 +34,10 @@
   }
 
   @Override
-  protected float func(int doc, DocValues[] valsArr) {
+  protected float func(int doc, FunctionValues[] valsArr) {
     boolean first = true;
     float val = 0.0f;
-    for (DocValues vals : valsArr) {
+    for (FunctionValues vals : valsArr) {
       if (first) {
         first = false;
         val = vals.floatVal(doc);


diff -ruN -x .svn -x build lucene-clean-trunk/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/MinFloatFunction.java lucene-3622/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/MinFloatFunction.java
--- lucene-clean-trunk/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/MinFloatFunction.java	2011-12-06 18:45:01.116810932 -0500
+++ lucene-3622/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/MinFloatFunction.java	2011-12-12 15:20:30.629624899 -0500
@@ -17,7 +17,7 @@
 
 package org.apache.lucene.queries.function.valuesource;
 
-import org.apache.lucene.queries.function.DocValues;
+import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 
 /**
@@ -34,10 +34,10 @@
   }
 
   @Override
-  protected float func(int doc, DocValues[] valsArr) {
+  protected float func(int doc, FunctionValues[] valsArr) {
     boolean first = true;
     float val = 0.0f;
-    for (DocValues vals : valsArr) {
+    for (FunctionValues vals : valsArr) {
       if (first) {
         first = false;
         val = vals.floatVal(doc);


diff -ruN -x .svn -x build lucene-clean-trunk/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/MultiBoolFunction.java lucene-3622/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/MultiBoolFunction.java
--- lucene-clean-trunk/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/MultiBoolFunction.java	2011-12-06 18:45:01.124810932 -0500
+++ lucene-3622/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/MultiBoolFunction.java	2011-12-12 15:20:30.629624899 -0500
@@ -18,7 +18,7 @@
 package org.apache.lucene.queries.function.valuesource;
 
 import org.apache.lucene.index.IndexReader.AtomicReaderContext;
-import org.apache.lucene.queries.function.DocValues;
+import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.queries.function.docvalues.BoolDocValues;
 import org.apache.lucene.search.IndexSearcher;
@@ -37,11 +37,11 @@
 
   protected abstract String name();
 
-  protected abstract boolean func(int doc, DocValues[] vals);
+  protected abstract boolean func(int doc, FunctionValues[] vals);
 
   @Override
   public BoolDocValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
-    final DocValues[] vals =  new DocValues[sources.size()];
+    final FunctionValues[] vals =  new FunctionValues[sources.size()];
     int i=0;
     for (ValueSource source : sources) {
       vals[i++] = source.getValues(context, readerContext);
@@ -58,7 +58,7 @@
         StringBuilder sb = new StringBuilder(name());
         sb.append('(');
         boolean first = true;
-        for (DocValues dv : vals) {
+        for (FunctionValues dv : vals) {
           if (first) {
             first = false;
           } else {


diff -ruN -x .svn -x build lucene-clean-trunk/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/MultiFloatFunction.java lucene-3622/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/MultiFloatFunction.java
--- lucene-clean-trunk/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/MultiFloatFunction.java	2011-12-06 18:45:01.124810932 -0500
+++ lucene-3622/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/MultiFloatFunction.java	2011-12-12 15:20:30.629624899 -0500
@@ -17,7 +17,7 @@
  */
 
 import org.apache.lucene.index.IndexReader.AtomicReaderContext;
-import org.apache.lucene.queries.function.DocValues;
+import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.queries.function.docvalues.FloatDocValues;
 import org.apache.lucene.search.IndexSearcher;
@@ -39,7 +39,7 @@
   }
 
   abstract protected String name();
-  abstract protected float func(int doc, DocValues[] valsArr);
+  abstract protected float func(int doc, FunctionValues[] valsArr);
 
   @Override
   public String description() {
@@ -59,8 +59,8 @@
   }
 
   @Override
-  public DocValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
-    final DocValues[] valsArr = new DocValues[sources.length];
+  public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
+    final FunctionValues[] valsArr = new FunctionValues[sources.length];
     for (int i=0; i<sources.length; i++) {
       valsArr[i] = sources[i].getValues(context, readerContext);
     }
@@ -75,7 +75,7 @@
         StringBuilder sb = new StringBuilder();
         sb.append(name()).append('(');
         boolean firstTime=true;
-        for (DocValues vals : valsArr) {
+        for (FunctionValues vals : valsArr) {
           if (firstTime) {
             firstTime=false;
           } else {


diff -ruN -x .svn -x build lucene-clean-trunk/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/MultiFunction.java lucene-3622/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/MultiFunction.java
--- lucene-clean-trunk/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/MultiFunction.java	2011-12-06 18:45:01.112810932 -0500
+++ lucene-3622/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/MultiFunction.java	2011-12-12 15:20:30.633624899 -0500
@@ -17,7 +17,7 @@
  */
 
 import org.apache.lucene.index.IndexReader.AtomicReaderContext;
-import org.apache.lucene.queries.function.DocValues;
+import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.search.IndexSearcher;
 import org.apache.lucene.util.BytesRef;
@@ -58,8 +58,8 @@
     return sb.toString();
   }
 
-  public static DocValues[] valsArr(List<ValueSource> sources, Map fcontext, AtomicReaderContext readerContext) throws IOException {
-    final DocValues[] valsArr = new DocValues[sources.size()];
+  public static FunctionValues[] valsArr(List<ValueSource> sources, Map fcontext, AtomicReaderContext readerContext) throws IOException {
+    final FunctionValues[] valsArr = new FunctionValues[sources.size()];
     int i=0;
     for (ValueSource source : sources) {
       valsArr[i++] = source.getValues(fcontext, readerContext);
@@ -67,10 +67,10 @@
     return valsArr;
   }
 
-  public class Values extends DocValues {
-    final DocValues[] valsArr;
+  public class Values extends FunctionValues {
+    final FunctionValues[] valsArr;
 
-    public Values(DocValues[] valsArr) {
+    public Values(FunctionValues[] valsArr) {
       this.valsArr = valsArr;
     }
 
@@ -87,11 +87,11 @@
   }
 
 
-  public static String toString(String name, DocValues[] valsArr, int doc) {
+  public static String toString(String name, FunctionValues[] valsArr, int doc) {
     StringBuilder sb = new StringBuilder();
     sb.append(name).append('(');
     boolean firstTime=true;
-    for (DocValues vals : valsArr) {
+    for (FunctionValues vals : valsArr) {
       if (firstTime) {
         firstTime=false;
       } else {


diff -ruN -x .svn -x build lucene-clean-trunk/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/NormValueSource.java lucene-3622/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/NormValueSource.java
--- lucene-clean-trunk/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/NormValueSource.java	2011-12-06 18:45:01.128810932 -0500
+++ lucene-3622/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/NormValueSource.java	2011-12-12 15:20:30.633624899 -0500
@@ -18,7 +18,7 @@
 package org.apache.lucene.queries.function.valuesource;
 
 import org.apache.lucene.index.IndexReader.AtomicReaderContext;
-import org.apache.lucene.queries.function.DocValues;
+import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.queries.function.docvalues.FloatDocValues;
 import org.apache.lucene.search.IndexSearcher;
@@ -49,7 +49,7 @@
   }
 
   @Override
-  public DocValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
+  public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
     IndexSearcher searcher = (IndexSearcher)context.get("searcher");
     Similarity sim = searcher.getSimilarityProvider().get(field);
     if (!(sim instanceof TFIDFSimilarity)) {


diff -ruN -x .svn -x build lucene-clean-trunk/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/NumDocsValueSource.java lucene-3622/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/NumDocsValueSource.java
--- lucene-clean-trunk/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/NumDocsValueSource.java	2011-12-06 18:45:01.120810932 -0500
+++ lucene-3622/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/NumDocsValueSource.java	2011-12-12 15:20:30.637624899 -0500
@@ -17,7 +17,7 @@
 package org.apache.lucene.queries.function.valuesource;
 
 import org.apache.lucene.index.IndexReader.AtomicReaderContext;
-import org.apache.lucene.queries.function.DocValues;
+import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.util.ReaderUtil;
 
@@ -35,7 +35,7 @@
   }
 
   @Override
-  public DocValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
+  public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
     // Searcher has no numdocs so we must use the reader instead
     return new ConstIntDocValues(ReaderUtil.getTopLevelContext(readerContext).reader.numDocs(), this);
   }


diff -ruN -x .svn -x build lucene-clean-trunk/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/NumericIndexDocValueSource.java lucene-3622/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/NumericIndexDocValueSource.java
--- lucene-clean-trunk/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/NumericIndexDocValueSource.java	2011-12-06 18:45:01.120810932 -0500
+++ lucene-3622/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/NumericIndexDocValueSource.java	2011-12-12 15:20:30.637624899 -0500
@@ -19,16 +19,16 @@
 import java.io.IOException;
 import java.util.Map;
 
+import org.apache.lucene.index.DocValues.Source;
+import org.apache.lucene.index.DocValues.Type;
 import org.apache.lucene.index.IndexReader.AtomicReaderContext;
-import org.apache.lucene.index.values.IndexDocValues;
-import org.apache.lucene.index.values.ValueType;
-import org.apache.lucene.queries.function.DocValues;
+import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 
 /**
- * Expert: obtains numeric field values from a {@link IndexDocValues} field.
+ * Expert: obtains numeric field values from a {@link FunctionValues} field.
  * This {@link ValueSource} is compatible with all numerical
- * {@link IndexDocValues}
+ * {@link FunctionValues}
  * 
  * @lucene.experimental
  * 
@@ -42,15 +42,15 @@
   }
 
   @Override
-  public DocValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
-    final IndexDocValues.Source source = readerContext.reader.docValues(field)
+  public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
+    final Source source = readerContext.reader.docValues(field)
         .getSource();
-    ValueType type = source.type();
+    Type type = source.type();
     switch (type) {
     case FLOAT_32:
     case FLOAT_64:
       // TODO (chrism) Change to use FloatDocValues and IntDocValues
-      return new DocValues() {
+      return new FunctionValues() {
 
         @Override
         public String toString(int doc) {
@@ -64,7 +64,7 @@
       };
 
     case VAR_INTS:
-      return new DocValues() {
+      return new FunctionValues() {
         @Override
         public String toString(int doc) {
           return "float: [" + floatVal(doc) + "]";
@@ -113,6 +113,6 @@
 
   @Override
   public String toString() {
-    return "DocValues float(" + field + ')';
+    return "FunctionValues float(" + field + ')';
   }
 }


diff -ruN -x .svn -x build lucene-clean-trunk/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/OrdFieldSource.java lucene-3622/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/OrdFieldSource.java
--- lucene-clean-trunk/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/OrdFieldSource.java	2011-12-06 18:45:01.124810932 -0500
+++ lucene-3622/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/OrdFieldSource.java	2011-12-12 15:20:30.641624899 -0500
@@ -19,7 +19,7 @@
 
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexReader.AtomicReaderContext;
-import org.apache.lucene.queries.function.DocValues;
+import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.queries.function.docvalues.IntDocValues;
 import org.apache.lucene.search.FieldCache;
@@ -62,7 +62,7 @@
 
 
   @Override
-  public DocValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
+  public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
     final int off = readerContext.docBase;
     final IndexReader topReader = ReaderUtil.getTopLevelContext(readerContext).reader;
     final FieldCache.DocTermsIndex sindex = FieldCache.DEFAULT.getTermsIndex(topReader, field);


diff -ruN -x .svn -x build lucene-clean-trunk/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/PowFloatFunction.java lucene-3622/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/PowFloatFunction.java
--- lucene-clean-trunk/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/PowFloatFunction.java	2011-12-06 18:45:01.120810932 -0500
+++ lucene-3622/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/PowFloatFunction.java	2011-12-12 15:20:30.645624899 -0500
@@ -17,7 +17,7 @@
 
 package org.apache.lucene.queries.function.valuesource;
 
-import org.apache.lucene.queries.function.DocValues;
+import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 
 /** Function to raise the base "a" to the power "b"
@@ -37,7 +37,7 @@
   }
 
   @Override
-  protected float func(int doc, DocValues aVals, DocValues bVals) {
+  protected float func(int doc, FunctionValues aVals, FunctionValues bVals) {
     return (float)Math.pow(aVals.floatVal(doc), bVals.floatVal(doc));
   }
 }


diff -ruN -x .svn -x build lucene-clean-trunk/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/ProductFloatFunction.java lucene-3622/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/ProductFloatFunction.java
--- lucene-clean-trunk/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/ProductFloatFunction.java	2011-12-06 18:45:01.116810932 -0500
+++ lucene-3622/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/ProductFloatFunction.java	2011-12-12 15:20:30.645624899 -0500
@@ -17,7 +17,7 @@
 
 package org.apache.lucene.queries.function.valuesource;
 
-import org.apache.lucene.queries.function.DocValues;
+import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 
 /**
@@ -34,9 +34,9 @@
   }
 
   @Override
-  protected float func(int doc, DocValues[] valsArr) {
+  protected float func(int doc, FunctionValues[] valsArr) {
     float val = 1.0f;
-    for (DocValues vals : valsArr) {
+    for (FunctionValues vals : valsArr) {
       val *= vals.floatVal(doc);
     }
     return val;


diff -ruN -x .svn -x build lucene-clean-trunk/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/QueryValueSource.java lucene-3622/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/QueryValueSource.java
--- lucene-clean-trunk/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/QueryValueSource.java	2011-12-06 18:45:01.116810932 -0500
+++ lucene-3622/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/QueryValueSource.java	2011-12-12 15:20:30.649624899 -0500
@@ -18,7 +18,7 @@
 package org.apache.lucene.queries.function.valuesource;
 
 import org.apache.lucene.index.IndexReader.AtomicReaderContext;
-import org.apache.lucene.queries.function.DocValues;
+import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.queries.function.docvalues.FloatDocValues;
 import org.apache.lucene.search.*;
@@ -51,7 +51,7 @@
   }
 
   @Override
-  public DocValues getValues(Map fcontext, AtomicReaderContext readerContext) throws IOException {
+  public FunctionValues getValues(Map fcontext, AtomicReaderContext readerContext) throws IOException {
     return new QueryDocValues(this, readerContext, fcontext);
   }
 
@@ -193,8 +193,8 @@
   public ValueFiller getValueFiller() {
     //
     // TODO: if we want to support more than one value-filler or a value-filler in conjunction with
-    // the DocValues, then members like "scorer" should be per ValueFiller instance.
-    // Or we can say that the user should just instantiate multiple DocValues.
+    // the FunctionValues, then members like "scorer" should be per ValueFiller instance.
+    // Or we can say that the user should just instantiate multiple FunctionValues.
     //
     return new ValueFiller() {
       private final MutableValueFloat mval = new MutableValueFloat();


diff -ruN -x .svn -x build lucene-clean-trunk/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/RangeMapFloatFunction.java lucene-3622/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/RangeMapFloatFunction.java
--- lucene-clean-trunk/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/RangeMapFloatFunction.java	2011-12-06 18:45:01.116810932 -0500
+++ lucene-3622/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/RangeMapFloatFunction.java	2011-12-12 15:20:30.653624899 -0500
@@ -18,7 +18,7 @@
 package org.apache.lucene.queries.function.valuesource;
 
 import org.apache.lucene.index.IndexReader.AtomicReaderContext;
-import org.apache.lucene.queries.function.DocValues;
+import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.queries.function.docvalues.FloatDocValues;
 import org.apache.lucene.search.IndexSearcher;
@@ -55,8 +55,8 @@
   }
 
   @Override
-  public DocValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
-    final DocValues vals =  source.getValues(context, readerContext);
+  public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
+    final FunctionValues vals =  source.getValues(context, readerContext);
     return new FloatDocValues(this) {
       @Override
       public float floatVal(int doc) {


diff -ruN -x .svn -x build lucene-clean-trunk/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/ReciprocalFloatFunction.java lucene-3622/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/ReciprocalFloatFunction.java
--- lucene-clean-trunk/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/ReciprocalFloatFunction.java	2011-12-06 18:45:01.124810932 -0500
+++ lucene-3622/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/ReciprocalFloatFunction.java	2011-12-12 15:20:30.653624899 -0500
@@ -18,7 +18,7 @@
 package org.apache.lucene.queries.function.valuesource;
 
 import org.apache.lucene.index.IndexReader.AtomicReaderContext;
-import org.apache.lucene.queries.function.DocValues;
+import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.queries.function.docvalues.FloatDocValues;
 import org.apache.lucene.search.IndexSearcher;
@@ -61,8 +61,8 @@
   }
 
   @Override
-  public DocValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
-    final DocValues vals = source.getValues(context, readerContext);
+  public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
+    final FunctionValues vals = source.getValues(context, readerContext);
     return new FloatDocValues(this) {
       @Override
       public float floatVal(int doc) {


diff -ruN -x .svn -x build lucene-clean-trunk/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/ReverseOrdFieldSource.java lucene-3622/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/ReverseOrdFieldSource.java
--- lucene-clean-trunk/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/ReverseOrdFieldSource.java	2011-12-06 18:45:01.120810932 -0500
+++ lucene-3622/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/ReverseOrdFieldSource.java	2011-12-12 15:20:30.653624899 -0500
@@ -19,7 +19,7 @@
 
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexReader.AtomicReaderContext;
-import org.apache.lucene.queries.function.DocValues;
+import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.queries.function.docvalues.IntDocValues;
 import org.apache.lucene.search.FieldCache;
@@ -62,7 +62,7 @@
   }
 
   @Override
-  public DocValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
+  public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
     final IndexReader topReader = ReaderUtil.getTopLevelContext(readerContext).reader;
     final int off = readerContext.docBase;
 


diff -ruN -x .svn -x build lucene-clean-trunk/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/ScaleFloatFunction.java lucene-3622/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/ScaleFloatFunction.java
--- lucene-clean-trunk/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/ScaleFloatFunction.java	2011-12-06 18:45:01.116810932 -0500
+++ lucene-3622/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/ScaleFloatFunction.java	2011-12-12 15:20:30.657624898 -0500
@@ -18,7 +18,7 @@
 package org.apache.lucene.queries.function.valuesource;
 
 import org.apache.lucene.index.IndexReader.AtomicReaderContext;
-import org.apache.lucene.queries.function.DocValues;
+import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.queries.function.docvalues.FloatDocValues;
 import org.apache.lucene.search.IndexSearcher;
@@ -67,7 +67,7 @@
 
     for (AtomicReaderContext leaf : leaves) {
       int maxDoc = leaf.reader.maxDoc();
-      DocValues vals =  source.getValues(context, leaf);
+      FunctionValues vals =  source.getValues(context, leaf);
       for (int i=0; i<maxDoc; i++) {
 
       float val = vals.floatVal(i);
@@ -98,7 +98,7 @@
   }
 
   @Override
-  public DocValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
+  public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
 
     ScaleInfo scaleInfo = (ScaleInfo)context.get(source);
     if (scaleInfo == null) {
@@ -109,7 +109,7 @@
     final float minSource = scaleInfo.minVal;
     final float maxSource = scaleInfo.maxVal;
 
-    final DocValues vals =  source.getValues(context, readerContext);
+    final FunctionValues vals =  source.getValues(context, readerContext);
 
     return new FloatDocValues(this) {
       @Override


diff -ruN -x .svn -x build lucene-clean-trunk/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/ShortFieldSource.java lucene-3622/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/ShortFieldSource.java
--- lucene-clean-trunk/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/ShortFieldSource.java	2011-12-06 18:45:01.116810932 -0500
+++ lucene-3622/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/ShortFieldSource.java	2011-12-12 15:20:30.657624898 -0500
@@ -20,7 +20,7 @@
 import java.util.Map;
 
 import org.apache.lucene.index.IndexReader.AtomicReaderContext;
-import org.apache.lucene.queries.function.DocValues;
+import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.search.FieldCache;
 
 
@@ -47,10 +47,10 @@
   }
 
   @Override
-  public DocValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
+  public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
     final short[] arr = cache.getShorts(readerContext.reader, field, parser, false);
     
-    return new DocValues() {
+    return new FunctionValues() {
       @Override
       public byte byteVal(int doc) {
         return (byte) arr[doc];


diff -ruN -x .svn -x build lucene-clean-trunk/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/SimpleBoolFunction.java lucene-3622/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/SimpleBoolFunction.java
--- lucene-clean-trunk/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/SimpleBoolFunction.java	2011-12-06 18:45:01.116810932 -0500
+++ lucene-3622/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/SimpleBoolFunction.java	2011-12-12 15:20:30.657624898 -0500
@@ -18,7 +18,7 @@
 package org.apache.lucene.queries.function.valuesource;
 
 import org.apache.lucene.index.IndexReader.AtomicReaderContext;
-import org.apache.lucene.queries.function.DocValues;
+import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.queries.function.docvalues.BoolDocValues;
 import org.apache.lucene.search.IndexSearcher;
@@ -36,11 +36,11 @@
 
   protected abstract String name();
 
-  protected abstract boolean func(int doc, DocValues vals);
+  protected abstract boolean func(int doc, FunctionValues vals);
 
   @Override
   public BoolDocValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
-    final DocValues vals =  source.getValues(context, readerContext);
+    final FunctionValues vals =  source.getValues(context, readerContext);
     return new BoolDocValues(this) {
       @Override
       public boolean boolVal(int doc) {


diff -ruN -x .svn -x build lucene-clean-trunk/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/SimpleFloatFunction.java lucene-3622/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/SimpleFloatFunction.java
--- lucene-clean-trunk/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/SimpleFloatFunction.java	2011-12-06 18:45:01.124810932 -0500
+++ lucene-3622/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/SimpleFloatFunction.java	2011-12-12 15:20:30.661624899 -0500
@@ -18,7 +18,7 @@
 package org.apache.lucene.queries.function.valuesource;
 
 import org.apache.lucene.index.IndexReader.AtomicReaderContext;
-import org.apache.lucene.queries.function.DocValues;
+import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.queries.function.docvalues.FloatDocValues;
 
@@ -32,11 +32,11 @@
     super(source);
   }
 
-  protected abstract float func(int doc, DocValues vals);
+  protected abstract float func(int doc, FunctionValues vals);
 
   @Override
-  public DocValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
-    final DocValues vals =  source.getValues(context, readerContext);
+  public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
+    final FunctionValues vals =  source.getValues(context, readerContext);
     return new FloatDocValues(this) {
       @Override
       public float floatVal(int doc) {


diff -ruN -x .svn -x build lucene-clean-trunk/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/SumFloatFunction.java lucene-3622/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/SumFloatFunction.java
--- lucene-clean-trunk/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/SumFloatFunction.java	2011-12-06 18:45:01.120810932 -0500
+++ lucene-3622/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/SumFloatFunction.java	2011-12-12 15:20:30.673624900 -0500
@@ -17,7 +17,7 @@
 
 package org.apache.lucene.queries.function.valuesource;
 
-import org.apache.lucene.queries.function.DocValues;
+import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 
 /**
@@ -34,9 +34,9 @@
   }
 
   @Override
-  protected float func(int doc, DocValues[] valsArr) {
+  protected float func(int doc, FunctionValues[] valsArr) {
     float val = 0.0f;
-    for (DocValues vals : valsArr) {
+    for (FunctionValues vals : valsArr) {
       val += vals.floatVal(doc);
     }
     return val;


diff -ruN -x .svn -x build lucene-clean-trunk/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/SumTotalTermFreqValueSource.java lucene-3622/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/SumTotalTermFreqValueSource.java
--- lucene-clean-trunk/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/SumTotalTermFreqValueSource.java	2011-12-06 18:45:01.120810932 -0500
+++ lucene-3622/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/SumTotalTermFreqValueSource.java	2011-12-12 15:20:30.677624900 -0500
@@ -20,7 +20,7 @@
 import org.apache.lucene.index.Fields;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.Terms;
-import org.apache.lucene.queries.function.DocValues;
+import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.queries.function.docvalues.LongDocValues;
 import org.apache.lucene.search.IndexSearcher;
@@ -50,8 +50,8 @@
   }
 
   @Override
-  public DocValues getValues(Map context, IndexReader.AtomicReaderContext readerContext) throws IOException {
-    return (DocValues)context.get(this);
+  public FunctionValues getValues(Map context, IndexReader.AtomicReaderContext readerContext) throws IOException {
+    return (FunctionValues)context.get(this);
   }
 
   @Override


diff -ruN -x .svn -x build lucene-clean-trunk/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/TermFreqValueSource.java lucene-3622/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/TermFreqValueSource.java
--- lucene-clean-trunk/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/TermFreqValueSource.java	2011-12-06 18:45:01.120810932 -0500
+++ lucene-3622/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/TermFreqValueSource.java	2011-12-12 15:20:30.677624900 -0500
@@ -19,7 +19,7 @@
 
 import org.apache.lucene.index.*;
 import org.apache.lucene.index.IndexReader.AtomicReaderContext;
-import org.apache.lucene.queries.function.DocValues;
+import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.docvalues.IntDocValues;
 import org.apache.lucene.search.DocIdSetIterator;
 import org.apache.lucene.util.BytesRef;
@@ -38,7 +38,7 @@
   }
 
   @Override
-  public DocValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
+  public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
     Fields fields = readerContext.reader.fields();
     final Terms terms = fields.terms(field);
 


diff -ruN -x .svn -x build lucene-clean-trunk/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/TFValueSource.java lucene-3622/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/TFValueSource.java
--- lucene-clean-trunk/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/TFValueSource.java	2011-12-06 18:45:01.128810932 -0500
+++ lucene-3622/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/TFValueSource.java	2011-12-12 15:20:30.677624900 -0500
@@ -19,7 +19,7 @@
 
 import org.apache.lucene.index.*;
 import org.apache.lucene.index.IndexReader.AtomicReaderContext;
-import org.apache.lucene.queries.function.DocValues;
+import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.docvalues.FloatDocValues;
 import org.apache.lucene.search.DocIdSetIterator;
 import org.apache.lucene.search.IndexSearcher;
@@ -41,7 +41,7 @@
   }
 
   @Override
-  public DocValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
+  public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
     Fields fields = readerContext.reader.fields();
     final Terms terms = fields.terms(field);
     final Similarity sim = ((IndexSearcher)context.get("searcher")).getSimilarityProvider().get(field);


diff -ruN -x .svn -x build lucene-clean-trunk/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/TotalTermFreqValueSource.java lucene-3622/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/TotalTermFreqValueSource.java
--- lucene-clean-trunk/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/TotalTermFreqValueSource.java	2011-12-06 18:45:01.124810932 -0500
+++ lucene-3622/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/TotalTermFreqValueSource.java	2011-12-12 15:20:30.681624900 -0500
@@ -18,7 +18,7 @@
 package org.apache.lucene.queries.function.valuesource;
 
 import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.queries.function.DocValues;
+import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.queries.function.docvalues.LongDocValues;
 import org.apache.lucene.search.IndexSearcher;
@@ -54,8 +54,8 @@
   }
 
   @Override
-  public DocValues getValues(Map context, IndexReader.AtomicReaderContext readerContext) throws IOException {
-    return (DocValues)context.get(this);
+  public FunctionValues getValues(Map context, IndexReader.AtomicReaderContext readerContext) throws IOException {
+    return (FunctionValues)context.get(this);
   }
 
   @Override


diff -ruN -x .svn -x build lucene-clean-trunk/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/VectorValueSource.java lucene-3622/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/VectorValueSource.java
--- lucene-clean-trunk/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/VectorValueSource.java	2011-12-06 18:45:01.116810932 -0500
+++ lucene-3622/modules/queries/src/java/org/apache/lucene/queries/function/valuesource/VectorValueSource.java	2011-12-12 15:20:30.697624900 -0500
@@ -17,7 +17,7 @@
  */
 
 import org.apache.lucene.index.IndexReader.AtomicReaderContext;
-import org.apache.lucene.queries.function.DocValues;
+import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.search.IndexSearcher;
 
@@ -27,8 +27,8 @@
 
 
 /**
- * Converts individual ValueSource instances to leverage the DocValues *Val functions that work with multiple values,
- * i.e. {@link org.apache.lucene.queries.function.DocValues#doubleVal(int, double[])}
+ * Converts individual ValueSource instances to leverage the FunctionValues *Val functions that work with multiple values,
+ * i.e. {@link org.apache.lucene.queries.function.FunctionValues#doubleVal(int, double[])}
  */
 //Not crazy about the name, but...
 public class VectorValueSource extends MultiValueSource {
@@ -53,14 +53,14 @@
   }
 
   @Override
-  public DocValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
+  public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
     int size = sources.size();
 
     // special-case x,y and lat,lon since it's so common
     if (size==2) {
-      final DocValues x = sources.get(0).getValues(context, readerContext);
-      final DocValues y = sources.get(1).getValues(context, readerContext);
-      return new DocValues() {
+      final FunctionValues x = sources.get(0).getValues(context, readerContext);
+      final FunctionValues y = sources.get(1).getValues(context, readerContext);
+      return new FunctionValues() {
         @Override
         public void byteVal(int doc, byte[] vals) {
           vals[0] = x.byteVal(doc);
@@ -105,12 +105,12 @@
     }
 
 
-    final DocValues[] valsArr = new DocValues[size];
+    final FunctionValues[] valsArr = new FunctionValues[size];
     for (int i = 0; i < size; i++) {
       valsArr[i] = sources.get(i).getValues(context, readerContext);
     }
 
-    return new DocValues() {
+    return new FunctionValues() {
       @Override
       public void byteVal(int doc, byte[] vals) {
         for (int i = 0; i < valsArr.length; i++) {
@@ -165,7 +165,7 @@
         StringBuilder sb = new StringBuilder();
         sb.append(name()).append('(');
         boolean firstTime = true;
-        for (DocValues vals : valsArr) {
+        for (FunctionValues vals : valsArr) {
           if (firstTime) {
             firstTime = false;
           } else {


diff -ruN -x .svn -x build lucene-clean-trunk/modules/queries/src/java/org/apache/lucene/queries/function/ValueSource.java lucene-3622/modules/queries/src/java/org/apache/lucene/queries/function/ValueSource.java
--- lucene-clean-trunk/modules/queries/src/java/org/apache/lucene/queries/function/ValueSource.java	2011-12-06 18:45:01.112810932 -0500
+++ lucene-3622/modules/queries/src/java/org/apache/lucene/queries/function/ValueSource.java	2011-12-12 15:20:30.681624900 -0500
@@ -33,7 +33,7 @@
 import java.util.Map;
 
 /**
- * Instantiates {@link DocValues} for a particular reader.
+ * Instantiates {@link FunctionValues} for a particular reader.
  * <br>
  * Often used when creating a {@link FunctionQuery}.
  *
@@ -45,7 +45,7 @@
    * Gets the values for this reader and the context that was previously
    * passed to createWeight()
    */
-  public abstract DocValues getValues(Map context, AtomicReaderContext readerContext) throws IOException;
+  public abstract FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException;
 
   @Override
   public abstract boolean equals(Object o);
@@ -129,12 +129,12 @@
 
   /**
    * Implement a {@link org.apache.lucene.search.FieldComparator} that works
-   * off of the {@link DocValues} for a ValueSource
+   * off of the {@link FunctionValues} for a ValueSource
    * instead of the normal Lucene FieldComparator that works off of a FieldCache.
    */
   class ValueSourceComparator extends FieldComparator<Double> {
     private final double[] values;
-    private DocValues docVals;
+    private FunctionValues docVals;
     private double bottom;
     private Map fcontext;
 


diff -ruN -x .svn -x build lucene-clean-trunk/modules/queries/src/java/org/apache/lucene/queries/function/ValueSourceScorer.java lucene-3622/modules/queries/src/java/org/apache/lucene/queries/function/ValueSourceScorer.java
--- lucene-clean-trunk/modules/queries/src/java/org/apache/lucene/queries/function/ValueSourceScorer.java	2011-12-06 18:45:01.112810932 -0500
+++ lucene-3622/modules/queries/src/java/org/apache/lucene/queries/function/ValueSourceScorer.java	2011-12-12 15:20:30.693624900 -0500
@@ -28,11 +28,11 @@
   protected IndexReader reader;
   private int doc = -1;
   protected final int maxDoc;
-  protected final DocValues values;
+  protected final FunctionValues values;
   protected boolean checkDeletes;
   private final Bits liveDocs;
 
-  protected ValueSourceScorer(IndexReader reader, DocValues values) {
+  protected ValueSourceScorer(IndexReader reader, FunctionValues values) {
     super(null);
     this.reader = reader;
     this.maxDoc = reader.maxDoc();


diff -ruN -x .svn -x build lucene-clean-trunk/solr/CHANGES.txt lucene-3622/solr/CHANGES.txt
--- lucene-clean-trunk/solr/CHANGES.txt	2011-12-10 12:55:54.406464590 -0500
+++ lucene-3622/solr/CHANGES.txt	2011-12-10 13:05:12.426474308 -0500
@@ -19,7 +19,7 @@
 See the tutorial at http://lucene.apache.org/solr/tutorial.html
 
 
-$Id: CHANGES.txt 1212735 2011-12-10 05:38:01Z yonik $
+$Id: CHANGES.txt 1212830 2011-12-10 18:05:08Z rmuir $
 
 ==================  4.0.0-dev ==================
 Versions of Major Components


diff -ruN -x .svn -x build lucene-clean-trunk/solr/contrib/dataimporthandler/CHANGES.txt lucene-3622/solr/contrib/dataimporthandler/CHANGES.txt
--- lucene-clean-trunk/solr/contrib/dataimporthandler/CHANGES.txt	2011-12-09 08:23:19.764675175 -0500
+++ lucene-3622/solr/contrib/dataimporthandler/CHANGES.txt	2011-12-09 08:30:41.904682874 -0500
@@ -7,7 +7,7 @@
 HTTP data sources quick and easy.
 
 
-$Id: CHANGES.txt 1212394 2011-12-09 13:17:12Z mvg $
+$Id: CHANGES.txt 1212405 2011-12-09 13:30:38Z rmuir $
 ==================  4.0.0-dev ==============
 
 (No Changes)


diff -ruN -x .svn -x build lucene-clean-trunk/solr/core/src/java/org/apache/solr/core/SolrCore.java lucene-3622/solr/core/src/java/org/apache/solr/core/SolrCore.java
--- lucene-clean-trunk/solr/core/src/java/org/apache/solr/core/SolrCore.java	2011-12-09 08:23:19.492675170 -0500
+++ lucene-3622/solr/core/src/java/org/apache/solr/core/SolrCore.java	2011-12-09 08:30:41.908682874 -0500
@@ -1828,11 +1828,11 @@
   }
 
   public String getSourceId() {
-    return "$Id: SolrCore.java 1212292 2011-12-09 09:13:39Z uschindler $";
+    return "$Id: SolrCore.java 1212405 2011-12-09 13:30:38Z rmuir $";
   }
 
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/core/SolrCore.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene3622/solr/core/src/java/org/apache/solr/core/SolrCore.java $";
   }
 
   public URL[] getDocs() {


diff -ruN -x .svn -x build lucene-clean-trunk/solr/core/src/java/org/apache/solr/handler/admin/CoreAdminHandler.java lucene-3622/solr/core/src/java/org/apache/solr/handler/admin/CoreAdminHandler.java
--- lucene-clean-trunk/solr/core/src/java/org/apache/solr/handler/admin/CoreAdminHandler.java	2011-12-09 08:23:19.492675170 -0500
+++ lucene-3622/solr/core/src/java/org/apache/solr/handler/admin/CoreAdminHandler.java	2011-12-09 08:30:41.908682874 -0500
@@ -604,16 +604,16 @@
 
   @Override
   public String getVersion() {
-    return "$Revision: 1212292 $";
+    return "$Revision: 1212405 $";
   }
 
   @Override
   public String getSourceId() {
-    return "$Id: CoreAdminHandler.java 1212292 2011-12-09 09:13:39Z uschindler $";
+    return "$Id: CoreAdminHandler.java 1212405 2011-12-09 13:30:38Z rmuir $";
   }
 
   @Override
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/handler/admin/CoreAdminHandler.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene3622/solr/core/src/java/org/apache/solr/handler/admin/CoreAdminHandler.java $";
   }
 }


diff -ruN -x .svn -x build lucene-clean-trunk/solr/core/src/java/org/apache/solr/handler/component/QueryComponent.java lucene-3622/solr/core/src/java/org/apache/solr/handler/component/QueryComponent.java
--- lucene-clean-trunk/solr/core/src/java/org/apache/solr/handler/component/QueryComponent.java	2011-12-09 12:24:31.612927194 -0500
+++ lucene-3622/solr/core/src/java/org/apache/solr/handler/component/QueryComponent.java	2011-12-10 13:05:12.434474308 -0500
@@ -1051,17 +1051,17 @@
 
   @Override
   public String getVersion() {
-    return "$Revision: 1212420 $";
+    return "$Revision: 1212830 $";
   }
 
   @Override
   public String getSourceId() {
-    return "$Id: QueryComponent.java 1212420 2011-12-09 13:49:03Z mvg $";
+    return "$Id: QueryComponent.java 1212830 2011-12-10 18:05:08Z rmuir $";
   }
 
   @Override
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/handler/component/QueryComponent.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene3622/solr/core/src/java/org/apache/solr/handler/component/QueryComponent.java $";
   }
 
   @Override


diff -ruN -x .svn -x build lucene-clean-trunk/solr/core/src/java/org/apache/solr/handler/component/QueryElevationComponent.java lucene-3622/solr/core/src/java/org/apache/solr/handler/component/QueryElevationComponent.java
--- lucene-clean-trunk/solr/core/src/java/org/apache/solr/handler/component/QueryElevationComponent.java	2011-12-08 10:46:43.555320396 -0500
+++ lucene-3622/solr/core/src/java/org/apache/solr/handler/component/QueryElevationComponent.java	2011-12-09 08:30:41.908682874 -0500
@@ -450,17 +450,17 @@
 
   @Override
   public String getVersion() {
-    return "$Revision: 1211827 $";
+    return "$Revision: 1212405 $";
   }
 
   @Override
   public String getSourceId() {
-    return "$Id: QueryElevationComponent.java 1211827 2011-12-08 11:09:47Z gsingers $";
+    return "$Id: QueryElevationComponent.java 1212405 2011-12-09 13:30:38Z rmuir $";
   }
 
   @Override
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/handler/component/QueryElevationComponent.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene3622/solr/core/src/java/org/apache/solr/handler/component/QueryElevationComponent.java $";
   }
 
   @Override


diff -ruN -x .svn -x build lucene-clean-trunk/solr/core/src/java/org/apache/solr/response/transform/ValueSourceAugmenter.java lucene-3622/solr/core/src/java/org/apache/solr/response/transform/ValueSourceAugmenter.java
--- lucene-clean-trunk/solr/core/src/java/org/apache/solr/response/transform/ValueSourceAugmenter.java	2011-12-06 18:44:13.108810096 -0500
+++ lucene-3622/solr/core/src/java/org/apache/solr/response/transform/ValueSourceAugmenter.java	2011-12-12 15:20:30.681624900 -0500
@@ -20,7 +20,7 @@
 import java.util.Map;
 
 import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.queries.function.DocValues;
+import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.util.ReaderUtil;
 import org.apache.solr.common.SolrDocument;
@@ -63,7 +63,7 @@
     try {
       IndexReader reader = qparser.getReq().getSearcher().getIndexReader();
       readerContexts = reader.getTopReaderContext().leaves();
-      docValuesArr = new DocValues[readerContexts.length];
+      docValuesArr = new FunctionValues[readerContexts.length];
 
       searcher = qparser.getReq().getSearcher();
       fcontext = ValueSource.newContext(searcher);
@@ -77,7 +77,7 @@
   Map fcontext;
   SolrIndexSearcher searcher;
   IndexReader.AtomicReaderContext[] readerContexts;
-  DocValues docValuesArr[];
+  FunctionValues docValuesArr[];
 
 
   @Override
@@ -89,7 +89,7 @@
       // TODO: calculate this stuff just once across diff functions
       int idx = ReaderUtil.subIndex(docid, readerContexts);
       IndexReader.AtomicReaderContext rcontext = readerContexts[idx];
-      DocValues values = docValuesArr[idx];
+      FunctionValues values = docValuesArr[idx];
       if (values == null) {
         docValuesArr[idx] = values = valueSource.getValues(fcontext, rcontext);
       }


diff -ruN -x .svn -x build lucene-clean-trunk/solr/core/src/java/org/apache/solr/schema/BoolField.java lucene-3622/solr/core/src/java/org/apache/solr/schema/BoolField.java
--- lucene-clean-trunk/solr/core/src/java/org/apache/solr/schema/BoolField.java	2011-12-06 18:44:12.608810087 -0500
+++ lucene-3622/solr/core/src/java/org/apache/solr/schema/BoolField.java	2011-12-12 15:20:30.497624897 -0500
@@ -22,7 +22,7 @@
 import org.apache.lucene.search.FieldCache;
 import org.apache.lucene.search.SortField;
 import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.queries.function.DocValues;
+import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.queries.function.docvalues.BoolDocValues;
 import org.apache.lucene.queries.function.valuesource.OrdFieldSource;
@@ -170,7 +170,7 @@
 
 
   @Override
-  public DocValues getValues(Map context, IndexReader.AtomicReaderContext readerContext) throws IOException {
+  public FunctionValues getValues(Map context, IndexReader.AtomicReaderContext readerContext) throws IOException {
     final FieldCache.DocTermsIndex sindex = FieldCache.DEFAULT.getTermsIndex(readerContext.reader, field);
 
     // figure out what ord maps to true


diff -ruN -x .svn -x build lucene-clean-trunk/solr/core/src/java/org/apache/solr/schema/DateField.java lucene-3622/solr/core/src/java/org/apache/solr/schema/DateField.java
--- lucene-clean-trunk/solr/core/src/java/org/apache/solr/schema/DateField.java	2011-12-06 18:44:12.616810087 -0500
+++ lucene-3622/solr/core/src/java/org/apache/solr/schema/DateField.java	2011-12-12 15:20:30.529624897 -0500
@@ -22,7 +22,7 @@
 import org.apache.lucene.search.Query;
 import org.apache.lucene.search.SortField;
 import org.apache.lucene.search.TermRangeQuery;
-import org.apache.lucene.queries.function.DocValues;
+import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.queries.function.docvalues.StringIndexDocValues;
 import org.apache.lucene.queries.function.valuesource.FieldCacheSource;
@@ -462,7 +462,7 @@
   }
 
   @Override
-  public DocValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
+  public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
     return new StringIndexDocValues(this, readerContext, field) {
       @Override
       protected String toTerm(String readableValue) {


diff -ruN -x .svn -x build lucene-clean-trunk/solr/core/src/java/org/apache/solr/schema/LatLonType.java lucene-3622/solr/core/src/java/org/apache/solr/schema/LatLonType.java
--- lucene-clean-trunk/solr/core/src/java/org/apache/solr/schema/LatLonType.java	2011-12-06 18:44:12.620810087 -0500
+++ lucene-3622/solr/core/src/java/org/apache/solr/schema/LatLonType.java	2011-12-12 15:20:30.617624899 -0500
@@ -20,7 +20,7 @@
 import org.apache.lucene.index.IndexableField;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexReader.AtomicReaderContext;
-import org.apache.lucene.queries.function.DocValues;
+import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.queries.function.valuesource.VectorValueSource;
 import org.apache.lucene.search.*;
@@ -383,8 +383,8 @@
     final int maxDoc;
     final float qWeight;
     int doc=-1;
-    final DocValues latVals;
-    final DocValues lonVals;
+    final FunctionValues latVals;
+    final FunctionValues lonVals;
     final Bits liveDocs;
 
 


diff -ruN -x .svn -x build lucene-clean-trunk/solr/core/src/java/org/apache/solr/schema/RandomSortField.java lucene-3622/solr/core/src/java/org/apache/solr/schema/RandomSortField.java
--- lucene-clean-trunk/solr/core/src/java/org/apache/solr/schema/RandomSortField.java	2011-12-06 18:44:12.620810087 -0500
+++ lucene-3622/solr/core/src/java/org/apache/solr/schema/RandomSortField.java	2011-12-12 15:20:30.649624899 -0500
@@ -23,7 +23,7 @@
 import org.apache.lucene.index.IndexableField;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexReader.AtomicReaderContext;
-import org.apache.lucene.queries.function.DocValues;
+import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.queries.function.docvalues.IntDocValues;
 import org.apache.lucene.search.*;
@@ -157,7 +157,7 @@
     }
 
     @Override
-    public DocValues getValues(Map context, final AtomicReaderContext readerContext) throws IOException {
+    public FunctionValues getValues(Map context, final AtomicReaderContext readerContext) throws IOException {
       return new IntDocValues(this) {
           private final int seed = getSeed(field, readerContext);
           @Override


diff -ruN -x .svn -x build lucene-clean-trunk/solr/core/src/java/org/apache/solr/schema/SortableDoubleField.java lucene-3622/solr/core/src/java/org/apache/solr/schema/SortableDoubleField.java
--- lucene-clean-trunk/solr/core/src/java/org/apache/solr/schema/SortableDoubleField.java	2011-12-06 18:44:12.612810087 -0500
+++ lucene-3622/solr/core/src/java/org/apache/solr/schema/SortableDoubleField.java	2011-12-12 15:20:30.661624899 -0500
@@ -17,7 +17,7 @@
 
 package org.apache.solr.schema;
 
-import org.apache.lucene.queries.function.DocValues;
+import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.queries.function.docvalues.StringIndexDocValues;
 import org.apache.lucene.queries.function.valuesource.FieldCacheSource;
@@ -122,7 +122,7 @@
   }
 
   @Override
-  public DocValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
+  public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
     final double def = defVal;
 
     return new StringIndexDocValues(this, readerContext, field) {


diff -ruN -x .svn -x build lucene-clean-trunk/solr/core/src/java/org/apache/solr/schema/SortableFloatField.java lucene-3622/solr/core/src/java/org/apache/solr/schema/SortableFloatField.java
--- lucene-clean-trunk/solr/core/src/java/org/apache/solr/schema/SortableFloatField.java	2011-12-06 18:44:12.620810087 -0500
+++ lucene-3622/solr/core/src/java/org/apache/solr/schema/SortableFloatField.java	2011-12-12 15:20:30.661624899 -0500
@@ -17,7 +17,7 @@
 
 package org.apache.solr.schema;
 
-import org.apache.lucene.queries.function.DocValues;
+import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.queries.function.docvalues.StringIndexDocValues;
 import org.apache.lucene.queries.function.valuesource.FieldCacheSource;
@@ -125,7 +125,7 @@
   }
 
   @Override
-  public DocValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
+  public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
     final float def = defVal;
 
     return new StringIndexDocValues(this, readerContext, field) {


diff -ruN -x .svn -x build lucene-clean-trunk/solr/core/src/java/org/apache/solr/schema/SortableIntField.java lucene-3622/solr/core/src/java/org/apache/solr/schema/SortableIntField.java
--- lucene-clean-trunk/solr/core/src/java/org/apache/solr/schema/SortableIntField.java	2011-12-06 18:44:12.624810087 -0500
+++ lucene-3622/solr/core/src/java/org/apache/solr/schema/SortableIntField.java	2011-12-12 15:20:30.665624900 -0500
@@ -17,7 +17,7 @@
 
 package org.apache.solr.schema;
 
-import org.apache.lucene.queries.function.DocValues;
+import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.queries.function.docvalues.StringIndexDocValues;
 import org.apache.lucene.queries.function.valuesource.FieldCacheSource;
@@ -127,7 +127,7 @@
   }
 
   @Override
-  public DocValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
+  public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
     final int def = defVal;
 
     return new StringIndexDocValues(this, readerContext, field) {


diff -ruN -x .svn -x build lucene-clean-trunk/solr/core/src/java/org/apache/solr/schema/SortableLongField.java lucene-3622/solr/core/src/java/org/apache/solr/schema/SortableLongField.java
--- lucene-clean-trunk/solr/core/src/java/org/apache/solr/schema/SortableLongField.java	2011-12-06 18:44:12.620810087 -0500
+++ lucene-3622/solr/core/src/java/org/apache/solr/schema/SortableLongField.java	2011-12-12 15:20:30.665624900 -0500
@@ -17,7 +17,7 @@
 
 package org.apache.solr.schema;
 
-import org.apache.lucene.queries.function.DocValues;
+import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.queries.function.docvalues.StringIndexDocValues;
 import org.apache.lucene.queries.function.valuesource.FieldCacheSource;
@@ -125,7 +125,7 @@
   }
 
   @Override
-  public DocValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
+  public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
     final long def = defVal;
 
     return new StringIndexDocValues(this, readerContext, field) {


diff -ruN -x .svn -x build lucene-clean-trunk/solr/core/src/java/org/apache/solr/schema/StrFieldSource.java lucene-3622/solr/core/src/java/org/apache/solr/schema/StrFieldSource.java
--- lucene-clean-trunk/solr/core/src/java/org/apache/solr/schema/StrFieldSource.java	2011-12-06 18:44:12.624810087 -0500
+++ lucene-3622/solr/core/src/java/org/apache/solr/schema/StrFieldSource.java	2011-12-12 15:20:30.669624900 -0500
@@ -18,7 +18,7 @@
 package org.apache.solr.schema;
 
 import org.apache.lucene.index.IndexReader.AtomicReaderContext;
-import org.apache.lucene.queries.function.DocValues;
+import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.docvalues.StringIndexDocValues;
 import org.apache.lucene.queries.function.valuesource.FieldCacheSource;
 
@@ -37,7 +37,7 @@
   }
 
   @Override
-  public DocValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
+  public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
     return new StringIndexDocValues(this, readerContext, field) {
 
       @Override


diff -ruN -x .svn -x build lucene-clean-trunk/solr/core/src/java/org/apache/solr/search/function/distance/GeohashFunction.java lucene-3622/solr/core/src/java/org/apache/solr/search/function/distance/GeohashFunction.java
--- lucene-clean-trunk/solr/core/src/java/org/apache/solr/search/function/distance/GeohashFunction.java	2011-12-06 18:44:12.788810090 -0500
+++ lucene-3622/solr/core/src/java/org/apache/solr/search/function/distance/GeohashFunction.java	2011-12-12 15:20:30.593624898 -0500
@@ -17,7 +17,7 @@
  */
 
 import org.apache.lucene.index.IndexReader.AtomicReaderContext;
-import org.apache.lucene.queries.function.DocValues;
+import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.spatial.geohash.GeoHashUtils;
 
@@ -46,12 +46,12 @@
   }
 
   @Override
-  public DocValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
-    final DocValues latDV = lat.getValues(context, readerContext);
-    final DocValues lonDV = lon.getValues(context, readerContext);
+  public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
+    final FunctionValues latDV = lat.getValues(context, readerContext);
+    final FunctionValues lonDV = lon.getValues(context, readerContext);
 
 
-    return new DocValues() {
+    return new FunctionValues() {
 
       @Override
       public String strVal(int doc) {


diff -ruN -x .svn -x build lucene-clean-trunk/solr/core/src/java/org/apache/solr/search/function/distance/GeohashHaversineFunction.java lucene-3622/solr/core/src/java/org/apache/solr/search/function/distance/GeohashHaversineFunction.java
--- lucene-clean-trunk/solr/core/src/java/org/apache/solr/search/function/distance/GeohashHaversineFunction.java	2011-12-06 18:44:12.792810090 -0500
+++ lucene-3622/solr/core/src/java/org/apache/solr/search/function/distance/GeohashHaversineFunction.java	2011-12-12 15:20:30.597624898 -0500
@@ -17,7 +17,7 @@
  */
 
 
-import org.apache.lucene.queries.function.DocValues;
+import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.queries.function.docvalues.DoubleDocValues;
 import org.apache.lucene.spatial.DistanceUtils;
@@ -55,9 +55,9 @@
   }
 
   @Override
-  public DocValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
-    final DocValues gh1DV = geoHash1.getValues(context, readerContext);
-    final DocValues gh2DV = geoHash2.getValues(context, readerContext);
+  public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
+    final FunctionValues gh1DV = geoHash1.getValues(context, readerContext);
+    final FunctionValues gh2DV = geoHash2.getValues(context, readerContext);
 
     return new DoubleDocValues(this) {
       @Override
@@ -75,7 +75,7 @@
     };
   }
 
-  protected double distance(int doc, DocValues gh1DV, DocValues gh2DV) {
+  protected double distance(int doc, FunctionValues gh1DV, FunctionValues gh2DV) {
     double result = 0;
     String h1 = gh1DV.strVal(doc);
     String h2 = gh2DV.strVal(doc);


diff -ruN -x .svn -x build lucene-clean-trunk/solr/core/src/java/org/apache/solr/search/function/distance/HaversineConstFunction.java lucene-3622/solr/core/src/java/org/apache/solr/search/function/distance/HaversineConstFunction.java
--- lucene-clean-trunk/solr/core/src/java/org/apache/solr/search/function/distance/HaversineConstFunction.java	2011-12-06 18:44:12.788810090 -0500
+++ lucene-3622/solr/core/src/java/org/apache/solr/search/function/distance/HaversineConstFunction.java	2011-12-12 15:20:30.597624898 -0500
@@ -17,7 +17,7 @@
  */
 
 import org.apache.lucene.index.IndexReader.AtomicReaderContext;
-import org.apache.lucene.queries.function.DocValues;
+import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.queries.function.docvalues.DoubleDocValues;
 import org.apache.lucene.queries.function.valuesource.ConstNumberSource;
@@ -198,9 +198,9 @@
   }
 
   @Override
-  public DocValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
-    final DocValues latVals = latSource.getValues(context, readerContext);
-    final DocValues lonVals = lonSource.getValues(context, readerContext);
+  public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
+    final FunctionValues latVals = latSource.getValues(context, readerContext);
+    final FunctionValues lonVals = lonSource.getValues(context, readerContext);
     final double latCenterRad = this.latCenter * DistanceUtils.DEGREES_TO_RADIANS;
     final double lonCenterRad = this.lonCenter * DistanceUtils.DEGREES_TO_RADIANS;
     final double latCenterRad_cos = this.latCenterRad_cos;


diff -ruN -x .svn -x build lucene-clean-trunk/solr/core/src/java/org/apache/solr/search/function/distance/HaversineFunction.java lucene-3622/solr/core/src/java/org/apache/solr/search/function/distance/HaversineFunction.java
--- lucene-clean-trunk/solr/core/src/java/org/apache/solr/search/function/distance/HaversineFunction.java	2011-12-06 18:44:12.792810090 -0500
+++ lucene-3622/solr/core/src/java/org/apache/solr/search/function/distance/HaversineFunction.java	2011-12-12 15:20:30.601624897 -0500
@@ -17,7 +17,7 @@
  */
 
 import org.apache.lucene.index.IndexReader.AtomicReaderContext;
-import org.apache.lucene.queries.function.DocValues;
+import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.queries.function.docvalues.DoubleDocValues;
 import org.apache.lucene.queries.function.valuesource.MultiValueSource;
@@ -70,7 +70,7 @@
    * @param p2DV
    * @return The haversine distance formula
    */
-  protected double distance(int doc, DocValues p1DV, DocValues p2DV) {
+  protected double distance(int doc, FunctionValues p1DV, FunctionValues p2DV) {
 
     double[] p1D = new double[2];
     double[] p2D = new double[2];
@@ -96,10 +96,10 @@
 
 
   @Override
-  public DocValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
-    final DocValues vals1 = p1.getValues(context, readerContext);
+  public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
+    final FunctionValues vals1 = p1.getValues(context, readerContext);
 
-    final DocValues vals2 = p2.getValues(context, readerContext);
+    final FunctionValues vals2 = p2.getValues(context, readerContext);
     return new DoubleDocValues(this) {
       @Override
       public double doubleVal(int doc) {


diff -ruN -x .svn -x build lucene-clean-trunk/solr/core/src/java/org/apache/solr/search/function/distance/SquaredEuclideanFunction.java lucene-3622/solr/core/src/java/org/apache/solr/search/function/distance/SquaredEuclideanFunction.java
--- lucene-clean-trunk/solr/core/src/java/org/apache/solr/search/function/distance/SquaredEuclideanFunction.java	2011-12-06 18:44:12.792810090 -0500
+++ lucene-3622/solr/core/src/java/org/apache/solr/search/function/distance/SquaredEuclideanFunction.java	2011-12-12 15:20:30.669624900 -0500
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-import org.apache.lucene.queries.function.DocValues;
+import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.valuesource.MultiValueSource;
 import org.apache.lucene.spatial.DistanceUtils;
 
@@ -43,7 +43,7 @@
    * @param doc The doc to score
    */
   @Override
-  protected double distance(int doc, DocValues dv1, DocValues dv2) {
+  protected double distance(int doc, FunctionValues dv1, FunctionValues dv2) {
 
     double[] vals1 = new double[source1.dimension()];
     double[] vals2 = new double[source1.dimension()];


diff -ruN -x .svn -x build lucene-clean-trunk/solr/core/src/java/org/apache/solr/search/function/distance/StringDistanceFunction.java lucene-3622/solr/core/src/java/org/apache/solr/search/function/distance/StringDistanceFunction.java
--- lucene-clean-trunk/solr/core/src/java/org/apache/solr/search/function/distance/StringDistanceFunction.java	2011-12-06 18:44:12.792810090 -0500
+++ lucene-3622/solr/core/src/java/org/apache/solr/search/function/distance/StringDistanceFunction.java	2011-12-12 15:20:30.673624900 -0500
@@ -18,7 +18,7 @@
  */
 
 import org.apache.lucene.index.IndexReader.AtomicReaderContext;
-import org.apache.lucene.queries.function.DocValues;
+import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.queries.function.docvalues.FloatDocValues;
 import org.apache.lucene.search.spell.StringDistance;
@@ -49,9 +49,9 @@
   }
 
   @Override
-  public DocValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
-    final DocValues str1DV = str1.getValues(context, readerContext);
-    final DocValues str2DV = str2.getValues(context, readerContext);
+  public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
+    final FunctionValues str1DV = str1.getValues(context, readerContext);
+    final FunctionValues str2DV = str2.getValues(context, readerContext);
     return new FloatDocValues(this) {
 
       @Override


diff -ruN -x .svn -x build lucene-clean-trunk/solr/core/src/java/org/apache/solr/search/function/distance/VectorDistanceFunction.java lucene-3622/solr/core/src/java/org/apache/solr/search/function/distance/VectorDistanceFunction.java
--- lucene-clean-trunk/solr/core/src/java/org/apache/solr/search/function/distance/VectorDistanceFunction.java	2011-12-06 18:44:12.788810090 -0500
+++ lucene-3622/solr/core/src/java/org/apache/solr/search/function/distance/VectorDistanceFunction.java	2011-12-12 15:20:30.693624900 -0500
@@ -17,7 +17,7 @@
  */
 
 import org.apache.lucene.index.IndexReader.AtomicReaderContext;
-import org.apache.lucene.queries.function.DocValues;
+import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.queries.function.docvalues.DoubleDocValues;
 import org.apache.lucene.queries.function.valuesource.MultiValueSource;
@@ -69,7 +69,7 @@
    * @param dv2 The values from the second MultiValueSource
    * @return The distance
    */
-  protected double distance(int doc, DocValues dv1, DocValues dv2) {
+  protected double distance(int doc, FunctionValues dv1, FunctionValues dv2) {
     //Handle some special cases:
     double[] vals1 = new double[source1.dimension()];
     double[] vals2 = new double[source1.dimension()];
@@ -79,11 +79,11 @@
   }
 
   @Override
-  public DocValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
+  public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
 
-    final DocValues vals1 = source1.getValues(context, readerContext);
+    final FunctionValues vals1 = source1.getValues(context, readerContext);
 
-    final DocValues vals2 = source2.getValues(context, readerContext);
+    final FunctionValues vals2 = source2.getValues(context, readerContext);
 
 
     return new DoubleDocValues(this) {


diff -ruN -x .svn -x build lucene-clean-trunk/solr/core/src/java/org/apache/solr/search/function/FileFloatSource.java lucene-3622/solr/core/src/java/org/apache/solr/search/function/FileFloatSource.java
--- lucene-clean-trunk/solr/core/src/java/org/apache/solr/search/function/FileFloatSource.java	2011-12-06 18:44:12.788810090 -0500
+++ lucene-3622/solr/core/src/java/org/apache/solr/search/function/FileFloatSource.java	2011-12-12 15:33:31.933638505 -0500
@@ -33,7 +33,7 @@
 import org.apache.lucene.index.TermsEnum;
 import org.apache.lucene.index.IndexReader.AtomicReaderContext;
 import org.apache.lucene.index.IndexReader.ReaderContext;
-import org.apache.lucene.queries.function.DocValues;
+import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.queries.function.docvalues.FloatDocValues;
 import org.apache.lucene.util.BytesRef;
@@ -76,7 +76,7 @@
   }
 
   @Override
-  public DocValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
+  public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
     final int off = readerContext.docBase;
     ReaderContext topLevelContext = ReaderUtil.getTopLevelContext(readerContext);
 
@@ -327,17 +327,17 @@
 
     @Override
     public String getSource() {
-      return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/search/function/FileFloatSource.java $";
+      return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene3622/solr/core/src/java/org/apache/solr/search/function/FileFloatSource.java $";
     }
 
     @Override
     public String getSourceId() {
-      return "$Id: FileFloatSource.java 1210176 2011-12-04 18:50:58Z mikemccand $";
+      return "$Id: FileFloatSource.java 1213426 2011-12-12 20:33:26Z rmuir $";
     }
 
     @Override
     public String getVersion() {
-      return "$Revision: 1210176 $";
+      return "$Revision: 1213426 $";
     }    
   }
 }


diff -ruN -x .svn -x build lucene-clean-trunk/solr/core/src/java/org/apache/solr/search/FunctionRangeQParserPlugin.java lucene-3622/solr/core/src/java/org/apache/solr/search/FunctionRangeQParserPlugin.java
--- lucene-clean-trunk/solr/core/src/java/org/apache/solr/search/FunctionRangeQParserPlugin.java	2011-12-06 18:44:12.824810091 -0500
+++ lucene-3622/solr/core/src/java/org/apache/solr/search/FunctionRangeQParserPlugin.java	2011-12-12 15:20:30.593624898 -0500
@@ -17,7 +17,7 @@
 package org.apache.solr.search;
 
 import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.queries.function.DocValues;
+import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.FunctionQuery;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.queries.function.ValueSourceScorer;
@@ -114,7 +114,7 @@
     @Override
     public void setNextReader(IndexReader.AtomicReaderContext context) throws IOException {
       maxdoc = context.reader.maxDoc();
-      DocValues dv = rangeFilt.getValueSource().getValues(fcontext, context);
+      FunctionValues dv = rangeFilt.getValueSource().getValues(fcontext, context);
       scorer = dv.getRangeScorer(context.reader, rangeFilt.getLowerVal(), rangeFilt.getUpperVal(), rangeFilt.isIncludeLower(), rangeFilt.isIncludeUpper());
       super.setNextReader(context);
     }


diff -ruN -x .svn -x build lucene-clean-trunk/solr/core/src/java/org/apache/solr/search/SolrIndexSearcher.java lucene-3622/solr/core/src/java/org/apache/solr/search/SolrIndexSearcher.java
--- lucene-clean-trunk/solr/core/src/java/org/apache/solr/search/SolrIndexSearcher.java	2011-12-11 19:21:45.588372364 -0500
+++ lucene-3622/solr/core/src/java/org/apache/solr/search/SolrIndexSearcher.java	2011-12-12 15:13:08.165617194 -0500
@@ -1921,11 +1921,11 @@
   }
 
   public String getSourceId() {
-    return "$Id: SolrIndexSearcher.java 1213117 2011-12-12 00:21:40Z rmuir $";
+    return "$Id: SolrIndexSearcher.java 1213405 2011-12-12 20:13:02Z rmuir $";
   }
 
   public String getSource() {
-    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/trunk/solr/core/src/java/org/apache/solr/search/SolrIndexSearcher.java $";
+    return "$URL: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene3622/solr/core/src/java/org/apache/solr/search/SolrIndexSearcher.java $";
   }
 
   public URL[] getDocs() {


diff -ruN -x .svn -x build lucene-clean-trunk/solr/core/src/java/org/apache/solr/search/ValueSourceParser.java lucene-3622/solr/core/src/java/org/apache/solr/search/ValueSourceParser.java
--- lucene-clean-trunk/solr/core/src/java/org/apache/solr/search/ValueSourceParser.java	2011-12-06 18:44:12.824810091 -0500
+++ lucene-3622/solr/core/src/java/org/apache/solr/search/ValueSourceParser.java	2011-12-12 15:20:30.689624900 -0500
@@ -19,7 +19,7 @@
 import org.apache.lucene.index.IndexReader.AtomicReaderContext;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.queries.function.BoostedQuery;
-import org.apache.lucene.queries.function.DocValues;
+import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.queries.function.docvalues.BoolDocValues;
 import org.apache.lucene.queries.function.docvalues.DoubleDocValues;
@@ -173,7 +173,7 @@
           }
 
           @Override
-          protected float func(int doc, DocValues vals) {
+          protected float func(int doc, FunctionValues vals) {
             return Math.abs(vals.floatVal(doc));
           }
         };
@@ -209,7 +209,7 @@
           }
 
           @Override
-          protected float func(int doc, DocValues aVals, DocValues bVals) {
+          protected float func(int doc, FunctionValues aVals, FunctionValues bVals) {
             return aVals.floatVal(doc) - bVals.floatVal(doc);
           }
         };
@@ -353,133 +353,133 @@
 
     addParser(new DoubleParser("rad") {
       @Override
-      public double func(int doc, DocValues vals) {
+      public double func(int doc, FunctionValues vals) {
         return vals.doubleVal(doc) * DistanceUtils.DEGREES_TO_RADIANS;
       }
     });
     addParser(new DoubleParser("deg") {
       @Override
-      public double func(int doc, DocValues vals) {
+      public double func(int doc, FunctionValues vals) {
         return vals.doubleVal(doc) * DistanceUtils.RADIANS_TO_DEGREES;
       }
     });
     addParser(new DoubleParser("sqrt") {
       @Override
-      public double func(int doc, DocValues vals) {
+      public double func(int doc, FunctionValues vals) {
         return Math.sqrt(vals.doubleVal(doc));
       }
     });
     addParser(new DoubleParser("cbrt") {
       @Override
-      public double func(int doc, DocValues vals) {
+      public double func(int doc, FunctionValues vals) {
         return Math.cbrt(vals.doubleVal(doc));
       }
     });
     addParser(new DoubleParser("log") {
       @Override
-      public double func(int doc, DocValues vals) {
+      public double func(int doc, FunctionValues vals) {
         return Math.log10(vals.doubleVal(doc));
       }
     });
     addParser(new DoubleParser("ln") {
       @Override
-      public double func(int doc, DocValues vals) {
+      public double func(int doc, FunctionValues vals) {
         return Math.log(vals.doubleVal(doc));
       }
     });
     addParser(new DoubleParser("exp") {
       @Override
-      public double func(int doc, DocValues vals) {
+      public double func(int doc, FunctionValues vals) {
         return Math.exp(vals.doubleVal(doc));
       }
     });
     addParser(new DoubleParser("sin") {
       @Override
-      public double func(int doc, DocValues vals) {
+      public double func(int doc, FunctionValues vals) {
         return Math.sin(vals.doubleVal(doc));
       }
     });
     addParser(new DoubleParser("cos") {
       @Override
-      public double func(int doc, DocValues vals) {
+      public double func(int doc, FunctionValues vals) {
         return Math.cos(vals.doubleVal(doc));
       }
     });
     addParser(new DoubleParser("tan") {
       @Override
-      public double func(int doc, DocValues vals) {
+      public double func(int doc, FunctionValues vals) {
         return Math.tan(vals.doubleVal(doc));
       }
     });
     addParser(new DoubleParser("asin") {
       @Override
-      public double func(int doc, DocValues vals) {
+      public double func(int doc, FunctionValues vals) {
         return Math.asin(vals.doubleVal(doc));
       }
     });
     addParser(new DoubleParser("acos") {
       @Override
-      public double func(int doc, DocValues vals) {
+      public double func(int doc, FunctionValues vals) {
         return Math.acos(vals.doubleVal(doc));
       }
     });
     addParser(new DoubleParser("atan") {
       @Override
-      public double func(int doc, DocValues vals) {
+      public double func(int doc, FunctionValues vals) {
         return Math.atan(vals.doubleVal(doc));
       }
     });
     addParser(new DoubleParser("sinh") {
       @Override
-      public double func(int doc, DocValues vals) {
+      public double func(int doc, FunctionValues vals) {
         return Math.sinh(vals.doubleVal(doc));
       }
     });
     addParser(new DoubleParser("cosh") {
       @Override
-      public double func(int doc, DocValues vals) {
+      public double func(int doc, FunctionValues vals) {
         return Math.cosh(vals.doubleVal(doc));
       }
     });
     addParser(new DoubleParser("tanh") {
       @Override
-      public double func(int doc, DocValues vals) {
+      public double func(int doc, FunctionValues vals) {
         return Math.tanh(vals.doubleVal(doc));
       }
     });
     addParser(new DoubleParser("ceil") {
       @Override
-      public double func(int doc, DocValues vals) {
+      public double func(int doc, FunctionValues vals) {
         return Math.ceil(vals.doubleVal(doc));
       }
     });
     addParser(new DoubleParser("floor") {
       @Override
-      public double func(int doc, DocValues vals) {
+      public double func(int doc, FunctionValues vals) {
         return Math.floor(vals.doubleVal(doc));
       }
     });
     addParser(new DoubleParser("rint") {
       @Override
-      public double func(int doc, DocValues vals) {
+      public double func(int doc, FunctionValues vals) {
         return Math.rint(vals.doubleVal(doc));
       }
     });
     addParser(new Double2Parser("pow") {
       @Override
-      public double func(int doc, DocValues a, DocValues b) {
+      public double func(int doc, FunctionValues a, FunctionValues b) {
         return Math.pow(a.doubleVal(doc), b.doubleVal(doc));
       }
     });
     addParser(new Double2Parser("hypot") {
       @Override
-      public double func(int doc, DocValues a, DocValues b) {
+      public double func(int doc, FunctionValues a, FunctionValues b) {
         return Math.hypot(a.doubleVal(doc), b.doubleVal(doc));
       }
     });
     addParser(new Double2Parser("atan2") {
       @Override
-      public double func(int doc, DocValues a, DocValues b) {
+      public double func(int doc, FunctionValues a, FunctionValues b) {
         return Math.atan2(a.doubleVal(doc), b.doubleVal(doc));
       }
     });
@@ -629,7 +629,7 @@
             return "exists";
           }
           @Override
-          protected boolean func(int doc, DocValues vals) {
+          protected boolean func(int doc, FunctionValues vals) {
             return vals.exists(doc);
           }
         };
@@ -642,7 +642,7 @@
         ValueSource vs = fp.parseValueSource();
         return new SimpleBoolFunction(vs) {
           @Override
-          protected boolean func(int doc, DocValues vals) {
+          protected boolean func(int doc, FunctionValues vals) {
             return !vals.boolVal(doc);
           }
           @Override
@@ -664,8 +664,8 @@
             return "and";
           }
           @Override
-          protected boolean func(int doc, DocValues[] vals) {
-            for (DocValues dv : vals)
+          protected boolean func(int doc, FunctionValues[] vals) {
+            for (FunctionValues dv : vals)
               if (!dv.boolVal(doc)) return false;
             return true;
           }
@@ -683,8 +683,8 @@
             return "or";
           }
           @Override
-          protected boolean func(int doc, DocValues[] vals) {
-            for (DocValues dv : vals)
+          protected boolean func(int doc, FunctionValues[] vals) {
+            for (FunctionValues dv : vals)
               if (dv.boolVal(doc)) return true;
             return false;
           }
@@ -702,9 +702,9 @@
             return "xor";
           }
           @Override
-          protected boolean func(int doc, DocValues[] vals) {
+          protected boolean func(int doc, FunctionValues[] vals) {
             int nTrue=0, nFalse=0;
-            for (DocValues dv : vals) {
+            for (FunctionValues dv : vals) {
               if (dv.boolVal(doc)) nTrue++;
               else nFalse++;
             }
@@ -888,7 +888,7 @@
         }
 
         @Override
-        protected float func(int doc, DocValues aVals, DocValues bVals) {
+        protected float func(int doc, FunctionValues aVals, FunctionValues bVals) {
           return ms1 - bVals.longVal(doc);
         }
       };
@@ -902,7 +902,7 @@
         }
 
         @Override
-        protected float func(int doc, DocValues aVals, DocValues bVals) {
+        protected float func(int doc, FunctionValues aVals, FunctionValues bVals) {
           return aVals.longVal(doc) - ms2;
         }
       };
@@ -916,7 +916,7 @@
         }
 
         @Override
-        protected float func(int doc, DocValues aVals, DocValues bVals) {
+        protected float func(int doc, FunctionValues aVals, FunctionValues bVals) {
           return aVals.longVal(doc) - bVals.longVal(doc);
         }
       };
@@ -945,7 +945,7 @@
   }
 
   @Override
-  public DocValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
+  public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
     return new LongDocValues(this) {
       @Override
       public float floatVal(int doc) {
@@ -1034,7 +1034,7 @@
     super(name);
   }
 
-  public abstract double func(int doc, DocValues vals);
+  public abstract double func(int doc, FunctionValues vals);
 
   @Override
   public ValueSource parse(FunctionQParser fp) throws ParseException {
@@ -1052,8 +1052,8 @@
     }
 
     @Override
-    public DocValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
-      final DocValues vals =  source.getValues(context, readerContext);
+    public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
+      final FunctionValues vals =  source.getValues(context, readerContext);
       return new DoubleDocValues(this) {
         @Override
         public double doubleVal(int doc) {
@@ -1074,7 +1074,7 @@
     super(name);
   }
 
-  public abstract double func(int doc, DocValues a, DocValues b);
+  public abstract double func(int doc, FunctionValues a, FunctionValues b);
 
   @Override
   public ValueSource parse(FunctionQParser fp) throws ParseException {
@@ -1100,9 +1100,9 @@
     }
 
     @Override
-    public DocValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
-      final DocValues aVals =  a.getValues(context, readerContext);
-      final DocValues bVals =  b.getValues(context, readerContext);
+    public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
+      final FunctionValues aVals =  a.getValues(context, readerContext);
+      final FunctionValues bVals =  b.getValues(context, readerContext);
       return new DoubleDocValues(this) {
          @Override
         public double doubleVal(int doc) {
@@ -1154,7 +1154,7 @@
   }
 
   @Override
-  public DocValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
+  public FunctionValues getValues(Map context, AtomicReaderContext readerContext) throws IOException {
     return new BoolDocValues(this) {
       @Override
       public boolean boolVal(int doc) {


diff -ruN -x .svn -x build lucene-clean-trunk/solr/core/src/test/org/apache/solr/core/DummyValueSourceParser.java lucene-3622/solr/core/src/test/org/apache/solr/core/DummyValueSourceParser.java
--- lucene-clean-trunk/solr/core/src/test/org/apache/solr/core/DummyValueSourceParser.java	2011-12-06 18:44:12.256810081 -0500
+++ lucene-3622/solr/core/src/test/org/apache/solr/core/DummyValueSourceParser.java	2011-12-12 15:20:30.573624898 -0500
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-import org.apache.lucene.queries.function.DocValues;
+import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.queries.function.valuesource.SimpleFloatFunction;
 import org.apache.lucene.queryparser.classic.ParseException;
@@ -47,7 +47,7 @@
       }
 
       @Override
-      protected float func(int doc, DocValues vals) {
+      protected float func(int doc, FunctionValues vals) {
         float result = 0;
         return result;
       }


diff -ruN -x .svn -x build lucene-clean-trunk/solr/core/src/test/org/apache/solr/search/function/NvlValueSourceParser.java lucene-3622/solr/core/src/test/org/apache/solr/search/function/NvlValueSourceParser.java
--- lucene-clean-trunk/solr/core/src/test/org/apache/solr/search/function/NvlValueSourceParser.java	2011-12-06 18:44:12.288810081 -0500
+++ lucene-3622/solr/core/src/test/org/apache/solr/search/function/NvlValueSourceParser.java	2011-12-12 15:20:30.641624899 -0500
@@ -17,7 +17,7 @@
 
 package org.apache.solr.search.function;
 
-import org.apache.lucene.queries.function.DocValues;
+import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.queries.function.valuesource.SimpleFloatFunction;
 import org.apache.lucene.queryparser.classic.ParseException;
@@ -57,7 +57,7 @@
 	    }
 
 	    @Override
-      protected float func(int doc, DocValues vals) {
+      protected float func(int doc, FunctionValues vals) {
 		float v = vals.floatVal(doc);
 		if (v == nvlFloatValue) {
 		    return nvl;


diff -ruN -x .svn -x build lucene-clean-trunk/solr/core/src/test/org/apache/solr/search/TestIndexSearcher.java lucene-3622/solr/core/src/test/org/apache/solr/search/TestIndexSearcher.java
--- lucene-clean-trunk/solr/core/src/test/org/apache/solr/search/TestIndexSearcher.java	2011-12-06 18:44:12.324810082 -0500
+++ lucene-3622/solr/core/src/test/org/apache/solr/search/TestIndexSearcher.java	2011-12-12 15:26:04.157630707 -0500
@@ -18,7 +18,7 @@
 
 import org.apache.lucene.index.IndexReader.AtomicReaderContext;
 import org.apache.lucene.index.IndexReader.ReaderContext;
-import org.apache.lucene.queries.function.DocValues;
+import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 import org.apache.lucene.util.ReaderUtil;
 import org.apache.solr.SolrTestCaseJ4;
@@ -53,7 +53,7 @@
     AtomicReaderContext[] leaves = ReaderUtil.leaves(topReaderContext);
     int idx = ReaderUtil.subIndex(doc, leaves);
     AtomicReaderContext leaf = leaves[idx];
-    DocValues vals = vs.getValues(context, leaf);
+    FunctionValues vals = vs.getValues(context, leaf);
     return vals.strVal(doc-leaf.docBase);
   }
 
