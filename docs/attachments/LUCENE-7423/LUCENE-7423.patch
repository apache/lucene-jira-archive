diff --git a/lucene/sandbox/src/java/org/apache/lucene/codecs/autoprefix/AutoPrefixFieldsConsumer.java b/lucene/sandbox/src/java/org/apache/lucene/codecs/autoprefix/AutoPrefixFieldsConsumer.java
new file mode 100644
index 0000000..06a7f60
--- /dev/null
+++ b/lucene/sandbox/src/java/org/apache/lucene/codecs/autoprefix/AutoPrefixFieldsConsumer.java
@@ -0,0 +1,103 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.lucene.codecs.autoprefix;
+
+import java.io.Closeable;
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Iterator;
+import java.util.List;
+
+import org.apache.lucene.codecs.FieldsConsumer;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.Fields;
+import org.apache.lucene.index.SegmentWriteState;
+import org.apache.lucene.index.Terms;
+import org.apache.lucene.util.IOUtils;
+
+/**
+ * A fields consumer that detects prefixes that match "enough" terms and write auto-prefix terms into their own virtual field.
+ */
+public class AutoPrefixFieldsConsumer extends FieldsConsumer {
+  private final int minTermsInAutoPrefix;
+  private final int maxAutoPrefixSize;
+  private final FieldsConsumer delegate;
+  private final SegmentWriteState state;
+
+  public AutoPrefixFieldsConsumer(SegmentWriteState state, FieldsConsumer delegate, int minTermsInAutoPrefix, int maxAutoPrefixSize) {
+    this.state = state;
+    this.delegate = delegate;
+    this.minTermsInAutoPrefix = minTermsInAutoPrefix;
+    this.maxAutoPrefixSize = maxAutoPrefixSize;
+  }
+
+  @Override
+  public void write(Fields fields) throws IOException {
+    List<String> fieldNames = new ArrayList<>();
+    for (FieldInfo fieldInfo : state.fieldInfos) {
+      fieldNames.add(fieldInfo.name);
+    }
+    CloseableFields allFields = new CloseableFields() {
+      AutoPrefixTermsBuilder writer;
+
+      @Override
+      public Iterator<String> iterator() {
+        return fields.iterator();
+      }
+
+      @Override
+      public Terms terms(String field) throws IOException {
+        if (writer != null) {
+          writer.close();
+          writer = null;
+        }
+        if (field.endsWith("-autoprefix")) {
+          final Terms terms = fields.terms(field.substring(0, field.length()-11));
+          writer = new AutoPrefixTermsBuilder(state, terms, minTermsInAutoPrefix, maxAutoPrefixSize);
+          return writer.build();
+        } else {
+          return fields.terms(field);
+        }
+      }
+
+      @Override
+      public int size() {
+        return fields.size();
+      }
+
+      @Override
+      public void close() throws IOException {
+        if (writer != null) {
+          writer.close();
+        }
+      }
+    };
+    try {
+      delegate.write(allFields);
+    } finally {
+      allFields.close();
+    }
+  }
+
+  @Override
+  public void close() throws IOException {
+    IOUtils.close(delegate);
+  }
+
+  static abstract class CloseableFields extends Fields implements Closeable {}
+}
diff --git a/lucene/sandbox/src/java/org/apache/lucene/codecs/autoprefix/AutoPrefixNode.java b/lucene/sandbox/src/java/org/apache/lucene/codecs/autoprefix/AutoPrefixNode.java
new file mode 100644
index 0000000..e34afc4
--- /dev/null
+++ b/lucene/sandbox/src/java/org/apache/lucene/codecs/autoprefix/AutoPrefixNode.java
@@ -0,0 +1,39 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.lucene.codecs.autoprefix;
+
+import java.util.LinkedList;
+import java.util.List;
+
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.DocIdSetBuilder;
+
+class AutoPrefixNode {
+  BytesRef prefix;
+  AutoPrefixNode parent;
+  List<AutoPrefixNode> children = new LinkedList<>();
+  int numTerms;
+  DocIdSetBuilder docIdSetB;
+
+  AutoPrefixNode(AutoPrefixNode parent, BytesRef prefix, int numTerms, DocIdSetBuilder docIdSetB) {
+    this.parent = parent;
+    this.prefix = prefix;
+    this.numTerms = numTerms;
+    this.docIdSetB = docIdSetB;
+  }
+}
diff --git a/lucene/sandbox/src/java/org/apache/lucene/codecs/autoprefix/AutoPrefixPostingsFormat.java b/lucene/sandbox/src/java/org/apache/lucene/codecs/autoprefix/AutoPrefixPostingsFormat.java
new file mode 100644
index 0000000..4ee7e38
--- /dev/null
+++ b/lucene/sandbox/src/java/org/apache/lucene/codecs/autoprefix/AutoPrefixPostingsFormat.java
@@ -0,0 +1,87 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.lucene.codecs.autoprefix;
+
+import java.io.IOException;
+
+import org.apache.lucene.codecs.FieldsConsumer;
+import org.apache.lucene.codecs.FieldsProducer;
+import org.apache.lucene.codecs.PostingsFormat;
+import org.apache.lucene.codecs.PostingsReaderBase;
+import org.apache.lucene.codecs.PostingsWriterBase;
+import org.apache.lucene.codecs.blocktree.BlockTreeTermsReader;
+import org.apache.lucene.codecs.blocktree.BlockTreeTermsWriter;
+import org.apache.lucene.codecs.lucene50.Lucene50PostingsReader;
+import org.apache.lucene.codecs.lucene50.Lucene50PostingsWriter;
+import org.apache.lucene.index.SegmentReadState;
+import org.apache.lucene.index.SegmentWriteState;
+import org.apache.lucene.util.IOUtils;
+
+/**
+ * A PostingsFormat optimized for prefix queries on text fields.
+ * It detects prefixes that match "enough" terms and writes auto-prefix terms into their own virtual field.
+ * At search time the virtual field is used to speed up prefix queries that match "enough" terms.
+ * See {@link AutoPrefixQuery}.
+ */
+public class AutoPrefixPostingsFormat extends PostingsFormat {
+  private final int minTermsInAutoPrefix;
+  private final int maxAutoPrefixSize;
+
+  public AutoPrefixPostingsFormat() {
+    this(2, Integer.MAX_VALUE);
+  }
+
+  public AutoPrefixPostingsFormat(int minTermsInAutoPrefix, int maxAutoPrefixSize) {
+    super("AutoPrefix");
+    this.minTermsInAutoPrefix = minTermsInAutoPrefix;
+    this.maxAutoPrefixSize = maxAutoPrefixSize;
+  }
+
+  @Override
+  public FieldsConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
+    PostingsWriterBase postingsWriter = new Lucene50PostingsWriter(state);
+    boolean success = false;
+    try {
+      FieldsConsumer termsWriter = new BlockTreeTermsWriter(state,
+          postingsWriter,
+          BlockTreeTermsWriter.DEFAULT_MIN_BLOCK_SIZE,
+          BlockTreeTermsWriter.DEFAULT_MAX_BLOCK_SIZE);
+      success = true;
+      return new AutoPrefixFieldsConsumer(state, termsWriter, minTermsInAutoPrefix, maxAutoPrefixSize);
+    } finally {
+      if (!success) {
+        IOUtils.closeWhileHandlingException(postingsWriter);
+      }
+    }
+  }
+
+  @Override
+  public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {
+    PostingsReaderBase postingsReader = new Lucene50PostingsReader(state);
+    boolean success = false;
+    try {
+      FieldsProducer ret = new BlockTreeTermsReader(postingsReader, state);
+      success = true;
+      return ret;
+    } finally {
+      if (!success) {
+        IOUtils.closeWhileHandlingException(postingsReader);
+      }
+    }
+  }
+}
\ No newline at end of file
diff --git a/lucene/sandbox/src/java/org/apache/lucene/codecs/autoprefix/AutoPrefixQuery.java b/lucene/sandbox/src/java/org/apache/lucene/codecs/autoprefix/AutoPrefixQuery.java
new file mode 100644
index 0000000..f97fabe
--- /dev/null
+++ b/lucene/sandbox/src/java/org/apache/lucene/codecs/autoprefix/AutoPrefixQuery.java
@@ -0,0 +1,114 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.lucene.codecs.autoprefix;
+
+import java.io.IOException;
+
+import org.apache.lucene.index.LeafReaderContext;
+import org.apache.lucene.index.Term;
+import org.apache.lucene.index.Terms;
+import org.apache.lucene.index.TermsEnum;
+import org.apache.lucene.search.ConstantScoreScorer;
+import org.apache.lucene.search.ConstantScoreWeight;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.PrefixQuery;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.Scorer;
+import org.apache.lucene.search.Weight;
+import org.apache.lucene.util.StringHelper;
+
+/**
+ * A Query that matches documents containing terms with a specified prefix.
+ * Works only with auto-prefix fields built by {@link AutoPrefixFieldsConsumer}.
+ */
+public class AutoPrefixQuery extends Query {
+  private final Term prefix;
+
+  public AutoPrefixQuery(Term prefix) {
+    this.prefix = prefix;
+  }
+
+  @Override
+  public Weight createWeight(IndexSearcher searcher, boolean needsScores, float boost) throws IOException {
+    return new AutoPrefixWeight(this, searcher);
+  }
+
+  @Override
+  public String toString(String field) {
+    return null;
+  }
+
+  @Override
+  public boolean equals(Object o) {
+    if (this == o) return true;
+    if (o == null || getClass() != o.getClass()) return false;
+    AutoPrefixQuery that = (AutoPrefixQuery) o;
+    return prefix.equals(that.prefix);
+  }
+
+  @Override
+  public int hashCode() {
+    return prefix.hashCode();
+  }
+
+  final class AutoPrefixWeight extends ConstantScoreWeight {
+    final IndexSearcher searcher;
+    protected AutoPrefixWeight(AutoPrefixQuery query, IndexSearcher searcher) {
+      super(query, 1.0f);
+      this.searcher = searcher;
+    }
+
+    protected Scorer getScorer(LeafReaderContext context, Terms terms, Terms prefixTerms) throws IOException {
+      if (prefixTerms != null) {
+        TermsEnum termsEnum = terms.iterator();
+        TermsEnum prefixEnum = prefixTerms.iterator();
+        TermsEnum.SeekStatus status = prefixEnum.seekCeil(prefix.bytes());
+        if (status == TermsEnum.SeekStatus.FOUND) {
+          // this is an exact match so we need only one inverted list.
+          return new ConstantScoreScorer(this, 1.0f, prefixEnum.postings(null, 0));
+        }
+        TermsEnum.SeekStatus termStatus = termsEnum.seekCeil(prefix.bytes());
+        if (status != TermsEnum.SeekStatus.END && StringHelper.startsWith(prefixEnum.term(), prefix.bytes())) {
+          if (termStatus != TermsEnum.SeekStatus.END) {
+            if (StringHelper.startsWith(termsEnum.term(), prefixEnum.term())) {
+              // the auto-prefix is bigger than the requested prefix but it's also the first match so we can use this
+              // inverted list to answer the query.
+              return new ConstantScoreScorer(this, 1.0f, prefixEnum.postings(null, 0));
+            }
+          }
+        }
+        if (termStatus == TermsEnum.SeekStatus.END) {
+          // no match
+          return null;
+        }
+      }
+      // we don't have any auto-prefixes that can answer the query so we fallback into a PrefixQuery.
+      Query query = new PrefixQuery(prefix);
+      query = query.rewrite(searcher.getIndexReader());
+      final Weight weight = searcher.rewrite(query).createWeight(searcher, false, score());
+      return weight.scorer(context);
+    }
+
+    @Override
+    public Scorer scorer(LeafReaderContext context) throws IOException {
+      Terms terms = context.reader().terms(prefix.field());
+      Terms prefixTerms = context.reader().terms(new String(prefix.field() + "-autoprefix"));
+      return getScorer(context, terms, prefixTerms);
+    }
+  }
+}
diff --git a/lucene/sandbox/src/java/org/apache/lucene/codecs/autoprefix/AutoPrefixTermsBuilder.java b/lucene/sandbox/src/java/org/apache/lucene/codecs/autoprefix/AutoPrefixTermsBuilder.java
new file mode 100644
index 0000000..a360208
--- /dev/null
+++ b/lucene/sandbox/src/java/org/apache/lucene/codecs/autoprefix/AutoPrefixTermsBuilder.java
@@ -0,0 +1,497 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.lucene.codecs.autoprefix;
+
+import java.io.Closeable;
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Collections;
+import java.util.Iterator;
+import java.util.List;
+
+import org.apache.lucene.index.FilterLeafReader;
+import org.apache.lucene.index.PostingsEnum;
+import org.apache.lucene.index.SegmentWriteState;
+import org.apache.lucene.index.Terms;
+import org.apache.lucene.index.TermsEnum;
+import org.apache.lucene.search.DocIdSet;
+import org.apache.lucene.search.DocIdSetIterator;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.DocIdSetBuilder;
+import org.apache.lucene.util.FixedBitSet;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util.packed.MonotonicBlockPackedReader;
+import org.apache.lucene.util.packed.MonotonicBlockPackedWriter;
+import org.apache.lucene.util.packed.PackedInts;
+
+/**
+ * Used to build auto-prefix terms and their associated inverted lists from a {@link TermsEnum}.
+ * This is done in two pass, the first pass builds a compact prefix tree.
+ * Since the terms enum is sorted the prefixes are flushed on the fly depending on the input.
+ * For each prefix we build its corresponding inverted lists using a {@link DocIdSetBuilder}.
+ * The first pass visits each term of the specified {@link TermsEnum} only once.
+ * When a prefix is flushed from the prefix tree its inverted lists is dumped into a temporary file for further use.
+ * This is necessary since the prefixes are not sorted when they are removed from the tree.
+ * The selected auto prefixes are sorted at the end of the first pass.
+ * The second pass is a sorted scan of the prefixes and the temporary file is used to read the corresponding inverted lists.
+ **/
+class AutoPrefixTermsBuilder implements Closeable {
+  static final BytesRef EMPTY_BYTES = new BytesRef(0);
+
+  private final Directory directory;
+  private final Terms terms;
+  private final int minTermsInAutoPrefix;
+  private final long maxAutoTermSize;
+  private final int maxDoc;
+
+  private List<PrefixTerm> prefixes = new ArrayList<>();
+  // The root of the compact prefix tree
+  private AutoPrefixNode root;
+  private final FixedBitSet docsSeen;
+  private PostingsEnum reusePE;
+
+  private final String tempFileNamePrefix;
+  private IndexInput input;
+  // keep track of the temporary file name for deletion
+  private String name;
+
+  public AutoPrefixTermsBuilder(SegmentWriteState state, Terms terms, int minTermsInAutoPrefix, int maxAutoTermSize) {
+    this.directory = state.directory;
+    this.terms = terms;
+    this.maxDoc = state.segmentInfo.maxDoc();
+    this.docsSeen = new FixedBitSet(maxDoc);
+    this.tempFileNamePrefix = state.segmentInfo.name;
+    this.minTermsInAutoPrefix = minTermsInAutoPrefix;
+    this.maxAutoTermSize = maxAutoTermSize;
+  }
+
+  public Terms build() throws IOException {
+    boolean success = false;
+    // First pass
+    // we push the terms into the compact prefix tree and flush the prefixes on the fly.
+    try (IndexOutput output = directory.createTempOutput(tempFileNamePrefix, "autoprefix", IOContext.DEFAULT)) {
+      name = output.getName();
+      TermsEnum termsEnum = terms.iterator();
+      while (termsEnum.next() != null) {
+        reusePE = termsEnum.postings(reusePE, 0);
+        DocIdSetBuilder docIdSetB = newDocIdSetBuilder();
+        while (reusePE.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {
+          docIdSetB.grow(1).add(reusePE.docID());
+        }
+        BytesRef term = termsEnum.term();
+        int max = (int) Math.min(term.length, maxAutoTermSize+1);
+        term.length = max;
+        pushTerm(term, docIdSetB.build(), output);
+      }
+      if (root != null) {
+        flushPrefix(EMPTY_BYTES, root, output);
+      }
+      success = true;
+    } finally {
+      if (success == false && name != null) {
+        IOUtils.deleteFiles(directory, name);
+      }
+    }
+    // End of the first pass
+    // Sort the selected prefixes
+    Collections.sort(prefixes);
+    // Now we can read the sorted prefixes and their inverted lists
+    input = directory.openInput(name, IOContext.READONCE);
+    // The second pass is the enumeration of the sorted prefixes
+    return new TermsReader();
+  }
+
+  @Override
+  public void close() throws IOException {
+    // close the temporary file and delete it if needed.
+    if (input != null) {
+      input.close();
+      input = null;
+    }
+    if (name != null) {
+      IOUtils.deleteFilesIgnoringExceptions(directory, name);
+      name = null;
+    }
+  }
+
+  /**
+   * Adds the specified term in the compact prefix tree and flushes the prefixes smaller than the term.
+   * @warning The terms must be added in sorted order.
+   */
+  private void pushTerm(BytesRef term, DocIdSet docIdSet, IndexOutput output) throws IOException {
+    if (root == null) {
+      this.root = new AutoPrefixNode(null, BytesRef.deepCopyOf(term), 1, or(docIdSet, newDocIdSetBuilder()));
+      return;
+    }
+    innerPushTerm(root, EMPTY_BYTES, term, docIdSet, output);
+  }
+
+  private void innerPushTerm(AutoPrefixNode node, BytesRef prefix, BytesRef suffix,
+                             DocIdSet docIdSet, IndexOutput output) throws IOException {
+    int prefixLen = findLargestPrefix(suffix, node.prefix);
+    assert prefixLen != suffix.length : "Found duplicate entry";
+    if (prefixLen == 0) {
+      // Nothing in common, we can flush this node and replace it with the suffix
+      assert suffix.compareTo(node.prefix) > 0 : "Not sorted";
+      flushPrefix(prefix, node, output);
+      node.prefix = BytesRef.deepCopyOf(suffix);
+      node.children.clear();
+      node.numTerms = 1;
+      node.docIdSetB = or(docIdSet, newDocIdSetBuilder());
+      return;
+    }
+
+
+    if (prefixLen < suffix.length) {
+      node.numTerms += 1;
+      // The suffix candidate and the prefix located in this node share a prefix
+      BytesRef nextSuffix = new BytesRef(suffix.bytes, suffix.offset + prefixLen, suffix.length - prefixLen);
+      if (node.prefix.length == prefixLen) {
+        // The prefix located in this node is a prefix of the suffix candidate so we need to check if
+        // there's a child that can possibly share the remaining suffix.
+        boolean found = false;
+        BytesRef newPrefix = concatenate(prefix, node.prefix);
+        Iterator<AutoPrefixNode> it = node.children.iterator();
+        while (it.hasNext()) {
+          AutoPrefixNode child = it.next();
+          if (child.prefix.bytes[child.prefix.offset] == nextSuffix.bytes[nextSuffix.offset]) {
+            assert found == false : "Found twice";
+            innerPushTerm(child, newPrefix, nextSuffix, docIdSet, output);
+            found = true;
+          } else {
+            // does not match the remaining part, we can flush this part of the tree
+            assert nextSuffix.compareTo(child.prefix) > 0 : "Not sorted";
+            flushPrefix(newPrefix, child, output);
+            it.remove();
+          }
+        }
+        if (found) {
+          // found a child that matches the remaining suffix
+          return;
+        }
+        // No child exist with any prefix so we add a new one
+        assert node.children.isEmpty();
+        node.children.add(new AutoPrefixNode(node, BytesRef.deepCopyOf(nextSuffix), 1,
+            or(docIdSet, newDocIdSetBuilder())));
+        return;
+      }
+      // The candidate suffix and the prefix located in this node share a prefix so we need to split the node
+      assert suffix.compareTo(node.prefix) > 0 : "Not sorted";
+      BytesRef splitPrefix = new BytesRef(node.prefix.bytes, node.prefix.offset, prefixLen);
+      BytesRef splitSuffix = new BytesRef(suffix.bytes, suffix.offset + prefixLen, suffix.length - prefixLen);
+
+      // first we can flush the current node
+      flushPrefix(prefix, node, output);
+
+      // and then we split the node with the remaining part of the suffix candidate.
+      node.prefix = BytesRef.deepCopyOf(splitPrefix);
+      node.children.clear();
+      AutoPrefixNode newNode = new AutoPrefixNode(node, BytesRef.deepCopyOf(splitSuffix), 1,
+          or(docIdSet, newDocIdSetBuilder()));
+      node.children.add(newNode);
+    }
+  }
+
+
+  private void flushPrefix(BytesRef prefix, AutoPrefixNode node, IndexOutput output) throws IOException {
+    DocIdSet docIdSet = innerFlushPrefix(prefix, node, output);
+    // propagates the doc ids matching the flushed prefix to its parent
+    AutoPrefixNode parent = node.parent;
+    if (parent != null) {
+      if (parent.docIdSetB == null) {
+        parent.docIdSetB = newDocIdSetBuilder();
+      }
+      or(docIdSet, parent.docIdSetB);
+    }
+  }
+
+  private DocIdSet innerFlushPrefix(BytesRef prefix, AutoPrefixNode node, IndexOutput output) throws IOException {
+    if(node.docIdSetB == null) {
+      node.docIdSetB = newDocIdSetBuilder();
+    }
+    BytesRef newPrefix = concatenate(prefix, node.prefix);
+    for (AutoPrefixNode child : node.children) {
+      DocIdSet childDocIdSetB = innerFlushPrefix(newPrefix, child, output);
+      or(childDocIdSetB, node.docIdSetB);
+    }
+    DocIdSet docIdSet = node.docIdSetB.build();
+    node.docIdSetB = null;
+    if (node.numTerms >= minTermsInAutoPrefix) {
+      DocIdSetIterator it = docIdSet.iterator();
+      PrefixTerm prefixTerm = flushInvertedList(newPrefix, output, it);
+      prefixes.add(prefixTerm);
+    }
+    return docIdSet;
+  }
+
+  /**
+   * flushes the {@link DocIdSetIterator} in the temporary file and record the poiinter for further retrieval in the
+   * returned {@link PrefixTerm}.
+   */
+  private PrefixTerm flushInvertedList(BytesRef newPrefix,
+                                       IndexOutput output, DocIdSetIterator docIdSet) throws IOException {
+    long filePtr = output.getFilePointer();
+    int size = 0;
+    if (docIdSet.cost() >= Integer.MAX_VALUE) {
+      MonotonicBlockPackedWriter writer = new MonotonicBlockPackedWriter(output, 64);
+      while (docIdSet.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {
+        docsSeen.set(docIdSet.docID());
+        writer.add(docIdSet.docID());
+        size ++;
+      }
+      writer.finish();
+    } else {
+      int lastDoc = 0;
+      while (docIdSet.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {
+        docsSeen.set(docIdSet.docID());
+        int doc = docIdSet.docID() - lastDoc;
+        output.writeVInt(doc);
+        lastDoc = docIdSet.docID();
+        size ++;
+      }
+    }
+    return new PrefixTerm(BytesRef.deepCopyOf(newPrefix), size, filePtr);
+  }
+
+  private DocIdSetBuilder newDocIdSetBuilder() throws IOException {
+    /**
+     * NOCOMMIT Fake Terms#getDocCount and Terms#getSumDocFreq to make the {@link DocIdSetBuilder} aware that it needs to deduplicate the doc ids.
+     */
+    Terms wrappedTerms = new FilterLeafReader.FilterTerms(terms) {
+      @Override
+      public int getDocCount() throws IOException {
+        return -1;
+      }
+
+      @Override
+      public long getSumDocFreq() throws IOException {
+        return -1;
+      }
+    };
+    return new DocIdSetBuilder(maxDoc, wrappedTerms);
+  }
+
+  private static DocIdSetBuilder or(DocIdSet docIdSet, DocIdSetBuilder builder) throws IOException {
+    DocIdSetIterator it = docIdSet.iterator();
+    while (it.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {
+      builder.grow(1).add(it.docID());
+    }
+    return builder;
+  }
+
+  private static int findLargestPrefix(BytesRef a, BytesRef b) {
+    int size = Math.min(a.length, b.length);
+    int len = 0;
+    for (; len < size; ++len) {
+      if (a.bytes[len + a.offset] != b.bytes[len + b.offset]) {
+        break;
+      }
+    }
+    return len;
+  }
+
+  private static BytesRef concatenate(BytesRef prefix, BytesRef suffix) {
+    byte[] newPrefix = new byte[prefix.length + suffix.length];
+    System.arraycopy(prefix.bytes, prefix.offset, newPrefix, 0, prefix.length);
+    System.arraycopy(suffix.bytes, suffix.offset, newPrefix, 0 + prefix.length, suffix.length);
+    return new BytesRef(newPrefix);
+  }
+
+  private static class PrefixTerm implements Comparable<PrefixTerm> {
+    final BytesRef prefix;
+    final int docCount;
+    final long ptr;
+
+    PrefixTerm(BytesRef prefix, int docCount, long ptr) {
+      this.prefix = prefix;
+      this.docCount = docCount;
+      this.ptr = ptr;
+    }
+
+    @Override
+    public int compareTo(PrefixTerm other) {
+      return prefix.compareTo(other.prefix);
+    }
+  }
+
+  private class TermsReader extends Terms {
+    int docCount = docsSeen.cardinality();
+    @Override
+    public TermsEnum iterator() throws IOException {
+      final Iterator<PrefixTerm> it = prefixes.iterator();
+      return new TermsEnum() {
+        PrefixTerm current;
+
+        @Override
+        public SeekStatus seekCeil(BytesRef text) throws IOException {
+          throw new IllegalArgumentException("");
+        }
+
+        @Override
+        public void seekExact(long ord) throws IOException {
+          throw new IllegalArgumentException("");
+        }
+
+        @Override
+        public BytesRef term() throws IOException {
+          return current.prefix;
+        }
+
+        @Override
+        public long ord() throws IOException {
+          return -1;
+        }
+
+        @Override
+        public int docFreq() throws IOException {
+          return current.docCount;
+        }
+
+        @Override
+        public long totalTermFreq() throws IOException {
+          return -1;
+        }
+
+        @Override
+        public PostingsEnum postings(PostingsEnum reuse, int flags) throws IOException {
+          input.seek(current.ptr);
+          final MonotonicBlockPackedReader reader;
+          if (current.docCount >= Integer.MAX_VALUE) {
+            reader = MonotonicBlockPackedReader.of(input, PackedInts.VERSION_CURRENT, 64, current.docCount, false);
+          } else {
+            reader = null;
+          }
+          return new PostingsEnum() {
+            int doc;
+            int lastDoc = 0;
+            long pos = 0;
+
+            @Override
+            public int freq() throws IOException {
+              return -1;
+            }
+
+            @Override
+            public int nextPosition() throws IOException {
+              return -1;
+            }
+
+            @Override
+            public int startOffset() throws IOException {
+              return -1;
+            }
+
+            @Override
+            public int endOffset() throws IOException {
+              return -1;
+            }
+
+            @Override
+            public BytesRef getPayload() throws IOException {
+              return null;
+            }
+
+            @Override
+            public int docID() {
+              return doc;
+            }
+
+            @Override
+            public int nextDoc() throws IOException {
+              if (pos >= current.docCount) {
+                return NO_MORE_DOCS;
+              }
+              if (reader != null) {
+                doc = (int) reader.get(pos);
+              } else {
+                doc = input.readVInt() + lastDoc;
+                lastDoc = doc;
+              }
+              ++pos;
+              return doc;
+            }
+
+            @Override
+            public int advance(int target) throws IOException {
+              throw new IllegalArgumentException();
+            }
+
+            @Override
+            public long cost() {
+              return current.docCount;
+            }
+          };
+        }
+
+        @Override
+        public BytesRef next() throws IOException {
+          if (it.hasNext()) {
+            current = it.next();
+            return current.prefix;
+          }
+          current = null;
+          return null;
+        }
+      };
+    }
+
+    @Override
+    public long size() throws IOException {
+      return prefixes.size();
+    }
+
+    @Override
+    public long getSumTotalTermFreq() throws IOException {
+      return -1;
+    }
+
+    @Override
+    public long getSumDocFreq() throws IOException {
+      return -1;
+    }
+
+    @Override
+    public int getDocCount() throws IOException {
+      return docCount;
+    }
+
+    @Override
+    public boolean hasFreqs() {
+      return false;
+    }
+
+    @Override
+    public boolean hasOffsets() {
+      return false;
+    }
+
+    @Override
+    public boolean hasPositions() {
+      return false;
+    }
+
+    @Override
+    public boolean hasPayloads() {
+      return false;
+    }
+  }
+}
\ No newline at end of file
diff --git a/lucene/sandbox/src/java/org/apache/lucene/codecs/autoprefix/package-info.java b/lucene/sandbox/src/java/org/apache/lucene/codecs/autoprefix/package-info.java
new file mode 100644
index 0000000..dde6455
--- /dev/null
+++ b/lucene/sandbox/src/java/org/apache/lucene/codecs/autoprefix/package-info.java
@@ -0,0 +1,25 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+ 
+/** 
+ * A primary-key postings format that associates a version (long) with each term and
+ * can provide fail-fast lookups by ID and version.
+ */
+/**
+ * A postings format optimized for prefix queries on text fields.
+ */
+package org.apache.lucene.codecs.autoprefix;
\ No newline at end of file
diff --git a/lucene/sandbox/src/resources/META-INF/services/org.apache.lucene.codecs.PostingsFormat b/lucene/sandbox/src/resources/META-INF/services/org.apache.lucene.codecs.PostingsFormat
index a319f24..9cd2fb8 100644
--- a/lucene/sandbox/src/resources/META-INF/services/org.apache.lucene.codecs.PostingsFormat
+++ b/lucene/sandbox/src/resources/META-INF/services/org.apache.lucene.codecs.PostingsFormat
@@ -14,3 +14,4 @@
 #  limitations under the License.
 
 org.apache.lucene.codecs.idversion.IDVersionPostingsFormat
+org.apache.lucene.codecs.autoprefix.AutoPrefixPostingsFormat
diff --git a/lucene/sandbox/src/test/org/apache/lucene/codecs/autoprefix/AutoPrefixPerf.java b/lucene/sandbox/src/test/org/apache/lucene/codecs/autoprefix/AutoPrefixPerf.java
new file mode 100644
index 0000000..dad0c2b
--- /dev/null
+++ b/lucene/sandbox/src/test/org/apache/lucene/codecs/autoprefix/AutoPrefixPerf.java
@@ -0,0 +1,280 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.lucene.codecs.autoprefix;
+
+import java.io.BufferedReader;
+import java.io.ByteArrayOutputStream;
+import java.io.File;
+import java.io.FileInputStream;
+import java.io.IOException;
+import java.io.InputStream;
+import java.io.InputStreamReader;
+import java.io.PrintStream;
+import java.io.StringReader;
+import java.nio.file.Files;
+import java.nio.file.Path;
+import java.nio.file.Paths;
+import java.util.ArrayList;
+import java.util.List;
+
+import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.analysis.AnalyzerWrapper;
+import org.apache.lucene.analysis.TokenFilter;
+import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.analysis.core.KeywordAnalyzer;
+import org.apache.lucene.analysis.ngram.EdgeNGramTokenFilter;
+import org.apache.lucene.analysis.standard.StandardAnalyzer;
+import org.apache.lucene.analysis.tokenattributes.TermToBytesRefAttribute;
+import org.apache.lucene.codecs.Codec;
+import org.apache.lucene.codecs.PostingsFormat;
+import org.apache.lucene.codecs.lucene62.Lucene62Codec;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.document.FieldType;
+import org.apache.lucene.document.TextField;
+import org.apache.lucene.index.CheckIndex;
+import org.apache.lucene.index.DirectoryReader;
+import org.apache.lucene.index.IndexOptions;
+import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.index.IndexWriterConfig;
+import org.apache.lucene.index.LeafReaderContext;
+import org.apache.lucene.index.SegmentCommitInfo;
+import org.apache.lucene.index.SegmentReader;
+import org.apache.lucene.index.Term;
+import org.apache.lucene.search.BooleanClause;
+import org.apache.lucene.search.BooleanQuery;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.PrefixQuery;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.TermQuery;
+import org.apache.lucene.search.TopDocs;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.FSDirectory;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IOUtils;
+
+public class AutoPrefixPerf {
+  static Analyzer getEdgeNgramAnalyzer(Analyzer delegate, int minChars, int maxChars) {
+    return new AnalyzerWrapper(Analyzer.PER_FIELD_REUSE_STRATEGY) {
+      @Override
+      protected Analyzer getWrappedAnalyzer(String fieldName) {
+        return delegate;
+      }
+
+      @Override
+      protected TokenStreamComponents wrapComponents(String fieldName, TokenStreamComponents components) {
+        TokenFilter filter = new EdgeNGramTokenFilter(components.getTokenStream(), minChars, maxChars);
+        if (fieldName.endsWith("-edge")) {
+          return new TokenStreamComponents(components.getTokenizer(), filter);
+        }
+        return components;
+      }
+    };
+  }
+
+  static Analyzer createAnalyzer(String name) {
+    if (name.equals("standard")) {
+      return new StandardAnalyzer();
+    } else if (name.equals("keyword")) {
+      return new KeywordAnalyzer();
+    } else if (name.startsWith("edge_")) {
+      String suffix = name.substring(5);
+      String[] split = suffix.split("_");
+      int minChars = Integer.parseInt(split[0]);
+      int maxChars = Integer.parseInt(split[1]);
+      String delegate = split[2];
+      return getEdgeNgramAnalyzer(createAnalyzer(delegate), minChars, maxChars);
+    }
+    throw new IllegalArgumentException("Unknown analyzer " + name);
+  }
+
+  public static void main(String[] args) throws IOException {
+    String filename = args[0];
+    String queryFile = args[1];
+    String output = args[2];
+    String analyzerName = args[3];
+    int minAutoPrefix = args.length > 4 ? Integer.parseInt(args[4]) : -1;
+    int maxAutoPrefixSize = args.length > 5 ? Integer.parseInt(args[5]) : Integer.MAX_VALUE - 1;
+    Path indexPath = Paths.get(output);
+    boolean isEdge = analyzerName.startsWith("edge_");
+
+    Analyzer analyzer = createAnalyzer(analyzerName);
+    if (Files.notExists(indexPath)) {
+      Directory dir = FSDirectory.open(indexPath);
+      File file = new File(filename);
+      InputStream is = new FileInputStream(file);
+      BufferedReader reader = new BufferedReader(new InputStreamReader(is, "UTF-8"), 1 << 16);
+
+      IndexWriterConfig config = new IndexWriterConfig(analyzer);
+      if (minAutoPrefix != -1) {
+        PostingsFormat pf = new AutoPrefixPostingsFormat(minAutoPrefix, maxAutoPrefixSize);
+        config.setCodec(new Lucene62Codec() {
+          @Override
+          public PostingsFormat getPostingsFormatForField(String field) {
+            return pf;
+          }
+        });
+      }
+      IndexWriter writer = new IndexWriter(dir, config);
+
+      FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);
+      ft.setIndexOptions(IndexOptions.DOCS);
+      ft.setOmitNorms(true);
+      ft.freeze();
+      Document doc = new Document();
+      Field field = new Field("field", new StringReader(""), ft);
+      Field autoPrefixField = minAutoPrefix != -1 ? new Field("field-autoprefix", new StringReader(""), ft) : null;
+      Field edgeField = isEdge ? new Field("field-edge", new StringReader(""), ft) : null;
+      doc.add(field);
+      if (edgeField != null) {
+        doc.add(edgeField);
+      }
+      if (autoPrefixField != null) {
+        doc.add(autoPrefixField);
+      }
+
+      String line;
+      long startMS = System.currentTimeMillis();
+      long count = 0;
+      while ((line = reader.readLine()) != null) {
+        String value = line;
+        field.setReaderValue(new StringReader(value));
+        if (edgeField != null) {
+          edgeField.setReaderValue(new StringReader(value));
+        }
+        if (autoPrefixField != null) {
+          autoPrefixField.setReaderValue(new StringReader("fake"));
+        }
+        writer.addDocument(doc);
+        count++;
+        if (count % 200000 == 0) {
+          long ms = System.currentTimeMillis();
+          System.out.println("Indexed " + count + ": " + ((ms - startMS) / 1000.0) + " sec");
+        }
+      }
+
+      reader.close();
+      System.out.println("Final Indexed " + count + ": " + ((System.currentTimeMillis() - startMS) / 1000.0) + " sec");
+
+      System.out.println("Optimize...");
+      writer.forceMerge(1);
+      System.out.println("After force merge: " + ((System.currentTimeMillis() - startMS) / 1000.0) + " sec");
+
+      System.out.println("Close...");
+      writer.close();
+      System.out.println("After close: " + ((System.currentTimeMillis() - startMS) / 1000.0) + " sec");
+
+
+      // Print CheckIndex:
+      ByteArrayOutputStream bos = new ByteArrayOutputStream(1024);
+      CheckIndex checker = new CheckIndex(dir);
+      checker.setInfoStream(new PrintStream(bos, false, IOUtils.UTF_8), true);
+      CheckIndex.Status status = checker.checkIndex();
+      System.out.println("Done CheckIndex:");
+      System.out.println(bos.toString(IOUtils.UTF_8));
+      if (status.clean == false) {
+        throw new IllegalStateException("CheckIndex failed");
+      }
+
+      long totBytes = 0;
+      DirectoryReader directoryReader = DirectoryReader.open(dir);
+      for (LeafReaderContext context : directoryReader.leaves()) {
+        if (context.reader() instanceof SegmentReader) {
+          SegmentCommitInfo info = ((SegmentReader) context.reader()).getSegmentInfo();
+          totBytes += info.sizeInBytes();
+        }
+      }
+      directoryReader.close();
+      System.out.println("\nTotal index size: " + totBytes + " bytes");
+      dir.close();
+    } else {
+      System.out.println("Skip indexing: index already exists");
+      Directory dir = FSDirectory.open(indexPath);
+      Codec.reloadCodecs(AutoPrefixPostingsFormat.class.getClassLoader());
+      DirectoryReader directoryReader = DirectoryReader.open(dir);
+      IndexSearcher searcher = new IndexSearcher(directoryReader);
+      InputStream is = new FileInputStream(queryFile);
+      BufferedReader reader = new BufferedReader(new InputStreamReader(is, "UTF-8"), 1 << 16);
+      List<Query> queries = new ArrayList<>();
+      String line;
+      while ((line = reader.readLine()) != null) {
+        TokenStream tokenStream = analyzer.tokenStream("field", line);
+        tokenStream.reset();
+        TermToBytesRefAttribute termAtt = tokenStream.getAttribute(TermToBytesRefAttribute.class);
+        List<BytesRef> tokens = new ArrayList <> ();
+        while (tokenStream.incrementToken()) {
+          tokens.add(BytesRef.deepCopyOf(termAtt.getBytesRef()));
+        }
+        tokenStream.close();
+        for (int i = 2; i < 10; i++) {
+          for (BytesRef token : tokens) {
+            if (token.length <= i) {
+              continue;
+            }
+            BooleanQuery.Builder builder = new BooleanQuery.Builder();
+            final Query query;
+            BytesRef prefix = new BytesRef(token.bytes, token.offset, i);
+            if (minAutoPrefix != -1) {
+              query = new AutoPrefixQuery(new Term("field", prefix));
+            } else if (analyzerName.startsWith("edge_")) {
+              if (i > 5) {
+                query = new PrefixQuery(new Term("field-edge", prefix));
+              } else {
+                query = new TermQuery(new Term("field-edge", prefix));
+              }
+            } else {
+              query = new PrefixQuery(new Term("field", prefix));
+            }
+            builder.add(query, BooleanClause.Occur.SHOULD);
+            queries.add(builder.build());
+          }
+        }
+      }
+
+      long bestMS = Long.MAX_VALUE;
+      for (int iter = 0; iter < 10; iter++) {
+        long startMS = System.currentTimeMillis();
+        long totalHits = 0;
+        long hash = 0;
+        for (Query query : queries) {
+          TopDocs hits = searcher.search(query, 10);
+          totalHits += hits.totalHits;
+          hash = hash * 31 + hits.totalHits;
+        }
+        long ms = System.currentTimeMillis() - startMS;
+        System.out.println("iter " + iter + ": " + ms + " msec; totalHits=" + totalHits + " hash=" + hash);
+        if (ms < bestMS) {
+          System.out.println("  **");
+          bestMS = ms;
+        }
+      }
+
+      long t0 = System.currentTimeMillis();
+      long bytesUsed = 0;
+      for (int i = 0; i < 1000; i++) {
+        for (LeafReaderContext ctx : directoryReader.leaves()) {
+          bytesUsed += ((SegmentReader) ctx.reader()).ramBytesUsed();
+        }
+      }
+      System.out.println((System.currentTimeMillis() - t0) + " msec for 1000 ramBytesUsed: " + (bytesUsed / 1000));
+
+      directoryReader.close();
+      dir.close();
+    }
+  }
+}
diff --git a/lucene/sandbox/src/test/org/apache/lucene/codecs/autoprefix/TestAutoPrefixPostingsFormat.java b/lucene/sandbox/src/test/org/apache/lucene/codecs/autoprefix/TestAutoPrefixPostingsFormat.java
new file mode 100644
index 0000000..b3c9046
--- /dev/null
+++ b/lucene/sandbox/src/test/org/apache/lucene/codecs/autoprefix/TestAutoPrefixPostingsFormat.java
@@ -0,0 +1,55 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.lucene.codecs.autoprefix;
+
+import org.apache.lucene.codecs.Codec;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.document.FieldType;
+import org.apache.lucene.index.BasePostingsFormatTestCase;
+import org.apache.lucene.index.IndexOptions;
+import org.apache.lucene.util.TestUtil;
+
+public class TestAutoPrefixPostingsFormat extends BasePostingsFormatTestCase {
+  private final Codec codec = TestUtil.alwaysPostingsFormat(new AutoPrefixPostingsFormat());
+
+  @Override
+  protected Codec getCodec() {
+    return codec;
+  }
+
+  @Override
+  protected void addRandomFields(Document doc) {
+    for (IndexOptions opts : IndexOptions.values()) {
+      if (opts == IndexOptions.NONE) {
+        continue;
+      }
+      FieldType ft = new FieldType();
+      ft.setIndexOptions(opts);
+      ft.freeze();
+      FieldType ft2 = new FieldType();
+      ft2.setIndexOptions(IndexOptions.DOCS);
+      ft2.freeze();
+      final int numFields = random().nextInt(5);
+      for (int j = 0; j < numFields; ++j) {
+        doc.add(new Field("f_" + opts, TestUtil.randomSimpleString(random(), 10), ft));
+        doc.add(new Field("f_" + opts + "-autoprefix", "fake", ft2));
+      }
+    }
+  }
+}
diff --git a/lucene/sandbox/src/test/org/apache/lucene/sandbox/queries/AutoPrefixQueryTest.java b/lucene/sandbox/src/test/org/apache/lucene/sandbox/queries/AutoPrefixQueryTest.java
new file mode 100644
index 0000000..2d4dd4d
--- /dev/null
+++ b/lucene/sandbox/src/test/org/apache/lucene/sandbox/queries/AutoPrefixQueryTest.java
@@ -0,0 +1,167 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.lucene.sandbox.queries;
+
+import java.io.IOException;
+import java.util.HashSet;
+import java.util.Set;
+
+import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.analysis.MockAnalyzer;
+import org.apache.lucene.codecs.autoprefix.AutoPrefixPostingsFormat;
+import org.apache.lucene.codecs.autoprefix.AutoPrefixQuery;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.LeafReaderContext;
+import org.apache.lucene.index.MultiFields;
+import org.apache.lucene.index.RandomIndexWriter;
+import org.apache.lucene.index.Term;
+import org.apache.lucene.index.Terms;
+import org.apache.lucene.index.TermsEnum;
+import org.apache.lucene.search.Collector;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.LeafCollector;
+import org.apache.lucene.search.PrefixQuery;
+import org.apache.lucene.search.Scorer;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util.LuceneTestCase;
+import org.apache.lucene.util.TestUtil;
+
+public class AutoPrefixQueryTest extends LuceneTestCase {
+  private Directory directory;
+  private IndexSearcher searcher;
+  private IndexReader reader;
+  private Analyzer analyzer;
+
+  @Override
+  public void setUp() throws Exception {
+    super.setUp();
+
+    analyzer = new MockAnalyzer(random());
+    directory = newDirectory();
+    RandomIndexWriter writer = new RandomIndexWriter(random(), directory,
+        newIndexWriterConfig(analyzer)
+            .setMergePolicy(newLogMergePolicy())
+            .setCodec(TestUtil.alwaysPostingsFormat(new AutoPrefixPostingsFormat())));
+    addDoc(writer, "Hotel Cala'n Blanes", "1");
+    addDoc(writer, "Hotel Calabahia", "2");
+    addDoc(writer, "Hotel Calacoto", "3");
+    addDoc(writer, "Hotel Calamidoro", "4");
+    addDoc(writer, "Hotel Calarasi", "5");
+    addDoc(writer, "Hotel Calas de Ibiza", "6");
+    addDoc(writer, "Hotel Calasanz Real", "7");
+    addDoc(writer, "Hotel Caleta Beach Resort Fishing and Diving Club", "8");
+    addDoc(writer, "Hotel Cali Plaza Granada", "9");
+    addDoc(writer, "Hotel Caliber", "10");
+    addDoc(writer, "Hotel Califfo", "11");
+    addDoc(writer, "Hotel California Bangkok", "12");
+    addDoc(writer, "Hotel California Istanbul", "13");
+    addDoc(writer, "Hotel California Saint Germain", "14");
+    addDoc(writer, "Hotel California L.L.C.", "15");
+    addDoc(writer, "Hotel Calinda Geneve Mexico City", "16");
+    addDoc(writer, "Hotel Calipolis", "17");
+    addDoc(writer, "Hotel Call", "18");
+    addDoc(writer, "Hotel Calypso", "19");
+    addDoc(writer, "Hotel Cam", "20");
+    addDoc(writer, "Hotel Camarao", "21");
+    addDoc(writer, "Hotel Cambiocavallo", "22");
+    addDoc(writer, "Pic Cove", "23");
+    addDoc(writer, "Pic Cove Head", "24");
+    addDoc(writer, "Pic de la Hutte", "25");
+    addDoc(writer, "Pic de l'Ours", "26");
+    addDoc(writer, "Pic du chevreuil ", "27");
+    addDoc(writer, "Pic du chemin", "28");
+    reader = writer.getReader();
+    writer.close();
+    searcher = newSearcher(reader);
+  }
+
+  @Override
+  public void tearDown() throws Exception {
+    IOUtils.close(reader, directory, analyzer);
+    super.tearDown();
+  }
+
+  private void addDoc(RandomIndexWriter writer, String name, String id) throws IOException {
+    Document doc = new Document();
+    doc.add(newStringField("title", name, Field.Store.YES));
+    doc.add(newStringField("id", id, Field.Store.YES));
+    doc.add(newStringField("title-autoprefix", "fake", Field.Store.YES));
+    writer.addDocument(doc);
+  }
+
+  public void testAgainstPrefixQuery() throws Exception {
+    Terms terms = MultiFields.getFields(reader).terms("title");
+    TermsEnum termsEnum = terms.iterator();
+    while (termsEnum.next() != null) {
+      for (int i = 1; i < termsEnum.term().length; i++) {
+        BytesRef prefix = new BytesRef(termsEnum.term().bytes, termsEnum.term().offset, i);
+        AutoPrefixQuery autoPrefixQuery = new AutoPrefixQuery(new Term("title", prefix));
+        PrefixQuery prefixQuery = new PrefixQuery(new Term("title", prefix));
+        assertEquals(searcher.count(autoPrefixQuery), searcher.count(prefixQuery));
+        Set<Integer> seenDocs = new HashSet<>();
+        searcher.search(prefixQuery, new Collector() {
+          @Override
+          public LeafCollector getLeafCollector(LeafReaderContext context) throws IOException {
+            return new LeafCollector() {
+              @Override
+              public void setScorer(Scorer scorer) throws IOException {
+
+              }
+
+              @Override
+              public void collect(int doc) throws IOException {
+                seenDocs.add(doc + context.docBase);
+              }
+            };
+          }
+
+          @Override
+          public boolean needsScores() {
+            return false;
+          }
+        });
+        searcher.search(autoPrefixQuery, new Collector() {
+          @Override
+          public LeafCollector getLeafCollector(LeafReaderContext context) throws IOException {
+            return new LeafCollector() {
+              @Override
+              public void setScorer(Scorer scorer) throws IOException {
+
+              }
+
+              @Override
+              public void collect(int doc) throws IOException {
+                seenDocs.remove(doc + context.docBase);
+              }
+            };
+          }
+
+          @Override
+          public boolean needsScores() {
+            return false;
+          }
+        });
+        assertTrue(seenDocs.size() == 0);
+      }
+    }
+  }
+}
